IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

1

Design Automation for Fast, Lightweight, and
Effective Deep Learning Models: A Survey

Dalin Zhang, Member, IEEE, Kaixuan Chen, Member, IEEE, Yan Zhao, Member, IEEE, Bin Yang, Senior
Member, IEEE, Lina Yao, Senior Member, IEEE, and Christian S. Jensen, Fellow, IEEE

2
2
0
2

g
u
A
2
2

]

G
L
.
s
c
[

1
v
8
9
4
0
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—Deep learning technologies have demonstrated re-
markable effectiveness in a wide range of tasks, and deep
learning holds the potential to advance a multitude of appli-
cations, including in edge computing, where deep models are
deployed on edge devices to enable instant data processing
and response. A key challenge is that while the application of
deep models often incurs substantial memory and computational
costs, edge devices typically offer only very limited storage and
computational capabilities that may vary substantially across
devices. These characteristics make it difﬁcult to build deep
learning solutions that unleash the potential of edge devices
while complying with their constraints. A promising approach
to addressing this challenge is to automate the design of effective
deep learning models that are lightweight, require only a little
storage, and incur only low computational overheads. This survey
offers comprehensive coverage of studies of design automation
techniques for deep learning models targeting edge computing.
It offers an overview and comparison of key metrics that are
used commonly to quantify the proﬁciency of models in terms
of effectiveness, lightness, and computational costs. The survey
then proceeds to cover three categories of the state-of-the-art
of deep model design automation techniques: automated neural
architecture search, automated model compression, and joint
automated design and compression. Finally, the survey covers
open issues and directions for future research.

Index Terms—deep learning, neural architecture search,

lightweight model, model compression

I. INTRODUCTION

A. Background

Deep learning has achieved state-of-the-art performance
at a multitude of tasks and has affected people’s lives in
myriad areas, including recommendation systems [1], natural
language understanding [2], and biomedical engineering [3].
Deep learning frees researchers from manually designing
purposeful feature representations of objects by introducing
multi-layer neural architectures capable of automatic feature
extraction. This enables researchers to work at a higher level
of abstraction, focusing on architecture engineering rather
than on feature engineering and model building. The neural
architectures of deep models tend to be increasingly intricate
and complex, thus requiring substantial hardware resources
for deployment. This may not be a problem when a powerful

Dalin Zhang, Kaixuan Chen, Yan Zhao, Bin Yang, and Christian S.
Jensen is with the Department of Computer Science, Aalborg University,
Aalborg Øst 9220, Denmark (e-mail: dalinz@cs.aau.dk, kchen@cs.aau.dk,
yanz@cs.aau.dk, byang@cs.aau.dk, and csj@cs.aau.dk).

Lina Yao is with the School of Computer Science and Engineering,
University of New South Wales, UNSW Sydney 2052, Australia (e-mail:
lina.yao@unsw.edu.au).

server is available, but important settings occur where this is
not the case:: 1) computing on mobile hardware, e.g., smart-
phones and tablet PCs; 2) computing on industrial hardware
optimized for low deployment cost and low power consump-
tion. Furthermore, although cloud computing may be available,
it is often fundamentally unattractive to transfer data from
edge devices to the cloud. On the one hand, such transfer may
incur privacy, ownership, and consequent regulatory concerns,
including for human-related data like audio and video data
and utility consumption data [4]; on the other hand, data
transfer incurs substantial latency due to low bandwidth “last
mile” connectivity. Last but not least, deployment of complex
models is expensive, due to the cost of specialized hardware
and energy consumption, and it is also bad for the environment
due to the carbon footprint of producing the required electricity
[5], [6].

Fortunately, research has demonstrated that deep learning
models generally have large numbers of redundant parameters
and computations that contribute to their performance [7],
[8], so there is considerable room for reducing redundancies
without compromising model accuracy. Hence, it is highly
desirable and completely possible to design simpliﬁed deep
learning models with reduced computational complexity, thus
achieving lighter weight and more efﬁcient deep models*.

B. Design of Efﬁcient Deep Learning Models

Research on building efﬁcient deep learning models can be
categorized into two categories: efﬁcient network architecture
design and model compression. In efﬁcient network architec-
ture design, the aim is to create compact neural modules and
to connect these according to a carefully designed topology.
The objective is to achieve efﬁcient deep learning models
with acceptable accuracy but small structures (low memory
requirement) and low computational complexity (high speed).
MobileNet [9] proposes an efﬁcient network structure using a
depthwise separable convolution module as the basic building
block, and superior size, speed, and accuracy characteristics
over a variety of computer vision tasks are documented. A
second version, called MobileNetV2, incorporates a new basic
building block, bottleneck depth-separable convolution with
residuals/inverted residual [10]. As a result, MobileNetV2
generally needs 30% fewer parameters, requires two times
fewer operations and is about 30–40% faster on a Google Pixel
phone while achieving higher accuracy than its predecessor. In

*By efﬁcient deep learning models we mean deep learning models with

Manuscript received March 10, 2022

low memory usage or low inference cost or latency.

 
 
 
 
 
 
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

2

ShufﬂeNetV1 [11], bottleneck-like structures with pointwise
group convolutions and ”channel shufﬂe” operations are the
basic building blocks that are used to achieve efﬁcient neural
structures. Like in the MobileNet case, ShufﬂeNet also has
a second generation, called ShufﬂeNetV2 [12]. Here a new
“channel split” operation is introduced in order to further im-
prove speed and accuracy. In addition, four practical guidelines
for efﬁcient network architecture design are provided.

In contrast to directly designing efﬁcient architecture from
a pool of basic building blocks, model compression aims at
modifying a given neural model to reduce its memory and
computational cost. Pruning is one such powerful technique
that tries to remove unimportant components from a model
[13], [14]. It is ﬂexible in that it is possible to remove layers,
neurons, connections, or channels. While pruning shrinks a
model by removing redundant parts, quantization aims to
reduce the number of bits required to represent model pa-
rameters [15]. Most processors use 32 bits or more to store
the parameters of a deep model. However, research estimates
that the human brain stores information in a discrete format
that uses 4–7 bits [16], [17], [18]. Indeed, many efforts have
been devoted to investigating using fewer bits to store model
parameters to reduce memory and computational cost [19],
[20]. Other techniques like knowledge distillation [21] and
tensor decomposition [22] are also popular and effective at
compressing deep models.

C. Design Automation for Efﬁcient Deep Learning Models

Although remarkable progress has been achieved in building
efﬁcient deep learning models, initial proposals were heuristic
rule-based and hand-tuned with inevitable limitations. First,
building an efﬁcient deep learning model still requires ad-
vanced prior knowledge and experience, making it difﬁcult
for beginners and even deep learning experts without domain
knowledge to develop specialized models that meet given
requirements. Second, as it is impossible to apply the same
uniform model across diverse mobile platforms and tasks,
enabling specializations that address such diversity is essential.
Yet, it remains excessively time-consuming and inconvenient.
Third, hand-crafted rules offer limited capabilities at utilizing
hardware potentials fully, while satisfying the size and latency
requirements. Different deep learning models can satisfy the
same hardware constraints, and it is impractical to manually
exhaust all possibilities.

Observations such as the above have prompted a multitude
of studies of design automation techniques for efﬁcient deep
learning models. Fig. 1 shows a general automated design
process. The design automation algorithm (i.e., Automatic
Designer) applies a search strategy to ﬁnd network archi-
tectures and compression in the predeﬁned Deep Learning
Architecture Design Space; next, a speciﬁc deep learning
model is Derived through executing the identiﬁed operations
and is then Deployed on target devices (e.g., CPU, GPU,
or IoT devices); lastly, model performance metrics such as
accuracy, latency, and memory use are estimated and provided
to the Automatic Designer. In the next iteration, the Automatic
Designer considers both the feedback and specialized con-
straints and takes a new design action to ﬁnd a better model in

the design space. This process is repeated until a satisfactory
model is achieved.

Neural architecture search (NAS) aims to automate the
design of neural networks that achieve the best possible
accuracy. Next, more targeted studies that aim to automate
the design of efﬁcient neural networks build on generic NAS
and involve the design of search spaces and the modiﬁcation of
the optimization objective from sole accuracy to both accuracy
and efﬁciency. Cai et al. [23] incorporate model latency into
the optimization goal of their binarized design automation
framework (i.e., ProxylessNAS) by means of a differentiable
loss. Due to targeting optimized inference latency directly,
ProxylessNAS can achieve efﬁcient neural architectures 1.83
times faster than MobileNetV2 with the same level of top-
1 accuracy. Notably, the search space of ProxylessNAS is
based on the inverted residual blocks of different convolution
sizes, as proposed by MobleNetV2. This implies that design
automation is not only able to reduce human labor but even
enables architectures that surpass handcrafted architectures.

In addition to studies that target the direct design of efﬁcient
neural networks, other studies target the automated compres-
sion of deep neural networks [24], [25], [26]. For example,
Xiao et al. [24] design an automatic pruning approach that is
based on learnable pruning indicators instead of pruning rules
designed individually for speciﬁc architectures and datasets.
This approach achieves superior compression performance on
different widely-used neural models (e.g., AlexNet, ResNet,
and MobileNet). Considering the progress in, and promise of,
design automation for efﬁcient deep learning models, it is a
comprehensive and systematic survey is called for and holds
the potential to accelerate future research.

D. Key Contributions

To the best of our knowledge, this is the ﬁrst survey of
state-of-the-art design automation methods that target fast,
lightweight, and effective deep learning models. The key
contributions of the survey are summarized as follows:

• We provide a comprehensive review of design automation
techniques targeting fast, lightweight, and effective deep
learning models. In doing so, more than 150 papers are
covered, analyzed, and compared.

• We propose a new taxonomy of deep design automation
methods from the perspectives of how to design, i.e., by
search (Section III), by compression (Section IV), or by
joint search and compression (Section V); and by what to
design, i.e., the search space, the search strategy, and the
performance estimation strategy; and by what to com-
press, e.g., tensors, knowledge, and representation. The
detailed categories provide convenience to the readers in
obtaining an overview of the literature and identifying a
direction of interest.

• We summarize and compare the evaluation metrics that
are used for both the obtained models and the design
approaches. By means of the comparison, we emphasize
the role of each metric and explicate the associated pros
and cons.

• We discuss open issues and identify future directions on

automated design and compression.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 1: Design Automation for Efﬁcient Deep Learning Models. The automatic designer takes the hardware constraints into
its search strategy to explore a predeﬁned deep learning architecture design space which could be either an architecture space
or a compression space; with the explored design choices, a new deep learning model from scratch or a compressed deep
learning model is derived and deployed on the target device; the model’s performance including both accuracy and hardware
consumption is ﬁnally estimated and fed into the automatic designer.

The remainder of the survey is organized as follows:
Section II summarizes the evaluation metrics of efﬁciency
(i.e., speed and lightness) and effectiveness of deep learning
models. Section III covers studies of searching for efﬁcient
deep models. Section IV then covers research on automated
compression, and Section V considers studies of joint auto-
mated search and compression. Section VI presents research
directions, and Section VII concludes the survey.

II. EVALUATION METRICS

Different evaluation metrics or objectives may lead to dif-
ferent or even opposite conclusions. Thus, it is indispensable
to introduce and distinguish the relevant evaluation metrics be-
fore diving into the details of efﬁcient models. In this section,
we will introduce the evaluation metrics for measuring the
efﬁciency and effectiveness of the obtained efﬁcient models.
These metrics are also critical to evaluating the effectiveness of
a design automation approach. Furthermore, we introduce met-
rics for evaluating the cost of a design automation approach
as well. We brieﬂy summarize the characteristics including
advantages and limitations of the commonly used evaluation
metrics in Table I.

A. Device-agnostic Evaluation Metrics

The device-agnostic metrics can be calculated directly from
the model architecture without real-world implementation on

hardware. These metrics do not essentially reﬂect a model’s
real performance that we care about [12], [27], [28].

1) FLoating-point OPerations (FLOPs): FLOPs is gener-
ally deﬁned as the number of ﬂoating-point multiplication-add
operations in a model for approximating the latency/speed
or computation complexity [11], [12], [29]. There are also
other commonly used analogous metrics, such as the num-
ber of multiply-add operations (MAdds) and the number
of multiply-accumulate operations (MACs) [30], [27], [31].
However, one contention remains regarding the deﬁnition:
whether multiplication-add should be considered as one or
two operations. Some researchers argue that in many recent
deep learning models, convolutions are bias-free and it makes
sense to count multiplication and add as separate FLOPs [32].
Moreover, some non-multiplication or non-add operations re-
quire FLOPs in some implementations as well, such as an
activation layer [33], [27]. Whether such operations should be
counted into total FLOPs is also a dispute.

In addition to the inconsistency of the deﬁnition,

there
exist three main issues that induce the discrepancy between
FLOPs and real latency. First, counting only FLOPs ignores
some decisive factors that affect latency remarkably. One such
factor is parallelism. With the same FLOPs, a model with a
high degree of parallelism may be much faster than another
model with a low degree of parallelism [34], [35]. Another
important factor arises from the memory access cost. Since
the on-device RAM is usually limited, data cannot be entirely

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

4

h
c
a
o
r
p
p
A
n
o
i
t
a
m
o
t
u
A
n
g
i
s
e
D
e
h
t

d
n
a

l
e
d
o
M

t
n
e
i
c
ﬁ
f
E

d
e
n
i
a
t
b
O
e
h
t

h
t
o
B

f
o

s
s
e
n
e
v
i
t
c
e
f
f
E

d
n
a

y
c
n
e
i
c
ﬁ
f
E

e
h
t

g
n
i
r
u
s
a
e

M

r
o
f

s
c
i
r
t
e

M
n
o
i
t
a
u
l
a
v
E

:
I

E
L
B
A
T

m
s
i
l
e
l
l
a
r
a
p
l
a
n
o
i
t
a
t
u
p
m
o
c

f
o
e
e
r
g
e
d
e
h
t
g
n
i
t
t
i

m
o

s
n
o
i
t
i
n
ﬁ
e
d

t
n
e
t
s
i
s
n
o
c
n
i

y
r
a
r
b
i
l

n
o
i
t
a
t
n
e
m
e
l
p
m

i

e
h
t

g
n
i
t
t
i

m
o

t
s
o
c

s
s
e
c
c
a

y
r
o
m
e
m
e
h
t

g
n
i
t
t
i

m
o

•

•

•

•

y
c
n
e
t
a
l

f
o

n
o
i
t
a
m
i
x
o
r
p
p
a

e
s
r
a
o
c

n
i
a
t
b
o

o
t

h
c
a
e

s
c
i
t
s
i
r
e
t
c
a
r
a
h
C

s
n
o
i
t
a
t
i

m
L

i

s
e
g
a
t
n
a
v
d
A

e
g
a
s
u

y
r
o
m
e
m
k
a
e
p

t
c
e
ﬂ
e
r

o
t

e
l
b
a
n
u

•

g
n
i
r
u
s
a
e
m

r
o
f

g
n
i
h
c
a
c

n
o
i
t
a
t
u
p
m
o
c

e
h
t

g
n
i
t
t
i

m
o

•

n
i
a
t
b
o

o
t

y
s
a
e

y
r
o
m
e
m

t
n
e
m
e
r
i
u
q
e
r

e
g
a
r
o
t
s

g
n
i
r
u
s
a
e
m
n
e
h
w
e
s
i
c
e
r
p

s
l
e
d
o
m
n
o
i
t
c
i
d
e
r
p

r
o

e
r
a
w
d
r
a
h

l
a
e
r

n
o

n
o
i
t
a
t
n
e
m
e
l
p
m

i

h
g
u
o
r
h
t

n
i
a
t
b
o

s
e
i
r
a
r
b
i
l

n
o
i
t
a
t
n
e
m
e
l
p
m

i

d
n
a

s

m
r
o
f
t
a
l
p

e
r
a
w
d
r
a
h

t
n
e
r
e
f
f
i
d

n
o

d
n
e
p
e
d

t
u
o
b
a

e
r
a
c

e
w

t
a
h
t

n
o
i
r
e
t
i
r
c

l
a
e
r

e
h
t

s
l
e
d
o
m
n
o
i
t
c
i
d
e
r
p

r
o

e
r
a
w
d
r
a
h

l
a
e
r

n
o

n
o
i
t
a
t
n
e
m
e
l
p
m

i

h
g
u
o
r
h
t

n
i
a
t
b
o

s
r
e
t
t
a
m
y
l
l
a
e
r

t
a
h
t

e
g
a
s
u

k
a
e
p

e
h
t

t
c
e
ﬂ
e
r

t
u
o
b
a

e
r
a
c

e
w

t
a
h
t

n
o
i
r
e
t
i
r
c

l
a
e
r

e
h
t

s
l
e
d
o
m
g
n
i
n
r
a
e
l

p
e
e
d

l
a
m
r
o
n

g
n
i
t
a
u
l
a
v
e

r
o
f

e
s
o
h
t

o
t

l
a
c
i
t
n
e
d
i

s
k
s
a
t

g
n
i
t
e
g
r
a
t

e
h
t

n
o

t
n
e
d
n
e
p
e
d

d
n
a

e
s
r
e
v
i
d

d
o
h
t
e
m
n
o
i
t
a
m
o
t
u
a

n
g
i
s
e
d

a

f
o

d
e
e
p
s

e
h
t

e
t
a
u
l
a
v
e

o
t

d
e
s
u

d
e
s
u

s
U
P
G

f
o

r
e
b
m
u
n

e
h
t

n
o

d
n
e
p
e
d

n
o
i
t
a
m
o
t
u
a

n
g
i
s
e
d

a

f
o

t
n
e
m
e
r
i
u
q
e
r

y
r
o
m
e
m
e
h
t

e
t
a
u
l
a
v
e

o
t

d
e
s
u

t
e
s

e
t
a
d
i
d
n
a
c

e
h
t

f
o

e
z
i
s

e
h
t

.
t
.
r
.

w
y
l
r
a
e
n
i
l

s
w
o
r
g

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

s
c
i
r
t
e

M

s
P
O
L
F

s
r
e
t
e
m
a
r
a
P

f
o

r
e
b
m
u
N

y
c
n
e
t
a
L

e
g
a
s
U
y
r
o
m
e
M
k
a
e
P

y
c
n
e
i
c
ﬁ
f
E

c
i
t
s
o
n
g
a
-
e
c
i
v
e
D

s
c
i
r
t
e

M
n
o
i
t
a
u
l
a
v
E

y
c
n
e
i
c
ﬁ
f
E

e
r
a
w
a
-
e
c
i
v
e
D

s
c
i
r
t
e

M
n
o
i
t
a
u
l
a
v
E

I

.
c
t
e
U
O
m
/
P
A
m
/
y
c
a
r
u
c
c
A

n
o
i
t
a
u
l
a
v
E

s
s
e
n
e
v
i
t
c
e
f
f
E

s
c
i
r
t
e

M

s
r
u
o
H
U
P
G

y
r
o
m
e
M
U
P
G

t
s
o
C

n
o
i
t
a
m
o
t
u
A
n
g
i
s
e
D

s
c
i
r
t
e

M

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

5

loaded at one time and thus reading data from external memory
is required. It has an increasing impact on the latency as the
computation unit is getting stronger recently, thus becoming
the bottleneck for latency. This intrinsic factor should not be
simply neglected. Third, the implementation library of a deep
learning model signiﬁcantly inﬂuences its latency as well.
For example, NVIDIA’s cuDNN library provides different
implementations of a convolution operation [36] that clearly
require different amounts of FLOPs although the network
architecture is identical. Some works also found that a smaller
number of FLOPs could be even slower due to the library
abstractions [12], [13].

it

2) Number of Parameters: It is usually required to ﬁt all
parameters of a neural network within on-chip memory to
execute the model fast [37]. Thus, the number of parameters
mainly constrains the memory requirement of a deep learning
model. Although some mobile devices like smartphones have
abundant memory, there are still a few mobile devices that
have particularly scarce memory, such as microcontrollers
that typically have 10’s-100’s of KB [38]. These memory-
scarce devices are in demand in many ﬁelds due to their
low prices. Thus,
is desirable to design a small-sized
model that can ﬁt into “tiny” memory and the number of
parameters is a common device-agnostic metric for evaluating
such a requirement. Nevertheless, the number of parameters
only accounts for a portion of memory usage; input data,
computation caching (i.e., intermediate tensors produced at
runtime), and network structure information take over a rela-
tively larger portion of memory [39], [40]. In some extreme
scenarios, only computation caching and active parameters
(i.e., the parameters used for current computation) occupy
memory. Some researchers propose to optimize the in-memory
computation caching to reduce memory consumption but their
performance largely depends on the network structure [41].
Thus, a model with a larger number of parameters is not
certainly more memory-hungry than a model with a smaller
number of parameters. On the other hand, model parameters
account for a major component of storage/external memory
(e.g., FLASH memory) usage.

B. Device-aware Efﬁciency Evaluation Metrics

Unlike device-agnostic metrics, device-aware metrics can
reﬂect the computational cost that we really care about. They
can be collected on real and target hardware platforms or
approximated.

1) Latency: The latency is used to evaluate the running
speed of a deep model making inferences. It
is usually
measured in the form of running time per inference (e.g.,
millisecond) [42] or inferences per unit time (e.g., batches/s or
images/s) [12]. In practice, the precise latency is an average
value computed on a large batch or several bathes [42]. Some
works collect this information through implementations on real
devices including GPUs, TPUs, and CPUs [25], [12], [43],
[42]. It should be noted that the reliability is unknown when
evaluating a deep model’s latency not on its target mobile
devices (e.g., smartphone CPUs) but on non-mobile devices
(e.g., GPUs). In addition to directly measuring latency, some

researchers try to approximate it [23], [44]. The lookup table
is a latency approximation method that enumerates all the
possible layers that a family of models can have along with
the latency of each of the layers [44]. It is hardware and
model family-speciﬁc and requires a signiﬁcant amount of
time to maintain a large and dynamic database. In addition,
this simple summation of the latency of an individual does not
take memory access cost and parallelism into consideration,
and thus shows low precision. A latency prediction model is
another latency approximation method that models the network
latency as a function of network structures and/or hardware
parameters [23], [45], [44]. This approach can make latency
differentiable to be directly involved in an objective function
for gradient-based optimization [23].

2) Memory Usage: Different from latency where we care
about the total inference time of a sample/batch, the peak
memory usage during inference is our major concern [37],
[46]. As long as the peak memory usage is lower than the
memory capacity, a model is able to run on the device. It is
not necessary to keep memory usage as low as possible. Ex-
treme scenarios with maximal memory saving are considered.
The peak memory usage is dominated by the intermediate
tensors (so-called activation metrics), so a small model (i.e.,
a small number of model parameters) doesn’t guarantee a
low peak memory usage. For example, at similar ImageNet
accuracy (70%), even though MobileNetV2 [10] reduces its
model size by 4.6× compared to ResNet-18 [47], the peak
memory requirement increases by 1.8× [46]. Thus, it is highly
recommended to evaluate peak memory usage when targeting
a device with quite constrained memory resources.

In addition to the peak memory usage, which considers
the on-chip memory, storage/external-memory usage is also
an essential evaluation metric. It mainly restricts the model
size of which model parameters occupy the largest proportion.
Therefore, the bit-precision of parameters has a crucial impact
on the external-memory usage.

C. Effectiveness Evaluation Metrics

The metrics for evaluating the effectiveness of an efﬁcient
model are quite diverse and mainly dependent on the tar-
geting tasks. Vision tasks, such as image recognition, object
detection, and semantic segmentation, are such commonly
used benchmark tasks [12], [10], [48]. Different tasks have
different evaluation metrics: top-1 or top 5 accuracy for image
recognition [48], mAP for object detection [48], and mIOU for
semantic segmentation [10]. In addition to vision tasks, audio
tasks, such as keyword spotting, are leveraged as an evaluation
task [46]. However, accuracy is also used as the evaluation
metric in this case.

D. Design Automation Cost Metrics

As the price of freeing human efforts, design automation
normally demands an excessive computational cost that pro-
hibits its wide deployment.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

6

1) GPU Hours: GPU hours/days are metrics used to eval-
uate the time cost of a design automation method especially a
NAS-based method [49], [50]. The GPU days can be deﬁned
as:

GPU days = N × t,

(1)

where N denotes the number of GPUs, and t denotes the
number of days that are used for searching [51]. GPU hours
have a similar deﬁnition. At the early stage, it requires several
or even tens of days for searching [52], [49], while currently
researchers have pushed the time to the magnitude of multiple
hours [53].

2) GPU Memory: Although differentiable neural architec-
ture search has reduced the cost of GPU hours considerably, it
suffers from an intensive GPU memory cost. The consumption
of GPU memory depends on the size of the candidate set for
searching. Speciﬁcally, the required memory grows linearly
w.r.t. the number of choices in a candidate set [54]. This
issue restricts the search space size that prevents the capability
of discovering novel and strong models. Some works have
targeted this issue and achieved impressive progress [23].
Thus, it is important to involve GPU memory for a thorough
evaluation.

III. SEARCH FOR EFFICIENT DEEP LEARNING MODELS

Driven by the growing demand for mobile applications,
efﬁcient deep learning models have gained explosive attention.
A tremendous number of studies have been proposed to
explore manually designed efﬁcient neural architectures or
modules and achieved impressive progress [29], [12], [10],
[55], [56]. Though the notable success, it is challenging for
human engineers to heuristically exhaust the design space
to trade off accuracy and hardware constraints. Hardware-
aware neural architecture search plays an inﬂuential role in
advancing this ﬁeld as it automates the design process to
ﬁnd an optimal solution. Similar to regular NAS [57], the
hardware-aware NAS also has three components, i.e., search
space, search strategy, and performance estimation strategy,
but with additional freedom or constraints (Fig. 1 left bottom).
In this section, we review recent achievements from these three
aspects and summarize the main results in TABLE II.

A. Efﬁcient Search Space

A search space is the basis of NAS and determines what
architectures NAS can discover in principle and their per-
formance upper-limit [57]. A well-deﬁned search space can
not only accelerate the search process but also promote the
searched model’s performance [49], [58]. Since some man-
ually explored efﬁcient neural architectures have achieved
considerable advances, it is an intuitive yet practical idea to
construct a search space with the heuristics of these hand-
crafted efﬁcient structures, and leverage the NAS technology
to automatically discover novel efﬁcient models upon this
search space. There are usually two components that deﬁne
a search space: (i) operators that each layer executes, and (ii)
a backbone that decides the topological connections of these
layers.

1) Operators: Operators, such as convolutional layers or
fully connected layers, are basic building components for a
deep learning model. A simple way to build a deep learning
model is to directly stack several operators, such as VGG Net
[59]. Other work connects several neural network layers to
construct a motif (e.g., a residual block) and builds a model
by repeating and arbitrarily connecting these motifs [60], [47],
[57]. In this paper, we use the term ”operator” to also represent
a motif, which is usually used as a whole and regarded as a
basic component of an end-to-end model.

As early-developed and widely-demonstrated neural archi-
tectures, the convolutional layer and its variants are the domi-
nant operators of many deep learning models, especially those
in the computer vision area. A standard convolution can be
denoted as:

Conv(W, X)(i,j) =

M,N,K
(cid:88)

m,n,k

w(m, n, k) · x(i+m, j+n, k),

(2)

where W is the convolution kernel weight, X is the input
feature maps to a convolution layer, (i, j) is the coordinate
of an output feature map, and (m, n, k) is the coordinate
of the convolutional kernel. NSGA-Net [61] utilizes standard
convolutions as the operator and devotes to automatically
determining their connections in a block-based manner. As
it only seeks to optimize the connection of operators, limited
efﬁciency is gained. Scheidegger et al. [62] and Lu et al. [63]
also consider standard convolutions but, instead of optimizing
the connections, they allow to search the convolution hyper-
parameters such as the ﬁlter size, the stride and the number
of ﬁlters.

In addition to the

standard convolutions,

extensive
manually-designed variants have been demonstrated both ef-
ﬁcient and effective. Therefore, it is intuitive to employ these
architectures to construct an efﬁcient search space. The depth-
wise separable convolution (DSConv), which can signiﬁcantly
reduce the number of parameters and computation and works
as the primary module in MobileNet [9], is such a widely
used efﬁcient operator. As shown in Fig 2a, a DSConv block
is made up of two components: a depthwise convolution and a
pointwise convolution. The depthwise convolutions (DWConv)
applies a single ﬁlter per each input feature map (input depth)
for spatial ﬁltering:

DWConv(W, X)(i,j) =

M,N
(cid:88)

m,n

w(m, n) · x(i+m, j+n).

(3)

The pointwise convolution (PWConv), a simple 1 × 1 convo-
lution, is used to create a linear combination of the output of
the depthwise convolutions for feature fusion:

PWConv(W, X)(i,j) =

K
(cid:88)

k

wk · x(i, j).

(4)

Thus, by combining DWConv and PWConv, the DSConv is
denoted as:

DSConv(Wp, Wd, X)(i,j) =

PWConv(Wp, DWConv(Wd, X)(i,j))(i,j).

(5)

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

7

(a) DSConv

(b) MBConv

(c) MBConv+SE

(d) GConv+channel shufﬂe

Fig. 2: Schematics of efﬁcient operators. (a) DSConv: depthwise separable convolution; (b) MBConv: mobile inverted bottleneck
convolution; (c) MBConv+SE: MBConv with the squeeze and excitation module; (d) GConv+channel shufﬂe: group convolution
with channel shufﬂe.

LEMONADE [64] adopts both DSConvs and standard con-
volutions as the basic operators of its search space and
supports increasing the number of ﬁlters and pruning ﬁlters to
search efﬁcient neural architectures. RENA [65] also includes
DSConvs but allows to automatically decide not only their
hyperparameters but also whether they should be used in
each layer. In addition to the standard DSConv, ProxylessNAS
[23] includes a variant, namely dilated depthwise separable
convolution, in its search space as well. The search engine
can choose between the standard one and the variant.

In MobileNetV2, a more advanced mobile convolution
block, mobile inverted bottleneck convolutions (MBConv), is
proposed [10] and soon becomes a popular operator in favour
of an efﬁcient search space. Fig. 2b illustrates the structure of
MBConv. It is made up of three convolutions and one residual
connection. First, a PWConv is applied to expand the input
feature map to a higher-dimensional space so that non-linear
activations (ReLU6) can better extract information. Then, a
depthwise convolution is performed with 3 × 3 kernels and
ReLU6 activations to achieve spatial ﬁltering of the higher-
dimensional feature maps. Furthermore, the spatially-ﬁltered
feature maps are projected back to a low-dimensional space
with another pointwise convolution. Since the low-dimensional
projection results in loss of information, linear activation is
used after pointwise convolution. Finally, an optional residual
connection (depending on whether the stride of the depthwise
layer is 1) is added to combine the original input and the output
of the low-dimensional projection. Note that the last two con-
volutions (depthwise convolution and pointwise convolution)
are essentially a DSConv with dimension reduction. There
are multiple works on purely using MBConv to construct an
efﬁcient search space but with different backbones or search
strategies [66], [67], [68]. Xiong et al. [69] argue that DSConv
is inexpensive based on FLOPs or the number of parameters,
which are not necessarily correlated with the inference ef-
ﬁciency, so they propose a fused MBConv layer that fuses
together the ﬁrst pointwise convolution and the subsequent
depthwise convolution into a single standard convolution. They
achieve higher mAP and lower latency on EdgeTPU and DSP
than the pure MBConv search space. Li et al. [70] provide an

in-depth comparison between fused MBConv and MBConv
and summarize that fused MBConv has higher operational
intensity but higher FLOPs than MBConv depending on the
shape and size of ﬁlters and activations. Therefore, they add
both fused MBConv and MBConv into the search space to let
the search strategy determine automatically.

In a more popular manner, MnasNet [48] upgrades MBConv
with a squeeze and excitation (SE) module[71] after the
DWConv for attentions on feature maps (as shown in Fig.
2c). Speciﬁcally, the S (squeeze) procedure ﬁrst converts each
individual feature map into a scalar descriptor using global
average pooling so the input feature maps are converted into
a vector z ∈ Rn with its k-th element calculated by:

zk = GlobalArgPool(xk) =

1
H × W

H
(cid:88)

W
(cid:88)

i=1

j=1

x(i,j,k),

(6)

where x(i,j) is the (i, j)-th element of the k-th input feature
map, which is of size H × W . The E (excitation) procedure
converts the S procedure’s output z into a vector of activations
s using the gating mechanism of two fully connected layers:

s = σ(W2 · ReLU(W1 · z)),

(7)

where σ(·) is the sigmoid activation function. The ﬁnal output
of the SE module is ˜X with its k-th feature map denoted as:

˜xk = SE(xk) = sk · xk.

(8)

MobileNetV3 [72], the latest version of the MobileNet series,
also adopts the MBConv plus SE operator and further enhances
it by using the hard-swish (HS) [73] nonlinearities instead of
ReLU and replacing the sigmoid function in SE with hard
sigmoid [74]. The authors also point out that HS in deeper
layers is more beneﬁcial. This choice is based on the fact that
the sigmoid function is expensive to deploy on mobile devices.
Thereafter, many more works embrace the MBConv plus SE
operator in their efﬁcient search space [75], [76], [77], [78],
[79]. However, SE is not always paired with MBConv due to
either not being supported by hardware [31] or unfavourable
performance [80]. Some other works maintain diverse convo-
lution operators (i.e., standard convolution, DSConv, MBConv,

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

8

and MBConv+SE) in the search space and let
the search
strategy choose automatically [81], [48]. The commonly used
searchable parameters of the convolution operator family are
kernel sizes, the number of output channels, expansion ratios
(MBConv, MBConv+SE), and SE ratio (MBConv+SE). Al-
though different works do not have exactly the same searching
ranges, they are basically similar. For example, Stamoulis et
al. [66] consider kernel sizes of {3, 5} and expansion ratios
of {3, 6}; Fang et al. [67] consider kernel sizes of {3, 5, 7}
and expansion ratios of {3, 6}; Cai et al. [77] consider kernel
sizes of {3, 5, 7} and expansion ratios of {3, 4, 6}.

Besides the above convolution operators of the MobileNet
family, the group convolution (GConv) and its variants [82],
[83], [11] are also considered to be signiﬁcant operators for
constructing an efﬁcient search space [84], [85], [86], [87],
[88]. Fig. 2d illustrates the structure of GConv, where we
take into account the feature map dimension of convolutional
layers. The feature maps and ﬁlters are divided into G (G = 3
in Fig. 2d) groups respectively: X = {X1, X2, ..., XG} and
W = {W1 W2 ..., WG}. In GConv, the convolution is only
performed within each group so the output ˜X is denoted as:
˜X = {W1 ⊗ X1, W2 ⊗ X2, ..., WG ⊗ XG},

(9)

where ⊗ is the convolution operation between two sets. In this
way, GConv can not only reduce parameters and computation
but also provide a simple way to model parallelism. Therefore,
AlexNet [83] can be trained on multiple GPUs with only 3GB
RAM each. Note that depthwise convolution is a special case
of GConv with the number of groups being the same as the
number of channels. A channel shufﬂe usually comes after
GConv to enable inter-group communications [11]. Xu et al.
[84] include GConv in their search space with the number
of groups as searchable from 1 (standard convolution) to
N (the number of input channels). FBNet [86] and RCAS
[87] encompasses a new convolution operator in their search
space by replacing the ﬁrst and last pointwise convolution in
MBConv with 1 × 1 GConv. This design expands MBConv
but also allows the search strategy to automatically determine
whether this expansion is needed. However,
their allowed
maximum group amount is quite small (2 for FBNet and 4 for
RCAS). DPP-Net [85] and MONAS [88] also contain a variant
of GConv, Learned Group Convolution (LGConv), which is the
key operator of CondenseNet [89], in their search space. The
LGConv prunes away unimportant ﬁlters with low magnitude
weights further reducing the computational complexity on top
of the GConv [82].

efﬁcient

in their

Some other studies use customized operators or non-
convolution operators
search spaces.
FasterSeg [90] proposes a zoomed convolution, where the input
is sequentially processed with bilinear downsampling, standard
convolution, and bilinear upsampling. The authors demonstrate
that the zoomed convolution has 40% latency trim compared
to a standard convolution on a GTX 1080i GPU. Different
from previous works, which focus on 2D processing, Tang
et al. [91] target 3D scenes. They propose sparse point-voxel
convolutions and allow to search for channel numbers and
network depth. However, as the computation of 3D CNN
increases more signiﬁcantly with increasing kernel sizes than

2D CNN,
the authors keep the kernel size as a constant
of 3. HR-NAS [92] also involves Transformer [93], [94] in
addition to convolutions due to its recent success in computer
vision [95], [96]. The authors design a lightweight Transformer
that requires less computation when facing high-resolution
images. Convolutional channels and Transformer queries are
progressively reduced during the search. In order to facilitate
high parallelism of convolution operators on TPUs and GPUs,
Li et al. [70] add space-to-depth/batch into the search space to
increase the depth and batch dimensions. The results show that
even without the lowest FLOPs, their searched EfﬁcientNet-
X models are the fastest among compared model families
on TPUs and GPUs. The main reason is that EfﬁcientNet-
X models strike a balance between FLOPs (lower is good for
speed) and operational intensity (higher is good for speed).

In addition to parametric operators, non-parametric opera-
tors are critical components of an efﬁcient search space as
well. Pooling is usually coupled with convolutions in hand-
crafted CNN model family, and so does in convolution-based
search space [87], [80]. It can help to reduce redundant infor-
mation (favourable for accuracy) and computation (favourable
for speed) without requiring additional parameters. Skip is
another widely adopted operator that impacts the topology of
achieved models. It can have two concepts: 1) drop skip di-
rectly feeding input to output without any actual computations,
i.e., the entire layer is dropped and thus the model depth is
reduced [66], [86], [67]; 2) residual skip providing an identity
residual connection in parallel with another computation op-
erator [64], [80]. The residual connection can be achieved by
either concatenation or by addition [64]. Activation functions
are another searchable non-parametric operators that impact
both accuracy and speed [87], [70]. The computation cost of
activation functions decreases when going deeper since the
resolution of feature maps decreases [72]. In some research,
the non-linearity is associated with convolution operators as a
whole, and the operator together with its activation function
is determined automatically [79].

2) Backbones: After deﬁning the operators that a model
can have, it is essential to determine how many operators
there are and how these operators are connected, i.e., the
backbone of a model. However, operators and backbones are
not completely isolated, such as the skip operator, which
removes layers or adds connections and can change a model’s
backbone. Depending on the connection topology, backbones
can be roughly classiﬁed into two categories: chain-structured
and multi-branch.

Chain-structured Backbones. The chain-structured back-
bone [57] is the earliest and simplest topological structure
of a neural network. It directly stacks multiple operators in
sequence. As shown in Fig. 3 (a), the ith operator (Oi) takes
the output of the i-1th operator (Oi−1) as input and its output
serves as the input of the operator (Oi+1). Therefore, the
topological connection of different operators is determined
and not searchable. However, it is possible to search how
many operators there are, e.g., via the drop skip. This simple
backbone is a favourable practice of many hand-crafted CNN
models [83], [11], [9], [10].

An intuitive implementation is to stack operators layer

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

9

Fig. 3: Schematics of different backbones. Each block represents an operator Oi in (a) and (b), and represents a cell Ci
in (c), and different colours indicate different operator/cell types. The arrows indicate the information ﬂow direction. In the
macro-micro backbone, the macro-structure describes the topology of multiple cells, each of which has a micro-structure of
operators.

by layer [87], [77]. Motivated by the design principle of
existing success [10], [11], the chain-structured backbone is
more commonly implemented in a cell-based, a.k.a. block- or
stage-based, manner [68], [81], [79], [91], [86], [67], where a
backbone is composed of multiple chain-structured cells, each
of which contains multiple chain-structured operators. The
operators in the same cell can be either identical or diverse in
hyperparameters but the same in type. In the sequence of cells,
it is a usual principle that the input resolutions are reduced
and widths are increased gradually. To achieve the reduced
resolutions, the ﬁrst or last operator in a cell usually has man-
ually set strides and widths. This cell-based implementation
is not only effective but also reduces the search space. Wu et
al. [86] design a chain-structured backbone consisting of four
searchable cells. There are 8 candidate convolution operators
with various expansion rates, kernel sizes and numbers of
groups, and a drop skip operator for searching. Different cells
are separated by their input resolutions and widths, which are
determined by manually set parameters. Yan et al. [81] also
follow the same practice to construct their backbone but with
more ﬂexibility in searchable parameters. In these studies, it is
usually required to predeﬁne some parameters of the backbone,
like the number of cells.

In comparison with setting backbone structure empirically,
some work directly embraces the backbone of existing models
as a starting point and then searches beyond that [76], [69],
[88], [23], [66], [69]. This design principle relieves experts’
burden of search space design by reusing prior knowledge,
which is important in NAS [97]. MONAS [88] relies on the
backbone of a simpliﬁed version of AlexNet [83] to search the
convolution ﬁlter sizes and amounts. Scheidegger et al. [62]
investigate the backbone of MobileNetV2 [10] and allows to
search the number of operators in each cell but all operators

in a cell have the same settings except for the stride, which is
used to modify the output resolution. The complexity reduc-
tion is mainly obtained by lowering the channel widths and
reducing the number of topological replications. In addition
to following the backbone of hand-crafted models, there is
a trend to using the backbone of existing searched efﬁcient
models. MOGA [76] adopts MobileNetV3-large [72] as its
backbone and keeps the same number and type of operators.
It only searches the parameters of operators, like kernel sizes,
expansion ratios for MBConv, and whether SE is enabled
or not. Cai et al. [77] also adopts MobileNetV3, but they
additionally provide the ﬂexibility of searching the number
of operators in each cell.

Mutli-branch Backbones. While the chain-structured back-
bone is simple, it restricts the information ﬂow to be sequential
and single-path. Current hand-crafted architectures have sug-
gested that a multi-branch architecture, which allows multi-
path information ﬂow and residual skip connections, works
impressively better than a chain-structured architecture [98],
[47]. As illustrated in Fig. 3 (b), in the multi-branch backbone,
an operator Oi is allowed to accept the outputs from some of
its previous operators (i.e., O1, O2 ... Oi−1) as the input but
not necessarily takes all. This setting provides more degrees
of freedom on network topology. Note that a chain-structured
backbone with residual skip connections, where an operator
Oi must receive the output of its immediate previous operator
Oi−1, is a special case of the multi-branch backbone.

The multi-branch backbone can be achieved by using an
insertion operation in the search space [65]. However, search-
ing for a multi-branch backbone as a whole is time-consuming
and difﬁcult to ﬁnd an optimal structure. Similar to the chain-
structured backbone, the cell-based principle is also widely
adopted in designing a multi-branch backbone [80], [61], [68],

O1O2On-1OnOUTPUTINPUTO1O3O4O6O2O5O7O8INPUTOUTPUT(a)Chain-structuredBackbone(b)Multi-branchBackbone(c)Macro-microBackboneC1C2Cn-1CnOUTPUTINPUTmicrostructuremacrostructureIEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

10

where the topology within a cell has a multi-branch structure.
This results in a Macro-micro Backbone (Fig. 3 (c)) [48],
[61], [92], [84]. In the macro-structure, residual skips are
optional to provide the ﬂexibility of multi-path information
ﬂow; in the micro-structure, operators are connected in the
multi-branch fashion with the last operator (either parametric
or non-parametric) assembling a single output of the cell. For
example, MnasNet [48] predeﬁnes a backbone of 7 sequen-
tially stacked cells and each cell contains sequentially stacked
operators with a searchable amount, types, and parameters.
It further allows searchable residual skip connections among
operators within a cell. This multi-branch backbone simply
augments the chain-structured backbone with residual skips,
resulting in more ﬂexibility of multi-path information ﬂow
but marginal research space expansion. NSGA-Net [61] and
HR-NAS [92], in contrast, allow more general multi-branch
connections within a cell. NSGA-Net supports searchable
topology while HR-NAS sets ﬁxed multi-branch connections
and searches for discarded convolutional channels and trans-
former queries.

Different from searching for a customized structure, fol-
lowing existing success is also a favoured choice in designing
multi-branch backbones [70], [31], [80], [72]. Li et al. [70]
the multi-branch backbone of the EfﬁcientNet and
adopt
only search operators. MnasFPN [80] constructs its search
space based on the NAS-FPN(Lite) backbone and searches
both multi-branch structures and operators in a cell
for
merging various resolutions. However, to reduce the search
burden, it does not allow general connectivity patterns and
applies limited merging connections. MobileNetV3 [9] uses
the backbone of MnasNet [48] as the seed and proceeds
layer-wise search on it. Scheidegger et al. [62] investigate
the backbone of several existing models (DenseNet121 [60],
MobileNetV2 [10], GoogLeNet [99], PNASNet [100], and
ResNeXt [82]) and demonstrate that their approach is able
to provide improved accuracy with hardware constraints and
different backbones. While considering that searching both
macro and micro structures overburdens the searching process,
most works utilize the multi-branch macro backbone of exist-
ing models and only search the micro-structures [23], [85],
[88]. ProxylessNAS [23] accepts the backbone of the residual
PyramidNet [101], which has a residual skip connection every
two operators, and replaces the original operators with their
own tree-structured cells [102]. DPP-Net [85] selects the
backbone of CondenseNet [89], which repeats an identical cell
abundant times with both residual skip and chain connections,
and only searches the operators in the cell. MONAS [88] also
reuses the backbone of CondenseNet but it uses the same cell
structure and searches the number of stages and growth rate.
Although the macro-micro backbone provides the highest
ﬂexibility and complexity among the above three categories,
there is no evidence that
is the best choice. However,
we note that no matter in which backbone category, cell-
based implementation is the most common practice. This is
due to the following considerations: i) an effective network
usually has gradually shrinking resolutions as going deep, and
a cell can have multiple operators on the same resolution
to strengthen feature extraction; ii) previous experience with

it

the manually designed network indicates that the cell-based
structure is effective for deep learning;
iii) the cell-based
backbone provides high search efﬁciency by limiting the
search space (e.g., the whole network can stack the same
cell and a cell can have the same operator); iv) the cell-based
backbone simpliﬁes the network topology with inter- and intra-
cell connectivity instead of random edges among all operators.
Some recent works reveal that search performance may be
impeded by these search space design biases [103]. Neverthe-
less, the choice of the search space principally regulates the
difﬁculty of the search process. Current NAS algorithms are
imperfect so more freedoms in the search space do not always
lead to better resultant models but inversely overburden the
search algorithms. With the continued improvement of NAS
algorithms, it is essential to reduce design biases and construct
a more general search space.

B. Search Strategy

The search strategy describes how to explore a search space
to ﬁnd an optimal efﬁcient network. Essentially, the search
process is to ﬁnd the top candidate networks regarding some
evaluation metrics (e.g., accuracy and latency). However, it
is computationally prohibitive to achieve these metrics of all
candidate networks since it requires fully training a network
to obtain its metrics. Therefore, the aim of a search strategy is
to efﬁciently ﬁnd top-ranking networks without exhaustively
examining all candidate networks. In this section, we ﬁrst
conclude the search algorithms that describe how the search
proceeds and then summarize how the hardware constraint is
incorporated into the search process.

1) Search Algorithms: The most intuitive and easiest search
algorithm is the grid search, which exhaustively explores the
search space and evaluates every possible architecture. This
approach works well for a small space [68] but is terribly
inefﬁcient for a large space due to the exponentially increased
number of evaluations. Another problem with this type of
method is that it only supports a bounded and discrete space
and needs careful selection of the grid interval. Random
search [104], on the other hand, samples neural architectures
randomly from the search space. It can be used not only
for a discrete space but also for a continuous space with a
predeﬁned distribution. This algorithm is superior to the grid
search also when the search dimensions (e.g., kernel sizes,
expansion ratios, and network depths) have different effects on
the ﬁnal performance. Random search and its simple variants
are usually used when the performance of a candidate model
is easy to obtain [84], [75]. Advanced search algorithms, like
reinforcement learning and evolutionary search, are widely
demonstrated better performance than random search [49],
[105], [106]. In the following, we summarize the advanced
search algorithms, including Bayesian optimization, evolution-
ary search, reinforcement learning, and differentiable methods,
regarding design automation for efﬁcient deep learning mod-
els.

Bayesian Optimization (BayesOpt) is an important ap-
proach for automated hyperparameter tuning and architecture
search. Given a search space S that contains a large set of

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

11

(a) Bayesian optimization

(b) Evolutionary search

(c) Reinforcement learning

(d) Differentiable search

Fig. 4: Schematics of different search algorithms. (a) Bayesian optimization; (b) Evolutionary search; (c) Reinforcement
learning; (d) Differentiable search.

neural architectures, and a black-box objective function f (·)
from an neural architecture to an evaluation metric (e.g.,
accuracy), the goal of BayesOpt is to ﬁnd an architecture a ∈ S
that maximizes f (a): a∗ = argmaxa∈S f (a). However, it is
expensive to achieve f (a) because it requires fully training
the architecture a from scratch. Therefore, a statistical model,
which is invariably a Gaussian process (GP), is used as a
surrogate model. The more neural architectures (i.e., points
of f (·)) are evaluated,
the less uncertainty the surrogate
model has. Another important component of BayesOpt
is
the acquisition function for deciding which architectures are
sampled and then evaluated at each iteration, which is often
expected improvement (EI):

EI(a) = Emax(f (a) − f (a+), 0),
a+ = argmax
ai∈a1:t

f (ai),

(10)

where a is a sampling architecture and f (a+) is the evaluation
metric of the best architecture a+ so far, that is until t-th
exploration. The general process of BayesOpt is shown in Fig.
4a, where multiple architectures are ﬁrst randomly sampled
from the efﬁcient search space; the sampled architectures are
then trained and evaluated for updating a GP model;
the
acquisition function is computed based on the updated GP
model to decide which neural architectures should be sampled
next; the process proceeds iteratively until certain conditions
are met. The best architecture among all sampled architectures
is usually output as the ﬁnal model.

When additionally considering the hardware constraints,
Multi-Objective Bayesian Optimization (MOBO) is commonly
used [107], [108], [109], [110], where GP is ﬁtted for each
objective independently, and Pareto-frontier is identiﬁed as the
set of optimal trade-off solutions of the multiple objectives.
Different MOBO approaches mainly differ in how to achieve
the Pareto-frontier during the acquisition process. For example,
Parsa et al. [109] use a Gaussian distribution to estimate the
Pareto-frontier function; Eriksson et al. [110] use the Noisy
Expected Hypervolume Improvement (NEHVI) acquisition
function, which is a noise-tolerant and extended version of
EI for the multi-objective setting,
to sample intermediate
Pareto-frontiers. Although BayesOpt has shown promising
performance, especially in hyperparameter optimization, it is
computationally expensive and struggling when handling a

high-dimensional search space and multiple objectives [111],
[112].

Evolutionary Search (ES) is a long-thriving NAS ap-
proach, which is based on the concept of biological evolution.
It manages to ﬁnd an optimal solution by iteratively improving
upon a population of candidate solutions according to a ﬁtness
function. As illustrated in Fig. 4b (the schematic of the basic
ES process), a population of candidate networks is ﬁrst initial-
ized by randomly sampled from the efﬁcient search space; then
evolutionary operations, such as crossover (combination of two
parents) and mutation (arbitrarily mutating operators with new
ones from the search space), are applied to current population
to generate next generation; lastly, all offsprings need to be
tested against a ﬁtness function, which is to select the most
powerful candidate networks and update the population; the
process proceeds repeatedly until stopping criteria are met. The
best network in the last population is the ﬁnal achieved model.
The major beneﬁt of evolutionary search is its ﬂexibility
in directly controlling the offspring generation process and
population updating process [105].

The hardware constraints are usually considered in the
ﬁtness function of ES in two ways: hard-constraint and soft-
constraint, which we will present concretely in the ﬂowing sec-
tion III-B2. Thus, most studies directly use existing evolution-
ary algorithms with customised ﬁtness function to incorporate
hardware constraints [78], [91], [81], [113], [77], [114], [105],
[115]. For example, FBNetV3 [78] uses the adaptive genetic
algorithms [116] to realize adaptive probabilities of crossover
and mutation, and customizes the ﬁtness function with its
proposed accuracy predictor and hard hardware constraints;
OFA [77] adopts the regularized evolutionary search [117],
which introduces an age property to favor younger generation
during search; DONNA [113], FairNAS [118], and MOGA
[76] appeal to the famous NSGA-II [119], which is a multi-
objective evolutionary algorithm, to ﬁnd the Pareto-optimal
solution instead of the solution under hard constraints in
previous studies. In contrast to directly using off-the-shelf
ES algorithms, some researchers develop ES algorithms spe-
ciﬁc to the hardware efﬁcient application [64], [61], [62].
LEMONADE [64] is such a representative work, which han-
dles various objectives differently considering that different
the
objectives have different evaluation costs. Speciﬁcally,

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

12

efﬁciency objective (e.g., FLOPs) is cheap to evaluate while
the accuracy objective is much more expensive as it requires
fully training the model at each iteration. The authors propose
to ﬁrst select the architectures that fulﬁl the Pareto front for
the cheap objectives and then only train and evaluate these
networks. As traditional ES can only sample a limited amount
of networks that satisfy hardware constraints, Scheidegger et
al. [62] use ES algorithms to search sampling laws that can
better cover the sub search space under speciﬁc constraints.
Thus, better Pareto frontiers can be achieved. NSAG-Net [61]
designs an additional exploitation step after the traditional
evolutionary operating to learn and leverage the history of
evaluated populations. Since the evaluated populations at each
iteration rank relatively high regarding the ﬁtness score, it is
quite possible that the optimal model has similarities to the
evaluated populations and thus extracting common patterns
from the evaluated populations can accelerate convergence.
Although the above studies manage to lighten the search
process from the algorithm perspective, it is still costly due to
the requirement of training each network of the new generation
at each interaction.

Reinforcement Learning (RL) has achieved notable suc-
cess and grabbed great attention in the NAS community since
Zoph and Le’s work in 2017 [52]. The general framework of
RL for NAS is illustrated in Fig. 4c: the controller, usually an
RNN network, is the core component, which generates actions
to sample operators or hyperparameters from the efﬁcient
search space; a neural network will then be constructed using
the sampled options and trained and evaluated to achieve a
reward, which will be used to update the controller using a
policy gradient method [120]. This process will be performed
repeatedly until stopping criteria are met.

The hardware information is often considered in the RL
reward function. Some studies thus directly use the existing
RL-based NAS approach [52], but optimize with a customized
reward function [88], [65]. Furthermore, most recent work
mainly follows the advanced RL search algorithms in two rep-
resentative studies, MnasNet [48] and TuNAS [106]. Speciﬁ-
cally, [80], [72] follow MnasNet, and [69], [31] follow TuNAS.
Similar to [52], MnasNet [48] also uses RNN as the controller
while maximizing the expected reward using an advanced
policy gradient method, Proximal Policy Optimization (PPO)
[121], to alleviate the high gradient variance of the vanilla
policy gradient method. TuNAS is based on ENAS [122] and
ProxylessNAS [23]. It improves with warmup and channel
masking techniques for better search robustness and scalability.
Instead of building one candidate network at each training step,
TuNAS encompasses all candidate networks into a supernet.
Each path of the supernet represents a candidate network. In
addition, TuNAS is not a sequential decision-making process
and does not rely on an RNN controller; alternatively, it uses
learnable probability distribution that spans over architectural
choices as the RL controller. At each step, a candidate network
is sampled from the distribution, and then the portion of the
supernet correlated with the sampled network is trained; a
reward is calculated with the sampled network to update the
probability distribution i.e., the RL controller.

Differentiable Search is the most recently developed search

paradigm that ﬁnds an optimal network by gradient descent
[54]. As shown in Figure 4d, a network is as a directed
acyclic graph consisting of an ordered sequence of N nodes
(N = 4 in Figure 4d: X0, X1, X2, X3). Each node Xi is
a latent representation and each directed edge represents a
candidate operator Oi that is applied to Xi. For example, in
Figure 4d, there are three candidate operators (i.e., O1, O2, and
O3) between node X0 and X3. In this way, the search process
is formulated as an optimal path-ﬁnding problem. All outputs
of the candidate operators from a node’s predecessors are
summed with weights αi to achieve the node representation:
Xj = (cid:80)m
i=1 αiOi(Xi), subject to αi ≥ 0, (cid:80)m
i=1 αi = 1. In
Figure 4d, edges with darker colours indicate larger weights.
The weights αi can be represented with a softmax function of
exp(βi)
architectural parameters βi: αi =
i=1 exp(βi) . The network
weights (w) and architectural parameters (βi) are trained al-
ternatively with training data and validation data, respectively.
This induces a bi-level optimization problem [54]:

(cid:80)m

min
α

min
wα

L(α, wα).

(11)

Therefore, unlike ES and RL, a differentiable search uniﬁes
model training and search into a joint procedure. As the whole
network contains multiple parallel operators in each layer, it is
called a supernet, and its subnet with one operator in each layer
is a candidate network. The candidate operator with the highest
associated weight is chosen to construct the ﬁnal model. The
training of the supernet is also the search process.

The hardware information can be considered in the loss
function and optimized when training the architectural param-
eters. Various differentiable search algorithms are proposed to
achieve better search efﬁciency and performance [79], [86],
[23], [66], [92], [67]. FBNetV1 [86] replaces the softmax
function with the Gumbel softmax function [123] to better
represents the subnet sampling process, and thus reduce the
performance gap between the supernet and subnet. HR-NAS
[92] progressively discards the paths with low path weights
during the search to increase the search efﬁciency. DenseNAS
[67] splits the search procedure into two stages: the ﬁrst stage
optimizes the model weights only for enough epochs and the
second stage alternatively optimizes the model weights and
architectural parameters. This strategy alleviates the bias of
fast convergence operators. Considering the small size of the
search space of the conventional differentiable search due to
its requirement of loading the whole supernet into memory,
many studies propose novel differentiable search algorithms
to reduce the memory footprint [23], [66], [79], [67]. Proxy-
lessNAS [23] proposes to factorize the task of training of all
paths into training two sampled paths, which have the highest
sampling probabilities, and discard the other paths temporarily
at each iteration. The architectural parameters of the two
selected paths are updated during training and then rescaled
after training to keep the path weights of unsampled paths
unchanged. In this way, the memory requirement is decreased
to the level of training two subnets. Different from previous
papers, which use multi-path supernets and thereby have mem-
ory issues, Single-Path NAS [66] and FBNetV2 [79] develop
masking mechanisms that train a supernet with the largest

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

13

hyperparameters (e.g., 7 × 7 kernel) and a mask/indicator
function that determines whether to just use a small part (e.g.,
5 × 5 or 3 × 3 kernel) of the largest hyperparameters.

Other types of search algorithms are also reported [87],
[85]. Xiong et al. [87] propose a modiﬁed Cost-Effective
Greedy algorithm for the submodular NAS process, where
starting from an empty network, each block is ﬁlled iteratively
with the highest marginal gain ratio regarding both accuracy
and cost. DPP-Net [85] follows the progressive search algo-
rithm in [100] to ﬁnd the optimal operator layer by layer.
Different from searching for a new model for each new
resource budget, another way is to build a baseline model ﬁrst
and then scale it to obtain a family of models for various
budgets [68], [70]. Speciﬁcally, they adopt the conventional
RL-based hardware-aware search algorithm [48] to search for
a small baseline model, and use a simple grid search to
determine the best scaling coefﬁcient (i.e., α, β, γ) for network
depth, width, and input size. A family of specialized models
for different budgets can be obtained by scaling up the baseline
model by αN , βN , and γN with 2N times increased resource.
Incorporation Strategy: Despite
some studies only using an efﬁcient search space without
considering hardware constraint [49], there are two strategies
commonly used to consider hardware budget for ﬁnding opti-
mal compact models. The ﬁrst considers a speciﬁc hardware
platform and treats its resources as hard constraint to build
a specialized model that is the most accurate under the ﬁxed
constraint. In contrast, the second strategy is to directly search
networks without considering speciﬁc hardware constraint.
This strategy, dubbed soft-constraint incorporation, treats the
model efﬁciency as an additional optimization objective and
tries to ﬁnd the Pareto frontiers. In this section, we summarize
the design automation techniques from the above two perspec-
tives.

2) Hardware-constraint

Hard-constraint Incorporation can be easily adopted by
evolutionary search [81], [91], [77], [78], [62] since the off-
spring generation and selection step can be directly controlled.
The incorporation process is quite straightforward: when a set
of candidate networks is generated (e.g., by evolutionary oper-
ating), their hardware costs are then tested, and the networks
that do not meet the constraint are simply discarded. This
strategy is also easy to implement for other search algorithms,
which generate a set of candidate networks one time or at each
iteration, such as random search [84], [75], grid search [68],
and greedy search [87]. The RL-based search algorithms can
also embrace the hard-constraint incorporation strategy but in
a different way [65], [48], [88]. The general idea is to have
only the accuracy in the reward function when the constraint is
met; otherwise, both accuracy and the hardware objective are
considered in the reward function. A typical reward formula
R(·) is proposed in MnasNet [48]:

(cid:40)

R(a) =

ACC(a),
ACC(a) × (C(a)/C0)β,

if C(a) ≤ C0
if C(a) > C0

(12)

hyperparameter that controls the convergence speed. In [106],
the authors empirically ﬁnd that if β is too large, the RL-
controller will prefer to sample the architectures whose cost
is signiﬁcantly smaller than the target cost constraint. This will
result in suboptimal models regarding accuracy.

Soft-constraint Incorporation does not target any speciﬁc
constraint and is realized by adding an additional optimization
objective to the accuracy objective. Since these objectives are
competing, no unique optimal solution exists in the multi-
objective space. Thus, Pareto frontiers are sought, especially
by multi-objective BayesOpt and multi-objective evolutionary
algorithms, and one speciﬁc model can be identiﬁed according
to different application requirements [61], [109], [113], [76],
[107], [64], [108], [115], [118]. For example, ChamNet [115]
designs a multi-objective ﬁtness function:

F (a) = ACC(a) − [αH(C(a) − C0)]β,

(13)

where C(a) and C0 denote the hardware cost of model
a and the target constraint, respectively, H is the Heav-
iside step function, and α and β are positive constants.
The searching objective is to maximize the ﬁtness function:
a∗ = argmaxa(F (a)). This ﬁtness function guides the search
process to ﬁnd a model approaching the hardware target
but without a hard guarantee. Some work also pursues the
soft-constraint objective under a hard constraint with a two-
phase approach, which ﬁrst ﬁlters out models that do not
satisfy the hard constraint and then performs multi-objective
optimization [85], [62]. Instead of ﬁnding a set of solutions,
some studies attempt to ﬁnd the best tradeoff between efﬁ-
ciency and effectiveness. Two schemes are commonly used
in this regard and mainly for the RL-based and differentiable
algorithms:multiplication and linear combination. For the
multiplication scheme, the effectiveness objective (e.g., accu-
racy) and efﬁciency objective (e.g., latency) are multiplied to
construct the loss function [86] or reward function [72], [48],
[80], [70]:

loss function: L(a) = LCE(w|a) × log(C(a))β,

reward function: R(a) = ACC(a) × (C(a)/C0)β.

(14)

(15)

The LCE(w|a) is the cross-entropy loss of model a with
parameter w. The exponent coefﬁcient (β > 0 for the loss
function; β < 0 for the reward function) modulates the trade-
off between effectiveness and efﬁciency. Note that although
there is a target constraint term C0 in the reward function,
this is still a soft constraint since this just pushes the cost
to be lower than the target but without any guarantee. Al-
ternatively, it is possible to keep running the search process
until the hardware constraint is satisﬁed [72]. For the linear
combination scheme, the two competing objectives are linearly
combined to construct the loss function [66], [67], [79], [92],
[23] or reward function [69], [106], [31]:

loss function: L(a) = LCE(w|a) + β log(C(a)),

(16)

where ACC(a) and C(a) denote the accuracy and hardware
cost (e.g., latency in [48]) of model a, respectively, and C0
denotes the target cost constraint. β < 0 is the only tunable

reward function: R(a) = ACC(a) + β|C(a)/C0 − 1|. (17)

Similar to the multiplication scheme, the β coefﬁcient balances
the effectiveness and efﬁciency. The log(·) function in the

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

14

hardware-related term in the loss function is used to scale the
cost and are omitted in some studies [79], [23], [92]. TuNAS
[106] empirically demonstrates that search results are robust
to the exact value of β. It shows that the same β value works
great for different search spaces and hardware constraints. This
β value-invariance alleviates the tedious tuning of β for new
scenarios (e.g., new devices).

For non-differentiable search algorithms, the hardware con-
straint can be smoothly incorporated; by contrast, differen-
tiable search algorithms require the cost term to be differ-
entiable, which is intrinsically not. Therefore, speciﬁc cost
estimation strategies are expected to resolve this contradiction,
which we will introduce in Section III-C2.

C. Performance Estimation Strategy

The search process is managed by search strategies, which
we have discussed in Section III-B, and guided by the per-
formance of candidate models, which we will discuss in this
section. The performance in the context of hardware-efﬁcient
models has two aspects: task-related performance (e.g., accu-
racy) and hardware-related performance (e.g., latency). The
most natural way to obtain a model’s performance is to fully
train the model and evaluate it on validation data for task-
related performance and deploy it on the target hardware for
hardware-related performance (for direct metrics, e.g., latency)
[107], [61]. However, it demands massive computation and
time to fully train each candidate model from scratch and
deploy them on target hardware. Therefore, efforts are made
toward alleviating the performance estimation process. We
categorize and discuss these efforts into two routes: one is
to speed up the training process (section III-C1), and the
other is to directly predict the performance without training
(section III-C2). Since the hardware cost does not require
training, studies of estimating hardware-related performance
are reviewed in section III-C2 as well.

1) Training Strategy: The hardware-aware NAS does not
distinguish from conventional NAS regarding the training
strategies of candidate models so the training strategy sum-
marized in this section can also be applied to conventional
NAS and vice versa. The simplest training strategy is to train
each candidate model from scratch and use the sample-eval-
update loop [52] to achieve an effective search agent. This
strategy is not only widely adopted by RL-based search [48],
[80], [72], [65], [49], but also by BayesOpt [107], [108], [109],
evolutionary search [61], [64], [62], and other algorithms [70],
[68]. To alleviate the drawback of the straightforward training
approach, some studies [80], [48], [87] follow Zoph et al. [49]
to ﬁrst train and search on proxy tasks, such as smaller datasets
or fewer epochs, then transfer to the large-scale target task.
The proxy task is a reduced version of the target task. Xiong
et al. [87] also propose lazy evaluation to further allow fewer
evaluation. However, this is speciﬁc to its search strategy. An
inevitable shortcoming of using a proxy task is that the model
optimized on proxy tasks is not guaranteed to be favourable
on the target task.

Another way of speeding up the training process is pa-
rameter sharing [124]. The search space is represented as

a single super Directed Acyclic Graph (DAG), dubbed a
supernet, whose nodes are computation operators and edges
illustrate the ﬂow of information. Each candidate model is
a subgraph/subnet of the supernet and candidate models can
share their parameters if they share common operators. When
the supernet is well trained (e.g., trained for enough epochs),
a candidate model can directly inherit parameters from the
supernet without any separate training. A more widely used
supernet is to have each node represent a latent representation
(e.g., feature maps of a CONV operator) and each directed
edge represents some operation (e.g., a CONV operator).
There are multiple edges (i.e., different operators) between
every pair of nodes, and the search process is to determine
which edge/edges should be retained between each pair of
nodes. The common retained edges share parameters between
different candidate models. Different studies differ in training
the supernet. Some work [88], [106], [114] ﬁrst pre-trains the
whole supernet, and then only trains a part (e.g. a subnet) of
the supernet, which is selected by search algorithms. Speciﬁ-
cally, TuNAS [106] disables the RL controller at the ﬁrst 25%
of the search and only trains the parameters of the supernet.
It randomly selects CNN ﬁlters and candidate operators to
train with some probability p, which is linearly decreased
from 1 to 0 over the ﬁrst 25% search. During the parameter
sharing, TuNAS further shares the “submodule” parameters.
For example, MBConv operators with different DWSConv
kernel sizes are in different paths/edges, but they can share the
PWConv weights in MBConv regardless of which DWConv
kernel is selected. In addition, TuNAS also applies channel
masking for parameter sharing; it creates a convolution with
the largest possible number of channels while simulating
smaller channel numbers by choosing the ﬁrst N channels
while zeroing out the remaining ones. These techniques are
well recognized and followed by many researchers [31], [69].
Differentiable NAS naturally uses a parameter-sharing train-
ing strategy. It jointly trains the supernet’s parameters and
the importance weights of each path/edge [86]. Furthermore,
some researchers [67], [92] propose to progressively discard
the paths/edges with low importance during training to further
accelerate the training process. This progressive shrinking
strategy speeds up the training process signiﬁcantly. A defect
of the differentiable parameter sharing strategy is that the
supernet consumes high GPU memory, which would grow
linearly with regard to the number of candidate models. To
ﬁll
the single-path training strategy is proposed
to reduce the memory cost to the same level as training a
single candidate model. The conventional parameter sharing
strategy has different operators on different paths, even though
these operators are of the same type but with different hy-
perparameters (e.g., different kernel sizes). In light of this
observation, Single-Path NAS [66] designs a superkernel wk
for the DWConv inside a MBConv to choose between a 3 × 3
or a 5 × 5 kernel:

this gap,

wk = w3×3 + 1(||w5×5\3×3||2 > tk) · w5×5\3×3.

(18)

It views the 3 × 3 kernel as the inner core of the 5 × 5 kernel,
while zeroing out the weights of the outer shell. The 5 × 5
kernel can be viewed as the summation of this inner core w3×3

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

15

and the outer shell w5×5\3×3: w5×5 = w3×3 + w5×5\3×3.
The choice of the kernel size can be done using the indicator
function 1(·) ∈ {0, 1} in Equation (18), which is relaxed
to be a sigmoid function σ(·) to compute gradients. The tk
is a learned threshold. Likewise, the NAS decision of the
expansion ratio e ∈ {3, 6} of MBConv can be represented
as:

wk,e = 1(||wk,3||2 > te=3)·(wk,3 +1(||wk,6\3||2 > te=6)·wk,6\3),
(19)
where wk,3 is the channels with expansion ratio e = 3, and
can be viewed as the ﬁrst half of the channels of the MBConv
with expansion ratio e = 6, while zeroing out the second
half of the channels wk,6\3. The ﬁrst indicator function in the
Equation (19) is used to decide whether to skip the MBConv
layer. This NAS problem can then be formulated as a common
single-level optimization problem:

min
w

L(w|tk, te),

(20)

as opposed to the bi-level optimization in Equation (11). This
optimization is solved in a differentiable way. The memory
consumption of the single-path NAS is of the same level
as the largest candidate model. Nevertheless, this single-path
strategy only validates searching for different hyperparameters
of the same type of operators, especially for convolution-based
operators. ProxylessNAS [23], on the other hand, still relies
on a multi-path structure but only activates two paths at each
training iteration, which are sampled with the highest learnable
probabilities, and mask out all the other paths. FBNetV2 [79]
combines the single-path and multi-path strategies. It also
considers the smaller number of channels as part of the larger
volume of channels via vector masks, each of which has ones
in the ﬁrst entries and zeros in the remaining entries; different
vector masks are selected with Gumbel softmax weights; in
this way, only the largest set of ﬁlters needs to be trained.
For the resolution search, the authors propose to subsample
smaller feature maps from the largest feature map, perform
convolution, and enlarge with inserted zeros to the largest
size. This design resolves the resolution mismatch problem
during single-path training. Although FBNetV2 is still a multi-
path structure, it reduces the network paths with the single-
path strategy thus accelerating the training and reducing the
memory cost.

Different from the joint optimization of gradient-based
methods, evolutionary search or RL-based search decouples
the training and search process into two sequential steps. Thus,
it is unnecessary to use indicator functions or learnable weights
in the training. The single-path training strategy becomes
simpler. For example, SPOS [105] proposes to uniformly
sample a single path from a supernet and makes it well
trained. As a result, all candidate models are trained evenly
and simultaneously. This strategy alleviates the co-adaptation
problem of shared weights [125]. After training, subnets can
be directly sampled from the supernet for search evaluation.
Many papers [81], [91], [118], [76] then follow this simple yet
effective strategy, and design novel techniques to improve fair
sampling and training of candidate models [91], [118], [76]. A
comprehensive study is FairNAS [118], [76] that ensures the

parameters of every candidate operator be updated the same
amount of times at any training iteration. Speciﬁcally, similar
to SPOS, it randomly samples a candidate operator at each
layer to form a subnet but differently, the chosen operators are
not put back at the next sampling step. The sampling process
continues until all operators are sampled. The sampled models
are trained individually with back-propagation, but their gradi-
ents are accumulated to update the supernet’s parameters. This
strategy can alleviate the ordering issue [118], where candidate
models are trained with an inherent training order in SPOS.
Though SPOS and FairNAS accelerate the training process and
reduce the memory cost, it is conventionally required to retrain
the searched model before deployment because the inherited
parameters from the supernet are not specialized for a speciﬁc
model. Several studies [75], [84], [77] attempt to form a single-
training strategy that does not require any post training after
searching. BigNAS [75], [84] employs a bunch of training
techniques (e.g., sandwich rule,
in place distillation, and
batch norm calibration) [126], [75] to achieve a high quality
supernet so the inherited parameters can work well for any
subnets. OFA [77] is a more promising strategy that prevents
interference between subnets and thus a derived model can be
directly deployed. It trains candidate models from the largest
size (i.e., largest kernel size, depth, and width) to the smallest
size (i.e., smallest kernel size, depth, and width) progressively.
When training smaller subnets, the authors keep the last layers
or channels untouched and ﬁnetune the early ones from shared
parameters. As for elastic kernel size, the authors train kernel
transformation matrices to transform the center of the larger
kernel into the smaller kernel. The OFA strategy ensures each
candidate model has a speciﬁcally trained or ﬁnetuned part so
that mitigates their interference.

2) Performance Predictor: Another strategy to speed up
the training process is to estimate the accuracy of candidate
models using an accuracy predictor [77], [78], [85], [115],
[113]. Although it requires substantial [model, accuracy] pairs
thus computation and time to construct an accuracy predictor,
it is a one-time cost as the predictor can be re-used for multiple
hardware constraints. In addition, the accuracy predictor can
also be easily ﬁnetuned for new datasets. The input into the
accuracy predictor is the representation of candidate models,
which is often one-hot encoding of candidate operators and
hyperparameters [77], [85] or continuous values of hyperpa-
rameters (e.g., for channel counts) [78], [115]. A different
study is DONNA [113], which uses block-quality metrics
derived from blockwise knowledge distillation as the input into
the accuracy predictor. For the predictor architecture, some
studies use neural networks (e.g., MLP [78], [77] or RNN
learning models (e.g.,
[85]) while others use conventional
linear regressor [113] or Gaussian Process regressor [115]).
ChamNet [115] additionally adopts Bayesian optimization for
model sampling to achieve better sampling efﬁciency and
reliable prediction.

In addition to an accuracy predictor, predictors for hard-
ware cost are also widely studied to reduce the huge com-
munication expense between the target device and the model
training machine. Furthermore, the hardware cost predictor
is demonstrated high ﬁdelity across platforms (r2 ≥ 0.99)

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

16

[69]. Therefore, the cost predictor is popular and necessary.
A broadly investigated approach is the latency Lookup Table
(LUT) [106], [80], [86], [67], [66], [114], [76], [115], [77],
which records the runtime of each operator in the search space.
The basic assumption is that the runtime of each operator is
independent of other operators [86], [23], so that the latency of
an entire model a can be estimated as the sum of the latency
of each individual operator Oi:

LAT (a) =

(cid:88)

i

LAT (Oi).

(21)

Researchers proﬁle the target hardware and record the runtime
for each candidate operator to estimate the latency of candidate
models. Some research also considers the connectivity of
operators and the communication overheads in sequential
layers [67], [114], [80]. Another possible way is to construct
a regressor to estimate the hardware cost based on critical
features of a candidate model [107], [23], [115], [69], [31].
The performance regressor can be a linear regressor [69], [31],
a Gaussian Process regressor [115], or a neural network [107],
[23]. Different from directly measuring the hardware cost
and training a black-box performance regressor, two papers
examine speciﬁc devices and derive the runtime [108] or power
consumption [109] through theoretical analysis. Nevertheless,
this approach requires the knowledge of speciﬁc devices and
is inﬂexible to different devices.

The hardware cost predictor is essential for differentiable
search algorithms. Since each candidate operator is to be
selected by a binary indicator in a differentiable supernet,
the latency of it can be the weighted sum (i.e., with binary
indicators {0,1}) of the latency of each candidate operator
[86]:

LAT (a) =

(cid:88)

(cid:88)

l

i

Il,i · LAT (Ol,i), Il,i ∈ {0, 1},

(22)

where Ol,i and Il,i are the ith operator of the lth layer and
its associated binary indicator, respectively. LAT (Ol,i) can be
achieved through either LUT [86], [66], [67] or performance
regressor [23]. However,
the loss function (14) and (16)
with indicator Il,i is not directly differentiable. To sidestep
this problem,
the indicator is relaxed to be a continuous
variable computed via the Gumbel Softmax function [123] with
learnable parameters:

Il,i =

exp[(θl,i + gl,i)/τ ]
i exp[(θl,i + gl,i)/τ ]

(cid:80)

,

(23)

where τ ∈ (0, 1) is the temperature parameter that controls
the search efﬁciency and efﬁcacy. Larger τ makes the in-
dicator distribution smoother; smaller τ approaches discrete
categorical sampling. Despite the value of τ , the loss function
(14) and (16) with the latency calculation (22) and the relaxed
indicator (23) are differentiable. For single-path training, the
latency calculation (22) can be simpliﬁed [23] as:

LAT (a) =

(cid:88)

Il · LAT (Ol),

(24)

l
where Ol is the selected operator at layer l and Il is the path
probability of operator Ol.

The cost of preparing a hardware cost predictor is minimal
since it does not require training models. Only one forward
pass of a test model is sufﬁcient to record its cost.

IV. AUTOMATED COMPRESSION OF DEEP LEARNING
MODELS

Since deep learning models have proved remarkable perfor-
mance in a wide spectrum of problems, it is a natural idea
to compress these well-established models for memory saving
and compute acceleration and thus hardware-efﬁciency [127].
This idea is fervently supported by the fact that the hand-
crafted deep learning models are often over-parameterized
[128], [7], [8]. The goal of deep learning compression is
to modify well-trained models for efﬁcient execution without
signiﬁcantly compromising accuracy. Although related works
are quite divergent,
they can be broadly summarized into
four categories [128], [127]: tensor decomposition, knowledge
distillation, pruning, and quantization. In the past few years,
the above compression techniques have achieved great success
while they crucially rely on domain knowledge, hand-crafted
designs, and tremendous efforts for tuning. Recently, there is
a growing demand and trend for automating the compression
the above four compression categories. In
process on all
this section, we aim to provide a comprehensive review of
recent research studies on automated compression of neural
architectures with regard to the four compression categories.

A. Automated Tensor Decomposition

As the computation in neural networks is based on tensor
operations, it is intuitive to compress tensors to squeeze and
accelerate a neural network. The basic operation of tensor
decomposition is the mode-i product of a tensor with a matrix.
For a dth-order tensor X ∈ Rn1×n2×...×nd and a matrix
B ∈ Rm×ni (i ∈ {1, 2, ..., d}), the mode-i product between X
and B is R = X ×iB, where R ∈ Rn1×...×ni−1×m×ni+1...×nd
is also a dth-order tensor. Given this deﬁnition, a dth-tensor
A ∈ Rn1×n2×...×nd can be decomposed into one core
dth-order tensor G ∈ Rr1×r2×...×rd and d factor matrices
U(i) ∈ Rni×ri(i ∈ 1, 2, ..., d):

A ≈ G ×1 U(1) ×2 U(2) ×3 ... ×d U(d).

(25)

This decomposition is called Tucker decomposition [129], and
the tuple (r1, r2, ..., rd) is called the Tucker rank. By selecting
proper low ranks (i.e., ri < ni), the original tensor can be
represented by lightweight decomposed pieces. The parameter
compression ratio is:

.

(cid:80)d

P =

i=1 ni

(cid:81)d
i=1 rini + (cid:81)d
Given that the parameters of a neural network are in the form
of tensors (e.g., the ﬁlter in a convolutional layer is a 4-
way tensor), tensor decomposition can be naturally applied to
model the parameters in a more efﬁcient way. The speed-up
ratio of a convolution layer is:

i=1 ri

(26)

S =

n1n2n3n4H (cid:48)W (cid:48)
n1r1H (cid:48)W +n2r2HW (cid:48)+n3r3HW +n4r4H (cid:48)W (cid:48)+r1r2r3r4H (cid:48)W (cid:48) ,

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

17

TABLE II: Summary of Searching for Efﬁcient Deep Learning Models. The “hardware” column indicates on which the achieved
models are evaluated. The latency is reported per input; otherwise is speciﬁed in the relevant table cells (e.g., FPS). ∼ indicates
the exact data is not reported in the paper and thus estimated from the reported ﬁgures. ‘-’ indicates unavailable records.

Search

strategy

Reference

Dataset

Hardware

Xilinx Zynq-7000 SoC ZC70

Xilinx Virtex-7 FPGA VC707

Xilinx Zynq-7000 SoC ZC70

Xilinx Virtex-7 FPGA VC707

[108]

[61]

[64]

MNIST

CIFAR10

CIFAR10

CIFAR10

-

-

error: 3.85/2.75/2.50

error: 4.57/3.69/3.05/2.58

Accuracy

acc: 99.49

acc: 99.46

acc: 88.25

acc: 88.52

Latency

64.74ms

105.54ms

136.15ms

160.75ms

-

-

[113]

ImageNet

NVIDIA V100/Samsung S20

Top1 acc: ∼79/78.5

∼20ms/16.25ms

[76]

ImageNet

MACE (Mi MIX3)/

Top1/Top5 acc: 75.5/92.6

10.3ms/10.0ms/81ms

SNPE (Mi MIX3)/

Top1/Top5 acc: 75.9/92.8

11.8ms/11.1ms/101ms

Google Pixel 1

Top1/Top5 acc: 75.3/92.5

9.6ms/8.8ms/71ms

FLOPs

# of Parameters

-

-

1290M/535M/4147M

3.3M/3.3M/26.8M

-

-

304M

248M

221M

0.5M/1.1M/4.7M/13.1M

-

5.1M

5.5M

5.4M

-

-

-

[115]

ImageNet

Snapdragon 835

Top1 acc: 75.4/73.8/71.6/69.1/64.2

29.8ms/19.9ms/15.0ms/10.0ms/6.1ms

553M/323M/212M/120M/54M

[114]

ImageNet

[62]

[118]

CIFAR10

ImageNet

NVIDIA GV100/

Top1/Top5 error: 23.6/6.9

12.0ms/31.6ms/76.9ms

Intel Xeon Gold 6136/

Top1/Top5 error: 23.5/6.8

13.4ms/26.4ms/69.1ms

NVIDIA Jetson Xavier

Top1/Top5 error: 23.8/6.9

12.9ms/31.8ms/52.7ms

Raspberry Pi 3(B+)

acc: ∼81/∼91/∼94

10ms/100ms/1000ms

-

-

-

Top1 acc: 75.3/77.5

[48]

ImageNet

Google Pixel 1

[72]

[80]

[106]

CIFAR10

CIFAR100

ImageNet

PTB

WT2

COCO

ImageNet

COCO

-

Google Pixel 1

-

[31]

ImageNet

Google Pixel 4(CPU/GPU/

DSP/EdgeTPU)

Top1/Top5 acc: 75.2/92.5

Top1/Top5 acc: 75.6/92.7

Top1/Top5 acc: 76.7/93.3

error: 2.82

error: 18.13

Top1/Top5 error: 27.5/9.1

Perplexity: 57.5

Perplexity: 69.4

mAP: 24.9/25.5

Top1 acc: 75.4

mAP: 22.5

Top1 acc: 74.9

Top1 acc: 75.8

-

78ms

84ms

103ms

-

388M/392M

4.6M/5.9M

312M

340M

403M

-

497M

-

3.9M

4.8M

5.2M

2.5M

4.4M

23M

33M

187ms/196ms

900M/940M

2.5M/2.6M

57.1ms

106ms

25.2ms/4.47ms/3.38ms/2.22ms

31.0ms/5.40ms/3.81ms/2.40ms

-

349M

433M

-

4.39M

4.91M

[69]

COCO

Google Pixel 4 EdgeTPU

mAP: 25.5/25.4/24.7

6.9ms/6.8ms/7.4ms

1530M/1760M/970M

4.20M/4.79M/4.17M

Google Pixel 1 CPU

mAP: 23.7/22.7/23.4

122ms/107ms/113ms

510M/390M/450M

3.85M/2.57M/4.21M

[23]

[86]

[79]

[66]

[67]

[23]

[92]

CIFAR10

ImageNet

ImageNet

ImageNet

ImageNet

ImageNet

ImageNet

ImageNet

COCO key-point

Cityscapes

ADE20K

KITTI

Google Pixel 4 DSP

mAP: 28.5/28.5/26.9

12.3ms/11.9ms/12.2ms

2820M/3220M/1430M

7.16M/9.15M/4.85M

-

error: 2.30/2.08

Google Pixel 1

Top1/Top5 acc: 74.6/92.2

Tesla V100

Top1/Top5 acc: 75.1/92.5

-

78ms

5.1ms

-

5.8M/5.7M

-

Samsung Galexy S8

Top1 acc: 73.0/74.1/74.9

19.8ms/23.1ms/28.1ms

249M/295M/375M

4.3M/4.5M/5.5M

-

Top1 acc: 68.3/73.2/76.0/77.2

Google Pixel 1

Top1/Top5 acc:74.96/92.21

-

79.48ms

56M/126M/238M/325M

-

TITAN-XP

Top1 acc: 76.1/73.1/74.6/75.3

28.9ms/13.6ms/15.4ms/17.9ms (per batch)

479M/251M/314M/361M

Google Pixel 1

Top1/Top5 acc: 74.2/91.7

79ms

-

-

-

-

-

Top1 acc: 76.6/77.3

AP/APM/APL/AR: 75.5/72.6/81.7/79.4

-

AP/APM/APL/AR: 65.7/62.5/72.1/71.4

-

mIoU: 74.26/75.90

mIoU: 33.22/34.92

AP moderate/easy/hard: 78.49/87.62/75.53

AP moderate/easy/hard: 69.74/83.09/74.89

267M/325M

5.5M/6.4M

3720M

350M

6.6M

1.1M

1910M/4660M

2.20M/3.85M

1420M/2190M

2.49M/3.86M

15650M

3220M

4.74M

2.13M

t
f
o
S
+
t
p
O
s
e
y
a
B

t
f
o
S
+
S
E

t
f
o
S
+
L
R

t
f
o
S
+
e
l
b
a
i
t
n
e
r
e
f
f
i

D

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

18

TABLE II: Summary of Searching for Efﬁcient Deep Learning Models (cont.).

Search

strategy

Reference

Dataset

Hardware

Accuracy

Latency

CIFAR10

[85]

Titan X/

Maxwell 256/

ARM Cortex53

error: 4.62

error: 4.36

error: 4.78

error: 4.93

error: 4.84

9ms/82ms/149ms

13ms/62ms/912ms

6ms/75ms/210ms

7ms/44ms/381ms

8ms/65ms/145ms

ImageNet

Maxwell 256/

Top1/Top5 error: 24.16/7.13

218ms/5421ms

ARM Cortex53

Top1/Top5 error: 25.98/8.21

69ms/676ms

VOT-19

A10 Fusion PowerVR GPU/

EAO/acc/robustness: 0.333/0.590/0.376

GOT-10K

Kirin 985 Mali-G77 GPU/

AO/SR0.5: 0.611/0.710

52.6FPS/27.4FPS/

TrackingNet

Snapdragon 845 GPU/

Snapdragon 845 DSP

P/Pnorm/AUC: 69.5/77.9/72.5

38.4FPS/43.5FPS

FLOPs

63.5M

1364M

137M

270M

59.27M

9276M

523M

530M

# of Parameters

0.52M

11.39M

1.00M

2.04M

0.45M

77.16M

4.8M

1.97M

SemanticKITTI

NVIDIA GTX1080Ti

mIoU: 60.3/63.7/66.4

89ms/110ms/259ms

8900M/15000M/73800M

1.1M/2.6M/12.5M

ImageNet

Google Pixel 1

Top1 acc: 76.9/80.0

58ms/-

230M/595M

-

-

-

-

-

-

-

-

-

-

-

-

Top1/Top5 acc: 79.1/94.5

Top1/Top5 acc: 80.5/95.1

Top1/Top5 acc: 81.3/95.5

Top1/Top5 acc: 82.8/96.3

mAP: 30.5/33

acc: ∼0.925/∼0.85

error: 3.48/3.87/2.95/3.98/3.22

acc: 95.81/94.04/94.82/93.16/

95.02/95.64/95.18/93.65/93.07

AP: 83.2/78.2

AP: 73.9

Top1 acc: 76.5/78.9/79.5/80.9

Top1/Top5 acc: 76.3/93.2

Top1/Top5 acc: 78.8/94.4

Top1/Top5 acc: 79.8/94.9

Top1/Top5 acc: 81.1/95.5

Top1/Top5 acc: 82.6/96.3

Top1/Top5 acc: 83.3/96.7

Top1/Top5 acc: 84.0/96.9

Top1/Top5 acc: 84.4/97.1

[70]

ImageNet

TPUv3/GPUv100

Top1 acc: 80.0

15.7ms/45.5ms

Top1 acc: 77.3

8.71ms/22.5ms

Top1 acc: 79.4

13.6ms/34.4ms

Top1 acc: 81.4

31.9ms/66.6ms

Top1 acc: 83.0

64.9ms/149.2ms

Top1 acc: 83.7

125.9ms/290.2ms

Top1 acc: 84.4

258.1ms/467.2ms

Top1 acc: 84.7

396.1ms/847.7ms

Top1/Top5 acc: 76.1/94.0

Top1/Top5 acc: 72.2/91.0

Top1/Top5 acc: 74.7/92.0

-

[87]

CIFAR100

ImageNet

-

357M

557M

762M

2100M

2900M/5300M

∼20M/∼2M

-

-

5.3M/10.6M

-

-

7.7M/3.4M/29M/2.2M/4.0M

3390M/1400M/6530M/890M/

0.143M/0.047M/0.067M/0.425M/

3300M/13590M/210120M/12570M/1000M

0.171M/0.733M/2.626M/0.074M/0.035M

1440M/370M

5640M

7.3M/2.5M

16.3M

242M/418M/586M/1040M

4.5M/5.5M/6.4M/9.5M

390M

700M

1000M

1800M

4200M

9900M

19000M

37000M

910M

1580M

1890M

4300M

10400M

22200M

52000M

93000M

87.3M

294M

471M

5.3M

7.8M

9.2M

12M

19M

30M

43M

66M

7.6M

10.4M

11.5M

16M

34M

60M

137M

199M

1.92M

3.4M

4.7M

t
f
o
S
+
r
e
h
t
O

d
r
a
H
+
S
E

d
r
a
H
+
L
R

d
r
a
H
+
r
e
h
t
O

[81]

[91]

[77]

[78]

ImageNet

COCO

[88]

CIFAR10

[65]

[84]

CIFAR10

Google speech

commands

PoseTrack2018

COCO2017

[75]

ImageNet

[68]

ImageNet

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

19

where H × W is the input feature map size, H (cid:48) × W (cid:48) is
the output feature map size, and n1 × n2 × n3 × n4 is
the kernel size. Tucker decomposition is a widely applied
approach to this aim by tensorization of neural network layers
[130], [131]. Another commonly used tensorization approach
is the Canonical Polyadic (CP) decomposition [132], which is
a special case of the Tucker decomposition [133], [134]. After
decomposition, it is usually required to ﬁne-tune the tensorized
network to recover the accuracy.

When tensorizing neural networks, the decomposition rank
is the most important hyperparameter that needs to be carefully
selected since it controls the compression-accuracy trade-off.
A typical selection method is cross-validation, which is quite
cumbersome for selecting a diverse range of ranks, so the
common practice is to set the ranks of different layers to be the
same. This simpliﬁcation is coarse and sub-optimal. Therefore,
automating the rank-selection process is crucial to determining
optimal decomposition ranks. Kim et al. [130] propose to
employ global analytic solutions for variational Bayesian ma-
trix factorization (VBMF) [135] for automatic rank selection
of Tucker decomposition. Publicly available tools can easily
implement the VBMF. They perform full model compression
including fully connected and convolutional layers and show
that the accuracy degradation after compression can be well
recovered by ﬁne-tuning. However, Gusak et al. [136] claim
that they ﬁnd it difﬁcult to restore the initial accuracy by ﬁne-
tuning with the global analytic VBMF ranks. Therefore, they
use the global analytic solution of Empirical VBMF (EVBMF)
to automatically select ranks. Instead of directly using the
ranks achieved from EVBMF, the authors design a weakened
rank for decomposition, which is larger than the extreme rank
(i.e., obtained by EVBMF) and thus not optimal with a certain
amount of redundancy after decomposition. The reason for
this design is that this work proposes an iterative compression
and ﬁne-tuning strategy that gradually compresses the original
model and restores the initial accuracy. Different from tensor
decomposition on a well-trained network followed by ﬁne-
tuning in [130], [136], Hawkins et al. [137], [138] oper-
ate automatic rank determination and tensor decomposition
along with the model training process. They propose a prior
distribution mask, which can be optimized during training,
over the decomposition core tensor to control the rank. If
an element of the resultant posterior mask is smaller than a
threshold, it will be regarded as zero and the rank will be
automatically selected. The authors use a Bayesian inference
method to train this low-rank tensorized model and prove
its efﬁcacy for various decomposition schemes. MARS [139]
sets a factorized Bernoulli prior and optimizes the model via
relaxed MAP (maximum a posteriori) estimation to achieve a
binary mask to control the rank so the manually set threshold
used in [137], [138] is avoided. It shows slightly worse
in compression but better accuracy than [138]. Inspired by
the success of reinforcement learning in making decisions,
Javaheripi et al. [140], [141] propose a state-action-reward
system to automatically select the optimal rank for each layer.
The hardware cost and accuracy are both considered in the
reward and the action of decomposing a layer with the highest
rewarded rank is selected. Bayesian optimization can also

utilized to achieve the optimal ranks [142].

TABLE III summarizes the main results of automated tensor
decomposition studies. Note that the latency is achieved with
the hardware listed in the table, which is different from the
hardware for training.

B. Automated Knowledge Distillation

Knowledge distillation (KD) is extended from knowledge
transfer (KT) [143] by Ba and Caruana [144] to compress
a cumbersome network (teacher) into a smaller and simpler
network (student). This is done by making the student model
mimic the function learned by the teacher model in order to
achieve a competitive accuracy. It is later formally popularized
by Hinton et al. [21] as a student-teacher paradigm, where
the knowledge is transferred from the teacher to the student
by minimizing the difference between the logits (features
before the ﬁnal softmax) of the teacher and student. In many
situations, the performance of the teacher is almost perfect
with a very high classiﬁcation probability for the correct
class and ﬂat probabilities for the other classes. Therefore,
the teacher is not able to provide much more information
than the ground truth labels. Hinton et al. [21] introduce the
concept of softmax temperature to transfer knowledge, which
can better deliver the information of which classes the teacher
ﬁnd similar to the correct class. Formally, given the logits of
the teacher model, the classiﬁcation probability pi of the class
i is:

pi =

exp( zi
τ )
j exp( zi
τ )

(cid:80)

,

(27)

where τ is the temperature parameter. It controls how soft the
labels from the teacher are. The soft labels together with the
ground truth labels are used to supervise a compact student
model.

Vanilla knowledge distillation mostly focuses on trans-
ferring knowledge to a student model with a ﬁxed small
architecture, which is manually designed in advance. However,
different teachers and tasks favour different student archi-
tectures, and hand-crafted architectures are prone to be sub-
optimal. Considering these limitations, there is a growing trend
to automate the architecture design of a student model [145],
[146], [147], [148], [149], [150], [151]. The ground-truth
labels are combined with the distillation labels to guide the
automatic design process. AKDNet [145] proposes to search
optimal student architectures for distilling a given teacher by
RL-based NAS. It adopts the efﬁcient search space of [48] and
designs a KD-guided reward with a teacher network. KDAS-
ReID [147], AdaRec [150], and NAS-KD [151] leverage
the differential NAS [54] to determine the optimal student
structures. In addition, Bayesian optimization [149], [148] and
evolutionary search [146] are also popular search strategies in
the context of student architecture search.

Although the soft labels from the ﬁnal output of a teacher
have been demonstrated effective for transferring knowledge,
some works argue that it is also helpful to mimic the teacher
from the intermediate layers. AdaRec [150] and NAS-KD
[151] try to minimize the difference between the intermediate
features of the student and the teacher. Similarly, Li et al. [152]

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

20

TABLE III: Summary of Automated Tensor Decomposition Studies. The “hardware” column indicates on which the achieved
models are evaluated. The model name (e.g., AlexNet) in the “Accuracy” column indicates the original model before tensor
decomposition. Some studies only report relative performance changes which are denoted as n× in the table, indicating the
original model is n times its compressed counterpart. The latency is reported per input. ‘-’ indicates unavailable records.

Method

Reference

Dataset

Hardware

Accuracy

FLOPs

Latency

# of Parameters

[130]

ImageNet

Nvidia Titan X

AlexNet/VGG-S/GoogleNet/VGG-16 (Top5 acc):

272M/549M/760M/3139M

Samsung Galaxy S6

78.33/84.05/88.66/89.40

0.30ms/0.92ms/1.48ms/4.58ms

43ms/97ms/192ms/576ms

11M/14M/4.7M/127M

n
o
i
t
i
s
o
p
m
o
c
e
D

r
o
s
n
e
T

d
e
t
a
m
o
t
u
A

[136]

ARD-LU[137]

ARD-HU[137]

[138]

[139]

VOC2007

COCO2014

MNIST

IMDB

Criteo Ad Kaggle

MNIST

IMDB

Criteo Ad Kaggle

CIFAR-10

MNIST

CIFAR-10

-

-

-

-

-

Faster R-CNN (mAP): 69.2/68.3/77.0/75.0

10.49×/13.95×/1.57×/1.49×

Faster R-CNN FPN (mAP/mAP.50): 35.4/56.2

Faster R-CNN FPN (mAP/mAP.50): 36.2/57.1

1.8×

1.7×

MLP (Acc): 98.06/98.30/96.28/98.24

DLRM (Acc): 87.61/87.79/85.33/88.93

BiLSTM (Acc): 78.61/78.64/78.67/78.72

MLP (Acc): 97.98/98.30/97.04/98.23

DLRM (Acc): 87.54/88.01/85.82/88.78

BiLSTM (Acc): 78.57/78.62/78.63/78.73

ResNet-110 (Acc): 90.4

LeNet-5 (Acc): 99.0

ResNet-110 (Acc): 90.7/91.1

-

-

-

1.19×

-

-

-

-

-

-

[140], [141]

ImageNet

ARM-A57

AlexNet (Top5 acc): 81.01/80.37/79.98

543M/349M/277M

4.39s/2.21s/1.61s

VGG-16 (Top5 acc): 90.05/89.61/88.89

4950M/3170M/2360M

72.15s/42.55s/30.48s

[142]

ImageNet

-

ResNet18(Top1/Top5 acc): 68.16/88.15

ResNet50(Top1/Top5 acc): 74.83/92.28

-

-

-

7k/101k/4k/6k

6k/62k/23k/16k

564k/437k/154k/200k

7k/91k/4k/5k

6k/58k/19k/14k

571k/402k/160k/164k

7.4×

10×

7×/5.5×

-

3.15M

4.49M

and MFAGAN [153] compress the student generator in Gen-
erative Adversarial Networks (GANs) via intermediate feature
distillation and once-for-all NAS [77]. PPCD-GAN [154] also
compresses the generator but with a teacher-guided learnable
mask to automatically reduce the number of channels.

Unlike searching for how to reduce, Kang et al. [155] and
Mitsuno et al. [156] propose to search on how to increase.
They set an extremely small student backbone at the start
point, and augment operations [155] or additional layer chan-
nels [156] to the backbone during the search procedure. A
large pre-trained teacher [156] or an ensemble teacher [155]
is used to guide the search process. The motivation of these
works is to alleviate the search burden with a start point and to
maximize the distilled knowledge by optimally reducing the
capacity gap between teacher and student.

A different direction is NAS-BERT [157], where the block-
wise KD [158] is applied to train a supernet composing a
bunch of candidate compact subnets. Then, the meta infor-
mation (e.g., parameter, latency) of the candidate networks
is summarized in a lookup table. Given certain hardware
constraints, all candidate networks that satisfy the constraints
are evaluated and the best performing one is selected as the
ﬁnal compressed model. The beneﬁt of this strategy is that
no retraining and researching are required for new hardware
constraints.

Not only the student architecture is searchable, but also
related hyperparameters (e.g., the temperature τ in Equation
(27)) [148] can be included in the search process. AutoKD
[148] uses the BayesOpt to simultaneously explore the optimal
student structure and KD hyperparameters, temperature τ and
loss weight α, during the KD process.

We outline the major results of automated knowledge dis-

tillation papers in TABLE IV.

C. Automated Pruning

Neural network pruning, a.k.a. sparsiﬁcation, attempts to
reduce memory storage and computation cost by removing
unimportant weights or neurons from the base network. Ac-
cording to the granularity, pruning can be roughly categorized
into structured pruning and unstructured pruning. Unstructured
pruning enforces weights or layers to be sparse by removing
individual connections or neurons. Structured pruning, on the
other hand, targets discarding the entire channels or layers.
Although unstructured pruning can theoretically achieve a
better accuracy-compression tradeoff, it is less compatible with
existing deep learning platforms and hardware than structured
pruning. Thus, structured pruning attracts more research focus.
A typical pruning procedure is to ﬁrst identify and cut
away unimportant weighs or layers from an existing over-
parameterized network and then ﬁne-tune the pruned network
to restore the accuracy. The ﬁrst step is the key and largely
relies on expert knowledge and hand-crafted heuristics, such
as manually setting the criterion of parameter importance
[159] and the pruning rate [160]. However, heuristic settings
are typically based on trial and error and are sub-optimal
so researchers actively seek automated solutions to alleviate
human intervention during pruning. These efforts can roughly
be categorized into three directions.

1) Reducing the number of hyperparameters (i.e., per-
layer pruning rates) needed to be tuned [161], [162],
[163]. Zeng et al. [161] compute the importance of model
parameters using the Taylor expansion of the loss function

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

21

TABLE IV: Summary of Automated Knowledge Distillation Studies. The “hardware” column indicates on which the achieved
models are evaluated. The model name (e.g., Inception-ResNet-V2) in the “Accuracy” column indicates the teacher model; if
the teacher is not speciﬁed, the teacher is searched during training. The latency is reported per input; otherwise is speciﬁed in
the relevant table cells (e.g., s/batch). ‘-’ indicates unavailable records.

Method

Reference

Dataset

Hardware

Accuracy

FLOPs

Latency

# of Parameters

Memory

[145]

ImageNet

Pixel 1 phone

Inception-ResNet-v2 (Top5 acc): 87.5/89.1/93.1

Inception-ResNet-v2 (Top1 acc): 66.5/69.6/75.5

-

-

-

TPUv4i

-

-

-

BERT (Acc): 84.1

BERT (Acc): 90.3

ResNet50 (mAP/Rank): 94.7/95.6

Inception-Resnet-V2 (Acc): 81.2

DARTS (Acc): 76.0

Inception-Resnet-V2 (Acc): 78.0

IB-BERTLARGE (AvgAcc): 80.38/81.69

IB-BERTLARGE (F1): 88.4/88.1

NextItNet (MRR@5/HR@5/NDCG@5): 0.7345/0.7964/0.7500

NextItNet (MRR@5/HR@5/NDCG@5): 0.6343/0.7151/0.6544

NextItNet (MRR@5/HR@5/NDCG@5): 0.4489/0.6519/0.4995

BERTbase (SST-2/MRPC/QQP/MNLI-m/MNLI-mm/QNLI/RTE):
92.2/86.3/70.4/81.0/80.2/88.6/65.9

86.9/79.3/67.5/76.1/75.5/83.9/58.9

Pix2Pix (mIoU): 41.71

GauGAN (mIoU): 61.17

GauGAN (mIoU/FID): 35.34/25.06

MFANet (PSNR/LPIPS): 30.16/0.0571

5450M

31200M

35400M

-

-

-

-

-

-

-

15ms/25ms/75ms

-

-

-

-

24M

16.9M

14.3M

4M

6M

6M

0.45ms/0.58ms

20.6M/28.5M

0.59ms/0.49ms

22.8M/20.6M

-

-

-

-

42.4M

33.2M

0.89M

20.2M

26M

-

-

-

-

-

-

-

-

n
o
i
t
a
l
l
i
t
s
i
D
e
g
d
e
l
w
o
n
K
d
e
t
a
m
o
t
u
A

[146]

SST-2

Ag News

[147]

Market-1501

[148]

[149]

[150]

CIFAR100

MIT67

ImageNet

GLUE

SQuAD

RetailRocket

30Music

ML-2K

[151]

GLUE

[152]

Cityscapes

[153]

COCO-Stuff

Set5

Set14

B100

Urban100

NVIDIA V100

MFANet (PSNR/LPIPS): 26.69/0.113

MFANet (PSNR/LPIPS): 25.33/0.1332

MFANet (PSNR/LPIPS): 24.23/0.1132

8410M

21.9ms

0.55M

0.52G

[154]

ImageNet

AMD Ryzen 9 3900X

NVIDIA GTX3090

BigGAN (IS/FID/LPIPS): 83.13/12.76/0.62

1600M

2.05s/batch

1.19s/batch

13.6M

[157]

GLUE

SQuADv1.1

SQuADv2.0

BERTbase (MNLI/QQP/QNLI/CoLA/SST-2/STS-B/RTE/MRPC)
84.1/88.8/91.2/50.5/92.6/86.9/72.7/86.4

-

-

-

60M

BERTbase(EM/F1): 81.2/88.3

BERTbase(EM/F1): 73.9/77.1

-

-

and prune unimportant ones. They introduce an auxiliary
hyperparameter that controls the model shrinking proportion
at each pruning iteration and show that this hyperparameter
is insensitive to the ﬁnal performance. In this way, they avoid
carefully tuning the overall pruning rate. Li et al. [162] use the
regularization approach, Alternating Direction Method Multi-
pliers (ADMM), as the core pruning algorithm but substitute
its hard constraint with the soft constraint-based formulation
and solve the optimization problem with the Primal-Proximal
solution. Their approach naturally does not require predeﬁning
the per-layer pruning rates and thus reduces the number of
hyperparameters. Zheng et al. [163] introduce the normalized
Hilbert-Schmidt Independence Criterion (nHSIC) from the
information theory to measure the per-layer importance and
derive the compression problem with constraints into a linear
programming problem, which is convex and easily solved. In
such a manner, only two hyperparameters are required and
demonstrated robust to different values.

2) Searching for optimal pruning rates or magnitude
thresholds for pruning [26], [164], [165], [166], [167], [168],

[169], [170], [171], [172], [173]. Note that the magnitude
thresholds of weights can be naturally derived from prun-
ing rates. AMC [26] and Auto-Prune [165] leverage deep
reinforcement learning (DRL) to automatically determine the
pruning rate of each layer and empirically prune the weights
with the least magnitude. Arguing that DRL has an inherited
incompatibility with the pruning problem, Liu et al. [164] sug-
gest using the heuristic search technique, simulated annealing
(SA), to search for the per-layer pruning rates and then use
the ADMM to dynamically regularize network weights for
structural pruning. They hypothesize that the layers with more
weights can have higher compression rates. The authors also
apply the ADMM to searching for the magnitude thresholds
for the second-phase unstructured pruning after the ﬁrst-phase
structural pruning. Tung et al. [167] propose to employ a
Bayesian optimization framework to search the pruning hyper-
parameters including the magnitude threshold. Some studies
[168], [169] follow this work to make it constraint-aware [168]
and applicable to pruning deeper networks [169]. Other than
pruning full-precision neural networks, Guerra and Drummond

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

22

[171] aim at automatically pruning quantized neural networks
by a customized, rule-based pruning strategy with Bayesian
optimization of layer-wise pruning ratios.

[176],

[177]. Different

3) Learning the weight/channel importance [174], [24],
[175],
from relying on heuristic
weight/channel importance measurement like the weight mag-
nitude, this direction struggles to learn the importance mea-
surement through optimizing the ﬁnal objective. Liu et al.
[175] propose a slimming scheme that regularizes the scaling
factors in batch normalization (BN) layers as a channel selec-
tion indicator to identify unimportant channels (or neurons).
Then, they apply a threshold to the trained scaling factors for
channel pruning. AutoPrune [24] and DAIS [174] are follow-
ing works that rely on auxiliary learnable channel selection
indicator for pruning. AutoPrune argues that decoupling the
channel selection indicators and network parameters will help
to stabilize the weight learning and make the pruning insensi-
tive to hyperparameters. DHP [176] introduces an additional
hypernetwork that can generate weights for the backbone
network (i.e., the network to be pruned). The input of the
hypernetwork is the latent vectors, which are attached to
each layer of the backbone network. Binary channel masks
will then be achieved by setting those latent vector elements,
which are smaller than a threshold, to be zero otherwise to
be one. In contrast to learning channel importance, ASBP
[177] targets to automatically decide the binary importance
of each bit of weight. This bit-level pruning has the ﬁnest
granularity but is impractical to achieve a real reduction of
resource consumption due to poor hardware support. To solve
this problem, the authors target a speciﬁc sort of hardware,
RRAM, which assembles multiple low-precision cells together
as a crossbar to represent a high-precision data value. They
then prune at the granularity of crossbar size, that is all cells in
a crossbar share the same pruning strategy. The DL algorithm,
deep deterministic policy gradient (DDPG), is used to search
for the optimal set of bit-pruning strategies.

1) Unifying knowledge distillation and pruning: Since
pruning and knowledge distillation both require a pre-trained
model (i.e., the base model in pruning; teacher in KD) to guide
the compression process, researchers naturally explore ways to
unify these two techniques. One route is the two-step unifying
scheme [178], [179]. TAS [178] is an early and representative
work. It ﬁrst searches for the shrunken width and depth of
the pruned network with the help of learnable probability
from the base model. The searched architecture is then trained
from scratch as the student of a simple KD approach with
the base model serving as the teacher. The other route is to
unify pruning and KD in a single step [180], [181]. Gu and
Tresp [181] introduce learnable channel gates into the base
model and train the network weights and channel gates with
a distillation-aware loss function. Thus, this scheme prunes
the base model and distils its knowledge simultaneously and
automatically. Yao et al. additionally search for both teacher
and student to pursue the optimal distillation pair.

D. Automated Quantization

Unlike the above three compression techniques, which
struggle to optimize network architectures, quantization ap-

peals to reduce the representation precision of network weights
and intermediate activation tensors. Neural networks are gen-
erally trained using the 32-bit ﬂoating-point precision; if we
were to perform network inference in the 32-bit ﬂoating-point
as well, MAC operations, data transfer, and data saving would
have to be done all in 32-bit ﬂoating-point. Hence, using
lower bit precision would substantially reduce the hardware
overhead, including communication bandwidth, computation
and memory usage [182]. For example, when moving from 32
to 8 bits, the memory cost and computation would decrease
by a factor of 4 and 16 respectively. In addition, ﬁxed-
point computation is more efﬁcient
than its ﬂoating-point
counterpart [182]. The most commonly explored quantization
scheme, uniform quantization, converts a ﬂoating point tensor
x = {x1, ..., xN } with range (xmin, xmax) into its integer
coding xq with range [n, p] = [0, 2b − 1] via the following
deﬁnition [183]:

xq = (cid:98)clamp(

x
∆

+ z; n, p)(cid:101),

(28)

∆ =

xmax − xmin
p − n

, z = clamp(−(cid:98)

xmin
∆

(cid:101) + z; n, p),

(29)

ˆx = (xq − z)∆,

(30)

where ∆ is the scale factor that speciﬁes the step size of the
quantizer; z is the zero-point that represents the real value
zero without any precision loss; b is the quantization bitwidth;
(cid:98)·(cid:101) is the round-to-nearest operation. The clamp(·) function
is to truncate all values to fall between n and p. Through this
procedure, two sorts of errors are induced: a clipping error
induced by the clamp function and a rounding error induced
by the round-to-nearest operation (cid:98)·(cid:101). The quantized tensor
ˆx is then used for efﬁcient but low precision computation.
Since there are no theoretical correlations between the model
accuracy and b, simple try-and-error is only feasible in the
scenario where all weights and activation tensors have the
same precision. However, different layers have different re-
dundancy and behaviours on both the hardware and the task.
Therefore, automatically determining the bitwidth of weights
and activations for each layer is stunningly attractive, and
this mixed-precision feature has recently been supported by
hardware manufacturers [184], [185].

One direction is to model the layer-wise bit-precision as-
learning problem, where the
signment as a reinforcement
action is to determine the bitwidth values [25], [186]. HAQ
[25], [186] is such a representative work. It utilizes the DDPG
agent to explore a continuous action space of [0, 1], from
which selected actions (i.e., real numbers) can be rounded
into discrete bitwidth values for the weights and activations
of each layer. The use of a continuous action space is based
on the consideration that a discrete action space is not able to
represent the relative order of bitwidth. The quantized model is
then retrained one more epoch to recover the accuracy. Since
the authors induce the hardware constraints by limiting the
search space, they only consider minimizing the accuracy drop
after quantization in the reward function. Another direction is
to develop a differentiable framework that searches for the
quantization bitwidth in the same way as searching for the

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

23

TABLE V: Summary of Automated Pruning Studies. The “hardware” column indicates on which the achieved models are
evaluated. The model name (e.g., LeNet-300-100) in the “Accuracy” column indicates the original model before pruning; if
the original model is not speciﬁed, it is searched during training. The latency is reported per input; otherwise is speciﬁed in
the relevant table cells (e.g., FPS (frame per second)). n% indicates the compressed model is n per cent of its original model;
↓ n% indicates the compressed model decreases n per cent compared to its original model. n× indicates that the original
model is n times its compressed counterpart. ‘-’ indicates unavailable records.

Method

Reference

Dataset

Hardware

Accuracy

MNIST

LeNet-300-100/LeNet-5(Top1 error): ↑0.08/↑0.04

[161]

CIFAR-10

-

CifarNet(Top1 error):↑ 0.17

Latency

# of Parameters

Memory

-

77×/200×

15.6×

[162]

[163]

[26]

[164]

[165]

ImageNet

CIFAR-10

CIFAR-100

Qualcomm Kryo

485 Octacore

Qualcomm

Adreno 640

Qualcomm Kryo

485 Octacore

Qualcomm

Adreno 640

VGG16(Top1/Top5 error): ↑2.3/↑1.2

ResNet50(Top1/Top5 error): ↑2.0/↑1.1

VGG16/ResNet-18/MobileNetV2(acc): 93.5/94.2/94.6

VGG16/ResNet-18/MobileNetV2(acc): 72.2/75.3/78.7

CIFAR-10

-

VGG/ResNet-20/ResNet56(Top1 acc): 94.00/92.01/94.05

98.8M/20.8M/59.5M

-

ImageNet

Google Pixel 2

MobleNetV1/MobileNetV2(Top1 acc): 68.06/69.13

149M/149M

MobleNetV1/MobileNetV2(Top1 acc): 70.92/71.54

283M/219M

Plain-20/ResNet56(acc): 90.2/91.9

50%/50%

VGG16/MobileNetV1/MobileNetV2(acc): ↓1.4/↓1.7/↓1.0

20%/40%/50%

MobileNetV1(Top1/Top5 acc): 70.2/89.2

MobileNetV1(Top1/Top5 acc): 70.5/89.3

272M

285M

VGG16/ReNet-18(Acc): 93.21/93.81

8.8 ×/12.2 ×

2.7ms/1.45ms

-

Google Pixel 1

Qualcomm

Adreno 640

CIFAR-10

ImageNet

CIFAR-10

ImageNet

CIFAR-10

MNIST

CIFAR-10

-

VGG16/ReNet-18(Top5 acc): ↓0.6/↓0.1

ReRAM simulator

AlexNet/VGG16/Plain20(Acc5): 99.10/98.62/98.29

AlexNet/VGG16/Plain20(Acc1): 98.49/98.63/98.00

-

-

ResNet20/ResNet32/ResNet56(Top1 acc): 92.06/92.60/93.66

↓48.35%/↓45.05%/↓52.00%

ResNet20/ResNet32/ResNet56(Top1 acc): 66.93/68.98/70.50

↓46.35%/↓55.17%/↓55.29%

g
n
i
n
u
r
P

d
e
t
a
m
o
t
u
A

[166]

CIFAR-100

ImageNet

[167]

UCMerced Land

Use

Describable

Textures

-

-

ResNet18/ResNet34/ResNet50/MobileNetV2(Top1 acc):

69.65/73.56/76.84/71.60

AlexNet(Acc): 94.1

AlexNet(Acc): 52.8

[168]

ImageNet

Intel Core(TM)

CaffeNet(Top1 acc): 53.70

i7-4790

Describable

Intel Core(TM)

CaffeNet(Top1 acc): 59.00

Textures

i7-7700

[169]

ImageNet

-

MobileNetV1(Top1/Top5 acc): 49.34/74.52

MobileNetV2(Top1/Top5 acc): 52.86/78.57

[170]

[171]

UCSD

Mobile Robot

Detection

Sim2real

Detection (sim)

Sim2real

Detection (realr)

CIFAR-10

ImageNet

NVIDIA MX250

YOLOv3(mAP): 69.6

YOLOv3(mAP): 92.1

YOLOv3(mAP): 98.0

YOLOv3(mAP): 76.1

-

VGG11/ResNet-14(Top1 acc): 86.30/89.84

ResNet-18(Top1 acc): 59.30

MobileNetV1(Top1 acc): 46.3/69.1

MobileNetV2(Top1/Top5 acc): 70.9/70.0

[172]

ImageNet

Google Pixel 1

[173]

ImageNet

Google Pixel 1

MobileNetV3(Top1 acc): 77.0/78.5

225M/314M

51ms/-

FLOPs

-

5.4×

2.3×

-

↓46.70%/↓48.72%/

↓50.60%/↓51.20%

-

-

50%

60%

4458M

327M

-

-

-

-

13.2MB

14.3MB

-

14.3 ×/11.9×/10.3×

21.4×/19.3×/6.2×

-

-

-

-

-

-

-

-

-

-

6.4×/3.3×

-

-

1.17M

2.41M

-

-

4.685M

0.299M

2.52ms/3.39ms/2.98ms

2.32ms/3.28ms/2.97ms

2.98ms/3.91ms/3.63ms

3.07ms/3.70ms/3.50ms

10ms/13ms

15ms/17ms

-

16.0FPS

14.6FPS

-

-

-

-

69.7ms/batch

58.2ms/batch

-

16ms

3ms

1581M

8ms

2.545M

-

-

1.4×/1.2×

1.4×

6.01ms/74.9ms

61.6ms/53.5ms

-

-

-

0.94MB/0.73MB

4.36MB

-

-

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

24

TABLE V: Summary of Automated Pruning Studies (cont.).

Method

Reference

Dataset

Hardware

Accuracy

FLOPs

Latency

# of Parameters

Memory

ImageNet

Galaxy S9

ResNet18/ResNet34/ResNet50(Top1 acc): 67.56/72.77/74.45

1030M/2130M/1830M

190ms/310ms/310ms

[174]

[175]

CIFAR-100

CIFAR-10

CIFAR-10

CIFAR-100

SVHN

[24]

ImageNet

[176]

CIFAR-10

Tiny-

ImageNet

CIFAR-10

[178]

CIFAR-100

ImageNet

CIFAR-10

CIFRA-100

[179]

g
n
i
n
u
r
P

d
e
t
a
m
o
t
u
A

-

-

-

-

-

-

[180]

MS COCO

NVIDIA V100

ResNet18/ResNet34/ResNet50(Top5 acc): 87.90/90.99/92.21

-

ResNet32/ResNet56/ResNet110(acc): 72.20/72.57/74.69

39.4M/58.4M/114M

ResNet20/ResNet32/ResNet56/ResNet110/MobileNetV1(acc):

19.1M/31.9M/36.4M/

91.87/93.49/93.53/95.02/91.87

101M/115M

VGGNet/DenseNet40/ResNet164(error): 6.20/5.65/5.27

391M/240M/275M

VGGNet/DenseNet40/ResNet164(error): 26.52/25.72/23.91

501M/281M/247M

VGGNet/DenseNet40/ResNet164(error): 2.06/1.81/1.81

398M/267M/225M

MobileNetV2(Top1 acc): 66.83/73.32/74.0

102M/209M/305M

ResNet20/ResNet56/ResNet110/

51.80%/39.07%/21.63%/

ResNet164/DenseNet-12-40(Top1 error):

21.78%/29.52%

8.46/7.06/6.61/6.30/6.51

MobileNetV1/MobileNetV2(Top1 error): 51.63/52.43

51.91%/11.92%

ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc):

22.4M/35.0M/59.5M/

92.88/93.16/93.69/94.33/94.00

119M/178M

ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc):

22.4M/42.5M/61.2M/

68.90/72.41/72.25/73.16/77.76

120M/171M

ResNet18/ResNet50(Top1 acc): 69.15/76.20

ResNet18/ResNet50(Top5 acc): 89.19/93.07

ResNet164(test error): 4.58/5.01

ResNet164(test error): 21.63/22.38

(AP/AP.5/AP.7/APS /APM/APL ):
42.3/62.6/46.2/26.2/45.1/50.6

43.9/63.8/47.9/27.0/46.8/52.8

50.7/69.6/55.4/31.3/53.8/64.0

1210M/2310M

190M/133M

178M/123M

-

-

-

-

-

-

-

25.4 FPS

23.3 FPS

10.1 FPS

2.30M/0.35M/1.10M

5.00M/0.46M/1.21M

3.04M/0.44M/1.12M

-

56.13%/41.10%/22.40%/

20.46%/26.01%

36.95%/6.50%

-

-

-

-

-

-

-

-

-

-

network architecture [187], [188], [189], [190], [191], [192].
Given a network to be quantized, a supernet is ﬁrst constructed
by inserting multiple parallel quantization operations into
every pair of adjacent layers of the network. Each inserted
operation constitutes an edge from the i-th layer to the (i + 1)-
th layer and is associated with a learnable parameter. Multiple
parallel edges/operations on the same level are of different
quantization bitwidths and summarized by assembling all
edges with the weights derived from the learnable parameters.
Then, the network parameters and the quantization weights are
jointly trained with the combination of accuracy and model
size as the target. Wu et al. [192] and Xu et al. [189] simply
try to diminish the model size while Yu et al. [188] and
Wei et al. [191] consider hardware constraints in the training
target. SSPS [190] further borrows the idea from [23] to use
a one-hot mask instead of continuous weights to select the
bitwidth. In this way, only one path is active during training
so the search cost is considerably reduced. The bitwidth search
space is usually below eight since a previous study has shown
that simple 8-bit post-training quantization is able to achieve
marginal accuracy degradation [193]. We summarize the main
studies of automated quantization in TABLE VI.

V. JOINT AUTOMATED DESIGN STRATEGIES

Neural architecture design and compression are divergent
routines to achieve efﬁcient deep learning models and a com-
bination of these two strategies is an intuitive way to achieve
even more efﬁciency than merely relying on either technique

individually. The most natural way in this regard is a sequential
pipeline: design then compress. Under the context of design
automation, the pipeline is commonly recognized as a search-
compress scheme. There are bare studies on combining tensor
decomposition and NAS. We will thus concentrate on the
other three compression techniques in this section. TABLE
VII summarizes main references on joint automated design
work.

A. Joint Search and Knowledge Distillation

For knowledge distillation,

the intuitive joint automated
design strategy is to ﬁrst search for a powerful teacher model
and then achieve a compressed student model by automated
knowledge distillation. An improved strategy is FasterSeg [90],
which searches for student and teacher alternatively so as to
ensure optimal teacher-student matching pairs and effective
knowledge distillation. Instead of searching for the entire
teacher model, Guan et al. [194] target the feature distillation
and search for the optimal teacher’s feature aggregation for
distillation. For each layer group in the teacher model, the
authors try to search for the aggregation weights of each
layer’s feature in that group; then the features are weighted
sum to guide the knowledge distillation of the corresponding
student layer. This work mimics the multi-teacher distillation
scheme but with a single-teacher, multi-feature distillation
proposal and automated teacher knowledge aggregation.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

25

TABLE VI: Summary of Automated Quantization Studies. The “hardware” column indicates on which the achieved models are
evaluated. The model name (e.g., MobileNetV1) in the “Accuracy” column indicates the original model before quantization.
The latency is reported per input. n× indicates that the original model is n times its quantized counterpart. ‘-’ indicates
unavailable records.

Method

Reference

Dataset

Hardware

Accuracy

Bitwdith

Latency

Model Size

MobileNetV1 (Top1 acc): 67.40/70.58/71.20

BISMO on

MobileNetV1 (Top5 acc): 87.90/89.77/90.19

Xilinx Zynq-7020

MobileNetV2 (Top1 acc): 66.99/70.90/71.89

MobileNetV2 (Top5 acc): 87.33/89.91/80.36

MobileNetV1 (Top1 acc): 65.33/69.97/71.20

BISMO on

MobileNetV1 (Top5 acc): 86.60/89.37/90.08

Xilinx VU9P

MobileNetV2 (Top1 acc): 67.01/69.45/71.85

n
o
i
t
a
z
i
t
n
a
u
Q
d
e
t
a
m
o
t
u
A

[25], [186]

ImageNet

BitFusion

-

-

-

-

-

-

-

[187]

[188]

[189]

[190]

MNIST

ImageNet

CIFAR-10

ImageNet

Penn Treebank
Corpus

Conversational
Telephone
Speech

CIFAR-10

ImageNet

COCO

[191]

NWPU-RESISC45

[192]

CIFAR-10

ImageNet

MobileNetV2 (Top5 acc): 87.46/88.94/90.24

MobileNetV1 (Top1 acc): 67.45/70.40/70.90

MobileNetV1 (Top5 acc): 87.85/89.69/89.95

MobileNetV1 (Top1 acc): 57.14/67.66/71.74

MobileNetV1 (Top5 acc): 81.87/88.21/90.36

MobileNetV2 (Top1 acc): 66.75/70.90/71.47

MobileNetV2 (Top5 acc): 87.32/89.76/90.23

ResNet50 (Top1 acc): 70.63/75.30/76.14

ResNet50 (Top5 acc): 89.93/92.45/92.89

LeNet5 (Val. error): 1.14/1.69/2.32

AlexNet (Val. error): 47.46/47.69/48.54

-

-

ResNet20 (Top1 acc): 92.30/92.12/92.04

Avg: 3.5/3.3/2.9

ResNet50 (Top1 acc): 76.67/75.71

ResNet50 (Top5 acc): 93.55/92.83

Avg: 3.8/2.9

Transformer (Perplexity): 56.82/58.23

Avg: 2.0/2.2

Transformer (Perplexity): 42.39/42.75

Avg: 1.9/2.5

ResNet20 (Top1 acc): 92.54

Avg: 3.04

ResNet18/ResNet34/ResNet50/MobileNetV2 (Top1 acc):
70.70/74.30/76.22/69.10

Avg:
3.95/4.01/3.98/4.02

Faster R-CNN (AP/AP.5/AP.75/APS/APM/APL)
37.4/58.1/40.6/22.1/40.4/47.9

RetinaNet (AP/AP.5/AP.75/APS/APM/APL)
36.4/55.8/38.6/20.8/39.9/47.6

ResNet34 (acc): 92.66/92.75/92.73

SqueezeNet (acc): 88.45/88.59/88.61

ResNet20/ResNet56/ResNet110 (acc): 92.00/94.12/94.39

ResNet18/ResNet34 (acc): 69.58/73.37

Avg: 4.00

Avg: 4.00

-

-

45.51ms/57.70ms/70.35ms

52.12ms/66.92ms/82.34ms

57.40ms/77.49ms/99.66ms

-

73.97ms/99.07ms/127.03ms

7.86ms/11.09ms/19.98ms

-

-

-

-

-

-

-

1.09MB/1.58MB/2.07MB

0.95MB/1.38MB/1.79MB

6.30MB/9.22MB/12.14MB

-

10.19×/10.74×/12.08×

-

6.5MB/4.8MB

8.0MB/9.1MB

-

13.17MB/16.50MB/18.86MB

0.45MB/0.55MB/0.69MB

16.6×/18.93×/20.3×

21.1×/19.0×

B. Joint Search and Pruning

Different from the search-compress scheme, pruning can
be incorporated into neural architecture search as integrity.
The aforementioned automated pruning approaches (in Section
IV-C) decide the pruning strategy based on the weights of
trained networks. However, a recent study [195] indicates that
the essence of network pruning is the pruned structure instead
of the weights inherited from the base network, which means
that the pruning strategy can be determined before the model
is trained. In light of this critical conclusion, the research
direction turns to ﬁnding optimal pruned structures, e.g., the
channel number in each layer, rather than to determining

important channels from the base network. This objective
is basically the same as NAS so numerous works emerge
to use NAS to achieve pruning, where the pruning strategy
and network structure are sampled together before training
in the search process [196], [197], [198], [199], [200], [201].
However, different from pure NAS, the NAS-based pruning 1)
requires an existing state-of-the-art model as the pruning base,
2) only searches the network hyperparameters (e.g., channel
number, layer number) instead of the type of operators, and 3)
is bounded by searching smaller hyperparameters than the base
network. MetaPruning [196] is an early work in this direction,
which attempts to ﬁnd the optimal number of channels for
each layer. It ﬁrst constructs a PruningNet to generate weights

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

26

for pruned networks so that ﬁne-tuning is not required during
search time; then it utilizes an evolutionary algorithm to search
for the optimal combination of layers with different numbers
of channels. The search procedure is highly efﬁcient and no
ﬁne-tuning is obliged after searching. However, the search
space is large especially when there are many channels in the
base network. Some following works thus attempt to shrink
the search space by only searching channel number ratios
[198] or using a clustering algorithm to cluster channels in
each layer as the initial number of channels in the pruned
network for later search [199]. Other works [200], [201], on
the other hand, seek to develop a performance estimator to
accelerate the searching process. Overall, most NAS-based
pruning process is analogous to searching optimal pruning
rates but without pruning low magnitude channels at each
iteration,
trains from scratch with the searched
structure or quickly derives the performance with an additional
performance estimator.

instead,

it

Alternative to unifying NAS and pruning by sampling
the pruning strategy and model architecture before training,
SpArSe [202] selects the pruning strategy based on a trained
model but optimizes the pruning hyperparameters and model
architecture jointly. Speciﬁcally, it ﬁrst samples a conﬁguration
of the model architecture (e.g., operators and connectivity) and
hyperparameters of pruning algorithms (i.e., Sparse Variational
Dropout (SpVD) and Bayesian Compression (BC)); it then
trains the model and prunes it. After evaluation, it optimizes
the sampling conﬁguration with multi-objective Bayesian opti-
mization. In this way, both structured and unstructured pruning
strategies can be optimized together with the model architec-
ture.

C. Joint Search and Quantization

1) Optimizing Search and quantization separately: Some
researchers consider optimizing the two components sepa-
rately and particularly focus on automating the architecture
design process. For example, Cai et al. use the ProxylessNAS
[23] framework to ﬁrstly search for an efﬁcient architecture
and then perform quantization-aware ﬁne-tuning to further
compress the model [203]; Liu et al. [204] include the
quantization-friendly activation function ReLU6 in the search
space to facilitate the following quantization; Peter et al.
[205] compare the performance of post-training quantization
(PTQ) and quantization-aware training (QAT) and indicate
that the PTQ performance drops rapidly when the bitwidth
is smaller than 4 while QAT only has marginal drop even
with 1-bit precision. This is due to the incompatibility between
quantization and the network structure, which is primarily
optimized for full precision.

In light of

this, attentions are drawn to quantization-
aware NAS, where the search space is based on quantized
operators [206], [207], [208], [209], [210]. Most of these
works are for binary neural networks (BNNs) because of
their extreme computation and memory savings [206], [207],
[208], [209], [211]. Shen et al. [209] apply an evolutionary
search to optimize the layer-wise channel expansion ratio
for binarized VGG and ResNet. Phan et al. [207] discover

that binary DWConv has limited representation capability
and thus attempt to search for the group number for each
layer of the MobileNet via an evolutionary algorithm as well.
BATS [206] and BNAS [208] concurrently emerge in the
sense of differentiable quantization-aware NAS. They both
design a BNNs-oriented search space discarding the separable
convolution due to its failure during quantization; BNAS keeps
convolutions and dilated convolutions, and BATS keeps group
convolutions and dilated group convolutions. It is interesting to
note that the BATS binarizes only activations during training
but keeps weights at full precision and binarizes the weights
after training with a marginal drop in accuracy (∼1%). Instead
of optimizing the architecture as a whole, Xu et al. [211] probe
layer-wise searching under the KD guidance for BNN object
detectors.

The above work only automatically optimizes the neural
architectures while using ﬁxed quantization policies. As a
result, the ﬁnal model has a large possibility to be sub-optimal:
e.g., the BNN with the searched architecture is not necessarily
smaller and more accurate than a mixed-precision model.

2) Optimizing search and quantization jointly: Jointly op-
timizing the neural architecture and quantization policy can
ﬁnd the best combination of both worlds. The most intuitive
joint approach is to construct a new search space that contains
both structure and bitwidth choices, and the search process is
the same as for traditional hardware-aware NAS [212], [213].
For example, JASQ [212] employs a classical evolutionary
search algorithm to explore both operators and cell-wise
quantization bitwidths; Gong et al. [213],
instead, engage
differentiable neural architecture search to explore a mixed-
precision supernet of MBConv hyperparameters and bitwidths.
Though simple and effective, this strategy is inefﬁcient: the
search space is enlarged by many times that JASQ requires
three GPU days to search for a suitable model for each given
resource budget.

To tackle this issue, some studies decouple the supernet
training and search process so that no training is required in the
search process [105], [214], [215]. The weight-sharing training
paradigm is further adopted to reduce the training cost. SPOS
[105] designs a single path weight-sharing strategy that trains a
supernet independently to the search process. In each iteration
of the supernet training, only one single path of the supernet is
activated and trained. The single-path activation is realized by
uniformly sampling one candidate block at each block level of
the supernet. The weights of the same block are shared among
different supernet training iterations (i.e., different paths). In
the search process, an evolutionary algorithm is performed on
randomly sampled paths of the pre-trained supernet to ﬁnd the
optimal combination of architecture and block-wise bitwidths.
As the supernet is well pre-trained, each architecture only
performs inference during the evolutionary search. This SPOS
strategy improves the efﬁciency of both the supernet training
and search. APQ [214] adopts the once-for-all [77] supernet
training strategy, which requires little time for retraining the
searched model but is inefﬁcient to support different bitwidths
in the supernet. Therefore, the authors devise a quantization-
aware accuracy predictor so that they can train the once-for-all
supernet in full precision and derive the accuracy of a quan-

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

27

tized network via the accuracy predictor. However, it is hard
and time-consuming to collect a large volume of [quantized
network, accuracy] pairs for preparing the quantization-aware
accuracy predictor due to lengthy quantization-aware training.
To alleviate this issue, the authors ﬁrst train a full precision
accuracy predictor with abundant [full-precision network, ac-
curacy] pairs from the supernet, and then ﬁne-tune it with
only a few [quantized network, accuracy] pairs to obtain its
quantization-aware counterpart. Next, they then do a simple
evolutionary search with the accuracy predictor to achieve a
new model for a given resource budget. Instead of using the
accuracy predictor to deal with the different bitwidths problem,
Shen et al. [215] conceive the once quantization-aware (OQA)
training strategy, where a set of quantized supernets with
sequential bitwidths are created via the bit inheritance scheme:
the k − 1 bit supernet uses double quantization step size of
its k bit counterpart. The authors experimentally show that
with just one epoch ﬁne-tuning the lower-bit network with
bit inheritance outperforms its counterpart with QAT from
scratch. The ﬁnal architecture is derived by a simple coarse-
to-ﬁne architecture selection procedure without any retraining.
However, OQA only supports ﬁxed precision quantization
policies as well.

The most recent work, QFA [183], manages to allow joint
mixed-precision quantization and architecture search without
retraining. The authors propose a new quantization strategy,
batch quantization, to stabilize the estimation of the scale
factor so the mixed-precision supernet can be stably trained.
They replace the progressive shrinking training strategy in the
work [215] with a newly proposed two-stage training strategy
reducing more than half training epochs.

VI. FUTURE DIRECTIONS

The design automation for efﬁcient deep learning models is
an exciting area yet still in its infant research phase. Existing
work has covered almost its every research aspect and laid
ﬁrm foundations for its future evolution. In this section, we
discuss several future directions that are worthy to delve into.

A. Beyond Computer Vision

Most current works validate their performance on computer
vision (CV) tasks, e.g., image classiﬁcation and object de-
tection. This is ascribable to two reasons: 1) the CV area is
the most developed area regarding deep learning research that
abundant resources are available and experimental protocol
is well standardized;
thus it requires only bare efforts to
prepare comparison benchmarks; 2) the CV models are mainly
based on the CNN family and much manual engineering
has been devoted to devising efﬁcient CNN models so it is
relatively easy to establish an efﬁcient search space or base
models for compression. Nevertheless, CV is not the only ﬁeld
that expects efﬁcient deep learning models; applications like
IoT are more resource-constrained but seldom investigated
regarding the design automation for efﬁcient deep learning.
Since these domains have dissimilar characteristics to CV,
speciﬁc research is desired to go beyond the CV. Though
some works have already been presented, like [146], [149],

[189] on automatically compressing language models, these
are very limited and more room for different applications and
the deeper investigation remains to be explored.

B. Accelerating Search

this is still

While searched models are driven to be efﬁcient, the search
process is often time- and computation-intensive because vast
numbers of candidate models need to be evaluated before
choosing the best one. This is problematic as the search
process is task- and/or hardware-speciﬁc and thus a new search
is required when a new task or hardware arises. Accelerating
the search process can not only facilitate the task/hardware
adaptation but also save the energy consumed during the
search. The weight-sharing paradigm [124], [54] can alleviate
too slow, especially for the
this problem but
hardware-aware settings. Using proxies is another strategy to
accelerate the search process, which approximates different
models’ performance without fully training them [117]. Most
recently, zero-cost proxies for NAS have attained great interest
and shown outstanding performance on some NAS bench-
marks [216]. This strategy aims to rank network architectures
without any training or even without seeing data [217], [216].
Though remarkable efforts have been made in accelerating
conventional NAS, how these approaches can be efﬁciently
and effectively applied to the hardware-aware NAS and au-
tomated compression is still an open question. For example,
which proxies can be used for automated quantization? Or can
we use the same proxies for different quantization bitwidths?
Another direction for accelerating the search is to reduce
the search space. For example, EfﬁcientMet [68] ﬁxes a
baseline model and only searches the constant scale ratios of
width/depth/resolution. Only a small grid search can produce
extraordinary performance. An effective and reduced search
space is based on extensive investigation and human knowl-
edge and thus demands considerable effort in this direction.

C. Hardware Simulation

The feedback (e.g., latency and memory usage) from target
devices is a necessity for exploring efﬁcient deep learning
models automatically. However, running an enormous amount
of models on target devices is labour- and time-intensive,
especially when the devices are general-purpose computing
systems (e.g., MCU) instead of speciﬁc NN accelerators (e.g.,
TPUs). Furthermore,
the frequent communication between
the target devices and the devices, on which the design
program runs, takes even more time. Therefore, it is desirable
to have an accurate hardware simulator that simulates the
behaviour of DNN hardware implementations. This simulator
would accelerate the automated design process by running all
models and obtaining all feedback on a powerful server. In
addition, with a hardware simulator, beginners do not need to
have cross-disciplinary knowledge in the hardware setting and
compilation to collect hardware-cost data; the design process
becomes easy to get started with. Although some works have
reported hardware simulators like [218], [219], the precision
and hardware information that can be provided are far from
satisfactory. It is also attractive to have a general software

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

28

TABLE VII: Summary of Joint Automated Design Strategies. The “hardware” column indicates on which the achieved models
are evaluated. The model name (e.g., MobileNetV1) in the “Accuracy” column indicates the base model used for search; if
the original model is not speciﬁed, it is searched during training. The latency is reported per input; otherwise is speciﬁed in
the relevant table cells (e.g., FPS). ↓ n% indicates the compressed model decreases n per cent compared to its original model.
∼ indicates the exact data is not reported in the paper and thus estimated from the reported ﬁgures. ‘-’ indicates unavailable
records.

Method

Reference

Dataset

Hardware

Accuracy

FLOPs

Latency

# of Parameters

Memory

[194]

[90]

-

CIFAR100

CINIC-10

Cityscapes

CamVid

Nvidia Geforce

GTX1080Ti

BDD

(Acc): 79.74/78.18/75.85

(Acc): 85.41/79.45/79.38

(mIoU): 71.5

(mIoU): 71.1

(mIoU): 55.1

-

-

[196]

ImageNet

65.0/63.8/58.3

-

MobileNetV2(Top1 acc): 72.7/68.2/67.3/

291M/140M/124M/

105M/84M/43M

MobileNetV1(Top1 acc): 70.9/66.1/57.2

324M/149M/41M

-

163.9 FPS

398.1 FPS

318.0 FPS

-

2.77M/1.47M/0.7M

1.27M/0.36M/0.36M

-

-

[197]

[198]

[199]

[200]

[201]

ResNet50(Top1 error/Top5 error): 76.2/75.4/73.4

3000M/2000M/1000M

NVIDIA Titan

MobileNetV1(Top1 acc): 71.0/67.4/59.6

Xp

-

0.48ms/0.30ms/0.17ms

MobileNetV2(Top1 acc): 73.2/71.7/64.5

0.67ms/0.47ms/0.29ms

CIFRA-10

VGG16/ResNet18(acc): 91.62/94.37

↓85.04%/↓41.15%

↓59.56%/↓21.3%

↓94.46%/↓61.71%

CIFAR-100

NVIDIA

VGG16/ResNet34(acc): 69.42/74.59

↓69.03%/↓56.91%

↓49.4%/↓27.91%

↓81.14%/↓69.94%

GeForce RTX

2080 Ti

Tiny-

ImageNet

CIFAR-10

ImageNet

CIFAR-10

CIFAR-100

ImageNet

PASCAL

VOC

CIFAR-10

ResNet18/ResNet34(acc): 56.87/58.64

↓50.16%/↓61.89%

↓22.15%/↓27.12%

↓24.67%/↓31.58%

VGGNet16/GoogLeNet/ResNet56/ResNet110(Top1 acc):

93.08/94.84/93.23/93.58

82.81M/513.19M/

58.54M/89.87M

ResNet18/ResNet34/ResNet50/ResNet101/ResNet152(Top1 acc):

968.13M/2170.77M/1890.60M/

67.80/70.98/73.86/75.82/77.12

3164.91M/4309.52M

ResNet56/ResNet110(acc): 93.39/93.65

↓54.43%/↓78.32%

ResNet56(acc): 71.15

ResNet34(Top1/Top5 acc): ↓1.32/↓0.87

ResNet50(Top1/Top5 acc): ↓0.89/↓0.53

↓52.21

↓55.73%

↓63.34%

SSD(mAP): 74.92/74.80/73.73

↓35.85%/↓49.51%/↓66.44%

VGG16/GoogLeNet(acc): 93.78/93.18

81.19M/680M

-

-

-

-

-

-

1.67M/2.46M/0.39M/0.56M

9.50M/10.12M/11.75M/

17.72M/24.07M

↓58.82%/↓80.92%

-

↓58.07%

↓64.36%

↓35.41%/↓50.55%/↓61.13%

1.64M/2.76M

Carvana

NVIDIA Tesla

Unet(Dice score): 0.9944/0.9896/0.9926

81110M/162230M/237260M

190ms/210ms/220ms

4.3M/5.88M/12.74M

P40

-

CIFAR-10

ImageNet

MNIST

[202]

CIFAR10-

STM32F413

binary

MCUs

CUReT-

binary

Chars4K-

binary

VGG16/ResNet56/ResNet110(acc): 93.57/93.31/93.56

↓70%/↓50%/↓65%

ResNet50/MobileNetV2(Top1 acc): 75.46/71.1

↓51.7%/↓30.2%

(acc): 96.97/95.76

(acc): 73.4/70.48

(acc): 73.22

(acc): 74.87

-

-

285.82ms/27.06ms

2529.84ms/298.57ms

-

-

-

-

-

-

-

-

-

-

1.32KB/0.71KB

2.4KB/2.12KB

2.06KB

1.87KB

Reference

Dataset

Hardware

Accuracy

Bitwdith

[203]

ImageNet

Google Pixel 2

ProxylessNAS(Top1 acc): 69.2

[204]

ImageNet

Cortex-A72

ARM

Top-1 acc: 75.1

Coral USB

TPU

Top-1 acc: 76.3

Nvidia 2080Ti

Top1 acc: 76.3

[205]

[206]

[207]

Google

Speech

ImageNet

ImageNet

-

-

-

acc: 96

Top1/Top5 acc: 66.1/87.0

Top1/Top5 acc: 60.90/82.60

8

8

8

32

7

1

1

103.67ms

77.89ms

Latency

35ms

∼100ms

∼5ms

22ms

-

-

-

FLOPs

Model Size

-

-

19.6M

121M

154M

-

-

-

-

-

D
K
d
n
a

h
c
r
a
e
S

t
n
i
o
J

g
n
i
n
u
r
P

d
n
a

h
c
r
a
e
S

t
n
i
o
J

n
o
i
t
a
z
i
t
n
a
u
Q
d
n
a

h
c
r
a
e
S

t
n
i
o
J

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

29

TABLE VII: Summary of Joint Automated Design Strategies (cont.).

Method

Reference

Dataset

Hardware

Accuracy

Bitwdith

Latency

[208]

[209]

ImageNet

CIFAR10

ImageNet

CIFAR10

-

-

[210]

ImageNet

AMD Ryzen Threadripper 3960X

Top1/Top5 acc: 63.51/83.91

Top1 acc: 94.43

ResNet18(Top1/Top5 acc): 69.65/89.08

VGG-small(Top1 acc): 93.06

Top1 acc: 77.8/77.0

Top1 acc: 69.8/67.7

-

-

ResNet18/ResNet34/ResNet50(mAP): 73.2/75.8/76.9

error: 27.22/34.10

error: 2.90/2.97

Top1/Top5 error: 31.62/11.56

NVIDIA Tesla P100 GPU

Top1/Top5 error: 28.23/9.94

1

1

32/8

1

-

-

-

297ms/100ms

151ms/53ms

-

-

mixed

21.19ms

24.71ms

FLOPs

90M

656M

660M

59.3M

717M

141M

Model Size

-

-

-

18.49G/21.49G/21.95G

16.61MB/24.68MB/29.61MB

-

-

[105]

ImageNet

-

error: 22.16/21.27

2.53ms/2.98ms

ResNet18(Top1 acc): 66.4/69.4/7

mixed

-

BitOps: 6.21G/13.49G/24.31G

ResNet34(Top1 acc): 71.5/73.9/74.6

BitOps: 13.11G/28.78G/51.92G

BitFusion

Top1 acc: 75.1/74.1

mixed

12.17ms/8.40ms

BitOps: 16.5G/23.6G

-

-

STM32F746

STM32F412

STM32F746

STM32F765

STM32H743

Top1 acc: 61.7/71.3/74.1

Top1 acc: ∼71.5

Top1 acc: 49.9/40.5

Top1 acc: 62.0

Top1 acc: 63.5

Top1 acc: 65.9

Top1 acc: 70.7

mAP: 51.4

2/3/4

mixed

-

-

5FPS/10FPS

-

8

4

8

BitOPs: 1.21G/3.07G/9.28G

∼200M

-

Pascal VOC

STM32H743

168M

peak SRAM: 466KB

n
o
i
t
a
z
i
t
n
a
u
Q
d
n
a

h
c
r
a
e
S

t
n
i
o
J

[211]

PASCAL VOC

[212]

[213]

ImageNet

CIFAR10

ImgeNet

CIFAR100

[214]

[215]

[183]

ImageNet

ImageNet

ImageNet

[46]

ImageNet

4.9MB/2.5MB

2.5MB/0.9MB

1.44MB

2.06MB

0.6MB/0.8MB

-

-

-

-

-

platform that offers the simulation of deploying DNNs on
diverse and conﬁgurable devices.

D. Benchmarking

Despite tremendous results that have been reported to
achieve state-of-the-art performance, most of these works are
not based on the same protocol
thus resulting in skewed
comparison. First, some literature uses direct evaluation met-
rics while others use indirect ones. Superiority on indirect
metrics does not necessarily mean more efﬁciency on real
devices. Second, the comparison results can vary on different
hardware or different
implementation details on the same
hardware. Finally, the dataset/task settings also inﬂuence the
performance signiﬁcantly. For example, even using the same
dataset, different
input resolutions would lead to different
hardware consumption. It is thus imperative to benchmark
these evaluation settings to resolve unfair comparisons and
make this domain more reproducible. In addition, a uniﬁed
benchmark dataset can also facilitate the comparison of design
automation algorithms. Li et al. [220] recently develop a
comprehensive benchmark dataset of hardware-aware NAS on
three categories of hardware (e.g., commercial edge devices,
FPGA, and ASIC). However, this benchmark dataset primarily
aims to facilitate the development of search strategies, and
therefore is relatively small and based on only two spaces.
There so far lacks well-recognized benchmark settings and a
thorough dataset in this domain.

Apart from the above less noticed and underdeveloped
directions, other directions like developing novel efﬁcient
operators and competent design algorithms have been widely
researched, but there is still large room to improve.

VII. CONCLUSION

This survey provides a thorough review of the design
automation techniques for fast, lightweight and effective deep
learning models. We analyze and summarize current studies
into three categories: 1) by searching, 2) by compressing,
and 3) by jointly. In addition, we conclude the evaluation
metrics speciﬁc to efﬁcient deep learning models. At the end
of this work, we afford discussion of existing issues and future
directions for both novices and experienced researchers.

REFERENCES

[1] S. Zhang, L. Yao, A. Sun, and Y. Tay, “Deep learning based rec-
ommender system: A survey and new perspectives,” ACM Computing
Surveys, vol. 52, no. 1, pp. 5:1–5:38, 2019.

[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: pre-
training of deep bidirectional transformers for language understanding,”
in NAACL-HLT, 2019, pp. 4171–4186.

[3] D. Zhang, K. Chen, D. Jian, L. Yao, S. Wang, and P. Li, “Learning
attentional temporal cues of brainwaves with spatial embedding for
motion intent detection,” in ICDM, 2019, pp. 1450–1455.

[4] D. Chen and H. Zhao, “Data security and privacy protection issues in

cloud computing,” in ICCSEE, 2012, pp. 647–651.

[5] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-
erations for deep learning in NLP,” in ACL, 2019, pp. 3645–3650.
[6] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,”
Communications of the ACM, vol. 63, no. 12, pp. 54–63, 2020.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

30

[7] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas,
“Predicting parameters in deep learning,” in NIPS, 2013, pp. 2148–
2156.

[8] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in

NIPS, 1989, pp. 598–605.

[9] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, pp. 1–9, 2017.

[10] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in CVPR,
2018, pp. 4510–4520.

[11] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely
efﬁcient convolutional neural network for mobile devices,” in CVPR,
2018, pp. 6848–6856.

[12] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufﬂenet v2: Practical
guidelines for efﬁcient cnn architecture design,” in ECCV, 2018, pp.
122–138.

[13] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very

deep neural networks,” in ICCV, 2017, pp. 1398–1406.

[14] J. Luo, J. Wu, and W. Lin, “Thinet: A ﬁlter level pruning method for

deep neural network compression,” in ICCV, 2017, pp. 5068–5076.

[15] M. Courbariaux, Y. Bengio, and J.-P. David, “Training deep neural
networks with low precision multiplications,” in ICLR Workshop, 2015,
pp. 1–10.

[16] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas im-
manent in nervous activity,” The Bulletin of Mathematical Biophysics,
vol. 5, no. 4, pp. 115–133, 1943.

[17] T. M. Bartol, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M.
Harris, and T. J. Sejnowski, “Hippocampal spine head sizes are highly
precise,” bioRxiv, p. 016329, 2015.

[18] D. J. Linden, Think Tank: Forty Neuroscientists Explore the Biological

Roots of Human Experience. Yale University Press, 2018.

[19] D. Lin, S. Talathi, and S. Annapureddy, “Fixed point quantization of
deep convolutional networks,” in ICML, 2016, pp. 2849–2858.
[20] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep
learning with limited numerical precision,” in ICML, 2015, pp. 1737–
1746.

[21] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” in NIPS Workshop, 2014, pp. 1–9.

[22] J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu,
“Learning compact recurrent neural networks with block-term tensor
decomposition,” in CVPR, 2018, pp. 9378–9387.

[23] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture
search on target task and hardware,” in ICLR, 2019, pp. 1–11.
[24] X. Xiao, Z. Wang, and S. Rajasekaran, “Autoprune: Automatic network
pruning by regularizing auxiliary parameters,” in NeurIPS, 2019, pp.
13 681–13 691.

[25] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “HAQ: hardware-aware
automated quantization with mixed precision,” in CVPR, 2019, pp.
8612–8620.

[26] Y. He, J. Lin, Z. Liu, H. Wang, L. Li, and S. Han, “AMC: automl
for model compression and acceleration on mobile devices,” in ECCV,
2018, pp. 815–832.

[27] D. Langerman, A. Johnson, K. Buettner, and A. D. George, “Beyond
ﬂoating-point ops: CNN performance prediction with critical datapath
length,” in HPEC, 2020, pp. 1–9.

[28] S. Yao, Y. Zhao, H. Shao, S. Liu, D. Liu, L. Su, and T. Abdelzaher,
“Fastdeepiot: Towards understanding and optimizing neural network
execution time on mobile and embedded devices,” in SenSys, 2018,
pp. 278–291.

[29] L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, and Q. Tian,
“Condensenet v2: Sparse feature reactivation for deep networks,” in
CVPR, 2021, pp. 3569–3578.

[30] V. Camus, C. Enz, and M. Verhelst, “Survey of precision-scalable
multiply-accumulate units for neural-network processing,” in AICAS,
2019, pp. 57–61.

[31] G. Chu, O. Arikan, G. Bender, W. Wang, A. Brighton, P.-J. Kinder-
mans, H. Liu, B. Akin, S. Gupta, and A. Howard, “Discovering multi-
hardware mobile models via architecture search,” in CVPR Workshops,
2021, pp. 3022–3031.

[32] S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark
analysis of representative deep neural network architectures,” IEEE
Access, vol. 6, pp. 64 270–64 277, 2018.

[33] C. Zhengbo, T. Lei, and C. Zuoning, “Research and design of activation
function hardware implementation methods,” in Journal of Physics:
Conference Series, vol. 1684, no. 1, 2020, p. 012111.

[34] N. Dryden, N. Maruyama, T. Benson, T. Moon, M. Snir, and B. Van Es-
sen, “Improving strong-scaling of cnn training by exploiting ﬁner-
grained parallelism,” in IPDPS, 2019, pp. 210–220.

[35] W. Niu, M. Sun, Z. Li, J. Chen, J. Guan, X. Shen, Y. Wang, S. Liu,
X. Lin, and B. Ren, “Rt3d: Achieving real-time execution of 3d
convolutional neural networks on mobile devices,” in AAAI, 2021, pp.
9179–9187.

[36] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catan-
zaro, and E. Shelhamer, “cudnn: Efﬁcient primitives for deep learning,”
arXiv preprint arXiv:1410.0759, pp. 1–9, 2014.

[37] E. Liberis and N. D. Lane, “Neural networks on microcontrollers:
saving memory at inference via operator reordering,” arXiv preprint
arXiv:1910.05110, pp. 1–8, 2019.

[38] N. Suda and D. Loh, “Machine learning on arm cortex-m microcon-

trollers,” Arm Ltd.: Cambridge, UK, 2019.

[39] K. Siu, D. M. Stuart, M. Mahmoud, and A. Moshovos, “Memory
requirements for convolutional neural network hardware accelerators,”
in IISWC, 2018, pp. 111–121.

[40] J. Xu, K. Ota, and M. Dong, “Saving energy on the edge: In-memory
caching for multi-tier heterogeneous networks,” IEEE Communications
Magazine, vol. 56, no. 5, pp. 102–107, 2018.

[41] H. Unlu, “Efﬁcient neural network deployment for microcontroller,”

arXiv preprint arXiv:2007.01348, pp. 1–6, 2020.

[42] Y. Ioannou, D. Robertson, R. Cipolla, and A. Criminisi, “Deep roots:
Improving cnn efﬁciency with hierarchical ﬁlter groups,” in CVPR,
2017, pp. 1231–1240.

[43] L. Yang, B. Lu, and S. Ren, “A note on latency variability of deep neu-
ral networks for mobile inference,” arXiv preprint arXiv:2003.00138,
pp. 1–5, 2020.

[44] M. Syed and A. A. Srinivasan, “Generalized latency performance
estimation for once-for-all neural architecture search,” arXiv preprint
arXiv:2101.00732, pp. 1–12, 2021.

[45] Q. Wang and X. Chu, “Gpgpu performance estimation with core
and memory frequency scaling,” IEEE Transactions on Parallel and
Distributed Systems, vol. 31, no. 12, pp. 2865–2881, 2020.

[46] J. Lin, W. Chen, Y. Lin, J. Cohn, C. Gan, and S. Han, “Mcunet: Tiny
deep learning on iot devices,” in NeurIPS, 2020, p. 11711–11722.
[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image

recognition,” in CVPR, 2016, pp. 770–778.

[48] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for
mobile,” in CVPR, 2019, pp. 2820–2828.

[49] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable
architectures for scalable image recognition,” in CVPR, 2018, pp.
8697–8710.

[50] X. He, K. Zhao, and X. Chu, “Automl: A survey of the state-of-the-art,”

Knowledge-Based Systems, vol. 212, p. 106622, 2021.

[51] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A
survey on evolutionary neural architecture search,” IEEE Transactions
on Neural Networks and Learning Systems, 2021.

[52] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement

learning,” in ICLR, 2017, pp. 1–11.

[53] X. Dong and Y. Yang, “Searching for a robust neural architecture in

four GPU hours,” in CVPR, 2019, pp. 1761–1770.

[54] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture

search,” in ICLR, 2019, pp. 1–11.

[55] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and
M. Tomizuka, “Squeezesegv3: Spatially-adaptive convolution for ef-
ﬁcient point-cloud segmentation,” in ECCV, 2020, pp. 1–19.

[56] Y. Qiu, Y. Liu, S. Li, and J. Xu, “Miniseg: An extremely minimum
network for efﬁcient covid-19 segmentation,” in AAAI, 2021, pp. 4846–
4854.

[57] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
A survey,” The Journal of Machine Learning Research, vol. 20, pp.
55:1–55:21, 2019.

[58] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu,
“Hierarchical representations for efﬁcient architecture search,” in ICLR,
2018, pp. 1–11.

[59] K. Simonyan and A. Zisserman, “Very deep convolutional networks

for large-scale image recognition,” in ICLR, 2015, pp. 1–10.

[60] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely

connected convolutional networks,” in CVPR, 2017, pp. 2261–2269.

[61] Z. Lu, I. Whalen, V. Boddeti, Y. D. Dhebar, K. Deb, E. D. Goodman,
and W. Banzhaf, “Nsga-net: neural architecture search using multi-
objective genetic algorithm,” in GECCO, 2019, pp. 419–427.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

31

[62] F. Scheidegger, L. Benini, C. Bekas, and A. C. I. Malossi, “Constrained
deep neural network architecture search for iot devices accounting for
hardware calibration,” in NeurIPS, 2019, pp. 6054–6064.

[63] Q. Lu, W. Jiang, X. Xu, Y. Shi, and J. Hu, “On neural architecture
search for resource-constrained hardware platforms,” arXiv preprint
arXiv:1911.00105, pp. 1–8, 2019.

[65] Y. Zhou, S. Ebrahimi, S.

[64] T. Elsken, J. H. Metzen, and F. Hutter, “Efﬁcient multi-objective neural
architecture search via lamarckian evolution,” in ICLR, 2019, pp. 1–11.
¨O. Arık, H. Yu, H. Liu, and G. Diamos,
“Resource-efﬁcient neural architect,” arXiv preprint arXiv:1806.07912,
pp. 1–14, 2018.

[66] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha,
J. Liu, and D. Marculescu, “Single-path NAS: designing hardware-
efﬁcient convnets in less than 4 hours,” in ECML-PKDD, 2019, pp.
481–497.

[67] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely
connected search space for more ﬂexible neural architecture search,”
in CVPR, 2020, pp. 10 625–10 634.

[68] M. Tan and Q. V. Le, “Efﬁcientnet: Rethinking model scaling for
convolutional neural networks,” in ICML, 2019, pp. 6105–6114.
[69] Y. Xiong, H. Liu, S. Gupta, B. Akin, G. Bender, Y. Wang, P. Kin-
dermans, M. Tan, V. Singh, and B. Chen, “Mobiledets: Searching for
object detection architectures for mobile accelerators,” in CVPR, 2021,
pp. 3825–3834.

[70] S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P.
Jouppi, “Searching for fast model families on datacenter accelerators,”
in CVPR, 2021, pp. 8085–8095.

[71] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in

CVPR, 2018, pp. 7132–7141.

[72] A. Howard, R. Pang, H. Adam, Q. V. Le, M. Sandler, B. Chen,
W. Wang, L. Chen, M. Tan, G. Chu, V. Vasudevan, and Y. Zhu,
“Searching for mobilenetv3,” in ICCV, 2019, pp. 1314–1324.

[73] R. Avenash and P. Viswanath, “Semantic segmentation of satellite
images using a modiﬁed CNN with hard-swish activation function,”
in VISIGRAPP, 2019, pp. 413–420.

[74] M. Courbariaux, Y. Bengio, and J. David, “Binaryconnect: Training
deep neural networks with binary weights during propagations,” in
NIPS, 2015, pp. 3123–3131.

[75] J. Yu, P. Jin, H. Liu, G. Bender, P. Kindermans, M. Tan, T. S. Huang,
X. Song, R. Pang, and Q. Le, “Bignas: Scaling up neural architecture
search with big single-stage models,” in ECCV, 2020, pp. 702–717.

[76] X. Chu, B. Zhang, and R. Xu, “Moga: Searching beyond mobilenetv3,”

in ICASSP, 2020, pp. 4042–4046.

[77] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train
one network and specialize it for efﬁcient deployment,” in ICLR, 2020.
[78] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y. Tian,
M. Yu, P. Vajda, and J. E. Gonzalez, “Fbnetv3: Joint architecture-recipe
search using predictor pretraining,” in CVPR, 2021, pp. 16 276–16 285.
[79] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu,
K. Chen, P. Vajda, and J. E. Gonzalez, “Fbnetv2: Differentiable neural
architecture search for spatial and channel dimensions,” in CVPR, 2020,
pp. 12 962–12 971.

[80] B. Chen, G. Ghiasi, H. Liu, T. Lin, D. Kalenichenko, H. Adam, and
Q. V. Le, “Mnasfpn: Learning latency-aware pyramid architecture for
object detection on mobile devices,” in CVPR, 2020, pp. 13 604–13 613.
[81] B. Yan, H. Peng, K. Wu, D. Wang, J. Fu, and H. Lu, “Lighttrack:
Finding lightweight neural networks for object tracking via one-shot
architecture search,” in CVPR, 2021, pp. 15 180–15 189.

[82] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in CVPR, 2017, pp. 5987–
5995.

[83] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in NIPS, 2012, pp. 1106–
1114.

[84] L. Xu, Y. Guan, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang,
and X. Wang, “Vipnas: Efﬁcient video pose estimation via neural
architecture search,” in CVPR, 2021, pp. 16 072–16 081.

[85] J. Dong, A. Cheng, D. Juan, W. Wei, and M. Sun, “Dpp-net: Device-
aware progressive search for pareto-optimal neural architectures,” in
ECCV, 2018, pp. 540–555.

[86] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda,
Y. Jia, and K. Keutzer, “Fbnet: Hardware-aware efﬁcient convnet design
via differentiable neural architecture search,” in CVPR, 2019, pp.
10 734–10 742.

[87] Y. Xiong, R. Mehta, and V. Singh, “Resource constrained neural
network architecture search: Will a submodularity assumption help?”
in ICCV, 2019, pp. 1901–1910.

[88] C.-H. Hsu, S.-H. Chang, J.-H. Liang, H.-P. Chou, C.-H. Liu, S.-C.
Chang, J.-Y. Pan, Y.-T. Chen, W. Wei, and D.-C. Juan, “Monas: Multi-
objective neural architecture search,” arXiv preprint arXiv:1806.10332,
pp. 1–8, 2018.

[89] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger, “Con-
densenet: An efﬁcient densenet using learned group convolutions,” in
CVPR, 2018, pp. 2752–2761.

[90] W. Chen, X. Gong, X. Liu, Q. Zhang, Y. Li, and Z. Wang, “Fasterseg:
Searching for faster real-time semantic segmentation,” in ICLR, 2020,
pp. 1–11.

[91] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Search-
ing efﬁcient 3d architectures with sparse point-voxel convolution,” in
ECCV, 2020, pp. 685–702.

[92] M. Ding, X. Lian, L. Yang, P. Wang, X. Jin, Z. Lu, and P. Luo,
“HR-NAS: searching efﬁcient high-resolution neural architectures with
lightweight transformers,” in CVPR, 2021, pp. 2982–2992.

[93] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in
NIPS, 2017, pp. 5998–6008.

[94] D. R. So, Q. V. Le, and C. Liang, “The evolved transformer,” in ICML,

2019, pp. 5877–5886.

[95] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S. Zagoruyko, “End-to-end object detection with transformers,” in
ECCV, 2020, pp. 213–229.

[96] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in ICLR, 2021, pp. 1–11.
[97] Y. Chen, R. Gao, F. Liu, and D. Zhao, “Modulenet: Knowledge-
inherited neural architecture search,” IEEE Transactions on Cybernet-
ics, pp. 1–11, 2021.

[98] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in CVPR, 2015, pp. 1–9.

[99] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-
ing the inception architecture for computer vision,” in CVPR, 2016, pp.
2818–2826.

[100] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. Fei-Fei,
A. L. Yuille, J. Huang, and K. Murphy, “Progressive neural architecture
search,” in ECCV, 2018, pp. 19–35.

[101] D. Han, J. Kim, and J. Kim, “Deep pyramidal residual networks,” in

CVPR, 2017, pp. 6307–6315.

[102] H. Cai, J. Yang, W. Zhang, S. Han, and Y. Yu, “Path-level network
transformation for efﬁcient architecture search,” in ICML, 2018, pp.
677–686.

[103] S. Xie, A. Kirillov, R. B. Girshick, and K. He, “Exploring randomly
wired neural networks for image recognition,” in ICCV, 2019, pp.
1284–1293.

[104] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-
mization,” Journal of Machine Learning Research, vol. 13, pp. 281–
305, 2012.

[105] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single
path one-shot neural architecture search with uniform sampling,” in
ECCV, 2020, pp. 544–560.

[106] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P. Kindermans, and
Q. V. Le, “Can weight sharing outperform random architecture search?
an investigation with tunas,” in CVPR, 2020, pp. 14 311–14 320.
[107] M. Odema, N. Rashid, B. U. Demirel, and M. A. A. Faruque, “Lens:
Layer distribution enabled neural architecture search in edge-cloud
hierarchies,” in DAC, 2021, pp. 403–408.

[108] Z. Yang, S. Zhang, R. Li, C. Li, M. Wang, D. Wang, and M. Zhang,
“Efﬁcient resource-aware convolutional neural architecture search for
edge computing with pareto-bayesian optimization,” Sensors, vol. 21,
no. 2, p. 444, 2021.

[109] M. Parsa, J. P. Mitchell, C. D. Schuman, R. M. Patton, T. E. Potok,
and K. Roy, “Bayesian multi-objective hyperparameter optimization
for accurate, fast, and efﬁcient neural network accelerator design,”
Frontiers in Neuroscience, p. 667, 2020.

[110] D. Eriksson, I. Pierce, J. Chuang, S. Daulton, P. Xia, A. Shrivastava,
A. Babu, S. Zhao, A. A. Aly, G. Venkatesh et al., “Latency-aware
neural architecture search with multi-objective bayesian optimization,”
in ICML Workshop on Automated Machine Learning (AutoML), 2021,
pp. 1–9.

[111] D. Gaudrie, “High-dimensional bayesian multi-objective optimization,”

Ph.D. dissertation, Lyon, 2019.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

32

[112] S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy, “Multi-objective
bayesian optimization over high-dimensional search spaces,” arXiv
preprint arXiv:2109.10964, pp. 1–11, 2021.

[113] B. Moons, P. Noorzad, A. Skliar, G. Mariani, D. Mehta, C. Lott, and
T. Blankevoort, “Distilling optimal neural networks: Rapid search in
diverse spaces,” in ICCV, 2021, pp. 12 209–12 218.

[114] X. Luo, D. Liu, S. Huai, and W. Liu, “Hsconas: Hardware-software
co-design of efﬁcient dnns via neural architecture search,” in DATE,
2021, pp. 418–421.

[115] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan,
Y. Hu, Y. Wu, Y. Jia, P. Vajda, M. Uyttendaele, and N. K. Jha,
“Chamnet: Towards efﬁcient network design through platform-aware
model adaptation,” in CVPR, 2019, pp. 11 398–11 407.

[116] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover
and mutation in genetic algorithms,” IEEE Transactions on Systems,
Man, and Cybernetics, vol. 24, no. 4, pp. 656–667, 1994.

[117] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution
for image classiﬁer architecture search,” in AAAI, 2019, pp. 4780–4789.
[118] X. Chu, B. Zhang, and R. Xu, “Fairnas: Rethinking evaluation fairness
of weight sharing neural architecture search,” in ICCV, 2021, pp.
12 219–12 228.

[119] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast and elitist
multiobjective genetic algorithm: NSGA-II,” IEEE Transactions on
Evolutionary Computing, vol. 6, no. 2, pp. 182–197, 2002.

[120] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine Learning, vol. 8, pp.
229–256, 1992.

[121] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
pp. 1–9, 2017.

[122] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efﬁcient neural
architecture search via parameters sharing,” in ICML, 2018, pp. 4095–
4104.

[123] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with

gumbel-softmax,” in ICLR, 2017, pp. 1–11.

[124] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efﬁcient
neural architecture search via parameter sharing,” in ICML, 2018, pp.
4092–4101.

[125] G. Bender, P. Kindermans, B. Zoph, V. Vasudevan, and Q. V. Le, “Un-
derstanding and simplifying one-shot architecture search,” in ICML,
2018, pp. 550–559.

[126] J. Yu and T. S. Huang, “Universally slimmable networks and improved

training techniques,” in ICCV, 2019, pp. 1803–1811.

[127] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression and
hardware acceleration for neural networks: A comprehensive survey,”
Proceedings of the IEEE, vol. 108, no. 4, pp. 485–532, 2020.
[128] T. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, “A com-
prehensive survey on model compression and acceleration,” Artiﬁcial
Intelligence Review, vol. 53, no. 7, pp. 5113–5155, 2020.

[129] L. R. Tucker, “Some mathematical notes on three-mode factor analy-

sis,” Psychometrika, vol. 31, no. 3, pp. 279–311, 1966.

[130] Y. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compression
of deep convolutional neural networks for fast and low power mobile
applications,” in ICLR, 2016, pp. 1–11.

[131] J. Kossaiﬁ, A. Khanna, Z. Lipton, T. Furlanello, and A. Anandkumar,
“Tensor contraction layers for parsimonious deep nets,” in CVPR
Workshops, 2017, pp. 1940–1946.

[132] J. D. Carroll and J.-J. Chang, “Analysis of individual differences
in multidimensional scaling via an n-way generalization of “eckart-
young” decomposition,” Psychometrika, vol. 35, no. 3, pp. 283–319,
1970.

[133] M. Astrid and S. Lee, “Cp-decomposition with tensor power method
for convolutional neural networks compression,” in BigComp, 2017,
pp. 115–118.

[134] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky, “Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition,” in ICLR, 2015, pp. 1–11.

[135] S. Nakajima, M. Sugiyama, S. D. Babacan, and R. Tomioka, “Global
analytic solution of fully-observed variational bayesian matrix factor-
ization,” Journal of Machine Learning Research, vol. 14, no. 1, pp.
1–37, 2013.

[136] J. Gusak, M. Kholyavchenko, E. Ponomarev, L. Markeeva,
P. Blagoveschensky, A. Cichocki, and I. V. Oseledets, “Automated
multi-stage compression of neural networks,” in ICCVW, 2019, pp.
2501–2508.

[137] C. Hawkins, X. Liu, and Z. Zhang, “Towards compact neural networks
via end-to-end training: A bayesian tensor approach with automatic
rank determination,” SIAM Journal on Mathematics of Data Science,
vol. 4, no. 1, pp. 46–71, 2022.

[138] C. Hawkins and Z. Zhang, “Bayesian tensorized neural networks with
automatic rank selection,” Neurocomputing, vol. 453, pp. 172–180,
2021.

[139] M. Kodryan, D. Kropotov, and D. Vetrov, “Mars: Masked au-
tomatic ranks selection in tensor decompositions,” arXiv preprint
arXiv:2006.10859, pp. 1–12, 2020.

[140] M. Javaheripi, M. Samragh, and F. Koushanfar, “Autorank: Automated
rank selection for effective neural network customization,” IEEE Jour-
nal on Emerging and Selected Topics in Circuits and Systems, vol. 11,
no. 4, pp. 611–619, 2021.

[141] M. Samragh, M. Javaheripi, and F. Koushanfar, “Autorank: Automated
rank selection for effective neural network customization,” in ISCA,
2019, pp. 611–619.

[142] X. Ma, A. R. Triki, M. Berman, C. Sagonas, J. Cal`ı, and M. B.
Blaschko, “A bayesian optimization framework for neural network
compression,” in ICCV, 2019, pp. 10 273–10 282.

[143] C. Bucila, R. Caruana, and A. Niculescu-Mizil, “Model compression,”

in KDD, 2006, pp. 535–541.

[144] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in NIPS,

2014, pp. 2654–2662.

[145] Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, and X. Wang,
“Search to distill: Pearls are everywhere but not the eyes,” in CVPR,
2020, pp. 7536–7545.

[146] X. Guo, J. Yang, H. Zhou, X. Ye, and J. Li, “Rosearch: Search
for robust student architectures when distilling pre-trained language
models,” arXiv preprint arXiv:2106.03613, pp. 1–10, 2021.

[147] Z. Lei, K. Yang, K. Jiang, and S. Chen, “Kdas-reid: Architecture
search for person re-identiﬁcation via distilled knowledge with dynamic
temperature,” Algorithms, vol. 14, no. 5, p. 137, 2021.

[148] R. H. Eyono, F. M. Carlucci, P. M. Esperanc¸a, B. Ru, and P. Torr,
“Autokd: Automatic knowledge distillation into a student architecture
family,” arXiv preprint arXiv:2111.03555, pp. 1–12, 2021.

[149] X. Zhang, Z. Zhou, D. Chen, and Y. E. Wang, “Autodistill: an end-
to-end framework to explore and distill hardware-efﬁcient language
models,” arXiv preprint arXiv:2201.08539, pp. 1–14, 2022.

[150] L. Chen, F. Yuan, J. Yang, M. Yang, and C. Li, “Scene-adaptive
knowledge distillation for sequential recommendation via differentiable
architecture search,” arXiv preprint arXiv:2107.07173, pp. 1–11, 2021.
[151] Z. Zhang, W. Zhu, J. Yan, P. Gao, and G. Xie, “Automatic student
network search for knowledge distillation,” in ICPR, 2020, pp. 2446–
2453.

[152] M. Li, J. Lin, Y. Ding, Z. Liu, J. Zhu, and S. Han, “Gan compression:
Efﬁcient architectures for interactive conditional gans,” in CVPR, 2020,
pp. 5283–5293.

[153] W. Cheng, M. Zhao, Z. Ye, and S. Gu, “Mfagan: A compression
framework for memory-efﬁcient on-device super-resolution gan,” arXiv
preprint arXiv:2107.12679, pp. 1–10, 2021.

[154] D. M. Vo, A. Sugimoto, and H. Nakayama, “PPCD-GAN: progressive
pruning and class-aware distillation for large-scale conditional gans
compression,” in WACV, 2022, pp. 1422–1430.

[155] M. Kang, J. Mun, and B. Han, “Towards oracle knowledge distillation
with neural architecture search,” in AAAI, 2020, pp. 4404–4411.
[156] K. Mitsuno, Y. Nomura, and T. Kurita, “Channel planting for deep
neural networks using knowledge distillation,” in ICPR, 2020, pp.
7573–7579.

[157] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T. Liu, “NAS-
BERT: task-agnostic and adaptive-size BERT compression with neural
architecture search,” in KDD, 2021, pp. 1933–1943.

[158] C. Li, J. Peng, L. Yuan, G. Wang, X. Liang, L. Lin, and X. Chang,
“Block-wisely supervised neural architecture search with knowledge
distillation,” in CVPR, 2020, pp. 1986–1995.

[159] S. Park, J. Lee, S. Mo, and J. Shin, “Lookahead: A far-sighted

alternative of magnitude-based pruning,” in ICLR, 2020, pp. 1–10.

[160] T. Zhang, S. Ye, Y. Zhang, Y. Wang, and M. Fardad, “A systematic
weight pruning framework of dnns using alternating direction method
of multipliers,” in ECCV, 2018, p. 191–207.

[161] W. Zeng, Y. Xiong, and R. Urtasun, “Network automatic pruning: Start
nap and take a nap,” arXiv preprint arXiv:2101.06608, pp. 1–14, 2021.
[162] Z. Li, Y. Gong, X. Ma, S. Liu, M. Sun, Z. Zhan, Z. Kong, G. Yuan,
and Y. Wang, “Ss-auto: A single-shot, automatic structured weight
pruning framework of dnns with ultra-high efﬁciency,” arXiv preprint
arXiv:2001.08839, pp. 1–8, 2020.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

33

[163] X. Zheng, Y. Ma, T. Xi, G. Zhang, E. Ding, Y. Li, J. Chen, Y. Tian, and
R. Ji, “An information theory-inspired strategy for automatic network
pruning,” arXiv preprint arXiv:2108.08532, pp. 1–10, 2021.

[164] N. Liu, X. Ma, Z. Xu, Y. Wang, J. Tang, and J. Ye, “Autocompress:
An automatic DNN structured pruning framework for ultra-high com-
pression rates,” in AAAI, 2020, pp. 4876–4883.

[165] S. Yang, W. Chen, X. Zhang, S. He, Y. Yin, and X. Sun, “Auto-prune:
automated dnn pruning and mapping for reram-based accelerator,” in
ICS, 2021, pp. 304–315.

[166] B. Li, Y. Fan, Z. Pan, Y. Bian, and G. Zhang, “Automatic channel
pruning with hyper-parameter search and dynamic masking,” in MM,
2021, pp. 2121–2129.

[167] F. Tung, S. Muralidharan, and G. Mori, “Fine-pruning: Joint ﬁne-
tuning and compression of a convolutional network with bayesian
optimization,” in BMVC, 2017, pp. 115.1–115.12.

[168] C. Chen, F. Tung, N. Vedula, and G. Mori, “Constraint-aware deep

neural network compression,” in ECCV, 2018, pp. 409–424.

[169] J. Mu, H. Fan, and W. Zhang, “High-dimensional bayesian optimization
for cnn auto pruning with clustering and rollback,” in ECCV, 2022, pp.
1–17.

[170] J. Li, H. Li, Y. Chen, Z. Ding, N. Li, M. Ma, Z. Duan, and D. Zhao,
“Abcp: Automatic block-wise and channel-wise network pruning via
joint search,” arXiv preprint arXiv:2110.03858, pp. 1–12, 2021.
[171] L. Guerra and T. Drummond, “Automatic pruning for quantized neural

networks,” in DICTA, 2021, pp. 1–8.

[172] T. Yang, A. G. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze,
and H. Adam, “Netadapt: Platform-aware neural network adaptation for
mobile applications,” in ECCV, 2018, pp. 289–304.

[173] T. Yang, Y. Liao, and V. Sze, “Netadaptv2: Efﬁcient neural architecture
search with fast super-network training and architecture optimization,”
in CVPR, 2021, pp. 2402–2411.

[174] Y. Guan, N. Liu, P. Zhao, Z. Che, K. Bian, Y. Wang, and J. Tang,
“Dais: Automatic channel pruning via differentiable annealing indicator
search,” IEEE Transactions on Neural Network and Learning Systems,
pp. 1–12, 2020.

[175] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning
efﬁcient convolutional networks through network slimming,” in ICCV,
2017, pp. 2755–2763.

[176] Y. Li, S. Gu, K. Zhang, L. V. Gool, and R. Timofte, “DHP: differen-
tiable meta pruning via hypernetworks,” in ECCV, 2020, pp. 608–624.
[177] S. Qu, B. Li, Y. Wang, and L. Zhang, “ASBP: automatic structured
bit-pruning for rram-based NN accelerator,” in DAC, 2021, pp. 745–
750.

[178] X. Dong and Y. Yang, “Network pruning via transformable architecture

search,” in NeurIPS, 2019, pp. 759–770.

[179] F. Xue and J. Xin, “Network compression via cooperative architecture

search and distillation,” in AI4I, 2021, pp. 42–43.

[180] L. Yao, R. Pi, H. Xu, W. Zhang, Z. Li, and T. Zhang, “Joint-detnas:
Upgrade your detector with nas, pruning and dynamic distillation,” in
CVPR, 2021, pp. 10 175–10 184.

[181] J. Gu and V. Tresp, “Search for better students to learn distilled

knowledge,” in ECAI, 2020, pp. 1159–1165.

[182] M. Horowitz, “1.1 computing’s energy problem (and what we can do

about it),” in ISSCC, 2014, pp. 10–14.

[183] H. Bai, M. Cao, P. Huang, and J. Shan, “Batchquant: Quantized-for-
all architecture search with robust quantizer,” in NeurIPS, 2021, pp.
1074–1085.

[184] EENews, “Apple describes 7nm a12 bionic chips,” 2018.
[185] Nvidia, “Nvidia tensor cores,” 2018.
[186] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Hardware-centric automl
for mixed-precision quantization,” International Journal of Computer
Vision, vol. 128, no. 8, pp. 2035–2048, 2020.

[187] G. Lacey, G. W. Taylor, and S. Areibi, “Stochastic layer-wise precision

in deep neural networks,” in UAI, 2018, pp. 663–672.

[188] H. Yu, Q. Han, J. Li, J. Shi, G. Cheng, and B. Fan, “Search what
you want: Barrier panelty NAS for mixed precision quantization,” in
ECCV, 2020, pp. 1–16.

[189] J. Xu, S. Hu, J. Yu, X. Liu, and H. Meng, “Mixed precision quantization
of transformer language models for speech recognition,” in ICASSP,
2021, pp. 7383–7387.

[190] Q. Sun, L. Jiao, Y. Ren, X. Li, F. Shang, and F. Liu, “Effective and fast:
A novel sequential single path search for mixed-precision quantization,”
IEEE Transactions on Cybernetics, pp. 1–13, 2022.

[191] X. Wei, H. Chen, W. Liu, and Y. Xie, “Mixed-precision quantization
for cnn-based remote sensing scene classiﬁcation,” IEEE Geoscience
and Remote Sensing Letters, vol. 18, no. 10, pp. 1721–1725, 2021.

[192] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer, “Mixed
precision quantization of convnets via differentiable neural architecture
search,” arXiv preprint arXiv:1812.00090, pp. 1–11, 2018.

[193] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van
Baalen, and T. Blankevoort, “A white paper on neural network quan-
tization,” arXiv preprint arXiv:2106.08295, pp. 1–27, 2021.

[194] Y. Guan, P. Zhao, B. Wang, Y. Zhang, C. Yao, K. Bian, and J. Tang,
“Differentiable feature aggregation search for knowledge distillation,”
in ECCV, 2020, pp. 469–484.

[195] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the

value of network pruning,” in ICLR, 2019, pp. 1–16.

[196] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K. Cheng, and J. Sun,
“Metapruning: Meta learning for automatic neural network channel
pruning,” in ICCV, 2019, pp. 3295–3304.

[197] J. Liu, J. Sun, Z. Xu, and G. Sun, “Latency-aware automatic cnn chan-
nel pruning with gpu runtime analysis,” BenchCouncil Transactions on
Benchmarks, Standards and Evaluations, vol. 1, no. 1, p. 100009, 2021.
[198] M. Lin, R. Ji, Y. Zhang, B. Zhang, Y. Wu, and Y. Tian, “Channel
pruning via automatic structure search,” in IJCAI, 2020, pp. 673–679.
[199] J. Chang, Y. Lu, P. Xue, Y. Xu, and Z. Wei, “Acp: Automatic channel
pruning via clustering and swarm intelligence optimization for cnn,”
arXiv preprint arXiv:2101.06407, pp. 1–10, 2021.

[200] Y. Liu, Y. Wang, H. Qi, and X. Ju, “Superpruner: Automatic neural
network pruning via super network,” Scientiﬁc Programming, vol.
2021, pp. 1–11, 2021.

[201] L. Lin, Y. Yang, and Z. Guo, “Aacp: Model compression by accurate
and automatic channel pruning,” arXiv preprint arXiv:2102.00390, pp.
1–10, 2021.

[202] I. Fedorov, R. P. Adams, M. Mattina, and P. N. Whatmough, “Sparse:
Sparse architecture search for cnns on resource-constrained microcon-
trollers,” in NeurIPS, 2019, pp. 4978–4990.

[203] H. Cai, T. Wang, Z. Wu, K. Wang, J. Lin, and S. Han, “On-device
image classiﬁcation with proxyless neural architecture search and
quantization-aware ﬁne-tuning,” in ICCVW, 2019, pp. 2509–2513.

[204] C. Liu, Y. Han, Y. Sung, Y. Lee, H. Chiang, and K. Wu, “FOX-NAS:
fast, on-device and explainable neural architecture search,” in ICCVW,
2021, pp. 789–797.

[205] D. Peter, W. Roth, and F. Pernkopf, “Resource-efﬁcient dnns for
keyword spotting using neural architecture search and quantization,”
in ICPR, 2020, pp. 9273–9279.

[206] A. Bulat, B. Mart´ınez, and G. Tzimiropoulos, “BATS: binary architec-

ture search,” in ECCV, 2020, pp. 309–325.

[207] H. Phan, Z. Liu, D. Huynh, M. Savvides, K. Cheng, and Z. Shen,
“Binarizing mobilenet via evolution-based searching,” in CVPR, 2020,
pp. 13 417–13 426.

[208] D. Kim, K. P. Singh, and J. Choi, “Learning architectures for binary

networks,” in ECCV, 2020, pp. 575–591.

[209] M. Shen, K. Han, C. Xu, and Y. Wang, “Searching for accurate binary

neural architectures,” in ICCVW, 2019, pp. 2041–2044.

[210] T. Kim, Y. Yoo, and J. Yang, “Frostnet: Towards quantization-awareof
network architecture search,” arXiv preprint arXiv:2006.09679, pp. 1–
18, 2020.

[211] S. Xu, J. Zhao, J. Lu, B. Zhang, S. Han, and D. S. Doermann, “Layer-

wise searching for 1-bit detectors,” in CVPR, 2021, pp. 5682–5691.

[212] Y. Chen, G. Meng, Q. Zhang, X. Zhang, L. Song, S. Xiang, and C. Pan,
“Joint neural architecture search and quantization,” arXiv preprint
arXiv:1811.09426, pp. 1–10, 2018.

[213] C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z. Pan, “Mixed
precision neural architecture search for energy efﬁcient deep learning,”
in ICCAD, 2019, pp. 1–7.

[214] T. Wang, K. Wang, H. Cai, J. Lin, Z. Liu, H. Wang, Y. Lin, and S. Han,
“APQ: joint search for network architecture, pruning and quantization
policy,” in CVPR, 2020, pp. 2075–2084.

[215] M. Shen, F. Liang, R. Gong, Y. Li, C. Li, C. Lin, F. Yu, J. Yan,
and W. Ouyang, “Once quantization-aware training: High performance
extremely low-bit architecture search,” in ICCV, 2021, pp. 5320–5329.
[216] M. S. Abdelfattah, A. Mehrotra, L. Dudziak, and N. D. Lane, “Zero-
cost proxies for lightweight NAS,” in ICLR, 2021, pp. 1–11.
[217] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin,
“Zen-nas: A zero-shot NAS for high-performance image recognition,”
in ICCV, 2021, pp. 337–346.

[218] A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. H. Jin, S. Zhao, and
K. Keutzer, “Squeezenext: Hardware-aware neural network design,” in
CVPR Workshops, 2018, pp. 1638–1647.

[219] M. Sun, P. Zhao, Y. Wang, N. Chang, and X. Lin, “HSIM-DNN:
hardware simulator for computation-, storage- and power-efﬁcient deep
neural networks,” in GLSVLSI, 2019, pp. 81–86.

IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 14, NO. 8, AUGUST 2015

34

[220] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao,
and Y. Lin, “Hw-nas-bench: Hardware-aware neural architecture search
benchmark,” in ICLR, 2019, pp. 1–14.

