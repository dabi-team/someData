A Hybrid Method for Tensor Decompositions that Leverages Stochastic and
Deterministic Optimization∗

Jeremy M. Myers† and Daniel M. Dunlavy†

Abstract. In this paper, we propose a hybrid method that uses stochastic and deterministic search to compute
the maximum likelihood estimator of a low-rank count tensor with Poisson loss via state-of-the-
art local methods. Our approach is inspired by Simulated Annealing for global optimization and
allows for ﬁne-grain parameter tuning as well as adaptive updates to algorithm parameters. We
present numerical results that indicate our hybrid approach can compute better approximations
to the maximum likelihood estimator with less computation than the state-of-the-art methods by
themselves.

Key words. tensor, canonical polyadic decomposition, GCP, CPAPR

AMS subject classiﬁcations. 15A69, 65F55

1. Introduction. Low-rank tensor decompositions in general, and canonical polyadic (CP)
tensor decompositions speciﬁcally, are now ubiquitous in the area of multi-way data analy-
sis. Recent research in developing eﬃcient methods for computing CP tensor decomposi-
tions via maximum likelihood estimation reﬂects an emergent dichotomy in numerical linear
algebra: deterministic versus randomized algorithms. CP Alternating Poisson Regression
(CPAPR) [17] is a deterministic CP tensor decomposition method that alternates over a se-
quence of convex Poisson loss subproblems iteratively. Generalized CP (GCP) tensor decom-
positions extend previous work on CP decompositions to incorporate general loss functions
and stochastic optimization methods [32, 42]. Previously, in [49], we showed that CPAPR is
performant and can compute accurate maximum likelihood estimator approximations with
a higher probability than GCP. Here we explore a hybrid method for computing CP tensor
decompositions that leverages GCP for scalability and CPAPR for performance and accuracy.
This paper is structured as follows. In Section 2, we introduce notation and formalize
several metrics, some used previously in [49], to compare CP decomposition methods. In Sec-
tion 3, we introduce Cyclic GCP-CPAPR (CGC), a CP decomposition method that alternates
between stochastic and deterministic methods to avoid local minima. We also formally deﬁne
parameterizations, strategies, and policies to optimize CGC for precision and accuracy.
In
Section 4, we report the results of numerical experiments comparing the hybrid CGC method
with the individual GCP and CPAPR methods on real and synthetic sparse tensor data. These
experiments oﬀer evidence that our hybrid method can reduce maximum likelihood estimator
approximation error and computational cost versus current methods. In Section 5, we discuss
our conclusions and proposed future work including:

∗Sandia National Laboratories is a multimission laboratory managed and operated by National Technology &
Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S.
Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. SAND2022-
5616R.

†Sandia National Laboratories (jermyer@sandia.gov, dmdunla@sandia.gov)

1

2
2
0
2

l
u
J

8
1

]

A
N
.
h
t
a
m

[

1
v
1
4
3
4
1
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

J. M. MYERS AND D. M. DUNLAVY

• potential improvements in CGC computational eﬃciency via the choices of method

parameterizations, strategies, and policies (Subsection 5.1); and

• comparisons of CGC with generic optimization methods (Subsection 5.2).

2. Background and Related Work.

2.1. Notation and conventions. The ﬁeld of real numbers and the ring of integers are
denoted as R and Z, respectively. The real numbers and integers restricted to non-negative
values are denoted as R+ and Z+, respectively. The order of a tensor is the number of
dimensions or ways. Each tensor dimension is called a mode. A scalar (tensor of order zero)
is represented by a lowercase letter, e.g., x. A bold lowercase letter denotes a vector (tensor
of order one), e.g., v. A matrix (tensor of order two) is denoted by a bold capital letter, e.g.,
A ∈ Rm×n. Tensors of order three and higher are expressed with a bold capital script letter,
e.g., X ∈ Rm×n×p. Values derived, computed, approximated, or estimated are typically written
with a hat or a tilde—e.g., (cid:99)M ∈ Rm×n×p may be a tensor model of parameters ﬁt to data and
(cid:101)Σ ∈ Rn×n may be a diagonal matrix containing approximate singular values of a matrix.

The i-th entry of a vector v is denoted vi, the (i, j) entry of a matrix M is denoted mij,
and the (i, j, k) entry of a three-way tensor T is denoted tijk. Indices are scalar values that
can range from 1 to a value denoted by the capitalized version of the index variable, e.g.,
i = 1, . . . , I. We use MATLAB-style notation for subarrays formed from a subset of indices
of a vector, matrix, or tensor mode. We use the shorthand ij : ik when the subset of indices
forming a subarray is the range ij, . . . , ik. The special case of a colon : by itself indicates all
elements of a mode, e.g., the j-th column of the matrix A is A( : , j) = A(i1 : iI , j). We use
the multi-index

(2.1)

i := (i1, i2, . . . , id) with ij ∈ {1, 2, . . . , Ij} for

j = 1, . . . , d,

as a convenient shorthand for the (i1, i2, . . . , id) entry of a d-way tensor.

Superscript T denotes non-conjugate matrix transpose. We assume vectors u and v are
column vectors so that uT v is an inner product of vectors and uvT is an outer product of
vectors. We also denote outer products of vectors as u ◦ v = uvT . The number of matrix
or tensor non-zero elements is denoted nnz(·); conversely, the number of zeros in a matrix or
tensor is denoted nz(·).

2.2. Canonical polyadic decomposition. There is growing interest to extend low-rank
matrix decompositions to multi-way arrays, or tensors. One fundamental low-rank tensor
decomposition is the canonical polyadic (CP) decomposition. The CP tensor decomposition
represents a tensor as a ﬁnite sum of rank-one outer products, a generalization of the matrix
singular value decomposition (SVD) to tensors. For example, the rank-k SVD of a matrix
A ∈ RI1×I2 is

(2.2)

A ≈ U1ΣUT

2 =

k
(cid:88)

r=1

σru1( : , r) ◦ u2( : , r),

subject to orthogonality constraints UT
2 U2 = Ik, where Ik is the identity matrix
with k rows and columns. Note that with U1 = U and U2 = V we recover the standard

1 U1 = UT

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

3

form of the matrix SVD: A = UΣVT . In the parlance of CP, U1 and U2 are factor matrices
corresponding to the ﬁrst and second modes of A, respectively. The column vectors of the
factor matrices are referred to as components; for example, U1( : , 1) refers to the ﬁrst com-
ponent of the factor matrix corresponding to the ﬁrst mode of A. We will ﬁx this language
in the next section. In contrast to the matrix SVD, there are no orthogonality constraints on
the columns of the factor matrices U1 and U2 of the CP decomposition; thus we treat the
matrix SVD as a special case of CP decomposition. Nonetheless, low-rank CP decomposi-
tions are appealing for reasons similar to those of the low-rank SVD, including dimensionality
reduction, compression, de-noising, and more. Interpretability of CP decompositons on real
problems is well-documented, with applications including exploratory temporal data analysis
and link prediction [18], chemometrics [48], neuroscience [4], and social network and web link
analysis [40, 41].

One particular application of interest is when the tensor contains non-negative entries that
are assumed to follow a Poisson distribution. In this case, the low-rank CP tensor model of
Poisson parameters must satisfy certain nonnegativity and stochasticity constraints. In the
next few sections we cover the details of the low-rank CP tensor models of Poisson parameters
and decompositions which are the focus of this work.

2.3. Low-rank CP tensor model. Assume X is an d-way tensor of size I1 × · · · × Id. The
tensor X is rank-one if it can be expressed as the outer product of d vectors, each corresponding
to a mode in X, i.e.,

(2.3)

X = a1 ◦ a2 ◦ · · · ◦ ad.

More broadly, the rank of a tensor X is the smallest number of rank-one tensors that generate
X as their sum [41] and is the generalization of matrix rank to tensors. We concentrate on
the problem of approximating a tensor of data with a low-rank CP tensor model, i.e., the sum
of relatively few rank-one tensors.

Let λ = [λ1, λ2, . . . , λd] ∈ Rd be a vector of scalars and let A1 ∈ RI1×R, A2 ∈ RI2×R, . . . ,
Ad ∈ RId×R be matrices. The rank-R canonical polyadic (CP) tensor model of X [15, 26] is:

(2.4)

X ≈ M =

λ; A1, . . . , Ad
(cid:74)

(cid:75)

=

R
(cid:88)

r=1

λrA1( : , r) ◦ · · · ◦ Ad( : , r).

Each Ak ∈ RIk×R is a factor matrix with Ik rows and R columns, which we refer to as factors.
For example, the j-th component of the mode-k factor matrix is the column vector Ak( : , j).
We refer to the form M =

as a Kruskal tensor.

λ; A1, . . . , Ad
(cid:74)

(cid:75)

2.4. Computing the CP decomposition for non-negative tensors. We focus on an ap-
plication where all of the entries in a data tensor are non-negative integers or counts. For the
remainder of this work, let X ∈ ZI1×···×Id
be a d-way tensor of non-negative integers, let M
be a CP tensor model of the form (2.4), and assume the following about X:

+

1. the data in X are sampled from a ﬁxed Poisson distribution,
2. the relationships of entries in X can be represented by a multilinear form,
3. the tensor X has low-rank structure, and

4

J. M. MYERS AND D. M. DUNLAVY

4. the rank of X is known a priori.

Chi and Kolda showed in [17] that under these assumptions a Poisson CP tensor model is an
eﬀective low-rank approximation of X. The Poisson CP tensor model has shown to be eﬀective
in analyzing latent patterns and relationships in count data across many application areas,
including food production [13], network analysis [11, 19], term-document analysis [16, 29],
email analysis [14], link prediction [18], geosptial analysis [22, 28], web page analysis [39], and
phenotyping from electronic health records [27, 30, 31]

One numerical approach to ﬁt low-rank Poisson CP tensor models to data, tensor maxi-
mum likelihood estimation, has proven to be eﬀective. Computing a Poisson CP tensor model
via tensor maximum likelihood estimation involves minimizing the following non-linear, non-
convex optimization problem:

(2.5)

f (X, M) = min

min
M

(cid:88)

i

mi − xi log mi,

where i is the multi-index (2.1), xi ≥ 0 is an entry in X, and mi > 0 is a parameter in
the Poisson CP tensor model M. The function f (X, M) in (2.5) is the negative of the log-
likelihood of the Poisson distribution (omitting the constant (cid:80)
i log (xi!) term). We will refer
to it simply as negative log-likelihood (NLL).

In contrast to linear maximum likelihood estimation [51], where a single parameter is
estimated using multiple data instances, tensor maximum likelihood estimation ﬁts a single
parameter in an approximate low-rank model to a single data instance. Within this context,
low-rank structure means that multiple instances in the data are linked via the low-rank
structure to a single model parameter, a sort of multilinear maximum likelihood estimation.
This distinction is not made anywhere else in the literature, to the best of our knowledge. One
additional clariﬁcation is that we estimate the Poisson parameters that most likely generate
the data by minimizing (2.5), rather than the Poisson parameters used to sample the data.
To see this, consider the univariate case of the Poisson distribution, whose probability mass
function is

(2.6)

Pµ(X = x) =

µxe−µ
x!

.

If µ = 1.5 then there is a high likelihood of drawing a sample that is 1 or 2. In either case,
maximum likelihood estimation tends to ﬁnd the parameter that generates the sample (say
µ = 1.95 if the observation is 2) than of computing the natural parameter that generated the
underlying distribution of the sampled model (namely µ = 1.5).

Much of the research associated with computing low-rank Poisson CP tensor models via
tensor maximum likelihood estimation has focused on local methods [17, 25, 32, 42], particu-
larly with respect to computational performance [7, 8, 10, 45, 50, 53, 58]. Many of the current
methods for Poisson CP tensor decomposition can be broadly classiﬁed as either an alter-
nating [15, 26] or an all-at-once optimization method [1, 2, 52]. Alternating local methods
iteratively solve a series of subproblems by ﬁtting each factor matrix sequentially, with the
remaining factor matrices held ﬁxed. Alternating tensor decomposition methods are a form of
coordinate descent (CD) [62], where each factor matrix is a block of components that are ﬁt
sequentially while the remaining factor matrices (i.e., component blocks) are held ﬁxed. Since

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

5

each block corresponds to a lower-dimensional problem, alternating tensor methods employ
block CD iteratively to solve a series of easier problems. CP Alternating Poisson Regression
(CPAPR) was introduced by Chi and Kolda in [17] as a non-linear Gauss-Seidel approach
to block CD that uses a ﬁxed-point majorization-minimization algorithm called Multiplica-
tive Updates (CPAPR-MU). Hansen et al.
in [25] presented two Newton-based, active set
gradient projection methods using up to second-order information, Projected Damped New-
ton (CPAPR-PDN) and Projected Quasi-Newton (CPAPR-PQN). Moreover, they provided
extensions to these methods where each component block of the CPAPR minimization can be
further separated into independent, highly-parallelizable row-wise subproblems; these meth-
ods are Projected Damped Newton for the Row subproblem (CPAPR-PDNR) and Projected
Quasi-Newton for the Row subproblem (CPAPR-PQNR).

All-at-once optimization methods update all optimization variables simultaneously. Gen-
eralized Canonical Polyadic decomposition (GCP) [32] is a meta-method: an all-at-once opti-
mization approach where an approximate CP tensor model is ﬁt with arbitrary loss function
via tensor maximum likelihood estimation. The original GCP method has two variants: 1)
deterministic, which uses quasi-Newton optimization L-BFGS; and 2) stochastic, which uses
either gradient descent (SGD) or Adam [37] optimization. We focus here on GCP-Adam [42],
which applies Adam for scalability.

More generally, we focus on the GCP and CPAPR families of tensor maximum likelihood-

based local methods for Poisson CP tensor decomposition for the following reasons:

1. Theory: Method convergence, computational costs, and memory demands are well-

understood.

2. Software: High-level MATLAB code implementing both families is available in MAT-
LAB Tensor Toolbox12 [5, 6]. High-performance C++ code that leverages the Kokkos
hardware abstraction library [20] to provide parallel computation on diverse computer
architectures (e.g., x86-multicore, ARM, GPU, etc.) is available with SparTen3 for
CPAPR [58] and Genten4 for GCP [53]. Additional open-source software for MAT-
LAB includes N-Way Toolbox [3] and Tensorlab [61]. Commercial software includes
ENSIGN Tensor Toolbox [9].

There are other approaches in the literature that seek to ﬁt models with other distributions
in the exponential family or that use other algorithms to estimate parameters. Alternating
least squares methods are relatively easy to implement and eﬀective when used with LASSO-
type regularization [12, 23]. The method of Ranadive et al. [55], CP-POPT-DGN, is an all-
at-once active set trust-region gradient-projection method. CP-POPT-DGN is functionally
very similar to CPAPR-PDN. Whereas CP-POPT-DGN computes the search direction via
preconditioned conjugate gradient (PCG), CPAPR-PDNR computes the search direction via
Cholesky factorization. The most signiﬁcant diﬀerences are: 1) CP-POPT-DGN is all-at-once
whereas all CPAPR methods are alternating and 2) CPAPR can take advantage of the sepa-
rable row subproblem formulation to achieve more ﬁne-grained parallelism. The Generalized

1https://gitlab.com/tensors/tensor toolbox.
2See functions cp apr for CPAPR-MU, PQNR, and PDNR and gcp opt for GCP-SGD and Adam.
3https://gitlab.com/tensors/sparten.
4https://gitlab.com/tensors/genten.

6

J. M. MYERS AND D. M. DUNLAVY

Gauss-Newton method of Vandecapelle et al. [60] follows the GCP framework to ﬁt arbitrary
non-least squares loss via an all-at-once optimization and trust-region-based Gauss-Newton
approach. Hu et al. [33,34] re-parameterized the Poisson regression problem to leverage Gibbs
sampling and variational Bayesian inference to account for the inability of CPAPR to han-
dle missing data. Other problem transformations include probabilistic likelihood extensions
via Expectation Maximization [35, 54] and a Legendre decomposition [57] instead of a CP
decomposition.

Our ﬁnal consideration is how we run CPAPR and GCP. These local methods tend to
converge to local minima. We mitigate this by using a multi-start strategy [24, 47] to com-
pute a set of approximations from many random starting points in the feasible domain of
the optimization problem. From this set, we choose the “best” local minimizer—i.e., the ap-
proximation that minimizes (2.5)—as the approximation to the global optimizer. In turn, the
eﬀectiveness of a given method is determined in part by the probability it will converge to a
solution approximating the global optimizer.

2.5. Assessing approximation error. We deﬁne several tools that we will use to compare
the eﬀectiveness of a given method in computing a model that minimizes (2.5). Let X be a d-
way data tensor with dimensions I1, . . . , Id. Let S = {(cid:99)M1, . . . , (cid:99)MN } be a set of rank-R Poisson
CP tensor approximations computed from N unique starting points by some process, e.g.,
CPAPR. Let M∗ denote the maximum likelihood estimator (MLE), i.e., the global minimizer
of (2.5). In general, the MLE is not known. As a result, we compute approximations of M∗.
Let (cid:99)M∗
denote the current approximate MLE, i.e. the rank-R Poisson CP tensor model that is
the best approximation to M∗. We specify the current approximate MLE restricted to S, i.e.,
(cid:99)M∗
∈ S, as the current approximate MLE when we consider only the set of Poisson
CP tensor approximations S resulting from one of the local numerical optimization methods
described above:

S ≡ (cid:99)M∗

(2.7)

(cid:99)M∗

S = {(cid:99)Mj ∈ S | f (X, (cid:99)Mj) ≤ f (X, (cid:99)Mk), k = 1, . . . , |S|, j < k}.

Specifying the current approximate MLE restricted to a speciﬁc set will be useful in Section 4
when comparing between diﬀerent algorithms. The condition that j < k guarantees that the
set is nonempty in the case of a tie. If a Poisson CP tensor model is found that is a better
approximation to M∗ than (cid:99)M∗

S, then we denote it as (cid:99)M∗
+.

1. The signed relative diﬀerence in NLL between (cid:99)Mn and (cid:99)M∗

is

(2.8)

∆(n)
r

:=

f (X, (cid:99)Mn) − f (X, (cid:99)M∗
|f (X, (cid:99)M∗

)|

)

.

r < 0 means (cid:99)Mn has lower NLL than (cid:99)M∗

In our experiments in Section 4, along with our speciﬁc choices for the data tensor X,
∆(n)
+. That is, it is a
better minimizer of (2.5) than (cid:99)M∗

and that (cid:99)Mn = (cid:99)M∗
. The opposite holds when ∆(n)

r > 0.

2. The probability that a given Poisson CP tensor method converges to (cid:99)Mn ∈ S such
) from any starting point in

that f (X, (cid:99)Mn) is within a ball of radius (cid:15) > 0 of f (X, (cid:99)M∗

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

7

the feasible region of (2.5) is deﬁned as

(cid:16)

P

|f (X, (cid:99)Mn) − f (X, (cid:99)M∗

(cid:17)

)| < (cid:15)

, n = 1, . . . , |S|.

We estimate P over a set of models S = {(cid:99)M1, . . . , (cid:99)MN } as

(2.9)

(cid:16)

(cid:99)M∗

(cid:98)P

(cid:17)

, S, (cid:15)

=

#(cid:99)Mn ∈ S for which |∆(n)

r

| < (cid:15)

|S|

.

Equation (2.9) is a conservative estimator since it does not account for solutions which
may be closer to M∗ but have distance from (cid:99)M∗

more than (cid:15).

3. We will frequently use a measure describing similarity between two Kruskal tensors
based on their algebraic properties called factor match score (FMS) [17, 42–44]. FMS
is the maximum sum of cosine similarities over all permutations of the column vectors
of all the factor matrices between two Kruskal tensors, (cid:99)M =
and
˜λ; ˜A1, . . . , ˜Ad
(cid:102)M =
(cid:74)
(2.10)

ˆλ; ˆA1, . . . , ˆAd
(cid:74)

:
(cid:75)

(cid:75)

FMS((cid:99)M, (cid:102)M) = max
ˆπ(·), ˜π(·)

1
R

R
(cid:88)

r=1



1 −

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆξr − ˜ξr
(cid:12)
(cid:12)
max{ ˆξr, ˜ξr}





d
(cid:89)

n=1

ˆAn( : , ˆπ(j))T ˜An( : , ˜π(j))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
˜An( : , ˜π(j))
ˆAn( : , ˆπ(j))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

where

ˆξr = ˆλr

d
(cid:89)

(cid:13)
(cid:13)
(cid:13) and ˜ξr = ˜λr
ˆAn( : , r)
(cid:13)
(cid:13)
(cid:13)

n=1

d
(cid:89)

n=1

(cid:13)
(cid:13)
˜An( : , r)
(cid:13)
(cid:13)
(cid:13).
(cid:13)

An FMS score of 1 indicates collinearity among the columns of all factor matrices and
thus a perfect match between the two Kruskal tensors. As in [46], we say (cid:99)M and
(cid:102)M are similar if FMS((cid:99)M, (cid:102)M) ≥ 0.85 and equal if FMS((cid:99)M, (cid:102)M) ≥ 0.95, which are
common values used to deﬁne acceptable matches in recent work [17, 25, 42]. FMS is
a particularly useful measure of the eﬀectiveness of a method in relating the low-rank
structure of an approximation to that of a known model. One approach that we take
in Section 4 is to take the current approximate MLE from a very large set of computed
models. Using FMS, we estimate the probability that a method computes models that
have the same algebraic structure as the approximate MLE. We formalize this now.

For each computed solution (cid:99)Mn ∈ S, n = 1, . . . , |S|, deﬁne an indicator function
ψn((cid:99)M∗
, (cid:99)Mn) ≥ t and 0 otherwise;
i.e.,

, (cid:99)Mn, t) that is 1 when the n-th model has FMS((cid:99)M∗

(2.11)

ψn((cid:99)M∗

, (cid:99)Mn, t) =

(cid:40)

1,

0,

if FMS((cid:99)M∗
otherwise.

, (cid:99)Mn) ≥ t

We use (2.11) in our discussions below to quantify the fraction over N solves with

8

J. M. MYERS AND D. M. DUNLAVY

FMS greater than t,

(2.12)

Ψ((cid:99)M∗

, S, t) =

1
N

|S|
(cid:88)

ψn((cid:99)M∗

, (cid:99)Mn, t),

n=1
where (cid:99)Mn ∈ S, ∀n ∈ {1, . . . , |S|}.

The area under the curve (AUC) associated with this proportion, i.e., FMS when t is
greater than a threshold τ ∈ [0, 1], is

AUCFMS((cid:99)M∗

, S, τ ) =

(cid:90) t=1

t=τ

Ψ((cid:99)M∗

, S, t)dt

(2.13)

=

(cid:90) t=1

|S|
(cid:88)

1
N

ψn((cid:99)M∗

, (cid:99)Mn, t)dt,

t=τ

n=1
where (cid:99)Mn ∈ S, ∀n ∈ {1, . . . , |S|}.

AUC takes real values in [0, 1]. It can be visualized as the unit square with area 0
when the fraction of N solves with FMS greater than t = 0 is 0. We deﬁne the edge
case to be AUC with area 1, which includes the fraction of N solves with FMS equal
to 1.

3. GCP-CPAPR Hybrid Method. We develop Cyclic GCP-CPAPR (CGC), a hybrid
Poisson CP tensor method that cycles between a stochastic method to compute a model
approximation and a deterministic method to resolve the model to the best accuracy possible
at scale. We deﬁne parameterizations and cycle strategies in Section 3, which prescribe how
CGC iterates in each cycle. In Subsection 3.1 we introduce the concept of policies, automated
updates to strategies.

For simplicity of exposition, we assume access to a wrapper function cp poisson() as an

interface to diﬀerent Poisson CP solvers with the following usage:

• M = cp poisson( X, R, Minit, METHOD, OPTS ) is a Poisson CP tensor model M

with R components,

• Minit is an initial guess to M,
• METHOD speciﬁes the Poisson CP tensor algorithm,
• OPTS speciﬁes parameters used by METHOD.

Let L ∈ N be a number of cycles. Deﬁne strategy to be the L-length array of structures,

strat, specifying the following for each cycle l ∈ {1, . . . , L}:

• S method: stochastic search method, e.g. GCP-Adam.
• D method: deterministic search method, e.g. CPAPR-MU.
• S opts: stochastic search parameterization, including stochastic search budget, j,

measured in epochs.

• D opts: deterministic search parameterization, including deterministic search budget,

k, measured in iterations.
CGC iterates from an initial guess (cid:99)M(0)
deterministic search for L cycles to return a Poisson CP tensor approximation (cid:99)M(L)

via a two-stage alternation between stochastic and

. In the

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

9

for j epochs, parameterized by strat(l).S opts to return an intermediate solution,

ﬁrst stage of the l-th cycle, stochastic search method strat(l).S method is run starting from
(cid:99)M(l−1)
(cid:102)M(l)
. In the second stage, deterministic search method strat(l).D method is run to reﬁne
for k iterations, parameterized by strat(l).D opts, to return the l-th iterate, (cid:99)M(l)
(cid:102)M(l)
. The
details of CGC are given below in Algorithm 3.1. We only consider stochastic search followed
by deterministic search in each cycle and not the opposite. Since stochastic search directions
are found using estimates of the objective function from sample points, it is possible that
the algorithm converges to a minimum yet remains marked as not converged if the objective
function value is only coarsely estimated. Subsequently, it is likely that stochastic search will
move away from the optimum.

Algorithm 3.1 Cyclic GCP-CPAPR (CGC)
1: function CGC(Sparse tensor of count data X ∈ RI1×···×Id, number of components R,
number of cycles L, L-array of structures strat deﬁning L strategies, initial guess for
R-component Kruskal tensor model (cid:99)M(0)

.)

2:

3:

4:

5:

for l = 1, . . . , L do

(cid:102)M(l)
(cid:99)M(l)

← cp poisson( X, R, (cid:99)M(l−1)
← cp poisson( X, R, (cid:102)M(l)
return Kruskal tensor model (cid:99)M(L)
.

, strat(l).S method, strat(l).S opts );

, strat(l).D method, strat(l).D opts );

3.1. Cycle policies. A static policy prescribes the cycle strategies before runtime. How-
ever, we also are interested in developing a strategy that changes dynamically based on conver-
gence metrics collected at runtime, which we call an adaptive policy. We leave a comprehensive
investigation of their impact on CGC performance to future work.

4. Numerical Experiments. In this section we present the results of preliminary numerical
experiments with CGC. Subsection 4.1 compares GCP-Adam and CPAPR-MU, both using
out-of-the-box parameter settings on a real-world dataset, which motivates our inquiry into
CGC. The experiments in Subsection 4.2 use parameters for GCP-Adam and CPAPR-MU
taken from prior work [49], also on real-data. The experiments in Subsection 4.3 reﬂect
controlled experiments of CGC on synthetic data. We use SG, SC, and SH to refer to the
sets of approximations computed with GCP-Adam, CPAPR-MU, and CGC (hybrid method),
respectively.

Each experiment assumes access to either the CGC solver (i.e. Algorithm 3.1) or cp poisson
mentioned in Section 3. Additionally, each experiment assumes access to a function K =
create guess([I1, . . . , Id], R) that generates a random initial guess Kruskal tensor according
to the process described in [17, §6.1] taking inputs: 1) the row dimensions of the factor
matrices, [I1, . . . , Id], and 2) number of components, R, i.e., the column dimension for all factor
matrices in the Kruskal tensor. A reference implementation can be found in MATLAB Tensor
Toolbox [6].5 We provide the experimental setups in MATLAB-style syntax throughout.

5See https://gitlab.com/tensors/tensor toolbox/-/blob/master/create guess.m.

10

J. M. MYERS AND D. M. DUNLAVY

4.1. Default parameterization. Our ﬁrst experiment establishes baseline results for GCP-
Adam and CPAPR-MU using only default parameters. Brieﬂy, both methods are called to
compute a rank-10 CP decomposition provided the same initial guess. The stochastic search
method is set to “GCP-Adam” and the deterministic search method is set to “CPAPR-MU”;
no other method options are speciﬁed. The results are reported in Table 1 as 1) the fraction
of solves with FMS greater than 0.95 and 2) runtime. We note that CPAPR-MU converges
to the approximate MLE a higher fraction of times than GCP-Adam. We caution that the
dismal performance of GCP-Adam compared to CPAPR-MU is directly attributable to the
use of default software parameters. In private correspondence,6 the authors of GCP-Adam
recommended against the use of default parameters.

Table 1: Baseline results using GCP-Adam and CPAPR-MU on Chicago Crime data ten-
sor [56]. SG and SC are each sets of 20 approximations computed by GCP-Adam and CPAPR-
MU, respectively; Ψ((cid:99)M∗
S, S, 0.95) is the fraction of solves “equal” to the current approximate
MLE restricted to S; time (sec) is the median time in seconds to solution across all random
starts.

GCP-Adam (S = SG) CPAPR-MU (S = SG)

Ψ((cid:99)M∗
Time (sec)

S, S, 0.95)

0.00
1837

0.35
842

4.2. GCP→CPAPR. In this section, we show that computing an initial approximation
with GCP-Adam and reﬁning it with CPAPR-MU, which we refer to simply as GCP→CPAPR,
produces good approximations more eﬃciently than GCP-Adam alone. We compute rank-10
Poisson tensor approximations of the Chicago Crime [56] sparse data tensor from 20 random
starting points. The parameterizations of stochastic and deterministic search are speciﬁed as
ﬁelds in the structure strat, following the conventions provided in Section 3. The parame-
terizations for both stochastic and deterministic search methods were taken from [49]. In the
ﬁrst stage, GCP-Adam is run with initial learning rate α0 = 10−3 rate until ﬁnal learning rate
αf = 10−15. Solutions are checkpointed after the last iteration using learning rates α0 and
αf , as well as the midpoint learning rate, αm = 10−9.

The fraction of solves equal to the approximate MLE, Ψ((cid:99)M∗

S, S, 0.95) column in Table 2,
increases as GCP performs more work. This is expected behavior; see the Stage 1 column.
Since the α0 is ﬁxed across all experiments and since the stochastic learning rate is reduced
monotonically, smaller ﬁnal learning rate αf means more GCP-Adam epochs, i.e., more GCP-
Adam work. In the second stage, we pass each approximation from the ﬁrst stage to CPAPR-
MU as an initial guess and iterate until KKT-based convergence, which was set to the default
tolerance 10−4.

The fraction of solves equal to the approximate MLE after the second stage are reported
in the Stage 2 column. Beginning with a small amount of ﬁrst stage computation (learning

6T.G. Kolda, email to author, March 8, 2021 .

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

11

Table 2: Quality of GCP→CPAPR as a function of stochastic learning rate on Chicago Crime
data tensor [56] from 20 random starting points. Stage 1 refers to GCP-Adam computations;
Stage 2 refers to CPAPR-MU computations. Ψ((cid:99)M∗
S, S, 0.95) is the fraction of random starts
converging to solutions “equal” to the numerical Kruskal tensor with the lowest NLL. Time
(sec.) is the median time to solution across all random starts. We note that in these experi-
ments the GCP-Adam solver used tuned stochastic sampling values to compute better model
approximations than those in Table 1, hence the discrepancy.

Ψ((cid:99)M∗

S, S, 0.95)

Time (sec.)

Time ratio

learning rate Stage 1 Stage 2 Stage 1 Stage 2 Stage 2/Stage 1

α0 = 10−03
αm = 10−09
αf = 10−15

0.25
0.30
0.45

0.30
0.45
0.45

556
1841
2850

1342
2643
3649

2.4
1.4
1.3

= 10−3), second stage CPAPR-MU computes additional equal approximations, from 0.25 to
0.30 of solves. To achieve the 0.30 level, ﬁrst stage GCP-Adam must be run to learning rate
10−9, which requires more wall clock time (see: Stage 1 with learning rate 10−9 versus Stage 2
with learning rate 10−3). Second stage CPAPR-MU computes additional equal approximations
with ﬁrst stage initial guesses computed to this learning rate, increasing from 0.30 to 0.45
fraction of solves. Similarly, to achieve the 0.45 level, ﬁrst stage GCP-Adam must be run
to learning rate 10−15, which again requires more wall clock time than the second stage
with learning rate 10−9. Second stage CPAPR-MU ﬁnds that all ﬁrst stage approximations
computed to learning rate 10−15 have converged in its termination criterion.

The Time Ratio Stage 2/Stage 1 column reports the ratio of median wall clock times of
Stage 2 over Stage 1. As the learning rate becomes smaller, the second stage requires less
work. This scaling is promising because it is indicative of a computational cost-approximation
error trade-oﬀ that can be optimized for with a less naive parameterization. At the lowest
learning rate, 10−15, the second stage incurs computational overhead to verify the ﬁrst stage
approximations converged in its criterion, even if no CPAPR-MU iterations take place. This,
coupled with the discussion above regarding wall clock time, demonstrates a diminishing
return of performing too much GCP work in the ﬁrst stage followed by second stage CPAPR-
MU. We conclude from the above discussion that two-stage GCP→CPAPR can achieve the
same quality of solution with less work than GCP-Adam alone on real-world data with proper
parameterization.

4.3. Single-cycle, constant-work unit parameterization. The experiments in Subsec-
tion 4.2 provide cursory evidence that GCP→CPAPR can produce better results than CPAPR-
MU or GCP-Adam alone. However, we must view those experiments as purely anecdotal since
real-world data can be only hypothesized to follow the assumptions in Subsection 2.4. In this
section, we report on preliminary controlled experiments with the full CGC method described
in Section 3 using synthetic low-rank Poisson multilinear data.

The experiments that follow are restricted to Algorithm 3.1 with L = 1 cycles and a

12

J. M. MYERS AND D. M. DUNLAVY

static strategy, detailed below. The study of Algorithm 3.1 with L > 1 cycles and adaptive
strategies is left to future work. We ﬁrst describe the data in Subsection 4.3.1 and methodology
in Subsection 4.3.2. We report numerical results in Subsection 4.3.3.

4.3.1. Data. We limit our numerical experiments to a single synthetic dataset that is
suﬃciently challenging for our methods—in terms of size, sparsity, and low-rank structure—
yet small enough to support reasonable solution times. We generated a 1000 × 1000 × 1000
rank-20 tensor of non-negative integers drawn from a Poisson distribution with 98,026 non-
zero entries (approximately 0.01% dense) using the function create problem7 in MATLAB
Tensor Toolbox. The function create problem generates a “true” solution Poisson CP tensor
model, which is sampled to generate the synthetic sparse count tensor. In our experiments
below, we do not attempt to recover this model. See Subsection 2.4 for further discussion.

4.3.2. Methodology.
Out-of-sample test sets. For our experiments, we ﬁrst generated 10,000 random Poisson
CP tensor models as initial guesses using create problem. Starting from each random ini-
tial guess, we computed a rank-R Poisson CP tensor approximation using GCP-Adam and
CPAPR-MU, which were both run to the smallest reasonable tolerance of each method: ﬁnal
learning rate αf = 10−15 for GCP-Adam and KKT violation τ = 10−15 for CPAPR-MU. As
before, we denote the set of solutions in that were computed using GCP-Adam as

(4.1)

SG = {(cid:99)Mj | (cid:99)Mj computed by GCP-Adam},

the set of solutions computed using CPAPR-MU as

(4.2)

SC = {(cid:99)Mj | (cid:99)Mj computed by CPAPR-MU},

and the set of solutions computed using (hybrid) CGC as

(4.3)

SH = {(cid:99)Mj | (cid:99)Mj computed by CGC}.

CGC run. In these experiments, we ﬁx W = 100 so that the number of GCP-Adam epochs
and CPAPR-MU iterations sum to W in the ﬁrst (and only) cycle. Starting from a random
initial guess, we run Algorithm 3.1 for j GCP-Adam epochs and k CPAPR-MU iterations,
with j ∈ {0, . . . , W } and k = W − j. We refer to the combination of GCP-Adam epochs and
CPAPR-MU iterations as the CGC (j, k) pair for convenience. Note that when j = 0, CGC
with L = 1 cycles is equivalent to a single run of 100 CPAPR iterations; conversely, when
j = 100 so that k = 0, CGC with L = 1 cycles is equivalent to a single run of 100 GCP epochs.
We generated N = 100 random initial guesses, which were diﬀerent from the random initial
guesses chosen to compute the 10,000 models in each of SG and SC. We believe this aids the
statistical interpretability of our results. The set of these 10,100 CGC model approximations
is denoted SH .

4.3.3. Numerical results. We contrast CGC with GCP-Adam and CPAPR-MU alone
using the tools in Subsection 2.5. Figure 1 plots the minimum signed relative diﬀerence (2.8)

7See https://gitlab.com/tensors/tensor toolbox/-/blob/master/create problem.m.

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

13

Figure 1: Best signed relative diﬀerence in NLL (2.8) of CGC model approximations (cid:99)Mn ∈ K
for various (j, k) versus the approximate MLE, (cid:99)M∗
. The data points along the x-axis
correspond to the minimum signed relative diﬀerence in NLL ∆(n)
among all n = 1, . . . , 100
random initial guesses for each CGC (j, k) pair. Negative (blue) values indicate the best (cid:99)Mn
for a given (j, k) pair is a better minimizer than (cid:99)M∗
; positive (red) values indicate the
opposite.

SG∪SC

SG∪SC

r

SG∪SC

of the approximations for each CGC (j, k). Negative values (in blue) are models from SH
that are closer to M∗ than (cid:99)M∗
; positive values (in red) are the opposite. We see three
regimes. In the ﬁrst, the best CGC approximation is worse than the approximate MLE for
CGC with little to no work budget allocation to CPAPR. The second regime demonstrates
that some amount of stochastic search followed by deterministic search can compute Poisson
CP tensor approximations that are better minimizers of (2.5) than the approximate MLE.
This holds for stochastic work budget allocations between 4–36%. There is no clear pattern
in the third regime, where the stochastic work budget is above 36%.

0102030405060708090100#GCP-Adamepochsj-0.002-0.00100.0010.002signedrelativedi,erence1009080706050403020100#CPAPR-MUiterationsk14

J. M. MYERS AND D. M. DUNLAVY

Table 3: Estimate of probability each method computes a solution within (cid:15)-radius of approx-
imate global optimizer. The set S = SG ∪ SC ∪ SH is the set of all numerical approximations
considered in these experiments; (cid:99)M∗
S is the MLE from S.

(cid:15)

10−1
10−2
10−3
10−4
10−5
10−6

(cid:98)P ((cid:99)M∗

S, SG, (cid:15))
1.00
0.27
0
0
0
0

(cid:98)P ((cid:99)M∗

S, SC, (cid:15))
1.00
0.69
0.05
< 0.01
0
0

(cid:98)P ((cid:99)M∗

S, SH , (cid:15)) best CGC (j, k) pair

1.00
0.65
0.16
0.13
0.03
0.01

all
(0, 100)
(1, 99)
(4, 96)
(8, 92)
(8, 92)

Our second result considers whether CGC ﬁnds a good approximation to the global opti-
mizer with the same accuracy or better than GCP-Adam or CPAPR-MU. In Table 3 we report
the fraction of model approximations near to the approximate global optimizer, computed us-
ing (2.9). For CGC results, we compute (cid:98)P ((cid:99)M∗
S, SH , (cid:15)) for each combination of GCP-Adam
epochs j and CPAPR-MU iterations k across N = 100 random initializations, but report
only the maximum of (2.9) among all combinations. The rightmost column lists the corre-
sponding best (j, k) pair. Within radius (cid:15) = 10−2, CPAPR-MU has the highest probability
of computing solutions with NLL close to the f (X, (cid:99)M∗
S). Note that the best CGC result for
this level of (cid:15) is equivalent to CPAPR-MU since the GCP-Adam work allocation is zero, i.e.,
(j, k) = (0, 100). The lower CGC proportion in Table 3 (0.65 versus 0.69) is likely attributed
to either 1) smaller computational budget of the CGC run versus CPAPR-MU run in the out-
of-sample test (100 vs. 1000 iterations) or 2) higher variance from fewer multi-starts in the
CGC run versus the out-of-sample test (100 vs. 10000 multi-starts). With radius (cid:15) ≤ 10−3,
CGC computes Poisson CP tensor approximations with NLL closest to f (X, (cid:99)M∗
S) with the
highest probability. An interesting trend is that as the radius (cid:15) gets smaller, the best CGC
parameterization corresponds to increasing the allocation to GCP steps. We will see similar
patterns in the analyses that follow.

Lastly, we evaluate CGC as a method to approximate the global optimizer in terms of the
algebraic ﬁt of CGC models to the MLE using measures deﬁned by equations (2.11)–(2.13).
For each constant work unit allocation (j, k), we compute the fraction over N solves with FMS
greater than t, Ψ((cid:99)M∗
S, SH , t), with t ∈ [0, 1]. Since all curves showed the same behavior for
t < 0.5, we plot values for t ∈ [0.5, 1] in Figure 2, together with the same values computed for
GCP-Adam and CPAPR-MU. A detailed view of the same data is presented in Figure 3, which
partitions the data by the ﬁrst 11 (j, k) pairs (Figure 3a), the 79 middle pairs (Figure 3b),
and the last 11 pairs (Figure 3c). As described earlier, the diﬀerences in the plots Figure 2–
Figure 3 between CGC with (j, k) = (0, 100) and CPAPR-MU (blue triangles) and CGC with
(j, k) = (100, 0) and GCP-Adam (green circles) is mainly a function of the work constraint
of CGC. CGC was constrained to 100 work units in both cases, whereas in the out-of-sample
test that generated the GCP-Adam and CPAPR-MU results, GCP-Adam ran for a maximum

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

15

of 10,000 epochs and CPAPR-MU ran for a maximum of 1000 iterations. We call out this
distinction to highlight the results in these ﬁgures. Especially in Figure 3a and Figure 3b, we
see a higher proportion of CGC solutions equal to the approximate MLE than GCP-Adam
or CPAPR-MU despite the computational constraint. Thus, these ﬁgures provide evidence
that some amount of stochastic search followed by deterministic search can ﬁnd models with
superior ﬁt and higher accuracy than GCP-Adam and CPAPR-MU by themselves. This
conclusion is particularly clear for scores in the range ≥ 0.95.

Figure 2: Factor match scores between CP models computed with CGC, CPAPR-MU, and
GCP-Adam and the approximate global optimizer, (cid:99)M∗
S. The dash-dot gray vertical lines and
dotted black vertical lines denote the levels of “similar” and “equal” described in [46].

We also compute the area under each curve (2.13) in Figure 2 as a metric that estimates
the probability of ﬁnding a model with good algebraic ﬁt to the MLE from the out-of-sample
test set. Figure 4 summarizes model accuracy as the area under the curves in Figure 2–
Figure 3 for “similar” and “equal” ﬁts. We can conclude that CGC has a higher probability
of computing approximations that are equal to the MLE for a range of GCP-CPAPR work
budget allocations than GCP-Adam or CPAPR-MU alone.

0.50.60.70.80.91t00.20.40.60.81*(^M$S;K;t)FractionoverNsolveswithFMS>tCGC,K=SHGCP-Adam,K=SGCPAPR-MU,K=SC020406080100j16

J. M. MYERS AND D. M. DUNLAVY

(a) First 11 pairs.

(b) Middle 79 pairs.

(c) Last 11 pairs.

Figure 3: Detail of curves in Figure 2. Colormaps scaled for clarity.

0.50.60.70.80.91t00.20.40.60.81*(^M$S;K;t)FractionoverNsolveswithFMS>tCGC,K=SHGCP-Adam,K=SGCPAPR-MU,K=SC012345678910j0.50.60.70.80.91t00.20.40.60.81*(^M$S;K;t)FractionoverNsolveswithFMS>tCGC,K=SHGCP-Adam,K=SGCPAPR-MU,K=SC1220273543515866748189j0.50.60.70.80.91t00.20.40.60.81*(^M$S;K;t)FractionoverNsolveswithFMS>tCGC,K=SHGCP-Adam,K=SGCPAPR-MU,K=SC90919293949596979899100jA HYBRID METHOD FOR TENSOR DECOMPOSITIONS

17

Figure 4: For each CGC (j, k) pair, areas under the curve AUCFMS((cid:99)M∗
S, SH , 0.85) (left) and
AUCFMS((cid:99)M∗
S, SH , 0.95) (right). The area under the CPAPR and GCP curves across all 10,000
random starts are displayed as horizontal lines. All curves in both plots are normalized by
the area of the ﬁt level (0.15 and 0.05, respectively).

5. Conclusions and Future Work. The results in Section 4 beg for further inquiry into
CGC. Our primary conclusion is CGC can minimize low-rank approximation error while re-
ducing computational costs. We saw that CGC can reduce approximation error with high
accuracy relative to GCP-Adam and CPAPR-MU. Additionally, since CGC was run in our
experiments with a far stricter computational budget than GCP-Adam and CPAPR-MU—
CGC was constrained to 100 total CGC work units whereas GCP-Adam and CPAPR-MU
runs were allowed for far longer to run to very low convergence tolerance—we argue that
CGC is more computationally eﬃcient. The implication is that the performance gain allows
even more multi-starts, and subsequently, a greater number of high accuracy approximations.
Motivated by these observations, we propose to explore the following ideas.

5.1. Parameterizing CGC. How to parameterize CGC in such a way that model ap-
proximation error be reduced and computational cost be optimized further? Our numerical
experiments showed promising results despite a naive strategy. We will extend our ideas to
running CGC with L > 1 cycles to study the eﬀect of adaptive versus static per-cycle strate-
gies. A goal is to condition the strategy updates on convergence metrics, which will require
deeper understanding of both stochastic search and deterministic reﬁnement.

5.2. Comparison with other black-box methods. CGC is similar in spirit to Simulated
Annealing [38]. In a hybrid simulated annealing method [21], stochastic search “heats” the
iterative path away from local minima and deterministic search “cools” the iterative path
towards the global optimum. In addition to comparisons with GCP-Adam and CPAPR-MU,
we believe a direct comparison to Simulated Annealing, and potentially other global optimiza-
tion methods, is necessary. This can be done easily using the standard Simulated Annealing

020406080100#GCP-Adamepochsj00.050.10.150.20.250.30.35AUCCGCCPAPR-MUGCP-Adam100806040200#CPAPR-MUiterationsk020406080100#GCP-Adamepochsj00.050.10.150.20.250.30.35AUCCGCCPAPR-MUGCP-Adam100806040200#CPAPR-MUiterationsk18

J. M. MYERS AND D. M. DUNLAVY

method from MATLAB Global Optimization Toolbox [59] which is an implementation of
Adaptive Simulated Annealing [36].

Acknowledgments. We thank Eric Phipps of Sandia National Laboratories for assistance

with Genten, a high-performance GCP solver.

REFERENCES

[1] E. Acar, D. M. Dunlavy, and T. G. Kolda, A scalable optimization approach for ﬁtting canonical

tensor decompositions, Journal of Chemometrics, 25 (2011), pp. 67–86.

[2] E. Acar, T. G. Kolda, and D. M. Dunlavy, All-at-once Optimization for Coupled Matrix and Tensor

Factorizations, May 2011, https://arxiv.org/abs/1105.3422v1.

[3] C. Andersen and R. Bro, The N-way Toolbox for Matlab, Chemometrics and Intelligent Laboratory

Systems, 52 (2000), pp. 1–4, https://doi.org/10.1016/S0169-7439(00)00071-X.

[4] C. Andersen and R. Bro, Practical aspects of PARAFAC modeling of ﬂuorescence excitation-emission

data, Journal of Chemometrics, 17 (2003), pp. 200–215, https://doi.org/10.1002/cem.790.

[5] B. Bader and T. Kolda, Eﬃcient MATLAB Computations with Sparse and Factored Tensors, SIAM

Journal on Scientiﬁc Computing, 30 (2008), pp. 205–231.

[6] B. W. Bader, T. G. Kolda, et al., MATLAB Tensor Toolbox Version 3.0-dev, Aug. 2017.
[7] M. Baskaran, T. Henretty, and J. Ezick, Fast and Scalable Distributed Tensor Decompositions, in
2019 IEEE High Performance Extreme Computing Conference (HPEC), Waltham, MA, USA, Sept.
2019, IEEE, pp. 1–7, https://doi.org/10.1109/HPEC.2019.8916319.

[8] M. Baskaran, T. Henretty, B. Pradelle, M. H. Langston, D. Bruns-Smith, J. Ezick, and
R. Lethin, Memory-eﬃcient parallel tensor decompositions, in 2017 IEEE High Performance Extreme
Computing Conference (HPEC), 2017, pp. 1–7.

[9] M. Baskaran, D. Leggas, B. von Hofe, M. H. Langston, J. Ezick, and P.-D. Letourneau,
ENSIGN. [Computer Software] https://doi.org/10.11578/dc.20220120.1, Jan. 2022, https://doi.org/
10.11578/dc.20220120.1.

[10] M. Baskaran, B. Meister, and R. Lethin, Low-overhead load-balanced scheduling for sparse tensor
computations, in 2014 IEEE High Performance Extreme Computing Conference (HPEC), Waltham,
MA, USA, Sept. 2014, IEEE, pp. 1–6, https://doi.org/10.1109/HPEC.2014.7041006.

[11] M. M. Baskaran, T. Henretty, J. Ezick, R. Lethin, and D. Bruns-Smith, Enhancing Network
Visibility and Security through Tensor Analysis, Future Generation Computer Systems, 96 (2019),
pp. 207–215.

[12] J. A. Bazerque, G. Mateos, and G. B. Giannakis, Inference of Poisson count processes using low-
rank tensor data, in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing,
2013, pp. 5989–5993, https://doi.org/10.1109/ICASSP.2013.6638814.

[13] R. Bro, Multiway Analysis in the Food Industry. Models, Algorithms and Applications, PhD thesis,

University of Amsterdam, 1998.

[14] B. W. B. B.W, M. W. Berry, and M. Browne, Survey of Text Mining II, Springer, London, 2008,

ch. Discussion Tracking in Enron Email Using PARAFAC.

[15] J. D. Carroll and J.-J. Chang, Analysis of individual diﬀerences in multidimensional scaling via an

n-way generalization of “Eckart-Young” decomposition, Psychometrika, 35 (1970), pp. 283–319.

[16] P. A. Chew, B. W. Bader, T. G. Kolda, and A. Abdelali, Cross-language Information Retrieval
using PARAFAC2, in KDD ’07: Proceedings of the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM, 2007, pp. 143–152.

[17] E. C. Chi and T. G. Kolda, On Tensors, Sparsity, and Nonnegative Factorizations, SIAM Journal on

Matrix Analysis and Applications, 33 (2012), pp. 1272–1299.

[18] D. M. Dunlavy, T. G. Kolda, and E. Acar, Temporal Link Prediction using Matrix and Tensor

Factorizations, ACM Transactions on Knowledge Discovery from Data, 5 (2011), p. 10 (27 pages).

[19] D. M. Dunlavy, T. G. Kolda, and W. P. Kegelmeyer, Multilinear Algebra for Analyzing Data with
Multiple Linkages, in Graph Algorithms in the Language of Linear Algebra, J. Kepner and J. Gilbert,
eds., Fundamentals of Algorithms, SIAM, 2011, pp. 85–114.

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

19

[20] H. C. Edwards, C. R. Trott, and D. Sunderland, Kokkos: Enabling manycore performance porta-
bility through polymorphic memory access patterns, Journal of Parallel and Distributed Computing,
74 (2014), pp. 3202–3216.

[21] M. EL-Alem, A. Aboutahoun, and S. Mahdi, Hybrid gradient simulated annealing algorithm for
ﬁnding the global optimal of a nonlinear unconstrained optimization problem, Soft Computing, 25
(2021), pp. 2325–2350, https://doi.org/10.1007/s00500-020-05303-x.

[22] J. Ezick, T. Henretty, M. Baskaran, R. Lethin, J. Feo, T.-C. Tuan, C. Coley, L. Leonard,
R. Agrawal, B. Parsons, and W. Glodek, Combining Tensor Decompositions and Graph Analyt-
ics to Provide Cyber Situational Awareness at HPC Scale, in 2019 IEEE High Performance Extreme
Computing Conference (HPEC), 2019, pp. 1–7.

[23] M. P. Friedlander and K. Hatz, Computing non-negative tensor factorizations, Optimization Methods

and Software, 23 (2008), pp. 631–647, https://doi.org/10.1080/10556780801996244.

[24] A. Gy\:orgy and L. Kocsis, Eﬃcient Multi-Start Strategies for Local Search Algorithms, Journal of

Artiﬁcial Intelligence Research, 41 (2011), pp. 705–720.

[25] S. Hansen, T. Plantenga, and T. G. Kolda, Newton-based optimization for Kullback–Leibler non-
negative tensor factorizations, Optimization Methods and Software, 30 (2015), pp. 1002–1029.
[26] R. A. Harshman, Foundations of the PARAFAC procedure: Models and conditions for an “explanatory”

multi-modal factor analysis, UCLA Working Papers in Phonetics, 16 (1970), pp. 1–84.

[27] J. Henderson, J. C. Ho, A. N. Kho, J. C. Denny, B. A. Malin, J. Sun, and J. Ghosh, Granite:
Diversiﬁed, Sparse Tensor Factorization for Electronic Health Record-Based Phenotyping, in 2017
IEEE International Conference on Healthcare Informatics (ICHI), 2017, pp. 214–223, https://doi.
org/10.1109/ICHI.2017.61.

[28] T. Henretty, M. Baskaran, J. Ezick, D. Bruns-Smith, and T. A. Simon, A quantitative and
qualitative analysis of tensor decompositions on spatiotemporal data, in 2017 IEEE High Performance
Extreme Computing Conference (HPEC), 2017, pp. 1–7.

[29] T. S. Henretty, M. H. Langston, M. Baskaran, J. Ezick, and R. Lethin, Topic modeling for analy-
sis of big data tensor decompositions, in Disruptive Technologies in Information Sciences, vol. 10652,
2018, pp. 52–64.

[30] J. C. Ho, J. Ghosh, S. R. Steinhubl, W. F. Stewart, J. C. Denny, B. A. Malin, and J. Sun,
Limestone: High-throughput candidate phenotype generation via tensor factorization, Special Section:
Methods in Clinical Research Informatics, 52 (2014), pp. 199–211, https://doi.org/10.1016/j.jbi.2014.
07.001.

[31] J. C. Ho, J. Ghosh, and J. Sun, Marble: High-Throughput Phenotyping from Electronic Health Records
via Sparse Nonnegative Tensor Factorization, in Proceedings of the 20th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA, 2014,
Association for Computing Machinery, pp. 115–124, https://doi.org/10.1145/2623330.2623658.

[32] D. Hong, T. G. Kolda, and J. A. Duersch, Generalized Canonical Polyadic Tensor Decomposition,

SIAM Review, 62 (2020), pp. 133–163.

[33] C. Hu, P. Rai, and L. Carin, Zero-Truncated Poisson Tensor Factorization of Massive Binary Tensors,

Aug. 2015.

[34] C. Hu, P. Rai, C. Chen, M. Harding, and L. Carin, Scalable Bayesian Non-negative Tensor Fac-
torization for Massive Count Data, in Machine Learning and Knowledge Discovery in Databases,
A. Appice, P. P. Rodrigues, V. Santos Costa, J. Gama, A. Jorge, and C. Soares, eds., Cham, 2015,
Springer International Publishing, pp. 53–70.

[35] K. Huang and N. D. Sidiropoulos, Kullback-Leibler principal component for tensors is not NP-hard,
in 2017 51st Asilomar Conference on Signals, Systems, and Computers, 2017, pp. 693–697, https:
//doi.org/10.1109/ACSSC.2017.8335432.

[36] L. Ingber, Adaptive simulated annealing (ASA): Lessons learned, Control Cybernetics, 25 (1996), pp. 33–

54.

[37] D. P. Kingma and J. Ba, Adam: A Method for Stochastic Optimization, in 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Y. Bengio and Y. LeCun, eds., 2015.

[38] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, Optimization by simulated annealing, SCIENCE,

220 (1983), pp. 671–680.

20

J. M. MYERS AND D. M. DUNLAVY

[39] T. Kolda and B. Bader, The TOPHITS Model for Higher-order Web Link Analysis, in Proceedings of

Link Analysis, Counterterrorism and Security 2006, 2006.

[40] T. Kolda, B. Bader, and J. Kenny, Higher-order Web link analysis using multilinear algebra, in Fifth
IEEE International Conference on Data Mining (ICDM’05), 2005, pp. 8 pp.–, https://doi.org/10.
1109/ICDM.2005.77.

[41] T. G. Kolda and B. W. Bader, Tensor Decompositions and Applications, SIAM Review, 51 (2009),

pp. 455–500.

[42] T. G. Kolda and D. Hong, Stochastic Gradients for Large-Scale Tensor Decomposition, SIAM Journal

on Mathematics of Data Science, 2 (2020), pp. 1066–1095.

[43] B. Korth and L. R. Tucker, The distribution of chance congruence coeﬃcients from simulated data,

Psychometrika, 40 (1975), pp. 361–372.

[44] B. Korth and L. R. Tucker, Procrustes matching by congruence coeﬃcients, Psychometrika, 41 (1976),

pp. 531–535.

[45] P.-D. Letourneau, M. Baskaran, T. Henretty, J. Ezick, and R. Lethin, Computationally Eﬃcient
CP Tensor Decomposition Update Framework for Emerging Component Discovery in Streaming Data,
in 2018 IEEE High Performance Extreme Computing Conference (HPEC), 2018, pp. 1–8.
[46] U. Lorenzo-Seva and J. M. F. ten Berge, Tucker’s Congruence Coeﬃcient as a Meaningful Index of

Factor Similarity, Methodology, 2 (2006), pp. 57–64, https://doi.org/10.1027/1614-2241.2.2.57.

[47] R. Mart´ı, P. Pardalos, and M. Resende, Handbook of Heuristics, Springer International Publishing,

Aug. 2018, https://doi.org/10.1007/978-3-319-07124-4.

[48] J. Mocks, Topographic components model for event-related potentials and some biophysical considerations,
IEEE Transactions on Biomedical Engineering, 35 (1988), pp. 482–484, https://doi.org/10.1109/10.
2119.

[49] J. M. Myers and D. M. Dunlavy, Using Computation Eﬀectively for Scalable Poisson Tensor Fac-
torization: Comparing Methods Beyond Computational Eﬃciency, in 2021 IEEE High Performance
Extreme Computing Conference, HPEC 2020, Waltham, MA, USA, September 21-25, 2020, IEEE,
2021, pp. 1–7, https://doi.org/10.1109/HPEC49654.2021.9622795.

[50] J. M. Myers, D. M. Dunlavy, K. Teranishi, and D. S. Hollman, Parameter Sensitivity Analysis
of the SparTen High Performance Sparse Tensor Decomposition Software, in 2020 IEEE High Perfor-
mance Extreme Computing Conference, HPEC 2020, Waltham, MA, USA, September 21-25, 2020,
IEEE, 2020, pp. 1–7, https://doi.org/10.1109/HPEC43674.2020.9286210.

[51] I. J. Myung, Tutorial on maximum likelihood estimation, Journal of Mathematical Psychology, 47 (2003),

pp. 90–100, https://doi.org/10.1016/S0022-2496(02)00028-7.

[52] A.-H. Phan, P. Tichavsk´y, and A. Cichocki, Low Complexity Damped Gauss–Newton Algorithms for
CANDECOMP/PARAFAC, SIAM Journal on Matrix Analysis and Applications, 34 (2013), pp. 126–
147, https://doi.org/10.1137/100808034.

[53] E. T. Phipps and T. G. Kolda, Software for Sparse Tensor Decomposition on Emerging Computing

Architectures, SIAM Journal on Scientiﬁc Computing, 41 (2019), pp. C269–C290.

[54] P. Rai, C. Hu, M. Harding, and L. Carin, Scalable Probabilistic Tensor Factorization for Binary and
Count Data, in Proceedings of the 24th International Conference on Artiﬁcial Intelligence, IJCAI’15,
AAAI Press, 2015, pp. 3770–3776.

[55] T. M. Ranadive and M. M. Baskaran, An All–at–Once CP decomposition method for count tensors,

2021 IEEE High Performance Extreme Computing Conference (HPEC), (2021), pp. 1–8.

[56] S. Smith, J. W. Choi, J. Li, R. Vuduc, J. Park, X. Liu, and G. Karypis, FROSTT: The Formidable

Repository of Open Sparse Tensors and Tools, 2017.

[57] M. Sugiyama, H. Nakahara, and K. Tsuda, Legendre Decomposition for Tensors, in Advances in
Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, eds., vol. 31, Curran Associates, Inc., 2018, https://proceedings.neurips.cc/
paper/2018/ﬁle/56a3107cad6611c8337ee36d178ca129-Paper.pdf.

[58] K. Teranishi, D. M. Dunlavy, J. M. Myers, and R. F. Barrett, SparTen: Leveraging Kokkos
for On-node Parallelism in a Second-Order Method for Fitting Canonical Polyadic Tensor Models
to Poisson Data, in 2020 IEEE High Performance Extreme Computing Conference (HPEC), 2020,
pp. 1–7, https://doi.org/10.1109/HPEC43674.2020.9286251.

[59] I. The MathWorks, Symbolic Math Toolbox, Natick, Massachusetts, United State, 2019, https://www.

A HYBRID METHOD FOR TENSOR DECOMPOSITIONS

21

mathworks.com/help/symbolic/.

[60] M. Vandecappelle, N. Vervliet, and L. D. Lathauwer, A second-order method for ﬁtting the
canonical polyadic decomposition with non-least-squares cost, IEEE Transactions on Signal Processing,
68 (2020), pp. 4454–4465, https://doi.org/10.1109/TSP.2020.3010719.

[61] N. Vervliet, O. Debals, and L. De Lathauwer, Tensorlab 3.0 — Numerical optimization strategies
for large-scale constrained and coupled matrix/tensor factorization, in 2016 50th Asilomar Confer-
ence on Signals, Systems and Computers, 2016, pp. 1733–1738, https://doi.org/10.1109/ACSSC.
2016.7869679.

[62] S. J. Wright, Coordinate descent algorithms, Mathematical Programming, 151 (2015), pp. 3–34, https:

//doi.org/10.1007/s10107-015-0892-3.

