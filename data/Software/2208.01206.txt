2
2
0
2

g
u
A
4

]

G
L
.
s
c
[

2
v
6
0
2
1
0
.
8
0
2
2
:
v
i
X
r
a

FAST KERNEL DENSITY ESTIMATION WITH DENSITY
MATRICES AND RANDOM FOURIER FEATURES ∗

Joseph A. Gallego M., Juan F. Osorio, Fabio A. González
MindLab
Universidad Nacional de Colombia
Bogotá, Colombia
{jagallegom,josorior,fagonzalezo}@unal.edu.co

ABSTRACT

Kernel density estimation (KDE) is one of the most widely used nonparametric density estimation
methods. The fact that it is a memory-based method, i.e., it uses the entire training data set for
prediction, makes it unsuitable for most current big data applications. Several strategies, such as
tree-based or hashing-based estimators, have been proposed to improve the efﬁciency of the kernel
density estimation method. The novel density kernel density estimation method (DMKDE) uses
density matrices, a quantum mechanical formalism, and random Fourier features, an explicit kernel
approximation, to produce density estimates. This method has its roots in the KDE and can be
considered as an approximation method, without its memory-based restriction. In this paper, we
systematically evaluate the novel DMKDE algorithm and compare it with other state-of-the-art fast
procedures for approximating the kernel density estimation method on different synthetic data sets.
Our experimental results show that DMKDE is on par with its competitors for computing density
estimates and advantages are shown when performed on high-dimensional data. We have made all
the code available as an open source software repository.

Keywords density matrix · random Fourier features · kernel density estimation · approximations of
kernel density estimation · quantum machine learning

1

Introduction

In many applications we have a ﬁnite set of data and we would like to know what probability distribution has generated
the data. From the point of view of statistical inference, this problem has played a central role in research and has
inspired many methods that are based on the use of the density function. Also in machine learning there are many
methods base on density estimation, such as anomaly detection methods [15], generative models [14], agglomerative
clustering [18], spatial analysis [4], sequence-to-sequence models [22], among others.

Making certain assumptions on the probability model that generated the data leads to parametric estimation. Another
common approach is non-parametric estimation. The most representative non-parametric method is called Kernel
Density Estimation (KDE) [21, 25] and can be understood as a weighted sum of density contributions that are centered
on each data point. In this method, one has to choose a function called kernel and a smoothing parameter that controls
the dispersion of the estimate. Given n as the number of training data points, direct evaluation of the KDE on m test
data points requires O(mn) kernel evaluations and O(mn) additions and multiplications, i.e., if the number of training
n is sufﬁciently large, similar to the number of m test points, its complexity is quadratic. This makes it a very expensive
process, especially for large data sets and higher dimensions as stated in [9]. One approach to the problem of scalability
of KDE focuses on ﬁnding a fast approximate kernel evaluation. According to [27], we can identify three main lines of
work: space partitioning methods, random sampling, and hashing-based estimators. In the Subsection 3 we elaborate on
each type of approximation. In [8], a novel KDE approach using random Fourier features and density matrices was
proposed, which appears to be a promising way to scale up the KDE. This method uses an explicit Gaussian kernel

∗Citation: Joseph et al., Fast Kernel Density Estimation with Density Matrices and Random Fourier Features.

 
 
 
 
 
 
Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

approximation and a density matrix to produce density estimates. A more detailed explanation can be found in the
subsection 2.3.

The goal of this paper is to compare, within a statistical experimental setup, several fast KDE implementations that
are based on famous theoretical approaches for kernel density estimation approximation, including DMKDE, all the
code is available as an open source software repository [7]. The paper is organized as follows: Section 2 covers
the background of random features, kernel density estimation; Section 3 presents some methods for kernel density
estimation approximation; Section 4 presents the experimental setup and systematic evaluation, where seven synthetic
data sets are used to compare DMKDE against other KDE approximation methods. This section shows that DMKDE
outperformed other KDE approximation methods in terms of time consumed to make new predictions; ﬁnally, Section 5
discusses the conclusions of the paper and future research directions.

2 Background and Related Work

2.1 Kernel Density Estimation

The multivariate kernel density estimator at a query point x ∈ Rd for a given random sample X = {xi}n
the number of samples drawn from an unknown density f , is given by

i=1, where n is

ˆf (x) =

1
n

n
(cid:88)

i=1

|h|−1/2k

(cid:16)

(cid:17)
h−1/2 (x − xi)

(1)

where H is the d × d bandwidth matrix, which is positive deﬁnite and symmetric and k : Rd → R≥0 is the kernel
function. Deﬁning kh(u) = |h|−1/2k (cid:0)h−1/2u(cid:1) we can write 1 as

ˆf (x) =

1
n

n
(cid:88)

i=1

kh (x − xi)

(2)

Throughout the paper we will work with the rescaled multivariate KDE version written in the Equation 2. We will also
assume that h = σ2Id where σ2 represents the bandwidth in each dimension, Id is the d × d identity matrix, and the
kernel will be Gaussian expressed by

Kγ(x, y) = (2π)−d/2 exp (cid:0)−γ||x − y||2(cid:1) .

where γ = 1/(2σ2).

All of these assumptions lead to the functional form

ˆfγ,X (x) =

1
n(π/γ) d

2

n
(cid:88)

i=1

e−γ(cid:107)x−xi(cid:107)2

(3)

(4)

of the KDE estimator for a query point x ∈ Rd where we deﬁne γ = 1
2σ .

Kernel density estimation has multiple applications some examples include: to visualize clusters of crime areas using
it as a spatio-temporal modiﬁcation [18]; to assess injury-related trafﬁc accidents in London, UK [1], to generate the
intensity surface in a spatial environment for moving objects couple with time geography [6]; to make a spatial analysis
of city noise with information collected from a French mobile application installed on citizens’ smartphones using their
GPS location data [10]; to generate new samples from a given dataset by dealing with unbalanced datasets [11]; to
process the image by subtracting the background in a pixel-based method [13]; to propose an end-to-end pipeline for
classiﬁcation [12]; to propose a classiﬁcation method using density estimation and random Fourier features [8].

2.2 Random Fourier Features

Using random Fourier features (RFF) to approximate kernels was initially presented in [23]. In this method, the authors
approximate a shift invariant kernel by an inner product in an explicit Hilbert spaces. Formally, given a shift invariant
kernel k : Rd × Rd → R they build a map φrff : Rd → RD such that

k(x, y) ≈ φrff(x)∗φrff(y)

(5)

2

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

for all {x, y} ⊆ Rd, and in the case of the Gaussian kernel the mapping is deﬁned as follows

√

(6)
where w is sampled from N (0, ID) and b is samples from Uniform(0,2π). The main theoretical result supporting 5
comes from an instance of Bochner’s theorem [26] which states that a shift invariant continuous kernel is the Fourier
transform of a nonnegative probability measure. This methodology suggests that we can compute shift-invariant kernel
approximations via a sampling strategy.

φrff(x) =

2 cos(ω∗x + b)

2.3 Quantum Kernel Density Estimation Approximation

The central idea of the method density matrix kernel density estimation (DMKDE), introduced by [8] and systematically
evaluated in [16], is to use density matrices along with random Fourier features to represent arbitrary probability
distributions by addressing the important question of how to encode probability density functions in Rn into density
matrices. The overall process is divided into a training and a testing phases. The training phase is deﬁned as follows:

1. Input: a set of n d-dimensional samples x1, · · · , xn, D number of random Fourier features and a bandwidth

parameter γ ∈ R

2. Sampling of the vector w and b as explained in Subsection 2.2
3. Mapping: compute the random Fourier feature vector for each data point as explained in Equation 6.
4. xi as zi = φrff(xi)
5. Compute a density matrix as ρ = 1
n

i=1 zizT
i

(cid:80)n

The testing phase is as follows:

1. Apply step 3 to each testing data point.
2. The density estimation of a testing point x is calculate using the Born’s rule

where the normalizing constant is: Z = (π/(2γ))d/2

ˆfρ(x) =

φrff(x)T ρφrff(x)
Z

Note that this algorithm does not need to store each training data point, but a square matrix whose dimensions are
equal to D × D sufﬁces computed in step 5. Moreover, the time consumed in estimating a new data point does not
depend on the training size. The ρ-density matrix is one of the main building blocks used in quantum physics to capture
classical and quantum probability in a given physical system. This formalism was conceived by [28] as the foundation
of quantum statistical mechanics. The density matrix describes the states of the quantum system and explains the
relationship between the pure state and the mixed states of the system. Deﬁne the quantum state of a system in pure state
as ψ. Then the ρ-density matrix for the pure state ψ is deﬁned as ρ := |ψ(cid:105)(cid:104)ψ|. Let us now consider a quantum ensemble
system of n systems (objects) that are not in the same state. Let pi := ni/n where ni is the number of systems that are
in state |ψi(cid:105) and (cid:80) ni = n . Therefore, the mixed density matrix is deﬁned as ρmix = (cid:80)
A matrix factorization of the ρ-matrix simpliﬁes the computation of the matrix as ρ = V ∗ΛV , where V ∈ Rr×D, Λ ∈
Rr×r is a diagonal matrix and r < D is the reduced rank of the factorization. Thus, using this matrix factorization, the
method called DMKDE-SGD is expressed as: ˆfρ = 1
Z ||Λ1/2V φrff(x)||2. This method reduces the time required for a
new density to O(Dr).

i piρpure

i |ψi(cid:105)(cid:104)ψi|.

= (cid:80)

i

3 KDE Approximation Methods

Fast density estimation methods are of paramount importance in several applications, as shown above in 2.1. There
are four main approaches for fast kernel density estimation approximation. First, space partitioning methods do not
work well in high dimensions and make use of geometric data structures to partition the space, and achieve speedup
by limiting the contribution of data points to the kernel density using partitions. Second, random sampling focuses
on randomly sampling the kernel density and gives good results in high dimensions. Third, the most recent methods
and the most promising state-of-the-art tools are hash-based estimators for kernel density estimation, in which a hash
structure is used to construct close distance boxes that allow the calculation of few distances in the prediction step [2].
Here, a sampling scheme is used, where points are sorted into buckets thanks to a hash function whose main objective is
to send similar objects to the same hash value. Finally, the novel DMKDE as explained above in 2.3 can approximate
kernel density estimation. The following algorithms including DMKDE are used in the experimental setup.

3

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

• Tree kernel density estimation (TREEKDE): the density approximation is obtained using the partitioning of
the space through recursive segmentations of the space into smaller sections. By obtaining the partition, we
can approximate the kernel of a speciﬁc point by traversing the tree, without having to explicitly evaluate the
kernel function [17].

• Kernel Density Estimation using k-dimensional Tree (KDEKDT): this method addresses the k-nearest neighbor
problem using a kd-tree structure that generalizes two-dimensional Quad-trees and three-dimensional Oct-trees
to an arbitrary number of dimensions. Internally, it is a binary search tree that partitions the data into nested
orthotopic regions that are used to approximate the kernel at a speciﬁc point [3].

• Kernel Density using Ball Tree (KDEBT): in this method the Ball Tree structure is used to avoid the inefﬁcien-
cies of KD trees in high dimensionality spaces. In these spaces, the Ball Tree divides the space into nested
hyperspheres [19].

4 Experimental Evaluation

In this section, we systematically assess the performance of DMKDE on various synthetic data sets and compare it with
kernel density estimation approximation methods.

4.0.1 Data sets and experimental setup

We used seven synthetic data sets to evaluate DMKDE against kernel density approximation methods. The data sets are
characterized as follows:

• The data set Arc corresponds to a two-dimensional random sample drawn from a random vector X = (X1, X2)

with probability density function given by

f (x1, x2) = N (x2|0, 4)N (x1|0.25x2

2, 1)

where N (u|µ, σ2) denotes the density function of a normal distribution with mean µ and variance σ2. [20]
used this data set to evaluate his neural density estimation methods.

• The data set Potential 1 corresponds to a two-dimensional random sample drawn from a random vector

X = (X1, X2) with probability density function given by

f (x1, x2) =

1
2

(cid:18) ||x|| − 2
0.4

(cid:19)2

− ln

(cid:18)

exp

(cid:26)

−

1
2

(cid:20) x1 − 2
0.6

(cid:21)2(cid:27)

+ exp

(cid:26)

−

1
2

(cid:20) x1 + 2
0.6

(cid:21)2(cid:27)(cid:19)

with a normalizing constant of approximately 6.52 calculated by Monte Carlo integration.

• The data set Potential 2 corresponds to a two-dimensional random sample drawn from a random vector

X = (X1, X2) with probability density function given by

f (x1, x2) =

(cid:20) x2 − w1(x)
0.4

1
2

(cid:21)2

where w1(x) = sin ( 2πx1
integration.

4 ) with a normalizing constant of approximately 8 calculated by Monte Carlo

• The data set Potential 3 corresponds to a two-dimensional random sample drawn from a random vector

X = (X1, X2) with probability density function given by

f (x1, x2) = − ln

exp

−

(cid:32)

(cid:40)

1
2

(cid:20) x2 − w1(x)
0.35

(cid:21)2(cid:41)

(cid:40)

+ exp

−

(cid:34)

1
2

x2 − w1(x) + w2(x)
0.35

2(cid:35)(cid:41)(cid:33)

where w1(x) = sin ( 2πx1
13.9 calculated by Monte Carlo integration.

4 ) and w2(x) = 3 exp

(cid:110)

− 1
2

(cid:2) x1−1
0.6

(cid:3)2(cid:111)

with a normalizing constant of approximately

• The data set Potential 4 corresponds to a two-dimensional random sample drawn from a random vector

X = (X1, X2) with probability density function given by

f (x1, x2) = − ln

exp

−

(cid:32)

(cid:40)

1
2

(cid:21)2(cid:41)

(cid:40)

+ exp

−

(cid:34)

1
2

x2 − w1(x) + w3(x)
0.35

2(cid:35)(cid:41)(cid:33)

where w1(x) = sin ( 2πx1
approximately 13.9 calculated by Monte Carlo integration.

4 ), w3(x) = 3σ

, and σ(x) =

1

1+exp(x) with a normalizing constant of

(cid:20) x2 − w1(x)
0.4
(cid:16)(cid:2) x1−1
0.3

(cid:3)2(cid:17)

4

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

Figure 1: True density of each data set: arc, potential 1 to 4, and 2D mixture. High density points are colored as yellow
and low-density points are colored as white.

• The data set 2D mixture corresponds to a two-dimensional random sample drawn from the random vector

X = (X1, X2) with a probability density function given by

f (x) =

1
2

N (x|µ1, Σ1) +

1
2

N (x|µ2, Σ2)

with means and covariance matrices µ1 = [1, −1]T , µ2 = [−2, 2]T , Σ1 =

(cid:20) 1
0

0
2

(cid:21)
, and Σ1 =

(cid:21)

(cid:20) 2 0
0 1

• The data set 10D-mixture corresponds to a 10-dimensional random sample drawn from the random vector
X = (X1, · · · , X10) with a mixture of four diagonal normal probability density functions N (Xi|µi, σi),
where each µi is drawn uniformly in the interval [−0.5, 0.5], and the σi is drawn uniformly in the interval
[−0.01, 0.5]. Each diagonal normal probability density has the same probability of being drawn 1/4.

The functions from Potential 1 to 4 were presented in [24] to test their normalizing ﬂow algorithms. The ARC data set
was presented in [20] to test his autoregressive models.

For this experiment, we compared DMKDE with different methods of approximate kernel density estimation. We
used for this comparison: (1) a raw implementation of kernel density estimation using Numpy (RAWKDKE, (2) a
naive implementation of kernel density estimation using the KDE.py library (NAIVEKDE) 2, (3) tree-based kernel
density estimation (TREEKDE), (4) kernel density estimation using a k-dimensional tree (KDEKDT), and (5) kernel
density using a ball tree (KDEBT). Details of NAIVEKDE, KDEKDT, and KDEBT can be found above in 3. In
this experimental setup, we did not compare with the hashing-based approach due to the restriction of the algorithm
implementations. For all experiments, the Gaussian kernel was used. Two different types of experiments were performed.
In the ﬁrst, we evaluated the accuracy of each of the algorithms on each data set. In the second, we evaluated the time it
takes to make a new density prediction on the training set. Each run was performed with different training set sizes,
using a scale of 10i where i ∈ {1, 2, 3, 4, 5}, and we set the test set to 104 test examples. The spread parameter was
found using a 5-fold cross validation for each data set and each training size in a logarithmic scale γ ∈ {2−20, · · · , 220}.
The optimal number of random Fourier features used by DMKDE-SGD was searched in the set {50, 100, 500, 1000}.
After ﬁnding the best-hyperparameter, several attempts were performed for each algorithm in each data set.

(cid:80)n

We measure the efﬁcacy of each algorithm on each data set using the L1-error also known as average error. Some
advantages over L2-error are outlined in [5]. The L1-error is deﬁned over n samples by the following equation:
i=1 | ˆf (x) − f (x)|. In [5], the author shows that the loss L1 loss (cid:82) | ˆf − f | is invariant under monotone
M AE = 1
n
transformations of the coordinate axes and points out that it is related to the maximum error made if we were estimating
the probabilities of all Borel sets of ˆf and f respectively. Efﬁciency was evaluated using CPU time in milliseconds
(ms), which deﬁnes the amount of time it takes the central processing unit (CPU) to execute its processing instructions
to compute the evaluation query. We used the built-in time package in the Python programming language to measure
the elapsed time in prediction time for each algorithm.

5

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

Table 1: Efﬁcacy test results measured in MAE for training size 1 × 105

DATA SET

RAW

NAIVE

TREE

KDBTREE

KDKDTREE

DMKDE-SGD

ARC
2D MIXTURE
10D MIXTURE
POTENTIAL 1
POTENTIAL 2
POTENTIAL 3
POTENTIAL 4

0.0012±2E-4
0.0010±1E-4
2.5282±3E-4
0.0046±1E-4
0.0456±2E-4
0.0182±1E-4
0.0190±2E-4

0.0012±2E-4
0.0010±3E-4
583000±0.0004
0.0046±3E-4
0.0456±0.0004
0.0182±5E-4
0.0190±2E-4
Table 2: Efﬁciency test results in millisecond (ms) for training size 1 × 105

0.0012±1E-4
0.0010±5E-4
2.5282±3E-4
0.0046±6E-4
0.0456±0.0007
0.0182±6E-4
0.0190±8E-4

0.0012±1E-4
0.0010±5E-4
2.6216±2E-4
0.0046±2E-4
0.0456±0.0007
0.0182±6E-4
0.0190±5E-4

0.0012±1E-4
0.0010±1E-4
2.5282±3E-4
0.0046±1E-4
0.0456±2E-4
0.0182±1E-4
0.0190±1E-4

0.0080±1E-4
0.0016±2E-4
1.7420±1E-4
0.00334±3E-4
0.0332±2E-4
0.0299±2E-4
0.0235±3E-4

DATA SET

RAW

NAIVE

TREE

KDBTREE

KDKDTREE

DMKDE-SGD

ARC
2D MIXTURE
10D MIXTURE
POTENTIAL 1
POTENTIAL 2
POTENTIAL 3
POTENTIAL 4

52400±500
42400±282
88000±115
46100±378
28800±212
30200±0.000
30700±0.000

103000±447
68000±707
123000±1527
76000±547
70000±0.000
72000±0.000
68000±707

24700±171
35900±282
583000±435
22100±141
4920±210
19900±410
54700±424

49200±700
54800±505
190000±436
39901±212
29400±302
41100±520
54800±0.000

56000±141
62000±577
193000±577
52400±141
36700±700
48900±0.000
64000±0.000

4330 ± 145
3500 ± 190
504 ± 110
3640 ± 311
3630 ± 106
3280 ± 176
3700 ± 158

4.0.2 Results and discussion

Table 1 shows the comparison of the mean error of each algorithm on each data set against the true density. The results
obtained by each approximation method are better than those of DMKDE-SGD, except for the 10-dimensional mixture
data set. Table 2 shows the time consumed by each algorithm on each data set. The DMKDE-SGD method is at least 6
times faster than the TREE algorithm, 10 times faster than RAW, KDBTREE and KDKDTREE. And 20 times faster
than NAIVE. It is worth noting that if we increase the number of training sizes, all algorithms except DMKDE-SGD
will consume more time to produce a prediction. Figure 2 shows the comparison of the efﬁcacy measure with the mean
average error (MAE) of each algorithm on six synthetic data sets. The MAE of DMKDE and DMKDE-SGD is close
to other KDE approximation methods in ARC, Potential 1, Potential3, Potential 4, and 2d mixture. In Arc, however,
their performance does not improve after 103 training data points. In Potential 2, both DMKDE and DMKDE-SGD are
better than the KDE approximation methods. On 10D Mixture, DMKDE and DMKDE-SGD outperform other KDE
approximation methods. Figure 3 shows the comparison of the efﬁciency measure in time taken of the central processing
unit (CPU) of approximation methods of KDE. All approximation methods, except DMKDE and DMKDE-SGD,
increase their prediction time when the number of points increases. However, it is observed that DMKDE does not
increase linearly like the other methods. DMKDE-SGD has a larger initial footprint, but it remains constant as the
number of data points increases. If we evaluate it with more than 105 points, we would expect all methods to exceed the
time consumed by DMKDE-SGD.

5 Conclusion

In this paper we systematically evaluate the performance of the method called Density Matrix Kernel Density Estimation
(DMKDE). This method uses the kernel approximation given by random Fourier features and density matrices which
are a fundamental tool in quantum mechanics. The efﬁciency and efﬁcacy of the model was compared with three kernel
density approximation methods: tree kernel density estimation method, kernel density estimation using a k-dimensional
tree and kernel density using a ball tree. Systematic comparison shows that the new method is close in terms of mean
error to these kernel density estimation approaches, but uses ten times less computational resources. The method can be
used in domains where the size of the training data set is large (>104), where kernel density estimation will suffer given
its memory-based behavior.

References

[1] Anderson, T.K.: Kernel density estimation and K-means clustering to proﬁle road accident hotspots. Accident

Analysis and Prevention 41(3), 359–364 (may 2009)

[2] Backurs, A., Indyk, P., Wagner, T.: Space and time efﬁcient kernel density estimation in high dimensions. vol. 32

(2019)

[3] Bentley, J.L.: Multidimensional binary search trees used for associative searching. Communications of the ACM

18(9), 509–517 (1975)

[4] Borruso, G.: Network density estimation: a gis approach for analysing point patterns in a network space.

Transactions in GIS 12(3), 377–402 (2008)

2Implementation of KDE.py: https://github.com/Daniel-B-Smith/KDE-for-SciPy/blob/master/kde.py

6

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

[5] Devroye, L.: Nonparametric density estimation. The L_1 View (1985)
[6] Downs, J.A.: Time-geographic density estimation for moving point objects (2010)
[7] Gallego M., J.A., Osorio, J.F., Gonzalez, F.A.: Fast Kernel Density Estimation with Density Matrices and Random

Fourier Features Software (7 2022). https://doi.org/10.5281/zenodo.6941020

[8] González, F.A., Gallego, A., Toledo-Cortés, S., Vargas-Calderón, V.: Learning with density matrices and random

features (2021)

[9] Gramacki, A.: Nonparametric kernel density estimation and its computational aspects. Springer (2018)
[10] Guardnaccia, C., Grimaldi, M., Graziuso, G., Mancini, S.: Crowdsourcing noise maps analysis by means of kernel

density estimation pp. 1691–1697 (2021)

[11] Kamalov, F.: Kernel density estimation based sampling for imbalanced class distribution. Information Sciences

512, 1192–1201 (2020)

[12] Kristan, M., Leonardis, A., Skoˇcaj, D.: Multivariate online kernel density estimation with gaussian kernels. Pattern

Recognition 44, 2630–2642 (10 2011)

[13] Lee, J., Park, M.: An adaptive background subtraction method based on kernel density estimation. Sensors

(Switzerland) 12(9), 12279–12300 (sep 2012)

[14] Liu, D., Yao, Z., Zhang, Q.: Quantum-Classical Machine learning by Hybrid Tensor Networks. Tech. rep. (2020)
[15] Lv, P., Yu, Y., Fan, Y., Tang, X., Tong, X.: Layer-constrained variational autoencoding kernel density estimation

model for anomaly detection. Knowledge-Based Systems 196 (5 2020)

[16] M., J.A.G., González, F.A.: Quantum adaptive fourier features for neural density estimation (2022).

https://doi.org/10.48550/ARXIV.2208.00564, https://arxiv.org/abs/2208.00564

[17] Maneewongvatana, S., Mount, D.M.: It’s okay to be skinny, if your friends are fat. In: Center for geometric

computing 4th annual workshop on computational geometry. vol. 2, pp. 1–8 (1999)

[18] Nakaya, T., Yano, K.: Visualising crime clusters in a space-time cube: An exploratory data-analysis approach
using space-time kernel density estimation and scan statistics. Transactions in GIS 14, 223–239 (6 2010)
[19] Omohundro, S.M.: Five balltree construction algorithms. International Computer Science Institute Berkeley

(1989)

[20] Papamakarios, G., Pavlakou, T., Murray, I.: Masked autoregressive ﬂow for density estimation (5 2017), http:

//arxiv.org/abs/1705.07057

[21] Parzen, E.: On estimation of a probability density function and mode. The annals of mathematical statistics 33(3),

1065–1076 (1962)

[22] Peng, K., Ping, W., Song, Z., Zhao, K.: Non-autoregressive neural text-to-speech (5 2019), http://arxiv.org/

abs/1905.08459

[23] Rahimi, A., Recht, B.: Random features for large-scale kernel machines. In: Proceedings of the 20th International
Conference on Neural Information Processing Systems. p. 1177–1184. NIPS’07, Curran Associates Inc. (2007)

[24] Rezende, D.J., Mohamed, S.: Variational inference with normalizing ﬂows (2015)
[25] Rosenblatt, M.: Remarks on some nonparametric estimates of a density function. Ann. Math. Statist. 27(3),

832–837 (09 1956)

[26] Rudin, W.: Fourier analysis on groups, vol. 121967. Wiley Online Library (1962)
[27] Siminelakis, P., Rong, K., Bailis, P., Charikar, M., Levis, P.: Rehashing kernel evaluation in high dimensions. vol.

2019-June, pp. 10153–10173 (2019)

[28] Von Neumann, J.: Wahrscheinlichkeitstheoretischer aufbau der quantenmechanik. Nachrichten von der
Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse 1927, 245–272 (1927)

7

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

Figure 2: Comparison of the efﬁcacy of each algorithm on six synthetic data sets. The x-axis is a logarithmic scale of
10i where i ∈ {1, · · · , 5}. The y-axis represents the mean average error between the prediction of the algorithm and
the true density.

8

Fast Kernel Density Estimation with Density Matrices and Random Fourier Features

Figure 3: Comparison of the efﬁciency of each algorithm on six synthetic data sets. The x-axis is a logarithmic scale
of 10i where i ∈ {1, · · · , 5}. The y-axis represents the time consumed by each algorithm in milliseconds used of the
central processing unit (CPU).

9

