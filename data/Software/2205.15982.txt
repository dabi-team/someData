Noname manuscript No.
(will be inserted by the editor)

Testing Research Software: A Survey

Nasir U. Eisty · Jeﬀrey C. Carver

2
2
0
2

y
a
M
1
3

]
E
S
.
s
c
[

1
v
2
8
9
5
1
.
5
0
2
2
:
v
i
X
r
a

Received: Sept. 28, 2021 / Accepted: May 26, 2022

Abstract Background: Research software plays an important role in solving
real-life problems, empowering scientiﬁc innovations, and handling emergency
situations. Therefore, the correctness and trustworthiness of research software
are of absolute importance. Software testing is an important activity for identi-
fying problematic code and helping to produce high-quality software. However,
testing of research software is diﬃcult due to the complexity of the underlying
science, relatively unknown results from scientiﬁc algorithms, and the culture
of the research software community. Aims: The goal of this paper is to better
understand current testing practices, identify challenges, and provide recom-
mendations on how to improve the testing process for research software de-
velopment. Method: We surveyed members of the research software developer
community to collect information regarding their knowledge about and use
of software testing in their projects. Results: We analysed 120 responses and
identiﬁed that even though research software developers report they have an
average level of knowledge about software testing, they still ﬁnd it diﬃcult due
to the numerous challenges involved. However, there are a number of ways,
such as proper training, that can improve the testing process for research soft-
ware. Conclusions: Testing can be challenging for any type of software. This
diﬃculty is especially present in the development of research software, where
software engineering activities are typically given less attention. To produce
trustworthy results from research software, there is a need for a culture change

N. U. Eisty
Department of Computer Science
Boise State University
Boise, ID, USA
E-mail: nasireisty@boisestate.edu

J. C. Carver
Department of Computer Science
University of Alabama
Tuscaloosa, AL, USA
E-mail: carver@cs.ua.edu

 
 
 
 
 
 
2

Nasir U. Eisty, Jeﬀrey C. Carver

so that testing is valued and teams devote appropriate eﬀort to writing and
executing tests.

Keywords Software Testing · Survey · Research Software · Software
Engineering

1 Introduction

Research software is software developed by researchers from a wide variety
of domains including, but not limited to, science, engineering, business, and
humanities (Eisty et al. 2018). Research software can serve diﬀerent purposes.
Sometimes researchers develop software to make predictions about or to better
understand real-world processes (Kanewala and Bieman 2014). This type of
research software solves computationally complex or data-intensive problems.
For example, researchers develop software for large-scale physical phenom-
ena, such as weapons or medical simulations, that run on high-performance
computers. Researchers also develop smaller simulations that run on desktop
machines or small cluster (Kelly et al. 2008). Research software can provide
infrastructure support (e.g., messaging middleware or scheduling software) or
libraries for mathematical and scientiﬁc programming (e.g., linear algebra or
symbolic computing).

The last few years have seen signiﬁcant growth in research software de-
velopment. While researchers have been building software to support their
research for many decades, their primary goals relate to the research outputs,
not to the quality or sustainability of the underlying software (Easterbrook
and Johns 2009). More recently, the research software community has begun
to recognize the need for focusing on the quality and sustainability of their
software (Katz et al. 2019). Increasingly, the development of research soft-
ware requires the use of advanced skill-sets that developers must build over
time (Segal 2005). These developers need to use best practices to ensure the
reliability and sustainability of the software they develop. Therefore, it is the
right time to investigate how research software developers use a key practice
like testing to identify the challenges and develop best practices for research
software.

The quality of research software is critical. Researchers use research soft-
ware in mission-critical situations and decision making (Kanewala and Bieman
2014). In addition, researchers use the results from research software as evi-
dence in research publications (Sanders and Kelly 2008; Drake et al. 2005).
Therefore, it is important that research software have a correct design and
implementation. Low quality software produces less trustworthy results and
is prone to failure in mission-critical situations. Software quality problems
have even caused scientists to retract publications (Miller 2006). Therefore,
researchers need to develop high-quality software that produces trustworthy
results and functions properly in critical situations.

The term research software developer refers to a person who develops
research software. Research software is most often produced by researchers

Testing Research Software: A Survey

3

themselves, often within academia, by faculty, staﬀ, postdocs, and students.
Research software developers range from researchers who possess little or no
software engineering knowledge to experienced professional software develop-
ers with considerable software engineering knowledge (Kanewala and Bieman
2014).

Testing is a useful practice for producing high-quality software. Unfortu-
nately, because of its complex computational behavior, it is very diﬃcult to
test research software. Research software developers often build software based
upon a set of mathematical equations and use mathematical analysis to ver-
ify the corrections of the computational model (Kelly et al. 2009; Post and
Kendall 2004). For example, researchers use research software to determine
the impact of modiﬁcations to nuclear weapon simulations since real-world
testing is too dangerous and not allowed (Post and Kendall 2004).

While testing is a useful practice, there are some technical challenges for
testing research software. The ﬁrst challenge is the lack of test oracles (Hook
and Kelly 2009). An oracle is pragmatically unattainable in most of the cases
for research software because researchers develop software to ﬁnd previously
unknown answers. Due to the lack of test oracles, research software developers
often use judgment and experience to check the correctness of the software. The
second challenge is the large number of tests required to test research software
using standard testing techniques. Also, the large number of input parameters
makes it challenging to manually selecting a suﬃcient test suite (Vilkomir et al.
2008). Finally, the presence of legacy code makes testing research software very
challenging (Clune and Rood 2011).

Because of these challenges, research software developers are unlikely to
use systematic testing to check the correctness of their code (Hook and Kelly
2009; Murphy et al. 2011; Kelly et al. 2008). Even though these developers
conduct validation checks to ensure the software correctly models the physi-
cal phenomenon of interest (Kelly et al. 2009; Murphy et al. 2011), there is
still a need for testing that identiﬁes diﬀerences between the model and the
code (Farrell et al. 2011). In addition, sometimes the reason for limited use
of systematic testing results from the testing challenges posed by the software
itself (Easterbrook 2010).

Because of the various challenges to properly and fully testing research
software, there is a need to better understand how research software developers
actually perform testing activities in practice. Therefore, the goal of this paper
is to better understand the testing process, identify challenges, and provide
recommendations on how to improve the testing process in research software
development. To make this high-level goal more tractable, Section 2 reviews
related work and poses a set of speciﬁc research questions. Then, to answer
these questions, the paper describes the results from our survey of research
software developers.

Based on the results from the 120 respondents to the survey, the key con-

tribution of this paper are:

4

Nasir U. Eisty, Jeﬀrey C. Carver

– An overview of the level of knowledge research software developers have on

software testing;

– A description of the current testing practices used in the research software

community;

– A list of the diﬃculties of testing research software;
– An analysis of the compatibility of commercial/IT testing techniques to

research software; and

– An identiﬁcation of areas of improvement in the testing process for research

software.

2 Research Questions

This section discusses the research questions with related work. We use the
related work to motivate a series of research questions that will ultimately
drive the survey design.

RQ1: What level of knowledge do research software developers have
about software testing?

Research software developers often have little or no formal software engi-
neering knowledge (Easterbrook 2010; Easterbrook and Johns 2009; Hannay
et al. 2009). There is a lack of recognition for the skills and knowledge required
for software development (Hill 2016). These developers are typically unfamiliar
with available testing methods (Eddins 2009; Hannay et al. 2009). As a con-
sequence, they do not usually have a set of written quality goals. Researchers
even treat software development as a secondary activity. Therefore, we pose
this research question to better understand the level of knowledge research
software developers possess about software testing and verify the claims from
the literature.

RQ2: How do research software developers test software?

The literature only provides a few examples of how research software de-
velopers perform testing. Research software developers commonly omit unit
testing because they have misconceptions about the beneﬁts and diﬃculties of
implementing unit tests (Clune and Rood 2011). Research software develop-
ers under-utilize veriﬁcation testing because they are unaware of the need for
it and the methods for applying it (Easterbrook and Johns 2009). Research
software projects often do not even include automated acceptance testing and
regression testing (Nguyen-Hoan et al. 2010). Furthermore, the research soft-
ware community is lagging in the use of available testing tools, at least partially
due to the wide use of FORTRAN (Sanders and Kelly 2008; Easterbrook and
Johns 2009; Lemos and Martins 2012). Therefore, to better understand how
research software developers actually perform testing, we pose this research
question.

RQ3: Why is testing research software diﬃcult?

Testing research software is challenging. A previous systematic literature
review (SLR) reported testing challenges due to the characteristics of research

Testing Research Software: A Survey

5

software and the cultural diﬀerences between researchers and more traditional
software engineers (Kanewala and Bieman 2014). The authors subdivided the
testing challenges resulting from the characteristics of research software into
four categories: a) test case development, b) producing expected test case
output values, c) test execution, and d) test results interpretation. Then sub-
divided the testing challenges resulting from the cultural diﬀerences between
research software developers and more traditional software engineers into three
categories: a) limited understanding of testing concepts, b) limited understand-
ing of the testing process, and c) not applying known testing methods. Because
the SLR was only able to capture and report on challenges actually published
in the literature, to better evaluate whether these challenges, and others, hap-
pen in practice, we pose this research question.

RQ4: Is it possible to adapt existing testing methods to test research
software?

Based on the SLR described in the previous research question (Kanewala
and Bieman 2014), there is little evidence that research software projects use
the available testing methods as we describe below. Only a few projects men-
tioned the use of unit testing (Drake et al. 2005; Farrell et al. 2011; Ackroyd
et al. 2008; Kelly et al. 2011). There is only one study that mentioned the use
of integration testing (Drake et al. 2005). A few other studies mentioned the
use of system testing (Farrell et al. 2011; Clune and Rood 2011), acceptance
testing (Clune and Rood 2011), and regression testing (Drake et al. 2005; Far-
rell et al. 2011; Hochstein and Basili 2008). There are a lot of research software
projects in existence, but the number that appear in literature about testing
is very low (Kanewala and Bieman 2014). Therefore, to understand whether
existing testing methods could be adapted to address this lack of software
testing in research software, we pose this research question.

RQ5: What improvements to the testing process do research soft-
ware developers need?

To better identify ways to advance the testing of research software, there
is a need to better understand speciﬁc ways to improve the testing process
and overcome the testing challenges that exist. Because of the challenges,
developers often do not want to write tests. Therefore, addressing the testing
challenges can make the process welcoming to research software developer. For
example, developing a test oracle is a key challenge for testing research soft-
ware. Some research software projects have addressed this challenge through
creating pseudo oracles, which is code developed separately to produce the
intended output given the same input as the original program (Farrell et al.
2011; Easterbrook and Johns 2009; Nguyen-Hoan et al. 2010). Though creat-
ing a pseudo oracle is not commonly applied to all research software projects
but this technique may work for some. Another challenge for testing research
software is obtaining adequate budget (Nguyen-Hoan et al. 2010; Hill 2016; Se-
gal 2009). Projects may be able to overcome this challenge through increasing
awareness among project decision-makers of the need for providing a budget
for testing activities. To identify other approaches for improving the current

6

Nasir U. Eisty, Jeﬀrey C. Carver

testing process in research software development, we pose this last research
question.

3 Methodology

To answer the research questions described in Section 2, we conducted an
online questionnaire survey of members of the research software development
community.

To reach a broad audience and produce useful results, we decided to con-
duct a survey because surveys are useful for describing the characteristics of
a large population. Section 3.1 describes the design of the study. Section 3.2
explains the process we followed to analyze the results of the survey.

3.1 Survey Design

3.1.1 Questionnaire

Using the research questions, we enumerated the survey questions shown in
Figures 1 and 2. Note the bold headings that enumerate the Research Ques-
tions did not appear in the survey, but are included here for clarity. Because
we anticipated survey respondents may not be familiar with standard software
engineering deﬁnitions, we deﬁned key software engineering concepts on the
survey. We provided deﬁnitions for the roles listed in Q2, the testing methods
listed in Q11, and the techniques listed in Q13 in the survey (see Appendix A)
to address the possibility that a respondent may be familiar with a concept
but unaware of the proper term. In addition, to ensure respondent had the
proper context we included the following statement at the beginning of the
survey: “When answering these questions, please consider the research soft-
ware project on which you are the most active.”

3.1.2 Pilot Study

We piloted the initial survey with three experienced research software devel-
opers from Los Alamos National Laboratory (LANL), where the ﬁrst author
was an intern. The pilot testers each work in a diﬀerent scientiﬁc domain:
Computational Physics, Computational Material Science, and Applied Math-
ematics. Each pilot tester has more than 15 years of experience developing
and using software for their research. Based on their feedback, we reworded
some questions for clarity to research software developers. We then deployed
the survey via the Qualtrics platform.

3.1.3 Distribution

We used the following solicitation methods to reach a broad population of
research software developers. First, we sent the survey to mailing lists that

Testing Research Software: A Survey

7

reach research software developers. Those lists include the prior attendees of
the SE4Science workshops1 along with a custom-made email list of contribu-
tors to research software repositories mined from GitHub. Second, we adver-
tised the survey in two Slack channels, one for international research software
engineers (RSE ) and one focused on research software engineers in the US
(US-RSE ). The subscribers to these slack channels are research software en-
gineers that range from graduate students to experienced researchers working
in academia or research labs. Third, we advertised the survey in the monthly
newsletter of Better Scientiﬁc Software (BSSw) and the IDEAS-productivity
mailing list. The participants in these mailing lists are research scientists, fac-
ulty members, graduate students, and postdocs who develop research software.
Fourth, we asked people reached by the above advertising to also forward the
survey invitation within their own networks. Because of the solicitation ap-
proach we used, we cannot estimate how many people received our invitation.
We distributed the survey on August 27, 2019 and left it open for one month.

3.2 Data Analysis

We anticipated the respondents might not have the background to answer all
of the survey questions, therefore we allowed them to skip any question they
did not want to answer. Because we did not require respondents to answer all
questions, we received some partially complete responses. To ensure that we
included only valid responses in our analysis, the ﬁrst author manually went
through each response to check its level of completeness. We included responses
that answered all quantitative questions and provided useful information in
response to one or more qualitative questions.

We received both quantitative and qualitative data from the survey2. We
used the tool SPSS to analyze the quantitative data. For any data that are
qualitative, we used a grounded theory approach for the analysis. We individu-
ally coded the text using NVivo. Then we compared our results and identiﬁed
the discrepancies. We discussed all the discrepancies on an in-person meeting
to solve any disagreements. After that, we identiﬁed the relationships between
the coded data and categorize them. Finally, we categorized the categories into
high-level core categories. We used the programming language R to visualize
the results into charts.

4 Results

This section describes the results from the 120 valid responses, as deﬁned in
Section 3.2.

1 https://se4science.org/workshops/
2 The survey data is available in a public repository but set to private until publication

of this paper (Carver and Eisty 2021)

8

Nasir U. Eisty, Jeﬀrey C. Carver

General Questions

Q1 Which research software project are you most actively involved in? (Optional)
Q2 Please choose your roles on the project (Choose all that apply)? [Developer,
Architect, Quality Assurance Engineer, Maintainer, Manager, Executive, Other]
Q3 How many years have you worked on the project? [Less than 1 year, 1 year to

less than 5 years, 5 years to less than 10 years, More than 10 years]

Q4 Which best describes the current development stage of your project? [Plan-
ning/Requirements Gathering, Initial Development/Prototyping, Active Devel-
opment/Unreleased Software, Active Development/Released Software, Mainte-
nance/No New Development Planned, Other]

Q5 How many developers are currently working on your project? [1 to 5, 5 to 10,

10 to 15, 15 to 20, More than 20]

RQ1: What level of knowledge do research software developers have
about software testing?

Q6 How conﬁdent are you on your knowledge of software testing? [Very Low, Low,

Average, High, Very High]

Q7 What is your level of understanding on the testing concepts USED in your

project? [Very Low, Low, Average, High, Very High]

Q8 What is your level of understanding on the testing concepts NEEDED in your

project? [Very Low, Low, Average, High, Very High]

Q9 List any software testing techniques with which you are familiar. [Free response]

RQ2: How do research software developers test software?

Q10 Please select one of the items below that most closely resembles your goal of
testing on your project. [Level 0 - There is no diﬀerence between testing and
debugging, Level 1 - The purpose of testing is to show correctness, Level 2 -
The purpose of testing is to show that the software does not work, Level 3 -
The purpose of testing is not to prove anything speciﬁc, but to reduce the risk
of using the software, Level 4 - Testing is a mental discipline that helps all
researchers develop higher quality software]

Q11 Which of the following testing methods does your team use? (Choose all that
apply) [Unit testing, Integration testing, System testing, Acceptance testing,
Module testing]

Q12 How often is software testing useful in your project? [Never, Rarely, Some-

times, Most of the time, Always]

Q13 Please select any speciﬁc testing techniques you use in your project (Choose
all that apply). [Metamorphic testing, Assertion checking, Performance testing,
Monte carlo test, Dual coding, Fuzzing test, Backward compatibility testing,
Using machine learning, Using statistical test, Test driven development, Input
space partitioning, Graph coverage, Logic coverage, Statement coverage, Condi-
tion coverage, Branch coverage, Syntax-based testing, Boundary value analysis,
Equivalence partitioning, Decision table based testing, State transition, Error
guessing, Backward compatibility testing, other]

RQ3: Why is testing research software diﬃcult?

Q14 How complex is the testing process on your project? [Not, Slightly, Moderately,

Very, Extremely]

Q15 Please explain any barriers or challenges you face to test your project?

Fig. 1 Survey Questions Part-1

Testing Research Software: A Survey

9

RQ4: Is it possible to adapt existing testing methods to test research
software?

Q16 How often does your team apply Commercial/IT testing methods in your

project? [Never, Rarely, Sometimes, Most of the time, Always]

Q17 How often do you PERSONALLY apply Commercial/IT testing methods in

your project? [Never, Rarely, Sometimes, Most of the time, Always]

Q18 How much value do you see personally in using Commercial/IT testing meth-

ods? [Very Low, Low, Average, High, Very High]

Q19 Please explain any challenges to adapt Commercial/IT testing methods to

your project. [Free response]

Q20 Please explain any challenges you think that could not be met by Commer-

cial/IT testing testing methods? [Free response]

RQ5: What improvements to the testing process do research software
developers need?

Q21 How could the overall testing process be improved? [Free response]

Fig. 2 Survey Questions Part-2

4.1 Demographics

The following subsections detail each of the demographics gathered in Q1-Q5.

4.1.1 Projects

Q1 asked participants to optionally identify their research software project.
Respondents mentioned 66 unique research software projects. Because of their
project’s privacy policies, 56 respondents did not name their project. All but
two project names were unique, with one identiﬁed by three respondents and
the other by two. To get a sense of project domains, we examined the web-
site for each named project. The domains included: Physics, Chemistry, Biol-
ogy, Geology, Astronomy, Mathematics, Climate Science, Neuroscience, Atmo-
spheric science, Astrophysics, Computing infrastructure, Numerical Libraries,
and Simulation tools. Given the small number of respondents from any one
domain, we were not able to analyze the eﬀect of project domain on the re-
sults. This result indicates that the respondents came from a wide variety of
research software projects, which allows our overall ﬁndings to be useful to a
broad audience of research software developers.

4.1.2 Project Role

Because people in diﬀerent project roles likely have diﬀerent perspectives on
software quality and diﬀerent experience with testing their projects, the re-
spondent’s role is an important demographic. The results of Q2 (Figure 3)
shows the distribution of respondents is skewed towards technical roles (e.g.
Developer, Architect, and Maintainer). Almost all respondents indicated De-
veloper as at least one of their roles. Note that because research software

10

Nasir U. Eisty, Jeﬀrey C. Carver

developers often hold multiple roles on a project, the survey allowed them to
choose multiple responses. Therefore, the sum of the bars in Figure 3 is larger
than the total number of respondents.

Fig. 3 Respondents’ role on project

4.1.3 Project Experience

A respondent’s experience provides insight into whether she or he has enough
knowledge to draw upon to provide helpful responses to the survey. The results
from Q3 (Figure 4) show the large majority of respondents had more than one
year of experience in working research software projects. Almost 1/3 of had at
least ﬁve years of experience, with 1/2 of those having more than ten years of
experience. This result shows that, the respondents had enough experience to
provide valid responses to the survey.

4.1.4 Project Stage

Because diﬀerent types of testing are relevant at diﬀerent stages of project
development, the stage of the respondent’s project could impact how he or she
answers the survey questions. The responses to Q4 (Figure 5) show projects
were overwhelmingly at the released stage. This result is important because
the projects at this stage should have already established testing practices in
the project.

4.1.5 Project Size

Because teams of diﬀerent sizes may have diﬀerent perspectives on the use of
software engineering practices (Eisty et al. 2018), we asked the respondents
about their project size. The responses to Q5 (Figure 6) shows, that while

Testing Research Software: A Survey

11

Fig. 4 Number of years worked on research software

Fig. 5 Project stage

the largest group of respondents were on smaller teams, there is also a good
distribution of respondents across larger team sizes.

4.2 RQ1: What level of knowledge do research software developers have
about software testing?

Survey question Q6 asked respondents how conﬁdent they were in their knowl-
edge of software testing. The results in Figure 7 show most respondents in-
dicated they possessed at least an average level of conﬁdence about testing
knowledge, with more than 1/3 indicating high or very high conﬁdence in
their knowledge. On a positive note, only a few respondents indicated they
had low conﬁdence on knowledge, with none responding very low.

Next, Q7 asked respondents about their level of understanding of the test-
ing concepts they actually used in their projects. The results in Figure 8 show

12

Nasir U. Eisty, Jeﬀrey C. Carver

Fig. 6 Number of developers

Fig. 7 Conﬁdence on knowledge of software testing

that more than half of the respondents indicated their level of understanding
was high or very high. Another 1/3 indicated their understanding was average.
Only a small number had a low or very low level of understanding. Similarly,
Q8 asked respondents about their level of understanding of the testing con-
cepts needed for their projects, which might be diﬀerent than those actually
being used. The results in Figure 9, show a slightly diﬀerent distribution than
the previous question. Most respondents still reported at least an average level
of understanding. However, the responses shifted away from very high into high
and average. These distributions are signiﬁcantly diﬀerent (χ2 = 31.0068, p
< .001). Together, the responses to these two questions indicate that while
survey respondents believed they had an adequate understanding of the test-
ing concepts used in their projects, they were less conﬁdent about the testing
concepts they actually needed.

Testing Research Software: A Survey

13

Fig. 8 Level of understanding on the testing concepts used

Fig. 9 Level of understanding on the testing concepts needed

To gain better insight into the respondents’ level of knowledge, Q9 asked
them to list any software testing techniques with which they were familiar. By
providing a free response question rather than a set of checkboxes, this question
allowed us to judge the breadth of the respondents’ testing knowledge. Across
all respondents, we collected a long list of testing techniques (see Appendix B).

4.3 RQ2: How do research software developers test software?

Given that the respondents had a reasonable understanding of testing (RQ1)
and given the inherent diﬃculties in testing research software, the results for
this question help us understand how testing occurs in practice.

Survey question Q10 asked respondents to choose which of the following
testing goals (obtained from Introduction to Software Testing by Ammann and
Oﬀutt (Ammann and Oﬀutt 2016)) most closely matched their project:

14

Nasir U. Eisty, Jeﬀrey C. Carver

– Level 0 - There is no diﬀerence between testing and debugging
– Level 1 - The purpose of testing is to show correctness
– Level 2 - The purpose of testing is to show that the software does not work
– Level 3 - The purpose of testing is not to prove anything speciﬁc, but to

reduce the risk of using the software

– Level 4 - Testing is a mental discipline that helps all researchers develop

higher quality software

The results (Figure 10) show the most common response is Level 4, followed
by Level 1. This result means respondents care about producing high-quality
software to show correctness and doing so is a mental satisfaction to them. It is
encouraging because of having a concrete testing goal represents respondents’
willingness to produce trustworthy software by employing proper testing on
the projects.

Fig. 10 Goal of testing

The next question (Q11) asked respondents to indicate which testing meth-
ods (chosen from a list obtained from Introduction to Software Testing by
Ammann and Oﬀutt (Ammann and Oﬀutt 2016) and provided on the sur-
vey) their team uses. The results in Figure 11 show Unit Testing is the most
commonly used method among the respondents. Many respondents also chose
Integration Testing and System Testing. The use of Acceptance Testing and
Module Testing is less frequent. Following on this question, Q12 asked respon-
dents how useful they found software testing in their projects. The results
(Figure 12) show most found testing to be useful always or most of the time.
Only a few respondents indicated testing was sometimes or rarely useful, with
no one indicating it was never useful. These results indicate that respondents
found some types of testing (system, unit, and integration) to be useful most
of the time.

Last, Q13 provided an example list of testing techniques, along with their
deﬁnitions, and asked respondents to indicate which of those techniques they

Testing Research Software: A Survey

15

Fig. 11 Testing methods used

Fig. 12 Usefulness of testing

actually use in their project. In addition to the techniques provided on the sur-
vey, respondents could write in other techniques they use. Table 1 summarizes
the responses. The top portion of the table lists the testing techniques in-
cluded on the survey. The bottom portion of the table contains the techniques
respondents provided in the other section.

This result suggests that research software developers use a wide variety of
testing techniques in their projects. This result is also consistent with the re-
sults in Figure 7 showing that respondents have an adequate level of knowledge
and Figure 8 showing they have good understanding of the testing concepts
used in their projects.

16

Nasir U. Eisty, Jeﬀrey C. Carver

Table 1 List of Used Testing Techniques.

Used Testing Techniques (from options)

Name
Assertion checking
Performance testing
Backward compatibility testing
Statement coverage
Test driven development
Condition coverage
Dual coding
Branch coverage
Monte carlo test
Boundary value analysis
Metamorphic testing

Count
94
72
53
53
50
33
33
32
27
26
26

Name
Error guessing
Fuzzing test
Graph coverage
State transition
Logic coverage
Decision table based testing
Input space partitioning
Syntax-based
Using machine learning
Equivalence partitioning

Regression testing
Bit-for-bit comparison
Benchmarking

Others (Write-ins)

4
1
1

Portability testing
Code coverage
Scaling test

Count
21
20
17
17
11
9
6
6
4
4

1
1
1

4.4 RQ3: Why is testing research software diﬃcult?

To make any progress in overcoming the diﬃculties with testing research soft-
ware, it is important to understand the speciﬁc reasons why testing is diﬃcult.
This research question helps identify speciﬁc challenges and barriers. Survey
question Q14 asked respondents to rate the complexity of testing their projects.
According to the results (Figure 13), the distribution of responses is skewed
towards the lower complexity end of the scale, with the peak at Moderately
Complex. This result means that while there is some level of complexity in
testing research software, most respondents do not ﬁnd the complexity to be
too high.

Fig. 13 Complexity of testing research software

Testing Research Software: A Survey

17

To gain a deeper understanding of the diﬃculties with testing research
software, Q15 asked respondents to explain any barriers or challenges they
face with testing their software. This question was open-ended. Our qualitative
analysis of these free-response answers resulted in the 12 high-level categories
of challenges shown in Figure 14. The following text goes through each high-
level challenge to explain what it means.

The most commonly mentioned class of challenges is test case design.
One respondent described the challenge as having diﬃculty “[e]ngineering good
test cases and making sure that all equivalence classes of test cases are cov-
ered”. Another respondent indicated that “[a]pplication cases are usually too
big/expensive to test, so breaking them down for meaningful system tests is a
challenge.”

The second most commonly mentioned challenge is the lack of resources,
which includes, as one respondent said, “[l]ack of funding, people, and calendar
time”. Another respondent commented that “[t]he amount of time needed to
write tests gives them a bad reputation with developers who aren’t convinced
they are necessary”. Another respondent described the problem as “[l]imited
amount of time is ”allowed”/”allocated” for writing tests and setting up testing
environments.”

The third most commonly mentioned challenge is external dependen-
cies. The following is a representative response: “Tests should execute external
proprietary software which is unavailable on services like TravisCI.”

The fourth most commonly mentioned class of challenges is lack of knowl-
edge. One respondent described the problem as “..lack of testing knowledge
– collectively, as a team, we’ve probably heard of all of the possible ways of
testing mentioned in the questions above, but several of these categories are
not well understood by the team (or any individual within.”

The ﬁfth most commonly mentioned class of challenges is slow. One re-
spondent described this problem as “[t]ests take a long time to run, slows down
continuous integration.”

The sixth most commonly mentioned class of challenges is culture. One
respondent explained the challenge as “[m]ostly cultural, convincing my team
mates that this is important to the sustainability of the code base”. Moreover,
research software has “[a] culture that doesn’t value test development.”

The seventh most commonly mentioned class of challenges is that testing
aﬀects continuous integration. As a summary of these responses, one re-
spondent indicated “[t]esting across multiple machines regularly is a challenge,
due to the continuous integration tests running on a single machine.”

The eighth most commonly mentioned class of challenges comes from the
fact that comparing results with reality makes testing very diﬃcult. As
a speciﬁc example, one respondent said “[s]ince we’re developing a computa-
tional ﬂuid dynamics code (it’s an ocean model), the most diﬃcult part was
testing that the model produces the physically correct output.”

The ninth most commonly mentioned challenge is a result of the code-
base itself. One respondent describes the problem as “[t]he code has not been
designed in a very modular way, so unit testing is not easy to implement.”

18

Nasir U. Eisty, Jeﬀrey C. Carver

The tenth most commonly mentioned challenge is legacy code. Sometimes
there are “[l]arge amounts of legacy code that were not developed with testing
in mind.”

The last class of challenges is cost. Testing research software is diﬃcult
because of the “high cost in maintaining tests, expensive/slow to run full test
suite.”

In addition to these high-level classes, there were a number of other
challenges, including environmental changes, testing graphics production, and
challenges related to programming languages and database systems.

Fig. 14 Challenges in testing research software

4.5 RQ4: Is it possible to adapt existing testing methods to support the
testing of research software?

In this case, “existing testing methods” refer to the testing methods currently
used in Commercial/IT software development. These methods include unit,
integration, system, acceptance, and module testing. Gaining a better under-
standing of how these methods apply to the testing of research software will
help research software developers have more conﬁdence in the methods they
can use and reduce the need to discover this information on their own.

We began with two survey questions about the frequency with which re-
spondents use Commercial/IT testing methods as a team (Q16) and individ-
ually (Q17). As Figures 15 and 16 show most teams and individuals apply
these methods at least sometimes. There is no signiﬁcant diﬀerence between
the distribution of responses. Beyond whether the respondents use the Com-
mercial/IT testing methods, Q18 ask the level of value they see personally in
using these methods. The distribution of results in Figure 17 show that the
respondents generally saw value in using such methods, with more than half
answering high or very high.

Testing Research Software: A Survey

19

Fig. 15 Applying Commercial/IT testing methods by team

Fig. 16 Applying Commercial/IT testing methods personally

To gain insight into where the Commercial/IT methods cause problems,
Q19 asked respondents to explain any challenges they faced in an open-ended
manner. Our qualitative analysis of the results identiﬁed the nine high-level
categories of challenges shown in Figure 18. Because we discuss some of these
challenges in detail in response to other questions, we only highlight a subset
here.

Overwhelmingly, the most common challenge respondents reported was
that the methods were not useful. This challenge arises because “[r]esearch
software is typically not production software” and “[s]ome commercial tools
do not account for issues with numerical tolerances. Legacy codes are hard to
get under test”. Another respondent indicated it is “Diﬃcult to adapt [Com-
mercial/IT testing methods] to the development of scientiﬁc software because
of numerical errors and often not knowing the expected output”.

20

Nasir U. Eisty, Jeﬀrey C. Carver

Fig. 17 Value seen in using Commercial/IT testing methods

The second most common challenge was lack of resources. One respon-
dent explained the challenge as “[l]ack of expertise, schedule demands, lack of
R&D (i.e. exploratory) s/w development oriented tools.”

The third most common response is research software developer mindset.
One respondent summarized the problem as “[d]evelopers often feel like any
time not spent developing code for the core software is not real work, thus time
spent writing tests is less enjoyable”. In addition, the problem is “[c]ultural
– convincing people that it is beneﬁcial, necessary, and worth their time and
not insulting to their work” There are also challenges that originate outside
the team, such as “[o]ur funding agencies are generally not aware of their
importance, and eﬀort spent on testing is eﬀort not spent writing/publishing
papers”. Finally, there is a “[l]ack of familiarity with the ensemble of testing
patterns and goals available.”

In addition to challenges with adapting Commercial/IT methods, there are
cases where those methods are just not applicable. Survey question Q20 asked
respondents to explain any challenges that could not be met by Commercial/IT
testing methods. The qualitative analysis produced ﬁve high-level challenges
(Figure 19).

The most common challenge, by a large margin, was that the methods do
not meet the speciﬁc needs of research software. These speciﬁc needs can
include “[v]isualization [, i]mage processing & analysis [, f ]luid ﬂow simulation
[, s]ituations where there is no analytic solution or known correct answer [, i].e.
no oracle”. A unique challenge of research software is that “[v]alidation requires
domain expertise which is sometimes diﬃcult to express in the commercial
methods”. Another situation that is common in research software is whether
the results are meaningful, as a respondent stated “IT methods are good for
preventing errors / seg faults, but not good at catching if numbers are no longer
meaningful (eg demand curves have inverted) or triaging to determine.”

Similar to the responses to other questions, the respondents also mentioned
lack of expertise, slow to execute the test and continuous integration

Testing Research Software: A Survey

21

Fig. 18 Challenges to adapt Commercial/IT testing methods

issues in response to this question. Finally, benchmarks is another challenge
that could not be met by Commercial/IT testing methods. One respondent
indicated “[b]enchmarking scientiﬁc codes for accuracy (e.g. expected error
convergence rates) is something that is not usually discussed in Commercial
IT testing.”

Fig. 19 Challenges could not met by Commercial/IT testing methods

22

Nasir U. Eisty, Jeﬀrey C. Carver

4.6 RQ5: What improvements to the testing process do research software
developers need?

Finally, to help provide guidance to the research software community, Q21
asked respondents to describe how to improve the testing process. We consider
the testing process in general here in this section. Then, in a follow-up study,
we consider speciﬁc scenarios like test case design, test execution, testing tools,
test coverage evaluation, and test quality. The qualitative analysis produced
ten high-level categories of improvements (Figure 20).

The most commonly mentioned improvement was proper training. Some
of the speciﬁc suggestions provided by respondents include: “[t]each it to grad
students as an essential part of writing software”, “[m]ore training, earlier
on, in scientists’ careers”, “[s]eminars for practitioners, classes on scientiﬁc
software engineering”, and “[e]ducate scientists about the beneﬁts of testing
software”.

Then the second most commonly mentioned improvement was more tests.
This improvement focuses on developer behavior to “[w]rite more unit tests,
preferably using a dedicated testing framework as pFunit” and through “[i]ncreased
use of customized fuzz testing covering more input features.”

The third most commonly mentioned improvement was infrastructure.
One respondent requested “[a] public service for testing that (1) is freely avail-
able for open source, (2) with many-tier (detailed, incremental) pricing struc-
ture for more machine time if needed, with (3) a sophisticated testing dash-
board, similar to that of TeamCity”. Another respondents described the need
as “[o]ur group needs more developers. This is highest priority. But, just as
high: we need support from the academic infrastructure providers to be able
to run tests before our users hit the resources”. Another respondent wanted a
“[simpliﬁed] infrastructure for creating new tests.”

The fourth most commonly mentioned improvement was automation.
Speciﬁcally respondents wanted automation for “setting tests and analysis of
results” and “simpler methods for enabling/using tools.”

Tied for the fourth most commonly mentioned improvement was contin-
uous integration. These systems need improvement because “[o]ur CI /
testing system is very frail, and tends to break when there are system updates,
requiring supervision and triage, and then there isn’t a easy way to rerun tests
on PRs that came in during the down time.”

Also tied for the fourth most commonly mentioned improvement was chang-
ing the culture of testing in the research software developer community.
“Changing the culture. Ensuring any pull requests that come in have adequate
code coverage. Incentivizing test development eﬀorts. Educating the developers
on the importance of software testing and how best to go about it. Making sure
when bugs are getting ﬁxed that the ﬁx isn’t accepted until there’s a new unit
test in place to cover the bug”. Similarily, there is a need to “[c]onvince other
developers that are a part of the same project to adapt to these practices.”

Also tied for the fourth most commonly mentioned improvement was the
need to improve code quality. This improvement has to happen among the

Testing Research Software: A Survey

23

developers themselves, as one response suggested to “[h]ave HPC researchers,
scientists, and engineers write better code. While improvements can certainly
be made in testing, the vast majority of the problem currently is the state of
the code written. You can’t really test a 1000-line main() - you need structure,
seams, and modularity in whatever language or paradigm you use”. Another
respondent suggested that developers need to “Simplify[] code, reduce redun-
dant tests, design better input variables, study other individual components and
other possible problems with diﬀerent architectures or compilers, etc.”

Also tied for the fourth most commonly mentioned improvement is the
need for proper acknowledgement to help motivate the research software
developers to improve their testing. There is a need for “[f ]unding agencies
and application users understanding the beneﬁts of testing and support it en-
thusiastically” and a need for “better incentive for others to contribute tests,
period.”

Moreover, initiatives to make simpler and provide adequate resources
could potentially improve the testing process in research software development.

Fig. 20 Improvement of the testing methods

5 Discussion

This section discusses key insights from the detailed results relative to each of
the overall research questions.

24

Nasir U. Eisty, Jeﬀrey C. Carver

5.1 RQ1 - Knowledge

In general, research software developers are conﬁdent about their knowledge
of software testing. The survey respondents think they have at least an av-
erage level of understanding of the testing concepts used and needed in their
projects. Our ﬁndings are somewhat inconsistent with the previous literature
(Section 2) which found that research software developers have little or no
knowledge of software engineering. While it is possible that respondents over-
estimated their knowledge of software testing, there may also be a growing
awareness of software engineering among the research software community.
This result is encouraging because as research software developers become
more knowledgeable about software engineering practices, they will tend to
produce higher-quality software. We need further study to better understand
the source of the diﬀerences between our survey and the prior work in this
area.

5.2 RQ2 - Practices

Research software developers have a clear goal for testing. Many of the re-
spondents viewed testing as a mental discipline that makes them conﬁdent in
producing trustworthy results. Again, this result is somewhat inconsistent with
prior research (Section 2) indicating that research software developers have no
written quality goal. Conversely, another set of respondents viewed the goal
of testing as showing the correctness of the software, which evidences a much
lower maturity level. These inconsistent results show that there is still a large
disparity in knowledge among the research software developer community.

The literature did not provide much evidence that research software devel-
opers use diﬀerent testing methods (Kanewala and Bieman 2014; Heaton and
Carver 2015). However, the results in Figure 11 show that research software
developers do, in fact, use a wide variety of testing methods. The respondents
also reported using a wide variety of software testing techniques. While the lit-
erature (Heaton and Carver 2015) suggests limited eﬀectiveness of the testing
practices currently used by research software developers, most of the respon-
dents indicated testing was useful at least most of the time. The increased use
of testing could result from recent initiatives on applying software engineering
practices to research software and an increased emphasis on making research
software sustainable and reusable.

5.3 RQ3 - Diﬃculties

In terms of challenges and barriers to testing research software, our results are
consistent with the literature (Kanewala and Bieman 2014). The respondents
indicated that there is some level of complexity in testing their projects. The
survey also identiﬁed a number of challenges for testing research software. Test

Testing Research Software: A Survey

25

case design, lack of resources, and external dependencies are the most common
challenges. Lack of knowledge, test execution slowing down the development
process, the lack of a proper testing culture all make testing diﬃcult. Even
though our results are consistent with the literature, we were able to contribute
a list of concrete and current challenges taken directly from the experiences of
research software developers.

5.4 RQ4 - Adapting existing testing methods

In terms of applying existing Commercial/IT testing methods for testing re-
search software projects, our results provide some insight where previous lit-
erature is deﬁcient. A previous literature review (Kanewala and Bieman 2014)
found very few papers that reported the use of any type of testing methods.
Conversely, most of the respondents to our survey indicate that they apply
Commercial/IT testing methods at least sometimes both personally and as a
team. They see at least an average level of value in applying Commercial/IT
methods, while more than half of the respondents see high or very high value.
While this result contradicts the published literature, it is encouraging. Ap-
plying available testing methods can be beneﬁcial to many research software
developers.

Even with the positive view of Commercial/IT testing techniques, there
are still some challenges to adapting those techniques for use in research soft-
ware. In many cases, Commercial/IT testing techniques are often not useful.
Respondents identiﬁed speciﬁc needs of research software that could not be
met by the existing testing methods. In spite of the challenges to adapting
these testing methods for use in research software, this result is encouraging
because it indicates that many research software developers are trying to apply
these methods.

5.5 RQ5 - Improvement

We identiﬁed many potential improvements to the testing process for research
software. These results can serve as guidance to software teams who want
to incorporate good practices as well as to testing researchers who want to
provide more appropriate techniques for research software.

One critical need is formal training on fundamental testing concepts and
the importance of testing. The training should provide research software de-
velopers with hands-on experience so they are able to understand and utilize
existing testing techniques and tools in their projects, where appropriate. As
research software developers better understand the existing tools and tech-
niques, they will also be able to identify gaps that can be ﬁlled by modifying
existing tools and techniques or by creating new ones.

There is a need for more tests, better infrastructure, and test automa-
tion. Incorporating more tests increases the changes of having correct results.

26

Nasir U. Eisty, Jeﬀrey C. Carver

Developers of research software may limit their testing because of lack of in-
frastructure to ease the process. Proper infrastructure support and test au-
tomation can help motivate research software developers to test their projects
well. Moreover, changing the culture of research software to one that embraces
testing, improved code quality, and proper acknowledgment for testing eﬀort
will help improve the overall testing process.

6 Threats

This section describes the threats to validity of the study.

6.1 Internal Threats

The primary threat to internal validity is whether participants understood
the software engineering concepts in the same way we intended them. If the
respondents did not understand the concepts or had diﬀerent deﬁnitions, then
the results would be less reliable. Because the members of the target survey
population are not traditional software engineers, it is possible that they lacked
the necessary knowledge to properly answer the questions.

We provided appropriate deﬁnitions of terms to minimize confusions of the
respondents. Because we did not observe any misunderstandings and inconsis-
tencies in the free-response questions, we ﬁnd this threat is minimal.

6.2 External Threats

If the survey respondents are not representative of the population of research
software developers, the results are less generalizable. In addition, it is not
possible to measure the size of a community as diverse as the research software
community. So, it is possible that the sample included in our study is not
representative of the overall population. To reduce this threat, we recruited
participants from diﬀerent countries and projects. While it is clear that all
participants are research software developers, some of their responses suggest
they may be more interested in software testing than the average research
software developer. In addition, they took time to answer a survey about
testing. Therefore, the responses may be biased towards developers who are
already predisposed towards the use of testing. In addition, respondents’ self-
assessment may be wrong. They may have overestimated or underestimated
their knowledge relative to the survey questions.

6.3 Construct Threats

The primary construct validity threat is the participants may have misunder-
stood the questions. We took great care in writing the survey questions and
veriﬁed them by expert research software developers and software engineering

Testing Research Software: A Survey

27

researchers. In addition, we provided enough deﬁnitions to the respondents
without biasing them so they could use their own judgment to respond.

6.4 Conclusion Threats

It is possible that, with additional information, other conclusions could be
drawn from the data gathered in this study. We rely on the participants’
perceptions of software testing, which may not match reality. Another potential
threat is that we used a standard software engineering textbook Ammann and
Oﬀutt (2016) to deﬁne the testing terminology in the survey. There are other
sources, including SWEBOK Bourque and Fairley (2014) or ISTQB3, which
have diﬀerent perspectives. While the textbook and these other sources are
not mutually exclusive, there is a chance that survey participants could have
diﬀerent understandings of the terminology.

7 Conclusion and Future work

Testing is an essential practice for building high-quality and trustworthy soft-
ware. Because research software often supports critical situations and produces
evidence for research publications, it is important for researchers to use ap-
propriate testing approaches to complement their development methods. To
gain insight into the practice of testing for research software, we conducted a
survey of practicing research software developers. The results from this survey
help other research software developers understand how their peers test their
software.

This paper report insights about testing research software gained from 120
responses to a survey of research software developers. The paper discusses the
overall knowledge of software testing among research software developers, cur-
rent practices of testing, diﬃculties in testing research software, challenges in
adapting existing testing methods, and potential improvements to the testing
process.

Research software developers are somewhat conﬁdent (Figure 7) in their
knowledge of testing and report a level of average or higher (Figure 8) for
their understanding of the testing concepts used and needed in their projects.
Research software developers have clear testing goals and ﬁnd many types
of testing techniques useful. However, respondents also report a number of
challenges and barriers for testing research software, including: test case design,
lack of resources, external dependencies, and lack of knowledge. In addition,
it is not always easy for research software developers to use of Commercial/IT
testing techniques. Providing proper training and creating a culture that values
testing could address many of these diﬃculties and improve the testing process.
Given the fact that our results were drawn from a convenience sample
and the fact that there is much diversity in the research software space, there

3 https://www.istqb.org/

28

Nasir U. Eisty, Jeﬀrey C. Carver

is a need for further study to verify these ﬁndings in other contexts. In our
future work, we will study the speciﬁc technical challenges of testing research
software. We will analyze how testing practices and methods diﬀer across
project types and domains. Once we identify the speciﬁc technical challenges,
we will better understand how to develop new testing approaches for research
software. Gathering this type of information will require close interaction with
research software teams across diﬀerent domains. Ultimately, this work will
allow us to develop best practices around testing that will be of value to the
research software community.

Acknowledgements We thank the study participants and NSF-1445344.

A Deﬁnitions Provided

We refer to Figures 1 and 2 for the survey question. In this section we listed the deﬁnitions
we provided in the actual survey.

– Acceptance testing - Assess software with respect to requirements or users’ needs.
– Architect - An individual who is a software development expert who makes high-level
design choices and dictates technical standards, including software coding standards,
tools, and platforms.

– Assertion checking - Testing some necessary property of the program under test using

a boolean expression or a constraint to verify.

– Backward compatibility testing - Testing whether the newly updated software works

well with an older version of the environment or not.

– Branch coverage - Testing code coverage by making sure all branches in the program

source code are tested at least once.

– Boundary value analysis - Testing the output by checking if defects exist at boundary

values.

– Condition coverage - Testing code coverage by making sure all conditions in the

program source code are tested at least once.

– Decision table based testing - Testing the output by dealing with diﬀerent combi-

nations of inputs which produce diﬀerent results.

– Developer - An individual who writes, debugs, and executes the source code of a

software application.

– Dual coding - Testing the models created using two diﬀerent algorithms while using

the same or most common set of features.

– Equivalence partitioning - Testing a set of the group by picking a few values or

numbers to understood that all values from that group generate the same output.

– Error Guessing - Testing the output where the test analyst uses his / her experience

to guess the problematic areas of the application.

– Executive - An individual who establishes and directs the strategic long term goals,

policies, and procedures for an organization’s software development program.

– Fuzzing test - Testing the software for failures or error messages that are presented

due to unexpected or random inputs.

– Graph coverage - Testing code coverage by mapping executable statements and branches

to a control ﬂow graph and cover the graph in some way.

– Input space partitioning - Testing the output by dividing the input space according
to logical partitioning and choosing elements from the input space of the software being
tested.

– Integration testing - Asses software with respect to subsystem design.
– Logic coverage - Testing both semantic and syntactic meaning of how a logical ex-

pression is formulated.

Testing Research Software: A Survey

29

– Maintainer - An individual who builds source code into a binary package for distribu-

tion, commit patches or organize code in a source repository.

– Manager - An individual who is responsible for overseeing and coordinating the people,
resources, and processes required to deliver new software or upgrade existing products.
– Metamorphic testing - Testing how a particular change in input of the program would

change the output.

– Module testing - Asses software with respect to detailed design.
– Monte carlo test - Testing numerical results using repeated random sampling.
– Performance testing - Testing some of the non-functional quality attributes of soft-

ware like Stability, reliability, availability.

– Quality Assurance Engineer - An individual who tracks the development process,
oversee production, testing each part to ensure it meets standards before moving to the
next phase.

– State transition - Testing the outputs by changes to the input conditions or changes

to ’state’ of the system.

– Statement coverage - Testing code coverage by making sure all statements in the

program source code are tested at least once.

– Syntax-based testing - Testing the output using syntax to generate artifacts that are

valid or invalid.

– System testing - Asses software with respect to architectural design and overall be-

havior.

– Test driven development - Testing the output by writing an (initially failing) auto-
mated test case that deﬁnes a desired improvement or new function, then produces the
minimum amount of code to pass that test.

– Unit testing - Asses software with respect to implementation.
– Using machine learning - Testing the output values using diﬀerent machine learning

techniques.

– Using statistical tests - Testing the output values using diﬀerent statistical tests.

B List of Testing Techniques

This appendix provides the list of testing techniques respondents mentioned they were fa-
miliar with in response to the survey question Q9. The numbers in the parenthesis represent
how many respondents indicated that testing technique.

B.1 Testing Methods

Acceptance testing (9), Integration testing (43), System testing (14), Unit testing (87)

B.2 Testing Techniques

A/B testing (1), Accuracy testing (1), Alpha testing (1), Approval testing (2), Answer
testing (1), Assertions testing (3), Behavioral testing (1), Beta testing (1), Bit-for-bit (1),
Black-box testing (4), Built environment testing (1), Builtd testing (1), Checklist testing
(1), Checksum (1), Compatibility Testing (1), Concolic testing (1), Correctness tests (1),
Dependencies testing (1), Deployment testing (1), Dynamic testing (3), End-to-end testing
(2), Equivalence class (1), Engineering tests (1), Exploratory tests (1), Functional testing
(6), Fuzz testing (12), Golden master testing (1), Install testing (1), Jenkins automated
testing (1), Load testing (1), Manual testing (2), Memory testing (6), Mock testing (6),
Mutation testing (5), Penetration testing (1), Performance testing (6), Periodic testing (1),
Physics testing (1), Property-based testing (2), Random input testing (2), Reference runs
on test datasets (1), Regression testing (39), Reliability testing (1), Resolution testing (1),
Scientiﬁc testing (2), Security testing (1), Smoke test (2), Statistical testing (1), Stress test
(1), Usability testing (1), Use case test (1), User testing (2), Validation testing (9), White-
box testing (2)

30

Nasir U. Eisty, Jeﬀrey C. Carver

B.3 Testing Tools

CTest (3), gtest (1), jUnit (1)

B.4 Other types of QA

Code coverage (16), Code reviews (2), Documentation checking (3), Static analysis (6)

B.5 Others

Agile (1), Asan (1), Automatic test-case generation (1), Behavior-Driven Development (1),
Bamboo (1), Benchmarking (1), Caliper (1), Code style checking (1), Coding standards
(1), Comparison with analytical solutions (1), Continuous integration (33), Contracts (1),
DBC (1), Design by contract (1), Doctests (1), Formal Methods (2), GitLab (1), License
compliance (1), Linting (1), Method of exact solutions (1), Method of manufacture solution
(2), Monitoring production apps (1), Msan (1), N-version (1), Nightly (1), Pre-commit (1),
Proﬁling (1), Release (1), Run-time instrumentation and logging (1), Squish (1), Test-driven
development (18), Test suites (1), Tsan (1), Visual Studio (2)

References

Ackroyd KS, Kinder SH, Mant GR, Miller MC, Ramsdale CA, Stephenson PC (2008) Sci-
entiﬁc software development at a research facility. IEEE Software 25(4):44–51, DOI
10.1109/MS.2008.93

Ammann P, Oﬀutt J (2016) Introduction to Software Testing, 2nd edn. Cambridge Univer-

sity Press

Bourque P, Fairley RE (eds) (2014) SWEBOK: Guide to the Software Engineering Body
of Knowledge, version 3.0 edn. IEEE Computer Society, Los Alamitos, CA, URL http:
//www.swebok.org/

Carver JC, Eisty N (2021) Testing research software: Survey data. DOI 10.6084/m9.ﬁgshare.

16663561, URL https://figshare.com/articles/dataset/_/16663561/0

Clune T, Rood R (2011) Software testing and veriﬁcation in climate model development.

IEEE Software 28(6):49–55, DOI 10.1109/MS.2011.117

Drake JB, Jones PW, George R Carr J (2005) Overview of the software design of the
community climate system model. The Int’l Journal of High Performance Computing
Applications 19(3):177–186, DOI 10.1177/1094342005056094, URL https://doi.org/
10.1177/1094342005056094, https://doi.org/10.1177/1094342005056094

Easterbrook SM (2010) Climate change: A grand software challenge. In: Proceedings of the
FSE/SDP Workshop on Future of Soft. Eng. Research, ACM, FoSER ’10, pp 99–104,
DOI 10.1145/1882362.1882383, URL http://doi.acm.org/10.1145/1882362.1882383

Easterbrook SM, Johns TC (2009) Engineering the software for understanding climate
change. Computing in Science Engineering 11(6):65–74, DOI 10.1109/MCSE.2009.193
Eddins SL (2009) Automated software testing for matlab. Computing in Science Engineering

11(6):48–55, DOI 10.1109/MCSE.2009.186

Eisty NU, Thiruvathukal GK, Carver JC (2018) A survey of software metric use in research
software development. In: 2018 IEEE 14th Int’l Conf. on e-Science (e-Science), pp 212–
222, DOI 10.1109/eScience.2018.00036

Farrell PE, Piggott MD, Gorman GJ, Ham DA, Wilson CR, Bond TM (2011) Automated
continuous veriﬁcation for numerical simulation. Geoscientiﬁc Model Development
4(2):435–449, DOI 10.5194/gmd-4-435-2011, URL https://www.geosci-model-dev.
net/4/435/2011/

Testing Research Software: A Survey

31

Hannay JE, MacLeod C, Singer J, Langtangen HP, Pfahl D, Wilson G (2009) How do
scientists develop and use scientiﬁc software? In: 2009 ICSE Workshop on Soft. Eng. for
Computational Science and Engineering, pp 1–8, DOI 10.1109/SECSE.2009.5069155
Heaton D, Carver JC (2015) Claims about the use of software engineering practices in sci-
ence: A systematic literature review. Inf Softw Tech 67:207 – 219, DOI http://dx.doi.org/
10.1016/j.infsof.2015.07.011, URL http://www.sciencedirect.com/science/article/
pii/S0950584915001342

Hill C (2016) Socio-economic status and computer use: Designing software that supports
low-income users. In: 2016 IEEE Symp. on Visual Languages and Human-Centric Com-
puting, pp 1–1, DOI 10.1109/VLHCC.2016.7739651

Hochstein L, Basili VR (2008) The asc-alliance projects: A case study of large-scale parallel

scientiﬁc code development. Computer 41(3):50–58, DOI 10.1109/MC.2008.101

Hook D, Kelly D (2009) Testing for trustworthiness in scientiﬁc software. In: 2009 ICSE
Workshop on Soft. Eng. for Computational Science and Eng., pp 59–64, DOI 10.1109/
SECSE.2009.5069163

Kanewala U, Bieman JM (2014) Testing scientiﬁc software: A systematic literature review.
Inf Softw Technol 56(10):1219–1232, DOI 10.1016/j.infsof.2014.05.006, URL http://dx.
doi.org/10.1016/j.infsof.2014.05.006

Katz DS, McInnes LC, Bernholdt DE, Mayes AC, Hong NPC, Duckles J, Gesing S, Her-
oux MA, Hettrick S, Jimenez RC, Pierce M, Weaver B, Wilkins-Diehr N (2019) Com-
munity organizations: Changing the culture in which research software is developed
and sustained. Computing in Science Engineering 21(2):8–24, DOI 10.1109/MCSE.2018.
2883051

Kelly D, S R, Saint R, Floor P, Sanders R, Kelly D (2008) The challenge of testing scientiﬁc

software. In: in Proc. Conf. for the Association for Soft. Testing, pp 30–36

Kelly D, Hook D, Sanders R (2009) Five recommended practices for computational sci-

entists who write software. Computing in Science Engineering 11(5):48–53, DOI
10.1109/MCSE.2009.139

Kelly D, Thorsteinson S, Hook D (2011) Scientiﬁc software testing: Analysis with four

dimensions. IEEE Software 28(3):84–90, DOI 10.1109/MS.2010.88

Lemos GS, Martins E (2012) Speciﬁcation-guided golden run for analysis of robustness
testing results. In: 2012 IEEE Sixth Int’l Conf. on Soft. Security and Reliability, pp
157–166, DOI 10.1109/SERE.2012.28

Miller G (2006) A scientist’s nightmare: Software problem leads
314(5807):1856–1857, DOI

tions.
https://science.sciencemag.org/content/314/5807/1856,
sciencemag.org/content/314/5807/1856.full.pdf

to ﬁve retrac-
10.1126/science.314.5807.1856, URL
https://science.

Science

Murphy C, Raunak M, King A, Chen S, Imbraino C, Kaiser G, Lee I, Sokolsky O, Clarke
L, Osterweil L (2011) On eﬀective testing of health care simulation software. Technical
Reports (CIS) DOI 10.1145/1987993.1988003

Nguyen-Hoan L, Flint S, Sankaranarayana R (2010) A survey of scientiﬁc software devel-
opment. In: Proc. of the 2010 ACM-IEEE Int’l Symp. on Empirical Soft. Eng. and
Measurement, ACM, ESEM ’10, pp 12:1–12:10, DOI 10.1145/1852786.1852802, URL
http://doi.acm.org/10.1145/1852786.1852802

Post DE, Kendall RP (2004) Software project management and quality engineering prac-
tices for complex, coupled multiphysics, massively parallel computational simulations:
Lessons learned from asci. The Int’l Journal of High Performance Computing Applica-
tions 18(4):399–416, DOI 10.1177/1094342004048534, URL https://doi.org/10.1177/
1094342004048534, https://doi.org/10.1177/1094342004048534

Sanders R, Kelly D (2008) Dealing with risk in scientiﬁc software development. IEEE Soft-

ware 25(4):21–28, DOI 10.1109/MS.2008.84

Segal J (2005) When software engineers met research scientists: A case study. Empirical

Software Engineering 10, DOI 10.1007/s10664-005-3865-y

Segal J (2009) Software development cultures and cooperation problems: A ﬁeld study of the
early stages of development of software for a scientiﬁc community. Computer Supported
Cooperative Work 18(5):581, DOI 10.1007/s10606-009-9096-9, URL https://doi.org/
10.1007/s10606-009-9096-9

32

Nasir U. Eisty, Jeﬀrey C. Carver

Vilkomir SA, Swain WT, Poore JH, Clarno KT (2008) Modeling input space for testing
scientiﬁc computational software: A case study. In: Bubak M, van Albada GD, Dongarra
J, Sloot PMA (eds) Computational Science – ICCS 2008, Springer Berlin Heidelberg,
pp 291–300

