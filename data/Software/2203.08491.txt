2
2
0
2

r
a

M
6
1

]

G
L
.
s
c
[

1
v
1
9
4
8
0
.
3
0
2
2
:
v
i
X
r
a

Deepchecks: A Library for Testing and Validating Machine Learning Models and Data

Deepchecks: A Library for Testing and Validating Machine
Learning Models and Data

Shir Chorev
Philip Tannor
Dan Ben Israel
Noam Bressler
Itay Gabbay
Nir Hutnik
Jonatan Liberman
Matan Perlmutter
Yurii Romanyshyn
Deepchecks Ltd.
Derech Menachem Begin 14
Ramat Gan, 5270002
Israel

Lior Rokach,
Deepchecks Ltd. and
Department of Software and Info. Sys. Eng.,
Ben-Gurion University of the Negev.

shir@deepchecks.com
philip@deepchecks.com
danb@deepchecks.com
noam@deepchecks.com
itay@deepchecks.com
nir@deepchecks.com
jonatan@deepchecks.com
matan@deepchecks.com
yurii@deepchecks.com

liorrk@bgu.ac.il

Abstract

This paper presents Deepchecks, a Python library for comprehensively validating machine
learning models and data. Our goal is to provide an easy-to-use library comprising of
many checks related to various types of issues, such as model predictive performance,
data integrity, data distribution mismatches, and more. The package is distributed under
the GNU Aﬀero General Public License (AGPL) and relies on core libraries from the
scientiﬁc Python ecosystem: scikit-learn, PyTorch, NumPy, pandas, and SciPy. Source
code, documentation, examples, and an extensive user guide can be found at https://
github.com/deepchecks/deepchecks and https://docs.deepchecks.com/.

Keywords: Supervised Learning, Testing Machine Learning, Random Forest, Gradient
Boosting Machine, Concept Drift, Python, Data Leakage, MLOps, Bias, Explainable AI
(XAI)

1. Introduction

Machine learning models are becoming increasingly popular in a variety of ﬁelds, including
healthcare, ﬁnance, biology, and others. Complex models can now be easily trained using
modern software packages and yield high predictive performance on test sets. Nevertheless,
models are often challenged when deployed outside the lab.

As indicated in previous works (e.g.

(Xie et al., 2011)) detecting faults in machine
learning models can be diﬃcult. This is especially true when models are used in sensitive

1

 
 
 
 
 
 
Shir Chorev, Philip Tannor et al.

decision-making processes, where mistakes can have serious consequences. Despite the
sensitivity of the matter, many ML models go into production unmonitored and without
proper testing, and thus they are prone to signiﬁcant risks.

Running ML models in the real world poses a variety of challenges, which may cause

degradation in its predictive performance, e.g.:

• Data integrity issues: Data pipelines are often complex, and the format of the data
may change over time. Fields may be renamed, categories may be added or split, and
more. Such changes can have a major impact on your model’s performance.

• Data drift and concept drift: Data in the real world is constantly changing. This in
turn may aﬀect the distribution of the data that is being fed to the model, or that
of the desired target prediction. Thus, the data the model was trained on becomes
less and less relevant over time. The format of the data is still valid, but the model
will become unstable and its predictive performance will deteriorate over time (in
particular in the case of out-of-distribution (Geirhos et al., 2020)).

2. The Deepchecks Library: An Overview

Deepchecks library introduces a framework of data, models, checks, suites, and conditions
that enable customizable and extensible testing for ML. It suggests a uniﬁed framework
with a corresponding API for testing models and data and enables the user to concatenate
several checks to be executed later as a single test command.

In addition, Deepchecks library comes with many built-in checks and suites that can help
validate various points throughout the machine learning development process (see Figure
1).

Of course, every process has its unique steps and challenges, and therefore all checks and
suites can be easily customized. In particular, the user can add new checks and their results
to support the validation of various phases in the pipeline. Alongside that, we have identiﬁed
that there are several recurring scenarios, that each has its own needs and characteristics.
In particular, deepchecks includes pre-deﬁned suites for the following scenarios:

• New Data: When a user starts working on a new task, Deepchecks helps to validate
data’s integrity (For example, detecting duplicate samples, problems with string or
categorical features, signiﬁcant outliers, inconsistent labels, etc.)

• After Splitting the Data (Train-Test Validation) - When splitting the data (e.g. to
train, validation or test), and just before training the model, Deepchecks ensures
that the splits are indeed representative. For example, it veriﬁes that the classes are
balanced similarly, that there is no signiﬁcant drift in the distributions between the
features or labels in each of the datasets, that there is no potential data leakage that
may contaminate the model, etc.

• After Training a Model (Analysis and Validation) - Once a trained model is available
Deepchecks examines several performance metrics, compares them to various bench-
marks, and provides a clear picture about the model’s performance. Furthermore,

2

Deepchecks: A Library for Testing and Validating Machine Learning Models and Data

Deepchecks attempts to ﬁnd sub-spaces where the model underperforms and provide
insights that may be used to improve its performance.

While the above-mentioned scenarios are predeﬁned, a common use case is also to run
speciﬁc checks on an ”on-demand” basis. This is particularly useful when the user is looking
into a problem, such as over-ﬁtting.

Figure 1: When Deepchecks library can be used

3. Deepchecks’ Building Blocks

Figure 2 presents a typical ﬂow of the Deepchecks library. Depending on the checks that the
user wishes to execute, some of the following objects should be provided as input: Raw data
(before pre-processing), with optional labels, the training data (after pre-processing) with
the target attribute, test data (which the model is not exposed to), with optional labels,
and the model to be checked.

Figure 2: The building blocks of Deepchecks library

3

Shir Chorev, Philip Tannor et al.

For tabular data, the Deepchecks library requires that the model have a predict method
for regression tasks and in addition predict proba method for classiﬁcation tasks, both of
which should be implemented using the scikit-learn API conventions (Pedregosa et al.,
2011). Some checks may attempt using additional model methods if those exist. For
example, it uses the built-in feature importance property if it exists, and if it does not,
it calculates the feature importance using permutation importance procedure (Breiman,
2001). Note that built-in scikit-learn classiﬁers and regressors, along with many additional
popular models types (e.g. XGBoost (Chen and Guestrin, 2016), LightGBM (Ke et al.,
2017), CatBoost(Prokhorenkova et al., 2018) etc.) implement these methods and are thus
supported.

The three main building blocks of the Deepchecks library are checks, conditions, and

suites.

3.1 Checks

A check aims to inspect a speciﬁc aspect of the data or model. Checks can cover all kinds
of common issues, such as data leakage, concept drift, etc. To date, Deepchecks library
contains more than 39 checks and supports checks for classiﬁcation and regression models
that are trained on tabular data. The library includes ﬁve main categories of checks (called
modules):

1. distribution - this module contains various checks for estimating if the training and
test set have diﬀerent distributions. This module includes checks such as: comparing
the model’s trust score (Jiang et al., 2018) of the train and test sets; calculating the
drift between the train set and test set per each input feature (in particular Earth
Movers Distance for numerical variables and Population Stability Index for nominal
features); calculating the target attribute drift between train set and test set.

2. integrity - this module contains all data integrity checks, such as: checking for
duplicate samples in the dataset; detecting a small amount of a rare data type within
a column, such as few string samples in a mostly numeric column; checking if there
are columns which have only a single unique value in all rows, etc.

3. methodology - this module contains checks for methodological ﬂaws in the model
building process, such as: Checking for overﬁtting caused by using too many iterations
in a gradient boosted model; Detect features that are nearly unused by the model;
Calculating the Predictive Power Score1 of all features, in order to detect features
whose ability to predict the target is due to leakage; Detecting samples in the test
data that appear also in training data.

4. evaluation - Module that contains checks of model performance metrics, such as:
Comparing the predictive performance of a given model to that of a relatively simple
model (e.g. a single tree model) which is used as a baseline for performance compar-
ison.; Calculating the calibration curve with brier score for each class; Checking the
distribution of errors; Finding features that best split the data into segments of high
and low model error.

1. https://github.com/8080labs/ppscore

4

Deepchecks: A Library for Testing and Validating Machine Learning Models and Data

5. overview - Module that provides meta-information regarding the model and the

dataset, such as: the role and logical type of each column; the model’s hyper-parameters,
etc.

Each check can have two types of results:

1. A visual result meant for display (e.g. a ﬁgure or a table). Figure 3 shows one of the
outputs that measure the distribution drift between the train set and the test set for
two input attributes.

2. A return value that can be used for validating the expected check results. These
values can be further used as predicates in conditions (validations are typically done
by adding a ”condition” to the check, as explained below).

Figure 3: An illustration of drift check output. The upper graph shows a feature with
relatively high drift score and the lower graph shows a feature with relatively low
drift score

3.2 Condition

A condition is a function that can be added to a Check for validating if the Check’s return
value complies with a predeﬁned threshold or logic. A condition returns a status of either
pass, fail, or warning result, as well as a statement that describes the status (e.g. ”found

5

Shir Chorev, Philip Tannor et al.

7% duplicate samples”). The last two results indicate that a ﬂaw may exist and further
investigations are required.

For example, a condition attached to the check of DataDuplicates may return the
status fail if there are more than 5% duplicate samples in the training set. This may be
valid if the duplicates are there on purpose (e.g., as a result of intentional oversampling,
or because the dataset’s nature has identical-looking samples), but if this is a hidden issue
that is not expected to occur, it may be an indicator for a problem in the data pipeline that
needs to be addressed (Barz and Denzler, 2020).

3.3 Suites

A suite is an ordered collection of checks, that can have conditions added to them. Once
a suite is executed, a summary report is generated which consists of high level results and
detailed results. The suite mechanism enables eﬃciently running a large group of checks
with a single call and displaying a concluding report for all of the Checks that ran. The
library comes with a list of common predeﬁned suites for tabular data. The user can build
a customized validation scenario, adapted per models, domains, and timing in the pipeline.
These suits can be shared with the community and re-used, serving as a framework for
methodological testing.

4. Conclusion and Future Work

Here, we presented Deepchecks, a library of validating machine learning models and their
corresponding datasets. The library currently supports classiﬁcation and regression mod-
els for tabular data, and at the time of writing is in beta version for computer vision
models trained on images 2. We are continuously adding new checks, improving usabil-
ity, documents, and tutorials. Finally, we welcome contributors to help us at https:
//github.com/deepchecks/deepchecks.

Acknowledgments

We would like to thank everyone on this list for contributing to this project: https:
//github.com/deepchecks/deepchecks/graphs/contributors. We would like to thank
the users of Deepchecks library for the continuous valid feedback over the last year.

2. https://docs.deepchecks.com/en/latest/examples/vision/checks/

6

Deepchecks: A Library for Testing and Validating Machine Learning Models and Data

References

Bj¨orn Barz and Joachim Denzler. Do we train on test data? purging cifar of near-duplicates.

Journal of Imaging, 6(6):41, 2020.

Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.

Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings
of the 22nd acm sigkdd international conference on knowledge discovery and data mining,
pages 785–794, 2016.

Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence, 2(11):665–673, 2020.

Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a

classiﬁer. Advances in neural information processing systems, 31, 2018.

Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye,
and Tie-Yan Liu. Lightgbm: A highly eﬃcient gradient boosting decision tree. Advances
in neural information processing systems, 30, 2017.

Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand
Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learn-
ing research, 12:2825–2830, 2011.

Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and
Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural
information processing systems, 31, 2018.

Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and Tsong Yueh
Chen. Testing and validating machine learning classiﬁers by metamorphic testing. Journal
of Systems and Software, 84(4):544–558, 2011.

7

