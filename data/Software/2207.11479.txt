3D LABELING TOOL

by

John RACHWAN
Charbel ZLAKET

Submitted to the
Faculty of Engineering

In partial fulﬁllment of the requirement
For the degree of Bachelor
of Engineering - Computer
Engineering
at the
Holy Spirit University of Kaslik (USEK)

Kaslik, Lebanon

2
2
0
2

l
u
J

3
2

]

V
C
.
s
c
[

1
v
9
7
4
1
1
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
ii

3D LABELING TOOL

by

John RACHWAN
Charbel ZLAKET

APPROVAL:

Prof. Joseph ZALAKET

Advisor / Supervisor

Associate Dean

[Signature]

Dr. Charles YAACOUB

Internal / External Examiner

Associate Professor

[Signature]

Dr. Adib AKL

Internal / External Examiner

Assistant Professor

[Signature]

Date Defended : July 29, 2019

iii

Plagiarism Statement

I conﬁrm that this ﬁnal year project is my own work, is not copied from any other

person’s work (published or unpublished) and has not been previously submitted for

assessment either at the Holy Spirit University of Kaslik (USEK) or elsewhere.

I conﬁrm that I have read and understood the Academic Integrity regulations on

plagiarism in the Academic Rules and Student Life Handbook.

Signature

Date

iv

Final Year Project
Release Form

I, the under signed, hereby submit this ﬁnal year project to the Holy Spirit University

of Kaslik (USEK)

as partial fulﬁllment of the requirements for a Bachelor of Engineering’s degree.

(cid:3) By signing and submitting this agreement: I grant USEK Library the right to

(a) reproduce electronic copies of my ﬁnal year project for the purpose of

preservation

(b) include the scholarly material ﬁnal year project in the archives and digital

repositories at the Holy Spirit University of Kaslik “USEK Digital Gate”

(c) release my ﬁnal year project or dissertation to ProQuest/UMI

(d) keep more than one copy of my ﬁnal year project or dissertation for pur-

poses of security and backup

(e) reproduce, publicly display, and distribute the material to users world-

wide at no cost for academic purposes

(cid:3) I should inform the Main Library at the University if I intend to publish or

post my ﬁnal year project before the transaction is completed and listing the

University as my afﬁliation.

(cid:3) I request an embargo of this ﬁnal year project for . . . . . . . . . . . . . months (max-

imum of 36 months) from the date of submission of the ﬁnal year project.

USEK will clearly identify your name as the author or owner of the ﬁnal year

project, and will not make any alteration, other than as allowed by this license,

to your submission.

Signature

Date

v

ACKNOWLEDGMENT

We would like to express our immense gratitude for all the people who supported

us during the period of this project. Beginning with BMW Group for providing us

with the necessary hardware and connections to make this project possible.

Speciﬁcally, we thank Mr. Norman Muller, our supervisor at BMW Group for his

continued support and hands on contribution to the project. We would also like

to thank our lab members Miss Joyce Abi Saleh and Mr. Hadi Ayoub for their

assistance in certain parts of the project that matched their expertise. Additionally,

we thank Logitech’s virtual reality team for providing us with their prototype pen.

On a special note, we address our warmest thanks towards our supervisor Prof.

Joseph ZALAKET and the head of department Dr. Tilda KARKOUR AKIKI for

giving us the opportunity to work with BMW Group. We are also thankful for their

continuous support, motivation and assistance during the period of this project.

vi

ABSTRACT

Training and testing supervised object detection models require a large collection

of images with ground truth labels. Labels deﬁne object classes in the image, as

well as their locations, shape, and possibly other information such as pose. The

labeling process has proven extremely time consuming, even with the presence of

manpower. We introduce a novel labeling tool for 2D images as well as 3D trian-

gular meshes: 3D Labeling Tool (3DLT). This is a standalone, feature-heavy and

cross-platform software that does not require installation and can run on Windows,

macOS and Linux-based distributions. Instead of labeling the same object on every

image separately like current tools, we use depth information to reconstruct a trian-

gular mesh from said images and label the object only once on the aforementioned

mesh. We use registration to simplify 3D labeling, outlier detection to improve 2D

bounding box calculation and surface reconstruction to expand labeling possibility

to large point clouds. Our tool is tested against state of the art methods and it greatly

surpasses them in terms of speed while preserving accuracy and ease of use.

Keywords: Object Detection – Image Labeling – Mesh Labeling – Registration –

Outlier Detection – Surface Reconstruction

vii

RESUME

La formation et la mise à l’essai de modèles de détection d’objets supervisés néces-

sitent une grande collection d’images avec des étiquettes de vérité (groundtruth).

Les étiquettes déﬁnissent les classes d’objets dans l’image, ainsi que leur emplace-

ment, leur forme et probablement d’autres informations telles que la pose. Le

processus d’étiquetage s’est avéré extrêmement long, même en présence de main-

d’œuvre. Nous introduisons un nouvel outil d’étiquetage pour les images 2D ainsi

que les maillages triangulaires 3D : 3D Labeling Tool (3DLT). Il s’agit d’un logi-

ciel autonome, riche en fonctionnalités et multiplateforme qui ne nécessite pas

d’installation et qui peut fonctionner sous Windows, MacOS et Linux. Au lieu

d’étiqueter le même objet sur chaque image séparément comme les outils actuels,

nous utilisons les informations de profondeur pour reconstruire un maillage trian-

gulaire à partir desdites images et étiqueter l’objet une seule fois sur ledit maillage.

Nous utilisons le concept d’enregistrement pour simpliﬁer l’étiquetage 3D, la détec-

tion des valeurs aberrantes pour améliorer le calcul des rectangles de délimitation

2D et la reconstruction de surface pour étendre les possibilités d’étiquetage aux

gros nuages de points. Notre outil est comparé à des méthodes de pointe et il les

surpasse largement en termes de vitesse tout en conservant la précision et la facilité

d’utilisation.

Mots-clés: Détection d’Objets – Étiquetage des Images – Étiquetage de Maille –

Enregistrement – Détection des Valeurs Aberrantes – Reconstruction de Surface

viii

TABLE OF CONTENTS

PLAGIARISM STATEMENT . . . . . . . . . . . . . . . . . . . . . . .

ACKNOWLEDGMENT . . . . . . . . . . . . . . . . . . . . . . . . . .

iv

vi

ABSTRACT .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii

TABLE OF CONTENTS . . . . . . . . . . . . . . . . . . . . . . . . . .

ix

List of Figures .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii

List of Tables .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii

ACRONYMS .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix

1

Introduction

2 Related Work

2.1 Manual Annotation . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 LabelImg . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2 Labelbox . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.3 Anno-Mage . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2

Intensive Group Annotation . . . . . . . . . . . . . . . . . . . . . .

2.3 Collaborative Annotation Over the Internet

. . . . . . . . . . . . .

3 Data Set Importer

3.1

Introduction .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1 Camera Speciﬁcations

. . . . . . . . . . . . . . . . . . . .

3.2.2 Recording and Processing . . . . . . . . . . . . . . . . . .

3.2.3 Camera Trajectory . . . . . . . . . . . . . . . . . . . . . .

3.2.4 Mesh Construction . . . . . . . . . . . . . . . . . . . . . .

ix

1

5

5

6

6

6

7

7

8

8

8

8

9

9

9

3.2.5

IRIS Format

. . . . . . . . . . . . . . . . . . . . . . . . . 10

3.3 PLY File Importer . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

3.3.1

Polygon File Format

. . . . . . . . . . . . . . . . . . . . . 11

3.3.2

PLY Parser in Unity . . . . . . . . . . . . . . . . . . . . . 13

4 User Interface

16

4.1

Introduction .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

4.2 Workﬂow of the 3D Labeling Tool

. . . . . . . . . . . . . . . . . . 16

4.2.1

Import Window . . . . . . . . . . . . . . . . . . . . . . . . 16

4.2.2 Main Labeling Window . . . . . . . . . . . . . . . . . . . 17

4.3 User Interface Enhancements . . . . . . . . . . . . . . . . . . . . . 20

4.3.1

Import Window Enhancements . . . . . . . . . . . . . . . . 20

4.3.2 Main Window Enhancements

. . . . . . . . . . . . . . . . 21

5 Bounding Box

24

5.1 Minimum Bounding Box . . . . . . . . . . . . . . . . . . . . . . . 24

5.2 Axis-Aligned Minimum Bounding Box . . . . . . . . . . . . . . . 24

5.3 Calculating the Minimum Bounding Rectangle

. . . . . . . . . . . 25

5.3.1

Projection of Labeling Object onto the Image . . . . . . . . 25

5.3.2 Minimum Bounding Rectangle . . . . . . . . . . . . . . . . 28

5.3.3 Outlier Detection . . . . . . . . . . . . . . . . . . . . . . . 31

5.3.4

Implementation . . . . . . . . . . . . . . . . . . . . . . . . 44

5.3.5 Drawing the bounding rectangle on image . . . . . . . . . . 44

5.3.6 Exporting Annotations . . . . . . . . . . . . . . . . . . . . 46

6 Phantom Mode

47

6.1

Introduction .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

6.2 Rigid Registration Methods . . . . . . . . . . . . . . . . . . . . . . 49

6.3 Nonrigid Registration . . . . . . . . . . . . . . . . . . . . . . . . . 50

6.3.1 Optimization methods

. . . . . . . . . . . . . . . . . . . . 50

6.4 Choice of Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 56

6.4.1 Registration Accuracy . . . . . . . . . . . . . . . . . . . . 56

x

6.4.2 Correspondence . . . . . . . . . . . . . . . . . . . . . . . . 57

6.4.3 Limiting to 9 DoF . . . . . . . . . . . . . . . . . . . . . . 57

6.5 TPS-RPM .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

6.5.1 Thin Plate Splines

. . . . . . . . . . . . . . . . . . . . . . 57

6.5.2 Robust Point Matching Algorithm Description . . . . . . . 61

6.5.3

Integrating TPS in RPM . . . . . . . . . . . . . . . . . . . 65

6.5.4

Problem with correspondence . . . . . . . . . . . . . . . . 66

6.5.5 Algorithm Pseudo-Code . . . . . . . . . . . . . . . . . . . 69

6.5.6

Implementation . . . . . . . . . . . . . . . . . . . . . . . . 71

6.6 Colliders in Unity3D . . . . . . . . . . . . . . . . . . . . . . . . . 71

6.6.1 Mesh Simpliﬁcation . . . . . . . . . . . . . . . . . . . . . 73

6.7 Websockets .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

6.7.1

Server Implementation . . . . . . . . . . . . . . . . . . . . 78

6.7.2 Message Structure . . . . . . . . . . . . . . . . . . . . . . 80

6.7.3 Client Implementation . . . . . . . . . . . . . . . . . . . . 80

6.8 The Labeling Element Transformation . . . . . . . . . . . . . . . . 82

6.9 User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

7 Labeling Without a Mesh

84

7.1 Problem Statement

. . . . . . . . . . . . . . . . . . . . . . . . . . 84

7.2 Assumption .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

7.3 Theory .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

7.3.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

7.3.2

Problem Breakdown . . . . . . . . . . . . . . . . . . . . . 85

7.3.3

Solving the Equation . . . . . . . . . . . . . . . . . . . . . 87

7.3.4 Rigid Registration . . . . . . . . . . . . . . . . . . . . . . 89

7.4 Problem Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . 89

7.5

Implementation in Unity . . . . . . . . . . . . . . . . . . . . . . . 90

7.5.1 CAD Objects as Labeling Elements . . . . . . . . . . . . . 90

7.5.2

Point Selection . . . . . . . . . . . . . . . . . . . . . . . . 92

7.5.3

Sending the Points . . . . . . . . . . . . . . . . . . . . . . 94

xi

7.6

Implementation in Python Server . . . . . . . . . . . . . . . . . . . 95

7.7 Results .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

8 Labeling Large Point Clouds

98

8.1 Motivation . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

8.2 Surface Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 98

8.2.1

Point Cloud Imperfections . . . . . . . . . . . . . . . . . . 99

8.2.2

Input

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

8.2.3

Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

8.2.4 Analysis

. . . . . . . . . . . . . . . . . . . . . . . . . . . 107

8.2.5 Global Surface Smoothness Prior

. . . . . . . . . . . . . . 109

8.2.6 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . 111

8.2.7

Point Cloud Size . . . . . . . . . . . . . . . . . . . . . . . 114

8.2.8 Workﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

8.2.9

Implementation . . . . . . . . . . . . . . . . . . . . . . . . 117

8.3 Data Sampler

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

8.3.1 Recording and Generating the RGBD Data . . . . . . . . . 121

8.3.2 Generating the Camera Information . . . . . . . . . . . . . 122

9 Snapping

124

9.1

Introduction .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

9.2 Data Pre-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . 124

9.2.1 Depth Camera Setup . . . . . . . . . . . . . . . . . . . . . 125

9.2.2 Capturing the Depth Images . . . . . . . . . . . . . . . . . 126

9.2.3 Generating the Camera Intrinsic and Extrinsic . . . . . . . . 126

9.2.4

Sending the Data . . . . . . . . . . . . . . . . . . . . . . . 127

9.3 Server Side .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127

9.3.1 Extracting depth values . . . . . . . . . . . . . . . . . . . . 127

9.3.2 Calculating World Space Coordinates . . . . . . . . . . . . 128

9.3.3 Reducing Mesh Points . . . . . . . . . . . . . . . . . . . . 129

9.3.4 Registration . . . . . . . . . . . . . . . . . . . . . . . . . . 129

9.4 Results .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

xii

10 Labeling in Virtual Reality

132

10.1 Introduction .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

10.2 First Integration into Unity . . . . . . . . . . . . . . . . . . . . . . 132

10.2.1 Teleportation in VR . . . . . . . . . . . . . . . . . . . . . . 132

10.2.2 Switching from 2D to VR . . . . . . . . . . . . . . . . . . 133

10.2.3 Basic Object Manipulations Using SteamVR . . . . . . . . . 133

10.3 Collaboration with Logitech . . . . . . . . . . . . . . . . . . . . . 134

10.3.1 Logitech’s Stylus Pen . . . . . . . . . . . . . . . . . . . . . 135

10.3.2 Integration into the 3D Labeling Tool

. . . . . . . . . . . . 136

10.4 Added Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

10.4.1 User Interface . . . . . . . . . . . . . . . . . . . . . . . . . 137

10.4.2 Labeling Objects Creation . . . . . . . . . . . . . . . . . . 137

10.4.3 Objects Manipulation . . . . . . . . . . . . . . . . . . . . . 140

10.4.4 World Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 142

11 Experimental Results

143

11.1 Experiment 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

11.2 Experiment 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

11.3 Discussion .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

12 Conclusion

147

xiii

List of Figures

3.1 Data Collection Workﬂow Diagram . . . . . . . . . . . . . . . . . 10

3.2

IRIS Format Example . . . . . . . . . . . . . . . . . . . . . . . . . 11

3.3 PLY File Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

3.4

Imported mesh in the tool . . . . . . . . . . . . . . . . . . . . . . . 14

3.5 Point Cloud example with labeling object

. . . . . . . . . . . . . . 15

4.1

Import Window Example . . . . . . . . . . . . . . . . . . . . . . . 17

4.2 Main Labeling Window Example . . . . . . . . . . . . . . . . . . . 18

4.3 Run-Time Gizmo Tool

. . . . . . . . . . . . . . . . . . . . . . . . 19

4.4 Color Picker .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

4.5 New Import Window Design . . . . . . . . . . . . . . . . . . . . . 21

4.6 Mesh Top View Example . . . . . . . . . . . . . . . . . . . . . . . 22

4.7 Element Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

4.8 New Main Labeling Window Example . . . . . . . . . . . . . . . . 23

5.1 Shot View Example . . . . . . . . . . . . . . . . . . . . . . . . . . 25

5.2 Principal Point Offset . . . . . . . . . . . . . . . . . . . . . . . . . 27

5.3 Blocking Mesh Example . . . . . . . . . . . . . . . . . . . . . . . 28

5.4 Sent Image of the Bounding Box . . . . . . . . . . . . . . . . . . . 29

5.5 Example of an object blocking the annotated object. . . . . . . . . . 32

5.6 Bounding box calculated using the simple method.

. . . . . . . . . 32

5.7 Bounding box calculated using the simple method with outlier de-

tection enabled.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

5.8 Example of global anomalies(x1,x2), local anomaly x3 and micro-

cluster c3 [26] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

xiv

5.9 Covariance matrix for 3-dimensional data . . . . . . . . . . . . . . 40

5.10 Bounding Box Example . . . . . . . . . . . . . . . . . . . . . . . . 45

5.11 2D & 3D Annotations Example . . . . . . . . . . . . . . . . . . . . 46

6.1 A Thin Plate Splines that passes through a set of control points . . . 58

6.2 Thin plates can be used to deform image (a) to (b) . . . . . . . . . . 60

6.3

Interpolation of displacement in the (a) x direction and in the (b) y

direction .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

6.4 An example of the binary correspondence matrix.

. . . . . . . . . . 62

6.5 Box and Sphere Colliders . . . . . . . . . . . . . . . . . . . . . . . 72

6.6 Convex Mesh Collider

. . . . . . . . . . . . . . . . . . . . . . . . 73

6.7 Edge Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

6.8 Non-Edge Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . 75

6.9 Loading Collider Flowchart . . . . . . . . . . . . . . . . . . . . . . 77

6.10 Concave Mesh Collider . . . . . . . . . . . . . . . . . . . . . . . . 77

6.11 Message Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

6.12 Python JSON handling . . . . . . . . . . . . . . . . . . . . . . . . 80

6.13 Phantom Mode User Interface . . . . . . . . . . . . . . . . . . . . 83

7.1 Rectangular Cuboid B . . . . . . . . . . . . . . . . . . . . . . . . . 85

7.2 Problem Representation . . . . . . . . . . . . . . . . . . . . . . . . 85

7.3 Bounded Problem B . . . . . . . . . . . . . . . . . . . . . . . . . . 88

7.4 CAD Object Example in Scene . . . . . . . . . . . . . . . . . . . . 90

7.5

7.6

Inventory Panel

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

“2D" Toggle .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93

7.7 Source Point Selection Example . . . . . . . . . . . . . . . . . . . 93

7.8 Selecting Points on Image Example

. . . . . . . . . . . . . . . . . 95

7.9 Results - 1 .

7.10 Results - 2 .

7.11 Results - 3 .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

8.1

2D representation of point cloud imperfections

. . . . . . . . . . . 100

8.2 Example of Point Cloud received from BMW plants . . . . . . . . . 100

xv

8.3 Taking the gradient of the indicator function χ reveals its connec-

tion to the point cloud normals. Poisson reconstruction [81] opti-

mizes for an indicator function whose gradient at P is aligned to

N.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

8.4 Depth 10 reconstructions of Neptune with a close up on the beard

using: original Poisson, Wavelet, SSD and Screened Poisson (left

to right)

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

8.5 Depth 10 reconstructions of David with a close up on the eye using:

original Poisson, Wavelet, SSD and Screened Poisson (left to right) . 112

8.6 For all models, plots of one-sided RMS errors, measured from the

evaluation points to the reconstructed surface, as a function of the

resolution depth (8, 9, and 10)

. . . . . . . . . . . . . . . . . . . . 112

8.7 Difference between virtual machine and docker architectures . . . . 117

8.8 Top Level View of Surface Reconstruction Workﬂow . . . . . . . . 120

8.9 Example of a Reconstructed Mesh of the Point Cloud in Figure 8.2 . 121

8.10 Data Sampler UI Example . . . . . . . . . . . . . . . . . . . . . . 122

8.11 Sampler Generated Data Imported in 3DLT . . . . . . . . . . . . . 123

9.1 Depth Images Example . . . . . . . . . . . . . . . . . . . . . . . . 126

9.2

u,v image coordinate axes . . . . . . . . . . . . . . . . . . . . . . . 128

9.3 Resulting matrix containing three columns, [u, v, z] . . . . . . . . . 128

9.4 Snapping Example . . . . . . . . . . . . . . . . . . . . . . . . . . 131

10.1 HTC Vive Pro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

10.2 Teleportation Example . . . . . . . . . . . . . . . . . . . . . . . . 133

10.3 HTC VIVE Controllers . . . . . . . . . . . . . . . . . . . . . . . . 134

10.4 Grabbed Object Example . . . . . . . . . . . . . . . . . . . . . . . 135

10.5 Logitech Stylus Pen . . . . . . . . . . . . . . . . . . . . . . . . . . 135

10.6 Stylus And HTC Models in VR . . . . . . . . . . . . . . . . . . . . 136

10.7 VR User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

10.8 Plane Projection to Add Depth Example . . . . . . . . . . . . . . . 138

10.9 Drawing a Cuboid Example . . . . . . . . . . . . . . . . . . . . . . 139

xvi

10.10Convex Hull to Cuboid Example . . . . . . . . . . . . . . . . . . . 140

10.11Convex Hull Example . . . . . . . . . . . . . . . . . . . . . . . . . 140

10.12Cad Objects in VR . . . . . . . . . . . . . . . . . . . . . . . . . . 141

10.13World Scale Setting in UI . . . . . . . . . . . . . . . . . . . . . . . 142

11.1 Comparison on Different Data Sets . . . . . . . . . . . . . . . . . . 145

11.2 Comparison on Different Number of Objects . . . . . . . . . . . . . 146

xvii

List of Tables

5.1 Outlier detection methods . . . . . . . . . . . . . . . . . . . . . . . 35

5.2 Comparison of AUC between selected models on different data sets

43

5.3 Comparison of time in seconds between selected models on differ-

ent data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

8.1 Comparison of runtime in seconds between the four models for

depths 8,9,10 and 11 on Neptune and David point clouds . . . . . . 113

8.2 Comparison of memory usage in MB between the four models for

depths 8,9,10 and 11 on Neptune and David point clouds . . . . . . 113

xviii

ACRONYMS

L2 : Euclidean distance

L2E : Euclidean distance Estimation

3DLT : 3D Labeling Tool

AABB : Axis-Aligned Bounding Box

ANN : Artiﬁcial Neural Networks

API: Application Program Interface

AUC : Area Under Curve

BMW : Bavarian Motor Works

CBLOF : Cluster-Based Local Outlier Factor

CLI : Command Line Interface

CPPSR : Color-Based Point Set Registration

CPU: Central Processing Unit

DLD : Dependent landmark drift

DoF : Degrees of Freedom

EM : Expectation Maximization

FFT : Fast Fourrier Transform

GAN : Generative Adversarial Neural Network

GLTP : Global-Local Topology Preservation

GMM : Gaussian Mixture Model

GPU: Graphics Processing Unit

GUI : Graphical User Interface

HBOS : Histogram-Based Outlier Score

ICP : Iterative Closest Point

IPDA : Point Clouds Registration with Probabilistic Data Association

IR : Infrared

JSON : JavaScript Object Notation

K-NN : K Nearest Neighbour

LLE : Local Linear Embedding

LOCI : Local Correlation Integral

LOF : Local Outlier Factor

xix

LRD : Local Reachability Density

LU : Lower-Upper

LiDAR : Laser Imaging, Detection And Ranging

LoOP : Local Outlier Probability

MAP : Maximum a Priori

MBB : Minimum Bounding Box

ML : Maximum Likelihood

MSD : Micro- structure descriptor

OCSVM : One-Class Support Vector Machine

OS : Operation System

PCA : Principal Component Analysis

PNG: Portable Network Graphics

RAM: Random-Access Memory

RBF : Radial Basis Function

RGBA : Red Green Blue Alpha

RMS : Root Mean Square

ROC : Receiver Operating Characteristic

RPM-VFC : Robust Point Matching via Vector Field Consensus

RPM : Robust Point Matching

SDK: Software Development Kit

SLAM : Simultaneous Localization and Mapping

SSD : Smooth Signed Distance

SVGM : Support Vector-Parametrized Gaussian Mixture

SVM : Support Vector Machine

SVR : Support Vector Registration

TOF : Time of Flight

TPS : Thin Plate Splines

VM : Virtual Machine

VR : Virtual Reality

XML : Extensible Markup Language

rPCA : robust Principal Component Analysis

xx

Chapter 1

Introduction

Manual annotation of digital images is a fundamental processing stage in many re-

search projects and industrial applications. It requires human annotators to deﬁne

and describe spatial regions associated with an image. Spatial regions are deﬁned

using standard axis-aligned minimum bounding rectangles and are described using

textual metadata. A manual labeling tool permits human annotators to deﬁne these

spatial regions around a desired object. Current state of the art tools have made

the labeling process a very long and tedious affair, one that delays development of

industrial applications and research in object detection. For instance, the process of

preparing the necessary training and testing data in order to train a model to detect

a given object would span several days.

Current annotation tools are based on the same core concept. The annotator ob-

serves a digital image on the screen, deﬁnes a special region around a certain object

and gives it a textual description. Every tool may implement this in a slightly dif-

ferent way, or may add shortcuts that make the process slightly faster. However, in

all of them, the user has to label this object in every image it is present in. In this

report, we present a novel labeling tool – 3D Labeling Tool (3DLT) – that tackles

this problem in a different manner. We use the depth information of a sequence of

RGB images to compute the camera trajectory and a sparse 3D mesh reconstruc-

tion. Then, labeling a certain object in all of the images it is present in is equivalent

to labeling this individual object on the mesh. This is possible because the object

present in this mesh is reconstructed from the RGB images we wish to label.

1

Our tool is a standalone, feature-heavy and cross platform software that does not

require installation and can run on Windows, macOS and Linux-based distribu-

tions. This is possible because it is developed using Unity; a real-time game engine

that supports multiple platforms and contains all the necessary camera features that

complement the development of this project, e.g emulating real world cameras’ be-

havior. To handle complex computations such as in Chapters 5, 6, 7 and 9, we

created a Python server. Communication between the Unity side and the Python

side is done through Websockets, see Section 6.7.

The data that is going to be labeled using our tool is collected by an RGBD camera

that generates corresponding RGB and depth images of the scanned environment.

Using this information and Open3D’s reconstruction library, a 3D mesh is generated

and encoded in PLY format along with a point cloud of each shot (depending on the

user’s needs). The RGB-D images, the mesh, the point clouds and the camera’s

information are all stored in a custom format called IRIS format that can be later

on imported to the 3D labeling tool. Capturing depth information is now accessible

for all users especially with the presence of commodity sensors such as Intel’s Re-

alSense camera. More on that in Chapter 3.

3DLT holds a user interface that decomposes the workﬂow of labeling into two

parts: importing the data set and then labeling it. This user interface is intive and

aesthetic, also it isolates the complex back-end functionalities of the tool from the

end user. The development of this user interface is examined in Chapter 4.

Once the labeling object is placed on the mesh, we need to calculate the equivalent

axis-aligned bounding rectangle on the 2D images. First, the labeling object has

to be projected on the images; this is done by using single view geometry and the

camera’s projection matrix. A minimum bounding rectangle can then be calculated

by getting the minima and maxima of the pixels on both the Y and Y axes. In some

special cases, the object can be hidden by another object in one image shot but

clearly visible in another. Since we do not want this blocking object to be present in

the minimum bounding rectangle we make use of a fast outlier detection method to

remove labeling object pixels that are very far from the others. The bounding box

calculation and subsequent improvements are depicted in Chapter 5.

2

Even though it takes one labeling action to label an object in all of its images, the

labeling action in 3D space is not as simple as drawing a rectangle on a 2D image

plane. A labeling object has to be instantiated and then translated, rotated and scaled

by the user so that it can label an object on the mesh. To make this process triv-

ial, we decided to use nonrigid registration to calculate the perfect transformation

parameters (translation, rotation and scaling) that place the labeling object exactly

in the position we want on the mesh. All the user has to do is deﬁne a rectangular

cuboid on the mesh by selecting 4 points (2 for each dimension), we then register

4 points on a labeling object to the 4 points chosen by the user. In doing this, we

shorten the time taken to label a certain object on a mesh to only a few seconds. We

discuss this approach in detail in Chapter 6.

For some special cases where the reconstructed mesh is unavailable or very sparse,

we have developed an alternative way of labeling the object using the 2D images

while retaining the speed and accuracy described in the previous paragraph, given

that we know the exact size of the object we wish to label. This is done by reduc-

ing the problem to a system of non-linear equations whose solution would reveal

the exact position where the labeling object should be placed. The solution to this

problem is discussed in Chapter 7.

Due to the basis of the tool being labeling in 3D, it is only natural to assume that it

should support the possibility to label a provided 3D point cloud. This can be useful

for object detection applications based on the revolutionary point cloud based neu-

ral network PointNet or for normal object detection models by extracting images

from these point clouds. Obviously, capturing images from a surface-less point

cloud would be useless as the machine learning models need to know what the

object looks like in the real world. Therefore, we use surface reconstruction to ex-

tract a high quality triangular mesh from the point cloud and use Unity to navigate

around the mesh and extract RGBD images to complete the tool’s required input.

A comprehensive comparison of surface reconstruction techniques, a designed sur-

face reconstruction workﬂow and a description of the tool used to take the RGBD

images are detailed in Chapter 8.

3

In some rare cases1, placing the 4 points discussed earlier may be non trivial.

Henceforth, we introduced a new feature that snaps the labeling element in its con-

venient place if it is more or less close to it and it has the same size. We do this by

registering the points of the labeling object viewed by the camera to the points of

the mesh viewed by the same camera. For this task, we selected chose an algorithm

that is robust against outliers, noise and missing data. Especially for correspon-

dence calculation. The feature and its implementation are discussed in Chapter 9.

Finally, we added the ability to label in virtual reality using Unity’s SteamVR plu-

gin and in collaboration with Logitech’s VR development team. This idea adds an

immersive experience and exploits the 3D aspect of the tool to is farthest limits. It

will be explained thoroughly in Chapter 10.

The rest of the report is described as follows. In Chapter 11, we perform a series of

tests to compare our proposed tool to current state of the art competitors. Finally,

we sum up our work and propose interesting future improvements in Chapter 12.

1The object has very odd shape, missing depth in mesh, the object if very small etc.

4

Chapter 2

Related Work

There are two approaches in computer vision literature that associate textual infor-

mation with images: annotation and categorization. Annotation is associated with

keywords or detailed text descriptions in images, while categorization assigns each

image to one of a set of preset categories [1]. This can be more common in two cate-

gories, for example indoor / outdoor or city / landscape, to more speciﬁc categories,

such as villages in Africa, dinosaurs, mode ships and ﬁght ships [1]. Categorization

can be used to guide further image processing as an initial step in image learning.

For instance, as a pre-processing step, a categorization into textured / non-textured

classes is done in [2]. Recognition is about identifying speciﬁc instances of ob-

jects. Annotation-based Object recognition would distinguish between images of

two structurally distinct cups [3] and category-level object recognition would put

them in the same class. This report focuses solely on image annotation.

2.1 Manual Annotation

The best method to create ground truth for algorithm evaluation is to ﬁrst create the

required keyword vocabulary and then manually annotate the images using these

keywords. This is done by placing a polygon 1 around the desired object and giving

it a descriptive keyword or phrase. This textual description is usually from a dataset

of words which the annotator can choose from. The manual annotation of images

1usually a rectangle

5

is a very time-consuming and labour-intensive task. In consequence, most compre-

hensively annotated datasets contain few images, while more "lightly" annotated

datasets are large. One example of the former is the Sowerby database [4] contain-

ing 250 images with manually corrected segmentations and a keyword assigned to

each segmentation region. The pictures are all rural or urban outdoor scenes, and

this dataset is limited to an 85 word vocabulary. A larger set of manually labeled

segmented images was presented in [5]: the regions were labeled on 1014 manually

segmented images.

2.1.1 LabelImg

LabelImg [6] is a simple ofﬂine annotation tool that allows the user to import mul-

tiple images and to draw bounding rectangles around the desired object.

It also

lets him describe the drawn box by a textual keyword. Finally, it exports all of the

bounding boxes in an XML ﬁle.

2.1.2 Labelbox

Similarly to LabelImg, Labelbox [7] is a very simple annotation tool that gives users

the capability of drawing bounding rectangles with textual description around the

object. The only difference is that Labelbox is an online tool that can be accessed

by any device with internet browser support. However, one downside to Labelbox

is that the user relies on the the tool’s server which may slow down labeling time in

case of excessive user activity.

2.1.3 Anno-Mage

Anno-Mage [8] offers the same functionality as LabelImg, but it also possesses

RetinaNet as a suggesting algorithm.

It suggests 80 class objects from the MS

COCO dataset [9]. This allows the user to receive suggested bounding rectangles

if any object pertaining to one of these 80 classes is present in the images. In these

situations, Anno-Mage speeds up the labeling process drastically.

6

2.2

Intensive Group Annotation

Different approaches and systems have been set up to simplify image annotation by

receiving input from a large number of people. The simplest way is to get a group of

people together to create the annotations— the PASCAL VOC challenge [10] orga-

nizes an annual "annotation party" in which a group intensively annotates over 3–4

days. In [10], it is proven that this is more effective than distributed asynchronous

annotation.

2.3 Collaborative Annotation Over the Internet

LabelMe [11] is an online annotation tool designed to collect keywords that describe

image regions for evaluation of object recognition. The user deﬁnes a polygon

around an object and then enters a keyword that describes the object. Misspelled

keywords often occur because the vocabulary is not controlled. This issue is solved

through a veriﬁcation step by the database administrators.

7

Chapter 3

Data Set Importer

3.1

Introduction

To start the labeling process, a data set should be imported in a custom format that

contains a collection of RGB and depth images, a mesh, the point clouds and ﬁnally

the camera’s intrinsic and extrinsic for each shot (More on that in Chapter 5).

In this chapter, we will examine this custom format and how the data is being col-

lected and processed. Then, we will shed the light on the PLY ﬁle extension in

which the mesh and the point clouds are encoded with. We end by illustrating the

development of a PLY run-time importer in Unity.

3.2 Data Collection

Before importing the data in the 3D labeling tool, it is essential that it is collected,

processed and encoded into a speciﬁc format to be able to receive accurate results

when labeling.

3.2.1 Camera Speciﬁcations

The data is collected using an Intel RealSense D435 depth camera also known as

RGBD camera. It has a frame rate of up to 90 fps, a depth resolution of 1280 × 720

and an RGB resolution of 1920 × 1080. To set up and run the camera, we used

8

Librealsense which is Intel’s RealSense python library [12]. In addition, Matlab’s

Camera Calibrator app was used to extract the camera’s intrinsic parameters that are

saved for later use.

3.2.2 Recording and Processing

When the camera starts to record, the depth and color stream are both enabled at a

resolution of 1280 × 720 and a frame rate of 30fps. Note that it is important to set

both the depth and color on the same frame rate so that both images match. Each

frame is saved separately as a depth and RGB image and a timestamp is created for

each image (depth and RGB) along with its name.

Moreover, after the data is collected, blurry frames are eliminated and new depth

and RGB text ﬁles (timestamps with names of the images) are generated without

the blurry data.

3.2.3 Camera Trajectory

To compute the camera’s trajectory, ORB-SLAM 2 [13] was used. It is a real-time

SLAM library for RGB-D cameras and requires the RGB and depth images along

with an association ﬁle that contains the RGB and depth text ﬁles. ORB-SLAM

iterates over all the images in the sequence and generates the camera trajectory

ﬁle that contains the camera’s transformation (position and rotation) along with the

frame’s timestamp. After the trajectory is computed, the system iterates over every

camera’s position and calculates the difference between frames; if the frames are

too close to each other, the second one is eliminated. This process is repeated for

every frame and ﬁnally a camera trajectory ﬁle is generated.

3.2.4 Mesh Construction

To build the mesh, the system uses Open3D reconstruction method [14] that uses

the camera’s intrinsic parameters and the depth scale. This function iterates over all

the RGB and depth images, creates RGBD images and integrates volume to them.

As a result, the mesh is extracted from the volume.

9

Please refer to Figure 3.1 for a diagram presentation of the data collection and pro-

cessing workﬂow.

Figure 3.1: Data Collection Workﬂow Diagram

3.2.5 IRIS Format

After collecting the needed data, we convert it into a custom format called IRIS that

is acceptable by the 3D labeling tool. In this format, the RGB and depth images

are renamed and numbered from 0 onward. At this stage we should have obtained

two folders, one contains the RGB images and the second one contains the depth

images. In addition, this format consists of a “timestamps.json" ﬁle that provides

the correspondence between the old timestamps and the new image names. It also

contains an “intrinsics.json" ﬁle that holds information about the calibration of the

camera. Also, this format contains a ﬁle called “extrinsics.json" that is basically the

camera trajectory ﬁle produced by ORB-SLAM.

Finally, the constructed mesh is encoded in a PLY format and saved in a folder

called “registration" and the point clouds are saved also in the same format in a

folder named “pc". Figure 3.2 shows an example of a data set in IRIS format.

10

Figure 3.2: IRIS Format Example

3.3 PLY File Importer

To import the RGB and depth images to Unity, we need a byte reader in C# to parse

the PNG images. However, for the mesh and point clouds, we had to develop a

custom parser for the PLY ﬁle format, which will be explained thoroughly in this

section.

3.3.1 Polygon File Format

The polygon ﬁle format(ply), also known as the Stanford Triangle Format, is used

for storing a graphical collection of polygons, more speciﬁcally, a collection of

polygons. The ﬁle extension has two representations: an ASCII representation and

a binary version used for fast loading and saving. The PLY format deﬁnes the

object as a group of vertices, faces and other elements (surface normals, texture

coordinates, colors, etc.). However, it does not include transformation matrices,

object subpart or modeling hierarchies. Therefore, this format is not a general scene

description language.

Moreover, this ﬁle extension is typically intended to represent the object by a list

of “X, Y, Z” triples for the vertices and a list of indices. The latter indicates the

sequence of the vertices that describes the face of a polygon. The vertices and faces

are an example of “elements” in a PLY ﬁle, each element has a ﬁxed number of

“properties”, e.g. the “vertices” element can be associated with the properties “red,

green and blue” that indicate the color of each vertex. In addition, one can create a

new element type and deﬁne its corresponding properties, elements like “material,

11

edges, etc.”. Note that added elements could be discarded by programs that don’t

understand them. The PLY format is divided into two complementary sections: the

header and the body. The start and end of the header are indicated respectively by

the keywords: “ply” and “end_header”. The header is a description that deﬁnes

the remainder of the ﬁle. First, it states whether the ﬁle is binary or ASCII, then

it deﬁnes the elements of that object by indicating the element’s name, the number

of such element in the object and a list of properties associated with each element.

These properties are deﬁned by the keyword “property”, followed by the data type

of the property, this list also deﬁnes the order in which the property appears for

each element. Then, the body contains a list of elements for each element type,

presented in the order described in the header. Figure 3.3 represents a basic example

Figure 3.3: PLY File Example

of a cube written in PLY format. This example illustrates a basic structure of the

header, describing how the object is only a collection of vertices and faces (element

vertex and element face), and how each element has its own properties. Moreover,

this example shows the two data types a property may have: scalar and list. The

scalar is the data type of the vertex’s properties (ﬂoat, int, uchar, etc.). However,

the property “vertex_index” of the element “face” contains an unsigned char that

indicates the number of indices the property contains, followed by a list containing

that many integers. Each integer is an index to a vertex.

12

3.3.2 PLY Parser in Unity

In the process of developing the 3D labeling tool, a parser was needed to be able to

import any PLY ﬁle at run-time. The idea was to give the user the ability to import

any 3D object represented by a PLY ﬁle in a few clicks. This 3D object could be

the global mesh or a point cloud that is generated from the RGBD images taken by

the camera. First, the parser starts with the header of the PLY ﬁle and checks the

number of the vertices and/or the faces. Then, for each element, the parser takes

into consideration the data type of each property and stores it in a custom class

that contains only the properties in the same order as given in the header of the

ﬁle. This method allows the binary reader to parse the body of the PLY ﬁle in the

same order of the header by considering the data type of each property. However,

the import program must be designed to take into consideration meshes and point

clouds. In fact, the only difference between a mesh and a point cloud is that point

clouds don’t have faces, they are only constructed using vertices. So, to take this

note into consideration, the parser checks in the header of the PLY ﬁle, if the latter

contains only vertices or not, to be able to call the corresponding function to render

the 3D object as a mesh or as a point cloud in Unity.

Import the 3D object as a Mesh

After collecting the vertices and the faces of the 3D object, the mesh could be con-

structed by using Unity’s built-in mesh class that contains the functions that set the

vertices of the mesh and the same for the faces. The triangles/faces are enough to

deﬁne the basic shape of the object, but extra information that describe the detail

of the object need to be added. In the application, there’s always a global light in

the scene that hits all the objects including the imported mesh. Therefore, to allow

the object to be shaded correctly for lighting, a normal vector should be deﬁned for

each vertex. During the shading calculations, each vertex normal is compared with

the direction of the light: if both vectors are aligned then this point is receiving full

light and it’s being shaded with full brightness. Typically, the light will arrive at an

angle to the normal and the shading will be somewhere between full brightness and

13

complete darkness, depending on the angle. However, not all imported 3D objects

contain information about the normal of each vertex and a recalculation of the ver-

tices’ normals is done using the Mesh class function: Mesh.RecalculateNormals(),

after constructing the mesh. Finally, the 3D object is imported, and the mesh is

rendered in the application as shown in Figure 3.4.

Figure 3.4: Imported mesh in the tool

Import the 3D object as a Point Cloud

The input data set may contain a collection of point clouds generated from the RGB

images using Open3D libraries as a pre-processing algorithm before handing the

data as an input for the tool; therefore, each image has its own point cloud. Adding

this type of object representation could enhance the labeling action for the user,

especially if the data set does not contain a 3D mesh. In this case, the user takes

advantage of the presence of the point clouds to ensure an accurate labeling of the

object in all angles as shown in Figure 3.5. Point clouds are a collection of indepen-

dent spatial points, which are used to represent a 3D object. The point clouds that

are being imported into the tool are stored in PLY format. However, Unity does not

provide a built-in renderer for this type of objects. Thus, a method for this prob-

lem has been given in [15] that implements two classes to ensure the rendering of

the point clouds into the application. The class “PointCloudData” implements the

constructor to store the data in an object which uses a “ComputeBuffer” to store the

points. This latter is a class in Unity that supports GPU data buffer. As discussed in

14

Figure 3.5: Point Cloud example with labeling object

section 3.3.2, the same goes for the point clouds; the buffer is fed with an array of

points, where each point contains two attributes: position and color. After deﬁning

the point cloud data, the author implements another class responsible for rendering

the point cloud already stored in the point cloud data object. The class “PointClou-

dRenderer” is added as a component for the imported point cloud. The main idea

behind this class is to create a material from a custom shader that is responsible for

the mathematical calculations that computes the color of each pixel rendered based

on the material conﬁguration and the lighting input. Before drawing the points,

the material sets the input for the shader calculation (points transform matrix, GPU

instancing, colors and the point buffer). Finally, Unity’s “Graphics” class sends a

draw call to the GPU, setting the attribute “MeshTopology” to “Points” because, the

user is importing point clouds and not meshes with triangles in this case. Further-

more, the RGB images and the point clouds are coupled using their identiﬁcations.

15

Chapter 4

User Interface

4.1

Introduction

An application’s user interface ensures that the services of the tool are offered to

the user in a clear way without ambiguity. In this chapter, we examine the devel-

oped workﬂow of the 3D labeling tool. Then we detail how we enhanced the user

interface throughout the project to facilitate the labeling experience along with the

usability of this application.

4.2 Workﬂow of the 3D Labeling Tool

When the application is launched, an import window pops up and is responsible

for loading the IRIS data set that we mentioned in the previous chapter. This tool

proposes as well a main labeling step that holds all the main functionalities that will

be discussed thoroughly in what follows.

4.2.1

Import Window

This window displays the ﬁrst workﬂow phase of the tool where the user uploads

the data set using a ﬁle browser, decides whether to include only the RGB data or

adds the mesh and/or the point cloud data of the imported images. Moreover, the

user has the capability to ﬁlter some of the images. This was done by including a

16

slider to the import window such that its value is used as follows: if the key of the

dictionary that contains the camera positions is a multiple of the slider value then

the image is loaded. We The idea behind this ﬁlter is to decrease the load on the

tool in case the user doesn’t want to label the whole data set.

Finally, two preview panels were added where the user can check if the imported

data is the right one or not, hence giving him the opportunity to re-import another

set before starting to label (see Figure 4.1).

Figure 4.1: Import Window Example

4.2.2 Main Labeling Window

The main functionalities of the tool lie in this window. The latter allows the user to

add a labeling object (Primitive Cube or a CAD model). After that, the user care-

fully positions that object on top of the imported mesh while checking the images

on the right side of the screen (see Figure 4.2) to verify that the object is in the right

place.

Navigation

To ensure that the user can navigate with freedom in the scene and especially around

the mesh, we added a camera control script and attached it to the main camera that

17

Figure 4.2: Main Labeling Window Example

renders the imported mesh. This script makes the arrow keys and the mouse input

available for the user to navigate and orbit around the mesh. The user can also

control the speed of the navigation.

The Labeling Object’s Transformation

One of the essential functionalities in this phase is the gizmo1 tool that is respon-

sible for changing the transformation of the labeling element at run-time. This is

done with the help of this repository [16] that implements a run-time gizmo tool so

when the user clicks on the labeling element they will have the option to change its

position, rotation(see Figure 4.3). Note that the gizmo is always placed according

to the center of geometry of the labeling element.

Color Picker

Changing the color of the labeling element is a key feature in this tool because it

will affect the labeling mechanism that is discussed in the following chapter. To

implement a color picker in the tool at run-time, we used the following open-source

repository [17] in which the user clicks on the object and pops out a color picker

1Gizmos in Unity are used to give visual aids to move, rotate and scale the object.

18

Figure 4.3: Run-Time Gizmo Tool

panel (see Figure 4.4) to change the color and transparency of the object.

Figure 4.4: Color Picker

Extra Features

As shown in Figure 4.2, the main labeling window features many buttons that trig-

ger the functionalities implemented in the tool:

First, the user has the ability to hide/show the mesh, the point clouds and the label-

ing elements. Second, the trajectory of the camera can be also shown in the main

window by clicking on the “Trajectory" button in case the user is curious about how

the images where taken (More on that feature in the next chapter). Also, if the user

needs to block the mesh2 when labeling to ensure a better placement of the labeling

2Enabling the camera to project the labeling element before rendering the mesh. More on that

in the next chapter

19

element they can click on the “Block Mesh" button. Finally, by clicking “Phantom

Mode", the user can execute a key feature that consists in transforming the labeling

element onto its position on the mesh using a registration algorithm and that is thor-

oughly explained in Chapter 6.

Moreover, when adding the labeling element, the user can enter the Edit Model

mode and alter the size of the object along with its color and save it for later use,

in case this element is redundant in the environment where they are labeling. These

elements are then showed on the top left corner of the screen to be later on instanti-

ated.

In case of a large mesh to label, the user has the option to save the session and load

it back again to resume the labeling process later. In addition, the labeling action is

triggered when the user hits the “Current" button in the “Labeling Elements" panel

(Figure 4.2) where the labeling is triggered.

Finally, after the user ﬁnishes the labeling, they can export the annotations by click-

ing on Next button and exporting the ﬁles to the data set location as explained in

details in the next chapter.

4.3 User Interface Enhancements

The version shown in Section 4.2 showcases the main workﬂow of the tool with an

old version of the application that was later on modiﬁed to make the user experience

more intuitive and aesthetic. In this section, we are going to examine the major

changes that were done to the user interface design and functionalities.

4.3.1 Import Window Enhancements

The main idea behind the upgrade to the import window is transitioning to a project-

based approach. For instance, after the import actions, the user can decide whether

to start labeling on an already saved session of this data set or to start a new session.

The main functionalities of Section 4.2.1 were preserved, with the exception of

the data set ﬁlter which is now a number rather than a slider. We also added the

possibility of loading a recent session as shown in Figure 4.5. In addition, a preview

20

of the top view of the mesh is also added to the import window.

Figure 4.5: New Import Window Design

If the user is satisﬁed with the preview of the data set, they can start the labeling

process by clicking the “Start Labeling" button on the bottom right corner of the

screen. Notice that even before the session starts, the path of the data set is presented

in the footer. This can help the user to check if the data path is right and give the

opportunity to re-import a new data set.

4.3.2 Main Window Enhancements

Improving the user interface mainly focused on making things simpler in the main

labeling window by removing the obsolete buttons or that did not have any effect

on the labeling process. In the simpliﬁed interface, the user can chose either one or

two ways to add a labeling element: (1) Add a simple primitive cube or (2) Load a

CAD model from the inventory (Explained in details in Chapter 7).

Dynamic Panels

We added a dynamic panel for the images that lets the user move and minimize it.

We also added two extra panels that show the mesh’s top and side view. This allows

the user to keep track of every side of the mesh while labeling, ensuring the object’s

position is correct from all angles. In addition, the user can navigate and zoom on

the top and side view panels (see Figure 4.6).

21

Figure 4.6: Mesh Top View Example

Labeling Elements

The labeling element’s information is shown in the “Elements Details" on the top

left part of the screen (see Figure 4.7). This panel showcases the transform3 of the

labeling element along with detailed information on its color. This gives the user the

ability to make ﬁne adjustments to the object’s color, position, rotation and scale.

Furthermore, in case the user wants to identify a certain labeling element’s position

on the mesh, they can simply double click on its name in the “Elements" panel on

the left (see Figure 4.8) and the camera will directly change its position to point at

that object and navigate the user to it.

New Features

In this version of the tool, the user can now copy and paste the labeling elements to

ease the labeling in case of redundant objects on the mesh. Also, the “Save Session"

button will write the positions of the placed labeling elements along with their color

in a JSON ﬁle. This ﬁle is saved in the data set’s folder.

Moreover, the camera shot can also be identiﬁed in the scene by clicking the search

3Information about the position, rotation and scale in Unity

22

Figure 4.7: Element Details

Figure 4.8: New Main Labeling Window Example

button on the bottom left corner of the tool.

The slider in the footer helps the user to navigate between the imported images.

Also, the “VR" button launches the labeling into virtual reality. This is explained

and examined in details in Chapter 10.

Finally, we managed to separate the user interface from the main and core func-

tionalities by using C#’s delegates functions that manage events. In other words, a

button calls an event that enables a functionality which is completely isolated from

the user interface. As a result, this reinforces the scalability of our application since

it allows a seamless addition of functionalities.

23

Chapter 5

Bounding Box

5.1 Minimum Bounding Box

Before we deﬁne the type of box used to annotate 2D images that train object de-

tection models, we will deﬁne its more general counterpart; the minimum bounding

box. In geometry, the box with the smallest measure (area, volume, or hypervol-

ume in higher dimensions) within which all the points are located is the minimum

bounding or enclosing box for a point set (S) in N dimensions.

A point set’s minimum bounding box is the same as its convex hull’s minimum

bounding box 1, a fact that can be heuristically used to accelerate computation. The

name "box"/"hyperrectangle" derives from its use in the Cartesian coordinate sys-

tem, where it can be actually visualized as a rectangle (two-dimensional case), hor-

izontal parallelepiped (three-dimensional case), etc. The arbitrarily oriented mini-

mum bounding box is the minimum bounding box, calculated without any limita-

tions regarding the result orientation.

5.2 Axis-Aligned Minimum Bounding Box

For a given point set, the axis-aligned minimum bounding box (AABB) is its min-

imum bounding box subject to the constraint that the box edges are parallel to the

1The convex hullof a set X of points in the Euclidean plane or in a Euclidean space (or, more

generally, in an afﬁne space over the reals) is the smallest convex set that contains X.

24

Cartesian coordinate axes. Axis-aligned minimal bounding boxes are used as an

easy descriptor of the form of an object at an estimated place. For example, when it

is necessary to ﬁnd intersections in the set of objects in computational geometry and

its applications, the initial check is the intersections between their MBBs. Because

it is generally much less costly than checking the actual intersection (since it in-

volves only coordinate comparisons), it rapidly enables the exclusion of distant pair

checks. As a result, they are used in object detection neural networks to identify a

speciﬁc object. They are also paired with a small text descriptor that identiﬁes what

the annotated object is.

5.3 Calculating the Minimum Bounding Rectangle

In this section, we will describe how the labeling object is being projected onto the

images and how the minimum bounding rectangle is being calculated. It is done on

the Python side to allow for complex mathematical calculations.

5.3.1 Projection of Labeling Object onto the Image

The data set that is being loaded in Unity contains two ﬁles that enclose details

about the camera that captured the images. These ﬁles are the camera’s extrinsics

and intrinsics. However, to be able to mimic the real camera into a virtual one in

Unity, we had to create a game object that describes a shot view which is formed of

a camera and its image frame (children of the shot view game object).

Figure 5.1: Shot View Example

25

The Extrinsic Camera Matrix

The extrinsic matrix [R|t] describes how the world transforms according to the cam-

era. The inverse of the extrinsic matrix describes how the camera transforms ac-

cording to the world origin, in other words it deﬁnes the camera’s position in the

world. In order to construct it, we start with a 3 × 3 rotation matrix on the left. A

3 × 1 translation column vector on the right, where t = −RC such that C is a col-

umn vector describing the camera’s location in world coordinates. An extra row

of (0,0,0,1) is also added to make it a square matrix making the calculation of the

inverse easy. The extrinsic matrix looks as follows:

[R|t] =











r1,1

r2,1

r3,1

r1,2

r2,2

r3,2

r1,3

r2,3

r3,3

0

0

0











t1

t2

t3

1

(5.1)

We use the inverse of the camera’s extrinsics to position the shot views.

The Intrinsic Camera Matrix

The intrinsic camera matrix K, is a 3 × 3 upper triangular matrix that transforms the

3D camera coordinates to 2D image coordinates:

K =








fx

0

0








s

x0

fy y0

0

1

(5.2)

fx and fy represent the components of the focal length of the camera, s is the axis

skew which is responsible for the shear distortion of the image and ﬁnally x0 and
y0 are the principal point offset2, see Figure 5.2. We use the camera’s intrinsics to

calibrate the placement of the image frame and obtain an accurate projection that

corresponds to the real camera’s image frame, see Figure 5.1.

2The principal point offset is the location of the point relative to the frame’s origin, which in our

case, on the bottom-left of the image

26

Figure 5.2: Principal Point Offset

Projection

The projection matrix is a 3 × 4 matrix which describes the mapping of a pinhole

camera3 from 3D points in world coordinates to 2D points in image coordinates.

We multiply the intrinsic matrix and the extrinsic matrix to obtain the projection

matrix P:

P = K[R|t]

(5.3)

Let w be a 4 × 1 column vector containing world coordinates of a certain point

[x y z 1] and let W be a 3 × 1 column vector containing the coordinates of the same

point in image coordinates [u v 1], the equation transforming the former into the

latter is:

W = Pw

(5.4)

Finally, P is assigned to the virtual camera’s projection matrix property in Unity so

that this camera can render according to the projection matrix. As a result the 3D

labeling elements appear in the corresponding position on the images. Note that,

the image frame position is also calculated using the projection matrix: by deﬁning

the position on the x and y axis using the camera’s offset and focal length on the

corresponding axis.

3A simple camera represented as a light-proof box with a small hole on one side. The light of

the scene passes through the hole and projects an inverted image on the opposite side of the box.

27

(a) Non-Blocking Mesh

(b) Blocking Mesh

Figure 5.3: Blocking Mesh Example

Rendering Selectively

To allow the camera to render selectively the image frame object and the 3D la-

beling object (3D bounding box) on the mesh, we deﬁned a culling mask4 for the

camera and assigned to it the culling layers of both the image frame and the la-

beling element. As a result, the camera shows only both of these layers without

including another 3D object. In addition, the user is given the option to block the

mesh, this allows the labeling elements that are behind the mesh to be seen. This

is done using Unity’s shader programs that are tailored to calculate post processing

effects and also used to manipulate the camera’s rendering information. Thus, to

be able to block the mesh, we had to assign a blocking shader to the mesh. Brieﬂy,

we assign the shader a rendering queue number that is responsible for ordering how

the camera should render objects. In our case we assigned a lower value to the ren-

dering queue of the mesh comparing to the one of the labeling objects. As a result,

when the user clicks on "Block" the mesh shader is switched to "Blocking" and the

labeling objects are rendered before the mesh (see Figure 5.3).

5.3.2 Minimum Bounding Rectangle

In the previous section, we described how the labeling object is rendered on the

images. However as we discussed earlier, the input to any object detection machine

learning model is an axis-aligned bounding rectangle. Therefore, we need to extract

this box from the rendering we got from the back-propagation before we feed it to

the machine learning model.

4In Unity, the culling mask is a camera property used to include or omit layers of objects to be

rendered by the Camera after assigning the corresponding layer to the object.

28

Getting the Image Ready

The images are sent through websockets by preparing a message as an object that

encapsulates an array of bytes, a dictionary that holds the color of each labeling

object and the shot ID. A "render texture" is created and assigned to the shot view

camera as a target texture where everything that is rendered by this camera is pro-

jected onto this texture. It is not necessary to send all of the images to the server.

To calculate the bounding rectangle, the server only needs to know the position of

the labeling object in the image. Henceforth, we should only send the labeling ob-

ject(s) by coloring the remaining image’s pixels in black. For this case, we modify

the camera’s culling mask to only render the labeling element(s) and we changed

the clear ﬂag of the shot view camera to be able to clear with a solid background

black color (see Figure 5.4).

Since we are designing a real-time application and our whole goal is to make label-

ing faster, we also decided to down-sample the image to a smaller resolution. After

thorough testing, we found that 320 × 180 is the best compromise between speed

and accuracy. The texture is then encoded into an array of bytes.

Moreover, the labeling object colors are sent so the server can differentiate between

them to calculate the corresponding bounding box.

Finally, the message is serialized into a JSON object and sent to the Python server.

Figure 5.4: Sent Image of the Bounding Box

Decoding and Reading Image

Once the image is received on the Python side, we use the Pillow Library to decode

and read the image. We also access the additional color information to identify the

29

labeling object(s) on the decoded image. To perform manipulations on the Image,

we transform it to a 320 × 180 × 4 Numpy matrix in which each entry represents the

color(RGBA) of a pixel at a given position.

As we previously mentioned, the 320 × 180 × 4 matrix will contain the color black

and a different color for each additional labeling object. The next step is to identify

the coordinates of each labeling object on the image. To do this, for each color of a

labeling object we loop through the matrix and each time we encounter a pixel that

has this color we save its coordinates(row and column in the matrix).

Once we have the coordinates each labeling object. We can now calculate the min-

imum bounding rectangle.

Simple Method

To deﬁne a rectangle, we only need 3 points. Given the indices of the labeling

object, we simply need to ﬁnd three of the following four values:(1) The pixel with

the smallest x value, (2) The pixel with the smallest y value, (3) The pixel with the

largest x value and (4) the pixel with the largest y value.

Once this is achieved, all we need to do is solve the trivial geometry problem of

calculating the fourth variable.

Small Bounding Rectangle

In certain cases, when the labeling object is back-propagated on to the images.

On certain images it might be very small because the object is exiting the view

or it is hidden by another object. Calculating a bounding rectangle around this

labeling object would lead to a rectangle with a very small area and would confuse

the machine learning model. Therefore, it is better if we don’t return any rectangles

to Unity in these cases. This problem can be solved in two ways: (1) Make sure

that the rectangle’s area is larger than a certain threshold or (2) make sure that its

length and width are each larger than a certain threshold. We decided to implement

both options, after performing the steps described in section 5.3.2 we only send the

30

result if all of these conditions are met:

rectanglearea
imagearea

∗ 100 > T hreshold

rectangleheight
imageheight

rectanglewidth
imagewidth

∗ 100 >

√

T hreshold

∗ 100 >

√

T hreshold

(5.5)

(5.6)

(5.7)

Threshold: Machine learning models usually struggle to train on objects with

sizes smaller than 25 × 25 pixels. Thereupon we get the threshold by the following

equation:

T hreshold =

25 ∗ 25
Resolution

=

625
320 ∗ 180

=

625
57600

(cid:39) 0.0108 (cid:39) 1.1%

(5.8)

5.3.3 Outlier Detection

In some cases, a second object may be blocking the labeled object in some image

but not blocking it at all in another. For example in Figure 5.5, in both images the

two objects are ﬁxed in the same position, only the camera moves. The blue object

is the annotated one and the red object is the blocking one. The left image shows

an angle where the labeled object is not blocked while the image on the right shows

another angle where it is. In these cases the simple method that we described earlier

would calculate the light blue axis-aligned bounding rectangle illustrated in Figure

5.6. This result is not the desired one, as including the red object in the bounding

rectangle will confuse the machine learning model. The preferred calculated bound-

ing rectangle is the one shown in Figure 5.7. This way the machine learning model

will only see the actual object. Even though it is not the full object it is still bet-

ter than including a foreign object in the bounding rectangle. Theoretically, this is

equivalent to saying the right most part of the labeled object is very small compared

to the left part and therefore can be considered as an outlier. Therefore, if we use

outlier detection 5 on the labeling object and removing the outliers before calling

5Outlier detection is the method of identifying unusual objects in datasets or occurrences that

vary from the norm.

31

Figure 5.5: Example of an object blocking the annotated object.

Figure 5.6: Bounding box calculated using the simple method.

the simple method described in 5.3.2, then the resulting bounding box would be the

one illustrated in Figure 5.7.

Categorization of Anomaly Detection

Unlike the well-known classiﬁcation system, where training data is used afterwards

to train a classiﬁer and test information measure efﬁciency, various setups are fea-

sible when speaking about anomaly detection. Basically, the setting for anomaly

detection relies on the labels in the dataset and we can differentiate between three

primary kinds:

Supervised Anomaly Detection: Describes the conﬁguration where the informa-

tion includes fully labeled training and test data sets. First a normal classiﬁer can

be trained and subsequently implemented. With the exception that classes are typ-

32

Figure 5.7: Bounding box calculated using the simple method with outlier detection
enabled.

ically heavily unbalanced, this situation is very comparable to traditional pattern

recognition. Therefore, not all classiﬁcation algorithms are perfectly suited for this

assignment. For example decision trees such as in [18], cannot handle unbalanced

data well, while Support Vector Machines (SVM) [19], Artiﬁcial Neural Networks

(ANN) [20] generally perform better. However, because of the assumption that

anomalies are recognized and properly labeled, this conﬁguration is practically not

very important for numerous applications.

Semi-supervised Anomaly Detection: Once again training and test datasets are

used, but training data consists only of ordinary data without any outliers thus re-

quiring no labels. The fundamental concept is that a standard class model is learned

and anomalies can be identiﬁed afterwards by deviating from that model. This con-

cept is also known as the "one-class" classiﬁcation [21]. Well-known algorithms

are one-class SVMs [22] as well as autoencoders [23]. Of course, any density es-

timation technique can generally be used to model the ordinary classes’ likelihood

density function, such as Gaussian Mixture Models [24] or Kernel Density Estima-

tion [25].

Unsupervised Anomaly Detection:

It is the most versatile conﬁguration because

it doesn’t require any labels and it does not differentiate between a test and a training

dataset. An unsupervised algorithm for anomaly detection scores the information

33

based exclusively on the information set’s inherent characteristics. Typically, dis-

tances or densities are used to offer what is normal and what is an outlier to an

estimation. For these reasons, we will explore unsupervised methods more thor-

oughly.

There are two possible outputs for an anomaly detection algorithm. First, a label that

indicates whether an instance is an anomaly or not. Second, a score or conﬁdence

may be a more informative outcome that indicates the degree of abnormality. Due

to available classiﬁcation algorithms, a label is often used for supervised anomaly

detection. On the other hand, scores are more prevalent for semi-supervised and

unsupervised algorithms of anomaly detection. This is primarily owing to practical

purposes, where applications frequently rank anomalies and only the top anomalies

report to the user. We also use scores in this report as output and rank outcomes so

that the ranking can be used to evaluate the performance. Moreover, any score can

be converted to a label assuming a certain threshold.

Type of Anomalies

In practice, the idea of an anomaly is ambiguous. It heavily depends on the applica-

tion it is used upon. The work in [26] proposes a very informative representation of

this ambiguity in Figure 5.8. By looking at this ﬁgure we can clearly discern that x1

Figure 5.8: Example of global anomalies(x1,x2), local anomaly x3 and micro-
cluster c3 [26]

and x3 are outliers. They are called global outliers and can be extracted by looking

34

Formulations
Nearest-
neighbor

Clustering

Statistical
Subspace

Semi-Supervised
Unsupervised GANs

Type
Global
Local
Global
Local
Multi
Multi
Multi
Multi

References
K-NN[27] and Kth-NN[28]
LOF[29], COF[30], LoOP[31], LOCI[32] and aLOCI[32]
CBLOF[32] and uCBLOF[32]
LDCOF[33] and CMGOS[34]
HBOS[35]
rPCA [36]
One-Class SVM [37]
SO-GAAL[38] and MO-GAAL[38]

Table 5.1: Outlier detection methods

at the entire dataset. On the other hand, x3 may seem an inlier by performing the

aforementioned observation. But if we only focus on c2 and x3, we can conclude

that x3 is an outlier since it is the only one that is far away from the cluster. In this

case, x3 is known as a local outlier. Finally c3 presents itself as another odd case,

it is hard to say whether it should be identiﬁed as 3 anomalies or one miniature

cluster. As we can observe, deﬁning what an anomaly consists of is not set in stone.

On the contrary, it entirely depends on the application and requires a very clear un-

derstanding of the data to be analyzed. To this extent, it is more obvious now that

using scores (fuzzy) in favor of labels(binary) is the right way to go.

Anomaly in our problem The purpose of using anomaly detection in our appli-

cation is to minimize as much as possible the possibility of creating training data

that can confuse the object detection model. Since the user is labeling data in 3D,

in certain pictures the object being labeled could be blocked by another object. In

some cases this blocking may not be severe, but in others such as in the right side

of Figure 5.5, it can cause the creation of a bounding box that includes the blocking

object and accordingly confuse the model about the objects form, see Figure 5.6.

Therefore, an anomaly in our case is a point or small cluster in the data set that is

far from the biggest cluster (biggest visible part of the object we are labeling).

Now that we know in which cases we need to detect anomalies, we can make infor-

mative decision on the algorithm we will choose.

35

Outlier detection methods

Over the years, many solutions have been proposed to solve the outlier detection

problem. Table 5.1 shows a list of the most relevant methodes for outlier detection,

but we will brieﬂy go through some of them in this section.

K-NN and Kth-NN:

The global unsupervised anomaly detection algorithm k-

nearest-neighbor is a simple way to detect anomalies and not to be confused with

the classiﬁcation k-nearest neighbor. It focuses on global anomalies, as the name

already suggests, and is unable to identify local anomalies. First, the k-nearest-

neighbors must be discovered for each record in the dataset. Then, using these

neighbors, one of two variants for an anomaly score is calculated: (1) the distance to

the kth-nearest-neighbor [28] or (2) the median distance to all k-nearest-neighbors

[27]. The ﬁrst technique is referred to as kth-NN and the latter K-NN. In practice,

K-NN is yields more favorable results [39, 40].

Naturally, selecting the parameter k is essential for the outcomes. If its value is

selected too low, the record density estimate may not be accurate. On the other hand,

if it is too big, it may be too coarse to estimate the density. As a rule of thumb, k

should be in the range ]10, 50[. An appropriate k can be determined in classiﬁcation,

e.g. by using cross-validation. Unfortunately, in unsupervised anomaly detection,

there is no such method due to missing labels.

LOF:

The local outlier factor [29] is the most well-known local anomaly detec-

tion algorithm and the pioneer of local anomalies. Today, in many nearest-neighbor

algorithms, such as those outlined below, its concept is carried out. To calculate the

LOF score, it is necessary to calculate three measures:

1. For each record x, the k-nearest-neighbors must be found. More than k neighbors

are used in the event of a kth neighbor’s distance tie.

2. The local density for a record is estimated by computing the local reachability

density (LRD) using the computed k-nearest-neighbors Nk.

LRDk(x) =

1
∑a∈Nk(x) dk(x,o)
|Nk(x)|

,

)

(

36

(5.9)

where dk(.) is the Euclidean distance.

3. Finally, the score is calculated by comparing the LRD of a certain instance’s

neighbors and its own LRD:

LOF(x) =

∑a∈Nk(x)

LRDk(o)
LRDk(x)

|Nk(x)|

(5.10)

Therefore, the LOF score is basically a local density ratio. This results in LOF’s

good property to get a score of about 1.0 in ordinary cases, where densities are as

large as their neighbors’ densities. Anomalies that have a small local density will

lead to higher results. It is also evident at this stage why this algorithm is local:

it depends only on its immediate neighborhood hood and the score is a proportion

based primarily only on the neighbors k. Global anomalies can also be identiﬁed, of

course, as they also have a small LRD compared to their neighbours. It is essential

to note that this algorithm will generate false positives when local anomalies are not

of interest. Similarly, for this algorithm, choosing the correct k is essential.

LoOP: As we have already discussed, anomaly scores are a better metric com-

pared to labels. Unfortunately, in LOF it is not yet evident after which score thresh-

old we can obviously believe of an istance as an outlier.

Local outlier probability (LoOP) [31] attempts to tackle this problem by outsetting

a probability of anomaly rather than a score, which could also lead to a better com-

parison of anomalous data between distinct datasets. One thing to consider however

is that this probability is relative to the outlier with the highest anomaly score, i.e.

if record x has the highest score out of all records, LoOP will assign a probability of

100% to it. This could lead to a problem if for instance after assigning the probabil-

ities, we add to the dataset another record that is more anomalous than the highest

one set. As we can see in this easy case, the probabilities are still relative to the

records and may not be too different from a regular score.

LOCI: Choosing k is a key choice for detection efﬁciency for all the above algo-

rithms. There is no way, as already stated, to estimate a good k based on the given

information. Nevertheless, using a maximization strategy, the Local Correlation In-

37

tegral (LOCI) [32] algorithm addresses this problem. The fundamental concept is

that for each record, all feasible k values are used and the highest score is taken

in the end. LOCI describes the r-neighborhood by using a r radius, which is ex-

tended over time, to accomplish this objective. Like LoOP, local density is also

estimated using a half-Gussian distribution, but instead of distances, the quantity

of records in the neighborhood is used. Local density estimation is also distinct in

LOCI: it compares two neighbourhoods of distinct sizes instead of the local den-

sity ratio. A parameter α regulates the various neighborhoods’ ratio. However, this

modiﬁcation comes at cost since typically, anomaly detection algorithms based on

nearest-neighbor have a O(n2) computational complexity to ﬁnd the closest neigh-

bors. As the radius r also needs to be expanded from one instance to the furthest in

LOCI, the complexity increases to O(n3), making LOCI too slow and impractical

for larger datasets.

An improvement, aLOCI [32] was also proposed by the authors to alleviate the

complexity problem. They use quad trees to make the counting of the two neigh-

borhoods faster by using a constraint on the parameter α. The authors state that

O(NLdg + NL(dg + 2d)) is the computational complexity of their algorithm, con-

sisting of tree formation and outlier detection, where d is the amount of dimensions.

As typical of tree approaches, the amount of dimensions can be seen to have a very

adverse effect on runtime. Moreover, it is shown in [26] that aLOCI is highly un-

stable and often produces bad results.

CBLOF: All previous algorithms for anomaly detection are based on estimation

of density using nearest neighbours. Alternatively, the cluster-based local outlier

factor (CBLOF) [41], uses clustering to identify dense areas in the data and then

performs a density estimate for each cluster. In theory, each clustering algorithm can

be used in a ﬁrst step to cluster the data. In practice however, k-means are commonly

used to take advantage of the low computational complexity that is linear compared

to the nearest-neighbor search’s quadratic complexity. CBLOF uses a heuristic after

clustering to classify the clusters resulting in large and small clusters. Finally, an

anomaly score is calculated by multiplying the instances belonging to its cluster by

38

the distance of each instance to its cluster center.

HBOS:

The histogram-based outlier score [35] is a simple algorithm for statis-

tical anomaly detection assuming the characteristics(dimensions) are independent.

The fundamental concept is to create a histogram for each function of the dataset.

Then, for each instance in the data set, the inverse of each bin (each feature) it re-

sides in are multiplied. The concept is very similar to the Naive Bayes algorithm

in classiﬁcation, where all autonomous probabilities of features are multiplied. At

ﬁrst, neglecting the dependencies between characteristics may seem a bit counter-

productive, but comes with a large beneﬁt in terms of computational complexity.

HBOS can process a dataset within a minute, while computations based nearest-

neighbor take more than 23 hours [42].

OCSVM: One-class support vector machines[24] are often used for semi-supervised

detection of anomalies[15]. A one-class SVM is trained on anomaly-free data in

this setting, and later it classiﬁes the test set with either anomalies or normal data.

One-class SVMs intend to separate the origin from the kernel space data instances,

resulting in some type of complex hulls describing the normal data in the feature

space. While one-class SVMs are strongly used as a semi-supervised technique of

detection of anomalies, the use of a soft margin converts it into an unsupervised

algorithm. In the unsupervised anomaly detection scenario, the one-class SVM is

trained using the dataset and then each instance in the dataset is scored by a normal-

ized distance to the determined decision boundary[40]. The parameter regulariza-

tion γ must be set to a value larger than zero so that a soft-margin properly handles

the contained anomalies. In addition, one-class SVMs have been altered to include

additional robust methods to deal explicitly with outliers during training. For exam-

ple in [40], an improvement named η-OCSVM is introduced. In this enhancement,

the parameter η is integrated in the training process to estimate the normality of an

instance. Basically, this allows for outliers to have less of an impact on the ﬁnal

decision boundary.

39

rPCA: A frequently used method for identifying subspaces in data sets is the

principal component analysis. It may also serve as an anomaly detection method,

where deviations from usual subspaces is an indication of anomalous data. First, the

data is standardization of the input variables(dimensions) in order for all of them to

contribute equally in the analysis. This is done by applying the following equation:

z =

t − µ
τ

(5.11)

on each variable in the data set where τ is the standard deviation, µ is the mean and

t is the value.

This is followed by the calculation of the covariance matrix between the variables.

Let’s consider the number of variables in the data set to be p. Then the covariance

matrix would be a p × p matrix. In Figure 5.9, an example of a covariance matrix

is shown representing a 3 dimensional data set. The diagonal is just the variances

of each variable separately (Cov(x, x) = Var(x)) and we can clearly see that since

Cov(x, y) = Cov(y, x), this matrix is also symmetric. The entries in the covariance

matrix provide us with one of two conclusions: (1) Correlation, in case of a positive

value, means the two variables increase and decrease together or (2) Inverse Corre-

lation, in the opposite case, means when one variable increases the other decreases

and vice versa.

After that, we can calculate the principal components. There exists as many prin-

Figure 5.9: Covariance matrix for 3-dimensional data

cipal components as there are variables. To compute them we have to compute

the eigenvectors of the covariance matrix. Each eigenvector will also have its own

eigenvalue. If we sort these eigenvectors (principal components) in decreasing order

of eigenvalue, we will end up with the effective rank of the principal components

in terms of variation captured. In other words, the ﬁrst ranked principal component

captures the largest possible variance in the data set.

40

Finally, we have the option to choose which principal components to keep. For ex-

ample, if we have 10 dimensions and decide to keep half of the calculated principal

components then we would have successfully reduced a 10 dimensional problem

into a 5 dimensional one. This of course comes at the cost of accuracy. Moreover,

choosing the highest ranked principal components is not always the best choice.

To recall what an outlier is, it is an instance that deviates from the norm. In other

words, looking at the principal component that shows the most variation would ac-

tually help in detecting global (clustered) outliers and looking at the lower ranked

ones would help detect more local outliers.

Computational Complexity

All nearest-neighbor algorithms require a computational time of O(n2) to calculate

the nearest neighbors. The rest of the calculations are neglected since they don’t ac-

count for a lot of the computation. Similarly in cluster based methods, the clustering

algorithm is quadratic as well. HBOS, is a better candidate compared to clustering

methods since it assumes that features are not related. Thus achieving near linear

time complexity. Concerning One-Class SVMs, it is hard to gage the complexity

since changing the gamma (with quadric complexity) and the number of support

vectors has a signiﬁcant impact on the runtime. Finally, rPCA has a complexity of

O(nd2 + d3) and therefore depends on the number of dimensions. It is hard to make

an informed decision on speed of computation given only theoretical runtimes, it

would be more beneﬁcial to compare these algorithms in a practical use case. This

will be done in the following section.

Benchmark & Comparison

To evaluate the most prominent models, we used two measures: (1) AUC-ROC Per-

formance and (3) Time Complexity. They are executed on 17 benchmark datasets

with each one being split into 60% training and 40% testing. This evaluation is

made possible by the Python library pyod [43] which provides implementations to

many state of the art models.

41

AUC-ROC:

It is not as straightforward as in the classical supervised classiﬁ-

cation situation to compare the anomaly detection performance of unsupervised

anomaly detection algorithms. As opposed to simply comparing an accuracy value

or precision/recall, consideration should be given to the order of the anomalies.

An incorrectly classiﬁed example is deﬁnitely an error in classiﬁcation. This is dis-

tinct in unsupervised anomaly detection. For example, if there are ten anomalies

in a large dataset and they are ranked among the top-15 outliers, this is still a good

result even though it is not perfect. To this end, a popular evaluation strategy for

unsupervised anomaly detection algorithms is to rank outcomes by anomaly score

and then iteratively apply a limit from the ﬁrst to the last rank. This comes in N-

tuple values (true positive rate and false positive rate) which form a single receiver

operator characteristic (ROC). Then, as a detection performance measure, the area

under the curve (AUC) calculated by the integral of the ROC, can be used. A good

interpretation of the AUC-ROC is also provided in [70] where it is proven to be a

good measure in the anomaly detection domain. To clarify, the AUC-ROC is the

probability that, given a random normal instance the algorithm would assign this

instance a lower score than a random anomaly instance. Therefore, even though

AUC-ROC favors ranking between instances over comparing relative differences in

scores, we think it is the most adequate evaluation measure for these unsupervised

models. By looking at Table 5.2, we can see that the accuracies are quite similar.

LOF has the worst results which is most probably due to its focus on locality and

fails in instances where outliers are all global. CBLOF is also close behind and

moreover in our practical tests CBLOF failed on instances where there were no

anomalies in the data.

Time Complexity: Accuracy is not the only measure we decide upon. Since we

are dealing with a real time application, computational complexity is extremely

important. In Table 5.3 we show the time it took to run each model on the same

datasets. The results are not surprising with rPCA and HBOS clearly the best in

terms of speed. As we can see, the algorithm we should select is between the latter

42

Table 5.2: Comparison of AUC between selected models on different data sets

Table 5.3: Comparison of time in seconds between selected models on different
data sets

43

two. Making the ﬁnal decision comes down to testing both in the tool on more

practical cases. We realized that not only did rPCA outperform HBOS in runtime

by a large margin – probably due to the low number of dimensions rendering the

complexity to O(4n + 8) which is practically linear – but also in accuracy because

of the better performance of rPCA against global anomalies.

5.3.4 Implementation

To implement rPCA we use the aformentionned python library pyod [43]. As for

the number of principal points we evaluate on we can choose between 3 options: (1)

use the most prominent one, (2) use the less prominent one or (3) use both. We tried

all three options with different practical examples and found that the ﬁrst option

focused more on global anomalies, hence yielding the results we wanted with even

better runtime6.

The outlier detection step is performed directly after reading the image into a NumPy7

array [44] as described in Section 5.3.2. After we get the prediction from rPCA we

remove all the anomaly points from the numpy array and then we perform the steps

in sections 5.3.2 & 5.3.2.

After all of the steps are done, we have the coordinates of four points in 2D. These

are the 4 points that constitute the rectangle on the image, as previously described

in Section 5.3.2. Of course, each image has its own calculated rectangle since the

object moves in each camera shot. Lastly, this process can be easily scaled in the

presence of multiple labeling objects, as for each image we repeat the same process

for each labeling object. The color array which is sent to us along with the image

contains the colors of each labeling object in that image.

5.3.5 Drawing the bounding rectangle on image

On the Unity side, the client receives the message from the server that holds a dic-

tionary containing the minimum and maximum boundaries of each labeling object

along with the shot ID. However, a class for the 2D objects is created that encapsu-

6reduction to 1 dimension
7NumPy is the fundamental package for scientiﬁc computing with Python.

44

lates the object’s ID, color, 2D bounding box and class name. On each annotation

an object of this class is created and stored in a dictionary.

To draw the received bounding box on the texture of the image frame (see Fig-

ure 5.10), we used the "Shader" calculations in Unity, knowing that shaders calcu-

late how the pixels are colored based on a given input and the material of the object.

The calculations were based on the minimum and maximum boundaries that are

given to the shader, this latter is computing the pixel to be colored by the 3D label-

ing object’s color based on the line width of the 2D bounding box that was set to

a static value; if the distance from the texture’s pixel to the min or max boundaries

is less than or equal to the line width then the pixel is colored with the labeling

element corresponding color.

Nevertheless, if two objects have the same color, the 2D bounding box will encapsu-

late both objects in the images. To avoid this error, the user is given the opportunity

to change the color of the labeling object.

Figure 5.10: Bounding Box Example

45

Figure 5.11: 2D & 3D Annotations Example

5.3.6 Exporting Annotations

After the user is done with the labeling process, they can export the annotations in

the 2D and 3D formats. The 2D annotations hold information about the bounding

boxes that are labeled on the images. This information is established as follows: the

shot ID, the object ID, class name, bounding box min and max boundaries and the

box’s color. All of this information is stored in a dictionary that is ﬁnally serialized

into a JSON Format (see Figure 5.11 on the left).

Alternatively, the 3D annotations describe the labeling object’s center position, ro-

tation and color (see Figure 5.11 on the right). Finally, both of these ﬁles can be

fed to a machine learning model for training purposes along with the images and

the depth as soon as this information is re-parsed into the accepted format by the

machine learning algorithm.

46

Chapter 6

Phantom Mode

6.1

Introduction

Unlike deﬁning a bounding rectangle on a 2D image1, placing a cube in its correct

position in space is non trivial2. We can greatly simplify the process by deﬁning

corresponding points on the labeling object and the reconstructed mesh. Finding the

optimal transformation parameters that transform the chosen points on the labeling

object to the chosen points on the mesh is a known problem in Computer Vision,

the registration problem.

Problem Statement Given a reconstructed mesh M and labeling object L; we

want to give the user the possibility to select three or more points V {va|a = 1, 2, . . . , N}

and X{xi|i = 1, 2, . . . , K} on L and M respectively. We then have to ﬁnd the transfor-

mation parameters (Rotation Matrix R, Translation vector T and Scale vector S) that

transform the selected point set V to the selected point set X. The transformation

parameters will satisfy the following equation:

X = (S (cid:12) R)V + T

(6.1)

1We only need to deﬁne the position and the scale of the rectangle on axes X and Y (4 DoF)
2We need to deﬁne the position, rotation and scale of the cube on the axes X, Y and Z (9 DoF)

47

Where R is a 3 × 3 matrix, T is a 1 × 3 vector, S is a 1 × 3 vector, V is a 3 × N matrix

and (cid:12) is a special vector-matrix multiplication deﬁned as such:

(cid:16)

s1

(cid:17)

(cid:12)

s2

s3








a b c

d e

f

g h i








=








s1 ∗ a s2 ∗ b s3 ∗ c

s1 ∗ d s2 ∗ e s3 ∗ f

s1 ∗ g s2 ∗ h s3 ∗ i








(6.2)

The point sets do not suffer from outliers, noise or missing data. Our three biggest

challenges are: (1) Registration accuracy, (2) Correspondence between the points

and (3) Limiting the registration algorithm to only 9 degrees of freedom (Skew is

not possible in unity)

Registration of two given point sets is a common problem in computer vision. Given

two point sets Xi and Yi, the problems we require to solve are two-fold. First, we

need to ﬁnd correspondence between the points that lie in the aforementioned point

sets. Second, we need to ﬁnd the value of the transformation parameters that map

one point set to the other.

Good registration algorithms satisfy the following requirements: 1) the ability to

handle high dimensionalities of the point sets, 2) the ability to solve the problem

with tractable computational complexity and 3) robustness to practical errors such

as noise, missing points and outliers, all of which can occur due to errors in scan-

ning. However, as we have already mentioned, we do not suffer from the latter.

The transformations can be placed in two different categories: rigid or nonrigid.

Truly Rigid transformations only allow for translation and rotation (6 DoF). How-

ever, conventional methods that solve for similarity transformations which also

include isotropic (uniform) scaling (7 DoF) are also considered rigid registration

methods. Both of these transformations preserve the shape of the point set but the

former also preserves the Euclidian distance, hence its common name: Euclidian

transformation. The simplest nonrigid transformation is the afﬁne transformation.

It allows for anisotropic scaling and skews (12 DoF). In its infancy, the problem

was usually simpliﬁed to piece-wise afﬁne and polynomial models, both of which

are not truly nonrigid and are neither adequate for robust correspondence discovery

nor correct alignment. Because of the large number of degrees of freedom, it is easy

48

to deduce that registration methods that solve for nonrigid transformations tend to

be very sensitive to outliers and noise, they also usually have high computational

complexity. We seek a simpler version of the afﬁne transformation, one that allows

only anisotropic scaling (9 DoF). However, since registration algorithms don’t tar-

get this speciﬁc case, we will ﬁnd the best nonrigid registration algorithm and limit

its degrees of freedom from 12 to 9.

Many algorithms exist for rigid and nonrigid registration. We will provide a brief

overview of the former and then give a comprehensive review of the latter and later

advocate for the one we chose to solve our problem, as well as the modiﬁcations we

performed on it.

6.2 Rigid Registration Methods

The Iterative Closest Point (ICP) algorithm [45, 46] is a staple of rigid body registra-

tion algorithms due to its simplicity and respectable computational complexity. As

its name suggests, ICP is an iterative algorithm: it assigns correspondences based on

the closest distances between the points then ﬁnds the least squares transformation

that maps one to the other. It does this until it reaches a local minima. Many vari-

ants of ICP have been proposed over the years, each variant attempting to improve a

speciﬁc part of the ICP algorithm [47, 48]. ICP requires the point sets to have good

initial positions so that the local minima achieved will be in fact the global minima.

ICP uses binary correspondences. A logical improvement would be to use fuzzy

correspondences, thus the emergence of probabilistic methods that use soft assign-

ment for correspondences. The most successful of these methods is the Robust Point

Matching (RPM) algorithm [49] and some of its variants [50, 51]. In [52], it was

proven that RPM behaves similarly to the Expectation Maximization (EM) algo-

rithm for the Gaussian Mixture Model, where one point set is treated as data points

and the other is treated as GMM centroids with equal isotropic covariances.

In

fact, the point set registration problem can be formulated as a Maximum Likelihood

(ML) estimation problem to ﬁt the centroids to the point sets. This formulation has

been explored in [53, 54] where the centroids are parametrized by translation and

49

rotation. The EM algorithm used to optimize the likelihood function consists of two

steps: E-step to calculate the correspondence probabilities and M-step to calculate

the transformation given these probabilities. Some methods add parameters to con-

trol the EM algorithm for the purpose of making it more robust to noise and outliers.

These methods generally perform better than ICP. The work in [55] uses Genera-

tive Adversarial Neural Networks (GANs) to effectively ﬁnd correspondence even

if the initial positions are not good, it achieves impressive results; however, neural

networks are still hindered by their large running time which prevents this method

from being used in real-time applications.

6.3 Nonrigid Registration

In this section, we will categorize nonrigid registration algorithms in terms of op-

timization methods. Then, we will provide a comprehensible walk-through of our

chosen algorithm. After that, a modiﬁcation which limits the algorithm’s output

results to only 9 degrees of freedom (rotation, translation and scaling) is proposed.

Finally, a pseudo-code of the implemented algorithm is provided, analyzed and its

implementation described.

6.3.1 Optimization methods

There are four main types of optimization methods: 1) Local Deterministic meth-

ods that locally minimize an objective function but good initialization is critical or

they will get stuck in local minima, 2) Global Deterministic methods avoid local

minima and try to always converge to the global optimum, they either perform a

full search with bounds and ﬁnds the exact global solution or relaxes the problem

and ﬁnds a near global solution, 3) Stochastic methods that model correspondence

and registration with probabilistic and statistic approaches to handle missing data,

noise and outliers and (4) Machine learning methods that rely deep neural networks

to learn to extract good correspondences and registration from training data.

50

Global Deterministic

Branch and Bound & Tree Search Correspondence between the two point sets

can be found using a decision tree, where each node represents a correspondence

between two points {xi, y j} and the root is the correspondence between two empty

sets. Each path from the root to a leaf is the set of correspondences between two

point sets. The search technique utilized by these methods is called branch-and-

bound, it is based on a lower bound on the cost function. Naturally, the tighter the

bound is, the more efﬁcient the search will be. A sophisticated implementation of

this method is proposed in [56]. They use the previously deﬁned self-deformation

distortion measure on the represented node correspondences of the tree to prune en-

tire branches and drastically minimize the search space, hence increasing efﬁciency.

Graduated Assignment The deterministic annealing technique approximates a

non-linear objective function by adding a regularization term. Another parameter

is also added (chaos parameter) that controls convergence similar to how temper-

ature controls simulated annealing in energy functions. In [51], thin plate splines

are used to deﬁne the transformation model and graduated assignment for registra-

tion and optimization. They also add an additional parameter to the ones previously

mentioned, one that controls the “rigidity” of the registration allowing the algorithm

to favor and explore rigid transformations before gradually increasing the degrees

of freedom. In [57], a ﬁnite mixture model able to deal with two features is in-

troduced. The authors smoothly combine the original coordinates with the local

structure descriptor through an annealing scheme to obtain the mixture structure

descriptor (MSD). Then they obtain a fuzzy matrix by substituting the MSD into

the constructed model. Additionally, they use the kernel Hilbert space to model the

transformation space by an energy function which contains three main terms; the

ﬁrst is the L2 estimation (L2E) and the other two complementarily improve accuracy

and robustness for transformation estimation at both a local and global scale.

51

Local Deterministic methods

Expectation Maximization Some techniques alternate between solving the cor-

respondence and the registration problems. For instance, ﬁrst they ﬁx transforma-

tion parameters and solve for correspondence, then they ﬁx correspondences and

solve for the transformation parameters. In [58] the centroids of one point set is

represented using a Gaussian Mixture Model (GMM), then they are aligned with

the other point set. Next, the EM algorithm substitutes between the two following

steps: (1) it uses the Bayes theorem to compute the posterior probability distribu-

tion of the aforementioned GMM centroids, and it improves the parameters (Co-

variances of GMM and transformations), (2) it calculates the rigid transformation

parameters that maximize the likelihood. Adding to that, they extend this method

to include nonrigid registration parameters by regularizing the displacement ﬁelds

using coherence.

Machine learning models

Supervised Recently, PR-Net [59] introduced a novel machine learning model to

solve the registration problem. In contrast to non-learning based methods, this type

of optimization is rarely studied. Similar to [52] PR-Net uses TPS to model the

geometric transformation, it also uses correlation tensor and shape descriptor tensor

to solve the feature learning problem.

Semi-Supervised In RPM-MR [60], registration is superiorly solved by casting

it into a semisupervised learning problem, where a set of indicator variables are

selected to differentiate outliers in a mixture model. They constrain the transforma-

tion with manifold regularization exploiting the intrinsic structure of the point sets

which also plays a role of prior knowledge. Unlike PR-Net the transformation is

modeled in the kernel Hilbert Space, and a sparsity-induced approximation is uti-

lized to boost efﬁciency. Although this algorithm has proven to be superior to state

of the art methods on public data sets, it still suffers from the downsides of local

optimization techniques.

52

Stochastic Methods

VBPSM VBPSM [61] is a Probabilistic Model for Robust Afﬁne and Non-Rigid

Point Set Matching. They propose a combination strategy based on regression and

clustering to solve point-set matching issues within a Bayesian framework where

the regression estimates the transformation from model to scene and the clustering

establishes the correspondence between two point-sets. A hierarchically directed

graph illustrates the point-set matching model, and a coarse-to-ﬁne variational infer-

ence algorithm approximates the matching uncertainties. In addition, two Gaussian

mixtures are proposed to estimate heteroscedastic3 noise and spurious outliers.

IPDA Point Clouds Registration with Probabilistic Data Association [62] is a

novel algorithm that, instead of solving two given dense point clouds, tackles the

problem of aligning one dense point cloud to a sparse one and vice versa. It can

be very useful when the two point sets are taken using different sensors, such as

a vision-based sensor and laser scanner or two different laser-based sensors. Each

point in the source point cloud is associated with a set of points in the target point

cloud in this method; each association is then weighted to form a probability dis-

tribution. The result is an ICP-like algorithm, but more robust against noise and

outliers.

CPPSR CPPSR [63] is a Probabilistic Framework for Color-Based Point Set Reg-

istration. In almost all registration algorithms, only the position of the points is used

from the point clouds, although state of the art scanners also acquire color informa-

tion that can be useful information in registration. This algorithm exploits available

color information by creating a mixture model of the point-color space. It uses el-

lipses to represent spatial mixture components where each ellipse being associated

with a mixture model from the color space.

3In statistics, a collection of random variables is heteroscedastic if there are sub-populations that

have different variance from others.

53

SVR SVR [64] uses support vector machines and Gaussian mixture models to cal-

culate registrations robust to occlusions4, outliers and noise. The main idea is that

the robustness of a registration algorithm depends mostly on how the data is repre-

sented. The authors use a support vector-parametrized Gaussian Mixture (SVGM)

to represent the data. The way it works is, each point set provided to the algo-

rithm is mapped to the continuous domain by training a Support Vector Machine

and mapping it to a Gaussian Mixture Model. Since SVMs are parameterized by

a sparse intelligently-selected subset of data points, SVGM is compact and robust

to fragmentation, occlusions and noise [65]. The motivation for a continuous rep-

resentation is that a typical scene consists of a single, rarely disjointed continuous

surface, which cannot be fully modelled by a discrete sampled point-set from the

scene. SVR calculates the optimal transformation parameters that minimise an ob-

jective function based on the L2 distance between SVGMs.

An SVM classiﬁes data by building a hyperplane separating data into two different

classes, maximizing the margin between classes while allowing some mislabeling.

Since point-set data contains only positive examples, one-class SVM [22] can be

used to ﬁnd the hyperplane that separates data points in feature space from the ori-

gin or point of view. Training data is mapped to a higher-dimensional space with

a non-linear kernel function (Gaussian Radial Basis Function (RBF) kernel) where

it can be linearly separated from the origin. The optimization formulation in [22]

has a parameter ν which controls the trade-off between the training error and the

complexity of the model. It is a lower bound on the support vector fraction and an

upper bound on the misclassiﬁcation rate [22]. The other parameter is the kernel

width γ, which the authors estimate by noting that it is inversely proportional to the

square of the scale σ .

To make use of the trained SVM for point-set registration, it must ﬁrst be approx-

imated as a GMM . Without altering the decision boundary, the authors use the

transformation identiﬁed by Deselaers et al. [66] to represent the SVM within a

GMM framework. Unlike standard generative GMMs, a GMM converted from an

SVM will necessarily optimize classiﬁcation performance instead of data repre-

4Missing Data

54

sentation, as SVMs are discriminative models. This enables redundant data to be

discarded and reduces its susceptibility to varying point densities that are prevalent

in real datasets.

Once the point-sets are in the form of a mixture model, the problem of registration

can be posed as minimizing the distance between the two mixtures. The authors

use the L2 distance which can be expressed in a closed form. Conversely, the L2E

estimator minimizes the distance between densities between L2 and is known to be

inherently robust to outliers.

Special Methods

In this section we will describe methods that we could not clearly classify with the

rest of the registration algorithms.

Robust Point Matching via Vector Field Consensus

In RPM-VFC [67], the au-

thors start by creating a set of putative correspondences which will contain a lot

of wrong correspondences and a limited amount of correct correspondences. Fol-

lowing that, they estimate a consensus of inlier points whose matching follows a

nonparametric geometrical constraint. As a result they can then interpolate a vector

ﬁeld between the two point sets and solve for correspondence. They formulate a

maximum a posteriori (MAP) estimate of a Bayesian model with hidden / latent

variables indicating whether matches are outliers or inliers in the putative set. In

addition, they use Tikhonov regularizers in a reproducing kernel Hilbert space to

impose non-parametric geometric constraints on correspondence as a prior distri-

bution. MAP estimation is carried out by the EM algorithm which is able to obtain

good estimates very quickly (e.g. avoiding many of the local minima inherent in

this formulation) by also estimating the variance of the previous model (initialized

to a large value). This method proves extremely robust to outliers’ frequencies even

as high as 90%.

Dependent landmark drift DLD [68] uses prior geometric feature information

as a way of improving registration accuracy. It encodes the provided shape infor-

55

mation as a statistical shape model and deﬁnes a transformation model based on the

combination of: (1) Motion coherence, (2) Statistical shape model and (3) Simi-

larity transformation. Therefore, unlike previous methods, this method works ex-

tremely well if point sets have missing regions (due to knowledge of shape). More-

over, its computational cost is linear which makes it scalable to huge data sets.

GLTP Global-Local Topology Preservation [69] formulates the registration as a

Maximum Likelihood (ML) estimation problem with two topologically comple-

mentary constraints. First, the established Coherent Point Drift [CPD] that encodes

a global topology constraint by moving one point set to coherently align with the

other. Second, Local Linear Embedding (LLE) is introduced to handle highly artic-

ulated deformations between the two point sets while sustaining local structure.

6.4 Choice of Algorithm

Choosing the best algorithm is equivalent to choosing the algorithm that best solves

the four challenges described in section 6.1.

6.4.1 Registration Accuracy

A very good comparison of registration accuracy between state of the art methods

with respect to the degree of deformation is presented in Figure 3 of [70]. We

can clearly see that even for the highest degrees of deformation, the average error

between all of the compared algorithms varies roughly between 0.01 to 0.03. At this

point, we have to select the algorithm that best ﬁts our needs. First, since the user

is choosing the corresponding points there will never be noise, outliers or missing

data. Their always has to be 4 points on the labeling object and 4 corresponding

points on the triangular mesh. Henceforth, we can rule out stochastic methods.

Moreover, we have no exact idea what the object on the mesh will look like, nor

the initial positions of either the labeling object or the object on the mesh. Adding

to that, the labeling object is extremely far from the mesh when it is instantiated.

These reasons rule out the use of local deterministic methods. We are left with

56

global deterministic models and global machine learning models. We will not go

for the latter since it require training and usually have high runtime complexities.

6.4.2 Correspondence

Now that we know we need a global deterministic registration algorithm, we need to

narrow down the choices even more by choosing one that calculates correspondence

as well as registration. The only global deterministic methods that contains both

correspondence and registration calculation are the graduated assignment based op-

timization methods.

6.4.3 Limiting to 9 DoF

TPS-RPM contains a parameter λ that controls the rigidity of the transformation.

It allows the algorithm to ﬁrst explore rigid transformations(6 DoF) and then if it

doesn’t minimize the objective function under a certain threshold, it starts incremen-

tally increasing nonrigidity and allowing scaling and eventually sheering. Thanks

to this parameter and given the correct correspondences, the algorithm will always

choose our desired constrained (9 DoF) transformation before even exploring the

fully nonrigid one. Therefore, even though the methods in [57, 71] very slightly

surpass TPS-RPM in accuracy for full non-rigid registrations(12 DoF), TPS-RPM

guarantees constrained(9 DoF) transformations if they exist, whereas other methods

would sometimes give afﬁne ones.

6.5 TPS-RPM

6.5.1 Thin Plate Splines

Overview

There exists quite a few techniques to ensure a smooth interpolation between a set

of control points. One such technique is Thin Plate Splines, we will describe a quick

overview based on the pioneering work done in [72]. A surface that passes through

57

each control point is interpolated. Thus, a set of 3 points creates a ﬂat plane. Control

points are easy to think of as constraints of position on a bending surface. The ideal

surface bends the least. Figure 6.1 shows a 7-point example of such a surface. All

these 7 control points are forced to belong to the surface. This least bent surface is

Figure 6.1: A Thin Plate Splines that passes through a set of control points

given by the following equation:

f (x, y) = a1 + a2x + a3y +

n
∑
i=1

wiU(|Pi − (x, y)|)

(6.3)

The ﬁrst three terms correlate to the linear part characterizing a ﬂat plane that best

ﬁts all control points (this can be interpreted as a minimum square ﬁt). The last

term corresponds to a weighted sum over the n control points bending forces. For

each control point, there is a wi coefﬁcient. |Pi − (x, y)| is the distance between a

given point (x, y) and each control point Pi. The U function deﬁnes this distance as
U(r) = r2logr . So far, for each control point, the coefﬁcients a1, a2, a3 and wi are

unknown. All wi form the W vector. The deﬁnition of these unknowns is:

L−1Y = (W |a1a2a3)T

(6.4)

We do know however the set of points xi and yi (chosen by the user) and their

heights. We can therefore simply write:

P =











1 x1 y1

1 x2 y2

..

1 xn yn











, the positions of the control points

(6.5)

58

Y =







































v1

v2

..

vn

0

0

0

, 0 padded control point heights

(6.6)

We deﬁne a matrix K that evaluates U(ri j) such that ri j is the distance between two

given control points, ri j = |Pi − P j|:

U(r11) U(r12)

K =

U(r21) U(r22)








..

..








..

..

U(rnn)

(6.7)

The above-mentioned matrix L is composed of matrix K at its top-left corner, matrix

P at its right side, matrix PT at its bottom, and zeros at its bottom-right corner:





L =





K P

PT

0

(6.8)

To ﬁnd the matrix (W |a1a2a3), we either ﬁnd the inverse L−1, or solve the linear

system L(W |a1a2a3) = Y . The latter can be exactly solved using LU decomposition

by building on the knowledge that L is symmetric. Once we calculate (Wa1a2a3),

we can go back to equation 6.3 and ﬁnd the height v = f (x, y) for any point (x, y).

By using this reasoning, we can ﬁnd the thin plate spline for points in 2D. The

math can be easily adapted to 3D space and even to N − D space by using a larger

P vector and adding more ai terms. We can observe that the number of unknown

terms is proportional to the number of control points, i.e. computational complexity

increases as the number of control points increase. That being said, since our user

will more often than not choose either three or four points computation of the thin

plate spline is instant and no delay will be perceived by the user.

59

Deformation

Normally, to deform an image, you require the position (x, y) of each pixel on the

initial image to then calculate the position (x + dx, y + dy) on the deformed image.

Conversely, by using thin plate splines we only require the height information. If we

consider x and y separately, then two separate surfaces can represent their displace-

ments dx and dy respectively. Logically by moving to 3D space, another hypersur-

face for dz will be added and so on with each increasing dimension. Considering the

deformation depicted in Figure 6.2, we deﬁne 6 points: 1 in an eye, 4 in the image

corners and 1 in the smile corner. There is no displacement at the corners, the dx

Figure 6.2: Thin plates can be used to deform image (a) to (b)

and dy surfaces have no height. In this use case, the smile corner moved upwards,

so the surface dy should have a considerable height, whereas the surface dx should

have nearly no height. The eye position was moved to the left, which means that

the surface dx should have a negative height while the surface dy height should be

close to 0. The thin plate spline deformation is depicted in Figure 6.3.

Figure 6.3: Interpolation of displacement in the (a) x direction and in the (b) y
direction

We can clearly conclude that dx and dy are then only parameters we need to perform

deformation and ﬁnding the transformation parameters of two point sets means ﬁnd-

60

ing these two surfaces if we are working in 2D. In our problem we would need to

ﬁnd the three surfaces dx, dy and dz to solve the registration problem.

6.5.2 Robust Point Matching Algorithm Description

A binary linear assignment-least squares energy function

First, let’s reconsider V and X deﬁned in section 6.1, two point sets in 2 dimensional

space (For simplicity of explanation). They consist of the points {va|a = 1, 2, . . . , N}

and {xi|i = 1, 2, . . . , K}, respectively. We will represent the nonrigid registration by

the function f . Given any point va in 2D space it can be mapped to a new point

ua = f (va). Of course,

f in this case englobes rotation, translation and scaling

and this function is a simpliﬁcation of function 6.1. The authors then introduce an

operator L to deﬁne the smoothness measure ||L f ||2 which will help place appro-

priate constraints on the mapping. The correspondence problem is cast as a linear

assignment problem. Therefore, the goal is to minimize the following binary linear

assignment-least squares energy function :

min
Z f

E(Z, f ) = min
Z f

K
∑
i=1

N
∑
a=1

zai||xi − f (va)||2 + λ ||L f ||2 − ζ

K
∑
i=1

K
∑
a=1

zai

(6.9)

With Z or zai being the binary correspondence matrix consisting of two parts: (1)

The last column and last row handle outliers and (2) the inner part is either 1 or 0,

the former in the case of correspondence between va and xi and the latter if no corre-

spondence exists. Naturally, the correspondence is always one-to-one so the matrix
Z has to satisfy the following summation constraints, ∑K=1
∑N+1

i=1 zai = 1 for a ∈ 1, 2, .., K,
a=1 zai = 1 for i ∈ 1, 2, .., N, and z{ai} ∈ {0, 1}. An example of Z is given in Fig-
ure 6.4, where points v1 and v2 correspond to x1 and x2, respectively, and the rest

of the points are outliers. Note that the existence of an extra outlier row and outlier

column makes it possible for the row and column constraints to always be satis-

ﬁed. The second term of the equation is the constraint on the transformation. The

third and ﬁnal term is the robustness control term preventing rejection of too many

points as outliers. The parameters λ and ζ are the weights that balance these terms.

61

Figure 6.4: An example of the binary correspondence matrix.

In this way, the point matching objective function (Equation 6.9) consists of two

interlocking optimization problems: a discrete linear assignment problem on the

correspondence and a least-squares continuous one on the transformation. When

considered separately, both problems have unique solutions. It is their combination

that makes it difﬁcult to match the non-rigid point problem. To solve these two

problems, the authors propose an alternating algorithm in which the ﬁrst step esti-

mates correspondence and the second one calculates the transformation. Solving in

this manner while using binary correspondences in Z is not meaningful, therefore

they use fuzzy correspondence until the algorithm starts to converge. Once it nears

a reasonable solution correspondences switch to binary values. To achieve this, the

authors utilize two techniques we will describe in the following paragraph.

Deterministic annealing and Softassign Softassign relaxes the binary correspon-

dence matrix Z to a fuzzy(continuous) matrix M in the interval [0,1], yet keeps the

column and row constraints using iterative normalization on both of them. From

an optimization point of view, fuzzy correspondences will allow the deﬁned en-

ergy function to behave better because correspondences will improve gradually in-

stead of jumping around in binary space. Correspondingly, deterministic anneal-

ing [18,45] is used to control this fuzziness. It adds the following entropy term
T ∑K=1

a=1 mailogmai to the energy function in Equation 6.9. T is called the tem-
perature parameter because similar to physical annealing as we minimize the value

i=1 ∑N+1

of T the energy function 6.9 will be minimized. The higher the entropy term, the

fuzzier the correspondences are. Each minimum obtained at a certain temperature

will serve as a starting point for the next stage as the temperature is gradually low-

62

ered.

A fuzzy linear assignment least squares energy function These two techniques

will transform the binary equation described in Equation 6.9 to the following fuzzy-

assignment-least squares energy function:

E(M, f ) =

K
∑
i=1

N
∑
a=1

mai||xi − f (va)||2 + λ ||L f ||2

+ T

K
∑
i=1

N
∑
a=1

mailogmai − ζ

K
∑
i=1

N
∑
a=1

mai

(6.10)

where mai satisﬁes the same sum constraints that were present on zai.

We can deduce that when T becomes 0 the equation reverts to Equation 6.9 and

accordingly M reverts back to the binary matrix Z. An algorithm that alternatively

solves for correspondence then for transformation reducing the temperature at each

step has proven successful in rigid registration but two problems arise in the non-

rigid case. First, there is no clear way to estimate the outlier control parameter

ζ . Since we do not suffer from the outlier problem, we have decided to set this

parameter to zero thus reducing Equations 6.9 and 6.10 to the following :

min
Z f

E(Z, f ) = min
Z f

K
∑
i=1

N
∑
a=1

zai||xi − f (va)||2 + λ ||L f ||2

(6.11)

E(M, f ) =

K
∑
i=1

N
∑
a=1

mai||xi − f (va)||2 + λ ||L f ||2 + T

K
∑
i=1

N
∑
a=1

mailogmai

(6.12)

Second, setting λ for the prior smoothness term can be difﬁcult as on one hand, the

transformation can turn out to be too ﬂexible at small values of λ . On the other

hand, large values of λ greatly limit the range of nonrigidity of the transformation.

The authors decided to gradually reduce λ via an annealing schedual by setting

λ = λinitial × T . The larger λ is the more global and rigid registrations are favored,

and as it decreases with the temperature, more local and nonrigid transformation are

calculated.

63

The robust point matching (RPM) algorithm It is essentially a dual update pro-

cess, we will now describe these two steps after applying our modiﬁcations to the

algorithm.

Step 1 Update correspondance for points i = 1, 2, .., K and a = 1, 2, .., N in the fuzzy

correspondance matrix M using the following equation:

mai =

1
T

exp(−

(xi − f (va))T (xi − f (va))
2T

)

(6.13)

Then we run the normalization algorithm on both the row and columns to satisfy

the constraints until convergence is reached:

mai =

mai =

mai
∑N+1
b=1 mbi

mai
∑K+1
j=1 ma j

, i = 1, 2, . . . , K

, a = 1, 2, . . . , N

(6.14)

(6.15)

Step 2 Update Transformation: After dropping the terms independent of f , the

following least square problem needs to be solved,

min
f

E( f ) = min

f

K
∑
i=1

N
∑
a=1

mai (cid:107)xi − f (va)(cid:107)2 + λ T (cid:107)L f (cid:107)2

(6.16)

The solution to this least squares problem depends on the particular form of the

non-rigid transformation. We’re going to discuss the solution for one form in the

next sectionm, the thin-plate spline.

Annealing : We previously described an annealing scheme to control the energy

function. T is initiated to a temperature T0 and is gradually reduced via an linear

annealing schedual, Tnew = Told × r (r is the annealing rate). This is performed until

a previously set temperature Tf inal is reached.

The parameters are chosen as follows: (1) T0 is set to the largest square distance of

all point pairs, (2) r is set to 0.93 so the algorithm is slow enough to be robust, yet
not too slow and (3) Tf inal is chosen as T0
100.

64

6.5.3

Integrating TPS in RPM

In the previous section, we deﬁned the nonrigid transformation parameter in our

energy function as f . We will now discuss the authors’ chosen form of nonrigid

registration; Thin Plate Splines [72]. In section 6.5.1 we described how thin plate

splines can be used to describe deformation. In this section, we will describe its

integration in the robust matching algorithm.

Since nonrigidity allows for multiple mappings between the two point sets. The

second term of the energy function (smoothness measure) is added to constrain the

mapping between two point sets, it avoids mappings that are too arbitrary. One

of the simplest measures is the space integral of the square of the second order

derivatives of the mapping function. This leads us to the thin plate spline. Therefore,

ﬁnding the TPS that ﬁts the two point sets is equivalent to solving the following least

squares energy function:

ET PS( f ) =

N
∑
a=1

||ya − f (va)||2 + λ

(cid:90) (cid:90)

[(

δ 2 f
δ x2 ) + 2(

δ 2 f
δ xδ y

) + (

δ 2 f
δ y2 )]dxdy

(6.17)

There exists a unique minimizer f which comprises two matrices w and d:

f (va, d, w) = va.d + φ (va).w

(6.18)

where d is a (D + 1) × (D + 1) matrix representing the afﬁne transformation and w

a K × (D + 1) warping coefﬁcient matrix representing the non-afﬁne deformation.

Replacing 6.18 in 6.17 yields the following energy function:

ET PS(d, w) = ||Y −V d − φ w||2 + λtrace(wtφ w)

(6.19)

where Y and V are just concatenated versions of the point coordinates ya and va,

and φ is a K × K matrix formed from φ (va)

Finding the least squares solution for equation x can be cumbersome.

Instead,

the authors perform a QR decomposition to separate afﬁne and non afﬁne warp-

65

ing space:

Equation 6.19 becomes:



V = |Q1Q2|



R





0

(6.20)

ET PS(d, w) = ||Y −V d − φ w||2 + λ1trace(wT φ w) + λ2trace[d − I]T [d − I] (6.21)

A detailed mathematical explanation of the ﬁnal equation can be seen in section 4

of [52]. Minimizing this equation solves the registration problem. For the same

reasons explained before, λ1 and λ2 follow an annealing schedule. We set λ1 to 1

and λ2 to 0.01 so as to always favor afﬁne transformations. Each temperature is

calculated during ﬁve iterations before it is decreased. This is done upon reaching

a global solution.

6.5.4 Problem with correspondence

After implementing the TPS-RPM algorithm described in the previous sections, we

ran into some problems regarding the correspondence calculation step. Even though

we took measures to limit the registration algorithm to only 9 degrees of freedom

(choosing optimal temperature values), on some rare occasions, it calculated the

correspondences in some way that requires skewing to be possible. As we have

already discussed, this is not possible in Unity. Even if the transformation is techni-

cally 100% accurate, in Unity it will seem wrong since the skew and the rotation will

be applied as if they were just a rotation. Since the algorithm would have searched

for an optimal restricted transformation (9 DoF) before as a last resort, searching for

a fully nonrigid one. The calculated correspondences did not permit for a restricted

transformation to be possible. We know that there exists another correspondence

between the points that only needs rotation, translation and scaling to be optimal.

Therefore in this section, we will ﬁnd a solution that always calculates the correct

correspondences in those rare instances when the algorithm fails.

66

Proposed Solution

There are three invariants in our problem: (1) there are no outliers between the two

point sets, (2) the user will choose 4 points to deﬁne the three dimensions (height,

width, depth) and (3) there is always a certain one-to-one correspondence between

the the points that allows for a skew-less transformation. In our test cases, the prob-

ability of the algorithm choosing a correspondence that contains skewing is almost

5%. That is because the parameters T and λ have been carefully chosen. However,

even a 95% success rate is unacceptable in an application whose main objective is to

reduce time and effort in labeling data sets. Henceforth, we need to ﬁnd a solution

that not only provides an almost perfect accuracy but also does it without losing the

tool’s “real-time" property.

Since the user always choses four points, then at most there are 4! = 24 possible one-

to-one correspondences between the points. Therefore, in the case of the algorithm

choosing wrong correspondences, we can start by exploring the 23 other possible

correspondences to ﬁnd the one that minimizes the skew parameter. Hence, after

the execution of the normal algorithm, we extract the skew parameter from the cal-

culated 4 × 4 transformation matrix and test if it is signiﬁcant or not. In our testing

we found that testing if it is larger than 0.1 is the best option. If this condition is met

then we have to calculate the transformation at each of the 23 correspondences and

choose which one minimizes the standardized euclidean distance, cdist, between

the result and the target chosen by the user to calculate the result we simply apply

Equation 6.1. Therefore, we need to keep track of the smallest cdist achieved; let’s

call the variable that does this bookkeeping mind. For each correspondence, we test

the new found cdist value to the smallest one calculated to date and replace it in the

case it is smaller, otherwise we keep it the same. When the algorithm is completed,

the transformation resulting from the correspondence that had the smallest cdist is

the one returned. Keep in mind that for any transformation’s cdist to be eligible of

being compared to mind, its skew has to be lower than 0.1.

67

Improvements

The ﬁrst improvement we can add is setting a breaking condition where, we no

longer try the rest of the possible correspondences. After checking that the skew is

lower than 0.1, if the cdist is lower than a certain threshold. Testing cases showed

that a decent value of the error threshold is 0.15.

The second improvement is parallelizing the full process. By parallelizing we mean

launch a separate thread that evaluates each correspondence separately. One thing

we have to keep in mind is for the variable mind that keeps track of the smallest

cdist, concurrency should be handled as it is checked and possibly changed by each

thread. We protect this concurrency by applying a fundamental concept in parallel

computing, the Semaphore. We use a simple semaphore that we initialize to the

value 1; it is acquired by each thread to check the condition on mind: if the condition

is true mind is updated and then the semaphore is released, otherwise the semaphore

is directly released. In the next section, we will provide the pseudo-code explaining

the proposed solution and its improvements.

68

6.5.5 Algorithm Pseudo-Code

Result: 4 × 4 Transformation Matrix

Initialize parameters T , λ1, λ2 and M;

Initialize parameters d and w;

Begin A: Deterministic Annealing:

Begin B: Alternating Update:

Update transformation parameters (d, w) using 6.16.;

End B;

Decrease T , k1; and k2.;

End A;

Acquire mutex;

if skew< 0.1 then

if cdist < meand then
meand = cdist

optimal_trans = calculated_trans;

end

end

Release mutex;

Algorithm 1: TPS-RPM assuming given correspondences (launched in threads)

69

Result: 4 × 4 Transformation Matrix

Initialize parameters T , λ1 and λ2;

Initialize parameters M, d, and w;

Begin A: Deterministic Annealing:

Begin B: Alternating Update:

Step I: Update correspondence matrix M using (3)–(5).;

Step II: Update transformation parameters (d, w) using Equation 6.16.;

End B;

Decrease T , k1 and k2.;

End A;

if skew >0.1 then

Initialize global parameters meand, mutex, optimal_trans;

for i ← 0 to 23 do

LaunchThread(Algorithm1(correspondance(i)));

if meand <0.15 then

return optimal_trans

end

end

end

return optimal_trans

Algorithm 2: Original TPS-RPM algorithm without outlier calculation in M

Analysis

The authors claim that TPS-RPM has a worst case runtime complexity of O(n3). Af-

ter our modiﬁcations, in the worst case, we will call a simpliﬁed (given correspon-

dence) TPS-RPM at most 23 more times. Thus, on a single threaded machine or if

the loop is not broken by any thread, the worst case time complexity is O(24 ∗ n3),

which is rendered asymptotically to O(n3). As a result, even in the most unlikely of

cases, our modiﬁcations preserve the asymptotic run time complexity of the original

algorithm is preserved. Consequently, we have preserved the speed of TPS-RPM

while increasing our registration accuracy to 100%.

70

6.5.6

Implementation

Everything regarding the registration calculation was done on the Python server

side. The point sets are provided by Unity through websockets. Upon retrieval the

Python server executes Algorithm 2 and returns its output to Unity. The latter ap-

plies it on the labeling object to transform it to the correct position. We implemented

TPS-RPM from scratch and modiﬁed it with the aforementioned improvements.

The returned transformation matrix has the following form:











sx ∗ r11

sy ∗ r12

sz ∗ r13

sx ∗ r21

sy ∗ r22

sz ∗ r23

sx ∗ r31

sy ∗ r32

sz ∗ r33

0

0

0











tx

ty

tz

1

(6.22)

where sx, sy and sz are the values of S in the x, y and z axes respectively, tx,ty and tz

are the values of T in the x, y and z axes respectively and ri j/i,j∈[1, 3] are the values

of R.

6.6 Colliders in Unity3D

The points that are being selected are primitive objects of type spheres. To be able

to add the point on the desired object, the concept of colliders should be introduced

so that the point (sphere object) can collide with the corresponding object and be

attached to it.

In Unity3D, there’s a possibility to add a “Collider” component on an object, that

deﬁnes the shape of the object for the purpose of physical collisions, the collider is

invisible and should be the exact same shape as the object to produce accurate colli-

sions with other objects. There are multiple types of colliders already implemented

whether for 3D or 2D use. In this application, three types were used:

1. Box Colliders: This component was added to the labeling objects.

2. Sphere Colliders: It was attached to the spheres that are eventually added as

a set of points on the labeling object.

71

3. Mesh Colliders: This type was only attached to complex meshes, including

labeling object imported as CAD objects.

Figure 6.5: Box and Sphere Colliders

As a result, the points can now be attached to the labeling objects as shown in Fig-

ure 6.5. Note that, in this ﬁgure the objects are not rendered, only for the purpose of

showing the sphere and box colliders (in green color). After the points are attached

to the labeling object, they are assigned as children of this object; therefore, their

coordinate system is now referenced to the object and not Unity’s world coordinate

system. That being said, the coordinates of these points can now be sent to the

server and the algorithm can work efﬁciently to return the transformation of this

labeling object.

Furthermore, to select the points on the target object, a mesh collider should be

attached to the imported mesh. In Unity3d, the mesh collider component takes a

mesh as an input and builds the collider based around that mesh. However, to a add

a collider on a speciﬁc mesh, that latter must contain a number of vertices which is

less than or equal to 65536.

Two solutions were proposed, either adding multiple primitive colliders to the mesh

or simplifying the mesh so that each time it is loaded the number of vertices is sim-

pliﬁed to the desired amount that Unity3D accepts. However, the ﬁrst solution was

somehow impossible, hence the complexity of the mesh and the fact that it is loaded

as one object that is not segmented into sub-meshes. The second solution was fea-

sible, but with the condition of keeping the imported rendered mesh with the same

high number of vertices and only simplify the mesh that has to be taken as an input

72

for the collider. In addition to the two solutions already mentioned, mesh colliders

in Unity3D possess a convex property that could be enabled or disabled depending

on whether the developer wants to add the option of colliding with other mesh col-

liders. However, one of the drawback of using this option is that it only supports up

to 255 triangles and it is obviously not suitable for this mode’s use case. Figure 6.6

displays a convex mesh collider attached to one of the test meshes that have been

used. In fact, it is clear that this property limits the details of the collider and thus

provides a poor experience for the user trying to set the points on a speciﬁc part

of the mesh. For example: the green trash bin showing in Figure 6.6. Therefore,

applying a simpliﬁcation algorithm on the mesh and feeding it to the collider for

maximum collision accuracy, was the chosen solution to tackle the given problem.

Figure 6.6: Convex Mesh Collider

6.6.1 Mesh Simpliﬁcation

Background

In recent years, the problem of surface simpliﬁcation has received a lot of atten-

tion. Many algorithms have been formulated to solve this problem and they can be

categorized into 3 classes:

73

1. Vertex Decimation: It is based on iteratively selecting a vertex to be re-

moved, removing the adjacent faces and re-triangulating the resulting hole.

2. Vertex Clustering: This algorithm is based on creating a bounding box

around the mesh and dividing it into a grid containing different cell. Within

each cell, the vertices are clustered together into one vertex and the faces are

updated accordingly. This method could be very fast but it provides a quality

that is often quite low. A solution for this problem was later on introduced,

explaining that this method could be generalized to use an octree5.

3. Iterative Edge Contraction: This method consists of simplifying the model

by choosing an edge and collapsing it (see Figure 6.7). Many algorithms

implemented this idea and the real difference between them is the way that

they chose the edge that is going to be contracted.

Figure 6.7: Edge Collapse

Micheal Garland and Paul S.Heckbert [73] discuss the limitations of the algorithms

already mentioned, by saying that these 3 classes of algorithms don’t provide a

high quality approximation (Vertex Clustering), nor general (Vertex Decimation)

nor supports aggregation. However, they developed an algorithm that supports all

these 3 properties at the same time. This method is based on decimation via pair

contraction which is a general form of edge contraction. Nevertheless, since the

algorithm uses edge collapse, a criterion should be added to select an edge. Given

two vertices, v1 andv2, a pair (v1, v2) is a valid pair for collapsing if:

• (v1, v2) is an edge (see Figure 6.7), or

5An octree is a tree data structure in which each internal node has exactly eight children.

74

• |v1 − v2| < ε, where ε is a user deﬁned constant that should be carefully set.

High threshold could enable vertices that are far from each other to collapse

which will ruin the topology of the model. (see Figure 6.8)

Figure 6.8: Non-Edge Collapse

In addition, to select the appropriate contraction at a given iteration, this algorithm

introduces a notion of cost of a contraction. This cost is deﬁned as the error at each

vertex ∆(v) = vT Qv, where Q is a 4 × 4 symmetric matrix assigned to each vertex.

Note that, since Qv, is 4 × 4 matrix ∆V = δ , is the set of all points whose error with

respect to Q is δ which is a surface of second degree in v (quadric surface). Hence,

this error is referred to as the quadric error metric. In fact, Qv = ∑ MP(v) where

P =< a, b, c, d > is a plane that contains an incident triangle of v and MP is equal to

the following matrix:



MP = PPT =

a2 ab ac ad

ab b2 bc bd

ac bc

c2

cd

ad bd cd d2



















(6.23)

Furthermore, after the pair of vertices is chosen, a simple way to collapse is to move

v1 to v2 or vice versa, or to (v1 + v2)/2. However, this algorithm introduces a better

way to collapse the vertices by moving to a new point v that minimizes the error
∆(v) = (vT Qv1v + vT Qv2v)/2 = (vT (Qv1 + Qv2)v)/2. After v is computed, (v1,v2)

is collapsed into v with the error value ∆(v) and an error metric matrix of Qv1 + Qv2.

Moreover, ﬁnding the minimum of a quadratic function is a linear problem and to

ﬁnd the new vertex v we need to solve the following equation:

δ ∆/δ xv = δ ∆/δ yv = δ ∆/δ zv = 0.

(6.24)

75

Algorithm

Compute the error value and error matrix for each vertex of the mesh;
Select all valid edges (v1, v2) such that |v1 − v2| < ε ;
while an edge (v1, v2) exists do

Minimize ∆(v) = (vT (Qv1 + Qv2)v)/2 to ﬁnd v;
Let ∆(v) = (∆(v1) + ∆(v2))/2 and Qv = Qv1 + Qv2 ;
Place all selected edges in a heap using ∆(v) as a key;

end
while Heap contains edges do

Remove the top edge (v1, v2);
Collapse it to the computed v;
Update the mesh and the keys;

end

Algorithm 3: Mesh Simpliﬁcation Using Quadric Error Metrics

Implementation in Unity

This algorithm is already implemented by Mattias Edlund [74] and conﬁgured to be

able to work in Unity projects. It introduces two functions that simplify the mesh,

one with a quality parameter and the other without one. In our implementation, we

used the function that gave the opportunity to change the target quality of the mesh,

because it makes the target vertex count more controllable than the one with no pa-

rameter. Moreover, this parameter is a number between 0 and 1 that is multiplied by

the number of triangles to provide a threshold for the main iterations. The function

returns the simpliﬁed mesh when the original number of triangles subtracted by the

number of deleted triangles is less than or equal to the threshold.

However, after integrating this method into the tool and testing it, we started chang-

ing the quality by trial and error and deduced the resulting number of vertices by

making sure that the vertex count is always less than or equal to 65536. After the

testing phase, we concluded that the mesh vertices are always varying and could

lead to a simpliﬁed mesh that could not be loaded as a collider in Unity. As a pre-

processing method before loading the data into the tool, the mesh is simpliﬁed so

the vertex count is ∼ 1 million.

Furthermore, to optimize the workﬂow of loading the collider when entering Phan-

tom Mode, we serialize the simpliﬁed mesh into a JSON ﬁle saved in the same

76

Figure 6.9: Loading Collider Flowchart

Figure 6.10: Concave Mesh Collider

folder of the input mesh. Hence, the mesh will be simpliﬁed once, and then it will

be de-serialized and loaded much faster. The example in Figure 6.10 shows the

same mesh of Figure 6.6 but after using the mesh simpliﬁer and adding the concave

mesh collider. It shows the high level of details that could be reached in the mesh

in case of trying to collide with it. In conclusion, this workﬂow (see Figure 6.9)

77

added the possibility to apply Phantom Mode on any loaded mesh, leading to a

faster labeling action.

6.7 Websockets

WebSocket is a standardized communication protocol which produces a communi-

cation over a TCP connection. It is a way to communicate in an asynchronous way

between a client which is the Unity software in our case and the server which can

be on the same machine or even on another machine. Communication can be done

using a secure protocol and can be used by any client or server application.

The connection is bidirectional which means the data ﬂows in both ways. The Web-

Socket API lets the server and the client to push messages to each other at any given

time. This is be achieved without having the need to establish a new connection,

which allows a real time data ﬂow.

6.7.1 Server Implementation

Before implementing the server side of our application, many technologies were

considered to enable a bi-directional communication between the Unity application

and the server, that is responsible to compute the main algorithm and send the results

back to the client. The ﬁrst technology that we came across is Soﬁ, a framework

that was developed to provide a “pythonic” GUI by packaging HTML5 libraries, in

a way that all the processing is done using web sockets within Python. However,

this framework does not suit the use case of our project, knowing that, it doesn’t

include any interactions with the web, for the meantime. The same issue was en-

countered when using the SocketIO technology. SocketIO is a protocol that enables

a real time connection between clients and a server, but the client in this case is a

web application or mobile application, which leads to multiple complications when

implementing a SocketIO server, using the SocketIO Python library, and issuing a

connection to the Unity application.

Therefore, websockets Python library was used on top of asyncio [75] to implement

the server and ensure a real-time communication with the Unity client. Neverthe-

78

less, the asyncio library provides the ability to write asynchronous programs using

the async/awaits syntax. With asyncio programming there is no parallelism, the

program is running on a single thread. However, when the functions’ execution is

pending, Python can run other functions and proceed when they all have what they

need to continue their execution. These functions are called Coroutines, they are

declared with async/await syntax placed before the function, for example: “async

def main():”. When a coroutine runs, it generates a task that executes till the end,

except if it encounters an await, then it gets suspended and runs the other corou-

tine. The asyncio.get_event_loop() will return the suspended task to continue its

execution. The websockets library provides the program with the server’s send and

receive functions, using the websockets.server module that deﬁnes the WebSocket

server API. This module includes a serve function that takes as parameters:

the

coroutine to be executed, the host and the port on which the server is listening, and

returns an awaitable object that is handled by the asyncio library using the asyn-

cio.get_event_loop().run_until_complete (the awaitable object).

The server implemented in Python, listens on port 4444 and returns an awaitable

that executes the coroutine. In this function, the server awaits data from the con-

nected client using the coroutine websocket.recv(), handles the data and then sends

it back to the client using the coroutine websocket.send(). To keep the server on and

listening at all times, the asyncio.get_event_loop().run_forever() function that runs

the main coroutine indeﬁnitely was used.

The client sends two Nx3 matrixes, one for the source points and the other for the

target points, where N signiﬁes the number of points selected. At ﬁrst, the client

sends the source and target matrixes separately as a chain of characters where the

points are separated by a “/”, and the coordinates by a comma. On the server side,

each sent matrix was handled by splitting the points and the coordinates to be able

to create the arrays and pass them to the main algorithm. The main registration

algorithm returns a 4 × 4 matrix, this latter is also transformed into a chain of char-

acters and sent to the client.

Moreover, to ensure that the computations done by the server are fast and reliable,

the server code was implemented on another machine. The client is connected to

79

the same network on which the server is on, and for the websockets.server.serve

function, the argument “None” that maps to the host was passed. By that, the server

was capable of receiving and sending data to the client on the port 4444.

6.7.2 Message Structure

JSON is a lightweight data-structure that is easy to read and to parse by the ma-

chines, this data structure is written in key/value pairs. The format used to represent

the data is seen in Figure 6.11. On the server side, the json Python library provided

Figure 6.11: Message Structure

functions to parse the JSON sent by the client. The json.loads() function is used to

convert the chain of characters taken as an argument, into a JSON object that could

be easily manipulated by calling the keys of the JSON format (see Figure 6.12).

Figure 6.12: Python JSON handling

6.7.3 Client Implementation

In our project, multiple data must be sent to the server, so it can do the calculations

on the backend. Then, the client will receive back the 4 × 4 matrix containing nec-

essary information for the translation and rotation. There are two cases, either the

client wishes to perform a rotation and translation normally, or they wish to only

80

perform a rotation using the normal of the points.

In the ﬁrst case, the user can choose as much points as they wisheon the source and

target objects. Then, these points will be sent to the server in the corresponding

order using the WebSocket in a JSON format, specifying the type of transformation

requested by the user, along with the source and target points, so it can easily be

interpreted by the server. In the second case, the user should choose three points on

two objects, the source and target. The program will calculate the normal of each

object using the chosen points and send these two vectors with the type of task to

the server.

Moreover, the client should establish a connection using the server’s IP and port

number. Then, after clicking on an execution button to activate one of the two

transformation methods, the source and target points can each be sent separately

using only one connection. Nevertheless, JSON englobes the data in one block,

thus, the content of the data is easily extracted using JSON’s functions. For futures

tasks, JSON increases the scalability: if more types of transformations or variables

are to be added, it could be simply included as a JSON key/value.

For the client’s implementation, WebSocketSharp library [76] was used, which is a

subset of the Socket.IO library. This latter can be found in the Unity asset store for

free which also contains a library to handle JSON objects. It also works on most

platform focusing equally on reliability and speed. For the WebSocket implemen-

tation, some basic event handlers should be deﬁned: “On Open”, “On Close”, and

“On Message”.

• “On Open” event will be activated when the connection between the client

and the server is established.

• “On Message” event will be called when receiving data to the client socket.

After receiving a message, the connection will automatically close, but this

can be modiﬁed in the library.

• “On Close” event will run when the connection to the server ends.

To send a message, the send() function already implemented in the imported Web-

SocketSharp library has to be called, followed by the message of type String which

81

will be the JSON message to be sent.

Moreover, a JSON object from the received message is created to access its data,

but a problem occurred when using the JSON library’s constructor: the accuracy of

the data went down from numbers in 62 bits to 32 bits. A solution had to be found

since the accuracy plays a major role in moving objects. Since only double types

were needed, some unnecessary functions using or converting to ﬂoat type were re-

moved. After creating the object, the program will access each data in the received

message, add them to a 4 × 4 matrix and ﬁnally lunch a block of code to run, which

will be the start of the transformation.

6.8 The Labeling Element Transformation

After selecting the points on the source and target objects in the right spots with

the help of the accurate colliders, the points are sent to the Python server through

websockets in a custom format. Then, the server returns a transformation matrix,

responsible for the change of position, rotation and scale of the source object onto

the target mesh.

Knowing that the points were relative to the source object origin, the transformation

should be done accordingly. To be able to do that in Unity, we dissected the returned

transformation matrix to extract the position, rotation and scale, then we applied

each one on the source object so it can be ﬁnally transformed to the desired location.

6.9 User Interface

For the user interface of this mode, we added a button labeled Phantom Mode, takes

the user into another panel (see Figure 6.13). In this window, we notice that the

source points are already been selected on the source object, the user needs only to

add the points on the mesh and click on Run Transformation in the footer to launch

the process.

However, the user can still select the custom points on the source target if they

click on Rest Selection button. In addition, if a point is wrongly added the user

82

Figure 6.13: Phantom Mode User Interface

can still click on the escape key to revert the last point added. As a result, the user

experience became more intuitive. When the user is done using this mode, they can

click on Normal Mode to go back to the standard labeling process.

83

Chapter 7

Labeling Without a Mesh

7.1 Problem Statement

We have already covered the process of using a 3D reconstructed mesh from RGBD

images to label said images. However, in certain cases, where we only have access

to the RGB images and the camera positions, we cannot reconstruct the mesh. In

this chapter, we tackle the case where we label the images without the presence of

the mesh.

7.2 Assumption

To render this problem feasible, we make a prior assumption that we already know

the size of the object we want to label. For instance, if we have a CAD object that

represents the object we want to label.

7.3 Theory

7.3.1 Deﬁnition

Let’s deﬁne a rectangular cuboid B (Represented in Figure 7.1) with speciﬁc known

dimensions (height, width, depth). Let’s take three points on this object (p1, p2, p3).

We also deﬁne O as a point in space. We can solve the problem in Section 7.1 by

84

reducing it to the following problem:

Given three lines (l1, l2, l3) created by connecting O and (c1, c2, c3) respectively, we

need to ﬁnd the position of three points ( f1, f2, f3) on (l1, l2, l3) respectively while

keeping the distances (p1 p2, p2 p3, p1 p3) equal to ( f1 f2, f2 f3, f1 f3).

A geometrical representation of the problem can be found in ﬁgure 7.2.

Figure 7.1: Rectangular Cuboid B

Figure 7.2: Problem Representation

7.3.2 Problem Breakdown

The ﬁrst thing to consider is that f1, f2 and f3 should each be on l1, l2 and l3 respec-

tively, thus deﬁning the following three constraints: (1) f1 ∈ l1, (2) f2 ∈ l2 and (3)

f3 ∈ l3.

85

The equation (in vector form) of a line in 3D space passing through the point

(x0, y0, z0) and traveling in the direction (a, b, c) is:

(x, y, z) = (x0, y0, z0) + t(a, b, c)

(7.1)

where t is a parameter describing a particular point on the line. We can calculate t

for the points (c1, c2, c3) for each line:






tc1 = c1 − O

tc2 = c2 − O

tc3 = c3 − O

Thus the coordinates of p1, p2 and p3 are :






f1 = O + tc1 ∗ u1

f2 = O + tc2 ∗ u2

f3 = O + tc3 ∗ u3

(7.2)

(7.3)

where u1, u2 and u3 are the unknowns we need to determine.

Second we deﬁne three equations that represent the distance constraint mentioned

in section 7.3.1:

p1 p2 = f1 f2

p2 p3 = f2 f3

p1 p3 = f1 f3

(7.4)

(7.5)

(7.6)

The distance between two points in 3D space can be calculated using the following

equation:

dist =

(cid:113)

(x1 − x2)2 + (y1 − y2)2 + (z1 − z2)2

(7.7)

86

If we replace Equation 7.7 & 7.3 in Equations 7.4, 7.5 & 7.6 we get the following

non-linear system of equations:






dist(p1, p2) = dist(O + tc1 ∗ u1, O + tc2 ∗ u2)

dist(p2, p3) = dist(O + tc2 ∗ u2, O + tc3 ∗ u3)

(7.8)

dist(p1, p3) = dist(O + tc1 ∗ u1, O + tc3 ∗ u3)

7.3.3 Solving the Equation

We can clearly see that equation 7.8 is a system of three equations with three

unknowns(u1, u2, u3). We can also deduce that it is a non linear system since if

we square both sides of the equations we obtain a sum of squares. We will solve for

the three unknowns using a numerical method.

Newton-Raphson

The Newton-Raphson method is the ﬁrst in the class of Householder’s methods, it

is deﬁned by the following equation:

xn+1 = xn −

f (xn)
f (cid:48)(xn)

(7.9)

It can be generalized to solve systems of k nonlinear equations by left multiplying
with the inverse of the k × k Jacobian matrix JF (xn) instead of dividing by f (cid:48)(xn):

xn+1 = xn − JF (xn)−1F(xn)

(7.10)

Newton-Raphson is a method of quadratic convergence and hence rapid. However,

global convergence is not guaranteed for all initial estimates and for example fails

for the two following cases:

Stationary Iteration point: Consider the function f (x) = 1 − x2, if we start at

x0 = 0 then x1 will be undeﬁned since the tangent to a point on the X axis (0,1) is
parallel to the X axis: x1 = x0 − f (x0

f (cid:48)(x0) = 0 − 1

0

87

Starting point enters a cycle: Consider the function f (x) = x3 − 2x + 2, for a

starting point of 0, the algorithm will alternate between 1 and 0 and will never

converge.

As we can see we cannot rely on this method directly. We decided to use a method

that supports constrained solution searching and use its result as an input to the

Newton-Raphson method.

DOG_BOX

The DOG_BOX method [77] is a trust region method for solving bound constrained

nonlinear optimization problems. Unlike other popular methods that use ellipsoid

trust regions, this procedure uses a rectangular shape which is much simpler due to

the constraints being linear. It uses a modiﬁcation of Powel’s dogleg technique [78]

to ﬁnd the solutions.

The advantage of this method is that it supports bounds on the unknown variables,

which means we can constrain the depth value of the unknowns to be superior to

the depth value of O. This is done because in our problem the box B can ﬁt either in

front of O or behind it, please refer to Figure 7.3 for a visual representation. After

Figure 7.3: Bounded Problem B

we solve the system of non linear equations 7.8 using the DOX_BOX method. We

input the approximated (u1, u2, u3) as a starting estimate to the generalized Newton-

Raphson method and use its output as the ﬁnal solution.

88

Calculating ( f1, f2, f3)

We replace the ﬁnal values of (u1, u2, u3) in equation 7.3 and get the coordinates of

the three points ( f1, f2, f3).

7.3.4 Rigid Registration

Now that we have the three points ( f1, f2, f3) on the lines (l1, l2, l3), and we already

have the three points (p1, p2, p3) on the box B, we calculate the optimal rigid trans-

formation matrix (6 DoF) that transforms (p1, p2, p3) to ( f1, f2, f3).

For the same reasons discussed before, we need a global deterministic rigid regis-

tration method. We used [79] in the the ﬁrst part of our ﬁnal year project, it assumes

correspondence are given and always ﬁnds the optimal rigid transformation matrix

while avoiding any reﬂection. The returned transformation matrix has the following

form:











r11

r21

r31

0

r12

r22

r32

0

r13

r23

r33

0











tx

ty

tz

1

(7.11)

7.4 Problem Reduction

To solve the problem described in Section 7.1, we have to reduce it the the problem

in Section 7.3. In consequence, solving the ﬁrst problem is done by simply solving

the second one. By performing the following reductions:

• B is the labeling object.

• O is the center of the camera that took a 2D image in which and object we

want to label exists.

• (p1, p2, p3) are the points selected by the user on B

• (c1, c2, c3) are the points selected on the 2D image that together with O will

create the lines.

89

• ( f1, f2, f3) are the 3D points we wish to ﬁnd, they represent where B should

be placed (in other words, where (p1, p2, p3) should be transformed to)

We successfully reduce our original problem into one we can solve. In the next sec-

tion we will present a detailed walk-through of how we implemented this solution.

7.5

Implementation in Unity

In this section, we will talk about how this idea is translated into Unity along with

the implementation of the user interface and all of the necessary functionalities.

7.5.1 CAD Objects as Labeling Elements

To be able to apply this method in the tool, the user must acquire the CAD models

of the objects that the user wants to label, knowing that this method is used as a rigid

transformation and the source object must have the same size of the target object.

For that matter a "Run Time Importer" was integrated to the tool from Unity’s assets

store. This package loads an ".obj" ﬁle and creates a game object in the scene that

contains the mesh of this imported 3D model (see Figure 7.4).

Figure 7.4: CAD Object Example in Scene

90

Inventory Panel

To be able to get the most out of this asset to complement the user experience in

the tool, an inventory panel was implemented to encapsulate all the CAD models

that the user imports. In this case the user does not have to import the model each

time he wants to add it to the scene, the user can check the inventory and load the

CAD object from it. Nevertheless, we used Unity’s "Streaming Assets" to store

the objects in it after they gets imported. When a project is built in Unity, most of

the assets are encapsulated into one package except the "Streaming Assets" folder

which is a normal ﬁle system accessible via a path name: “Application.dataPath +

"/StreamingAssets"".

Furthermore, the inventory was created as a panel on the right hand side of the tool

where the user can toggle it on or off. In this panel there’s a "+" button to import the

desired CAD objects into the inventory. Note that if there is an image of the CAD

model in its path, it also get imported and shown in the inventory. Finally, the user

clicks on the image to load the CAD object to the scene (see Figure 7.5).

Figure 7.5: Inventory Panel

91

CAD Objects in Phantom Mode

When the user enters “Phantom Mode", all of the added labeling objects (CAD or

primitive objects) are shown in a preview window in the bottom right corner, where

the user clicks on the object to add the points that needs to be registered using the

Python server. However, the preview window is a camera that renders only objects

that the user wants to apply the transformations to. Moreover, because it is hard to

know the exact size of the CAD objects, so there is a possibility that they wont ﬁt

into the preview window. We added a camera control script for the preview window

camera, and as a result, the user could navigate in the preview window while adding

the source points.

In contrast, when the CAD models are imported into the scene, we made sure that

if they contained sub-meshes, they would be combined into one mesh, hence one

object. This is done to make the transformation straightforward for one object, with-

out thinking about how to transform the child objects in case the point is selected

on that speciﬁc child object; in other terms, after the transformation is applied, the

object will be decomposed.

7.5.2 Point Selection

To activate this mode, the user must toggle on the “2D" text on the down left panel

(see Figure 7.6). As a result, the user can now select points on the CAD object in

the same fashion as the normal mode and the RGB images (top-right side of the

main panel) are now interactive and the user can click on them to select the target

points. In addition, if the mesh isn’t loaded and the user clicks on “Phantom Mode",

the “2D" mode is directly activated.

Note that in this mode, the points correspondence is mandatory, that is why the user

should keep track of the color sequence on the source and target objects and ensure

that the order and placement of the points is convenient.

92

Figure 7.6: “2D" Toggle

Source Point Selection

As already discussed in the “Phantom Mode" chapter on colliders and speciﬁcally

mesh colliders, in this mode we apply the mesh collider on the CAD model to be

able to select the points (spheres) on it because of the collision between the two

game objects. Nevertheless, to perform the problem reduction in our application,

the object shown in Figure 7.7 represents the labeling object B and the points that

are selected on it represent the set (p1, p2, p3) that are ﬁnally added to an array to

be sent to the Python server.

Figure 7.7: Source Point Selection Example

Point Selection on the Image

Furthermore, after selecting the points on the source labeling object, the user can

now click on the image and add the three points on the target object with the same

color correspondence of the source set of points.

First, the image that the user clicks on is the render texture of the shot view camera

that we talked about in the bounding box chapter.

In other terms, it is a scaled

93

version of the camera’s image frame. For that matter, in this application, the origin

of the camera is the origin of the shot view camera.

Second, when the user clicks on the image, they are actually activating a ray to

be generated from the shot view camera to the image frame on the same position

of his cursor. This is done using the “ScreenPointToRay" function in Unity that

is responsible to return a ray object from the camera through a screen point given

as an argument to the function. Nevertheless, the challenge was to calculate the

cursor’s position that should be given to this function. Knowing that the camera’s

raycast in Unity is equivalent to where the mouse position hits, helped us to form

a logic of the workﬂow that should be done to produce the results that we needed.

First of all, we calculated the x and y coordinates of the cursor’s position relative

to the texture of the raw image on which the user clicks on (top-right image of

Figure 7.5). To do that, we used Unity’s “ScreenPointToLocalPointInRectangle"

function that transforms the cursor’s position to the rectangle’s (raw image) local

space. Then, knowing that the resolution of the showing rectangle is different than

the texture’s resolution, we had to ﬁnd the coordinates of the point in the texture’s

coordinate system by multiplying the cursor position by the ratio of the texture’s and

the rectangle’s resolutions, then limiting the value between 0 and the texture’s width

or height. This can simply be translated into the following equation that calculates

the x coordinate of the point to be added on the image frame (same equation for the

Y axis using the height):

Image f rame.x = Localcursor.x

Texture.width
Rectangle.width

(7.12)

7.5.3 Sending the Points

As a result, on each ray hit, a sphere is created on the image frame (that contains

a collider) and then rendered and shown directly onto the raw image on which the

user clicked (see Figure 7.8). These spheres corresponds to the set (c1, c2, c3), they

are added to an array along with the camera’s position (origin).

Finally, the source and target points are sent to the server along with a key that says

"2D" (to notify the server of the transformation to be done) through websockets.

94

Figure 7.8: Selecting Points on Image Example

7.6

Implementation in Python Server

After receiving the two arrays of points from the Unity side, we unpack the target

array and get the 4 points (origin and 3 points selected by the user on the image).

We then basically follow the same steps described in Section 7.3 to build the system

of non linear Equation 7.8. Once that is done, we use the pyneqsys [80] to apply the

DOX_BOX method with the only bound given on the depth value to be larger than

the origin’s depth. We then use the result of DOG_BOX as an initial estimate to

the Newton-Raphson method (also implemented in the pyneqsys library). Then, we

get the target points from the unknowns we calculated. Finally, call the Umeyama

method[79] passing the source and the calculated target points. We will receive the

optimal rigid transformation matrix that transforms the source points onto the target

points. We send that matrix back to the Unity side through the same websockets

connection.

95

7.7 Results

When the server ﬁnishes all the calculations, it sends back a transformation matrix

describing how the labeling object should be translated and rotated. On the Unity

side, this transformation is applied and the results are shown in the following Fig-

ures 7.9 and 7.10. Note that, these results are produced from a data set where the

mesh was not imported. Hence, this translates the use case of this functionality in

the tool in case the data set does not include a mesh.

In contrast, this method has a drawback concerning the user experience in terms of

labeling, because the object can cover the whole image and that prevents the user to

continue labeling other objects behind it, as showing in Figure 7.10. As a solution,

we recommend the user to change the transparency of the labeling object to be able

to see behind it (see Figure 7.11).

Figure 7.9: Results - 1

96

Figure 7.10: Results - 2

Figure 7.11: Results - 3

97

Chapter 8

Labeling Large Point Clouds

8.1 Motivation

We have already described the usual way to create data for the tool. That is using

a camera equipped with a depth sensor and then reconstructing the mesh from the

RGBD images taken by it. However, at BMW Group, the majority of the plants

possess laser scanned point clouds of their plants; see Figure 8.2. These scans are

usually taken by a scanner named FARO. They would like to label objects in these

scans but labeling point clouds directly is still an unexplored ﬁeld, moreover all

of the object recognition models learn on labeled 2D images. Therefore, we need

to create a workﬂow that can take these large pointclouds and transform them into

data that can be labeled by our tool. In other words, (1) extract a triangular mesh

(Section 8.2) and (2) extract RGB images (taken at speciﬁc camera positions) of

that mesh (Section 8.3).

8.2 Surface Reconstruction

A blossoming subﬁeld of computer graphics is perfect candidate to solve our ﬁrst

problem. In essence, it is the process of inferring (reconstructing) a 3D object from

a set of unorganized discrete points that represent its shape. New advancements

in data acquisition are the reason in the spark of interest in surface reconstruction.

From optical laser-based range scanners, LiDAR scanners, structured light scan-

98

ners, multi-view stereo to now inexpensive commodities like the Microsoft Kinect,

point cloud extraction has become more accessible as time passes. Although the

plethora of acquisition methods are great advancements in computer graphics, they

also pose signiﬁcant problems since each scanner produces point clouds with cer-

tain properties and certain imperfections. To cope with this variety, different classes

of surface reconstruction algorithms were created each solving for a certain set of

prior assumptions and robust against a certain type of imperfection. From methods

that make little to no assumption on the quality of the point cloud and output non-

mesh based reconstructions, to methods that assume a well sampled point cloud and

produce a water tight triangular mesh.

In this section, we will present an overview on reconstruction methods comparing

their different characteristics and explaining different prior assumptions they pose

in the inputted point cloud. We will then delve deeper into the algorithms that we

tried and compare their pros and cons. Finally, we will present a workﬂow that takes

an unorganized point cloud and outputs a watertight mesh.

8.2.1 Point Cloud Imperfections

The ﬁve main imperfections that can plague a certain point clouds are: (1) noise,

(2) sampling density, (3) misalignment, (4) outliers and (5) missing data. Refer to

Figure 8.1 for a visual representation.

Noise

Traditionally, points that are spread randomly close to the surface are regarded as

noise. Noise can be introduced in different ways such as depth quantization, sensor

noise and orientation or distance of the surface in relation to the scanner. The now

conventional way of dealing with noise in surface reconstruction algorithms is to

to produce a surface that passes near the points as much as possible but without

over-ﬁtting the noise. [81] that imposes smoothness on the output and [82] which

uses robust statistics are two algorithms very robust agains noise.

99

Figure 8.1: 2D representation of point cloud imperfections

Figure 8.2: Example of Point Cloud received from BMW plants

Sampling Density

Sampling density is one of the most import artifacts for point clouds. It describes

the distribution of the points that sample the surface. It is critical because it deﬁnes a

neighborhood which in turn deﬁnes the local geometry of the surface. The neighbor-

hood should be small enough to capture small details in the mesh, yet large enough

to describe the local geometry. The problem is that usually, scanners produce non-

uniform sampling on the surface. A good practice we adopted is using poisson disk

sampling [83] and keep point samples given a speciﬁc distance between them. Thus

the point cloud becomes uniformly sampled.

100

Misalignment

The biggest cause of misalignment is the ﬂawed registration of range scans. Con-

trarily to what one might think, handling misalignment is not the same as handling

noise. For example, by assuming a prior of Manhattan world [84], a scene consists

of planar primitives aligned by three orthogonal axes, so planar primitives from

mistakenly rotated scans can be robustly "snapped" onto one of these axes.

Outliers

Points that are very far from the surface are outliers. Unlike noise, outliers should

not be taken into account when reconstructing the point cloud. In most cases, out-

liers can be handled implicitly using robust surface reconstruction methods [85],

but in some cases where the outliers are more structured (high density clusters of

points exist far from the surface) like in multi-view stereo acquisition (where view-

dependent specularities can results in false correspondences) it’s better to handle

them explicitly through outlier detection.

Missing Data

Missing data is a motivating factor behind many reconstruction techniques. Miss-

ing data is due to variables such as restricted sensor range, elevated light absorption,

and occlusions in the scanning method that do not sample big parts of the form. De-

spite continuous improvement of the aforementioned artifacts, missing data tends

to persist due to the device’s physical limitations. We notice that missing data dif-

fers from non-uniform sampling, since the sampling density in these areas is zero.

Most methods deal with missing data by assuming that the scanned point cloud is

watertight. Other approaches seek to reconstruct higher-level information such as

shape primitives, canonical regularities, skeleton and symmetry relationships for

signiﬁcant missing data.

101

8.2.2

Input

Different inputs are required by different reconstructions algorithms. The base input

however is the position of each point in 3D space. That may sufﬁce for some simple

reconstructions but most of the time other inputs are added to make reconstruction

simpler, especially for very challenging point clouds. We detail the different possi-

ble inputs in this section.

Surface Normals

Surface normals are deﬁned at every point and represent the direction perpendicular

to the points tangent space. In other words, its the direction perpendicular to the

surface that surrounds a certain point. Normals can be oriented where a normal is

consistently pointing inside or outside the surface, or unorientated in the opposite

case.

Unoriented Normals: When a surface normal is either pointing on the outside or

the inside then the normal is unoriented. According to our research they are used in

one of three ways: (1) Constructing an unsigned distance ﬁeld [86], (2) Projecting a

point onto an approximation of the surface [87] and (3) determining planar regions

in a point cloud [88].

Unoriented Normals can be estimated from a raw point cloud. The best known way

is using principal component analysis of the neighborhood of a point and use the

smallest principal component from the calculated covariance matrix [89]. However,

since it uses the neighborhoods of the points it can be sensitive to imperfections

such as noise and sampling density. As a result, reconstruction methods used after

estimation of these sometimes “noisy" normals should be robust to them.

Oriented Normals: Oriented normals also either point to outside or the inside,

but the difference is when an oriented normal is pointing on the inside we know the

point is on the interior of the surface and when it is pointing to the outside then the

point is on the exterior of the surface. At ﬁrst, it was used to construct a signed

distance ﬁeld over the ambient space, where the ﬁeld either takes a positve value

102

indicating that the point is on the exterior and a negative values in the opposite case.

We can then extract the surface by performing a zero crossing on the calculated

signed distance ﬁeld. More recently, this concept was generalized to implicit ﬁelds

and indicator functions, but the basic idea still holds true [90, 81, 91].

Similarly to unoriented normals, oriented normals can also be estimated. As an ex-

ample in [89], a graph is constructed of the point cloud and weights each edge wi j

for points pi and p j based on the similarity between the respective points’ unori-

ented normals ni and n j as wi j = 1 − |nin j|. A minimal spanning tree is then built,

where upon ﬁxing a normal orientation at a single point serving as the root, nor-

mal orientation is propagated over the tree. Comparatively to unoriented normals,

these methods are sensitive to imperfections in the point clouds and thus can orient

normals in the opposite direction.

Scanner information

The scanner that obtained the point cloud can provide helpful surface reconstruction

data. Its 2D grid design makes it possible to estimate the sampling density that can

be used to identify certain types of outliers–points whose lattice neighbors are at a

much higher range than the sampling density are probable outliers. Caution must

be taken to distinguish outliers from sharp features, however.

Scanner data can also be used to deﬁne a point’s conﬁdence, which is helpful to

manage noise. Some scanners (e.g. LiDAR) provide measurements of conﬁdence

in the form of reﬂectivity measured at each point. One can also derive trust through

information on the line of sight. Line of sight is the collection of line segments be-

tween each point in the point cloud and the position of the scanner head from which

the point was obtained. If the angle between the line of sight and the normal is big

in active scanning systems, i.e. laser-based scanners, this can lead to a noisy depth

estimate, i.e. bad laser peak estimate [92], which implies low conﬁdence.

Note that the line of sight also deﬁnes the regions that are lying outside of the shape.

The combination of line of sight from multiple scans reﬁnes the boundary area in

which the surface is located, this is called the visual hull.

103

RGB Images

Different methods of acquisition that complement the acquisition of depth can be

of excellent help. RGB image acquisition is a very popular method that goes hand

in hand with countless sensors, like the Microsoft Kinect. In the case of the Kinect,

the RGB camera is co-located with the IR camera, so if we assume the two are cal-

ibrated, the identiﬁcation of corresponding depth and RGB values at a pixel level is

straightforward.

RGB images are most helpful for reconstruction in complementing depth informa-

tion that is either not as descriptive as their visual appearance, or simply not mea-

sured by the data. For example, if a color image and 3D scan are at a wide baseline

with very different views, then segmented parts of the image can be used in the

original scan to infer 3D geometry [93].

8.2.3 Priors

Priors’ growth is mainly inﬂuenced by emerging technologies for data acquisition.

Acquisition techniques set expectations for the class of objects that can be obtained

and the type of artifacts connected with the information obtained, also informing the

sort of output generated by reconstruction algorithms and the ﬁdelity of reconstruc-

tion. In this section we will lay out an overview of available priors, discussing their

produced outputs and expected inputs, additionally characterizing these scenarios

by the typical shape classes and procurement techniques. We will end by analyz-

ing and choosing the best prior for our case to narrow our search for the optimal

reconstruction algorithm.

Surface Smoothness

There are three types of surface smoothness priors. First, local smoothness seeks

smoothness in close proximity to the data. The output of these methods are usually

robust to noise and nonuniform sampling but they struggle to preserve ﬁne detail

in the reconstruction [89, 87]. Second, global smoothness strives for large-scale

smoothness, higher order smoothness or both. Large-scale herein relates to the spa-

104

tial scale where smoothness is enforced not just near the input. It is common for

these methods to focus on reconstructing individual objects, producing watertight

surfaces [90, 81]. High order smoothness relates to the variation of differential

properties of the surface: area, tangent plane, curvature, etc. As a consequence, this

conﬁnes the shape class to objects that can be captured as fully as possible from

various perspectives. Desktop scans capable of scanning tiny objects (i.e. 1 inch)

to medium-sized objects (i.e. several feet) are frequently used to create such point

clouds. Ideal devices for such scenarios are laser-based optical triangulation scan-

ners, time-of-ﬂight (TOF) scanners, and IR-based structured lighting scanners.In

addition, due to the close proximity of the sensor to an object and its high resolution

capabilities, the reconstruction of very ﬁne-grained detail is commonly emphasized.

Visibility

The previous visibility makes assumptions about the rebuilt scene’s exterior space,

and how this can provide clues to combat noise, non-uniform sampling, and miss-

ing information. Visibility of scanners is a powerful precedent as discussed in Sec-

tion 8.2.2, as it can provide for an acquisition-dependent noise model and be used

to infer empty space areas [92]. This allows the ﬁltering of powerful, organized

noise for water-tight reconstruction of individual objects [94], a common feature

of multi-view stereo inputs. More recently, some scanners expanded the visibility

prior to scene reconstruction like for example The Microsoft Kinect and the Intel

Real Sense(Already seen in Chapter 3).

Volume Smoothness

Similarly to surface smoothness, volume smoothness imposes smoothness on the

shape’s volume. This prior mainly solves for point clouds with signiﬁcant missing

data [95]. In fact, they assume that the scanners capturing the point cloud have

limited range and mobility and therefore cannot extract a well sampled point cloud.

Other methods extract the skeleton structure of the shape from signiﬁcant missing

data.

105

Primitives

The geometric primitives prior assumes that a compact set of easy geometric forms,

i.e. cylinders, boxes, spheres, planes, etc., can explain the scene’s geometry. In

instances where watertight reconstruction of individual objects is concerned, the

detection of primitives [96] may subsequently be used for primitive reconstruc-

tion extrapolation when confronted with big quantities of missing information [96].

Some examples that beneﬁt from this type of prior are CAD objects(usually mod-

eled through simple geometry shapes) and indoor environments(can be summarized

as a collection of boxes and planes)

GLobal Regularity

Many shapes such as man-made shapes, CAD models and architectural shapes, con-

tain a certain level of regularity in their higher level composition. The global regu-

larity prior takes advantage of this fact combat missing data. Regularity in a shape

can take many forms, such as a building made up of façade components, building

interiors made up of periodic shape arrangements, or a mechanical component made

up of recurrent orientation relationships between sub-parts [97].

Data Driven

The data driven prior takes advantage of the large-scale availability of 3D data ac-

quired or modeled, mainly in scenarios where the input point cloud is highly in-

complete. In these scenarios we are mainly concentrated on the reconstruction of

individual objects, or a collection of objects, as 3D databases tend to be populated

with well-deﬁned classes of semantic objects. For example, we may be concerned

with acquiring a watertight reconstruction of an individual object from just one or

more depth scans. An object database may be used to best match the unﬁnished

point cloud to a full model, or to compose components from various designs [98].

In other cases, the reconstruction of the scene and the use of the database to com-

plement the objects detected in the scene [99] may be implicated. Because of its

generality, the data driven prior’s reconstruction is only as good as the provided

106

data.

User Driven

The user guided prior includes the user into the reconstruction phase of the sur-

face, enabling them to provide intuitive and helpful signals for reconstruction. The

speciﬁc form of interaction is mainly motivated by the type of shape being recon-

structed and how it was obtained. Often the focus is on topology recovery owing

to incomplete sampling from the sensor. Some methods therefore concentrate on

the reconstruction of arbitrary forms, while others acknowledge interactions with

the skeletal model of a shape, relying on volumetric smoothness before guiding

reconstruction. Scanning in outdoor environments can cause large gaps in acquisi-

tion in the reconstruction of architectural buildings, similar to the case of façades

mentioned in Section 8.2.3. User interaction can therefore assist discover global

regularity, as well as how the identiﬁed regularity can be applied to the remainder

of the point cloud [100]. If ﬁne grained reconstruction control is required, the user

may indicate primitive geometry to model the construction, guided by the interac-

tions found in the input [101].

8.2.4 Analysis

Imperfections

The point clouds we receive are captured using FARO laser scanner. After analyz-

ing these point clouds we realized that Missing data, misalignment of scans and

outliers were rare occurrences. On the other hand, noisy data and nonuniform sam-

pling were very prominent. Therefore, we should ﬁrst use poisson disk sampling

to sample the points in a uniform manner as seen in Section 8.2.1. Moreover we

should choose a surface reconstruction algorithm that is very robust against noise.

Point Cloud inputs

As we have previously discussed, the base requirement of any surface reconstruc-

tion algorithm is the coordinates of the points. The more inputs we use the easier it

107

is to deal with imperfections and the better accuracy we will achieve. In the scans

we receive, we are provided with oriented normals which as we have discussed in

Section 8.2.2 provide extremely useful cues for reconstruction algorithms – see [90,

81]. Unfortunately, scanner information and RGB images are not provided.

Priors

To choose the best prior we should use we have to recapitulate the nature of our re-

ceived point clouds. The point clouds we receive are very large scans of the indoors

of a warehouse. It is full of pallets, boxes, robots and dolleys among other objects.

Since we will use this mesh to extract rgb images that we label in the tool and train

the machine learning model with, we require the reconstruction of this point cloud

with the highest possible accuracy. The ﬁnest details should be preserved. Priors

like visibility and primitives are not beneﬁcial since they are mainly robust against

reconstructing single objects like in the case of CAD Models. Data-driven priors

rely on available databases of objects which we don’t poses. User-driven priors re-

quires input from the user that might be useful but for very large meshes it would be

extremely time consuming and difﬁcult for the user to provide good enough feed-

back that would justify the compromise of time. Global Regularity is more focused

on reconstructing objects that contain pattern in them, however again, the size of the

mesh and therefore its diversity will make it nearly impossible to extract good pat-

terns, additionally it usually fails to preserve ﬁne detail. Volume smoothness solves

for the very speciﬁc problem of the abundance of missing data, a problem which

we do not face thanks to the mobility of the FARO laser scanner. Finally, surface

smoothness comes three-fold, local smoothness does not preserve sharp details so

it’s not the best candidate. Lastly, Global smoothness targets large scale reconstruc-

tions and are usually very robust against noise, they require point clouds without

missing data which we possess and reconstruct with emphasis on preserving the

smallest details. Forthwith, we will explore reconstruction algorithms based on the

global smoothness prior and requiring only oriented normals as inputs.

108

8.2.5 Global Surface Smoothness Prior

Radial Basis Functions

RBFs are a well-known interpolation technique for scattered information. Because

of a collection of points with prescribed function values, RBFs reproduce features

that maintain a large degree of smoothness through a linear combination of features

that are radially symmetric. For surface reconstruction, the [90] method builds the

surface by ﬁnding a signed scalar ﬁeld deﬁned by RBFs whose zero level set repre-

sents the surface. According to Table 1 from [102], The aformentioned method is

very weak when dealing with noise therefore we will not develop it further.

Indicator Functions

These methods approach the reconstruction of the surface by estimating a soft la-

beling that discriminates the inside of a solid shape from the outside.

Indicator

function methods are an instance of gradient-domain techniques [103]. Such a

gradient-domain formulation results in robustness to non-uniform sampling, noise,

and outliers and missing information to some extent for surface reconstruction. This

is achieved by ﬁnding an implicit function χ which best reﬂects the indicator func-

tion. In this class of techniques, the main observation is that, assuming a point cloud

with oriented normals, χ can be calculated by making sure that the gradient of the

indicator function measured at the point cloud P is aligned with normals N; see

Figure 8.3 The indicator function thus minimizes the following quadratic energy:

Figure 8.3: Taking the gradient of the indicator function χ reveals its connection
to the point cloud normals. Poisson reconstruction [81] optimizes for an indicator
function whose gradient at P is aligned to N.

109

(cid:90)

argmin
χ

(cid:107)∇χ(x) − N (x)(cid:107)2

2dx

(8.1)

The differential equation that describes the solution to this problem is a Poisson
problem; it can be derived by applying variational calculus as ∆χ = ∇N . Once a

solution of this equation is discovered, a suitable surface-speciﬁc iso-value is cho-

sen as the average (or median) of the indicator function assessed at P. The implicit

function’s gradient being well-constrained at the data points enforces smoothness

and a quality ﬁt for the data, and since the gradient is assigned zero away from the

point cloud, in such regions χ is smooth and well-behaved.

[104]’s strategy solves the Poisson problem by transforming it into the frequency
domain where the Fourier transforms of ∆χ and ∇N result in a straightforward

algebraic shape to obtain the Fourier representation of χ. However, by working in

the frequency domain, a regular grid must be used to apply the FFT, thus restricting

the output’s spatial resolution. For the purpose of scaling to larger resolutions, the

[81] technique solves for χ directly in the spatial domain through a multi-grid strat-

egy, hierarchically resolving χ in a coarse-to-ﬁne resolution way. An extension of

this method for streaming surface reconstruction, where the reconstruction is done

on a subset of the data at a time, has also been proposed [105]. It represents the

wavelet-based indicator function, and calculates the base coefﬁcients efﬁciently us-

ing simple local sums over an adjusted octree.

A known problem with [81]’s strategy is that ﬁtting directly to the χ gradient can

result in data being over-smoothed[ [106], Fig. 4(a)]. To tackle this, the [106] tech-

nique directly utilizes the point cloud’s positional limitations in the optimization ,

leading to the screened Poisson problem:

(cid:90)

argmin
χ

(cid:107)∇χ(x) − N (x)(cid:107)2

2dx + λ ∑
pi∈P

χ 2 (pi)

(8.2)

Setting a big value for λ ensures that the zero-crossing input of χ will be a closer

ﬁt to the input samples P. While this may reduce over-smoothing, it may also

result, similarily to terpolatory methods, to over-ﬁtting. The [107] technique also

incorporates limitations on position and gradient, but also involves a third term, an

110

indicator function constraint on the Hessian:

argmin
χ

∑
pi∈P

(cid:107)∇χ (pi) − ni(cid:107)2

2 + λ1 ∑
p∈P

χ 2 (pi)

+ λ2

(cid:90) (cid:13)

(cid:13)Hχ (x)(cid:13)
2
F dx
(cid:13)

(8.3)

Which may improve surface extrapolation in missing data regions[ KH13, Fig.

6(a)]. The primary distinction between these two methods is that [106] solves the

issue through a multi-grid ﬁnite-element formulation, whereas [107] uses ﬁnite-

differences, due to the complexity of discretizing the Hessian word; in speciﬁc, the

Poisson formulation [106] is up to two orders of magnitude quicker than [107], see[

[106], Table 1].

8.2.6 Comparison

In this section we compare the four most prominent global smoothness surface re-

construction algorithms together: (1) Screened Poisson Reconstruction, (2) Original

Poisson Reconstruction, (3) Wavelet reconstruction and (4) Smooth Signed Dis-

tance reconstruction. For all of these methods we use the recommended parameter

settings as deﬁned by the authors. For both of the Poisson methods, we used the

publicly available code made available by the authors on GitHub [108]. For the SSD

reconstruction, we used an improvement that uses a hash octree data structure based

on Morton codes which is observed to allow much more efﬁcient access to leaf cells

and their neighbors, also available on GitHub [109]. Finally, for the wavelet recon-

struction we modiﬁed the authors original implementation [110] to adapt to the new

speciﬁcations of the OBJ ﬁle format.

Accuracy

To evaluate the accuracy of these methods, we used an assortment of real world data

scanned from laser scanners. These include: (1) The Stanford Bunny (0.2M points),

(2) the Lucy (1.0M points), (3) the David (11M points), (4) the Awakening (10M

points) and (5) the Neptune (2.4M points). We randomly divided the points into

two subsets of the same size for each dataset: input points for the reconstruction

111

algorithms, and validation points for measuring point-to-reconstruction distances.

We visualize the reconstructions of Neptune and David in Figures 8.4 & 8.5 re-

Figure 8.4: Depth 10 reconstructions of Neptune with a close up on the beard using:
original Poisson, Wavelet, SSD and Screened Poisson (left to right)

Figure 8.5: Depth 10 reconstructions of David with a close up on the eye using:
original Poisson, Wavelet, SSD and Screened Poisson (left to right)

spectively. We can observe that the Wavelet reconstruction has apparent derivative

discontinuities. Additionally, the original Poisson and SSD reconstructions are to a

certain extent over-smoothing the resulted mesh. Conversely, the screened Poisson

method offers a reconstruction that ﬁts the samples faithfully without causing any

noise.

Moreover, Figure 4b displays quantitative outcomes across all datasets, measured

using distances from the validation points to the reconstructed surface in the form

of RMS errors. We can deduce that SSD and screened Poisson produce comparable

Figure 8.6: For all models, plots of one-sided RMS errors, measured from the eval-
uation points to the reconstructed surface, as a function of the resolution depth (8,
9, and 10)

results and greatly surpass the other two.

112

Model

Neptune

David

Depth Poisson Wavelet
10
25
89
320
41
108
412
1710

8
9
10
11
8
9
10
11

3
4
6
9
9
12
20
43

SSD
275
547
3302
15441
492
2355
19158
119119

Screened
14
20
44
126
48
73
182
609

Table 8.1: Comparison of runtime in seconds between the four models for depths
8,9,10 and 11 on Neptune and David point clouds

Model

Neptune

David

Depth Poisson Wavelet
113
149
422
1387
427
510
1498
5318

4
11
35
118
11
38
151
545

8
9
10
11
8
9
10
11

SSD Screened
238
455
1247
3495
863
1724
4895
>8192

133
269
604
1622
454
932
2194
6188

Table 8.2: Comparison of memory usage in MB between the four models for depths
8,9,10 and 11 on Neptune and David point clouds

Computational Complexity

To evaluate the computational complexity, we compared the running time and mem-

ory usage of the four algorithms for depths 8, 9, 10 and 11 on the point clouds of

Neptune and David. All of these experiments were run on the same machine pro-

vided in the Lab.

Time Complexity: Table 8.1 compares the four reconstruction algorithms in terms

of time in seconds. We vary the octree depth from 8 to 11 and the results are shown

for both David and Neptune models. Wavelet reconstruction is quick ; its use of

compactly supported, orthogonal baseline functions allows the reconstruction algo-

rithm to calculate the coefﬁcients of the implicit function through integration; never

needing an explicit linear system solution. On the other hand, the others use non-

orthogonal basis functions, thus requiring a global system solution. For the Poisson

algorithms, the multigrid solver performs a constant number of conjugate gradient

113

iterations at each level, giving linear complexity in the number N of octree nodes.

Increasing the depth by one therefore quadruples the time of computation. Whereas

the SSD reconstruction utilizes conjugate gradients to solve simultaneously for all

coefﬁcients, which has an O(N1.5) complexity, resulting in signiﬁcantly slower per-

formance at greater resolutions.

Space Complexity: The memory usage of the various reconstruction algorithms

further highlights the expense of formulating the reconstruction problem in terms

of a linear system solution. Because the Poisson and SSD reconstructions use the

two-ring neighbors to deﬁne a linear system, the scheme matrix can have as many

as 125 entries per row, leading in a signiﬁcant overhead to store the matrix. By

contrast, a linear system does not need to be solved by the Wavelet reconstruction

algorithm and therefore avoids the associated memory overhead.

Algorithm decision

It is evident to rule out SSD as a candidate since its time & memory consuming

and it’s reconstruction results are similar to the screened Poisson reconstruction

algorithm. Moreover, we can rule out the original poisson reconstruction since

the screened version surpassed it in every aspect of our comparison. Finally, even

though the wavelet reconstruction is fast and memory efﬁcient, its reconstruction

results are visually the worst when compared to the others. Therefore, we will solve

this problem using the screened Poisson reconstruction algorithm.

8.2.7 Point Cloud Size

All point clouds we receive for reconstruction are usually huge in size. Containing

around 300 Million points each. This led the screened Poisson reconstruction algo-

rithm to fail due to memory overload. To deal with this, we modiﬁed the algorithms

tree node indexes. In the authors’ implementation, they use integers to keep track

of the tree nodes’ indexes. This limits the number of nodes to 231 = 2, 147, 483, 648

possible nodes. After performing this improvement, the algorithm could ﬁnish suc-

cessfully but the maximum possible octree depth we could reconstruct at was 13.

114

According to the poisson paper, the number of nodes at the maximum depth d

in an octree is 4d. Which means that for a depth of 13, the number of nodes is

413 = 67, 108, 864. It gave good results but if we compare that number to the num-

ber of input points (300 million), we can clearly see that there is room for improve-

ment. We decided to cut up the point cloud into smaller pieces that we reconstruct

independently and once we ﬁnish we stitch them all back together. We know that

if we cut the mesh into 4 pieces then a reconstruction of depth 12 on each piece

would yield the same overall resolution as reconstructing the whole point cloud us-

ing depth 13 (4 ∗ 412 = 413). Following the same logic, if we cut the point cloud into

8 pieces and reconstruct each one at depth 12 then it would be equaivalent to recon-

structing the original at depth 14 which was impossible to use before. Of course,

we could also reconstruct four pieces with depth 13 and that would lead to a depth

14 overall reconstruction. Thus, there are two key parameters to keep in mind: (1)

the depth of the octree when upon reconstruction and (2) the number of pieces we

cut the original point cloud to. We can take as an example one of the point clouds

we received of the Brazil Plant which had 370, 038, 992 points. We can deduce that

this number of points ﬁts between a depth 14 (268,435,456 nodes) and a depth 15

(1,073,741,824 nodes) octree. We need to choose the best compromise between

time and accuracy, as achieving each incremental depth will quadruple the runtime

complexity. If we choose to achieve a reconstruction of depth 14 then we can do it

in multiple ways: (1) reconstructing 4 pieces at depth 13, (2) reconstructing 8 pieces

at depth 12 or (3) reconstructing 12 pieces at depth 11 and so on.. In the next section

we will provide our implemented workﬂow for reconstructing any point cloud and

reaching any desired resolution no matter the machine’s hardware capabilities, of

course, at the expense of time.

8.2.8 Workﬂow

The point clouds we receive are multiple .ptx scans of the same plant, therefore

we have to merge all of the these scans and reconstruct the merged point cloud.

As we have already said, given a certain point cloud we will provide two ways of

reconstructing it.

115

Meshlab

Meshlab [111] is an open source system for processing and editing 3D triangular

meshes and point clouds. It provides a set of tools for editing, merging, converting,

cleaning, healing, inspecting, rendering, texturing and converting meshes. In this

section, we will use its merging and conversion features. Meshlab also installs a

local server which you can communicate with through the command prompt and

execute Meshlab commands without opening the GUI version.

ChunkPLY

To perform the mesh cutting, we use an open source script written in C++ available

on Github [108]. It is a simple script for breaking up geometry looking at which

cell of a regular grid it falls into. For points, the cell is determined by the point’s

position.

Approach 1

1. Merge all .ptx point clouds into one large .ptx point cloud using Meshlab.

2. Convert the large .ptx point cloud into a .ply point cloud using Meshlab.

3. Reconstruct the .ply point cloud using the modiﬁed screened Poisson recon-

struction at an octree depth chosen by the user.

As we have already mentioned, for the large point clouds we are reconstructing, the

maximum octree depth our machine can handle is 13.

Approach 2

1. Merge all .ptx point clouds into one large .ptx point cloud using Meshlab.

2. Convert the large .ptx point cloud into a .ply point cloud using Meshlab.

3. Cut .ply into chunks depending on the users input

4. Reconstruct each chunk using the modiﬁed screened Poisson reconstruction

at an octree depth chosen by the user.

116

5. Merge the reconstructed chunks into one large triangular mesh

In this approach, even though the individual octree depth limit is 13, when they are

combined they can produce even higher resolutions.

8.2.9

Implementation

Each approach is implemented in a separate Python script. Because these ap-

proaches heavily rely on external application and scripts we decided to create a

containerized environment that already has all these required applications installed.

Docker

Deﬁnition: As per Docker’s own deﬁnition [112]: Docker is “an open source

project to pack, ship, and run any application as a lightweight container.” In lay-

man terms a Docker “allows you to package your application along with all of its

dependencies and conﬁgurations, making sure that the application can run on any

infrastructure with almost no conﬁguration changes on the customer premises”.

Figure 8.7: Difference between virtual machine and docker architectures

Docker Container Vs Virtual-machine:

117

• Footprint: VMs are inherently heavyweights. They need to run a complete

OS to be able to run a packaged application. This is needed because the

system calls made by the Apps are made to the underlying Guest OS. The

Guest OS sends the system calls to the Host OS, via Hypervisor, and then

relays the return value of the call back to the App. In the case of containers,

the Docker engine does not need a Host OS. All System calls are intercepted

by the Docker Engine and are relayed back to the Host OS. Hence, the Docker

containers are extremely lightweight.

• Resources: VMs should be allocated a deﬁned amount of resources, which

cannot be shared between multiple VMs. If you share X amount of RAM to a

particular VM, then this X amount of RAM would be dedicatedly allocated to

the VM. In the case of containers though, they utilize the resources as per the

need. If a container is running a very lightweight application, it will utilize

just the right amount of RAM

• Automation: It is possible to create Docker containers on the ﬂy by writing a

couple of lines of conﬁguration.

• Instantiation: Instantiating a VM instance is a time taking process, sometimes

taking tens of minutes. But Docker containers can be started within seconds.

• Collaboration: It is very easy to share your Docker images (and containers)

with other users. A Docker provides you with Registries which can store and

share your images, publicly or privately. VMs are not that easy to share.

Docker Architecture:

• Docker Images: A Docker image is a read-only template with instructions for

creating a Docker container. For example, an image might contain an Ubuntu

operating system with an Apache web server and your web application in-

stalled. You can build or update images from scratch or download and use

images created by others. An image may be based on or may extend, one or

more of the other images. A Docker image is described in a text ﬁle known

as a Docker ﬁle, which has a simple, well-deﬁned syntax.

118

• Docker Container: A Docker container is a runnable instance of a Docker

image. You can run, start, stop, move, or delete a container using the Docker

API or CLI commands. When you run a container, you can provide conﬁg-

uration metadata such as networking information or environment variables.

Each container is an isolated and secure application platform but can be given

access to resources running on a different host or container, as well as persis-

tent storages or databases.

Creating Docker Image

First, we started with a pre-built docker image that runs Ubuntu and has Meshlab

and its server installed. For ChunkPLY we clone the Github Repository where

its code exists and compile it using the g++ compiler of Linux. For the modiﬁed

screened Poisson reconstruction, we forked the authors’ original repository, made

the modiﬁcations and use our own forked repository in the docker. We also compile

its code using g++. Finally, we install Python 3.7 and all of the required libraries to

run our script.

Python Script

To run the approaches, we create two separate scripts. We will ﬁrst talk about what

is common between the two scripts. Both scripts take multiple arguments:

• data path: the path where the multiple ptx scans lie.

• output path: the path where the reconstructed mesh will be created.

• octree depth: the upper bound octree depth used for reconstruction (max is

13)

The difference is that the second script also takes the number of chunks required by

the user. The scripts execute both approaches in the order described in the previous

section. To execute Meshlab, ChunkPLY and the screened Poisson reconstruction

we call Linux terminal commands from the python script using the library subpro-

cess.

119

Until now the way we would run reconstructions is we launch a container from our

image that has access to the folder from the Host OS that contains the ptx point

clouds. We then run the python script we want providing our desired arguments and

destroy the container upon reconstruction completion. In the next section, we will

automate the step of creating and destroying the container using yet another Python

script.

Automation

Our ﬁnal level of abstraction is creating a Python script that starts the docker con-

tainer that has access to the ptx point clouds and the two scripts, runs the desired

script (desired approach) and kills the container upon completion.

This is easily achievable by using the Python library docker. It allows to automate

any docker commands in Python. Naturally, the user has to give the arguments they

wish to execute in the docker scripts to this script. This script will then pass these

arguments to the scripts inside the docker. Please see Figure 8.8 for a visualization

of the full process and Figure 8.9 for a reconstruction results of the example layed

out in Figure 8.2.

Figure 8.8: Top Level View of Surface Reconstruction Workﬂow

120

Figure 8.9: Example of a Reconstructed Mesh of the Point Cloud in Figure 8.2

8.3 Data Sampler

When the surface reconstruction is done and we have a fully constructed mesh,

we can feed it to the data sampler tool that we have developed for the purpose of

generating the IRIS format data set that will ﬁnally be imported into the 3D Labeling

tool.

The data sampler tool is built also in Unity3D. The idea behind it is to import a

mesh and then the user can start recording what the camera is seeing and generating

rgb and depth images while going through the mesh. In other words, it is a mimic

of how the data for the 3D labeling tool is usually collected in real environments.

8.3.1 Recording and Generating the RGBD Data

When the mesh is imported to the data sampler tool, the user can navigate around

it using the arrow keys and the mouse that basically changes the positions and ro-

tations of the main camera. Before doing so, to start recording and capturing the

RGB images from the mesh, they click on the start/stop recording button (see Fig-

ure 8.10). When the recording is on, the camera’s transform (position and rotation)

is saved on each frame (fps can be set by the user) in a list.

When the user clicks on the generate datasets button, a screenshot is taken on each

camera position that has been recorded. However, to capture what the camera is

seeing we had to add a target render texture on the camera with a resolution that is

set by the user (The top left values in Figure 8.10). Then, using Unity’s ReadPixels

121

function, everything that is read by the main screen is projected into the texture that

is ﬁnally an RGB image stored on the disk to form the ﬁrst part of the IRIS format.

In addition, for the depth images, we used Unity’s depth shader script and applied

a post processing effect on the rendered texture to obtain a depth map which is a

depth image stored in IRIS format. Note that taking the depth image is done with

the same method as of the RGB images but using another camera that is created as

a child of the main camera to preserve the main camera’s position and rotation for

the purpose of having for the same RGB image the corresponding depth image.

Figure 8.10: Data Sampler UI Example

8.3.2 Generating the Camera Information

To complete the IRIS format creation, we still need the camera’s extrinsic list that

deﬁnes all the positions of the camera in the world and the direction that is pointing

at. Along with the camera’s extrinsic we also need the intrinsic that represent the

camera’s calibration and how the 3D camera coordinates are transformed to a 2D

image coordinates.

First, the camera extrinsic are the positions stored when the user is recording.

Hence, using these positions and transforming them into world space with Unity’s

transform.localToWorldMatrix function, we obtain a 4x4 transformation matrix that

122

is added to a list which is ﬁnally JSON serialized and stored in the “extrinsic.json"

ﬁle of the IRIS format.

Second, to generate the camera’s calibration we had to calculate the Unity camera’s

focal length. Knowing the camera’s ﬁeld of view, we used the following equation

to extract the focal length:

f ocallength =

resolution.height
2 ∗ tan( f ieldo f view ∗ (π/360))

(8.4)

In addition to the focal length, we added to the intrinsic matrix the resolution’s width

and height divided by 2. Finally, we got the following intrinsic matrix serialized into

JSON and written on a ﬁle called “intrinsic.json" in the IRIS format directory:








f ocallength

0

width/2

0

0

f ocallength height/2

0

1








Figure 8.11: Sampler Generated Data Imported in 3DLT

In summary, the generated data set from this tool is stored as an IRIS format which

later on can be imported to the 3D labeling tool (see Figure 8.11). That, gives the

user the ability to label any data set, whether it is only coming from a large point

cloud or a standard IRIS format data set.

123

Chapter 9

Snapping

9.1

Introduction

In the previous chapters we showed the various features that the user can beneﬁt

from to ensure an intuitive labeling experience. However, in some cases, the place-

ment of the labeling elements on the mesh in the right position and rotation can be

time consuming.

As a result, we introduced a new feature that snaps the labeling element in its con-

venient place if it is more or less close to it and the labeling object has the same size

as the object to be labeled.

This functionality is based on the concept of registering the labeling element onto

its correct place in the mesh. Before applying the registration algorithm, two depth

images should be sent to the server along with the tool’s main camera intrinsic and

extrinsic. We will begin by examining how the depth images are captured and what

do they represent. Then, we will present the server side of this feature and ﬁnally

we will highlight the results of this enhancement.

9.2 Data Pre-Processing

The snapping is launched when the user selects a certain labeling element in the

scene and clicks on the button "S" on the keyboard. Before the snapping is applied,

the data is prepared in the Unity side by forming a message that contains two en-

124

coded depth images and the main camera’s intrinsic and extrinsic. This message

is sent to the server through websockets to ensure the corresponding computations

and to produce a rigid transformation that is applied on the labeling element.

9.2.1 Depth Camera Setup

First, a camera is added as a child of the main camera’s game object to ensure that

both have the same transformation. This camera is going to play the role of a depth

camera that takes a screenshot of what it is rendering the moment the snapping is

applied (Same concept as mentioned in Chapter 8). Moreover, the depth property

is given to this camera by enabling the “DepthTextureMode.Depth" that generates

a depth texture. To calculate the depth map in values and encoding it into an RGBA

format, we used the “Shader" language in Unity and applied it as a post rendering

effect on the camera. This is done using Unity’s OnRenderImage and Graphics.Blit

that is given the source and destination textures along with the material that will

affect the destination texture. It is the texture to encode and send to the server.

Furthermore, in the shader calculations, the depth is received from the Depth Tex-

ture of the camera that is already enabled. However, the depth value returned from

this texture is not linear; therefore, we used the Linear01Depth function to set the

depth value between 0 and 1, in the case where 1 is the maximum value correspond-

ing to the far clipping plane that is set to 65 meters in our case. Note that the far

clipping plane is the farthest point that the camera can render.

Finally, the linear depth value is encoded on 4 channels R, G, B and A on a scale

of [0, 1] in the “Shader" script. Knowing that there’s 256 color in each channel, we

multiply the depth value by 256i (i ranging between 1 and 4, representing the val-

ues of R, G, B and A) while limiting its value to 256 using modulo. In addition, to

enclose the values between the range [0, 1] we divide each one by 255. As a result,

we obtain a vector of 4 values (RGBA) representing the encoded depth value.

125

9.2.2 Capturing the Depth Images

To apply the “Shader" script mentioned above, we have to attach it to a material on

which the camera should render the texture to ensure the post processing effect and

generate the depth map. To differ between the labeling element and the mesh, we

assigned a culling layer for each one, then we added a culling mask to the depth

camera for each layer. As a result, the camera captures one image that contains

the labeling element and then changes its culling mask and captures another image

containing only the mesh.

The images in Unity are 2D textures where we speciﬁed the resolution to 256 × 144

for the purpose of decreasing the number of pixels and by that optimizing the time

complexity of the registration algorithm. Finally, to capture the images, we used

the ReadPixels function in Unity that reads the pixels from the screen into the saved

texture with the corresponding resolution (see Figure 9.1). Then, we encode the

following texture into an array of bytes using Unity’s EncodeToPNG function. Note

that, this is done twice, once for the image that contains only the labeling element

and the other for the image that contains the mesh; however, when we encode to

PNG, the RGBA values are then transformed from the range [0, 1] to a range [0, 255].

Figure 9.1: Depth Images Example

9.2.3 Generating the Camera Intrinsic and Extrinsic

As part of the data preparation, we generated the depth camera’s intrinsic and extrin-

sic in the same way we did in Chapter 8 in which we formed the camera’s extrinsic

126

using Unity’s transform.localToWorldMatrix property that returns a transformation

matrix in the world space of the game object. On the other hand, the intrinsic ma-

trix was constructed after calculating the camera’s focal length and the texture’s

resolution.

9.2.4 Sending the Data

In order to send the data to the Python server, we created a class that contains

two byte arrays one for each image and two matrices for the camera’s intrinsic and

extrinsic. This class is serialized into a JSON message and sent to the server through

websockets.

9.3 Server Side

The Python server receives a total of four inputs: (1) Encoded depth image contain-

ing the labeling object, (2) Encoded depth image containing the mesh, (3) Intrinsics

of the camera and (4) Extrinsics of the camera. In this section we will describe how

we calculate the optimal transformation matrix that transforms the labeling object

onto its correct spot on the mesh.

9.3.1 Extracting depth values

Decoding the received images is simply performing the reverse steps that were used

to encode them on the Unity side. First, to extract the RGBA values from the byte

array sent from Unity, we use the Python library Pillow’s Open function. Evidently,

we will get a 3-dimensional array of shape heightxwidthx4. In other words, it is

essentially a heightxwidth matrix with each entry being a 4 element vector repre-

senting the four values red, green, blue and alpha on a scale of [0, 255]. We divide

all of the values by 255 reducing the range to [0,1]. Second, we perform the inner

product of each entry with the vector [1, 1

1
2563 ] which will result in the z
value of each pixel on a scale from 0 to 1. Third, we multiply this matrix by 65 (be-

1
2562 ,

256,

cause we know the max depth that the Unity camera can calculate is 65) to receive

127

the actual z value in world coordinates. Finally, we construct a (width ∗ height)X3

matrix in which the ﬁrst two columns are the (u, v) coordinates of each pixel on the

image starting from the top left and row by row, see Figure 9.2. The third column

is the depth(z in meters) value of each pixel in world coordinates, see Figure 9.3.

Figure 9.2: u,v image coordinate axes

Figure 9.3: Resulting matrix containing three columns, [u, v, z]

9.3.2 Calculating World Space Coordinates

Since we know the depth coordinate of each pixel, we can use single view geometry

to calculate each pixels’ position in world space. The projection matrix, formed by

multiplying the intrinsics by the extrinsics; see Equation 5.3, is used to transform

128

points in world space into points in image space; see Equation 5.4. What we require

is the opposite of the latter equation, we need to ﬁnd the point’s coordinates in world

space knowing both it’s coordinates in image space and its depth in world space. By

left multiplying equation 5.4 by the inverse of P we will get the following equation:

X = P−1x

(9.1)

We apply this equation on all of the points in the matrix we calculated in section

9.3.1, then we will get the world coordinates of each pixel in the original images.

9.3.3 Reducing Mesh Points

Since the mesh will contain a lot more pixels than the labeling object, we should

remove points that are very far from the object so as to increase the accuracy and

speed of the registration algorithm. To do this, we ﬁrst calculate the minimum and

maximum coordinate of the labeling object on each axis. That would leave us with

6 values (3 minimums and 3 maximums). Each pair will serve as a bound on a

certain axis. Because the labeling object will be in an awkward position, the bounds

we choose should not be very strict. Therefore, we add/subtract a certain offset

from the maximums/minimums. Choosing the correct offset value for all cases is

impossible, with our testing we found that 0.15 (15 cm) yielded the best results. We

then simply remove any points in the mesh that lie outside the bounds we created

on each axis.

9.3.4 Registration

We have already covered registration extensively in a previous chapter. Our needs

in this problem are different from the previous one though. In this problem there is

a lot of outliers and noise. As we have previously said, the registration algorithms

that are the most robust against these two artifacts are the stochastic registration

methods. We have reviewed 4 state of the art stochastic methods.

129

Choosing the Best Algorithm

Even though it would seem like CPPSR [63] would be the obvious choice since

it leverages the possibility of using the pixel’s colors to stregthen registration ac-

curacy. Unfortunately, the reconstructions usually lose color accuracy while CAD

objects retain a very accurate real world estimate of the colors. Henceforth, we in

our case colors would be a detriment rather than a beneﬁt. Additionally, we will not

use IPDA [62] since its main focus is registering a dense point cloud to a sparse one.

However, since we are capturing the same resolution from the same camera, then

the pixels will be uniformly distributed on both point sets and hence both would

have the same denseness. Although VBPSM [61] achieves promising results, it’s

cubic time complexity prevents it from being used in any real time application. Fi-

nally, SVR [64] presents itself as the best candidate as it is the best compromise

between registration accuracy and computational complexity(O(mn)/where m and

n are the number of components in GX and GY respectively).

Modiﬁcation

We have used a modiﬁed version of SVR in which we use fast Gauss Transform to

approximate the trained SVMs to a Gaussian Mixture Model instead of the one im-

plemented by the authors, thus reducing the computational complexity to O(n+m).

Output

We provide the algorithm with the labeling object points as the source point set and

the reduced mesh points as the target point set. As for the algorithm’s hyperparame-

ters, according to the authors changing them will barely affect the registration result

and therefore we left the ones the authors used in their paper. Naturally, the algo-

rithm will output the optimal rigid transformation matrix (containing only rotation

and translation) that transforms the labeling object onto the mesh. This transforma-

tion matrix is sent to Unity via the same websockets connection to be applied on the

labeling object.

130

9.4 Results

On the unity side, a transformation matrix is received that deﬁnes the translation

and rotation that will be applied on the labeling element to ensure the snapping into

the correct position on the mesh. However, if we apply the transformation on the

labeling object directly it will lead to wrong results because of the difference in the

coordinate system. In other words, the center of rotation will be the center of the

labeling element and not the center of the world. The transformation that should be

applied is relative to the world space, to ﬁx this issue we added an empty game ob-

ject on the zero position (Unity’s world space position) and made the labeling object

a child of the created game object. Therefore, when applying the transformation on

the new center of rotation, the labeling element will move accordingly to its parent

(see Figure 9.4).

Figure 9.4: Snapping Example

131

Chapter 10

Labeling in Virtual Reality

10.1

Introduction

In this chapter we introduce the concept of labeling in virtual reality as an added

feature to the tool. The idea is to place the user in front of the mesh and allow

labeling using VR controllers. This feature provides an immersive experience and

exploits the 3D aspect of the tool to its maximum limits.

10.2 First Integration into Unity

The ﬁrst objective of this integration was to install the SteamVR module and add

the OpenVR SDK to Unity. SteamVR is the software that will pair the VR headset

and the controllers to the user’s machine. Hence, the user must setup his platform

and download SteamVR as a prerequisite for the application to run in virtual reality

mode. Note that during the development of this feature we used the HTC Vive Pro

hardware package (see Figure 10.1).

10.2.1 Teleportation in VR

To start developing the VR application, we imported demo scenes from the SteamVR

asset that got us familiar with the capabilities of the virtual reality features in Unity.

One of these features is the teleportation that allows the user to change its position

132

Figure 10.1: HTC Vive Pro

in the scene by clicking the corresponding button. It is represented by a line and a

target spot on the ﬂoor that indicates the ﬁnal position of the user (see Figure 10.2).

This functionality is useful because the headset is linked using a cable which limits

the movement of the user.

Figure 10.2: Teleportation Example

10.2.2 Switching from 2D to VR

When the user clicks on the “VR" button in the main window of the tool, all of the

2D user interface panels are switched off and a ﬂoor (plane) is added on the origin

of the world (it is the grid shown in Figure 10.2). However, if the user does not have

the “SteamVR" module then the virtual reality functionality is not activated.

10.2.3 Basic Object Manipulations Using SteamVR

After setting up the mesh and the VR environment, we started to test simple object

creation, deletion and manipulation that are included in the SteamVR library.

133

Simple Object Creation

This simple object creation is based on adding a cube in the middle of both con-

trollers when the user clicks on their trigger button (see Figure 10.3 for the HTC

VIVE controllers’ key bindings terminology). Knowing the controllers’ distance

to each other in the scene, we calculated the center and added the object in that

position.

Figure 10.3: HTC VIVE Controllers

Grabbing Objects

SteamVR library provides also a useful functionality that could be added to any

game object so it can be grabable whenever the controller and the object are col-

liding and the ‘Grab" button is clicked. The grabbed object becomes a child of the

controller’s game object and it will move relatively to the parent game object (see

Figure 10.4). This is an essential feature in the use case of this tool because the

user can now create the 3D object, grab it and place it on the mesh to label a certain

object.

10.3 Collaboration with Logitech

During the development of this feature in the tool, Logitech contacted our supervisor

at BMW Group and suggested to collaborate with them in the virtual reality part of

134

Figure 10.4: Grabbed Object Example

the tool. They were launching a new product called VR Ink stylus pen and they were

willing to integrate it in the 3D labeling tool.

10.3.1 Logitech’s Stylus Pen

This pen is an input device for VR that allows the creation of controlled lines and

drawing on virtual and physical surfaces. It has the same key bindings as the normal

HTC Vive controller but with an added analog tip that could be used with the analog

button. For example, it could be used to adjust the thickness of a line when drawing

(see Figure 10.5).

Figure 10.5: Logitech Stylus Pen

135

10.3.2 Integration into the 3D Labeling Tool

With the help of Logitech’s team, we managed to integrate the corresponding game

objects in our project that are responsible to enable the camera (The HTC VIVE

headset view) along with the model of both controllers (one of them is the HTC

VIVE controller and the other one is a model of the stylus as showing in Fig-

ure 10.6). They also developed some functionalities for cube creations that we later

on upgraded to a 3D aspect that can be useful for the tool (Section 10.4.2).

Nevertheless, the user does not have to acquire this stylus pen to be able to use the

VR option in the tool. The key bindings and the pairing mechanism is the same as

a normal HTC VIVE controller.

Figure 10.6: Stylus And HTC Models in VR

10.4 Added Features

In this section we are going to discuss in details the added features to Logitech’s

already implemented API that can beneﬁt the user experience of the labeling process

in VR.

136

10.4.1 User Interface

The user has to continuously check if the labeling objects that they are placing on

the mesh are well projected to the images. Therefore, we added a user interface

that consists of a series of panels that pops up on the non-dominant controller (see

Figure 10.7). To implement this canvas, we have added it as a child game object

on the controller after aligning it in a way that is convenient for the user (size and

location).

Moreover, this canvas includes the RGB images to be labeled along with the slider

ﬁlter to go through the images, the CAD model inventory and a settings menu to en-

able or disable world scale (Section 10.4.4). In addition, for the user to enable this

panel, they have to click on the "Menu Button" on the non-dominant hand, which is

also the same button to switch between menus.

Logitech already implemented a raycaster program that generates a ray from the

pen to a UI Element in the scene. Thus, it can be used to trigger the buttons or

the images from a distance without having the need to make the pen touch the user

interface elements.

Figure 10.7: VR User Interface

10.4.2 Labeling Objects Creation

We gave the user 3 options to create a 3D object in the world:

137

Drawing a Cuboid

For the user to draw a cuboid using the tracker, they have to choose 2 points and

draw a line, then drag the controller to create a rectangle and ﬁnally drag again to

add depth for the rectangle so it becomes a cuboid (see Figure 10.9).

The idea behind this process is to project the position of the controller on the planes

created by the ﬁrst two points and their normal vectors passing by these points. As a

result of these 2 projections, we obtain 2 points that will ﬁnish drawing the rectangle

as shown in the second photo in Figure 10.9.

Moreover, adding the depth was based on the same idea of projecting to a plane but

we had to do it twice. For example, in Figure 10.8 we have the rectangle already

drawn and we want to add the depth based on the trackers position which is the

point E. First, we project E to the plane formed by A and the vector AB, then the

result of this projection is then projected to the plane formed by B and the vector

BC. To complete the drawing, this process is done 4 times for the remaining planes

and points.

To create the labeling object we used the ﬁrst 3 points calculated using the projection

to the plane and a fourth point that represents the depth of the cuboid. Furthermore,

using the vectors composed by these 4 points, we managed to calculate the position,

rotation and scale of the cuboid.

In conclusion, this method enhances the user experience by giving the user the

opportunity to draw a custom cuboid that suits the object that they want to label in

the mesh.

Figure 10.8: Plane Projection to Add Depth Example

138

Figure 10.9: Drawing a Cuboid Example

Convex Hull to Cuboid

In this method, the user presses a button on the controller and starts painting a line

as if they were scratching on the mesh the object to be labeled. As a result, they

obtain the minimum box that envelopes the created line (see Figure 10.10).

This feature is based on the concept of Convex Hull which is an approach in mathe-

matics used to envelope a set of points in the Euclidean space in the smallest convex

shape (see Figure 10.11). To implement it in Unity we used [113] that already has

implemented the method to produce a convex hull game object from an array of

points.

For the user experience to be intuitive we decided to let the user draw a line and not

place points in the scene. We used the Line Renderer Unity class to show the line on

the screen yet extract the points from it and assign it to the convex hull algorithm.

To transform the convex shape obtained from the previous step to a cuboid, we

decided to take advantage of Unity’s box collider properties. When this collider

is assigned to a game object, it automatically creates the minimum bounding box

around it. Therefore, the next step was to add the box collider on the convex hull

object and retrieve its vertices that will help produce the cuboid using the “Mesh"

class in unity.

Finally, after creating the box and rendering it to the scene (using the Mesh Renderer

component), we made sure that it has the same transformation as the box collider

139

that encapsulates the convex hull. Then, we delete the convex hull game object and

keep the box as a labeling element.

Figure 10.10: Convex Hull to Cuboid Example

Figure 10.11: Convex Hull Example

CAD Model Importer

This feature is based on the same one that is used in the 2D mode of the tool.

However, we added a CAD model inventory panel in the user interface part of this

mode to give the user the option to load a CAD object into the environment when

labeling in VR (see Figure 10.12).

10.4.3 Objects Manipulation

After creating the labeling element, the user wants to manipulate it: change its

position, copy, delete and scale. As already mentioned, Logitech uses the same

grabbing mechanism as the one of SteamVR which is responsible for changing the

position and rotation of the labeling element in relative to the movement of the

140

Figure 10.12: Cad Objects in VR

dominant controller.

To copy the labeling element, we duplicate the object that the controller is colliding

with by instantiating the collided game object.

In addition, to delete a labeling

element, the user must be pointing or colliding with the object and press the Trigger

button. Note that when deleting the object it does not mean to only destroy the game

object but also we had to remove it from the list that contains all the other labeling

objects.

Furthermore, we added the feature of grabbing the object from afar using a raycaster

program that is issued from the dominant hand controller in case it is pointing at a

labeling element. This implementation caused some issues when trying to use it

with the user interface raycaster program (see Section 10.4.1). Hence, to solve that

problem, we managed to synchronize both of the raycaster programs using boolean

variables and giving the priority for the user interface raycaster program in case

there’s an object behind the panel.

Finally, the user has the ability scale the labeling element if they are colliding with

it and pressing on the non dominant hand trigger button. The object will be scaled

along the distance between the two controllers. In other words, the idea behind this

type of scaling is to transform the controllers game object along with the labeling

object to the camera’s coordinate system. Then, we calculate the difference (Vector)

between both of the controllers and transform it to the object’s coordinate system.

This allows the user to scale the object on its axis regardless of its rotation relative

to the user’s view. In addition, to be able to determine the axis on which the object

has to scale, we calculate the dominant axis; in other terms, we check the position

141

of the controllers and see if it is dominant on the x, y or z axis. For example: if

the user’s controllers are on top of each other than the dominant axis is the y axis

and the object should be scaled on that axis while taking into account the distance

between the controllers.

Figure 10.13: World Scale Setting in UI

10.4.4 World Scale

The last functionality that we implemented in VR was the ability to scale the entire

scene/world. This came as a solution for an issue that we faced when we were trying

to label meshes that are too high or too large for the user to physically reach.

This solution consists of toggling the World Scale setting in the user interface panel

( 10.13), which adds an empty game object that parents all the labeling objects

available along with the mesh. Therefore, when the Grab Button is triggered on the

controllers, the World game object is transformed along them. Then, the mesh and

the labeling objects are transformed accordingly. Note that the scaling in this case

is uniform and it differs from the scaling that was discussed in Section 10.4.3. In

addition, when the world scale option is enabled, we made sure that the copy and

object creation is done according to the world game object. In other words, these

objects are added directly as children of the world object. Nevertheless, this will not

disrupt the labeling process, because when the world scale option is off or the user

turns off VR mode and gets back to the 2D mode, all of the features of that option

are reverted and the labeling objects will preserve their alignment with respect to

the mesh.

142

Chapter 11

Experimental Results

In the ﬁnal phase of this project, we conducted a series of experiments to test the

run time efﬁciency of the tool, compared to the other state of the art labeling tools

presented in Chapter 2.

These tests were performed on Windows 10 Home Edition running on a personal

computer hosting an NVIDIA GTX 1060 GPU, 16GB of RAM and a 6 cores 2.2

GHz i7 CPU.

The experiments were performed in two different ways. First, we compare the

labeling tools (see Chapter 2) on three distinct data sets: (1) shots taken by a robotic

arm of a box containing door handles (92 images), (2) shots of a kitchen (98 images)

and (3) images of pallets, trollies and boxes in a factory (184 images). Second,

we compared the methods based on the number of objects to be labeled in a data

set. Note that the timer starts when the labeling process begins and stops when the

annotations are exported.

11.1 Experiment 1

The ﬁrst chosen labeling tool is an ofﬂine application called LabelImg [6].

It

achieves 2D labeling by drawing a rectangle on each imported image and gener-

ates the annotations in XML format. As shown in Figure 11.1, the data set that

contains 92 images took more time to label than the one with 98 images. This is

correlated with the position of the object in the images when drawing the bounding

143

box around it (object on the image’s edges or behind other objects). Also, when the

number of images increased to 184, the time to ﬁnish the labeling almost doubled

from 12.25 to 22.08 minutes.

The second tool that was chosen is the LabelBox [7] web application on which the

2D labeling process is done in the same fashion as LabelImg but online rather than

on premise. Figure 11.1 displays how this method struggles in run time comparing

to the previously mentioned tool. It is caused by the delays in the server’s response

time when the annotations are drawn on the image. Therefore, this method is not

reliable because its performance depends on the user’s activity and the network’s

congestion when annotating the images.

The third and ﬁnal tool that was mentioned in Chapter 2 is the Semi-Auto Anno-

tation Tool [8] that basically enhances the idea of the previous 2D labeling tools.

It adds the ability to detect the object before drawing the 2D bounding box on it

accordingly. However, as shown in Figure 11.1, this method only shows its best

performance on the kitchen data set because it is already trained to detect a fridge

which is present in this data set. As a result, the fridge in each image is detected

and annotated directly by a bounding box, which made the labeling faster (3.26

minutes). However, the other two data sets do not hold any object that is present

in the pre-trained model of this tool, hence resolving to approximately the same

run-time as the previous tools by drawing the rectangle on the objects manually for

each image.

144

Figure 11.1: Comparison on Different Data Sets

11.2 Experiment 2

This experiment is performed on the robot arm data set in which we compared the

3DLT with the LabelImg tool. As you can see in Figure 11.2, the same data set

is imported 3 times, where we increased the number of objects to be labeled in

an image at each distinct import. The 2D labeling tool’s time to label showed an

exponential increase in time when expanding the number of objects because the

user has to draw a 2D bounding box on each one in every image.

145

Figure 11.2: Comparison on Different Number of Objects

11.3 Discussion

When comparing any labeling tool mentioned in Chapter 2 to our tool, we notice

that in the ﬁrst experiment, the time taken to label any of the three different data sets

in the 3DLT is the same constant time for each data set, and is highly faster than any

of the others (see Figure 11.1) in each of the tests. It is mainly because the act of

placing one object on the mesh is not affected by the number of images in the data

set. In contrast, the run time of the other tools depends on the number of images

because the user annotates each image to generate the ground truth labels.

In the second experiment, the act of placing multiple objects in our 3D labeling

tool and generating the annotations is almost 34 times faster than a 2D annotation

software (see Figure 11.2). As a result, this shows how time consuming the 2D

labeling process is, and how the 3D Labeling tool that we developed is much more

efﬁcient for any data set or any type of objects to be labeled.

146

Chapter 12

Conclusion

In this report we introduced a novel labeling tool, the 3D Labeling Tool (3DLT), that

greatly reduces time spent on preparing training data for object detection machine

learning models. The tool’s user interface is easy to use and intuitive. It is a cross

platform tool that can run on Windows, MacOS and Linux that has a client-server

architecture with a Unity Client and a Python server.

We used the recorded RGBD images in ORB-SLAM to compute the camera trajec-

tory and in Open3D to extract a 3D triangular mesh. We also deﬁned a special data

format to standardize our data preparation process, the IRIS format. Moreover, we

showed how any raw point cloud can be labeled using 3DLT. We compared many

state of the art surface reconstruction techniques and selected the Screened Poisson

Reconstruction. We surpassed hardware limitations set on reconstruction resolution

by creating a workﬂow that splits the point cloud into multiple pieces, reconstructs

each piece separately and merges them all back together. We containerized the

aforementioned reconstruction workﬂow in Docker to make it portable and devoid

of requirements. The reconstructed mesh can then be imported into a Unity appli-

cation that we developed, Surface Sampler (SS). This application allows the user to

sample data from the mesh by navigating around it and taking RGBD images. The

collected data is exported into the IRIS format and can therefore be loaded in 3DLT.

To calculate the 2D bounding rectangle, we projected the 3D labeling object to its

equivalent position on the 2D images using the camera’s projection matrices. We

then used Python to calculate the smallest possible axis-aligned bounding rectangle

147

that contains the projected labeling object. In the cases where a certain part of the

labeling object was blocked by another, we carefully selected the unsupervised out-

lier detection algorithm, rPCA, to allow for the computation of smarter bounding

rectangles. This algorithm has linear time complexity allowing it to preserve the

tool’s “real-time" property.

Since manually deﬁning the position of a labeling object in 3D is non trivial, we

simpliﬁed the process by allowing the user to select corresponding points on the

labeling object and the mesh. Then the tool calculates the optimal transformation

parameters that map the former point set to the latter. We compared and catego-

rized many nonrigid registration algorithms, ultimately selecting TPS-RPM. Ad-

ditionally, we parallelized it to perfect correspondence calculation and restrict its

transformation parameters to 9 degrees of freedom.

In the cases where the user knew the size of the object to be labeled, we introduced

two ways to simplify the process. First, we gave the user the ability to select the

labeling object’s corresponding points on the 2D image instead of the mesh. We do

this by reducing the computation of the desired labeling object position to a system

of non linear equations. We solve it by using the DOG_BOX method’s output as a

starting estimate to the Newton-Raphson method. Allowing the user to label using

the images broadens the tools capabilities and removes the requirement of a mesh

in the input data. Second, we implemented a feature that allows the user to snap the

labeling object onto the desired object as long as it is relatively close to it. We use

Unity’s shader program to apply post-processing effects on the camera’s renders in

order to retrieve their depth map as a 2D texture and decrease the frequency of out-

liers. The rendered texture is encoded and sent as point sets to the Python server.

After that, we use Support Vector Registration to ﬁnd the optimal rigid registration

between these point sets.

Moreover, we introduced Virtual Reality as a practical add-on to the 3D labeling

tool. It allows the user to label any importable data set in VR using the appropriate

controllers with intuitive mechanisms for creating and manipulating labeling ele-

ments using Convex Hulls and simple geometry concepts.

Finally, we performed a series of experiments to compare our tool against current

148

state of the art methods. The experiments were performed in order to highlight

how tedious it is to label thousands of images by drawing 2D bounding boxes. The

performance of the tools was benchmarked based on both the number of images in

each data set and the number of objects to be labeled in a single data set. The 3D

labeling tool excelled in all experiments displaying superior results compared to the

2D labeling tools.

Even though the tool can be built on the most popular operating systems, we think

developing a WebGL version and abstracting all heavy computation to the server

level is the next logical step. Even though this is possible in Unity, we need to re-

design the architecture of the tool to handle importing data from the user’s machine

onto the WebGL application in an efﬁcient manner. Additionally, another area of

possible improvement is taking cues from Anno-Mage and allowing the tool to pre-

dict bounding boxes for pre-trained object classes. Contrastingly to Anno-Mage,

our tool would predict 3D bounding cubes instead of 2D rectangles. Therefore we

cannot use the established RetinaNet as a suggestion engine. However, PointNet

[114] has recently revolutionized deep learning on point sets for 3D classiﬁcation

and we think it is worth exploring as a potential suggestion engine.

149

Bibliography

[1] Yixin Chen and James Z. Wang. “Image Categorization by Learning and

Reasoning with Regions”. In: J. Mach. Learn. Res. 5 (Dec. 2004), pp. 913–

939. ISSN: 1532-4435.

[2]

J. Z. Wang, Jia Li, and G. Wiederhold. “SIMPLIcity: semantics-sensitive

integrated matching for picture libraries”. In: IEEE Transactions on Pattern

Analysis and Machine Intelligence 23.9 (Sept. 2001), pp. 947–963. ISSN:

0162-8828.

[3] Gabriela Csurka et al. “Visual categorization with bags of keypoints”. In:

Work Stat Learn Comput Vision, ECCV Vol. 1 (Jan. 2004).

[4] Collins, Wright, and Greenway. “The Sowerby Image Database”. In: Image

Processing And Its Applications, 1999. Seventh International Conference

on (Conf. Publ. No. 465). Vol. 1. July 1999, 306–310 vol.1.

[5] Kobus Barnard et al. “Evaluation of Localized Semantics: Data, Method-

ology, and Experiments”. In: International Journal of Computer Vision 77

(May 2008), pp. 199–217.

[6]

tzutalin. LabelImg. URL: https : / / github . com / tzutalin / labelImg

(visited on 07/05/2019).

[7]

labelbox. Labelbox. URL: https://github.com/Labelbox/Labelbox

(visited on 07/05/2019).

[8] virajmavani. semi-auto-image-annotation-tool. URL: https : / / github .

com/%5C%5Cvirajmavani/semi-auto-image-annotation-tool (vis-

ited on 07/05/2019).

150

[9] Tsung-Yi Lin et al. “Microsoft COCO: Common Objects in Context”. In:

CoRR abs/1405.0312 (2014). arXiv: 1405.0312.

[10] Mark Everingham et al. “The Pascal Visual Object Classes (VOC) chal-

lenge”. In: International Journal of Computer Vision 88 (June 2010), pp. 303–

338.

[11] Bryan C. Russell et al. “LabelMe: A Database and Web-Based Tool for

Image Annotation”. In: International Journal of Computer Vision 77 (May

2008).

[12]

IntelRealSense. librealsense. URL: https://github.com/IntelRealSense/

librealsense/tree/master/wrappers/python (visited on 05/14/2019).

[13]

raulmur. ORB_SLAM2. URL: https://github.com/raulmur/ORB%5C_

SLAM2 (visited on 05/14/2019).

[14] open3d. Reconstruction System. URL: http://www.open3d.org/docs/

release/tutorial/ReconstructionSystem/index.html (visited on

07/05/2019).

[15] Keijiro Takahashi. Pcx. URL: https://github.com/keijiro/Pcx (vis-

ited on 03/12/2019).

[16] HiddenMonk. Unity3DRuntimeTransformGizmo. URL: https://github.

com/HiddenMonk/Unity3DRuntimeTransformGizmo (visited on 06/20/2019).

[17]

judah. HSVColorPickerUnity. URL: https://github.com/judah4/HSV-

Color%5C-Picker%5C-Unity/tree/master/Assets/HSVPicker (vis-

ited on 06/23/2019).

[18]

J. Ross Quinlan. C4.5: Programs for Machine Learning. San Francisco, CA,

USA: Morgan Kaufmann Publishers Inc., 1993. ISBN: 1-55860-238-0.

[19] Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Sup-

port Vector Machines, Regularization, Optimization, and Beyond. Cambridge,

MA, USA: MIT Press, 2001. ISBN: 0262194759.

151

[20] Kishan Mehrotra, Chilukuri K. Mohan, and Sanjay Ranka. Elements of Ar-

tiﬁcial Neural Networks. Cambridge, MA, USA: MIT Press, 1997. ISBN:

0-262-13328-8.

[21] Mary M. Moya and Don R. Hush. “Network Constraints and Multi-objective

Optimization for One-class Classiﬁcation”. In: Neural Netw. 9.3 (Apr. 1996),

pp. 463–474. ISSN: 0893-6080.

[22] Bernhard Scholkopf et al. “Estimating the Support of a High-Dimensional

Distribution”. In: Neural Comput. 13.7 (July 2001), pp. 1443–1471. ISSN:

0899-7667.

[23] Simon Hawkins et al. “Outlier Detection Using Replicator Neural Networks”.

In: Proceedings of the 4th International Conference on Data Warehous-

ing and Knowledge Discovery. DaWaK 2000. London, UK, UK: Springer-

Verlag, 2002, pp. 170–180. ISBN: 3-540-44123-9.

[24] Bruce G. Lindsay. “Mixture Models: Theory, Geometry and Applications”.

In: NSF-CBMS Regional Conference Series in Probability and Statistics 5

(1995), pp. i–163. ISSN: 19355920, 23290978.

[25] Richard A. Davis, Keh-Shin Lii, and Dimitris N. Politis. “Remarks on Some

Nonparametric Estimates of a Density Function”. In: Selected Works of

Murray Rosenblatt. Ed. by Richard A. Davis, Keh-Shin Lii, and Dimitris

N. Politis. New York, NY: Springer New York, 2011, pp. 95–100. ISBN:

978-1-4419-8339-8.

[26] Markus Goldstein and Seiichi Uchida. “A Comparative Evaluation of Unsu-

pervised Anomaly Detection Algorithms for Multivariate Data”. In: PLOS

ONE 11.4 (Apr. 2016), pp. 1–31.

[27] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. “Efﬁcient Algo-

rithms for Mining Outliers from Large Data Sets”. In: Proceedings of the

2000 ACM SIGMOD International Conference on Management of Data.

SIGMOD ’00. Dallas, Texas, USA: ACM, 2000, pp. 427–438. ISBN: 1-

58113-217-4.

152

[28] Fabrizio Angiulli and Clara Pizzuti. “Fast Outlier Detection in High Dimen-

sional Spaces”. In: Proceedings of the 6th European Conference on Princi-

ples of Data Mining and Knowledge Discovery. PKDD ’02. London, UK,

UK: Springer-Verlag, 2002, pp. 15–26. ISBN: 3-540-44037-2.

[29] Markus M. Breunig et al. “LOF: Identifying Density-based Local Outliers”.

In: Proceedings of the 2000 ACM SIGMOD International Conference on

Management of Data. SIGMOD ’00. Dallas, Texas, USA: ACM, 2000,

pp. 93–104. ISBN: 1-58113-217-4.

[30]

Jian Tang et al. “Enhancing Effectiveness of Outlier Detections for Low

Density Patterns”. In: Proceedings of the 6th Paciﬁc-Asia Conference on

Advances in Knowledge Discovery and Data Mining. PAKDD ’02. Berlin,

Heidelberg: Springer-Verlag, 2002, pp. 535–548. ISBN: 3-540-43704-5.

[31] Hans-Peter Kriegel et al. “LoOP: Local Outlier Probabilities”. In: Proceed-

ings of the 18th ACM Conference on Information and Knowledge Manage-

ment. CIKM ’09. Hong Kong, China: ACM, 2009, pp. 1649–1652. ISBN:

978-1-60558-512-3.

[32] S. Papadimitriou et al. “LOCI: fast outlier detection using the local cor-

relation integral”. In: Proceedings 19th International Conference on Data

Engineering (Cat. No.03CH37405). Mar. 2003, pp. 315–326.

[33] Mennatallah Amer and Markus Goldstein. “Nearest-Neighbor and Cluster-

ing based Anomaly Detection Algorithms for RapidMiner”. In: Aug. 2012.

[34] Gabriel Martos, Alberto Muñoz, and Javier González. “On the Generaliza-

tion of the Mahalanobis Distance”. In: Progress in Pattern Recognition, Im-

age Analysis, Computer Vision, and Applications. Ed. by José Ruiz-Shulcloper

and Gabriella Sanniti di Baja. Berlin, Heidelberg: Springer Berlin Heidel-

berg, 2013, pp. 125–132. ISBN: 978-3-642-41822-8.

[35] Markus Goldstein and Andreas Dengel. “Histogram-based Outlier Score

(HBOS): A fast Unsupervised Anomaly Detection Algorithm”. In: Sept.

2012.

153

[36] R. Kwitt and U. Hofmann. “Unsupervised Anomaly Detection in Network

Trafﬁc by Means of Robust PCA”. In: 2007 International Multi-Conference

on Computing in the Global Information Technology (ICCGI’07). Mar. 2007,

pp. 37–37.

[37] Varun Chandola, Arindam Banerjee, and Vipin Kumar. “Anomaly Detec-

tion: A Survey”. In: ACM Comput. Surv. 41.3 (July 2009), 15:1–15:58.

ISSN: 0360-0300.

[38] Yezheng Liu et al. “Generative Adversarial Active Learning for Unsuper-

vised Outlier Detection”. In: CoRR abs/1809.10816 (2018). arXiv: 1809.

10816.

[39]

J. Gebhardt et al. “Document Authentication Using Printing Technique Fea-

tures and Unsupervised Anomaly Detection”. In: 2013 12th International

Conference on Document Analysis and Recognition. Aug. 2013, pp. 479–

483.

[40] Markus Goldstein et al. “Enhancing Security Event Management Systems

with Unsupervised Anomaly Detection”. In: Feb. 2013.

[41] Zengyou He, Xiaofei Xu, and Shengchun Deng. “Discovering Cluster-based

Local Outliers”. In: Pattern Recogn. Lett. 24.9-10 (June 2003), pp. 1641–

1650. ISSN: 0167-8655.

[42] Markus Goldstein. Anomaly Detection in Large Datasets. June 2014. ISBN:

978-3-8439-1572-4.

[43] Yue Zhao, Zain Nasrullah, and Zheng Li. “PyOD: A Python Toolbox for

Scalable Outlier Detection”. In: Journal of Machine Learning Research

20.96 (2019), pp. 1–7.

[44] Travis E Oliphant. A guide to NumPy. Vol. 1. Trelgol Publishing USA, 2006.

[45] P. J. Besl and N. D. McKay. “A method for registration of 3-D shapes”. In:

IEEE Transactions on Pattern Analysis and Machine Intelligence 14.2 (Feb.

1992), pp. 239–256. ISSN: 0162-8828.

154

[46] Zhengyou Zhang. “Iterative point matching for registration of free-form

curves and surfaces”. In: International Journal of Computer Vision 13.2

(Oct. 1994), pp. 119–152. ISSN: 1573-1405.

[47] Andrew Fitzgibbon. “Robust Registration of 2D and 3D Point Sets”. In:

Image and Vision Computing 21 (Apr. 2002), pp. 1145–1153.

[48] S. Rusinkiewicz and M. Levoy. “Efﬁcient variants of the ICP algorithm”. In:

Proceedings Third International Conference on 3-D Digital Imaging and

Modeling. May 2001, pp. 145–152.

[49] Steven Gold et al. “New Algorithms for 2D and 3D Point Matching: Pose

Estimation and Correspondence”. In: Pattern Recognition 31 (1997), pp. 957–

964.

[50] Anand Rangarajan et al. “A robust point-matching algorithm for autora-

diograph alignment”. In: Medical Image Analysis 1.4 (1997), pp. 379–398.

ISSN: 1361-8415.

[51] Haili Chui and A. Rangarajan. “A new algorithm for non-rigid point match-

ing”. In: Proceedings IEEE Conference on Computer Vision and Pattern

Recognition. CVPR 2000 (Cat. No.PR00662). Vol. 2. June 2000, 44–51

vol.2.

[52] Haili Chui and Anand Rangarajan. “A new point matching algorithm for

non-rigid registration”. In: Computer Vision and Image Understanding 89.2

(2003). Nonrigid Image Registration, pp. 114–141. ISSN: 1077-3142.

[53] Bin Luo and E. R. Hancock. “Structural graph matching using the EM algo-

rithm and singular value decomposition”. In: IEEE Transactions on Pattern

Analysis and Machine Intelligence 23.10 (Oct. 2001), pp. 1120–1136. ISSN:

0162-8828.

[54] H. Chui and A. Rangarajan. “A feature registration framework using mix-

ture models”. In: Proceedings IEEE Workshop on Mathematical Methods in

Biomedical Image Analysis. MMBIA-2000 (Cat. No.PR00737). June 2000,

pp. 190–197.

155

[55] Sergei Divakov and Ivan V. Oseledets. “Adversarial point set registration”.

In: CoRR abs/1811.08139 (2018). arXiv: 1811.08139.

[56] Chenyang Zhu et al. “Deformation-driven Shape Correspondence via Shape

Recognition”. In: ACM Trans. Graph. 36.4 (July 2017), 51:1–51:12. ISSN:

0730-0301.

[57] Su Zhang et al. “Non-rigid point set registration using dual-feature ﬁnite

mixture model and global-local structural preservation”. In: Pattern Recog-

nition 80 (2018), pp. 183–195. ISSN: 0031-3203.

[58] A. Myronenko and X. Song. “Point Set Registration: Coherent Point Drift”.

In: IEEE Transactions on Pattern Analysis and Machine Intelligence 32.12

(Dec. 2010), pp. 2262–2275. ISSN: 0162-8828.

[59] Lingjing Wang et al. “Non-Rigid Point Set Registration Networks”. In:

CoRR abs/1904.01428 (2019). arXiv: 1904.01428.

[60]

J. Ma et al. “Nonrigid Point Set Registration With Robust Transformation

Learning Under Manifold Regularization”. In: IEEE Transactions on Neu-

ral Networks and Learning Systems (2018), pp. 1–14. ISSN: 2162-237X.

[61] H. Qu et al. “Probabilistic Model for Robust Afﬁne and Non-Rigid Point

Set Matching”. In: IEEE Transactions on Pattern Analysis and Machine

Intelligence 39.2 (Feb. 2017), pp. 371–384. ISSN: 0162-8828.

[62] G. Agamennoni et al. “Point Clouds Registration with Probabilistic Data

Association”. In: 2016 IEEE/RSJ International Conference on Intelligent

Robots and Systems (IROS). Oct. 2016, pp. 4092–4098.

[63] M. Danelljan et al. “A Probabilistic Framework for Color-Based Point Set

Registration”. In: 2016 IEEE Conference on Computer Vision and Pattern

Recognition (CVPR). June 2016, pp. 1818–1826.

[64] D. Campbell and L. Petersson. “An Adaptive Data Representation for Ro-

bust Point-Set Registration and Merging”. In: 2015 IEEE International Con-

ference on Computer Vision (ICCV). Dec. 2015, pp. 4292–4300.

156

[65] H. Van Nguyen and F. Porikli. “Support Vector Shape: A Classiﬁer-Based

Shape Representation”. In: IEEE Transactions on Pattern Analysis and Ma-

chine Intelligence 35.4 (Apr. 2013), pp. 970–982. ISSN: 0162-8828.

[66] Thomas Deselaers, Georg Heigold, and Hermann Ney. “Object Classiﬁca-

tion by Fusing SVMs and Gaussian Mixtures”. In: Pattern Recogn. 43.7

(July 2010), pp. 2476–2484. ISSN: 0031-3203.

[67]

J. Ma et al. “Robust Point Matching via Vector Field Consensus”. In: IEEE

Transactions on Image Processing 23.4 (Apr. 2014), pp. 1706–1721. ISSN:

1057-7149.

[68] Osamu Hirose. “Dependent landmark drift: robust point set registration based

on the Gaussian mixture model with a statistical shape model”. In: CoRR

abs/1711.06588 (2017). arXiv: 1711.06588.

[69] S. Ge, G. Fan, and M. Ding. “Non-rigid Point Set Registration with Global-

Local Topology Preservation”. In: 2014 IEEE Conference on Computer Vi-

sion and Pattern Recognition Workshops. June 2014, pp. 245–251.

[70]

Jiayi Ma, Ji Zhao, and Alan L. Yuille. “Non-Rigid Point Set Registration by

Preserving Global and Local Structures”. In: IEEE transactions on image

processing : a publication of the IEEE Signal Processing Society 25 (Aug.

2015).

[71]

J. Ma et al. “Robust Estimation of Nonrigid Transformation for Point Set

Registration”. In: 2013 IEEE Conference on Computer Vision and Pattern

Recognition. June 2013, pp. 2147–2154.

[72] F. L. Bookstein. “Principal warps: thin-plate splines and the decomposition

of deformations”. In: IEEE Transactions on Pattern Analysis and Machine

Intelligence 11.6 (June 1989), pp. 567–585. ISSN: 0162-8828.

[73] Micheal Garland and Paul S.Heckbert. “Surface Simpliﬁcation Using Quadric

Error Metrics”. In: Carnegie Mellon University (1996).

[74] Mattias Edlund. UnityMeshSimpliﬁer. URL: https://github.com/Whinarn/

UnityMeshSimplifier (visited on 04/03/2019).

157

[75] Python. Asynchronous I/O Documentation. URL: https://docs.python.

org/3/library/asyncio.html (visited on 02/23/2019).

[76]

sta. websocketsharp. URL: https://github.com/sta/websocket%5C-

sharp (visited on 02/22/2019).

[77] C VOGLIS and Isaac Lagaris. “A Rectangular Trust Region Dogleg Ap-

proach for Unconstrained and Bound Constrained Nonlinear Optimization”.

In: (June 2019).

[78] M.J.D. POWELL. “A New Algorithm for Unconstrained Optimization”. In:

Dec. 1970.

[79] S. Umeyama. “Least-squares estimation of transformation parameters be-

tween two point patterns”. In: IEEE Transactions on Pattern Analysis and

Machine Intelligence 13.4 (Apr. 1991), pp. 376–380. ISSN: 0162-8828. DOI:

10.1109/34.88573.

[80] Björn Dahlgren. “pyneqsys: Solve symbolically deﬁned systems of non-

linear equations numerically”. In: The Journal of Open Source Software 3

(Jan. 2018), p. 531.

[81] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. “Poisson Surface

Reconstruction”. In: Proceedings of the Fourth Eurographics Symposium

on Geometry Processing. SGP ’06. Cagliari, Sardinia, Italy: Eurographics

Association, 2006, pp. 61–70. ISBN: 3-905673-36-3.

[82] A. Cengiz Öztireli, Gaël Guennebaud, and Markus H. Gross. “Feature Pre-

serving Point Set Surfaces based on Non-Linear Kernel Regression”. In:

Comput. Graph. Forum 28 (2009), pp. 493–501.

[83] Li-Yi Wei. “Parallel Poisson Disk Sampling”. In: ACM Trans. Graph. 27.3

(Aug. 2008), 20:1–20:9. ISSN: 0730-0301.

[84] Carlos A Vanegas, Daniel G. Aliaga, and Bedrich Benes. “Automatic Ex-

traction of Manhattan-World Building Masses from 3D Laser Range Scans”.

In: IEEE Transactions on Visualization and Computer Graphics 18.10 (Oct.

2012), pp. 1627–1637. ISSN: 1077-2626.

158

[85] Patrick Mullen et al. “Signing the Unsigned: Robust Surface Reconstruction

from Raw Pointsets”. In: Computer Graphics Forum 29 (July 2010).

[86] Nina Amenta and Yong Joo Kil. “Deﬁning Point-set Surfaces”. In: ACM

SIGGRAPH 2004 Papers. SIGGRAPH ’04. Los Angeles, California: ACM,

2004, pp. 264–270.

[87] M. Alexa et al. “Computing and rendering point set surfaces”. In: IEEE

Transactions on Visualization and Computer Graphics 9.1 (Jan. 2003), pp. 3–

15. ISSN: 1077-2626.

[88] Ruwen Schnabel, Roland Wahl, and Reinhard Klein. “Efﬁcient RANSAC

for point-cloud shape detection”. In: Comput. Graph. Forum 26 (June 2007),

pp. 214–226.

[89] Hugues Hoppe et al. “Surface Reconstruction from Unorganized Points”.

In: SIGGRAPH Comput. Graph. 26.2 (July 1992), pp. 71–78. ISSN: 0097-

8930.

[90]

J. C. Carr et al. “Reconstruction and Representation of 3D Objects with

Radial Basis Functions”. In: Proceedings of the 28th Annual Conference

on Computer Graphics and Interactive Techniques. SIGGRAPH ’01. New

York, NY, USA: ACM, 2001, pp. 67–76. ISBN: 1-58113-374-X.

[91] Yutaka Ohtake et al. “Multi-level Partition of Unity Implicits”. In: ACM

SIGGRAPH 2003 Papers. SIGGRAPH ’03. San Diego, California: ACM,

2003, pp. 463–470. ISBN: 1-58113-709-5.

[92] Brian Curless and Marc Levoy. “A Volumetric Method for Building Com-

plex Models from Range Images”. In: Proceedings of the 23rd Annual Con-

ference on Computer Graphics and Interactive Techniques. SIGGRAPH

’96. New York, NY, USA: ACM, 1996, pp. 303–312. ISBN: 0-89791-746-4.

[93] Liangliang Nan, Andrei Sharf, and Baoquan Chen. “2D-D Lifting for Shape

Reconstruction”. In: Comput. Graph. Forum 33 (2014), pp. 249–258.

159

[94] C. Zach, T. Pock, and H. Bischof. “A Globally Optimal Algorithm for Ro-

bust TV-L1 Range Image Integration”. In: 2007 IEEE 11th International

Conference on Computer Vision. Oct. 2007, pp. 1–8.

[95] Yotam Livny et al. “Automatic Reconstruction of Tree Skeletal Structures

from Point Clouds”. In: ACM Trans. Graph. 29.6 (Dec. 2010), 151:1–151:8.

ISSN: 0730-0301.

[96] Ruwen Schnabel, Patrick Degener, and Reinhard Klein. “Completion and

Reconstruction with Primitive Shapes”. In: Comput. Graph. Forum 28 (Apr.

2009), pp. 503–512.

[97] Yangyan Li et al. “2D-3D Fusion for Layer Decomposition of Urban Fa-

cades”. In: Proceedings of the 2011 International Conference on Computer

Vision. ICCV ’11. Washington, DC, USA: IEEE Computer Society, 2011,

pp. 882–889. ISBN: 978-1-4577-1101-5.

[98] Chao-Hui Shen et al. “Structure Recovery by Part Assembly”. In: ACM

Trans. Graph. 31.6 (Nov. 2012), 180:1–180:11. ISSN: 0730-0301.

[99] Young Kim et al. “Guided Real-Time Scanning of Indoor Objects”. In:

Computer Graphics Forum 32 (Oct. 2013).

[100] Liangliang Nan et al. “SmartBoxes for Interactive Urban Reconstruction”.

In: ACM Trans. Graph. 29.4 (July 2010), 93:1–93:10. ISSN: 0730-0301.

[101] Murat Arikan et al. “O-snap: Optimization-based Snapping for Modeling

Architecture”. In: ACM Trans. Graph. 32.1 (Feb. 2013), 6:1–6:15. ISSN:

0730-0301.

[102] Matthew Berger et al. “A Survey of Surface Reconstruction from Point

Clouds”. In: Comput. Graph. Forum 36.1 (Jan. 2017), pp. 301–329. ISSN:

0167-7055.

[103] Patrick Pérez, Michel Gangnet, and Andrew Blake. “Poisson Image Edit-

ing”. In: ACM SIGGRAPH 2003 Papers. SIGGRAPH ’03. San Diego, Cal-

ifornia: ACM, 2003, pp. 313–318. ISBN: 1-58113-709-5.

160

[104] Michael Kazhdan. “Reconstruction of Solid Models from Oriented Point

Sets”. In: Proceedings of the Third Eurographics Symposium on Geome-

try Processing. SGP ’05. Vienna, Austria: Eurographics Association, 2005.

ISBN: 3-905673-24-X.

[105]

J. Manson, G. Petrova, and S. Schaefer. “Streaming Surface Reconstruc-

tion Using Wavelets”. In: Proceedings of the Symposium on Geometry Pro-

cessing. SGP ’08. Copenhagen, Denmark: Eurographics Association, 2008,

pp. 1411–1420.

[106] Michael Kazhdan and Hugues Hoppe. “Screened Poisson Surface Recon-

struction”. In: ACM Trans. Graph. 32.3 (July 2013), 29:1–29:13. ISSN:

0730-0301.

[107] F. Calakli and G. Taubin. “SSD: Smooth Signed Distance Surface Recon-

struction”. In: Computer Graphics Forum 30.7 (2011), pp. 1993–2002.

[108] mkazhdan. PoissonRecon. URL: https://github.com/mkazhdan/PoissonRecon

(visited on 04/25/2019).

[109] StOriJimmy. SSD. URL: https://github.com/StOriJimmy/SSD (visited

on 04/20/2019).

[110] Manson Josiah. Streaming Surface Reconstruction Using Wavelets. URL:

http://josiahmanson.com/research/wavelet_reconstruct/supplemental/

wavelet_reconstruct.zip (visited on 04/15/2019).

[111] Paolo Cignoni et al. “MeshLab: an Open-Source Mesh Processing Tool.”

In: vol. 1. Jan. 2008, pp. 129–136.

[112] Dirk Merkel. “Docker: Lightweight Linux Containers for Consistent Devel-

opment and Deployment”. In: Linux J. 2014.239 (Mar. 2014). ISSN: 1075-

3583.

[113] Scrawk. Hull-Delaunay-Voronoi. URL: https://github.com/Scrawk/

Hull-Delaunay-Voronoi (visited on 05/02/2019).

[114] R Qi Charles et al. “PointNet: Deep Learning on Point Sets for 3D Classiﬁ-

cation and Segmentation”. In: July 2017, pp. 77–85.

161

