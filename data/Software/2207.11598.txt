Generative Artisan: A Semantic-Aware and Controllable CLIPstyler

Zhenling Yang, Huacheng Song, Qiunan Wu
The University of Edinburgh
s1862671@ed.ac.uk

2
2
0
2

l
u
J

3
2

]

V
C
.
s
c
[

1
v
8
9
5
1
1
.
7
0
2
2
:
v
i
X
r
a

Figure 1. Our style transfer results under various textual conditions: the generated images are created according to a semantic-aware
mechanism and contain realistic texture eﬀects and vibrant colors.

Abstract

Recall that most of the current image style trans-
fer methods require the user to give an image of
a particular style and then extract that styling
feature and texture to generate the style of an
image, but there are still some problems: the
user may not have a reference style image, or it
may be diﬃcult to summarise the desired style in
mind with just one image. The recently proposed
CLIPstyler has solved this problem, which is able
to perform style transfer based only on the pro-
vided description of the style image. Although
CLIPstyler can achieve good performance when
landscapes or portraits appear alone, it can blur
the people and lose the original semantics when
people and landscapes coexist. Based on these
issues, we demonstrate a novel framework that
uses a pre-trained CLIP text-image embedding
model and guides image style transfer through
an FCN semantic segmentation network. Speciﬁ-
cally, we solve the portrait over-styling problem
for both selﬁes and real-world landscape with
human subjects photos, enhance the contrast be-

tween the eﬀect of style transfer in portrait and
landscape, and make the degree of image style
transfer in diﬀerent semantic parts fully control-
lable. Our Generative Artisan resolve the failure
case of CLIPstyler and yield both qualitative and
quantitative methods to prove ours have much
better results than CLIPstyler in both selﬁes and
real-world landscape with human subjects pho-
tos. This improvement makes it possible to com-
mercialize our framework for business scenarios
such as retouching graphics software.

1. Introduction

The author Gatys et al. proposed a neural network-based
style transfer algorithm in 2015 (Gatys et al., 2015b), which
was subsequently published on CVPR 2016 (Gatys et al.,
2016b). The algorithm caused quite a stir and quickly
became the benchmark for all style transfer today. Although
the image generated by the algorithm in this paper looks
very pleasing to the user, it will be distorted when applied
to the photo for style transfer, because the VGG network
cannot preserve the content of the image well.

1

 
 
 
 
 
 
In practical situations, the user may not have access to
the reference style image for various reasons or cannot
summarize the desired style of image. However, users
are still interested in textures that ’mimic’ the style image.
For example, a user wishes to convert their photo into the
style of a Monet or Van Gogh just by typing in a paragraph,
without actually owning a painting by these famous painters.
To overcome this limitation and create genuinely creative
works of art, we should come up with a way to incorporate
our imagination of style into a painting.

The limitations had been breached by CLIPstyler (Kwon
& Ye, 2021), which innovatively allows style transfer to
be carried out ’without’ a reference style image. The ap-
proach uses the recently proposed text-image embedding
model - CLIP (Radford et al., 2021) to directly transfer
semantic textures of the style description into images. A
lightweight CNN network is trained to transform content
images towards stylised images, allowing it to express tex-
tural information related to textual conditions and produce
realistic and colourful results. Rather than optimising the
style loss by using the feature maps generated by the net-
work directly, the method optimizes the similarity between
the CLIP model output of the images generated by the net-
work and the textual conditions entered by the user. To
reduce the content loss of the original image, data enhance-
ment is applied by slicing the output image and sampling
the patch with diﬀerent perspective views. This enhance-
ment allows for a more vivid and varied style of patches.
Furthermore, to overcome the problem of patch-dependent
over-stylization, a new threshold adjustment is applied so
that patches with unusually high scores do not aﬀect net-
work training (Kwon & Ye, 2021).

Although very high-quality images can be obtained with
CLIPstyler, there are still many problems. For example,
when stylizing people in a landscape picture, people are
prone to being over-stylized and lose their original shape.
The same is true for parts that contain much detail, such
as folds of clothing, dense ﬂoor tiles, etc. These problems
motivated our research, so we proposed speciﬁc research
questions: to solve the problem of excessive stylization of
portraits in selﬁes and real-world landscape with human
subjects photos, to optimise the contrast of style transfer
eﬀect between portrait and landscape. In order to achieve
our goals, we decided to use a semantic segmentation net-
work to guide patchwise CLIP loss to get the optimal place-
ment of patches and improve global CLIP loss by adding
a portrait mask to enhance the contrast of style transfer
eﬀect between portrait and landscape. We propose a new
semantic-aware and controllable patchwise CLIP loss for
this purpose. Our experiments show that our new losses
can achieve more reasonable and powerful style transfer
than the original losses of CLIPstyler.

2. Related Work

2.1. Neural Style Transfer

Until now, neural style transfer methods for deep learn-
ing can be broadly classiﬁed into two categories: Image-
Optimisation-Based Online Neural Methods (IOB-NST)
and Model-Optimisation-Based Oﬄine Neural Methods
(MOB-NST). IOB-NST transfers the style by iteratively
optimising an image; MOB-NST optimises the model and
produces the stylised image with a single forward pass.

2.1.1. Image-Optimisation-Based Online Neural Methods

(IOB-NST)

The goal of IOB-NST is to make an empty white noise
image match both content features in the content image and
style features in the style image, resulting in a stylised con-
tent image. The method can generate high quality synthetic
images without training the network. The parameters are
also easy to adjust. However, it has a long computation
time and great reliance on pre-trained models.

Based on Maximum Mean Discrepancy: In 2015, Gatys
et al. proposed a neural network-based style transfer algo-
rithm (Gatys et al., 2015a), which was subsequently pub-
lished on CVPR 2016 (Gatys et al., 2016a). After applying
a pre-trained VGG-19 network, Gatys et al. found that it
is possible to extract content features from any arbitrary
photograph by reconstructing the representations from inter-
mediate layers. They also found that the network is capable
of extracting style features by constructing the Gram ma-
trix. The method optimises the image per-pixel by jointly
minimising content and style losses. Subsequently, Li et al.
(Li et al., 2017a) showed that matching the Gram matrices
of feature maps is equivalent to minimise the Maximum
Mean Discrepancy (MMD).

2.1.2. Model-Optimisation-Based Offline Neural

Methods (MOB-NST)

Although IOB-NST can generate high quality synthetic
images, they suﬀer from computational ineﬃciency. In
contrast, MOB-NST uses a large number of images to train
generative models that can produce stylised images, which
largely solves the problem of computational ineﬃency in
image style transfer.

Inspired by Gatys et al., Johnson et al. (Johnson et al.,
2016) proposed to train a feed-forward network to solve the
optimisation problem in real-time (i.e. real-time style trans-
fer). Building on the Gatys et al. algorithm, Johnson et al.
use perceptual loss functions to train a generative model for
a particular style. In contrast to previous loss functions that
used pixel-by-pixel comparisons when training generative
models, the perceptual loss function squares the diﬀerence
between high-level features extracted from the pre-trained
VGG model. There are other eﬀorts to achieve real-time
style transfer which can transfer multiple or even arbitrary
styles ((Li et al., 2017b); (Li et al., 2018b); (Li et al., 2018a)
etc.).

3. Methodology

3.1. Basic Framework of CLIPstyler

The CLIPstyler aims to transfer the semantic style of target
text tsty to the content image Ic through the pre-trained
text-image embedding model CLIP (Radford et al., 2021).
Since the style is reﬂected in the form of text, we are able to
perform image style transfer based only on the description
of the style, without the need for a speciﬁc style image.

Figure 2. CLIPstyler Architecture (Kwon & Ye, 2021)

The speciﬁed architecture of the network is shown in Figure
2. When a content image Ic is given, we aim to obtain the
style transfer output Ics. In order to transfer the style to the
content image, a CNN encoder-decoder model f (StyleNet)
is used, which can capture the visual features of the content
image and simultaneously stylise the image in deep feature
space to obtain a realistic texture representation. There-
fore, the stylised image Ics is f (Ic), and the ﬁnal goal is to
optimise the parameter of f .

In 2016, (Gatys et al., 2016a) proposed to combine content
loss and style loss function, which is still widely used nowa-
days in style transfer tasks. In order to optimise the neural
network f , content loss is used as a part of the loss function.
However, since the style image is not provided, style loss
cannot be computed. In order to achieve better performance,
Kwon & Ye (2021) proposed a patchwise CLIP loss based
on directional CLIP loss, combined these two losses with
content loss and total variance regularisation loss, to form
the following loss function:

Ltotal = λdLdir + λpLpatch + λcLc + λtvLtv

(1)

where Ltotal, Ldir, Lpatch, Lc, Ltv represents the total loss,
directional CLIP loss, patchwise CLIP loss and total vari-
ation regularization loss, respectively. And λd, λp, λc, λtv
are weights for each of losses.

3.2. Components of CLIPstyler

CLIP: (Radford et al., 2021) CLIP is a state-of-the-art pre-
trained text-to-image embedding model. It is trained by
predicting which caption goes with which image and en-
ables the model to zero-shot transfer to downstream tasks.
In the CLIPstyler, the CLIP pre-trained text transformer en-
coder and the vision transformer encoder are used to obtain

both the CLIP space representation of image generated by
StyleNet and the CLIP space representation of input text
describing the style.

StyleNet: (Kwon & Ye, 2021) StyleNet is a CNN encoder-
decoder model proposed by CLIPstyler. It captures hier-
archical visual features of content images while stylizing
images in a deep feature space for realistic texture repre-
sentations. It uses a lightweight U-net structure and uses
residual blocks to improve the content preservation and
training stability. In the project, the StyleNet is trained to
have the ability to transfer the semantic style of text to the
content image. Model architecture is shown in Figure 3.

Directional CLIP Loss: (Kwon & Ye, 2021) Directional
CLIP loss forces generated image to have a similar semantic
style to the input text by aligning the CLIP-space direction
between the text-image pairs of source and output. It op-
timizes the original CLIP loss to keep the style of image
generated by StyleNet and semantic style of input text con-
sistent with the original CLIP style. It can be deﬁned by
the following formula:

∆T = ET (tsty) − ET (tsrc)

∆I = EI( f (Ic)) − EI(Ic)

Ldir = 1 −

∆I · ∆T
| ∆I || ∆T |

(2)

(3)

(4)

where ∆T represents the direction of CLIP space repre-
sentation of input text and source text, ∆I represents the
direction of CLIP space representation of stylized image
and content image, and Ldir represents the one minus cosine
similarity between two directions mentioned above.

Patchwise CLIP Loss: (Kwon & Ye, 2021) Patchwise
CLIP loss uses randomly cropped patches instead of whole
generated image to compute directional CLIP loss. It calcu-
lates the directional CLIP loss separately after prescriptive
augmentation for each randomly cropped patch, and calcu-
lates the average of the loss of all patches after rejecting
overly stylized patches (threshold regularization). It can be
deﬁned by the following formula:

∆T = ET (tsty) − ET (tsrc)

∆I = EI(aug( ˆI(i)

l(i)
patch

= 1 −

cs )) − EI(Ic)
∆I · ∆T
| ∆I || ∆T |

Lpatch = 1
N

N(cid:88)

R(l(i)

patch, τ)

(5)

(6)

(7)

(8)

where R(s, τ) =

if s <= τ
otherwise

i


0

s
where ∆T represents the direction of CLIP space representa-
tion of input text and source text, ∆I represents the direction
of CLIP space representation of a patch from stylized image
after augmentation and content image, and l(i)
patch represents
the one minus cosine similarity between two directions
mentioned above. Lpatch represents the average of all patch

(9)

Figure 3. StyleNet Model Architecture (Kwon & Ye, 2021)

losses after doing threshold rejection with threshold value
τ. The patches with loss lower than τ will be nulliﬁed.

Content Loss: (Gatys et al., 2016a) Content loss computes
the mean squared error between content image and gener-
ated image on content features extracted from a pretrained
VGG-19 network, which was ﬁrstly proposed by Gatys et
al. In the CLIPstyler, content loss is obtained by adding
the mean squared error of the content features of Conv_4_2
and Conv_5_2 layers, which is used to preserve the content
of the original image on generated image.

Total Variance Regularization Loss: (Rodríguez, 2013)
Total variation regularization loss is inspired by noise re-
moval technique. It uses mean squared error to minimize
the diﬀerence between adjacent pixels, thus alleviate the
side artifacts from irregular pixels.

Multiview Augmentation: (Wang et al., 2019) Multiview
augmentation performs a random perspective transforma-
tion on a given image with a given probability.
In the
CLIPstyler, RandomPerspective implemented by Pytorch
torchvision library is used with degree of distortion 0.5 in
all experiments.

Threshold Regularization: (Kwon & Ye, 2021) Threshold
regularization simply invalidates patches whose directional
CLIP loss is below a certain threshold value during gradient
optimization process, thus avoiding generated images suf-
fering from over-stylization. In the CLIPstyler, threshold
value is set to 0.7 in all experiments.

Prompt Engineering: (Radford et al., 2021) Prompt engi-
neering technique combines some texts of similar meaning
with the target text and feeds them to the text encoder, and
then uses the averaged embedding to replace the original
embedding with single text condition, thereby reducing the
noise of the text embedding. In the CLIPstyler, 79 similar
text templates are used to compute the averaged embedding.

3.3. Our Proposed Methods

During experiments, we found that the CLIPstyler will
over-stylize portraits when portraits and landscapes coexist,

especially the distortion of human faces (Figure 6). We
believe this is caused by the fact that patchwise CLIP loss
randomly stylizes diﬀerent regions, resulting in some un-
desired regions being overly stylized. In addition, due to
the large diﬀerence between landscape and portrait, global
CLIP loss cannot achieve the desired eﬀect on images with
both portrait and landscape. To address these issues, we
propose to use a semantic segmentation network to guide
the style transfer of CLIPstyler, which makes style transfer
semantically aware and controllable. Our new CLIPstyler
architecture is shown in Figure 4.

Figure 4. Our New CLIPstyler Architecture

Semantic Segmentation Network: In order to solve the
problem that CLIPstyler cannot make portraits less stylized
while making background more stylized. We propose a
method that performs semantic segmentation of portrait
and background using the semantic segmentation network
(Figure 5) proposed by Evan Shelhamer et al. (Shelhamer
et al., 2016). They proposed to use a Resnet101 as the back-
bone for end-to-end semantic segmentation. The reason for
choosing this network is that it performs well on semantic
segmentation of human portraits. This network will provide
information on the location of diﬀerent things in the image
for our reﬁnement.

Figure 5. Semantic Segmentation Network based on Fully-
Convolutional Network (Shelhamer et al., 2016)

∆T = ET (tsty) − ET (tsrc)

region. This method is proposed because the original
patch size of patchwise CLIP loss is ﬁxed, which will
lead to obvious diﬀerences at the junction of diﬀerent
semantic parts. In order to maintain the uniformity of
the picture style, we can only use a small part of the
patch to decide that the patch belongs to the portrait
area. We experimentally verify that such a setting can
make the eﬀect of image style transfer more uniform.

The deﬁnition of Semantic-aware Patchwise CLIP Loss can
be formulated as follow:

(10)

(11)

(12)

(13)

(14)

(15)

∆I = EI(aug( ˆI(i)

l(i)
patch

= 1 −

cs )) − EI(Ic)
∆I · ∆T
| ∆I || ∆T |

Lpatch = 1
N

N(cid:88)

i

W(l(i)

patch, α) ∗ R(l(i)

patch, τ)

where R(s, τ) =



0

s

if s <= τ
otherwise



τportrait

τback


αportrait

αback

where τ =

if patch belongs to portrait
if patch belongs to back

where W(s, α) =

if s belongs to portrait
if s belongs to back

(16)

where the number of ˆI(i)
where size of ˆI(i)

cs in portrait is constrained (17)
(18)

cs to determine part is constrained

The semantic-aware patchwise CLIP loss solves the prob-
lem that patchwise CLIP loss in the CLIPstyler randomly
crop patches, resulting in bad patches that distorts undesired
regions such as faces. Using the semantic segmentation
network to guide the degree of style transfer of diﬀerent
patches can eﬀectively avoid the inﬂuence of bad patches
at locations where excessive style transfer is not expected.
Not only that, but it also means that diﬀerent semantically
segmented parts can change their degree of style transfer
according to our artiﬁcial settings, making the image style
transfer freely controllable.

Background Global CLIP Loss: In order to enable global
CLIP loss in the CLIPstyler to pay more attention to the
style transfer of the background without damaging the por-
trait, we propose to use a mask for the portrait part of the
generated image, so that the pixels of the portrait part are
not aﬀected by the global CLIP loss. At the same time,
we use the generated image containing the portrait mask to
replace the original generated image as the input of CLIP,
and only make the style of the background part match the
semantic style of the input text. The Background Global
CLIP Loss is formulated as follow:

∆T = ET (tsty) − ET (tsrc)

∆I = EI(Mask( f (Ic))) − EI(Mask(Ic))

(19)

(20)

Semantic-aware Patchwise CLIP Loss: We propose a
semantic-aware patchwise CLIP loss, which applies a pre-
trained FCN network to obtain semantic information of
portraits and backgrounds in the original image, and uses
them to determine the degree of style transfer for diﬀerent
semantic parts. We propose to use the following mecha-
nisms to control the degree of style transfer across diﬀerent
semantic parts:

• Semantic-aware Random Cropping: In the original
patchwise CLIP loss, random cropping does not have
any restrictions, so it is very easy to crop to areas that
are not suitable for style transfer, resulting in corrupted
images. We propose to use the semantic information in
the image to control the number of patches for diﬀerent
semantic parts. We have experimentally veriﬁed that
using fewer patches for portraits and more patches for
landscapes can increase the degree of style transfer for
landscapes while avoiding damage to portraits.

• Semantic-aware Weight Penalty:

In the original
patchwise CLIP loss, every patch that is not rejected
is treated equally. In this way, diﬀerent degrees of
style transfer cannot be performed according to dif-
ferent semantic parts. Therefore we propose to use
diﬀerent weight penalties for patches in diﬀerent se-
mantic parts. We experimentally verify that using a
higher weight penalty for the patch in portrait part
and a lower weight penalty for the patch in landscape
part can make the style transfer of the landscape better
without destroying the portrait.

• Semantic-aware Threshold Rejection: In the origi-
nal patchwise CLIP loss, threshold rejection is a ﬁxed
value for each patch. This means that it does not take
into account that diﬀerent semantic parts should rede-
ﬁne more appropriate thresholds. We experimentally
verify that using a higher threshold value for the patch
in portrait part and a lower threshold value for the
patch in landscape part can make the style transfer of
the landscape better without destroying the portrait.

• Semantic-aware Regulatable Patch: We propose a
regulatable patch that can determine which semantic
part a patch belongs to based on only part of the patch

Ldir = 1 −

∆I · ∆T
| ∆I || ∆T |

(21)

where Mask represents portrait mask which can be deﬁned
using the following formula:




if pixel belong to the portrait
if pixel belong to the back

Mask =

(22)

0
1

Our experiments found that this approach not only eﬀec-
tively avoids damage to the portrait, but also makes the
style transfer eﬀect of the background better than that of
CLIPstyler. This may be because it is easier for CLIPstyler
to match the text semantic style with the background style
when only the background is present.

The above two losses enable CLIPstyler to achieve results
in pictures greatly improving the original eﬀect of CLIP-
styler. Based on our methods, the loss function for training
CLIPstyler is modiﬁed as follows:

Ltotal = λdLback_dir + λpLsem_patch + λcLc + λtvLtv

(23)

where Lback_dir represents background global CLIP loss and
Lsem_patch represents semantic-aware patchwise CLIP loss.

4. Data Set and Evaluation Criteria

4.1. Data

Since our model is based on image optimisation (IOB-NST),
the model only needs one image as input and does not
need to use training set, validation set and test set. In this
project, we will use the template portrait pictures provided
by CLIPstyler and real real-world landscape with human
subjects photos (e.g. sky + mountains + people + ground,
etc.) to test the performance of the model.

4.2. Evaluation

4.2.1. Quantitative Evaluation

In CLIPstyler, there is no quantitative evaluation method
provided for model performance directly. We decided to
evaluate the performance of style transfer by using four
diﬀerent losses provided in CLIPstyler, as well as our newly
proposed two losses. In theory, lower values of these losses
would represent better style transfer. For example, the two
CLIP losses of CLIPstyler are used to measure the eﬀect of
style transfer, the content loss is used to measure the degree
to which the original image content is preserved, and the
total variation regularization loss is used to measure the
impact of irregular pixels. If all the losses of our optimized
image are lower than the baseline image, it will mean that
our methods will be completely better than the baseline.

4.2.2. Qualitative Evaluation

We used both visual comparison and ablation experiments
for qualitative evaluation. We decided to use several well-
known painter style descriptions such as ’The Scream by
Edvard Munch’ to evaluate the diﬀerence between our opti-
mized images and baseline images. During the experiment,

we found that the style transfer eﬀect of the baseline is
extremely unstable, which may be caused by the ambiguity
of the style description. Therefore, we choose the picture
with the best quality generated by the baseline as the com-
parison object. We also select optimized images similar to
the baseline images for comparison.
For ablation experiments, we found that there is no diﬀer-
ence to the original image without using any CLIP loss or
only using the global CLIP loss, so we decided to investi-
gate whether our optimized global CLIP loss can make the
style transfer eﬀect of background stronger by only discard-
ing the global CLIP loss, and whether our optimized global
CLIP loss plays a more important role in the style trans-
fer process than baseline ones. Furthermore, we can also
compare the performance diﬀerence between our proposed
patchwise CLIP loss and the original patchwise CLIP loss.

5. Experiments

5.1. Experimental setup

We use 512x512 resolution for all template images pro-
vided by CLIPstyler and 1024x1024 HD images for real
photos. For training, we set λd, λp, λc, λv to be 5 × 102, 9
× 103, 150, and 2 × 10−3, respectively. For content loss, we
use features from conv4_2 and conv5_2 layers to handle
content loss. We use the lightweight U-net architecture
provided by CLIPstyler as StyleNet, and use the Adam op-
timizer with a learning rate of 5 × 10−4 to train the model.
The model iteratively optimize the pixels of the input image
in each epoch and our total number of training iterations is
set to 400 epochs and the learning rate is reduced to half
every 100 iterations. We train the model on a single P100
GPU, and the training time for each style transfer is about 5
minutes. For patch cropping, we use a patch size of 128 as
our default setting, and the total number of cropped patches
is set to n = 64. For perspective argumentation, we use
functions provided by the Pytorch library. For threshold
rejection, we set threshold value to 0.7 in baseline model,
while for optimized loss, the size of threshold will be pa-
rameterized according to the speciﬁc photo. To reduce the
noise of text embeddings, we use prompt engineering tech-
niques mentioned in methodology. Finally, to present better
visual eﬀects to readers, we apply the contrast enhancement
technique which enhances image contrast 1.5 times to all
outputs including the baseline.

5.2. Experimental results

5.2.1. Comparison with Baseline

In a qualitative way, we decide to visually compare the
images generated by our model with those generated by the
base CLIPstyler, which is shown in Appendix Figure A. In
general, we have perfectly solved the problem of the style
having over-speciﬁed representations on the portrait.

In the ﬁrst column of Figure A, we have applied the style in
The Scream by Edvard Munch to our real-world landscape
with human subjects photo, and Lena, respectively. As

Figure 6. A comparison of the generated results of our methods - Generative Artisan CLIPstyler with the original version - Base
CLIPstyler in the absence of the (background) global CLIP loss.

we can see, the images generated by the base CLIPstyler
have over-speciﬁed the element of "scream" and apply it
on human faces, which causes a very serious distortion.
In contrast, our model removes the eﬀect of the scream
expression on the portraits, while making the background
section more stylised. In our real-world landscape with
human subjects photo, we can see that more elements of
scream appear in the sky and on the ground, making the
overall image look more "screamy". This optimisation is
also evident in the second column (the style of The Starry
Night by Vincent van Gogh) and the third column (the style
of The Great Wave oﬀ Kanagawa by Katsushika Hokusai)
of the generated images.
In the top two images in the
second column, the image generated by the base CLIPstyler
has a lot of stars covering the portrait, while few stars are
visible in the background sky. After optimisation, it can be
seen that the human portraits are clearly shown and the sky
becomes closer to that of the Van Gogh’s work. A similar
optimisation is seen in the two images below in the second
column, where the swirling texture on the face disappears
and is replaced by a more complete portrait.

5.2.2. Ablation Experiment Results

The results of our ablation experiments are shown in Figure
6. The ﬁgure shows the images of the starry night style
when using and not using the global CLIP loss. The image
pair on the left are generated from base CLIPstyler and
the image pair on the right are generated from Generative
Artisan CLIPstyler. For each pair of images, the image
on the left is with global CLIP loss, and the image on the
right is without global CLIP loss. For the image pair of
Generative Artisan CLIPstyler, we use our own proposed
CLIP loss instead of the original CLIP loss to compare with
the baseline.

Artisan CLIPstyler, we can ﬁnd that our proposed back-
ground global CLIP loss can greatly improve the degree
of style transfer of the background. This veriﬁes that us-
ing a portrait mask can eﬀectively improve the degree of
style transfer of the background by making the text seman-
tics match with the image style of background only. This
also means that the background global CLIP loss can play a
more important role in style transfer than the original global
CLIP loss.

Comparing the images of base CLIPstyler and Generative
Artisan CLIPstyler using only patchwise CLIP loss, we can
ﬁnd that the image of base CLIPstyler obviously damages
the portrait, which veriﬁes that the randomly cropped patch
is indeed easy to transfer excessive texture to the human
face. However, the image of Generative Artisan CLIPstyler
perfectly preserves the portrait while optimizing the con-
trast between the portrait and the background. This veriﬁes
that the semantic-aware patchwise CLIP loss can avoid
excessive style transfer of portraits and improve the con-
trast of the degree of style transfer between portrait and
background.

Overall, our newly proposed CLIP loss outperforms the
original CLIP loss in all aspects of visual performance. Not
only does this allow our portraits to retain a similar quality
to a real photo, but it also enriches the stylistic elements of
the landscape.

5.2.3. Quantitative Experimental Results

In addition to judging which style-transferred image is bet-
ter by visual comparison, we also need to judge the pros and
cons by model performance. The ﬁgures below shows the
diﬀerent loss variations for two diﬀerent image examples
and the comparison between the individual losses.

From the image pair of base CLIPstyler, we can ﬁnd that
there is not much diﬀerence between images with and with-
out using global CLIP loss. This veriﬁes that the global
CLIP loss mentioned in CLIPstyler cannot achieve the de-
sired eﬀect. However, in the image pair of Generative

Figure 7 shows the results of Lena picture we used for
our experiments. We can see that no matter what type of
style transfer, all the losses of the optimized images are
lower than the baseline ones. This shows that the style
transfer eﬀect of our model on Lena picture is better than

Figure 7. Lena diﬀerent loss line graph.

the baseline style transfer eﬀect in all aspects.

Figure 9. real-world landscape with human subjects photo diﬀer-
ent loss line graph.

Figure 8. Lena diﬀerent loss bar graph.

Figure 10. real-world landscape with human subjects photo diﬀer-
ent loss bar graph.

Figure 8 shows the comparison of all individual losses
in Lena. we can ﬁnd that the optimised content loss is
much lower compared to baseline and the optimized CLIP
losses are slightly lower than the baseline. This meets our
expectations, as our goal is to have better style transfer than
the original CLIPstyler while preserving the full portrait.
The reduction in TV loss also shows that there are fewer
irregular pixels left in the image.

Figure 9 shows the results of real-world landscape with
human subjects photos we used for our experiments. We
can also see that no matter what type of style transfer, all the
losses of the optimized images are lower than the baseline
ones. This shows that the style transfer eﬀect of our model
on real-world landscape with human subjects photo is also
better than the baseline style transfer eﬀect in all aspects.

Figure 10 shows the comparison of all individual losses
in real-world landscape with human subjects photo. We
can get almost the same analysis results as the Lena, i.e.
all losses outperform the baseline. This shows that the
generalization performance of our model is also reﬂected in
the real-world landscape with human subjects photo, which
in a quantitative comparison proves that our model works
optimally.

6. Conclusions

In this paper, we proposed a novel image style transfer
framework that allows the user to input a piece of text to
change the current image style without a reference style
image, and incorporate a semantic segmentation network
to guide the style transfer so that it focuses on the style
transfer eﬀects of portraits and landscapes, respectively.
We proposed two losses based on semantic segmentation
networks, namely the semantic-aware patchwise CLIP loss
and the background global CLIP loss. These two losses
addresses major issues with CLIPstyler, such as pathwise
CLIP loss being too random and prone to excessive style
transfer for undesired regions, as well as addressing some
failure case in CLIPstyler and obtained much better results
in some scenarios such as selﬁes and real-world landscape
with human subjects photos. This improvement makes the
image style transfer of CLIPstyler full controllable based
on diﬀerent semantic parts and makes CLIPstyler commer-
cially viable for more business scenarios such as retouching
software. Although we have the excellent results, the cur-
rent technology is limited to backgrounds and portraits.
There is no control over style transfer for more ﬁne-grained
background elements. In future work, we can use a more

Rodríguez, Paul. Total variation regularization algorithms
for images corrupted with diﬀerent noise models: a re-
view. Journal of Electrical and Computer Engineering,
2013, 2013.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor.
Fully convolutional networks for semantic segmentation.
IEEE transactions on pattern analysis and machine in-
telligence, 39(4):640–651, 2016.

Wang, Ke, Fang, Bin, Qian, Jiye, Yang, Su, Zhou, Xin, and
Zhou, Jie. Perspective transformation data augmentation
for object detection. IEEE Access, 8:4935–4943, 2019.

advanced semantic segmentation network to obtain richer
semantic information, so that we can control more semantic
parts for diﬀerent degrees of style transfer, so as to obtain
more brilliant results. In addition, we could also measure
image quality via non-reference metrics e.g. FID or NIQE
scores.

References

Gatys, Leon A., Ecker, Alexander S., and Bethge,
Matthias. A neural algorithm of artistic style. CoRR,
abs/1508.06576, 2015a. URL http://arxiv.org/abs/1508.
06576.

Gatys, Leon A., Ecker, Alexander S., and Bethge, Matthias.
A neural algorithm of artistic style, Sep 2015b. URL
https://arxiv.org/abs/1508.06576.

Gatys, Leon A, Ecker, Alexander S, and Bethge, Matthias.
Image style transfer using convolutional neural networks.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2414–2423, 2016a.

Gatys, Leon A., Ecker, Alexander S., and Bethge, Matthias.
Image style transfer using convolutional neural networks.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016b.

Johnson, Justin, Alahi, Alexandre, and Fei-Fei, Li. Per-
ceptual losses for real-time style transfer and super-
resolution. CoRR, abs/1603.08155, 2016. URL http:
//arxiv.org/abs/1603.08155.

Kwon, Gihyun and Ye, Jong Chul. Clipstyler: Image style
transfer with a single text condition. arXiv preprint
arXiv:2112.00374, 2021.

Li, Xueting, Liu, Sifei, Kautz, Jan, and Yang, Ming-Hsuan.
Learning linear transformations for fast arbitrary style
transfer. CoRR, abs/1808.04537, 2018a. URL http://
arxiv.org/abs/1808.04537.

Li, Yanghao, Wang, Naiyan, Liu, Jiaying, and Hou,
Xiaodi. Demystifying neural style transfer. CoRR,
abs/1701.01036, 2017a. URL http://arxiv.org/abs/1701.
01036.

Li, Yijun, Fang, Chen, Yang, Jimei, Wang, Zhaowen, Lu,
Xin, and Yang, Ming-Hsuan. Universal style transfer via
feature transforms. CoRR, abs/1705.08086, 2017b. URL
http://arxiv.org/abs/1705.08086.

Li, Yijun, Liu, Ming-Yu, Li, Xueting, Yang, Ming-Hsuan,
and Kautz, Jan. A closed-form solution to photorealistic
image stylization. CoRR, abs/1802.06474, 2018b. URL
http://arxiv.org/abs/1802.06474.

Radford, Alec, Kim, Jong Wook, Hallacy, Chris, Ramesh,
Aditya, Goh, Gabriel, Agarwal, Sandhini, Sastry, Girish,
Askell, Amanda, Mishkin, Pamela, Clark, Jack, et al.
Learning transferable visual models from natural lan-
guage supervision. In International Conference on Ma-
chine Learning, pp. 8748–8763. PMLR, 2021.

Figure A. A comparison of the generated results of our methods with the original version. The ﬁrst and third rows are by Basic CLIPstyler,
and the second and fourth rows are by Generative Artisan CLIPstyler.

