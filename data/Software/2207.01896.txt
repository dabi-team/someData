Situation-Aware Environment Perception for Decentralized Automation
Architectures

Matti Henning1, Michael Buchholz1 and Klaus Dietmayer1

2
2
0
2

l
u
J

6
2

]

O
R
.
s
c
[

2
v
6
9
8
1
0
.
7
0
2
2
:
v
i
X
r
a

Abstract— Advances in the ﬁeld of environment perception
for automated agents have resulted in an ongoing increase in
generated sensor data. The available computational resources to
process these data are bound to become insufﬁcient for real-time
applications. Reducing the amount of data to be processed by
identifying the most relevant data based on the agents’ situation,
often referred to as situation-awareness, has gained increasing
research interest, and the importance of complementary ap-
proaches is expected to increase further in the near future. In
this work, we extend the applicability range of our recently
introduced concept for situation-aware environment perception
to the decentralized automation architecture of the UNICARagil
project. Considering the speciﬁc driving capabilities of the
vehicle and using real-world data on target hardware in a
post-processing manner, we provide an estimate for the daily
reduction in power consumption that accumulates to 36.2 %.
While achieving these promising results, we additionally show
the need to consider scalability in data processing in the design
of software modules as well as in the design of functional
systems if the beneﬁts of situation-awareness shall be leveraged
optimally.

I. INTRODUCTION

Environment perception within automated driving has
received signiﬁcant attention within the last decades. This
attention is accompanied by a similarly intense increase in
employed sensors, sensor modalities, sensor redundancies,
and computational hardware to process the large amount of
generated data for state-of-the-art automated vehicles [1].
This has mainly been possible due to the drastic increase
in the availability of sensors and computational power of
processing hardware as well as the drastic decrease in their
acquisition cost. However,
the amount of generated data
paired with the increasing complexity of processing algo-
rithms is already exceeding the computational resources [1].
One approach to counteract the limitations imposed by
the computational resources is situation-awareness for auto-
mated agents, as summarized by Dahn et al. [2]. Originating
from the ﬁeld of active perception, where actuated sensor
platforms are employed to obtain relevant data of the envi-
ronment, situation-awareness aims to identify which infor-
mation an agent perceives is relevant for its task execution.
By this distinction, the amount of data to be processed can
be drastically reduced and real-time capabilities of automated
agents can be improved. Besides, even where the computa-
tional resources are sufﬁcient to process all available data,

1M. Henning, M. Buchholz, and K. Dietmayer are with the Institute of
Measurement, Control and Microtechnology at Ulm University, 89081, Ulm,
Germany. E-Mail: <ﬁrstname>.<lastname>@uni-ulm.de

This research is accomplished within the UNICARagil project (FKZ
16EMO0290). We acknowledge the ﬁnancial support for the project by the
German Federal Ministry of Education and Research (BMBF).

Fig. 1: Perception capabilities of an UNICARagil vehicle
(blue),
including the perception area of the four sensor
modules (upper case, ﬁlled colors), as well as the separation
of the environment into eight regions (lower case, textured).

these methods allow using resources efﬁciently and conserv-
ing energy. A few years ago, Bajcsy et al. [3] surveyed
the advances in situation-awareness and active perception
within automated vehicles and concluded that existing and
new approaches would need to receive increased attention to
accelerate future progress within the ﬁeld.

Various approaches for selective data processing exist,
e.g., [4] based on image saliency or [5] a-priori knowledge
of the environment. Recently, we have published a novel,
ﬂexible and modular concept for situation-aware environ-
ment perception [6] that allows the integration of such ap-
proaches in a formalized way. While we previously focused
on a centralized processing architecture for environment
perception, in this work, we extend the applicability range
towards a decentralized processing architecture as introduced
by Keilhoff et al. [7]: the disruptive and modular vehicle
prototypes for driverless operation within the publicly funded
UNICARagil project [8], [9].

A. Environment Perception in the UNICARagil Vehicles

The core concept of the UNICARagil project bases on
modularity and service orientation. Consequently, the en-
vironment perception of the prototype vehicles is designed
in a modular way, so that all vehicles employ an identical
set of four identical sensor modules, one at each corner of
the vehicle. Each sensor module comprises one lidar sensor,
two radar sensors, four cameras, and one inertial sensor,
resulting in a multi-modal perception area of 270°. A high-
end consumer-grade PC accompanies the sensors. Fig. 1
illustrates the sensor modules, indicated by either front (F)
or rear (R) and either left (L) or right (R) for the respective

© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/IV51971.2022.9827081

 
 
 
 
 
 
vehicle corners, as well as their 270° perception area.

The decentralized processing in each sensor module gen-
erates an independent environment model with both model-
based as well as model-free representations of the surround-
ings of the vehicle. Reﬂecting the service orientation, the
operation mode of the sensor modules can be actively con-
trolled during run-time via the automotive service-oriented
architecture (ASOA) [10]. The independent sensor module
representations of the environment are transmitted to a
centralized processing unit, the cerebrum. Here, after high-
level fusion, the interface to the planning modules is realized,
which is outside of the scope of this work. A more detailed
description of UNICARagil automation concept and the
perception system can be found in [11].

B. Contribution

In this work, we extend our concept for situation-aware
environment perception from [6] towards decentralized au-
tomation architectures with integrated software and hardware
modules, i.e., sensor modules, and show that also these new,
disruptive architectures can be easily considered within this
concept. Besides, instead of monitoring the hardware load
of the system to verify the effectiveness of the application,
we measure the actual power consumption of the system
and provide estimates on the reduction of consumption
within our experiments. Lastly, with our application results,
we show the necessity for considering situation-awareness
in the design of software modules and architectures for
environment perception to maximize the beneﬁts obtained
by situation-awareness.

II. SITUATION-AWARE ENVIRONMENT PERCEPTION

This section brieﬂy summarizes awareness processing [6],
our recently published concept for situation-aware environ-
ment perception. We refer the interested reader to the publi-
cation for more details. Second, we present its application to
the decentralized automated vehicle architecture within the
UNICARagil project.

A. Awareness Processing

The concept roots on the assumption that, for any task an
automated agent needs to solve, some information about its
environment is more relevant than other information. Further,
the external circumstances, i.e., the situation of the agent,
might inﬂuence what information is relevant, even though
the task remains identical.

With these assumptions established,

the key elements
of awareness processing are ﬁrst to describe the relevant
regions within the environment and deﬁne requirements for
the environment perception in these relevant regions. The
relevant regions need to be disjoint and depend on the agent’s
situation. They can be described by any geometric form,
e.g., as a grid. The requirements are represented via an
attention map corresponding to the regions of the environ-
ment. Second, the agent’s environment perception software
is conﬁgured to optimally meet the deﬁned requirements of
the attention map. Lastly, the attention map, representing the

Fig. 2: Simpliﬁed block diagram of the functional interaction
between awareness processing (blue) and an existing percep-
tion processing chain (gray). Inactive modules are indicated
by fading. Diagram adapted from [6].

relevant and non-relevant regions, is fed into the conﬁgured
perception modules, so that the processing resources can be
allocated efﬁciently. Fig. 2 presents an overview of the key
elements and their interaction with an existing processing
chain for environment perception.

1) Situation Detection and Attention Map Generation: To
generate the performance requirement preq
towards the envi-
i
ronment perception for each region ri within the environment
E, the set of relevance deﬁning layers L is introduced. Each
layer lk ∈ L represents an independent environmental inﬂu-
ence for the agent and assigns a performance requirement

preq
i,k = lk(ri)

(1)

to any region ri. The situational inﬂuence on the relevance
of information is realized by using the current situation s as
an activation function for a subset of relevant layer functions
La ⊆ L. To ensure applicability, the set of possible situations
S corresponds to combinations of high-level situational inﬂu-
ences, e.g., weather conditions, geographic location, or agent
operation states. The attention map, containing performance
requirements

(cid:88)

preq
i =

lk(ri)

(2)

lk∈La

for all regions, is then obtained by summation of the in-
dividual performance requirements from each layer. It is
consequently labeled the multi-layer attention map (MLAM).
A visual example using a Cartesian grid representation for
the description of the environment is shown in Fig. 3.

2) Processing Chain Conﬁguration: For the identiﬁcation
of the optimal environment perception software conﬁgura-
tion, its elements are represented by a set of functional
modules m ∈ M. These modules correspond to common
algorithm types for the perception of automated driving
applications, e.g., object tracking, object detection, semantic
segmentation, or free space detection. Each module m pro-
vides a set of properties, comprising estimates for its resource
consumption in terms of cost cm, its provided beneﬁt for
the quality of environment representation w.r.t. the agents
driving behavior in terms of a performance value pm, and

TABLE I: Activation of attention layers per maneuver.

directional intention lateral intention maneuvering

maneuver

(DI)

(LI)

(M)

x
x
x
x

forward
backward
left
right
maneuvering
turn left
turn right
change left
change right
standby

x

x
x
x
x

B. Application for the UNICARagil Vehicle

With the summary of awareness processing provided, we
present its application to the decentralized architecture of
the UNICARagil prototype vehicles (cf. Section I-A) in the
following.

1) Situation Detection and Attention Map Generation:
Our project goal in UNICARagil is a maneuver-dependent
environment perception. Thus, we deﬁne the set of situations
S to contain all valid maneuvers, where each maneuver
corresponds to a tuple between a directional maneuver (dm)
and a lateral maneuver (lm):

S = {(dm, lm)}, with

dm ∈ {forward, backward, left, right,
maneuvering, standby},

lmy ∈ {turn left, turn right,

change left, change right, ∅}.

(4)

(5)

(6)

No lateral maneuver is represented by ∅. The lateral maneu-
vers are invariant to the vehicle’s directional maneuver, i.e.,
side deﬁnitions do not change with the driving direction. The
directional maneuvers left and right reﬂect the capability of
the disruptive dynamic modules [8] of the vehicles to move
sideways. During these maneuvers, no lateral maneuver will
be conducted.

The corresponding MLAM is constructed from three at-
tention layers, which are activated as deﬁned in Tab. I. The
corresponding layer functions l are represented by a direct
mapping between the situation, i.e., the maneuver in our case,
and the regions of the environment (cf. Fig. 1). The regions
ri are deﬁned as eight zones around the vehicle that are
derived by considering the borders of overlapping perception
areas between the sensor modules. Each layer assigns a
performance requirement of l(ri) = 1 to mapped regions
as described in the following. An overview is presented
in Tab. II. The maneuvering layer is active only for the
identically named maneuver. It represents low-velocity, high-
attention scenarios, such as roundabouts or areas shared with
vulnerable road users. Consequently, it assigns relevance to
all regions. The directional intention layer is active in all
directional maneuvers. Relevance is assigned to the three
regions corresponding to the maneuver. The lateral intention
layer is active for all lateral movements, i.e., turning and

Fig. 3: Example for a Cartesian Multi-Layer Attention Map
(MLAM) representation constructed by the aggregation of
three layers L. Darker cells correspond to higher required
attention. Graphic taken from [6].

its relations R, representing linkabilities, dependencies, or
interactions to other modules or external inﬂuences. Besides,
each module refers to a subset of regions Em ⊆ E where it
can process data from and is either classiﬁed as a source or
a non-source module.

With these properties in consideration, the optimal conﬁg-
uration M∗ is found by searching for a module combination
candidate M(cid:48) that is valid with respect to the relations R of
the modules and satisﬁes the performance requirement of the
MLAM P req = (cid:8)preq

(cid:9) ∀ ri at lowest aggregated cost:

i

M∗ = arg min
M(cid:48)⊆M

(cid:33)

cm

s.t.

(cid:32)

(cid:88)

m∈M(cid:48)

(cid:88)

m∈M(cid:48)

pm ≥ P req.

(3)

To structure the search for the optimal conﬁguration, the
relations R of the modules are used to generate a set
of conﬁguration trees. All nodes within the set of trees
correspond to valid combination candidates M(cid:48), where root
nodes strictly correspond to candidates that only contain
source modules. The coverage of the candidates, i.e., the
uniﬁcation of the contained modules’ coverage, is evaluated
against the requirements of the MLAM to invalidate insuf-
ﬁcient trees entirely. Consequently, the search space for the
optimal conﬁguration can be reduced drastically for complex
conﬁguration trees.

Finally, the conﬁguration of the identiﬁed, optimal subset
of processing modules is realized via the underlying software
architecture. ASOA [10] is used within the UNICARagil
project. Other examples of existing software architectures
are [12], [13], or [14].

3) Attention Map Application: In the ﬁnal step of aware-
ness processing,
the MLAM is fed into the conﬁgured
software modules m ∈ M∗. Existing methods and algorithms
that consider situation-awareness, e.g., in the form of selec-
tive data processing (cf. examples from Section I), can be
leveraged. The available system resources are hence allocated
efﬁciently towards processing only relevant information in-
stead of the usual uniform distribution, i.e., processing all
data with equal importance.

TABLE II: Mapping between assigned attention and maneu-
vers by attention layers. Abbreviations used as per Tab. I.
Superscripts f and b indicate the dependency on the forward
and backward directional maneuver, respectively.

maneuver

ﬂ

f

fr

r

br

b

bl

l

forward DI DI DI

backward

left DI

right

DI DI DI

DI DI

DI DI DI

TABLE III: Module coverage of the regions (cf. Fig. 1).

FL

FR

RL

RR

x
x
x
x
x

ﬂ
f
fr
r
br
b
bl
l

x
x
x

x
x

x

x
x
x
x

x
x
x
x
x

maneuvering M M M M M M M M
LI

LI

LI

LIb

LI

LIb

LI

LI

LI

LIf

LIf

LI

turn left
turn right
change left
change right
standby

(a) MLAM for maneuver
s = {(forward, ∅)}.

(b) MLAM for maneuver
s = {(backward, turn right)}.

Fig. 4: Visual examples for the resulting MLAM.

lane changing. Here, the assigned relevance depends on the
current directional maneuver.

In Fig. 4, visual examples for the resulting attention map
are presented. For a common s = {(forward, ∅)} maneuver,
Fig. 4a shows the vast reduction of the required perception
ﬁeld according to the activation of only the directional inten-
tion layer. Considering a maneuver with a lateral component
as in Fig. 4b for s = {(backward, turn right)}, the resulting
requirements towards the environment perception cover a
more extensive set of regions, while still presenting potential
for resource reduction compared to complete processing of
all data in all regions.

2) Processing Chain Conﬁguration: Based on the intro-
duced decentralized functional architecture of the vehicles,
the set of processing modules for environment perception is
deﬁned as the set of available sensor modules as per Fig. 1:

M ={FL, FR, RL, RR}.

Adhering to the modular concept, the properties of each
identically, so that cost = 1.0,
sensor module are set
performance = ∞, and R = ∅, i.e., no relations are to
be considered. Besides, each module is classiﬁed as source.
Their coverage is summarized in Tab. III. Deﬁning the per-
formance of each module as inﬁnite reﬂects the assumption
that any sensor module is capable of generating a trustworthy
environment model. Additional relations might be imposed,
e.g., requirements for redundant coverage between sensor
modules, that are out of the scope of this work.

The resulting set of conﬁguration trees contains all combi-
nations between the sensor modules, i.e., the powerset of the
set of modules P (M), including the set M itself (all modules
active) and the empty set ∅ (no module active). Hence, the
set of conﬁguration trees comprises 16 root nodes, with no
further leaf nodes, as no non-source modules are considered
within the scope of this work.

3) Attention Map Application: Lastly, the attention map
is provided to the conﬁgured sensor modules. Based on
the identiﬁed relevant regions, the data processing within
the functional modules of the independent sensor module
environment model is restricted to a subset of the three
visible 90° quadrants of their 270° ﬁeld of view. For each
sensor module, the middle quadrant coincides with the cor-
responding corner region, e.g., ﬂ for the FL module. Each
of the two outer quadrants corresponds to a coverage of two
regions, e.g., f and fr for the FL module’s second quadrant
and l and bl for its third.

III. EVALUATION

Since the driverless prototype vehicles are completely built
from scratch in the UNICARagil project, they are not yet
available to test our method during run-time. However, we
have a complete sensor module available in the lab [11]
with the same hardware and software components as in the
vehicles, which we use during the project development phase.
Thus, it enables us to generate and process data with identical
behavior and properties as during a real-world drive of such
a sensor module.

To showcase the application in a realistic setup, we deﬁne
a hypothetical route for the autoSHUTTLE [8] between
the Aldenhoven Testing Center [15], where future project
presentations will be conducted, and the nearest train station
in Alsdorf, Germany. The route represents the application
as an automated, close-range public transport vehicle and
comprises a track length of approximately 7.5 km at a
journey-time of 9 min 15 s. It includes various urban and
rural driving conditions and is outlined in Fig. 5.

The maneuver distribution during the hypothetical route is
shown in Tab. IV. The sequence of maneuvers is generated
with 1Hz based on map data from OpenStreetMap [16] and
a simpliﬁed movement model of the vehicle. As expected
the forward maneuver is
for regular driving conditions,
dominating. The maneuvers left and right are conducted only
at the start and at the end of the route to represent sideways
parking maneuvers.

Fig. 5: Outline of the hypothetical route between Alsdorf,
Germany and Aldenhoven Testing Center [15] in pink (map
data from OpenStreetMap [16]).

TABLE IV: Maneuver distribution during the route.

directional
maneuver

forward
backward
left
right
maneuvering
standby

occurance

89.7%
0.0%
1.5%
1.5%
7.3%
0.0%

lateral
maneuver

turn left
turn right
change left
change right
∅

occurance

2.0%
2.2%
0.7%
1.1%
94.1%

A. Module Conﬁguration

We can derive the resulting module conﬁguration using
the introduced awareness processing application along the
hypothetical route in simulation. Fig. 6 shows the module
uptime, referring to the number of cycles the sensor modules
were conﬁgured to be active, as well as the corresponding
number of active quadrants.

The module uptime (blue) accumulates to 100 % between
the two front modules, reﬂecting coverage of the front area
through the entire route as well as a strictly disjoint activation
between these modules. The module uptime for the front-left
module exceeds the uptime of the front-right module. This
behavior depends on the sequence of maneuvers, as an acti-
vation hysteresis is embedded in the simulation. Coherently,
a sensor module will stay active to reduce ramp-up and ramp-
down efforts and only switches to an active state if a lateral
maneuver requires the opposite side to be observed. During
these lateral maneuvers, the third quadrant is active for the
respective front module. This is reﬂected in the averaged
number of active quadrants (orange) slightly above 2 for
both modules. For the two rear modules, their low uptime
corresponds to the low count of maneuvering situations
(7.3 %, cf. Tab. IV). During these maneuvers, the rear module
on the opposite side of the currently active front module is
conﬁgured to ensure a 360° coverage of the environment,
as required by the MLAM parameterization. The averaged
number of active quadrants veriﬁes the functionality, as for
every active cycle of a rear-side module, all quadrants are
active, and the average equals exactly 3.00.

Fig. 6: Summarized module conﬁguration on the hypothetical
route.

Fig. 7: Power consumption per number of active quadrants.

B. System Power Consumption

To provide quantitative results of the energy reduction po-
tential of awareness processing using the introduced MLAM
application, i.e., restricting the processing of sensor data to
active quadrants within active sensor modules, we leverage
measurements of the power consumption of the available
sensor module in the lab. For the scope of our experiments,
we have generated a data set within the vicinity of the Ulm
University, Germany, of roughly the duration of the hypo-
thetical route. To ensure reproducibility, the data evaluation
is conducted in a post-processing manner. The generated
dataset is processed fully per conﬁguration, and the power
consumption is averaged from 100 samples per second.

Fig. 7 provides the resulting distribution of a sensor mod-
ule’s power consumption per number of active quadrants as
boxplots. Contradictory to the initial assumption of resource
reduction, the ﬁgure shows that by reducing the processed
data to one or two quadrants,
the median consumption
increases by approximately 2.3 % compared to the baseline
with all three quadrants being active. The spread of power
consumption remains largely unaffected in both cases. By
reducing the processing to zero processing quadrants, i.e.,
conﬁguring the module into a standby state, the median con-
sumption is reduced by approximately 44 %, complying with
the assumptions. Hence, limiting data processing to active
quadrants is ineffective in reducing the power consumption
of a sensor module in its current conﬁguration.

Considering the software modules for environment per-
ception (cf.
[11]), we conclude that the used deep learning
architectures of the object detection modules used in this
work are not suitable for an effective MLAM application. For
example, the underlying PointNet [17] and PointPillar [18]
architectures used for lidar and radar data use a ﬁxed
number of inputs, independent of the ﬁeld-of-view currently
processed, leading to a ﬁxed number of internal calculations.
The MLAM application induces additional processing effort
to convert the reduced size of input data to the expected
size, e.g., by padding and copying operations. Since we
have already shown in our previous work (cf.
[6]) that
hardware load can be reduced by MLAM application for
other software components, we conclude in compliance with
Bajscy et al. [3] that research within the ﬁeld of aware-
ness processing should be strengthened. To fully leverage
a distinction between relevant and non-relevant information,
either employed software modules must be designed scalable,
or vehicle architectures need to contain conﬁgurable module
options that are speciﬁcally designed for the processing of
data regions instead of full sensor ranges.

Nonetheless, even from the module conﬁguration alone,
a signiﬁcant reduction in power consumption could be
achieved. To obtain an estimate of the reduction in power
consumption for the combined perception architecture, we
aggregate the results presented in Fig. 6 and Fig. 7 for the
hypothetical route. Traversing the route once, the estimated
power consumption drops by 31.9 % from 0.171 kW h for the
baseline (all four sensor modules fully active at all times)
to 0.116 kW h with awareness processing. Further, we scale
the hypothetical application to an entire working day of
10 hours, assuming boarding times of 5 min between every
transfer, where all sensor modules are in a standby state.
For the baseline, the resulting power consumption yields
11.06 kW h, while the power consumption with awareness
processing is only 7.05 kW h. That means that even without
the MLAM application being effective in this example,
awareness processing allows for an estimated reduction in
resource consumption of the system’s environment percep-
tion of impressive 36.2 %.

IV. CONCLUSIONS

In this work, we have shown the applicability of our
previously published concept for awareness processing [6]
towards a decentralized software and processing architec-
ture and disruptive vehicle dynamics, including, e.g., side-
ways movement. Representing the future use of one of the
UNICARagil prototype vehicles [9] as a short-range shuttle
service, we have generated a hypothetical route for which
the daily power consumption of the systems environment
perception is estimated to reduce by 36.2 % by application of
awareness processing. While this reduction is very promis-
ing, we found out that the current sensor modules’ software
components do not allow for energy consumption reduction
by regional separation of data to be processed using our
MLAM concept.

We conclude that the potential of situation-awareness can
only be leveraged fully if automated vehicles are designed to
incorporate regionally separated data processing. Thus, with
this work, we aspire to accelerate the inclusion of situation-
awareness within the ﬁeld of perception in automated driv-
ing applications. For the continued development within the
UNICARagil project, we will similarly pursue the outlined
challenges.

REFERENCES

[1] L. Liu, S. Lu, R. Zhong, B. Wu, Y. Yao, Q. Zhang, and W. Shi,
“Computing systems for autonomous driving: State of the art and
challenges,” IEEE Internet of Things Journal, vol. 8, no. 8, pp. 6469–
6486, 2021.

[2] N. Dahn, S. Fuchs, and H.-M. Gross, “Situation awareness for au-
tonomous agents,” in IEEE International Symposium on Robot and
Human Interactive Communication, 2018, pp. 666–671.

[3] R. Bajcsy, Y. Aloimonos, and J. K. Tsotsos, “Revisiting active per-

ception,” in Autonomous Robots, vol. 42, 2018, pp. 177–196.

[4] A. Pal, S. Mondal, and H. I. Christensen, “"Looking at the right stuff"
- Guided semantic-gaze for autonomous driving,” in IEEE Computer
Society Conference on Computer Vision and Pattern Recognition,
2020, pp. 11 883–11 892.

[5] Y. Nager, A. Censi, and E. Frazzoli, “What lies in the shadows?
Safe and computation-aware motion planning for autonomous vehicles
using intent-aware dynamic shadow regions,” in International Confer-
ence on Robotics and Automation, 2019, pp. 5800–5806.

[6] M. Henning, J. Müller, F. Gies, M. Buchholz, and K. Dietmayer,
“Situation-Aware Environment Perception Using a Multi-Layer Atten-
tion Map,” in IEEE Transactions on Intelligent Vehicles, early access,
doi: 10.1109/TIV.2022.3164236.

[7] D. Keilhoff, D. Niedballa, H.-C. Reuss, M. Buchholz, F. Gies,
K. Dietmayer, M. Lauer, C. Stiller, S. Ackermann, H. Winner et al.,
“UNICARagil - New architectures for disruptive vehicle concepts,” in
19. Internationales Stuttgarter Symposium, 2019, pp. 830–842.
[8] T. Woopen, B. Lampe, T. Böddeker, L. Eckstein et al., “UNICARagil
— Disruptive Modular Architectures for Agile, Automated Vehicle
Concepts,” in 27th Aachen Colloquium Automobile and Engine Tech-
nology, 2018, pp. 663–694.

[9] T. Woopen, R. van Kempen, T. Böddeker, and L. Eckstein,
“UNICARagil — Where We Are and Where We Are Going,” in 29.
Aachen Colloquium Sustainable Mobility, 2020, pp. 285–308.
[10] A. Mokhtarian, B. Alrifaee, and A. Kampmann, “The Dynamic
Service-oriented Software Architecture for the UNICARagil Project,”
in 29th Aachen Colloquium Sustainable Mobility, 2020, pp. 275–284.
[11] M. Buchholz et al., “Automation of the UNICARagil Vehicles,” in
29th Aachen Colloquium Sustainable Mobility, vol. 2, 2020, pp. 1531–
1560.

[12] J. Kim, G. Bhatia, R. Rajkumar, and M. Jochim, “SAFER: system-
level architecture for failure evasion in real-time applications,” in IEEE
Real-Time Systems Symposium, 2012, pp. 227–236.

[13] J. Schlatow, M. Moestl, and R. Ernst, “An extensible autonomous
reconﬁguration framework for complex component-based embedded
systems,” in IEEE International Conference on Autonomic Computing,
2015, pp. 239–242.

[14] D. Thomas, W. Woodall, and E. Fernandez, “Next-generation ROS:

Building on DDS,” ROSCon Chicago, 2014.

[15] Aldenhoven Testing Center, https://www.aldenhoven-testing-center.de/

en/, (accessed: 25.01.2022).

“Planet

[16] OpenStreetMap

contributors,
,”

retrieved
https://www.openstreetmap.org,

from
https://planet.osm.org
2017,
published under ODbL (https://opendatacommons.org/licenses/odbl).
[17] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep Learning
on Point Sets for 3D Classiﬁcation and Segmentation,” in IEEE
conference on computer vision and pattern recognition, 2017, pp. 652–
660.

dump

[18] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“PointPillars: Fast Encoders for Object Detection from Point Clouds,”
in IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2019, pp. 12 697–12 705.

