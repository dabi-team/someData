On the Energy Consumption of Different Dataframe Processing
Libraries - An Exploratory Study

Shriram Shanbhag, Sridhar Chimalakonda
Research in Intelligent Software & Human Analytics (RISHA) Lab
Department of Computer Science & Engineering
Indian Institute of Technology Tirupati, India
{cs20s503,ch}@iittp.ac.in

2
2
0
2

p
e
S
2
1

]
E
S
.
s
c
[

1
v
8
5
2
5
0
.
9
0
2
2
:
v
i
X
r
a

ABSTRACT
Background: The energy consumption of machine learning and
its impact on the environment has made energy efficient ML an
emerging area of research. However, most of the attention stays fo-
cused on the model creation and the training and inferencing phase.
Data oriented stages like preprocessing, cleaning and exploratory
analysis form a critical part of the machine learning workflow.
However, the energy efficiency of these stages have gained little
attention from the researchers. Aim: Our study aims to explore
the energy consumption of different dataframe processing libraries
as a first step towards studying the energy efficiency of the data
oriented stages of the machine learning pipeline. Method: We mea-
sure the energy consumption of 3 popular libraries used to work
with dataframes, namely Pandas, Vaex and Dask for 21 different
operations grouped under 4 categories on 2 datasets. Results: The
results of our analysis show that for a given dataframe processing
operation, the choice of library can indeed influence the energy
consumption with some libraries consuming 202 times lesser en-
ergy over others. Conclusion: The results of our study indicates
that there is a potential for optimizing the energy consumption
of the data oriented stages of the machine learning pipeline and
further research is needed in the direction.

KEYWORDS
dataframe, data preprocessing, data cleaning, energy efficiency

1 INTRODUCTION
Machine learning is a branch of artificial intelligence that focuses
on using data and algorithms to make the systems learn. The prob-
lems addressed by machine learning applications include computer
vision [22], speech recognition [3], natural language processing
[6], machine translation [45] and software engineering [25] among
others. Data plays a critical role in machine learning as it is required
to train the models to make predictions. In recent years, there is a
drastic improvement in performance of machine learning models
driven by new architectures like neural networks, availability of
GPU based powerful hardware infrastructure and huge amounts of
data to train the machine learning models.However, there is also
a critical and growing concern of energy consumption of these
models. A study from 2019 by Strubell et al. [43] revealed that a
single neural network model with 626,155 parameters has a carbon
footprint equivalent to lifetime carbon footprint of 5 cars.

The machine learning workflow [2] typically contains “data ori-
ented” stages like data preprocessing, cleaning and exploratory
data analysis. The use of relational databases for these stages come

with a lot of limitations. Oftentimes, the data collected would not
be well structured making it difficult to work with database queries
[44]. Writing complex queries would require a strong familiarity
with the schema which is not easy in case of wide tables with many
columns [14]. Dataframes have been introduced as an alternative to
overcome some of these limitations. Dataframe is a data structure
that organizes data into rows and columns like a spreadsheet. Over
the years, the use of dataframes has become widely popular as
they provide an intuitive way of storing and working with data
[33, 44]. They are convenient for use in REPL style imperative inter-
faces and data science notebooks [32]. Several libraries [5, 27, 36]
have been developed to enable the handling and manipulation of
dataframes. These libraries provide a convenient interface to work
with dataframes and are widely used for exploratory analysis, data
ingest, data preparation and feature engineering. Dataframes of-
fer several features including implicit ordering on both rows and
columns by treating them symmetrically. The dataframe manipula-
tion libraries offer data analysis modalities that include relational
operators like filter, join, transpose and spreadsheet-like operators
such as pivot.

The dataframe manipulation libraries are used in the data ori-
ented stages of the machine learning workflow. Pandas, the most
popular dataframe processing library for instance, has been down-
loaded over 300 million times1 with its Github repository starred
over 33k times as of April 20222. Machine learning however, has
become very data intensive in recent years. As the size of the data
gets bigger, it cannot fit in the system RAM leading to the need for
more efficient, out-of-core dataframe processing methods.This has
prompted the need for development of faster and scalable dataframe
processing methods. This has led to the development of scalable
ways of dataframe processing using the libraries such as Dask, Vaex,
RAPIDS etc. These libraries enable scaling either through paral-
lelization or through the use of efficient computation techniques
and memory usage. While the emphasis in these libraries remains
focused on achieving faster processing and scalability, energy con-
sumption aspect of these libraries is typically ignored.

In the recent years, energy consumption of the software systems
have gained a major attention from the researchers. Several works
have observed a link between design of a software and its energy
consumption [10, 18]. The lack of knowledge about the energy
implications of poor design choices have also been observed in a
study [34]. In the domain of machine learning, the computation and
energy demands have been increasing exponentially which would
require a larger energy production. This has the potential to cause a

1https://pypistats.org/packages/pandas
2https://github.com/pandas-dev/pandas

 
 
 
 
 
 
significant portion of the carbon emissions [13]. Owing to this many
researchers have focused on energy efficiency of machine learning.
There has been a focus on measuring the energy consumption of
machine learning models [24, 39], reducing the energy requirements
of training [19, 40] and inferencing [39]. However, the work on
energy efficient machine learning have largely ignored the data
oriented stages of the machine learning pipeline. To fill this gap, as a
first step, we perform an exploratory study on energy consumption
of dataframe libraries.

In this paper, we perform an empirical evaluation of the energy
consumption of three popular libraries namely Pandas3, Vaex4 and
Dask5 for four categories of tasks. Although Dask is not a library
specifically mean for working with dataframes, it does contain a
dataframe module that can be perform operations on dataframes
efficiently using computation graphs [36]. The four types of tasks in-
clude input-output operations, handling missing data, row/column
operations and statistical aggregation.

The rest of the paper is organized as follows. We provide a brief
description about the libraries used in our study in Section 2. The
tasks addressed in the study are discussed in Section 3. The datasets
used in the study are presented in Section 4 The experimental
settings and the procedure followed are described in Section 5. The
results of the experiments are presented in Section 6. Section 7
discusses the potential threats that may impact the validity of our
study. The discussion, related work and conclusion are presented
in Section 8, Section 9 and Section 10 respectively.

2 DATAFRAME PROCESSING LIBRARIES
The data used in machine learning is normally structured in tabular
form called dataframes. Dataframe is a 2 dimensional labeled data
structure with columns of potentially different types. Dataframes
provide a common structure to work with data from domains such
as finance, statistics, social sciences among many others. Libraries
like Pandas and Vaex provide a rich set of functionalities to manipu-
late the dataframes that enable easier data cleaning and preparation.
Pandas [27] is an open source Python library for data manipu-
lation developed in the year 2008. It provides data structures and
operations for manipulation of tabular data. It was originally devel-
oped by Wes McKinney for working with financial data. In addition
to data manipulation, it can also be used for data analysis and visu-
alization. Vaex [5] on the other hand is a Python library for working
with lazy out of core dataframes on huge amounts of tabular data.
The project has over 7K stars and over 500 forks on GitHub as of
March 20226. Vaex claims to be an alternative to Pandas that has
a more efficient and faster implementation of data manipulation
functions using memory mapping, a zero memory copy policy, and
lazy computations.

Dask is an opens source library for parallel computing in Python.
Dask provides support for dataframe processing through a module
that has an interface similar to Pandas. The module implements
a large dataframe out of many Pandas dataframes [36]. It uses a
threaded scheduler that enables efficient computation on parti-
tioned datasets [36]. Dask minimizes the amount of data held in

3https://pandas.pydata.org/
4https://vaex.io
5https://docs.dask.org/en/stable/dataframe.html
6https://github.com/vaexio/vaex

Figure 1: Flowchart describing the experimental procedure
followed in the study based on Georgiou et al. [15]

memory while parallelizing the tasks using computation graphs
[36].

3 ADDRESSED TASKS
In order to compare the energy consumption of these libraries while
working with the dataframes, we used different libraries to perform
the same set of operations on three different datasets. We picked
some of the commonly performed operations on dataframes as
tasks for our evaluation. The operations included loading of the csv
file, dropping columns, conatenatination of dataframes, duplicate
removal, merging dataframes, subset, sampling, and aggregation op-
erations such as sum, median, min, max, unique. A brief description
of each of these tasks is given in Table 1.

Dataframe processing libraries can be used to perform several
operations on dataframes including read/write operations, visual-
ization, relational and statistical operations. In order to compare
the energy consumption of these libraries while working with the

2

dataframes, we used different libraries to perform the same set of
operations on three different datasets. we address a few frequently
performed tasks input/output operations, relational operation and
statistical operations.

3.1 Input/Output Operations
Dataframes can be read and stored in several formats including
CSV, HDF5, Parquet, JSON etc. The choice of the format is typically
decided based on the type of data being stored and the type of
operations being performed on the data. As an example, storing
dataframes in a format like Parquet can be benifitial when used in
case of big data systems like Hadoop or Spark. However, unlike
CSV, the storage format is also not human readable. Since our work
focuses on energy, we compare the energy consumption of input
and output operations on different file formats used to represent the
dataframes. Specifically we compare the energy consumption values
of reading and writing the dataframes in three popular formats
namely CSV, JSON and HDF5 using Pandas, Vaex and Dask libraries.
CSV stands for comma-separated values. CSVs store data as plain
text where each row is a line of columns separated by ",". It is very
widely used due to the fact that its human readable and can be
read from and written to by most data software. JSON stands for
JavaScript Object Notation is another human readable format that
uses text to store data using attribute-value pairs and arrays. Like
CSV, JSON can also be readily displayed and edited in simple editors.
It also has an advantage of being a preferred format for transmitting
data in web applications. HDF5 stands for Hierarchical Data Format
5 suited for storing and organizing large amounts of heterogeneous
data. The data is stored as an internal file-like structure which
provides the ability to randomly access different parts of the data.
HDF5 offers efficient storage and faster access when dealing with
huge data.

3.2 Handling missing data
Missing data is a very common problem in real-world datasets.
They are generally referred to as NA values in dataframes. Miss-
ing data is generally caused by lack of collection of existing data
or because of the non-existence of the data. Missing data in the
dataframes is undesirable as they generally interfere with the op-
erations and calculations being performed on rows/columns. This
makes handling of missing data a very frequently performed task in
data preprocessing. The missing data handling functions compared
in our study are found in Table 1

3.3 Row/Column operations
Dataframes represent the data in the form of a table consisting of
rows and columns. Dataframes support relational operations like
join, filter, groupby etc. They also support reshape and transpose
operations on the tables. This makes it easier to merge data obtained
from multiple sources in multiple dataframes and order them in
a desired way to enable a better exploratory analysis. They also
enable the users to apply functions on rows/columns that allow
modification and filtering of the data. The missing data handling
functions compared in our study are found in Table 1

3.4 Statistical Aggregation Operations
Statistical aggregation functions enable the users to perform sta-
tistical analysis of their data. They can be used for operations like
aggregating multiple values under a row or column using a sin-
gle summary value, computing the correlation and covariance be-
tween pairs of rows/columns, finding the unique occurrences, count
of rows/columns etc. These functions are frequently used in ex-
ploratory data analysis to get the summary or the description of
the dataset. The statistical aggregation functions compared in our
study are found in Table 1

4 DATASETS USED
In order to compare the energy consumption of the dataframe
processing libraries, we have to perform the same set of operations
on the same datasets. Towards this end, we relied on well known
standard datasets from UCI Machine learning repository7. The
energy consumption of the libraries may also vary based on the
size of the dataframe they process on. A library that may be more
energy efficient on a smaller dataframe may become less energy
efficient as the dataframes get bigger. In order to account for these
variations, we performed our experiments on 2 different datasets
of varying sizes from the UCI repository shown in Table 2. The
datasets used are briefly described below.

4.1 Adult Dataset
The dataset was extracted based on the 1994 census database. The
task associated with the dataset is the classification task to deter-
mine whether a person makes over 50K a year or not. The dataset
contains 48842 data points with 14 attributes. We would refer to
this dataset as D1 in the rest of the paper.

4.2 Drug Review Dataset
This dataset contains patient reviews on specific drugs along with
related health conditions. It also contains the 10 star patient rating
of the drug. The dataset has 215063 data points with 6 attributes.
We would refer to this dataset as D2 in the rest of the paper.

5 EXPERIMENTAL PROCEDURE
All the experiments regarding energy consumption are run on a
system with an intel i5 4200 four core processor equipped with a
frequency of 1.6 GHz. The system was equipped with a RAM of
8 GB. We used Python 3.8 as the programming language for all
our experiments. The system had Ubuntu 20.04 installed as the
operating system. We used Pandas version 1.2.4, Vaex version 4.8.0
and Dask version 2022.2.1 for our study.

To measure the energy consumption values, we used Intel’s Run-
ning Average Power Limit (RAPL) interface. Using the RAPL tool
is one of the most accurate ways to measure the global energy con-
sumption of the processor [12]. Several studies related to measuring
the energy consumption of the software systems have relied on the
RAPL interface [23, 29, 31]. The tool provides energy consumption
values of various power domains such as package, core, uncore,
power plane, DRAM etc. The energy consumption values of the
following domains were noted down during the experiments

7https://archive.ics.uci.edu/ml/index.php

3

Category
I/O operation
I/O operation
I/O operation
I/O operation
I/O operation

Task ID Task Name
csvin
Read CSV
jsonin
Read JSON
hdfin
Read HDF5
Write to CSV
csvout
jsonout Write to JSON
Write to HDF5
hdfout
Detect missing values Detect missing values in a given series object
isna
Drop missing values
dropna
Fill missing values
fillna
Replace values
replace
drop
Drop rows/columns
groupby Groupby Operation
concat
sort
merge
count
sum
mean
min
max
unique

Description
Read a comma-separated values (csv) file into dataframe
Read JSON string into a Dataframe
Read an heirarchical data format 5 (hdf5) file into dataframe
Write dataframe into a comma-separated values (csv) file
Write dataframe into a JSON file
Write dataframe into a heirarchical data format 5 (hdf5) file Handling missing data
Handling missing data
Handling missing data
Handling missing data
Handling missing data
Row/Column operations
Row/Column operations
Row/Column operations
Row/Column operations
Row/Column operations
Statistical Aggregation
Statistical Aggregation
Statistical Aggregation
Statistical Aggregation
Statistical Aggregation
Statistical Aggregation

Drop rows/columns containing missing values
Fill missing values with a default
Replace a value with a specified value
Drop specified labels from rows or columns
Group rows that have the same values together
Concatenate dataframes along a particular axis
Sort the values along an axis
Merge dataframes with a database-join style
Count the number of non-NA cells along an axis
Sum values along an axis
Average values along an axis
Find the minimum of the values along an axis
Find the maximum of the values along an axis
Find the set of unique values along the axis

Concatenation
Sort values
Merge dataframes
Count cells
Sum values
Average values
Minimum value
Maximum value
Unique values

Table 1: Tasks addressed in the study

Dataset Name Characteristics Associated Tasks
Adult

Multivariate
Multivariate,
Text

Classification
Classification,
Regression, Clustering

Durg Review

No. of Data Points No. of Attributes
48842

14

215063

6

Table 2: Datasets used for the study

• Package (PKG): This domain provides the energy consump-
tion values of all the cores, integrated graphics and un-
core components such as last level caches and memory con-
trollers.

• Core: The energy consumption values of all the CPU cores

are provided by this domain.

• Uncore: This domain provides the energy consumption val-
ues of all the caches, integrated graphics and the memory
controllers.

• DRAM: The energy consumption values corresponding to
the random access memory attached to the memory con-
troller is provided by this domin.

Since the PKG values are inclusive of core and non-core values,

we only report the PKG values in the results in Section 6.

The experimental procedure using a single library is represented
as a flowchart in Figure 1. At the start of the experiment, we shut
down all the processes that were not required to run the OS. This
was done to ensure minimal variation in the readings due to inter-
ference from the processes running in the background.

Power consumption measurement in the system can be influ-
enced by noise. So one of the methods used to minimize the effect
of the noise on the measurements is to run the same experiment
multiple times and take the mean value of the measurements for

all the experiments as followed in a study by Georgiou et al. [15].
Based on this, we ran the experiment for each operation 10 times
and recorded the mean value of the energy measurements as shown
in Figure 1. After running each task on a dataset, we let the system
remain idle for 30 seconds using the command “sleep”. This was
done in order to avoid the power tail states [4] influencing the
reading and allow the system to reach a stable state again before
the start of another task. At the end of each task, the RAPL readings
were stored in an excel sheet for further analysis. The information
included the time required to run each task along with the energy
consumption values of the components.

The experiment was performed using all the three libraries con-
sidered for the study. The mean values of all the trials for each task
was computed and noted down using automated python script. The
scripts used for the study along with the detailed energy consump-
tion values of the tasks for each trial is available here8.

6 RESULTS
This section discusses the results of the experiments measuring
energy consumption of 3 different libraries on various dataframe op-
erations. The energy consumption measurements for input/output

8https://anonymous.4open.science/r/esem2022-5D62

4

operations are shown in Table 3 and Table 4. The energy consump-
tion of operations on handling missing data is shown in Table 5. The
energy consumption values of table and statistical operations are
shown in Table 6 and Table 7 respectively. We use the abbreviation
PKG for the energy consumption values of all core and non-core
components of the processor. The RAPL interface also provides the
energy consumption of the core and non-core components sepa-
rately. Since those are covered under PKG, we do not report them
explicitly. We use RAM for energy consumption of the main mem-
ory. The RAPL interface provides the energy consumption values
in micro-joules (1 micro-joule = 10−6 Joules). However, we present
them in the results by converting them into milli-joules (1 mini-
joule = 10−3 Joules) and rounding them off to one decimal. This is
done to ensure better readability. The energy consumption values
in all the tables are mean values from 10 trials of each operation
performed as described in Section 5.

6.1 Input/Output Operations
Input Operations. The energy consumption of the dataframe
6.1.1
input operation in CSV, JSON and HDF5 formats for the datasets D1
and D2 are shown in Table 3. For the dataset D1, Vaex consumes the
highest amount of energy for reading the dataframes in all three
file formats. Dask consumes the least amount of energy for inputs
in CSV and HDF5 formats while Pandas consumed the least energy
for reading the dataframe in JSON format. For the dataset D2, we
observe that Vaex consumes the most energy for inputs in CSV and
JSON formats while Pandas consumes the most energy for input
in HDF5. However, unlike D1, Dask consumes lesser energy than
Pandas for input operations in JSON format as well along with CSV
and HDF5.However, we could not get values for HDF5 for input
and output operations for Dask library in the Drugs Review Dataset
due to technical glitches because of the size of the data.

Across all libraries and datasets, we can also observe that the
reading the dataframe stored as JSON is most expensive in terms
of energy. For Pandas, while reading D1, the energy consumption
remains comparable for CSV and HDF5 formats. However, as the
size of the dataset increases, HDF5 becomes less expensive. This
can be observed from the values for D2 with Pandas library in
Table 3. We can also observe that the energy consumption of CSV
inputs for Dask is not affected much by increase in the dataset
size. The reason could be because of the lazy execution [36] using
computation graphs where it only loads information about the
header and the datatypes [36].

6.1.2 Output Operations. The energy consumption of the dataframe
output operation in CSV, JSON and HDF5 formats for the datasets
D1 and D2 are shown in Table 4. For dataset D1, Dask consumes the
most energy while Pandas consumes the least energy for outputs in
all three formats. A similar observation can be made for the dataset
D2. The output of dataframe into a CSV file consumes the most
energy across Pandas and Vaex. However, for output using Dask
library, HDF5 is the most energy-expensive file format.

Figure 2: Cumulative energy consumption of missing data
operations of the libraries for both the datasets

(1) Overall, Vaex is the most energy-expensive library

for input operations.

(2) JSON is the most expensive of the three file formats

for dataframe input.

(3) The energy consumption of a CSV input remains
unaffected by the size of the dataset for Dask.
(4) Dask consumes the most energy for output opera-

tions while Pandas consumes the least.

(5) CSV is the most energy-expensive file format for

outputting a dataframe in Pandas and Vaex.

(6) HDF5 is the most energy-expensive file format for

output using Dask library.

6.2 Handling Missing Data
The energy consumption values of the missing data handling func-
tions are shown in Table 5. For the dataset D1, Pandas consumes
the highest energy for isna, dropna, and fillna operations. While
Dask consumes the highest energy for replace operation, it is very
close the energy consumption of Pandas with a difference of just
1.4 milli-joules. Vaex consumes the least energy for isna and re-
place while Dask consumes the least energy for dropna and fillna
operations.

For the dataset D2, Pandas consumes the most energy for all
operations. Vaex consumes the lowest energy for alloperations
except fillna. Dask is the least expensive library for fillna operation.
It must be noted that for isna and replace functions for both datasets,
the energy values of both Dask and Vaex are comparable with a
maximum difference between them being just 5.6 milli-joues. The
cumulative energy consumption of missing data operations of the
libraries for both the datasets is shown in Figure 2.

The following are some of the key highlights from the observa-

The following are some of the key highlights from the results

tion of the results of input/output operations.

on missing data handling operations.

5

File
format
CSV
JSON
HDF5

File
format
CSV
JSON
HDF5

Pandas

Adult Dataset
Vaex

Dask

Pandas

Vaex

Dask

Drugs Review Dataset

PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM

551.2
3178.7
565

62.5
432.4
55.7

65.1
154388.9
NA
Table 3: Mean values for energy consumption for dataframe input in different formats.

13042.2
160102.2
4272.1

9696.6
156434.5
5568.2

1327.6
11791.9
418.2

963.7
11384.1
804.7

62.6
3471.9
502.7

1030.4
3697.8
3379.2

95.1
480.1
283.1

6
465.2
48.8

6.1
11382.7
NA

Pandas

Adult Dataset
Vaex

Dask

Pandas

Vaex

Dask

Drugs Review Dataset

PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM PKG

DRAM

1673.1
774.1
288.7

113.6
76.5
34.9

2261.6
1515.7
1713.6

260
206.9
138.8

4473.8
3683.7
5336.7

460.2
416.8
684.1

28941.1
9797.8
3993.2

2229
1105.3
523.8

29620.6
12402.3
4732.7

2688.4
1600.6
611.1

193428.6
170093.8
NA

14391.7
13343
NA

Table 4: Mean values for energy consumption for dataframe output in different formats.

(1) Overall, Pandas is the most energy-expensive library

for missing data handling operations.

(2) Dask is the most energy-efficient library for fillna

operation on both the datasets.

(3) Vaex is the most energy-efficient library for isna

and replace operations on both datasets.

6.3 Row/Column Operations
The energy consumption values for the row/column operations are
shown in Table 6. For dataset D1, Vaex is the least energy-expensive
library on all operations except groupby. Both Pandas and Dask have
a significantly lower energy consumption for groupby operation
compared to Vaex. For the dataset D2, Vaex is the most energy
efficient library for drop and merge operations. Dask consumes the
least energy for sort operation. Similar to D1, Vaex is the most
energy-expensive library for groupby operation with both other
libraries having a significantly lower energy consumption.

We can also observe that despite the increase in the number of
rows in D2, the energy consumption of Dask does not change very
significantly except in the case of merge. The size of the dataset also
does not seem to have much influence on gropuby operation on any
of the libraries. The energy consumption values for the remaining
operations increases with the increase in the dataset size for the
Pandas library.

The following are the key highlights from the results on row/-

column operations

Figure 3: Cumulative energy consumption of statistical ag-
gregation operations of the libraries for both the datasets

(1) Vaex is the most energy-efficient library for drop,

concat, and merge operations.

(2) Vaex is the most energy-expensive framework for

groupby operation.

(3) The energy consumption of Dask does not change
very significantly despite change in the size of the
dataset.

(4) For the dataset D2, Pandas is the most energy-
expensive library for all operations other than
groupby.

6.4 Statistical Aggregation Operations
The energy consumption values for the statistical aggregation op-
erations performed on the datasets is shown in Table 7. For the
dataset D1, we observe that Dask consumes the highest energy for
all operations. Of all the operations considered, count is the most

6

Operation
isna
dropna
fillna
replace

Operation
drop
groupby
concat
sort
merge

Adult Dataset

Vaex

Dask

Pandas

Pandas
PKG DRAM PKG DRAM PKG DRAM PKG
184.6
26.2
1329.9
392.8
1263
411.5
124.7
11.2

10.6
1
324.5
3
34.6
3
9.9
0.9
Table 5: Mean values for energy consumption for missing data handling operations.

DRAM PKG DRAM PKG DRAM
21.6
111.1
99.7
20

9.5
137.7
155.7
5.4

2.3
26.3
29.3
1

0.9
13.8
10.4
0.4

10
77
63.4
4.6

10.6
39
41
11.8

0.9
45.3
3.1
0.7

0.7
7.9
4.4
0.4

Drugs Review Dataset

Vaex

Dask

Adult Dataset

Drugs Review Dataset

Vaex

Dask

Pandas

Pandas
PKG DRAM PKG DRAM PKG DRAM PKG
93.5
27.8
3.3
5.5
3.3
0.4
462.1
102.2
9.8
303.8
57.6
2.9
4249.3
17.6
498.5
Table 6: Mean values for energy consumption for missing row/column operations.

DRAM PKG DRAM PKG DRAM
20
0.5
92.1
48.3
534.6

12
245.7
27.8
30.1
46.5

6.3
272.2
16.7
92.1
24.2

33.6
3.5
105.3
30.6
154.4

44.3
5.1
139.9
38.5
257

0.8
26.9
2.8
2.4
3.4

3.6
0.5
15.5
7.2
43.9

0.6
35.3
1.6
8.7
1.9

2.5
0.3
7.4
2.2
10.4

Dask

Vaex

energy-expensive operation across all libraries. The sum, mean,
min, max and unique are the cheapest in Pandas followed by Vaex.
For the count operation however, Vaex consumes the least energy.
For the dataset D2, a similar observation can be made where
Dask is the most energy-expensive library for all the operations.
The count operation is the most expensive one for Pandas and Dask.
For Vaex however, unique is the most energy-expensive operation.
Like in case of D1, Pandas is the least expensive library for all the
operations other than count. Vaex is the least expensive operation
for count operation.

The cumulative energy consumption of statistical aggregation
operations of the libraries for both the datasets is shown in Figure
3.

The following are the key highlights from the results on statisti-

cal aggregation operations.

(1) Dask is the most expensive library for statistical

aggregation operations on both datasets.

(2) Pandas is the most energy-efficient library for all

operations considered other than count.

(3) Vaex is the most energy-efficient library for count

operation.

minimal impact of these factors, eliminating their effect completely
is very difficult as it would require us to have control over the back-
ground operations that are needed to run the operating system.Due
to hardware constraints, we used a system with basic configuration
to run our experiments. The possibility of obtaining significantly
different values on a system with a different configuration must
also be considered.

Another factor that may influence the results of our study is the
choice of datasets. Libraries such as Vaex and Dask are optimized for
computations and memory usage. The optimizations may influence
the amount of work done for performing a certain operation based
on the nature of attributes, type and size of the data in the dataset
thereby influencing energy readings. To minimize its influence,
we performed our experiments with two datasets of different of
varying size and attributes.

The results that we obtained were from the time we set up the
study. Open source libraries are often developed by people across
the globe. The "competition" between different libraries would mean
that the implementations would get more efficient over time. Thus,
the results of the experiment obtained in the future for similar
experiments may vary based on the modifications to the libraries.
Keeping this in mind, we have provided the versions of the libraries
used in Section 5.

7 THREATS TO VALIDITY
In this section, we discuss potential systematic errors in our work
that may pose threats to the outcome of our study.

One of the most significant threats to the validity of our study
are the possible impact of noise, voltage spikes, daemons and other
background processes that may cause inaccurate energy measure-
ment We tried to minimize this issue by repeating the same task 10
times and averaging the recorded values. We also made sure to have
the system idle for 30 seconds before the start of the next task to
avoid the impact of tail states. Despite the best care taken to ensure

8 DISCUSSION
This paper is our first step towards exploring energy efficiency of
the data oriented stages of the machine learning pipeline. Through
this work, we also aim to call the attention of the research commu-
nity to work towards energy efficiency of the data oriented stages
of the machine learning pipeline. The results of the study indicate
that the choice of libraries for a given task and the format of data
storage could potentially influence the energy consumption of the
dataframe processing. As an example, based on the results on sta-
tistical aggregation operations, we can see that using Dask can be

7

Adult Dataset

Drugs Review Dataset

Operation
count
sum
mean
min
max
unique

Pandas

Vaex

Dask

Pandas
PKG DRAM PKG DRAM PKG DRAM PKG
393.7
5.6
2.4
3.3
3.4
4.2

1075
379.5
380.5
354.9
344.8
312

100.5
40.4
40.4
38.9
38.2
35.2

25.8
0.4
0.1
0.3
0.3
0.2

77.2
34.6
54
40
40.2
56.2

1218.5
7.5
28.7
6.8
4.7
99.6

6.2
3.3
5.3
3.6
3.4
5.9

Vaex

Dask
DRAM PKG DRAM PKG
84.4
0.8
3.9
0.6
0.6
7.3

33.4
35.2
46.2
50
34.5
407.5

11816.3
5222
5265.4
5185.2
5175.5
5312.9

2.6
3.2
4.4
5.5
3.3
39.3

DRAM
1224.6
551.4
567.9
551.8
532.1
562.6

Table 7: Mean values for energy consumption for missing statistical aggregation operations.

a poor choice in terms of energy when aggregation operations are
performed very frequently. Similarly, when we have too many and
too large dataframes to read, Dask can be the most energy-efficient
choice.

Although we have explored the energy consumption of different
libraries on a set of tasks, it must be noted that there may exist an
alternate ways or functions to accomplish the same task in a library.
As an example, many of the missing data handling operations could
also be performed using the apply9 function. The energy consump-
tion of these alternate methods may be different and they may
be more energy-efficient or energy-hungry. However, the current
work does not focus on such cases.

We also observe in some cases that the energy consumption
values of a task on a larger dataset (D2) is lesser compared to the
values for the smaller dataset (D1). Few examples of this include the
merge operation for Vaex and Dask, drop operation for Dask, and
concat operation for Vaex. Although it may look counter-intutive,
we argue a possible reason for this may be because of other factors
such as the number of attributes or the inner working of these
libraries. The influence of such factors could be explored further.

9 RELATED WORK
In recent years, the energy consumption of software elements have
caught the attention of researchers. This has led the researchers to
explore and study the energy consumption of software components
across various domains with the intention gaining insights that
help the developers make energy efficient choices. Measuring the
energy consumption and comparing the variations for different
available options in the is a popular theme among such studies.

Rouvoy et al. [37] performed an empirical investigation of dif-
ferences in energy consumption of read/write methods of some of
the famous Java libraries. They found some methods to be more
efficient than others. Hasan et al. [17] studied the energy profiles
of various Java collection classes and found that choice of an inef-
ficient collection can cost as much as 300% more energy. Schuler
et al. [41] studied the relation between System API utilization and
energy consumption in third-party software libraries. Ournani et al.
[29] compared the energy consumption of 27 Java I/O methods for
different file sizes and found that the energy consumption varies
across APIs with some of them consuming about 30% less energy
than the others. Pereira et al. [31] analysed and compared the en-
ergy efficiency across 27 different programming languages. They

9https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html

8

also explored how energy consumption relates to speed and mem-
ory utilization in these languages [31]. Kumar et al. [23] analyzed
the energy consumption of Java command line options and found
Oracle JDK to be more energy efficient than Open JDK. They also
found that UseG1GC and Xint were the most and the least energy
efficient command line options respectively [23]. Maleki et al. [26]
studied the impact of OOP design patterns on energy efficiency
and found that the use of overloading and decorator patterns can
degrade the energy efficiency of the application.

Energy efficiency is of a critical importance in battery powered
devices such as mobile phones. To this end, exploratory and com-
parative studies directed towards energy efficiency have also been
performed in this area [1, 8, 9, 11, 16, 38]. Gupta et al. [16] explored
the energy consumption of software modules and the impact of
co-occurrences of modules on the change in energy consumption
values on a windows phone. They also explored the anomalies in
power traces. Aggarwal et al. [1] investigated the relationship be-
tween the system call invocation and energy consumption across
multiple versions of Android application. Chowdhury et al. [9]
compared the energy consumption of HTTP/2 with its predecessor
HTTP/1.1 for mobile apps found that the former outperforms the
latter in most scenarios. Chowdhury et al. [8] studied the energy
impact of logging on Android applications and found that limited
logging has little to no impact on energy consumption. Cruz et
al. [11] analyzed the energy consumption of eight UI automation
frameworks and found that certain frameworks can increase the
energy consumption by over 2000 percent. Sahin et al. [38] explored
the energy impact of code obfuscation by studying the impact of 18
obfuscations for 21 usage scenarios accross 11 Android applications
on four different mobile phone platforms. They found that obfusca-
tions have a significant impact on energy consumption and is more
likely to increase energy consumption than decrease it [38].

Energy efficiency is crucial in server systems and cloud applica-
tions due to its economic and ecological costs. Due to this, many
of the-efficiency energy studies have been directed towards this
domain [20, 35, 42]. Singh et al. [42] explored the energy costs of
running several Java APIs on servers to accomplish a set of tasks and
found that the developers can reduce the energy costs of server by
choosing energy-efficient APIs. Procaccianti et al. [35] performed
an empirical investigation of two energy efficient software practices
namely "sleep" and "use of efficient queries" and found that they
can reduce energy consumption by upto 25%. Khomh et al. [20]
explored the energy consumption of 6 cloud patterns and found
that they can be energy efficient in some cases.

In the domain of machine learning, several researchers have
been working towards improving the energy efficiency. However,
the methods mainly remain focused on the machine learning model
itself. The methods include model compression techniques like
quantization [24, 28], pruning [46], distillation [39], use of efficient
hardware accelerators [7, 21, 30] and use of efficient computation
methods in the hardware [19, 40]. However, the stages of the ma-
chine learning pipeline that focus on the data preprocessing and
cleaning have largely been ignored. Our exploratory study serves a
step in that direction.

10 CONCLUSION AND FUTURE WORK
In this paper, we presented an exploratory analysis of energy con-
sumption of dataframe processing libraries. This work is intended
to be our first study towards the energy efficiency in the data ori-
ented stages of the machine learning pipeline which involves data
preprocessing, data cleaning and exploratory data analysis.

We analyzed the energy consumption of three different dataframe
processing libraries, namely Pandas, Vaex and Dask for different
dataframe processing tasks. The tasks included i/o operations, han-
dling missing data, row/column operations and statistical aggre-
gation operations as shown in Table 1. We used the Intel’s RAPL
interface to measure the energy consumption of the operations
using experimental setttings described in Section 5. We used two
datasets from the UCI repository, namely Adult Dataset and the
Drug Review dataset to run our experiments. The results of our
experiments contained some interesting observations summarized
in Section 6. The results show us that the choice of library and
the format of the data storage can indeed influence the energy
consumption of the data preprocessing and data cleaning stages.
This choice may be based on factors such as the type of operations
performed more frequently, the format in which data is available
and the size of the dataset.

An additional goal of this work is to also call the attention of the
research community to focus on the energy efficiency in the stages
of the machine learning pipeline other than model training and
inferencing. A more comprehensive analysis of energy consumption
of the data oriented stages could potentially lead to the discovery
of energy efficient practices in data preprocessing, data cleaning
and exploratory analysis stages of the pipeline. We have merely
scratched the surface of this area and we plan several extensions
to the study. In the future, we plan to further explore the energy
consumption of dataframe processing libraries on multiple systems
with different configurations to get more generalized results. We
also plan to repeat the experiments on several larger datasets to
explore trends with respect to the energy consumption values for
different tasks. The influence of the data types in the dataframe on
energy could also be a possible future extension. The list of tasks
considered in the study is limited. Given that the libraries have
many more functionalities, their energy consumption could also be
explored in the future.

REFERENCES
[1] Karan Aggarwal, Chenlei Zhang, Joshua Charles Campbell, Abram Hindle, and
Eleni Stroulia. 2014. The power of system call traces: predicting the software
energy consumption impact of changes.. In CASCON, Vol. 14. 219–233.

[2] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.

9

Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st
International Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP). IEEE, 291–300.

[3] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric
Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang
Chen, et al. 2016. Deep speech 2: End-to-end speech recognition in english and
mandarin. In International conference on machine learning. PMLR, 173–182.
[4] James Bornholt, Todd Mytkowicz, and Kathryn S McKinley. 2012. The model is
not enough: Understanding energy consumption in mobile devices. In 2012 IEEE
Hot Chips 24 Symposium (HCS). IEEE, 1–3.

[5] Maarten A Breddels and Jovan Veljanoski. 2018. Vaex: big data exploration in

the era of gaia. Astronomy & Astrophysics 618 (2018), A13.

[6] Ruichu Cai, Boyan Xu, Xiaoyan Yang, Zhenjie Zhang, Zijian Li, and Zhihao Liang.
2017. An encoder-decoder framework translating natural language to database
queries. arXiv preprint arXiv:1711.06061 (2017).

[7] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. 2016. Eyeriss:
An energy-efficient reconfigurable accelerator for deep convolutional neural
networks. IEEE journal of solid-state circuits 52, 1 (2016), 127–138.

[8] Shaiful Chowdhury, Silvia Di Nardo, Abram Hindle, and Zhen Ming Jack Jiang.
2018. An exploratory study on assessing the energy impact of logging on android
applications. Empirical Software Engineering 23, 3 (2018), 1422–1456.

[9] Shaiful Alam Chowdhury, Varun Sapra, and Abram Hindle. 2016. Client-side
energy efficiency of HTTP/2 for web and mobile app developers. In 2016 IEEE
23rd international conference on software analysis, evolution, and reengineering
(SANER), Vol. 1. IEEE, 529–540.

[10] Luis Cruz and Rui Abreu. 2019. Catalog of energy patterns for mobile applications.

Empirical Software Engineering 24, 4 (2019), 2209–2235.

[11] Luis Cruz and Rui Abreu. 2019. On the energy footprint of mobile testing
frameworks. IEEE Transactions on Software Engineering 47, 10 (2019), 2260–2271.
[12] Howard David, Eugene Gorbatov, Ulf R Hanebutte, Rahul Khanna, and Christian
Le. 2010. RAPL: Memory power estimation and capping. In 2010 ACM/IEEE
International Symposium on Low-Power Electronics and Design (ISLPED). IEEE,
189–194.

[13] Ottmar Edenhofer. 2015. Climate change 2014: mitigation of climate change. Vol. 3.

Cambridge University Press.

[14] Sneha Gathani, Peter Lim, and Leilani Battle. 2020. Debugging database queries:
A survey of tools, techniques, and users. In Proceedings of the 2020 CHI Conference
on Human Factors in Computing Systems. 1–16.

[15] Stefanos Georgiou, Maria Kechagia, Tushar Sharma, Federica Sarro, and Ying
Zou. 2022. Green AI: Do Deep Learning Frameworks Have Different Costs? ACM:
Association for Computing Machinery.

[16] Ashish Gupta, Thomas Zimmermann, Christian Bird, Nachiappan Nagappan,
Thirumalesh Bhat, and Syed Emran. 2011. Detecting energy patterns in soft-
ware development. Microsoft Research Microsoft Corporation One Microsoft Way
Redmond, WA 98052 (2011).

[17] Samir Hasan, Zachary King, Munawar Hafiz, Mohammed Sayagh, Bram Adams,
and Abram Hindle. 2016. Energy profiles of java collections classes. In Proceedings
of the 38th International Conference on Software Engineering. 225–236.

[18] Abram Hindle, Alex Wilson, Kent Rasmussen, E Jed Barlow, Joshua Charles
Campbell, and Stephen Romansky. 2014. Greenminer: A hardware based mining
software repositories software energy consumption framework. In Proceedings of
the 11th working conference on mining software repositories. 12–21.

[19] Xun Jiao, Vahideh Akhlaghi, Yu Jiang, and Rajesh K Gupta. 2018. Energy-efficient
neural networks using approximate computation reuse. In 2018 Design, Automa-
tion & Test in Europe Conference & Exhibition (DATE). IEEE, 1223–1228.

[20] Foutse Khomh and S Amirhossein Abtahizadeh. 2018. Understanding the impact
of cloud patterns on performance and energy consumption. Journal of Systems
and Software 141 (2018), 151–170.

[21] Jong Hwan Ko, Burhan Mudassar, Taesik Na, and Saibal Mukhopadhyay. 2017.
Design of an energy-efficient accelerator for training of convolutional neural
networks using frequency-domain computation. In 2017 54th ACM/EDAC/IEEE
Design Automation Conference (DAC). IEEE, 1–6.

[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012).

[23] Mohit Kumar and Weisong Shi. 2019. Energy consumption analysis of java
command-line options. In 2019 Tenth International Green and Sustainable Com-
puting Conference (IGSC). IEEE, 1–8.

[24] Edward H Lee, Daisuke Miyashita, Elaina Chai, Boris Murmann, and S Simon
Wong. 2017. Lognet: Energy-efficient neural networks using logarithmic com-
putation. In 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 5900–5904.

[25] Sun-Ro Lee, Min-Jae Heo, Chan-Gun Lee, Milhan Kim, and Gaeul Jeong. 2017.
Applying deep learning based automatic bug triager to industrial projects. In
Proceedings of the 2017 11th Joint Meeting on foundations of software engineering.
926–931.

[26] Sepideh Maleki, Cuijiao Fu, Arun Banotra, and Ziliang Zong. 2017. Understanding
the impact of object oriented programming and design patterns on energy effi-
ciency. In 2017 Eighth International Green and Sustainable Computing Conference
(IGSC). IEEE, 1–6.

[27] Wes McKinney et al. 2011. pandas: a foundational Python library for data analysis
and statistics. Python for high performance and scientific computing 14, 9 (2011),
1–9.

[28] Bert Moons, Bert De Brabandere, Luc Van Gool, and Marian Verhelst. 2016.
Energy-efficient convnets through approximate computing. In 2016 IEEE Winter
Conference on Applications of Computer Vision (WACV). IEEE, 1–8.

[29] Zakaria Ournani, Romain Rouvoy, Pierre Rust, and Joel Penhoat. 2021. Comparing
the Energy Consumption of Java I/O Libraries and Methods. In 37th International
Conference on Software Maintenance and Evolution (ICSME).

[30] Eunhyeok Park, Dongyoung Kim, and Sungjoo Yoo. 2018. Energy-efficient neural
network accelerator based on outlier-aware low-precision computation. In 2018
ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA).
IEEE, 688–698.

[31] Rui Pereira, Marco Couto, Francisco Ribeiro, Rui Rua, Jácome Cunha, João Paulo
Fernandes, and João Saraiva. 2017. Energy efficiency across programming lan-
guages: how do energy, time, and memory relate?. In Proceedings of the 10th ACM
SIGPLAN International Conference on Software Language Engineering. 256–267.
[32] Fernando Perez and Brian E Granger. 2015. Project Jupyter: Computational
narratives as the engine of collaborative data science. Retrieved September 11,
207 (2015), 108.

[33] Devin Petersohn. 2021. Dataframe Systems: Theory, Architecture, and Implemen-
tation. Ph. D. Dissertation. Ph. D. Dissertation. EECS Department, University of
California, Berkeley . . . .

[34] Gustavo Pinto and Fernando Castor. 2017. Energy efficiency: a new concern for

application software developers. Commun. ACM 60, 12 (2017), 68–75.

[35] Giuseppe Procaccianti, Héctor Fernández, and Patricia Lago. 2016. Empirical eval-
uation of two best practices for energy-efficient software development. Journal
of Systems and Software 117 (2016), 185–198.

[36] Matthew Rocklin. 2015. Dask: Parallel computation with blocked algorithms and
task scheduling. In Proceedings of the 14th python in science conference, Vol. 130.

Citeseer, 136.

[37] Romain Rouvoy, Pierre Rust, and Joel Penhoat. 2021. Comparing the Energy
Consumption of Java I/O Libraries and Methods. In ICSME 2021-37th International
Conference on Software Maintenance and Evolution.

[38] Cagri Sahin, Mian Wan, Philip Tornquist, Ryan McKenna, Zachary Pearson,
William GJ Halfond, and James Clause. 2016. How does code obfuscation impact
energy usage? Journal of Software: Evolution and Process 28, 7 (2016), 565–588.

[39] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 (2019).

[40] Syed Shakib Sarwar, Swagath Venkataramani, Aayush Ankit, Anand Raghu-
nathan, and Kaushik Roy. 2018. Energy-efficient neural computing with approxi-
mate multipliers. ACM Journal on Emerging Technologies in Computing Systems
(JETC) 14, 2 (2018), 1–23.

[41] Andreas Schuler and Gabriele Anderst-Kotsis. 2020. Characterizing energy con-
sumption of third-party API libraries using API utilization profiles. In Proceedings
of the 14th ACM/IEEE International Symposium on Empirical Software Engineering
and Measurement (ESEM). 1–11.

[42] Jasmeet Singh, Kshirasagar Naik, and Veluppillai Mahinthan. 2015. Impact of
developer choices on energy consumption of software on servers. Procedia
Computer Science 62 (2015), 385–394.

[43] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and Policy
Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics. 3645–3650.

[44] Yifan Wu. 2020. Is a dataframe just a table?. In 10th Workshop on Evaluation
and Usability of Programming Languages and Tools (PLATEAU 2019). Schloss
Dagstuhl-Leibniz-Zentrum für Informatik.

[45] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
2016. Google’s neural machine translation system: Bridging the gap between
human and machine translation. arXiv preprint arXiv:1609.08144 (2016).
[46] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. 2017. Designing energy-efficient
convolutional neural networks using energy-aware pruning. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 5687–5695.

10

