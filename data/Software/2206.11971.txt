Noname manuscript No.
(will be inserted by the editor)

Looking for related discussions on GitHub
Discussions

M´arcia Lima* · Igor Steinmacher · Denae
Ford · Evangeline Liu · Grace Vorreuter ·
Tayana Conte · Bruno Gadelha

2
2
0
2

n
u
J

3
2

]
E
S
.
s
c
[

1
v
1
7
9
1
1
.
6
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Software teams are increasingly adopting diﬀerent tools and commu-
nication channels to aid the software collaborative development model and co-
ordinate tasks. Among such resources, Programming Community-based Question
Answering (PCQA) forums have become widely used by developers. Such envi-
ronments enable developers to get and share technical information. Interested in
supporting the development and management of Open Source Software (OSS)
projects, GitHub announced GitHub Discussions — a native forum to facilitate
collaborative discussions between users and members of communities hosted on
the platform. As GitHub Discussions resembles PCQA forums, it faces challenges
similar to those faced by such environments, which include the occurrence of re-
lated discussions (duplicates or near-duplicated posts). While duplicate posts have
the same content—and may be exact copies—near-duplicates share similar topics
and information. Both can introduce noise to the platform and compromise project
knowledge sharing. In this paper, we address the problem of detecting related posts
in GitHub Discussions. To do so, we propose an approach based on a Sentence-
BERT pre-trained model: the RD-Detector. We evaluated RD-Detector using data

*Corresponding Author : M´arcia Lima
Federal University of Amazonas (UFAM) & Amazonas State University (UEA)
Tel.: +55 92 3305-1181
Present address: Av. Rodrigo Otavio - 6200. Manaus-AM-Brazil
E-mail: marcia.lima@icomp.ufam.edu.br;msllima@uea.edu.br

Igor Steinmacher
Federal Technological University of Paran´a (UTFPR)
E-mail: igorfs@utfpr.edu.br

Denae Ford
Microsoft Research
E-mail: denae@microsoft.com

Evangeline Liu & Grace Vorreuter
GitHub
E-mail: evi-liu;gracevor@github.com

Tayana Conte & Bruno Gadelha
Federal University of Amazonas (UFAM)
E-mail: tayana;bruno@icomp.ufam.edu.br

 
 
 
 
 
 
2

Lima et al.

from diﬀerent OSS communities. OSS maintainers and Software Engineering (SE)
researchers manually evaluated the RD-Detector results, which achieved 75% to
100% in terms of precision. In addition, maintainers pointed out practical applica-
tions of the approach, such as merging the discussions’ threads and making discus-
sions as comments on one another. OSS maintainers can beneﬁt from RD-Detector
to address the labor-intensive task of manually detecting related discussions and
answering the same question multiple times.

Keywords Communication tool · GitHub Discussions forum · Knowledge
sharing · OSS communities · Related discussions · Sentence-BERT · Software
teams interaction

1 Introduction

Software engineering teams actively adopt social resources to support the collab-
orative software development and coordinate team members’ tasks (Storey et al.,
2014, 2016; Tantisuwankul et al., 2019). Teams use such resources to communi-
cate, learn, answer questions, obtain and give feedback, show results, manage,
and coordinate activities (Storey et al., 2016). E-mail, chats, and forums are ex-
amples of collaborative social media that support software teams’ communica-
tions (Storey et al., 2014, 2016; P´erez-Soler et al., 2018). In recent years, Pro-
gramming Community-based Question Answering (PCQA) has increasingly at-
tracted diﬀerent users’ attention and has become widely used by software develop-
ers (Wang et al., 2020; Pei et al., 2021; Ford et al., 2018). Developers rely on PCQA
to quickly ﬁnd answers for technical questions (Mamykina et al., 2011; Zhang et al.,
2017), impacting the software development process (Yazdaninia et al., 2021).

Concerned with helping the development and management of OSS projects
hosted on GitHub, in 2020, the company announced GitHub Discussions
(Liu,
2022; Hata et al., 2022). In order to be “A new way for software communities to
collaborate outside the codebase” (Niyogi, 2020), the GitHub Discussions forum
provides opportunities for OSS communities to interact and discuss project-related
issues collaboratively. The Discussions forum is a place where OSS communities
can talk about work, ask questions, plan new releases, request code reviews, make
announcements, disclose information, recruit contributors, get insights into the
project, feature important information, or simply chat (Hata et al., 2022; Liu,
2022).

However, the PCQA type forums, like any other question and answering fo-
rums, should be concerned about the quality decay of their contents. Silva et al.
(2018) point out that repeated questions put at risk that quality. Prior research
handles this problem by focusing on automating the duplicate question detection
in PCQA forums (Zhang et al., 2015; Ahasanuzzaman et al., 2016; Zhang et al.,
2017; Silva et al., 2018; Wang et al., 2020; Pei et al., 2021). Research that aims to
detect duplicate questions in Stack Overﬂow highlights that duplicates can (1) pol-
lute the platform with already-answered questions (Silva et al., 2018); (2) consume
the time of experts, as they must manually analyze and look for duplicates (Zhang
et al., 2017; Wang et al., 2020); and (3) make users wait unnecessarily for answers
to questions that had been already asked and answered (Ahasanuzzaman et al.,
2016).

Looking for related discussions on GitHub Discussions

3

Although both GitHub Discussions and Stack Overﬂow resemble a PCQA fo-
rum, they diﬀer regarding the communicative intentions behind the posts. Posts
on GitHub Discussions
cover a software project ecosystem (Hata et al., 2022)
(on GitHub, each project has its own forum). Stack Overﬂow questions cover a
broader context; they aim to answer developers’ technical questions regardless of
a speciﬁc software project. However, text evidence collected from GitHub Discus-
sions
threads (discussion ﬁrst posts and comments) shows that the forum, like
Stack Overﬂow, contains duplicate and related posts123. Currently, maintainers
manually manage duplicates on the GitHub Discussions
forum. Based on their
previous knowledge, maintainers identify duplicates as new discussions arise. How-
ever, manual strategies are less eﬀective and are subject to human subjectivity and
imprecision (Ahasanuzzaman et al., 2016; Zhang et al., 2017; Wang et al., 2020).
Diﬀerent approaches have already been proposed and evaluated to solve the
problem of duplicates in PCQA forums. Such methods often use pre-labeled datasets
to train or optimize the duplicates detection process. However, the posts on GitHub
Discussions are project-related, and they are not previously categorized using pre-
deﬁned topics. In addition, diﬀerent project contexts emerge every time an OSS
community starts to use the Discussions forum. Therefore, creating a manual la-
beled sample is costly and can always be considered ineﬃcient due to human
imprecision, and small, given the diversity of projects hosted on GitHub. Thence,
we did not train or optimize domain-speciﬁc machine learning models. The dy-
namic growth of the GitHub Discussions and the diversity of projects hosted on
the platform create opportunities and challenges to develop an approach to detect
related discussions on GitHub Discussions.

Therefore, motivated by the context mentioned, this work aims to propose the
RD-Detector, an automated approach to detect related discussions in the GitHub
Discussions forum. More speciﬁcally, related discussions are those duplicated or
near-duplicated ones. Duplicate posts have the same content and could be exact
copies. Meanwhile, near-duplicates have similar topics and share similar informa-
tion. In this work, we consider duplicates a subtype of related discussions for
practical purposes. From now on, we use the term ’related discussions’ to refer
to duplicate and near-duplicate discussion posts. More speciﬁcally, our research
question (RQ) is:

RQ: Are general-purpose deep machine learning models to Natural Language
Processing (NLP) problems eﬀective in detecting related discussions in OSS com-
munities?

To do so, we propose the RD-Detector approach. RD-Detector uses a Sentence-
BERT (SBERT) pre-trained general-purpose model to create sentence embedding
representations of discussion posts and compute the semantic similarity between
pairs of discussions. The approach outputs a set of related discussion candidates.
The RD-Detector diﬀers from the previous approaches as (1) it does not rely on
pre-labeled datasets; (2) it does not rely on speciﬁc models to understand OSS com-
munities’ discussions context; (3) it can assess diﬀerent software contexts hosted
on GitHub; and (4) it aims for precision in detecting related discussion posts. OSS
maintainers and Software Engineering (SE) researchers evaluated the RD-Detector

1 https://github.com/homebrew/discussions/discussions/1531
2 https://github.com/homebrew/discussions/discussions/707
3 https://github.com/vercel/next.js/discussions/22211

4

Lima et al.

results. They both classiﬁed the pairs of related discussion candidates as related
or not and pointed to evidence on the challenges and practical applications in
detecting related discussion posts in the GitHub Discussions.

Our results show that we can use a general-purpose machine learning model
for detecting related discussions in OSS communities. We assessed RD-Detector
over diﬀerent datasets, achieving 75% to 100% in terms of precision. We identi-
ﬁed the imprecision of the term ‘related discussions’ as a limitation during the
evaluation. The general-purpose machine learning model brought ﬂexibility to the
approach. As an advantage, we can point out that the results does not restrict to a
speciﬁc software project context. OSS maintainers can beneﬁt from the results to
minimize the work overhead in manually detecting related threads and the rework
in answering the same question multiple times. Besides, the results support OSS
maintainers to tackle the platform pollution and to address the project knowledge
sharing degradation, as it occurs in diﬀerent discussions threads.

The main contributions of this research include:

– The RD-Detector, an approach based on deep machine learning models to

detect related posts in GitHub Discussions.

– Empirical evidence on using a general-purpose machine learning model to de-

tect related discussions held by OSS communities.

– Empirical evidence regarding the RD-Detector practical applications in OSS

communities, from OSS maintainers’ perspective.

2 Background

OSS projects are highly distributed environments usually composed of self-directed
development teams (Chen et al., 2013; Tantisuwankul et al., 2019). Therefore,
knowledge sharing is a critical factor for the success of OSS teams (Chen et al.,
2013; Tantisuwankul et al., 2019). Chen et al. (2013) point out that ‘knowledge
sharing is an interactive cuing process in which knowledge provided by one team
member becomes the cue for other members to retrieve relevant but diﬀerent
knowledge stored in their own memory.’ Given that the GitHub Discussions
is
a collaborative communication channel where OSS communities make questions,
debate, and announce project-related issues, we conjecture that the Discussions
is an online environment that promotes knowledge sharing in OSS communities.
Tantisuwankul et al. (2019) point out that the key to software projects hosted on
collaborative platforms such as GitHub success are the communities’ interaction
and the project knowledge sharing.

This section presents the main concepts used in this research and previous
works on detecting duplicates in PCQA forums. Section 2.1 describes the GitHub
Discussions forum (our object of study). Section 2.2 presents the concept of dupli-
cates from the researchers’ perspective and how they tackle duplicates or related
posts in programming Q&A forums.

2.1 GitHub Discussions

is a feature to any public or private repository on GitHub
GitHub Discussions
(Liu, 2022). It facilitates collaborative discussions among maintainers and the com-

Looking for related discussions on GitHub Discussions

5

munity for a project on GitHub (Liu, 2022). GitHub company suggests using the
Discussions forum to ask and answer questions, share information, make announce-
ments, and lead or participate in project-related conversations (GitHub, 2021a).
The Discussions is a collaborative communication forum for OSS maintainers, code
and non-code contributors, newcomers, and users to discuss projects’ use, devel-
opment, and updates in a single place without third-party tools (GitHub, 2022,
2021a). In addition, the Discussions stands out for being a place to distinguish
day-to-day conversations and conversations aimed at engineering teams (Issues or
Pull Request) (Hata et al., 2022).

Users, maintainers, contributors, and newcomers can join in a conversation
by creating, commenting, reacting, or reading a discussion post (GitHub, 2021a).
The Discussions users can also search selected topics in discussion posts (GitHub,
2021c). To do so, they must specify keywords in the GitHub search engine. Users
can restrict the search results to the discussion title, body text, or comments by
applying correct qualiﬁers.

Authors must specify the discussion title, body text, and category to create a
new discussion (Figure 1). The category is a mandatory attribute of a discussion
post. It helps organize conversations into predeﬁned classes, allowing community
members to chat in the right place and ﬁnd discussions with similar characteris-
tics (GitHub, 2021b). Authorized members can deﬁne, create, or delete categories
in a repository according to the project needs. By default, the GitHub Discus-
sions
forum provides ﬁve types of categories: Announcements, Q&A, Ideas, Show
and tell, and General (GitHub, 2021b). Maintainers can create Announcements
discussions to share project updates and news. Users can create Q&A discussions to
ask questions, suggest answers, and vote on the most appropriate feedback. Ideas
discussions can report or share ideas regarding the project improvements. Show and
tell discussions discuss relevant creations, experiments, or tests. Finally, General
discussions address any issue relevant to the project (GitHub, 2021b).
Maintainers report a positive acceptance of GitHub Discussions

in the OSS
communities (Liu, 2021). They highlight that the forum enables the growth of the
communities in the same place they use to collaborate, improve, and increase the
OSS community members’ engagement, and separate the issues trackers from ques-
tions, feature requests, and general chatting (Liu, 2021). In addition, maintainers
point out they can use the discussions’ threads to access historical data (Liu, 2021).
They can keep track of the questions already asked, their proposed solutions, and
the suggestions made. With this threading support, they can individually address
demands without losing them in broader discussions.

Liu (2022) highlights the use of GitHub Discussions by ﬁve selected OSS
communities: Dogecoin,4 NASA,5 Next.js,6 Pixar,7 and React.8 The Dogecoin
community uses GitHub Discussions
to centralize its developers’ conversations
in one place, NASA uses it to collaborate with the OSS community on its core
ﬂight systems. The Next.js community uses Discussions to plan features, pro-
mote community interaction, and exchange ideas about the project. The Pixar

4 https://github.com/dogecoin/dogecoin/discussions
5 https://github.com/nasa/cFS/discussions
6 https://github.com/vercel/next.js/discussions
7 https://github.com/PixarAnimationStudios/OpenTimelineIO/discussions
8 https://github.com/reactwg/react-18/discussions

6

Lima et al.

project uses the forum to engage the OSS community responsible for creating its
OpenTimelineIO API. Finally, React started using discussions to introduce a new
release and give collaborators a place to ask questions about it.

This work uses public discussion posts collected from three OSS communities
to evaluate our approach. Our dataset comprises the discussion posts of Gatsby,
Homebrew, and Next.js communities (Table 1).

Fig. 1: An example of a discussion post.

2.2 Related Work

Several studies aim to detect duplicates on PCQA forums by proposing diﬀerent
approaches for duplicate question detection (Zhang et al., 2015; Ahasanuzzaman
et al., 2016; Zhang et al., 2017; Silva et al., 2018; Wang et al., 2020; Pei et al., 2021).
Generally, researchers model the duplicate detection task as a ranking problem, a
binary classiﬁcation problem, or a combination of both. Ranking problems combine
features to sort the top-k most similar documents (discussions, texts, questions)
(Liu, 2011) and predict duplicates. Classiﬁcation problems classify pairs of docu-
ments into predeﬁned categories (e.g., duplicates or non-duplicates) (Ahasanuzza-
man et al., 2016; Zhang et al., 2017).

The literature characterize duplicate posts on PCQA forums as: (1) questions
that address the same topic, but are not necessarily identical copies (Silva et al.,
2018; Wang et al., 2020); (2) questions conceptually equivalent to other questions
previously posted (Abric et al., 2019); (3) questions that were already asked and
answered before (Zhang et al., 2017; Wang et al., 2020); (4) questions asked to solve
the same problem (Ahasanuzzaman et al., 2016); and (5) questions that ‘express
the same point’ (Zhang et al., 2015). We can note the duplicates conceptualization
is not a rigid deﬁnition; it can depend on human subjective criteria. Ahasanuzza-
man et al. (2016) point out that exact duplicates are signiﬁcantly rarer, and many
users duplicate questions by asking the same thing in diﬀerent ways.

Looking for related discussions on GitHub Discussions

7

Zhang et al. (2015) proposed DupPredictor, an approach to detect potential
duplicate questions in Stack Overﬂow. The DupPredictor combines the similar-
ity scores of diﬀerent features. The authors evaluated the approach’s eﬀective-
ness using a pre-labeled Stack Overﬂow dataset, which achieved 63.8% of recall
rate. Ahasanuzzaman et al. (2016) and Zhang et al. (2017) addressed duplicate
detection as a supervised classiﬁcation problem. The authors used a pre-labeled
dataset to train and validate a classiﬁer to detect duplicates in Stack Overﬂow and
measured the proposed approachs’ eﬀectiveness using the recall rate. Silva et al.
(2018) implemented the DupPredictorRep and DupeRep approaches. The authors
adopted an oﬃcial Stack Overﬂow dataset dump to train, optimize, and evaluate
the approaches that predict and classify questions as duplicate or not, respec-
tively. Wang et al. (2020) used deep learning techniques to detect duplicate ques-
tions in Stack Overﬂow. The authors proposed three diﬀerent approaches based on
Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and
Long Short-Term Memory (LSTM) to identify duplicates. The authors also used
a pre-labeled Stack Overﬂow dataset to train, evaluate, and test the classiﬁers
and measured the approaches’ eﬀectiveness using the recall rate. Finally, Pei et al.
(2021) proposed an Attention-based Sentence pair Interaction Model (ASIM) to
predict the relationship between Stack Overﬂow questions. The authors used a
Stack Overﬂow dump to train the software engineering specif domain. The pro-
posed model achieved 82.10% precision and 82.28% recall rates, outperforming the
baseline used.

Conversely, Abric et al. (2019) analyzed how helpful duplicate questions are to
the software development community. The authors argue that duplicates on Stack
Overﬂow help the developer community by providing diﬀerent formulations of the
same problem or solution. Also, the additional answers may be more understand-
able for some users.

Previous work has also highlighted the problem of duplicates in issue reports
and pull requests (PRs) on GitHub (Li et al., 2017; Yu et al., 2018; Li et al., 2018;
Ren et al., 2019; Li et al., 2020; Zhang et al., 2020). Li et al. (2018) analyzed
explicit links in both issues reports and PRs. They reported the importance of
such links in identifying duplicates. The authors analyzed 70,686 links that rep-
resented duplication relationships, from which 59.03% identiﬁed duplicate issues
and 40.97% identiﬁed duplicate pull requests. Li et al. (2020) presented empirical
evidence on the impact of duplicate PRs on the software development eﬀort. They
observed that the ’inappropriateness of OSS contributors’ work patterns and the
drawbacks of their collaboration environment would’ result in duplicates. In ad-
dition, researchers have been proposing diﬀerent approaches to address duplicates
in GitHub (Li et al., 2017; Yu et al., 2018; Ren et al., 2019; Zhang et al., 2020).
Li et al. (2017) and Ren et al. (2019) proposed automatic approaches based on
traditional Information Retrieval (IR) and NLP techniques to detect duplicates in
pull requests. Yu et al. (2018) constructed a dataset of historical duplicate PRs
extracted from projects on GitHub - the DupPR dataset - by using a semi-automatic
approach. Zhang et al. (2020) proposed the iLinker, an approach to detect related
issues in GitHub. The authors trained iLinker to learn the embedding corpus and
models from the project issue text.

The mentioned approaches to detect duplicates on PCQA forums have two
common points: the use pre-labeled datasets to learn or better understand the do-
main application, and their eﬀectiveness is measured using the Recall rate. How-

8

Lima et al.

ever, this research faces some challenges that make it diﬀerent from the previously
mentioned papers: (1) there is not a pre-labeled dataset of duplicated or related
discussions, extracted from GitHub Discussions , through which we can train or
optimize machine learning models; (2) creating a pre-labeled base of related dis-
cussions is a time-consuming task and can always be considered ineﬃcient and
small; and (3) there is a vast diversity of repositories hosted on GitHub, which
provide diﬀerent software project contexts (in January 2020 the platform already
hosted more than 200 million repositories9). Diﬀerent from the mentioned strate-
gies to detect duplicates in issue reports and pull requests, our approach is based
on general-purpose deep learning models to address the topic duplication problem
in Discussions. Therefore, in order to detect related discussion candidates (dupli-
cate or near-duplicate) in the GitHub Discussions forum, our proposed approach
(1) does not rely on a pre-labeled dataset; (2) does not rely on domain-speciﬁc
models; (3) applies to diﬀerent OSS project domains; (4) is based on a Sentence-
BERT pretrained model, and (4) aims at improving precision rates rather than
the recall rates. In Section 3, we propose a method to detect related discussions
in GitHub Discussions.

3 RD-Detector: Related Discussions Detector

In this section, we present the RD-Detector approach. The RD-Detector aims to
detect pairs of related discussion candidates in collaborative discussion forums
of OSS communities. To this end, the RD-Detector scores the Semantic Textual
Similarity (STS) between discussion posts to ﬁnd discussions with similar mean-
ings. Using the STS value, we can detect exact copies (duplicates) and discussions
that share the same topic (near-duplicates). The greater the similarity values, the
greater the chances of duplicates. We consider any duplicated or near-duplicated
discussions as ‘related discussions.’ The RD-Detector’s output is a set of related
discussion candidates, which, from now on, we denote R.

Figure 2 shows the overall process conducted to detect the sets of related dis-
cussion candidates. The process comprises three phases: Data collection (Figure 2
- A), Preprocessing (Figure 2 - B), and Relatedness Checker (Figure 2 - C). We
describe the phases of the proposed approach as follows.

3.1 Data collection

In the “Data collection” phase, we focus on gathering data from OSS communities.
RD-Detector was assessed using data collected from public discussions threads held
in three GitHub Discussions forums (Table 1).

Our data collection comprises three steps (Figure 2 - A). In the ﬁrst step, we
select the OSS repositories of interest. Next, we collect the discussions threads
of the selected repositories. Finally, we extract some preselected attributes from
each discussion thread. The RD-Detector considers the title and the body of the
discussion posts to calculate the semantic text similarity values between pairs of
discussion posts. We do not consider comments because they are feedback or design

9 https://github.com

Looking for related discussions on GitHub Discussions

9

Fig. 2: The RD-Detector approach

reasoning about the discussions’ main topics. During the data collection phase, the
RD-Detector extracts the author, date, and category type of the discussion posts.
The RD-Detector uses the attributes to select the discussion posts and create
sub-datasets according to predeﬁned ﬁlters (Section 4.2).

3.2 Preprocessing

Applying text similarity algorithms in discussion content requires data prepro-
cessing to optimize the algorithm execution (Zhou et al., 2017). We executed a
preprocessing pipeline to remove noisy information and format the dataset collec-
tion (Figure 2 - B). The preprocessing phase comprises ﬁve steps. In the “Data
selection” step, the RD-Detector selects the title and the body text of the discus-
sion’ posts. In addition, in “Data selection” we can ﬁlter discussion posts according
to the values of their attributes. We can select discussion posts that fulﬁll certain
conditions, such as category type (using the category attribute).

The following two preprocessing steps, “Code snippets and URL removal” and
“Punctuation, emoji, numbers, and stopwords removal,” aim to remove noise data
from the content of the discussions. We remove the code snippets since the machine
learning model used is a general-purpose model trained to handle NLP problems.
Furthermore, the structure and vocabulary of natural language texts diﬀer from
machine codes (Sirres et al., 2018). We use HTML tags, e.g. <code>, to remove
code snippets and URLs.

The “Punctuation, emoji, numbers, and stopwords removal” step removes
punctuations, emojis, numbers, and stopwords from discussions content. To this
end, we use the Python Natural Language Tool Kit (NLTK) library 10. We do
not remove the following punctuation symbols ‘.’ and ‘ ’ because they concatenate
words, e.g., ‘Next.js’ and ‘version 1.3’. In the next preprocessing step, we convert

10 https://www.nltk.org/

10

Lima et al.

the remaining text to lowercase and apply lemmatization (Zhang et al., 2017).
Finally, we eliminate zero-length discussion posts in the “Empty posts removal”
step.

3.3 Relatedness Checker

The “Relatedness Checker” phase is the core of the RD-Detector (Figure 2 - C).
In this phase, the approach computes the similarity score of discussions’ pairs and
detects the related discussion candidates. The preprocessed data is the input of
this phase (Algorithm 1, lines 0-3). The set R, containing pairs of related discussion
candidates, is the output of the “Relatedness Checker” phase (Algorithm 1, line
30). This phase comprises two steps: (1) the similarity measurement (Similarity
checker) and (2) the Selection of related discussion candidates. The steps are as
follows.

3.3.1 Similarity checker

We use a semantic text similarity (STS) checker (Agirre et al., 2015) to score
the similarity between pairs of discussion posts. The similarity checker measures
the similarity score using a general-purpose Sentence-BERT pre-trained model
(Reimers and Gurevych, 2019).

The RD-Detector uses the public all-mpnet-base-v211 model to compute
semantically signiﬁcance sentence embeddings of each discussion post. The model
maps sentences and paragraphs to a 768-dimension dense vector space. At the time
of this work execution, the all-mpnet-base-v2 model provided the best quality in
sentences embeddings computing and semantic searching performance (Reimers,
2021).

We compare the sentences embeddings using the cosine-similarity score. We
used the score to set the similarity value between pairs of discussions and to rank
sentences with similar meaning (similarity(master, target)). The cosine similarity
value outcome is bounded in [0,1] (Abric et al., 2019). Cosine similarity value 1
refers to identical discussion contents, value 0 refers to dissimilar discussions (Abric
et al., 2019). There is no hierarchy or priority relationship between pairs of masters
and targets discussions. Masters discussions are the oldest post, and targets ones
are the most recent. Therefore, the similarity value for (masteri,targetj) is equal
to the similarity value for (targetj,masteri).

Researchers train and optimize custom-built machine learning models using
labeled dataset samples. We propose an approach based on a general-purpose
machine learning model to address the lack of a pre-labeled dataset and gener-
alize the outcomes to diﬀerent OSS communities. One can beneﬁt from general-
purpose models to detect related discussions on the GitHub Discussions forum
because of the lack of an oﬃcial dump of related posts to train or optimize auto-
matic approaches and the signiﬁcant number of OSS projects ecosystems hosted
on the platform. General-purpose models application is not restricted to a single
context, as they are trained and optimized in diﬀerent datasets. Speciﬁcally, the

11 https://huggingface.co/sentence-transformers/all-mpnet-base-v2

Looking for related discussions on GitHub Discussions

11

all-smpnet-base-v2’s base model is the microsoft/mpnet-base model. Experts op-
timized the all-smpnet-base-v2 model over 1 billion sentence pairs collected from
various datasets (Reddit comments (2015-2018), WikiAnswers Duplicate ques-
tion pairs, Stack Exchange Duplicate questions (titles+bodies), etc.) (Face, 2021).
Context-speciﬁc models are trained and optimized according to pre-labeled sam-
ples that summarize context speciﬁcs, not guaranteeing eﬀectiveness when used
outside the training context. In addition, general-purpose models usage provide
advantages such as (1) no need for local complex computational structures for
training, validation, and testing of models, (2) no need for model parametrization,
and (3) no need for model retraining, revalidation, and retest whenever the context
change (Polyzotis et al., 2017; Zhou et al., 2017; Schelter et al., 2018; Lee and Shin,
2020). The dynamism with which OSS communities grow justiﬁes general-purpose
machine learning model usage. Indeed, public machine learning models bring ﬂexi-
bility to the RD-Detector approach. As experts release new exchangeable machine
learning models, one can update the RD-Detector.

Therefore, for each pair of a (master, target) discussion, the RD-Detector com-
putes the semantic similarity value that captures the relatedness of the posts, Al-
gorithm 1 - lines 4-13. The similarity values are the input to the “Selection of
related discussion candidates” step.

3.3.2 Selection of related discussion candidates

We rank the related discussion candidates according to their similarity values.
We do not use a pre-determined threshold, x to ﬁlter the related discussions.
Instead, the RD-Detector computes what we call ‘local threshold,’ denoted by
Trelated based on each project dataset. The Trelated is deﬁned using descriptive
statistics that describe the characteristics of a speciﬁc similarity value set. The
local threshold use allows the adaptation of the proposed approach to diﬀerent
OSS communities’ contexts. We set the Trelated value following four steps:

1. Deﬁning the K value: The K value delimits the search bounds for related
discussion candidates. The RD-Detector uses the K value to select the simi-
larity values of the top-K most similar discussions to each post in the input
dataset.The greater the value of K, the greater is the number of similarity
values selected and the search bounds. Setting K = 5, the approach uses the
similarity values of the top-5 most similar discussions to every post in the
dataset. Setting K = 10, the RD-Detector selects the similarity values of top-
10 most similar discussion posts. One can specify diﬀerent K values. K is an
input value (Algorithm 1).

2. Creating the distribution S: The S distribution is a collection of similarity
values. S contains the similarity values of the K most similar target discussions
for each discussion post in the dataset (Algorithm 1, lines 14- 19).
Let n be the number of discussion posts in the dataset, K the number of
the most similar target discussions to every discussion in the dataset, and
value ij the similarity value of a given masteri and targetj discussion pair,
the distribution S is:

S =< value1 1, value1 2, ..., value1 K , value2 1,
value2 2, ..., value2 K , ..., valuen 1, valuen 2, ..., valuen K >

12

Lima et al.

Algorithm 1: Identifying Related Discussions

Input: Set of discussion posts D
Input: Set of discussion attributes ∆
Input: K value
Input: Sentence-BERT model
1 foreach discussion di ∈ D’ do
2
3 end
4 /* Similarity Checker */
5 foreach discussion di ∈ D(cid:48) do
6

D(cid:48) ← D(cid:48) + preprocessing(di);

foreach discussion dj , (dj (cid:54)= di) ∈ D(cid:48) do

7

8

9

10

11

masteri ← di;
targetj ← dj ;
similarity valuei j ← similarity(masteri, targetj ) /* Using model */ ;
tuple ← (masteri, targetj , similarity valuei j );
save(tuple, similarity values f ile);

end

12
13 end
14 /* Threshold definition */
15 S ← {} ;
16 foreach discussion di ∈ D(cid:48) do
17

masteri ← di;
S = S + topK sim values(masteri, K);

18
19 end
20 Q1 ← 25th percentile(S);
21 Q2 ← 50th percentile(S);
22 Q3 ← 75th percentile(S);
23 IQR ← Q3 − Q1;
24 Trelated = Q3 + (1.5 ∗ IQR);
25 /* Selection of related discussion candidates */
26 R ← {} ;
27 foreach masteri, targetj , similarity valuei j ∈ similarity values f ile do
if (similarity valuei j ≥ Trelated) then R ← R + (masteri, targetj );
28
29 end
30 return R ;

3. Determining the descriptive statistics of S: We use descriptive statistics
variability measures to understand how dispersed the distribution is. To this
end, we calculate the interquartile range (IQR), along with the 25th percentile
(Q1), the 50th percentile (Q2), and the 75th percentile (Q3), Algorithm 1 -
lines 20 to 23. Next, we ﬁnd the Upper Inner Fence value (Equation 1) that
identiﬁes the outliers in S (Tukey et al., 1977).

Upper inner fence = Q3 + (1.5 ∗ IQR)

(1)

4. Setting the local threshold (Trelated): Because we assume that the greater
the semantic similarity value of a pair of discussion posts, the greater the
chances they are related discussion, we set the local threshold to the upper
inner fence value (Algorithm 1 - line 24). Therefore, we have that:

Trelated = Upper inner fence

(2)

The K value deﬁnes the S distribution size. Consequently, it changes the co-
eﬃcients Q1, Q2, Q3, and IQR values that summarize S. As a result, it also

Looking for related discussions on GitHub Discussions

13

causes changes in the local threshold value, Trelated. Since Trelated is directly
inﬂuenced by S, we call Trelated as ‘local threshold’.

After setting the local threshold, the RD-Detector detects the pairs of related
discussion candidates. Related discussion candidates are those discussion pairs that
similarity values are equal to or greater than the local threshold. RD-Detector
creates R, the set of related discussion candidates (Algorithm 1, lines 26-30).
We consider related discussions those identiﬁed as outliers in the S distribution.
Calefato et al. (2021) also use descriptive statistics to identify core OSS developers’
inactivity periods.

We measured the precision of the RD-Detector based on the assessment of
OSS maintainers and SE researchers who manually evaluated the R set. Since we
do not know the number of related posts or the truly related discussion posts in
the GitHub Discussions
forum, we focus on increasing the RD-Detector preci-
sion rather than its recall. Higher precision values ensure greater assertiveness in
detecting true positives.

The precision rate refers to the number of true positives divided by the ap-
proaches’ total number of positive predictions. In this work, evaluators judged the
RD-Detector predictions and highlighted the true positives ones. We present the
overall process of the RD-Detector evaluation in Section 4.4.

Let N be the set of true-positive related discussions, and R be the set of pre-
dicted related discussions made by RD-Detector. The precision rate measurement
is as follows (Kim et al., 2005):

P recision =

|N ∩ R|
|R|

(3)

The precision value ranges from 0% to 100% (Kim et al., 2005). A precision
of 100% means that all identiﬁed related discussion candidates are indeed related
discussion posts.

The RD-Detector emerges as a tool that OSS maintainers can use to min-
imize the work overhead in manually detecting related threads and reduce the
rework in answering the same question multiple times. The periodic execution of
RD-Detector supports maintainers to deal with related discussions propagation in
OSS communities.

4 Assessing RD-Detector over the GitHub Discussions forum

To assess RD-Detector, we collected and used pairs of discussion posts of three
selected OSS projects hosted on the GitHub platform (Table 1). RD-Detector
generated sets of related discussion candidates for each project. Maintainers of the
selected OSS projects and SE researchers evaluated the RD-Detector outcomes.

4.1 Data collection - GitHub Discussions

To perform the data collection, we wrote a customized web crawler that searches
for discussion posts on the GitHub platform and collects all public discussion
posts of a given OSS community. To do so, we used the Python BeautifulSoup

14

Lima et al.

Table 1: Repositories used in the dataset

Repository #Discussions Download date

Gatsby 13
Homebrew14
Next.js15

sum

1,276
1,464
8,569

11,309

11-10-2021
10-09-2021
11-18-2021

library 12. We stored the collected data in a local dataset. The web crawler input
is the project URL, and its outputs comprise of (1) a list with the discussions’
IDs, categories, authors, dates, and title; and (2) the HTML ﬁles of the discussion
threads (discussion ﬁrst post and comments).

We collected data from Gatsby (Dp=Gatsby), Homebrew (Dp=Homebrew), and
Next.js (Dp=N ext.js) repositories (Table 1). We selected these three repositories
based on (1) the usage of the Discussions forum, (2) the GitHub professional
team members’ demands (coauthors in this research), and (3) OSS maintainers’
availability. We executed multiples data collection runs between October 2021 and
November 2021. In total, we collected 11,309 discussions.

4.1.1 Dataset characterization

According to the Gatsby community documentation (Community, 2022), ‘Gatsby
is a free and open source framework based on React that helps developers build
blazing fast websites and apps.’. We call DGatsby the set of discussion posts col-
lected from Gatsby’s repository. On the date we performed the data collection, we
collected 1,276 discussion posts from Gatsby, |Dp=Gatsby| = 1, 276 (Table 1). Fig-
ure 3-(a) shows the frequency distribution of discussions created in the Gatsby com-
munity over 21 months (01-2020 to 09-2021). According to this time window, 01-
2020 to 09-2021, the average growth rate of forum usage by the Gatsby community
was 22% (considering the frequency of new discussions).Gatsby makes available the
following categories types community, help, ideas-feature-requests, RFC, and
umbrella-discussions. Help discussions are most common, totaling 73.35%, fol-
lowed by ideas-fearute-requests, umbrella-discussions, community, and RFC,
totaling 21.23%, 2.50%, 1.33%, 1.56% of discussion posts, respectively (Figure 3-
(b)).

Homebrew is an OSS project that makes it easy to ‘install the UNIX tools Apple
didn’t include with macOS. It can also install software not packaged for your Linux
distribution to your home directory without requiring sudo.’(Project, 2022). We
call DHomebrew the set of discussion posts collected from Homebrew’s repository.
We collected 1,464 discussion posts from Homebrew, |Dp=Homebrew| = 1, 464.

Figure 3-(c) shows the frequency distribution of discussion posts created in
Homebrew Discussions forum over 14 months (09-2020 to 10-2021). The oldest
discussion post collected from Homebrew dates from September 2020. According
to data in DHomebrew, the average growth rate of Discussions forum usage in
Homebrew between 09-2020 and 10-2021 was 25.12% (considering the frequency
of new discussions). The months with the highest frequency of new discussions

12 https://pypi.org/project/beautifulsoup4/

Looking for related discussions on GitHub Discussions

15

were December/20, January, and February 2021, totaling 148, 151, and 195 new
discussions, respectively. Such values indicate the peak in the frequency distri-
bution graph (Figure 3 (c)). Unlike Gatsby and Next.js projects, Homebrew or-
ganizes its discussion posts according to the problem types and not by ques-
tions type. Homebrew provides the following categories: casks, getting-started,
tap-maintenance-and-brew-development, everyday-usage, linux, and writing-
formulae-casks. The everyday-usage discussions are the most common in the
DHomebrew dataset, totaling 54.87%. Followed by getting-started, casks, tap-
maintenance-and-brew-development, linux, and writing-formulae-casks, to-
taling 17.27%, 10.85%, 8.84%, 5.80% and 2.34% of discussions, respectively (Fig-
ure 3 (d)).

Next.js is an OSS project that ‘provides a solution to build a complete web
application with React’(Vercel, 2022). The Next.js community has stood apart
in supporting the launch of GitHub Discussions
since the start (Liu, 2022).
In January 2022, the project forum already had almost 400 pages of discussion
threads (Liu, 2022). Compared to Gatsby and Homebrew, Next.js is the project
with the highest number of discussions analyzed, characterizing an active OSS com-
munity. We collected 8,569 public discussion posts from Next.js. Figure 3 - (e)
shows the frequency distribution of new discussions over 22 months (01-2020 to 10-
2021). In this period, the average growth rate of the forum in the Next.js commu-
nity was 44.62%(considering the number of new discussions created). The Next.js
project has the highest average growth rate among the three analyzed projects. The
peaks of the distribution indicate the months with the highest number of discus-
sions created, which were May, June, and October 2020 with 589, 544, and 562, re-
spectively (Figure 3 - (e)). When we collected the dataset, the discussion categories
were ideas, help, react-server-components, and show-and-tell. Help discus-
sions are the most frequent in the Next.js dataset, totaling 87.01% of the discus-
sion posts. Followed by ideas, show-and-tell, and react-server-components,
totaling 11.84%, 1.03% and 0.10% of discussions, respectively (Figure 3-(f)).

Finally, all discussion posts collected composes the dataset D used in this work.

Therefore, D = Dp=Gatsby ∪ Dp=Homebrew ∪ Dp=N ext.js.

4.2 Preprocessing phase applied to Discussions Dataset

We preprocessed the dataset D. First, we split the dataset into subsets using
predeﬁned ﬁlters. We considered two ﬁlter types: project and category ﬁlters. We
used the project ﬁlter, p, to select discussions from a speciﬁc OSS repository by
deﬁning the project’s name. We used the category ﬁlter, c, to select discussion
posts that matched predeﬁned categories’ types.

We set both ﬁlter values according to the dataset characterization. One can
also combine ﬁlters to create diﬀerent sub-datasets, e.g., we can select discussions
of type β belonging to project α. The category ﬁlter values depend on the types of
categories provided by the project. We express the use of both ﬁlters by p = α|c =
β, where p identiﬁes to the project name (ex.: Gatsby, Homebrew, and Next.js )
and c identiﬁes the discussion category type.

We standardized the category labels to report our results, making them easier
to describe and follow. We use the following labels: Q&A, Ideas, and ALL. The Q&A
label refers to ‘Question and Answers’ (help) discussions types. The Ideas category

16

Lima et al.

Fig. 3: Dataset characterization - Gatsby, Homebrew, and Next.js

to refer to ideas or ideas-feature-requests categories. In addition, we deﬁned
a special label, ALL, used to refer to all discussion posts, regardless of their original
type. As described in Section 4.1.1, the Homebrew project discussion organization
diﬀers from the other projects. Due to this fact, we chose not to ﬁlter Homebrew
discussions according to the category type. Therefore, we report the results of the
Homebrew project by setting c = ALL.

Besides, on the this step, RD-detector selected the discussions’ title and body
text. We derived seven sub-datasets by applying diﬀerent ﬁlter conﬁgurations to
the original dataset D (Table 2, column 3). We use Dp=Gatsby|c=Q&A to designate
the subset of Q&A discussion collected from the Gatsby project, Dp=Gatsby|c=Ideas
to denote the subset of ideas-feature-requests discussions collected from the
Gatsby project, and so on. We assessed the proposed approach (RD-Detector) over
the seven sub-datasets.

Next, the sub-datasets went through cleaning, denoising, and formatting steps.
The last preprocessing step ignores zero-length discussions. In total, the approach
discarded 3, 17, and 127 discussions from Gatsby, Homebrew, and Next.js projects,
respectively.

4.3 Relatedness Checker applied to GitHub Discussions

After the preprocessing phase, the RD-Detector calculated the semantic similarity
values for each pair of discussions (as presented on Section 3.3.1), using the seven
sub-datasets presented in Table 2, column 3. As discussed in Section 3.3.2, the
value of K impacts the set of related discussion candidates R. The RD-Detector
uses the K value to select the similarity values of the top-K most similar discussions
to each post in the dataset. In order to evaluate the RD-Detector eﬀectiveness, we
set the K value to 5 and 10. For each Dp=α|c=β sub-dataset presented in Table 2,
we conﬁgured the approach to run over K = 5 and K = 10. This way, we evalu-
ate the approach’s eﬀectiveness by considering the ﬁve and the ten most similar

Looking for related discussions on GitHub Discussions

17

Table 2: Conﬁguration groups: projects, categories, and K-values

Project

Category Dataset

Gatsby

Q&A

Idea

ALL

Dp=Gatsby|c=Q&A

Dp=Gatsby|c=idea

Dp=Gatsby|c=ALL

Homebrew ALL

Dp=Homebrew|c=ALL

Next.js

Q&A

Idea

ALL

Dp=N ext.js|c=Q&A

Dp=N ext.js|c=idea

Dp=N ext.js|c=ALL

Set of related discussion
candidates

Rp=Gatsby|c=Q&A|K=5
Rp=Gatsby|c=Q&A|K=10
Rp=Gatsby|c=idea|K=5
Rp=Gatsby|c=idea|K=10
Rp=Gatsby|c=ALL|K=5
Rp=Gatsby|c=ALL|K=10
Rp=Homebrew|c=ALL|K=5
Rp=Homebrew|c=ALL|K=10
Rp=N ext.js|c=Q&A|K=5
Rp=N ext.js|c=Q&A|K=10
Rp=N ext.js|c=idea|K=5
Rp=N ext.js|c=idea|K=10
Rp=N ext.js|c=ALL|K=5
Rp=N ext.js|c=ALL|K=10

discussions to every discussion in the dataset. Values greater than 10 expand the
search bounds for related discussions. We did not consider K values greater than
ten because the proposed approach aims at increasing the precision rates rather
than recall rates.

In total, we assessed the approach over 14 diﬀerent conﬁguration groups (Ta-
ble 2, column 4). For each conﬁguration group, RD-Detector computed the local
threshold value, Trelated, and detected the set of related discussion candidates,
R. We denote the set of related discussion candidates detected considering the
conﬁguration group p = Gatsby, c = Q&A, and K = 5 by Rp=Gatsby|c=Q&A|K=5.
Rp=Gatsby|c=Q&A|K=10 denotes the set of related discussion candidates detected
considering the conﬁgurations p = Gatsby|c = Q&A|K = 10. One can use the
same reasoning for the other 12 related discussion sets presented in Table 2, col-
umn 4. We used the conﬁguration groups to evaluate the RD-Detector eﬀectiveness
and report our results.

4.4 The RD-Detector evaluation

We recruited maintainers of the three OSS projects analyzed and SE researchers
to evaluate the RD-Detector outcomes. Both OSS maintainers and SE researchers
manually classiﬁed pairs of related discussion candidates as duplicates, related or
not related. OSS maintainers judged pairs of related discussion candidates accord-
ing to their work project. The SE researchers judged pairs of related discussion can-
didates detected on the three projects. The maintainers, M Gatsby, M Homebrew,
and M Next.js, were contacted via the GitHub team of professionals. The three
SE researchers, SE R1, SE R2, and SE R3, contacted have diﬀerent expertise in
software development. SE R1 is an industry practitioner and a Software Engineer-
ing researcher, SE R2 is an active OSS contributor and a Software Engineering
researcher, and SE R3 is a Software Engineering researcher.

Evaluators received online documents containing instructions to evaluate the
sets of the related discussion candidates. The documents (1) described the con-

18

Lima et al.

cept of duplicates and related discussions, and (2) listed pairs of related discussion
candidates containing. We characterized each pair of related discussions by de-
scribing the ID and title of the master and target discussions. The ID was a link
from which evaluators could access the original discussion posts. We instructed
the evaluators to add the label ‘D’ to duplicates, ‘R’ to related ones, and ‘N’ for
unrelated discussions. We also asked the evaluators to add comments to justify
their judgments.

We measured the RD-Detector precision rate compared to the OSS maintain-
ers’ and SE researchers’ judgment. Their judgments identiﬁed the true-positive
RD-Detector predictions and deﬁned the set N used in Equation 3. Section 5 re-
ports the precision rate reached assessing RD-Detector over the 14 conﬁgurations
groups presented in Table 2.

5 Results

In this section, we ﬁrst present the experimental results of RD-Detector evalua-
tion. Next, we describe the OSS maintainers’ feedback about the detected related
discussion candidates and the RD-Detector practical applications.

To evaluate the eﬀectiveness of our approaches, we ran RD-Detector using
diﬀerent conﬁgurations (Table 2) and measured its precision rate compared to
the OSS maintainers’ and SE researchers’ judgment (Section 4.4). First, to bet-
ter analyze and report our results, we ﬁltered the dataset according to the OSS
repositories names and the discussion category. Next, we parameterized the ap-
proach to use diﬀerent values of K. In total, combining diﬀerent conﬁgurations, the
RD-Detector outputs 14 related discussion candidates sets (Table 2), over which
we assessed the RD-Detector approach. The selected evaluators judged all 14 sets
of related discussion candidates.

As discussed in Section 3.3.2, as one changes the K value, the Trelated threshold
also changes and, consequently, the number of detected related discussion candi-
date pairs. Tables 3, 4, and 5 present the local threshold values, Trelated, and the
number of related discussion candidates pairs detected, |R|, for Gatsby, Homebrew,
and Next.js projects, respectively. Tables 3, 4, and 5 also show, for each conﬁg-
uration group (project name, category type, and K value), the distribution size
(size(S)) and the descriptive statistics coeﬃcients that summarize S (IQR, Q1,
Q2, and Q3). We used the values of the coeﬃcients to calculate the local threshold
Trelated (Equation 2).

We can note that as we increase the value of K, the local threshold values,
Trelated, decrease (Tables 3, 4, and 5). Since the local threshold value decreases,
the RD-Detector detects new pairs of related discussion candidates. The new pairs
are those outliers considered by changing the Upper Inner Fence value, according
to Equations 1 and 2.

Table 3 presents the results for the Gatsby project. Considering K = 5 and
c = Q&A, RD-Detector calculated the local threshold value based on the similarity
values of 3,924 unique discussion pairs, size(S), setting Trelated = 0.9278 and
detecting four related discussion candidates, |R|. Considering the conﬁguration
group |p = Gatsby|c = Q&A|K = 10, the local threshold value, Trelated = 0.9040,
was calculated based on 7,806 similarity values of unique pairs of discussions,
detecting six pairs of related discussion candidates. Besides, considering c = Ideas

Looking for related discussions on GitHub Discussions

19

Table 3: Descriptive Statistics - Gatsby

Gatsby - Similarity values

c = Q&A

c = Ideas
K = 5 K = 10 K = 5 K = 10 K = 5 K = 10

c = ALL

Size(S)
IQR
Q1
Q2
Q3

3924
0.145
0.566
0.648
0.711

7806
0.145
0.542
0.622
0.687

1063
0.132
0.499
0.568
0.630

2080
0.135
0.461
0.540
0.596

5277
0.137
0.571
0.649
0.708

10517
0.139
0.545
0.623
0.684

Trelated
|R|

0.9278
4

0.9040
6

0.8278
6

0.7992
8

0.9127
7

0.8925
12

and K = 5, the local threshold Trelated calculated was 0.8278. This value was based
on the analysis of 1,063 similarity values and detected six pairs of related discussion
candidates. In addition, for the conﬁguration group p = Gatsby|c = Ideas|K = 10,
the RD-Detector detected eight pairs of related discussion candidates, considering
Trelated = 0.7992. Finally, one can apply the same standard interpretation to
Gatsby’s results by ﬁxing c = ALL and varying K = 5 and K = 10.

Table 4: Descriptive Statistics - Homebrew

Homebrew - Similarity values

c = ALL
K = 5 K = 10

Size(S)
IQR
Q1
Q2
Q3

6009
0.130
0.535
0.602
0.665

11948
0.130
0.508
0.578
0.638

Trelated
|R|

0.8592
15

0.8336
34

Table 4 presents the RD-Detector outcomes for the Homebrew project. Consid-
ering the conﬁguration group p = Homebrew|c = ALL|K = 5, the RD-Detector
calculated the local threshold value based on the similarity values of 6,009 unique
discussion pairs, size(S), achieving Trelated = 0.8592. As we can see in Table 4, the
approach identiﬁed 15 pairs of related discussion candidates considering c = ALL
and K = 5. However, considering c = ALL and K = 10, the approach calculated
the local threshold value, Trelated = 0.8336, based on 11,948 similarity values. Us-
ing this last conﬁguration group, the RD-Detector detected 34 related discussion
candidate pairs.

Table 5 presents the distribution size, Size(S), and the number of detected
related discussion pairs, |R|, for the Next.js project. These numbers endorse the
scenario described by Ahasanuzzaman et al. (2016). The authors highlighted that
as PCQA forums become popular, the number of posts increases. Consequently,
it may also increase the number of duplicates or related posts. Since the Next.js

20

Lima et al.

Table 5: Descriptive Statistics - Next.js

Next.js - Similarity values

c = Q&A

c = Ideas
K = 5 K = 10 K = 5 K = 10 K = 5 K = 10

c = ALL

Size(S)
IQR
Q1
Q2
Q3

30665
0.111
0.581
0.639
0.693

60704
0.113
0.558
0.617
0.672

4006
0.119
0.543
0.607
0.662

7834
0.114
0.514
0.574
0.629

35171
0.109
0.588
0.646
0.697

69616
0.111
0.565
0.623
0.676

Trelated
|R|

0.8605
95

0.8427
122

0.8402
10

0.8014
35

0.8608
106

0.8441
137

project is the largest considering the number of discussion posts (Table 1), the
chances of related discussion occurring can increase. Using the conﬁguration group
|p = Next.js|c = ALL|K = 10, the RD-Detector detected 137 pairs of related dis-
cussion candidates - the most extensive related discussion set detected. For this last
conﬁguration group, the local threshold value, Trelated = 0.8441, was calculated
based on the similarity value of 69,616 unique pairs of discussions (Table 5).

For each of the 14 conﬁguration groups presented in Table 2 column 4, the
RD-Detector calculated diﬀerent values of local thresholds and detected diﬀerent
amounts of related discussion candidate pairs. We highlight that all sets of related
discussion candidates detected considering K = 5 are subsets of the related dis-
cussion candidate sets setting K = 10. Table 6 shows the precision rate reached
by the RD-Detector considering the 14 conﬁguration groups.

Table 6: Evaluation Results

Category K = 5 K = 10

Q&A
Ideas
ALL

Gatsby

100%
83.33%
100%

100%
75%
91.66%

Homebrew

ALL

100%

91.17%

Q&A
Ideas
ALL

Next.js

95.78% 90.98%
88.57%
97.16% 93.43%

90%

The smallest set size of related discussion candidates was detected using the
conﬁguration group p = Gatsby|c = Q&A|K = 5 (Tables 3, 4, and 5). According
to this conﬁguration group, the approach detected four related discussion can-
didate pairs, |Rp=Gatsby|c=Q&A|K=5| = 4. Evaluators emphasized that all four
pairs are truly related discussions, which means that all of them contain discus-
sion posts related to one other. On the other hand, the RD-Detector approach
detected the biggest set of related discussion candidates using the conﬁguration

Looking for related discussions on GitHub Discussions

21

|p = Next.js|c = ALL|K = 10. In total, the approach detected 137 pairs of related
discussion candidates, |Rp=Next.js|c=ALL|K=10| = 137, of which 128 were classiﬁed
as related by the evaluators, achieving a precision rate of 93.43% (Table 6).

The OSS maintainer, M Gatsby, and two SE researchers evaluated Gatsby
project’s related discussion candidates. In order to save M Gatsby’s time and ef-
fort in judging cases of duplicates that are exact copies, the GitHub professional
team requested researchers to judge pairs of related discussion with a high similar-
ity value. Two SE researchers judged pairs of related discussion candidates with
similarity values greater than or equal to 0.9415. The researchers agreed that all
judged pairs contained related discussions. The maintainer M Gatsby judged re-
lated discussion candidate pairs that required prior technical knowledge about the
project. According to Table 6, RD-Detector reached the maximum precision value
(100%) in detecting related discussion pairs ﬁxing c = Q&A and varying the K
value (K = 5 and K = 10) for the Gatsby project. Considering the Gatsby project
discussions classiﬁed as Ideas (c = Ideas), the approach achieved an 83.33% preci-
sion rate for K = 5 and 75% setting K = 10. Still processing Gatsby’s discussions
and setting c = ALL, the RD-Detector achieved a precision rate of 100% and
91.66% for K = 5 and K = 10, respectively.

The maintainer M Homebrew and two SE researchers evaluated the Homebrew
project’s set of related discussion candidates. As we already mentioned, to save the
maintainer time and eﬀort, the researchers judged pairs of related discussion candi-
dates with the highest similarity value (greater than or equal to 0.9558). Consider-
ing the set Rp=Homebrew,c=ALL,K=5, the RD-Detector achieved the highest precision
rate in detecting related discussion pairs. Evaluators judged all of the 15 pairs
of related discussion candidates presented in Rp=Homebrew,c=ALL,K=5 as related.
However, setting the conﬁguration group |p = Homebrew|c = ALL|K = 10, the
approach achieved 91.17% of the precision rate. From evaluators’ perspective, 31
out of 34 are truly related pairs of discussion posts. The maintainer M Homebrew
judged the discussions of three pairs as unrelated.

As discussed in Section 4.1.1, compared to Gatsby and Next.js projects, the
Homebrew project organized the discussion posts diﬀerently. It does not provide the
same default categories provided by Gatsby and Next.js. Homebrew discussions
categorization is according to the problem type. That is why we did not split the
Homebrew dataset by category and present the achieved precision rate setting the
category ﬁlter to ALL (c = ALL).

Finally, the number of detected related discussion candidates for the Next.js
project required greater participation of SE researchers in the evaluation phase.
Because the related discussion candidate sets of the Next.js project are the largest,
the project maintainer judgment became unfeasible. To ensure the reliability of
the reported results, we measured the researchers’ inter-rater agreement using the
Cohens Kappa Coeﬃcient (Cohen, 1960), which was 0.85. This value indicates
almost perfect agreement according to the interpretation proposed by Landis and
Koch (1977). Setting the conﬁguration group |p = N ext.js|c = Q&A|K = 5, the
approach reached precision values of 95.78%. The RD-Detector reached a precision
of 90.98%, setting the K value to 10. Fixing p = Next.js and c = Ideas, the
RD-Detector reached precision rates of 90% and 88.57% for K = 5 and K = 10,
respectively. Besides, when we ﬁxed p = Next.js and c = ALL, the RD-Detector
reached precision rates of 97.16% and 93.43% for K = 5 and K = 10, respectively.

22

Table 7: False-positive related discussion candidates.

#

ID
Master

Title Master

ID
Target

Title Target

Gatsby

Lima et al.

Similarity
value

1 29766

2 26984

I want to lint ts ﬁle with Gatsby’s native
support.
More ﬂexible default Typescript transpiler
handling

32122

31662

On creating a better method to extend the
default ESLint conﬁguration
Need better CSS modules +
Typescript support

0,897

0,8172

3

4

5

1906

814

72

Questions on Homebrew’s third-party
mirroring policy
Homebrew not installing on Big Sur v 11.2.1
- Failed during: git fetch –force origin
Installing Homebrew on a Windows OS

1917 Setting up mirrors for Homebrew bottles

0,8528

1218 Can’t get homebrew installed

657 Unable to install HomeBrew

0,8396

0,8353

Homebrew

Next.js

6 13368

7 15780

8 13617

9 23195

10 23871

Can I replace next-routes with
the new versions?
ModuleNotFoundError: Module not found:
Error: Can’t resolve ’fs’
NextJS not showing TypeScript
errors on Runtime
Tailwind CSS not being bundled in
static export
Typescript - Can you change where Next.js
outputs the ”next-env.d.ts” ﬁle?

24996

25845

24996

11 14416 deploy to Vercel error(Build error occurred)

21107

12 19089

Image Component with AWS Throws 400
Errors in Production

19568

13 24778 Cannot process DELETE request via CORS

29589

14 15613

next export - missing stylesheets - sass
ﬁles not being linked in the static html export

23945

15 14508 Props are undeﬁned from getServerSideProps 20529

16 22688

Changes to .env.local are not loaded even
after restarting

17 17320 [RFC] ESLint out of the box
18 17320 [RFC] ESLint out of the box

19 24850 Support Promises in next.conﬁg.js

20361 nextjs with next-routes upgrade

0,9281

19154 import sub module of other module is fail

0,8887

I am unable to setup “typescript-is”
with nextjs.
NextJS with Tailwind does not work when
importing from Global.css
I am unable to setup “typescript-is”
with nextjs.
Deploy Vercel Error ’Error: Command
”yarn run build” exited with 1’
Image component not work when next is
deployed in sub path
same-origin policy on API routes not
working
[Sass] Errors in basic use of Sass with
3rd party node modules
Run getServerSideProps again on back
broswer

0,8646

0,8624

0,8593

0,8522

0,8456

0,8455

0,8449

0,8446

22918 Problem with less loader in next@10.0.8

0,843

24900 ESLint in Next.js and Create Next App
28571 Thoughts on Conformance in Next.js
(re)Expose ‘nextConﬁg‘ on top-level
NextServer for custom servers

24851

0,8479
0,8099

0,8088

0,8037

20 24850 Support Promises in next.conﬁg.js

31205 runtime env vars in next.conﬁg.js

Table 6 shows that precision rates degrade as we increase the value K. The
RD-Detector identiﬁed 18, 34, and 166 unique pairs of related discussion candi-
dates for the Gatsby, Homebrew, and Next.js projects, respectively. The diﬀerent
numbers of detected related discussion pairs highlight the uniqueness of the OSS
communities regarding the use of the GitHub Discussions forum. It also endorses
the need for approaches that meet local thresholds to detect related discussion
pairs and adapt to diﬀerent contexts of OSS communities.

Table 7 lists all discussion pairs judged unrelated, grouped by project. The
table presents the master and target discussions identiﬁers (IDs) and titles and
their respective similarity values. Regarding Gatsby project results, two pairs of
related discussion candidates were classiﬁed as unrelated by the M Gatsby main-
tainer. Thus, 16 out of 18 detected pairs were indeed related (precision=88.88%).
From the perspective of the maintainer M Homebrew, out of the 34 related dis-
cussion candidate pairs detected, three are false positives. Therefore, 91.17% of
the detected pairs are truly related discussions. Finally, 90.96% of discussion pairs
detected for the Next.js project are truly related. Out of 166 detected pairs, 15
are false positive. We discuss the false positives in Section 6.

Answering our RQ: The general purpose Sentence-BERT model ap-
plicable to NLP problems eﬀectively detects related discussion posts
forum. The results presented in Ta-
held in the GitHub Discussions

Looking for related discussions on GitHub Discussions

23

ble 6 show the eﬀectiveness of using the all-mpnet-base-v2 model to
compute the sentence embeddings of the discussion pots and detect
related discussions.

5.1 OSS maintainers’ perspective regarding the judgment of related discussion
candidates

We asked OSS maintainers to comment on their judgment regarding their decision
about the related discussion candidates. Based on their comments, we could iden-
tify (1) challenges deciding what are duplicate and related discussions, (2) reasons
why users create related discussions, and (3) practical applications of RD-Detector.

5.1.1 Challenges deciding what are duplicate and related discussions

We noted that the concept of ‘related discussions’ — and even ‘duplicate discus-
sions’ — depends on the evaluators’ perspective. Therefore, we see this conceptual
imprecision as a limitation that may introduce some bias on the RD-Detector
evaluation.

Based on maintainers’ feedback regarding the sample of related discussion can-

didates analyzed, we found four ways to conceptualize duplicates:

1. Duplicates can be exact copies - ‘...(the discussions) are identical duplicates...’

(M Next.js).

2. Duplicates address the same issues - ‘The initial problem is essentially the
same and they have the same primary solution...’ (M Homebrew), ‘The two
discussions are about the same issue and the solution is functionally the same,
although the speciﬁc commands given are slightly diﬀerent...’ (M Homebrew),
‘The problem the user in the target discussion is experiencing appears to be
identical (as noted by them in their description)...’ (M Homebrew), ‘These
topics are identical ...’ (M Homebrew), ‘Diﬀerent context but same problem
and solution.’(M Next.js).

3. Duplicates share common information (topic overlapping) - ‘...the target dis-
cussion also discusses a secondary problem that was raised during the initial
ﬁx.’ (M Homebrew), ‘Same thing exposed using an external library for data
fetching’ (M Next.js).

4. Sometimes, the same user can create duplicate discussions - ‘...and were opened
by the same user’ (M Homebrew), ‘Posted by the same author also.’ (M Next.js).

We found that the conceptualization of duplicates tends to vary according to
objective and subjective criteria. The ﬁrst criterion - exact copies - is an objective
one. To judge duplicates that are exact copies does not require evaluators to do a
deep semantic analysis of the discussions’ content. Exactly copies tend to have high
similarity values (the similarity value of the discussion pair judged by M Next.js
as duplicates is 0.9793). However, the second and the third criteria require evalua-
tors to analyze discussions semantically in order to identify the overlapping topic
occurrence. Although the fourth criterion is objective, evaluators can not consider
it separately to judge duplicates.

Compared to duplicates, maintainers classiﬁed related discussions as those
posts that are not precisely copies but have similar topics and share similar infor-
mation. They justify their judgment by highlighting that related discussions:

24

Lima et al.

1. Are not precisely copies - ‘...similar problems but not an exact duplicate’
(M Homebrew), ‘Since diﬀerent users are on diﬀerent networks they will never
be 100% duplicate.’ (M Homebrew), ‘These are all related to images so I’d say
they’re ’related’, they’re not exactly the same though’ (M Next.js).

2. Have the same solution and similar problems - ‘Same solution and similar
problems...’ (M Homebrew), ‘These (discussions) are both discussing the same
thing...’ (M Homebrew), ‘Both users are experiencing the same initial prob-
lem...’ (M Homebrew), ‘...These are all related to images...’ (M Next.js).

3. Express complimentary topics - ‘The target discussion is actually a continuation
of the main discussion...’ (M Homebrew), ‘This one is the initial RFC of a fea-
ture implementation, the other discussion is a post release issue.’ (M Next.js),
‘This discussion caused the 14890 RFC to be created’ (M Next.js).

4. Cover diﬀerent aspects of the same topic - ‘Very similar issue on the surface that
relates to the same initial problem, but the actualy underlying problem in the
two was totally diﬀerent’ (M Homebrew), ‘...One user opened the discussion
for help because the suggested solution didn’t work. The other user is asking
why the problem occured in the ﬁrst place.’ (M Homebrew), ‘A feature request
for the image optimization feature, related but talking about diﬀerent things.’
(M Next.js), ‘So the overall topic (guide) is the same, the guide which it should
be about is diﬀerent.’ (M Gatsby).

5. May address the same problem in diﬀerent contexts - ‘...is the same exact bug...

but diﬀerent context.’ (M Next.js).

Given the subjective conceptualization of both duplicates and related discus-
sions concepts, and based on OSS maintainers’ feedback, we note they also faced
challenges in classifying the discussion pairs as duplicates or related - ‘This blurs
the lines a bit between R and D...’ (M Homebrew), ‘There’s deﬁnitely some simi-
larity between the two, but it’s not really clear whether the issue is the same...So,
it’s hard to tell, but I don’t think they’re the same’ (M Homebrew), ‘...I would con-
sider them to be duplicates without the additional context of what was needed to
ﬁx the issue in the target discussion’ (M Homebrew), ‘I’m struggling with this one
bit. They’re technically diﬀerent questions, so I understand why the user opened
both (they both have the same author)...’ (M Homebrew), ‘Looks very similar,
almost duplicated ... but he has a solution using a diﬀerent...’ (M Next.js). This
challenge endorses our decision to consider duplicates as a subtype of related dis-
cussions.

We also note that the way users describe their questions, problems, or requests
may impact the RD-Detector results and the evaluator’s judgment. Sometimes, it
can be challenging for users to state the issue clearly and with enough contextual
detail: ‘Both people had a similar problem but described so vaguely that it could
be anything’ (M Homebrew).

5.1.2 Reasons to create related discussions

Based on the feedback of Homebrew’s maintainer, we note the three reasons Dis-
cussions users create related discussion posts. Although we likely do not list all
causes, the following three items enable OSS maintainers to understand the rea-
sons behind the occurrence of related discussions and plan actions to deal with
them. Users of GitHub Discussions create related discussions because:

Looking for related discussions on GitHub Discussions

25

1. They want to emphasize their need for help - ‘... After the maintainers stopped
responding, the user opened a new discussion in an attempt to continue.’
(M Homebrew).

2. They want to add new information to better detail the discussion’s main topic
- ‘The user never got a response, so they opened a new issue with some more
information a few weeks later.’ (M Homebrew).

3. They want to ask for diﬀerent solutions to the same issue - ‘...but they are
unhappy with the solution presented in the master discussion and would like
an alternative’ (M Homebrew).

Speciﬁcally, for the Homebrew project results, the maintainer identiﬁed multiple
pairs of related discussions related. The maintainer states that the discussion posts
refer to a peculiar situation, ‘These are a little tricky... .’ Although they are related
discussions, ‘so, I’d call the issues duplicates because they all are the same prob-
lem...’, the solutions of the problems depend on the users’ hardware and software
conﬁguration, ‘..each situation is diﬀerent it totally depends on the user’s speciﬁc
machine and conﬁguration.’ The maintainer further states that the solutions are
diﬀerent and that there is no single way to help users, ‘The actual solutions may be
diﬀerent, but there’s no way for us to help with that so...’ Consequently, they pro-
vide the same cues to users participating in the multi-related discussions ‘...from
our perspective, the advice we can give is the same’ (M Homebrew). Based on this
feedback, we envision the RD-Detector can also provide results to maintainers pri-
oritize project issues solving or intervene in recurring situations demanded by the
OSS communities.

5.1.3 Practical applications of RD-Detector

Finally, maintainers suggested some practical applications for RD-Detector results,
as follows :

1. to combine the discussions’ content merging the related discussion threads -
‘These (discussions) could have been combined into one discussion and it would
have made sense...’ (M Homebrew), ‘... But, if it were up to me, they should
have gone together in the same discussion.’ (M Homebrew).

2. to move a discussion content to another location reorganizing the discussions
threads as comments to each other - ‘...the new issue should probably have
been posted as a comment in the master discussion’ (M Homebrew), ‘...(they)
would’ve received better traction as a comment on one another.’ (M Next.js),
‘This discussion could’ve suﬃced as a comment on 21633.’ (M Next.js).

3. to recruit collaborators for speciﬁc tasks - ‘...could be useful though for people
looking for other guides to contribute to (in this instance).’ (M Gatsby).

The variety of OSS projects on the GitHub platform provides opportunities to
develop innovative approaches to detect related discussion candidates on GitHub
Discussions. However, the signiﬁcant number of OSS communities, the projects’
singularity, and the mentioned limitations challenge developing and validating such
systems. Those reasons endorse the RD-Detector design decision to use a general-
purpose machine learning model, calculate local threshold values, and achieve bet-
ter precision rates.

26

6 Discussions

Lima et al.

As we reported in Section 5, increasing the K values impacts on the local threshold
values and increases the number of detected related discussion candidates. In this
section, we discuss the eﬀects of changing the K value, the false positives presented
in Table 7, and the implications of this research from the perspective of the OSS
communities and software engineering researchers.

6.1 The impacts of changing the K value

As discussed in Section 3.3.2, the K value delimits the search bounds for related
discussion candidates. As one changes the value of K, the threshold Trelated also
varies and, consequently, the number of detected related discussion candidates.
By conﬁguring the RD-Detector to run over K = 5 and K = 10, we note that
the sets of related discussion candidates created when we set K = 5 are subsets
of K = 10. Consequently, there is a risk of false-positive predictions propagation
through the related discussion candidates’ sets. We identiﬁed this propagation
problem by analyzing the sets of related discussion candidates detected considering
the conﬁguration groups p = Gatsby and c = Ideas. The same unrelated discussion
pair (master, target) occurs in Rp=Gatsby|c=Ideas|K=5 and Rp=Gatsby|c=Ideas|K=10.
Although the precision rate tends to decrease, the approach detects new pairs of
related discussions when we vary the K values from 5 to 10.

Gatsby project: Analyzing the sets of candidates Rp=Gatsby|c=Q&A|K=5 and
Rp=Gatsby|c=Q&A|K=10, RD-Detector detected two new pairs of related discussions
when changing the K value from 5 to 10. The maintainer of the Gatsby project,
M Gatsby, judged discussions from both new pairs as related. Since all related dis-
cussion candidates from both sets were indeed related, the RD-Detector achieved
the best precision rate (100%). Considering p = Gatsby|c = Ideas|K = 5 and
p = Gatsby|c = Ideas|K = 10, the RD-Detector also detected two new pairs of
related discussion candidates by increasing the K value. However, one out of the
two new pairs was judged unrelated by M Gatsby. In this case, the precision rate
decreased from 83.33% to 75%.

As discussed in Section 4.2, when we set the category ﬁlter to ALL (c = ALL),
the RD-Detector calculates the similarity values between all discussion pairs in
the dataset. The approach detected seven related discussion candidates using the
conﬁguration p = Gatsby|c = ALL|K = 5 (Rp=Gatsby|c=ALL|K=5 = 7), of which
ﬁve elements are also present in set Rp=Gatsby|c=Q&A|K=10. Evaluators judged
all ﬁve elements in Rp=Gatsby|c=Q&A|K=10 set as related. M Gatsby judged that
the discussions of the two new pairs were related. We analyzed the discussions’
content and found that users create related discussions in diﬀerent categories.
The discussions of these two speciﬁc pairs were from Idea and Q&A categories,
respectively.

Regarding the set Rp=Gatsby|c=ALL|K=10, six of the 12 pairs are also present in
the set Rp=Gatsby|c=Q&A|K=10, two pairs are in Rp=Gatsby|c=Idea|K=5, and two pairs
also belongs to Rp=Gatsby|c=ALL|K=5. All those ten related discussion pairs contain
indeed related discussion posts. The maintainer endorsed the relatedness between
the discussion posts of one new pair and refuted the second one. By increasing the
value of K (K = 5 to K = 10) and setting p = Gatsby|c = ALL, the RD-Detector

Looking for related discussions on GitHub Discussions

27

detected ﬁve new pairs of related discussion candidates, of which one was judged
as unrelated by M Gatsby.

Homebrew project: Analyzing the sets of related discussion candidates cre-
ated considering the conﬁguration groups p = Homebrew|c = ALL|K = 5 and
p = Homebrew|c = ALL|K = 10, the RD-Detector detected 19 new related dis-
cussion candidates when changing the K value from 5 to 10. Out of the new 19
candidates, M Homebrew judged three pairs as not related (Table 7). The preci-
sion rate decreased from 100%, for p = Homebrew|c = ALL|K = 5, to 91.17% for
p = Homebrew|c = ALL|K = 10.

Next.js project: The RD-Detector detected 27 new related discussion candi-
dates when we varied the K value and ﬁxed p = N ext.js|c = Q&A. Out of the 27
new candidates, SE researchers judged seven pairs as not related. This means that
the approach detected 20 new pairs of related discussions by increasing the value
of K. However, the false positives decreased the RD-Detector precision rate from
almost 96% to nearly 91%. This scenario repeats to p = N ext.js|c = Ideas and
p = N ext.js|c = ALL. When we changed the K value from 5 to 10, the approach
detected 25 and 31 new related discussion candidates for c = Ideas and c = ALL,
respectively. In total, three and six new candidates for c = Ideas and c = ALL
were judged unrelated by SE researchers, respectively. In both cases, the precision
rate decreased.

Because of the intersection relationship between sets of related discussion can-
didates, SE researchers had already judged 101 of the 106 pairs of related discus-
sion candidates in set Rp=Next.js|c=ALL|K=5. Researchers evaluated the new ﬁve
pairs as related. The ﬁve pairs have two common characteristics: (1) they had
discussions created by the same user, and (2) they had one of the discussions cre-
ated as Q&A and the other as Ideas. This ﬁnding corroborates the Gatsby project
ﬁndings. Users create related discussion posts in diﬀerent categories. Regarding
Rp=Next.js|c=ALL|K=10, researchers had already judged 133 out of the 137 related
discussion pairs in the set. They also judged the four new pairs as being related.
Maintainers can set the value of K according to their respective interests. De-
creasing the K value increases the RD-Detector precision rate. Higher precision
values ensure greater assertiveness in detecting true positives. Conversely, increas-
ing the K value may reduce the precision rate. However, increasing the K value
also increases the number of detected related discussion candidates.

6.2 False-positive RD-Detector predictions

Four authors of this paper manually analyzed the false positives presented in
Table 7. Based on evidence extracted from the discussion posts, we identiﬁed
some limitations on using the proposed approach. We describe the false positive
as follows.

Gatsby project: The two false-positive cases are related to discussions classiﬁed
as Idea. The researchers identiﬁed that the RD-Detector did not capture the
project-related speciﬁcs on both pairs. Although the two posts of pair #1 (Table 7)
address the same topic (‘JavaScript linting utility ESLint’) and have keywords
intersection, they address diﬀerent problems. The discussion posts of the #2 pair
(Table 7) address the topic ‘Typescript’ and have project keywords intersection.
However, the speciﬁcity of the issues described in the discussion posts diﬀers.

28

Lima et al.

In both cases, we observed that the general-purpose model used to calculate
the similarity values of the discussions identiﬁed similar topical conversations.
However, it did not identify the project issue speciﬁcity. Based on these ﬁndings,
we observed that the RD-Detector could fail to treat particular contexts of software
projects. We will call this limitation the ‘project-speciﬁc limitation.’

Homebrew project: The analysis of the ﬁrst Homebrew’s unrelated pair, #3 (Ta-
ble 7), shows that the target discussion contains a link to the master discussion,
‘... for Homebrew mirror conﬁgurations. #1906’. Link references can endorse or
refute relationships between discussion posts. In this case, the text fragment that
contains the link reference does not refute that the discussions are related; how-
ever, it also does not clearly emphasize that they are related. We analyzed the
content of discussion pair #3. We identiﬁed a limitation regarding the concept of
‘related discussions’ that can directly inﬂuence the evaluation of the approach. We
concluded that the interpretation of the ‘related discussions’ concept depends on
the evaluators’ perspective. We call this limitation ‘concept imprecision.’

However, M Homebrew’s feedback pointed out that pair #3 is unrelated due
to the ‘project-speciﬁc limitation.’ The maintainer claims that both discussions
address the same project feature, but they diﬀer on the speciﬁcity of the issue
addressed. According to M Homebrew maintainer, one discussion ‘...is asking what
the policy is’ and the other one ‘...is announcing support for a new feature. ’ We also
identiﬁed that the ‘project-speciﬁc limitation’ justiﬁes the other two false positives
detected for the Homebrew project. The maintainer justiﬁes the unrelatedness of
the pair #4 (Table 7) arguing that the strategy captured a high level of abstraction
from the two discussions, ‘... problems with installing Homebrew.’ However, the
RD-Detector did not capture the speciﬁcity of the problem, ‘...the problem that’s
encountered is diﬀerent... .’ Therefore, the relationship between the two is not
conﬁrmed because, according to M Homebrew ‘...each needs a diﬀerent solution.’
We also analyzed the discussions’ content of the third unrelated pair #5 (Ta-
ble 7). Like the pair #4, the discussions in #5 address issues related to ‘Homebrew
installation’ but in diﬀerent computing environments. Again, the RD-Detector did
not capture the speciﬁcity of the problem; however, both discussion posts address
the same issue and have the project-related keywords.

Next.js project: Analyzing the Next.js false-positives, we note that the dis-
cussion creators (1) used screenshots to detail or describe the issues and (2) used
error logs descriptions to show the stack trace of where the error took place. We
also note the predominance of (3) template keywords and (4) project keywords in
the false-positive discussion posts.

Users can add screenshots in the discussion body to help explain their prob-
lems. However, the RD-Detector measures the semantic textual similarity between
discussions content. The approach does not use images as a source of evidence. The
discussion pairs #7 and #15 (Table 7) exemplify this scenario. In addition, the
master and target discussions use the same description template. After preprocess-
ing, the templates’ keywords may stand out against the real discussion content.
We also identiﬁed the predominance of the template keywords in the false-positive
pair #16.

Discussions creators also use error logs or descriptions to describe the system’s
discrepancies or nonconformities. During the preprocessing phase, we remove error
descriptions embedded in HTML tags. However, when users use error log content
to express their questions, they usually intend to ask for help in solving a speciﬁc

Looking for related discussions on GitHub Discussions

29

problem highlighted in the error log content. Removing the log also eliminates the
problem speciﬁcity. The false-positive discussion pair #11 (Table 7) exempliﬁes
the use of error log descriptions in discussion posts. However, project keywords in
discussions of pair #11 show similarities in the discussions’ contents.

The analysis of the pairs #13 and #14 shows that (again) both discussions use
the same description templates and have the same project keywords. Finally, the
false-positive pairs #6, #8, #9, #10, #12, #17, #18, #19, #20 (Table 7) present
the same set of keywords, which are limited due to the scope of the project.
Keywords match lead to the ‘project-speciﬁc limitation.’

Based on maintainers’ feedback, we can propose improvements to the proposed
approach. For example, we can use the maintainers’ judgments to optimize the
classiﬁer by providing samples of related and not related discussions. Furthermore,
we can design strategies to minimize the project-speciﬁc limitation by treating the
predominance of projects’ keywords and templates’ keywords.

6.3 The implications of this research

We envision this research enables opportunities for OSS communities and software
engineering researchers as follows.

Long-term sustainability of OSS communities: the proposed approach increases
the eﬀectiveness of related discussions detection in GitHub Discussions. Their de-
tection will no longer depend on the knowledge and availability of OSS maintain-
ers. Maintainers can beneﬁt from RD-Detector to address the labor-intensive task
of manually detecting related discussions and the time-intensive task of answer-
ing the same question multiple times. Such beneﬁts enable maintainers to focus
eﬀorts on performing other activities to support the OSS community build and
growth and the project sustainability, as pointed out by Dias et al. (2021). The
precision values presented in Table 6 show that we can use a general-purpose
deep learning model to detect related discussions in GitHub Discussions. Con-
sequently, we aim to build an automated agent (Bot) to run the RD-Detector
algorithm and report occurrences of related discussion posts in the GitHub repos-
itories. In addition, we envision a practical application of our results in supporting
the commenters’ tasks on GitHub. The commenters are contributors that enrich
discussions in the OSS project by adding comments on collaborative conversation
threads (C´anovas Izquierdo and Cabot, 2022).

Knowledge sharing in OSS communities: due to the importance of knowledge
sharing to OSS teams (Chen et al., 2013; Tantisuwankul et al., 2019), RD-Detector
can help maintainers reorganize project-related issues repeatedly discussed in OSS
communities. According to the OSS maintainers’ perspective, the approach helps
in identifying discussion threads that could be merged or made as comments on
one another (Section 5.1), allowing users to ﬁnd the correct answers in one place
and avoid project knowledge decentralization (Ahasanuzzaman et al., 2016; Silva
et al., 2018). In addition, managing related discussions can make ﬁnding the infor-
mation community members are looking for easier since it enables reducing noise
occurrences and helps valuable discussions posts become ‘more visible’ (Mamykina
et al., 2011).

OSS communities’ growth: the remarkable acceptance of GitHub Discussions
in OSS projects (Liu, 2021) creates expectations about the forum growth. As

30

Lima et al.

GitHub Discussions grows and becomes popular, it faces the same limitations
and problems that other similar forums, such as related discussions (duplicate
or near-duplicate) occurrence (Zhang et al., 2015; Ahasanuzzaman et al., 2016;
Zhang et al., 2017; Silva et al., 2018; Wang et al., 2020; Pei et al., 2021). Identify-
ing related threads can help users ﬁnd previous posts already asked and answered,
reducing the waste of time while waiting for feedback (Ahasanuzzaman et al.,
2016) and the fear of submitting duplicates (Ford et al., 2016). It also helps to
suggest related posts before creating a new thread (Zhang et al., 2017), assisting
users to ﬁnd posts with similar issues (Ahasanuzzaman et al., 2016; Zhang et al.,
2017), helping new users asking better questions (Ford et al., 2018), and reducing
the manual responsibility of maintainers to sustain a healthy community (Guizani
et al., 2022). Besides, every online community depends on volunteer activity, so
it is essential to attract and retain new members to guarantee community sus-
tainability (Cho and Wash, 2021; Guizani et al., 2022). However, it is well-known
that newcomers face barriers to onboard to OSS communities (Steinmacher et al.,
2015). Given that the GitHub Discussions is a rich project source of information,
newcomers and inexperienced users also may beneﬁt from identifying related posts
in the forum. Such users can analyze the related discussion threads to get diﬀerent
formulations of the same problem and better understand a project-related issue
(Abric et al., 2019). Through a cognitive process called ‘social or collaborative
reﬂection,’ newcomers and inexperienced users can learn from the reﬂection that
others registered in the related discussion threads (Prilla et al., 2020). In this way,
our approach can support the long-term growth and maintenance of the GitHub
Discussions forum.

OSS Maintainers: previous research report that OSS maintainers need to man-
age multiple aspects of the project to ensure that the project vision endures
(Guizani et al., 2022) and the projects’ long-term sustainability (Dias et al., 2021).
To do so, maintainers’ perform diﬀerent activities encompassing code and non-
coding tasks (Dias et al., 2021; Trinkenreich et al., 2021). Such responsibilities in-
tensify the workload of OSS maintainers. Tan and Zhou (2020) highlight proposing
tools as a best practice to decentralize maintainers’ responsibilities. In addition,
Dias et al. (2021) report that ‘to sustain a long term vision of the project, main-
tainers should delegate tasks.’ In this way, the RD-Detector emerges as a tool
to alleviate the maintainers’ labor-intensive task of detecting related discussion
occurrences and managing duplicates or near-duplicate discussions.

Research Opportunities: to address the diversity and uniqueness of OSS com-
munities hosted on GitHub, the RD-Detector is based on a Sentence-BERT general-
purpose machine learning model. Besides, the approach uses descriptive statistics
to calculate the local threshold used to detect related discussion pairs. Given that
software projects are unique development ecosystems, we can not use a single and
universal number to detect related discussion pairs. Our results (Table 6) endorse
the eﬀectiveness of the RD-Detector. However, we do not know in advance how
many or which discussions are genuinely related in the GitHub Discussions
fo-
rum, which prevents us from measuring the approach recall rate. This gap brings
opportunities for future work.

Looking for related discussions on GitHub Discussions

31

7 Limitations

Although we proposed a parameterizable approach based on general-purpose ma-
chine learning models and descriptive statistics, this research may likely present
limitations.

The research’s main limitation relates to the generalization of the results. We
are aware of the diversity and uniqueness of the OSS communities hosted on
GitHub. Since we assessed RD-Detector over three OSS communities, we cannot
guarantee whether our ﬁndings generalize to all projects. GitHub’s professionals
(coauthors in this research) singled out the selected OSS communities to mini-
mize this limitation. In addition, the dataset refers to a speciﬁc time window that
does not reﬂect the current moment of GitHub Discussions. However, the Next.js
project stands out as it has a high rate of use of the forum. More experiments are
needed to assess the results in diﬀerent OSS communities.

Moreover, judging the relatedness between discussions is a subjective task and
could introduce biases in reporting the RD-Detector eﬀectiveness. The RD-Detector
evaluation presents some challenges: (1) people who judge the relatedness of the
discussions need to semantically analyze the content of the posts; (2) judging
the technical aspects of the discussion posts requires prior knowledge about the
project; and (3) the judgment involves human (in)precision regarding the concept
of relatedness, although we deﬁned the concept of related and duplicate discus-
sions, the interpretation depends on the evaluators’ perspective. To minimize this
threat, we introduced the ‘related discussions’ meaning to OSS maintainers and
SE researchers before classifying related discussion candidates. We also contacted
selected OSS maintainers to judge the related discussion candidates.

The local threshold calculation can also be a limitation. We consider related dis-
cussion candidates those pairs identiﬁed as outliers in a distribution S. S contains
the similarity values of the K most similar target discussions for each discussion
post in the dataset. As we increase the value of K, the median of the distribution
decreases, and so does the local threshold value. However, we focused on improving
the RD-Detector precision. Higher precision values ensure greater assertiveness in
detecting related discussions. So, we set the K value to 5 and 10.

Finally, it was not possible to measure the RD-Detector’s recall rate. To the
best of our knowledge, this is the ﬁrst work detecting related discussions in the
GitHub Discussions forum. However, achieved precision values ( Table 6 ) show
the eﬀectiveness of the RD-Detector in detecting related discussions. Maintain-
ers beneﬁt from the RD-Detector quality by decreasing the time spent reviewing
related discussion candidates.

8 Conclusion

In this work, we presented the RD-Detector, an approach to detect related discus-
sion candidates in the GitHub Discussions forum. We assessed RD-Detector over
public discussions collected from three OSS communities. In total, the approach
evaluated the semantic similarity of 11,162 discussion posts. OSS maintainers and
SE researchers judged the detected related discussion candidates. They classiﬁed
pairs of related discussion candidates as duplicates, related or not related. We mea-
sured the RD-Detector precision rate using OSS maintainers’ and SE researchers’

32

Lima et al.

judgment. Our results show that we can use a general-purpose deep machine learn-
ing model applicable to NLP problems to detect related discussions in the GitHub
Discussions forum. The RD-Detector achieved an average precision rate of 90.11%,
considering the top-10 most similar discussion pairs for each discussion post in the
dataset D (K = 10).

The RD-Detector uses a Sentence-BERT (SBERT) pre-trained general-purpose
model to compute semantically signiﬁcant sentence embeddings of discussion posts
and detect related discussion candidates. Using publicly available machine learning
models brings ﬂexibility to the approach. As researchers release new exchangeable
models, one can update the RD-Detector. Besides, the RD-Detector calculates
local threshold values to detect related discussion candidates in diﬀerent OSS
communities. We use descriptive statistics to calculate the upper inner fence value
of a distribution containing the similarity values data of the K most similar target
discussions to each discussion under processing. We consider related discussions
those identiﬁed as outliers in the distribution.

The approach’s outputs were unique (Tables 3, 4, and 5). Our results highlight
the need for strategies designed to quickly adapt to the dynamism with which
OSS communities grow and change. We found that OSS communities create re-
lated discussions to emphasize their need for help, add new information on previ-
ous discussions threads, and ask for alternative solutions. From the maintainers’
perspective, we also found that related discussions have the same resolution and
similar problems, address additional issues, and address diﬀerent speciﬁcs within
the same topic.

Maintainers can beneﬁt from RD-Detector to address the labor-intensive task
of manually detecting related discussions and the time-intensive task of answer-
ing the same question multiple times. In addition, OSS maintainers can beneﬁt
from the approach to prioritize the development or update project-related issues
frequently discussed, understand why users duplicate questions, control the propa-
gation of duplicates, and support the project knowledge sharing. According to the
maintainers’ feedback, one can merge related discussion threads or make related
discussions as comments on one another.

We reported and discussed our results with the professional GitHub team (co-
authors of this work). Our ﬁndings showed a real need to plan and tackle related
discussions on GitHub Discussions. Consequently, the GitHub engineering team
is testing some changes to the Discussions interface. In addition to providing the
discussion title, body text, and category, users would have to conﬁrm they have
searched for similar threads before creating new posts (using a checkbox).

As the next step, we intend to implement RD-Detector as a bot to run over
the GitHub Discussions data and fetch, analyze, detect, and report related discus-
sions occurrences. We believe that this research also brings opportunities to enable
project knowledge acquisition and transfer by providing users with project-related
issues and by making the projects’ knowledge easy to ﬁnd. In addition, our results
can enable project knowledge reuse as users can access related discussions that
have already been asked and answered and the project knowledge categorization
by identifying similar documents.

Acknowledgements We would like to thank GitHub and the OSS maintainers for support-
ing this research. We also would like to thank the ﬁnancial support granted by CNPq through
processes number 314174/ 2020-6 and 313067/2020-1. CAPES ﬁnancial code 001. FAPESP un-

Looking for related discussions on GitHub Discussions

33

der grant 2020/05191-2. FAPEAM through process number 062.00150/2020. This research was
also carried out within the scope of the Samsung-UFAM Project for Education and Research
(SUPER), according to Article 48 of Decree number 6.008/2006(SUFRAMA).

Declaration of competing interest

The fourth and the ﬁfth authors of this manuscript participate in the GitHub
Discussions engineering team. The other authors have no competing interests to
declare that are relevant to the content of this article.

References

Abric, D., Clark, O. E., Caminiti, M., Gallaba, K., and McIntosh, S. (2019). Can
duplicate questions on stack overﬂow beneﬁt the software development commu-
nity? In 2019 IEEE/ACM 16th International Conference on Mining Software
Repositories (MSR), pages 230–234. IEEE.

Agirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W.,
Lopez-Gazpio, I., Maritxalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe,
J. (2015). SemEval-2015 task 2: Semantic textual similarity, English, Spanish
and pilot on interpretability. In Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015), pages 252–263, Denver, Colorado. As-
sociation for Computational Linguistics.

Ahasanuzzaman, M., Asaduzzaman, M., Roy, C. K., and Schneider, K. A. (2016).
Mining duplicate questions of stack overﬂow. In 2016 IEEE/ACM 13th Working
Conference on Mining Software Repositories (MSR), pages 402–412. IEEE.
Calefato, F., Gerosa, M. A., Iaﬀaldano, G., Lanubile, F., and Steinmacher, I.
(2021). Will you come back to contribute? investigating the inactivity of oss
core developers in github. arXiv preprint arXiv:2103.04656.

C´anovas Izquierdo, J. L. and Cabot, J. (2022). On the analysis of non-coding roles

in open source development. Empirical Software Engineering, 27(1):1–32.

Chen, X., Li, X., Clark, J. G., and Dietrich, G. B. (2013). Knowledge sharing in
open source software project teams: A transactive memory system perspective.
International Journal of Information Management, 33(3):553–563.

Cho, J. and Wash, R. (2021). How potential new members approach an online
community. Computer Supported Cooperative Work (CSCW), 30(1):35–77.
Cohen, J. (1960). A coeﬃcient of agreement for nominal scales. Educational and

psychological measurement, 20(1):37–46.

Community, G. (2022). Gatsby v4. url: https://github.com/gatsbyjs/gatsby/

blob/master/readme.md .accessed 23 january 2022.

Dias, E., Meirelles, P., Castor, F., Steinmacher, I., Wiese, I., and Pinto, G. (2021).
What makes a great maintainer of open source projects? In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE), pages 982–994.
IEEE.

Face,

H.

(2021).

sentence-transformers/all-mpnet-base-v2.
url:https://huggingface.co/sentence-transformers/all-mpnet-base-v2 . accessed
13 january 2022.

34

Lima et al.

Ford, D., Lustig, K., Banks, J., and Parnin, C. (2018). ”we don’t do that here”:
How collaborative editing with mentors improves engagement in social q&a com-
munities.
In Proceedings of the 2018 CHI Conference on Human Factors in
Computing Systems, CHI ’18, page 1–12, New York, NY, USA. Association for
Computing Machinery.

Ford, D., Smith, J., Guo, P. J., and Parnin, C. (2016). Paradise unplugged: Iden-
tifying barriers for female participation on stack overﬂow. In Proceedings of the
2016 24th ACM SIGSOFT International Symposium on Foundations of Soft-
ware Engineering, FSE 2016, page 846–857, New York, NY, USA. Association
for Computing Machinery.

GitHub, I. (2021a). Github discussions. url: https://docs.github.com/en/ discus-

sions. accessed 23 january 2022.

GitHub, I. (2021b). Managing categories for discussions in your repository.
url:
https://docs.github.com/en/discussions/managing-discussions-for-your-
community/managing-categories-for-discussions-in-your-repository. accessed 23
january 2022.
I.

discussions.

Searching

GitHub,

(2021c).

url:

https://docs.github.com/en/github/searching-for-information-on-
github/searching-discussions. accessed 23 january 2022.

GitHub,

I. (2022). What is github discussions? a complete guide. url:
https://resources.github.com/devops/ process/planning/discussions/. accessed
21 april 2022.

Guizani, M., Zimmermann, T., Sarma, A., and Ford, D. (2022). Attracting
and retaining oss contributors with a maintainer dashboard. arXiv preprint
arXiv:2202.07740.

Hata, H., Novielli, N., Baltes, S., Kula, R. G., and Treude, C. (2022). Github
discussions: An exploratory study of early adoption. Empirical Software Engi-
neering, 27(1):1–32.

Kim, Y., Lee, S., Dollmann, M., and Geierhos, M. (2005). Improving classiﬁers for
semantic annotation of software requirements with elaborate syntatic structure.
International Journal of Advanced Science and Technology, ISSN, 4238:123–136.
Landis, J. R. and Koch, G. G. (1977). The measurement of observer agreement

for categorical data. biometrics, pages 159–174.

Lee, I. and Shin, Y. J. (2020). Machine learning for enterprises: Applications,

algorithm selection, and challenges. Business Horizons, 63(2):157–170.

Li, L., Ren, Z., Li, X., Zou, W., and Jiang, H. (2018). How are issue units linked?
empirical study on the linking behavior in github. In 2018 25th Asia-Paciﬁc
Software Engineering Conference (APSEC), pages 386–395. IEEE.

Li, Z., Yin, G., Yu, Y., Wang, T., and Wang, H. (2017). Detecting duplicate
pull-requests in github. In Proceedings of the 9th Asia-Paciﬁc Symposium on
Internetware, pages 1–6.

Li, Z., Yu, Y., Zhou, M., Wang, T., Yin, G., Lan, L., and Wang, H. (2020). Redun-
dancy, context, and preference: An empirical study of duplicate pull requests in
oss projects. IEEE Transactions on Software Engineering.

Liu, E. (2021). Github discussions is out of beta. url: https://github.blog/2021-

08-17-github-discussions-out-of-beta/. accessed 15 january 2022.

Liu, E. (2022). How ﬁve open source communities are using github discus-
sions. url:https://github.blog/2022-01-13-how-ﬁve-open-source- communities-
are-using-github-discussions/.

Looking for related discussions on GitHub Discussions

35

Liu, T.-Y. (2011). Learning to rank for information retrieval.
Mamykina, L., Manoim, B., Mittal, M., Hripcsak, G., and Hartmann, B. (2011).
Design lessons from the fastest q&a site in the west.
In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, page
2857–2866, New York, NY, USA. Association for Computing Machinery.

Niyogi, S. (2020). New from satellite 2020: Github discussions, codespaces, secur-
ing code in private repositories, and more. url: https://github.blog/2020-05-06-
new-from-satellite-2020-github-codespaces-github- discussions-securing-code-in-
private-repositories-and-more/. accessed 21 january 2022.

Pei, J., Wu, Y., Qin, Z., Cong, Y., and Guan, J. (2021). Attention-based model
for predicting question relatedness on stack overﬂow. In 2021 IEEE/ACM 18th
International Conference on Mining Software Repositories (MSR), pages 97–
107. IEEE.

P´erez-Soler, S., Guerra, E., and de Lara, J. (2018). Collaborative modeling
and group decision making using chatbots in social networks. IEEE Software,
35(6):48–54.

Polyzotis, N., Roy, S., Whang, S. E., and Zinkevich, M. (2017). Data management
In Proceedings of the 2017 ACM

challenges in production machine learning.
International Conference on Management of Data, pages 1723–1726.

Prilla, M., Blunk, O., and Chounta, I.-A. (2020). How does collaborative reﬂection
unfold in online communities? an analysis of two data sets. Computer Supported
Cooperative Work (CSCW), 29(6):697–741.

Project, H. (2022). Homebrew documentation. url:https://docs.brew.sh/. accessed

23 january 2022.

Reimers,

N.

(2021).

Sentencetransformers

documentation.

url:

https://www.sbert.net/. accessed 15 january 2022.

Reimers, N. and Gurevych, I. (2019). Sentence-bert: Sentence embeddings using

siamese bert-networks. arXiv preprint arXiv:1908.10084.

Ren, L., Zhou, S., Kastner, C., and Wasowski, A. (2019). Identifying redundan-
cies in fork-based development. In 2019 IEEE 26th International Conference
on Software Analysis, Evolution and Reengineering (SANER), pages 230–241.
IEEE.

Schelter, S., Biessmann, F., Januschowski, T., Salinas, D., Seufert, S., and Szarvas,

G. (2018). On challenges in machine learning model management.

Silva, R. F., Paix˜ao, K., and de Almeida Maia, M. (2018). Duplicate question
detection in stack overﬂow: A reproducibility study. In 2018 IEEE 25th inter-
national conference on software analysis, evolution and reengineering (SANER),
pages 572–581. IEEE.

Sirres, R., Bissyand´e, T. F., Kim, D., Lo, D., Klein, J., Kim, K., and Traon, Y. L.
(2018). Augmenting and structuring user queries to support eﬃcient free-form
code search. Empirical Software Engineering, 23(5):2622–2654.

Steinmacher, I., Conte, T., Gerosa, M. A., and Redmiles, D. (2015). Social barri-
ers faced by newcomers placing their ﬁrst contribution in open source software
projects. In Proceedings of the 18th ACM conference on Computer supported
cooperative work & social computing, pages 1379–1392.

Storey, M.-A., Singer, L., Cleary, B., Figueira Filho, F., and Zagalsky, A. (2014).
The (r) evolution of social media in software engineering. In Future of Software
Engineering Proceedings, pages 100–116.

36

Lima et al.

Storey, M.-A., Zagalsky, A., Figueira Filho, F., Singer, L., and German, D. M.
(2016). How social and communication channels shape and challenge a par-
ticipatory culture in software development.
IEEE Transactions on Software
Engineering, 43(2):185–204.

Tan, X. and Zhou, M. (2020). Scaling open source software communities: Chal-

lenges and practices of decentralization. IEEE Software, 39(1):70–75.

Tantisuwankul, J., Nugroho, Y. S., Kula, R. G., Hata, H., Rungsawang, A., Lee-
laprute, P., and Matsumoto, K. (2019). A topological analysis of communication
channels for knowledge sharing in contemporary github projects. Journal of Sys-
tems and Software, 158:110416.

Trinkenreich, B., Guizani, M., Wiese, I. S., Conte, T., Gerosa, M., Sarma, A., and
Steinmacher, I. (2021). Pots of gold at the end of the rainbow: What is success
for open source contributors. IEEE Transactions on Software Engineering.
Tukey, J. W. et al. (1977). Exploratory data analysis, volume 2. Reading, Mass.
Vercel, I. (2022). Create a next.js app. url:https://nextjs.org/learn/basics/create-

nextjs-app.accessed 23 january 2022.

Wang, L., Zhang, L., and Jiang, J. (2020). Duplicate question detection with deep

learning in stack overﬂow. IEEE Access, 8:25964–25975.

Yazdaninia, M., Lo, D., and Sami, A. (2021). Characterization and prediction
of questions without accepted answers on stack overﬂow. In 2021 IEEE/ACM
29th International Conference on Program Comprehension (ICPC), pages 59–
70. IEEE.

Yu, Y., Li, Z., Yin, G., Wang, T., and Wang, H. (2018). A dataset of duplicate
pull-requests in github. In Proceedings of the 15th International Conference on
Mining Software Repositories, pages 22–25.

Zhang, W. E., Sheng, Q. Z., Lau, J. H., and Abebe, E. (2017). Detecting duplicate
posts in programming qa communities via latent semantics and association rules.
In Proceedings of the 26th International Conference on World Wide Web, pages
1221–1229.

Zhang, Y., Lo, D., Xia, X., and Sun, J.-L. (2015). Multi-factor duplicate ques-
tion detection in stack overﬂow. Journal of Computer Science and Technology,
30(5):981–997.

Zhang, Y., Wu, Y., Wang, T., and Wang, H. (2020). ilinker: a novel approach for
issue knowledge acquisition in github projects. World Wide Web, 23(3):1589–
1619.

Zhou, L., Pan, S., Wang, J., and Vasilakos, A. V. (2017). Machine learning on big

data: Opportunities and challenges. Neurocomputing, 237:350–361.

