2
2
0
2

r
a

M
8
2

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
4
3
4
8
0
.
4
0
2
2
:
v
i
X
r
a

Better automation of beamline control at HEPS

Yu Liu1,∗, Xue-Wei Dong1, Gang Li1

Synopsis

Keywords: EPICS; beamline control; package management; conﬁguration management; soft-

ware architecture.

A simple, ﬂexible packaging system for EPICS modules and related software is implemented at
HEPS, which also produces reusable modular IOC executables that can be composed to replace
many multi-device IOC applications. A conﬁguration convention is suggested that helps to imple-
ment easily maintainable multi-IOC setups; also introduced is our ongoing project of comprehensive
beamline services to further simplify conﬁguration management.

Abstract

At the High Energy Photon Source (HEPS) where up to 90 beamlines can be provided in the
future, minimisation of workload for individual beamlines and maximisation of knowledge about
one beamline that can be applied to other beamlines is essential to minimise the total complexity
in beamline control. Presented in this paper are our eﬀorts to achieve these goals by composing
relatively simple utilities and mechanisms to automate tasks, and always remembering to keep
our automation solutions simple and clear. After an introduction to our choice of basic software
in EPICS-based beamline control, the issues encountered in introducing package management to
EPICS modules, as well as our solutions to them, are presented; then the design and implementa-
tion of our packaging system is concisely discussed. After a presentation of our eﬀorts to reduce
the need for self-built multi-device EPICS IOC applications by providing reusable modular IOC
executables, our implementation of easily maintainable multi-IOC setups through the separation
and minimisation of each user’s IOC conﬁgurations is given. Finally, the ongoing project of com-
prehensive beamline services at HEPS to further simplify conﬁguration management on multiple
scales, ranging from individual beamline devices to all beamlines at HEPS, is introduced.

1

Introduction

The High Energy Photon Source (HEPS) (Jiao et al., 2018) is a 4th-generation synchrotron
radiation facility under steady construction in Beijing, China. 14 beamlines will be provided in
2025, when Phase I of HEPS is planned to open up to the public; up to about 90 beamlines can
be provided in further phases. The beamline control system at HEPS (Chu et al., 2018) is mainly
responsible for the proper operation of individual beamline devices and the abstraction of the
various device-speciﬁc control interfaces for these devices as consistent interfaces. Given the huge
number of beamlines, it is undoubted that for unnecessary work to be reduced, reusable designs and
plans are essentials instead of luxuries. In this paper, we focus on the beamline control system of
the facility; nevertheless, we believe that in other parts like the accelerator control system, or even
in other large scientiﬁc facilities like free-electron laser facilities and spallation neutron sources, the
same principles described here are still applicable, and most techniques can be adapted.

Given the background above, the tasks of beamline control can be considered a kind of computer
system administration with a focus on diverse device support and large-scale deployment; with

∗ Correspondence e-mail: liuyu91@ihep.ac.cn.
1 Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, People’s Republic of China.

1

 
 
 
 
 
 
this focus, it is easy to note that tasks for HEPS beamline control need to be done in simple
yet reproducible ways. For beamline control at HEPS, we predominantly use Unix-like operating
systems, where oﬀ-the-shelf utility programs are abundant and it is very easy to compose these
utilities to automate tasks. With the utilities, we can express many tasks as command line code,
and further abstract common tasks as scripts; as the amount of manual operations decrease, the
tasks cost less time and labour, yet the results become more reliable and maintainable – in other
words, both eﬃciency and reproducibility increase.

If “control” is to be seen as a kind of “automation”, then the practice above can be seen as
“automation of automation”; in essence, what we do is programming speciﬁc to the domain of
system administration. When designing and implementing the beamline control system of HEPS,
we ﬁnd that by seriously considering the programming nature of tasks, we can often identify the key
factors in problems; we may ﬁnd surprisingly simple solutions to these problems by the composition
of relatively simple mechanisms and utilities, like the use of the ﬂock utility to implement critical
sections (cf. Subsection 3.2). In this paper, by showing some examples for the practice above, we
attempt to convey the idea that it is beneﬁcial to regard beamline control as a serious domain of
programming.

As is the case with any other kind of programming, while the abstraction of beamline control
tasks reduces complexity, it also brings about additional complexity itself. For multiple reasons,
Experimental Physics and Industrial Control System (EPICS) is chosen as the basis for the device
control interfaces at HEPS. We ﬁnd that when performing certain tasks, by deviating from the
“orthodox” EPICS practice, we are able to make the task much simpler – sometimes by an order of
magnitude, or even more; in some cases, we also deviate from the “orthodox” in certain non-EPICS
parts of beamline control. Therefore in this paper, what we discuss is not only “automation of
automation” itself, but also our choices of strategy when implementing certain requirements, and
more importantly the rationale behind the choices.

2 Package management

2.1 Choice of basic software

Due to the requirement of large-scale deployment at HEPS, a ﬁrst issue to consider is the
installation of supporting software for the many kinds of devices used in beamline control. The
automation of this task belongs to the ﬁeld of package management, which revolves around the
automated installation of software modules and the automatic management of dependencies. There
are many package managers, like DPKG/APT of Debian, RPM /yum of Red Hat, Portage of
Gentoo, etc; there are also cross-distribution package managers like Conda, pkgsrc and Guix,
and we are aware of eﬀorts, like (Bertrand and Harrisson, 2019), to use this kind of package
managers for software involved in beamline control. However, we ﬁnd that in order to properly
handle dependencies, we need to duplicate eﬀorts in packaging software already provided by the
distribution, or otherwise we would need to resort to the oﬃcial package manager of the distribution
for many dependencies. The latter would not only be inconvenient, but also degrade the portability
of our packages, which is an important reason to use cross-distribution package managers in the
ﬁrst place.

For various reasons, CentOS is currently the primary operating system running on computers in
control systems at our institute. For us, while CentOS is not known for minimalism/simplicity or
ﬂexibility, it is usable enough for automation work in control systems. More importantly, the strong
compatibility between minor releases (like 7.1 and 7.9) of CentOS makes its basic behaviour (with-
out deep customisation to the system itself) quite predictable. Factors like the inclusion of all oﬃ-
cial base packages for a minor release in single .iso ﬁles (like CentOS-7-x86_64-Everything-2009
.iso) and the availability of deployment tools like Kickstart (cf. Subsection 3.2) also contribute
to easy automated batch deployment. Because of the lifecycle of CentOS releases, we currently
base the beamline control system of HEPS on CentOS 7. We are aware of the discontinuation
of non-rolling releases of CentOS 8 (Bowen, 2020) and the resulting uncertainty; depending on
future developments of the situation, we will evaluate the migration to a suitable alternative Linux
distribution one or two years prior to the public opening of HEPS Phase I in 2025. We estimate
that the migration will take roughly 3 months if we are to use a Red Hat-like distribution, or about

2

6 months if not. Anyway, for reasons above, we currently use RPM, the oﬃcial packaging format
of CentOS, for beamline control software.

We are aware of the trend in the EPICS community to migrate to EPICS 7, and the declining
activity resulted around EPICS 3; considering this background, we plan to use EPICS 7 whenever
applicable, so that we have the best community resources around the version we use. However,
due to the relatively immature development and documentation status of new features (most
importantly the PVA protocol) in EPICS 7, we deliberately avoid these features, and instead
use EPICS 7 like EPICS 3. Finally, as is quite common with EPICS in beamline control at
major synchrotron radiation facilities, we extensively use the synApps collection (Mooney, 2010)
of EPICS modules for HEPS; this has very profound eﬀects on the packaging of EPICS for HEPS,
which is discussed in Subsection 2.2. Here we also note that although the oﬃcial documentation
of synApps does not explicitly state about its compatibility with EPICS 7, very few build-time or
runtime errors have actually occurred in our experience that are traced back to using EPICS 7,
which proves that EPICS 7 is mostly backward compatible with EPICS 3 in terms of functionalities
shared between both major versions.

2.2 Packaging EPICS

EPICS base explicitly supports installation to a separate location with few negative eﬀects on
downstream code like synApps etc, by specifying the ${INSTALL_LOCATION} variable which is fairly
similar to the ${DESTDIR} variable supported by many open-source software projects. However, on
the current version of synApps homepage, the recommended way to install the software collection
is to extract the source code archive into a certain directory (like /opt/epics/support) in the sys-
tem, build the source code, and then use the built libraries and executables in place. So there is no
separation between building and installation phases with synApps, and consequently no clear dis-
tinction between source code and ﬁles which would be installed separately if ${INSTALL_LOCATION}
was used. This allows applications to refer to code that is not explicitly to be “installed”, eg.
areaDetector modules referencing ﬁles in ${ADCORE}/ADApp/Db. While perhaps convenient, this
also results in implicit dependence on the synApps directory layout, which is very unfriendly to
packaging.

A ﬁrst issue we can easily notice is the directory layout. Ideally, there should be preset direc-
tories for diﬀerent types of installed ﬁles: executables to a common directory, libraries to another
common directory, etc. This way, modules just need to refer to these preset directories for nec-
essary ﬁles without worrying about where to ﬁnd each and every module they depend on. There
do exist attempts to package EPICS modules in this way, with the NSLS-II package repository for
EPICS and RTEMS (Brookhaven National Laboratory, 2018) being perhaps the most well-known
example. Nevertheless as far as we know, none of them provide a satisfactory coverage of modules
we are interested in, most importantly motor and areaDetector modules. From the open-source
packaging code we obtain from these projects, we conclude that attempts to use use common
directories for diﬀerent modules instead of the synApps layout require an inordinate amount of
work by packagers, who need to work around references outside ${INSTALL_LOCATION} that may
occur in too many ways possible to handle in a way representable by simple, clear code. For these
reasons, the synApps layout is preserved in our packaging system; and not only modules provided
by synApps are organised in its original way, extra modules, including but not limited to additional
motor and area detector modules, are also put into directories similar to the built-in modules they
look like.

The second issue is ﬁle conﬂicts. With ${INSTALL_LOCATION} speciﬁed, ﬁles in the configure
directory are copied into the configure subdirectory of ${INSTALL_LOCATION}; no consideration
is taken for the coexistence of multiple modules, which may have conﬂicting configure contents.
This kind of conﬂicts also contribute to the diﬃculty in proper packaging of EPICS modules; with
the synApps directory layout, we do not need to worry about the conﬂicts between configure
contents of modules, but another kind of ﬁle conﬂicts arise. Because our system builds synApps
and many other EPICS modules in place, we cannot directly build them on a computer with
themselves already installed, which is highly inconvenient; we used to solve this by running our
packaging system in virtual machines, and now we use Docker containers (cf. Subsection 2.3).

The third issue is permissions. The synApps directory layout is unrelocatable: the directories,

3

even as a whole, cannot be moved around once it has been built, mainly because of parameters
like library search paths and environment variables set to certain absolute paths by the build sys-
tem of EPICS. Because of this, our packaging system builds synApps and other EPICS modules
depending on it in /opt/epics/support; to avoid accidentally tampering with system ﬁles, the
packaging system itself is run as a normal user. However, for the same purpose of avoiding un-
intended manipulation of packaged ﬁles by users, these ﬁles should normally be installed into a
directory hierarchy owned by the root user, and themselves also be owned by root. So in order
to build synApps and many other modules, we need write access in directory hierarchies owned
by root, even though we build packages as a normal user. We resolve this issue by using the sudo
utility for controlled privilege escalation: we set up password-less sudo permission for a “builder”
user, and run our packaging system as this user, so it can acquire root privilege when necessary
without manual interaction. The use of root privilege is purposefully minimised: eg. when build-
ing synApps, we ﬁrst use it to move a user-owned support hierarchy, extracted from the source
code archive and properly patched, into the /opt/epics directory; we also use root privilege to
move the mostly-built support hierarchy into a staging directory speciﬁed by the packaging tool
we use, and then do some necessary processing that can proceed without the privilege. For certain
modules (like motor modules that are not yet integrated into synApps), some built ﬁles may end
up in directories (like /opt/epics/support/motor/db) where ﬁles from other modules can also
exist. We handle this kind of situations by temporarily change the owner of directories with ﬁles
from multiple modules to the builder user in an early phase, and selectively move the non-root-
owned ﬁles in these directories into corresponding directories inside the staging directory when
the building procedure is mostly done (cf. Figure 2); the selective moving may be regarded as a
case where the separation between root and non-root operations actually simpliﬁes the building
procedure.

2.3 Beyond EPICS itself

Considering the limited human resources at our facility, maintainability is a ﬁrst concern in
the design of our packaging system. For this reason, we strive to make the system as “small” as
reasonable: we keep the packaging code simple and clear, with proper abstractions that are thin
yet ﬂexible enough. A ﬁrst choice we do is about the organisation of packaging code; one approach
is to put the code speciﬁc to one package alongside the source code archive(s), then generate a
source package, and ﬁnally build the binary package we normally use. This is how Debian-like and
Red Hat-like distributions usually do, and is also how the NSLS-II package repository is created;
however, a most important drawback of this approach is that the packager may be tempted to
apply site-speciﬁc modiﬁcations directly in source code repositories.
In a sense, this disperses
packaging code into original upstream code, which creates diﬃculty in packaging updates and
reviewing modiﬁcations; we feel this is at least an implicit reason for the diﬃculty in maintenance
of the NSLS-II packages (Lange, 2021). Due to reasons above, we put all our packaging code in a
single code repository (Figure 1; a fully open-source edition of our packaging system is available
at https://github.com/CasperVector/ihep-pkg-ose), and directly build binary packages from
there (the source package phase is optional in CentOS 7, and is also not used in our packaging
system); this approach is also prevalent in more recent Linux distributions like Alpine and Void.

As has been discussed in Subsection 2.2 and will be further exempliﬁed in this paper, in order
to reduce the intrinsic amount of tasks needed in packaging, we attempt to package EPICS and
related software modules similar to what have been intended by their authors, with only rela-
tively minor and non-intrusive modiﬁcations. To reduce duplication in the actual packaging code,
multiple measures have also been taken to abstract common tasks, like how eclasses for Gentoo
ebuild abstract common tasks. Because the majority of packaging code is written in the Shell
language, we currently use two library scripts, fn-build.sh and fn-setup.sh (Figure 2(a)), to
deﬁne abstractions for most tasks in the .spec package deﬁnition ﬁles used in RPM packaging;
they are not combined into one script because the code in fn-build.sh is only used during the
building procedure, while the code infn-setup.sh needs to be used when the binary package is
installed into the system. Certain common deﬁnitions are needed by both .spec ﬁles themselves
and the packaging code therein, so we deﬁne them in a rpmmacros ﬁle (Figure 2(b)) that will be
loaded by rpmbuild, the oﬃcial program to build RPM packages; also present in rpmmacros are

4

Figure 1: Directory layout of our packaging system

wrapper macros that pass parameters, which are known to rpmbuild but unknown to the library
scripts, to functions deﬁned in the latter, so as to further simplify .spec ﬁles (Figure 2(c)). By the
way, although we follow the synApps directory layout, we also try to keep a slightly higher level of
granularity by packaging basic modules (like asyn and calc), frequently used modules (like motor
and areaDetector ) and other modules as three separate packages (cf. Figure 3(b)); this not only
helps user to slim down systems, but also helps packagers to better understand the dependencies.
Apart from running rpmbuild, quite a lot of ancillary tasks are also involved in the creation
and management of a readily usable repository of binary packages: downloading and managing
source archives, downloading build-time or runtime dependencies (eg. re2c required by synApps)
that are not provided by the base repositories, running createrepo to generate necessary metadata
for the repository, etc. We use a script, boot.sh, to abstract these tasks, so that the packager
only needs to run simple commands (Figure 3(a)) to do common tasks. A point worth mentioning
is that instead of downloading dependencies as needed when building a package, we use a locally
mounted CentOS Everything image (cf. Subsection 2.1) to provide all base dependencies, and
batch-download all non-base dependencies into the epel subdirectory of our repository with the
epel_get subcommand of boot.sh. With the same mechanism, we also download frequently used
third-party packages, like procServ and Docker, into our repository, so that our users (which are
not very skilled Linux users in average) have a one-stop solution for common packages. The source
archives can also be batch-downloaded (using the src_get subcommand); the fact that all source
materials can be batch-downloaded enables us to pre-download the materials, transfer them to a
network-isolated machine with tens of CPU cores, and then perform the building procedure alone
on said machine, thus exploiting high-performance machines without regular network access for
big tasks.

Traditional RPM packaging does not involve the integrity check of source archives, which is
a common feature in many packaging frameworks, and also a crucial security feature given the
recently increasing threats posed by supply chain attacks. To compensate for this shortcoming, we
maintain a group of checksum ﬁles (SHA512SUMS etc) and compare the checksums of downloaded
source archives against them at the end of the src_get subcommand of boot.sh. Actually, for
each input outside of our packaging system, we have a chain of trust (Figure 4) to ensure its
integrity: there is SHA512SUMS-iso for the CentOS Everything image we use, which ensures every
base package contained therein is to an extent trusted; also in SHA512SUMS are the checksums for

5

.miscdockerDockerfile,...pybuildophyd-1.6.1.patch,ophyd.sh,...boot.sh,dboot.sh,pybuild.shSHA512SUMS,SHA512SUMS-isoSHA512SUMS-pyfetch,SHA512SUMS-pypi...SPECSsourcesepics-base-7.0.3.1-softIoc.patch,...epics-base.spec,epics-medm.spec,...fn-build.sh,fn-setup.sh,rpmmacrospypi...RPMS...SOURCES...(Filesusedinternallybydboot.sh)(Packagedefinitionsforpybuild.sh)(Coreutilityscripts)(Filesrelatedtothechainsoftrust)(Otherancillaryfiles)(WillbelinkedintoSOURCESbydboot.sh)(Packagedefinitionsforrpmbuild)(“Libraries”usedin.specfiles)(Willbecreatedbydboot.sh)Figure 2: Example snippets from (a) fn-build.sh, (b) rpmmacros and (c) our .spec ﬁles for extra
motor modules (where %{repo} has been set to strings like motorAcsMotion earlier)

Figure 3: Example usage of (a) boot.sh and (b) dboot.sh, both expected to be executed from
the root directory of the build system

6

(a)_mv_build(){if[-d"$2"];thensudormdir"$2";fi;sudomv"$1""$2";}_mv_dest(){mkdir-p"$2";sudomv"$1""$2";sudomkdir"$1";}_mv_me(){localdest="$1"d;shiftfordin"$@";domkdir-p"$dest$d";find"$d"-mindepth1-maxdepth1-not-userroot\-execmv-t"$dest$d"'{}''+'sudochownroot:root"$d";done}(b)%epics_archlinux-%{_arch}%epics_root/opt/epics%etop_synapps%{epics_root}/support%_chown_mesudochown"$(id-nu):$(id-ng)"%_mv_me_mv_me%{buildroot}(c)%build.%{_specdir}/fn-build.shcd%{etop_synapps}/motor;%_chown_medbdbdlib/%{epics_arch};cdmodules_mv_build%{_builddir}/%{name}/%{repo}%{repo};cd%{repo}make%{?_smp_mflags}CMD_CXXFLAGS="%{optflags}"%install.%{_specdir}/fn-build.shmtrdir=%{etop_synapps}/motor%_mv_me"$mtrdir"/db"$mtrdir"/dbd"$mtrdir"/lib/%{epics_arch}_mv_dest"$mtrdir"/modules/%{repo}%{buildroot}"$mtrdir"/modules(a)DownloadsourcearchivesandfileslikeOpenPGPpublickeysforrepositories:$./misc/boot.shsrc_getDownloaddependenciesandotherusefulthird-partypackages:$./misc/boot.shepel_getBuildaRPMpackage:$./misc/boot.shbuildepics-baseDownloadusefulPythonpackagesandsourcearchivesofcustomised/self-developedpackages:$./misc/boot.shpypi_getBuildacustomised/self-developedPythonpackage:$./misc/boot.shpybuildophyd(b)Createthebasecontainerimage:$./misc/dboot.shbaseDownloadexternalfilesusedtobuildRPMpackages:$./misc/dboot.shbase'src_get;epel_get'BuildEPICSbaseandsynApps:$./misc/dboot.shbase'buildepics-base;buildepics-synApps_core;buildepics-synApps_useful;buildepics-synApps_extras'Createtheepicscontainerimage,wherebaseandsynAppsarepreloaded:$./misc/dboot.shepicsBuildsomeextraEPICSmodules:$./misc/dboot.shepicsbuildepics-medmepics-motorAcsMotionepics-ADGenICamOpenPGP public keys of repositories we download third-party packages (including Docker which
is discussed later in this subsection) from, and the signatures of downloaded packages are checked
against the keys at the end of epel_get; the base image which our Docker container images are
based upon is also referenced by its checksum to avoid tampering. Another issue loosely related to
beamline control is also handled in our packaging system: providing a repository of Python packages
(currently mostly related to Bluesky, cf. Allan et al. 2019) for use in beamline experiments, some
of them with customised patches and some of them developed internally at our facility. We ﬁrst
use the pypi_get subcommand of boot.sh to batch-download most of these Python packages
from the Python Package Index (PyPI) and source archives of the several internally developed
packages; after this, we can use the pybuild subcommand to build the customised or internally
developed packages (the former also downloaded as source archives). The pybuild subcommand
delegates most of its tasks to a script, pybuild.sh (Figure 5), which is essentially a miniature
build system for Python packages heavily inspired by Gentoo ebuild. We also note that given the
high ﬂuidity of PyPI packages, a “hidden” advantage of our internal Python package repository is
the relative stability of systems using the packages therein, assuming our repository is not updated
too frequently.

Figure 4: Chains of trust for external ﬁles in our packaging system

As has been mentioned in Subsection 2.2, we used to run our packaging system in virtual ma-
chines to avoid interference with actually used machines where EPICS-related packages have been
installed; this also reduced the potential security risks resulted by password-less sudo permission of
the builder user. Nevertheless, the preparation and usage of virtual machine images for packaging
tasks is still fairly complex and involves quite a number of manual steps, which reduces repro-
ducibility and consequently reliability; moreover, ﬁle transmission between virtual machines and
the host machine is not very convenient to perform in an automated fashion. To further increase
reproducibility, we developed the helper script dboot.sh (Figure 3(b)) to create suitable Docker
container images and perform packaging tasks in containers instantiated from these images; built
RPM and Python packages are respectively put into the RPMS and pypi directories (Figure 1)
automatically, by using the “mounting” feature of Docker. To save time used in installing common
build-time dependencies, we distinguish between base and non-base container images: the former
used to build basic packages like EPICS base and synApps, with preparation tasks (like setting up
password-less sudo) done on image creation time; the latter based on the former but with the basic
packages pre-installed (and therefore needs to be recreated when these packages are updated). A
common practice in software engineering is continuous integration (CI), where updates to the code
repository trigger rebuild of the changed parts automatically. We currently do not have a fully
automatic CI workﬂow, due to the complexity in managing source archives as well as in rebuilding
dependencies and container images; however, we also ﬁnd the current script-assisted rebuilding pro-
cedure suﬃciently succinct and eﬃcient that it can be regarded as some kind of “slightly manual”
CI.

3 Minimisation of conﬁguration

3.1 Reusable modular IOC executables

Most time in EPICS-based beamline control are spent on maintaining IOC (“input/output
controllers”) applications which translate various device-speciﬁc control protocols into the Channel
Access (CA) protocol exposed by EPICS. IOC applications are mainly composed of IOC executables

7

SHA512SUMS-pyfetchSHA512SUMS-pypiSHA512SUMSSHA512SUMS-isoSourcearchivesforself-developedPythonpkgsSourcearchivesforcustomisedPythonpkgsUsefulPythonpkgsfromPyPISourcearchivesforRPMpkgs.repo&pubkeysforext.reposCentOSEverything.isofileUsefulext.pkgsExt.depsBasepkgsUsedbypybuild.shUsedasisUsedbyrpmbuildFigure 5: (a) ophyd.sh, the deﬁnition ﬁle of the ophyd package customised at HEPS; (b) a con-
densed pybuild.sh, expected to be executed from the root directory of the build system

(based on underlying support libraries) and data ﬁles (most importantly .dbd ﬁles, .db ﬁles and
.cmd ﬁles). From a slightly academic viewpoint, IOC executables are interpreters that interpret
.cmd ﬁles, which most importantly tell interpreters to load speciﬁed .dbd ﬁles and .db ﬁles. Upon
closer observation, it is not diﬃcult to ﬁnd that for IOC applications that handles the same type of
devices, the source ﬁles of the IOC executables and .dbd ﬁles are mostly unchanged: although the
ﬁlename of the executable and the .dbd ﬁle may change, these factors do not lead to any substantial
change in the behaviours of the IOC application. In other words, the exposed interface of an IOC
application is mostly aﬀected by the interpreted ﬁles excluding .dbd ﬁles; we may consider .dbd
ﬁles as an external yet closely coupled part of IOC executables, and other data ﬁles as conﬁguration
ﬁles in general. For many support modules, it is common to provide shared .db ﬁles, and let users
compose them into more complex interfaces by using .substitutions ﬁles; we regard this kind
of shared data ﬁles as conﬁguration fragments provided by upstream that can be composed or
customised by the user. From the above, it may be concluded that for IOC applications dealing
with at most one type of device, we may provide reusable IOC executables (including .dbd ﬁles
and other shared data ﬁles) that can be fed with diﬀerent conﬁguration ﬁles to act as diﬀerent
application instances. We will show in the end of this subsection that application instances for
diﬀerent devices, seen as modules, can be composed to implement the same requirements done by
at least a big fraction of previous multi-device IOC applications; thanks to this change, we can
minimise the need for users to build IOC applications by themselves, which greatly reduces the
need for systems like Sumo (Franksen, 2019). The applications instances that replace a multi-
device application can still be run on the same computer, and be started/stopped as a group; since
they would then communicate through the local loopback interface which can only be disrupted

8

(a)#Usage:./misc/pybuild.shophyddo_buildsrc="$(src_pypi"$pkg")"patches="$pkg-1.6.1.patch"(b)#!/bin/sh-etoppath="$PWD"buildpath="$toppath"/BUILDpybuild="$toppath"/misc/pybuildsrc_pypi(){localfgf="$(ls"$toppath/pypi/orig/$1"-[0-9]*2>/dev/null||true)"if[-f"$f"];thenecho"$f";return0;fif="$(ls"$toppath/pypi/$1"-[0-9]*)"g=pypi/orig/"$(basename"$f")"mv"$f""$g";echo"$g"}do_unpack(){localf;f="$(basename"$src")";case"$src"in*.tar.*)[-n"$workdir"]||workdir="${f%.tar.*}"tar-C"$buildpath"-xpf"$src";;*)exit1;;esac;workpath="$buildpath/$workdir"}do_prepare(){localf;forfin$patches;dopatch-p1<"$pybuild/$f";done;}do_compile(){python3./setup.pybdist_wheel;}do_clean(){mv"$workpath"/dist/*pypi;rm-rf"$workpath";}do_build(){do_unpack;(cd"$workpath";do_prepare;do_compile);do_clean;}pkg="$1";shift;set-x;."$pybuild"/"$pkg".sh;eval"$@"in the case of very serious errors, the splitting does not hurt reliability. By properly organising
conﬁguration ﬁles for diﬀerent instances of the same IOC executable (cf. Subsection 3.2), conﬂicts
between diﬀerent instances can be easily avoided, so we do not need to use mechanisms like Docker
to isolate diﬀerent IOC applications (Derbenev, 2019), which simpliﬁes deployment and saves both
memory and disk space. Another point worth mentioning is that the idea of reusable modular IOC
executables seems more natural and easier to understand for us than the require mechanism (Paul
Scherrer Institute, 2015), which loads support libraries on demand in .cmd ﬁles and requires much
more patching in the packaging procedure for every EPICS support module involved.

For reusable modular IOC executables to be provided, they must be built ﬁrst; for some EPICS
modules this is done by default, or is disabled but can be easily enabled. For other modules which
we want reusable executables of, we need to build them by ourselves; for various purposes, we
also add features like iocStats (IOC “health” monitoring) and autosave (automatic state sav-
ing and restore) to most IOC executables. We do these by patching the build system (usually
configure/RELEASE and xxxApp/src/Makefile in a module) before actually running it; the cen-
tralised management of packaging code (cf. Subsection 2.3) also helps to review patches and com-
pare the patches of modules that are similar in packaging, both of which prove to be very helpful
in quality assurance. The build systems of modules that belong to a common type (most impor-
tantly motors and area detectors) often only diﬀer in a few highly “templated” locations. This
similarity can sometimes be exploited to simplify the patching procedure above: since the patches
for these modules would also be templated, we may provide a template (Figure 6) and generate
the patches on the ﬂy for them based on this template. For this reason, we prefer to make the
build systems of internally developed motor and areaDetector modules similar to their counter-
parts in synApps, instead of using the more “standardised” (and much more bloated) build system
generated by traditional tools like makeBaseApp.pl; to generalise a little, we have the policy for
fundamentally similar code in diﬀerent places that if they cannot yet be abstracted, at least make
them templated. A situation where we further exploit the similarity between EPICS build systems
is building generic extra modules: we ﬁnd that although the build systems of these EPICS modules
(eg. Keithley 648x for Keithley 6485/7 picoammeters and s7nodave for Siemens PLCs) do not look
immediately similar, they can usually be replaced by a “standard” counterpart with only minimal
changes in some “template parameters”. For this reason, when packaging synApps, we provide
standard configure directory and Makefiles that would appear in xxxApp, iocBoot and iocxxx,
respectively at configure, app.mk, iocBoot.mk and ioc.mk in the /opt/epics/support/utils
directory, and have successfully used them to replace their module-speciﬁc counterparts.

Figure 6: A “template” used to batch-add support for iocStats and autosave to motor modules

9

Usage:eg.forthemotorAcsMotionmodule,thispatchcanbeappliedwith$cd/path/to/motorAcsMotion/iocs/acsMotionIOC/acsMotionApp$sed's/ims/acsMotion/g'/path/to/this.patch|patch-p1diff-urimsApp/src/MakefileimsApp/src/Makefile---imsApp/src/Makefile2019-08-1705:55:38.000000000+0800+++imsApp/src/Makefile2021-06-1820:19:33.128733413+0800@@-36,6+36,14@@#ifdefSNCSEQims_LIBS+=seqpv#endif+#ifdefAUTOSAVE+ims_DBD+=asSupport.dbd+ims_LIBS+=autosave+#endif+#ifdefDEVIOCSTATS+ims_DBD+=devIocStats.dbd+ims_LIBS+=devIocStats+#endif#ims_registerRecordDeviceDriver.cppderivesfromims.dbdims_SRCS+=ims_registerRecordDeviceDriver.cppWhen we talked above reusing IOC executables above, what we meant to reuse was not only
those that deal with speciﬁc devices, but also those that do not directly communicate with any
real device. They may control other devices indirectly by communicating with other applications
through the CA protocol, like the optics (monochromators and similar utilities) and sscan (inter-
locked action of motors and detectors, functionally like Bluesky) modules; they may also provide
PVs (“process variables” in EPICS) which other applications may read or write through the CA
protocol. The behaviours of some applications among them are completely determined by the .db
ﬁles supplied, to the point that their behaviour remain intact if they are instead fed to the intu-
itively named softIoc executable instead; this is how we implement these applications, so that we
do not even need to write .cmd ﬁles for them. However, given our requirement of status monitoring
mentioned above, we also need to add the iocStats feature to our softIoc executable (cf. Figure
9(a)), which results in a chicken-and-egg situation: synApps depends on EPICS base which pro-
vides softIoc, while softIoc needs to be added with support for iocStats which is from synApps. We
break this dependency loop by disabling the building of softIoc in EPICS base, lifting the former’s
source code from the latter, using the templates in /opt/epics/support/utils to form the build
system for a standalone softIoc, and ﬁnally building it with iocStats support added. Another de-
pendency loop is that the sscan IOC executable depends on the calc module, which in turn depends
on the sscan library; we solve it in a similar way – ﬁrst building libraries in the two modules only,
and then building the executables separately. We also note that in order to make references to
EPICS modules (including executables and libraries) stable across updates, we remove the version
tags from the directory names of all synApps modules (like the -R4-36 from asyn-R4-36, cf. also
Figure 9); this does not prevent the user from querying the version numbers, which can still be
found in /opt/epics/support/assemble_synApps.sh. And to make OPIs (“operator interfaces”,
GUIs for IOCs) easier to ﬁnd, we create symbolic links to all OPI ﬁles under /opt/epics/support
in /opt/epics/resources: links to .edl ﬁles for EDM in the edl subdirectory, links to .adl ﬁles
for MEDM in the adl subdirectory, etc. Search paths like ${EPICS_DISPLAY_PATH} are also set to
these resource subdirectories, so that users do not even need to type the directory names for OPI
ﬁles; in case of “missing” resource ﬁles (eg. kohzu.gif referenced by kohzuGraphic.adl from the
optics module, expected to be in the same directory), the user can also easily ﬁnd the real path of
OPI ﬁles by looking at where the symbolic links point to.

Interlocked actions between devices are a well-known strength of EPICS, and we provide full
compatibility with self-building of multi-device IOC applications: the libraries and ancillary ﬁles
are provided as usual, with reusable modular IOC executables just as an added bonus. However,
we also ﬁnd what is done by at least a big fraction of previous multi-device applications can also
be done by the composition of their single-device counterparts; the ﬁrst examples are optics and
sscan, which in our experiments work well whether the devices involved are controlled by the same
IOC application or not. From a theoretical perspective, a multi-device application can be split
into multiple single-device applications as long as the main modules involved (like sscan and the
motor/detector modules it is supposed to control) do not actually assume each other were in the
application, or in EPICS terms they do not mandate that they communicated through DB links
instead of CA links. For some applications, even if this is not true, the link relation between
the modules can be refactored to eliminate the demand for DB links; this certainly needs to be
analysed on a case-by-case basis, but here we give a relatively simple real-world example that is
nevertheless fundamentally similar to many other IOC applications in production, which is why
we believe many multi-device applications can actually be split up. For a certain application
scenario, we need to poll temperatures from a group of Cryo-con 18C monitor channels, and write
them to a group of memory addresses of Siemens PLCs; our previous and current solutions are
shown in Figure 7. For each pair of temperature channel and PLC address, we use a pair of EPICS
analogue-input and analogue-output PVs based on StreamDevice (cf. Subsection 3.2) and s7nodave
respectively. In the previous solution, the StreamDevice PV periodically updates (“scans”) from
the temperature channel, and after every update uses a “forward link” to trigger update of the
s7nodave PV, which pulls the temperature from the StreamDevice PV and writes it to the speciﬁed
memory address. According to the EPICS documentation (Kraimer et al., 2018), a CA forward
link only works as expected if it explicitly points to the .PROC “ﬁeld” of the destination PV (eg.
CryMon:18c1_AtoPLC.PROC), so we can eliminate the demand for DB links by batch-adding the
.PROC suﬃx to the forward links. For multiple reasons, we actually use a slightly more complex

10

solution, with “soft channel”-based analogue-out PVs (supported by EPICS base) as intermediates
for temperature values; the “soft channel” PVs are scanned instead of the StreamDevice PVs, and
the latter are only passively updated (using the “process passive” option) before the s7nodave PVs.

Figure 7: A requirement implemented with (a) multi-device and (b) single-device IOC applications;
for reliability reasons, the latter should be run on a same computer, and be started/stopped as a
group

3.2 Conﬁguration minimisation and automated deployment

An issue with introducing package management into EPICS is the management of conﬁguration
ﬁles. The ﬁrst problem is that if the user tunes the behaviours of IOC applications by directly
changing packaged ﬁles, these changes would get lost when the corresponding package is updated;
although package managers usually support the declaration of certain packaged ﬁles (like all ﬁles
under /etc) as “conﬁguration”, which helps to prevents the loss of conﬁguration changes, there
are often too many customisable ﬁles in EPICS to be batch-declared as conﬁguration. The second
problem is that since packaged ﬁles are typically installed as root-owned (cf. Subsection 2.2),
we often would need to use root privilege to change them, with implications in security and
even convenience: eg. with reusable IOC executables (cf. Subsection 3.1), the same executable
may be used by multiple users for diﬀerent applications, and one user may accidentally overwrite
conﬁguration ﬁles that actually belong to another user. Both problems can be easily solved if
the conﬁgurations ﬁles by each user are kept inside the user’s own home directory; this is a
common practice outside the EPICS ecosystem, and is in essence analogous to the separation of
packaging code and source archives (cf. Subsection 2.3) in that both distinguish between upstream
and downstream (upstream developers vs. packagers, packagers vs. users) in order to reduce the
amount of code each party needs to maintain. As has been mentioned just now and in Subsection
2.3, this also helps with software update and migration: a very nice side eﬀect of the separation
of conﬁgurations is that if somehow we ﬁnd a clean way to get rid of the packaging-unfriendly
synApps directory layout (cf. Subsection 2.2), users will be able to migrate to the new layout
relatively easily by batch-changing all path references in conﬁgurations ﬁles accordingly.

At our facility, we have formed what we call the ~/iocBoot convention: putting conﬁgurations
corresponding to an EPICS module (eg. motorIms for MDrive motors) into a suitable subdirectory
(eg. iocIms) of ${HOME}/iocBoot named after the subdirectory of iocBoot where the conﬁgura-
tion ﬁles are derived from. This is chosen because .cmd ﬁles (Figure 8) usually change to the
“${TOP}/iocBoot/${IOC}” directory before loading their default conﬁgurations, so we can eas-
ily adapt them by changing the references into “${HOME}/iocBoot/${IOC}” where appropriate.
For modules where the ${IOC} subdirectories are named too generally (eg. iocAny from optics),
the packager renames them into more distinguishable names (eg. iocOptics); for packages where
iocBoot is not provided or the contents are not very good examples, the packager provides self-
made examples that can be easily customised. The StreamDevice module is widely used in EPICS

11

ai,SCAN="1second"DTYP="stream"CryMon:18c1_Aao,SCAN="Passive"DTYP="s7nodave"CryMon:18c1_AtoPLCFLNKDOL(a)ai,SCAN="Passive"DTYP="stream"CryMon:18c1_Aao,SCAN="1second"DTYP="SoftChannel"CryMon:18c1_Ascanao,SCAN="Passive"DTYP="s7nodave"CryMon:18c1_AtoPLCDOLPPOUT(b)streamApps7nodaveAppfor talking to simple devices in a request/reply fashion, where developers mainly need to provide
.proto “protocol ﬁles” and corresponding .db ﬁles; we have a dedicated package that collects these
ﬁles, and groups them into directories like iocCounter and iocThermo according to the device type
(cf. Figure 9(b)). softIoc is a special case where the iocBoot directory is not strictly needed, but in
the name of uniformity all .db ﬁles for softIoc are put in ${HOME}/iocBoot/iocSoft. Furthermore,
all .cmd ﬁles are patched to store autosave state ﬁles in ${HOME}/iocBoot/autosave/${IOC}; nev-
ertheless, to prevent IOC executables from outputting warning messages over and over when the
speciﬁed directories do not yet exist, the saving and loading of state ﬁles are disabled by default. We
use the ${HOME}/iocBoot/service directory to store “run scripts” (Figure 9) that when executed
run corresponding applications in the foreground; utilities based on procServ are also provided to
start/stop speciﬁed application(s) in the background, and we are developing procServControl -based
mechanisms to provide GUIs for centralised status management. With all these elements combined,
we are able to implement multi-application setups, where the hundreds of IOC applications needed
for a beamline (excluding areaDetector and other applications that are resource-intensive, as well
as VME-based applications) can be accommodated on just a few computers with very modest
hardware. We ﬁnd this much easier to implement and maintain than alternatives, eg. (Konrad and
Bernal, 2019) which uses much more abstraction based on Puppet.

Figure 8: Default .cmd ﬁle for Cryo-con 18C temperature monitors

Figure 9: Run scripts in ${HOME}/iocBoot/service in a production environment for (a) a
LabView-based monochromator gatewayed to EPICS with CA Lab (run-mono.sh) and (b) some
Cryo-con 18C temperature monitors (run-temp18c.sh), with (c) example invocations of utilities
that start and then stop corresponding applications

System administration is closely related to the concept of “conﬁguration management”; to us,

12

#!../../bin/linux-x86_64/streamApp<envPathsepicsEnvSet("STREAM_PROTOCOL_PATH","${TOP}/iocBoot/${IOC}")cd"${TOP}"dbLoadDatabase"dbd/streamApp.dbd"streamApp_registerRecordDeviceDriverpdbbasedrvAsynIPPortConfigure("T18C","192.168.1.4:5000")#Inordertoloadacustomised`temp18c.substitutions',#thisoccurenceof`${TOP}'canbechangedto`${HOME}'.cd"${TOP}/iocBoot/${IOC}"dbLoadTemplate("temp18c.substitutions","P=xxx:18c1,PORT=T18C,SCAN=1second")iocInit(a)#!/bin/sh-ecd~/iocBoot/iocSoftexecsoftIoc-m'C=BL3W1:,M=mono:,SCAN=1second,IOC=BL3W1:Stats:mono:'\-dmono.db-d/opt/epics/support/iocStats/db/iocAdminCore.db(b)#!/bin/sh-ecd/opt/epics/support/StreamDevice/iocBoot/iocThermoexec../../bin/linux-x86_64/streamApp~/iocBoot/iocThermo/temp18c.cmd(c)$ioc-startmonotemp18cmono:startingprocServtemp18c:startingprocServ$ioc-stopmonotemp18cmono:killingprocServtemp18c:killingprocServconﬁguration is the entire procedure of composing hardware and software, whether oﬀ-the-shelf or
self-developed, into systems that can readily work in production. In the above, we have shown the
basic aspects of our eﬀorts to minimise the complexity in conﬁguring software on a single com-
puter for comfortable EPICS-based beamline control; in addition to these, we also further reduce
the complexity of said conﬁguration by simplifying conﬁguration ﬁles using various mechanisms,
whether based on EPICS (eg. .substitutions ﬁles) or not (mainly code generators, provided the
workload saved by them signiﬁcantly outweighs the workload to maintain them). To also sim-
plify hardware conﬁguration, we have written a software/hardware handbook to give simple yet
reproducible instructions that can be followed to conﬁgure beamline devices and corresponding
software into basic working states. On a larger scale, we are developing a backup system where
the actual conﬁguration of computers (whether running IOCs, OPIs or software like Bluesky) are
automatically synchronised to a central backup service, which in turn distributes the computer
conﬁgurations on a beamline to every computer on this beamline, so that the conﬁguration of a
computer can be replicated onto a backup computer in case of hardware error. Git-based version
control will be applied to the backups, so that we can revisit recent changes in case misconﬁguration
is suspected; in order to minimise downtime spent on replicating conﬁguration, automation mech-
anisms will be developed for the replication procedure, and backup computers will be preloaded
with basic software after procurement if applicable. In addition to regular computers, we also plan
to extend the backup system to other programmable hardware, like PLCs and network switches,
that accept automated and structured conﬁguration input, so that their conﬁguration procedures
can also be simpliﬁed.

From our deﬁnition of “conﬁguration” above, we can see that reducing the amount of conﬁgura-
tion for a single beamline device is reducing the amount of device-speciﬁc workload, and that reduc-
ing the amount of conﬁguration of an entire beamline is reducing the amount of beamline-speciﬁc
workload. Therefore in order to maximise the scalability of beamline control, we are developing
a group of comprehensive beamline services (CBS), which covers most aspects in beamline control
that are shared between beamlines. A documentation library service will be provided, which gives
users access to not only the software/hardware handbook (also including a recommended list of
hardware for various use on a beamline) and the operation manuals of individual beamline de-
vices, but also training materials (which would “conﬁgure” new users for various aspects of the
facility) and other useful documentation. Centralised network services will be provided, including
the backup system and the documentation library mentioned above, our package repositories (cf.
Subsection 2.3) and other services like an EPICS PV archiver/alarm service and CA gateways to
forward information from eg. the accelerator. For reliability reasons, our control network (where
control information is transmitted) will be isolated from the data-transfer network (where bulk
data produced by area detectors and data produced by software like Bluesky are transmitted).
Beamlines cannot directly communicate with each other, and instead can only communicate with
the CBS; transmission of outside information, like NTP information and PVs from the accelerator,
is done by gateway services in the CBS. A beamline development laboratory (BDL) will be pro-
vided, where beamline devices can be tested without interfering with the production environment;
the BDL is like a small beamline in terms of networking, which the CBS is formally a part of.

Another prominent use of the BDL is batch deployment, where basic software is installed on
new hardware before the latter is moved to the beamlines or stored as backup hardware. In the end
of this paper, we use the batch installation of operating systems onto new computers as an example
for how certain requirements in system administration can be implemented in surprisingly simple
ways, when we compose simple mechanisms and utilities according to careful analyses of the nature
of the problems. Regular installation of operation systems is often performed with CDs/DVDs or
USB ﬂash drives, which are exclusive media that cannot be accessed by multiple computers simul-
taneously; thus for batch deployment, network-based installation (now almost universally based on
PXE) is preferred, which is only limited by the network bandwidth of the installation environment
and perhaps disk performance of the installation server. In order to facilitate large-scale deploy-
ment, network installers of Linux distributions often support some kind of templated unsupervised
installation mechanism, like Kickstart of CentOS; many of them, including Kickstart, also support
some kind of mechanism that allows for post-installation execution of speciﬁed programs (“hooks”),
so we can automatically preload basic software during unsupervised installation. However, for some
requirements, we also need diﬀerentiation between computers, eg. installation of systems onto a

13

group of servers for EPICS PV archiving with automated conﬁguration of hostnames and related
settings. This can be done if we set up a service on the installation server that automatically
assigns tokens (which the hostnames will be based on) according to certain rules, and let the hook
program obtain the token from the service; using the socat utility and a little Shell scripting, this
can be done in just a few lines of code (Figure 10). The na¨ıve implementation of the service suﬀers
from an obvious race condition when multiple clients ask for tokens at the same time; this can be
solved if we execute the service script in a critical section, which can be easily implemented by the
ﬂock utility.

Figure 10: Implementation of a token service

4 Conclusion

At HEPS, we currently use RPM for packaging beamline control software, and deliberately
use EPICS 7 like EPICS 3, with most modules based on synApps. We preserve the synApps
directory layout in our packaging system, which however results in potential ﬁle conﬂicts and
permission issues; we solve these problems using a builder user with password-less sudo permission
in Docker containers. The packaging system, including the packaging code and the Docker wrapper,
is managed centrally and properly abstracted to minimise complexity. Both self-built packages
and some very useful third-party packages are provided in our internal RPM repository, and we
also provide a similar Python package repository for internal use; all external inputs involved in
the creation of both repositories are checked against a chain of trust to avoid tampering. We
use reusable modular EPICS IOC executables, built with support for iocStats and autosave, to
minimise the need for self-built multi-device applications, and facilitate the use of these executables
by providing easy access to resources like OPIs and example conﬁguration ﬁles. Full compatibility
with multi-device applications is kept, and we also ﬁnd at least a big fraction of them can be
replaced by the composition of their single-device counterparts. We have formed the ~/iocBoot
convention to separate each user’s IOC conﬁguration ﬁles from the default conﬁgurations provided
by RPM packages, and provide utilities that help to implement maintainable multi-IOC setups for
beamlines. Rigorous eﬀorts are being made under the umbrella project of comprehensive beamline
services to further simplify conﬁguration management on multiple scales: beamline devices and
related software on individual computers, all computers and other programmable hardware on an
entire beamline, as well as all beamlines at HEPS.

14

(a)Thepost-installationhookprogramthatrunsoneachcomputerundergoingbatchdeploymentisashellscriptthatincludesthefollowinglineofcommand,where10.0.2.2istheIPaddressoftheinstallationserver,get_macprintstheMACaddressofthecurrenthost,anddo_configconfiguresthecurrenthostaccordingtothetokeninitsinput:get_mac|socattcp:10.0.2.2:9999-,ignoreeof|do_config(b)Attheheartofthetokenserviceisthefollowingshellscript,supposedlyas./token.sh,whichassignsauto-incrementingnumerictokens,butisnot‘‘thread-safe’’:#!/bin/sh-[-f/tmp/token.txt]&&token="$(cat/tmp/token.txt)"||token=0timeout5sed"s/^/$token\\t/;q">&2||exit1echo"$token";expr"$token"+1>/tmp/token.txt(c)Thetokenservice,whichsavesthemappingtablefromtokenstoMACaddressestothefiletoken.log,implementednaïvely:$socattcp-listen:9999,reuseaddr,forkexec:./token.sh2>token.logThecorrectlyimplementedtokenservice,withtoken.shruninacriticalsection:$socattcp-listen:9999,reuseaddr,fork\exec:'flock/tmp/token.lock./token.sh'2>token.logAcknowledgements

We would like to thank Yu-Jun Zhang and Wei Xu for the ﬁrst adoption of the ~/iocBoot

convention and their continuous feedback.

References

Allan, D., Caswell, T., Campbell, S. and Rakitin, M., 2019. Bluesky’s ahead: A multi-facility
collaboration for an a la carte software project for data acquisition and management. Synchrotron
radiat. news, 32(3), pp.19–22.

Bertrand, B. and Harrisson, A., 2019. Building and packaging epics modules with conda. Proceed-
ings of the 17th international conference on accelerators and large experimental physics control
systems (icalepcs2019). New York, NY, USA, MOPHA014, pp.223–227.

Bowen, R., 2020. Centos-announce: Centos project shifts focus to centos stream. Available from:
https://lists.centos.org/pipermail/centos-announce/2020-December/048208.html.

Brookhaven National Laboratory, 2018. Nsls-ii controls package repository. Available from: https:

//epicsdeb.bnl.gov/debian/.

Chu, P., Jin, D.P., Lei, G., Wang, C.H. and Zhu, L.X., 2018. Heps controls status update.
Proceedings of the 12th international workshop on emerging technologies and scientiﬁc facilities
controls (pcapac2018). Hsinchu, Taiwan, China, WEC4, pp.14–16.

Derbenev, A.A., 2019. Improvement of epics software deployment at nsls-ii. Proceedings of the
17th international conference on accelerators and large experimental physics control systems
(icalepcs2019). New York, NY, USA, TUDPP03, pp.847–851.

Franksen, B., 2019. Building epics support modules with sumo. June 2019 epics collabora-
tion meeting. Cadarache, France. Available from: https://indico.cern.ch/event/766611/
contributions/3438294/.

Jiao, Y., Xu, G., Cui, X.H., Duan, Z., Guo, Y.Y., He, P., Ji, D.H., Li, J.Y., Li, X.Y., Meng, C.,
Peng, Y.M., Tian, S.K., Wang, J.Q., Wang, N., Wei, Y.Y., Xu, H.S., Yan, F., Yu, C.H., Zhao,
Y.L. and Qin, Q., 2018. The heps project. J. synchrotron rad., 25(6), pp.1611–1618.

Konrad, M. and Bernal, E., 2019. Continuous delivery and deployment of epics iocs at frib. June
2019 epics collaboration meeting. Cadarache, France. Available from: https://indico.cern.
ch/event/766611/contributions/3438444/.

Kraimer, M.R., Anderson, J.B., Johnson, A.N., Norum, W.E., Hill, J.O., Lange, R., Franksen, B.,
Denison, P. and Davidsaver, M., 2018. Epics application developer’s guide (base release 3.15.6).
pp.85–94. Available from: https://epics.anl.gov/base/R3-15/6-docs/AppDevGuide.pdf.

Lange, R., 2021. A future for epics linux packaging? Spring 2021 epics collaboration meeting.
Online. Available from: https://indico.lightsource.ca/event/2/contributions/27/.

Mooney, T.M., 2010. synapps: Epics application software for synchrotron beamlines and laborato-
ries. Proceedings of the 8th international workshop on personal computers and particle accelerator
controls (pcapac2010). Saskatoon, SK, Canada, THCOMA02, pp.106–108.

Paul Scherrer Institute, 2015. require: Psi dynamic modules for epics. Available from: https:

//github.com/paulscherrerinstitute/require.

15

