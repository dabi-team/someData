Investigating the eﬃciency of marginalising over discrete parameters in
Bayesian computations

Wen Zhang1, Jeﬀrey Pullin1,2, Lyle Gurrin3, Damjan Vukcevic1,2,*

1School of Mathematics and Statistics, University of Melbourne, Australia
2Melbourne Integrative Genomics, University of Melbourne, Australia
3Melbourne School of Population and Global Health, University of Melbourne, Australia
*Corresponding author: Damjan Vukcevic, damjan.vukcevic@unimelb.edu.au

13 September 2022

Abstract

Bayesian analysis methods often use some form of iterative simulation such as Monte Carlo computa-
tion. Models that involve discrete variables can sometime pose a challenge, either because the methods
used do not support such variables (e.g. Hamiltonian Monte Carlo) or because the presence of such vari-
ables can slow down the computation. A common workaround is to marginalise the discrete variables
out of the model. While it is reasonable to expect that such marginalisation would also lead to more
time-eﬃcient computations, to our knowledge this has not been demonstrated beyond a few specialised
models.

We explored the impact of marginalisation on the computational eﬃciency for a few simple sta-
tistical models. Speciﬁcally, we considered two- and three-component Gaussian mixture models, and
also the Dawid–Skene model for categorical ratings. We explored each with two software implementa-
tions of Markov chain Monte Carlo techniques: JAGS and Stan. For JAGS, it was possible to compare
marginalised and non-marginalised versions of the same model with the same samplers.

Our results show that marginalisation on its own does not necessarily boost performance. Nevertheless,
the best performance was usually achieved with Stan, which requires marginalisation. We conclude that
there is no simple answer to whether or not marginalisation is helpful. It is not necessarily the case that,
when turned ‘on’, this technique can be assured to provide computational beneﬁt independent of other
factors, nor is it likely to be the model component that has the largest impact on computational eﬃciency.

2
2
0
2

p
e
S
3
1

]
E
M

.
t
a
t
s
[

2
v
3
1
3
6
0
.
4
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Contents

1 Introduction

2 Methods

2.1.1 Gaussian mixture models
2.1.2 Dawid–Skene model

2.1 Models and marginalisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Data simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Software implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 Gaussian mixture models with JAGS . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2 Dawid–Skene model with JAGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Evaluating computational eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1 Two-component Gaussian mixture model
2.2.2 Three-component Gaussian mixture model
2.2.3 Dawid–Skene model

2

3
3
3
4
5
5
6
6
6
7
7
7

3 Results

8
8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1 Gaussian mixture models
3.2 Dawid–Skene model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3 Main conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

4 Discussion

References

1

Introduction

14

16

In a Bayesian framework, performing inference often involves the use of Markov chain Monte Carlo (MCMC)
methods. Recently, MCMC methods based on Hamiltonian Monte Carlo (HMC), such as the ‘No U-Turn
Sampler’ (NUTS) algorithm (Hoﬀman and Gelman, 2014) implemented in the probabilistic programming
language Stan (Carpenter et al., 2017), have become popular, replacing methods based on Gibbs sampling
in many domains. HMC-based methods require gradients of the likelihood function to be computed with
respect to all parameters in the model. However, these gradients cannot be computed for discrete parameters,
potentially limiting the use of HMC-based methods. One common approach to overcome this drawback is to
marginalise the discrete parameters out of the model. HMC-based methods can then used to draw samples
from the joint posterior distribution of the marginalised model. Today, marginalisation is the only way to
ﬁt models with discrete parameters in Stan. The widespread use of both Stan and models with discrete
parameters has therefore made marginalisation a widely used technique in applied Bayesian modelling.

Recently, some of the authors of the current manuscript presented the R package rater, which implements
Bayesian versions of several statistical models that allow analysis of repeated categorical rating data (Pullin
et al., 2021). The models are based on, and include, the Dawid–Skene (DS) model (Dawid and Skene, 1979),
and as such they include a discrete latent class parameter for each item. The rater package uses Stan
to perform inference, and marginalised versions of the models are employed. The manuscript describing
rater discusses in some detail the techniques of marginalisation and conditioning in an eﬀort to explain
the mathematical basis of the technique. In addition, other authors such as Joseph (2020) have sought to
explain how to marginalise out discrete parameters in speciﬁc models.

While marginalisation requires substantial mathematical eﬀort, folk wisdom in the Stan community suggests
that ﬁtting models with marginalisation is more eﬃcient than using Gibbs sampling.
Indeed, the Stan
User’s Guide asserts that marginalisation can allow “more eﬃcient sampling on an iteration-by-iteration
basis” (Stan Development Team, 2022). To date, however, there has been little empirical evidence for the
idea that marginalisation can improve computational eﬃciency. Recently, Yackulic et al. (2020) considered
marginalising out latent states in a variety of Bayesian population models such as the Cormack–Jolly–Seber

2

(CJS) model. In particular, they demonstrated that marginalisation can improve computational performance
by orders of magnitude in a CJS model. In their comparison, the fastest model—implemented using Stan—
was more than one thousand times more eﬃcient than the slowest non-marginalised model. These results
highlight the potential of marginalisation, and Stan, to greatly increase the eﬃciency of Bayesian inference
for models with discrete parameters. It is not clear however, whether the results presented by Yackulic et al.
(2020) generalise to other models that contain discrete parameters.

Whether or not marginalisation makes computation more eﬃcient is of both practical and theoretical in-
terest. Practically, the eﬃciency of marginalisation would guide both whether marginalisation should be
considered as a possible way to speed up generic Bayesian computations, and decisions about which software
(i.e. JAGS vs Stan) and options within these software to use. Theoretically, it is of interest to determine
whether the claimed improvements in eﬃciency suggested by the Rao–Blackwell Theorem actually hold
in practice. Finally, this work could guide whether future probabilistic programming languages (such as
SilcStan (Gorinova et al., 2019)) should implement automatic marginalisation for some, or even all, mod-
els.

In this paper, we explore the impact of marginalisation on computational eﬃciency for other models that
contain discrete parameters: two- and three-component Gaussian mixture models and the Dawid–Skene
model for categorical ratings. We implement various marginalised and non-marginalised version of the
models in JAGS and marginalised versions of the models in Stan. We then compare the computational
eﬃciency of the diﬀerent inference approaches across various simulation scenarios.

2 Methods

2.1 Models and marginalisation

Here we describe the Gaussian mixture models and the Dawid–Skene model that we used for the generation
and analysis of simulated data, including the prior distributions we chose for Bayesian inference. We
include two versions of the likelihood functions: (i) ‘full’ versions, that include the discrete variables; and
(ii) marginalised versions, where the discrete variables are marginalised out.

2.1.1 Gaussian mixture models

A Gaussian mixture model assumes that each observation is generated from one of a ﬁnite set of Gaussian
distributions, referred to as the mixture components. The model uses a discrete latent variable as a category
label to indicate the mixture component from which each observation was generated.

Suppose that we have n continuously valued observations, x1, . . . , xn. We assume that each observation xi
belongs to one of K mixture components. Associated with each xi is a discrete latent variable zi ∈ {1, . . . , K}
that indicates the mixture component from which xi was drawn. We assume that z1, . . . , zn are independent
and identically distributed as follows. For i = 1, . . . , n,

zi ∼ Categorical(π1, . . . , πK),

where πk ∈ [0, 1] for k = 1, . . . , K and (cid:80)K
population of continuously valued observations (the xi’s) belonging to the kth mixture component.

k=1 πk = 1. The parameter πk speciﬁes the proportion of the

We assume that x1, . . . , xn conditional on zi, . . . , zn are independent and identically distributed as follows.
For i = 1, . . . , n and k = 1, . . . , K,

xi | zi = k ∼ N(µk, σ2).
The variance, σ2, is assumed to be the same for all mixture components. For k ∈ {1, . . . , K}, µk and σ2 are
unknown parameters.

For convenience, we combine the above quantities into vector formats: let x = (x1, . . . , xn); z = (z1, . . . , zn);
π = (π1, . . . , πK) and µ = (µ1, . . . , µK).

3

The full likelihood function (that includes z) for this model is

Pr(x, z | π, µ, σ2) =

n
(cid:89)

i=1

f (xi | µzi, σ2) · πzi,

where f (xi | µzi, σ2) is the probability density function of a Gaussian distribution with mean µzi and variance
σ2.

The marginalised likelihood function, where we marginalise over z, is

(cid:88)

z
K
(cid:88)

Pr(x, z | π, µ, σ2)

K
(cid:88)

· · ·

K
(cid:88)

Pr(x, z | π, µ, σ2)

zn=1
K
(cid:88)

n
(cid:89)

zn=1

i=1

· · ·

f (xi | µzi, σ2) · πzi

Pr(x | π, µ, σ2) =

=

=

=

=

z1=1
K
(cid:88)

z2=1
K
(cid:88)

z2=1

z1=1
(cid:32) K
(cid:88)

z1=1
K
n
(cid:88)
(cid:89)

i=1

zi=1

f (xi | µzi, σ2) · πzi.

f (x1 | µz1, σ2) · πz1

(cid:33) (cid:32) K
(cid:88)

z2=1

f (x2 | µz2, σ2) · πz2

. . .

(cid:33)

(cid:32) K
(cid:88)

zn=1

f (xn | µzn, σ2) · πzn

(cid:33)

(1)

We place the following prior distributions on the parameters:

π ∼ Dirichlet(α),

σ ∼ Lognormal(0, 1),
µ1 ∼ N(0, 102),
µk ∼ N(0, 102, µk−1),

for k = 2, . . . , K,

where α is a vector of 1’s with length K, and N(0, 102, µk−1) is the truncated normal distribution where we
use the truncation µk > µk−1 to avoid label switching. We chose 102 for the variance of the prior distribution
after trying a range of values and selecting one that improved convergence.

2.1.2 Dawid–Skene model

The Dawid–Skene model for categorical ratings is applicable to scenarios where a set of items is classiﬁed
into categories by one or more raters. Speciﬁcally, we have I items and J raters. We assume that each item
belongs to one of K categories. Associated with each item is a (discrete) latent parameter zi ∈ {1, . . . , K}
that indicates the true category for item i. Associated with each item–rater pair is a rating yi,j for item
i given by rater j1. Let y be the vector of all ratings. We assume that zi, . . . , zI are independent and
identically distributed as follows. For i = 1, . . . , I,

zi ∼ Categorical(π1, . . . , πK),

where πk ∈ [0, 1] for k = 1, . . . , K and (cid:80)K
the population from which the items are sampled. We let π = (π1, . . . , πK) and z = (z1, . . . , zI ).

k=1 πk = 1. One can consider πk as the prevalence of category k in

For all possible pairs of i, j where i ∈ {1, . . . , I}, j ∈ {1, . . . , J}, we assume that yi,j conditional on zi are
independent and identically distributed as follows

1For simplicity, we assume that each item is rated exactly once by each rater. The model naturally generalises to arbitrary

numbers of ratings and more complex experimental designs, see Pullin et al. (2021).

yi,j | zi ∼ Categorical(θj,zi,1, . . . , θj,zi,K)

4

where θj,zi,k ∈ [0, 1] for k = 1, . . . , K and (cid:80)K
item of true category zi as being in category k. We let θ be the vector of all possible θj,zi,k’s.

k=1 θj,zi,k = 1. θj,zi,k is the probability that rater j rates an

For this model, the full likelihood function that includes z is

Pr(y, z | θ, π) =



πzi ·

I
(cid:89)

i=1

J
(cid:89)

j=1



θj,zi,yi,j

 .

The likelihood function where we marginalise over z is

Pr(y | θ, π) =

I
(cid:89)





K
(cid:88)

i=1

k=1



πk ·

J
(cid:89)

j=1





θj,k,yi,j



 .

We place weakly informative prior probability distributions on the parameters:

π ∼ Dirichlet(α),
θj,k ∼ Dirichlet(βk),

where α is a vector of length K with positive elements, θj,k is the vector (θj,k,1, . . . , θj,k,K), β is a K ×K ma-
trix with positive elements, and βk is the kth row of this matrix. Speciﬁcally, β contains the elements:

βk,k(cid:48) =

(cid:40)

N p
N (1−p)
K−1

if k = k(cid:48)
otherwise

∀k, k(cid:48) ∈ 1, . . . , K.

For the hyper-parameters α, N and p, we used the default values from the R package rater (Pullin et al.,
2021): α is a vector of 3’s, N = 8 and p = 0.6. See Pullin et al. (2021) for a description of how to interpret
these parameters.

2.2 Data simulations

Here we describe how we simulated each test dataset. For each model and experimental scenario, we
simulated 5 replicate datasets.

2.2.1 Two-component Gaussian mixture model

We simulated observations from the following probability density function:

g(xi) = π1 · f (xi | µ1, 22) + π2 · f (xi | µ2, 22),

where f (xi | µ, σ2) is the probability density function of a Gaussian distribution with mean µ, variance
σ2 and mixture proportions (π1, π2). We deﬁne the distance between components by |µ2 − µ1|, and also
µ = (µ1, µ2) and π = (π1, π2).

For this model, we were interested in how the mixture proportions and distance between components mod-
iﬁed the impact of marginalisation. We simulated four datasets, each of size 200. We set the values of µ1
and µ2 as in the second column of Table 1.

We paired the two options for the distances between components with three options for the mixture pro-
portions: one with equal proportions, one with moderately imbalanced proportions and one with very
imbalanced proportions. See the third and fourth columns of Table 1. The mixture proportion for Dataset 2
was set to be highly imbalanced. This allows us to investigate how discrete sampling for small probabilities
impacts marginalisation. Plots of the density function for each dataset can be found in the ﬁrst rows of
Figure 1 and Figure 2.

5

Table 1: Data simulation parameters for the two-component Gaussian mixture model.

Dataset

µ

π

|µ2 − µ1|

Dataset 1
Dataset 2
Dataset 3
Dataset 4

(−5, 5)
(−5, 5)
(−2.5, 2.5)
(−2.5, 2.5)

(0.5, 0.5)
(0.9, 0.1)
(0.5, 0.5)
(0.7, 0.3)

10
10
5
5

Table 2: Data simulation parameters for the three-component Gaussian mixture model.

Dataset

Dataset 1
Dataset 2
Dataset 3
Dataset 4
Dataset 5
Dataset 6
Dataset 7
Dataset 8

µ

π

|µ3 − µ1| Equidistant components

(−10.5, 0, 10.5)
(−10.5, 0, 10.5)
(−7, 0, 7)
(−7, 0, 7)
(−6, 0, 15)
(−6, 0, 15)
(−4, 0, 10)
(−4, 0, 10)

(0.33, 0.33, 0.33)
(0.5, 0.3, 0.2)
(0.33, 0.33, 0.33)
(0.5, 0.3, 0.2)
(0.5, 0.3, 0.2)
(0.33, 0.33, 0.33)
(0.33, 0.33, 0.33)
(0.5, 0.3, 0.2)

21
21
14
14
21
21
14
14

Yes
Yes
Yes
Yes
No
No
No
No

2.2.2 Three-component Gaussian mixture model

Similar to the above, we simulated observations from the following the probability density distribution:

g(xi) = π1 · f (xi | µ1, 22) + π2 · f (xi | µ2, 22) + π3 · f (xi | µ3, 22).

We assume µ1 < µ2 < µ3 and deﬁne the maximal distance between components by |µ3 − µ1|. We consider
two scenarios: (i) where the components are equidistant, µ3−µ2
= 1; and (ii) where the components are
µ2−µ1
non-equidistant, speciﬁcally µ3−µ2
µ2−µ1

= 2
5 .

For the three-component Gaussian mixture model, we index the simulations by both the maximal distance
between components and whether the three components are equidistant or not.

We simulated eight datasets, each of size 200. We set the value of µ as in the second column of Table 2.
The combinations of diﬀerent mixture proportions, maximal distances between components and whether
the components are equidistant or not gives us a 2 × 2 × 2 balanced design, as shown in Table 2.

Plots of the density function for each dataset can be found in the ﬁrst rows of Figure 3 and Figure 4.

2.2.3 Dawid–Skene model

We simulated a dataset with 5 raters and 100 items, where each items belongs to one of the 5 categories
(J = 5, I = 100, K = 5). We set the prevalence of each category to be uniform, i.e. π = ( 1

5 , 1

5 , 1

5 , 1

5 , 1

5 ).

For j ∈ {1, . . . , 5} and k ∈ {1, . . . , 5}, we let θj,k,k = 0.7 and θj,k,l = 0.075 for all l (cid:54)= k. This means all
raters classify the items correctly 70% of the time, and otherwise make errors uniformly amongst the other
categories.

2.3 Software implementations

We explored the computational eﬃciency for each model–data pair using both JAGS and Stan. Speciﬁcally,
we used the main R interfaces to each one: rstan (Stan Development Team, 2020) and rjags (Plummer,
2019). All of our models were coded ‘by hand’. We used the R package posterior (B¨urkner et al., 2022)
for summarising output from both JAGS and Stan.

6

JAGS supports direct sampling of discrete parameters. In contrast, Stan cannot be used when the posterior
distribution explicitly contains discrete parameters, so marginalisation is required. Using JAGS, we directly
compared marginalised and full versions of the same model (which we refer to as jags-full and jags-marg), and
also compared the performance of the JAGS models with the (necessarily marginalised) Stan model (which
we refer to as stan). All implemented likelihood functions are described mathematically in Section 2.1.

Source code for reproducing all of the results in this manuscript is available on GitHub2.

2.3.1 Gaussian mixture models with JAGS

For the two- and three-component Gaussian mixture models, we have two versions of each of the full and
marginalised models in JAGS, as follows.

Marginalised models:

1. The jags-marg-inbuilt model is marginalised with the dnormmix distribution from the mix module.

2. The jags-marg model is marginalised manually as in Equation 1.

Full models:

1. The jags-full model is unmarginalised and allowed to use any samplers from the base, bugs and mix

modules in JAGS.

2. The jags-full-restricted model is unmarginalised but only allowed to use the following samplers:

• base::Slice;

• bugs::Dirichlet.

We selected this restricted set of samplers to try and force the jags-full-restricted model to use the same set of
samplers as the jags-marg-inbuilt and jags-marg models. We did this to assess the impact of marginalisation
itself, rather than also the use of diﬀerent samplers (which JAGS chooses automatically, based on the model
it is using). The set of samplers was selected by turning all samplers ‘oﬀ’ except for a candidate set and
looking for error messages when running the jags-marg-inbuilt and jags-marg models.

2.3.2 Dawid–Skene model with JAGS

We have the jags-marg model which is marginalised manually and the unmarginalised jags-full model which
is allowed to use any samplers from the base and bugs modules in JAGS. The jags-full-restricted model is
unmarginalised but only allowed to use the same set of samplers as for the jags-full-restricted for Gaussian
mixture models. We selected this restricted set of samplers by going through the same procedure for the
Dawid–Skene model as we did for the Gaussian mixture model (see above).

2.4 Evaluating computational eﬃciency

We evaluated computational eﬃciency by looking at four quantities of interest:

1. Computation time

2. Minimum eﬀective sample size for the continuous parameters

3. Time per minimum eﬀective sample

(Computation time divided by Minimum eﬀective sample size of continuous parameters)

4. The ˆR statistic (Gelman and Rubin, 1992)

The way we measure these quantities is speciﬁed in Table 3.

2https://github.com/katezhangwen/Efficiency-of-marginalising-over-discrete-latent-parameters

7

Table 3: Methods used for measuring the quantities to assess computational eﬃciency (see Section 2.4).

Quantity

JAGS

Stan

Computation time

Using the system.time() function to
measure the time elapsed running
jags.model() and coda.samples().
The quantity is the sum of the two com-
putation times.

Using the system.time() function
to measure the time elapsed running
stan().

sample

eﬀec-
size
continuous

Minimum
tive
for
the
parameters

(For both JAGS and Stan) Looking at ess bulk and ess tail for all con-
tinuous parameters of posterior’s draws object and taking the minimum.
The numbers were similar, so we only present the ess bulk measure in the
result.

Time per minimum
eﬀective sample

(For both JAGS and Stan) Computation time divided by Minimum eﬀective
sample size of continuous parameters.

ˆR

(For both JAGS and Stan) Looking at rhat for all continuous parameters
of posterior’s draws object and taking the maximum.

We ran the models on each of the 5 replicate datasets to obtain 5 observations of each of the quantities
above for each model–data pair. The analysis was done with 3 chains, a total of 3000 iterations, with 1500
iterations of warm up.

3 Results

3.1 Gaussian mixture models

We used two plots to summarise results for two- and three-component Gaussian mixture models respectively.
Figure 1 and Figure 3 shows results on the natural scale, while Figure 2 and Figure 4 shows results on a
logarithmic scale (base 10) with common limits on the vertical axes. Figure 1 and Figure 3 highlight
diﬀerences between models with the same dataset; Figure 2 and Figure 4 demonstrate the diﬀerences across
diﬀerent datasets.

The time per minimum eﬀective sample measures the overall eﬃciency of each method. It is clear from
Figure 1 and Figure 3 that stan usually outperformed all other models. Of the unrestricted JAGS models
(i.e. all except jags-full-restricted ), jags-full tended to outperform slightly when the (maximal) distance
between components was large. When the (maximal) distance between components was small, the diﬀerence
between jags-full, jags-marg-inbuilt and jags-marg was minor. The restricted model, jags-full-restricted, was
consistently the worst performer across all scenarios.

Considering computation time and minimum eﬀective sample size separately, although stan’s computation
time was often higher than (in the two-component case) or comparable (in the three-component case) to
those of the JAGS models, it had the highest minimum eﬀective sample size in all cases, typically allowing
it to be the most eﬃcient overall. When the (maximal) distance between components was small, jags-
marg-inbuilt and jags-marg, jags-full were signiﬁcantly faster than stan when it comes to computation time
(but they typically also had smaller eﬀective sample sizes, thus did not typically outperform stan in overall
eﬃciency).
The realised values of ˆR demonstrate that stan converges the most consistently across all scenarios, whereas
the JAGS models tend to perform inconsistently on many of the datasets.

The results in Figure 2 and Figure 4 suggest that is it ‘easier’ (in the sense that the time per minimum eﬀec-
tive sample is lower) to draw posterior samples when using Data 1 and Data 2 for both the two- and three-

8

component Gaussian mixture models than for the other data structures. What distinguishes these datasets
from the others is that their mixture components are distinct and well-separated (they have the largest
distance between components, and in the three-component case the components are equidistant).

A highly imbalanced mixture proportion didn’t have any substantial impact on the relative performance of
the models. This is evident from comparing Data 1 and Data 2 in Figure 2. Although Data 2 was ‘harder’
(in the sense that the time per minimum eﬀective sample is higher) for all models, the relative diﬀerence
between models, in time per minimum eﬀective sample, is similar between the two datasets.

9

Figure 1: Time per minimum eﬀective sample, computation time, minimum eﬀective sample size and ˆR
when performing Bayesian inference of two-component Gaussian mixture models. The analysis is done with
3 chains, a total of 3000 iterations with 1500 iterations of warm up. Each box in the boxplot represents 5
results. The 5 results are obtained from 5 trials of analysis for each model–data pair. For each trial, we
simulate one replicate of the dataset and run analysis for stan, jags-full, jags-marg, jags-marg-inbuilt and
jags-full-restricted respectively. The horizontal line in the ˆR plots is the conventional 1.1 threshold.

10

DensityData 1DensityData 2DensityData 3DensityData 4−20−1001020−20−1001020−20−1001020−20−10010200.0000.0500.1000.1500.0000.0500.1000.1500.0000.0500.1000.1500.0000.0500.1000.150xdensityMin Effective Sample SizeData 1Min Effective Sample SizeData 3Min Effective Sample SizeData 2Min Effective Sample SizeData 4Computation TimeData 1Computation TimeData 3Computation TimeData 2Computation TimeData 4Time per min Effective SampleData 1Time per min Effective SampleData 3Time per min Effective SampleData 2Time per min Effective SampleData 40126912151802505007500.00.51.01.5101503006009000129121518050010001500200025000.0040.0080.012789101112131000200030004000valueRhatData 1RhatData 3RhatData 2RhatData 4stanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restricted1.01.21.41.61.81.01.11.21.31.01.21.41.61.0001.0251.0501.0751.100modelvalueFigure 2: Same as Figure 1 but with results in each row shown on a common log10 scale to more easily
compare across diﬀerent datasets.

11

Data 1Data 2Data 3Data 4Density−20−1001020−20−1001020−20−1001020−20−10010200.000.050.100.15xdensityTime per min Effective SampleComputation TimeMin Effective Sample Size10−2.510−210−1.510−110−0.5100100.5100.8100.9101101.1101.2101101.5102102.5103103.5Rhatstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restricted1.0001.2001.4001.6001.800modelvalue1
2

Figure 3: Same as Figure 1 but now showing the results for the three-component Gaussian mixture model.

DensityData 1DensityData 2DensityData 3DensityData 4DensityData 5DensityData 6DensityData 7DensityData 8−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−10010200.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.0900.0000.0300.0600.090xdensityMin Effective Sample SizeData 1Min Effective Sample SizeData 2Min Effective Sample SizeData 3Min Effective Sample SizeData 4Min Effective Sample SizeData 5Min Effective Sample SizeData 6Min Effective Sample SizeData 7Min Effective Sample SizeData 8Computation TimeData 1Computation TimeData 2Computation TimeData 3Computation TimeData 4Computation TimeData 5Computation TimeData 6Computation TimeData 7Computation TimeData 8Time per min Effective SampleData 1Time per min Effective SampleData 2Time per min Effective SampleData 3Time per min Effective SampleData 4Time per min Effective SampleData 5Time per min Effective SampleData 6Time per min Effective SampleData 7Time per min Effective SampleData 80123152025303501002003004005000121520250250500750012342030050010001500200001231520253001002003004000.00.51.01.52.02.5121620240200400600800012310121416050010000.0050.0100.0150.02010.012.515.010002000300040000.0050.0100.0150.02010152025301000200030004000valueRhatData 1RhatData 2RhatData 3RhatData 4RhatData 5RhatData 6RhatData 7RhatData 8stanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restricted1.001.251.501.751.01.11.21.31.41.21.51.82.11.01.21.41.61.81.01.11.21.31.01.21.41.61.0001.0251.0501.0751.1001.0001.0251.0501.0751.100modelvalue1
3

Figure 4: Same as Figure 3 but with results in each row shown on a common log10 scale to more easily compare across diﬀerent datasets.

Data 1Data 2Data 3Data 4Data 5Data 6Data 7Data 8Density−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−1001020−20−10010200.0000.0250.0500.0750.100xdensityTime per min Effective SampleComputation TimeMin Effective Sample Size10−210−1100101101.1101.2101.3101.4101.5100.5101101.5102102.5103103.5valueRhatstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restrictedstanjags−fulljags−marg−inbuiltjags−margjags−full−restricted1.201.501.802.10modelvalue3.2 Dawid–Skene model

Unlike the Gaussian mixture models, jags-full slightly outperformed stan as seen in the ﬁrst row of Figure 5.
The stan model had both a longer computation time and a larger minimum eﬀective sample size compared
to jags-full. However, the latter was so fast that its time per minimum eﬀective sample size was smaller.
The ˆR values demonstrate that stan and jags-full converged the most consistently out of all models. The
marginalised model, jags-marg, takes signiﬁcantly longer computation time than all other models which
makes it the worst performer when looking at time per minimum eﬀective sample.

3.3 Main conclusions

Comparing the JAGS Gaussian mixture models, it is clear that marginalisation on its own does not neces-
sarily boosts computational eﬃciency. The full model, jags-full, outperforms the two marginalised models
(jags-marg-inbuilt and jags-marg) in almost all cases in terms of overall eﬃciency.

Furthermore, the software implementations and choice of samplers had a much greater impact on compu-
tational eﬃciency. We can see this in two ways. First, by comparing the performance of jags-full and
jags-full-restricted. The jags-full-restricted model consistently performed worse, and jags-full was the most
eﬃcient out of all JAGS models. This suggests that the boost from marginalisation is insuﬃcient to overtake
the boost from using a more eﬃcient sampler. Second, we can compare the performance of stan against
that of the marginalised JAGS models. The stan model (which is marginalised) was more eﬃcient than the
JAGS models, highlighting the diﬀerence due to choice of sampler and software implementation.

The results for Dawid–Skene suggest that marginalisation does not improve computational eﬃciency at
all. Through comparing the performance of jags-marg and jags-full-restricted, the two models which we
believe are using the same set of samplers, we see that the unmarginalised jags-full-restricted model per-
forms better than the jags-marg model in terms of time per minimum eﬀective sample. This means the
unmarginalised model is in fact more computationally eﬃcient even without the boost from the software
implementation. In contrast, the fact that the marginalised stan model had computational eﬃciency similar
to jags-full shows that, once again, the sampler and software implementation details are a more substan-
tial factor in performance; in this case, the beneﬁt was enough to counteract the seeming disadvantage of
marginalisation.

4 Discussion

State-of-the-art software implementations of Markov chain Monte Carlo methods for Bayesian computation
and optimisation currently use sampling techniques that require discrete parameters to be marginalised out
of the full probability model whose joint posterior is the target distribution. Sampling from the model object
that remains after marginalisation should be quicker and the process should reach convergence sooner, but
we know of only one study that investigated this claim empirically. This question is important because
overcoming the computational burden when using iterative simulation is a perennial challenge. If marginal-
isation of discrete parameters improves eﬃciency then it could be used more generally in models for which
it is not strictly required. Marginalisation might then be viewed as one potential tool in the implementation
of any data analysis. The posterior expectation of the marginalised discrete variable is often a quantity of
interest in its own right, and many of the properties of the distribution of the marginalised parameter(s) can
be identiﬁed and estimated from the sampled chains of values for the remaining model parameters.

We presented numerical results reﬂecting the impact of marginalisation on the computational eﬃciency for
two- and three-component Gaussian mixture models and the Dawid–Skene model for categorical ratings.
This investigation was robust to the extent that we explored two software implementations of Markov
chain Monte Carlo techniques (JAGS and Stan) and directly compared marginalised and non-marginalised
models while holding constant other aspects of the sampling procedure. Our results did not show that
marginalisation on its own is suﬃcient to boost performance. Nevertheless, the most computationally
eﬃcient implementation of our models was usually Stan, which requires marginalisation.

One limitation of our study is that the models considered are either simple (normal mixtures) or of only

14

Figure 5: Time per minimum eﬀective sample, computation time, minimum eﬀective sample and ˆR when
performing Bayesian analysis on Dawid–Skene models for categorical ratings. Each box in the boxplot
represents 5 results. The horizontal line in the ˆR plots is the 1.1 threshold.

15

Time per minEffective SampleComputation TimeMin EffectiveSample Size0.00.51.01.520406080010002000300040005000valueRhatstanjags−fulljags−margjags−full−restricted1.021.051.081.11modelvaluemid-level complexity (Dawid–Skene). The models are much more analytically tractable and therefore better
understood than the Cormack–Jolly–Seber (CJS) model. This may be why an investigation of the CJS
model showed that code for marginalised models was anywhere from ﬁve to more than 1,000 times faster than
representations of the model that retained the discrete parameters while maintaining essentially identical
inferences (Yackulic et al., 2020). The authors noted that “understanding how marginalisation works shrinks
the divide between Bayesian and maximum likelihood approaches to population models [and] allows users
to minimise the speed that is sacriﬁced when switching from a maximum likelihood approach”. That is,
a wider appreciation of the beneﬁts of marginalisation has the potential to promote the use of Bayesian
statistical methods.

Another speciﬁc feature of the two- and three-component Gaussian mixture models is that we used only
informal experiments to conclude that using 102 as the variance of the prior distribution for the µ’s would
boost convergence performance. A more thorough investigation could and should be done to choose a value
for the variance that provides the greatest improvement in convergence. We agree with Yackulic et al. (2020)
that “widespread application of marginalisation in Bayesian population models will facilitate more thorough
simulation studies, comparisons of alternative model structures, and faster learning”.

References

P.-C. B¨urkner, J. Gabry, M. Kay, and A. Vehtari. posterior: Tools for working with posterior distributions,

2022. URL https://mc-stan.org/posterior/. R package version 1.3.0.

B. Carpenter, A. Gelman, M. D. Hoﬀman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li,
and A. Riddell. Stan: A Probabilistic Programming Language. Journal of Statistical Software, 76(1),
2017. ISSN 1548-7660. doi: 10.18637/jss.v076.i01. URL http://www.jstatsoft.org/v76/i01/.

A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the EM al-
gorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28, 1979. doi:
10.2307/2346806. URL https://www.jstor.org/stable/10.2307/2346806.

A. Gelman and D. B. Rubin. Inference from iterative simulation using multiple sequences. Statistical Science,
7(4):457–472, Nov. 1992. doi: 10.1214/ss/1177011136. URL https://doi.org/10.1214/ss/1177011136.

M. I. Gorinova, A. D. Gordon, and C. Sutton. Probabilistic programming with densities in slicstan: Eﬃcient,
ﬂexible, and deterministic. Proc. ACM Program. Lang., 3(POPL), jan 2019. doi: 10.1145/3290348. URL
https://doi.org/10.1145/3290348.

M. D. Hoﬀman and A. Gelman. The no-u-turn sampler: Adaptively setting path lengths in hamilto-
nian monte carlo. Journal of Machine Learning Research, 15:1593–1623, 2014. URL http://jmlr.org/
papers/v15/hoffman14a.html.

M.

B.

Joseph.

for

ters
2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/.

ecologists

2020.

URL

A
using

step-by-step
Stan,

guide

to marginalizing

parame-
https://mbjoseph.github.io/posts/

discrete

over

M. Plummer. rjags: Bayesian Graphical Models using MCMC, 2019. URL https://CRAN.R-project.org/

package=rjags. R package version 4-10.

J. Pullin, L. Gurrin, and D. Vukcevic. Statistical Models of Repeated Categorical Ratings: The R package

rater. arXiv:2010.09335 [stat], July 2021. URL http://arxiv.org/abs/2010.09335.

Stan Development Team. RStan: the R interface to Stan, 2020. URL http://mc-stan.org/. R package

version 2.21.1.

Stan Development Team.

Stan User’s Guide, 2022.

URL https://mc-stan.org/docs/2_29/

stan-users-guide/. Version 2.29.

16

C. B. Yackulic, M. Dodrill, M. Dzul, J. S. Sanderlin, and J. A. Reid. A need for speed in Bayesian population
models: a practical guide to marginalizing and recovering discrete latent states. Ecological Applications,
30(5):e02112, 2020. doi: 10.1002/eap.2112. URL https://doi.org/10.1002/eap.2112.

17

