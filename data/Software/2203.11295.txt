Benchmarking Test-Time Unsupervised Deep
Neural Network Adaptation on Edge Devices

Kshitij Bhardwaj, James Diffenderfer, Bhavya Kailkhura, and Maya Gokhale
Lawrence Livermore National Laboratory, Livermore, CA

2
2
0
2

r
a

M
1
2

]

G
L
.
s
c
[

1
v
5
9
2
1
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—The prediction accuracy of the deep neural networks
(DNNs) after deployment at the edge can suffer with time
due to shifts in the distribution of the new data. To improve
robustness of DNNs, they must be able to update themselves
to enhance their prediction accuracy. This adaptation at the
resource-constrained edge is challenging as: (i) new labeled data
may not be present; (ii) adaptation needs to be on device as
connections to cloud may not be available; and (iii) the process
must not only be fast but also memory- and energy-efﬁcient.
Recently, lightweight prediction-time unsupervised DNN adap-
tation techniques have been introduced that improve prediction
accuracy of the models for noisy data by re-tuning the batch
normalization (BN) parameters. This paper, for the ﬁrst time,
performs a comprehensive measurement study of such techniques
to quantify their performance and energy on various edge
devices as well as ﬁnd bottlenecks and propose optimization
opportunities. In particular, this study considers CIFAR-10-
C image classiﬁcation dataset with corruptions, three robust
DNNs (ResNeXt, Wide-ResNet, ResNet-18), two BN adaptation
algorithms (one that updates normalization statistics and the
other that also optimizes transformation parameters), and three
edge devices (FPGA, Raspberry-Pi, and Nvidia Xavier NX).
We ﬁnd that the approach that only updates the normalization
parameters with Wide-ResNet, running on Xavier GPU, to be
overall effective in terms of balancing multiple cost metrics.
However, the adaptation overhead can still be signiﬁcant (around
213 ms). The results strongly motivate the need for algorithm-
hardware co-design for efﬁcient on-device DNN adaptation.

I. INTRODUCTION
While deep neural networks (DNNs) are trained and vali-
dated extensively using a large dataset before deployment on
edge, these networks are still prone to degradation in pre-
diction accuracy in the post-deployment real world operation.
The new input data samples that the trained DNNs encounter
may have different distributions than their training dataset
(called dataset shifts) [1]. This change in distributions could be
caused due to sensor or environmental noise [2]. To improve
robustness of DNNs, they are trained using data augmen-
tation [3] and adversarial training [4] techniques. However,
these ofﬂine robust training methods may not be enough to
completely handle the real-time noise as they cannot cover
the excessively wide range of potential data shifts that can
occur post deployment. Therefore, the neural networks need
to be adapted to improve their prediction accuracy [5]–[7].

Various transfer learning techniques exist that can be used
for neural network adaptation on edge devices, most of which
however, require use of new labeled data that
is seldom
available at test time [8]–[10]. This paper targets challenging,

This paper was selected for poster presentation in International Symposium

on Performance Analysis of Systems and Software (ISPASS), 2022.

albeit real-world scenarios where labels for the new, poten-
tially domain-shifted, test data is not present: such as devices
operating in remote places without human intervention [11],
[12] or when the cost of annotating the new data with labels is
too high and not feasible [2], [13]. Some key examples of these
scenarios are: (i) DNNs performing human action recognition
on drones without labeled samples [11]; (ii) techniques such
as laser-induced breakdown spectroscopy in extreme environ-
ments (e.g., other planets) [12]; and (iii) medical imaging
where noise could be added due to scanners and the DNN
for analysis needs to rapidly adapt without labeled data [13].
In order to continually maintain or improve prediction accu-
racy of deployed DNNs and meet tight performance constraints
the DNNs must be adapted on
of streaming applications,
device based on the new shifted test data. Due to hard dead-
lines, adaptation through cloud services may not be always
feasible. Moreover, certain devices can be operating in areas
with limited or no connectivity (e.g., military zones, other
planets). This prediction-time (or test-time) adaptation at the
edge is challenging as: (i) new labeled data may not available;
(ii) it should be fast as the networks are typically operating on
streaming data with strict timing deadlines, and (iii) it should
be lightweight and energy-efﬁcient as the edge devices are
generally resource-constrained and could be battery operated.
test-time unsupervised DNN adaptation tech-
niques have been introduced that only update the batch-norm
(BN) parameters of pre-trained DNNs based on newly seen
data. BN layers are typically added to DNNs for faster and
more robust training and are common in the modern DNNs.
One such algorithm, which we call BN-Norm, recomputes the
normalization statistics during test time [6], [14]. The other
approach, which we call BN-Opt, not only re-estimates these
statistics but also optimizes transformation BN parameters
using a single backpropagation pass during prediction [5].
Both of these algorithms have been shown to be effective in
improving robustness for noisy data. The simplicity of these
approaches make them ideal candidates for real-time adapta-
tion at the edge. However, as these techniques are designed in
isolation without considering/evaluating the hardware cost, it
is unclear if they can run efﬁciently on edge devices.

Recently,

Contributions. In this paper, we perform a comprehen-
sive measurement study of prediction-time unsupervised DNN
adaptation techniques at the edge. To the best of our knowl-
edge,
this is the ﬁrst study aimed at exploring such un-
supervised approaches for resource-constrained devices. In
particular, we consider the following: (i) three robust DNNs

 
 
 
 
 
 
that are pre-trained using data augmentation [3] and adver-
sarial training [4] on CIFAR-10 image classiﬁcation dataset
(ResNeXt, Wide-ResNet, and ResNet-18). These models are
among the top models on the robustbench leaderboard1. They
use ﬂoat32 data type as robustness to corruptions has not been
well explored for ﬂoat16 or lower types; (ii) two adaptation
algorithms (BN-Norm [6], [14] and BN-Opt [5]); (iii) three
edge devices with varying compute capability, memory, and
price (an Ultra96-v2 FPGA, a Raspberry-Pi, and Nvidia Jetson
AI acceleration platform: Xavier NX); and (iv) CIFAR-10-C
dataset, that includes a variety of image corruptions, is used
to test the models and the adaptation approaches.

The study answers the following algorithm-hardware co-
design questions: (i) for each device, what is the optimal
choice of robust DNN and test-time adaptation algorithm in
terms of three objective functions: prediction accuracy, adap-
tation time, and energy dissipated during adaptation? (ii) what
are the bottlenecks faced when executing these algorithms
on the various devices? and (iii) are there potential ML and
hardware optimization opportunities to improve adaptation
time and energy? We also select
the overall best DNN,
adaptation technique, and device for varying constraints on
the different cost metrics. The results show that there exists
a gap between algorithmic advances in adaptation and edge
hardware designs to efﬁciently support such advancements.
The study reveals interesting and somewhat non-obvious out-
comes and demonstrates critical trade-offs between accuracy,
performance, energy, and memory.

In particular, there are ﬁve key outcomes: (i) for all three
DNNs, BN-Opt outperforms BN-Norm in terms of reducing
prediction errors during test time (by 2.45% on average) for
CIFAR-10-C. Both are signiﬁcantly better than no adaptation
(6.67% and 4.02%, respectively) even though the models were
trained ofﬂine with robust methods; (ii) while having a high
number of BN parameters in a DNN (ResNeXt) leads to
best prediction accuracy after adaptation, such models cannot
be used with BN-Opt on memory-constrained edge devices
such as Ultra96-v2 FPGA. Even when using the GPU on
Xavier NX, ResNeXt and BN-Opt cause out of memory
issues; (iii) if for a given application, prediction accuracy is
top priority, ResNeXt with BN-Opt should be used. For this
conﬁguration, Xavier NX leads to the lowest runtime while
Raspberry Pi delivers the lowest energy; (iv) if all three cost
metrics are equally critical, then Wide-ResNet with BN-Norm
should be used running on Xavier NX GPU, which outper-
forms other devices. Compared to (iii), this conﬁguration is
220× faster and 114× more energy-efﬁcient but with 5.6%
increase in prediction error. However, the extra adaptation
time is still signiﬁcant (213 ms) and can be a bottleneck for
tight deadlines; (v) embedded hardware DNN accelerators in
edge devices can certainly improve performance and energy
efﬁciency during prediction-time adaptation. The overhead

1Robustbench systematically tracks the progress being made in adversarial
robust machine learning and keeps a record of the DNN models that perform
the best in terms of prediction accuracy for corrupted/noisy datasets. However,
it does not include adaptation approaches.

for BN-Opt is more signiﬁcant than BN-Norm, due to extra
backpropagation, which becomes a considerable bottleneck
when run on low-power Arm cores. Therefore, BN-Opt re-
quires high-performance advanced accelerator, such as Volta
GPU on Xavier, to accelerate this training step effectively,
showing up to 7.89× speedup compared to the CPU cluster
on Xavier. The above outcomes strongly motivate the need
for algorithm-hardware co-design when designing on-device
DNN adaptation techniques. We conclude the paper with some
architecture-algorithm insights gained from this study.

II. BACKGROUND

This section provides an overview of the techniques to
improve robustness of DNNs as well as the BN-Norm and
BN-Opt adaptation approaches that are the focus of this paper.

A. Improving robustness of DNNs during training

There are two widely used ofﬂine training techniques to

improve robustness of DNNs as described below.

1) Data augmentation: Data augmentation involves train-
ing a neural network using not only the “clean” original
samples (such as images in CIFAR-10) but also with additional
noisy versions of the original samples [3]. This technique
greatly improves the generalization performance of the DNNs.
Common examples of augmentation include: Cutout, where a
portion of an image is removed [15] and Cutmix that replaces a
portion of an image with another [16]. A recent augmentation
approach that was shown to achieve better robustness is
Augmix, which applies a series of augmentations to an image
(such as rotate, posterize, etc.) then mixes the transformed
images into a new image [3]. Such samples, while quite
different from the original, are not unrealistic and capture
various noises. Augmix is used in this study.

2) Adversarial training: This training makes a model ro-
bust to any adversarial attacks. The training involves solving
a min-max optimization problem, where: (i) adversarial image
samples are generated from the clean samples by ﬁnding
imperceptible perturbations to the clean images such that the
prediction loss of a DNN is maximized, and (ii) the prediction
loss for these adversarial samples is minimized. In our study,
we use a state-of-the-art adversarial training approach where
imperceptibility is deﬁned using learned perceptual
image
patch similarity (LPIPS) distance [4]. LPIPS distance is based
on the activations of a DNN evaluated on two different images
and is more suitable to common image corruptions.

B. BN-Norm test-time adaptation

BN-Norm adaptation is a lightweight approach to adapt
DNNs at test time without supervision to improve their classi-
ﬁcation accuracy [6], [14]. BN-Norm only updates the batch-
norm (BN) layers without modifying the other layers. The BN
statistics, mean (µ) and standard deviation (σ), are typically
ﬁxed after training. However, in BN-Norm, these statistics
are recomputed during test-time using an incoming batch of
unlabeled data (other BN parameters are not modiﬁed).

2

well-known image classiﬁcation dataset with corruptions, and
three edge devices. The robust DNNs are pre-trained models:
trained with techniques such as Augmix data augmentation
or adversarial training. The two adaptation approaches are:
BN-Norm adaptation (Section II-B) and BN-Opt adaptation
(Section II-C). In addition, we also include no adaptation
approach to provide a baseline for comparisons. For each
adaptation algorithm, the amount of recently-seen unlabeled
test data used for model update (batch size) is also varied:
50, 100, and 200 samples. We use CIFAR-10-C dataset [17]
that applies 15 different common corruptions to the standard
CIFAR-10 image classiﬁcation dataset. Finally, the three edge
devices considered have varying processing capability and
memory (as well as power requirements), and are typically
deployed for a variety of streaming applications. These devices
are: (i) with no inherent DNN acceleration capability (Ultra96-
v2 FPGA and Raspberry-Pi), and (ii) with a DNN accelerator
(Nvidia Jetson Xavier NX).

B. Robust DNN models

1) ResNet-18: To improve robustness, ResNet-18 is trained
using both Augmix data augmentation technique for CIFAR-
10 dataset (Section II-A1, similar to [3]) as well as us-
ing LPIPS-based adversarial training (Section II-A2, similar
to [4]). This trained ResNet-18 has 0.56 Giga multiply-
accumulate (GMAC) operations and 11.17M total parameters,
out of which there are 7808 batch-norm parameters (target for
BN-Norm and BN-Opt). Overall it takes 86 MB memory.

2) Wide-ResNet: We leverage Wide-ResNet-40-2 model,
trained using Augmix data augmentation technique for CIFAR-
10 (Section II-A1, similar to [3]). This model has 0.33
GMAC operations, 2.24M total parameters, 5408 batch-norm
parameters, and takes 9 MB memory.

3) ResNeXt: We use ResNeXt29 32x4d with a cardinality
of 4 and a base width of 32. ResNeXt is also trained using
Augmix for CIFAR-10 (Section II-A1, similar to [3]). This
model has 1.08 GMAC operations, 6.81M total parameters,
25216 batch-norm parameters, and takes 26 MB memory.
ResNeXt includes signiﬁcantly more batch-norm parameters
than the above two models.

C. Image classiﬁcation data with corruptions

While the above three models are pre-trained on CIFAR-
10, the CIFAR-10-C dataset is used for testing the models.
CIFAR-10-C includes 15 different types of corruptions, such
as gaussian noise, blur, snow, fog, frost, brightness, etc., with
5 different severity levels (5 being the most severe and 1 being
the least). In our study, all 15 corruptions (level 5) are used.
The test data consist of streaming 10000 unlabeled CIFAR-
10-C image samples for each corruption type. To perform
online adaptation during test time, batches of recently seen
image samples are used by the adaptation algorithms. Three
different batch sizes are used which have a direct impact on
the adaptation efﬁcacy as well as performance/energy/memory
during adaptation: 50, 100, and 200.

Fig. 1: Measurement study overview

C. BN-Opt test-time adaptation

BN-Opt also only updates the BN parameters during test-
time without any supervision [5]. BN-Opt, however, not only
recomputes the mean and variance parameters for each of
the batch-norm layers but also optimizes the transformation
parameters that apply channel-wise scales and shifts to the
features. This approach is efﬁcient as the transformation
parameters constitute < 1% of the total model parameters.

In particular, two steps are performed for each batch-norm
layer during test-time: (i) normalization that standardizes the
input x into x(cid:48) = (x − µ)/σ using its mean and standard devi-
ation, and (ii) transformation that turns x(cid:48) into x(cid:48)(cid:48) = γx(cid:48) + β
using scale (γ) and shift (β) parameters. While the statistics
for (i) are recomputed from the unlabeled data, scale and shift
parameters in (ii) are optimized by a loss function. Since,
the optimization is performed using unlabeled data, entropy
of model predictions is used as the loss function. Shannon
entropy for a prediction y, which can be computed without
any labeled data, is deﬁned as: H(y) = − (cid:80)
c p(yc)logp(yc)
for probability of y for class c. Predictions with lower entropy
are shown to exhibit lower prediction error rate as well.
III. MEASURING DNN ADAPTATION AT THE EDGE

This section provides an overview of our measurement

study, followed by the details on the various parameters.

A. Measurement study overview

Figure 1 shows an overview of our hardware-algorithm
measurement study for test-time unsupervised DNN adaptation
at the edge. The aim is threefold: (i) for each target device,
among the state-of-the-art robust DNN models and adaptation
strategies, ﬁnd the best model and adaptation algorithm that
leads to low prediction errors as well as low adaptation runtime
and energy consumption for a given noisy data stream. While
the former cost metric is important for robustness, minimizing
the latter two are critical
the constrained timing
requirements for various streaming applications and be able
to run on low-power devices. We also ﬁnd the overall best
DNN, adaptation scheme, and device in terms of the three
cost metrics; (ii) measure and provide a detailed analysis of
the adaptation time overhead for the different models and
algorithms running on the various edge devices; and (iii) ﬁnd
algorithm and hardware optimization opportunities.

to meet

In this paper, we consider three state-of-the-art robust neural
two unsupervised NN adaptation approaches, a

networks,

3

D. Adaptation techniques

In this study, a no adaptation and two DNN adaptation
algorithms are used on the CIFAR-10-C test data: (i) no
adaptation, where the three pre-trained robust DNNs are used
without any changes; (ii) BN-Norm adaptation (Section II-B)
that uses the recently seen batch of image samples to simply
recompute the mean and standard deviation BN parameters;
and (iii) BN-Opt adaptation (Section II-C) which recalculates
the normalization parameters as well as optimizes the BN
transformation parameters. While BN-Norm does not need to
retrain when adapting the model, BN-Opt, on the other hand,
performs a single backpropagation pass using the collected
batch of image samples to optimize the transformation param-
eters for the entropy loss function using the Adam optimizer.
The algorithms are implemented using Pytorch [18]. During
test time with CIFAR-10-C, the no adaptation algorithm keeps
the model in eval() mode of Pytorch (inference only), while
BN-Norm and BN-Opt require the model to be in train() mode
(to allow for adaptation with inference). The forward passes
for both BN-Norm and BN-Opt are modiﬁed to adapt the
DNN: at each adaptation point, these algorithms ﬁrst perform
inference followed by updating (and optimizing in case of
BN-Opt) the batch-norm parameters based on the collected
recently seen batch of data. Since our focus is on adaptation
(or retraining) on edge devices, optimized frameworks for
fast
inference on edge, such as Tensorﬂow-Lite [19] and
TensorRT [20], are not applicable and hence not used.

E. Edge devices

Details on the three edge devices are presented next.
1) Ultra96-v2 FPGA: We use an FPGA due to their in-
creased use in streaming application [21], both due to the
support for programmability as well as low-power operations.
The Ultra96-v2 consists of a Xilinx Zynq UltraScale+ MPSoC
ZU3EG A484 processing system (PS) part and 2 GB LPDDR4
memory. The DNN adaptation algorithms are run on the PS
while the programming logic (PL) part is not used in this study.
We used the PS quad-core ARM Cortex A53 multi-processing
system that can run up to 1.5 GHz, with 32 KB L1 caches
and a 1 MB L2 cache. On the software side, we use the Pynq
framework [22] with Petalinux to create the Linux image on
the PS. Pytorch-1.8.0 for ARM is used.

2) Raspberry-Pi: Another

low-power edge device that
has become critical for streaming applications is Raspberry-
Pi [23]. We use the Raspberry-Pi 4 Model B that consist of a
quad-core ARM A72 SoC running at up to 1.5 GHz with 32
KB data + 48 KB instruction L1 caches and 1 MB L2 cache.
This device also comes with 8 GB LPDDR4 memory. We
booted Ubuntu 21.04 and installed Pytorch-1.8.0 for ARM.

3) Nvidia Jetson Xavier NX: While the above two devices
did not use hardware neural network acceleration capability,
NX includes a GPU to accelerate DNN processing. NX uses
a 6-core Nvidia CARMel ARM SoC (128 KB L1 instruction
cache + 64 KB L1 data cache, and 6 MB L2 cache and 4 MB
L3 cache) that can run up to 1.9 GHz speed. It integrates a 384-
core Volta GPU (maximum frequency of 1.1GHz) and 8GB

Fig. 2: Average prediction errors for CIFAR-10-C

LPDDR4 memory. NX uses Jetpack 4.4 software development
kit (SDK) that boots Linux4Tegra operating system (similar
to Ubuntu 18.04 but for Nvidia devices) and comes with
CUDA 10.2 and GPU NN acceleration libraries (cuDNN 8.0).
Pytorch-1.8.0 is installed using Nvidia’s pre-built installer.

F. Objective functions

Three objective functions are considered: overall prediction
accuracy for the entire test stream, average forward time per
batch (inference + any adaptation), and average energy con-
sumption per batch. Accuracy and performance are computed
using Pytorch. To compute energy, we measure power per
batch using Kuman wall outlet power meter.

Different applications give priorities to different objectives.
For example, some scenarios may have strict performance
constraints, hence adaptation times should be minimized,
while for others maintaining prediction accuracy during noise
is more important. To ﬁnd the best DNN model and adaptation
approach for a variety of cases, we combine the multiple ob-
jective functions into a single one using a weighted approach.
In particular, we use: w1∗time+w2∗energy+w3∗pred error
as the single objective function to minimize, where w1, w2,
and w3 are weights whose sum should be equal to 1. We ﬁnd
optimal conﬁgurations for four cases that cover a wide variety
of scenarios: (i) same importance to all three objectives (each
weight equal to 0.33); (ii) performance is the top priority but
other metrics are not completely discarded and hence assigned
much smaller weights (w1 = 0.8 and others are 0.1 each);
(iii) prediction error is more important (w3 is 0.8) with other
metrics much less critical but still considered (0.1 each); and
(iv) energy is critical with 0.8 weight and others as 0.1.

IV. MEASUREMENT RESULTS AND ANALYSIS
This section ﬁrst presents the prediction accuracy results for
the different robust DNNs and adaptation approaches, followed
by their detailed performance analysis and multi-objective cost
trade-offs for each of the three edge devices. The section
concludes with a summary of the overall outcomes of our
study, a comparison with standard MobileNet-V2 model, and
the architecture-algorithm insights gained from this study.
A. Prediction accuracy

Figure 2 shows the prediction error for the three algorithms:
no adaptation, BN-Norm, and BN-Opt, for an entire stream
of 10000 unlabeled CIFAR-10-C image samples. The error
is averaged across the 15 noise types. Three robust NNs are

4

more suitable for test-time NN adaptation on an A53 device
(while incurring a prediction error overhead of around 2.65%).
In addition, while BN-Norm is able to run for all 9 cases
on the FPGA PS, BN-Opt however, runs out of memory
for RXT for 100 and 200 batch sizes. So, while RXT-AM-
100/200 with BN-Opt achieved the lowest prediction error
(Figure 2), they cannot be used on a constrained memory
device with only 2 GB memory. Even though RXT’s model
size is 26 MB,
it runs out of
lower than R18 (86 MB),
memory as the number of BN parameters optimized through
backpropagation for RXT are signiﬁcantly higher than the
others (25216 vs. 5408/7808). To perform backpropagation,
Pytorch creates a dynamic computational graph during forward
pass that includes the nodes that are enabled for gradient
computation. RXT’s graph takes 3.12 GB and 5.1 GB memory
(based on a memory proﬁler used for R-Pi and NX) with
batch size of 100 and 200, respectively, which is more than
the FPGA’s 2GB memory. Moreover, compared to the other
ResNet models, RXT also shows signiﬁcantly higher forward
time due to many more number of MAC operations (RXT:
1.08 GMACs, WRN: 0.33 GMACs, R18: 0.56 GMACs).

Figure 4 shows the breakdown of performance for BN-Opt,
BN-Norm, and No-Adapt for Wide-ResNet and ResNet-18
in terms of the time spent on forward and backpropagation
backward passes (averaged for convolution and BN layers).
This breakdown is obtained using the Pytorch Autograd pro-
ﬁler with batch size 50 (similar trend is expected for other
batch sizes). The proﬁler runs out of memory for RXT-AM
(hence not included). While the forward times for convolution
layers are almost the same for a given network for each of
the three algorithms, the BN forward time is higher for BN-
Norm and BN-Opt (up to 3.68× for WRN, 4.71× for R18)
compared to no adaptation. The reason is the re-tuning of the
normalization parameters in the forward pass which adds a
signiﬁcant cost (higher for R18 as it has more number of
BN parameters). Moreover, while there is no backpropagation
performed for No-Adapt and BN-Norm, this backward pass of
BN-Opt shows considerable overhead for both convolution and
BN layers compared to the forward passes (up to 2.51× and
2.78×, respectively). Although this result is expected as A53s
are not designed to run compute-intensive training algorithms,
it also motivates the need to explore potentially ofﬂoading
training kernels to the PL side of the FPGA for acceleration.
Some of the recent works have started looking into training
acceleration on FPGAs [24].

Performance-energy-accuracy trade-offs. Figure 5 shows
all three objectives for the various cases of adaptation algo-
rithms, DNNs, and batch sizes. Note that RXT-AM-100/200
for BN-Opt are not present as they ran out of memory. Three
important cases are shown based on the single weighted objec-
tive function (w1 ∗f orward time+w2 ∗energy +w3 ∗error,
Section III-F): (i) all three objectives are assigned a weight of
0.33 each, which leads to WRN-AM-50 with BN-Norm to
be the optimal point (3.95 secs, 4.93 J, and 15.21% error)
because of lightweight adaptation of BN-Norm, coupled with
the smallest number of BN parameters in Wide-ResNet; (ii)

Fig. 3: Ultra96 PS forward times (inference + any adaptation)

considered: ResNeXt with AugMix [3] (RXT-AM), Wide-
ResNet with AugMix [3] (WRN-AM), and ResNet-18 with
both AugMix and adversarial training [4] (R18-AM-AT). For
each network, three test-time batch sizes are used for online
adaptation: 50, 100, and 200. The average prediction error is
found to be consistent across the edge devices in cases where
they do not run out of memory (RPi, Xavier NX).

As evident, no adaptation algorithm, even when using
AugMix and adversarial training techniques, shows the worst
prediction error. BN-Norm, which recomputes the normaliza-
tion parameters during test time, is able to improve on no
adaptation by an average of 4.02% (across the 9 cases). BN-
Opt, on the other hand, shows an average improvement of
6.67% over no adaptation and 2.65% over BN-Norm as it
also optimizes the transformation parameters. For both BN-
Norm and BN-Opt, reduction in prediction error going from
50 to 100 batch size is higher than from 100 to 200 showing
that larger batch sizes than 200 will yield diminishing returns.
Additionally, bigger batch sizes for online adaptation may not
be reasonable for memory-constrained devices.

B. Ultra96-v2 processing system (PS)

This section presents the performance analysis for the FPGA
PS, followed by the performance-energy-accuracy trade-offs
discussion. Average forward time per batch is used as the
performance metric to compare the different neural networks
using no adaptation (No-Adapt), and BN-Norm and BN-Opt.
Forward time for No-Adapt is simply the inference latency
while for the others it also includes any extra adaptation time:
re-estimating normalization parameters for BN-Norm and BN-
Opt, and a single backpropagation pass for BN-Opt to optimize
the transformation parameters. The times are averaged over
multiple adaptations performed over an input data stream of
CIFAR-10-C samples. Pytorch is run on the FPGA PS in
a multi-threaded setting utilizing all four Arm A53 cores,
while exploiting intra-operation parallelism. Similarly, energy
is also measured for No-Adapt (inference only) and BN-Norm
and BN-Opt (inference+adaptation) at the different adaptation
points and then averaged.

Performance analysis. As shown in Figure 3, compared
to No-Adapt, the extra average adaptation overhead for BN-
Norm is 1.40 secs while for BN-Opt it is much higher: 30.27
secs. The simple recompute operation of the former makes it

5

Fig. 4: Forward (fw) and any backpropagation backward (bw)
pass performance on Ultra96-v2 for batch size 50

Fig. 6: RPi forward times (inference + any adaptation time)

Fig. 5: Performance-energy-accuracy trade-offs: Ultra96-v2 PS

when accuracy is assigned a higher weight of 0.8 while still
giving small weights of 0.1 each to the other two, WRN-AM-
50 with BN-Opt is selected (13.35 secs, 14.35 J, 12.37% error)
as BN-Opt is highly effective at reducing prediction error and
WRN balances accuracy with performance and energy better
than other models; and (iii) when either performance or energy
are top priority (0.8 weight), WRN-AM-50 with No-Adapt
(3.58 secs, 4.47 J, 18.26% error) is the reasonable choice.

Summary. Even for Ultra96-v2 with A53 cores and 2
GB memory, real-time unsupervised adaptation is possible
although not efﬁcient. Wide-ResNet using BN-Opt (batch size
of 50) is optimal when priority is given to prediction error.
In contrast, BN-Norm for the same network is the best when
equal weight is given to prediction error, performance, and
energy. While ResNeXt (highest number of BN parameters)
with BN-Opt achieved the lowest prediction error, it is unable
to run on Ultra96-v2 PS, showing the need for device-aware
design of robust DNN models and adaptation algorithms tar-
geting memory-constrained devices. BN-Opt incurs signiﬁcant
performance overheads over no adaptation and BN-Norm as
it involves backpropagation. Use of PL side of the FPGA to
ofﬂoad training kernels can be explored.

C. Raspberry Pi (RPi) 4

Performance analysis and the various cost trade-offs are now

presented for RPi.

Performance analysis. As shown in Figure 6, all three
DNNs, with both BN-Norm and BN-Opt, are able to run on
the RPi as it packs more memory (8GB) than the FPGA.
Pytorch is run multi-threaded on quad-core Arm A72 cores.
Compared to No-Adapt, the extra adaptation time (in addition
to the inference, across all 9 cases) for BN-Norm is only 0.86

Fig. 7: Forward (fw) and any backpropagation backward (bw)
pass performance for RPi for batch size 50

secs on average but for BN-Opt it is 24.9 secs. Due to the use
of A72s, these times are reduced compared to the FPGA.

Figure 7 shows the breakdown of performance for BN-Opt,
BN-Norm, and No-Adapt approaches for all three models in
terms of the average time spent on forward and backpropaga-
tion backward passes. Pytorch Autograd proﬁler is used with
the batch size ﬁxed to 50 (similar trends for other batch sizes).
Similar to Ultra96-v2, forward BN time for BN-Norm and
BN-Opt is up to 4.6× higher than No-Adapt. Backward times
for convolutions and BN in BN-Opt (zero for BN-Norm and
No-Adapt) are also signiﬁcant and explain the high adaptation
times for BN-Opt in Figure 6.

Performance-energy-accuracy trade-offs. Figure 8 shows
all three objectives for the various cases. The same weighted
multi-objective optimization function is used. The important
outcomes are: (i) for all three objectives assigned an equal
weight of 0.33 each, WRN-AM-50 with BN-Norm is the best
(2.59 secs, 5.95 J, and 15.21% error) because of lightweight
WRN and fast adaptation of BN-Norm; (ii) when accuracy is
assigned a higher weight of 0.8 while the other two are 0.1
each, WRN-AM-50 with BN-Opt is selected (7.97 secs, 19.12
J, 12.37% error) as BN-Opt is highly effective at reducing
prediction error for noisy data. However, this conﬁguration
shows 3.07× higher forward time and 3.21× more energy
than (i); (iii) when performance is the top priority (using
0.8 weight), interestingly, WRN-AM-50 with Norm is again
selected. Although its forward time is very slightly higher than

6

other cases, speedup using the GPU is observed for the three
algorithms and DNN models: for No-Adapt 90.5% on average,
for BN-Norm 68.13%, and for BN-Opt 79.21%.

trade-offs. Figure

Figure 10 shows the average forward and backward times
for convolution and BN layers. The convolution backward
pass overhead when using GPU for BN-Opt (over forward
pass on GPU) is 2.2× on average. The same difference
for CPU on Xavier is 2.5×. Faster forward and backward
convolution passes for Xavier explain the speedup obtained
for BN-Opt using GPU. However, interestingly, the forward
BN performance is worse for RXT when using GPU over
CPU for both BN-Opt and BN-Norm, but it does not have a
major impact on the overall time.
Performance-energy-accuracy

11
shows these cost metrics for the various cases. The important
outcomes are: (i) when all three costs are equally weighted,
WRN-AM-50 with BN-Norm on GPU leads to overall best
solution (0.31 secs, 2.96 J, 15.21%); (ii) when prediction
error has 0.8 weight, WRN-AM-50 with BN-Opt on GPU
should be selected (0.82 secs, 7.96 J, 12.37%). Also, note
that the forward time (inference + adaptation) is under 1 sec.
While the GPU burns more power than CPU (2.2×), the
signiﬁcantly faster execution of the former makes it more
energy-efﬁcient (2.86×); and (iii) if the focus is more on
energy or performance, then WRN-AM-50 with No-Adapt
(on GPU) is the optimal choice (0.10 secs, 1.02 J, 18.26%).
Summary. Volta in NX leads to signiﬁcant speedups over
CPU for not only No-Adapt and BN-Norm but also for BN-
Opt. Interestingly, we see out-of-memory issues with the
GPU for RXT. The lightweight Wide-ResNet with BN-Norm
algorithm running on GPU is the best when equal priority to
given to all three metrics. The same network with BN-Opt
(using the GPU) is optimal when accuracy is more important
while still giving small weights to performance and energy.
E. Overall outcomes

In this section, we ﬁnd the overall best DNN model, robust
training technique, adaptation approach (with batch sizes), and
the device/accelerator.

Figure 12 shows the complete results, plotting all the design
points from Figures 5, 8, 11 for the three costs. For an
application where prediction accuracy is the only priority,
RXT-AM-200 with BN-Opt should be used with the lowest
(10.15%) error. This conﬁguration will require at least 8 GB
of memory for execution, therefore Ultra96-v2 is not feasible.
Among RPi and NX for this model and adaptation approach,
NX with CPU leads to the smallest runtime (A1: 69.58 secs) as
GPU runs out of memory for batch size 200. However, in terms
of energy, RPi is the most efﬁcient (A2: 337.43 J). But as is
evident, both the runtime and energy costs are signiﬁcant and
may not be practical. Alternatively, if accuracy, performance,
and energy are all equally important for an application, then
WRN-AM-50 with BN-Norm, running on NX GPU is the best
(A3: 0.315 secs, 2.96 J, 15.21%). Compared to BN-Opt (on
GPU) with WRN-AM-50, BN-Norm outperforms by 61.6%
lower latency and 62.8% lower energy for the same network.
Compared to RXT-AM-200 and BN-Opt that achieved the best

7

Fig. 8: Performance-energy-accuracy trade-offs analysis: RPi

Fig. 9: Xavier NX forward times (inference + any adaptation)

No-Adapt for WRN-AM-50 (2.59 secs vs. 2.04 secs) but due
to the 0.1 weight assigned to accuracy, BN-Norm leads to the
overall better conﬁguration; and (iv) when energy is assigned
the highest weight (0.8), WRN-AM-50 with no adaptation is
the best choice (2.4 secs, 5.04 J, 18.26%).

Summary. Bigger memory in RPi allows models with
higher number of BN parameters, such as ResNeXt, to run
with BN-Opt. BN-Norm, with small loss of accuracy, can be
used to perform online adaptation more efﬁciently than BN-
Opt. Therefore, while Wide-ResNet with BN-Opt is the best
choice when the top priority is accuracy, the same network
with BN-Norm should be used when fast adaptation is key.
D. Nvidia Jetson Xavier NX

The forward times of the previous devices are high due
to lack of acceleration. This study evaluates if the adaptation
can be accelerated using embedded GPUs (NX has a 384-
core Volta). It also includes 6-core Nvidia Carmel Arm, and
8GB memory that is shared between the CPU and the GPU.
For CPU results, Arm cores are used to run multi-threaded
Pytorch, and for GPU, Pytorch CUDA is used.

Performance analysis. Figure 9 shows the performance of
No-Adapt, BN-Norm, and BN-Opt for all 9 cases using CPU
and GPU. Interestingly, RXT-AM-200 with BN-Opt runs out
of memory when executed on the GPU. The dynamic Pytorch
graph created for RXT-AM-200 takes up around 5.1GB (when
proﬁled on CPU). But execution on the GPU leads to higher
use of memory due to loading of extra cuDNN libraries
by Pytorch that are utilized for GPU acceleration. For all

(a) ResNeXt

(b) Wide-ResNet
Fig. 10: Xavier NX forward (fw) and any backpropagation backward (bw) time for three DNNs (batch size: 50)

(c) ResNet-18

Fig. 11: Performance-energy-accuracy trade-offs analysis for Jetson Xavier NX

accuracy, BN-Norm and WRN-50 on NX GPU is 220× faster
and 114× more energy-efﬁcient. While WRN-AM-50 with BN-
Norm shows 3.05% lower prediction error than the same
network with No-Adapt, the extra overhead for the former is
signiﬁcant for real-time operation: 213 ms and 1.9 J (GPU).

F. Other edge-based non-robust DNN models

We also evaluated MobileNet-V2, a model which is opti-
mized for low-power edge systems. The model size is 9 MB,
includes 0.096 GMAC operations, but has 34112 batch-norm
parameters (larger than the three robust ResNet models in this
work). Closest to MobileNet is WRN (0.33 GMACs, 9 MB,
but with 5408 BN parameters). MobileNet is pretrained on
CIFAR-10, but without adversarial training, and then evalu-
ated with or without adaptation on CIFAR-10-C. MobileNet
showed 81.2% prediction error without adaptation, which was
reduced to 28.1% using BN-Opt (batch size of 200). This error
is still high compared to the three robust models used above
(10.15-12.97% with BN-Opt), highlighting the importance of
ofﬂine robust training (adversarial training/data augmentation).
Table I shows the forward times for MobileNet for various
approaches on the NX GPU. For No-Adapt, MobileNet shows,
on average, 80.9%, 19.2%, 34.8% better performance than
RXT, WRN, and R18, respectively (performance of these

Batch Size

BN-Opt

BN-Norm No-Adapt

50
100
200

1.63 s
3.7 s
8.28 s

0.58 s
1.18 s
2.95 s

0.07 s
0.13 s
0.25 s

TABLE I: MobileNet forward time on Xavier NX GPU
models in Figure 9). However, for both BN-Norm and BN-Opt,
it shows an average overhead of 2.1× compared to WRN and
R18 as it includes much higher number of BN parameters that
are adapted (34112 compared to the two models: 5408, 7808).
On the other hand, MobileNet outperforms ResNeXt for BN-
Norm and BN-Opt by 2.7× as ResNeXt includes almost 10×
more GMACs (1.08) and is closer in terms of number of BN
parameters (25216). In summary, while MobileNet is designed
for edge, its large number of BN parameters is a bottleneck
for BN-Norm and BN-Opt like adaptation algorithms.
G. Architecture-algorithm insights

The key algorithm-hardware insights revealed by our study
are: (i) trade-offs between BN parameters, prediction accuracy,
and execution time/memory requirements must be considered
when designing a robust DNN for edge. A model with smaller
number of BN parameters is more amenable to on-device NN
adaptation even though it might not lead to the best improve-
ment in prediction accuracy after adaptation, compared to a

8

needed to improve accuracy against corruptions.

V. RELATED WORK
There has been use of generative adversarial networks
(GANs) to design domain-adaptation techniques for DNNs but
they do not target unsupervised test-time adaptation [25], [26].
These approaches use a semi-supervised method to improve
DNN robustness by adding a domain classiﬁer that discrim-
inates between the source data (used for training) and target
data (seen during test time), and tries to maximize the domain
classiﬁcation loss so as to extract features that are domain-
invariant. However, these methods require full retraining of
the DNN with labeled source data and unlabeled target data,
and are therefore not applicable for fast test time adaptation.
There has been considerable work on characterizing DNN
performance on a variety of edge devices, however, they have
only targeted inference and not adaptation. Bianco et al. have
benchmarked accuracy and inference time of a variety of
DNNs on Nvidia Jetson TX1 [27], and found some of the
DNNs bottlenecked by the amount of memory available on the
device (such as ResNeXt). Kljucaric et al. characterized the
performance of AlexNet and GoogleNet on several devices:
Nvidia jetson AGX Xavier, Intel Neural Compute Stick, and
Google Edge TPU [28]. Their results showed best latency
is achieved by AGX, while for GoogleNet,
for AlexNet
TPU is faster. Similar analysis is also targeted for robotics
application [29]. An exhaustive characterization of a variety
of DNNs on many devices is performed but only for on-edge
inference without considering adaptation or robustness [30].
This analysis included performance, energy, and temperature
as well as various pruning/quantization optimizations.

A recent work performed semi-supervised domain adap-
tation for an internet-of-things edge device (TX2) for ges-
ture recognition and image classiﬁcation applications [31].
A teacher-student
technique is used, where knowledge is
transferred from a sophisticated teacher model (trained on
a server on labeled source data) to a simple student model
running on the edge while considering the domain shift in
a new environment. However,
this method requires initial
labeled source data to adapt the student model as well as
assumes connection to a server. Hence, it is not meant for un-
supervised on-device prediction-time adaptation. Other similar
edge-based transfer learning approaches have been introduced
for health monitoring at home, but the ﬁne tuning of the
deployed model is again performed using supervision and
cloud connections [8]. There are also distributed training based
DNN update approaches that do not require connection to a
cloud, but they distribute training tasks to multiple connected
edge devices. A recent paper proposed a load balancing
approach for distributed learning on the edge [32].

VI. CONCLUSION
This paper performed a comprehensive measurement study
of prediction-time unsupervised DNN adaptation techniques
to quantify their performance and energy on various edge
devices. For each device, the study found the optimal adap-
tation approach (between BN-Norm and BN-Opt) and the
type of robust DNN (among ResNeXt, Wide-ResNet, and

Fig. 12: Overall results with all the points from Figs. 5, 8, 11.
A1/A2: when accuracy is the only priority, A1 shows the
lowest runtime and A2 the lowest energy (i.e. among all points
with 10.15% error). A3: optimal point when all three costs are
equally important (0.31s, 2.96J, 15.21%).

model with higher number of BN parameters. For example,
Wide-ResNet, with smallest number of BN parameters,
is
shown to be overall more effective at balancing the various
cost metrics during adaptation than ResNeXt and ResNet-18
(and MobileNet); (ii) simply updating BN parameters (BN-
Norm) is more suited to test-time adaptation on the edge as it
is signiﬁcantly faster and more energy-efﬁcient than BN-Opt
with only 2.45% increase in prediction error on average. BN-
Opt’s single backpropagation pass is a major bottleneck. Our
results motivate the design of new hardware-aware adaptation
algorithms; (iii) we show that embedded GPUs are effective in
speeding up BN-Norm and BN-Opt. However, the adaptation
time is considerable which can lead to real-time constraint
violations. For example, even when using BN-Norm with
WRN-50 and NX GPU, the extra adaptation time overhead
is still signiﬁcant: 213 ms. As these devices are primarily
built for inference, the adaptation overhead can be reduced
with custom accelerators designed to support fast BN-based
adaptation (and/or backpropagation); (iv) while the above
results are using unpruned and full precision models, pruning
and quantization should be explored. However, care must
be taken that any model reduction should not compromise
the robust accuracy against corruptions; (v) online adaptation
algorithms are not designed for hardware-constrained edge
platforms. Algorithms should minimize memory high water
mark (streaming approaches?), and make efﬁcient use of
accelerators while minimizing memory overhead of doing so.
On the hardware design side, additional MACs and routing
fabric would make back propagation less costly, and low
power memories including nonvolatile and 3D would enable
larger batch sizes with less energy; and (vi) as shown by the
MobileNet study, online adaptation alone is not sufﬁcient and
ofﬂine robust training, such as using data augmentation, is also

9

[16] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk
Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train
the
strong classiﬁers with localizable features.
IEEE/CVF International Conference on Computer Vision, pages 6023–
6032, 2019.

In Proceedings of

[17] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas
Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robust-
bench: a standardized adversarial robustness benchmark. arXiv preprint
arXiv:2010.09670, 2020.

[18] Pytorch. Pytorch. http://www.pytorch.org/.
[19] TensorFlow. TensorFlow Lite. http://www.tensorﬂow.org/lite.
[20] Nvidia. Nvidia TensorRT. http://developer.nvidia.com/tensorrt.
[21] Song Wu, Die Hu, Shadi Ibrahim, Hai Jin, Jiang Xiao, Fei Chen, and
Haikun Liu. When FPGA-accelerator meets stream data processing in
the edge. In 2019 IEEE 39th International Conference on Distributed
Computing Systems (ICDCS), pages 1818–1829. IEEE, 2019.

[22] Xilinx. Pynq: Python Productivity. http://www.pynq.io/.
[23] Diyi Hu and Bhaskar Krishnamachari. Fast and accurate streaming
CNN inference via communication compression on the edge. In 2020
IEEE/ACM Fifth International Conference on Internet-of-Things Design
and Implementation (IoTDI), pages 157–163. IEEE, 2020.

[24] Cheng Luo, Man-Kit Sit, Hongxiang Fan, Shuanglong Liu, Wayne Luk,
and Ce Guo. Towards efﬁcient deep neural network training by fpga-
based batch-level parallelism. FCCM, 2019.

[25] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation
by backpropagation. In International conference on machine learning,
pages 1180–1189. ICML, 2015.

[26] Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Duplex
generative adversarial network for unsupervised domain adaptation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1498–1507, 2018.

[27] Simone Bianco, Remi Cadene, Luigi Celona, and Paolo Napoletano.
Benchmark analysis of representative deep neural network architectures.
IEEE Access, 6:64270–64277, 2018.

[28] Luke Kljucaric, Alex Johnson, and Alan D George. Architectural
In 2020 IEEE High
analysis of deep learning on edge accelerators.
Performance Extreme Computing Conference (HPEC), pages 1–7. IEEE,
2020.

[29] Alexander Biddulph, Trent Houliston, Alexandre Mendes,

and
Stephan K Chalup. Comparing computing platforms for deep learning on
a humanoid robot. In International Conference on Neural Information
Processing, pages 120–131. Springer, 2018.

[30] Ramyad Hadidi, Jiashen Cao, Yilun Xie, Bahar Asgari, Tushar Krishna,
and Hyesoon Kim. Characterizing the deployment of deep neural
In 2019 IEEE International
networks on commercial edge devices.
Symposium on Workload Characterization (IISWC), pages 35–48. IEEE,
2019.

[31] Jianfei Yang, Han Zou, Shuxin Cao, Zhenghua Chen, and Lihua Xie.
IEEE Internet of Things

Mobileda: Toward edge-domain adaptation.
Journal, 7(8):6909–6918, 2020.

[32] Anirban Bhattacharjee, Ajay Dev Chhokra, Hongyang Sun, Shashank
Shekhar, Aniruddha Gokhale, Gabor Karsai, and Abhishek Dubey.
Deep-edge: An efﬁcient framework for deep learning model update on
heterogeneous edge. In 2020 IEEE 4th International Conference on Fog
and Edge Computing (ICFEC), pages 75–84. IEEE, 2020.

ResNet-18) in terms of different cost metrics. A detailed
bottleneck analysis of the approaches was also presented,
followed by identifying optimization opportunities. Overall,
we found that Wide-ResNet with BN-Norm, running on Xavier
NX GPU, to be highly effective at balancing the various costs.
However, the adaptation overhead is still expensive for meeting
tight deadlines. Algorithm-hardware co-design is required to
achieve efﬁcient on-device adaptation.

ACKNOWLEDGMENT

This work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National Lab-
oratory under Contract DE-AC52-07NA27344. IM number:
LLNL-CONF-824925.

REFERENCES

[1] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and
Dawn Song. Anomalous example detection in deep learning: A survey.
IEEE Access, 8:132330–132347, 2020.

[2] Garrett Wilson and Diane J Cook. A survey of unsupervised deep
ACM Transactions on Intelligent Systems and

domain adaptation.
Technology (TIST), 11(5):1–46, 2020.

[3] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin
Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data pro-
cessing method to improve robustness and uncertainty. arXiv preprint
arXiv:1912.02781, 2019.

[4] Klim Kireev, Maksym Andriushchenko, and Nicolas Flammarion. On
the effectiveness of adversarial training against common corruptions.
arXiv preprint arXiv:2103.02325, 2021.

[5] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and
Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization.
ICLR Spotlight, 2021.

[6] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji
Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch
arXiv preprint
normalization for robustness under covariate shift.
arXiv:2006.10963, 2020.

[7] James Diffenderfer, Brian R Bartoldson, Shreya Chaganti, Jize Zhang,
and Bhavya Kailkhura. A winning hand: Compressing deep net-
arXiv preprint
works can improve out-of-distribution robustness.
arXiv:2106.09129, 2021.

[8] Abu Suﬁan, Changsheng You, and Mianxiong Dong. A deep transfer
learning-based edge computing method for home health monitoring. In
2021 55th Annual Conference on Information Sciences and Systems
(CISS), pages 1–6. IEEE, 2021.

[9] Peiying Zhang, Hao Sun, Jingyi Situ, Chunxiao Jiang, and Dongliang
Xie. Federated transfer learning for iiot devices with low computing
power based on blockchain and edge computing. IEEE Access, 2021.

[10] Phil Meier, Kris Rohrmann, Marvin Sandner, and Marcus Prochaska.
Transfer learning for neuronal networks deployed on the sensors edge.
In 2020 IEEE Sensors, pages 1–4. IEEE, 2020.

[11] Jinwoo Choi, Gaurav Sharma, Manmohan Chandraker, and Jia-Bin
Huang. Unsupervised and semi-supervised domain adaptation for action
In Proceedings of the IEEE/CVF Winter
recognition from drones.
Conference on Applications of Computer Vision, pages 1717–1726,
2020.

[12] Kshitij Bhardwaj and Maya Gokhale. Semi-supervised on-device neural
network adaptation for remote and portable laser-induced breakdown
spectroscopy. On-device Intelligence Workshop (MLSys Conference),
2021.

[13] Yufan He, Aaron Carass, Lianrui Zuo, Blake E Dewey, and Jerry L
Prince. Autoencoder based self-supervised test-time adaptation for
medical image analysis. Medical Image Analysis, page 102136, 2021.
[14] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann,
Wieland Brendel, and Matthias Bethge.
Improving robustness against
common corruptions by covariate shift adaptation. Advances in Neural
Information Processing Systems, 33, 2020.

[15] Terrance DeVries and Graham W Taylor.

tion of convolutional neural networks with cutout.
arXiv:1708.04552, 2017.

Improved regulariza-
arXiv preprint

10

