2
2
0
2

p
e
S
6

]

R
C
.
s
c
[

1
v
2
4
4
2
0
.
9
0
2
2
:
v
i
X
r
a

Fun2Vec:a Contrastive Learning Framework of
Function-level Representation for Binary

Sun RuiJina, Guo ShiZeb, Guo JinHongc, Sun Menga, Pan ZhiSonga

aArmy Engineering University of PLA
bNational Computer Network and Information Security Management Center
cShanghai Jiao Tong University

Abstract

Function-level binary code similarity detection is essential in the ﬁeld of cy-

berspace security.

It helps us ﬁnd bugs and detect patent infringements in

released software and plays a key role in the prevention of supply chain attacks.

A practical embedding learning framework relies on the robustness of vector

representation system of assembly code and the accuracy of the annotation of

function pairs. Supervised learning based methods are traditionally emploied.

But annotating diﬀerent function pairs with accurate labels is very diﬃcult.

These supervised learning methods are easily overtrained and suﬀer from vector

robustness issues. To mitigate these problems, we propose Fun2Vec: a con-

trastive learning framework of function-level representation for binary. We take

an unsupervised learning approach and formulate the binary code similarity

detection as instance discrimination. Fun2Vec works directly on disassembled

binary functions, and could be implemented with any encoder. It does not re-

quire manual labeled similar or dissimilar information. We use the compiler

optimization options and code obfuscation techniques to generate augmented

data. Our experimental results demonstrate that our method surpasses the

state-of-the-art in accuracy and have great advantage in few-shot settings.

Keywords: Binary code similarity detection, Contrastive Learning,

Embedding, Cyberspace security

(cid:63)Fully documented templates are available in the elsarticle package on CTAN.

Preprint submitted to Journal of LATEX Templates

September 7, 2022

 
 
 
 
 
 
2010 MSC: 00-01, 99-00

1. Introduction

Function-level binary code similarity detection (BCSD) is to analyze whether

two given binary code snippets are similar. It plays an important part in the

ﬁeld of cyberspace security. Speciﬁcally, it includes identifying encrypted soft-

ware, plagiarism detection, software property rights protection, vulnerability

discovery, and malicious code analysis. Binary code similarity analysis faces

many diﬃculties. First and foremost one is its enormous number. With the de-

velopment of the cyberspace, new functions and new applications emerge in an

endless stream, and the number of binary ﬁles increases rapidly. Additionally

to the number of new applications, the architectures of the devices are becom-

ing more and more diverse. In addition to the traditional x86 with Windows,

there are diﬀerent operating systems (Ubuntu, IOS, Android, and etc.) and

diﬀerent CPU processing types (arm, MIPS, and etc.). Developers often do not

write new softwares from scratch, they may reuse existing code snippets or open

source libraries. When there is a bug in the original library ﬁle, that bug will

be present in all programs that use this library. It is impossible to manually

identify the components of each piece of software in the face of massive data,

which urgently requires us to ﬁnd automated methods based on the semantics

of binary ﬁles.

Second, despite the high practical and academic value of binary ﬁle similarity

comparison, progress in this ﬁeld has been very slow because there is a lack of

quality labeled datasets in the BCSD ﬁeld. Annotation binary function pair is

very diﬃcult. Under most circumstances, security analysts cannot obtain the

source code of the analyzed software for various reasons. There are no high-level

language components such as variable names and data structures in the compiled

binary ﬁle, because the binary codes have lost a lot of program semantics during

the compilation process. Binaries generated from the same high-level language

can change signiﬁcantly when they are compiled by diﬀerent compilers, diﬀerent

2

compiler optimization options, and choosing diﬀerent CPU architectures. Many

hackers use obfuscation techniques to distort binary code, and it makes BCSD

more and more diﬃcult. Unlike labelling in NLP, labeling binary code segments

requires professionals.

Several recent studies [26][22][24][19][32] have presented promising results in

binary code similarity analysis. These methods suﬀer from one of these disad-

vantages. First, all these articles adapt supervised learning models [26] [24] [19]

[32], they all suﬀer from the labelling diﬃculties. Some distance functions are

used to measure the distance between diﬀerent functions such as cosine simi-

larity [24]. They mark the distance between the two similar functions as {1}

and the two diﬀerent functions as {−1}. But the mutual information between

any of the two functions could not be −1. For example, we may mark recv ()

and read () as {−1}, because they are not the same function. But in fact, these

two functions are very similar, the ground truth should be a score between −1

and 1. It is diﬃcult for us to ﬁnd the reasonable number to label each of the

diﬀerent function pairs. If we use {−1} to mark them, it can lead to overtrain.

Second, some models suﬀer the representation degeneration problem. A good

binary code similarity analysis framework relies on the robust vector represen-

tation of assembly code. Although these existing methods achieve SOTA results

in BCSD [26][22][35][19], we have found that the native representations derived

from the pretrained models are proved to be somehow collapsed. We use the

SVD to approximate the learned embeddings of TREX [26]. As shown in Fig-

ure 1, we plot the 2 rank approximation of embeddings of all the data set for

ﬁnetuning as mentioned in Section 4.1. And we can see that the representation

collapse to a corner.

Inspired by recent academic advances in the ﬁeld of contrastive learning [31]

[14][5][34], we introduce a novel function-level BCSD model to alleviate these

drawbacks and we name it Fun2Vec. It is a self-supervised framework based

on contrastive learning. Through the use of the contrastive learning module,

Fun2Vec only needs positive examples and sample augmentation techniques,

which avoids the over-training problem caused by the inaccurate labelling of

3

Figure 1: The 2D visulaization of embeddings of all data for ﬁnetuning from TREX [26](a

pretrained model of assembly code).

4

negative sample pairs. At the same time, the experimental results show that

our model can solve the anisotropy[12] caused by the pretrained model. Our

contributions can be summarized as follows:

• First, we develop multiple sets of metrics to examine the robustness of the

embeddings of existing SOTA models. We ﬁnd that the existing BCSD

models have problems of expression degradation. We analyze the reasons

for these problems.

• Second, we create Fun2Vec:a function-level embedding vector learning

model based on contrastive learning. This is the ﬁrst article I know of

that using the unsupervised learning model in related ﬁelds. Our model

avoids the problem of labelling diﬃculties.

• Third, Our model achieves the eﬀect of SOTA. We examine the model on

diﬀerent datasets. The results show that our model can outperform the

existing SOTA model. We also tested our model with a real vulnerability

dataset and our model achieved perfect results.

• Forth, our model is more eﬃcient in few-shot settings. With only 2,048

binary pairs we achieve better performance over TREX[26] a BERT base

model with about 89,809 labeled binary pairs.

• We hope our models will inspire others. We released the code of Fun2Vec

at https://github.com/fun2vec.

2. Background and Related Work

In this section, we brieﬂy introduce the deﬁnition of BCSD and the related

works.

2.1. Deﬁnition of BCSD

Software development is mostly not done from scratch. Since various source

codes are often reused during software development, there are a lot of clones

5

in the underlying assembly code. An eﬃcient BCSD engine can greatly reduce

the burden of the manual analysis process involved in reverse engineering. By

leveraging the large amount of binary data that exists, it can meet the infor-

mation needs of downstream tasks. The deﬁnition of binary code similarity

detection(BCSD) can be described as follows. When we are given two pieces

of binary code, without being given any other information such as source code

or compilation information, we can judge whether the two pieces of code are

similar. Here we refer to the similarity as semantic consistency. We mean that

the two pieces of code have the same semantics but not exactly the same char-

acter. If given the same input, two functions behave the same. We will consider

them as semantically similar binary functions regardless of whether they be-

long to the same architecture or are compiled from diﬀerent optimizations. In

practice, we use functions pairs that are compiled from the same source code

as similar. This saves a lot of manual labelling costs, and it is also a common

practice in academia [11] [26]. The BCSD methods can generally be divided

into two types. One is based on features, another is based on embeddings. We

will describe these two methods in details.

2.2. BCSD Based on Features

Traditional BCSD methods rely heavily on some speciﬁc features or struc-

tures. The earliest binary ﬁle similarity comparison tool is named EXEDIFF

[2], which was appeared in 1999, it is used to determine the patch information

of diﬀerent versions of executable ﬁles. They mainly use the disassembled code

structure information. String-based detection methods are the most basic ones

[3] [4]. They use the string or string edit distance to ﬁnd duplication. n-grams

is another frequently used feature [18]. They use the content in a sliding window

of size N in bytes. Many articles adopt the graph-based approach. Graph-based

methods are roughly divided into those that utilize PDG and those that utilize

CFG. A PDG (program dependence graph) is a graphic representation of the

data and control dependencies within a procedure [23]. It was used in program

slicing at ﬁrst [33]. More people use CFG(Control Flow Graph), such as [28]

6

and [27]. They may transform the binary code into CFGs, and use a graph

analysis algorithm to ﬁnd the similarity.

2.3. BCSD Based on Embeddings

Embedding is to convert discrete variables into high-dimensional continu-

ous space. In embeddings based solutions, people compare functions by trans-

forming the binary functions into multi-dimensional vector representations (em-

beddings) ﬁrst. Then they compare the two vectors through simple geometric

operations[24]. Functions with similar semantics have embeddings close to each

other in the high-dimensional space. Ding et al. [11] proposed a function em-

bedding solution called Asm2Vec. Asm2Vec changes the assembly code into

embeddings by Word2Vec[25]. Then they learn the function representations

based on the PV-DM model[20]. Some articles use the method of graph em-

bedding. [32] take a neural network-based approach to learn the embedding of

an ACFG(Attributed Control Flow Graph). SAFE [24] turns the instructions

into embeddings by using Word2Vec at ﬁrst. A GRU Recurrent Neural Network

(GRU RNN) is then used to capture the sequential interaction of the instruc-

tions.

With the success of pure attention structured networks in the NLP domain

such as Transformer [30] and BERT [10], more and more models adopt simi-

lar structures in BCSD. These articles [19] [22] [26][35] all adopt the structure

based on the pure attention structured pre-training model and achieve the very

outstanding eﬀect. Yu et al. [35] adopts a method based on BERT. Specially,

they use BERT to pre-train the binary code on the one-token level and one-

block level. Then they adopt convolutional neural networks (CNN) on adja-

cency matrices to extract the order information. PalmTree [22] utilizes three

pre-training tasks to capture various characteristics of assembly language and

generate general-purpose instruction embeddings by conducting self-supervised

training on large-scale unlabeled binary corpora. TREX [26] develops a novel

neural architecture, hierarchical Transformer, which can learn execution seman-

tics from micro-traces during the pretraining phase. But none of these articles

7

have studied the quality of embedding that they’ve learned.

3. The Basic Structure of the Model

In this section, we present Fun2Vec : a framework for contrastive learning

of binary function representations, and discuss the technical approach behind

our framework. We start by describing the general algorithm. Then, we brieﬂy

introduce several major parts of the model. Throughout this article, we use

the following mathematical notation. x refers to the element or binary function

in the original dataset X, f (·) is a DNNs based encoder. We mainly use a

BERT-like pretrained model TREX[26] as the encoder. We aim at ﬁnetuning

f (·) to make the function representation more robust. T rans(·) is the data

augmentation function. z is the learned embedding of the function x. τ is a

temperature parameter. d(·) is a distance function, (cid:96)(·) is a loss function.

3.1. General Framework

Inspired by the progress in the ﬁelds of contrastive learning and NLP, we

propose Fun2Vec to address the two challenges posed by the aforementioned

articles. Fun2Vec consists of a siamese network to implement the proposed

encoder and a contrastive learning network, as shown in Figure 2. With this

model, we can avoid labelling diﬀerent function pairs. We will drastically im-

prove the quality of the embedding. The model consists of the following three

parts.

• First, a dataset X and the sample augmentation module T rans(·).

• Second, the binary function representation learning module f (·).

• Third, the comparative learning module, which consists of a distance func-

tion d(·) and a loss function (cid:96)(·).

Given a function xi in the original dataset X, we could produce the augmentated

data by passing the original xi to the data augmentation module T rans(·). We

could get two augmented binary functions, x1

i and x2

i . Then, two embeddings

8

Figure 2: The framework of the proposed unsupervised learning method with siamese net-

works. The input function pairs are projected into low-dimensional normalized embedding

features with the DNN based encoder. Features of the same function instance with diﬀerent

data augmentations are invariant, while embedding features of diﬀerent function instances are

spread-out.

of x1

i and x2

i ) where f is the
representation learning module. Then we use the comparative learning module

i are produced, z1

i ) and z2

i = f (x1

i = f (x2

i and z2

i , z1

to optimize the encoder. Here we introduce the concept of non-parametric

instance discrimination [31]. We treat each function instance as a distinct class

of its own and train a classiﬁer to distinguish between individual instance classes.

That is given any function xi, xj. If xi and xj are semantically diﬀerent, we will

think they are diﬀerent classes and d(i, j) > threshold.

3.2. Data Augmentation Strategies

The role of the data augment module in contrastive learning is very critical.

In the realm of binary code, each assembly has a deﬁnite meaning. Given a

binary function code, we could produce diﬀerent binaries with the same seman-

tics. These binaries may suit diﬀerent architectures or be obfuscated or be com-

piled with diﬀerent optimizaitons. We explore three diﬀerent data augmentation

strategies, including cross-architecture, cross optimization and obfuscation.

cross-architecture. Generally, we randomly choose the CPU platform on

which the code runs. There are two main parameters, the architectures of CPU

and the width of the CPU. The architectures we support include x86-32, x86-64,

9

arm-32,arm-64, mips-32 and mips-64. Our model contains common mainstream

CPU types.

cross-optimization. The assembly codes obtained by diﬀerent compilation

options for the same function written in the same high-level language are also

very diﬀerent. Given a speciﬁc function, we will compile these functions into

diﬀerent architectures with 4 optimization options,i.e. ,O0, O1, O2 and O3.

obfuscation. Hackers often use code obfuscation to prevent security personnel

and tools from analyzing it. Hikari [37] contains several common obfuscation

techniques such as Pseudo Control Flow, Instruction Substitution, BasicBlock-

Spliting and etc. We use it to make obfuscated augmentation.

3.3. Function Representation Learning Module

In this section we brieﬂy introduce the binary function representation learn-

ing module. Our framework allows any network architecture for encoding. Given

a speciﬁc binary function pair, we can convert them into continuous vectors in

high dimensional space. Then according to the two vectors, the similarity of

the binary pair is judged. In this article, we encode the input functions using

a pretrained language model which is named TREX[26]. This is the existing

SOTA model. This model can transform each token in an assembly function

into embeddings. We use the average pooling of each token embedding at the

last layer to obtain the function representations.

3.4. Contrastive Learning Module

In this section, we describe in detail the contrastive learning module, which

consists of a distance function and a loss function. We use the cosine distance

function sim() to judge the similarity of function pairs. Given a function pair
x1 and x2, z1 = (cid:126)f (x1) and z2 = (cid:126)f (x2) are the embedding vectors produced by

the encoder. We could use cosine similarity as the distance metric d. The cosine

10

similarity formula is shown as follow:

(cid:62) (cid:126)f (x2)
(cid:126)f (x1)
(cid:13)
(cid:13)
(cid:13)
(cid:126)f (x2)
(cid:126)f (x1)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

=

(cid:113)

d(x1, x2) = sim(x1, x2) =

(cid:126)f (x2) [i]2
(1)
and (cid:126)f (x) [i] indicates the i − th component of the embedding vector of function

(cid:126)f (x1) [i]2 ·

i=1

i=1

(cid:80)d

(cid:80)d

(cid:80)d

i=1

(cid:16) (cid:126)f (x1) [i] ·
(cid:113)

(cid:17)
(cid:126)f (x2) [i]

x. The cosine distance between two function embeddings is [−1, 1]. Where 1

means exactly the same, -1 means completely diﬀerent.

When there are n diﬀerent functions, zi, i ∈ [1, n] is the learned embedded rep-

resentation of the i − th function. We enforce (cid:107)z(cid:107) = 1 via a L2 − normalization

layer. Inspired by article [31], we formulate the probability of any embedding

z belong to the i − th function as instance discimination objective using the

softmax-like criterion,

P (zi | z) =

exp (cid:0)zT
i z(cid:1)
j=1 exp (cid:0)zT

j z(cid:1)

(cid:80)n

(2)

, where P (zi | z) means the probability that z belongs to class i. We furtherly

introduce a temperature parameter τ that controls the concentration level [17].

The probability P (zi | z) becomes:

P (zi | z) =

exp (cid:0)zT
i z(cid:1) /τ
j=1 exp (cid:0)zT
We aim to maximiaze the joint probabilty (cid:81)n
minimize the negative log-likelihood − (cid:80)n

i=1 P (zi | z). It equivalently to
i=1 log P (zi | z). The loss function
becomes the NT-Xent (the normalized temperature-scaled cross entropy loss)

j z/τ (cid:1)

(cid:80)n

(3)

[5]. The goal of ﬁne-tuning is to minimize it.

During each train epoch, we select a speciﬁc function xi in the batch of N .

And we random make two augmented function x1

i of the same semantic,
resulting in 2N data points. Then the loss function for a positive pair of examples

i and x2

i and x2
x1

i is deﬁned as:

(cid:32)

ιxi = −1/2·

log

(cid:80)N

k=1

exp(sim(z1
L[k(cid:54)=i]exp(sim(z1

i , z2

i )/τ )

i , zk)/τ )

+ log

(cid:80)N

k=1

11

(cid:33)

exp(sim(z1
L[k(cid:54)=i]exp(sim(z2

i , z2

i )/τ )

i , zk)/τ )
(4)

The ﬁnal loss (cid:96) is computed across all positive pairs, (cid:96) = 1
N

k=1 ιxi. By mini-
mizing the loss function (cid:96), we can maximize the distance between the function

(cid:80)N

and its augmented data, and we can minimize the cosine distance between dif-

ferent functions. In this way, the goal of instance classiﬁcation can be achieved

without labelling samples of diﬀerent types. The overall process is shown in the

algorithm 1.

Algorithm 1 Framework of Contrastive learning for our system.

Input:

The set of positive samples for current batch, {xi}n

1 ; temperature constant
τ ; a DNNs structure of encoder f (); the data augmentaion function T rans();

Output:
1: for each xi ∈ sampled minibatch {xi}n

1 do

2:

3:

4:

5:

6:

7:

8:

Extracting the samples xi
i ,x2

initialize x1

encode them with z1

each functions

for
sim(zi, zj) = z(cid:62)
i zj
(cid:107)zi(cid:107)(cid:107)zj (cid:107)
loss
The

i with the augmentaion function x1
i ), z2
embeddings,

i = f (x2
i )

i = f (x1

calculate

i , x2

i = T rans(xi)

their

cosine distance,

function for a function pair

is deﬁned as

ιxi =

i ,z2

−log

exp(sim(z1
i )/τ )
L[k(cid:54)=i]exp(sim(z1

exp(sim(z2
i )/τ )
L[k(cid:54)=i]exp(sim(z2
i ,zk)/τ )
The ﬁnal loss is computed across all positive pairs,(cid:96) = 1
N

(cid:80)2N
k=1

(cid:80)2N
k=1

− log

i ,z1

i ,zk)/τ )
(cid:80)N

k=1 ιxi ,

update networks f to minimize (cid:96)

9: end for

10: return ﬁnetuned network f ;

4. Experiments and Evaluation

We ﬁrst introduce our dataset and experimental conﬁguration. Then our

evaluation aims to answer the following question:

RQ1. Basic statistical laws of assembly code. Is there a problem of expression

degradation in the results of the original pretrained model? Has the expression

12

degradation problem been alleviated after the ﬁnetuning?

RQ2. The accuracy of the model based on contrastive learning. Is the accuracy

of our model improved compared with the traditional SOTA model?

RQ3. Ablation Research. How do the diﬀerent parts contribute to improve the

performance of the model?

RQ4. Performance under Few-shot Settings.

RQ5. Temperature and batchsize. How do diﬀerent hyperparameters aﬀect the

performance of the model?

4.1. Setups

Setup. Our experimental environment is setup on a Linux server, running

Ubuntu 20.04, equipped with two Intel Xeon 4210r CPUs at 2.4Ghz, each CPU

has 10 cores, 20 virtual cores, 128G memory, and 1 Nvidia RTX 3090 GPU. The

software environment is Python 3.8 and PyTorch 1.11.0 with CUDA 11.7.

Hyperparameters. We implement our Fun2Vec model, based on TREX, a

BERT based pretrained model[26]. The TREX model provides a BERT-based

model that has been pretrained for 10 epochs. The total number of epochs we

will ﬁnetune the model is 40, the temperature is 0.07, and the dimension of

the output embedding is 768. The learning-rate is set to 0.00001, and the adm

optimizer is used. We ﬁx the largest input length to be 512. We train with

weight decay of 1e − 4 and we use a batch size of 32.

Metrics. We use the cosine similarity function to measure the distance between

the embedding vectors of two functions. When the distance between the function

pairs is less than a speciﬁc threshold, we will label that the two functions as

diﬀerent. But we will not use a speciﬁc threshold to judge the robustness of the

detection model. We use the AUC-ROC mechanism to evaluate the model. In

most cases, we will plot the Receiver Operating Characteristic (ROC) curve [36]

ﬁrst. The abscissa of the ROC curve is the false positive rate, and the ordinate

is the true positive rate. Then we measure the performance of the models in

this paper using the area under the plotted ROC curve, or AUC (Area Under

Curve) for short.

13

Figure 3: The frequency of the assembly code of the function conforms to the zipf distribution

4.2. RQ1: Statistical Laws of Assembly Code

The ﬁrst question we discuss is how the assembly code is distributed and

whether the models based on the pretrained models have the problem of ex-

pression collapse. Although multiple deep neural networks based encoders have

achieved very good results in binary code similarity detection, the performance

of these models may be questioned because of the complexity and uninter-

pretability of neural networks [1]. Articles such as [13][21] have pointed out

that the pre-trained models in NLP have the problem of expression collapse.

It stems from the uneven distribution of words. High-frequency words appear

more frequently, and low-frequency words appear less frequently, resulting in less

gradient descent for low-frequency words in the optimization process. High-

frequency words get more gradient descent. As a result, the embeddings of

high-frequency words appear to collapse into a corner.

In the assembly lan-

guage ﬁeld, we have observed a similar phenomenon, as shown in Figure 1. The

embeddings of assembly tokens appear to collapse into a corner too.

So we have a conjecture that the assembly code’s distribution is also close to

the zipf distribution. We counte the frequency of each assembly token of the

ﬁnetuning dataset in [26] which is about 300M, and made a picture as shown in

Figure 3. It can be seen that the word frequency of the assembly code conforms

to the zipf distribution, which will also lead to the problem of expression col-

14

Figure 4: (a) most pairs of functions of the same meaning achived a score around 0.6.(b):most

pairs of functions of diﬀerent meaning also achived a score around 0.6.

lapse in the TREX model. The vocabulary size across all architectures is 3,300

and only 866 words appear in our data set. And the token with the highest

frequency is comma, it accounts for 7.4% of the total frequency.

Since we have observed the collapse of the embedded representation at the word

level, we estimate that the representation at the function level is also aﬀected.

The most commonly used approach to derive ﬁxed size sentence embedding is

to average the BERT output layer [29]. Before we ﬁnetune the TREX model,

if we directly adopt average pooling of each token embeddings at the last layer

to obtain the function representations, this practice yields rather bad results.

Almost all pairs of functions achieved a similarity score of between 0.6 and

1.0, as illustrated in Figure 4. Both similar and dissimilar function pairs have

cosine distances of around 0.6. After ﬁnetuning the model with Fun2Vec, we

successfully remove the model’s anisotropy [12]. When we use average pooling

to obtain the function embeddings, we can eﬀectively distinguish similar and

dissimilar functions as shown in Figure 5. The cosine similarity of the same

semantic function pair is [0.3, 1], and that of dissimilar functions is [−1, 0.3].

4.3. RQ2:The Accuracy of The Model

In order to study the accuracy of the Fun2Vec model, we compare it with

diﬀerent SOTA models, and the results show that the Fun2Vec models can

15

Figure 5: After ﬁnetuning the model, we can use embeddings to eﬀectively distinguish similar

and dissimilar functions.

achieve the best results.

4.3.1. Compared with SAFE model

(a)

ROC

curves

on

(b)

ROC

curves

on

AMD64ARMOpenSSL Dataset

AMD64multipleCompilers Dataset

Figure 6: ROC curves on AMD64ARMOpenSSL Dataset and AMD64multipleCompilers

Dataset. The blue ones are the curves of Fun2Vec that outperform all the others.

We ﬁrst compare Fun2Vec with the SAFE on the dataset published by [24].

There are two datasets in this subsection. The ﬁrst one is named AMD64ARMOpenSSL

which contains 95,535 functions. All the data in AMD64ARMOpenSSL have

been compiled for x86-64 and ARM using gcc-5.4 with 4 optimization levels.

The dataset contains two open source software repositories: OpenSSL-1.0.1f

and OpenSSL-1.0.1u. Another dataset is AMD64multipleCompilers, which con-

16

tains 452,598 functions.

It comes from the following open source software:

binutils-2.30, ccv-0.7, coreutils-8.29, curl-7.61.0, gsl-2.5, libhttpd-2.0, openmpi-

3.1.1, openssl-1.1.1-pre8, and valgrind-3.13.0. The compilation has been done

using 3 diﬀerent compilers, clang-3.9, gcc-5.4, gcc-3.47 and 4 optimization lev-

els. For each dataset, we ran 2 experiments. In ﬁrst experiment, we trained

our model with the TREX data and test on SAFE data.

In second exper-

iment, we trained our Fun2Vec model with the AMD64ARMOpenSSL and

AMD64multipleCompilers. We compare the result with what SAFE and Gemini

[32] claimed. We ran experiments 2 times and average the results. The results

for the AMD64ARMOpenSSL case are in Figure 6 a. The mean testing AUC

scores of Fun2Vec is 0.999, the AUC scores of SAFE is 0.992. The results for

the AMD64multipleCompilers case are shown in Figure 6 b. The mean testing

AUC scores of Fun2Vec is 0.999, the AUC scores of SAFE is 0.990. We also list

the result of Gemini [32] on SAFE data [24].

Table 1: AUC socores on AMD64ARMOpenSSL and AMD64multipleCompilers.

model

AMD64ARMOpenSSL AMD64multipleCompilers

Fun2Vec trained on TREX data

Fun2Vec trained on SAFE data

SAFE trained on SAFE data

Gemini trained on SAFE data

0.982

0.999

0.992

0.948

0.987

0.999

0.990

0.932

4.3.2. Compared with TREX Model

TREX is the state-of-art model proposed in [26]. They use the hierarchical

Transformer to learn execution semantics from micro-traces during the pre-

training phase. Then they used a supervised embedding model–InferSent [8][26]

to ﬁnetune the pretrained model. Because TREX doesn’t share their ﬁnetuned

model. We train a new TREX model with the pretrained model and the dataset

they shared which contains 89, 809 labeled function pairs. We also ﬁnetune our

17

Fun2Vec model with the same dataset, but we only used 22, 453 positive exam-

ples in the dataset. We test the Fun2Vec with TREX’s test dataset. The test

dataset collect 13 popular open-source software projects. These projects include

Binutils-2.34, Coreutils-8.32, Curl-7.71.1, Diﬀutils-3.7, Findutils-4.7.0, GMP-

6.2.0, ImageMagick-7.0.10, Libmicrohttpd-0.9.71, LibTomCrypt-1.18.2, OpenSSL-

1.0.1f and OpenSSL-1.0.1u, PuTTy-0.74, SQLite-3.34.0, and Zlib-1.2.11. As

shown in Table 2, we prepare function pairs for each project (ﬁrst column) with

5 types of partitions. (1)ARCH: the function pairs have diﬀerent architectures

but the same optimizations without obfuscations (2nd column). (2)OPT: the

function pairs have diﬀerent optimizations but the same architectures without

obfuscations (3rd column).

(3)OBF: the function pairs have diﬀerent obfus-

cations with the same architectures (x64) and no optimization (4th column).

(4)ARCH+OPT: the function pairs have both diﬀerent architectures and op-

timizations without obfuscations (5th column).

(5)ARCH+OPT+OBF: the

function pairs can come from arbitrary architectures, optimizations, and ob-

fuscations (6th column). We compare Fun2Vec with TREX on the same test

datasets, results are shown in Table 2. On average, Fun2Vec achieves AUC score

better than 0.984, and makes perfect performance with 0.999 AUC score in case

of 3. In the case of 1, 2, 3, the results of Fun2Vec model and TREX are very

close. In the case of more complex samples, the performance of our model signif-

icantly exceeds that of TREX. It shows that our model has stronger analytical

ability for diﬃcult examples. The results show that our model comprehensively

surpasses the original model.

18

Table 2: Results on function pairs across architectures, optimizations, and obfuscations.

ARCH

OPT

OBF

ARCH+OPT

ARCH+OPT+OBF

TREX Fun2Vec TREX Fun2Vec TREX Fun2Vec TREX Fun2Vec TREX

Fun2Vec

compiler

Binutils

Coreutils

Curl

Diﬀutils

Gmp

Findutils

0.998

0.998

0.995

0.998

0.984

0.999

ImageMagick

0.984

Libmicrohttpd

0.996

LibTomCrypt

0.983

OpenSSL

PuTTy

SQLite

Zlib

Average

0.996

0.995

0.996

0.973

0.992

0.998

0.998

0.995

0.999

0.984

0.999

0.984

0.996

0.983

0.996

0.995

0.996

0.974

0.992

0.993

0.993

0.989

0.994

0.982

0.991

0.978

0.987

0.978

0.993

0.979

0.979

0.973

0.986

0.993

0.994

0.989

0.994

0.982

0.991

0.978

0.987

0.995

0.993

0.979

0.979

0.974

0.987

0.998

0.998

0.997

0.998

0.996

0.995

0.998

0.993

0.998

0.998

0.995

0.997

0.998

0.997

0.999

0.999

0.998

0.999

0.998

0.997

0.999

0.999

0.999

0.999

0.997

0.999

0.999

0.999

0.975

0.977

0.972

0.973

0.972

0.975

0.965

0.965

0.963

0.964

0.957

0.957

0.957

0.967

0.996

0.995

0.990

0.991

0.990

0.993

0.987

0.986

0.985

0.994

0.977

0.977

0.976

0.988

0.974

0.973

0.972

0.973

0.973

0.974

0.953

0.959

0.949

0.953

0.953

0.953

0.953

0.963

0.994

0.993

0.992

0.992

0.990

0.993

0.980

0.978

0.975

0.978

0.977

0.978

0.978

0.984

4.3.3. Vulnerability Search

Table 3: Basic information of the Vulnerability dataset

# Alias/Method

CVE/stats

functionname

number

1

2

3

4

Heartbleed

cve-2014-0160

tls1 process heartbeat

Venom

cve-2015-3456

fdctrl handle drive speciﬁcation command

WS-snmp

cve-2011-0444

snmp usm password to key sha1

wget

cve-2014-4877

ftp syst

8

6

7

5

To furtherly evaluate the performance of Fun2Vec, we try the model in a

real applicable scenario. In this task, we evaluate Fun2Vec to look up for vul-

nerable functions on a public available dataset [9]. The dataset contains several

vulnerable binaries compiled with 11 compilers in the families of clang, gcc and

icc[24]. We selecte some data whose length is less than 512. There are 2, 220

functions in total and 26 functions contain exploits. The dataset contains 4

diﬀerent vulnerabilities and we list the them in the Table 3. We disassemble

19

the dataset with objdump and turn the target functions into embeddings. We

compare the embedding with the functions that contain exploits. On the vul-

nerability search experiment, our model have a perfect performance. Fun2Vec

found all the 26 vulnerable functions in the top-k results. If the number of the

functions that contain the vulnerability is k, Fun2Vec found all the k vulnerable

functions in the ﬁrst k results in all the queries. For k = 10, SAFE [24] reach a

recall of 84%, while Gemini reaches a recall of 55%.

4.4. RQ3:Qualitative Analysis

In this section, we quantify the impact of diﬀerent parts of the Fun2Vec

model on the results. All reported results in this section are based on the

following settings. We train these models using the parameters described in

Section 4.1, and then test the model on 13 diﬀerent datasets. The datasets are

described in Section 4.3.2.

4.4.1. Projection Head

Several articles have pointed out that project head can improve the perfor-

mance of the model before it [7] [5] [15][6]. We study the inﬂuences of including

a projection head g(h). g(h) maps representations to the space where the con-

trastive loss is applied after the training process. After the ﬁnetuning phase, we

would throw away the projection head g(h). Figure 7 shows the diﬀerence be-

tween the model with and without the projection head. We use the embedding

z as the representation for downstream tasks.

20

(a) model without head

(b) model with head

Figure 7: Schematic diagram of the structure of the two models

Many articles [7] [5] [15][6] have used the contrastive learning model. Al-

though the role of projection head g(h) in other articles is obvious, the reason

for its eﬀect is debated. But in our model, it doesn’t work well after adding the

projection head. Figure 8 shows that the model’s AUC score rises a little (on

average 3%) when the model is trained with projection head g(h) compared to

the model without ﬁne-tuning. The AUC score of the model with projection

head is much lower than the model without the projection head. Chen et al.

[5] conjecture that the importance of using the representation before the non-

linear projection is due to loss of information induced by the contrastive loss.

One possible reason is that [5] contains millions of data, some information will

be lost. But in our experiments, the amount of data is relatively small (tens

of thousands). In the process of ﬁnetuning, we use the BERT based encoder.

Compared with ResNet-50 [5], BERT has better encoding ability. The infor-

mation lost by contrastive learning is relatively small and therefore could be

ignored.

21

Figure 8: Comparison of AUC scores on test dataset between models pretrained with and

without projection head.

4.4.2. Pretraining Eﬀectiveness.

To further investigate the impact of pre-training modules, we compared the

result achieved by removing the pre-training modules. After removing the at-

tention module, the model degenerates into a Word2Vec model. Figure 9 shows

that the model’s AUC score on diﬀerent dataset. It shows that the model’s AUC

score rises on average 10%. The results show that the role of the pre-training

module is critical.

22

Figure 9: Comparison of testing AUC scores between models pretrained with and without

pretraining modules.

4.4.3. Only Finetuning The Projection Head

Although the model with the addition of projection head could not achieve

good results, we also try diﬀerent network structures. We add a full connection

layer to the TREX model,and when ﬁnetuning, the pretrained model is ﬁxed.

We only ﬁnetune the project head. Figure 10 shows that with only ﬁnetuning

the projection head, AUC score drops a little.

23

Figure 10: Comparison of testing AUC scores between models only ﬁnetuning the projection

head and without projection head.

4.5. RQ4:Performance under Few-shot Settings

To validate the performance of Fun2Vec under the data scarcity scenarios,

we conduct the experiments under few-shot setting. We limit the number of

unlabeled function pairs to 2, 8, 32, 128, 512 and 2048 respectively. And we

compare their performance with the model trained with all data in dataset. We

list the average AUC scores below.

Table 4: Performance under Few-shot Settings

number

0

2

8

32

128

512

2048

all(22453)

AUC score

0.679

0.719

0.737

0.777

0.859

0.929

0.962

0.984

The results is presented in Table 4. Where the number of 0 means the orig-

inal model without ﬁne-tuning. The original model achieve 0.679 AUC score.

We could get 0.719 with only 2 function pairs. Our approach can make a dra-

matic improvement over the baseline with only 512 samples available. When

the training samples increase to 2048, our approach can basically achieve com-

parable results with the models trained on the full dataset. The results reveal

24

the eﬀectiveness of our approach under the few-shot settings. Our model could

keep robust, when data is scary, which is common in reality. We also compare

the results with TREX model ﬁne-tuned with InferSent model [26]. By using

TREX ﬁne-tuned with InferSent model, we can achieve a maximum AUC of

0.942 with 10,240 labeled data. The results show that by avoiding the over-

training problem caused by mislabelling, our model can achieve better training

results with less data.

4.6. RQ5: Temperature and Epoch

We further discuss the inﬂuence of parameters such as number of training

epochs and temperature. Since both our model and TREX use the same pre-

trained model, we only compare the time diﬀerence of ﬁnetuning stage. The

data for ﬁnetuning the TREX model contains about 89, 809 function pairs. The

operation of Fun2Vec model only requires 22, 453 functions. Although Fun2Vec

model requires fewer samples, it converges signiﬁcantly faster. After about 2

rounds, better accuracy can be obtained. As shown in the table, using the TREX

model, the convergence speed of the model is much slower. After 20 training ses-

sions, the average AUC is not as good as the 2-round training eﬀect of Fun2Vec.

Table 5: The Performance of Diﬀerent Training Epochs

number of epoch

2

10

20

40

auc score of TREX

0.679

0.719

0.737

0.984

auc score of Fun2Vec

0.917

0.989

0.990

0.992

We also test the importance of temperature τ in our default loss function.

The temperature τ in the loss (Equation 4) is used to control the smoothness

of the distribution. The smaller the τ is the more sharpen the distribution

will be. We list the AUC under diﬀerent temperature in Figure 11. The best

performance is achieved when the temperature is set to 0.07.

25

Figure 11: The inﬂuence of diﬀerent temperatures in the loss function. The best performance

is achieved when the temperature is set to 0.07.

5. Discussion

Inspired by the progress in the ﬁeld of NLP, many people [26][24] use related

technologies in NLP to deal with problems in the ﬁeld of assembly language.

Although many people use the BERT model to generate embeddings [26][22],

they all ignore the discussion of the robustness of function-level embeddings. In

this paper, we use contrastive learning to transform the function-level embed-

ding learning problem into an instance discrimination problem, We avoid the

problem of labelling diﬀerent function pairs and achieve SOTA. At the same

time, our contrastive learning model shows some characteristics diﬀerent from

images, such as very high accuracy without project heads which merits fur-

ther investigation. Other advances in comparative learning [16] are also worth

discussing in BCSD ﬁelds.

6. Conclusion

In this paper, we propose Fun2Vec, a self-supervised contrastive learning

framework for transferring binary functions into embeddings for downstream

tasks. The framework does not need any extra complex structure and could

be implemented with any encoder. We are the ﬁrst to adopt unsupervised

learning in BCSD ﬁelds. We demonstrate the eﬀectiveness of our framework on

various datasets, our methods could achieve new state-of-the-art performance.

26

Furthermore, our framework is robust in the few-shot setting. We hope our

work can inspire others.

References

[1] Dos and don’ts of machine learning in computer security.

In 31st

USENIX Security Symposium (USENIX Security 22), Boston, MA, August

2022. USENIX Association. URL https://www.usenix.org/conference/

usenixsecurity22/presentation/arp.

[2] Brenda S. Baker, Udi Manber, and Robert Muth. Compressing diﬀerences

of executable code. In In ACM SIGPLAN 1999 Workshop on Compiler

Support for System Software (WCSSS’99, 1999.

[3] B.S. Baker. On ﬁnding duplication and near-duplication in large software

systems. In Proceedings of 2nd Working Conference on Reverse Engineer-

ing, pages 86–95, 1995. doi: 10.1109/WCRE.1995.514697.

[4] Mikhail Bilenko and Raymond J. Mooney. Adaptive duplicate detection us-

ing learnable string similarity measures. In Proceedings of the Ninth ACM

SIGKDD International Conference on Knowledge Discovery and Data Min-

ing, KDD ’03, page 39–48, New York, NY, USA, 2003. Association for

Computing Machinery.

ISBN 1581137370. doi: 10.1145/956750.956759.

URL https://doi.org/10.1145/956750.956759.

[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton.

A simple framework for contrastive learning of visual representations. URL

http://arxiv.org/abs/2002.05709. Number: arXiv:2002.05709.

[6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and

Geoﬀrey E. Hinton. Big self-supervised models are strong semi-supervised

learners. CoRR, abs/2006.10029, 2020. URL https://arxiv.org/abs/

2006.10029.

27

[7] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He.

Improved

baselines with momentum contrastive learning. CoRR, abs/2003.04297,

2020. URL https://arxiv.org/abs/2003.04297.

[8] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine

Bordes. Supervised learning of universal sentence representations from nat-

ural language inference data. URL http://arxiv.org/abs/1705.02364.

Number: arXiv:1705.02364.

[9] Yaniv David, Nimrod Partush, and Eran Yahav. Statistical similarity of

binaries. page 15.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

BERT: Pre-training of deep bidirectional transformers for language un-

derstanding. In Proceedings of the 2019 Conference of the North American

Chapter of the Association for Computational Linguistics: Human Lan-

guage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186,

Minneapolis, Minnesota, June 2019. Association for Computational Lin-

guistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/

N19-1423.

[11] Steven H. H. Ding, Benjamin C. M. Fung, and Philippe Charland. Asm2vec:

Boosting static representation robustness for binary clone search against

code obfuscation and compiler optimization. In 2019 IEEE Symposium on

Security and Privacy (SP), pages 472–489. IEEE. ISBN 978-1-5386-6660-

9. doi: 10.1109/SP.2019.00003. URL https://ieeexplore.ieee.org/

document/8835340/.

[12] Kawin Ethayarajh. How contextual are contextualized word representa-

tions?

comparing the geometry of BERT, ELMo, and GPT-2 embed-

dings.

In Proceedings of the 2019 Conference on Empirical Methods in

Natural Language Processing and the 9th International Joint Conference

on Natural Language Processing (EMNLP-IJCNLP), pages 55–65. Associ-

28

ation for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL

https://www.aclweb.org/anthology/D19-1006.

[13] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. REPRE-

SENTATION DEGENERATION PROBLEM IN TRAINING NATURAL

LANGUAGE GENERATION MOD-. page 14, .

[14] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive

learning of sentence embeddings, . URL http://arxiv.org/abs/2104.

08821. Number: arXiv:2104.08821.

[15] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec,
Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo ´Avila

Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Ko-

ray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own la-

tent: A new approach to self-supervised learning. CoRR, abs/2006.07733,

2020. URL https://arxiv.org/abs/2006.07733.

[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Mo-

mentum contrast for unsupervised visual representation learning. page 10.

[17] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in

a neural network, 2015.

[18] Wei Ming Khoo, Alan Mycroft, and Ross Anderson. Rendezvous: A

search engine for binary code.

In 2013 10th Working Conference on

Mining Software Repositories (MSR), pages 329–338. IEEE.

ISBN 978-

1-4673-2936-1 978-1-4799-0345-0. doi: 10.1109/MSR.2013.6624046. URL

http://ieeexplore.ieee.org/document/6624046/.

[19] Hyungjoon Koo, Soyeon Park, Daejin Choi, and Taesoo Kim. Semantic-

aware binary code representation with BERT. CoRR, abs/2106.05478,

2021. URL https://arxiv.org/abs/2106.05478.

[20] Quoc Le and Tomas Mikolov. Distributed representations of sentences and

documents. page 9.

29

[21] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and

Lei Li. On the sentence embeddings from pre-trained language models.

In Proceedings of the 2020 Conference on Empirical Methods in Natural

Language Processing (EMNLP), pages 9119–9130. Association for Com-

putational Linguistics, . doi: 10.18653/v1/2020.emnlp-main.733. URL

https://www.aclweb.org/anthology/2020.emnlp-main.733.

[22] Xuezixiang Li, Qu Yu, and Heng Yin. PalmTree: Learning an assembly

language model for instruction embedding, . URL http://arxiv.org/

abs/2103.03809.

[23] Chao Liu, Chen Chen, Jiawei Han, and Philip S. Yu. Gplag: Detection of

software plagiarism by program dependence graph analysis. In In the Pro-

ceedings of the 12th ACM SIGKDD International Conference on Knowledge

Discovery and Data Mining (KDD’06, pages 872–881. ACM Press, 2006.

[24] Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Leonardo Quer-

zoni, and Roberto Baldoni. SAFE: Self-attentive function embeddings for

binary similarity. URL http://arxiv.org/abs/1811.05296. Number:

arXiv:1811.05296.

[25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀrey Dean.

Distributed representations of words and phrases and their compositional-

ity. In Proceedings of the 26th International Conference on Neural Informa-

tion Processing Systems - Volume 2, NIPS’13, page 3111–3119, Red Hook,

NY, USA, 2013. Curran Associates Inc.

[26] Kexin Pei, Junfeng Yang, Zhou Xuan, and Suman Jana. TREX: Learning

execution semantics from micro-traces for binary similarity. page 19.

[27] Jannik Pewny, Felix Schuster, Lukas Bernhard, Thorsten Holz, and Chris-

tian Rossow. Leveraging semantic signatures for bug search in binary pro-

grams. In Proceedings of the 30th Annual Computer Security Applications

30

Conference, ACSAC ’14, page 406–415, New York, NY, USA, 2014. As-

sociation for Computing Machinery. ISBN 9781450330053. doi: 10.1145/

2664243.2664269. URL https://doi.org/10.1145/2664243.2664269.

[28] Jannik Pewny, Behrad Garmany, Robert Gawlik, Christian Rossow, and

Thorsten Holz. Cross-architecture bug search in binary executables.

In

2015 IEEE Symposium on Security and Privacy, pages 709–724, 2015. doi:

10.1109/SP.2015.49.

[29] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings

using siamese BERT-networks. URL http://arxiv.org/abs/1908.10084.

Number: arXiv:1908.10084.

[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,

Aidan N. Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all

you need. In Proceedings of the 31st International Conference on Neural

Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY,

USA, 2017. Curran Associates Inc. ISBN 9781510860964.

[31] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsuper-

vised feature learning via non-parametric instance discrimination.

In

2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,

pages 3733–3742. IEEE.

ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.

2018.00393. URL https://ieeexplore.ieee.org/document/8578491/.

[32] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song.

Neural network-based graph embedding for cross-platform binary code sim-

ilarity detection. page 14.

[33] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. Modeling

and discovering vulnerabilities with code property graphs. In 2014 IEEE

Symposium on Security and Privacy, pages 590–604, 2014. doi: 10.1109/

SP.2014.44.

31

[34] Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and

Weiran Xu. ConSERT: A contrastive framework for self-supervised sen-

tence representation transfer. URL http://arxiv.org/abs/2105.11741.

Number: arXiv:2105.11741.

[35] Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu.

Order matters: Semantic-aware neural networks for binary code similar-

ity detection. 34(1):1145–1152.

ISSN 2374-3468, 2159-5399. doi: 10.

1609/aaai.v34i01.5466. URL https://aaai.org/ojs/index.php/AAAI/

article/view/5466.

[36] Feng Zhang, Ti Gong, Victor E. Lee, Gansen Zhao, Chunming Rong, and

Guangzhi Qu. Fast algorithms to evaluate collaborative ﬁltering recom-

mender systems. Knowledge-Based Systems, 96:96–103, 2016. ISSN 0950-

7051. doi: https://doi.org/10.1016/j.knosys.2015.12.025. URL https:

//www.sciencedirect.com/science/article/pii/S0950705115005079.

[37] Naville Zhang. Hikari – an improvement over obfuscator-llvm. URL https:

//github.com/HikariObfuscator/Hikari.

32

