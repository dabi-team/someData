2
2
0
2

g
u
A
9

]
h
p
-
c
c
a
.
s
c
i
s
y
h
p
[

2
v
2
8
3
2
0
.
8
0
2
2
:
v
i
X
r
a

NEXT GENERATION COMPUTATIONAL TOOLS FOR THE MODELING
AND DESIGN OF PARTICLE ACCELERATORS AT EXASCALE‚àó

A. Huebl‚Ä†, R. Lehe, C. E. Mitchell, J. Qiang, R. D. Ryne, R. T. Sandberg and J.-L. Vay
Lawrence Berkeley National Laboratory, 94720 Berkeley CA, USA

Abstract

Particle accelerators are among the largest, most com-
plex devices. To meet the challenges of increasing en-
ergy, intensity, accuracy, compactness, complexity and ef-
Ô¨Åciency, increasingly sophisticated computational tools are
required for their design and optimization. It is key that con-
temporary software take advantage of the latest advances
in computer hardware and scientiÔ¨Åc software engineering
practices, delivering speed, reproducibility and feature com-
posability for the aforementioned challenges. A new open
source software stack is being developed at the heart of
the Beam pLasma Accelerator Simulation Toolkit (BLAST)
by LBNL and collaborators, providing new particle-in-cell
modeling codes capable of exploiting the power of GPUs
on Exascale supercomputers. Combined with advanced nu-
merical techniques, such as mesh-reÔ¨Ånement, and intrinsic
support for machine learning, these codes are primed to pro-
vide ultrafast to ultraprecise modeling for future accelerator
design and operations.

INTRODUCTION

Large-scale computer simulations of charged particle mo-
tion inside of particle accelerators play a crucial role in ac-
celerator design and operation. In order to quickly simu-
late charged particle dynamics, including collective eÔ¨Äects,
advanced software must be developed to take advantage of
state-of-the-art computer hardware.

With the onset of the Exascale supercomputing era, om-
nipresent GPU-accelerated machines require multi-level
parallelism, multi-paradigm programming and dynamic
load balancing. The U.S. Department of Energy Exascale
Computing project addressed this need by co-developing
applications and software for Exascale computing acqui-
sitions. The laser-plasma modeling code WarpX [1], e.g.,
used in plasma-based particle acceleration, is a result of this
project and recently provided the Ô¨Årst full-scale runs at the
Ô¨Årst demonstrated Exascale machine, Frontier [2].

‚àó Work supported by the Exascale Computing Project (17-SC-20-SC), a
joint project of the U.S. Department of Energy‚Äôs OÔ¨Éce of Science and
National Nuclear Security Administration, responsible for delivering a
capable exascale ecosystem, including software, applications, and hard-
ware technology, to support the nation‚Äôs exascale computing imperative.
This work was supported by the Laboratory Directed Research and De-
velopment Program of Lawrence Berkeley National Laboratory under
U.S. Department of Energy Contract No. DE-AC02-05CH11231. This
research used resources of the National Energy Research ScientiÔ¨Åc Com-
puting Center (NERSC), a U.S. Department of Energy OÔ¨Éce of Science
User Facility located at Lawrence Berkeley National Laboratory, oper-
ated under Contract No. DE-AC02-05CH11231 using NERSC award
HEP-ERCAP0020744.

‚Ä† axelhuebl@lbl.gov

Beam, Plasma & Accelerator Simulation Toolkit

WarpX is a code in the Beam, Plasma & Accelerator Simu-
lation Toolkit (BLAST, https://blast.lbl.gov), a suite
of open source particle accelerator modeling codes. Origi-
nally developed under the name Berkeley Lab Accelerator
Simulation Toolkit, included codes achieved compatibility
through a common meta-data standard in I/O, openPMD [3],
yet were implemented in disjoint code bases. BLAST has
been renamed in 2021 to reÔ¨Çect international contributions
from LIDYL (CEA, France), SLAC (USA), LLNL (USA),
DESY (Germany), UHH (Germany), HZDR (Germany),
Radiasoft (USA), CERN (Switzerland) and more; BLAST
development involves deep collaboration among physicists,
applied mathematicians, and computer scientists.

With the emergence of the Ô¨Årst Exascale Computing
supercomputers, modeling codes that were originally de-
signed for parallel CPU-powered machines need to undergo
a fundamental modernization eÔ¨Äort. This became neces-
sary, as compute nodes are now equipped with accelerator
hardware such as GPUs (and potentially FPGAs in the fu-
ture). Selected as application for the Department of Energy
Exascale Computing Project, the BLAST code WARP [4, 5]
underwent a complete rewrite from Fortran to modern C++
resulting in its successor WarpX [1]. Building on the mo-
mentum of this transition to form a more cohesive Acceler-
ator Toolkit, the specialized plasma wakeÔ¨Åeld acceleration
code HiPACE++ [6] and beam dynamics code ImpactX [7],
presented herein, are developed.

Software Design

A central goal of the modernization of BLAST is modu-
larity for eÔ¨Écient code reuse and tight integration for cou-
pling, i.e., in hybrid particle accelerators with conventional
and advanced (plasma) elements. Figure 1 shows the design
of BLAST‚Äôs software dependencies, with upper components
depending and sharing lower blocks in the schema. Shared
code, common application programming interfaces (APIs)
and data standards ensure composability and connection
to the AI/ML and data science ecosystems. Performance-
critical routines are implemented and reused in modern
C++, using a single-source approach to program both CPUs
and GPUs via a performance-portability layer in AMReX [8].
The newly introduced ABLASTR library collects common
particle-in-cell (PIC) routines.

Python high-level interfaces are used for user eÔ¨Éciency
and to provide standardized APIs to data science and
AI/ML frameworks, which are mostly driven from the
same language. Documentation and examples are devel-
oped in lock-step with documentation and published on

 
 
 
 
 
 
Figure 1: Design of the BLAST software stack. Modularization
enables code sharing and tight coupling.

1 from impactx import ImpactX, RefPart, \

2

3

distribution, elements

4 sim = ImpactX() # simulation object

5

6 # set numerical parameters and IO control
7 sim.set_particle_shape(2) # B-spline order
8 sim.set_slice_step_diagnostics(True)
9 sim.set_space_charge(False)

10

11 # domain decomposition & space charge mesh
12 sim.init_grids()

https://impactx.readthedocs.io. Examples and test
cases are continuously run against expected results.

All development is carried out in the open using open
source licenses, contributable code repositories, code re-
views, regular releases and change logs [7]. The commu-
nity reports open ‚Äúissues‚Äù for feature requests, bug reports,
etc. Installation for users and developers is supported by
package managers and HPC modules.

13

14 # load a 2 GeV electron beam with an initial
15 # unnormalized rms emittance of 2 nm
16 energy_MeV = 2.0e3 # reference energy
17 charge_C = 1.0e-9 # used with space charge
18 mass_MeV = 0.510998950 # mass
19 qm_qeeV = -1.0e-6/mass_MeV # charge/mass
20 npart = 10000 # number of macro particles

IMPACTX

ImpactX is an ùë†-based beam dynamics code. Leveraging
expertise and models in IMPACT-Z [9] and MaryLie [10,
11], this new simulation code is built from the ground up
to take GPU-accelerated computing, mesh-reÔ¨Ånement for
space-charge eÔ¨Äects, load balancing, and coupling AI/ML
frameworks into consideration. ImpactX design relies on
open community standards for I/O and data interfaces.

As of version 22.08, many features are still under active
development. Generalized and reused from WarpX via the
ABLASTR library are GPU-accelerated routines for charge
deposition, beam statistics, Poisson solve, proÔ¨Åling, warn-
ing logging, Unix signal handling, build/installation logic,
among others.

Model Assumptions

Tracking through lattice elements is performed by push-
ing particles in ùë† using a symplectic map. Currently, each
map applied during tracking is accurate through linear or-
der (with respect to the reference orbit). The linearization
implies that, when solving for space-charge eÔ¨Äects, we as-
sume that the relative spread of velocities of particles in the
beam is negligible compared to the velocity of the reference
particle.

The space charge Ô¨Åelds are treated as electrostatic in the
bunch rest frame. In particular, no retardation or radiation
eÔ¨Äects are included, and we solve the Poisson equation in
the bunch rest frame. The eÔ¨Äect of space charge is included
in tracking using a map-based split-operator approach [9].

Usage Example

21

22 distr = distribution.Waterbag(

23

24

25

26

27

28

29

30

31

sigmaX = 3.9984884770e-5,
sigmaY = 3.9984884770e-5,
sigmaT = 1.0e-3,
sigmaPx = 2.6623538760e-5,
sigmaPy = 2.6623538760e-5,
sigmaPt = 2.0e-3,
muxpx = -0.846574929020762,
muypy = 0.846574929020762,
mutpt = 0.0)

32 sim.add_particles(

33

qm_qeeV, charge_C, distr, npart)

34
35 # set the energy in the reference particle
36 sim.particle_container().ref_particle() \
.set_energy_MeV(energy_MeV, mass_MeV)

37

38
39 # design the accelerator lattice
40 ns = 25
41 fodo = [

# steps slicing through ds

42

43

44

45

elements.Drift(ds=0.25, nslice=ns),
elements.Quad(ds=1.0, k=1.0, nslice=ns),
elements.Drift(ds=0.5, nslice=ns),
elements.Quad(ds=1.0, k=-1.0, nslice=ns),
elements.Drift(ds=0.25, nslice=ns)

46
47 ]
48 # assign a fodo segment
49 sim.lattice.extend(fodo)

50
51 # run simulation
52 sim.evolve()

ImpactX can be executed in two ways: a traditional ex-
ecutable reading a textual input Ô¨Åle or driven from Python.
The latter approach is compact and expressive for

Listing 1: An example Python script FODO.py that can be
used to design an FODO cell setup with ImpactX v22.08.
This script can equally drive CPU and GPU simulations and
executes over multiple nodes if started with MPI.

simulation design, since Python‚Äôs well-known syntax can
be used to design complex beamlines with lines and seg-
ments. From a performance viewpoint, both are equiva-
lent since computational C++ kernels are compiled and just
wrapped from Python. Listing 1 shows such a script-driven
simulation.

Furthermore,

such scripts can be extended since
ImpactX exposes the underlying AMReX data structures to
Python via pyAMReX, implementing AMReX data storage
for zero-copy access via a standardized Array Interface [12].
For instance, beam particles and generated Ô¨Åelds can be ma-
nipulated and analyzed in memory, combined with solvers
from other BLAST or third party software packages and addi-
tional computational routines can be added, even with GPU
support, using packages such as cupy or numba.

NUMERICAL EXPERIMENTS

Correctness tests are performed continuously, for every
code change, on ImpactX. The goal of these tests is to
cover all implemented functionality and verify that com-
puted results are within expected precision, independently
of the compute hardware used. Following such test-driven
development eases the entry burden for accelerator scien-
tists adding new functionality to the project, since auto-
mated testing will inform them if unexpected side-eÔ¨Äects of
changed code would change benchmarked physics results.
Tests and examples also add a solid body of documented
examples to the project.

As of version 22.08, the following benchmarks and ex-
amples are implemented: a FODO cell, a magnetic bunch
compression chicane [13], a stationary beam in a constant
focusing channel, a Kurth-distribution beam in a periodic
isotropic focusing channel [14], a stable FODO cell with
short RF (buncher) cavities added for longitudinal focus-
ing [15], a chain of thin multipoles, a nonlinear focusing
channel based on the IOTA nonlinear lens, and a model of
the Fermilab IOTA storage ring (linear optics) [16]. De-
tailed numerical parameters are archived online [7, 17].

e
z
i
s
m
a
e
b

]

m
m

[

0.08

0.06

œÉ

x

œÉ

y

0

1

2

3

z

 [m]

Figure 2: Evolution of the horizontal and vertical rms beam
sizes in the FODO benchmark.

]
d
a
r
m

[

x
p

]
d
a
r
m

[

y
p

= 0.0

1.25

1.75

2.75

3.0 [m]

z

0.1

‚àí0.1

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

x

x

x

x

x

 [mm]

 [mm]

 [mm]

 [mm]

 [mm]

0.1

‚àí0.1

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

‚àí0.2

0.2

y

y

y

y

y

 [mm]

 [mm]

 [mm]

 [mm]

 [mm]

Figure 3: Transverse phase space projections in the FODO
cell benchmark.

Benchmark: Berlin-Zeuthen Chicane

This benchmark is a simple, unshielded four-bend chi-
cane with parameters similar to the ones required for
the compression stages at LCLS (at 5 GeV) or TESLA
XFEL (at 500 MeV) [13]. Here, a 5 GeV electron bunch
with normalized transverse rms emittance of 1 ¬µm is used.
Figure 4 shows the longitudinal beam size and transverse
emittance evolution. The former is compressed 10√ó, while
the initial emittance is recovered at the end of transport. The
longitudinal phase space evolution during the transport is
shown in Fig. 5.

e
z
i
s
m
a
e
b

0.2

]

m
m

[

20

œÉ

t

Œµ

x

e
c
n
a
t
t
i

m
e

]

m
n
[

0.0

0

0

2

4

6

8

10

12

14

z

 [m]

Figure 4: Evolution of longitudinal beam size (rms) and
transverse emittance in the chicane benchmark.

Benchmark: FODO Cell

z

= 0.0

6.0

7.5

15.0 [m]

3
‚àí

]

0
1
[

t

p

10

0

‚àí10

In this benchmark, a stable FODO lattice with a zero-
current phase advance of 67.8 degrees per cell is modeled.
An rms-matched 2 GeV electron beam with initial unnor-
malized rms emittance of 2 nm propagates through a single
cell, with the beam size evolution as shown in Fig. 2. In
Fig. 3, the evolution of the transverse phase space is shown.
The benchmark veriÔ¨Åes for every code change that the beam
second moments remain matched and that the emittances re-
main constant.

‚àí0.5 0.0 0.5

‚àí0.5 0.0 0.5

‚àí0.5 0.0 0.5

‚àí0.5 0.0 0.5

ct

 [mm]

ct

 [mm]

ct

 [mm]

ct

 [mm]

Figure 5: Evolution of the longitudinal phase space in the
chicane benchmark, illustrating 10√ó compression.

Benchmark: Fermilab IOTA Storage Ring

This benchmark is a model of the bare (linear) lattice
of the Fermilab IOTA storage ring (v. 8.4) [16], with op-

 
 
 
 
 
tics conÔ¨Ågured for operation with a 2.5 MeV proton beam.
An rms-matched proton beam with an unnormalized emit-
tance of 4.5 ¬µm propagates over a single turn. The second
moments of the particle distribution after a single turn are
checked to coincide with the initial second moments of the
particle distribution, to within the level expected due to nu-
merical particle noise.

In Fig. 6, the reference orbit indicating the global beam
position within the ring is shown. Figure 7 shows the rms
beam size evolution as a function of path length over a sin-
gle turn. The thin dark lines are from ImpactX, while the
light bold lines in the background are from IMPACT-Z. The
results of the two codes are in excellent agreement.

]

m

[

x

0

‚àí4

‚àí8

‚àí8

‚àí6

‚àí4

‚àí2

0

2

4

6

8

z

 [m]

Figure 6: The reference orbit in the IOTA lattice bench-
mark, as produced by ImpactX, showing the storage ring
Ô¨Çoorplan.

e
z
i
s
m
a
e
b

5

]

m
m

[

0

œÉ

x

œÉ

y

0

4

8

12

16

20

24

28

32

36

40

distance 

 [m]

s

Figure 7: Evolution of the rms beam size in the IOTA lat-
tice benchmark. Thin dark lines: ImpactX, light bold lines:
IMPACT-Z results.

Performance

Although ImpactX in version 22.08 is in an early de-
velopment state, initial performance comparisons can be
made between conventional (CPU) and accelerated com-
pute hardware (GPU). In the following, the above IOTA lat-
tice benchmark is used as a computing benchmark (without
I/O or space charge solvers) to measure the performance of
ImpactX on a node of the NERSC Perlmutter Phase 2 super-
computer. Prototypical for future large-scale simulations,
but not strictly needed for this benchmark to converge, 108
beam macro particles are used.

Figure 8 shows the ImpactX strong-scaling speedup in
time-to-solution relative to a 16 CPU core run (146 sec).
Compilation uses -O3 and fast-math enabled; compilers are
GNU 11.2.0 and NVCC 11.7.64, respectively. Values above
1.0 are faster, below are slower. Dynamic load-balancing is
not yet used.

p
u
d
e
e
p
s

8

6

4

2

0

6.3x

3.6x

0.2x

0.8x

1.0x

1.1x

1 core

8 cores

16 cores 32 cores

1 A100

2 A100

Figure 8: Relative performance between CPU and GPU
runs on Perlmutter Phase 2 (NERSC). 16 CPU cores (w/o
hyperthreading) and one A100 GPU are each 1/4th of a
node‚Äôs resources: one AMD EPYC 7763 (Milan) proces-
sor and four NVIDIA Ampere A100 SXM2 (40 GB) GPUs.

Data Availability

Simulation code and documentation are openly devel-
oped in Ref. [7]. Numerical experiments and performance
results are archived in Ref. [17].

CONCLUSION

This paper presents computational tools for the model-
ing and design of particle accelerators, readying codes up
for next generation machines in the Exascale era. The open
source software toolkit BLAST provides modeling tools to
model hybrid accelerators, containing both plasma and con-
ventional beamline elements. ABLASTR is a modern C++17
library used to share particle-in-cell routines between sim-
ulation codes. Based on this, ImpactX is developed to suc-
ceed IMPACT-Z as a new, ùë†-based beam dynamics code with
intrinsic GPU, mesh-reÔ¨Ånement and tight coupling to time-
based codes and AI/ML capabilities.

ImpactX is in an early state, but already able to model
signiÔ¨Åcantly larger particle ensembles than its predecessor
codes. Ongoing and future developments will add capabil-
ities for: space charge eÔ¨Äects, scalable I/O, non-linear el-
ements, RF-modeling, wakeÔ¨Åeld eÔ¨Äects, surface methods
and support to read accelerator lattices from MAD-X Ô¨Åles,
among others.

ACKNOWLEDGEMENTS

We acknowledge the community of contributors to
the open source projects ABLASTR, AMReX, ImpactX,
IMPACT-Z, openPMD, pybind11 and WarpX.

REFERENCES

[1] A. Myers et al, ‚ÄúPorting WarpX to GPU-accelerated plat-
forms‚Äù, J. Parallel Comput., vol. 108, p. 102833, 2021.
doi:10.1016/j.parco.2021.102833

[2] L. Fedeli, A. Huebl, et al., ‚ÄúPushing the frontier in the de-
sign of laser-based electron accelerators with groundbreak-
ing mesh-reÔ¨Åned Particle-In-Cell simulations on Exascale-
class supercomputers‚Äù, SC‚Äô22: Proceedings of the Interna-
tional Conference on High Performance Computing, Net-
working, Storage and Analysis, to be published.

[3] A. Huebl et al, ‚ÄúopenPMD 1.0.0: A meta data standard for

particle and mesh based data‚Äù, 2015.
doi:10.5281/zenodo.33624

 
 
[4] J.-L. Vay, D. P. Grote, R. H. Cohen, and A. Friedman, ‚ÄúNovel
methods in the Particle-In-Cell accelerator Code-Framework
Warp‚Äù, Comput. Sci. & Disc., vol. 5, p. 014019, 2012.
doi:10.1088/1749-4699/5/1/014019

[11] R. D. Ryne et al, ‚ÄúRecent Progress on the MaryLie/IMPACT
ICAP‚Äô06, Cha-
Beam Dynamics Code‚Äù,
monix, France, 2006, paper TUAPMP03, pp. 157‚Äì159.
https://accelconf.web.cern.ch/icap06/papers/tuapmp03.pdf

in Proc.

[5] A. Friedman, R. H. Cohen, D. P. Grote, S. M. Lund, W. M.
Sharp, J.-L. Vay, I. Haber, Irving and R. A. Kishek, ‚ÄúCompu-
tational Methods in the Warp Code Framework for Kinetic
Simulations of Particle Beams and Plasmas‚Äù, IEEE Trans.
Plasma Sci., vol. 42, pp. 1321-1334, 2014.
doi:10.1109/TPS.2014.2308546

[6] S. Diederichs et al, ‚ÄúHiPACE++: A portable, 3D quasi-static
particle-in-cell code‚Äù, Computer Physics Communications,
vol. 278, p. 108421, 2022.
doi:10.1016/j.cpc.2022.108421

[7] A. Huebl, C. E. Mitchell, R. Lehe, J. Qiang, et al, ‚ÄúECP-

WarpX/impactx: 22.08‚Äù, 2022.
doi:10.5281/zenodo.6954923

[8] W. Zhang et al, ‚ÄúAMReX: a framework for block-
of
structured
Open Source Software, vol. 4, no. 37, p. 1370, 2019.
doi:10.21105/joss.01370

adaptive mesh

reÔ¨Ånement‚Äù,

Journal

[9] J. Qiang, R. Ryne, S. Habib, and V. Decyk, ‚ÄúAn Object-
Oriented Parallel Particle-in-Cell Code for Beam Dynam-
ics Simulation in Linear Accelerators‚Äù, J. Comput. Phys.,
vol. 163, pp. 434-451, 2000.
doi:10.1006/jcph.2000.6570

[10] A. Dragt et al, ‚ÄúMARYLIE 3.0 User‚Äôs Manual‚Äù, Department
of Physics and Astronomy, University of Maryland, College
Park, MD; https://www.physics.umd.edu/dsat/

[12] Consortium for Python Data API Standards, array interface,

2021, https://data-apis.org/array-api

[13] Berlin-Zeuthen

benchmark

chicane,

https://www.desy.de/csr/

[14] C.E. Mitchell, K. Hwang, and R.D. Ryne, ‚ÄúKurth Vlasov-
Poisson Solution for a Beam in the Presence of Time-
Dependent Isotropic Focusing‚Äù, in Proc. IPAC‚Äô21, Camp-
inas, SP, Brazil, May 2021, pp. 3213‚Äì3216.
doi:10.18429/JACoW-IPAC2021-WEPAB248

[15] R. D. Ryne et al, ‚ÄúA Test Suite of Space-charge Problems
in Proc. EPAC‚Äô04, Lucerne,
for Code Benchmarking‚Äù,
Switzerland, 2004, paper WEPLT047, pp. 1942-1945.
https://accelconf.web.cern.ch/e04/papers/weplt047.pdf

[16] S. Antipov et al, ‚ÄúIOTA (Integrable Optics Test Accelerator):
facility and experimental beam physics program‚Äù, JINST,
vol. 12, p. T03002, 2017.
doi:10.1088/1748-0221/12/03/T03002

[17] A. Huebl, C. E. Mitchell, et al, ‚ÄúSupplementary Ma-
terials: Next Generation Computational Tools for the
Modeling and Design of Particle Accelerators at Exas-
cale‚Äù, NAPAC‚Äô22 data artifact for paper TUYE2, 2022.
doi:10.5281/zenodo.6954898

