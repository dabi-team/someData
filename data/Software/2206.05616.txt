2
2
0
2

n
u
J

1
1

]
E
S
.
s
c
[

1
v
6
1
6
5
0
.
6
0
2
2
:
v
i
X
r
a

Is Kernel Code Different From Non-Kernel Code?
A Case Study of BSD Family Operating Systems

Gunnar Kudrjavets∗, Jeff Thomas†, Nachiappan Nagappan†, Ayushi Rastogi∗
∗Bernoulli Institute
University of Groningen
9712 CP Groningen, Netherlands
Email: g.kudrjavets@rug.nl, a.rastogi@rug.nl
†Meta Platforms, Inc.
1 Hacker Way
Menlo Park, CA 94025, USA
Email: jeffdthomas@fb.com, nnachi@fb.com

Abstract—Code churn and code velocity describe the evolution
of a code base. Current research quantiﬁes and studies code
churn and velocity at a high level of abstraction, often at the
overall project level or even at the level of an entire company. We
argue that such an approach ignores noticeable differences among
the subsystems of large projects. We conducted an exploratory
study on four BSD family operating systems: DragonFlyBSD,
FreeBSD, NetBSD, and OpenBSD. We mine 797,879 commits
to characterize code churn in terms of the annual growth rate,
commit types, change type ratio, and size taxonomy of commits
for different subsystems (kernel, non-kernel, and mixed). We
also investigate differences among various code review periods,
i.e., time-to-ﬁrst-response, time-to-accept, and time-to-merge, as
indicators of code velocity.

Our study provides empirical evidence that quantiﬁable evo-
lutionary code characteristics at a global system scope fail to
take into account signiﬁcant individual differences that exist at a
subsystem level. We found that while there exist similarities in the
code base growth rate and distribution of commit types (neutral,
additive, and subtractive) across BSD subsystems,
(a) most
commits contain kernel or non-kernel code exclusively, (b) kernel
commits are larger than non-kernel commits, and (c) code reviews
for kernel code take longer than non-kernel code.

Index Terms—BSD, code churn, code velocity, kernel code,

non-kernel code

I. INTRODUCTION

A widely accepted deﬁnition for software evolution is a
continual change from a lesser or worse state to a higher
or better state [1], [2]. The essence of this change is a
“process by which programs are modiﬁed and adapted to
their changing environment” [3]. At the lowest implementation
level, evolution of software depends on modiﬁcations to lines
of code (LOC) [7]. We use code churn [5] to quantify software
evolution [6].

Engineers identify code changes differently depending on
historical conventions, a speciﬁc project, or a code collabo-
ration tool. Developers can refer to proposed code changes
as diffs, patches, or pull requests. In this paper, the unit of
analysis for code churn is a commit. Developers use Modern
Code Review to validate the correctness of proposed code
changes [9]. As a result, the frequency and speed of software

evolution depend on how fast the proposed code changes are
reviewed and merged to the target branch.

Existing research about the size-related characteristics of
code churn (e.g., size of commits) [8], [9], [10], [8], [11],
[12] and code reviews [13], [14], [15], [16], [17], [18] fo-
cuses either at the level of an entire project or a company.
For example, the role of patch review in software evolution
was investigated in the context of an entire Mozilla Firefox
project [4].

Given the large number of engineers working on more sig-
niﬁcant software projects, our exploratory case study proposes
there exist many subsystems with potentially different
that
subcultures in the context of one large system. A subsystem
can be one of the many products a company such as Google or
Microsoft is developing or even a different architectural layer
of the Linux kernel. Code churn patterns, commit sizes, and
code velocity may differ between those subsystems. Viewing
the entire system as a monolithic entity may lead to incor-
rect conclusions about the characteristics of a system. For
example, statements like “the median code review size for X
is N” is overly simplistic, omitting the potentially signiﬁcant
differences between either various products developed by the
same company or the components of a more extensive system.
We set forward to ﬁnd out if there is empirical evidence
to show that code churn and code velocity differ between
various subsystems. We investigate various characteristics of
code churn, such as commit size, commit type, and the ratio
of different types of code changes. To quantify the speed at
which a system evolves, we measure the various code review
intervals: time-to-ﬁrst-response, time-to-accept, and time-to-
merge.

The unit of analysis in this paper is a speciﬁc type of
large software system—operating systems. We divide the
operating system code into two categories: kernel and non-
kernel code (see Section II-C for terminology and rationale).
This categorization is just one of the possible ways to split
the code. There are many other options to classify various
operating system components. For example, we can divide an
operating system into subsystems such as a ﬁle system, I/O

 
 
 
 
 
 
manager, memory manager, or user mode utilities.

We did not ﬁnd any existing studies comparing code
churn and code review velocity between kernel and non-
kernel code in the context of the same operating system.
Therefore, we conduct an exploratory study on a subset of
BSD family operating systems (DragonFlyBSD, FreeBSD,
NetBSD, and OpenBSD) to understand if there is any validity
to our intuition. The need to analyze a complete operating
system that contains everything from various command-line
utilities to kernel and windowing systems dictates the choice of
various BSD versions. For example, hundreds of distributions
include the Linux kernel. Because of various distributions,
understanding what exactly constitutes non-kernel code for
Linux is not clear and depends highly on a distribution.

We pose the following research questions:
1) RQ1: are code churn characteristics different between

kernel and non-kernel code?

2) RQ2: is there a difference in the code velocity between

kernel and non-kernel code?

We collect information by mining GitHub mirrors of all the
operating systems that we study. To investigate code velocity,
we mine Phabricator [19] instance used for FreeBSD code
reviews. Phabricator is a modern code collaboration tool used
by popular projects such as Mozilla and LLVM. We report
descriptive statistics for all the research questions. We ﬁnd
that (a) engineers mainly make changes that are restricted to
a speciﬁc abstraction level, i.e., they change either kernel or
non-kernel code and rarely mix both in the same commit,
(b) kernel commits are larger than non-kernel commits, (c) me-
dian annual growth rate and distribution for neutral, additive,
and subtractive commits for kernel and non-kernel code are
similar, and (d) code reviews in FreeBSD kernel code take
longer than for non-kernel code.

II. BACKGROUND AND RELATED WORK

A. Related work

1) Code churn size: From industry studies, we know that
median change size at AMD is 44 lines [8], the median
number of lines modiﬁed at Google is 24 [9], at Facebook
each deployed software update involved, on average, 92 lines
of code [10], and for Lucent, the number of non-comment
lines changed is 263 lines [8], [20]. We observe an order of
magnitude in the code change between companies.

Including data about open-source software adds even more
variety to our data set. In open-source software projects, the
median change size ranges from 11 to 32 changed lines [11,
p. 37]. For Android, the median change size is 44 lines, for
Apache 25 lines, for Linux 32 lines, and Chrome 78 lines [8].
The median size of pull requests in GitHub is 20 total lines
changed [12].

The critical observation from available data is that the size
of changes varies signiﬁcantly even between various open-
source software projects. We did not ﬁnd any existing studies
differentiating the size of commits between various layers of
abstraction in the same operating system.

2) Code velocity: Existing research about code churn and
code velocity in operating systems focuses on open-source
software such as FreeBSD or Linux. Industry researchers
generally publish data within the scope of the company. We are
not aware of any externally published research related to com-
mercial operating systems, such as closed-source derivatives
of Darwin (foundation for Apple’s operating systems such as
iOS, macOS, and watchOS) or Microsoft Windows.

Based on current ﬁndings, we know that the code reviews
at Google, Microsoft, and open-source software projects take
approximately 24 hours [21]. In commercial software devel-
opment (e.g., Google), 24 hours is the general expectation for
the code review turnaround time [22, p. 176]. An immediate
observation we make is that
this number is an order of
magnitude smaller than the time it takes to review the Linux
kernel code.

Code review practices for the Linux kernel have been
studied [23]. However, that research mainly focuses on code
review activity and its participants, not speciﬁcally on the code
churn and velocity. A study on the Linux kernel ﬁnds that
“33% of the patches makes it into a Linux release, and that
most of them need 3 to 6 months for this” [16]. Another study
analyzing 139,664 accepted patches in the Linux kernel states
that review time (from patch being published to its acceptance)
is on median 20 days [18]. An existing study on FreeBSD
results in ﬁndings differing by order of magnitude [17] albeit
it is another UNIX-like operating system. The data source is
approximately 25,000 code review submissions from FreeBSD
mailing lists. The study deﬁnes resolve time as “the time
spent from submission to the ﬁnal issue or pull request status
operation (stopped at committed, resolved, merged, closed) of
a contribution.” The median resolve time for FreeBSD is 23
hours.

We observe from these data points at least an order-of-
magnitude difference in code velocity even in the context
of Linux. The kernel code (Linux), company-wide statis-
tics (Google, Microsoft), and overall operating system scope
(FreeBSD) have wide variance. Existing studies show the
difference in software evolution for code churn characteristics
and code velocity. We argue that these differences exist even
within the same system.

B. Classiﬁcation, history, and scope of BSD

The history of UNIX [24] and BSD family operating
the
systems is extensively documented [25, p. 3–14]. All
operating systems we investigate have a common set of
ancestors: (a) NetBSD has its roots in both 386BSD and
4.4BSD Lite, (b) OpenBSD is based on NetBSD source tree,
(c) FreeBSD 1.0 is based on 386BSD 0.1, and (d) initial
commit for DragonFlyBSD is based on FreeBSD 4.8.

Different ﬂavors of BSD have a speciﬁc focus. For example,
OpenBSD has a “fanatical attention to security, correctness,
usability, and freedom” and “strives to be the ultimate secure
operating system” [26, p. 4]. At the same time, “NetBSD’s
main purpose is to provide an operating system that can be
ported to any hardware platform” [27, p. xxxiii].

Another set of differences is related to the organizational
aspects of BSD development when compared to commercial
operating systems. The number of engineers working on
similar open-source software and closed-source projects can
differ by order of magnitude. BSD family operating systems
have hundreds of committers (see Table I). We know from
grey literature that the team developing Microsoft Windows
2000 contained 3,100 engineers responsible for developing and
testing the operating system [28]. For Windows Server 2003,
the number of engineers reached 4,400 [29, p. xxxiii]. The
team size has likely increased in the last 20 years.

Testament to the wide adoption of BSD in the consumer
space (in addition to traditionally being thought of as a server
software) is the fact
that Apple’s closed-source operating
systems such as iOS, macOS, watchOS base themselves on
BSD [30], [31].

C. Terminology

1) Kernel and user mode: Various deﬁnitions for kernel
is
exist
in textbooks about operating systems. The kernel
deﬁned as “single binary program” [32, p. 94], “provider of
services” [33, p. 18], “interface between the hardware and
the software” [27, p. 62], “core operating system code” [34,
p. 2], “[t]he most important program” [35, p. 8], “minimal
facilities necessary for implementing additional operating-
system services” [25, p. 22], “nucleus which contains the most
frequently used functions in the OS, at a given time, other
portions of the OS currently in use” [36, p. 53], or “supervisor,
core, or internals of the operating system” [37, p. 4].

In this paper, we use the following deﬁnition: “[t]he kernel is
the part of the system that runs in protected mode and mediates
access by all user programs to the underlying hardware (e.g.,
CPU, keyboard, monitor, disks, network links) and software
constructs (e.g., ﬁlesystem, network protocols)” [25, p. 22].

Code running as part of the kernel is called kernel mode.
“Kernel mode refers to a mode of execution in a proces-
sor that grants access to all system memory and all CPU
instructions” [38, p. 17]. Sometimes kernel mode is also
called supervisor mode, “where everything is allowed” and
“where the processor regulates direct access to hardware and
unauthorized access to memory” [39, p. 18]. The consequences
of defects in kernel mode are catastrophic. For example, “error
in the kernel programming can block the entire system” [33,
p. 18].

The counterpart to the kernel mode is the user mode. User
mode is traditionally associated with the abstraction level at
which code executes. It
is not necessarily related to how
the source tree layout is structured. Therefore, we use the
term non-kernel code to describe all the source that is not
part of the kernel. The deﬁnitions of user mode vary from
describing it as a speciﬁc type of execution environment to a
more technical scope. For example, “programs running outside
the kernel” [32, p. 298], “[c]ompilers and editors run in user
mode” [32, p. 3] “certain areas of memory are protected from
the user’s use and in which certain instructions may not be
executed” [36, p. 58], “all pages in the user address space

are accessible from user mode” [38, p. 17], or a container
where “a thread executes application code with the machine
in nonprivileged protection mode” [25, p. 90].

User mode is sometimes referred to as user space [39,
p. 18], user-space [37, p. 4], or userspace [40, p. 11]. Another
way to look at user mode is “[i]ndividual processes exist
independently alongside each other and cannot affect each
other directly” [33, p. 18] or “[s]oftware that runs in user space
has no direct access to hardware” [34, p. 2]. Limitations related
to execution and resource access are the overall deﬁning
factor for user mode. User mode applications are said to see
a “subset of the machine’s available resources and cannot
perform certain system functions, directly access hardware,
or otherwise misbehave” [37, p. 4]. The term userland “is the
nomenclature typically preferred by the BSD community for
all things that do not belong to the kernel” [40, p. 11].

Though some sources deﬁne an operating system as a
“portion of the software that runs in kernel mode or supervisor
mode” [32, p. 3], such deﬁnition is outdated. We use a more
inclusive understanding where an operating system is deﬁned
as a “layer of software that manages a computer’s resources for
its users and their applications” [41, p. 6]. Modern operating
systems such as Microsoft Windows enable execution of
critical services and device drivers in user mode in addition
to kernel mode [?].

2) Committers, contributors, and maintainers: The BSD
development process and open-source software, in general,
have clearly deﬁned roles for engineers contributing code.
Using deﬁnitions from FreeBSD1, the roles are as follows:
• Contributor – an individual who is contributing code.
• Committer – an individual with write access to the source

code repository.

• Maintainer – a committer who maintains a particular

subsystem.

As an alternative deﬁnition [25, p. 14–15], the roles are
divided into (a) developers (“able to access the source-code
repository, but they are not permitted to change it”), (b) com-
mitters (“permitted to make changes to those parts of the
source-code repository in which they have been authorized
to work”), and (c) core team (a subset of committers who act
as “ﬁnal gatekeepers of the source code”).

In this paper, we do not distinguish between various roles.
From a data mining point of view, we cannot determine who
exactly authored the code changes based purely on source
code history. When a contributor submits a patch and the
patch gets reviewed and approved, the committer makes the
actual code change. There is no formal and uniform way
to specify the original author. Phrases such as “patch from
alias@domain.com,” “patch by alias@domain.com,” or “from
alias” are sometimes used.

In addition, code reviews are not mandatory for committers.
Committers are “required to have any nontrivial changes
reviewed by at least one other person before committing them
to the tree” [25, p. 14]. Each committer can determine the

1https://wiki.freebsd.org/BecomingACommitter

interpretation of “nontrivial.” However, code reviews do not
always have to be public and can be conducted by email, IRC,
or other mechanisms.2 As a result, we have only access to a
subset of all code reviews. We list this explicitly as a threat
to validity in Section VII.

III. STUDY DESIGN

A. Choice of data

We focus on operating systems that enable a clear dis-
tinction between kernel and non-kernel source code. Out of
the widely used operating systems, only a few have open-
sourced their code and history of changes to source code.
Microsoft Windows uses a closed-source model. The Windows
Research Kernel [42] is a subset of the Windows kernel made
public to researchers. It contains only kernel code and is not
accompanied by the history of source code changes. Though
Apple makes some parts of the XNU kernel and Darwin open-
source software3, the complete source code and its history are
not accessible to researchers.

Linux source code and history of changes are public.
Though Linux is open-source software, it is not suitable for
this investigation. There are hundreds of Linux distributions
available (everything from Android to Ubuntu) [43], [44].
However, determining what constitutes non-kernel code is
highly dependent on a particular Linux distribution. Contrary
to popular opinion, “the term Linux refers to only the ker-
nel” [37, p. 3]. It is the nucleus “of an OS rather than the
complete OS” [45, p. 3]. Linux “does not include all Unix
applications, such as ﬁlesystem utilities, windowing systems
and graphical desktops, system administrator commands, text
editors, compilers, and so on” [35, p. 2]. Therefore, we exclude
Linux from our analysis.

Given the limited number of popular open-source operating
systems under active development, we use purposive sampling
to select our study samples. We choose to target the BSD
family of operating systems for the following reasons: (a) ex-
tensive and well-documented history of operating systems’
evolution, (b) source tree that contains a complete operating
system, (c) similar nature of the operating systems to each
other, and (d) in case of FreeBSD the availability of detailed
information about code reviews over an extended period.

The operating systems we gather data for are DragonFly-
BSD, FreeBSD, NetBSD, and OpenBSD. Table I describes
the characteristics of each operating system, the number of
commits we process, and the size of the code base. The
average age of these operating systems is 26 years. We
calculate the LOC per operating system and category of code
using scc [46]. We collect the number of committers per
operating system (DragonFlyBSD4, FreeBSD5, NetBSD6, and
OpenBSD7) from publicly available data.

2https://docs.freebsd.org/en/articles/committers-guide/
3https://opensource.apple.com/
4https://www.dragonﬂybsd.org/team/
5https://docs.freebsd.org/en/articles/contributors/#staff-committers
6http://www.netbsd.org/people/developers.html
7https://marc.info/?l=openbsd-announce&m=163422237101753&w=2

Only FreeBSD uses a formal code collaboration tool
(Phabricator) to conduct code reviews out of these operating
systems. The rest of the operating systems use a code review
model where not all the code reviews are public. Therefore,
the author’s identity is not always formally tracked (see Sec-
tion II-C2). Only the identity of the committer is recorded as
an author of changes.

B. Data extraction

Source code repositories for each version of BSD use sev-
eral different revision control systems: CVS, Git, Mercurial,
and Subversion. To avoid switching between different environ-
ments and develop multiple versions of the same toolset, we
utilize the fact that each operating system has an up-to-date
GitHub mirror. To verify the validity of the mirror image, we
select a random sample of 50 commits from each operating
system and compare the data from the GitHub mirror to the
changes in the original revision control system. We do not
observe any discrepancies in the data. We use Git commands
with custom shell scripts to fetch the information about the
commit history. The collected data is parsed by an application
developed in C#.

FreeBSD code reviews are performed using a code col-
laboration tool called Phabricator. The Phabricator instance8
used to review FreeBSD code has code review data starting
from 2013. We use Phabry [47] to extract the information
about code reviews. The extracted data is exposed in JSON
format. We develop a custom application written in C# to parse
the JSON data. All the statistical analysis is performed using
custom code written in R.

C. Selection and elimination criteria

it

1) Filtering commits: We focus on the main branch used
for the development of each operating system. Depending
is called main, master, or
on the operating system,
trunk. We use the --first-parent option for git log
commands to have a linear commit history We remove all
the commits that do not have any code changes. The empty
commits result from either commits containing only the binary
ﬁles or “placeholder commits” (a side-effect of converting the
source code change history from systems like CVS to GitHub
mirror). Our initial dataset contained 797,879 commits, and
after ﬁltering, it decreased to 796,821 commits.

2) Filtering code reviews: We perform a thorough ﬁltering
process to ensure a valid comparison between kernel and non-
kernel code reviews. The Phabricator instance for FreeBSD
contained 32,884 code reviews during our data collection
process. We fetch all the code reviews that are accessible to
registered users. We use only the code reviews that have gone
through the entire code review cycle, i.e., they were published,
accepted, and eventually merged to the target branch. The
code reviews that are abandoned, ignored, or still open are not
suitable for analysis because we cannot calculate metrics such
as time-to-merge (see Section V for deﬁnitions). We removed

8https://reviews.freebsd.org/

TABLE I
OVERVIEW OF BSD FAMILY OPERATING SYSTEMS. COMMIT HASHES USED TO CALCULATE LOC ARE BASED ON GITHUB MIRRORS. NUMBER OF
COMMITTERS IS AS OF FEBRUARY 10, 2022.

Project

Established

Committers

Total commits

Kernel LOC

Non-kernel LOC

Commit hash

DragonFlyBSD
FreeBSD
NetBSD
OpenBSD

2003
1993
1992
1995

56
393
277
167

36,283
248,565
294,140
217,833

3,553,712
7,444,049
8,677,777
5,490,127

6,795,448
10,724,281
39,446,639
14,448,982

0dd847d4a4fb5725
8a7404b2aeeb6345
557728cbecd15a53
90c94250217113fe

the code reviews where the author “self-accepted” the changes,
and no other engineer was involved in the code review process.
To ensure that a code review contains actual code changes, we
eliminate the reviews where the only content is binary ﬁles. As
an additional constraint that enforces validity, we require that
the time-to-ﬁrst-response is before time-to-accept, and time-to-
merge is after time-to-accept. That restriction ﬁlters out code
reviews submitted and accepted immediately without an actual
review being conducted. After applying these selection criteria,
our ﬁnal dataset9 contains 14,875 code reviews between 2013
and 2021.

D. Statistical analysis

1) Characteristics of data: We report statistically signif-
icant results at a p < .05 and use APA conventions [48].
Our initial observation based on the histogram of total code
changes per commit is that the code churn per commit is not
distributed normally. By code churn we mean the sum of the
“added, removed or modiﬁed” lines [5]. We deﬁne different
commit types that we use in our analysis in Section IV-A.
We use the Shapiro-Wilk test [49] to conﬁrm our observation
about non-normality of commit sizes for all
the commit
types. The tests conﬁrm that commit sizes for neither kernel
(W = 0.085, p < .05), non-kernel code (W = 0.061, p < .05)
or mixed code (W = .019, p < .05) commits are normally
distributed.

We make a similar observation about the non-normality
of various periods describing the code reviews: time-to-ﬁrst-
response (W = 0.12, p < .05),
(W =
0.17, p < .05), and time-to-merge (W = 0.25, p < .05).
See Section V for how we deﬁne these periods. The lack of
normality in data leads us to use nonparametric statistical tests
to analyze the relationships between different groups.

time-to-accept

2) Handling outliers: As a part of data validation, we
inspect the potential outlier values [50] for the commit dates,
the total size of changes (LOC per commit), and the duration
of code reviews. We notice only one abnormal commit in
DragonFlyBSD that, according to the Git log, was supposedly
made in 197010, but the changes were committed in 2011.
We observe that commit sizes have a non-normal (right-
skewed) distribution. Because of skewed distribution, we did
not apply techniques such as Tukey 1.5×IQR fence exclusion
criteria [51] to attempt to remove the potential outliers [52]. To

9https://ﬁgshare.com/s/467523b4c41b51e80d7e
10https://www.dragonﬂybsd.org/mailarchive/commits/2011-05/msg00120.

html

investigate the potential outliers in more detail, we manually
inspect a stratiﬁed sample of 100 commits where the size of a
commit in LOC is more than 100,000. Similarly, we analyze
100 code reviews where time-to-merge exceeds a month to
detect abnormalities. None of the changes were “inconsistent
with the remainder of that set of data” [53, p. 4] or “surprising
or discrepant to the investigator” [54]. We do not classify
any of those changes as outliers based on our ﬁndings. We
conduct our analysis using the complete dataset that remains
after applying all the ﬁlters described in Section III-C.

IV. RQ1: ARE CODE CHURN CHARACTERISTICS
DIFFERENT BETWEEN KERNEL AND NON-KERNEL CODE?

A. Commit taxonomy

For the BSD family of operating systems, the distinction
in the location between kernel and non-kernel source code is
clearly deﬁned. All the kernel source ﬁles reside under the
sys directory (relative to the root of the source tree) [55].
We use that fact to categorize each commit as follows:

• Kernel. Commit contains changes pertaining only to

kernel code.

• Non-kernel. Commit contains changes related only to

non-kernel code.

• Mixed. Commit contains changes to both kernel and non-

kernel code.

The categorization applies only to the location of the source
code. It does not mean that a commit contains code that is exe-
cuted only at a speciﬁc level of abstraction during the runtime.
For example, a user mode application can include headers
describing kernel data structures to pass correct parameters
to a syscall.

TABLE II
COMMIT TYPE PERCENTAGES PER OPERATING SYSTEM.

Project

Commits

Kernel

Non-kernel Mixed

DragonFlyBSD
FreeBSD
NetBSD
OpenBSD

36,283
248,565
294,140
217,833

50.91%
52.44%
52.68%
37.43%

44.22% 4.87%
45.10% 2.46%
46.04% 1.27%
61.11% 1.46%

The distribution of commits is displayed in Table II. The
majority of commits contain either kernel or non-kernel code.
Mixed commits represent, on average, only 2.5% of the
population. We can observe that this trend is consistent across
different operating systems.

1) Mixed commits: To further understand the composition
of mixed commits, we investigate these commits in more
detail. We use stratiﬁed random sampling to select 50 mixed
commits for each operating system resulting in 200 samples.
We then manually analyze the contents of each commit and
record the presence of the following attributes: kernel code,
non-kernel code, documentation (e.g., content for a man
page), conﬁguration (e.g., contents of the operating system
distribution, values for default settings), and test code (e.g.,
functional or unit test code).

Out of all

the possible combinations (25 = 32), four

combinations account for 88.5% of mixed commits.

• The presence of both kernel code and non-kernel code is

responsible for 44.5% of mixed commits.

• Both kernel code and documentation account for 29%.
• Both kernel code and conﬁguration account for 8.5%.
• The kernel code with non-kernel code and documentation

account for 6.5%.

Finding 1

The majority of code changes are either in kernel
or non-kernel code. On average, only 2.5% of code
changes are a mix of these categories. In case of mixed
changes, the dominating combinations are kernel with
non-kernel code and kernel code with updates to
accompanying documentation.

An alternative approach to classiﬁcation is to ignore all
the auxiliary changes because they are not code per se. We
choose to categorize them separately. Based on our experience
with operating system development, a similar amount of rigor
and effort goes into reviewing changes to conﬁguration and
documentation, as to reviewing source code.

B. Commit size and change ratio characteristics

Extracting the contents of a diff from a revision control
system and estimating precisely the commit size are complex
tasks [56], [57]. In this paper, we use git show with the
default diff algorithm and diffstat to calculate the code
churn for each commit [58]. The diffstat outputs the
number of inserted, deleted, and modiﬁed lines based on the
contents of the diff. The sum of these lines is commit size.
Descriptive statistics about commit sizes for different types
of commits (kernel, non-kernel, and mixed) are displayed
in Table III.

We compare our results with the existing data about code
review sizes (see Section II-A1). Each commit does not neces-
sarily have to result from the code review. For example, com-
mitters in an open-source software project are not obligated to
send out a code review for each change (see Section II-C2) and
can commit trivial changes without a code review. Therefore,
only a subset of commits is reviewed by someone other than
the author. We utilize the data about code review size as a close
approximation because engineers tend to submit the contents
of a code review as a single commit.

The median commit sizes in our dataset differ from the
numbers presented in the existing studies (see Section II-A1).
Our ﬁndings are smaller than the ﬁndings from various compa-
nies and open-source software projects. Multiple reasons may
explain this outcome:

• For kernel and non-kernel commits, the median commit
size is smaller than existing data points. We theorize
that this is because committers can produce lots of small
trivial changes. This behavior leads to the median values
describing the commit size decreasing. We choose a
stratiﬁed sample of 100 commits with the size of ≤ 5
LOC and inspect
these commits
manually. Our observations conﬁrm this theory.

the contents of all

• For mixed commits, the median size is larger. Based
on our investigation in Section IV-A, we observe that
mixed changes involve different layers of abstraction. For
example, kernel, corresponding user mode libraries, test
cases, and tools code in the same commit. Mixed commits
quite often involve updates to documentation as well. In
our industry experience, this is an expected result. Making
atomic changes across different subsystems requires more
work and increased code churn.

• Differences in calculation of commit or code review sizes.
This paper uses the accepted deﬁnition of code churn,
which means “added, removed or modiﬁed” LOC [5].
Without more formal details from each study, we specu-
late that it is possible that different methods of counting
the LOC accounts for the variance we see.

A Kruskal-Wallis test for stochastic dominance [59] reveals
that there is a statistically signiﬁcant difference between the
mean ranks of commit sizes for at least one pair of commit
types (H(2) = 16,010.20, p < .05). After performing a
post hoc pairwise Dunn test [60], [61] with a Bonferroni
correction [62], we observe a difference in commit sizes
between all commit types (p < .05).

Finding 2

Kernel commits are bigger than non-kernel commits.
Mixed commits involving both kernel and non-kernel
code are the largest.

C. Commit impact on code base size

The fact that the total size of the software grows over
time can be easily observed in open-source software and
commercial software development. For Linux [63], [64], [65],
and FreeBSD [66],
the growth has been documented and
studied.

We can see in Figure 1 that all the operating systems we
study exhibit a pattern of continuous growth. The rate of the
annual increase in the code base size between operating sys-
tems is similar. The NetBSD growth pattern is an outlier, for
which we lack an explanation. The self-declared focus of each
operating system may partially account for the growth rates
we observe. Future studies should investigate its relationship
to the growth in the size of code base.

TABLE III
COMMIT SIZES PER OPERATING SYSTEM. N = TOTAL NUMBER, M = MEAN, MDN = MEDIAN, SD = STANDARD DEVIATION.

Kernel

Non-kernel

Mixed

Project

N

M Mdn

SD

N

M Mdn

SD

N

M Mdn

SD

DragonFlyBSD
FreeBSD
NetBSD
OpenBSD

18,471
130,348
154,967
81,526

472
168
186
176

13
9
9
9

9,960
3,611
6,421
7,828

16,046
112,106
135,426
133,125

1,748
298
2,126
644

10
6
7
9

32,193
11,085
73,152
26,662

1,766
6,111
3,747
3,182

7,309
1,777
1,785
1,801

92
71
69
51

227,835
14,447
33,372
65,444

non-kernel code. Based on our experience with commercial
operating system development, we know that each unnecessary
line in kernel code is generally treated as a liability due to
the catastrophic consequences of kernel defects. Therefore, at
least amongst the kernel engineers, there is a ﬁrm conviction
and desire to constantly reduce the size of the code base and
remove any unnecessary code.

TABLE IV
PERCENTAGES OF NEUTRAL (=), ADDITIVE (↑), AND SUBTRACTIVE (↓)
COMMITS PER OPERATING SYSTEM AND COMMIT TYPE.

Project

Commit type

=

↑

↓

DragonFlyBSD

FreeBSD

NetBSD

OpenBSD

Kernel
Non-kernel

Kernel
Non-kernel

Kernel
Non-kernel

Kernel
Non-kernel

18.16% 58.22% 23.62%
23.83% 53.30% 22.88%

22.27% 55.26% 22.47%
28.31% 54.48% 17.21%

24.99% 55.15% 19.86%
29.77% 54.48% 15.75%

22.56% 53.88% 23.56%
27.44% 51.74% 20.82%

Fig. 1. Evolution of the size of various BSD source trees. LOC is calculated
using the snapshot of the source tree on January 1st of each year using scc.

Finding 3

The median annual growth rate across all operating
systems for kernel code is 7.15%. For non-kernel code,
the growth rate is 6.19%. The size of both subsystems
increases at a similar rate.

A limited amount of data about closed-source commercial
operating systems is available to the public. Based on the
grey literature, the growth trend is similar. Microsoft Windows
NT 3.1, released in 1993, contained 4–5 million LOC [29,
p. xxxiii]. In 10 years, the size of the code base increased by
order of magnitude to approximately 50 million LOC during
the release of Windows Server 2003 [29, p. xxxiii]. From our
industry experience, we know that the size of the Windows
code base has only continued to increase since.

A reasonable hypothesis is that most commits are either
adding or modifying the code, with only a tiny subset of
commits reducing the size of the code base. We do not
know if that commit pattern applies equally to the kernel and

To verify this theory, we investigate the impact of commits
on the size of the code base in more detail. We categorize the
commits into the following three categories:

• Neutral. All code changes are modiﬁcations (i.e., chang-
ing the existing LOC) or an equal number of insertions
and deletions. Therefore, the commit does not increase
the size of the code base.

• Additive. The number of LOC inserted is greater than

the number of LOC deleted.

• Subtractive. The number of LOC deleted is greater than

the number of LOC inserted.

From Table IV, we can see that the distribution of commits
is uniform across different operating systems using this catego-
rization. We can observe (a) more additive non-kernel commits
than kernel commits, and (b) more subtractive kernel commits
than non-kernel commits. However, the differences between
the percentages of commit types for kernel and non-kernel
code are relatively minor (approximately 1–5%). This ﬁnding
is insufﬁcient to conﬁrm our intuition that kernel commits are
focused more on eliminating the dead code or reducing the size
of the kernel code base than non-kernel commits. In theory,
a small number of commits may systematically remove large
amounts of LOC from the kernel code base to remove dead
code. Based on our sampling of commits that delete more

DragonFlyBSDFreeBSDNetBSDOpenBSD05,000,00010,000,00015,000,00020,000,00025,000,00030,000,00035,000,00040,000,00045,000,00050,000,000199520002005201020152020YearTotal LOC on January 1st of each yearthan 1,000 LOC in the kernel, we do not ﬁnd any evidence
for this. In addition, given the general guidance for commit
size in open-source software development, we ﬁnd that the
frequent usage of commits containing many LOC to remove
code is unlikely. For example, Linux kernel guidelines state
that kernel developers should “break their changes down into
small, logical pieces” [65].

Finding 4

Both kernel and non-kernel commits have similar
neutral, additive, and subtractive commit distributions.
More than 50% of commits in both categories increase
the size of the code base.

D. Ratio of code changes in a commit

To further understand the composition of commits, we
investigate the commit content in more detail. We calculate a
percentage ratio of different types of code changes (insertions,
deletions, and modiﬁcations) per each commit. For example,
we deﬁne the percentage of insert ratio as:

T otal insertions (LOC)
Commit size (LOC)

× 100.

Similarly, we calculate the delete ratio and modify ratio. We
present the percentages of various ratios for each operating
system and commit type in Table V. On average only 2.5%
of code changes are mixed (see Section IV-A1). Our analysis
is primarily interested in the differences between kernel and
non-kernel code.

As an immediate observation, we notice that the median
delete ratio is 0% for almost all the categories. Based on the
discussion in Section IV-C this is an expected ﬁnding. The
only outlier behavior in DragonFlyBSD has a 5.25% of delete
ratio in mixed commits. To investigate this ﬁnding, we pick
a random sample of 100 DragonFlyBSD commits where the
delete ratio was more than 50% and analyze each commit
manually. Our analysis indicates that 82% of commits are
associated with a deliberate effort to remove obsolete fea-
tures11 and functionality.12 Therefore, mixed commits have a
very high delete ratio percentage (up to 100%). Consequently,
that increases both the mean and median values for mixed
commits. The rest of the commits are associated with tasks
such as refactoring, reverting previous commits, or general
code hygiene13 related work.

We notice after observing mean and median values for
the insert ratio that kernel commits tend to have a higher
insert ratio than non-kernel commits A Mann-Whitney U
test [67] indicated that this difference was statistically sig-
niﬁcant U (NKernel = 100,000, NN on−kernel = 100,000) =
10,133,125,316.5, z = 10.61, p < .05. Similarly, we ob-
serve that modiﬁcation ratio tends to be smaller for ker-
nel code than non-kernel code. Based on Mann-Whitney U

11https://github.com/DragonFlyBSD/DragonFlyBSD/commit/5ca0a96
12https://github.com/DragonFlyBSD/DragonFlyBSD/commit/7c87aae
13https://github.com/DragonFlyBSD/DragonFlyBSD/commit/0df73a2

test the difference was statistically signiﬁcant U (NKernel =
100,000, NN on−kernel = 100,000) = 9,699,003,058.5, z =
−23.50, p < .05.

One possible explanation for the higher insert ratio in the
kernel is our anecdotal observation that engineers who want to
work on operating systems generally tend to work on kernel
code as opposed to non-kernel code, focusing on new features
and improvements to the kernel. We ﬁnd that theory plausible
because it is beneﬁcial for the operating system’s adoption
rate. That means code changes to support new hardware
platforms, modern networking protocols, or port over existing
device drivers.

For the lower modiﬁcation ratio in the kernel, we speculate
that engineers are more careful when changing the existing
code than adding new code. With new code, the author has
detailed knowledge about why the code is being added and
what it does. The existing code, especially the kernel, which
may be decades old, has a limited test coverage (if any),
and there is often no context or sufﬁcient documentation to
understand the implementation details. Making kernel changes
under those constraints is riskier than adding new code. In
non-kernel code, the consequences of introducing defects are
less severe, and therefore engineers are willing to take more
risks when updating existing code. Future studies should test
that speculative intuition.

Finding 5

The delete ratio is abysmal across all commit types.
The insert ratio in kernel code is higher than in non-
kernel code. The modiﬁcation ratio in kernel code is
lower than in non-kernel code.

V. RQ2: IS THERE A DIFFERENCE IN THE CODE VELOCITY
BETWEEN KERNEL AND NON-KERNEL CODE?
We deﬁne code velocity as the speed with which code
changes are reviewed and merged into a destination branch.
Code velocity is an essential metric in the industry and is
associated with engineers’ job satisfaction [68], [69]. In envi-
ronments using CI/CD (e.g., Facebook), the code velocity is
essential to the entire development process [70]. To investigate
the code velocity in FreeBSD, we use the metrics previously
found to be meaningful in the industry.

• We deﬁne time-to-ﬁrst-response as the time from pub-
lishing the code review to the ﬁrst interaction on the
code review by someone else than the author (exclud-
ing automated bots). An existing study from Microsoft
that researches challenges encountered during the code
review process indicates that delayed response time is the
number one concern [13]. Another Microsoft study about
code velocity identiﬁes two points in time that engineers
consider critical: the ﬁrst comment or sign-off from a
reviewer and when the code review has been marked as
completed [14].

• We deﬁne time-to-accept as the time from publishing the
code review to when someone else than the author accepts

TABLE V
RATIO OF CODE CHANGES PER OPERATING SYSTEM AND COMMIT TYPE. N = TOTAL NUMBER OF COMMITS, M = MEAN, MDN = MEDIAN, SD =
STANDARD DEVIATION.

Project info

Insert ratio

Delete ratio

Modiﬁcation ratio

Project

Commit type

N

M

Mdn

SD

M

Mdn

SD

M

Mdn

SD

DragonFlyBSD

FreeBSD

NetBSD

OpenBSD

Kernel
Non-kernel
Mixed

Kernel
Non-kernel
Mixed

Kernel
Non-kernel
Mixed

Kernel
Non-kernel
Mixed

18,471
16,046
1,766

130,348
112,106
6,111

154,967
135,426
3,747

81,526
133,125
3,182

40.94% 40.00% 36.98% 17.99% 0.00% 29.53% 41.07% 33.33% 35.31%
37.80% 28.57% 38.47% 16.34% 0.00% 29.49% 45.86% 40.00% 37.97%
44.98% 46.86% 36.79% 22.76% 5.25% 32.56% 32.26% 21.43% 32.18%

41.47% 36.84% 39.40% 17.73% 0.00% 30.51% 40.81% 31.71% 38.15%
40.72% 33.33% 40.60% 13.17% 0.00% 27.91% 46.12% 40.00% 40.18%
54.64% 63.93% 37.81% 16.47% 0.00% 29.15% 28.90% 16.67% 31.97%

33.91% 27.27% 34.06% 12.59% 0.00% 24.13% 53.50% 50.00% 34.29%
34.22% 25.00% 35.44% 10.13% 0.00% 22.65% 55.65% 50.00% 35.94%
43.86% 44.57% 36.55% 13.84% 0.00% 25.43% 42.31% 33.33% 34.88%

35.47% 30.95% 35.01% 15.51% 0.00% 26.78% 49.02% 44.44% 34.30%
33.02% 22.99% 35.17% 13.59% 0.00% 25.76% 53.40% 50.00% 35.87%
42.60% 41.68% 36.02% 15.50% 0.00% 28.02% 41.90% 34.48% 33.49%

TABLE VI
CHARACTERISTICS OF 14,875 FREEBSD CODE REVIEWS. M = MEAN, MDN = MEDIAN, SD = STANDARD DEVIATION. TIME PERIODS ARE GIVEN IN
HOURS.

Kernel (6,405 reviews)

Non-kernel (7,573 reviews)

Mixed (897 reviews)

Period

M

Mdn

SD

M

Mdn

SD

M

Mdn

SD

Time-to-ﬁrst-response
Time-to-accept
Time-to-merge

97.86
219.18
476.57

4.57
11.66
71.44

578.13
1019.39
1678.20

107.10
221.88
547.36

4.16
7.71
46.03

682.67
1185.22
2411.48

90.17
445.75
1013.39

4.55
22.43
157.41

588.79
1866.96
2895.97

the code changes. Once accepted, the changes are ready
to be merged into the target branch.

time-to-merge (p < .05) and only between kernel and non-
kernel commits for time-to-ﬁrst-response (p < .05).

• We deﬁne time-to-merge as the time from publishing the
code review to when changes are merged to the target
branch. A study that focuses on code review performance
in Xen hypervisor ﬁnds that time-to-merge is a crucial
metric to help to investigate the delays caused by the
code review process [15].

We calculate these metrics for each type of commit (kernel,
non-kernel, and mixed). We present an overview of various
code review periods in Table VI. The median size of FreeBSD
code review is 17 LOC. An initial observation we make is that
initial engagement from reviewers is similar. Median time-to-
ﬁrst-response for all commit types is ± 1 hour. After that,
the completion of various code review milestones signiﬁcantly
diverges between commit types. Comparison between medians
shows that while time-to-ﬁrst-response for kernel code reviews
is only 9.85% longer than for non-kernel commits, time-to-
accept is 51.23%, and time-to-merge is 55.2% longer.

A Kruskal-Wallis test for stochastic dominance reveals that
there is a statistically signiﬁcant difference between the mean
ranks of time-to-ﬁrst-response, time-to-accept, and time-to-
merge for at least one pair of commit types. For time-to-
ﬁrst-response the test returned (H(2) = 6.42, p < .05), for
time-to-accept (H(2) = 165.81, p < .05), and for time-to-
merge (H(2) = 344.16, p < .05). After performing a post hoc
pairwise Dunn test with a Bonferroni correction, we observe
a difference between all commit types for time-to-accept and

Finding 6

time-to-accept, and time-to-merge)

Different phases of the code review (time-to-ﬁrst-
response,
take
longer for the kernel code than for non-kernel code.
Mixed code reviews take the most time to be accepted
and merged.

This ﬁnding feels intuitively correct. It is reasonable to as-
sume that reviewing kernel code requires greater care because
of the consequences of potential defects and the technical level
of knowledge required. Mixed commits contain approximately
in 44.5% cases code from both kernel and non-kernel code.
These commits may need more reviewers (e.g., a maintainer
for each affected subsystem) or require a reviewer who under-
stands the nuances of multiple components.

Existing data about FreeBSD code velocity is limited.
FreeBSD study [17] ﬁnds that the median resolve time (com-
parable to time-to-merge) is 23 hours and approximately 6
hours for the time-to-ﬁrst-response. The times for time-to-
ﬁrst-response are comparable to what we present in Table VI.
The values for time-to-merge are longer for the code reviews
we analyzed. Several reasons can cause the differences. For
example, we compare our data from 2013 to 2021 to code
reviews from 1995 to 2006. Another reason is that we focus
only on code reviews that were accepted and merged. That

restriction is necessary to calculate the values for time-to-
accept and time-to-merge. That means ignoring the category
of patches that the study classiﬁed as resolved or closed.

VI. DISCUSSION

Code churn characteristics We ﬁnd that, on average, ap-
proximately 49% of code changes are in non-kernel code (see
Table II). This ﬁnding suggests that to meaningfully contribute
to operating system development, an engineer does not have
to work exclusively on the kernel or know the intrinsic details
of the development of an operating system. The fact that the
insert ratio in kernel code changes is bigger than in non-
kernel code changes implies innovative opportunities (e.g.,
implement a new feature) to contribute to the kernel code base.
A surprising ﬁnding is that kernel code changes are bigger
than non-kernel code changes. This contradicts what we have
observed during the development of commercial operating
systems. The code reviews in kernel taking longer than non-
kernel code reviews is an expected ﬁnding and makes intuitive
sense.

We ﬁnd similar patterns related to commit sizes and tax-
onomy emerging across all different operating systems we
investigate. Our ﬁndings about kernel code reviews taking
longer than non-kernel code reviews match with our industry
experience when working on the development of commercial
operating systems. Findings about mixed changes are also in
agreement with our past experiences.

Though we expected all operating systems to have annual
the size of growth in OpenBSD during the
code growth,
last 10 years is surprising to us. OpenBSD has historically
identiﬁed itself as an operating system focused on leanness and
security [26]. These goals encourage developers to constantly
“prune” (i.e., delete) and polish their source code to reduce
the attack surface [71]. Nevertheless, the code base size has
doubled in the last 10 years.

Drawing conclusions based on system-wide metrics When
characterizing a particular company or a more extensive sys-
tem, researchers need to be careful with interpreting the results
and drawing conclusions from them. For example, companies
like Apple, Facebook, Google, and Microsoft have tens of
complex products in development. Each of them may evolve
at a different cadence or speed. Researchers need to be more
precise when describing various evolutionary characteristics of
the code base and ﬁne-tune the scope of inquiry.

Separation of abstractions layers. One of the ﬁndings in our
study is that mixed code changes represent a small fraction of
all the changes. Intuitively, that makes sense based on inspect-
ing a sample of mixed commits and our industry experiences.
Code contributions spanning different components are rarer
than code changes made to each component in separation.
The mixed changes also take the longest to review and contain
the largest amount of code. We speculate that longer reviews
times are mainly caused because of the size of changes, lack
of reviewers with expertise at multiple abstraction layers, and
a need to have approvals from multiple individuals. Based on
our ﬁndings, we speculate that the presence of code changes

from different abstraction layers is a valuable predictor to
indicate that code reviews for those changes can take longer.
Accepted practice in the industry and open-source software
is that the main branch is always kept in a working state.
The guidance for the Linux kernel
is that “[e]ach patch
should, when applied, yield a kernel which still builds and
works properly” [65, p. 4]. Working state means both an
ability to build the code without errors and the majority of
the features working. As an implication this means that any
commit cannot break this assumption. Based on the taxonomy
of commits in Table II (a tiny number of commits changing
kernel and non-kernel code simultaneously), we can infer that
it is possible to work efﬁciently on kernel and non-kernel code
in separation. This ﬁnding implies that various components are
isolated enough to make changes in separation.

VII. THREATS TO VALIDITY

Like any other study, the results we present in our paper
are subject to speciﬁc categories of threats. We enumerate the
threats to construct, internal, and external validity [72].

We thoroughly validate raw data to avoid issues with
construct validity and interpretation of theoretical constructs.
We analyze the potential outlier commits to verify that we
calculate code churn correctly. We ﬁlter out code reviews
where the meaningful reviews did not occur (e.g., “self-
accepted” changes or the only changes to binary ﬁles). We
verify that kernel and non-kernel code locations include the
code that is supposed to run at that abstraction layer.

Threats to internal validity include impact by potential
unknown factors which may inﬂuence the results. When
analyzing the code review related periods (e.g., time-to-ﬁrst-
into all
response) for FreeBSD, we do not have insight
confounding variables that can inﬂuence code review times.
For example, the availability of reviewers, the time-zone for
authors or reviewers, or the state of a CI system. No system
records that data, and the lack of data may limit internal
validity. Another threat in this category is that code reviews
are not required part of the FreeBSD development process for
committers. “FreeBSD committers are only required to respect
each other by asking for code review before committing code
to ﬁles that are actively maintained by other committers” [73].
Our analysis bases itself on a subset of all FreeBSD code
reviews.

Concerns related to external validity focus on applying
our ﬁndings in other contexts. We do not have access to
the commit history and source code of commercial operating
systems such as Darwin derivatives or Microsoft Windows.
We do know from grey literature [74], [75], [76] and our
industry experience that the characteristics of a development
process for commercial and open-source software operating
systems are different. For example, motivation for product
development, number of engineers involved, and development
methodology. Our ﬁndings may not be applicable in that
context. A conﬁrmatory case study by industrial researchers
is necessary to test our ﬁndings in the context of commercial
operating systems.

VIII. CONCLUSIONS AND FUTURE WORK

We conduct a large-scale study on four BSD family
operating systems: DragonFlyBSD, FreeBSD, NetBSD, and
OpenBSD. Based on the literature review, we are the ﬁrst
to explore the differences between commit sizes, commit
taxonomy, and code velocity in kernel and non-kernel code
in the context of operating systems development.

Our key ﬁnding is that researchers and practitioners should
view a larger software system as a collection of subsystems
and sub-components, not just one entity. Our analysis shows
that when making code changes (a) developers modify either
kernel or non-kernel code, but rarely code belonging to both
categories, (b) the median size of commits to kernel code
is larger than non-kernel commits, (c) both kernel and the
non-kernel code bases have a similar annual growth rate, and
(d) in FreeBSD, the code reviews for kernel code take longer
than code reviews for non-kernel code.

As part of our future research, we intend to focus on the

following topics:

• Do developers gravitate towards kernel as they gain more
experience with operating systems development? It is typ-
ical for new contributors joining an open-source software
project to start contributing by making more straightfor-
ward changes Generally, the opportunities associated with
the least amount of risk are in user mode, e.g., making
changes in a command-line tool. As engineers gain more
conﬁdence and experience, do they change their focus to
areas where the stakes are higher than in user mode, e.g.,
device drivers?

• Do developers mainly contribute to their abstraction
layer of choice? It is common in BSD and Linux de-
velopment to have maintainers for each area (e.g., boot
sequence, tracing subsystem). One of the interesting ques-
tions is related to the distribution between “specialists”
(engineers who contribute only to a few narrow areas)
and “generalists” (engineers who make changes in various
components). Based on the Linux kernel research, we
know that most engineers (62%) who contribute to the
kernel have a narrow specialist proﬁle [77]. We do not
know if this holds in the context of an entire operating
system.

REFERENCES

[1] L. J. Arthur, Software Evolution: The Software Maintenance Challenge.

USA: Wiley-Interscience, 1988.

[2] P. Tripathy and K. Naik, A Practitioner’s Approach, Software Evolution

and Maintenance. USA: John Wiley & Sons, Inc., 2014.

[3] I. Herraiz, D. Rodriguez, G. Robles, and J. M. Gonzalez-Barahona,
“The evolution of the laws of software evolution: A discussion based
on a systematic literature review,” ACM Comput. Surv., vol. 46, no. 2,
dec 2013. [Online]. Available: https://doi.org/10.1145/2543581.2543595
[4] M. Nurolahzade, S. M. Nasehi, S. H. Khandkar, and S. Rawal, “The
role of patch review in software evolution: An analysis of the Mozilla
Firefox,” in Proceedings of the Joint International and Annual ERCIM
Workshops on Principles of Software Evolution (IWPSE) and Software
Evolution (Evol) Workshops, ser. IWPSE-Evol ’09. New York, NY,
USA: Association for Computing Machinery, 2009, p. 9–18. [Online].
Available: https://doi.org/10.1145/1595808.1595813

[5] J. Munson and S. Elbaum, “Code churn: a measure for estimating the
impact of code change,” in Proceedings. International Conference on
Software Maintenance (Cat. No. 98CB36272), 1998, pp. 24–31.

[6] G. A. Hall and J. C. Munson, “Software evolution: code delta and code
churn,” Journal of Systems and Software, vol. 54, no. 2, pp. 111–118,
Oct. 2000. [Online]. Available: https://linkinghub.elsevier.com/retrieve/
pii/S0164121200000315

[7] D. Spinellis, P. Louridas, and M. Kechagia, “Software evolution: the
lifetime of ﬁne-grained elements,” PeerJ Computer Science, vol. 7, p.
e372, Feb. 2021. [Online]. Available: https://peerj.com/articles/cs-372

[8] P. C. Rigby and C. Bird, “Convergent contemporary software peer review
practices,” in Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering, ser. ESEC/FSE 2013.
Saint Petersburg,
Russia: Association for Computing Machinery, Aug. 2013, pp. 202–212.
[Online]. Available: https://doi.org/10.1145/2491411.2491444

[9] C. Sadowski, E. S¨oderberg, L. Church, M. Sipko, and A. Bacchelli,
“Modern code review: A case study at Google,” in Proceedings of
the 40th International Conference on Software Engineering: Software
Engineering in Practice, ser. ICSE-SEIP ’18. Gothenburg, Sweden:
Association for Computing Machinery, May 2018, pp. 181–190.
[Online]. Available: https://doi.org/10.1145/3183519.3183525

[10] C. Rossi, E. Shibley, S. Su, K. Beck, T. Savor, and M. Stumm,
“Continuous deployment of mobile software at Facebook (showcase),” in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering, ser. FSE 2016. Seattle, WA,
USA: Association for Computing Machinery, Nov. 2016, pp. 12–23.
[Online]. Available: https://doi.org/10.1145/2950290.2994157

[11] P. C. Rigby, “Understanding open source software peer review: Review
processes, parameters and statistical models, and underlying behaviours
and mechanisms,” Ph.D. dissertation, University of Victoria, CAN, 2011,
aAINR80365.

[12] G. Gousios, M. Pinzger, and A. v. Deursen, “An exploratory study of
the pull-based software development model,” in Proceedings of the 36th
International Conference on Software Engineering, ser. ICSE 2014.
Hyderabad, India: Association for Computing Machinery, May 2014, pp.
345–355. [Online]. Available: https://doi.org/10.1145/2568225.2568260
[13] L. MacLeod, M. Greiler, M.-A. Storey, C. Bird, and J. Czerwonka,
“Code reviewing in the trenches: Challenges and best practices,” IEEE
Software, vol. 35, no. 4, pp. 34–42, 2018.

[14] C. Bird, T. Carnahan, and M. Greiler, “Lessons learned from building
and deploying a code review analytics platform,” in 2015 IEEE/ACM
12th Working Conference on Mining Software Repositories (MSR). Los
Alamitos, CA, USA: IEEE Computer Society, may 2015, pp. 191–201.
[Online]. Available: https://doi.ieeecomputersociety.org/10.1109/MSR.
2015.25

[15] D. Izquierdo-Cortazar, N. Sekitoleko, J. M. Gonzalez-Barahona, and
L. Kurth, “Using Metrics to Track Code Review Performance,” in
Proceedings of the 21st International Conference on Evaluation and
in Software Engineering, ser. EASE’17. Karlskrona,
Assessment
Sweden: Association for Computing Machinery,
Jun. 2017, pp.
214–223. [Online]. Available: https://doi.org/10.1145/3084226.3084247
[16] Y. Jiang, B. Adams, and D. M. German, “Will my patch make it? and
how fast?: Case study on the Linux kernel,” in Proceedings of the 10th
Working Conference on Mining Software Repositories, ser. MSR ’13.
IEEE Press, 2013, pp. 101–110.

[17] J. Zhu, M. Zhou, and A. Mockus, “Effectiveness of code contribution:
From patch-based to pull-request-based tools,” in Proceedings of the
2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering, ser. FSE 2016. New York, NY, USA:
Association for Computing Machinery, 2016, p. 871?882. [Online].
Available: https://doi.org/10.1145/2950290.2950364

[18] X. Tan and M. Zhou, “How to communicate when submitting
patches: An empirical study of the Linux kernel,” Proc. ACM Hum.-
Comput. Interact., vol. 3, no. CSCW, nov 2019. [Online]. Available:
https://doi.org/10.1145/3359210

[19] Phacility, “Phacility - Home,” 2021.

[Online]. Available: https:

//www.phacility.com/

[20] A. Porter, H. Siy, A. Mockus, and L. Votta, “Understanding the
sources of variation in software inspections,” ACM Trans. Softw. Eng.
Methodol., vol. 7, no. 1, p. 41–79,
jan 1998. [Online]. Available:
https://doi.org/10.1145/268411.268421

[21] P. C. Rigby and C. Bird, “Convergent contemporary software peer
review practices,” in Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering, ser. ESEC/FSE 2013. New

York, NY, USA: Association for Computing Machinery, 2013, p.
202–212. [Online]. Available: https://doi.org/10.1145/2491411.2491444
[22] T. Winters, T. Manshreck, and H. Wright, Software Engineering at
Google: Lessons Learned from Programming Over Time, 1st ed. Beijing
Boston Farnham Sebastopol Tokyo: O’Reilly, 2020.

[23] B.

Erdamar,

“Measuring

Kernel,” Master’s
of

the
Uni-
Linux
versity
[Online].
Munich,
Available: https://lpc.events/event/11/contributions/905/attachments/773/
1795/Erdamar 2021 Measuring-Code-Review-in-the-Linux-Kernel.pdf
Reading, Mass: Addison-

in
Technical
2021.

The
Mar.

[24] P. H. Salus, A quarter century of UNIX.

Munich,

Review

thesis,

Code

Wesley Pub. Co, 1994.

[25] M. K. McKusick, G. V. Neville-Neil, and R. N. M. Watson, The Design
and Implementation of the FreeBSD Operating System, 2nd ed. Upper
Saddle River, NJ: Addison Wesley, 2015.

[26] M. W. Lucas, Absolute OpenBSD: Unix for the Practical Paranoid. San

Francisco: No Starch Press, 2003.

[27] ——, Absolute BSD: the Ultimate Guide to FreeBSD. San Francisco:

No Starch Press, 2002.

[28] M. Lucovsky.
odyssey.
usenix-win2000/invitedtalks/lucovsky html/

engineering
[Online]. Available: https://www.usenix.org/legacy/events/

(2000) Windows — a

software

[29] V. Maraia, The Build Master: Microsoft’s Software Conﬁguration Man-

agement Best Practices. Addison-Wesley Professional, 2005.

[30] J. Levin, *OS internals. Volume 1: User space, 2nd ed. Edison, N.J:

Technologeeks.com, 2017.

[31] A. Singh, Mac OS X Internals: a Systems Approach. Addison-Wesley

Professional, 2016, OCLC: 1005337597.

[32] A. S. Tanenbaum and A. Woodhull, Operating Systems: Design and
Implementation, 2nd ed. Upper Saddle River, NJ: Prentice Hall, 1997.
Harlow, England ;

[33] M. Beck, Ed., Linux Kernel Internals, 2nd ed.

Reading, Mass: Addison-Wesley, 1998.

[34] O. H. Halvorsen and D. Clarke, OS X and iOS Kernel Programming:
Master Kernel Programming for Efﬁciency and Performance. New
York, NY: Apress, 2011.

[35] D. P. Bovet and M. Cesati, Understanding the Linux Kernel, 2nd ed.

Beijing ; Sebastopol, Calif: O’Reilly, 2003.

[36] W. Stallings, Operating Systems: Internals and Design Principles,
6th ed. Upper Saddle River, N.J: Pearson/Prentice Hall, 2009.

[37] R. Love, Linux Kernel Development, 2nd ed.

Indianapolis, Ind: Novell

Press, 2005.

[38] M. E. Russinovich, D. A. Solomon, and A. Ionescu, Windows Internals,
6th ed. Redmond, Wash: Microsoft Press, 2012, oCLC: ocn753301527.
Sebastopol:

[39] A. Rubini and J. Corbet, Linux Device Drivers, 2nd ed.

O’Reilly & Associates, 2001.

[40] W. Mauerer, Professional Linux Kernel Architecture, ser. Wrox pro-
IN: Wiley Pub, 2008, oCLC:

Indianapolis,

fessional guides.
ocn227198266.

[41] T. Anderson and M. Dahlin, Operating Systems: Principles and Practice,

2nd ed.

s.l.: Recursive Books, 2014.

[42] A. Schmidt, A. Polze, and D. Probert, “Teaching operating systems:
Windows kernel projects,” in Proceedings of the 41st ACM Technical
Symposium on Computer Science Education, ser. SIGCSE ’10. New
York, NY, USA: Association for Computing Machinery, 2010, pp.
490–494. [Online]. Available: https://doi.org/10.1145/1734263.1734429
is a Linux Distribution? [Online]. Available:

[43] SUSE. (2022) What

https://www.suse.com/suse-deﬁnes/deﬁnition/linux-distribution/

[44] Eklektix, Inc. (2021) The LWN.net Linux Distribution List. [Online].

Available: https://lwn.net/Distributions/

[45] G. J. Nutt, Kernel Projects for Linux.

Boston: Addison Wesley

Longman, 2001.

[46] B. E. C. Boyter. (2022, Mar.) Sloc Cloc and Code (scc). [Online].

Available: https://github.com/boyter/scc/

[47] D. Cotet. (2021, may) Phabry. [Online]. Available: https://github.com/

dimonco/Phabry
[48] A. P. Association.

(2020) Publication Manual of

the American
Psychological Association, Seventh Edition (2020). [Online]. Available:
https://apastyle.apa.org/products/publication-manual-7th-edition
[49] S. S. Shapiro and M. B. Wilk, “An analysis of variance test for
normality (complete samples),” Biometrika, vol. 52, no. 3-4, pp.
591–611, Dec. 1965. [Online]. Available: https://academic.oup.com/
biomet/article-lookup/doi/10.1093/biomet/52.3-4.591

[50] B. Iglewicz and D. C. Hoaglin, How to detect and handle outliers, ser.
ASQC basic references in quality control. Milwaukee, Wis: ASQC
Quality Press, 1993, no. v. 16.

[51] H. Beyer, “Tukey, John W.: Exploratory Data Analysis. Addison-
Wesley Publishing Company Reading, Mass. ? Menlo Park, Cal.,
London, Amsterdam, Don Mills, Ontario, Sydney 1977, XVI, 688
S.” Biometrical Journal, vol. 23, no. 4, pp. 413–414, 1981. [Online].
Available: http://doi.wiley.com/10.1002/bimj.4710230408

[52] M. Hubert and E. Vandervieren, “An adjusted boxplot for skewed
distributions,” Computational Statistics & Data Analysis, vol. 52, no. 12,
pp. 5186–5201, 2008. [Online]. Available: https://www.sciencedirect.
com/science/article/pii/S0167947307004434

[53] V. Barnett and T. Lewis, “Outliers in statistical data,” Biometrical
[Online]. Available:

Journal, vol. 30, no. 7, pp. 866–867, 1984.
https://doi.org/10.1002/bimj.4710300725

[54] R. J. Beckman and R. D. Cook, “Outlier. . . . . . . . . s,” Technometrics,
vol. 25, no. 2, pp. 119–149, May 1983.
[Online]. Available:
http://www.tandfonline.com/doi/abs/10.1080/00401706.1983.10487840
[55] The FreeBSD Project. (2022) The layout of /usr/src. Publisher: The
FreeBSD Project. [Online]. Available: https://docs.freebsd.org/en/books/
developers-handbook/introduction/#introduction-layout

[56] P. Hofmann and D. Riehle, “Estimating commit sizes efﬁciently,”
in Open Source Ecosystems: Diverse Communities Interacting, 5th
IFIP WG 2.13 International Conference on Open Source Systems,
OSS 2009, Sk¨ovde, Sweden, June 3-6, 2009. Proceedings,
ser.
in Information and Communication Technology,
IFIP Advances
C. Boldyreff, K. Crowston, B. Lundell, and A.
I. Wasserman,
Eds., vol. 299.
Springer, 2009, pp. 105–115. [Online]. Available:
https://doi.org/10.1007/978-3-642-02032-2 11

[57] Y. S. Nugroho, H. Hata, and K. Matsumoto, “How different are
different diff algorithms in Git?” Empirical Software Engineering,
vol. 25, no. 1, pp. 790–823, Sep. 2019.
[Online]. Available:
https://doi.org/10.1007/s10664-019-09772-z

[58] T. E. Dickey.

(2021, mar) diffstat manual.

[Online]. Available:

https://invisible-island.net/diffstat/diffstat.html

[59] W. H. Kruskal and W. A. Wallis, “Use of Ranks in One-Criterion
Variance Analysis,” Journal of the American Statistical Association,
vol. 47, no. 260, pp. 583–621, Dec. 1952.
[Online]. Available:
http://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441
[60] O. J. Dunn, “Multiple Comparisons among Means,” Journal of the
American Statistical Association, vol. 56, no. 293, pp. 52–64, Mar.
1961. [Online]. Available: http://www.tandfonline.com/doi/abs/10.1080/
01621459.1961.10482090
“Multiple

comparisons using rank sums,” Technometrics,
[Online]. Available: https:

vol. 6, no. 3, pp. 241–252, 1964.
//www.tandfonline.com/doi/abs/10.1080/00401706.1964.10490181
[62] H. Abdi, “The bonferonni and ˇSid´ak corrections for multiple compar-
isons,” Encyclopedia of measurement and statistics, vol. 3, 01 2007.

[61] ——,

[63] O. Koren, “A study of the Linux kernel evolution,” SIGOPS Oper.
Syst. Rev., vol. 40, no. 2, p. 110–112, apr 2006. [Online]. Available:
https://doi.org/10.1145/1131322.1131325

[64] The Linux Foundation. (2017) State of Linux Kernel Development
https://www.linuxfoundation.org/tools/

[Online]. Available:
2017.
state-of-linux-kernel-development-2017/

[65] J. Corbet, G. Kroah-Hartman, and A. McPherson. (2012, 03/2012) Linux
kernel development: How fast it is going, who is doing it, what they are
doing, and who is sponsoring it.

[66] C. Izurieta and J. Bieman, “The evolution of FreeBSD and Linux,”
in Proceedings of the 2006 ACM/IEEE International Symposium on
Empirical Software Engineering, ser. ISESE ’06. New York, NY, USA:
Association for Computing Machinery, 2006, p. 204–211. [Online].
Available: https://doi.org/10.1145/1159733.1159765

[67] H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two
Random Variables is Stochastically Larger than the Other,” The Annals
of Mathematical Statistics, vol. 18, no. 1, pp. 50–60, Mar. 1947.
[Online]. Available: http://projecteuclid.org/euclid.aoms/1177730491

[68] T. Savor, M. Douglas, M. Gentili, L. Williams, K. Beck, and
M. Stumm, “Continuous Deployment at Facebook and OANDA,” in
2016 IEEE/ACM 38th International Conference on Software Engineer-
ing Companion (ICSE-C), May 2016, pp. 21–30.

[69] O. Kononenko, O. Baysal, and M. W. Godfrey, “Code review quality:
How developers see it,” in Proceedings of
the 38th International
Conference on Software Engineering, ser. ICSE ’16. Austin, Texas:

Association for Computing Machinery, May 2016, pp. 1028–1038.
[Online]. Available: https://doi.org/10.1145/2884781.2884840

[70] D. G. Feitelson, E. Frachtenberg, and K. L. Beck, “Development and
deployment at Facebook,” IEEE Internet Computing, vol. 17, no. 4, pp.
8–17, 2013.

[71] T. Unangst, “Pruning and Polishing: Keeping OpenBSD Modern,” in
Proceedings of AsiaBSDCon 2015. Tokyo, Japan: Tokyo University
of Science, Mar. 2015. [Online]. Available: https://www.openbsd.org/
papers/pruning.html

[72] F. Shull, J. Singer, and D. I. K. Sjøberg, Guide to Advanced Empirical

Software Engineering. London: Springer, 2008.

[73] T. Dinh-Trong and J. Bieman, “Open source software development: a
case study of FreeBSD,” in 10th International Symposium on Software
Metrics, 2004. Proceedings., 2004, pp. 96–105.

[74] G. P. Zachary, Show-stopper! The breakneck race to create Windows
NT and the next generation at Microsoft. New York : Toronto : New
York: Free Press ; Maxwell Macmillan Canada ; Maxwell Macmillan
International, 1994.

[75] S. Maguire, Writing solid code: Microsoft’s techniques for developing
bug-free C programs. Redmond, Wash: Microsoft Press, 1993.

[76] J. McCarthy, Dynamics of software development.

Redmond, Wash:

Microsoft Press, 1995.

[77] G. Avelino,

L.

Passos, A. Hora,

authorship: The

“Assessing code
in Open Source Systems: Towards Robust Practices.
International Publishing, 2017, pp. 151–163.
https://doi.org/10.1007/978-3-319-57735-7 15

case of

and M.

T. Valente,
the Linux kernel,”
Springer
[Online]. Available:

