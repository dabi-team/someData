PREPRINT

1

A Sequential Metamorphic Testing Framework for
Understanding Automated Driving Systems

Quang-Hung Luu, Huai Liu, Tsong Yueh Chen and Hai L. Vu

2
2
0
2

n
u
J

7

]
E
S
.
s
c
[

1
v
5
7
0
3
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Automated driving systems (ADS) are expected to be
reliable and robust against a wide range of driving scenarios.
Their decisions, ﬁrst and foremost, must be well understood.
Understanding a decision made by ADS is a great challenge,
because it is not straightforward to tell whether the decision
is correct or not, and how to verify it systematically. In this
paper, a Sequential MetAmoRphic Testing (SMART) framework is
proposed based on metamorphic testing, a mainstream software
testing approach. In metamorphic testing, metamorphic groups are
constructed by selecting multiple inputs according to the so-called
metamorphic relations, which are basically the system’s necessary
properties; the violation of certain relations by some corresponding
metamorphic groups implies the detection of erroneous system
behaviors. The proposed framework makes use of sequences of
metamorphic groups to understand ADS behaviors, and is applica-
ble without the need of ground-truth datasets. To demonstrate its
effectiveness, the framework is applied to test three ADS models
that steer an autonomous car in different scenarios with another
car either leading in front or approaching in the opposite direction.
The conducted experiments reveal a large number of undesirable
behaviors in these top-ranked deep learning models in the scenarios.
These counter-intuitive behaviors are associated with how the
core models of ADS respond to different positions, directions and
properties of the other car in its proximity. Further analysis of the
results helps identify critical factors affecting ADS decisions and
thus demonstrates that the framework can be used to provide a
comprehensive understanding of ADS before their deployment.

Index Terms—Automated driving systems, self-driving car, meta-

morphic testing, sequential metamorphic groups

I. INTRODUCTION

The advancement of automated driving systems (ADS) in the
last few years has been inevitably revolutionizing the transporta-
tion sector. At the full automation level, the ADS will take over
all driving tasks without the attention of human, offering a great
deal of convenience for drivers. The autonomous vehicles (AVs)
underpinned by ADS promise to decrease fuel consumptions,
reduce congestions, provide a safer transportation and increase
the mobility [1]. In addition to the modular ADS, the end-to-end
approach has emerged as an alternative trend in ADS research
[2], [3]. Having said that, the current systems are not as reliable
and robust as expected. Numerous fatal crashes due to the failure
of ADS have been reported. In 2016, the ﬁrst driver was killed
while operating his Tesla Model S in Autopilot (Tesla’s ADS)
mode just three months after the ﬁrst crash of Google’s self-
driving cars into a bus in Mountain View in February [4]. Within
two years from these accidents, the ﬁrst pedestrian was killed in

Corresponding author(s): Huai Liu.

Quang-Hung Luu, Huai Liu, Tsong Yueh Chen are with the Department of Com-
puting Technologies, Swinburne University of Technology, Hawthorn, VIC 3122,
Australia (e-mail: hluu@swin.edu.au; hliu@swin.edu.au; tychen@swin.edu.au).
Hai L. Vu, Quang-Hung Luu are with the Department of Civil Engineering,
Monash University, Clayton, VIC 3800, Australia (e-mail: hai.vu@monash.edu;
hung.luu@monash.edu).

Tempe, Arizona by an Uber test vehicle [5]. More recently, in
May 2021, a crash caused by the Tesla Model 3 killed its driver
while the Autopilot mode was in operating in Fontana, California
[6]. These fatal accidents raise scepticisms for the safety of the
state-of-the-art ADS and prohibit AVs from being accepted to
be used on-road. To gain the trustworthiness, decisions made by
ADS must be well understood and explainable before they can
be deployed [7], [8], [9], [10].

Obtaining a comprehensive understanding of the ADS is
a great challenge, because it is often not straightforward to
determine whether a decision made by the ADS is correct or not.
For example, for the same situation of a simple car-following sce-
nario involving an autonomous vehicle and another car appearing
at a certain distance ahead, system A may drive the AV to steer
rightward by 10o, whereas system B may guide it to go straight.
Under many circumstances, it is unable to determine which
decision is more correct, as well as whether both decisions are
correct or incorrect. From the testing perspective, to understand
and then explain the decisions made by a system, several tests are
required alongside a mechanism for determining whether or not
the tests have passed (for making a correct decision) or failed (for
making a wrong decision). Such a mechanism, which is referred
to as the test oracle in software testing, often does not exist in
the state-of-the-art ADS due to two main reasons. Firstly, people
may have different views on which decision can be considered
correct [11]. For example, it is debatable about whether the
ADS, while foreseeing a potential crash, should give priority
to the individual safety of the driver or to the “overall” safety
of other road users [4], [11]. Such a choice signiﬁcantly affects
the algorithm implemented and thus makes the judgement hard
to be conclusive. Secondly, most systems adopt the deep neural
networks (DNN) for object and event detection and responses
(OEDR), among other tasks. While DNN is advantageous in its
strong learning capability without supervision which allows them
to process a great amount of data [12], the decision made by such
systems is unexplainable due to several reasons, such as the bias
in the datasets used for training, the distinct patterns captured by
a DNN architecture [7], [8], [9], [10] and even the bugs existing
in the system [13].

These testing challenges can be alleviated using the Meta-
morphic Testing (MT) technique. MT has been successfully
applied to test a wide range of sophisticated systems [14], [15],
[16], [17], [18]. Its applications have been expanded beyond
testing to other purposes such as system understanding, fault
localization, program repair, and cybersecurity [19], [14], [20],
[21]. The emergence of AVs has triggered the application of
MT in assuring their safety with a number of state-of-the-art
frameworks including DeepTest [22] and DeepRoad [23]. The
use of MT helped identify thousands of erroneous behaviors
in different deep learning models driving the ADS. However,

 
 
 
 
 
 
PREPRINT

2

deeper and more profound information is yet to be explored.
More importantly, there does not exist a framework facilitating
a comprehensive system understanding of ADS. To ﬁll this gap,
a novel and effective approach is proposed for uncovering deeper
information in ADS, which has been overlooked by existing
MT-based testing techniques. In this study, a new framework
is proposed to provide a systematic understanding of ADS. The
key contributions of the present paper are as follows.

• Proposal of a novel framework (SMART) to make use of the
relationships between metamorphic groups (that is, groups
of multiple related inputs) to uncover deeper information
for the understanding of ADS’ decisions.

• Application of the framework to test three well-known ADS
models predicting steering angle, which helps reveal a larger
number of undesirable behaviors of these models.

• Development of an open source software to support the

implementation of the SMART framework.

The rest of the paper is organized as follows. In Section II,
the background of MT, the state of the art for MT in testing
the ADS and the gap in understanding the ADS decisions are
presented. Then the motivation and description of framework
are presented in Section III. The experiment setting is detailed
in Section IV, which includes deep learning models to be tested,
the metrics for checking undesirable behaviors of steering angles
and the generation of data. Experimental results are reported
in Section V before their insights are discussed in Section VI.
Finally, the paper is concluded in Section VII.

II. BACKGROUND AND LITERATURE REVIEW

In this section, the concept of MT is brieﬂy introduced before
reviewing key related studies in testing ADS. The review then
points to the importance of a systematic understanding of ADS,
the research direction that has not been fully explored, and the
reason why MT is an ideal technique to address it.

A. Preliminary of metamorphic testing

MT differs from traditional testing techniques in the sense that
it makes use of the relations between multiple executions of the
systems under test. These relations, referred to as metamorphic
relations (MRs), are derived from necessary properties of the
system. Once an MR is violated, it is known that the system is
faulty. For example, assume that G is a graph having 100 nodes
and a program P ﬁnding shortest paths P (G, a, c) between any
two nodes a and c in G. A naive process to verify this program
needs to cover against 100! possible paths. However, based on
the domain knowledge of the shortest path in a graph, one can
derive the MR “|P (G, a, c)| ≤ |P (G, a, b)| + |P (G, b, c)|” with
an arbitrary (connected) node b where | · | denotes the length of
shortest path. If the program P does not uphold this MR for a
set of nodes (a, b, c) of the graph G, it is known to be faulty.

MT can provide an alternative mechanism to the test oracle for
verifying and validating a system [24]. It can also help generate
new test cases automatically and effectively. The basic steps for
implementing MT are as follows:

1) Necessary properties of the target system are identiﬁed in
the form of relations among multiple inputs and corre-
sponding expected outputs, namely metamorphic relations
(MRs).

2) From a set of given inputs (referred to as source inputs), a
new set of inputs (namely, follow-up inputs) is constructed
using the MR. Both sets of inputs form a metamorphic
group (MG). Different MGs can be constructed using the
same MR.

3) The source outputs and the follow-up outputs are obtained
after executing the program under test with an MG. In
general, the source output may also be involved in the
construction of the follow-up input.

4) The outputs are checked against the MR to determine
whether it is violated or not. The violation of an MR
implies that the system is faulty.

two follow-up inputs are generated,

As described above, the violation of MR (thus, the failure of
the system) is determined by evaluating decisions for individual
MGs of input. In the previous graph example, from the source
input (G, a, c),
is,
(G, a, b) and (G, b, c),
to form an MG. Multiple executions
using P for this MG give us one source output |P (G, a, c)| and
two follow-up outputs |P (G, a, b)| and |P (G, b, c)|, respectively.
These outputs are then validated against the MR to check the
correctness of program or system under test.

that

Let us look at another example of autonomous vehicles.
Assume that a system controlling steering angle (SA) based on
images captured by a car’s camera is tested. An MR is deﬁned
as: the system’s decisions are robust against different weathers,
that is, the steering angle shall not change much, with respect to
changes of weathers [22]. An image capturing a driving scenario
in the sunny weather is adopted as a source input. The follow-
up input can be generated by transforming it to a new image,
keeping the same scenario but under a snowy weather. If the
SA for the snowy weather signiﬁcantly differs the SA for the
normal day, the relation does not hold; hence, system is faulty
in the sense that it is not robust enough against weathers.

B. Testing the ADS with metamorphic testing

In the existing literature, two testing strategies have been
commonly applied to test the ADS: the model-based and the
MT approaches. The former approach adopts formal model
speciﬁcations or cross-model references to determine system
faults. They are deﬁned by means of “safety cages” to determine
the operational bounds [25], “cross referencing oracles” deﬁned
by the output of majority of various models in combination with
the notion of neuron coverage to judge incorrect corner-case
behaviors [26] or frontier boundaries in which the system starts
responding undesirably [27]. This approach suffers from limita-
tions of efﬁciency, effectiveness and ﬂexibility in applications to
complex ADS systems [10].

Recently, the MT approach has gained noticeable successes in
testing complex ADS systems [22], [23], [28], [29]. Tian et al.
[22] conducted a pioneer study where an MT-based framework,
namely DeepTest, was developed. DeepTest used the image
dataset from Udacity challenge as the source inputs. They then
generated follow-up inputs from the dataset to mimic different
weather and distortable conditions. Each pair of the source
(original) image and the follow-up (transformed) image forms
an MG. Their proposed MR requires the difference between
predicted SA for each MG to be smaller than a certain predeﬁned
threshold; otherwise, the system is considered as faulty. They

PREPRINT

3

successfully revealed erroneous behaviors of three ADS models
in handling transformed inputs, including rains, fogs, lightings
or signal degradations.

Following the success of DeepTest framework, Zhang et al.
[23] devised DeepRoad, an ofﬂine framework that employed gen-
erative adversarial networks (GAN)-based techniques to generate
synthesized driving scenes, and obtain inputs that are diverse
and more realistic. Similar to Tian et al. [22], they constructed
individual MGs, each composed of a source driving image and
a follow-up image with the scene transformed by a GAN tool.
They used MRs being deﬁned such that differences between
predicted SAs for individual MGs are negligible. DeepRoad
helped uncover thousands of inconsistent behaviors in three
ADS models, and at the same time, is potential to be used for
validating the images transformed by GAN.

Recently, MT has been combined with other techniques to
tackle more challenging testing tasks for the ADS. Zhou and
Sun [29] adopted fuzzing-based MT techniques to examine the
reliability of LiDAR obstacle-perception module of Apollo, a
well-known multi-platform for self-driving cars. They found
several issues with Apollo. For example, from the source inputs
containing 3D cloud points, they generated follow-up inputs by
adding 1,000 random data points outside the region of interest
(ROI) of the Apollo. It was found after comparing the executions
for these MGs that surrounding cars currently in the ROI would
no longer be detectable. In another case, having a follow-up input
imitating a small insect ﬂying 100 meters away from the AV,
which is a simulated vehicle underpinned by the Apollo platform,
may confuse the system in detecting the pedestrian moving in
front of that car. DeepHunter is another framework integrating
MT with fuzzing [28] and is applicable to ADS. In DeepHunter,
MRs were combined with multiple extensible coverage criteria
as a strategy to generate new test cases that preserve the test
semantics.

Furthermore, it helps engineers to build better versions based
on revealed corner cases, pitfalls and failures for system under
development. For well-developed ADS, it helps obtain trusts
on decisions made and the system itself. Critical safety factors
can be used to train human to gain a better driving skill for
systems that outperform human and adopt the use of emerging
technologies. Cornelissen et al. [32] concluded that software sys-
tems “must be sufﬁciently understood before it can be properly
modiﬁed”.

To the best of our knowledge, there does not exist a framework
that provides us a satisfactory understanding of ADS. A number
of studies have been attempted to “understand” the ADS system
empirically only. Endsley [33] reported the system behavior
from end-users’ perspective by sharing her empirical driving
investigation with the Tesla Model S. The author noticed a
number of undesirable behaviors but no systematic investigation
was conducted. Teoh et al. [34] analyzed records of the Google’s
AV after its road tests but was limited to statistical analyses
and focused on occurred collisions only. On the other hand,
the state-of-the-art MT-based frameworks such as DeepTest [22],
DeepRoad [23] and DeepHunter [28] have been developed for
testing only, but not for the purpose of understanding, and hence
are unable to give us satisfactory knowledge about the system.
That is to say, none of the previous frameworks was designed
for a systematic understanding of the ADS.

In summary, although a comprehensive understanding of ADS
is of great necessity, it remains unclear how to do it systemati-
cally. MT has been shown to be an effective technique to test the
ADS, and will be demonstrated in this paper that it can be applied
for the system understanding. In the next section, the proposed
framework underpinned by MT technique will be described in
more details.

III. PROPOSED FRAMEWORK

C. Understanding of ADS and its research gaps

A. Motivation

Understanding helps users gain the in-depth knowledge about
the system [30]. It is the deeper layer of information that is
explainable by human but
is not reﬂected in the test score
counting the numbers of passed and failed test cases [7]. On the
other hand, testing is a major approach to veriﬁcation check-
ing whether the software meets the predeﬁned speciﬁcations.
Understanding and testing are strongly related in a feedback-
loop fashion where understanding helps improve the adequacy
of testing and testing facilitates understanding. However, there
is a major difference between them. A good “test score” in the
testing to reveal faults is not enough to guarantee a safe system
and increase the usability. That is to say, “incorrect” programs
may still be useful whilst non-faulty program may be impractical
[31], [30].

A number of studies [9], [7] have pointed out that parties
involved in the development and use of ADS all have beneﬁts
in understanding its behaviors. Understanding is the ﬁrst step
to help end-users build up their trust. For legal authorities,
the understanding helps resolve problems with the liability,
especially in regulating the safety requirements. By having a
sound understanding of the ADS, designers and manufacturers
are able to know their ADS’ limitations and way to improve it.

Let us start with the example illustrated in Figure 1 and
Figure 2. Suppose Autumn, one of the well-known deep learning
models in the Udacity challenge (https://github.com/udacity/self-
driving-car) is used to predict steering angle (SAs) for a given
image (acting as the source image) (Figure 1a) by the target
AV (i.e., the ego vehicle) whose ADS is under test. An MR is
adopted to generate a follow-up image representing a scenario
where there is another car at a certain distance and of a certain
color (Figure 1b) moving in front of the ego vehicle. The test
outcome of this MR requires that the predicted SA (SAf ) for the
follow-up scenario must be smaller than a threshold κ, that is
|SAs − SAf | < κ. It is similar to the criteria deﬁnition adopted
in DeepTest [22].

To this end, the MR is applied to add a red car so that
the source image and the follow-up image with the red car
form an MG, namely MG1. The (normalized) SAs predicted
by the Autumn model for the MG1 shown in Figure 2d are
SA1,s = 0.02 and SA1,f = 0.02 (positive values indicates
leftward/anticlockwise direction, negative ones represents right-
ward/clockwise tendency). With the MR violation condition
being deﬁned as of κ = 0.12, no fault is detected (i.e., the MR
satisﬁes |0.02 − 0.02| < 0.12).

PREPRINT

4

(a) Source image without car, (b) Follow-up image with red car inserted
Fig. 1.
and (c) Follow-up image with blue car inserted at the same location as the
red car in (b). White, red and blue arrows are the directions of SA predicted
by the Autumn model for (a), (b) and (c) images, respectively. (Black arrows
indicates the direction of SA from the ground truth when there is no leading
car.) Speciﬁcally, the SAs (normalized to 1) predicted by the Autumn model for
these images are (a) 0.02 leftward, (b) 0.02 leftward, and (c) 0.09 rightward.

Algorithm 1 Outline of framework’s algorithm.
Require:

inputs and user-deﬁned functions

• Source dataset: D 1
• Metamorphic relation: MR 2
• Set of conﬁgurations for SMGs: C 3
• User-deﬁned function to generate an MG from an
MR, a source datum d and a conﬁguration c: GENER-
ATEMG(MR, d, c) 4

• User-deﬁned function to obtain an outcome from the

target ADS: EXECUTEADS(d) 5

• User-deﬁned function to determine undesirable behav-
iors from an MR, data s, f and a conﬁguration c: DETER-
MINEUB(MR, s, f, c) 6

Data generation: given a metamorphic relation MR, the data

ds and a conﬁguration C
function GENERATESMG(MR, ds, C)

G ← ∅
for c ∈ C do

(cid:46) 7
(cid:46) A sequence of MG

df ← GENERATEMG(MR, ds, c)
G ← G ∪ {df }

end for
return G
end function

System execution: given a metamorphic relation MR, a

dataset D and a conﬁguration C
function TESTING(MR, D, C)

(cid:46) 8
(cid:46) Set of testing outcomes

(cid:46) Set of scenarios
(cid:46) Set of undesirable behaviors

T ← ∅
for ds ∈ D do
S ← ∅
U ← ∅
G ← GENERATESMG(MR, ds, C)
os ← EXECUTEADS(ds)
for (df , c) ∈ (G, C) do

(cid:46) Source SA

of ← EXECUTEADS(df )
u ← DETERMINEUB(MR, os, of , c)
S ← S ∪ {(ds, c)}
U ← U ∪ {u}

(cid:46) Follow-ups SAs

end for
T ← T ∪ {(S, U )}

end for
return T
end function

Fig. 2.
The illustration of process for generating and making further use of
sequences of Metamorphic Groups (MG) with SMART. A source (original) image
and a follow-up image generated by adding a leading car red using an MR form
an MG, namely MG1. This MR is used to generate an image with a leading
blue car in front, combining with the source image to construct the MG2. In the
diagram, single-headed arrows indicate the generation of the follow-up image
from the original one by the chosen MR and black double-headed arrows depict
the MR validation from the SA outputs. Green areas denote the relationship
between MG1 and MG2, and their corresponding outputs, which can be further
utilized to detect the system’s undesirableness.

In the following, one may be interested in knowing whether
the color of the added car affects the decision by such an ADS
system. For this reason, the MR is applied with a blue car
(Figure 1c). The original image and a follow-up image with the
blue car with exactly the same position and size form another
MG, namely MG2. Again, the MR is satisﬁed with predicted
SAs for MG2 (SA2,s = 0.02 and SA2,f = −0.09, that is
|0.02 − (−0.09)| < 0.12). Hence, no violation is detected, and
one can argue that adding the blue car does not affect the decision
because the system again only checks if the leading car is at a
safe distance, and hence decides to steer right.

However, despite no violation is detected with these MGs, it
is known that there is something undesirable with this system.
The color of the leading car yields a signiﬁcant change in the
steering angle from left to right. It is argued that decision of a
robust system should be consistent regardless of the color of the
leading car. This undesirable behavior has only been revealed
when MG1 and MG2 above are compared in a sequence. For
this reason, a new framework is proposed in this paper to make
use of sequences of MGs to conduct a systematic and in-depth
testing and understanding of the ADS. In contrast to the existing
methods such as DeepTest that separately check the MR violation
of each individual MG (to determine erroneous behaviors), our
new framework investigate the behaviors of a sequence of MGs
in a collective way, as detailed in the following sections.

B. Sequential Metamorphic Testing (SMART) Framework

1) The framework: The proposed Sequential MetAmoRphic
Testing (SMART) framework consists of four main components,
as shown in Figure 3. First,
it contains a set of MRs that
facilitates the automatic generation of test cases and veriﬁcation
of their execution results. Second, SMART includes categories
of MGs in a certain sequence generated from each MR. Third,
quantitative metrics are deﬁned to determine the degree of unde-
sirable behaviors in the sequences of MGs. Last, the framework
has a collection of necessary tools for test case generation,
testing execution, and understanding of the ADS systems. The
algorithm of SMART is presented in Algorithm 1. Its source code
is accessible at: https://github.com/luuqh/smart.

2) Sequences of Metamorphic Groups: Given an MR and a
set of source inputs, different MGs can be generated. In the

PREPRINT

5

Fig. 3. The SMART framework works without the need for ground truth data. The source input data can be used to generate different follow-up input data, forming
sequences of metamorphic groups. They are executed by the ADS to produce corresponding outputs. Inputs and outputs of each SMG can be further examined to
detect undesirable behaviors. Circled numbers are deﬁned in Algorithm 1.

SMART, MGs are designed and generated in a sequential way
which have a rich relationship between them. Figure 2 illustrates
the progress for generating different MGs. While the proposed
MR (Figure 1) may not be helpful in detecting any violation
in the outputs, making use of the relationship between MGs
may reveal undesirable behaviors of the system (Figure 2) as
explained previously. The guideline for designing a sequence of
metamorphic groups (SMG) is given as follows:

1) Determine the feature (or aspect) of the ADS for the
understanding. For example, if it is necessary to understand
the ADS’ behaviors in response to the positions of car
moving ahead, the positions can be considered as a feature.
2) Work out the range of values of each feature. For instance,
the range of positions can be within the vision of AV.
3) Divide the range into partitions. One naive way of par-
titioning is to divide it equally or proportionally. For
example, the range can be divided into 400, 300, 200,
100 or zero pixels left or right away from the center
of the car moving ahead. Note that different granularity
levels of the partitioning refer to different precisions of
the understanding.

4) Construct the SMG by creating one MG corresponding to

each partition derived in Step 3.

What makes an SMG different from a regular set of MGs is
the sequential way the MGs are implemented. By considering
them in the sequential order, we can examine the variations of
the undesirableness from a speciﬁc perspective instead of con-
sidering a whole set but without a particular order. For example,
we can reveal at which position, scenario, or characteristics the
system will respond undesirably.

C. Undesirable behavior metrics

An undesirable behavior is a behavior that

is considered
abnormal by the system’s user. In most cases, the metric M for
an undesirable behavior can be deﬁned by the common sense
for a certain driving scenario. In the example above, when the
system decides to steer leftward for the leading red car in MG1

and rightward for the leading blue car in MG2, this abnormal
case might be referred to as an undesirable behavior, measuring
by the difference between their corresponding steering angles.
When the difference is larger than a certain threshold θ, the
Heaviside step function is adopted to determine an undesirable
behavior, that is

U =

(cid:40)

1,
0,

M > θ
otherwise

(1)

where 1 is to assert that it is an undesirable behavior, and 0 is
to indicate otherwise or inclusiveness. In general, it is up to the
system’s user to decide whether a behavior is undesirable or not
by selecting the metric M and the threshold θ. The metrics are
not only to justify whether or not a scenario is an undesirable
behavior, but also to measure the degree of undesirableness.

IV. EXPERIMENT

To demonstrate the effectiveness of SMART, experiments are
conducted with deep learning models used to predict steering
angles (SA). After the introduction of models to be tested, the
metric of undesirable behaviors will be deﬁned which will later
help better understand the systems under test. The method to
generate a speciﬁc sequence of MGs from selected MRs and the
adoption of the metric for assessment are presented.

A. ADS systems under test

To evaluate the proposed SMART framework, the experiments
adopt deep learning models that are in the leaderboards of
the Udacity Self-Driving Car Challenge 2 and have been used
in earlier studies [22], [23]. Among them, three models that
have both source code and supporting data (trained weights,
parameters) available, namely Chauffeur, Rambo and Autumn,
are used. Given the set of input images, their output is the SA.
Note that the SA decision to be tested in this study is only based
on object detection from the traditional two-dimensional camera
images without any depth information or perception that would
be available from stereo cameras or LiDAR sensors.

PREPRINT

6

TABLE II
SMGS GENERATED AND USED IN THIS STUDY.

Name

Description

SMG1

Forward
moving car

SMG2

Approaching
car

SMG3

SMG4

Different
leading car
Combined
car & weather

Conﬁguration for generating
follow-up photos
left 400 pixels, left 300 pixels,

left 200 pixels, left 100 pixels,
center, right 100 pixels,
right 200 pixels, right 300
pixels, right 400 pixels

Reference
MG
center

left 400 pixels, left 300 pixels,

center

left 200 pixels, left 100 pixels,
center, right 100 pixels,
right 200 pixels, right 300
pixels, right 400 pixels

red, blue, grey

car, car+snow:0.2, car+snow:0.4,
car+snow:0.6, car:snow:0.8,
car+snow:1.0

red

car

Fig. 4.
Comparison between (a) Chauffeur model performance against the
ground truth; and (b) Chauffeur model prediction without any car ahead during
the trip and the one with the car. Dashed black line indicates the line of perfect
ﬁt where model prediction is exactly the same with the ground truth. Green color
indicates higher density of points.

TABLE I
PERFORMANCE OF SELECTED ADS MODELS

Model
mean absolute errors (MAE)
root mean square error (RMSE)
Pearson’s correlation (CORR)
standard deviation (STDEV)

Chauffeur
0.055
0.092
0.910
0.091

Rambo
0.074
0.101
0.885
0.100

Autumn
0.050
0.098
0.884
0.098

its follow-up one form an MG. Assume the model prediction
returns two values, SAs and SAf . The SAs could be used as
the reference to obtain the SA difference, and use it to validate
the MR later. This difference is deﬁned as

The overall architectures of both Chauffeur and Autumn
models consist of a convolutional neural network (CNN) and
a recurrent neural network (RNN). The CNN is used to extract
dominant features from images, and Long Short-Term Memory
(LSTM), a commonly used RNN, to predict the SA. Meanwhile,
Rambo combines three different CNN streams for directly pre-
dicting the SA.

A Conda working environment that contains Python, Ten-
sorFlow, Keras, OpenCV and supporting libraries are built. To
maintain the computability of Udacity models, the speciﬁc build
(TensorFlow 1.15, Keras 1.2.2) is adopted. The experiments are
carried out using the Swinburne Supercomputer (OZSTAR1).
Each of its compute nodes contains two Intel Gold 6140 18-
core processors, two NVIDIA P100 12GB PCIe GPUs, 192 GB
DDR4 RAM and 400 GB local SSD.

The performance of these three models are presented in
Table I. The mean absolute errors (MSE) of Chauffeur and
Rambo were 0.055 and 0.074, respectively, which are similar
to the results reported earlier [22]. The performance of Autumn
model is also in the same order of magnitude as other two
models. The Pearson’s correlation coefﬁcients for all of them
are high, in the range of 0.884 to 0.910. These data imply that
these models are well conﬁgured and could be used for further
investigation.

B. Undesirable behavior metrics for steering angle

The notion of steering angle difference (AD) is used to
facilitate the evaluation of undesirable behavior metrics. AD is
deﬁned as the difference between a certain SA and a reference
SA∗, that is,

AD = SA − SA∗

(2)

To evaluate the model, follow-up images representing sce-
narios in a certain way are generated. The source image and

1https://supercomputing.swin.edu.au/ozstar/

AD = SAf − SAs

(3)

Other frameworks, such as DeepTest, consider an MR to be
violated when |AD| > κ for this MG. The proposed framework
can further help unveil undesirable behaviors of the system,
which works even in case there is no violation detected in
individual MRs which are the basis for conventional MT-based
frameworks. It
is a result of harnessing MGs from a new
perspective for determining the violation. For instance, given
AD1 and AD2 computed from the outputs of MG1 and MG2 in
the example (Figure 2). If the inputs are the same or have similar
semantics, it is expected that AD1 and AD2 have a marginal
difference. Otherwise, there is an undesirable behavior detected.
From an MR, multiple SMGs can be generated in a certain
way. As a result, a set of ADs could be obtained, each of which
is associated with an MG. For the ease of comparison, one of
them can be selected as a reference, being denoted by AD∗.
The change of AD against this baseline can be compared to
determine the degree of undesirable behaviors. Quantitatively,
the following metrics are used.

The unchange of steering metric (UN) is to capture the notion

of no change in steering, that is,

MN =

1
4

|AD − AD∗|

(4)

The rightward change of steering metric (MR) is to measure

the rightward deviation of steering, that is,

MR =

(cid:40) 1

4 |AD − AD∗|,
0,

if AD < AD∗
otherwise

(5)

The leftward change of steering metric (ML) is to measure

the leftward deviation of steering, that is,

ML =

(cid:40) 1

4 |AD − AD∗|,
0,

if AD > AD∗
otherwise

(6)

PREPRINT

7

• The metric MR is applied to measure the undesirable
behavior of the ADS when it steers rightward (clockwise)
in response to the approaching car on the right (Figure 5g).
3) SMG3: Different leading car: The follow-up input If is an
image being generated by adding a forward moving car, either
in red (Figure 5b), blue (Figure 5h) or grey color.

• The metric MN is applied to measure the undesirable behav-
ior of the ADS when it steers either leftward (anticlockwise)
or rightward (clockwise) in response to the leading car with
different colors.

4) SMG4: Combined moving car and weather: The follow-
up input If is an image being generated by adding a car and
transforming the scene into a snowy weather with different
intensities (Figure 5i).

• The metric MN is applied to measure the undesirable
behavior of the ADS when it steers differently against the
scenario with a snowy weather.

For a complete list of experiments, please refer to the sup-
plementary materials and the framework’s public repository
(https://github.com/luuqh/smart/).

Let us look at an example. In the given dataset, the ego car
is driving in a single-lane road. In this scenario, the behavior
of an artiﬁcial car would be undesirable and hazardous if it is
on the centre of a dividing line. In response to that, the ego car
may decide to go straight if it perceives that the distance is safe
to do so, or to steer left to avoid a potential collision. However,
if it steers to the right, its tendency is to move outside of the
current lane and into the roadside areas, which is an undesirable
behavior.

D. Test case generations and evaluations

The datasets are obtained from the Udacity where images
are extracted from cameras placed behind the wind-shield of
the ego car, and SA of the human driver was recorded. Three
models are pre-trained with the CH_001 dataset consisting of
101,397 photos and labelled SA, covering a wide range of
driving conditions. This study adopts the CH_002 dataset, the
same set being used for evaluating the performance of models
participating in the competition, as the source inputs. It consists
of 5,615 photos captured by the center camera in a resolution of
640 × 480 pixels. To generate the sequential follow-up inputs,
a package based on the OpenCV library is developed, which
enables rescaling and inserting a chosen object to any location
in the original photo. It
is used to generate four SMGs as
in Table II. To examine the feasibility and robustness of the
generated datasets, all generated images have been carefully
inspected one by one to ensure that
they are realistic. For
example, when the image shown in Figure 5c was generated,
it was observed that the added artiﬁcial car actually blocks the
view the car in source image. After examining these two cars’
different sizes and positions, it could be conﬁrmed that they were
not overlapped, that is, it is still realistic and feasible to have the
artiﬁcial car in Figure 5c, which happens to make part of the
car in source image invisible to the ego car. Two thresholds are
used to evaluate the models. With the low threshold θ = 0, it
is aimed to reveal all possible undesirable behaviors. For the
high threshold θ = 0.02 (which is the same order as of 25% of
standard deviations as in Table I), the main focus is on severe
undesirable behaviors only.

Fig. 5. Snapshot of images from MGs: (a) Source image; and follow-up images
with (b) forward (red) car, (c) forward left-side car, (d) right-side car, (e) facing
car, (f) facing left-side car, (g) facing right-side car, (h) forward blue car, and
(i) forward car under a snowy weather. The blue and red arrows shows expected
leftward and rightward steering angles AD, respectively, in relative to AD∗.

It is up to the users to deﬁne which metric should be adopted
in a certain driving scenario. In this study, only one metric is
used at a time for a scenario. Since AD and AD∗ are in the
range [−2, 2] for normalized SAs (between −1 and 1 [22]), the
metrics (MN, MR and ML) are in the range of [0, 1] with the
adjustment of the fraction 1/4. Here, 1 and 0 indicate the highest
and the lowest degrees, respectively, associated with the (chosen
undesirable) behaviors.

The steering angle for the source image at a certain timeframe
is used as a reference SA∗ . The reference AD∗ varies, subject
to the MGs. It is listed in the last column of Table II.

C. Sequences of Metamorphic Groups

Given an MR to generate a car image at a certain distance
ahead and the source input Is consisting of a source image
(Figure 5a). The follow-up image If (Figure 5b) is generated
using the MR, forming an MG (Is, If ). AD is obtained from the
outputs of ADS after its execution. From the same source image,
the follow-up images are generated in a horizontal or a certain
sequence (Figure 5), obtaining a set of ADs. Among them, AD∗
is selected for each SMG as shown in Table II, and then deﬁned
the undesirable behaviors. The details are as follows.

1) SMG1: Forward moving car: The follow-up input If is
an image being generated by adding a forward moving car to
the source image, either on the left (Figure 5c), in the middle
(Figure 5b) or on the right (Figure 5d).

• The metric ML is applied to measure the undesirable
behavior of the ADS when it steers leftward (anticlockwise)
in response to the forward car on the left (Figure 5c).
• The metric MR is applied to measure the undesirable
behavior of the ADS when it steers rightward (clockwise) in
response to the forward moving car on the right (Figure 5d).
2) SMG2: Approaching car: The follow-up input If is an
image being generated by adding an approaching car in front
of the ego car, either on the left (Figure 5e), in the middle
(Figure 5f) or on the right (Figure 5g).

• The metric ML is applied to measure the undesirable
behavior of the ADS when it steers leftward (anticlockwise)
in response to the approaching car on the left (Figure 5f).

PREPRINT

8

V. RESULT AND DISCUSSION

A. The effectiveness of framework in uncovering undesirable
behaviors of the ADS

The challenge is that in general, there is no way to determine
whether or not the decision made by model is correct or wrong,
which implies whether or not there is an issue in the ADS system.
A statistical comparison (Figure 4a) between the model predic-
tion (Figure 2a, white arrow) and the ground truth (Figure 2a,
black arrow) may not be helpful because the difference is often
attributed to model performance. A similar statistical analysis
shown in Figure 4b may not be afﬁrmative to determine whether
there is a fault in the ADS. Alternatively, one may use a scenario
with a car moving forward in front of the ego car (Figure 5b) to
see how the ADS responses against the original scenario where
there is no car ahead (Figure 5a). One may argue that the SA
difference between the scenario with a car ahead and the one
without it can be referred to as the impact of the object.

With the SMART framework, the behaviors of the systems can
be examined in a ﬁne-grained scale. Figure 6a depicts a heatmap
representing the “raw” predicted SA of Chauffeur model for
different positions of a car moving forward. From the heatmap,
scenarios and time where the ADS decides to steer leftward,
manoeuvre to the right or go straight (presenting by blue, red
and grey colors in Figure 6a, respectively) can be compared. Fur-
thermore, by adopting the reference MG in constructing SMGs
(Table II), the relative difference between MGs within SMGs is
obtainable. The main product of the framework is the heatmap
displaying the degree of undesirable behaviors (Figure 6b). From
the map, one could locate a subset (by means of timeframe) of
the dataset that has a higher degree of undesirable behaviors
(highlighted by red rectangles in Figure 6b), which is referred
to as the explanation-by-examples [35]. On the other hand, it is
possible to identify which feature in the image associated with
the car that has the higher inﬂuence on the undesirable behaviors,
what is referred to as explanation-by-features [35]. For example,
the Chauffeur model has more issues when the moving car is at
the rightmost position (right-400 in Figure 6b) other than it is
closer to the center (right-100 in Figure 6b).

The sequential combination of SMGs helps better understand
the undesirableness of ADS. Table III depicts the summary of
undesirable behaviors revealed for Chauffeur model in response
to the car. The ﬁgure shows that our SMART framework could
detect the undesirableness in the model for different scenarios
corresponding to different positions of the car moving forward
(SMG1), either in the left or in the right. The degrees of
undesirable behaviors are surprisingly high, ranging from 30.3%
to 55.0% (with an average of 43.2%) for the low threshold
(θ = 0) and from 4.4%–11.7% (with an average of 8.0%)
for the high threshold (θ = 0.02) (Table III). In other words,
from 5,615 original photos, total numbers of 19,420 and 3,610
scenarios associated with low and high thresholds, respectively,
of undesirable behaviors in the Chauffeur model are identiﬁable
just by examining the SMG1. These high percentages indicate
that the model is much more problematic than one would expect,
highlighting the effectiveness of our framework.

Similar high effectiveness of SMART in detecting undesirable
behaviors of the model could be observed in other SMGs. For
example, SMG2 contains different scenarios, each corresponding

TABLE III
UNDESIRABLE BEHAVIORS OF DIFFERENT MODELS (STRAIGHT ROADS
ONLY).

Model
Threshold
SMG1
left-400
left-300
left-200
left-100
left
right-100
right-200
right-300
right-400
right
SMG2
left-400
left-300
left-200
left-100
left
right-100
right-200
right-300
right-400
right
SMG3
car-blue
car-grey
SMG4
car+snow:0.2
car+snow:0.4
car+snow:0.6
car+snow:0.8
car+snow:1.0

Chauffeur

Autumn
0.02
0
4.9% 45.4%
1.9% 39.3%
5.9% 49.8%

2.9% 58.4%
6.0% 50.0%
3.6% 53.1%
5.4% 46.5%
4.3% 34.4%
2.0% 28.8%
3.8% 40.7%
5.2% 47.8%
2.1% 40.9%
7.1% 48.0%

Rambo
0.02
0.02
0
0
5.2%
8.0% 45.0%
43.2%
3.5%
4.4% 45.3%
30.3%
6.4% 54.5%
8.4%
34.6%
6.5% 62.9% 13.4% 52.5% 15.1%
33.7%
4.2%
8.3% 59.2%
39.1%
7.8%
34.4%
6.4% 55.5%
3.9%
55.0% 11.7% 42.9%
3.1%
8.1% 40.6%
47.5%
1.8%
50.7%
8.0% 19.5%
1.5%
55.0% 10.9% 35.2%
2.6%
52.1%
9.7% 34.6%
6.6%
35.5% 13.2% 46.5%
3.6%
39.9% 11.5% 44.7%
35.6% 13.6% 54.8%
5.9%
30.7% 12.4% 62.4% 13.4% 58.7% 21.9%
3.3% 66.4%
5.7%
41.6% 15.3% 58.1%
6.5% 53.5%
9.3%
37.0% 13.2% 55.0%
3.1% 55.5%
4.5%
30.3%
8.1% 38.4%
5.7% 47.7%
4.3%
28.1% 12.9% 40.3%
4.7% 33.7%
4.0%
36.0% 15.4% 39.6%
1.8% 31.2%
2.7%
41.5% 16.1% 33.7%
3.8% 42.0%
3.9%
34.0% 13.1% 38.0%
0.1% 65.6%
0.6%
75.9% 18.9% 75.8%
0.5%
0.0% 65.4%
75.9% 26.6% 75.8%
0.7%
75.9% 11.2% 75.8%
0.2% 65.7%
1.7%
75.9% 35.8% 75.8% 15.0% 70.8%
1.2%
75.9% 38.6% 75.8% 15.8% 68.1%
2.2%
75.9% 38.8% 75.8% 15.0% 74.3%
2.1%
75.9% 33.7% 75.8% 13.8% 74.3%
1.2%
75.9% 33.2% 75.8% 14.4% 68.5%
1.7%
75.9% 34.6% 75.8% 15.8% 69.0%

to a position of the approaching car. The degrees of undesirable
behaviors of SMG2 are slightly lower than that of SMG1
(Table III) for the low threshold, ranging from 30.3% to 41.6%.
However, more severe issues are found in the facing car for the
high threshold, with values ranging 4.4%–11.7% (Table III) for
SMG2. The highest effectiveness found among all SMGs exam-
ined is with the SMG3 and SMG4. While replacing the red car
by either blue or grey car, the degrees of undesirable behaviors
for the low threshold are as high as 75.9% (Table III). For the
high threshold SMART reveals that 11.2%–38.8% (Table III) of
all decisions made by the Chauffeur model are undesirable.

In brief, SMART is highly effective in revealing undesirable
behaviors. It helps address 30.3%–75.9% of undesirable scenar-
ios for the low threshold and 4.4%–38.8% for the high threshold
with four SMGs, equivalent to several thousands of undesirable
scenarios.

B. Applicability of the framework in distinguishing different ADS
systems

The performance of SMART on three different ADS models,
namely Chauffeur, Rambo and Autumn, are compared. First, it
is found that SMART not only is highly effective in revealing
undesirable behaviors with Chauffeur model, but also performs
well for Rambo and Autumn models. In response to the car
moving forward (SMG1), the Rambo has a ratio of 19.5%–
62.9% undesirableness among all scenarios tested (Table III).
It is also indicated that 28.8%–58.4% predictions using the low
threshold by Autumn model are undesirable (Table III). They

PREPRINT

9

(a) Heatmap of raw steering angles of Chauffeur model during the trip for different positions of a car moving forward (SMG1). Blue color represents
Fig. 6.
leftward SA whilst red color shows rightward SA. Green hatch areas highlight SAs for original images. (b) Heatmap of undesirable behaviors of Chauffeur model
during the trip as from the examination of SMG1. Purple color represents the high degree of undesirable behaviors. Grey areas highlight (24.1%) road segments
that are curved (deﬁned by SA > 0.2 from ground truth data). Red rectangles highlight some notable regions with high degree of undesirable behaviors.

are in the same order of magnitude as of Chauffeur model
(30.3%–55.0%) (Table III). For the high threshold, the degrees of
undesirable behaviors for Rambo and Autumn models are in the
ranges of 1.9%–13.4% and 1.5%–15.1%, respectively (Table III),
which are comparable to the Chauffeur model. Similar results
are observed in other SMGs. For the SMG2 (related to the
approaching car), SMART reveals the same order of undesirable
behaviors in the Rambo and the Autumn models in comparison
with the Chauffeur model (Table III). For the SMG3 (on having
an alternative appearance for the leading car) and SMG4, SMART
can reveal a high percentage of issues (Table III). These results
demonstrate that the effectiveness of SMART is consistent across
different models.

Having said that, SMART can distinguish ﬁne details in
the responses of different models. For example, the Chauffeur
model has more severe undesirable behaviors (that is, having
a larger number of undesirable behaviors at the high threshold
in comparison with other models) for the scenario with the car
moving forward (SMG1) and positioned 100 pixels rightward
(Table III) compared to other positions. In contrast, both Rambo
and Autumn models are prone to issues in which the car is
moving forward and positioned 200 pixels leftward (Table III).
For the SMG2 (Table III), both Rambo and Autumn have the
highest percentage of undesirableness for the high threshold at
the car positioned 200 pixels leftward, being 13.4% and 21.9%,
respectively. In comparison, the Chauffeur model has a high
percentage (>10%) for most MGs. For the SMG3 (Table III),

only the Chauffeur model has a high degree of undesirable
behaviors for both cases (11.2%–26.6%). Other two models have
a low percentage (<1%) of undesirable behaviors. For the SMG4,
Chauffeur is still the least robust model, whilst the best one is
Autumn model, especially for the high threshold (Table III).

In a word, SMART remains highly effective in revealing
undesirable behaviors of different models and can be used to
distinguish them in a ﬁne-grained scale.

VI. INSIGHTS INTO ADS’ DECISION-MAKING PROCESS

The SMART framework helps reveal different critical factors
and features affecting the ADS. As a result, it may help gain
deeper insights into their decisions, some of which have never
been provided by the other traditional MT-based techniques.
Particularly, the proposed SMART framework helps uncover the
(un)favorable factors, the extent of responses to hazardous sce-
narios, the robustness and consistency and new critical features,
as discussed in the following.
A. Determination of (un)favorable factors of each ADS

The framework enables to, for example, examine whether
or not
there is any (un)favorable factor in the decision for
scenarios related to different positions of the car moving ahead
for each ADS model. It has been found that in responding to
the car moving forward (SMG1), the Chauffeur model has more
undesirable behaviors if the car ahead is in the right side of
the ego car other than in the left side (Table III). In details,
the average of undesirable behaviors of the left side-scenarios is

PREPRINT

10

34.4%, which is smaller than the right-side value as of 52.1%
(Table III). In contrast, both the Autumn and the Rambo models
have more undesirable behaviors for the forward moving car
on the right side whose averages values are 20.9% and 9.3%,
respectively (for the low threshold, Table III). In other words,
the Chauffeur model tends to have more issues on the forward
moving car on the right, compared to the Rambo and Autumn
models which contain more undesirableness on the left.

B. Identifying the extent of response to hazardous scenarios

It is unknown whether or not speciﬁc hazardous scenarios will
cause undesirable behaviors in the ADS and, if it is the case,
to what extent. It is the aspect that has not been investigated in
existing MT-based frameworks. In our framework, for instance, a
comparison of results from the sequence of scenarios correspond-
ing to different positions of the approaching car can provide
such information. In particular, it is found that all three models
have the same severe problem with the scenario for the car
moving forward at the position of 200 pixels leftward (Table III).
Similar undesirable issues are observed with the MGs related
to the scenario for the approaching car at the position of 200
pixels leftward (Table III). This position is associated with the
hazardous scenario where the car is on the continuous dividing
lines between two opposite lanes. All models have struggled to
make good decisions for this scenario. An obvious suggestion is
that the future ADS model should be trained to cope with such
unusual but risky scenarios.

C. Inspecting the robustness against a certain feature

It is important to ensure that the ADS decisions are robust and
consistent against a feature, which has not been paid enough
attention in existing MT-based frameworks. For instance, the
robustness of a model against the change of appearance could
be examined by our frameworks. It has been found that the
decisions made by both the Rambo and the Autumn models
are quite consistent. There are undesirable behaviors for the
low threshold, but only a low percentage (< 1%) of severe
undesirable behaviors found in both models (Table III). The
result indicates that these two models are quite robust against
different appearance of the car. Meanwhile,
the decision of
Chauffeur is sensitive to the colors of the car with more severe
undesirable behaviors (11.2%–26.6% in Table III). In a word,
the characteristics of object are a critical factor affecting the
robustness of the decision of a certain ADS.

D. Obtaining new critical features for a better understanding

The proposed framework allows to reveal new critical features
affecting their decisions that are not currently available in
existing MT-based frameworks. For example, it is now clear
that the Chauffeur model is sensitive to the colour of the car
(Table III), which has never been disclosed. In addition, the
scenarios that combine a snowy weather with a car in front could
lead to different undesirable responses (for high threshold) in
different models (Table III), which is also revealed for the ﬁrst
time. Experiments in this study help understand three models
just with simple test cases, demonstrating that the proposed
framework can work effectively. End users can adopt SMART
to propose more sophisticated test cases and reveal new features
for their interest.

VII. CONCLUSION

With more than one billion cars on the road worldwide
being replaced by ADS in the future, any potentially unsafe
issues should be well understood, and attended to prior to
production and deployment. To enable that, this paper proposes
a novel framework (SMART) to uncover undesirable behaviors
of the ADS. Underpinning by the metamorphic testing, SMART
is applicable even when there are no ground-truth data. To
demonstrate the capability of the proposed framework, SMART
is applied to identify and measure undesirable behaviors of three
deep learning models developed during the Udacity competition
to predict steering angles. As observed from the experimen-
tal results, the SMART framework, through exploring in-depth
information hidden in a sequence of MGs, is highly effective
in discovering more undesirable behaviors of ADS on top of
those already detected by existing techniques that refer to the
test outcomes of individual MGs separately. As a result, the
framework delivers a deeper and more comprehensive under-
standing of ADS. Further investigations of the results also help
uncover and distinguish some critical characteristics of different
ADS models. Furthermore, SMART helps identify critical factors
associated with positions, side and properties of the cars that
affect the performance of ADS in our experiment. To this end,
the study demonstrates the applicability and effectiveness of
sequences of MGs in providing a comprehensive understanding
of ADS and how the framework may help understand the ADS.
A guideline on how to construct SMGs for understanding the
ADS is presented.

The study suggests that SMART can be used to test and
understand complex ADS systems prior to their deployment
to minimize potential safety-critical problems. In future work,
SMART can be optimized and used to have a full and comprehen-
sive understanding of some state-of-the-art ADS systems. The
proposed framework has potentials for a broad applicability. The
key idea embedded within the framework is the use of sequential
MGs in revealing undesirable behaviors of the system under test.
A preliminary MT-based testing with Baidu Apollo has been
conducted [36]. The extension to the modular ADS is part of
our ongoing tasks for the next version of the framework. This
work will not be technically difﬁcult but will certainly require a
close collaboration with the ADS system developers.

ACKNOWLEDGEMENT

This project is supported by the grant DP210102447 from
Australian Research Council. We thank Swinburne Supercom-
puting Center for providing experimental facilities.

REFERENCES

[1] J. M. Anderson, N. Kalra, K. D. Stanley, P. Sorensen, C. Samaras, and T. A.
Oluwatola, Autonomous Vehicle Technology: A Guide for Policymakers.
Santa Monica, CA: RAND Corporation, 2016.

[2] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A survey of
autonomous driving: Common practices and emerging technologies,” IEEE
Access, vol. 8, pp. 58 443–58 469, 2020.

[3] A. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad, “A
survey of end-to-end driving: Architectures and training methods,” IEEE
Transactions on Neural Networks and Learning Systems, pp. 1–21, 2020.
[4] S. Nyholm, “The ethics of crashes with self-driving cars: A roadmap, I,”

Philosophy Compass, vol. 13, no. 7, pp. e12 507(1–10), 2018.

PREPRINT

11

[27] V. Riccio and P. Tonella, “Model-based exploration of the frontier of
behaviours for deep learning system testing,” Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pp. 876–888,
2020.

[28] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao, B. Li,
J. Yin, and S. See, “DeepHunter: A coverage-guided fuzz testing framework
for deep neural networks,” in ISSTA 2019: Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. New
York, NY, USA: Association for Computing Machinery, 2019, p. 146–157.
[29] Z. Q. Zhou and L. Sun, “Metamorphic testing of driverless cars,” Commu-

nications of ACM, vol. 62, no. 3, pp. 61–67, Feb. 2019.

[30] Z. Q. Zhou, L. Sun, T. Y. Chen, and D. Towey, “Metamorphic relations for
enhancing system understanding and use,” IEEE Transactions on Software
Engineering, vol. 46, no. 10, pp. 1120–1154, 2020.

[31] M. Lehman, “Programs,

life cycles, and laws of software evolution,”

Proceedings of the IEEE, vol. 68, no. 9, pp. 1060–1076, 1980.

[32] B. Cornelissen, A. Zaidman, A. van Deursen, L. Moonen, and R. Koschke,
“A systematic survey of program comprehension through dynamic analy-
sis,” IEEE Transactions on Software Engineering, vol. 35, no. 5, pp. 684–
702, 2009.

[33] M. R. Endsley, “Autonomous driving systems: A preliminary naturalistic
study of the Tesla Model S,” Journal of Cognitive Engineering and Decision
Making, vol. 11, no. 3, pp. 225–238, 2017.

[34] E. R. Teoh and D. G. Kidd, “Rage against the machine? Google’s self-
driving cars versus human drivers,” Journal of Safety Research, vol. 63,
pp. 57–60, 2017.

[35] S. C. Yang, T. Folke, and P. Shafto, “Abstraction, validation, and general-
ization for explainable artiﬁcial intelligence,” arXiv, vol. abs/2105.07508,
pp. 1–12, 2021.

[36] J. Seymour, D.-T.-C. Ho, and Q.-H. Luu, “An empirical testing of au-
tonomous vehicle simulator system for urban driving,” in 2021 IEEE
International Conference on Artiﬁcial Intelligence Testing (AITest), 2021,
pp. 111–117.

[5] S. Levin and J. C. Wong, “Self-driving Uber kills Arizona woman
pp.

in ﬁrst
https://www.theguardian.com/technology/2018/mar/19/uber–self–driving–
car–kills–woman–arizona–tempe (Accessed 31 May 2021), 2018.

pedestrian,” The Guardian,

involving

crash

fatal

on

“Tesla driver killed, 2 seriously hurt
210

[6] S. CBSLA,
Wreck
https://losangeles.cbslocal.com/2021/05/05/tesla–driver–killed–big–rig–
wreck–on–210–freeway–in–fontana–good–samaritan–hurt/
May 2021), 2021.

in Fontana,” CBS Los Angeles,

freeway

in Big-Rig
pp.

(Accessed 31

[7] É. Zablocki, H. Ben-Younes, P. Pérez, and M. Cord, “Explainability of
vision-based autonomous driving systems: Review and challenges,” arXiv,
vol. abs/2101.05307, pp. 1–36, 2021.

[8] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and
X. Yi, “A survey of safety and trustworthiness of deep neural networks:
Veriﬁcation, testing, adversarial attack and defence, and interpretability,”
Computer Science Review, vol. 37, pp. 100 270(1–35), 2020.

[9] O. Benchekroun, A. Rahimi, Q. Zhang, and T. Kodliuk, “The need for
standardized explainability,” arXiv, vol. abs/2010.11273, pp. 1–7, 2020.

[10] N. Rajabli, F. Flammini, R. Nardone, and V. Vittorini, “Software veriﬁca-
tion and validation of safe autonomous cars: A systematic literature review,”
IEEE Access, vol. 9, pp. 4797–4819, 2021.

[11] E. Awad, S. Dsouza, R. Kim, J. Schulz, J. Henrich, A. Shariff, J.-F.
Bonnefon, and I. Rahwan, “The moral machine experiment,” Nature, vol.
563, no. 7729, pp. 59–64, Nov 2018.

[12] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,

2016.

[13] J. Garcia, Y. Feng, J. Shen, S. Almanee, Y. Xia, and Q. A. Chen, “A
comprehensive study of autonomous vehicle bugs,” in 2020 IEEE/ACM
42nd International Conference on Software Engineering (ICSE), 2020, pp.
385–396.

[14] T. Y. Chen, F. C. Kuo, H. Liu, P. Poon, D. Towey, T. H. Tse, and Z. Q.
Zhou, “Metamorphic testing: A review of challenges and opportunities,”
ACM Computing Surveys, vol. 51, no. 1, 2018.

[15] X. Xie, J. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Y. Chen, “Testing and
validating machine learning classiﬁers by metamorphic testing,” Journal of
Systems and Software, vol. 84, no. 4, pp. 544–558, 2011.

[16] A. Dwarakanath, M. Ahuja, S. Sikand, R. M. Rao, R. P. J. C. Bose,
N. Dubash, and S. Podder, “Identifying implementation bugs in machine
learning based image classiﬁers using metamorphic testing,” in Proceedings
of the 27th ACM SIGSOFT International Symposium on Software Testing
and Analysis, ser. ISSTA 2018. New York, NY, USA: Association for
Computing Machinery, 2018, p. 118–128.

[17] J. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Machine learning testing:
Survey, landscapes and horizons,” IEEE Transactions on Software Engi-
neering (Early Access), no. 01, pp. 1–37, Feb 2020.

[18] Q.-H. Luu, M. F. Lau, S. P. Ng, and T. Y. Chen, “Testing multiple linear
regression systems with metamorphic testing,” Journal of Systems and
Software, vol. 182, pp. 111 062(1–21), 2021.

[19] S. Segura, G. Fraser, A. Sanchez, and A. Ruiz-Cortes, “A survey on
metamorphic testing,” IEEE Transactions on Software Engineering, vol. 42,
no. 9, pp. 805–824, Sept 2016.

[20] A. Chan, L. Ma, F. Juefei-Xu, Y.-S. Ong, X. Xie, M. Xue, and Y. Liu,
“Breaking neural reasoning architectures with metamorphic relation-based
adversarial examples,” IEEE Transactions on Neural Networks and Learn-
ing Systems (Early Access), vol. 40, pp. 1–7, 2021.

[21] T. Y. Chen and T. H. Tse, “New visions on metamorphic testing after a
quarter of a century of inception,” in Proceedings of the 29th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ser. ESEC/FSE 2021. New
York, NY, USA: ACM, 2021, p. 1487–1490.

[22] Y. Tian, K. Pei, S. Jana, and B. Ray, “DeepTest: Automated testing of
deep-neural-network-driven autonomous cars,” in Proceedings of the 40th
International Conference on Software Engineering, ser. ICSE ’18. New
York, NY, USA: ACM, 2018, pp. 303–314.

[23] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “DeepRoad: GAN-
based metamorphic testing and input validation framework for autonomous
driving systems,” in 2018 33rd IEEE/ACM International Conference on
Automated Software Engineering (ASE), 2018, pp. 132–142.

[24] S. Segura, D. Towey, Z. Q. Zhou, and T. Y. Chen, “Metamorphic testing:

Testing the untestable,” IEEE Software, vol. 37, pp. 46–53, 2020.

[25] S. Kuutti, R. Bowden, H. Joshi, R. de Temple, e. Y. Saber Fallah", D. Ca-
macho, P. Tino, A. J. Tallón-Ballesteros, R. Menezes, and R. Allmendinger,
“Safe deep neural network-driven autonomous vehicles using software
safety cages,” in Intelligent Data Engineering and Automated Learning
(IDEAL 2019). Springer, 2019, pp. 150–160.

[26] K. Pei, Y. Cao, J. Yang, and S. Jana, “DeepXplore: Automated whitebox
testing of deep learning systems,” Communications of ACM, vol. 62, no. 11,
p. 137–145, Oct. 2019.

