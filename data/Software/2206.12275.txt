A Literature Review on Serverless Computing

JINFENG WEN, Peking University, China
ZHENPENG CHEN, University College London, United Kingdom
XUANZHE LIU, Peking University, China

Serverless computing is an emerging cloud computing paradigm. Moreover, it has become an attractive development option for

cloud-based applications for software developers. The most significant advantage of serverless computing is to free software developers

from the burden of complex underlying management tasks and allow them to focus on only the application logic implementation.

Based on its benign characteristics and bright prospect, it has been an increasingly hot topic in various scenarios, such as machine

learning, scientific computing, video processing, and the Internet of Things. However, none of the studies focuses on a comprehensive

analysis of the current research state of the art of serverless computing from the research scope and depth. Such an analysis is a

foundation for understanding an evolving research area. It can provide a global and detailed overview for researchers and practitioners

and help them understand and propose promising strategies and best software practices for serverless application engineering.

To fill this knowledge gap, we present a comprehensive literature review to summarize the current research state of the art of

serverless computing. This review is based on selected 164 research papers to answer three key aspects, i.e., research directions (What),

existing solutions (How), and platforms and venues (Where). Specifically, first, we construct a taxonomy linked to research directions

about the serverless computing literature. Our taxonomy has 18 research categories covering performance optimization, programming

framework, application migration, multi-cloud development, cost, testing, debugging, etc. Second, we classify the related studies of

each research direction and elaborate on existing solutions. Third, we investigate the distributions of experimental serverless platforms

for existing techniques and publication venues for selected research papers. Finally, based on our analysis, we discuss some key

challenges and envision promising opportunities for future research on the serverless platform side, serverless application side, and

serverless computing community side. Our study’s summary can give a quick overview of the current state-of-the-art research on

serverless computing for researchers and practitioners. Meanwhile, it will inspire researchers and practitioners to venture into new

research topics, novel solutions, and best software practices and significantly contribute to serverless application engineering.

Additional Key Words and Phrases: serverless computing, literature view

1 INTRODUCTION

Cloud computing has attracted more and more software developers to develop and execute their applications on cloud

infrastructure [106, 119, 123]. Compared with purchasing new machines and managing numerous operational tasks in

traditional software development, software developers can directly benefit from resource availability and scalability

provided by cloud computing. In cloud computing, with the promising growth of microservices architecture, a new and

emerging cloud computing paradigm named serverless computing emerged and became an increasingly hot topic in

industry [1, 3, 12, 18, 28] and academia [111, 112, 114, 184, 215]. According to a report [2], the serverless market size
will reach nearly $22 thousand million in 2025 from $3 thousand million in 2017.

In serverless computing, it offers an additional abstraction layer into the traditional cloud computing, and this layer

abstracts away complex and error-prone underlying management tasks from software developers. Therefore, software

developers can focus on only the application logic. The reduction in underlying cloud management is undoubtedly

exciting news for software developers without a background in hardware infrastructure. Moreover, in serverless

computing, software developers pay for only the resources actually consumed or allocated by their applications at a

fine-grained pattern. This point is different from traditional cloud computing, where software developers always rent and

retain resources regardless of whether the application is running. On the other hand, serverless computing also makes

2
2
0
2

l
u
J

6

]
E
S
.
s
c
[

3
v
5
7
2
2
1
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
, ,

Wen et al.

cloud providers manage resources in a unified manner, improving resource utilization and reducing resource waste.

Based on these benign characteristics and its bright prospect, many major cloud providers have rolled out their serverless

platforms, such as AWS Lambda [12], Microsoft Azure Functions [18], and Google Cloud Functions [28]. Moreover,

there are also some available open-source serverless implementations, e.g., OpenWhisk [38] and OpenFaaS [36].

However, this emerging paradigm and its development way significantly differ from traditional cloud computing

and traditional software development. Thus, it may face numerous issues or challenges in terms of technique and

operation. Related issues or challenges have been concluded and identified [120, 215, 218, 219], such as cold start

performance, programming framework, and testing and debugging. Some studies like empirical analysis, survey,

literature view, evaluation measurement, etc., have discussed different aspects of serverless computing, including

serverless architecture design [138, 210], development features and limitations [63, 107, 215], technology aspects [232],

performance properties of serverless platforms [142, 219, 220], characteristics of serverless applications [93, 204],

developers’ challenges [134, 218], etc. For instance, Li et al. [138] detailed the serverless architecture by introducing

the related concepts, pros, and cons and provided some architecture implications. Yussupov et al. [232] presented

a comprehensive technology review to analyze and compare the ten most prominent serverless platforms from

development, event source, observability, access management, etc. A measurement study on commodity serverless

platforms [220] was conducted to compare performance differences to help software developers choose the appropriate

one. Eismann et al.[93] collected 89 serverless applications from various sources to characterize them from 16 aspects.

To facilitate software developers’ practices on serverless applications, Wen et al. [218] uncovered 36 specific challenges

that software developers encounter in developing serverless applications. In addition to the above studies, there have

also been efforts to focus on analyzing research work for specific aspects, such as cold start problem and scheduling

policy [137], and resource management [153]. These studies cannot provide a global overview of the current state-

of-the-art in research, making the authors may give some wrong or already existing discussions and prospects for

future work. Hassan et al. [110] presented a survey on serverless computing. However, this survey mainly summarized

statistical information about research papers, such as the number of published papers per year, researcher distribution,

and use cases. Overall, existing studies cannot provide a global and detailed overview of the current research state of

the art of serverless computing.

Serverless computing is not only prevalent and important in industry, but a portion of its papers have been published

in the software engineering (SE) research community. These papers focus on a wide range of topics about serverless

computing, including serverless evolution [205], characteristic analysis of serverless applications [93, 94], developers’

challenges [218], application modelling [233], programming framework of specific applications [68, 237], multi-cloud

development [183], stateful serverless applications [67], application migration [178], serverless economic [50], serverless

dataset [98], technical debt conceptualization [135], testing and debugging [136], etc. However, to the best of our

knowledge, neither the SE community nor any other community has a thorough analysis effort to investigate the

current research state of the art of serverless computing from the research scope and depth. A comprehensive literature

review is a foundation for understanding an evolving research area. In the absence of such a literature review, researchers

and practitioners are hard to quickly know a global overview of research directions that the serverless computing field

has been investigated, as well as specific solutions that have been carried out. Moreover, it may prevent best software

practices for serverless application engineering and the long-term evolution of the serverless computing ecosystem.

In this paper, to fill this knowledge gap, we present a comprehensive literature review to explore the research scope

and depth of the serverless computing literature. Our literature review is based on selected 164 research papers to

analyze and answer three key aspects, i.e., research directions (What), existing solutions (How), and platforms and

A Literature Review on Serverless Computing

, ,

venues (Where). Specifically, first, we aim to construct a taxonomy for research directions of serverless computing to

provide a global literature overview. Our taxonomy contains 18 research categories covering performance optimization,

programming framework, application migration, cost, testing, debugging, etc. Second, we aim to provide a depth

analysis of existing solutions. We classify related studies of each research direction and elaborate on proposed solutions.

Third, we investigate the distribution of experimental serverless platforms for existing solutions and the distribution

of publication venues for selected research papers. Finally, we discuss key challenges and further envision promising

opportunities for future research on the serverless platform side, serverless application side, and serverless computing

community side.

Fig. 1 shows the content structure of this paper. In brief, Section 2 introduces the background of serverless computing.

Section 3 presents our research questions and the selection strategy for research papers. Section 4 summarizes research

directions of serverless computing. Section 5 classifies and elaborates on existing solutions for each research direction.

Section 6 investigates the distributions of experimental platforms and publication venues. Section 7 explores key

challenges and promising opportunities for future research, and Section 8 concludes this work.

Fig. 1. Tree structure of the contents in this paper.

2 BACKGROUND

In this section, we introduce the background of serverless computing, including its evolution, architecture, key charac-

teristics, and mainstream serverless platforms. Moreover, we briefly summarize and compare the differences between

serverless-based software development and traditional software development.

2.1 Evolution of Serverless Computing

Cloud computing provides the ability of computation services via the Internet. According to the NIST definition [156],

traditional cloud computing has three service categories: “Infrastructure as a Service” (IaaS), “Platform as a Service”

(PaaS), and “Software as a Service” (SaaS). Specifically, IaaS allows software developers to configure and use computation,

storage, and network resources. For example, AWS provides the computation service like Elastic Compute Cloud (AWS

EC2) [6] and the storage service like Simple Storage Service (AWS S3) [7]. However, IaaS does not hide the operation

Literature Review on ServerlessComputingIntroductionof ServerlessComputingMethodologyEvolutionArchitectureKeycharacteristicsComparisonwithtraditionalsoftware developmentResearchquestionpresentationData selection strategyResearchQuestionsRQ1: Researchdirections (what research directions)RQ2: Exiting solutions (how to solve)RQ3: Platformsandvenues(where to implement and publish)ChallengesandOpportunitiesFor the serverless platform sideFor the serverless application sideFor the serverless computing community side(Section 2)(Section 3)(Sections 4, 5, and 6)(Section 7)Serverless platforms, ,

Wen et al.

complexity of the application; thus, developers are still responsible for resource provisioning, runtime configuration,

application code management, etc. SaaS allows software developers to directly use the cloud provider’s applications,

such as Gmail [30] and Docs [29] provided by Google. SaaS completely hides the underlying operation complexity, but

use cases are limited. Moreover, developers completely lose control of the application. PaaS allows software developers

to develop, run, and manage applications using the execution environment supported by the cloud provider. For example,

Google provides the App Engine [27], while Azure offers App Service [16]. PaaS compromises the operation complexity

between IaaS and SaaS, but software developers still are responsible for and manage some underlying tasks.

To ease the cloud management burden on software developers, cloud providers presented a new paradigm, i.e.,

serverless computing. Serverless computing is similar to PaaS; differently, it almost hides all complex underlying

management tasks for developers, i.e., “server-less”, and it also allows developers to control their applications. Serverless

computing-related applications (a.k.a., serverless applications) follow the microservice software style, which decomposes

the application into a subset of independent tasks. However, the differences between serverless applications and

microservice-based applications are as follows. First, the serverless application’s unit (a.k.a., serverless function) is a

smaller granularity than the unit of the microservice-based application. Second, microservice-based applications still

make developers face the additional effort of underlying tasks like scalability, fault tolerance, and load balancing. Third,

serverless functions adopt the event-driven pattern while microservices are usually responsive to their interfaces. In

addition, serverless computing is more suitable for short-lived and bursty applications because its platforms provide

high and automatic scalability, while microservices are suited for long-running and stable applications.

2.2 Architecture of Serverless Computing

Serverless computing is an emerging and potential cloud computing paradigm, and its significant advantage is to free

software developers from the burden of complex and error-prone server management tasks. Serverless computing

provides “Backend as a Service” (BaaS) and “Function as a Service” (FaaS) [120], as shown in Fig. 2. Specifically, BaaS

represents tailor-made cloud services provided by cloud providers, e.g., cloud storage and notification services. These

services can service FaaS optionally to simplify the backend functionality development for software developers. FaaS

represents that software developers can write stateless, event-driven serverless functions, making them focus on the

logic of serverless applications. Generally, FaaS is the core of serverless computing, allowing developers to develop and

control their applications.

Fig. 2. The development diagram on the serverless platform.

As shown in Fig. 2, serverless functions will be triggered by pre-defined events, e.g., HTTP requests, data updates of

the cloud storage, and the arrival of a notification. These events represent the developers’ requirements, and developers

ServerlessPlatformsFaaSFunction AFunction BFunction C…EventBaaSDatabaseStorageNotification…triggerdevelopA Literature Review on Serverless Computing

, ,

can define some rules to bind their serverless functions with the corresponding events. When serverless functions are

triggered, the serverless platform automatically prepares required runtime environments, e.g., containers or virtual

machines (VMs), to serve them. These runtime environments are called function instances, and their preparation process

generally contains instance initialization, application transmission, application code loading, etc. After executions are

complete, the platform will automatically recycle and release these function instances and the corresponding resources.

2.3 Key Characteristics of Serverless Computing

To better understand serverless computing, we introduce its key characteristics as follows.

• Functionality and no operations (NoOps): In serverless platforms, software developers can select their most
appropriate and familiar languages (e.g., Python, JavaScript, and Java) to write the function-level code snippet

to create serverless applications. Moreover, serverless platforms provide user-friendly integrated development

environments (IDEs). For the deployment of serverless applications, software developers only need to upload

their application code to the serverless platform without complex environment configuration. In addition, BaaS is

the equivalent of off-the-shelf backend functionality. Its related services can be directly applied in the application

by developers to replace similar backend functionalities. Therefore, developers do not have to redevelop these

functionalities and deal with server configurations.

• Auto-scaling: Serverless platforms can automatically scale function instances horizontally and vertically according
to the application workload dynamics. Horizontal scaling is to launch (i.e., scale-in) new function instances or

recycle (i.e., scale-out) running ones, while vertical scaling is to add (i.e., scale-up) or remove (i.e., scale-down)

the amount of computation and other resources from running function instances. After completing requests, the

corresponding function instances and allocated resources will retain in memory for a short time to prepare to be

reused by subsequent requests of the same function. If there are no subsequent requests, these instances and

resources will be automatically recycled by the serverless platform, i.e., scaling to zero. However, scaling to zero

makes incoming new requests face the cold start problem, which takes a long time to prepare required runtime

environments from scratch.

• Utilization-based billing: In serverless computing, software developers charge for the actually allocated or
consumed resources of the serverless application in the fine-granular execution unit. For example, AWS Lambda’s

pricing is related to the allocated memory, and Azure Functions considers the consumed memory. On the other

hand, serverless functions are event-driven; thus, they will not run without being triggered, and developers do

not pay any cost. This feature eliminates the concern of paying for idle resources. In summary, the billing pattern

of serverless computing is relatively reasonable and inexpensive compared with traditional cloud computing,

which requires always renting and paying resources in memory on standby.

• Separation of computation and storage: Serverless computing adopts the separation way of computation and
storage, i.e., separately scaling and independently provisioning and pricing. Generally, computation refers to

stateless serverless functions, while storage represents cloud storage services provided by cloud providers to

store data from the serverless function. This separation way can ensure the auto-scaling ability of the serverless

platform for bursty workloads.

• Additional limitations: Cloud providers set some additional limitations for serverless functions to keep the vital
auto-scaling feature of serverless platforms. Generally, these limitations contain function execution timeout,

deployment package size, local disk size, memory allocation maximum, etc. Moreover, different serverless

, ,

Wen et al.

platforms have different demands regarding these additional limitations. The following section will list specific

demands for some serverless platforms.

The above key characteristics show the unique advantages of serverless computing. In addition, some features and

limitations will also essentially influence the development of cloud-based applications.

2.4 Serverless Platforms

Major cloud providers have rolled out their commercial serverless platforms, such as AWS Lambda [12], Microsoft

Azure Functions [18], and Google Cloud Functions [28]. However, these commercial serverless platforms hide the

platform’s underlying details and have a vendor lock-in problem for software developers. To address these restrictions,

the serverless computing community has presented some open-source serverless implementations, such as Open-

Whisk [38] and OpenFaaS [36]. Their recognition and popularity are also due in part to the popularity of container

orchestration like Kubernetes [33]. The serverless computing community is actively maintaining these open-source

implementations. Commercial serverless platforms and open-source serverless implementations are designed based on

serverless computing characteristics. Therefore, in our study, they are uniformly called serverless platforms. Next, we

will introduce some mainstream serverless platforms.

AWS Lambda: AWS Lambda is the first widely mentioned serverless platform. After it was released in November

2014, serverless computing started to gain increasing attention, and other major cloud providers followed this trend

to present their serverless platforms. AWS Lambda offers different interaction ways for developers, including the

command-line interface (CLI), HTTP-based application programming interface (API), and graphical user interface (GUI).

Moreover, software developers can use the related plugins for IDEs (e.g., Visual Studio and Visual Studio Code [44])

to access the platform through language-specific client libraries. The pricing of AWS Lambda is related to function

execution time (in increments of 1 millisecond [13]), allocated memory size, and the number of invocations. The

function execution timeout limitation of AWS Lambda is 900 seconds. For the deployment package size, AWS Lambda

supports up to 250 MB uncompressed size and 50 MB compressed size. For the memory allocation configuration, AWS

Lambda can allocate the memory of the function instance as 128 MB to 10,240 MB in the increment of 1 MB. For the

observability of AWS Lambda, Amazon provides AWS CloudWatch [5] and AWS CloudTrail [9] to monitor and log

serverless functions. In addition, AWS Lambda provides a serverless marketplace called AWS Serverless Application

Repository (AWS SAR) [14] for application development purposes. This marketplace contains some serverless functions

or applications contributed by third-party teams.

Microsoft Azure Functions: Microsoft released its serverless platform Azure Functions in 2016. Azure Functions

offers various interaction ways like CLI, API, and GUI and uses related plugins for Visual Studio and Visual Studio

Code [44] to access the platform. The pricing model of Azure Functions is similar to AWS Lambda, but it relies on the

consumed memory of serverless functions. Moreover, the minimum execution time of billing is in increments of 100

milliseconds [19]. The function execution timeout limitation of Azure Functions is 600 seconds. In Azure Functions,

it uses the function app as the execution and management unit, which is still composed of several functions. Azure

Functions has no deployment package limit and uses a flexible memory allocation, supporting 1,536 MB at most.

Microsoft uses Azure Application Insights [17] to provide the observability of Azure Functions. In addition, Azure

Functions adopts three hosting plans: consumption, premium, and dedicated plans for serverless applications. For the

marketplace, Microsoft provides a general-purpose Azure Marketplace [21] to include serverless applications.

A Literature Review on Serverless Computing

, ,

Google Cloud Functions: In 2017, Google released its serverless platform, i.e., Google Cloud Functions. Like AWS

Lambda and Azure Functions, Google Cloud Functions supports various interaction ways like CLI, API, and GUI.

However, Google Cloud Functions seems to have no related plugins for IDEs to access the platform. The pricing model

of Google Cloud Functions is related to provisioned memory and CPU. The function execution timeout limitation is 540

seconds. The deployment package size of Google Cloud Functions has a 250 MB uncompressed size limit and a 100 MB

compressed size limit. Google Cloud Functions can assign memory values like 128 MB, 256 MB, 512 MB, 1,024 MB,

2,048 MB, 4,096 MB. In addition, Google does not provide a marketplace of serverless applications, but it has some code

samples to guide the development process.

OpenWhisk: Apache OpenWhisk is an open-source serverless implementation developed and maintained by IBM

and Apache. It was released in 2016. In OpenWhisk, it combines several key technologies, e.g., Nginx [35], CouchDB [22],

Kafka [32], and Docker [25]. Function invocations can be transformed as HTTP requests and imported into the Nginx

server that supports web protocol. The Nginx server pushes the request to the controller, which collaborates with

the CouchDB that stores the application’s data. The controller and underlying worker nodes rely on Kafka, a publish-

subscribe messaging system, to communicate. Kafka can receive messages from the controller to confirm the invoked

worker node. For the programming model of OpenWhisk, actions, triggers, and rules are primary concepts. Specifically,

actions represent functions to be executed, triggers are predefined events, and rules refer to the binding description

between actions and triggers. In addition, OpenWhisk offers CLI and API interactions and supports local development

and cloud environment development. Developers can leverage IDEs like Visual Studio Code [44] and Xcode [48] to

connect OpenWhisk. In OpenWhisk, there is no marketplace for serverless applications.

OpenFaaS: OpenFaaS is a project developed by Alex Ellis in 2016. The underlying system of OpenFaaS is based on

Docker [25] and Kubernetes [33]. OpenFaaS can be deployed in public or private clouds, even in edge devices, due

to its lightweight. In OpenFaaS, developers can use CLI, API, and GUI to implement interaction operations, but no

related development IDEs. Generally, developers utilize CLI to communicate with the OpenFaaS gateway. This gateway

connects a function monitor tool called Prometheus [40], which records values of function-related metrics. In addition,

OpenFaaS also supports workflow orchestration with synchronous and asynchronous function chains, parallelism, and

branch. In OpenFaaS, it has an OpenFaaS Function Store [37] as its marketplace.

In these serverless platforms, CLI and API are common interface types to establish programmatic access. Except for

Google Cloud Functions, other platforms support the development and deployment of the custom container image. This

deployment way allows developers to use any programming language and heavy third-party library to avoid potential

language and deployment package size limits. However, it also increases the additional burden of container management

efforts, such as interface requirements and container interaction. For commercial serverless platforms, the corresponding

cloud providers offer tailor-made monitoring and logging services to observe serverless functions. However, open-source

serverless platforms generally integrate external tools to achieve the observability of serverless functions. In addition,

commercial serverless platforms natively offer access management for authentication and resource, while open-source

serverless platforms rely on only the hosting environment to implement the related access management.

2.5 Comparison with Traditional Software Development

Serverless-based software development and traditional software development are different in many aspects. In this

section, we summarize some primary differences in Table 1 to further understand the software development paradigm

of serverless computing.

, ,

Wen et al.

Server management: In serverless computing, software developers do not manage complex server tasks, focusing

only on application development. Therefore, the serverless application will require fewer engineers related to operation,

maintenance, and resource management. However, in traditional software development, developers need to coordinate

various components and implement all server-side functionalities. Moreover, developers endure a high failure rate for

physical servers. In addition, server resources are not guaranteed to be optimally utilized.

Functionality implementation: Serverless functions are implemented as event-triggered functionalities. Moreover,

these functions are short-lived and stateless; thus, runtime data cannot be stored in their function instances, making

communication with other functions hard. An external storage service may be required to save data of serverless

functions to solve the communication problem. Moreover, services contained in BaaS can be optionally used in

applications to simplify the development of backend functionalities. However, in traditional software development,

developers first pick a technology stack and development framework and then configure the local development

environment based on the selected programming language. Moreover, developers need to implement complex backend

functionalities themselves. Due to the local development, functions are stateful; thus, they can directly communicate.

Invocation pattern: Serverless applications are composed of multiple event-driven serverless functions. Therefore,

invocations rely on the developer’s pre-defined events, and the invocation process may be automatic. However,

invocations in traditional software development are dependent on client-side calls from the software developer.

Execution limitations: Serverless functions have inherent execution limitations, including function execution

timeout, confined memory size, restricted local disk size of instances, etc. In traditional software development, the

execution limitations have a high degree of uncertainty because they depend on the capacity of the leased servers.

Execution place: Serverless applications can be executed in function instances from the cloud with enough resource

provision, while traditional software applications are executed in the limited local environment of developers.

Performance: The cloud provider of serverless computing manages runtime environments required for serverless

functions. The advantage of unified resource management is that the serverless platform can respond to any bursty

workload. These runtime environments are activated only if serverless functions are triggered. When required environ-

ments are not active, serverless functions may face the cold start problem, which introduces a long preparation time.

There have been a lot of efforts to alleviate this problem [54, 74, 86, 164, 214, 243]. For traditional software development,

runtime environments are always in the active condition to respond to application requests immediately. However, this

will waste too many resources when there are no requests. Moreover, the local environment may not be able to handle

workloads with variable requirements.

Cost: In serverless computing, software developers pay for only actually resources allocated or consumed by

the serverless function. Moreover, using serverless computing saves a lot of application development time, and the

application can be released to the market faster. However, in traditional software development, developers pay for

everything, such as physical server purchase and installation, as well as the cost of maintenance-related engineers and

electricity. Moreover, non-uniform architecture and low product maturity make application development time longer

and market release time slower. In addition, once the application is deployed successfully to the server, the server will

be “always-on”, and developers have to pay for it.

Tool maturity: Existing serverless platforms lack rich support tools, such as testing and debugging. The reason is

that the event-driven, distributed, and platform detail masking features make the application architecture more complex

and the execution flow harder to reproduce. For traditional software development, testing and debugging can be freely

designed and evaluated based on the local environment. Furthermore, the relevant tools are already well-grounded in

the software engineering research community.

A Literature Review on Serverless Computing

, ,

Table 1. Comparison between serverless-based software development and traditional software development.

Features
Server management

Serverless-based software development
No management tasks

Functionality
implementation

Directly write event-driven and stateless code
Use BaaS to simplify development

Invocation pattern

Execution limitations

Execution place

Performance

Cost

Tool maturity

Event triggers
Fixed inherent limitations,e.g.,
execution timeout, confined memory size
Cloud
Activate only if triggered
Cold starts
Flexibility
Pay for actually allocated/assumed resources
Less application development time
Less market release time
Low

Traditional software development
All management tasks
Pick technology stack
Pick development framework
Configure development environment
Implement all functionalities from scratch
Client-side calls
Uncertain limitations, depending
on server capacity
Local
Always activate
No cold starts
No flexibility
Pay for everything
More application development time
More market release time
High

3 METHODOLOGY

In this section, we present our research questions and introduce the selection strategy of related research papers.

3.1 Research Questions

To better understand the research scope and depth of the current research state of the art for the serverless computing

field, we aim to focus on the following three research questions.

• RQ1 (What - Research directions): What have research directions been investigated in the serverless computing
literature? This research question aims to investigate the research goal of existing studies and provide a global

overview of research directions about serverless computing.

• RQ2 (How - Existing solutions): How do existing studies tackle specific problems for each research direction?
This research question aims to understand how to tackle the research problem for each research direction. We

classify the related studies of each research direction and dissect the solutions to problems associated with each

research direction.

• RQ3 (Where - Platforms and venues): Where are the existing solutions implemented/evaluated and published?
This research question aims to investigate which serverless computing platforms existing techniques are imple-

mented or evaluated on, and where existing serverless computing-related papers are published.

3.2 Data Selection Strategy

To answer these research questions, we select the relevant published research papers about serverless computing. In

this section, we explain our data selection strategy. In brief, we search the related research papers by defining some

keywords on the widely adopted engines and databases. Then, we make the corresponding selection rules to select the

final research papers of literature review for analysis.

, ,

Wen et al.

3.2.1

Literature sources. In our study, seven standard online engines or databases [110, 143, 194, 216, 232] are selected

as literature sources. These sources contain (1) ACM Digital Library, (2) IEEE Xplore, (3) Google Scholar, (4) Elsevier

ScienceDirect, (5) Scopus, (6) SpringerLink, and (7) DBLP Computer Science Bibliography.

3.2.2

Search string. To determine the research papers related to serverless computing, we follow the previous work [110,

143] to define extensive search string and then apply it to literature sources. The search string used in a survey of

serverless computing [110] is (serverless OR FaaS OR function as a service OR function-as-a-service) AND (computing OR

paradigm OR architecture OR model OR application OR service OR platform OR programming). Similarly, this search

string is also applied in our study to crawl research papers from our defined literature sources on 2022.01.01. In this

process, we obtain 398 research papers in total.

3.2.3

Inclusion and exclusion criteria. We formulate the inclusion and exclusion criteria to effectively filter and select

relevant research papers of serverless computing.

Inclusion criteria: (1) Publications that belong to serverless computing; (2) Publications that address and present

the corresponding design solutions, algorithms, optimization approaches, or general ideas in terms of specific aspects

of serverless computing; (3) Publications that are written in English.

Exclusion criteria: (1) Publications that are the benchmark suite; (2) Secondary or tertiary studies, e.g., empirical

studies, literature reviews, and surveys; (3) Publications that are not available or full text because they cannot provide

complete information; (4) Publications that are bachelor, master, or doctoral dissertations; (5) Pre-printed studies that

are submitted in the arXiv website.

After the paper screening, exclusion, and duplicate removal, we end up with 118 initial relevant research papers.

3.2.4

Snowballing. Based on the initial papers, we apply the commonly used backward snowballing process [110, 216]

to increase the set of relevant research papers. The main idea of backward snowballing is to search for other relevant

papers in the “Related Work” section of each selected paper. In addition, in our study, the newly added papers also

adopt the same snowballing strategy to continue to search for other new papers. In this phase, we add 46 research

papers to our paper list. As a result, we obtain 164 research papers in total for our literature review.

4 RQ1 (RESEARCH DIRECTIONS)

To answer the research question of which research directions have been investigated, we label all selected research

papers to determine the research goal by viewing their Abstract, Introduction, Related Work, and Conclusion sections.

As shown in Fig. 3, we construct a taxonomy linked to research directions of serverless computing for selected

research papers. Note that a research paper may belong to two or multiple research directions. For example, the work

presented by Cordingly et al. [82] addressed the prediction problem of both application performance and cost because

the cost is closely associated with the performance of the serverless application. Therefore, this work is assigned to the

“Performance Prediction” research direction and “Cost Prediction” research direction. In our taxonomy, its total number

of papers (174) is larger than the original number of selected research papers (164).

Our taxonomy includes 12 root categories of research directions represented in the black box, such as “Resource

Management (22)” and “Performance (67)”. The number in parentheses indicates the number of papers that investigate

the corresponding research direction. For example, 22 papers address resource management. In addition, the boxes with

colors (e.g., blue, orange, and grey) represented sub-research directions under a particular research direction scope.

For instance, the research direction “Performance” contains two sub-research directions represented in the blue box:

A Literature Review on Serverless Computing

, ,

“Performance Prediction” and “Performance Optimization”. The sub-research direction “Performance Optimization”

includes two smaller research directions represented in the orange box: “Cold Start Performance” and “Runtime

Performance”. Furthermore, “Runtime Performance” is still divided into two smaller research directions that cannot be

subdivided and represented as the grey box, i.e., “Function Execution” and “Function Communication”. In total, there

are 18 non-divisible research directions in this taxonomy.

Fig. 3. A taxonomy of research directions in the serverless computing literature.

Next, we explain our taxonomy in detail. First, studies related to “Performance” are the most, accounting for 38.51%

(67/174) of all papers in this taxonomy. These studies contain 10.45% (7/67) “Prediction Prediction” and 89.55% (60/67)

“Performance Optimization”. Specifically, the performance of serverless platforms or applications affects resource

utilization or developers’ experience, respectively. Moreover, performance variance affects cost. Therefore, some studies

have targeted the performance prediction of platforms or applications. However, most performance efforts are made on

performance optimization, including 56.67% (34/60) “Cold Start Performance” optimization and 43.34% (26/60) “Runtime

Performance” optimization. For the cold start performance, it refers to the overhead generated by the cold start process.

When there are incoming requests, serverless platforms will automatically prepare the runtime environment from scratch

to serve serverless functions, i.e., cold starts. This process needs to prepare instance initialization, application runtime,

application code preparation, etc., and thus introduce undesired latency for requests and affect the user experience. For

the runtime performance, it refers to the execution overhead of serverless functions (i.e., “Function Execution”, 30.77%

(8/26)) and communication overhead between serverless functions (i.e., “Function Communication”, 69.23% (18/26)).

The function execution overhead represents the time of executing the actual functionality of serverless functions, not

the end-to-end response time of a request. For the function communication overhead, it is due to the stateless feature of

serverless functions. Current serverless platforms do not provide a mature point-to-point communication mechanism

between serverless functions via the network. Generally, developers leverage cloud storage services like AWS S3 [7]

to save data of serverless functions for intermediary state management. In this situation, serverless functions need

to keep fetching the required data in external services to accomplish the collaboration of functions, thus producing a

high-latency communication overhead.

Second, in our taxonomy, the second-largest research direction in proportion is “Programming Framework” (18.39%

(32/174)). Serverless computing is an emerging cloud computing paradigm, and thus it may not fully support applications

with unique type requirements (i.e., “Specific Applications”, 59.38% (19/32)) or no type requirements (i.e., “General

Applications”, 40.63% (13/32)). Therefore, related researchers have designed specific or generic programming frameworks

to adapt to the corresponding requirement.

Third, our taxonomy shows that studies related to “Resource Management” account for the third-largest percentage,

i.e., 12.64% (22/174). In serverless computing, resource management aims to manage the resource requirements of

ResearchDirectionson Serverless Computing (174)Specific Applications(19)General Applications(13)Performance Prediction(7)Performance Optimization(60)Function Communication(18)Function Execution(8)Runtime Performance(26)Cold Start Performance(34)Cost Prediction(4)Cost Optimization(10)GPUSupport (2)Multi-Cloud Development (3)Resource Management(22)Programming Framework(32)Performance(67)Stateful FaaS(8)Application Modelling(7)Cost(14)Security(5)Application Migration(10)Testing(1)FPGASupport (2)AcceleratorSupport(4)Debugging(1), ,

Wen et al.

serverless applications and ensure the resource efficiency of serverless platforms. Better resource management will

more easily make serverless platforms and applications achieve service-level agreements.

In addition, our taxonomy contains nine other research directions: “Stateful FaaS” (4.60% (8/174)), “Application

Modelling” (4.02% (7/174)), “Application Migration” (5.75% (10/174)), “Cost” (8.05% (14/174)), “Multi-Cloud Development”

(1.72% (3/174)), “Accelerator Support” (2.30% (4/174)), “Security” (2.87% (5/174)), “Testing” (0.57% (1/174)), and “Debugging”

(0.57% (1/174)). The specific illustration is as follows.

•“Stateful FaaS”: Serverless functions are developed in a stateless way. However, supporting the stateful feature in
serverless applications can fulfill a broader range of workloads. Therefore, some efforts have aimed to design stateful

serverless computing.

•“Application Modelling”: Serverless applications follow a new development paradigm. Therefore, it may be limited
in the expression of the serverless application. Some studies have addressed the application modelling problem to

clearly show the specific representation and dependency for serverless applications.

•“Application Migration”: The prevalence and popularity of serverless computing make more and more legacy
applications migrate to the serverless platform to enjoy its advantages like low cost and high scalability. This migration

process corresponds to the application migration research direction of the serverless computing literature.

•“Cost”: Major serverless platforms adopt a unique cost pattern, i.e., utilization-based billing. The cost is closely
related to the execution time of the serverless function, allocated or consumed memory size, and the number of

invocations. Some researchers have explored how to predict and optimize the cost of serverless functions or applications

based on these factors and possible impact factors.

•“Multi-Cloud Development”: Generally, software developers select a fixed serverless platform to develop their
applications. However, different cloud providers offer various features for their serverless platforms. Using multiple

clouds will allow application development and execution to enjoy more benefits. Therefore, some studies have made

efforts toward the multi-cloud development of serverless computing.

•“Accelerator Support”: Serverless platforms mainly provide the CPU resource-dominated runtime environment.
Therefore, software developers’ applications cannot use other accelerators like Graphics Processing Unit (GPU) and

Field Programmable Gate Arrays (FPGA) hardware resources. However, more and more tasks like machine learning and

deep learning require leveraging such hardware resources to accelerate their performance. In this situation, how to

design accelerator-enabled serverless computing is essential to facilitate a wide application availability.

•“Security”: When using serverless computing, the serverless platform and application details are agnostic to each
other. This situation also leads software developers and cloud providers to distrust each other. Some security problems

have been investigated in the serverless computing literature.

•“Testing”: Similar to traditional software applications, serverless-related developers are also required to test their
serverless applications. Testing plays a vital role in the software quality assurance of serverless applications. However,

the distributed and event-driven natures of execution on the cloud make it challenging to test serverless applications.
•“Debugging”: Serverless computing hides the underlying system implementation for software developers. Moreover,
serverless functions contained in applications are independent and event-driven. These features make the application

debugging hard so that developers cannot determine the application’s correctness. However, debugging serverless

applications is essential to facilitate serverless application engineering, although few attempts have been made in the

current research effort.

A Literature Review on Serverless Computing

, ,

5 RQ2 (EXISTING SOLUTIONS)

To answer the research question of how existing studies tackle specific problems for each research direction, we read

the entire content of the research paper. Then, we classify related studies of each research direction and elaborate on

the solutions to problems associated with each research direction.

5.1 Resource Management

In serverless computing, resource management is responsible for allocating the proper resource provision for serverless

functions and scheduling serverless functions on appropriate function instances. This process guarantees software

developers’ and cloud providers’ quality of service (QoS) requirements. The QoS goal of software developers is related

to the serverless application’s performance, cost, security, etc. In contrast, the QoS goal of cloud providers is associated

with the serverless platform’s resource utilization, load balancing, throughput, etc. Existing studies have tried to improve

resource management from four kinds of solutions, including platform QoS requirement, dynamic resource adjustment,

application characteristic, and architecture design.

• Platform QoS requirement: From the cloud provider’s perspective, achieving efficient resource utilization is
critical in reducing resource waste. Therefore, some studies have considered managing the resource by measuring the

QoS of the serverless platform. A QoS-aware resource manager can satisfy the QoS enforcement while maximizing

the overall resource utilization of the serverless platform. HoseinyFarahabady et al. [112, 113] leveraged the model

predictive controller to present the QoS-aware controller with feedback to guarantee the well-utilization of computation

resources, the response time of processing events, and the QoS demand level for serverless functions. However, their

studies have not considered the effect of the number saturation of the working thread. To address this problem, they

presented a controller that can adjust the number of working threads for each QoS class [128]. This controller used a

QoS violation index to determine the required resources, not the prediction module of the previous work [112, 113].

Tariq et al. [208] first conducted a measurement study to uncover existing serverless platforms’ problems. They

found inconsistent and incorrect concurrency limits, difficulty supporting bursty workloads, and inefficient resource

allocation. To alleviate these problems, they introduced an effective QoS scheduler called Sequoia to realize a variety of

flexible policies. Yuvaraj et al. [234] also presented a hierarchical resource allocation framework to address inconsistent

and runtime limitations. This framework can enhance scalability and job parallelism degree. Schuler et al. [187] found

the impact of concurrency levels on application performance; thus, they got the idea of adjusting the concurrency level

for workloads. Specifically, they presented the reinforcement learning-based model, where the agent is responsible for

evaluating the current system state in each iteration and making decisions for the concurrency level.

• Dynamic resource adjustment: Efficient and flexible resource management can handle various serverless
workloads with different resources and latency requirements. Moreover, it can minimize cloud providers’ costs. Some

studies have considered how to dynamically adjust resources like CPU in the serverless platform. For example, Kim et

al. [127] presented a fine-grained CPU cap controller to dynamically adjust CPU usage limit according to the performance

requirement similarity of serverless applications. This adjustment can minimize resource contention, improve the

robustness of the controller, and thus reduce performance degradation. Similarly, function-level schedulers [202, 203]

were presented to analyze the resource consumption and lifetime of serverless functions and then classify and schedule

serverless functions. Meanwhile, these schedulers can dynamically adjust CPU-shares resources of containers for

serverless functions. Furthermore, FnSched [202] used a greedy algorithm, which can allocate fewer function instances

to respond to requests. However, these studies above on dynamic resource adjustment have not considered developers’

, ,

Wen et al.

requirements (e.g., application deadline) in the load balancing policy. It may limit optimal resource usage and cannot

generate efficient resource management. Therefore, Mampage et al. [152] designed a deadline-sensitive heuristic

algorithm to manage the resource and minimize the cloud provider’s cost. Moreover, they dynamically adjust CPU

resources during executions to meet the developer’s application deadline.

However, some applications may be complex since their consumed resources could vary with workload demands.

Stronger resource management is essential for applications like big data analytics. Leveraging real-time, precise resource

monitoring and feedback may be a possible solution. Enes et al. [97] presented such a real-time approach. This approach

relied on the operating-system-level virtualization to dynamically change provided resources via cgroups and kernel-

backed accounting features. In addition, Yu et al. [228] presented a new serverless resource manager to make full use of

idle resources. This manager used an experience-driven algorithm to estimate the resource saturation point for the

serverless function and then dynamically decided to harvest or offer resources for this serverless function.

• Application characteristic: Some studies have considered application characteristics to design the corresponding
resource management strategies. An efficient resource management system named Emars [181] analyzed the function

execution history to predict the right memory consumption for the serverless function, enhancing memory resource

utilization. Skedulix [84] was a greedy algorithm that used predictive models about the function execution time and

network latency for each function to determine the function placement.

A serverless application can be viewed as a workflow. Existing serverless platforms may launch multiple containers

to serve multiple serverless functions contained in the workflow, causing resource over-provisioning. In this situation,

Bhasi et al. [70] presented a resource management framework called Kraken, which captured the workflow characteristic

and invocation probability to estimate the number of containers provided and ensure the application performance. An

application workflow may also have a workflow execution deadline. A scheduling algorithm should consider this factor

and meet the constraint cost. Therefore, serverless deadline-budget workflow scheduling algorithms [123, 151, 167] were

presented. These algorithms used a set of heuristic rules to process the workflow graph and assign the corresponding

resources. Palma et al. [166] was a novel scheduling idea. It provided a declarative language for developers to describe

the application requirements, such as expected scheduling policy and performance goals. The underlying scheduler can

understand these requirements to select the appropriate instances.

The above studies have mainly maximized resource utilization. However, considering the billing model of serverless

functions, resource management should also evaluate the task execution cost related to aggregated runtimes. Inter-task

scheduling requires minimizing both cost and task completion time. It is hard to trade-off between cost and completion

time since a short completion time means large allocated memory that causes a high pricing unit. A fine-grained

task-level scheduler, Caerus [236], was presented to address this problem by on-demand invoking functions. Caerus

used a step dependency model to model and schedule pipeline-able and non-pipeline-able dependencies across tasks.
• Architecture design: Existing scheduling policies have been basically coarse-grained and may not suitably satisfy
the bursty, stateless, and short-lived applications. In this situation, Kaffes et al. [121] presented a new scheduling

architecture with a centralized core-granular scheduler. Core granular can directly assign functions to individual cores,

guaranteeing performance stability. Centralized design can maintain a global view of the cluster to manage cores and

resources and eliminate the migration of heavy functions. However, this scheduling architecture design considered

only the CPU resource allocation.

Generally, serverless platforms adopt the strategy of over-provision resources for serverless applications to guarantee

the application QoS. However, developers still pay for only resources that their applications actually consume. This

kind of resource provision way is not friendly for cloud providers of serverless computing. In this situation, a new

A Literature Review on Serverless Computing

, ,

architecture may be required to maximize the cloud provider’s benefits. Zhang et al. [240] leveraged Harvest VMs to

design a new architecture. Harvest VMs is a faster, cheaper alternative than VMs. The authors leveraged a series of

measurement results to illustrate the suitability of Harvest VMs in serverless computing. Meanwhile, these results also

guided the architecture design with Harvest VMs.

5.2 Performance Prediction

For studies related to performance prediction, regression model-based prediction and statistical learning-based prediction

are two common solutions. A summary of studies on performance prediction is shown in Table 2.

• Regression model prediction: Some studies have tried to train the regression model about performance by
considering several affected factors. Cordingly et al. [82] thought about the performance prediction problem from the

perspective of system resource usage. They generated the regression model for each serverless function to predict its

runtime performance (e.g., CPU user mode time, CPU kernel mode time). This model considered the CPU heterogeneity

of the serverless platform and memory settings. However, this approach trained the corresponding model for each

serverless function; thus, this process may take a long time. Therefore, Eismann et al. [90] presented an approach

called Sizeless that did not require dedicated performance learning. Sizeless designed an offline phase, which monitored

some synthetic functions and collected the resource consumption data (e.g., system CPU time, bytes received) for all

memory sizes. Based on the collected data, Sizeless trained the multi-target regression model to predict the execution

latency in all memory sizes for a real function. However, these approaches above targeted only independent serverless

functions, not the serverless application with multiple serverless functions and complex structures. Moreover, obtained

performance models did not consider function features such as task type and input parameter size.

• Statistical learning prediction: Another kind of approach is to consider statistical learning to predict the
performance of serverless platforms, functions, or applications. Some studies [105, 148, 149] aimed to predict the

performance of serverless platforms. Some prediction models were presented to create performance-driven and predictive

serverless platforms. These models considered some key parameters, such as the cold start rate, cold start latency,

arrival rate of warm instances, and instance expiration rate, to the original system with the Markovian arrival process.

Obtained platforms can decrease resource waste and guarantee the QoS of serverless applications.

Other studies aimed to predict the response time of serverless functions or applications. Akhtar et al. [52] presented

a framework called COSE to learn a performance model of the serverless function from execution logs. This model

used the Bayesian Optimization strategy to learn the gap between configuration and runtime statistically, and it

predicted the optimal configuration that provided the satisfying user-specified performance. Lin and Khazaei [139]

modeled the application performance into a probabilistic graph considering the structure transformation as well as the

runtime response time of each serverless function. However, these approaches leveraged the collected history runtime

information to predict the function or application performance. It is impossible to know the historical performance

situation for new serverless functions or applications.

5.3 Performance Optimization

Studies on performance optimization are related to optimizing the cold start performance and runtime performance.

5.3.1 Cold Start Performance. To address the cold start problem, serverless platforms like AWS Lambda adopt the fixed

“keep-alive” policy to keep resources in memory for a few minutes, i.e., becoming warm instances, when serverless

functions finish their executions. Subsequent requests can reuse warm instances with required resources to reduce the

, ,

Wen et al.

Table 2. A summary of studies about performance prediction.

Study
Cordingly et al. [82]

Eismann et al. [90]

Solution
Regression model
prediction
Regression model
prediction

Target object
Serverless function

Serverless function

Mahmoudi et al. [149]

Mahmoudi et al. [148]

Gias et al. [105]

Akhtar et al. [52]

Lin and Khazaei [139]

Statistical learning
prediction
Statistical learning
prediction
Statistical learning
prediction

Statistical learning
prediction
Statistical learning
prediction

Serverless platform

Serverless platform

Serverless platform

Serverless function

Impact factors
CPU time (e.g., CPU user mode time and
CPU kernel mode time)
Resource consumption (e.g., heap used, user
CPU time, system CPU time, voluntary con-
text switches, bytes written to file system,
and bytes received over network)
Cold start rate, arrival rate of warm in-
stances, and instance expiration rate
Cold start rate, arrival rate of each server,
and server expiration rate
Number of functions, function popularity,
arrival rate, service rate, cold start rate, and
idle lifetime rate
Configuration (e.g., allocated memory and
location) and execution time

Serverless application Allocated memory, response time, and av-

erage number of invocations

number of cold starts. However, this policy cannot capture the actual invocation frequency of serverless functions, leading

to resource waste in no requests. Therefore, many researchers have continued to tackle the cold start problem. The main

solutions contain instance prewarm preparation, data cache-based optimization, function scheduling, snapshot-based

optimization, and architecture design.

• Instance prewarm preparation: Instance prewarm preparation is to launch some required function instances in
advance to serve the incoming requests. This kind of solution prevents the serverless application from going through

the cold start process. Some studies [86, 243] have focused on the cold start problem of serverless applications with the

chain structure. The first serverless function contained in the chain-type serverless application is invoked, meaning

that the following serverless functions will also be invoked in a cold start manner. Moreover, Daw et al. [86] found that

the cold start latency increase with the application chain length. Therefore, some studies [86, 243] have leveraged this

knowledge to design the corresponding approaches. These approaches can predict and prepare the runtime execution

environment in advance for subsequent serverless functions contained in a chain-type serverless application. However,

if the prediction fails, these approaches will introduce additional resource waste.

In order to alleviate the cold start problem, some other studies have used a prewarm pool to process requests

quickly. Ling et al. [140] introduced an oversubscribed prewarm container pool, where containers had different resource

configurations. This pool was based on a static setting and combined all resource sizes, e.g., 1-CPU, 2-CPU, and 4-CPU.

However, such a pool led to low resource utilization and the risk of resource fragmentation. Suo et al. [201] maintained

an adaptive live container pool to service incoming requests. This pool can be adjusted according to the container

runtime history over time. However, these approaches may not quickly deal with burst requests and avoid resource

waste. To further address these problems, Horovitz et al. [111] learned the seasonal invocation pattern of incoming

functions through history invocation timestamps and then launched required instances for serverless functions in time.

Similarly, Xu et al. [226] presented an adaptive strategy called AWU to predict when the serverless function will be

invoked and then warm up the runtime execution environment in advance. AWU was based on time series prediction,

A Literature Review on Serverless Computing

, ,

which considered the invocation number of the serverless function over time and the moments between serverless

functions being called. In addition, Shahrad et al. [188] also presented a flexible and practical resource management

policy. This policy can dynamically manage the prewarm time window size for each serverless function through the

time series prediction. Unlike the time series prediction of the study [226], this policy [188] was based on each serverless

function’s invocation frequency and pattern to dynamically adjust the prewarm time and keep alive time.

• Data cache-based optimization: The traditional runtime environment of serverless computing is VMs, which
have strong isolation and flexibility. However, VMs lack the advantage of low start latency for applications, and

this latency is usually greater than 1000 ms. In this situation, serverless computing uses another common runtime

environment, containers, which can achieve low start latency (usually 50 ms - 500 ms) than VMs. Moreover, containers

typically include binaries and libraries. Pre-importing necessary or commonly used data on such a runtime environment

can speed up the cold start process.

The representative studies based on data cache-based optimization are SOCK [164] and SAND [54]. Specifically,

SOCK [164] and its previous implementation, Pipsqueak [163], cached per-warmed interpreters and commonly used

libraries in containers, and they provided the lightweight isolation mechanism for serverless functions. Following this

caching idea, Nuka [176] was a generic serverless engine with millisecond initialization. This engine designed a local

package caching to import required software packages. Moreover, it provided an isolation pool to reduce the latency of

pulling container images. SAND [54] applied the application-level sandbox runtime sharing to reduce the number of

containers and thus container preparation latency. SAND also provided the isolation mechanism for serverless functions

to allocate or deallocate resources quickly. Similar to SAND, Dukic et al. [89] thought that current strict isolation is not

necessary for safe concurrent requests of the same serverless function. Sharing the runtime may be a possible solution

to alleviate the cold start problem for concurrent requests. Therefore, they presented an ultralightweight execution

context named Photons, which can co-locate multiple function instances of the same serverless function in the same

runtime via workload parallelism. However, the above studies on data cache-based optimization may be impractical

since caching all required libraries or functions in memory will increase resource overhead, and cache policies are not

easy to capture in the real workload.

Some related studies have designed other data cache approaches considering different objects, e.g., networking

resources, image data, and containers. For example, Mohan et al. [157] found that the major overhead during container

startup for concurrency invocations is the creation and initialization of network namespaces. Therefore, they cached

networking resources in some pre-created containers to reduce the network overhead of container startup. Hermes [227]

was a two-level caching mechanism including memory caching and local disk caching to support on-demand loading of

image data and repeated fetching elimination. Such a mechanism can alleviate the transmission latency in cold starts.

WLEC [195] was a container-based caching policy, which used cold, warm, and template queues to place required

containers according to runtime information. These containers were managed and selected by a Container Management

Service to respond to requests.

Except for considering different cache objects, Fuerst et al. [103] mapped keep serverless computing alive to caching

study, where keeping instances warm is viewed to cache an object, and the warm start is a cache hit. They designed

FaasCache containing a set of caching-based keep-alive policies to reduce cold start overhead.

In most cases, using the data cache can effectively alleviate the cold start problem. However, this kind of approach

will introduce high resource overhead and imbalanced resource consumption, leading to performance interference from

the system environment. Moreover, cache policies may be hard to be determined in the real world.

, ,

Wen et al.

• Function scheduling: Generally, the function scheduling solution is to distribute incoming requests to warm
instances to serve them. Warm instances have the prepared runtime execution resources, and thus they can serve

requests faster than cold starts. Related studies have aimed to improve the existing schedulers or compensate for the

insufficiency of scheduling strategies in reusing warm containers. First, existing schedulers may be agnostic of container

lifecycle, i.e., creation, use, pause, or eviction of containers, and thus they are ineffective in reducing cold starts. In

this situation, Wu et al. [224] proposed a container lifecycle-aware scheduler called CAS to distribute requests. This

scheduler leveraged an affinitive worker to maintain and manage the states of containers. Second, existing schedulers

showed erratic performance behavior in application workloads with multi-tenant and high concurrency. Therefore, Kim

et al. [124] designed a novel scheduling algorithm called FPCSch, allowing serverless platforms to schedule available

containers into requests to obtain stable performance while reducing cold starts.

Other researchers have considered the application structure [69, 132] or dependency relationships [190] among

serverless functions to schedule serverless functions. If independent serverless functions can be fused or composed

into a serverless function, the scheduler assigns these independent serverless functions to be executed on the same

function instance. In this situation, the number of cold starts will directly decrease. Leveraging this knowledge to reduce

the number of cold starts, Bermbach et al. [69] orchestrated all serverless functions into a lightweight choreography

middleware, and Lee et al. designed a function fusion solution [132] based on graph representation. In addition, Shen et

al. [190] leveraged frequent pattern mining and invocation history to mine dependencies between serverless functions.

These dependencies can guide the scheduler to schedule the connected functions on the same instance, thus diminishing

the occurrences of cold starts.

Considering that some studies like SOCK [164] and SAND [54] used data cache techniques to alleviate the cold start

problem, the scheduler or load balancing may significantly affect the cache-hit ratio, thus influencing the performance of

cold start and overall application. Based on it, some scheduling algorithms like PASch [61], GRAF [133], and others [49]

have been presented to distribute incoming requests into container instances with pre-loaded cache to improve the

cache hit rate and thus the performance of serverless applications.

• Snapshot-based optimization: The industry and academia have used the snapshot way, a promising solution,
to alleviate the high latency of cold starts. Specifically, this way captures the complete state (called snapshot) of the

current function execution and stores the state in local storage (e.g., SSD) or in a disaggregated storage service. When

the same function is invoked again, the serverless platform can quickly initialize the function instance according to the

corresponding snapshot to immediately process this request. Snapshot-based optimization becomes attractive since it

does not require main memory during functional inactivity and can reduce the high latency of cold starts.

Cadden et al. [74] presented SEUSS to capture the function state (containing function logic, language interpreter,

and libraries) at an arbitrary point during executions. Moreover, SEUSS saved the state as an in-memory snapshot.

New invocations of the same function can be rapidly started from its snapshot. A similar idea to SEUSS, Replayable

Execution [214] was to use process-level checkpointing to save and restore the state. Moreover, the state was allowed

to share between different containers. Du et al. [88] found that sandboxes have a high application initialization latency,

which dominated the overhead of cold start latency. To reduce this part of overhead, they designed Catalyzer based on

Google’s gVisor [31] to set checkpoints and restore application and sandbox runtime. Catalyzer minimized the critical

path processing time of VM loading through the snapshot way. Following the same design principles as Catalyzer, a

runtime environment called Firecracker [51] was presented by AWS. Its Firecracker VM was loaded from a snapshot,

which contained the state of the virtual machine monitor and the emulated devices. Moreover, Firecracker customized

the virtual machine manager (e.g., hypervisor) to create microVMs to isolate multiple tenants with affordable overhead.

A Literature Review on Serverless Computing

, ,

However, Ustiugov et al. [209] characterized the snapshot-based serverless infrastructure Catalyzer [88] and found that

a function executed from the snapshot took 95% longer to execute than if the same function was executed from a resident

in memory, on average. Due to the state constantly being written to the page, the page frequently generates faults,

thus causing high latency. Fortunately, they found the same stable working set of pages among different invocations

of the same function. Therefore, they leveraged this insight to present REAP. REAP can obtain the function’s stable

working set of guest memory pages and pre-load into memory to speed up the performance. However, REAP also posed

shortcomings. For example, generating a stable working set is not enough when the input data payload is significantly

different among invocations. In this situation, the runtime environment waited until all data was loaded to start, leading

to a slower execution latency.

• Architecture design: Some studies have used novel design principles or underlying environments to present
the new serverless platform, which will fundamentally alleviate the cold start problem. We briefly summarize the

related studies as shown in Table 3. Specifically, Boucher et al. [71] presented a novel serverless platform based on

language-based isolation. Using language-based isolation is faster than using process-level isolation for microsecond-

scale serverless functions. In language-based isolation, a single-threaded worker process is hosted in each worker core,

and it can directly execute serverless functions, one at a time. Moreover, the presented serverless platform used task

preemption, supported by commodity CPUs at a microsecond scale. Other studies have applied the lightweight runtime.

Hall and Ramachandran [109] and Long et al. [144] used the lightweight and fast WebAssembly runtime [46] to replace

the container to remedy the performance overhead of cold starts. In contrast to VMs and containers, WebAssembly is a

high-level language VM runtime and shows a binary format with inherent memory and execution safety guarantees.

Unikernel [43] is an emerging fine-grained, lightweight sandbox that uses libarayOS with essential dependency

libraries. Security of the Unikernel is higher than containers, the image size is also small, and notably, the start latency

is within 10 ms. USETL [99] was a Unikernel-based design to be specific to the cold start performance of serverless

extract, transform, load (ETL) workloads. USETL leveraged strong language preference and maintained a pool of

Unikernels with initialized runtime. Following the Unikernel idea, Tian et al. [206] also used the Unikernel design to

make serverless functions run in the Unikernel, offering extremely low cold start latency to execute functions. Moreover,

this design leveraged a hardware technique named VMFUNC [45] to achieve communication among serverless functions

in different Unikernels.

Table 3. A summary of architecture design studies on cold start performance optimization.

Study
Boucher et al. [71]

Architecture design
Language-based isola-
tion (Rust)

Description
It is faster than traditional process-level
isolation

Security
High

Hall and Ramachandran [109]
Long et al. [144]

WebAssembly

Fingler et al. [99]
Tian et al. [206]

Unikerel

It is a lightweight high-level language
VM runtime

It is a lightweight sandbox with li-
braryOS and has strong flexibility

High

High

5.3.2 Runtime Performance. Studies on runtime performance optimization are about performance optimizations of

function execution and function communication.

1. Function Execution

, ,

Wen et al.

In the related studies on performance optimization of function execution, memory configuration, function scheduling,

and architecture design are common solutions.

• Memory configuration: Generally, the response time of the serverless function is affected by the allocated
memory [220, 230]. However, when the allocated memory size is large enough, the response time of the serverless

function becomes insensitive to the memory [139]. Therefore, changing the size of the allocated memory may be

an opportunity for performance optimization. Lin and Khazaei [139] used their performance model with a heuristic

algorithm to find a memory configuration. This configuration can achieve the minimum average response time of the

serverless application under budget constraints.

• Function scheduling: Designing an appropriate scheduling strategy can reduce resource competition for CPU
and network among function instances, thus improving the execution performance of serverless applications. Current

serverless platforms are agnostic to the application type or invocation frequency during the request processing. It may

make certain serverless functions be located on the same VM node, influencing their execution performance. Mahmoudi

et al. [150] considered the application type (e.g., CPU, memory, and I/O) to design an adaptive function placement

algorithm. This algorithm used statistical machine learning to analyze resource utilization and the application’s profile.

Then, it arranged the runtime environment with the best performance for functions. Przybylski et al. [172] considered

the collected information, including the invocation frequency and function duration time, to propose a data-driven

function scheduling to minimize the response time.

When there are bursty or real-time workloads, a real-time serverless prototype is required to execute them at a

guaranteed invocation rate and minimal performance effect. Such a prototype can be achieved through predictive

container management and admission control [162]. However, the Alibaba Cloud Function Compute team found that

the deployment way to use custom container images needs to pull large container images (larger than 1.3 GB) from the

backend store. When bursty workloads are to pull the same large container image, it will cause a severe performance

problem of network bandwidth. Therefore, they presented a rapid container provisioning FaaSNet [212] to accelerate

container provisioning to serve bursty requests. FaaSNet organized VMs as function-based tree structures and used a

tree balancing algorithm to dynamically adapt the tree topology to adjust VM joining and leaving.

• Architecture design: Some new architectures have been designed to optimize the function execution performance.
Atoll [193] was a scalable, low-latency serverless platform where the control and data planes of its architecture were

redesigned via decoupling sandbox allocation from scheduling, introducing deadline-aware scheduling, and co-designing

the load balancing and scheduling layers. Atoll can handle short-lived serverless functions with unpredictable arrival

patterns and maximize the request processing with the user-specified deadline. In addition, instead of using traditional

pre-compiled libraries, Chadha et al. [78] used a Just-in-Time compiler, which is based on LLVM [34] for Python, to

optimize the performance of compute-intensive serverless functions. In addition, Carreira et al. [75] tried to utilize

runtime knowledge to optimize the executed function code. First, they demonstrated the significant impact of runtime

optimizations. Moreover, the code to execute multiple times (i.e., in hot starts) was optimized code, which is executed at

the maximum performance. Then, they presented a holistic system named Ignite to share code optimization information

across runtimes for the same function to reduce profiling and compilation overheads.

2. Function Communication

Since serverless functions are stateless and are executed in different function instances, they generally rely on external

storage to communicate with each other to accomplish complex tasks. However, such communication collaboration

introduces additional overhead for application execution. Some studies have aimed to alleviate this overhead through

different strategies: memory sharing, cache-based design, storage optimization, and network optimization.

A Literature Review on Serverless Computing

, ,

• Memory sharing: When serverless functions are co-located on the same machine, using memory sharing can
avoid the overhead of data movement. Generally, memory sharing-based approaches [83, 117, 130, 180, 192] refer

to designing and managing shared memory to achieve low-latency communication between serverless functions.

Specifically, Shillaker and Pietzuch [192] presented a runtime called Faasm, which contained a WebAssembly-based

isolation abstraction to isolate the memory of running functions and allow memory to be shared among functions

in the same address space. However, Faasm was based on specific assumptions about the language-based isolation

and application programming interface. Jia et al. [117] presented a runtime named Nightcore to address the overhead

imposed by interactive serverless functions. Nightcore supported arbitrary invocation patterns to execute multiple

invocation requests of the same function in the same container. Moreover, it optimized I/O between requests via efficient

threading. Overall, the internal function call of Faasm [192] and Nightcore [117] had the same functionality. Differently,

Faasm made serverless functions execute within the same process leveraging WebAssembly-based isolation. Nightcore

directly co-located multiple requests on the same container and used shared memory to achieve efficient function

communication. However, Faasm and Nightcore were both to modify the serverless application. In this situation,

the lightweight Faastlane [130] was presented to minimize the latency between function interactions, supporting

unmodified applications. Its data sharing leveraged simple load/store instructions, and Faastlane made serverless

functions contained in a workflow execute as threads within a shared virtual address space.

• Cache-based design: Some approaches have used caches to improve state consistency guarantees and runtime
performance of serverless applications. Lambdata [207] was a novel serverless system that allowed developers to declare

the data read and write intents of their serverless functions. To speed up the communication performance, Lambdata

leveraged the additional object caching layer of the computation node to save the same data for multiple function

invocations and scheduled the related functions to the same computation node to reuse data. Linking the caching

layer to the computation node was also adopted by Wu et al. [223]. They presented HydroCache to optimize network

traffics. HydroCache chose an autoscaling key-value storage engine, Anna [8], to provide low-latency data access and

transactional causal consistency. However, the worker of HydroCache established multiple times with the storage to

obtain the latest version, leading to the tail latency of function execution. To address this problem, Lykhenko et al. [145]

proposed a solution called FaaSTCC, combining a multi-site cache with the storage layer. The key idea was to leverage

a small amount of metadata that is passed from function to function to capture the difference between versions.

Similar to HydroCache, Sreekanti et al. [199] introduced a stateful serverless platform named Cloudburst, which

also used Anna and imported a local storage cache to execute multiple serverless functions. However, HydroCache

and Cloudburst introduced specific assumptions, such as consistency semantics and protocols, since they relied on

Anna. Moreover, the work of Cloudburst did not discuss the specific caching configuration. A work designed at the

same time as Cloudburst was OFC [159]. OFC designed an opportunistic RAM-based caching system to dynamically

predict caching efficiency and memory usage through machine learning. OFC had stronger consistency and persistence

guarantees than Cloudburst, and it supported more widely workload types. However, these approaches have not

considered another critical factor, i.e., scaling, which may mitigate the data access impact. Therefore, Romero et al. [179]

presented FaaT, which was serverless and bundled in the serverless application as an in-memory caching layer. In FaaT,

different cache replacement and persistence policies were designed to achieve auto-scaling and transparency.

• Storage optimization: Since the data of serverless functions are generally stored in external storage, characteristics
(e.g., data correctness, scalability, and efficiency) of the storage are essential to cooperate serverless functions. Some

studies have aimed at storage optimization to present new storage systems. Sanity [160] was a storage system to tackle

the limitation of duplicated data. Data de-duplication operation was performed close to the event sources to reduce

, ,

Wen et al.

the function redundancy activation. To use an elastic and distributed data storage, Klimovic et al. [129] presented a

novel storage system named Pocket to access data and automatically scale with the serverless application under desired

performance and cost. Pocket can dynamically adjust resources and provide low latency, high throughput, scalable

resources, and smart data placement across multi-tier storage, such as DRAM, Flash, and disk. Locus [173] used a

mixture of fast but expensive storage and slow but cheap storage to balance the communication performance and

cost. Overall, Pocket and Locus were both to implement the multi-tier storage solution to improve the performance

and cost-efficiency of serverless workloads. A data-driven middleware Zion [184] for object storage was presented to

improve data locality and reduce communication latency. Zion incorporated computation into the data pipeline and

executed it in a scalable way to address the storage’s scalability and resource contention problems. Besides incorporating

computation into the data pipeline, embedding small storage functions into the storage was also noticed by Zhang et

al. [238]. They presented low-latency cloud storage, Shredder, for serverless function chains. Developers can embed

some small storage functions in the storage to directly interact with the required data. These storage functions can

mitigate the network overhead between the serverless application and data.

Mahgoub et al. [147] compared different data passing methods, including VM-Storage, Direct-Passing, and state-of-

practice Remote-Storage. The results showed that these methods performed poorly in all serverless scenarios. Therefore,

they presented a management layer called SONIC to deploy a hybrid approach about three data passing methods to

improve the application performance. A unified API was exposed to developers, allowing them to select the optimal

data passing method according to application-specific factors, such as input payloads. SONIC also can leverage the

optimized storage like Pocket [129] and Locus [238] in its selection methods.

• Network optimization: In practice, current serverless functions cannot directly communicate with each other
via the network. To address this problem, Wawrzoniak et al. [217] presented a system called Boxer to support direct

function-to-function communication in the existing serverless platform. Boxer used the TCP hole-punching technique

of P2P to bypass the network constraints in conventional TCP/IP network stack. Such a system can directly achieve

direct data exchange to benefit serverless computing.

5.4 Stateful FaaS

In the research direction of stateful FaaS, some studies have tried to address it by considering application model design,

log-based mechanism, and architecture design.

• Application model design: Mainstream cloud providers have rolled out their serverless orchestration services.
In fact, these services play a kind of glue to compose serverless functions together. For example, AWS Step Func-

tions [15] uses the state machine to combine multiple serverless functions in different patterns (e.g., sequential or

parallel executions) and provides the state store in this serverless orchestration service. Microsoft Azure used durable

functions [47] as the extension of serverless functions to allow developers to add the state and establish communication.

In academia, Akhter et al. [53] presented a high-level application model to help developers develop and deploy their

stateful applications. This model can transfer serverless functions and the required data as a stateful dataflow graph,

and the data can be shared automatically to achieve scalability and low latency. Another high-level application model

with an extension stateful function language and a compiler [73] was developed to help software developers generate

cloud infrastructure supporting the state access of the serverless application.

• Log-based mechanism: To build serverless stateful functions on existing serverless platforms, Zhang et al. [235]
was inspired by the log-based fault tolerance protocol to propose Beldi. Beldi introduced new refinements to the existing

log-based fault tolerance approach in terms of data structure and specific algorithms. These refinements can record

A Literature Review on Serverless Computing

, ,

the application state and logs. Moreover, Beldi periodically re-executed functions that have not yet finished executing,

guaranteeing the at-least-once execution semantics to prevent duplicated operation execution. In addition, Beldi’s

design motivated Boki’s shared log approach [116] for stateful serverless computing. Boki exported the shared log API

to serverless functions to store the state. Moreover, read and write paths were separated and individually optimized.

However, developers need to adapt the use way of Boki with shared logs to write their applications since Boki-based

development is different from the original development of serverless applications.

• Architecture design: Some studies have designed new architectures for serverless computing to support stateful
applications. Cloudburst [199] was a specified architecture for incorporating states into serverless functions. In Cloud-

burst, it used Anna storage with flexible auto-scaling, which is a good fit for serverless computing. Crucial [66, 67] was

also a new design for providing fine-grained state management. The invocation of a serverless function was mapped to

a thread, and a distributed shared object layer was built on the in-memory data store. This way can guarantee strong

state consistency and simplify the global state semantics across threads. Since the Beldi solution [235] offered the

at-least-once execution semantics for failed function attempts, it may expose fractional writes. Therefore, Sreekanti et

al. presented AFT [198] with stronger fault tolerance. AFT demanded servers to interpose and coordinate all database

accesses and introduced a fault-tolerant shim layer for stateful serverless functions. Moreover, it used built-in atomicity

and idempotence guarantees to provide exact-once semantics with safety and liveness in failures. However, AFT

modified the server running so that it was limited to other platforms.

5.5 Application Modelling

This research direction refers to modeling the serverless application to deepen the understanding of software application

design and reflect the changes in high-level abstraction or low-level implementation. There are two kinds of solutions:

specification-based analysis and configuration-based analysis.

• Specification-based analysis: It is helpful for application modelling to leverage certain specifications [131,
174, 191, 225]. For example, F(X)-MAN [174] was a model considering services and connectors as entities. Different

connectors (e.g., composition connectors, adaptation connectors, and parallel connectors) are composed of atomic

or composite services. Considering the event-driven feature of serverless computing, Wurster et al. [225] used the

standard topology and orchestration specification for cloud applications (TOSCA) to design an event-driven deployment

modelling approach. TOSCA can capture the lifecycle of application provision and management. However, TOSCA was

incomplete since it did not contain the platform’s requirement representation and could not support all application

lifecycle aspects, such as deployment, requirement, and metric ones. Overall, there is no specification language to

model the serverless application in a platform/cloud-independent manner. To address this problem, Kritikos et al. [131]

proposed the extension of the cloud modelling language named CAMEL to specify serverless functions in an independent

manner. CAMEL was a multi-domain-specific language that could cover all application lifecycle aspects. Furthermore,

Yussupov et al. [233] relied on TOSCA and business process model and notation (BPMN) to present a vendor-and

technology-agnostic modelling method. BPMN captured the control flow composed of multiple activities, and this flow

can be represented as a semantically-transparent graphical notation.

• Configuration-based analysis: In serverless computing, the configuration operation is essential for serverless
applications since it represents some developers’ requirements. However, the event-driven feature complicates serverless

application modeling compared to traditional applications. Considering that some configuration information may be

written in a configuration file like “.yml” [42], Obetz et al. [165] analyzed events associated with serverless functions in

this configuration file to construct a new type of call graph for the serverless application. The call graph modeled the

, ,

Wen et al.

relationships between serverless functions, events, and used services. On the other hand, configuration changes may

reflect the low-level implementation of the serverless application. To simplify the low-level implementation, Samea

et al. [182] introduced a model-driven approach to automatically transform the source model of applications into the

low-level implementation by analyzing the configuration.

5.6 Programming Framework

Some limitations of serverless computing urge researchers to present new programming frameworks to serve specific

applications and general applications. Fig. 4 shows the type distribution of programming frameworks presented by

existing studies. We find that designing frameworks for general applications is the most common, accounting for

40.63% of all frameworks. In frameworks of specific applications, the Internet of Things (IoT) scenario is most widely

investigated, accounting for 21.88% of all frameworks. Next, we introduce specific frameworks and general frameworks.

Fig. 4. The type distribution of programming frameworks presented by existing studies.

5.6.1

Specific Applications. Serverless computing has been applied to various specific scenarios, including numerical

computing, video processing, Internet of Things, file processing, big data analytics, and machine/deep learning. However,

different scenarios may pose new challenges that need to be solved in the serverless computing paradigm or have some

problems that leverage serverless computing to overcome further.

• Numerical computing: Traditional numerical computing tasks need scientists to manage infrastructure and
maintain scalability in order to handle the varied resource parallelism. Serverless computing can free scientists from

these burdens. Moreover, serverless computing shows a disaggregated data center pattern. In fact, this disaggregation

makes linear algebra workloads with large dynamic memory and computation requirements obtain benefits. However,

the computation time of such workloads is the dominant overhead. In this situation, a serverless linear algebra framework

named NumPyWren [189] was presented to address this problem. NumPyWren analyzed the data dependencies in

the serverless application to extract the task graph that had potential parallel executions. Parallel functions can use

an intermediate state in a distributed object store to speed up the processing latency. In practice, NumPyWren was

implemented based on PyWren [119], which was a queue-based master-worker approach and processed serverless

functions in parallel when possible.

• Video processing: Video processing workloads would invoke thousands of threads of execution in a few seconds.
However, fine-grained parallelism did not support video encoders. In this demand, ExCamera [102] and Sprocket [60]

6.25 %6.25 %21.88 %6.25 %6.25 %12.50 %40.63 %General ApplicationsNumerical ComputingVideo ProcessingInternet of ThingsFile ProcessingBig Data AnalyticsMachine/Deep LearningA Literature Review on Serverless Computing

, ,

were developed for serverless video processing. ExCamera achieved a high-parallel video encoder to handle chunks

of video. Furthermore, ExCamera used a state machine for tasks to support fine-grained control and overcome the

communication challenge. Sprocket was a scalable video processing framework to enable much more sophisticated video

processing applications. Sproket transformed a single video input according to a user-specified program. Moreover, it

supported dynamic task creation during processing through dynamic levels of parallelism.

• Internet of Things: The development paradigm of serverless computing could be beneficial for IoT applications.
In the industry, AWS presented AWS IoT Greengrass service [11] to process data on edge. Azure designed Azure IoT

edge service [20] to connect edge devices and serverless functions. In academia, some researchers also have focused

on IoT frameworks. A summary is shown in Table 4. Specifically, the event-driven programming pattern and the

separation of computation and storage make IoT applications inefficient. To address the supportability for data-intensive

or dataflow IoT applications, Cheng et al. [80] proposed a functional programming model to allow the movement

between code and data. Moreover, they designed a context-driven orchestration system to optimize deployment and

resource utilization. However, considering that resources are constrained on edge nodes, Pfandzelter and Bermbach [171]

designed a lightweight serverless framework, tinyFaaS, to adapt the edge feature. Specifically, tinyFaaS provided a

specified endpoint with alternative messaging protocols for the communication of low-power devices. Moreover, their

team also presented another approach called AuctionWhisk [68]. AuctionWhisk used the auction-inspired mechanism

to control and arrange function locations across geo-distributed sites. In addition, some cloud platforms may provide

specified hardware resources such as GPU to speed up the application performance. A serverless teleoperable hybrid

cloud system called STOIC [237] was presented to cooperate with edge clouds and public clouds. In STOIC, dynamic

feedback control mechanism and hardware resource acceleration were considered. Dyninka [100] was designed to

define and compose multiple serverless functions leveraging the multi-tier programming paradigm and compiler.

To simplify the deployment operation of IoT applications, a new IoT programming model, CSPOT [222], was designed

to host services at device scale, edge scale, and cloud scale. CSPOT integrated various features, including multi-tier

processing, robustness, compatibility, security guarantee, and record-and-replay debugging. To consider the platform

heterogeneity, Jindal et al. [118] introduced an extension framework of FaaS to address the data access behavior over

heterogeneous clusters via Function Delivery Network (FDN). FDN can deliver the function to the appropriate cluster

according to the required computation and data.

Table 4. A summary of problem goals for IoT frameworks.

Study
Cheng et al. [80]
Bermbach et al. [68, 171]
Zhang et al. [237]
Fortier et al. [100]
Wolski et al. [222]
Jindal et al. [118]

Objective of the problem to be solved
Supportability of data moving for data-intensive IoT applications

Resource constraint and function placement issues on edge nodes

Function composition and communication
Deployment operations simplification
Data access over heterogeneous clusters

• File processing: Serverless computing allows developers to upload their files to the serverless platform. Triggered
serverless functions handle them and return the processed output. However, early serverless platforms were limited

to support all programming languages. Therefore, Pérez et al. [169, 170] introduced a highly-parallel event-driven

programming model, which combined a middleware that can simplify the development and deployment process.

Moreover, it used customized runtime environments, i.e., container image, to bypass the language limitation. However,

, ,

Wen et al.

at the time our paper was written, serverless platforms had provided the deployment way of the container image

to support serverless applications written in any language. Compared with simply providing the source code to the

serverless platform, using a custom container image has to follow specific interface requirements, introducing additional

management efforts for software developers.

• Big data analytics: MapReduce is one of the most widely used programming models for big data applications.
Giménez-Alventosa et al. [106] investigated the suitability that serverless computing was applied in MapReduce tasks.

Moreover, they presented a new framework with high performance for MapReduce tasks. This framework automated

the partitioning of input data according to the allocated memory. Similarly, for MapReduce jobs, Enes et al. [97] relied on

operating-system-level virtualization technology to design a novel scalable framework to provide resources dynamically.
• Machine/deep learning: Serverless computing can provide flexible resource provisioning and easy-to-use de-
ployment opportunity for machine learning (ML) applications. Moreover, the billing advantage of serverless computing

is attractive to developers of machine learning. SIREN [213], a serverless programming framework for distributed

machine learning, was presented to process data via serverless functions. SIREN leveraged deep reinforcement learning

to design a novel serverless scheduler. This scheduler can dynamically adjust the assignment of function instances and

memory sizes in each ML training epoch, reducing the training cost.

ML tasks generally generate and save the trained models in order to be used for the subsequent inference. However,

some models may be larger than the deployment size limit of serverless computing, showing the infeasibility issue.

To fill this gap, researchers presented Gillis [229] and AMPS-Inf [115] frameworks. They used the automatic model

partitioning idea for the large model in the serverless computing environment. Gillis [229] leveraged the coarse-grained

partitioning to transfer multiple consecutive layers as a layer group, and multiple layer groups were parallelized across

multiple serverless functions. AMPS-Inf [115] considered model partitioning dimension and resource provisioning,

which was formulated as a Mixed-Integer Quadratic Programming problem to minimize cost and maintain performance.

Serverless computing is suitable for ML inference tasks due to the short-lived execution feature. Batching is a crucial

factor in improving the execution performance in ML inference tasks. However, serverless computing is stateless, which

may not support batching and its associated setting parameters (batch size and timeout). Ali et al. [57] demonstrated

that ML inference tasks could not benefit from serverless computing without batching. Therefore, they presented the

BATCH framework to support batching for ML tasks in serverless computing using a dispatching buffer. Moreover,

BATCH can provide automated adaptive batching and setting parameters to guarantee the performance and cost.

5.6.2 General Applications. Many general programming frameworks have been designed to make serverless computing

popular and less restrictive and facilitate the practice of broader applications. These frameworks have been based

on various improvement points, including orchestration model design, function scheduling, runtime mechanism,

abstract-based design, and architecture design.

• Orchestration model design: Major cloud providers of serverless computing have presented the corresponding
orchestration services of serverless functions, such as AWS Step Functions [15] and Azure Durable Functions [47].

These services allow software developers to construct complex applications with more serverless functions and various

structures via a central orchestrator or workflow engine. This way is similar to the service composition approach [141],

integrating multiple services with different types to obtain new benefits.

Except for orchestration services from industry, some studies in academia have also proposed new orchestration

models. Baldini et al. [65] presented a robust programming model to express and build the orchestration of existing

software function blocks. Moreover, they provided the foundation for implementing function orchestration leveraging

A Literature Review on Serverless Computing

, ,

OpenWhisk reactive core. Considering that the type of input arguments and intermediate results should be checked

on their compatibility during the function orchestration, Gerasimov [104] designed an orchestration-specific domain

language called Anzer to check the compatibility between serverless functions. GlobalFlow [241] was a new orchestration

model considering the cross-region scenario of serverless functions. There were two strategies, including copy-based

strategy and connector-based strategy. In the copy-based strategy, all serverless functions were copied to execute in one

region. This strategy is suitable for tasks without any data usage or data communication. In the connector-based strategy,

serverless functions were grouped according to regions, and functions with the same region were integrated into a

sub-workflow. Then, a lightweight connector was used to establish the communication of sub-workflows. This strategy

can achieve data locality and reduce the data communication overhead. In addition, Maslov and Petrashenko [154] used

a control-flow graph and the finite automaton to arrange the function orchestration according to developers’ demands.

First, it accepted the workload, wrote to storage, and added to the task queue. Then, it used the graph to represent the

state and connection among serverless functions.

• Function scheduling: Researchers have considered designing efficient schedulers in new programming frame-
works. WUKONG [76, 77] was presented as a serverless-oriented and decentralized framework. It adopted a decentralized

scheduler that incorporated static scheduling and dynamic scheduling. Specifically, first, it partitioned the global task

graph into multiple local subgraphs before and during executions. Then, each WUKONG’s executor scheduled and

executed the functions contained in the subgraph, improving the data locality. Moreover, the data reading and writing

problem was addressed by task clustering and delayed I/O. WUKONG can greatly improve application performance

and resource utilization. However, WUKONG may introduce additional security risks due to its weak isolation.

• Runtime mechanism: For some presented general frameworks, researchers have modified their runtime mecha-
nisms to obtain a stronger processing ability. Specifically, to allow software developers to develop general-purpose

parallel applications in serverless platforms, functionality partition challenges need to be solved to satisfy the ex-

ecution constraint of serverless functions. A programming framework called Kappa [239] was designed to address

these challenges. Kappa ran the serverless application code on the original serverless platforms like AWS Lambda.

Then, it used checkpoints to check the timeout issue of serverless functions and provided concurrency mechanisms

for general-purpose parallel applications. Instead of checking the application code, Al-Ali et al. [55] designed the new

application primitive that provided the process, not the function, to execute a broad class of applications in serverless

architecture. In addition, gg [101] was presented. In gg, a unique intermediation representation layer was designed to

manage various applications by abstracting the computation and storage. Additionally, gg leveraged common services

like dependency management, straggler mitigation, and scheduling to support widely used applications, e.g., software

compilation, unit tests, and video encoding.

• Abstract-based design: Some studies have considered different abstract representations to design general pro-
gramming frameworks. 𝜆 [114] was an operational semantics to keep low-level behaviors of serverless platforms,
including concurrency, function retry, failure, and instance usage. In addition, retro-𝜆 [155] was a concept specified for
serverless applications. It extended the event sources to obtain the application persistence, allowing the application

logic decomposition to support the retroactive programming capability of serverless platforms.

• Architecture design: Serverless platforms restrict the function execution time and storage space of function
instances, making some applications unable to run in the platform. Therefore, Mujezinović and Ljubović [158] proposed

a novel architecture, which combined AWS Lambda with AWS Fargate technology [10] to bypass these limitations.

Moreover, the architecture used the famous producer-consumer architectural pattern to handle applications with

high-frequency data. In addition, to address the shuffle operation problem that needs to exchange data across all

, ,

Wen et al.

instances, Sánchez-Artigas et al. [185] presented the design of Primula, which had the shuffle operator ability by

employing object storage for serverless functions. Moreover, Primula also provided automatic parallelism and data

detection to guarantee executions.

5.7 Application Migration

In studies related to application migration, manual and automated conversion approaches have been presented.

• Manual conversion: For manual conversion approaches, researchers of related studies have illustrated step-by-
step how to transform applications in a manual way. However, major studies have been specific to a certain class of

applications. For example, a partial migration method [64] was proposed to handle web applications. In this method,

some scalable and resource-intensive components were implemented as microservices, while components with short-

lived functionality were transformed as serverless functions. Christidis et al. [81] and Chahal et al. [79] processed

AI-related applications by considering some restricted factors, such as the code size, model storage, training and

inference difference, and performance tuning. Elordi et al. [96] aimed at deep neural network inference tasks. They

provided a specific decomposition methodology, where original applications can be transformed into an application

pattern that can execute on AWS Lambda.

Except for specific applications, Stafford et al. [200] designed multiple refactoring iterations for general applications

to analyze memory usage and execution time before and after the refactoring. Then, they determined the best practices,

such as function type, dependency relationship, and request type.

• Automated conversion: For automated conversion approaches, the application conversion process is done
automatically. However, in this process, applications need to be assessed for their portability on a specific serverless

platform before migration. SEAPORT [231] was an automated assessment method that used a canonical serverless

application model and assessment metrics (e.g., service and component similarity). TheArchitect [168] was a rule-based

system to automatically generate high-level serverless architecture design diagrams, simplifying the conversion process.

However, how to automate conversions is still a critical question. ToLambda [122] targeted Java applications to serverless

applications. It automatically parsed the original code, transferred it as the event-driven functions, and changed service

requests as the function call. With a strategy similar to ToLambda, Node2FAAS [87] converted Node.js monolithic

applications into serverless applications. However, Node2FAAS cannot process or access any external functions or

libraries. Therefore, complex applications still have to be reconstructed manually, leading to an impractical effort.

Ristov et al. [178] also targeted Node.js applications and resolved the problem of Node2FAAS. They presented the

Dependency-Aware FaaSifier (DAF) to automatically transform source code and dependency libraries to equivalent

serverless functions via specific annotations. However, DAF was limited to processing nested functions and recursive

calls of the same function, as well as chained functions.

5.8 Cost

Regarding the cost aspect of serverless computing, related researchers have addressed problems about cost prediction

and cost optimization of serverless applications.

5.8.1 Cost Prediction. Cost prediction studies mainly have two common solutions, including regression model prediction

and statistical learning prediction.

• Regression model prediction: Some cloud providers of serverless computing have provided pricing calculators
on their websites, but these calculators do not generate the corresponding cost for average runtime and memory size.

A Literature Review on Serverless Computing

, ,

Therefore, Cordingly et al. [82] referenced the platform’s pricing policy to estimate the cost for the predicted runtime

performance. The runtime performance was obtained from the regression model that considered different CPUs and

memory settings. Differently, Eismann and Grohmann [92] found that the execution latency of the function was related

to the function’s input parameters. Based on this insight, they presented a prediction approach, which applied the

monitoring data from this function to mixture density networks to predict distributions of execution latency and output

parameters of the serverless function. Then, runtime performance information was combined into a workflow model to

estimate the cost via the Monte-Carlo simulation.

• Statistical learning prediction: Another kind of approach is to consider statistical learning. Considering that
the cost is affected by the allocated memory, Akhtar et al. [52] proposed a framework called COSE. COSE used Bayesian

Optimization to learn the relationship between cost and unseen configurations of a serverless function from trace logs.

COSE can predict the cost under different configurations. Lin and Khazaei [139] treated the serverless application as

the directed acyclic graph. Then, they introduced a probability-based cost graph that can get the average cost of the

serverless application. This graph considered the consumed memory and execution time of each serverless function, as

well as transition probability between serverless functions.

5.8.2 Cost Optimization. In studies related to cost optimization, there are three solutions, including function scheduling,

underlying combination, and resource adjustment.

• Function scheduling: Generally, the serverless application is composed of one or more serverless functions. The
transition from one function to another will increase the number of invocations. However, the price is related to the

number of invocations. In this situation, a possible solution is to fuse multiple functions as a function and then schedule

this function to an appropriate instance for executions. Based on it, Elgamal [95] discussed the problems of function

fusion and function placement and then presented the corresponding cost graph. The cost optimization was concluded

as the constrained shortest path problem about this cost graph to find the best structure. In addition, developers may

use a hybrid public-private cloud to develop and execute their applications. In such an infrastructure environment, the

scheduling strategy of functions is critical in minimizing the cost of public cloud usage while meeting the specified

performance deadline. A hybrid cloud scheduling strategy called Skedulix [84] was presented to solve this problem.

Moreover, Skedulix used a greedy algorithm to determine the function placement dynamically.

• Underlying combination: Specific applications executed in serverless platforms may be costly since serverless
computing is currently good at short-lived and stateless tasks. The combination of serverless computing and VM

rentals may make applications cost more cost-effective. Some studies [108, 111, 146] have presented scalable and hybrid

approaches to analyze and choose the optimal cost in the environment of serverless computing and VM rentals, while

ensuring the performance constraint.

• Resource adjustment: The cost of the serverless function is affected by the pre-allocated memory size. Therefore,
some studies [52, 72, 139, 242] have presented optimization algorithms to provide an optimal configuration setting (e.g.,

memory) that can achieve the minimum cost under performance constraints. In addition to finding the optimal memory

configuration, Spillner [197] measured the memory consumption situation of the serverless function within a particular

time and then created trace profiles in advance to automatically update memory.

5.9 Multi-Cloud Development

To support the multi-cloud development of serverless computing, researchers have mainly designed new serverless

computing architectures or frameworks. Soltani [196] was a Peer to Peer architecture that used container cluster manager

, ,

Wen et al.

technology to allow developers to enjoy the respective strengths of multiple clouds. The multi-cloud development

architecture presented by Vasconcelos [211] contained three key components, i.e., FaaS Cluster, FaaS Proxy, and

Multi-Cloud Resource Allocator. Specifically, FaaS Cluster represented a cluster that contained multiple instances

from geographically different cloud infrastructures, while FaaS Proxy provided the request interface like a gateway.

Multi-Cloud Resource Allocator can control resources in any of the clouds. In addition, Sampé et al. [183] presented

an extensible multi-cloud framework to execute regular Python code in any serverless platform transparently. The

architecture design of this framework was motivated by Python’s multiprocessing and dynamism features.

5.10 Accelerator Support

Generally, the runtime environment of serverless platforms is mainly based on CPU resources. However, more and

more tasks like video processing and deep learning require leveraging other hardware resources to accelerate. In the

related studies, researchers have tried to support GPU and FPGA accelerators in serverless platforms.

5.10.1 GPU Support. To address the GPU supportability, Kim et al. [125] presented a new serverless framework, which

integrated NVIDIA-Docker into the open-source serverless framework. Moreover, this new framework can support the

deployment of GPU-supported containers. However, the biggest disadvantage of this approach is that each GPU cannot

serve multiple function invocations simultaneously.

Different from using GPU-support containers, Naranjo et al. [161] used GPU virtualization. They tried several

virtualized access methods to GPUs, including remote access to GPU devices via the rCUDA framework [41], as well as

direct access to GPU devices via PCI passthrough [39]. The results showed that using GPU virtualization is an efficient

and accelerated way in serverless computing, achieving the sharing of GPUs among functions.

5.10.2

FPGA Support. To make the serverless platform support FPGAs, Ringlein et al. [177] designed a platform

architecture with disaggregated FPGAs for serverless computing. In addition, BlastFunction [62] was a distributed FPGA

sharing platform, which contained multiple device managers to monitor and time-share FPGAs. Moreover, BlastFunction

can achieve multi-tenancy for FPGAs.

5.11 Security

In the security aspect of serverless computing, there are mainly two kinds of solutions, including SGX-based design and

information flow tracking.

• SGX-based design: Serverless applications follow the event-driven paradigm, and they generally are triggered by
HTTP requests; thus, the API gateway is a crucial component. However, this component often suffers from malicious

attacks. To alleviate this situation, Qiang et al. [175] leveraged Intel Software Guard Extensions (SGX) and WebAssembly

sandboxed environment to design Se-Lambda to protect the API gateway. Particularly, SGX can create a trusted

execution environment where sensitive data can be stored and used normally, preventing malicious attackers from

stealing sensitive data from the serverless application. Moreover, Alder et al. [56] used SGX with the trustworthy

resource measurement mechanism in the serverless platform to guarantee security and accountability.

• Information flow tracking: Since serverless applications are composed of multiple serverless functions, using
information flow control (IFC) methods may be suitable and beneficial for information security in serverless computing.

Trapeze [58] was implemented as a dynamic IFC model to track and monitor the global information flow. Trapeze

achieved the security guarantee through static program labeling with dynamic data labeling. However, on one hand,

Trapeze left the burden for developers on the definition of information flow policies and the implementation of

A Literature Review on Serverless Computing

, ,

declassification functions. In contrast, Valve [85] did not require declassification. Valve was a transparent function-level

information flow model, allowing developers to use fine-grained controls in their serverless applications and write

proper policy configurations. On the other hand, Trapeze’s implementation also relied on the programming language of

serverless functions and pre-defined key-value store functions. Based on these limitations of Trapeze, a transparent

approach named will.iam [186] was presented, which was agnostic to serverless functions and underlying platforms.

will.iam can automatically check access control policies and make a decision for requests in advance. Valve and will.iam

were complementary, where Valve was to understand the data flow information of their serverless applications and

write reasonable policies for will.iam.

5.12 Testing

In the distribution environment of serverless computing, serverless functions interact with other functions or cloud

services. It makes serverless application testing hard [136]. Winzinger and Wirtz [221] implemented a data flow testing

framework to obtain the function coverage. This framework used additional instrumentations in the source code to

detect data flows between functions and services, between return values, and between functions and functions.

5.13 Debugging

Debugging the serverless application is an essential step in facilitating serverless computing [134, 205, 218]. However,

few efforts are made, and the current solution is to use the record and replay approach. Watchtower [59] was presented

to observe the application changes through instrumenting libraries. Watchtower can be structured as a serverless

application to scale at the same rate. Then, developers can analyze output logs to detect the violations affecting the

correctness of serverless applications. If a violation was found, a record-and-replay debugger contained in Watchtower

can be used to recreate and re-execute the application from a specific state on the developer’s local machine.

6 RQ3 (PLATFORMS AND VENUES)

To answer the research question of which platforms existing techniques are implemented/evaluated on and where

research papers are published, we view the Implementation and Evaluation sections and other parts that may be

mentioned in each research paper to determine the experimental or evaluated serverless platforms for the presented

solution. Meanwhile, we search the Internet to determine each research paper’s publication venue, e.g., a specific

publication conference or journal. The results are shown in Fig. 5 and Fig. 6.

Fig. 5. The distribution of experimental or evaluated serverless platforms for existing solutions.

32.00 28.57 14.29 6.29 4.00 3.43 3.43 3.43 4.57 05101520253035AWS LambdaCustomSystemOpenWhiskOpenFaaSOpenLambdaGoogle CloudFunctionsIBM CloudFunctionsAzureFunctionsOthersPercentage (%), ,

Wen et al.

Fig. 5 shows the distribution of experimental or evaluated serverless platforms for existing solutions. The distribution

shows that a wide variety of serverless platforms are available for implementing or evaluating proposed solutions. These

serverless platforms include AWS Lambda, custom systems related to serverless computing, OpenWhisk, OpenFaaS,

OpenLambda, Google Cloud Functions, IBM Cloud Functions, Azure Functions, and other serverless platforms like

Alibaba Cloud Function Compute, Knative, etc.

From the percentage of serverless platforms in Fig. 5, we can summarize the following four points. First, AWS

Lambda is the most widely used serverless platform in the serverless computing literature, accounting for 32.00% of all

serverless platforms. Moreover, AWS Lambda is used far more than other commercial serverless platforms like Google

Cloud Functions and Azure Functions. It illustrates that AWS Lambda is more mature regarding serverless features

and infrastructure to help researchers design or present their solutions. For instance, Pocket [129] was a novel storage

system for optimizing the communication problem between serverless functions. Pocket placed the original storage

and collaborated with AWS Lambda to scale with the serverless functions automatically. Second, 28.57% of serverless

platforms are custom systems, the second-largest percentage. To address problems of a certain serverless aspect, some

researchers [54, 74, 88, 121, 127, 208] have designed new serverless platforms to implement or host their solutions. For

example, SAND [54] was a novel, high-performance serverless platform, which designed some custom components

to improve the cold start performance. Kaffes et al. [121] designed a new serverless system to execute highly bursty,

stateless, and short-lived applications. This platform highlighted a centralized core-granular scheduler, which is more

fine-grained than traditional serverless schedulers. The new requirement of the scheduler forced researchers to present

the new underlying design of serverless platforms; thus, their solution was implemented in a custom system. Third,

the most commonly used open-source serverless platform is OpenWhisk, which accounts for 14.29% of all serverless

platforms. OpenWhisk is an open-source implementation with serverless features. Specific details can see in Section 2.

Researchers can modify the underlying architecture of OpenWhisk to add or redesign components to achieve their

goals. For instance, Zhang et al. [240] presented a new insight into using Harvest VMs, whose resource management

differed from traditional underlying resource management for VMs and containers. Considering the characteristics of

Harvest VMs, they redesigned a load balancer on OpenWhisk. Finally, the “Others” category in Fig. 5 refers to serverless

platforms with low usage, containing Alibaba Cloud Function Compute, Knative, Fission, Kubeless, and Huawei’s FaaS

Framework. These serverless platforms account for only 4.57% of the total platforms, illustrating that these platforms

have not been widely used in the serverless computing literature. For example, the Alibaba Cloud Function Compute

team [4] presented a rapid container provisioning approach, FaaSNet [212], to improve its own platform’s container

provisioning speed to serve bursty requests.

Fig. 6 shows the distribution of publication venues for research papers addressing specific aspects. Due to the long

name of the publication venue, we use abbreviations to represent them, and the corresponding full names are shown

in Fig. 6. For example, “SoCC” represents ACM Symposium on Cloud Computing, while “CLOUD” represents IEEE

International Conference on Cloud Computing. The “Others” category represents a collection of publication venues

publishing only one paper related to serverless computing. For example, the work presented by Barcelona-Pons et

al. [67] was published in “TOSEM”, which refers to ACM Transactions on Software Engineering and Methodology.

From Fig. 6, we can summarize three key points. First, we find that research papers related to serverless computing

are published in a wide range of conferences or journals. There were 24 categories of publication venues publishing more

than two or more papers related to serverless computing. Moreover, in the “Others” category, there are 58 publication

venues, where one paper related to serverless computing is published in each publication venue. In total, there are

82 publication venues in the serverless computing literature. Second, the top three publication venues are “SoCC”,

A Literature Review on Serverless Computing

, ,

Fig. 6. The distribution of publication venues for research papers addressing specific aspects.

“CLOUD” and “USENIX ATC”, accounting for 16, 9, and 9 of the 164 papers, respectively. Serverless computing is the

next generation of a promising cloud computing paradigm. Therefore, it is reasonable that more research papers are

published in cloud computing-related conferences like “SoCC” and “CLOUD”. Third, current publication venues contain

two kinds of communities, i.e., the Software Engineering community and the System community. Specifically, some

studies have been published in software engineering-related journals, such as “SPE” (5), “IEEE Software” (4), “TOSEM”

(1), etc. These studies have addressed problems with application modelling [233], programming framework of specific

applications [68, 237], stateful applications [67], multi-cloud development [183], application migration [178], etc. Other

researchers have presented their approaches in system-related conferences or journals, such as “TPDS” (4), “EuroSys”

(4), “ASPLOS” (4), and “SOSP” (2). These approaches mainly resolved problems with resource management [236, 240],

cold start issues [88, 103, 209], function communication [129, 173], etc. Overall, this key point illustrates that current

serverless computing has difficult unresolved issues on the software application side and serverless platform side. In

practice, the biggest beneficiaries of serverless computing are software developers. Existing difficult issues will prevent

software developers from enjoying the advantages of easy development and fast application execution. Therefore,

addressing issues on the software application side and serverless platform side is to better facilitate software developers’

application development practices on serverless platforms.

7 CHALLENGES AND OPEN OPPORTUNITIES

In this section, we aim to discuss key challenges compared to the efforts already made and envision promising

opportunities on the serverless platform side, serverless application side, and serverless computing community side.

5822222222223444445566799160102030405060OthersINFOCOMWWWSOSPICSOCCLUSTERSECHotCloudPACMPLICPEICDCSFGCSCLOSERASPLOSEuroSysTPDSIEEE SoftwareSPENSDIWoSCMiddlewareCCGRIDUSENIX ATCCLOUDSoCCSoCC:               ACM Symposium on Cloud ComputingCLOUD:IEEE International Conference on Cloud ComputingUSENIX ATC:  USENIX Annual Technical ConferenceCCGRID:         International Symposium on Cluster, Cloud and Internet ComputingMiddleware:      International Middleware ConferenceWoSC:International Workshop on Serverless ComputingNSDI:                 Symposium on Network System Design and ImplementationSPE:                   Software: Practice and ExperienceIEEE Software: IEEE SoftwareTPDS:                Transactions on Parallel and Distributed SystemsEuroSys:            European Conference on Computer SystemsASPLOS:International Conference on Architectural Support for Programming Languages and Operating SystemsCLOSER:          International Conference on Cloud Computing and Services ScienceFGCS:                Future Generation Computer SystemsICDCS:              International Conference on Distributed Computing SystemsICPE:             International Conference on Performance EngineeringPACMPL:         Proceedings of the ACM on Programming LanguagesHotCloud:          USENIX Workshop on Hot Topics in Cloud ComputingSEC:             Symposium on Edge ComputingCluster:              IEEE International Conference on Cluster ComputingICSOC:              International Conference on Service-Oriented ComputingSOSP:                 ACM Symposium on Operating Systems PrinciplesWWW:       International World Wide Web ConferencesINFOCOM:       IEEE International Conference on Computer Communications, ,

Wen et al.

7.1 For Serverless Platform Side

Security and efficient direct communication between serverless functions. To alleviate the function communica-

tion problem, researchers have adopted various optimization solutions, such as memory sharing [117, 130], cache-based

design [207, 223], and storage optimization [129, 173]. However, these strategies have still been built on the idea

of temporary data stored in a specific place like memory and external storage for function communication. On the

one hand, this idea may expose security issues, especially for sensitive data. Even if data can be encrypted during

function communication, data shared in memory or cached in containers may still be maliciously attacked or stolen by

other tenants. Therefore, providing a secure serverless platform remains challenging but also an opportunity. On the

other hand, this idea of temporary data stored in a specific place cannot avoid the additional overhead of saving and

fetching or managing data. Boxer [217] supported the direct communication between serverless functions. It leveraged

a modified network, which used TCP hole-punching techniques of P2P to solve the limitation of the conventional

network. However, for large-scale communication-intensive applications, Boxer may not provide high-throughput

and low-latency networks for functions. Therefore, it will elicit a promising research opportunity, i.e., how to design

efficient direct network communication for serverless functions.

Effectiveness of the pricing model. Existing studies about cost prediction and optimization [95, 146, 197, 242] have

been based on the billing model that pays for actually consumed computation resources. Such a billing model is

effective and economical for computation-intensive workloads. However, when developers execute I/O-intensive or

disk-intensive workloads, the current billing model may be expensive, and allocated computation resources are also not

efficiently utilized. In this situation, the challenge that cloud providers of serverless computing face is to further consider

a more suitable billing pattern for various resource types, e.g., CPU, memory, networking, and storage. Moreover,

exiting solutions of cost prediction and optimization studies relied solely on the consumption of computation resources.

A dynamic pricing prediction and optimization scheme may be an opportunity in the future.

Fine-grained resource configuration. Generally, software developers configure a small set of parameters (e.g.,

function timeout, memory size, and cloud providers) for their applications, and these parameters are simple. This

development way frees developers from underlying resource management. However, some experienced developers

may be willing to configure the fine-grained resource policies to effectively improve the overall quality of service of

applications [52]. Therefore, this situation also motivates an opportunity for performance improvement. The serverless

platform can optionally support some fine-grained configuration options about the underlying runtime information.

For example, developers can configure network conditions, function placement, I/O bandwidth, and so on.

Heterogeneous accelerator support. Besides existing studies about GPU support [125, 161] and FPGA support [62,

177], other accelerators like Tensor Processing Unit (TPU) also should be noticed for cloud providers of serverless

computing. However, supporting new accelerators may be very challenging in serverless platforms because it may

require designing a new scheduler, resource allocation pattern, or billing model. Moreover, accelerators generally

have some internal restrictions, e.g., I/O bandwidth and energy efficiency, and thus they are difficult to implement in

the serverless platform. On the other hand, supporting more accelerators may not be economically viable for certain

scenarios. However, providing heterogeneous accelerator support creates a new research dimension of significant

importance for scenarios that use function instances with a mix of CPU, GPU, FPGA, and TPU.

Monitoring tools. The monitoring capability may not be enough in current serverless platforms. Some third-party

monitoring tools, such as Epsagon [26] and Datadog [23], may also be applied to trace the serverless application.

However, they still do not contain resource consumption and infrastructure-related metrics, e.g., CPU utilization,

A Literature Review on Serverless Computing

, ,

network condition, and infrastructure performance, to further understand the serverless application and platform.

Therefore, in the serverless platform, providing comprehensive observability of both serverless applications and

platforms is a complex undertaking, but it is also an opportunity for future research on serverless computing.

7.2 For Serverless Application Side

Generalizability of application conversion approaches. Based on the benign characteristics of serverless comput-

ing, various applications are migrated to the serverless platform for executions. As reported in the characterization

study about serverless applications [93], applications are diverse and not limited to any specific types. However, existing

application conversion approaches have targeted only a few specific applications, such as AI applications [79, 81, 96],

Web applications [64], and Java applications [87, 122]. There are not yet genetic conversion tools for any application.

Though Stafford et al. [200] presented a multiple-refactoring iteration approach, this approach just changed the runtime

results of different states. Then, it determined which change was beneficial to the application. Moreover, this approach

did not provide specific conversion steps. Providing a generic application conversion approach is challenging, which

requires addressing a series of problems, e.g., application type, vendor lock-in, function granularity identification,

event-driven code transformation, cloud service selection, and state communication.

Cold start performance. Although the state-of-the-art solutions for cold start performance optimization (e.g., data

cache-based optimization [54, 164] and snapshot-based optimization [74, 214]) can effectively improve the cold start

performance, they mainly target the acceleration of container creation or runtime initiation. However, containers

generally lack isolation or flexibility due to their inherent nature. Therefore, providing isolation or flexibility for current

common sandboxes like containers is a tricky problem. On the other hand, even if there is a short time for container

acceleration, cold start performance still faces another overhead, i.e., application initialization overhead, which may

take up a large portion of the cold start time in the future. In this situation, it also reveals a new opportunity, i.e., how

to optimize the overhead of application initialization in the case of faster runtime initialization already available.

Variability factor of performance prediction. In related studies of performance prediction for serverless functions,

most solutions have been based on the collected runtime information of a serverless function in a period to predict the

performance value of a constant [52, 82, 90]. However, the auto-scaling feature of serverless computing makes function

performance variable and resource allocation dynamic. It is not enough to rely on factors considered constant value to

predict dynamic performance. Eismann et al. [91] discussed the performance variability of serverless computing and

determined serverless-specific changes, such as uncertainty in cold and warm starts, load intensity, and short-term and

long-term performance fluctuations. Therefore, it is essential that the performance prediction problem of serverless com-

puting take into account the dynamic feature or distributions. Unfortunately, most serverless platforms are untouchable

to software developers and expose little to no information about the underlying environment. Therefore, considering

performance variability in performance prediction is difficult, but it also hints at a future research opportunity.

Data privacy of IoT applications. In the related studies of the programming framework, IoT-specified serverless

frameworks are the most widely studied in specific application scenarios, accounting for 36.84% (7/19) of all specific

frameworks. Presented frameworks have addressed the movement problem of data and code [80], resource constraint

problem of edge devices [171, 237], deployment operation problem [222], heterogeneous cluster access problem [118],

etc. However, these studies did not focus on the data privacy challenge for IoT applications. Edge devices are placed in

different geographical locations and may collect data not wanted to be made public. Moreover, they communicate with

each other. In this situation, edge devices have no unified cloud management like serverless functions; thus, they are

vulnerable to attacks. Data privacy preservation is critical for IoT applications. With respect to data confidentiality,

, ,

Wen et al.

developers can encrypt the data before storing it in external cloud storage when writing IoT functions. However, when

another IoT function queries the required data from the storage, query operation may encounter obstacles in the absence

of decryption. In future work, a possible solution is to design a novel “index”, which uses encryption techniques that

can support the execution of operations or query evaluation.

Testing tools. AWS Lambda can use GUI to invoke serverless functions, indirectly implementing the intent of the

function testing. In practice, there is still a lack of mature testing tools for serverless applications. Although Winzinger

and Wirtz [221] implemented a data flow testing framework, tested applications still need to be deployed in the serverless

platform after modifying the corresponding source code. The most challenging part of implementing testing tools may

be to mock the real serverless environment in the local environment [136]. This is unlike monolithic applications and

microservice applications, where the environment can be tested directly locally. In serverless computing, software

developers do not know which containers will be used in underlying platforms during deployment; thus, it shows an open

challenge for serverless application testing. Moreover, the serverless application is composed of multiple dependent

serverless functions. Integration testing may be necessary to ensure the correctness of the serverless application.

Therefore, providing a serverless-specific testing tool will be a promising opportunity for serverless applications.

7.3 For Serverless Computing Community Side

Representativeness and completeness of benchmark dataset. Studies related to serverless computing have used

some benchmark datasets to verify the efficiency of their solutions [54, 92, 116, 240] or characterization and measure-

ment results [93, 142, 219, 220]. However, we find that these studies have not used a standard benchmark dataset. For

example, SAND [54] used image processing applications, while Boki used the real applications from DeathStarBench

microservices [24]. Moreover, the measurement work [240] used multiple Python serverless functions from Function-

Bench [126]. To characterize serverless applications, the authors [93] collected different applications from open-source

projects, academic literature, industrial literature, and domain-specific feedback. This situation of the used benchmark

dataset indicates that the serverless computing community has not a uniform and representative dataset, which may

be a challenge for the serverless computing field. On the other hand, the completeness of the benchmark dataset (i.e.,

containing diverse application types) is also critical for evaluating generic techniques or frameworks. A complete

dataset can validate the key insights and find potential weaknesses.

Serverless computing is an emerging concept, and its related techniques will continue to be updated and adapted in

the future. In our study, we aim to provide a snapshot of the current research state of the art of serverless computing at

the time of writing. Moreover, we will make the list of collected research papers and summarized data publicly available

to allow other researchers to update the taxonomy of research directions, supplement other solutions, and refresh the

distributions of experimental platforms and publication venues for solutions.

8 CONCLUSION

In this paper, we presented a comprehensive literature review to summarize the current research state of the arts

of serverless computing. Specifically, first, we analyzed 164 research papers to construct a taxonomy containing 18

non-divisible categories linked to research directions of the serverless computing literature. Second, we classified the

related studies of each research direction and elaborated on existing solutions. Third, we investigated the distributions

of experimental serverless platforms for existing solutions and publication venues for selected research papers. Finally,

we discussed key challenges and envisioned promising opportunities for future research on the serverless platform

side, serverless application side, and serverless computing community side. Our analysis of available research work on

A Literature Review on Serverless Computing

, ,

serverless computing can significantly decrease ambiguity and the entry barrier for novice researchers and practitioners.

Moreover, summarized taxonomy, solutions, distributions, and analyses will be of great value for (1) future researchers

to pursue promising research topics and insightful ideas for solutions and (2) future practitioners to conduct best

software practices of serverless application engineering.

REFERENCES

[1] 2018. 2018 serverless community survey: huge growth in serverless usage. https://www.serverless.com/blog/2018-serverless-community-survey-

huge-growth-usage. Retrieved on May 01, 2022.

[2] 2019. A research and markets report. https://www.researchandmarkets.com/reports/4828585/serverless-architecture-market-by-deployment.

Retrieved on May 01, 2022.

[3] 2021. 8 key trends in serverless: a detailed analysis of its usage in the real world. https://www.alibabacloud.com/blog/8-key-trends-in-serverless-

a-detailed-analysis-of-its-usage-in-the-real-world_597977. Retrieved on May 01, 2022.

[4] 2022. Alibaba Cloud Function Compute. https://www.alibabacloud.com/products/function-compute. Retrieved on May 01, 2022.
[5] 2022. Amazon CloudWatch. https://aws.amazon.com/cloudwatch/. Retrieved on May 01, 2022.
[6] 2022. Amazon Elastic Compute Cloud. https://aws.amazon.com/ec2/?nc1=h_ls. Retrieved on May 01, 2022.
[7] 2022. Amazon S3. https://aws.amazon.com/s3/. Retrieved on May 01, 2022.
[8] 2022. Anna. http://highscalability.com/blog/2018/9/8/the-anna-key-value-store-now-has-355x-the-performance-of-dyn.html. Retrieved on May

01, 2022.

[9] 2022. AWS CloudTrail. https://aws.amazon.com/cloudtrail/. Retrieved on May 01, 2022.
[10] 2022. AWS Fargate. https://aws.amazon.com/fargate/. Retrieved on May 01, 2022.
[11] 2022. AWS IoT Greengrass. https://aws.amazon.com/greengrass/. Retrieved on May 01, 2022.
[12] 2022. AWS Lambda. https://docs.aws.amazon.com/lambda/latest/dg/welcome.html. Retrieved on May 01, 2022.
[13] 2022. AWS Lambda pricing. https://aws.amazon.com/lambda/pricing/. Retrieved on May 01, 2022.
[14] 2022. AWS Serverless Application Repository. https://aws.amazon.com/serverless/serverlessrepo/. Retrieved on May 01, 2022.
[15] 2022. AWS Step Functions documentation. https://docs.aws.amazon.com/step-functions/index.html. Retrieved on May 10, 2022.
[16] 2022. Azure App Service. https://azure.microsoft.com/en-us/services/app-service/. Retrieved on May 01, 2022.
[17] 2022. Azure Application Insights. https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview. Retrieved on May 01, 2022.
[18] 2022. Azure Functions. https://docs.microsoft.com/en-us/azure/azure-functions/. Retrieved on May 01, 2022.
[19] 2022. Azure Functions pricing. https://azure.microsoft.com/en-us/pricing/details/functions/. Retrieved on May 01, 2022.
[20] 2022. Azure IoT Edge documentation. https://docs.microsoft.com/en-us/azure/iot-edge/?view=iotedge-2018-06. Retrieved on May 01, 2022.
[21] 2022. Azure Marketplace. https://azuremarketplace.microsoft.com/en-us/. Retrieved on May 01, 2022.
[22] 2022. CouchDB. https://couchdb.apache.org/. Retrieved on May 01, 2022.
[23] 2022. DataDog: modern monitoring and security. https://www.datadoghq.com/. Retrieved on May 10, 2022.
[24] 2022. DeathStarBench: Open-source benchmark suite for cloud microservices. https://github.com/delimitrou/DeathStarBench. Retrieved on May

01, 2022.

[25] 2022. Docker. https://www.docker.com/. Retrieved on May 01, 2022.
[26] 2022. epsagon: application monitoring built for containers and serverless. https://epsagon.com/. Retrieved on May 10, 2022.
[27] 2022. Google App Engine. https://cloud.google.com/appengine. Retrieved on May 01, 2022.
[28] 2022. Google Cloud Functions. https://cloud.google.com/functions. Retrieved on May 01, 2022.
[29] 2022. Google Docs. https://google-docs.en.softonic.com/. Retrieved on May 01, 2022.
[30] 2022. Google Gmail. https://play.google.com/store/apps/details?id=com.google.android.gm&hl=en_US&gl=US. Retrieved on May 01, 2022.
[31] 2022. Google gVisor. https://github.com/google/gvisor. Retrieved on May 01, 2022.
[32] 2022. Kafka. https://kafka.apache.org/. Retrieved on May 01, 2022.
[33] 2022. Kubernetes. https://kubernetes.io. Retrieved on May 01, 2022.
[34] 2022. LLVM. https://llvm.org/. Retrieved on May 01, 2022.
[35] 2022. Nginx. https://www.nginx.com/. Retrieved on May 01, 2022.
[36] 2022. OpenFaaS. https://www.openfaas.com/. Retrieved on May 01, 2022.
[37] 2022. OpenFaaS Function Store. https://github.com/openfaas/store. Retrieved on May 01, 2022.
[38] 2022. Openwhisk. https://openwhisk.apache.org/. Retrieved on May 01, 2022.
[39] 2022. PCI passthrough. https://pve.proxmox.com/wiki/Pci_passthrough. Retrieved on May 01, 2022.
[40] 2022. Prometheus. https://github.com/prometheus/prometheus. Retrieved on May 01, 2022.
[41] 2022. rCUDA. http://www.rcuda.net/. Retrieved on May 01, 2022.
[42] 2022. Serverless Framework. https://www.serverless.com/. Retrieved on May 01, 2022.
[43] 2022. Unikernel. http://unikernel.org/. Retrieved on May 01, 2022.

, ,

Wen et al.

[44] 2022. Visual Studio Code. https://code.visualstudio.com/. Retrieved on May 01, 2022.
[45] 2022. VMFUNC. felixcloutier.com/x86/vmfunc. Retrieved on May 01, 2022.
[46] 2022. WebAssembly. https://webassembly.org/. Retrieved on May 01, 2022.
[47] 2022. What are Durable Functions? https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp.

Retrieved on May 10, 2022.

[48] 2022. Xcode. https://developer.apple.com/xcode/. Retrieved on May 01, 2022.
[49] Cristina L Abad, Edwin F Boza, and Erwin Van Eyk. 2018. Package-aware scheduling of FaaS functions. In Proceedings of the Companion of the 2018

ACM/SPEC International Conference on Performance Engineering. 101–106.

[50] Gojko Adzic and Robert Chatley. 2017. Serverless computing: economic and architectural impact. In Proceedings of the 2017 11th joint meeting on

foundations of software engineering. 884–889.

[51] Alexandru Agache, Marc Brooker, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and Diana-Maria Popa. 2020. Firecracker:
Lightweight virtualization for serverless applications. In Proceedings of the 17th USENIX symposium on networked systems design and implementation.
419–434.

[52] Nabeel Akhtar, Ali Raza, Vatche Ishakian, and Ibrahim Matta. 2020. Cose: Configuring serverless functions using statistical learning. In Proceedings

of the IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE, 129–138.

[53] Adil Akhter, Marios Fragkoulis, and Asterios Katsifodimos. 2019. Stateful functions as a service in action. Proceedings of the VLDB Endowment 12,

12 (2019), 1890–1893.

[54] Istemi Ekin Akkus, Ruichuan Chen, Ivica Rimac, Manuel Stein, Klaus Satzke, Andre Beck, Paarijaat Aditya, and Volker Hilt. 2018. SAND: Towards

high-performance serverless computing. In Proceedings of the 2018 USENIX Annual Technical Conference. 923–935.

[55] Zaid Al-Ali, Sepideh Goodarzy, Ethan Hunter, Sangtae Ha, Richard Han, Eric Keller, and Eric Rozner. 2018. Making serverless computing more

serverless. In Proceedings of the 2018 IEEE 11th International Conference on Cloud Computing. IEEE, 456–459.

[56] Fritz Alder, N Asokan, Arseny Kurnikov, Andrew Paverd, and Michael Steiner. 2019. S-faas: Trustworthy and accountable function-as-a-service

using intel SGX. In Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop. 185–199.

[57] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni. 2020. Batch: machine learning inference serving on serverless platforms with adaptive

batching. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.

[58] Kalev Alpernas, Cormac Flanagan, Sadjad Fouladi, Leonid Ryzhyk, Mooly Sagiv, Thomas Schmitz, and Keith Winstein. 2018. Secure serverless

computing using dynamic information flow control. Proceedings of the ACM on Programming Languages 2, 118 (2018), 1–26.

[59] Kalev Alpernas, Aurojit Panda, Leonid Ryzhyk, and Mooly Sagiv. 2021. Cloud-scale runtime verification of serverless applications. In Proceedings of

the ACM Symposium on Cloud Computing. 92–107.

[60] Lixiang Ao, Liz Izhikevich, Geoffrey M Voelker, and George Porter. 2018. Sprocket: A serverless video processing framework. In Proceedings of the

ACM Symposium on Cloud Computing. 263–274.

[61] Gabriel Aumala, Edwin Boza, Luis Ortiz-Avilés, Gustavo Totoy, and Cristina Abad. 2019. Beyond load balancing: Package-aware scheduling for
serverless platforms. In Proceedings of the 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing. IEEE, 282–291.

[62] Marco Bacis, Rolando Brondolin, and Marco D Santambrogio. 2020. BlastFunction: an FPGA-as-a-service system for accelerated serverless

computing. In Proceedings of the 2020 Design, Automation and Test in Europe Conference and Exhibition. IEEE, 852–857.

[63] Timon Back and Vasilios Andrikopoulos. 2018. Using a microbenchmark to compare function as a service solutions. In European Conference on

Service-Oriented and Cloud Computing. Springer, 146–160.

[64] Deepali Bajaj, Urmil Bharti, Anita Goel, and SC Gupta. 2020. Partial migration for re-architecting a cloud native monolithic application into
microservices and faas. In Proceedings of the International Conference on Information, Communication and Computing Technology. Springer, 111–124.
[65] Ioana Baldini, Perry Cheng, Stephen J Fink, Nick Mitchell, Vinod Muthusamy, Rodric Rabbah, Philippe Suter, and Olivier Tardieu. 2017. The
serverless trilemma: Function composition for serverless computing. In Proceedings of the 2017 ACM SIGPLAN International Symposium on New
Ideas, New Paradigms, and Reflections on Programming and Software. 89–103.

[66] Daniel Barcelona-Pons, Marc Sánchez-Artigas, Gerard París, Pierre Sutra, and Pedro García-López. 2019. On the faas track: Building stateful

distributed applications with serverless architectures. In Proceedings of the 20th International Middleware Conference. 41–54.

[67] Daniel Barcelona-Pons, Pierre Sutra, Marc Sánchez-Artigas, Gerard París, and Pedro García-López. 2022. Stateful serverless computing with crucial.

ACM Transactions on Software Engineering and Methodology 31, 3 (2022), 1–38.

[68] David Bermbach, Jonathan Bader, Jonathan Hasenburg, Tobias Pfandzelter, and Lauritz Thamsen. 2022. AuctionWhisk: Using an auction-inspired

approach for function placement in serverless fog platforms. Software: Practice and Experience 52, 5 (2022), 1143–1169.

[69] David Bermbach, Ahmet-Serdar Karakaya, and Simon Buchholz. 2020. Using application knowledge to reduce cold starts in FaaS services. In

Proceedings of the 35th Annual ACM Symposium on Applied Computing. 134–143.

[70] Vivek M Bhasi, Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita Das. 2021. Kraken:
Adaptive container provisioning for deploying dynamic DAGs in serverless platforms. In Proceedings of the ACM Symposium on Cloud Computing.
153–167.

[71] Sol Boucher, Anuj Kalia, David G Andersen, and Michael Kaminsky. 2018. Putting the" micro" back in microservice. In Proceedings of the 2018

USENIX Annual Technical Conference. 645–650.

A Literature Review on Serverless Computing

, ,

[72] Edwin F Boza, Cristina L Abad, Mónica Villavicencio, Stephany Quimba, and Juan Antonio Plaza. 2017. Reserved, on demand or serverless:
Model-based simulations for cloud budget planning. In Proceedings of the 2017 IEEE Second Ecuador Technical Chapters Meeting. IEEE, 1–6.
[73] Lukas Brand and Markus Mock. 2021. SFL: A compiler for generating stateful aws lambda serverless applications. In Proceedings of the 17th

International Workshop on Serverless Computing. 29–35.

[74] James Cadden, Thomas Unger, Yara Awad, Han Dong, Orran Krieger, and Jonathan Appavoo. 2020. SEUSS: skip redundant paths to make serverless

fast. In Proceedings of the 15th European Conference on Computer Systems. 1–15.

[75] Joao Carreira, Sumer Kohli, Rodrigo Bruno, and Pedro Fonseca. 2021. From warm to hot starts: Leveraging runtimes for the serverless era. In

Proceedings of the Workshop on Hot Topics in Operating Systems. 58–64.

[76] Benjamin Carver, Jingyuan Zhang, Ao Wang, Ali Anwar, Panruo Wu, and Yue Cheng. 2020. Wukong: A scalable and locality-enhanced framework

for serverless parallel computing. In Proceedings of the 11th ACM Symposium on Cloud Computing. 1–15.

[77] Benjamin Carver, Jingyuan Zhang, Ao Wang, and Yue Cheng. 2019. In search of a fast and efficient serverless dag engine. In Proceedings of the 2019

IEEE/ACM Fourth International Parallel Data Systems Workshop. IEEE, 1–10.

[78] Mohak Chadha, Anshul Jindal, and Michael Gerndt. 2021. Architecture-specific performance optimization of compute-intensive faas functions. In

Proceedings of the 2021 IEEE 14th International Conference on Cloud Computing. IEEE, 478–483.

[79] Dheeraj Chahal, Manju Ramesh, Ravi Ojha, and Rekha Singhal. 2021. High Performance Serverless Architecture for Deep Learning Workflows. In

Proceedings of the 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 790–796.

[80] Bin Cheng, Jonathan Fuerst, Gurkan Solmaz, and Takuya Sanada. 2019. Fog function: Serverless fog computing for data intensive iot services. In

Proceedings of the 2019 IEEE International Conference on Services Computing. IEEE, 28–35.

[81] Angelos Christidis, Sotiris Moschoyiannis, Ching-Hsien Hsu, and Roy Davies. 2020. Enabling serverless deployment of large-scale ai workloads.

IEEE Access 8 (2020), 70150–70161.

[82] Robert Cordingly, Wen Shu, and Wes J Lloyd. 2020. Predicting performance and cost of serverless computing functions with SAAF. In Proceedings

of the IEEE International Symposium on Dependable, Autonomic and Secure Computing. IEEE, 640–649.

[83] Abdul Dakkak, Cheng Li, Simon Garcia De Gonzalo, Jinjun Xiong, and Wen-mei Hwu. 2019. Trims: Transparent and isolated model sharing for low
latency deep learning inference in function-as-a-service. In Proceedings of the 2019 IEEE 12th International Conference on Cloud Computing. IEEE,
372–382.

[84] Anirban Das, Andrew Leaf, Carlos A Varela, and Stacy Patterson. 2020. Skedulix: Hybrid cloud scheduling for cost-efficient execution of serverless

applications. In Proceedings of the 2020 IEEE 13th International Conference on Cloud Computing. IEEE, 609–618.

[85] Pubali Datta, Prabuddha Kumar, Tristan Morris, Michael Grace, Amir Rahmati, and Adam Bates. 2020. Valve: Securing function workflows on

serverless computing platforms. In Proceedings of the Web Conference 2020. 939–950.

[86] Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni. 2020. Xanadu: Mitigating cascading cold starts in serverless function chain deployments.

In Proceedings of the 21st International Middleware Conference. 356–370.

[87] Leonardo Rebouças de Carvalho and Aletéia Patrícia Favacho de Araújo. 2019. Framework Node2FaaS: automatic NodeJS application converter for

function as a service.. In Proceedings of the 10th International Conference on Cloud Computing and Services Science. 271–278.

[88] Dong Du, Tianyi Yu, Yubin Xia, Binyu Zang, Guanglu Yan, Chenggang Qin, Qixuan Wu, and Haibo Chen. 2020. Catalyzer: Sub-millisecond
startup for serverless computing with initialization-less booting. In Proceedings of the 25th International Conference on Architectural Support for
Programming Languages and Operating Systems. 467–481.

[89] Vojislav Dukic, Rodrigo Bruno, Ankit Singla, and Gustavo Alonso. 2020. Photons: Lambdas on a diet. In Proceedings of the 11th ACM Symposium on

Cloud Computing. 45–59.

[90] Simon Eismann, Long Bui, Johannes Grohmann, Cristina Abad, Nikolas Herbst, and Samuel Kounev. 2021. Sizeless: Predicting the optimal size of

serverless functions. In Proceedings of the 22nd International Middleware Conference. 248–259.

[91] Simon Eismann, Diego Elias Costa, Lizhi Liao, Cor-Paul Bezemer, Weiyi Shang, André van Hoorn, and Samuel Kounev. 2022. A case study on the

stability of performance tests for serverless applications. Journal of Systems and Software 189 (2022), 111294.

[92] Simon Eismann, Johannes Grohmann, Erwin Van Eyk, Nikolas Herbst, and Samuel Kounev. 2020. Predicting the costs of serverless workflows. In

Proceedings of the ACM/SPEC International Conference on Performance Engineering. 265–276.

[93] Simon Eismann, Joel Scheuner, Erwin Van Eyk, Maximilian Schwinger, Johannes Grohmann, Nikolas Herbst, Cristina Abad, and Alexandru Iosup.
2021. The state of serverless applications: collection, characterization, and community consensus. IEEE Transactions on Software Engineering (2021).
[94] Simon Eismann, Joel Scheuner, Erwin Van Eyk, Maximilian Schwinger, Johannes Grohmann, Nikolas Herbst, Cristina L Abad, and Alexandru Iosup.

2020. Serverless applications: Why, when, and how? IEEE Software 38, 1 (2020), 32–39.

[95] Tarek Elgamal. 2018. Costless: Optimizing cost of serverless computing through function fusion and placement. In Proceedings of the 2018 IEEE/ACM

Symposium on Edge Computing. IEEE, 300–312.

[96] Unai Elordi, Luis Unzueta, Jon Goenetxea, Sergio Sanchez-Carballido, Ignacio Arganda-Carreras, and Oihana Otaegui. 2020. Benchmarking deep

neural network inference performance on serverless environments with MLPerf. IEEE Software 38, 1 (2020), 81–87.

[97] Jonatan Enes, Roberto R Expósito, and Juan Touriño. 2020. Real-time resource scaling platform for big data workloads on serverless environments.

Future Generation Computer Systems 105 (2020), 361–379.

[98] Nafise Eskandani and Guido Salvaneschi. 2021. The wonderless dataset for serverless computing. In Proceedings of 2021 IEEE/ACM 18th International

Conference on Mining Software Repositories. IEEE, 565–569.

, ,

Wen et al.

[99] Henrique Fingler, Amogh Akshintala, and Christopher J Rossbach. 2019. USETL: Unikernels for serverless extract transform and load why should

you settle for less?. In Proceedings of the 10th ACM SIGOPS Asia-Pacific Workshop on Systems. 23–30.

[100] Patrik Fortier, Frédéric Le Mouël, and Julien Ponge. 2021. Dyninka: a FaaS framework for distributed dataflow applications. In Proceedings of the

8th ACM SIGPLAN International Workshop on Reactive and Event-Based Languages and Systems. 2–13.

[101] Sadjad Fouladi, Francisco Romero, Dan Iter, Qian Li, Shuvo Chatterjee, Christos Kozyrakis, Matei Zaharia, and Keith Winstein. 2019. From laptop to
lambda: Outsourcing everyday jobs to thousands of transient functional containers. In Proceedings of the 2019 USENIX Annual Technical Conference.
475–488.

[102] Sadjad Fouladi, Riad S Wahby, Brennan Shacklett, Karthikeyan Vasuki Balasubramaniam, William Zeng, Rahul Bhalerao, Anirudh Sivaraman,
George Porter, and Keith Winstein. 2017. Encoding, fast and slow: low-Latency video processing using thousands of tiny threads. In Proceedings of
the 14th USENIX Symposium on Networked Systems Design and Implementation. 363–376.

[103] Alexander Fuerst and Prateek Sharma. 2021. FaasCache: keeping serverless computing alive with greedy-dual caching. In Proceedings of the 26th

ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 386–400.

[104] Nikita Gerasimov. 2019. The DSL for composing functions for FaaS platform. In Proceedings of the 4th Conference on Software Engineering and

Information Management. 13.

[105] Alim Ul Gias and Giuliano Casale. 2020. Cocoa: Cold start aware capacity planning for function-as-a-service platforms. In Proceedings of the 2020

28th International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems. IEEE, 1–8.

[106] Vicent Giménez-Alventosa, Germán Moltó, and Miguel Caballer. 2019. A framework and a performance assessment for serverless MapReduce on

AWS Lambda. Future Generation Computer Systems 97 (2019), 259–274.

[107] Jake Grogan, Connor Mulready, James McDermott, Martynas Urbanavicius, Murat Yilmaz, Yalemisew Abgaz, Andrew McCarren, Silvana Togneri
MacMahon, Vahid Garousi, Peter Elger, et al. 2020. A multivocal literature review of function-as-a-service (faas) infrastructures and implications
for software developers. In Proceedings of the European Conference on Software Process Improvement. Springer, 58–75.

[108] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. 2019. Spock:
Exploiting serverless functions for slo and cost aware resource procurement in public cloud. In Proceedings of the 2019 IEEE 12th International
Conference on Cloud Computing. IEEE, 199–208.

[109] Adam Hall and Umakishore Ramachandran. 2019. An execution model for serverless functions at the edge. In Proceedings of the International

Conference on Internet of Things Design and Implementation. 225–236.

[110] Hassan B Hassan, Saman A Barakat, and Qusay I Sarhan. 2021. Survey on serverless computing. Journal of Cloud Computing 10, 1 (2021), 1–29.
[111] Shay Horovitz, Roei Amos, Ohad Baruch, Tomer Cohen, Tal Oyar, and Afik Deri. 2018. Faastest-machine learning based cost and performance faas

optimization. In Proceedings of the International Conference on the Economics of Grids, Clouds, Systems, and Services. Springer, 171–186.

[112] MohammadReza HoseinyFarahabady, Young Choon Lee, Albert Y Zomaya, and Zahir Tari. 2017. A qos-aware resource allocation controller for

function as a service (faas) platform. In Proceedings of the International Conference on Service-Oriented Computing. Springer, 241–255.

[113] M Reza HoseinyFarahabady, Albert Y Zomaya, and Zahir Tari. 2017. A model predictive controller for managing QoS enforcements and
microarchitecture-level interferences in a lambda platform. IEEE Transactions on Parallel and Distributed Systems 29, 7 (2017), 1442–1455.
[114] Abhinav Jangda, Donald Pinckney, Yuriy Brun, and Arjun Guha. 2019. Formal foundations of serverless computing. Proceedings of the ACM on

Programming Languages 3, 149 (2019), 1–26.

[115] Jananie Jarachanthan, Li Chen, Fei Xu, and Bo Li. 2021. AMPS-Inf: Automatic model partitioning for serverless inference with cost efficiency. In

Proceedings of the 50th International Conference on Parallel Processing. 1–12.

[116] Zhipeng Jia and Emmett Witchel. 2021. Boki: Stateful serverless computing with shared logs. In Proceedings of the ACM SIGOPS 28th Symposium on

Operating Systems Principles. 691–707.

[117] Zhipeng Jia and Emmett Witchel. 2021. Nightcore: efficient and scalable serverless computing for latency-sensitive, interactive microservices. In
Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 152–166.
[118] Anshul Jindal, Michael Gerndt, Mohak Chadha, Vladimir Podolskiy, and Pengfei Chen. 2021. Function delivery network: Extending serverless

computing for heterogeneous platforms. Software: Practice and Experience 51, 9 (2021), 1936–1963.

[119] Eric Jonas, Qifan Pu, Shivaram Venkataraman, Ion Stoica, and Benjamin Recht. 2017. Occupy the cloud: Distributed computing for the 99%. In

Proceedings of the 2017 symposium on cloud computing. 445–451.

[120] Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth,
Neeraja Yadwadkar, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica, and David A. Patterson. 2019. Cloud programming simplified: A berkeley
view on serverless computing. arXiv preprint arXiv:1902.03383 (2019).

[121] Kostis Kaffes, Neeraja J Yadwadkar, and Christos Kozyrakis. 2019. Centralized core-granular scheduling for serverless functions. In Proceedings of

the ACM symposium on cloud computing. 158–164.

[122] Alex Kaplunovich. 2019. ToLambda–automatic path to serverless architectures. In Proceedings of the 2019 IEEE/ACM 3rd International Workshop on

Refactoring. IEEE, 1–8.

[123] Joanna Kijak, Piotr Martyna, Maciej Pawlik, Bartosz Balis, and Maciej Malawski. 2018. Challenges for scheduling scientific workflows on cloud

functions. In Proceedings of the 2018 IEEE 11th International Conference on Cloud Computing. IEEE, 460–467.

[124] Dong Kyoung Kim and Hyun-Gul Roh. 2021. Scheduling containers rather than functions for function-as-a-service. In Proceedings of the 2021

IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 465–474.

A Literature Review on Serverless Computing

, ,

[125] Jaewook Kim, Tae Joon Jun, Daeyoun Kang, Dohyeun Kim, and Daeyoung Kim. 2018. Gpu enabled serverless computing framework. In Proceedings

of the 2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing. IEEE, 533–540.

[126] Jeongchul Kim and Kyungyong Lee. 2019. Functionbench: A suite of workloads for serverless cloud function service. In Proceedings of the 2019 IEEE

12th International Conference on Cloud Computing. IEEE, 502–504.

[127] Young Ki Kim, M Reza HoseinyFarahabady, Young Choon Lee, and Albert Y Zomaya. 2020. Automated fine-grained cpu cap control in serverless

computing platform. IEEE Transactions on Parallel and Distributed Systems 31, 10 (2020), 2289–2301.

[128] Young Ki Kim, M Reza HoseinyFarahabady, Young Choon Lee, Albert Y Zomaya, and Raja Jurdak. 2018. Dynamic control of CPU usage in a lambda

platform. In Proceedings of the 2018 IEEE International Conference on Cluster Computing. IEEE, 234–244.

[129] Ana Klimovic, Yawen Wang, Patrick Stuedi, Animesh Trivedi, Jonas Pfefferle, and Christos Kozyrakis. 2018. Pocket: Elastic ephemeral storage for

serverless analytics. In Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation. 427–444.

[130] Swaroop Kotni, Ajay Nayak, Vinod Ganapathy, and Arkaprava Basu. 2021. Faastlane: Accelerating function-as-a-service workflows. In Proceedings

of the 2021 USENIX Annual Technical Conference. 805–820.

[131] Kyriakos Kritikos, Pawel Skrzypek, Alexandru Moga, and Oliviu Matei. 2019. Towards the modelling of hybrid cloud applications. In Proceedings of

the 2019 IEEE 12th International Conference on Cloud Computing. IEEE, 291–295.

[132] Seungjun Lee, Daegun Yoon, Sangho Yeo, and Sangyoon Oh. 2021. Mitigating cold start problem in serverless computing with function fusion.

Sensors 21, 24 (2021), 8416.

[133] Youngsoo Lee and Sunghee Choi. 2021. A greedy load balancing algorithm for faaS platforms. In Proceedings of the 2021 5th International Conference

on Cloud and Big Data Computing. 63–69.

[134] Philipp Leitner, Erik Wittern, Josef Spillner, and Waldemar Hummer. 2019. A mixed-method empirical study of Function-as-a-Service software

development in industrial practice. Journal of Systems and Software 149 (2019), 340–359.

[135] Valentina Lenarduzzi, Jeremy Daly, Antonio Martini, Sebastiano Panichella, and Damian Andrew Tamburri. 2020. Toward a technical debt

conceptualization for serverless computing. IEEE Software 38, 1 (2020), 40–47.

[136] Valentina Lenarduzzi and Annibale Panichella. 2020. Serverless testing: Tool vendors’ and experts’ points of view. IEEE Software 38, 1 (2020), 54–60.
[137] Yongkang Li, Yanying Lin, Yang Wang, Kejiang Ye, and Cheng-Zhong Xu. 2022. Serverless computing: state-of-the-art, challenges and opportunities.

IEEE Transactions on Services Computing (2022).

[138] Zijun Li, Linsong Guo, Jiagan Cheng, Quan Chen, BingSheng He, and Minyi Guo. 2021. The serverless computing survey: a technical primer for

design architecture. Comput. Surveys (2021).

[139] Changyuan Lin and Hamzeh Khazaei. 2020. Modeling and optimization of performance and cost of serverless applications. IEEE Transactions on

Parallel and Distributed Systems 32, 3 (2020), 615–632.

[140] Wei Ling, Lin Ma, Chen Tian, and Ziang Hu. 2019. Pigeon: A dynamic and efficient serverless and FaaS framework for private cloud. In Proceedings

of the 2019 International Conference on Computational Science and Computational Intelligence. 1416–1421.

[141] Xuanzhe Liu, Gang Huang, Qi Zhao, Hong Mei, and M Brian Blake. 2014. iMashup: a mashup-based framework for service composition. Science

China Information Sciences 57, 1 (2014), 1–20.

[142] Wes Lloyd, Shruti Ramesh, Swetha Chinthalapati, Lan Ly, and Shrideep Pallickara. 2018. Serverless computing: An investigation of factors

influencing microservice performance (Proceedings of 2018 IEEE International Conference on Cloud Engineering). IEEE, 159–169.

[143] Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, and Liming Zhu. 2021. A systematic literature review on federated machine learning: From a

software engineering perspective. ACM Computing Surveys (CSUR) 54, 5 (2021), 1–39.

[144] Ju Long, Hung-Ying Tai, Shen-Ta Hsieh, and Michael Juntao Yuan. 2020. A lightweight design for serverless function as a service. IEEE Software 38,

1 (2020), 75–80.

[145] Taras Lykhenko, Rafael Soares, and Luis Rodrigues. 2021. FaaSTCC: efficient transactional causal consistency for serverless computing. In

Proceedings of the 22nd International Middleware Conference. 159–171.

[146] Kunal Mahajan, Daniel Figueiredo, Vishal Misra, and Dan Rubenstein. 2019. Optimal pricing for serverless computing. In Proceedings of the 2019

IEEE Global Communications Conference. IEEE, 1–6.

[147] Ashraf Mahgoub, Li Wang, Karthick Shankar, Yiming Zhang, Huangshi Tian, Subrata Mitra, Yuxing Peng, Hongqi Wang, Ana Klimovic, Haoran
Yang, et al. 2021. SONIC: Application-aware Data Passing for Chained Serverless Applications. In Proceedings of the 2021 USENIX Annual Technical
Conference. 285–301.

[148] Nima Mahmoudi and Hamzeh Khazaei. 2020. Performance modeling of serverless computing platforms. IEEE Transactions on Cloud Computing

(2020).

[149] Nima Mahmoudi and Hamzeh Khazaei. 2020. Temporal performance modelling of serverless computing platforms. In Proceedings of the 2020 Sixth

International Workshop on Serverless Computing. 1–6.

[150] Nima Mahmoudi, Changyuan Lin, Hamzeh Khazaei, and Marin Litoiu. 2019. Optimizing serverless computing: introducing an adaptive function
placement algorithm. In Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering. 203–213.
[151] Marcin Majewski, Maciej Pawlik, and Maciej Malawski. 2021. Algorithms for scheduling scientific workflows on serverless architecture. In

Proceedings of the 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 782–789.

[152] Anupama Mampage, Shanika Karunasekera, and Rajkumar Buyya. 2021. Deadline-aware dynamic resource management in serverless computing
environments. In Proceedings of the 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 483–492.

, ,

Wen et al.

[153] Anupama Mampage, Shanika Karunasekera, and Rajkumar Buyya. 2021. A holistic view on resource management in serverless computing

environments: taxonomy and future directions. ACM Computing Surveys (CSUR) (2021).

[154] Vadym Maslov and Andriy Petrashenko. 2021. Distributed serverless computing orchestration based on finite automaton. In Proceedings of the

International Conference on Computer Science, Engineering and Education Applications. Springer, 290–303.

[155] Dominik Meissner, Benjamin Erb, Frank Kargl, and Matthias Tichy. 2018. Retro-𝜆: An event-sourced platform for serverless applications with

retroactive computing support. In Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems. 76–87.

[156] Peter Mell and Timothy Grance. 2011. The NIST Definition of Cloud Computing. https://doi.org/10.6028/NIST.SP.800-145
[157] Anup Mohan, Harshad Sane, Kshitij Doshi, Saikrishna Edupuganti, Naren Nayak, and Vadim Sukhomlinov. 2019. Agile cold starts for scalable

serverless. In Proceedings of the 11th USENIX Workshop on Hot Topics in Cloud Computing.

[158] Ajdin Mujezinović and Vedran Ljubović. 2019. Serverless architecture for workflow scheduling with unconstrained execution environment. In
Proceedings of the 2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics. IEEE, 242–246.
[159] Djob Mvondo, Mathieu Bacou, Kevin Nguetchouang, Lucien Ngale, Stéphane Pouget, Josiane Kouam, Renaud Lachaize, Jinho Hwang, Tim Wood,
Daniel Hagimont, et al. 2021. OFC: an opportunistic caching system for FaaS platforms. In Proceedings of the 16th European Conference on Computer
Systems. 228–244.

[160] Shripad Nadgowda, Nilton Bila, and Canturk Isci. 2017. The less server architecture for cloud functions. In Proceedings of the 2nd International

Workshop on Serverless Computing. 22–27.

[161] Diana M Naranjo, Sebastián Risco, Carlos de Alfonso, Alfonso Pérez, Ignacio Blanquer, and Germán Moltó. 2020. Accelerated serverless computing

based on GPU virtualization. J. Parallel and Distrib. Comput. 139 (2020), 32–42.

[162] Hai Duc Nguyen, Chaojie Zhang, Zhujun Xiao, and Andrew A Chien. 2019. Real-time serverless: Enabling application performance guarantees. In

Proceedings of the 5th International Workshop on Serverless Computing. 1–6.

[163] Edward Oakes, Leon Yang, Kevin Houck, Tyler Harter, Andrea C Arpaci-Dusseau, and Remzi H Arpaci-Dusseau. 2017. Pipsqueak: Lean lambdas
with large libraries. In Proceedings of the IEEE 37th International Conference on Distributed Computing Systems Workshops. IEEE, 395–400.
[164] Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2018. SOCK: Rapid task
provisioning with serverless-optimized containers. In Proceedings of the 2018 USENIX Annual Technical Conference. USENIX Association, 57–70.
[165] Matthew Obetz, Stacy Patterson, and Ana Milanova. 2019. Static call graph construction in aws lambda serverless applications. In Proceedings of

the 11th USENIX Workshop on Hot Topics in Cloud Computing.

[166] Giuseppe De Palma, Saverio Giallorenzo, Jacopo Mauro, and Gianluigi Zavattaro. 2020. Allocation priority policies for serverless function-execution

scheduling optimisation. In Proceedings of the International Conference on Service-Oriented Computing. Springer, 416–430.

[167] Maciej Pawlik, Pawel Banach, and Maciej Malawski. 2019. Adaptation of workflow application scheduling algorithm to serverless infrastructure. In

Proceedings of the European Conference on Parallel Processing. Springer, 345–356.

[168] KJPG Perera and I Perera. 2018. A rule-based system for automated generation of serverless-microservices architecture. In Proceedings of the IEEE

International Systems Engineering Symposium. IEEE, 1–8.

[169] Alfonso Pérez, Germán Moltó, Miguel Caballer, and Amanda Calatrava. 2018. Serverless computing for container-based architectures. Future

Generation Computer Systems 83 (2018), 50–59.

[170] Alfonso Pérez, Germán Moltó, Miguel Caballer, and Amanda Calatrava. 2019. A programming model and middleware for high throughput serverless

computing applications. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing. 106–113.

[171] Tobias Pfandzelter and David Bermbach. 2020. tinyfaas: A lightweight faas platform for edge environments. In Proceedings of the 2020 IEEE

International Conference on Fog Computing. IEEE, 17–24.

[172] Bartłomiej Przybylski, Paweł Żuk, and Krzysztof Rzadca. 2021. Data-driven scheduling in serverless computing to reduce response time. In

Proceedings of the 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 206–216.

[173] Qifan Pu, Shivaram Venkataraman, and Ion Stoica. 2019. Shuffling, fast and slow: Scalable analytics on serverless infrastructure. In Proceedings of

the 16th USENIX Symposium on Networked Systems Design and Implementation. 193–206.

[174] Chen Qian and Wenjing Zhu. 2020. F(X)-MAN: An algebraic and hierarchical composition model for function-as-a-service. In Proceedings of the

32nd International Conference on Software Engineering and Knowledge Engineering. 210–215.

[175] Weizhong Qiang, Zezhao Dong, and Hai Jin. 2018. Se-lambda: Securing privacy-sensitive serverless applications using sgx enclave. In Proceedings

of the International Conference on Security and Privacy in Communication Systems. Springer, 451–470.

[176] Shijun Qin, Heng Wu, Yuewen Wu, Bowen Yan, Yuanjia Xu, and Wenbo Zhang. 2020. Nuka: A generic engine with millisecond initialization for

serverless computing. In Proceedings of the 2020 IEEE International Conference on Joint Cloud Computing. IEEE, 78–85.

[177] Burkhard Ringlein, François Abel, Dionysios Diamantopoulos, Beat Weiss, Christoph Hagleitner, Marc Reichenbach, and Dietmar Fey. 2021. A
case for function-as-a-service with disaggregated FPGAs. In Proceedings of the 2021 IEEE 14th International Conference on Cloud Computing. IEEE,
333–344.

[178] Sasko Ristov, Stefan Pedratscher, Jakob Wallnoefer, and Thomas Fahringer. 2020. Daf: Dependency-aware faasifier for node.js monolithic applications.

IEEE Software 38, 1 (2020), 48–53.

[179] Francisco Romero, Gohar Irfan Chaudhry, Íñigo Goiri, Pragna Gopa, Paul Batum, Neeraja J Yadwadkar, Rodrigo Fonseca, Christos Kozyrakis,
and Ricardo Bianchini. 2021. FaaT: A transparent auto-scaling cache for serverless applications. In Proceedings of the ACM Symposium on Cloud
Computing. 122–137.

A Literature Review on Serverless Computing

, ,

[180] Andrea Sabbioni, Lorenzo Rosa, Armir Bujari, Luca Foschini, and Antonio Corradi. 2021. A shared memory approach for function chaining in

serverless platforms. In Proceedings of the 2021 IEEE Symposium on Computers and Communications. IEEE, 1–6.

[181] Aakanksha Saha and Sonika Jindal. 2018. EMARS: efficient management and allocation of resources in serverless. In Proceedings of the 2018 IEEE

11th International Conference on Cloud Computing. IEEE, 827–830.

[182] Fatima Samea, Farooque Azam, Muhammad Waseem Anwar, Mehreen Khan, and Muhammad Rashid. 2019. A UML profile for multi-cloud service
configuration (UMLPMSC) in event-driven serverless applications. In Proceedings of the 2019 8th International Conference on Software and Computer
Applications. 431–435.

[183] Josep Sampé, Pedro Garcia-Lopez, Marc Sanchez-Artigas, Gil Vernik, Pol Roca-Llaberia, and Aitor Arjona. 2020. Toward multicloud access

transparency in serverless computing. IEEE Software 38, 1 (2020), 68–74.

[184] Josep Sampé, Marc Sánchez-Artigas, Pedro García-López, and Gerard París. 2017. Data-driven serverless functions for object storage. In Proceedings

of the 18th International Middleware Conference. 121–133.

[185] Marc Sánchez-Artigas, Germán T Eizaguirre, Gil Vernik, Lachlan Stuart, and Pedro García-López. 2020. Primula: A practical shuffle/sort operator

for serverless computing. In Proceedings of the 21st International Middleware Conference Industrial Track. 31–37.

[186] Arnav Sankaran, Pubali Datta, and Adam Bates. 2020. Workflow integration alleviates identity and access management in serverless computing. In

Proceedings of the Annual Computer Security Applications Conference. 496–509.

[187] Lucia Schuler, Somaya Jamil, and Niklas Kühl. 2021. AI-based resource allocation: Reinforcement learning for adaptive auto-scaling in serverless

environments. In Proceedings of 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing. IEEE, 804–811.

[188] Mohammad Shahrad, Rodrigo Fonseca, Íñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark
Russinovich, and Ricardo Bianchini. 2020. Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider.
In Proceedings of the 2020 USENIX Annual Technical Conference. 205–218.

[189] Vaishaal Shankar, Karl Krauth, Kailas Vodrahalli, Qifan Pu, Benjamin Recht, Ion Stoica, Jonathan Ragan-Kelley, Eric Jonas, and Shivaram

Venkataraman. 2020. Serverless linear algebra. In Proceedings of the 11th ACM Symposium on Cloud Computing. 281–295.

[190] Jiacheng Shen, Tianyi Yang, Yuxin Su, Yangfan Zhou, and Michael R Lyu. 2021. Defuse: A dependency-guided function scheduler to mitigate cold

starts on faaS platforms. In Proceedings of the 2021 IEEE 41st International Conference on Distributed Computing Systems. IEEE, 194–204.

[191] KR Sheshadri and J Lakshmi. 2021. QoS aware FaaS platform. In Proceedings of the 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud

and Internet Computing. IEEE, 812–819.

[192] Simon Shillaker and Peter Pietzuch. 2020. Faasm: Lightweight isolation for efficient stateful serverless computing. In Proceedings of the 2020 USENIX

Annual Technical Conference. 419–433.

[193] Arjun Singhvi, Arjun Balasubramanian, Kevin Houck, Mohammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella. 2021. Atoll: A

Scalable Low-Latency Serverless Platform. In Proceedings of the ACM Symposium on Cloud Computing. 138–152.

[194] Dalia Sobhy, Rami Bahsoon, Leandro Minku, and Rick Kazman. 2021. Evaluation of software architectures under uncertainty: a systematic literature

review. ACM Transactions on Software Engineering and Methodology 30, 4 (2021), 1–50.

[195] Khondokar Solaiman and Muhammad Abdullah Adnan. 2020. WLEC: A not so cold architecture to mitigate cold start problem in serverless

computing. In Proceedings of the 2020 IEEE International Conference on Cloud Engineering. IEEE, 144–153.

[196] Boubaker Soltani, Afifa Ghenai, and Nadia Zeghib. 2018. Towards distributed containerized serverless architecture in multi cloud environment.

Procedia computer science 134 (2018), 121–128.

[197] Josef Spillner. 2020. Resource management for cloud functions with memory tracing, profiling and autotuning. In Proceedings of the 2020 Sixth

International Workshop on Serverless Computing. 13–18.

[198] Vikram Sreekanti, Chenggang Wu, Saurav Chhatrapati, Joseph E Gonzalez, Joseph M Hellerstein, and Jose M Faleiro. 2020. A fault-tolerance shim

for serverless computing. In Proceedings of the Fifteenth European Conference on Computer Systems. 1–15.

[199] Vikram Sreekanti, Chenggang Wu, Xiayue Charles Lin, Johann Schleier-Smith, Joseph E. Gonzalez, Joseph M. Hellerstein, and Alexey Tumanov.

2020. Cloudburst: Stateful Functions-as-a-Service. Proceedings of the VLDB Endowment 13, 12 (2020), 2438–2452.

[200] Adam Stafford, Farshad Ghassemi Toosi, and Anila Mjeda. 2021. Cost-aware migration to functions-as-a-service architecture. In Proceedings of the

ECSA 2021 Companion Volume, Virtual (CEUR Workshop Proceedings, Vol. 2978).

[201] Kun Suo, Junggab Son, Dazhao Cheng, Wei Chen, and Sabur Baidya. 2021. Tackling cold start of serverless applications by efficient and adaptive

container runtime reusing. In Proceedings of the 2021 IEEE International Conference on Cluster Computing. IEEE, 433–443.

[202] Amoghvarsha Suresh and Anshul Gandhi. 2019. Fnsched: An efficient scheduler for serverless functions. In Proceedings of the 5th International

Workshop on Serverless Computing. 19–24.

[203] Amoghavarsha Suresh, Gagan Somashekar, Anandh Varadarajan, Veerendra Ramesh Kakarla, Hima Upadhyay, and Anshul Gandhi. 2020. Ensure:
Efficient scheduling and autonomous resource management in serverless environments. In Proceedings of the 2020 IEEE International Conference on
Autonomic Computing and Self-Organizing Systems. IEEE, 1–10.

[204] Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler. 2020. Serverless cloud computing (function-as-a-service) patterns: A

multivocal literature review. In Proceedings of the 10th International Conference on Cloud Computing and Services Science.

[205] Davide Taibi, Josef Spillner, and Konrad Wawruch. 2020. Serverless computing-where are we now, and where are we heading? IEEE software 38, 1

(2020), 25–31.

, ,

Wen et al.

[206] Bo Tan, Haikun Liu, Jia Rao, Xiaofei Liao, Hai Jin, and Yu Zhang. 2020. Towards lightweight serverless computing via unikernel as a function. In

Proceedings of the 2020 IEEE/ACM 28th International Symposium on Quality of Service. IEEE, 1–10.

[207] Yang Tang and Junfeng Yang. 2020. Lambdata: Optimizing serverless computing by making data intents explicit. In Proceedings of the 2020 IEEE

13th International Conference on Cloud Computing. IEEE, 294–303.

[208] Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric Rozner, and Siddharth Lanka. 2020. Sequoia: Enabling quality-of-service in serverless computing.

In Proceedings of the 11th ACM Symposium on Cloud Computing. 311–327.

[209] Dmitrii Ustiugov, Plamen Petrov, Marios Kogias, Edouard Bugnion, and Boris Grot. 2021. Benchmarking, analysis, and optimization of serverless
function snapshots. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating
Systems. 559–572.

[210] Erwin Van Eyk, Johannes Grohmann, Simon Eismann, André Bauer, Laurens Versluis, Lucian Toader, Norbert Schmitt, Nikolas Herbst, Cristina L
Abad, and Alexandru Iosup. 2019. The spec-rg reference architecture for faas: From microservices and containers to serverless platforms. IEEE
Internet Computing 23, 6 (2019), 7–18.

[211] Adbys Vasconcelos, Lucas Vieira, Ítalo Batista, Rodolfo Silva, and Francisco V Brasileiro. 2019. DistributedFaaS: execution of containerized serverless
applications in multi-cloud infrastructures.. In Proceedings of the 10th International Conference on Cloud Computing and Services Science. 595–600.
[212] Ao Wang, Shuai Chang, Huangshi Tian, Hongqi Wang, Haoran Yang, Huiba Li, Rui Du, and Yue Cheng. 2021. FaaSNet: Scalable and fast provisioning
of custom serverless container runtimes at alibaba cloud function compute. In Proceedings of the 2021 USENIX Annual Technical Conference. 443–457.
[213] Hao Wang, Di Niu, and Baochun Li. 2019. Distributed machine learning with a serverless architecture. In Proceedings of the IEEE INFOCOM

2019-IEEE Conference on Computer Communications. IEEE, 1288–1296.

[214] Kai-Ting Amy Wang, Rayson Ho, and Peng Wu. 2019. Replayable execution optimized for page sharing for a managed runtime environment. In

Proceedings of the Fourteenth EuroSys Conference 2019. 1–16.

[215] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018. Peeking behind the curtains of serverless platforms. In

Proceedings of the 2018 USENIX Annual Technical Conference, ATC 2018. 133–146.

[216] Cody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, and Denys Poshyvanyk. 2022. A systematic literature review on the use of deep

learning in software engineering research. ACM Transactions on Software Engineering and Methodology 31, 2 (2022), 1–58.

[217] Mike Wawrzoniak, Ingo Müller, Rodrigo Fraga Barcelos Paulus Bruno, and Gustavo Alonso. 2021. Boxer: Data Analytics on Network-enabled

Serverless Platforms. In Proceedings of the 11th Annual Conference on Innovative Data Systems Research.

[218] Jinfeng Wen, Zhenpeng Chen, Yi Liu, Yiling Lou, Yun Ma, Gang Huang, Xin Jin, and Xuanzhe Liu. 2021. An empirical study on challenges of
application development in serverless computing. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 416–428.

[219] Jinfeng Wen and Yi Liu. 2021. A measurement study on serverless workflow services. In Proceedings of the 2021 IEEE International Conference on

Web Services. IEEE, 741–750.

[220] Jinfeng Wen, Yi Liu, Zhenpeng Chen, Junkai Chen, and Yun Ma. 2021. Characterizing commodity serverless computing platforms. Journal of

Software: Evolution and Process (2021), e2394.

[221] Stefan Winzinger and Guido Wirtz. 2021. Data flow testing of serverless functions. In Proceedings of International Conference on Cloud Computing

and Services Science. 56–64.

[222] Rich Wolski, Chandra Krintz, Fatih Bakir, Gareth George, and Wei-Tsung Lin. 2019. Cspot: Portable, multi-scale functions-as-a-service for IOT. In

Proceedings of the 4th ACM/IEEE Symposium on Edge Computing. 236–249.

[223] Chenggang Wu, Vikram Sreekanti, and Joseph M Hellerstein. 2020. Transactional causal consistency for serverless computing. In Proceedings of the

2020 ACM SIGMOD International Conference on Management of Data. 83–97.

[224] Song Wu, Zhiheng Tao, Hao Fan, Zhuo Huang, Xinmin Zhang, Hai Jin, Chen Yu, and Chun Cao. 2022. Container lifecycle-aware scheduling for

serverless computing. Software: Practice and Experience 52, 2 (2022), 337–352.

[225] Michael Wurster, Uwe Breitenbücher, Kálmán Képes, Frank Leymann, and Vladimir Yussupov. 2018. Modeling and automated deployment of
serverless applications using TOSCA. In Proceedings of the 2018 IEEE 11th conference on service-oriented computing and applications. IEEE, 73–80.
[226] Zhengjun Xu, Haitao Zhang, Xin Geng, Qiong Wu, and Huadong Ma. 2019. Adaptive function launching acceleration in serverless computing

platforms. In Proceedings of the IEEE 25th International Conference on Parallel and Distributed Systems. 9–16.

[227] Bowen Yan, Heran Gao, Heng Wu, Wenbo Zhang, Lei Hua, and Tao Huang. 2020. Hermes: Efficient cache management for container-based

serverless computing. In Proceedings of the 12th Asia-Pacific Symposium on Internetware. 136–145.

[228] Hanfei Yu, Hao Wang, Jian Li, Xu Yuan, and Seung-Jong Park. 2022. Accelerating serverless computing by harvesting idle resources. In Proceedings

of the ACM Web Conference. 1741–1751.

[229] Minchen Yu, Zhifeng Jiang, Hok Chun Ng, Wei Wang, Ruichuan Chen, and Bo Li. 2021. Gillis: Serving large neural networks in serverless functions
with automatic model partitioning. In Proceedings of the 2021 IEEE 41st International Conference on Distributed Computing Systems. IEEE, 138–148.
[230] Tianyi Yu, Qingyuan Liu, Dong Du, Yubin Xia, Binyu Zang, Ziqian Lu, Pingchao Yang, Chenggang Qin, and Haibo Chen. 2020. Characterizing

serverless platforms with serverlessbench. In Proceedings of the 2020 ACM Symposium on Cloud Computing. 30–44.

[231] Vladimir Yussupov, Uwe Breitenbücher, Ayhan Kaplan, and Frank Leymann. 2020. SEAPORT: Assessing the portability of serverless applications.

In Proceedings of the 10th International Conference on Cloud Computing and Services Science. 456–467.

A Literature Review on Serverless Computing

, ,

[232] Vladimir Yussupov, Jacopo Soldani, Uwe Breitenbücher, Antonio Brogi, and Frank Leymann. 2021. FaaSten your decisions: A classification

framework and technology review of function-as-a-Service platforms. Journal of Systems and Software 175 (2021), 110906.

[233] Vladimir Yussupov, Jacopo Soldani, Uwe Breitenbücher, and Frank Leymann. 2022. Standards-based modeling and deployment of serverless

function orchestrations using BPMN and TOSCA. Software: Practice and Experience (2022).

[234] N Yuvaraj, T Karthikeyan, and K Praghash. 2021. An improved task allocation scheme in serverless computing using gray wolf Optimization

(GWO) based reinforcement learning (RIL) approach. Wireless Personal Communications 117, 3 (2021), 2403–2421.

[235] Haoran Zhang, Adney Cardoza, Peter Baile Chen, Sebastian Angel, and Vincent Liu. 2020. Fault-tolerant and transactional stateful serverless

workflows. In Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation. 1187–1204.

[236] Hong Zhang, Yupeng Tang, Anurag Khandelwal, Jingrong Chen, and Ion Stoica. 2021. Caerus:NIMBLE task scheduling for serverless analytics. In

Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation. 653–669.

[237] Michael Zhang, Chandra Krintz, and Rich Wolski. 2021. Edge-adaptable serverless acceleration for machine learning Internet of Things applications.

Software: Practice and Experience 51, 9 (2021), 1852–1867.

[238] Tian Zhang, Dong Xie, Feifei Li, and Ryan Stutsman. 2019. Narrowing the gap between serverless and its state with storage functions. In Proceedings

of the ACM Symposium on Cloud Computing. 1–12.

[239] Wen Zhang, Vivian Fang, Aurojit Panda, and Scott Shenker. 2020. Kappa: A programming framework for serverless computing. In Proceedings of

the 11th ACM Symposium on Cloud Computing. 328–343.

[240] Yanqi Zhang, Íñigo Goiri, Gohar Irfan Chaudhry, Rodrigo Fonseca, Sameh Elnikety, Christina Delimitrou, and Ricardo Bianchini. 2021. Faster and
cheaper serverless computing on harvested resources. In Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles. 724–739.
[241] Ge Zheng and Yang Peng. 2019. GlobalFlow: a cross-region orchestration service for serverless computing services. In Proceedings of the 2019 IEEE

12th International Conference on Cloud Computing. IEEE, 508–510.

[242] Lulai Zhu, Giorgos Giotis, Vasilis Tountopoulos, and Giuliano Casale. 2021. RDOF: Deployment optimization for function as a service. In Proceedings

of the 2021 IEEE 14th International Conference on Cloud Computing. IEEE, 508–514.

[243] Pawel Zuk and Krzysztof Rzadca. 2020. Scheduling methods to reduce response latency of function as a service. In Proceedings of the 2020 IEEE

32nd International Symposium on Computer Architecture and High Performance Computing. 132–140.

