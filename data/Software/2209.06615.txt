2
2
0
2

p
e
S
4
1

]
E
S
.
s
c
[

1
v
5
1
6
6
0
.
9
0
2
2
:
v
i
X
r
a

HyperPUT:
Generating Synthetic Faulty Programs
to Challenge Bug-Finding Tools

Riccardo Felici · Laura Pozzi · Carlo A. Furia

USI Università della Svizzera italiana, Lugano, Switzerland

September 15, 2022

Abstract

As research in automatically detecting bugs grows and produces new techniques, having
suitable collections of programs with known bugs becomes crucial to reliably and mean-
ingfully compare the effectiveness of these techniques. Most of the existing approaches
rely on benchmarks collecting manually curated real-world bugs, or synthetic bugs seeded
into real-world programs. Using real-world programs entails that extending the existing
benchmarks or creating new ones remains a complex time-consuming task.

In this paper, we propose a complementary approach that automatically generates pro-
grams with seeded bugs. Our technique, called HyperPUT, builds C programs from a
“seed” bug by incrementally applying program transformations (introducing program-
ming constructs such as conditionals, loops, etc.) until a program of the desired size is
generated. In our experimental evaluation, we demonstrate how HyperPUT can gener-
ate buggy programs that can challenge in different ways the capabilities of modern bug-
finding tools, and some of whose characteristics are comparable to those of bugs in existing
benchmarks. These results suggest that HyperPUT can be a useful tool to support further
research in bug-finding techniques—in particular their empirical evaluations.

Keywords— Program Generation, Testing Benchmarks, Synthetic Bug Injection, Testing Frameworks,

Fuzzing, Symbolic Execution.

1 Introduction

Research in detecting bugs automatically spans several decades, and has produced a wide array of di-
verse tools such as static analyzers, symbolic execution engines, and fuzzers—to mention just a few. In
contrast to this long and successful history of developing bug-finding tools, there still is a somewhat lim-
ited agreement about how to rigorously evaluate and compare their bug-finding capabilities in realistic
settings.

In the last few years, to address this conspicuous gap, we have seen several proposals of ground-truth
benchmarks: curated collection of real programs including known bugs [36] or seeded with synthetic
bugs [29, 55], complete with detailed information about the bugs’ location, triggering inputs, and other

1

 
 
 
 
 
 
fundamental characteristics. Ground-truth benchmarks have been instrumental in improving the rigor
and thoroughness of bug-finding tools—especially those that generate test inputs using symbolic execu-
tion or fuzzing, which are the benchmarks’ usual primary focus. While the usefulness of ground-truth
benchmarks is undeniable, extending a benchmark with additional bugs and programs—not to men-
tion creating a new domain-specific benchmark from scratch—remains a complex and time-consuming
endeavor.

In this paper, we explore a complementary approach to building ground-truth benchmarks, where we
automatically generate from scratch programs with seeded bugs. The idea of generating programs to
be used as test inputs (PUTs: programs under test) has been successfully used for other purposes, such
as to detect semantic compiler bugs that result in incorrect compilation [61].

Our technique, which we call HyperPUT, builds programs starting from a seed that consists of a sim-
ple block that fails when executed; this represents a seeded bug. Then, it repeatedly grows the program
by adding features (branching, looping, and so on) that make it larger and more complex to test. Hy-
perPUT’s generation process is highly configurable: the user can choose aspects such as which features
to include in the generated programs, the range of variability of the generated branching conditions,
and how many programs to generate. Clearly, there is no a priori guarantee that the synthetic PUTs
generated by HyperPUT are representative of real-world bugs. However, a fully synthetic approach
also has clear advantages over manually curated collections: since the whole generation is automatic
and customizable, generating new benchmarks collecting programs with specific characteristics is in-
expensive. In addition, HyperPUT’s generated faulty programs come with precise information about
the bug location and any bug-triggering inputs. Thus, they can supplement the programs in curated
ground-truth benchmark to better evaluate the capabilities of bug-finding tools.

After discussing HyperPUT’s design and implementation in Section 3, in Section 4 we design some
experiments where we generated hundreds of PUTs with bugs using HyperPUT, and we ran three pop-
ular, mature bug-finding tools—AFL, CBMC, and KLEE—on these PUTs. Our goal is demonstrating that
HyperPUT can generate bugs with diverse characteristics, which can challenge different capabilities of
bug-finding tools and can usefully complement the programs in ground-truth benchmarks. To this end,
we follow Roy et al. [55]’s description of the features of “ecologically valid” bugs, and analyze whether
HyperPUT can generate bugs that are fair, reproducible, deep, and rare, and that can exercise the differ-
ent capabilities of common bug-finding techniques. The high-level summary of the experiments, which
we detail in Section 5, confirms that HyperPUT is capable of generating “interesting” buggy programs
that share some characteristics with those of benchmarks. Thus, HyperPUT can support flexible empir-
ical analysis of the capabilities of the various bug-finding tools in a way that complements and extends
what is possible using manually-curated benchmarks.

Contributions. This paper makes the following contributions:

• HyperPUT, a configurable technique to automatically generate PUTs with certain characteristics

and seeded bugs.

• An implementation of the HyperPUT technique in a tool—also named HyperPUT.

• An experimental evaluation of HyperPUT that demonstrates its ability to generate bugs with
characteristics comparable to “ecologically valid” ones [55], which exercise from different angles
the capabilities of bug-finding tools.

The prototype implementation of HyperPUT is available in a public repository [5].

Organization. The rest of the paper is organized as follows. Section 2 discusses the main related
work in the development of benchmarks of bugs, as well as bug-finding techniques and tools. Section 3
describes the HyperPUT technique and its current implementation as a tool with the same name that

2

Organic PUTs

Synthetic PUTs

Organic bugs

FuzzBench
MAGMA
CGC, Test-Comp, SV-Comp (datasets)

Synthetic bugs

LAVA

CSmith
HyperPUT

Table 1: Classification of evaluation benchmarks according to whether they consist of organic
or synthetic bugs within organic or synthetic programs (PUTs). Underlined systems
support the automatic generation of new benchmarks by seeding bugs into existing
programs.

generates programs in C. Section 4 introduces the paper’s research questions, and the experiments that
we carried out to answer the questions. Section 5 presents the results of the experiments, and how
they address the research questions. Finally, Section 6 concludes with a summary and discussion of the
paper’s contributions.

2 Related Work

We discuss related work in two areas: benchmarks of bugs to evaluate bug-finding tools (Section 2.1), and
the main techniques and tools to find bugs and vulnerabilities in programs (Section 2.2). Consistently
with the paper’s main focus, we principally consider techniques and tools that work on programs written
in the C programming language and systems programming.

2.1 Benchmarks of Bugs
Different applications of program analysis, including different approaches to test-case generation, use
different benchmarks, consistent with the goals of the program analysis evaluated using the benchmark.
Here, we focus on extensible benchmarks to evaluate the bug-finding capabilities of test-case generation
frameworks (for brevity, testing framework).

Table 1 shows a natural classification in terms of the origin of programs and their bugs, and displays
the category several well-known benchmarks belong to. A program included in a benchmark can be
organic or synthetic. The bugs of a benchmark’s PUTs can also be organic or synthetic.

Organic programs. An organic program is one that was designed and implemented by human pro-
grammers, and hence reflects the characteristics of real-world programs (or at least a sample of them).
For this reason, many existing benchmarks are based on organic PUTs. For example, the International
Competition on Software Testing (Test-Comp) [13] is a comparative evaluation of automatic tools for
software test generation, which uses benchmarks consisting of C programs equipped with testing ob-
jectives (such as coverage, and bug finding). Similar benchmarks are used by the Competition on Soft-
ware Verification (SV-Comp) [12]. Another example is the CGC dataset, which collects about 300 small
manually-written programs produced for the Darpa Cyber Grand Challenge [4]; for each bug in the
programs, the CGC also includes a triggering input.

Google’s FuzzBench is an open benchmarking platform and service [49] based on open source pro-
grams. FuzzBench has been useful both in the industrial and the academic fields—both to evaluate the

3

capabilities of fuzzing frameworks and to identify their limitations and own bugs.

Organic benchmarks exist also for other programming languages, such as the DaCapo benchmarks [14]

and Defects4J [37] for the Java programming language.

Synthetic programs.
a set of templates, rules, or heuristics.

In contrast, a synthetic program is one that is generated automatically from

CSmith [61] is a program generator mainly employed for validating compilers through differential
testing [48]. It has been used to find several security problems in popular compiler frameworks [46, 30],
including GCC [58] and LLVM [42]. Timotej and Cadar [38] applied a similar combination of grammar-
based program generation and differential testing in order to find bugs in symbolic execution engines.
While tools such as CSmith could be used to build benchmarks that challenge testing frameworks, they
are most directly useful for differential testing, where the goal is comparing the behavior of different
versions of a compiler. HyperPUT revisits some of the ideas behind tools like CSmith (in particular,
grammar-based program generation) so that they are directly applicable to generate PUTs with seeded
bugs.

Organic bugs. An organic bug is one that occurred “in the wild”, and hence comes from a program’s
actual development history. Just like organic programs, organic bugs have the clear advantage of being
realistic. In fact, the majority of current systems for the evaluation of testing frameworks consist of
organic PUTs and organic bugs. The MAGMA benchmark [36] can extend the usability of such “fully
organic” benchmarks by performing “forward-porting” of real bugs to recent version of the target PUT.
This way, a historically relevant bug can still be reproduced (and tested for) in up-to-date setups. Still,
applying MAGMA to new bugs and new PUTs requires substantial manual effort.

Seeding synthetic bugs into an existing program has become an increasingly pop-
Synthetic bugs.
ular approach to generate large benchmarks of bugs, thanks to its scalability compared to manual selec-
tion and curation. The Large-scale Automated Vulnerability Addition (LAVA) dataset [29]—commonly
used to compare fuzzing frameworks—consists of synthetic bugs seeded into existing programs. LAVA’s
bug injection is based on the PANDA dynamic analysis platform [28], built on top of the QEMU emu-
lator [11]. First, an analysis of the target program identifies dead, unused, and available (DUA) bytes of
the input, which can be altered (“fuzzed”) without affecting the program’s behavior. Then, LAVA can
use the DUA bytes to encode vulnerabilities when they are accessed by buffer overflows or other kinds
of inconsistent memory access. In order to work on real-world programs, LAVA’s implementation has
some limitations such as the location of the seeded bugs, which can only be chosen within the code that
has been reached in a previous execution.

Ferrer et al.’s work [31] is an example of fully synthetic benchmarks (consisting of synthetic bugs and
synthetic PUTs) for the Java programming language. Their main goal is generating programs where
every branch is reachable to serve as ground truth when evaluating the branch-coverage capabilities of
testing frameworks.

Mutation testing is another approach based on injecting synthetic bugs in organic programs [40, 41].
The original goal of mutation testing was to measure the bug-detection capabilities of a test suite: the
more “mutants” (i.e., variants of program with injected bugs) trigger failures in the test suite, the more
comprehensive the test suite is [54]. More recently, mutation testing ideas have been applied to different
dynamic analysis techniques, such as fault localization [52, 18]. As a bug-injection technique [33],
mutation testing suffers from the problem of equivalent mutants, which occur when a mutation does not
alter a program’s behavior, and hence the mutant does not actually have a bug; a number of approaches
have tried to address this problem [51, 62, 56].

4

2.2 Bug-finding Tools
A detailed discussion of the main techniques used to find bugs in programs is beyond the scope of the
present paper; we refer the interested readers to surveys [22, 21, 9] and textbooks [6, 54]. In this section,
we briefly describe the bug-finding techniques and tools that feature in our evaluation of HyperPUT—
which are also widely used outside of research.

Fuzz testing (or fuzzing) encompasses a broad spectrum of dynamic techniques to generate
Fuzzing.
program inputs [45, 39]. It is widely used to find bugs in software; Google, for instance, found thousands
of security-related bugs in their software using fuzzing [8]. The key idea of fuzzing is to randomly
mutate a known valid program input (the “seed”) to generate new inputs that may cause the program
to crash or expose other kinds of vulnerabilities. Fuzzers differ according to the kind of strategies they
In particular, black-box fuzzers do not have access to the
use to randomly mutate program inputs.
target program’s control flow, and hence can only generate new inputs independent of the program’s
structure. In contrast, white-box fuzzers can take the program’s control flow into account in order to
generate new inputs that exercise specific portions of the program.

American Fuzzy Lop (AFL) is one of the most popular fuzzing frameworks for C programs. It is a
gray-box coverage-based fuzzer, which means that some of its fuzzing strategies are driven by coverage
information about the analyzed program. Originally developed by Zalewsky [63], different extensions
of AFL—such as REDQUEEN [7], AFLFast [16] and AFL++ [32]—have been introduced more recently
and remain widely used.

Symbolic execution. As the name suggests, symbolic execution executes a program with symbolic
inputs, which are placeholders for every possible valid inputs [21, 9]. As it enumerates different execu-
tion paths, symbolic execution builds path constraints, which are logic formula that encode each path’s
feasibility. Then, a constraint solver such as Z3 [26] determines which abstract paths are feasible, and
generates matching concrete inputs.

Most modern implementations of symbolic execution perform dynamic symbolic execution (also called
“concolic” execution), which combines symbolic and concrete state in order to overcome some limita-
tions of symbolic execution (such as its scalability and applicability to realistic programs) [17]. EXE [20]
and DART [34] pioneered the idea of dynamic symbolic execution. More recently, other tools perfecting
and extending this technique include KLEE [19], SAGE [35], S2e [23], and Angr [57]. KLEE is one of
the most widely used dynamic symbolic execution engines for C programs. It is implemented on top
of LLVM [42], and has been successfully employed to find several bugs in production software, such as
the MINIX [60] and BUSYBOX [1] tools.

Driller is a vulnerability discovery tool that combines symbolic execution and fuzzing [59]. When the
latter fails to make progress, it uses the former to continue the exploration of new execution paths. This
approach is effective to improve code coverage, and to test features such as cryptographic hash functions
and random number generators, which are notoriously difficult for approaches that are exclusively based
on constraint solving. The T-Fuzz fuzzer [53] applies program transformations in order to remove the
conditions guarding some code blocks that are hard to reach. If a crash occurs in these code blocks, it
then checks a posteriori whether the locations are actually reachable in the original program.

Model checking In a nutshell, model checking is verification technique for finite-state models,
which can exhaustively check properties expressed in temporal logic (including reachability properties,
which can be expressed as assertions in the code) or find counterexamples when the properties do not
hold in general [24].

Since real-world programs are not finite state, one needs to introduce some kind of finite-state ab-
straction in order to be able to apply model checking to them. A natural way of doing so is by bounding

5

transformation

code

IC(v1 : long, v2 : long, T, E)

if (v1 == v2) { T } else { E }

SC(s1 : char*, s2 : char*, T, E)

if (strcmp(s1, s2) == 0) { T } else { E }

FL(e : long long, B)

for (long long j = 0; j < e; j++) { do_something(); } B

PC(s : char*, n : int, B)

if (strlen(s) < n) exit(0);
size_t l = 0, h = strlen(s) - 1;
while (h >= l) { if (s[h] != s[l]) exit(0); h--; l++; } B

CC(s : char*, c : char, n : int, T, E)

int count = 0;
for (int k = 0; k < strlen(s); k++) { if (s[k] == c) count++; }
if (count == n) { T } else { E }

Table 2: HyperPUT’s transformations and the corresponding generated code. In a transforma-
tion, lowercase letters denote parameters and uppercase letters denote holes.

the program state to be within a finite (but possibly very large) range. Then, model checking such a
bounded abstraction is not equivalent to verifying the original program, but can still be a very effective
way of thoroughly testing the program and finding bugs. In this paper, we experiment with the popular
CBMC [25] bounded model checker for C programs.

3 How HyperPUT Works

HyperPUT builds arbitrarily complex PUTs by recursively applying transformations to an initial simple
program.

3.1 Transformations

A transformation consists of a program template with (typed) parameters and holes. When we apply
a transformation, we choose concrete values for its parameters and holes. A parameter can be replaced
with any constant or variable of suitable type. A hole is replaced by another snippet of code, which can
be given explicitly or as the result of nesting another transformation. Table 2 lists the transformations
HyperPUT currently supports, together with the code they correspond to. There are five main kinds of
transformations:

IC (integer comparison) introduces a conditional that checks whether the two integer parameters

v1, v2 are equal.

SC (string comparison) introduces a conditional that checks whether the two string parameters s1, s2

are equal.

FL (for loop) introduces a loop that iterates e times (where e is the transformation’s integer parame-

ter), and then executes code B.

PC (palindrome check) introduces a loop that checks whether the string parameter s is a palindrome

of length at least n; if it is, it executes code B.

CC (character counting) introduces a loop that counts the number of occurrences of character pa-
rameter c in string parameter s; if the count equals the integer parameter n, it executes code T ;
if not, it executes code E.

6

Let’s present a few more details about transformation IC, as an example to illustrate how transfor-
mations work. Transformation IC consists of two parameters v1 and v2 and two holes T and E. The
parameters denote two integer values or variables. Then, the transformation introduces a conditional
if that checks whether v1 and v2 have the same value. If they have, T executes; otherwise, E executes.

3.2 Transformation Sequences
More complex PUTs combine several transformations by nesting one inside another. When we specify
a sequence of transformations, we can give a concrete value to any transformation parameter or use a
fresh identifier. In the latter case, HyperPUT will instantiate the parameter with a suitable random value
(usually within a range)—for every PUT generated from the transformation sequence. For example, the
expression IC(atoll(argv[1]), β, assert 0 == 1, exit(0)), where β is a fresh identifier, denotes a conditional
that checks whether the first command-line argument argv[1], when interpreted as an integer, is equal
to a random integer value; if it is, the program fails (assert 0 == 1), otherwise, it exits normally (exit(0)).
We can also use fresh identifiers, instead of concrete code snippets, for holes, to denote that the next
transformation in the sequence will instantiate the hole. In other words, this is just a notational short-
hand that helps readability by avoiding nesting transformations explicitly. For example, the sequence
of two transformations

SC(argv[2], "hello", ;, E) IC(atoll(argv[1]), 69, assert 0 == 1, return(0))

(1)

nests an integer comparison inside the else branch of a string comparison, and thus it is equivalent to
the explicitly nested expression

(cid:16)

SC

argv[2], "hello", ;, (cid:0)IC(atoll(argv[1]), 69, assert 0 == 1, return(0))(cid:1)(cid:17)

and determines the PUT in Figure 1.

1

2

3

4

5

6

7

8

9

int main(int argc, char** argv) {

if (strcmp(argv[2], "hello") == 0)

;
else {

if (atoll(argv[1]) == 69)

assert 0 == 1;

else

return 0;

}

10

}

Figure 1: Specification of a PUT that combines transformations SC and IC as in (1).

Figure 1 also shows that HyperPUT inserts the code generated by applying a sequence of transforma-
tions into a template main function, so that the PUT is a complete program. HyperPUT also automat-
ically generates boilerplate code—such as library includes, and checks that the required command-line
arguments are indeed present—that makes PUTs syntactically correct programs. For simplicity, Figure 1
and all other PUTs shown in the paper omit this boilerplate code.

Reaching inputs. The structure of every transformation also suggests which values of the trans-
formation’s parameters determine an execution of the resulting PUT that reaches code in any of the
transformation’s holes. For example, hole T in transformation IC executes for any v1 = v2; hole B in
transformation FL always executes; hole T in transformation CC executes if s includes n occurrences of

7

characters c; and so on. Based on the transformations’ structure and how they are combined, HyperPUT
outputs, for every PUT it generates, values for all variables used in any transformation’s parameters that
reach any of the PUT’s holes. In Figure 1’s example, there are two variables argv[1] and argv[2], and three
leaf holes at lines 3, 6, and 8; HyperPUT determines that the inputs (cid:104)"", "hello"(cid:105), (cid:104)"69", ""(cid:105), and (cid:104)"", ""(cid:105)
respectively reach each of the leaves.

3.3 Implementation Details
We implemented the HyperPUT technique in a tool with the same name. The tool is implemented in
a combination of C (for the core program-generation functionalitie), Python (front end and connection
of the various modules), and Bash scripts (to run batches of experiments).

The user input to HyperPUT consists of a sequence of transformations specified as described in Sec-
tion 3.2, and a number of PUTs to be generated. HyperPUT’s front end processes this input and passes
the information to the generator engine, which takes care of generating PUTs by applying the transfor-
mation sequences, embedding the resulting code into a main function to build a complete program, and
also recording a reaching input for every generated PUT.

Extensibility. HyperPUT is extensible with new transformations. However, as we demonstrate in
Section 5, the current selection of transformations is already sufficient to generate a large number of
“interesting” PUTs, which can challenge different test-case generators and share some characteristics
with the programs in widely used test-case generation benchmarks.

In principle, HyperPUT’s pipeline could also generate PUTs in programming languages other than
C. To this end, one should extend it with transformations that generate valid snippets of code in other
programming languages.

4 Experimental Design

The experimental evaluation of HyperPUT addresses the following research questions:

RQ1: Can HyperPUT generate bugs that are fair?
RQ2: Are the bugs generated by HyperPUT reproducible?
RQ3: Can HyperPUT generate bugs that are deep and rare?
RQ4: Can HyperPUT generate diverse programs that exercise different capabilities of bug-finding tech-

niques?

This section describes the experiments we designed to answer these research questions. Our exper-
imental design is after Roy et al. [55]’s, modified to suit our goal of evaluating the characteristics of
HyperPUT’s synthetic PUTs.

4.1 Testing Frameworks
To assess the characteristics of the bugs generated by HyperPUT, we ran several testing frameworks on
the generated PUTs and determined which bugs each framework could uncover.

We used testing frameworks implementing different bug-finding techniques for C programs:

• AFL [63] is a popular grey-box fuzzer, which combines random generation of input and coverage

metrics.

8

• CBMC [25] is a bounded model checker for C/C++ programs. Bounded model-checking exhaus-
tively explores a program’s state-space up to a finite size bound, checking for the violation of
basic correctness properties (such as memory safety) and assertions within this explored space.

• KLEE [19] is a state of the art dynamic-symbolic execution engine. Dynamic-symbolic execution
is a white-box testing technique, which uses constraint solving to generate inputs that lead to
exploring new paths in the PUT.

These tools offer numerous configuration options; Table 3 lists the configurations that we used in the
experiments. We deploy each tool in two configurations: we first execute it with its first configuration;
if it fails to find a bug before the timeout expires, we execute it again on the same PUT with its second
configuration (using any remaining time). For brevity, henceforth we use the expression “we run X
on a program P ” to mean “we run the testing framework X using sequentially the two configurations
in Table 3 on P ”.

id framework

configurations

A AFL

C CBMC

K KLEE

afl-clang-fast with options CMPLOG [3], LAF [2], MOpt [44]
afl-clang-fast with default options
automated bounded loop unwinding
loop unwinding with bound 10

symbolic arguments, random state search, LLVM optimization
symbolic arguments, default options

Table 3: Configurations of the testing tools used in the experiments. Each row specifies two

configurations for a testing tool in terms of the used options.

4.2 Experimental Subjects
We generate PUTs in batches, where each batch runs HyperPUT with a sequence of n ≥ 1 transforma-
tions:

T1(p1,1, p1,2, . . . , H1,1, . . .) T2(p2,1, p2,2, . . . , H2,1, . . .) . . . Tn(pn,1, pn,2, . . . , fail(), . . .)

(2)

and a matching sequence of actual parameters p1,1, p1,2, . . . , pn,1, pn,2, . . .. Each transformation Tk in
(2) is one of IC, SC, FL, PC, and CC listed in Table 2. In the experiments, we always nest into the
“then” hole T of conditional transformations IC and SC; therefore, all “else” holes E are simply filled
with a “skip” snippet that does nothing. Snippet fail() indicates code that triggers a crashing bug when
executed; for example, an assertion failure assert 0 == 1 or an out-of-bound error int a[3]; a[4] = 0. In
our experiments, we always add the snippet fail() in the innermost transformation Tn.

Each actual parameter pj is either a random constant of the appropriate type (chosen within a limited
range) or i) argv[i] (for i ≥ 1) for parameters of type char*; ii) atoll(argv[i]) (for i ≥ 1) for parameters of
integer type (int, long, long long). More precisely, parameters v1 in transformation IC, s1 in transforma-
tion SC, and s in transformations PC and CC are always instantiated with a command-line argument;
all other parameters are chosen as random constants within a small range. Table 5 shows the actual
ranges for the randomly chosen parameters in each transformation in the batches that we used in the
experiments. For example, every instance of IC uses an integer between 0 and 255 as its second param-
eter v2. We introduce the described restrictions on the choice of parameters so as to generate PUTs of

9

batch

B1
B2
B10
B100
B1000

n #puts
10
1
45
2
200
2–10
100
100
100
1000

inputs used as parameters v1, s1, s

argv[1]
argv[1], argv[2]
argv[1], . . . , argv[10]
argv[1], . . . , argv[100]
argv[1], . . . , argv[1000]

Table 4: List of the batches of PUTs used in HyperPUT’s experimental evaluation to answer
RQ1, RQ2, and RQ3. For each batch, the table lists the number n of transformations
used to generate each PUT in the batch, the number #puts of different PUTs in the
batch, and the command-line input arguments used as parameters in the transforma-
tions.

transformation parameter min max

IC
SC
FL
PC
CC

v2
s2
e
n
n

0

255

"0"

"255"

0

1

1

255

20

20

Table 5: Range of values, between a minimum and a maximum value, for the parameters of

the transformations in Table 2 used in the experiments.

homogeneous characteristics, where the number and kinds of transformations used to generate them
are the primary determinant of their complexity. These constraints also ensure that, in every generated
PUT, i) there is exactly one bug; ii) there is (at least one) program input that triggers the bug. Hyper-
PUT’s reaching input for the unique bug’s location is thus also the triggering input that ensures that
the bug is executable.

Batches.
of 455 PUTs in 5 batches. Table 4 outlines the characteristics of each batch.

For the experiments with HyperPUT to answer RQ1, RQ2, and RQ3, we generated a total

Batch B1 includes 10 PUTs, each consisting of a single transformation.
Batch B2 includes 45 PUTs, each consisting of two different transformations.
Batch B10 includes 200 PUTs, each consisting of between 2 and 10 transformations (possibly with rep-
etitions), with the transformations and the actual length chosen randomly. More precisely, this
batch includes: i) 1 PUT consisting of 2 transformations; ii) 4 PUTs consisting of 3 transforma-
tions; iii) 9 PUTs consisting of 4 transformations; iv) 41 PUTs consisting of 5 transformations;
v) 44 PUTs consisting of 6 transformations; vi) 43 PUTs consisting of 7 transformations; vii) 29
PUTs consisting of 8 transformations; viii) 20 PUTs consisting of 9 transformations; ix) 9 PUTs
consisting of 10 transformations.

Batch B100 includes 100 PUTs, each consisting of exactly 100 transformations (possibly with repeti-

tions) chosen randomly.

10

Figure 2: Distribution of size (in number of transformations) of the PUTs used in the experi-

mental evaluation.

Batch B1000 includes 100 PUTs, each consisting of exactly 1000 transformations (possibly with repe-

titions) chosen randomly.

Henceforth, B denotes the union of all batches B1 ∪ B2 ∪ B10 ∪ B100 ∪ B1000. Figure 2 overviews the
distribution of all PUTs in B.

For the experiments with HyperPUT to answer RQ4, we generated another 60 PUTs in 6 batches
BIC, BSC, BFL, BPC, BCC, B(cid:63). For each transformation T among IC, SC, FL, PC, and CC, batch BT
consists of 10 PUTs P 1

corresponds to the sequence of transformations

. Each PUT P m
T

T , . . . , P 10
T

T (p1,1, p1,2, . . . , H1,1, . . .) T (p2,1, p2,2, . . . , H2,1, . . .) . . . T (pm,1, pm,2, . . . , fail(), . . .)

(3)

with m transformations, all equal to T . In other words, BT consists of increasingly long sequences of the
;
same transformation T repeated multiple times. Similarly, batch B(cid:63) consists of 10 PUTs P 1
(cid:63) , . . . , P 10
(cid:63)
each PUT P m
corresponds to the sequence of transformations (2), with n = m transformations, each
(cid:63)
transformation randomly chosen (possibly with repetitions) among IC, SC, FL, PC, and CC.

4.3 Experimental Setup
We ran all experiments on an Intel® CoreTM i5 machine with 2 cores and 8 GB of RAM running Ubuntu
18.04 Bionic, LLVM 6.0.1, AFL 2.68c, CBMC 5.10, and KLEE 2.1.

Every PUT generated by HyperPUT accepts command-line arguments as input for its main function.
This is the only input that a testing tool controls when testing a PUT. For example, when running KLEE,
the command line argument array argv is instrumented with klee_make_symbolic, and the rest of the PUT
is unmodified.

11

Each experiment runs one of the tools in Table 3 on a PUT with a timeout of 1 hour. The outcome
is success if the testing framework successfully generates command-line inputs that trigger the fail()
injected bug in the PUT. To accommodate fluctuations due to the operating system’s nondeterministic
scheduling, as well as in possible randomization used by the testing frameworks, we repeat each exper-
iment four to ten times, and report the average wall-clock running time as the experiment’s duration.
The outcome is success if at least one of the repeated runs is successful (i.e., it triggers the bug).

4.4 RQ1: Fairness
A collection of bugs is fair if state-of-the-art bug detection techniques, especially those that are widely
used in practice, can discover the bugs with reasonable effort; and if it is not strongly biased in favor or
against any one detection technique. For a PUT-generation system like HyperPUT, fairness means that
it should be capable of generating bugs with a broad spectrum of “detection hardness”—from simple to
very challenging to discover.

To demonstrate fairness, we ran each of the tools AFL, CBMC, and KLEE on all PUTs in B. We then

analyzed which tools were successful in triggering the bugs in the PUTs within the timeout.

4.5 RQ2: Reproducibility
A bug is reproducible if there is a known input that consistently triggers the bug. For a PUT-generation
system like HyperPUT, reproducibility also entails that the PUTs compile without errors and do not rely
on any undefined behavior of the C language. All PUTs generated by HyperPUT come with an input
that triggers their unique bug. To assess reproducibility, we ran each PUT generated in the experiments
with the triggering input, and checked whether the bug was triggered as expected.

HyperPUT generates PUTs that should be syntactically and semantically correct. To confirm this, we
compiled each PUT generated in the experiments using both GCC (with options -O0 -Wall and -O1 -Wall)
and LLVM (with options -O0 -Wall and -O1 -Wall), and checked that: i) both compilations succeeded with-
out errors; and ii) both compiled versions behaved in the same way—namely, they fail when executed
with the triggering input. To detect the potential presence of undefined behavior, we also checked ev-
ery generated PUT using LLVM’s Undefined Behavior Sanitizer,1 a static analysis that can detect several
instances of undefined behavior.

4.6 RQ3: Depth and Rarity
Depth and rarity are two different ways of assessing the “hardness” of a bug for bug-detection tech-
niques.

4.6.1 Depth
A bug is deep if triggering it requires to follow a long sequence of statements and branches. For a PUT-
generation system like HyperPUT, bug depth depends on the structure and complexity of the PUTs
themselves. To determine whether HyperPUT’s bugs are deep, we measured the following on every
PUT in batch B generated in the experiments:

• The cyclomatic complexity of the PUT.2 Cyclomatic complexity [47] is a static measure of com-
plexity of a program’s branching structure, which counts the number of distinct simple execution
paths a program has.

1IBSEN: https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html.
2Measured using CCCC [43] and PMCCABE [10] open source tools.

12

In order to assess the complexity of HyperPUT’s PUTs compared to that of programs in other
benchmarks, we compare the cyclomatic complexity of PUTs in B to that of programs in CGC [4]
and LAVA-1 [29]. Note that the PUTs generated by HyperPUT consist of a single main function,
but programs in other benchmarks usually consist of several different functions; thus, we mea-
sure the cyclomatic complexity of each function in the programs in isolation, and report statistics
about their distribution in each benchmark. We only measure the cyclomatic complexity of func-
tions in the actual PUTs, not in any external library that is used by the PUTs.

• The length (in number of instructions executed at runtime) of the execution path that goes from
the PUT’s entry to the bug-triggering statement,3 when the PUT is executed with a triggering
input. Path length is a dynamic measure of how deep a bug is within a path that triggers it.
Similarly to cyclomatic complexity, we compare the path length of bugs in B to that of bugs in
benchmark LAVA-1.

4.6.2 Rarity
A bug is rare if it is only triggered by a small fraction of all possible program inputs.

To determine whether HyperPUT’s bugs are rare, we ran KLEE on each buggy PUT with a timeout of

1 hour and measure the following:

• The number f of test cases generated by KLEE before first triggering the bug.

• The number t of test cases, among those generated within the timeout, that trigger the bug.

These measures give an idea of how sparse the bug-triggering inputs are in the space of all inputs that
are generated by a systematic strategy.

In order to be able to compare HyperPUT’s measures of rarity with those of other benchmarks’, we
only considered PUTs in batch B≥6 for this experiment. Batch B≥6 consists of the 19 PUTs in B10 with
6, 7, 8, 9, or 10 transformations that KLEE can discover within the 1-hour timeout. We exclude PUTs
with a much smaller or much larger input space, where these metrics would be arguably less robust and
less indicative of rarity. We also exclude PUTs whose bugs KLEE cannot uncover, as the measures f and
t are essentially undefined in these cases.

We compare these metrics of rarity for HyperPUT to those reported by Roy et al. [55] for 41 manually
seeded bugs in the TCAS benchmark [27], as well as 82 synthetic bugs seeded using their Apocalypse
system in the same TCAS programs. More precisely, Table 4 in [55] reports the number of all bug-
triggering tests generated by KLEE within 1 hour, which corresponds to measure t. Figure 5 in [55]
plots the number of tests generated by KLEE before hitting a first bug, which corresponds to measure
f . We directly compare these to the same measures on HyperPUT’s PUTs, without repeating [55]’s
experiments. We only use KLEE to investigate rarity both because it is a standard choice for this kind
of assessment [55], and because its systematic exploration of program paths provides a more robust
measure than others (such as testing time) that are strongly affected by the sheer size and complexity
of the PUT as a whole—as opposed to its bugs’ specifically.

4.7 RQ4: Capabilities
To further demonstrate the flexibility of HyperPUT’s generation, we look more closely at how different
bug-finding tools perform on different batches of PUTs generated by HyperPUT. Which PUTs are easier
or harder to analyze suggests which capabilities of the bug-finding tools are more or less effective to
analyze programs with certain features.

3Measured using the profiling tool Cachegrind [50].

13

batch

%

#

%

#

%

#

%

0
0%
0%
0
22.0% 44
0
0%
0
0%

0
0%
2.2%
1
12.0% 24
0
0%
0
0%

0
0%
0.0%
0
6.5% 13
0
0

0%
0%

0%
15.6%
4.0%
0%
0%

#

0
7
8
0
0

%

#

% #

%

#

%

#

0
0%
17.8%
8
8.5% 17
0
0

0%
0%

10% 1
0% 0
0.5% 1
0% 0
0% 0

90%

9
64.4% 29
5.5% 11
0
0

0%
0%

0
0%
0
0%
82
41.0%
100% 100
100% 100

9.7% 44

5.5% 25

2.9% 13

3.3% 15

5.5% 25

0.4% 2

10.8% 49

62.0% 282

○
○␣
○␣

○␣
○
○␣

○␣
○␣
○

○
○
○␣

○
○␣
○

○␣
○
○

○
○
○

○␣
○␣
○␣

B1
B2
B10
B100
B1000

B

A
C
K

Table 6: For each combination of tools (those marked by ○ in each column), for each batch of
PUTs used in the experiments, the percentage % and the absolute number # of PUTs
in the batch whose unique bugs were triggered exclusively by the tests generated by
those tools. For example, the leftmost column indicates that tool A managed to find
bugs in 44 PUTs in batch B (9.7% of all PUTs in B), which no other tool could find.

We ran each of the tools AFL, CBMC, and KLEE on the PUTs in BIC, BSC, BFL, BPC, BCC, and B(cid:63). Since
these batches include multiple repetitions of the same transformation, they demonstrate the genera-
tion of PUTs with homogeneous characteristics. By observing how each tool’s bug-finding capabilities
change in different batches, and within each batch as the same transformation is repeated multiple times,
we can outline each tool’s strengths and weaknesses in comparison with the other tools’ and link them
to the characteristics of the transformations.

5 Experimental Results

5.1 RQ1: Fairness
Table 6 reports, for each batch of PUTs in Table 4, which testing tools were able to generate inputs
triggering the PUTs’ unique bugs in our experiments. Row B corresponds to all PUTs used in these
experiments. At least one of the tools A, C, and K managed to detect bugs in 67.8% of all PUTs with
less than 100 transformations. The distribution is not strongly biased in favor of any tool—even though
A was noticeably more effective than K and C, as it was the only tool capable of detecting the bugs in
9.7% of all PUTs. On the other hand, every tool was somewhat effective, and all three of them detected
10.8% of the bugs.

Among the individual batches of PUTs, B10 is the “fairest”, in that it includes PUTs that are challeng-
ing for each individual testing tool. In contrast, the PUTs in batches B1 and B2 are generally simple
to analyze for most of the tools; and the PUTs in batches B100 and B1000 are overly complex, so much
that no testing tool could detect their bugs in the allotted time. These results are a consequence of the
different parameters chosen to create the PUTs in these batches. Overall, these results suggests that
HyperPUT can generate PUTs with bugs that are fair, as they are a mix of elusive (highly challenging)
bugs and simpler bugs that most practical testing frameworks can discover.

5.2 RQ2: Reproducibility
As expected, all PUTs produced by HyperPUT for our experiments passed the reproducibility checks
discussed in Section 4.5. Namely:

14

1. Running each PUT on HyperPUT’s generated input triggers the unique bug in the PUT.

2. The PUTs compile without errors or warnings.

3. The PUTs behave in the same way regardless of which compiler is used to compile them.

4. LLVM’s Undefined Behavior Sanitizer does not report any source of undefined behavior in the

PUTs.

These checks confirm that HyperPUT produces PUTs with reproducible seeded bugs, since they are

well-formed and behave consistently as expected.

5.3 RQ3: Depth and Rarity

5.3.1 Depth
Figure 3 summarizes the distribution of cyclomatic complexity measures for the functions in the PUTs
generated by HyperPUT (batch B), and compares it to the functions featuring in the benchmarks CGC
and LAVA-1. HyperPUT can generate very complex PUTs according to this metric: even though some
of CGC’s programs are an order of magnitude more complex, HyperPUT’s PUTs cover a broad range
of cyclomatic complexities, and are those with the highest average complexity. This is a consequence
of the way we configured HyperPUT to generate also large and complex PUTs in batches B100 and
B1000 (as described in Section 4.2). It suggests that HyperPUT is capable of generating simple as well
as complex PUTs, and hence can generate a diverse collection of synthetic buggy programs.

Cyclomatic complexity measures the branching complexity of programs, which is only a proxy for
the complexity of the bugs that appear in the programs. In principle, a very complex program may have
very shallow bugs if they occur in the first few lines of executable code. Path length—the number of
instructions executed from program entry until the bug is triggered—better assesses the depth of the

HyperPUT

cgc

lava-1

444
18
727
3
1902

14
13
144
1
16386

12
4
20
1
179

Mean
Median
Stddev
Min
Max

Functions

455

22893

18906

(b) Statistics about the distributions of cyclo-

matic complexity per function.

(a) Box plots of the distributions of cyclomatic
complexity per function. The vertical axis uses
a logarithmic scale.

Figure 3: Distributions of cyclomatic complexity per function in three collections of buggy pro-
grams: the PUTs in batch B generated by HyperPUT, and benchmarks CGC [4] and
LAVA-1 [29].

15

Mean
Median
Stddev
Min
Max

Bugs

HyperPUT

lava-1

4 858 702
486 500
7 710 406
210 260
22 936 332

4 108 228
3 339 297
1 644 766
2 728 637
8 775 398

455

69

(b) Statistics about the distributions of path

length per injected bug.

(a) Box plots of the distributions of path length per
bug. The vertical axis uses a logarithmic scale.

Figure 4: Distributions of the length of the execution path on a bug-triggering input in two
collections of buggy programs: the PUTs in batch B generated by HyperPUT, and
benchmark LAVA-1 [29].

synthetic bugs in HyperPUT’s generated PUTs. Figure 4 summarizes the distribution of path length for
each bug in the PUTs generated by HyperPUT (batch B), and compares it to the path length of synthetic
bugs in the benchmark LAVA-1.

HyperPUT’s synthetic bugs are deeper on average (mean), but LAVA-1’s bugs are not that far behind,
and have a much higher median. In fact, HyperPUT’s have a higher standard deviation, as the batch
B includes both small PUTs with shallow short-path bugs and large PUTs with bugs that are deeply
nested.

As for other measures, this variety is a direct consequence of the way we configured HyperPUT
(as described in Section 4.2). Overall, HyperPUT can generate shallow as well as deep bugs, including
several that exhibit metrics similar to those of organic bugs.

5.3.2 Rarity
Table 7 shows statistics about the rarity of bugs in HyperPUT’s PUTs in B≥6, and compares them to
the analogous measures reported in Figure 5 and Table 4 of Roy et al. [55] about: i) bugs in the TCAS
benchmark, which consist of manually seeded bugs in several variants of an organic program; ii) bugs
seeded using the Apocalypse system (introduced in [55]) in the same programs of the TCAS benchmark.
As it can be seen, the average number of test cases is significantly higher for HyperPUT. Conse-
quently, the corresponding PUTs require considerably more queries before being correctly analyzed by
the testing framework.

In principle, our generator also allows to always inject bugs with exactly a single triggering input, but
for the purpose of achieving more variety in the process, several additional transformations with small
parameters have been included. In addition, some transformations (such as the ones of type FL) allow the
testing framework to easily generate new triggering inputs by randomly modifying the corresponding
command-line argument argv[i]. For this reason the values in parenthesis were introduced. They refer
to the number of triggering test cases for PUTs in Batch B≥6 with non-negligible parameter size (n > 3
for transformation P C and e >= 90 for transformation FL).

16

(a) PUTs in batch BIC.

(b) PUTs in batch BSC.

(c) PUTs in batch BFL.

(d) PUTs in batch BPC.

(e) PUTs in batch BCC.

(f) PUTs in batch B(cid:63).

Figure 5: Running time to discover the bug in each PUT in batches BIC, BSC, BFL, BPC, BCC,
B(cid:63). The horizontal axis enumerates the 10 PUTs in each batch in order of size (number
of transformations). The vertical axis measures the running time (in seconds) until
the tool terminates or times out (as in all other experiments, we report the average of
4 repeated runs). A colored filled disc indicates that the tool terminated successfully
(it discovered the bug); a grayed out circle indicates that the tool terminated or timed
out without discovering the bug. Data about AFL are in color blue, about CBMC are in
color black, about KLEE are in color yellow.

17

HyperPUT

[55, Fig. 5]
tcas Apocalypse

Mean
Median
Stddev
Min
Max

Bugs

7 888
3 575
12 240
29
53 389

20

23
17
22
8
152

41

345
165
569
7
4 366

82

Mean
Median
Stddev
Min
Max

Bugs

HyperPUT

7 655 (409)
2 518 (64)
10 782 (541)
1 (1)
40 708 (1 402)

363
213
431
24
1805

20

41

[55, Tab. 4]
tcas Apocalypse

13
1
51
1
341

82

(a) Statistics about the number f of all test inputs
generated by KLEE per bug before triggering
the bug in: HyperPUT’s batch B≥6, manu-
ally seeded bugs in TCAS, and synthetic bugs
seeded with Apocalypse; the latter two are
after [55, Fig. 5].

(b) Statistics about the number t of bug-trigger-
ing test inputs per bug generated by KLEE: Hy-
perPUT’s batch B≥6, manually seeded bugs in
TCAS, and synthetic bugs seeded with Apoca-
lypse; the latter two are after [55, Tab. 4].

Table 7: Number of KLEE-generated inputs as a measure of bug rarity.

5.4 RQ4: Capabilities
The previous research questions demonstrated that HyperPUT is capable of producing PUTs with bugs
with a broad range of characteristics, some comparable to those present in commonly used benchmarks
for bug-finding tools. In particular, Section 5.1 suggests that different PUTs are more or less challenging
for different bug-finding tools. In this section, we demonstrate how the variety of PUTs generated by
HyperPUT can be used to exercise different capabilities of bug-finding tools.

To this end, we generated new batches of PUTs BIC, BSC, BFL, BPC, BCC, B(cid:63). As described in Sec-
tion 4.7, PUTs in each batch BT only use the same transformation T , and differ only in their size—
measured as the number of repetitions of T . This way, we can understand how the characteristics of
each transformation challenge a tool’s bug-finding capabilities. Figure 5 plots the running time of the
considered testing frameworks when searching for bugs in these PUTs. Unsurprisingly, the perfor-
mance of a tool clearly depends on the transformations that make up a PUT. Let’s look into each tool’s
performance on the different batches.

CBMC is very effective on PUTs using transformations IC, SC, and FL, where it scales effortlessly. PUTs
using transformations IC and SC have no loops, and hence CBMC can easily build an exhaustive finite-
state abstraction. PUTs using transformations FL do have loops, but in this case CBMC manages to find a
suitable loop unrolling bound that makes the analysis exhaustive without blowing up the search space.
In contrast, CBMC’s performance suddenly blows up for the largest PUTs using 10 transformations PC and
CC; in these case, loops whose exit condition depends on an input string become hard to summarize with
a fixed, small unrolling bound past a certain size. Similarly, CBMC’s performance on batch B(cid:63) depends on
how many and which transformations are used; in particular, as soon as the randomly generated PUTs
include several nested loops with transformations PC or CC, CBMC runs out of resources and terminates
in about 40 minutes without detecting the bugs.

KLEE is as effective as CBMC on PUTs using transformation SC. It outperforms CBMC on PUTs using
transformations PC and CC, where it scales graciously to the largest PUTs thanks to its symbolic rea-
soning capabilities. On PUTs using transformation FL, KLEE is always effective, but its running times
fluctuate somewhat unpredictably—albeit remaining reasonably low in absolute value. This is probably
a result of running KLEE with randomized search (see Table 3), a feature that can speed up the search
for bugs but also introduces random fluctuations from run to run. In contrast, KLEE struggles to scale

18

on PUTs using transformation IC (both in batch BIC and in batch B(cid:63)). The problem here is not the
transformation per se, but rather how it is instantiated in the PUTs generated for the experiments. As
we explain in Section 4.2, parameter v1 in transformation IC is instantiated with atoll(argv[i]), which
interprets a string command-line argument as an integer; since KLEE does not have access to the source
code of library function atoll, it treats it as a black box, and hence its constraint solving capabilities are
of little use to find efficiently a suitable string argument that atoll converts to the integer v2 (the trans-
formation’s second parameter, instantiated with a random integer). This also explains the difference in
performance with transformation SC, where there is no black-box function involved, and hence KLEE
can easily find a suitable input string from the transformation’s condition itself.

AFL remains reasonably effective largely independent of which transformations are used; however, its
running time tends to grow with the size of the analyzed PUT. This behavior—complementary to KLEE’s
and CBMC’s—is a result of AFL being a gray box tool. In a nutshell, this means that AFL does not have
direct access to the source code of the analyzed functions; thus, it cannot extract path constraints from
it but has to “guess” them indirectly by trial and error. AFL’s gray-box strategy, combined with its many
heuristics and optimizations, achieves a different trade off than white-box tools like KLEE and CBMC:
AFL is an overall more flexible tool (in that it is less dependent on the characteristics of the analyzed
software), but usually requires more time and has more random fluctuations in its behavior. Another
difference is in scalability: AFL’s analysis time necessarily grows with the size of the inputs; in contrast,
symbolic techniques like KLEE are much more insensitive to input size, as long as the complexity of the
symbolic constraints does not vary.

Overall, these results demonstrate how HyperPUT can be used to generate PUTs with heterogeneous

characteristics and sizes, which challenge different capabilities of diverse bug-finding techniques.

5.5 Limitations and Threats to Validity
We discuss the main limitations of HyperPUT’s technique, its current implementation, and other threats
to the validity of the experiments described in this section, as well as how we mitigated them.

Construct validity depends on whether the measurements taken in the experiments reflect the
features that are being evaluated. In our experiments, we mainly collected standard measures, such as
running time, whether a bug-finding tool managed to trigger a bug, and static (cyclomatic complexity)
and dynamic (path length) measures of complexity. For the experiments to answer RQ3, we also counted
the number of triggering test cases and generated test cases for each bug—the same measures used by
Roy et al. [55] to assess bug rarity. Using standard measures reduces the risk of threats to construct
validity, and helps ensure that our results are meaningfully comparable with those in related work.

Our experiments to answer RQ4 were limited by the transformations currently supported by Hy-
perPUT, and by how we combined them. These restrictions are still consistent with RQ4’s aim, which
is to explore HyperPUT’s capabilities to exercise different testing techniques with PUTs of different
characteristics.

Internal validity depends on whether the experiments adequately control for possible confounding
factors. One obvious threat follows from possible bugs in our implementation of HyperPUT. As usual,
we mitigated this threat with standard software development practices, such as (manual) regression
testing, code reviews, and periodic revisions and refactoring.

To account for fluctuations due to the nondeterministic/randomized behavior of some testing tools,
we followed standard practices by repeating each experiment multiple times, and reporting the average
values (see Section 4.3). We usually observed only a limited variance in the experiments, which indicates
that the practical impact of randomness was usually limited.

19

Our experiments ran with a timeout of one hour per analyzed bug; it is possible that some experiments
would have resulted in success if they had been allowed a longer running time. We chose this timeout
as it is standard in such experiments [55], and compatible with running a good number of meaningful
experiments in a reasonable time. Our experiments showed a considerable variety of behavior, which
suggests that the testing tools we used can be successful within this timeout.

A related threat is in how we configured the testing tools (see Table 3). AFL, CBMC, and KLEE are highly-
configurable tools, and their performance can vary greatly depending on which options are selected. Our
goal was not an exhaustive exploration of all capabilities of these tools, but rather a demonstration of
their “average” behavior. Correspondingly, we mitigated this threat by: i) running each tool with two
configurations; ii) including the default configuration (with no overriding of default options); iii) using
common, widely used options.

To answer RQ3 in Section 4.6.2, we compared some measures taken on PUTs generated by HyperPUT
with the same measures reported by Roy et al. [55]. Since we did not repeat [55]’s experiments in the
same environment where we ran HyperPUT, we cannot make strong, quantitative claims about the
results of this comparison. This limitation does not, however, significantly threaten our overall answer
to RQ3, which is that HyperPUT can generate bugs whose rarity is realistic. [55]’s experiments are used
as a reference for what “realistic” means, whereas our work’s aims are largely complementary.

External validity depends on whether the experimental results generalize, and to what extent.

HyperPUT currently generates PUTs with a trivial modular structure, consisting of a single function
that only uses a handful of standard C libraries. On the other hand, each function can be structurally
quite intricate, with bugs nested deep in the function’s control-flow structure. This is partly a limita-
tion of the current implementation, but also an attempt to focus on generating PUTs that are comple-
mentary to organic bug-seeded programs. Detecting “deep” bugs is a relevant open challenge in test
automation [15], and synthetic buggy programs may be interesting subjects to demonstrate progress in
addressing the challenge.

HyperPUT generates programs in C since this is a widely popular target for the research on auto-
mated testing and fuzzing. The ideas behind HyperPUT can certainly be applied to other programming
languages, possibly with different results.

Similarly, the choice of transformations currently supported by HyperPUT obviously limits its broader
applicability. HyperPUT’s implementation is extensible with new transformations; deciding which ones
to add depends on the goal of the experiments one would like to make.

6 Conclusions

In this paper, we presented HyperPUT, a technique and tool to generate PUTs (Program Under Tests)
with seeded bugs automatically according to desired characteristics. The PUTs generated by Hyper-
PUT can be useful as experimental subjects to assess the capabilities of bug-finding tools, and how they
change according to the characteristics of the analyzed PUT. To demonstrate this, we generated hun-
dreds of PUTs using HyperPUT, and ran the popular bug-finding tools AFL, CBMC, and KLEE on them.
Our experiments suggest that HyperPUT can generate heterogeneous collections of PUTs, with several
characteristics that resemble those of “ecologically valid” bugs [55].

The implementation of HyperPUT is extensible, so that users can easily add transformations and
parameters to configure the generation of bugs according to the intended usage. As future work, we
plan to further extend the flexibility of HyperPUT, so that it can also generate programs consisting of
multiple functions and files, or it can extend an existing program with new functions and seeded bugs.

20

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships
that could have appeared to influence the work reported in this paper.

Acknowledgments

The authors gratefully acknowledge the financial support of the Swiss National Science Foundation for
the project (SNF Grant Number 200020-188613).

References

[1] Busybox. https://www.busybox.net/. Accessed: 2022-05-13.
[2] Circumventing fuzzing roadblocks with compiler transformations. https://clang.llvm.org/

docs/UndefinedBehaviorSanitizer.html. Accessed: 2022-06-01.

[3] Cmplog instrumentation.

https://github.com/AFLplusplus/AFLplusplus/blob/stable/

instrumentation/README.cmplog.md. Accessed: 2022-06-01.

[4] DARPA CGC 2018. https://github.com/CyberGrandChallenge/. Accessed: 2022-02-09.
[5] HyperPUT. https://github.com/user28134zx2734/HyperPUT, 2022.
[6] Paul Ammann and Jeff Offutt. Introduction to Software Testing. Cambridge University Press, 2nd

edition, 2007.

[7] Cornelius Aschermann, Sergej Schumilo, Tim Blazytko, Robert Gawlik, and Thorsten Holz.
REDQUEEN: Fuzzing with Input-to-State Correspondence. In 26th Annual Network and Distributed
System Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27, 2019. The In-
ternet Society, 2019.

[8] Domagoj Babic, Stefan Bucur, Yaohui Chen, Franjo Ivancic, Tim King, Markus Kusano, Caroline
Lemieux, László Szekeres, and Wei Wang. FUDGE: fuzz driver generation at scale.
In Marlon
Dumas, Dietmar Pfahl, Sven Apel, and Alessandra Russo, editors, Proceedings of the ACM Joint
Meeting on European Software Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, pages 975–985.
ACM, 2019.

[9] Roberto Baldoni, Emilio Coppa, Daniele Cono D’Elia, Camil Demetrescu, and Irene Finocchi. A

Survey of Symbolic Execution Techniques. ACM Comput. Surv., 51(3):50:1–50:39, 2018.

[10] Paul Bame. McCabe cyclomatic complexity for C and Cpp. https://manpages.ubuntu.com/

manpages/trusty/man1/pmccabe.1.html/. Accessed: 2022-06-01.

[11] Fabrice Bellard. QEMU, a Fast and Portable Dynamic Translator. In Proceedings of the FREENIX
Track: 2005 USENIX Annual Technical Conference, April 10-15, 2005, Anaheim, CA, USA, pages 41–46.
USENIX, 2005.

[12] Dirk Beyer. Software Verification: 10th Comparative Evaluation (SV-COMP 2021).

In Jan Friso
Groote and Kim Guldstrand Larsen, editors, Tools and Algorithms for the Construction and Analysis
of Systems - 27th International Conference, TACAS 2021, Held as Part of the European Joint Confer-
ences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27 -
April 1, 2021, Proceedings, Part II, volume 12652 of Lecture Notes in Computer Science, pages 401–422.
Springer, 2021.

21

[13] Dirk Beyer. Status Report on Software Testing: Test-Comp 2021. In Esther Guerra and Mariëlle
Stoelinga, editors, Fundamental Approaches to Software Engineering - 24th International Conference,
FASE 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS
2021, Luxembourg City, Luxembourg, March 27 - April 1, 2021, Proceedings, volume 12649 of Lecture
Notes in Computer Science, pages 341–357. Springer, 2021.

[14] Stephen M. Blackburn, Robin Garner, Chris Hoffmann, Asjad M. Khan, Kathryn S. McKinley,
Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton, Samuel Z. Guyer, Martin Hirzel,
Antony L. Hosking, Maria Jump, Han Bok Lee, J. Eliot B. Moss, Aashish Phansalkar, Darko Ste-
fanovic, Thomas VanDrunen, Daniel von Dincklage, and Ben Wiedermann. The DaCapo bench-
marks: java benchmarking development and analysis. In Peri L. Tarr and William R. Cook, ed-
itors, Proceedings of the 21th Annual ACM SIGPLAN Conference on Object-Oriented Programming,
Systems, Languages, and Applications, OOPSLA 2006, October 22-26, 2006, Portland, Oregon, USA,
pages 169–190. ACM, 2006.

[15] Marcel Böhme, Cristian Cadar, and Abhik Roychoudhury. Fuzzing: Challenges and Reflections.

IEEE Softw., 38(3):79–86, 2021.

[16] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. Coverage-based Greybox Fuzzing
as Markov Chain.
In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C.
Myers, and Shai Halevi, editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, Vienna, Austria, October 24-28, 2016, pages 1032–1043. ACM, 2016.
[17] James Bornholt and Emina Torlak. Finding code that explodes under symbolic evaluation. Proc.

ACM Program. Lang., 2(OOPSLA):149:1–149:26, 2018.

[18] David Bowes, Tracy Hall, Mark Harman, Yue Jia, Federica Sarro, and Fan Wu. Mutation-aware fault
prediction. In Proceedings of the 25th International Symposium on Software Testing and Analysis,
ISSTA 2016, Saarbrücken, Germany, July 18-20, 2016, pages 330–341. ACM, 2016.

[19] Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. KLEE: Unassisted and Automatic Gener-
In Richard Draves and Robbert
ation of High-Coverage Tests for Complex Systems Programs.
van Renesse, editors, 8th USENIX Symposium on Operating Systems Design and Implementation,
OSDI 2008, December 8-10, 2008, San Diego, California, USA, Proceedings, pages 209–224. USENIX
Association, 2008.

[20] Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R. Engler. EXE:
Automatically Generating Inputs of Death. ACM Trans. Inf. Syst. Secur., 12(2):10:1–10:38, 2008.
[21] Cristian Cadar and Koushik Sen. Symbolic execution for software testing: three decades later.

Commun. ACM, 56(2):82–90, 2013.

[22] George Candea and Patrice Godefroid. Automated Software Test Generation: Some Challenges,
Solutions, and Recent Advances. In Bernhard Steffen and Gerhard J. Woeginger, editors, Computing
and Software Science - State of the Art and Perspectives, volume 10000 of Lecture Notes in Computer
Science, pages 505–531. Springer, 2019.

[23] Vitaly Chipounov, Volodymyr Kuznetsov, and George Candea. The S2E Platform: Design, Imple-

mentation, and Applications. ACM Trans. Comput. Syst., 30(1):2:1–2:49, 2012.

[24] Edmund M. Clarke, E. Allen Emerson, and A. Prasad Sistla. Automatic Verification of Finite-
State Concurrent Systems Using Temporal Logic Specifications. ACM Trans. Program. Lang. Syst.,
8(2):244–263, 1986.

22

[25] Edmund M. Clarke, Daniel Kroening, and Flavio Lerda. A Tool for Checking ANSI-C Programs. In
Kurt Jensen and Andreas Podelski, editors, Tools and Algorithms for the Construction and Analysis
of Systems, 10th International Conference, TACAS 2004, Held as Part of the Joint European Confer-
ences on Theory and Practice of Software, ETAPS 2004, Barcelona, Spain, March 29 - April 2, 2004,
Proceedings, volume 2988 of Lecture Notes in Computer Science, pages 168–176. Springer, 2004.
[26] Leonardo Mendonça de Moura and Nikolaj Bjørner. Z3: An Efficient SMT Solver. In C. R. Ramakr-
ishnan and Jakob Rehof, editors, Tools and Algorithms for the Construction and Analysis of Systems,
14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on The-
ory and Practice of Software, ETAPS 2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings,
volume 4963 of Lecture Notes in Computer Science, pages 337–340. Springer, 2008.

[27] Hyunsook Do, Sebastian G. Elbaum, and Gregg Rothermel. Supporting Controlled Experimen-
tation with Testing Techniques: An Infrastructure and its Potential Impact. Empir. Softw. Eng.,
10(4):405–435, 2005.

[28] Brendan Dolan-Gavitt, Josh Hodosh, Patrick Hulin, Tim Leek, and Ryan Whelan. Repeatable
In Jeffrey Todd McDonald, Mila Dalla Preda, and Natalia
Reverse Engineering with PANDA.
Stakhanova, editors, Proceedings of the 5th Program Protection and Reverse Engineering Workshop,
PPREW@ACSAC, Los Angeles, CA, USA, December 8, 2015, pages 4:1–4:11. ACM, 2015.

[29] Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti, William K.
Robertson, Frederick Ulrich, and Ryan Whelan. LAVA: Large-Scale Automated Vulnerability Ad-
dition. In IEEE Symposium on Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016,
pages 110–121. IEEE Computer Society, 2016.

[30] Karine Even-Mendoza, Cristian Cadar, and Alastair F. Donaldson. Closer to the Edge: Testing
In 35th
Compilers More Thoroughly by Being Less Conservative About Undefined Behaviour.
IEEE/ACM International Conference on Automated Software Engineering, ASE 2020, Melbourne, Aus-
tralia, September 21-25, 2020, pages 1219–1223. IEEE, 2020.

[31] Javier Ferrer, Francisco Chicano, and Enrique Alba. Benchmark Generator for Software Testers.
In Lazaros S. Iliadis, Ilias Maglogiannis, and Harris Papadopoulos, editors, Artificial Intelligence
Applications and Innovations - 12th INNS EANN-SIG International Conference, EANN 2011 and 7th
IFIP WG 12.5 International Conference, AIAI 2011, Corfu, Greece, September 15-18, 2011, Proceedings,
Part II, volume 364 of IFIP Advances in Information and Communication Technology, pages 378–388.
Springer, 2011.

[32] Andrea Fioraldi, Dominik Maier, Heiko Eißfeldt, and Marc Heuse. AFL++ : Combining Incremental
Steps of Fuzzing Research. In Yuval Yarom and Sarah Zennou, editors, 14th USENIX Workshop on
Offensive Technologies, WOOT 2020, August 11, 2020. USENIX Association, 2020.

[33] Gordon Fraser and Andreas Zeller. Mutation-Driven Generation of Unit Tests and Oracles. IEEE

Trans. Software Eng., 38(2):278–292, 2012.

[34] Patrice Godefroid, Nils Klarlund, and Koushik Sen. DART: directed automated random testing.
In Vivek Sarkar and Mary W. Hall, editors, Proceedings of the ACM SIGPLAN 2005 Conference on
Programming Language Design and Implementation, Chicago, IL, USA, June 12-15, 2005, pages 213–
223. ACM, 2005.

[35] Patrice Godefroid, Michael Y. Levin, and David A. Molnar. SAGE: whitebox fuzzing for security

testing. Commun. ACM, 55(3):40–44, 2012.

[36] Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. Magma: A Ground-Truth Fuzzing Bench-
mark. In Longbo Huang, Anshul Gandhi, Negar Kiyavash, and Jia Wang, editors, SIGMETRICS ’21:

23

ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems,
Virtual Event, China, June 14-18, 2021, pages 81–82. ACM, 2021.

[37] Rene Just, Darioush Jalali, and Michael Ernst. Defects4J: a database of existing faults to enable

controlled testing studies for Java programs. July 2014.

[38] Timotej Kapus and Cristian Cadar. Automatic testing of symbolic execution engines via program
generation and differential testing. In Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen,
editors, Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engi-
neering, ASE 2017, Urbana, IL, USA, October 30 - November 03, 2017, pages 590–600. IEEE Computer
Society, 2017.

[39] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. Evaluating Fuzz Testing.
In David Lie, Mohammad Mannan, Michael Backes, and XiaoFeng Wang, editors, Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communications Security, CCS 2018, Toronto,
ON, Canada, October 15-19, 2018, pages 2123–2138. ACM, 2018.

[40] Karam Al Kontar, Freddy Naji, Salma Demiane, and Ramzi Haraty. A Survey on Mutation Testing
Approaches. In 2019 IEEE CHILEAN Conference on Electrical, Electronics Engineering, Information
and Communication Technologies (CHILECON), pages 1–7, 2019.

[41] Markus Kusano and Chao Wang. CCmutator: A mutation generator for concurrency constructs
in multithreaded C/C++ applications. In Ewen Denney, Tevfik Bultan, and Andreas Zeller, editors,
2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013, Silicon
Valley, CA, USA, November 11-15, 2013, pages 722–725. IEEE, 2013.

[42] Chris Lattner and Vikram S. Adve. LLVM: A Compilation Framework for Lifelong Program Anal-
ysis & Transformation. In 2nd IEEE / ACM International Symposium on Code Generation and Opti-
mization (CGO 2004), 20-24 March 2004, San Jose, CA, USA, pages 75–88. IEEE Computer Society,
2004.

[43] Tim Littlefair. C and Cpp Code Counter. http://cccc.sourceforge.net/, 2005. Accessed: 2022-

02-09.

[44] Chenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, Yu Song, and Raheem Beyah.
MOPT: Optimized Mutation Scheduling for Fuzzers. In Nadia Heninger and Patrick Traynor, edi-
tors, 28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16,
2019, pages 1949–1966. USENIX Association, 2019.

[45] Valentin J. M. Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J.
Schwartz, and Maverick Woo. The Art, Science, and Engineering of Fuzzing: A Survey. IEEE Trans.
Software Eng., 47(11):2312–2331, 2021.

[46] Michaël Marcozzi, Qiyi Tang, Alastair F. Donaldson, and Cristian Cadar. Compiler fuzzing: how

much does it matter? Proc. ACM Program. Lang., 3(OOPSLA):155:1–155:29, 2019.

[47] Thomas J. McCabe. A Complexity Measure. IEEE Trans. Software Eng., 2(4):308–320, 1976.
[48] William M. McKeeman. Differential Testing for Software. Digit. Tech. J., 10(1):100–107, 1998.
[49] Jonathan Metzman, László Szekeres, Laurent Simon, Read Sprabery, and Abhishek Arya.
FuzzBench: an open fuzzer benchmarking platform and service. In Diomidis Spinellis, Georgios
Gousios, Marsha Chechik, and Massimiliano Di Penta, editors, ESEC/FSE ’21: 29th ACM Joint Euro-
pean Software Engineering Conference and Symposium on the Foundations of Software Engineering,
Athens, Greece, August 23-28, 2021, pages 1393–1403. ACM, 2021.

24

[50] Nicholas Nethercote. Dynamic binary analysis and instrumentation: or building tools is easy. PhD

thesis, University of Cambridge, UK, 2004.

[51] Mike Papadakis, Yue Jia, Mark Harman, and Yves Le Traon. Trivial Compiler Equivalence: A Large
Scale Empirical Study of a Simple, Fast and Effective Equivalent Mutant Detection Technique. In
37th IEEE/ACM International Conference on Software Engineering, ICSE 2015, Florence, Italy, May
16-24, 2015, Volume 1, pages 936–946. IEEE Computer Society, 2015.

[52] Mike Papadakis and Yves Le Traon. Metallaxis-FL: mutation-based fault localization. Software

Testing, Verification and Reliability, 25(5-7):605–628, 2015.

[53] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. T-Fuzz: Fuzzing by Program Transformation. In
2018 IEEE Symposium on Security and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco,
California, USA, pages 697–710. IEEE Computer Society, 2018.

[54] Mauro Pezzè and Michal Young. Software testing and analysis. Process, principles and techniques.

Wiley, 2007.

[55] Subhajit Roy, Awanish Pandey, Brendan Dolan-Gavitt, and Yu Hu. Bug synthesis: challenging bug-
finding tools with deep faults. In Gary T. Leavens, Alessandro Garcia, and Corina S. Pasareanu,
editors, Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena
Vista, FL, USA, November 04-09, 2018, pages 224–234. ACM, 2018.

[56] David Schuler and Andreas Zeller. Covering and Uncovering Equivalent Mutants. Softw. Test.

Verification Reliab., 23(5):353–374, 2013.

[57] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario Polino, Andrew
Dutcher, John Grosen, Siji Feng, Christophe Hauser, Christopher Krügel, and Giovanni Vigna.
SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis. In IEEE Symposium on
Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016, pages 138–157. IEEE Computer
Society, 2016.

[58] Richard Stallman and the GCC Developer Community. Using the GNU Compiler Collection (GCC).

GCC version 10.2.0. https://gcc.gnu.org/onlinedocs/gcc/. Accessed: 2022-02-09.

[59] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang, Jacopo Corbetta,
Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna. Driller: Augmenting Fuzzing
Through Selective Symbolic Execution. In 23rd Annual Network and Distributed System Security
Symposium, NDSS 2016, San Diego, California, USA, February 21-24, 2016. The Internet Society, 2016.
[60] Andrew S. Tanenbaum, Raja Appuswamy, Herbert Bos, Lorenzo Cavallaro, Cristiano Giuffrida,
Tomás Hrubý, Jorrit N. Herder, Erik van der Kouwe, and David C. van Moolenbroek. MINIX 3:
Status Report and Current Research. Login Usenix Mag., 35(3), 2010.

[61] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. Finding and understanding bugs in C compil-
ers. In Mary W. Hall and David A. Padua, editors, Proceedings of the 32nd ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2011, San Jose, CA, USA, June 4-8, 2011,
pages 283–294. ACM, 2011.

[62] Xiangjuan Yao, Mark Harman, and Yue Jia. A study of equivalent and stubborn mutation operators
In 36th International Conference on Software Engineering,

using human analysis of equivalence.
ICSE ’14, Hyderabad, India - May 31 - June 07, 2014, pages 919–930. ACM, 2014.

[63] Michal Zalewski. American Fuzzing Lop (AFL). http://lcamtuf.coredump.cx/afl/, 2016. Ac-

cessed: 2022-02-09.

25

