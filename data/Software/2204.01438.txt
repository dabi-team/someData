2
2
0
2

r
p
A
6

]
E
S
.
s
c
[

2
v
8
3
4
1
0
.
4
0
2
2
:
v
i
X
r
a

How Can We Develop Explainable Systems?
Insights from a Literature Review and an Interview Study

Larissa Chazette
larissa.chazette@inf.uni-hannover.de
Leibniz University Hannover
Software Engineering Group
Hannover, Germany

Merve Balci
aylakci@stud.uni-hannover.de
Leibniz University Hannover
Software Engineering Group
Hannover, Germany

Jil Klünder
jil.kluender@inf.uni-hannover.de
Leibniz University Hannover
Software Engineering Group
Hannover, Germany

Kurt Schneider
kurt.schneider@inf.uni-hannover.de
Leibniz University Hannover
Software Engineering Group
Hannover, Germany

ABSTRACT
Quality aspects such as ethics, fairness, and transparency have been
proven to be essential for trustworthy software systems. Explain-
ability has been identified not only as a means to achieve all these
three aspects in systems, but also as a way to foster users’ senti-
ments of trust. Despite this, research has only marginally focused
on the activities and practices to develop explainable systems. To
close this gap, we recommend six core activities and associated
practices for the development of explainable systems based on the
results of a literature review and an interview study. First, we iden-
tified and summarized activities and corresponding practices in the
literature. To complement these findings, we conducted interviews
with 19 industry professionals who provided recommendations for
the development process of explainable systems and reviewed the
activities and practices based on their expertise and knowledge.
We compared and combined the findings of the interviews and the
literature review to recommend the activities and assess their ap-
plicability in industry. Our findings demonstrate that the activities
and practices are not only feasible, but can also be integrated in
different development processes.

KEYWORDS
explainability, explainable systems, software process, interview
study

ACM Reference Format:
Larissa Chazette, Jil Klünder, Merve Balci, and Kurt Schneider. 2022. How
Can We Develop Explainable Systems? Insights from a Literature Review
and an Interview Study. In Proceedings of the International Conference on
Software and System Processes and International Conference on Global Soft-
ware Engineering (ICSSP’22), May 20–22, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3529320.3529321

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s).
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in Proceedings
of the International Conference on Software and System Processes and International
Conference on Global Software Engineering (ICSSP’22), May 20–22, 2022, Pittsburgh, PA,
USA, https://doi.org/10.1145/3529320.3529321.

1 INTRODUCTION
Decisions made or supported by software have an increasing in-
fluence on our daily lives. It all starts with deciding which route
to take to work. Is there an accident somewhere on the route? Or
more traffic than on a typical morning? Navigation software appli-
cations (apps) retrieve such kind of information to recommend the
best route under the present circumstances. However, the rationale
behind the software’s suggestions is not always clear, potentially
leading to confusion and misunderstandings. Besides that, at the
same time that we are getting more dependent on those systems to
make decisions, these systems are also getting more complex. As a
result, there is an increasing demand for system transparency and
fairness [5], motivating the development of explainable systems.

We refer to an explainable system as a system presenting an
explanation about a given system aspect to an addressee (e.g, the
end user), such that the addressee can understand this aspect of
the system [11]. System explainability can positively influence user
experience, and can foster more trust in systems [9, 12, 18]. For
example, proposing a different route accompanied by the informa-
tion that there is an accident (or that, simply spoken, the new route
requires less time) increases the chance that the driver follows the
suggestion and also increases transparency in the communication
between the system and the user.

Recent research has strongly focused on the relevance of expla-
nations and how they should be provided in a software application
(cf. [13, 34, 62]). For example, the correct amount of explanations
is required to keep users as informed as necessary, but too many
explanations can overwhelm them [62]. Other authors explore the
difference between stakeholders’ needs with respect to explanations
and how those should be presented [13, 34, 37].

However, so far, it remains unclear how explainable systems can
be developed [6]. That is, how does a software process look like that
specifically supports the integration of explanations? Which phases
of the software lifecycle are affected by the need for explanations?
Are existing processes, activities, methods, and practices sufficient
for the development of these systems?

In this paper, we want to provide deeper insights in how to de-
velop such systems. We combine results from a literature review, in
which we analyzed 79 papers with respect to the proposed methods

 
 
 
 
 
 
ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

and practices for developing explainable systems, with the feedback
provided by 19 practitioners in an interview study. We synthesize
all insights into six core activities that are crucial for the creation
of explainable systems: (1) vision definition, (2) stakeholder analy-
sis, (3) back-end analysis, (4) trade-off analysis, (5) explainability
design, and (6) evaluation. These activities cover three phases of
the software lifecycle: requirements engineering, design, and evalu-
ation. All participants recommended a user-centered development
approach for the development of explainable systems, confirming
our previous assumption [12]. In the context of this paper, we refer
to a phase as a step in the software lifecycle; to activity a collec-
tion of practices to achieve a goal; to practice as a specific way of
performing a task during an activity.

Outline. The rest of the paper is structured as follows: In Section 2,
we present background information and related research. Section 3
presents our research design. Section 4 summarizes our results
which we discuss in Section 5. We conclude our paper in Section 6.

2 BACKGROUND AND RELATED WORK
The development of explainable systems has been subject to recent
research. Eiband et al. [18] present an end-user-centered partici-
patory process that incorporates perspectives of users, designers,
and providers to develop transparent and comprehensible user in-
terfaces, both of which are quality aspects related to explainability.
The authors distinguish between three types of mental models that
summarize these different perspectives and work as stages in the
process of defining what needs to be explained in a system: the
expert mental model, the user mental model, and the target mental
model. The expert mental model summarizes the key components
of the algorithm in an “optimal” version of a user mental model.
The user mental model describes how the user perceives the sys-
tem. Any differences and matches with the expert mental model
are recorded to form the target mental model, which combines the
essential components regarded as relevant and useful in the mental
models of users and experts.

Tsai and Brusilovsky [59] use the framework proposed by Eiband
et al. [18] to develop explainable interfaces for a social recommender
system. In a first step, the authors create an expert mental model
to collect the functionality of five system models. In a second step,
they develop a user mental model to ponder on design decisions for
an explainable interface. They performed a survey with 14 partici-
pants to create this user mental model. In a third step, the authors
develop a target mental model using an experimental setting with
15 participants. The target mental model summarizes information
on the most important components that require explanations. In
a fourth and last step, they develop 25 different explainable user
interfaces for five recommendation models which they evaluate to
select the best options.

Mohseni et al. [41] describe a framework for the design and
evaluation of explainable systems concentrating on the aspects that
need to be explained, and how and when the explanation should be
presented. Their framework is based on a literature review in the
area of explainable artificial intelligence. In addition, the authors
propose an iterative process to develop such systems. Wolf [64]
proposes to develop explainable systems based on scenarios, as they
allow to elicit and analyze requirements. The author introduces

so-called explainability scenarios to help developers focus on what
end users need to comprehend in a given scenario and what type
of explanation they need.

Chazette and Schneider [12] discuss factors that affect the inclu-
sion of explainability as a necessary quality aspect within a system
and that also affect the design choices toward its operationalization.
The authors encourage the use of user-centered practices when
developing explainable systems [12]. They argue that explainabil-
ity and usability are intrinsically related and, therefore, methods
from usability engineering would be suitable in the development
of explainable systems. Schoonderwoerd et al. [52] present a user-
centered design approach with reusable patterns for explanations.
These patterns cover domain analysis, requirements elicitation, the
design of the system, and the evaluation of interactions. Weigand et
al. [61] specifically propose the user-centered development process
defined in the ISO 9241-210 [27] to develop explainable systems,
which takes into account user-centered principles and an iterative
approach that incorporates feedback loops.

In this paper, we review all above-mentioned papers (and sev-
eral more) to provide a comprehensive and synthesized process to
develop explainable systems.

3 RESEARCH DESIGN
Our research design follows a mixed approach, with a literature
review and an interview study. In the remainder of this section, we
discuss the research questions and the data analysis procedures.

3.1 Research Goal and Research Questions
The overall goal of the research presented in this paper is to pro-
vide recommendations on activities and practices to support the de-
velopment of explainable systems. To reach this goal, we pose the
following research questions:
RQ1: Which activities and practices can be used when developing
explainable systems? This question aims to offer an overview of
activities and practices that have been proved to be effective in the
development of explainable systems.
RQ2: How can an explainable system be developed? Based on the find-
ings of RQ1, our objective is to synthesize the identified practices
into activities that should be incorporated into the development
process of explainable systems. The activities are evaluated based
on practitioners’ experiences in an interview study.

3.2 Data Collection and Analysis
The data collection consists of two parts: (1) a literature review
and (2) an interview study. While the literature review strives for
an overview of already existing approaches to develop explainable
systems (RQ1), the interview study was intended to collect feedback
on these existing ideas, which were synthesized in six activities
(RQ2).

3.2.1 Literature Review. The literature review aims to provide a
broad overview of the employed methods and practices when devel-
oping explainable systems. We opted to perform a rapid literature
review rather than a systematic literature review based on our goal
and time restrictions. Rapid reviews are a form of knowledge synthe-
sis in which components of the systematic review process are simpli-
fied or omitted to produce information in a timely manner [20, 56].

How Can We Develop Explainable Systems?

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

We performed this review based in parts on the guidelines provided
by Kitchenham et al. [29], but deviated to some extent given our
constraints. We are aware that this reduces the generalizability of
our results compared to more thorough reviews, but we deemed
this form of literature study adequate for our purposes. The threats
to validity introduced by this decision are presented in Section 3.3.
Nevertheless, we performed all steps required to get an overview
of existing literature, namely (1) definition of the search string, (2)
definition of inclusion and exclusion criteria, (3) selection of the
database(s), (4) definition of the termination criterion, (5) execution,
and (6) data analysis.

(1) Definition of the search string. The search string is based on a
combination of the keywords we wanted to cover with this study,
namely explainability, software, and system. Note that, as we did
not expect papers to explicitly propose a process to develop such
systems, we did not include words like practice, or process in the
search string. As this decision enlarged the solution space, this
is not a threat to validity. We specifically focused on the fields of
requirements engineering and human-computer interaction since
the development of explainable systems is highly dependent on
understanding the needs of the end user. The final search string
results from this line of thought and is extended by synonyms and
abbreviations (e.g., SE for software engineering). This leads to the
following search string:

(explain* OR XAI)
AND
(system OR software OR design OR interface
OR HCI OR ”human-computer interaction”
OR RE OR ”requirements engineering”
OR SE OR ”software engineering”)

(2) Definition of inclusion and exclusion criteria. We eliminated
studies and publications that are not relevant for answering our
research questions. To increase the objectivity of the decision on
the inclusion or exclusion of a paper, we formulated the inclusion
and exclusion criteria presented in Table 1. In particular, as explain-
ability is an interdisciplinary research topic, we needed to explicitly
remove publications that are not related to computer science (EC5).

Table 1: Inclusion (IC) and Exclusion (EC) Criteria

ID
𝐼𝐶1

𝐼𝐶2

𝐸𝐶1
𝐸𝐶2

𝐸𝐶3
𝐸𝐶4
𝐸𝐶5

Description
The paper presents activities or practices to develop
explainable systems.
The paper proposes a methodology or a process to
develop explainable systems.
The paper does not mention activities or practices.
The paper is not a peer-reviewed contribution to a
conference or a journal.
The paper is not accessible (via university licenses).
The paper is neither written in German nor in English.
The paper is not related to computer science.

(3) Selection of database(s). In rapid reviews, sources are limited
due to time constraints of searching. Since the process of select-
ing studies for a literature review can be very laborious and time-
consuming, we opted to perform our search using Google Scholar
(GS). GS retrieves results from all major databases, such as ACM
Digital Libraries, IEEExplore, and Web of Science. However, the use
of just one database introduces a threat to validity that we discuss
in Section 3.3.

(4) Definition of the termination criterion. According to Wolf-
swinkel et al. [65], a literature review is never complete but at most
saturated. This saturation is achieved when no new concepts or cat-
egories arise from the inspected data. We followed this approach to
decide when to conclude our search process. It is important to note
that we do not assume saturation in the traditional sense, based
on all available data; rather, we assume saturation with respect to
the subset of publications inspected in our literature search. More
specifically, we decided to end our search as soon as we found 50
papers one after another without gaining new insights, i.e., without
at least one paper extending our solution space of methods and prac-
tices. Therefore, we are confident that the number of sources we
found provide sufficient insights to adequately answer our research
questions.

(5) Execution. We inserted the search string in GS, leading to 446
publications that were selected based on their titles. The application
of the execution criteria to these papers led to a removal of 367
publications: We removed 300 publications not related to the focus
of our study (EC1), 12 publications that were not peer-reviewed
(EC2), 20 that were not accessible (EC3), 4 that were neither written
in German nor in English (EC4), and 31 publications due to the miss-
ing relation to computer science (EC5). All remaining publications
were peer-reviewed and meet one of the two inclusion criteria, so
that we considered 79 papers as relevant for our study.

(6) Data Analysis. From the 79 papers found to be relevant, we
extracted information that was summarized in a concept matrix. We
extracted information on the practices, as well as on the purpose
of using them during the development process. We grouped this
information into categories, in a process that consisted of three
steps: 1) We clustered the extracted information to avoid duplicates
and to get a unique list of elements, being careful to preserve the
traceability between practice and phase or its purpose during the
development process. 2) Based on the number of papers that men-
tioned these practices (at least two), we considered them to be more
or less relevant for our analysis. 3) We discussed the set of practices
until we reached an agreement about the final set.

To create the activities, we analyzed the associated information
about the purpose of these practices and how they are used in the
development process. We grouped this information into categories,
following the same three-steps process as before. Then, we clas-
sified the practices into the corresponding activities based on the
information in the literature. Finally, we assigned each activity to
the appropriate phase of the software lifecycle. This resulted in
the first version of the six activities (and corresponding practices),
based only on information from the literature. For this first version,
the activities were created using the most relevant practices from
this concept matrix, which can be found in 39 publications from
our review (Tab. 3). The activities and practices were given as a
reference to the participants during the interview study.

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

Interview Study. To get feedback on the applicability of the
3.2.2
process resulting from the literature review, we conducted an inter-
view study with practitioners. We elaborated an interview protocol
with questions and tasks for the participants. The interviews were
semi-structured and helped us to learn from the practitioners’ ex-
periences. Our goal was to combine this feedback with the first
version of the six activities, synthesized from our literature review.
Interview structure. The interviews were exploratory and con-
sisted of two predefined tasks and a set of predefined questions that
could be adapted during the interview. The overall structure of the
interviews is presented below:

(1) Welcome
(2) Presentation of the topic and the structure of the interview
(3) Task 1: Draw a diagram of the company’s existing software

development process

(4) Definition of explainability and presentation of a scenario
(5) Example: Scenario on the planned development of a system
that should be explainable. The participant should develop a
process allowing to address explainability in the process
(6) Task 2: Draw a diagram of a development process for ex-

plainable software systems

(7) Follow-up question: Applicability in industry
(8) Closure
After asking the practitioners about the current software devel-
opment process in their companies, an introduction to the topic of
explainability was given, since we cannot assume that everyone
has the same understanding of what explainability means. Then,
each interviewee was asked to describe and draw the current devel-
opment process in his/her company, including activities, methods,
and practices. Using think-aloud, the interviewees were asked to
explain their thoughts. Afterwards, we introduced explainability
by using the definition provided by Chazette et al. [11] (presented
in the introduction). We highlighted the relevance of explainabil-
ity by presenting a scenario: The participants were asked to put
themselves in the role of a process engineer in a fictitious company
that wants to develop an autonomous car. As process engineer,
they should develop a process (including activities, methods, and
practices) that explicitly addresses explainability for a system in
a self-driving car. We asked the participants to draw the process
as well, and describe their rationale. To facilitate this step, we pro-
vided a list of activities and practices from software engineering
and human-computer interaction, including the ones found in our
literature review. Afterwards, we asked about their opinion on the
applicability of the recommended process in the industry.

Participant selection. In this study, we wanted to interview per-
sons who have experience in software development. That is, we
considered software developers and software engineers (among
others) suitable interview participants. In addition, we invited prod-
uct owners and requirements engineers to participate, as the lit-
erature on the development of explainable systems put a strong
focus on requirements engineering. We invited 87 practitioners via
LinkedIn (43 invitations), personal contacts (35 invitations) and via
one contact person in a company (11 invitations). In total, 19 experts
accepted our invitation, resulting in a response rate of 22%. The
main reasons for not participating were time constraints, holidays,
or workplace policy that prohibited participation.

Setting. All interviews were performed online via BigBlueBut-
ton (hosted by the university). In case of technical problems, we
switched to Jitsi Meets (also hosted by the university). All partici-
pants agreed upon recording the interview. Based on the recorded
data, we transcribed all interviews. In addition, we used the collab-
orative tool Miro as a virtual board for the tasks.

Pilot interviews. We performed three pilot interviews with PhD
students, which resulted in changes to the interview and tasks’
structure.

Data Collection. We conducted 19 interviews with participants
from seven companies. The interviews had an average duration of
60 minutes and were mostly conducted in German. Two interviews
were conducted in English.

A complete overview of the participants is presented in Table 2.
The participants had an average age of 32.2 years (min: 23 years,
max: 41 years, SD: 4.6 years) and an average of 6.6 years of expe-
rience (min: 2 years, max: 17 years, SD: 4.4 years). Almost half of
them work in small companies with less then 50 employees, five
work in medium-sized companies with less then 250 employees,
and five work in large companies with more than 250 employees.

Table 2: Overview of the participants’ demographics

Requirements engineer
Product owner
Requirements engineer
Developer
Developer
Requirements engineer
Developer
Developer
Developer
Requirements engineer

ID Role
1
2
3
4
5
6
7
8
9
10
11 Developer
12 Developer
13 Developer
14
15
16
17
18
19

Product owner
Product owner
Product owner
Requirements engineer
Requirements engineer
Product owner

Years of
experience
17
5
2
3
7
5
2
9
4
8
10
6
3
5
16
2
8
2
12

Company
size
small
small
small
small
small
small
small
small
small
large
medium
medium
large
large
large
medium
large
medium
medium

Age
37
32
27
23
32
31
27
33
25
35
34
28
35
29
41
34
35
35
39

Data Analysis. The 19 interview transcripts resulted in almost
95K lines of text. We deductively categorized the statements based
on the guidelines presented by Mayring [38] to systematically re-
trieve insights from qualitative data. That is, given the overall goal
of our study, we considered activities, methods, and practices from
human-computer interaction or software engineering as categories
and classified the interview data in these categories. In addition, we
analyzed the interviewees’ sketches of process recommendations
and compared them to our literature findings. We discussed the
differences and merged the findings throughout the course of sev-
eral sessions until we arrived at a final version of six core activities

How Can We Develop Explainable Systems?

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

assigned to their corresponding phases in the software lifecycle
(Fig. 1).

lifecycle. Studies that explicitly provide insights into the technical
part of the development of explainable systems are required.

3.3 Validity Procedures and Threats to Validity
Both studies are subject to some threats to validity. We tried to mit-
igate some threats by implementing validity procedures. However,
some factors still threaten the validity of our results. We describe
the validity procedures and discuss the threats to validity according
to the classification by Wohlin et al. [63].

Conclusion Validity. Since the results of our study are based on
insights from 79 publications and a total of 19 interviews, they must
not be over-interpreted. However, we are confident that the data
suffices for the analysis presented in this paper, as we strive for an
overview and not for the ultimate truth. Future studies are required
to extend our conclusions, and to derive more thorough and fine-
grained results that strengthen the reliability of our findings.

Internal Validity. The internal validity of the results of the inter-
view study may be threatened by the fact that a researcher was
present during the data collection, adjusting the questions on-the-
fly, when necessary. This might have introduced a researcher bias,
because questions can be asked in a way that influences the answer.
To mitigate this threat, each research step, including the interview
protocol, was reviewed by the other authors of this paper. In the
literature review, in case of doubts, we included a paper rather than
excluded it to avoid loss of data.

The decision to use only Google Scholar to perform our litera-
ture search also poses a threat, even though GS covers all major
databases. However, Yasin et al. [69] investigated the suitability of
GS for literature reviews and found that GS was able to retrieve
96% of primary studies. Therefore, we consider the choice of GS
to be appropriate given our time constraints and since our goal
was to conduct a rapid literature review rather than a systematic
literature review. However, we acknowledge that this is a limitation
of our study and do not dismiss the need for a systematic review to
provide an even more complete set of relevant publications.

Construct Validity. To mitigate the threat of construct validity, we
implemented a thorough review process. While one of the authors
planned and conducted the two studies, each step was carefully
reviewed and discussed by other authors of this paper. In addition,
for the interview study, we conducted pilot interviews that led to
slight adjustments in the questions and in the interview structure.
During data analysis, we carefully extracted and discussed relevant
information from both publications and interview transcripts.

The choice of interview participants is also a threat to construct
validity. As there are still few development teams that have expe-
rience in the development of explainable systems, the insights we
gained in the interviews are based on a hypothetical scenario. There-
fore, it is possible that the hypothetical ideas that emerged from the
interviews do not work in industry. Future studies are required to
retrieve quantitative data from a real-world setting. Furthermore,
all interviewees work in an agile work environment, biasing their
view on plan-driven approaches to develop explainable systems and
reducing the solution space derived from the interview study. Due
to the selection of participants and the results from the literature
review, our results do not equally cover all phases of the software

External Validity. Based on the literature review and the inter-
views, we derived six core activities for the development of explain-
able systems. These activities have only been analyzed with regard
to their potential applicability in industry (but not for their “real”
applicability). So far, it was not possible to conduct a concrete case
study highlighting how these activities can be used to support a
real development process. Nevertheless, we are confident that the
conclusions made are also correct for other companies and scenar-
ios. However, we must not assume that the activities are suitable
for all possible contexts.

4 RESULTS
In this section, we present the results related to both our literature
review and the interviews. We give an overview of our findings
rather than an in-depth analysis since the space allowed for research
papers is limited to elaborate on all findings.

4.1 Literature Review
We conducted the literature review to assess what activities and
practices are used or recommended for the development of explain-
able systems. We considered the unique properties of explainable
systems as described in the literature in order to classify the appro-
priate activities and practices.

Many authors recommend the integration of a user-centered de-
velopment strategy for the creation of explainable systems (cf. [12,
13, 43, 59, 60]). An explainable system provides explanations to ad-
dress knowledge gaps, which are highly specific to each individual.
Furthermore, explanations constitute a communication interface
between humans and machines. These are two important reasons
to incorporate approaches and experiences from human-machine
interaction inside the process.

We found no studies that expressly stated that a completely dif-
ferent methodology from existing development models is required.
Instead, we identified six core activities and associated practices,
presented in Figure 1. The practices in blue (brainstorming sessions
and end user observation) were only mentioned in the interview
study, while the others were mentioned in both the literature and
the interview study. We assigned the activities to the corresponding
phases of the software lifecycle. Table 3 shows the relevant prac-
tices for the development of explainable systems identified in the
literature, organized by phase.

The identified activities can be integrated into any software pro-
cess that implements the software lifecycle, being waterfall or agile.
One of the main distinctions between waterfall and agile method-
ologies is how the lifecycle is conducted. Agile is an incremental
and iterative approach that repeats specific phases of the lifecycle;
whereas waterfall is a linear and sequential approach that conducts
the lifecycle once. The arrows at the top of Fig. 1 indicate that the
activities can be iterated as needed.

4.1.1 Requirements Engineering.

Vision Definition: Before requirements can be specified, broader
visions are developed. A vision can refer to the capabilities, features,
or quality aspects of a system [50]. A vision defines the long term

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

Figure 1: Six core activities and recommended practices for developing explainable systems, assigned to the respective phases
of the software lifecycle

Table 3: Practices found in the literature, organized by phase

Practice

Sources

Requirements Engineering

Focus Groups/Workshops
Interviews

Mental Models
Personas

Questionnaires
Scenarios

[43],[68]
[7],[10],[14],[15],[18],[21],[22],
[25],[41],[51],[59],[68]
[18],[59]
[2],[3],[12],[54],[28],[47],
[53],[67]
[18],[41],[42],[47],[58]
[2],[14],[17],[16],
[34],[42],[43],[46],[48],
[49],[53],[64],[67],[54]

Design/Implementation

Low-Fidelity Prototypes
High-Fidelity Prototypes

[18],[33],[53],[66],[71]
[7],[9],[13],[18],[23],[24],
[25],[41],[43],[52],[66],[71]

Validation/Testing

Usability Tests

A/B Tests
Interviews
Mental Models
Questionnaires

[13],[18],[26],[42],[43],
[58],[71]
[42],[64]
[9],[18],[41],[43],[48],[7],[59],[71]
[33],[58],[18],[41],[59]
[13],[42],[58],[46],[18]

goals of a project and can facilitate communication and the scope
definition of a software project, boosting the chances of producing
a successful system. By building a shared vision, it is possible to
resolve misunderstandings with respect to the system-related goals
and stakeholders’ expectations, and have a grip on the essential
quality aspects that must be considered in the system [19, 50]. In
this phase, the first thing that needs to be determined is to what ex-
tent explainability really provides additional value for stakeholders.
After all, explainability should only be considered if it is identified

as a need [12] and if it aggregates value to the system since in some
cases the cost of explanations might outweigh their benefits [8].
Qualitative practices can be used to support this process such as
interviews and workshops. These practices help to understand as-
pects such as behaviors, attitudes, and domain-specific aspects such
as technical, business, and environmental contexts. Understanding
these aspects is fundamental to identify and specify requirements.
We consider that vision definition is an important part of every
software project, and that it occurs either intentionally or uninten-
tionally. During this activity, the need for explainability may be
assessed, and from there, it can be determined whether explainabil-
ity is a non-functional requirement for the system, after which the
other activities that we recommend can be carried out.

Stakeholder Analysis: Langer et al. [34] discuss the importance of
paying special attention to understanding the existing stakeholder
groups in the case of explainable systems. Examples of stakeholder
groups are non-technical end users, domain experts, IT experts,
regulators, ethicists, supervisors, and customers. Identifying stake-
holder groups is important to analyze which interests and needs the
relevant stakeholder groups have with respect to explainability [45].
Different stakeholders have different goals and needs regarding
the software system, having also other requirements on explana-
tions. Again, qualitative approaches such as interviews and end
user observation give valuable information on users’ genuine needs
and expectations regarding the system, on where explainability
might be needed in the system, and on how explanations should
be designed [7, 18]. The most frequent practices used to achieve
these goals are interviews, personas, scenarios, questionnaires, fo-
cus groups, and workshops. After the two first activities (vision
definition and stakeholder analysis), broad explainability goals may
be set, which can be further refined over the subsequent phases.

Back-End Analysis:

In this activity, the algorithm’s logic is eval-
uated or planned with respect to the explainability goals. The first
stage of this activity is to determine if the component of the system
to be explained already exists (e.g., explainability must be imple-
mented into an existing system) or whether the algorithm must still
be developed/integrated (e.g., a system developed from scratch). If
it already exists, the first thing to consider is whether the algorithm
to be explained (we will refer to it as the “back-end algorithm” from
now on) is interpretable [36]. As an example, consider that the

Design/ImplementationExplainability DesignHigh-Fidelity PrototypesLow-Fidelity PrototypesValidation/TestingEvaluationA/B TestsFocus GroupsInterviewsQuestionnairesUsability TestsScenariosWorkshopsRequirements EngineeringRequirements  Vision Definition    Stakeholder AnalysisBrainstorming SessionsPersonasEnd User ObservationFocus GroupsScenariosInterviewsQuestionnairesWorkshopsBack-End Analysis   Trade-Off AnalysisBrainstorming SessionsInterviewsWorkshopsExpert  mental modelUser mental modelDiff. shows the need for improvementHow Can We Develop Explainable Systems?

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

back-end algorithm is based on a machine learning model. Machine
learning models, especially deep learning, can produce accurate
system outputs but are often referred to as black-boxes, since their
rationale cannot be easily understood [4]. Even data scientists fre-
quently struggle to grasp how the model produces its outcomes
[39]. In the machine learning domain, a model is considered inter-
pretable when it is possible to determine why it produces a specific
outcome. In a nutshell, the more interpretable a model is, the easier
it is to explain its rationale and outcomes. In some cases, the back-
end algorithm consists of a model that is not interpretable, so that
alternative explainability techniques (e.g., post-hoc explanations)
need to be employed to explain it [4].

Another factor to be analyzed in this case is whether the desired
explanations require global or local interpretability. Global inter-
pretability often relates to understanding how a model works in
general, whereas local interpretability refers to explaining each spe-
cific prediction [32]. Therefore, it is essential to examine and specify
which explainability approach is required and practicable in light of
the algorithm to be explained [36]. Communication between team
members is essential in order to understand if there are technical
constraints that limit the attainment of the explainability goals, and
to find appropriate solutions [35]. Suitable practices are workshops,
brainstorming sessions and interviews with team members.

Trade-off Analysis: During trade-off analysis, it is important to
evaluate how explainability interacts with other quality aspects.
Chazette et al. [11] investigated the interaction between explain-
ability and other quality aspects and identified 57 quality aspects on
which explainability can have both a positive and negative impact.
For instance, in the case of usability, on the positive side, explana-
tions can increase the ease of use of a system [44] or lead to a more
efficient use [70]. On the negative side, explanations can overwhelm
users with excessive information [57] and can impair the user in-
terface design. This negative or positive impact depends, in the end,
on the design decisions toward explainability. During this activity,
knowledge catalogues are useful artifacts that can assist software
developers in avoiding quality-related conflicts and determining
the best techniques for achieving the intended quality outcomes
[12]. Hence, practitioners should focus on design decisions and
interactions in order to turn explainability into a positive catalyst
for other crucial quality aspects in modern systems. Practices from
back-end analysis are also recommended for this activity.

Requirements and Mental Models. To define requirements, it is
important to compare and understand what the goals of the differ-
ent stakeholders in the process are and how these goals point to
what is expected from the system when it comes to the commu-
nication with the user [40]. Mental models are often mentioned
in the literature as a way to capture expectations regarding the
understandability and explainability of a system [18, 33, 41, 58, 59],
supporting the definition of requirements. For instance, after back-
end and trade-off analysis, an expert mental model can be defined.
An expert mental model represents the expected behavior of the
system, based on the designers’ (e.g., the experts) conception. Ex-
pert mental models can be used to capture how the system should
be understood and how an explanation should help to understand
the system. Practitioners can use expert mental models as a refer-
ence when defining or refining requirements. In addition, expert

mental models can serve as reference later on, to be compared with
the users’ mental models, to evaluate the understandability of the
system or the quality of the explanations. Essentially, mental mod-
els can help to define the right requirements, and to meet the right
design decisions.

4.1.2 Design / Implementation.

Explainability Design: During this activity, software practition-
ers must make decisions about the specific characteristics of ex-
planations based on the requirements. Important design aspects
concerning explanations should be specified in this phase: whether
explanations should be static or interactive; the language to be
used in the explanations (e.g., technical, casual); when explanations
should be presented (e.g., before or after an event); and how the
information should be presented (e.g., audio, text, and other UI
aspects) [36, 44]. During explainability design, prototypes (low-
fidelity and high-fidelity prototypes) are useful for presenting de-
sign ideas and various forms of explanations during explainability
design, allowing for rapid visualization and discussion of design
concepts before the actual implementation. Prototypes can be eval-
uated for effectiveness and to assess whether they meet the ex-
plainability requirements. Once design decisions have been made,
their implementation can take place according to the company’s
development culture.

4.1.3 Validation / Testing.

Evaluation: The evaluation activity checks whether the system is
explainable, that is, whether the explanations are adequate or need
to be improved. The focus of this activity is on end-user feedback
in combination with prototype or version testing (A/B tests), since
the effectiveness and quality of an explanation is subjective [37].
Evaluating the explainability of a system is challenging, as each
individual has different cognitive processes while understanding
something [37]. Since explanations fill knowledge gaps, understand-
ing the end users’ cognitive processes is essential. Therefore, an
important aspect for the evaluation is, again, the concept of mental
models, since they provide a good way to capture cognitive pro-
cesses [18]. During the system evaluation, users’ mental models
can be compared to the expert mental model created during re-
quirements engineering. Deviations of the end-user model from
the expert model reveal misunderstandings and/or problems with
explanation requirements that need to be addressed, by adapting
the system itself or the provided explanations so that the system
can be better understood [18, 41].

4.2 Interview Study
In order to evaluate the insights of our literature review, we con-
ducted an interview study. To understand in what context the study
participants worked, the first point of the interview was to ask
them what kind of software process is applied in their companies.
All participants stated that they use an agile software development
process: The smaller companies use Scrum, the larger ones apply
the V-model extended with SAFe. In the end of each iteration, there
is a timeframe for testing or evaluation. This feedback loop was
often emphasized by the participants as being essential.

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

We compared the processes developed by our participants to the
findings of the literature to validate the conclusions of the litera-
ture review. Figure 2 shows which practices (and in which phase)
are perceived as useful to support the development of explainable
systems, according to the participants.

Regarding the methods that can be used during requirements en-
gineering, participants suggested interviews (73.7%), focus groups
and workshops (57.9%), personas (47.4%), questionnaires (42.1%),
brainstorming with customers and colleagues (36.8%), scenarios
(31.6%), and/or end user observation (42.1%). During design, partic-
ipants suggested low-fidelity prototypes (84.2%) (such as mock-ups
and paper prototypes) and/or high-fidelity prototypes (26.3%). Pro-
totypes were often mentioned as a practice useful for requirements
prioritization. For the evaluation, usability tests (68.4%), end user ob-
servation (63.1%), interviews (36.8%), questionnaires (57.9%) and/or
A/B tests (52.6%) were suggested.

Figure 2: Recommended practices per phase (n=19)

Overall, the participants’ recommendations match the findings in
the literature. Even though the participants did not mention mental
models explicitly, these were implicitly present in their statements
at various points when emphasizing the need for a comparison be-
tween users’ expectations and the system’s functioning. Therefore,
we see it as a useful and plausible practice. In addition, the intervie-
wees recommended practices such as brainstorming sessions with
the team and/or with stakeholders, and also end user observation.
In the following sections, we discuss main points that were
deemed important by the interviewees in addition to the activities
or practices. These points include the significance of an iterative
and user-centered approach, the impact of process type, how real-
istic the activities and practices are, and the potential problems of
incorporating them into practice.

Iterative and User-Centered Approach. In summary, the par-
4.2.1
ticipants were unanimously in favor of developing explainable
systems using an iterative process. However, note that all partici-
pants work in an agile environment and might not be experienced
with plan-driven approaches, which might have influenced their
recommendation. All participants recommend an agile develop-
ment process, since [sic] “agile procedures are iterative and allow

changes to be made in an uncomplicated way”. Since explainability
is still quite unknown and, hence, not explicitly addressed in exist-
ing software processes, participants advocate for a trial-and-error
iterative procedure to develop explainable systems. This way, the
users’ understanding can be constantly evaluated and the design
can be changed to improve the explainability of the system

One participant mentioned that the development of explainable
systems is rather independent of the software development pro-
cess and that it is also possible within a waterfall development
process, but it can take a longer development time compared to
agile approaches. Another participant affirmed that the waterfall
development process could be less suitable for developing explain-
able systems due to its sequential nature since a perfect solution
is not likely to be found upfront. One participant highlighted the
importance of feedback loops in the process, since they allow to
adapt the design according to user feedback.

An end-user-centered process was recommended by all partici-
pants for the development of explainable systems. Since all partic-
ipants stated that the development of explainable systems would
be possible using the current development methodology in their
companies, and often used the process as a base for their recom-
mendations, we asked the participants if they see any need for
optimization. All participants agreed that adopting more end-user-
centered practices can help to better capture end-user requirements
and evaluate design decisions. Eight participants mentioned that
including the team members in the evaluation phase and making
them aware of user feedback can help them better understand the
existing challenges and design concerns.

4.2.2 Realism and Challenges. Four participants mentioned that
the best development process for explainable systems would be
useless if the team is not aware of the importance of explainability.
Therefore, the motivation for integrating explainability should be
clear to the team and, at best, the team should be convinced of and
inspired by its value. Just as the requirement for, e.g., security is
constantly considered, explainability must also be kept in mind
whenever changes or adjustments are made.

Finally, the participants were asked about the suitability of their
recommendations when considering their company’s context. As
an example, resources such as time, cost, or the number of employ-
ees in a company were mentioned by the interviewer as possible
factors influencing the practicability of the suggested activities and
practices. All participants agreed that the activities and practices
could be integrated. Three participants noted that the choice of
the activities and practices depends on each company’s resources,
on the team, and on the product. Seven participants said that a
trial-and-error approach is necessary until an ideal strategy about
the optimal activities and practices, and how to integrate them into
the company’s specific context is identified.

5 DISCUSSION
New quality requirements such as explainability always bring new
challenges and uncertainties as how to proceed or adapt the im-
plementation of systems. Therefore, we combined the findings of
a literature review with the perspectives of practitioners gathered
through an interview study to compile a set of activities and prac-
tices for the development of explainable systems. Practitioners

6891411871651311712100123456789101112131415161718192021Usability TestsScenariosQuestionnairesPersonasLow-Fidelity PrototypesInterviewsHigh-Fidelity PrototypesFocus Groups / WorkshopsEnd-User ObservationBrainstorming SessionsA/B TestsRequirements EngineeringDesign / ImplementationEvaluation / TestingHow Can We Develop Explainable Systems?

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

believe that existing user-centered approaches and practices are
effective when dealing with explainability in a software project,
which supports the literature findings. The participants also ex-
pressed familiarity with these practices. Based on this analysis, we
answer our first research question:

→ Answer to RQ1: We identified six core activities that should
be considered when developing explainable software systems: vi-
sion definition, stakeholder analysis, back-end analysis, trade-off
analysis, explainability design, and evaluation. These activities can
be supported by user-centered practices such as end user observa-
tion, interviews, questionnaires, personas, prototypes, focus groups,
workshops, brainstorming sessions, scenarios, storyboards, A/B and
usability tests.

This finding supports the conclusions of a previous work. In
this work, Chazette and Schneider [12] recommend the adoption of
existing user-centered practices during requirements engineering
to develop explainable systems. One the one hand, this approach
is derived from an empirical study with end users, in which the
participants stated their opinions on potential issues regarding em-
bedded explanations. The responses were analyzed and compared
to existing usability heuristics. The heuristics were shown to be
helpful in the resolution of the pointed issues and to support the de-
sign of adequate explanations which, in turn, favors the assumption
that user-centered methods are adequate to develop explainable
systems. On the other hand, this assumption had not yet been val-
idated beyond this study, and, more importantly, the feedback of
software experts on whether this approach is compatible with the
reality of the industry was still missing. As a result of the current
investigation, we discovered that these user-centered practices are
also used in the literature. Additionally, feedback from software ex-
perts confirms that these practices are also feasible in the industry,
indicating an alignment between research and practice.

Our study lays the foundation on the necessary activities that
should be integrated into the software lifecycle to assist practition-
ers through the steps required for the development of meaningful,
explainable systems. The set of activities proposed in this paper and
the respective practices constitute our recommendation on how to
develop explainable systems:

→ Answer to RQ2: We synthesized the practices identified in
the literature review in six core activities that belong to three life-
cycle phases: requirements engineering, design/implementation,
and validation/testing. In order to develop explainable systems,
these activities need to be included into the software lifecycle and
therefore carried out in the real process.

Based on the answers of our two research questions, we consider

the main takeaways of our work:

(1) End-user-centered practices are most suited to understand
explainability requirements and develop explainable sys-
tems.

(2) The six core activities can be integrated into existing pro-
cesses, allowing them to be carried out in both traditional
and agile settings.

These takeaways evidence the flexibility of the set of activities
and practices recommended in this paper. Firstly, our results point

to the use of practices rather than methods. This goes in line with
findings from recent research on software development processes
showing that practices appear to be way more important than
methods, as the use of practices does not depend on the selected
development process [31, 55], making the activities and practices
applicable in either waterfall or agile development environments.
Secondly, there is evidence that many of the methods and prac-
tices coming from the research community have not been embraced
by practitioners, since it is often difficult for practitioners to find a
way to incorporate new research ideas into their busy workdays [1].
The major concern of software professionals is cost, both in terms
of time and money [30]. As a consequence, embracing current activ-
ities and practices rather than introducing new ones that increase
costs is a favorable option [12]. Even though integrating such activi-
ties and practices might still represent a not negligible overhead for
the industry, the proposed activities can still be adapted according
to the necessity of each particular project.

It is important to notice that the activities cover the software
lifecycle only partially, addressing only requirements engineering,
design, and testing. We consider that current knowledge on how
to structure the remainder of the development process is equally
true for the creation of explainable systems and, hence, is outside
the focus of this study. However, even though the activities can be
integrated into an existing process alongside other activities and
phases, we do not discard the need for further research to cover the
whole lifecycle. In either case, it was possible to identify that there
is a special focus on the requirements process and on validating
these requirements, both in the works found in the literature and
in the interviews. Therefore, requirements-related activities in the
process should be given special attention since the development of
explainable systems is heavily reliant on meticulous requirement
analysis, as well as how those requirements are implemented and
transformed into explanations, and how those explanations are
displayed on the user interface.

6 CONCLUSION
Although explainability is considered a catalyst for essential quali-
ties in modern software systems such as transparency and fairness,
it is not yet clear how to approach it in practice. We conducted a
literature review to understand how explainability is addressed and
which activities and practices can be used to design explanations.
Based on the results from the literature, we built a recommendation
consisting of six core activities and associated practices for the
development of explainable systems. We conducted an interview
study with 19 software practitioners to assess the feasibility of the
activities in practice.

As researchers, we have to take action in investigating methods
and techniques that are aligned with the practice and offer advan-
tages for practitioners instead of more overhead. Therefore, we
recommend a set of activities and well-known practices that may
be included in the development process, instead of conceiving a
completely new process with unfamiliar practices for the devel-
opment of explainable systems. Our research shows that software
practitioners can employ well-known user-centered practices to
elicit, implement, and test requirements that are aligned with users’
needs, as well as their demands and context.

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

Future research is needed to learn more about the other phases of
the software lifecycle and whether they need to be adapted. So far,
despite the attempts to analyze software processes, there were no
proposals that explicitly focus on the development of explainable
systems. Therefore, we believe our proposal provides both a starting
point and a foundation that can be further updated and improved.

ACKNOWLEDGMENTS
This work was supported by the research initiative Mobilise be-
tween the Technical University of Braunschweig and Leibniz Uni-
versity Hannover, funded by the Ministry for Science and Culture of
Lower Saxony. We thank our colleague Nils Prenner for his valuable
feedback and fruitful discussions.

REFERENCES
[1] David Ameller, Claudia Ayala, Jordi Cabot, and Xavier Franch. 2012. How do
software architects consider non-functional requirements: An exploratory study.
In 2012 20th IEEE International Requirements Engineering Conference (RE). 41–50.
https://doi.org/10.1109/RE.2012.6345838

[2] Josh Andres, Christine T. Wolf, Sergio Cabrero Barros, Erick Oduor, Rahul Nair,
Alexander Kjærum, Anders Bech Tharsgaard, and Bo Schwartz Madsen. 2020.
Scenario-Based XAI for Humanitarian Aid Forecasting. In Extended Abstracts of
the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,
USA) (CHI EA ’20). Association for Computing Machinery, New York, NY, USA,
1–8. https://doi.org/10.1145/3334480.3382903

[3] Oren Barkan, Yonatan Fuchs, Avi Caciularu, and Noam Koenigstein. 2020.
Explainable Recommendations via Attentive Multi-Persona Collaborative Filter-
ing. Association for Computing Machinery, New York, NY, USA, 468–473.
https://doi.org/10.1145/3383313.3412226

[4] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Ben-
netot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel
Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Ex-
plainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities
and challenges toward responsible AI.
Information Fusion 58 (2020), 82–115.
https://doi.org/10.1016/j.inffus.2019.12.012

[5] Yuriy Brun and Alexandra Meliou. 2018. Software Fairness. In Proceedings of the
2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (Lake Buena Vista, FL,
USA) (ESEC/FSE 2018). Association for Computing Machinery, New York, NY,
USA, 754–759. https://doi.org/10.1145/3236024.3264838

[6] Wasja Brunotte, Larissa Chazette, Verena Klös, and Timo Speith. 2022. Quo
Vadis, Explainability? – A Research Roadmap for Explainability Engineering. In
Requirements Engineering: Foundation for Software Quality, Vincenzo Gervasi and
Andreas Vogelsang (Eds.). Springer International Publishing, Cham, 26–32.
[7] Peter Brusilovsky and Chun Hua Tsai. 2019. Designing explanation interfaces for
transparency and beyond. CEUR Workshop Proceedings 2327 (1 Jan. 2019). 2019
Joint ACM IUI Workshops, ACMIUI-WS 2019 ; Conference date: 20-03-2019.
[8] Andrea Bunt, Matthew Lount, and Catherine Lauzon. 2012. Are Explanations
Always Important? A Study of Deployed, Low-Cost Intelligent Interactive Sys-
tems. In Proceedings of the 2012 ACM International Conference on Intelligent User
Interfaces (Lisbon, Portugal) (IUI ’12). Association for Computing Machinery,
New York, NY, USA, 169–178. https://doi.org/10.1145/2166966.2166996

[9] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The Role of
Explanations on Trust and Reliance in Clinical Decision Support Systems. In 2015
International Conference on Healthcare Informatics. 160–169. https://doi.org/10.
1109/ICHI.2015.26

[10] Florian Cech and Marlene Wagner. 2019. ERollin’ On Green: A Case Study on Eco-
Feedback Tools for EMobility. In Proceedings of the 9th International Conference
on Communities & Technologies - Transforming Communities (Vienna, Austria)
(C&T ’19). Association for Computing Machinery, New York, NY, USA, 121–125.
https://doi.org/10.1145/3328320.3328402

[11] Larissa Chazette, Wasja Brunotte, and Timo Speith. 2021. Exploring Explain-
ability: A Definition, a Model, and a Knowledge Catalogue. In 2021 IEEE 29th
International Requirements Engineering Conference (RE). 197–208. https://doi.org/
10.1109/RE51729.2021.00025

[12] Larissa Chazette and Kurt Schneider. 2020. Explainability as a non-functional
requirement: challenges and recommendations. Requirements Engineering 25, 4
(2020), 493–514. https://doi.org/10.1007/s00766-020-00333-1

[13] Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O’Connell, Terrance Gray,
F. Maxwell Harper, and Haiyi Zhu. 2019. Explaining Decision-Making Algorithms
through UI: Strategies to Help Non-Expert Stakeholders. In Proceedings of the
2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland

Uk) (CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–12.
https://doi.org/10.1145/3290605.3300789

[14] Douglas Cirqueira, Dietmar Nedbal, Markus Helfert, and Marija Bezbradica. 2020.
Scenario-Based Requirements Elicitation for User-Centric Explainable AI. In
Machine Learning and Knowledge Extraction, Andreas Holzinger, Peter Kieseberg,
A Min Tjoa, and Edgar Weippl (Eds.). Springer International Publishing, Cham,
321–341.

[15] Fan Du, Catherine Plaisant, Neil Spring, Kenyon Crowley, and Ben Shneiderman.
2019. EventAction: A Visual Analytics Approach to Explainable Recommendation
for Event Sequences. ACM Trans. Interact. Intell. Syst. 9, 4, Article 21 (aug 2019),
31 pages. https://doi.org/10.1145/3301402

[16] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz.
2021. Expanding Explainability: Towards Social Transparency in AI Systems. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
(Yokohama, Japan) (CHI ’21). Association for Computing Machinery, New York,
NY, USA, Article 82, 19 pages. https://doi.org/10.1145/3411764.3445188
[17] Upol Ehsan and Mark O. Riedl. 2020. Human-Centered Explainable AI: Towards
a Reflective Sociotechnical Approach. In HCI International 2020 - Late Breaking
Papers: Multimodality and Intelligence, Constantine Stephanidis, Masaaki Kurosu,
Helmut Degen, and Lauren Reinerman-Jones (Eds.). Springer eBook Collection,
Vol. 12424. Springer International Publishing and Imprint Springer, Cham, 449–
466. https://doi.org/10.1007/978-3-030-60117-1_33

[18] Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike
Haug, and Heinrich Hussmann. 2018. Bringing Transparency Design into Practice.
In 23rd International Conference on Intelligent User Interfaces (Tokyo, Japan) (IUI
’18). Association for Computing Machinery, New York, NY, USA, 211–223. https:
//doi.org/10.1145/3172944.3172961

[19] Martin Glinz and Samuel A. Fricker. 2015. On shared understanding in software
engineering: an essay. Computer Science - Research and Development 30, 3-4
(2015), 363–376. https://doi.org/10.1007/s00450-014-0256-x

[20] Maria J. Grant and Andrew Booth. 2009. A typology of reviews: an analysis of
14 review types and associated methodologies. Health Information & Libraries
Journal 26, 2 (2009), 91–108. https://doi.org/10.1111/j.1471-1842.2009.00848.x
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1471-1842.2009.00848.x
[21] Ana I. Grimaldo and Jasminko Novak. 2019. User-Centered Visual Analytics
Approach for Interactive and Explainable Energy Demand Analysis in Prosumer
Scenarios. In Computer Vision Systems, Dimitrios Tzovaras, Dimitrios Giakoumis,
Markus Vincze, and Antonis Argyros (Eds.). Springer International Publishing,
Cham, 700–710.

[22] Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana-
Amate, Alistair Nottle, and Alun Preece. 2019. A systematic method to understand
requirements for explainable AI (XAI) systems. In Proceedings of the IJCAI Work-
shop on eXplainable Artificial Intelligence (XAI 2019), Macau, China, Vol. 11.
[23] Xiangnan He, Tao Chen, Min-Yen Kan, and Xiao Chen. 2015. TriRank: Review-
Aware Explainable Recommendation by Modeling Aspects. In Proceedings of the
24th ACM International on Conference on Information and Knowledge Management
(Melbourne, Australia) (CIKM ’15). Association for Computing Machinery, New
York, NY, USA, 1661–1670. https://doi.org/10.1145/2806416.2806504

[24] Jennifer Heier. 2021. Design Intelligence - Taking Further Steps Towards New
Methods and Tools for Designing in the Age of AI. In Artificial Intelligence in
HCI, Helmut Degen and Stavroula Ntoa (Eds.). Springer International Publishing,
Cham, 202–215.

[25] Eric Holder and Ning Wang. 2021. Explainable artificial intelligence (XAI) interac-
tively working with humans as a junior cyber analyst. Human-Intelligent Systems
Integration 3, 2 (2021), 139–153. https://doi.org/10.1007/s42454-020-00021-z

[26] Andreas Holzinger, André Carrington, and Heimo Müller. 2020. Measuring
the Quality of Explanations: The System Causability Scale (SCS): Comparing
Human and Machine Explanations. Kunstliche intelligenz 34, 2 (2020), 193–198.
https://doi.org/10.1007/s13218-020-00636-z

[27] ISO Central Secretary. 2019. ISO/IEC 9241-210:2019 Ergonomics of human-system
interaction — Part 210: Human-centred design for interactive systems. Standard
ISO 9241-210:2019. International Organization for Standardization. https://www.
iso.org/standard/77520.html

[28] Sérgio Jesus, Catarina Belém, Vladimir Balayan, João Bento, Pedro Saleiro, Pedro
Bizarro, and João Gama. 2021. How Can I Choose an Explainer? An Application-
Grounded Evaluation of Post-Hoc Explanations. In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada)
(FAccT ’21). Association for Computing Machinery, New York, NY, USA, 805–815.
https://doi.org/10.1145/3442188.3445941

[29] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bai-
ley, and Stephen Linkman. 2009. Systematic literature reviews in software
engineering–a systematic literature review. Information and software technology
51, 1 (2009), 7–15.

[30] Jil Klünder, Regina Hebig, Paolo Tell, Marco Kuhrmann, Joyce Nakatumba-
Nabende, Rogardt Heldal, Stephan Krusche, Masud Fazal-Baqaie, Michael Felderer,
Marcela Fabiana Genero Bocco, et al. 2019. Catching up with method and process
practice: An industry-informed baseline for researchers. In 2019 IEEE/ACM 41st
International Conference on Software Engineering: Software Engineering in Practice

How Can We Develop Explainable Systems?

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

(ICSE-SEIP). IEEE, 255–264.

[31] Jil Klünder, Dzejlana Karajic, Paolo Tell, Oliver Karras, Christian Münkel, Jürgen
Münch, Stephen G MacDonell, Regina Hebig, and Marco Kuhrmann. 2020. De-
termining context factors for hybrid development methods with trained models.
In Proceedings of the International Conference on Software and System Processes.
61–70.

[32] Leon Kopitar, Leona Cilar, Primoz Kocbek, and Gregor Stiglic. 2019. Local vs.
Global Interpretability of Machine Learning Models in Type 2 Diabetes Mellitus
Screening. In Artificial Intelligence in Medicine: Knowledge Representation and
Transparent and Explainable Systems, Mar Marcos, Jose M. Juarez, Richard Lenz,
Grzegorz J. Nalepa, Slawomir Nowaczyk, Mor Peleg, Jerzy Stefanowski, and
Gregor Stiglic (Eds.). Springer International Publishing, Cham, 108–119.
[33] Todd Kulesza, Simone Stumpf, Margaret Burnett, Weng-Keen Wong, Yann Riche,
Travis Moore, Ian Oberst, Amber Shinsel, and Kevin McIntosh. 2010. Explanatory
Debugging: Supporting End-User Debugging of Machine-Learned Programs. In
2010 IEEE Symposium on Visual Languages and Human-Centric Computing. 41–48.
https://doi.org/10.1109/VLHCC.2010.15

[34] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner,
Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want from
Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and
a conceptual model guiding interdisciplinary XAI research. Artificial Intelligence
296 (2021), 103473. https://doi.org/10.1016/j.artint.2021.103473

[35] Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing
Design Practices for Explainable AI User Experiences. Association for Computing
Machinery, New York, NY, USA, 1–15. https://doi.org/10.1145/3313831.3376590
[36] Luca Longo, Randy Goebel, Freddy Lecue, Peter Kieseberg, and Andreas Holzinger.
2020. Explainable artificial intelligence: Concepts, applications, research chal-
lenges and visions. In International Cross-Domain Conference for Machine Learning
and Knowledge Extraction. Springer, 1–16.

[37] Kyle Martin, Anne Liret, Nirmalie Wiratunga, Gilbert Owusu, and Mathias Kern.
2021. Evaluating Explainability Methods Intended for Multiple Stakeholders. KI
- Künstliche Intelligenz 35, 3-4 (2021), 397–411. https://doi.org/10.1007/s13218-
020-00702-6

[38] Philipp Mayring. 2019. Qualitative content analysis: Demarcation, varieties,
developments. In Forum: Qualitative Social Research, Vol. 20. Freie Universität
Berlin, 1–26.

[39] David Alvarez Melis, Harmanpreet Kaur, Hal Daumé III, Hanna Wallach, and
Jennifer Wortman Vaughan. 2021. From Human Explanation to Model Inter-
pretability: A Framework Based on Weight of Evidence. In Proceedings of the
AAAI Conference on Human Computation and Crowdsourcing, Vol. 9. 35–47.
[40] Deepti Mishra, Alok Mishra, and Ali Yazici. 2008. Successful requirement elici-
tation by combining requirement engineering techniques. In 2008 First Interna-
tional Conference on the Applications of Digital Information and Web Technologies
(ICADIWT). 258–263. https://doi.org/10.1109/ICADIWT.2008.4664355

[41] Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2021. A Multidisciplinary
Survey and Framework for Design and Evaluation of Explainable AI Systems.
ACM Trans. Interact. Intell. Syst. 11, 3–4, Article 24 (aug 2021), 45 pages. https:
//doi.org/10.1145/3387166

[42] Yazan Mualla, Igor Haman Tchappi, Amro Najjar, Timotheus Kampik, Stéphane
Galland, and Christophe Nicolle. 2020. Human-agent Explainability: An Experi-
mental Case Study on the Filtering of Explanations.. In ICAART (1). 378–385.

[43] Mohammad Naiseh, Dena Al-Thani, Nan Jiang, and Raian Ali. 2021. Explainable
recommendation: when design meets trust calibration. World Wide Web 24, 5
(2021), 1857–1884. https://doi.org/10.1007/s11280-021-00916-0

[44] Ingrid Nunes and Dietmar Jannach. 2017. A systematic review and taxonomy of
explanations in decision support and recommender systems. User modeling and
user-adapted interaction 27, 3-5 (2017), 393–444. https://doi.org/10.1007/s11257-
017-9195-0

[45] Bashar Nuseibeh and Steve Easterbrook. 2000. Requirements Engineering: A
Roadmap. In Proceedings of the Conference on The Future of Software Engineering
(Limerick, Ireland) (ICSE ’00). Association for Computing Machinery, New York,
NY, USA, 35–46. https://doi.org/10.1145/336512.336523

[46] Daniel Omeiza, Konrad Kollnig, Helena Web, Marina Jirotka, and Lars Kunze. 2021.
Why Not Explain? Effects of Explanations on Human Perceptions of Autonomous
Driving. In 2021 IEEE International Conference on Advanced Robotics and Its Social
Impacts (ARSO). 194–199. https://doi.org/10.1109/ARSO51874.2021.9542835
[47] Henrique Ramos, Mateus Fonseca, and Lesandro Ponciano. 2021. Model-
ing and Evaluating Personas with Software Explainability Requirements.
In
Human-Computer Interaction, Pablo H. Ruiz, Vanessa Agredo-Delgado, and
André Luiz Satoshi Kawamoto (Eds.). Springer eBook Collection, Vol. 1478.
Springer International Publishing and Imprint Springer, Cham, 136–149. https:
//doi.org/10.1007/978-3-030-92325-9_11

[48] Juan Rebanal, Jordan Combitsis, Yuqi Tang, and Xiang ’Anthony’ Chen. 2021.
XAlgo: A Design Probe of Explaining Algorithms’ Internal States via Question-
Answering. Association for Computing Machinery, New York, NY, USA, 329–339.
https://doi.org/10.1145/3397481.3450676

[49] Mireia Ribera and Agata Lapedriza. 2019. Can we do better explanations? A
proposal of user-centered explainable AI.. In IUI Workshops, Vol. 2327. 38.

[50] Kurt Schneider, Melanie Busch, Oliver Karras, Maximilian Schrapel, and Michael
Rohs. 2019. Refining Vision Videos. In Requirements Engineering: Foundation for
Software Quality, Eric Knauss and Michael Goedicke (Eds.). Springer International
Publishing, Cham, 135–150.

[51] Tobias Schneider, Joana Hois, Alischa Rosenstein, Sabiha Ghellal, Dimitra
Theofanou-Fülbier, and Ansgar R.S. Gerlicher. 2021. ExplAIn Yourself! Trans-
parency for Positive UX in Autonomous Driving. In Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ’21).
Association for Computing Machinery, New York, NY, USA, Article 161, 12 pages.
https://doi.org/10.1145/3411764.3446647

[52] Tjeerd A.J. Schoonderwoerd, Wiard Jorritsma, Mark A. Neerincx, and Karel
van den Bosch. 2021. Human-centered XAI: Developing design patterns for
explanations of clinical decision support systems. International Journal of Human-
Computer Studies 154 (2021), 102684. https://doi.org/10.1016/j.ijhcs.2021.102684
[53] Hariharan Subramonyam, Colleen Seifert, and Eytan Adar. 2021. Towards A Pro-
cess Model for Co-Creating AI Experiences. In Designing Interactive Systems Con-
ference 2021 (Virtual Event, USA) (DIS ’21). Association for Computing Machinery,
New York, NY, USA, 1529–1543. https://doi.org/10.1145/3461778.3462012
[54] Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde,
Kartik Talamadupula, and Justin D. Weisz. 2022. Investigating Explainability of
Generative AI for Code through Scenario-Based Design. In 27th International
Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI ’22). Association
for Computing Machinery, New York, NY, USA, 212–228. https://doi.org/10.
1145/3490099.3511119

[55] Paolo Tell, Jil Klünder, Steffen Küpper, David Raffo, Stephen G MacDonell, Jürgen
Münch, Dietmar Pfahl, Oliver Linssen, and Marco Kuhrmann. 2019. What are
hybrid development methods made of? An evidence-based characterization. In
2019 IEEE/ACM International Conference on Software and System Processes (ICSSP).
IEEE, 105–114.

[56] Andrea C. Tricco, Jesmin Antony, Wasifa Zarin, Lisa Strifler, Marco Ghassemi,
John Ivory, Laure Perrier, Brian Hutton, David Moher, and Sharon E. Straus.
2015. A scoping review of rapid review methods. BMC Medicine 13, 1 (2015), 224.
https://doi.org/10.1186/s12916-015-0465-6

[57] Chun-Hua Tsai and Peter Brusilovsky. 2019. Explaining Recommendations in an
Interactive Hybrid Social Recommender. In Proceedings of the 24th International
Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI ’19).
Association for Computing Machinery, New York, NY, USA, 391–396. https:
//doi.org/10.1145/3301275.3302318

[58] Chun-Hua Tsai and Peter Brusilovsky. 2021. The effects of controllability and
explainability in a social recommender system. User Modeling and User-Adapted
Interaction 31, 3 (2021), 591–627. https://doi.org/10.1007/s11257-020-09281-5

[59] Chun-Hua Tsai, Yue You, Xinning Gui, Yubo Kou, and John M. Carroll. 2021.
Exploring and Promoting Diagnostic Transparency and Explainability in Online
Symptom Checkers. In Proceedings of the 2021 CHI Conference on Human Factors
in Computing Systems (Yokohama, Japan) (CHI ’21). Association for Computing
Machinery, New York, NY, USA, Article 152, 17 pages. https://doi.org/10.1145/
3411764.3445101

[60] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Design-
ing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)
(CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–15.
https://doi.org/10.1145/3290605.3300831

[61] Anna Christina Weigand, Daniel Lange, and Maria Rauschenberger. 2021. How
can Small Data Sets be Clustered?. In Mensch und Computer 2021 - Workshop-
band, Carolin Wienrich, Philipp Wintersberger, and Benjamin Weyers (Eds.).
Gesellschaft für Informatik e.V., Bonn. https://doi.org/10.18420/muc2021-mci-
ws02-284

[62] Gesa Wiegand, Malin Eiband, Maximilian Haubelt, and Heinrich Hussmann. 2020.
“I’d like an Explanation for That!”Exploring Reactions to Unexpected Autonomous
Driving. Association for Computing Machinery, New York, NY, USA. https:
//doi.org/10.1145/3379503.3403554

[63] Claes Wohlin, Per Runeson, Martin Höst, Magnus C. Ohlsson, Björn Regnell, and
Anders Wesslén. 2012. Experimentation in software engineering. Springer, Berlin
and Heidelberg. https://doi.org/10.1007/978-3-642-29044-2

[64] Christine T. Wolf. 2019. Explainability Scenarios: Towards Scenario-Based XAI
Design. In Proceedings of the 24th International Conference on Intelligent User Inter-
faces (Marina del Ray, California) (IUI ’19). Association for Computing Machinery,
New York, NY, USA, 252–257. https://doi.org/10.1145/3301275.3302317
[65] Joost F Wolfswinkel, Elfi Furtmueller, and Celeste P M Wilderom. 2013. Using
grounded theory as a method for rigorously reviewing literature. European
Journal of Information Systems 22, 1 (2013), 45–55. https://doi.org/10.1057/ejis.
2011.51

[66] Yao Xie, Melody Chen, David Kao, Ge Gao, and Xiang ’Anthony’ Chen. 2020.
CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled
Medical Imaging Analysis. Association for Computing Machinery, New York, NY,
USA, 1–13. https://doi.org/10.1145/3313831.3376807

[67] Wei Xu. 2019. Toward human-centered AI: a perspective from human-computer

interaction. Interactions 26, 4 (2019), 42–46.

ICSSP’22, May 20–22, 2022, Pittsburgh, PA, USA

Chazette et al.

[68] Lingxue Yang, Hongrun Wang, and Léa A. Deleris. 2021. What Does It Mean to
Explain? A User-Centered Study on AI Explainability. In Artificial Intelligence in
HCI, Helmut Degen and Stavroula Ntoa (Eds.). Springer International Publishing,
Cham, 107–121.

[69] Affan Yasin, Rubia Fatima, Lijie Wen, Wasif Afzal, Muhammad Azhar, and Richard
Torkar. 2020. On Using Grey Literature and Google Scholar in Systematic Lit-
erature Reviews in Software Engineering. IEEE Access 8 (2020), 36226–36243.
https://doi.org/10.1109/ACCESS.2020.2971712

[70] Jianlong Zhou, Huaiwen Hu, Zhidong Li, Kun Yu, and Fang Chen. 2019. Physiolog-
ical Indicators for User Trust in Machine Learning with Influence Enhanced Fact-
Checking. In Machine Learning and Knowledge Extraction, Andreas Holzinger,
Peter Kieseberg, A Min Tjoa, and Edgar Weippl (Eds.). Springer International
Publishing, Cham, 94–113.

[71] Tongyu Zhou, Haoyu Sheng, and Iris Howley. 2020. Assessing Post-Hoc Explain-
ability of the BKT Algorithm. Association for Computing Machinery, New York,
NY, USA, 407–413. https://doi.org/10.1145/3375627.3375856

