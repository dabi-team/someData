ğ‘€ğ¶2: Rigorous and Efficient Directed Greybox Fuzzing
Samanway Sadhu
Dongdong She
Columbia University
Columbia University

Abhishek Shah
Columbia University

Krish Singal
Columbia University

Peter Coffman
Columbia University

Suman Jana
Columbia University

2
2
0
2

g
u
A
0
3

]

R
C
.
s
c
[

1
v
0
3
5
4
1
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
Directed greybox fuzzing is a popular technique for targeted soft-
ware testing that seeks to find inputs that reach a set of target sites
in a program. Most existing directed greybox fuzzers do not provide
any theoretical analysis of their performance or optimality.

In this paper, we introduce a complexity-theoretic framework to
pose directed greybox fuzzing as a oracle-guided search problem
where some feedback about the input space (e.g., how close an
input is to the target sites) is received by querying an oracle. Our
framework assumes that each oracle query can return arbitrary
content with a large but constant amount of information. Therefore,
we use the number of oracle queries required by a fuzzing algorithm
to find a target-reaching input as the performance metric. Using
our framework, we design a randomized directed greybox fuzzing
algorithm that makes a logarithmic (wrt. the number of all possible
inputs) number of queries in expectation to find a target-reaching
input. We further prove that the number of oracle queries required
by our algorithm is optimal, i.e., no fuzzing algorithm can improve
(i.e., minimize) the query count by more than a constant factor.

We implement our approach in MC2 and outperform state-of-the-
art directed greybox fuzzers on challenging benchmarks (Magma
and Fuzzer Test Suite) by up to two orders of magnitude (i.e., 134Ã—)
on average. MC2 also found 15 previously undiscovered bugs that
other state-of-the-art directed greybox fuzzers failed to find.

CCS CONCEPTS
â€¢ Security and privacy â†’ Software and application security.

KEYWORDS
Greybox Fuzzing; Automated Vulnerability Detection; Noisy Binary
Search; Monte Carlo Counting; Execution Complexity

ACM Reference Format:
Abhishek Shah, Dongdong She, Samanway Sadhu, Krish Singal, Peter Coff-
man, and Suman Jana. 2022. ğ‘€ğ¶2: Rigorous and Efficient Directed Greybox
Fuzzing. In Proceedings of the 2022 ACM SIGSAC Conference on Computer
and Communications Security (CCS â€™22), November 7â€“11, 2022, Los Ange-
les, CA, USA. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/
3548606.3560648

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA.
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9450-5/22/11. . . $15.00
https://doi.org/10.1145/3548606.3560648

1 INTRODUCTION
Directed greybox fuzzing is a popular technique for targeted soft-
ware testing with many security applications such as bug finding,
crash reproduction, checking static analyzer reports, and patch test-
ing [13, 16, 30, 36, 66, 70]. Given a set of target sites in a program,
directed greybox fuzzers automatically search a programâ€™s input
space for inputs that reach the targets. Since the input spaces of
real-world programs are very large, most existing directed greybox
fuzzers use evolutionary algorithms to focus their search on promis-
ing input regions identified using feedback information through
instrumented program execution. For example, the fuzzers often
collect feedback information about control-flow graph distance or
branch constraint distance from the target and prioritize mutating
inputs that are close to the target [13, 16, 36].

The designs of existing evolutionary directed fuzzers in the liter-
ature are typically guided by intuition and empirical results, but,
to the best of our knowledge, they do not provide any theoretical
analysis of their performance. While strong empirical results are a
necessary metric for evaluating any fuzzer design, without a rigor-
ous theoretical understanding, it is difficult to understand the key
guiding principles of fuzzer design. For example, what is the best
possible (i.e., optimal) directed fuzzer? How well does a fuzzing
algorithm scale with the input space size? What kind of feedback
information is most useful for fuzzing? How can an algorithm best
use the feedback? Can one do better than evolutionary algorithms
in using this feedback information?

In this paper, we introduce a novel computational complexity-
theoretic framework to answer some of these questions and design
an asymptotically optimal directed greybox fuzzing algorithm. We
further demonstrate the practical advantages of our algorithm with
extensive empirical results where our algorithm is up to two orders-
of-magnitude faster, on average, than the state-of-the-art directed
greybox fuzzers in challenging benchmarks (Magma and Fuzzer
Test Suite).
Complexity-Theoretic Framework. To reason about an optimal
fuzzer, we introduce a complexity-theoretic formulation of directed
greybox fuzzing that abstracts away the specific details about the
type of instrumentation and fuzzing algorithm into a unified frame-
work. We model the task of directed greybox fuzzing as an oracle-
guided search problem: to find inputs that reach the target given
query access to an oracle that executes the program and reveals
some information about the search space (i.e., the identity of the
promising input regions) to the fuzzing algorithm.

Our framework makes no assumptions about program behaviors
or input/output distributions and faithfully adheres to the design
of modern greybox fuzzing. To model the lightweight fuzzing in-
strumentation used by practical fuzzers for collecting feedback

 
 
 
 
 
 
information during a program execution, we allow the oracle to
return arbitrary content with a large but constant amount of infor-
mation per query. Formally, we allow the oracle to be any function
ğ‘‚ : ğ¼ â†’ {0, 1}ğ‘ that internally executes the instrumented target
program on a constant number of inputs from an input region ğ¼
and returns ğ‘ bits of information (see Section 2.2 for more details).
We model the adaptive, feedback-driven paradigm used in prac-
tical fuzzers by enabling the fuzzer to arbitrarily adapt its choice
of executions/input regions provided to the oracle based on the
information received from the oracle in prior queries.
Execution Complexity. We then introduce the notion of execu-
tion complexity, a metric to asymptotically measure the perfor-
mance of any fuzzing algorithm in our framework in terms of the
number of oracle queries made by the fuzzer before finding an input
that reaches the target site. This metric is a very good fit for our
setting as program execution and feedback generation dominate
the runtime of real-world fuzzers [42]. At a conceptual level, our
complexity metric is similar to the query complexity used to reason
about the lower bounds on potential advantages provided by quan-
tum algorithms by separating the quantum part of the algorithm
from the classical part through an oracle [5, 6].

In our case, we want to establish lower bounds on the advantages
provided by information feedback to the fuzzing algorithms and
measure the corresponding execution (i.e., query) complexity. It
has been shown that any search in an input space of size ğ‘ with a
boolean oracle that only indicates whether a given test input is the
desired one or not (e.g., a blackbox fuzzer) will take at least ğ‘‚ (ğ‘ )
queries [51]. In this paper, we use our framework to explore the
lower bounds on the advantages of extra feedback information (i.e.,
large but constant per oracle query) and how to design an adaptive
algorithm that can best exploit such feedback.
An Optimal Fuzzing Algorithm. For designing a concrete fuzzing
algorithm, we further introduce a special type of oracle called the
noisy counting oracle. A noisy counting oracle takes two arbitrary
input regions, approximately counts the number of inputs in each
region reaching the target sites, and returns the one with higher
count as the more promising one. Due to the approximate nature
of the counts, we assume that the noisy oracle returns the incorrect
answer with probability ğ‘ < 1/2. Later in this section, we describe
how we design an approximate Monte Carlo counting algorithm to
implement the noisy oracle in practice.

Given access to such a noisy oracle, we design a randomized
algorithm for directed greybox fuzzing that achieves the optimal
log(ğ‘ )
expected execution complexity ğ‘‚ (
2 âˆ’ğ‘) 2 ), up to constant factors, in
( 1
an input space with ğ‘ inputs. We further prove that this execution
complexity cannot be improved (beyond a constant factor) by any
other fuzzing algorithm (evolutionary or otherwise) for any given
ğ‘ < 1/2.

As we detail in Section 2.3, our fuzzing algorithm, at its core,
uses a counting oracle to select regions with higher counts and
repeatedly narrows down promising regions with binary search.
If our counting oracle was noiseless, a simple binary search based
fuzzing algorithm will efficiently find target-reaching inputs. How-
ever, the noise in the counting oracle can cause the fuzzer to some-
times incorrectly select input regions with low counts. As we do

not know which input regions have larger counts with full cer-
tainty, our algorithm must be robust to such noise. In this paper, we
build on the noisy binary search approach introduced by Ben-Or
et al. [10]. To deal with the noise, the randomized noisy binary
search algorithms [10, 32] maintain a set of weights for each re-
gion representing the belief of the algorithm about how likely the
desired input is in that region. The algorithm iteratively increases
the weights in promising regions based on each oracle query and
prioritizes narrowing down these promising input regions.
Approximate Counting with Monte Carlo. We develop a Monte
Carlo algorithm for implementing the noisy counting oracle that
is needed by our randomized directed greybox fuzzing algorithm.
Monte Carlo random sampling is one of the popular approaches to
approximate counts [15, 33]. For example, to approximately count
the number of people in the world who like ice cream, Monte Carlo
methods will poll a random subset of individuals if they like ice
cream and multiply the worldâ€™s population by the ratio of people
who liked ice cream in the poll to the total number of participants.
The more people one polls, the more accurate the count is.

However, applying such techniques naively in our setting can
result in most of the approximate counts being zero even though
their true values might be small non-zero numbers. Consider a tar-
get program with multiple branch constraints guarding the target.
Suppose we wish to count the number of inputs that reach the
target in an input region with 1 million inputs and the true count
of inputs reaching the target, unknown to us, is 10. To efficiently
approximate this count, we might execute the target program on
a small number of inputs selected uniformly at random from the
input region and multiply the size of the input region by the ratio
of the number of inputs that reach the target in our executions to
the total number of tested inputs. The challenge, however, is that
unless we generate a prohibitively large number of inputs âˆ¼ 105,
we will estimate the count as zero because we are unlikely to find
the few target-reaching inputs with a small number of randomly
selected inputs.

Estimating the count of inputs reaching the target as zero can
be highly detrimental to the fuzzing algorithm. In practice, most
fuzzing target sites are only reachable by a small number of inputs
satisfying one or multiple branch constraints. Estimating the cor-
responding count for any large input region as zero will degrade
our fuzzing algorithmâ€™s performance significantly because it will
fail to identify input regions with larger counts. To overcome this
problem, we observe that even if we did not find the few inputs
that reach the target, we can still compute a upper bound on the
count with high confidence.
Concentration Bounds. We compute such probabilistic bounds
by using concentration bounds [22], a well-studied technique to
upper-bound the probability of a random variable taking a value
within a given range based on the variableâ€™s mean and variance.
The intuition behind such bounds is that the closer the mean (with
small variance) is to the range, the higher the likelihood that the
random variable will take a value within the range. For example,
if we model the branch distance [38] of a branch constraint as a
random variable and compute the mean and variance of the values
observed at the branch during the program executions with the
randomly selected inputs, we can use the concentration bounds to

â€¢ We design an asymptotically optimal randomized directed
greybox fuzzer that has logarithmic expected execution com-
plexity in the number of possible inputs. We also show that
this expected execution complexity cannot be improved, up
to constant terms, by any fuzzer.

â€¢ We develop a Monte Carlo algorithm for implementing a
noisy counting oracle that can work efficiently together with
our fuzzing algorithm.

â€¢ We implement our technique in MC2 and show its promise,
outperforming existing state-of-the-art greybox fuzzers in
challenging benchmarks (Magma and Fuzzer Test Suite) by
up to two orders-of-magnitude.

2 METHODOLOGY
We first introduce a generic complexity-theoretic framework for
reasoning about the best possible directed greybox fuzzer in terms
of an oracle-guided search problem. We next instantiate a specific
type of oracle called the noisy counting oracle and use it to build
an optimal directed greybox fuzzer using noisy binary search. We
then show how to implement a noisy counting oracle in practice.

2.1 Terminology and Notation
Below, we provide a summary of the terminology used throughout
this paper. In this paper we use the word fuzzer to refer to a directed
greybox fuzzer unless otherwise noted. We denote the target pro-
gram as ğ‘ƒ and its large but finite-sized input space to be explored
during fuzzing as I. We denote the count of elements in a set with
the cardinality | Â· | symbol and the size of the input space as ğ‘ = |I|.
Input Region. We refer to any subset ğ¼ âŠ† I as input region ğ¼ . Since
programs being fuzzed generally accept inputs as a sequence of
bytes, we express, without loss of generality, the target programâ€™s
input space as all possible combinations of byte values in the form
of a hyperrectangle: I = [0, 255]ğ‘‘ where ğ‘‘ indicates the number
of input bytes. Although ğ‘ = |I| is finite, it is exponentially large
in ğ‘‘, the number of inputs bytes to be fuzzed, which is generally
bounded in real-world fuzzers for performance reasons [1, 3].
Control Flow Graph. In the greybox setting, we have access to a
target programâ€™s control flow graph: ğ¶ğ¹ğº = (ğ‘‰ , ğ¸), where the set of
vertices ğ‘‰ represents basic blocks and the set of edges ğ¸ represents
control-flow transitions (e.g., branches). We define a path ğœ‹ as a
finite sequence of edges in the CFG ğœ‹ : ğ¸0 â†’ ğ¸1 â†’ ... â†’ ğ¸ğ‘˜ , in
which any two consecutive edges are adjacent.

Since we only care about paths that start from the program entry
point (e.g., main), we denote the set of paths in the CFG that start
from a program entry basic block and terminate at a program exit
basic block as Î . As directed fuzzers are interested in reaching some
target ğ¸ğ‘‡ âˆˆ ğ¸, we say that a path ğœ‹ âˆˆ Î  reaches a target if ğ¸ğ‘‡ âˆˆ ğœ‹
and denote the subset of paths that reach target ğ¸ğ‘‡ as Î ğ‘‡ .
Program Execution. We denote executing a program ğ‘ƒ on input
ğ‘– as ğ‘ƒ (ğ‘–) and its corresponding path through the CFG as ğœ‹ğ‘ƒ (ğ‘–) . We
also say an input ğ‘– reaches the target ğ¸ğ‘‡ if its path ğœ‹ğ‘ƒ (ğ‘–) reaches
the target ğ¸ğ‘‡ .
Branch Constraint. Each edge in our CFG corresponds to a con-
ditional branch constraint: ğ‘ : I â†’ {0, 1} that takes the following
normalized form:

Figure 1: Workflow of MC2

derive an upper bound on the likelihood of satisfying the branch
constraint.

One issue with these concentration bounds is that they might
incur large over-approximation error that can increase the noise
in the oracle, hindering the performance of the fuzzing algorithm.
Therefore, we use the concentration bounds sparingly. Specifically,
during a single oracle query with a set of randomly selected inputs,
we first observe which branches are satisfied for which inputs and
apply probabilistic upper bounds only for the branches that have
never been satisfied for any input. For the rest, we use the empiri-
cally observed non-zero ratio of the number of inputs satisfying a
branch to the total number of tested inputs.
Counting along Multiple Branches. For any given path reaching
a target, we approximately count the number of inputs that satisfy
each branch in the path as described above and combine them to
get an estimate on the count of inputs that reach the target (See
Section 2.4). To efficiently approximate counts for nested branches,
we further introduce Monte Carlo Execution, a lightweight form
of program execution that modifies control-flows at runtime to
ensure nested inner branches will be visited during the program
execution even if the input did not satisfy the outer branch con-
straints. This technique enables us to collect information about any
branch and subsequently efficiently approximate their counts with
a small number of program executions. We note that even though
our counting algorithm cannot provide guaranteed error bounds
for arbitrary programs, our experimental results demonstrate that
the method is highly effective on real-world programs.

We implement our approach in MC2 (Monte Carlo Counting)
shown in Figure 1 and evaluate MC2 against state-of-the-art di-
rected greybox fuzzers on challenging benchmarks with real-world
programs. Our results are very promising. In the Magma bench-
mark, MC2 finds bugs faster by 134x, on average, and finds 16 more
bugs than the next-best fuzzer. In the Fuzzer Test Suite benchmark,
MC2 reaches targets 77x faster, on average, and reaches 2 more
targets than the next-best fuzzer. In addition, MC2 found 49 pre-
viously undiscovered bugs in real-world programs, 15 more than
the next-best fuzzer. We release an open source version of MC2 at
https://hub.docker.com/r/abhishekshah212/mc2.
Our contributions are summarized as follows.

â€¢ We introduce a complexity-theoretic framework for defining
directed greybox fuzzing as an oracle-guided search problem
and introduce execution complexity, a metric, to measure a
fuzzing algorithmâ€™s asymptotic performance.

CorpusCFGTargetsInput  RegionsCountsGenerateRandom InputsMonte CarloExecutions ApproximateCountsNoisy Counting OracleSplit InputRegionUpdateWeightsNoisy Binary SearchTarget  Reaching  Inputs:=
:=
:=

c(i)
âŠ²âŠ³
i

Constraint
Predicate
Input

d(i) âŠ²âŠ³ 0
{==,<,<=,>,>=}
[ğ‘–1, ğ‘–2, ..., ğ‘–ğ‘‘ ]
The branch distance [38] ğ‘‘ (i) captures the effect of all program
instructions preceding the branch constraint during a program
execution on input ğ‘–.

2.2 A Framework for Directed Greybox Fuzzing
While empirical measures can compare the performances of fuzzers
with different designs, such measurements alone cannot tell us how
close the designs are to the best possible (i.e., optimal) fuzzer. To-
wards this end, in this section, we introduce a complexity theoretic
framework to reason about the lower bounds on the performance of
an optimal fuzzer in terms of the number of target program execu-
tions. We design our framework to faithfully capture the character-
istics of modern greybox fuzzers: collecting feedback information
about a program execution through lightweight instrumentation
and adapting their algorithms based on such feedback.
Fuzzing as Oracle-Guided Search. Our framework allows the
fuzzer to learn information about any bounded input region ğ¼ by
querying an oracle. Formally, the oracle ğ‘‚ : ğ¼ â†’ {0, 1}ğ‘ is any func-
tion that internally executes the program ğ‘ƒ on some pre-determined
constant number of inputs ğ‘– âˆˆ ğ¼ , since brute-forcing all inputs is
not practical, and returns arbitrary content with a large but con-
stant amount, ğ‘ bits, of information. In this context, constant means
independent of input size, as is customary in complexity-theoretic
analysis.

Specifically, we assume that each oracle query can provide at
most ğ‘ bits of information about a given input region either cover-
ing the entirety of the input space or some parts of it, where ğ‘ is
some constant, capturing the information collected from a fuzzerâ€™s
lightweight instrumentation. As we later demonstrate in Section 2.3,
a fuzzer can potentially use these ğ‘ bits to reduce the number of
inputs it considers by a multiplicative factor of 1
2ğ‘ (e.g., ğ‘ = 1 bit
cuts input space in half), which captures modern fuzzerâ€™s use of
feedback information to reduce the number of inputs by adapting
the generated inputs towards particular seeds.

We model each oracle query as providing at most ğ‘ bits of infor-
mation because it captures the trade-off made by real-world fuzzers:
lowering the amount of instrumentation for faster execution times.
Unlike symbolic/concolic execution that capture more information
by collecting all the path constraints along the execution path and
invoking a Satisfiability Modulo Theory (SMT) solver repeatedly
for a large number of paths, modern fuzzers use lightweight in-
strumentation to minimize execution overhead. Hence, the upper
bound of a constant number of bits of information is a natural fit
for these fuzzers.

To ensure it can reason about generic programs, our framework
is distribution-free: it makes no assumptions about the target pro-
gram behaviors or the types of inputs. Our framework also makes
no assumptions on the type of instrumentation or the data (e.g., dis-
tance, dataflow, etc.) collected by the fuzzers as feedback. Moreover,
our framework posits that fuzzers have no prior knowledge about
programs and only acquire information through oracle queries,
mimicking real-world fuzzing deployments that run on a large

collection of programs without pre-built knowledge of program
specifics.
Problem Definition. We define the task of directed greybox fuzzing
as an oracle-guided search problem: given access to a program ğ‘ƒ,
its control-flow graph ğ¶ğ¹ğº, a bounded input space I, a target edge
ğ¸ğ‘‡ , and an oracle ğ‘‚ : ğ¼ â†’ {0, 1}ğ‘ , a fuzzing algorithm has to find
an input ğ‘– âˆˆ I such that ğœ‹ğ‘ƒ (ğ‘–) reaches the target ğ¸ğ‘‡ .
Execution Complexity. In this framework, to measure a fuzzerâ€™s
performance, we analyze the number of oracle queries to solve
the underlying search problem. Since the number of oracle queries
directly maps to the number of program executions, up to con-
stant factors, we define the performance metric of fuzzers in our
framework as execution complexity: the number of oracle queries
needed before finding an input that reaches the target. This asymp-
totic performance metric is well-fit for real-world fuzzers because
instrumented target program executions dominate the fuzzing per-
formance overhead [42]. In our analysis, we ignore constant factors
because they significantly depend on the hardware and we desire a
hardware-independent analysis as fuzzers are run over a heteroge-
neous collection of hardware.

We can now reason about the best possible fuzzer with our frame-
work. The framework provides a lower bound on the performance
of any fuzzer (evolutionary or otherwise) including the best possible
fuzzer, a result which we prove in Appendix A.

Theorem 2.1 (Lower Bound for Any Fuzzing Algorithm).
Given any oracle revealing a constant ğ‘ bits of information per query,
any fuzzing algorithm requires Î©(log(ğ‘ )) execution complexity to
find inputs that reach the target in an input space of size ğ‘ .

In the next section, we design an asymptotically optimal fuzzer.
Greybox vs Blackbox Oracle. We highlight that this lower bound
is not achievable with a blackbox oracle, where an oracle query
outputs a boolean value indicating if the target was reached or
not for a given input. Such a blackbox fuzzing oracle, unlike the
greybox one, does not provide ğ‘ = 1 bit of information as one oracle
query decreases the number of inputs a fuzzer considers by only
1, rather than by a multiplicative factor of 1
21 . Therefore, a fuzzer
using a blackbox oracle (e.g., blackbox fuzzer) has ğ‘‚ (ğ‘ ) execution
complexity [51] to find target-reaching inputs in an input space of
size ğ‘ and is asymptotically slower than a greybox fuzzer, which
also matches empirical evidence [46].

2.3 Optimal Directed Fuzzer with Noisy

Counting Oracle

In this section, we first introduce a special kind of oracle called the
noisy counting oracle that identifies which region, among two arbi-
trary input regions, is more promising by approximately counting
the number of inputs in each region that reach the targets. Given
this oracle, we then design an asymptotically optimal fuzzing al-
gorithm that we introduce in two stages. We first describe our
algorithm in an idealized setting with a noiseless counting oracle
and then extend our algorithm to a realistic setting with a noisy
counting oracle.
Noisy Counting Oracle. We define a noisy counting oracle as
taking two arbitrary input regions, approximately counting the

number of inputs in each region that reach the targets, and returning
ğ‘ = 1 bit of information about which region contains more inputs
reaching the targets. Due to the approximate nature of the counts,
we assume that the noisy oracle returns the incorrect answer with
probability ğ‘ < 1/2. More formally, on input regions ğ¼ğ¿ and ğ¼ğ‘…, the
counting oracle computes the following formula:

ğ‘‚ (ğ¼ğ¿, ğ¼ğ‘…) =

if ğ¶ (ğ¼ğ¿) â‰¥ ğ¶ (ğ¼ğ‘…)

(cid:40)1
0 otherwise

(1)

where ğ¶ (ğ¼ğ¿), ğ¶ (ğ¼ğ‘…) denotes the count of inputs that reach the
target in the left and right input region, respectively as defined
below:

(2)

ğ¶ (ğ¼ ) = |{ğ‘– âˆˆ ğ¼ |ğœ‹ğ‘ƒ (ğ‘–) reaches the target }|
Optimal Deterministic Fuzzer. We present a deterministic fuzzing
algorithm that achieves the lower bound on execution complexity
(i.e., optimal) in Theorem 2.1 using a noiseless (ğ‘ = 0) counting
oracle introduced above. Our fuzzer leverages binary search and
splits a given input region into two input regions to use the count-
ing oracle. However, as input regions often span multiple bytes,
splitting an input region in half is ambiguous. For example, given
a 2 byte input region [0, 255] Ã— [0, 255] that can be visualized as
a square, we can split the input region in half either vertically or
horizontally.

To ensure that the splitting process is unambiguous, we assign
a total order to the input space, which conceptually flattens the
input space into an array of size ğ‘ = |I|. There are many possible
such orderings, but in this paper, we use lexicographic order (unless
otherwise specified) which flattens byte 1, then byte 2, and so forth.
In the prior example, lexicographic order conceptually flattens the
input space into the following form [(0, 0), (0, 1), ..., (0, 255), (1,0),
(1, 1), ... (255, 255)].

Our fuzzing algorithm iteratively splits an input region in half
and queries the counting oracle to pick the half with higher counts,
eventually finding inputs that reach the target. At each iteration,
this algorithm reduces the input region size by a multiplicative fac-
tor of 1
21 (i.e., ğ‘ = 1 bit of information) and therefore has ğ‘‚ (log(ğ‘ ))
execution complexity, achieving the lower bound in Theorem 2.1.
Algorithm 4 in the Appendix illustrates this process.
Optimal Randomized Fuzzer. Even though the deterministic al-
gorithm presented earlier achieves the theoretical lower bounds, it
is not practical because it depends on a noiseless counting oracle
that cannot be efficiently implemented in practice. Precisely, these
types of counting problems belong to #P, a complexity class that is
at least as hard as NP [33]. Therefore, in practice, one can only hope
to implement an oracle that will approximate the underlying counts.
Unfortunately, our prior binary search based deterministic fuzzer
design cannot work with such approximations as any error can
mislead our fuzzerâ€™s selection of the input region with the largest
count. We therefore design a new randomized fuzzing algorithm
resilient to the noise from approximation error with techniques
from the noisy binary search literature [10, 21, 32].

In noisy binary search [10, 21, 32], algorithms perform binary
search in a setting where comparisons may be unreliable, a natu-
ral fit for us because our noisy counting oracle may not reliably
compare the counts between input regions due to the approxi-
mate nature of the counts. Since there is always some source of

uncertainty, it is customary to develop randomized algorithms that
succeed with high probability and require a small number of oracle
queries in expectation (i.e., expected execution complexity), since
the exact number may change each time the algorithm runs.

We note that the expectation considers all potential behaviors
of the randomized algorithm and makes no assumption about the
input distribution, which is a natural fit for fuzzing because we
are trying to build a practical randomized fuzzing algorithm that
works well on any program. Moreover, any randomized fuzzing
algorithm with some probability of success can be re-ran multiple
times, so that its probability of failure exponentially decreases with
more trials to a negligibly small value. Such repeated runs can
also be very easily incorporated in fuzzers that run computations
repeatedly in a long-running loop.

To build our noise-resilient fuzzer, we adapt a randomized al-
gorithm proposed in prior work by Ben-Or et al. in noisy binary
search [10]. We state the expected execution complexity and opti-
mality of our algorithm below, where we provide proofs in Appen-
dix A.

Theorem 2.2 (Algorithm 1 Execution Complexity). Given
a noisy counting oracle that returns ğ‘ = 1 bit of information with
failure probability ğ‘ < 1
2 per query, our fuzzing algorithm has ğ‘‚ ((1âˆ’
log(ğ‘ )
ğ›¿) âˆ—
2 âˆ’ğ‘) 2 ) expected execution complexity to find inputs that reach
( 1
the target with success probability at least 1 âˆ’ ğ›¿.

Theorem 2.3 (Algorithm 1 Optimality). Given any oracle that
returns a constant ğ‘ bits of information with failure probability ğ‘ < 1
2
per query, any fuzzing algorithm that succeeds with probability at
log(ğ‘ )
least 1 âˆ’ ğ›¿ has Î©((1 âˆ’ ğ›¿) âˆ—
2 âˆ’ğ‘) 2 ) expected execution complexity.
( 1
We describe our fuzzer in Algorithm 1. To better understand
our fuzzing algorithmâ€™s design, we first compare it to a naive al-
gorithm that makes multiple oracle queries at each splitting point
and takes the majority result returned by those queries. Clearly,
such an algorithm will be robust to a noisy oracle. However, it has
) expected execution complexity [32], which
ğ‘‚ (
is not optimal from Theorem 2.3. By contrast, our algorithm low-
ers the expected execution complexity by adaptively selecting the
splitting point based on each oracle query, so that there are fewer
queries in total.

log(ğ‘ )âˆ—log(log(ğ‘ ))
( 1
2 âˆ’ğ‘) 2

In more detail, our algorithm decides where to split by main-
taining a set of weights which represent its belief that an input
region contains the target-reaching inputs. Starting from the be-
lief that any input region is equally likely to contain inputs that
reach the target, our fuzzing algorithm iteratively increases the
weights in promising regions based on each oracle query and pri-
oritizes splitting at points within these promising input regions.
We note that the weight update rule can be thought of as updat-
ing the Bayesian posterior [10] or using the multiplicative weights
algorithmic paradigm [7].

We note that our algorithm has two key desirable properties.
First, as the failure probability ğ‘ in the noisy counting oracle in-
creases, its performance degrades gracefully with a quadratic, not
exponential relationship. Moreover, for a given failure probability ğ‘,
it is optimal (i.e., cannot be improved upon) within constant factors
as shown in Theorem 2.3.

Algorithm 1 Optimal Randomized Fuzzer.

Input:

I â† Input Space as Array
ğ‘‚ â† Noisy Counting Oracle from Equation 1 and Algorithm 3

âŠ² Initialize a single weight group: (input region, weight)
âŠ² List of weight groups

/* Replace W[MidIdx] with two new groups */
ğ¼ğ¿, ğ¼ğ‘… = Split input region in half at W[MidIdx]
W[MidIdx] = (ğ¼ğ¿, W[MidIdx].weight)
W.insert(MidIdx+1, (ğ¼ğ‘… , W[MidIdx].weight)) âŠ² Insert group at specific index

/* Find group that best splits total weight in half */
CumulativeWeight = 0
MidIdx = 0;
for group âˆˆ W do

/* Perform Multiplicative Weights Update */
if ğ‘‚ (ğ¼ğ¿, ğ¼ğ‘… ) = 1 then

do

log(|I|)

1âˆš

break
MidIdx += 1

CumulativeWeight += group.weight
if CumulativeWeight â‰¥ 0.5 then

1: ğº = WeightGroup()
2: ğ‘Š = [G]
3: while max(W) <
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
group.weight *=
38: ğº âˆ— = arg maxğº âˆˆW (ğº .ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ )
39: return random input from ğº âˆ—â€™s input region

/* Right region is more promising */
for idx âˆˆ {0, 1, ..., MidIdx} do

/* Left region is more promising */
for idx âˆˆ {0, 1, ..., MidIdx} do

for idx âˆˆ {MidIdx+1, ..., |W| - 1} do

for idx âˆˆ {MidIdx+1, ..., |W| - 1} do

TotalWeight += group.weight

for group âˆˆ W do

W[idx] *= (1-p)

W[idx] *= (1-p)

1
TotalWeight

W[idx] *= (p)

W[idx] *= (p)

else

/* Normalize such that sum of group weights is 1 */
TotalWeight = 0
for group âˆˆ W do

âŠ² Increase left group weights by (1-p)

âŠ² Decrease right group weights by (p)

âŠ² Decrease left group weights by (p)

âŠ² Increase right group weights by (1-p)

âŠ² Select group with largest weight
âŠ² Inputs in ğº âˆ— reach the target

summation over individual path counts. We then show how we
efficiently approximate this count from individual path counts.
Monte Carlo Counting. Suppose we wish to predict the number
of votes that a political candidate will receive in some country. In-
stead of asking every person in the country if they will vote for
the candidate, Monte Carlo counting techniques efficiently approx-
imate this count by polling a small number of randomly selected
people and multiplying the number of people in the country by
the ratio of people who liked the candidate in the poll to the total
number of participants. Hence, Monte Carlo counting techniques
trade-off accuracy for efficiency (i.e., the more people polled, the
more accurate the count). In our context, a naive Monte Carlo count-
ing strategy will be to execute the program on a small number of
randomly selected inputs from the input region and multiply the in-
put region size by the ratio of inputs that reached the target during
our executions to the total number of tested inputs.

The challenge with such a strategy is that for most input regions,
the approximate count will be zero. The main problem is that the
input region size |ğ¼ |, for most real-world programs, is significantly
larger than the count ğ¶ (ğ¼ ) of inputs that reach the target in the given
input region, so the chance of reaching the target with a randomly
selected input can be very small: ( ğ¶ (ğ¼ )
|ğ¼ | âˆ¼ 256âˆ’ğ‘‘ ). To approximate
this count effectively, the naive Monte Carlo Counting strategy will
require a prohibitively large number of program executions, âˆ¼ 1
.
ğ¶ (ğ¼ )
|ğ¼ |

If the counting oracle estimates the count of inputs that reach the
target as zero for most input regions, our fuzzerâ€™s performance
significantly degrades because it will struggle to identify the input
region that contains more inputs reaching the target.
Exploiting CFG Structure for Counting. We observe that the
graph structure of the CFG enables us to decompose the count ğ¶ (ğ¼ )
of inputs that reach the target in an input region ğ¼ into a summation
of counts along any individual path ğœ‹ âˆˆ Î ğ‘‡ that reaches the target.
More formally:

ğ¶ (ğ¼ ) =

âˆ‘ï¸

ğ¶ğœ‹ (ğ¼ )

(3)

Furthermore, our algorithm achieves logarithmic storage com-
plexity ğ‘‚ (log(ğ‘ )) in terms of the size of I. A naive implementation
would store a distinct weight for each input and therefore require
an exponential amount of space ğ‘‚ (ğ‘ ). Based on the observation
that many inputs share the same weight, our algorithm groups in-
put weights together by storing the sum of their individual weights
with the corresponding input region in a weight group to avoid
redundancy. Each oracle query adds one additional weight group,
and since there are logarithmically many oracle queries, the al-
gorithm uses logarithmic amount of space: ğ‘‚ (log(ğ‘ )) groups in
expectation.

2.4 Noisy Counting Oracle through Monte

Carlo Counting

In this section, we design a noisy counting oracle by approximately
counting the number of inputs in a given input region that reach
the target. Our oracle builds upon Monte Carlo counting, so we
first introduce the intuitions behind this method and explain why
directly using it fails in our setting. We next show how we exploit
the graph structure in programs to decompose the count into a

ğœ‹ âˆˆÎ ğ‘‡
where ğ¶ğœ‹ (ğ¼ ) denote the count of inputs that reach the target along
path ğœ‹ âˆˆ Î ğ‘‡ .

Although it is not feasible to compute this summation exactly
because there can potentially be a large number of paths in a real-
world program, this observation informs the design of an efficient
approximation method: we can use information about how large
the count is for individual paths as hints for the approximate count.
In the next section, we describe how we efficiently approximate
the count of inputs that reach the target along an individual path,
for a given input region. In the subsequent section, we describe how
we efficiently approximate this summation by selecting the path
with the largest count. We show the entire approximate counting
process in Algorithm 3. Although this algorithm does not have
guaranteed error bounds for arbitrary programs, our experimental
results in Section 4 demonstrate that the method is effective on
real-world programs.

2.4.1 Efficiently Approximating Individual Path Counts. In this sec-
tion, we first describe how we use uniconstraint counts to efficiently
approximate the count of inputs that reach the target along an in-
dividual path in a given input region. We then describe two classes

of uniconstraint counts that are challenging to compute and then
how we address them.
Approximating Path Counts. We wish to compute ğ¶ğœ‹ (ğ¼ ) which
represents the count of inputs that reach the target ğ¸ğ‘‡ along path
ğœ‹ : ğ¸0 â†’ ... â†’ ğ¸ğ‘‡ â†’ ... in input region ğ¼ . We observe the set of
inputs that reach the target represents an intersection of multiple
sets: ğ¼ğ¸1 âˆ© ğ¼ğ¸2 âˆ© ...ğ¼ğ¸ğ‘‡
indicates the set of inputs that
satisfy only the single branch constraint at edge ğ¸ğ‘– in path ğœ‹. The
count of inputs in this intersection is strictly less than or equal to
the count of inputs in any individual set ğ¼ğ¸ğ‘–
because intersections
are subsets of individual sets. Therefore, we express the count of
inputs that reach the target with the following formula:

, where ğ¼ğ¸ğ‘–

ğ¶ğœ‹ (ğ¼ ) = ğ¶ (ğ¼ğ¸1 âˆ© ğ¼ğ¸2 ...ğ¼ğ¸ğ‘‡ ) â‰¤ ğ‘šğ‘–ğ‘›(ğ¶ (ğ¼ğ¸1 ), ğ¶ (ğ¼ğ¸2 ), ..., ğ¶ (ğ¼ğ¸ğ‘‡ ))

(4)

where using the minimum count allows us to put an upper bound
on the count of inputs along a path.
Uniconstraint Counts. In the above equation, ğ¶ (ğ¼ğ¸ğ‘– ) represents
the count of inputs that satisfy a single branch constraint at edge ğ¸ğ‘–
in path ğœ‹, so we call them uniconstraint counts. Hence, to compute
the count of inputs that reach the target along a path, we use the
minimum uniconstraint count.

We can efficiently approximate uniconstraint counts through
Monte Carlo counting because individual branch constraints are
likely to have a larger count of inputs that satisfy them in contrast
to an intersection of multiple branch constraints, which matches
empirical evidence from the symbolic execution literature [64].
Therefore, we are more likely to approximate them effectively with
a smaller number of program executions. Formally, we use the
following approximation formula for uniconstraint counts:

ğ¶ (ğ¼ğ¸ğ‘– ) = |ğ¼ | âˆ— ğ‘Ÿğ¸ğ‘–

(5)

where ğ‘Ÿ is the ratio of inputs that satisfy the branch constraint at
edge ğ¸ğ‘– along path ğœ‹ in our random subset to the total number of
inputs selected uniformly at random with replacement from input
region ğ¼ .
Challenges in Approximating Uniconstraint Counts. Even
though uniconstraint counts are more tractable to be approximated
with the naive Monte Carlo counting strategy, there are two classes
of uniconstraint counts that are difficult to efficiently approximate.
First, some individual branch constraints may be evaluated but not
satisfied in a small number of program executions (C1) and second,
some individual branch constraints may not be evaluated at all (e.g.,
nested branches) in a small number of program executions (C2).
For these two classes of uniconstraint counts, a naive strategy will
approximate their counts as zero and hence the minimum unicon-
straint count that is used to approximate the count of inputs along
a path, will be zero. We might choose to increase the number of
program executions to handle them, but recall from Section 2.2 that
the oracle must internally execute the program a pre-determined
constant number of times to avoid brute-forcing. Since we cannot
know a priori how many program executions are required to ef-
fectively approximate a uniconstraint count, we describe how we
address these two classes of branches below.
C1: Handling Evaluated but Unsatisifed Branches. Although
some branches might be evaluated in a small number of executions,

Algorithm 2 Monte Carlo Executions.

Input:

P â† Program
ğœ‹ â† Path reaching the target

Inputs â† Set of inputs

âŠ² Tracks branch distances across executions
âŠ² Tracks if branches satisfied

âŠ² Execute program ğ‘ƒ on input ğ‘–
âŠ² Check branch instruction

dist = GetBranchDistance(inst)
BranchDistances[inst].Add(dist)
is_satisfied = IsBranchSatisfied(inst)
BranchSatisfied[inst] += is_satisfied
inst.SetBranchDirection(ğœ‹ ) âŠ² Enforce runtime control-flows follow ğœ‹
âŠ² Handle program exceptions

âŠ² If branch satisfied 1 else 0

if inst.RaiseException() then

if inst.ReadInvalidMem() then
rand = GenerateRandom()
inst.SetDestination(rand)

for inst âˆˆ P(i) do

if IsBranch(inst) then

1: BranchDistances = HashMap()
2: BranchSatisfied = HashMap()
3: for i âˆˆ Inputs do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: ratios = []
18: for branchğ‘– âˆˆ BranchDistances do
19:
20:
21:
22:
23:
24:
25:

s = BranchSatisfied[branchğ‘– ]
if ğ‘  > 0 then
ğ‘ 
ğ‘Ÿğ¸ğ‘– =
|Inputs|

continue

else

ratios.append(ğ‘Ÿğ¸ğ‘– )

26:
27: return ratios

m = Mean(BranchDistances[branchğ‘– ])
v = Variance(BranchDistances[branchğ‘– ])
ğ‘Ÿğ¸ğ‘– = Chebyshev(m, v, branchğ‘– .predicate)

âŠ² Use Table 1
âŠ² See Equation 5 for interpretation of ğ‘Ÿ

âŠ² Go to next instruction

âŠ² Compute ğ‘Ÿ for each branch

âŠ² Branch satisfied at least once

âŠ² Probabilistic upper bound

they might never be satisfied for any tested input, so we will approx-
imate their uniconstraint count as zero (i.e., ğ‘Ÿ = 0). We overcome
this by computing a probabilistic upper bound on these branches
uniconstraint counts using a concentration bound called Cheby-
shevâ€™s inequality [48] that sets ğ‘Ÿ , the likelihood of satisfying the
branch constraint, based on the sample mean and variance of ob-
served branch distances [38] during the program executions. Such
probabilistic upper bounds are a natural fit in our setting since our
uniconstraint counts are themselves upper bounds of the counts of
all inputs taking a path.

Specifically, we model branch distance ğ‘‘ (ğ‘–) as a random variable
ğ‘‹ with mean ğœ‡ and variance ğœ to use Chebyshevâ€™s inequality. Table 1
shows Chebyshevâ€™s inequality for any form of branch constraint. It
also models logical operators AND and OR as seen by the equality
and inequality. Note that Chebyshevâ€™s inequality assumes nothing
about the distribution except that the mean and variance are finite,
which holds for programs run on finite bit-precision hardware.
Therefore, it applies for any program behavior and variable type
(floats, integers, etc).

Moreover, we can be confident in our probabilistic upper bounds
because the approximation error of the sample mean and variance
decreases exponentially fast in the number ğ‘˜ of program executions
âˆ¼ 1
ğ‘’ğ‘˜ for any random variable (i.e., Chernoffâ€“Hoeffding inequal-
ity [22, 40]), and therefore, we can derive high quality approxima-
tions with a small number of executions. In addition, the quality
of the approximation error and how likely it is to occur can be
quantified and controlled through (ğ›¿, ğœ–) bounds as shown in the
Probably Approximately Correct (P.A.C.) framework [57]. Note
that since these upper bounds can potentially result in a large over-
approximation error from the true count, we use them only for
branches that were never satisfied in our tested inputs. For the rest,

Table 1: Rules for computing an upper bound on ğ‘Ÿ from Equation 5.
We model the branch distance ğ‘‘ (ğ‘–) as a random variable ğ‘‹ with
mean ğœ‡ and variance ğœ. â„ represents the smallest positive number
for the data type of ğ‘‘ (ğ‘–) (i.e., for integers, â„ = 1).

Branch constraint Rule to compute ğ‘Ÿ
ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¤ 0) â‰¤ ğœ

ğ‘‘ (ğ‘–) â‰¤ 0

ğœ+ğœ‡2

ğ‘‘ (ğ‘–) < 0
ğ‘‘ (ğ‘–) â‰¥ 0

ğ‘‘ (ğ‘–) > 0
ğ‘‘ (ğ‘–) = 0
ğ‘‘ (ğ‘–) â‰  0

ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¤ âˆ’â„) = ğ‘ƒğ‘Ÿ (ğ‘‹ + â„ â‰¤ 0)
ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¥ 0) â‰¤ ğœ

ğœ+ğœ‡2

ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¥ â„) = ğ‘ƒğ‘Ÿ (ğ‘‹ âˆ’ â„ â‰¥ 0)
ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¥ 0 âˆ§ ğ‘‹ â‰¤ 0) = min(ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¥ 0), ğ‘ƒğ‘Ÿ (ğ‘‹ â‰¤ 0))
ğ‘ƒğ‘Ÿ (ğ‘‹ > 0 âˆ¨ ğ‘‹ < 0) = ğ‘ƒğ‘Ÿ (ğ‘‹ > 0) + ğ‘ƒğ‘Ÿ (ğ‘‹ < 0)

we use the empirically observed non-zero ratio of the number of
inputs satisfying the branch to the total number of tested inputs.
C2: Handling Unevaluated Nested Branches. In our small num-
ber of program executions, some branches may not be evaluated
at all because they are nested and since we have no information
about such unevaluated nested branches, we will approximate their
uniconstraint counts as zero. Inspired by prior work in malware
analysis [44, 58, 62], we instead design a new form of execution
called Monte Carlo Execution that ensures that any input will
visit and evaluate all nested inner branches, even if the prior outer
constraints along the way are unsatisfied. Hence, in a single execu-
tion, we will visit and evaluate all branches together and therefore
effectively approximate the uniconstraint counts for all branches,
even with a small number of program executions.

Given a path ğœ‹ âˆˆ Î ğ‘‡ consisting of a set of desired branches
reaching the target ğ‘‡ , Monte Carlo Execution modifies control-
flows at runtime to always visit these branches, irrespective of the
input. Note that Monte Carlo Execution does not necessarily
change the original execution path: if an input satisfies all branch
constraints in ğœ‹, Monte Carlo Execution behaves as an original
execution. However, it can deviate from the original execution path
if the input does not satisfy any one of these branch constraints.
Even if Monte Carlo Execution deviates from the original execu-
tion path for an input, the input goes through the same computation
as if it was a valid input. Hence, Monte Carlo Execution always
preserve the sequential ordering of computation.

To ensure that it will always visit the desired set of branches,
Monte Carlo Execution must handle program exceptions. For
example, the program can make an out of bounds memory access
if the input controls the index of an array. We handle them by
advancing the instruction pointer and if the program exception was
raised by an invalid memory read, we also replace the destination
with a uniformly random value to avoid bias in the computed values
between individual executions.

This design can increase the set of possible values for the des-
tination, but this is a natural fit since we use upper bounds for
counts. Even though this design loses dependencies across memory
reads and writes, it has low overhead in contrast to prior work that
attempts to preserve these dependencies [44, 58]. In Section 4.3
and Appendix D, we run experiments to better understand this
overhead. Algorithm 2 depicts the entire process of Monte Carlo
Execution on a set of inputs.

Algorithm 3 Noisy Counting Oracle.

Input:

ğ¼ğ¿ â† Left Input Region
ğ¼ğ‘… â† Right Input Region

if ğœ‹ âˆ‰ PathCache then

1: Counts = HashMap()
2: for ğœ‹ âˆˆ Î ğ‘‡ do
3:
4:
5:
6:

ğ¶ğœ‹ = ApproxCount(I, ğœ‹ )
Insert (ğ¶ğœ‹ , 1) into PathCache

âŠ² Initialize PathCache with count over I

âŠ² Uncertainty term on ğ‘¡ -th oracle query

âŠ² Select path with largest count

âŠ² Update count based on latest information

âŠ² Send back answer to Algorithm 1

âˆšï¸ƒ log(ğ‘¡ )
ğ‘‡ğœ‹

Lookup (ğ¶ğœ‹ ,ğ‘‡ğœ‹ ) in PathCache
Counts[ğœ‹ ] = ğ¶ğœ‹ +
7:
8: ğœ‹ = arg maxğ‘–âˆˆCounts (Counts[i])
9:
10: ğ¶ (ğ¼ğ¿), ğ¶ (ğ¼ğ‘… ) = ApproxCount(ğ¼ğ¿, ğœ‹ ), ApproxCount(ğ¼ğ‘…, ğœ‹ )
11: ğ¶ğœ‹ = max(ğ¶ (ğ¼ğ¿), ğ¶ (ğ¼ğ‘… ))
12: Update PathCache entry for ğœ‹ with (ğ¶ğœ‹ ,ğ‘‡ğœ‹ + 1)
13: if ğ¶ (ğ¼ğ¿) â‰¥ ğ¶ (ğ¼ğ‘… ) then
14:
15: else
16:
17:
18: procedure ApproxCount(ğ¼ , ğœ‹ )
19:
20:
21:

Inputs = Select k uniformly random inputs from I
ratios = MonteCarloExecutions(Program, ğœ‹ , Inputs)
return |ğ¼ | âˆ— min(ratios)

return 0

return 1

âŠ² Approximate ğ¶ğœ‹ (ğ¼ )

âŠ² Algorithm 2
âŠ² Equation 4

2.4.2 Efficiently Approximating The Summation. The method de-
scribed in the prior section only deals with a single path reaching
the target. To handle multiple paths, we need to sum each pathâ€™s
count to get a total count as mentioned in Equation 3. The challenge,
however, is that although we can efficiently approximate counts
for a single path, performing this procedure for each path at each
oracle query quickly becomes computationally intractable if there
are a large number of paths reaching the target.

Instead of computing this sum through contributions from each
individual path count at each oracle query, we approximate the sum
by only including contributions from a single path with the largest
corresponding count. We select the largest-count path as its count
best preserves the sum compared to any other single path. However,
we do not apriori know which path has the largest count, so we
initially spend some computation approximating each pathâ€™s indi-
vidual count, amortizing this cost over subsequent oracle queries.
Hence, on the fuzzerâ€™s first oracle query, we identify the path with
the largest count by approximating the count over the input space I
along each path. However, there will always be uncertainty in this
path identification process due to approximation error.

To capture the uncertainty from our approximate counts, we

âˆšï¸ƒ log(ğ‘¡ )
ğ‘‡ğœ‹

add a correction factor
shown in Algorithm 3 to also ex-
plore alternative paths a small number of times, borrowed from the
multi-armed bandit literature [55]. ğ¶ğœ‹ denotes our latest count of
inputs that reach the target along path ğœ‹ and ğ‘‡ğœ‹ denotes number
of times path ğœ‹ has been selected prior to the ğ‘¡-th oracle query.
This correction factor conceptually balances uncertainty because
as the algorithm acquires more certainty about a path ğœ‹ by select-
ing it more, thereby increasing ğ‘‡ğœ‹ , the correction factor gradually
decreases as the term is inversely proportional to ğ‘‡ğœ‹ . We keep track
of the most recent count information per path through a cache data
structure called the PathCache.

3 IMPLEMENTATION
Toolchain. We implement algorithms 1, 2, 3 in C. We use LLVM [35]
instrumentation and signal handlers to handle the branch and excep-
tion logic, respectively in Algorithm 2. We also incorporate the fork-
server optimization used in state-of-the-art fuzzers [13, 17, 43, 65].
As described in Section 2.4, the error of the sample mean and vari-
ance drops exponentially fast in the number of program executions
âˆ¼ 1
ğ‘’ğ‘˜ , so we set ğ‘˜ = 5 in Algorithm 3 such that ğ‘ = 0.01 in Al-
gorithm 1 because 1
ğ‘’5 â‰¤ 0.01. To reduce storage overheads, we
implement the PathCache as a trie which avoids duplication when
paths share edges. Moreover, we do not track ğ‘˜ branch distances
per branch in Algorithm 2, but rather compute the sample mean
and variance in a streaming setting [60], so that we only store a
constant number of values for any number of program executions ğ‘˜.
Such techniques contribute to our minimal performance overheads
in Section 4.3.
Reducing Loop Overheads. Real-world programs use loops caus-
ing the same branch to be visited many times during a program
execution. If a single branch is visited a million times per execution,
a naive implementation of Algorithm 2 will store a million branch
distances for this branch per execution. Instead, we share infor-
mation across multiple visits to a branch to reduce loop storage
overheads. Specifically, in a single Monte Carlo Execution, if a
branch is visited multiple times, the branch distance at each visit
contributes to the (streaming) mean and variance of the branch.
We also enforce control-flows at runtime across multiple visits to a
branch by attaching count information to each branch. In addition
to the techniques mentioned earlier, these techniques better help us
scale to large real-world programs and contribute to our minimal
overheads in Section 4.3.
Assigning A Total Order. In Section 2.2, we use the lexicographic
total order (i.e., flatten first byte, then second byte, and so forth) to
unambiguously split the input space. Although noisy binary search
is agnostic to the underlying total order, using lexicographic order
in real-world programs assumes that any region of the input space
is equally likely to change the counts of inputs that reach the target
(i.e., all bytes equally contribute to program behavior). However,
for many real-world programs, this assumption does not hold as
experimental evidence shows that not all bytes equally contribute
to program behaviors [8, 25, 52, 53, 61].

Therefore, instead of assigning a total order based on lexico-
graphic order, we assign an order based on the observed program
executions in the noisy counting oracle. Specifically, starting with
the set of all byte indices, the algorithm partitions the set into two
disjoint subsets of equal size, and for each subset, performs Monte
Carlo Execution on inputs generated by perturbing byte values
whose index belongs to the subset. If the program executions change
the approximate count, the algorithm recursively repeats the prior
step on the subset. Otherwise, the subset is ignored. The algorithm
repeats this process until the only sets that remain are sets with
a single byte index. We then assign a total order by prioritizing
byte indices from these remaining sets ranked by how much each
byte index increases the approximate count. We experimentally
demonstrate the effectiveness of this approach in Appendix B.

Preprocessing. Existing work in directed greybox fuzzing [13, 16,
30] pre-computes information about a program (e.g., static analysis
information or CFG distance) to better guide the directed greybox
fuzzer. In our setting, we need to pre-compute the set of all paths
that reach the target, a task where algorithms require prohibitively
expensive runtimes over large real-world CFGs [24, 50]. Moreover,
algorithms that generate a subset of paths [24] generally do not
produce paths with repeated edges, and since loops are a common
construct in real-world programs, the set of generated paths is
unlikely to be realizable in real program executions.

We instead use the initial seed corpus to bootstrap a set of paths.
Specifically, we generate a set of paths that reach the target by
executing the program on a seed close to the target while randomly
inverting the direction of branches along the corresponding exe-
cution path, keeping paths based on the program executions after
the branches were inverted (e.g., reach the target). Consequently,
we use this seedâ€™s length to set the input region size. In Appen-
dix C, we measure our preprocessing time, comparing it to that
of directed greybox fuzzers to show that our preprocessing times
are similar. We plan to explore better path generation strategies in
future work, potentially using ideas from the symbolic execution
literature [14, 19, 27, 47, 64].
Randomly Generating Inputs. We represent the input region as
a ğ‘‘-dimensional hyperrectangle encoded as ğ‘‘ intervals, where each
interval represents upper and lower bounds on input values per
dimension. Used in Algorithm 3, we select ğ‘˜ inputs uniformly at
random from the hyperrectangle by generating ğ‘‘ integers indepen-
dently and uniformly at random from each interval, repeating this
process ğ‘˜ times for ğ‘˜ inputs of length ğ‘‘. If the initial seed belongs
to a given hyperrectangle (i.e., the seedâ€™s byte values are within the
ğ‘‘ intervals), we include it as part of the ğ‘˜ inputs to better utilize
initial seed corpus information when applicable.

Note that we do not keep track of a seed corpus. Instead, we
keep track of a list of groups as shown in Algorithm 1, where each
group corresponds to a tuple: (hyperrectangle, weight) and splitting
an input region corresponds to adjusting the hyperrectangleâ€™s per-
dimension intervals. To mitigate potential error in the input region
weights if the selected path changes during the oracle queries, we
also keep track of the groups per path, which does not introduce
significant storage overhead as shown in our performance over-
heads in Section 4.3 since our algorithm uses logarithmic number
of groups in expectation with respect to the size of I.

4 EVALUATION
Our evaluation seeks to answer the following research questions.

(1) Comparison against directed greybox fuzzers: How does
MC2 compare to state-of-the-art directed greybox fuzzers?

(2) Bug Finding: Can MC2 find new real-world bugs?
(3) Performance Overhead: What is the performance over-

head of MC2?

(4) Design Choices: Are MC2â€™s design choices justified?

Compute Infrastructure. Unless otherwise noted, we ran all
experiments on a Ubuntu 18.04 workstation with a Ryzen Thread-
ripper 2970WX 24-Core CPU and 128 GB RAM.

4.1 RQ1: Fuzzers Comparison
Tested Benchmarks. To avoid any potential bias while creating
our own CVE benchmark in terms of bug class or program type,
we use the publicly available Magma benchmark [28], which was
specifically curated from a diverse set of CVEs. We also evaluate on
a subset of the Fuzzer Test Suite benchmark [2] covered by prior
work [16, 43] to enable fair comparison.
Baseline Fuzzers. Following prior works in directed greybox
fuzzing [16, 30, 36, 70], we primarily compare MC2 against other
directed greybox fuzzers like AFLGo [13]. Other directed greybox
fuzzers are either not available in any form (source or binary) [16, 36,
70] or have not made their source code public yet [30] and cannot
support our benchmarks (i.e., Magma and Fuzzer Test Suite) without
significant modifications. We also reached out to the authors of
several of these fuzzers and confirmed that their code is not available
for a release at the time of this writing, but they are working on
releasing their code soon. Therefore, we could not compare against
them on our benchmarks (i.e., Magma and Fuzzer Test Suite).

To compare against alternative designs for directed greybox
fuzzing other than AFLGo, we also evaluate MC2 against ParmeSan [43]
which supports a directed greybox fuzzer mode. We contacted the
authors of ParmeSan and followed their advice to set it up. Fur-
thermore, as ParmeSan and AFLGo build upon two significantly
different regular (i.e., undirected) fuzzers: Angora [17] and AFL [65],
respectively, we also include the results of the underlying fuzzer im-
plementations to show the improvement a directed greybox fuzzer
has over its undirected counterpart in Appendix Tables 14 and 15.
Experimental Setup. We follow the experimental setup based on
prior work [13, 16, 30, 36, 43, 70]. We assign each fuzzer a single
core and keep 20% of the cores unused to minimize interference. We
configure each directed greybox fuzzer to use the default seeds and
targets provided by the benchmarks. To avoid potential unfairness
or bias in the results arising from how different fuzzing implemen-
tations deal with multiple targets, we give fuzzers one target per
run to enable a fair comparison in line with prior work [36, 43]. We
measure the time it takes to trigger the bug target (for Magma) or
reach the target (for Fuzzer Test Suite) with a 6 hour timeout.

We pick 6 hours because it is the arithmetic mean of the times
used by Hawkeye [16] and AFLGo [13] evaluations. Since each fuzzer
includes some amount of preprocessing (e.g., distance computa-
tions), we also separately measure this time in Table 12 in Appendix
C. We run with 20 independent trials, using arithmetic mean when
reporting results. We note that our Fuzzer Test Suite experiments
were performed on a workstation running Ubuntu 18.04 using an
Xeon E5-2640 24-Core CPU with 128 GB RAM.
Magma Results. Table 2 summarizes the results as well as the
result from applying the Mann-Whitney U test between MC2 and
the tested directed greybox fuzzers. We note that although we
evaluated over the entire benchmark, not all bugs were triggered,
and therefore, for space constraints, we only list the bugs triggered
within the time budget in Table 2 following prior work [29].

MC2 finds bugs 134x faster in arithmetic mean and 38x faster in
median compared to the next best fuzzer AFLGo. Moreover, MC2â€™s
improvement is statistically significant with a significance level of
0.05 for all bugs except PNG003. MC2 was also able to find 28 bugs in

Table 2: Mean time to trigger Magma bugs for each tested fuzzer
over 20 trials. We only include the bugs that were triggered within
6 hours for space constraints. (x) refers to the speedup of MC2 rel-
ative to the tested fuzzer. (p) refers to the p-value from the Mann-
Whitney U test. Since ParmeSan crashed on php, we write N/A for it.
ğ‘‡ .ğ‘‚â˜… indicates 6 hour timeout. We highlight bugs only triggered by
MC2 in blue .

Bug ID

MC2
Time
3m15s
3m23s
1m04s
1m07s
1m01s
15s
1m36s
1m44s
1m39s
4m59s
9m33s
9m36s
8m18s
9m59s
1m36s
16s
1m39s
3m21s
1m41s
1m43s
1m37s
3m17s
3m21s
9m16s
9m43s
9m58s
9m49s
13s

PDF010
PDF016
PHP004
PHP009
PHP011
PNG003
PNG006
SSL002
SSL003
SSL009
TIF005
TIF006
TIF007
TIF012
TIF014
XML017
PDF003
PDF008
PDF011
PDF018
PDF019
PNG001
PNG007
SSL020
TIF001
TIF002
TIF009
XML009
Mean speedup
Median speedup

Time
4h02m15s
51m43s
4m09s
17m08s
15m24s
15s
ğ‘‡ .ğ‘‚â˜…
5m58s
4m30s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
1h39m40s
2h46m00s
5h49m19s
1m09s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

ParmeSan
(ğ‘¥)

AFLGo
(ğ‘¥)
74x
15x
3x
15x
15x
1x

3x
2x

(ğ‘)
<0.01
<0.01
<0.01
<0.01
<0.01
0.25
>225x <0.01
<0.01
<0.01

Time
ğ‘‡ .ğ‘‚â˜…
7m10s
N/A
N/A
N/A
1m38s
2m03s
32m27s
16m27s
>72x <0.01 4h51m19s
>37x <0.01 3h48m49s
>37x <0.01 4h03m29s
<0.01
56m40s
12x
<0.01 3h52m50s
16x
218x <0.01
<0.01
4x
>218x <0.01
>107x <0.01
>213x <0.01
>209x <0.01
>216x <0.01
>109x <0.01
>107x <0.01
>38x <0.01
>37x <0.01
>36x <0.01
>36x <0.01
>1661x <0.01

ğ‘‡ .ğ‘‚â˜…
23m15s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

2x
N/A
N/A
N/A
6x
1x
18x
9x
58x
23x
25x
6x
23x

(ğ‘)
>110x <0.01
<0.01
N/A
N/A
N/A
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
>225x <0.01
<0.01
>218x <0.01
>107x <0.01
>213x <0.01
>209x <0.01
>216x <0.01
>109x <0.01
>107x <0.01
>38x <0.01
>37x <0.01
>36x <0.01
>36x <0.01
>1661x <0.01

87x

134x
38x

144x
39x

total, 16 more than the next-best fuzzer AFLGo, which found only
12 bugs within the time budget. We note that since MC2 does not
generate inputs of different length, we also ran this experiment with
variants of AFLGo and ParmeSan that do not change the input length.
We found the results to be nearly identical (mean speedup changed
by 2%), so we did not insert the full table for space constraints.
Overall, our results show the promise of using noisy binary search
and approximate counting for directed greybox fuzzing.
Case Study. We highlight a particular bug PNG001 in Figure 2
found only by MC2. This bug is guarded by constraints and only a
single input value width=0x55555555 will cause a divide by zero
when row_factor overflows. We hypothesize that AFLGo did not
trigger this bug in the time budget because the chance of produc-
ing this specific input value through mutations is small and fuzzer
heuristics such as setting values to MAX_INT also fail. In addition,
we hypothesize that ParmeSan did not trigger this bug in the time
budget because although it uses gradient descent and taint track-
ing to narrow down the input space, it cannot effectively reason
about nested constraints (Lines 6 and 7). In contrast, MC2 was able
to successfully find this input value through noisy binary search.
Moreover, upon manual source code analysis, we found this bug can
only be triggered along an execution path that sets channels=3,
showing that MC2 was able to successfully reason across multiple
execution paths.

1 void png_check_chunk_length () {
// set based on input file
2
u32 width , height , colortype ;

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

/* constraints from libpng_read_fuzzer . cc */
if ( width < UINT_31_MAX ) {

if ( width * height < 10^8) {

u32 channels ;
switch ( colortype ) {

case PALETTE : channels = 1; break
case GRAY : channels = 2; break ;
case RGB : channels = 3; break ;
case ALPHA : channels = 4; break ;

}

u32 row_factor = width * channels + 1;
if ( row_factor == 0) {

// divide -by - zero bug target

}

}

}

21
22 }

Table 4: Tested programs in bug finding experiments.

Library

libpng
poppler
binutils
binutils
openssl
libxml2

Program

Version

libpng_read_fuzzer
pdf_fuzzer
nm -C
objdump -xD
x509
xmllint

Commit a37d483...
Commit 1d23101...
2.36
2.36
Commit 3bd5319...
Commit 07920b4...

work [16, 43] and use Undefined Behavior Sanitizer [4] to
identify bug targets. This tool often reports a large number of bug
targets, and if all are set as targets, the fuzzer effectively becomes
a coverage-guided fuzzer instead of being directed. Instead, we
randomly pick one target per function and run each fuzzer with
these same targets over a 24 hour run. We start each fuzzer with the
initial Magma corpus and a small set of valid ELF files. We report
the total number of bugs found, repeating this experiment 10 times
to minimize variability.

Figure 2: Simplified code of Magma PNG001 (CVE-2018-13785).

Table 5: Categorization of new bugs found by each fuzzer.

Table 3: Mean time to reach Fuzzer Test Suite targets for each
tested fuzzer over 20 trials. (x) refers to the speedup of MC2 rela-
tive to the tested fuzzer. (p) refers to the p-value from the Mann-
Whitney U test. ğ‘‡ .ğ‘‚â˜… indicates 6 hour timeout. We highlight targets
only reached by MC2 in blue .

Bug Type

ParmeSan

AFLGo

divide-by-zero
denial-of-service
stack/heap overflow
integer overflow

Total

0
3
10
21

34

0
4
8
17

29

MC2
1
6
13
29

49

MC2
Time Time

AFLGo
(ğ‘¥)
1x

Bug ID

1s

1s

(ğ‘) Time
0.07
ttgload.c:1710
ttinterp.c:2186 9m57s ğ‘‡ .ğ‘‚â˜… >36x <0.01
cf2intrp.c:361
jdmarker.c:659
pngrutil.c:139
pngrutil.c:3182
pngread.c:738
pngrutil.c:1393

(ğ‘)
0.07
1s
<0.01
20m
23x <0.01 ğ‘‡ .ğ‘‚â˜… >372x <0.01
<0.01
5m
0.07
1s
<0.01
1m
0.07
1s
ğ‘‡ .ğ‘‚â˜… >423x <0.01 ğ‘‡ .ğ‘‚â˜… >423x <0.01

23m
1h07m 125x <0.01
1x
0.07
<0.01
5x
0.07
1x

58s
32s
1s
28s
1s
51s

1s
2m30s
1s

9x
1x
2x
1x

ParmeSan
(ğ‘¥)
1x
2x

Mean speedup
Median speedup

77x
15x

102x
2x

Fuzzer Test Suite Results. Table 3 summarizes the results. MC2
reaches targets 102x faster in arithmetic mean and 2x faster in
median than ParmeSan and 77x faster in arithmetic mean and 15x
faster in median than AFLGo, with statistical significance on all
targets that were not reached within a few seconds. Moreover,
MC2 reaches 2 more targets compared to either ParmeSan or AFLGo.
While cross-comparisons between papers is challenging due to
stochasticity in fuzzers and hardware, our results are similar to
prior work [16, 43], giving us confidence in our experimental setup
of the tested fuzzers.

Result 1: Over the Magma benchmark, MC2 finds bugs 134x faster
in arithmetic mean and 38x faster in median compared to the next
best fuzzer AFLGo. It also finds 28 bugs in total, 16 more than the
next-best fuzzer AFLGo.

4.2 RQ2: Bug Finding
For our bug finding experiments, we evaluate over programs based
on prior work [31, 37, 39, 45, 53, 59, 63] and Magma listed in Table 4.
To find the targets for directed fuzzing, we re-use an idea from prior

In our 24 hour runs, we found previously-unknown real-world
bugs in binutils, libxml2, and libpng. Table 5 summarizes the
results in terms of bug type. Since counting the number of crashing
inputs may inflate the bug count, we take the following approach
to better compute the bug count based on prior work [8, 17, 53]. We
first use AFL-CMin to filter out duplicate crashing inputs, followed
by another deduplication procedure based on unique stack traces.
From this reduced set of inputs, we manually review the stack
traces and corresponding source code to further deduplicate these
inputs. We responsibly disclosed these bugs to the developers and
all bugs were confirmed, most of which have been fixed in the latest
versions of the programs. Our results show that MC2 finds 15 more
bugs than the next best fuzzer ParmeSan.
Result 2: MC2 finds 49 previously-unknown real world bugs, 15
more than the next best fuzzer ParmeSan.

4.3 RQ3: Performance Overhead
Since instrumented target program executions dominate the fuzzing
overhead [42], we evaluate the performance overhead of Monte
Carlo Execution relative to native (uninstrumented) execution as
well as a fuzzer-instrumented execution that tracks edge coverage
and distance (i.e., AFLGo). We run the Magma programs over the
initial seed corpus inputs and take the arithmetic mean of the
results from 10 independent trials. In addition, we measure the total
memory footprint of MC2â€™s data structures (e.g., PathCache and
weight groups in Algorithm 1) by re-running our Magma evaluation
and tracking the total memory consumed in MBs, reporting the
arithmetic mean over 10 independent trials.

Table 6: Monte Carlo Execution Overheads relative to native (unin-
strumented) and fuzzer-instrumented execution over Magma.

Table 8: Mean time to trigger the bug across various techniques to
approximate uniconstraint counts over 10 trials.

Library

libpng
libtiff
libxml2
openssl
php
poppler
sqlite3

Arithmetic mean
Median

MC2 vs Native

Runtime Memory Runtime

MC2 vs Fuzzer (AFLGo)
Memory

94%
78%
135%
117%
86%
87%
136%

105%
94%

30%
2%
37%
15%
9%
10%
7%

16%
10%

26%
16%
38%
42%
29%
25%
34%

30%
29%

4%
1%
8%
6%
4%
3%
4%

4%
4%

Table 7: MC2â€™s data structures size in MBs over Magma benchmark.

Bug ID

XML009
PNG003
XML017
PHP004
PDF011
PHP009
SSL020
TIF009
PDF008

MC2
13s
15s
16s
1m04s
1m41s
1m07s
9m16s
9m49s
3m21s

Arithmetic mean speedup
Median speedup

Rule-Of-3 Good-Turing

ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
2m17s
15m8s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

427x
107x

ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
1m54s
10m40s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

426x
107x

Data Structures Size (MBs)

Table 9: Mean time to trigger the bug across various techniques to
approximate path counts over 10 trials.

Library

libpng
libtiff
libxml2
openssl
php
poppler
sqlite3

Arithmetic mean
Median

12.1
21.8
1.6
59.7
1.6
28.1
20.8

20.9
20.8

Table 6 summarize the performance overheads of Monte Carlo
Execution. Monte Carlo Execution adds runtime overheads of
105% in arithmetic mean and 94% in median as well as memory
overheads of 16% in arithmetic mean and 10% in median relative
to native execution. Relative to a fuzzer-instrumented execution,
the overheads are smaller: runtime overheads of 30% in arithmetic
mean and 29% in median as well as memory overheads of 4% in
arithmetic mean and 4% in median. We attribute the additional
memory and runtime overheads to computing the (streaming) mean
and variance for each branch, which requires additional memory
as well as floating point arithmetic.

We also summarize the memory footprint: 20.9 MBs in arith-
metic mean and 20.8 MBs in median (< 1 GB) with full details in
Table 7. These results show that the data structures do not consume
large amounts of memory. Note that MC2, a prototype, still consis-
tently outperforms other fuzzers despite this overhead, showing
the promise of our technique. Nonetheless, we believe there are
still ways to further cut down our prototypeâ€™s overhead.

Result 3: MC2 adds 30% runtime and 4% memory overhead in
arithmetic mean relative to a fuzzerâ€™s instrumentation and 105%
runtime and 16% memory overhead in arithmetic mean relative
to native execution. In addition, MC2 data structures consume < 1
GB of memory.

4.4 RQ4: Design Choices
We conduct experiments to measure the effect of three design
choices: (i) Chebyshevâ€™s inequality for uniconstraint counts, (ii)
using the minimum uniconstraint count, and (iii) path selection.

For each design choice experiment, we run MC2 on a represen-
tative subset from the Magma benchmark, repeated 10 times. To
form a representative subset, we pick 3 bugs randomly from three
categories: bugs found within 60 seconds, bugs found more than 120

Bug ID

XML009
PNG003
XML017
PHP004
PDF011
PHP009
SSL020
TIF009
PDF008

MC2
13s
15s
16s
1m04s
1m41s
1m07s
9m16s
9m49s
3m21s

Arithmetic mean speedup
Median speedup

Multiply Uniconstraint Counts

5m59s
6m15s
1m31s
13m01s
34m31s
7m16s
1h10m26s
19m09s
56m57s

13x
12x

seconds, and bugs found between these times. Our subset includes
at least one bug from each library in Magma. Moreover, it includes
bugs that only MC2 triggers as well as other tested fuzzers trigger.
We describe each design choice experiment in more detail below.

4.4.1 Chebyshevâ€™s Inequality for Uniconstraint Counts. In this ex-
periment, we compare our Chebyshev-based technique to com-
pute probabilistic upper bounds on ğ‘Ÿ (i.e., see Equation 5 in Sec-
tion 2.4) against alternate techniques when ğ‘Ÿ = 0 (i.e., zero uni-
constraint counts). Specifically, we compare against the Rule-of-
3 and Good-Turing techniques from the Natural Language Pro-
cessing and Biostatistics literature [18, 34], which have also been
used in prior work in fuzzing [11, 68]. In contrast to our proba-
bilistic upper bounds which use mean and variance information,
these methods upper bound ğ‘Ÿ by computing ğ‘Ÿ = 3
ğ‘ (Rule-of-3)
or the smallest non-zero ğ‘Ÿ across all branches (Good-Turing) via
such that ğ‘Ÿğ¸ğ‘– â‰  0}).
ğ‘Ÿ = ğ‘šğ‘–ğ‘›({ğ‘Ÿğ¸1, ğ‘Ÿğ¸2, ..., ğ‘Ÿğ¸ğ‘‡
Table 8 summarizes the results. MC2 improves upon the next-best
technique Good-Turing by 426x in arithmetic mean and 107x in
median. Our results highlight the importance of probabilistic upper
bounds in MC2.

4.4.2 Minimum Uniconstraint Count. In Section 2.4, we placed an
upper bound on the count of inputs that reach the target along
an execution path for a given input region using the minimum
uniconstraint count. In this experiment, we compare our technique
which uses information from a single uniconstraint count with an
alternate one that incorporates information from all uniconstraint
counts by multiplying them.

Table 9 summarizes the results. MC2 improves upon the multiply
uniconstraint counts technique by 13x in arithmetic mean and 12x

Table 10: Mean time to trigger the bug across various techniques
for path selection over 10 trials.

Bug ID

XML009
PNG003
XML017
PHP004
PDF011
PHP009
SSL020
TIF009
PDF008

MC2
13s
15s
16s
1m04s
1m41s
1m07s
9m16s
9m49s
3m21s

Arithmetic mean speedup
Median speedup

Epsilon-greedy

Greedy

9s
38s
11s
2m08s
2m06s
2m14s
14m50s
15m42s
5m22s

1.5x
1.6x

11s
23s
13s
3m12s
1m03s
3m21s
4h38m
4h54m30s
1h40m30s

11x
3x

in median, showing the utility of approximating the count along an
execution path using the minimum uniconstraint count. We hypoth-
esize this improvement occurs because multiplying uniconstraint
counts to approximate the count along a path corresponds to an
independence assumption between individual constraints (i.e., the
branch constraints share no variables and hence the counts are inde-
pendent), which is generally not true for most real-world programs,
as shown in the symbolic execution literature [14, 19, 26, 27, 56, 64].

4.4.3 Path Selection. We discuss in Section 2.4 the importance of
selecting alternate paths with large counts due to approximation
error, leading us to use the uncertainty term from the multi-armed
bandit literature [55] in Algorithm 3. In this experiment, we com-
pare against alternate strategies based on the multi-armed bandit
literature. We compare against a strategy that sets the uncertainty
term to zero and greedily picks the path with the largest count
(Greedy). We also compare against a variant called Epsilon-greedy
that also sets the uncertainty term to zero but instead of following
Greedy all the time, it randomly selects another path based on a
coin flip with bias ğœ–, set to ğœ– = 0.5 to equally balance the trade-off.
Table 10 summarizes the results. While MC2 improves upon Greedy
by 11x on average and 3x in median, it only improves upon Epsilon-
greedy by 1.5x on average and 1.6x in median. Our results show
the utility of selecting alternate paths to reflect our uncertainty,
but also indicate that simple strategies such as Epsilon-greedy can
work as well as more advanced ones that incorporate an uncertainty
correction factor.

Result 4: Our experimental results justify MC2â€™s design choices
with speedups â‰¥ 1.5ğ‘¥ in arithmetic mean and â‰¥ 1.6ğ‘¥ in median.

5 RELATED WORK
Approximate Counting. Approximate counting has been used
in many different contexts including counting the number of solu-
tions to SAT formulas [15, 33], flash memory [20], and database sys-
tems [9]. Techniques for approximate counting build upon Monte
Carlo counting as well as universal hash functions [15], which
provide the property of uniformly partitioning each object to be
counted into roughly equally-sized groups. We plan to investigate
incorporating such techniques in the future.

Recently, approximate counting was also used in seed scheduling
for coverage-guided fuzzing. She et al. approximate the count of

reachable and feasible edges using graph centrality [54]. In con-
trast, we approximate the count of inputs that reach the target
using Monte Carlo counting for directed greybox fuzzing. Gener-
alizing MC2 from directed greybox fuzzing to the coverage-guided
fuzzer setting remains an open question for future work and poten-
tially may involve information entropy from BÃ¶hme et al. [12] or
abstraction functions from Salls et al. [49].
Directed Greybox Fuzzing. Starting with the promising results
of AFLGo: finding the HeartBleed vulnerability orders-of-magnitude
faster than a directed whitebox fuzzer [13], directed greybox fuzzing
has seen multiple research directions. One line of work incorpo-
rates additional information into the distance computations such
as branch distance [36] or function similarity [16]. In contrast, MC2
uses noisy binary search and approximate counts, not distance, to
guide the fuzzer.

Based on the observation that directed greybox fuzzers consume
a lot of time on executions that fail to reach the target, another
promising line of work seeks to increase the fuzzerâ€™s efficiency
by not executing on inputs that are either unlikely to reach the
target [70] or provably cannot [30]. Our approach is complementary
to such techniques as we can potentially use them to bias our
random input selection process to avoid such inputs. Recent work
has also directed a fuzzer with application-specific techniques [41,
43, 67, 69] and incorporating such application-specific techniques
is an interesting question for future work.

6 CONCLUSION
In this paper, we build an asymptotically optimal directed greybox
fuzzer using noisy binary search and a noisy counting oracle. We
also empirically show the promise of our fuzzer as it outperforms
existing directed greybox fuzzers by up to two orders of magnitude,
on average, over Magma and Fuzzer Test Suite.

ACKNOWLEDGEMENTS
We thank Clayton Sanford, Samuel Deng, Andreas Kellas, Amol
Pasarkar, Dennis Roelke, Gabriel Ryan, Zhongtian Chen, Yuhao
Li, Ming Yuan, Christian Kroer, and Junfeng Yang for their helpful
comments, and the reviewers for their valuable feedback. Peter
Coffman helped create tables, improve code quality, and optimize
the implementation. Abhishek Shah is supported by an NSF Grad-
uate Fellowship. This work is supported partially by NSF grants
CNS-18-42456, CNS-18-01426; a NSF CAREER award; a Google Fac-
ulty Fellowship; a JP Morgan Faculty Fellowship; a Capital One
Research Grant; and an Institute of Information & Communications
Technology Planning & Evaluation (IITP) grant funded by the Ko-
rea Government (MSIT) (No.2020-0-00153). Any opinions, findings,
conclusions, or recommendations expressed herein are those of the
authors, and do not necessarily reflect those of the US Government,
NSF, Google, Capital One, J.P. Morgan, or the Korean Government.

REFERENCES
[1] 2022.

AFLGo Max Input Size.

https://github.com/aflgo/aflgo/blob/

b170fad54396f376160befd87adbba28b27c15d9/config.h#L142.

[2] 2022. Fuzzer Test Suite. https://github.com/google/fuzzer-test-suite.
[3] 2022. ParmeSan Max Input Size. https://github.com/vusec/parmesan/blob/

fac580130146c07a2a0f82a24dfe0704e1851ab3/common/src/config.rs#L12.

[4] 2022.

Undefined Behavior Sanitizer.

https://clang.llvm.org/docs/

UndefinedBehaviorSanitizer.html.

[5] Scott Aaronson, Andris Ambainis, Andrej Bogdanov, Krishnamoorthy Dinesh,
and Cheung Tsun Ming. 2021. On Quantum Versus Classical Query Complexity.
Electron. Colloquium Comput. Complex. (2021), 115.

[6] Andris Ambainis. 2018. Understanding Quantum Algorithms via Query Complex-
ity. In International Congress of Mathematicians: Rio de Janeiro. World Scientific,
3265â€“3285.

[7] Sanjeev Arora, Elad Hazan, and Satyen Kale. 2012. The Multiplicative Weights
Update Method: a Meta-Algorithm and Applications. Theory of Computing (2012),
121â€“164.

[8] Cornelius Aschermann, Sergej Schumilo, Tim Blazytko, Robert Gawlik, and
Thorsten Holz. 2019. REDQUEEN: Fuzzing with Input-to-State Correspondence.
In 26th Annual Network and Distributed System Security Symposium. The Internet
Society.

[9] Morton M. Astrahan, Mario Schkolnick, and Kyu-Young Whang. 1987. Approxi-
mating the Number of Unique Values of an Attribute Without Sorting. Inf. Syst.
12, 1 (1987), 11â€“15.

[10] Michael Ben-Or and Avinatan Hassidim. 2008. The Bayesian Learner is Optimal
for Noisy Binary Search (and Pretty Good for Quantum as Well). In 49th Annual
IEEE Symposium on Foundations of Computer Science. IEEE Computer Society,
221â€“230.

[11] Marcel BÃ¶hme, Danushka Liyanage, and Valentin WÃ¼stholz. 2021. Estimating
Residual Risk in Greybox Fuzzing. In 29th ACM Joint European Software Engi-
neering Conference and Symposium on the Foundations of Software Engineering.
ACM, 230â€“241.

[12] Marcel BÃ¶hme, Valentin J. M. ManÃ¨s, and Sang Kil Cha. 2020. Boosting fuzzer
efficiency: an information theoretic perspective. In 28th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering. ACM, 678â€“689.

[13] Marcel BÃ¶hme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoud-
hury. 2017. Directed Greybox Fuzzing. In ACM SIGSAC Conference on Computer
and Communications Security. ACM, 2329â€“2344.

[14] Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
In 8th USENIX Symposium on Operating Systems Design and Implementation.
USENIX Association, 209â€“224.

[15] Supratik Chakraborty, Kuldeep S. Meel, and Moshe Y. Vardi. 2013. A Scalable
Approximate Model Counter. In Principles and Practice of Constraint Programming
- 19th International Conference (Lecture Notes in Computer Science, Vol. 8124).
Springer, 200â€“216.

[16] Hongxu Chen, Yinxing Xue, Yuekang Li, Bihuan Chen, Xiaofei Xie, Xiuheng
Wu, and Yang Liu. 2018. Hawkeye: Towards a Desired Directed Grey-box Fuzzer.
In ACM SIGSAC Conference on Computer and Communications Security. ACM,
2095â€“2108.

[17] Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search.

IEEE Symposium on Security and Privacy (2018), 711â€“725.

[18] Stanley F. Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing
Techniques for Language Modeling. In 34th Annual Meeting of the Association for
Computational Linguistics. Morgan Kaufmann Publishers / ACL, 310â€“318.
[19] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang,
Tao Wei, and Long Lu. 2020. SAVIOR: Towards Bug-Driven Hybrid Testing. In
IEEE Symposium on Security and Privacy. IEEE, 1580â€“1596.

[20] Jacek Cichon and Wojciech Macyna. 2011. Approximate Counters for Flash Mem-
ory. In 17th IEEE International Conference on Embedded and Real-Time Computing
Systems and Applications. IEEE Computer Society, 185â€“189.

[21] Dariusz Dereniowski, Aleksander Lukasiewicz, and Przemyslaw Uznanski. 2021.
An Efficient Noisy Binary Search in Graphs via Median Approximation. In Com-
binatorial Algorithms - 32nd International Workshop (Lecture Notes in Computer
Science, Vol. 12757). Springer, 265â€“281.

[22] Devdatt P. Dubhashi and Alessandro Panconesi. 2009. Concentration of Measure
for the Analysis of Randomized Algorithms. Cambridge University Press.
[23] Philippe Flajolet. 1985. Approximate Counting: A Detailed Analysis. BIT 25, 1

(1985), 113â€“134.

[24] Luigi Fratta and Ugo Montanari. 1975. A Vertex Elimination Algorithm for
Enumerating all Simple Paths in a Graph. Networks 5, 2 (1975), 151â€“177.
[25] Shuitao Gan, Chao Zhang, Peng Chen, Bodong Zhao, Xiaojun Qin, Dong Wu, and
Zuoning Chen. 2020. GREYONE: Data Flow Sensitive Fuzzing. In 29th USENIX
Security Symposium. USENIX Association, 2577â€“2594.

[26] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Au-
tomated Random Testing. ACM SIGPLAN 2005 Conference on Programming
Language Design and Implementation (2005), 213â€“223.

[27] Patrice Godefroid, Michael Y. Levin, and David A. Molnar. 2008. Automated
Whitebox Fuzz Testing. In Network and Distributed System Security Symposium.
The Internet Society.

[28] Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. 2020. Magma: A Ground-
Truth Fuzzing Benchmark. ACM Measurement and Analysis of Computing Systems
(2020), 49:1â€“49:29. Issue 3.

[29] Adrian Herrera, Hendra Gunadi, Shane Magrath, Michael Norrish, Mathias Payer,
and Antony L. Hosking. 2021. Seed Selection for Successful Fuzzing. In 30th ACM

SIGSOFT International Symposium on Software Testing and Analysis. Association
for Computing Machinery.

[30] Heqing Huang, Yiyuan Guo, Qingkai Shi, Peisen Yao, Rongxin Wu, and Charles
Zhang. 2022. Beacon: Directed Grey-Box Fuzzing with Provable Path Pruning. In
IEEE Symposium on Security and Privacy.

[31] Jinho Jung, Hong Hu, David Solodukhin, Daniel Pagan, Kyu Hyung Lee, and
Taesoo Kim. 2019. Fuzzification: Anti-Fuzzing Techniques. In Proceedings of the
28th USENIX Security Symposium.

[32] Richard M. Karp and Robert Kleinberg. 2007. Noisy Binary Search and its Ap-
plications. In Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms.
SIAM, 881â€“890.

[33] Richard M. Karp, Michael Luby, and Neal Madras. 1989. Monte-Carlo Approxima-
tion Algorithms for Enumeration Problems. J. Algorithms 10, 3 (1989), 429â€“448.
[34] Panagiotis I. Koukos and Nicholas M. Glykos. 2014. On the Application of Good-
Turing Statistics to Quantify Convergence of Biomolecular Simulations. J. Chem.
Inf. Model. 54, 1 (2014), 209â€“217.

[35] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In International Symposium on
Code Generation and Optimization: Feedback-directed and Runtime Optimization
(CGO â€™04). IEEE Computer Society, 75â€“.

[36] Gwangmu Lee, Woochul Shim, and Byoungyoung Lee. 2021. Constraint-guided
Directed Greybox Fuzzing. In 30th USENIX Security Symposium. USENIX Associ-
ation, 3559â€“3576.

[37] Caroline Lemieux and Koushik Sen. 2018. Fairfuzz: Targeting Rare Branches to
Rapidly Increase Greybox Fuzz Testing Coverage. In 33rd IEEE/ACM International
Conference on Automated Software Engineering. ACM.

[38] Yun Lin, Jun Sun, Gordon Fraser, Ziheng Xiu, Ting Liu, and Jin Song Dong. 2020.
Recovering Fitness Gradients for Interprocedural Boolean Flags in Search-based
Testing. In 29th International Symposium on Software Testing and Analysis.
[39] Chenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, Yu Song, and
Raheem Beyah. 2019. MOPT: Optimized Mutation Scheduling for Fuzzers. In
28th USENIX Security Symposium. USENIX Association, 1949â€“1966.

[40] Andreas Maurer and Massimiliano Pontil. 2009. Empirical Bernstein Bounds and
Sample-Variance Penalization. In The 22nd Conference on Learning Theory.
[41] Ruijie Meng, Zhen Dong, Jialin Li, Ivan Beschastnikh, and Abhik Roychoud-
hury. 2022. Finding Counterexamples of Temporal Logic Properties in Software
Implementations via Greybox Fuzzing. In International Conference on Software
Engineering.

[42] Stefan Nagy and Matthew Hicks. 2019. Full-Speed Fuzzing: Reducing Fuzzing
Overhead through Coverage-Guided Tracing. In IEEE Symposium on Security and
Privacy. IEEE, 787â€“802.

[43] Sebastian Ã–sterlund, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida. 2020.
ParmeSan: Sanitizer-guided Greybox Fuzzing. In 29th USENIX Security Sympo-
sium. USENIX Association, 2289â€“2306.

[44] Fei Peng, Zhui Deng, Xiangyu Zhang, Dongyan Xu, Zhiqiang Lin, and Zhendong
Su. 2014. X-Force: Force-Executing Binary Programs for Security Applications.
In 23rd USENIX Security Symposium. USENIX Association, 829â€“844.

[45] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. 2018. T-Fuzz: Fuzzing by
Program Transformation. In IEEE Symposium on Security and Privacy. IEEE
Computer Society, 697â€“710.

[46] Van-Thuan Pham, Marcel BÃ¶hme, Andrew E. Santosa, Alexandru Razvan Caci-
ulescu, and Abhik Roychoudhury. 2021. Smart Greybox Fuzzing. IEEE Trans.
Software Eng. 47, 9 (2021), 1980â€“1997.

[47] Sebastian Poeplau and AurÃ©lien Francillon. 2020. Symbolic Execution with
SymCC: Donâ€™t Interpret, Compile!. In 29th USENIX Security Symposium. USENIX
Association, 181â€“198.

[48] Napat Rujeerapaiboon, Daniel Kuhn, and Wolfram Wiesemann. 2018. Chebyshev
Inequalities for Products of Random Variables. Math. Oper. Res. 43, 3 (2018),
887â€“918.

[49] Christopher Salls, Aravind Machiry, Adam DoupÃ©, Yan Shoshitaishvili, Christo-
pher Kruegel, and Giovanni Vigna. 2020. Exploring Abstraction Functions in
Fuzzing. In Proceedings of the IEEE Conference on Communications and Network
Security (CNS).

[50] Robert Sedgewick. 2002. Algorithms in C - Part 5: Graph Algorithms. Addison-

Wesley-Longman.

[51] C.A. Shaffer. 2012. Data Structures and Algorithm Analysis in C++, Third Edition.

Dover Publications.

[52] Dongdong She, Yizheng Chen, Abhishek Shah, Baishakhi Ray, and Suman Jana.
2020. Neutaint: Efficient Dynamic Taint Analysis with Neural Networks. In IEEE
Symposium on Security and Privacy. IEEE, 1527â€“1543.

[53] Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman
Jana. 2019. NEUZZ: Efficient Fuzzing with Neural Program Smoothing. In IEEE
Symposium on Security and Privacy. IEEE, 803â€“817.

[54] Dongdong She, Abhishek Shah, and Suman Jana. 2022. Effective Seed Scheduling
for Fuzzing with Graph Centrality Analysis. In IEEE Symposium on Security and
Privacy.

[55] Aleksandrs Slivkins. 2019. Introduction to Multi-Armed Bandits. Found. Trends

Mach. Learn. 12, 1-2 (2019), 1â€“286.

[56] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
23rd Annual Network and Distributed System Security Symposium. The Internet
Society.

[57] Leslie G. Valiant. 1984. A Theory of the Learnable. In 16th Annual ACM Symposium

on Theory of Computing,. ACM, 436â€“445.

[58] Xiaolei Wang, Yuexiang Yang, and Sencun Zhu. 2019. Automated Hybrid Analysis
of Android Malware through Augmenting Fuzzing with Forced Execution. IEEE
Trans. Mob. Comput. 18, 12 (2019), 2768â€“2782.

[59] Yanhao Wang, Xiangkun Jia, Yuwei Liu, Kyle Zeng, Tiffany Bao, Dinghao Wu,
and Purui Su. 2020. Not All Coverage Measurements Are Equal: Fuzzing by
Coverage Accounting for Input Prioritization. In Network and Distributed System
Security Symposium.

[60] B. P. Welford. 1962. Note on a Method for Calculating Corrected Sums of Squares

and Products. Technometrics 4, 3 (1962), 419â€“420.

[61] Wei You, Xuwei Liu, Shiqing Ma, David Mitchel Perry, Xiangyu Zhang, and
Bin Liang. 2019. SLF: Fuzzing Without Valid Seed Inputs. In 41st International
Conference on Software Engineering. IEEE / ACM, 712â€“723.

[62] Wei You, Zhuo Zhang, Yonghwi Kwon, Yousra Aafer, Fei Peng, Yu Shi, Carson
Harmon, and Xiangyu Zhang. 2020. PMP: Cost-effective Forced Execution with
Probabilistic Memory Pre-planning. In IEEE Symposium on Security and Privacy.
IEEE, 1121â€“1138.

[63] Tai Yue, Pengfei Wang, Yong Tang, Enze Wang, Bo Yu, Kai Lu, and Xu Zhou.
2020. EcoFuzz: Adaptive Energy-Saving Greybox Fuzzing as a Variant of the
Adversarial Multi-Armed Bandit. In 29th USENIX Security Symposium. USENIX
Association, 2307â€“2324.

[64] Insu Yun, Sangho Lee, Meng Xu, Yeongjin Jang, and Taesoo Kim. 2018. QSYM: A
Practical Concolic Execution Engine Tailored for Hybrid Fuzzing. In 27th USENIX
Security Symposium. USENIX Association, 745â€“761.

[65] MichaÅ‚ Zalewski. 2022. American Fuzzy Lop (AFL) README.

http://

lcamtuf.coredump.cx/afl/README.txt.

[66] Andreas Zeller, Rahul Gopinath, Marcel BÃ¶hme, Gordon Fraser, and Christian

Holler. 2022. The fuzzing book.

[67] Lei Zhang, Keke Lian, Haoyu Xiao, Zhibo Zhang, Peng Liu, Yuan Zhang, Min Yang,
and Haixin Duan. 2022. Exploit the Last Straw That Breaks Android Systems. In
IEEE Symposium on Security and Privacy.

[68] Lei Zhao, Yue Duan, Heng Yin, and Jifeng Xuan. 2019. Send Hardest Problems
My Way: Probabilistic Path Prioritization for Hybrid Fuzzing. In 26th Annual
Network and Distributed System Security Symposium. The Internet Society.
[69] Xiaogang Zhu and Marcel BÃ¶hme. 2021. Regression Greybox Fuzzing. In ACM
SIGSAC Conference on Computer and Communications Security. ACM, 2169â€“2182.
[70] Peiyuan Zong, Tao Lv, Dawei Wang, Zizhuang Deng, Ruigang Liang, and Kai
Chen. 2020. FuzzGuard: Filtering out Unreachable Inputs in Directed Grey-box
Fuzzing through Deep Learning. In 29th USENIX Security Symposium. USENIX
Association, 2255â€“2269.

A PROOFS

Proof of Theorem 2.1. Information theory [23] states that to
identify (i.e., encode) a unique element in a set containing ğ‘ el-
ements, we require at least log(ğ‘ ) bits. Similarly, to identify a
target-reaching input in an input space of size ğ‘ , any directed
fuzzing algorithm requires at least log(ğ‘ ) oracle queries, up to
constant factors, since each oracle query provides a constant ğ‘ bits
â–¡
of information.

Proof of Theorem 2.2. Ben-Or et al. [10] in Theorem 2.1 prove
log(ğ‘ )
that their noisy binary search algorithm requires (1 âˆ’ ğ›¿) âˆ—
( 1
2 âˆ’ğ‘) 2
comparisons in expectation to find the target with success probabil-
ity at least 1 âˆ’ ğ›¿. If we map our noisy oracle queries to their noisy
comparisons and our input space with a lexicographic total order to
their array, our fuzzing algorithm in Algorithm 1 directly translates
to their noisy binary search algorithm and therefore inherits the
same analysis. The algorithm analysis uses information entropy
arguments to show that the expected information gain increases at
each query, followed by concentration bounds to show that when
the algorithm terminates, the algorithm can identify the region
containing the target with high probability. For more details, see
Section 2.3 in [10].

â–¡

Proof of Theorem 2.3. Theorem 2.8 by Ben-Or et al. [10] shows
log(ğ‘ )
that Î©((1âˆ’ğ›¿)âˆ—
2 âˆ’ğ‘) 2 ) is a lower bound for any noisy binary search
( 1
algorithm (i.e., cannot be improved upon) and therefore implies
that Algorithm 1 is optimal. This lower bound results from a re-
duction to a well-studied problem in information theory where
two parties wish to communicate over a noisy channel (i.e., noisy
channel coding problem). For exact details, we refer the reader to
Section 2.4 in [10]. We note in the special case of a noisy counting
oracle that returns ğ‘ = 1 bit of information without noise (ğ‘ = 0),
Algorithm 1 also meets the lower bound, up to constant factors,
from Theorem 2.1, so it is optimal in both noisy and noiseless
â–¡
settings.

B TOTAL ORDER ASSIGNMENT
In Section 3, we discussed that although our binary search algorithm
is agnostic to the underlying total order, which is necessary to
ensure splitting an input region is unambiguous, lexicographic
order is a poor choice because it assumes that all bytes equally
contribute to the count of inputs reaching the target. Empirical
evidence from prior work [8, 25, 52, 53, 61] has shown that not all
input bytes equally contribute to program behaviors and therefore
such an assumption does not hold for many real-world programs.
In this experiment, we show that lexicographic order is poor choice
by comparing it with our technique to assign a total order.

Table 11 summarizes the results. MC2 outperforms the lexico-
graphic ordering by 210x in arithmetic mean and 54x in median,
showing that lexicographic ordering is a poor choice. In the future,
we plan to investigate what properties constitute an optimal total
order assignment.

C PREPROCESSING TIMES
Existing directed greybox fuzzers use preprocessing to better iden-
tify which inputs are more likely to reach the target as described in
Section 3. We measure the preprocessing times of the tested fuzzers
to see how they compare.

For AFLGo, we measure the time it takes to compute distance over
the control-flow graph, which consists of visiting every function,
computing intra-function distances, and using callgraphs to com-
pute distances between functions. For ParmeSan, it uses a dynamic

Table 11: Mean time to trigger the bug with and without lexico-
graphic order across 10 trials.

Bug ID

XML009
PNG003
XML017
PHP004
PDF011
PHP009
SSL020
TIF009
PDF008

MC2
13s
15s
16s
1m04s
1m41s
1m07s
9m16s
9m49s
3m21s

Arithmetic mean speedup
Median speedup

Lexicographic Order

8m14s
13m27s
4m12s
4h12m21s
ğ‘‡ .ğ‘‚â˜…
3h30m17s
20m18s
16m23s
ğ‘‡ .ğ‘‚â˜…

210x
54x

Algorithm 4 Optimal Deterministic Fuzzer.
I â† Input Space as Array
ğ‘‚ â† Noiseless (ğ‘ = 0) Counting Oracle from Equation 1

Input:

Table 12: Mean preprocessing times over 10 trials for the Magma
and Fuzzer Test Suite benchmarks.

ğ‘š = âŒŠ (ğ‘™ + ğ‘Ÿ )/2âŒ‹
ğ¼ğ¿, ğ¼ğ‘… = left and right input regions of index m
if ğ‘‚ (ğ¼ğ¿, ğ¼ğ‘… ) = 1 then

1: ğ‘™ = 0; ğ‘Ÿ = |I| âˆ’ 1
2: while l < r do
3:
4:
5:
6:
7:
8:

ğ‘Ÿ = ğ‘š

ğ‘™ = ğ‘š + 1

else

âŠ² initialize left and right bounds of I

âŠ² select midpoint input

âŠ² select left subregion

âŠ² select right subregion

Table 14: Mean time to trigger Magma bugs for each tested fuzzerâ€™s
undirected counterpart over 20 trials. Since Angora crashed on php,
we write N/A for it. See Table 2 for the full caption.

Bug ID

MC2
Time
3m15s
3m23s
1m04s
1m07s
1m01s
15s
1m36s
1m44s
1m39s
4m59s
9m33s
9m36s
8m18s
9m59s
1m36s
16s
1m39s
3m21s
1m41s
1m43s
1m37s
3m17s
3m21s
9m16s
9m43s
9m58s
9m49s
13s

PDF010
PDF016
PHP004
PHP009
PHP011
PNG003
PNG006
SSL002
SSL003
SSL009
TIF005
TIF006
TIF007
TIF012
TIF014
XML017
PDF003
PDF008
PDF011
PDF018
PDF019
PNG001
PNG007
SSL020
TIF001
TIF002
TIF009
XML009
Mean speedup
Median speedup

Time
4m25s
4m02s
2m39s
3m38s
2m29s
15s
ğ‘‡ .ğ‘‚â˜…
4m06s
2m50s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
5h21m19s
1h13m28s
1h54m37s
4h55m49s
1m23s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

AFL

(ğ‘¥)
1x
1x
2x
3x
2x
1x

2x
2x
>72x
>38x
33x
9x
11x
185x
5x

Time
ğ‘‡ .ğ‘‚â˜…
12m
N/A
N/A
N/A
59s
2m42s
55m53s
20m40s

(ğ‘)
0.09
0.11
<0.01
<0.01
<0.01
0.25
>225x <0.01
<0.01
<0.01
<0.01 4h56m06s
<0.01 3h57m57s
<0.01 4h40m09s
0.04 1h31m48s
<0.01 4h51m22s
<0.01 5h38m17s
<0.01
>218x <0.01
>107x <0.01
>214x <0.01
>210x <0.01
>223x <0.01
>110x <0.01
>107x <0.01
>39x
<0.01
>37x
<0.01
>36x
<0.01
>37x
<0.01
>1662x <0.01

1m57s
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…
ğ‘‡ .ğ‘‚â˜…

Angora
(ğ‘¥)

4x
N/A
N/A
N/A
4x
2x
32x
13x
59x
25x
29x
11x
29x
211x
7x

(ğ‘)
>111x <0.01
<0.01
N/A
N/A
N/A
<0.01
0.03
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
>218x <0.01
>107x <0.01
>214x <0.01
>210x <0.01
>223x <0.01
>110x <0.01
>107x <0.01
>39x
<0.01
>37x
<0.01
>36x
<0.01
>37x
<0.01
>1662x <0.01

128x
37x

142x
37x

Table 15: Mean time to reach Fuzzer Test Suite targets for each
tested fuzzerâ€™s undirected counterpart over 20 trials. See Table 3 for
the full caption.

MC2
Time Time

AFL
(ğ‘¥)
1x

Angora
(ğ‘¥)
1x
2x

Bug ID

1s

(ğ‘) Time
(ğ‘)
0.07
1s
0.07
ttgload.c:1710
ttinterp.c:2186 9m57s ğ‘‡ .ğ‘‚â˜… >36x <0.01
<0.01
40m 41x <0.01 ğ‘‡ .ğ‘‚â˜… >372x <0.01
58s
cf2intrp.c:361
32s 1h10m 131x <0.01 1h15m 141x <0.01
jdmarker.c:659
0.07
1s
0.07
pngrutil.c:139
<0.01
28s 3m20s
pngrutil.c:3182
1s
0.07
0.07
pngread.c:738
51s ğ‘‡ .ğ‘‚â˜… >424x <0.01 ğ‘‡ .ğ‘‚â˜… >424x <0.01
pngrutil.c:1393

<0.01 1m22s

1s
24m

1x
3x
1x

1x
7x
1x

1s

1s

1s

1s

Mean speedup
Median speedup

80x
22x

118x
3x

Library

libpng (Magma)
libtiff (Magma)
libxml2 (Magma)
openssl (Magma)
php (Magma)
poppler (Magma)
libjpeg (FTS)
libpng (FTS)
freetype2 (FTS)

Arithmetic mean
Median

MC2
54s
55s
2m41s
5m11s
3m21s
3m27s
7s
38s
1m15s

2m04s
1m15s

AFLGo

ParmeSan

CFG Nodes

1m52s
10m39s
24m08s
1h31m11s
14h20m09s
2h28m09s
1m45s
54s
12m07s

2h07m53s
12m07s

32s
33s
8m18s
58m15s
N/A
2m26s
32s
31s
38s

8m58s
35s

6940
15485
65735
95949
371648
71591
11173
5257
28662

74716
28662

Table 13: Average proportion of Monte Carlo Executions with Ex-
ceptions on the Magma benchmark over 10 trials.

Library

libpng
libtiff
libxml2
openssl
php
poppler
sqlite3

Arithmetic mean
Median

Proportion of Executions with Exceptions (%)

0.26%
2.31%
0.84%
1.22%
4.04%
2.24%
10.9%

3.11%
2.14%

CFG, so it is difficult to accurately measure this time since pre-
processing is conflated with runtime. We instead approximate this
time by measuring the time it takes to run over only the initial seed
corpus, in which the dynamic CFG is constructed and distances are
computed. We emailed the authors to ensure our setup was reason-
able and they confirmed that our experimental setup is reasonable
given the dynamic CFG component. For MC2, we measure the time
it takes to perform preprocessing as described in Section 3. Table 12
summarizes the results for both Magma and Fuzzer Test Suite for
the bugs targets found in Section 4.1.

D MONTE CARLO EXECUTION EXCEPTIONS
Since handling a large number of program exceptions can poten-
tially incur high overheads (i.e., context switches from signal han-
dling), in this experiment, we investigate how many times Monte
Carlo Execution handles program exceptions. Specifically, we
measure the ratio between the number of executions which require
Monte Carlo Execution to handle program exceptions to the total
number of executions in our Magma evaluation, repeated 10 times
to reduce variability.

Table 13 summarizes the results, with 3.11% in arithmetic mean
and 2.14% in median for the proportion. This experiment shows that
many Monte Carlo Executions do not involve program exceptions
and therefore incur low overhead, a finding that better helps explain
our speedups.

