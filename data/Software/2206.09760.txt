Towards Perspective-Based Speciﬁcation of
Machine Learning-Enabled Systems

Hugo Villamizar, Marcos Kalinowski, and H´elio Lopes
Informatics Department
Pontiﬁcal Catholic University of Rio de Janeiro
Rio de Janeiro, Brazil
{hvillamizar, kalinowski, lopes}@inf.puc-rio.br

2
2
0
2

n
u
J

0
2

]
E
S
.
s
c
[

1
v
0
6
7
9
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Machine learning (ML) teams often work on a
project just to realize the performance of the model
is not
good enough. Indeed, the success of ML-enabled systems involves
aligning data with business problems, translating them into
ML tasks, experimenting with algorithms, evaluating models,
capturing data from users, among others. Literature has shown
that ML-enabled systems are rarely built based on precise
speciﬁcations for such concerns, leading ML teams to become
misaligned due to incorrect assumptions, which may affect the
quality of such systems and overall project success. In order to
help addressing this issue, this paper describes our work towards
a perspective-based approach for specifying ML-enabled systems.
The approach involves analyzing a set of 45 ML concerns grouped
into ﬁve perspectives: objectives, user experience, infrastructure,
model, and data. The main contribution of this paper is to provide
two new artifacts that can be used to help specifying ML-enabled
systems: (i) the perspective-based ML task and concern diagram
and (ii) the perspective-based ML speciﬁcation template.

Index Terms—software engineering, requirements speciﬁca-

tion, machine learning, machine learning enabled systems

I. INTRODUCTION

Companies from all sectors are increasingly incorporating
machine learning (ML) components into the software that sup-
ports their operations. We call this ML-enabled systems. The
main difference between ML-enabled systems and non-ML is
that data, to some extent, replaces code when determining the
behavior of ML-enabled systems. This is challenging from the
point of view of software engineering (SE). For instance, data
should be tested and models be validated just as thoroughly as
code, but there is a lack of best practices on how to do so [2].
Data scientists often work on a project just to realize that
the performance of the model is not good enough. However, it
is important to understand several aspects. For example, where
will the model run? What data will it have access to? How
fast does it need to be? What is the business impact of a false
positive? A false negative? How should the model be tuned
to maximize business results? In short, a model is just one
component of a system as a whole.

Within the SE discipline, requirements engineering (RE) has
drawn the attention of researchers and practitioners to the fact
that ML can beneﬁt from this perspective [6]. Literature has
shown that current research on the intersection between RE
and ML mainly focuses on using ML techniques to support
RE activities rather than on exploring how RE can improve
the development of ML-based systems [1]. In addition, most

ML models lack requirements speciﬁcations since current RE
practices are not well deﬁned and organized for ML [7]. This
indicates that there is an incredible amount of work to be done
between the development of a model, the incorporation of it
into a system and the eventual sustainable customer impact.

Given this landscape, we propose a perspective-based ap-
proach for specifying ML-enabled systems. The approach
relies on a set of concerns grouped into ﬁve perspectives:
objectives, user experience, infrastructure, model, and data.
The approach, built based on the literature [12], industrial
experiences working on ML-enabled systems [9], and an
initial industrial validation of relevant concerns [13], covers
relevant perspectives and practical concerns that have not been
considered as part of related work (e.g., [5], [10], [11]).

II. PERSPECTIVE-BASED SPECIFICATION OF ML SYSTEMS

The proposed perspective-based approach provides two ar-
tifacts to support the speciﬁcation of ML-enabled systems:
(i) a perspective-based ML task and concern diagram to be
analyzed; and (ii) a speciﬁcation template to be ﬁlled. These
artifacts were designed based on ﬁndings from a literature
review [12], industrial experiences [9], and an initial validation
of the ML concerns [13]. While the literature review showed us
how ML could beneﬁt from the RE perspective, what particular
properties and research opportunities could be addressed, our
industrial experience allowed us to understand how ML works
in practice from early to ﬁnal stages. Hence, we established
links between theory and practice to learn more about the
context in which our approach operates.

A. Perspective-Based ML Task and Concern Diagram

The perspective-based ML task and concern diagram shown
in Figure 1 consists of ﬁve perspectives: objectives, user
experience, infrastructure, model, and data. Together these
perspectives mediate the involvement of actors such as busi-
ness owners (BO), designers (DG), software engineers (SE),
and data scientists (DS). Each perspective contains a set of
concerns that should be analyzed by a requirement engineer
with the support of at least one actor. In a previous study [13],
we conducted a focus group session with eight software
professionals with experience developing ML-enabled systems
to validate the importance, quality and feasibility of using the
concerns.

 
 
 
 
 
 
Fig. 1. Perspective-based ML task and concern diagram

The diagram contains ﬁve rounded rectangles that repre-
sent the ML perspectives. Each perspective is divided into
rectangles that connect a task (at the top right) to one or
more concerns (at the bottom). Note that each task-concern
has at least one actor suggested (at the top left) related to
the execution of the task and the analysis of the concerns.
For instance, typically in ML projects, DS are tasked for
training, running, and evaluating models. These tasks involve
implicit concerns that are not easily identiﬁed such as learning
time, inference time, and model size (Cf. model perspective
in Figure 1). The diagram also models dashed arrows that
represent dependencies between the tasks. It is noteworthy
that dependencies typically implicitly imply an inﬂuence in the
opposite direction. For example, to transport data, that is a task
of the infrastructure perspective that can require implementing
real time analytics, it is necessary ﬁrst to access the data and
deﬁne the source. In the following, we describe the role of the
actors involved in our approach.

BO typically understand how to plan ML projects in order
to make good decisions. How to connect business objectives
with ML outcomes? What represents future success from a
business point of view? SE typically understand how the
system will interact with the model DS produce and identify
the components that need to exist within a successful ML-
enabled system. What are the pros and cons of running a
model in a client or in a service? DS typically understand
the constraints regarding the ML models. For instance, what
quality properties the model should consider? What domain
restrictions may apply? How fast does it need to be? DG
typically understand how to improve the experience of end-
users of the system and its ML inferences. For analyzing some
tasks and concerns associated to the DG, if possible, end-users
should also be involved. For instance, where and how often

should the ML outcomes appear? How forcefully should they
appear?

Note that speciﬁc concerns can beneﬁt from involving more
than one actor in the analysis. E.g., the execution engine
concern involves knowledge in charge of DS and may also
involve expertise of SE that provide the ML-enabled system
infrastructure. Hereafter, we further describe the perspectives
and concerns of the diagram.

1) ML Objectives Perspective: Bridging the gap between
the high-level goals and the detailed properties of ML models
is one of the most common causes of failure to succeed [3].
Table I presents the concerns related to ML objectives that
could inﬂuence the speciﬁcation of ML-enabled systems.

TABLE I
ML OBJECTIVES PERSPECTIVE

Concern
Customer expecta-
tions

Leading indicators

ML functionality

Model goals

Organizational
goals

Problem & Context

User goals

Description (Addressing this concerns involves ... )
Specifying expectations of customers and end-user in
terms of how the system should behave (e.g., how often
they expect predictions to be right or wrong).
Specifying measures correlating with future success, from
the business’ perspective. This could include the users’
affective states when using the ML-enabled system (e.g.,
customer sentiment and engagement).
Specifying the ML results in terms of functionality that
the model will provide (e.g., classify customers, predict
probabilities).
Specifying metrics and acceptable measures the model
should achieve (e.g., for classiﬁcation problems this could
involve accuracy ≥ X%, precision ≥ Y%, recall ≥ Z%).
Specifying measurable beneﬁts ML is expected to bring
to the organization. E.g., increase the revenue in X%,
increase the number of units sold in Y%, number of trees
saved.
Specifying the problem that ML will address and its
context before coding. ML must be targeted at the right
problem.
to achieve by using
Specifying what
ML. E.g., for recommendation systems this could involve
helping users ﬁnding content they will enjoy.

the users want

2) User Experience (UX) Perspective: Better ML includes
building better experiences of using ML. Connecting the pre-
dictions with users is critical for achieving success. However,
this requires deep analysis. For instance, one mistake in the
way of interacting with the user, and all the work spent on
data pre-processing and modeling may be wasted. The goal
of this perspective is to create effective UX so that users can
interact appropriately and understand what is going on with
the predictions of the model. Table II shows the UX concerns
and a brief description of each one.

TABLE II
UX PERSPECTIVE

Concern
Accountability

Cost

Forcefulness

Frequency

Interactiveness

Value

Prediction Visual-
ization

Description (Addressing this concerns involves ... )
Specifying who is responsible for unexpected model results
or actions taken based on unexpected model results.
Specifying costs involved in executing the inferences and
also the user impact of a wrong model prediction.
Specifying how strongly the system forces the user to do
what the model indicates they should (e.g., automatic or
assisted actions).
Specifying how often the system interacts with users (e.g.,
interact whenever the user asks for it or whenever the system
thinks the user will respond).
Specifying what interactions the users will have with the
ML-enabled system, (e.g., to provide new data for learning,
or human-in-the-loop systems where models require human
interaction).
Specifying the added value as perceived by users from the
predictions to their work.
Specifying how the ML outcomes will be presented so that
users can understand them (e.g., specifying dashboard and
visualization prototypes for validation).

3) Infrastructure Perspective: ML models need to be inte-
grated with other services. This includes components such as
ingesting and learning from new data. The adoption of ML
is growing, but proper implementations are needed to fulﬁll
their promise. Hence, software engineers play an important
role to orchestrate these ML components. Table III presents
the infrastructure concerns.

Concern
Data streaming

Execution en-
gine

Incremental
learning

Integration

Safety

Security

Storage

Telemetry

TABLE III
INFRASTRUCTURE PERSPECTIVE.

Description (Addressing this concerns involves ... )
Specifying what data steaming strategy will be used (e.g., real
time data transportation or in batches).
Specifying how the model of the ML-enabled system will
be executed and consumed (e.g., client-side, back-end, cloud-
based, web service end-point).
Specifying the need for ML-enabled system abilities to con-
tinuously learn from new data, extending the existing model’s
knowledge.
Specifying the integration that the model will have with the rest
of the system functionality.
Specifying how the system deals with risks to prevent dan-
gerous failures. Critical systems that incorporate ML should
analyze the probability of the occurrence of harm and its
severity.
Specifying how the system deals with security issues (e.g.,
vulnerabilities) to protect the data. ML systems often contain
sensitive data that should be protected.
Specifying where the ML artifacts (e.g., models, data, scripts)
will be stored.
Specifying what ML-enabled system data needs to be collected.
Telemetry involves collecting data such as clicks on particular
buttons and could involve other usage and performance moni-
toring data.

4) Model Perspective: Building a model implies not only
training an algorithm with data to predict or classify well some
phenomenon. Many other aspects determine its success. The
aim of this perspective is to provide a set of model concerns a
requirements engineer should analyze. Table IV presents these
concerns.

Concern

Algorithms

Explainability

Inference time

Input & Output

Learning time
Maintainability

ML
type

problem

Model
performance

Model size

Reproducibility

TABLE IV
MODEL PERSPECTIVE.

Description (Addressing this concerns involves ... )

of

set

the

that

could

algorithms

Specifying
be
used/investigated, based on the ML problem and other
concerns to be considered (e.g., constraints regarding
explainability or model performance, for instance, can limit
the solution options).
Specifying the need to understand reasons of the model
inferences. The model might need to be able to summarize
the reasons of its decisions. Other related concerns, such as
transparency and interpretability, may apply.
Specifying the acceptable time to execute the model and
return the predictions.
Specifying the expected inputs (features) and outcomes of
the model. Of course, the set of meaningful inputs can be
reﬁned/improved during pre-processing activities, such as
feature selection.
Specifying the acceptable time to train the model.
Specifying the need for preparing the model to go through
changes with reasonable effort (e.g., refactoring, documen-
tation, automated redeployment).
Specifying the problem type tackled by the ML algorithm
(e.g., classiﬁcation, regression, clustering, extract informa-
tion from text).
Specifying the metrics used to evaluate the model (e.g., pre-
cision, recall, F1-score, mean square error) and measurable
performance expectations.
Specifying the size of the model in terms of storage and its
complexity (e.g., for decision trees there might be needs for
pruning).
Specifying the need for replicating the model creation pro-
cess and its experiments.

5) Data Perspective: Data is essential for ML-enabled
systems. Poor data will result in inaccurate predictions, which
is referred to in the ML context as “garbage in, garbage
out”. Hence, ML requires high-quality input data. From the
viewpoint of RE, it is clear that data constitutes a new type
of requirements [4], [14]. Based on the Data Quality model
deﬁned in the standard ISO/IEC 25012 [8], we elaborate on
the data perspective. Table V presents the data concerns.

B. Perspective-based ML Speciﬁcation Template

Requirements speciﬁcation produces a description of part
of what should be built and delivers it for approval and
requirements management. Based on the perspective-based
ML task and concern diagram,
the requirements engineer
can identify, with the support of the actors, which tasks and
concerns should be analyzed and described as part of the
speciﬁcation.

The division into perspectives, the inﬂuence and dependency
relationships between tasks and concerns and the suggested
involvement of actors can help in this analysis. For instance,
it can be seen in Figure 1 that if the solution requires to know
reasons of ML decisions (task) then the explainability concern
arises. Explainability depends on the chosen algorithm, and
should be speciﬁed considering this aspect. For classiﬁcation

TABLE V
DATA PERSPECTIVE.

Concern

Description (Addressing this concerns involves ... )

Accuracy
Baseline

Bias

Completeness

Consistency
Credibility

Data operations

Distribution
mismatches
Ethic & privacy

Quantity

Real usage

Source
Timeliness

Specifying the need to get correct data.
Specifying the need for a baseline dataset approved by a
domain expert that reﬂects the problem. It is employed to
monitor other data acquired afterwards.
Specifying the need to get data fair samples and representative
distributions.
Specifying the need to get data containing sufﬁcient observa-
tions of all situations where the model will operate.
Specifying the need to get consistent data in a speciﬁc context.
Specifying the need to get true data that is believable and
understandable by users.
Specifying what operations must be applied on the data (e.g.,
data cleaning and labeling).
Specifying expected data distributions and how data will be
split into training and testing data.
Specifying the need to get data to prevent adversely impacting
society (e.g., listing potential adverse impacts to be avoided).
Specifying the expected amount of data according to the type
of the problem and the complexity of the algorithm.
Specifying the need to get real data representing the real
problem.
Specifying from where the data will be obtained.
Specifying the time between when data is expected and when
it is readily available for use.

problems, decisions taken by neural networks are typically less
explainable than decisions taken by decision trees.

To support the speciﬁcation, we provide an ML requirement
speciﬁcation template that allows specifying the applicable
ML-enabled system concerns. Inherently to the nature of ML-
enabled system projects, some types of requirements (e.g.,
required algorithm, required data pre-processing operations,
model performance requirements) are uncertain at
the be-
ginning of the project, mainly due to a common need of
experimentation with data and algorithms to get a better
understanding on achievable requirements. Hence, they may be
reﬁned or more precisely speciﬁed as the project progresses.
The ML speciﬁcation template highlights these concerns with
the letter “E”. Our online material 1 presents an example
of the ML-enabled system speciﬁcation template, ﬁlled out
retroactively with the support of practitioners with information
of two real industrial ML-enabled system projects.

III. CONCLUDING REMARKS

The success of ML-enabled systems involves understanding
business context and problems, translating them into ML tasks,
designing and experimenting with algorithms, evaluating mod-
els, implementing telemetry, among other tasks. This needs to
be considered from early stages of ML software development.
Based on a literature review [12], practical experiences [9],
and an initial validation of the ML speciﬁcation concerns[13],
we presented our work towards an approach for specifying
ML-enabled systems considering ﬁve different perspectives:
objectives, UX,
infrastructure, model, and data. The main
contribution of this paper is to provide two new artifacts
that can be used to help specifying ML-enabled systems:
(i) the perspective-based ML task and concern diagram and
(ii) the ML requirements speciﬁcation template. We believe

1https://doi.org/10.5281/zenodo.6503232

that these artifacts can help requirements engineers with an
overview of ML concerns that should be analyzed together
with business owners, designers, software engineers, and data
scientists to specify ML-enabled systems. Future work in-
cludes exploring how traditional and new RE techniques can
use the perspective-based ML task and concern diagram and
the ML speciﬁcation template. In addition, we plan to conduct
empirical studies to evaluate the completeness and feasibility
of using the artifacts presented in this work.

ACKNOWLEDGMENT
We would like to thank the CAPES agency for ﬁnancial

support.

REFERENCES

[1] K. Ahmad, M. Bano, M. Abdelrazek, C. Arora, and J. Grundy, “What’s
up with requirements engineering for artiﬁcial intelligence systems?”
in 2021 IEEE 29th International Requirements Engineering Conference
(RE).

IEEE, 2021, pp. 1–12.

[2] A. Arpteg, B. Brinne, L. Crnkovic-Friis, and J. Bosch, “Software
engineering challenges of deep learning,” in 2018 44th Euromicro Con-
ference on Software Engineering and Advanced Applications (SEAA).
IEEE, 2018, pp. 50–59.

[3] G. Barash, E. Farchi, I. Jayaraman, O. Raz, R. Tzoref-Brill, and
M. Zalmanovici, “Bridging the gap between ml solutions and their
business requirements using feature interactions,” in Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2019, pp. 1048–1058.

[4] H. Challa, N. Niu, and R. Johnson, “Faulty requirements made valuable:
on the role of data quality in deep learning,” in 2020 IEEE Seventh
International Workshop on Artiﬁcial Intelligence for Requirements En-
gineering (AIRE).

IEEE, 2020, pp. 61–69.

[5] T. Chuprina, D. Mendez, and K. Wnuk, “Towards artefact-based
requirements engineering for data-centric systems,” arXiv preprint
arXiv:2103.05233, 2021.

[6] F. Dalpiaz and N. Niu, “Requirements engineering in the days of

artiﬁcial intelligence,” IEEE Software, vol. 37, no. 4, pp. 7–10, 2020.

[7] F. Ishikawa and N. Yoshioka, “How do engineers perceive difﬁculties
in engineering of machine-learning systems?-questionnaire survey,” in
International Workshop on Conducting Empirical Studies in Industry
(CESI) and 6th International Workshop on Software Engineering Re-
search and Industrial Practice (SER&IP), 2019, pp. 2–9.

(2012)

[8] ISO/IEC.

Iso/iec 25012: Software engineering – software
product quality requirements and evaluation (square) – data quality
model. [Online]. Available: https://www.iso.org/standard/35736.html
[9] M. Kalinowski, H. Lopes, A. F. Teixeira, G. da Silva Cardoso, A. Ku-
ramoto, B. Itagyba, S. T. Batista, J. A. Pereira, T. Silva, J. A. Warrak
et al., “Lean r&d: An agile research and development approach for
digital transformation,” in International Conference on Product-Focused
Software Process Improvement. Springer, 2020, pp. 106–124.

[10] K. Nakamichi, K. Ohashi, I. Namba, R. Yamamoto, M. Aoyama,
L. Joeckel, J. Siebert, and J. Heidrich, “Requirements-driven method
to determine quality characteristics and measurements for machine
learning software and its evaluation,” in 2020 IEEE 28th International
Requirements Engineering Conference (RE).
IEEE, 2020, pp. 260–270.
[11] S. Nalchigar, E. Yu, and K. Keshavjee, “Modeling machine learning
requirements from three perspectives: a case report from the healthcare
domain,” Requirements Engineering, vol. 26, no. 2, pp. 237–254, 2021.
[12] H. Villamizar, T. Escovedo, and M. Kalinowski, “Requirements engi-
neering for machine learning: A systematic mapping study,” in 2021
47th Euromicro Conference on Software Engineering and Advanced
Applications (SEAA).
IEEE, 2021, pp. 29–36.

[13] H. Villamizar, M. Kalinowski, and H. lopes, “A catalogue of concerns
for specifying machine learning-enabled systems,” in Workshop on
Requirements Engineering (WER), 2022, p. 14.

[14] A. Vogelsang and M. Borg, “Requirements engineering for machine
learning: Perspectives from data scientists,” in 2019 IEEE 27th Interna-
tional Requirements Engineering Conference Workshops (REW).
IEEE,
2019, pp. 245–251.

