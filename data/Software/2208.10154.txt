DRAFT VERSION AUGUST 23, 2022
Typeset using LATEX twocolumn style in AASTeX631

2
2
0
2

g
u
A
2
2

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
4
5
1
0
1
.
8
0
2
2
:
v
i
X
r
a

Accelerating spherical harmonic transforms for a large number of sky maps

CHI TIAN,1 SIYU LI,2 AND HAO LIU

∗ 1, 2

1School of Physics and Optoelectronics Engineering, Anhui University, 111 Jiulong Road, Hefei, Anhui, China 230601.
2Key Laboratory of Particle and Astrophysics, Institute of High Energy Physics, CAS, 19B YuQuan Road, Beijing, China, 100049.

ABSTRACT

The spherical harmonic transform is a powerful tool in the analysis of spherical data sets, such as the cosmic
microwave background data. In this work, we present a new scheme for the spherical harmonic transforms
that supports both CPU and GPU computations, which is especially efﬁcient on a large number of sky maps.
By comparing our implementation with the standard Libsharp-HEALPix code, which is one of the top-speed
programs, we demonstrate 4–10 times speedup for the CPU implementation, and up to 60 times speedup when
a state-of-the-art GPU is employed. This new scheme is available at the following GitHub repository: fastSHT .

Contents

1. Introduction

2. Mathematical basis

2.1. SHT of CMB temperature maps
2.2. Simpliﬁed matrix form
2.3. SHT of the CMB polarization maps

3. Code implementation and performance

3.1. Implementation detail
3.2. Memory cost
3.3. Performance

4. An improved iteration scheme

5. Summary and discussions

A. A few technical tips

A.1. FFT-mapping with the unit circle
A.2. Polynomials for polarization

B. The possibility of allowing more iterations

1. INTRODUCTION

1

2
2
3
3

4
4
5
6

8

9

10
10
10

11

The spherical harmonic transform (SHT) decomposes sig-
nals on a sphere into sets of spherical harmonic coefﬁcients
based on the spherical harmonic functions. As an analog to
the Fourier transform, the SHT extracts scale-dependent fea-
tures on the sphere and has been extensively used in ana-
lyzing spherical data in various areas of science, including
cosmology and the cosmic microwave background (CMB)

∗Corresponding author, ustc liuhao@163.com

science. For example, by performing SHTs on the CMB
anisotropy data and ﬁtting with the concordance cosmologi-
cal model (ΛCDM), we are able to make precise predictions
about the cosmological parameters. The CMB polarization
data processing pipeline based on SHTs is also essential in
detecting primordial gravitational waves. Recent develop-
ments in cosmological surveys in CMB temperature and po-
larization anisotropies (Keating et al. 2003; The Planck Col-
laboration 2006; Rubi˜no-Mart´ın et al. 2010; Niemack et al.
2010; Keating et al. 2011; Austermann et al. 2012; Hazumi
et al. 2012; Abazajian et al. 2016; Li et al. 2018; Ade et al.
2019) pose new challenges to data analysis, which usually re-
quires processing a large amount of CMB mock data. There-
fore, it is important to reduce the time cost of SHTs on a large
number of spherical maps.

The optimizations for the SHT have been greatly devel-
oped in the past years. Highly optimized codes, such as Lib-
sharp (Reinecke & Seljebotn 2013) and SHTns (Schaeffer
2013) have been used in various cosmological studies. How-
ever, it is very difﬁcult to further accelerate the SHT on a sin-
gle map due to the fact that (as we shall see later), the most
time-consuming part of the SHT is effectively a BLAS (ba-
sic linear algebra subprograms) level 2 problem, which is not
very suitable for high-performance computing (HPC). How-
ever, we ﬁnd that when there are many input maps, the prob-
lem can be converted to a well-deﬁned BLAS level 3 prob-
lem, which is not only suitable for HPC but also has many
mature solutions. Following this idea, we propose a highly
efﬁcient SHT algorithm that is able to process a large number
of spherical maps in a batch. We will demonstrate the power
of this new scheme by carrying out SHTs on mock CMB tem-
perature and polarization maps with our open source FOR-

 
 
 
 
 
 
2

TRAN implementation — fastSHT 1 (including both CPU
and GPU versions); and we will benchmark the performance
by comparing with the highly optimized SHT toolkit, Lib-
sharp, which is wrapped in HEALPix (G´orski et al. 2005)
and Healpy, where the latter provides a python interface.

Also note that the recent stunning developments in com-
puter hardware, such as Graphic Processing Units (GPUs),
pave the way to further accelerating the traditional spherical
data analysis pipeline, which was mostly based on compu-
tations with CPUs. Pioneering studies such as Hupca et al.
(2012) have demonstrated the power of GPUs in computing
the SHT on a single map. In this work, we will explore both
the GPU and the CPU implementations of our new SHT al-
gorithm on a large number of input maps with state-of-the-art
computer hardware.

This paper is organized as follows, we ﬁrst introduce the
mathematical basis of the SHT in section 2, and then take
CMB temperature and polarization maps as examples to ex-
plain in detail how to accelerate SHTs for a large number
of input maps. The implementation detail of the new SHT
scheme is presented in section 3, where comprehensive mem-
ory and performance benchmarks of the code with CPUs and
GPUs are also included. Then a new iteration scheme based
on our implementation is introduced in section 4, which can
signiﬁcantly improve the quality of iterative estimation. Fi-
nally, we conclude and discuss in section 5.

2. MATHEMATICAL BASIS

The basic algorithm of fast SHT has been described in a
number of literatures (Mohlenkamp 1999; Suda & Takami
2002; Doroshkevich et al. 2005; Huffenberger & Wandelt
2010; Reinecke 2011; Fabbian et al. 2012; McEwen et al.
2013). In particular, some tips of acceleration are summa-
rized in Schaeffer (2013). Till the present day, there seems to
be no signiﬁcant changes to these basic aspects of the algo-
rithm, particularly for the single-map computation; thus, for
convenience of reading, the basis of the algorithm is brieﬂy
summarized below, followed by our new batch SHT scheme.

2.1. SHT of CMB temperature maps

The forward SHT decomposes a scalar ﬁeld on a sphere,
such as the CMB temperature map T (θ, φ), into spherical
harmonic coefﬁcients a(cid:96)ms, following the equation

a(cid:96)m =

(cid:88)

θφ

T (θ, φ)Y(cid:96)m(θ, φ)dσ,

(1)

where θ and φ are discretized azimuthal and polar angle re-
spectively, and dσ = 4π/Npix is the area of one pixel, which
is a constant in the HEALPix pixelization scheme.

1 https://github.com/liuhao-cn/fastSHT

Considering a series of maps denoted by index k, and de-
composing Y(cid:96)m(θ, φ) into the associate Legendre polynomi-
als P m
(cid:96) [cos(θ)] with a phase term2, the above equation be-
comes

ak
(cid:96)m =

=

=

=

(cid:88)

θφ
(cid:88)

θφ

(cid:88)

θ
(cid:88)

θ

T k(θ, φ)P m

(cid:96) [cos(θ)]e−imφdσ

(2)

T k(θ, φ)P m

(cid:96) [cos(θ)]

(cid:104)
e−i 2πmn

N e−imφ0(θ)(cid:105)

dσ

P m

(cid:96) [cos(θ)]

(cid:34)

(cid:88)

n

T k(θ, n)e−i 2πmn

N

e−imφ0(θ)dσ

(cid:35)

P m

(cid:96) [cos(θ)]
T

k
m(θ),

N

(cid:105)

T

≡

(cid:104)(cid:80)

k
m(θ)

n T k(θ, n)e−i 2πmn

where N is the number of pixels in the iso-latitude ring lo-
cated at θ, n is the local index of the pixels inside the ring,
and φ0(θ) is the φ-coordinate of the ﬁrst pixel in each ring.
e−imφ0(θ) is
The value
the combination of Fourier transforms of T (θ, φ) with con-
stant phase shifts, which can be computed by a forward fast
Fourier transform (FFT). The result of FFT is phase-shifted
as above and properly rearranged into matrix forms, which is
called the “FFT mapping” in the following part of this work.
The FFT mapping usually takes about 10-15% of the total
computation time, whereas the most time-consuming part is
the last row of the above equation. If there is only one input
map, the operation in the last row effectively involve only
matrix-to-vector multiplications, since the subscript m does
not participate in the computation. This kind of task is called
a BLAS level 2 problem. Unfortunately, it is well known
that a BLAS level 2 problem cannot be signiﬁcantly accel-
erated, where as a BLAS level 3 problem, such as a matrix-
to-matrix multiplication (MM), can be effectively optimized
by fully exploiting the cache-based architecture (see Goto &
Van De Geijn (2008)).

When there are a large number of spherical maps, the last
row of eq. (2) becomes a matrix-to-matrix multiplication that
contracts the θ-dimension, which looks like

(cid:1)

(cid:0)ak

(cid:96)

m = (cid:0)P θ

(cid:96) · T

(cid:1)

k
θ

m .

(3)

m is written outside the brackets to indicate that it is not part
of the matrix multiplication. This reformulation suggests that
matrix multiplications could be employed to accelerate SHTs
of a batch of sky maps.

2 Here the associated Legendre polynomials are usually re-normalized to the
(cid:96) [cos(θ)]| = |Y(cid:96)m(θ, φ)|, and this kind of re-
(cid:96) and shortened as “lam” in sev-

spherical harmonics as |P m
normalized P m
(cid:96)
eral source codes, e.g., HEALPix.

are also referred to as λm

Similarly, the backward transform is given by equations

(cid:88)

(cid:96)m
(cid:88)

(cid:96)m

lmax(cid:88)

below:

T k(θ, φ) =

=

=

=

(cid:96)mP m
ak

(cid:96) [cos(θ)]eimφ

(4)

(cid:96)mP m
ak

(cid:96) [cos(θ)]ei 2πmn

N eimφ0(θ)

(cid:34)

ei 2πmn

N

eimφ0(θ)

(cid:35)
(cid:96) [cos(θ)]

(cid:96)mP m
ak

lmax(cid:88)

l=m

m=0

lmax(cid:88)
m=0 T

k

m(θ)ei 2πmn
N ,

(cid:104)
eimφ0(θ) (cid:80)lmax

T

k
m(θ) =

should
where
be computed by another matrix multiplication that contracts
(cid:96), which looks like

l=m ak

(cid:96)mP m

(cid:105)
(cid:96) [cos(θ)]

(cid:0)

k
θ

(cid:1)

m = (cid:0)ak
(cid:96) ·

T

(cid:1)

P (cid:96)
θ

m ,

(5)

and then the last row of eq. (4) is computed by a backward-
k
m(θ) being the input FFT-
FFT ring by ring, with
coefﬁcients.

T

To summarize, for a batch of input sky maps, the for-
ward SHT can be accomplished by computing the FFT, FFT-
mapping, and MMs; and the backward SHT can be com-
puted in reverse order by performing the MM, FFT-mapping,
and FFT. Compared to the regular scheme that processes the
maps one by one, this newly designed pipeline fully exploits
the optimizations from matrix multiplications and can there-
fore be more efﬁcient. Moreover, this new scheme also al-
lows more effective usage of the GPU and enables a much
bigger room for acceleration.

2.2. Simpliﬁed matrix form

Eq. (3) and eq. (5) are the cores of our new SHT method,
and they can be further simpliﬁed by deﬁning the following
matrices:

Am = (ak

(cid:96) )m, T m = (
T

i )m, Pm = (P i
k

(cid:96) )m.

(6)

Note that these matrices are usually non-square. The spheri-
cal harmonic transforms are then given by the following ma-
trix multiplications at each m, with the convention that the
superscript of the ﬁrst matrix is contracted with the subscript
of the second matrix.

Am = PmT m
T m = P t
mAm.

(7)

Then T m is connected to the temperature map by the FFT.

2.3. SHT of the CMB polarization maps

3

In the context of rotations on the sphere, the Q- and U -
2 spherical
Stokes parameters can be decomposed into spin
harmonics (Zaldarriaga & Seljak 1997; Zaldarriaga 1998) as

±

Q(ˆn)

±

iU (ˆn) =

(cid:88)

l,m

a±2,lm ±2Ylm(ˆn),

(8)

where ±2Ylm(ˆn) are the spin
ilar to eq. (6–7), the spin
a±2,lm are given by the following matrix multiplications:

2 spherical harmonics. Sim-
2 spherical harmonic coefﬁcients

±

±

A±2,m = F±2,m(Qm

i U m),

±

(9)

where Qm and U m are the phase shifted Fourier transforms
of the Q and U Stokes parameters of the k-th map at the i-
th ring, similar to T m; and F±2,m are the spin
2 spherical
harmonics without the phase term, so F±2,m are real value
matrices.

±

Because the E- and B-mode spherical harmonic coefﬁ-

cients are deﬁned as

aE,lm =
aB,lm = i(a2,lm

(a2,lm + a−2,lm)/2,
a−2,lm)/2,

−

−

(10)

we get

AE

m = −

= −

AB

m =

=

i
2
i
2

1
2

1
2

[F+2,mQm + F+2,m(i U m) + F−2,mQm − F−2,m(i U m)]

(11)

[(F+2,m + F−2,m)Qm + (F+2,m − F−2,m)(i U m)]

[F+2,mQm + F+2,m(i U m) − F−2,mQm + F−2,m(i U m)]

[(F+2,m − F−2,m)Qm + (F+2,m + F−2,m)(iU m)] .

By deﬁning

F+,m =
F−,m =

−

−

(F+2,m + F−2,m)/2
F−2,m)/2,
(F+2,m

−

eq. (11) can be simpliﬁed to:

AE
i AB

m = F+,mQm + F−,m(i U m)
m = F−,mQm + F+,m(i U m),

(12)

(13)

which can be further simpliﬁed to the following matrix form:

(cid:32)

AE
iAB

(cid:33)

=

m

(cid:32)

F+ F−
F− F+

(cid:33)

m

(cid:32)

Q
i U

(cid:33)

.

m

(14)

From eq. (14), we immediately get the backward transform
as:

(cid:32)

Q
i U

(cid:33)

=

m

(cid:32)

F+ F−
F− F+

(cid:33)t

m

(cid:32)

AE
iAB

(cid:33)

.

m

(15)

4

Therefore, iAB
in the spherical harmonic transforms of polarized data.

m and i U m are more natural than AB

m and U m

From eqs. (14–15), it is also convenient to derive pure

pixel-domain decompositions of the E- and B-modes:

(cid:32)

Q
i U

(cid:33)E,B

m

=

=

(cid:32)

F+ F−
F− F+

(cid:32)

F+ F−
F− F+

(cid:33)t

m
(cid:33)t

m

λE,B

λE,B

(cid:32)

AE
iAB

(cid:33)

m

(16)

(cid:32)

F+ F−
F− F+

(cid:33)

m

(cid:32)

Q
i U

(cid:33)

,

m

where superscripts E, B means to keep only the E- and B-
mode components, and λE,B is a diagonal matrix with either
0 or 1 along the diagonal line. For λE, only the ﬁrst half of
the diagonal line is equal to 1, and for λB, only the second
half of the diagonal line is equal to 1.

The above form might be the most elegant representation
of the essence of E- and B-modes, because it shows that, the
only difference between the E- and B-modes is to divide the
identity matrix I into two parts and keep either the ﬁrst or
the second part non-zero.

Apparently, eqs. (14–16) are block form representations of
larger matrices, and the multiplication of each pair of ele-
ments means another matrix multiplication. The combined
larger matrices can be written as

S m =

(cid:32)

Q
i U

(cid:33)

m

, F m =

(cid:33)

(cid:32)

F+ F−
F− F+

m

, Am =

(cid:32)

AE
iAB

(cid:33)

,

m

(17)

then the SHTs of the polarized data can be simply computed
by the following matrix multiplications:

Am = F mS m
S m = F t
mAm,

(18)

where F m is a real matrix but might be non-square. When
an EB-decomposition is required, it can be done as

S E,B

m = (F t

mλE,BF m)S m,

(19)

where F mF t
m and λE,B are both real, diagonal and contain
only 0 or 1. These equations are enough to handle the spher-
ical harmonic transforms of polarized data in either pixel or
harmonic domains.

3. CODE IMPLEMENTATION AND PERFORMANCE

In this section, we will introduce and benchmark our code,
fastSHT, which is a highly optimized implementation of the
new SHT scheme introduced above. Its core part is written in
FORTRAN with both CPU and GPU implementations, and
all of its subroutines are wrapped by the software f90wrap
(Kermode 2020) to expose Python interfaces to users. The

GPU feature can be optionally turned on by users when com-
piling the code. For convenience, we will from now on re-
fer to the name fastSHT-CPU as the fastSHT code complied
with the CPU only; and we will use the name fastSHT-GPU
as the fastSHT code compiled with the GPU feature enabled.
The fastSHT-CPU code employs the Intel MKL library for
the FFT and general matrix multiplication (gemm) to carry
out the operations mentioned above. Other than the faster
matrix multiplications, the fastSHT-GPU also beneﬁt greatly
from more efﬁcient FFT mappings and iterative operations.
Instead of rewriting with CUDA, these calculations in the
fastSHT-GPU are done by using the OpenACC (Open Accel-
erators), which greatly simpliﬁes the coding complications.

3.1. Implementation detail

Algorithm 1: fastSHT algorithm – non-iterative
Data: Input sky maps as matrix: S
Result: Spherical harmonic coefﬁcients: ak

(cid:96)m

θm = Batch FFT(S)

1 F k
2 for m = 0 to (cid:96)max do
3

for θ = 0 to Nrings do

for (cid:96) = 0 to (cid:96)max do

compute Legendre polynomials Pm(l, θ)

end
for k = 0 to Nmaps do

remap F k

θm to Tm(θ, k)

end

end
am(k, (cid:96)) = gemm(Pm((cid:96), θ), Tm(θ, k))

4

5

6

7

8

9

10

11
12 end

Our fastSHT code is based on the HEALPix pixelization
scheme, where the sphere is divided into iso-latitude rings,
and then into equal-area pixels. The iso-latitude rings in this
pixelization scheme naturally discretize the azimuthal angle
θ. We give the pseudo code of our new SHT algorithm in Al-
gorithm 1. The deﬁnitions of the auxiliary ﬁelds (Pm and θm)
are in section 2.1, and the variable Nrings is the total number
of iso-latitude rings. In addition, to accelerate SHT as much
as possible for a large number of sky maps, the following
points should be taken into account in the implementation:

1. All SHT operations are done with real numbers to

maximize efﬁciency.

2. The FFT needs to be performed in batch, and any un-
necessary memory operations, i.e, read/write opera-
tions during the FFT, should be done by the FFT pro-
gram itself. This means that the FFT implementation
should support I/O distances and I/O strides.

3. In a professional FFT implementation such as FFTW
or Intel MKL, a plan/handle will be created before

computing, in which the FFT size is ﬁxed to maximize
the speed. This plan/handle should be reused for rings
of the same size until all rings of the same size are
done.

4. Since the FFTs are done ring by ring for all maps, in
the GPU implementation, once an FFT on one ring is
done, the memory copy from the output of the FFT to
the device memory can be done asynchronously with
the unﬁnished FFT operations, which are performed
on the CPU only. This overlaps the memory copy with
the FFT operations and saves time signiﬁcantly.

5. By default, the primary dimension3 of the FFT result
is occupied by m. However, because m does not par-
ticipate in matrix multiplication, and current “gemm”
programs do not allow non-continuous primary dimen-
sion; a transpose procedure is needed, which should be
done by the FFT program via setting of the FFT I/O
strides.

6. The most important point is that, the FFT-to-a(cid:96)m op-
erations (eqs. (3 & 5)) should be done by the matrix
multiplication, which is a part of lower level BLAS
libraries. These libraries are the core parts of numer-
ous HPC applications, and therefore are usually fully
supported and highly optimized. Top-quality BLAS
implementations, such as the Intel MKL library, the
cuBLAS BLAS library, and third party libraries like
Eigen, OpenBLAS and clBLAS, have been widely
used. Moreover, these implementations also allow var-
ious deployments, from laptop to super computers,
from Windows to Linux, and from CPU platforms to
GPU platforms.
In this study, we use the “dgemm”
subroutine from the Intel MKL BLAS library for the
CPU code and the “cublasDgemm” subroutine from
the Nvidia cuBLAS library for the GPU code. These
subroutines and their sisters4 have been proven to be
the top-notch BLAS libraries.

7. To fully exploit the power of the “dgemm” or “cublas-
Dgemm” implementation, the storage scheme should
be carefully designed. We employ a novel storage
scheme of a(cid:96)ms, which compactiﬁes complex data into
real matrices (see appendix A for details). Moreover,
the built-in read-write support of the “dgemm” should

3 The “primary dimension” is the dimension with continuous physical stor-
age. For example, in FORTRAN this is the ﬁrst dimension, and in C this is
the last dimension.

4 In the naming system of BLAS, the ﬁrst letter is one of s, d, c or z,
which mean single/double precision real numbers and single/double pre-
cision complex numbers, respectively, and “gemm” means general matrix
multiplication.

5

be used as much as possible to reduce redundant read-
write operations. In other words, the reading, conver-
sion, computing, writing, and scaling are done by the
“dgemm” as much as possible, and different types of
the SHTs are implemented by changing the constants
and the order of calling the “dgemm” subroutine.

8. When running with some iterative schemes, e.g., by
setting a non-zero Niter parameter in HEALPix , it
k
is important to restrict the iterations in between
mθ
and ak
(cid:96)m (eq. (3) and eq. (5)), because the forward
k
m(θ) and
and backward FFT transforms between
T k(θ, φ) are lossless. Meanwhile, the iterative updat-
ing of a(cid:96)ms should be done by the “dgemm” via a op-
timal setting of parameters to save time.

T

T

9. The program should support forward and backward
transforms of aE
(cid:96)m separately. In some data
processing pipelines, each step may only need one of
them (Liu et al. 2019; Liu et al. 2019; Liu 2019), so the
speed of SHTs can be increased by 100% for free.

(cid:96)m and aB

10. The ring optimization (Schaeffer 2013) should be used,
which means that for a given m, the rings with ex-
tremely low amplitudes of P(cid:96)m are ignored in com-
putation. This saves about 15% of time, which is
consistent with (Schaeffer 2013). Note that the ring
optimization affects not only P(cid:96)m, but also the FFT-
mapping.

11. When Nside > 512, it is necessary to scale some P(cid:96)ms
during recursions, because the recursions start from
the diagonal elements ((cid:96) = m) that contain factor
sinm(θ), which can cause arithmetic underﬂow for a
64-bit ﬂoating number at high m. However, a normal
scaling method causes considerable performance loss
when P(cid:96)m is required to be a matrix. Therefore, the
following dedicated methods should be adopted: for
Nside
512, we do not use the scaling; and for higher
resolutions, the recursion starts from “safe positions”
that allow the scaling to be ignored. Here the “safe
positions” mean the (cid:96)-positions from which the ampli-
tudes of P(cid:96)m[cos(θ)] are larger than a given threshold,
such as 10−20. These positions are computed once and
saved as an input table (cid:96)safe(m, θ).

≤

12. For the polarized SHT that deals with both E and B-
modes, the order of cumulation should be adjusted,
so that the FFT-mapping operation can be reduced by
50%. This works for both forward and backward trans-
forms.

3.2. Memory cost

6

In Table 1 and 2, we show estimates for the memory cost
of performing SHTs on 1000 maps with Nside = 128, for
the CPU and GPU versions, respectively. Input maps, out-
put spherical harmonic coefﬁcients (a(cid:96)ms), and any neces-
sary caches are all included in the estimation. Due to allocat-
ing pinned memory for buffer arrays instead of using regular
memory, the fastSHT-GPU usually uses slightly more CPU
memory when the polarization and iterations are not included
in the calculations. Note that the memory cost approximately
scales quadratically as Nside increases and scales linearly as
the number of maps increases. These tables thus provide a
baseline for a quick estimation of the total memory cost for
users. For extremely large numbers of maps, it is fastSHT to
process them. To achieve better performance, the size of the
sub-sets should be set so that the memory cost approaches
the platform memory limit, especially for fastSHT-GPU.

Nside Nmaps
1000
128
1000
128
1000
128
1000
128

Pol.?
No
No
Yes
Yes

Iterative?
No
Yes
No
Yes

mem.
4.4 GB
5.9 GB
8.8 GB
11.8 GB

GPU mem.
N/A
N/A
N/A
N/A

Table 1. Estimation of the memory cost of the fastSHT-CPU code.
The cost includes input maps, a(cid:96)ms and necessary caches.

Nside Nmaps
1000
128
1000
128
1000
128
1000
128

Pol.?
No
No
Yes
Yes

Iterative? mem.
4.9 GB
4.9 GB
7.2 GB
7.2 GB

No
Yes
No
Yes

GPU mem.
3.2 GB
4.7 GB
5.9 GB
8.9 GB

Table 2. Estimations of the memory cost of the fastSHT-GPU code.
The cost includes input maps, a(cid:96)ms and necessary caches.

3.3. Performance

We test the performance of the fastSHT code on a docker
virtualized machine with state-of-the-art hardware — it con-
tains an Intel Platinum 8336C CPU and an Nvidia Tesla A100
GPU. We are limited to using 8 CPU cores since the per-
formance of the CPU version is also limited by the system
memory bandwidth, and the use of more CPU cores will
not improve the performance signiﬁcantly on this platform.
The compilers used in these tests are the Intel FORTRAN
compiler and the Nvidia PGI (Portland Group Inc.) FOR-
TRAN compiler for the CPU and GPU versions, respectively.
Even though GPUs are known to be good at massive single-
precision ﬂoating-point computations, we stick to computing
with double precision in all our tests to fully meet the pre-
cision requirement of various cosmological simulations and
observations.

In our tests, we notice that the fastSHT-GPU usually in-
troduces a signiﬁcant amount of initialization overhead. This
overhead includes the initialization of the cuBLAS library
and the allocation of cache arrays. Moreover, when a GPU is
involved, the CPU buffer arrays are allocated in the form of
pinned arrays, which ensures faster communications with the
GPU memory at the cost of a larger allocation overhead. For-
tunately, the fastSHT-GPU only needs to be initialized once
for any amount of computation at the same resolution and
(cid:96)max, i.e., an extremely large number of maps can be packed
into several smaller batches, and the fastSHT-GPU initial-
ization overhead will only happen once for the ﬁrst batch,
and then all follow-up batches are free from this overhead.
This feature makes this initialization overhead distinct from
other in-computation costs, such as the data transfer cost be-
tween the CPU and GPU. On the other hand, for a compli-
cated analysis that evolves massive ﬂoating point calculation,
the initialization overhead is relatively negligible (see Table 3
for a realistic example). Therefore, in all of the performance
evaluations presented in this work, we choose to separate the
time cost of the initialization overhead and main SHT com-
putations, and exclude this overhead when quantifying the
acceleration factor for the fastSHT-GPU code.

.
×

In Fig. 1, we compare the time cost of fastSHT with
Healpy for the cases with/without iterations. The input map
1, and we
resolution is Nside = 128 and (cid:96)max = 3Nside
compute the a(cid:96)ms without considering the polarization. As
explained above, the time cost in the GPU case includes the
GPU memory copy-in and copy-out time cost but excludes
In the left panel of Fig. 1, we
the initialization overhead.
focus on the case without iteration: fastSHT-CPU gives ac-
celeration factors between 3.6
, which increases
slightly with the number of maps; whereas fastSHT-GPU
gives acceleration factors that are close to 10

and 4.2

×

×

−

When a standard iteration scheme is performed to improve
the SHT precision, the advantage of fastSHT is even more
manifest because the initialization overhead happens only
once, and the time cost of memory copying is also relatively
small. We show these time costs in the right panel of Fig. 1,
where identical iterative schemes are performed for both fast-
SHT and Healpy to ensure an “apples-to-apples” compari-
son (an alternative iteration scheme and its effects will be
introduced in section 4). An acceleration factor bigger than
5.5
can be achieved for fastSHT-CPU with more than 4000
maps. Strikingly, the acceleration factor of fastSHT-GPU
when there are as many as 8000 maps.
reaches about 45
The initialization overhead of the GPU version is relatively
small, and the computational costs can thus beneﬁt greatly
from the GPU even after including the initialization over-
head.

×

×

In Fig. 2, we show how the performance of the fastSHT
scales as the map resolution Nside increases. The panels from

7

Figure 1. The computational time costs (no polarization) for the Libsharp (Healpy), fastSHT-CPU and fastSHT-GPU for performing SHTs on
1000 to 8000 maps, all with Nside = 128 and (cid:96)max = 383. The initialization overhead for the fastSHT-GPU is included as the dashed line, and
the initialization overhead for other schemes are negligible. The left panel shows the comparison without iteration, and the right panel shows
the results with 3 rounds of iterations. For both panels: upper gives the comparison of the time costs and lower gives the acceleration factors.

left to right show the results for processing batches contain-
ing Nmaps = 2000, 1000, 250 maps respectively. The maxi-
mum N is different for different panels since when working
with a larger number of maps, the maximum resolution is
limited by the GPU memory capacity. Due to a better par-
allel efﬁciency of the traditional method when dealing with
a single-but-larger map, the acceleration factor of fastSHT-
CPU is gradually suppressed at higher resolution (but still
>2 in all cases). The fastSHT-GPU, on the other hand, ex-
hibits a signiﬁcant performance improvement with an accel-
eration factor greater than 50
at higher resolutions, and the
×
acceleration factor approximately scales linearly with Nside,
implying an acceleration factor of nearly 100
when extrap-
olating to Nside = 1024 for the middle panel. The exact per-
formance improvement, however, remains to be conﬁrmed
by future dedicated hardware with larger GPU memory. Ac-
cording to the benchmarks, in most cases, using the fastSHT-
GPU is recommended at high map resolutions, until one hits
the GPU memory limit.

×

In particular, to demonstrate potential applications of our
new SHT scheme in a realistic situation, we perform a
pipeline of ﬁxing the EB-leakages by the best blind estimator
(Liu et al. 2019; Liu 2019), which not only includes all types
of SHTs (forward, backward, temperature, polarization, iter-
ative scheme), but also involves a linear regression procedure
and a lot of memory updating operations. We choose to work

×

×

×

×

and 3.6

with 3 iterations and at two different resolutions:
the ﬁrst
one is Nside = 256 and Nmaps = 2000, and the second one
is Nside = 512 and Nmaps = 500. The time costs are listed
in Table 3, which shows that the CPU acceleration factors are
4.8
for the two tests respectively, and the GPU ac-
celeration factors are 32
and 37
, which means a task that
used to take one hour can now be done within 1.5 minutes. It
is worth noting that the GPU initialization overheads in these
two tests are only 4.8 s and 5.7 s, respectively, accounting
for only about 5
7% of the total computation time. This
is a direct consequence of the massive ﬂoating-point compu-
tation required by the ﬁxing EB-leakages pipeline. In both
tests, the fastSHT-GPU outperforms the traditional scheme
signiﬁcantly even after including the initialization overhead,
suggesting a great application potential of our SHT scheme
in a realistic CMB data processing.

−

Nside Nmaps
2000
256
500
512

t1
2189 s
3602 s

t2
460 s (4.8×)
988 s (3.6×)

t3
68 s (32×)
98 s (37×)

Overhead
4.8 s
5.7 s

Table 3. The time costs for Healpy, fastSHT-CPU, and fastSHT-
GPU (t1 ∼ t3, without the GPU overhead) in a realistic job that
ﬁxes the EB-leakage at two different resolutions respectively. The
GPU overhead itself is also presented in the last column, and the
acceleration factors relative to Healpy are given in brackets.

100101time(s)Libsharp(Healpy)fastSHT-CPUfastSHT-GPUGPUinit.2000400060008000No.ofmaps51015Acceleration100101102time(s)Libsharp(Healpy)fastSHT-CPUfastSHT-GPUGPUinit.1000200030004000No.ofmaps2040Acceleration8

Figure 2. The computational time costs and the acceleration factor (3 rounds of iterations, no polarization) for the Libsharp (Healpy), fastSHT-
CPU and fastSHT-GPU codes. The numbers of maps are 2000 (left), 1000 (middle) and 250 (right) respectively, and Nside varies from 32 to
1024 depending on the number of maps, and (cid:96)max = 3Nside − 1. The initialization overhead for the fastSHT-GPU is presented in the dashed
line, whereas the initialization overheads for other schemes are negligible. The black dashed line marks the position of 50× as a reference.

Lastly, we notice that Libsharp also supports an optimiza-
tion option when processing many input maps based on a
straightforward idea: compute the P(cid:96)ms only once, and then
reuse them for each input map. Nevertheless, the calculation
of the P(cid:96)ms only takes a small fraction of the total compu-
tational time in both Libsharp and fastSHT, and indeed we
see no signiﬁcant improvement when enabling this feature in
Libsharp in our tests. Meanwhile, this is an advanced feature
and is not included in the Healpy. As a result, we ignore this
feature in the Libsharp and do not cache P(cid:96)ms in our fastSHT
code.

4. AN IMPROVED ITERATION SCHEME

For iterative approaches, two schemes are considered: one
is the “traditional” mode adopted by the HEALPix , in which
the a(cid:96)ms are updated outside the m-loop; and the other one
is the “immediate” mode, in which the a(cid:96)ms are updated im-
mediately within the m-loop. The pseudo codes with more
details for these two schemes are described in Algorithm 2
and 3 respectively.

The traditional mode gives better results when the input
maps are strictly band-limited to (cid:96)max
2Nside, whereas the
“immediate” mode is much more accurate at higher multi-
ples, especially for (cid:96)max is close to 3Nside
1. A comparison
is presented in Figure 3, which clearly shows that the imme-
diate mode has much better performance at higher (cid:96)s. Since
a realistic input map cannot be strictly band-limited, the im-
mediate mode can be more accurate in practice.

≤

−

Moreover, we also notice that, when input maps have no
harmonic domain band limit (a typical case is that they con-
tain pure white noise), the new scheme with a high (cid:96)max
can still give a reasonably good pixel domain convergence,

Algorithm 2: Fast SHT algorithm for many sky maps
with the traditional iterative method
Data: Input maps as matrix: S

Number of iterations: Niter

Result: Spherical harmonic coeﬁcients: a(cid:96)m

θm = Batch FFT(S)

1 F k
2 for n = 1 to Niter do
3

for m = 0 to (cid:96)max do

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

for θ = 0 to Nrings do

for (cid:96) = 0 to (cid:96)max do

compute Legendre polynomials Pm((cid:96), θ)

end

end
δ(T k
update F k

θ )m = (T k

θ )m − (ak
θ )m

θm from δ(T k

(cid:96) · P (cid:96)

θ )m

end
for m = 0 to (cid:96)max do

for θ ← 0 to Nrings do

for (cid:96) = 0 to (cid:96)max do

compute Legendre polynomials Pm((cid:96), θ)

end
for k = 0 to Nmaps do
θm to (T k

remap F k

θ )m

end

end
(cid:1)
(cid:0)ak
m

(cid:96)

= (cid:0)P θ

(cid:96) · T k
θ

(cid:1)
m

end

22
23 end

whereas the traditional HEALPix scheme always leaves sig-
niﬁcant residual. Even at the center region of the sky, the
traditional scheme is not able to provide a good convergence
for the case without any band-limit, see ﬁgure 4.

No.ofmaps10−1100101102time(s)Libsharp(Healpy)fastSHT-CPUfastSHT-GPUGPUoverhead3264128256Nside100101102AccelerationNo.ofmaps100101102103time(s)Libsharp(Healpy)fastSHT-CPUfastSHT-GPUGPUoverhead64128256512Nside100101102AccelerationNo.ofmaps100101102103time(s)Libsharp(Healpy)fastSHT-CPUfastSHT-GPUGPUoverhead1282565121024Nside100101102AccelerationAlgorithm 3: Fast SHT algorithm for many sky maps
with the immediate iterative method
Data: Input maps as matrix: S

Number of iterations: Niter

Result: Spherical harmonic coeﬁcients: a(cid:96)m

9

θm = Batch FFT(S)

1 F k
2 for n = 1 to Niter do
3

for m = 0 to (cid:96)max do

for θ = 0 to Nrings do

for (cid:96) = 0 to (cid:96)max do

compute Legendre polynomials Pm((cid:96), θ)

end

θ )m = (T k

end
δ(T k
update F k
θm from δ(T k
for k = 0 to Nmaps do
θm to (T k

θ )m − (ak
θ )m

remap F k

θ )m

(cid:96) · P (cid:96)

θ )m

end
(cid:0)ak
(cid:1)
m

(cid:96)

= (cid:0)P θ

(cid:96) · T k
θ

(cid:1)
m

4

5

6

7

8

9

10

11

12

13

14

end

15
16 end

Figure 4. The pixel domain errors of iterative solution, presented as
the pixel domain differences between the input/output white noise
maps with RMS=1 at Nside = 128. Upper: the iteration is limited
to (cid:96)max = 256. Lower: the iteration is limited to (cid:96)max = 512.
From left to right: the old/new scheme residuals.

∼

In this work, we design a dedicated algorithm for SHTs on
a large number of sky maps, which is also a complete SHT
toolkit for the CMB data, including temperature/polarization,
forward/backward, and iterative/non-iterative transforms.
The results of the new code are identical to the traditional
SHT toolkit like Healpy, but the performance is signiﬁcantly
improved on a large number of sky maps. For the CPU im-
10 times faster
plementation, the speed of the new code is 4
than the state-of-the-art SHT program; when GPU is em-
ployed, the typical acceleration factor is more than one order
of magnitude without iteration, and can be further boosted to
nearly two orders of magnitude with 3 rounds of iterations.
By applying this new scheme to the standard ﬁx-EB leak-
age pipeline for 2000 masked CMB maps at Nside = 256,
even after including the initialization overhead, the GPU ver-
sion still gets a 20
speed-up compared to the Healpy SHT
toolkit that does the same job. Moreover, the acceleration
factor of the GPU version increases almost linearly with
Nside, indicating that the new code can be much more efﬁ-
cient at a higher resolution (given enough GPU memory). As
far as we know, this is the fastest SHTs implementation for a
large number of spherical maps till the present day.

×

Figure 3. The pixel domain errors of iterative solution, presented as
the pixel domain differences between the input/output CMB inten-
sity simulations based on the Planck 2018 best-ﬁt CMB spectrum
(Planck Collaboration et al. (2018)). Left: results of the traditional
(HEALPix) iteration. Right: results for the immediate mode itera-
tion (see section 4). The input map resolution is Nside = 128 and
(cid:96)max = 383, 448 and 512 from top to bottom, respectively. Ap-
parently, the immediate mode has much lower errors at higher (cid:96)s.
Meanwhile, we also point out that, if the input map is strictly band-
limited to (cid:96)max ≤ 2Nside, then the errors of the traditional mode are
lower. However, in this case, the errors of both modes are negligi-
ble.

5. SUMMARY AND DISCUSSIONS

However, we would like to point out that for a single in-
put map, the computation of SHT will degenerate from the
BLAS level 3 to the BLAS level 2, so the core part of the
new scheme will become less efﬁcient. Moreover, the accel-
eration requires using P(cid:96)ms as matrices, but for a single map,
it is more efﬁcient to compute the P(cid:96)ms in small segments to
use the CPU L1 cache more efﬁciently. Similar reasons ap-
ply for the GPU code, and the large initialization overheads
also prevent the fastSHT-GPU from outperforming Healpy
when dealing with only a small number of maps. Since it is
unlikely that one algorithm can be the best for all cases, it is
highly recommended to use our code on a large number of
maps and work in batch.

10

We have also provided a new algorithm integrated in the
fastSHT to improve the accuracy of SHTs by optimizing the
iteration strategy. The test results show that this new iterative
scheme can give much better convergence at high multiples,
especially for the temperature maps. We also point out that
when the input map is strictly band-limited to (cid:96) < 2Nside,
the traditional iterative scheme is slightly better – but both
iterative schemes nevertheless give negligible errors in this
case.

Numerous ongoing and planned CMB experiments are
going to bring heavy data forecasting and analyzing tasks.
These tasks involve processing large amounts of mock data
with extremely high resolution. As the core component in al-
most all CMB data processing pipelines, the performance of
SHTs has become very critical. The new scheme introduced
in this paper fully exploits the state-of-the-art software and
hardware to accelerate SHTs. As a consequence, the time
costs for SHT operations are reduced considerably, and stan-
dard data processing pipelines, such as ﬁxing EB-leakages,
can be accelerated correspondingly. Also note that SHTs are
widely used not only in astrophysics and cosmology but in
other areas of science as well, including geoscience, atmo-

spheric sciences, oceanography, etc., where spherical data
is used frequently. As a result, we anticipate that our new
scheme will have broader applications.

Finally, we would like to refer to the most recent devel-
opment of the multi-GPU computation technique that uses
distributed GPU memory in a 2D block-cyclic fashion (The
NVIDIA company 2022). This technique is still in an early
view stage, and it is expected to provide access to much more
GPU memory and computational power in a dedicated way
for matrix multiplications; hence it will likely solve the GPU
memory size problem that prevents our fastSHT-GPU code
from working sufﬁciently well at a very high resolution, like
Nside
2048. We shall keep tracking the development of
this technique and update our code accordingly in the future.

≥

1

2

3

4

5

6

This work is supported by the National Key R&D Program of
China (2021YFC2203100, 2021YFC2203104) and the An-
hui project Z010118169. We would like to thank Matthew
Carney, James Mertens and Glenn Starkman for helpful dis-
cussions, and our USTC colleagues for helping us with the
GPU test environment.

APPENDIX

A. A FEW TECHNICAL TIPS

A.1. FFT-mapping with the unit circle

One thing to be noticed in a SHT is that, in most cases, the
number of pixels in one ring and the value of m do not match,
thus one should properly map m to the FFT frequency circle,
as shown in Fig. 5, which is a unit circle centered around the
original point, with points on it marked by the polar angles
In the frequency circle, φ = 0 correspond to the zero
φ.
frequency, φ = π correspond to the Nyquist frequency, and
all frequencies above the Nyquist frequency will be mapped
to lower frequencies by moving along the circle in an anti-
clockwise direction. This is also a clear presentation of the
Nyquist-Shannon sampling theorem, which naturally shows
why one cannot exceed the Nyquist frequency. As an exam-
ple, m = 0 is always mapped to φ = 0, regardless of the
value of N (size of one ring); but other m should be mapped
to φ = 2πm
N . This means, even if m is the same, in rings with
different N , it will be mapped to different positions, and vice
versa.

A.2. Polynomials for polarization

In the context of rotation, Q and U can be decomposed into
2 spherical harmonics Zaldarriaga & Seljak (1997);

spin
Zaldarriaga (1998) as follows:

±

Figure 5. Left: An example of the FFT-frequency circle showing
the mapping of different m when the FFT size is N = 4. Right:
Illustration of the new saving scheme of a(cid:96)m, adopted to match the
requirement of a large number of matrix operations. The lower half
(red) is for the real part, and the upper half (cyan) is for the imag-
inary part. The real and imaginary parts of a(cid:96)m for one m are dis-
tributed as shown by the black arrows.

where ±2Ylm(ˆn) are the spin
ics. Thus, the spin
are given by the following:

±

2 weighted spherical harmon-
2 spherical harmonic coefﬁcients a±2,lm

±

ak
±2,lm =

(cid:88)

θ

P±2,(cid:96)m(cos(θ))(

k
im ±
Q

k
im),

i

U

(A2)

Q(ˆn)

±

iU (ˆn) =

(cid:88)

l,m

a±2,lm ±2Ylm(ˆn),

(A1)

k
im are the Fourier transforms of the Q and
where
U Stokes parameters of the k-th map at the i-th ring. For

k
im and
Q

U

m=0, 4, 8, ...m=1, 5, 9, ...m=2, 6, 10, ...m=3, 7, 11, ... lmsimplicity, we deﬁne the following matrix forms:

matrix with a very simple addressing rule (see also Fig. 5):

11

A±

m = (ak

±2,(cid:96))m, P±

m = (P±2,(cid:96))m

(A3)

Real part:
Imaginary part:

((cid:96), m) =
((cid:96), m) =

⇒
⇒

((cid:96), m)

((cid:96)

−

m, (cid:96)max

−

m + 1),

(A7)

Because the E- and B-mode spherical harmonic coefﬁ-

cients are deﬁned as

aE,lm =
aB,lm = i(a2,lm

(a2,lm + a−2,lm)/2,
a−2,lm)/2,

−

−

(A4)

In this work, we use a HEALPix-like program to compute
the P(cid:96)m coefﬁcients for both temperature and polarization.
One should note that (seems missing in the HEALPix man-
ual), the convention is to compute the summation and dif-
ference of the polarized P(cid:96)m coefﬁcients, which are used in
SHTs instead of polarized P(cid:96)ms themselves. With such con-
vention, the polarized a(cid:96)ms are computed from the Q and U
stokes parameters by

F +

(cid:96)m(σ) =

−

F −

(cid:96)m(σ) =

−

1
2

2

[Y+2,(cid:96)m(σ) + Y−2,(cid:96)m(σ)] = λ+

(cid:96)m(θ)eimφ

Y−2,(cid:96)m(σ)] = λ−

(A5)
(cid:96)m(θ)eimφ,

[Y+2,(cid:96)m(σ)

−

where Y±2,(cid:96)m are the spin
2 spherical harmonics, and
±
λ±
(cid:96)m(θ) are the effective Legendre coefﬁcients generated by
HEALPix . Let X be either Q or U , then the E and B-
mode spherical harmonic coefﬁcients, computed with the
HEALPix convention, are

(cid:90)

X ±

(cid:96)m =

X(σ)F ±∗

(cid:96)m (σ)dσ

(A6)

aE
(cid:96)m =
aB
(cid:96)m =

Q+
U +

(cid:96)m −
(cid:96)m −

−

−

iU −
(cid:96)m
iQ−
(cid:96)m.

Lastly, a new storage scheme of a(cid:96)m is introduced to match
the requirement of high performance computing with many
input maps, in which all a(cid:96)m coefﬁcients are stored in a real-
valued square matrix of size ((cid:96)max + 1)
((cid:96)max + 1). The
real and imaginary parts are compactly5 stored in the square

×

where we have the natural constraints that all non-zero imag-
inary parts satisfy: 1 (cid:54) m (cid:54) (cid:96)max and m (cid:54) (cid:96) (cid:54)
(cid:96)max. This new scheme is especially important for the GPU-
implementation, because this kind of regular shape tensor-
style storage with very simple addressing rule is apparently
favored by the GPU. Even in a CPU-implementation, it is
able to signiﬁcantly reduce the time cost of memory opera-
tions.

5 Here “compact” means all matrix elements are used without redundancy.

B. THE POSSIBILITY OF ALLOWING MORE

ITERATIONS

We know that the accuracy of SHTs can be improved with
an increasing number of iteration rounds. The default choice
in Healpy is 3 rounds, which is a balance between the time
cost and accuracy – with the “time cost” estimated under
the old scheme (The HEALPix team 2005). Our new SHT
scheme, especially the GPU version, is signiﬁcantly more ef-
ﬁcient with a bigger number of iterations, because the itera-
tive part of SHT is done in GPU instead of CPU. This fact
will break the old balance and allow more iterations at a rea-
sonable time cost. In order to evaluate this fact, We have done
a non-polarization fastSHT-GPU test with Nside = 256,
(cid:96)max = 3Nside
1, Nmaps = 1000 and Niter = 3 and
9 respectively: When Niter = 3, the time cost of the non-
iterative part (including the operations outside the iteration)
is 1.002 s and each round of iteration (including two SHTs,
one forward and one backward) costs only 0.426 s. Thus we
can predict the time cost with any number of iterations by the
following equation:

−

ttotal = t0 + (2Niter + 1)t1,

(B8)

and for the example here, we have t0 = 1.002 s and t1 =
0.213 s respectively. To double check the above equation:
for Niter = 9, the total time cost is 5.070 s, which is fully
consistent with the time computed from the above equation.
Therefore, when Niter increase from 3 to 9 (three times), the
time cost only increases from 2.5 s to 5.0 s (two times), in-
dicating that we get relatively higher efﬁciencies when Niter
increases for fastSHT-GPU.

REFERENCES

12

Abazajian, K. N., Adshead, P., Ahmed, Z., et al. 2016, ArXiv

Liu, H. 2019, Journal of Cosmology and Astroparticle Physics,

e-prints. https://arxiv.org/abs/1610.02743

Ade, P., Aguirre, J., Ahmed, Z., et al. 2019, JCAP, 2019, 056,

doi: 10.1088/1475-7516/2019/02/056

Austermann, J. E., Aird, K. A., Beall, J. A., et al. 2012, in Society

of Photo-Optical Instrumentation Engineers (SPIE) Conference

Series, Vol. 8452, Millimeter, Submillimeter, and Far-Infrared

Detectors and Instrumentation for Astronomy VI, ed. W. S.

Holland & J. Zmuidzinas, 84521E, doi: 10.1117/12.927286

2019, 001, doi: 10.1088/1475-7516/2019/10/001
Liu, H., Creswell, J., & Dachlythra, K. 2019, Journal of
Cosmology and Astroparticle Physics, 2019, 046,
doi: 10.1088/1475-7516/2019/04/046

Liu, H., Creswell, J., von Hausegger, S., & Naselsky, P. 2019,
PhRvD, 100, 023538, doi: 10.1103/PhysRevD.100.023538

McEwen, J. D., Puy, G., Thiran, J. P., et al. 2013, IEEE

Transactions on Image Processing, 22, 2275,
doi: 10.1109/TIP.2013.2249079

Doroshkevich, A. G., Naselsky, P. D., Verkhodanov, O. V., et al.

Mohlenkamp, M. J. 1999, Journal of Fourier Analysis and

2005, International Journal of Modern Physics D, 14, 275,

Applications, 5, 159, doi: 10.1007/BF01261607

doi: 10.1142/S0218271805006183

Fabbian, G., Szydlarski, M., Stompor, R., Grigori, L., & Falcou, J.

2012, Publications of the Astronomical Society of the Paciﬁc,

ASP Conference Series, 61

G´orski, K. M., Hivon, E., Banday, A. J., et al. 2005, ApJ, 622, 759,

doi: 10.1086/427976

Goto, K., & Van De Geijn, R. 2008, ACM Trans. Math. Softw., 35,

doi: 10.1145/1377603.1377607

Hazumi, M., Borrill, J., Chinone, Y., et al. 2012, in Proc. SPIE,

Vol. 8442, Space Telescopes and Instrumentation 2012: Optical,

Infrared, and Millimeter Wave, 844219, doi: 10.1117/12.926743

Huffenberger, K. M., & Wandelt, B. D. 2010, The Astrophysical

Journal Supplement Series, 189, 255,

doi: 10.1088/0067-0049/189/2/255

Hupca, I. O., Falcou, J., Grigori, L., & Stompor, R. 2012, in

Euro-Par 2011: Parallel Processing Workshops (Berlin,

Heidelberg: Springer Berlin Heidelberg), 355–366

Keating, B., Moyerman, S., Boettger, D., et al. 2011, ArXiv

e-prints. https://arxiv.org/abs/1110.2101

Keating, B. G., Ade, P. A. R., Bock, J. J., et al. 2003, in Society of

Niemack, M. D., Ade, P. A. R., Aguirre, J., et al. 2010, in Society
of Photo-Optical Instrumentation Engineers (SPIE) Conference
Series, Vol. 7741, Millimeter, Submillimeter, and Far-Infrared
Detectors and Instrumentation for Astronomy V, ed. W. S.
Holland & J. Zmuidzinas, 77411S, doi: 10.1117/12.857464
Planck Collaboration, Aghanim, N., Akrami, Y., et al. 2018, arXiv
e-prints, arXiv:1807.06209. https://arxiv.org/abs/1807.06209

Reinecke, M. 2011, A&A, 526, A108,
doi: 10.1051/0004-6361/201015906

Reinecke, M., & Seljebotn, D. S. 2013, A&A, 554, A112,

doi: 10.1051/0004-6361/201321494

Rubi˜no-Mart´ın, J. A., Rebolo, R., Tucci, M., et al. 2010, in
Astrophysics and Space Science Proceedings, Vol. 14,
Highlights of Spanish Astrophysics V, 127,
doi: 10.1007/978-3-642-11250-8 12

Schaeffer, N. 2013, Geochemistry, Geophysics, Geosystems, 14,

751, doi: 10.1002/ggge.20071

Suda, R., & Takami, M. 2002, Math. Comput., 71, 703,

doi: 10.1090/S0025-5718-01-01386-2
The HEALPix team. 2005, online document,

https://healpix.sourceforge.io/html/sub map2alm iterative.htm

The NVIDIA company. 2022, The cuBLAS preview page,

Photo-Optical Instrumentation Engineers (SPIE) Conference

https://developer.nvidia.com/cublas

Series, Vol. 4843, Polarimetry in Astronomy, ed. S. Fineschi,

The Planck Collaboration. 2006, arXiv e-prints, astro.

284–295, doi: 10.1117/12.459274

https://arxiv.org/abs/astro-ph/0604069

Kermode, J. R. 2020, Journal of Physics: Condensed Matter, 32,

Zaldarriaga, M. 1998, The Astrophysical Journal, 503, 1.

305901, doi: 10.1088/1361-648x/ab82d2

Li, H., Li, S.-Y., Liu, Y., et al. 2018, National Science Review, 6,

145, doi: 10.1093/nsr/nwy019

http://stacks.iop.org/0004-637X/503/i=1/a=1

Zaldarriaga, M., & Seljak, U. c. v. 1997, Phys. Rev. D, 55, 1830,

doi: 10.1103/PhysRevD.55.1830

