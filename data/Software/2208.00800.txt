GANDSE: GENERATIVE ADVERSARIAL NETWORK BASED
DESIGN SPACE EXPLORATION FOR NEURAL NETWORK
ACCELERATOR DESIGN

2
2
0
2

g
u
A
1

]

G
L
.
s
c
[

1
v
0
0
8
0
0
.
8
0
2
2
:
v
i
X
r
a

Lang Feng†, Wenjian Liu†, Chuliang Guo‡, Ke Tang†, Cheng Zhuo∗‡, Zhongfeng Wang∗†
†School of Electronic Science and Engineering, Nanjing University
‡College of Information Science & Electronic Engineering, Zhejiang University
ﬂang@nju.edu.cn; wjliu@nju.edu.cn; chuliang007@zju.edu.cn
mf21230105@smail.nju.edu.cn; czhuo@zju.edu.cn; zfwang@nju.edu.cn

August 2, 2022

ABSTRACT

With the popularity of deep learning, the hardware implementation platform of deep learning has
received increasing interest. Unlike the general purpose devices, e.g., CPU, or GPU, where the deep
learning algorithms are executed at the software level, neural network hardware accelerators directly
execute the algorithms to achieve higher both energy efﬁciency and performance improvements.
However, as the deep learning algorithms evolve frequently, the engineering effort and cost of
designing the hardware accelerators are greatly increased. To improve the design quality while
saving the cost, design automation for neural network accelerators was proposed, where design space
exploration algorithms are used to automatically search the optimized accelerator design within a
design space. Nevertheless, the increasing complexity of the neural network accelerators brings
the increasing dimensions to the design space. As a result, the previous design space exploration
algorithms are no longer effective enough to ﬁnd an optimized design. In this work, we propose
a neural network accelerator design automation framework named GANDSE, where we rethink
the problem of design space exploration, and propose a novel approach based on the generative
adversarial network (GAN) to support an optimized exploration for high dimension large design
space. The experiments show that GANDSE is able to ﬁnd the more optimized designs in negligible
time compared with approaches including multilayer perceptron and deep reinforcement learning.

Keywords Electronic Design Automation · Design Space Exploration · Generative Adversarial Networks · Neural
Network Accelerators

1

Introduction

Deep learning is prevalent in the area of artiﬁcial intelligence (AI) due to its success in many ﬁelds such as vision and
speech processing. Initially, the training and inference of deep learning were deployed on the central processing unit
(CPU). After exploring the parallel computing capability of the graphics processing unit (GPU), GPU became the most
common platform for deep learning [1]. However, with the growth of the deep learning model size, using GPU can
incur large power consumption that is not acceptable for many low power applications, such as edge computing.

To control the power consumption while maintaining the performance of executing large neural network models, the
dedicated hardware neural network accelerators are proposed, such as Eyeriss [2], NVDLA [3], Gemmini [4], etc.
However, deep learning algorithms keep updating frequently. As designing the hardware accelerators requires a long
period, frequent algorithm updating can introduce signiﬁcant engineering effort and cost. To avoid this, researchers try
to seek the design automation approaches for neural network accelerators.

∗The corresponding authors.

 
 
 
 
 
 
A PREPRINT - AUGUST 2, 2022

There are various studies of neural network accelerator design automation, such as DnnWeaver [5], DNNBuilder [6],
AutoDNNchip [7], ConfuciuX [8], etc. Each of them proposed an automatic design framework to search and generate
the optimized accelerator, given a deep learning inference task. Given the requirements from the user, the searching
problem to get the optimized design that satisﬁes the requirements is called the design space exploration (DSE)
problem, which is a step in neural network accelerator design automation. However, each of the previous DSE
algorithm [5, 6, 9, 7, 10, 11, 12, 8] iteratively uses a model to evaluate the quality of the searching result, and use a
searching algorithm to update the searching result given the evaluation. The iterative searching can incur large searching
time cost. Recent study [13] proposes a non-iterative approach by using multilayer perceptron (MLP) for DSE, but our
experiments show that MLP is still not competent under high dimension design space. Therefore, a new algorithm that
is able to effectively search the optimized design in a high dimension large design space is needed.

The neural network accelerator design automation is to generate a neural network accelerator, given the objectives
from the designers. This is similar to a kind of neural networks called generative adversarial networks (GANs), which
are usually used to generate images that satisfy the requirements from the users. Inspired by this, we investigate
the potential to apply GAN to DSE tasks, and propose a GAN-based DSE ﬂow. Because GAN usually has millions
of outputs for image pixels, when using GAN for outputting the conﬁgurations, it has the potential to support high
dimension large design space. Besides, the iterative searching is also avoided.

In our work, we propose GANDSE, which is a neural network accelerator design automation framework with the
GAN-based DSE. Different from previous work, GANDSE introduces a new DSE ﬂow that can effectively ﬁnd more
optimized design within a high dimension large design space. The detailed contributions of this work are in the
following.

• A neural network accelerator design automation framework named GANDSE is proposed to automatically generate
the synthesizable RTL code of the optimized neural network accelerator design given a deep learning task from the
user. The generated code can be implemented on an FPGA.

• A new DSE ﬂow based on GAN is proposed, which provides a new direction for handling DSE problems, and the

experiment results indicate the effectiveness.

• To ﬁt our neural network accelerator design automation problem, the modiﬁcations are proposed to the structure of
GAN, the training ﬂow, the loss function design, the encoding of the features, and the candidate selection after the
inference, etc.

• Throughout experiments are performed, which prove the effectiveness of the DSE algorithm in GANDSE. Compared
with other approaches including simulated annealing, multilayer perceptron, and deep reinforcement learning,
GANDSE ﬁnds more designs that satisfy the user’s requirements in negligible time.

In the following sections, Section 2 introduces the background of design space exploration and generative adversarial
networks. Section 3 discusses the related work. Section 4 introduces the motivation and design rationale of GANDSE.
The overview of GANDSE is introduced in Section 5, and the design details are then proposed in Section 6. After that,
the experimental results are analyzed in Section 7. Finally, Section 8 concludes the paper.

2 Background

2.1 Ordinary Neural Network Accelerator Design Space Exploration

Design space exploration (DSE) is the approach to search for the design, which satisﬁes the given objectives, in the
design space. For example, the DSE discussed in this work is searching for a set of conﬁgurations of the neural
network accelerators (such as the number of the processing elements (PEs), the size of SRAM, etc.), which makes the
accelerators satisfy some objectives such as the requirements to the latency, the power consumption, etc.

According to the survey [14], most of the DSE algorithms can be illustrated in Figure 1. Two critical components
are the conﬁguration updating algorithm and the design model. The design model is a model of the metrics in the
objectives. Given a set of conﬁgurations, the design model is able to output the values of the metrics (Note that for easy
of discussion, in the following paper, the values of the metrics in the objectives such as “Latency = 90 cycles” are also
referred as “objectives”). The design model can be constructed in various ways, including the analytical model, the
deep neural networks, etc. For the conﬁguration updating algorithm, it is to output a set of the conﬁgurations, given the
design space, the user’s objectives, and the latest results of the metrics in the objectives. Some typical algorithms are
exhaustive searching, simulated annealing, genetic algorithms, etc.

2

A PREPRINT - AUGUST 2, 2022

Figure 1: The ordinary DSE ﬂow and examples of neural network accelerator design automation.

However, the DSE ﬂow in Figure 1 greatly depends on iteratively searching the optimized design, which is relatively less
effective. In contrast, to avoid the iterative searching and explore the new direction for DSE, we propose a completely
new DSE ﬂow based on GAN in GANDSE.

2.2 Generative Adversarial Networks

Generative Adversarial Networks (GANs) [15] are the neural networks that are able to generate new data, which have
the same distribution as the training data. Speciﬁcally, conditional GANs (cGANs) [16] can generate the new data given
some speciﬁcations, which are called the conditional information. For example, a cGAN can be trained to generate new
images of speciﬁc kinds of animals with speciﬁc colors required by the users. Two critical neural networks involved
in cGAN are called the generator (G) and the discriminator (D). The generator can generate the new data while the
discriminator can tell if the data is real (from the training set) or fake (generated from the generator).

Figure 2: (a) An example of a cGAN’s architecture; (b) The training procedures of G; (c) The training procedures of D.
(Note that G has an additional input with small random numbers as noise, which is not shown in this ﬁgure.)

Figure 2 shows an example of using cGAN for image generation. Given the conditional data, G tries to generate an
image satisfying the condition and confuse D, while the goal of D is to discriminate if an image is a real image or a fake
image. When training G, G is given the conditional data (C) of the images in the training set, and outputs the generated
fake images (FI) accordingly. The fake images are then input to D, and the output classiﬁcation results (CR) indicate if
the images are real or not. The loss of G is to measure the difference between CR and “Real”, since it means G can
generate the images that seem real enough if D classiﬁes FI as “Real”. This loss is used to do the back propagation of G.
When training D, the real images (RI) from the training set and fake images (FI) generated from G are input to D. Then,
D will output the classiﬁcation results CRR and CRF on the real images and the fake images, respectively. The loss of D
is to measure the difference between CRF and “Fake”, and that between CRR and “Real”, which represent the precision
of the classiﬁcation. Then, this loss is used for the back propagation of D. As the relationship between the image and

3

ConfigurationsSimulationLogic SynthesisDeep Learning based ModelAnalytical Model...Exhaustive SearchingSimulated AnnealingGenetic Algorithm...ResultsConfiguration Updating AlgorithmObjectivesDesign SpaceDesign ModelLatency=90Power=300Given CNN ALatency≤100Power≤200PE numSRAM sizePE numSRAM sizePE num=16SRAM size=32KB⑤DiscriminatorReal/Fake...ConditionalData (C)...ImageImageloss=diff(CR, Real)GeneratorCR...CC...FIFICRF...CC...FIFICRRRIloss=diff(CRF, Fake)+diff(CRR, Real)(a)(b)(c)GDGGDDBack Prop.①②③④①②③Back Prop.④ConditionalData (C)A PREPRINT - AUGUST 2, 2022

the conditional data is similar to that between the accelerator conﬁgurations and the user’s objectives, cGAN might also
be used for the DSE. Because cGAN usually has millions of outputs for pixels, when using cGAN for outputting the
conﬁgurations, it has the potential to support high dimension large design space. Besides, since only one-time inference
for each conﬁguration is needed, it may save the time cost compared with running the iterative DSE algorithms.

3 Related Work

There have been various studies about neural network accelerator design automation. Two typical design frameworks
are DnnWeaver [5] and DNNBuilder [6]. DnnWeaver proposes an end-to-end design framework for generating the
RTL code of the neural network accelerator, given the deep neural network (DNN) model written by the users using a
proposed domain-speciﬁc language. The neural network accelerator is generated by setting the parameters of a Verilog
template. DNNBuilder is similar to DnnWeaver, except that the template of DNNBuilder contains multiple systolic
arrays, each of which processes one layer of the calculation of DNN inference. Meanwhile, AutoDNNchip [7] provides
multiple detailed IP templates and global architectures that are composed of the chosen IPs. It expands the design
space with higher granularity. DNNExplorer [10] uses a divide and conquer method for the DSE. In work [17], a fast
DSE algorithm is proposed for searching the optimized size of the FFT for CNN calculations. The optimized size is
found by calculating the intersections of the curves of the communication bound and computation bound. To avoid
the iterative searching, AIRCHITECT [13] uses multilayer perceptron (MLP) for generating the accelerator design.
However, our experiments show that only applying MLP ﬁnds limited optimized results given high dimension large
design space. ConfuciuX [8] uses reinforcement learning and genetic algorithm for searching the optimized dataﬂow
and hardware resource assignment. NAAS [11] proposes a design framework integrating the searches for the neural
network architectures, the mapping methods, and the neural network accelerators.

Besides the approaches using the templates, using high level synthesis (HLS) tools can also generate the neural network
accelerators. AutoDSE [18] proposes a DSE approach that uses the bottleneck-guided gradient optimizer to ﬁnd the
bottleneck of the design and optimize it accordingly. The work [9] optimizes the HLS conﬁgurations with the use
of exhaustive searching as the DSE algorithm. The work [12] uses the graph neural network (GNN) to increase the
precision of the design model, and thus helps the quality of the DSE.

However, the DSE algorithms used in the previous studies related to neural network accelerator design automation
either follow the ordinary DSE ﬂow in Figure 1 or have low dimension design space, so they cannot effectively ﬁnd the
accelerator design that is optimized enough with the growing of the design space dimensions.

4 Motivation and Design Rationale

In this section, we introduce the motivation and the core design rationale of GANDSE. The goal of the DSE for neural
network accelerator design automation is that, given the user’s objectives and the neural networks to be executed, a set
of the conﬁgurations of the neural network accelerator needs to be generated, so that the conﬁgured neural network
accelerator can satisfy the objectives.

Basically, to avoid the iterative searching of the ordinary DSE algorithms in Figure 1, similar as work [13], the DSE can
be realized by the basic neural networks named multilayer perceptron (MLP), where the inputs are the objectives and
the network parameters that interpret the neural network architectures, and the outputs are the conﬁgurations. The
training ﬂow of MLP is shown in Figure 3(a). First, the training set is generated by evenly choosing enough sets of
the network parameters and conﬁgurations, and using the design model for the corresponding objectives. Then, by
using the difference between the conﬁgurations in the training set and the generated conﬁgurations as the loss, the
MLP is trained to learn the distribution of the data in the training set. Ideally, the MLP is able to generate the satisﬁed
conﬁgurations during the inference.

However, there is a critical challenge in the approach in Figure 3(a). That is, if the generated conﬁgurations differ from
the corresponding conﬁgurations in the training set, it is regarded as a wrong set of conﬁgurations since it incurs the loss.
Nevertheless, there are usually multiple sets of the conﬁgurations that can satisfy the objectives for the given network
parameters. Although the generated conﬁgurations are different from those in the training set, it is still possible that the
generated conﬁgurations are also satisﬁed, or even better satisﬁed the objectives. The naive approach in Figure 3(a)
ignores this scenario, so it can miss many opportunities to generate better conﬁgurations.

To try to address this problem, a direct approach is to use the design model to verify if the generated conﬁgurations
satisfy the objectives, when the generated conﬁgurations are different from the training set. The training ﬂow is in
Figure 3(b). However, the loss function contains the corresponding objectives “G_Obj” by the generated conﬁgurations,
which relates to the weights of the MLP and the design model. This means the gradients of MLP also relate to the

4

A PREPRINT - AUGUST 2, 2022

Figure 3: The training ﬂows in different DSE schemes: (a) Barely using MLP; (b) Using MLP with the design model
for improving the loss function, which is not viable; (c) Using GAN, where the left and right parts are for training the
G and the D, respectively. (“Net. Param.”, “Obj.”, “G_Conﬁg.”, “G_Obj.” stand for network parameters, objectives,
generated conﬁgurations, objectives under the generated conﬁgurations, respectively.)

design model. As the design model is not necessarily analytic, it means the loss is not necessarily able to be back
propagated. Therefore, the approach in Figure 3(b) is not viable.

Finally, inspired by using the GAN (speciﬁcally, cGAN) for the text-image problem, where the scenario is similar
(creating the data that satisfy the objectives), we proposed a GAN-based DSE algorithm. The training ﬂow of the
proposed GAN is shown in Figure 3(c). Compared with Figure 3(b), now the MLP is called the generator (G), and the
design model is replaced by the discriminator (D). The D outputs if the conﬁgurations can satisfy the objectives given
the network parameters, which is called “satisfaction” (“Sat.” in Figure 3(c)). In this case, since the calculations in both
G and D are differentiable, the loss can be back propagated to train G. When training D, the generated conﬁgurations
from G are input to D to get the satisfaction prediction. Besides, the corresponding objectives “G_Obj” of the same
generated conﬁgurations are obtained from the design model. If “G_Obj” is overall better than the user’s objectives,
the actual satisfaction should be “True”, otherwise, it is “False”. Then the actual satisfaction is compared with the
satisfaction prediction to get the loss. Note that although the actual satisfaction in the loss function relates to the design
model, it does not relate to the weight in D. This means, the actual satisfaction is a constant for calculating the gradients
of D, and thus, the same problem in Figure 3(b) does not exist in this case. The G and D are trained alternatively based
on each other’s outputs, and thus the adversarial training is realized.

5 System Overview

Based on the discussion in Section 4, an end-to-end neural network accelerator design automation framework called
GANDSE is proposed to search for the highly optimized design in a high dimension large design space with small time
cost, where a novel DSE ﬂow is designed. GANDSE can generate the optimized neural network accelerator, which can
be used for the inference of the given neural networks while satisfying the user’s objectives of the latency and the power
consumption. The framework of GANDSE is shown in Figure 4, with three inputs and four phases involved.

The inputs are listed in the following.
• Design Template: The neural network accelerator design template with a set of conﬁgurations needed to be
determined, which is written in synthesizable RTL code. The conﬁgurations not only contain the architecture
parameters such as the SRAM size, but also contain the mapping strategies such as the tiling size of the neural
networks.

• Network Architecture: The layer architecture of the neural networks that need to be executed on the generated

neural network accelerator design. An example is shown in Figure 4.

• Objectives: The user’s requirements to the generated neural network accelerator when performing the inference of
the given network architecture. Our work uses latency and power as the examples, but the objectives are not limited
to them. Besides, the format of the objectives in GANDSE is “objective≤ x”, where x is a number given from the
user.

The four phases are brieﬂy described in the following.
• Training Phase: The design explorer in GANDSE is trained to be able to generate the optimized conﬁgurations of

the template. For each design template, the training phase only needs to be performed for one time.

• Parsing Phase: This phase parses user readable inputs to the numbers that can be directly used in the next phase.
• Exploration Phase: This phase generates the optimized conﬁgurations, which satisﬁes the user’s objectives.

5

...Net. Param.G_Config.Back Prop.Obj.loss=diff(G_Config., Config.)Net. Param.         Config.                      Obj.Fea. W. …   PE Num. Tile W. ... Latency Power16                 4              2               64          17...Training Set①②③...Net. Param.G_Config.Back Prop.Obj.loss=diff(G_Config., Config.)+diff(G_Obj., Obj.)①②③Design ModelNet. Param.G_Obj.G_Config.④⑤(a)(b)...Net. Param.G_Config.GBack Prop.Obj.loss=diff(G_Config., Config.)+diff(Sat., True)①②③Net. Param.Sat.G_Config.④⑤D......Obj.①④(c)Design ModelNet. Param.G_Obj.G_Config....Back Prop.loss=diff(Sat., G_Obj. is better than Obj.)②③MLPMLPNet. Param.G_Config.D...Obj.Sat.A PREPRINT - AUGUST 2, 2022

Figure 4: The overview of GANDSE framework. (The training phase only needs to be performed once for each design
template. The other 3 phases need to be perform for each set of network architecture and objectives.)

• Implementation Phase: This phase applies the generated conﬁgurations to the design template and generates the

synthesizable RTL code of the neural network accelerator.

After performing the training phase to set up GANDSE according to the given neural network accelerator design
template, GANDSE can automatically generate synthesizable RTL code of the neural network accelerator with
the conﬁgurations, which satisﬁes the user objectives when executing the neural networks with the given network
architecture. The circuits synthesized from the generated RTL code can be implemented on an FPGA.

The details of the four phases are introduced as follows.

5.1 Training Phase

Before GANDSE is able to be used, the critical component, which is the GAN in the Design Explorer, needs to be
trained as the ﬂow in Figure 3(c). Note that different from the GNN that is used for the design model in work [12],
the GAN in the design explorer is for creating new conﬁgurations according to the given the objectives, which is the
reverse of the design model, and has different challenges discussed in Section 4.

Given the design template, the design model is created to predict the latency and power. The latency is constructed by
using the rooﬂine model, while the power is constructed by combining the static power model and the dynamic power
model of the template. The design model is also veriﬁed by simulation and synthesis.

After the design model is constructed, an automatic Dataset Generator is employed to generate a dataset containing
various neural network architectures, conﬁgurations, and the latency and power consumption. The number of the data
in the dataset is large enough so that the data can evenly cover the design space. An example of the generated dataset is
shown in Figure 4.

Next, the GAN inside the design explorer is trained with the dataset, and then applied to the design explorer. Since the
generator of the GAN in the design explorer learns the mapping from the network parameters and objectives to the
conﬁgurations through the adversarial learning, it is able to generate the conﬁgurations given the user’s objectives in the
exploration phase.

5.2 Parsing Phase

In the parsing phase, the Network Parser can parse the user’s description that contains the information of the neural
network architectures, then generate the network parameters that can be directly input to the next phase along with
the objectives.

The user’s description is an abstract intermediate representation for the neural network architectures, such as the network
architecture example in Figure 4. It can be handwritten or extracted from deep learning frameworks such as Caffe [19]
or PyTorch [20].

6

Roofline ModelStatic Power ModelDynamic Power ModelDesign Templatemodule systolic_array(...)parameter PE_num=?parameter SRAM_size=?parameter Tile_Width=?…endmodulemodule …endmodule...Dataset GeneratorNetwork ArchitectureObjectivesLatency ≤ 100 cyclesPower ≤ 200mWFeature Map:  Width: 64  Height: 64  Input Channels: 8Kernel:  Width: 4  Height: 4  Output Channels: 16….ParserDesign ExplorerDesign SelectorRTL GeneratorGenerated RTL Codemodule systolic_array(...)parameter PE_num=16parameter SRAM_size=256parameter Tile_Width=16…endmodulemodule …endmodule...DatasetConfig.Net. Param.LatencyPowerGDNetwork ParametersConfigurationsCandidate Configuration SetsPE Number Tile Width …16                1632                8...PE Number Tile Width …16                16Feature Width Kernel Width …64                    4Net. Param.         Config.                      Obj.Fea. W. …   PE Num. Tile W. ... Latency Power1                   1              1               1            5…16                 4              2               64          17...256               16            8               1200      200  ...Design Model: Data: Software Program: Procedure: Model: DatasetSatisfied......Config.GDConfig.Obj.Net. Param.Obj.Net. Param.Legends and ExamplesTrainingFrameworkNetwork ArchitectureDesign TemplateObjectivesDesign ModelNetwork ParametersDatasetGenerated RTL CodeCandidate Configuration SetsConfigurationsDesign ModelDesign ModelTraining PhaseParsing PhaseExploration PhaseImplementation PhaseA PREPRINT - AUGUST 2, 2022

5.3 Exploration Phase

With the outputs of the parsing phase, the exploration phase can generate the optimized conﬁgurations of the neural
network accelerator design template that satisfy the user’s latency and power objectives. The Design Explorer contains
a GAN, which is trained in the ofﬂine phase. Based on the generator networks in the GAN, the design explorer tries to
generate multiple sets of the conﬁgurations including the architecture parameters and the mapping strategies, so that
when the conﬁgured neural network accelerators are implemented on the FPGA to execute the neural networks with the
given network parameters, the latency and power objectives can be satisﬁed. These sets of the conﬁgurations are called
Candidate Conﬁguration Sets. After the candidate conﬁguration sets are generated, the Design Selector then selects
the most optimized set of conﬁgurations with the use of the selection algorithm, which will be introduced in Section 6.

5.4

Implementation Phase

Using the design template with the optimized conﬁgurations including the architecture parameters and the mapping
strategies, the RTL Generator can generate the synthesizable RTL code, which can be implemented on FPGA boards
directly.

6 GAN-based Design Space Exploration

In this section, the critical part of GANDSE, which is the GAN-based DSE algorithm, is further elaborated. The DSE
algorithm relates to the training of the GAN, the design explorer (the inference of the GAN), and the design selector in
Figure 4.

6.1 Training and Inference Schemes for GAN-based DSE

The details of the training ﬂow proposed in Section 4 are further introduced. The proposed training scheme is shown in
Algorithm 1, during which the generator (G) and the discriminator (D) are trained alternately. The expectation is, G can
output the conﬁgurations that can satisfy the objectives for given the network parameters, when the conﬁgured neural
network accelerator is implemented on the FPGA. In contrast, the D takes network parameters, conﬁgurations, and
user’s objectives as the inputs. The D will decide if the objectives can be satisﬁed or not and outputs the satisfaction as
“True” or “False”, respectively. Note that if and only if all the objectives are better than the user’s requirements, the
satisfaction is “True”. The conﬁgurations are one-hot encoded, since most of the conﬁgurations of the architectures
and mapping strategies are not successive and only some speciﬁc numbers are meaningful. Otherwise, the generated
conﬁgurations might be decimal or negative, which can not be employed. The user’s objectives and the network
parameters are encoded as the binary numbers, and normalized by the standard deviation. We also employ the one-hot
encoding for the satisfaction output of D, similar to other neural networks classiﬁcation tasks.

The detailed training procedures are illustrated in Algorithm 1, where we use the mini-batch training (Lines 1-4). For
each sample s in the training set, the user’s objectives LOs and POs and the network parameters Nets are input to G
to get the generated conﬁgurations Con f igg (Line 5). The generated conﬁgurations are then input to D to get the
satisfaction Sat (Line 6). Next, Con f igg are input to the latency model Ml and power model Mp to get the actual latency
Lg and power consumption Pg (Lines 7-8), where Mp may need the latency Lg as input of the dynamic power model.
Then, three losses need to be calculated. The conﬁgurations loss Losscon f ig represents if the generated conﬁgurations
Con f igg are close to the actual conﬁgurations Con f igs of sample s. The critic loss Losscritic represents if D believes the
generated conﬁgurations Con f igg satisﬁes the user’s objectives or not. The discriminator loss Lossdis represents if the
classiﬁcation on the satisfaction made by D is correct or not. Losscritic is calculated by Line 9 by cross entropy loss.
The actual latency Lg and power Pg corresponding to the generated conﬁgurations are then compared with the objectives
LOs and POs of the sample s (Lines 10-16). If Lg and Pg are both better than LOs and POs, respectively, Con f igg is
regarded as the satisﬁed conﬁgurations, so there is no Losscon f ig (Line 11), and the output Sat of D should be “True”
(Line 12). Otherwise, Losscon f ig is calculated by Line 14, and Sat needs to be false (Line 15). After calculating the
losses for one batch of the samples, G and D are backpropagated and updated (Lines 18-19). Note that we employ the
weight wcritic for Losscritic to control the effect of D.

The trained G and D are used by the design explorer. When performing the inference, given the network parameters
and the objectives, G can output the conﬁgurations encoded by one-hot. Since ordinary one-hot encoding outputs
the probabilities of each choice of each conﬁguration, we use another number between 0 and 1 called Probability
Threshold (such as 0.2), to allow multiple sets of generated conﬁgurations output from G, which are the candidate
conﬁguration sets shown in Figure 4. For each conﬁguration, if the one-hot output of one choice excesses the probability
threshold, the choice is employed. Then the candidate conﬁguration sets are the combinations of all the employed
choices of all the conﬁgurations. For example, assume there are only two kinds of conﬁgurations that are the PE number

7

A PREPRINT - AUGUST 2, 2022

for Each batch b ∈ Settrain do

Losscon f ig, Losscritic, Lossdis ← 0
for Each sample s ∈ b do

Con f igg ← G(Nets, LOs, POs)
Sat ← D(Nets,Con f igg, LOs, POs)
Lg ← Ml(Ns,Con f igg)
Pg ← Mp(Ns,Con f igg, Lg)
Losscritic+ = E(Sat, True)/bs
if Lg ≤ LOs and Pg ≤ POs then

Algorithm 1 The proposed training scheme. (Nets: Network parameters of sample s; Con f igs: Conﬁgurations of s;
LOs: Latency objective of s; POs: Power objective of s.)
Require: G, generator. D, discriminator. Settrain, training set. iter, number of iterations. Ml, latency model. Mp, power
model. E, cross entropy loss function. bs, batch size.
1: for iter of training iterations do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end for
22: return G and D

end for
update G with Losscon f ig + wcritic × Losscritic
update D with Lossdis

Losscon f ig+ = E(Conigs,Con f igg)/bs
Lossdis+ = E(Sat, False)/bs

Losscon f ig+ = 0
Lossdis+ = E(Sat, True)/bs

end for

end if

else

and the SRAM size, the conﬁgurations are denoted as (PE number, SRAM size). If the one-hot outputs of “PE number
= 4”, “PE number = 16”, “SRAM size = 2KB”, and “SRAM size = 8KB” are more than 0.2, there are four sets of
conﬁgurations are candidates, which are (4, 2KB), (4, 8KB), (16, 2KB), and (16, 8KB).

6.2 Selection Algorithm

Since normally there are thousands of the candidate conﬁguration sets after performing the inference on G in the design
explorer, the design selector, which is a program, is proposed to select the most optimized set of the conﬁgurations.

In Algorithm 2, we keep updating two variables Lopt and Popt recording the optimized latency and power objectives
(Lines 1-2), respectively. For each set of the conﬁgurations Con f igg in the candidate conﬁgurations sets (Line 3), the
corresponding actual latency Lg and power Pg are calculated by the design model (Lines 4-5). The algorithm maintains
a variable U pdate to indicate if Lopt and Popt need to be updated (Line 6). For example, if Lopt and Popt have never
been updated, they need to be initialized for the ﬁrst time (Lines 7-8). Otherwise, there are three scenarios to perform
the updating. When both current optimized objectives (Lopt and Popt ) are worse/better than the user’s objectives (LO
and PO), it is the ﬁrst scenario. In this scenario, if the objectives (Lg and Pg) of Con f igg this time are both better
than Lopt and Popt , respectively, Lopt and Popt need to be updated (Lines 10-13). This can ensure that the optimized
objectives (Lopt and Popt ) keep changing for the better. In contrast, The second scenario is that the current optimized
latency Lopt is worse than the user’s objective LO, while the current optimized power Popt satisﬁes the objective PO. In
this case, if the latency Lg of Con f igg this time is better than the current optimized latency Lopt , and the power Pg still
satisﬁes the user’s objective PO, even though Pg might be worse than the current optimized power Popt , the updating is
still performed (Lines 15-18). This is because for the second scenario, the most signiﬁcant priority is to make all the
objectives satisﬁed. The third scenario (Lines 20-22), where the conditions of the latency and power are reversed, is
handled similarly to the second scenario. Finally, the optimized conﬁgurations Con f igopt that have the most optimized
latency Lopt and power Popt are returned.

8

A PREPRINT - AUGUST 2, 2022

else

else

end if

U pdate ← True

U pdate ← True

if Lg < Lopt and Pg < Popt then

if (Lopt > LO and Popt > PO) or (Lopt < LO and Popt < PO) then

Algorithm 2 The proposed selection algorithm of the design selector.
Require: Setcon f ig, candidate conﬁguration sets. Ml, latency model. Mp, Power model. Net, the network parameters
from the user. LO, the user’s latency objective. PO, the user’s power objective.
1: Lopt ← 0
2: Popt ← 0
3: for Each Con f igg ∈ Setcon f ig do
Lg ← Ml(Net,Con f igg)
4:
Pg ← Mp(Net,Con f igg)
5:
U pdate ← False
6:
if Lopt == 0 and Popt == 0 then
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31: end for
32: return Con f igopt

end if
if U pdate then
Lopt ← Lg
Popt ← Pg
Con f igopt ← Con f igg

if Lopt > LO and Popt < PO then

if Lg < Lopt and Popt < PO then

if Pg < Popt and Lopt < LO then

U pdate ← True

U pdate ← True

end if

end if

end if

end if

end if

else

7 Experiments

7.1 Experimental Setup

The experiments are conducted on Intel Xeon Silver 4210R CPU @ 2.40GHz with 64GB DDR4 memory, and NVIDIA
GeForce RTX 3090. The synthesis tool is Vivado 2018.3. The GAN of the framework is deployed and trained using
PyTorch [20].

7.1.1 Design Model

In our experiments, convolutional neural networks (CNNs) are used as the neural networks that will be executed on the
neural network accelerators to be generated, since CNN is one of the most popular neural networks. There are two
design models of the neural network accelerators involved in our experiments.

im2col model: This design model adopts the im2col computation pattern [21] similar to GPU computation. As shown
in Table 1, the main model explores the limits of design space and includes conﬁgurable parameters as many as possible.
The establishment of the main model is based on the rooﬂine model, which analyzes the relationship between DRAM
bandwidth, SRAM bandwidth, and on-chip computation. This model is used to illustrate the effectiveness of GANDSE
in high dimension design space.

DnnWeaver model: Based on the open-source DnnWeaver code [22] of the systolic array, the DnnWeaver model is
established. The model is calibrated by the simulation and synthesis results of code to ensure the output of the model
is aligned with the metrics of hardware employment. Note that the number of the conﬁgurations for the DnnWeaver

9

A PREPRINT - AUGUST 2, 2022

model is larger than that in the paper of DnnWeaver, since we extend the allowed conﬁgurations of the DnnWeaver
code. This model is used to make a comparison between GANDSE and DnnWeaver.

Table 1: The network parameters, the conﬁgurations, and the objectives used in our experiments. (*The conﬁguration is
not included in DnnWeaver model.)

Network
Parameters (CNN)
Conﬁgurations:
Architecture
Parameters

Conﬁgurations:
Mapping Strategies

Objectives

Input Channel, Output Channel, Output Width,
Output Height, Kernel Width, Kernel Height
PE Number, SRAM to DRAM Bandwidth*,
DRAM to SRAM Bandwidth*, Input SRAM
Size, Weight SRAM Size, Output SRAM Size
Tiling Input Channel*, Tiling Output Channel*,
Tiling Output Width*, Tiling Output Height*,
Tiling Kernel Width*, Tiling Kernel Height*

Latency, Power

Table 1 shows the detailed network parameters, conﬁgurations, and objectives of the design models in our experiments.

7.1.2 Dataset

The dataset is generated by Dataset Generator discussed in Section 5.1, where the values of the network parameters,
the architecture parameters, and the mapping strategies are evenly sampled. In this case, various CNN layers can be
learned. Then, the objectives are calculated by the design model. For im2col model, the dataset contains 23420 data for
the training and 1000 for the testing. For DnnWeaver model, the dataset contains 31250 data for the training and 1000
for the testing. An example part of the dataset is shown in Table 2, which is under im2col model.

Table 2: An example part of the dataset. (The abbreviations of the columns are the combinations of the ﬁrst letters of
the names in Table 1.)

IC OC OW OH KW KH PEN SDB DSB
64
32
128
32
...
...
128
64
512
64
...
...

2048
1024
...
512
2048
...

32
32
...
128
128
...

32
32
...
32
32
...

32
32
...
64
64
...

32
32
...
64
64
...

1
1
...
5
5
...

1
1
...
5
5
...

ISS WSS OSS
512
512
512
512
512
512
...
...
...
4096
2048
4096
4096
2048
4096
...
...
...

TIC TOC TOW TOH TKW TKH
64
16
...
64
16
...

128
16
...
256
64
...

16
16
...
16
64
...

16
64
...
64
64
...

4
5
...
5
1
...

1
5
...
1
1
...

L
21632
27112
...
831648
206978
...

P
349.30
310.80
...
7842.03
12451.89
...

7.1.3 Hyperparameters and Training

Table 3 listed the hyperparameters when training the proposed GAN in GANDSE, where both G and D are multilayer
perceptrons. The training phase only needs to be performed for each design template. In our experiments, we only
tune the number of the layers, the number of the neurons in each layer (which is the same in each layer, and is tuned
by setting it as 512/1024/2048/4096), and the learning rate (which is tuned by setting it in the range of 1e-5 to 1e-4).
Therefore, simple hyperparameter tuning is already enough to make GANDSE have good DSE results after training.

Table 3: The hyperparameters of the GAN. (wcritic is not listed as it is separately discussed in the results; The batch
sizes under both design models are 1024.)

Design Model

im2col
DnnWeaver

Hidden
Layers
11
14

Neurons
Per Layer
2048
2048

Learning Activation
Function
ReLU
ReLU

Rate
2e-5
2.5e-5

Optimizer

Adam
Adam

Hidden
Layers
11
11

Neurons
Per Layer
2048
2048

Learning Activation
Function
ReLU
ReLU

Rate
2e-5
2.5e-5

Optimizer

Adam
Adam

G

D

7.1.4 Compared DSE Algorithms

The following DSE algorithms are used for both im2col model and DnnWeaver model for comparison.

• Simulated annealing (SA). Simulated annealing is used for comparison. SA terminates once the user’s objectives

are satisﬁed, or the temperature is 3 × 10−8 as the initial one.

• Deep reinforcement learning (DRL). For DRL, similar to ConfuciuX [8], the policy gradient is employed in our
experiment, and a neural network is used as the actor network to update the underlying policy network. The states are

10

A PREPRINT - AUGUST 2, 2022

the current network parameters and conﬁgurations, and the actions are the modiﬁcations to the conﬁgurations. The
reward is obtained when the current action is approaching the states that satisﬁed the objectives. When the current
state already satisﬁes the objectives, a bonus is also added to the reward. After performing hyperparameter tuning,
the relatively better results are shown in our experiments.

• Large multilayer perceptron (Large MLP). Similar as AIRCHITECT [13], we also tested the results when only
applying the MLP as shown in Figure 3(a). Besides, we also apply the design selector to improve the results. To
show the effectiveness of GANDSE, the number of the parameters in the MLP is set to match that in the GAN, which
makes the MLP much larger than the G in the GAN. The training epochs of both GAN and large MLP are also set to
the same.

7.2 GANDSE Results Analysis

We ﬁrst analyze the effectiveness of the GAN-based DSE algorithm in GANDSE. The results are shown in Table 4,
where the bold numbers are the main results. When training GAN, different values of wcritic in Algorithm 1 are chosen.
Note that we allow 1% of the noise when evaluating all DSE algorithms in Table 4, which means if the latency/power
of the DSE output is ≤ 1% worse than the user’s objective, the objective is still regarded as satisﬁed.

Table 4: The DSE results of different methods under two design models. (†“Cand. Conﬁg." stands for candidate
conﬁgurations. ‡“Param." stands for parameters; The NN is the neural network used for DSE, but not the one to be
executed on the accelerator.)

Method

wcritic

SA
DRL
Large MLP

GAN

SA
DRL
Large MLP

GAN

l
o
c
2
m

i

)
e
g
r
a
L
(

l
e
d
o
M

r
e
v
a
e

W
n
n
D

)
l
l
a
m
S
(

-
-
-
0
0.5
0.7
1
1.2
-
-
-
0
0.5
1.0
1.2
1.5

Training
Time
-
133709s
47267s
78806s
116229s
118474s
116228s
118566s
-
27634s
11618s
173345s
36345s
36298s
36421s
25224s

# of Cand.
Conﬁg.†
-
-
10794.01
732.68
5360.19
5059.44
6861.21
9202.87
-
-
26.54
20.17
21.17
18.42
20.52
14.2

# of NN
Param.‡
-
93M
93M
93M
93M
93M
93M
93M
-
110M
110M
105M
105M
105M
105M
105M

DSE
Time
17.60s
0.08s
1.11s
0.09s
0.59s
0.74s
0.77s
1.23s
13.60s
<0.01s
<0.01s
<0.01s
<0.01s
<0.01s
<0.01s
<0.01s

# of Sat.
Results
786/1000
687/1000
839/1000
554/1000
944/1000
792/1000
893/1000
909/1000
936/1000
948/1000
862/1000
847/1000
938/1000
964/1000
945/1000
938/1000

Improvement
Ratio
0.2526
0.3399
0.3281
0.3286
0.3579
0.3405
0.3458
0.3399
0.0010
0.0019
0.0015
0.0017
0.0014
0.0015
0.0016
0.0016

Under im2col model, different wcritic values have different effects on GAN-based DSE algorithm. When wcritic = 0, the
D has no effect in the GAN, and the training scheme is the same as Figure 3(a) with MLP only. Given 1000 DSE tasks,
there are only around 50% results that satisfy the user’s objectives. In contrast, when wcritic = 0.5, the D can help the
learning of G, and almost all the results successfully satisfy the objectives. When further increasing wcritic, the results
start getting slightly worse but still much better than that without D. This is because the satisfaction classiﬁcation result
from D contributes too much to the loss of G. It becomes hard for G to directly learn from the training set. The results
show the effectiveness of GANDSE for ﬁnding the optimized conﬁgurations, and by hyperparameter tuning, GANDSE
can be further improved with a proper wcritic (0.5 under im2col model).
Table 4 shows the average runtime of each DSE task. The GAN-based DSE algorithm with wcritic = 0.5 only needs
< 1s to complete one DSE task, which is negligible in the hardware design ﬂow. Besides, the value of wcritic can affect
the runtime. This is mainly because of the different numbers of candidate conﬁgurations.

Besides, the improvement ratio is also analyzed. GANDSE can ﬁnd the conﬁgurations that outperform the user’s
objectives. Assume the latency and power objectives of the conﬁgurations from GANDSE are Lopt and Popt , respectively,
PO )2) when
and the user’s objectives are LO and PO, the improvement ratio is deﬁned as
Lopt ≤ LO and Popt ≤ PO. This can indicate how much the results of GANDSE outperform the objectives. Shown in
Table 4, with a proper wcritic, the results of GANDSE are more than 35% better than the user’s objectives on average.
This also proves the effectiveness of GANDSE.

LO )2 + ( Popt −PO

2 × (( Lopt −LO

(cid:113)

1

11

The training time of GANDSE is more than large MLP, but it is still acceptable. Once it is trained, it can be used for
any times for high quality designs given different objectives. Meanwhile, compared with the time period needed by the
hardware design, which are counted by months, the training time is already negligible.

A PREPRINT - AUGUST 2, 2022

Figure 5: The standard deviations of latency and power errors. (The numbers under “GAN” is wcritic.)

Figure 5 shows the standard deviations of latency and power errors. The latency and power errors are deﬁned as Lopt −LO
and Popt −PO
PO , respectively. It is indicated that for im2col model, GAN-based DSE has the least standard deviation
when wcritic = 0.5. This also proves that D can help the regression of the results of G, compared training G only when
wcritic = 0.

LO

We also test the effectiveness of GANDSE on DnnWeaver model, which has a much lower dimension of design space.
For DnnWeaver model, the optimized design can be obtained by training the GAN with wcricit = 1.0 in Table 4. If
wcricit = 0 and the training of G is not affected by the D, compared with the results of wcritic = 1.0, fewer results satisfy
the user’s objectives. Meanwhile, when wcritic is too large, the results are also slightly worse than wcritic = 1.0, but is
still much better than wcritic = 0. Besides, shown in Figure 5, when training GAN, the setup with the best results also
has small enough standard deviations for latency and power errors, proving the effectiveness of the regression. Although
SA has a smaller standard deviation, the number of the satisﬁed results and the improvement ratio are both worse than
GAN. More importantly, compared with GAN, SA performs worse in all metrics under the large design model. The
average runtime under DnnWeaver model is around 0.01s, which is negligible. The improvement ratio of GAN on
DnnWeaver model is not as obvious as large im2col model, but most of the results still satisfy the user’s objectives.
Overall, the results under DnnWeaver model also indicate the effectiveness of the proposed DSE algorithm and the
automation framework, and GANDSE has more advantages on the DSE within high dimension large design space.

7.3 Comparison with Other DSE Algorithms

We also compared GANDSE with other DSE algorithms including SA, DRL, and large MLP. The results are shown in
Table 4 and Figure 5. For Large MLP, although the parameter size of this MLP is the same as or even more than that of
the GAN in GANDSE, the large MLP performs worse compared with GAN. First, large MLP ﬁnds almost 100 fewer
satisﬁed results under both models, compared with GANDSE. One can notice that MLP has a much larger candidate
conﬁguration set size, which implies that MLP has less conﬁdence in the results. Next, large MLP also has a worse
improvement ratio. Large MLP have almost 1x more runtime under im2col model. Finally, the standard deviation of the
MLP results shown in Figure 5 is also much larger than that of GANDSE’s results. This proves that the effectiveness of
GAN-based DSE cannot be achieved by simply increasing the size of a MLP to the same level, without the guidance
of D network. For DRL, after performing hyperparameter tuning, the relatively better DRL results are obtained in
our experiments. Although DRL also has smaller runtime, it has worse results and standard deviations under both
im2col and DnnWeaver model, compared with the GAN-based DSE. Under DnnWeaver model, DRL has a slightly
better improvement ratio, but its improvement ratio under im2col model is still worse than GAN-based DSE. For SA, it
performs better under the small model than the large model, but the detailed number of satisﬁed results, the standard
deviation of the results, the runtime, and the improvement ratio are all worse than GAN-based DSE. In conclusion,
GAN-based DSE has overall better performance in the comparison.

12

SADRLLargeMLPGAN0.0GAN0.5GAN0.7GAN1.0GAN1.210-2100102DnnWeaver Modelim2col Model  Standard Deviation Latency Error Power ErrorSADRLLargeMLPGAN0.0GAN0.5GAN1.0GAN1.2GAN1.510-2100Standard Deviation  Latency Error Power ErrorA PREPRINT - AUGUST 2, 2022

Figure 6: The satisfaction rates of the objectives with different difﬁculties under im2col model.

Figure 7: The satisfaction rate of the objectives with different difﬁculties under DnnWeaver model.

7.4 DSE Results with Different Objective Difﬁculties

We also analyze the effectiveness of GANDSE given the user’s objectives with different difﬁculties, and the results are
shown in Figure 6 and Figure 7. There are multiple Pareto frontiers in the dataset, with latency and power Lpareto−i
and Ppareto−i. Assume the latency and power objectives are LO and PO, respectively, the related distance to Pareto

frontier is deﬁned as

, where (Lpareto−i, Ppareto−i) is the closest Pareto frontier to (LO,

√

(LO−Lpareto−i)2+(PO−Ppareto−i)2

(cid:113)

pareto−i+P2
L2

pareto−i

PO). Therefore, objectives with smaller related distances have higher difﬁculties to be satisﬁed. Figure 6 and Figure 7
indicate the percentage of the satisﬁed results (y-asix) given the topmost n% closest user’s objectives to Pareto frontiers
(x-asix). One can easily ﬁgure out that most DSE algorithms have better results when more objectives are far from

13

A PREPRINT - AUGUST 2, 2022

the Pareto frontiers, since the objectives are easier to be satisﬁed. Under im2col model (Figure 6), when selecting the
topmost 10% closest objectives to Pareto frontiers (including the frontiers), only GAN (wcritic = 0.5 and 1.0), Large
MLP, and SA has relatively better results. However, with the growing size of the objectives, GAN (wcritic = 0.5 and
1.0) has better satisfaction rate, while the curves of other approaches ﬂuctuate and become worse than GAN. Since
GAN with wcritic = 0 has no D, it is not a real GAN structure and the results are worse than those with D involved. Note
that with objective difﬁculty decreasing, Large MLP has even worse results compared with difﬁcult objectives. This
might be because it strictly learns the architectures corresponding to the objectives in the training set, and misses the
opportunities to learn better architectures, which has been discussed in Section 4. This is even worse for easy objectives
since there are more such opportunities. Similar as im2col model, under DnnWeaver model (Figure 7), the results also
prove the effectiveness of GAN, even with difﬁcult objectives. In contrast, other approaches performs worse when the
objectives are difﬁcult to be satisﬁed.

Figure 8: The latency and power improvement results (dots) of the different DSE algorithms based on im2col model. A
dot with non-negative coordinate of an axis represents the satisfaction of the corresponding objective.

Figure 9: The latency and power improvement results (dots) of the different DSE algorithms based on DnnWeaver
model. A dot with non-negative coordinate of an axis represents the satisfaction of the corresponding objective.

7.5 Results Distribution Analysis

In this subsection, we analyze the result distribution of different DSE algorithms, which is shown in Figure 8 and
Figure 9. Each dot is a result of the DSE, which indicates the improvements of the latency/power of the DSE
output conﬁgurations compared with the user’s objectives. The x-axis and y-axis represent log2( LO
),
Lopt
respectively. For Figure 8(a), under im2col model, when wcritic = 0, the D has no effect in the GAN. One can easily
ﬁgure out that there are lots of results in the second (or fourth) quadrant, which represent they fail to satisfy the user’s
latency (or power) objective. In contrast, when wcritic = 0.5 in Figure 8(b), the D can help the learning of G, and the
results are well regressed into the ﬁrst quadrant. However, with the increasing of wcritic to 1.0 in Figure 8(c), the results
are getting slightly worse. This is because it becomes harder for G to directly learn from the training set than from D.
For Figure 9(a)-(c), since the design space has a much smaller dimension, the results distribution under DnnWeaver
model is different from that under im2col model, but more results are still in the ﬁrst quadrant with the effect of D than
that without D (wcritic = 0). The best wcritic can be obtained from hyperparameter tuning.

) and log2( PO
Popt

For im2col model, compared with Figure 8(a), large MLP in Figure 8(d) has similar distribution, but there are more dots
successfully regress to the ﬁrst quadrant due to larger number of parameters. However, for large MLP, there are still
obvious distributions out of the ﬁrst quadrant, which make the results worse than GAN-based DSE in Figure 8(b) and
(c). This also proves the effectiveness of D even with fewer neural network parameters. For other previous approaches
including DRL and SA, they both have worse distributions compared with GAN-based DSE since there are more dots
that fail to fall in the ﬁrst quadrant. For DnnWeaver model, the related distributions are also similar to im2col model.
In Figure 9(d) and (e), the density of the results in the second and fourth quadrants are obviously higher than that in

14

(cid:20)(cid:19)(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(a) GAN wcritic = 0(cid:20)(cid:19)-(cid:20)(cid:19)(cid:19)-(cid:20)(cid:19)(cid:20)(cid:19)(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(b) GAN wcritic = 0.5(cid:20)(cid:19)-(cid:20)(cid:19)(cid:19)-(cid:20)(cid:19)(cid:20)(cid:19)(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(c) GAN wcritic = 1.0(cid:20)(cid:19)-(cid:20)(cid:19)(cid:19)-(cid:20)(cid:19)(cid:20)(cid:19)(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(d) Large MLP(cid:20)(cid:19)-(cid:20)(cid:19)(cid:19)-(cid:20)(cid:19)15(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(e) DRL15-15 (cid:19)-15(cid:20)(cid:19)(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(f) SA(cid:20)(cid:19)-(cid:20)(cid:19)(cid:19)-(cid:20)(cid:19)2(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(a) GAN wcritic = 02-2 (cid:19)-2(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(b) GAN wcritic = 1.0(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(c) GAN wcritic = 1.2(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(d) Large MLP(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(e) DRL(cid:47)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(f) SA22-2 (cid:19)-222-2 (cid:19)-222-2 (cid:19)-222-2 (cid:19)-222-2 (cid:19)-2Figure 9(b), which indicates fewer results are satisﬁed when using large MLP and DRL. For SA in Figure 9(f), although
the results are concentrated to the middle, there are still fewer results in the ﬁrst quadrant compared with GAN. The
distribution comparison also shows that GAN can help GANDSE ﬁnd better results.

A PREPRINT - AUGUST 2, 2022

Figure 10: The training losses of the GAN under im2col model.

Figure 11: The training losses of the GAN under DnnWeaver model.

7.6 Training Losses

More training insights can be analyzed by the losses recorded through the training, which are shown in Figure 10 and
Figure 11. Under im2col model, to analyze the G’s output behaviors, when wcritic = 0, the D is stilled trained, but the
output of D never affect the training of G. In this case shown in Figure 10(a), the critic loss of G keeps increasing,
which indicates that D believes some sets of the conﬁgurations output from G cannot satisfy the user’s objectives. When
wcritic = 0.5 shown in Figure 10(b), since the classiﬁcation result of D is used to train G, the critic loss of G is much
lower than that in Figure 10(a). At the same time, all the losses become regressed, indicating the balance of G and D in
the GAN. When wcritic = 1.0 shown in Figure 10(c), the critic loss cannot quickly regress as the classiﬁcation results
of D contribute too much and the G cannot easily learn from the training set. Similar conclusions can also be made
from the training losses under DnnWeaver model shown in Figure 11. The results in Figure 10 and Figure 11 show that
different wcritic has different impacts on the training of the proposed DSE algorithm.

8 Conclusion

In conclusion, this work proposed a neural network accelerator design automation framework GANDSE. With the
neural network accelerator template that can be conﬁgured, given the user’s objectives and the architecture of the
neural networks that need to be executed on the neural network accelerator, GANDSE is able to effectively search the
optimized conﬁgurations of the neural network accelerator that can satisfy the objectives. A novel DSE algorithm, which
is based on GAN, is proposed and applied in GANDSE. Compared with the previous work including using multilayer
perceptron and deep reinforcement learning, the GAN-based DSE algorithm can effectively ﬁnd more optimized designs
within negligible time in the high dimension large design space. The experiments proved the effectiveness of GANDSE.

15

8002468100Loss6004000200080060040002008006004000200LossLossEpoch  Configuration Loss  Epoch  Critic Loss(a)wcritic = 0(c)wcritic = 1.0(b)wcritic = 0.5 Discriminate LossEpoch   2468100246810024602460246Loss020010030002001003000200100LossLossEpoch  Configuration Loss  Epoch   Critic Loss(a) wcritic = 0(c) wcritic = 1.2(b) wcritic = 1.0Epoch Discriminate loss300400400400A PREPRINT - AUGUST 2, 2022

References

[1] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan

Shelhamer. cuDNN: Efﬁcient Primitives for Deep Learning. arXiv Preprint, 2014.

[2] Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and Vivienne Sze. Eyeriss: An Energy-Efﬁcient Reconﬁgurable
Accelerator for Deep Convolutional Neural Networks. IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017.

[3] NVDLA. http://nvdla.org/, 2018.

[4] Hasan Genc, Ameer Haj-Ali, Vighnesh Iyer, Alon Amid, Howard Mao, John Wright, Colin Schmidt, Jerry Zhao,
Albert Ou, Max Banister, Yakun Sophia Shao, Borivoje Nikolic, Ion Stoica, and Krste Asanovic. Gemmini: An
Agile Systolic Array Generator Enabling Systematic Evaluations of Deep-Learning Architectures. arXiv preprint,
2019.

[5] Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung Kim, Chenkai Shao, Asit Mishra,
and Hadi Esmaeilzadeh. From High-Level Deep Neural Models to FPGAs. IEEE/ACM International Symposium
on Microarchitecture, pages 1–12, 2016.

[6] Xiaofan Zhang, Junsong Wang, Chao Zhu, Yonghua Lin, Jinjun Xiong, Wen-mei Hwu, and Deming Chen.
DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs.
IEEE/ACM International Conference on Computer-Aided Design, pages 1–8, 2018.

[7] Pengfei Xu, Xiaofan Zhang, Cong Hao, Yang Zhao, Yongan Zhang, Yue Wang, Chaojian Li, Zetong Guan, Deming
Chen, and Yingyan Lin. AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs and
ASICs. ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, page 40–50, 2020.

[8] Sheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. ConfuciuX: Autonomous Hardware Resource Assignment
for DNN Accelerators using Reinforcement Learning. IEEE/ACM International Symposium on Microarchitecture,
pages 622–636, 2020.

[9] Kamel Abdelouahab, Cédric Bourrasset, Maxime Pelcat, François Berry, Jean-Charles Quinton, and Jocelyn
Serot. A Holistic Approach for Optimizing DSP Block Utilization of a CNN Implementation on FPGA. ACM
International Conference on Distributed Smart Camera, page 69–75, 2016.

[10] Xiaofan Zhang, Hanchen Ye, Junsong Wang, Yonghua Lin, Jinjun Xiong, Wen-mei Hwu, and Deming Chen.
DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of FPGA-Based DNN Accelerator.
IEEE/ACM International Conference on Computer-Aided Design, pages 1–9, 2020.

[11] Yujun Lin, Mengtian Yang, and Song Han. NAAS: Neural Accelerator Architecture Search. ACM/IEEE Design

Automation Conference, pages 1051–1056, 2021.

[12] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. GNN-DSE: Automated Accelerator

Optimization Aided by Graph Neural Networks. arXiv Preprint, 2021.

[13] Ananda Samajdar, Jan Moritz Joseph, Matthew Denton, and Tushar Krishna. AIRCHITECT: Learning Custom

Architecture Design and Mapping Space. arXiv preprint, 2021.

[14] Benjamin Carrion Schafer and Zi Wang. High-Level Synthesis Design Space Exploration: Past, Present, and
Future. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 39(10):2628–2639,
2020.

[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,

and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing Systems, 2014.

[16] Mehdi Mirza and Simon Osindero. Conditional Generative Adversarial Nets. arXiv preprint, 2014.

[17] Hanqing Zeng, Ren Chen, Chi Zhang, and Viktor Prasanna. A Framework for Generating High Throughput CNN
Implementations on FPGAs. ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, page
117–126, 2018.

[18] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong. AutoDSE: Enabling Software Programmers to
Design Efﬁcient FPGA Accelerators. ACM Transactions on Design Automation of Electronic Systems, 27(4):1–27,
2022.

[19] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional Architecture for Fast Feature Embedding. ACM International Conference
on Multimedia, pages 675–678, 2014.

[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,

16

A PREPRINT - AUGUST 2, 2022

Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information
Processing Systems, 2019.

[21] Kumar Chellapilla, Sidd Puri, and Patrice Simard. High Performance Convolutional Neural Networks for

Document Processing. International Workshop on Frontiers in Handwriting Recognition, 2006.

[22] DnnWeaver v2.0. http://dnnweaver.org/, 2016.

17

