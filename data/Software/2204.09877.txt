2
2
0
2

r
p
A
1
2

]
E
S
.
s
c
[

1
v
7
7
8
9
0
.
4
0
2
2
:
v
i
X
r
a

Non-autoregressive Model for Full-line Code Completion

Fang Liu1,2 , Zhiyi Fu1,2 , Ge Li1,2∗ , Zhi Jin1,2∗ , Hui Liu3 , Yiyang Hao4
1Key Laboratory of High Conﬁdence Software Technologies (Peking University), Ministry of Education
2 School of Computer Science, Peking University, Beijing, China
3Beijing Institute of Technology
4Silicon Heart Tech Co., Ltd
{liufang816, ypfzy, lige, zhijin}@pku.edu.com, liuhui08@bit.edu.cn, haoyiyang@nnthink.com

Abstract

Code completion tools are frequently used by soft-
ware developers to accelerate software develop-
ment by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full
line of code) has been proved more efﬁcient than
predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRe-
gressive (AR) decoders to generate tokens in a left-
to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previ-
ously generated tokens, which leads to high latency
in inference. To improve the efﬁciency and accu-
racy of full-line code completion, in this paper, we
propose a Non-AutoRegressive (NAR) model for
code completion boosted by a syntax-aware sam-
pling strategy. Our experimental results on two
widely used datasets suggest that our model out-
performs both AR and NAR baselines on full-line
code completion, and it is faster than the AR model
with up to 9× speed-up.

1 Introduction
Code completion is one of the most useful features in the In-
tegrated Development Environments (IDEs), improving the
software development efﬁciency by suggesting future code
In recent years, as the development of machine
snippets.
learning and deep learning technologies and easy-to-acquire
open-source codebases, researchers have started to tackle
code completion by learning from large-scale code corpora.
Various Language Models (LM) have been employed for
code completion, including N-gram [Hindle et al., 2016],
RNN [Li et al., 2018], and Transformer based models [Liu
et al., 2020]. Several popular industry code completion tools,
e.g., Tabnine [2018], aiXcoder [2018], and Copilot [2021],
also rely on LMs to provide suggestions for whole lines or en-
tire functions in a token-by-token manner. Recently, Wang et
al. [2020] further explored full-line code completion. Given
a partially completed code snippet, they built a Transformer-
based LM to predict the next line of code. Although such
code completion algorithms/tools can suggest longer code

∗Corresponding authors.

snippets, they intrinsically are suffering from inefﬁciency be-
cause of the autoregressive decoding process, where each to-
ken is generated conditioned on all the previously generated
tokens. Consequently, the generation is not parallelizable and
thus particularly slow. However, the efﬁciency of the gener-
ation process is of great importance for code completion
because it is expected to respond instantly on the developer’s
own devices.

Tokens within a code line have the potential to be predicted
concurrently, although existing code completion algorithms
predict them one by one (from left to right) by exploiting
all tokens previously predicted. For example, multiple argu-
ments of the same method are independent of each other, and
thus they could be predicted independently and concurrently.
In Python code, you can even change the order (position) of
the arguments if a function with the keyword ‘arguments’ is
called. For example, Func(arg1=a, arg2=b) is equivalent
to Func(arg2=b, arg1=a). We also argue that even if there
exists strong dependency among tokens within a code line,
the dependency is not necessarily left-to-right. A typical ex-
ample is “one line IF-ELSE statement” (Python), formed as:
value_1 if condition else value_2. The value 1 is
dependent on the following condition (on its right-hand side),
instead of the verse. We also perform in-depth analysis in
section 2 to verify that the dependency among code tokens
is weaker than that among tokens in natural languages where
parallelized generation has been employed successfully. Our
analysis in section 2 also suggests that predicting code tokens
from right-to-left or the verse (left-to-right) results in compa-
rable performance. Based on those observations, we conclude
that generating tokens in parallel for code completion is pos-
sible and reasonable.

Non-autoregressive (NAR) model, proposed by Gu et al.
[2018], has been successfully applied to Natural Machine
Translation (NMT) to generate tokens non-autoregressively
through a parallel decoding process:

LN AR =

T
(cid:88)

t=1

log p(yt|X; θ)

(1)

It allows an order of magnitude lower latency during infer-
ence. There are two kinds of NAR models: Fully NAR mod-
els [Gu et al., 2018; Ma et al., 2019] and iterative NAR mod-
els [Ghazvininejad et al., 2019; Kasai et al., 2020]. These

 
 
 
 
 
 
NAR models can speed up the inference process of NMT sig-
niﬁcantly compared with autoregressive models. However,
NAR models are hard to train without specially designed
training strategies, as the left-to-right inductive bias among
target tokens is ignored during decoding.

Considering the feasibility of parallel generation of code
and inspired by the success of Non-AutoRegressive gener-
ation in NLP, in this paper, we propose a Syntax-Aware
Non-AutoRegressive model (SANAR) for code completion.
Speciﬁcally, we propose an adaptive and syntax-aware sam-
pling strategy to boost the training process of SANAR, which
dynamically glances some code tokens from the target se-
quence according to their difﬁculties and token types. The
better the model is trained, the fewer code snippets would be
glanced. Once the model is well-trained, it can generate the
whole line of tokens in a single pass. Our experimental results
show that our sampling strategy results in obvious improve-
ment in performance on both Python and Java code com-
pletion. The proposed approach is signiﬁcantly faster than
the widely used Auto-Regressive models, achieving 5 ∼ 6×
speed-up on average, and 9× for long targets.

To summarize, the major contributions of our work are as

follows:

• An empirical study whose results suggest that it is po-
tentially practical to predict code tokens in parallel.

• A novel approach for full-line code completion. To the
best of our knowledge, it is the ﬁrst non-autoregressive
approach to code completion. We boost the approach by
an adaptive and syntax-aware sampling strategy, that is
specially designed for source code.

• A large-scale evaluation of the proposed approach
whose results suggest that our approach outperforms
both AR and NAR baselines on full-line code comple-
tion, and signiﬁcantly reduces the inference time com-
pared with the AR model.

2 Dependency among Generated Code Tokens
We argue that the dependency among the generated tokens in
full-line completion is not as strong as in NMT (where NAR
has been successfully applied), and the left-to-right genera-
tion order is not always optimal for code completion. To ver-
ify this assumption, we conduct experiments to analyze the
dependency among target tokens in the full-line code com-
pletion task in this section.

2.1 Reversing the Order of Code Generation
To verify our conjecture of left-to-right is not always the opti-
mal order for code completion, we conduct an experiment to
analyze the impact of different generation orders. We employ
standard auto-regressive Transformer architecture to perform
left-
full-line code completion using two reverse orders:
to-right (Transformer-L2R) and right-to-left (Transformer-
R2L). Table 1 presents the results. For most of the cases, the
performances of the L2R model and R2L model are compara-
ble. For Python language, 2.65% and 2.47% of programs can
only be correctly generated by L2R and R2L, respectively;
6.57% and 7.21% of the programs can only be approximately

Table 1: The percentage of the code sequence that can be correctly
handled by different models. EM stands for exact match, and ES
stands for edit similarity.

only Transformer-L2R

PY

only Transformer-R2L

Both

only Transformer-L2R

JAVA

only Transformer-R2L

Both

Metrics

Percentage

EM
ES>0.5
EM
ES>0.5
EM
ES>0.5

EM
ES>0.5
EM
ES>0.5
EM
ES>0.5

2.65%
6.57%
2.47%
7.21%
13.59%
60.39%

3.28%
6.71%
3.86%
8.22%
26.15%
60.43%

Code Completion

NMT

)
p
(
R

0.7

0.65

0.6

0.55

0.2

0.3

0.4

0.5

P

Figure 1: The attention density ratio R(P) under different masking
probability P in code completion and NMT.

generated (edit similarity >50%) by L2R and R2L, respec-
tively. For Java programs, we can also get similar results.
Therefore, we conclude that left-to-right is not always the op-
timal order for code completion. Thus generating code in
parallel (non-autoregressively) could be a better choice.

2.2 Quantitative Dependency among Code Tokens

In this section, we measure the quantitative dependency
among the generated tokens by attention density and compare
the dependency among code tokens against that among nat-
ural language tokens (in NMT tasks). NMT task is selected
for comparison because NAR models have been successfully
applied to it.

To characterize and quantify the target-token dependency,
inspired by Ren et al. [2020], we build Dependency-Analysis-
Model (DAM) to measure the dependency among target to-
kens using the ratio of the attention weights on target con-
text over that on full (both source and target) context when
predicting a target token, where a bigger ratio indicates a
larger dependency among target tokens. Speciﬁcally, we
use masked language modeling task [Devlin et al., 2019] to
train DAM. We employ mix-attention [He et al., 2018] to cal-
culate the attention weights, where source tokens can only
attend to source tokens with self-attention and target tokens
can attend to all source and target tokens with mix-attention.
During training, we randomly mask the tokens on the target
side, and the model is trained to predict the original value
of these tokens. After the model has been trained, we can
measure the target token dependency based on the ratio of at-

Figure 2: The training procedure with syntax-aware sampling strategy in SAS.

tention weights αi on target context over that on full context
when predicting a speciﬁc target token yi, which is deﬁned
as:

αi =

1
N

(cid:80)N

1
N
j=1 Ai,j + 1
M

(cid:80)N

j=1 Ai,j

(cid:80)N +M

j=N +1 Ai,j

(2)

where Ai,j denotes the attention weights from token i to to-
ken j in mix-attention, and j ∈ [1, N ] represents the target
token, and j ∈ [N + 1, N + M ] represents the source token.
M and N is the length of source and target input, respec-
tively. (cid:80)N +M
j=1 Ai,j = 1. αi represents the ratio of attention
density on target context when predicting target token i. For
a given masking probability P , the ﬁnal attention density ra-
tio R(P ) is calculated by averaging αi over all test data. As
seen in Figure 1, the attention density ratio for NMT is big-
ger than code generation for all masking probability P , which
demonstrates the dependency among the target tokens in code
completion is less than NMT.

We conclude based on the preceding analysis that it is po-

tentially practical to predict code tokens in parallel.

3 SANAR
3.1 Overview
In this section, we present our model SANAR in detail. We
build SANAR to perform full-line code completion in par-
allel, which uses conditional independent factorization for
the target tokens. Speciﬁcally, given the contextual code se-
quence X = {x1, x2, ..., xm}, the non-autoregressive full-
line code completion task is to predict a sequence of tokens
Y = {y1, y2, ..., yn} that form a complete statement in a non-
autoregressive way:

P (y1, ..., yn|x1, x2, ..., xm) =

n
(cid:89)

t=1

P (yt|x1, x2, ..., xm) (3)

Figure 2 shows the architecture and the trainig procedure of
our model. SANAR adopts the encoder-decoder transformer
architecture: a context encoder that does self-attention, and
a generated-code decoder that has one set of attention heads
over the encoder’s output and another set (self-attention) for
the generated code. During the training procedure, we pro-
pose an adaptive syntax-aware sampling strategy, which en-
able SANAR to generate target code sequences in a single-
pass during inference procedure by gradual training.

3.2 Training
During training, SANAR adopts an adaptive Syntax-Aware
Sampling (SAS) strategy to dynamically glance code snip-
pets from the target sequence depending on its difﬁculty
and the tokens’ syntax types, aiming to incorporate the ex-
plicit syntactic information of the program. To achieve the
sampling strategy, SANAR performs decoding twice dur-
ing training. As shown in Figure 2, the ﬁrst decoding is
performed in a fully-NAR way, where the input to the de-
coder H = {h1, h2, ..., hn} are copied from the encoder
output using soft-copy [Wei et al., 2019], which maps the
encoder embeddings S = {s1, s2, ..., sm} into target length
H = {h1, h2, ..., hn} depending on the distance relationship
between the source position i and the target position j. The
initial predicted tokens ˆY are predicted as:

ˆY = fdec(soft-copy(fenc(X; θ(cid:48))); θ)
(4)
The prediction accuracy indicates the difﬁculty of ﬁtting cur-
rent target. In the second decoding, SANAR samples words
of the targets as the extra decoding input by SAS sampling
according to the ﬁrst decoding results and the syntax infor-
mation of the target tokens, and learn to predict the rest words
that are not selected. It is important to note that only the sec-
ond decoding will update the model parameters.

The training objective is to maximize the following:

LSAS =

(cid:88)

log P (yt|X, SAS(Y, ˆY , T ); θ)

yt∈Y \SAS(Y, ˆY ,T )

(5)
where ˆY is the initially predicted tokens in a fully-NAR de-
coding way. T is the syntax type sequence of the target se-
quence, and SAS(Y, ˆY , T ) is a subset of tokens selected via
the adaptive SAS strategy.

3.3 Syntax-Aware Sampling Strategy
Syntax-Aware Sampling (SAS) strategy is an important com-
ponent of SANAR, which adaptively selects the positions of
tokens from the target sequence depending on its difﬁculty
and the syntax types of the tokens. The selected tokens can
provide “correct” information from the ground-truth target,
thus can reduce the burden of decoder to predict the rest non-
selected tokens in the training phase. SAS samples more to-
kens for SANAR to glance at the beginning, and then reduces
the sampling number gradually, which helps SANAR to learn,
eventually, how to predict the whole line code snippet without
seeing any ground truth tokens in one pass.

Encoder𝑥𝑥1𝑥𝑥2𝑥𝑥3𝑥𝑥4𝑠𝑠2𝑠𝑠3𝑠𝑠4𝑠𝑠1Shared Decoderℎ2ℎ3ℎ4ℎ1�𝑦𝑦2�𝑦𝑦4SAS Module𝑦𝑦1𝑦𝑦3Soft Copyℎ5�𝑦𝑦5�𝑦𝑦2�𝑦𝑦3�𝑦𝑦4�𝑦𝑦1�𝑦𝑦5ℎ2ℎ4ℎ5(𝑦𝑦1,𝑡𝑡1)(𝑦𝑦5,𝑡𝑡5)⋯𝑆𝑆𝑆𝑆𝑆𝑆(𝑌𝑌,�𝑌𝑌,𝑇𝑇)(𝑦𝑦2,𝑡𝑡2)Shared DecoderFormally, given the context code sequence X, its predicted
token sentence ˆY , the ground truth Y , and the token’s syn-
tax type sequence of the target T , the goal of SAS strategy
SAS(Y, ˆY , T ) is to obtain a subset of tokens sampled from
Y . Speciﬁcally, there are two steps:
1) Deciding the sampling numbers N depending on the difﬁ-
culty of correctly generating the target token sequence:

N = λ · dis(Y, ˆY )

(6)

N is computed by comparing the difference between ˆY and
Y , and we adopt the Hamming distance as the metric follow-
ing Qian et al. [2021], dis(Y, ˆY ) = (cid:80)T
t=1(yt (cid:54)= ˆyt). λ is the
sampling ratio, which is a hyper-parameter to ﬂexibly control
the number of sampled tokens. More tokens will be selected
and fed as input of the second-pass decoding if the network’s
initial prediction is less accurate. Thus, the sampling num-
ber can be decided adaptively considering the current trained
model’s prediction capability and the training sample’s com-
plexity.
2) Sampling N tokens from the target sequence. The most
direct method is to randomly select N tokens from Y like
in Qian et al. [2021]. However, we argue that considering
the syntactic information of the code explicitly during token
selection will be beneﬁcial for understanding the programs,
and thus can help train the decoder to predict the rest tokens
more precisely.
In programs, the keyword, identiﬁers, and
operators contain more symbolic and syntactic information
than other tokens (for example, literals and separators in Java,
like ‘;’, ‘{’, ‘}’, etc.). Glancing these tokens will help a lot.

To capture the syntactic information of the code during
the sampling procedure, we present a Hybrid-Syntax-Guided
(HSG) sampling strategy. Speciﬁcally, 1 − p of the time, the
sampling is performed randomly, where the tokens are ran-
domly selected from the target sequence; and p of the time,
tokens are selected depending on their syntax-types1, we ran-
domly select K ≤ N/2 keywords, I ≤ N/4 identiﬁers, and
O ≤ N/4 operators from Y , where K + I + O ≤ N 2. The
reason for introducing randomness (randomly sampling for
1−p of the time) in the sampling process is to enable SANAR
to explore more interdependency among target tokens. Com-
pared with the totally randomly sampling strategy in Qian et
al. [2021], our Hybrid-Syntax-Guided sampling strategy can
increase the probability of glancing the keyword, operators,
and identiﬁers, which can help the SANAR capture the pro-
gram’s syntactic and semantic information better.

Finally, we can obtain a subset of words sampled from Y :

SAS(Y, ˆY , T ) = HSG(Y, N, p)

N = λ · dis(Y, ˆY )

(7)

Inference

3.4
As SANAR is well-tuned, it will adaptively reduce the per-
centage of sampling, making sure that the trained model
could learn to generate the whole line of code in the single

1p is a hyper-parameter to control the probability of syntax-

guided sampling, the best setting is p = 30%

2The number of these speciﬁc tokens might be not enough

Table 2: Dataset statistics.

Python

Java

# of training pairs
# of testing pairs
Avg. tokens in cxt
Avg. tokens in tgt

7,531,208
3,693,213
90.8
9.4

12,993,112
671,236
71.2
6.9

pass. Thus, the inference of SANAR is fully parallel with
only a single pass.

In traditional left-to-right code completion, where the tar-
get sequence is predicted token by token, the length of the
sequence can be determined by predicting a special EOS
(end of sentence) token. However, for NAR-based generation
where the entire sequence is predicted in parallel, the length
of the target sequence must be determined in advance. To
achieve this, we follow Ghazvininejad et al. [2019] to add an
additional [LENGTH] token to the source input, and the en-
coder output for the [LENGTH] token is used to predict the
length, formed as a max-target-length classiﬁcation task. We
use 1,000 as the max length. The loss is added to the cross-
entropy loss from the target sequence.

4 Experiments
4.1 Experimental Settings
Datasets. We conduct experiments on two program bench-
marks crawled from Github repositories: Py150 [Raychev et
al., 2016] contains 150,000 Python ﬁles, split into a training
set of 100,000 ﬁles and test set of 50,000 ﬁles, GitHub-Java
[Allamanis and Sutton, 2013] contains 14,317 projects, we
follow Hellendoorn and Devanbu [2017] and Karampatsis et
al. [2020] to split validation and test set, and randomly sam-
ple 1,000 projects from the rest projects as the training set.

We use the Python ofﬁcial library tokenizer3 and Javalang4
to split Python and Java programs into lines of tokens, and
extract their syntax types. Then we employ a sliding context
window of 10-lines to create the data pairs, that is, the context
code sequence contains 10-lines of code tokens, and the next
line is considered as the target code sequence. The detailed
information of the datasets is shown in Table 2.
Metrics and Baselines. We use the following metrics to eval-
uate the performance of our approach:

• Exact Match Accuracy (EM): We compare the ex-
act matching accuracy between the generated code se-
quence and the ground truth.

• BLEU: We use the BLEU score to measure the n-gram
similarity between the target code sequence and the gen-
erated code sequence.

• Edit similarity (ES): We use the character-level edit sim-
ilarity of the predicted output ˆY and the target output Y ,
which is computed as:

ES = 1 −

Lev( ˆY , Y )
| ˆY | + |Y |

(8)

where Lev is Levenshtein distance.

3https://docs.python.org/3/library/tokenize.html
4https://github.com/c2nes/javalang

Table 3: Results on Python benchmark.

Table 4: Results on Java benchmark.

Model

BLEU

EM

ES

Latency

Speedup

Model

BLEU

EM

ES

Latency

Speedup

Transformer

CMLM
GLAT

SANAR

21.93

23.98
26.78

29.07

16.24

62.73

121ms

13.44
16.95

63.82
66.36

18.35

66.90

39ms
20ms

20ms

1.0×

3.1×
6.1×

6.1×

Transformer

CMLM
GLAT

SANAR

25.73

28.97
30.56

32.41

30.33

63.95

101ms

28.36
28.63

63.66
65.80

30.57

66.98

31ms
20ms

19ms

1.0×

3.3×
5.1×

5.3×

• Latency: The inference latency is computed as the time
to decode a single target code sentence without mini
batching, averaged over the whole test set. The decoding
is implemented in PyTorch on a single NVIDIA Tesla
V100.

We compare our model with the following baselines, includ-
ing both strong AR Transformer and several representative
NAR models:

• Transformer [Vaswani et al., 2017]: Base Transformer
seq2seq architecture, where the decoder is AR. We also
tried the TransformerLM decoder architecture in Wang
et al. [2020] using the comparable number of param-
eters. The results show that seq2seq architecture can
outperform the LM decoder in performance, and with
a comparable decoding speed. Thus we use Transformer
(seq2seq) model as the strong AR baseline, and also use
seq2seq framework as our base architecture instead of
LM decoder.

• CMLM [Ghazvininejad et al., 2019]: A strong iterative-
NAR model, which adopts multi-pass decoding strategy
to reﬁne the target tokens.

• GLAT [Qian et al., 2021]: A strong fully-NAR model,
which can generate high-quality target only with single-
pass parallel decoding.

Setup. For all the seq2seq baselines and our model, we use
a 6-layers of encoder and decoder with a model size of 5125.
We set the vocabulary size to 50,000 for both Python and Java
datasets. We train the model with batches of 16k tokens us-
ing Nvidia V100 GPUs. We set the dropout rate to 0.1 and
use Adam optimizer with β = (0.9, 0.999). The learning rate
warms up to 5e-5 in 4k steps and gradually decays according
to the inverse square root of the step number [Vaswani et al.,
2017]. Following Qian et al. [2021], we set λ to 0.3. For the
hyper-parameter p in Hybrid-Syntax-Guided sampling strat-
egy, we use 30% as the ﬁnal value, which can achieve the
best results. The speciﬁc syntax types used for guiding the
sampling include keyword, operator, and identiﬁer.

4.2 Main Results
The main results on Python and Java benchmarks are pre-
sented in Table 3 and 4. SANAR signiﬁcantly improves
the full-line code completion quality and outperforms all the
baselines on all evaluation metrics. Especially, SANAR out-
performs strong AR baseline (Transformer) by a large margin
in BLEU. We can observe that the improvements on BLEU

5For TransformerLM [Wang et al., 2020], to keep the compara-
ble number of the parameters, we employ a 12-layers decoder with
a model size of 512.

Table 5: Speed-up on long target sequences.

Model

Latency

Speed up

Transformer
CMLM
GLAT
SANAR

171ms
34ms
19ms
19ms

1.0×
5.0×
9.0×
9.0×

and Edit Similarity are more signiﬁcant than the Exact Match
accuracy. As a recommendation tool, code completion add-in
serves as developers’ cooperator, and developers can accept
to make a few edits to correct the recommended code. Be-
sides, different code snippets can have identical functional-
ity. For example, these two Python return statements have the
same functionality: return True if a==0 else False
and return False if a!=0 else True. Only evaluat-
ing the quality of the generated code by comparing the ex-
act match with the ground truth is not convincing enough.
Thus, BLEU and Edit Similarity which calculate the token
overlapping and the minimum number of operations required
to transform the predicted code sequence into the target se-
quence can evaluate the generated code more thoroughly.
Thus, the signiﬁcant improvements on BLEU and ES fur-
ther demonstrate that SANAR can generate the code sequence
which meets the developers’ expect more.

It is interesting to note that, most of the NAR models
can achieve higher BLEU and ES scores than the AR-based
Transformer model, and can achieve better or comparable EM
accuracy on all the datasets. These results are quite different
from NMT task, where the NAR models can achieve worse
or comparable results with the AR model. The results fur-
ther verify our assumptions in section 2, that is, the non-
autoregressive manner can ﬁt code completion better than
NMT, and can boost the performance on both efﬁciency and
quality for code completion.

For the inference latency, SANAR can signiﬁcantly reduce
the inference time within 5 ∼ 6× speedup compared with the
AR-based Transformer model. As a fully-NAR model, the in-
ference speed of SANAR is also faster than the iterative-NAR
CMLM model. Compared with NMT, the target sequence in
full-line code completion is shorter, thus the overall speedup
is not as large as in NMT, but is still signiﬁcant. We also
present the latency comparison for completing lines of ≥ 10
tokens, which account for about 30% of the test set. The re-
sults are shown in Table 5. As SANAR can generate tokens
in parallel, thus the latency will not be affected by the tar-
get lengths, and still keeps comparable with previous results.
However, for AR models, as the number of generated tokens
increases, the latency increases a lot, and SANAR can achieve
9× speedup. Thus, when completing longer token sequences,
the speedup of SANAR will be more signiﬁcant.

Table 6: Performance of full-line code completion with different
sampling probabilities.

p

0
0.15
0.3
0.5

BLEU

26.78
28.20
29.07
27.37

Python
EM

16.95
16.96
18.35
17.31

ES

BLEU

65.94
66.36
66.90
66.30

30.56
32.16
32.41
31.29

Java
EM

28.63
29.94
30.57
29.82

ES

65.80
66.81
66.98
66.43

Table 7: Token repetition ratio results.

Transformer
CMLM
GLAT
SANAR

Python

Java

3.72% 4.36%
3.66% 4.92%
4.55% 4.56%
5.52% 5.40%

[Li et al., 2018; Liu et al., 2020]. Wang et al. [2020] pro-
pose to use a Transformer-based LM to perform full-line code
completion. In full-line code completion, instead of predict-
ing the next single token, the model predicts a sequence of
tokens that form a complete statement given a partially com-
plete code context. The experimental results show that Trans-
former seq2seq architecture can outperform the LM decoder
in Wang et al. [2020] with comparable number of parameters.
Thus, we use Transformer seq2seq architecture as the AR
baseline, and also employ seq2seq architecture for SANAR
instead of only using a decoder as Wang et al. [2020]. Guo et
al. [2021] proposed GRAMMFORMER, a transformer-based
grammar-guided code generation model that generates code
sketches, i.e. sequences of code tokens with holes. As the
generation target is different from our model, we did not com-
pare with them.

4.3 Analysis

5.2 NAR Models of Text Generation

Inﬂuence of Parameter p.
To analyze how the sampling
probability p of our Hybrid-Syntax-Guided sampling strat-
egy affects the model’s performance, we conduct experiments
with different ps, where a larger p indicates more tokens are
sampled depending on their syntax-types. When p is zero,
the sampling will become totally random, which is the same
with GLAT [Qian et al., 2021]. The results for different ps
are listed in Table 6. When p is increased from 0 to 0.3, the
performance becomes better, demonstrating the effectiveness
of the syntax-guided sampling. When p is further increased,
where the randomness decreases, the performance began to
drop, but still outperforms GLAT. This suggests that it is also
necessary to introduce randomness during the syntax-aware
sampling, which can help SANAR explore more interdepen-
dency among target tokens.
Token Repetition Ratio.
In NMT, one of the known prob-
lems in non-autoregressive models is the high token repeti-
tion ratio, since the interdependency among target tokens is
not considered. We are also interested in whether this prob-
lem also troubles code completion. We measure the percent-
age of repeated tokens on the test set of Python and Java
benchmarks. The results are shown in Table 7. As seen
from the results, the token repetition ratio of the NAR-based
models is not increased obviously when compared with the
AR-based Transformer model, which is quite different from
NMT. These results also demonstrate that the left-to-right in-
terdependency among target tokens in the code completion
task is much weaker than that of NMT task. Thus, the non-
autoregressive decoder can ﬁt code completion better, and
achieve better performance.

5 Related Work

5.1 Code Completion

Since Hindle et al. [2016] have shown that source code is
highly repetitive and predictable, language models began to
be used for source code modeling and code completion [Hin-
dle et al., 2016; Tu et al., 2014; Hellendoorn and Devanbu,
2017; Li et al., 2018; Liu et al., 2020]. Most of the LM-based
code completion models perform next one token completion

Fully NAR The Fully Non-AutoRegressive model consists
of the same encoder as Transformer and a parallel decoder
[Gu et al., 2018]. During training, it uses the conditional in-
dependent factorization for the target sentence by maximizing
the following likelihood:

LN AR = log P (Y |X; θ) =

T
(cid:88)

t−1

log p(yt|X; θ)

(9)

During inference, the encoder representation is copied as the
input for the decoder, therefore all tokens on the target side
can be generated in parallel without depending on the previ-
ously generated tokens. Recently, Qian et al. [2021] propose
GLAT, which can achieve parallel text generation with only a
single decoding pass by gradual training.
Iterative NAR The conditional independence assumption
in fully NAR does not hold in general, resulting in inferior
performance.
In order to improve the fully NAR model,
multi-pass iterative decoding approaches such as CMLM
Ghazvininejad et al. [2019] is proposed, which ﬁrst predicts
all of the target words non-autoregressively, and then repeat-
edly mask out and regenerate the words that the model is least
conﬁdent about using the masking scheme.

6 Conclusion

In this paper, we propose SANAR, a syntax-aware non-
autoregressive model to improve the efﬁciency and accuracy
of full-line code completion. We perform an in-depth analysis
to explore the dependency among the target tokens, and the
results show that the dependency is weaker than the assump-
tion in existing code generation models, suggesting that NAR
can ﬁt code generation better. Experimental results show
that our approach outperforms both NAR and AR baselines,
as well as signiﬁcantly reduces the inference time compared
with AR based code completion models. We are the ﬁrst to
apply Non-autoregressive model for generating code. We be-
lieve this work represents a signiﬁcant advance in code gen-
eration, which will be beneﬁcial as a building block for many
other code generation applications.

sequence generation with generative ﬂow. In EMNLP/IJC-
NLP (1), pages 4281–4291. Association for Computational
Linguistics, 2019.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu,
Weinan Zhang, Yong Yu, and Lei Li. Glancing trans-
former for non-autoregressive neural machine translation.
In ACL/IJCNLP (1), pages 1993–2003. Association for
Computational Linguistics, 2021.

Veselin Raychev, Pavol Bielik, and Martin T. Vechev. Prob-
abilistic model for code with decision trees. In OOPSLA,
pages 731–747. ACM, 2016.

Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, and
Tie-Yan Liu. A study of non-autoregressive model for se-
In ACL, pages 149–159. Association
quence generation.
for Computational Linguistics, 2020.

Tabnine. Tabnine. https://www.tabnine.com/, 2018.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On
In Proceedings of the 22nd
the localness of software.
ACM SIGSOFT International Symposium on Foundations
of Software Engineering, pages 269–280, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. In Advances in
neural information processing systems, pages 5998–6008,
2017.

Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. Towards full-
line code completion with neural language models. arXiv
preprint arXiv:2009.08603, 2020.

Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and
Xu Sun. Imitation learning for non-autoregressive neural
machine translation. In ACL (1), pages 1304–1312. Asso-
ciation for Computational Linguistics, 2019.

References
aiXcoder. aixcoder. https://www.aixcoder.com/, 2018.
Miltiadis Allamanis and Charles Sutton. Mining source code
repositories at massive scale using language modeling. In
MSR, pages 207–216. IEEE Computer Society, 2013.

Copilot. Copilot. https://copilot.github.com/, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT (1),
pages 4171–4186. Association for Computational Linguis-
tics, 2019.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke
Zettlemoyer. Mask-predict: Parallel decoding of condi-
tional masked language models. In EMNLP/IJCNLP (1),
pages 6111–6120. Association for Computational Linguis-
tics, 2019.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li,
and Richard Socher. Non-autoregressive neural machine
translation. In ICLR (Poster). OpenReview.net, 2018.

Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc
Brockschmidt, and Miltiadis Allamanis. Learning to gen-
arXiv preprint arXiv:2106.10158,
erate code sketches.
2021.

Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen,
and Tie-Yan Liu. Layer-wise coordination between en-
coder and decoder for neural machine translation. In Pro-
ceedings of the 32Nd International Conference on Neural
Information Processing Systems, pages 7955–7965, 2018.
Vincent J Hellendoorn and Premkumar Devanbu. Are deep
neural networks the best choice for modeling source code?
In Proceedings of the 2017 11th Joint Meeting on Founda-
tions of Software Engineering, pages 763–773, 2017.

Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and
Premkumar Devanbu. On the naturalness of software.
Communications of the ACM, 59(5):122–131, 2016.

Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
Charles Sutton, and Andrea Janes. Big code!= big vo-
cabulary: Open-vocabulary models for source code.
In
2020 IEEE/ACM 42nd International Conference on Soft-
ware Engineering (ICSE), pages 1073–1085. IEEE, 2020.
Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao
Gu. Non-autoregressive machine translation with disentan-
gled context transformer. In International Conference on
Machine Learning, pages 5144–5155. PMLR, 2020.

Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. Code
completion with neural attention and pointer networks. In
IJCAI, pages 4159–4165. ijcai.org, 2018.

Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
A self-attentional neural architecture for code completion
In Proceedings of the 28th In-
with multi-task learning.
ternational Conference on Program Comprehension, pages
37–47, 2020.

Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and
Eduard H. Hovy. Flowseq: Non-autoregressive conditional

