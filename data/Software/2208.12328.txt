Autonomous Unmanned Aerial Vehicle Navigation using
Reinforcement Learning: A Systematic Review

Fadi AlMahamid, Katarina Grolingerâˆ—

Department of Electrical and Computer Engineering, Western University, London, Ontario, Canada

A R T I C L E I N F O

A B S T R A C T

2
2
0
2

g
u
A
5
2

]

O
R
.
s
c
[

1
v
8
2
3
2
1
.
8
0
2
2
:
v
i
X
r
a

Keywords:
Reinforcement Learning
Autonomous UAV Navigation
UAV
Systematic Review

There is an increasing demand for using Unmanned Aerial Vehicle (UAV), known as drones, in
diï¬€erent applications such as packages delivery, traï¬ƒc monitoring, search and rescue operations, and
military combat engagements. In all of these applications, the UAV is used to navigate the environment
autonomously - without human interaction, perform speciï¬c tasks and avoid obstacles. Autonomous
UAV navigation is commonly accomplished using Reinforcement Learning (RL), where agents act
as experts in a domain to navigate the environment while avoiding obstacles. Understanding the
navigation environment and algorithmic limitations plays an essential role in choosing the appropriate
RL algorithm to solve the navigation problem eï¬€ectively. Consequently, this study ï¬rst identiï¬es the
main UAV navigation tasks and discusses navigation frameworks and simulation software. Next,
RL algorithms are classiï¬ed and discussed based on the environment, algorithm characteristics,
abilities, and applications in diï¬€erent UAV navigation problems, which will help the practitioners
and researchers select the appropriate RL algorithms for their UAV navigation use cases. Moreover,
identiï¬ed gaps and opportunities will drive UAV navigation research.

Symbol

ğ’” âˆˆ ğ‘º
ğ’‚ âˆˆ ğ‘¨

ğ’“ âˆˆ ğ‘¹

ğœ¸

ğ‘®ğ’•

ğ‘· (ğ’”â€², ğ’“|ğ’”, ğ’‚)

ğ‰

ğ‘¸(ğ’”, ğ’‚)

Description

ğ’Œ=ğŸ ğœ¸ğ’Œğ‘¹ğ’•+ğ’Œ+ğŸ

State ğ’” belongs to all possible states ğ‘º
Action ğ’‚ belongs to the set of all possible
Actions ğ‘¨
Reward ğ’“ belongs to the set of all
generated Rewards ğ‘¹
Discounted factor ğœ¸ decreases the
contribution of the future rewards, where
ğŸ < ğœ¸ < ğŸ
The Expected Summation of the
Discounted Rewards; ğ‘®ğ’• = âˆ‘âˆ
The probability of the transition to state ğ’”â€²
with reward ğ’“ from taking action ğ’‚ in state
ğ’” at time ğ’•
A trajectory ğ‰ consists of a sequence of
actions and states pairs, where the actions
inï¬‚uence the states, also called an episode.
Each trajectory has a start state and ends
in a ï¬nal state that terminates the
trajectory
Action-value function expresses the
expected return of the state-action pairs
(ğ’”, ğ’‚); ğ‘¸ğ’˜(ğ’”, ğ’‚) is ğ‘¸(ğ’”, ğ’‚) parameterized by
ğ’˜

âˆ—Corresponding author.

falmaham@uwo.ca (F. AlMahamid); kgrolinger@uwo.ca (K.

Grolinger)

ORCID(s):

0000-0002-6907-7626 (F. AlMahamid);

0000-0003-0062-8212 (K. Grolinger)

https://doi.org/10.1016/j.engappai.2022.105321
The article submitted 5 February 2022 to Engineering Applications
of Artiï¬cial Intelligence Journal; Received in revised form 12 July 2022;
Accepted 4 August 2022.

0952-1976/Â© 2022 Elsevier Ltd. All rights reserved.

Symbol

ğ‘½ (ğ’”)

ğ‘¨(ğ’”, ğ’‚)

ğ…(ğ’‚|ğ’”)

ğ(ğ’”)

ğ‘¸ğ…(ğ’”, ğ’‚)

ğ‘½ğ…(ğ’”)

ğ†
âˆ¼

ğœ¼(ğ…)

Description

State-value function is similar to ğ‘¸(ğ’”, ğ’‚)
except it measures how good to be in a
state ğ’”; ğ‘½ ğ’˜(ğ’”) is a State-value function
parameterized by ğ’˜
Advantage-Value function ğ‘¨(ğ’”, ğ’‚) measures
how good an action is in comparison to
alternative actions at a given state;
ğ‘¨(ğ’”, ğ’‚) = ğ‘¸(ğ’”, ğ’‚) âˆ’ ğ‘½ (ğ’”)
Stochastic Policy ğ… is a function that maps
the probability of selecting an action ğ’‚
from the state ğ’”. It describes agent
behavior
Deterministic Policy ğ is similar to
Stochastic Policy ğœ‹, except ğ symbol is
used to distinguish it from Stochastic
Policy ğ…
Action-value function ğ‘¸(ğ’”, ğ’‚), when
following a policy ğ…
State-value function ğ‘½ (ğ’”), when following
a policy ğ…
State visitation probability
Sampled from. For example, ğ’” âˆ¼ ğ† means ğ’”
sampled from state visitation probability ğ†
The expected discounted reward following
a policy ğ…, similar to ğ‘®ğ’•

1. Introduction

Autonomous Systems (AS) are systems that can perform
desired tasks without human interference, such as robots
performing tasks without human involvement, self-driving
cars, and delivery drones. AS are invading diï¬€erent domains
to make operations more eï¬ƒcient and reduce the cost and
risk incurred from the human factor.

An Unmanned Aerial Vehicle (UAV) is an aircraft with-
out a human pilot, mainly known as a drone. Autonomous

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 1 of 24

 
 
 
 
 
 
Autonomous UAV Navigation using RL: A Systematic Review

UAVs have been receiving an increasing interest due to
their diverse applications, such as delivering packages to
customers, responding to traï¬ƒc collisions to attain injured
with medical needs, tracking military targets, assisting with
search and rescue operations, and many other applications.
Typically, UAVs are equipped with cameras, among
other sensors, that collect information from the surrounding
environment, enabling UAVs to navigate that environment
autonomously. UAV navigation training is typically con-
ducted in a virtual 3D environment because UAVs have lim-
ited computation resources and power supply, and replacing
UAV parts due to crashes can be expensive.

Diï¬€erent Reinforcement Learning (RL) algorithms are
used to train UAVs to navigate the environment autonomously.
RL can solve various problems where the agent acts as a
human expert in the domain. The agent interacts with the
environment by processing the environmentâ€™s state, respond-
ing with an action, and receiving a reward. UAV cameras
and sensors capture information from the environment for
state representation. The agent processes the captured state
and outputs an action that determines the UAV movementâ€™s
direction or controls the propellersâ€™ thrust, as illustrated in
Figure 1.

The research community provided a review of diï¬€erent
UAV navigation problems, such as Visual UAV navigation
[1, 2], UAV Flocking [3] and Path Planning [4]. Neverthe-
less, to the best of the authorsâ€™ knowledge there is no survey
related to applications of RL in UAV navigation. Hence, this
paper aims to provide a comprehensive and systematic re-
view on the application of various RL algorithms to diï¬€erent
autonomous UAV navigation problems. This survey has the
following contributions:

â€¢ Help the practitioners and researchers to select the
right algorithm to solve the problem on hand based
on the application area and environment type.

â€¢ Explain primary principles and characteristics of var-
ious RL algorithms,
identify relationships among
them, and classify them according to the environment
type.

Figure 1: UAV training using deep reinforcement agent

â€¢ Discuss and classify diï¬€erent RL UAV navigation

frameworks according to the problem domain.

â€¢ Recognize the various techniques used to solve dif-
ferent UAV autonomous navigation problems and the
diï¬€erent simulation tools used to perform UAV navi-
gation tasks.

The remainder of the paper is organized as follows:
Section 2 presents the systematic review process, Section 3
introduces RL, Section 4 provides a comprehensive review
of the application of various RL algorithms and techniques
in autonomous UAV navigation, Section 5 discusses the
UAV Navigation Frameworks and simulation software, Sec-
tion 6 classiï¬es RL algorithm and discusses the most promi-
nent algorithms, Section 7 explains RL algorithms selection
process, and Section 8 identiï¬es challenges and research
opportunities. Finally, Section 9 concludes the paper.

2. Review Process

This section described the inclusion criteria, paper iden-

tiï¬cation process, and threats to validity.

2.1. Inclusion Criteria and Identiï¬cation of Papers
The studyâ€™s main objective is to analyze the application
of Reinforcement Learning in UAV navigation and provide
insights into RL algorithms. Therefore, the survey consid-
ered all papers in the past ï¬ve years (2016-2021) written
in the English language that include the following terms
combined, alongside with their variations: Reinforcement
Learning, Navigation, and UAV.

In contrast, RL algorithms are listed based on the au-
thorsâ€™ domain knowledge of the most prominent algorithms
and by going through the related work of the identiï¬ed
algorithms with no restriction to the publication time to
include a large number of algorithms.

The identiï¬cation process of the papers went through the

following stages:

â€¢ First stage: The authors identiï¬ed all studies that
strictly applied RL to UAV Navigation and acknowl-
edged that model-free RL is typically utilized to tackle
UAV navigation challenges, except for a single arti-
cle [5] that employs model-based RL. Therefore, the
authors choose to concentrate on model-free RL and
exclude research irrelevant to UAV Navigation, such
as UAV networks and traditional optimization tools
and techniques [6â€“10].

â€¢ Second stage: The authors listed all RL algorithms
based on authorsâ€™ knowledge of the most prominent
algorithms, the references of recognized algorithms,
then identiï¬ed the corresponding paper of each algo-
rithm.

â€¢ Third stage: the authors identiï¬ed how RL is used to
solve diï¬€erent UAV navigation problems, classiï¬ed
the work, and then recognized more related papers
using exiting work references.

IEEE Xplore and Scopus were the primary sources of
papersâ€™ identiï¬cation between 2016 and 2021. The search
query was applied using diï¬€erent terminologies that are used

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 2 of 24

Autonomous UAV Navigation using RL: A Systematic Review

to describe the UAV alternatively, such as UNMANNED
AERIAL VEHICLE, DRONE, QUADCOPTER, or QUADRO-
TOR, and these terms are cross-checked with REINFORCE-
MENT LEARNING, and NAVIGATION, which resulted in a
total of 104 papers. After removing 15 duplicate papers and
5 unrelated papers, the count became 84.

The authors identiï¬ed another 75 papers that mainly
describe the RL algorithms based on the authorsâ€™ experience
and the references list of the recognized work, using Google
Scholar as the primary search engine. While RL for UAV
navigation studies were restricted to ï¬ve years, all RL al-
gorithms are included as many are still extensively used re-
gardless of their age. The search was completed in November
2021, with a total of 159 papers after all exclusions.

2.2. Threats to Validity

Despite the authorsâ€™ eï¬€ort to include all relevant papers,
the study might be subject to the following main threats to
the validity:

â€¢ Location bias: The search for papers was performed
using two primary digital libraries (databases), IEEE
Xplore and Scopus, which might limit the retrieved
papers based on the published journals, conferences,
and workshops in the database.

â€¢ Language bias: Only papers published in English are

included.

â€¢ Time Bias: The search query is only limited to retriev-
ing papers between 2016 and 2021, which results in
excluding relevant papers published before 2016.
â€¢ Knowledge reporting bias: The research papers of RL
algorithms are identiï¬ed using authorsâ€™ knowledge of
variant algorithms and the related work in the recog-
nized algorithms. It is hard to pinpoint all algorithms
utilizing a search query, which could result in missing
some RL algorithms.

3. Reinforcement Learning

RL can be explained using the Markov Decision Process
(MDP), where a RL agent learns through experience by
taking actions in the environment, causing a change in the
environmentâ€™s state, and receiving a reward for the action
taken to measure the success or failure of the action. Equa-
tion 1 deï¬nes the transition probability from state ğ’” by taking
the action ğ’‚ to the new state ğ¬â€² with a reward ğ’“, for all
ğ‘ â€² âˆˆ ğ‘†, ğ‘  âˆˆ ğ‘†, ğ‘Ÿ âˆˆ ğ‘…, ğ‘ âˆˆ ğ´(ğ‘ ) [11].

ğ‘ƒ (ğ‘ â€², ğ‘Ÿ|ğ‘ , ğ‘) = ğ‘ƒ ğ‘Ÿ{ğ‘†ğ‘¡ = ğ‘ â€², ğ‘…ğ‘¡ = ğ‘Ÿ|ğ‘†ğ‘¡âˆ’1 = ğ‘ , ğ´ğ‘¡âˆ’1 = ğ‘} (1)
The reward ğ‘¹ is generated using a reward function,
which can be expressed as a function of the action ğ‘…(ğ‘),
or as a function of action-state pairs ğ‘…(ğ‘, ğ‘ ). The reward
helps the agent learn good actions from bad actions, and
as the agent accumulates experience, it starts taking more
successful actions and avoiding bad ones [11].

All actions the agent takes from a start state to a ï¬nal
(terminal) state make an episode (trajectory). The goal of

MDP is to maximize the expected summation of the dis-
counted rewards by adding all the rewards generated from
an episode. However, sometimes the environment has an
inï¬nite horizon, where the actions cannot be divided into
episodes. Therefore, using a discounted factor (multiplier)
ğœ¸ to the power ğ’Œ, where ğœ¸ âˆˆ [ğŸ, ğŸ] as expressed in Equation
2 helps the agent to emphasize the reward at the current time
step and reduce the reward value granted at future time steps,
and, moreover, helps the expected summation of discounted
rewards to converge if the horizon is inï¬nite [11].

ğºğ‘¡ = ğ¸

]

ğ›¾ ğ‘˜ğ‘…ğ‘¡+ğ‘˜+1

[ âˆ
âˆ‘

ğ‘˜=0

(2)

The following subsections introduce important rein-

forcement learning concepts.

3.1. Policy and Value Function

A policy ğ… deï¬nes the agentâ€™s behavior by deï¬ning the
probability of taking action ğ’‚ while being in a state ğ’”, which
is expressed as ğ…(ğ’‚|ğ’”). The agent evaluates its behavior
(action) using a value function, which can be either state-
value function, which estimates how good it is to be in state
ğ’” after executing an action ğ’‚, or using a action-value function
that measures how good it is to select action ğ’‚ while being
in a state ğ’”. The value produced by the action-value function
in Equation 3 is known as the Q-value and is expressed in
terms of the expected summation of the discounted rewards
[11].

ğ‘„ğœ‹(ğ‘ , ğ‘) = ğ¸ğœ‹

[ âˆ
âˆ‘

]
ğ›¾ ğ‘˜ğ‘…ğ‘¡+ğ‘˜+1 | ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ = ğ‘

(3)

ğ‘˜=0

Since the objective is to maximize the expected summa-
tion of discounted rewards under the optimal policy ğ…, the
agent tries to ï¬nd the optimal Q-value ğ‘¸âˆ—(ğ’”, ğ’‚) as deï¬ned in
Equation 4. This optimal Q-value must satisfy the Bellman
Optimality Equation 5 which is deï¬ned as the sum of the
expected reward received from executing the current action
, and sum of all future rewards (discounted) received
ğ‘¹ğ’•+ğŸ
from any possible future state-action pairs (ğ’”â€², ğ’‚â€²). In other
words, the agent tries to select the actions that grant the high-
est rewards in an episode. In general, selecting the optimal
value means selecting the action with the highest Q-value;
however, the action with the highest Q-value sometimes
might not lead to better rewarding actions in the future [11].

ğ‘„âˆ—(ğ‘ , ğ‘) = ğ‘šğ‘ğ‘¥

ğœ‹

ğ‘„ğœ‹(ğ‘ , ğ‘)

ğ‘„âˆ—(ğ‘ , ğ‘) = ğ¸

[
ğ‘…ğ‘¡+1 + ğ›¾ ğ‘šğ‘ğ‘¥

ğ‘â€²

]
ğ‘„âˆ—(ğ‘ â€², ğ‘â€²)

(4)

(5)

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 3 of 24

3.2. Exploration vs Exploitation

3.5. Deep Reinforcement Learning

Autonomous UAV Navigation using RL: A Systematic Review

Exploration vs. Exploitation may be demonstrated using
the multi-armed bandit dilemma, which accurately portrays
the behavior of a person experiencing their ï¬rst slot machine
experience. The money (reward) player receives early in
the game is unrelated to any previously selected choices,
and as the player develops a comprehension of the reward,
he/she begins selecting choices that contribute to earning a
greater reward. The choices made randomly by the player to
acquire knowledge might be deï¬ned as the player Exploring
the environment. In contrast, the playerâ€™s Exploiting the
environment is described as the options selected based on
his/her experience.

The RL agent needs to ï¬nd the right balance between
exploration and exploitation to maximize the expected return
of rewards. Constantly exploiting the environment and se-
lecting the action with the highest reward does not guarantee
that the agent performs the optimal action because the agent
may miss out on a higher reward provided by future actions
taking alternative sets of actions in the future. Finding the
ratio between exploration and exploitation can be deï¬ned
through diï¬€erent strategies such as ğœ–-greedy strategy, Upper
Conï¬dence Bound (UCB), and Gradient Bandits [12].

3.3. Experience Replay

RL agent does not need data to learn; rather, it learns
from experiences by interacting with the environment. The
agent experience ğ’† can be formulated as tuple ğ’†(ğ’”, ğ’‚, ğ’”â€², ğ’“),
which describes the agent taking an action ğ’‚ at a given
state ğ’” and receiving a reward ğ’“ for the performed action
and causing a new state ğ’”â€². Experience Replay (ER) [13]
is a technique that suggests storing experiences in a replay
memory (buï¬€er) ğ‘« and using a batch of uniformly sampled
experiences for RL agent training.

On the other hand, Prioritized Experience Replay (PER)
[14] prioritizes experiences according to their signiï¬cance
using Temporal Diï¬€erence error (TD-error) and replays ex-
periences with lower TD-error to repeatedly train the agent,
which improves the convergence.

3.4. On-Policy vs Oï¬€-Policy

In order to interact with the environment, the RL agent
attempts to learn two policies: the ï¬rst one is referred to
as the target policy ğœ½(ğ’‚|ğ’”), which the agent learns through
the value function, and the second one is referred to as
the behavior policy ğœ·(ğ’‚|ğ’”), which the agent uses for action
selection when interacting with the environment.

A RL algorithm is referred to as on-policy algorithm
when the same target policy ğœ½(ğ’‚|ğ’”) is employed to collect
the training sample and to determine the expected return. In
contrast, oï¬€-policy algorithms are those where the training
sample is collected in accordance to the behavior policy
ğœ·(ğ’‚|ğ’”), and the expected reward is generated using the target
policy ğœ½(ğ’‚|ğ’”) [15]. Another main diï¬€erence is that Oï¬€-
policy algorithms can reuse past experiences and do not
require all the experiences within an episode (full episode)
to generate training samples, and the experiences can be
collected from diï¬€erent episodes.

Deep Reinforcement Learning (DRL) uses deep agents
to learn the optimal policy where it combines artiï¬cial Neu-
ral Networks (NN) with Reinforcement Learning (RL). The
NN type used in DRL varies from one application to another
depending on the problem being solved, inputs type (state),
and the number of inputs passed to the NN. For example, the
RL framework can be integrated with Convolutional Neural
Network (CNN) to process images representing the environ-
mentâ€™s state or combined with Recurrent Neural Network
(RNN) to process inputs over diï¬€erent time steps.

The NN loss function, also known as the Temporal
Diï¬€erence (TD), is generically computed by ï¬nding the
diï¬€erence between the output of the NN ğ‘¸(ğ’”, ğ’‚) and the op-
timal Q-value ğ‘¸âˆ—(ğ’”, ğ’‚) obtained from the Bellman equation
as shown in Equation 6 [11]:

ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡
âââââââââââââââââââââââââââââââââââââââââ
]
ğ‘„âˆ—(ğ‘ â€², ğ‘â€²)

[
ğ‘…ğ‘¡+1 + ğ›¾ ğ‘šğ‘ğ‘¥

ğ¸

ğ‘â€²

ğ‘ƒ ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘
âââââââââââââââââââââââ
]

âˆ’

ğ¸

ğ›¾ ğ‘˜ğ‘…ğ‘¡+ğ‘˜+1

(6)

[ âˆ
âˆ‘

ğ‘˜=0

The architecture of the deep agent can be simple or
complex based on the problem at hand, where a complex
architecture combines multiple NN. But what all deep agents
have in common is that they receive the state as an input, then
they output the optimal action and maximize the discounted
return of rewards.

The application of Deep NN to the RL framework en-
abled the research community to solve more complex prob-
lems in autonomous systems that were hard to solve before
and achieve better performance than previous state-of-the-
art, such as drone navigation and avoiding obstacles using
images received from the droneâ€™s monocular camera.

4. Autonomous UAV Navigation using DRL

Diï¬€erent DRL algorithms and techniques were used to
solve various problems in autonomous UAV navigation,
such as UAV control, obstacle avoidance, path planning,
and ï¬‚ocking. The DRL agent acted as an expert in all of
these problems, selecting the best action that maximizes the
reward to achieve the desired objective. The input and the
output of the DRL algorithm are generally determined based
on the desired objective and the implemented technique.

RL agent design for UAV navigation depicted in Figure
2 shows diï¬€erent UAV input devices used to capture the
state processed by the RL agent. The agent produces action
values that can be either the movement values of the UAV
or the waypoint values where the UAV needs to relocate.
Once the agent executes the action in the environment, it
receives the new state and the generated reward based on the
performed action. The reward function is designed to gener-
ate the reward subject to the intended objective while using
various information from the environment. The agent design
(â€™Agentâ€™ box in the ï¬gure) is inï¬‚uenced by the RL algorithms
discussed in Section 6 where the agent components and inner
working varies from one algorithm to another.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 4 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Figure 2: RL agent design for UAV navigation task

Table 1 summarizes the application of RL to diï¬€erent
UAV navigation tasks (objectives), and the following sub-
sections discuss the UAV navigation tasks in more detail.
As seen from this table, most of the research focused on
two UAV navigation objectives: 1) Obstacle avoidance using
various UAV sensor devices such as cameras and LIDARs
and 2) Path planning to ï¬nd the optimal or shortest route.

4.1. UAV Control

RL is used to control the movement of the UAV in the
environment by applying changes to the ï¬‚ight mechanics of
the UAV, which varies based on the UAV type. In general
UAVs can be classiï¬ed based on the ï¬‚ight mechanics into 1)
Multirotor, 2) Fixed-Wing, and 3) single-rotor, and 4) ï¬xed-
wing hybrid Vertical Take-Oï¬€ and Landing (VTOL) [92].

Multirotor, also known as multicopter or drone, uses
more than two rotors to control the ï¬‚ight mechanics by
applying diï¬€erent amounts of thrust to the rotors causing
changes in principal axes leading to four UAV movements

1) pitch, 2) roll, 3) yaw, and 4) throttle as explained in Figure
3. Similarly, single-rotor and ï¬xed-wing hybrid VTOL apply
changes to diï¬€erent rotors to generate the desired movement,
except they both use tilt-rotor(s) and wings in ï¬xed-wing
hybrid VTOL. On the other hand, ï¬xed-wing can only
achieve three actions pitch, roll, and yaw, where they take
oï¬€ by generating enough speed that causes the air-dynamics
to lift-up the UAV.

Quad-rotors has four propellers: two diagonal propellers
rotate clockwise and the other two propellers rotate counter-
clockwise causing the throttle action. When the propellers
generate a thrust more signiï¬cant than the UAV weight they
cause elevation, and when the thrust power equals the UAV
weight, the UAV stops elevation and starts hovering in place.
In contrast, if all propellers rotate in the same direction, they
cause a yaw action in the opposite direction, as shown in
Figure 4.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 5 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Table 1
DRL application to diï¬€erent UAV Navigation tasks

Objective

UAV Control

Obstacle Avoidance

Path Planning

Flocking

Sub-Objective

Controlling UAV ï¬‚ying behavior (attitude control)
Obstacle avoidance using images and sensor information
Obstacle avoidance while considering the battery level
Local and global path planning (ï¬nding the shortest/optimal route)
Path planning while considering the battery level
Find ï¬xed or moving targets (points of interest)
Landing the UAV on a selected point
Maintain speed and orientation with other UAVs (formation)
Obstacle avoidance
Target tracking
Flocking while considering the battery level
Covering geographical region
Path planning and ï¬nding the safest route

Paper

[5, 16â€“22]
[21, 23â€“58]
[59]
[24, 26, 44, 51, 55, 58, 60â€“70]
[24, 71, 72]
[58, 67, 73â€“77]
[78â€“80]
[39, 81â€“83]
[83, 84]
[85â€“89]
[86]
[86, 90]
[83, 91]

Figure 3: Multirotor Flight Mechanics

Figure 4: Yaw vs Throttle Mechanics

The steps described in Figure 5 depicts the RL process
used to control the UAV, which depends on the used RL al-
gorithm, but the most important takeaway is that RL uses the
UAV state to produce actions. These actions are responsible
for moving the UAV in the environment and can be either
direct changes in the value of pitch, roll, yaw, and throttle
values or indirect changes that require transformation to
commands understood by the UAV.

4.2. Obstacle Avoidance

Avoiding obstacles is an essential task required by the
UAV to navigate any environment, which can be achieved by
estimating the distance to the objects in the environment us-
ing diï¬€erent devices such as front-facing cameras or distance
sensors. The output generated by these diï¬€erent devices
provides input to the RL algorithm and plays a signiï¬cant
role in the NN architecture.

Lu et al. [1] described diï¬€erent front-facing cameras
such as monocular cameras, stereo cameras, and RGB-D
cameras that a UAV can use. Each camera type produces
a diï¬€erent image type used as raw input to the RL agent.
However, regardless of the camera type, these images can
be preprocessed using computer vision to produce speciï¬c
image types as described below:

â€¢ RGB Images: are renowned colored images where
each pixel is represented in three values (Red, Green,
Blue) ranging between (0, 255).

â€¢ Depth-Map Images: contains information related to
the distance of the objects from the Field Of View
(FOV).

â€¢ Event-Based Images: are special images that output
the changes in brightness intensity instead of stan-
dard images. Event-based images are produced by an
event camera, also known as Dynamic Vision Sensor
(DVS).

RGB images lack depth information, and, therefore, the
agent cannot estimate how far or close the UAV is to the
object leading to unexpected ï¬‚ying behavior. On the other
hand, depth information is essential for building a successful
reward function that penalizes moving closer to the objects.
Some techniques used RGB images and depth-map simul-
taneously as input to the agent to provide more information
about the environment. In contrast, event-based images data
are represented as one-dimensional sequences of events over
time, which is used to capture quickly changing information
in the scene [23].

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 6 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Figure 5: UAV Control using RL

Similar to cameras, distance sensors have diï¬€erent types,
such as LiDAR, RADAR, and acoustic sensors: they esti-
mate the distance of the surrounding objects to the UAV but
require less storage size than 2D images since they do not
use RGB channels.

The output generated by these devices reï¬‚ects the diï¬€er-
ent states that the UAV has over time, used as an input to
the RL agent to make actions causing the UAV to move in
diï¬€erent directions to avoid obstacles. The NN architecture
of the RL agent is based on: 1) input type, 2) the number
of inputs, and 3) the used algorithm. For example, pro-
cessing RGB images or depth-map images using the DQN
algorithm requires Convolutional Neural Network (CNN)
followed by fully-connect layers since CNN is known for its
power in processing images. In contrast, processing event-
based images is performed using Spiking Neural Networks
(SNN), which is designed to handle spatio-temporal data and
identify spatio-temporal patterns [23].

4.3. Path Planning

Autonomous UAVs must have a well-deï¬ned objective
before executing a ï¬‚ying mission. Typically, the goal is to
ï¬‚y from a start to a destination point, such as in delivery
drones. But, the goal can also be more sophisticated, such
as performing surveillance by hovering over a geographical
area or participating in search and rescue operations to ï¬nd
a missing person.

Autonomous UAV navigation requires path planning to
ï¬nd the best UAV path to achieve the ï¬‚ying objective while
avoiding obstacles. The optimal path does not always mean
the shortest path or a straight line between two points;
instead, the UAV aims to ï¬nd a safe path while considering
UAVâ€™s limited power and ï¬‚ying mission.

Path planning can be divided into two main types:
â€¢ Global Path Planning: concerned with planning the
path from the start point to destination point in attempt
to select the optimal path.

â€¢ Local Path Planning: concerned with planning the
local optimal waypoints in an attempt to avoid static
and dynamic obstacles while considering the ï¬nal
destination.

Path planning can be solved using diï¬€erent techniques
and algorithms; in this work, we focus on RL techniques
used to solve global and local path planning, where the RL
agent receives information from the environment and out-
puts the optimal waypoints according to the reward function.
RL techniques can be classiï¬ed according to the usage of the
environmentâ€™s local information 1) map-based navigation
and 2) mapless navigation.

4.3.1. Map-Based Navigation

A UAV that adopts map-based navigation uses a repre-
sentation of the environment either in 3D or 2D format. The
representation might include one or more of the following
about the environment: 1) the diï¬€erent terrains, 2) ï¬xed-
obstacles locations, and 3) charging/ground stations.

Some maps oversimplify the environment representa-
tion: the map is divided into a grid with equally-sized
smaller cells that store information about the environment
[68, 73, 93]. Others oversimplify the environmentâ€™s structure
by simplifying objects representation or by using 1D/2D to
represent the environment [34, 36, 39, 41, 47, 55, 65, 67, 74,
86, 87] . The UAV has to plan a safe and optimal path over
the cells to avoid cells containing obstacles until it reaches
its destination and has to plan its stopover at the charging
stations based on the battery level and path length.

In a more realistic scenario, the UAV calculates a route
using the map information and the GPS signal to track the
UAVâ€™s current location, starting point, and destination point.
The RL agent evaluates the change in the distance and the
angle between the UAVâ€™s current GPS location and target
GPS location, and penalizes the reward if the diï¬€erence
increases or if the path is unsafe depending on the reward
function (objective).

4.3.2. Mapless Navigation

Mapless navigation does not rely on maps; instead, it
applies computer vision techniques to extract features from
the environment and learn the diï¬€erent patterns to reach
the destination, which requires computation resources that
might be overwhelming for some UAVs.

Localization information of the UAV obtained by dif-
ferent means such as Global Positioning System (GPS)

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 7 of 24

Autonomous UAV Navigation using RL: A Systematic Review

or Inertial Measurement Unit (IMU) is used in mapless
navigation to plan the optimal path. DRL agent receives
the scene image, the destination target, and the localization
information as input and outputs the change in the UAV
movements.

For example, Zhou et al. [50] calculated and tracked the
angle between the UAV and destination point, then encoded
it with the depth image extracted from the scene and used
both as a state representation for the DRL agent. Although
localization information seems essential to plan the path,
some techniques achieved navigation with high speed using
monocular visual reactive navigation system without a GPS
[94].

4.4. Flocking

Although UAVs are known for performing individual
tasks, they can ï¬‚ock to perform tasks eï¬ƒciently and quickly,
which requires maintaining ï¬‚ight formation. UAV ï¬‚ocking
has many applications, such as search and rescue operations
to cover a wide geographical area.

UAV ï¬‚ocking is considered a more sophisticated task
than a single UAV ï¬‚ying mission because UAVs need to
orchestrate their ï¬‚ight to maintain ï¬‚ight formation while
performing other tasks such as UAV control and obstacle
avoidance. Flocking can be executed using diï¬€erent topolo-
gies:

â€¢ Flock Centering: maintaining ï¬‚ight formation as sug-
gested by Reynolds [95] involves three concepts: 1)
ï¬‚ock centering, 2) avoiding obstacles, and 3) velocity
matching. This topology was applied in several re-
search papers [82, 96â€“99].

â€¢ Leader-Follower Flocking: the ï¬‚ock leader has its
mission of reaching destination, while the followers
(other UAVs) ï¬‚ock with the leader with a mission of
maintaining distance and relative position to the leader
[100, 101].

â€¢ Neighbors Flocking: close neighbors coordinate with
each other, where each UAV communicates with two
or more nearest neighbors to maintain ï¬‚ight forma-
tion by maintaining relative distance and angle to the
neighbors [81, 102, 103].

Maintaining ï¬‚ight formation using RL requires commu-
nication between UAVs to learn the best policy to maintain
the formation while avoiding obstacles. These RL systems
can be trained using a single agent or multi-agents in cen-
tralized or distributed settings.

and decentralized execution for obstacle avoidance accord-
ing to each UAV local environment information. Similarly,
Hung and Givigi [101] trained a leader UAV to reach a
destination while avoiding obstacles and trained a shared
policy for followers to ï¬‚ock with the leader considering the
relative dynamics between the leader and the followers.

Zhao et al. [57] used a Multi-Agent Reinforcement
Learning (MARL) to train a centralized ï¬‚ock control policy
shared by all UAVs with decentralized execution. MARL
received position, speed, and ï¬‚ight path angle from all UAVs
at each time step and tried to ï¬nd the optimal ï¬‚ocking control
policy.

The centralized training would not produce a good gen-
eralization in neighbors ï¬‚ocking topology since the learned
policy for one neighbor is diï¬€erent from other neighborsâ€™
policies due to the diï¬€erences in neighborsâ€™ dynamics.

4.4.2. Distributed Training

UAV ï¬‚ocking can be trained using a distributed (decen-
tralized) approach, where each UAV has its designated RL
agent responsible for ï¬nding the optimal ï¬‚ock policy for the
UAV. The reward function is deï¬ned to maintain distance
and ï¬‚ying direction with other UAVs and can be customized
to include further information depending on the objective.

Flight information such as location and heading angle
should be communicated to other UAVs since the RL agents
are distributed, and the state representation must include
information of other UAVs. Any UAV that fails to receive
the information from other UAVs will cause the UAV to be
isolated from the ï¬‚ock.

Liu et al. [86] proposed a decentralized DRL framework
to control each UAV in a distributed setting to maximize
average coverage score, geographical fairness, and minimize
UAVsâ€™ energy consumption.

5. UAV Navigation Frameworks and

Simulation Software
Subsection 5.1 discusses and classiï¬es the UAV naviga-
tion frameworks based on the UAV navigation objectives/sub-
objectives explained in Section 4, and identiï¬es categories
such as Path Planning Frameworks, Flocking Frameworks,
Energy-Aware UAV Navigation Frameworks, and others.
On the other hand, Subsection 5.2 explains the simulation
softwareâ€™s components and the most common simulation
software utilized to perform the experiments.

4.4.1. Centralized Training

A centralized RL agent trains a shared ï¬‚ocking policy to
maintain the ï¬‚ock formation using the experience collected
from all UAVs, while each UAV acts individually according
to its local environment information such as obstacles. The
reward function of the centralized agent can be customized to
serve the ï¬‚ocking topology, such as ï¬‚ock centering or leader-
follower ï¬‚ocking.

Yan et al. [39] used Proximal Policy Optimization (PPO)
algorithm to train a centralized shared ï¬‚ocking control pol-
icy, where each UAV ï¬‚ocks as close as possible to the center

5.1. UAV Navigation Frameworks

In general, a software framework is a conceptual struc-
ture analogous to a blueprint used to guide the compre-
hending construction of the software by deï¬ning diï¬€erent
functions and their interrelationships. By deï¬nition, RL can
be considered a framework by itself. Therefore, we consid-
ered only UAV navigation frameworks that add to traditional
navigation using sensors or camera data for navigation. As
a result, Table 2 classiï¬es UAV frameworks based on the
framework objective. The subsequent sections discuss the
frameworks in more detail.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 8 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Table 2
UAV Navigation Frameworks

Framework Objective

Papers

Energy-aware UAV Navigation
Path Planning
Flocking
Vision-Based Frameworks
Transfer Learning

[59, 71]
[30, 32, 47, 60, 62, 66, 70]
[61, 91]
[42, 43, 73, 77]
[40]

5.1.1. Energy-Aware UAV Navigation Frameworks

UAVs has limited ï¬‚ight time, hence operate mainly
using batteries. Therefore, planning ï¬‚ight route and recharge
stopover are crucial to reach destinations. Energy-aware
UAV navigation frameworks aim to provide obstacles avoid-
ance navigation while considering the UAV battery capacity.
Bouhamed et al. [59] developed a framework based on
Deep Deterministic Policy Gradient (DDPG) algorithm to
guide the UAV to a target position while communicating
with ground stations, allowing the UAV to recharge its bat-
tery if it drops below a speciï¬c threshold. Similarly, Iman-
berdiyev et al. [71] monitor battery level, rotorsâ€™ condition,
and sensor readings to plan the route and apply necessary
route changes for required battery charging.

5.1.2. Path Planning Frameworks

Path planning is the process of determining the most
eï¬ƒcient route that meets the ï¬‚ight objective, such as ï¬nding
the shortest, fastest, or safest route. Diï¬€erent frameworks
[47, 60] implemented a modular path planning scheme,
where each module has a specialized function to achieve
while exchanging data with other modules to train action
selection policies and discover the optimal path.

Similarly, Li et al. [32] developed a four-layer framework
in which each layer generates a set of objective and constraint
functions. The functions are intended to serve the lower layer
and consider the upper layerâ€™s objectives and constraints,
with their primary goal generating trajectories.

Other frameworks suggested stage-based learning to
choose actions from the desired stage depending on current
environment encounters. For example, Camci and Kay-
acan [66] proposed learning a set of motion primitives
oï¬„ine, then using them online to design quick maneuvers
to enable switching seamlessly between two modes: near-
hover motions, which is responsible for generating motion
plans allowing a stable completion of maneuvers and swift
maneuvers to deal smoothly with abrupt inputs.

In a collaborative setting, Zhang et al. [62] suggested a
coalition between Unmanned Ground Vehicle (UGV) and
UAV complementing each other to reach the destination,
where UAV cannot get to far locations alone due to limited
battery power, and UGV cannot reach high altitude due to
limited abilities.

5.1.3. Flocking Frameworks

UAV ï¬‚ocking frameworks have functionality beyond
UAV navigation while maintaining ï¬‚ight formation. For

example, Bouhamed et al. [61] presented a RL-based spa-
tiotemporal scheduling system for autonomous UAVs. The
system enables UAVs to autonomously arrange their sched-
ules to cover the most signiï¬cant number of pre-scheduled
events physically and temporally spread throughout a spec-
iï¬ed geographical region and time horizon. On the other
hand, Majd et al. [91] predicted the movement of drones and
dynamic obstacles in the ï¬‚ying zone to generate eï¬ƒcient and
safe routes.

5.1.4. Vision-Based Frameworks

Vision-Based Framework depends on UAV camera for
navigation, where the images produced by the camera are
used to draw on additional functionality for improved navi-
gation. It is possible to employ frameworks that augment the
agentâ€™s CNN architecture to fuse data from several sensors,
use Long-Short Term Memory cells (LSTM) to maintain
navigation choices, use RNN to capture the UAV states over
diï¬€erent time steps, or pre-process images to provide more
information about the environment [42, 42, 43].

5.1.5. Transfer Learning Frameworks

UAVs are trained on target environments before exe-
cuting the ï¬‚ight mission; the training is carried either in a
virtual or real-world environment. The UAV requires retrain-
ing when introduced to new environments or moving from
virtual training as the environments have diï¬€erent terrains
and obstacle structures or textures. Besides, UAV training
requires a long time and it is hardware resource intensive
while actual UAVs have limited hardware resources. There-
fore, when UAV is introduced to new environments, transfer
learning frameworks reduce the training time by reusing
the NN weights trained from the previous environment and
retraining only parts of the agentâ€™s NN.

Yoon et al. [40] proposed algorithm-hardware co-design,
where the UAV is trained in a virtual environment, and after
the UAV is deployed to a real-world environment; the agent
loads the weights stored in embedded Non-Volatile Memory
(eNVM), and then evaluates new actions and only trains the
last few layers of CNN whose weights are stored in the on-die
SRAM (Static Random Access Memory).

5.2. Simulation Software

The research community used diï¬€erent evaluation meth-
ods for autonomous UAV navigation using RL. Simulation
software is used widely over actual UAVs to execute the
evaluation due to the cost of the hardware (drone) in addition
to the cost of replacement parts required due to UAV crashes.
Comparison between simulation software is not the intended
purpose, rather than making the research community aware
of the most commonly used tools for evaluation as illustrated
in Figure 6. 3D UAV navigation simulation requires mainly
three components as illustrated in Figure 7:

â€¢ RL Agent: represents the RL algorithm used with all
computations required to generate the reward, process
the states, and compute the optimal action. RL Agent
interacts directly with the UAV Flight simulator to
send/receive UAV actions/states.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 9 of 24

Autonomous UAV Navigation using RL: A Systematic Review

â€¢ UAV Flight Simulator: responsible for simulating
the UAV movements and interactions with the 3D
environment, such as obtaining images from the UAV
camera or reporting UAV crashes with diï¬€erent ob-
stacles. Examples of UAV ï¬‚ight simulators are Robot
Operating systems (ROS) [104] and Microsoft AirSim
[105].

â€¢ 3D Graphics Engine: provides a 3D graphics envi-
ronment with the physics engine, which is responsible
for simulating the gravity and dynamics similar to
the real world. Examples of 3D graphics engines are
Gazebo [106] and Unreal Engine [107].

Due to compatibility/support issues, ROS is used in
conjunction with Gazebo, where AirSim uses Unreal Engine
to run the simulations. However, the three components might
not always be present, especially if the simulation software
has internal modules or plugins that provide the required
functionality, such as MATLAB.

6. Reinforcement Learning Algorithms

Classiï¬cation
The previous sections discussed the UAV navigation
tasks and frameworks without elaborating on RL algorithms.
However, to choose a suitable algorithm for the application
environment and the navigation task, the comprehension of
RL algorithms and their characteristics is necessary. For
example, the DQN algorithm and its variations can be used
for UAV navigation tasks that use simple movement actions
(UP, DOWN, LEFT, RIGHT, FORWARD) since they are
discrete. Therefore, this section examines RL algorithms and
their characteristics.

AlMahamid and Grolinger [11] categorized RL algo-
rithms into three main categories according to the number
of states and the type of actions: 1) limited number of
states and discrete actions, 2) unlimited number of states

Figure 6: UAV Simulation Software Usage

Figure 7: UAV Simulation Software Components

and discrete actions, and 3) unlimited number of states and
continuous actions. We extend this with sub-classes, analyze
more than 50 RL algorithms, and examine their use in UAV
navigation. Table 3 classiï¬es all RL algorithms found in
UAV Navigation studies and includes other prominent RL
algorithms to show the intersection between RL and UAV
navigation. Furthermore, this section discusses algorithms
characteristics and highlights RL applications in diï¬€erent
UAV navigation studies. Note that Table 3 includes many
algorithms, but only the most prominent ones are discussed
in the following subsections.

6.1. Limited States and Discrete Actions

Generally, simple environments have a limited number
of states and the agent transitions between states by execut-
ing discrete (limited number) actions. For example, in a tic-
tac-toe game, the agent has a predeï¬ned set of two actions
X or O that are used to update the nine boxes constituting
the predeï¬ned set of known states. Q-Learning [108] and
Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action (SARSA) [110] algo-
rithms can be applied to environments with a limited number
of states and discrete actions, where they maintain a Q-Table
with all possible states and actions while iteratively updating
the Q-values for each state-action pair to ï¬nd the optimal
policy.

SARSA is similar to Q-Learning except to update the
current ğ‘¸(ğ’”, ğ’‚) value it computes the next state-action
ğ‘¸(ğ’”â€², ğ’‚â€²) by executing the next action ğ’‚â€² [126]. In contrast,
Q-learning updates the current ğ‘¸(ğ’”, ğ’‚) value by computing
the next state-action ğ‘¸(ğ’”â€², ğ’‚â€²) using the Bellman equation
since the next action is unknown, and takes a greedy action
by selecting the action that maximizes the reward [126].

6.2. Unlimited States and Discrete Actions

An RL agent uses Deep Neural Network (DNN) - usu-
ally a CNN, in complex environments such as the pong
game, where the states are unlimited and the actions are
discrete (UP, DOWN). The deep agent/DNN processes the
environmentâ€™s state as an input and outputs the Q-values
of the available actions. The following subsections discuss
the diï¬€erent algorithms that can be used in this type of
the environment, such as DQN, Deep SARSA, and their
variations [11].

6.2.1. Deep Q-Networks Variations

Deep Q-Learning, also known as Deep Q-Network
(DQN)[111], is a primary method used in settings with an
unlimited number of states and discrete actions, and it serves
as an inspiration for other algorithms used for the same goal.
As illustrated in Figure 8 [29], DQN architecture frequently
employs convolutional and pooling layers, followed by fully
connected layers that provide Q-values corresponding to the
number of actions. A signiï¬cant disadvantage of the DQN
algorithm is that it overestimates the action-value (Q-value),
with the agent selecting the actions with the highest Q-value,
which may not be the optimal action [151].

Double DQN solves the overestimation issue in DQN by
using two networks. The ï¬rst network, known as the Policy

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 10 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Table 3
RL Algorithms usage and classiï¬cation

State Action Class

Algorithm

On/Oï¬€
Policy

Actor-
Critic

Multi-
Thread

Dis-
tributed

Multi-
Agent

Usage

d
e
t
i

m
L

i

e
t
e
r
c
s
i
D

e
l
p
m
S

i

L Q-Learning [108]
R

SARSA [110]

d
e
t
i

m

i
l

n
U

e
t
e
r
c
s
i
D

s
u
o
n
i
t
n
o
C

N
Q
D

s
n
o
i
t
a
i
r
a
V

l

a
n
o
i
t
u
b
i
r
t
s
i
D

d
e
t
u
b
i
r
t
s
i
D

A
S
R
A
S

p
e
e
D

N
Q
D

N
Q
D

s
n
o
i
t
a
i
r
a
V

y
c
i
l

o
P

d
e
s
a
B

c
i
t
i
r
C
-
r
o
t
c
A

d
e
t
u
b
i
r
t
s
i
D
d
n
a

t
n
e
g
A

-
i
t
l
u
M

c
i
t
i
r
C
-
r
o
t
c
A

DQN [111]

Double DQN [113]

Dueling DQN [114]
DRQN [115]
DD-DQN [114]
DD-DRQN
Noisy DQN [116]
C51-DQN [117]
QR-DQN [118]
IQN [119]
Rainbow DQN
[120]
FQF [121]
R2D2 [122]
Ape-X DQN [123]
NGU [124]
Agent57 [125]
Deep SARSA [126]
Double SARSA
Dueling SARSA
DR-SARSA
DD-SARSA
DD-DR-SARSA
REINFORCE [127]
TPRO [128]
PPO [129]
PPG [130]
SVPG [131]
SLAC [132]
ACE [133]
DAC [134]
DPG [15]
RDPG [135]

DDPG [136]

TD3 [137]
SAC [138]
Ape-X DPG [123]
D4PG [139]
A2C [140]
DPPO [141]
A3C [140]
PAAC [142]
ACER [143]
Reactor [144]
ACKTR [145]
MADDPG [146]
MATD3 [147]
MAAC [148]
IMPALA [149]
SEED [150]

-

-

Oï¬€

Oï¬€

Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€

Oï¬€

Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
On
On
On
On
On
On
On
On
On
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€

Oï¬€

Oï¬€
Oï¬€
Oï¬€
Oï¬€
On
On
On
On
Oï¬€
Oï¬€
On
Oï¬€
Oï¬€
Oï¬€
Oï¬€
Oï¬€

No

No

No

No

No
No
No
No
No
No
No
No

No

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
Yes
Yes
Yes
Yes
Yes

Yes

Yes
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

No

No

No

No

No
No
No
No
No
No
No
No

No

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
Yes
No
No
No

No

No
No
No
No
Yes
No
Yes
Yes
Yes
Yes
Yes
No
No
No
Yes
Yes

No

No

No

No

No
No
No
No
No
No
No
No

No

No
Yes
Yes
Yes
Yes
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No

No

No
No
Yes
Yes
Yes
Yes
No
No
No
No
No
No
No
No
Yes
Yes

No

No

No

No

No
No
No
No
No
No
No
No

No

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No

No

No
No
No
Yes
No
Yes
Yes
No
No
No
No
Yes
Yes
Yes
Yes
Yes

[17, 21, 24, 61, 63â€“
65, 67, 68, 74, 75, 109]
-
[19, 23, 25, 26, 33, 35, 37, 41, 44,
46, 47, 49, 55, 66, 70, 72, 77, 83,
88, 89, 109, 112]
[16, 26, 29, 31, 37, 40, 45, 52, 78,
79, 109]
[26, 37]
[43, 58, 73, 76]
[26, 48]
-
-
-
-
-

-

-
-
-
-
-
-
-
-
-
-
-
-
[22]
[18, 22, 38, 39, 51, 53, 56, 62, 69]
-
-
-
-
-
[20]
-
[22, 24, 30, 32, 34, 36, 42, 50, 54,
59, 80, 81, 84, 86]
[87]
[34]
-
-
[76, 82]
-
[36]
-
-
-
-
-
-
[90]
-
-

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 11 of 24

Autonomous UAV Navigation using RL: A Systematic Review

As explained by Wang et al. [114], Double Dueling DQN
(DD-DQN) extends DQN by combining Dueling DQN and
Double DQN to determine the optimal Q-value, with the
output of Dueling DQN passed to Double DQN.

The Deep Recurrent Q-Network (DRQN) [115] algo-
rithm is a DQN variation, using a recurrent LSTM layer in
place of the ï¬rst fully connected layer. This changes the input
from a single environment state to to a group of states as
a single input, which aids in the integration of information
over time [115]. The techniques of doubling and dueling can
be utilized independently or in combination with a recurrent
neural network.

6.2.2. Distributional DQN

The goal of distributional Q-learning is to obtain a
more accurate representation of the distribution of observed
rewards. Fortunato et al. [116] introduced NoisyNet, a deep
reinforcement learning agent that uses gradient descent to
learn parametric noise added to the network weights, and
demonstrated how the agentâ€™s policyâ€™s induced stochasticity
can be used to aid eï¬ƒcient exploration [116].
Categorical Deep Q-Networks (C51-DQN) [117] applied
a distributional perspective using Wasserstein metric to the
random return received by Bellmanâ€™s equation to approxi-
mate value distributions instead of the value function. The
algorithm ï¬rst performs a heuristic projection step and then
minimizes the Kullback-Leibler (KL) divergence between
the projected Bellman update and the prediction [117].
Quantile Regression Deep Q-Networks (QR-DQN) [118]
performs a distributional reinforcement learning over the
Wasserstein metric in a stochastic approximation setting. Us-
ing Wasserstein distance, the target distribution is minimized
by stochastically adjusting the distributionsâ€™ locations using
quantile regression [118]. QR-DQN assigns ï¬xed, uniform
probabilities to ğ‘ adjustable locations and minimizes the
quantile Huber loss between the Bellman updated distri-
bution and current return distribution [121], whereas C51-
DQN uses ğ‘ ï¬xed locations (ğ‘ = 51) for distribution
approximation and adjusts the locations probabilities [118].
Implicit Quantile Networks (IQN) [119] incorporates QR-
DQN [118] to learn full quantile function controlled by the
size of the network and the amount of training, in contrast
to QR-DQN quantile function that learns a discrete set of
quantiles dependent on the number of quantiles output [119].
IQN distribution function assumes the base distribution to
be non-uniform and reparameterizes samples from a base
distribution to the respective quantile values of a target
distribution.
Rainbow DQN [120] combines several improvements of
the traditional DQN algorithm into a single algorithm, such
as 1) addressing the overestimation bias, 2) using Priori-
tized Experience Replay (PER) [14], 3) using Dueling DQN
[114], 4) shifting the bias-variance trade-oï¬€ and propagat-
ing newly observed rewards faster to earlier visited states
as implemented in A3C [140], 5) learning a distributional
reinforcement learning instead of the expected return similar

Figure 8: DQN using AlexNet CNN

Figure 9: DQN vs. Dueling DQN

Network, optimizes the Q-value, while the second network,
known as the Target Network, is a clone of the Policy
Network and is used to generate the estimated Q-value [113].
After a speciï¬ed number of time steps, the parameters of the
target network network are updated by copying the policy
network parameters instead of performing backpropagation.
Dueling DQN, as depicted in Figure 9 [114], is a fur-
ther enhancement to DQN. To improve Q-value evaluation,
Dueling DQN employs the following functions in place the
Q-value function:

â€¢ The State-Value function ğ‘½ (ğ’”) quantiï¬es how desir-

able it is for an agent to be in a state ğ’”.

â€¢ The Advantage-Value function ğ‘¨(ğ’”, ğ’‚) assesses the
superiority of the selected action in a given state ğ’” over
other actions.

The two functions depicted in Figure 9 are integrated
using a custom aggregation layer to generate an estimate of
the state-action value function [114]. The aggregation layer
has the same value as the sum of the two values produced by
the two functions:

ğ‘„(ğ‘ , ğ‘) = ğ‘‰ (ğ‘ ) +

(ğ´(ğ‘ , ğ‘) âˆ’

1
îˆ­
|
|

âˆ‘

)
ğ´(ğ‘ , ğ‘)

(7)

ğ‘â€²

âˆ‘

The term ğŸ
|î‰‡|

ğ’‚â€² ğ‘¨(ğ’”, ğ’‚) denotes the mean, whereas
|î‰‡| denotes vector ğ‘¨ length. This assists the identiï¬ability
problem while having no eï¬€ect on the relative rank of the
ğ´ (and thus Q) values. This also improves the optimization
because the advantage function only needs to change as fast
as the mean [114].

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 12 of 24

Autonomous UAV Navigation using RL: A Systematic Review

to C51-DQN [117], and 6) implementing stochastic network
layers using Noisy DQN [116].

Yang et al. [121] proposed Fully parameterized Quan-
tile Function (FQF) for distributional RL providing full pa-
rameterization for both quantile fractions and corresponding
quantile values. In contrast, QR-DQN [118] and IQN [119]
only parameterize the corresponding quantile values, while
quantile fractions are either ï¬xed or sampled [121].

FQF for distributional RL uses two networks: 1) quantile
value network that maps quantile fractions to corresponding
quantile values, and 2) fraction proposal network that gen-
erates quantile fractions for each state-action pair with the
goal of distribution approximation while minimizing the 1-
Wasserstein distance between the approximated and actual
distribution [121].

6.2.3. Distributed DQN

Distributed DRL architecture used by diï¬€erent RL algo-
rithms as depicted in Figure 10 [125] aims to decouple acting
from learning in distributed settings relaying on prioritized
experience replay to focus on the signiï¬cant experiences
generated by actors. The actors share the same NN and
replay experience buï¬€er, where they interact with the en-
vironment and store their experiences in the shared replay
experience buï¬€er. On the other hand, the learner replays
prioritized experiences from the shared experience buï¬€er
and updates the learner NN accordingly [123]. In theory,
both acting and learning can be distributed across multiple
workers or running on the same machine [123].
Ape-X DQN [123], based on the Ape-X framework, was the
ï¬rst algorithm to suggest distributed DRL, which was later
extended by Recurrent Replay Distributed DQN (R2D2)
[122] with two main diï¬€erences: 1) R2D2 adds an LSTM
layer after the convolutional stack to overcome partial ob-
servability, and 2) it trains a recurrent neural network from
randomly sampled replay sequences using the â€œburn-inâ€
strategy, which produces a start state through using a portion

Figure 10: Distributed DRL agent scheme

of the replay sequence and updates the network only on the
remaining part of the sequence [122].
Never Give Up (NGA) [124] is another algorithm that
combines R2D2 architecture with a novel approach that
encourages the agent to learn exploratory strategies through-
out the training process using a compound intrinsic reward
consisting of two modules:

â€¢ Life-long novelty module uses Random Network Dis-
tillation (RND) [152], which consists of two networks
used to generate an intrinsic reward: 1) target network,
and 2) prediction network. This mechanism is known
as curiosity because it motivates the agent to explore
the environment by going to novel or unfamiliar states.
â€¢ Episodic novelty module uses dynamically-sized episodic
memory ğ‘€ that stores the controllable states in an
online fashion, then turns state-action counts into a
bonus reward, where the count is computed using the
ğ‘˜-nearest neighbors.

While NGA uses intrinsic reward to promote explo-
ration, it promotes exploitation by generating extrinsic re-
ward using the Universal Value Function Approximator
(UVFA). NGA uses conditional architecture with shared
weights to learn a family of policies that separate exploration
and exploitation [124].
Agent57 [125] is the ï¬rst RL algorithm that outperforms the
human benchmark on all 57 games of Atari 2600. Agent57
implements NGA algorithms with the main diï¬€erence of ap-
plying an adaptive mechanism for exploration-exploitation
trade-oï¬€ and utilizes parameterization of the architecture
that allows for more consistent and stable learning [125].

6.2.4. Deep SARSA

SARSA is based on Q-learning and is designed for situa-
tions with limited states and discrete actions, as explained in
subsection 6.1. Deep SARSA [126] uses a deep neural net-
work similar to DQN and has the same extensions: Double
SARSA, Dueling SARSA, Double Dueling SARSA (DD-
SARSA), Deep Recurrent SARSA (DR-SARSA), and Dou-
ble Dueling Deep Recurrent (DD-DR-SARSA). The main
diï¬€erence compared to DQN is that Deep SARSA computes
ğ‘¸(ğ’”â€², ğ’‚â€²) by taking the next action ğ’‚â€², which is necessary to
determine the current state-action ğ‘¸(ğ’”, ğ’‚) rather than taking
a greedy action that maximizes the reward.

6.3. Unlimited States and Continuous Actions

While discrete actions are adequate to drive a car or
unmanned aerial vehicle in a simulated environment, they
do not enable realistic movements in real-world scenarios.
Continuous actions specify the quantity of movement in
various directions, and the agent does not select from a
predetermined set of actions. For instance, a realistic UAV
movement deï¬nes the amount of roll, pitch, yaw, and throt-
tle changes necessary to navigate the environment while
avoiding obstacles, as opposed to ï¬‚ying the UAV in preset
directions: forward, left, right, up, and down [11].

Continuous action space demands learning a parameter-
that maximizes the expected summation of

ized policy ğ…ğœ½

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 13 of 24

Autonomous UAV Navigation using RL: A Systematic Review

discounted rewards since it is not feasible to determine the
action-value ğ‘¸(ğ’”, ğ’‚) for all continuous actions in all distinct
is considered a
states. Learning a parameterized policy ğ…ğœ½
maximization problem, which can be handled using gradient
descent methods to get the optimal ğœ½ in the following manner
[11]:

ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼âˆ‡ğ½ (ğœƒğ‘¡)

(8)

Here, ğ› is the gradient and ğœ¶ is the learning rate.

The goal of the reward function ğ‘± is to maximize the ex-
pected reward applying the following parameterized policy
ğœ‹ğœƒ

[12]:

ğ½ (ğœ‹ğœƒ) =

=

âˆ‘

ğ‘ âˆˆğ‘†
âˆ‘

ğ‘ âˆˆğ‘†

(ğ‘ ) ğ‘‰ ğœ‹ğœƒ (ğ‘ )

ğœŒğœ‹ğœƒ

ğœŒğœ‹ğœƒ

(ğ‘ )

âˆ‘

ğ‘âˆˆğ´

ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) ğœ‹ğœƒ(ğ‘|ğ‘ )

(9)

(ğ’”) denotes the stationary probability ğ…ğœ½

start-
where ğ†ğ…ğœ½
and transitioning to future states according
ing from state ğ’”ğŸ
. To determine the best ğœ½ that maximizes the
to the policy ğ…ğœ½
function ğ‘± (ğ…ğœ½), the gradient ğ›ğœ½ğ‘± (ğœ½) is calculated as follows:

(

âˆ‘

âˆ‡ğœƒğ½ (ğœƒ) = âˆ‡ğœƒ

ğ‘ âˆˆğ‘†

ğœ‡(ğ‘ )

âˆ

âˆ‘

ğ‘ âˆˆğ‘†

ğœŒğœ‹ğœƒ

(ğ‘ )

âˆ‘

ğ‘âˆˆğ´

)
ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) ğœ‹ğœƒ(ğ‘|ğ‘ )

âˆ‘

ğ‘âˆˆğ´

(10)

ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) âˆ‡ğœ‹ğœƒ(ğ‘|ğ‘ )

Due to the fact that âˆ‘

ğ’”âˆˆğ‘º ğœ¼(ğ’”) = ğŸ and the action space

is continuous, Equation 10 can be rewritten as:

âˆ‡ğœƒğ½ (ğœƒ) = ğ”¼ğ‘ âˆ¼ğœŒğœ‹ğœƒ ,ğ‘âˆ¼ğœ‹ğœƒ

[

ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) âˆ‡ğœƒ ln ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)

]

(11)

Equation 12 [15], referred to as the oï¬€-policy gradient
theorem, deï¬nes the policy change in relation to the ratio of
target policy ğ…ğœ½(ğ’‚|ğ’”) to behavior policy ğœ·(ğ’‚|ğ’”). Take note
that the training sample is selected according to the target
policy ğ’” âˆ¼ ğ†ğ…ğœ½ , and the expected return is calculated for the
same policy ğ…ğœ½
, where the training sample adheres to the
behavior policy ğœ·(ğ’‚|ğ’”).

6.3.1. Policy-Based Algorithms

Policy-based algorithms are devoted to improving the
gradient descent performance by means of applying diï¬€erent
methods such as REINFORCE [127], Trust Region Policy
Optimization (TRPO) [128], Proximal Policy Optimization
(PPO) [129], Phasic Policy Gradient (PPG) [130], and Stein
Variational Policy Gradient (SVPG) [131].

REINFORCE
REINFORCE is a Monte-Carlo policy gradient approach
that creates a sample by selecting from an entire episode pro-
portionally to the gradient and updates the policy parameter
ğœ½ with the step size ğœ¶. Given that ğ”¼ğ…[ğ‘®ğ’•|ğ‘ºğ’•, ğ‘¨ğ’•] = ğ‘¸ğ…(ğ’”, ğ’‚),
REINFORCE may be deï¬ned as follows [12]:

âˆ‡ğœƒğ½ (ğœƒ) = ğ”¼ğœ‹

[

ğºğ‘¡ âˆ‡ğœƒ ln ğœ‹ğœƒ(ğ´ğ‘¡|ğ‘†ğ‘¡)

]

(13)

The Monte Carlo method has a high variance and, hence, a
slow pace of learning. By subtracting the baseline value from
the expected return ğ‘®ğ’•
, REINFORCE decreases variance
and accelerates learning while maintaining the bias [12].

Trust Region Policy Optimization (TRPO)
Trust Region Policy Optimization (TRPO) [128] belongs to
a category of PG methods: it enhances gradient descent by
performing protracted steps inside trust zones speciï¬ed by a
KL-Divergence constraint and updates the policy after each
trajectory instead of after each state [11]. Proximal Policy
Optimization (PPO) [129] may be thought of as an extension
of TRPO, where the KL-Divergence constraint is applied as
a penalty and the objective is clipped to guarantee that the
optimization occurs within a predetermined range [154].
Phasic Policy Gradient (PPG) [130] is an extension of PPO
[129]: it incorporates a recurring auxiliary phase that distills
information from the value function into the policy network
to enhance the training while maintaining decoupling.

Stein Variational Policy Gradient (SVPG) Stein Vari-
ational Policy Gradient (SVPG) [131] algorithm updates the
using Stein variational gradient descent (SVGD)
policy ğ…ğœ½
[155], therefore reducing variance and improving conver-
gence. When used in conjunction with REINFORCE and the
advantage actor-critic algorithms, SVPG enhances average
return and data eï¬ƒciency [155].

âˆ‡ğœƒğ½ (ğœƒ) = ğ”¼ğ‘ âˆ¼ğœŒğ›½ ,ğ‘âˆ¼ğ›½

[ ğœ‹ğœƒ(ğ‘|ğ‘ )
ğ›½ğœƒ(ğ‘|ğ‘ )

ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) âˆ‡ğœƒ ln ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)
(12)

]

6.3.2. Actor-Critic

The term "Actor-Critic algorithms" refers to a collection
of algorithms based on the policy gradients theorem. They
are composed of two components:

The policy gradient theorem depicted in Equation 9
[153] served as the foundation for a variety of other Policy
Gradients (PG) algorithms, including REINFORCE, Actor-
Critic algorithms, and various multi-agent and distributed
actor-critic algorithms.

1. The Actor who is liable of ï¬nding the optimal policy

.

ğ…ğœ½

2. The Critic who estimates the value function ğ‘¸ğ’˜(ğ’”ğ’•, ğ’‚ğ’•) â‰ˆ
ğ‘¸ğ…(ğ’”ğ’•, ğ’‚ğ’•) utilizing a parameterized vector ğ’˜ and
a policy assessment
technique such as temporal-
diï¬€erence learning [15].

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 14 of 24

Autonomous UAV Navigation using RL: A Systematic Review

The actor can be thought of as a network that is at-
tempting to discover the probability of all possible actions
and perform the one with the largest probability, whereas
the critic can be thought of as a network that is evaluating
the chosen action by assessing the quality of the new state
created by the performed action. Numerous algorithms can
be classiï¬ed under the actor-critic category including De-
terministic policy gradients (DPG) [15], Deep Deterministic
Policy Gradient (DDPG) [136], Twin Delayed Deep Deter-
ministic (TD3) [137], and many others.

Deterministic Policy Gradients (DPG)
Deterministic policy gradients (DPG) algorithms implement
a deterministic policy ğ(ğ’”) instead of a stochastic policy
ğ…(ğ’”, ğ’‚). The deterministic policy is a subset of a stochastic
policy in which the target policy objective function is av-
eraged over the state distribution of the behavior policy, as
depict in 14 [15].

ğ½ğ›½(ğœ‡ğœƒ) = âˆ«
ğ‘†

ğœŒğ›½(ğ‘ ) ğ‘‰ ğœ‡(ğ‘ ) ğ‘‘ğ‘ 

ğœŒğ›½(ğ‘ ) ğ‘„ğœ‡(ğ‘ , ğœ‡ğœƒ(ğ‘ )) ğ‘‘ğ‘ 

= âˆ«
ğ‘†

(14)

Importance sampling is frequently used in oï¬€-policy
techniques with a stochastic policy to account for mis-
matches between behavior and target policies. The deter-
ministic policy gradient eliminates the integral over actions;
therefore, the importance sampling can be skipped, resulting
in the following gradient [11]:

âˆ‡ğœƒğ½ğ›½(ğœ‡ğœƒ) â‰ˆ âˆ«
ğ‘†

ğœŒğ›½(ğ‘ ) âˆ‡ğœƒ ğœ‡ğœƒ(ğ‘|ğ‘ ) ğ‘„ğœ‡(ğ‘ , ğœ‡ğœƒ(ğ‘ )) ğ‘‘ğ‘ 
]

âˆ‡ğœƒ ğœ‡ğœƒ(ğ‘ )âˆ‡ğ‘ğ‘„ğœ‡(ğ‘ , ğ‘)|ğ‘=ğœ‡ğœƒ(ğ‘ )

[

= ğ”¼ğ‘ âˆ¼ğœŒğ›½

(15)

Numerous strategies are employed to enhance DPG; for
example, Experience Replay (ER) can be used in conjunc-
tion with DPG to increase the stability and eï¬ƒciency of data
[135]. Deep Deterministic Policy Gradient (DDPG) [136],
on the other hand, expands DPG by leveraging DQN to
operate in continuous action space whereas Twin Delayed
Deep Deterministic (TD3) [137] expands on DDPG by uti-
lizing Double DQN to prevent the overestimation of the
value function by taking the minimum value between the two
critics [137].

Recurrent Deterministic Policy Gradients (RDPG)
Wierstra et al. [156] applied RNN to Policy Gradient (PG) to
build a model-free RL - namely Recurrent Policy Gradient
(RPG), for Partially Observable Markov Decision Problem
(POMDP), which does not require the agent to have a com-
plete assumption about the environment [156]. RPG applies
a method for backpropagating return-weighted characteristic
eligibilities through time to approximate a policy gradient
for a recurrent neural network [156].

Recurrent Deterministic Policy Gradient (RDPG) [135]
implements DPG using RNN and extends the work of RGP

[156] to partially observed domains. The RNN with LSTM
cells preserves information about past observations over
many time steps.

Soft Actor-Critic (SAC)
The objective of Soft Actor-Critic (SAC) is to maximize an-
ticipated reward and the entropy [138]. By adding the antic-
ipated entropy of the policy across ğœŒğœ‹(ğ‘ ğ‘¡), SAC improves the
maximum sum of rewards established by adding the rewards
]
over states transitions ğ½ (ğœ‹) = âˆ‘ğ‘‡
ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡)
[138]. Equation 16 illustrates an extended entropy goal, in
which the temperature parameter ğ›¼ inï¬‚uences the stochas-
ticity of the optimum policy by specifying the importance
of the entropy îˆ´(ğœ‹(.|ğ‘ ğ‘¡)) term to the reward [138].

ğ‘¡=1 ğ”¼ğ‘ âˆ¼ğœŒğœ‹ ,ğ‘âˆ¼ğœ‹

[

ğ½ (ğœ‹) =

ğ‘‡
âˆ‘

ğ‘¡=1

ğ”¼ğ‘ âˆ¼ğœŒğœ‹ ,ğ‘âˆ¼ğœ‹

[

]
ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡) + ğ›¼îˆ´(ğœ‹(.|ğ‘ ğ‘¡))

(16)

By using function approximators and two independent
NNs for the actor and critic, SAC estimates a soft Q-function
ğ‘¸ğœ½(ğ’”ğ’•, ğ’‚ğ’•) parameterized by ğœ½, a state value function ğ‘½ğ (ğ’”ğ’•)
parameterized by ğ, and an adjustable policy ğ…ğ“(ğ’‚ğ’•|ğ’”ğ’•)
parameterized by ğ“ [11].

6.3.3. Multi-Agent and Distributed Actor-Critic

This group of algorithms includes multi-agent and dis-
tributed actor-critic algorithms. They are grouped together
as multi-agents can be deployed across several nodes making
it a distributed system.

Advantage Actor-Critic
Asynchronous Advantage Actor-Critic (A3C) [140] is a
policy gradient algorithm that parallelizes training by using
multi-threads, commonly known as workers or agents. Each
agent has a local policy ğ…ğœ½(ğ’‚ğ’•|ğ’”ğ’•) and a value function
estimate ğ‘½ğœ½(ğ’”ğ’•). The agent and the same-structured global
network asynchronously exchange the parameters in both
directions, from agent to the global network and vice-versa.
After ğ‘¡ğ‘šğ‘ğ‘¥
actions or when a ï¬nal state is reached, the policy
and the value function are modiï¬ed [140].
Advantage Actor-Critic (A2C) [140] is a policy gradient
method identical to A3C, except that it includes a coordi-
nator for synchronizing all agents. After all agents complete
their work, either by arriving at a ï¬nal state or by completing
actions, the coordinator updates the policy and value
ğ’•ğ’ğ’‚ğ’™
function in both directions between the agents and the global
network and vice versa.

Another variant of A3C is Actor-Critic with Kronecker-
Factored Trust Region (ACKTR) [145] which uses Kronecker-
factored approximation curvature (K-FAC) [157] to optimize
the actor and critic. It improves the computation of natural
gradients by eï¬ƒciently inverting the gradient covariance
matrix.

Actor-Critic with Experience Replay (ACER)
Actor-Critic with Experience Replay (ACER) [143] is an
oï¬€-policy actor-critic algorithm with experience replay that

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 15 of 24

Autonomous UAV Navigation using RL: A Systematic Review

estimates the policy ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) and the value function ğ‘‰ ğœ‹
(ğ‘ ğ‘¡)
ğœƒğ‘£
using a single deep neural network [143]. In comparison to
A3C, ACER employs a stochastic dueling network and a
novel trust region policy optimization [143], while improv-
ing importance sampling with a bias correction [140].

ğ’•

ACER applies an improved Retrace algorithm [158] by
using a truncated importance sampling with bias correction
and the value ğ‘¸ğ’“ğ’†ğ’• as the target value to train the critic
is determined by truncating the
[143]. The gradient Ì‚ğ’ˆğ’‚ğ’„ğ’†ğ’“
(ğ’”ğ’•),
importance weights by a constant ğ’„, and subtracting ğ‘½ğœ½ğ’—
which reduces variance.
Retrace-Actor (Reactor) [144] increases sampling and time
eï¬ƒciency by combining contributions from diï¬€erent tech-
niques. It employs Distributional Retrace [158] to provide
multi-step oï¬€-policy distributional RL updates while prior-
itizing replay on transitions [144]. Additionally, by taking
advantage of action values as a baseline, Reactor improves
the trade-oï¬€ between variance and bias via ğ›½-leave-one-out
(ğ›½-LOO) resulting in an improvement of the policy gradient
[144].

Multi-Agent Reinforcement Learning (MARL)
Distributed Distributional DDPG (D4PG) [139] adds fea-
tures such as N-step returns and prioritized experience replay
to the distributed settings of DDPG [139]. On the other hand,
Multi-Agent DDPG (MADDPG) [146] expands DDPG to
coordinate between multiple agents and learn policies while
considering each agentâ€™s policy [146]. In comparison, Multi-
Agent TD3 (MATD3) expands TD3 [147] to work with
multi-agents using centralized training and decentralized
execution while,similarly to TD3, controlling the overesti-
mation bias by employing two centralized critics for each
agent.
Importance Weighted Actor-Learner Architecture (IM-
PALA) [149] is an oï¬€-policy algorithm that separates action
execution and policy learning. It can be applied using two
distinct conï¬gurations: 1) a single learner and multiple ac-
tors, or 2) multiple synchronous learners and multiple actors.
Using a single learner and several actors, the trajectories
generated by the actors are transferred to the learner. Before
initiating a new trajectory, the actors are waiting for the
learner to update the policy, while the learner simultaneously
queues the received trajectories from the actors and con-
structs the updated policy. Nonetheless, actors may acquire
an older version due to their lack of awareness of one another
and the lag between the actors and the learner. To address
this challenge, IMPALA employs a unique v-trace correction
approach that takes into account a truncated importance
sampling (IS), deï¬ned as the ratio of the learnerâ€™s policy ğ…
to the actorâ€™s present policy ğ [11]. Likewise, with multiple
synchronous learners, policy parameters are spread across
numerous learners who communicate synchronously via a
master learner [149].
Scalable, Eï¬ƒcient Deep-RL (SEED RL) [150] provides
a scalable architecture that combines IMPALA with R2D2
and can train on millions of frames per second with a lower
cost of experiments compared to IMPALA [150]. SEED

moves the inference to the learner while the environments
run remotely, introducing a latency issue due to the increased
number of remote calls, which is mitigated using a fast
communication layer using gRPC.

7. Problem Formulation and Algorithm

Selection

The previous section categorized RL algorithms based
on state and action types and reviewed the most prominent
algorithms. With such a large number of algorithms, it is
challenging to select the RL algorithms suitable to tackle the
task at hand. Consequently, Figure 11 depicts the process
of selecting a suitable RL algorithm or a group of RL
algorithms through six steps/questions that are answered to
guide an informed selection.

The selection process places a greater emphasis on how
the environment and RL objective are formulated than on
the RL problem type because the algorithm selection is
dependent on the environment and objective formulation.
For instance, UAV navigation tasks can employ several sets
of algorithms dependent on the desired action type. The
six steps, indicated in Figure 11, guide the selection of
algorithms: the selected option at each step limits the choices
available in the next step based on the available algorithmsâ€™
characteristics. The steps are as follows:
â€¢ Step 1 - Deï¬ne State Type: When assessing an RL task,
it is essential to comprehend the state that can be obtained
from the surrounding environment. For instance, some nav-
igation tasks simplify the environmentâ€™s states using grid-
cell representations [68, 73, 93], where the agent has a
limited and predetermined set of states, whereas in other
tasks, the environment can have unlimited states [34, 38, 40].
Therefore, this steps involves a decision between limited vs.
unlimited states.
â€¢ Step 2 - Deï¬ne Action Type: Choosing between discrete
and continuous action types limits the number of applicable
algorithms. For instance, discrete actions can be used to
move the UAV in pre-speciï¬ed directions (UP, DOWN,
RIGHT, LEFT, etc.), whereas continuous actions, such as
the change in pitch, roll, and yaw angles, specify the quantity
of the movement using a real number ğ‘Ÿ âˆˆ â„.
â€¢ Step 3 - Deï¬ne Policy Type: As addressed and explained
in Subsection 3.4, RL algorithms can be either oï¬€-policy or
on-policy algorithms. The policy type selected restricts the
alternatives accessible in the subsequent stage. On-policy
algorithms converge faster than oï¬€-policy algorithms and
ï¬nd a sub-optimal policy, making them a good ï¬t for envi-
ronments requiring much exploration. Moreover, on-policy
algorithms provide stable training since one policy uses
learning and data sampling. On the other hand, oï¬€-policy
algorithms provide an optimal policy and require a good
exploration strategy.

The oï¬€-policy algorithmsâ€™ convergence can be improved
using techniques such as prioritized experience replay and
importance sampling, making them a good ï¬t for navigation
tasks that require ï¬nding the optimal path.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 16 of 24

Autonomous UAV Navigation using RL: A Systematic Review

â€¢ Step 4 - Deï¬ne Processing Type: While some RL algo-
rithms run in a single thread, others support multi-threading
and distributed processing. This steps select the processing
type that suits the application needs and the available com-
putational power.
â€¢ Step 5 - Deï¬ne Number of Agents: This steps speciï¬es
the number of agents the application should have. This is
needed as some RL algorithms enable MARL, which accel-
erates learning but requires more computational resources,
while other techniques only employ a single agent.
â€¢ Step 6 - Select the Algorithms: The last phase of the
process results in a collection of algorithms that may be
applied to the RL problem at hand. However, the perfor-
mance of the algorithms is aï¬€ected by a number of fac-
tors and may vary depending on variables such as hyper-
parameter settings, reward engineering, and the agentâ€™s NN
architecture. Consequently, the procedure seeks to reduce
the algorithm selection to a group of algorithms rather than
a single algorithm.

While this section presented the process of narrowing
down the algorithm for use case, Section 6 provided a
description and references to many algorithms to assist in
comprehending the distinctions between the algorithms and
making an informed selection.

8. Challenges and Opportunities

Previous sections demonstrated the diversity of UAV
navigation tasks besides the diversity of RL algorithms.
Due to such a high number of algorithms, selecting the
appropriate algorithms for the task at hand is challenging.
Table 3 and discussion in Section 6 provide an overview of
the RL algorithms and assist in selecting the RL algorithm
for navigation task. Nevertheless, there are still numerous
challenges and opportunities in RL for UAV navigation,
including:
Evaluation and benchmarking: Atari 2600 is a home video
game console with 57 built-in games that laid the foundation
to establish a benchmarking for RL algorithms. The bench-
mark was established using 57 diï¬€erent games to compare
various RL algorithms and set a benchmark baseline against
human performance playing the same games. The agentâ€™s
performance is evaluated and compared to other algorithms
using the same benchmark (Atari 2600), or evaluated us-
ing various non-navigation environments other than Atari
2600 [159]. The performance of the algorithms using the
benchmark might diï¬€er when applied to the UAV navigation
simulated on a 3D environment or the real world because
the games in Atari 2600 can provide a full state of the
environment, which means the agent does not need to make
assumptions about the state, and MDP can be applied to
these problems. Whereas in UAV navigation simulation, the
agent knows partial states of the environment (observation),
these observations are used to train the agent using POMDP,
which results in changing behavior for some of the algo-
rithms. Furthermore, images processed from the games are
2D images, where the agent in most algorithms tries to learn

an optimal policy based on the pattern of the pixels in the
image. The same cannot be inferred for images received
from the 3D simulators or the real-world images because
objectsâ€™ depth plays a vital role in learning the optimal
policy avoiding nearby objects. Therefore, there is a need for
new evaluation and benchmarking techniques for RL driven
navigation.
Environment complexity: The tendency to oversimplify
the environment and the absence of a standardized bench-
marking tools makes it impossible to compare and conclude
performances obtained using diï¬€erent algorithms and sim-
ulated using various tools and environments. Nevertheless,
the UAV needs to perform tasks in diï¬€erent environments
and is subject to various conditions, for example:

â€¢ Navigating in various environment types such as in-

door vs. outdoor.

â€¢ Considering the changing environment conditions
such as wind speed, lighting conditions, and moving
objects.

Some of the simulation tools discussed in Section 5,
such as AirSim combined with Unreal Engine, provide dif-
ferent environment types out-of-the-box and are capable of
simulating several environmental eï¬€ects such as changing
wind speed and lighting conditions. Still, these complex en-
vironments remain to be combined with new benchmarking
techniques for improved comparison of RL algorithms for
UAV navigation.
Knowledge transfer: Knowledge transfer imposes another
challenge, where the RL agent training in a selected envi-
ronment does not guarantee similar performance in another
environment due to the diï¬€erence in environmentsâ€™ nature
such as diï¬€erent object/obstacles types, background texture,
lighting density, and added noise. Most of the existing re-
search focused on applying transfer learning to reduce the
training time for the agent in the new environment [40].
However, generalized training methods or other techniques
are needed to guarantee a similar performance of the agent
in diï¬€erent environments and under various conditions.
UAVs complexity: Training UAVs is often accomplished
in a 3D virtual environment since UAVs have limited com-
putational resources and power supply, with a typical ï¬‚ight
time of 10 to 30 minutes. Reducing the computation time
will create possibilities for more complex navigation tasks
and increase the ï¬‚ight time since it will reduce energy
consumption. Figure 6 shows that only 10% of the inves-
tigated research used real drones for navigation training.
Therefore, more research is required to focus on energy-
aware navigation utilizing low-complexity and eï¬ƒcient RL
algorithms while simulating using real drones.
Algorithm diversity: As seen from Table 3, many recent
and very successful algorithms have not been applied in
UAV navigation. As these algorithms have shown great
surcease in other domains outperforming the human bench-
mark, there is a prodigious potential in their application
in UAV navigation. The algorithms are expected to gain
better generalization on diï¬€erent environments, speed up the

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 17 of 24

Autonomous UAV Navigation using RL: A Systematic Review

Figure 11: Algorithm Selection Process

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 18 of 24

Autonomous UAV Navigation using RL: A Systematic Review

training process, and even solve eï¬ƒciently more complex
tasks such as UAVs ï¬‚ocking.

9. Conclusion

This review deliberates on the application of RL for
autonomous UAV navigation. RL uses an intelligent agent
to control the UAV movement by processing the states from
the environment and moving the UAV in desired directions.
The data received from the UAV camera or other sensors
such as LiDAR are used to estimate the distance from various
objects in the environment and avoid colliding with these
objects.

RL algorithms and techniques were used to solve navi-
gation problems such as controlling the UAV while avoiding
obstacles, path planning, and ï¬‚ocking. For example, RL is
used in single UAV path planning and multi-UAVs ï¬‚ocking
to plan path waypoints of the UAV(s) while avoiding obsta-
cles or maintaining ï¬‚ight formation (ï¬‚ocking). Furthermore,
this study recognizes various navigation frameworks simu-
lation software used to conduct the experiments along with
identifying their use within the reviewed papers.

The review discusses over ï¬fty RL algorithms, explains
their contributions and relations, and classiï¬es them accord-
ing to the application environment and their use in UAV nav-
igation. Furthermore, the study highlights other algorithmic
traits such as multi-threading, distributed processing, and
multi-agents, followed by a systematic process that aims to
assist in ï¬nding the set of applicable algorithms.

The study observes that the research community tends
to experiment with a speciï¬c set of algorithms: Q-learning,
DQN, Double DQN, DDPG, PPO, although some recent
algorithms show more promising results than the mentioned
algorithms such as agent57. To the best of the authorsâ€™
knowledge, this study is the ï¬rst systematic review identi-
fying a large number of RL algorithms while focusing on
their application in autonomous UAV navigation.

Analysis of the current RL algorithms and their use
in UAV navigation identiï¬ed the following challenges and
opportunities: the need for navigation-focused evaluation
and benchmarking techniques, the necessity to work with
more complex environments, the need to examine knowl-
edge transfer, the complexity of UAVs, and the necessity to
evaluate state-of-the-art RL algorithms on navigation tasks.

A. Acronyms

â€¢ A2C : Advantage Actor-Critic
â€¢ A3C : Asynchronous Advantage Actor-Critic
â€¢ AC : Actor-Critic
â€¢ ACE : Actor Ensemble
â€¢ ACER : Actor-Critic with Experience Replay
â€¢ ACKTR : Actor-Critic using Kronecker-Factored

Trust Region

â€¢ Agent57 : Agent57
â€¢ Ape-X DPG : Ape-X Deterministic Policy Gradients
â€¢ Ape-X DQN : Ape-X Deep Q-Networks
â€¢ AS : Autonomous Systems

â€¢ C51-DQN : Categorical Deep Q-Networks
â€¢ CNN : Recurrent Neural Network
â€¢ D4PG : Distributed Distributional DDPG
â€¢ DAC : Double Actor-Critic
â€¢ DD-DQN : Double Dueling Deep Q-Networks
â€¢ DD-DRQN : Double Dueling Deep Recurrent Q-

Networks

â€¢ DDPG : Deep Deterministic Policy Gradient
â€¢ Double DQN : Double Deep Q-Networks
â€¢ DPG : Deterministic Policy Gradients
â€¢ DPPO : Distributed Proximal Policy Optimization
â€¢ DQN : Deep Q-Networks
â€¢ DRL : Deep Reinforcement Learning
â€¢ DRQN : Deep Recurrent Q-Networks
â€¢ Dueling DQN : Dueling Deep Q-Networks
â€¢ DVS : Dynamic Vision Sensor
â€¢ eNVM : embedded Non-Volatile Memory
â€¢ FOV : Field Of View
â€¢ FQF : Fully parameterized Quantile Function
â€¢ GPS : Global Positioning System
â€¢ IMPALA : Importance Weighted Actor-Learner Ar-

chitecture

â€¢ IMU : Inertial Measurement Unit
â€¢ IQN : Implicit Quantile Networks
â€¢ K-FAC : Kronecker-factored approximation curvature
â€¢ KL : Kullback-Leibler
â€¢ LSTM : Long-Short Term Memory
â€¢ MAAC : Multi-Actor-Attention-Critic
â€¢ MADDPG : Multi-Agent DDPG
â€¢ MARL : Multi-Agent Reinforcement Learning
â€¢ MATD3 : Multi-Agent Twin Delayed Deep Deter-

ministic

â€¢ MATD3 : Multi-Agent TD3
â€¢ MDP : Markov Decision Problem
â€¢ NGA : Never Give Up
â€¢ Noisy DQN : Noisy Deep Q-Networks
â€¢ PAAC : Parallel Advantage Actor-Critic
â€¢ PER : Prioritized Experience Replay
â€¢ PG : Policy Gradients
â€¢ POMDP : Partially Observable Markov Decision

Problem

â€¢ PPG : Phasic Policy Gradient
â€¢ PPO : Proximal Policy Optimization
â€¢ QR-DQN : Quantile Regression Deep Q-Networks
â€¢ R2D2 : Recurrent Replay Distributed Deep Q-Networks
â€¢ Rainbow DQN : Rainbow Deep Q-Networks
â€¢ RDPG : Recurrent Deterministic Policy Gradients
â€¢ Reactor : Retrace-Actor
â€¢ REINFORCE : REward Increment = Nonnegative

Factor Ã— Oï¬€set Reinforcement Ã— Characteristic Eligibility

â€¢ RL : Reinforcement Learning
â€¢ RND : Random Network Distillation
â€¢ RNN : Recurrent Neural Network
â€¢ ROS : Robot Operating System
â€¢ SAC : Soft Actor-Critic
â€¢ SARSA : State-Action-Reward-State-Action
â€¢ SEED RL : Scalable, Eï¬ƒcient Deep-RL

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 19 of 24

Autonomous UAV Navigation using RL: A Systematic Review

â€¢ SLAC : Stochastic Latent Actor-Critic
â€¢ SRAM : Static Random Access Memory
â€¢ SVPG : Stein Variational Policy Gradient
â€¢ TD : Temporal Diï¬€erence
â€¢ TD3 : Twin Delayed Deep Deterministic
â€¢ TRPO : Trust Region Policy Optimization
â€¢ UAV : Unmanned Aerial Vehicle
â€¢ UBC : Upper Conï¬dence Bound
â€¢ UGV : Unmanned Ground Vehicle
â€¢ UVFA : Universal Value Function Approximator

CRediT authorship contribution statement

Fadi AlMahamid: Conceptualization, Methodology,
Investigation, Formal Analysis, Validation, Writing - Origi-
nal Draft, Writing - Review & Editing. Katarina Grolinger:
Supervision, Writing - Review & Editing, Funding Acquisi-
tion.

Acknowledgments
This research has been supported by NSERC under grant
RGPIN-2018-06222

References

[1] Y. Lu, Z. Xue, G.-S. Xia, L. Zhang, A survey on vision-based UAV
navigation, Taylor & Francis Geo-spatial information science 21 (1)
(2018) 21â€“32. doi:10.1080/10095020.2017.1420509.

[2] F. Zeng, C. Wang, S. S. Ge, A survey on visual navigation for
artiï¬cial agents with deep reinforcement learning, IEEE Access 8
(2020) 135426â€“135442.

[3] R. Azoulay, Y. Haddad, S. Reches, Machine Learning Methods for
UAV Flocks Management-A Survey, IEEE Access 9 (2021) 139146â€“
139175.

[4] S. Aggarwal, N. Kumar, Path planning techniques for unmanned
aerial vehicles: A review, solutions, and challenges, Computer Com-
munications 149 (2020) 270â€“299.

[5] W. Lou, X. Guo, Adaptive trajectory tracking control using rein-
forcement learning for quadrotor, SAGE International Journal of
Advanced Robotic Systems 13 (1) (2016). doi:10.5772/62128.
[6] A. Guerra, F. Guidi, D. Dardari, P. M. DjuriÄ‡, Networks of UAVs
localization, arXiv preprint

of low-complexity for time-critical
arXiv:2108.13181 (2021).

[7] A. Guerra, F. Guidi, D. Dardari, P. M. DjuriÄ‡, Real-Time Learning
for THZ Radar Mapping and UAV Control, in: IEEE International
Conference on Autonomous Systems, 2021, pp. 1â€“5. doi:10.1109/
ICAS49788.2021.9551141.

[8] A. Guerra, D. Dardari, P. M. DjuriÄ‡, Dynamic radar network of
UAVs: A joint navigation and tracking approach, IEEE Access 8
(2020) 116454â€“116469.

[9] Y. Liu, Y. Wang, J. Wang, Y. Shen, Distributed 3D relative local-
ization of UAVs, IEEE Transactions on Vehicular Technology 69
(2020) 11756â€“11770.

[10] S. Zhang, R. PÃ¶hlmann, T. Wiedemann, A. Dammann, H. Wymeer-
sch, P. A. Hoeher, Self-aware swarm navigation in autonomous
exploration missions, Proceedings of the IEEE 108 (7) (2020) 1168â€“
1195.

[11] F. AlMahamid, K. Grolinger, Reinforcement Learning Algorithms:
An Overview and Classiï¬cation, in: IEEE Canadian Conference on
Electrical and Computer Engineering, 2021, pp. 1â€“7.

[12] R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction,

MIT Press, 2018.

[13] L.-J. Lin, Self-improving reactive agents based on reinforcement
learning, planning and teaching, Springer Machine learning 8 (3-4)
(1992) 293â€“321.

[14] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized experience

replay, arXiv:1511.05952 (2015).

[15] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller,
Deterministic Policy Gradient Algorithms, in: PMLR International
conference on machine learning, 2014, pp. 387â€“395.

[16] S. Zhou, B. Li, C. Ding, L. Lu, C. Ding, An Eï¬ƒcient Deep Rein-
forcement Learning Framework for UAVs, in: IEEE International
Symposium on Quality Electronic Design, 2020, pp. 323â€“328.
[17] P. Karthik, K. Kumar, V. Fernandes, K. Arya, Reinforcement Learn-
ing for Altitude Hold and Path Planning in a Quadcopter, in: IEEE
International Conference on Control, Automation and Robotics,
2020, pp. 463â€“467.

[18] A. M. Deshpande, R. Kumar, A. A. Minai, M. Kumar, Develop-
mental reinforcement learning of control policy of a quadcopter
UAV with thrust vectoring rotors, in: ASME Dynamic Systems and
Control Conference, Vol. 84287, 2020, p. V002T36A011.
10.1115/DSCC2020-3319.

doi:

[19] E. Camci, E. Kayacan, Learning motion primitives for planning swift
maneuvers of quadrotor, Springer Autonomous Robots 43 (7) (2019)
1733â€“1745.

[20] S. Li, P. Durdevic, Z. Yang, Optimal Tracking Control Based on Inte-
gral Reinforcement Learning for An Underactuated Drone, Elsevier
IFAC-PapersOnLine 52 (8) (2019) 194â€“199.

[21] C. Greatwood, A. G. Richards, Reinforcement learning and model
predictive control for robust embedded quadrotor guidance and
control, Springer Autonomous Robots 43 (7) (2019) 1681â€“1693.

[22] W. Koch, R. Mancuso, R. West, A. Bestavros, Reinforcement learn-
ing for UAV attitude control, ACM Transactions on Cyber-Physical
Systems 3 (2) (2019) 1â€“21.

[23] N. Salvatore, S. Mian, C. Abidi, A. D. George, A Neuro-Inspired
Approach to Intelligent Collision Avoidance and Navigation, in:
IEEE Digital Avionics Systems Conference, 2020, pp. 1â€“9.

[24] O. Bouhamed, H. Ghazzai, H. Besbes, Y. Massoud, A UAV-Assisted
Data Collection for Wireless Sensor Networks: Autonomous Navi-
gation and Scheduling, IEEE Access 8 (2020) 110446â€“110460.
[25] H. Huang, J. Gu, Q. Wang, Y. Zhuang, An Autonomous UAV
Navigation System for Unknown Flight Environment, in: IEEE
International Conference on Mobile Ad-Hoc and Sensor Networks,
2019, pp. 63â€“68.

[26] S.-Y. Shin, Y.-W. Kang, Y.-G. Kim, Automatic Drone Navigation in
Realistic 3D Landscapes using Deep Reinforcement Learning, in:
IEEE International Conference on Control, Decision and Informa-
tion Technologies, 2019, pp. 1072â€“1077.

[27] C. Wang, J. Wang, X. Zhang, X. Zhang, Autonomous Navigation
of UAV in Large-Scale Unknown Complex Environment with Deep
Reinforcement Learning, in: IEEE Global Conference on Signal and
Information Processing, 2017, pp. 858â€“862.

[28] C. Wang, J. Wang, Y. Shen, X. Zhang, Autonomous Navigation of
UAVs in Large-Scale Complex Environments: A Deep Reinforce-
ment Learning Approach, IEEE Transactions on Vehicular Technol-
ogy 68 (3) (2019) 2124â€“2136.

[29] A. Anwar, A. Raychowdhury, Autonomous Navigation via Deep
Reinforcement Learning for Resource Constraint Edge Nodes using
Transfer Learning, IEEE Access 8 (2020) 26549â€“26560.

[30] O. Bouhamed, H. Ghazzai, H. Besbes, Y. Massoud, Autonomous
UAV Navigation: A DDPG-Based Deep Reinforcement Learning
Approach, in: IEEE International Symposium on Circuits and Sys-
tems, 2020, pp. 1â€“5.

[31] Y. Yang, K. Zhang, D. Liu, H. Song, Autonomous UAV Navigation
in Dynamic Environments with Double Deep Q-Networks, in: IEEE
Digital Avionics Systems Conference, 2020, pp. 1â€“7.

[32] Y. Li, M. Li, A. Sanyal, Y. Wang, Q. Qiu, Autonomous UAV with
Learned Trajectory Generation and Control, in: IEEE International
Workshop on Signal Processing Systems, 2019, pp. 115â€“120.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 20 of 24

Autonomous UAV Navigation using RL: A Systematic Review

[33] Y. Chen, N. GonzÃ¡lez-Prelcic, R. W. Heath, Collision-free UAV nav-
igation with a monocular camera using deep reinforcement learning,
in: IEEE International Workshop on Machine Learning for Signal
Processing, 2020, pp. 1â€“6.

[34] R. B. Grando, J. C. de Jesus, P. L. Drews-Jr, Deep Reinforcement
Learning for Mapless Navigation of Unmanned Aerial Vehicles, in:
IEEE Latin American Robotics Symposium, Brazilian Symposium
on Robotics and Workshop on Robotics in Education, 2020, pp. 1â€“6.
[35] E. Camci, D. Campolo, E. Kayacan, Deep Reinforcement Learning
for Motion Planning of Quadrotors Using Raw Depth Images, Learn-
ing (RL) 10 (2020).

[36] C. Wang, J. Wang, J. Wang, X. Zhang, Deep-Reinforcement-
Learning-Based Autonomous UAV Navigation With Sparse Re-
wards, IEEE Internet of Things Journal 7 (7) (2020) 6180â€“6190.

[37] E. Cetin, C. Barrado, G. MuÃ±oz, M. Macias, E. Pastor, Drone
navigation and avoidance of obstacles through deep reinforcement
learning, in: IEEE Digital Avionics Systems Conference, 2019, pp.
1â€“7.

[38] S. D. Morad, R. Mecca, R. P. Poudel, S. Liwicki, R. Cipolla,
Embodied Visual Navigation with Automatic Curriculum Learning
in Real Environments, IEEE Robotics and Automation Letters 6 (2)
(2021) 683â€“690.

[39] P. Yan, C. Bai, H. Zheng, J. Guo, Flocking Control of UAV Swarms
with Deep Reinforcement Learning Approach, in: IEEE Interna-
tional Conference on Unmanned Systems, 2020, pp. 592â€“599.
[40] I. Yoon, M. A. Anwar, R. V. Joshi, T. Rakshit, A. Raychowdhury,
Hierarchical memory system with STT-MRAM and SRAM to sup-
port transfer and real-time reinforcement learning in autonomous
drones, IEEE Journal on Emerging and Selected Topics in Circuits
and Systems 9 (3) (2019) 485â€“497.

[41] G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg,
B. Boots, E. A. Theodorou, Information theoretic MPC for model-
based reinforcement learning, in: IEEE International Conference on
Robotics and Automation, 2017, pp. 1714â€“1721.

[42] L. He, N. Aouf, J. F. Whidborne, B. Song, Integrated moment-
based LGMD and deep reinforcement learning for UAV obstacle
avoidance, in: IEEE International Conference on Robotics and Au-
tomation, 2020, pp. 7491â€“7497.

[43] A. Singla, S. Padakandla, S. Bhatnagar, Memory-based deep re-
inforcement learning for obstacle avoidance in UAV with limited
environment knowledge, IEEE Transactions on Intelligent Trans-
portation Systems (2019).

[44] T.-C. Wu, S.-Y. Tseng, C.-F. Lai, C.-Y. Ho, Y.-H. Lai, Navigating
assistance system for quadcopter with deep reinforcement learning,
in: IEEE International Cognitive Cities Conference, 2018, pp. 16â€“
19.

[45] M. A. Anwar, A. Raychowdhury, NavREn-Rl: Learning to ï¬‚y in
real environment via end-to-end deep reinforcement learning using
monocular images, in: IEEE International Conference on Mecha-
tronics and Machine Vision in Practice, 2018, pp. 1â€“6.

[46] B. Zhou, W. Wang, Z. Wang, B. Ding, Neural Q-learning algorithm
based UAV obstacle avoidance, in: IEEE CSAA Guidance, Naviga-
tion and Control Conference, 2018, pp. 1â€“6.

[47] Z. Yijing, Z. Zheng, Z. Xiaoyi, L. Yang, Q-learning algorithm
based UAV path learning and obstacle avoidance approach, in: IEEE
Chinese Control Conference, 2017, pp. 3397â€“3402.

[48] A. Villanueva, A. Fajardo, Deep Reinforcement Learning with Noise
Injection for UAV Path Planning, in: IEEE International Conference
on Engineering Technologies and Applied Sciences, 2019, pp. 1â€“6.
[49] A. Walvekar, Y. Goel, A. Jain, S. Chakrabarty, A. Kumar, Vision
based autonomous navigation of quadcopter using reinforcement
learning, in: IEEE International Conference on Automation, Elec-
tronics and Electrical Engineering, 2019, pp. 160â€“165.

[50] B. Zhou, W. Wang, Z. Liu, J. Wang, Vision-based Navigation of
UAV with Continuous Action Space Using Deep Reinforcement
Learning, in: IEEE Chinese Control And Decision Conference,
2019, pp. 5030â€“5035.

[51] M. Hasanzade, E. Koyuncu, A Dynamically Feasible Fast Replan-
ning Strategy with Deep Reinforcement Learning, Springer Journal
of Intelligent & Robotic Systems 101 (1) (2021) 1â€“17.

[52] G. MuÃ±oz, C. Barrado, E. Ã‡etin, E. Salami, Deep reinforcement
learning for drone delivery, MDPI Drones 3 (3) (2019). doi:10.3390/
drones3030072.

[53] V. J. Hodge, R. Hawkins, R. Alexander, Deep reinforcement learning
for drone navigation using sensor data, Springer Neural Computing
and Applications 33 (6) (2021) 2015â€“2033.

[54] O. Doukhi, D.-J. Lee, Deep Reinforcement Learning for End-to-End
Local Motion Planning of Autonomous Aerial Robots in Unknown
Outdoor Environments: Real-Time Flight Experiments, MDPI Sen-
sors 21 (7) (2021) 2534. doi:10.3390/s21072534.

[55] V. A. Bakale, Y. K. VS, V. C. Roodagi, Y. N. Kulkarni, M. S. Patil,
S. Chickerur, Indoor Navigation with Deep Reinforcement Learning,
in: IEEE International Conference on Inventive Computation Tech-
nologies, 2020, pp. 660â€“665.

[56] C. J. Maxey, E. J. Shamwell, Navigation and collision avoidance
with human augmented supervisory training and ï¬ne tuning via re-
inforcement learning, in: SPIE Micro-and Nanotechnology Sensors,
Systems, and Applications XI, Vol. 10982, 2019, pp. 325 â€“ 334.
doi:10.1117/12.2518551.

[57] Y. Zhao, J. Guo, C. Bai, H. Zheng, Reinforcement Learning-Based
Collision Avoidance Guidance Algorithm for Fixed-Wing UAVs,
Hindawi Complexity 2021 (2021). doi:10.1155/2021/8818013.
[58] G. Tong, N. Jiang, L. Biyue, Z. Xi, W. Ya, D. Wenbo, UAV naviga-
tion in high dynamic environments: A deep reinforcement learning
approach, Elsevier Chinese Journal of Aeronautics 34 (2) (2021)
479â€“489.

[59] O. Bouhamed, X. Wan, H. Ghazzai, Y. Massoud, A DDPG-
Based Approach for Energy-aware UAV Navigation in Obstacle-
constrained Environment, in: IEEE World Forum on Internet of
Things, 2020, pp. 1â€“6.

[60] O. Walker, F. Vanegas, F. Gonzalez, S. Koenig, A Deep Reinforce-
ment Learning Framework for UAV Navigation in Indoor Environ-
ments, in: IEEE Aerospace Conference, 2019, pp. 1â€“14.

[61] O. Bouhamed, H. Ghazzai, H. Besbes, Y. Massoud, A Generic
Spatiotemporal Scheduling for Autonomous UAVs: A Reinforce-
ment Learning-Based Approach, IEEE Open Journal of Vehicular
Technology 1 (2020) 93â€“106.

[62] J. Zhang, Z. Yu, S. Mao, S. C. Periaswamy, J. Patton, X. Xia, IADRL:
Imitation augmented deep reinforcement learning enabled UGV-
UAV coalition for tasking in complex environments, IEEE Access
8 (2020) 102335â€“102347.

[63] X. Yu, Y. Wu, X.-M. Sun, A Navigation Scheme for a Random Maze
Using Reinforcement Learning with Quadrotor Vision, in: IEEE
European Control Conference, 2019, pp. 518â€“523.

[64] H. Li, S. Wu, P. Xie, Z. Qin, B. Zhang, A Path Planning for One
UAV Based on Geometric Algorithm, in: IEEE CSAA Guidance,
Navigation and Control Conference, 2018, pp. 1â€“5.

[65] D. Sacharny, T. C. Henderson, Optimal Policies in Complex Large-
scale UAS Traï¬ƒc Management, in: IEEE International Conference
on Industrial Cyber Physical Systems, 2019, pp. 352â€“357.

[66] E. Camci, E. Kayacan, Planning swift maneuvers of quadcopter
using motion primitives explored by reinforcement learning, in:
IEEE American Control Conference, 2019, pp. 279â€“285.

[67] A. Guerra, F. Guidi, D. Dardari, P. M. Djuric, Reinforcement learn-
ing for UAV autonomous navigation, mapping and target detection,
in: ION Position, Location and Navigation Symposium, 2020, pp.
1004â€“1013.

[68] Z. Cui, Y. Wang, UAV Path Planning Based on Multi-Layer Re-
inforcement Learning Technique, IEEE Access 9 (2021) 59486â€“
59497.

[69] Z. Wang, H. Li, Z. Wu, H. Wu, A pretrained proximal policy
optimization algorithm with reward shaping for aircraft guidance to
a moving destination in three-dimensional continuous space, SAGE
International Journal of Advanced Robotic Systems 18 (1) (2021).
doi:10.1177/1729881421989546.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 21 of 24

Autonomous UAV Navigation using RL: A Systematic Review

[70] H. Eslamiat, Y. Li, N. Wang, A. K. Sanyal, Q. Qiu, Autonomous
waypoint planning, optimal trajectory generation and nonlinear
tracking control for multi-rotor UAVs, in: IEEE European Control
Conference, 2019, pp. 2695â€“2700.

[71] N. Imanberdiyev, C. Fu, E. Kayacan, I.-M. Chen, Autonomous Nav-
igation of UAV by using Real-Time Model-Based Reinforcement
Learning, in: IEEE International conference on control, automation,
robotics and vision, 2016, pp. 1â€“6.

[72] S. F. Abedin, M. S. Munir, N. H. Tran, Z. Han, C. S. Hong, Data
freshness and energy-eï¬ƒcient UAV navigation optimization: A deep
reinforcement learning approach, IEEE Transactions on Intelligent
Transportation Systems (2020).

[73] W. Andrew, C. Greatwood, T. Burghardt, Deep learning for explo-
ration and recovery of uncharted and dynamic targets from UAV-like
vision, in: IEEE RSJ International Conference on Intelligent Robots
and Systems, 2018, pp. 1124â€“1131.

[74] H. X. Pham, H. M. La, D. Feil-Seifer, L. Van Nguyen, Reinforcement
learning for autonomous UAV navigation using function approxi-
mation, in: IEEE International Symposium on Safety, Security, and
Rescue Robotics, 2018, pp. 1â€“6.

[75] S. Kulkarni, V. Chaphekar, M. M. U. Chowdhury, F. Erden, I. Gu-
venc, UAV aided search and rescue operation using reinforcement
learning, in: IEEE SoutheastCon, Vol. 2, 2020, pp. 1â€“8.

[76] A. Peake, J. McCalmon, Y. Zhang, B. Raiford, S. Alqahtani, Wilder-
ness search and rescue missions using deep reinforcement learning,
in: IEEE International Symposium on Safety, Security, and Rescue
Robotics, 2020, pp. 102â€“107.

[77] M. A. Akhlouï¬, S. Arola, A. Bonnet, Drones chasing drones: Re-
inforcement learning and deep search area proposal, MDPI Drones
3 (3) (2019). doi:10.3390/drones3030058.

[78] R. Polvara, M. Patacchiola, S. Sharma, J. Wan, A. Manning, R. Sut-
ton, A. Cangelosi, Toward end-to-end control for UAV autonomous
landing via deep reinforcement learning, in: IEEE International
conference on unmanned aircraft systems, 2018, pp. 115â€“123.
[79] R. Polvara, S. Sharma, J. Wan, A. Manning, R. Sutton, Autonomous
Vehicular Landings on the Deck of an Unmanned Surface Vehicle
using Deep Reinforcement Learning, Cambridge Core Robotica
37 (11) (2019) 1867â€“1882. doi:10.1017/S0263574719000316.
[80] S. Lee, T. Shim, S. Kim, J. Park, K. Hong, H. Bang, Vision-
based autonomous landing of a multi-copter unmanned aerial vehicle
using reinforcement learning, in: IEEE International Conference on
Unmanned Aircraft Systems, 2018, pp. 108â€“114.

[81] C. Wang, J. Wang, X. Zhang, A Deep Reinforcement Learning
Approach to Flocking and Navigation of UAVs in Large-Scale
Complex Environments, in: IEEE Global Conference on Signal and
Information Processing, 2018, pp. 1228â€“1232.

[82] G. T. Lee, C. O. Kim, Autonomous Control of Combat Unmanned
Aerial Vehicles to Evade Surface-to-Air Missiles Using Deep Rein-
forcement Learning, IEEE Access 8 (2020) 226724â€“226736.
[83] Ã. Madridano, A. Al-Kaï¬€, P. Flores, D. MartÃ­n, A. de la Escalera,
Software Architecture for Autonomous and Coordinated Navigation
of UAV Swarms in Forest and Urban Fireï¬ghting, MDPI Applied
Sciences 11 (3) (2021). doi:10.3390/app11031258.

[84] D. Wang, T. Fan, T. Han, J. Pan, A Two-Stage Reinforcement Learn-
ing Approach for Multi-UAV Collision Avoidance Under Imperfect
Sensing, IEEE Robotics and Automation Letters 5 (2) (2020) 3098â€“
3105.

[85] J. Moon, S. Papaioannou, C. Laoudias, P. Kolios, S. Kim, Deep
Reinforcement Learning Multi-UAV Trajectory Control for Target
Tracking, IEEE Internet of Things Journal (2021).

[86] C. H. Liu, X. Ma, X. Gao, J. Tang, Distributed energy-eï¬ƒcient multi-
UAV navigation for long-term communication coverage by deep
reinforcement learning, IEEE Transactions on Mobile Computing
19 (6) (2019) 1274â€“1285.

[87] S. Omi, H.-S. Shin, A. Tsourdos, J. Espeland, A. Buchi, Introduction
to UAV swarm utilization for communication on the move terminals
tracking evaluation with reinforcement learning technique, in: IEEE
European Conference on Antennas and Propagation, 2021, pp. 1â€“5.

[88] A. Viseras, M. Meissner, J. Marchal, Wildï¬re Front Monitoring with
Multiple UAVs using Deep Q-Learning, IEEE Access (2021).
[89] A. Bonnet, M. A. Akhlouï¬, UAV pursuit using reinforcement learn-
ing, in: SPIE Unmanned Systems Technology XXI, Vol. 11021,
International Society for Optics and Photonics, 2019, pp. 51 â€“ 58.
doi:10.1117/12.2520310.

[90] S. Fan, G. Song, B. Yang, X. Jiang, Prioritized Experience Replay
in Multi-Actor-Attention-Critic for Reinforcement Learning, IOP-
science Journal of Physics: Conference Series 1631 (2020). doi:
10.1088/1742-6596/1631/1/012040.

[91] A. Majd, A. Ashraf, E. Troubitsyna, M. Daneshtalab, Integrating
learning, optimization, and prediction for eï¬ƒcient navigation of
swarms of drones, in: IEEE Euromicro International Conference on
Parallel, Distributed and Network-based Processing, 2018, pp. 101â€“
108.

[92] A. Chapman, Drone Types: Multi-Rotor vs Fixed-Wing vs Sin-
gle Rotor vs Hybrid VTOL, https://www.auav.com.au/articles/
drone-types/, (Accessed: 01.11.2021) (2016).

[93] M. Elnaggar, N. Bezzo, An IRL Approach for Cyber-Physical Attack
Intention Prediction and Recovery, in: IEEE American Control
Conference, 2018, pp. 222â€“227.

[94] H. D. Escobar-Alvarez, N. Johnson, T. Hebble, K. Klingebiel, S. A.
Quintero, J. Regenstein, N. A. Browning, R-ADVANCE: Rapid
Adaptive Prediction for Vision-based Autonomous Navigation, Con-
trol, and Evasion, WOL Journal of Field Robotics 35 (1) (2018) 91â€“
100. doi:10.1002/rob.21744.

[95] C. W. Reynolds, Flocks, herds and schools: A distributed behavioral
model, in: Proceedings of the 14th annual conference on Computer
graphics and interactive techniques, 1987, pp. 25â€“34.

[96] R. Olfati-Saber, Flocking for multi-agent dynamic systems: Algo-
rithms and theory, IEEE Transactions on automatic control 51 (3)
(2006) 401â€“420.

[97] H. M. La, W. Sheng, Flocking control of multiple agents in noisy
environments, in: IEEE International Conference on Robotics and
Automation, 2010, pp. 4964â€“4969.

[98] Y. Jia, J. Du, W. Zhang, L. Wang, Three-dimensional leaderless
ï¬‚ocking control of large-scale small unmanned aerial vehicles, El-
sevier IFAC-PapersOnLine 50 (1) (2017) 6208â€“6213.

[99] H. Su, X. Wang, Z. Lin, Flocking of multi-agents with a virtual
leader, IEEE transactions on automatic control 54 (2) (2009) 293â€“
307.

[100] S. A. Quintero, G. E. Collins, J. P. Hespanha, Flocking with ï¬xed-
wing UAVs for distributed sensing: A stochastic optimal control
approach, in: IEEE American Control Conference, 2013, pp. 2025â€“
2031.

[101] S.-M. Hung, S. N. Givigi, A Q-learning approach to ï¬‚ocking with
UAVs in a stochastic environment, IEEE transactions on cybernetics
47 (1) (2016) 186â€“197.

[102] K. Morihiro, T. Isokawa, H. Nishimura, M. Tomimasu, N. Kamiura,
N. Matsui, Reinforcement Learning Scheme for Flocking Behavior
Emergence, Journal of Advanced Computational Intelligence and
Intelligent Informatics 11 (2) (2007) 155â€“161.

[103] Z. Xu, Y. Lyu, Q. Pan, J. Hu, C. Zhao, S. Liu, Multi-vehicle ï¬‚ocking
control with deep deterministic policy gradient method, in: IEEE
International Conference on Control and Automation, 2018, pp.
306â€“311.

[104] O. Robotics, ROS Home Page, https://www.ros.org/, (Accessed:

01.11.2021) (2021).

[105] M. Research, Microsoft AirSim Home Page, https://microsoft.

github.io/AirSim/, (Accessed: 01.11.2021) (2021).

[106] O. S. R. Foundation, Gazebo Home Page, https://gazebosim.org/,

(Accessed: 01.11.2021) (2021).

[107] E. Games, Epic Games Unreal Engine Home Page, https://www.

unrealengine.com, (Accessed: 01.11.2021) (2021).

[108] C. J. Watkins, P. Dayan, Q-learning, Springer Machine learning 8 (3-

4) (1992) 279â€“292. doi:10.1007/978-1-4615-3618-5_4.

[109] A. Fotouhi, M. Ding, M. Hassan, Deep Q-Learning for Two-Hop
Communications of Drone Base Stations, MDPI Sensors 21 (6)

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 22 of 24

Autonomous UAV Navigation using RL: A Systematic Review

(2021). doi:10.3390/s21061960.

[110] G. A. Rummery, M. Niranjan, On-line Q-learning using connection-

ist systems, Vol. 37, University of Cambridge, 1994.

[111] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, M. Riedmiller, Playing Atari with Deep Reinforcement
Learning, arXiv:1312.5602 (2013).

[112] H. Huang, Y. Yang, H. Wang, Z. Ding, H. Sari, F. Adachi, Deep
reinforcement learning for UAV navigation through massive MIMO
technique, IEEE Transactions on Vehicular Technology 69 (1)
(2019) 1117â€“1121.

[113] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning
with double Q-Learning, AAAI Conference on Artiï¬cial Intelli-
gence (2016) 2094â€“2100.

[114] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, N. Freitas,
Dueling Network Architectures for Deep Reinforcement Learning,
in: PMLR International Conference on Machine Learning, Vol. 48,
2016, pp. 1995â€“2003.

[115] M. Hausknecht, P. Stone, Deep Recurrent Q-learning for partially

observable MDPS, arXiv:1507.06527 (2015).

[116] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves,
V. Mnih, R. Munos, D. Hassabis, O. Pietquin, et al., Noisy networks
for exploration, arXiv:1706.10295 (2017).

[117] M. G. Bellemare, W. Dabney, R. Munos, A distributional perspective
on reinforcement learning, in: PMLR International Conference on
Machine Learning, 2017, pp. 449â€“458.

[118] W. Dabney, M. Rowland, M. G. Bellemare, R. Munos, Distributional
reinforcement learning with quantile regression, AAAI Conference
on Artiï¬cial Intelligence (2018).

[119] W. Dabney, G. Ostrovski, D. Silver, R. Munos, Implicit quantile
networks for distributional reinforcement learning, in: PMLR Inter-
national conference on machine learning, 2018, pp. 1096â€“1105.

[120] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski,
W. Dabney, D. Horgan, B. Piot, M. Azar, D. Silver, Rainbow:
Combining improvements in deep reinforcement learning, AAAI
Conference on Artiï¬cial Intelligence (2018).

[121] D. Yang, L. Zhao, Z. Lin, T. Qin, J. Bian, T.-Y. Liu, Fully parame-
terized quantile function for distributional reinforcement learning,
Advances in Neural Information Processing Systems 32 (2019)
6193â€“6202.

[122] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, W. Dabney,
Recurrent experience replay in distributed reinforcement learning,
International Conference on Learning Representations (2018).
[123] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel,
H. Van Hasselt, D. Silver, Distributed prioritized experience replay,
arXiv:1803.00933 (2018).

[124] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kap-
turowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al., Never
give up: Learning directed exploration strategies, arXiv:2002.06038
(2020).

[125] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi,
Z. D. Guo, C. Blundell, Agent57: Outperforming the atari human
benchmark, in: PMLR International Conference on Machine Learn-
ing, 2020, pp. 507â€“517.

[126] D. Zhao, H. Wang, K. Shao, Y. Zhu, Deep reinforcement learning
with experience replay based on SARSA, in: IEEE Symposium
Series on Computational Intelligence, 2016, pp. 1â€“6.

[127] R. J. Williams, Simple statistical gradient-following algorithms for
connectionist reinforcement learning, Springer Machine learning
8 (3-4) (1992) 229â€“256.

[128] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust
Region Policy Optimization, in: PMLR International Conference on
Machine Learning, Vol. 37, 2015, pp. 1889â€“1897.

[132] A. X. Lee, A. Nagabandi, P. Abbeel, S. Levine, Stochastic latent
actor-critic: Deep reinforcement learning with a latent variable
model, arXiv:1907.00953 (2019).

[133] S. Zhang, H. Yao, ACE: An Actor Ensemble Algorithm for contin-
uous control with tree search, in: AAAI Conference on Artiï¬cial
Intelligence, Vol. 33, 2019, pp. 5789â€“5796.
v33i01.33015789.

doi:10.1609/aaai.

[134] S. Zhang, S. Whiteson, DAC: The double actor-critic architecture for

learning options, arXiv:1904.12691 (2019).

[135] N. Heess, J. J. Hunt, T. P. Lillicrap, D. Silver, Memory-Based Control
with Recurrent Neural Networks, arXiv:1512.04455 (2015).
[136] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, D. Wierstra, Continuous control with deep reinforcement
learning, arXiv:1509.02971 (2015).

[137] S. Fujimoto, H. Hoof, D. Meger, Addressing function approximation
error in actor-critic methods, in: PMLR International Conference on
Machine Learning, 2018, pp. 1587â€“1596.

[138] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft Actor-Critic:
Oï¬€-policy maximum entropy deep reinforcement learning with a
stochastic actor, in: PMLR International Conference on Machine
Learning, 2018, pp. 1861â€“1870.

[139] G. Barth-Maron, M. W. Hoï¬€man, D. Budden, W. Dabney, D. Hor-
gan, D. Tb, A. Muldal, N. Heess, T. Lillicrap, Distributed distribu-
tional deterministic policy gradients, arXiv:1804.08617 (2018).

[140] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, K. Kavukcuoglu, Asynchronous methods for deep rein-
forcement learning, in: PMLR International Conference on Machine
Learning, 2016, pp. 1928â€“1937.

[141] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne,
Y. Tassa, T. Erez, Z. Wang, S. Eslami, et al., Emergence of loco-
motion behaviours in rich environments, arXiv:1707.02286 (2017).
[142] C. Alfredo, C. Humberto, C. Arjun, Eï¬ƒcient parallel methods for
deep reinforcement learning, in: The Multi-disciplinary Conference
on Reinforcement Learning and Decision Making, 2017, pp. 1â€“6.

[143] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu,
N. de Freitas, Sample eï¬ƒcient actor-critic with experience replay,
arXiv:1611.01224 (2016).

[144] A. Gruslys, W. Dabney, M. G. Azar, B. Piot, M. Bellemare,
R. Munos, The reactor: A fast and sample-eï¬ƒcient actor-critic agent
for reinforcement learning, arXiv:1704.04651 (2017).

[145] Y. Wu, E. Mansimov, R. B. Grosse, S. Liao, J. Ba, Scalable Trust-
Region method for deep reinforcement learning using Kronecker-
Factored Approximation, in: Advances in Neural Information Pro-
cessing Systems, 2017, pp. 5279â€“5288.

[146] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch,
Multi-Agent Actor-Critic for mixed cooperative-competitive envi-
ronments, arXiv:1706.02275 (2017).

[147] J. Ackermann, V. Gabler, T. Osa, M. Sugiyama, Reducing overesti-
mation bias in multi-agent domains using double centralized critics,
arXiv:1910.01465 (2019).

[148] S. Iqbal, F. Sha, Actor-Attention-Critic for Multi-Agent Reinforce-
ment Learning, in: PMLR International Conference on Machine
Learning, 2019, pp. 2961â€“2970.

[149] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward,
Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al., IMPALA: Scalable
distributed deep-rl with importance weighted actor-learner archi-
tectures, in: PMLR International Conference on Machine Learning,
2018, pp. 1407â€“1416.

[150] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, M. Michalski,
Seed RL: Scalable and Eï¬ƒcient Deep-RL with accelerated central
inference, arXiv:1910.06591 (2019).

[151] H. Hasselt, Double Q-learning, Advances in Neural Information

[129] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proxi-

Processing Systems 23 (2010) 2613â€“2621.

mal Policy Optimization Algorithms, arXiv:1707.06347 (2017).

[152] Y. Burda, H. Edwards, A. Storkey, O. Klimov, Exploration by

[130] K. Cobbe, J. Hilton, O. Klimov, J. Schulman, Phasic Policy Gradient,

random network distillation, arXiv:1810.12894 (2018).

arXiv:2009.04416 (2020).

[131] Y. Liu, P. Ramachandran, Q. Liu, J. Peng, Stein Variational Policy

Gradient, arXiv:1704.02399 (2017).

[153] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, Policy gradi-
ent methods for reinforcement learning with function approximation,
in: Advances in Neural Information Processing Systems, 2000, pp.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 23 of 24

Autonomous UAV Navigation using RL: A Systematic Review

1057â€“1063.

[154] S.-Y. Shin, Y.-W. Kang, Y.-G. Kim, Obstacle Avoidance Drone by
Deep Reinforcement Learning and Its Racing with Human Pilot,
MDPI Applied Sciences 9 (24) (2019). doi:10.3390/app9245571.

[155] Q. Liu, D. Wang, Stein variational gradient descent: A general

purpose bayesian inference algorithm, arXiv:1608.04471 (2016).

[156] D. Wierstra, A. FÃ¶rster, J. Peters, J. Schmidhuber, Recurrent Policy
Gradients, Logic Journal of the IGPL 18 (5) (2010) 620â€“634.
[157] J. Martens, R. Grosse, Optimizing Neural Networks with Kronecker-
Factored Approximate Curvature, in: PMLR International confer-
ence on machine learning, 2015, pp. 2408â€“2417.

[158] R. Munos, T. Stepleton, A. Harutyunyan, M. G. Bellemare, Safe
and eï¬ƒcient oï¬€-policy reinforcement learning, arXiv:1606.02647
(2016).

[159] M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin,
R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, et al.,
What Matters In On-Policy Reinforcement Learning? A Large-Scale
Empirical Study, CoRR abs/2006.05990 (2020).

Fadi AlMahamid received the B.Sc. degree
(Hons.)
in Computer Science from Princess
Sumaya University for Technology (PSUT), Am-
man, Jordan, in 2001, and M.Sc. degree (Hons.)
in Computer Science from New York Institute
of Technology (NYIT), Amman, Jordan, in 2003.
Also, he obtained another M.Sc. from the Univer-
sity of Western Ontario (UWO), London, Ontario,
Canada, in 2019, where he is currently pursuing the
Ph.D. degree in Software Engineering with the De-
partment of Electrical and Computer Engineering.
He has extensive industry experience of more than
15 years. His current research interests include ma-
chine learning, autonomous vehicles focusing on
navigation problems, IoT architectures, and sensor
data analytics.

Katarina Grolinger received the B.Sc. and M.Sc.
degrees in mechanical engineering from the Uni-
versity of Zagreb, Croatia, and the M.Eng. and
Ph.D. degrees in software engineering from West-
ern University, London, Canada. She is currently
an Assistant Professor with the Department of
Electrical and Computer Engineering, Western
University. She has been involved in the software
engineering area in academia and industry, for over
20 years. Her current research interests include
machine learning, sensor data analytics, data man-
agement, and the IoT.

AlMahamid & Grolinger: Preprint submitted to Elsevier

Page 24 of 24

