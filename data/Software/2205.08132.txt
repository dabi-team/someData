LATENT VARIABLE METHOD DEMONSTRATOR – SOFTWARE
FOR UNDERSTANDING MULTIVARIATE DATA ANALYTICS
ALGORITHMS

2
2
0
2

p
e
S
6
2

]
L
M

.
t
a
t
s
[

2
v
2
3
1
8
0
.
5
0
2
2
:
v
i
X
r
a

Joachim Schaeffer∗

Control and Cyber-Physical Systems Laboratory
Technical University of Darmstadt
Karolinenpl. 5
Darmstadt, 64289, Germany

Richard D. Braatz

Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, 02139, MA, USA
braatz@mit.edu

May 15, 2022

ABSTRACT

The ever-increasing quantity of multivariate process data is driving a need for skilled engineers to
analyze, interpret, and build models from such data. Multivariate data analytics relies heavily on
linear algebra, optimization, and statistics and can be challenging for students to understand given that
most curricula do not have strong coverage in the latter three topics. This article describes interactive
software – the Latent Variable Demonstrator (LAVADE) – for teaching, learning, and understanding
latent variable methods. In this software, users can interactively compare latent variable methods such
as Partial Least Squares (PLS), and Principal Component Regression (PCR) with other regression
methods such as Least Absolute Shrinkage and Selection Operator (lasso), Ridge Regression (RR), and
Elastic Net (EN). LAVADE helps to build intuition on choosing appropriate methods, hyperparameter
tuning, and model coefﬁcient interpretation, fostering a conceptual understanding of the algorithms’
differences. The software contains a data generation method and three chemical process datasets,
allowing for comparing results of datasets with different levels of complexity. LAVADE is released
as open-source software so that others can apply and advance the tool for use in teaching or research.

Keywords Education · Teaching · Data Analytics · Software · Machine Learning

1

Introduction

Latent variable methods such as partial least squares are among the most widely applied data analytics tools for
applications to chemical, physical, and biological processes. It can be challenging to teach these multivariable statistical
methods in a way that the students are able to consistently apply these methods effectively in practice, especially
considering that many undergraduate programs do not require that their students take courses in linear algebra or applied
statistics. The result is that many practitioners use alternative methods instead, such as correlating a peak absorbance
to each concentration when calibrating a spectral measurement, which can produce models of much lower accuracy.
Alternatively, the latent variable methods are often used in a black-box manner, i.e., without understanding when a
situation occurs in which the methods should not be applied, or how to revise the way that data are fed to the method to
work around some imperfections in the data such as sensor bias. After all, many software packages are available for
applying latent variable methods, and it might seem that a deep understanding is not needed for problem-solving.

A lack of understanding, however, makes it challenging to explain and interpret the models created by these methods,
particularly in relating the models generated with the chemistry/physics/biology occurring in the process, to reconcile
the data analytics results with domain knowledge. The synthesis of domain knowledge with data analytics is the added
value of a well-trained applied data scientist/engineer and is likely to result in the best solutions for the particular

∗Parts of this work were done at ETH Zurich.

 
 
 
 
 
 
LAVADE

problem at hand. Furthermore, preliminary black-box results can lead to choosing overly complicated models that
overﬁt the data. Advancing the understanding and intuition on latent variable methods are needed to avoid overﬁtting
for some types of biased data and ultimately assure model interpretability, leading to higher value, acceptance, and
applicability.

This article describes software and examples developed to train students to achieve a deep understanding of latent
variable methods (e.g., Chiang et al., 2000; Mardia, 2003) and the related machine learning methods of lasso (Tibshirani,
1996), ridge regression (Hoerl and Kennard, 1970), and elastic net (Zou and Hastie, 2005). The graphical user interface
is designed for the explicit purpose of teaching undergraduate and graduate students, which is a distinguishing feature
from the graphical user interfaces in existing chemometrics software packages that are focused on just applying a
method to a dataset. The software takes the perspective of the optimization being solved to help the understanding of
the relationship between the latent variable method selected and the results produced.

This tool, referred to as the Latent Variable Demonstrator (LAVADE), compares a wide range of latent variable
regression and other techniques for four different datasets. The examples are designed to be easy to understand, and
various options to customize the problem are available to learn precisely how the different algorithms approach the
model construction.

This article is organized as follows. Section 2 describes the latent variable and other regression techniques in the
software. Section 3 describes the software design and architecture from a high-level perspective, and Section 4 describes
the datasets. Section 5 shows illustrative examples of the software and is followed by the conclusion.

2 Regression Techniques

The regression models considered in this article are linear, static models with the general form

y = β0x0 + β1x1 + · · · + βn−1xn−1 + (cid:15)
where the output y, inputs xi, and regression coefﬁcients βi are real scalars; x0 = 1; n is the number of inputs and
regression coefﬁcients; and (cid:15) denotes the error term. Without loss of generality, this model can be rewritten in matrix
form as

(1)

y = Xβ + (cid:15)
(2)
where the input data matrix X ∈ Rm×n, the vector of regression coefﬁcients β ∈ Rn×1, y ∈ Rm×1, and m is the
number of observations. The elements of X can be any mix of raw input data and transformations of the raw data (aka
features). The errors (cid:15) are assumed to be homoscedastic, have zero mean, and are uncorrelated.

Model building involves determining the vector β from the data X and y that minimizes the error (cid:15) concerning a
deﬁned measure of the error. The goal of multivariate regression is to ﬁnd a good solution for β when there is a
relatively low number of observations relative to the number of regression coefﬁcients. For some speciﬁc applications,
such as building a model to relate spectra to concentrations, (2) may not even have a unique solution.

Some special cases of (2) of interest are:

(a) The case where the input data matrix X has more rows than columns (m > n) and has full column rank, i.e.,
rank(X) = n. This case arises when many samples are available for building a model for predicting the output
y from a small number of inputs (or calculated features). It is thus very unlikely that any vector y ∈ Rm×1 is
in the column space of X, due to the fact that the columns of X span only a small subspace of Rm.

(b) The case where the input data matrix X has fewer rows than columns (m < n) and X has full row rank,
i.e., rank(X) = m. This case can arise, for example, when spectral data are available for a large number of
frequencies for building a model for predicting concentration from a relatively small number of samples of
known concentration. In this case, the vector y lies in the column space of X, and there exists no unique
solution to (2). This case can be interpreted from the perspective of the right nullspace of X, N (X), which
is the set of nonzero vectors v that satisfy Xv = 0. In this case, this nullspace is not empty, that is, the
nullspace consists of more vectors than the zero vector (0). The vectors can be used to deﬁne a basis for the
nullspace, and subsequently used to parameterize the inﬁnite set of vectors of regression coefﬁcients β that
satisfy y = Xβ.

(c) The case where m = n and rank(X) = m = n, for which (2) has a unique solution for y = Xβ. This case
does not occur when working with larger datasets – due to biophysicochemical constraints such as associated
with conservation equations and reaction networks – and is not considered further in this article.

Matrices with redundant rows and/or columns can be reduced to cases (a) or (b) by removing redundant rows and/or
columns.

2

LAVADE

Different approaches ﬁnd good solutions for cases (a) and (b) by solving

Xtrain

ˆβ = ˆy

(3)

where ˆβ is the vector of parameters estimated from the data Xtrain leading to the response ˆy. The latter is related to the
measured model outputs by additive noise (cid:15).

ˆy + (cid:15) = y.

(4)

The intent of the model is to be capable of making predictions of unseen data by taking a weighted combination of
columns based on the estimated weights ˆβ. A fundamental question in statistics is how to ﬁnd a good (or best) vector of
model parameters ˆβ.2
This section describes different approaches that have been developed for ﬁnding ˆβ. To simplify the notation, β and X
are used instead of ˆβ and Xtrain; in all cases, β refers to parameters estimated from training data and X refers to the
training data. The ﬁrst part of this section is a compact introduction to Ordinary Least Squares (OLS), Least Absolute
Shrinkage and Selection Operator (lasso), Ridge Regression (RR), and elastic net (EN) regression. The second part is
an introduction to Principal Component Analysis (PCA), Principal Component Regression (PCR), and Partial Least
Squares (PLS), which are methods for dimensionality reduction and regression is performed subsequently in the lower
dimensional space.

2.1 Ordinary Least Squares Regression

The key idea of OLS regression is to ﬁnd a solution β that minimizes the L2-norm of the model error (cid:15) (Strang, 2016),

min
β

(cid:107)y − Xβ(cid:107)2
2.

Practically, this optimization can be solved by multiplying (3) on the left by X(cid:62),

X(cid:62)Xβ = X(cid:62)y,

which is known as the normal equation. For Case a, X(cid:62)X is invertible, which leads to the analytical solution

β = (X(cid:62)X)−1X(cid:62)y.

(5)

(6)

(7)

This optimization formulation and solution are known as OLS. The Gauss-Markov theorem states that the OLS solution
is optimal with the assumed error structure (4).
In Case b, X(cid:62)X is singular. A solution for β in this case is

β = X(cid:62)(XX(cid:62))

−1

y

(8)

which is known as the minimum norm solution (Monticelli, 1999).

2.2 Lasso

OLS estimates may have low bias but have very large variance for many real-world data analytics problems, resulting in
low prediction accuracy on unseen data. Lasso is a strategy for addressing this problem by adding the L1-norm of the
weights as a penalty to the least-squares objective (Tibshirani, 1996),

min
β

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

(9)

For positive values for λ, lasso builds a model in which some weights are zero, βi = 0, due to the structure of the
optimization. The larger the value of λ, the more weights are forced to zero.

While there exists no closed-form solution for (9), common solvers are available. The L1-norm penalty on β in (9) can
be rewritten as linear constraints and the resulting optimization can be written as a convex quadratic program which can
be solved efﬁciently using convex programming. A visual explanation for how the shape of the constraints and shape of
the objective function in the optimization are responsible for the sparse solution can be found in Tibshirani (1996).

2Multiple terms are used in the literature to refer to ˆβ, including model parameters and regression coefﬁcients. From (3), it is
clear that ˆβ can be interpreted as being a vector of weights, that is, the model output is a weighted combination of the model inputs,
with the weights being the elements of ˆβ.

3

LAVADE

Models that use only a subset of available columns of the data matrix X are called sparse models. lasso selects a subset
of columns to be used in the regression and can potentially lead to increased interpretability of results by removing
measurements that are not needed in the model prediction.

A drawback of lasso is that, when some columns in X have a high pairwise correlation, lasso tends to select only one
column and the choice of column can be sensitive to arbitrarily small perturbation in the data (Zou and Hastie, 2005).
This randomness reduces the suitability of such models for interpreting the system in terms of which model inputs
appear in the model.

2.3 Ridge Regression

The underlying motivation for Ridge Regression (RR) is identical to lasso. The difference between the methods is that
Ridge Regression (RR) adds a L2-norm penalty to the objective function,
2 + λ(cid:107)β(cid:107)2
2.

(cid:107)y − Xβ(cid:107)2

(10)

min
β

Setting the derivative of the objective function of (10) to zero leads to the closed-form solution (Hoerl and Kennard,
1970)

β = (X(cid:62)X + λI)−1X(cid:62)y.

(11)

As in lasso, the L2-norm penalty on β in (10) can be rewritten as a constraint. Due to the shape of the constraint in the
resulting optimization, the weights βi in RR will never reach zero although they can be arbitrarily small (James et al.,
2021). Similarly to lasso, RR improves the predictive accuracy of the model by introducing a bias that reduces variance
in the estimated parameters (Zou and Hastie, 2005). However, models produced by RR can be challenging to interpret,
since all model inputs are retained in the model even for high values of λ.

2.4 Elastic Net

The Elastic Net (EN) is a combination of lasso and RR,

min
β

(cid:107)y − Xβ(cid:107)2

2 + λP (β)

P (β) =

1 − α
2

(cid:107)β(cid:107)2

2 + α(cid:107)β(cid:107)1

(12)

(13)

for α ∈ (0, 1) and non-negative values of λ. The L1-norm provides the ability to set unimportant parameters to zero
whereas the L2-norm improves robustness in the selection of which parameters to retain in the model. Lasso and RR
are limiting cases of the EN, for α equal to 1 and 0, respectively.

2.5 PCA and PCR

Principal Component Analysis (PCA) is an approach to manipulating the data matrix X ∈ Rm×n. The idea of PCA
is to ﬁnd a lower dimensional representation of X while conserving the variations in the data. From an optimization
perspective, PCA solves

max
(cid:107)w(cid:107)2=1

(cid:107)Xw(cid:107)2
2

(14)

for each principal component, with subsequent principal components also required to be orthogonal to the previous
principal components. PCA ﬁnds a weighted combination of the mean subtracted columns of X retaining maximal
variance. For the ﬁrst principal component, the constrained optimization (14) is equivalent to the unconstrained
optimization

w1 = arg max

W

w(cid:62)X(cid:62)Xw
w(cid:62)w

.

(15)

The optimization (15) is solved by w being the eigenvector corresponding to the largest eigenvalue of the positive
semideﬁnite matrix X(cid:62)X.

More broadly, performing PCA involves the steps:

1. Center the columns of X to have zero mean.

2. Find the singular values and corresponding singular vectors. The singular values and right singular vectors of

X correspond to the eigenvalues and eigenvectors of the covariance matrix S = 1

m−1 X(cid:62)X.

4

LAVADE

3. Project X onto the hyperplane spanned by the (cid:96) largest eigenvectors of the covariance matrix S. The symmetry

of the covariance matrix S implies that all of its eigenvectors are orthogonal.

The complete projection step of PCA can be written as

T = XW

(16)

where T ∈ Rm×n is the PCA score matrix, X ∈ Rm×n is the data matrix, and W ∈ Rn×n is a coefﬁcient matrix. The
columns of W are called loadings and correspond to the eigenvectors of X(cid:62)X in descending order.

Equation 16 describes a linear transformation. The loadings form an orthogonal basis, which results in the columns of
T being decorrelated (Bengio et al., 2013). The ﬁrst (cid:96) < n components of T form the (cid:96)-dimensional representation of
X preserving most variance and are denoted by T(cid:96).

Applied data analytics problems nearly always have (cid:96) < m, in which case OLS can be used for regressing the lower
dimensional representation T versus y in an additional step. The combination of these two methods is known as
Principal Component Regression (PCR). Combining (7) and (16) leads to

β(cid:96) = (T(cid:96)

(cid:62)T(cid:96))−1T(cid:96)

(cid:62)y

(17)

where (cid:96) is the number of principal components. The regression coefﬁcients γ(cid:96) correspond to the lower dimensional
space based on (cid:96) principal components. Consequently, the regression coefﬁcients in the original space are constructed
by

β = W(cid:96)β(cid:96)

(18)

where W(cid:96) denotes the ﬁrst (cid:96) columns of the matrix W.

2.6 PLS

Partial Least Squares (PLS) aims to ﬁnd lower dimensional representations of X and Y and is not restricted to scalar
objectives. While PCA performs dimensionality reduction in an unsupervised way, PLS incorporates information about
the target Y in the dimensionality reduction scheme. In general, the governing equations for PLS are

X = TP(cid:62) + E
Y = UQ(cid:62) + F

(19)

(20)

where the matrix X ∈ Rm×n is the data matrix, and Y ∈ Rm×p is the matrix of responses, T ∈ Rm×l is the score
matrix, P ∈ Rn×l is the loading matrix, and E ∈ Rm×n is the residual matrix corresponding to X. Similarly for Y,
the matrix U ∈ Rm×l is its score matrix, Q ∈ Rp×l is its loading matrix, and F ∈ Rm×p is its residual matrix.

In data analytics applications, the case p = 1 occurs frequently and is known as PLS1. The next paragraph introduces
PLS1 with y being an single column vector. Readers interested in more details on other variants of PLS are directed to
other references (Chiang et al., 2000; Wold et al., 2001; MacGregor et al., 2005; Boulesteix and Strimmer, 2006).

Similarly to PCA, PLS performs a linear transformation on the input X, with

T = XW.

(21)

From an optimization perspective, PLS maximizes the sample covariance between the X scores and the responses
(Wold et al., 2001; Boulesteix and Strimmer, 2006). The calculation of the ﬁrst component can be written in the form of
the unconstrained optimization

w1 = arg max

W

w(cid:62)X(cid:62)yy(cid:62)Xw
w(cid:62)w

.

(22)

The regression part of PLS is identical to (17), but using T(cid:96) estimated via the PLS algorithm.

3 Demonstrator Design

The Latent Variable Demonstrator (LAVADE) software is designed to evaluate and compare the regression techniques
described in Section 2 on datasets of different complexity. The Graphical User Interface (GUI) of the software is a
single window, with drop-down menus, sliders, buttons, and axis objects (Fig. 1). Depending on the selection of the
dataset and the regression technique, the relevant elements on the window become visible or invisible, reducing user
distraction.

5

2

LAVADE

1

3

6

4

5

Figure 1: Layout of Latent Variable Demonstrator (LAVADE)

The six elements in the GUI are

1. Dataset and Method selection drop-down menu, and check box, to select whether inputs shall be standardized3
2. Hyperparameters for the respective regression method

3. Modiﬁcation of data: Data generation parameters (“Example case”), addition of white Gaussian noise

4. Visualization of the data, training, and test data, color-coded

5. Visualization of regression coefﬁcients

6. Regression results on the training and test data, respectively

On the backend, the software consists of two MATLAB classes. The “lavade” class inherits from the “AppBase” class,
a generic MATLAB app class and contains all the elements of the GUI and general variables associated with the
functionality of the software. The “data” class is a superclass from which all the individual data set classes inherit. The
constructor of the individual dataset classes loads or generates the data. Furthermore, the data class contains methods to
perturb the signal by noise, and to split the data. This structure helps to implement new datasets quickly.

4 Datasets

The LAVADE software includes an illustrative numerical example data generator and three real-world datasets. The
datasets can also be downloaded separately from the corresponding GitHub repository.

Example The motivation of the example dataset is to generate a dataset similar in character to process measurements,
in which only a subset of the measurements have predictive power. The dataset is constructed by starting by deﬁning
four data points: start, ﬁnal, and two intermediate data points (Fig. 2). The four data points are drawn from a Gaussian
distribution with deﬁned means and standard deviations. The output responses y are the (average) slopes in the relevant
section.

Figure 2 shows the default case in which the parameters of the four Gaussian distributions are µstart = 2, µleft = −1,
µright = −4, µend = −5, and σstart = σend = 0, σleft = σright = 2. The data are within three sections. The slope in the
relevant section can also be inferred from data outside of the relevant section due to σstart = σend = 0.

3Standardization refers to centering the columns and scaling to have unit variance.

6

MethodENSNR: X10203040506070Train-Test Split Ratio0.10.30.50.70.9#Datapoints50#rel. Datapoints10#Experiments30Start  μ2SNR: y10203040506070σ2σ2Draw New SampleLatent Variable Method DemonstratorAdd NoiseRegularization0.001Shuﬄe Splitsσ0σ0Standardize Data HoldEN α0.050.250.50.751DatasetExampleEnd  μ-5Left  μ-1Right  μ-4Position of Signal00.10.20.30.40.50.60.70.80.91LAVADE

Start

Relevant
Section

Left

Right

End

Figure 2: Default case: σstart = σend = 0, σleft = σright = 2. “Relevant section” refers to the middle section among the
three sections.

Figure 3: Default standardized case: σstart = σend = 0, σleft = σright = 2, standardized columns.

This dataset illustrates the common situation in which the data are highly correlated while not having any datapoints
repeated, while being simple enough to easily understand the dataset and the model being constructed, and be able to
interpret the results.

Figure 3 shows the result of standardizing the data in Fig. 2. For each data vector, all datapoints left of the relevant
section have the same value and carry the same information, except for the ﬁrst and the last datapoints. The same holds
true for the datapoints right of the relevant section.

7

5101520253035404550Index-8-6-4-20246ValueExampleRank X: 3TrainingTest5101520253035404550Index-2-10123ValueExampleRank X: 3TrainingTestLAVADE

Figure 4: Noisy case: σstart = σend = 2, σleft = σright = 2, SNR = 20 for all sections.

A noisy dataset adds white Gaussian noise with deﬁned Signal-to-Noise Ratio (SNR) to the matrix X and vector of
responses y (Fig. 4). By varying the Signal-to-Noise Ratio (SNR), the user can assess how noise affects the model
training and model predictions. This example dataset encourages the user to develop a general understanding of model
types, hyperparameters, noise, training, and prediction accuracy on a simple dataset inspired by common process data
analytics problems.

ATR-FTIR Spectra This dataset consists of Attenuated Total Reﬂectance-Fourier Transform Infrared (ATR-FTIR)
spectra that were recorded and analyzed in Fujiwara et al. (2002). The model objective is to predict the paracetamol
concentration C associated with each spectrum. The dataset consists of spectra collected for six values of the
concentration (six “groups”), with varying temperature. For simplicity, we omit the temperature readings and use only
the ATR-FTIR spectra as model input.

Figure 5: Visualization of ATR-FTIR spectra for paracetamol in aqueous solution.

8

5101520253035404550Index-3-2-10123ValueExampleRank X: 30TrainingTest1200130014001500160017001800Wavenumber [cm-1]00.050.10.15AbsorbanceATR-FTIR SpectraRank X: 180TrainingTestLAVADE

Each spectrum is highly correlated with the other spectra, and the absorbances within each spectrum are highly
correlated. Each group of spectra is evenly spaced from adjacent groups of spectra except for the group of spectra
at C = 0.014 g/g, which is the second group from the bottom in Fig. 5. Because the rest of the data appears to have
roughly a linear relationship, it is expected that much of the C = 0.014 g/g data are biased, most likely by some
experimental error. As such, the data at C = 0.014 g/g are placed in the test dataset as the default. This data assignment
leads to more conservative test errors and prevents the model from learning an aphysical correlation due to this bias.
The strategy of splitting of this dataset can be chosen in the GUI to be either fully at random or such that groups of
spectra roughly corresponding to the same concentration are always part of only the training or the test dataset. This
latter respects the fact that, for spectral data, the variation of data between groups is typically larger than the variation
of data within a group (Sun and Braatz, 2021). Given that error structure, placing the data from all groups into the
training dataset, for example, would result in overﬁtting and overly conﬁdent/optimistic prediction errors for the test
data (Escobar et al., 2014). Usually, data are standardized before applying ridge regression, lasso, and elastic net, to
account for widely different ranges of values for each data type (e.g., a temperature could be more than 100 degrees for
a system in which absorbance spans 0.01 absorbance units). However, this particular dataset consists only of spectra,
and all readings have the same order of magnitude. In this case, skipping standardization can result in better models.
Typically many absorbances are not associated with the molecule of interest and contain nearly all noise; standardization
normalizes those absorbances to have the same magnitude of importance in the matrix X in the optimization objective.
Thereby standardization feeds noise into the parameter estimation and degrades the quality of the model. Also, the
potential beneﬁts of standardization are lower for spectral data, as most absorbance values are already of a similar order
of magnitude.

Raman Spectra Raman scattering refers to the inelastic scattering of photons by matter. Raman spectra have high
molecular speciﬁcity and are used to build models to predict molecular concentrations in solutions (Das and Agrawal,
2011; Pandey et al., 2017). A promising use case for this type of model is noninvasive glucose monitoring (Mehdizadeh
et al., 2015), which uses Raman spectra from different bioreactor runs to learn mappings to glucose concentrations from
Nova FlexII cell culture measurements. This task is challenging due to noise and background signals in the Raman
spectra and noise in the Nova FlexII readings. Furthermore, the biochemical reactions taking place in the bioreactor
lead to correlations in the concentration changes of various metabolites. We removed outliers from the cell culture
measurements of Bozinovski et al. (2022) by applying a moving median ﬁlter having a window size of three days and
removing all values that were above three standard deviations. The Raman spectra are available in the software (1)
without background removal, (2) background removal by the method of Lieber and Mahadevan-Jansen (2003) using the
Biodata MATLAB Toolbox implementation (De Gussem et al., 2009), and (3) background removal with the MATLAB
algorithm msbackadj. The range of Raman shifts was restricted to the range 400–1800 cm−1 as suggested by Whelan
et al. (2012) and Rowland-Jones and Jaques (2019).

Figure 6: Visualization of Raman spectra without background removal from a bioreactor.

9

4006008001000120014001600Raman Shift (cm-1)00.511.52Counts#106Raman SpectraRank X: 210TrainingTestLAVADE

Figure 6 shows 327 Raman spectra, each corresponding to a cell culture measurement. The cell culture measurements
were collected at different times than the Raman spectra. The relatively slow concentration changes enabled the
concentrations corresponding to the Raman spectra to be linearly interpolated from the cell culture measurements.

Lithium-Iron-Phosphate Batteries The Lithium-Iron-Phosphate (LFP) battery dataset contains cycling data for 124
batteries that have underwent widely varying charging protocols and discharged at a uniform current (Severson et al.,
2019). The model objective is the cycle life, i.e., the number of cycles until the battery’s capacity drops below 80% of
its nominal capacity.

Severson et al. (2019) showed that features based on the difference between the discharge capacity of voltage curves
for two cycles, subsequently called ∆Qa−b, linearly correlate well with the logarithm of the cycle life. The cycle
pair a = 100 and b = 10 was used in Severson et al. (2019). While other cycle combinations also work well, the
implemented input data X is ∆Q100−10 for this demonstrator. More information about the dataset and reasoning about
the modeling objective can be found in Severson et al. (2019).

Figure 7: Visualization of the LFP dataset in the LAVADE software.

Some charging protocols were applied to only on a single cell, whereas others were used on up to nine different cells,
leading to a grouping structure. In the LAVADE software, the train-test split can be chose to be fully at random or
randomly assigning groups such that cells with the same charging protocol are only part of either the training or the
test dataset. The cycle life distribution of the dataset is non-Gaussian, with a heavy concentration of cells having a
relatively low cycle life and only a few having longer cycle life. Depending on the split, models will tend to struggle to
extrapolate to the longer-lived cells, leading to high variability in prediction errors calculated for the test set.

Furthermore, the batteries were cycled in three batches with different starting dates. Thus the cells were stored over
different durations between manufacturing and cycling. The ﬁrst batch was cycled approximately a year earlier than the
last batch leading to prolonged calendar aging for some cells. This additional information is crucial when building
robust data analytics models but is not elaborated on further in this context.

5

Illustrative Examples

This section illustrates the LAVADE software by applying the various regression methods to the datasets. While small
differences can arise when reproducing the results, due to the randomized splitting of the datasets, the observations made
in this section are general. The regularization parameters in the software must be chosen by hand. For the following
examples, we chose the hyperparameters to showcase interesting regression coefﬁcients and prediction results.

10

22.533.5Voltage (V)-0.15-0.1-0.050"Q100-10LFPRank X: 124TrainingTestLAVADE

5.1 Example Dataset

(a) Noise-free example dataset

(b) PLS and RR regression coefﬁcients

(c) lasso regression coefﬁcients

Figure 8: Noise-free example dataset and regression coefﬁcients for PLS, RR, and lasso, with hyperparameters and
RMSEs reported in Table 1.

Regression Method

PLS (2 components)
RR (λ = 1×10−6)
lasso (λ = 0.015)

Regression
Coefﬁcients
Fig. 8b
Fig. 8b
Fig. 8c

RMSE Training RMSE Test

1.4×10−16
4.6×10−10
0.011

7.3×10−17
4.6×10−10
0.011

Table 1: Regression methods, hyperparameters, regression coefﬁcients, and RMSE for the training and test datasets for
the noise-free example.

The regression results for the noise-free example dataset illustrate several important properties of the implemented
methods. PLS and RR result in nearly identical regression coefﬁcients in this ideal dataset (Fig. 8b). Due to the
mathematical similarities between PCR, PLS, and RR (de Jong and Farebrother, 1994; Sundberg, 1993; Stone and
Brooks, 1990) regression coefﬁcients are expected to have certain similarities given appropriate parameters. Lasso leads
to sparse regression coefﬁcients that are intuitive; an observant human would also recognize that all of the data points in
Fig. 8a can be determined from the difference in values between the 30th and 21st measurements.4 Table 1 shows that
the three methods investigated here lead to regression coefﬁcients that result in accurate model predictions, with PLS
and lasso being the most and least accurate, respectively, given the chosen regularization parameters. The regression

4Or, equivalently, by the slope of the line in the middle region computed from the maximum and minimum values.

11

5101520253035404550Index-8-6-4-20246ValueExampleRank X: 3TrainingTest5101520253035404550Index-0.015-0.01-0.00500.0050.010.015-Regression Coefficients5101520253035404550Index-0.1-0.0500.050.1-Regression CoefficientsLAVADE

coefﬁcients, however, have very different shapes (cf. Fig. 8bc). Because the methods result in very similar predictions,
the differences between the regression coefﬁcients must be within or very close to the nullspace of the data, N (X).

5.2 FTIR Spectral Data

(a) Interpolation data split

(b) Extrapolation data split
Figure 9: Three ways of splitting the ATR-FTIR spectral dataset (top) and the corresponding regression coefﬁcients
obtained by PLS (bottom), with number of components and RMSEs reported in Table 2.

(c) Data split ignoring grouping

Regression
Coefﬁcients
Fig. 9a
Fig. 9b
Fig. 9c

Number of
Components
7
7
8

RMSE Train RMSE Test

1.4×10−5
1.4×10−5
1.5×10−5

3.5×10−5
8×10−4
1.7×10−5

Table 2: PLS regression coefﬁcients, numbers of components, and RMSE for the training and test ATR-FTIR spectral
datasets.

The results of PLS regression for the ATR-FTIR spectral dataset shown in Fig. 9 and Table 2 highlight implications
of different ways of splitting data into training and test datasets in the presence of groups. In Fig. 9ab, groups of
spectra belonging to the same concentration are randomly assigned to either the training or test dataset. In all cases, the
root-mean-squared errors for the test data are higher than for the training data, due to the biased spectra corresponding
to C = 0.014 g/g that was chosen to be in the test dataset for all three splits. Fig. 9b shows a split where the test
dataset contains groups with concentrations outside the training data. The root-mean-squared errors of prediction of the
training and test datasets differ by one order of magnitude and show clearly that the model fails to extrapolate to the
concentrations in the test dataset. It is recommended to assign the groups with spectra corresponding to the lowest and
highest concentration as part of the training dataset, as is the case in Fig. 9a, so that the model interpolates rather than
extrapolates.

Fig. 9c shows prediction results with randomized splitting of the training and test datasets. This violation of the grouping
structure when splitting the dataset leads to what appears to be more accurate predictions for the test dataset (Table
2). It is important to take into account that the prediction error for the test dataset obtained in case c approximates the
expected generalization error only for new data that are within the groups that are already part of the training data.
The reason for this restriction is that the between-group variance is higher than the within-group variance. Such an
estimated prediction error will under-predict the prediction error when the model is applied to data that are not within
the groups in the original dataset. In practice, the model is intended for use at any paracetamol concentration within the

12

1200130014001500160017001800Wavenumber [cm-1]00.050.10.15AbsorbanceParacetamolRank X: 180TrainingTest1200130014001500160017001800Wavenumber [cm-1]-0.04-0.0200.020.040.060.08-Regression Coefficients1200130014001500160017001800Wavenumber [cm-1]00.050.10.15AbsorbanceParacetamolRank X: 180TrainingTest1200130014001500160017001800Wavenumber [cm-1]-0.04-0.0200.020.040.060.08-Regression Coefficients1200130014001500160017001800Wavenumber [cm-1]00.050.10.15AbsorbanceParacetamolRank X: 180TrainingTest1200130014001500160017001800Wavenumber [cm-1]-0.04-0.0200.020.040.060.08-Regression CoefficientsLAVADE

range of concentration that was in the training data, and the groupings constraint must be respected to obtain a model
accuracy from the test set that is a reasonably accurate approximation of the generalization error.

5.3 Raman Spectral Data

(a)

(b)

(c)

(d)

Figure 10: (a) Raman spectral dataset without background subtraction with the training data (bioreactor run 1) in blue
and the test data (bioreactor run 2) in red, (b) the regression coefﬁcients for a PLS model with 7 components, (c,d) the
regression results for the training and test data.

Figure 10 shows the results for a PLS model trained on data from bioreactor run 1, with the test set being data from
bioreactor run 2 (Fig. 10d). The regression coefﬁcients show that the Raman shifts in the region around 1000–1200
cm−1 are most important for predicting glucose concentration. The model predicts the glucose concentration in the test
dataset fairly well, however, the model accuracy is signiﬁcantly higher on the training dataset than on the test dataset.

Figure 10 shows the results for a PLS model trained on data from bioreactor run 2, with the test set being data from
bioreactor run 1 (Fig. 11d). Most of the glucose concentrations predicted by the model are biased and the prediction
errors are much larger than estimated by the model that was trained on bioreactor run 1 and predicted on bioreactor
run 2 (cf. Fig. 10d). However, the regression coefﬁcients determined from bioreactor run 2 are very similar to the
regression coefﬁcients determined from bioreactor run 1. Ensuring a strict split of the data, in which data from each
bioreactor are only in the training or test datasets, is advised to obtain a more realistic generalization error of the model.
As seen in the ATR-FTIR spectral dataset example, grouping should be respected when building such models, due to
the between-group variation being higher than the within-group variation. The large test errors in Fig. 11 illustrate the
challenges that occur when trying to build models that will predict glucose concentration well for new bioreactor runs.
The intrinsically very low photon efﬁciency of Raman spectroscopy and batch-to-batch and time-varying concentrations
of other species makes it challenging to build highly accurate predictive models of glucose in bioreactors.

13

4006008001000120014001600Raman Shift (cm-1)00.511.52Counts#106Raman SpectraRank X: 210TrainingTest4006008001000120014001600Raman Shift (cm-1)-2-101234-#10-6Regression Coefficients02468ytrue (g/L)02468ypred (g/L)Glucose Concentration TrainingRMSE: 0.2102468ytrue (g/L)02468ypred (g/L)Glucose Concentration TestRMSE: 0.88LAVADE

(a)

(b)

(c)

(d)

Figure 11: (a) Raman spectral dataset without background subtraction with the training data (bioreactor run 2) in blue
and the test data (bioreactor run 1) in red, (b) the regression coefﬁcients for a PLS model with 6 components, (c,d) the
regression results for the training and test data.

Figure 12 shows the results of building a PLS model for Raman spectra from multiple bioreactor runs. The data are
split randomly, leading to data from all of the bioreactors being present in both the training and test datasets. The
model accurately predicts the glucose concentration in the test dataset. The number of PLS components is signiﬁcantly
higher than in the cases of Fig. 10 and 11, but overall the regression coefﬁcients are similar. However the regression
coefﬁcients in Fig. 12) have a higher level of noise, suggesting that the model overﬁts to speciﬁc conditions being
present in the respective bioreactors, i.e., that too many components were used. The split leads to what appears to
be more accurate predictions for the test dataset similar to the ATR-FTIR spectral dataset. It is important to take into
account that the prediction error for the test dataset obtained approximates the expected generalization error only for
new data that are within the reactor runs that are already part of the training data. In addition, the random split also
does not respect the time series structure of the dataset and is thus not a fair estimate of the model accuracy for reactor
conditions further in the future (i.e., not in between conditions present in the training dataset as it is the case here for
most data in the test dataset).

14

4006008001000120014001600Raman Shift (cm-1)00.511.52Counts#106Raman SpectraRank X: 210TrainingTest4006008001000120014001600Raman Shift (cm-1)-3-2-101234-#10-6Regression Coefficients02468ytrue (g/L)02468ypred (g/L)Glucose Concentration TrainingRMSE: 0.2402468ytrue (g/L)02468ypred (g/L)Glucose Concentration TestRMSE: 2.3LAVADE

(a)

(b)

(c)

(d)

Figure 12: (a) Raman spectral dataset without background subtraction with the training data in blue and the test data
in red using a random split, (b) the regression coefﬁcients for a PLS model with 12 components, (c,d) the regression
results for the training and test data.

5.4 LFP Battery Dataset

The distribution of cycle life of batteries in the LFP dataset is non-Gaussian, with a high amount of cells with medium
cycle life, three cells with a signiﬁcantly shorter cycle life, and a few cells with signiﬁcantly longer cycle life. This
distribution results in the root-mean-squared-errors varying depending on precisely which data are in the training and
test datasets, even with well-tuned hyperparamaters. For the split in Fig. 13, the RMSE for the test data is lower than for
the training data, which is because the test dataset contains only medium- to short-lived cells that are well captured by
the model. The RMSE for the training dataset for medium- to short-lived cells in Fig. 13 is similar to the RMSE for the
test dataset. The RMSE for the training data is strongly inﬂuenced by the higher RMSE of the longest lived cells. This
example illustrates the signiﬁcant effect that a small number of data points at extreme values of the data distribution,
although reasonably consistent with the rest of the data, can have on the estimated prediction errors.

Figure 14 shows the regression coefﬁcients for models that were trained on ten training-test splits of the dataset. The
resulting coefﬁcients vary signiﬁcantly, particularly in the region around 3 volts marked by a red box. This high
variability suggests that PLS ﬁnds it challenging to extract the information contained in this voltage region. Reasons for
this behaviour could be nonlinear effects or inherent biases due to battery physics that manifest in this voltage region.
For more insights, we refer to the supplementary information of Attia et al. (2020) that contains more details about the
measuring campaign and corresponding differences in calendar aging due to different storage times of batteries before
the start of the measurements.

15

4006008001000120014001600Raman Shift (cm-1)00.511.52Counts#106Raman SpectraRank X: 210TrainingTest4006008001000120014001600Raman Shift (cm-1)-6-4-202468-#10-6Regression Coefficients246ytrue (g/L)123456ypred (g/L)Glucose Concentration TrainingRMSE: 0.13246ytrue (g/L)123456ypred (g/L)Glucose Concentration TestRMSE: 0.14LAVADE

(a)

(b)

Figure 13: (a) LFP dataset with the training data in blue and the test data in red, (b) the regression coefﬁcients for a PLS
model with four components, (c,d) the regression results for the training and test data.

(c)

(d)

Figure 14: Varying regression coefﬁcients for different train-test splits.

16

22.533.5Voltage (V)-0.15-0.1-0.050"Q100-10LFPRank X: 124TrainingTest22.533.5Voltage (V)-0.6-0.4-0.200.20.40.6-Regression Coefficients500100015002000ytrue (cycles)500100015002000ypred (cycles)Cycle Life TrainingRMSE: 1.6e+02500100015002000ytrue (cycles)500100015002000ypred (cycles)Cycle Life TestRMSE: 1e+0222.533.5Voltage (V)-0.3-0.2-0.100.10.20.3-Regression CoefficientsLAVADE

6 Conclusion

The LAVADE software enables users to easily compare different latent variable regression and alternative regression
methods when applied to four datasets of different complexity ranging from a synthetic dataset to real-world datasets.
The visualizations of the data, train-test splits, regression coefﬁcients, and predictions are useful to understanding in
these regression methods. These visualizations reveal important details about the data and implications of the model
building strategy. The interactive environment of the GUI fosters a deeper understanding of how models interact with
data. Comparing regularization parameters of lasso, RR, EN, and the number of components for PLS and PCR helps to
understand how the different approaches of regularization affect the resulting regression coefﬁcients.

The open-source LAVADE software intends to be a starting point to gain intuition on latent variable and other
multivariate regression methods. Ultimately we hope to inspire teachers to use this tool to advance their teaching
methods and encourage extensions of the software and examples.

Code and Data Availability

The code is available at https://github.com/JoachimSchaeffer/LAVADE. The repository contains the datasets that are
described in this article. The paracetamol dataset was recorded and an analysis of the dataset was published with Fujiwara et al.
(2002). The repository contains only a small subset of the LFP dataset that was published with Severson et al. (2019). The entire
LFP dataset is available at https://data.matr.io/1/.

Acknowledgments

Financial support is acknowledged from the German Academic Exchange Service (DAAD) for Joachim Schaeffer and from the U.S.
Food and Drug Administration, Grant #U01FD006483, for Richard Braatz. Eric Lenz from the Control and Cyber-Physical Systems
Laboratory at TU Darmstadt is thanked for providing valuable feedback and reviewing the paper before submission. Alexis Dubs
and Nili Persits at MIT are thanked for providing information on the Raman spectral data and the associated experimental data.

References

L. H. Chiang, E. L. Russell, and R. D. Braatz. Fault Detection and Diagnosis in Industrial Systems. Springer Verlag, London, UK,

2000. doi:10.1007/978-1-4471-0347-9.

K. Mardia. Multivariate Analysis. Academic Press, London, UK, 2003.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological),

58(1):267–288, 1996. ISSN 0035-9246. doi:10.1111/j.2517-6161.1996.tb02080.x.

A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67,

1970. ISSN 15372723. doi:10.1080/00401706.1970.10488634.

H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B:

Statistical Methodology, 67(2):301–320, 2005. ISSN 13697412. doi:10.1111/j.1467-9868.2005.00503.x.

G. Strang. Introduction to Linear Algebra. Cambridge Press, Wellesley, Massachusetts, ﬁfth edition, 2016. ISBN 9780980232776.

A. Monticelli. State Estimation in Electric Power Systems: A Generalized Approach. Springer, Boston, 1999.

G. James, D. Witten, T. Hastie, and R. Tibshirani. Linear Model Selection and Regularization. Springer, New York, 2021. ISBN

978-1-0716-1417-4. doi:10.1007/978-1-0716-1418-1.

Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 35(8):1798–1828, 2013. doi:10.1109/TPAMI.2013.50.

S. Wold, M. Sjöström, and L. Eriksson. PLS-regression: A basic tool of chemometrics. Chemometrics and Intelligent Laboratory

Systems, 58(2):109–130, 2001. ISSN 0169-7439. doi:10.1016/S0169-7439(01)00155-1.

J. F. MacGregor, H. Yu, S. García Muñoz, and J. Flores-Cerrillo. Data-based latent variable methods for process
ISSN 0098-1354.

analysis, monitoring and control. Computers & Chemical Engineering, 29(6):1217–1223, 2005.
doi:10.1016/j.compchemeng.2005.02.007.

A.-L. Boulesteix and K. Strimmer. Partial least squares: A versatile tool for the analysis of high-dimensional genomic data. Brieﬁngs

in Bioinformatics, 8(1):32–44, 05 2006. ISSN 1467-5463. doi:10.1093/bib/bbl016.

M. Fujiwara, P. S. Chow, D. L. Ma, and R. D. Braatz. Paracetamol crystallization using laser backscattering and ATR-FTIR
spectroscopy: Metastability, agglomeration, and control. Crystal Growth & Design, 2(5):363–370, 2002. doi:10.1021/cg0200098.

W. Sun and R. D. Braatz. Smart process analytics for predictive modeling. Computers & Chemical Engineering, 144:107134, 2021.

doi:10.1016/j.compchemeng.2020.107134.

17

LAVADE

M. S. Escobar, H. Kaneko, and K. Funatsu. Flour concentration prediction using GAPLS and GAWLS focused on data sampling
ISSN 0169-7439.

issues and applicability domain. Chemometrics and Intelligent Laboratory Systems, 137:33–46, 2014.
doi:10.1016/j.chemolab.2014.06.005.

R. S. Das and Y. Agrawal. Raman spectroscopy: Recent advancements, techniques and applications. Vibrational Spectroscopy, 57(2):
163–176, 2011. ISSN 0924-2031. doi:10.1016/j.vibspec.2011.08.003. URL https://www.sciencedirect.com/science/
article/pii/S0924203111001111.

R. Pandey, S. K. Paidi, T. A. Valdez, C. Zhang, N. Spegazzini, R. R. Dasari, and I. Barman. Noninvasive monitoring
ISSN 0001-4842.

of blood glucose with raman spectroscopy. Accounts of Chemical Research, 50(2):264–272, 2017.
doi:10.1021/acs.accounts.6b00472.

H. Mehdizadeh, D. Lauri, K. M. Karry, M. Moshgbar, R. Procopio-Melino, and D. Drapeau. Generic raman-based calibration models
enabling real-time monitoring of cell culture bioreactors. Biotechnology Progress, 31(4):1004–1013, 2015. doi:10.1002/btpr.2079.
URL https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/btpr.2079.

D. Bozinovski, E. Cummings Bende, A. Maloney, J. Sangerman, A. Dubs, A. Lu, M. S. Hong, N. Persits, A. Artamonova, R. W.
Ou, W. Sun, J. Wolfrum, P. Barone, S. Springs, R. Braatz, and A. Sinskey. Biomanufacturing and testbed development for the
continuous production of monoclonal antibodies. In ACS Spring Meeting, San Diego, California, 2022. Abstract 3661909,
associated journal manuscript in preparation.

C. A. Lieber and A. Mahadevan-Jansen. Automated method for subtraction of ﬂuorescence from biological raman spectra. Applied
Spectroscopy, 57(11):1363–1367, Nov 2003. URL http://opg.optica.org/as/abstract.cfm?URI=as-57-11-1363.

K. De Gussem, J. De Gelder, P. Vandenabeele, and L. Moens. The biodata toolbox for matlab. Chemometrics and In-
ISSN 0169-7439. doi:10.1016/j.chemolab.2008.08.003. URL https:

telligent Laboratory Systems, 95(1):49–52, 2009.
//www.sciencedirect.com/science/article/pii/S0169743908001561.

J. Whelan, S. Craven, and B. Glennon. In situ raman spectroscopy for simultaneous monitoring of multiple process parameters
in mammalian cell culture bioreactors. Biotechnology Progress, 28(5):1355–1362, 2012. doi:10.1002/btpr.1590. URL https:
//aiche.onlinelibrary.wiley.com/doi/abs/10.1002/btpr.1590.

R. C. Rowland-Jones and C. Jaques. At-line raman spectroscopy and design of experiments for robust monitoring and control
of miniature bioreactor cultures. Biotechnology Progress, 35(2):e2740, 2019. doi:https://doi.org/10.1002/btpr.2740. URL
https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/btpr.2740.

K. A. Severson, P. M. Attia, N. Jin, N. Perkins, B. Jiang, Z. Yang, M. H. Chen, M. Aykol, P. K. Herring, D. Fraggedakis, M. Z.
Bazant, S. J. Harris, W. C. Chueh, and R. D. Braatz. Data-driven prediction of battery cycle life before capacity degradation.
Nature Energy, 4(5):383–391, 2019. doi:10.1038/s41560-019-0356-8.

S. de Jong and R. W. Farebrother. Extending the relationship between ridge regression and continuum regression. Chemometrics and

Intelligent Laboratory Systems, 25(2):179–181, 1994. doi:https://doi.org/10.1016/0169-7439(94)85041-0.

R. Sundberg. Continuum regression and ridge regression. Journal of the Royal Statistical Society: Series B (Methodological), 55(3):

653–659, 1993. doi:10.1111/j.2517-6161.1993.tb01930.x.

M. Stone and R. J. Brooks. Continuum regression: Cross-validated sequentially constructed prediction embracing ordinary
least squares, partial least squares and principal components regression. Journal of the Royal Statistical Society: Series B
(Methodological), 52(2):237–258, 1990. doi:10.1111/j.2517-6161.1990.tb01786.x.

P. M. Attia, A. Grover, N. Jin, K. A. Severson, T. M. Markov, Y.-H. Liao, M. H. Chen, B. Cheong, N. Perkins, Z. Yang, P. K. Herring,
M. Aykol, S. J. Harris, R. D. Braatz, S. Ermon, and W. C. Chueh. Closed-loop optimization of fast-charging protocols for batteries
with machine learning. Nature, 578(7795):397–402, 2020. doi:10.1038/s41586-020-1994-5.

18

