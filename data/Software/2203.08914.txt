2
2
0
2

l
u
J

1
2

]

V

I
.
s
s
e
e
[

2
v
4
1
9
8
0
.
3
0
2
2
:
v
i
X
r
a

KNEE ARTHRITIS SEVERITY MEASUREMENT USING DEEP
LEARNING: A PUBLICLY AVAILABLE ALGORITHM WITH A
MULTI-INSTITUTIONAL VALIDATION SHOWING
RADIOLOGIST-LEVEL PERFORMANCE

A PREPRINT

Hanxue Gu

Keyu Li

Roy J. Colglazier

Jichen Yang Michael Lebhar

Jonathan O’Donnell

William A. Jiranek Richard C. Mather Rob J. French Nicholas Said

Jikai Zhang Christine Park

Maciej A. Mazurowski
∗†‡§¶(cid:107)

July 22, 2022

ABSTRACT

The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a central criteria for the
use of total knee arthroplasty. However, this assessment suffers from imprecise standards and a
remarkably high inter-reader variability. An algorithmic, automated assessment of KOA severity
could improve overall outcomes of knee replacement procedures by increasing the appropriateness of
its use. We propose a novel deep learning-based ﬁve-step algorithm to automatically grade KOA from
posterior-anterior (PA) views of radiographs: (1) image preprocessing (2) localization of knees joints
in the image using the YOLO v3-Tiny model, (3) initial assessment of the severity of osteoarthritis
using a convolutional neural network-based classiﬁer, (4) segmentation of the joints and calculation
of the joint space narrowing (JSN), and (5), a combination of the JSN and the initial assessment to
determine a ﬁnal Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation masks
used to make the assessment, our algorithm demonstrates a higher degree of transparency compared
to typical “black box” deep learning classiﬁers. We perform a comprehensive evaluation using two
public datasets and one dataset from our institution, and show that our algorithm reaches state-of-the
art performance. Moreover, we also collected ratings from multiple radiologists at our institution and
showed that our algorithm performs at the radiologist level.
The software has been made publicly available at https://github.com/MaciejMazurowski/osteoarthritis-
classiﬁcation.

∗Michael Lebhar is with the Duke University of Medicine.
†Roy J. Colglazier, Nicholas Said, Rob J. French are with the Department of Radiology, Radiology, Musculoskeletal Imaging,

Radiology.

‡Jonathan O’Donnell is with the Practice Transformation Unit in the Department of Orthopaedic Surgery at Duke University.
§William A. Jiranek is with the Orthopaedic Surgery, Orthopaedics, Orthopaedics at Duke University.
¶Richard C. Mather is with the Orthopaedic Surgery, Orthopaedics, Orthopaedics; Population Health Sciences, Population Health
Sciences, Basic Science Departments; Duke Clinical Research Institute, Duke Clinical Research Institute, Institutes and Centers;
Duke-Margolis Center for Health Policy, Duke - Margolis Center For Health Policy, Initiatives.

(cid:107)Maciej A. Mazurowski is with Radiology, Radiology, Clinical Science Departments; Biostatistics and Bioinformatics, Biostatis-
tics & Bioinformatics, Basic Science Departments; the Department of Electrical and Computer Engineering, Electrical and Computer
Engineering, Pratt School of Engineering; Computer Science, Computer Science, Trinity College of Arts & Sciences; Duke Cancer
Institute, Duke Cancer Institute, Institutes and Centers in Duke University, Durham 27708, United States.

 
 
 
 
 
 
arXiv Template

A PREPRINT

Keywords Supervised learning · Image classiﬁcation · Medical imaging · Knee Osteoarthritis.

1

Introduction

Osteoarthritis (OA) 1 is a common form of joint disorder, with knee OA being a leading cause of disability among older
adults. Knee osteoarthritis (KOA) typically manifests itself as the loss of articular cartilage from the medial and/or
lateral femoral condyles as well as the patellofemoral articular surfaces. This can be idiopathic, caused by mechanical
overload or post-traumatic changes. KOA can also manifest with the development of subchondral sclerosis, osteophytes,
and periarticular cyst formation Guccione et al. [1994], Vos et al. [2012], Helmick et al. [2008].

Knee joint replacement surgery is a safe and effective treatment to address functional deﬁcits, pain and deformities
5. However, signiﬁcant concerns of this procedure exist, and as many as 20% of patients remain dissatisﬁed after
knee replacement for multiple reasons, such as mental health issues and degenerative physical function Nak [2020].
According to The Agency for Healthcare Research and Quality, more than 790,000 knee replacements are performed
each year in the US 6. Based on a new report of medical claims data from 2010-2017 by the Blue Cross Blue
Shield Association (BCBSA), the average price for an inpatient’s knee replacement is $30,249, the number of knee
replacements is up by 17%, and the average cost increased by 6% during this period 7. Furthermore, due to an
aging population with a longer life expectancy Losina et al. [2012], there is a projected 143% increase in total knee
arthroplasty (TKA) by 2050 Inacio et al. [2017]. This combination of a signiﬁcant dissatisfaction rate, surgical risks,
increased utilization and high expenses have fueled an increased focus on determining when knee replacement surgery
is appropriate Porter [2010].

However, by using published appropriateness criteria, one study Riddle et al. [2015] found that about one-third of TKA
in the US was “inappropriate”, and only about half were “clearly appropriate”. The radiographic evaluation of KOA
severity plays an important role in several TKA appropriateness criteria that have been proposed 12, Riddle et al. [2014],
with knee replacement being indicative of more advanced diseases. Different criteria have been proposed for this
purpose, with Kellgren-Lawrence (KL) grade assessment emerging as an effective tool that incorporates a radiologist’s
measurement of joint space narrowing (JSN), osteophyte description and bony changes. The KL grade scales from
0 to 4, and is correlated with increasing severity of OA KELLGREN and LAWRENCE [1957]. KL assessment was
originally performed using anteroposterior (AP) knee radiographs, but recently demonstrated a greater sensitivity and
speciﬁcity on posteroanterior (PA) views Meyer et al. [2017].

Unfortunately, KL assessment is associated with signiﬁcant inter-reader variability, i.e., non-negotiable differences
of grades among readers, which diminishes its wide applicability Sun et al. [1997], Riddle et al. [2013], Klara et al.
[2016]. To improve the process of KOA assessment while also solving the aforementioned issues, the research of
computer-aided assessment methods has increased in recent years. In contrast to approaches based mainly on manual
visual feature extraction, deep learning (DL) has recently shown revolutionary success in imaging subspecialties, such
as object detection Ren et al. [2015], Cai et al. [2016] and face recognition Lin et al. [1997], Lawrence et al. [1997],
due to the automated learning of features directly from data. In addition, a growing number of attempts to apply DL
models to medical images have achieved decent results, indicating the possibility of using these powerful tools in
clinical practice in the near future. Indeed, multiple deep learning-based methods have recently been proposed for
OA assessment Tiulpin et al. [2018], Antony et al. [2017], Swiecicki et al. [2021], Kondal et al. [2020], Mengko et al.
[2005], Cheung et al. [2021], Selvaraju et al. [2016], Saleem et al. [2020]. However, most of them rely solely on models
that automatically provide a graded output from the radiographs, but this decision procedure is difﬁcult to interpret,
and often considered to be a “black box”. For example, the method of Selvaraju et al. [2016] adopts the Grad-CAM
method for knee radiographs, and shows prediction heatmaps from a multitask deep learning model, but it cannot
provide healthcare stakeholders with the precise information needed to help diagnostic procedures since it only shows
which features help the model’s decision, such as emphasizing joint regions, but not helping to analyze those features.
In clinical practice, transparency of decision procedures is crucial for practitioner use and validation Haibe-Kains et al.
[2020]. On the other hand, there are methods Mengko et al. [2005], Cheung et al. [2021], Saleem et al. [2020] that forgo
classiﬁcation directly in the original image for the sake of interpretability of OA evaluation, and instead select features
commonly used by radiologists as the basis for OA evaluation, such as the most commonly used JSN. These methods
are proven to be effective to some extent, but using selected features alone often leaves out other referable conditions in
the image, such as bony changes, which making the judgments less informative and the performance relatively worse.

In our study, we propose a deep learning-based algorithm for improved accuracy and consistency of KL score assessment,
while also improving transparency by providing a segmentation mask generated by the algorithm which is used for
determining the KL score as well as a joint space narrowing grading. By showing the segmentation results at the knee
joint, we can clearly display the morphological characteristics of the knee joints; at the same time, visualizing the
distance of the joint, we can more intuitively infer the degree of knee osteoarthritis. We evaluate our method with several

2

arXiv Template

A PREPRINT

Figure
(https://github.com/MaciejMazurowski/osteoarthritis-classiﬁcation).

KOA severity

prediction

pipeline.

1:

The

software

has

been

shared

publicly

datasets, including the publicly available MOST datasets MOS, the OAI dataset OAI, and a Duke institutional dataset
that gathered from Duke Health System. We further provide assessments of several radiologists on these datasets, in
order to validate our model. In summary, our contributions include:

1. We propose and implement a novel algorithm that combines classiﬁcation and segmentation in parallel to arrive
at a more accurate measurement of KL grade. Our algorithm also results in a higher degree of transparency,
as it provides a measurement of joint space narrowing and displays the segmentation mask that was used for
assessment.

2. We provide a thorough multi-institutional evaluation of our method with three datasets, including two publicly
available datasets and one collected at the Duke Health System. We demonstrate state-of-the art performance
on all datasets, and are the ﬁrst to our knowledge to provide this comprehensive of an evaluation.

3. We collect KL grade ratings from multiple radiologists on two of the datasets to evaluate the agreement between
our method and radiologists. We compare to multiple radiologists to mitigate the problem of inter-reader
variability, which is crucial to establish a clear baseline for our algorithm. We are the ﬁrst in the literature to
demonstrate this on these datasets for this task.

4. We share our software publicly as both Python code and executable software with a graphical user interface.
We anticipate that this will signiﬁcantly aid research on osteoarthritis assessment and surgery appropriateness
criteria, and make our algorithm more widely accessible.

2 METHODS

In this section, we ﬁrst detail the preparation of the datasets and the corresponding annotations needed for training and
validation, and then we introduce our KL grading classiﬁcation algorithm consisting of ﬁve main steps in Section 2.2.

2.1 Dataset

2.1.1 Dataset 1

We built our algorithm and conducted the ﬁrst phase of testing based on the dataset from the Multicenter Osteoarthritis
Study (MOST) MOS, which collected longitudinal knee radiological data and clinical assessments. The original dataset
consists of 10,052 exams from 3,036 patients, and it includes Kellgren-Lawrence grades, joint space narrowing (JSN)
scores and osteophyte grade assessments. Radiologists assign Kellgren-Lawrence grades according to the following
rules Mildenberger et al. [2002]:

1. Grade 0: no radiographic features of OA are present.
2. Grade 1: doubtful joint space narrowing (JSN) and possible osteophytic lipping.
3. Grade 2: deﬁnite osteophytes and possible JSN.
4. Grade 3: multiple osteophytes, deﬁnite JSN, sclerosis, possible bony deformity.

3

arXiv Template

A PREPRINT

Table 1: The prepared dataset 1.

Classiﬁcation dataset
Number of patients
Number of exams
Number of knees

Training
2040
7062
13404

Validation Test
503
259
1763
914
3359
1740

Table 2: The distribution of different KL grades for our dataset 1 (Table 1.

Set

Train
Valid
Test

KL grade

0
5600
678
1283

1
1951
263
526

2
2228
329
588

3
2475
316
697

4
1150
154
265

5. Grade 4: large osteophytes, marked JSN, severe sclerosis and deﬁnite bony deformity.

To create a subset from the dataset for the analyses, we used the following inclusion criteria:

1. If a knee at a visit was speciﬁcally marked as unﬁt in the dataset (for example, due to missing data), this was

not considered in our analysis for that visit.

2. We excluded images lacking either JSN or osteophyte grades.

3. We excluded all visits with missing Posterior-Anterior (PA) or lateral (LAT) views. For visits with multiple PA

views, we randomly select one view.

Following these exclusions, our dataset for analysis contained 9,739 exams from 2,802 patients. This translated to
18,053 knees with 9,739 PA images. We randomly split these 2,802 patients into non-overlapping training, validation,
and test subsets for basic classiﬁcation, shown in Table 1, it follows the same criteria of Swiecicki et al. [2021]. The
distribution of respective KL grades is presented in Table 2. In the next sections, we further describe how to annotate and
prepare the detection, segmentation, and classiﬁcation datasets needed for training and the test datasets for validation as
well as additional datasets annotated by our institutional radiologists.

1) Detection annotations. 300 PA view images (300 left and 300 right knees) were randomly selected from the training
set of dataset 1. These 300 images were then randomly divided into 210 images for training of the detection model,
45 images for the validation of the model, and 45 images for the testing of the detection model. Three experienced
researchers under our institution annotated the center point of each knee joint as the key-point for detection.

2) Segmentation annotations. 600 PA view images were selected from the classiﬁcation dataset to develop and evaluate
our segmentation algorithm. We randomly selected 400 images from the training set, and 72 images from the validation
set, of dataset 1. Researchers from our institution manually annotated the segmentation masks based on software OsiriX
osi, which included skeletal contours of the ﬁbula, tibia, and femur.

3) Kellgren-Lawrence grade annotations by radiologists from our institution. In addition to the KL score annotations
available in the MOST dataset, we randomly selected 110 PA view images from the test set of dataset 1 for additional
assessment of the KL grade by 5 radiologists at our institution. These images were graded by 4 board-certiﬁed Radiology
Fellows/Clinical Instructors and 1 attending Radiologist with 5 years post-fellowship experience in the Musculoskeletal
Division of the Duke Department of Radiology.

2.1.2 Dataset 2

To demonstrate the stability and generalization ability of our algorithm, we also utilized another publicly available
knee dataset, The Osteoarthritis Initiative (OAI) OAI. OAI is a longitudinal study of 4796 participants examined with
X-ray, MRI, and other metadata during nine follow-up examinations (0-96 months). Our Dataset 2 come from the OAI
released screening package 0.C.2. After ﬁltering out cases which lack JSN or KL grading, there are 2529 subjects
with 5028 knees remaining. To be noticed, MOST dataset and OAI dataset covered subjects aged 50-79 and 45-79,
respectively, which introduces different OA distributions across all subjects, and the different composition of data
sources also introduced various image acquisition artefacts.

4

arXiv Template

A PREPRINT

2.1.3 Dataset 3

For the Duke dataset, 200 cases are selected from the electronic medical record (EMR) database at Duke in 2019 and
the corresponding DICOM objects were retrieved by querying a large institutional picture archiving and communication
system (PACS). The 200 images were graded on an assessment of KL grade by 3 radiologists at our institution. It
is worth noting that these two datasets are only applied for the model performance testing stage and play no rule for
algorithm construction.

2.2 Algorithm

Our method has ﬁve steps:

1. Step 1: Image preprocessing to resize the pixel spacing and normalize the image intensity.

2. Step 2: Joint detection algorithm to localize the joint in the entire image and prepare a bounding box for future

cropping.

3. Step 3: Classiﬁcation algorithm, which makes an initial guess which offers the probability for each grade.

4. Step 4: Segmentation algorithm and assessment of JSN level based on the segmentation masks. This proceeds

in parallel with step 3.

5. Step 5: Combining the information from the third and the fourth step, to make a ﬁnal KL grade decision.

Figure 1 shows the entire pipeline, and we also share the software.

Figure 2: Joint detection model using Yolo v3-tiny. (a) Yolo v3-tiny network structure (b) the protocol of getting two
detected knees.

2.2.1 Step 1: Image preprocessing

We use the“PixelSpacing” tag from the DICOM images and the bicubic interpolation algorithm to resize the images into
a uniform resolution, 0.2mm per pixel, which is a common value for radiographs. Then we convert all 16-bit grayscale
images into 8-bit grayscale images. Finally, we normalize the pixel intensities by dividing all pixels by 255.

2.2.2 Step 2: Joint Detection Algorithm

To provide a focused region of interest (ROI) and decrease downstream computations, we utilize a detection model to
automatically localize the knee joints beforehand. Our detection model is built based on the Yolo V3-Tiny structure
Adarsh et al. [2020], and the detected bounding boxes are utilized to get the centers of joints. Yolo V3-Tiny is a
simpliﬁed model from the well-known detection model Yolo V3, which implements two-scales of Yolo output layers
instead of Residual layers. Yolo V3-Tiny achieves a speed three orders of magnitude higher than R-CNN, and two
higher than Fast R-CNN, which could improve time efﬁciency for future usage of our approach. Our entire detection
model is shown as Figure 2.

During training, we provided bounding boxes from previously annotated images, which were centered on the knee joint
and were 500 pixels high and wide. The traditional loss function in Yolo V3 is a combination of position-prediction
loss, size-prediction loss, conﬁdence loss and classiﬁcation loss. For our single-class object detection model, the last

5

arXiv Template

A PREPRINT

component only distinguishes the background out. To improve the model’s generalization ability, random rotations of
up to 15 degrees were applied at each batch.

During the inference process, the conﬁdence level of proposed boxes was in great danger of changing dramatically as
the image quality of this large dataset varies, which might incur an uncertain amount of detected knees and hinder our
next process. Therefore, instead of controlling detected objects through conﬁdence thresholds, we ranked the detected
boxes for each image by conﬁdence, and in which we select the top two as the ﬁnal detected knees. By this method, we
could consistently detect two knees even in low-quality images.

2.2.3 Step 3: Classiﬁcation algorithm

We employed a convolutional neural network-based classiﬁer, which accepts images of joints as inputs and automatically
assign KL grades to them. The network backbone is based on the VGG-16 network Simonyan and Zisserman [2014],
with added batch-normalization before max-pooling layers.

The inputs to the classiﬁcation network were the cropped images of the knee joint predicted by the detection model
described in Section 2.2.2. It is worth noting that in order to preserve the resolution and scale of the original image, we
did not crop the image based on the detected box but cut a ﬁxed patch of 672*672 pixels centered around the detected
knee joint, and scaled it to 256*256 pixels to match the input of the VGG-16 network. Also, since there is a imbalance
between classes table 2, we adopted a balanced-sampling to get rid of class bias during training.

2.2.4 Step 4: Segmentation and joint space narrowing assessment

When the cartilage no longer keeps the bones a normal distance apart, joint space narrowing arises Braun and Gold
[2012]. JSN is a crucial marker of osteoarthritis and plays an instrumental role in the assessments of the KL grade.
For this step, we assessed JSN through deep learning-based bone segmentation and an automatic measurement of the
distance between bones.

Step 4a Segmentation algorithm. With the same input as the classiﬁcation algorithm mentioned before, after detecting
the joint position based on the detection algorithm, we obtained a square with an edge length of 672 pixels extracted
from the center of the detected knee joint.

Figure 3: Segmentation algorithm and the joint space narrowing assessment. For step 4b, the green points are the lowest
points on upper bone for LAT side and MED side; the blue lines are the generated series of vertical lines.

In this step, we adopted the U-net segmentation architecture Ronneberger et al. [2015] with Coordinate Convolution
layers Liu et al. [2018] as the model for the bone segmentation, which is a well-established method in medical image
segmentation models in recent years. The network architecture is shown in Figure 3 (step 4a). We set the input size of
the U-net to 672 × 672 × 3 with 3 RGB channels after converting our joint images from gray to RGB. We also divided
each knee annotation into two objects to be treated separately in the segmentation process: upper bone (femur) and the
bone below (we regarded ﬁbula and tibia as one object).

Some images suffer from poor quality, thus, we applied a gamma-correction during evaluation, making our bones more
distinguishable. As Figure 4 (a) shown, when taking gamma γ > 1, it provides a wider range for the lighter parts,
making bones more detectable. Also, gamma is automatically adapted by the average intensity of the 50 × 50 box above
the joint center (which must occur within the upper bone). Based on this gamma transformation, the dice coefﬁcient on

6

arXiv Template

A PREPRINT

Figure 4: gamma correction, (a) The algorithm diagram; (b) the segmentation performance before and after adding
gamma transformation.

the evaluation set was improved. Figure 4 (b) shows a signiﬁcant improvement in segmentation accuracy, especially at
the articular cartilage, which is the area of interest. After gamma correction, Laplacian sharpening, a well-established
method for sharpening image edges Gupta and Porwal [2016], was also applied, and a sharpening ratio of 30% was
used. These image enhancements are only applied during the inference stage, as we found based on experiments that
adding the above enhancements reduces the model’s generalization ability for low quality images, which affects the
overall performance.

Step 4b Joint space narrowing assessment. The segmentation algorithm above provided two masks: one for the upper
part of the joint and one for the lower part.

Figure 5: Diagram of joint space narrowing, (left) is the transfer mapping for medial knee, x-axis is the label for medial
JSN, and y-axis is the calculated joint space distance. (right) is the transfer mapping for lateral knee. x-axis is the labels
for JSN and y-axis is the predicted joint space distance.

The joint space narrowing is deﬁned based on the MED (the inner part of joints), JSN and LAT (the external part of
joints) side. We calculated the joint space distance (JSD) with the following method: (1) ﬁnd the two lowest points for
the upper bone, one is on the MED side and the other is on the LAT side, shown as green points in Figure 3 (step 4b);
(2) generate a series of vertical lines between the femur and tibia near the lowest points, shown as blue lines in Figure 3
(step 4b); (3) get the average length Davg of these lines for MED and LAT side respectively.

After obtaining the JSD, k-means clustering Bock [2007] is applied to complete the JSN degree assessment, which is
based on the JSN ground-truth labels provided by the MOST dataset. Speciﬁcally, by calculating the average MED

7

arXiv Template

A PREPRINT

Figure 6: Procedure of Random Forest for the ﬁnal KL-grade decision making.

joint space distance and the average LAT joint space distance of all the pairs on the evaluation set, we will get 1740
pairs of distance. If by dividing them into 4 classes and minimizing the intra-class variance, we can obtain decision
boundaries tau, 8, 17, 23 for the inner side and 7, 14, 24 for the outer side, with the transformed labels Figure 7 (left)
and (right). It is important to note that these assessment labels are only used for performance evaluation of JSN grading
and for practitioners’ reference. We will directly pass the 2 × 1 JSDs (MED and LAT sides) to the next step.

2.2.5 Step 5: Decision making algorithm

At this point of the algorithm, we obtain a classiﬁcation output in the form of a probability vector representing the
possibilities of each class in step 3, and in step 4 we obtain the JSD from the segmentation model. In this step, we merge
these two vectors and adopt them to the ﬁnal evaluation of the KL grades. Speciﬁcally, connecting these two vectors
gives us a 7 × 1 vector, where we then use them as input to the ﬁnal random forest classiﬁer, Breiman [2001] to make
a reﬁned decision. The random forest algorithm (RFA) (Figure 6) is an ensemble learning method for classiﬁcation,
which is a combination of decision trees and the output is a vote of all the individual trees. During training, bootstrap
sampling is applied to reduce correlations between trees so that each tree acts like an independent expert (analogous to
a committee of radiologists), and our forest contains a set of 100 decision trees, each with a maximum depth of 8. To
improve time efﬁciency and avoid overﬁtting, the minimum number of sample leaves per branch is limited to two and
the bagging method is used in the ﬁtting process. This large set of relatively uncorrelated models (trees) receive inputs
simultaneously and operate as a committee to vote for the ﬁnal decision, which we consider as a strategy to reduce
inter-reader instability.

3 MODEL EVALUATION AND RESULTS

3.1 Model Evaluation

3.1.1 Evaluation of the knee detection algorithm

To evaluate our detection algorithm, we applied the commonly applied Intersection over Union (IoU) metric on the
detection test set of the MOST dataset (Dataset 1). The IoU is the intersection area of the annotated box and the
predicted joint knees’ bounding box divided by the union area of the two boxes. Since our annotation is based on the
joint center Section 2.2.2, we also computed the Euclidean distance Cd between the labeled center and our predicted
center, and called it the predicted deviation distance, and computed the average and maximum deviation distances on
the detection dataset to evaluate the stability of the detection algorithm.

3.1.2 Evaluation of the bone segmentation algorithm

The dice similarity coefﬁcient, also known as the Sorensen-Dice index or simply as the dice coefﬁcient, is a statistical
metric for measuring the similarity between two sets of data. We employed this metric to evaluate segmentation
performance, as it is the most commonly employed evaluation metric for segmentation algorithms. We conducted the
evaluation using the validation set of the MOST dataset (dataset 1).

3.1.3 Evaluation of the joint space narrowing assessment algorithm

To evaluate the JSN grading performance, we included two metrics for evaluation: accuracy and the confusion matrix.
The ground truth JSN labels were annotated from the MOST dataset (dataset 1). Confusion matrix, in which the
number of correct and incorrect predictions are summarized by count values and subdivided by each class, provides a
visualization of the performance of the classiﬁcation algorithm.

8

arXiv Template

A PREPRINT

Figure 7: Segmentation results on the test set, the red line is the outline of the predicted mask.

3.1.4 Evaluation of the complete algorithm for assessing KL grades using Dataset 1 and Dataset 2

For the ﬁnal classiﬁcation model, in addition to the basic KL classiﬁcation accuracy and confusion matrix, we
also included the average recall and accuracy to provide more comprehensive evaluation details. We compared the
classiﬁcation results with other state-of-the-art methods that also use OAI or MOST as the ﬁnal evaluation dataset;
classiﬁcation accuracy and multi-class balance accuracy were chosen as the ﬁnal metrics, which are shared by the
different methods separately Antony et al. [2017], Tiulpin et al. [2018], Thomas et al. [2020], Swiecicki et al. [2021].
For a fair comparison, we displayed both the MOST test results and the OAI test results of our algorithm. As described
in Section 2, the MOST test set was collected as a separate subset consisting of 1763 examinations, while the OAI test
set was the entire collection of 2763 examinations. From previous studies, the performance in detecting the presence of
radioactive OA was also an important aspect of the KL grading method. A KL score of 2 was of exceptional signiﬁcance
as it was often used as a threshold for determining the occurrence of OA. Following the studies of Antony et al. [2017],
Tiulpin et al. [2018], Thomas et al. [2020], we combined KL scores of 0 and 1 as one class (negative) and KL scores of
2, 3 and 4 as another class (positive) to determine the presence of OA, i.e., KL ≥ 2 represents the presence of OA.
Precision, F1-score and classiﬁcation accuracy were chosen as metrics for this binary OA instance analysis task.

3.1.5 Evaluation of the complete algorithm compared with multiple radiologists on Dataset 1 and Dataset 3

To evaluate our models’ performance compared with radiologists, we introduced the coefﬁcient κ to evaluate the
agreement between the two labeling methods, where κ ≤ 0 indicates no agreement, and 1.0 means perfect agreement.
The evaluation was based on 110 cases selected from the MOST dataset (Dataset 1) as described in section 2 A 3)
and 200 selected cases from the Duke dataset (Dataset 3) as described in section 2 A 4). For the MOST dataset, κ
among the 5 radiologists, our ML predictions and the labels provided by MOST were all calculated separately, without
stipulating which instance was the ground truth. For the Duke dataset, since ground truth labels are lacking, we only
compare the performance of our model with the 3 radiologists grading by κ.

3.2 Results

3.2.1 Knee Joint Detection

The detection algorithm achieved an average IoU of 95.42% for right knees and 95.12% for left knees (Table 3). Among
the testing images, all 90 joints were successfully detected, and the center of the detected joints (Cd) are all within a
mismatch distance of 30 pixels, within less than 2% of the image scale.

9

arXiv Template

A PREPRINT

Table 3: Detection IoU on test set

IoU mean (%)
Cd mean (pixels)
Cd max (pixels)

Right Knee
95.42 +/- 3.10
8.52 +/- 0.31
26.01 +/- 2.32

Left Knee
95.12+/-2.81
8.41 +/- 0.25
23.79 +/- 3.43

Table 4: Joint Space Narrowing Prediction on MOST (dataset 1) test set

Predicted Accuracy
F1-score
Precision

Medial side
67.01%
0.65
0.65

Lateral side
84.40%
0.84
0.81

3.2.2 Bone Segmentation

Our segmentation performed with an average Dice coefﬁcient of 98.78%. Some examples of segmentation results are
provided in Figure 7; the presented images are pre-processed to a better quality (Section 2.2: step 4) and the red curve is
the outline of our segmentation masks. Note that our segmentation algorithm can detect bone contour details, especially
on the challenging joint region, which is crucial for accurate joint space distance calculation.

3.2.3 Joint Space Narrowing

After transforming the measured joint space distances into JSN grades, our JSN classiﬁcation algorithm yielded 67.01%
accuracy on medial knees and 84.40% on lateral knees on the MOST test set, as shown in Table 4. Multi-class weighted
F1-scores and precisions are 0.65, 0.65 for the medial side and 0.84, 0.81 for the lateral side, respectively. Confusion
matrices are shown in Figure 8.

3.2.4 Improvement of KL Score Prediction.

The ﬁnal model yielded 75.86% average accuracy and a 73.11% multi-class balanced accuracy on the entire MOST
test set, and it obtained a mean class-wise precision of 0.73 and a mean class-wise recall of 0.73. The confusion matrix
is shown in Figure 9 (a), and it indicated that 93.52% of the mis-graded knees were off by only one grade. When the
deep learning-based classiﬁcation model was used alone (without the JSN component), the average accuracy for the
single classiﬁcation model (no segmentation applied) was 71.93%. The accuracy of the model that was based on JSN
alone was 58.86%. Besides the testing result on the MOST dataset (dataset 1), the average accuracy on OAI dataset
(dataset 2) is 64.48% and balanced accuracy is 68.56%. The confusion matrix is shown as Figure 9 (b). The detailed
comparison of classiﬁcation performance with the state-of-art algorithms is shown in Table 5.

For binary OA instance analysis, we achieved 0.92 for the precision, 0.94 for the F1-score and 0.93 for the average
classiﬁcation accuracy on the MOST dataset (dataset 1), and 0.89 for precision, 0.87 for F1-score and 0.87 for average
classiﬁcation accuracy.

The table of weighted quadratic-weighted Kappa (κ) is shown in Figure 10 (left). On MOST dataset, the average
quadratic weighted Kappa coefﬁcient among all pairs of radiologists at our institution was 0.748 and it was 0.763
between our algorithm and the radiologists. The average κ between Duke Hospital readers and MOST readers was
0.759, and the average κ between our algorithm and MOST readers is 0.896. On Duke dataset, the average quadratic
weighted Kappa among all the radiologists was 0.837, and it was 0.807 between our algorithm and the radiologists. By
transferring the average 1
κ into distances, we also drew a distance diagram as Figure 10(right). The diagram indicates that our algorithm showed
an improved level of similarity to radiologists than radiologists between each other.

4 Discussion

We developed a novel multi-step deep learning model for automatic diagnosis of knee osteoarthritis through Kellgren-
Lawrence grading. In contrast to previous studies, our model does not simply rely on a deep learning model to
automatically make a knee OA classiﬁcation from plain radiographs, but also incorporates bone segmentation, which is
crucial to overall assessment accuracy. By utilizing an image segmentation model, we automatically calculated the JSN
on both medial and lateral side and utilized them as supporting data for ﬁnal decision. Compared to previous published
studies, our model performs better in the predicted multi-class KL grading results and achieves a similar state-of-art

10

arXiv Template

A PREPRINT

Study

Train data Val data

Test data

Detection

Methodology

Acc.

Table 5: Comparison of classiﬁcation results with other methods

Balanced
Acc.

MOST+OAI MOST+OAI MOST+OAI Manual

Classiﬁcation only

61.90% -

MOST+OAI MOST+OAI MOST+OAI Faster R-CNN

MOST

OAI

OAI

MOST

OAI

OAI

OAI

OAI

OAI

OAI

OAI

MOST

Faster R-CNN
with
manual
corrections
Faster R-CNN
manual
with
corrections
Unknown

Random forest re-
gression

Classiﬁcation and Regres-
sion
Separate channels for lat-
eral and medial compart-
ments and model fusion
pretrained
Fine-tuned
(Transfer
ResNet-34
learning)
Fine-tuned
ResNet-169
learning)
Ensemble two models
(Transfer learning)

pretrained
(Transfer

63.60% -

-

-

66.71%

67.49%

70.66% -

-

66.69%

MOST

MOST

MOST

Faster R-CNN

LAT view

61.18% -

MOST

MOST

MOST

Faster R-CNN

PA view

70.85% -

MOST

MOST

MOST

Faster R-CNN

PA and LAT channels

71.90% -

et al.,

et
al.,
Thomas

et al.,
Antony,
2017 Antony et al.
[2017]
Antony,
2017
et al.,
Tiulpin,
2018 Tiulpin et al.
[2018]
Tiulpin,
et al.,
2018 Tiulpin et al.
[2018]
Thomas
2020
et al. [2020]
et al.,
Tiulpin,
2020
Tiulpin
and Saarakkala
[2020]
A. Swiecicki et
al., 2021 Swieci-
cki et al. [2021]
A. Swiecicki et
al., 2021 Swieci-
cki et al. [2021]
A. Swiecicki et
al., 2021 Swieci-
cki et al. [2021]
Ours
Ours
Ours

MOST
MOST
MOST

MOST
MOST
MOST

MOST
MOST
MOST

Yolo v3 tiny
Yolo v3 tiny
Yolo v3 tiny

Ours

MOST

MOST

OAI

Yolo v3 tiny

Just Classiﬁcation
Segmentation based JSN 58.86% -
Segmentation
JSN+ Classiﬁcation
Segmentation
JSN+ Classiﬁcation

based

based

71.93% 72.51%

75.86% 73.11%

64.48% 68.56%

Figure 8: JSN classiﬁcation confusion matrix, (left) JSN for medial knee; (right) JSN for lateral knee.

11

arXiv Template

A PREPRINT

Figure 9: Confusion matrix for ﬁnal KL classiﬁcation results. (left) for MOST (dataset1) test set; (right) for OAI dataset
(dataset 2).

Figure 10: Agreement between the algorithm and radiologists measured by quadratic weighted Kappa (QWK) coefﬁcient
shown in the matrix as well as in a graphical form (distance between the nodes is inversely proportional to QWK), row
1: (left) is the QWK matrix for MOST dataset, (right) is the corresponding graphical form for the left matrix. Row 2:
(left) is the QWK matrix for Duke dataset, (right) is the graphical form for Duke dataset.

12

arXiv Template

A PREPRINT

level of OA instance (KL ≥ 2) detection. Moreover, to our knowledge, this is the ﬁrst study in OA detection to offer
automatic knee segmentation as well as JSN grading visualization, which resulted in signiﬁcantly improved accuracy
and explainability since the segmentation masks and the JSN measurements can be displayed to the user.

Importantly, we made our software publicly available, both as code and as an executable, easy to use by those familiar
with coding in python and those that have only rudimentary knowledge. The software, along with a user guide is
available here: https://github.com/MaciejMazurowski/osteoarthritis-classiﬁcation. We believe that the combination of
the state-of-the-art performance of our algorithm, thorough evaluation with multiple datasets, and public availability of
the code will allow for more rapid advances in OA imaging research. Our code could be applied to large repositories of
OA cases for further research on the questions of surgery appropriateness and outcomes. To our knowledge there is no
publicly available software of this type, until now.

As shown in table 5, our algorithm outperforms the state-of the-art methods in the ability to assess KL scores on the
MOST dataset on both average accuracy and multi-class balanced accuracy, exceeding all the previously reported
algorithms Tiulpin and Saarakkala [2020], Antony et al. [2017], Swiecicki et al. [2021]. Besides superior performance
on MOST dataset, during the testing stage on OAI dataset, we also achieved a multi-class accuracy 64.48% and a
balanced accuracy 68.56%. This setting shares a similarity as Tiulpin and Saarakkala [2020], which is also trained on
MOST dataset, and achieved a balanced accuracy at 66.71% and 67.49% on non-pretrained and pretrained models,
respectively. However, though trained on MOST images, their model validation and selection are based on the OAI
dataset performance, which might introduce a bias in our ﬁnal reporting. Our model is developed only by supporting
MOST training and validation sets with other subsets excluded, and the OAI test set indubitably indicates our method’s
robustness to different artifacts and data acquisition settings. Thomas et al. [2020] reports a 70.66% accuracy on OAI
with a model that is directly trained on it. Our results are slightly lower, but this other method lacks our algorithm’s
detection explainability, and their method’s lack of evaluation on additional datasets remains a potential concern for
their model’s generalization ability. Finally, compared with the method of Swiecicki et al. [2021], our algorithm only
requires PA view images, which is more applicable to radiologists’ common practice.

To highlight the importance of detecting OA instances, our 2-class classiﬁcation model combines (KL≥ 2) for the
presence of osteophytes and radiographic OA. As a result, our model achieves an accuracy of 0.93 and an F1-score of
0.94. This is higher than the reported score in other studies Antony et al. [2017], Tiulpin and Saarakkala [2020], Mengko
et al. [2005], Cheung et al. [2021]. Moreover, Mengko et al. [2005], Cheung et al. [2021] methods all simplify the
problem into binary classiﬁcation only, i.e., they only focus on the determination of OA and abandon the discrimination
of osteoarthritis severity, and we believe that our classiﬁcation based on the 5 grades of KL-score is more speciﬁc and
clinically meaningful.

One consideration to be made is that the assessment of KL grades is subjective, and differents radiologists may have
slight disagreements with one another, a phenomena known as inter-reader variability. Through analyzing the confusion
matrix in Fig. 10, we observed that the highest difference between our algorithm and the MOST radiologist’s label is
three degrees, and 93.52% of the cases where our algorithm misguided suffers only 1-grade mismatch. The quadratic
weighted Kappa of our method and MOST radiologists is 0.896. If we compare this to the average human agreement in
KL grading (0.5-0.8) Tiulpin et al. [2018], our method achieves a relatively high quadratic Kappa. Furthermore, from
the comparison with the additional readers at our institution, we concluded that our algorithm performs at the level of a
radiologist. Since, the average kappa coefﬁcient between our algorithm and the readers achieves a similar level as the
average kappa coefﬁcient among the Duke hospital readers, 0.763 for our algorithms and 0.748 among Duke readers
on the MOST dataset, and 0.807 for our algorithm and 0.837 among Duke readers for Duke dataset. Our algorithm
was placed at the center of the visualization mapping if we treat 1/κ as distance (Figure 10 row 1 (right), showing a
more substantial similarity of our algorithm with radiologists than the similarity between radiologists themselves. This
indicates that our method can perform at a human level.

As described before, we observed an improvement in the segmentation-embedded algorithm’s performance compared
with the single classiﬁcation algorithm. This corresponds to our estimation that JSN plays a vital role in determining
KL grades, both in machine learning and real-world diagnosis. An essential beneﬁt of our method is the supplementary
segmentation results and JSN grading, which could foster better reliability in clinical settings than other DL-based
methods Tiulpin et al. [2018], Tiulpin and Saarakkala [2020], Antony et al. [2017], Thomas et al. [2020]. In addition, the
software that we have made is publicly available with automatic processing of detection, segmentation, and classiﬁcation,
as well as a display of each step’s results after loading raw DICOM images, making deployment immediately feasible.
We believed that these features can provide further information and reduce the costs of routine work for further
practitioners.

13

arXiv Template

A PREPRINT

5 Limitations

There are a few limitations of our study. First, our model is only trained on the MOST dataset, and it may perform and
generalize better if trained on a combination of other datasets, such as Tiulpin et al. [2018], because the MOST images
are collected by a standardized protocol, and therefore may not represent the entire distribution of possible applicable
images. Including data from other institutions could result in an ever-higher generalizability.

Furthermore, in some of the misclassiﬁed images, our radiologists strongly disagreed with the ground truth KL grades
in both OAI and MOST datasets. As such, it is possible that in certain cases in the training set, the ground truth KL
grades were erroneous. It remains a question of how to get rid of these potentially misleading cases during training, or
perform further study on these misclassiﬁed cases. Further studies could also consider incorporating additional OA
features, such as medial tibial attrition, medial tibial sclerosis, and lateral femoral sclerosis.

6 Conclusion

In this paper, we a proposed ﬁve-step deep learning algorithm which uses segmentation and classiﬁcation in parallel
to achieve state-of-the art assessment performance of knee osteoarthritis severity at the radiologist level. Thorough
evaluation with three datasets and a comparison to the performance of multiple experienced readers showed that our
algorithm performs at the level of radiologists. Our software has been made publicly available and easy to use for
further research.

References

Osteoarthritis (OA) | Arthritis | CDC. URL https://www.cdc.gov/arthritis/basics/osteoarthritis.htm.

A. A. Guccione, D. T. Felson, J. J. Anderson, J. M. Anthony, Y. Zhang, P. W.F. Wilson, M. Kelly-Hayes, P. A.
Wolf, B. E. Kreger, and W. B. Kannel. The effects of speciﬁc medical conditions on the functional limitations
of elders in the Framingham Study. American Journal of Public Health, 84(3):351, 1994.
ISSN 00900036.
doi:10.2105/AJPH.84.3.351. URL /pmc/articles/PMC1614827/?report=abstracthttps://www.ncbi.nlm.
nih.gov/pmc/articles/PMC1614827/.

Theo Vos, Abraham D. Flaxman, and et al. Naghavi. Years lived with disability (YLDs) for 1160 sequelae of 289
diseases and injuries 1990-2010: a systematic analysis for the Global Burden of Disease Study 2010. Lancet
(London, England), 380(9859):2163–2196, 2012. ISSN 1474-547X. doi:10.1016/S0140-6736(12)61729-2. URL
https://pubmed.ncbi.nlm.nih.gov/23245607/.

Charles G. Helmick, David T. Felson, Reva C. Lawrence, Sherine Gabriel, Rosemarie Hirsch, C. Kent Kwoh, Matthew H.
Liang, Hilal Maradit Kremers, Maureen D. Mayes, Peter A. Merkel, Stanley R. Pillemer, John D. Reveille, and
John H. Stone. Estimates of the prevalence of arthritis and other rheumatic conditions in the United States.
Part I. Arthritis and rheumatism, 58(1):15–25, jan 2008.
ISSN 0004-3591. doi:10.1002/ART.23177. URL
https://pubmed.ncbi.nlm.nih.gov/18163481/.

Total Joint Replacement

- OrthoInfo - AAOS.

URL https://orthoinfo.aaos.org/en/treatment/

total-joint-replacement/.

Why are patients dissatisﬁed following a total knee replacement? A systematic review. International Orthopaedics,
ISSN 14325195. doi:10.1007/S00264-020-04607-9/TABLES/13. URL https:

44(10):1971–2007, oct 2020.
//link.springer.com/article/10.1007/s00264-020-04607-9.

Home | Agency for Healthcare Research and Quality. URL https://www.ahrq.gov/.

replacement
Planned
Cross
URL
planned-knee-and-hip-replacement-surgeries-are-the-rise-the-us.

surgeries
Blue
rise
https://www.bcbs.com/the-health-of-america/reports/

knee
Blue

the U.S.

Shield.

and

hip

the

are

on

in

|

Elena Losina, Thomas S. Thornhill, Benjamin N. Rome, John Wright, and Jeffrey N. Katz. The dramatic increase in
total knee replacement utilization rates in the United States cannot be fully explained by growth in population size
and the obesity epidemic. The Journal of bone and joint surgery. American volume, 94(3):201–207, feb 2012. ISSN
1535-1386. doi:10.2106/JBJS.J.01958. URL https://pubmed.ncbi.nlm.nih.gov/22298051/.

M. C.S. Inacio, E. W. Paxton, S. E. Graves, R. S. Namba, and S. Nemes. Projected increase in total knee arthroplasty in
the United States - an alternative projection model. Osteoarthritis and cartilage, 25(11):1797–1803, nov 2017. ISSN
1522-9653. doi:10.1016/J.JOCA.2017.07.022. URL https://pubmed.ncbi.nlm.nih.gov/28801208/.

14

arXiv Template

A PREPRINT

Michael E. Porter. What Is Value in Health Care? New England Journal of Medicine, 363(26):2477–2481, dec
2010. ISSN 0028-4793. doi:10.1056/NEJMP1011024/SUPPL_FILE/NEJMP1011024_DISCLOSURES.PDF. URL
https://www.nejm.org/doi/full/10.1056/nejmp1011024.

Daniel L. Riddle, Robert A. Perera, William A. Jiranek, and Levent Dumenci. Using surgical appropriateness criteria to
examine outcomes of total knee arthroplasty in a United States sample. Arthritis care & research, 67(3):349–357,
mar 2015. ISSN 2151-4658. doi:10.1002/ACR.22428. URL https://pubmed.ncbi.nlm.nih.gov/25132662/.
the Knee - Clinical Practice Guideline (CPG)
| Ameri-
URL https://www.aaos.org/quality/quality-programs/

Surgical Management of Osteoarthritis of
can Academy of Orthopaedic Surgeons.
lower-extremity-programs/surgical-management-of-osteoarthritis-of-the-knee/.

Daniel L. Riddle, William A. Jiranek, and Curtis W. Hayes. Use of a validated algorithm to judge the appropriateness
of total knee arthroplasty in the United States: a multicenter longitudinal cohort study. Arthritis & rheumatology
(Hoboken, N.J.), 66(8):2134–2143, 2014. ISSN 2326-5205. doi:10.1002/ART.38685. URL https://pubmed.ncbi.
nlm.nih.gov/24974958/.

J. H. KELLGREN and J. S. LAWRENCE. Radiological assessment of osteo-arthrosis. Annals of the rheumatic diseases,
16(4):494–502, 1957. ISSN 0003-4967. doi:10.1136/ARD.16.4.494. URL https://pubmed.ncbi.nlm.nih.
gov/13498604/.

Maximilian A Meyer, Timothy S Leroux, David M Levy, Annemarie K Tilton, Paul B Lewis, Adam B Yanke, and
Brian J Cole. Flexion Posteroanterior Radiographs Affect Both Enrollment for and Outcomes After Injection
Therapy for Knee Osteoarthritis. Orthopaedic journal of sports medicine, 5(5):2325967117706692, may 2017. ISSN
2325-9671. doi:10.1177/2325967117706692. URL http://www.ncbi.nlm.nih.gov/pubmed/28589160http:
//www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5444580.

Y. Sun, K. P. Günther, and H. Brenner. Reliability of radiographic grading of osteoarthritis of the hip and knee.
Scandinavian journal of rheumatology, 26(3):155–165, 1997. ISSN 0300-9742. doi:10.3109/03009749709065675.
URL https://pubmed.ncbi.nlm.nih.gov/9225869/.

Daniel L. Riddle, William A. Jiranek, and Jason R. Hull. Validity and reliability of radiographic knee osteoarthritis
ISSN 1938-2367. doi:10.3928/01477447-

measures by arthroplasty surgeons. Orthopedics, 36(1), jan 2013.
20121217-14. URL https://pubmed.ncbi.nlm.nih.gov/23276348/.

Kristina Klara, Jamie E. Collins, Ellen Gurary, Scott A. Elman, Derek S. Stenquist, Elena Losina, and Jeffrey N.
Katz. Reliability and Accuracy of Cross-sectional Radiographic Assessment of Severe Knee Osteoarthritis:
Role of Training and Experience. The Journal of rheumatology, 43(7):1421–1426, jul 2016. ISSN 0315-162X.
doi:10.3899/JRHEUM.151300. URL https://pubmed.ncbi.nlm.nih.gov/27084912/.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137–1149, jun
2015. ISSN 01628828. doi:10.1109/TPAMI.2016.2577031. URL https://arxiv.org/abs/1506.01497v3.
Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, and Nuno Vasconcelos. A Uniﬁed Multi-scale Deep Convolutional
Neural Network for Fast Object Detection. Lecture Notes in Computer Science (including subseries Lecture Notes
in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 9908 LNCS:354–370, jul 2016. ISSN 16113349.
doi:10.1007/978-3-319-46493-0_22. URL https://arxiv.org/abs/1607.07155v1.

Shang Hung Lin, Sun Yuan Kung, and Long Ji Lin. Face recognition/detection by probabilistic decision-based neural
network. IEEE Transactions on Neural Networks, 8(1):114–132, 1997. ISSN 10459227. doi:10.1109/72.554196.
Steve Lawrence, C. Lee Giles, Ah Chung Tsoi, and Andrew D. Back. Face recognition: A convolutional neural-network
approach. IEEE Transactions on Neural Networks, 8(1):98–113, 1997. ISSN 10459227. doi:10.1109/72.554195.
Aleksei Tiulpin, Jérôme Thevenot, Esa Rahtu, Petri Lehenkari, and Simo Saarakkala. Automatic Knee Osteoarthritis
Diagnosis from Plain Radiographs: A Deep Learning-Based Approach. Scientiﬁc Reports 2018 8:1, 8(1):1–10,
jan 2018. ISSN 2045-2322. doi:10.1038/s41598-018-20132-7. URL https://www.nature.com/articles/
s41598-018-20132-7.

Joseph Antony, Kevin McGuinness, Kieran Moran, and Noel E. O’Connor. Automatic Detection of Knee Joints and
Quantiﬁcation of Knee Osteoarthritis Severity using Convolutional Neural Networks. Lecture Notes in Computer
Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 10358
LNAI:376–390, mar 2017. ISSN 16113349. doi:10.1007/978-3-319-62416-7_27. URL https://arxiv.org/abs/
1703.09856v1.

Albert Swiecicki, Nianyi Li, Jonathan O’Donnell, Nicholas Said, Jichen Yang, Richard C. Mather, William A. Jiranek,
and Maciej A. Mazurowski. Deep learning-based algorithm for assessment of knee osteoarthritis severity in
radiographs matches performance of radiologists. Computers in biology and medicine, 133, jun 2021. ISSN 1879-
0534. doi:10.1016/J.COMPBIOMED.2021.104334. URL https://pubmed.ncbi.nlm.nih.gov/33823398/.

15

arXiv Template

A PREPRINT

Sudeep Kondal, Viraj Kulkarni, Ashrika Gaikwad, Amit Kharat, and Aniruddha Pant. Automatic grading of knee

osteoarthritis on the kellgren-lawrence scale from radiographs using convolutional neural networks, 2020.

T.L. Mengko, R.G. Wachjudi, A.B. Suksmono, and D. Danudirdjo. Automated detection of unimpaired joint space for
knee osteoarthritis assessment. In Proceedings of 7th International Workshop on Enterprise networking and Comput-
ing in Healthcare Industry, 2005. HEALTHCOM 2005., pages 400–403, 2005. doi:10.1109/HEALTH.2005.1500491.

James Chung-Wai Cheung, Andy Yiu-Chau Tam, Lok-Chun Chan, Ping-Keung Chan, and Chunyi Wen. Superiority
of multiple-joint space width over minimum-joint space width approach in the machine learning for radiographic
severity and knee osteoarthritis progression. Biology, 10(11), 2021. ISSN 2079-7737. doi:10.3390/biology10111107.
URL https://www.mdpi.com/2079-7737/10/11/1107.

Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. International Journal of
Computer Vision, 128(2):336–359, oct 2016. doi:10.1007/s11263-019-01228-7. URL http://arxiv.org/abs/
1610.02391http://dx.doi.org/10.1007/s11263-019-01228-7.

Mahrukh Saleem, Muhammad Shahid Farid, Saqib Saleem, and Muhammad Hassan Khan. X-ray image analysis for
automated knee osteoarthritis detection. Signal, Image and Video Processing, 14(6):1079–1087, September 2020.
ISSN 1863-1711. doi:10.1007/s11760-020-01645-z. URL https://doi.org/10.1007/s11760-020-01645-z.

Benjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh Khodakarami, Thakkar Shraddha, Re-
becca Kusko, Susanna Assunta Sansone, Weida Tong, Russ D. Wolﬁnger, Christopher E. Mason, Wendell Jones,
Joaquin Dopazo, Cesare Furlanello, Levi Waldron, Bo Wang, Chris McIntosh, Anna Goldenberg, Anshul Kun-
daje, Casey S. Greene, Tamara Broderick, Michael M. Hoffman, Jeffrey T. Leek, Keegan Korthauer, Wolfgang
Huber, Alvis Brazma, Joelle Pineau, Robert Tibshirani, Trevor Hastie, John P.A. Ioannidis, John Quackenbush,
and Hugo J.W.L. Aerts. Transparency and reproducibility in artiﬁcial intelligence. Nature, 586(7829):E14, oct
2020. ISSN 14764687. doi:10.1038/S41586-020-2766-Y. URL /pmc/articles/PMC8144864//pmc/articles/
PMC8144864/?report=abstracthttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC8144864/.

Multicenter Osteoarthritis Study (MOST) Public Data Sharing | MOST Public Data Sharing. URL https://most.

ucsf.edu/multicenter-osteoarthritis-study-most-public-data-sharing.

Multicenter Osteoarthritis Study (MOST) Public Data Sharing | MOST Public Data Sharing. URL https://most.

ucsf.edu/multicenter-osteoarthritis-study-most-public-data-sharing.

Peter Mildenberger, Marco Eichelberg, and Eric Martin. Introduction to the DICOM standard. European radiology, 12
(4):920–927, apr 2002. ISSN 0938-7994. doi:10.1007/S003300101100. URL https://pubmed.ncbi.nlm.nih.
gov/11960249/.

Osirix: An open-source software for navigating in multidimensional dicom images. URL https://www.

osirix-viewer.com/.

Pranav Adarsh, Pratibha Rathi, and Manoj Kumar. YOLO v3-Tiny: Object Detection and Recognition using one stage
improved model. 2020 6th International Conference on Advanced Computing and Communication Systems, ICACCS
2020, pages 687–694, mar 2020. doi:10.1109/ICACCS48705.2020.9074315.

Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. 3rd
International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, sep 2014. URL
https://arxiv.org/abs/1409.1556v6.

Hillary J. Braun and Garry E. Gold. Diagnosis of osteoarthritis: Imaging. Bone, 51(2):278–288, aug 2012. ISSN

87563282. doi:10.1016/J.BONE.2011.11.019.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image
Segmentation. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics), 9351:234–241, may 2015. ISSN 16113349. URL https://arxiv.org/abs/
1505.04597v1.

Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An
Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution. Advances in Neural Information
Processing Systems, 2018-December:9605–9616, jul 2018. ISSN 10495258. URL https://arxiv.org/abs/
1807.03247v2.

Suneet Gupta and Rabins Porwal. COMBINING LAPLACIAN AND SOBEL GRADIENT FOR GREATER SHARP-
ISSN 09769099.

ICTACT Journal on Image and Video Processing, 06(04):1239–1243, may 2016.

ENING.
doi:10.21917/IJIVP.2016.0180.

16

arXiv Template

A PREPRINT

Hans-Hermann Bock. Clustering Methods: A History of k-Means Algorithms, pages 161–172. Springer Berlin
ISBN 978-3-540-73560-1. doi:10.1007/978-3-540-73560-1_15. URL

Heidelberg, Berlin, Heidelberg, 2007.
https://doi.org/10.1007/978-3-540-73560-1_15.

Leo Breiman. Random Forests. Machine Learning 2001 45:1, 45(1):5–32, oct 2001.

ISSN 1573-0565.

doi:10.1023/A:1010933404324. URL https://link.springer.com/article/10.1023/A:1010933404324.
Kevin A. Thomas, Łukasz Kidzi´nski, Eni Halilaj, Scott L. Fleming, Guhan R. Venkataraman, Edwin H. G. Oei,
Garry E. Gold, and Scott L. Delp. Automated Classiﬁcation of Radiographic Knee Osteoarthritis Severity
Using Deep Neural Networks. Radiology. Artiﬁcial intelligence, 2(2):e190065, mar 2020.
ISSN 2638-6100.
doi:10.1148/RYAI.2020190065. URL https://pubmed.ncbi.nlm.nih.gov/32280948/.

Aleksei Tiulpin and Simo Saarakkala. Automatic Grading of Individual Knee Osteoarthritis Features in Plain Radio-
graphs Using Deep Convolutional Neural Networks. Diagnostics 2020, Vol. 10, Page 932, 10(11):932, nov 2020.
ISSN 20754418. doi:10.3390/DIAGNOSTICS10110932. URL https://www.mdpi.com/2075-4418/10/11/
932/htmhttps://www.mdpi.com/2075-4418/10/11/932.

17

