Generative Adversarial Networks for
Pseudo-Radio-Signal Synthesis

Haythem Chaker, Graduate Student Member, IEEE, Soumaya Hamouda, Senior Member, IEEE,
and Nicola Michailow

1

2
2
0
2

n
u
J

9
2

]
P
S
.
s
s
e
e
[

1
v
2
4
7
4
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—For many wireless communication applications, traf-
ﬁc pattern modeling of radio signals combined with channel
effects is much needed. While analytical models are used to
capture these phenomena, real world non-linear effects (e.g.
device responses, interferences, distortions, noise) and especially
the combination of such effects can be difﬁcult to capture by these
models. This is simply due to their complexity and degrees of
freedom which can be hard to explicitize in compact expressions.
In this paper, we propose a more model-free approach to jointly
approximate an end-to-end black-boxed wireless communication
scenario using software-deﬁned radio platforms and optimize
for an efﬁcient synthesis of subsequently similar “pseudo-radio-
signals”. More precisely, we implement a generative adversarial
network based solution that automatically learns radio properties
from recorded prototypes in speciﬁc scenarios. This allows for
a high degree of expressive freedom. Numerical results show
that the prototypes’ trafﬁc patterns jointly with channel effects
are learned without the introduction of assumptions about the
scenario or the simpliﬁcation to a parametric model.

Index Terms—Deep Learning, Generative Adversarial Net-

works, Over-The-Air Learning, Software-Deﬁned Radio

I. INTRODUCTION

R ADIO signals are all around us and they serve as a

key enabler for both communications and sensing, as
our daily, commercial and industrial needs increasingly grow
reliant on a heavily interconnected and automated world. Over
the past century, much effort has gone into expert system
design and optimization for both wireless communication
and localization systems [1]. The main considerations are on
how to precisely represent, shape, adapt, and recover these
signals through a lossy, non-linear, and often interference
heavy channel environment.

In the recent years on an other hand, heavily expert-tuned
basis functions, such as Gabor ﬁlters in the vision domain,
have been largely discarded due to the speed at which they
can be naively learned and adapted using feature learning
approaches in deep neural networks [2].

In this paper, we aim to characterize wireless trafﬁc patterns.
We explore making a similar transition from using expert-
designed representations and models of wireless links towards
using a data driven approach to generate waveforms related to
an arbitrary end-to-end wireless communication trafﬁc pattern.

This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.

H. Chaker is with the SIGCOM research group at the Interdisciplinary
Centre for Security, Reliability and Trust (SnT), University of Luxembourg,
Luxembourg (corresponding author: haythem.chaker@uni.lu). S. Hamouda
is with the MEDIATRON research laboratory at the Telecommunications
Engineering School of Tunis (Sup’Com), University of Carthage, Tunisia.
N. Michailow is with Siemens Technology, Munich, Germany.

For instance, in context of product testing, network plan-
ning or network optimization, R&D engineers usually turn to
network and waveform simulators (e.g. [3]) that use tractable
channel models and have solid foundations on information
theory and statistics. Past experience has shown that, in such
an approach, data trafﬁc and waveform generation can prove
to be time consuming and not efﬁcient for all scenarios. This
is because, in simulators, not all details of wireless standards
are implemented. Moreover, once transmitted over-the-air, the
signal passes through many harsh effects imbued with the
chaotic and random laws of nature.

To expand the scope of wireless signal emulation beyond
purely theoretical analysis, real radio signals can be digitally
recorded and processed using state-of-the-art software-deﬁned
radio (SDR) hardware platforms (e.g. [4]). The recorded sig-
nal includes the end-to-end wireless communication scenario
characteristics, namely:

•

•

•

•

The trafﬁc pattern (e.g. modulation and multiple access
schemes),
Setup conﬁguration (e.g. distance between the devices
and signal attenuation),
Radio frequency (RF) medium particularities (e.g multi-
path propagation and thermal noise),
Hardware imperfections (e.g. non-linear ampliﬁcation and
ﬁnite resolution sampling).

With the goal of retransmission, populating a database of
real
trafﬁc pattern prototypes recorded with SDR is cost-
effective and can replace software-over-the-air simulators.
However, recordings of raw wireless signals at sampling rates
as high as 160 MHz, in real-time, challenge the read/write
speeds and storage capacities of modern commercial storage
solutions (around 1.28 GByte for 160 MSamples/s).

In this work, the storage and timelessness issues are treated
as a signal generation problem. The idea is to digitally store
a relatively short prototype recording of a real wireless signal
under a certain trafﬁc pattern scenario, and further develop a
machine learning (ML) method able to synthesize a pseudo-
radio-signal (with an arbitrary length) that has the same radio
properties as the prototype. This would particularly solve
the storage problem by building, for different scenarios, a
catalogue of lightweight signal generation models that are non-
parametric, i.e., with no prior information on the channel.

Nevertheless, channel modeling happens to be critical, not
only in the design and evaluation but also when learning online
over a wireless communication chain. Related works [2], [4]–
[8] regarding the design and learning involving end-to-end
wireless systems have focused on using simpliﬁed analytics
such as the additive white Gaussian noise and the Rayleigh

 
 
 
 
 
 
fading channel models. Recent works [9]–[13] tackled channel
agnostic over-the-air learning approaches (with no simulated
approximations) to learn modulation schemes from real world
measurements using SDR hardware platforms and capabili-
ties. Historically, the latter method is enabled following the
introduction of generative adversarial networks (GAN) [14].
GANs have been extensively applied in computer vision
and text analytics to generate synthetic data that is statistically
similar to real data [15]. More recently, there have been efforts
to apply GANs to wireless communications ofﬂine too to
augment training (waveform) data sets such as those used to
train classiﬁers for spectrum sensing and jamming [12].

To solve the signal generation problem, we propose a GAN-
based method along with its parameter tuning and the digital
signal processing (DSP) necessary to efﬁciently command
the pseudo-radio-signal synthesis operation. The geometric
disposition of radio devices in a prototyped scenario are known
as domain knowledge, while no assumptions are made on the
channel where the trafﬁc pattern is occurring (e.g. carrier(s)
and bandwidth(s) of interest).

This paper is organized as follows. In Section II, the system
model and problem formulation are introduced. In Section III,
GAN basics and advantages are elaborated. In Section IV,
the pseudo-radio-signal synthesis operation is detailed. The
structure of the proposed GAN model is explored in Section
V. Numerical results and outlook are presented in Section VI.
Conclusions are drawn at the end of this paper in Section VII.

II. SYSTEM MODEL AND PROBLEM FORMULATION

A. System model

Consider a simple device-to-device wireless communication
system where a transmitter T interacts with a receiver R over-
the-air through a radio channel H.

Let the transmitted signal s(t) incorporate detectable wave-
froms typical in digital communication and related to modu-
lation, coding and multiple access schemes. The properties of
s(t) then denote the trafﬁc pattern particular to the end-to-end
wireless communication between T and R. Before being cap-
tured as x(t), the waveform undergoes joint transformations
such as path-loss, multi-path propagation effects, thermal noise
as well as other hardware imperfections like phase shifts and
non-linear ampliﬁcations all depicted in h(t).

For sake of our proposed method, we add to the system an
SDR platform able to capture x(t) but with no prior synchro-
nization nor negotiations with the transmitter T . Assuming
wideband SDR capabilities, the signal x(t) is recorded for a
duration Tprototype and is dubbed the scenario prototype.

The analog signal x(t) captures the joint non-linearities
between the stochastic signal s(t) and the unknown channel
response h(t) characterizing the black-box end-to-end wire-
less link between T and the SDR (playing the role of R)
placed at a ﬁxed seperation distance. Our objective is to
generate a pseudo-radio-signal x(cid:48)(t) for an arbitrary duration
Tgen > Tprototype, while keeping the same radio properties of
the prototype. In other words, the task is to ﬁnd x(cid:48)(t) with
a response at R that is statistically similar to the prototype’s

2

x(t) (once we broadcast it with SDR). Therefore a correct
eventual solution would be to synthesize

x(cid:48) ∼ pprototype(x(cid:48)).

(1)

In traditional engineering workﬂow, ﬁnding the continuous
probability distribution pprototype is a model deﬁcit problem [16]
as no physics-based parametric mathematical model of h(t)
can be extracted (even in the absence of diagnosable trafﬁc).
On an other hand, for sake of generality, the transmitter
device T and its distance from the SDR hardware can differ
from scenario to another which entails different prototype
measurements for a same trafﬁc pattern. SDR platforms can
be purposefully suited for this: where digital spectrum sensing
and synchronization efforts can be done to identify the type
of the device T and the nature of the detected waveforms for
a number of cases and within a range of possible separation
distances (see for example [4]). This, however, is too complex
to design for all wireless systems and use-cases which would
limit the generalization of the problem at hand. Therefore,
pseudo-radio-signal synthesis is also an algorithm deﬁcit prob-
lem for which lower-complexity solutions are envisaged.

B. Problem formulation

In these model and algorithm deﬁcit contexts,

the re-
lationship (1)
is interpreted as a density approximation
problem. In the digital domain,
the stochastic process
(cid:17)
}0<t<Tprototype sampled at a sampling rate
X = {x = x
Rs > 0 yield Nprototype observations x. We formulate the
density approximation statistically by constructing an objective
model deﬁned by a probability distribution pmodel and param-
eters θ, to be used to sample x(cid:48), such that

(cid:16) t
Rs

let

X = {x | x ∼ pprototype(x)}, |X| = Nprototype;

pmodel(x; θ) ≈ pprototype(x).

(2)

Finding the probabilistic model pmodel with optimal param-
eters θ∗ is called a generative modeling problem [17] as
it allows to synthesize x(cid:48). By resolving this problem, the
trafﬁc pattern in s(t) and the natural channel effects h(t)
can be substituted by such an optimal waveform sampler
pmodel(· ; θ∗) to synthesize pseudo-radio-signal x(cid:48)(t) of chosen
length Tgen > Tprototype using DSP. Fig. 1 gives a high level
formulation of this generative modeling problem for pseudo-
radio-signal synthesis. This technique is useful for many radio
signal applications including trafﬁc pattern emulation.

If domain knowledge was available on x(t), maximum
likelihood estimation (MLE) (or Kullback–Leibler divergence)
could be used to explicitly approximate the density pmodel. The
problem could then be solved using MLE inference equations
in an auto-regressive manner. However, explicit density esti-
mation techniques are discarded in our case because as we
formulate it, the problem is agnostic to the wideband channel
and the transmit-receive chain is treated as a black box. In
consequence, no a priori information would be available.

On this account,

the density pmodel must be implicitly
estimated [17]. This means that a solution must be found
without explicitly deﬁning the density function pmodel(x; θ). In
a probabilistic framework, consider the large set S of possible

3

Fig. 1. High level formulation of the generative modeling problem for pseudo-radio-signal synthesis.

samples featuring the trafﬁc pattern “dynamics”. The solution
we target would optimally limit the (larger) boundary of space
S to the one of space X.

Recent advances in computational resources [16], [18] al-
low to investigate data-driven methods, namely deep neural
networks (DNN) mechanisms,
to processes raw data, e.g.
our prototype X [19], as input in order to analyse its high-
dimensional dynamics. In our case, these dynamics are consist
of the captured joint non-linearities between s(t) and h(t)
that virtually describe the prototyped trafﬁc pattern. In fact,
the minimization of the arbitrary sampling space S towards
the prototyped dynamics space X is the problem we aim to
tackle using a deep learning (DL) approach. If the DL model
is successfully tested and validated to output samples with
magnitudes following the distribution pmodel, it is named a
deep generative model.

With proper DSP, a deep generative model substitutes the
step of acquiring domain knowledge on the prototyped trafﬁc
pattern with the potentially easier task of collecting a sufﬁcient
number of prototype samples Ns
to conceive a generative
algorithm of interest. Learning is made possible by the choice
of a set of possible “DNN machines” with a certain process
that optimizes their “trainable parameters” θDNN in order
to build pmodel(x, θ∗) ≈ pprototype(x). In the next section,
we explore GAN: a (modern) deep generative model that
implicitly estimate pmodel directly.

III. GAN BASICS

A. Adversarial function approximation
To implicitly estimate pprototype(x),

the prototype X is
processed, ﬁrst, by a DNN binary classiﬁer D(., θD) (with two
outputs), henceforth termed the “discriminator”. This DNN is
described with the trainable parameters θD and aims to distin-
guish samples of X from a prior random noise z ∼ pz(z). The
noise samples z are the output of a second DNN G(z, θG),
henceforth called the “generator”, described with the trainable
parameters θG.

In a ﬁrst version, called pre-training, the discriminator alone
is trained to maximize its ability to distinguish its input
between the two categories: i) from X or ii) not from X.

The classiﬁcation results are then sent to the generator as
feedback. The latter then manipulates pz in order to synthesize
samples x(cid:48) = G(z, θG) with similar properties to the input
samples x of the prototype X. Consequently, this minimizes
the classiﬁcation success of the discriminator. This process
continues ofﬂine (in training) with joint updates on G(z, θG)
and D(., θD).

In game theory, the described design is called a minimax
game because it involves minimization in an outer loop and
maximization in an inner loop. Accordingly, simultaneous
training of the two adversary players is guided by a minimax
value function V , i.e.:

(θ∗D, θ∗G) = arg min
G

max
D

V (G, D).

(3)

The game is implemented using an iterative numerical
approach with updates on (θD, θG): the trainable parameters of
the two DNNs. In theory, if the generator and the discriminator
are given enough capacity in the non-parametric limit, a
convergence criterion allows to recover the data generating
distribution pmodel(x, θ∗) on the generator’s output. An opti-
mum (θ∗D, θ∗G) is called Nash equilibrium [15] of the game. At
Nash equilibrium, the generator is said to be playing the role
of a density estimator of pprototype(x) as it would able to output
samples identical to the prototyped samples. In this case, the
discriminator would distinguishes between the two classes i)
and ii) with an equal probability of 1
2 .

In practice, however, pprototype(x) and pG(x) are not always
non-zero, therefore the success ratio of the discriminator is
numerically unstable and D(x) itself is typically not convex
for all values of x. Hence, ﬁnding the optimum in D(x)
is neither unique nor guaranteed. This is why, in parameter
space, adversarial training on (θD, θG) makes the performance
of D(x, θD) get closer to D∗(x, θ∗D) upon a stop criterion:
at which the best associated generator G∗(x, θ∗G) implicitly
approximates pprototype(x).

Optimizing the discriminator to completion in the inner
loop in function space is computationally prohibitive [14].
Moreover, a ﬁnite number of inputs would result in overﬁtting.
Since the updates happen on (θD, θG) in the DNN parameters
space, and not in the function space, this discards convexity
issues but introduces multiple critical points dependent on both
DNN architectures. In fact, choosing pmodel(x, θ) is limited
by a family of possible densities pG that represent a space
X (cid:48) = {x(cid:48) ∼ pG(z,θG)(x(cid:48)) | ∀θG} constructable via any
function G(z; θG) instead of θ.

In a less formal illustration, in Fig. 2 [20], global optimality
of the value function V denotes convergence of the game. In
more detail, the generator here (in yellow) learns in the best
possible way, an implicit distribution pmodel(x) ≈ pprototype(x)
(respectively in green and blue) that inherently captures all
prototyped dynamics of X such as waveforms, channel effects
and radio device imperfections. In the same ﬁgure, the space
S can be viewed as the space of the z samples (in red) trans-

Ts(t)Hh(t)R(SDR)x(t)GenerativemodelXprototypeprototypedtraﬃcpatternpmodel(x;θ)x0(t)pseudo-radio-signal4

function (4). Therefore, the loss function of the generative
model can be written as the opposite of the loss function of
the discriminative model:

Ex

LG(D, G) =

LG(D, G) = − LD(D, G);
1
2
1
2
1
2

LG(G) =

Ez

Ez

∼

∼

∼

pprototype(x)[log (D(x, θD))]+

pz(z)[log (1 − D(G(z, θG), θD))];

(6)

pz(z)[log (1 − D(G(z, θG), θD))].

The latter expression is useful for theoretical analysis.
However, if we consider the case where the discriminator
rejects the generator’s samples with high conﬁdence, we have:

D(G(z, θG), θD) → 0;

LG(G) =

1
2

Ez

∼

pz(z)[log (1 − D(G(z, θG), θD))] → 0.

(7)

Here, this situation indicates that the gradients on θG would
vanish during training, i.e. the vanishing gradients problem.
This would remain true unless a bias signal is added to the
generator’s parameters.

To solve this without biasing the prototype, one heuristically
veriﬁed approach [14] is to continue to use cross-entropy
minimization for the generator and instead of ﬂipping the sign
on the discriminator’s loss function, the target used to construct
the cross-entropy cost in (4) is ﬂipped, i.e.:

LG(G) = −

1
2

Ez

∼

pz(z)[log (D(G(z, θG), θD))].

(8)

The generator would now try to maximize the log-probability
of the discriminator being wrong. And when the latter rejects
the former’s samples with a high accuracy, gradients on θG
also get better, i.e.:

D(G(z, θG), θD) → 0;

LG(G) = −

1
2

Ez

∼

pz(z)[log (D(G(z, θG), θD))] → ∞.

(9)

In this design, the game is no longer zero-sum, and it cannot
be described with a single value function.

A good example to understand the vanishing gradients
problem in the pseudo-radio-signal synthesis application is the
differentiation of raw prototype data samples coming from an
unknown wireless channel H. Broadly speaking, the discrim-
inator does not make any assumptions on pprototype because it
is a binary classiﬁer initially trained in a supervised learning
setting. Hence, D is less prone to biases caused by channel
agnosticism. Contrastingly, the generator’s gradients can be
blocked by the total blindness of the stochastic channel and the
fact that its joint non-linearities are not modelled in pz(z) (in
some pre-training version similar to D). In other words, when
training the generator on minimizing the objective function
(4), its gradients can turn out to be relatively ﬂat (e.g. at high
path-loss) and they would rapidly vanish as explained in (7).
This would make G learn no inherent dynamic features while
D would gets too successful at its classiﬁcation. Therefore,
we use expression (8) as the loss function of the generative
model as a work-around to this issue.

Fig. 2. Global optimality formulation in probabilistic spaces [20]

formed, at Nash equilibrium, to a space X (cid:48) in a minimization
effort of a particular probabilistic loss function.

is

In deep generative modeling,

the described game is
function approximation [15], and the
called adversarial
chained generator-discriminator DNN pair
called a
Generative Adversarial Network or GAN [14]. The adversary
DNN players D and G are iteratively trained on stochasti-
cally optimizing (θD, θG). This optimization is done using a
supervised loss LD (with labels on the two classes) for the
discriminative model and an unsupervised embedded loss LG
for the generative model (hence the implicit density estima-
tion notion), until a stop criterion, signalling convergence, is
reached. A training stop criterion makes the GAN chained
model approximation subject
to the failures of supervised
learning: overﬁtting and underﬁtting [15], [17]. Other deep
generative models make other approximations that are prone
to other failures [17]. In principle, with rigorous optimiza-
tion, hyper-parameter tuning and enough training data, GAN
training limitations can be mitigated. The loss functions of the
GAN model are explained in the following subsections.

B. Loss function of the discriminative model

According to the original GAN paper [14], the value func-
tion V of both the GAN components can be written as a
zero-sum game between two Kullback-Leibler divergences:
where D(x, θD) is the discriminator output for prototyped
data x ∼ pprototype(x) and D(G(z, θG), θD) is the discriminator
output for generated data G(z, θG), i.e.:

V (D, G) =Ex
Ez

pprototype(x)[log (D(x, θD))]+
∼
pz(z)[log (1 − D(G(z, θG), θD))].

∼

(4)

The quantity V (D, G) can be viewed as the standard cross-
entropy loss function of a binary classiﬁer scaled by −2 where:
data coming from the prototype is labeled “1” for all entries,
and data coming from the generator is labeled “0” for all
entries. The loss function of the discriminative model can then
be written as

LD(D, G) = −

−

1
2
1
2

Ex

∼

pprototype(x)[log (D(x, θD))]

Ez

pz(z)[log (1 − D(G(z, θG), θD))].

∼

(5)

C. Loss function of the generative model

Similarly in this adversarial training setting, the generator
iteratively plays the zero-sum the game expressed by the value

D. GAN advantages and challenges

The last paragraph of the previous section depicts a good
example on why domain knowledge is important in ML and
DL. In fact, unless one is willing to make some assumptions
about the problem from domain knowledge, estimating the
optimal GAN parameters (θ∗G, θ∗D) from the training set alone
is evidently impossible no matter the training capacities. This
impossibility is also known as the no free-lunch theorem [16],
stating that: without making assumptions about the relation-
ship between input and output, it is not possible to generalize
the avaiable observations outside the training set.

GANs potentially solve major limitations in learning end-to-
end physical channel properties such as trafﬁc pattern features,
as we will show. We ﬁrst note that the weights of the chained-
GAN are usually updated using a stochastic gradient update
algorithm that computes error gradients propagated from the
discriminator to the generator. Using the prototyped input, the
back-propagation of the gradients can be blocked (whitened) at
the level of the discriminator or the generator when the channel
H is unknown or uncontrolled a priori. This situation prevents
the overall learning of the end-to-end dynamics. Hence, the
channel transfer function for example can be assumed and
injected as a bias signal, but any such assumption would
prejudice the learned weights, repeating the pitfalls caused by
the discrepancy between the assumed and the actual channel
like in simulators. In any case, in real wireless systems, an
accurate estimate of h(t) is usually hard to obtain in advance
and it can not be expressed analytically. As a result, it is
desirable to develop this channel agnostic trafﬁc modeling tool
of for pseudo-radio-signal synthesis, where different types of
sandboxed waveforms and colored effects can be automatically
learned without using analytical models.

In its essence,

learning directly from complex systems
with high degrees of freedom, such as radio hardware and
wireless links, is generally troublesome [11]. Moreover, DL
its
for wireless is a new ﬁeld, and little is known about
optimal training strategies. In the rest of the paper, we aim
to outline and experimentally solve some of the challenges
(i.e. parameter tuning) in context of our GAN-based pseudo-
radio-signal synthesis method.

IV. PSEUDO-RADIO-SIGNAL SYNTHESIS METHOD

that

We recall

the synthesis of a signal based on the
prototype x(t) yields a pseudo-radio-signal x(cid:48)(t) of duration
Tgen > Tprototype. The GAN models prove to be lightweight (in
order of few MBytes) and combine the prototyped waveforms,
channel and hardware combined effects. This means that the
analysis and engineering of wireless interference signals would
only require information about the transmitter and the SDR
hardware disposition in a known RF medium during model
training. Systematically, this solves the storage problem by
building, for different radio scenario compositions, a catalogue
of agile models instead of raw signal recordings. The method
will necessitate DSP and data science practices [15] invoking
conﬁdence metrics and performance testing on the model. For
these reasons, a crucial model validation step is necessary
before transmitting x(cid:48)(t) over-the-air. The general concept is
illustrated in Fig. 3.

5

Fig. 3. Block diagram staging the pseudo-radio-signal generation method

On the other hand, with respect to the no-free-lunch theo-
rem, few expert piloting is made in training and testing of the
model. This includes:

•

•

•

the GAN model architecture,
the prototype input data structure and pre-processing,
The introduction of a signal-to-noise (SNR) variable in
the latent prior pz statistics,
regularizations and optimizations.

•
In the following, Section IV-A presents the framework and
the data structure of the radio signals, following that, Section
IV-B explains the normalization procedure prior to inputting
the prototype into the model. Section IV-C explores the model
training step for GAN and Section IV-D exposes the necessary
DSP for the synthesis of the new signal. Finally, Section IV-E
details how we choose to validate the generated signal.

A. Data structure

Real wireless signals are captured and transmitted in a
controlled environment (i.e. RF anechoic chamber) using a
Universal Software Radio Peripheral (USRP) [21] hardware
in the SDR platform. The necessary DSP is implemented with
Python [22] and the DNN architectures are built using the
Keras [23] library.

With an USRP gain equal to GRX, the energy over time of
any radio signal centred at fc with a sampling bandwidth Rs
is stored for the duration TRX in a vector array s[n] having
a number of samples Ns holding the temporal in-phase and
quadrature (I/Q) data components an = In + jQn ∈ C in a
numpy.complex64 format and canonicalized as

s[n] = an ∈ ∆ | ∆ = {C : |an| = 1}; n = 0 . . . Ns − 1;
Ns = (cid:98)TRX · Rs(cid:99) ∈ N+.

(10)
This I/Q complex-valued representation of the signal
is
commonly used in communications and is signiﬁcant for many
signal processing algorithms that rely on phase rotations,
complex conjugates, absolute values, etc. For this reason, it
would be desirable to have artiﬁcial neural networks operate
on complex rather than real numbers. However, for several
reasons, none of the available ML libraries, including Keras,
supports this at the time of writing this paper. First, it is pos-
sible to represent all mathematical operations in the complex
domain with purely real-valued neural network of twice the
size, i.e., each complex number is simply represented by two
real values. Second, a complication arises in complex-valued
neural networks since traditional loss and activation functions
are generally not holomorphic so that their gradient is not
deﬁned. A solution to this is Wirtinger calculus [18]. Although
complex-valued neural networks might be easier to train and
consume less memory, no research to this date has provided

THs(t)x(t)SDRpseudo-radio-signalsynthesismodeltrainingnewsignalgenerationmodelvalidationXx0(t)any signiﬁcant advantage in terms of expressive power. For this
reason, given a prototype x(t) of duration Tprototype, its digital
temporal representation must be reshaped in order to be used
in the Keras environment outside of the USRP ecosystem. We
deﬁne the tensor notation for a given prototype by

T x = c[d1,d2,d3,d4] ∈ RN x
f ×

N x

p ×

DIMIQ

NFFT.

×

(11)

This 4D tensor is the input structure of the prototype for the
proposed GAN implementation. DIMIQ = 2 seperates the I
and Q data components which downgrades the data struc-
ture of c from numpy.complex64 to numpy.float32.
However, automatic differentiation environments for double
valued neural networks are also not sufﬁciently mature, for
the same previously stated reason that a double convolutional
layer can obtain much of the beneﬁt within this representation.
consequently, it is believed that it is sufﬁcient for the time
being to use one I or Q component at a time. A number
Nf > 0 is used to construct different frames of the prototype in
order to easily separate the training and the testing set as well
as to provide diversity in the pseudo-radio-signal synthesis
process discussed in Section IV-D. Each frame is composed
of “packets” of a sequence of NFFT > 0 captured c samples.
The number of packets is equal to

N x

p =

(cid:23)

(cid:22) Ns
N x

f NFFT

∈ N+.

(12)

B. Normalization

Data normalization is an important step prior to any ML
application [15]. Data values υ of each element c of the
prototype tensor T x are scaled to unit energy for each frame
envelop ψ using the linear transformation

T x
[d1=ψ,d4=υ] ←

T x
[d4=υ]
Pψ

;

(13)

Pψ =

1
N x
p

Np
1
−
(cid:88)

NFFT

(cid:88)
−

1

p=0

υ=0

d[d1=ψ,d2=p,d4=υ];

(14)

where Pψ designates the average power of the frame ψ. This
destroys any residual features which are simply real world
artifacts possibly due to hardware (USRP or T ) imperfections.
Normalization also makes training of the GAN less sensitive to
the scale of features and improves analysis and comparison of
multiple models. In fact, normalization makes the data better
conditioned for convergence because in case of massive vari-
ances, optimization can stagnate. This also allows to initialize
the gradients (∇θD , ∇θG ) to null values prior to training and
keeps the weights (θD, θG) values in (−1, 1).

C. Model training

To minimize the loss functions LD(D) and LG(G) ex-
pressed respectively in the equations (5) and (8), the feed-
forward back-propagation algorithm [24] with mini-batch [15]
is used. Feed-forward is the algorithm that calculates the out-
put vector from the input vector of each layer of a DNN. Back-
propagation is the algorithm used to stochastically update
the weight parameters (θD, θG) using the respective gradients

6

(∇θD , ∇θG) according to an optimizer. Fig. 4 illustrates this
within the pseudo-radio-signal synthesis framework.

The chained-GAN has two modes: the training mode (for
updating the weights (∇θD , ∇θG)) and the generation mode
(for pseudo-radio-signal synthesis). In the generation mode
only feed-forward for G is used, while in the training mode,
both algorithms are used.

In the training mode, D and G are trained iteratively on the
the training set which is one (d3) DIMIQ dimension of one
(d1) frame with Nexamples < N x
f packets from T x and Nexamples
packets from G(z; θG). Both example entries are randomly fed
to the discriminator (with the corresponding binary classiﬁca-
tion labels) a number of Nepoch iteration times. The tensor data
values are fed by chunks of size Smini-batch < NFFT in order
to parallelise and optimize the learning process. Embedded
updates on G(z, θG) happen with the feed-back sent from the
discriminator as highlighted in Fig. 4.

Initialization of the two DNN weights has no theoretical
foundations for GAN at the time of writing this work. For
both the discriminator and the generator, we chose to uti-
lize the practical technique of Xavier initialization [25]. The
discriminator’s weights are initialized in an earlier number
of epochs Nepoch-pretrain for pre-training. In principle, with a
sufﬁcient stop criterion Nepoch, the generator is observed to be
able to approximate dynamics of a prototyped input in terms
of packets. The output pseudo-radio-packets are tested in the
validation step explained in Section IV-E.

The z values are sampled from pz(z) which can be a
uniform distribution or a Gaussian distribution. The choice
of the pz(z) density is critical and it is an active research
topic [26] in the DL community. In our work, we propose
to use a Gaussian distribution for the latent noise with mean
µ and variance σ2, i.e. z ∼ N (µ, σ2). Consider the training
step given the prototyped frame ψ. The proposed z introduces
favorable gradient propagation at the generator weights to
simulate wireless effects: with a noise power level σ2 cor-
responding to a signal-to-noise (SNR) ratio γdB of the known
average power Pψ w.r.t. the noise z.

With sufﬁcient training capacities, using Algorithm 1, and
for an adequate SNR value γdB at each epoch, we observe that
the latent noise z ∼ N (µ, Λ(P, γdB)) can virtualize additive
channel effects like noise and co-channel and adjacent channel
interferences existent in the prototyped waveform.

For robust feature extraction, we highlight that the mini-
batch size during training Sbatch should be few-orders bigger
than the smallest kernel or dense layer dimensions utilized
in the chained-GAN and at the same time few-orders smaller
than the packet size, i.e. min(Sk, ζD, ζG) (cid:28) Sbatch (cid:28) NFFT.

Algorithm 1 Pseudo-code to compute latent Gaussian noise
variance from virtual SNR quantity.

input: Signal power level P and SNR ratio γdB
output: Noise power level σ2
function Λ (P, γdB):
PdB = 10 × log10(P)
noisedB = PdB − γdB
σdB
σ2 = 10
10

7

Fig. 4. High level illustration of GAN model training for pseudo-radio-signal synthesis

It

is clearly desirable that

In practical wireless communication environments, this would
be useful for radio channel feature learning over a variety of
stochastic effects and also for pseudo-radio-signals synthesis.
is not obvious, however, at which SNRs the mini-
batches should be trained. It
the
GAN should operate at any SNR or SNR-range. However,
previous research argues that this is generally not the case.
In [5], training at low SNR did not allow for the discovery
of the wanted structure important in higher SNR scenarios.
Also, training across a wide range of SNR severely affects
training time. On another hand, authors of [27] have observed
that starting off the training at high SNR and then gradually
lowering it with each epoch led to signiﬁcant performance
improvements for their DL application in astrophysics. In
our proposed solution, for each prototype X, and enabled by
normalization, the µ value for the latent Gaussian distribution
is set to 0 and σ2 = Λ(P, γx
dB) is chosen at each epoch with
dB ∈ Γx = [γx
γx
For a ﬁxed scenario and a ﬁxed USRP receive gain GRX
(i.e. in a controlled environment), results have heuristically
shown that, with convenient and thorough regularizations (to
avoid overﬁtting and underﬁtting) on the GAN architecture,
dB from Γx
it is best-practice to randomly pick SNR values γx
uniformally at each epoch. This way, G(z, θG) builds a prob-
abilistic generative model pmodel(x) at the end of the training
mode. The reasoning behind this is that the GAN learns more
radio dynamics and channel effects for different noise level
variances at each epoch. This can be viewed as a special
kind of regularization itself: since in training mode, picking
different SNRs can be extended to other operational situations.
For example, with the same GAN architecture, deriving SNR-
range proﬁles Γx, based on other digitally simulatable radio
parameters, enables the building generative models for more
complex trafﬁc patterns, e.g., including transmitter mobility or
a harsher RF propagation environment.

dB,min . . . γx

dB,max].

D. New signal synthesis

At stop criterion, in the generation mode, associated neural
layer of the generator are able to
holding

outputs of the output
construct a current tensor output GNepoch (T z) = T x(cid:48)

a frame of pseudo-radio-packets of size NFFT generated using
the eventually learned and detected patterns of the input. G is
able to generate any arbitrary number Ngen of these packets.
Note that the frames of the prototype have a power statistic
P. Therefore, randomizing the frame identiﬁer d1 (Nexamples
times) in a range less than Nf diversiﬁes some generated
packets. This allows us to build the test-set after rescaling
these output frames (from G) for denormalization.

×

Doing this process in both DIMIQ dimension gives N I
gen
and N Q
gen pseudo-sequences. These are assembled in the I/Q
format in a 2D matrix Ax(cid:48)
NFFT. As we show
[p,τ ] ∈ ∆Ngen
next, visualizing this pseudo-radio-packets matrix Ax(cid:48)
reveals
serious phase shifts because the NFFT packet size is not
necessarily chosen using any a priori information about the
trafﬁc. This means that
the packet boundaries can reﬂect
desynchronizations between the USRP and the transmitter
learned as features during training. Such phenomena can
be viewed as unfavorable co-channel and adjacent channel
interference. This can be overcome, using the Grifﬁn-Lim [28]
method commonly used in audio reconstruction, or by adding
bias on the GAN while training using the same Grifﬁn-Lim
method [13]. It is also possible to train a separate GAN to
learn the phase patterns, and any combination between GANs
and the Grifﬁn-Lim method is attractive for future research.

Herein, reconstruction is made using a discrete circular
with a

convolution ((cid:13)∗ ) on the time domain matrix Ax(cid:48)
discrete raised-cosine (RC) ﬁlter hRC[n], i.e.:

x(cid:48)[τ ] = Ax(cid:48)

[p,τ ] (cid:13)∗ NFFT hRC[τ ].

(15)

hRC[τ, L, β] =

1 + cos






1,
(cid:104)

1
2

0,

(cid:16) πL

β (|τ | − 1

β
2L )
−

β
−
2L

|τ | ≤ 1
(cid:17) (cid:105)
,

1

β

−

2L < |τ | ≤ 1+β
otherwise.

2L

Filtering is done between consequent packets using an
overlap-save technique with padding [29]. The chosen ﬁlter

(16)

length is L = 129 with the roll-off β equal to 0.25. Other
values give other boundary reconstruction qualities.

E. Model validation

Given that the prototype is totally arbitrary, it is necessary
to choose a sandbox set of ﬁxed inputs and assess the GAN
performance on them as proof of concept. This is called model
validation, and is done to evaluate the spectral and temporal
radio properties of the synthesized pseudo-radio-signal. For
this we use the same frames utilized in the training mode to
construct the validation-set in the temporal domain.

Discrete Fourier transform using numpy.fft.fft [30] is
applied on the prototype’s 2D temporal representation Ax, to
yield a matrix Bx = b[ν,p] ∈ ∆NFFT
Np holding frequency
samples bm with the ﬁnite-precision duration NFFT. For sake
of generality, and to insure that the GAN learns relevant fre-
quency dynamics while keeping diversity, the spectral notation
of the prototype Bx is compared to the output of the generator
for an equal number of packets. The goal is to test the GAN’s
ability to learns qualitative spectral features.

×

On another hand, The GAN temporal output will be quan-
titatively compared to the prototype using their probabilistic
density functions (PDF) Fpmodel and Fpprototype respectively. This
is a classic approach for comparing the time varying behaviors
of signals. Alternatively, the dependency on the time-series
samples of the prototype can be addressed using recurrent
neural network(s) sequence modeling. This topic is interesting
but is out of our research scope here.

V. PROPOSED GAN MODEL

In this section we introduce our proposed GAN model
and its implementation using the previously presented tensor
data structure. The model includes optimization techniques
for computational efﬁciency in terms of training time and
memory occupation, as well as regularization [15] techniques
to avoid situations where the generator’s output is equivalent
to a random noise (underﬁtting) and to avoid reusing the same
pseudo-radio-packet in synthesis (overﬁtting).

A. Loss functions and optimizers implementations

Inside Keras, different

loss functions can be used for
adversarial function approximation problems, which lead to
different variants of GANs. Herein, we choose to continue
with the simplest approach as advised in the litterature [15].
We implement the loss functions elaborated in Section III.

At the time of writing this paper, little is known about hypo-
thetical GAN parametrization and training practices, however
a literature of heuristics exists [31]. The adaptive moment
estimator (Adam) [32] is chosen as the optimizer [33] for
both the discriminator and the generator as it gives the best
simulated validation results. The same choice manifests in
other (DL and) GAN applications and variants such as deep
convolutional generative adversarial networks [34]. It is worth
to stress that this depends on the application and other degrees
of freedom. For example, the original GAN paper [14] used
a momentum optimizer [33]. The same philosophy applies to
the rest of the hyper-parameters.

8

We detail

in the sequel

the tuned architectures of the
proposed DNN models and their respective hyper-parameters
veriﬁed according to the model validation methodology de-
scribed in Section IV-E.

B. The generative DNN model

Table I and table II both summarize the hyper-parameters
of the fully connected DNN of the generative model G(z; θG).

TABLE I
LAYERS OF THE GENERATIVE DNN MODEL

Layer type Parameters

Description

#

L0

Input
layer

SL0 = NFFT

a1(·) = tanh(·)

a2(.) = tanh(.)
λg = 0.001

A tensor holding a packet of
size NFFT z values sampled from
pz(z)
Fully connected layer with a
non-linear
function
activation
a1(·) = tanh(·) → (−1; 1)
applied on L0’s weighted output.
Fully connected dense layer with
L1’s weighted output with weight
decay.

L1 Non-
linear
dense
layer
L2 Non-
linear
dense
layer w/
weight
decay
L3 Output

layer

SL0 = NFFT

A tensor holding a packet of
size NFFT with values equal to
G(z, θG)

TABLE II
GENERATIVE MODEL HYPER-PARAMETERS

Hyper-parameter

Value

Stochastic optimizer of the gener-
ator
Progressive learning rate
Dense layers dimension

Adam (ηG, β1 = 0.9, β2 = 0.999)

ηG = 0.011
ζG = 128

According to [10] and [15], normalization is a key step
enabling the generative model to converge using the non-linear
activation function tanh, usually used in the generator’s ﬁrst
and last layers. It is also worth noting that using dropout
layers in the generative model would cause underﬁtting in the
resulted pseudo-radio-packets.

For each prototype, the tensor notation allows the chained-
GAN model to train using 2 × Nexamples frames of size Np
of packets of size NFFT. Half of these packets come from the
generator’s output layer and the other half comes from the
prototype training frame as explained in Section IV-C.

C. The discriminative DNN model

The discriminative model D(·; θD) is a fully connected
DNN binary classiﬁer with the tuned hyper-parameters pre-
sented in both table III and table IV.

Some individual packets from the prototype can include
negative clipping effects caused either by the USRP’s analog-
to-digital converters or sharing the channel. These can escape
the normalization constraints and can cause un-helpful gradi-
ents at some epochs. This manifests as sequences with very
low scalar data values compared to previous epochs. Moreover,

TABLE III
LAYERS OF THE DISCRIMINATIVE DNN MODEL

TABLE IV
DISCIRIMINATIVE MODEL HYPER-PARAMETERS

Layer type Parameters

Description

Hyper-parameter

Value

9

#

L0

L1

Input
layer

1D con-
volution
layer

SL0 = NFFT

Nk = 32
Sk = 128

L2 Non-
linear
dense
layer

a2(·) = ReLu(·)
Linear
Rectiﬁed
Unit

L3 Dropout

δD = 0.5

layer

L4

Flatten
layer

L5 Dense

λD = 0.0001

layer w/
weight
decay

L6 Dropout

δD = 0.5

layer

L7 Dense

λD = 0.0001

layer w/
weight
decay

a

reason

L1’s weighted

connected
non-linear

Normalized input
(packets) of
size SL0 from either the proto-
type TX or G(z; θG).
Nk ﬁlters (kernels) of size Sk
producing Sk features from L0.
Output of L1 is a 5D tensor with
a dimension of size Sk holding
weights for each ﬁlter.
layer with
Fully
a
activation
function a2(·) = ReLU(·) =
max(0, ·) → [0; 1] applied
output.
on
(max(·)=1
of
by
normalization.)
Is
regularization technique
where weight connections be-
tween L2 and L3 are randomly
dropped out with a probability of
δD to ensure the same feature is
learned with different connection
architectures during training [15].
Consecutively joins L3 weights
(separated by L1) in a 4D tensor.
Fully connected layer (with a lin-
ear activation function a5(·) =
·) connecting L4’s weighted out-
put with rigid penalization on its
output weights done by adding
a factor equal to λD||W||2
2 to
the loss function. This is called
the weight decay regularization
technique and it is useful to avoid
overﬁtting for unbiased classi-
ﬁcations over large neural net-
works [15].
Dropout layer on L5.

connected

Fully
weight decay on L6.

layer with

L8 Dropout

δD = 0.5

Dropout layer on L7.

layer

L9 Dense

layer

Dense layer with with L8.

L10 Dropout

δD = 0.5

Dropout layer on L9.

layer
L11 Non-
linear
dense
layer

a11(.) =
sof tmax(.)

L12 Output

SL12 = 2

layer

Fully connected layer with a
non-linear
function
activation
a11(·) = softmax(·) =
e(zi)
j exp (zj ) → (0; 1) applied

(cid:80)
on L10’s weighted output.
Vector containing probabilities of
the input packet belonging to
each class, that is, probability of
i) belonging to the prototype T x,
and probability of ii) being gen-
erated by G.

this situation can cause saturation during training especially if
the the generator is outputting large pseudo-radio-packets at
the time. In this game, the discriminator (which is a classiﬁer)
tends to linearly extrapolate and produce extremely conﬁdent
predictions that cause the saturation. If pprototype(x) → 0 and
pG(x) is large, erroneous samples from the current pG(x) then

Stochastic optimizer of the dis-
criminator
Progressive learning rate
Binary classiﬁcation labels

Dense layers dimension
Nepoch-pretrain
Smini-batch-pretrain

Adam (ηD, β1 = 0.9, β2 = 0.999)

ηD = 0.0001
1 − α for the prototype examples.
α = 0.2
0 for the generator’s output.
ζD = 32
1
32

to

To

than

rather

encourage

discriminator

estimate
to

have no incentive to approximate the data.
the

to
extrapolate

soft
extremely
probabilities
conﬁdent classiﬁcations,
the regularization technique of
one-sided label smoothing [31] is used. The parameter α is
the smoothing parameter. It is important to smooth only the
prototyped data [17], which explains the term “one-sided”.
In fact, non-zero values for generated data coinciding with a
large pG(x) and a small pprototype(x) would reinforce incorrect
behavior in the generator. This would train G either to
produce samples that resemble the data or to produce samples
that resemble the sequences it already makes.

D. The chained-GAN model

The constants in table V construct

the set of hyper-
parameters for the chained-GAN model (see Fig. 4) which
subsequently determine the number of overall trainable param-
eters (θG, θD) and ﬁnal model size. These values are chosen
for big enough frames to show validation results in a timely
manner using TensorFlow’s [35] central processing unit (CPU)
backend for various prototypes.

TABLE V
CHAINED-GAN MODEL HYPER-PARAMETERS

Hyper-parameter

Value

Nexamples
NFFT
Np

Ngen
Nepoch
Sbatch

128
2048
∼ 400: kept as constant for different prototypes
and chosen w.r.t. dividing the smallest validation
prototype to a number of frames Nf = 2
Np × 20
1000
300

So far, the stop criterion is the number of epochs Nepoch,
which theoretically gives better results the more the chained-
GAN trains. However, given the limited amount of prototyped
data (due to wideband signal recording), longer training can
conducive to overﬁtting. For this reason, the training losses
and the discriminator’s accuracy [23] are manually monitored
while tuning the model. The monitoring of these single valued
metrics resulted in valuable preliminary observations in this
application. Advanced early stopping [15] based on optimal
model weights is encouraged to be investigated in the future.

VI. NUMERICAL RESULTS

In this section, we present training and validation numerical
results for our method illustrated in Fig. 3. Additionally, we
discuss insight and remarks.

The following example is based on a real signal processed
via a USRP X300 [21] SDR for prototyping and transmis-
sion of pseudo-radio-signals. The selected prototype x[n] is
a wireless signal recording of duration TRX = 3 s and an
Rs = 200 MHz sampling band centred at fc = 5.25 GHz and
captured with a receive gain GRX = 31.5 dB in a controlled
environment. This is the unlicensed WiFi channel 50 where
we generate the trafﬁc at maximum benchmark transfer rate
using an IEEE 802.11ax capable transmitter T at a distance
equal to 4 m from the USRP.

A. Training of the proposed model

During training, the mean success rate across all predic-
tions for binary cross-entropy problems can be referred to
as accuracy inside of Keras. The optimal accuracy for the
discriminator for each training process is equal
to 0.5 as
explained in Section III-A. If the accuracy is more than
0.5 then the GAN output is exposed to overﬁtting; if the
accuracy is less than 0.5, then the GAN output is exposed
to underﬁtting.

Fig. 5 jointly shows the discriminative loss (in red), the
generative loss (in blue) and the discriminator’s accuracy (in
yellow), during an 8 mn : 38 s training of Nepochs = 1000 on
T x with a decibel SNR-range Γx = [−30 · · · − 24].

After trial with different types of prototype signals, the most
important notes for the GAN training mode, that happen to be
common in most GAN applications are the following:

•

•

The chained model’s architecture and hyper-parameters
are evidently key for training convergence. In this ex-
ample, and for the mentioned duration of training, the
average discriminative accuracy is 0.5898, which implies
chances of overﬁtting. The accuracy metric is important,
because training can continue even if the accuracy is
saturated, which implies that the model is defective. In
such a case, divergence of the chained model is inevitable,
and saturation of the accuracy is an indicator that Nepochs
must be reduced.
A model with losses converging very fast
to zero is
defective. A model with discriminative losses having

Fig. 5. Training metrics evolution on the prototype

10

•

huge variances and spikes is defective. A model with
generative losses steadily increasing is defective. These
unbalances obviously indicate underﬁtting or overﬁtting.
The loss function of the discriminator and the loss
function of the generator are opposite (adversarial) and
the amount of times each model gets better relatively to
the other is interesting to monitor to compare different
chained-GAN architectures. High absolute peaks in the
relative losses imply changes in the SNR value and sub-
sequently allowing the GAN to learn new radio patterns.

B. Validation of the proposed model

Let us ﬁrst evaluate the spectral dynamics, i.e. B. Fig. 6
shows visualization of Ngen packets samples from T x. Fig. 7
shows visualization of Ngen pseudo-radio-packets sampled
from GNepoch (T z) = T x(cid:48)
of the proposed GAN model in its
generation mode.

The generator’s output (Fig. 7) is colored and is different
from a Gaussian noise. This indicates that G has the learned
qualitative degrees of the spectral dynamics (circled in green
in Fig. 6) of the prototyped signal. The output quality is linked
to the model architecture and hyper-parameters. For different
regularization techniques, table VI shows performance com-
parison using the same prototype and Nepochs. In particular,
it is worth to note that the most inﬂuential regularization
technique is one-sided label smoothing, which allowed to learn
the magnitude peaks efﬁciently. Pay-off for different values of
α is an open quest for better performance.

TABLE VI
PERFORMANCE COMPARISION USING DIFFERENT REGULARIZATION
TECHNIQUES

Regularization

Average D accuracy

Runtime

No regularization
Dropout
Weight decay
One-sided label smoothing

0.1822
0.3001
0.6652
0.7699

22 mn : 30 s
10 mn : 4 s
32 mn : 40 s
26 mn : 10 s

Moreover, pprototype is dubbed multi-modal, meaning that
many modes (of selectable packets) are possible. For non-
convex games, this can cause the gradients to not converge
using gradient descent optimizers (see Section V-A). This is
because, intuitively, the longer the training, the harder it is
for the DNN models to converge into describing all modes
(or radio dynamics). Mathematically, this makes it harder for
the chained model to converge using the cross-entropy loss
functions to describe the adversarial game because of the non-
symmetry of the Kullback-Leibler divergence (which drops to
0 for areas where pprototype(x) → 0). As discussed in Section
III,
this makes some gradients vanish during propagation
as we discussed. Such ﬂat gradients are common issue for
GAN known as mode-collapse [36] which states that: there is
inherently no motivation for the generator to produce a diverse
set of samples because the discriminator only penalizes for
producing bad samples. Therefore, it easier for the generator
to learn a few modes than all modes for a multi-modal
distribution as we see in Fig. 7.

Moving on to the temporal dynamics, i.e. A, Fig. 8 shows
the PDFs of the previously presented signals and the quantita-
tive time-domain performance. In particular, Fig. 9 shows two
generated pseudo-radio-packets and two randomly sampled
packets of length NFFT = 2048.

11

Fig. 8. PDFs of signals from both the prototype and the generator’s output

Fig. 6. Visualization of spectral features of a sampled signal from prototype

Fig. 9. Two packets from both the prototype and the generator’s output

Fig. 7. Visualization of spectral features of a generated signal by the generator

Classically, working with RF signals numerically trans-
formed to stochastic time-series has always brought
into
attention the time-slotted property of the signal; meaning that
the signal’s variance (and basically it’s power) depends on
the time of measurement. Some simulation models assume
all disturbances to be stationary while others are built around
interference is not stationary. For
the speciﬁc notion that
the GAN to model complex trafﬁc models (especially with

a transmitter in movement, more dense environment, fading,
etc), the generator would ﬁnd it difﬁcult to map the trafﬁc with
a ﬁxed NFFT. If NFFT is a trainable degree of freedom, the
generator could potentially learn the radio channel response
and synchronize with the actual prototype data symbols. In this
case, cyclic stationarity becomes important in training. This
can be ﬁxed with a cyclic correlation logic [1]. Namely, if the
training size Np × NFFT is close to some cyclic window of the
trafﬁc, GAN would converge faster. This could be addressed
using recurrent neural network(s) sequence modeling as a

dependency on the time-series samples of the prototype or
using another GAN to help ﬁnd an adequate NFFT. This bias
can then be added to the ﬁrst GAN each time it is necessary.
Alternatively, for a ﬁxed NFFT, a lower DNN dimensions ζD
and ζG can be used alongside a higher training time.

VII. CONCLUSION

In this work, we proposed a DL-based channel agnostic
lightweight trafﬁc modeling tool. Different types of trafﬁc
patterns and channel effects can be automatically learned
without knowing details about the prototyped trafﬁc or the
channel transfer function.

By using an adversarial approach, we showed that it is
possible to learn function approximations for arbitrary over-
the-air communication trafﬁcs with colored characteristics
such as trafﬁc waveform, noise and channel interferences.
For our pseudo-radio-signal synthesis method, we proposed a
GAN model featuring an implicit regularization technique to
detect spectral and temporal dynamics of the prototype around
a range of virtual SNR(s).

GANs and ML in general, come with the promise of
potential gains over existent algorithms for solving non-linear
layer. While enhancements are
problems for the physical
yet possible for the proposed model in terms of improving
stability and performance, we have, nonetheless illustrated
that such a DL approach, can, even with a relatively simple
architecture, obtain reasonable trafﬁc approximations without
the introduction of assumptions about the effects occurring or
the simpliﬁcation to a parametric model.

ACKNOWLEDGMENTS

This work is partially supported by the Luxembourg Na-
tional Research Fund (FNR) under the frame of the research
project (BRIDGES19/IS/13778945/DISBuS).

REFERENCES

[1] D. Tse, Fundamentals of wireless communication. Cambridge Univer-

sity Press, 2005.

[2] T. O’Shea, J. Corgan, and C. Clancy, “Unsupervised representation
learning of structured radio communication signals,” 1st International
Workshop: Sensing, Processing and Learning for Intelligent Machines,
Jul. 2016.

[3] “GNURadio documentation,” [Online] : gnuradio.org\docs.
[4] N. Rupasinghe and L. Babun, “Recording, processing and playback of
LTE signals using Universal Software Radio Peripheral devices,” De-
partment of Electrical and Computer Engineering, Florida International
University, 2014.

[5] T. O’Shea and J. Hoydis, “An introduction to Deep Learning for the
Physical Layer,” IEEE Transactions on Cognitive Communications and
Networking, Volume 3, Issue 4, Dec. 2017.

[6] M. Amini and E. Balarastaghi, “Universal neural network demodulator
for Software Deﬁned Radio,” International Journal of Machine Learning
and Computing, Volume 1, No. 3, Aug. 2011.

¨Onder, A. Akan, and H. Do˘gan, “Neural network based receiver
design for Software Deﬁned Radio over unknown channels,” 8th Inter-
national Conference on Electrical and Electronics Engineering, Nov.
2013.

[7] M.

[8] T. O’Shea and N. West, “Radio Machine Learning dataset generation
with GNU Radio,” Proceedings of the 6th GNU Radio Conference
Volume 1, No 1, Sep. 2016.

[9] T. O’Shea, T. Roy, and N. West, “Approximating the void: learning
stochastic channel models from observation with variational Generative
Adversarial Networks,” International Conference on Computing, Net-
working and Communications (ICNC), Aug. 2018.

12

[10] H. Ye, G. Y. Li, B.-H. F. Juang, and K. Sivanesan, “Channel agnostic
end-to-end learning based communication systems with Conditional
GAN,” IEEE Globecom Workshops, Jul. 2018.

[11] T. O’Shea, T. Roy, N. West, and B. Hilburn, “Physical Layer commu-
nications system design over-the-air using adversarial networks,” 26th
European Signal Processing Conference (EUSIPCO), Mar. 2018.
[12] Y. Shi, K. Davaslioglu, and Y. E. Sagduyu, “Generative adversarial
network for wireless signal spooﬁng,” Association for Computing Ma-
chinery (ACM), WiseML, p. 55–60, May 2019.

[13] K. Oyamada, H. Kameoka, T. Kaneko, K. Tanaka, N. Hojo, and
H. Ando, “Generative adversarial network-based approach to signal
reconstruction from magnitude spectrograms,” University of Tsukuba
and NTT Communication Science Laboratories, Japan, Apr. 2018.
[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative Adversarial Net-
works,” NIPS Proceedings of
the 27th International Conference on
Neural Information Processing Systems - Volume 2, 2014.

[15] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. The MIT

press, 2015.

[16] O. Simeone, “A Very Brief Introduction to Machine Learning With Ap-
plications to Communication Systems,” King’s College London, United
Kingdom. Funded by the European Union Horizon 2020 research and
innovation program, Aug. 2018.

[17] I. Goodfellow, “Tutorial: Generative Adversarial Networks,” NIPS’2016
(Conference on Neural Information Processing Systems), 2016.
[18] T. O’Shea and J. Hoydis, “An Introduction to Machine Learning

Communications Systems,” arXiv, 1702.0832, Feb. 2017.

[19] K. Oyamada, H. Kameoka, T. Kaneko, K. Tanaka, N. Hojo, and H. Ando,
“Deep Learning in Mobile and Wireless Networking: A Survey,” Univer-
sity of Tsukuba and NTT Communication Science Laboratories, Japan,
Apr. 2018.

[20] “OpenAI blog on generative models,” [Online] : openai.com/blog/

generative-models.

[21] “National Instruments™, Ettus Research™ USRP X300 SDR,” [Online]

: ettus.com/all-products/X300-KIT.

[22] “UHD™ Python API,” [Online] : kb.ettus.com/UHD Python API.
[23] “Keras, the open-source neural-network library,” [Online] : keras.io.
[24] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski, “Deep
generative stochastic networks trainable by backprop,” International
Conference on Machine Learning (ICML), May. 2014.

[25] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” 13th International Conference on
Artiﬁcial Intelligence and Statistics, PMLR 9:249-256, Jan. 2010.
[26] T. White, “Sampling Generative Networks,” International Conference

on Learning Representations, Apr. 2017.

[27] D. George and E. A. Huerta, “Deep Neural Networks to Enable Real-
time Multimessenger Astrophysics,” 2018 American Physical Society,
Oct. 2017.

[28] D. Grifﬁn and J. Lim, “Signal Estimation from Modiﬁed Short-Time
Fourier Transform,” IEEE Transactions on Acoustics Speech and Signal
Processing (ASSP), Volume 32, No. 2, pp. 236–243, Jan. 2015.

[29] L. Rabiner and B. Gold, Theory and Application of Digital Signal

Processing. Prentice-Hall, 1975.

[30] “NumPy,

the Python package for scientiﬁc computing,” [Online] :

numpy.org.

[31] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
and X. Chen, “Improved Techniques for Training GANs,” NIPS’16
Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 2234-2242, Jun. 2016.

[32] D. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
3rd International Conference for Learning Representations, San Diego,
Apr. 1984.

[33] S. Ruder, “An overview of gradient descent optimization algorithms,”
Insight Centre for Data Analytics, NUI Galway Aylien Ltd., Dublin, Jun.
2017.

[34] A. Radford, L. Metz, and S. Chintala, “Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks,”
International Conference on Learning Representations’16, Jan. 2016.

[35] “TensorFlow, the open-source software library for dataﬂow and differ-

entiable programming,” [Online] : tensorﬂow.org.

[36] Bhagyashree, V. Kushwaha, and G. C. Nandi, “Study of prevention of
mode collapse in generative adversarial network,” in 2020 IEEE 4th
Conference on Information Communication Technology (CICT), 2020,
pp. 1–6.

13

Haythem Chaker received the State Engineer-
ing degree (Dipl.-Ing.) in Computer Networks and
Telecommunications from the National Institute of
Applied Sciences and Technology (INSAT), Tunisia
in 2019 and the M.Res. in Information Processing
and Complexity of Living Systems (TICV) from
the National Engineering School of Tunis (ENIT),
Tunisia, in co-graduation with the M.Sc. in Math-
ematics and Computer Science from Paris Univer-
sity, France, in the same year. In 2020, he joined
the Signal Processing and Satellite Communications
(SIGCOM) research group at SnT, University of Luxembourg as a Ph.D.
candidate. His research interests are in wireless systems prototyping with
focus on dynamic beamforming design and optimization through the use of
new digital signal processing techniques and hardware demonstrations.

Soumaya Hamouda received the Engineering de-
gree in electrical engineering and the DEA (M.Sc.)
degree in communication systems from the National
Engineering School of Tunis (ENIT), Tunisia, in
1998 and 2000, respectively, and the Ph.D. degree
and the “Habilitation Universitaire” degree in tech-
nologies of information and communication from the
Telecommunications Engineering School of Tunis
(Sup’Com), Tunisia, in 2007 and 2015, respectively.
She is currently an Associate Professor in telecom-
munications, a coordinator of the Professional Mas-
ter in Telecommunication Network Technologies, Faculty of Sciences of
Bizerte (Tunisia), the Head of the National Engineering Studies in Telecom-
munications Committee, and also a member of the Research Laboratory
in radio mobile networks and multimedia (MEDIATRON), Sup’Com. Her
research interests include radio resource allocation, PHY/MAC protocols
and mobility management in wireless networks, 5GNR, mmWaves, IoT in
precision agriculture, and e-Health areas. She participated in several national
and international research projects with INRIA-Rennes (France), ETRI (South
Korea), UAB/CTTC (Spain), and University of Pretoria (South Africa).

Nicola Michailow received the Dr.-Ing. degree
in electrical engineering from Technische Univer-
sit¨at Dresden in 2015, where he researched non-
orthogonal waveforms and ﬂexible numerologies for
the 5G PHY. From 2015 to 2018, he was with
National Instruments, working on SDR-based proto-
typing platforms for 4G, 5G and WiFi. Since 2018,
he is advancing industrial 5G at Siemens Technol-
ogy. His current research interests include integrated
communications and sensing, 6G and embodied AI
systems. He participated in the research projects

5GNOW, CREW, ORCA, IC4F, KICK and Hexa-X.

