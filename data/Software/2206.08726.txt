2
2
0
2

n
u
J

7
1

]
E
S
.
s
c
[

1
v
6
2
7
8
0
.
6
0
2
2
:
v
i
X
r
a

Evaluation of Contrastive Learning with Various Code
Representations for Code Clone Detection

Maksim Zubkov
maksim.zubkov@epfl.ch
Swiss Federal Institute of Technology (EPFL)

Egor Bogomolov
egor.bogomolov@jetbrains.com
JetBrains Research

ABSTRACT

Code clones are pairs of code snippets that implement similar func-
tionality. Clone detection is a fundamental branch of automatic
source code comprehension, having many applications in refac-
toring recommendation, plagiarism detection, and code summa-
rization. A particularly interesting case of clone detection is the
detection of semantic clones, i.e., code snippets that have the same
functionality but significantly differ in implementation. A promis-
ing approach to detecting semantic clones is contrastive learning
(CL), a machine learning paradigm popular in computer vision but
not yet commonly adopted for code processing.

Our work aims to evaluate the most popular CL algorithms
combined with three source code representations on two tasks.
The first task is code clone detection, which we evaluate on the
POJ-104 dataset containing implementations of 104 algorithms. The
second task is plagiarism detection. To evaluate the models on this
task, we introduce CodeTransformator, a tool for transforming
source code. We use it to create a dataset that mimics plagiarised
code based on competitive programming solutions. We trained
nine models for both tasks and compared them with six existing
approaches, including traditional tools and modern pre-trained
neural models. The results of our evaluation show that proposed
models perform diversely in each task, however the performance
of the graph-based models is generally above the others. Among
CL algorithms, SimCLR and SwAV lead to better results, while
Moco is the most robust approach. Our code and trained models
are available at https://doi.org/10.5281/zenodo.6360627, https://doi.
org/10.5281/zenodo.5596345.

KEYWORDS

Source Code Comprehension, Clone Detection, Contrastive Learn-
ing, Source Code Representation

ACM Reference Format:
Maksim Zubkov, Egor Spirin, Egor Bogomolov, and Timofey Bryksin. 2022.
Evaluation of Contrastive Learning with Various Code Representations for

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Egor Spirin
spirin.egor@gmail.com
JetBrains Research

Timofey Bryksin
timofey.bryksin@jetbrains.com
JetBrains Research

Code Clone Detection. In Proceedings of ACM Conference (Conferenceâ€™17).
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION

An open problem in the software engineering (SE) domain is the
building of systems that accurately detect code similarity. Such
systems aim to determine whether code snippets solve a similar (or
equivalent) problem, even if the snippetsâ€™ implementation, language,
program input, or output structure are different. Finding similar
code fragments confidently turns out to be very useful in various
SE tasks, such as refactoring recommendation [4], detection of
duplicates [56], bugs [57], vulnerabilities [61], and cross-language
code search [8, 63].

An important case of code similarity detection is the detection
of code clones, i.e., code snippets with identical functionality. Code
clones are commonly divided into four types [52], with Type IV
being the hardest to detect. Type I-III clones refer to gradually
increasing differences in the implementation (e.g., changed names
of tokens, order of operations, insertion of dead code). Meanwhile,
code fragments that constitute Type IV clones have completely
different implementations, only sharing the functionality. Detection
of such clones is hard as it requires algorithms to capture the codeâ€™s
internal semantics.

Detection of essentially identical objects arises in other domains:
for example, in computer vision (CV) to find images of the same
object or in natural language processing (NLP) to find texts with the
same meaning. Both in these domains and in code clone detection,
researchers commonly employ machine learning (ML) techniques
to identify such objects. In particular, the application of contrastive
learning (CL) approaches is of great interest as in this task it shows
superior results compared to other ML approaches [11â€“13, 20, 46,
58].

Previous works that studied applications of CL approaches for
the code clone detection task focused only on one classical con-
trastive learning method each [26, 59]. Given the recent progress
in the contrastive learning methods in CV and NLP, there is a need
to evaluate modern CL approaches on the clone detection task.
Additionally, an important choice one should make when apply-
ing contrastive learning to code is how to represent code snippets
before passing them to an ML model: as text [18, 26], as an ab-
stract syntax tree [3, 8], as a more complex graph structure [61], or
use a custom representation [62]. Thus, an evaluation should not
only compare CL approaches, but also combinations of contrastive
learning methods and code representations.

 
 
 
 
 
 
Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

With this work, we evaluate combinations of three widely adopted
contrastive learning methods and three code representations on
the code clone detection task. We use two datasets of C/C++ code:
POJ-104 [39] that contains algorithms implemented by different
programmers and a dataset of solutions to competitive program-
ming problems. We also present a tool called CodeTransformator
which we use to augment the latter dataset by transforming code
snippets into functionally equivalent ones. We compare the stud-
ied models with multiple baselines, both ML-based and traditional
(non-ML) token-based ones.

The results we obtain partially agree with the conclusions drawn
in the research done by Han et al. [22]. The evaluation results show
that models utilizing graph representation of code are generally
more accurate than models that treat code as raw text only. Another
interesting finding is that Transformer-based models are much more
sensitive to the choice of hyperparameters than other models we
experiment with. Overall, we develop and make publicly available
a unified, extensible framework for enabling easy side-by-side com-
parison of various encoders and contrastive learning algorithms on
the code clone detection task.

The contributions of this work are:

â€¢ Comparison of multiple contrastive learning approaches,
namely Moco [12], SimCLR [11], and SwAV [9], combined
with three different code representations on two datasets for
the code clone detection task.

â€¢ CodeTransformator, an extensible open-source tool for aug-
menting datasets for the plagiarism detection task by apply-
ing functionality-preserving transformations.

â€¢ An open-source framework that enables comparison of CL
approaches powered by different code representations.

The rest of this paper is organized as follows. Section 2 describes
the concept of contrastive learning and presents CL algorithms
and code representation models that we evaluate in this work.
Section 3 describes the experimental setup: tasks, datasets, and
metrics that we use. In Section 4 we present implementation details
of the framework we developed to compare the models, as well as
model configurations that we use in each experiment. In Section 5
we discuss the evaluation results. Section 6 summarizes the related
studies and compares them to our work. Finally, in Section 7 we
conclude and outline possible directions for the future work.

2 CONTRASTIVE LEARNING APPROACH

Contrastive learning (CL) is a machine learning paradigm used to
train models to identify similar and distinguish dissimilar objects.
CL has demonstrated impressive progress in self-supervised learn-
ing1 for various domains, such as CV, NLP, graph representation
learning, and audio processing [31]. In this paradigm, the model is
trained to produce embeddings (i.e., numerical vectors) for objects
such that similar objects will have close embeddings and different
objects, respectively, will have distinctive ones. Therefore, the idea
of applying this approach to code clone detection task seems nat-
ural. In the end, similar programs will have close embeddings, so
the decision whether two programs are clones or not is determined
by the distance between two vectors.

1A group of machine learning methods that are able to train on an unlabeled dataset.

Driven by the rapid development of deep learning, the number of
contrastive learning algorithms has grown substantially in recent
years. In this work, we focus on the three most successful and
popular CL algorithms that have shown good results in recent
works [31] and explore their core differences.

Figure 1: Illustration of the general contrastive learning
pipeline. ğ¿ is the objective function we aim to optimize dur-
ing training.

Figure 1 shows the common pipeline of a CL algorithm. That is,
given two different objects ğ‘œ and ğ‘œ â€², we pass them into an encoder
model ğ‘“ and obtain their embeddings ğ’’ and ğ’Œ. We further pass these
embeddings into a special projector model ğ‘” which is specific for
each CL algorithm. In a simple case, ğ‘” can be an identical function,
i.e., just pass ğ’’ and ğ’Œ onwards. After the transformation by the
projector, we receive vectors ğ’’â€² and ğ’Œ â€². Based on the transformed
vectors, we compute the loss function (e.g., Noise Contrastive Es-
timation loss [21]) that is optimized during the training stage. As
in other deep learning approaches, we update model parameters
through the backpropagation algorithm to optimize the loss func-
tion.

Thus, for training, we need pairs of two objects: a target object ğ‘œ
and a reference object ğ‘œ â€². During the training process, we alternately
choose ğ‘œ â€² to be either a positive or a negative example, i.e., to be
equivalent or distinct to ğ‘œ, respectively. The choice of both positive
and negative examples is very important to train a robust model.
The more diverse and complex training pairs are, the more robust
model we get after training.

In order to make a prediction with a trained model, we use the
encoder model ğ‘“ to build embeddings of target objects and then
compute the distance between embeddings to determine whether
the respective objects are clones or not. Thus, we use projector
model ğ‘” only during the training phase in order to improve the
performance and convergence of CL algorithms.

2.1 Contrastive Learning Algorithms

In this work, we focus on three CL algorithms that recently proved
to be useful in the CV and NLP domains.

2.1.1
Simple Framework for Contrastive Learning of Visual Repre-
sentations (SimCLR) [11]. SimCLR is a fairly simple yet effective
CL approach that combines a number of ideas proposed in ear-
lier works. Figure 2 presents the overview of this algorithm. A
distinctive feature of SimCLR is the usage of a projector model ğ‘”
represented by a multilayer perceptron. The projector allows to
separate the embeddings learned by the model to solve the optimiza-
tion problem (minimizing the loss function) from the embedding
generated by the encoder model.

ooâ€² fÎ¸fÎ¾qkgÎ¾gÎ¸qâ€² kâ€² LEvaluation of Contrastive Learning with Various Code Representations for Code Clone Detection

Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 2: Illustration of the SimCLR approach [11].

To collect negative examples, SimCLR uses the following mech-
anism. Given a batch of clone pairs {âŸ¨ğ‘œğ‘– ; ğ‘ğ‘– âŸ©}ğµ
ğ‘–=1, where ğ‘ğ‘– is an
object similar to ğ‘œğ‘– (i.e., a positive example), SimCLR considers
other target objects {ğ‘œğ‘– }ğ‘–â‰ ğ‘— to be negative examples for the given
one ğ‘œ ğ‘— . This way, the algorithm makes an assumption that all the
target objects ğ‘œğ‘– in the batch are dissimilar.

2.1.2 Momentum Contrast for Unsupervised Visual Representation
Learning (Moco) [12]. Moco is another popular approach to train
encoder networks in the CL paradigm. Similar to SimCLR, Moco
was originally developed in the computer vision domain. However,
it has already shown good results when applied to source code [26].
Figure 3 shows the overview of the Moco approach.

Figure 3: Illustration of the Moco approach [12].

One of the key differences between Moco and SimCLR is the
buffer, which essentially is a queue of processed examples. At each
training step, there is a batch of object embeddings and their re-
spective positive examples. To collect negative examples, Moco
uses embeddings of objects from previous steps that are stored in
a queue. At the end of each training step, Moco adds vector rep-
resentations {ğ’’â€²
ğ‘–=1, where ğµ is the batch size, to the end of the
queue.

ğ‘– }ğµ

The second distinguishing feature of Moco is the way it operates
with embeddings. This approach uses the same architectures ğ‘“ and
ğ‘” for building embeddings ğ’Œ and ğ’Œ â€² respectively, but at each step it
updates their parameters using the momentum, which is essentially
a moving average:

(1)

ğ‘“ğœ‰ = ğ‘š Â· ğ‘“ğœ‰ + (1 âˆ’ ğ‘š) Â· ğ‘“ğœƒ ,
ğ‘”ğœ‰ = ğ‘š Â· ğ‘”ğœ‰ + (1 âˆ’ ğ‘š) Â· ğ‘”ğœƒ ,
(2)
where ğ‘š âˆˆ [0, 1) is a hyperparameter of the model. Backprop-
agation is only used to update the models ğ‘“ğœƒ and ğ‘”ğœƒ , which are
the models for processing target objects. This way, ğ‘“ğœ‰ and ğ‘”ğœ‰ only
approximate the parameters of the main models. Therefore, embed-
dings of reference objects are slightly different from the embedding
of the target object, making the model more robust to changes in
the reference objects.

2.1.3
Swapping Assignments between multiple Views (SwAV) [9].
SwAV reformulates the representation extraction task as online
clustering. Figure 4 shows an overview of the SwAV approach.

The authors introduce a set of ğ¿ trainable vectors {ğ’„1, ğ’„2, ..., ğ’„ğ¿ },
called prototypes, that may be considered as the clusters in which
the dataset should be partitioned. After computing the embeddings
ğ’’â€² and ğ’Œ â€², the algorithm decomposes them into weighted sums of
{ğ’„1, ğ’„2, ..., ğ’„ğ¿ }. The coefficients of these decompositions ğ’›ğ‘ and ğ’›ğ‘˜
are then used to calculate the loss function.

A major difference of SwAV compared to the previously described

approaches is that it does not use negative examples.

Figure 4: Illustration of the SwAV approach [9].

All in all, there are many possible approaches to train object
representations in the contrastive learning paradigm. They differ
in the way of collecting negative examples, processing intermedi-
ate embeddings, and choosing loss functions. Existing works that
applied CL in the SE did not compare different CL approaches and
rather focused on a single algorithm each. Thus, it remains an open
question which CL approaches are more suitable for SE tasks. In
this work, we study the code clone detection task.

2.2 Representation of Source Code in Neural

Networks

The first step of contrastive learning algorithms is the transforma-
tion of objects into numerical vectors, or embeddings. This transfor-
mation is usually done using a neural network. For the code clone
detection task, the objects are snippets of source code: functions,
classes, or even complete files.

In order to transform source code into an embedding, we need
to represent the code fragment in a way suitable for a neural net-
work. These representations differ in the way they treat code: as
plain text, as an abstract syntax tree (AST), or as a more complex
graph structure. Depending on the representation, types of neural
networks that we can use also vary. In the rest of this subsection
we describe different kinds of code representations and models that
we will use with them.

2.2.1 Text-based Representation of Code, BERT. Raw text is a natu-
ral representation of source code. It was originally used in most of
the early works on program analysis and still remains popular as it
allows direct application of methods from the NLP domain while
being rather easy to use [2, 18, 26, 53]. Figure 5a shows an example
of a code snippet. In text representation, we split the snippet into
tokens, and treat tokens as if they were words in a text written in
natural language.

We use BERT [15] as an encoder model that works with text
representation of code. BERT is a Transformer [54] model that
achieves results close to state of the art across a variety of NLP

ooâ€² fÎ¸fÎ¸qkgÎ¸gÎ¸qâ€² kâ€² LSimCLRBNReLULinearMLP projector  gBNLinearofÎ¸fÎ¾momentumqk+gÎ¾gÎ¸kâ€² +momentumLMocoNegativeÂ samplesÂ bufferupdate bufferqâ€² LinearReLULinearMLP projector  goâ€² +kâ€² âˆ’ofÎ¸fÎ¸qkgÎ¸gÎ¸qâ€² kâ€² LSwAVBNReLULinearMLP projector  gLinearClusterszqzkoâ€² Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

(a) An example of a code
snippet. Text representation
of code uses it directly after
splitting into tokens.

(b) The corresponding graph representation with edges from AST,
CFG, and PDG.

Figure 5: An example of a code snippet and more complex representations build from it.

tasks [49]. We follow the original implementation of BERT using a
bidirectional Transformer to build embeddings of each token and
then average them into a single vector to represent the code snippet
(see the original paper on BERT [15] for more details).

2.2.2 AST-based Representation of Code, code2seq. Compared to
texts in natural languages, code has a richer internal structure. This
structure can be represented with an abstract syntax tree (AST). To
get an AST, code should be processed by a parser that depends on
the programming language and its grammar. Solid green edges in
Figure 5b represent AST edges for the code snippet from Figure 5a.
We use code2seq [3] as a model that works with AST represen-
tation of code. Code2seq shows state-of-the-art results for code
summarization among the models that use information solely from
the AST. In order to represent code, code2seq first samples triples
of tokens in two AST leaves and a path between them and encodes
the path with an LSTM model [25]. Then the model aggregates
the resulting triples into a single vector using the attention mecha-
nism [37].

2.2.3 Graph-based Representation of Code, DeeperGCN. While an
AST represents the internal structure of code, it does not capture
all of the dependencies between code entities. In order to use the
information about them, we can build other representations of code:
control flow graphs (CFGs) or program dependence graphs (PDGs).
Control flow graphs describe all paths that can be traversed
through the program during its execution. Nodes in a CFG are
operators and conditions connected by directed edges to indicate
the direction of control transfer. For example, edges between sub-
sequent statements or edges from â€œifâ€ statement to its branches.
Program dependence graphs consist of two types of edges: data
dependence edges and control dependence edges. Data edges show
dependencies between usages of a variable, e.g., they link variable
declaration with all other usages. Control edges show that certain
code fragments execute depending on condition statements. For
example, ğ¶true connects â€œifâ€ statement with all statements in its
â€œtrueâ€ branch.

All these representations along with the AST may be combined
into a single graph. Figure 5b shows an example of such graph
representation for code snippet from Figure 5a. In enriches the
existing AST with CFG (shown in dotted red) and PDG (shown in
dashed blue) edges. According to recent works, using such complex

graph representations leads to good results in vulnerabilities [61] or
variable misuse [24] detection tasks and code summarization [19].
In order to work with graph-based representation of code, we
use the DeeperGCN [32] model. The model consists of multiple
layers. First, DeeperGCN embeds graph nodes into numeric vec-
tors â„ğ‘£, then each layer updates the node representation based on
the information from the adjacent nodes ğ‘ğ‘£ using the following
formula:

âˆ‘ï¸

â„ğ‘˜
ğ‘£ = ğœ (cid:169)
(cid:173)
ğ‘¢ âˆˆNğ‘£ âˆª{ğ‘£ }
(cid:171)

1
ğ‘ğ‘¢,ğ‘£

ğ‘Š ğ‘˜â„ğ‘˜âˆ’1
ğ‘¢

(cid:170)
(cid:174)
(cid:172)

,

(3)

where ğœ is sigmoid function, ğ‘Š ğ‘˜ is a matrix with learnable weights,
and ğ‘ğ‘¢,ğ‘£ is a coefficient depending on the degrees of nodes ğ‘£ and ğ‘¢.
The number of layers is a hyperparameter of the model that should
be fixed in advance. Following the ideas from the CV domain and
ResNet architecture in particular [23], DeeperGCN utilizes residual
connections between layers in order to improve the quality and
speed up the convergence.

As contrastive learning approaches do not impose restrictions on
the encoder models, we can use different encoders when applying
CL algorithms to source code. In this work, we evaluate the influ-
ence of the encoder choice on the results of different CL algorithms
in the code clone detection task.

3 EXPERIMENTAL SETUP

This section defines the experiments we conduct to evaluate the
described code representation models paired with different CL ap-
proaches. Our goal is to compare the applicability of the trained
models to the task of detecting Type IV clones. We evaluate the mod-
els in two settings: detection of functionally equivalent programs
on the POJ-104 [39] dataset, and the plagiarism detection task on
the dataset of solutions to competitive programming contests held
on the Codeforces2 platform. In both tasks, the datasets contain
pairs of programs, labeled whether they are clones or not.

3.1 Clone Detection
Task description. In the clone detection task, the model is trained
to predict whether two snippets of code are functionally identical.
Evaluating the model in this task we gain insights into how the
model can comprehend the programsâ€™ underlying functionality.

2https://codeforces.com

void () {foo   x = source();int   (x < MAX) {if     y =  * x;int2    sink(y);  }}ENTRYDECLPREDDECLCALLEXITCALLsourceint=x<x<xMAXint=y*2xARGsinkyÎµÎµÎµÎµtruefalseDxCtrueDxCtrueDyAST edgeCFG edgePDG edgeEvaluation of Contrastive Learning with Various Code Representations for Code Clone Detection

Conferenceâ€™17, July 2017, Washington, DC, USA

Dataset

Train
Samples Classes

Test
Samples Classes

Val
Samples Classes

POJ-104
Codeforces

31,000
227,833

65
55,473

10,000
88,128

23
21,070

11,000
56,482

25
13,493

Table 1: Statistics for the POJ-104 and Codeforces datasets.

Dataset. We use one of the most popular datasets in the clone
detection field, POJ-104 [39], which was previously used in many
clone detection studies [8, 59]. POJ-104 consists of 104 different
algorithms (e.g., sorting or string matching algorithms) written in C
by the users of the LeetCode3 platform. There are 500 files per each
of the 104 algorithms, which results in a total dataset size of 52,000
files. We consider all the implementations of the same algorithm to
be clones. We split the dataset into training, validation, and testing
sets by classes (i.e., implemented algorithms do not overlap between
training/validation/testing sets) in the proportion of 60 : 20 : 20.
Table 1 summarizes the datasetâ€™s statistics.

Metrics. A common way to measure modelsâ€™ performance in
clone detection task is to use F-score [55, 60]. However, this metric
has its problems: it does not consider any ordering of the result,
and in a real-world setting it requires tuning the threshold, which
is highly sensitive to each particular dataset [45]. Due to this fact,
instead of tuning thresholds for F-score, we considered F-score@ğ‘…
that calculates F-score but using only ğ‘… most relevant samples
for each given query. Although such metric handles problem with
threshold selection, F-score@ğ‘… still does not take into account
any ordering. To address this problem we also considered Mean
Average Precision at ğ‘… (MAP@ğ‘…). MAP@ğ‘… was initially introduced
for recommendation tasks [40] and is now commonly employed in
the clone detection setting [59]. MAP@ğ‘… measures how accurately
a model can retrieve a relevant object from a dataset given a query.
It is defined as the mean of average precision scores, each evaluated
for retrieving ğ‘… samples most similar to the query. In our setting,
the query set contains all programs from the testing set. We set
ğ‘… to be equal to the number of other programs implementing the
same algorithm as a given query program. That is, in POJ-104 ğ‘…
equals 500 since each file in this dataset has 500 clones, including
the file itself.

3.2 Plagiarism Detection
Task description. The main objective of this task is to identify
whether two snippets of code are the plagiarized copies of each
other. In other words, the objective is to train a model to be invariant
to the transformations of source code, introduced by plagiarism.
A decent solution to this task could be applied in the educational
settings to fight cheating.

To obtain a dataset for this task, we designed and implemented
a tool CodeTransformator which augments the existing dataset
with code files that mimic plagiarism. This choice was made due to
the origin of the problem itself: it is very hard to create a manually
labeled dataset for plagiarism detection. When manually collecting
such a dataset, positive labels would only correspond to examples
when an assessor detected cheating, and therefore, all successful

3https://leetcode.com

plagiarism attempts would be marked as original code. To deal with
it, we use synthetically generated plagiarism examples. This way,
for all pairs of code samples, we know whether they should be
classified as clones or not.

CodeTransformator. There were several previous attempts to
augment source code by introducing transformations [10, 16, 26, 44].
Tools that introduce code transformations heavily rely on the syntax
of the particular programming language, and therefore, cannot be
reused with other languages. For example, the tool developed by
Jain et al. [26] implements transformations only for JavaScript.

Quiring et al. [44] created a tool for code transformation that
targeted C and C++ languages, which is suitable for our needs.
Despite our best efforts, we were unable to reuse it due to the
multiple issues we faced when building the project. However, the
set of transformations performed by this tool is very useful. For
these reasons, we decided to develop an easy-to-use and extensible
tool of our own.

In CodeTransformator [5], we implement nine different code
transformations, they are presented in Table 2. In addition to the
transformations already proposed in literature, which we filtered
to be suitable for the plagiarism detection task, we also added new
transformations, which were suggested to us by experts in compet-
itive programming and teaching, who have extensive experience in
reading studentsâ€™ code and encountered the most popular patterns
for cheating.

Transformations

1. Add, remove comments
2. Rename variables
3. Rename functions
4. Swap if and else blocks and change

the corresponding if condition
5. Rearrange function declarations
6. Replace for with while
7. Replace while with for
8. Replace printf with std::cout
9. Expand macros

2: List of
Table
CodeTransformator.

implemented transformations

in

We implemented CodeTransformator in C++ using Clang/L-
LVMâ€™s Tooling library (libTooling) â€” a set of libraries for travers-
ing, analyzing and modifying ASTs of programs written in C/C++.
While developing the tool, we aimed to make CodeTransformator
easily extensible with new transformations. As a result, to add a
new transformation one needs to implement just a few interfaces
without diving deep into the parserâ€™s API. The tool uses multipro-
cessing to achieve better performance while working with a large

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

number of code snippets. To make the tool easily reusable by other
researchers and practitioners, we provide a Docker container that
already contains all necessary dependencies.

In order to apply CodeTransformator for their task, the user
should define a configuration through a YAML file. The config-
uration includes the number of augmented files for each source
file, the list of transformations, and the probability of applying
them to code. Thus, for each file, CodeTransformator applies the
listed transformations with a certain probability. Figure 6 shows
an example of augmented code. In this work, we apply all nine
transformations with an equal probability of 0.3, which leads us to
an average of three transformations per file. For each file, we create
four augmented versions of it. The number of augmented files is
a trade-off between the datasetâ€™s diversity and its size. Thus, the
dataset contains five copies of each file â€“ the original one and four
augmentations â€“ which are considered pairwise clones.

Dataset. In the scope of this research, we employ a dataset of
solutions to competitive programming problems hosted on Code-
forces, a platform for coding competitions. All the solutions in
this dataset are written in C/C++. We augment the dataset by us-
ing CodeTransformator. The final dataset consists on average of
4 plagiarised snippets per each solution from the original Code-
forces dataset. We split the dataset into training, validation, and
testing sets in the proportion of 60 : 20 : 20. Table 1 summarizes
the datasets we use in this work.

Metrics. Similar to the clone detection setting, we evaluate the
trained models using F1@ğ‘… and MAP@ğ‘…. In the plagiarism de-
tection task with Codeforces we chose ğ‘… to be equal to 5 since
each file from the original Codeforces dataset on average has 4
plagiarised copies in the final dataset.

Negative samples in CL. SimCLR and Moco, unlike SwAV, re-
quire negative examples (pairs of varying code snippets) along with
the positive ones (pairs of clones). The original implementations of
SimCLR and Moco assume the elements of a training batch to be
augmented versions of different origins. However, this assumption
may be violated in the plagiarism detection task since the model
may receive a batch containing multiple plagiarised copies of the
same original code snippet. To deal with this problem, a common
approach is to embed the information about intra-batch clones
into the loss function. Generalized versions of SimCLR and Moco,
which take this aspect into account, are called SupCon [28] and
UniMoco [14], respectively. We use them in our experiments with
SimCLR and Moco.

3.3 Baselines

We compare our models with several baselines, including a non-
ML approach Simian,4 CCAligner [55], JPlag [43], and modern
ML-based approaches Infercode [8] and Trans-Coder [30].

Simian is a commonly used baseline in the code clone detec-
tion task, which can handle various programming languages, and
showed good results when dealing with Type I-III clones [51].
Simian is a token-based tool, which finds clones ignoring less rele-
vant information (e.g., whitespaces, comments, imports). For Simian,
we use default hyperparameters and measure the similarity of two
programs as a number of lines marked similar by Simian.

4https://www.harukizaemon.com/simian/

# i n c l u d e < s t d i o . h>

v o i d f o o b a r ( i n t keq )

{

l e c = 0 ;
f u r = 0 ;

i n t
i n t
/ /
w h i l e ( f u r < keq )

l o o o o o p

{

l e c += f u r + 1 ;
f o r ( i n t

i < 3 ; ++ i )

{

i = 0 ;
f u r + + ;
p r i n t f ( " % d " ,

l e c ) ;

}

}

}

i n t main ( v o i d )

{

f o o b a r ( 1 2 ) ;
r e t u r n 0 ;

}

(a) Initial snippet of code

# i n c l u d e < iomanip >
# i n c l u d e < i o s t r e a m >
# i n c l u d e < s t d i o . h>

v o i d a i ( i n t ddk )

{

i n t
i n t
f o r

j = 0 ;
sdd = 0 ;
( ;

sdd < ddk ; )

/ âˆ—

' f o r '

âˆ— /

{

j += sdd + 1 ;
i n t
f o r

t j = 0 ;
( ;
sdd + + ;
s t d : : c o u t << j ;
++ t j ;

t j < 3 ; )

/ âˆ—

' f o r '

âˆ— /

{

}

}

}

{

i n t main ( )
a i ( 1 2 ) ;
r e t u r n 0 ;

}

(b) Snippet of code after augmentation with CodeTransformator.
All transformations except swapping if-else, rearranging func-
tion declarations, and expanding macros are applied.

Figure 6: Example of applying CodeTransformator to source
code

Evaluation of Contrastive Learning with Various Code Representations for Code Clone Detection

Conferenceâ€™17, July 2017, Washington, DC, USA

JPlag [43] is another popular code clone detection tool. JPlag
takes a set of programs, converts each program into a string of
canonical tokens (e.g., BEGIN_WHILE, END_METHOD) and com-
pares these representations pair-wise, computing a total similarity
value for each pair. Due to token unification, in practice, JPlag
demonstrates robust performance in detecting plagiarism among
student program submissions [38]. In our experiments, we used the
tool with its default parameters.

Another non-ML approach that we considered is CCAligner [55].
It utilizes edit distance to calculate a similarity score for code frag-
ments. While performing generally good with Type I-III code clones,
CCAligner dominates in detecting clones that strongly differ in size.
However, CCAligner is only capable of working with C and Java,
which prevents this tool from being used on C++ dataset generated
with CodeTransformator for the Plagiarism Detection task. In our
experiments, we used the tool with its default parameters.

InferCode [8] is an ML-based approach, based on a novel pre-
training method that does not require any labeled data. In InferCode,
the authors utilize codeâ€™s AST and train the model to find sub-
trees of the given tree. This model has shown impressive results
in various SE tasks, including clone detection and method name
prediction. We extracted embeddings from the final layer of the
InferCode network. We use cosine similarity of the embeddings as
a measure of similarity of two programs.

The final architecture which we use as a baseline is Trans-
Coder [30]. Trans-Coder is a Transformer-based model initially
designed for the code translation task. Trans-Coder represents the
class of models that produce meaningful embeddings of source code
because they were pre-trained on a large dataset. The pre-training
task of Trans-Coder is code translation, which requires the model
to understand the underlying functionality of the program. In this
regard, we expect Trans-Coder to show high results in the clone
search task as well. As with the InferCode, we extract embeddings
from Trans-Coder from its final layer and measure the similarity
between code snippets as cosine similarity of their embeddings.

4 IMPLEMENTATION DETAILS
4.1 Contrastive framework

We propose a unified, modular, and extensible open-source frame-
work [6] to train and evaluate models in clone detection tasks. Our
framework is implemented with PyTorch [42]. Figure 7 presents an
overview of the framework. It consists of three core components:
(1) building representations of source code, (2) encoding the repre-
sentation via trainable encoder networks, and (3) training models
with a CL approach.

We make an effort to design the framework to be extensible
and reusable. It allows users to experiment with their own code
representations, encoder architectures, CL approaches, and datasets
for the clone detection task. In order to change any step of the
pipeline, the user needs to implement a single Python interface.
Moreover, we provide an interface to integrate embeddings from
pre-trained code comprehension models, allowing researchers and
practitioners to benchmark the performance of such embeddings
in the clone detection task. The framework already contains all the
approaches we describe in this work, which can make it a strong
basis for future research on clone detection.

4.2 Model configurations

In this subsection we discuss the choice of modelsâ€™ parameters and
tools that we use for code processing.

Hyperparameters in machine learning are non-trainable param-
eters that are usually fixed before training the model. Since it is
impossible to optimize these parameters during training, their val-
ues should either be pre-selected manually or be tuned based on
the validation set. In our experiments, we have three sets of hy-
perparameters which we have to tune: hyperparameters of the CL
approaches, of the encoder models, and general training parame-
ters.

4.2.1 CL hyperparameters. Table 3 shows core hyperparameters
of the CL approaches. We aim to use default hyperparameters from
the original implementations of the respective CL algorithms, but
alter them in order to ensure that each model fits on a single Nvidia
T4 GPU. For Moco, we reduce the number of negative samples ğ‘€
to 15,360. For SwAV, we choose the number of prototypes ğ¿ to be
100 for POJ-104 since in this task we have only 104 different classes
of algorithms, and 1,000 for Codeforces.

4.2.2 Encoder configurations. To extract representations of source
code, we employ widely adopted tools, which were previously used
in ML and SE domains. We select hyperparameters of encoders in
such a way that the models have approximately equal capacities
(i.e., numbers of parameters), and can be trained on a single Nvidia
T4 GPU. Additionally, we choose the size of embeddings to be 128
for all models. Next, we describe the details of preprocessing data
for each encoder along with their configurations.

Text-based representation, BERT. We tokenize the source
code using the publicly available tool YouTokenToMe.5 Dictionary
generation is done using Byte Pair Encoding [50]. We crop the
input sequence to the maximum length of 386 tokens since BERT
has a quadratic memory footprint with respect to the input se-
quence length. The BERT encoder has 8 heads and 4 layers with
the dimension of the feed-forward part equal to 1,024.

AST-based representation of code, code2seq. We obtain path-
based representation of code using the ASTMiner6 [29] tool. The
encoder we use is essentially a code2seq path encoder, equipped
with a 2-layer perceptron classifier for path aggregation. We refer
to this encoder as code2class. For the path encoder, we use default
parameters from the original implementation [3].

Graph-based representation of code, DeeperGCN. We build
AST, CFG, and PDG from code snippets using Joern7 [48], an open-
source tool for C/C++ source code analysis. The encoder is a 6-layer
DeeperGCN network with default hyperparameters.

4.2.3 Training details. In our experiments, we observe that some
models are extremely sensitive to the learning rate. In this regard,
we conduct a grid search [34] for them in each experiment. We
search for learning rates among the values of {10âˆ’2, 10âˆ’3, 10âˆ’4, 10âˆ’5}.
Table 5 presents the best values we obtained for all the models.

For all our experiments, we use a fixed batch size ğ‘ equal to 80.
Since in most CL approaches the number of negative examples is
proportional to the batch size, we choose the batch size as big as

5https://github.com/VKCOM/YouTokenToMe
6https://github.com/JetBrains-Research/astminer
7https://joern.io

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

Figure 7: High-level view of the proposed framework. At the first step, we sample a pair of snippets from a dataset. Then,
snippets transform into one of the source code representations. Next, we compute embeddings of the respective snippets with
the selected model, and finally, feed the embeddings to a contrastive learning algorithm. The framework can be extended by
introducing new clone detection datasets, code representation methods, encoder models, and CL approaches.

Hyperparameters

POJ-104 Codeforces

Hyperparameters

POJ-104 Codeforces

Moco

SimCLR Softmax temperature, ğœ
Encoder momentum, ğ‘š
Buffer size, ğ‘€
Softmax temperature, ğœ
Number of prototypes, ğ¿
Softmax temperature, ğœ

SwAV

1000
0.1
Table 3: Hyperparametrs of the CL aproaches.

100
0.1

0.1
0.999
15 360
0.07

0.1
0.999
15 360
0.07

Vocabulary size, ğ‘‰
Max sequence length, ğ¿max
Number of heads, ğ‘heads
Number of layers, ğ‘layers
Feadforward size, ğ·feedforward
Max context, ğ‘context
Path length, ğ¿path
Classifier layers, ğ‘clf layers
Number of layers, ğ‘layers

20 000
386
8
4
1024

200
9
2

6

40 000
386
8
4
1024

200
9
2

6

BERT

code2class

GCN

Table 4: Hyperparametrs of the encoder models.

possible to fit the computational resources we use. We select the
best model using the validation set and then apply it to the testing
set in order to compare the modelsâ€™ quality.

4.3 Plagiarism Detection

According to prior research, selection of negative examples plays
an important role in the convergence of CL approaches [47]. Indeed,
with our first series of experiments on the Codeforces dataset, we
observed that the models rapidly overfitted on the training data,
i.e., showed high scores on the training set but demonstrated much
worse performance on the validation set. We attribute this result to
the fact that the models received solutions of different problems as
input, which caused them to learn to distinguish the functionality
of code fragments instead of solving the plagiarism detection task.
Due to this fact, we enforce batches in the experiments on the
Codeforces dataset to consist of solutions to the same problem.
This issue does not apply to the POJ-104 dataset, as in this case we
consider all solutions to the same problem to be clones.

5 EVALUATION RESULTS

We trained all the models and performed the hyperparameter search
on a single Nvidia Tesla T4 GPU. Table 6 presents the results of the

modelsâ€™ evaluation on the testing parts of both POJ-104 and Code-
forces datasets. In both experiments, all CL algorithms showed the
best results when used with the DeeperGCN model. It suggests that
information contained in the graph representation of code turns
out to be very useful when solving the clone detection problem.

The BERT and code2class models demonstrate worse perfor-
mance with a significant margin. Notably, the BERT model did not
converge at all in three out of six experiments, even though we
performed a grid search to identify the suitable learning rate.

5.1 BERT Convergence

Three out of six experiments we performed with the BERT model
did not converge at all. It means that the optimization procedure did
not find model parameters that would allow the model to extract
embeddings which are similar for the semantically equivalent code
snippets. For Moco, even though we got adequate results for both
datasets, only specific learning rate values led to the optimization
convergence. For SwAV on both datasets and for SimCLR on Code-
forces, all the learning rate values led to convergence failure. Based
on the experiments that produced more adequate results, we draw
the conclusion that the usage of BERT in a contrastive learning
framework is very sensitive to the choice of hyperparameters.

Clones DatasetRaw TextAST PathsGraphâ€¦BERTcode2classGCNâ€¦SimCLRMocoSwAVâ€¦Code  representationfor(i=0; i<5; ++i) { ... }Code  representationi=0; while(i<5) { ... ++i; }Evaluation of Contrastive Learning with Various Code Representations for Code Clone Detection

Conferenceâ€™17, July 2017, Washington, DC, USA

BERT
code2class
GCN

SimCLR SwAV Moco
10âˆ’3
10âˆ’5
10âˆ’4
10âˆ’5
10âˆ’5
10âˆ’4

10âˆ’4
10âˆ’5
10âˆ’2

SimCLR SwAV Moco
10âˆ’3
10âˆ’5
10âˆ’3
10âˆ’4
10âˆ’3
10âˆ’4

10âˆ’3
10âˆ’3
10âˆ’3

Table 5: Optimal learning rate values, chosen using grid search.

Model

CCAligner
Simian
JPlag
InferCode
Trans-Coder C++ to Python
Trans-Coder C++ to Java

POJ-104, MAP@500
3.35
0.6
12.09
16.25
48.39
49.39

POJ-104, F1@500
7.81
3.16
18.82
27.87
55.54
56.41

Codeforces, MAP@5

Codeforces, F1@5

â€“
32.59
38.88
43.01
42.43
42.68

â€“
40.69
46.58
44.50
43.64
43.91

Contrastive

SimCLR SwAV Moco
48.94
0.43
36.15
33.28
59.27
57.19
Table 6: Experiment results for all the studied models on two datasets: POJ-104 and Codeforces. The values are MAP@ğ‘… and
F1@ğ‘…, with ğ‘… being 500 for the POJ-104 dataset and 5 for the Codeforces one.

SimCLR SwAV Moco
44.09
5.03
48.01
45.51
48.19
47.32
60.32
62.59
62.84

SimCLR SwAV Moco
44.66
0.38
34.11
31.23
58.99
56.19

SimCLR SwAV Moco
32.24
0.44
35.38
33.09
37.14
36.59
52.17
55.16
55.75

BERT
code2class
GCN

1.21
40.70
58.94

1.58
42.65
59.23

Another conclusion that can be drawn from the experiments
conducted with the BERT encoder and Moco is that Moco is way
more robust than other CL algorithms. According to Liu et al. [36],
using large batch sizes during training usually positively affects the
BERT model. Since Moco utilizes a queue of previously processed
code snippets to treat them as negative examples, the number of
samples processed by the model at each training step is several
times higher than that for SimCLR.

5.2 Clone Detection Task

In the clone detection task with the POJ-104 dataset, we obtain
the best results when using DeeperGCN paired with SimCLR and
SwAV, while code2class and BERT showed nearly identical (and
substantially lower) scores. It can be attributed to the fact that
DeeperGCN works with a richer representation of code, which by
design contains more information. Notably, as we want to analyze
the underlying semantics of the program, graph-based models are
more resistant to changes in code, as long as these changes do not
break the code semantics: e.g., renaming, reordering of operations.
Out of the three studied CL algorithms, SimCLR performed
best with BERT and GCN or on par with other CL methods with
code2class. The superior performance of SimCLR and SwAV can
be attributed to the fact that these CL approaches, in contrast to
Moco, encode both objects with a shared network. In this regard,
the encoder model receives updates from each element of the batch.
On the other hand, such training scheme has proven to be less
stable in experiments with BERT, which has a larger number of
parameters.

As expected, Simian and CCAligner failed to achieve good quality
when detecting Type-IV clones. Both approaches measure similarity

between files based on their tokens, and for different programs im-
plementing the same algorithm in the POJ-104 dataset, the overlap
of tokens (even computed with some heuristics) can be extremely
small. However, another classical approach, JPlag, surpassed others,
indicating that token unification is the most accurate approach in
detecting Type-IV clones among the token-based tools we com-
pared.

Interestingly, the embeddings learned by Trans-Coder in code-
to-code translation tasks turned out to be nearly as useful in clone
detection tasks as the ones learned in the CL setting. It shows
that pre-training with the task of code translation indeed produces
models that can successfully extract the underlying functionality
of source code.

In contrast, InferCode showed unexpectedly low results, which
can be attributed to the fact that the pre-training objective of In-
ferCode made it robust only to minor transformations of code.
However, it is still challenging for the model to find similarities
among the significantly different programs from POJ-104.

5.3 Plagiarism Detection Task

The Codeforces dataset that we used for the plagiarism detection
task considers code snippets to be clones when one of them was
made from another with a number of pre-defined transformations
(see Table 2). Thus, in order to be considered clones in this dataset,
pairs of code snippets should not only be functionally equivalent (as
regular Type-IV clones) but also share more code-level similarities.
This difference impacts the results significantly compared to the
POJ-104 dataset.

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

Similar to the previous experiment with POJ-104, DeeperGCN
performs best with all the CL algorithms. When detecting plagia-
rism, the gap in quality between graph-based models and other
approaches becomes even more significant compared to the clone
detection case. As with clone detection, graph representation turns
out to be the most robust to variations in the implementation, as it
pays more attention to the internal structure of the solution, which
is harder to change.

BERT successfully converged only in combination with the Moco
approach. However, in this case, it achieved decent results, outper-
forming all other approaches except for graph-based models. Poor
performance of the BERT encoder coupled with SimCLR and SwAV
again supports the hypothesis that the momentum encoding scheme
is more robust in terms of huge models.

In contrast to the clone detection case, the approaches based
on the code2class encoder perform significantly worse than the
others. In order to represent a code fragment, code2class samples
a fixed number of paths from the AST. When the code fragment
is large (and in this experiment, the model takes a whole program
as input), a particular choice of sampled paths strongly influences
the prediction. Even for the nearly identical files, if the sampled
sets of paths significantly differ, the model might fail to identify the
similarity. A possible way to mitigate this issue is the development
of better techniques for choosing which paths to sample from the
syntax tree.

As the plagiarism detection task required models to identify
transformed files, Simian, JPlag, and InferCode significantly im-
proved their performance compared to the code clone detection. We
attribute this to the fact that in this setting code snippets considered
to be clones share more textual similarities than in the previous
setting.

In contrast to other approaches, the performance of Trans-Coder
models dropped compared to the POJ-104 dataset. Since we used
embeddings produced by these models without any fine-tuning
for the specific task, these models continued to consider files that
they believed shared the same functionality to be clones. However,
for the plagiarism detection task, the models had to take into ac-
count the implementation-level similarity, as we considered only
the transformed snippets to be clones.

Overall, as expected, ML-based models significantly outperformed
token-based Simian, CCAligner, and JPlag in both tasks. The pre-
trained Trans-Coder have indeed learned meaningful features from
the code translation task and achieved results comparable to the
models trained in the CL paradigm. In both clone and plagiarism
detection tasks, graph-based representations led to the best perfor-
mance. In terms of contrastive learning approaches, we conclude
that SimCLR and SwAV performed on par or even better than Moco.
This result can be attributed to the fact that the encoder in SimCLR
and SwAV receives more information during training than the en-
coder in Moco due to the fact that the latter one uses only a single
encoder. However, Moco shows to be more robust combined with
BERT and even outperforms the pre-trained Trans-Coder in the
plagiarism detection task.

6 RELATED WORK

Searching for code clones is an important yet not fully solved task
in the Software Engineering domain. According to the survey by
Ain et al. [1], there are plenty of different approaches to detecting
code clones that differ in the way the code is processed, in target
languages, or supported platforms. While approaches targeting
Type I-III clones show decent performance, the detection of Type IV
clones turns out to be still very difficult for most of the models.

The existing works on the detection of Type IV clones commonly
used machine learning techniques [8, 26, 33, 35, 56, 59]. These works
differ in two major ways: by the ML approaches they use and by
the way they extract information from source code.

From the perspective of ML approaches, in our work we mainly
focus on the contrastive learning paradigm for training the models.
In recent years, contrastive learning proved to be useful in detecting
similar objects in both computer vision and natural language pro-
cessing [11â€“13, 20, 46, 58]. Driven by this success, CL approaches
already found application in the task of clone detection in program
code [26, 41, 56, 59].

The study by Jain et al. [26] focused on different types of model
pre-training on source code, while studying a single contrastive
learning algorithm. Ye et al. presented another CL approach to clone
detection [59]. The authors introduced CASS, a new graph repre-
sentation of code, and showed that enriching AST with additional
edges leads to a significant performance improvement in the clone
detection task. However, the CL algorithm utilized in this work
was previously outperformed by several modern approaches [27].
A recent comparative study of code representations assessed eight
ML models, including text-based and AST-based ones on several
tasks, including code clone detection [22]. However, the authors
trained the models to solve a classification task rather than using a
contrastive learning paradigm.

In contrast to the described studies, we compare multiple modern
contrastive learning algorithms between themselves, as well as
with solutions that do not involve contrastive learning: Simian,
InferCode [8], and Trans-Coder [30].

From the perspective of code representations, we explore which
representations work better for encoders in CL methods. In previous
works on code clone detection, researchers represented code as
a sequence of tokens [18, 26], as Abstract Syntax Trees and its
derivatives [3, 8], and as more complex graph structures such as
CFG and PDG [7, 17, 59]. Following these works, as encoders we
selected three models that represent code differently: BERT [15],
code2seq [3], and DeeperGCN [32].

7 CONCLUSION

With this work, we present a comprehensive study of modern con-
trastive learning approaches in combination with different source
code representations and encoder models in the setting of the
code clone detection task. We demonstrate that graph-based mod-
els indeed show better results than text-based models. Moreover,
we show that the SimCLR and SwAV contrastive approaches can
achieve higher scores, while Moco is generally more robust. Fi-
nally, we demonstrate that for the code clone detection task, the
embeddings learned within various pre-training objectives (e.g.,
AST subtree prediction or code translation) can serve as a strong

Evaluation of Contrastive Learning with Various Code Representations for Code Clone Detection

Conferenceâ€™17, July 2017, Washington, DC, USA

baseline, even compared with the models trained in a supervised
manner.

We introduce a novel tool called CodeTransformator for aug-
mentation of source code. In CodeTransformator, we implemented
nine source code transformations, simulating plagiarism. We de-
signed the tool to be extensible and easy to use. In addition, we
propose a highly extensible framework for training and evaluation
of ML models for the clone detection task. We believe that the tool
and the framework developed in the scope of this work can serve
as a basis for future development in the clone detection field. To
this end, we make all the code, tools, datasets, and trained models
publicly available.

We make both the evaluation framework and CodeTransformator

publicly available [5, 6]. We will also make all the datasets we used
public upon the paper acceptance. We believe that these artifacts
will serve as a basis for future development in the clone detection
task. Possible future work includes the reuse of the embeddings
learned in the contrastive learning paradigm for other tasks, e.g.,
refactoring recommendation or documentation generation, adding
new code transformations for more robust plagiarism detection,
and extension of our work to other programming languages.

REFERENCES
[1] Qurat Ul Ain, Wasi Haider Butt, Muhammad Waseem Anwar, Farooque Azam,
and Bilal Maqbool. 2019. A systematic review on code clone detection. IEEE
access 7 (2019), 86121â€“86144.

[2] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting accurate method and class names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. 38â€“49.

[3] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations. https://openreview.net/forum?id=H1gKYo09tX
[4] Mauricio Aniche, Erick Maziero, Rafael Durelli, and Vinicius Durelli. 2020. The
effectiveness of supervised machine learning algorithms in predicting software
refactoring. IEEE Transactions on Software Engineering (2020).

[5] Anonymous. 2021. Supplementary Materials: CodeTransformator tool for source

code augmentation. https://doi.org/10.5281/zenodo.5595464

[6] Anonymous. 2021. Supplementary Materials: Contrastive Learning Framework.

https://doi.org/10.5281/zenodo.5596345

[7] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neu-
ral Code Comprehension: A Learnable Representation of Code Semantics.
arXiv:1806.07336 [cs.LG]

[8] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2020.

InferCode: Self-
Supervised Learning of Code Representations by Predicting Subtrees.
arXiv:2012.07023 [cs.SE]

[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin. 2020. Unsupervised learning of visual features by contrasting
cluster assignments. arXiv preprint arXiv:2006.09882 (2020).

[10] Hayden Cheers, Yuqing Lin, and Shamus P Smith. 2020. Detecting pervasive
source code plagiarism through dynamic program behaviours. In Proceedings of
the Twenty-Second Australasian Computing Education Conference. 21â€“30.
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations.
arXiv:2002.05709 [cs.LG]

[12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved Baselines
with Momentum Contrastive Learning. arXiv preprint arXiv:2003.04297 (2020).
[13] Xinlei Chen, Saining Xie, and Kaiming He. 2021. An Empirical Study of Training

Self-Supervised Vision Transformers. arXiv:2104.02057 [cs.CV]

[14] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. 2021. UniMoCo: Unsu-
pervised, Semi-Supervised and Full-Supervised Visual Representation Learning.
arXiv:2103.10773 [cs.CV]

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 [cs.CL]

[16] Breanna Devore-McDonald and Emery D. Berger. 2020. Mossad: Defeating

Software Plagiarism Detection. arXiv:2010.01700 [cs.CR]

[17] Chunrong Fang, Zixi Liu, Yangyang Shi, Jeff Huang, and Qingkai Shi. 2020. Func-
tional Code Clone Detection with Syntax and Semantics Fusion Learning. In
Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing

and Analysis (Virtual Event, USA) (ISSTA 2020). Association for Computing Ma-
chinery, New York, NY, USA, 516â€“527. https://doi.org/10.1145/3395363.3397362
[18] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint arXiv:2002.08155
(2020).

[19] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2021. Structured

Neural Summarization. arXiv:1811.01824 [cs.LG]

[20] John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021.

De-
CLUTR: Deep Contrastive Learning for Unsupervised Textual Representations.
arXiv:2006.03659 [cs.CL]

[21] M. Gutmann and A. HyvÃ¤rinen. 2010. Noise-contrastive estimation: A new

estimation principle for unnormalized statistical models. In AISTATS.

[22] Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu. 2021. A Comparison of

Code Embeddings and Beyond. arXiv preprint arXiv:2109.07173 (2021).

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.

[24] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David
Bieber. 2019. Global relational models of source code. In International conference
on learning representations.

[25] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-term Memory. Neural

computation 9 (12 1997), 1735â€“80. https://doi.org/10.1162/neco.1997.9.8.1735

[26] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E. Gonzalez, and Ion
Stoica. 2020. Contrastive Code Representation Learning. arXiv:2007.04973 [cs.LG]
[27] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Baner-
jee, and Fillia Makedon. 2021. A Survey on Contrastive Self-supervised Learning.
arXiv:2011.00362 [cs.CV]

[28] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2021. Supervised Contrastive
Learning. arXiv:2004.11362 [cs.LG]

[29] Vladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, and Alberto Bacchelli.
2019. PathMiner: a library for mining of path-based representations of code. In
Proceedings of the 16th International Conference on Mining Software Repositories.
IEEE Press, 13â€“17.

[30] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample.
2020. Unsupervised translation of programming languages. arXiv preprint
arXiv:2006.03511 (2020).

[31] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive repre-

sentation learning: A framework and review. IEEE Access (2020).

[32] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. 2020. DeeperGCN:

All You Need to Train Deeper GCNs. arXiv:2006.07739 [cs.LG]

[33] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. 2019.
Graph Matching Networks for Learning the Similarity of Graph Structured Ob-
jects. arXiv:1904.12787 [cs.LG]

[34] Petro Liashchynskyi and Pavlo Liashchynskyi. 2019. Grid search, random search,
genetic algorithm: a big comparison for NAS. arXiv preprint arXiv:1912.06059
(2019).

[35] Xiang Ling, Lingfei Wu, Saizhuo Wang, Tengfei Ma, Fangli Xu, Alex X. Liu,
Chunming Wu, and Shouling Ji. 2020. Hierarchical Graph Matching Networks
for Deep Graph Similarity Learning. arXiv:2007.04395 [cs.LG]

[36] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[37] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effec-
tive approaches to attention-based neural machine translation. arXiv preprint
arXiv:1508.04025 (2015).

[38] Marko Misc, Zivojin Sustran, and Jelica Protic. 2016. A comparison of software
tools for plagiarism detectionin programming assignments. The International
journal of engineering education 32, 2 (2016), 738â€“748.

[39] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2015. Convolutional
Neural Networks over Tree Structures for Programming Language Processing.
arXiv:1409.5718 [cs.LG]

[40] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. 2020. A Metric Learning

Reality Check. arXiv:2003.08505 [cs.CV]

[41] Aravind Nair, Avijit Roy, and Karl Meinke. 2020. funcGNN: A Graph Neural
Network Approach to Program Similarity. In Proceedings of the 14th ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM). 1â€“11.

[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. arXiv:1912.01703 [cs.LG]

[43] Lutz Prechelt, Guido Malpohl, Michael Philippsen, et al. 2002. Finding plagiarisms
among a set of programs with JPlag. J. Univers. Comput. Sci. 8, 11 (2002), 1016.

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

[44] Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading Authorship
Attribution of Source Code using Adversarial Learning. arXiv:1905.12386 [cs.LG]
[45] Chaiyong Ragkhitwetsagul, Jens Krinke, and David Clark. 2018. A comparison of
code similarity analysers. Empirical Software Engineering 23, 4 (2018), 2464â€“2519.
[46] Nils Rethmeier and Isabelle Augenstein. 2021. A Primer on Contrastive Pretrain-
ing in Language Processing: Methods, Lessons Learned and Perspectives. arXiv
preprint arXiv:2102.12982 (2021).

[47] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. 2021.

Contrastive Learning with Hard Negative Samples. arXiv:2010.04592 [cs.LG]

[48] Marko A. Rodriguez and Peter Neubauer. 2010. The Graph Traversal Pattern.

arXiv:1004.1001 [cs.DS]

[49] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertol-
ogy: What we know about how bert works. Transactions of the Association for
Computational Linguistics 8 (2020), 842â€“866.

[50] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, Berlin, Germany, 1715â€“1725. https:
//doi.org/10.18653/v1/P16-1162

[51] Jeffrey Svajlenko and Chanchal Roy. 2014. Evaluating Modern Clone Detection
Tools. Proceedings - 30th International Conference on Software Maintenance and
Evolution, ICSME 2014 (12 2014), 321â€“330. https://doi.org/10.1109/ICSME.2014.54
[52] Jeffrey Svajlenko and Chanchal K Roy. 2020. A Survey on the Evaluation of
Clone Detection Performance and Benchmarking. arXiv preprint arXiv:2006.15682
(2020).

[53] Bart Theeten, Frederik Vandeputte, and Tom Van Cutsem. 2019. Import2vec:
Learning embeddings for software libraries. In 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR). IEEE, 18â€“28.

[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. arXiv:1706.03762 [cs.CL]

[55] Pengcheng Wang, Jeffrey Svajlenko, Yanzhao Wu, Yun Xu, and Chanchal K Roy.
2018. CCAligner: a token based large-gap clone detector. In Proceedings of the
40th International Conference on Software Engineering. 1066â€“1077.

[56] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code
Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree.
arXiv:2002.08653 [cs.SE]

[57] Yiwei Wang, Wei Wang, Yujun Ca, Bryan Hooi, and Beng Chin Ooi. 2020. Detect-
ing Implementation Bugs in Graph Convolutional Network based Node Classifiers.
In 2020 IEEE 31st International Symposium on Software Reliability Engineering
(ISSRE). 313â€“324. https://doi.org/10.1109/ISSRE5003.2020.00037

[58] Yaochen Xie, Zhao Xu, Zhengyang Wang, and Shuiwang Ji. 2021. Self-Supervised
Learning of Graph Neural Networks: A Unified Review. arXiv:2102.10757 [cs.LG]
[59] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nesime Tatbul,
Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen, Timothy Mattson, Tim
Kraska, Pradeep Dubey, Vivek Sarkar, and Justin Gottschlich. 2021. MISIM: A
Novel Code Similarity System. arXiv:2006.05265 [cs.LG]

[60] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. 2019. A Novel Neural Source Code Representation Based on Abstract Syntax
Tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). 783â€“794. https://doi.org/10.1109/ICSE.2019.00086

[61] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks. arXiv:1909.03496 [cs.SE]

[62] Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of Source Code
from Structure and Context. In International Conference on Learning Representa-
tions (ICLR).

[63] Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of Source Code
from Structure and Context. arXiv:2103.11318 [cs.LG]

