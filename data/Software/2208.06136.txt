How far are German companies in improving
security through static program analysis tools?

Goran Piskachev, Stefan Dziwok, Thorsten Koch, Sven Merschjohan
Fraunhofer IEM
{name.surname}@iem.fraunhofer.de

Eric Bodden
Paderborn University & Fraunhofer IEM
eric.bodden@upb.de

2
2
0
2

g
u
A
2
1

]

R
C
.
s
c
[

1
v
6
3
1
6
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—As security becomes more relevant for many com-
panies, the popularity of static program analysis (SPA) tools
is increasing. In this paper, we target the use of SPA tools
among companies in Germany with a focus on security. We give
insights on the current issues and the developers’ willingness
to conﬁgure the tools to overcome these issues. Compared to
previous studies, our study considers the companies’ culture
and processes for using SPA tools. We conducted an online
survey with 256 responses and semi-structured interviews with
17 product owners and executives from multiple companies. Our
results show a diversity in the usage of tools. Only half of our
survey participants use SPA tools. The free tools tend to be
more popular among software developers. In most companies,
software developers are encouraged to use free tools, whereas
commercial tools can be requested. However, the product owners
and executives in our interviews reported that their developers
do not request new tools. We also ﬁnd out that automatic security
checks with tools are rarely performed on each release.

Index Terms—empirical research, survey, interviews, software

development, software security, static analysis, SAST

I. INTRODUCTION

In an increasingly digitalized world, the security of assets
processed by software is highly relevant. One way to ensure
better security is through static program analysis (SPA) tools.
The number of SPA tools with security focus is rising (e.g.,
CodeSonar [1], LGTM [2], Checkmarx [3], Infer [4]). From
pattern-based approaches such as FindBugs [5] to more com-
plex data-ﬂow analyses such as Checkmarx [3], these tools
have versatile applications with different stakeholders (e.g.,
software developers, testers, security experts).

Due to their increasing popularity, usability researchers have
studied SPA tools. Christakis et al. [6] identiﬁed problems that
software developers have while using SPA tools at Microsoft.
Lewis et al. [7] performed user studies and interviews at
Google to investigate whether tools’ ﬁndings inﬂuence the
behavior of software developers. Nguyen Quang Do et al. [8]
used a survey to identify the needs and motivators that
software developers at Software AG have for SPA tools.

These studies mainly focus on software developers as pri-
mary users. Moreover, they are based on a single company
case. A possible threat is that the company’s culture and pro-
cesses inﬂuence the software developers’ perception. Hence, it
is essential to address this when similar studies are conducted.
This paper presents an empirical study on SPA tools per-
formed in 2019 among German-speaking software develop-
ment teams in multiple companies of different sizes. The

results give insights into how software development teams
use and conﬁgure SPA tools. The objective of the study is
three-fold: (1) understanding the current usage of SPA tools
in the industry, (2) identifying how software developers use
SPA tools and conﬁgure them, and (3) understanding the
companies’ culture and processes for using SPA tools.

To increase the credibility and validity of our research re-
sults, we apply a triangulation method consisting of an online
survey and semi-structured interviews. Through the online
survey, we target a more signiﬁcant number of participants,
in particular 256. In contrast, we target a more focused group
of 17 experts from different companies through the interviews,
including product owners and executives. The study reached a
broad range of companies. However, 59 responses (23%) from
the survey are from a single large company, which remains
anonymous. In the rest of the paper, we use the anonymous
alias ABC. The results show the same trend from the single
company vs. multiple companies with a few minor exceptions
we identiﬁed.

In general, the use of SPA tools among the participants
in our survey is 51.8%, of which the free tools are the
most popular among developers. Aligning with several existing
studies, our study conﬁrms some of the results of other studies,
however also identiﬁes new ones. E.g., the high number of
false warnings remain a problem. In contrast
to previous
studies, the participants in our study think that the existing
SPA tools are fast enough.

Most participants are willing to conﬁgure the rules of the
tools, even though only a few have experience with that. The
product owners and executives are willing to provide enough
tools to enable secure software development, but there is a
low demand from their software developers. Finally, most
participants think that the existing processes for SPA tools
are not clearly deﬁned. Although aware of the importance
for automatic security checks before each release, these are
performed only by third of the teams of our participants.
The main contributions of this paper are as follows:

• An online survey design targeting members of software

development teams

• Semi-structured interviews with 17 product owners and

executives from six companies

• Recommendations for the next generation tools based on

results from this empirical study

 
 
 
 
 
 
• The artifact from our study is publicly available as an
open science framework project1 including all relevant
documents used to perform the survey and the interviews,
the raw and processed data, and the anonymized protocols
from the interviews.

II. RELATED WORK

This section lists relevant studies that we grouped into

usability-related (II-A) and security-related (II-B) studies.

A. Usability of SPA tools

Christakis et al. [6] performed a study at Microsoft consist-
ing of a survey and interviews with developers, and analyzed
data from high-critical bug reports handled by on-call engi-
neers. As a result, they identiﬁed usability issues and missing
functionalities that program analysis tools should address,
such as aiming for a lower false positive rates, suppressing
warnings, better ﬁt into the workﬂow, etc. Nguyen Quang
Do et al. [8] performed a study with the German company
Software AG by conducting a survey with developers and
collecting usage data from the SPA tool Checkmarx [3]. They
identiﬁed users’ needs when using Checkmarx, such as better
explainability of the warnings, an easier conﬁguration of the
rules, integrated recommendation system, and collaboration
options. Sadowski et al. [9] performed another similar study
at Google by evaluating the usage of Tricoder, a platform
for integrating different analysis tools to scan the codebase at
Google. The goal of the study was requirements elicitation for
the platform. Luo et al. [10] performed a user study to evaluate
the cloud-based integration of the CodeGuru Reviewer tool at
Amazon. Compared to our study, all studies targeted a single
company, in particular large corporations, and a single tool to
reason about usability issues and potential new features of the
tools.

Vassallo et al. [11] gathered participants from multiple
companies for a survey and interviews to study the context in
which the SPA tools are used. Similarly, Thomas et al. [12]
performed a user study and interviews with participants from
multiple companies to study the context of the SPA tools
within the IDE, in particular, the interactive features that the
IDE provides. The usage of SPA tools within the CI pipelines
was studied by Zampetti et al. [13] by mining open-source
projects. Compared to our study, none of these studies consider
the companies’ culture and processes for using SPA tools.

B. Security-related studies

Smith et al. [14] used the tool FindBugs in a user study and
asked the participants to explain their thoughts while using
the tool to ﬁnd and ﬁx security vulnerabilities. Using a card-
sorting method with the collected data, they identiﬁed how
users interact with SPA tools. Another study by Smith et
al. [15] evaluated four SPA tools using a walkthrough method
and interviews. This study evaluated the user interfaces of
the selected SPA tools. As a result, they identiﬁed issues
and potential improvements for the tools. Finally, Witschey et

1https://osf.io/k37c9/

al. [16] performed a survey and identiﬁed factors that impact
the adoption of SPA tools. All studies focus on a given usage
scenario. Moreover, none of them considered the impact of
the company’s culture and processes.

Lastly, a recent study by Weir et al. [17] implemented
intervention activities in eight development teams to increase
the security awareness and security standards of their software
products. These activities included trainings and workshops
with the teams over four months. This study addressed a
broad range of security activities such as threat modeling,
penetration tests, security management, SPA tools, etc. The
authors reported that
these intervention activities, also for
teams without security experts, are helpful. We collected
similar data showing that our participants ﬁnd activities, such
as trainings and the use of SPA tools, highly relevant.

III. METHODOLOGY
We present our triangulation methodology [18], consisting
of an online survey and semi-structured interviews. Both
techniques are used to gather data from different roles. In
the following, we state the research questions this paper
addresses and the motivation behind them. Section III-B and
Section III-C discuss the method applied for the survey and
the interviews, respectively.

A. Research questions

As seen in the previous section,

the use of SPA tools
has been researched in multiple studies, targeting a selected
company or speciﬁc context of use (e.g., use in IDE, use of
a speciﬁc tool, etc.). Since the SPA research community has
been growing, many new approaches have been published [19],
[20], [21], [22]. Additionally, new tools appeared, especially
in the security domain, e.g., LGTM [2], SemGrep [23],
Snyk [24], CodeGuru [25], MarianaTrench [26]. Therefore,
studying the current practical use of these tools is still relevant.
In particular, we investigate the use of SPA tools in one
country, i.e., Germany. As security gains more attention among
German companies [27], our study targets the companies’
speciﬁc cultures and processes. Understanding the context in
which SPA tools are currently used, will help tool vendors
develop new features and better tools.

This study answers the following research questions:

RQ1 To what extent are SPA tools used in practice among

software development teams in Germany?

RQ2 How are SPA tools used in practice, and what are the

problems faced by the users?

RQ3 What are typical culture and processes for using SPA
tools and other security checks in German companies?
Since the study targeted software development teams in
Germany, the survey and the interviews were conducted in
German. The participation in the study was voluntary and
without any personal compensation.

B. Survey

1) Population: To understand the usage of SPA tools, we
invited participants from all roles involved in software devel-
opment, including developers, architects, product owners, and

executives. The study focuses on companies from Germany
because our research institutions have contacts and regularly
conduct projects with German companies. We used three ways
to gather participants. First, we used our direct contacts.
Second, we created posts on our institution’s social media
channels and website. Third, the survey was promoted by the
media of one of the leading publishing houses in Germany,
Heise [28], which publishes technology-focused magazines
and organizes events with an audience mainly from German
companies. Additionally, the survey was promoted among the
networks Bitkom[29], it’s OWL [30], and innozent OWL [31].
In total, we received responses from 350 participants. We
excluded all incomplete responses, answered in an unrealis-
tically short time, or not from Germany. After this ﬁltering,
we gathered 256 responses, of which 204 developers. If we
consider that there are roughly 900.000 software developers in
Germany2, we get a margin of error of only 7%3, which has
a conﬁdence level of 95%, making our study representative of
the target population.

Of all participants, 47% are from large companies with
more than 1000 employees, and the rest are from small-
and medium-sized companies (Q244). The size of most of
the teams (Q29) is 6-15 persons (56%) and about one third
are small teams 1-5 persons (32%). 61% of the participants
have long experience of more than ten years in software
development (Q28). Of all participants, 80% have software
developer role, followed by 12% executive, 10% other, 8%
product owner, 6% project manager, 5% data protection
ofﬁcer, and 4% security analyst, where multiple answers were
allowed (Q27). The participants come from 37 sectors, such
as automotive, electrical, chemical, insurance, transportation,
etc. (Q25).

2) Data collection: We conducted the survey using the
online tool Survey Monkey [32]. The tool allowed us to use
different links for the six companies we invited and one link
shared publicly for other companies. Among all links, we
received a considerable number of responses from one large
company: 59 responses from 256. Hence, in our results, we
compare the trends among multiple companies of mixed size
and a single large company. The survey was open for six weeks
during summer 2019. On average, the participants needed 25
minutes to complete the questionnaire, measured based on the
session duration per participant collected by Survey Monkey.
3) Design: We conducted the survey as part of a large
research project [33], which goal is to understand the security-
related activities in software development among German
companies. This paper only presents the results on SPA tools.
We followed the guidelines for opinion surveys by Kitchen-
ham et al. [34]. Initially, we conducted a literature search to
identify relevant related work (Section II). None of the existing
studies provided a survey instrument (i.e., a questionnaire)

that can be reused. Hence, we created a new questionnaire
for a cross-sectional survey. Five researchers were involved
in creating and selecting the questions in a top-down process,
starting from the research questions and breaking them into
more concrete questions. The questionnaire was reviewed by
three more researchers not part of the project. We performed
two internal tests with students from our research group to
verify the clarity of the questions and measure the time needed
to complete the questionnaire. After that, we performed three
external tests with professionals from the industry.

The questionnaire with 42 questions is available in our arti-
fact. The relevant questions for the paper are in the Appendix.

C. Interviews

1) Population: We performed 17 interviews with product
owners and executives. Four of them were our previously
known contacts. The rest were selected through convenience
sampling [35]. We invited several randomly chosen companies
from our region and used the ﬁrst come, ﬁrst served principle
to conduct the interviews with persons that volunteered to
participate. Seven interviewees were product owners, six were
executives, and the remaining four had both roles. All experts
had professional experience in software development.

2) Data collection: Each interview was performed by two
researchers. In total, six researchers took the role of inter-
viewer. In each interview, one researcher was the moderator
asking the questions and the other researcher wrote a protocol
and, in rare cases, asked questions. Additionally, an audio
recording of all sessions was made. After the interviews, the
recordings were automatically transcribed and used to extend
the protocols created during the interview. On average, each
interview took 45 minutes. The interviews were conducted
during the second half of 2019. For the evaluation of the
interviews, we used the codebook method [36], where three
researchers manually annotated all transcripts.

3) Design: We applied a similar process as the survey
(Section III-B) to design the questionnaire for the interviews.
We created two versions, one for each role, product owner and
executives, which differ only in a few questions. The experts
who had both roles were asked all questions.

IV. RESULTS

This section presents the study results and answers our
research questions. With N, we denote the number of responses
collected for each question. Note many participants answered
different sections of the survey due to the optional questions.
Hence, in the following, we report the percentage and the
absolute numbers. To ﬁnd correlations between two questions,
we use the Cramer v value [37] (where values are between 0
and 1, with values over 0.25 having strong correlation) and
for statictical signiﬁcance the p value.

2https://www.daxx.com/de/blog/entwicklungstrends/anzahl-an-

softwareentwicklern-deutschland-weltweit-usa

3https://www.surveymonkey.com/mp/margin-of-error-calculator/
4throughout the paper we denote QX, to refer to the relevant question with

number X from the online survey

A. Use of SPA tools (RQ1)

Among the survey participants, there is a heterogeneous
use of IDEs and programming languages. The top three used
IDEs (Q12) are IntelliJ IDEA (60%), Eclipse (53%), and

Visual Studio Code (36%). The top three used programming
languages (Q13) are Java (76%), JavaScript/TypeScript (45%),
and SQL (34%). There is a correlation that those developers
who use Java also use IntelliJ IDEA (Q13-Q12, Cramer
V=0.537 p=0, N=123) and develop web applications (Q13-
Q30, Cramer V=0.385 p=0.0001, N=114).

In total, 114 (51.8%) out of 220 responses said they use SPA
tools. When asked which tools they use (Q16), there were 57
uniquely named tools, of which only four tools were named
in at least ten responses, i.e., SonarQube (50 responses), Find-
bugs (19), Checkstyle (13), and OWASP Dependency-Check
(10). These tools are freely available or at least have a free
version. They mostly perform simple pattern-based matching
techniques to detect
issues. SonarQube additionally has a
taint analysis as more complex and sophisticated analysis.
Other tools with more sophisticated dataﬂow analyses that
the survey participants mentioned are: Checkmarx, Fortify,
Klockwork, and Coverity. Only two participants noted that
they use internally developed tools.

Furthermore, the participants were asked to prioritize, where
the warnings from the tools should be reported. From the
possible options: (1) within the IDE, (2) on an internal website,
and (3) in the ticket system, 160 out of 239 responses (66.9%)
selected the option (1) with the highest priority.

To compare the results within a single company with
multiple differently-sized companies, we extracted the results
from the company ABC. 27 out of 59 responses (45.8%) said
that they use SPA tools which is lower than the 51.8% among
all companies including ABC or 54% excluding ABC.

There is a diversity in the use of IDEs and programming
languages. Moreover, only about half of the teams use SPA
tools, of which the most popular are SonarQube, Findbugs,
Checkstyle, and OWASP Dependency-Check.

B. Problems and Tools Conﬁguration (RQ2)

In the following, we discuss the problems identiﬁed by the
participants when using the tools as well as their perspective
on conﬁguring the tools.

Previous studies from several years ago [6] have reported
that the existing tools are not fast enough to be used in
development time. The participants in our study perceive the
current situation differently. Ninety-three out of 114 collected
responses (82%) think that the tools they use are fast enough
(Q18.1). However, the number of false warnings from the SPA
tools remains high (Q18.2). Sixty-ﬁve out of 114 participants
(57%) have claimed this. In particular, those who answered
that the number of false warnings is high also said that they
program in the C language (Q18.2-Q13, Cramer V=0.305
p=0.0205, N=112)).

Based on their expertise and the warnings from the tools,
69% of the participants can conﬁrm which warnings are true
positives (79 out of 114 responses) (Q18.3). The participants
think that
issues regularly (80 out of
114 responses) (Q18.4). Finally, 81% (92 out of 114) of the

the tools ﬁnd real

participants said that the messages reported from the warnings
help them ﬁx the issues found in the code (Q18.5).

When it comes to the conﬁguration of the tools, we asked
the participants to what extent they are willing to change,
add, or remove the rules used by the tools to ﬁnd different
security issues. Sixty-one percent (69 out of 114 responses)
are willing to deﬁne their own custom rules to be used by
the tool (Q18.6). Nevertheless, only 35% (40 out of 114
responses) have experience deﬁning custom rules for the tools.
From them, many have answered that they have the role of
Security Analyst (Cramer V=0.377 p=0.0026, N=108). Those
participants who are willing to deﬁne custom rules are in
teams that have testing responsibilities (Q18.8-Q2, Cramer
V=0.313 p=0.0163, N=114). Eighty-three percent (95 out
of 114 responses) of the participants are willing to provide
feedback to the tools in terms of marking false positives to
get better results in future runs of the tools (Q18.7).

When observing the results from ABC, there is a slight
higher willingness with 63% (17 out of 27 responses) to
conﬁgure or with 89% (24 out of 27 responses) to provide
feedback to the tools. For the questions on the quality of the
results, there are only minor differences (under 3 %) except
for one: 78% (21 out of 27 responses) of the participants from
ABC think that the tools ﬁnd real issues regularly, compared
to 68% (59 out of 87 responses) to the rest.

Finally, we asked the participants in the interviews if they
allow their software developers to invest time in providing
feedback to the SPA tools. Fourteen participants answered,
of which only one did not agree. The reasoning behind not
allowing this is that there is a risk of providing feedback, due
to lack of expertise, may reduce the true warnings.

Our results show that users consider static code analysis tools
fast enough but still have issues with the high number of false
warnings. The messages of the warnings are helpful for most
of the users to ﬁx the issues. Most users are willing to invest
time in adapting the rules of the tools or provide feedback for
improved future results.

C. Culture and processes for using SPA tools (RQ3)

The most popular tools listed by the participants from
Section IV-A are free tools. The most popular commercial
tools listed are CheckMarx and Fortify, listed only 5 and 4
times, correspondently (Q17). We asked the product owners
and executives about their opinion on open and free tools.
Fourteen have answered this question, of which all allow their
software developers to use free and open source SPA tools.
Even most of them encourage the teams to use open and
free SPA tools. When asked whether there is a budget for
commercial SPA tools, only one participant said that there is
no budget. Ten participants said there is only a budget when
there are requests, whereas six said there is a dedicated budget
for this purpose. One participant commented ”These tools are
a good investment”. Most interviewees said that developers
rarely request commercial tools.

Even though there is a budget for SPA tools in most German
companies that we interviewed, the free SPA tools are still
more popular among the developers than the commercial tools.

The participants from ABC have different options than the
rest of the participants regarding the availability of tools for
secure software development. Fifty-nine percent (16 out of 27
responses) from ABC think that they have the right amount of
tools, whereas only 38% (33 out of 87 responses) have this
opinion in the other companies.

In particular, our study targeted the security aspect of the
processes. We asked for each phase (requirements, design, and
implementation/testing) and whether the security is considered
during the activities. Security is considered differently in each
phase: 57% in requirements (Q4.1), 76% in design (Q6.1),
and 75% in implementation/testing (Q10.1). When we asked
all participants if they use tools to check the security properties
of their code (Q10.3), 52% answered positive (59 out of
114 responses) and 43% negative (49 out of 114 responses).
Moreover, among the participants that use SPA tools, only
17% (11 out of 63 responses) answered that they perform an
ofﬁcial security review before each release (Q22.1), while 80%
do not perform and 3% do not know. Similarly, only 36%
(38 out of 134 responses) said that they perform automatic
security checks after the release is built (Q22.2), while 62%
disagree and 2% do not know. In addition, only 22% said that
they perform automatic security checks on the software while
operating (Q22.3), while 68% disagree and 10% do not know.

There is a correlation showing that the teams, in which
the developer checks the security requirements, are in need
of better tools to accomplish their tasks (Q11-Q15.2, Cramer
V=0.364 p=0.0444, N=67). There is also a correlation show-
ing that when the security requirements are checked, these
checks are done during development time (Q14-Q10.3, Cramer
V=0.376 p=0.00007, N=123). The participants who answered
that
they perform security checks before the release also
have a dedicated security team (Q22.2-Q11, Cramer V=0.455
p=0.000001, N=123) and/or hire external companies for secu-
rity (Q22.2-Q11, Cramer V=0.358 p=0.0001, N=123).

Finally, only 29% of the participants (28 out of 98 re-
sponses) have a clear process how to verify the implementation
concerning security (Q10.4), whereas 64% do not (63 out of
98 responses). Nine percent of the participants (21 out of 221
responses) said that nobody is responsible for checking the the
software’s speciﬁed security requirements (Q11). For 34% (75
out of 221 responses), the programmer does this. Only for 16%
(36 out of 221 responses), this is performed by a security team,
or 9% (20 out of 221 responses) from an external company.

Concerning the security processes, many companies do not
have clear regulations. The executives see value in having tools
and are willing to allocate enough budget.

D. Discussion

In the following, we discuss the ﬁndings from our study,

including few anecdotes.

a) Open-source tools vs. commercial tools: In RQ1, we
found out that software developers more use the free SPA
tools. Additionally, via RQ3, we found that executives are
willing to provide a (more) budget for commercial SPA tools.
These ﬁndings are contrast to some degree, leaving space for
discussion and speculation. One possible explanation might
be that the executives have higher expectations of the tools
and their developers when a costly tool is requested. Hence,
developers are more comfortable using free and open-source
SPA tools. One of the executives in our interviews said:
”They are using more and more tools because they do not
cost anything. They use beautiful graphics, which in many
places leads me to the fact that the relevant thing is that these
numbers are there, but interpretations does not take place.”.
The same executive has further elaborated the expectations
about commercial tools: ”So, if you now put your feet on the
table for 6,000 euros, then at some point someone will stand
in front of the door... You wanted the 6,000 euros, now show
me what you’re doing with it! Most of the time, no one asks
that with free tools. They then say (for the free tools) ’Oh how
nice is that, yes it’s orange. At least it’s not red.”.

Moreover, some executives also have high expectations from
the commercial tools when it comes to the stability of their
processes. One of the product owners in our interviews said:
”A paid tool naturally has the charm that you can hang
the responsibility around the neck of tool manufacturers.”.
Later, referring to the tool manufacturers, he said: ”We’ll
pay them. If anything goes wrong, it’s their fault... It’s a
relatively convenient measure for the managers”. As seen in
this statement, the executive who invests in commercial SPA
tools is shifting the responsibility to the tool manufacturers.

b) Conﬁguring the tools: Concerning RQ2, we asked
the participants about the effort they are willing to put into
improving the tools. This includes activities such as labeling
the ﬁndings or conﬁguring the rules used by the tool i.e.,
writing new rules or changing the existing rules. In the
interviews, one executive said that the interpretation of the
results is much more important for him than improving the
tools. While some executives are not convinced that putting
effort into improving the tools is a good investment of time,
few others stated the opposite. Most participants in the study
think that providing feedback to the tool is a good idea.
Studies have shown that the conﬁguration of the tools requires
codebase-speciﬁc information [38].

c) Fast SPA tools: Via RQ2, we found out that the
participants consider the SPA tools fast enough. In contrast,
previous studies [6], [39] reported the opposite. A possible
explanation for this is that the most popular tools named
among our participants are free SPA tools that perform more
lightweight analyses (pattern-based) compared to most com-
mercial tools with deep data-ﬂow analyses.

d) SPA tools integrations: Developers use the SPA
tools in different workﬂows, e.g., as part of the integrated

development environment (IDE), integrated pipeline builds, or
in external apps such as dashboards. Most executives reported
that they favor having the tools as part of the pipeline, with
other tests, as a continuous integration step, as one said,
”because this is done continuously, permanently anyway”.
However, the developers want fast feedback as they need to
work on issues and improve the code. Most participants in
the study, most of whom are developers, said that the results
should be reported in the IDE. They want the issues to be
reported as early as possible and not wait for a possible slow
build pipeline. Our recommendation for tool manufacturers is
to develop integrations for both views, as many tools on the
market already do.

e) False warnings: The issue of the high number of false
warnings from the SPA tools have been reported in multiple
previous studies. Many of the participants in our interviews
have conﬁrmed this issue as well. Since there is a high interest
from the survey participants in providing feedback to the tools
and reconﬁguring the rules of the tools, the issue of ”false
positives” can be avoided when users are given more time and
appropriate training on how to use the tools more effectively.
Recent studies from the static analysis community have shown
the importance of the conﬁguration of the tools concerning the
results [40]. However, as earlier stated, some executives have
slight scepticism. This requires further research. One possible
factor is the required security expertise of the developers who
are using the tools. This can be conﬁrmed by the fact that
only 69% of the participants reported that they can conﬁrm
which warning is a true positive (Question 17.3). The rest of
the participant need appropriate security training.

V. THREATS TO VALIDITY

We conducted a survey study based on the guidelines by
Kitchenham et al. [41] and Runeson et al. [42]. Next, we
discuss construct validity, external validity, and reliability.

a) Construct Validity: The set of questions used in the
survey and the interviews is the outcome of several workshops
with software security researchers. A possible threat is the
level of expertise that these persons have which inﬂuences
the quality of the questions. Moreover, we avoided the pos-
sibility that the interviewers ask wrong questions as we - the
researchers that conducted this study - held the interviews
ourselves. Moreover, we knew only four of the 17 interviewed
persons upfront - thus, they were not inﬂuenced by us. We pre-
tested our survey and interviews with people from our target
group to identify whether our questions were understandable
and interpreted as intended.

b) External Validity: As our study was conducted only
with companies from Germany, it might be the case that our
results do not apply to companies outside of Germany as they
have other standards and laws. Though several international
standards like ISO IEC 27001 exist.

Even though we made pre-tests, it might be the case that
the survey participants misinterpreted some questions. In our
semi-structured interviews, this possibility is even less as our

interviewees always had the chance to ask whether they under-
stood our questions correctly. Our survey is representative for
our intended target group. However, the number of interviews
might not be sufﬁcient for a representative result.

c) Reliability: The questions of our study are based on
our experience. Three of the authors are in the late stage
of their Ph.D. with +6 years of academic experience, and
the other two authors are on a senoir level with a proven
academic record. To minimize human errors, we used as many
automated tools when possible, e.g., we used Survey Monkey
to collect the survey data, and we recorded all interviews
and used a reliable voice-to-text software for the transcription.
With a self-created script, we processed all raw data wherever
possible.

VI. ETHICS

The participation in the study was voluntary. The partic-
ipants in the interviews signed a consent form. For most
questions, we provided an option for participants not wanting
to give any details (i.e., “I don’t know”). Furthermore, we
aligned our study to the data protection laws in Germany
and the EU. The questions were reviewed at our institution
by multiple researchers, including one expert on professional
trainings and surveys, the head of the department, the data
protection ofﬁcer, and one of the directors.

VII. CONCLUSION

We presented our study among software development teams
in differently-sized companies in Germany. We conducted an
online survey reaching a large number of software developers
and other roles involved in software development and 17 semi-
structured interviews with product owners and executives.

Our study shows that SPA tools are used in different
contexts and only by half of our participants. It conﬁrms
that there are still issues such as the high number of false
warnings, but there are also improvements, such as the analysis
runtime. The participants are willing to conﬁgure the tools
and provide feedback to better results. In the future similar
studies should be conducted in other countries to compare the
situation in different regions and cultures. In Germany, further
studies should be performed to understand why commercial
tools are not requested by the development teams even though
executives are willing to allocate a budget. Future research
should help us understand whether the existing trainings on
security and SPA tools are sufﬁcient, or new ones are needed.

ACKNOWLEDGEMENT

We acknowledge the funding by the project ”AppSecure.nrw
- Security-by-Design of Java-based Applications” of the Euro-
pean Regional Development Fund (ERDF-0801379). We thank
Katharina Altemeier for her contribution to the questionnaires
and to Sebastian Leuer and Boris Budweg for their contribu-
tion in processing the data.

REFERENCES

[1] Grammatech, “Codesonar.” https://www.grammatech.com/products/codesonar,

2021. Online; accessed January 2021. 1

[2] S. Github, “Lgtm.” http://lgtm.com/, 2021. Online; accessed January

2022. 1, 2

[3] Checkmarx, “Checkmarx.” https://www.checkmarx.com/, 2021. Online;

accessed January 2022. 1, 2

[4] Facebook, “Infer.” https://fbinfer.com/, 2021. Online; accessed January

2022. 1

[5] U. of Maryland, “Findbugs.” http://ﬁndbugs.sourceforge.net/, 2021. On-

line; accessed January 2021. 1

[6] M. Christakis and C. Bird, “What developers want and need from
the 31st
program analysis: An empirical study,” in Proceedings of
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2016, (New York, NY, USA), p. 332–343, Association for
Computing Machinery, 2016. 1, 2, 4, 5

[7] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J. Whitehead Jr.,
“Does bug prediction support human developers? ﬁndings from a google
case study,” in Proceedings of the 2013 International Conference on
Software Engineering, ICSE ’13, p. 372–381, IEEE Press, 2013. 1
[8] L. Nguyen Quang Do, J. R. Wright, and K. Ali, “Why do software
developers use static analysis tools? a user-centered study of developer
needs and motivations,” in Proceedings of the Sixteenth Symposium on
Usable Privacy and Security, 2020. 1, 2

[9] C. Sadowski, J. van Gogh, C. Jaspan, E. S¨oderberg, and C. Winter,
“Tricorder: Building a program analysis ecosystem,” in Proceedings of
the 37th International Conference on Software Engineering - Volume 1,
ICSE ’15, p. 598–608, IEEE Press, 2015. 2

[10] L. Luo, M. Schaef, D. Sanchez, and E. Bodden, “Ide support for cloud-
based static analyses,” in ESEC/FSE ’21: 29th ACM Joint European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ESEC/FSE ’21, (New York, NY, USA), Asso-
ciation for Computing Machinery, 2021. To appear. 2

[11] C. Vassallo, S. Panichella, F. Palomba, S. Proksch, A. Zaidman, and
H. C. Gall, “Context is king: The developer perspective on the usage of
static analysis tools,” in 2018 IEEE 25th International Conference on
Software Analysis, Evolution and Reengineering (SANER), pp. 38–49,
2018. 2

[12] T. W. Thomas, H. Lipford, B. Chu, J. Smith, and E. Murphy-Hill, “What
questions remain? an examination of how developers understand an
interactive static analysis tool,” in Twelfth Symposium on Usable Privacy
and Security (SOUPS 2016), (Denver, CO), USENIX Association, June
2016. 2

[13] F. Zampetti, S. Scalabrino, R. Oliveto, G. Canfora, and M. Di Penta,
“How open source projects use static code analysis tools in continuous
integration pipelines,” in 2017 IEEE/ACM 14th International Conference
on Mining Software Repositories (MSR), pp. 334–344, 2017. 2
[14] J. Smith, B. Johnson, E. Murphy-Hill, B. Chu, and H. R. Lipford,
“How developers diagnose potential security vulnerabilities with a static
analysis tool,” IEEE Transactions on Software Engineering, vol. 45,
no. 9, pp. 877–897, 2019. 2

[15] J. Smith, L. Nguyen Quang Do, and E. Murphy-Hill, “Why can’t johnny
ﬁx vulnerabilities: A usability evaluation of static analysis tools for
security,” in Proceedings of the Sixteenth Symposium on Usable Privacy
and Security, SOUPS 2020, 2020. 2

[16] J. Witschey, O. Zielinska, A. Welk, E. Murphy-Hill, C. Mayhorn, and
T. Zimmermann, “Quantifying developers’ adoption of security tools,” in
Proceedings of the 2015 10th Joint Meeting on Foundations of Software
Engineering, ESEC/FSE 2015, (New York, NY, USA), pp. 260–271,
ACM, 2015. 2

[17] C. Weir, I. Becker, and L. Blair, “A passion for security: Intervening
to help software developers,” in 2021 IEEE/ACM 43rd International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP), pp. 21–30, 2021. 2

[18] C. Seaman, “Qualitative methods in empirical studies of software
engineering,” IEEE Transactions on Software Engineering, vol. 25, no. 4,
pp. 557–572, 1999. 2

[19] J. Sp¨ath, K. Ali, and E. Bodden, “Context-, ﬂow-, and ﬁeld-sensitive
data-ﬂow analysis using synchronized pushdown systems,” Proceedings
of the ACM SIGPLAN Symposium on Principles of Programming Lan-
guages, vol. 3, pp. 48:1–48:29, Jan. 2019. 2

[20] T. Tan, Y. Li, X. Ma, C. Xu, and Y. Smaragdakis, “Making pointer
analysis more precise by unleashing the power of selective context
sensitivity,” Proc. ACM Program. Lang., vol. 5, oct 2021. 2

[21] A. Antoniadis, N. Filippakis, P. Krishnan, R. Ramesh, N. Allen, and
Y. Smaragdakis, “Static analysis of java enterprise applications: Frame-
works and caches, the elephants in the room,” in Proceedings of the
41st ACM SIGPLAN Conference on Programming Language Design
and Implementation, PLDI 2020, (New York, NY, USA), p. 794–807,
Association for Computing Machinery, 2020. 2

[22] Y. Lyu, S. Volokh, W. G. J. Halfond, and O. Tripp, “Sand: A static
analysis approach for detecting sql antipatterns,” in Proceedings of the
30th ACM SIGSOFT International Symposium on Software Testing and
Analysis, ISSTA 2021, (New York, NY, USA), p. 270–282, Association
for Computing Machinery, 2021. 2

[23] S. Gitlab, “Semgrep.” https://semgrep.dev/, 2021. Online; accessed

January 2022. 2

[24] Synk and DeepCode, “Snyk.” https://snyk.io/, 2021. Online; accessed

January 2022. 2

[25] Amazon, “Codeguru.” https://aws.amazon.com/codeguru/, 2021. Online;

accessed January 2022. 2

[26] Facebook/Meta, “Marianatrench.” https://mariana-tren.ch/, 2021. On-

line; accessed January 2022. 2

[27] Hiscox,

“Cyber

readiness

report

https://www.hiscox.de/cyber-readiness-report-2021/, 2021.
accessed January 2022. 2

2021.”
Online;

[28] H. M. G. . C. KG, “Heise website.” https://www.heise.de/, 2021. Online;

accessed January 2022. 3

[29] B. e. V., “Bitkom website.” https://www.bitkom.org/, 2021. Online;

accessed January 2022. 3

[30] it’s OWL Clustermanagement GmbH,
https://www.its-owl.de/home/, 2021.
2022. 3

“it’s

owl website.”
accessed January

Online;

[31] Innozent, “Das innovationszentrum f¨ur internettechnologie und multime-
diakompetenz.” https://www.innozent-owl.de/, 2021. Online; accessed
January 2022. 3

[32] S. Monkey, “Survey online tool.” https://www.surveymonkey.com/,

2021. Online; accessed January 2021. 3

[33] F. IEM, “Research project appsecure.nrw.” https://appsecure.nrw, 2021.

Online; accessed August 2021. 3

[34] B. Kitchenham and S. Pﬂeeger, Personal Opinion Surveys, pp. 63–92.

01 2008. 3

[35] L. M. Given, “Convenience sample,” in In The SAGE encyclopedia of
qualitative research methods, vol. 1, pp. 125–125, SAGE Publications,
Inc., 2021. 3

[36] V. Reyes, E. Bogumil, and L. E. Welch, “The living codebook: Docu-
menting the process of qualitative data analysis,” Sociological Methods
& Research, vol. 0, no. 0, p. 0049124120986185, 0. 3

[37] H. Cram´er, Mathematical Methods of Statistics (PMS-9). Princeton

University Press, 2016. 3

[38] G. Piskachev, L. N. Q. Do, and E. Bodden, “Codebase-adaptive de-
tection of security-relevant methods,” in ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA), July 2019. 5
[39] B. Reaves, J. Bowers, S. A. Gorski III, O. Anise, R. Bobhate, R. Cho,
H. Das, S. Hussain, H. Karachiwala, N. Scaife, B. Wright, K. Butler,
W. Enck, and P. Traynor, “*droid: Assessment and evaluation of android
application analysis tools,” ACM Comput. Surv., vol. 49, Oct. 2016. 5
[40] L. Qiu, Y. Wang, and J. Rubin, “Analyzing the analyzers: Flow-
droid/iccta, amandroid, and droidsafe,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
ISSTA 2018, (New York, NY, USA), p. 176–186, Association for
Computing Machinery, 2018. 6

[41] B. Kitchenham, L. Pickard, and S. L. Pﬂeeger, “Case studies for method
and tool evaluation,” IEEE Softw., vol. 12, pp. 52–62, July 1995. 6
[42] P. Runeson, M. Host, A. Rainer, and B. Regnell, Case Study Research
in Software Engineering: Guidelines and Examples. Wiley Publishing,
1st ed., 2012. 6

APPENDIX

The survey consists of 7 parts. We list all questions from

the survey that are relevant for this paper.

Part 1: Questions for all roles

1 To what extent do you agree with the following state-
ments? [likert scale: ”strongly agree”, ”agree”, ”dis-
agree”, ”strongly disagree”, ”do not know”]

1.1 For the topic Secure Software Engineering (SSE) our

team invests the right amount of time.

1.2 In our team, we have members who are responsible for

Security.

1.3 We have clearly deﬁned regulations and policies how

scale: ”strongly agree”, ”agree”, ”disagree”, ”strongly
disagree”, ”do not know”]

8.1 Our current processes should be more precise and clear.
8.2 More or better tools would help us to perform our tasks

with higher quality.

Part 4: Questions related to implementation and testing

9 Is your role involved in implementation and testing?
[single choice: ”Yes”, ”No” (if no, then part 4 is skipped)]
10 Please answer the following questions [single choice:

”Yes”, ”No”, ”I don’t know”]

10.1 Is security considered during implementation?
10.2 Are there templates, in particular standards for imple-

to develop secure software.

menting secure software?

1.4 We have the right amount of tools to support us in

10.3 Do you use tools to automatically check security

developing secure software.

checks of the implemented code?

1.5 Our development process and the existing tools are

10.4 Do you have a process to check the security properties

enough for our needs.

of the code?

1.6 Security requirements (e.g. secure processing of conﬁ-

dential data) are clearly deﬁned in our team.

2 What

is your

regarding your
team responsible for
software product? [multiple choice: ”Deployment/Server
conﬁguration”, ”Programming”, ”Testing”, ”Build pro-
cesses”, ”Software product operation”, ”Requirements”,
”Architecture and design”]

Part 2: Questions related requirements

3 Is your role involved in requirements elicitation? [single

choice: ”Yes”, ”No” (if no, then part 2 is skipped)]

4 Please answer the following questions [single choice:

”Yes”, ”No”, ”I don’t know”]

4.1 Is security considered during activities related to re-

quirements management?

4.2 Do you have security experts that check the security

requirements?

5 To what extent do you agree with the following statement
related to security requirements? [likert scale: ”strongly
agree”, ”agree”, ”disagree”, ”strongly disagree”, ”do not
know”]

5.1 Our current processes should be more precise and clear.
5.2 More or better tools would help us to perform our tasks

with higher quality.

Part 3: Questions related design and architecture

6 Is your role involved in design and architecture? [single

choice: ”Yes”, ”No” (if no, then part 3 is skipped)]

7 Please answer the following questions [single choice:

”Yes”, ”No”, ”I don’t know”]

7.1 Is security considered during activities related to design

and architecture?

7.2 Do you have a process to check the security properties
of the design with respect to the implemented program?
7.3 Do you have security experts that check the architec-

ture from security perspective?

8 To what extent do you agree with the following state-
ment related to secure design and architecture? [likert

11 Who checks the code agains security vulnerabilities?
[multiple choice with option for free text: ”Same person
who wrote the code”, ”Another person from the team
who did not write the code”, ”Internal security team”,
”External security team”, ”Nobody”, ”Other”]

12 Which IDEs are used within your team? [multiple choice
with option for free text: ”Eclipse”, ”IntelliJ Idea”, ”Net-
Beans”, ”Visual Studio Code”, ”Visual Studio”, ”vi /
vim”, ”Notepad++ (or similar editor)”, ”Apple Xcode”,
”Other”]

13 Which programming languages are used primarily within
your
free
team? [multiple choice with option for
text: ”Java”, ”JavaScript/TypeScript”, ”C#”, ”C”, ”C++”,
”Kotlin”, ”Objective-C”, ”Python”, ”Ruby”, ”PHP”,
”Go”, ”SQL”, ”Swift”, ”Rust”, ”R”, ”Other”]

14 When is the program checked against security vulnerabil-
ities? [multiple choice with option for free text: ”During
implementation in the IDE”, ”Before each commit in
the repository”, ”After each commit from the server
pipeline”, ”Before ofﬁcial release”, ”During one sprint”,
”Never”, ”Other”]

15 To what extent do you agree with the following state-
ment related to secure software implementation and test-
ing? [likert scale: ”strongly agree”, ”agree”, ”disagree”,
”strongly disagree”, ”do not know”]

15.1 Our current processes should be more precise and clear.
15.2 More or better tools would help us to perform our tasks

with higher quality.

Part 5: Questions related to SPA tools

16 Does your team use static analysis tools, such as SPA
tools? [single choice: ”Yes”, ”No” (if no, then part 5 is
skipped)]

17 Which SPA tools are used within your team? [open

question]

18 To what extent do you agree with the following state-
ments? [likert scale: ”strongly agree”, ”agree”, ”dis-
agree”, ”strongly disagree”, ”do not know”]

18.1 The tools we use return the results fast enough for our

needs.

18.2 The number of reported warnings that are false (false

positives) is too high.

18.3 I can conﬁrm the true warnings (true positives) easily.
18.4 The tools we use often report true warnings.
18.5 The messages of the warnings help me to ﬁx the issues

in the code.

27 What

is your position? [multiple choice with option
for free text: ”Management”, ”Project lead”, ”Product
imple-
owner”, ”Software development (requirements,
mentation, testing)”, ”Security analyst”, ”Information se-
curity ofﬁcer”, ”Other”]

28 How many years of experience in software development
do you have? [single choice: ”< 2 years”, ”2-5 years”,
”6-10 years”, ”> 10 years”]

18.6 I am willing to write our project-speciﬁc custom rules

29 How many members has your team? [single choice: ”1-

5”, ”6-15”, ”16-30” ”> 30”]

30 What type of applications do you develop? [multiple
choice with option for free text: ”Mobile”, ”Desktop”,
”Web”, ”Embedded”, ”Server”, ”Other”]

for the SPA tools.

18.7 I am willing to label the false warnings to give feed-
back to the tools so that the tools can improve in the
future.

18.8 I have experience in writing custom rules for some of

the tools.

19 Please sort the following statements based on importance,
where 1 has the highest importance. [sorting of statements
from 1 to 4]
– The analysis should ﬁnish within few seconds.
– The messages of the reported ﬁndings should be un-
derstandable and provide hints how to ﬁx the issues.
– It should be easy for me to understand and adapt the

rules of the tools according to my needs.

– The tool should return only very few false warnings.
20 Where should the warnings from the SPA tools be

reported? [sorting of statements from 1 to 3]
– In my IDE (e.g. Eclipse)
– On an internal website (e.g. Jenkins)
– In our ticket system (e.g. Jira)

Part 6: Questions related to software operation and main-

tenance

21 Is your role involved in software operation and mainte-
nance? [single choice: ”Yes”, ”No” (if no, then part 6 is
skipped)]

22 Please answer the following questions [single choice:

”Yes”, ”No”, ”I don’t know”]

22.1 Does your team performs a ﬁnal security review before

each release?

22.2 Are there automatic security checks for each release?
22.3 Are there automatic security checks during operation?
23 To what extent do you agree with the following state-
ments related to secure software operation and main-
tenance? [likert scale: ”strongly agree”, ”agree”, ”dis-
agree”, ”strongly disagree”, ”do not know”]

23.1 Our current processes should be more precise and clear.
23.2 More or better tools would help us to perform our tasks

with higher quality.
Part 7: Meta-data questions
24 How many employees has your company? [single
choice: ”1-3”, ”4-10”, ”11-50”, ”51-250”, ”251-1000”,
”> 1000”]

25 In which domain operate your company? [open question]
26 How many employees work in software development?

[single choice: ”1-50”, ”51-250”, ”> 250”]

