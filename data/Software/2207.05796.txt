Estimating Test Performance for AI Medical Devices under Distribution Shift
with Conformal Prediction

Charles Lu * 1 Syed Rakin Ahmed * 1 2 Praveer Singh 1 3 Jayashree Kalpathy-Cramer 1 3

Abstract

Estimating the test performance of software AI-
based medical devices under distribution shifts is
crucial for evaluating the safety, efﬁciency, and
usability prior to clinical deployment. Due to
the nature of regulated medical device software
and the difﬁculty in acquiring large amounts of
labeled medical datasets, we consider the task
of predicting the test accuracy of an arbitrary
black-box model on an unlabeled target domain
without modiﬁcation to the original training pro-
cess or any distributional assumptions of the orig-
inal source data (i.e. we treat the model as a
“black-box” and only use the predicted output
responses). We propose a “black-box” test es-
timation technique based on conformal predic-
tion and evaluate it against other methods on
three medical imaging datasets (mammography,
dermatology, and histopathology) under several
clinically relevant types of distribution shift (in-
stitution, hardware scanner, atlas, hospital). We
hope that by promoting practical and effective es-
timation techniques for black-box models, man-
ufacturers of medical devices will develop more
standardized and realistic evaluation procedures
to improve the robustness and trustworthiness of
clinical AI tools.

2
2
0
2

l
u
J

2
1

]

G
L
.
s
c
[

1
v
6
9
7
5
0
.
7
0
2
2
:
v
i
X
r
a

1. Introduction

Developing and evaluating AI healthcare software is ex-
tremely expensive due to the collection and curation of
large amounts of medical data by clinical specialists. Even
more costly is the process of submitting medical device
software for regulatory approval, integrating it into existing
clinical data infrastructure, training clinicians on appropri-

*Equal contribution 1Department of Radiology, Massachusetts
General Hospital 2Department of Biophysics, Harvard University
3Department of Ophthalmology, University of Colorado. Corre-
spondence to: Charles Lu <clu@mgh.harvard.edu>.

Presented at ICML Workshop on Principles of Distribution Shift
2022,Copyright 2022 by the author(s).

ate usage, and continual post-deployment monitoring for
failures.

Thus, it would be extremely valuable to be able to accu-
rately estimate the prediction performance of models at
new hospitals, patient populations, medical scanner equip-
ment, etc. before actual clinical deployment. From a reg-
ulatory point of view, unusually low (or high) test perfor-
mance could indicate unidentiﬁed or unmitigated risks that
could degrade the quality of AI medical devices in differ-
ent scenarios. For example, drift in disease prevalence
over time might be quantiﬁed and necessitate periodic re-
calibration of the AI algorithm by the device manufacturer.

This problem of trying to identify and rectify perfor-
mance degradation under new data populations has been
extensively studied as distribution shift, out-of-distribution
detection, and domain generalization (Yang et al., 2021;
Zhou et al., 2021). Recent works have begun to investigate
techniques and frameworks for estimating test performance
on unlabeled, domain-shifted distribution.

Deng & Zheng (2020) introduced the notion of predict-
ing performance on an unlabeled test set and, for this
task, evaluated a regression approach using feature vectors
from models trained under different distribution shifts. A
simpler and more practical technique for estimating accu-
racy on an unlabeled target distribution was proposed by
Garg et al. (2022), which selects a conﬁdence threshold us-
ing accuracy on a source dataset.

In this paper, we propose a novel estimation technique
based on conformal prediction and evaluate it against other
“black-box” test estimation methods on several medical
imaging datasets.

2. Estimating “Black-box” Test Performance

We make the assumption that the model of a software
medical device will be “static” and not directly accessible.
Therefore, for an estimation test method to be valid for
an arbitrary black-box model, we assume only access to
predicted outputs from the model from which to estimate
performance. For example, for a multi-class classiﬁcation
model, F , we might only receive a predicted response of
probability scores, ∆|Y|, where |Y| is the number of classes

 
 
 
 
 
 
Accepted to ICML Workshop on PODS 2022

and ∆ is the probability simplex. Additionally, we assume
we can access to a portion of the source distribution’s la-
beled test set, {(x, y)}m
i=1 ∼ DS, to use to estimate the
target distribution unlabeled test set {x}n

i=1 ∼ DT .

While there are existing techniques for domain adaption
and distribution shift, they necessarily assume a modiﬁable
training procedure or preexisting knowledge of the type or
character of distribution shift (e.g.
label shift, covariate
shift, etc.) (Lipton et al., 2018). Absent these restrictions,
our objective is to empirically evaluate arbitrary black-box
models for medical applications that might be easily imple-
mented into current deployment practices for AI medical
devices or otherwise useful in informing regulatory testing
and monitoring of machine learning algorithms.

A ﬁrst intuitive approach to estimating test performance on
target data would be to use the maximum softmax prob-
ability as an estimate of accuracy (Hendrycks & Gimpel,
2016). While deep learning models have been found to be
poorly calibrated (Guo et al., 2017), more recent work ﬁnds
newer architectures to be better calibrated than previously
thought (Minderer et al., 2021). Therefore, one method to
estimate test accuracy is to calculate the average conﬁdence
(AC) score on the unlabeled target test dataset:

AC(X T ) = E

x∼DT [maxj∈Y F (x)j] ,

(1)

where j is the index of a particular label’s softmax score.

Another general approach involves computing the differ-
ence between source and target distributions. Guillory et al.
(2021) proposes one method, which does not require model
access, that estimates target accuracy with the source ac-
curacy plus the difference in conﬁdence (DOC) between
source and target datasets:

DOC(X S , Y S, X T ) = E

(x,y)∼DS

I

arg maxy∈YF (x)j = y

E

x∼DT

(cid:2)
F (x)j

(cid:21)

(cid:2)
− E

max
j∈Y

(cid:20)

x∼DS

max
j∈Y

(cid:20)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where I is the indicator function.

F (x)j

(cid:3)(cid:3)
,

(cid:21)(cid:12)
(cid:12)
(2)
(cid:12)
(cid:12)

2.1. Estimating Test Accuracy with Conformal

Prediction

Conformal prediction is an approach to distribution-free
uncertainty quantiﬁcation that provides marginal coverage
guarantees on arbitrary models, such as deep convolutional
neural networks (Angelopoulos & Bates, 2021). In confor-
mal prediction, a score threshold, ˆt, is estimated such that
the prediction set, T (x) = {j ∈ Y | s(x)j > ˆt}, contains
the true label on average, P (Y ∈ T (X)) ≥ 1 − α, where
(Vovk et al.,
α is some user-speciﬁed conﬁdence level.
2005). Therefore a prediction on a speciﬁc data point will
be a set of plausible classes instead of only that class with
the highest softmax score. The marginal coverage guar-
antee of conformal prediction assumes that t is calibrated
on a held-out set drawn exchangeably from the test distri-
bution. Naturally, this assumption is violated under dis-
tribution shift. While conformal prediction can be modi-
ﬁed if the type of shift is known (Tibshirani et al., 2019;
Podkopaev & Ramdas, 2021), we ﬁnd that base conformal
prediction by itself to be a useful measure of uncertainty
by which to estimate test accuracy on the target distribu-
tion. The basic intuition is that the more data points with
large prediction set sizes, the more difﬁcult for the model
to classify these points, and subsequently produce a lower
estimated test performance.

We propose conformal prediction conﬁdence (CPC) to es-
timate test accuracy on unlabeled target datasets. We use
the conformal prediction method by Sadinle et al. (2019)
and estimate ˆt using a small subset of source data and eval-
uate two methods of conformal prediction conﬁdence by
setting α to be the source accuracy (CPC-ACC) or the aver-
age conﬁdence of the target dataset (CPC-AC). To estimate
test accuracy on target distribution, we take the expected
value of the average softmax score of each prediction set in
the target dataset to be the estimated test accuracy.

CP C(X T ) = E

x∼DT [E [{s(x)j | j ∈ T (x)}]]

(4)

A third approach is to choose a score threshold on the
source distribution to estimate the test accuracy on the tar-
get distribution. Average threshold conﬁdence (ATC) is a
technique that estimates test accuracy as the expected num-
ber of target data points above a threshold:

AT C(X T ) = E

x∼DT

I

(cid:20)

max
j∈Y

(cid:20)

s(x)j > t

,

(cid:21)(cid:21)

(3)

where t is a threshold estimated on some score func-
tion, s : ∆k → R function. Garg et al. (2022) pro-
poses to choose t such that the percentage of data points
with a score above the threshold is equal
to the ac-
curacy on the source dataset: Ex∼DS [I [s(x) > t]] =
(x,y)∼DS (cid:2)I (cid:2)arg maxj∈Y F (x)j = y(cid:3)(cid:3).
E

See Algorithm 1 for implementation details of CPC-ACC.

Intuitively, more conﬁdent prediction will have smaller set
sizes and vice versa. A highly conﬁdent prediction may
have only one element in its set with a high score while a
less conﬁdent prediction may have many elements in the
prediction set. Thus, we would expect the average softmax
score for low conﬁdence predictions (sets with many items)
to be less than high conﬁdence predictions (sets with few
items with high scores). Then the average of these low and
high conﬁdence scores can be taken as an estimate of over-
all accuracy (in the special case of every prediction set only
having one item, namely the maximum score, CPC-AC is
equivalent to AC).

Accepted to ICML Workshop on PODS 2022

ATC-MC

ATC-MC

ATC-NE

ATC-NE

AC

AC

DOC

DOC

CPC-ACC

CPC-ACC

CPC-AC

CPC-AC

+0%

+20%

+40%

+60%

-10% -5% +0% +5% +10%

target accuracy error

target accuracy error

EfficientNet-B0

ResNet18

target test accuracy

EfficientNet-B3

DenseNet121

Figure1. Comparison of test estimation methods using four different architectures on Fitzpatrick dataset (atlas 1 → atlas 2); left if
without temperature scaling; right is with temperature scaling.

3. Experiments

We conduct experiments with four datasets with the associ-
ated distribution shifts:

• CIFAR-10 (Krizhevsky, 2009) – The source dataset is
the original CIFAR-10 (25,000 train / 25,000 valida-
tion / 10,000 test), while the target dataset is CIFAR-
10.1 (Recht et al., 2018) (2,000 test, 10 classes of
62,000 natural images)

• Fitzpatrick17K (Groh et al., 2021) – The source
dataset is the DermaAmin atlas (7295 train / 811 val-
idation / 3474 test), while the target dataset is Atlas
Dermatologico (3889 test, 114 lesion classes in der-
matology images)

• DMIST (Pisano et al., 2005) – Distribution shift is
evaluated between four scanner types (1: 12421, 2:
47896, 3: 41311, 4: 6562) and from DMIST (33 in-
stitutions) to an external healthcare institution (10,819
test) in a multi-class breast density rating task of mam-
mograms.

• WILDS-Camelyon17

2021;
(Sagawa et al.,
Bandi et al., 2018) – Distribution shift
is evalu-
ated under source hospitals (302,436 train / 34,904
validation / 33,560 test) to target hospital (85,054 test)
for metastases detection (binary) from histopathology.

We evaluate a variety of network architectures and perform
ﬁve training runs for each model. Additionally, we com-
pare results with and without temperature scaling (TS) to
study the effects of softmax calibration on test estimation
methods (although access to the original logits will not

likely be available in most commercial models) (Guo et al.,
2017).

We report state-of-the-art test accuracy prediction using
CPC on several medical imaging datasets without TS and
competitive performance post-TS, summarized in Table 1.
The performance gains with CPC are especially notice-
able on the Fitzpatrick17K dataset, which contains a much
larger number of classes compared to other datasets.

Interestingly, we observe that TS moderates the effect of
overestimation of predicted test accuracy across methods
and architectures, as can be seen in Figures 1– 3. We hy-
pothesize that further analysis of the effect of model archi-
tecture on different medical datasets and tasks will be cru-
cial in developing robust and reliable performance estima-
tion techniques for medical applications. Other follow-up
work might extend test estimation methods to other clini-
cally relevant tasks such as semantic segmentation, medical
image registration, and automatic medical report classiﬁca-
tion.

4. Clinical Relevance

When considering clinical deployment, the majority of
work incorporating AI for clinical classiﬁcation tasks
has traditionally focused on model accuracy and classi-
these
ﬁcation performance on in-distribution (ID) data;
models tend to break down when inferring on out-of-
distribution (OOD) data, which could be images acquired
from different cameras, scanners, and/or institutions that
may have different inherent modality, view, and/or qual-
ity. This brittleness of models is evidenced by examples
of one-pixel attacks (Su et al., 2019) and pose-perturbed

Accepted to ICML Workshop on PODS 2022

Table1. Comparison by dataset and distribution shift using EfﬁcientNet-B0 averaged over 5 training runs.

DATASET

SOURCE → TARGET (ACCURACY %)

METHOD

ABSOLUTE TARGET ERROR

NO SCALING

WITH SCALING

CIFAR-10

ORIGINAL (83%) → CIFAR-10.1 (71%)

FITZPATRICK17K ATLAS 1 (37%) → ATLAS 2 (51%)

SCAN. 1 (70%) → SCAN. 2 (58%)

SCAN. 1 (70%) → SCAN. 3 (68%)

DMIST

SCAN. 1 (70%) → SCAN. 4 (51%)

DMIST (67%) → EXTERNAL INST. (52%)

19.8% ± 1.4%
ATC-MC
25.3% ± 1.2%
ATC-NE
19.7% ± 0.9%
AC
18.4% ± 1.2%
DOC
CPC-ACC 19.9% ± 0.8%
16.2% ± 1.0%
CPC-AC

35.9% ± 0.7%
ATC-MC
44.5% ± 0.6%
ATC-NE
21.4% ± 1.0%
AC
23.2% ± 0.8%
DOC
CPC-ACC 17.3% ± 1.1%
1.1% ± 0.7%
CPC-AC

26.8% ± 6.5%
ATC-MC
33.5% ± 9.6%
ATC-NE
25.7% ± 3.6%
AC
22.7% ± 8.9%
DOC
CPC-ACC 26.0% ± 3.8%
21.4% ± 3.3%
CPC-AC
22.5% ± 1.8%
ATC-MC
28.7% ± 4.4%
ATC-NE
19.2% ± 3.6%
AC
16.7% ± 7.8%
DOC
CPC-ACC 19.4% ± 3.9%
17.1% ± 2.2%
CPC-AC

1.6% ± 0.7%
0.7% ± 0.7%
6.0% ± 0.5%
4.8% ± 0.7%
8.6% ± 0.6%
15.7% ± 1.5%

2.2% ± 2.3%
2.7% ± 2.3%
3.7% ± 1.7%
2.4% ± 1.1%
0.8% ± 0.5%
8.7% ± 1.0%
4.8% ± 2.0%
4.9% ± 1.7%
11.8% ± 12.1%
8.8% ± 0.5%
13.2% ± 11.6%
9.3% ± 0.6%

3.2% ± 1.6%
3.9% ± 1.3%
6.7% ± 11.3%
2.4% ± 1.0%
6.9% ± 11.5%
3.8% ± 5.1%

32.1% ± 7.2%
ATC-MC
37.4% ± 9.5%
ATC-NE
31.8% ± 4.1%
AC
28.8% ± 9.8%
DOC
CPC-ACC 32.7% ± 4.3%
CPC-AC

5.6% ± 2.6%
3.6% ± 4.0%
17.3% ± 12.5%
14.3% ± 2.2%
20.1% ± 11.6%
24.4% ± 11.0% 14.4% ± 7.5%
9.6% ± 5.3%
35.8% ± 5.6%
15.8% ± 5.6%
38.8% ± 6.2%
36.8% ± 2.0%
15.7% ± 12.9%
20.5% ± 15.6% 8.2% ± 4.2%

ATC-MC
ATC-NE
AC
DOC
CPC-ACC 26.3% ± 2.7%
21.1% ± 7.3%
CPC-AC

16.8% ± 11.2%
11.7% ± 4.1%

8.0% ± 5.3%
8.0% ± 5.6%
7.7% ± 12.9%
4.1% ± 1.6%
7.9% ± 4.2%
4.2% ± 3.5%

CAMELYON17

HOSPITAL 1 (99%) → HOSPITAL 2 (85%)

12.9% ± 3.0%
ATC-MC
12.9% ± 3.0%
ATC-NE
9.8% ± 2.8%
AC
20.9% ± 7.5%
DOC
CPC-ACC 8.8% ± 2.8%
11.8% ± 1.3%
CPC-AC

In
images (Alcorn et al., 2019) fooling neural networks.
the clinical realm, this imposes considerable concern and
distress: “black-box” models that misclassify OOD data
can incorrectly disqualify patients from clinical trials or re-
sult in unnecessary procedures. This is particularly rele-
vant for models designed for radiogenomic prediction in
tumors, where incorrect classiﬁcation of a tumor-speciﬁc
genetic marker status could, at worst, prevent patients from
accessing the appropriate targeted therapy.

A second concern is the lack of deﬁnitive ground truth
and uncertainty with labeling data in speciﬁc clinical do-

mains. Prominent examples of this include the signiﬁcant
inter-rater variability in ascertaining the pre-neoplastic or
neoplastic status of cervical lesions and the possible pres-
ence of intra/inter-lesional genetic heterogeneity for brain
tumors, both of which can cause “black-box” AI models to
fail on external data. Approaches that create site-speciﬁc or
dataset-speciﬁc models are often unfeasible for these clin-
ical tasks since this adds to the complexity of having to
train and deploy multiple models for the same task; multi-
ple networks also pose additional logistical concerns when
seeking FDA approval for deployment.

Accepted to ICML Workshop on PODS 2022

References

Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku,
W.-S., and Nguyen, A. Strike (with) a pose: Neural
networks are easily fooled by strange poses of familiar
In Proceedings of the IEEE/CVF Conference
objects.
on Computer Vision and Pattern Recognition, pp. 4845–
4854, 2019.

Angelopoulos, A. N. and Bates, S. A gentle introduction
to conformal prediction and distribution-free uncertainty
quantiﬁcation. CoRR, abs/2107.07511, 2021. URL
https://arxiv.org/abs/2107.07511.

Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balken-
hol, M., Hermsen, M., Bejnordi, B. E., Lee, B., Paeng,
K., Zhong, A., et al. From detection of individual metas-
tases to classiﬁcation of lymph node status at the patient
level: the camelyon17 challenge. IEEE Transactions on
Medical Imaging, 2018.

Deng, W. and Zheng, L. Are labels necessary for classi-
ﬁer accuracy evaluation? CoRR, abs/2007.02915, 2020.
URL https://arxiv.org/abs/2007.02915.

Garg,

S.,

Balakrishnan,

S.,

Lipton,

Neyshabur, B., and Sedghi, H.
labeled
to
formance.
https://arxiv.org/abs/2201.04234.

CoRR, abs/2201.04234, 2022.

out-of-distribution

predict

data

Z. C.,
Leveraging un-
per-
URL

Groh, M., Harris, C., Soenksen, L., Lau, F., Han, R.,
Kim, A., Koochek, A., and Badri, O. Evaluating deep
neural networks trained on clinical images in derma-
tology with the ﬁtzpatrick 17k dataset. arXiv preprint
arXiv:2104.09957, 2021.

Guillory, D., Shankar, V., Ebrahimi, S., Darrell, T., and
Schmidt, L.
Predicting with conﬁdence on unseen
distributions. CoRR, abs/2107.03315, 2021. URL
https://arxiv.org/abs/2107.03315.

Guo, C., Pleiss, G., Sun, Y.,

and Weinberger,
On calibration of modern neural net-
CoRR, abs/1706.04599, 2017.
URL

K. Q.
works.
http://arxiv.org/abs/1706.04599.

Hendrycks, D. and Gimpel, K. A baseline for detecting
misclassiﬁed and out-of-distribution examples in neu-
ral networks. CoRR, abs/1610.02136, 2016. URL
http://arxiv.org/abs/1610.02136.

Krizhevsky, A. Learning multiple layers of features from

tiny images. Technical report, 2009.

Lipton, Z. C., Wang, Y., and Smola, A. J.

De-
tecting and correcting for label shift with black box
CoRR, abs/1802.03916, 2018.
predictors.
URL
http://arxiv.org/abs/1802.03916.

Minderer, M., Djolonga, J., Romijnders, R., Hubis,
F., Zhai, X., Houlsby, N., Tran, D., and Lucic,
Revisiting the calibration of modern neural
M.
networks.
URL
https://arxiv.org/abs/2106.07998.

CoRR, abs/2106.07998, 2021.

Pisano, E. D., Gatsonis, C., Hendrick, E., Yaffe, M.,
Baum, J. K., Acharyya, S., Conant, E. F., Fajardo,
L. L., Bassett, L., D’Orsi, C., Jong, R., and Reb-
Diagnostic performance of digital versus
ner, M.
ﬁlm mammography
screening.
New England Journal of Medicine, 353(17):1773–
1783, 2005.
doi: 10.1056/NEJMoa052911. URL
https://doi.org/10.1056/NEJMoa052911.
PMID: 16169887.

breast-cancer

for

Podkopaev, A. and Ramdas, A. Distribution-free uncer-
tainty quantiﬁcation for classiﬁcation under label shift,
2021.

Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
2018.

Do cifar-10 classiﬁers generalize to cifar-10?
https://arxiv.org/abs/1806.00451.

Sadinle, M., Lei, J., and Wasserman, L. Least ambiguous
set-valued classiﬁers with bounded error levels. Journal
of the American Statistical Association, 114(525):223–
234, 2019. doi: 10.1080/01621459.2017.1395341. URL
https://doi.org/10.1080/01621459.2017.1395341.

Sagawa, S., Koh, P. W., Lee, T., Gao, I., Xie, S. M., Shen,
K., Kumar, A., Hu, W., Yasunaga, M., Marklund, H.,
Beery, S., David, E., Stavness, I., Guo, W., Leskovec,
J., Saenko, K., Hashimoto, T., Levine, S., Finn, C., and
Liang, P. Extending the WILDS benchmark for unsuper-
vised adaptation. CoRR, abs/2112.05090, 2021. URL
https://arxiv.org/abs/2112.05090.

Su, J., Vargas, D. V., and Sakurai, K. One pixel attack
for fooling deep neural networks. IEEE Transactions on
Evolutionary Computation, 23(5):828–841, 2019.

Tibshirani, R. J., Barber, R. F., Candes, E. J., and Ram-
das, A. Conformal prediction under covariate shift, 2019.
URL https://arxiv.org/abs/1904.06019.

Vovk, V., Gammerman, A., and Shafer, G. Algorithmic
Learning in a Random World. Springer-Verlag, Berlin,
Heidelberg, 2005. ISBN 0387001522.

Yang,

J., Zhou, K., Li, Y., and Liu, Z.

eralized
vey.
https://arxiv.org/abs/2110.11334.

out-of-distribution
CoRR,

abs/2110.11334,

detection:

2021.

Gen-
A sur-
URL

Zhou, K., Liu, Z., Qiao, Y., Xiang, T.,

Loy, C. C.
vey.
https://arxiv.org/abs/2103.02503.

Domain generalization:
2021.
abs/2103.02503,

CoRR,

and
A sur-
URL

A. Appendix

Accepted to ICML Workshop on PODS 2022

ATC-MC

ATC-MC

ATC-NE

ATC-NE

AC

AC

DOC

DOC

CPC-ACC

CPC-ACC

CPC-AC

CPC-AC

+0%

+10%

+20%

+30%

+0%

+5%

+10%

+15%

target accuracy error

target accuracy error

EfficientNet-B0

ResNet18

target test accuracy

EfficientNet-B3

DenseNet121

Figure2. Comparison of test estimation methods using four different architectures on CIFAR-10 dataset (original test set → CIFAR-10-1
test set); left if without temperature scaling; right is with temperature scaling.

ATC-MC

ATC-MC

ATC-NE

ATC-NE

AC

AC

DOC

DOC

CPC-ACC

CPC-ACC

CPC-AC

CPC-AC

+0% +10% +20% +30% +40% +50%

-10% +0% +10% +20% +30% +40%

target accuracy error

target accuracy error

EfficientNet-B0

ResNet18

target test accuracy

EfficientNet-B3

DenseNet121

Figure3. Comparison of test estimation methods using four different architectures on DMIST dataset (internal institutions → external
institution); left if without temperature scaling; right is with temperature scaling.

Accepted to ICML Workshop on PODS 2022

ATC-MC

ATC-MC

ATC-NE

ATC-NE

AC

AC

DOC

DOC

CPC-ACC

CPC-ACC

CPC-AC

CPC-AC

+0%

+5% +10% +15% +20%

-10% -5% +0% +5% +10% +15%

target accuracy error

target accuracy error

EfficientNet-B0

ResNet18

target test accuracy

EfficientNet-B3

DenseNet121

Figure4. Comparison of test estimation methods using four different architectures on Camelyon dataset (hospital 1 → hospital 2); left
if without temperature scaling; right is with temperature scaling.

i=1 ∼ DS,

scal ← scal ∩ max F (xi)

i=1 ∼ DT ,
(x,y)∼DS (cid:2)I (cid:2)arg maxj∈Y f (x)j = y(cid:3)(cid:3),

Algorithm 1 Conformal prediction conﬁdence using source accuracy (CPC-ACC)
Input:
source calibration set {(xi, yi)}k
target test set {(xi, yi)}n
conﬁdence level α = E
model F : X → ∆|Y|,
quantile function Q : {R}m × [0, 1] → R
1: c ← {}
2: for i ∈ {1, 2, . . . m} do
3:
4: end for
5: ˆt ← Q(s, ⌈α·(m+1)⌉
6: p ← 0
7: for i ∈ {1, 2, . . . n} do
8:
9:
10:
11: end for
12: p ← p/n
Output: estimated target accuracy p

s ← F (Xi)
τ ← {j ∈ Y | sj > ˆt}
p ← p + 1
|τ | Pj∈τ sj

m

)

