Springer Nature 2021 LATEX template

2
2
0
2

l
u
J

8

]

G
L
.
s
c
[

1
v
7
5
7
3
0
.
7
0
2
2
:
v
i
X
r
a

Combining Deep Learning with Good
Old-Fashioned Machine Learning

Moshe Sipper1*

1*Department of Computer Science, Ben-Gurion University, Beer
Sheva, 84105, Israel.

Corresponding author(s). E-mail(s): sipper@bgu.ac.il;

Abstract

We present a comprehensive, stacking-based framework for combining
deep learning with good old-fashioned machine learning, called Deep
GOld. Our framework involves ensemble selection from 51 retrained pre-
trained deep networks as ﬁrst-level models, and 10 machine-learning
algorithms as second-level models. Enabled by today’s state-of-the-art
software tools and hardware platforms, Deep GOld delivers consistent
improvement when tested on four image-classiﬁcation datasets: Fashion
MNIST, CIFAR10, CIFAR100, and Tiny ImageNet. Of 120 experiments,
in all but 10 Deep GOld improved the original networks’ performance.

Keywords: machine learning, deep learning, image analysis, pattern
recognition

1 Introduction

The rapid rise of artiﬁcial intelligence (AI) in recent years has been accom-
panied (and enabled) by staggering advances both in software and hardware
technologies. Tools such as PyTorch [1] for deep learning (DL), scikit-learn for
machine learning (ML) [2, 3], and graphics processing unit (GPU) hardware,
all enable faster and better prototyping and deployment of AI systems than
was possible a mere half-decade ago.

While deep learning has taken the world by storm, often—it would seem—
at the expense of other computational paradigms, these (plentiful) latter
are still quite alive and kicking. We propose herein to revisit stacking-based

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Combining Deep Learning with Good Old-Fashioned Machine Learning

modeling [4], but within a comprehensive framework enabled by modern
state-of-the-art software packages and hardware platforms.

As previously argued by [5, 6], signiﬁcant improvement can be attained
by making use of models we are already in possession of anyway, through
what they termed “conservation machine learning”: conserve models across
runs, users, and experiments–—and make use of all of them. Herein, focusing
on image-classiﬁcation tasks, we ask whether, given a (possibly haphazard)
collection of deep neural networks (DNNs), can the tools at our disposal—
speciﬁcally, “good old-fashioned” ML algorithms, many of which have been
around for quite some time—help improve prediction accuracy.

To wit, can we combine DL and ML in a manner that improves DL per-
formance? We answer positively, with a major novelty being the use of most
DL and ML models to date within a single, comprehensive framework.

Section 2 discusses related previous work. Section 3 describes Deep GOld—
Deep Learning and Good Old-Fashioned Machine Learning—which employs
51 deep networks and 10 ML algorithms. Section 4 presents the results
of 120 experiments over four image-classiﬁcation datasets: Fashion MNIST,
CIFAR10, CIFAR100, and Tiny ImageNet. We end with a discussion in
Section 5 and concluding remarks in Section 6.

2 Previous Work

There are many works that involve some form or other of ensembling several
models, and this section does not serve as a full review, but focuses on those
papers found to be most relevant to our topic.

In an early work, [7] presented a technique called Addemup that uses a
genetic algorithm to search for an accurate and diverse set of trained networks.
Addemup works by creating an initial population of networks, then evolving
new ones, keeping the set of networks that are as accurate as possible while
disagreeing with each other as much as possible. They tested these on three
DNA datasets of about 1000 samples.

A few years later, [8] presented an approach named GASEN (Genetic Algo-
rithm based Selective ENsemble) to select some neural networks from a pool of
candidates, and assign weights to their contributions to the resultant ensemble.
The networks had one hidden layer with ﬁve hidden units. The eﬃcacy of this
method was shown for regression and classiﬁcation problems over structural
(non-image) datasets of a few thousand samples. Another work by [9] studied
ﬁnancial decision applications, wherein a neural-network ensemble prediction
was similarly reached by weighting the decision of each ensemble member.

A more recent example (one of many) of straightforward ensembling is
given in [10], who presented an ensemble neural-network model for real-time
prediction of urban ﬂoods. Their ensemble approach used a number of artiﬁcial
neural networks with identical topology, trained with diﬀerent initial weights.
The ﬁnal result of maximum water level was the ensemble mean. Ensemble
sizes examined were 1, 5, and 10.

Springer Nature 2021 LATEX template

Combining Deep Learning with Good Old-Fashioned Machine Learning

3

In a similar vein, [11] trained multiple neural networks and combined their
outputs using three combination strategies: simple average, weighted average,
and what they termed a meta-learner, which applied a Bayesian regulation
algorithm to the network outputs. The application ﬁeld considered was real-
time production monitoring in the oil and gas industry, speciﬁcally, virtual
ﬂow meters that infer multiphase ﬂow rates from ancillary measurements, and
are attractive and cost-eﬀective solutions to meet monitoring demands, reduce
operational costs, and improve oil recovery eﬃciency.

[12] trained ﬁve convolutional neural networks (CNNs) to detect ankle frac-
tures in radiographic views. Model outputs were evaluated using both one and
three radiographic views. Ensembles were created from a combination of CNNs
after training. They implemented a simple voting method to consolidate the
output from the three views and ensemble of models.

[13] presented a malware detection method called MalNet, which uses a
stacking ensemble with two deep neural networks—CNN and LSTM—as ﬁrst-
level learners, and logistic regression as a second-level learner.

[14] examined neural-network ensemble classiﬁcation for lung cancer disease
diagnosis. They proposed an ensemble of Weight Optimized Neural Network
with Maximum Likelihood Boosting (WONN-MLB), which essentially seeks
to ﬁnd optimal weights for a weighted (linear) majority vote. [15] applied a
neural-network ensemble to intrusion detection, again using weighted majority
voting.

[16] recently presented a cogent case for the use of XGBoost for tabu-
lar data, demonstrating that it outperformed deep models. They also showed
that an ensemble comprising 4 deep models and XGBoost, predicting through
weighted voting, worked best for the tabular datasets considered.

[17] proposed an ensemble DNN for tumor detection in colorectal histology
images. The mechanism consists of weights that are derived from individual
models. The weights are assigned to the ensemble DNN based on their metrics
and the ensemble model is then trained. The model is again re-trained by
freezing all the layers, except for the fully connected and dense layers.

[18] presented an ensemble DL method to detect retinal disorders.
Their method comprised three pretrained architectures—DenseNet, VGG16,
InceptionV3—and a fourth Custom CNN of their own design. The individual
results obtained from the four architectures were then combined to form an
ensemble network that yielded superb performance over a dataset of retinal
images.

[19] examined Deep Q-learning, presenting an ensemble approach that
improved stability during training, resulting in improved average performance.
As noted above, [5, 6] presented conservation machine learning, which con-
serves models across runs, users, and experiments—and makes use of them.
They showed that signiﬁcant improvement could be attained by employing ML
models already available anyway.

Springer Nature 2021 LATEX template

4

Combining Deep Learning with Good Old-Fashioned Machine Learning

Table 1 Datasets.

Dataset

Images

Classes Training Test

Fashion MNIST 28x28 grayscale

CIFAR10
CIFAR100
Tiny ImageNet

32x32 color
32x32 color
64x64 color

10
10
100
200

60,000
50,000
50,000
100,000

10,000
10,000
10,000
10,000

3 Deep GOld: Algorithmic Setup

Stacking (or Stacked Generalization) [4] is an ensemble method that uses mul-
tiple models to tackle classiﬁcation or regression problems. The main idea is
to ﬁrst train diﬀerent models on the original problem. The outputs of these
models are considered to be a ﬁrst level, which are then passed on to a second
level to perform the ﬁnal prediction. The inputs to the second-level model are
thus the outputs of the ﬁrst-level models.

Our framework involves deep networks as ﬁrst-level models and ML meth-
ods as second-level models. For the former we used PyTorch, one of the top-two
most popular and performant deep-learning software tools [1]. The module
torchvision.models contains 59 deep-network models that were pretrained
on the large-scale (over 1 million images), 1000-class ImageNet dataset [20].

Of the the 59 models we retained 51 (8 models proved somewhat unwieldy
or evoked a “not implemented” error). Each of the models was ﬁrst retrained
over the four datasets we experimented with in this paper: Fashion MNIST,
CIFAR10, CIFAR100, and Tiny ImageNet. As seen in Table 1, these datasets
contain between 50,000 and 100,000 greyscale or color images in the training
set, 10,000 images in the test set, with number of classes ranging between 10
– 200. Retraining was necessary since the datasets contain images that diﬀer
in size and number of classes from ImageNet.

For retraining, we replaced the last FC (fully connected) 1000-class layer
with a sequence of blocks comprising three layers: {FC, batchnorm, leaky
ReLU}, denoted FBL. The ﬁnal number of features of the original network
was reduced to the dataset’s number of classes through halving the number of
nodes at each layer, starting with the closest power of 2. Consider en example:
If the original network ended with 600 features, and the dataset contains 100
classes, then our modiﬁed network’s ﬁnal layers comprised a 512-node, 3-layer
FBL block (512 being the closest power of 2 to 600), followed by a 256-node
FBL, followed by a 128-node FBL, and ending with the 100 classes. In addition,
the ﬁrst convolutional layer of the original network needed adjustment in some
cases. The retraining phase is detailed in Algorithm 1.

Once Algorithm 1 is run for all four datasets, we are in possession of 51
trained models per dataset. We can now proceed to perform the two-level
prediction, as detailed in Algorithm 2. Our interest herein was to study what
one can do with models one has at hand. Towards this end, we ﬁrst selected
from the 51 retrained models three random ensembles of networks, of sizes 3, 7,
and 11. Each network of an ensemble was then run over both the training and
test sets of the dataset in question (without any training—only feed-forward

Springer Nature 2021 LATEX template

Combining Deep Learning with Good Old-Fashioned Machine Learning

5

Algorithm 1 Retrain 51 pretrained models

Input:

eﬃcientnet b6,

dataset ← dataset to be used
pretrained ← {alexnet, densenet121, densenet161, densenet169, densenet201,
eﬃcientnet b0, eﬃcientnet b1, eﬃcientnet b2, eﬃcientnet b3, eﬃcientnet b4,
eﬃcientnet b7, mnasnet0 5, mnasnet1 0,
eﬃcientnet b5,
mobilenet v2, mobilenet v3 large, mobilenet v3 small,
reg-
net x 1 6gf,
regnet x 800mf,
regnet x 8gf, regnet y 16gf, regnet y 1 6gf, regnet y 32gf, regnet y 3 2gf, reg-
net y 400mf,
resnet18,
resnet34, resnet50, resnext101 32x8d, resnext50 32x4d, shuﬄenet v2 x0 5, shuf-
ﬂenet v2 x1 0, vgg11, vgg11 bn, vgg13, vgg13 bn, vgg16, vgg16 bn, vgg19,
vgg19 bn, wide resnet101 2, wide resnet50 2} # Networks pretrained over
ImageNet dataset

regnet x 400mf,

regnet y 800mf,

regnet x 3 2gf,

regnet x 32gf,

regnet x 16gf,

regnet y 8gf,

resnet101,

resnet152,

Output:

Retrained models and their test scores

1: Load training set and test set of dataset
2: for net ∈ pretrained do
3:

4:

5:

Tiny ImageNet), optimizer: SGD

Save trained net and test set score

Replace net ﬁnal layer, and possibly adjust ﬁrst convolutional layer
Train entire net for 20 epochs over training set # mini-batch size: 64 (8 for

6: end for

output computation). These ﬁrst-level outputs were then concatenated to form
an input dataset for the second level. For example, if the ensemble contains 7
networks, and the dataset in question is CIFAR100, then the ﬁrst level creates
two datasets: a training set with 50,000 samples and 701 features, and a test
set with 10,000 samples and 701 features (701: 7 networks × 100 classes + 1
target class).

After the ﬁrst level produced output datasets, we passed these along to the

second level, wherein we employed ten ML algorithms:

1. sklearn.linear model.SGDClassifier: Linear classiﬁers with SGD train-

ing.

2. sklearn.linear model.PassiveAggressiveClassifier: Passive Aggres-

sive Classiﬁer [21].

3. sklearn.linear model.RidgeClassifier: Classiﬁer using Ridge regres-

sion.

4. sklearn.linear model.LogisticRegression: Logistic Regression classi-

ﬁer.

5. sklearn.neighbors.KNeighborsClassifier: Classiﬁer implementing the

k-nearest neighbors vote.

6. sklearn.ensemble.RandomForestClassifier: A random forest classiﬁer.

Springer Nature 2021 LATEX template

6

Combining Deep Learning with Good Old-Fashioned Machine Learning

Algorithm 2 Two-level prediction

Input:

dataset ← dataset to be used
ml algs ← {SGDClassiﬁer, PassiveAggressiveClassiﬁer, RidgeClassiﬁer, Logis-
ticRegression, KNeighborsClassiﬁer, RandomForestClassiﬁer, MLPClassiﬁer,
XGBClassiﬁer, LGBMClassiﬁer, CatBoostClassiﬁer}

Output:

Test scores for majority prediction, and for all ML algorithms

# level 1: generate datasets from outputs of retrained networks
1: for i ∈ {3,7,11} do
2:

networks ← pick i networks at random from retrained networks of Algo-

rithm 1

for net ∈ networks do

Run net over training set and test set
Accumulate generated outputs along with (known) targets

end for
Generate 2 datasets, respectively: train-i, test-i

3:

4:

5:

6:

7:

8: end for

# level 2: run ML algorithms over datasets generated by level 1
9: for i ∈ {3,7,11} do
10:

for alg ∈ ml algs do
Load train-i, test-i
Run alg to ﬁt model to train-i
Test ﬁtted model on test-i

end for

11:

12:

13:

14:

15: end for

7. sklearn.neural network.MLPClassifier: Multi-layer Perceptron classi-

ﬁer, with 5 hidden layers of size 64 neurons each.
8. xgboost.XGBClassifier: XGBoost classiﬁer [22].
9. lightgbm.LGBMClassifier: LightGBM classiﬁer [23].
10. catboost.CatBoostClassifier: CatBoost classiﬁer [24].

4 Results

Unsurprisingly, we found signiﬁcant diﬀerences in the runtime of the level-2
ML algorithms (Algorithm 2). While some methods, such as RidgeClassiﬁer
and KNeighborsClassiﬁer, were very fast, usually ﬁnishing within minutes,
others proved slow (notably, XGBClassiﬁer and CatBoostClassiﬁer, which took
several hours). While the number of samples of the generated ML datasets
for the four problems studied is similar (identical to the original datasets—
Table 1), the number of features diﬀers by an order of magnitude: with 10
classes for Fashion MNIST and CIFAR10, 100 classes for CIFAR100, and 200

Springer Nature 2021 LATEX template

Combining Deep Learning with Good Old-Fashioned Machine Learning

7

Table 2 Hyperparameter value ranges and sets used by Optuna.

Algorithm
SGDClassifier

PassiveAggressiveClassifier

RidgeClassifier

LogisticRegression

KNeighborsClassifier

RandomForestClassifier

MLPClassifier

XGBClassifier

LGBMClassifier

CatBoostClassifier

Parameter
alpha
penalty
C
ﬁt intercept
shuﬄe
alpha
solver
penalty
solver
weights
algorithm
n neighbors
n estimators
min weight fraction leaf
max features
activation
solver
hidden layer sizes
n estimators
learning rate
gamma
n estimators
learning rate
bagging fraction
iterations
depth
learning rate

Values
[1e-05, 1]
{‘l2’, ‘l1’, ‘elasticnet’}
[1e-02, 10]
{True, False}
{True, False}
[1e-3, 10]
{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse cg’, ‘sag’, ‘saga’}
{‘l1’, ‘l2’}
{‘liblinear’, ‘saga’}
{‘uniform’, ‘distance’}
{‘auto’, ‘ball tree’, ‘kd tree’, ‘brute’ }
[2, 20]
[10, 1000]
[0, 0.5]
{‘auto’, ‘sqrt’, ‘log2’}
{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}
{‘lbfgs’, ‘sgd’, ‘adam’}
{(64,64), (64,64,64), (64,64,64,64), (64,64,64,64)}
[10, 1000]
[0.01, 0.2]
[0, 0.4]
[10, 1000]
[0.01, 0.2]
[0.5, 0.95]
[2, 10]
[2, 10]
[1e-2, 10]

classes for Tiny ImageNet, the latter two have 10 and 20 times more features
than the former two, respectively. Some ML methods are known to scale less
well with number of features.

ML runtimes for Fashion MNIST and CIFAR10 proved suﬃciently fast to
aﬀord the use of hyperparamater tuning. Towards this end we used Optuna, a
state-of-the-art, automatic, hyperparameter optimization software framework
[25], which we previously used successfully [26, 27]. Optuna oﬀers a deﬁne-
by-run style user API where one can dynamically construct the search space,
and an eﬃcient sampling algorithm and pruning algorithm. Moreover, our
experience has shown it to be fairly easy to set up. Optuna formulates the
hyperparameter optimization problem as a process of minimizing or maximiz-
ing an objective function given a set of hyperparameters as an input. The
hyperparameter ranges and sets are given in Table 2. With CIFAR100 and
Tiny ImageNet we did not use Optuna, but rather ran the ML algorithms with
their default values.

Table 3 presents our results (we set a 10-hour limit on an ML algorithm’s
run of a row in the table, i.e., the level-2 loop of Algorithm 2.) A total of
120 experiments were performed: 4 datasets × ensembles of size 3, 7, and
11 × 10 complete runs per dataset. In each experiment we generated level-1
datasets and then executed the ML algorithms, as delineated in Algorithm 2.
We then compared three values: 1) the test score of the top network amongst
the random ensemble (known from Algorithm 1); 2) the test score of majority
prediction, wherein the predicted class is determined through a majority vote
amongst the ensemble’s networks’ outputs; 3) the test score of the top ML
method. The code is available at https://github.com/moshesipper.

Table 3 Results for ensembles of 3, 7, and 11 random networks. Accuracy scores shown are over test sets. Net: score of best network. Maj: score of
majority prediction. ML: score of best ML method. For the latter, the ML method producing the best score is given in parentheses. RG: Classiﬁer
using Ridge regression; KN: k-nearest neighbors classiﬁer; SG: Linear classiﬁer with SGD training; PA: Passive Aggressive classiﬁer; LR: Logistic
regression; RF: Random Forest classiﬁer; MP: Multi-layer perceptron; LG: LightGBM; XG: XGBoost; CB: Catboost.

Dataset

Fashion-MNIST

CIFAR10

CIFAR100

Tiny ImageNet

Net
91.97%
91.97%
92.32%
92.05%
91.67%
91.98%
92.14%
91.82%
93.86%
91.82%
74.72%
71.67%
82.48%
74.82%
74.95%
75.60%
76.21%
86.90%
86.60%
76.43%
48.86%
48.74%
48.74%
60.08%
47.06%
47.55%
46.55%
61.30%
46.30%
9.58%
55.77%
53.50%
58.34%
58.34%
53.50%
57.23%
58.62%
57.40%
54.76%
57.23%

3 networks
Maj
92.38%
92.05%
92.95%
92.39%
91.40%
92.93%
92.69%
92.27%
92.79%
92.81%
71.00%
72.10%
82.03%
74.68%
76.48%
75.28%
77.77%
85.96%
81.76%
72.62%
51.02%
44.95%
49.47%
55.30%
47.46%
50.94%
43.81%
57.47%
44.65%
33.88%
54.32%
54.83%
48.45%
59.91%
50.97%
47.54%
59.98%
56.14%
53.81%
48.52%

ML
92.40% (KN)
92.50% (KN)
92.80% (KN)
92.61% (KN)
92.00% (KN)
92.74% (KN)
93.19% (KN)
92.51% (RF)
93.94% (KN)
92.42% (RG)
75.87% (KN)
75.05% (RG)
83.30% (KN)
75.05% (KN)
76.26% (KN)
76.79% (KN)
78.27% (KN)
88.16% (LR)
86.92% (KN)
77.42% (SG)
54.69% (KN)
51.68% (KN)
54.08% (KN)
63.67% (SG)
52.48% (RG)
53.64% (RG)
51.19% (KN)
63.86% (RG)
50.30% (SG)
44.23% (SG)
59.97% (RG)
59.61% (LR)
61.60% (RG)
63.05% (RG)
58.10% (RG)
59.88% (RG)
64.69% (RG)
62.76% (LR)
60.57% (RG)
60.31% (RG)

7 networks
Maj
Net
93.12%
92.23%
93.26%
92.40%
93.69%
93.24%
93.15%
94.01%
93.57%
92.95%
93.16%
92.27%
93.24%
93.86%
93.09%
92.27%
93.14%
92.95%
92.85%
93.86%
79.78%
75.60%
80.54%
86.90%
85.50%
87.33%
75.76%
74.72%
82.72%
86.90%
81.31%
83.57%
74.00%
74.30%
86.90% 84.74%
82.54%
86.90%
77.97%
86.60%
55.11%
55.70%
60.11%
60.76%
51.97%
46.38%
54.45%
61.30%
57.22%
61.30%
56.86%
60.08%
47.91%
47.55%
50.85%
48.86%
53.56%
9.58%
57.84%
61.66%
56.06%
57.40%
53.33%
54.83%
60.59%
58.34%
54.83%
55.77%
53.11%
55.47%
61.26%
58.62%
62.41%
58.62%
60.85%
56.16%
57.70%
56.61%
62.00%
58.34%

ML
93.38% (KN)
93.25% (KN)
93.59% (KN)
94.54% (RG)
93.95% (KN)
93.37% (KN)
94.00% (RG)
93.48% (KN)
93.82% (RF)
94.18% (KN)
80.42% (KN)
87.53% (RG)
89.15% (RG)
79.29% (KN)
87.57% (RG)
83.95% (KN)
77.96% (KN)
86.72% (RG)
88.00% (RG)
87.08% (RG)
59.50% (RG)
65.82% (LR)
54.61% (KN)
64.14% (RG)
64.08% (RG)
64.46% (RG)
52.79% (SG)
54.93% (RG)
56.81% (KN)
64.84% (SG)
63.32% (RG)
59.26% (RG)
64.30% (RG)
60.10% (RG)
60.11% (RG)
62.14% (RG)
66.24% (RG)
63.46% (RG)
63.76% (RG)
63.85% (RG)

11 networks
Maj
Net
93.79%
94.22%
93.62%
94.22%
93.57%
93.24%
93.71%
93.24%
94.01%
93.19%
93.86% 93.75%
93.83%
93.63%
93.81%
93.86%
93.78%
94.01%
93.53%
94.01%
80.94%
87.82%
80.09%
83.57%
87.33%
80.82%
86.60% 83.28%
79.93%
86.60%
87.22%
81.54%
86.90% 84.44%
82.26%
86.60%
83.36%
87.82%
85.02%
87.82%
60.10%
60.76%
62.50%
61.66%
57.51%
9.58%
58.43%
9.58%
59.00%
60.08%
56.07%
60.76%
58.29%
9.58%
57.09%
61.30%
59.39%
9.58%
58.78%
61.48%
65.33%
67.30%
63.72%
57.40%
58.45%
57.23%
60.42%
58.62%
60.78%
56.20%
64.33%
67.30%
60.02%
56.20%
62.44%
58.62%
63.05%
58.34%
62.90%
58.62%

ML
94.40% (RG)
94.57% (KN)
94.11% (RG)
93.92% (KN)
94.50% (KN)
93.84% (KN)
94.07% (KN)
94.25% (KN)
94.66% (RG)
94.39% (RG)
88.29% (RG)
84.82% (RG)
89.12% (RG)
86.43% (RG)
87.24% (RG)
88.33% (RF)
86.67% (RG)
87.71% (RG)
89.56% (RG)
89.59% (RG)
66.02% (RG)
67.30% (RG)
66.07% (LR)
64.86% (RG)
64.55% (RG)
64.25% (LR)
64.89% (RG)
65.07% (RG)
66.34% (RG)
65.20% (LR)
70.17% (RG)
66.00% (RG)
64.31% (RG)
65.19% (RG)
63.83% (RG)
69.88% (RG)
64.09% (RG)
65.94% (RG)
65.98% (RG)
66.46% (RG)

S
p
r
i

n
g
e
r
N
a
t
u
r
e

2
0
2
1
LA
T
E
X
t
e
m
p
l
a
t
e

8

C
o
m
b
i
n
i
n
g
D
e
e
p

L
e
a
r
n
i
n
g

w
i
t
h
G
o
o

d

O

l

d

-

F

a

s

h

i

o

n

e

d

M

a

c

h

i

n

e

L

e

a

r

n

i

n

g

Springer Nature 2021 LATEX template

Combining Deep Learning with Good Old-Fashioned Machine Learning

9

5 Discussion

As observed in Table 3, of the total of 120 experiments, an ML algorithm won
in all but 10 experiments (4 were won by the retrained network, and 6 by
majority prediction).

We note that classical algorithms, notably Ridge regression and k-nearest
neighbors, worked best (they account for 104 of the wins). They are also fast,
scalable, and amenable to quick hyperparameter tuning. If one wishes to focus
on a smaller batch of ML algorithms, these two seem like an excellent choice.
As noted in Section 1, we often ﬁnd ourselves in possession of a plethora
of models, either collected by us through many experiments, or by others
(witness our use of pretrained models herein). Beneﬁting from current state-
of-the-art technology, Deep GOld leverages this wealth of models to attain
better performance. One can of course tailor the framework to available deep
networks and to a personal predilection for any ML algorithm(s).

6 Concluding Remarks

We presented Deep GOld, a comprehensive, stacking-based framework for com-
bining deep learning with machine learning. Our framework involves ensemble
selection from 51 retrained pretrained deep networks as ﬁrst-level models, and
10 machine-learning algorithms as second-level models. We demonstrated the
unequivocal beneﬁts of the approach over four image-classiﬁcation datasets.

We suggest a number of paths for future research:

• Further analysis of ML algorithms whose inputs are the outputs of deep
networks. Do some ML methods inherently work better with such datasets?
• Currently, the features for level 2 comprise only the level-1 outputs. We

might enhance this setup through automatic feature construction.

• Train (or retrain) the level-1 networks alongside a level-2 ML model: 1) After
each training epoch of the networks in the ensemble, generate a dataset
from the network outputs; 2) a level-2 ML algorithm then ﬁts a model to
the level-1 dataset; 3) the ML model generates class probabilities, which are
used to ascribe loss values to the networks-in-training.

Acknowledgement

I thank Raz Lapid for helpful discussions.

Compliance with Ethical Standards

Disclosure of potential conﬂicts of interest: M. Sipper declares that he has no
conﬂict of interest.
Research involving human participants and/or animals: This article does not
contain any studies with human participants or animals performed by any of
the authors.

Springer Nature 2021 LATEX template

10

Combining Deep Learning with Good Old-Fashioned Machine Learning

Informed consent: N/A.

References

[1] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: PyTorch: An
imperative style, high-performance deep learning library. arXiv preprint
arXiv:1912.01703 (2019)

[2] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vander-
plas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay,
E.: Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research 12, 2825–2830 (2011)

[3] Scikit-learn: Machine Learning in Python. https://scikit-learn.org/.

Accessed: 2022-1-12 (2022)

[4] Wolpert, D.H.: Stacked generalization. Neural Networks 5(2), 241–259

(1992)

[5] Sipper, M., Moore, J.H.: Conservation machine learning. BioData Mining

13(1), 9 (2020)

[6] Sipper, M., Moore, J.H.: Conservation machine learning: a case study of

random forests. Nature Scientiﬁc Reports 11(1), 3629 (2021)

[7] Opitz, D., Shavlik, J.: Generating accurate and diverse members of a
neural-network ensemble. In: Touretzky, D., Mozer, M.C., Hasselmo, M.
(eds.) Advances in Neural Information Processing Systems, vol. 8. MIT
Press, Cambridge, MA (1996)

[8] Zhou, Z.-H., Wu, J., Tang, W.: Ensembling neural networks: many could

be better than all. Artiﬁcial Intelligence 137(1-2), 239–263 (2002)

[9] West, D., Dellana, S., Qian, J.: Neural network ensemble strategies for
ﬁnancial decision applications. Computers & Operations Research 32(10),
2543–2559 (2005)

[10] Berkhahn, S., Fuchs, L., Neuweiler, I.: An ensemble neural network model
for real-time prediction of urban ﬂoods. Journal of Hydrology 575, 743–
754 (2019)

[11] AL-Qutami, T.A., Ibrahim, R., Ismail, I., Ishak, M.A.: Virtual multi-
phase ﬂow metering using diverse neural network ensemble and adaptive
simulated annealing. Expert Systems with Applications 93, 72–85 (2018)

Springer Nature 2021 LATEX template

Combining Deep Learning with Good Old-Fashioned Machine Learning

11

[12] Kitamura, G., Chung, C.Y., Moore, B.E.: Ankle fracture detection utiliz-
ing a convolutional neural network ensemble implemented with a small
sample, de novo training, and multiview incorporation. Journal of Digital
Imaging 32(4), 672–677 (2019)

[13] Yan, J., Qi, Y., Rao, Q.: Detecting malware with an ensemble method
based on deep neural network. Security and Communication Networks
2018 (2018)

[14] ALzubi, J.A., Bharathikannan, B., Tanwar, S., Manikandan, R., Khanna,
A., Thaventhiran, C.: Boosted neural network ensemble classiﬁcation
for lung cancer disease diagnosis. Applied Soft Computing 80, 579–591
(2019)

[15] Ludwig, S.A.: Applying a neural network ensemble to intrusion detection.
Journal of Artiﬁcial Intelligence and Soft Computing Research 9 (2019)

[16] Shwartz-Ziv, R., Armon, A.: Tabular data: Deep learning is not all you

need. Information Fusion 81, 84–90 (2022)

[17] Ghosh, S., Bandyopadhyay, A., Sahay, S., Ghosh, R., Kundu, I., Santosh,
K.C.: Colorectal histology tumor detection using ensemble deep neural
network. Engineering Applications of Artiﬁcial Intelligence 100, 104202
(2021)

[18] Paul, D., Tewari, A., Ghosh, S., Santosh, K.C.: OCTx: Ensembled deep
learning model to detect retinal disorders. In: 2020 IEEE 33rd Inter-
national Symposium on Computer-Based Medical Systems (CBMS), pp.
526–531 (2020)

[19] Elliott, D.L., Santosh, K.C., Anderson, C.: Gradient boosting in crowd
ensembles for Q-learning using weight sharing. International Journal of
Machine Learning and Cybernetics 11(10), 2275–2287 (2020)

[20] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: ImageNet:
A large-scale hierarchical image database. In: 2009 IEEE Conference on
Computer Vision and Pattern Recognition, pp. 248–255 (2009)

[21] Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., Singer, Y.: Online
passive-aggressive algorithms. Journal of Machine Learning Research
7(19), 551–585 (2006)

[22] Chen, T., Guestrin, C.: XGBoost: A scalable tree boosting system. In:
Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. KDD ’16, pp. 785–794, New York,
NY, USA (2016)

Springer Nature 2021 LATEX template

12

Combining Deep Learning with Good Old-Fashioned Machine Learning

[23] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q.,
Liu, T.-Y.: LightGBM: A highly eﬃcient gradient boosting decision
tree. Advances in Neural Information Processing Systems 30, 3146–3154
(2017)

[24] Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A.V., Gulin, A.:
CatBoost: Unbiased boosting with categorical features. arXiv preprint
arXiv:1706.09516 (2017)

[25] Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A
next-generation hyperparameter optimization framework. In: Proceed-
ings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2623–2631 (2019)

[26] Sipper, M.: Neural networks with `a la carte selection of activation

functions. SN Computer Science 2(470) (2021)

[27] Sipper, M., Moore, J.H.: AddGBoost: A gradient boosting-style algorithm
based on strong learners. Machine Learning with Applications 7, 100243
(2022)

