2
2
0
2

y
a
M
8
1

]
S
M

.
s
c
[

1
v
9
0
9
8
0
.
5
0
2
2
:
v
i
X
r
a

Enhancing data locality of the conjugate gradient method for high-order
matrix-free ﬁnite-element implementations

Martin Kronbichler∗†

Dmytro Sashko‡

Peter Munch∗§

Abstract. This work investigates a variant of the conjugate gradient (CG) method and embeds
it into the context of high-order ﬁnite-element schemes with fast matrix-free operator evaluation
and cheap preconditioners like the matrix diagonal. Relying on a data-dependency analysis and
appropriate enumeration of degrees of freedom, we interleave the vector updates and inner products
in a CG iteration with the matrix-vector product with only minor organizational overhead. As a
result, around 90% of the vector entries of the three active vectors of the CG method are transferred
from slow RAM memory exactly once per iteration, with all additional access hitting fast cache
memory. Node-level performance analyses and scaling studies on up to 147k cores show that the CG
method with the proposed performance optimizations is around two times faster than a standard CG
solver as well as optimized pipelined CG and s-step CG methods for large sizes that exceed processor
caches, and provides similar performance near the strong scaling limit.

Key words. Conjugate gradient method, data locality, matrix-free implementation, sum factorization,
strong scaling.

1 Introduction

large

choice

sparse

the most

symmetric positive-deﬁnite

The conjugate gradient (CG) method is one of the
most popular algorithms for the iterative solution
linear
of
systems arising from discretization of partial diﬀerential
equations. While it needs to be combined with
strong preconditioners such as multigrid when applied
to elliptic equations, the conjugate gradient method
with simple preconditioners like the matrix diagonal
can be
for parabolic
eﬃcient
partial diﬀerential equations with small to moderate
For example in computational ﬂuid
time steps.
dynamics, many splitting schemes eventually lead to
a positive deﬁnite Helmholtz-like equation with a
mass matrix and a diﬀusive operator scaled by the
time step and viscosity, see, e.g., Tufo and Fischer
(1999), Deville et al. (2002, Sec. 6.5), and Fehn et al.
(2018)
for application in incompressible ﬂows as
well as Demkowicz et al. (1990) and Guermond et al.
Another important
(2021) for compressible ﬂows.
application is the projection with consistent ﬁnite-
element mass matrices,
possibly including some
regularization through diﬀusion (Kronbichler et al.,
2018).

In the conjugate gradient method with simple precon-
ditioners, the matrix-vector product has traditionally
been the most expensive operation. With the increase
in computing power through parallelism on the one hand
and algorithmic progress on the other hand, the matrix-
vector product may in fact be so cheap that attention

∗Technical University of Munich, Garching, Germany

({kronbichler,munch}@lnm.mw.tum.de).
of

†University

Augsburg,

Augsburg,

Germany

(martin.kronbichler@uni-a.de).

‡The University of Queensland, Australia (dmshko@gmail.com).
§Helmholtz-Zentrum Hereon,
Germany

Geesthacht,

(peter.muench@hzg.de).

1

latency/load
imbalance

from
cache

from
RAM

1.2

1

0.8

0.6

0.4

0.2

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

/

]
c
e
s
[

0
104

105

106
DoFs per node

107

108

matrix-vector
dot products

AXPY-style vector updates
diagonal preconditioner

Figure 1: Breakdown of times per CG iteration in the CEED
benchmark problem BP4 (Fischer et al., 2020) with ﬁnite
elements of degree p = 5 on 2 × 24 cores of Intel Xeon Platinum
8174. See Section 3.1 below for a more detailed explanation of
the steps.

must be turned to the other operations in the CG
method.

the

computers,

On large-scale parallel

global
reductions involved in the two inner products in each
CG iteration are generally seen as the main threat
to strong scaling, addressed by the development of
lower-synchronization variants, such as the pipelined
conjugate gradient method (Ghysels and Vanroose,
2014; Cornelis et al.,
s-step methods
2018)
These alternatives
(Chronopoulos and Gear, 1989).
rely on mathematical transformations of the basic
CG algorithm with redundant vector operations that
break some dependencies. The s-step method not only

or

 
 
 
 
 
 
allows to combine global communication for several
CG iterations into one block, but also to schedule
the communication of several matrix-vector products
together through matrix-power kernels.

The guiding theme of these recent contributions
has been the reduction of the communication latency,
see also Eller et al. (2019) for a broader overview on
large-scale methods. However, less attention has been
paid to the throughput of the memory hierarchy, i.e.,
bandwidth requirements from and to main memory
(RAM). This can be the more severe performance limit
in a number of applications, especially for solvers that
combine diﬀerent algorithms in tight sequence. One
example is incompressible ﬂuid ﬂow discretized with
splitting methods, where the pressure Poisson equation
solved with multigrid sets the limit for strong scaling,
but the much larger symmetric positive deﬁnite system
in the velocity contributes with 50% or more of the
runtime (Krank et al., 2017; Fehn et al., 2018). As
an illustration, Figure 1 shows the share of runtime
of diﬀerent operations in a preconditioned conjugate
gradient solver as a function of problem size on a single
compute node. While the matrix-vector product indeed
dominates the runtime for small sizes with less than
3 million degrees of freedom, this is not the case for
larger sizes relevant to those ﬂuid dynamics applications
where AXPY-style vector updates, dot products and the
application of the preconditioner take up two thirds of
the total run time.

The aim of the present work is to design a solver with
primary focus on the memory bandwidth behavior of
the CG algorithm in the context of high-order ﬁnite-
element methods implemented with matrix-free sum-
factorization algorithms (Deville et al., 2002).
The
main novelty is a set of techniques that allow to
interleave the vector updates and inner products in a CG
iteration with the matrix-vector product for a speciﬁc
pipelined-like CG formulation originally presented as
Algorithm 2.2 in Chronopoulos and Gear (1989). As a
result, we are able to perform the access to the three
active vectors in inner products and vector updates
of a complete CG iteration with a single load from
RAM memory for around 90% of the vector entries,
serving all other accesses from the fast cache memory
on contemporary cache-based CPU architectures. Our
experiments show similar performance as for pipelined
and s-step methods near the strong scaling limit when
all vector entries are hit in the caches, but we reach a
signiﬁcantly higher throughput when the vectors spill
out of the caches. While not directly reducing the
minimum achievable wall time, our contribution allows
to reach a predeﬁned throughput already on a smaller
machine.

The proposed techniques rely on introspection of the
matrix-vector product and simple preconditioners. The
idea of using the structure of the operations in the CG
iteration to increase performance is not new and can
be traced back to at least Eisenstat (1981). However,
the context of minimizing data movement for high-
order ﬁnite-element solvers within a single iteration

appears to be novel. These developments are necessary,
because the wide stencils from high-order ﬁnite-element
methods as well as multi-component systems make
traditional optimizations such as matrix-power kernels
and temporal wavefront blocking (Malas et al., 2017) in
the context of s-step Krylov methods ineﬀective.

The implementations used for the present study
are available as open-source software on GitHub.1
They build on the general-purpose ﬁnite-element library
deal.II (Arndt et al., 2021) and have been veriﬁed
on supercomputer scale (Arndt et al., 2020b). The
remainder of this contribution is structured as follows.
Section 2 introduces the state of the art of fast matrix-
free operator evaluation for higher-order ﬁnite-element
In Section 3, the classical conjugate
discretizations.
gradient algorithm as well as pipelined and s-step
variants are reviewed in terms of the memory access.
Section 4 discusses a variant of CG that avoids the
two synchronization points of the conventional CG
algorithm when using cheap diagonal preconditioning,
whereas Section 5 presents the ingredients necessary to
eﬃciently embed the vector operations into the matrix-
vector product. In Section 6, large-scale computations
are given to show the eﬀectiveness of the method, before
Section 7 summarizes our results.

2 Fast matrix-free operator evaluation

We consider a benchmark problem in the context of
high-order ﬁnite element methods to investigate the
beneﬁts of the proposed techniques,
in comparison
to well-studied optimized CG alternatives from the
literature.
the vector-valued Poisson
involves
equation in a d = 3 dimensional domain Ω

R3,

It

⊂

2u = f ,

− ∇

(1)

3

with the vector ﬁeld u(x) = (u1(x), u2(x), u3(x))
∈
H 1(Ω)
. On the domain
∈
boundary ∂Ω, Dirichlet boundary conditions u = g are
(cid:0)
(cid:1)
set.

L2(Ω)
(cid:1)
(cid:0)

and a forcing f

3

The ﬁnite-element discretization is derived from
the weak form of Equation (1),
restricted to a
space of polynomials on a mesh of elements Ωe of
the computational domain, e = 1, . . . , ncells. On a
hexahedral element Ωe, the solution interpolation is
given by

3(p+1)3

=

x∈Ωe

uh(x)
(cid:12)
(cid:12)

Xj=1

φj(ˆx(x))ue,j .

(2)

φj, j = 1, . . . , 3(p + 1)3

Here, ue = [ue,j]j denotes the vector of unknown
coeﬃcients on Ωe in an expansion with a polynomial
basis
. The basis functions are
constructed as the tensor product of one-dimensional
polynomials of degree p for each of
the vector
components. Collecting the functions deﬁned on all
the elements and inserting the expansions as tentative

(cid:8)

(cid:9)

1https://github.com/kronbichler/mf_data_locality,

retrieved on May 12, 2022.

2

solutions and test functions into the weak form, we
arrive at a matrix system

Au = b,

(3)

∈

∈

Rn and the discrete solution vector u

Rn×n, the right-hand-side
with a sparse matrix A
Rn.
vector b
3ncellsp3 denotes the number of degrees
The number n
of freedom (DoFs), counting the unique free coeﬃcients
in the expansion. The solution of this matrix system is
the subject of the present study.

∼

∈

≥

The relatively dense coupling of degrees of freedom in
the matrix stencil makes sparse matrix-vector products
in iterative solvers ineﬃcient for higher-order ﬁnite
elements with degree p
2. Considerable speedups
can be obtained by replacing the sparse matrix-vector
product by a matrix-free evaluation of the action
of the matrix on a vector. Whereas stencil-like
approaches are most beneﬁcial
for the lowest-order
elements on structured meshes (Bauer et al., 2018),
the method of choice for hexahedral elements with
general deformed shapes and higher degrees is to
compute the integrals underlying the ﬁnite-element
method on the ﬂy (Deville et al., 2002; Brown, 2010;
Kronbichler and Kormann, 2012; Fischer et al., 2020).
The matrix-vector product is computed as a sum of cell-
wise contributions,

v = Au =

ncells

Xe=1

PT

e Ae (Peu) ,

(4)

where Ae is the representation of the operator on
element Ωe and Pe denotes the local-to-global mapping
of unknowns such that ue = Peu gives restriction of
the global solution vector u to the element. The
local operation Aeue is again implemented in a matrix-
free fashion without building the element stiﬀness
matrix Ae,

[Aeue]i =

φi)T

(
∇

ZΩe

uhdx =

∇

nq

Xq=1 (cid:16)

φi

ˆ
∇

T

(cid:17)

e,q (wq det J e,q)J −T
J −1

e,q

3(p+1)3

Xj=1

φjue,j

ˆ
∇

(5)

.

ˆxq

(cid:12)
(cid:12)
(cid:12)

The integrals are approximated by numerical quadra-
ture on nq points. In this work, we consider the BP4
benchmark problem proposed by Fischer et al. (2020),
which selects the tensor-product Gaussian quadrature
formula with nq = (p + 2)3 points ˆxq per cell and the
associated quadrature weight wq. The integrals are
transformed to reference coordinates ˆx via a polynomial
mapping x(ˆx) and the derivatives in real space
are
transformed to derivatives in reference coordinates ˆ
∇
by multiplication with the inverse and transpose of
the Jacobian [J e(ˆx)]ij = ∂xi
. The local result Aeue
∂ ˆxj
is obtained by evaluating Equation (5) for all test
functions φi, i = 1, . . . , 3(p + 1)3.

∇

The eﬃciency of the matrix-free algorithm (4)–(5)
crucially depends on evaluating ˆ
uh at the quadrature
∇
points and the multiplication by the test function

O
(p2d).

gradient ˆ
φi as well as the summation over quadrature
∇
points, respectively. For tensor-product shape functions
that are integrated on a tensor-product quadrature
formula, sum factorization allows to decompose these
two steps into a series of one-dimensional interpolations
(pd+1) per element in d dimensions (or
of total cost
(p) per unknown), compared to the naive evaluation
O
cost of
The sum-factorization approach
has been developed in the context of the spectral
element method by Orszag (1980), Patera (1984),
and Tufo and Fischer (1999), see also the book by
Deville et al. (2002) as well as recent implementation
and vectorization studies by Kronbichler and Kormann
(2012, 2019), ´Swirydowicz et al. (2019), Fischer et al.
(2020), Sun et al. (2020), Moxey et al. (2020), and
Kempf et al. (2021).

O

2.1 Experimental setup

Our experiments use the implementation of matrix-
free operator evaluation in the deal.II ﬁnite-element
described in
library (Arndt et al., 2020a, 2021),
The main
Kronbichler and Kormann (2012, 2019).
computational kernels are
fully vectorized across
elements,
i.e., operation (5) is evaluated on several
cells for the diﬀerent SIMD lanes, and use an even-odd
decomposition (Solomonoﬀ, 1992) in sum factorization
to further reduce the arithmetic cost. The solution
vectors store unique unknowns, which necessitates
indirect addressing for the access of elemental data,
represented as a matrix Pe in Equation (4).
Indirect
instructions compared
addressing involves additional
to duplicating unknowns shared by several cells as
used, e.g., in Nek5000 (Fischer et al., 2021), but avoids
redundant storage and speeds up the other parts of the
In our implementation, the indices describing
solver.
Pe use a compressed format of 33 four-byte integers,
(p + 1)3 indices are deduced on the
from which all 3
ﬂy. The meshes are partitioned by space-ﬁlling curves
according to Bangerth et al. (2011).

×

The

code has been compiled with the GNU
compiler g++, version 9.2, with optimization ﬂags
-O3 -march=native -funroll-loops, which is
the
compiler with the best performance among GNU, Intel
and clang for our code. The experiments have been
conducted within a pure MPI setting. To reduce
the overhead due to communication between processes
within a single compute node, we perform the exchange
of ghost values manually via memcpy and MPI-3.0
shared-memory features (Munch et al., 2021), instead of
relying on plain MPI Isend and MPI Irecv.

Following the benchmark description by Fischer et al.
(2020), the algorithms are mainly compared in terms of
the throughput, i.e., the number of degrees of freedom
processed per second (DoFs/s) for one matrix-vector
product in this section or one iteration of the conjugate
The
gradient method in the subsequent sections.
throughput is obtained by the ratio of the number
of degrees of freedom in the linear system and the
The runtime is taken as the
measured runtime.

3

minimum of two separate jobs with four experiments
each in order to reduce the noise caused by other
concurrent jobs on the supercomputer. Apart from
isolated outliers, the arithmetic mean of those eight runs
is within 2% of the reported minimum.

Unless noted otherwise, the numerical experiments
are run on a dual-socket Intel Xeon Platinum 8174
(Skylake) system of the supercomputer SuperMUC-
NG.2 The CPU cores run at a ﬁxed frequency of 2.3
GHz, which gives an arithmetic peak of 3.5 TFlop/s.
The 96 GB of random-access memory (RAM) are
connected through 12 channels of DDR4-2666 with a
theoretical bandwidth of 256 GB/s and an achieved
STREAM triad memory throughput of 205 GB/s.

2.2 Identiﬁcation of fast matrix-vector prod-

uct

e,q

e,q (wq det J e,q)J −T

Contemporary implementations of matrix-free methods
with sum factorization often precompute and store
the metric terms in J −1
at each
quadrature point and load them during operator
evaluation. The precomputed setup is applicable to
deformed (curvilinear) cells and to variable coeﬃcients.
As shown in Kronbichler and Ljungkvist (2019), the
evaluation (4)–(5) is then memory-bound on modern
hardware.
to
maximize the throughput for cell integrals according
to Kronbichler and Kormann (2019), it might be more
economic to evaluate the metric terms on the ﬂy as well.
To identify a suitable method, we compare the following
variants regarding the terms representing the geometric
factors:

For an implementation that aims

×

• tri-quadratic geometry evaluated on the ﬂy from
compute’),
3 doubles per cell, giving a matrix-

33 = 27 points (‘quadratic geomet.
loading 27
vector product with 395 Flops/DoF for p = 5,
• geometry evaluated on the ﬂy from (p + 2)3
points at the position of the quadrature points
loading 3 doubles per
(‘isoparametric compute’),
quadrature point, yielding 417 Flops/DoF for
p = 5,

• precompute and load the inverse Jacobian J −1

e,q and
the Jacobian determinant times quadrature weight
(‘inverse Jacobian load’) at each quadrature point,
loading 10 doubles per quadrature point, yielding
316 Flops/DoF for p = 5,

e,q (wq det J e,q)J −T

• precompute and load the ﬁnal symmetric coeﬃcient
tensor, J −1
(‘ﬁnal tensor load’)
at each quadrature point,
loading 6 doubles per
quadrature point, yielding a matrix-vector product
with 267 Flops/DoF for p = 5, as done, e.g.,
in ´Swirydowicz et al. (2019); Fischer et al. (2020).

e,q

Figure 2 compares the computational throughput of
these variants on a single compute node. The operator
evaluation reaches a maximum for intermediate sizes
of around 106 DoFs when most data ﬁts into caches.
2https://top500.org/system/179566/, retrieved on January

4, 2021.

4

]
c
e
s
[

/

]
s
F
o
D
n
o

i
l
l
i

b
[

5

4

3

2

1

0

105

106

107

108

DoFs

quadratic geomet. compute
inverse Jacobian load
aﬃne mesh, nq = (p + 1)3

isoparametric compute
ﬁnal tensor load

Figure 2: Comparison of diﬀerent implementations of matrix-
free operator evaluation for polynomial degree p = 5 on 2 × 24
cores of Intel Xeon Platinum 8174.

As the problem size further increases, data must be
fetched from main memory, leading to a slowdown for
the cases that are dominated by memory access. We
note a slight zig-zag pattern in the reported throughput,
which is caused by diﬀerent costs of ghost exchange,
which changes when the number of cells is divisible by 48
leading to cube-like subdomains (higher throughput) or
by 64 leading to more irregular MPI subdomains (lower
throughput). Figure 2 also presents the throughput
of the evaluation on an aﬃne mesh with a constant
inverse Jacobian J −1 throughout the whole mesh and
using nq = (p + 1)d points of Gaussian quadrature,
a case studied in detail in Kronbichler and Kormann
(2012, 2019). This reduces the arithmetic cost to 206
Flops/DoF and the memory transfer to just the input
and output vectors with performance mainly limited by
the vector access with indirect addressing.

For large sizes with n > 107, the ‘load’ variants
are memory-limited at slightly more than 200 GB/s,
whereas the two ‘compute’ variants involve a memory
transfer of 100 GB/s and 140 GB/s for vector sizes of 100
million DoFs, all measured from hardware performance
counters with the LIKWID tool (Treibig et al., 2010).
Albeit slightly slower than the ‘load’ variants for the
in-cache case with n < 106, this study concentrates on
the quadratic geometry representation evaluated on the
ﬂy with polynomial degree p = 5 for the ﬁnite-element
expansion (2). The representation of curved geometries
diﬀers from the other three options in general, but we
argue that a tri-quadratic approximation is nonetheless
suitable for many applications. The bulk of a 3D
geometry can often be well-represented in such a way,
leading to a signiﬁcant reduction of the memory transfer
and cache pressure against the isoparametric high-
order case. By contrast, a tri-linear representation
(with approximately 10% higher throughput) might
be unacceptable in a whole region around strongly
It is conceivable to augment the
curved boundaries.

present strategy with a high-degree (isoparametric)
geometry representation of one element layer close
to the boundary, without signiﬁcantly aﬀecting the
throughput.

From the throughput values listed in Figure 2 and the
operation counts mentioned above, it can be deduced
that the matrix-vector product with quadratic geometry
runs at 1.1 TFlop/s with 50 million DoFs and at
1.3 TFlop/s with 1.2 million DoFs. While this is
clearly below the arithmetic peak of 3.5 TFlop/s, the
value is high for this kind of algorithm; the gap to
the peak can be explained by the cost of the indirect
addressing into the vectors u, v,
isolated additions
and multiplications that cannot be merged into fused
multiply-add operations, the throughput of caches, and,
for the larger case, insuﬃcient data prefetching from
RAM.

The throughput of 2.82 billion DoFs/s with 50 million
DoFs for a matrix-free operator evaluation (p = 5,
quadratic geometry computation) can be compared to
a sparse matrix-vector product: the lowest order p = 1
can reach a throughput of between 590 million DoFs/s
(separate matrix entries for all three vector components,
perfect caching of vector entries) and 1.6 billion DoFs/s
(same matrix for all three vector components; only
applicable for simple boundary conditions), or between
50 and 147 million DoFs/s for the p = 5 case. The
eﬀect of high-order matrix-free algorithms being several
times faster than low-order matrix-based algorithms on
a degree-of-freedom basis has been examined in detail,
e.g., in Kronbichler and Wall (2018).

3 Conjugate gradient algorithm

The high throughput of
the matrix-free operator
evaluation has important implications for performance
tuning of the CG iterative method as the matrix-vector
product might no longer be the dominant operation.
Despite using an accurate integration with p + 2 points
per direction, the throughput shown in Figure 2 is
around a third of that of simply copying one vector to
the other, which achieves a throughput of 8.5 billion
DoFs/s at 205 GB/s due to 24 bytes of access per
unknown with 8 bytes read, 8 bytes write, 8 bytes of
read-for-ownership transfer (Hager and Wellein, 2011)
on a dual-socket Intel Xeon 8174 machine.

For preconditioning, this work considers the case of
a simple point Jacobi preconditioner, i.e., the matrix
diagonal. This preconditioner is representative for
problems including a strong mass-matrix contribution
besides the Laplacian (1), as argued in Fischer et al.
(2020). Since the same coeﬃcient is used for all 3 vector
components of u, only the diagonal to a scalar Laplacian
(computed with Gauss–Lobatto integration on p + 1
points) is stored and applied to all three components.

3.1 Breakdown of runtime

Figure 1 shows a breakdown of the runtime per
unknown for one CG iteration, plotted over the number

Ax0, z0 = M−1r0, p0 = z0, e0 = rT

Algorithm 1 Preconditioned conjugate gradient
method.
1: r0 = b
2: k = 0
3: while not converged do
4:
5:

0 z0

−

1st region: r:2

vk = Apk
αk = ek
pT
kvk
xk+1 = xk + αkpk
αkvk
rk+1 = rk
−
rk+1k
if √γk+1 =
break

k

end if
zk+1 = M−1rk+1
ek+1 = rT
k+1zk+1
ek+1
βk =
ek
pk+1 = zk+1 + βkpk
k = k + 1

6:

7:
8:

9:

10:
11:

12:

13:

14:

15:
16: end while

2nd region: r:4/w:2

< ǫ then

3rd region: r:2

4th region: r:2/w:1

of unknowns for the basic CG variant presented in
Algorithm 1. In this study, we consider the termination
by the unpreconditioned residual norm
, which
involves a third global reduction in each iteration. Other
variants exist, and the main performance characteristics
carry over
The following kernels are
considered:

similarly.

rk

k

k

• the sparse matrix-vector product with matrix-free

operator evaluation,

• AXPY-like vector operations ( y = ax + y ),
• dot product computations (including l2 norm), and
• the application of the diagonal preconditioner.

The AXPY-like vector operations and preconditioner
application do not involve any communication, the
matrix-vector product communicates between nearest
neighbors in the mesh (e.g., 26 on a cube geometry
with perfect split), whereas the dot product involves
a reduction among all participating processes. The
experiment of Figure 1 has been conducted on a single
compute node with 48 cores.

×

In the left part of the plot in Figure 1 with fewer than
105 DoFs, the load imbalance of the partitioning of the
mesh elements onto 48 processes as well as the latency
of the communication between the diﬀerent cores on
the node lead to an approximately constant runtime
10−5 seconds per iteration. This appears as a
of 6
decrease of time per unknown as the size increases in the
106
ﬁgure. The latency limitations disappear for n
DoFs, indicating a throughput limitation instead with a
plateau in timings per unknown. For very large sizes
n > 107, the data set of the conjugate gradient exceeds
the caches and most data needs to be fetched from main
memory (RAM). Then, the vector operations start to
contribute signiﬁcantly to the runtime, causing a severe
slowdown compared to intermediate sizes.

∼

In order to understand the performance limitations
the CG algorithm, we take a closer look at
Treating the matrix-vector product

of
Algorithm 1.

5

and the preconditioner as black boxes, there are four
separate regions of vector access in the form of dot
products and AXPY-like vector operations. Within each
region,
loop fusion leading to a single loop over the
entries of all vectors in the region may improve the
locality of reference. Loop fusion can for example be
rk+1k
used to compute the sum needed for the norm
already during the computation of rk+1, avoiding an
extra vector load.

k

Between the regions, however, synchronization points
prevent loop fusion and all vector entries need to be
touched before starting the next region. For instance,
the computation of xk+1 and rk+1 depends on pk, rk,
vk, xk, and αk. The latter itself depends on pk and vk
and requires a full vector sweep through them. If the
size of the vectors pk and vk exceeds the capacity of
a particular cache level during the computation of the
dot product for αk and the entries are already evicted
from the cache in the form of capacity misses, a second
load from the upper levels of the memory hierarchy is
inevitable. Similarly, during the computation of pk+1
in the forth region, the vector entries of pk and zk+1
would have to be loaded again, despite being touched
in the second region and inside the preconditioner,
respectively. Note that even with an ideal cache
replacement strategy this problem cannot be resolved
for vectors considerably larger than the caches.

Summarizing the number of reads in each region of
the conjugate gradient algorithm, the preconditioned
conjugate gradient algorithm requires 10 full vector
reads in each iteration besides the access for the
matrix-vector product and preconditioner, despite only
4 vectors participating in the algorithm (assuming vk
and zk+1 use the same memory). This number can be
slightly reduced to 9 by moving the computation of xk+1
to the 4th region to reuse reads of pk.

3.2 Alternative CG methods

For a simpler comparison, we now consider plain
conjugate gradient algorithms without preconditioner.
The basic version (Algorithm 1 with zk+1 = rk+1 and
M−1 = I) requires 9 full vector reads and 3 vector writes
besides the access for the matrix-vector product.

In the literature, a series of alternative ﬂavors
of the CG algorithm have been developed with the
goal to reduce the number of synchronization points,
primarily driven by latency considerations. However,
they naturally also increase the possibility for loop
fusion and might therefore also improve the memory
transfer (Rupp et al., 2016). As a point of comparison of
the algorithm structure, we present Algorithm 2 for the
pipelined conjugate gradient method and Algorithm 3
for the s-step conjugate gradient methods, respectively.
To simplify the presentation, hereafter we ignore the
algorithms’ initialization and focus on the structure of
the main iteration.

In the pipelined CG method (Ghysels and Vanroose,
2014), the number of synchronization points is reduced
to one by introducing additional global auxiliary vectors.

6

Algorithm 2 Pipelined conjugate gradient method.

4:

5:
6:

ak−1 −
(cid:16)

1: while not converged do
2:
3:

qk = Awk
βk = γk−1/γk−2
αk = γk−1/
pk = rk + βkpk−1
xk = xk + αkpk
sk = wk + βksk−1
rk = rk−1 −
αksk
zk = qk + βkzk−1
wk = wk−1 −
γk = rT
k rk
ak = wT
k rk
12:
13: end while

αkzk

10:
11:

8:
9:

7:

βk

γk−1
αk−1 (cid:17)

r:7/w:6

Algorithm 3 s-step conjugate gradient method with
1) and Qk = Tk(:, 2 : s).
the aliases Rk = Tk(:, 1 : s
−
1: while not converged do
2:

3:

k−1(QT

Tk = [rk, Ark, . . . , Asrk]
W−1
Bk =
k Pk−1)
Pk = Rk + Pk−1Bk
k Pk

−

4:
5: Wk = QT
gk = PT
k rk
6:
ak = W−1
k gk
xk = xk−1 + Pkak
rk = b
γk = rT
10:
11: end while

Akxk

−
k rk

7:
8:

9:

r:2s/w:0

r:2s+1/w:s

r:s+1/w:1

r:2/w:1

Apart from the intended ability to overlap global
communication with the matrix-vector product, this
also allows vector operations to be concentrated in one
vector access region. A naive implementation using a
separate loop for each line of Algorithm 2 would yield
a total of 15 vector reads per CG iteration for the 7
participating vectors. Using loop fusion reduces the
number of reads to 7, the number of involved vectors. It
is possible to slightly reduce the memory transfer further
by performing the update of x every other iteration
(before and after the update of p).

In

CG

s-step

contrast,

meth-
ods (Chronopoulos and Gear, 1989; Naumov, 2016)
perform s CG iterations in a single phase, reducing
the number of global reductions to 3 per phase, i.e.,
to 3/s per CG iteration. This is especially interesting
when the global reductions are the bottleneck of the
CG algorithm. The global reductions are aggregated
by not working simply on vectors but on blocks of
s vectors, e.g., Pk instead of pk and Rk instead of
rk. Similarly, the scalar factor αk becomes a vector
Rs×s), and dot products
(ak
become block dot products. The communication time
of a block operation is similar to that of a scalar one,
since modern networks are latency-bound for global
reductions up to a few dozens of values.

Rs), βk a matrix (Bk

∈

∈

the high-order

for performance,

In the literature, the operation [Ark, ..., Asrk]
is
referred to as a “matrix-power kernel”. It is typically
since
considered to be uncritical
it only comprises of s point-to-point communication
steps in the worst case.
For low-order methods,
increasing the number of ghost layers allows to use
a single communication step per matrix-power-kernel
application (Malas et al., 2017), which might be useful
if the latency is the limiting factor. Furthermore, it
can also enable a higher throughput of the matrix-
vector product, since matrix and vector entries can
be held in caches.
(FEM)
For
methods investigated here, however,
it does not pay
oﬀ according to preliminary investigations: The wide
stencils lead to a much larger dependency region
and quickly saturate caches. Already in the absence
of communication, matrix-power-kernel applications
consisting of 3 matrix-vector products with the present
high-order FEM for p = 5 yield a lower throughput
than performing three operator evaluations in sequence.
Currently, we are not aware of more sophisticated
implementations
that
could exploit this temporal
Furthermore,
communication is negatively aﬀected as additional ghost
layers involve all unknowns on cells with a high
surface-to-volume ratio (MehriDehnavi et al., 2013). As
shown in Kronbichler and Kormann (2019), the cost of
communicating all solution coeﬃcients from a single
layer of elements is already substantial and leads to
pronounced slow-down of the matrix-vector product for
p > 3 in 3D.

this class of algorithms

locality.

for

In total, s + 1 matrix-vector multiplications are
performed per iteration and four update regions can
be identiﬁed with a total of 5s + 4 reads and s +
2 writes per vector entry.
Finally, we would like
the version of the s-step CG
to point out that
method investigated in the following is numerically
unstable due to the loss of orthogonality of
the
monomial Krylov subspace (Naumov, 2016). However,
as alternative formulations, which are numerically more
stable but involve additional steps, are structured
similarly, results obtained for this simple version are
generally transferable to other approaches.

Similarly to the s-step CG methods,
(ECG; Grigori and Tissot

enlarged
(2019);
CG methods
Lockhart et al. (2022)) also work on blocks of vectors
to accelerate convergence. The motivation for the
construction and the way to construct the blocks
are somewhat diﬀerent, but the resulting high-level
algorithms are similar from the performance point of
view to those of s-step CG. Due to this similarity, we
will not consider ECG in the remainder of this work.

4 Minimize data access in standard CG

Inspired by the increased chances to fuse loops over
vectors in the pipelined and s-step conjugate gradient
methods, we now study a version of CG that has
been introduced by (Chronopoulos and Gear, 1989,
Algorithm 2.2) and served as a starting point for the

derivation of pipelining methods. However,
in the
present work, we do not further modify the algorithm
by Chronopoulos and Gear (1989) and instead aim to
reduce the main memory transfer without introducing
additional auxiliary vectors, that inherently increase the
memory access.

We start our derivation by noting that the number of
synchronization barriers identiﬁed in Algorithm 1 can
be reduced by using redundant computations of partial
sums, which is possible in the case the preconditioner is
cheap.

4.1 No preconditioner

We ﬁrst consider the case of identity preconditioning
(zk = rk and M−1 = I) and aim to perform the
computation of contributions to βk before ﬁnalizing the
computation of αk and rk+1. We therefore expand
rT
k+1rk+1 into

rT
k+1rk+1 = (rk
= rT

−
k rk

αkvk)T(rk
2αkrT

−
k vk + α2

αkvk)
kvT

k vk.

−

(6)

k vk,

k vk, vT

By computing the three sums for the inner products
rT
k rk, rT
the ingredients for βk can be
scheduled in parallel to the inner product pT
k vk needed
by αk, as shown in Algorithm 4. Note that γk = rT
k rk
is computed explicitly rather than deﬁned recursively
from the previous iteration in order to avoid detrimental
inﬂuence of roundoﬀ errors (Chronopoulos and Gear,
1989; Saad, 1985). While this scheme adds an additional
read to rk during the summation compared to the
computation of vT
k pk alone, this is compensated by
computing rk+1 at the same time as using the respective
entry for pk+1. In addition, the fused scheduling uses pk
for both xk+1 and pk+1. In the end, number of vector
access regions is reduced to 2, one before (“pre”) and
one after (“post”) the matrix-vector product.

It is also possible to perform the updates to xk+1 only
every other iteration, reusing the content of the vector
pk−1 and rk−1 before they get updated. All together,
the number of vector reads is reduced from 9 in the basic
CG iteration to 6.5 in this improved variant.

Rupp et al. (2016) identiﬁed possibilities for addi-
tional performance optimizations by the three phases
“pre”, “matrix-vector product”, and “post”. Speciﬁ-
cally, that contribution proposed to merge the matrix-
vector product with the “post” region on the GPU for
matrix-vector products through sparse matrix represen-
tations in order to reduce the number of kernel calls.
Building upon this idea, we aim to merge both regions
with the matrix-vector product, which on the one hand
allows to reduce the memory transfer on the CPU, but
is also more involved in the context of matrix-free FEM.

4.2 Diagonal preconditioner

The ideas of the previous subsection can be extended
to the case of a preconditioner. Under the assumption
that the preconditioner is cheap and that there are no
long-range dependencies introduced to the computation

7

Algorithm 4 Conjugate gradient method with merged
vector operations.
1: k = 0, α0 = β0 = 0, r0 = b
2: while not converged do
3:
4:

k = k + 1
if k > 1 odd then

Ax0, p0 = v0 = 0

−

Algorithm 5 Preconditioned conjugate gradient
method with merged vector operations.
1: k = 0, α0 = β0 = 0, r0 = b
2: while not converged do
3:
4:

k = k + 1
if k > 1 odd then

Ax0, p0 = v0 = 0

−

5:

6:

7:

8:
9:

10:

11:
12:

13:
14:

15:

16:
17:

18:

19:
20:

21:

22:

23:

“pre” region:
r:3.5/w:2.5

(cid:1)

“post” region:
r:3/w:0

xk = xk−2 + αk−1pk−1

+ αk−2
βk−2

pk−1 −
(cid:0)
αk−1vk−1

rk−1

end if
rk = rk−1 −
pk = rk + βk−1pk−1
vk = Apk
ak = pT
k vk
γk = rT
k rk
ck = rT
k vk
dk = vT
k vk
γk
αk =
ak
γk+1 = γk
if √γk+1 < ǫ then
if k odd then

−

2αkck + α2

kdk

xk+1 = xk + αkpk

xk+1 = xk−1 + αkpk + αk−1
βk−1

else

end if
break

(pk −

rk)

end if
βk =

24:
25:
26: end while

γk+1
γk

of zk+1 = M−1rk+1, it is more economic to apply the
preconditioner several times.

Following Equation (6), we decompose the computa-
tion of the numerator for βk into several inner products
that do not depend on αk,

βk =

zT
k+1rk+1
zT
k rk
k M−1rk
rT

=

(M−1rk+1)Trk+1
zT
k rk

=

(7)

=

−

k M−1vk + α2
2αkrT
k M−1rk
rT
Thus, βk can be obtained only based on the value of rk
and vk from the beginning of the iteration, prior to the
update of the vectors xk+1, rk+1, zk+1.

k M−1vk

kvT

.

Similarly,

the value of γk+1 =

the
convergence criterion can be computed in parallel to the
reduction for αk, using the expansion

rk+1k
k

for

2

γk+1 = rT

k rk

−

2αkrT

k vk + α2

kvT

k vk.

(8)

Therefore, given the residual rk and the result of the
matrix-vector product vk, all scalars of the current
conjugate gradient iteration can be computed using
one reduction region. The vector zk+1 is no longer
stored explicitly, since we assume that the application
of the preconditioner is cheaper than the read and write
of zk+1. As a result of this restructuring, all vector
updates can be clustered in a single region. Simplifying
the notation and combining the expressions above, we
obtain Algorithm 5.

8

“pre” region:
r:3.83/w:2.5

(cid:1)

“post” region:
r:3.3/w:0

M−1rk−1

xk = xk−2 + αk−1pk−1
+ αk−2
pk−1 −
βk−2
(cid:0)
αk−1vk−1

end if
rk = rk−1 −
pk = M−1rk + βk−1pk−1
vk = Apk
γk = rT
k rk
ak = pT
k vk
bk = rT
k vk
ck = vT
k vk
k M−1rk
dk = rT
k M−1vk
ek = rT
k M−1vk
fk = vT
αk = dk
ak
if
γk

2αkbk + α2

−

if k odd then
p

kck < ǫ then

xk+1 = xk + αkpk

else

xk+1 = xk−1 + αkpk +

end if
break

αk−1
βk−1 (cid:0)pk

− M−1rk(cid:1)

5:

6:

7:
8:

9:

10:
11:

12:

13:
14:

15:
16:

17:

18:

19:
20:

21:

22:
23:

24:

25:
26:

end if

dk

βk =

27:
28: end while

2αkek + α2
dk

kfk

−

The reformulated algorithm results in two vector
access regions, with loop fusion applicable within each
region. As in Algorithm 4, the xk update can be delayed
and performed only every other iteration. The ﬁrst
fused loop region now consists of 3.5 full vector loads
per iteration, plus the load of the preconditioner that—
under the assumption that a single diagonal is used for
each component of the block PDE system (1)—consists
of 1
3 doubles per vector entry. The number of stores is
one for rk and one for pk as well as one for xk every
second iteration. The number of vector loads in the
second region is equal to 3 plus 1
3 for the diagonal
preconditioner. The present reformulation results in
seven global reductions, which can be computed by
summations local to each MPI process and a single
MPI Allreduce carrying 7 variables. Once the seven
scalars are available, the coeﬃcients αk and βk can
be computed locally. The presented reformulation also
enables a fusion into the matrix-vector product, as
discussed in the next section.

The proposed algorithm relies on three properties of

the preconditioner M−1:

• The preconditioner, which is applied twice in the
ﬁrst vector access region and twice in the second
is assumed to be cheap to
vector access region,

apply, with arithmetic costs hidden behind the
memory transfer of the involved vectors.

• We assume that there are no long-range depen-
dencies in the preconditioner, allowing to reuse
the respective entries of rk and vk from caches or
registers when M−1rk and M−1vk are computed.
• The memory access induced by the preconditioner
is assumed to be less expensive than the aggregated
store and load of zk+1 in Algorithm 1.

these
A diagonal preconditioner obviously fulﬁlls
properties, whereas, on the other extreme, a multigrid
V-cycle would violate all three requirements. Clearly, it
needs to be examined for each preconditioner whether
it ﬁts into this scheme on a case-by-case basis, with
preconditioners with more global action requiring a
separate storage step to get M−1rk+1 before the
reductions for βk.

5 Combining vector updates with matrix-

vector product

In the previous section, the matrix-vector product has
been treated as a black box. In order to further improve
the data reuse between the vector access regions of
Algorithms 4 and 5, we propose to embed the vector
updates and dot products into the matrix-free operator
evaluation, which allows to re-use hits of the entries of
p, r, v in caches during the “post” stages, leading to a
single memory read of the vectors p, r, v, and x in the
ideal case (3.83 doubles per unknown).

is

This

locality.

realized by performing the operations
identiﬁed in the previous section on subranges of
the vectors while looping over cells according to
Equation (4) to exploit temporal
In order
to produce a valid algorithm, the data dependencies
during the matrix-vector product need to be identiﬁed
and translated into subranges, as detailed in the
following three subsections. Note that this approach
is more involved than previously proposed algorithms
following the matrix-
that
fuse vector operations
vector product into the loop over
unknowns in
sparse matrix-vector products (Rupp et al., 2016) or
over cells for discontinuous Galerkin schemes, e.g., in
Kronbichler and Allalen (2018), Charrier et al. (2019)
and Munch et al. (2021).

5.1 Data dependencies in matrix-free loops

On a high level,
the matrix-vector multiplication
depends on the source vector u and produces the
destination vector v. However, due to the cell-wise
nature of our matrix-free algorithm, which

• runs through each cell of the mesh,
• reads all unknowns ue = Peu attached to a cell,
which can be shared with other cells for continuous
ﬁnite elements, and

• accumulates integral contributions in the same way

(PT

e ve),

Figure 3:
Illustration of data dependencies in matrix-free
operator evaluation with degree p = 3 for a lexicographic loop
through the cells starting from the bottom left. Each symbol
represents an unknown. Gray crosses denote unknowns where
the result of the operator evaluation is complete before the
highlighted cell. The 3 × 3 unknowns marked with black crosses
get the ﬁnal contribution from the highlighted cell and can
schedule the “post” operation afterwards together with the
3 × 3 circles indicate
unknowns marked with gray crosses.
unknowns that have their ﬁrst access on the highlighted cell,
thus necessitating to be preceded by the “pre” operation. Black
squares denote unknowns with pending integrals, i.e., the “pre”
operation has already been done, but the “post” operation
is not yet possible. Gray disks illustrate unknowns not yet
processed.

}

∈ {

1, . . . , n

we can reﬁne the dependency statement for each entry
of the source and the destination vector: the entry ui,
, is only needed once the ﬁrst cell reads
i
its value. Conversely, entry vi
is available as soon
as the last cell has added its contribution. From an
implementation point of view, this means that we can
postpone the update of ui until its ﬁrst usage and use
the value of vi for the dot product as soon as its value
has been ﬁnalized.
In the following, we are going to
refer to operations happening before the ﬁrst read access
to u but still within the matrix-free loop—in line with
the region names in Algorithms 4 and 5—as a “pre”
operation and to operations happening after the last
write access to v as a “post” operation.

Figure 3 visualizes the data dependencies in a matrix-
free operator evaluation as well as its interplay with
“pre” and “post” operations.
In an MPI-parallel
context, the ghost exchange adds additional constraints
(Kronbichler and Kormann,
2012, Algorithm 2.1).
More precisely, all unknowns owned by a process in
the vector u that need to be sent to remote processes
have to perform the “pre” operation before the ghost
exchange is initiated. Furthermore, the part of integrals
accumulated on remote processes need to be ﬁrst sent to
owner of the respective entry in the vector v before the
“post” operation can be scheduled on those unknowns.
It should, however, be noted that both communication
steps can be overlapped with computations on inner
cells.

We conclude this

subsection by discussing the
major diﬀerences to matrix-based implementations.
The popular compressed row storage and, similarly,
other sparse-matrix formats update an entry in the
destination vector only once by applying the whole row

9

of the matrix.
In such a context, it is obvious when
values in the destination vector are available and it
is straightforward to determine when to schedule the
“post” operation during the matrix-vector product in
a merged way, as was exploited by Rupp et al. (2016).
However, this relation is not given for the dependency
region of the source vector, allowing to embed the “pre”
operation closer to the user of vector entries only based
on a dependency analysis similar to the one proposed
here.

5.2 Batching work from several cells

Tracking the state of each individual vector entry
ui, vi for scheduling the “pre” and “post” operations
would lead to excessive overhead and inhibit loop
optimizations, such as vectorization and unrolling of
the vector operations in CG. Therefore, the “pre” and
“post” operations are tracked on ranges of vector entries.
The length of the range is given in multiples of 64,
a heuristic value that permits full vectorization with
typical SIMD lengths today, except for a single spot at
the end of the vectors.

The length of the ranges is crucially inﬂuenced by the
number of vector entries processed by the matrix-free
integrals in between. The intent is to reach a signiﬁcant
share of overlap between the “pre” and “post” ranges,
enabling to reuse data read during the “pre” operations
even during the “post” operations from the fast
cache memory. The range lookup and the callback
into matrix-free integration functions come with some
overhead in our implementation, which is especially
noticeable for low and intermediate polynomial degrees
with small work per cell. We therefore schedule the
“pre” and “post” operations not around every individual
cell, but around batches of cells. The size of the batches
is selected as

nbatch = max

1024
3(p + 1)3 (cid:23)

, 2

(cid:19)

(cid:18)(cid:22)

nsimd lanes.

(9)

The ﬁrst expression inside the maximum operation
ensures that more cells are grouped together for lower
polynomial degrees, by dividing by the number of
unknowns on each cell. The resulting number of cells
is multiplied by the number of SIMD lanes in the
instruction set in order to employ vectorization across
elements (Kronbichler and Kormann, 2012). For higher
4), at least two SIMD groups of cells are
degrees (p
used.

≥

Depending on the number of SIMD lanes, the number
of vectors accessed in the “pre” and “post” stages, as
well as the additional data access for the matrix-free
integrals, the criterion given by Equation (9) leads to a
few thousands to tens of thousands of double-precision
values corresponding to up to few hundreds of kB of
data. This ﬁts well within modern level-2 or level-
3 caches, which is why no additional tuning has been
performed.

We have integrated the proposed algorithm into
deal.II (Arndt et al., 2020a). It allows users to perform

process 0

process 1

process 0

process 1

Figure 4: Illustration of the numbering of degrees of freedom
for a 2D setup with polynomial degree p = 3. Cells are grouped
together into batches of 6 cells and the interior unknowns
are numbered ﬁrst (highlighted by green-shaded boxes). The
second set of numbers are unknowns located on more than one
cell batch (not marked). The third set consists of unknowns
that need to be exchanged with remote MPI processes (orange
shades).

a “pre” and “post” operation during any matrix-free
loop by providing—additionally to the cell operation—
appropriate anonymous functions in the style of:

vmult(dst, src) := loop(dst, src, op cell, op pre, op post)

Since the operation op cell, which contains the speciﬁcs
is interchangeable,
of a the considered PDE/physics,
our approach is modular and the proposed algorithms
are simply applicable to other PDEs—not just those
considered in this publication. For CG, we provide of-
the-shelf implementations of op pre and op post.

5.3 Numbering of unknowns

The second ingredient is to minimize the number of
ranges with a high number of cell batches between the
ﬁrst and last access in the matrix-free loop. As seen
from Figure 3, unknowns located on shared vertices,
edges, and faces all have the potential to reach over long
distances. This eﬀect is exacerbated when working on
blocks of 64 unknowns, because a single entry out of
64 can lead to a delay of the “post” operation.
It is
therefore crucial to develop a suitable cell traversal and
numbering of unknowns. The cell traversal should aim
for a high volume-to-surface ratio of the cell batches,
because all unknowns located inside the cell batch
have an optimal pre-post distance.
In this work, a
Morton space-ﬁlling curve is used for the partitioning of
elements among the processes (Burstedde et al., 2011;
Bangerth et al., 2011) and for the process-local mesh
traversal.

Given the mesh traversal, unknowns are enumerated
in the sequence of the following four steps, see also the
illustration in Figure 4:

• In the ﬁrst step, all unknowns touched only by
a single cell batch are enumerated following the
ordering of the cells. Except for reaching the next
multiple of 64, this group will have a minimal
distance of one between the “pre” and “post” phase.

10

]

%

[

n
o
i
t
c
n
u
f

n
o
i
t
u
b
i
r
t
s
i
d

l

e
v
i
t
a
u
m
u
c

100

90

80

70

60

50

40

30

20

vector, optimized unknown ordering
vector, no renumbering
scalar, optimized unknown ordering
scalar, no renumbering

1

10

100

1000

Liveliness of vector entries [# batches]

Figure 5: Liveliness of data in vector ranges for the 3D vector
and scalar Laplacians with polynomial degree p = 5 on 40 MPI
processes. The vector Laplacian involves 297 million DoFs
subdivided into 1229 cell batches on each MPI process, the
scalar Laplacian 99 million DoFs with 615 cell batches.

• Next,

the

enumeration is

continued on the
unknowns touched by several batches, but not in
contact with remote MPI processes. Here, some
ranges will have a high distance, whereas others can
still be completed reasonably close after the start.
• In the third step, the unknowns owned locally, but
requested by remote MPI ranks are assigned. These
unknowns will not proﬁt from overlap between
the “pre” and “post” steps, because the “pre”
step needs to be done before the initial MPI Isend
command, whereas the “post” step comes after the
ﬁnal MPI Recv command. A contiguous numbering
reduces the ranges of this unfavorable part of the
vector to a minimum, besides also facilitating the
pack/unpack operations.

• Unknowns that are subject to constraints (not
shown in Figure 4), such as Dirichlet boundary
conditions, will not receive contributions from the
matrix-free integrals with matrix A representing
If they are kept in the
a homogeneous operator.
linear system, like in the implementation of deal.II,
they are appended at the end of the locally owned
unknowns and updated during the last cell batch.

the numbering is

set up to ensure
Furthermore,
contiguous numbers of multiple unknowns associated
with each vertex, edge, face, and volume, in order to
reduce the memory for index storage from 3(p + 1)3
numbers per cell to 33 numbers (for consistently oriented
meshes). This reduces the memory requirements of
metadata, increases data locality and eﬀectiveness of
prefetching as well as allows for packed load operations.
Figure 5 visualizes the beneﬁts of the proposed
enumeration algorithm by plotting the cumulative
distribution function of the liveliness of each subrange.
We deﬁne “liveliness” as the number of cell batches
processed between the ﬁrst and the last access,
respectively. As a reference, we also show the liveliness
of the standard enumeration of degrees of freedom in
deal.II (enumeration in cell order). The reduction of
the liveliness is clearly visible. For the vector Laplacian,

11

n
o
i
t
a
r
e
t
i

G
C

/

F
o
D
/

d
e
d
a
o

l

s
e
l
b
u
o
d

64

32

16

8

4

2

1

10.3 (not combined)

5.7 (combined)

)
ﬀ
d
(

i

6
.
4

104

105

106
DoFs

107

108

L1 from L2 load
L2 from L3 load
Main memory load

PCG (Sec. 4)
PCG (Sec. 5)

Figure 6: Comparison of measured memory transfer for 2 × 20
cores of Intel Xeon Gold 6230 for standard and combined
versions of Algorithm 5.

around 76%—in contrast to 54%—of the subranges is
processed even in the same batch of cells. While the
possibility to process subranges within the same batch
of cells is not necessary, we can see similar trends for
subranges living less than 10 batches of cells. This
is an important threshold: For AVX-512 vectorization,
each cell batch of 16 cells touches 12 kB of unique data
(geometry, indices) for the matrix-vector product and

16 [cells]

3

53 [unique DoFs/cell]

×
= 48 [kB]

·

×

8 [byte/double]

(10)

of unique data per vector or 208 kB for four vectors
and the preconditioner. For around 10 cell batches,
the data thus reaches the combined size of the L2 and
L3 caches per core on the Intel Skylake architecture.
For the scalar Laplacian, our heuristics use batches of
32 cells instead due to a lower number of DoFs/cell.
This gives slightly better liveliness for our proposed
numbering scheme, whereas the case with deal.II’s
default numbering scheme has even higher liveliness
than in the vector case, as the DoFs with long liveliness
are spread to many blocks of 64 DoFs when the number
of unknowns per cell is lower.

While the consideration of

liveliness is a rather
theoretical approach of quantifying the beneﬁts for
combining the “pre” operation,
the matrix-vector
product, and the “post” operation, a clear reduction in
the data volume accessed from RAM can be observed.
A cache analysis (see Figure 6) conducted on the basis of
hardware performance counters using the LIKWID tool
(Treibig et al., 2010) reveals that the combined version
of the PCG algorithm, as proposed here, only reads 5.7
doubles per degree of freedom once the capacity of the
caches is exceeded. This value is lower by 4.6 doubles
than the value of 10.3 reads for the naive execution of

16.5

16.0

Table 1: Summary of the modeled ideal memory transfer of
vector access regions (see also the annotations in Algorithms 1-
5) and matrix-vector multiplication for diﬀerent CG variants.

n
o
i
t
a
r
e
t
i

G
C

/

F
o
D
/

d
e
d
a
o

l

s
e
l
b
u
o
d

15

10

5

0

n
o
i
t
a
r
e
t
i

G
C

/

F
o
D
/

d
e
r
o
t
s

s
e
l
b
u
o
d

8

6

4

2

0

12.1

12.0

10.2

10.0

9.4

8.6

5.1

4.5

5.7

4.8

3.0

2.0

C G

C G

C G

pipelined

s-step

C G

co m bined

P C G

P C G
co m bined

m at-vec

7.4

7.3

5.4

5.3

3.9

3.8

3.9

3.8

4.4

4.3

3.0

2.6

1.3

1.0

C G

C G

C G

pipelined

s-step

C G

co m bined

P C G

P C G
co m bined

m at-vec

Measured

Estimated

Figure 7: Comparison of measured and estimated memory
transfer for various methods on 2 × 20 cores of Intel Xeon Gold
6230 and 108 DoFs, using the measured values of the matrix-
vector multiplication as a baseline transfer.

Algorithm 5.
Note that the renumbering proposed
above has further beneﬁts beyond the liveliness shown in
Figure 5, as the proposed scheme leads to a more linear
data access pattern and fewer active streams, improving
the eﬀectiveness of hardware prefetching and reducing
stress on the translation-lookaside buﬀers.

5.4 Comparison of CG variants

Since the reduction of the data volume to be transferred
from/to main memory is the key strength of the CG
algorithm proposed in this work, we conclude this
section by comparing the measured read and write
data volumes of the basic CG, pipelined CG and s-
step CG algorithms (Algorithms 1–3, with loop fusion
applied where possible) with the results of the proposed
combined Algorithms 4 and 5.

Table 1 shows the predicted memory read and write
transfer volumes in diﬀerent regions of the CG versions.
In the proposed combined CG variants, we assume that
the reuse of memory reads allows vectors to be read
only once across the iteration of the algorithm and, as a

12

vector access

read

write

mat-vec
read write

CG
pipelined CG
s-step CG
combined CG

9
7
5 + 4/s
–

3
6
1 + 2/s
–

PCG
combined PCG

13
–

4
–

2
2
2
3.5

2
3.83

1
1
1
3.5

1
3.5

consequence, we do not separate estimates of the vector
access regions and of the matrix-vector product.

Figure 7 presents both the estimated and measured
averaged values of a complete iteration, derived from
experiments with 100 iterations. The cost of a single
matrix-vector product (“mat-vec”) is 3.0 double data
reads and 1.3 double data writes, which is slightly
higher than the theoretical expectation (2.0/1.0) due to
non-perfect caching and loading of the geometry data.
Since the optimization of the memory transfer of this
portion of the algorithm is not the focus of the current
work, we use the measured values of the matrix-vector
multiplication as a baseline transfer also for the CG
algorithms.

In Figure 7, one can see that the measured values
match well with the predicted ones. Furthermore, it
is clear that while the amount of data to be written
by the combined versions is comparable with the s-
step version, they read up to 5 doubles less data from
RAM compared both to the pipelined and the s-step
CG schemes. Given the considerations in the previous
subsection, this improvement is expected, since a large
fraction of the vector entries accessed during the “pre”
operation remains in caches until they are read again
during the “post” operation.

freedom,

Compared to the theoretical transfer of 4.8 doubles
per degree of
the excess transfer in the
combined preconditioned method can be explained to a
good extent by the liveliness in Figure 5 and the data-in-
ﬂight suggested by Equation (10): 13% of vector entries
have a liveliness of 10 or more cell batches, which can be
expected to give 2 additional reads between the “pre”
operation and the matrix-vector product as well as a
transfer of 3.3 doubles to the “post” operation for the
respective part of the vector. This explains 0.7 out of
the 0.9 excess reads of doubles per DoF.

Furthermore, it is worth noting that the cost of the
non-preconditioned and of the Jacobi-preconditioned
variant of the proposed CG algorithm is very close,
underlining that the beneﬁt is even clearer in the
preconditioned case.

In the next section, we evaluate the inﬂuence of the
reduced access to RAM and the reduced number of
global reductions on the throughput of CG algorithms
for diﬀerent scenarios of high-order matrix-free ﬁnite-
element methods.

]
c
e
s
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

3

2.5

2

1.5

1

0.5

0
104

105

CG
combined CG
mat-vec

106
DoFs

107

108

pipelined CG
PCG

s-step CG (s = 6)
combined PCG

Figure 8: Throughput over the problem size on a single Intel
Xeon Platinum 8174 node for the diﬀerent CG variants.

6 Numerical results

In Sections 4–5, we have proposed techniques that
reduce the access to main memory during CG iterations.
Since reducing the memory access is only a means to
the application goal of increasing the “throughput”,
the CG and PCG variants for diﬀerent numbers of
compute nodes are evaluated in the following, including
some variations of the benchmark and the behavior for
diﬀerent hardware. Similarly to Section 5.4 basic loop
fusion is applied in Algorithms 1–3 during evaluation.

6.1 Node-level performance

Figure 8 shows the throughput on a single compute
node for the diﬀerent CG variants (no preconditioner) as
well as the preconditioned (PCG) case with a diagonal
preconditioner.
For small sizes, the throughput is
largely similar between the methods, given that the
matrix-vector product is the dominating cost and fast
caches can absorb the various vector access patterns.
As anticipated by the memory transfer analysis from
Section 5.4, the picture changes when going to larger
sizes, where the memory-transfer-eﬃcient combined
variants are signiﬁcantly faster. The advantage is
particularly impressive considering that the proposed
CG and PCG variants, running at 2.36 and 2.13 billion
DoFs/s for the largest sizes, are separated from any
other method by a larger gap than what is observed
between the best and worst of the remaining schemes,
the s-step CG method (s = 6) with 1.46 billion DoFs/s
and the preconditioned CG scheme with 0.98 billion
DoFs/s.

The mix of memory-intensive operations on vectors
and the arithmetically heavy matrix-vector product
makes the throughput slightly deviate from the memory-
transfer predictions of Figure 7. For example, the
throughput of the combined CG scheme of 2.36 billion
DoFs/s corresponds to an average memory transfer

of around 170 GB/s aggregated over the whole CG
solver, whereas the s-step method with 1.46 billion
DoFs/s involves an average transfer of 145 GB/s.
While neither of the two variants saturates the memory
bandwidth on the present architecture, the achieved
bandwidth demonstrates an additional beneﬁt of our
CG implementation besides the lower memory transfer:
Fusing vector operations into an arithmetic-heavy
matrix-vector product allows to use spare memory
bandwidth,
leading to a better distribution of the
memory transfer.

Figure 8 also shows the throughput of the matrix-
vector product alone as a point of reference. As its
throughput is 20–30% higher than that of the proposed
merged variants, without the latter fully saturating
the available memory bandwidth, we suppose that
further performance improvements could be gained in
the proposed algorithms by suitable data prefetching.
Note that the slight oscillations in throughput are
caused by diﬀerences in the amount of data exchange
when cells are divisible by 48 or 64 as discussed before.

6.2 Scalability on up to 3,072 nodes

Figure 9 shows the throughput for diﬀerent CG and
PCG variants on 512 compute nodes. The plot scales
the achieved throughput by the number of nodes, which
allows a direct comparison with Figure 8.
It can be
seen that the behavior for large sizes is similar to the
single-node case, with a slight loss of around 5–10% in
the parallel eﬃciency due to inter-node communication.
106 per node, however, there
For intermediate sizes n
is a pronounced diﬀerence. In this regime, the timings of
a single iteration are in the range of the communication
cost in terms of a global reduction, canceling parts of
the cache eﬀect for those intermediate sizes.

∼

The experiments show that the proposed combined
CG variants achieve a similar performance for small
sizes as the pipelined and s-step methods, despite the
latency optimization of the latter methods. This can
be explained by the number of MPI Allreduce calls
per iteration, which are one for both the combined
CG method and the pipelined CG method (albeit
overlapped with the matrix-vector product for the
latter), whereas the s-step method with s = 6 results
in an average of 0.5 global reductions per iteration.
The scaling limit becomes even clearer when plotting
the measured throughput over the time consumed by
a single CG iteration in the lower panel of Figure 9,
directly showing the lowest possible iteration time.
While the CG and PCG methods are slower due to two
and three global reductions per iteration, respectively,
10−4 seconds as a
all other methods take around 1.3
minimum time, which is caused by the global reduction
10−5
combined with a scaling limit of around 8
seconds for the matrix-vector product.

×

×

In Figure 10, the throughput on 3,072 nodes against
the time of a single CG iteration is shown. By comparing
with the result on 512 nodes (dotted lines), a slight loss
in throughput for larger sizes can be seen, corresponding

13

]
c
e
s

×

s
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

]
c
e
s

×

s
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

2.5

2

1.5

1

0.5

0
104

2.5

2

1.5

1

0.5

0

105

106

107

108

DoFs / node

10−4

10−3

10−2

10−1

sec / CG it

pipelined CG
PCG

s-step CG (s = 6)
combined PCG

CG
combined CG
mat-vec

Figure 9: Throughput over the problem size per node (top) and
throughput over latency (bottom) on 512 nodes of Intel Xeon
Platinum 8174 (24,576 MPI ranks) for various formulations.

]
c
e
s

×

s
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

2.5

2

1.5

1

0.5

0

10−4

10−3

10−2

10−1

sec / CG it

s-step CG (s=6)

PCG

comb. PCG

mat-vec

Figure 10: Throughput over the problem size per node (top)
and throughput over latency (bottom) on 3,072 nodes of
Intel Xeon Platinum 8174 (147,456 MPI ranks) for various
formulations. The dotted lines show the scaled throughput on
512 nodes (see also Figure 9).

14

]
c
e
s

×
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

]
c
e
s

×
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

2

1.5

1

0.5

0
104

105

106

107

108

DoFs / node

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0
104

105

106

107

108

DoFs / node

p=3
p=7

PCG
PCG (combined)

p=1
p=5
p=9

Figure 11: Throughput over the problem size on four nodes
of Intel Xeon Platinum 8174 (top) and on one node of 2 × 64
core AMD Epyc 7742 (bottom) for the BP4 benchmark for
diﬀerent polynomial degrees p, all with quadratic geometry
representation and nq = (p + 2)3 quadrature points.

to a small reduction in parallel eﬃciency for weak
scaling. Near the strong scaling limit in the left part
of the ﬁgure, an increase in the minimal time can be
observed, which is due to the higher cost of the global
reductions on a larger scale. However, the increase is
similar between the proposed combined PCG algorithm
and the baseline methods. More importantly, the
combined CG algorithm with preconditioner achieves
a throughput that is 35–40% higher than the one of
the unpreconditioned s-step method for large sizes,
conﬁrming the beneﬁcial behavior of the proposed
variant.

6.3 Benchmark variations

In the following, we will consider variants of the
For the sake
benchmark introduced in Section 2.
of simplicity and given the results from the previous
subsection, we concentrate on the basic PCG algorithm
and the proposed combined PCG algorithm.

6.3.1 Variation of the polynomial degree

The results obtained for the polynomial degree p =
5 above are transferable also to other polynomials
degrees, as shown in Figure 11 (top panel).
As
examples, p = 1, 3, 5, 7, 9 are considered. For p = 1,
the cost of computations compared to the number of
unknowns is overwhelming, as (p + 2)3 = 27 integration
points are used per cell compared to one unique
unknown per cell,
leading to a low throughput of
0.4 GDoFs/sec. This behavior speciﬁc to the present
matrix-free operator evaluation behaves as (p + 2)3/p3
and thus gives less work per unknown for higher
degrees, as opposed to sparse matrix-vector products
(Kronbichler and Kormann, 2012; Kolev et al., 2021).
For higher degrees, we can observe signiﬁcant speedups
of the proposed PCG variants compared to the basic
PCG, with the highest throughput observed for p =
Note that the maximal achievable throughput
5.
decreases for the combined PCG algorithm as the
polynomial degree increases beyond p
7, as opposed
to constant throughput for the basic PCG scheme.
This suggests that the fusion of vector updates within
matrix-vector product as proposed in Section 5 loses
its beneﬁts due to a limited cache size.
Caches
need not only hold vector data of
increasing size
but also larger temporary arrays for sum factorization
(Kronbichler and Kormann, 2019), with data of 8
elements in ﬂight on the given AVX-512 hardware.
Note that no tuning of the parameters that have been
identiﬁed in Section 5 has been performed, relying on
simple heuristics.

≥

6.3.2 Variation of the geometric description

According to the discussion in Section 2.2, we have
concentrated on a tri-quadratic geometry description
as a compromise between higher-order geometry
representation and high throughput up to now.
However, the beneﬁcial behavior observed is transferable
to other geometric descriptions as well. Figure 12
compares the proposed algorithm with a basic PCG
scheme on the two extrema of matrix-vector products
from Figure 2, one loading the inverse Jacobian at
each quadrature point and the other using an aﬃne
mesh with nq = p + 1 and constant Jacobians. While
we observe a speedup of 2.17 in our base case of a
bilinear geometry description, the solver is 1.57
faster
when loading the inverse Jacobians and even 2.27
×
faster in the case of an aﬃne mesh. The relatively low
improvement when loading the inverse Jacobians can
be understood by recalling the node-level performance
analysis of Section 6.1 as the matrix-vector product is
itself limited by the memory bandwidth. Therefore,
no additional memory transfer can be hidden behind
computations, reducing the advantage to the reduction
in memory transfer only.

×

15

]
c
e
s
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

3.5

3

2.5

2

1.5

1

0.5

0

105

106

107

108

DoFs

PCG, quadratic geomet.
PCG, inverse Jacobian
PCG, aﬃne nq = (p + 1)3

PCG comb., quadratic geomet.
PCG comb., inverse Jacobian
PCG comb., aﬃne nq = (p + 1)3

Figure 12: Comparison of throughput of BP4 benchmark with
basic preconditioned CG algorithm and the proposed combined
variant with diﬀerent implementations of matrix-free operator
evaluation for polynomial degree p = 5 on 2 × 24 cores of Intel
Xeon Platinum 8174.

6.3.3 Variation of

the partial diﬀerential

equation

As a next set of tests, we consider variants of the
benchmark from Fischer et al. (2020), namely BP1
(scalar mass matrix, nq = p + 2), BP2 (vectorial mass
matrix, nq = p + 2), BP4 (scalar Laplace operator, nq =
p + 2), and BP5 (scalar Laplace operator, nq = p + 1,
Gauss–Lobatto quadrature). Figure 13 compares the
throughput of the basic CG algorithm and of the
combined version for BP1–BP5. For large problem sizes
106 DoFs/node), a clear trend is visible. While
(
≥
the throughput is limited to 0.8–1.2 GDoFs/sec for the
basic CG scheme, the value is around two times higher
for the proposed algorithms with 1.8–2.3 GDoFs/sec for
all cases. Also note that the BP1, BP2, and BP5 cases
with an arithmetically lighter matrix-vector product
saturate the RAM bandwidth with around 200 GB/s
for the combined CG iteration, whereas BP3 and BP4
reach around 170 GB/s bandwidth, as seen above.

×

5

6.4 Comparison of diﬀerent hardware

×

In the following, we present results obtained on a dual-
socket AMD Epyc 7742 CPU and a Nvidia Tesla V100
GPU. The AMD CPU consists of 2
64 cores running
at 2.25 GHz and uses code compiled for the AVX2
instruction set extension (4-wide SIMD). This gives an
arithmetic peak performance of 4.61 TFlop/s. The
memory conﬁguration uses 2
8 channels of DDR4-
3200, resulting in a peak bandwidth of 410 GB/s and
a measured STREAM triad bandwidth of 290 GB/s.
The size of the last-level cache is 4 MB per core or 512
MB in total. The Nvidia V100 provides an arithmetic
peak performance of 7.8 TFlop/s, a peak memory
bandwidth of 900 GB/s, and a measured bandwidth of
720 GB/s. The performance speciﬁcations of the V100

×

]
c
e
s

×
e
d
o
n
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

3.5

3

2.5

2

1.5

1

0.5

0
104

6

5

4

3

2

1

]
c
e
s
[

/

]
s
t
i

G
C
×

s
F
o
D
n
o

i
l
l
i

b
[

105

106

107

108

DoFs / node

BP2
BP4

PCG
PCG (combined)

BP1
BP3
BP5

0
104

105

106
DoFs

107

108

PCG, Intel CPU
PCG, AMD CPU
PCG, Nvidia GPU

PCG comb., Intel CPU
PCG comb., AMD CPU
PCG comb., Nvidia GPU

Figure 13: Throughput over the problem size for the standard
preconditioned CG scheme and the proposed improved version
on 4 nodes of
Intel Xeon Platinum 8174 for the CEED
benchmark problems BP1 (scalar mass matrix), BP2 (vector-
valued mass matrix), BP3 (scalar Laplace matrix), BP4 (vector-
valued Laplace matrix), and BP5 (scalar Laplace matrix,
collocation setting with Gauss–Lobatto quadrature on nq =
(p + 1)3 points) according to Fischer et al. (2020).

GPU are considerably higher on the GPU compared
to the two CPU systems, but with a less sophisticated
cache infrastructure.

6.4.1 Variation of the polynomial degree on

Intel and AMD CPUs

×

×

The lower panel of Figure 11 shows the experiment
from Subsection 6.3.1, varying the polynomial degree
on an AMD Epyc 7742 node. Here, we observe a
maximal throughput of 4 GDoFs/sec and a maximal
speedups of 3
compared to the baseline CG solver
speedup in the case
(compared to 2 GDoFs/sec and 2
of Intel). This diﬀerence can be explained by the higher
arithmetic performance of the AMD system, shifting
the performance limit with an achieved bandwidth of
around 270 GB/s closer to the memory throughput
limit of 290 GB/s. An interesting observation is the
fact that the performance does not drop for the high
polynomial degrees p > 5. This can be contributed
to larger caches as well as to the AVX-2 instruction-
set extension with vectorization aggregating work from
only 4 cells together, which increases the beneﬁt of the
combination of “pre”, “mat-vec”, and “post” regions.

6.4.2 BP5 on CPU and GPU

As a last experiment, we run Algorithm 5 on a GPU
architecture. Given the much smaller available cache
size compared to compute units, we have not been
able to embed the vector access regions into the cell-
based evaluation of the matrix-vector product. As
a result, we propose to run the three regions “pre”,

16

Figure 14: BP5: Throughput over the problem size on a single
node for the basic preconditioned CG method and the proposed
combined variant.

“mat-vec”, and “post” each as a separate kernel
with its own kernel call. Furthermore, the matrix-
vector product uses a precomputed ﬁnal coeﬃcient
on the GPU, due to a diﬀerent balance between
arithmetic performance, available registers, and memory
bandwidth compared to CPUs, see also the analysis
in ´Swirydowicz et al. (2019). Details on the GPU
infrastructure of deal.II can be found in Ljungkvist
(2017) and Kronbichler and Ljungkvist (2019).

Figure 14 shows the throughput of the regular and
the combined CG method run on a single GPU device
on Summit3(Nvidia V100). For small problem sizes, a
clear beneﬁt can be observed due to the reduced number
of kernel calls (3). For large problem sizes, a speed-up of
about 18% with 2.8 GDoF/s is reached. Note that this
represents a considerably lower improvement, which is
due to the missing overlap between the “pre” and “post”
operations. Nonetheless, Algorithm 5 also improves
the throughput for lower sizes because of fewer kernel
launches. Reducing the number of kernel calls in CG
on GPUs has been also the motivation in Aliaga et al.
(2013), Dehnavi et al. (2011), Rupp et al. (2016), and
Chalmers and Warburton (2020). The contribution by
Rupp et al. (2016) was even able to obtain two kernel
calls for vector-matrix-multiplication implementations
based on sparse matrices. However, the latter concept
is not straightforwardly extensible to matrix-free ﬁnite-
element computations with contributions to the result
vector being accumulated from computations on several
cells, as discussed in Section 5.1. Since the GPU’s high-
bandwidth memory is limited to 16 GB, the maximum
size of the problem that can be run is considerably
smaller on the GPU.

With the proposed combined CG method, the CPU
results appear more beneﬁcial than the GPU results:
Given that both the memory bandwidth and arithmetic

3https://www.top500.org/system/179397/,

retrieved

on

February 11, 2021.

performance is considerably higher on a single V100
device than on the dual-socket Intel and AMD systems,
one would expect best performance on the GPU.
However, the Intel result is only 20% lower than
the GPU, and the AMD result from RAM is 20%
better than on the GPU, because of the reduction of
memory transfer between the “pre” and “post” regions.
Furthermore, the CPU reach a higher throughput for
moderate sizes when the data ﬁts into caches.

7 Conclusions

We have presented an implementation of the conjugate
gradient method that aims to minimize the access to
auxiliary vectors for the case of high-order matrix-
free ﬁnite-element implementations with a diagonal
preconditioner. The development was motivated by
the observation that matrix-free operator evaluation
has become so fast that AXPY-style vector updates, dot
products and the application of the preconditioner can
consume around two thirds of the total runtime for
large problem sizes on modern hardware, relevant for
example for ﬂuid dynamics applications. The proposed
solver relies on interleaving the vector updates and dot
products of the conjugate gradient iteration with the
loop through the mesh elements of the matrix-vector
product, combined with redundant applications of the
preconditioner and summation of auxiliary quantities to
break the dependencies. We have shown that around
90% of the vector entries in the three active vectors of
a CG iteration can be re-used from fast cache memory,
resulting in a single load and store operation for each
vector.
Both

and
strong/weak-scaling studies on up to 147,456 CPU cores
conﬁrm the suitability of the proposed algorithm for
modern hardware. Experiments have been conducted
on CPU-based (Intel Xeon Platinum 8174, AMD
Epyc 7742) and GPU-based (Nvidia Tesla V100 GPU)
compute nodes for a large variety of polynomial degrees,
geometric descriptions, and PDEs (scalar/vector-valued
mass/Laplace matrix). Compared to a baseline CG
solver as well as optimized pipelined CG and s-step
have been
CG implementations, speedups of 2–3
reported.
Besides reducing the memory transfer,
the proposed method allows to run memory-heavy
vector operations near the arithmetic-heavy matrix-
free operator evaluation. As a result, new tuning
opportunities for implementing matrix-free methods
appear, allowing to gain performance from computing,
e.g., redundant geometry information on the ﬂy with
reduced memory transfer, an operation that might not
be beneﬁcial for the matrix-vector product alone.

performance

node-level

analyses

×

Future work aims to extend the algorithm towards
the data dependencies
imposed by discontinuous
Galerkin discretizations as well as more sophisticated
preconditioners with longer-range data dependencies.
Furthermore, it would be useful to apply analysis and
transformation tools from compiler constructions to
replace the current manual dependency management

17

for interleaving the matrix-vector product with vector
updates and inner products by a more automatic
approach based on hardware characteristics, which
would make the application to other algorithms, like
BiCGStab or GMRES, simpler.

Acknowledgements

The authors acknowledge collaboration with Momme Allalen,
Daniel Arndt, Paddy ´O Conbhu´ı, Prashanth Kanduri, Karl
Ljungkvist, Alexander Roschlaub, Bruno Turcksin, as well as the
deal.II community.

This work was

supported by the Bayerisches Kom-
petenznetzwerk f¨ur Technisch-Wissenschaftliches Hoch- und
H¨ochstleistungsrechnen (KONWIHR) through the projects “Per-
formance tuning of high-order discontinuous Galerkin solvers
for SuperMUC-NG” and “High-order matrix-free ﬁnite element
implementations with hybrid parallelization and improved data
locality”. The authors gratefully acknowledge the Gauss Centre
for Supercomputing e.V. (www.gauss-centre.eu) for funding this
project by providing computing time on the GCS Supercom-
puter SuperMUC-NG at Leibniz Supercomputing Centre (LRZ,
www.lrz.de) through project id pr83te.

References

J. I. Aliaga, J. P´erez, E. S. Quintana-Ort´ı, and H. Anzt.
Reformulated conjugate gradient for the energy-aware solution
of
In 2013 42nd International
Conference on Parallel Processing, pages 320–329. IEEE, 2013.

linear systems on GPUs.

D. Arndt, W. Bangerth, B. Blais, T. C. Clevenger, M. Fehling,
A. V. Grayver, T. Heister, L. Heltai, M. Kronbichler, M. Maier,
P. Munch, J.-P. Pelteret, R. Rastak, I. Tomas, B. Turcksin,
Z. Wang, and D. Wells. The deal.II library, version 9.2.
Journal of Numerical Mathematics, 28(3):131–146, 2020a. doi:
10.1515/jnma-2020-0043. URL https//dealii.org.

D. Arndt, N. Fehn, G. Kanschat, K. Kormann, M. Kronbichler,
P. Munch, W. A. Wall, and J. Witte. ExaDG – high-order
discontinuous Galerkin for the exa-scale.
In H.-J. Bungartz,
S. Reiz, B. Uekermann, P. Neumann, and W. E. Nagel,
editors, Software for Exascale Computing – SPPEXA 2016–
2019, Lecture Notes in Computational Science and Engineering
136, pages 189–224, Cham, 2020b. Springer International
Publishing. doi: 10.1007/978-3-030-47956-5 8.

D. Arndt, W. Bangerth, D. Davydov, T. Heister, L. Heltai,
M. Kronbichler, M. Maier, J.-P. Pelteret, B. Turcksin, and
D. Wells. The deal.II ﬁnite element library: design, features,
and insights. Computers & Mathematics with Applications, 81:
407–422, 2021. doi: 10.1016/j.camwa.2020.02.022.

W. Bangerth, C. Burstedde, T. Heister, and M. Kronbichler.
Algorithms and data structures for massively parallel generic
ACM Transactions on
adaptive ﬁnite element codes.
Mathematical Software, 38:14/1–28, 2011.
10.1145/
doi:
2049673.2049678.

S. Bauer, D. Drzisga, M. Mohr, U. R¨ude, C. Waluga, and
B. Wohlmuth. A stencil scaling approach for accelerating
matrix-free ﬁnite element implementations. SIAM Journal on
Scientiﬁc Computing, 40(6):C748–C778, 2018. doi: 10.1137/
17m1148384.

J. Brown. Eﬃcient nonlinear solvers for nodal high-order ﬁnite
elements in 3D. Journal of Scientiﬁc Computing, 45(1-3):48–
63, 2010. doi: 10.1007/s10915-010-9396-8.

C. Burstedde, L. C. Wilcox, and O. Ghattas. p4est: Scalable
algorithms for parallel adaptive mesh reﬁnement on forests of
octrees. SIAM J. Sci. Comput., 33(3):1103–1133, 2011. doi:
10.1137/10079163. URL http://p4est.org.

N. Chalmers and T. Warburton. Portable high-order ﬁnite element
arXiv:2009.10917

kernels I: Streaming operations, 2020.
preprint.

D. E. Charrier, B. Hazelwood, E. Tutlyaeva, M. Bader,
M. Dumbser, A. Kudryavtsev, A. Moskovsky, and T. Weinzierl.
Studies on the energy and deep memory behaviour of a cache-
oblivious, task-based hyperbolic PDE solver. The International
Journal of High Performance Computing Applications, 33(5):
973–986, 2019. doi: 10.1177/1094342019842645.

A. T. Chronopoulos and C. W. Gear. S-step iterative methods
for symmetric linear systems. Journal of Computational and
Applied Mathematics, 25(2):153–168, Feb. 1989.
ISSN 0377-
0427. doi: 10.1016/0377-0427(89)90045-9.

J. Cornelis, S. Cools, and W. Vanroose. The communication-
hiding conjugate gradient method with deep pipelines. ArXiv
e-prints, 1801.4728v3, 2018.

M. M. Dehnavi, D. M. Fern´andez, and D. Giannacopoulos.
Enhancing the performance of conjugate gradient solvers on
graphic processing units. IEEE Transactions on Magnetics, 47
(5):1162–1165, 2011.

L. Demkowicz, J. Oden, and W. Rachowicz. A new ﬁnite element
method for solving compressible Navier–Stokes equations based
on an operator splitting method and h-p adaptivity. Computer
Methods in Applied Mechanics and Engineering, 84(3):275–
326, 1990. doi: 10.1016/0045-7825(90)90081-v.

M. O. Deville, P. F. Fischer, and E. H. Mund. High-order methods
for incompressible ﬂuid ﬂow, volume 9. Cambridge University
Press, 2002.

S. C. Eisenstat.

Eﬃcient

implementation of a class of
preconditioned conjugate gradient methods. SIAM Journal
on Scientiﬁc and Statistical Computing, 2(1):1–4, 1981. doi:
10.1137/0902001.

scale for

P. R. Eller, T. Hoeﬂer, and W. Gropp. Using performance
models to understand scalable Krylov solver performance
at
In Proceedings
of the ACM International Conference on Supercomputing.
ACM, June 2019.
10.1145/3330345.3330358. URL
https://doi.org/10.1145/3330345.3330358.

structured grid problems.

doi:

N. Fehn, W. A. Wall, and M. Kronbichler.

Eﬃciency of
high-performance discontinuous Galerkin spectral element
methods for under-resolved turbulent incompressible ﬂows.
International Journal for Numerical Methods in Fluids, 88(1):
32–54, 2018. doi: 10.1002/ﬂd.4511.

P. Fischer, M. Min, T. Rathnayake, S. Dutta, T. Kolev,
V. Dobrev, J.-S. Camier, M. Kronbichler, T. Warburton,
K. ´Swirydowicz, and J. Brown. Scalability of high-performance
PDE solvers. The International Journal of High Performance
Computing Applications, 34(5):562–586, 2020. doi: 10.1177/
1094342020915762.

P. Fischer,

S. Kerkemeier, A. Peplinski, D.

Shaver,
A. Tomboulides, M. Min, A. Obabko, and E. Merzari.
Nek5000 Web page. 2021. https://nek5000.mcs.anl.gov.

P. Ghysels and W. Vanroose. Hiding global synchronization
latency in the preconditioned conjugate gradient algorithm.
Parallel Computing, 40(7):224–238, 2014. doi: 10.1016/j.parco.
2013.06.001. 7th Workshop on Parallel Matrix Algorithms and
Applications.

L. Grigori and O. Tissot. Scalable linear solvers based on enlarged
krylov subspaces with dynamic reduction of search directions.
SIAM Journal on Scientiﬁc Computing, 41(5):C522–C547,
2019.

J.-L. Guermond, M. Maier, B. Popov, and I. Tomas. Second-order
invariant domain preserving approximation of the compressible
Navier–Stokes equations.
Computer Methods in Applied
Mechanics and Engineering, 375:113608, 2021. doi: 10.1016/j.
cma.2020.113608.

G. Hager and G. Wellein.

Introduction to High Performance
Computing for Scientists and Engineers. CRC Press, Boca
Raton, 2011.

D. Kempf, R. Heß, S. M¨uthing, and P. Bastian. Automatic
code generation for high-performance discontinuous Galerkin
methods on modern architectures. ACM Transactions on
Mathematical Software, 47(1):6:1–31, 2021.
doi: 10.1145/
3424144.

18

T. Kolev, P. Fischer, M. Min, J. Dongarra, J. Brown, V. Dobrev,
T. Warburton, S. Tomov, M. S. Shephard, A. Abdelfattah,
V. Barra, N. Beams, J.-S. Camier, N. Chalmers, Y. Dudouit,
A. Karakus, I. Karlin, S. Kerkemeier, Y.-H. Lan, D. Med-
ina, E. Merzari, A. Obabko, W. Pazner, T. Rathnayake,
C. W. Smith, L. Spies, K. Swirydowicz, J. Thompson,
A. Tomboulides, and V. Tomov. Eﬃcient exascale discretiza-
tions: High-order ﬁnite element methods. The International
Journal of High Performance Computing Applications, 35(6):
527–552, 2021. doi: 10.1177/10943420211020803.

B. Krank, N. Fehn, W. A. Wall, and M. Kronbichler. A
high-order semi-explicit discontinuous Galerkin solver for 3D
incompressible ﬂow with application to DNS and LES of
turbulent channel ﬂow. Journal of Computational Physics, 348:
634–659, 2017. doi: 10.1016/j.jcp.2017.07.039.

M. Kronbichler and M. Allalen. Eﬃcient high-order discontinuous
Galerkin ﬁnite elements with matrix-free implementations. In
H.-J. Bungartz, D. Kranzlm¨uller, V. Weinberg, J. Weism¨uller,
and V. Wohlgemuth, editors, Advances and New Trends in
Environmental Informatics, pages 89–110. Springer, 2018. doi:
10.1007/978-3-319-99654-7 7.

M. Kronbichler and K. Kormann. A generic interface for parallel
cell-based ﬁnite element operator application. Computers and
ISSN 0045-7930. doi: 10.1016/j.
Fluids, 63:135–147, 2012.
compﬂuid.2012.04.012.

M. Kronbichler and K. Kormann. Fast matrix-free evaluation
of discontinuous Galerkin ﬁnite element operators. ACM
Transactions on Mathematical Software, 45(3):29:1–40, 2019.
doi: 10.1145/3325864.

M. Kronbichler and K. Ljungkvist. Multigrid for matrix-free high-
order ﬁnite element computations on graphics processors. ACM
Transactions on Parallel Computing, 6(1):2:1–32, 2019. doi:
10.1145/3322813.

M. Kronbichler and W. A. Wall. A performance comparison
of continuous and discontinuous Galerkin methods with fast
multigrid solvers. SIAM Journal on Scientiﬁc Computing, 40
(5):A3423–A3448, 2018. doi: 10.1137/16M110455X.

M. Kronbichler, A. Diagne, and H. Holmgren.

A fast
massively parallel two-phase ﬂow solver for microﬂuidic chip
simulation. The International Journal of High Performance
Computing Applications, 32(2):266–287, 2018. doi: 10.1177/
1094342016671790.

K. Ljungkvist. Matrix-free ﬁnite-element computations on
graphics processors with adaptively reﬁned unstructured
meshes. In HPC ’17: Proceedings of the 25th High Performance
Computing Symposium, pages 1–12, San Diego, CA, USA,
2017. Society for Computer Simulation International.

S. Lockhart, A. Bienz, W. Gropp, and L. Olson. Performance
analysis and optimal node-aware communication for enlarged
conjugate gradient methods. arXiv preprint arXiv:2203.06144,
2022.

T. M. Malas, G. Hager, H. Ltaief,

and D. E. Keyes.
Multidimensional intratile parallelization for memory-starved
stencil
ACM Transactions on Parallel
Computing, 4(3):12:1–32, 2017. doi: 10.1145/3155290.

computations.

M. MehriDehnavi, Y. El-Kurdi, J. Demmel, and D. Giannacopou-
los. Communication-avoiding Krylov techniques on graphic
processing units. IEEE transactions on magnetics, 49(5):1749–
1752, 2013. doi: 10.1109/TMAG.2013.2244861.

D. Moxey, R. Amici, and M. Kirby. Eﬃcient matrix-free high-
order ﬁnite element evaluation for simplicial elements. SIAM
Journal on Scientiﬁc Computing, 42(3):C97–C123, 2020. doi:
10.1137/19m1246523.

P. Munch, K. Kormann, and M. Kronbichler.

hyper.deal:
An eﬃcient, matrix-free ﬁnite-element
library for high-
dimensional partial diﬀerential equations. ACM Transactions
on Mathematical Software, 47(4):33:1–34, 2021. doi: 10.1145/
3469720.

M. Naumov.

S-step and communication-avoiding iterative

methods. Technical Report NVR-2016-003, NVIDIA, 2016.

S. A. Orszag.

Spectral methods for problems in complex
geometries. Journal of Computational Physics, 37(1):70–92,
1980. doi: 10.1016/0021-9991(80)90005-4.

A. T. Patera. A spectral element method for ﬂuid dynamics:
Laminar ﬂow in a channel expansion. Journal of Computational
Physics, 54(3):468–488, 1984.
doi: 10.1016/0021-9991(84)
90128-1.

K. Rupp, J. Weinbub, A. J¨ungel, and T. Grasser. Pipelined
iterative solvers with kernel fusion for graphics processing units.
ACM Transactions on Mathematical Software, 43(2):11:1–27,
2016. doi: 10.1145/2907944.

Y. Saad. Practical use of polynomial preconditionings for the
conjugate gradient method. SIAM Journal on Scientiﬁc and
Statistical Computing, 6(4):865–881, 1985.
10.1137/
0906059.

doi:

A. Solomonoﬀ. A fast algorithm for spectral diﬀerentiation.
10.1016/

J. Comput. Phys., 98(1):174–177, 1992.
0021-9991(92)90182-X.

doi:

T. Sun, L. Mitchell, K. Kulkarni, A. Kl¨ockner, D. A. Ham,
and P. H. Kelly. A study of vectorization for matrix-free
ﬁnite element methods. The International Journal of High
Performance Computing Applications, 34(6):629–644, 2020.
doi: 10.1177/1094342020945005.

K. ´Swirydowicz, N. Chalmers, A. Karakus, and T. Warburton.
Acceleration of tensor-product operations for high-order ﬁnite
element methods.
The International Journal of High
Performance Computing Applications, 33(4):735–757, 2019.
doi: 10.1177/1094342018816368.

J. Treibig, G. Hager, and G. Wellein. LIKWID: A lightweight
performance-oriented tool suite for x86 multicore environments.
In 2010 39th International Conference on Parallel Processing
Workshops, pages 207–216, 2010. doi: 10.1109/ICPPW.2010.
38.

H. M. Tufo and P. F. Fischer. Terascale spectral element
algorithms and implementations.
In Proceedings of the 1999
ACM/IEEE conference on Supercomputing, page 68. ACM,
1999. doi: 10.1109/SC.1999.10035.

19

