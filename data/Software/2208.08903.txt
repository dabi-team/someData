GomalizingFlow.jl: A Julia package for Flow-based
sampling algorithm for lattice ﬁeld theory

Akio Tomiya

Faculty of Technology and Science, International Professional University of Technology,
3-3-1, Umeda, Kita-ku, Osaka, 530-0001, Osaka, Japan

Satoshi Terasaki

AtelierArith, 980-0004, Miyagi, Japan

Abstract

GomalizingFlow.jl: is a package to generate conﬁgurations for quantum ﬁeld
theory on the lattice using the ﬂow based sampling algorithm in Julia pro-
gramming language. This software serves two main purposes: to accelerate
research of lattice QCD with machine learning with easy prototyping, and to
provide an independent implementation to an existing public Jupyter note-
book in Python/PyTorch. GomalizingFlow.jl implements, the ﬂow based
sampling algorithm, namely, RealNVP and Metropolis-Hastings test for two
dimension and three dimensional scalar ﬁeld, which can be switched by a
parameter ﬁle. HMC for that theory also implemented for comparison. This
package has Docker image, which reduces eﬀort for environment construction.
This code works both on CPU and NVIDIA GPU.

Keywords: Lattice QCD, Particle physics, Machine learning, Normalizing
ﬂow, Julia

2
2
0
2

g
u
A
8
1

]
t
a
l
-
p
e
h
[

1
v
3
0
9
8
0
.
8
0
2
2
:
v
i
X
r
a

Preprint submitted to SoftwareX

August 19, 2022

 
 
 
 
 
 
Required Metadata

Current code version

Nr. Code metadata description
C1
C2

Current code version
Permanent link to code/repository
used for this code version

C3

Code Ocean compute capsule

C4
C5
C6

C7

C8

C9

Legal Code License
Code versioning system used
Software code languages, tools, and
services used
Compilation requirements, operat-
ing environments & dependencies
If available Link to developer docu-
mentation/manual
Support email for questions

Please ﬁll in this column
v1.0.0
https://github.com/
AtelierArith/GomalizingFlow.
jl
To be uploaded. Docker image avail-
able
MIT
Git
Julia

Julia 1.7.3, Flux.jl v0.13.5, (Docker
image is provided)
https
//github.com/AtelierArith/GomalizingF low.jl/blob/main/READM E.md
akio@yukawa.kyoto-u.ac.jp

:

Table 1: Code metadata (mandatory)

2

1. Motivation and signiﬁcance

A lattice gauge theory with Markov chain Monte-Carlo calculations pro-
vides quantitative information on quantum ﬁeld theory and particle physics
[1, 2, 3, 4] Regularized quantum ﬁeld theory is deﬁned on discretized space-
time (lattice), and the discretization makes degrees of freedom ﬁnite. This
enables us to perform numerical calculations with Markov chain Monte-Carlo
on supercomputers. Quantum ﬁeld theory on a lattice has been applied
mainly for QCD (Quantum Chromo-dynamics), which is a fundamental the-
ory for the inside of nuclei [5]. Lattice calculations of QCD (Lattice QCD)
have succeeded in reproducing the hadron spectrum [6], and enable us to
access non-perturbative and quantitative information which is diﬃcult to
obtain with other methods [7, 8, 9].

The ultraviolet cutoﬀ is necessary to deﬁne a ﬁnite theory. However,
the ultraviolet cutoﬀ, namely ﬁnite lattice spacing, bring unwanted artifacts
into the theory. Finer lattice spacing gives smaller lattice artifacts, but it
causes long autocorrelation among the Monte-Carlo samples, which makes
Monte-Carlo calculations ineﬃcient because produced random samples are
correlated, which is called the critical slowing down [10]. To make Monte-
Calo samples in good quality in a short time, a new algorithm with shorter
autocorrelation is demanded because the autocorrelation time depends on an
algorithm.

The trivializing map has been investigated to resolve the critical slow-
ing down in lattice QCD calculations [11]. Trivializing maps is a change of
variables in the (path-)integral between a physical theory to a trivial theory,
which is easy to calculate. A concrete example of the trivializing map called
the Nicoli map has been realized in a supersymmetric ﬁeld theory in the ﬁrst
[12]. For non-super symmetric theory, M. Luscher has shown that the gra-
dient ﬂow with Jacobian is an approximate trivializing map, but his original
idea has not worked well because of the numerical cost.

Machine learning algorithms to generate conﬁgurations for lattice ﬁeld
theory are widely investigated [13]. First trial has been done with the Boltz-
mann machine [14], and others like GAN [15, 16] has been tried. The gauge
covariant neural network [17] is applied to perform simulations for four di-
mensional QCD. Neural network parametrized leapfrog enhances topology
change in simulations [18]. The normalizing ﬂow [19, 20, 21, 22, 23, 24, 25,
26, 27] is recently extensively investigated and we discuss more on this later.
Generative models are machine learning architecture for image generation,
and which has been investigated to generate good quality images. Images
are two dimensional data with certain structure, and it seems that it can be
applied to generate ﬁeld conﬁgurations.

3

The ﬂow based sampling algorithm, based on Real-NVP (Non-volume
preserving map), which is a generative model, and is an exact algorithm of
Monte-Carlo conﬁguration generation for quantum ﬁeld theories associated
with the Metropolis–Hastings test. It has potential to improve the critical
slowing down [19, 20], which has been extended to system with SU(N) links
[21] and with fermions [22, 23, 24]. This can be regarded as a realization of
a trivializing map a la M. Luscher using a neural network.

One of the purposes of quantum ﬁeld theory is to calculate the expectation

value in the following form,

where

(cid:90)

(cid:104)O(cid:105) ≡

Dφ P (QFT)

{L,m2,λ}[φ] O[φ],

Dφ =

Ld
(cid:89)

n=no

dφn

(1)

(2)

, the origin1, d is the dimensionality, and L is a size of

and no = (1, 1, · · · )
(cid:125)
(cid:124)
the system. And P (QFT)

(cid:123)(cid:122)
d

{L,m2,λ}[φ] is a probability density of the system,

P (QFT)
{L,m2,λ}[φ] =

1
Z

e−S(QFT)[φ],

(3)

and Z normalizes the expectation values as (cid:104)1(cid:105) = 1. The action is,

S(QFT)[φ] = −

Ld
(cid:88)

n=no

φ(n)∂2φ(n) +

Ld
(cid:88)

n=no

V [φ](n).

(4)

where ∂2φ(n) = (cid:80)
and,

µ[φ(n + ˆµ) + φ(n − ˆµ) − 2φ(n)] is a discretized Laplacian

V [φ](n) = m2φ2(n) + λφ4(n),

(5)

is a potential term.

To calculate integral Eq. (1), we use Markov Chain-Monte Carlo. We

generate a sequence of conﬁgurations,

φ(1) → φ(2) → φ(3) → φ(4) → · · · → φ(Nconf ),

(6)

1In Julia programming language, index is started from 1 mostly. We follow this nota-

tion.

4

where φ(k) is a conﬁguration sampled from Eq. (3) and Nconf is the number
of samples. Typically, HMC (Hybrid/Hamiltonian Monte-Carlo) has been
used [28, 29]. We can calculate the expectation value through

(cid:104)O(cid:105) = lim

Nconf →∞

1
Nconf

Nconf(cid:88)

k=1

O[φ(k)].

(7)

In practice, we cannot take Nconf inﬁnity, the expectation value is suﬀered
from the statistical error,

(cid:104)O(cid:105) =

1
Nconf

Nconf(cid:88)

k=1

O[φ(k)] + O

(cid:33)

(cid:32)

1
(cid:112)Nindep

where Nindep is the number of independent samples deﬁned by

Nindep =

Nconf
2τac

(8)

(9)

and τac is the autocorrelation time [30].
In some parameter regime, typi-
cally critical regime, τac grows and the number of independent conﬁgurations
Nindep becomes small, which is called the critical slowing down. The autocor-
relation time τac depends on an algorithm, and here we implement the ﬂow
based sampling algorithm to reduce the autocorrelation time.

The ﬂow based sampling algorithm utilizes three probability distributions.
First is, the target distribution. We want to draw a sample from the target
as,

φ ∼ P (QFT)

{L,m2,λ}[φ].

(10)

where “∼” represents “sampling” from a probability distribution.

Second one is, a trivial distribution (a prior distribution in terms of ma-

chine learning),

ϕ ∼ P (pri)

ϑ

[ϕ],

(11)

which has a set of parameters ϑ, this is a ﬁxed parameter of the calculations
(hyperparameter). This probability distribution should be point-wise inde-
pendent distribution and this can be regarded as a physical distribution at
inﬁnite temperature for spin system, or, β = 0 distribution for lattice gauge
systems.
In terms of quantum ﬁeld theory, one can regard a probability
distribution for a ﬁeld without kinetic term.

5

We apply a parameterized transformation (like cooling process),

φ = F −1

θ

[ϕ],

where θ is a set of parameters in the map and such that,

{L,m2,λ}[φ] ≈ P (ML)
P (QFT)

Θ

[φ] = P (pri)

ϑ

[ϕ]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[ϕ]

∂F −1
θ
∂ϕ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

and Θ = θ ∪ ϑ and

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[ϕ]

∂F −1
θ
∂ϕ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(12)

(13)

(14)

is the Jacobian for the transformation. Quality of this map is measured by
(shifted and reversed) KullbackLeiblerdivergence,

(cid:90)

Dφ P (ML)
Θ

[φ] ln P (ML)

Θ

[φ]/P (QFT)

{L,m2,λ}[φ] − ln Z,

(15)

and this works as variational energy of approximation. By tuning parameters
θ to mimize KullbackLeiblerdivergence, we can realize approximated (un)
trivializing map.

Fig. 1 is a schematic overview for the ﬂow based sampling algorithm.
First, it samples from a conﬁguration with point-wise independent distri-
bution (in terms of gauge theory, it is sampling from β = 0). Next, we
apply the ﬂow transformation F −1
, un-trivializing map, written by a neural
network. This makes correlations between sites. Output from the network
can be regarded as a sample from β > 0 approximately. Finally, with the
Metropolis-Hastings test with the Jacobian, we obtain an ensemble which
obeys correct distribution e−S/Z. Training also follows the process above.

θ

Here we describe how can we realize the map Fθ (trivializing map) and
F −1
(un-trivializing map) in terms of neural networks. The structure of aﬃne
θ
coupling layer is essential to get tractable Jacobian. We employ checker-
board decomposition to get tractable Jacobian as in the original work [19].
We alternately apply small and reversible trivializing map, which is realized
using a neural network, on even and odd sites. By composing these maps,
we approximately realize a map between the target distribution to the trivial
distribution and one for the opposite direction. The aﬃne coupling layer is
realized as follows.

We consider the aﬃne coupling layer on odd sites ﬁrst. We denote φ and
ϕ as non-trivial and trivial side variables respectively. The aﬃne coupling

6

Figure 1: Schematic picture for the ﬂow based sampling algorithm. Here we use β to
indicate triviality of the system, which is a coeﬃcient of the kinetic term in our current
application (please see the main text in details).

layer (scaling and shift transformation for ﬁeld variable) is deﬁned as,

(cid:40)

ϕe = es[φo]φe + t[φo],
ϕo = φo

(16)

where s and t are output of a neural network,

T o
θ : φo (cid:55)→

(cid:19)

(cid:18)s
t

(cid:19)

(cid:18)s
t

=

[φo] =

(cid:19)

(cid:18)s
t

(φ1, φ3, · · · )

(17)

θ are deﬁned in similar manner.

and the arguments are ﬁeld values on odd sites for a conﬁguration. We
remark that this transformation is bijective. The transformation for even
sites T e
In this work, this map is realized
using two or three dimensional convolutional neural network. We can realize
approximate trivializing map by applying this map with diﬀerent weights
repeatedly,

Fθ[φ] = T o

θNtri

◦ T e

θNtri−1

7

◦ · · · ◦ T e

θ2 ◦ T o

θ1[φ].

(18)

where θ = θ1 ∪ θ2 ∪ · · · ∪ θNtri and θi is a set of weights in each neural
network, and Ntri is the number of aﬃne coupling layers. This map Fθ[ϕ] is
also bijective since T is bijective. Thus, we can obtain un-trivializing map
F −1
. Thanks to the even-odd structure of the transformation, the calculation
θ
for Jacobian is O(N ) [19]. We remark that, (un-)tirivializing procedure is
analogous to integration steps in the gradient ﬂow [17].

This package is implemented by Julia, which is a new scientiﬁc program-
ming language [31]. Typically Julia is fast as C and Fortran [32], and it
is ﬁt to calculations in lattice QCD community. The ﬂow based sampling
algorithm has been implemented in Python/PyTorch [25], and this package
provides another implementation.

2. Software description

2.1. Software Architecture

This package works as follows (Fig. 2). First, a parameter conﬁgura-
tion ﬁle (e.g. “cfgs/example2d.toml”) is loaded. Parameters in the ﬁle are
classiﬁed as four groups. DeviceP arams (dp) speciﬁes what kind of acceler-
ation devices is used (-1 for no accelerator). T rainingP arams (tp) controls
training procedure (e.g. epoch, batchsize, optimiser). P hysicalP arams (pp)
should contain parameters of the target system (mass, and counpling, and
system size). M odelP arams (mp) is a set of parameters for a neural network
in the ﬂow model. These parameters are loaded through load hyperparams
function.

The code construct a objects according to the loaded hyper-parameters
and physical parameters on the memory. The objects is for example, action of
the target theory, neural network model, an optimizer, and setting of training
like, the number of epochs, batch size, and scheduling of learning rate (lr).

The simulation is performed with parameters with the action deﬁned in
src/action.jl. Jacobian associated with the transformation and all deriva-
tives are calculated with Flux.jl and Zygote.jl.

2.2. Software Functionalities

In the current version, the ﬂow based sampling algorithm for 2 and 3
dimensional scalar ﬁeld theory can be used in a Docker environment, which
is explained in the following section. HMC for these theories are implemented
to compare results. HMC uses same struct for parameters.

Jupyter lab environment for analysis, e.g. the condensate, zero-momentum
two point functions, calculation of the autocorrelation time, are available in
playground/notebook/julia/analysis tool.md. This ﬁle is a markdown ﬁle
but will be converted in ipynb ﬁle via jupytext automatically.

8

Figure 2: An illustration of code architecture

3. Illustrative Examples

Use of Docker image is the easiest way to use this package2.

In the
repository, one can execute the training, with commands in Listing 1. A ﬁle
cf gs/example2d.toml is a parameter ﬁle, which is explained below.

1 $ make
2 $ docker - compose run -- rm julia julia begin_training . jl cfgs /

example2d . toml

Listing 1: Command to start training

Then the training for the two dimensional scalar ﬁeld will be started. The
results are saved in results directory.

We provide example ﬁles for two dimensional scalar ﬁeld theory and three
dimensional scalar ﬁeld theory for broken and symmetric phases. Hyper-
parameters for training and parameters for physical system can be written
as Listing 2. This performs training with 200 epochs and a prior distribution
a Gaussian N (0, 1). A neural network is 16 layers with hidden size is 8 × 8.
Training parameter can be understood by a comments which started in line
14 in the list.

1 [ config ]

2To use Docker with NVIDIA GPU, we recommended to follow NVIDIA provided pro-
cedure in https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
install-guide.html#docker.

9

dp::DeviceParamstp::TrainingParamspp::PhysicalParamsmp::ModelParamsconfig.tomlSetup parametersload_hyperparamsTraining modelInitializationfor (epoch, eta) in zip(1:epochs, scheduler)opt.eta = etafor _ in 1:iterationsz = rand(prior, lattice_shape..., batchsize)logq = sum(logpdf.(prior, z),dims=(1:ndims(z)-1))gs = Flux.gradient(ps) dox, logq = model(z, logq)logp = -action(x)loss = calc_dkl(logp, logq)endFlux.update!(opt, ps, gs)endendreturn model # trained modeltraindevice = dp.device # e.g. GPU No. 0action = ScalarPhi4Action(pp.M2, pp.lam)lattice_shape = pp.lattice_shapelayers = [AffineCoupling(mp.kernel_sizemp.hidden_size)for _ in mp.n_layers]model = Chain(layers…)ps = get_training_params(model)eta = tp.lr e.g. 0.001opt = tp.opt # e.g. Adam(eta)scheduler = tp.lr_schedulerIterations = tp.iterationsbatchsize = tp.batchsizeepochs = tp.epochsprior = Normal(0, 1)HyperParamsdp, tp, pp, mp2 version = "0.1.0"
3
4 [ device ]
5 # setup DeviceParams for example
6 # device_id = -1 # <--- train with CPU
7 # device_id = 0 # <--- train with GPU its Device ID is 0
8 # device_id = 1 # <--- train with GPU its Device ID is 1
9 device_id = -1
10

# extract batchsize data
z = rand ( prior , lattice_shape ... , batchsize )
gs = Flux . gradient ( ps ) do

end
Flux . Optimise . update !( opt ( base_lr ) , ps , gs )

end

do something

for _ in 1: iterations

11
12 # setup TrainingParams
13 #
14 # for _ in 1: epochs
15 #
16 #
17 #
18 #
19 #
20 #
21 #
22 #
23 # end
24 [ training ]
25 seed = 12345
26 batchsize = 64
27 epochs = 200
28 iterations = 100
29 base_lr = 0.001
30 opt = " Adam "
31 prior = " Normal { Float32 }(0. f0 , 1. f0 ) "
32 lr_scheduler = ""
33 pretrained = ""
34
35 # setup PhysicalParams
36 #
37 # lattice_shape = (L , L ) if Nd = 2
38 # lattice_shape = (L , L , L ) if Nd = 3
39 # action = ScalarPhi4Action ( M2 , lam )
40 [ physical ]
41 L = 8
42 Nd = 2
43 M2 = -4.0 # m ^2
44 lam = 8.0 #
45

46
47 # setup ModelParams
48 #
49 [ model ]
50 seed = 2021

10

51 n_layers = 16
52 hidden_sizes = [8 , 8]
53 kernel_size = 3
54 inC = 1
55 outC = 2
56 use_final_tanh = true
57 use_bn = false

4. Impact

Listing 2: example2d.toml

This software enables us to perform extensive research with the ﬂow based
sampling algorithm in two dimension and three dimension on CPU and GPU,
which is preferred since modern computer clusters have GPU accelerater.
More and more works with the ﬂow based sampling algorithm are publishing,
and most of them are based on a Python code [25]. We provide a new code
in Julia language with Docker environment. Thanks to Julia environment,
this package can be run on large scale supercomputer or a culster with GPU
if the machine supports Julia language.

This package has also advantage for prototyping. If one wants to change
the theory, one can achieve it by changing src/action.jl. Diﬀerentiation can
be done by Flux.jl/Zygote.jl automatically as is explained above.

5. Conclusions

We implement ﬂow-based sampling algorithm for two or three dimen-
sional lattice scalar ﬁeld theory in Julia programming language. This pack-
age extensively use automatic diﬀerenciation in Flux.jl/Zygote.jl to calculate
In addition, this package
Jacobian in the ﬂow based sampling algorithm.
provides, HMC, analysis tools for autocorrelation and ESS (eﬀective sample
size) calculation with Jupyter notebook. A Docker image is provided which
enables us to perform simulations without eﬀort on environment construction
and this package works not only CPU but NVIDIA GPU. Hyper-parameters
and parameters including dimensionality can be speciﬁed in the a parameter
ﬁle.

This package pave a way of research directions not only the reduction of
autocorrelation time [19, 20, 22], scaling study [26], and investigation with
sign problem [33].

11

6. Conﬂict of Interest

There are no known conﬂicts of interest associated with this publication
and there has been no signiﬁcant ﬁnancial support for this work that could
have inﬂuenced its outcome.

Acknowledgements

Authors thank to a public notebook [25], and we refer the code for the
implementation. This work of AT was supported by JSPS KAKENHI Grant
Number JP20K14479, JP22H05112, JP22H05111, and JP22K03539.

References

[1] R. Frezzotti, P. A. Grassi, S. Sint, P. Weisz, Lattice QCD with a chirally
twisted mass term, JHEP 08 (2001) 058. arXiv:hep-lat/0101001, doi:
10.1088/1126-6708/2001/08/058.

[2] S. Capitani, M. L¨uscher, R. Sommer, H. Wittig, Non-perturbative quark
mass renormalization in quenched lattice QCD, Nucl. Phys. B 544 (1999)
669–698, [Erratum: Nucl.Phys.B 582, 762–762 (2000)]. arXiv:hep-lat/
9810063, doi:10.1016/S0550-3213(98)00857-8.

[3] D. E. Groom, et al., Review of particle physics. Particle Data Group,

Eur. Phys. J. C 15 (2000) 1–878.

[4] Y. Aoki, et al., FLAG Review 2021 (11 2021). arXiv:2111.09849.

[5] C. Gattringer, C. B. Lang, Quantum chromodynamics on the lattice,
Vol. 788, Springer, Berlin, 2010. doi:10.1007/978-3-642-01850-3.

[6] S. Durr, et al., Ab-Initio Determination of Light Hadron Masses, Sci-
ence 322 (2008) 1224–1227. arXiv:0906.3599, doi:10.1126/science.
1163233.

[7] A. Bazavov, et al., Equation of state in ( 2+1 )-ﬂavor QCD, Phys. Rev.
D 90 (2014) 094503. arXiv:1407.6387, doi:10.1103/PhysRevD.90.
094503.

[8] S. Borsanyi, G. Endrodi, Z. Fodor, A. Jakovac, S. D. Katz, S. Krieg,
C. Ratti, K. K. Szabo, The QCD equation of state with dynami-
cal quarks, JHEP 11 (2010) 077. arXiv:1007.2580, doi:10.1007/
JHEP11(2010)077.

12

[9] T. Aoyama, et al., The anomalous magnetic moment of the muon in the
Standard Model, Phys. Rept. 887 (2020) 1–166. arXiv:2006.04822,
doi:10.1016/j.physrep.2020.07.006.

[10] S. Schaefer, R. Sommer, F. Virotta, Critical slowing down and error
analysis in lattice QCD simulations, Nucl. Phys. B 845 (2011) 93–119.
arXiv:1009.5228, doi:10.1016/j.nuclphysb.2010.11.020.

[11] M. Luscher, Trivializing maps, the Wilson ﬂow and the HMC algorithm,
Commun. Math. Phys. 293 (2010) 899–919. arXiv:0907.5491, doi:
10.1007/s00220-009-0953-7.

[12] H. Nicolai, On a New Characterization of Scalar Supersymmetric The-
doi:10.1016/0370-2693(80)

ories, Phys. Lett. B 89 (1980) 341.
90138-0.

[13] D. Boyda, et al., Applications of Machine Learning to Lattice Quantum
Field Theory, in: 2022 Snowmass Summer Study, 2022. arXiv:2202.
05838.

[14] A. Tanaka, A. Tomiya, Towards reduction of autocorrelation in HMC

by machine learning (12 2017). arXiv:1712.03893.

[15] J. M. Pawlowski, J. M. Urban, Reducing Autocorrelation Times
in Lattice Simulations with Generative Adversarial Networks, Mach.
Learn. Sci. Tech. 1 (2020) 045011. arXiv:1811.03533, doi:10.1088/
2632-2153/abae73.

[16] K. Zhou, G. Endrodi, L.-G. Pang, H. St¨ocker, Generative Model Study
for 1+1d-Complex Scalar Field Theory, PoS AISIS2019 (2020) 007. doi:
10.22323/1.372.0007.

[17] A. Tomiya, Y. Nagai, Gauge covariant neural network for 4 dimensional

non-abelian gauge theory (3 2021). arXiv:2103.11965.

[18] S. Foreman, X.-Y. Jin, J. C. Osborn, LeapfrogLayers: A Trainable
Framework for Eﬀective Topological Sampling, PoS LATTICE2021
(2022) 508. arXiv:2112.01582, doi:10.22323/1.396.0508.

[19] M. S. Albergo, G. Kanwar, P. E. Shanahan, Flow-based generative mod-
els for Markov chain Monte Carlo in lattice ﬁeld theory, Phys. Rev. D
100 (3) (2019) 034515. arXiv:1904.12072, doi:10.1103/PhysRevD.
100.034515.

13

[20] G. Kanwar, M. S. Albergo, D. Boyda, K. Cranmer, D. C. Hackett,
S. Racani`ere, D. J. Rezende, P. E. Shanahan, Equivariant ﬂow-based
sampling for lattice gauge theory, Phys. Rev. Lett. 125 (12) (2020)
121601. arXiv:2003.06413, doi:10.1103/PhysRevLett.125.121601.

[21] D. Boyda, G. Kanwar, S. Racani`ere, D. J. Rezende, M. S. Albergo,
K. Cranmer, D. C. Hackett, P. E. Shanahan, Sampling using SU (N )
gauge equivariant ﬂows, Phys. Rev. D 103 (7) (2021) 074504. arXiv:
2008.05456, doi:10.1103/PhysRevD.103.074504.

[22] M. S. Albergo, G. Kanwar, S. Racani`ere, D. J. Rezende, J. M. Urban,
D. Boyda, K. Cranmer, D. C. Hackett, P. E. Shanahan, Flow-based
sampling for fermionic lattice ﬁeld theories, Phys. Rev. D 104 (11) (2021)
114507. arXiv:2106.05934, doi:10.1103/PhysRevD.104.114507.

[23] R. Abbott, et al., Gauge-equivariant ﬂow models for sampling in lattice
ﬁeld theories with pseudofermions (7 2022). arXiv:2207.08945.

[24] R. Abbott, et al., Sampling QCD ﬁeld conﬁgurations with gauge-
equivariant ﬂow models, in: 39th International Symposium on Lattice
Field Theory, 2022. arXiv:2208.03832.

[25] M. S. Albergo, D. Boyda, D. C. Hackett, G. Kanwar, K. Cranmer,
S. Racani`ere, D. J. Rezende, P. E. Shanahan, Introduction to Normal-
izing Flows for Lattice Field Theory (1 2021). arXiv:2101.08176.

[26] L. Del Debbio, J. M. Rossney, M. Wilson, Eﬃcient modeling of trivi-
alizing maps for lattice φ4 theory using normalizing ﬂows: A ﬁrst look
at scalability, Phys. Rev. D 104 (9) (2021) 094507. arXiv:2105.12481,
doi:10.1103/PhysRevD.104.094507.

[27] S. Foreman, T. Izubuchi, L. Jin, X.-Y. Jin, J. C. Osborn, A. Tomiya,
HMC with Normalizing Flows, PoS LATTICE2021 (2022) 073. arXiv:
2112.01586, doi:10.22323/1.396.0073.

[28] S. Duane, A. D. Kennedy, B. J. Pendleton, D. Roweth, Hybrid Monte
Carlo, Phys. Lett. B 195 (1987) 216–222. doi:10.1016/0370-2693(87)
91197-X.

[29] M. A. Clark, A. D. Kennedy, Accelerating dynamical fermion com-
putations using the rational hybrid Monte Carlo (RHMC) algorithm
with multiple pseudofermion ﬁelds, Phys. Rev. Lett. 98 (2007) 051601.
arXiv:hep-lat/0608015, doi:10.1103/PhysRevLett.98.051601.

14

[30] M. Luscher, Schwarz-preconditioned HMC algorithm for two-ﬂavour
lattice QCD, Comput. Phys. Commun. 165 (2005) 199–220. arXiv:
hep-lat/0409106, doi:10.1016/j.cpc.2004.10.004.

[31] J. Bezanson, S. Karpinski, V. B. Shah, A. Edelman, Julia: A fast dy-
namic language for technical computing (2012). doi:10.48550/ARXIV.
1209.5145.
URL https://arxiv.org/abs/1209.5145

[32] J. contributors, Julia Micro-Benchmarks, https://julialang.org/

benchmarks/, [Online; accessed 18-August-2022] (2008).

[33] M. Rodekamp, E. Berkowitz, C. G¨antgen, S. Krieg, T. Luu, J. Ost-
meyer, Mitigating the Hubbard Sign Problem with Complex-Valued
Neural Networks (3 2022). arXiv:2203.00390.

15

