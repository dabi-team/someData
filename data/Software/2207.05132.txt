2
2
0
2

l
u
J

1
1

]
E
S
.
s
c
[

1
v
2
3
1
5
0
.
7
0
2
2
:
v
i
X
r
a

Dev2vec: Representing Domain Expertise of Developers
in an Embedding Space

Arghavan Moradi Dakhela, Michel C. Desmaraisa, Foutse Khomha

aDepartment of Computer and Software Engineering, Polytechnique
Montreal, Montreal, H3T 1J4, Quebec, Canada

Abstract

Accurate assessment of the domain expertise of developers is important for
assigning the proper candidate to contribute to a project, or to attend a
job role. Since the potential candidate can come from a large pool, the
automated assessment of this domain expertise is a desirable goal. While
previous methods have had some success within a single software project,
the assessment of a developer’s domain expertise from contributions across
multiple projects is more challenging.

In this paper, we employ doc2vec to represent the domain expertise of
developers as embedding vectors. These vectors are derived from diﬀerent
sources that contain evidence of developers’ expertise, such as the description
of repositories that they contributed, their issue resolving history, and API
calls in their commits. We name it dev2vec and demonstrate its eﬀectiveness
in representing the technical specialization of developers.

Our results indicate that encoding the expertise of developers in an em-
bedding vector outperforms state-of-the-art methods and improves the F1-
score up to 21%. Moreover, our ﬁndings suggest that “issue resolving history”
of developers is the most informative source of information to represent the
domain expertise of developers in embedding spaces.

Keywords: Expertise of developers, Word embedding, Expert classiﬁcation,
Technical role, GitHub

1. Introduction

Large software projects require technical experts in diﬀerent domains [1].
Accurate assessment of developers’ expertise impacts the success of software

Preprint submitted to Nuclear Physics B

July 13, 2022

 
 
 
 
 
 
projects [2]. This assessment includes details of their soft skills, such as com-
munication skills [3, 4] and technical skills, such as programming languages
or libraries [5, 6]. Automatic identiﬁcation of the technical expertise of de-
velopers in diﬀerent domains is a signiﬁcant challenge in software engineering
because the appropriate candidate is generally selected from a large pool.

Platforms such as GitHub gather useful information from a large pool of
programmers that an be mined to extract the domain expertise of developers
based on their activities [7]. These platforms include the activity of devel-
opers over diﬀerent open source repositories. However, GitHub is a version
controlling system and is not an appropriate tool for speciﬁcally searching
and ﬁltering experts. A survey [8] shows recruiters use platforms such as
GitHub to search for an expert and 66% of non-technical recruiters struggled
with technical information contained in these platforms. Therefore, methods
to process the information in such platforms and assess the domain expertise
of developers are required.

Some studies have relied on general heuristics such as Line 10 rule and
count the number of contributions of developers on diﬀerent source ﬁles to
ﬁnd experts [9, 10]. Other studies present more speciﬁc methods to identify
an expert to perform a task, ﬁx a bug, or use a library. They count the
frequency of API calls in their commits [6, 11, 12] or use methods such
as bag-of-words on textual information available to infer the expertise of
developers [13, 5].

While these methods show good performance in identifying an expert
within a software project, they suﬀer from data sparsity and high dimen-
sionality when used to represent the domain expertise of developers across
diﬀerent types of activities and diﬀerent software projects [14]. Also, these
methods struggle to extract the semantics of words.

One of the solutions to extend the semantic capabilities of these meth-
ods is to rely on embeddings. Words, or aggregation of words, are en-
coded as ﬁxed-length vectors, resulting in low dimensionality representation.
word2vec [15] and doc2vec [16] are well known examples. These methods
are known to improve the performance of diﬀerent learning tasks in natural
language processing such as classiﬁcation [17]. A recent study [18] recom-
mends new contributors to a software project by applying doc2vec on the list
of APIs in source ﬁles that developers changed in diﬀerent software projects
in the past.

In this paper, we demonstrate a suitable presentation for the domain ex-
pertise of developers. It captures semantic similarity within diﬀerent domains

2

of expertise. We address the problem of identifying the domain expertise of
developers across their activities in various software projects on GitHub. We
employ doc2vec to encode the domain expertise of developers in embedding
vectors.

There are diﬀerent types of information about developers’ activities on
GitHub. These sources of information can inform the representation of de-
velopers’ expertise. We collect three diﬀerent sources of information from
GitHub that are prominent sources to assess the domain expertise of devel-
opers [5, 13, 19]. We derive embedding vectors of developers’ expertise from
the meta-data of repositories that developers contributed to, the issue resolv-
ing history of developers and the list of APIs in changes applied by develop-
ers on diﬀerent source ﬁles. We name these methods after their respective
sources: dev2vec:Repos, dev2vec:Issues and dev2vec:APIs, respectively.
In
addition, we merge the output of these three methods by concatenating the
embedding vectors from these three diﬀerent spaces of information and call
it dev2vec:RIAs (RIA stands for Repository, Issue and API).

We evaluate the performance of each dev2vec model in reﬂecting the
domain expertise of developers over a favored task in software engineering:
identifying the technical job roles of developers. We use a labeled dataset
of GitHub developers. They are categorized into ﬁve job roles: Backend,
Frontend, Mobile, DevOps, and Data Scientist [14]. Speciﬁcally, we answer
three diﬀerent research questions in this work:
RQ1: How eﬀective are embedding vectors to represent the domain expertise
of developers across various software projects compared to the state-of-the-
art methods?
RQ2: How sensitive is the performance of dev2vec to the source of informa-
tion (repositories, issues and API)?
RQ3: How eﬀective is the concatenation of expertise embedding vectors
from the diﬀerent information sources?

Our experimental results show that encoding the expertise of developers
in an embedding vector outperforms a state-of-the-art study [14] that used
bag-of-words with dimension reduction techniques on GitHub information to
represent the expertise of developers. Also, the performance of dev2vec is
sensitive to the sources of information: issue resolving history of developers
results in better performance.

This paper makes the following contributions:

3

• Propose a method to represent the expertise of developers in embed-
ding vectors by applying doc2vec on diﬀerent sources of information:
dev2vec:Repos, dev2vec:Issues, and dev2vec:APIs.

• Propose to aggregate three diﬀerent types of information about devel-
opers’ activities across diﬀerent projects on GitHub and represent the
domain expertise of developers by concatenating the embedding vectors
of developers’ expertise from diﬀerent spaces: dev2vec:RIAs.

• Evaluate the eﬀectiveness of dev2vec to represent the domain exper-
tise of developers by a prevalent problem in the software engineering
domain: assessing the technical specialization of developers.

The rest of this paper is organized as follows. Section 2 explains
the ground truth and describes diﬀerent datasets used in experiments. Sec-
tion 3 describes the methodology of our study. Section 4 presents the baseline
and experimental setup. Section 5 highlights diﬀerent factors that constitute
threats to the validity of our experiment. In Section 6, we discuss our ﬁnd-
ings and their application in other studies. Section 7 introduces the related
literature. Finally, Section 8 concludes the paper and discusses avenues for
future work.

2. Data collection

To determine the domain expertise of developers, we rely on three diﬀer-
ent types of information available from GitHub. The ﬁrst one is the infor-
mation of repositories that they contributed to GitHub. The second is their
issue resolving history on diﬀerent repositories. The last is the list of APIs
in changes applied by them across diﬀerent repositories. Figure 2 shows an
overview of the data collection process. In the rest of this section, we ﬁrst
introduce how we obtain the ground truth labeling. Then, we explain how
to collect the data for these three categories.

2.1. Ground Truth

Our training and validation method relies on the availability of labeled
data that represents the ground truth developer expertise. We used a labeled
dataset of developers published in [14]. It includes 1662 developers, catego-
rized into ﬁve job roles: Backend, Frontend, Mobile, DevOps, and Data Sci-
entist. They did not use GitHub’s data to generate expertise labels. Instead,

4

Figure 1: The distribution of developers in diﬀerent job roles. The distribution of
1272 developers in ﬁve job roles: Backend, Frontend, Mobile, DevOps, and Data Scientist.
This distribution is imbalanced.

they used the StackOverﬂow data of its users with GitHub proﬁles. They
extracted users’ job roles by applying diﬀerent regular expressions to the
proﬁle information of their StackOverﬂow. Also, they collected the GitHub
username of these developers to link them to their GitHub pages.

20% of these developers have more than one label. We limit the scope of
this study to developers with single domain expertise and kept 1340 develop-
ers with a single job role. Out of these 1340 developers, we successfully ﬁnd
the GitHub proﬁle of 1272 developers with their usernames. The remaining
68 developers may have changed their usernames or closed their accounts
on GitHub. Figure 1 shows the distribution of developers in these ﬁve job
categories.

2.2. Textual Information of Repositories

Based on previous studies, information about the repositories that devel-
opers contributed in the past is a good reference to determine their skills and
domain of expertise [13, 19].

In GitHub, each repository has diﬀerent features in natural language.
These features can describe its purpose of the domain. They are included
Name, Tags, Topics and ReadMe. “Name” is the repository’s title, and it is
mandatory. Repository names can be chosen in a way to represent the goal of
the project to some extent. However, these names are short, and are not very

5

Figure 2: Overview of the pipeline. This is an overview of the proposed methods.
Three separate sources of information are collected to represent developer expertise as
embedding vectors.

descriptive (e.g. “eth-tester-rpc 1”). “Tags” can be assigned to a repository
to specify its domain, programming language or technologies are used in
the project (i.e. #ethereum, #python, #crypto). Tags are not mandatory
and can be left empty. “Topic” or “About” is a short description, up to
two or three sentences, about the goal of the repository. The “ReadMe” ﬁle
describes more details such as the application of the project, the functionality
of diﬀerent source ﬁles, programming languages used inside the repository,
etc. Topic and ReadMe are optional, too.

We combine the content of all these four features for each repository and
call it the textual information of repositories. Then, we collect the textual
information of all repositories and aggregate it on a per developer basis. The
output is a document per developer that contains the textual information
from repositories they contributed.
If a repository is forked, we ﬁnd the
parent repository and, if the developer has any participation (commit) on
the parent repository, we collect its description too. Overall, we collect the
textual information of 58K repositories for all 1272 developers.

1https://github.com/voith/eth-tester-rpc

6

GitHubMerged APIs inCommits perdeveloperMerged TextualInformation ofRepositories perdeveloperMerged IssueResolving History perdeveloperAPI Calls in  CommitsTextual Information  of RepositoriesList of  DevelopersIssue Resolving  History Traindev2vecmodelsEmbedding vectors of developers expertiseClassification ofDevelopers in job roles2.3. Issue Resolving History

Issue resolving history of developers across diﬀerent projects on GitHub
is another source that contain useful information about their domain exper-
tise [20, 5]. Developers can have diﬀerent types of contributions on an issue
in GitHub repositories. An issue can be assigned to a developer or a list of
developers. It can be created by a developer on a repository. Alternatively, a
developer can participate in the discussion of an issue. We consider all these
cases as a contribution of developers to solving an issue on GitHub and call
it the Issue Resolving History of developers. The mixture of these activities
can give us valuable information about developers’ expertise.

We collect the header and the body of 60K issues assigned to 1272 de-
velopers. We combine the header and the body of each issue as its content.
Then for each developer, we merge the content of all her/his issue resolving
history in the past. Akin to the textual information of repositories, the out-
put of this step is a document per developer, and each document contains
the issue resolving history of a developer.

2.4. API Calls per Commits

The main contributions of developers on diﬀerent repositories are in the
format of commits. Another useful piece of information that is considered
as evidence of developers’ expertise is the list of API calls in commits across
diﬀerent repositories on GitHub. [12, 21]. Developers change one or more
source ﬁle(s) by submitting a commit. Each source ﬁle contains a list of
APIs that developer may or may not change. Based on previous studies, we
assume that developers have a basic knowledge about the APIs used in the
source ﬁles they modiﬁed [18].

For this step, per each commit, we extract all language-speciﬁc source
ﬁles linked to a commit after submitting it. Overall, we extract source ﬁles
of 21M commits (the changed version of the source ﬁle after a commit). As
a result, for each developer, we merge all APIs in the source ﬁles of their
commits. In section 3.4, we explain more details about the process of API
collection of developers’ commits.

3. Deriving Dev2vec from doc2vec

In this section, we describe how to represent the expertise of devel-
opers in embedding vectors with three proposed methods, dev2vec:Repos,
dev2vec:Issues and dev2vec:APIs, and describe how to train diﬀerent the

7

Figure 3: Detailed view of the pipeline. A more detailed view of the three proposed
methods, dev2vec:Repos, dev2vec:Issues and dev2vec:APIs. The embedding vectors gen-
erated by each of these models represent the expertise of developers in diﬀerent embedding
spaces. Each category of embedding vectors is fed into a classiﬁer separately to evaluate
the their eﬀectiveness in representing developer’s expertise

models with the data sources from Section 2, and how . Also, we explain
dev2vec:RIAs, the method for combining these three embedding vectors of
expertise.

3.1. The embedding of domain expertise with doc2vec

Assessing the expertise of developers across diﬀerent software projects
and diﬀerent programming languages, regardless of the data source, increases
the number of distinct tokens. For example, after cleaning the description of
repositories, we end up with 18M distinct keywords. The dimension increases
to 100M-dimension for API calls [18]. Previous methods that show good
performance to ﬁnd an expert within a single software project, such as bag-
of-words method, are not practical with these high dimensions.

As we already discussed, one of the solutions to address these kind of
problems is to encode this information into ﬁxed-length vectors with much
lower dimensions by word2vec or doc2vec algorithms. word2vec [15] learns a
numerical vector representation for each word in a corpus of documents with

8

GitHubBuild Tagged  DocumentsCollect APIs inCommitsAth_train_2:Ath_train_1:...Ath_test_1:Ath_test_2:......train a doc2vecmodel on trainset dev2vec:  issuesinfer vectors fordevelopers intestsetAth_test_1:Ath_test_2:...... dev2vec:APIs(pretrained) YesNodeveloper is  seen by pretrainedmodelGet vectors foreach API used bydevelopersAth_train_1:Ath_train_2: [author id 1] [w1,w2, w3,..,.]... dev2vec:repos Get vectors fordevelopers intrainsetAth_train_1:Ath_train_2:...Build Tagged_ DocumentsCollect TextualInformation ofRepositoriesTagstrain a doc2vecmodel on trainsetinfer vectors fordevelopers intestsetBuild Tagged  DocumentsCollect IssueResolving History [author id 1] [w1,w2, w3,..,.] [author id 2] [w1,w2, w3,..,.]...TagsDocumentsGet vectors fordevelopers intrainset [author id 1] [API1, API2, ...]TagsDocumentsGet vectors fordeveloperAverage APIvectors perdeveloperclassifier:  classify developersin job roles [author id 2] [API1, API2, ...]Documents [author id 2] [w1,w2, w3,..,.]...Ath_test_1:Ath_test_2:two diﬀerent algorithms CBOW and skip-gram. The vector size is much less
than the vocabulary dimension, mostly between 100 to 300 [15].

An extension of word2vec is doc2vec. It represents each document in an
embedding vector and is trained based on predicting the words of a document
within a corpus. The text that we feed into doc2vec can have variable-length
ranging from a single sentence to a large document [16]. In doc2vec, each
document has a paragraph id or tag. This tag can be a single identiﬁer or
a list of identiﬁers per document. Doc2vec learns an embedding vector for
each identiﬁer of documents and each word of documents. This embedding
vector is a representation of the document.

The Doc2vec algorithm has two stages. The ﬁrst stage is the training.
There are two approaches to training document vectors and word vectors.
The ﬁrst one is: Distributed Memory model (DM). In this approach docu-
ment embedding vector acts as an additional input word, and the average
(or concatenation) of its vector with the embedding vectors of the rest of
input words predicts output word. The embedding vector of documents are
considered as a memory of the document content. The second approach is:
Distributed Bag Of Words (DBOW). In DBOW, the document embedding
vector is learned to predict randomly sampled words in output [16].

The second stage is the inference phase (or predicting phase). In inferring
stage, the vectors of new documents are inferred by concatenating their word
vectors and repeating several iterations until convergence. Words that were
unseen in training are ignored.

3.2. dev2vec:Repos

The dev2vec:Repos model uses the data collected in Section 2.2 to train
a doc2vec model. We consider the textual information of repositories per
developer as a document. After tokenization and cleaning, we build tagged-
document data by tagging each document by the username of its developer or
Author id. The ﬁrst pipeline in Figure 3 shows the data entries that we use
to build dev2vec:Repos. It includes [developer id] as tag or paragraph id and
[w1, w2, ..., wn] are words collected
[w1,w2,...,wn] as document content.
from the description of repositories that a developer contributed in the past.
We divide developers into traintet and testset. We train the doc2vec
model on tagged documents of developers in trainset and infer vectors for
developers in testset based on the content of their documents. It is worth
mentioning that if there is a new word in textual information of repositories
for a developer in testset (the word is not seen during the trainset), the

9

inference stage ignores that new word.
details on the experimental setup of this model.

In Section 4.3, we provide more

3.3. dev2vec:Issues

The dev2vec:Issues model uses the data described in Section 2.3. Akin
to the approach in Section 3.2, we consider the issue resolving history of
each developer as a document. We tokenize and clean the content of each
document. Then, we tag each document by its related developer’s username
or Author id to build the tagged documents. The second pipeline in Figure 3
shows the format of data to train dev2vec:Issues. It includes [developer id] as
the tag and [w1,w2,...,wn] as document content. [w1, w2, ..., wn] are words
collected from the title and body of issues that a developer solved, created,
or contributed in the past.

In the training phase, the doc2vec model learns a vector for each developer
in the trainset. Then, we infer or predict vectors for developers in the testset
based on this new model. Akin to the inference stage in Section 3.2, if a new
word appears in issue resolving history of a developer in testset, it will be
ignored. Section 4.3 provides more details on the experimental setup of this
model.

3.4. dev2vec:APIs

The dev2vec:APIs model uses a pre-trained doc2vec model [18], which
is trained on API calls in source ﬁles collected from GitHub. We chose
this model because it is trained on a vast dataset of 36K projects, 690K
authors, and 1.2B source ﬁles after submitting commits. Training such a
huge model is time-consuming and needs high-capacity infrastructure.
In
addition, we cannot infer the embedding vector for new developers using
this model directly (not seen in their training phase) due to the granularity
level of data entries in their model. Therefore we introduce a new method
with dev2vec:APIs to use this model for predicting the embedding vector of
developers’ expertise based on their API usage.
It allows us to indirectly
compare this model with dev2vec:Repos and dev2vec:Issues. In the following
section, we provide more details on diﬀerent parts of dev2vec:APIs.

3.4.1. Parsing source ﬁles

Since we use the pre-trained model in [18], we follow the same structure
to collect the API calls in the source ﬁles. They collected the list of libraries
(imports) in the version of source ﬁles after submitting a commit for 17

10

programming languages: C, C#, Java, FORTRAN, Go, JavaScript, Python,
R, Rust, Scala, Perl, Ruby, Dart, Kotlin, TypeScript, Julia and Jupyter
Notebook (iPython). We parse each source ﬁle based on the syntax of its
programming language and collect the list of its libraries for each developer.
We match the extension of each source ﬁle after submitting a commit with
its programming language. Then, based on the grammar of its programming
language, we apply the related regular expression to collect its list of libraries.
For example, for Python, we use this regular expression to ﬁnd libraries:
“(? : f rom|import)\s + \w ∗ .+; ∗”.

3.4.2. Obtaining embedding vectors of developers expertise

In the pre-trained model [18] the granularity level of the input data is fed
into the doc2vec model, is per commit (changed version of source ﬁle after
submitting a commit). The model used a list of identiﬁers: [programming
language, repository, timestamp, author id] as the document tag and the
list of APIs in the changed version of the source ﬁle as the content of the
document or words. The Author id belongs to the developer who modiﬁed
this source ﬁle by submitting the commit.

In the inference stage of doc2vec, we must infer document vectors for
the same granularity level of documents as in the trainset. However, we
can get or fetch the vector of documents and words which have been seen
and trained during training. Of all 1272 developers in our dataset, only 40
of them are in their dataset. We directly extract the embedding vectors of
these 40 developers from their trained model.

One solution to predict the embedding vectors for the rest of the develop-
ers in our dataset, using this pre-trained model, could be using the embedding
vectors of APIs learned in this model. We concatenate the vectors of all APIs
in the list of each developer’s contributions. Similar to the inference stage
of doc2vec, we ignore the API calls which are not seen by this pre-trained
model. We calculate the average of all these API embedding vectors to build
a single vector that represents the domain expertise of the developer. The
bottom pipeline in Figure 3 shows the steps of dev2vec:APIs.

3.5. Dev2vec:RIAs

The previous sections generate embedding vectors from single sources.
We now turn to a method to aggregate sources. Two of the data sources
are in natural language and one is in the programming language. We merge
sources by concatenating the embedding vectors of developers expertise which

11

Figure 4: Dev2vec:RIAs. We concatenate three embedding vectors generated in three
diﬀerent spaces of dev2vec:Repos, dev2vec:Issues and dev2vec:APIs to represent the ex-
pertise of developers

are generated from these three spaces. Figure 4 shows the pipeline for this
section.

Suppose that the embedding size of vectors generated by dev2vec:Repos,
dev2vec:Issues and dev2vec:APIs are “srepo”, “sissue” and “sapi” respectively.
Then, the size of the concatenated vector is “srepo + sissue + sapi”. In Sec-
tion 4.3, we return to the size of embedding vectors.

Additionally, after concatenation, we explore dimension reduction to see
if we can obtain performance improvements. We apply PCA for this purpose.
Then, we use both concatenated vectors before and after dimension reduction
and compare their performance in representing developers’ expertise.

4. Evaluation

In this section, we ﬁrst describe how we evaluate the eﬃciency of each
dev2vec by the technical specialization of developers. Then we describe a
state-of-the-art method as the baseline comparison method and explain the
setup for each dev2vec model. Finally, we discuss our research questions in
light of the results.

4.1. Technical Specialization of Developers

To evaluate the eﬃciency of each dev2vec model to represent the expertise
of developers, we choose a common task in software engineering: the classi-
ﬁcation of developers into job roles. We use ﬁve roles: Backend, Frontend,
Mobile, DevOps, and Data Scientist. As shown in Figures 3 and 4, we feed

12

Embedding vectorsfrom dev2vec:ReposEmbedding vectorsfrom dev2vec:IssuesEmbedding vectorsfrom dev2vec:APIsAuthor id 1:Author id 2:Author id 3:Author id 1:Author id 2:Author id 3:Author id 1:Author id 2:Author id 3:.........Concatenating threeembedding vectors ofexpertise per developerAuthor id 1:Author id 2:Author id 3:...PCAClassifier: Classifydevelopers in jobrolesDimensionReductionembedding vectors derived from each of the models to classiﬁers to learn the
developer role.

We use three diﬀerent and well-known classiﬁers: SVM, Random Forest,
and Logistic Regression. We compare the performance of these models by
measuring the Precision, Recall and F1-score.

4.2. Comparable Method

Numerous studies used the bag-of-words technique to represent the exper-
tise of developers. We compare our proposed methods to the sate-of-the-arts
techniques that focuse more speciﬁcally on domain expertise of developers
across diﬀerent projects on GitHub. We replicate a recent study [14] that
focuses on representing the expertise of developers using the information on
GitHub. They worked on solving the classiﬁcation problem of developers in
their job roles. Also, we use the same list of developers in their dataset who
are categorized into ﬁve job roles to train dev2vecs.

Motandon et al.

[14] used GitHub information of developer contribu-
tions across diﬀerent projects to represent their expertise. They collected
the biography of developers, repositories names, the programming languages
of repositories, repositories topics, and repositories dependencies (libraries)
across diﬀerent projects as a group of information that evident the domain
expertise of developers. They used the bag-of-words technique to build a
vector for each developer based on the frequency of each feature in their con-
tributions. Because of the high dimension of the collected data, they used
feature selection with a correlation technique over features from each cate-
gory (repository names, topics, languages, and dependencies). They omitted
features with a high degree of correlation in each category. They end up
with 1471 features out of almost 18K. We also applied tf-idf to weight the
frequency of these features and call it “SOA:bow”.

4.3. dev2vec Setup

Doc2vec has several hyper-parameters. We empirically optimize impor-
tant parameters and keep the default values for the rest. The ﬁrst parameter
is the vector size or embedding size. The words and documents embedding
vector share the same size. Another parameter is window size which deﬁnes
the number of words considered around a target word to learn its embedding.
Another important parameter is the minimum frequency for a word to be
considered (min count). Another parameter is negative sampling size. The
window size around a word considers positive sampling words for the target

13

word in doc2vec. The words out of this window are considered as negative
labels or uncorrelated words to the target words. Since the size of vocabu-
laries is enormous, the negative sampling technique randomly downsampled
these words into negative sampling size to speed up the convergence. Finally,
we should ﬁnd the number of epochs to repeat the training and the inference
stages. For dev2vec:APIs model, we use the pre-trained model in [18]. This
model uses DBOW to do not attempt to the order of APIs because the order
of APIs in a source ﬁle is not important.

For cross-validation, we divide our developers into diﬀerent sets: 80% of
the trainset, 10% of the validation set, and 10% of the testset. Developers in
testset are not in trainset or validation set. To cross-validate the result, for
dev2vec:Repos and dev2vec:Issues, we search for the best parameter through
cross validation over the validation set. After learning the vector represen-
tations for the developers in trainset, we feed them to a classiﬁer to learn a
predictor of the developers job roles. For the inference stage, we infer vectors
for developers in testset through these two dev2vec models and use them as
separate testsets of the classiﬁers.

For dev2vec:API, since there is no inference stage, we build the embedding
vectors of the developers by averaging the embedding vector of APIs in the
list of their API expertise. We use the embedding vectors of developers in
trainset to train the classiﬁer and the embedding vector of developers in
testset to test it. We apply the same process for the comparable method,
“SOA:bow”.

Table 1: Experimental Setup for dev2vec models

Model

vector size window size min count algorithm negative sampling

epochs

dec2ve:Repos

dev2vec:Issues

dev2vec:APIs

230

150

200

5

5

30

5

5

5

DM

DM

5(default)

5(default)

15

20

DBOW

20

10(defualt)

4.4. Experimental Result

In this section, we address our research questions by comparing dev2vec
methods with a recent state-of-the-art method, investigating three diﬀerent
dev2vec models learned from diﬀerent sources of information, and the com-
bination of embedding vectors learned from three diﬀerent spaces.

14

4.4.1. RQ1: How eﬀective are embedding vectors to represent the domain
expertise of developers across various software projects compared to
the state-of-the-art methods?

To answer RQ1, we consider the method described in Section 4.2, as
a state-of-the-art method, “SOA:bow” for comparison purpose. Since the
distribution of developers in diﬀerent job roles is imbalanced and 41% of
developers are “frontend”, the baseline is a majority-rule classiﬁer that pre-
dicts all developers as “frontend”. For SOA:bow and all dev2vecs, we apply
Random Forest to classify developers in diﬀerent job roles.

Since the classes in our dataset are imbalance, we apply Macro-Weighted
Precision, Recall and F1-score to report the performance. With Macro-
Weighted metrics, the proportion of each class is weighted by its relative
number of examples available in dataset.

Table 2: The performance of a classiﬁer across ﬁve job roles based on three dev2vec
methods, dev2vec:Repos, dev2vec:Issues and dev2vec:APIs, compare to a sate-of-the-art,
SOA:bow, and a baseline. Since the classes are imbalance, Macro-Weighted Precision,
Recall and F1-score are used to report the performance.

Methods

Macro-Weighted Precision% Macro-Weighted Recall% Macro-Weighted F1-score %

Baseline

SOA:bow

dev2vec:Repos

dev2vec:Issues

dev2vec:APIs

18.73

41.73

60.78

64.75

56.90

43.28

43.93

61.90

65.04

55.10

26.15

42.35

60.02

63.08

55.51

We can ﬁnd in Table 2 that all dev2vec methods and the state-of-the-
art, SOA:bow, show a better performance than the baseline. The Macro-
Weighted Precision to classify developers in their job roles with SOA:bow
is 41.73%, while it is equal to 18.73% for the baseline. This precision is
equal to 60.78%, 64.75% and 56.90% for dev2vec:Repos, dev2vec:Issues and
dev2vec:APIs, respectively.

dev2vec:Issues shows 64.75% Macro-Weighted Precision for classifying
developers in their job roles. It improves the precision of the classiﬁcation
task up to 23.02% compared to the SOA:bow. The Macro-Weighted F1-score
for dev2vec:Repos, dev2vec:Issues and dev2vec:APIs equals 60.02%, 63.08%,
55.51% respectively compare to the SOA:bow with 42.35% F1-score.

15

Answer to RQ1: Representing the domain expertise of developers
across diﬀerent software projects in embedding vectors shows better
performance than both a baseline and a recent state-of-the-art in clas-
sifying developers in their technical job roles. The proposed dev2vec
methods improve F1-Score at least 13.16% with dev2vec:APIs and at
most 20.73% with dev2vec:Issues.

4.4.2. RQ2: How sensitive is the performance of dev2vec to the source of

information (repositories, issues and API)?

To answer RQ2, we compare three diﬀerent dev2vec models, dev2vec:Repos,
dev2vec:Issues, and dev2vec:APIs, that are trained on three diﬀerent sources
of activities to investigate their impact on representing the expertise of de-
velopers in embedding vectors.

Table 3 shows the result of all three dev2vec methods for classifying de-
velopers with three types of classiﬁers. We apply SVM, Random Forest and
Logistic Regression. The metrics are measured in each job role to study the
diﬀerence in performance between diﬀerent classes separately. The table also
presents the results of SOA:bow on each job role.

Among the three dev2vec models, dev2vec:Issue shows better perfor-
mance than dev2vec:Repos and dev2vc:APIs in classifying developers in dif-
ferent job roles. For example, the precision of dev2vec:Issue in classifying
developers with Data Scientist role is 80% and the recall for Frontend devel-
opers is 90.48%. It also improves the F1-score for DevOps developers up to
26.82% with Logistic Regression classiﬁer. The dev2vec:repos shows better
performance than dev2vc:APIs. For example, dev2vec:APIs has 46.43% and
36.11% precision and recall respectively for Mobile developers with SVM.
However, it improves to 53.82% and 46.42% with dev2vec:Repos.

In addition to the above comparison, all three dev2vec models improve
the performance of the classiﬁcation task in all job roles compared to the
state-of-the-art, SOA:bow. For example, in SOA:bow and SVM classiﬁer, re-
call for Backend and DevOps developers are 20% and 23.08%, respectively.
It increases into 31.25% and 44.45% with dev2vec models. As another ex-
ample, the precision for Data Scientist class in bag-of-words with Random
Forest classiﬁer is 46.15% and it increases into 78.57%, 80% and 64.71% with
dev2vec:Repos, dev2vec:Issues, and dev2vec:APIs, respectively.

Some job roles show better performance. For example, “Data Scientist”
developers are more accurately classiﬁed compared to “Backend” developers.

16

This is due to the intersection between the activities of developers in diﬀerent
roles. We discuss it more in Section 5.

Table 3: The result of three diﬀerent classiﬁers on embedding vector representation of
developers expertise in three diﬀerent spaces. The “Pre”, “Rec” and “F1” are referring to
Precision, Recall and F1-score, respectively.

SVM

SOA:bow

Dev2vec:Repos

Dev2vec: Issues

Dev2vec: APIs

Job Role

Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1%

Frontend
Backend
Mobile
DataScientist
DevOps

53.52
33.33
32.26
60.00
37.50

73.08
20.00
30.30
42.86
23.08

61.79
25.00
31.25
50.00
28.57

63.28
37.50
53.82
76.04
68.89

85.40
28.64
46.42
61.76
37.17

72.69
31.79
48.17
67.82
48.08

66.67
55.56
59.09
80.00
80.00

90.48
31.25
52.00
72.73
44.45

76.77
40.00
55.32
76.19
57.14

61.11
31.25
46.43
66.67
37.50

78.57
27.78
36.11
66.67
30.00

68.75
29.41
40.63
66.67
33.33

Random Forest

SOA:bow

Dev2vec:Repos

Dev2vec: Issues

Dev2vec: APIs

Job Role

Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1%

Frontend
Backend
Mobile
DataScientist
DevOps

55.56
31.25
38.46
46.15
20.00

68.18
20.00
40.54
46.15
15.38

61.22
24.39
39.47
46.15
17.39

64.29
50.00
53.13
78.57
50.00

84.91
27.27
45.95
64.71
25.00

73.17
35.29
49.28
70.97
33.33

67.27
55.56
56.00
80.00
75.00

88.10
31.25
56.00
72.73
33.33

76.29
40.00
56.00
76.19
46.15

56.00
33.33
50.00
64.71
30.00

73.68
31.25
41.03
64.71
21.43

63.64
32.26
45.07
64.71
25.00

Logistic Regression

SOA:bow

Dev2vec:Repos

Dev2vec: Issues

Dev2vec: APIs

Job Role

Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1% Prec% Rec% F1%

Frontend
Backend
Mobile
DataScientist
DevOps

46.43
23.53
37.50
40.00
25.00

47.27
19.05
40.00
46.15
23.08

46.85
21.05
38.71
42.86
24.00

74.47
25.00
50.00
62.50
26.67

68.63
30.77
43.24
58.82
50.00

71.43
27.59
46.38
60.61
34.78

76.92
50.00
53.13
66.67
50.00

71.43
31.25
68.00
72.73
55.56

74.07
38.46
59.65
69.57
52.63

60.53
20.00
42.86
50.00
25.00

50.00
28.57
42.86
52.63
26.67

54.76
23.53
42.86
51.28
25.81

Answer to RQ2: The performance of dev2vec model is sensitive to
embedding vectors learned from diﬀerent sources of developers’ activ-
ities. The dev2vec:Issues shows a better performance in representing
the expertise of developers in embedding vectors. The dev2vec:Repos
and dev2vec:APIs are in second and third place, respectively.

4.4.3. RQ3: How eﬀective is the concatenation of expertise embedding vectors

from the diﬀerent information sources?

We concatenate three embedding vectors from three diﬀerent spaces of
their activities, as explained in Section 3.5. The size of the ﬁnal embedding
vector that represents the expertise of developers is 580.

We feed these vectors into a Random Forest classiﬁer to predict devel-
oper job roles. We also apply PCA on these embedding vectors to reduce

17

their dimensionality from 580 to 50, 100, 200, 250, and 300, and each time,
train a new classiﬁer for predicting the developers job roles. Table 4 shows
the results for embedding vectors with and without dimension reduction.
Based on this table, concatenating embedding vectors from diﬀerent spaces
improves the performance of the classiﬁer only in two job roles, “Frontend”
and “Data Scientist”. But for the rest of job roles the performance is as good
as dev2vec:Issues. Applying PCA to reduce dimension has an adverse eﬀect
on the performance of the classiﬁer. Increasing the number of dimensions im-
proves the performance. However, even with “PCA-300”, the performance
of classiﬁer is not as good as dev2vec:Issues.

Answer to RQ3: Concatenating embedding vectors from diﬀerent
spaces to represent the expertise of developers, without dimension
reduction,
improves the performance of the classiﬁer for two job
roles, and for the rest of the roles, the performance is as good as
dev2vec:Issues.

5. Threat to Validity

In this section, we explain the scopes and assumptions in our study that

can threaten the validity of our results.

5.1. Data Assumption

For dev2vec:APIs, we use a pre-trained doc2vec model on API list col-
lected from source ﬁles that were modiﬁed by developers with diﬀerent com-
mits. Thus, we use the same approach as [18] to collect the API calls from
developers’ commits. We assume that if developers submit a commit on a ﬁle,
they have a basic knowledge about the libraries used in that source ﬁle [18].
With this assumption, we are not collecting the actual practice of develop-
ers. Because the developer may not change the list of APIs in a source ﬁle
by submitting a commit. Therefore, the performance of embedding vectors
derived from dev2vec:APIs to represent the expertise of developers can be
aﬀected by this assumption.

Further along, in dev2vec:repos, sometimes, developers with diﬀerent do-
mains of expertise contribute to the same repositories. Thus, we may collect
the same content (the textual information of repositories) for developers with
diﬀerent domains who contributed to the same project. This assumption can
impact the performance of embedding vectors derived from dev2vec:repos.

18

Table 4: The performance of dev2vec:RIAs with diﬀerent dimensionality reduction levels
in classifying developers in their job roles

Reduction Roles

Prec% Rec% F1%

None

PCA-50

PCA-100

PCA-200

PCA-250

PCA-300

Frontend
Backend
Mobile
DataScientist
DevOps

Frontend
Backend
Mobile
DataScientist
DevOps

Frontend
Backend
Mobile
DataScientist
DevOps

Frontend
Backend
Mobile
DataScientist
DevOps

Frontend
Backend
Mobile
DataScientist
DevOps

Frontend
Backend
Mobile
DataScientist
DevOps

69.09
55.56
56.00
81.82
75.00

55.36
30.77
40.00
58.33
40.00

56.14
30.77
42.11
58.33
40.00

60.71
38.46
47.37
58.33
50.00

61.82
38.46
47.37
61.54
50.00

62.50
41.67
50.00
61.54
60.00

88.37
31.25
56.00
81.82
33.33

73.81
23.53
32.00
58.33
20.00

76.19
23.53
32.00
58.33
20.00

80.95
29.41
36.00
58.33
30.00

80.95
29.41
36.00
66.67
30.00

83.33
29.41
40.00
66.67
30.00

77.55
40.00
56.00
81.82
46.15

63.27
26.67
35.56
58.33
26.67

64.65
26.67
36.36
58.33
26.67

69.39
33.33
40.91
58.33
37.50

70.10
33.33
40.91
64.00
37.50

71.43
34.48
44.44
64.00
40.00

19

5.2. Averaging API embedding vectors

In dev2vec:APIs, we cannot directly use the pre-trained model in [18]
to predict the embedding vectors of expertise for new developers. Thus,
we average the embedding vectors of APIs in the list of APIs related to a
developer’s activities and represent the expertise of the developer.

One naive solution could be inferring a vector per each commit of a new
developer. Then, merge all embedding vectors of commits to build a unique
vector that represents the expertise of the developer. But, we know that the
inference stage in doc2vec can predict diﬀerent vectors for the same docu-
ment in diﬀerent attempts [16]. However, the cosine similarity between these
vectors is high, but they are not the same. We explain with an example how
this fact can impact the performance of predicting vectors for a developer.
Suppose that two developers submitted two diﬀerent commits on the same
source ﬁle. None of these commits changed the list of APIs in this source
ﬁle. However, in the inference stage, the doc2vec model predicts two diﬀer-
ent embedding vectors for these two commits with the same content. This
small diﬀerence between embedding vectors (that are obtained from the same
content) can add noise into the ﬁnal vectors of expertise.

There are advantages to our proposed method, dev2vec:APIs. First, we
keep the repetition of APIs in developers’ activities by merging the API calls
in the source ﬁles of all commits. Second, we represent the same API with
the same vector in diﬀerent commits and for diﬀerent developers because
we fetch the learned embedding vectors for each API from the pre-trained
model. We can consider this averaging as a weighted average. It means if a
developer uses two APIs more than the rest of the APIs in her activities, the
ﬁnal embedding vector would be more similar to the average of embedding
vectors of these two APIs. However, this averaging can decrease the impacts
of other APIs (rare ones) in deﬁning the expertise of developers. In future
studies, we can use other mechanisms similar to the attention mechanism to
learn weights for aggregating the API vectors that can yield a more accurate
vector to represent developer expertise.

5.3. Type and source of activities

In this study, we focus on three well-known types of developers’ activities
on GitHub. We perform that the performance of representing the domain
expertise of developers in embedding vectors is sensitive to the source of
activities.
In addition to activities that we considered in this work, there
are other activities on GitHub that reﬂect developers’ expertise, such as

20

commit messages submitted by developers, the list of APIs that developers
practiced in a commit, or the structure of codes written by them. Also,
GitHub is not the only source of developers’ activities. Other platforms such
as StackOverﬂow contain valuable information about the domain expertise
of developers as well. Other activities on GitHub and other platforms such
as StackOverﬂow can be investigated in future studies to learn embedding
vectors of expertise.

5.4. Job roles of developers

The pre-labeled dataset that we used in this study categories a group
of developers in ﬁve job roles. However, the job roles of developers are not
limited to these ﬁve roles, but the investigation of authors in [14] showed
that 64% of job posts on StackOverﬂow are related to one of these ﬁve roles.

6. Discussion

In this section ﬁrst we discuss why we choose the combination of doc2vec
to represent the expertise of developers and machine learning algorithms for
classiﬁcation task. Then, we discuss why the performance of dev2vec models
varies across diﬀerent job roles.

6.1. Combination of doc2vec and machine learning classiﬁers

This study sheds light on representing the domain expertise of develop-
ers across diﬀerent projects with embedding vectors. We apply doc2vec to
developers and learn the embedding vector representation of their expertise.
Then, we use this model to predict embedding vectors for those developers.
In the last step, we investigate the performance of these vectors in represent-
ing developers’ expertise with the Random Forest classiﬁer. We exclude an
end-to-end neural network for the classiﬁcation task because the size of the
dataset is not suitable to train or train a transformer for representing the
expertise of developers.

6.2. The variety of the performance of the models in diﬀerent job roles

As Table 3 results show, the performance of classiﬁers in job roles such as
Frontend and Data Scientist is better than Backend or Mobile for SOA:bow
and all dev2vec models. In practice, we know that there are intersections
between the skills of developers. For example, there are conﬂuences between
the skills and projects of Frontend developers and Backend developers or

21

Mobile developers. However, this conﬂuence is smaller when it comes to
Data Scientists compared to Frontend or Mobile developers.

To illustrate the correlation between diﬀerent job roles, we investigate the
similarity between words across topics with the Cosine Inter Intra Topics
method [22]. We apply this method to the state-of-the-art SOA:bow dataset,
which we adopted as our ground truth. As discussed in Section 4.2, their
dataset includes words collected from the biography of developers and repos-
itory names, languages, topics, and dependencies (libraries) across diﬀerent
projects on GitHub after removing highly correlated words.

In Figure 5, rows represent the job roles, and columns represent the cen-
troid of each role. We ﬁnd that the diagonal values for Frontend and Data
Scientist roles are greater than oﬀ-diagonal values. This implies that the
similarity for developers within these two job roles is greater than the simi-
larity between them and developers across other roles. However, for Backend
and Mobile developers, the diagonal and oﬀ-diagonal values are very close.
Even this similarity between Mobile and Frontend developers is more signiﬁ-
cant than the similarity among Mobile developers. It could explain the lower
performance of classiﬁers in Backend and Mobile roles.

6.3. Implication

Assessing the expertise of developers has diﬀerent aspects. This assess-
ment includes their soft skills, such as teamwork or communication, and tech-
nical skills, such as programming languages, algorithm design, and libraries.
In this study, we focus on representing the domain expertise of developers
across diﬀerent projects. This study is initial ﬁltering for recruiters or project
managers. In addition to this initial categorization, more investigations, such
as interviews, are required to choose a proper candidate.

Moreover, we evaluate the performance of embedding vectors of exper-
tise in classifying developers in diﬀerent job roles, but their beneﬁts are not
limited to this task. The embedding vectors of developers’ expertise can be
applied in various manners in software engineering. For example, we can use
these embedding vectors of expertise to ﬁnd a proper candidate for a job
post or a new project contributor. We also can use these embedding vectors
to match developers with similar expertise (working in the same domain).
These signiﬁcances can be addressed in future works.

For the sake of replication, all trained models and scripts are made avail-

22

Figure 5: Cosine Inter Intra Topics (job roles). Rows are the roles and columns are
the role’s centroid. In “Frontend” or “Data Scientist” roles, the similarity between devel-
opers within the role is more signiﬁcant than developers across diﬀerent roles. However,
the inter similarity for job roles such as “Mobile” or “Backend” is very close to their intra
similarity with others. It impacts the performance of the classiﬁer for these roles.

23

able2.

7. Related Works

In this section, we review the previous works. First, we discuss the studies
focusing on developers expertise. Second, we describe the eﬃcacy of embed-
ding methods in software engineering literature.

Developers Expertise: The majority of studies to assess the expertise of
developers consider the number of changes or commits in a ﬁle path. A widely
used heuristic is the Line 10 Rule [9, 10], inspired by version control systems
that store the name of the author in line 10 of the commit log. All methods
motivated by Line 10 Rule heuristic state that if a developer changed a ﬁle in
the past, she/he should be one of the candidates to solve the tasks related to
this ﬁle [23, 20, 24, 25]. These methods cannot be expanded across diﬀerent
projects because they are limited to the path of a source ﬁle.

Other studies go further and show information such as commit mes-
sages [5], bug resolving history [26], the description of repositories and con-
tent of ReadMe ﬁles on GitHub [13, 19] or the API calls in source ﬁles [27],
touched by a developer, are good source of information to infer the exper-
tise of developers. Although, these methods have a good performance to
deﬁne the expertise of developers within a software project. Collecting these
types of information across diﬀerent software projects increases the number
of features and in turn, increases the sparsity that aﬀect the performance of
these methods. Montandon et al. in [14] represent the domain expertise of
developers by collecting their activities across diﬀerent projects. They apply
bag-of-word technique on data collected from the biography of developers on
Github, repository descriptions, programming languages and project depen-
dencies to represent the expertise of developers. To reduce the sparsity, they
reduce the dimension of collected data by calculating the correlation between
diﬀerent features and select high correlated ones.

A recent study represents the expertise of developers by collecting pro-
gramming syntax patterns of python code written by them across diﬀerent
software projects [28]. This model shows good performance in categorizing
developers in two levels of expert and novice. They focus on the levels of
expertise of developers in python programming language.

2https://github.com/ExpertiseModel/EmbeddingVectors

24

Vector Embedding in Software Domain: Recently in software engineering,
vector embedding methods have been widely used for diﬀerent purposes.
Zhang et al. [29] use average word embedding and document embedding for
issue knowledge acquisition. They derive embedding vectors for each issue
from the content in the title and body of them. Then, they link an issue to
potentially related issues by calculating the similarity between issue vectors.
Cod2vec [30] projects code into embedding vectors by training a model
on diﬀerent paths collected from the AST of methods and then attempt to
predict methods name for code snippets in the testset. Another study applies
code2vec pre-trained model on diﬀerent commits to represent them in em-
bedding vectors and then classify commits vectors into security relevant/non-
relevant ones [31]. Recently, a new study represented API calls in source ﬁle
of three programming languages (java, java script and python) in an embed-
ding vector. To investigate the performance of these vectors in representing
API calls, they consider all APIs in a source ﬁle as related APIs. Then, they
ﬁnd that the vectors of these APIs are more similar than the vector of APIs
that never occur together in a source ﬁle [32]. Dey et al.
[18] in a recent
study collect API calls in commits of developers in diﬀerent projects in 17
programming language. They obtain embedding vectors for 3 diﬀerent enti-
ties: APIs, projects and developers. They call this information ”API-related
Skill Space“ of projects and developers. Then, they ﬁnd that developers are
more likely to use new APIs or join new projects that have similar represen-
tations to themselves in the API-related Skill Space. However, they didn’t
evaluate their model in predicting the embedding vector for a new developer
which is not seen during training phase.

In this study, we identify the domain expertise of developers by represent-
ing their expertise in embedding vectors. We investigate the performance of
using diﬀerent source of information such as repositories description, issue
resolving history of developers and libraries used by developers to obtain
these embedding vectors of expertise.

8. Conclusion

The diversity in the technical specialization of developers in software en-
gineering is expanding. Automatic assessment of developers’ specialization
is consequential for the success of software projects. Appropriate recognition
of this specialization requires an accurate and comprehensive representation
of the domain expertise of developers. In this study, we investigate the ex-

25

pertise of developers across their contributions to various software projects
and multiple programming languages. Previous methods that show promis-
ing outcomes in assessing developers’ expertise within a software project are
not practical across various projects due to the data size. To address this
challenge, we proposed the dev2vec approach pivoted on doc2vec and rep-
resented the expertise of developers in embedding space. We trained three
models on three diﬀerent sources of information on GitHub that capture the
expertise of developers. We refer to them as dev2vec:Repos, dev2vec:Issues
and dev2vec:APIs. Furthermore, we merge the output of these three meth-
ods by concatenating the embedding vectors from three spaces and call it
dev2vec:RIAs. To study the eﬀectiveness of our proposed methods in rep-
resenting the domain expertise of developers, we employed these embedding
vectors in a vital problem in the software engineering domain: classifying
developers in their job roles. Our result on this classiﬁcation task shows
improvements on F1-score at least 13.16% with dev2vec:APIs and at most
20.73% with dev2vec:Issues compare to the state-of-the-art methods. In addi-
tion, we observed that the performance of embedding vectors in representing
the domain expertise of developers is sensitive to the source of information,
and, embedding vectors of expertise that are derived from issue resolving
history of developers has a better performance in reﬂecting their expertise.
Furthermore, the performance of concatenation of embedding vectors from
diﬀerent spaces, dev2vec:RIA, is as good as dev2vec:Issues.

In future works, we aim to apply mechanisms similar to the attention
mechanism and learn a weight vector for concatenating the embedding vec-
tors of expertise. We also want to extract more precisely the content of
changes made by the developers to derive these embedding vectors of exper-
tise such as the list of APIs that they practiced or the structure of codes
Information available on other platforms is also consid-
written by them.
ered. Finally, we plan to study the performance of embedding vectors of
expertise for other problems in software engineering besides the classiﬁcation
of developers in their job roles.

References

[1] B. Curtis, H. Krasner, N. Iscoe, A ﬁeld study of the software design
process for large systems, Communications of the ACM 31 (11) (1988)
1268–1287.

26

[2] T. DeMarco, T. Lister, Peopleware: productive projects and teams,

Addison-Wesley, 2013.

[3] J. T. Liang, T. Zimmermann, D. Ford, Towards mining oss skills from

github activity, arXiv preprint arXiv:2203.02027 (2022).

[4] C. Zhou, S. K. Kuttal, I. Ahmed, What makes a good developer? an em-
pirical study of developers’ technical and social competencies, in: 2018
IEEE Symposium on Visual Languages and Human-Centric Computing
(VL/HCC), IEEE, 2018, pp. 319–321.

[5] D. Matter, A. Kuhn, O. Nierstrasz, Assigning bug reports using a
vocabulary-based expertise model of developers, in: 2009 6th IEEE in-
ternational working conference on mining software repositories, IEEE,
2009, pp. 131–140.

[6] J. E. Montandon, L. L. Silva, M. T. Valente, Identifying experts in soft-
ware libraries and frameworks among github users, in: 2019 IEEE/ACM
16th International Conference on Mining Software Repositories (MSR),
IEEE, 2019, pp. 276–287.

[7] J. Marlow, L. Dabbish, Activity traces and signals in software devel-
oper recruitment and hiring, in: Proceedings of the 2013 conference on
Computer supported cooperative work, 2013, pp. 145–156.

[8] L. Singer, F. Figueira Filho, B. Cleary, C. Treude, M.-A. Storey,
K. Schneider, Mutual assessment in the social programmer ecosystem:
An empirical investigation of developer proﬁle aggregators, in: Proceed-
ings of the 2013 conference on Computer supported cooperative work,
2013, pp. 103–116.

[9] D. W. McDonald, M. S. Ackerman, Expertise recommender: a ﬂexible
recommendation system and architecture, in: Proceedings of the 2000
ACM conference on Computer supported cooperative work, 2000, pp.
231–240.

[10] A. Mockus, J. D. Herbsleb, Expertise browser: a quantitative approach
to identifying expertise, in: Proceedings of the 24th international con-
ference on software engineering. icse 2002, IEEE, 2002, pp. 503–512.

27

[11] J. Oliveira, M. Viggiato, E. Figueiredo, How well do you know this
library? mining experts from source code analysis, in: Proceedings of
the XVIII Brazilian Symposium on Software Quality, 2019, pp. 49–58.

[12] D. Schuler, T. Zimmermann, Mining usage expertise from version
archives, in: Proceedings of the 2008 international working conference
on Mining software repositories, 2008, pp. 121–124.

[13] G. J. Greene, B. Fischer, Cvexplorer: Identifying candidate developers
by mining and exploring their open source contributions, in: Proceed-
ings of the 31st IEEE/ACM International Conference on Automated
Software Engineering, 2016, pp. 804–809.

[14] J. E. Montandon, M. T. Valente, L. L. Silva, Mining the technical roles of
github users, Information and Software Technology 131 (2021) 106485.

[15] K. W. Church, Word2vec, Natural Language Engineering 23 (1) (2017)

155–162.

[16] Q. Le, T. Mikolov, Distributed representations of sentences and docu-
ments, in: International conference on machine learning, PMLR, 2014,
pp. 1188–1196.

[17] L. Ge, T.-S. Moh, Improving text classiﬁcation with word embedding,
in: 2017 IEEE International Conference on Big Data (Big Data), IEEE,
2017, pp. 1796–1805.

[18] T. Dey, A. Karnauch, A. Mockus, Representation of developer exper-
tise in open source software, in: 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), IEEE, 2021, pp. 995–1007.

[19] Y. Wan, L. Chen, G. Xu, Z. Zhao, J. Tang, J. Wu, Scsminer: mining
social coding sites for software developer recommendation with relevance
propagation, World Wide Web 21 (6) (2018) 1523–1543.

[20] Y. Tian, D. Wijedasa, D. Lo, C. Le Goues, Learning to rank for bug
report assignee recommendation, in: 2016 IEEE 24th International Con-
ference on Program Comprehension (ICPC), IEEE, 2016, pp. 1–10.

28

[21] R. Venkataramani, A. Gupta, A. Asadullah, B. Muddu, V. Bhat, Discov-
ery of technical expertise from open source code repositories, in: Pro-
ceedings of the 22nd International Conference on World Wide Web,
2013, pp. 97–98.

[22] A. Neishabouri, M. C. Desmarais, Estimating the number of latent top-
ics through a combination of methods, Procedia Computer Science 192
(2021) 1190–1197.

[23] S. Minto, G. C. Murphy, Recommending emergent teams, in: Fourth In-
ternational Workshop on Mining Software Repositories (MSR’07: ICSE
Workshops 2007), IEEE, 2007, pp. 5–5.

[24] D. Kim, Y. Tao, S. Kim, A. Zeller, Where should we ﬁx this bug? a
two-phase recommendation model, IEEE transactions on software En-
gineering 39 (11) (2013) 1597–1610.

[25] J. Anvik, G. C. Murphy, Determining implementation expertise from
bug reports, in: Fourth International Workshop on Mining Software
Repositories (MSR’07: ICSE Workshops 2007), IEEE, 2007, pp. 2–2.

[26] J. Anvik, G. C. Murphy, Determining implementation expertise from
bug reports, in: Fourth International Workshop on Mining Software
Repositories (MSR’07: ICSE Workshops 2007), IEEE, 2007, pp. 2–2.

[27] R. Sindhgatta, Identifying domain expertise of developers from source
code, in: Proceedings of the 14th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, 2008, pp. 981–989.

[28] A. Moradi Dakhel, M. C. Desmarais, F. Khomh, Assessing developer
expertise from the statistical distribution of programming syntax pat-
terns, in: Evaluation and Assessment in Software Engineering, 2021, pp.
90–99.

[29] Y. Zhang, Y. Wu, T. Wang, H. Wang, ilinker: a novel approach for
issue knowledge acquisition in github projects, World Wide Web 23 (3)
(2020) 1589–1619.

[30] U. Alon, M. Zilberstein, O. Levy, E. Yahav, code2vec: Learning dis-
tributed representations of code, Proceedings of the ACM on Program-
ming Languages 3 (POPL) (2019) 1–29.

29

[31] R. C. Lozoya, A. Baumann, A. Sabetta, M. Bezzi, Commit2vec: Learn-
ing distributed representations of code changes, SN Computer Science
2 (3) (2021) 1–16.

[32] B. Theeten, F. Vandeputte, T. Van Cutsem, Import2vec: Learning em-
beddings for software libraries, in: 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR), IEEE, 2019, pp.
18–28.

30

