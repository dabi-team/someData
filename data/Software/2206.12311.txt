Noname manuscript No.
(will be inserted by the editor)

Bugs in Machine Learning-based Systems: A
Faultload Benchmark

Mohammad Mehdi Morovati · Amin
Nikanjam · Foutse Khomh · Zhen Ming
(Jack) Jiang

2
2
0
2

n
u
J

4
2

]
E
S
.
s
c
[

1
v
1
1
3
2
1
.
6
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The rapid escalation of applying Machine Learning (ML) in vari-
ous domains has led to paying more attention to the quality of ML compo-
nents. There is then a growth of techniques and tools aiming at improving the
quality of ML components and integrating them into the ML-based system
safely. Although most of these tools use bugs’ lifecycle, there is no standard
benchmark of bugs to assess their performance, compare them and discuss
their advantages and weaknesses. In this study, we ﬁrstly investigate the re-
producibility and veriﬁability of the bugs in ML-based systems and show the
most important factors in each one. Then, we explore the challenges of gener-
ating a benchmark of bugs in ML-based software systems and provide a bug
benchmark namely defect4ML that satisﬁes all criteria of standard bench-
mark, i.e. relevance, reproducibility, fairness, veriﬁability, and usability. This
faultload benchmark contains 113 bugs reported by ML developers in GitHub
and Stack Overﬂow, using two of the most popular ML frameworks: Tensor-
Flow and Keras. defect4ML also addresses important challenges in Software
Reliability Engineering of ML-based software systems, like: 1) fast changes in
frameworks, by providing various bugs for diﬀerent versions of frameworks, 2)
code portability, by delivering similar bugs in diﬀerent ML frameworks, 3) bug
reproducibility, by providing fully reproducible bugs with complete informa-
tion about required dependencies and data, and 4) lack of detailed information
on bugs, by presenting links to the bugs’ origins. defect4ML can be of interest
to ML-based systems practitioners and researchers to assess their testing tools
and techniques.

Mohammad Mehdi Morovati · Amin Nikanjam · Foutse Khomh
SWAT Lab., Polytechnique Montr´eal, Montr´eal, Canada
E-mail: {mehdi.morovati,amin.nikanjam,foutse.khomh}@polymtl.ca
Zhen Ming (Jack) Jiang
York University, Toronto, Canada
E-mail: zmjiang@cse.yorku.ca

 
 
 
 
 
 
2

Mohammad Mehdi Morovati et al.

Keywords Benchmark, Machine Learning-based system, Software Bug,
Software Reliability Engineering, Software Testing

1 Introduction

Recent outstanding successes in applying Machine Learning (ML) and espe-
cially Deep Learning (DL) in various domains have encouraged more people
to use them in their systems. ML-based systems refer to software systems that
contain at least one ML component (software component whose functionality
relies on ML). Given the increasing deployment of ML-based systems in safety-
critical areas such as autonomous vehicles [56, 46] and healthcare systems [11],
we need to provide an acceptable level of reliability in such systems.

Software reliability is broadly considered to be the most important software
quality factor among all software quality attributes [45], where such attributes
measure the conformance level of the system, component, or process to the
identiﬁed functional and non-functional requirements [57]. Software Reliability
Engineering (SRE) is the methodology to ensure failure-free operations of the
software in a speciﬁed period of time. Substantial portion of SRE techniques
has been developed based on studying the lifecycle of bugs [58,45].

It is generally accepted that standardized benchmarks are the most eﬃ-
cient tools for evaluating and comparing products and methodologies [38]. A
benchmark must satisfy some quality criteria to be considered as standard, in-
cluding relevance, reproducibility, fairness, veriﬁability, and usability [38,77].
It is also worth mentioning that benchmark construction is a long-term and
iterative process that requires the cooperation of the community [44]. Accord-
ingly, a standard benchmark of bugs is an essential requirement to evaluate,
compare, and improve such research on the SRE approaches focusing on the
bug’s lifecycle.

Benchmark of software bugs that contains a set of real bugs is known as
faultload benchmark [77]. Several studies on the faultload benchmark for tra-
ditional software systems have been done, e.g., Defects4J [35] (a benchmark
of bugs in Java open source projects hosted on GitHub), BugBench [44] (a
benchmark of C/C++ programs’ bugs), ManyBugs [40] (a benchmark of de-
fects in C programming language), and Bears [47] (a Java bug benchmark for
automatic program repair). On the other hand, some faultload benchmarks
such as JaConTeBe [43] (a benchmark of java concurrency bugs) are designed
for speciﬁc types of bugs. Accordingly, defect4MLalso ignores general bugs and
considers bugs that are related to the ML components.

Similar to other faultload benchmarks, benchmark of ML-based systems’
bugs is the basic necessity for comparing, tuning, and improving testing tech-
niques/tools of ML-based systems. Extracting, reproducing, and isolating real
bugs in traditional software still need considerable time and eﬀort. Concerning
the higher complexity level of the ML-based systems in comparison with tradi-
tional ones [6] and challenges in the engineering of ML-based systems [13], pro-
viding reproducible bugs in these systems might require more eﬀort. Although

Bugs in Machine Learning-based Systems: A Faultload Benchmark

3

preceding studies have developed some benchmarks of bugs in ML-based sys-
tems [37,79], they totally disregarded the standard benchmark criteria. As an
example, Denchmark [37] does not provide enough information to reproduce
and trigger bugs. Meanwhile, several studies on the testing of ML-based sys-
tems have used synthetic bugs for assessment [54, 52] which may bias their
evaluation by hiding potential weaknesses. Some others have also used a lim-
ited number of real bugs [79,62] that may not be representative of a thorough
evaluation, implying an incorrect measure of the proposed approach’s reliabil-
ity. So, in this research we aim to answer the following research questions:
RQ1. What are the key factors in reproducibility of reported bugs in ML-based
systems?
RQ2. What are the important factors in veriﬁability of ML-based systems’
bug-ﬁxes?
RQ3. What are the challenges of generating standard faultload benchmark in
ML-based systems?

To answer these research questions, ﬁrstly, we investigate 5 public datasets
of ML-based systems’ bugs and manually check 513 bugs that they provided
from GitHub. Then, we checked 910 additional bug-ﬁx commits extracted from
ML-based systems repositories. Furthermore, we review 750 Stack Overﬂow
(SO) posts related to TensorFlow and Keras frameworks. We examine the
reproducibility and veriﬁability of the bugs in ML-based systems, as two of
the most demanding criteria of standard benchmark.

We also provide a faultload benchmark of ML-based systems namely de-
fect4ML. Figure 1 illustrates a high-level view of the proposed benchmark.
The base layer is the benchmark containing the database of bugs. The next
layer, Python virtual environment, refers to the environment that should be
conﬁgured to run applications and trigger the bugs. Because each buggy ap-
plication has diﬀerent dependencies and requires diﬀerent sorts of libraries to
be triggered, a Python virtual environment would be the best solution for
running buggy applications in isolation. The top layer represents the poten-
tial usage of bugs which is mostly ML testing tools such as NeuraLint and
DeepLocalize. Given the high cost of providing bugs from ML-based systems
that satisfy standard benchmark criteria, we set 113 bugs (75 from GitHub
and 38 from SO) as our goal for the ﬁrst release of defect4ML. The included
bugs are classiﬁed into diﬀerent categories, based on various criteria includ-
ing Python version, ML framework, violated testing property, and bug type.
Diﬀerent users such as developers, distributors, and researchers of ML-based
systems testing tools/techniques can beneﬁt from our proposed benchmark.
They can use defect4ML to evaluate their proposed approaches for bug de-
tection, or localization and compare them with previous works. Besides, de-
fect4ML has potential to be used for automatic bug repairing tools. To this
end, users should remove modiﬁcations from bug-ﬁx which are not related to
the reported bug (similar to methodology used in [34]). The contributions of
this study can be summarized as follows:

4

Mohammad Mehdi Morovati et al.

Fig. 1 High-level view of the benchmark.

– First standard benchmark of ML-based systems’ bugs: Proposed
benchmark satisﬁes all of the standard benchmark criteria (e.g. relevance,
fairness, etc)

– Large-scale and accurate bug benchmark: We collected the bugs from
GitHub commits and SO posts to provide a large-scale benchmark. Besides,
we applied several steps (including manual checking) to ﬁlter the bugs and
extract the ones satisfying our deﬁned criteria. We have also provided ﬁne-
grained classiﬁcations based on diﬀerent criteria (such as ML framework,
bug type, etc.) to enable users to ﬁlter the bugs and collect a desired
subset. Users can also add new bugs to the benchmark and raise a request
for removing an existing bug to keep the benchmark up-to-date.

– Bug reproducibility: Our analysis revealed that only about 5.3% of all
reviewed GitHub bugs and near to 3.34% of reported bugs in SO posts
are reproducible. However, all bugs in defect4ML are completely repro-
ducible. We have also provided contextual information for each bug in-
cluding the needed version of Python, dependencies (necessary libraries
and corresponding version), data, and the process of triggering bugs, to
allow for reproducibility.

– Bug-ﬁx veriﬁability:Our analysis revealed that only a small portion of
the studied ML-based systems’ bugs (i.e., 13.3% of collected bugs from
GitHub in defect4ML) can be veriﬁed by the provided test cases in their
applications. Moreover, none of the reviewed bugs reported in SO posts
has test cases.

– Detailed information of the bugs: We provide the URL to the origin
of gathered bugs in our proposed benchmark that includes textual infor-

Benchmark DatabaseBugs MetadataBug DatabaseReproducible BugsReproducible BugPython Virtual EnvironmentML Testing ToolsDeepLocalize…NeuraLintBugs in Machine Learning-based Systems: A Faultload Benchmark

5

mation about bugs including: buggy entities such as ﬁle name and line of
code, bug’s root cause, and the ﬁxed version (how the bug got ﬁxed).
– Diversity: We have covered 30 diﬀerent types of bugs based on the taxon-
omy proposed by Humbatova et al. [26] (including 95 types of ML-related
bugs), to promote diversity of defect4ML. Besides, we used GitHub and
SO as the primary sources of collecting bugs. To gather bugs from GitHub,
we have also explored repositories developed by users with diﬀerent levels
of expertise. In addition, we have presented bugs based on the two most
popular ML frameworks, TensorFlow and Keras.

The rest of the paper is organized as follows. We explain the back-
ground of the study in Section 2. The methodology followed to answer the
research questions is explained in Section 3. The results of analyzing collected
bugs and the proposed benchmark is described in Section 4. Section 5 rep-
resents the discussion of our study. Then, the related works are mentioned
in Section 6. We discuss threats to the validity of this research in Section 7.
Finally, we conclude the paper in Section 8.

2 Background

This section introduces concepts of ML-based systems and SRE in these sys-
tems, the general picture of the benchmark, and the criteria it should meet to
be considered a standard benchmark.

2.1 ML-based System

Software systems including at least one ML component are known as ML-
based systems. ML components are deﬁned to be software components working
based on ML algorithms with the aim of proving intelligent behavior [49]. An
ML component may be only a small part of a much larger system. Figure 2
shows a high-level view of the ML-based systems exposing the role of the ML
component in them.

To simplify the design, implementation, and integration of ML components
in software systems, several ML frameworks such as TensorFlow [1], Keras [9],
and PyTorch [55] have been developed. They help developers to create, train,
and then deploy various types of ML models. Hence, ML frameworks play
a vital role in developing ML-based systems [82]. Similar to any other soft-
ware components, ML components are also error-prone. However, current ML
frameworks do not provide any capability to validate and verify the developed
ML components.

2.2 Bugs in ML-based Systems

In general, software bug is known as the inconsistency between the existing
and expected software functionality, also called deﬁciency in satisfying software

6

Mohammad Mehdi Morovati et al.

Fig. 2 An high-level view of the ML-based systems [63]

requirements [84]. Accordingly, ML bug refers to the deﬁciencies in ML com-
ponents, which may lead to discrepancies between existing and the required
behavior of ML component [82]. An ML bug can occur in the ML frame-
work [33,60,75], program code [30,83], or the data. So, researchers study the
bugs in each area separately. In this study, we just investigate the bugs in
program code and do not consider bugs in ML framework and data.

There are three diﬀerent testing levels for ML-based systems: model test-
ing, integration testing, and system testing [59]. At the model testing level,
we consider the ML component in isolation and ignore other software com-
ponents. Integration testing level aims to assess the interaction between ML
and other components. System testing level studies all software components
to evaluate the ML-based system’s conformance to the intended requirements.
Accordingly, we can consider three diﬀerent categories of bugs in ML-based
systems, each one related to one of testing levels.

– Bugs in ML components: At this level, we consider the ML components in
isolation and study the bugs inside them ignoring other components of the
system.

– Bugs in any component that aﬀect functionality of ML components: At
this level, all identiﬁed bugs in the previous level have been taken into
account plus the bugs which are out of the ML components but aﬀect
the ML component’s functionality. In this paper, we call this category as
ML-related bugs.

– Bugs in all software components: At this level, we do not make any diﬀer-
ence among ML components and others considering all system bugs as the
same.

Based on the deﬁnition of faults in IEEE standard glossary of software en-
gineering terminology [28], software fault is manifestation of a bug in software.
In other words, when a software bug causes an incorrect software operation,
it becomes a software fault [13]. Concerning that faults emerge from the dis-
cordance between software requirements and the existing behavior, faults can

ConfigurationData CollectionML CoreData VerificationFeature ExtractionModel AnalysisResource ManagementProcess ManagementServing InfrastructureAutomationTesting and DebuggingMetadata ManagementMonitoringBugs in Machine Learning-based Systems: A Faultload Benchmark

7

be functional or non-functional. Functional faults refer to the inability of the
software to meet the required functionality. Non-functional faults stem from
the deﬁciencies in methodologies to achieve the required functionality, not the
functionality. An ML fault is also considered as an inadequacy in the behavior
of the ML component [26,32]. A software fault results in a failure only when a
user tries to use the faulty software component, leading to fault activation [13].
Generally speaking, failure in software engineering is known as the inability of
the system or its components to fulﬁll required functions [59]. Faults in ML-
based systems may also lead to bad performance, crash, data corruption, hang,
and memory out of band which are considered as failure in these systems [30].

2.3 SRE in ML-based Systems

Because of the essential diﬀerences between the paradigm of traditional and
ML-based software systems, we are facing several new challenges in SRE of
ML-based systems. Various studies have acknowledged the signiﬁcant chal-
lenges in SRE of the ML-based systems. Fast changes in the new versions
of ML frameworks is one of the major challenges that Islam et al. [31] re-
ported. As an example, they exposed that almost 26% of operations have
been changed from version 1.10 to 2.0 in TensorFlow. Code portability is an-
other crucial challenge in the SRE of ML-based systems [42]. There are mul-
tiple ML frameworks (e.g. TensorFlow, Keras, PyTorch, etc.) just for Python
programming language. Although they have some similarities, there are ma-
jor diﬀerences among them. Therefore, understanding and porting ML codes
from one framework to another can be a nontrivial task. Bug reproducibility
is another signiﬁcant challenge in the SRE of ML-based systems [83]. Wardat
et al. [79] also reported the lack of detailed information regarding the bugs in
ML-based systems as a basic challenge in SRE of ML-based systems. We aim
to cover these challenges in our proposed benchmark.

SRE techniques mostly use the lifecycle of bugs [45]. One of the major SRE
approaches using the bug’s lifecycle is fault removal that aims to detect the
existing faults and remove them. Such techniques use validation and veriﬁca-
tion approaches to cope with reliability concerns that are known as software
testing techniques. Overall, software testing is considered as one of the most
complicated tasks of the software development process. It is well-accepted that
the complexity of the testing has a direct relation with the complexity level of
the system to be tested [13]. That means, by increasing the complexity of the
system, testing becomes more complicated to be able to deal with the system
quality ﬂaws. It has also been proved that the complexity level of the ML-based
systems stays at a higher place compared to traditional ones [6]. Consequently,
testing of the ML-based systems is considered as more complicated tasks, in
comparison with traditional software systems.

ML testing properties refer to the conditions that are needed to be guar-
anteed for a trained model during testing. In other words, ML testing prop-
erties represent the quality attributes that should be tested and satisﬁed in

8

Mohammad Mehdi Morovati et al.

ML-based systems [82]. Existence of bugs in ML-based systems may result in
violation of various ML testing properties, depending on the impact of bugs
on the system. In this study, we used the introduced ML testing properties
by Zhang et al. [82] which are brieﬂy reviewed in this subsection. Correctness
represents the probability that the ML system works in the right way, as it
is intended. Model relevance checks the complexity of the ML model to make
sure it is not more complicated than required. Robustness is deﬁned as the
extent to which the ML system is able to handle invalid inputs and functions
correctly. Eﬃciency refers to the speed at which ML systems operate and per-
form the deﬁned tasks (e.g., prediction). Fairness ensures that ML systems
make decisions without bias. Interpretability is the degree to which humans
can understand the reasons behind the decisions that ML systems make. Al-
though testing properties have been categorized into six diﬀerent classes, they
may overlap with each other [82].

Bad performance, crash, data corruption, hang, incorrect functionality, and
memory out of bound are symptoms of ML-related bugs which are known as
various failure types in ML systems [30,83]. Bad/poor performance refers to
the situation where the accuracy of the ML component is not as good as
expected. Crash is the most common symptom of ML-related bugs in which
ML-based software stops running with or without showing an error message.
Data corruption means data has been corrupted when it passes the network
which leads to wrong output. When the ML software stops responding to the
input without prompting an error, it is known as hang. Incorrect functionality
refers to the situation that ML software behavior diﬀers from the expected,
without any error. Memory out of bound occurs due to the unavailability of
required memory for training. It should be also taken into consideration that
symptoms of bugs belonging to each ML testing property can be diﬀerent.
For instance, if the correctness testing property of an ML component gets
violated, its symptoms may be any of known types such as bad performance,
crash, hang, etc.

It should be also taken into consideration that symptoms of bugs belonging
to one ML testing property can be diﬀerent. For instance, when the correctness
testing property of the ML component has been dissatisﬁed, its symptoms may
be any of known ML-based systems failure types such as bad performance,
crash, hang, etc.

It is not surprising that researchers have used testing techniques from tra-
ditional software systems to cope with testing challenges of ML-based systems.
However, traditional testing methodologies would not be suﬃcient and eﬃcient
testing approaches for ML-based systems [48]. Traditional testing methods re-
quire adaptation to the context of ML to be eﬀective for them [8]. Moreover,
the concept of quality is not well-deﬁned in ML-based systems and its termi-
nology is diﬀerent from the traditional ones [42,7]. It is also worth noting that
the intrinsic diﬀerence between ML-based and traditional software systems
generates new types of bugs which do not exist in the traditional software
systems [59]. For instance, the behavior of the ML-based systems is heavily
dependent on factors such as training dataset, hyperparameters, optimizer,

Bugs in Machine Learning-based Systems: A Faultload Benchmark

9

etc. Besides, it is hardly possible for humans to debug the learned behavior
which is encoded by weights within the ML model.

Several studies have been carried out to provide tools to test ML-based sys-
tems. Wardat et al. [79] conducted research to localize the bugs in ML-based
systems. They explained that because understanding ML models’ behavior is
challenging, existing debugging methods for ML-based systems do not sup-
port localization of the bugs. They provided a dynamic mechanism to analyze
the ML components and implemented an alternative “callback” mechanism
in Keras to collect the detailed information of the ML component during the
training phase. Then, their proposed tool analyzes the collected data to dis-
cover possible bugs and their root causes. Islam et al. [31] carried out an
empirical study on the challenges that the automated repairing tools should
address. They reviewed SO posts and GitHub bug ﬁxes using the ﬁve most
popular ML frameworks (Caﬀe 1, Keras 2, TensorFlow 3, Theano4, and Torch 5)
to identify ﬁx patterns. They classiﬁed the bug-ﬁx patterns specially used in
Deep Neural Networks (DNN) into 15 diﬀerent categories and provided various
solutions to ﬁx bugs belonging to diﬀerent classes. Schoop et al. [62] oﬀered
a system namely UMLAUT to assist non-expert users in identifying, under-
standing, and ﬁxing bugs in the DL programs. UMLAUT can be attached to
the DL program to check the model structure and its behavior. Then, it sug-
gests the best practices to improve the quality of ML components. Nikanjam
et al. [52] provided an automatic fault detection tool for DL programs namely
NeuraLint that validates the DL programs by detecting faults and design in-
eﬃciencies in the implemented models. They identiﬁed 23 diﬀerent rules using
graph transformations to detect various types of bugs in DL programs.

2.4 Benchmark

Benchmark is known as a standard tool for the competitive evaluation of
systems and for making comparisons amongst systems or components, in terms
of speciﬁc characteristics like performance, security, and eﬃciency [77]. It is
widely acknowledged that a standardized benchmark is the most signiﬁcant
requirement to evaluate and compare the methodologies [38]. To generate a
standardized benchmark, several criteria should be satisﬁed in the development
process of the benchmark, including [38,77]:

– Relevance: asserts that the result of benchmark can be used to measure
the performance of the operation in the problem domain. That is to say,
how the benchmark behavior relates to the behavior of interest to its con-
sumers. Relevance is mostly considered as the most important factor of

1 https://caﬀe.berkeleyvision.org/
2 https://keras.io/
3 https://www.TensorFlow.org/
4 https://github.com/Theano/Theano
5 http://torch.ch/

10

Mohammad Mehdi Morovati et al.

any standard benchmark [38]. Without providing relevant information to
the benchmark users, it is highly possible that the benchmark will not
be in the users’ interest, even if it gives perfect services for other crite-
ria. As a general rule of thumb, the benchmark that is well-suited for a
particular domain has limited applicability, while the benchmark trying to
cover a broader range of domains will be less meaningful for any speciﬁc
domains [27].

– Reproducibility: refers to the benchmark ability to provide the same

results while it is run using the same conﬁguration.

– Fairness: explains that all competing systems be able to use the bench-
mark equally. In other words, the benchmark should be usable for all sys-
tems, without generating artiﬁcial limitations.

– Veriﬁability: ensures that the benchmark results are accurate.
– Usability: means that the benchmark should be understandable easily to
prevent credibility shortage. Credibility shows the level of conﬁdence that
users have in the results [61]. Ease of use is also another important property
belonging to this criterion.

Faultload benchmark is one of the main categories of standard benchmark
that includes a set of faults and tries to provide experience of the real faults
occurring in the system. It is also commonly conﬁrmed that faultload bench-
mark is the most complex one among all benchmark categories, because of the
complicated nature of the faults [77].

Concerning the drastic inﬂuence of ML in several safety-critical areas dur-
ing the last few years, the reliability engineering of ML-based systems has
become more crucial. A faultload benchmark of ML-based systems can play a
vital role in the assessment of methods working on the reliability engineering
of the ML-based systems. Although there may exist a great number of bench-
marks in each software domain, a few numbers of them satisﬁed requirements
of the standard benchmark [77]. Accordingly, there are several public datasets
of bugs in ML-based systems provided as either replication package of their
study or a faultload benchmark, but they have some considerable problems.
For example, bug’s dataset provided in [83] does not provide any information
about the application dependencies and ignores reproducibility of the collected
bugs entirely. Besides, several reported bugs were based on deprecated versions
of Python (older than version 3.6) which might be ineﬃcient for assessment
of the current ML-based systems testing tools.

Another public bugs dataset that is provided by Islam et al. [31] has some
similar problems. Firstly, they completely disregarded the reproducibility of
the bugs. While dealing with the dependency challenge, we came across bugs
unrelated to the ML or which occurred in old versions of Python (older than
3.6). On the other hand, some mentioned bugs were based on ML frameworks
which are discontinued. For instance, this public dataset noted 15 bugs using
Theano (a Python library to deﬁne, optimize, and evaluate mathematical ex-
pressions) [5]. With regards to the fact that Theano is a deprecated library
and not supported anymore, adding bugs that use Theano would not be valu-

Bugs in Machine Learning-based Systems: A Faultload Benchmark

11

able. Also, it reported 17 bugs based on Torch [10], a scientiﬁc computing
framework for ML algorithms, which development has been deactivated since
2018 [76].

Similar to the former studies, in the public bugs dataset published by
Humbatova et al. [26], many reported bugs depend on Python older than 3.6.
We also found several commits mentioned as bug-ﬁx, while they are not a
real bug [14] or not accessible [16]. Lack of dependency information is another
shortcoming of this public dataset. It is worth noting that datasets provided
in [26,31,83] are replication packages of those studies and authors do not aim
to introduce a faultload benchmark. However, those datasets can be useful for
researchers who are studying bugs in ML-based systems.

In the public bugs dataset that Wardat et al. [79] have delivered, the repro-
ducibility of the bugs received no attention resulting in many non-reproducible
bugs. Besides, the coverage of the provided bugs is also relatively limited. That
is to say, they cover limited types of bugs in ML-based systems.

To the best of our knowledge, there is no benchmark for ML-based systems’

bugs that satisﬁes the mentioned criteria of the standard benchmark.

Fig. 3 Methodology of collecting bugs

3 Methodology

In this section, we describe the methodology that we followed to answer our
RQs. To this end, we need to collect and investigate the ML-related bugs.
Figure 3 represents the methodology that we used to collect the bugs for
answering our RQs.

We used two main sources to gather bugs: (1) public datasets of previous
studies on the bugs in ML-based systems, and (2) ML-related bugs reported in
GitHub or SO. To gather the bugs from the prior research, we reviewed several
articles that were about bug’s lifecycle in ML-based systems and provided
public bugs datasets [83,31,79,26]. As the second source, we extracted the bugs
reported in (a) bug-ﬁx commits of GitHub repositories or (b) SO posts. We
focused on two of the most popular ML frameworks, TensorFlow and Keras,

Public DatasetsExclusion CriteriaChecking ReproducibilityManuallyProviding ArtifactBugsstackoverflowReproducible Bug12

Mohammad Mehdi Morovati et al.

Table 1 Detailed information about the selected ML frameworks

ML Framework

#stars #forks #subscribers

TensorFlow
Keras

160K
52.2K

85.7K
18.8K

8K
2K

respecting the well-known popularity metrics [81] (e.g., number of stars and
number of forks) of their GitHub repositories. Table 1 represents the detailed
information regarding the popularity metrics of the selected ML frameworks
(on the date we checked them).

We narrowed down our study to the bugs in DL systems, based on the main
categories of DL systems faults’ taxonomy provided by [26]: model, tensors
and input, training, GPU usage, and API categories. “Model” refers to the
bugs related to the structure of the model (such as [17]). “Tensor and input”
category covers the bugs regarding the problems in the shape and format
of data (such as [69]). “Training” includes the bugs in the model training
process (such as [18]). “GPU usage” category deals with bugs occurred when
using GPU devices in DL systems (such as [19]). “API” refers to the problems
related to the API usage in ML frameworks (such as [20]).

To examine the collected bugs and remove those which could not satisfy the
standard benchmark criteria in manual checking step, we used some exclusion
criteria:

– bugs using Python version older than 3.6. The reason behind this
criterion is removing applications using deprecated version of Python or
ML frameworks.

– bugs irrelevant to the identiﬁed ML frameworks (TensorFlow and
Keras). Some of the collected buggy applications use ML frameworks
other than our favorable (e.g., Caﬀe). They are collected because their
repositories have had “tensorﬂow” or “keras” keywords in their description.
So, we remove them using this criterion.

– bugs without dependency information. One of the most signiﬁcant
needed information to reproduce bugs is their dependencies. We applied
this criterion to ﬁlter out bugs that are not reproducible.

– bugs without required data or description for achieving it. Used
data while facing the bug is another key requirement for reproducing ML-
based systems’ bugs. We establish this criterion to delete irreproducible
bugs.

– bugs which are not related to the ML. With respect to our primary
aim to investigate characteristics of ML-related bugs and provide faultload
benchmark of ML-based systems, we use this criterion to exclude bugs that
their root cause is not ML.

– bugs with description in any language other than English. Because
we use commit messages as material to inform the users about the detailed
information of bug and the bug-ﬁx solution, we deleted the commits that
use other languages than English for their commit messages.

Bugs in Machine Learning-based Systems: A Faultload Benchmark

13

Table 2 Detailed information regarding the collected bugs from public datasets

Dataset

Zhang et al. [83]
Islam et al. [31]
Wardat et al. [79]
Humbatova et al. [26]
Nikanjam et al. [52]
Total

Num of Collected bugs
SO
GitHub
0
9
3
8
14
4
0
0
10
8
27
29

3.1 Collecting Bugs from Public Datasets

Zhang et al. [83] carried out a study on the bugs in TensorFlow programs
to identify the root causes and symptoms of various types of bugs. They pro-
vided a public dataset of 88 and 87 bugs studied in their paper, extracted from
GitHub and SO, respectively. After applying exclusion criteria, we obtained
9 GitHub related bugs to be added to the benchmark. But none of their re-
ported bugs from SO remains after applying exclusion criteria. Islam et al. [31]
provided a public dataset along with their research on the bug ﬁx patterns in
DNN programs. DNN program refers to the DL model and training algorithm,
where they investigated for bug ﬁx patterns. Their public dataset contains 347
and 320 bugs from GitHub and SO, respectively. By ﬁltering bugs using the
exclusion criteria, we obtained 8 reproducible GitHub related bugs and 3 SO
relate. Wardat et al. [79] provided a benchmark of bugs in DNN programs us-
ing Keras. They reported 11 and 29 bugs from GitHub and SO, respectively.
After applying our reﬁnement process, we obtained 4 GitHub related and 14
SO related bugs to add to the benchmark. Humbatova et al. [26] studied diﬀer-
ent types of bugs in DL programs and proposed a taxonomy on the identiﬁed
bugs. They also published the dataset of their studied bugs containing 60 bugs
from GitHub and 109 from SO. Our ﬁlters eliminated all their mentioned bugs
and we could not achieve any bug from their dataset. Nikanjam et al. [52] de-
livered a public dataset with their automatic bug detection tool including 34
real bugs in DL programs, 26 from SO and 8 from GitHub. After checking their
provided bugs using mentioned exclusion criteria, we added 8 of GitHub bugs
and 10 SO ones to the benchmark. Table 2 represents the number of bugs that
we added to defect4ML from public datasets of bugs. It is also worth noting
to mention that some SO bugs are repetead in diﬀerent public datasets. As an
example, post [66] exists in three public datasets [52, 79,31].

3.2 Collecting Bugs from GitHub

GitHub6 is considered the most signiﬁcant resource of open source software
repositories in the computer programming community. As of September 2020,

6 https://github.com/

14

Mohammad Mehdi Morovati et al.

Table 3 Detailed information about the number of remaining bug-ﬁx commits after each
ﬁltering step

All extracted ML-based repos
Bug-ﬁx commits
ML-related bug-ﬁx commits
Sampled bug-ﬁx commits

ML frameworks
TensorFlow Keras
30387
98562
26326
379

51151
157190
38463
380

Added bugs to the benchmark

17

29

GitHub hosts more than 56 million users and about 190 million software repos-
itories [23] including more than 28 million public repositories. GitHub provides
API to simplify the data extraction process that allows developers to create
their own requests and extract preferred data. We also used GitHub rest API
v3 [24] to gather repositories.

3.2.1 Selection of ML-based Systems Repositories

To collect the ML-based systems’ repositories, we used GitHub search API.
Firstly, we limited the results to the repositories that use Python programming
language which is deﬁned as “Python” and “Jupyter Notebook” programming
languages in GitHub. Python is the most popular programming language for
ML [78,25]. On the other hand, Keras and TensorFlow also provide Python
APIs. As it is mentioned, we analyzed ML-based systems that use TensorFlow
and/or Keras. We extracted the repositories using each of TensorFlow and
Keras separately. To this end, we used “tensorﬂow” and “keras” keywords
to extract the repositories using these ML frameworks. In the next step, we
limited the repositories to the ones with at least one push after 2019. This
criterion is to decrease the possibility of reviewing repositories using the old
versions of ML frameworks or Python, which may not be beneﬁcial to add to
the benchmark. At the same time, since it does not prevent the inclusion of
repositories using old versions of Python or ML framework, we also ﬁlter those
repositories during our manual inspection. Furthermore, repositories that are
forked or deﬁned as “disabled” are excluded in the search query.

GitHub search API limits the users to access just the ﬁrst 1000 results.
So, we divided the whole duration of search for push command from Jan 1,
2019 to Aug 30, 2021 (the time we run the queries) into snapshots of 5 days to
restrict the number of results to less than 1000. That is to say, we raised 192
search requests for each ML framework to extract repositories. We collected
30, 387 and 51, 151 repositories that use Keras and TensorFlow, respectively. In
the ﬁltering process, we did not ﬁlter the repositories based on the popularity
criteria (e.g., number of stars, number of commits, etc.) to keep more diversity
in the collected bugs. In other words, we aimed to collect as much as bugs from
developers with various expertise levels to avoid generating the benchmark
using a biased set of bugs.

Bugs in Machine Learning-based Systems: A Faultload Benchmark

15

3.2.2 Selection of Bug-ﬁx Commits

To extract bug-ﬁx commits from the collected repositories, we searched com-
mits’ messages for a list of bug-related keywords (bug, fail, crash, ﬁx, resolve,
failure, broken, break, error, hang, problem, overﬂow, issue, stop, etc.), which
are used successfully in the literature [3,2,4]. We also used PyDriller [65], a
python library to mine the GitHub repositories, to collect bug-ﬁx commits. In
this step, we collected 157, 190 bug-ﬁx commits from repositories using Ten-
sorFlow and 98, 562 from the ones using Keras. To exclude bug-ﬁx commits
which are irrelevant to ML, we performed another ﬁltering step based on the
approach used successfully in [26]. We searched a list of keywords that are
related to the various bug types in ML components (e.g., optimize, loss, layer,
etc.) in the commits’ messages and exclude ones with none of those keywords.
Thus, we reached 38, 463 and 26, 326 bug-ﬁx commits for TensorFlow and
Keras, respectively.

Afterwards, we used sampling with 95% for conﬁdence level and 5% for
conﬁdence interval that gives us 380 bug-ﬁx commit for repositories using
TensorFlow and 379 for Keras. In the next step, we manually checked all of
the gathered bug-ﬁx commits, applied the exclusion rules, and removed the
inappropriate ones. To achieve exclusion criteria, the ﬁrst author reviewed 200
bug-ﬁx commits and shared the results with the second and third authors (all
with more than 2 years of experience in engineering ML-based systems). After
three meetings, we achieved an agreement on the following exclusion criteria:

– Bug-ﬁx commits with messages written in languages other than English.
– Bug-ﬁx commits that do not demonstrate the problem clearly.
– Bug-ﬁx commits which used ML frameworks other than TensorFlow or

Keras.

Besides, based on the changed LOC and manipulated APIs by the commit,
we consider the bug-ﬁx commits that can be categorized as one of the most
recent taxonomy of DL bugs [26]. To identify ML-related bug-ﬁx commits,
the ﬁrst two authors separately checked the 100 randomly selected bug-ﬁx
commits, 50 from repositories developed using TensorFlow and 50 by Keras,
attaining 54.7% agreement using Cohen’s Kappa [50]. So, we had two meetings
to recognize the main reasons for disagreements and resolve them. Next, we
again checked the 100 reviewed bug-ﬁx commits achieving 89.6% agreement
based on Cohen’s kappa which is considered as almost perfect agreement [51].
Afterward, the ﬁrst author reviewed the rest of randomly selected bug-ﬁx
commits. Then, the second author checked those bug-ﬁx commits that gained
a level of agreement of 86.4%. Concerning most of the reviewed repositories do
not provide complete information of their dependencies, we tried to ﬁnd the
match version of the used ML framework with the date of bug-ﬁx commit. For
instance, it is obvious that commits which are done before March 5, 2018 could
not use Keras higher than version 2.1.4, because the release date of Keras 2.1.5
is March 6, 2018 [36]. Afterward, we attempted to ﬁnd out the Python version
and complete list of required libraries and their version, matching the version

16

Mohammad Mehdi Morovati et al.

of used ML framework. Finally, we achieved 38 bug-ﬁx commits which meet
benchmark requirements. Concerning our primary aim for providing 75 bugs
from GitHub in the ﬁrst release of defect4ML, we checked manually 151 other
bug-ﬁx commits from repositories using TensorFlow and/or Keras randomly
to attain our goal.

With respect to the need for both buggy and ﬁxed versions of the applica-
tion for each identiﬁed bug in our proposed benchmark, we have provided a
snapshot of the application’s repository exactly before ﬁxing the bug. To this
end, we use
command and extract the commit
prior to the bug-ﬁx (
refers to the buggy ﬁle that bug-ﬁx com-
mit will change). Using that commit, we can gather the version of repository
exactly before ﬁxing reported bug.

git log -p <fileName>
fileName

3.3 Extracting Bugs from Stack Overﬂow

Stack Overﬂow7 (SO) is taken into account as the largest Q&A platform for
software developers, with over 21 million questions and 14 million registered
users on March 2021 [74]. To collect intended posts, we used Stack Exchange
Data Explore8 platform, where one can gather information regarding SO posts
using SQL queries. Like for data extraction from GitHub, we used some criteria
for ﬁltering SO posts and collecting relevant ones. In the ﬁrst step, we collected
posts that have “tensorﬂow” or “keras” as post tags. So, we gathered 50, 001
and 37, 887 question regarding TensorFlow and Keras, respectively. Besides,
21, 908 posts have both “tensorﬂow” and “keras” tags at the same time. We
considered all of them as posts related to Keras. Then, we ﬁltered out the
posts without an accepted answer where one can not make sure of ﬁxing the
issue, in the SO posts without accepted answer. So, we reached 18, 812 posts
regarding TensorFlow and 14, 590 about Keras. Afterward, we used sampling
with 95% and 5% for conﬁdence level and conﬁdence interval, respectively.
Thus, we attained 376 posts related to TensorFlow and 374 for Keras. It is
worth noting that instead of selecting the posts randomly, we selected the
posts with the highest scores. Next, we manually checked all of the collected
bugs’ root cause to keep ones related to the ML and remove irrelevant ones.
Similar to the GitHub manual checking step, we used some exclusion criteria
to ﬁlter out the irrelevant SO posts. To obtain exclusion criteria, The ﬁrst
author analyzed 100 SO posts, 50 related to the TensorFlow and 50 regarding
Keras with the highest score and discussed the results with the second author.
After two meetings, we reached the following exclusion criteria:

– posts which mentioned conceptual questions about ML/DL components

(such as [70]).

– posts related to the users questions on developing ML/DL components,

not resolving a bug (such as [67]).

7 https://stackoverﬂow.com/
8 https://data.stackexchange.com/stackoverﬂow/query/new

Bugs in Machine Learning-based Systems: A Faultload Benchmark

17

Table 4 Detailed information about the number of remaining posts after each ﬁltering out
step.

All extracted posts
Posts with accepted answer
Sampled posts

ML frameworks
TensorFlow Keras
37887
14590
374

50001
18821
376

Added bugs to the benchmark

3

7

– posts that their root causes were not ML and they were just the typical

programming mistakes (such as [71]).

– posts that do not include the required script to reproduce the bug.
– posts without mentioning the employed dataset or a clear description about

it. it [68].

In the next step, the ﬁrst author checked the 50 posts with the highest score
for each ML framework (a total of 100) to identify relevant ones. In the next
step, the second author reviewed them which obtained 89.3% agreement using
Cohen’s kappa. Then, the ﬁrst author labeled the rest of the SO posts and the
second author checked them, gaining 87.4% agreement.

From the availability of the required data viewpoint, we can categorize the
SO posts into three main categories. First, the posts that mention popular
datasets such as MNIST [41] or CIFAR-10 [39] (such as [73]). We utilized
Keras datasets9 to reproduce bugs belonging to this group. Second, posts
that provide the link to achieve the needed data to reproduce the bugs (such
as [72]). Third, posts that did not give any description about the required data
(such as [69]). To address the required data problem of the posts that belong
to this group, we tried to reproduce the bug using popular datasets. In case
of inability to reproduce the reported bug, we excluded the post. By manual
investigating 376 sampled bugs related to TensorFlow and 374 regarding Keras
(750 in total), we achieved 10 reproducible bugs (3 TensorFlow and 7 Keras)
to add to the defect4ML.

3.4 Labeling Collected Bugs

To categorize the collected bugs, we used three kinds of labels, ﬁrstly based
on violated testing property [82], secondly bug type, and ﬁnally according to
symptoms of bugs [30]. Bug type refers to the class of ML bug taxonomy [26] to
which the reported bug belongs. For the ﬁrst step, the ﬁrst two authors held a
meeting to discuss ML testing properties and achieve common understanding
on each ML testing property. Then, the ﬁrst two authors labeled the ﬁrst
25% bugs reaching 85.7% agreement based on Cohen’s kappa [50] which is
interpreted as almost perfect agreement [51] and allows us to continue labeling
the rest of bugs. To assign ML testing property to each bug, we studied commit
message or SO post message to understand the property to which bug-ﬁx

9 https://keras.io/api/datasets/

18

Mohammad Mehdi Morovati et al.

aims. For instance,
[18] which is trying to improve the performance of the
ML model is categorized as eﬃciency property. As another example, [17] that
resolves the problem in the model structure is labeled as correctness. For
labeling the rest of bugs, we labeled them in three parts (25% of bugs in
each part) and held a meeting after each part to identify the major reasons
of disagreements and resolve them. In the case of disagreements between two
raters, they discussed disagreements with the third author which result in
labeling all bugs consistently, which is used in previous studies successfully [64].
Finally, we achieved 88.6% agreement using Cohen’s kappa after labeling all
bugs.

For the second labeling step, the ﬁrst two authors labeled the 25% bugs
separately achieving a 58% agreement based on Cohen’s kappa [50], which is
known as a moderate agreement [51]. Hence, to improve their understanding
of bug types, they had a meeting to have a clear knowledge about each label,
identify the main reasons of disagreements, and build a common understanding
of bug types. Afterward, raters labeled the ﬁrst 25 bugs again resulting in 87%
agreement, implying an almost perfect agreement between them. The rater
continues labeling the rest of bugs with this approach in three parts (every
25% of the bugs in each part). After labeling bugs in each part, we had a
meeting to recognize the main reasons behind the disagreements and resolve
them. Finally, we achieved 88.7% agreement by labeling all bugs, using Cohen’s
kappa. For the remaining disagreements, we used methodology mentioned for
labeling bugs based on the violated properties.

To label the bugs according to their symptoms, ﬁrstly we had a meeting to
achieve an obvious understanding about symptom of ML-related bugs. Next,
the ﬁrst two authors labeled 25% of the bugs gaining 88.5% agreement that is
interpreted as almost perfect agreement [51]. Then, raters labeled the rest of
bugs in three parts (similar to two prior steps) reaching 91.4% agreement using
Cohen’s kappa [50]. To achieve consistent labels for all bugs, raters discussed
the disagreements with the third author and resolved them (same as previous
steps).

4 Results

Generally, we manually checked 513 reported ML-related bugs provided in
previous articles. Also, we collected 64, 789 bug ﬁx commits from ML-based
systems repositories using TensorFlow and/or Keras. We selected 910 out of
them randomly and manually checked them as well for satisfaction of standard
benchmark criteria. Moreover, we manually inspected 750 SO posts related to
TensorFlow and/or Keras. In the following, we present our answers to each of
our formulated research questions.

Bugs in Machine Learning-based Systems: A Faultload Benchmark

19

4.1 RQ1. Key factors in reproducibility of ML-related bugs

Bug reproducibility plays a key role in this study, because it is taken into
account as a substantial challenge in SRE of ML-based systems on the one
hand [83] and in the benchmark generating on the other hand [38]. To make
sure of reproducibility of the collected bugs, we did several manual checking
steps. Firstly, we checked the buggy application for needed dependencies in-
formation to run it without dependency issues. In the next step, we looked
into needed data for running the buggy application and triggering the re-
ported bug. After addressing dependency and data requirements of each bug,
we faced some new challenges while trying to run the buggy applications. Re-
garding the extracted ML-related bugs from GitHub, we have found several
bugs with compilation errors while they are running (such as [15]). Besides,
some of the bugs were not triggered using the mentioned condition in bug-ﬁx
commit (such as [21]). We coped with the reproducibility problem of 75 out
of 1423 (513 from public datasets and 910 from GitHub) reviewed ML-related
bugs extracted from GitHub.

About the reported bugs in SO, almost all of the scripts mentioned in the
posts are just code snippets, not a complete code of the ML component, as
it is popular in SO. Therefore, we had to complete them by the default con-
ﬁguration and then check to ensure that the considered bug would occur. In
some cases, we failed to use the bug after spending considerable time. More-
over, most of the SO posts do not include the required dependency and data
information to reproduce the reported bugs. We could reproduce 25 out of 750
randomly selected SO posts.

Finding 1: The most inﬂuential factors in reproducing ML-related
bugs are required dependencies (including used libraries and their ex-
act version, ML framework version, and Python version) and data to
run the buggy application. Besides, only near 5.3% and 3.34% of all
manually inspected ML-related bugs extracted from GitHub and SO
can be reproduced.

4.2 RQ2. The important factors in veriﬁability of ﬁxing ML-related bugs

Software veriﬁcation refers to the process of checking software to ensure that
it achieves deﬁned goals without any bug [29]. To verify the ﬁxing of reported
bugs, we need to ensure that the mentioned problem will be resolved after
applying the ﬁx. Software systems use software testing methods to verify their
goals and objectives [45]. We investigated all reproducible ML-related bugs for
the test cases provided by buggy applications that check the veriﬁcation of the
software component consisting of reported bugs. We found 10 out of 75 buggy
applications extracted from GitHub that can be veriﬁed using provided test
cases by the applications. In case the buggy application does not give any test

20

Mohammad Mehdi Morovati et al.

cases, we have to trust the bug-ﬁx commit message to verify the bug ﬁxes, like
almost all of the previous articles studying ML-related bugs and their ﬁxes [26,
83,31].

Veriﬁability of ﬁxing reported bugs in SO posts is a more severe challenge.
Most of the scripts mentioned in SO posts are code snippets and do not have
additional information like test cases. So, we could not verify any of the re-
producible ML-related bugs extracted from SO using provided test cases by
developers who ask the question. Instead, we used the accepted answer ﬂag to
verify bug ﬁxes.

Finding 2: Lack of test cases and immaturity of providing test cases
for ML-related bugs are the most inﬂuential factors for verifying ﬁx of
ML-related bugs. Moreover, just near 13.3% of ML-related bugs gath-
ered from GitHub can be veriﬁed using provided test cases by their
applications. However, none of the reproducible ML-related bugs from
SO could be veriﬁed by test cases implemented by the user who created
the post.

4.3 RQ3. Providing a standard faultload benchmark for ML-based systems

Overall, we collected 113 bugs for defect4ML, 75 from GitHub, and 38 from
SO. Table 5 shows the detailed information of collected bugs. We will explain
the challenges we faced while generating the defect4ML and its merits in com-
parison with others in the upcoming subsections. Figure 4 also depicts the the
distribution of bugs, based on their symptoms.

Fig. 4 Distribution of bugs’ symptoms in defect4ML

0.9%23.9%33.6%41.6% Bad performance Crash Incorrect functionality Memory out of boundBugs in Machine Learning-based Systems: A Faultload Benchmark

21

Table 5 Detailed information of bugs.

Source Framework Bug category

Bug type

#bugs Total

API

Model

TensorFlow

Tensors & inputs

GitHub

Training

API

GPU Usage

Keras

Model

Depricated API
Missing variable initialization
Wrong API usage
Missing softmax layer
Suboptimal network structure
Wrong network architecture
Wrong type of activation function
Wrong weights initialisation
Tensor shape mismatch
Wrong input format
Wrong shape of input data
Redundant data augmentation
Suboptimal batch size
Suboptimal learning rate
Wrong loss function calculation
Wrong selection of loss function
Deprecated API
Missing API call
Missing argument scoping
Wrong API usage
Incorrect state sharing
Missing destination GPU device
Missing dense layer
Missing ﬂatten layer
Suboptimal network architecture
Suboptimal network structure
Wrong ﬁlter size for convolutional layer
Wrong layer type
Wrong model initialization
Wrong network architecture
Wrong type of activation function

TensorFlow

SO

Keras

Tensors & inputs Wrong tensor shape

Missing preprocessing step
Suboptimal batch size
Suboptimal number of epochs
Wrong loss function calculation
Wrong optimisation function
Wrong selection of loss function
Deprecated API

Training

API

Tensors & inputs Wrong input format

API

Model

Tensors & inputs

Wrong API usage
Suboptimal network structure
Wrong network architecture
Wrong type of activation function
Tensor shape mismatch
Wrong input format
Wrong shape of input data
Wrong tensor shape
Total

3
1
3
1
1
1
1
1
2
1
1
1
1
2
4
1
3
2
1
2
1
1
1
1
1
5
2
2
2
2
3
2
1
2
5
2
4
5
2
1
10
1
3
9
2
3
6
1

7

5

4

9

8

2

19

2

19

2
1
10

13

12

113

4.3.1 Satisﬁed Criteria of Standard Benchmark

We consider all criteria of standard faultload benchmark as the challenges of
generating standard bug benchmark in ML-based systems. We will describe the
methodologies we used to satisfy each criterion in the upcoming subsections.

Relevance. To meet the relevance criterion, we just include ML-related bugs
in defect4ML. Because there are well-suited benchmarks for traditional pro-
gramming bugs (non ML-related bugs) such as [80, 47,40,44,35,43], we exclude

22

Mohammad Mehdi Morovati et al.

any bugs that are not related to ML. For example, commit [22] which is related
to the ﬁx in README ﬁle has been excluded from defect4ML.

Reproducibility. To fulﬁll reproducibility, we have provided accurate infor-
mation required for reproducing bugs. The information consists of complete
list of dependencies (all needed libraries and their exact version including
ML framework), Python version compatible with ML framework and other li-
braries that application uses, used dataset while facing the reported bug, and
the instruction to activate the bug.

Fairness. We have applied three main measures to meet the fairness and pre-
vent generating artiﬁcial limitations for defect4ML. Firstly, we have equipped
the benchmark with two of the most popular ML frameworks (TensorFlow
and Keras). Secondly, we have used the taxonomy of bugs in DL programs
introduced in [26] and tried to label the benchmark bugs based on the leaves
of that taxonomy. We have covered 30 types of bugs mentioned in this tax-
onomy. So, the benchmark can satisfy various users’ requirements who focus
on speciﬁc types of bugs. We have also provided diﬀerent types of bugs using
various versions of ML frameworks and Python.

Veriﬁability. We have implicitly satisﬁed this criterion because all collected
bugs are real and discussed in GitHub projects or SO posts. The benchmark
also provides a link to the bug’s origin (GitHub bug-ﬁx commit or SO post) for
all bugs. Bug-ﬁx commit messages and SO posts represent detailed informa-
tion about the occurrence of the bug and the solution to ﬁx it. The benchmark
delivers two versions of the application where the bug has occurred, i.e., the
buggy and ﬁxed (or clean) versions. The Buggy version indicates the version
of the application before ﬁxing the bug. The Fixed version refers to the appli-
cation after ﬁxing the identiﬁed bug. Regarding the veriﬁability of ﬁxing bugs
collected for defect4ML, 10 out of 75 reproducible ML-related bugs gathered
from GitHub are veriﬁable by the provided test cases and the rest based on the
bug-ﬁx commit massage. We also used accepted answer ﬂag for SO posts that
prove the veriﬁability of the solution provided to resolve the reported bug.

Usability. We aim to generate an understandable benchmark by delivering
information with each bug, such as violated testing property, bug’s type, and
a link to the bug’s origin.

Users can achieve detailed information regarding the bug’s root cause,
symptoms, and possible ﬁxing methods using the mentioned link. We have
also provided several categorizations (bug’s origin, Python version, ML frame-
work, violated testing property, and bug’s type) to prepare a set of bugs that
ﬁt users’ aims. Users may use defect4ML for diﬀerent goals, such as assessing
the ability of testing tools that focus on bug detection, repair, and localization.

Bugs in Machine Learning-based Systems: A Faultload Benchmark

23

4.3.2 Addressed SRE Challenges in ML-based Systems

Several new challenges in the SRE of ML-based systems make it more compli-
cated compared to the traditional software systems [31, 79]. Neglecting these
challenges may deteriorate the satisfaction level of relevance, reproducibility,
veriﬁability, and usability of ML-based systems. We rise to these challenges in
defect4ML to ensure the relevance of our proposed benchmark. In the following
subsections we elaborate on the methods used to handle each challenge.

Fast Changes in ML Frameworks. To handle the challenge of fast changes
in the ML framework, users need to have the exact information regarding
the version of the used ML framework in the application containing the bug.
We have presented this information for current bugs in defect4ML10. Also,
defect4ML has been equipped with bugs that appeared in diﬀerent versions
of each ML framework.

Code Portability. To tackle the code portability challenge, defect4ML has
provided diﬀerent bugs that occurred in the applications developed using the
two most popular ML frameworks: TensorFlow and Keras. Therefore, users
have access to a list of bugs in their preferred ML framework, without requiring
porting bugs from one ML framework to another one. Users can also enhance
the benchmark by adding bugs from ML frameworks other than those studied
in this paper or recreating bugs in new ML frameworks.

Bug Reproducibility. Bug reproducibility is known as one of the most crit-
ical challenges in all SRE areas. But it is a more severe challenge in ML-based
systems, because of direct eﬀect on the other challenges of SRE in ML-based
systems such as fast changes of ML frameworks and code portability. This dif-
ﬁculty may result from 1) a high amount of operational changes in versions of
the ML frameworks [31], and 2) diﬀerent dependencies speciﬁed for every sin-
gle version of ML frameworks [83]. To meet this challenge, we have delivered
a complete list of dependencies (and corresponding versions) needed to run
the application10. We have also presented speciﬁc conﬁguration of each bug
including Python version, and process to trigger the bug11. Moreover, because
of the substantial eﬀect of data in ML components operations [82,12], we have
delivered the required training/testing datasets to reproduce the bugs.

Lack of Detailed Information about the Bugs. When an ML component
faces a bug, the compiler mostly gives no detailed information regarding the
root cause of the bug or the exact bug location. For example, the compiler
may not recognize the ML model structural bugs, or provides just an error
message to inform the developer about the existence of a problem in the ML
model structure. But it does not give any clue (such as bug location) to the

10 In requirements.txt ﬁle, that consists of detailed information of dependencies per bug.
11 In conf.ini ﬁle, that contains the required conﬁguration per bug.

24

Mohammad Mehdi Morovati et al.

Fig. 5 Sample implementation of XOR classiﬁcation problem using Keras.

developer for debugging the ML model. Figure 5 shows a sample script trying
to implement a classiﬁer for XOR problem using Keras [66]. Although loss
remains constant during training (because of wrong activation function in the
last dense layer), compiler does not give any information about the problem to
the user. To cope with this challenge, we use GitHub bug-ﬁx commit messages
and SO posts description that give detailed bugs speciﬁcations and the possible
solutions to ﬁx them. Bug-ﬁx commit messages may also include the link to
the raised issue in the issue tracker, which has in-depth information about the
bug.

4.3.3 Artifacts

Each bug consists of several components to meet the prerequisites of the bench-
mark. These components include:

– Buggy and ﬁxed versions of the application: each bug has two dif-
ferent versions of the application containing that bug. Buggy version is the
application including the bug and is generally used to evaluate ML-based
systems testing tools’ ability to detect the bugs. Fixed version is the same
application just after ﬁxing the bug and is mainly used to assess the re-
pairing tools. Repairing tools try to detect the bugs and provide another
version of the application where the identiﬁed bug has been ﬁxed based on
the best practices.

– Dependencies (Required libraries): each bug comes with a complete
list of dependencies required to run the buggy version (e.g., ML framework
and its version, and needed libraries and their version)

– Data: All bugs are equipped with the needed data to trigger the reported

bug.

– Conﬁguration: each bug delivers metadata (such as the source of the
bug, Python version, type of bug, link to the source of the bug, etc.) that
is used to produce bug categorization on the benchmark. Moreover, the
process of running the application to trigger the bug is mentioned in the
conﬁguration.

Bugs in Machine Learning-based Systems: A Faultload Benchmark

25

– Test case: Each bug has its own test case which can execute the buggy
application and trigger the bug, without requiring any user’s manipulation.
Besides, the test case enables users to observe the eﬀect of the bug and
compare the behavior of the system in buggy and ﬁxed versions.

We have classiﬁed bugs based on diﬀerent criteria in defect4ML. The ﬁrst
criterion is testing properties of ML-based systems [82] which includes correct-
ness, model relevance, robustness & security, eﬃciency, fairness, interpretabil-
ity, and privacy. This criterion refers to the conditions that should be guar-
anteed for the trained model. Another ﬁltering criterion is the ML framework
used to implement the buggy ML component. Last but not least, to indicate
the types of bugs, we used the taxonomy of DL bugs proposed by Humbatava
et al. [26].

Fig. 6 A screenshot of defect4ML web application

4.3.4 Provided API

In order to deliver an easy-to-use benchmark, we provide a web applica-
tion endpoint for defect4ML (accessible via http://defect4aitesting.
soccerlab.polymtl.ca). Figure 6 represents a screenshot of the defect4ML
web application. Users can browse the bugs, and ﬁlter them based on dif-
ferent criteria to attain a list of bugs that is suitable for their goals. Since
defect4ML is in the process of expansion, we have provided the possibility for
the users to add new bugs to the benchmark or raise a request for removing the
existing ones. Users can submit new bugs by providing an artifact of the bug.
Detailed explanation of adding new bugs has been presented in benchmark
web application page.

26

5 Discussion

Mohammad Mehdi Morovati et al.

This section presents an example application of defect4ML as a case study:
comparing two ML-based systems testing tools: NeuraLint and DeepLocalize.

5.1 Benchmark Applications

According to the characteristics of the defect4ML, it can be beneﬁcial to stud-
ies on bugs in ML-based systems. Developers of ML-based systems testing
tools can use defect4ML to show the advantages of their proposed tools and
techniques compared to the existing ones. The researchers can also use our
proposed benchmark to evaluate existing ML-based systems testing tools and
clarify the most critical challenges that should receive attention from new stud-
ies. We provide a case study of using defect4ML to evaluate ML-based systems
testing tools. So, the primary goals of this case study are the assessment of
ML-based systems testing tools and comparing them.

In this case study, we compare two testing tools for ML-based systems. We
selected two up-to-date testing tools published recently: NeuraLint [52] and
DeepLocalize [79]. NeuraLint is a model-based automatic fault detection tools
for DL programs. The authors proposed a meta-model for DL programs that
contains their basic properties. To detect the bugs, NeuraLint ﬁrst extracts
the graph of the DL program from its code. In the next step, it identiﬁes the
bugs using graph transformations that represent detection rules. It is worth
noting that NeuraLint is based on static analysis of DL programs meaning
that it does not need to run the DL program to identify the bugs.

DeepLocalize is another testing tool that can analyze DL programs, detect
the bugs, and localize them automatically. It provides a customized callback
function for Keras that collects DNN detailed information during the train-
ing process. In other words, DeepLocalize analyses the DNN training traces
to identify the possible bugs and their root causes (e.g., faulty layers or hy-
perparameters). Unlike NeuraLint that analyzes the DL programs statically,
DeepLocalize uses dynamic analysis. That is, DL programs should be exe-
cutable without any compilation error to be analyzable by DeepLocalize.

Concerning the fact that NeuraLint can analyze the DL programs that have
been written in one ﬁle, we had a limited number of bugs to use. On the other
hand, due to the existence of compile-time errors in the buggy version of some
bugs, they are not usable for DeepLocalize. We selected 20 bugs randomly from
defect4ML, 10 from SO bugs and 10 from GitHub based ones. With respect to
the limitations of the NeuraLint and DeepLocalize tools, we could just use 12
out of 20 which are usable for at least one of the tools. Table 6 demonstrates
the result of evaluating NeuraLint and DeepLocalize. The cells ﬁlled with ×
refer to the samples that have compile-time errors and could not be used by
DeepLocalize.

Table 6 demonstrates the result of evaluating NeuraLint and DeepLocalize.
The cells ﬁlled with × refer to the samples that have compile-time errors and

Bugs in Machine Learning-based Systems: A Faultload Benchmark

27

could not be used by DeepLocalize. Based on the gathered results, NeuraLint
was able to identify bugs in 9 out of 10 samples. Besides, it has detected design
issues in some examples, in addition to the reported bugs. Design issues are
poor design and/or conﬁguration decisions that can have a negative impact on
the performance and then quality of a DL-based software system [53,52]. For
instance, bug #91 is related to the wrong activation function of the DL model,
while NeuraLint has also identiﬁed that window size for spatial ﬁltering does
not deﬁne properly. Conversely, DeepLocalize could localize the bug correctly
in 1 out of 4 samples.

6 Related Work

The closest work to our proposed benchmark was carried out by Kim et al. [37]
providing a benchmark of bugs in ML-based systems, which is called Dench-
mark. They extracted 4577 bugs reported in the issue tracker of 193 GitHub
repositories. Although their benchmark was the ﬁrst bug benchmark focused
on ML-based systems, their study has several shortcomings. Firstly, they have
considered repositories with various programming languages such as Java, C,
C++, Python, etc., without considering any categorization on them. So, de-
velopers might have to inspect and then categorize the bugs based on their
favorite programming languages, which can be time-consuming. The second
drawback is ignorance of the big diﬀerence between bugs related to the ML
(ML-related bugs) and other ones. Denchmark has reported all bugs without
any diﬀerentiation. Moreover, this study has taken no notice of bug repro-
ducibility, which is one of the main challenges in the SRE of ML-based sys-
tems [83]. Last but not least, this benchmark has neglected standard bench-
mark criteria, which may result in benchmark eﬀectiveness detriment.

Wardat et al. [79] also provided a benchmark of 40 bugs to validate their
proposed tool. They oﬀered 11 bugs from GitHub and 29 bugs from SO and
introduced them as a benchmark of bugs in ML-based systems. The ﬁrst con-
cern about their proposed benchmark is ignorance of the standard benchmark
criteria (e.g., relevance, reproducibility, fairness) and SRE challenges in ML-
based systems. Besides, no information has been provided about the execution
process of applications extracted from GitHub to trigger the bug.

7 Threats to Validity

We now discuss the threats to validity of our study.

7.1 Internal Validity

The primary source of internal threat to the validity of the results provided
in this study can be the categorization of bugs. To diminish this threat, we

28

Mohammad Mehdi Morovati et al.

Table 6 Results of studied tools as case study

Bug
ID

25

Source Framework Violated
Property

Bug
Type

GitHub Keras

Correctness Wrong

26

GitHub Keras

Correctness Wrong

network
architec-
ture

44

GitHub Keras

Eﬃciency

of

type
acti-
vation
function
Suboptimal
network
structure

NeuraLint
result

no identiﬁed
error

no identiﬁed
error

no identiﬁed
error

DeepLocalize
result

batch 4 layer 9
: error in for-
ward

batch 4 layer
11 : error in
forward

batch 4 layer
12 : error in
forward
×

batch 0 layer 2
: error in delta
weights

batch 0 layer 0
: error in for-
ward

×

ac-

lack of pool-
ing, missing
ﬂatten
wrong
tivation
function,
wrong units’
shape
wrong
tivation
function,
wrong layers’
structure
wrong
tivation
function

ac-

ac-

ac-

wrong
tivation
function

batch 0 layer 0
: error in for-
ward

ac-

wrong
tivation
function,
wrong win-
size
dow
for
spatial
ﬁltering
no identiﬁed
error

batch 0 layer 9
: error in delta
weights

model
not learn

does

no identiﬁed
error

Error in delta
weights

missing ﬂat-
ten

×

80

SO

Keras

84

SO

Keras

Correctness Mising
ﬂatten
layer

Correctness Wrong

of

type
acti-
vation
function

86

SO

Keras

Correctness Wrong

of

type
acti-
vation
function

88

SO

Keras

Correctness Wrong

of

type
acti-
vation
function

89

SO

Keras

Correctness Wrong

of

type
acti-
vation
function

92

SO

Keras

Correctness Wrong

of

type
acti-
vation
function

95

SO

Keras

Correctness Wrong

111

SO

Keras

112

SO

Keras

API
usage

Correctness Missing
prepro-
cessing
Correctness Missing

ﬂatten
layer

used the predeﬁned taxonomy of bugs in DL programs discussed in [26]. An-
other internal threat to our proposed benchmark can be the manual inspection
of the bugs and making decisions about their inclusion or exclusion. The ﬁrst
author (a PhD student and practitioner of ML development) reviewed 100 bug-
ﬁx commits and discussed the result with the second and third authors (two
PhDs with research background in engineering ML-based systems) to mitigate

Bugs in Machine Learning-based Systems: A Faultload Benchmark

29

this threat. After three meetings, we reached an agreement on the including
and excluding rules of ﬁltering the bug-ﬁx commits. To ensure that the bench-
mark consists of real bugs, we used just the SO posts with an accepted answer
and bug-ﬁx commits that clearly explain the bug and its symptoms. Veriﬁa-
bility of the bug ﬁxes may be the last internal threat to the validity of this
research. To counteract this threat, we used diﬀerent methods for ML-related
bugs extracted from GitHub and SO.

Regarding the bugs extracted from GitHub, if the buggy applications do
not provide appropriate tests to verify the bug ﬁx, we used bug ﬁx commit
messages that clearly mention the changes to ﬁx the reported bug. To verify
bugs gathered from SO posts, we only used the posts with an accepted answer
that is considered as evidence of ﬁxing the bug correctly.

7.2 External Validity

The most crucial threat to the external validity of this study is its limitation
to the TensorFlow [1] and Keras [9] frameworks. Firstly, we have selected
them because they are two of the most popular ML frameworks. Second, we
have provided the feature to add new bugs using any ML framework to the
benchmark and request to remove the existing bugs from it by any user. Fur-
thermore, to achieve the highest diversity of ML-related bugs, we did not use
any popularity-based ﬁlter on the GitHub repositories. We collected the bugs
from the repositories developed by users with various expertise levels. Another
thread to the external validity of this study can be the selection of Python as
programming language of ML-based systems development. The main reason
behind the selection of Python is the fact that it is the most used programming
language for developing ML components [78, 25]. As a result, a larger variety
of ML-related bugs can be found in ML-based systems based on the Python
programming language and defect4ML will be also usable for a higher number
of users.

8 Conclusion and Future Works

The growing tendency to apply ML-based systems in safety-critical areas in-
creases the demand for reliable ML-based systems. A benchmark of bugs in
ML-based systems, a faultload benchmark, is a key requirement for assess-
ing the eﬀectiveness of studies on ML-based systems’ reliability (like testing)
which are based on bugs’ lifecycle. In this study, we reviewed 1423 ML-related
bugs from GitHub and 750 from SO which are related to the ML-based sys-
tems using two of the most common ML frameworks, TensorFlow and Keras,
and represented that only near 5.3% of GitHub bugs and 3.34% of reported
bugs in SO are reproducible. Besides, we showed that almost 13.3% of ﬁxing
of all reproducible bugs extracted from GitHub can be veriﬁed by their pro-
vided test cases. However, none of the SO posts has test cases for verifying

30

Mohammad Mehdi Morovati et al.

bug ﬁxes. We have also proposed defect4ML, a faultload benchmark of ML-
based systems consisting of 113 bugs extracted from the software systems using
TensorFlow and/or Keras (75 from Github, and 38 from SO). All of the stan-
dardized benchmark criteria have been satisﬁed by our proposed benchmark.
defect4ML also addresses the main SRE challenges in ML-based systems by
providing bugs in various ML frameworks with diﬀerent versions, comprehen-
sive information regarding dependencies and required data to trigger the bug,
detailed information about the type of bugs, and link to the origin of the bug.
Concerning the ongoing nature of creating benchmarks, we plan to add more
bugs to cover all types of bugs in ML-based systems. Moreover, we are going
to improve defect4ML to be usable for automatic bug repair tools. Besides, we
will add bugs based on other ML frameworks (such as PyTorch) to improve
the coverage of the defect4ML.

References

1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,
S., Irving, G., Isard, M., et al.: Tensorﬂow: A system for large-scale machine learning. In:
12th {USENIX} symposium on operating systems design and implementation ({OSDI}
16), pp. 265–283. USENIX, Savannah, GA, USA (2016)

2. Abidi, M., Grichi, M., Khomh, F., Gu´eh´eneuc, Y.G.: Code smells for multi-language
systems. In: Proceedings of the 24th European Conference on Pattern Languages of
Programs, pp. 1–13 (2019)

3. Abidi, M., Khomh, F., Gu´eh´eneuc, Y.G.: Anti-patterns for multi-language systems. In:
Proceedings of the 24th European Conference on Pattern Languages of Programs, pp.
1–14 (2019)

4. Abidi, M., Rahman, M.S., Openja, M., Khomh, F.: Are multi-language design smells
fault-prone? an empirical study. ACM Transactions on Software Engineering and
Methodology (TOSEM) 30(3), 1–56 (2021)

5. Al-Rfou, R., Alain, G., Almahairi, A., Angermueller, C., Bahdanau, D., Ballas, N.,
Bastien, F., Bayer, J., Belikov, A., Belopolsky, A., et al.: Theano: A python framework
for fast computation of mathematical expressions. arXiv e-prints pp. arXiv–1605 (2016)
6. Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi,
B., Zimmermann, T.: Software engineering for machine learning: A case study. In: 2019
IEEE/ACM 41st International Conference on Software Engineering: Software Engineer-
ing in Practice (ICSE-SEIP), pp. 291–300. IEEE (2019)

7. Borg, M.: The aiq meta-testbed: pragmatically bridging academic ai testing and indus-
In: International Conference on Software Quality, pp. 66–77. Springer

trial q needs.
(2021)

8. Bourque, P., Dupuis, R., Abran, A., Moore, J.W., Tripp, L.: The guide to the software

engineering body of knowledge. IEEE software 16(6), 35–44 (1999)

9. Chollet, F., et al.: Keras: The python deep learning library. Astrophysics Source Code

Library pp. ascl–1806 (2018)

10. Collobert, R., Bengio, S., Mari´ethoz, J.: Torch: a modular machine learning software

library. Tech. rep., Idiap (2002)

11. Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., Cui,
C., Corrado, G., Thrun, S., Dean, J.: A guide to deep learning in healthcare. Nature
medicine 25(1), 24–29 (2019)

12. Felderer, M., Ramler, R.: Quality assurance for ai-based systems: Overview and chal-
lenges (introduction to interactive session). In: International Conference on Software
Quality, pp. 33–42. Springer (2021)

13. Galin, D.: Software quality assurance: from theory to implementation. Pearson educa-

tion, England (2004)

Bugs in Machine Learning-based Systems: A Faultload Benchmark

31

14. https://github.com/dpressel/baseline/commit/4dad463 (2016). Accessed: 2021-11-

01

15. https://github.com/suchaoxiao/keras-frcnn_modify/commit/2f51f68 (2017). Ac-

cessed: 2021-11-01

16. https://github.com/albu/albumentations/commit/fec1f3b (2018). Accessed: 2021-

11-01

17. https://github.com/vmelan/cifar-experiment/commit/561c82e (2018). Accessed:

2022-06-01

18. https://github.com/acflorea/keras-playground/commit/d44c90c (2018). Accessed:

2022-06-01

19. https://github.com/keras-team/keras-tuner/commit/3758611 (2018).

Accessed:

2022-06-01

20. https://github.com/hunkim/DeepLearningZeroToAll/commit/9f8fb94 (2018).

Ac-

cessed: 2022-06-01

21. https://github.com/PhilippeNguyen/kinopt/commit/fdee16f (2018). Accessed: 2021-

11-01

22. https://github.com/vaclavcadek/keras2pmml/commit/4795ec6 (2019).

Accessed:

2021-11-01

23. GitHub: Github oﬃcial website. https://github.com/about (2021). Accessed: 2021-7-

27

24. GitHub: Github rest api. https://developer.github.com/v3/ (2021). Accessed: 2021-

7-27

25. Gupta, S.: What is the best language for machine learning? https://www.springboard.
Accessed:

com/blog/data-science/best-language-for-machine-learning (2021).
2021-10-06

26. Humbatova, N., Jahangirova, G., Bavota, G., Riccio, V., Stocco, A., Tonella, P.: Tax-
onomy of real faults in deep learning systems. In: Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering, pp. 1110–1121 (2020)

27. Huppler, K.: The art of building a good benchmark.

In: Technology Conference on

Performance Evaluation and Benchmarking, pp. 18–30. Springer (2009)

28. IEEE: Iso/iec/ieee international standard - systems and software engineering – vocab-
ulary. ISO/IEC/IEEE 24765:2010(E) pp. 1–418 (2010). DOI 10.1109/IEEESTD.2010.
5733835

29. IEEE: Ieee standard for system, software, and hardware veriﬁcation and validation.
IEEE Std 1012-2016 (Revision of IEEE Std 1012-2012/ Incorporates IEEE Std 1012-
2016/Cor1-2017) pp. 1–260 (2017). DOI 10.1109/IEEESTD.2017.8055462

30. Islam, M.J., Nguyen, G., Pan, R., Rajan, H.: A comprehensive study on deep learning
bug characteristics.
In: Proceedings of the 2019 27th ACM Joint Meeting on Euro-
pean Software Engineering Conference and Symposium on the Foundations of Software
Engineering, pp. 510–520 (2019)

31. Islam, M.J., Pan, R., Nguyen, G., Rajan, H.: Repairing deep neural networks: Fix pat-
terns and challenges. In: 2020 IEEE/ACM 42nd International Conference on Software
Engineering (ICSE), pp. 1135–1146. IEEE (2020)

32. ISO: Road vehicles — Safety of the intended functionality. Standard, International
Organization for Standardization (2019). URL https://www.iso.org/standard/70939.
html

33. Jia, L., Zhong, H., Wang, X., Huang, L., Lu, X.: The symptoms, causes, and repairs
of bugs inside a deep learning library. Journal of Systems and Software 177, 110935
(2021)

34. Jiang, Y., Liu, H., Niu, N., Zhang, L., Hu, Y.: Extracting concise bug-ﬁxing patches
In: 2021 IEEE/ACM 43rd

from human-written patches in version control systems.
International Conference on Software Engineering (ICSE), pp. 686–698. IEEE (2021)

35. Just, R., Jalali, D., Ernst, M.D.: Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In: Proceedings of the 2014 International
Symposium on Software Testing and Analysis, pp. 437–440 (2014)

36. Keras: Keras 2.1.5.

https://github.com/keras-team/keras/releases/tag/2.1.5

(2016). Accessed: 2021-11-01

32

Mohammad Mehdi Morovati et al.

37. Kim, M., Kim, Y., Lee, E.: Denchmark: A bug benchmark of deep learning-related soft-
ware. In: 2021 IEEE/ACM 18th International Conference on Mining Software Reposi-
tories (MSR), pp. 540–544. IEEE (2021)

38. v. Kistowski, J., Arnold, J.A., Huppler, K., Lange, K.D., Henning, J.L., Cao, P.: How
to build a benchmark. In: Proceedings of the 6th ACM/SPEC International Conference
on Performance Engineering, pp. 333–336 (2015)

39. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Tech.

Rep. 0, University of Toronto, Toronto, Ontario (2009)

40. Le Goues, C., Holtschulte, N., Smith, E.K., Brun, Y., Devanbu, P., Forrest, S., Weimer,
W.: The manybugs and introclass benchmarks for automated repair of c programs.
IEEE Transactions on Software Engineering 41(12), 1236–1256 (2015)

41. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)

42. Lenarduzzi, V., Lomio, F., Moreschini, S., Taibi, D., Tamburri, D.A.: Software quality
for ai: Where we are now? In: International Conference on Software Quality, pp. 43–53.
Springer (2021)

43. Lin, Z., Marinov, D., Zhong, H., Chen, Y., Zhao, J.: Jacontebe: A benchmark suite of
real-world java concurrency bugs (t). In: 2015 30th IEEE/ACM International Confer-
ence on Automated Software Engineering (ASE), pp. 178–189. IEEE (2015)

44. Lu, S., Li, Z., Qin, F., Tan, L., Zhou, P., Zhou, Y.: Bugbench: Benchmarks for evaluating
bug detection tools. In: Workshop on the evaluation of software defect detection tools,
vol. 5. Chicago, Illinois (2005)

45. Lyu, M.R.: Software reliability engineering: A roadmap. In: Future of Software Engi-

neering (FOSE’07), pp. 153–170. IEEE, Minneapolis (2007)

46. Ma, L., Juefei-Xu, F., Zhang, F., Sun, J., Xue, M., Li, B., Chen, C., Su, T., Li, L., Liu,
Y., et al.: Deepgauge: Multi-granularity testing criteria for deep learning systems. In:
Proceedings of the 33rd ACM/IEEE International Conference on Automated Software
Engineering, pp. 120–131. Association for Computing Machinery (ACM), New York,
NY, United States (2018)

47. Madeiral, F., Urli, S., Maia, M., Monperrus, M.: Bears: An extensible java bug bench-
mark for automatic program repair studies. In: 2019 IEEE 26th International Confer-
ence on Software Analysis, Evolution and Reengineering (SANER), pp. 468–478. IEEE
(2019)

48. Marijan, D., Gotlieb, A., Ahuja, M.K.: Challenges of testing machine learning based
In: 2019 IEEE International Conference On Artiﬁcial Intelligence Testing

systems.
(AITest), pp. 101–102. IEEE (2019)

49. Mart´ınez-Fern´andez, S., Bogner, J., Franch, X., Oriol, M., Siebert, J., Trendowicz, A.,
Vollmer, A.M., Wagner, S.: Software engineering for ai-based systems: A survey. arXiv
preprint arXiv:2105.01984 (2021)

50. McDonald, N., Schoenebeck, S., Forte, A.: Reliability and inter-rater reliability in qual-
itative research: Norms and guidelines for cscw and hci practice. Proceedings of the
ACM on Human-Computer Interaction 3(CSCW), 1–23 (2019)

51. McHugh, M.L.: Interrater reliability: the kappa statistic. Biochemia medica 22(3), 276–

282 (2012)

52. Nikanjam, A., Braiek, H.B., Morovati, M.M., Khomh, F.: Automatic fault detection
for deep learning programs using graph transformations. ACM Trans. Softw. Eng.
Methodol. 31(1) (2021). DOI 10.1145/3470006

53. Nikanjam, A., Khomh, F.: Design smells in deep learning programs: An empirical study.
In: 2021 IEEE International Conference on Software Maintenance and Evolution (IC-
SME), pp. 332–342 (2021)

54. Nikanjam, A., Morovati, M.M., Khomh, F., Braiek, H.B.: Faults in deep reinforce-
arXiv preprint

ment learning programs: A taxonomy and a detection approach.
arXiv:2101.00135 (2021)

55. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin,
Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance
deep learning library. arXiv preprint arXiv:1912.01703 (2019)

56. Pei, K., Cao, Y., Yang, J., Jana, S.: Deepxplore: Automated whitebox testing of deep
learning systems. In: proceedings of the 26th Symposium on Operating Systems Prin-
ciples, pp. 1–18. Association for Computing Machinery (ACM), New York, NY, United
States (2017)

Bugs in Machine Learning-based Systems: A Faultload Benchmark

33

57. Pressman, R.S.: Software engineering: a practitioner’s approach. Palgrave macmillan

(2005)

58. Radjenovi´c, D., Heriˇcko, M., Torkar, R., ˇZivkoviˇc, A.: Software fault prediction metrics:
A systematic literature review. Information and software technology 55(8), 1397–1418
(2013)

59. Riccio, V., Jahangirova, G., Stocco, A., Humbatova, N., Weiss, M., Tonella, P.: Testing
machine learning based systems: a systematic mapping. Empirical Software Engineering
25(6), 5193–5254 (2020)

60. Rivera-Landos, E., Khomh, F., Nikanjam, A.: The challenge of reproducible ml: an

empirical study on the impact of bugs (2021)

61. Rodr´ıguez-P´erez, G., Robles, G., Gonz´alez-Barahona, J.M.: Reproducibility and credi-
bility in empirical software engineering: A case study based on a systematic literature
review of the use of the szz algorithm. Information and Software Technology 99, 164–176
(2018)

62. Schoop, E., Huang, F., Hartmann, B.: Umlaut: Debugging deep learning programs using
program structure and model behavior. In: Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems, pp. 1–16 (2021)

63. Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary,
V., Young, M., Crespo, J.F., Dennison, D.: Hidden technical debt in machine learning
systems. Advances in neural information processing systems 28, 2503–2511 (2015)
64. Shen, Q., Ma, H., Chen, J., Tian, Y., Cheung, S.C., Chen, X.: A comprehensive study of
deep learning compiler bugs. In: Proceedings of the 29th ACM Joint Meeting on Euro-
pean Software Engineering Conference and Symposium on the Foundations of Software
Engineering, pp. 968–980 (2021)

65. Spadini, D., Aniche, M., Bacchelli, A.: PyDriller: Python framework for mining soft-
ware repositories. In: Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software En-
gineering - ESEC/FSE 2018, pp. 908–911. ACM Press, New York, New York, USA
(2018). DOI 10.1145/3236024.3264598. URL http://dl.acm.org/citation.cfm?doid=
3236024.3264598

66. https://stackoverflow.com/questions/34311586 (2016). Accessed: 2021-11-01
67. https://stackoverflow.com/questions/38080035 (2017). Accessed: 2021-11-01
68. https://stackoverflow.com/questions/42264649 (2017). Accessed: 2021-11-01
69. https://stackoverflow.com/questions/53119432 (2018). Accessed: 2021-11-01
70. https://stackoverflow.com/questions/44924690 (2018). Accessed: 2021-11-01
71. https://stackoverflow.com/questions/58636087 (2018). Accessed: 2021-11-01
72. https://stackoverflow.com/questions/50079585 (2018). Accessed: 2021-11-01
73. https://stackoverflow.com/questions/56103207 (2019). Accessed: 2021-11-01
74. StackOverﬂow: Stack overﬂow annual developer

survey.
stackoverflow.com/survey/2021 (2021). Accessed: 2022-04-01

https://insights.

75. Tambon, F., Nikanjam, A., An, L., Khomh, F., Antoniol, G.: Silent bugs in deep learning

frameworks: An empirical study of keras and tensorﬂow (2021)

76. Torch: Torch oﬃcial github repository. https://github.com/torch/torch7. Accessed:

2021-9-1

77. Vieira, M., Madeira, H., Sachs, K., Kounev, S.: Resilience benchmarking. In: Resilience

assessment and evaluation of computing systems, pp. 283–301. Springer (2012)

is

the

learning

C.: What

78. Voskoglou,
chine
what-is-the-best-programming-language-for-machine-learning-a745c156d6b7
79. Wardat, M., Le, W., Rajan, H.: Deeplocalize: Fault localization for deep neural networks.
In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),
pp. 251–262. IEEE (2021)

for ma-
language
https://towardsdatascience.com/

programming

(2017).

URL

best

80. Widyasari, R., Sim, S.Q., Lok, C., Qi, H., Phan, J., Tay, Q., Tan, C., Wee, F., Tan,
J.E., Yieh, Y., et al.: Bugsinpy: a database of existing bugs in python programs to
enable controlled testing and debugging studies.
In: Proceedings of the 28th ACM
Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, pp. 1556–1560 (2020)

34

Mohammad Mehdi Morovati et al.

81. Zerouali, A., Mens, T., Robles, G., Gonzalez-Barahona, J.M.: On the diversity of soft-
ware package popularity metrics: An empirical study of npm. In: 2019 IEEE 26th In-
ternational Conference on Software Analysis, Evolution and Reengineering (SANER),
pp. 589–593. IEEE (2019)

82. Zhang, J.M., Harman, M., Ma, L., Liu, Y.: Machine learning testing: Survey, landscapes

and horizons. IEEE Transactions on Software Engineering (2020)

83. Zhang, Y., Chen, Y., Cheung, S.C., Xiong, Y., Zhang, L.: An empirical study on ten-
sorﬂow program bugs. In: Proceedings of the 27th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis, pp. 129–140 (2018)

84. Zubrow, D.: Ieee standard classiﬁcation for software anomalies. IEEE Computer Society

(2009)

