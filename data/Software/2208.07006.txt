2
2
0
2

g
u
A
5
1

]
T
G
.
s
c
[

1
v
6
0
0
7
0
.
8
0
2
2
:
v
i
X
r
a

Cooperative and uncooperative institution designs:
Surprises and problems in open-source game theory

Andrew Critch1, Michael Dennis1, Stuart Russell1
Center for Human-Compatible AI, University of California, Berkeley

Abstract

It is increasingly possible for real-world agents, such as software-based agents
or human institutions, to view the internal programming of other such agents
that they interact with. For instance, a company can read the bylaws of
another company, or one software system can read the source code of another.
Game-theoretic equilibria between the designers of such agents are called
program equilibria, and we call this area open-source game theory.

In this work we demonstrate a series of counterintuitive results on open-
source games, which are independent of the programming language in which
agents are written. We show that certain formal institution designs that one
might expect to defect against each other will instead turn out to cooperate,
or conversely, cooperate when one might expect them to defect. The re-
sults hold in a setting where each institution has full visibility into the other
institution’s true operating procedures. We also exhibit examples and ten
open problems for better understanding these phenomena. We argue that
contemporary game theory remains ill-equipped to study program equilibria,
given that even the outcomes of single games in open-source settings remain
counterintuitive and poorly understood. Nonetheless, some of these open-
source agents exhibit desirable characteristics—e.g., they can unexploitably
create incentives for cooperation and legibility from other agents—such that
analyzing them could yield considerable beneﬁts.

Keywords: open-source game theory, program equilibria, commitment
games
PACS: 0000, 1111

Email address: critch@eecs.berkeley.edu (Andrew Critch)

Preprint submitted to arXiv.

August 16, 2022

 
 
 
 
 
 
2000 MSC: 0000, 1111

1. Introduction

Numerous sectors of the global economy are positioned for rapid advance-
ments in automation, due to recent progress in artiﬁcial intelligence and ma-
chine learning [23, 49, 38, 72, 35]. In fact, many experts speculate that AI
technology will eventually outperform humans on a very wide variety of tasks
[55, 10, 32]. This potentiality means it is important to understand how auto-
mated decision-making systems can interact, especially in cases where their
behavior might come as a surprise to their designers, such as in the stock
market “ﬂash crash” of 2010 [48].

In addition, opportunities abound in the structuring of novel human in-
stitutions both for using and for governing AI technology. There is already a
great deal of academic interest in establishing policy and governance norms
around the development and deployment of AI [11, 31, 19, 25, 15]. Dafoe
et al. [20] speciﬁcally call for further attention to the risks and beneﬁts of
designing AI systems to cooperate with each other.

What body of game-theoretic research and intuitions should be inform-
ing the deliberation of policy-makers in the AI governance arena, or of AI
researchers in the development and deployment of software-based agents?
Numerous results from classical game theory, bargaining, and mechanism de-
sign will no doubt prove useful. However, as we shall see, novel possibilities
arise in strategic interactions between algorithms, some of which are coun-
terintuitive to humans, and some of which might shed new light on existing
institutional behaviors or provide new routes to advantageous outcomes.

In this paper, we demonstrate a series of counterintuitive results showing
that certain cooperative-seeming formal institution designs turn out to defect
against each other—or conversely, turn out to cooperate when seeming like
they would defect—in settings where each institution’s true operating polices
are visible to the other institution. We formalize each institution as an open-
source agent, i.e., a program whose source code is readable by other programs.
Source code provides a written speciﬁcation of how a program will behave;
it’s what a software developer writes to create a new application, and can be
seen as a formal model of the processes by which an institution will operate.
A natural starting point for analyzing open-source agents is the setting
of program equilibria introduced by Tennenholtz [68]. In this setting, given

2

a game G to be played by two agents (“programs” in the terminology of
Tennenholtz), we consider a higher-level game G′ being played by the de-
signers of those agents. In G′, each designer chooses an agent to play in the
game G, and each agent reads the other agent’s source code (as a string of
text) before choosing its action (or ‘strategy’) for G. Mathematically, a simi-
lar formalism is employed by Kalai et al. [42], where again, each agent (‘device
response function’ in their terminology) gets to read the other agent’s source
code (‘commitment device’) before choosing its action (‘strategy’). We call
this area open-source game theory.

While the frameworks of Tennenholtz and Kalai et al. diﬀer on when ex-
actly the designers are allowed to introduce randomization, in both settings
the agents are open-source. Tennenholtz mainly takes the perspective that
the open-source condition will be necessary for modelling the economic in-
teraction of real-world software systems via the Internet. This perspective
could become increasingly important as artiﬁcial intelligence progresses, or
if so-called “decentralized autonomous organizations” (DAOs) ever become
economically prominent [39, 14, 24]. On the other hand, Kalai et al. view
their formalism mainly as a means of representing commitments between
human institutions, such as in price competitions or legal contracts. In our
view, both potential applications of the open-source agent concept—to artiﬁ-
cial agents and to human institutions—are important. Another shortcoming
of Tennenholtz’ approach is that he invents a particular programming lan-
guage in which to write agents, which is not immediately applicable to agents
written in other programming languages or formalisms.

In this paper, we prove theorems about open-source agents that do not
depend on which programming language they’re written in, using the formal-
ism of mathematical proofs as applicable to arbitrary Turing machines. We
argue that the consequences of the open-source condition are, in short, more
signiﬁcant than previously made apparent in the game theory literature. In
particular, even the outcome of a single game between fully speciﬁed open-
source agents can be quite counterintuitive, as the theorems of this paper
It is possible, therefore, that the overlap between com-
will demonstrate.
puter science and game theory, with a helping of logic, could progress into
what von Neumann, E. Kalai, and Shoham would call “the third stage” of
scientiﬁc development, where theory is able to make accurate predictions
beyond what practitioners would intuitively expect [64]. Intriguingly, such
results may be necessary for historically earlier concepts from game theory,
such as equilibria, to be applicable in reality: without these results, the de-

3

signers of artiﬁcial agents cannot acquire an adequate understanding of those
agents to be able to foresee even a single game outcome, so it would make
little sense to model those designers as being in equilibrium with one another.
For this reason, we call for the study of open-source games in their own
right, irrespective of whether the designers of the agents in the games are
in equilibrium. Thus, the study of open-source game theory as construed
here is both broader and more fundamental than that of program equilibria.
We begin this study by examining interactions between very simple agents
who reason about each other using mathematical proofs and make decisions
based on that reasoning (Section 2). We show how results in mathematical
logic can be applied to resolve the outcomes of such games (Section 3) and
conjecture that perhaps these results may be reducible to a form more easily
applied to human institutions (Section 3.4).

We anticipate that the methods of this paper will be unfamiliar to many
readers in game theory and economics. Facing this diﬃculty may be in-
evitable if we wish to understand the game theory of artiﬁcial agents or to
learn from that theory in our modelling of human institutions. To encour-
age readers to think about this topic through fresh eyes, we include simple
workable examples alongside theorems and open problems, with the hope of
inspiring more interdisciplinary work spanning game theory, computer sci-
ence, and logic.

1.1. Related work

As discussed in the introduction, Tennenholtz [68] and Kalai et al. [42]
have previously examined the interaction of open-source agents and char-
acterized the equilibria between the designers of such agents in the form
of so-called “folk theorems.” Tennenholtz calls the open-source agents “pro-
grams” and Kalai et al. call them “device response functions.” Tennenholtz,
as we sometimes do, allows the agents to randomize at runtime, and does not
examine settings where the designers can randomize. By contrast, the equi-
librium concept of Kalai et al. assumes the agents are deterministic at run-
time whereas the designers are allowed to randomize in choosing their agents.
Kalai’s equilibrium concept also allows the designers to choose whether their
agents are open-source. In this paper, we focus away from designer equilibria
and toward understanding the details of what kinds of agents can be built
and how they interact.

Tennenholtz [68] also demonstrates a fairly trivial means of achieving a
mutually cooperative equilibrium between agent designers: by writing an

4

agent that checks if its opponent is exactly equal to itself, and cooperates
only in that case. LaVictoire et al. [47] remark that this cooperative crite-
rion is too fragile for practical use and put forward the idea that formal logic,
particularly Löb’s theorem and more generally Gödel-Löb provability logic,
should play a role in agents thinking about each other before making deci-
sions. For the case in which the agents are given inﬁnite time and space in
which to do their thinking, Barasz et al. [3] exhibit a ﬁnite-time algorithm for
ascertaining the outcomes of games between these (necessarily physically im-
possible) inﬁnitely resourced agents, called modal agents because of their use
of modal logic. Currently it remains unclear to the present authors whether
existing results in logic are truly adequate to establish bounded versions of
all of the modal agents described by Barasz et al. [3] and LaVictoire et al.
[47]; it is, however, a highly promising line of investigation.

Oesterheld [59] exhibits a mutual simulation approach to cooperation in
an open-source setting and argues that this approach is more computationally
eﬃcient than formally verifying properties of the opponent’s program using
proofs, as we do in this paper. However, as Section 4.2 will elaborate, mutual
program veriﬁcation can be made more eﬃcient than mutual simulation, by
designing the veriﬁcation strategy to prioritize hypotheses with the potential
to collapse certain loops in the metacognition of the agents.

Halpern and Pass [34] and Capraro and Halpern [12] have shown how
partial transparency, or “translucency”, between players is more representa-
tive of real-world interactions between human institutions and creates more
opportunity for pro-social outcomes. Their results assume a state of common
knowledge in which the agents are known to be rational and counterfactually
rational, which may be predictably unrealistic for present-day systems and
institutions. However, the normative appeal of their results could be used to
motivate the design of more translucent and counterfactually rational enti-
ties.

Loosely speaking, revision games [43] and mechanism games [74, 62, 61]
are settings in which some players make plans or commitments visible to
others. Since plans and sometimes commitments can be written as programs,
results from studying interactions between open-source agents might reveal
interesting new strategies for revision games and mechanism games.

5

2. Setup

In this paper, agents will make decisions in part by formally verifying cer-
tain properties of each other, i.e., by generating mathematical proofs about
each other’s source code. We’ll call such agents formal veriﬁer agents.

For ease of exposition, pseudocode for the agents will be written in Python
because it is a widely known programming language. However, we emphasize
that unlike Tennenholtz [68], we are not inventing a programming language
for representing agents. Rather, we analyze agents (and describe agents that
analyze each other) using a proof-based approach, because proofs can be
written that are independent of the particular programming language used to
create the agents. The agents could even be written in diﬀerent programming
languages altogether, as long as they are Turing machines. Still, empirical or
exploratory research in this area will probably be more eﬃcient if the agents
are written in a programming language more speciﬁcally designed for formal
veriﬁability, such as HOL/ML [56, 45] or Coq [4, 13], but we emphasize that
our results do not depend on this.

Below is a simple agent, called CB for “CooperateBot”, who simply ignores
its opponent’s source code opp_source and returns C for “cooperate”. “De-
fectBot” is the opposite, returning D for “defect”. For any agent A, A.source
will carry a valid copy of the agent’s source code, like this:

# "CooperateBot":
def CB(opp_source):

return C
CB.source = """
def CB(opp_source):

return C

"""

# "DefectBot":
def DB(opp_source):

return D
DB.source = """
def DB(opp_source):

return D

"""

6

To save space when deﬁning subsequent agents, we’ll avoid writing out
the full agent.source deﬁnition in this document, since it’s always just a
copy of the lines directly above it.

The outcome of a game is deﬁned by providing each agent’s source code

as input to the other:

# Game outcomes:
def outcome(agent1, agent2):

return (agent1(agent2.source),agent2(agent1.source))

To get warmed up to thinking about these agents, and to check that our
deﬁnitions are being conveyed as intended, we ask the reader to work through
each of the short examples posed throughout this paper. Later examples will
turn out to be open problems, but we’ll start with easier ones:

Example 2.1. [easy] What is outcome(CB,DB)? (answered in footnote1)

2.1. Proof-searching via proof-checking

In this paper, our goal is to look at agents who perform some kind of
check on the opponent before deciding whether to cooperate or defect. For
this, let’s suppose the agents are equipped with the ability to read and write
formal proofs about each other’s source code using a formal proof language,
such as Peano Arithmetic or an extension thereof. Peano Arithmetic and
its extensions are useful because they allow proofs about programs written
in arbitrary programming languages, by representing arbitrary computable
functions in a mathematical form [17].
In this way, we move past what
might have seemed like a limitation of previous works on program equilibria,
which invented and employed domain-speciﬁc programming languages for
representing agents [68, 47].2 Speciﬁcally, we will assume the agents can

1CB(DB) returns C because CB always returns C by deﬁnition; similarly DB(CB) returns

D. Hence the outcome is the pair (C,D).

2The perception that program equilibrium results require agents to be written in
domain-speciﬁc languages work was remarked by several earlier reviewers of the present
draft. The issue of whether it’s “obvious” that the programming language requirements
can be broadly generalized seems to depend starkly on the reader. In any case, in light of
the programming-language-independent results presented here and by Critch [18], earlier
works [68, 47] can be argued to be more generalizable than they may have seemed at the
time.

7

invoke a function called proof_checker which, given a source code for an
opponent agent (encoded as a string), can check whether a given argument
(encoded as a string, referring to the opponent as “opp”) is a valid proof of a
given proposition (also encoded as a string) about the opponent:

def proof_checker(opp_source, proof_string, hypothesis_string):

# This is the only function in this paper that won’t be
Our assumption is that it
# be written out explicitly.
# checks if proof_string encodes a logically valid
# proof that hypothesis_string is a true statement about
# the opponent defined by opp_source.
# can refer to the opponent as a function (not just its
# source) as ’opp’.
If the hypothesis_string is a valid
# proof about that function, this function returns True;
# otherwise it returns False. For detailed assumptions on
# the proof system, see (Critch, 2019).

The hypothesis_string

...

Such proof-checking functionalities are available in programming lan-
guages such as HOL/ML [56, 45] and Coq [4, 13] that are designed to enable
formal veriﬁability, but in principle formal veriﬁers can be built for any pro-
gramming language, including probabilistic programming languages. HOL
in particular has been used to formally verify proofs involving metamathe-
matics and modal logic [60, 6], which might make it particularly suited to
the kinds of proofs in this paper. Some basic eﬃciency assumptions on the
proof system are laid out in Appendix B.

When trying to prove or disprove a given hypothesis about an opponent,
how will our agents search through potential proofs? For simplicity, we as-
sume a trivial search method: the agents will search through ﬁnite strings in
alphabetical order, and check each string to see if it is a proof. Real-world
formal veriﬁers use heuristics to more eﬃciently guide their search for proofs
or disproofs of desired properties; we expect our results will apply in those
settings as well. For concreteness, and to eliminate any ambiguity as to what
is meant in our examples and results, we exhibit the code for proof searching
below:

# string_generator iterates lexicographically through all
# strings up to a given length; see appendix:
from appendix import string_generator

8

# proof_search checks each generated proof string to see
# if it encodes a valid proof of a given hypothesis:
def proof_search(length_bound, opp_source, hypothesis):

for proof in string_generator(length_bound):

if proof_checker(opp_source, proof, hypothesis):

return True

# otherwise, if no valid proof of hypothesis is found:
return False

2.2. Basic formal veriﬁer agents

A proof_search function like the one above can be used to design more
interesting agents, who try for a while to formally verify properties about
each other before deciding to cooperate or defect.

The following class of agents are called CUPOD for “Cooperate Unless Proof
of Defection”, deﬁned by a function CUPOD that constructs an agent CUPOD(k)
for each k ≥ 0. Speciﬁcally, CUPOD(k) is a particular agent that behaves as
follows: CUPOD(k)(opp_source) searches for a proof, in k characters or less,
that the opponent opp is going to defect against CUPOD(k).
If a proof of
defection is found, CUPOD(k) defects; otherwise it “gives the beneﬁt of the
doubt” to its opponent and cooperates:

# "Cooperate Unless Proof Of Defection" (CUPOD):
def CUPOD(k):

def CUPOD_k(opp_source):

if proof_search(k, opp_source, "opp(CUPOD_k.source) == D"):

return D

else:

return C

CUPOD_k.source = ... # the last 5 lines with k filled in
return CUPOD_k

Next we have DUPOC, for “Defect Unless Proof of Cooperation", the mirror
image of CUPOD. DUPOC(k) will defect unless it ﬁnds aﬃrmative proof, in k
characters or less, that its opponent is going to cooperate:

# "Defect Unless Proof Of Cooperation" (DUPOC):
def DUPOC(k):

9

def DUPOC_k(opp_source):

if proof_search(k, opp_source, "opp(DUPOC_k.source) == C"):

return C

else:

return D

DUPOC_k.source = ... # the last 5 lines with k filled in
return DUPOC_k

3. Results, Part I

Our results are easiest to explain in the context of examples, and some
of our open problems are easiest to explain in the context of results, so we
deliver them all together.

3.1. Basic properties of CUPOD and DUPOC

Let us ﬁrst examine some basic properties and examples of interactions
with formal veriﬁer agents, as a foundation for understanding the more dif-
ﬁcult results.

Example 3.1. [medium] What is outcome(CUPOD(10), DB)? (answered in
footnote3)

Example 3.2. [medium] What is outcome(CUPOD(10^9), DB)? Here 10^9
denotes 109. (answered in footnote4)

These examples show the role of the length bound k. Next, let us observe
the following interesting property of CUPOD(k): that it never defects on an
opponent unless that opponent “deserves it” it in a certain sense:

3CUPOD(10)(DB.source) searches for a proof, using 10 characters or less, that
DB(CUPOD(10).source) == D. While it is true that DB(CUPOD(10).source) == D, the
proof of this fact will take more than 10 characters of text to write down, so CUPOD(10)
won’t ﬁnd the proof and will fall back on its default action. Hence, the outcome is (C,D).
4The fact that DB(CUPOD(10^9).source) == D is relatively easy to prove, via writing
down a proof that DB always returns D; a hardworking undergrad could probably write out
a fully rigorous proof by hand. The length of the proof depends a bit on the speciﬁcs of
the proof language we use to write down the proof, but for any reasonable proof language
the proof shouldn’t take more than a billion characters (around 200 thousand pages!). So,
CUPOD(10^9)(DB.source) will ﬁnd the proof and return D, yielding the outcome (D,D).

10

Proposition 3.1. CUPOD(k) never exploits its opponent. That is, for all
opponents opp and all k, outcome(CUPOD(k),opp) is never (D,C).

Proof. If CUPOD(k)(opp.source)==D, it must be because CUPOD(k) man-
aged to prove that opp(CUPOD(k).source)==D. Under the assumption that
the proof_check function employed by CUPOD(k) is sound, it must be that
opp(CUPOD(k).source)==D, so the outcome cannot be (D,C).

Dually, we have the following:

Proposition 3.2. DUPOC(k) is never exploited by its opponent. That is, for
all opponents opp and all k, outcome(DUPOC(k),opp) is never (C,D).

Proof. The proof is the same as for Proposition 3.1, with C and D switched.

3.2. CUPOD vs CUPOD: What happens?

Now let us examine our ﬁrst example interaction between two open-source

agents who both who both employ formal veriﬁers:

Example 3.3. [diﬃcult] What is outcome(CUPOD(k), CUPOD(k)) when we
set k = 1012?

We respectfully urge the reader to work through the earlier examples
before attending seriously to Example 3.3, to understand how the proof
length bound can aﬀect the answer. These nuances represent a key feature
of bounded rationality: when an agent thinks for a while about a hypothesis
and reaches no conclusion about it, it still takes an action, if only a “null” or
“further delay” action.

The next step is to notice what exactly CUPOD(k)(CUPOD(k).source)
is seeking to prove and what it will do based on that proof. Speciﬁcally,
CUPOD(k)(CUPOD(k).source) is searching for a proof in k characters or
less that CUPOD(k)(CUPOD(k).source) == D, and if it ﬁnds such a proof,
it will return D. Thus, we face a kind of circular dependency: the only way
CUPOD(k)(CUPOD(k).source) will return D is if it ﬁrst ﬁnds a proof in fewer
than k characters that CUPOD(k)(CUPOD(k).source) will return D!

Common approaches to resolving circularity. How does this cir-
cular thinking in the CUPOD(k) vs CUPOD(k) interaction resolve? Questions
of this sort are key to any situation where two agents reasoning about each

11

other interact, as each agent is analyzing the other agent analyzing itself an-
alyzing the other agent [...]. Below are some common approaches to resolving
how this circularity plays out for the CUPOD(k)s.

Approach 1 is to look for some kind of reduction to smaller values of k, as
one might do when studying an iterated game. But this game is not iterated;
it’s a one-shot interaction. CUPOD(k)(CUPOD(k).source) will base its coop-
eration or defection on whether it proves CUPOD(k)(CUPOD(k).source) == D,
not on the behavior of any CUPOD(j) for any smaller j < k. This inability
to reduce to smaller k values might suggest the problem is in some sense
intractable. However, from the outside looking in, if the problem is so in-
tractable that no short proof can answer it, then the result will have to be
C, because that’s what happens when no shorter-than-k proof of D can be
found.

Approach 2 is to wonder if both C and D are possible answers. One can
imagine the answer being C, so the algorithm ﬁnds a proof of D and therefore
returns C, or the answer being D, so algorithm ﬁnds no proof of D and
therefore returns C. However, the programs involved here are deterministic,
and have a bounded runtime! They must halt and return something. So what
do they return, C or D? Again, if one cannot write down a shorter-than-k
proof of either answer, the answer must be C.

Approach 3 is to envision the circularity unrolling into a kind of stack over-
ﬂow: for defection to occur, CUPOD(k)(CUPOD(k).source) must prove that
CUPOD(k)(CUPOD(k).source) proves that CUPOD(k)(CUPOD(k).source) proves
that. . . . Such a proof would seem to be inﬁnite in length, and thus would not
ﬁt within any ﬁnite length bound k. This way of thinking gives an intuitive
reason for why the answer should be C: the proof search for defection will
ﬁnd nothing, hit the length bound k, and then CUPOD(k)(CUPOD(k).source)
will return C:

No-proof conjecture: CUPOD(k)(CUPOD(k).source) == C, because there

exists no ﬁnite-length-proof that CUPOD(k)(CUPOD(k).source) == D.

3.3. Self-play results

The ﬁrst surprise of this paper is that the no-proof conjecture directly
above is false. Because of a computationally bounded version of a theorem in
logic called Löb’s theorem [18], a fairly short proof that CUPOD(k)(CUPOD(k).
source) == D in fact does exist, and as a result CUPOD(k)(CUPOD(k).source)
ﬁnds the proof and defects because of it. In other words, each of Approaches
1-3 for resolving the circular dependency in the previous section is incorrect:

12

Theorem 3.4. For k large, outcome(CUPOD(k),CUPOD(k)) == (D,D).

The proof of this theorem, and several others throughout this paper, will
make use of the lemma below, which depends on the following notation:

Notation 3.5 (S, ⊢, (cid:3), and ≻).

• S stands for the formal proof system being used by the agents; see

Appendix B for more details about it.

• ⊢ xyz... means “the statement ‘xyz...’ can be proved using the proof

system S.

• (cid:3)k(xyz...) means “a natural number exists which, taken as a string,
encodes a proof of xyz..., and that proof when written in the proof
language of S requires at most k characters of text.”

• f (k) ≻ O(lg k) means there is some positive constant c > 0 and some

threshold ˆk such that for all k > ˆk, f (k) > c lg k.

Lemma 3.6 (PBLT: Parametric Bounded Löb Theorem). Let p[k]
be a formula with a single unquantiﬁed variable k in the proof language of
the a proof system S, satisfying the conditions in Appendix B. Suppose that
k1 ∈ N and f : N → N is an increasing computable function satisfying
f (k) ≻ O(lg k), and S can verify that the formula p[k] is “potentially self-
fulﬁlling for large k” in the sense that

⊢ (∀k > k1)(cid:0)(cid:3)f (k)(p[k]) → p[k](cid:1).

Then there is some threshold k2 ∈ N such that

⊢ (∀k > k2)(p[k]).

Proof of Lemma 3.6. This is a special case of Critch [18, Theorem 4.2] where
the proof expansion function e(k) is in O(k). Speciﬁcally, in this paper we
have assumed (see Appendix B) that a proof can be expanded and checked
in time linear in the length of the proof, i.e., e(k) = e∗ · k for some constant
e∗. This assumption makes the condition f (k) ≻ O(lg k) suﬃcient to apply
Critch [18, Theorem 4.2], as described by Critch [18, Section 4.2].

Proof of Theorem 3.4. This follows directly from PBLT, via the substitutions
p[k]=(CUPOD(k)(CUPOD(k).source) == D), f (k) = k, and k1 = 0.

13

On the ﬂip side of Theorem 3.4, we also have the following symmetric

result:

Theorem 3.7. For k large, outcome(DUPOC(k),DUPOC(k)) == (C,C).

Proof. This follows directly from PBLT, via the substitutions
p[k] =((DUPOC(k)(DUPOC(k).source) == C), f (k) = k, and k1 = 0.

This theorem may be equally surprising to Theorem 3.4: most computer
science graduate students guess that the answer is D when asked to predict
the value of DUPOC(k)(DUPOC(k).source), even when given a few minutes
to reﬂect and discuss with each other.5 These guesses are almost always on
the basis of one of the three misleading thinking approaches in the previous
section, most commonly Approach 3, which expects the proof search to fail
for stack-overﬂow-like reasons.

3.4. Intuition behind the proofs of Theorems 3.4 and 3.7

Exactly what mechanism allows DUPOC(k) to avoid these circularity and

stack overﬂow problems?

A key feature of the proof of Löb’s theorem, and PBLT which underlies
Theorems 3.4 and 3.7, is the ability to a construct a statement that in some
sense refers to itself. This self-reference ability allows the proof to avoid the
stack overﬂow problem one might otherwise expect. In broad brushstrokes,
the proof of PBLT follows a similar structure to the classical modal proof
of Löb’s theorem [22], which can be summarized in words for the case of
Theorem 3.7 as follows:

1. We construct a sentence Ψ that says “If this sentence is veriﬁed, then
the agents will cooperate,” using a theorem in logic called the modal
ﬁxed point theorem.

2. We show that if Ψ can be veriﬁed by the agents, then mutual cooper-
ation can also be veriﬁed by the agents, without using any facts about
the agents’ strategies other than their ability to ﬁnd proofs of a certain
length.

5Across several presentations to a total of around 100 computer science graduate stu-
dents and faculty, when the audience is given around 5 minutes to discuss in small groups,
around 95% of attendees conjecture that the proof search would run out and the program
would return D.

14

3. We use the fact that verifying mutual cooperation causes the agents to

cooperate, to show that Ψ is true.

4. Finally, we use the above proof of Ψ to construct a formal veriﬁcation

of Ψ, which implies (by Ψ!) that cooperation occurs.

This explanation, and the underlying mathematical proof, may be some-
what intuitively unsatisfying, because the meaning of the sentence Ψ is some-
what abstract and diﬃcult to relate directly to the agents. Nonetheless, the
result is true. Moreover, we conjecture that a more illuminating proof may
be possible:

Deﬁnition 3.8. Let S be the formal (proof) system used by proof_checker.
⊢ X means “the proof system S can prove X”, and (cid:3)X means “a natural
number exists which encodes (via a Gödel encoding) a proof of X within the
proof language and rules of S.”.

Open Problem 1. Löb’s theorem states that ⊢ ((cid:3)C → C) implies
⊢ (C). We conjecture that Löb’s Theorem can be proven without the
use of the modal ﬁxed point Ψ ↔ ((cid:3)Ψ → C), by constructing an entire
proof that refers to itself, according to the following intuitive template:

1. This proof is a proof of C.

2. Therefore (cid:3)C.

3. By assumption, (cid:3)C → C.

4. Therefore, by (2) and (3), C.

Is such a proof possible? If not, why not?

Such a proof would be suggestive of the following argument for one-shot
cooperation between institutions, assuming each institution is DUPOC-like,
in that it has already adopted internal policies, culture and personnel that
will make it cooperate if it knows the other institution is going to cooperate:

15

Cooperative aﬃdavit for DUPOC-like institutions:

Institutions A and B have each recently undergone structural develop-
ments to prepare for cooperating with each other. Moreover, represen-
tatives from each institution have thoroughly inspected the other insti-
tution’s policies, culture, and personnel, and produced the attached in-
spection records with our ﬁndings, eﬀectively rendering A and B “open-
source” to one another. These records show a readiness to cooperate
from both institutions. Moreover, the records are suﬃcient supporting
evidence for the following argument:

1. This signed document and the attached records constitute a self-
evident (and self-fulﬁlling) prediction that Institutions A and B are
going to cooperate.

2. Members of Institutions A and B can all read and understand this
document and attached records, and can therefore tell that the
other institution is going to cooperate.

3. Institution A’s internal policies and culture are such that, upon
concluding that Institution B is going to cooperate, Institution A
will cooperate. The same is true of Institution B’s policies and
culture with regards to Institution A.

4. Therefore, by (2) and (3), the Institutions A and B are going to

cooperate.

Points (1)-(4) of this “cooperative aﬃdavit” are manifestly a kind of cir-
cular argument. However, the lesson of PBLT is that certain kinds of circular
arguments about bounded reasoners can be logically valid. In this case, most
of the “work” toward achieving cooperation is carried out in each institution’s
adoption of DUPOC-like internal culture and policies (“we cooperate once we
know they’re going to cooperate”), and in the furnishing of mutual inspection
records adequate to create common knowledge of that fact. Once that much
is done, the validation of the circular argument for cooperation becomes both
logically feasible and suﬃcient to trigger cooperation.

It is also interesting to note that the above “cooperative aﬃdavit” is not a
contract; it is merely a signed statement of fact made valid by each company’s
DUPOC-like internal policies and culture. As such, the document would add

16

no mechanism of enforcement to ensure cooperation; it would simply trigger
each institution’s internal policies and culture to take eﬀect in a cooperative
manner, assuming those policies and culture are suﬃciently DUPOC-like. In
Section 6.3 we will discuss this prospect further in the context of existing
literature spanning social psychology, economics, and education.

Finally, we remark that a similar phenomenon could result in CUPOD-like
institutions turning out to defect against each other. In other words, when
two institutions are cooperating primarily on the basis of failing to under-
stand each other, but stand ready to defect if they gain enough information
about the counterparty to conﬁdently predict defection, then a self-fulﬁlling
prophecy of defection can arise as soon as the institutions believe they have
become suﬃciently “open-source”.

Thus, CUPOD-like institutions could turn out to defect, and DUPOC-like
institutions can turn out to cooperate, as a result of self-fulﬁlling prophecies
analogous to Theorems 3.4 and 3.7. This suggests that institutions who are
going to (intentionally or inadvertently) reveal their inner workings should
consider ﬁrst shifting from CUPOD-like policies and culture to DUPOC-like
policies and culture.

3.5. Further properties of DUPOC(k)

Another counterintuitive observation is that CUPOD(k) seems “nicer” than
DUPOC(k), since by Propositions 3.1 and 3.2, CUPOD(k) never exploits its
opponent, while DUPOC(k) is more “defensive” in that it never allows it-
self to be exploited. This observation has given many readers the intuition
that CUPOD(k) would have an easier time achieving mutual cooperation than
DUPOC(k), but by Theorems 3.4 and 3.7, the opposite turns out to be true.
In fact, the agents DUPOC(k) for large values of k — henceforth “DUPOCs”
— have some very interesting and desirable game-theoretic properties:

1. (unexploitability) A DUPOC never cooperates with an opponent
who is going to defect, i.e., it never receives the outcome (C,D) in a
game; only (D,D), (D,C), or (C,C).

2. (broad cooperativity) DUPOCs achieve (C,C) not just with other
DUPOCs, but with a very broad class of agents, as long as those agents
follow a certain basic principle of “G-fairness” described in Critch [18].
Roughly speaking, if an opponent always cooperates with agents who
exhibit legible (easy-to-verify) cooperation, then DUPOCs cooperate
with that opponent.

17

3. (rewarding legible cooperation) In order to achieve (C,C) with a
DUPOC, the opponent must not only cooperate, but legibly cooperate,
i.e., cooperate in a way that the DUPOC is able to verify with its
bounded proof search. So, if there are opponents around that are made
of “spaghetti code” that the DUPOCs are unable to eﬃciently analyze,
those opponents won’t be rewarded.

4. (rewarding rewarding legible cooperation) Moreover, by item 2
above, the presence of DUPOCs in a population also rewards rewarding
legible cooperation, by cooperating with all agents who do it.

Because of these points, one can imagine DUPOCs having a powerful

eﬀect on any population that contains them:

Open Problem 2. Determine conditions under which a population con-
taining DUPOCs will evolve into a population where all agents reward
legible cooperation, i.e., are G-fair in the sense deﬁned by Critch [18].

Note that a DUPOC does not cooperate with every agent who has some

tendency to cooperate; in fact we have the following open problem:

Open Problem 3. For large values of k, we conjecture that
outcome(DUPOC(k),CUPOD(k))==(D,C). Is this the case?

A natural approach would be to ar-
Why this problem is challenging.
gue by symmetry that the result must be (C,D) or (D,C), and rule out
(C,D) on the grounds that DUPOC(k) is unexploitable. However, the pro-
posed symmetry argument isn’t quite valid: DUPOC(k) is not a perfect mirror
image of CUPOD(k), because the letters C and D are not switched inside the
proof_checker subroutine. It seems we need some way to reason about one
agent’s proof search running out, while the other agent is unable to prove
(cid:4)
that the ﬁrst agent’s proof search runs out.

4. Generalizability of results

4.1. Nondeterministic interactions

A common question to ask about these results is whether they depend
on the ‘rigidity’ of logic in some way. The answer is, roughly speaking, no.

18

To see this, let us examine a nondeterministic version of DUPOC, using a
"Prob" symbol to write proofs about the probability of random events. This
agent tries to show that the opponent cooperates with probability at least q,
and if successful, cooperates with probability q:

# Probabilistic DUPOC (PDUPOC):
def PDUPOC(K,Q):

def PDUPOC_k(opp_source):

k = K; q = Q
if proof_search(k, opp_source,
"Prob(’opp(DUPOC_k.source) == C’ >= q")):

if random.uniform(0,1) <= q:

return C

else:

return D

PDUPOC_k.source = ... # the last 8 lines with K,Q filled in
return DUPOC_k

Theorem 4.1. For large k and q ≥ 0.5, with probability at least 2q − 1,
outcome(PDUPOC(k,q),PDUPOC(k,q))==(C,C). In particular, for q ≈ 1, the
probability of mutual cooperation is also ≈ 1.

Proof. This follows directly from PBLT, using the substitutions
p[k] =(Prob('PDUPOC(k,q)(PDUPOC(k,q).source)==C') >= q), f (k) = k,
and k1 = 0. We use the fact that two events with probability q must have
a conjunction with probability at least 2q − 1. If the sources of randomness
are known to be independent, the bound 2q − 1 can be increased to q2.

4.2. More eﬃcient proof_search methods

For simplicity of analysis, we assumed that our proof_search method
searches alphabetically through arbitrary strings until it ﬁnds a proof, which
is terribly ineﬃcient, taking exponential time in the length of the proof. This
problem has been noted as potentially prohibitive [59].

However, much more eﬃcient strategies are possible. As a trivial example,
if string_generator is reconﬁgured to output, as its ﬁrst string, the proof of
Theorem 3.7, then both agents will terminate their searches in a tiny fraction
of a second and cooperate immediately! Indeed, the proof of PBLT makes no
use of the order in which potential proofs are examined. In reality, the details

19

of the proofs employing PBLT will need to vary based on the particularities of
the implementations of each agent, so heuristic searches such as those used
to complete proof goals in hybrid proof-and-programming languages such
as HOL [56, 45] and Coq [4, 13] will likely be crucial to automating these
strategies in real-world systems. And of course, there is a trade-oﬀ between
ﬂexibility on implementation standards and speed of veriﬁcation. This poses
an interesting research project:

Open Problem 4. Implement DUPOC using heuristic proof search in
HOL/ML or Coq. Can outcome(DUPOC(k),DUPOC(k)) run and halt with
mutual cooperation on a present-day retail computer? We conjecture the
answer is yes. If so, how much can the implementations of the two agents
be allowed to vary while cooperative halting is preserved?

5. Results, Part II: Strategies that verify conditionals

5.1. Rewarding conditional cooperation: CIMCIC

Notice how a DUPOC’s strategy is to prove that the opponent is going
to cooperate unconditionally. In other words, a DUPOC doesn’t check “If
I cooperate, then the opponent cooperates”; rather, it checks the stronger
statement “The opponent cooperates”. The following is the “conditional co-
operation” version of DUPOC:

# "Cooperate If My Cooperation Implies Cooperation
# the opponent" (CIMCIC):
def CIMCIC(k):

from

def CIMCIC_k(opp_source):

if proof_search(k, opp_source,
"(CIMCIC_k(opp.source)==C) => (opp(CIMCIC_k.source)==C)"):

return C

else:

return D

CIMCIC_k.source = ... # the last 6 lines with k filled in
return CIMCIC_k

Proposition 5.1. CIMCIC is unexploitable, i.e., outcome(CIMCIC(k),opp)
is never (C,D) for any any opponent.

20

Proof. This proof works the same as that of Proposition 3.1. Speciﬁcally,
if CIMCIC(k)(opp.source)==C, it must be because CIMCIC(k) managed to
prove CIMCIC(k)’s cooperative criterion, namely

(opp(CIMCIC(k).source)==C) => (opp(CIMCIC_k.source)==C).

Under the assumption that the proof_check function employed by CIMCIC(k)
is sound, this implication must hold, implying opp(CIMCIC(k).source)==C,
so the outcome cannot be (C,D).

Example 5.1. Evaluate the following (answered below):

a) [medium] Outcome(CIMCIC(k),CIMCIC(k))

b) [diﬃcult] Outcome(DUPOC(k),CIMCIC(k))

Since CIMCIC’s cooperative criterion is conditional, one can imagine two
CIMCICs being in a kind of stand-oﬀ, with each one thinking “I’ll cooperate if
I know it will imply you cooperate”, but never achieving cooperation because
there are no provisions in its code to break the cycle of “I’ll do it if you will”
reasoning. However, the following theorem shows that, by PBLT, no such
provision is necessary:

Theorem 5.2. For large k,

a) outcome(CIMCIC(k),CIMCIC(k)) == (C,C)

b) outcome(DUPOC(k),CIMCIC(k)) == (C,C)

Proof of (a). A short proof of the outcome (C,C) will lead, in a few addi-
tional lines comprising some number of characters c, to a proof of the material
implication

"(CIMCIC_k(CIMCIC_k.source)==C)=>(CIMCIC_k(CIMCIC_k.source)==C)"

This in turn will cause the agents to cooperate, bringing about the outcome
(C,C). Thus, we have a “self-fulﬁlling prophecy” situation of the kind where
PBLT can be applied. Speciﬁcally, if we let f (k) = ⌊k/2⌋, k1 = 2c, and p[k]
be the statement

"outcome(CIMCIC(k),CIMCIC(k))==(C,C)"

then the conditions of PBLT are satisﬁed. Therefore, for some constant k2,
we have ⊢ (∀k > k2)(p[k]).

21

Proof of (b). Again, a short proof of the outcome (C,C) will lead, in a few
additional lines comprising some number of characters c, to a proof of CIM-
CICs’ cooperation condition, namely

"(CIMCIC_k(DUPOC_k.source)==C)=>(DUPOC_k(CIMCIC_k.source)==C)"

as well as DUPOC’s cooperation condition,

"CIMCIC(k)(DUPOC(k).source)==C"

Thus, letting f (k) = ⌊k/2⌋, k1 = 2c, and p[k] be the statement
"outcome(DUPOC(k),CIMCIC(k))==(C,C)"

satisﬁes the conditions of PBLT. Therefore, for some constant k2, we have
⊢ (∀k > k2)(p[k]).

Open Problem 5. What is outcome(CUPOD(k),CIMCIC(k))?

Why this problem is challenging. Write CC, CD, DC, and DD as short-
hand for (C,C), (C,D), (D,C), and (D,D) respectively, and write C∗ for
“CC or CD” and ∗C for “CC or DC”. We know that the outcome DC is
impossible, because CUPOD(k) never exploits its opponent. However, the
other possibilities are diﬃcult to prove or rule out. If the outcome is CC,
it means CIMCIC(k) manages to prove mutual cooperation, which is consis-
tent with CUPOD(k) failing to prove defection. If the outcome is DD, then
it means CUPOD(k) manages to prove that CIMCIC(k) defects, but somehow
CIMCIC(k) fails to ﬁnd a short proof of the fact that CUPOD(k) proves that
CIMCIC(k) defects. Otherwise CIMCIC(k) could prove the material condi-
tional ∗C → C∗ with just a few more lines, which would cause it to coop-
erate, a contradiction. If the outcome is CD, it means both agents’ proof
It’s not immediately clear which of these outcomes will
searches run out.
(cid:4)
actually obtain.

5.2. Preempting exploitation: DIMCID

Another interesting strategy to consider is to preempt exploitation by

defecting if the opponent would exploit you:

# "Defect if My Cooperation Implies Defection from
# the opponent (DIMCID):
def DIMCID(k):

def DIMCID_k(opp_source):

22

if proof_search(k, opp_source,
"(DIMCID_k(opp.source)==C) => (opp(DIMCID_k.source)==D)"):

return D

else:

return C

DIMCID_k.source = ... # the last 6 lines with k filled in
return DIMCID_k

Example 5.3. [easy] Evaluate the following (answered in footnote6):

a) outcome(DIMCID(k), CB)

b) outcome(DIMCID(k), DB)

Example 5.4. [medium] Evaluate the following (answered below):

a) outcome(DIMCID(k), DIMCID(k))

b) outcome(CUPOD(k), DIMCID(k))

Theorem 5.5. For large k,

• outcome(DIMCID(k), DIMCID(k)) == (D,D)

• outcome(CUPOD(k), DIMCID(k)) == (D,D).

Proof of (a). A short proof of the outcome (D,D) will lead, in a few addi-
tional lines comprising some number of characters c, to a proof of the material
implication

"(DIMCID_k(DIMCID_k.source)==C)=>(DIMCID_k(DIMCID_k.source)==D)"

which in turn will cause the agents to defect, bringing about the outcome
(D,D). Thus, we have a “self-fulﬁlling prophecy” situation of the kind where
PBLT can be applied. Speciﬁcally, if we let f (k) = ⌊k/2⌋, k1 = 2c, and p[k]
be the statement

"outcome(DIMCID(k),DIMCID(k))==(D,D)"

6(C,C) and (D,D), respectively.

23

then the conditions of PBLT are satisﬁed. Therefore, for some constant k2,
we have ⊢ ∀k > k2, p[k].

Proof of (b). Again, a short proof of the outcome (D,D) will lead, in a few
additional lines comprising some number of characters c, to a proof of DIM-
CID’s defection condition, namely
"(DIMCID_k(CUPOD_k.source)==C)=>(CUPOD_k(DIMCID_k.source)==D)"
as well as CUPOD’s defection condition,
"DIMCID(k)(CUPOD(k).source)==D"
Thus, letting f (k) = ⌊k/2⌋, k1 = 2c, and p[k] be the statement
"outcome(CUPOD(k),DIMCID(k))==(D,D)"
satisﬁes the conditions of PBLT. Therefore, for some constant k2, we have
⊢ (∀k > k2)(p[k]).

Open Problem 6. What is outcome(DUPOC(k),DIMCID(k))?

This problem is very similar to Open
Why this problem is challenging.
Problem 5: one of the agent’s proof searches might fail while the other suc-
ceeds. If so, which way does it go? If both proof searches fail, why is that?
(cid:4)

6. Discussion

6.1. Applicability to non-logical reasoning processes

Must the agents in these theorems form their beliefs using logical proofs,
or would the same results hold true if they employed other belief-forming
procedures, such as machine learning? What abstract properties of these
proofs and proof systems are key to applying PBLT in Theorems 3.4, 3.7,
4.1, 5.2, and 5.5?

The following features of the agents’ reasoning capabilities are key to the

proof of PBLT, and hence to the results of this paper:

1. The proof language for representing the agents’ beliefs needs to be
expressive enough to represent numbers and computable functions and
to introduce and expand abbreviations.

2. There must be a process the agents can follow for deriving beliefs from

other beliefs (writing a proof is such a process).

24

3. The proof language must also be able to represent belief derivations
(e.g., proofs) as objects in some way, and refer to the cost (e.g., in time
or space) of producing various belief derivations.

Agents lacking some of these reasoning capabilities should not be ex-
pected to behave in accordance with the theorems of this paper. However,
these capabilities are all features of general intelligence, in the sense that a
reasoning process that lacks these capabilities can be made more deductively
powerful by adding them. Thus, there is some reason to expect that agents
designed to exhibit highly general reasoning capabilities may be amenable to
these results. The task of formalizing highly general deﬁnitions of “beliefs”
and “belief derivations” could itself make for interesting follow-up work.

6.2. Further open problems

As mentioned earlier in Section 1.1, Barasz et al. [3] exhibit a ﬁnite-
time algorithm written in Haskell for settling interactions between agents
that write proofs about each other, which unfortunately assumes the agents
themselves have unbounded (inﬁnite) computation with which to conduct
their proof searches. The Haskell algorithm—based on Kripke semantics
for Gödel–Löb provability logic [7, Chapter 4]—is sound for settling these
unbounded cases using abstract mathematics and is eﬃcient enough to run
tournaments between dozens of their abstractly speciﬁed agents, which they
call modal agents. The results of Critch [18] go some of the way toward
generalizing their results for bounded versions of very simple modal agent
interactions, but not for all interactions, such as those in Open Problems 3,
5, and 6.

Thus, it would be quite interesting to work out the details of how and
when theorems in the logic used by Barasz et al. [3] (Gödel–Löb provability
logic) have analogues with bounded proof lengths, and if they could really
be applied to make resource-bounded (and hence physically realizable) ana-
logues of all the modal agents of Barasz et al. [3] and LaVictoire et al. [47].
At this level of generality, it would be useful to also carry through those de-
tails to ascertain concrete proof lengths needed for any given game outcome,
so their Haskell algorithm could be modiﬁed to return the value of k needed
for each proof search to ﬁnd the requisite proofs:

Open Problem 7. Generalize PBLT to a bounded analogue of Gödel–
Löb that systematically tracks the proof lengths necessary or suﬃcient

25

for each instance of the provability operator (cid:3).

Open Problem 8. Apply the result of Open Problem 7 to improve the
Haskell algorithm at github.com/klao/provability, so that it returns
the proof length needed for each proof search conducted by each agent
in a given open-source interaction.

LaVictoire et al. [47] exhibit a computationally unbounded agent called
PrudentBot, which is similar to DUPOC except that it uses employs an
additional proof search that allows it to defect against CooperateBot. Such
agents are particularly interesting at a population scale because they have
the potential to drive CooperateBots out of existence, which in turn would
make it more diﬃcult for DefectBot to survive. Hence we ask:

Open Problem 9. Does a computationally bounded version of Pru-
dentBot [47] exist?

If so, questions regarding population dynamics among CooperateBots,
DefectBots, DUPOCs, and PrudentBots would be interesting to examine,
especially if the game payoﬀs take into account the additional cost of proof-
searching incurred by PrudentBot.

Finally, to begin generalizing this theory to games with more than two
actions, consider next an extended Prisoners’ Dilemma with a third option, E
for “encroachment”, that is even more tempting and harmful than defection:

C
C
2, 2
D 3, 0
E 4, −2

E

D
0, 3 −2, 4
1, 1 −1, 2
0, 0
2, −1

In such a setting, it is natural for an agent to ﬁrst target mutual cooperation,
and failing that, target mutual defection, and failing that, encroach. If the
agent uses proof searches to “target” outcomes, this means it would ﬁrst
cooperate if a proof of (C,C) can be found, and failing that, defect if a proof
of (D,D) can be found, and failing that, encroach. Let’s call this vaguely
described agent concept “CDEBot.” An interesting opponent for CDEBot
is EUPOD(k), for “Encroach Unless Proof of Defection,” which is just like
DUPOC(k) with with D and E respectively in place of C and D.

26

Open Problem 10. Implement a version of CDEBot using bounded
proof searches, ensuring that it achieves (C,C) with DUPOC(k), and (D,D)
with EUPOD(k).

Why this problem is challenging.

CDEBot’s proof search for (D,D)
needs to be somehow predicated on the failure of the proof search for (C,C)
by one or both of the agents, and it’s not immediately clear how PBLT can
(cid:4)
be applied in that setting.

6.3. Applicability to humans and human institutions: modelling self-fulﬁlling

prophecies

How and when might our framework be applicable to interactions between
individual humans or human institutions who reason about themselves and
each other?

One important and immediate observation is that the open-source condi-
tion provides a model under which certain self-fulﬁlling prophecies will turn
out to be true, by virtue of the single “speech act” of each agent’s source code
being revealed. Speciﬁcally, our main theorems (3.4, 3.7, 4.1, 5.2, and 5.5)
each involve a kind of self-fulﬁlling prophecy: upon the agents verifying that
a certain outcome is going to happen, they choose to make it happen, on the
basis of the veriﬁcation. For DUPOC(k)s, the prophecy is “made” once each
agent has successfully written down a proof that they will cooperate, and it
“comes true” in their reaction to that proof (deciding to cooperate).

Self-fulﬁlling prophecies are known to be an important phenomenon in
the social sciences, as Milton Friedman [29] remarked in his Nobel Lecture:

“Do not the social sciences, in which scholars are analyzing the
behavior of themselves and their fellow men, who are in turn
observing and reacting to what the scholars say, require funda-
mentally diﬀerent methods of investigation than the physical and
biological sciences? Should they not be judged by diﬀerent crite-
ria? I have never myself accepted this view. [...] In both, there is
no way to have a self-contained closed system or to avoid interac-
tion between the observer and the observed. The Gödel theorem
in mathematics, the Heisenberg uncertainty principle in physics,
the self-fulﬁlling or self-defeating prophecy in the social sciences
all exemplify these limitations.”

27

Indeed, as Friedman alludes, Gödel’s incompleteness theorems are special
cases of Löb’s theorem (itself a special case of PBLT) where the self-fulﬁlling
prophecy p is a contradiction. Löb’s theorem essentially says “If a particular
statement p would be self-fulﬁlling (i.e., (cid:3)(p) → p) then it does self-fulﬁll
((cid:3)(p) follows, and then p follows).” The logician Raymond Smullyan [65]
also noted, in “Logicians who reason about themselves,” that Löb’s theorem
“reﬂects itself in a variety of beliefs which of their own nature are necessarily
self-fulﬁlling.” He tells a story of a cure that works only if the recipient
believes in it, and argues by Löb’s theorem that the cure is therefore bound
to work. PBLT shows that this kind of argument can really be applied in
the minds of bounded reasoners.

In which areas of social science do self-fulﬁlling prophecies play an im-
portant role? Each of the following disciplines have included analyses of the
conditions and mechanisms by which self-fulﬁlling prophecies can arise and
aﬀect the world, and might therefore present interesting domains in which
to apply open-source agent models and PBLT more speciﬁcally:

• In social psychology, Merton [51], Kelley and Stahelski [44], Word et al.
[73], Snyder et al. [66], Darley and Fazio [21], and at least a dozen other
authors have examined conditions and mechanisms whereby believing
in things can cause them to become true, such as stereotypes or inter-
racial conﬂicts.

• In education research, Rist [63], Wilkins [70], Brophy [9], Wineburg
[71], Jussim et al. [40] and Jussim and Harber [41] have examined how
and when teachers’ positive or negative expectations of students can
cause those expectations to be fulﬁlled by students.

• In the study of management and leadership, Ferraro et al. [28] have
examined how theories of economics and social science can cause their
own validity, and Eden [27, 26] has examined how leaders’ expectations
can cause organization members to conform to those expectations.

• In political and international relations theory, Zulaika [75], Bottici and
Challand [8], Verhoeven [69], Houghton [37], Frisell [30], Ahler [1] and
Świętek [67] have examined how peace and conﬂict can both arise as
self-fulﬁlling prophecies.

In economics speciﬁcally, there has been considerable work examining the
potential causes and eﬀects of self-fulﬁlling prophecies. Typically these stud-

28

ies employ dynamical systems models of repeated interactions between play-
ers, or equilibrium assumptions that do not specify how exactly the players
manage to know each other’s strategy. By contrast, the open-source condition
makes it clear and concrete how each player’s strategy could be known to the
others, and the theorems of this paper show that open-source agents do not
require repeated interaction for self-fulﬁlling prophecies to manifest. Being
open-source can also be seen as a limiting case of an agent being nontrivially
but imperfectly understood by its opponents. Thus, in real-world institutions
or software systems that are only imperfectly visible to each other, one-shot,
self-fulﬁlling prophecies might still arise from merely probabilistic mutual
knowledge, as in Theorem 4.1.

When do economic self-fulﬁlling prophecies occur in the real world? Nu-
merous authors have argued that currency crises can arise from self-fulﬁlling
prophecies [58, 53, 52, 36, 33]. Relatedly, Obstfeld [57] showed conditions
under which balance-of-payments crises may be “purely self-fulﬁlling events
rather than the inevitable result of unsustainable macroeconomic policies.”
Azariadis [2] exhibits a model of intergenerational production and consump-
tion in which self-fulﬁlling prophecies constitute between one third and one
half of the possible pricing equilibria. Others have argued that self-fulﬁlling
prophecies can give rise to debt crises [16], credit market freezes [5], and
liquidity dry-ups [50]. On the other hand, Krugman [46] argues that “the
actual currency experience of the 1990s does not make as strong a case for
self-fulﬁlling crises as has been argued by some researchers. In general, it will
be very diﬃcult to distinguish between crises that need not have happened
and those that were made inevitable by concerns about future viability that
seemed reasonable at the time.”

In our view, these observations and debates are an invitation to develop
a new and more rigorous formalism for the study of bounded agents who
reason about themselves and each other. As can be seen from our main the-
orems, such a formalism can be used to construct and model self-fulﬁlling
prophecies using PBLT, even in single-shot games with no repetition of in-
teractions between players. Going further, understanding self-fulling prophe-
cies should not be the extent of our ambition with open-source game theory.
For instance, we should also address self-defeating prophecies, as alluded to
by Friedman [29], or prophecies that plausibly-but-don’t-quite self-fulﬁll, as
noted by Krugman [46].

29

6.4. Comparison to Nash Equilibria

In this section we argue that, in real-world single-shot games, the condi-
tions under which a genuine Nash (or correlated) equilibrium could arise are
often conditions under which an open-source game (or commitment game)
could be played instead, enabling more desirable equilibria. In short, the rea-
son is that the Nash equilibrium concept already assumes a certain degree of
mutual transparency, and often that transparency could instead be used to
reveal programs/plans/commitments.

For a more detailed argument, let us ﬁrst reﬂect on the following impor-

tant observation due to Moulin regarding Nash equilibria:

“Nash equilibria are self-fulﬁlling prophecies. If every player guesses
what strategies are chosen by the others, this guess is consistent
with selﬁsh maximization of utilities if and only if all players bet
on the same Nash equilibrium. Here, we need an invisible media-
tor or some theory that pronounces x the rational strategic choice
for Player i.” [54, Ch. 5]

How does this “pronouncement” play out in the minds of the players? Suppose
in a game with two real-world players, before any discussion about what equi-
librium they should play, Player i’s subjective probability distribution over
Player j’s action is denoted ai
j(0). Let BRi denote player i’s best response
function, so bi(0) := BRi(ai
−i(0)) is the action distribution Player i will use if
no discussion happens and no prescribed equilibrium is “pronounced.” Then,
suppose a discussion or mediation occurs between the players in which a par-
ticular Nash equilibrium x = (x1, x2) is prescribed to be played instead of
the defaults (b1(0), b2(0)) that they would have played otherwise. How can
each player i become convinced that the other is really going to play x−i,
before their action is taken? There are (at least) two perspectives commonly
taken on this question.

The ﬁrst perspective is simply to say that each player has “no reason to
deviate” from the prescription: “I am not forced to follow our agreement,
but as long as you guys are faithful to it, I have no incentive to betray” [54,
Introduction]. But this does not answer the question: what reason do the
players have to believe the prescription will be carried out to begin with?
Player 1 can think, “If Player 2 is thinking that I (Player 1) will play x1,
then Player 2 will play x2”... but how would Player 1 know that Player 2
knows that Player 1 is going to do that? Indeed, Player 1 might reason, “My

30

posterior says Player 2 might play the prescribed distribution x2, but they
might instead do some other thing, such as a ‘safe’ action, or sampling from
some prior distribution over actions. Reﬂecting on this, my posterior over
Player 2’s action has updated to a new value, ai
j(1), and my best response
is b1(1) := BR1(a1
2(1)).” Player 2 can update similarly, yielding sequences
ai
j(n) and bi(n) as the number of rounds n of discussion and reﬂection grows.
This process is diﬀerent from a simple best-response iteration, because there
is also a discussion component giving rise to each ai
j(n). Is there any reason
to think it will converge on a ﬁxed point? And if so, does it converge on the
“pronounced” equilibrium (x1, x2), or some other point (x′

1, x′

2)?

These questions inspire a second perspective for applying the Nash equi-
librium concept, which is to simply deﬁne (x1, x2) to be whatever the players
converge to deciding and believing about each other through this discussion
In this view, if a mediator or compelling
process, assuming it converges.
bargaining theory is available, perhaps it can help to “steer” the conversation
toward a particular desirable equilibrium, rather than merely stating it once
and assuming conformity.

In both perspectives, the players end up in a state of mutual understand-
ing, where each is satisﬁed (at equilibrium) with her model of the other.
Indeed:

“Some kind of communication among the players is necessary to
endow them with mutually consistent beliefs, and/or allow mu-
tual observation of past outcomes.” [54, Ch. 5]

In other words, by the end of the discussion, the players no longer have
full privacy: they understand each other to some extent. Why not use that
understanding to make and reveal commitments to perform better than the
Nash equilibrium in games like the Prisoner’s Dilemma? That is to say,
why don’t the players simply make internal structural changes in the form of
unexploitable cooperative commitments (à la Theorem 3.7), and then reveal
those changes through the same process that would have allowed them to
understand each other in the Nash case?

If the players are either institutions or software systems capable of self-
repair, such self-imposed changes are both possible and potentially legible. If
we suppose they make themselves fully transparent, a question still arises to
whether their discussion of plans will converge, but as we’ve seen that ques-
tion can sometimes be resolved using PBLT. Speciﬁcally, when there appears
to be an inﬁnite regress of social metacognition, sometimes that regress can

31

be collapsed into a single self-reﬂective observation that the equilibrium is
going to be observed, along the lines of Theorem 3.7.

In summary, the kinds of mutual transparency adequate to allow Nash
equilibria in one-shot games might often allow the players to reveal enough
of their internal structure or “source codes” to enable open-source equilibria
or commitment game equilibria to arise instead.

6.5. Conclusion

Our most important conclusion is that fundamental questions regarding
the interaction of open-source agents should not be left unaddressed. As the
global economy becomes increasingly automated and artiﬁcial agents become
increasingly capable, more research is needed to prepare for novel possibilities
that the open-source condition might enable. Some results in this area might
also be applicable to self-fulﬁlling and self-defeating prophecies in the beliefs
and policies of human institutions.

Theorems 3.4, 3.7, 4.1, 5.2, and 5.5 each illustrate how one-shot interac-
tions between open-source agents can sometimes “short-circuit” unbounded
recursions of metacognition, leading to somewhat counterintuitive instances
of both mutual cooperation and mutual defection. Theorem 4.1 in particular
illustrates how similar results hold for probabilistic open-source agents. The
properties of the DUPOC(k) agents, and correspondingly their probabilistic
variants PDUPOC(k), are particularly interesting: DUPOC(k) is unexploitable,
and for large k, DUPOC(k) and PDUPOC(k) reward both cooperation and legi-
bility in their opponents; they even reward the principle of rewarding legible
cooperation. If Open Problem 1 is answered aﬃrmatively, it might shed light
on ways to implement DUPOC-like human institutions. If Open Problem 4
is answered aﬃrmatively, it might allow a practical implementation stan-
dard for DUPOC-like automated systems. And, many scenarios that would
enable enough mutual understanding to achieve a Nash equilibrium in one
round might also allow enough mutual understanding to make and reveal
new source code or commitments, allowing better-than-Nash outcomes.

Further open problems need to be resolved before these one-shot open-
source interactions can be thoroughly understood. Certainly it would be
satisfying to implement and investigate an eﬃcient version of DUPOC(k) or
CUPOD(k) using HOL/ML or Coq as in Open Problem 4, or to look at popu-
lation dynamics among agents with known interaction outcomes as in Open
Problem 2.
In Open Problems 3, 5, and 6, there seems to be no obvious
self-fulﬁlling prophecy that can be proven using PBLT. For these cases we

32

may need new techniques, or perhaps more general theoretical results such
as those described in Open Problems 1, 7, and 8. As for the successful design
of additional resource-bounded proof-based agents, such as the PrudentBot
and CDEBot described in Open Problems 9 and 10, these designs might be
easily achievable with currently available methods. Conversely, attempts at
their design might unfold into needing or generating progress on the more
general theoretical results.

References

[1] Ahler, D. J. (2014). Self-fulﬁlling misperceptions of public polarization.
Journal of Politics 76 (3), 607–620. University of Chicago Press. (link).

[2] Azariadis, C. (1981). Self-fulﬁlling prophecies. Journal of Economic The-

ory 25 (3), 380–396. Elsevier. (link).

[3] Barasz, M., P. Christiano, B. Fallenstein, M. Herreshoﬀ, P. LaVictoire,
and E. Yudkowsky (2014). Robust cooperation in the prisoner’s dilemma:
Program equilibrium via provability logic. arXiv preprint arXiv:1401.5577.
(link).

[4] Barras, B., S. Boutin, C. Cornes, J. Courant, J.-C. Filliâtre, E. Giménez,
H. Herbelin, G. Huet, C. Muñoz, C. Murthy, et al. (1997). The Coq proof
assistant reference manual: Version 6.1. Research Report. Inria. (link).

[5] Bebchuk, L. A. and I. Goldstein (2011). Self-fulﬁlling credit market
freezes. Review of Financial Studies 24 (11), 3519–3555. Society for Fi-
nancial Studies. (link).

[6] Benzmüller, C. and B. W. Paleo (2016). The inconsistency in Gödel’s
ontological argument: A success story for AI in metaphysics.
In Pro-
ceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence, pp. 936–942. (link).

[7] Boolos, G. (1995). The Logic of Provability. Cambridge, UK: Cambridge

University Press. (link).

[8] Bottici, C. and B. Challand (2006). Rethinking political myth: The clash
of civilizations as a self-fulﬁlling prophecy. European Journal of Social
Theory 9 (3), 315–336. Sage. (link).

33

[9] Brophy, J. E. (1983). Research on the self-fulﬁlling prophecy and teacher
expectations. Journal of Educational Psychology 75 (5), 631–661. Ameri-
can Psychological Association. (link).

[10] Brynjolfsson, E. and A. McAfee (2017). The business of artiﬁcial intel-
ligence. Harvard Business Review 7, 3–11. Harvard Business Publishing.
(link).

[11] Calo, R. (2017). Artiﬁcial intelligence policy: A primer and roadmap.

UC Davis Law Review 51, 399–435. UC Davis School of Law. (link).

[12] Capraro, V. and J. Y. Halpern (2019). Translucent players: Explaining
cooperative behavior in social dilemmas. Rationality and Society 31 (4),
371–408. Sage. (link).

[13] Chlipala, A. (2013). Certiﬁed programming with dependent types: A
pragmatic introduction to the Coq proof assistant. Cambridge, MA: MIT
Press. (link).

[14] Chohan, U. W. (2017). The Decentralized Autonomous Organization
and governance issues. Discussion paper. University of New South Wales,
Canberra. Available at SSRN 3082055. (link).

[15] Cihon, P. (2019). Standards for AI governance: International standards
to enable global coordination in AI research & development. Technical
report, Center for the Governance of AI, Future of Humanity Institute,
University of Oxford. (link).

[16] Cole, H. L. and T. J. Kehoe (2000). Self-fulﬁlling debt crises. Review of

Economic Studies 67 (1), 91–116. Oxford University Press. (link).

[17] Cori, R. and D. Lascar (2001). Mathematical Logic: A Course with

Exercises, Part II. Oxford: Oxford University Press.

[18] Critch, A. (2019). A parametric, resource-bounded generalization of
Löb’s theorem, and a robust cooperation criterion for open-source game
theory. Journal of Symbolic Logic 84 (4), 1368–1381. Cambridge University
Press. (link).

[19] Dafoe, A. (2018). AI governance: A research agenda. Technical report,
Center for the Governance of AI, Future of Humanity Institute, University
of Oxford. (link).

34

[20] Dafoe, A., E. Hughes, Y. Bachrach, T. Collins, K. R. McKee, J. Z.
Leibo, K. Larson, and T. Graepel (2020). Open problems in cooperative
AI. arXiv preprint arXiv:2012.08630. (link).

[21] Darley, J. M. and R. H. Fazio (1980). Expectancy conﬁrmation processes
arising in the social interaction sequence. American Psychologist 35 (10),
867–881. American Psychological Association. (link).

[22] Daryl, S. (2011). Modal proof of Löb’s theorem. Wikipedia. (link).

[23] Dirican, C. (2015). The impacts of robotics, artiﬁcial intelligence on
business and economics. Procedia - Social and Behavioral Sciences 195,
564–573. Elsevier. (link).

[24] DuPont, Q. (2017). Experiments in algorithmic governance: A history
and ethnography of “The DAO,” a failed decentralized autonomous organi-
zation. In M. Campbell-Verduyn (Ed.), Bitcoin and Beyond, pp. 157–177.
New York: Routledge. (link).

[25] Dwivedi, Y. K., L. Hughes, E. Ismagilova, G. Aarts, C. Coombs,
T. Crick, Y. Duan, R. Dwivedi, J. Edwards, A. Eirug, et al. (2019). Ar-
tiﬁcial intelligence (AI): Multidisciplinary perspectives on emerging chal-
lenges, opportunities, and agenda for research, practice and policy. Inter-
national Journal of Information Management 57, 101994. Elsevier. (link).

[26] Eden, D. (1984). Self-fulﬁlling prophecy as a management tool: Harness-
ing Pygmalion. Academy of Management Review 9 (1), 64–73. Academy
of Management. (link).

[27] Eden, D. (1992). Leadership and expectations: Pygmalion eﬀects and
other self-fulﬁlling prophecies in organizations. Leadership Quarterly 3 (4),
271–305. Elsevier. (link).

[28] Ferraro, F., J. Pfeﬀer, and R. I. Sutton (2005). Economics language
and assumptions: How theories can become self-fulﬁlling. Academy of
Management Review 30 (1), 8–24. Academy of Management. (link).

[29] Friedman, M. (1977). Nobel lecture: Inﬂation and unemployment. Jour-
nal of Political Economy 85 (3), 451–472. University of Chicago Press.
(link).

35

[30] Frisell, L. (2009). A theory of self-fulﬁlling political expectations. Jour-

nal of Public Economics 93 (5-6), 715–720. Elsevier. (link).

[31] Gasser, U. and V. A. Almeida (2017). A layered model for AI gover-

nance. IEEE Internet Computing 21 (6), 58–62. IEEE. (link).

[32] Grace, K., J. Salvatier, A. Dafoe, B. Zhang, and O. Evans (2018). When
will AI exceed human performance? Evidence from AI experts. Journal of
Artiﬁcial Intelligence Research 62, 729–754. AI Access Foundation. (link).

[33] Guimaraes, B. and S. Morris (2007). Risk and wealth in a model of
self-fulﬁlling currency attacks. Journal of Monetary Economics 54 (8),
2205–2230. Elsevier. (link).

[34] Halpern, J. Y. and R. Pass (2018). Game theory with translucent play-
International Journal of Game Theory 47 (3), 949–976. Springer.

ers.
(link).

[35] Hatcher, W. G. and W. Yu (2018). A survey of deep learning: Platforms,
applications and emerging research trends. IEEE Access 6, 24411–24432.
IEEE. (link).

[36] Hellwig, C., A. Mukherji, and A. Tsyvinski (2006). Self-fulﬁlling cur-
rency crises: The role of interest rates. American Economic Review 96 (5),
1769–1787. American Economic Association. (link).

[37] Houghton, D. P. (2009). The role of self-fulﬁlling and self-negating
prophecies in international relations. International Studies Review 11 (3),
552–584. Oxford University Press. (link).

[38] Huang, M.-H. and R. T. Rust (2018). Artiﬁcial intelligence in service.

Journal of Service Research 21 (2), 155–172. Sage. (link).

[39] Jentzsch, C. (2016). Decentralized Autonomous Organization to auto-

mate governance. White paper. Slock.IT. (link).

[40] Jussim, L., J. Eccles, and S. Madon (1996). Social perception, social
stereotypes, and teacher expectations: Accuracy and the quest for the
powerful self-fulﬁlling prophecy. In M. P. Zanna (Ed.), Advances in Exper-
imental Social Psychology, Volume 28, pp. 281–388. San Diego: Academic
Press. (link).

36

[41] Jussim, L. and K. D. Harber (2005). Teacher expectations and self-
fulﬁlling prophecies: Knowns and unknowns, resolved and unresolved con-
troversies. Personality and Social Psychology Review 9 (2), 131–155. Sage.
(link).

[42] Kalai, A. T., E. Kalai, E. Lehrer, and D. Samet (2010). A commitment
folk theorem. Games and Economic Behavior 69 (1), 127–137. Elsevier.
(link).

[43] Kamada, Y. and M. Kandori (2020). Revision games. Economet-

rica 88 (4), 1599–1630. The Econometric Society. (link).

[44] Kelley, H. H. and A. J. Stahelski (1970). Social interaction basis of
cooperators’ and competitors’ beliefs about others. Journal of Personality
and Social Psychology 16 (1), 66–91. American Psychological Association.
(link).

[45] Klein, G., K. Elphinstone, G. Heiser, J. Andronick, D. Cock, P. Derrin,
D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish, et al. (2009). seL4:
Formal veriﬁcation of an OS kernel. In Proceedings of the ACM SIGOPS
22nd Symposium on Operating Systems Principles, pp. 207–220. (link).

[46] Krugman, P. (1996). Are currency crises self-fulﬁlling? NBER Macroe-

conomics Annual 11, 345–378. University of Chicago Press. (link).

[47] LaVictoire, P., B. Fallenstein, E. Yudkowsky, M. Barasz, P. Christiano,
and M. Herreshoﬀ (2014). Program equilibrium in the prisoner’s dilemma
via Löb’s theorem. In AAAI Workshop on Multiagent Interaction without
Prior Coordination. AAAI. (link).

[48] Madhavan, A. (2012). Exchange-traded funds, market structure, and the
ﬂash crash. Financial Analysts Journal 68 (4), 20–35. Taylor & Francis.
(link).

[49] Makridakis, S. (2017). The forthcoming artiﬁcial intelligence (AI) revo-
lution: Its impact on society and ﬁrms. Futures 90, 46–60. Elsevier. (link).

[50] Malherbe, F. (2014). Self-fulﬁlling liquidity dry-ups. Journal of Fi-

nance 69 (2), 947–970. Wiley. (link).

37

[51] Merton, R. K. (1948). The self-fulﬁlling prophecy. The Antioch Re-

view 8 (2), 193–210. Antioch Review. (link).

[52] Metz, C. E. (2002). Private and public information in self-fulﬁlling
currency crises. Journal of Economics 76 (1), 65–85. Springer. (link).

[53] Morris, S. E. and H. S. Shin (1998). Unique equilibrium in a model of
self-fulﬁlling currency attacks. American Economic Review 88 (3), 587–
597. American Economic Association. (link).

[54] Moulin, H. (1986). Game Theory for the Social Sciences. New York:

NYU Press. (link).

[55] Müller, V. C. and N. Bostrom (2016). Future progress in artiﬁcial intel-
ligence: A survey of expert opinion. In V. C. Müller (Ed.), Fundamental
Issues of Artiﬁcial Intelligence, pp. 555–572. Springer. (link).

[56] Nipkow, T., L. C. Paulson, and M. Wenzel (2002). Isabelle/HOL: A
proof assistant for higher-order logic, Volume 2283 of Lecture Notes in
Computer Science. Berlin: Springer. (link).

[57] Obstfeld, M. (1986). Rational and self-fulﬁlling balance-of-payments
crises. American Economic Review 76 (1), 72–81. American Economic
Association. (link).

[58] Obstfeld, M. (1996). Models of currency crises with self-fulﬁlling fea-
tures. European Economic Review 40 (3-5), 1037–1047. Elsevier. (link).

[59] Oesterheld, C. (2019). Robust program equilibrium. Theory and Deci-

sion 86 (1), 143–159. Springer. (link).

[60] Paulson, L. C. (2015). A mechanised proof of Gödel’s incompleteness
theorems using Nominal Isabelle. Journal of Automated Reasoning 55 (1),
1–37. Springer. (link).

[61] Peters, M. (2014). Competing mechanisms. Canadian Journal of Eco-

nomics 47 (2), 373–397. Wiley. (link).

[62] Peters, M. and C. Troncoso-Valverde (2013). A folk theorem for compet-
ing mechanisms. Journal of Economic Theory 148 (3), 953–973. Elsevier.
(link).

38

[63] Rist, R. (1970). Student social class and teacher expectations: The self-
fulﬁlling prophecy in ghetto education. Harvard Educational Review 40 (3),
411–451. Harvard Education Publishing Group. (link).

[64] Shoham, Y. (2008). Computer science and game theory. Communica-

tions of the ACM 51 (8), 74–79. ACM. (link).

[65] Smullyan, R. M. (1986). Logicians who reason about themselves.

In
Theoretical Aspects of Reasoning about Knowledge: Proceedings of the 1986
Conference, pp. 341–352.

[66] Snyder, M., E. D. Tanke, and E. Berscheid (1977). Social perception and
interpersonal behavior: On the self-fulﬁlling nature of social stereotypes.
Journal of Personality and Social Psychology 35 (9), 656–666. American
Psychological Association. (link).

[67] Świętek, H. (2017). The Yemen War: A proxy war, or a self-fulﬁlling
prophecy? Polish Quarterly of International Aﬀairs 26 (2), 38–54. Polski
Instytut Spraw Międzynarodowych. (link).

[68] Tennenholtz, M. (2004). Program equilibrium. Games and Economic

Behavior 49 (2), 363–373. Elsevier. (link).

[69] Verhoeven, H. (2009). The self-fulﬁlling prophecy of failed states: So-
malia, state collapse and the Global War on Terror. Journal of Eastern
African Studies 3 (3), 405–425. Taylor & Francis. (link).

[70] Wilkins, W. E. (1976). The concept of a self-fulﬁlling prophecy. Sociology
of Education 49 (2), 175–183. American Sociological Association. (link).

[71] Wineburg, S. S. (1987). The self-fulﬁllment of the self-fulﬁlling prophecy.

Educational Researcher 16 (9), 28–37. Sage. (link).

[72] Wirtz, J., P. G. Patterson, W. H. Kunz, T. Gruber, V. N. Lu, S. Paluch,
and A. Martins (2018). Brave new world: Service robots in the frontline.
Journal of Service Management 29 (5), 907–931. Emerald Publishing Lim-
ited. (link).

[73] Word, C. O., M. P. Zanna, and J. Cooper (1974). The nonverbal me-
diation of self-fulﬁlling prophecies in interracial interaction. Journal of
Experimental Social Psychology 10 (2), 109–120. Elsevier. (link).

39

[74] Yamashita, T. (2010). Mechanism games with multiple principals and

three or more agents. Econometrica 78 (2), 791–801. Wiley. (link).

[75] Zulaika, J. (2003). The self-fulﬁlling prophecies of counterterrorism.

Radical History Review 85, 191–199. Duke University Press. (link).

Appendix A. Appendix functions

## file appendix.py
import string

# string_generator iterates lexicographically through all strings
# up to a given length:
def string_generator(length_bound):
char_set = string.printable
array = [0]
char_pos = 1 # end of the string
while True:

if array[-char_pos] == len(char_set):
if char_pos == length_bound:

break

for i in range(1,char_pos+1):

array[-i] = 0

char_pos += 1
if char_pos > len(array):
array = [0]+array

else:

array[-char_pos] += 1

continue

yield ’’.join(char_set[i] for i in array)
char_pos = 1
array[-char_pos] += 1
continue

Appendix B. Proof system assumptions

We have assumed that the proof system S employed by proof_checker
has the following properties typical of real-world proof systems, as discussed
by Critch [18, §2.2]:

40

a) S can write down expressions that represent arbitrary computable func-

tions.

b) S can write down a number k using O(lg(k)) characters.

c) S allows for the deﬁnition and expansion of abbreviations in the middle

of proofs.

d) There exists a constant e∗ with the following property: Suppose we are
given a proof ρ that is k characters long. Then, it is possible to write
out another proof E(ρ) of length at most e∗ · k, that checks the steps of
ρ and veriﬁes that ρ is a valid proof [18, §4.2]. We call this number e∗ a
“proof expansion constant.” In the notation and terminology of Critch
[18, Deﬁnition 4.1], it deﬁnes a “proof expansion function”, e(k) = e∗ ·k.

Note that Critch [18] operates under the more general assumption that
e(k) can be any increasing computable function of k. However, since proofs
written in realistic formal proof systems can be checked in linear time [18],
we focus in this paper on the simpler special case where e(k) can be taken
to be a linear function e∗ · k.

41

