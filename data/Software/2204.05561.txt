2
2
0
2

r
p
A
2
1

]
E
S
.
s
c
[

1
v
1
6
5
5
0
.
4
0
2
2
:
v
i
X
r
a

Toward Granular Automatic Unit Test Case Generation

Fabiano Pecorelli
fabiano.pecorelli@tuni.ﬁ
Tampere University
Tampere, Finland

Giovanni Grano
grano@iﬁ.uzh.ch
SEAL Lab - University of Zurich
Zurich, Switzerland

Fabio Palomba
fpalomba@unisa.it
SeSa Lab - University of Salerno
Fisciano, Italy

Harald C. Gall
harald.gall@uzh.ch
SEAL Lab - University of Zurich
Zurich, Switzerland

Andrea De Lucia
adelucia@unisa.it
SeSa Lab - University of Salerno
Fisciano, Italy

ABSTRACT
Unit testing veriﬁes the presence of faults in individual software
components. Previous research has been targeting the automatic
generation of unit tests through the adoption of random or search-
based algorithms. Despite their eﬀectiveness, these approaches do
not implement any strategy that allows them to create unit tests in
a structured manner: indeed, they aim at creating tests by optimiz-
ing metrics like code coverage without ensuring that the resulting
tests follow good design principles. In order to structure the auto-
matic test case generation process, we propose a two-step system-
atic approach to the generation of unit tests: we ﬁrst force search-
based algorithms to create tests that cover individual methods
of the production code, hence implementing the so-called intra-
method tests; then, we relax the constraints to enable the creation
of intra-class tests that target the interactions among production
code methods.

CCS CONCEPTS
• Software and its engineering → Search-based software en-
gineering; Empirical software validation.

KEYWORDS
Search-based Software Testing, Test Code Quality, Automatic Test
Case Generation

ACM Reference Format:
Fabiano Pecorelli, Giovanni Grano, Fabio Palomba, Harald C. Gall,
and Andrea De Lucia. 2022. Toward Granular Automatic Unit Test
Case Generation. In Proceedings of the IEEE/ACM International Confer-
ence on Mining Software Repositories (MSR’22), May 23-24, 2022, Pitts-
burgh, PA, USA. ACM, New York, NY, USA, Article 111, 9 pages.
https://doi.org/10.1145/xxx.xxx.xxx

1 INTRODUCTION
Software testing is the process adopted by developers to verify the
presence of faults in production code [33]. The ﬁrst step of this

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MSR’22, May 23-24, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN xxx.xxx.xxx.xxx. . . $15.00
https://doi.org/10.1145/xxx.xxx.xxx

process consists of assessing the quality of individual production
code units [2], e.g., classes of an Object-Oriented project. Previous
studies [13; 53] have shown that unit testing alone may identify
up to 20% of a project’s defects and reduce up to 30% the costs con-
nected with development time. Despite the undoubted advantages
given by unit testing, things are worse in reality: most developers
do not actually practice testing and tend to over-estimate the time
spent in writing, maintaining, and evolving unit tests, especially
when it comes to regression testing [7].

To support developers during unit testing activities, the re-
search community has been developing automated mechanisms—
relying on various methodologies like random or search-based soft-
ware testing [3]—that aim at generating regression test suites tar-
geting individual units of production code. For instance, Fraser
and Arcuri [16] proposed a search-based technique, implemented
in the Evosuite toolkit,1 able to optimize whole test suites based
on the coverage achievable on production code by tests belonging
to the suite. Later on, Panichella et al. [37] built on top of Evo-
suite to represent the search process in a multi-objective, dynamic
fashion that allowed them to outperform the state-of-the-art ap-
proaches. Further techniques in literature proposed to (1) optimize
code coverage along with other secondary objectives (i.e., perfor-
mance [14; 23; 44], code metrics [35; 36], and others [31]) or (2)
empower the underlying search-based algorithms by working on
their conﬁguration [4; 30; 54]. Yet, these approaches often fail to
generate tests that are well-designed, easily understandable, and
maintainable [16].

In addition, existing approaches do not explicitly follow well-
established methodologies that suggest taking test case granularity
into account [43]. In particular, when developing unit test suites,
two levels of granularity should be preserved [27; 34; 43]: ﬁrst, the
creation of tests covering single methods of the production code
should be pursued, i.e., intra-method [43] or basic-unit testing [34];
afterwards, tests exercising the interaction between methods of the
class should be developed in order to verify additional execution
paths of the production code that would not be covered otherwise,
i.e., intra-class [43] or unit testing [34]. Besides producing test cases
of higher quality, a structured strategy might potentially lead to
the generation of tests whose oracle would be easier to be ﬁnd for
developers, as they would be required to check smaller portions of
code to identify the expected behavior [6].

In this paper, we target the problem of granularity in automatic
test case generation, advancing the state of the art by pursuing the

1 http://www.evosuite.org

 
 
 
 
 
 
MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

Pecorelli, et al.

ﬁrst steps toward the integration of a systematic strategy within
the inner-working of automatic test case generation approaches
that might possibly support the production of more eﬀective and
understandable test suites. We build on top of Mosa [37] to devise
an improved technique, coined Granular-Mosa (G-Mosa here-
after), that implements the concepts of intra-method and intra-
class testing. Our technique splits the overall search budget in two.
In the ﬁrst half, G-Mosa forces the search-based algorithm to gen-
erate intra-method tests by limiting the number of production calls
to one. In the second half, the standard Mosa implementation is
executed so that the generation can cover an arbitrary number of
production methods, hence producing intra-class test cases that
exercise the interaction among methods.

We conjecture that, given the same amount of time, it could be
possible to improve the methodology implemented to automati-
cally generate unit tests in order to produce test suites with dif-
ferent granularity levels, following a structured procedure.

Structure of the paper. Section 2 provides background required
to properly understand our research. In Section 3 we present the
algorithmic details of G-Mosa, while Section 4 overviews the re-
search questions that we will address. In Section 5 we report on
the experimental plan of the evaluation of our technique. Section
6 discusses the possible limitations of both the approach and the
experimental plan. Finally, Section 7 outlines our next steps.

2 BACKGROUND
This section reports the basic concepts on automated tools to gen-
erate unit test suites as well as a discussion on related work.

2.1 Automatic Unit Test Case Generation
The problem of automatically generating test data has been largely
investigated in the last decade [32]. Search-based heuristics—
genetic algorithms [21] in particular—have been successfully ap-
plied to solve such a problem [32] with the goal to generate tests
with high code coverage. Single-target approaches have been the
ﬁrst techniques proposed in the context of white-box testing [46].
These approaches divide the search budget among all the targets
(typically branches) and attempt to cover each of them at a time.
To overcome the limitation of single-target approaches, Fraser and
Arcuri [16] proposed a multi-target approach, called whole suite
test generation (WS), that tackles all the coverage targets at the
same time. Building on such idea, Panichella et al. [37] proposed
a many-objective algorithm called MOSA. While WS is guided by
an aggregate suite-level ﬁtness function, MOSA evaluates the over-
all ﬁtness of a test suite based on a vector of n objectives, one for
each branch to cover. The basic working of MOSA can be summa-
rized as follows. At ﬁrst, an initial population of randomly gener-
ated tests is initialized. Such a population is then evolved through
consecutive generations: new oﬀsprings are generated by select-
ing two parents in the current population and then both crossover
and mutation operators are applied [37]. MOSA introduced a novel
preference-sorting algorithm to focus the search toward uncov-
ered branches. This heuristic solves the problem of selecting non-
dominated solutions that typically occurs in many-objective algo-
rithms [51].

Algorithm 1: Random Test Cases Generation

Input :𝑀 = {𝑚1, 𝑚2, ..., 𝑚𝑖 }: methods of the CUT we want to cover

Maximum attempts 𝐴
Maximum size 𝐿

Result: 𝑇 {𝑠1, 𝑠2, ..., 𝑠𝑛 }: test case with with 𝑛 statements

1 begin
2

𝑇 ← ∅
𝑟 ← RANDOM-NUMBER(1, 𝐿)
while not(max attempts reached) AND (|𝑇 | ≤ L) do

3

4
5

6
7

8
9

10

11

𝑝 ← RANDOM-NUMBER(0, 1)
if 𝑝 ≤ INSERTION-UUT then

INSERT-CALL-ON-CUT(𝑇 )

else

𝑣 ← SELECT-VALUE(𝑇 )
INSERT-CALL-ON-VALUE(𝑇 , 𝑣)

return 𝑇

Random Test Case Generation. To provide the reader with the
necessary context, we introduce the basics of the mechanism used
by EvoSuite [15] to randomly initialize the ﬁrst generation of tests.
More details can be found in the paper by Fraser and Arcuri [16]. A
tests case is represented in EvoSuite by a sequence of statements
𝑇 = {𝑠1, 𝑠2, ..., 𝑠𝑙 } where |𝑇 | = 𝑙. Each 𝑠𝑖 has a particular value
𝑣 (𝑠𝑖) of type 𝜏. The pseudo-code for the random test cases genera-
tion is showed in Algorithm 1. At ﬁrst, EvoSuite chooses a random
𝑟 ∈ (1, 𝐿) where 𝐿 is the test maximum length (i.e., number of state-
ments) (line 3 of Algorithm 1). Thus, EvoSuite initializes an empty
test and tries to add new statements to it. Such a logic is imple-
mented in the RandomLengthTestFactory class. EvoSuite deﬁnes
ﬁve diﬀerent kinds of statements [16]: (i) primitive statements (𝑆𝑝 ),
e.g., creating an Integer or a String variable, (ii) constructor state-
ments (𝑆𝑐 ), that instantiate an object of a given type, (iii) ﬁeld state-
ments (𝑆 𝑓 ) that access public member variables, (iv) method state-
ments (𝑆𝑚), i.e., method invocations on objects (or static method
calls), and (v) assignment statements (𝑆𝑎) that assign a value to a
deﬁned variable. The value 𝑣 and the type 𝜏 of each statement de-
pend on the generated statement itself, e.g., the value and type of
method statement will depend on the return value of the invoked
method. In the preprocessing phase, a test cluster [52] analyzes the
entire SUT (system under test) and identiﬁes all the available classes
Ω. ∀𝑐 ∈ Ω, the test cluster deﬁnes a set of {C, M, F }, where respec-
tively, C is the set of constructors, M if the set of instance method
and F is the set of instance ﬁelds available for a class 𝑐.

EvoSuite tries to repetitively generate new statements (the loop
from line 4 to line 10 in Algorithm 1) and add them to a test. The
process continues until the test hits the maximum random length
or the maximum number of attempts (a parameter in EvoSuite set
to 1,000 by default) is reached (line 4 in Algorithm 1). EvoSuite
can insert two main kinds of statements. With a probability lower
than INSERTION-UUT (a property deﬁned to 0.5 by default), Evo-
Suite generates a random call of either a constructor of the class
under test (CUT) or a member class, i.e., instance ﬁeld of method
(lines 6-7 in Algorithm 1). Alternatively, the tool can generate a
method call to a value 𝑣 (𝑠 𝑗 ) where 𝑗 ∈ (0, 𝑖] and 𝑖 is the position on
which the statements will be added (lines 9-10 in Algorithm 1). In
other words, EvoSuite invokes a method on a value of a statement
already inserted into the test. Such a value is randomly selected
among all the values from the statements from the position 0 to
the actual position (line 9 in Algorithm 1) EvoSuite also takes care
of the parameters or the callee objects needed to generate a given

Toward Granular Automatic Unit Test Case Generation

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

Algorithm 2: G-Mosa Algorithm

Input :𝐵 = {𝜏1, ..., 𝜏𝑚 }: set of coverage targets of a program

Population size 𝑀

Result: A test suite 𝑇

Algorithm 3: Insert Random Call

Input :𝑇 {𝑠1, 𝑠2, ..., 𝑠𝑛 }: test case with with 𝑛 statements

𝑆 = {𝑠1, 𝑠2, ..., 𝑠 𝑗 }: setters of the CUT
Result: 𝑇 : test case with with 𝑛 + 1 statements

1 begin
2

3

4

5

6
7

8

9

10

11

𝑇 ← ∅
𝛼 ← intra-method-testing(𝑀)
𝛾 ← MOSA(𝑀)
𝑇𝛼 , 𝐵𝛼 ← GENERATE-TESTS(𝛼, 𝐵)
if 𝐵𝛼 == ∅ then

return 𝑇𝛼

𝑇 ← 𝑇 Ð𝑇𝛼
𝑇𝛾 , 𝐵𝛾 ← GENERATE-TESTS(𝛾, 𝐵𝛾 )
𝑇 ← 𝑇 Ð𝑇𝛾
return 𝑇

1 begin
2

𝑜 ← GET-RANDOM-TEST-CALL
if (𝑜 is a method) then

/* half search budget */

/* half search budget */

3
4
5

6

7

8

9
10

11

if RANDOM-NUMBER(0, 1) ≤ INSERTION-SET then

𝑇 ← 𝑇 Ð ADD-METHOD(𝑠 𝑗 ∈ 𝑆)
return 𝑇

𝑇 ← 𝑇 Ð ADD-METHOD(𝑜)
𝑇𝑐 ← true

else

𝑇 ← 𝑇 Ð (ADD-COSTRUCTOR(𝑜) OR 𝑇 Ð ADD-FIELD(𝑜))

return 𝑇

statement. For example, a call to an instance method of the CUT
requires (i) the generation of a statement instantiating the CUT it-
self, and (ii) the generation of a statement deﬁning values needed
as argument for the method call. The values for such parameters
can either (i) be selected among the sets of values already in the
test, (ii) set to null, or (iii) generated randomly.

1 S t r i n g R e a d e r s t r i n g R e a d e r0 = new S t r i n g R e a d e r(" # < z - K ~+*O 4 @s ^ W ") ;
2 char [] charArray 0 = new char [ 1 ];
3 s t r i n g R e a d e r0. read ( charArray 0 ) ;
4 J a v a C h a r S t r e a m j a v a C h a r S t r e a m0 = new J a v a C h a r S t r e a m( s t r i n g R e a d e r0

, ( -1 2 7 3 ) , 1 , 7 7 ) ;

5 J a v a P a r s e r T o k e n M a n a g er j a v a P a r s e r T o k e n M a n ag e r0 = new
J a v a P a r s e r T o k e n M a n a g er( j a v a C h a r S t r e a m0 ) ;
6 Token token 0 = j a v a P a r s e r T o k e n M a n a g e r0 . g e t N e x t T o k e n() ;

Listing 1: Example of a test generated by Evosuite

To better understand the generation process, let consider the
test case in Listing 1, which has been generated for the class
JavaParserTokenManager. To create this test, Evosuite works as
follows. Starting from an empty test, it decides with a certain ran-
dom probability to insert a statement invoking an instance method
of the CUT: in our example, the getNextToken() method (line
6 of Listing 1). However, Evosuite needs ﬁrst to generate two
other statements, i.e., line 5 and 6 of Listing 1, respectively: a
statements returning a value of type JavaParserTokenManager
(i.e., the callee of the method) and a statement returning a value
of type JavaCharStream (i.e., the parameter of the method). In
turn the constructor of JavaCharStream will need a value of type
StringReader (line 1 of Listing 1). Line 3 of Listing 1 is instead
the result of the other kind of possible insertion, i.e., a method call
to a value already present in the test: the stringReader0 object in
this case. Similarly, the tool will generate the primitive statement
at line 2 of Listing 1 to provide the parameter needed by such a
call.

3 G-MOSA: A TWO-STEP AUTOMATIC TEST

CASE GENERATION APPROACH

G-Mosa is deﬁned as a two-step methodology that combines intra-
method and intra-class unit testing [34; 43]. The pseudo-code of G-
Mosa is outlined in Algorithm 2. The ﬁrst step of the methodology
generates tests that exercise the behavior of production methods
in isolation: we indeed only allowed by design to generate intra-
method tests (details in Section 3.1). The second step is based on
the standard MOSA implementation [37] that performs intra-class

unit testing by exercising a class trough a sequence of method call
invocations. In the following, we detail each of these two steps.

3.1 Step I - Intra-Method Tests Generation
The intra-method testing process is the ﬁrst step to be initialized
(line 3 of Algorithm 2). Like any other test-case generation tech-
nique, a set of coverage targets 𝐵 is given as input. The intra-
method process starts (line 5 of Algorithm 2) with 𝐵 as target of
the search and sets its search budget to the half of the overall bud-
get available: in other words, if G-Mosa is given 180 seconds as
budget, the intra-method testing process will run for 90 seconds.
At the end of its search, the ﬁrst step returns (i) 𝑇𝛼 , the set of gen-
erated tests cases, and (ii) 𝐵𝛼 , the set of uncovered targets. 𝑇𝛼 and
𝐵𝛼 will be used then as input for the second phase (see Section 3.2).

Intra-Method Code-Generation Engine. As already mentioned, G-
Mosa is a variant of Mosa that applies ﬁrst an intra-method test-
ing methodology [34]: each generated test exercises a single pro-
duction method of the CUT. To enable intra-method testing, we
modiﬁed the code-generation engine used by EvoSuite to ran-
domly generate new tests. In Section 2 we described such a mech-
anism: in a nutshell, EvoSuite inserts randomly generated state-
ments (e.g., calls to a class constructor or invocation of instance
methods) in a test until a maximum number of statements is
reached. This approach does not guarantee—nor has been designed
to do it—any control on the number of instance method invoca-
tions of a test. As a consequence, tests might end up containing a
sequence of method calls for the CUT and thus, perform intra-class
unit testing.

To enable intra-method testing, we modiﬁed the high-level al-
gorithm described in Algorithm 1. With the current formulation,
the insertion loop (from line 4 to line 10 in Algorithm 1) has two
stopping conditions: either a maximum number of attempts or the
maximum length 𝐿 of the test is reached. We deﬁned a third stop-
ping criteria: as soon as a statement 𝑠𝑖 representing a method invo-
cation on a CUT object is inserted, we considered the test as com-
plete. To store this information, in our implementation each test
𝑇 has a property 𝑇𝑐 , initially set to false, that indicates whether
such a statement 𝑠𝑖 has been inserted in 𝑇 . Therefore, we added
𝑛𝑜𝑡 (𝑇𝑐 ) as additional stopping criteria for the insertion loop at line
4 of Algorithm 1. It is worth remarking that insertions of CUT in-
stance methods are managed by the INSERT-CALL-ON-CUT pro-
cedure (line 7 of Algorithm 1). Thus, we re-implemented such a
procedure to handle the newly deﬁned stopping criteria.

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

Pecorelli, et al.

Algorithm 3 shows our ad-hoc implementation of the INSERT-
CALL-ON-CUT procedure. The algorithm takes as input a test 𝑇
with 1 ≤ 𝑛 < 𝐿 statements and a set 𝑆 ⊆ hM𝐶𝑈𝑇 ∪ F𝐶𝑈𝑇 i
of setters for the CUT. For a class 𝑐, 𝑆 is composed of all its in-
stance ﬁelds F and of a subset of its instance methods M. We
deﬁned the following heuristic to detect the instance method ∈ 𝑆
for the CUT. We considered as setter every 𝑚 ∈ M whose method
name has the hpreﬁxihkeywordihsuﬃxi structure, with keyword ∈
{set, get, put}, if and only if ∃ 𝑚′ ∈ 𝑀 | hkeywordi′ == get and
hpreﬁxi′ == hpreﬁxi & hsuﬃxi′ == hsuﬃxi. It is worth noting
that the h𝑝𝑟𝑒 𝑓 𝑖𝑥i part of the method name is optional. For instance,
let consider the class SimpleNode of the jmca project: this has two
instance methods named jjtSetParent and jjtGetParent. Ac-
cording to our heuristic, the method jjtSetParent is considered
as a setter method of the class SimpleNode.

The ﬁrst step for generating a random call on the CUT is to
extract a random call 𝑜 in the set {C, M, F }. This is done by the
GET-RANDOM-TEST-CALL procedure (line 2 of Algorithm 3). If
𝑜 ∈ {C ∪ F }, a new statement 𝑠𝑖 including a call to 𝑜 is inserted
into the test (as described in Section 2). In case 𝑜 ∈ 𝑀—with a cer-
tain probability (set as property to 0.3 by default)—a new statement
with a randomly selected setter is generated and inserted into 𝑇 ;
therefore, the test is returned (lines from 4 to 6 in Algorithm 3). In
the opposite case, 𝑜 is added to the test 𝑇 and its property 𝑇𝑐 is set
to true (lines 7 and 8 of Algorithm 3). As a consequence, the code-
generation engine stops attempting new insertions: 𝑇𝑐 is now true
and the condition 𝑛𝑜𝑡 (𝑇𝑐 ) is not met anymore. Our implementation
of GET-RANDOM-TEST-CALL enables intra-method testing since
it allows by design the invocation of a single instance method of
the CUT. Note that our formulation does not consider setters as
units under test since they are needed only to set the state of the
CUT object required to properly exercise the method under test.

3.2 Step II - Intra-Class Tests Generation
As previously explained, the GENERATE-TESTS procedure returns
a set of generated tests (𝑇𝛼 ) and a set of uncovered targets 𝐵𝛼 ⊆ 𝐵.
If 𝐵𝛼 == ∅, the intra-method testing process achieved full coverage
on the CUT and 𝑇𝛼 is returned (lines 6-7 of Algorithm 2). In the
opposite case, 𝑇𝛼 is added to𝑇 (line 8 of Algorithm 2). In the second
step, Mosa is selected as algorithm for the search. This time, 𝐵𝛼
is given as set of target to Mosa (line 9 of Algorithm 2). In other
words, Mosa will attempt to cover only what has not been covered
in the ﬁrst step. At the end of the GENERATE-TESTS procedure,
the resulting 𝑇𝛾 is added to 𝑇 and the ﬁnal test suite 𝑇 is returned
(lines 9-10 of Algorithm 2). 𝑇 is formed by two diﬀerent kinds of
tests: 𝑇𝛼 generated by the intra-method process, that tests single
production methods in isolation and 𝑇𝛾 generated by Mosa, that
exercise a class by constructing sequences of method calls.

4 RESEARCH QUESTIONS AND OBJECTIVES
The goal of the empirical study, according to the Goal-Question-
Metric (GQM) template [8], is: evaluate the eﬀectiveness, size, and
maintainability of the test suites generated by G-Mosa with the
purpose of understanding the extent to which our approach can
generate higher-quality unit test cases when compared to a state
of the art automatic test case generation technique like Mosa.

To address our goal, we set up three research questions (RQs).
In the ﬁrst place, we target one of the risks associated with the
mechanisms implemented in G-Mosa, namely the decrease of both
code and mutation coverage: by design, we force G-Mosa to gener-
ate intra-method tests, naturally limiting its scope and potentially
lowering the number of tangentially covered branches.

Our ﬁrst RQ can therefore be seen as preliminary and aimed at
assessing this aspect by comparing the eﬀectiveness of test suites
generated by G-Mosa and Mosa [37]. We consider Mosa as base-
line because (1) previous techniques aimed at improving the qual-
ity of generated tests were compared to Mosa as well (e.g., [36])
and (2) we built G-Mosa on top of Mosa, making the comparison
required. We deﬁne the following research question:

RQ1 - Eﬀectiveness. How does G-Mosa compare to MOSA in
terms of branch and mutation coverage?

Once assessed the implications of G-Mosa for the eﬀectiveness
of test cases, we aim to investigate the potential beneﬁts given by
our technique. We take into account the size of the generated test
cases: according to previous research in the ﬁeld [16; 22; 37], this
is an indicator that has been often used to estimate the eﬀort that
developers would spend to comprehend and interact with the tests,
indeed, a number of previously proposed search-based automatic
test case generation approaches used it as a metric to optimize
[35; 38; 44]. Also in this case, we compare the size of test cases
generated by G-Mosa and Mosa, addressing the following RQ:

RQ2 - Size. How does G-Mosa compare to MOSA in terms of test
case size?

While the size assessment could already provide insights into
the comprehensibility of the generated test cases, in the context of
our research we aim to provide additional analyses to assess their
potential usefulness from a maintainability perspective. In partic-
ular, once generated, test cases not only need to be manually vali-
dated by testers to verify assertions [1; 6], but also maintained to
keep them updated as a consequence of the changes to the pro-
duction code [36]. Hence, it is reasonable to assess the capabilities
of our approach in this respect. We plan to compare G-Mosa and
Mosa in terms of the metrics that have been previously designed
to describe the quality and maintainability of test cases and that we
have surveyed in our previous work [41]. These pertain to (1) code
complexity, as measured by the weighted method count of a test
suite [49]; (2) eﬀerent coupling [19]; and (3) test smells, i.e., subop-
timal design or implementation choices applied when developing
test cases [20]. This lead to our third research question:

RQ3 - Maintainability. How does G-Mosa compare to MOSA
in terms of maintainability of test cases?

On the one hand, the quantitative measurements computed so
far can provide a multifaceted view of how the proposed approach
compares to state of the art in terms of performance. On the other
hand, these analyses cannot quantify the actual gain given by G-
Mosa in practice. For this reason, the last step of our methodology
includes a user study where we plan to inquiry developers on the

Toward Granular Automatic Unit Test Case Generation

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

understandability of the test cases output by G-Mosa when com-
pared to those of Mosa. This leads to the formulation of our last
research question:

RQ4 - Understandability. How does G-Mosa compare to MOSA
in terms of understandability of test cases?

5 EXPERIMENTAL PLAN
To answer our research questions, we aim to perform an empirical
study on Java classes comparing G-Mosa to MOSA [37]. This sec-
tion reports details about the experimental procedure planned to
address our RQs.

5.1 Experimental Environment
We will run G-Mosa and Mosa against a dataset of Java classes,
collecting the generated tests and the corresponding code coverage
indicators. In particular, we plan to consider around 100 classes
pertaining to the SF110 corpus [17]. This benchmark2 contains a
set of Java classes extracted from 110 projects of the SourceForge
repository. We select it since this is typically used in automatic
test case generation research [16; 17; 24; 37] and, therefore, can
allow us to experiment our technique on a “standard” benchmark
that would enable other researchers to build upon our ﬁndings and
compare other techniques.

To account for the intrinsic non-deterministic nature of genetic
algorithms, we will run each approach on each class in the dataset
for 30 times, as recommended by Campos et al. [9]. We use the time
criterion as search budget, allowing 180 seconds for the search [9].
In G-Mosa, this time is equally distributed amongst the two steps
of the approach, i.e., we reserve 90 seconds for intra-method and 90
for intra-class testing. Mosa could instead rely on the entire search
budget to generate tests, as it does not have multiple steps.

To run the experimented approaches, we rely on the default
parameter conﬁguration given by Evosuite. As shown by Arcuri
and Fraser [5], the parameter tuning process is long and expensive,
other than not necessarily paying oﬀ in the end.

5.2 Collecting Performance Metrics
In the context of RQ1, we will rely on the code and mutation cov-
erage analysis engine of Evosuite [18]. In particular, we will let
the tool collect the branch coverage of each test in each of the 30
runs. Additionally, the tool will also collect information on the mu-
tation score: despite the existence of other tools able to perform
mutation analysis (e.g., PiTest3), we rely on the one made by Evo-
suite since it can eﬀectively represent real defects [18] and has
been used in a series of recent studies on automatic test case gen-
eration [23; 39; 40]. We aim to perform the mutation analysis at
the end of the search, once generated the unit tests for all the ap-
proaches. To obtain meaningful results we give an extra-budget of
5 minutes to the mutation analysis—this step is required to gen-
erate more mutants and to verify the ability of tests to capture
them [18].

2 http://www.evosuite.org/experimental-data/sf110/
https://pitest.org.

3 The Pitest analyzer:

As for RQ2, we start from the set of test suites output by the
search process for the two experimented approaches and ﬁrst com-
pute their overall size, i.e., the lines of code of the generated test
classes. As shown by previous work in the ﬁeld [16; 39], this met-
ric represents an indicator of the usability of the test suites given
by the tools. While recognizing the value of this perspective, we
also know that such a validation could be excessively unfair in our
case. By design, G-Mosa aims at creating a larger amount of test
cases with respect to Mosa, with a ﬁrst set of many small tests
implementing the concept of intra-method testing and a second
set composed of larger tests that implement the concept of intra-
class testing. On the contrary, Mosa does not explicitly target the
creation of maintainable test cases, hence possibly generating a
fewer amount of tests that account for a lower overall test suite size
while reaching high branch coverage. As a consequence, the assess-
ment of the overall test suite size could be too simplistic, other than
providing coarse-grained considerations on the usefulness of test
suites, i.e., in practice, developers rarely look at the entire test suite
while ﬁxing defects [10]. Hence, we aim to complement the overall
test suite size assessment with an analysis of the properties of the
individual test cases: we plan to compute the mean size per test case,
namely the average amount of lines of code of the automatically
generated test cases within a test suite. Such a measurement can
allow us to verify whether our approach could provide developers
with smaller units that might better align to the actual eﬀort re-
quired by a developer to deal with the tests generated by G-Mosa
when compared to our baseline Mosa [10].

To answer our third research question (RQ3), we aim to com-
pute three metrics which have been previously associated with
maintainability and that might aﬀect the way developers interact
with test cases [22; 25; 41; 47]. Weighted Method Count of a Test
Suite (TWMC) [49] represents a complexity metric whose compu-
tation implies the sum of the complexity values of the individual
test cases of a test suite. The metric provides an estimation of how
complex a test would be to understand for a developer [12; 25]. In
the second place, we compute the eﬀerent coupling metric (EC)
[19], which provides an estimation of how coupled the test cases
of a suite are with the other test cases of the same suite. Keeping
coupling under control is a key concern when writing test cases,
as an excessive dependence among tests might potentially lead to
some sort of ﬂakiness [26]. Finally, we will detect the number of
test smells per test suite: these smells have been often associated
to the decrease of maintainability and eﬀectiveness of test suites
[24; 47] and likely represent the most suitable maintainability as-
pect to verify within the test code. In this respect, it is worth re-
marking that automatically generated test code is by design af-
fected by certain test smells: for instance, the generated tests come
without assertion messages and, therefore, are naturally aﬀected
by the smell known as Assertion Roulette [20], which arises when
a test has no documented assertions. At the same time, automatic
tests might not suﬀer from other types of smells. For example, ex-
ternal resources are mocked by the Evosuite framework, making
the emergence of a test smell like Mystery Guest [20]—which has
to do with the use of external resources—not possible. As such,
comparing the experimented approaches based on the presence of
these smells would not make sense. Hence, we will only consider
the test smells whose presence can be actually measured. In more

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

Pecorelli, et al.

practical terms, we will employ the tool by Spinellis [48] to com-
pute TWMC and EC metrics. As for test smells, we will rely on
TsDetect [42], which is a tool able to identify more than 25 dif-
ferent types of test smells—in this case, however, we will limit the
detection to the test smells that might actually arise in automati-
cally generated tests.

5.3 Collecting Understandability Metrics

The last step of our experimentation concerns with the assess-
ment of the actual gain provided by G-Mosa in practice. We there-
fore design an online experiment where we will (1) involve devel-
opers in tasks connected to the understandability of the test cases
generated by our approach and (2) compare our approach with the
baseline Mosa implementation. Our experiment will be online as
we are not currently allowed to perform in-situ controlled exper-
iments because of the COVID19 pandemic. We will therefore use
an online platform we have recently developed and that allow ex-
ternal participants to (1) navigate and interact with source code
elements and (2) answer closed and open questions.

Participant’s recruitment. We will recruit developers using
various channels. In the ﬁrst place, we will invite the original open-
source developers of the classes considered in the study. This will
be done via e-mail. Of course, we will only approach the developers
who have publicly released their e-mail address on GitHub. In a
complementary manner, we will recruit participants through Pro-
lific4 by carefully considering the guidelines recently proposed
by Reid et al. [45]. This is a research-oriented web-based platform
that enables researchers to ﬁnd participants for user studies. One
of the features of Prolific is the speciﬁcation of constraints over
participants, which in our case enabled to limit the participation
to software developers. It is important to point out that Prolific
implements an opt-in strategy [29], meaning that participants get
voluntarily involved. This might potentially lead to self-selection
or voluntary response bias [28]. To mitigate this risk, we will intro-
duce an incentive of 7 pounds per valid respondent. Once we will
receive the answers, we will ﬁlter out the answers coming from de-
velopers without the minimum expertise required (e.g., developers
without any expertise on testing) and participants who did not take
the task seriously. Should the amount of participants not reached
an acceptable level, quantiﬁed in 120 valid participants, we will
conduct the recruitment again until reaching the acceptable bar.

Experimental setting. The participants will be ﬁrst asked to
answer demographic questions that will serve to address their
background and level of expertise in software development and
testing.

Then, participants will be asked to perform the same task twice.
In particular, they will be provided with the source code of two
Java test classes aiming at exercising the same production class.
One of them will be generated by G-Mosa and the other one by
Mosa. In each task, after reading each of the two test classes partic-
ipants will be asked to (1) rate the overall understandability of the
class with a 5-points Likert scale (from 1, which indicates poorly
understandable code, to 5, which indicates fully understandable
code); (2) explain the reasons for the rating provided; (3) Write

4 Prolific website: https://www.proliﬁc.co/.

the assertion messages for the methods of the test class under con-
sideration. With the ﬁrst two questions, we assess the perceived
understandability of test cases, while the last question provides a
more accurate indication of how much a participant understand
the content of the test.

As for the order of the test classes, half of the participants will
ﬁrst engage with the test class generated by Mosa and then with
the one generated by G-Mosa. Conversely, the other half of the
participants will read the two test classes in the reverse order.

Through the experiment, we will be able to assess the extent to
which developers can understand and deal with the information
provided by the test cases generated by the two approaches.
5.4 Data Analysis
Once computed all the metrics to address our four research ques-
tions, we plan to run statistical tests to verify whether the diﬀer-
ences observed between G-Mosa and Mosa are statistically sig-
niﬁcant. More speciﬁcally, we aim to employ the non-parametric
Wilcoxon Rank Sum Test [11] (with 𝛼 == 0.05) on the distributions
of (1) code coverage, (2) mutation coverage, (3) size per test case,
(4) weighted method count of a test suite, (5) eﬀerent coupling, (6)
number of test smells, and (7) understandability scores assigned
by developers in the user study. In this respect, we formulate the
following null hypotheses:
Hn 1. There is no signiﬁcant diﬀerence in terms of branch coverage

achieved by G-Mosa and MOSA.

Hn 2. There is no signiﬁcant diﬀerence in terms of mutation cover-

age achieved by G-Mosa and Mosa.

Hn 3. There is no signiﬁcant diﬀerence in terms of size per unit

achieved by G-Mosa and Mosa.

Hn 4. There is no signiﬁcant diﬀerence in terms of weighted
method count of a test suite achieved by G-Mosa and Mosa.
Hn 5. There is no signiﬁcant diﬀerence in terms of eﬀerent coupling

achieved by G-Mosa and Mosa.

Hn 6. There is no signiﬁcant diﬀerence in terms of the number of

test smells achieved by G-Mosa and Mosa.

Hn 7. There is no signiﬁcant diﬀerence in terms of the understand-

ability scores achieved by G-Mosa and Mosa.
From a statistical perspective, we have to take into account the
fact that, if one of the null hypothesis is rejected, then one between
G-Mosa and Mosa is statistically better than the other. Hence, we
deﬁned a set of alternative hypotheses such as the following:
An 1. The branch coverage achieved by G-Mosa and MOSA is sta-

tistically diﬀerent.

An 2. The mutation coverage achieved by G-Mosa and Mosa is

statistically diﬀerent.

An 3. The size per unit of the unit test suites generated by G-Mosa

and Mosa is statistically diﬀerent.

An 4. The weighted method count of a test suite of the unit test
suites generated by G-Mosa and Mosa is statistically diﬀerent.
An 5. The eﬀerent coupling of the unit test suites generated by G-

Mosa and Mosa is statistically diﬀerent.

An 6. The number of test smells of the unit test suites generated

by G-Mosa and Mosa is statistically diﬀerent.

Toward Granular Automatic Unit Test Case Generation

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

An 7. The understandability scores of the unit test suites generated

by G-Mosa and Mosa is statistically diﬀerent.

We reject the null hypotheses if 𝐻𝑛𝑖 ⇐⇒ 𝑝 < 0.05. In addition
to the Wilcoxon Rank Sum Test, we rely on the Vargha-Delaney
( ˆ𝐴12) [50] statistical test to measure the magnitude of the diﬀer-
ences in the distributions of the considered metrics. Based on the
direction given by ˆ𝐴12, we can make a practical sense to the alter-
native hypotheses. Should the ˆ𝐴12 values be lower than 0.5, this
would denote that the test suites generated by G-Mosa would be
better than those provided by Mosa. For instance, a ˆ𝐴12 < 0.50
in the distribution of code coverage would indicate that the code
coverage achieved by G-Mosa is higher than the one reached by
the baseline. Similarly, a ˆ𝐴12 > 0.50 indicates the opposite, while
ˆ𝐴12 == 0.50 points out that the results are identical.

Besides the statistical analysis of the distributions collected in
our empirical study, we will also proceed with the veriﬁcation of
the assertion messages written by the user study participants. In
this case, the ﬁrst two authors of the paper will act as inspectors
and assess whether the reported messages are in line with the ac-
tual behavior of the test cases. In doing so, the inspectors will take
advantage of a code coverage analysis tool, which might reveal the
path covered by a test and, therefore, help assessing the match be-
tween the messages and the goals of the test case. We will ﬁnally
collect and report the number of matches between assertion mes-
sages and actual behavior of the tests for each of the experimented
tools. In addition, we will make use of the free answers provided
by participants when explaining the reasons for the understand-
ability score (question #2 of the task) to identify the reasons for
the correct/wrong assertion deﬁnitions. We will hence be able to
provide an overview of the advantages and disadvantages of each
test case generation tool with respect to the understandability of
the resulting test cases.

5.5 Publication of generated data
G-Mosa source code, as well as all the other data generated from
our study will be publicly available in an online repository (e.g.,
GitHub). We also plan to release the scripts to automatically gen-
erate the test suites, other than the data collected and used for the
statistical and content analysis that we will present in the paper.

6 LIMITATIONS
This section discusses the main limitations of the study.

A ﬁrst possible limitation could be connected to the selection of
the baseline technique on which we build G-Mosa. The selection
of Mosa is driven by the fact that this is the technique we know
best and feel most conﬁdent with to modify. Yet, we believe that the
selection of another baseline do not have an important impact on
the results obtained in the context of our study. In particular, our
aim is to deﬁne a systematic approach and to improve the result-
ing structure of the generated test cases, independently from the
baseline approach, i.e., the methodology implemented in G-Mosa
can be applied on any automatic test case generation technique. In
any case, we also plan to replicate the study with diﬀerent core
techniques in the future, in order to verify this consideration.

Other possible limitations could be related to the experimental
design discussed in Section 5. First, it is worth to note that the pa-
rameters used for the algorithms’ conﬁguration can inﬂuence the
outcomes. To this aim we will rely on the default settings avail-
able in Evosuite on the basis of previous research in the ﬁeld [5]
which showed that the conﬁguration of parameters is not only ex-
pensive but also possibly ineﬀective in improving the performance
of search-based algorithms. Moreover, since our approach and the
baseline selected for comparison are both implemented within the
same tool, i.e., Evosuite [15], we will rely on exactly the same the
same underlying implementation of the genetic operators, avoid-
ing possible confounding eﬀects due to the use of diﬀerent algo-
rithms.

To deal with the inherent randomness of genetic algorithms, we
are going to re-execute our experimental procedure 30 times—as
recommended by previous research [9]—and report their average
performance when discussing the results.

Finally, we decided to adopt well-known state-of-the-art metrics
for comparing techniques’ performance. For example, we select
branch coverage to assess the eﬀectiveness of the tests generated
by the two approaches. In addition, we employ appropriate statisti-
cal tests to verify the signiﬁcance of the diﬀerences achieved by our
approach and the baseline. Speciﬁcally, we rely on the Wilcoxon
Rank Sum Test [11] for statistical signiﬁcance and the Vargha-
Delaney eﬀect size statistic [50] to estimate the magnitude of the
observed diﬀerence.

Last but not least, in the context of the user study conducted
to assess the understandability of the generated test cases, we re-
cruit participants using a research-oriented platform like Prolific.
While this choice might potentially introduce some sort of selec-
tion bias [45], the platform allows to recruit participants from all
around the world, with diﬀerent expertise and experience. When-
ever needed, we will ﬁlter out responses of participants who did
not take the task seriously, other than recruit additional partici-
pants to reach an acceptable amount. Other than collecting back-
ground information by directly inquiring participants, the online
platform that we will use can keep track of the time spent by each
participant on each answer: this will enable an improved analysis
of the performance of the participants, possibly helping us spot
cases to discard. Nonetheless, we are aware of the limitations of
an online experiment - yet, in the current pandemic situation, this
is the only viable solution.

7 CONCLUSION
The ultimate goal of our research is to deﬁne a systematic strategy
for the automatic generation of test code. In this paper, we will
start working toward this goal by implementing the concepts of
intra-method and intra-class testing within a state-of-the-art au-
tomatic technique for test case generation like Mosa. One of the
risks connected to these mechanisms is the decrease of eﬀective-
ness: by forcing our approach to generate intra-method tests we
naturally limit its scope, potentially lowering the number of tan-
gentially covered branches.

The technique we propose is also prepared to allow the genera-
tion at diﬀerent granularity levels. Indeed, one can simply increase

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

Pecorelli, et al.

the number of production calls allowed in the ﬁrst part of the gen-
eration, that we limit to one in this ﬁrst concept, to generate tests at
incremental levels of granularity. This would potentially have key
implications, as the proposed strategy can be easily extended from
a two-step (i.e., intra-method + intra-class) to an n-step approach
in which the number of calls allowed to methods of the class under
test (CUT) is increased at each step. Since diﬀerent number of calls
to methods of the class under test corresponds to diﬀerent paths
on the state machine of the CUT, it would be possible to limit the
length of the paths to execute on the state machine, thus providing
shorter and more comprehensible tests for which it will be easier
to generate an oracle. In this sense, our work poses the basis for
the deﬁnition of a brand new way to generate test cases that might
be of particular interest for the researchers working at intersection
between software testing and software code quality.

As part of our future work, we plan to assess our technique fol-
lowing the methodology described in Section 5. Later, we plan to
exploit the granular nature of G-Mosa to perform an experimenta-
tion based on several granularity levels. Finally, we plan to imple-
ment our approach on top of a broader set of baselines techniques
as well as an in-vivo performance assessment involving real testing
experts.

ACKNOWLEDGEMENTS
Fabio is partially supported by the Swiss National Science Founda-
tion through the SNF Project No. PZ00P2_186090 (TED).

REFERENCES
[1] Sheeva Afshan, Phil McMinn, and Mark Stevenson. 2013. Evolving Readable
String Test Inputs Using a Natural Language Model to Reduce Human Oracle
Cost. In Proceedings of the 2013 IEEE Sixth International Conference on Software
Testing, Veriﬁcation and Validation (ICST ’13). IEEE Computer Society, Washing-
ton, DC, USA, 352–361. https://doi.org/10.1109/ICST.2013.11

[2] Paul Ammann and Jeﬀ Oﬀutt. 2016. Introduction to software testing. Cambridge

University Press.

[3] Saswat Anand, Edmund K Burke, Tsong Yueh Chen, John Clark, Myra B Cohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil Mcminn, Antonia
Bertolino, et al. 2013. An orchestrated survey of methodologies for automated
software test case generation. Journal of Systems and Software 86, 8 (2013), 1978–
2001.

[4] Andrea Arcuri. 2019. RESTful API automated test case generation with EvoMas-
ter. ACM Transactions on Software Engineering and Methodology (TOSEM) 28, 1
(2019), 1–37.

[5] Andrea Arcuri and Gordon Fraser. 2013. Parameter tuning or default values?
An empirical investigation in search-based software engineering. Empirical Soft-
ware Engineering 18, 3 (2013), 594–623.

[6] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo. 2015. The Oracle
Problem in Software Testing: A Survey. IEEE Transactions on Software Engineer-
ing 41, 5 (May 2015), 507–525. https://doi.org/10.1109/TSE.2014.2372785
[7] Moritz Beller, Georgios Gousios, Annibale Panichella, Sebastian Proksch, Sven
Amann, and Andy Zaidman. 2017. Developer testing in the ide: Patterns, beliefs,
and behavior. IEEE Transactions on Software Engineering 45, 3 (2017), 261–284.

[8] Victor R Basili1 Gianluigi Caldiera and H Dieter Rombach. 1994. The goal ques-
tion metric approach. Encyclopedia of software engineering (1994), 528–532.
[9] José Campos, Yan Ge, Gordon Fraser, Marcelo Eler, and Andrea Arcuri. 2017. An
Empirical Evaluation of Evolutionary Algorithms for Test Suite Generation. In
Proceedings of the 9th International Symposium on Search Based Software Engi-
neering SSBSE 2017. 33–48. https://doi.org/10.1007/978-3-319-66299-2_3
[10] Mariano Ceccato, Alessandro Marchetto, Leonardo Mariani, Cu D Nguyen, and
Paolo Tonella. 2015. Do automatically generated test cases make debugging eas-
ier? an experimental assessment of debugging eﬀectiveness and eﬃciency. ACM
Transactions on Software Engineering and Methodology (TOSEM) 25, 1 (2015), 1–
38.

[11] William Jay Conover. 1999. Practical nonparametric statistics (3. ed ed.). Wiley,

New York, NY [u.a.].

[12] Mahmoud O Elish and David Rine. 2006. Design structural stability metrics
and post-release defect density: An empirical study. In 2006 30th Annual Inter-
national Computer Software and Applications Conference (COMPSAC’06 Supple-
ment). IEEE, 1–8.

[13] Hakan Erdogmus, Maurizio Morisio, and Marco Torchiano. 2005. On the eﬀec-
tiveness of the test-ﬁrst approach to programming. IEEE Transactions on software
Engineering 31, 3 (2005), 226–237.

[14] Javier Ferrer, Francisco Chicano, and Enrique Alba. 2012. Evolutionary Algo-
rithms for the Multi-objective Test Data Generation Problem. Softw. Pract. Exper.
42, 11 (Nov. 2012), 1331–1362. https://doi.org/10.1002/spe.1135

[15] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Gen-
eration for Object-oriented Software. In Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of Software Engi-
neering (Szeged, Hungary) (ESEC/FSE ’11). ACM, New York, NY, USA, 416–419.
https://doi.org/10.1145/2025113.2025179

[16] Gordon Fraser and Andrea Arcuri. 2013. Whole Test Suite Generation.

IEEE
Trans. Softw. Eng. 39, 2 (Feb. 2013), 276–291. https://doi.org/10.1109/TSE.2012.14
[17] Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated
unit test generation using evosuite. ACM Transactions on Software Engineering
and Methodology (TOSEM) 24, 2 (2014), 1–42.

[18] Gordon Fraser and Andrea Arcuri. 2015. Achieving scalable mutation-based
generation of whole test suites. Empirical Software Engineering 20, 3 (2015), 783–
812.

[19] Enrico Fregnan, Tobias Baum, Fabio Palomba, and Alberto Bacchelli. 2019. A
survey on software coupling relations and tools. Information and Software Tech-
nology 107 (2019), 159–178.

[20] Vahid Garousi and Barış Küçük. 2018. Smells in software test code: A survey of
knowledge in industry and academia. Journal of systems and software 138 (2018),
52–81.

[21] David E. Goldberg. 1989. Genetic Algorithms in Search, Optimization and Machine
Learning (1st ed.). Addison-Wesley Longman Publishing Co., Inc., Boston, MA,
USA.

[22] Giovanni Grano, Cristian De Iaco, Fabio Palomba, and Harald C Gall. 2020. Pizza
versus Pinsa: On the Perception and Measurability of Unit Test Code Quality. In
2020 IEEE International Conference on Software Maintenance and Evolution (IC-
SME). IEEE, 336–347.

[23] Giovanni Grano, Christoph Laaber, Annibale Panichella, and Sebastiano
Testing with fewer resources: An adaptive approach to
IEEE Transactions on Software Engi-

Panichella. 2019.
performance-aware test case generation.
neering (2019).

[24] Giovanni Grano, Fabio Palomba, Dario Di Nucci, Andrea De Lucia, and Harald C
Gall. 2019. Scented since the beginning: On the diﬀuseness of test smells in
automatically generated test code. Journal of Systems and Software 156 (2019),
312–327.

[25] Lucas Gren and Vard Antinyan. 2017. On the relation between unit testing and
code quality. In 2017 43rd Euromicro Conference on Software Engineering and Ad-
vanced Applications (SEAA). IEEE, 52–56.

[26] Sarra Habchi, Guillaume Haben, Mike Papadakis, Maxime Cordy, and Yves Le
Traon. 2021. A Qualitative Study on the Sources, Impacts, and Mitigation Strate-
gies of Flaky Tests. arXiv preprint arXiv:2112.04919 (2021).

[27] Mary Jean Harrold, John D McGregor, and Kevin J Fitzpatrick. 1992. Incremental
testing of object-oriented class structures. In Proceedings of the 14th international
conference on Software engineering. 68–80.

[28] J. J Heckman. 1990. Selection bias and self-selection. In Econometrics. Springer,

201–224.

[29] K. J Hunt, N. Shlomo, and J. Addington-Hall. 2013. Participant recruitment in
sensitive surveys: a comparative trial of ‘opt in’versus ‘opt out’approaches. BMC
Medical Research Methodology 13, 1 (2013), 1–8.

[30] Joshua D Knowles and David W Corne. 2000. Approximating the nondominated
front using the Pareto archived evolution strategy. Evolutionary computation 8,
2 (2000), 149–172.

[31] Kiran Lakhotia, Mark Harman, and Phil McMinn. 2007. A multi-objective ap-
proach to search-based test data generation. In Proceedings of the 9th annual
conference on Genetic and evolutionary computation. 1098–1105.

[32] Phil McMinn. 2004.

Search-based Software Test Data Generation:
Softw. Test. Verif. Reliab. 14, 2 (June 2004), 105–156.

A Survey.
https://doi.org/10.1002/stvr.v14:2

[33] Glenford J Myers, Corey Sandler, and Tom Badgett. 2011. The art of software

testing. John Wiley & Sons.

[34] Alessandro Orso and Sergio Silva. 1998. Open Issues and Research Directions
in Object-Oriented Testing. In Proceedings of the 4th International Conference
on” Achieving Quality in Software: Software Quality in the Communication Soci-
ety”(AQUIS’98).

[35] Norbert Oster and Francesca Saglietti. 2006. Automatic test data generation
by multi-objective optimisation. In International Conference on Computer Safety,
Reliability, and Security. Springer, 426–438.

[36] Fabio Palomba, Annibale Panichella, Andy Zaidman, Rocco Oliveto, and Andrea
De Lucia. 2016. Automatic Test Case Generation: What if Test Code Quality
Matters?. In Proceedings of the 25th International Symposium on Software Testing
and Analysis (Saarbr&#252;cken, Germany) (ISSTA 2016). ACM, New York, NY,

Toward Granular Automatic Unit Test Case Generation

MSR’22, May 23-24, 2022, Pittsburgh, PA, USA

USA, 130–141. https://doi.org/10.1145/2931037.2931057

[37] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-
mulating Branch Coverage as a Many-Objective Optimization Problem. In ICST.
IEEE Computer Society, 1–10.

[38] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-
mulating Branch Coverage as a Many-Objective Optimization Problem. In ICST.
IEEE Computer Society, 1–10.

[39] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2018. Au-
tomated test case generation as a many-objective optimisation problem with
dynamic selection of the targets. IEEE Transactions on Software Engineering 44,
2 (2018), 122–158.

[40] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2018. Incre-
mental Control Dependency Frontier Exploration for Many-Criteria Test Case
Generation. In International Symposium on Search Based Software Engineering.
Springer, 309–324.

[41] Fabiano Pecorelli, Fabio Palomba, and Andrea De Lucia. 2021. The relation of
test-related factors to software quality: A case study on apache systems. Empir-
ical Software Engineering 26, 2 (2021), 1–42.

[42] Anthony Peruma, Khalid Almalki, Christian D Newman, Mohamed Wiem
Mkaouer, Ali Ouni, and Fabio Palomba. 2020. Tsdetect: An open source test
smells detection tool. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 1650–1654.

[43] Mauro Pezzè and Michal Young. 2008. Software testing and analysis: process,

principles, and techniques. John Wiley & Sons.

[44] Gustavo HL Pinto and Silvia R Vergilio. 2010. A multi-objective genetic algo-
rithm to test data generation. In 2010 22nd IEEE International Conference on Tools
with Artiﬁcial Intelligence, Vol. 1. IEEE, 129–134.

[45] Brittany Reid, Markus Wagner, Marcelo d’Amorim, and Christoph Treude. 2022.
Software Engineering User Study Recruitment on Proliﬁc: An Experience Report.
arXiv preprint arXiv:2201.05348 (2022).

[46] Simone Scalabrino, Giovanni Grano, Dario Di Nucci, Rocco Oliveto, and Andrea
De Lucia. 2016. Search-Based Testing of Procedural Programs: Iterative Single-
Target or Multi-target Approach?. In Search Based Software Engineering, Federica
Sarro and Kalyanmoy Deb (Eds.). Springer International Publishing, Cham, 64–
79.

[47] Davide Spadini, Fabio Palomba, Andy Zaidman, Magiel Bruntink, and Alberto
Bacchelli. 2018. On the relation of test smells to software code quality. In 2018
IEEE International Conference on Software Maintenance and Evolution (ICSME).
IEEE, 1–12.

[48] Diomidis Spinellis. 2005. Tool writing: a forgotten art?(software tools).

IEEE

Software 22, 4 (2005), 9–11.

[49] Ramanath Subramanyam and Mayuram S. Krishnan. 2003. Empirical analysis
of ck metrics for object-oriented design complexity: Implications for software
defects. IEEE Transactions on software engineering 29, 4 (2003), 297–310.

[50] Arie Van Deursen, Leon Moonen, Alex van den Bergh, and Gerard Kok. 2001.
Refactoring test code. In Proceedings of the 2nd international conference on ex-
treme programming and ﬂexible processes in software engineering (XP2001). 92–
95.

[51] Christian von Lücken, Benjamín Barán, and Carlos Brizuela. 2014. A survey on
multi-objective evolutionary algorithms for many-objective problems. Compu-
tational optimization and applications 58, 3 (2014), 707–756.

[52] Stefan Wappler and Frank Lammermann. 2005. Using Evolutionary Algorithms
for the Unit Testing of Object-Oriented Software. In Proceedings of the 7th Annual
Conference on Genetic and Evolutionary Computation (Washington DC, USA)
(GECCO ’05). Association for Computing Machinery, New York, NY, USA, 1053–
1060. https://doi.org/10.1145/1068009.1068187

[53] Laurie Williams, Gunnar Kudrjavets, and Nachiappan Nagappan. 2009. On the
eﬀectiveness of unit test automation at microsoft. In 2009 20th International Sym-
posium on Software Reliability Engineering. IEEE, 81–89.

[54] Shayan Zamani and Hadi Hemmati. 2020. A Cost-Eﬀective Approach for Hyper-
Parameter Tuning in Search-based Test Case Generation. In 2020 IEEE Interna-
tional Conference on Software Maintenance and Evolution (ICSME). IEEE, 418–
429.

