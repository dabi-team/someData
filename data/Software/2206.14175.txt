2
2
0
2

n
u
J

9
2

]
E
S
.
s
c
[

2
v
5
7
1
4
1
.
6
0
2
2
:
v
i
X
r
a

InvAASTCluster : On Applying Invariant-Based Program
Clustering to Introductory Programming Assignments

PEDRO ORVALHO, INESC-ID/IST - U. Lisboa, Portugal
MIKOLÃÅ  JANOTA, Czech Technical University in Prague, Czech Republic
VASCO MANQUINHO, INESC-ID/IST - U. Lisboa, Portugal

Due to the vast number of students enrolled in Massive Open Online Courses (MOOCs), there has been an
increasing number of automated program repair techniques focused on introductory programming assignments
(IPAs). Such state-of-the-art techniques use program clustering to take advantage of previous correct student
implementations to repair a given new incorrect submission. Usually, these repair techniques use clustering
methods since analyzing all available correct student submissions to repair a program is not feasible. The
clustering methods use program representations based on several features such as abstract syntax tree
(AST), syntax, control flow, and data flow. However, these features are sometimes brittle when representing
semantically similar programs.

This paper proposes InvAASTCluster, a novel approach for program clustering that takes advantage of
dynamically generated program invariants observed over several program executions to cluster semantically
equivalent IPAs. Our main objective is to find a more suitable representation of programs using a combination
of the programâ€™s semantics, through its invariants, and its structure, through its anonymized abstract syntax
tree. The evaluation of InvAASTCluster shows that the proposed program representation outperforms
syntax-based representations when clustering a set of different correct IPAs. Furthermore, we integrate
InvAASTCluster into a state-of-the-art clustering-based program repair tool and evaluate it on a set of IPAs.
Our results show that InvAASTCluster advances the current state-of-the-art when used by clustering-based
program repair tools by repairing a larger number of studentsâ€™ programs in a shorter amount of time.
CCS Concepts: â€¢ Applied computing â†’ Computer-assisted instruction; â€¢ Theory of computation â†’
Program semantics; Invariants; Program analysis.

Additional Key Words and Phrases: Program Clustering, Program Invariants, Program Equivalence, Program
Repair, Programming Education, MOOCS

1 INTRODUCTION

Nowadays, thousands of students enroll every year in programming-oriented Massive Open Online
Courses (MOOCs) [15]. On top of that, due to the current pandemic situation, even small-sized
programming courses are being taught online. Providing feedback to novice students in introductory
programming assignments (IPAs) in these courses requires substantial effort and time by the faculty.
Hence, there is an increasing need for automated semantic program repair frameworks [14, 15, 21,
27, 28, 30, 43, 44] capable of providing automated, comprehensive, and personalized feedback to
students in incorrect solutions to programming assignments.

Over the last few years, several program repair tools [15, 19, 32, 41] have exploited a large
number of previously enrolled students to obtain diverse correct implementations for each IPA.
Given an incorrect student submission, these frameworks use clustering methods to find the most
similar correct submission from previous years to provide a minimal set of repairs to the student.
Typically, having a similar correct implementation allows computing a smaller set of repairs to fix
a given incorrect program rather than always using the set of repairs needed to make the incorrect
submission semantically equivalent to a fixed reference solution. However, analyzing all previous
correct student submissions for an IPA is not feasible. To tackle this problem, different program

Authorsâ€™ addresses: Pedro Orvalho, pmorvalho@tecnico.ulisboa.pt, INESC-ID/IST - U. Lisboa, Lisboa, Portugal; MikolÃ¡Å¡
Janota, mikolas.janota@cvut.cz, Czech Technical University in Prague, Prague, Czech Republic; Vasco Manquinho, vasco.
manquinho@tecnico.ulisboa.pt, INESC-ID/IST - U. Lisboa, Lisboa, Portugal.

 
 
 
 
 
 
2

P. Orvalho, M. Janota, and V. Manquinho

clustering approaches have been proposed to use in program repair tools which enable focusing
only on the representatives of the clusters. Clara [15] clusters the correct programs based on their
dynamic equivalence [29] and control flow, i.e., the order in which program statements, instructions
and function calls are executed. Sarfgen [41] computes program representations based on each
programâ€™s abstract syntax tree. SemCluster [32] also uses each programâ€™s control flow. However,
SemCluster uses each programâ€™s data flow which tracks the number of occurrences of consecutive
values a variable takes during its lifetime.

The problem of program equivalence, i.e., deciding if two programs are equivalent, is undecid-
able [6, 34]. On that account, finding an adequate representation for programs that performs well
on program clustering is a challenging problem. The above-mentioned program representations
used in the field of program repair may be fragile, as we are going to show in Section 2. To address
this problem, our work proposes the use of dynamically-generated program invariants to cluster
semantically equivalent programs, overcoming some of the identified weaknesses. A program
invariant is a condition that must always be true at a given step of the program during its execution
(see Section 3.2). Program invariants are usually used to assert some assurances throughout a
program (assertions).

This paper proposes to leverage the information of a programâ€™s structure using its abstract syntax
tree (AST) together with semantic information provided by its set of invariants. Previous research
has been conducted regarding the use of invariants to promote patch diversity (i.e. diversity in the
set of possible repairs to a given incorrect program) on search-based program repair [5, 9, 42]. These
works use Daikon [10] to generate invariant sets for each possible patch. Daikon is a system that
infers likely dynamically generated invariants observed over several program executions. Therefore,
these invariants are dependent on the program executions. Nevertheless, previous work [5] showed
promising results in using invariants to semantically cluster patches to provide the user with a
semantic reason for a set of similar patches.

This paper presents a novel approach for clustering introductory programming assignments
(IPAs) leveraging their sets of invariants. Our approach for clustering IPAs also takes into account
each programâ€™s code and anonymized abstract syntax tree (AAST) (see Section 4.2). The main
contribution of this work is a vector representation of programs based on the programsâ€™ invariants
and their AASTs, bringing together the programsâ€™ semantics and syntactic features. The proposed
clustering technique has been implemented in a framework InvAASTCluster. This tool has been
designed as an independent clustering tool. Therefore, it can be used to help evaluate studentsâ€™
submissions for IPAs by clustering semantically equivalent solutions for programming exercises,
although InvAASTCluster can also be easily integrated into any clustering-based program repair
tool for IPAs. Furthermore, InvAASTCluster can even be used in a plagiarism detection tool, like
Moss [35].

Figure 1 shows the generic idea of clustering-based program repair frameworks [15, 32, 41]. These
frameworks [15, 32, 41] receive an incorrect student submission, a test suite, and a collection of ğ‘
correct student submissions for the same IPA. For scalability concerns, these frameworks eliminate,
through clustering techniques, semantically equivalent solutions, i.e., dynamically equivalent
correct programs given the provided input-output test suite. Those clustering approaches try to
aggregate the set of ğ‘ correct solutions into ğ¾ semantically different clusters (ğ‘ â‰« ğ¾). Finally,
the repair tool uses these ğ¾ clustersâ€™ representatives to repair the provided incorrect student
submission. As Figure 1 shows, InvAASTCluster can be used as the clustering technique of those
clustering-based program repair tools. However, some program repair tools [2, 23] use a single
reference implementation provided by the lecturer to repair a studentâ€™s program. These tools
usually are only able to use one correct implementation to repair each program. Therefore, given
an incorrect submission, InvAASTCluster was designed to be also capable of finding on a set

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

3

Fig. 1. Clustering-based Program Repair.

of correct student submissions which submission is the closest correct solution to the incorrect
program, i.e., a specific reference implementation for each incorrect submission, that may require
fewer changes to fix the program.

We evaluate InvAASTCluster on a set of real-world student programs developed during a univer-
sity introductory programming course. Experimental results show that the proposed invariant-based
representation improves upon syntax-based representations when performing program clustering.
We integrate InvAASTCluster into Clara, a clustering-based program repair tool, in order to
compare our clustering technique against Claraâ€™s clustering method, which is the current publicly
available state-of-the-art method for clustering IPAs.

To summarize, this paper makes the following contributions:

â€¢ We propose a novel and efficient approach for clustering submissions for introductory program-
ming assignments (IPAs) based on the submissionsâ€™ sets of invariants and AASTs representations.
â€¢ We present a study showing the results of using our program clustering tool, InvAASTCluster,
on a set of 1620 real-world IPAs correct submissions to show the effectiveness of invariant-based
program clustering.

â€¢ We compare InvAASTCluster with the clustering method used by the currently available
state-of-the-art program repair tools. Experimental results show that using our technique for
program clustering or finding the closest correct solution, InvAASTCluster outperforms current
state-of-the-art clustering methods.

â€¢ The InvAASTCluster framework is publicly available on GitHub at https://github.com/pmorva-

lho/InvAASTCluster.
The structure of the remainder of this paper is as follows. First, Section 2 illustrates the strengths of
using invariants for program representation. Section 3 presents important concepts used throughout
this manuscript and describes how to gather and represent sets of program invariants. Section 4
discusses several program representations, including a new invariant-based program representation.
Section 5 discusses the implementation of InvAASTCluster. Section 6 presents the experimental
evaluation that supports our claim that invariant-based program representations are beneficial to
cluster semantically programming assignments. Finally, Section 7 presents the related work, and
the paper concludes in Section 8.

2 MOTIVATION

Current program representations for repairing studentsâ€™ programming assignments leverage certain
program features, such as code syntax [16], abstract syntax tree [41], control flow [15] and data

Correct Submissions1.2.N.InvAASTClusterIncorrectSubmissionTest SuiteSubmissionRepairedRepairFrameworkK correct programsrepresentatives4

P. Orvalho, M. Janota, and V. Manquinho

flow [32], to encode each program into a vector representation. However, all of these features have
some weaknesses when we want to cluster programs based on their semantics.

Example 1. Consider the following two programs written in C:

1

2

3

4

5

6

7

8

int n , sum = 0, i;
scanf ("%d" , &n );
i = 0;
while (i < n) {

i ++;
sum = sum + i;

}
printf ("%d\n" , sum );

1

2

3

4

5

6

7

8

int j , n , s = 0;
scanf ("%d" , &n );

for (j = n; j > 0; j - -)
{

s = j + s;

}
printf ("%d\n" , s );

Both programs in Example 1 compute the sum of all the natural numbers from 1 to a given
number n i.e. (cid:205)ğ‘›
ğ‘–. Observe that the program on the left uses a while-loop that iterates over the
ğ‘–=1
natural numbers from 0 to n. However, the program on the right uses a for-loop that iterates from
n to 0 in decreasing order. Hence, both programs are semantically equivalent since both have the
same result. Nevertheless, if we build a program representation using the programsâ€™ syntax or
abstract syntax trees, both programs will have very different representations. In terms of syntax,
the names of the used variables (e.g. i, j, s, sum) and structures (e.g. while, for) are different.
Additionally, in terms of data flow and dynamic equivalence, both programs are also different since,
for example, the values assigned to the variable i go from 0 to n in the first program while in the
other the variable j is assigned the same values but in decreasing order.

Consider that the variable n is always assigned to a natural number, ğ‘› > 0. In the first program, if
a dynamic invariant detector (e.g. Daikon [10]) is used, the following set of invariants is observed:
â€¢ In the first program, at each iteration of the while-loop: ğ‘› > 0; ğ‘ ğ‘¢ğ‘š â‰¥ 0; 0 â‰¤ ğ‘– â‰¤ ğ‘›.
â€¢ In the second program, at each iteration of the for-loop: ğ‘› > 0; ğ‘  â‰¥ 0; 0 â‰¤ ğ‘— â‰¤ ğ‘›.
Therefore, after renaming some variables (ğ‘ ğ‘¢ğ‘š â†’ ğ‘ ; ğ‘– â†’ ğ‘—), these two sets of invariants would
be considered equivalent. Hence, using sets of invariants allows finding semantically equivalent
programs that can differ in their syntax and/or in their data flow.

Hence, the idea of this paper is to improve the semantic representation of IPAs using their sets
of invariants. These invariants are dynamically detected by Daikon [10] over several program
executions using a predefined set of test cases for each programming assignment. In addition to
the set of invariants, which provides semantic information about a program, the idea is also to
leverage the information of a programâ€™s anonymized abstract syntax tree (AAST), i.e. an AST after
removing all the variablesâ€™ names.

3 SYNTAX TREES AND INVARIANTS

This section provides some background on syntax trees and program invariants that will be used
throughout this paper.

3.1 Definitions
Definition 3.1 (Context-free Grammar (CFG)). A context-free grammar ğ”– is a 4-tuple (ğ‘‰ , Î£, ğ‘…, ğ‘†),
where ğ‘‰ is the set of non-terminals symbols, Î£ is the set of terminal symbols, ğ‘… is the set of rules and
ğ‘† is the start symbol. A CFG describes all the strings permitted in a certain formal language [18].

Definition 3.2 (Domain-Specific Language (DSL)). A Domain-specific Language (DSL) is a
tuple (ğ”–, Ops), where ğ”– is a context-free grammar (ğ”– = (ğ‘‰ , Î£, ğ‘…, ğ‘†)) and Ops is the semantics of

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

5

decl

decl

ğ‘–

id

ğ‘–ğ‘›ğ‘¡

type

ğ¼ğ· id

ğ‘–ğ‘›ğ‘¡

type

(a) AST representation.

(b) AAST representation.

Fig. 2. A small example of an AST and an AAST for the variable declaration, int i. An integer variable with
identifier ğ‘–.

DSL operators. The CFG ğ”– has the rules to generate all the programs in the DSL. The semantics of
DSL operators is necessary to analyze conflicts and make deductions.

Definition 3.3 (Abstract Syntax Tree (AST)). An abstract syntax tree (AST) is a syntax tree in
which each node represents an operation, and the children of the node represent the arguments of
the operation for a given programming language described by a Context-free Grammar [18]. An
AST depicts a programâ€™s grammatical structure [3].

Figure 2a presents a small example of the AST representation for the variable declaration int i.
Definition 3.4 (Anonymized Abstract Syntax Tree (AAST)). An anonymized abstract syntax
tree (AAST) is an AST in which nodes that have identifiers are anonymized, i.e., a nodeâ€™s identifier
(name of a function or variable) is replaced by a special token (ID).

Figure 2b shows the AAST representation for the same declaration presented previously, int i.
Definition 3.5 (Bag of Words (BoW)). A Bag of Words (BoW) representation [17] is a vector
representation where a tokenized sentence is represented as a bag of its words in a vector. The
vector representation contains information on the number of times each token in the language
appears in the sentence. Note that this model does not take into consideration the languageâ€™s
grammar and even word order. The tokenization step divides a string into ğ‘›-grams, which are
sub-sequences of the original string of ğ‘› items.

The following example presents a small illustration of a vector representation of a phrase using

a BoW model.
Example 2. Let ğ‘‰ be a language that contains only 5 symbols in its vocabulary i.e., Î£ğ‘‰ =
{ğ‘, ğ‘’, ğ‘–, ğ‘œ, ğ‘¢}. Let ğµğ‘‰ be a bag of words model computed using the following sentences written
in ğ‘‰ : {â€ğ‘ ğ‘â€, â€ğ‘’ ğ‘–â€, â€ğ‘ ğ‘’ ğ‘– ğ‘œ ğ‘¢â€, â€ğ‘œ ğ‘–â€}.

Given the phrase ğ‘ = â€ğ‘ ğ‘– ğ‘ ğ‘¢â€, the vector representation of ğ‘ is, ğµğ‘‰ (ğ‘) = [0.5, 0.0, 0.25,-
0.0, 0.25]. The size of ğµğ‘‰ (ğ‘) is 5 since 5 is the size of Î£ğ‘‰ . For each entry ğ‘  of ğµğ‘‰ (ğ‘), ğµğ‘‰ (ğ‘) [ğ‘ ]
corresponds to the percentage of ğ‘ that is equal to ğ‘ . For example, the symbol ğ‘ appears twice in a
four-symbol phrase. Hence ğµğ‘‰ (ğ‘) [â€ğ‘â€] = 0.5.

3.2 Program Invariants

Program invariants are conditions that must always be true at a given point during a programâ€™s
execution. Dynamically generated program invariants are likely invariants observed during several
program executions for a given program. The dynamically generated set of program invariants
provides information about a programâ€™s behavior, i.e. its semantics. If two programs share the
same set of program invariants, they are likely semantically equivalent. Hence, an invariant-
based representation of programs should allow to find out which student submissions in a given
programming assignment have the same or similar behavior.

6

P. Orvalho, M. Janota, and V. Manquinho

To compare two sets of program invariants, a relation between the variables in both sets is
required. We propose to rename all the variables in a program based on the variablesâ€™ type and
usage. All the variables are renamed the first time they are assigned to some value in a program. The
variableâ€™s new name is a concatenation between its type and a counter for how many variables have
already been renamed in the program. With this technique of variable renaming, two programsâ€™
sets of invariants can be easily compared. This method is very simple and fragile, although IPAs
are usually relatively small and simple imperative programs. Therefore, this naive approach should
work for IPAs.

Example 3. Consider again the programs presented in Example 1, after renaming all the variables
based on their usage, the following mapping of variables for the first program is obtained {ğ‘ ğ‘¢ğ‘š â†’
ğ‘–ğ‘›ğ‘¡0; ğ‘› â†’ ğ‘–ğ‘›ğ‘¡1; ğ‘– â†’ ğ‘–ğ‘›ğ‘¡2}. Regarding the second program the mapping is {ğ‘  â†’ ğ‘–ğ‘›ğ‘¡0; ğ‘› â†’ ğ‘–ğ‘›ğ‘¡1;
ğ‘— â†’ ğ‘–ğ‘›ğ‘¡2}. The two programs after renaming are:

1

2

3

4

5

6

7

8

int int1 , int0 = 0, int2 ;
scanf ("%d" , & int1 );
int2 = 0;
while ( int2 < int1 ){

int2 ++;
int0 = int0 + int2 ;

}
printf ("%d\n" , int0 );

1

2

3

4

5

6

7

8

int int2 , int1 , int0 = 0;
scanf ("%d" , & int1 );

for ( int2 = int1 ; int2 >= 0; int2 - -)
{

int0 = int2 + int0 ;

}
printf ("%d\n" , int0 );

Hence, the set of invariants of both cycles (for and while) would be the same: {ğ‘–ğ‘›ğ‘¡1 > 0; ğ‘–ğ‘›ğ‘¡0 â‰¥ 0;

0 â‰¤ ğ‘–ğ‘›ğ‘¡2 â‰¤ ğ‘–ğ‘›ğ‘¡1}.

In this work, we use Daikon [10] to compute dynamically-generated likely invariants observed
over several program executions for each student submission using a set of predefined input-output
tests for each programming assignment. First, the method for renaming variables is applied to
all the studentsâ€™ submissions. To use Daikon on small imperative C programs, one must inject
empty functions into each scope and pass the scopeâ€™s variables as parameters. Afterward, Daikon
is executed using all the input tests for each programming assignment. The dynamically-generated
invariants produced by Daikon are saved for each programâ€™s structure/scope (e.g. if, cycle,
block). We donâ€™t specifically ask Daikon to generate any type of invariant. The only type of
invariants we turned off is the â€œOneOfâ€ invariants (e.g. â€œx is OneOf {1,2}â€) that may cause overfitting
to the test suite.

4 PROGRAM REPRESENTATIONS
In this work, each program is represented as a feature vector. In particular, we propose to use a bag
of words (BoW) model (see Definition 3.5). Using BoW models, we generate vector representations
for each student submission based on several features. These features may include the Abstract
Syntax Tree (AST), set of invariants, or even the program code. It is also possible to combine several
of these features. Next, all the vector representations used in this work are described.

4.1 Syntax Vectors

The syntax vector program representation is the simplest to compute since it is based solely on the
program syntax (code). In the interest of comparing the syntax of programs independently of the
variablesâ€™ names, first, all the programming solutions are renamed using the method described in

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

7

Section 3.2. Next, all the student submissions are tokenized, and a vocabulary with all the available
tokens is obtained. Then, vectors for each student submission are created, where the ğ‘–ğ‘¡â„ entry is the
number of times the ğ‘–ğ‘¡â„ word of the vocabulary that appears in the program. Finally, the numbers
of occurrences in these vectors are normalized.

4.2 Anonymized Abstract Syntax Tree Vectors

An alternative representation is to compute a bag of words using the strings of the abstract syntax
trees (AST) of all submissions for a given programming assignment. This representation has already
been used in program clustering [20, 41]. However, we represent each AST as a string and remove
all names of variables and functions, keeping only their respective types in the AST. Thus, for
each submission, we have an anonymized abstract syntax tree (AAST) (see Definition 3.4). With
these AASTs, we keep the information about a programâ€™s structure, ignoring the name of its
variables. The information about a programâ€™s structure is kept since an AAST contains all the
non-terminal symbols of a languageâ€™s grammar. Next, a vocabulary is built with the tokens present
in all submissions and a normalized vector representation for each AAST is computed.

4.3 Invariant Vectors

Another approach is to use an invariant-based vector representation. In this case, we apply the
bag of words model to the set of invariants of the programs. We gather all program invariants
as described in Section 3.2. Previous work on the use of invariants to detect semantic similarity
between possible patches to a program [5] showed that using string distance measure between
invariant sets had similar results and was more efficient than computing the logical similarity
between their corresponding sets of program invariants. Therefore, we represent our invariants in
the form of strings. However, instead of using a string distance measure between invariant sets
(e.g. Levenshtein edit distance [22]), we create a bag of words model with those sets of program
invariants.

4.4 Combination of Program Features
Finally, observe that these vector representations (Syntax, AAST, Invariants) can be combined, thus
taking advantage of using several types of features. For example, we use in our work a bag of
words using the programâ€™s AST and the sets of invariants. In this case, first, we build two BoW
representations independently, one based on AASTs and another one based on invariants. Then,
we concatenate, for each submission, the submissionâ€™s vector representations using the two BoWs,
achieving a vector representation based on the programâ€™s AAST and set of invariants. The program
syntax was not included in this last representation since the BoW based on syntax has a large
vocabulary that generates vectors too sparse.

5 IMPLEMENTATION

This section presents the implementation of our program clustering technique. We implemented
the proposed approach in the tool InvAASTCluster (Invariants and AAST Program Clustering).
InvAASTCluster is publicly available on GitHub at https://github.com/pmorvalho/InvAASTClus-
ter. Figure 3 shows the overall architecture of the tool. Given a set of ğ‘ correct submissions and
a test suite, InvAASTCluster computes ğ¾ clusters of programs (ğ‘ â‰¥ ğ¾) and returns the set of
ğ¾ clustersâ€™ representatives, i.e. the set of correct programs that are in the center of each one of
the ğ¾ clusters. InvAASTCluster is divided into six main modules: variable renamer, invariants
detector, AASTs processor, bag of words maker, clustering procedure and the selection of each
cluster representative.

8

P. Orvalho, M. Janota, and V. Manquinho

Fig. 3. High-level overview of InvAASTCluster.

Variable Renamer. In this module, InvAASTCluster renames all variables of each one of the
ğ‘ given correct submissions. All variables are renamed based on their usage in each program,
as explained in Section 3.2. Hence, InvAASTCluster uses pycparser1 to find all variables in a
program. Then, when a variable is first used in the program (e.g. assignment) that variable receives
a new name taking into consideration the variableâ€™s type.

Invariants Detector. InvAASTCluster uses Daikon [10] to compute dynamically-generated
invariants for a given test suite. After all the variables have been renamed, this module produces a
set of invariants for each programâ€™s scope using the provided test suite. All these sets of invariants
are then sent to the BoW maker module.

AAST Processor. In this step, InvAASTCluster also uses pycparser to compute a programâ€™s
abstract syntax tree (AST). Additionally, InvAASTCluster removes from the AST all the variablesâ€™
and functionsâ€™ identifiers to transform the programâ€™s AST into an anonymized abstract syntax tree
(AAST), conserving only the programâ€™s structure.

Bag of Words (BoW) Maker. This module receives three sets as input: (1) the set of correct
program submissions with all their variables renamed from the Variable Renamer module; (2) all the
programâ€™s AASTs from the AAST processor and (3) the set of the programsâ€™ dynamically-generated
invariants. The BoW Maker computes the bag of words (BoW) model that is going to be used to
generate vector representations for each program. Depending on this moduleâ€™s parameterization,
the BoW maker can compute four different bags of words: (1) based on the programsâ€™ code (syntax),
(2) based on the programsâ€™ AASTs (structure), (3) using the set of programsâ€™ invariants (semantics)
and (4) joining the programsâ€™ AASTs and their sets of invariants (structure + semantics). To compute
these BoW models, InvAASTCluster uses scikit-learn package, feature_extraction2.

InvAASTCluster tokenizes the input strings, into tokens of size ğ‘› (ğ‘›-grams), to build a vo-
cabulary with all the submissionsâ€™ information, i.e. invariants, syntax, or AASTs. In our case, we
define ğ‘› = 3 (3-grams) for this parameter of the BoW maker. Afterward, once a vocabulary has
been collected, InvAASTCluster computes for each program a vector representation by counting

1https://github.com/eliben/pycparser
2sklearn.feature_extraction.text.TfidfVectorizer

1.2.2.1.3.Correct SubmissionsInvAASTClusterVariableRenamerInvariantsDetectorAASTProcessorBoW MakerClusteringProgramInvariants AASTProgram CodeN Vectors (Programs) 1.2.4.N.A.K Clustersâ€™Representatives2.1.B.2.1.K.C.K ClustersCluster 1Cluster KCluster 2Test SuiteInvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

9

Fig. 4. Finding the closest correct program, i.e. which is the closest correct program representative to the
incorrect submission vector representation. This approach passes only one program to the repair tool instead
of ğ¾ programs.

the number of times each token appears in the programâ€™s information string (invariants, syntax, or
AASTs) and normalizing the vector by the length of the BoWâ€™s vocabulary.

Clustering. The number of desired clusters ğ¾ is also a parameter of InvAASTCluster, although
by default InvAASTCluster searches for a number of clusters that corresponds to 10% of ğ‘ . The
BoW maker module passes to the clustering procedure the set of vector representations for each
one of the ğ‘ correct submissions. Then, InvAASTCluster uses the KMeans algorithm to cluster
these submissions into ğ¾ different clusters. The KMeans algorithm receives as a parameter the
number of clusters it should return (ğ¾). The KMeans algorithm divides the set of observations, in
our case studentsâ€™ programs, into ğ¾ clusters where each program is assigned to the cluster with
the nearest mean [37]. InvAASTCluster uses KMeans however other clustering algorithms can be
applied.

Clustersâ€™ representatives selection. In this last module, InvAASTCluster chooses a program
representative for each cluster, i.e. for each one of the ğ¾ clusters, InvAASTCluster computes
which is the program that is closest to the clusterâ€™s center. To compute these distances InvAAST-
Cluster calculates the Euclidean distance between the programsâ€™ vector representation of each
cluster. Afterward, InvAASTCluster returns a set of ğ¾ clustersâ€™ representatives.

Easy upgradability. InvAASTCluster was designed with modularity in mind. On that account,
one can easily remove, add or modify any module of InvAASTCluster. For example, one can
choose to use other models instead of the bag of words model only needing to replace that specific
procedure.

Several program repair tools [2, 19, 23] only accept one correct program to act as a reference
implementation for repairing a given incorrect program. Hence, these frameworks cannot take
advantage of a vast number of semantically different student correct submissions. In order to
integrate InvAASTCluster in these frameworks, we developed an additional module that finds
the closest correct program representative to an incorrect program.

Closest correct program finder. The overall idea of this additional module is presented in Fig-
ure 4. Given a studentâ€™s incorrect submission, InvAASTCluster finds which of the ğ¾ clustersâ€™
representatives, returned by InvAASTClusterâ€™s selection module, is the closest program to the

InvAASTClusterIncorrect SubmissionSubmissionRepairedRepairFrameworkK clustersâ€™Representatives 2.1.K.BoW MakerInc. sub.C.10

P. Orvalho, M. Janota, and V. Manquinho

Table 1. Description of our dataset of IPAs.

Labs

#IPAs

#Correct
Submissions

#Incorrect
Submissions

#IPAs
(Clara)

Lab02
Lab03
Lab04
Total

10
7
8
25

789
363
468
1620

118
35
43
196

10
5
5
20

#Correct
Submissions
(Clara)
738
244
159
1141

incorrect submission. This is done by identifying the smallest Euclidean distance between the
vector representation of each one of the clustersâ€™ representatives and the incorrect submission.
Hence, we can identify one correct program that is most likely the reference implementation to use
for repairing a specific studentâ€™s program. In the example of Figure 4, InvAASTCluster would
return only the ğ¶ğ‘¡â„ program to the repair framework since it is the closest program to the incorrect
submission.

6 EXPERIMENTS

The experimental results presented in this section aim to support our claims that the proposed
novel program representation based on a programâ€™s AAST and its set of program invariants help:
(1) to efficiently cluster semantically equivalent small imperative programs submitted in IPAs, and
(2) to repair faster and significantly more IPAsâ€™ incorrect submissions in current state-of-the-art
clustering-based program repair tools, such as Clara [15].

The goal of our experiments was to answer the following research questions:

Q1. How does invariant-based program clustering compare against AAST and syntax-based clus-
tering on a set of correct submissions?
Q2. How does InvAASTCluster compare against Claraâ€™s clustering technique in terms of the
number of clusters generated, the time spent, and the number of changes needed to fix IPAs?
Q3. Does Clara repair more programs using InvAASTClusterâ€™s closest correct submission or its
set of KMeans clustersâ€™ representatives?
Q4. Are program invariants a helpful source of information to use on program representations of
incorrect programs?

To answer these research questions, we evaluate InvAASTCluster on two different use cases:
(1) clustering IPAs, and (2) repairing IPAs. For this evaluation, we have gathered a set of IPAs,
described in Section 6.1, developed during an introductory programming university course in C
language. Section 6.2 presents the first use case where we perform clustering on the studentsâ€™
program set and evaluate its accuracy on different program representations. Afterward, Section 6.3
shows the second use case where we integrate our program representations into a state-of-the-art
program repair tool, Clara [15], to evaluate if our clustering technique is able to outperform
Claraâ€™s clustering method, which is the only current publicly available state-of-the-art clustering
method for repairing IPAs. All of the experiments were conducted on an Intel(R) Xeon(R) Silver
computer with 4210R CPUs @ 2.40GHz, using a memory limit of 64GB.

6.1 Introductory Programming Assignments (IPAs) Dataset

To evaluate the program representations described in Section 4, we gathered a set of student
programs developed during an introductory programming course in C language were collected over
three distinct practical classes at Instituto Superior TÃ©cnico for 25 different IPAs. Since this work

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

11

(a) Syntax Representation

(b) AAST Representation

(c) Invariant-Based Representation

(d) AAST + Invariants Representation

Fig. 5. Comparison between the ground truth (on the right) and the clusters and cluster accuracy obtained
using the KMeans algorithm (on the left) for each type of program representation.

Table 2. Cluster accuracy using four different clustering algorithms on each program representation.

Program
Representation
Syntax
AAST
Invariants
AAST + Invariants

KMeans

58.9%
78.7%
79.3%
83.4%

Clustering Algortihms

Mini-Batch
KMeans
56.3%
76.6%
79.4%
80.4%

Gaussian
Mixture
57.7%
67.8%
79.7%
80.4%

Birch

58.5%
77.7%
78.3%
82.4%

focuses only on program semantics, only submissions that compile without any error were selected.
The set of submissions was split into two sets: correct submissions and incorrect submissions. The
studentsâ€™ submissions that satisfied a set of input-output test cases for each IPA were considered
correct and selected as benchmark instances. The submissions that failed at least one input-output
test were considered incorrect. Table 1 presents the number of submissions gathered. For 25
different programming exercises, this dataset contains 1620 different correct and 196 incorrect
submissions. Claraâ€™s clustering method does not support all the features present in the correct
submissions collected. Hence, as shown in Table 1, after removing the set of exercises and correct
programs that Clara does not support, we achieved a final set of 1141 correct submissions for
20 IPAs. This dataset of introductory programming exercises, C-Pack-IPAs, is publicly available
at https://github.com/pmorvalho/C-Pack-IPAs and can be used by other IPAs repair/clustering
frameworks. In this git repository, the interested reader can find all the information about description
and the input-output tests used to evaluate each IPA [31].

6.2 Use Case 1 : Clustering IPAs

A study was performed to evaluate different program representations by applying program cluster-
ing to the set of correct programs described in Table 1. The main idea of this experiment was to

50050502502550KMeans500505025025500.589Ground Truth5005075502502550KMeans50050755025025500.787Ground Truth5005050050100KMeans50050500501000.793Ground Truth50050502502550KMeans500505025025500.834Ground Truth12

P. Orvalho, M. Janota, and V. Manquinho

evaluate if program invariants help identify different IPAsâ€™ submissions. Hence, InvAASTCluster
was used to cluster the 1620 correct submissions into 25 distinct clusters since our dataset has
25 different programming exercises. Other works [32] that perform program clustering on IPAs
perform an equivalent study on clustering submissions for different assignments. The main reason
to cluster submissions to different IPAs is that we know the ground truth label for each program
since we know for which specific IPA the students submit their assignments. Otherwise, we had to
manually choose semantically different implementations for the same IPA and assign labels which
might be subjective.

InvAASTCluster, as explained in Section 5, starts by renaming all the variables in the student
submissions. Then uses Daikon [10] to collect the student submissionsâ€™ dynamically generated
invariants sets as described in Section 3.2. Lastly, it uses the python library, pycparser3, to compute
all the anonymized abstract syntax trees (AAST) (see Section 4.2). Using all these program features,
we computed four different bags of words models. One model for each program representation
(syntax, AAST, and invariants) and one representation using a combination of a programâ€™s AAST
and its invariants set. The program syntax is not included in this last representation since the bag
of words based on program syntax has a large vocabulary that generates too sparse vectors.

The following clustering algorithms available in scikit-learn4 were applied to each program
representation: KMeans (see Section 5), MiniBatch KMeans, BIRCH and Gaussian Mixture. We
focus the discussion on the KMeans results, the clustering algorithm that achieved the best results
(see Table 2). Since this dataset has 25 different programming exercises, the ground truth has 25
different clusters. Each student program is a submission to a specific programming exercise (label)
that we know. On that account, the cluster accuracy metric can be used to evaluate the obtained
clusters. With this metric, each cluster is assigned to the label (exercise), which is most frequent
in the cluster. Afterward, the accuracy of this assignment is measured by counting the number
of correctly assigned student submissions and dividing by the number of total submissions. This
metric is also known as purity [36].

Figure 5 presents the results of applying the KMeans model to each one of the four program rep-
resentations being analyzed. To present graphically these results, we used a method for visualizing
high-dimensional data in a 2âˆ’dimension map, called t-SNE [40]. Each subfigure corresponds to a
different type of representation. The left-side of each subfigure shows the clustering results and
the value of the cluster accuracy (right-bottom corner). The right side presents the real clusters of
each programming exercise, i.e. the ground truth represented using each program representation.
Figure 5a shows the results after clustering all the student submissions using a syntax represen-
tation, which resulted in a cluster accuracy of almost 59%. The AAST representation achieved a
cluster accuracy of 78.7% as presented in Figure 5b.

Regarding the use of program invariants, Figure 5c and Table 2 support the idea that program
invariants help to improve program clustering since this representation obtained a cluster accu-
racy of 79.3% using KMeans. Furthermore, Table 2 shows that for all clustering algorithms, this
representation based on AAST and invariants achieved the best cluster accuracy. Lastly, Figure 5d
presents the representation that uses the combination AASTs and invariants sets, which also shows
an improvement compared to the invariant-based representation. This representation outperforms
all the other representations with an accuracy of 83.4%. Another advantage of this representation
is that it is the best one separating all the studentsâ€™ submissions in different regions of the space,
i.e. the majority of the clusters are visibly separated from each other.

3https://github.com/eliben/pycparser
4https://scikit-learn.org/stable/modules/clustering.html

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

13

Table 3. Description of ITSP [43] dataset. Correct programs that our approach and Clara do not support
were removed.

ITSP
Dataset
Lab3
Lab4
Lab5
Lab6
Total

#IPAs

4
6
7
6
23

#Correct
Submissions
45
74
64
19
202

#Incorrect
Submissions
63
75
62
24
224

Table 4. For each clustering method, this table presents the number of submissions repaired, the number of
structural mismatch errors and timeouts.

Clustering Method

1 Clara
2 KMeans - Invs
3 KMeans - Syntax
4 KMeans - AAST
5 KMeans - AAST + Invs
6 Closest Program (KMeans)

- AAST + Invs

#Submissions
Repaired
229 (71.79%)
263 (82.45%)
268 (84.01%)
270 (84.64%)
271 (84.95%)

#Structural
Mismatch
24 (7.52%)
38 (11.91%)
35 (10.97%)
31 (9.72%)
32 (10.03%)

269 (84.33%)

34 (10.66%)

#Timeouts
(600s)
66 (20.69%)
18 (5.64%)
16 (5.02%)
18 (5.64%)
16 (5.02%)

16 (5.02%)

6.3 Use Case 2 : Repairing IPAs

In this section, we present the results of integrating InvAASTCluster as the clustering approach
for Clara [15], a publicly available state-of-the-art clustering-based program repair tool. Since
our set of IPAs, described in Table 1, has a small percentage of incorrect submissions, only 196, for
this evaluation, we have also considered the ITSP dataset [43]. The ITSP dataset has been used by
other automated software repair tools [2, 43] that use only one reference implementation. This
dataset is also a collection of C programs although it is well balanced, i.e., the number of correct
submissions is closer to the number of incorrect submissions in this dataset. Table 3 presents
the number of programs in the ITSP dataset after we removed the programs that Clara and our
variable renamer module do not support. Thus, overall we have a total of 420 incorrect submissions
(196 from our dataset plus 224 from the ITSP dataset) and 1343 correct submissions (1141 from our
dataset plus 202 from the ITSP dataset) for 43 different IPAs (20 from our dataset plus 23 from the
ITSP dataset). To fully evaluate our clustering technique for repairing IPAs, we are going to compare
InvAASTClusterâ€™s results against Claraâ€™s in terms of: (1) the number of student submissions
repaired; (2) the number of clusters produced by each clustering approach for each IPA; (3) the
quality of the repairs proposed and (4) the time spent to repair each incorrect submission.

6.3.1 Claraâ€™s Clustering Approach. Clara puts two programs into the same cluster if they have
the same control flow structure and if there exists a mapping between their variables [8]. Clara
requires a perfect match between the two programsâ€™ control flow graphs (i.e. branches, loops,
functions) and a bijective relation between both programsâ€™ variables. Otherwise, Clara returns
a structural mismatch error, and those programs are not clustered together. For each computed

14

P. Orvalho, M. Janota, and V. Manquinho

(a) Cactus plot - Number of Clusters.

(b) Scatter plot - Number of repairs - InvAASTCluster
(KMeans Invariants + AASTs) VS Clara.

(c) Cactus plot - Time Performance (timeout=600s).

(d) Cactus plot - Time Performance (timeout=10s).

Fig. 6. Comparison between Claraâ€™s Clusters and InvAASTClusterâ€™s clusters.

cluster, Clara keeps all the programsâ€™ information (e.g. expressions, variables) that belong to that
cluster.

Claraâ€™s Repairing Process. To repair a given incorrect program, Clara receives one or a set of
correct programs. This set of programs can correspond to clustersâ€™ representatives produced by
Clara. If so, then Clara should also receive all the information about the programs that are in
each cluster to help with the repair process. During Claraâ€™s repair process, if none of the correct
programs provided has an exact match with the incorrect submissionâ€™s control flow, then Clara is
not able to repair the program and returns a Structural Mismatch error. Otherwise, Clara gathers
the set of repairs using each correct program and returns the minimal one.

6.3.2 Results. This experiment compares InvAASTCluster against Claraâ€™s clustering technique.
Different procedures for program clustering using InvAASTCluster (see Table 4) are evaluated:
â€¢ KMeans - BoW : Uses KMeans and four different BoW based on AAST, syntax and invariants

(lines 2â€“5 in Table 4);

â€¢ Closest Program (KMeans) - AAST + Invs: Uses the closest program (see Section 5) using the

AAST +Invs BoW, from a set of clustersâ€™ representatives using KMeans (line 6);
Table 4 presents the overall repair evaluation on 319 incorrect submissions since Claraâ€™s repair
algorithm does not support the C implementation of 101 incorrect submissions (24.05% of the
instances). Entries in bold correspond to the highest number of submissions repaired, the lowest

010203040#IPAs020406080NumberofClustersClosestProgram(KMeans)AAST+InvsKMeansInvsKMeansAAST+InvsKMeansAASTKMeansSyntaxClara#CorrectSubmissions100101KMeansClustersAAST+Invariants100101ClaraClustersMax.13(numberofrepairs)Max.13(numberofrepairs)050100150200250300#SubmissionsRepaired0100200300400500600Time(s)KMeansAAST+InvsKMeansAASTClosestProgram(KMeans)AAST+InvsKMeansSyntaxKMeansInvsClara050100150200250300#SubmissionsRepaired246810Time(s)KMeansAAST+InvsKMeansAASTClosestProgram(KMeans)AAST+InvsKMeansSyntaxKMeansInvsClaraInvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

15

number of errors of structural mismatch, or the lowest number of executions that did not repair a
program using a timeout of 10 minutes (600s). Claraâ€™s clustering technique (line 1, Table 4) can
only repair 229 (around 72%) of the incorrect submissions and shows the largest percentage of
instances that were not repaired due to timeout (20.69%). InvAASTCluster using KMeans and the
BoW based on AAST and Invariants achieved the highest score, repairing 84.95% of the supported
incorrect submissions. The bag of words based only on invariants has the highest percentage of
structural mismatch (11.91%), which may be explained by Claraâ€™s inability to use a program with
a different control flow in the repair process. Using only invariants on a vector representation
helps clustering programs with similar semantics, although it does not take into account the
programsâ€™ structure (control flow). Hence, a higher rate of structural mismatch is observed. Since
the BoW based on AASTs and invariants achieved the best results either in the program clustering
experiment (see Section 6.2) as when repairing submissions using InvAASTCluster with KMeansâ€™
clusters (lines 2-5 in Table 4), we opted to only use the BoW based on AAST and invariants when
finding the closest correct program (line 6, Table 4). Regarding the use of just one correct solution to
fix an incorrect submission, the Closest Program (KMeans) approach did not achieve better results
than using the set of clustersâ€™ representatives.

We have also analyzed this technique using all submissions, i.e., use the closest program among
all submissions (no clustering step). This approach, the Closest Program (All Submissions), was able
to repair 86.5% of the submissions. The number of timeouts in this approach and using KMeans was
similar. Once again, this high rate of repaired programs (86.5%) may be explained by Claraâ€™s strict
requirements for both programs, the program being repaired and the correct program used by the
repair process, to have the same control flow. Therefore, when InvAASTCluster finds the closest
program among all submissions instead of using clusters, InvAASTCluster has a collection of
programsâ€™ structures more diverse. Consequentially, the Closest Program (All Submissions) approach
also achieved the lowest score of structural mismatch errors (only 9.4%). Although we would like
to draw the readerâ€™s attention to the difference between the number of submissions repaired using
KMeans (85%) or using the closest correct program (86.5%), which is less than 2%. Furthermore, the
computation to find the closest correct program among all correct submissions can only be done
online since it requires the studentâ€™s incorrect program. On the other hand, the computation of the
KMeans clusters can be done offline since it only requires past studentsâ€™ correct submissions. In
this evaluation, this is not a concern since each IPA has at most a hundred correct submissions.
However, in a large-scale MOOC with thousands of correct submissions per exercise, the process
of finding the closest correct program among all of the submissions may become impractical. To
conclude, although the closest correct program with all submissions was able to repair slightly
more programs in this study, in practical large-scale scenarios, it is not feasible to compute in a
short period of time. However, KMeansâ€™ clusters do not suffer this limitation since clusters can be
computed offline.

6.3.3 Number of Clusters. Figure 6 shows several cactus plots. The plot in Figure 6a presents the
number of clusters used by each clustering method (ğ‘¦-axis) for each IPA (ğ‘¥-axis). This plot also
shows the number of correct submissions per IPA. The legend in Figure 6a is sorted in decreasing
order of the number of clusters used. One can see that Clara generates an enormous quantity
of clusters, almost half of the correct submissions of each IPA. This large number of clusters is
explained by Claraâ€™s strict clustering method, that does not allow two programs to be in the same
cluster if there is not an exact match between both programsâ€™ control flows. InvAASTCluster
using the KMeans clustering algorithm produces ğ¾, which in this experiment was always 10% of
the number of correct submissions of each exercise. The technique that uses the closest correct
program has only a single cluster which is the closest correct program. This evaluation of the

16

P. Orvalho, M. Janota, and V. Manquinho

number of clusters used by each approach allows us to observe that Clara produces a large number
of clusters, resulting in a detriment of performance. However, our approach can generate fewer
clusters resulting in a more effective repairing process.

6.3.4 Quality of repairs. To compare the quality of repairs obtained using Claraâ€™s clusters against
InvAASTClusterâ€™s clustering method, we analyze the number of changes needed to repair a given
incorrect submission, i.e. the set of fixes a program needs to become compliant with the test suite.
Figure 6b shows a scatter plot comparing Claraâ€™s clusters against InvAASTCluster using the
BoW based on AAST and Invariants. Each point in the scatter plot represents an incorrect student
submission where the ğ‘¥-value (resp. ğ‘¦-value) is the number of changes required to fix a program
using InvAASTClusterâ€™s (resp. Claraâ€™s) clusters. This plot shows that for the programs that
both approaches can repair, Clara usually requires fewer changes to fix the program. Once again,
this may be explained by Claraâ€™s strict requirements for both programs (incorrect and correct
programs) used by the repair process to have the same control flow. When Clara uses the set of
KMeansâ€™ clusters, if the best clusterâ€™s representative, according to our program representation,
does not have the same control flow, then Clara cannot use this program in the repair process.
Hence, Clara uses another clusterâ€™s representative, one that has the same control flow although
may not have the same semantics. This translates into more changes to fix the incorrect submission.
Since Claraâ€™s clustering technique produces significantly more clusters (almost 50% of the correct
submissions), when Clara uses its own clusters has a more diverse number of programs which
means more control flow options, although that is also the reason why Clara has the lowest repair
score in Table 4. Perhaps if we would integrate InvAASTCluster into another repair framework
more permissive in terms of control flow, it would result in fewer changes required to fix each
incorrect submission.

6.3.5 CPU time. Figure 6c shows the CPU time spent on repairing programs using different
clustering techniques. The legend in this plot is sorted in decreasing order of the number of
submissions repaired with a timeout of 600s. One can clearly see a gap between Claraâ€™s time
performance and InvAASTClusterâ€™s (considering any clustering approach). There are two main
reasons for this gap. First, Clara produces a significantly larger number of clusters compared to
InvAASTCluster (see Figure 6a). Consequently, Clara must compute a set of repairs for each
clusterâ€™s representative. Thus, a bigger number of clusters implies more time spent in the repair
process. As for the second reason, as explained in Section 6.3.1, Clara keeps the information (e.g.
variables, expressions) of all the programs that belong to the cluster. During the repair algorithm,
Clara uses all this information to repair a given submission. Hence, the use of all this information
translates into more time spent on the repair process. The main goal of educational program
repair frameworks is to provide real-time feedback to students on how they should repair their
submissions. In this evaluation, we used a timeout of 10 minutes. However, a student expects a
faster result. Therefore, Figure 6d shows another cactus plot that shows the time performance of
the clustering approaches, although with a timeout of 10 seconds. One can see that after 10 seconds,
Clara using its own clusters, is only able to repair around 150 submissions (47%). On the other
hand, using InvAASTCluster, Clara can repair around 200 submissions (63%).

6.3.6 Program invariants of incorrect submissions. We have also tried the method Closest Program
(KMeans) with InvAASTCluster not taking into account incorrect submissionsâ€™ invariants to
check if incorrect submissionsâ€™ sets of invariants had a negative impact on program representations.
First, InvAASTCluster clustered all the correct programs considering their AASTs and their sets
of invariants. Secondly, to find the closest correct program to an incorrect submission InvAAST-
Cluster used only the AAST BoW. However, this combined approach of clustering with one BoW

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

17

(AASTs + invariants) and calculating the programsâ€™ distances with another (AASTs only) was only
able to repair 269 submissions (84%). Thus, according to this experiment, incorrect submissionsâ€™
sets of program invariants do not cause negative effects on program representations.

To summarize, in Section 6.2 we used InvAASTCluster to cluster different IPAs correct studentsâ€™
submissions. The obtained results support that this workâ€™s novel program representation based
on a programâ€™s AAST and invariants has better performance when compared to representations
solely based on a programâ€™s code, AST or set of program invariants. Furthermore, in Section 6.3, we
integrated InvAASTCluster into Clara to evaluate our tool for repairing IPAs. This study shows
that InvAASTCluster significantly increases the current state-of-the-art clustering techniqueâ€™s
performance by allowing Clara to repair more student submissions and notably faster.

6.4 Threats to Validity

This work relies on Daikon to compute dynamically-generated likely invariants. Using another tool
to detect likely invariants may produce different results. Dynamically-generated likely invariants
depend highly on the test suite used for each programming exercise. Therefore, using a different set
of input-output tests may produce different sets of program invariants for each student submission.
For this workâ€™s evaluation, only C programs were used. However, our clustering methods are
general and can be applied to other programming languages. To apply InvAASTCluster on other
programming languages requires the replacement of: the variable renaming module with one
that supports the new language and InvAASTClusterâ€™s invariants detector module for another
program invariant detector that works for the desired language. A different repair tool may produce
different results since the repair process may be different. For example, another tool may support C
features that Clara currently does not support or the opposite. Only small imperative programs,
usually found in IPAs, were used to evaluate InvAASTCluster. The use of InvAASTCluster on
more complex programs is left for future work.

7 RELATED WORK
Program clustering has also been used to find different semantic solutions for a given programming
exercise [7, 12, 13]. PaCon [12] clusters programming assignments based on their symbolic analysis.
PaCon clusters two submissions together if their path conditions are equivalent. PaCon only
takes into consideration a programâ€™s semantics. Overcode [13] lets the user visualize and explore
different implementations for the same exercise. CodeBERT [11], code2seq [4] and other deep
learning models [45] build vector representations of programs by training machine learning models
using the programsâ€™ code and ASTs. However, these techniques only consider the programsâ€™ syntax
and not their semantics.

Code search techniques [1, 21, 25, 26, 33, 38, 39] repair a given erroneous program using code
snippets. However, these methods have no knowledge of the programâ€™s structure where that code
fragment came from. Therefore, there is no guarantee that the set of repairs proposed by a code
search tool is the minimal set required to fix a program.

Solution-driven program repair use one reference implementation to repair a given incorrect
submission [2, 19, 23]. AutoGrader [23] finds potential path differences between the executions of a
studentâ€™s submission and a reference implementation using symbolic execution. Then, AutoGrader
provides feedback to students in the form of counter-examples for each path difference found.
Verifix [2] aligns an incorrect program with the reference solution into an automaton. Then, using
that alignment relation and MaxSMT solving, Verifix proposes fixes to the incorrect program.
Regarding clustering-based program repair tools [15, 24, 32, 41], Clara [15] was already described,
and differences were highlighted in previous sections. SemCluster [32] clusters programs based

18

P. Orvalho, M. Janota, and V. Manquinho

on their control flow and data flow features. SemCluster creates vector representations using a
test-suite. For each program, it counts the number of times each control flow path is used and tracks
the number of occurrences of consecutive values a variable takes during its lifetime. SarfGen [41],
used for C# programs, creates program embeddings based on the programsâ€™ ASTs. Then, given an
incorrect program, finds the closest correct submission using those embeddings and tries to repair
the program by aligning the variables in both programs. More recently, Contractor and Rivero [8]
improved Claraâ€™s matching algorithm to a new graph matching algorithm that is more relax in
terms of control flow restrictions. However, this new graph matching algorithm only works for
Python programs. Some research has been conducted regarding the use of invariants to promote
patch diversity or to help with patch selection on a search-based program repair [5, 9, 42].

In this paper, we compared InvAASTCluster against Claraâ€™s clustering technique since, to the
best of our knowledge, Clara is currently the only publicly available state-of-the-art clustering-
based repair tool for repairing IPAs. However, it would be interesting to evaluate InvAASTCluster
on other repair frameworks. Unfortunately, we could not find public implementations for the other
tools [2, 12, 24, 32, 41] nor the datasets of IPAs used for their evaluation, except for the ITSP
dataset [43].

8 CONCLUSIONS AND FUTURE WORK

In the context of introductory programming assignments (IPAs) in university courses or Massive
Open Online Courses (MOOCs), it is possible to collect a large number of correct implementations
for the proposed IPAs. Hence, when a student submits an incorrect program, one can take advantage
of previously correct submissions to automatically suggest repairs that help the student. However,
it is not feasible to analyze all possible previous correct submissions to find an appropriate reference
implementation for the repair tool. Therefore, clustering is often used to identify similar program
implementations. Afterward, the automated repair tool just analyses a single reference program
from each cluster to find the most suitable correction to the studentâ€™s incorrect submission.

This work proposes InvAASTCluster, a novel approach for program clustering based on their
semantic and syntactic features. InvAASTCluster uses AASTs and invariant-based program
representations to distinguish small imperative programs according to their semantics (invariants)
and syntax (AAST). Results show that the proposed AASTs and invariant-based representation
improve upon syntax-based representations when performing program clustering on several correct
student submissions for different programming exercises. Additionally, given an incorrect student
submission and a set of correct studentsâ€™ submissions, InvAASTCluster can also find the closest
correct submission to the faulty program using InvAASTClusterâ€™s program vector representations.
Furthermore, InvAASTCluster has also been integrated into a state-of-the-art clustering-based
program repair framework to evaluate the proposed clustering techniques for repairing IPAs. This
evaluation showed that InvAASTCluster outperforms the current state-of-the-art clustering
method used in clustering-based program repair. Using InvAASTCluster, the automated repair
tool Clara can repair significantly more IPAs with a better time performance and with a smaller
number of program clusters.

To conclude, InvAASTCluster is a program clustering tool based on programsâ€™ invariants and
AASTs. InvAASTCluster can be used: (1) to cluster semantically equivalent implementations for
programming exercises; (2) by any clustering-based program repair tool; and (3) by any program
repair framework that requires a single reference implementation (InvAASTClusterâ€™s closest
correct program).

As future directions, we propose to evaluate the new program representations described in this
paper (i.e., using AASTs and invariants) as program encodings to use on deep learning models
on several tasks such as fault localization or program synthesis. As we expand to consider more

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

19

complex programs, we plan to evaluate InvAASTCluster on clustering-based repair tools focused
on repairing industrial software. Finally, InvAASTCluster will be evaluated on other clustering-
based program repair frameworks with a more permissive repair algorithm than Clara.

ACKNOWLEDGMENTS

This research was supported by FundaÃ§Ã£o para a CiÃªncia e Tecnologia (FCT) through grant
SFRH/BD/07724/2020 and projects UIDB/50021/2020, PTDC/CCI-COM/32378/2017, and
DSAIPA/AI/0044/2018. Any opinions, findings, conclusions, or recommendations expressed in
this material are those of the author and do not necessarily reflect the views of FCT.

REFERENCES

[1] Afsoon Afzal, Manish Motwani, Kathryn T. Stolee, Yuriy Brun, and Claire Le Goues. 2019. SOSRepair: Expressive
Semantic Search for Real-World Program Repair. IEEE Trans. Software Eng. 47, 10 (2019), 2162â€“2181. https://doi.org/10.
1109/TSE.2019.2944914

[2] Umair Z. Ahmed, Zhiyu Fan, Jooyong Yi, Omar I. Al-Bataineh, and Abhik Roychoudhury. 2022. Verifix: Verified Repair

of Programming Assignments. ACM Trans. Softw. Eng. Methodol. (jan 2022). https://doi.org/10.1145/3510418
[3] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1986. Compilers: Principles, Techniques, and Tools. Addison-Wesley.
[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Repre-
sentations of Code. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net.

[5] Padraic Cashin, Carianne Martinez, Westley Weimer, and Stephanie Forrest. 2019. Understanding Automatically-
Generated Patches Through Symbolic Invariant Differences. In 34th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 411â€“414. https://doi.org/10.1109/ASE.
2019.00046

[6] Berkeley R. Churchill, Oded Padon, Rahul Sharma, and Alex Aiken. 2019. Semantic program alignment for equivalence
checking. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, Kathryn S. McKinley and Kathleen Fisher (Eds.). ACM, 1027â€“1040.
[7] Joshua Clune, Vijay Ramamurthy, Ruben Martins, and Umut A. Acar. 2020. Program equivalence for assisted grading
of functional programs. Proc. ACM Program. Lang. 4, OOPSLA (2020), 171:1â€“171:29. https://doi.org/10.1145/3428239
[8] Maheen Riaz Contractor and Carlos R. Rivero. 2022. Improving Program Matching to Automatically Repair Introductory
Programs. In Intelligent Tutoring Systems, Scott Crossley and Elvira Popescu (Eds.). Springer International Publishing,
Cham, 323â€“335.

[9] Zhen Yu Ding, Yiwei Lyu, Christopher Steven Timperley, and Claire Le Goues. 2019. Leveraging program invariants
to promote population diversity in search-based automatic program repair. In Proceedings of the 6th International
Workshop on Genetic Improvement, GI@ICSE 2019, Montreal, Quebec, Canada, May 28, 2019, Justyna Petke, Shin Hwei
Tan, William B. Langdon, and Westley Weimer (Eds.). ACM, 2â€“9. https://doi.org/10.1109/GI.2019.00011

[10] Michael D. Ernst, Jeff H. Perkins, Philip J. Guo, Stephen McCamant, Carlos Pacheco, Matthew S. Tschantz, and Chen
Xiao. 2007. The Daikon system for dynamic detection of likely invariants. Sci. Comput. Program. 69, 1-3 (2007), 35â€“45.
https://doi.org/10.1016/j.scico.2007.01.015

[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,
Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In
Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings
of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,
1536â€“1547.

[12] Yingjie Fu, Jonathan Osei-Owusu, Angello Astorga, Zirui Neil Zhao, Wei Zhang, and Tao Xie. 2021. PaCon: a symbolic
analysis approach for tactic-oriented clustering of programming submissions. In Proceedings of the 2021 ACM SIGPLAN
International Symposium on SPLASH-E, Chicago, IL, USA. October 20, 2021, Charlie Curtsinger and Tien N. Nguyen
(Eds.). ACM, 32â€“42.

[13] Elena L. Glassman, Jeremy Scott, Rishabh Singh, Philip J. Guo, and Robert C. Miller. 2015. OverCode: Visualizing
Variation in Student Solutions to Programming Problems at Scale. ACM Trans. Comput. Hum. Interact. 22, 2 (2015),
7:1â€“7:35.

[14] Sumit Gulwani, Ivan Radicek, and Florian Zuleger. 2014. Feedback generation for performance problems in introductory
programming assignments. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software
Engineering, (FSE-22), Hong Kong, China, November 16 - 22, 2014. ACM, 41â€“51.

20

P. Orvalho, M. Janota, and V. Manquinho

[15] Sumit Gulwani, Ivan Radicek, and Florian Zuleger. 2018. Automated clustering and program repair for introductory

programming assignments. In PLDI 2018. ACM, 465â€“480.

[16] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. 2017. DeepFix: Fixing Common C Language Errors

by Deep Learning. In AAAI 2017, Satinder P. Singh and Shaul Markovitch (Eds.). AAAI Press, 1345â€“1351.

[17] Zellig S Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146â€“162.
[18] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2007.

Introduction to automata theory, languages, and

computation, 3rd Edition. Addison-Wesley.

[19] Yang Hu, Umair Z. Ahmed, Sergey Mechtaev, Ben Leong, and Abhik Roychoudhury. 2019. Re-Factoring Based Program
Repair Applied to Programming Assignments. In 34th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 388â€“398.

[20] Lingxiao Jiang, Ghassan Misherghi, Zhendong Su, and StÃ©phane Glondu. 2007. DECKARD: Scalable and Accurate
Tree-Based Detection of Code Clones. In 29th International Conference on Software Engineering (ICSE 2007), Minneapolis,
MN, USA, May 20-26, 2007. IEEE Computer Society, 96â€“105.

[21] Yalin Ke, Kathryn T. Stolee, Claire Le Goues, and Yuriy Brun. 2015. Repairing Programs with Semantic Code Search
(T). In 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015, Myra B. Cohen, Lars
Grunske, and Michael Whalen (Eds.). IEEE Computer Society, 295â€“306.

[22] Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics

doklady academii nauk, Vol. 10. Soviet Union, 707â€“710.

[23] Xiao Liu, Shuai Wang, Pei Wang, and Dinghao Wu. 2019. Automatic grading of programming assignments: an
approach based on formal semantics. In Proceedings of the 41st International Conference on Software Engineering:
Software Engineering Education and Training, ICSE (SEET) 2019, Sarah Beecham and Daniela E. Damian (Eds.). IEEE /
ACM, 126â€“137.

[24] Yunlong Lu, Na Meng, and Wenxin Li. 2021. FAPR: Fast and Accurate Program Repair for Introductory Programming

Courses. CoRR abs/2107.06550 (2021).

[25] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. 2019. Aroma: code recommendation via

structural code search. Proc. ACM Program. Lang. 3, OOPSLA (2019), 152:1â€“152:28.

[26] George Mathew and Kathryn T. Stolee. 2021. Cross-language code search using static and dynamic analyses. In ESEC/FSE
â€™21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,
Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta
(Eds.). ACM, 205â€“217.

[27] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2015. DirectFix: Looking for Simple Program Repairs. In
37th IEEE/ACM International Conference on Software Engineering, ICSE 2015, Antonia Bertolino, Gerardo Canfora, and
Sebastian G. Elbaum (Eds.). IEEE Computer Society, 448â€“458.

[28] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: scalable multiline program patch synthesis via
symbolic analysis. In ICSE 2016, Laura K. Dillon, Willem Visser, and Laurie A. Williams (Eds.). ACM, 691â€“701.
[29] Robin Milner. 1971. An Algebraic Definition of Simulation Between Programs. In Proceedings of the 2nd International
Joint Conference on Artificial Intelligence. London, UK, September 1-3, 1971, D. C. Cooper (Ed.). William Kaufmann,
481â€“489.

[30] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. SemFix: program repair via
semantic analysis. In 35th International Conference on Software Engineering, ICSE â€™13, David Notkin, Betty H. C. Cheng,
and Klaus Pohl (Eds.). IEEE Computer Society, 772â€“781.

[31] Pedro Orvalho, MikolÃ¡s Janota, and Vasco M. Manquinho. 2022. C-Pack of IPAs: A C90 Program Benchmark of

Introductory Programming Assignments. https://doi.org/10.48550/arXiv.2206.08768

[32] David Mitchel Perry, Dohyeong Kim, Roopsha Samanta, and Xiangyu Zhang. 2019. SemCluster: clustering of imperative
programming assignments based on quantitative semantic features. In Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, Kathryn S.
McKinley and Kathleen Fisher (Eds.). ACM, 860â€“873.

[33] Steven P. Reiss. 2009. Semantics-based code search. In 31st International Conference on Software Engineering, ICSE 2009,

May 16-24, 2009, Vancouver, Canada, Proceedings. IEEE, 243â€“253. https://doi.org/10.1109/ICSE.2009.5070525

[34] Henry Gordon Rice. 1953. Classes of recursively enumerable sets and their decision problems. Trans. Amer. Math. Soc.

74, 2 (1953), 358â€“366.

[35] Saul Schleimer, Daniel Shawcross Wilkerson, and Alexander Aiken. 2003. Winnowing: Local Algorithms for Document
Fingerprinting. In Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data, San Diego,
California, USA, June 9-12, 2003, Alon Y. Halevy, Zachary G. Ives, and AnHai Doan (Eds.). ACM, 76â€“85.

[36] Hinrich SchÃ¼tze, Christopher D Manning, and Prabhakar Raghavan. 2008. Introduction to information retrieval. Vol. 39.

Cambridge University Press Cambridge.

[37] Hugo Steinhaus. 1956. Sur la division des corps matÃ©riels en parties. Bull. Acad. Polon. Sci 1, 804 (1956), 801.

InvAASTCluster : On Applying Invariant-Based Program Clustering to Introductory Programming Assignments

21

[38] Kathryn T. Stolee and Sebastian G. Elbaum. 2012. Toward semantic search via SMT solver. In 20th ACM SIGSOFT
Symposium on the Foundations of Software Engineering (FSE-20), SIGSOFT/FSEâ€™12, Cary, NC, USA - November 11 - 16,
2012, Will Tracz, Martin P. Robillard, and Tevfik Bultan (Eds.). ACM, 25. https://doi.org/10.1145/2393596.2393625
[39] Kathryn T. Stolee, Sebastian G. Elbaum, and Matthew B. Dwyer. 2016. Code search with input/output queries:
Generalizing, ranking, and assessment. J. Syst. Softw. 116 (2016), 35â€“48. https://doi.org/10.1016/j.jss.2015.04.081
[40] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research

9, 11 (2008).

[41] Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Search, align, and repair: data-driven feedback generation for

introductory programming exercises. In PLDI 2018. ACM, 481â€“495.

[42] Bo Yang and Jinqiu Yang. 2020. Exploring the differences between plausible and correct patches at fine-grained level.

In 2020 IEEE 2nd International Workshop on Intelligent Bug Fixing (IBF). IEEE, 1â€“8.

[43] Jooyong Yi, Umair Z. Ahmed, Amey Karkare, Shin Hwei Tan, and Abhik Roychoudhury. 2017. A feasibility study of
using automated program repair for introductory programming assignments. In ESEC/FSE 2017, Eric Bodden, Wilhelm
SchÃ¤fer, Arie van Deursen, and Andrea Zisman (Eds.). ACM, 740â€“751.

[44] Kurtis Zimmerman and Chandan Raj Rupakheti. 2015. An Automated Framework for Recommending Program
Elements to Novices (N). In 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015,
Lincoln, NE, USA, November 9-13, 2015, Myra B. Cohen, Lars Grunske, and Michael Whalen (Eds.). IEEE Computer
Society, 283â€“288.

[45] Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan GÃ¼nnemann. 2021. Language-Agnostic
Representation Learning of Source Code from Structure and Context. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

