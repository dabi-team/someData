Journal of XXXX manuscript No.
(will be inserted by the editor)

Regularization path-following methods with the trust-region
updating strategy for linear complementarity problems

Xin-long Luo ∗ · Sen Zhang · Hang Xiao

2
2
0
2

y
a
M
2
2

]

C
O
.
h
t
a
m

[

1
v
7
2
7
0
1
.
5
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract In this article, we consider the regularization path-following method with
the trust-region updating strategy for the linear complementarity problem. Moreover,
we prove the global convergence of the new method under the standard assumptions
without the condition of the priority to feasibility over complementarity. Numerical
results show that the new method is robust and efﬁcient for the linear complemen-
tarity problem, especially for the dense linear complementarity problem. And it is
more robust and faster than some state-of-the-art solvers such as the built-in subrou-
tines PATH and MILES of the GAMS v28.2 (2019) environment. The computational
time of the new method is about 1/3 to 1/10 of that of PATH for the dense linear
complementarity problem.

Keywords Continuation Newton method · regularization · trust-region updating
strategy · complementarity · path-following method

Mathematics Subject Classiﬁcation (2010) 90C33 · 65K05 · 65L05 · 65L20

Xin-long Luo, Corresponding author
School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, P. O. Box 101,
Xitucheng Road No. 10, Haidian District, 100876, Beijing China, E-mail: luoxinlong@bupt.edu.cn

Sen Zhang
School of Artiﬁcial Intelligence,
Beijing University of Posts and Telecommunications, P. O. Box 101,
Xitucheng Road No. 10, Haidian District, 100876, Beijing China
E-mail: senzhang@bupt.edu.cn

Hang Xiao
School of Artiﬁcial Intelligence,
Beijing University of Posts and Telecommunications, P. O. Box 101,
Xitucheng Road No. 10, Haidian District, 100876, Beijing China
E-mail: xiaohang0210@bupt.edu.cn

 
 
 
 
 
 
2

1 Introduction

Luo et al.

In this article, we are mainly concerned with the linear complementarity problem as
follows:

y = Mx + q, xiyi = 0, i = 1, 2, . . . , n, x ≥ 0, y ≥ 0,

(1)

where q ∈ ℜn is a vector and M is an n × n positive semi-deﬁnite matrix. For the lin-
ear complementarity problem (1), there are many practical applications such as the
equilibrium of forces [17] and the economic equilibrium problem [43, 50]. And the
solutions of many problems such as the linear programming and the convex quadratic
programming can be obtained by solving it [8, 56]. Furthermore, there are many ef-
ﬁcient methods to solve it such as the Lemke’s complementary pivoting algorithm
(MILES) [8, 32, 50], the path-following methods [20, 55, 56, 59] and their mixture
method (PATH) [14, 15].

In this paper, we consider another path-following method based on the New-
ton ﬂow with nonnegative constraints, which is the variant of the primal-dual path-
following method. In order to improve its robustness and efﬁciency, we use the reg-
ularization technique to avoid the singularity of the Jacobian matrix, and adopt the
trust-region updating strategy to adjust the time step adaptively. Firstly, we construct
the regularization Newton ﬂow with nonnegative constraints for the linear comple-
mentarity problem (1) based on the primal-dual path-following method. Then, we
use the implicit Euler method and the linear approximation of the quadratic function
to obtain the regularization path-following method for following the the trajectory of
the Newton ﬂow. Finally, we adopt the trust-region updating strategy to adjust the
time step adaptively. The advantage of this adaptive adjustment strategy compared
with the line search method is that it does not require the condition of the priority to
feasibility over complementarity of the conventional path-following method [55, 59]
when we prove its global convergence.

The rest of this article is organized as follows. In the next section, we consider the
regularization path-following method and the adaptive trust-region updating strategy
for the linear complementarity problem. In section 3, we analyze the global con-
vergence of the new method under the standard assumptions without the condition
of the priority to feasibility over complementarity. In section 4, we compare the
new method with two state-of-the-art solvers, i.e. PATH [14, 15, 48] and MILES (a
Mixed Inequality and nonLinear Equation Solver) [43,44,50] for sparse problems and
dense problems, test matrices of which come from the linear programming subset of
NETLIB [45]. The new method is coded with the MATLAB language and executed in
MATLAB (R2020a) environment [42]. PATH and MILES are executed in the GAMS
v28.2 (2019) environment [21]. Numerical results show that the new method is robust
and efﬁcient for solving the linear complementarity problem. It is more robust and
faster than PATH and MILES for the dense problems. Finally, some discussions are
given in section 5. (cid:107) · (cid:107) denotes the Euclidean vector norm or its induced matrix norm
throughout the paper.

Regularization path-following methods for LCP

3

2 Regulation path-following methods

2.1 The continuous Newton ﬂow

For convenience, we rewrite the linear complementarity problem (1) as the following
nonlinear system of equations with nonnegativity constraints:

F(z) =

y − (Mx + q)







XYe

 = 0, (x, y) ≥ 0, z = (x, y),

(2)

where X = diag(x), Y = diag(y) and all components of vector e equal one. It is not
difﬁcult to know that the Jacobian matrix J(z) of F(z) has the following form:

J(z) =





−M I

Y X



 .

(3)

From the second block XYe = 0 of equation (2), we know that xi = 0 or yi = 0 (i =
1 : n). Thus, the Jacobian matrix J(z) of equation (3) may be singular, which leads
to numerical difﬁculties near the solution of the nonlinear system (2) for the Newton
method or its variants. In order to overcome this difﬁculty, we consider its perturbed
system [2, 53] as follows:

Fµ (z) = F(z) −







0

µe

 = 0, z = (x, y) > 0, µ > 0.

(4)

It is not difﬁcult to verify that the Jacobian matrix J(z) deﬁned by equation (3)
is nonsingular when M is a positive semi-deﬁnite matrix and (x, y) > 0 (see Lemma
5.9.8, p. 469 in [8]). Thus, by using the implicit theorem and the inequality (a −
b)(c − d) ≤ |ac − bd| (a > 0, b > 0, c > 0, d > 0), the perturbed system (4) has a
unique solution when M is a positive deﬁnite matrix (see Theorem 5.9.13, p. 471
in [8]) for its detailed proof). The solution z(µ) of the perturbed system (4) deﬁnes
the central path, and z(µ) approximates the solution z∗ of the nonlinear system (2)
when µ tends to zero [8, 56].

If the damped Newton method is applied to the perturbation system (4) [30, 46],

we have

zk+1 = zk − αkJ(zk)−1Fµ (zk),

(5)

where J(zk) is the Jacobian matrix of Fµ (z). We regard zk+1 = z(tk + αk), zk = z(tk)
and let αk → 0, then we obtain the continuous Newton ﬂow with nonnegativity con-
straints [4, 11, 13, 39, 53] of the perturbed system (4) as follows:

dz(t)
dt

= −J(z)−1Fµ (z), z = (x, y) > 0.

(6)

4

Luo et al.

Actually, if we apply an iteration with the explicit Euler method [51] for the continu-
ous Newton ﬂow (6), we obtain the damped Newton method (5).

Since the Jacobian matrix J(z) = F (cid:48)

µ (z) may be singular, we reformulate the con-
tinuous Newton ﬂow (6) as the following general formula for the linear complemen-
tarity problem (2) :

− M

dx(t)
dt

Y

dx(t)
dt

+ X

+

dy(t)
dt
dy(t)
dt

= −rq(x, y),

= −(XYe − µ(t)e), (x, y) > 0,

(7)

(8)

where the residual rq(x, y) = y − (Mx + q). The continuous Newton ﬂow (7)-(8) has
some nice properties. We state one of them as the following property 2.1 [5, 37–39,
52].

Property 2.1 Assume that (x(t), y(t)) is the solution of the continuous Newton ﬂow
(7)-(8), then rq(x(t), y(t)) converges to zero and xi(t)yi(t) (i = 1, 2, . . . , n) converge
to zero when 0 ≤ µ(t) ≤ σ min1≤i≤n{xi(t)yi(t)} (0 < σ < 1) and t → ∞. That is to
say, for every limit point (x∗, y∗) of (x(t), y(t)), it is also a solution of the linear
complementarity problem (2). Furthermore, x(t) and y(t) keep positive values when
the initial point (cid:0)x0

(cid:1) > 0 (i = 1, 2, . . . , n).

i , y0
i

Proof. Assume that z(t) is the continuous solution of the continuous Newton ﬂow
(7)-(8), then we have

d
dt
d
dt

rq(x, y) = −M

dx
dt

+

dy
dt

= −rq(x, y),

(XYe) = X

dy
dt

+Y

dx
dt

= −(XYe − µ(t)e).

(9)

(10)

Consequently, from equations (9)-(10) and 0 ≤ µ(t) ≤ σ min1≤i≤n{xi(t)yi(t)}, we
obtain

rq(x(t), y(t)) = rq

(cid:0)x0, y0(cid:1) exp(−t),

− XYe ≤

d
dt

(XYe) ≤ −(1 − σ )XYe.

(11)

(12)

From equations (11)-(12), we know that rq(x(t), y(t)) converges to zero with the
linear rate of convergence when t tends to inﬁnity. Furthermore, from equation (12)
and the Gronwall inequality [25], we have

i y0
x0

i exp(−t) ≤ xi(t)yi(t) ≤ x0

i y0

i exp(−(1 − σ )t), i = 1, 2, . . . , n.

(13)

Consequently, from equation (13), we know that xi(t)yi(t) ≥ 0 (i = 1, 2, . . . , n) and
limt→∞ xi(t)yi(t) = 0 (i = 1, 2, . . . , n).

From equation (13), we know that xi(t)yi(t) > 0 (t ≥ 0) when (cid:0)x0, y0(cid:1) > 0. If
we have xi(T ) < 0 or yi(T ) < 0 for a ﬁnite value T > 0, there exists ¯t ∈ (0, T ) such
that xi(¯t) = 0 or yi(¯t) = 0. Consequently, we have xi(¯t)yi(¯t) = 0, which contradicts
xi(¯t)yi(¯t) > 0. Thus, we have (x(t), y(t)) > 0 for all t > 0. Therefore, if the solution

Regularization path-following methods for LCP

5

(x(t), y(t)) of the continuous Newton ﬂow (7)-(8) belongs to a compact set, there
exists a limit point (x∗, y∗) when t tends to inﬁnity, and this limit point (x∗, y∗) is also
(cid:117)(cid:116)
a solution of the linear complementarity problem (2).

Remark 2.1 The inverse J(z)−1 of the Jacobian matrix J(z) can be regarded as the
pre-conditioner of Fµ (z) such that the every element zi(t) of z(t) has roughly the
same rate of convergence and it mitigates the stiffness of the ODE (6) [37, 38]. This
property is very useful since it makes us adopt the explicit ODE method to follow the
trajectory of the Newton ﬂow (6) efﬁciently.

2.2 The regularization path-following method

From property 2.1, we know that the continuous Newton ﬂow (7)-(8) has the global
convergence. However, when the Jacobian matrix J(z) is singular or nearly singular,
the ODE (7)-(8) is the system of differential-algebraic equations [3, 7, 26] and its
trajectory can not be efﬁciently solved by the general ODE method [6,29] such as the
backward differentiation formulas (the built-in subroutine ode15s.m of the MATLAB
environment [42, 51]). Thus, we need to construct the special method to solve this
problem. Furthermore, we expect that the new method has the global convergence as
the homotopy continuation methods [2, 47] and the fast rate of convergence as the
traditional optimization methods. In order to achieve these two aims, we consider
the continuation Newton method and the trust-region updating strategy for problem
(7)-(8).

We apply the implicit Euler method [3, 7] to the continuous Newton ﬂow (7)-(8),

then we obtain

− M

xk+1 − xk
∆tk

Y k+1 xk+1 − xk

∆tk

+

yk+1 − yk
= −rq
∆tk
+ X k+1 yk+1 − yk

∆tk

xk+1, yk+1(cid:17)
(cid:16)

,

= −

(cid:16)
X k+1Y k+1e − µ(tk+1)e

(cid:17)

.

(14)

(15)

Since equation (15) is a nonlinear system, it is not directly solved, and we seek for its
explicit approximation formula. We replace Y k+1 and X k+1 with Y k and X k in the left-
hand side of equation (15), respectively. And we substitute X k+1Y k+1e with its linear
approximation X kY ke + ∆tk
(Y k∆ xk + X k∆ yk) in the right-hand side of equation
1+∆tk
(15). We set µ(tk+1) = σkµk. Then, we obtain the continuation Newton method (one
of path-following methods) as follows:

− M∆ xk + ∆ yk = −rk
q,
Y k∆ xk + X k∆ yk = −rk
c,

xk+1 = xk +

∆tk
1 + ∆tk

∆ xk, yk+1 = yk +

∆tk
1 + ∆tk

∆ yk,

(16)

(17)

(18)

6

Luo et al.

q = rq(xk, yk), rk

where rk
bation parameter µk is selected as follows:

c = X kyk − σkµke, 0 < σmin ≤ σk ≤ σmax < 1 and the pertur-

µk =

(xk)T yk + (cid:107)rk
q(cid:107)
2n

.

(19)

yk/n.

The selection of µk in equation (19) is slightly different to the traditional selection
µk = (cid:0)xk(cid:1)T
yk/n [55, 56, 59]. According to our numerical experiments, the selection
of µk in equation (19) can improve the robustness of the path-following method, in
comparison to the traditional selection selection µk = (cid:0)xk(cid:1)T
Remark 2.2 If we set αk = ∆tk/(1 + ∆tk) in equation (18), we obtain the damped
Newton method (the primal-dual path-following method) (5). However, from the
view of the ODE method, they are different. The damped Newton method (5) is de-
rived from the explicit Euler method applied to the continuous Newton ﬂow (7)-(8).
Its time step αk is restricted by the numerical stability [26, 51]. That is to say, for the
linear test equation dx/dt = −λ x (λ > 0), its time step αk is restricted by the stable
region |1 − λ αk| ≤ 1. Therefore, the large step αk can not be adopted in the steady-
state phase. The continuation Newton method (16)-(18) is derived from the implicit
Euler method applied to the continuous Newton ﬂow (7)-(8) and the linear approxi-
mation of Fµ(tk+1)(zk+1), and its time step ∆tk is not restricted by the numerical sta-
bility for the linear test equation. Therefore, the large time step ∆tk can be adopted
in the steady-state phase, and the continuation Newton method (16)-(18) mimics the
Newton method. Consequently, it has the fast rate of convergence near the solution
z∗ of the nonlinear system (2). The most of all, the substitution αk with ∆tk/(∆tk + 1)
is favourable to adopt the trust-region updating strategy for adaptively adjusting the
time step ∆tk such that the continuation Newton method (16)-(18) accurately follows
the trajectory of the continuation Newton ﬂow (7)-(8) in the transient-state phase and
achieves the fast rate of convergence in the steady-state phase.

When the diagonal matrix X k is positive deﬁnite, from equations (16)-(17), ∆ xk

and ∆ yk can be solved by the following two subsystems:

M + (X k)−1Y k(cid:17)
(cid:16)
∆ yk = M∆ xk − rk
q.

∆ xk = rk

q − (X k)−1rk
c,

(20)

(21)

When matrix M is positive semi-deﬁnite and (xk, yk) > 0, the left hand-side matrix is
positive deﬁnite. Thus, the system (20) can be solved by the partial pivoting Gaussian
elimination method (see pp. 125-130, [24]).

2.3 The time-stepping control and the initial point selection

Another issue is how to adaptively adjust the time step ∆tk at every iteration. A pop-
ular and efﬁcient time-stepping control is based on the trust-region updating strat-
egy [9, 12, 27, 33–41, 58]. Its main idea can be described as follows. Firstly, we con-
struct a merit function reﬂecting the feasibility rq(x, y) = 0 and the complementarity

Regularization path-following methods for LCP

xiyi = 0 (i = 1, 2, . . . , n) as

φ (x, y) = xT y + (cid:107)rq(x, y)(cid:107),

7

(22)

where rq(x, y) = y − (Mx + q) and (x, y) ≥ 0.

Then, we deﬁne the linear approximation model mk of φ (xk + αk∆ xk, yk + αk∆ yk)

as

mk(α) = φ (xk, yk) + α

(cid:17)
(cid:16)
(yk)T ∆ xk + (xk)T ∆ yk − (cid:107)rk
q(cid:107)

,

(23)

where α = ∆t/(1 + ∆t) and (∆ xk, ∆ yk) is the Newton step and solved by equations
(16)-(17).

Finally, we adaptively adjust the time step ∆tk according to the difference be-
tween mk(αk) and φ (xk + αk∆ xk, yk + αk∆ yk). Namely, the time step ∆tk+1 will be
enlarged when mk(αk) approximates φ (xk + αk∆ xk, yk + αk∆ yk) well, and ∆tk+1 will
be reduced when mk(αk) approximates φ (xk + αk∆ xk, yk + αk∆ yk) badly.

We deﬁne the predicted reduction Predk and the actual reduction Aredk as fol-

lows:

Predk = φ (xk, yk) − mk(αk) = αk
Aredk = φ (xk, yk) − φ (xk + αk∆ xk, yk + αk∆ yk)
(cid:16)
q(cid:107) − (yk)T ∆ xk − (xk)T ∆ yk(cid:17)
(cid:107)rk

− α 2

= αk

k (∆ xk)T ∆ yk.

q(cid:107) − (yk)T ∆ xk − (xk)T ∆ yk(cid:17)
(cid:16)
(cid:107)rk

,

(24)

(25)

Then, we enlarge or reduce the time step ∆tk+1 at every iteration according to the
following ratio:

ρk =

Aredk
Predk

=

(cid:107)rk

q(cid:107) − (yk)T ∆ xk − (xk)T ∆ yk − αk(∆ xk)T ∆ yk
q(cid:107) − (yk)T ∆ xk − (xk)T ∆ yk

(cid:107)rk

,

(26)

where αk = ∆tk/(1 + ∆tk). A particular adjustment strategy is given as follows:

∆tk+1 =






2∆tk, if ρk ≥ η2 and (xk+1, yk+1) > 0,
∆tk, else if η1 ≤ ρk < η2 and (xk+1, yk+1) > 0,
1
2 ∆tk, others,

(27)

where the constants η1, η2 are selected as η1 = 0.25, η2 = 0.75, respectively. We set

(xk+1, yk+1) = (xk, yk) +

∆tk
1 + ∆tk

(∆ xk, ∆ yk).

(28)

When ρk ≥ ηa and (xk+1, yk+1) > 0, we accept the trial step, otherwise we discard
the trial step and set

(xk+1, yk+1) = (xk, yk),

(29)

where ηa is a small positive number such as ηa = 1.0 × 10−6.

8

Luo et al.

Remark 2.3 This new time-stepping control based on the trust-region updating strat-
egy has some advantages compared to the traditional line search strategy. If we use
the line search strategy and the damped Newton method (5) to follow the trajectory
z(t) of the continuous Newton ﬂow (6), in order to achieve the fast rate of conver-
gence in the steady-state phase, the time step αk of the damped Newton method is
tried from 1 and reduced by the half with many times at every iteration. Since the
linear model Fσk µk (zk) + J(zk)∆ zk may not approximate Fσk µk (zk + ∆ zk) well in the
transient-state phase, the time step αk will be small. Consequently, the line search
strategy consumes the unnecessary trial steps in the transient-state phase. However,
the selection of the time step ∆tk based on the trust-region updating strategy (26)-(27)
can overcome this shortcoming.

In order to ensure that the algorithm works well for the general linear comple-
mentarity problem, the initial point selection is also a key point. We select the starting
point (x0, y0) as follows:

x0 = 10 ∗ e, v0 = Mx0 + q, y0

i =

(cid:40)

v0
i , if v0
i > 0,
10−3, otherwise.

(30)

In order to improve the stability of the algorithm, we add a small regularization
item υI to matrix M [8, 22, 28, 54] when µk is not small, where µk is deﬁned by
equation (19). Speciﬁcally, we adopt the following strategy as the regularizer Mυ of
matrix M:

Mυ =

(cid:40)

M + υI, if µk ≥ υ,
M, otherwise,

(31)

where υ = 10−3.

According to the above discussions, we give the detailed descriptions of the path-
following method and the trust-region updating strategy for the monotone linear com-
plementarity problem (1) in Algorithm 1.

3 Convergence analysis

In this section, we analyze the global convergence of Algorithm 1. Without loss
of generality, we assume Mυ = M in the following analysis. Namely, we do not
discriminate between Mυ and M. Similarly to the analysis techniques of the refer-
ences [55, 57, 59], we ﬁrstly construct an auxiliary sequence (uk, vk) as follows:

uk+1 = uk + αk(xk + ∆ xk − uk),
vk+1 = vk + αk(yk + ∆ yk − vk),

(32)

(33)

Regularization path-following methods for LCP

9

Algorithm 1 Regularization path-following methods with the trust-region updating
strategy for linear complementarity problems (The RPFMTr method)
Input:

matrix M ∈ ℜn×n and vector q ∈ ℜn for the problem: y = Mx + q, xiyi = 0 (i = 1 : n), x ≥ 0, y ≥ 0.

Output:

the linear complementarity solution: (solx, soly).

1: Initialize parameters: ηa = 10−6, η1 = 0.25, η2 = 0.75, ε = 10−6, ∆t0 = 10−2, bigMfac = 10, υ =

10−3, σ0 = 0.5, maxit = 600.

break;

q(cid:107) + (xk)T yk)/(2n).

if (ﬂag success trialstep == 1) then

c = xk.yk − σk µk ∗ ones(n, 1).

Set itc = itc + 1.
q = yk − (Mυ xk + q); µk = ((cid:107)rk
Compute rk
Compute Resk = max{(cid:107)xk.yk(cid:107)∞, (cid:107)yk − (Mxk + q)(cid:107)∞}.
if (Resk < ε) then

end if
Set σk = min{σk, µk}.
Compute rk
By solving the linear system (20)-(21), we obtain ∆ xk and ∆ yk.

2: Initialize x0 = bigMfac*ones(n, 1); y0 = Mx0 + q; y0(y0 < 0) = 10−3.
3: Regularize matrix: Mυ = M + υI.
4: Set ﬂag success trialstep = 1, itc = 0, k = 0.
5: while (itc < maxit) do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33: end while
34: Return (solx, soly) = (xk, yk).

Accept the trial point (xk+1, yk+1); Set ﬂag success trialstep = 1.
if ((cid:107)xk+1 − xk(cid:107)∞ > 0.1) then

Set (xk+1, yk+1) = (xk, yk); ﬂag success trialstep = 0.

end if
if (µk < υ) then
Set Mυ = M.

end if
Set k ← k + 1.

Set σk+1 = 0.1.

Set σk+1 = 0.5.

(∆ xk, ∆ yk).

end if

else

else

end if
Set (xk+1, yk+1) = (xk, yk) + ∆tk
1+∆tk
Compute the ratio ρk from equation (26) and adjust ∆tk+1 according to the formula (27).
if ((ρk ≥ ηa) && (xk+1, yk+1) > 0)) then

where (∆ xk, ∆ yk) are solved by equations (16)-(17) and (u0, v0) ≥ (x0, y0) satisﬁes
the feasibility rq(u0, v0) = 0. Then, it is not difﬁcult to verify

rq(uk, vk) = vk − (Muk + q) = 0,
xk+1 − uk+1 = (1 − αk)(xk − uk) ≥ 0,
yk+1 − vk+1 = (1 − αk)(yk − vk) ≥ 0,

(34)

(35)

(36)

where αk = ∆tk/(1 + ∆tk).

10

Luo et al.

Meanwhile, in order to obtain the global convergence, we need to enforce the

condition speciﬁed below. Firstly, we select a constant γ ∈ (0, 1) that satisﬁes

0 < γ ≤

µ0

min1≤i≤n{x0

i y0
i }

=

(x0)T y0 + (cid:107)r0
q(cid:107)
i y0
2n min1≤i≤n{x0
i }

.

Then, we select αk such that

i (α)yk
xk

i (α) ≥ γ µk(α), i = 1, 2, . . . , n

(37)

(38)

holds for all α ∈ (0, αk] ⊂ (0, 1], where xk(α), yk(α), rk
by

q(α) and µk(α) are deﬁned

xk(α) = xk + α∆ xk, yk(α) = yk + α∆ yk,

q(α) = rq(xk(α), yk(α)), µk(α) =
rk

(xk)T yk + (cid:107)rk

q(α)(cid:107)

2n

.

(39)

(40)

Condition (38) is to prevent iterations from prematurely getting too close to the
boundary of the positive quadrant and its restriction on γ is very mild. In the practice,
it can be selected to be very small.

In order to establish the main global convergence of Algorithm 1, similarly to the

results [55, 59], we prove the following several technique lemmas.

Lemma 3.1 When (xk(α), yk(α)) satisﬁes the condition (38), where (∆ xk, ∆ yk) is
the solution of equation (16)-(17), we have

(xk(α), yk(α)) ≥ 0.

i ≥ γ µk (i = 1, 2, . . . , n) and α satisﬁes
(cid:27)

(cid:26)

1 − γ/2
1 + γ/2

σkµk
(cid:107)∆ X k∆ yk(cid:107)∞

,

Furthermore, when xk

i yk

0 ≤ α ≤ min

1,

the inequality (38) holds.

Proof. By summing two sides of the condition (38), we obtain

(xk(α))T yk(α) ≥ nγ µk(α) =

(cid:17)
(cid:16)
(xk(α))T yk(α) + (1 − α)(cid:107)rk
q(cid:107)

γ

1
2

≥

1
2

γ(xk(α))T yk(α).

(41)

(42)

(43)

Consequently, from equation (43) and 0 < γ < 1, we obtain (xk(α))T yk(α) ≥ 0. By
substituting it into the condition (38), we have xk
i (α) ≥ 0 (i = 1, 2, . . . , n). From
equation (17), we obtain

i (α)yk

i ∆ xk
i + αyk
i ∆ yk
i + αxk
i yk
ασkµk = αxk
i
i yk
i (α) + xk
i xk
i + yk
i yk
= (α − 2)xk
i (α), i = 1, 2, . . . , n.

(44)

Regularization path-following methods for LCP

11

i (α), yk
i , yk

If (xk
it with (xk
0 (i = 1, 2, . . . , n), which proves equation (41).

i (α)) < 0 or xk
i ) ≥ 0, it contradicts equation (44). Therefore, we obtain (xk

i (α) < 0 or xk

i (α) < 0, yk

i (α) = 0, yk

i (α) = 0, by combining
i (α), yk
i (α)) ≥

From equations (16)-(17), we have

i (α)yk
xk

i (α) − γ µk(α) = xk
(cid:16)
(xk)T ∆ yk + (yk)T ∆ xk − (cid:107)rk

i + α(xk

i + yk

i ∆ yk

i yk

γα

i ∆ xk
i ∆ yk
i ) + α 2∆ xk
i
q(cid:107) + α(∆ xk)T ∆ yk(cid:17)

− γ µk −

1
2n

= xk

i yk

i + α(σkµk − xk

i yk

i ) + α 2∆ xk

i ∆ yk
i

− γ µk −

= (1 − α)

≥ (1 − α)

γα

1
2n
(cid:16)
i yk
xk
(cid:16)
i yk
xk

(cid:16)
nσkµk − (xk)T yk − (cid:107)rk
(cid:16)

(cid:17)

(cid:16)

+ α

σkµk

1 −

i − γ µk

i − γ µk

(cid:17)

+ α

(cid:16)

σkµk

(cid:16)

1 −

where ∆ X k = diag(∆ xk). By substituting xk
(45), we have

(cid:17)

+ α

q(cid:107) + α(∆ xk)T ∆ yk(cid:17)
(cid:16)
γ
i ∆ yk
∆ xk
i −
2
(cid:13)∆ X k∆ yk(cid:13)
(cid:17) (cid:13)
(cid:16)
γ
γ
(cid:13)
(cid:13)
(cid:13)∞
2
2
i yk
i ≥ γ µk and 0 < α ≤ 1 into equation

(∆ xk)T ∆ yk(cid:17)(cid:17)
(cid:17)

γ
2n

(45)

− α

1 +

(cid:17)

,

i (α)yk
xk

i (α) − γ µk(α) ≥ α

(cid:16)

σkµk

(cid:16)

1 −

(cid:16)

1 +

− α

(cid:17)

γ
2

γ
2

(cid:17) (cid:13)
(cid:13)

(cid:13)∆ X k∆ yk(cid:13)
(cid:13)
(cid:13)∞

(cid:17)

.

(46)

Therefore, when α satisﬁes equation (42), from equation (46), we obtain

i (α)yk
xk

i (α) − γ µk(α) ≥ 0, i = 1, 2, . . . , n,

which gives the inequality (38). (cid:3)

Lemma 3.2 Assume that {(xk, yk)} is generated by Algorithm 1. Then, for any fea-
sible solution ( ¯x, ¯y) of the linear complementarity problem (2), there exists a positive
constant C1 such that

(yk)T (xk − uk) + (xk)T (yk − vk) ≤ C1

(47)

holds for all k = 0, 1, . . .. Furthermore, if ( ¯x, ¯y) is strictly feasible, {(xk, yk)} is
bounded.

From equation (34), we know that (uk, vk) satisﬁes the feasibility. By com-

Proof.
bining it with the semi-deﬁnite positivity of M, we have

( ¯x − uk)T ( ¯y − vk) = ( ¯x − uk)T M( ¯x − uk) ≥ 0.

(48)

Consequently, from equation (48), we have

0 ≤ ( ¯x − uk)T ( ¯y − vk)

= ( ¯x − xk + (xk − uk))T ( ¯y − yk + (yk − vk))
= ¯xT (yk − vk) − (xk)(yk − vk) + ¯xT ¯y + (xk)T yk − ¯xT yk − ¯yT xk
+ (xk − uk)T (yk − vk) + ¯yT (xk − uk) − (yk)T (xk − uk).

(49)

12

Luo et al.

From equations (35)-(36), we have

0 ≤ xk − uk ≤ x0 − u0, 0 ≤ yk − vk ≤ y0 − v0.

(50)

By substituting equation (50) into equation (49), from (xk, yk) ≥ 0, ( ¯x, ¯y) ≥ 0 and
φ (xk, yk) ≤ φ (x0, y0), we obtain

(yk)T (xk − uk) + (xk)(yk − vk) ≤ ¯xT (yk − vk) + ¯yT (xk − uk)

+ (xk)T yk + (xk − uk)T (yk − vk) + ¯xT ¯y
≤ ¯xT (y0 − v0) + ¯yT (x0 − u0) + φ (xk, yk) + (x0 − u0)T (y0 − v0) + ¯xT ¯y
≤ ¯xT (y0 − v0) + ¯yT (x0 − u0) + φ (x0, y0) + (x0 − u0)T (y0 − v0) + ¯xT ¯y
= ¯xT (y0 − v0) + ¯yT (x0 − u0) + (x0 − u0)T (y0 − v0) + 2nµ0 + ¯xT ¯y.

(51)

We set

C1 = (y0 − v0)T (x0 − u0) + ¯yT (x0 − u0) + ¯xT (y0 − v0) + ¯xT ¯y + 2nµ0.

Then, from (51), we obtain the inequality (47).

When ( ¯x, ¯y) is strictly feasible, from inequalities (49)-(50), we have

¯yT xk + ¯xT yk ≤ ¯xT (yk − vk) + ¯yT (xk − uk) + ¯xT ¯y + (xk)T yk + (xk − uk)T (yk − vk)

≤ ¯xT (y0 − v0) + ¯yT (x0 − u0) + ¯xT ¯y + φ (xk, yk) + (x0 − u0)T (y0 − v0)
≤ ¯xT (y0 − v0) + ¯yT (x0 − u0) + ¯xT ¯y + 2nµ0 + (x0 − u0)T (y0 − v0) (cid:44) C2,

(52)

where we use the property φ (xk, yk) ≤ φ (x0, y0) = 2nµ0 in the third inequality. Con-
sequently, from equation (52) and ( ¯x, ¯y) > 0, we have

0 ≤ xk

i ≤

C2
min1≤i≤n{ ¯yi}

, 0 ≤ yk

i ≤

C2
min1≤i≤n{ ¯xi}

, i = 1, 2, . . . , n,

which prove the boundedness of (xk, yk). (cid:3)

Lemma 3.3 Assume that {(xk, yk)} is generated by Algorithm 1 and satisﬁes the
condition (38). Then, when µk ≥ ε > 0, there exists a positive constant ω ∗ such that

(cid:107)Dk∆ xk(cid:107)2 + (cid:107)(Dk)−1∆ yk(cid:107)2 ≤ (ω ∗)2,

(53)

holds for all k = 0, 1, . . ., where Dk = diag (cid:0)(yk./xk)1/2(cid:1), (Dk)−1 = diag (cid:0)(xk./yk)1/2(cid:1)
and (∆ xk, ∆ yk) is the solution of equations (16)-(17).

Proof. We denote

(cid:107)Dk∆ xk(cid:107)2 + (cid:107)(Dk)−1∆ yk(cid:107)2(cid:17)1/2
(cid:16)

.

ωk =

(54)

Then, from equation (54), we have

(cid:107)Dk∆ xk(cid:107)∞ ≤ (cid:107)Dk∆ xk(cid:107) ≤ ωk, (cid:107)(Dk)−1∆ yk(cid:107)∞ ≤ (cid:107)(Dk)−1∆ yk(cid:107) ≤ ωk.

(55)

Regularization path-following methods for LCP

13

From equation (34), we know that (uk, vk) is feasible. By combining it with equa-

tion (16), we obtain

yk + ∆ yk − vk = M(xk + ∆ xk − uk).

(56)

Therefore, from equation (56) and the semi-deﬁnite positivity of M, we have

(yk + ∆ yk − vk)T (xk + ∆ xk − uk) = (xk + ∆ xk − uk)T M(xk + ∆ xk − uk) ≥ 0.

(57)

From equations (35)-(36), we know

x0 − u0 ≥ xk − uk, y0 − v0 ≥ yk − vk.

By substituting them into equation (57), we obtain

(x0 − u0)T (y0 − v0) + (yk − vk)T ∆ xk + (xk − uk)T ∆ yk + (∆ xk)T ∆ yk
≥ (xk − uk)T (yk − vk) + (yk − vk)T ∆ xk + (xk − uk)T ∆ yk + (∆ xk)T ∆ yk ≥ 0.

(58)

By using the inequality |xT y| ≤ (cid:107)x(cid:107)(cid:107)y(cid:107) ≤ (cid:107)x(cid:107)1(cid:107)y(cid:107), from equation (55), we have

(cid:12)
(cid:12)

(cid:12)(yk − vk)T ∆ xk(cid:12)
(cid:12)
(cid:12) =

≤

(cid:13)
(cid:13)
(cid:13)(Dk)−1(yk − vk)
(cid:13)
(cid:13)
(cid:13)1

(cid:12)
(cid:12)
(cid:12)(Dk)−1(yk − vk)T (Dk∆ xk)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)(Dk)−1(yk − vk)
(cid:107)Dk∆ xk(cid:107) ≤
(cid:13)
(cid:13)
(cid:13)1

ωk.

(59)

From the condition (38) and yk − vk ≥ 0, we have

(xk

i /yk

i )1/2(yk

i − vk

i ) = xk

i (yk

i − vk

i )/(xk

i yk

i )1/2 ≤ xk

i (yk

i − vk

i )/(γ µk)1/2.

By substituting equation (60) into equation (59), we obtain

(cid:12)
(cid:12)

(cid:12)(yk − vk)T ∆ xk(cid:12)
(cid:12)
(cid:12) ≤
≤ (xk)T (yk − vk)ωk/(γ µk)1/2.

i /yk

(xk

(cid:32) n
∑
i=1

(cid:33)

i )1/2(yk

i − vk
i )

ωk

Similarly to the proof of equation (61), we obtain

(cid:12)
(cid:12)

(cid:12)(xk − uk)T ∆ yk(cid:12)

(cid:12) ≤ (yk)T (xk − uk)ωk/(γ µk)1/2.
(cid:12)

By substituting inequalities (47) and (61)-(62) into inequality (58), we obtain

(∆ xk)T ∆ yk ≥ −(x0 − u0)T (y0 − v0) −

(cid:12)(xk − uk)T ∆ yk(cid:12)
(cid:12)
(cid:12)
≥ −(x0 − u0)T (y0 − v0) − ((yk)T (xk − uk) + (xk)(yk − vk))ωk/(γ µk)1/2
C1/(γε)1/2(cid:17)
(cid:16)
≥ −(x0 − u0)T (y0 − v0) −

(cid:12)(yk − vk)T ∆ xk(cid:12)
(cid:12)
(cid:12) −

ωk.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(60)

(61)

(62)

(63)

14

Luo et al.

From equation (17) and the condition (38), we have

(cid:107)Dk∆ xk(cid:107)2 + (cid:107)(Dk)−1∆ yk(cid:107)2 + 2(∆ xk)T ∆ yk = (cid:107)Dk∆ xk + (Dk)−1∆ yk(cid:107)2

= (σkµk)2

n
∑
i=1

1
i yk
xk
i

+ (xk)T yk − 2σkµkn

≤ nσ 2

k µk/γ + φ (xk, yk) − 2σkµkn ≤ nµ0

(cid:0)σ 2

max/γ + 2 − 2σmin

(cid:1) .

By substituting inequality (63) into inequality (64), we obtain

q(ωk) (cid:44) ω 2

k − 2

C1/(γε)1/2(cid:17)
(cid:16)

ωk − ζ ≤ 0,

where the positive constant ζ is deﬁned by

ζ = 2(x0 − u0)T (y0 − v0) + nµ0(σ 2

max/γ + 2 − 2σmin).

The quadratic function q(ω) is convex and has a unique positive root at

ω ∗ = C1/(γε)1/2 +

(cid:113)

C2

1/(γε) + ζ .

This implies

Consequently, we obtain inequality (53). (cid:3)

ωk ≤ ω ∗.

(64)

(65)

(66)

(67)

In order to prove the global convergence of Algorithm 1, we need to estimate the

positive lower bound of ∆tk (k = 0, 1, . . .).

Lemma 3.4 Assume that {(xk, yk)} is generated by Algorithm 1 and satisﬁes the
condition (38). Then, when µk ≥ ε > 0, there exists a positive constant δ∆t such that

∆tk ≥

1
2

δ∆t > 0

holds for all k = 0, 1, . . ..

Proof.

From inequality (53), we have

|∆ xk

i | =

i ∆ yk
1
2

≤

ω 2

k ≤

(cid:12)
(cid:12)
(cid:12)

i /yk

(cid:12)
(cid:12)(yk
(cid:12)

i )1/2∆ xk
i

(cid:12)
(cid:12)(xk
(cid:12)
1
(ω ∗)2, i = 1, 2, . . . , n,
2

i /xk

i )1/2∆ yk
i

(cid:12)
(cid:12) ≤ (cid:107)Dk∆ xk(cid:107)(cid:107)(Dk)−1∆ yk(cid:107)
(cid:12)

which gives

(cid:107)∆ X k∆ yk(cid:107)∞ ≤

1
2

(ω ∗)2, k = 0, 1, 2, . . . .

Consequently, we have

σkµk
(cid:107)∆ X k∆ yk(cid:107)∞

≥

2σmin ε
(ω ∗)2 , k = 0, 1, 2, . . . .

(68)

(69)

(70)

Regularization path-following methods for LCP

We denote

α ∗

µ = min

(cid:26)

1,

1 − γ/2
1 + γ/2

(2σmin ε)
(ω ∗)2

(cid:27)

.

Then, when ∆tk satisﬁes

∆tk ≤

α ∗
µ
1 − α ∗
µ

,

15

(71)

(72)

from equations (70)-(71) and (42), we know that the condition (38) holds, where
αk = ∆tk/(1 + ∆tk).

From equation (53) and the Cauchy-Schwartz inequality |xT y| ≤ (cid:107)x(cid:107)(cid:107)y(cid:107), we have

|(∆ xk)T ∆ yk| =

(cid:12)
(cid:12)
(cid:12) ≤ (cid:107)Dk∆ xk(cid:107)(cid:107)(Dk)−1∆ yk(cid:107)
(cid:12)(Dk∆ xk)T ((Dk)−1∆ yk)
(cid:12)
(cid:12)
1
2

(cid:107)Dk∆ xk(cid:107)2 + (cid:107)(Dk)−1∆ yk(cid:107)2(cid:17)
(cid:16)

(ω ∗)2.

≤

≤

1
2
From equation (17), we have

(xk)T ∆ yk + (yk)T ∆ xk = nσkµk − (xk)T yk.

By substituting equations (73)-(74) into equation (26), we obtain

ρk =

Aredk
Predk

= 1 −

(cid:107)rk
αk(∆ xk)T ∆ yk
(2 − σk)nµk

αk(∆ xk)T ∆ yk
q(cid:107) + (xk)T yk − nσkµk

≥ 1 −

(ω ∗)2
2(2 − σmax)nε

αk.

= 1 −

We denote

α ∗

ρ = min

(cid:26)

1,

2(2 − σmax)(1 − η2)nε
(ω ∗)2

(cid:27)

.

Then, when ∆tk satisﬁes

∆tk ≤

α ∗
ρ
1 − α ∗
ρ

,

from equations (75)-(76), we know ρk ≥ η2, where αk = ∆tk/(1 + ∆tk).

We denote

δ∆t (cid:44) min

(cid:40)

σ ∗
µ
1 − σ ∗
µ

,

α ∗
ρ
1 − α ∗
ρ

(cid:41)

, ∆t0

.

(73)

(74)

(75)

(76)

(77)

(78)

Assume that K is the ﬁrst index such that ∆tK ≤ δ∆t holds. Then, from equations (72)-
(78), we know that the condition (38) holds and ρK ≥ η2. Consequently, according to
the time-stepping adjustment scheme (27), ∆tK+1 will be enlarged. Therefore, ∆tk ≥
(1/2)δ∆t holds for all k = 0, 1, 2, . . . .
(cid:117)(cid:116)

By using the estimation results of Lemma 3.4, we prove that {µk} converges to

zero when k tends to inﬁnity as follows.

16

Luo et al.

Theorem 3.1 Assume that {(xk, yk)} is generated by Algorithm 1 and satisﬁes the
condition (38). Then, we have

lim
k→∞

µk = 0.

(79)

Proof. We prove the result (79) by contradiction. Assume that there exists a posi-
tive constant ε such that

µk ≥ ε > 0

(80)

holds for all k = 0, 1, . . .. Then, according to Algorithm 1 and Lemma 3.4, we know
that there exists an inﬁnite subsequence {(xkl , ykl )} such that

φ (xkl , ykl ) − φ (xkl + αkl ∆ xkl , ykl + αkl ∆ ykl )
φ (xkl , ykl ) − mkl (αkl )

≥ η1

(81)

holds for all l = 0, 1, 2, . . ., where αkl = ∆tkl /(1 + ∆tkl ). Otherwise, all steps are
rejected after a given iteration index, then the time step will keep decreasing, which
contradicts equation (68).

From equations (68), (75) and (81), we have

φ (xkl , ykl ) − φ (xkl + αkl ∆ xkl , ykl + αkl ∆ ykl )

≥

η1∆tkl
1 + ∆tkl

(2 − σkl )nµkl ≥

η1δ∆t /2
1 + δ∆t /2

(2 − σmax)nµkl .

(82)

Therefore, from equation (82) and φ (xk+1, yk+1) ≤ φ (xk, yk), we have

φ (x0, y0) ≥ φ (x0, y0) − lim
k→∞

φ (xk, yk) =

(cid:17)
(cid:16)
φ (xk, yk) − φ (xk+1, yk+1)

∞
∑
k=0

(cid:16)
(cid:17)
φ (xkl , ykl ) − φ (xkl +1, ykl +1)

≥

≥

∞
∑
l=0

η1δ∆t /2
1 + δ∆t /2

∞
∑
l=0

(2 − σmax)nµkl .

(83)

Consequently, from equation (83), we obtain

lim
l→∞

µkl = 0,

which contradicts the assumption µk ≥ ε > 0 for all k = 0, 1, . . .. Therefore, we have

lim
k→∞

inf µk = 0.

(84)

Since µk = φ (xk, yk)/(2n) and {φ (xk, yk)} is monotonically decreasing, we know
that {µk} is monotonically decreasing. By combining it with equation (84), we know
(cid:117)(cid:116)
that the result (79) is true.

Regularization path-following methods for LCP

17

Remark 3.1 In the analysis framework of the global convergence, in comparison to
that of the known path-following algorithm, we do not need to select αk such that the
condition of the priority to feasibility over complementarity

(cid:107)rk

q(α)(cid:107) ≤ (xk(α))T yk(α)

(cid:107)r0
q(cid:107)
(x0)T y0

(85)

holds for all α ∈ (0, αk] ⊂ (0, 1].

Theorem 3.2 Assume that {(xk, yk)} is the sequence of iterations generated by Al-
gorithm 1 and satisﬁes the condition (38). If the linear complementarity problem has
a strictly feasible solution ( ¯x, ¯y), {(xk, yk)} exists a limit point (x∗, y∗) and this limit
point satisﬁes the linear complementarity equation (1).

Proof.
Since the linear complementarity problem has a strictly feasible solution,
from Lemma 3.2, we know that {(xk, yk)} is bounded. Consequently, the sequence
{(xk, yk)} exists a convergence subsequence {(xkl , ykl )} and we denote its limit point
as (x∗, y∗). By combining it with Theorem 3.1, we obtain

lim
l→∞

µkl =

1
2n

(cid:18)

lim
l→∞

(xkl )T ykl + lim
l→∞

(cid:107)ykl − (Mxkl + q)(cid:107)

(cid:19)

=

1
2n

(cid:0)(x∗)T y∗ + (cid:107)y∗ − (Mx∗ + q)(cid:107)(cid:1) = 0.

(86)

Since (xkl , ykl ) ≥ 0, we have (x∗, y∗) ≥ 0. By substituting it into equation (86), we
conclude

y∗ − (Mx∗ + q) = 0, (x∗)T y∗ = 0, (x∗, y∗) ≥ 0.

It implies that (x∗, y∗) is the solution of the linear complementarity problem (1). (cid:3)

4 Numerical experiments

In this section, some numerical experiments are conducted to test the performance of
RPFMTr (Algorithm 1) for the linear complementarity problems (LCPs), in compar-
ison to two state-of-the-art commercial solvers, i.e. PATH [15, 18, 19, 48] and MILES
(a Mixed Inequality and nonLinear Equation Solver) [43, 44, 50]. RPFMTr (Algo-
rithm 1) is coded with the MATLAB language and executed in MATLAB (R2020a)
environment [42]. PATH and MILES are executed in the GAMS v28.2 (2019) envi-
ronment [21]. All numerical experiments are performed by a HP notebook with the
Intel quadcore CPU and 8Gb memory. MILES solves the LCP based on the Lemke’s
almost complementary pivoting algorithm [8, 32, 50]. PATH is a solver based on
the path-following procedure [14, 15], the Fisher’s non-smooth regularization tech-
nique [20] and the Lemke’s pivoting algorithm [8, 32]. PATH and MILES are two
robust and efﬁcient solvers for the complementarity problems and used in many gen-
eral modelling systems such as AMPL (A Mathematical Programming Language) [1]

18

Luo et al.

and GAMS (General Algebraic Modelling System) [21]. Therefore, we select these
two solvers as the basis for comparison.

The construction approach of test problems is described as follows. Firstly, we

generate a test matrix M with the following structure:

M =





0 −AT

A 0



 ,

(87)

where the test matrix A comes from the linear programming subset of NETLIB [45].
It is easy to verify that the test matrix M deﬁned by equation (87) is semi-deﬁnite
positive. Then, we generate two complementarity vectors x and y as follows:

x = [1 0 1 0 · · · 1 0]T , y = [0 1 0 1 · · · 0 1]T .

(88)

Finally, by using these two vectors x, y deﬁned by equation (88) and the test matrix
M deﬁned by equation (87), we generate the following test vector q:

q = y − Mx.

(89)

The scales of test problems vary from dimension 78 to 40216. And the termination
conditions of Algorithm 1 (RPFMTr), PATH and MILES for ﬁnding a solution of the
nonlinear system (2) are

(cid:107)y − (Mx + q)(cid:107)∞ ≤ 10−6, (cid:107)Xy(cid:107)∞ ≤ 10−6, (x, y) ≥ 0,

where X = diag(x).

According to the manual of GAMS [19], an LCP will be generated a list of all
variables appearing in the equations found in the model statement, and the number
of equations equals the number of variables. In other words, for an LCP solved in the
GAMS environment, its matrix M should not include the row with all zeros. In order
to avoid this error, we add ε = 1.0 × 10−6 to the ﬁrst element of the row or column
with all zeros in the test sub-matrix A of matrix M deﬁned by equation (87).

Most of matrices A coming from the linear programming subset of NETLIB [45]
are sparse. In order to test the performance of RPFMTr, PATH and MILES for the
dense LCP further, we add a small random perturbation to the elements of the sub-
matrix A in equation (87) and generate the dense test matrix M to replace the test
matrix M in equation (87) as follows:

M =





0 −AT
ε

Aε

0



 , Aε = A + rand(mA, nA) ∗ ε, [mA, nA] = sizes(A),

(90)

where ε = 10−3 and matrix A comes from the linear programming subset of NETLIB
[45].

Numerical results of the sparse test problems are arranged in Tables 1-3 and Fig-
ures 1-2. And numerical results of the dense test problems are arranged in Tables 4-6

Regularization path-following methods for LCP

19

and Figures 3-4. “major” in the fourth column of Tables 1-6 represents the number of
the linear mixed complementarity problems solved by a pivotal Lemke’s method of
PATH [19]. “minor” in the fourth column of Tables 1-6 represents the the number of
pivots performed per major iteration of PATH [19]. “grad” in the fourth column of Ta-
bles 1-6 represents the cumulative number of Jacobian evaluations used in PATH [19].
“major” in the sixth column of Tables 1-6 represents the number of Newton iterations
of MILES [44]. “pivots” in the sixth column of Tables 1-6 represents the number of
Lemke pivots of MILES [44]. “refactor” in the sixth column of Tables 1-6 represents
the number of re-factorizations in the LUSOL solver [23], which is called by MILES
for solving the linear systems of equations.

From Tables 1-6 and Figures 1-4, we ﬁnd that RPFMTr and PATH work well for
the sparse LCPs and the dense LCPs. However, MILES is not robust for solving the
sparse LCPs or the dense LCPs. For the sparse LCPs, PATH performs better than
RPFMTr and MILES from Tables 1-3 and Figures 1-2. For the dense LCPs, from
Tables 4-6, we ﬁnd that PATH and MILES fail to solve 5 problems and 57 problems
of 73 test problems, respectively. RPFMTr solves all the sparse test LCPs and the
dense test LCPs. Furthermore, from Tables 4-6 and Figures 3-4, we ﬁnd that the
computational time of RPFMTr is about 1/3 to 1/10 of that of PATH for the dense
LCPs. Therefore, RPFMTr is a robust and efﬁcient solver for the LCPs, especially
for the dense LCPs.

20

Luo et al.

Fig. 1: Iterations of RPFMTr, PATH and MILES for sparse LCPs.

Fig. 2: CPU (s) of RPFMTr, PATH and MILES for sparse LCPs.

01020304050607080Example100101102103104IterationsRPFMTrPATHMILES01020304050607080Example10-210-1100101102103104105Computational TimeRPFMTrPATHMILESRegularization path-following methods for LCP

21

Table 1: Numerical results of small-scale sparse LCPs (no. 1-27).

RPFMTr

Problems

(n*n)

Exam. 1 lp 25fv47
(n = 2697)

Exam. 2 lp adlittle

(n = 194)

Exam. 3 lp aﬁro
(n = 78)

Exam. 4 lp agg
(n = 1103)

Exam. 5 lp agg2
(n = 1274)

Exam. 6 lp agg3
(n = 1274)

Exam. 7 lp bandm
(n = 777)

steps
(time)

49
(4.13)

45

(0.03)

41
(0.01)

44
(0.19)

46
(0.84)

47
(0.50)

51
(0.27)

Exam. 8 lp beaconfd

50

(n = 468)

Exam. 9 lp blend
(n = 188)

Exam. 10 lp bnl1
(n = 2229)

Exam. 11 lp bore3d
(n = 567)

Exam. 12 lp brandy
(n = 523)

Exam. 13 lp capri
(n = 753)

Exam. 14 lp czprob

(n = 4491)

Exam. 15 lp degen2
(n = 1201)

Exam. 16 lp degen3
(n = 4107)

Exam. 17 lp e226
(n = 695)

Exam. 18 lp etamacro
(n = 1216)

Exam. 19 lp fffff800
(n = 1552)

Exam. 20 lp ﬁnnis
(n = 1561)

Exam. 21 lp ﬁt1d

(n = 1073)

Exam. 22 lp ganges
(n = 3015)

Exam. 23 lp gfrd pnc
(n = 1776)

Exam. 24 lp grow15
(n = 945)

Exam. 25 lp grow22
(n = 1386)

Exam. 26 lp lotﬁ
(n = 519)

Exam. 27 lp maros

(n = 2812)

(0.17)

46
(0.03)

50
(1.24)

48
(0.13)

49
(0.31)

46
(0.16)

51

(3.37)

42
(0.60)

47
(5.85)

54
(0.28)

49
(0.73)

64
(1.27)

47
(0.26)

65

(1.16)

43
(0.49)

61
(0.28)

33
(0.24)

33
(0.42)

52
(0.20)

68

(2.80)

Terr

8.84e-07

9.61e-07

6.20e-07

9.85e-07

8.24e-07

6.64e-07

8.58e-07

6.06e-07

6.12e-07

8.60e-07

8.63e-07

7.42e-07

9.07e-07

8.74e-07

8.53e-07

6.85e-07

9.05e-07

6.31e-07

6.05e-07

6.06e-07

6.45e-07

8.88e-07

7.85e-07

7.23e-07

7.27e-07

6.05e-07

6.18e-07

PATH
major+minor+grad
(time)

8+949+23
(0.30)

4+80+9

(0.02)

3+5+7
(0.02)

11+413+16
(0.08)

10+303+13
(0.03)

9+259+12
(0.05)

11+613+14
(0.05)

8+124+17

(0.03)

9+143+12
(0.02)

12+1727+17
(0.50)

11+423+16
(0.03)

10+384+13
(0.06)

11+573+15
(0.03)

4+25+14

(0.05)

4+447+13
(0.11)

4+185+26
(0.42)

9+412+17
(0.03)

6+440+15
(0.05)

15+1111+20
(0.17)

8+313+16
(0.03)

8+671+12

(0.24)

4+98+21
(0.05)

18+1250+21
(0.17)

7+703+9
(0.13)

8+1054+10
(0.23)

13+263+17
(0.03)

13+3222+14

(0.50)

Terr

5.82e-08

4.44e-12

1.01e-10

2.87e-09

6.96e-10

2.41e-07

3.31e-07

3.76e-09

6.50e-08

5.22e-11

6.99e-07

2.84e-10

7.18e-07

1.90e-10

1.71e-07

3.00e-07

2.94e-07

1.87e-12

1.39e-07

5.24e-09

2.34e-11

4.95e-07

3.80e-11

8.66e-07

3.86e-11

8.42e-07

2.16e-09

MILES
major+pivots+refactor
(time)

1+1684+24
(1.34)

1+1+3

(0.02)

1+1+3
(0.00)

1+1+6
(0.05)

1+959+6
(0.06)

1+955+6
(0.06)

1+1+8
(0.06)

1+1+6

(0.03)

1+1+4
(0.02)

1+1+10
(0.27)

1+1+6
(0.03)

1+1+6
(0.03)

1+1+6
(0.05)

1+1+13

(1.19)

1+1+10
(0.14)

1+1+51
(6.13)

1+1+7
(0.05)

1+58+6
(0.05)

100+72624+770
(13.80)

1+1+9
(0.14)

1+1+3

(0.03)

1+1+11
(0.47)

1+716+13
(0.25)

1+927+6
(0.08)

1+1403+8
(0.13)

1+1+4
(0.05)

1+776+20

(1.13)

Terr

7.48e-12

4.65e-14

3.91e-14

3.74e-12

5.08e-12

9.37e-12

1.19e-12

5.46e-12

1.96e-12

1.62e-12

1.31e-12

1.31e-12

2.60e-12

1.82e-12

2.22e-16

0.00e-00

5.86e-12

6.82e-12

1.78e+01
(failed)

4.55e-13

1.75e-10

2.40e-14

3.64e-12

3.33e-14

1.20e-14

9.10e-13

2.95e-09

22

Luo et al.

Table 2: Numerical results of small-scale sparse LCPs (no. 28-53).

Problems

(n*n)

RPFMTr

PATH

MILES

steps
(time)

Terr

major+minor+grad
(time)

Terr

major+pivots+refactor
(time)

Terr

Exam. 28 lp modszk1

42

(n = 2307)

Exam. 29 lp perold
(n = 2131)

Exam. 30 lp pilot ja
(n = 3207)

Exam. 31 lp qap8
(n = 2544)

Exam. 32 lp lp recipe
(n = 295)

Exam. 33 lp sc50a
(n = 128)

Exam. 34 lp scagr7

(n = 314)

Exam. 35 lp scagr25
(n = 1142)

Exam. 36 lp scfxm1
(n = 930)

Exam. 37 lp scfxm2
(n = 1860)

Exam. 38 lp scfxm3
(n = 2790)

Exam. 39 lp scorpion
(n = 854)

Exam. 40 lp shell

(n = 2313)

Exam. 41 lp ship04l
(n = 2568)

Exam. 42 lp ship04s
(n = 1908)

Exam. 43 lp ship08s
(n = 3245)

Exam. 44 lp ship12s
(n = 4020)

Exam. 45 lp sierra
(n = 3962)

(0.46)

67
(2.62)

78
(7.43)

43
(4.78)

53
(0.04)

40
(0.01)

42

(0.03)

42
(0.08)

52
(0.51)

52
(0.61)

53
(1.14)

40
(0.08)

45

(0.40)

50
(1.06)

49
(0.57)

47
(0.91)

48
(0.91)

63
(1.35)

Exam. 46 lp standgub

60

(n = 1744)

Exam. 47 lp tuff
(n = 961)

Exam. 48 lp wood1p
(n = 2839)

Exam. 49 lpi box1
(n = 492)

Exam. 50 lpi cplex2
(n = 602)

Exam. 51 lpi ex72a
(n = 412)

Exam. 52 lpi ex73a

(n = 404)

Exam. 53 lpi mondou2
(n = 916)

(0.70)

61
(0.69)

53
(6.00)

39
(0.03)

41
(0.07)

40
(0.03)

40

(0.03)

38
(0.09)

8.61e-07

9.15e-07

7.94e-07

8.80e-07

7.81e-07

8.30e-07

6.28e-07

6.30e-07

9.05e-07

8.84e-07

6.17e-07

8.40e-07

6.34e-07

7.03e-07

9.60e-07

5.31e-07

2.68e-07

6.79e-07

9.29e-07

3.85e-07

7.48e-07

6.17e-07

7.33e-07

6.16e-07

7.56e-07

6.85e-07

5+370+14

(0.05)

12+1498+23
(0.42)

14+2468+23
(1.81)

9+2413+11
(1.31)

8+103+11
(0.00)

3+59+6
(0.05)

5+110+8

(0.02)

4+205+13
(0.02)

12+721+15
(0.11)

14+1357+18
(0.14)

14+2110+17
(0.27)

5+111+15
(0.03)

9+2448+11

(0.38)

11+3359+14
(0.77)

12+2376+15
(0.24)

12+3328+15
(0.44)

13+3446+16
(0.52)

12+2135+23
(0.53)

4+99+12

(0.02)

9+828+13
(0.13)

7+1285+12
(19.56)

7+290+9
(0.02)

9+422+11
(0.03)

7+164+9
(0.00)

6+125+8

(0.00)

6+555+8
(0.08)

7.91e-11

1.72e-08

3.29e-07

1.83e-11

7.33e-12

9.21e-07

1.41e-10

4.24e-07

7.23e-10

3.36e-09

4.96e-07

1.35e-07

5.86e-10

4.05e-07

4.04e-07

9.32e-07

3.14e-07

7.90e-07

2.88e-12

4.81e-07

9.65e-10

7.28e-07

1.45e-09

1.24e-12

6.75e-10

5.02e-07

1+1+9

(0.22)

100+18168+1156
(35.88)

100+171048+1900
(140.47)

6+50530+477
(27.30)

1+1+4
(0.02)

1+1+3
(0.03)

1+1+3

(0.00)

1+1+6
(0.05)

1+1+7
(0.06)

1+1+11
(0.24)

1+200+14
(0.70)

1+1+5
(0.03)

1+1+9

(0.24)

1+1+8
(0.23)

1+1+6
(0.09)

1+1+9
(0.41)

1+1+9
(0.53)

1+1+11
(0.72)

1+1+7

(0.09)

1+1+10
(0.11)

1+3237+19
(1.36)

1+279+3
(0.02)

1+1+4
(0.03)

1+1+3
(0.02)

1+1+3

(0.02)

1+1+5
(0.03)

8.70e-14

6.64e+01
(failed)

3.92e+04
(failed)

2.13e-14

5.12e-13

1.33e-15

1.07e-14

1.07e-14

2.62e-12

1.00e-11

7.38e-12

6.22e-15

0.00e-00

1.81e-14

1.81e-14

5.68e-14

4.17e-14

4.37e-11

1.16e-13

3.82e-11

5.09e-09

1.11e-16

4.44e-15

0.00e-00

0.00e-00

0.00e-00

Regularization path-following methods for LCP

23

Table 3: Numerical results of large-scale sparse LCPs (no. 54-78).

RPFMTr

Problems

(n*n)

Exam. 54 lp 80bau3b
(n = 14323)

Exam. 55 lp bnl2

(n = 6810)

Exam. 56 lp cre a
(n = 10764)

Exam. 57 lp cre c
(n = 9479)

Exam. 58 lp cycle
(n = 5274)

Exam. 59 lp d6cube
(n = 6599)

Exam. 60 lp ﬁt2d
(n = 10549)

steps
(time)

49
(7.56)

47

(4.83)

53
(5.14)

50
(4.50)

59
(5.21)

56
(48.11)

69
(68.27)

Exam. 61 lp greenbea

50

(n = 7990)

Exam. 62 lp greenbeb
(n = 7990)

Exam. 63 lp ken 07
(n = 6028)

Exam. 64 lp pds 02
(n = 10669)

Exam. 65 lp pilot87
(n = 8710)

Exam. 66 lp qap12
(n = 12048)

Exam. 67 lp ship08l

(n = 5141)

Exam. 68 lp ship12l
(n = 6684)

Exam. 69 lp truss
(n = 9806)

Exam. 70 lp woodw
(n = 9516)

Exam. 71 lpi bgindy
(n = 13551)

Exam. 72 lpi gran
(n = 5183)

Exam. 73 lpi greenbea

(n = 7989)

Exam. 74 lp ken 11
(n = 36043)

Exam. 75 lp osa 07
(n = 26185)

Exam. 76 lp pds 06
(n = 39232)

Exam. 77 lp qap15
(n = 28605)

Exam. 78 lp stocfor3
(n = 40216)

(13.68)

50
(14.48)

42
(1.09)

38
(2.66)

59
(59.97)

45
(223.00)

48

(1.87)

47
(1.88)

44
(4.72)

86
(201.33)

44
(50.77)

81
(7.50)

50

(11.91)

43
(16.37)

45
(451.93)

38
(115.16)

46
(824.18)

54
(11.08)

Terr

6.82e-07

7.01e-07

8.39e-07

8.58e-07

4.59e-07

2.56e-07

6.72e-07

7.29e-07

7.29e-07

6.24e-07

9.65e-07

7.96e-07

6.25e-07

5.86e-07

8.51e-07

6.22e-07

6.31e-07

8.49e-07

9.94e-07

9.94e-07

6.95e-07

6.55e-07

9.02e-07

6.34e-07

8.52e-07

PATH
major+minor+grad
(time)

9+858+23
(0.30)

12+4597+17

(1.47)

12+5755+17
(2.22)

11+3604+14
(1.34)

17+4532+24
(2.58)

5+941+19
(0.53)

7+2114+10
(21.23)

12+3899+28

(2.91)

12+3899+28
(3.19)

8+4004+11
(0.98)

5+3264+26
(3.27)

10+3002+21
(2.36)

7+9138+9
(205.47)

10+1676+21

(0.80)

14+8950+17
(4.24)

6+5607+9
(6.92)

10+4715+19
(9.89)

4+614+25
(1.28)

15+1988+28
(1.27)

11+3173+29

(2.94)

9+23833+12
(172.78)

4+954+17
(2.64)

5+6809+37
(43.47)

5+20058+8
(18004.75)

21+1623+50
(5.69)

Terr

1.52e-10

2.72e-11

4.09e-07

1.77e-09

8.67e-09

7.37e-07

1.53e-09

1.01e-09

1.01e-09

9.71e-09

7.09e-09

6.47e-07

8.54e-07

5.38e-07

1.22e-10

4.99e-08

1.78e-09

1.20e-07

8.14e-07

4.68e-07

2.65e-08

1.77e-11

9.00e-11

2.11e+01
(failed)

2.78e-12

MILES
major+pivots+refactor
(time)

41+101114+1107
(1015.16)

1+1+27

(6.84)

1+1+42
(22.98)

1+490+40
(18.56)

100+197608+2800
(461.45)

1+6085+39
(10.75)

1+1+3
(0.72)

Terr

6.19e+01
(failed)

1.24e-12

1.48e-12

1.59e-12

7.15e+03
(failed)

2.73e-12

3.49e-10

100+74070+1754

9.90e+01

(513.08)

100+74070+1754
(514.13)

1+1+21
(3.31)

1+1+47
(24.70)

100+121000+1500
(498.08)

12+145940+1427
(1064.42)

1+1+12

(1.31)

1+1+16
(3.03)

98+299152+2379
(1011.18)

100+232954+1904
(735.08)

1+1+39
(34.14)

100+70036+700
(74.33)

100+303396+2731

(921.31)

1+1+142
(1081.75)

1+1+23
(78.39)

1+1+240
(1885.67)

3+39920+342
(1410.55)

2+4776+135
(1169.09)

(failed)

9.90e+01
(failed)

0.00e-00

0.00e-00

1.00e+03
(failed)

1.07e+01
(failed)

5.68e-14

4.17e-14

1.67e+00
(failed)

2.00e+05
(failed)

9.06e-08

5.68e+03
(failed)

6.72e+01

(failed)

0.00e-00

3.87e-09

0.00e-00

1.20e+01
(failed)

5.14e+01
(failed)

24

Luo et al.

Fig. 3: Iterations of RPFMTr, PATH and MILES for dense LCPs.

Fig. 4: CPU (s) of RPFMTr, PATH and MILES for dense LCPs.

01020304050607080Example100101102103104IterationsRPFMTrPATHMILES01020304050607080Example10-210-1100101102103104105Computational TimeRPFMTrPATHMILESRegularization path-following methods for LCP

25

Table 4: Numerical results of small-scale dense LCPs (no. 1-27).

RPFMTr

Problems

(n*n)

Exam. 1 lp 25fv47
(n = 2697)

Exam. 2 lp adlittle
(n = 194)

Exam. 3 lp aﬁro
(n = 78)

Exam. 4 lp agg
(n = 1103)

Exam. 5 lp agg2
(n = 1274)

Exam. 6 lp agg3

(n = 1274)

Exam. 7 lp bandm
(n = 777)

Exam. 8 lp beaconfd
(n = 468)

Exam. 9 lp blend
(n = 188)

Exam. 10 lp bnl1
(n = 2229)

Exam. 11 lp bore3d
(n = 567)

Exam. 12 lp brandy

(n = 523)

Exam. 13 lp capri
(n = 753)

Exam. 14 lp czprob
(n = 4491)

Exam. 15 lp degen2
(n = 1201)

Exam. 16 lp degen3
(n = 4107)

Exam. 17 lp e226
(n = 695)

steps

(time)

56
(16.83)

43
(0.26)

40
(0.03)

53
(2.78)

54
(2.80)

44

(1.64)

51
(0.78)

51
(0.25)

44
(0.03)

52
(9.88)

50
(0.40)

53

(0.34)

52
(0.75)

44
(42.87)

49
(2.32)

45
(35.17)

55
(0.65)

Exam. 18 lp etamacro

51

(n = 1216)

Exam. 19 lp fffff800
(n = 1552)

Exam. 20 lp ﬁnnis
(n = 1561)

Exam. 21 lp ﬁt1d
(n = 1073)

(2.46)

70
(5.76)

46
(3.87)

54
(2.09)

Exam. 22 lp ganges
(n = 3015)

40
(14.89)

Exam. 23 lp gfrd pnc
(n = 1776)

Exam. 24 lp grow15

(n = 945)

Exam. 25 lp grow22
(n = 1386)

Exam. 26 lp lotﬁ
(n = 519)

64
(7.15)

42

(1.17)

41
(2.65)

46
(0.29)

Exam. 27 lp maros
(n = 2812)

69
(21.44)

Terr

5.85e-07

7.90e-07

7.49e-07

7.48e-07

7.30e-07

7.03e-07

9.17e-07

8.57e-07

5.39e-07

6.71e-07

9.26e-07

5.13e-07

5.41e-07

8.31e-07

7.63e-07

6.85e-07

6.15e-07

8.41e-07

6.93e-07

9.59e-07

8.83e-07

5.23e-07

6.02e-07

7.34e-07

9.94e-07

6.47e-07

8.93e-07

PATH
major+minor+grad

(time)

12+3457+22
(200.42)

8+203+10
(0.03)

9+73+11
(0.03)

12+1284+17
(5.20)

10+1071+13
(4.61)

10+1053+13

(4.58)

14+1075+18
(2.80)

14+646+17
(0.45)

13+258+15
(0.03)

13+3083+17
(56.92)

13+745+18
(0.81)

13+697+16

(0.53)

10+761+19
(1.45)

9+3102+16
(650.25)

12+1582+14
(7.38)

9+4003+20
(388.39)

14+953+17
(1.14)

7+640+18

(6.08)

14+2172+21
(15.77)

10+1583+18
(14.41)

8+681+13
(0.53)

8+1910+28
(111.67)

14+1694+20
(40.06)

8+856+10

(2.91)

9+1535+11
(12.94)

11+408+18
(0.48)

15+4178+17
(193.91)

Terr

2.78e-09

1.33e-11

2.58e-11

5.24e-08

5.44e-07

1.51e-07

8.96e-11

9.99e-07

4.73e-08

1.65e-09

8.51e-08

2.73e-07

2.37e-09

3.36e-07

2.83e-07

9.91e-07

1.45e-10

5.85e-07

2.09e-08

1.92e-07

3.46e-10

6.15e-08

2.69e-09

9.04e-13

4.57e-08

3.33e-11

1.04e-08

MILES
major+pivots+refactor

(time)

100+61000+600
(132.30)

1+327+3
(0.02)

1+77+2
(0.08)

100+26000+600
(47.64)

1+132+8
(2.30)

1+1335+8

(2.25)

100+23867+901
(37.73)

100+25300+500
(4.72)

1+547+4
(0.03)

100+26600+800
(121.78)

2+1525+12
(0.38)

100+71600+600

(11.34)

100+79200+800
(37.36)

100+6200+500
(120.50)

3+566+16
(2.25)

100+66800+600
(129.42)

100+169800+1100
(34.97)

2+120+9

(1.17)

100+99000+700
(58.69)

100+24400+500
(28.81)

1+152+4
(0.06)

82+104468+1476
(1003.45)

100+6800+300
(14.97)

1+1+11

(1.02)

100+2600+400
(18.05)

100+47400+500
(13.56)

100+14400+300
(35.28)

Terr

3.56e+02
(failed)

1.38e-12

2.22e-14

1.43e+00
(failed)

3.58e-09

3.42e-11

2.74e+02
(failed)

1.76e+02
(failed)

1.83e-09

7.79e+01
(failed)

1.05e-11

2.72e+01

(failed)

2.10e+01
(failed)

5.33e+00
(failed)

2.47e-09

9.50e+01
(failed)

3.32e+01
(failed)

1.36e-08

1.09e+05
(failed)

5.84e+01
(failed)

5.64e-09

9.53e+00
(failed)

2.16e+03
(failed)

1.44e-09

1.68e+00
(failed)

4.44e-02
(failed)

2.35e+04
(failed)

26

Luo et al.

Table 5: Numerical results of small-scale dense LCPs (no. 28-53).

RPFMTr

Problems

(n*n)

Exam. 28 lp modszk1
(n = 2307)

Exam. 29 lp perold
(n = 2131)

Exam. 30 lp pilot ja
(n = 3207)

Exam. 31 lp qap8
(n = 2544)

Exam. 32 lp lp recipe
(n = 295)

Exam. 33 lp sc50a

(n = 128)

Exam. 34 lp scagr7
(n = 314)

Exam. 35 lp scagr25
(n = 1142)

Exam. 36 lp scfxm1
(n = 930)

Exam. 37 lp scfxm2
(n = 1860)

Exam. 38 lp scfxm3
(n = 2790)

steps

(time)

46
(9.37)

60
(10.21)

76
(32.29)

42
(10.69)

53
(0.09)

42

(0.02)

41
(0.07)

41
(1.72)

54
(1.37)

58
(7.08)

61
(18.68)

Exam. 39 lp scorpion

40

(n = 854)

Exam. 40 lp shell
(n = 2313)

Exam. 41 lp ship04l
(n = 2568)

Exam. 42 lp ship04s
(n = 1908)

Exam. 43 lp ship08s
(n = 3245)

Exam. 44 lp ship12s
(n = 4020)

Exam. 45 lp sierra

(n = 3962)

Exam. 46 lp standgub
(n = 1744)

Exam. 47 lp tuff
(n = 961)

Exam. 48 lp wood1p
(n = 2839)

Exam. 49 lpi box1
(n = 492)

Exam. 50 lpi cplex2
(n = 602)

Exam. 51 lpi ex72a

(n = 412)

Exam. 52 lpi ex73a
(n = 404)

Exam. 53 lpi mondou2
(n = 916)

(0.79)

50
(10.06)

42
(10.56)

42
(5.50)

43
(18.97)

42
(30.77)

47

(33.55)

53
(5.65)

61
(1.72)

64
(20.82)

30
(0.20)

43
(0.45)

34

(0.12)

33
(0.12)

38
(0.94)

Terr

8.17e-07

7.57e-07

9.82e-07

8.04e-07

7.61e-07

7.79e-07

8.19e-07

8.73e-07

5.12e-07

8.13e-07

9.23e-07

7.15e-07

6.00e-07

5.64e-07

7.15e-07

9.32e-07

6.94e-07

8.12e-07

6.12e-07

5.46e-07

6.11e-07

6.72e-07

5.29e-07

5.52e-07

9.00e-07

5.86e-07

PATH
major+minor+grad

(time)

13+3281+16
(89.69)

16+2738+24
(66.41)

18+4063+29
(272.11)

12+3423+14
(116.20)

10+240+15
(0.16)

5+92+7

(0.02)

12+391+14
(0.16)

9+648+19
(3.58)

12+1260+16
(2.67)

15+2685+19
(29.86)

13+3864+17
(135.97)

13+1094+15

(2.64)

13+3371+15
(85.95)

12+4243+15
(96.83)

12+3170+15
(32.59)

12+5907+15
(279.95)

9+3805+21
(333.53)

12+4069+16

(319.80)

12+2064+16
(19.86)

15+1404+18
(3.91)

58+3749+64
(473.97)

13+657+15
(0.56)

12+627+14
(0.95)

13+570+15

(0.41)

13+552+15
(0.38)

11+773+13
(2.36)

Terr

4.94e-07

1.33e-09

7.42e-07

1.81e-07

6.28e-08

2.25e-08

1.24e-08

1.65e-07

1.22e-07

3.10e-10

2.04e-07

2.19-10

4.39e-08

1.66e-09

4.05e-07

6.91e-08

2.36e-10

7.15e-09

3.20e-08

1.50e-10

1.45e-09

7.62e-08

6.53e-07

5.36e-09

7.35e-09

6.92e-10

MILES
major+pivots+refactor

(time)

100+34000+500
(33.89)

100+4200+400
(16.06)

100+71396+779
(233.45)

100+23000+400
(29.77)

100+5144+595
(4.11)

1+135+2

(0.02)

2+452+8
(0.11)

100+53500+700
(33.66)

100+102400+1462
(75.61)

100+104383+902
(94.95)

100+27200+500
(46.27)

100+84572+800

(31.31)

100+56406+497
(45.75)

100+31600+500
(61.70)

100+20000+500
(32.59)

100+70700+700
(162.72)

77+205665+1472
(1002.61)

100+5000+300

(35.28)

100+94960+792
(146.17)

100+24300+939
(62.39)

100+2700+300
(10.44)

2+2115+24
(0.66)

100+46800+798
(25.13)

1+867+6

(0.14)

100+6739+600
(9.94)

2+1151+9
(0.63)

Terr

9.03e+00
(failed)

2.88e+04
(failed)

5.77e+06
(failed)

1.18e+01
(failed)

1.91e+01
(failed)

7.11e-14

1.77e-13

1.09e+01
(failed)

2.23e+02
(failed)

2.35e+02
(failed)

3.89e+02
(failed)

1.38e+00

(failed)

1.17e+02
(failed)

1.31e+02
(failed)

5.69e+01
(failed)

2.38e+02
(failed)

1.97e+02
(failed)

9.99e+04

(failed)

2.44e+03
(failed)

7.78e+03
(failed)

7.27e+05
(failed)

2.67e-15

1.48e+02
(failed)

3.13e-14

1.50e-03
(failed)

1.55e-14

Regularization path-following methods for LCP

27

Table 6: Numerical results of large-scale dense LCPs (no. 54-73).

Terr

5.38e-07

8.21e-07

6.36e-07

8.33e-07

5.76e-07

6.46e-07

7.06e-07

8.66e-07

9.43e-07

8.68e-07

7.48e-07

5.54e-07

8.45e-07

9.12e-07

7.67e-07

6.61e-07

7.61e-07

7.46e-07

5.32e-07

5.37e-07

PATH
major+minor+grad
(time)

5+8283+8
(18049.72)

11+8765+18
(4549.92)

4+3835+7
(18099.73)

7+8042+10
(18008.97)

12+7861+17

(2322.98)

10+6621+16
(725.33)

8+5337+13
(185.28)

12+11911+16
(7877.61)

12+12033+16
(10248.31)

8+4721+28
(2922.75)

8+8486+26

(15408.33)

11+8828+23
(7239.50)

3+5789+6
(18016.27)

12+9419+15
(1537.80)

12+12138+15
(7256.50)

10+6890+12
(10715.00)

14+7490+23

(18080.28)

1+663+6
(18181.77)

14+5542+23
(1450.42)

12+12005+16
(12504.91)

Terr

3.92e+01
(failed)

3.93e-08

7.38e+01
(failed)

2.68e+00
(failed)

2.77e-07

5.15e-07

2.82e-09

5.44e-09

2.46e-08

1.13e-07

1.82e-08

8.07e-10

2.73e+01
(failed)

1.03-09

1.51-07

2.30e-08

1.35e-05

3.12e+02
(failed)

1.29e-08

7.46e-09

MILES
major+pivots+refactor
(time)

2+8245+54
(1432.94)

43+27624+298
(1009.19)

138+49799+558
(1003.36)

140+28754+599
(1004.88)

100+15300+400

(148.55)

80+66400+800
(1009.78)

2+147+4
(1.11)

100+15200+400
(373.86)

100+17600+500
(575.45)

56+6944+448
(1008.24)

100+9000+300

(504.81)

56+48048+560
1014.17)

100+23400+400
(771.77)

100+80800+900
(464.31)

29+11948+145
(1001.00)

23+8685+201
(1022.70)

100+15186+299

(340.14)

9+11664+81
(1025.18)

Terr

1.32e+03
(failed)

9.22e+01
(failed)

1.51e+03
(failed)

3.55e+03
(failed)

2.07e+03

(failed)

2.19e+03
(failed)

3.78e-10

1.14e+02
(failed)

1.14e+02
(failed)

5.01e+01
(failed)

1.86e+01

(failed)

9.99e+02
(failed)

2.14e+01
(failed)

4.34e+02
(failed)

7.53e+02
(failed)

8.80e+00
(failed)

2.06e+02

(failed)

2.08e+02
(failed)

500+145000+2500
(719.47)

47+15546+227
(1006.56)

2.357e+03
(failed)

1.13e+02
(failed)

Problems

(n*n)

RPFMTr

steps
(time)

Exam. 54 lp 80bau3b
(n = 14323)

45
(1385.62)

Exam. 55 lp bnl2
(n = 6810)

Exam. 56 lp cre a
(n = 10764)

Exam. 57 lp cre c
(n = 9479)

55
(153.40)

61
(583.43)

55
(373.29)

Exam. 58 lp cycle

62

(n = 5274)

Exam. 59 lp d6cube
(n = 6599)

Exam. 60 lp ﬁt2d
(n = 10549)

Exam. 61 lp greenbea
(n = 7990)

Exam. 62 lp greenbeb
(n = 7990)

Exam. 63 lp ken 07
(n = 6028)

(94.96)

54
(141.97)

59
(550.98)

50
(223.83)

50
(225.27)

42
(96.95)

Exam. 64 lp pds 02

48

(n = 10669)

Exam. 65 lp pilot87
(n = 8710)

Exam. 66 lp qap12
(n = 12048)

Exam. 67 lp ship08l
(n = 5141)

Exam. 68 lp ship12l
(n = 6684)

Exam. 69 lp truss
(n = 9806)

(437.80)

58
(307.63)

48
(722.82)

47
(66.37)

47
(124.64)

45
(339.85)

Exam. 70 lp woodw

71

(n = 9516)

Exam. 71 lpi bgindy
(n = 13551)

Exam. 72 lpi gran
(n = 5183)

Exam. 73 lpi greenbea
(n = 7989)

(494.92)

53
(1297.80)

60
(81.64)

50
(211.05)

5 Conclusions

In this paper, we give the regularization path-following method with the trust-region
updating strategy (RPFMTr) for the linear complementarity problem. Meanwhile,
we prove the global convergence of the new method under the standard assump-
tions without the condition of the priority to feasibility over complementarity. Nu-

28

Luo et al.

merical results show that RPFMTr is a robust and efﬁcient solver for the linear
complementarity problem, especially for the dense linear complementarity problem.
Furthermore, it is more robust and faster than some state-of-the-art solvers such as
PATH [15, 18, 19, 48] and MILES [43, 44, 50] (the built-in subroutines of the GAMS
v28.2 (2019) environment [21]). The computational time of RPFMTr is about 1/3
to 1/10 of that of PATH for the dense linear complementarity problem. Therefore,
RPFMTr is an alternating solver for the linear complementarity problem and worth
exploring further for the nonlinear complementarity problem.

Acknowledgments

This work was supported in part by Grant 61876199 from National Natural Science
Foundation of China, Grant YBWL2011085 from Huawei Technologies Co., Ltd.,
and Grant YJCB2011003HI from the Innovation Research Program of Huawei Tech-
nologies Co., Ltd..

Conﬂicts of interest / Competing interests: Not applicable.

Availability of data and material (data transparency): If it is requested, we will
provide the test data.

Code availability (software application or custom code): If it is requested, we will
provide the code.

References

1. AMPL, A Mathematical Programming Language, http://www.ampl.com, 2022.
2. E. L. Allgower and K. Georg, Introduction to Numerical Continuation Methods, SIAM, Philadelphia,

2003.

3. U. M. Ascher and L. R. Petzold, Computer Methods for Ordinary Differential Equations and

Differential-Algebraic Equations, SIAM, Philadelphia, 1998.

4. O. Axelsson and S. Sysala, Continuation Newton methods, Comput. Math. Appl. 70 (2015), 2621-

2637.

5. F. H. Branin, Widely convergent method for ﬁnding multiple solutions of simultaneous nonlinear equa-

tions, IBM J. Res. Dev. 16 (1972), 504-521.

6. J. C. Butcher and Z. Jackiewicz, Construction of high order diagonally implicit multistage integration

methods for ordinary differential equations, Appl. Numer. Math. 27 (1998), 1-12.

7. K. E. Brenan, S. L. Campbell, and L. R. Petzold, Numerical solution of initial-value problems in

differential-algebraic equations, SIAM, Philadelphia, 1996.

8. R. W. Cottle, J.-S. Pang and R. E. Stone, The Linear Complementarity Problem, SIAM, Philadelphia,

2009.

9. A. R. Conn, N. Gould, and Ph. L. Toint, Trust-Region Methods, SIAM, Philadelphia, 2000.
10. M. T. Chu and M. M. Lin, Dynamical system characterization of the central path and its variants- a

vevisit, SIAM J. Appl. Dyn. Syst. 10 (2011), 887-905.

11. D. F. Davidenko, On a new method of numerical solution of systems of nonlinear equations (in Rus-

sian), Dokl. Akad. Nauk SSSR 88 (1953), 601–602.

12. P. Deuﬂhard, Newton Methods for Nonlinear Problems: Afﬁne Invariance and Adaptive Algorithms,

Springer, Berlin, 2004.

13. P. Deuﬂhard, H. J. Pesch, and P. Rentrop, A modiﬁed continuation method for the numerical solution
of nonlinear two-point boundary value problems by shooting techniques, Numer. Math. 26 (1975),
327-343.

Regularization path-following methods for LCP

29

14. S. P. Dirkse, Robust Solution of Mixed Complementarity Problems, PhD thesis, Computer Sciences

Department, University of Wisconsin, Madison, Wisconsin, 1994.

15. S. Dirkse and M. C. Ferris, The PATH solver: A non-monotone stabilization scheme for mixed com-

plementarity problems, Optim. Methods Softw. 5 (1995), 123-156.

16. E. J. Doedel, Lecture notes in numerical analysis of nonlinear equations, in Numerical Continuation
Methods for Dynamical Systems, B. Krauskopf, H.M. Osinga, and J. Gal´an-Vioque, eds., Springer,
Berlin, 2007, pp. 1-50.

17. K. Erleben, Velocity-based shock propagation for multibody dynamics animation, ACM Trans. Graph.

26 (2):12 (2007), https://doi.org/10.1145/1243980.1243986.1.

18. M. C. Ferris and T. S. Munson, Complementarity problems in GAMS and the PATH solver, J. Econ.

Dyn. Control 24 (2000), 165-188.

19. M. C. Ferris and T. S. Munson, The manual of PATH, GAMS Corporation, https://www.gams.

com/latest/docs/S_PATH.html, 2022.

20. A. Fischer, A special Newton-type optimization method, Optimization 24 (1992), 269-284.
21. GAMS v28.2, GAMS Corporation, https://www.gams.com/, 2019.
22. A. Gana, Studies in the Complementarity Problem, Ph.D. dissertation, Department of Industrial and

Operations Engineering, University of Michigan, Ann Arbor, MI, 1982.

23. P. Gill, W. Murray, M. A. Saunders, M. H. Wright, Mamtaining LU factors of a general sparse matrix,

Linear Algebra Appl. 88-89 (1991), 239-270.

24. G. H. Golub and C. F. Van Loan, Matrix Computation, 4th ed., The John Hopkins University Press,

Baltimore, 2013.

25. T. H. Gronwall, Note on the derivatives with respect to a parameter of the solutions of a system of dif-
ferential equations, Ann. of Math. 20 (1919), pp. 292-296, JFM 47.0399.02 (https://zbmath.org/
?format=complete&q=an:47.0399.02), JSTOR 1967124 (https://www.jstor.org/stable/
1967124), MR 1502565 (https://www.ams.org/mathscinet-getitem?mr=1502565).

26. E. Hairer and G. Wanner, Solving Ordinary Differential Equations II, Stiff and Differential-Algebraic

Problems, 2nd ed., Springer, Berlin, 1996.

27. D. J. Higham, Trust region algorithms and timestep selection, SIAM J. Numer. Anal. 37 (1999), 194-

210.

28. P. C. Hansen, Regularization Tools: A MATLAB package for analysis and solution of discrete ill posed

problems. Numer. Algorithms 6 (1994), 23-29.

29. Z. Jackiewicz, S. Tracogna, A general class of two-step Runge-Kutta methods for ordinary differential

equations, SIAM J. Numer. Anal. 32 (1995), 1390-1427.

30. C. T. Kelley, Solving Nonlinear Equations with Newton’s Method, SIAM, Philadelphia, 2003.
31. C. T. Kelley, Numerical methods for nonlinear equations, Acta Numer. 27 (2018), 207-287.
32. C. E. Lemke and J. T. Howson, Jr. Equilibrium points of bimatrix games, Journal of the Society for

Industrial and Applied Mathematics 12 (1964), 413-423.

33. X.-L. Luo, A second-order pseudo-transient method for steady-state problems, Appl. Math. Comput.

216 (2010), 1752-1762.

34. X.-L. Luo, A dynamical method of DAEs for the smallest eigenvalue problem, J. Comput. Sci. 3

(2012), 113-119.

35. X.-L. Luo, H. Xiao, J.-H. Lv and S. Zhang, Explicit pseudo-transient continuation and the trust-region
updating strategy for unconstrained optimization, Appl. Numer. Math. 165 (2021), 290-302, available
at http://doi.org/10.1016/j.apnum.2021.02.019.

36. X.-L. Luo, J.-H. Lv and G. Sun, Continuation method with the trusty time-stepping scheme for linearly
constrained optimization with noisy data, Optim. Eng. 23 (2022), 329-360, available at http://doi.
org/10.1007/s11081-020-09590-z.

37. X.-L. Luo, H. Xiao and J.-H. Lv, Continuation Newton methods with the residual trust-region time-
stepping scheme for nonlinear equations, Numer. Algorithms 89 (2022), 223-247, available at http:
//doi.org/10.1007/s11075-021-01112-x.

38. X.-L. Luo and H. Xiao, Generalized continuation Newton methods and the trust-region updating
strategy for the underdetermined system, J. Sci. Comput. 88 (2021), article 56, published online at
http://doi.org/10.1007/s10915-021-01566-0, pp. 1-22, July 13, 2021.

39. X.-L. Luo and Y.-Y. Yao, Primal-dual path-following methods and the trust-region updating strategy
for linear programming with noisy data, J. Comp. Math., published online at http://doi.org/10.
4208/jcm.2101-m2020-0173, or available at http://arxiv.org/abs/2006.07568, pp. 1-21,
March 15, 2021.

30

Luo et al.

40. X.-L. Luo and H. Xiao, Continuation Newton methods with deﬂation techniques and quasi-
genetic evolution for global optimization problems, arXiv preprint available at http://arxiv.
org/abs/2107.13864, or Research Square preprint available at https://doi.org/10.21203/
rs.3.rs-1102775/v1, July 30, 2021. Software available at https://teacher.bupt.edu.cn/
luoxinlong/zh_CN/zzcg/41406/list/index.htm.

41. X.-L. Luo and H. Xiao, The regularization continuation method with an adaptive time step control for
linearly constrained optimization problems, submitted to Applied Numerical Mathematics, the second
round of review, arXiv preprint available at http://arxiv.org/abs/2106.01122, July 7, 2021.

42. MATLAB v9.8.0 (R2020a), The MathWorks Inc., http://www.mathworks.com, 2020.
43. L. Mathiesen, Computation of economic equilibria by a sequence of linear complementarity problems,

Math. Program. 23 (1985), 144-162.

44. T. F. Rutherford, The manual of MILES, GAMS Corporation, https://www.gams.com/latest/

docs/S_MILES.html, 2022.

45. The NETLIB collection of linear programming problems, available at http://www.netlib.org.
46. J. Nocedal and S. J. Wright, Numerical Optimization, Springer, Berlin, 1999.
47. J. M. Ortega and W. C. Rheinboldt, Iteration Solution of Nonlinear Equations in Several Variables,

SIAM, Philadelphia, 2000.

48. M. C. Ferris et al., PATH solver 5.0.00, software available at https://pages.cs.wisc.edu/

~ferris/path.html, 2019.

49. M. J. D. Powell, Convergence properties of a class of minimization algorithms, in Nonlinear Pro-
gramming 2, O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., Academic Press, New York,
1975, pp. 1-27.

50. T. F. Rutherford, Extension of GAMS for complementarity problems arising in applied economic anal-

ysis, J. Econ. Dyn. Control 19 (1995), 1299-1324.

51. L. F. Shampine, I. Gladwell, and S. Thompson, Solving ODEs with MATLAB, Cambridge University

Press, Cambridge, 2003.

52. K. Tanabe, Continuous Newton-Raphson method for solving an underdetermined system of nonlinear

equations, Nonlinear Anal. 3 (1979), 495-503.

53. K. Tanabe, Centered Newton method for mathematical programming, in System Modeling and Opti-
mization: Proceedings of the 13th IFIP conference, vol. 113 of Lecture Notes in Control and Infor-
mation Systems, Springer, Berlin, 1988, pp. 197-206.

54. V. Venkateswaran, An algorithm for the linear complementarity problem with a P0-matrix, SIAM J.

Matrix Anal. Appl. 14 (1993), 967-977.

55. S. J. Wright, An infeasible-interior-point algorithm for linear complementarity problems, Math. Pro-

gram. 67 (1994), 29-51.

56. S. J. Wright, Primal-dual Interior Point Methods, SIAM, Philadelphia, 1997.
57. Q. Xu, On the New Linear Programming Algorithms-New Sequential Penalty Function Method and
Two Point Barrier Function Method (in Chinese), Ph.D. thesis, Institute of Nuclear Technology, Ts-
inghua University, Beijing, China, 1991.

58. Y. X. Yuan, Recent advances in trust region algorithms, Math. Program. 151 (2015), 249-281.
59. Y. Zhang, On the convergence of a class of infeasible interior-point methods for the horizontal linear

complementarity problem, SIAM J. Optim. 4 (1994), 208-227.

