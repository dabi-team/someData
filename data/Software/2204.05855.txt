pysamoo: Surrogate-Assisted Multi-Objective
Optimization in Python

Julian Blank and Kalyanmoy Deb

1

2
2
0
2

r
p
A
2
1

]
E
N
.
s
c
[

1
v
5
5
8
5
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—Signiﬁcant effort has been made to solve computa-
tionally expensive optimization problems in the past two decades,
and various optimization methods incorporating surrogates into
optimization have been proposed. However, most optimization
toolboxes do not consist of ready-to-run algorithms for com-
putationally expensive problems, especially in combination with
other key requirements, such as handling multiple conﬂicting
objectives or constraints. Thus, the lack of appropriate software
packages has become a bottleneck for solving real-world ap-
plications. The proposed framework, pysamoo, addresses these
shortcomings of existing optimization frameworks and provides
multiple optimization methods for handling problems involving
time-consuming evaluation functions. The framework extends the
functionalities of pymoo, a popular and comprehensive toolbox
for multi-objective optimization, and incorporates surrogates
to support expensive function evaluations. The framework is
available under the GNU Affero General Public License (AGPL)
and is primarily designed for research purposes. For more
information about pysamoo, readers are encouraged to visit:
anyoptimization.com/projects/pysamoo.

Index Terms—Surrogate-Assisted Optimization, Model-based
Optimization, Simulation Optimization, Evolutionary Comput-
ing, Genetic Algorithms.

I. EXPENSIVE OPTIMIZATION

M ANY optimization problems are computationally ex-

pensive and require the execution of one or multiple
time-consuming functions to evaluate a solution. Expensive
Optimization Problems (EOPs) are especially important in
practice and are omnipresent in all kinds of research and
application areas, for instance Agriculture [1], Engineering [2],
Health Care [3], or Computer Science [4]. Often the expensive-
ness of the evaluation is caused by the requirement of running
a simulation, such as Computational Fluid Dynamic (CFD) [5],
Finite Element Analysis (FEA) [6], or processing a large
amount of data [7], [8]. It is worth noting that the majority
of simulation-based data-intensive problems are black-box in
nature [9] and gradient information is not available or is even
more time-consuming to derive. This makes it even more
vital to address the time-consuming objective and/or constraint
functions as an inherent part of the optimization problem and
signiﬁcantly limits the overall evaluation budget.

The execution of a time-consuming evaluation dominates
the algorithm’s computational overhead for ﬁnding new solu-
tions in each iteration. Thus, an algorithm has more time for
carefully selecting new designs than traditional optimization
algorithms; however, the evaluation budget is usually only
limited to a few hundred evaluations instead of a few thousand

Julian Blank and Kalyanmoy Deb are at Michigan State University, East

Lansing, USA (email: {blankjul,kdeb}@msu.edu).

TABLE I
TERMINOLOGY OF DIFFERENT ASPECTS USING SURROGATES.

Aspect

Model / Simulation

Surrogate

Time

Accuracy

Evaluation

computationally expensive
time-consuming
high-ﬁdelity
Expensive Solution
Evaluation (ESE)

computationally
inexpensive
low-ﬁdelity
Approximate Solution
Evaluation (ASE)

evaluations. A standard method to speed up the convergence
of existing methods is using a surrogate model (also called
metamodel, approximation model, simulation model, data-
driven model, response surface), which approximates the time-
consuming function. Incorporating a surrogate into the opti-
mization process is indicated by adding ”surrogate-assisted”
or ”metamodel-based” to the algorithm’s name or description.
The incorporation of surrogates into optimization is illustrated
in Figure 1. Commonly, surrogates – approximation or interpo-
lation models – are utilized during optimization to improve the
convergence behavior. First, one shall distinguish between two
different types of evaluations: Expensive solution evaluations
(ESEs), which require running the computationally expen-
sive evaluation, and approximate solution evaluations (ASEs),
which is a computationally inexpensive approximation by the
surrogate. Where the overall optimization run is limited by
ESEmax function evaluation, function calls of ASEs are only
considered as algorithmic overhead. The goal of surrogate-
assisted optimization is to provide efﬁcient ASEs with the least
approximation error possible and to exploit them to minimize
ESEs function calls. Thus, the overall goal is to improve the
convergence of an optimization algorithm as much as possible
in the usually very limited evaluation budget ESEmax.

II. THE FRAMEWORK

Most researchers rely on open-source packages to conduct
their research. Finding an appropriate open-source package can
be quite challenging, especially when attempting to solve real-
world and application problems. For optimization, one has
to note that most optimization toolboxes do not consist of
ready-to-run algorithms for computationally expensive prob-
lems. Moreover, other vital requirements, such as handling
multiple conﬂicting objectives or constraints, are often not
supported. The proposed framework, pysamoo, addresses these
shortcomings of existing optimization frameworks and pro-
vides different types of optimization methods targeting time-
consuming functions. The framework extends the functional-
ities of pymoo [10], a popular and comprehensive toolbox

 
 
 
 
 
 
2

Fig. 1. The relation between expensive simulations and optimization.

for multi-objective optimization, and incorporates surrogate
support.

The framework includes implementations of PSAF [11] and
GPSAF [12], which are two surrogate incorporation strategies
(proposed by the authors of this paper) to generalize model
assistance for all different types of metaheuristics. In the cur-
rent version of the framework surrogates are incorporated into
GA [13], DE [14], CMAES [15], ISRES [16] NSGA-II [17],
and NSGA-III [18], [19], [20] and others. The framework
is available under the GNU Affero General Public License
(AGPL) and is primarily designed for research purposes. For
further information on the constituent PSAF and GPSAF al-
gorithms which are in the core of pysamoo, refer to references
[11] and [12], respectively.

REFERENCES

[1] P. C. Roy, A. Guber, M. Abouali, A. P. Nejadhashemi, K. Deb, and
A. J. M. Smucker, “Simulation Optimization of Water Usage and
Crop Yield Using Precision Irrigation,” in Evolutionary Multi-Criterion
Optimization, K. Deb, E. Goodman, C. A. Coello Coello, K. Klamroth,
Cham: Springer
K. Miettinen, S. Mostaghim, and P. Reed, Eds.
International Publishing, 2019, pp. 695–706.

[2] H. Yin, H. Fang, G. Wen, Q. Wang, and Y. Xiao, “An adaptive RBF-
based multi-objective optimization method for crashworthiness design
of functionally graded multi-cell tube,” Structural and Multidisciplinary
Optimization, vol. 53, no. 1, pp. 129–144, 2016.

[3] S. Lucidi, M. Maurici, L. Paulon, F. Rinaldi, and M. Roma, “A
simulation-based multiobjective optimization approach for health care
service management,” IEEE Transactions on Automation Science and
Engineering, vol. 13, no. 4, pp. 1480–1491, 2016.

[4] Z. Lu, I. Whalen, V. Boddeti, Y. Dhebar, K. Deb, E. Goodman,
and W. Banzhaf, “NSGA-Net: Neural architecture search using
multi-objective genetic algorithm,” in Proceedings of the genetic and
evolutionary computation conference, ser. GECCO ’19. New York,
NY, USA: Association for Computing Machinery, 2019, pp. 419–427.
[Online]. Available: https://doi.org/10.1145/3321707.3321729

[5] J. D. Anderson and J. Wendt, Computational ﬂuid dynamics. Springer,

1995, vol. 206.

[6] B. Szab´o and I. Babuˇska, Finite element analysis.

John Wiley & Sons,

1991.

[7] W. Luo, R. Yi, B. Yang, and P. Xu, “Surrogate-assisted evolutionary
framework for data-driven dynamic optimization,” IEEE Transactions
on Emerging Topics in Computational Intelligence, vol. 3, no. 2, pp.
137–150, 2019.

[8] H. Wang and Y. Jin, “A random forest-assisted evolutionary algorithm
for data-driven constrained multiobjective combinatorial optimization of
trauma systems,” IEEE Transactions on Cybernetics, vol. 50, no. 2, pp.
536–549, 2020.

[9] S. Olafsson and J. Kim, “Simulation optimization,” in Proceedings of

the winter simulation conference, vol. 1, 2002, pp. 79–84.

[10] J. Blank and K. Deb, “pymoo: Multi-objective Optimization in Python,”

IEEE Access, vol. 8, pp. 89 497–89 509, 2020.

[11] ——, “PSAF: A Probabilistic Surrogate-Assisted Framework for
Single-Objective Optimization,” in GECCO ’21: Proceedings of the
genetic and evolutionary computation conference companion. New
York, NY, USA: ACM, 2021, place: New York, NY, USA. [Online].
Available: https://doi.org/10.1145/3449639.3459297

[12] ——, “GPSAF: A Generalized Probabilistic Surrogate-Assisted Frame-
work for Constrained Single- and Multi-objective Optimization,” IEEE
Transactions on Evolutionary Computation.

[13] D. E. Goldberg, Genetic algorithms in search, optimization and machine
learning, 1st ed. USA: Addison-Wesley Longman Publishing Co., Inc.,
1989.

[14] R. Storn and K. Price, “Differential Evolution –A Simple and Efﬁcient
Heuristic for global Optimization over Continuous Spaces,” Journal of
Global Optimization, no. 4, pp. 341–359, Dec. 1997.

[15] N. Hansen and A. Ostermeier, “Completely derandomized self-
adaptation in evolution strategies,” Evolutionary Computation, vol. 9,
no. 2, pp. 159–195, Jun. 2001. [Online]. Available: http://dx.doi.org/10.
1162/106365601750190398

[16] T. Runarsson and X. Yao, “Search biases in constrained evolutionary
optimization,” IEEE Transactions on Systems, Man, and Cybernetics,
Part C (Applications and Reviews), vol. 35, no. 2, pp. 233–243, 2005.
[17] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast
and elitist multiobjective genetic algorithm: NSGA-II,” Trans. Evol.
Comp, vol. 6, no. 2, pp. 182–197, Apr. 2002. [Online]. Available:
https://doi.org/10.1109/4235.996017

[18] K. Deb and H. Jain, “An evolutionary many-objective optimization
algorithm using reference-point-based nondominated sorting approach,
Part I: Solving problems with box constraints,” IEEE Transactions on
Evolutionary Computation, vol. 18, no. 4, pp. 577–601, 2014.

[19] H. Jain and K. Deb, “An evolutionary many-objective optimization
algorithm using reference-point based nondominated sorting approach,
part II: Handling constraints and extending to an adaptive approach,”
IEEE Transactions on Evolutionary Computation, vol. 18, no. 4, pp.
602–622, Aug. 2014.

[20] J. Blank, K. Deb, and P. Roy, “Investigating the normalization pro-
cedure of NSGA-III,” in Evolutionary multi-criterion optimization,
K. Deb, E. Goodman, C. A. Coello Coello, K. Klamroth, K. Miettinen,
S. Mostaghim, and P. Reed, Eds.
Springer International Publishing,
2019, pp. 229–240, place: Cham.

SurrogateModel / SimulationOptimizationfitupdateApproximate Solution Evaluation(low-fidelity)Exact Solution Evaluation(high-fidelity)Computationally ExpensiveAlgorithmicComputation