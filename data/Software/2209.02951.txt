Democratizing Domain-Specific Computing

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong
{chiyuze,wkqiao2015,atefehsz,jiewang,cong}@cs.ucla.edu
University of California Los Angeles
Los Angeles, CA, USA

2
2
0
2

p
e
S
7

]

R
A
.
s
c
[

1
v
1
5
9
2
0
.
9
0
2
2
:
v
i
X
r
a

Abstract

In the past few years, domain-specific accelerators (DSAs), such as
Google‚Äôs Tensor Processing Units, have shown to offer significant
performance and energy efficiency over general-purpose CPUs. An
important question is whether typical software developers can de-
sign and implement their own customized DSAs, with affordability
and efficiency, to accelerate their applications. This article presents
our answer to this question.

Keywords

customized computing, design automation, domain-specific archi-
tecture, design space exploration

1 Introduction

General-purpose computers are widely used in our modern society.
There were close to 24 million software programmers worldwide as
of 2019 according to Statista. However, the performance improve-
ment of general-purpose processors has slowed down significantly
due to multiple reasons. One is the end of Dennard scaling [1],
which scales transistor dimensions and supply powers by 30% ev-
ery generation (roughly every two years), resulting in a 2√ó increase
in the transistor density and a 30% reduction of the transistor delay
(or improvement in the processor frequency) [2]. Although the tran-
sistor density continues to double per generation according to the
Moore‚Äôs law, the increase of the processor frequency was slowed or
almost stopped with the Dennard scaling ending in the early 2000s
(due to the leakage current concern). The industry entered the era
of parallelization, with tens to thousands of computing cores inte-
grated in a single processor, and tens of thousands of computing
servers connected in a warehouse-scale data center. However, by
the end of 2000s, such massively parallel general-purpose comput-
ing systems were again faced with serious challenges in terms of
power supply, heat dissipation, space, and cost [3‚Äì5].

To further advance the computing performance, customized com-
puting was introduced where one can adapt the processor archi-
tecture to match the computing workload for much higher com-
puting efficiency using special-purpose accelerators [4, 6‚Äì8]. The
best known customized computing example is probably the Ten-
sor Processing Unit (TPU) [9] announced by Google in 2017 for
accelerating machine learning workloads. Designed in 28nm CMOS
technology as an application-specific integrated circuit (ASIC), TPU
demonstrated 196√ó performance/watts power efficiency advantage
over the general-purpose Haswell CPU, a leading server-class CPU
at that time of publication. One significant source of energy ineffi-
ciency of a general-purpose CPU comes from its long instruction
pipeline, time-multiplexed by tens or even hundreds of different
types of instructions, resulting in high energy overhead (64% for

a typical superscalar out-of-order pipeline studied in [10]). In con-
trast, the domain-specific accelerators (DSAs) achieve their efficiency
in the following five dimensions [8]: (i) use of special data types
and operations, (ii) massive parallelism, (iii) customized memory ac-
cesses, (iv) amortization of the instruction/control overhead, and (v)
algorithm and architecture co-design. When these factors are com-
bined, a DSA can offer significant (sometimes more than 100, 000√ó)
speedup over a general-purpose CPU [8].

Given that DSAs are domain-specific, a key question is if a typi-
cal application developer in a given application domain can easily
implement their own DSAs. For ASIC-based DSAs, such as TPUs,
there are two significant barriers. One is the design cost. According
to McKinsey, the cost of designing an ASIC with a leading edge
technology (7nm CMOS) is close to $300M [11], which is prohibi-
tively high for most companies and developers. The second barrier
is the turnaround time. It usually takes more than 18 months from
the initial design to the first silicon, and even longer to production.
During this time, new computation models and algorithms may
emerge, especially in some fast-moving application fields, making
the initial design out-dated.

In light of these concerns, we think that the field-programmable
gate-arrays (FPGAs) offer an attractive alternative for DSA imple-
mentation. Given its programmable logics, programmable inter-
connects, and customizable building blocks (BRAMs and DSPs),
an FPGA can be customized to implement a DSA without going
through a lengthy fabrication process and can be reconfigured to a
new design in a matter of seconds. Moreover, FPGAs have become
available in the public clouds, such as Amazon AWS F1 [12] and
Nimbix [13]. One can create their own DSAs on the FPGA and use
it at a rate of $1-2/hour to accelerate the desired applications, even
if FPGAs are not available in the local computing facility. Because
of its affordability and fast turnaround time, we think that FPGAs
offer the promise of democratization of customized computing, al-
lowing millions of software developers to create their own DSAs
on FPGAs for performance and energy efficiency. Although a DSA
implemented on an FPGA is less efficient than the one on an ASIC
due to the lower circuit density and clock frequency, it can still
deliver tens or hundreds of times better efficiency compared to
CPUs (as shown in Section 2).

However, to achieve the true democratization of customized
computing, a convenient and efficient compilation flow needs to be
provided for a typical performance-oriented software programmer
to create a DSA on an FPGA, either on premise or in the cloud. Un-
fortunately, this has not been the case. FPGAs used to be designed
with hardware description languages, such as Verilog and VHDL,
known only to the circuit designers. In the past decade, FPGA
vendors introduced the high-level synthesis (HLS) tools to compile
C/C++/OpenCL programs to FPGAs. Although these HLS tools raise
the level of design abstraction, they still require a significant amount

 
 
 
 
 
 
CACM‚Äô22, 2022, USA

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong

of hardware design knowledge, expressed in terms of pragmas, to
define how computation is parallalized and/or pipelined, how data
are buffered, how memory is partitioned, etc. As shown in Sec-
tion 3, the performance of a DSA design can vary from being 108√ó
slower (without performance-optimizing pragmas) than a CPU to
89√ó faster with proper optimization. But such architecture-specific
optimization is often beyond the reach of an average software pro-
grammer.

In this paper, we highlight our research on democratizing cus-
tomized computing by providing highly effective compilation tools
for creating customized DSAs on FPGAs. It builds on top of the
HLS technology, but greatly reduces (or completely eliminates in
some parts/sections) the need for pragmas for hardware-specific
optimization. This is achieved by high-level architecture-guided
optimization and automated design space exploration. Section 2
uses two examples to showcase the efficiency of DSAs on FPGAs
over the general-purpose CPUs. Section 3 discusses challenges of
programming an FPGA and Section 4 reviews our proposed solu-
tions using architecture-guided optimization, such as systolic array
(Section 4.1.1) or stencil computation (Section 4.1.2), automated
design space exploration for general applications (Section 4.2), and
raising the abstraction level to domain-specific languages (DSLs)
(Section 4.3). Section 5 concludes the paper and discusses future
research directions. The focus of this paper is on creating new DSAs
(on FPGAs) instead of programming existing DSAs, such as GPUs
and TPUs, which are also highly efficient for their target workloads.
Some of the techniques covered in this paper can be extended for
the latter, such as supporting systolic arrays or stencil computa-
tion on GPUs [14, 15]. This paper is based on the keynote speech
given by one of the co-authors at the 2021 International Parallel
and Distributed Processing Symposium (IPDPS) [16].

2 Promise of Customizable Domain-Specific

Acceleration

In this section, we highlight two DSAs on FPGAs targeting sort-
ing and deep learning applications to demonstrate the power of
customizable domain-specific acceleration.

2.1 High-Performance Sorting

Our first example to showcase an FPGA-based DSA is accelerated
sorting, which is a fundamental task in many big data applications.
One of the most popular sorting algorithms for large-scale sort-
ing is recursive merge sort, given its optimal computation and I/O
communication complexity. However, the slow sequential merg-
ing steps usually limit the performance of this recursive approach.
Although parallel merging is possible, this comparison-based ap-
proach often comes with high overhead on CPUs and GPUs and
limits the throughputs, especially in the last merging stages.

FPGAs have abundant on-chip computing resources (e.g., LUTs
and DSPs) and memory resources (e.g., registers and BRAM slices)
available. One can achieve impressive performance speedup by
implementing the recursive merging flow into a tree-based cus-
tomizable spatial architecture [17, 18] as in Figure 1. A merge tree
is uniquely defined by the number of leaves and the throughput at
the root. The basic building block in the hardware merge tree is a

ùëò-Merger (denoted as ùëò-M in Figure 1), which is a customized logic
that can merge two sorted input streams at a rate of ùëò elements per
cycle in a pipelined fashion. Using a combination of such mergers
with different throughputs, one can build a customized merge tree
with an optimized number of leaves and root throughput for a given
sorting problem and memory configuration.

Figure 1: A merge tree that merges 16 input streams simul-
taneously and outputs 4 elements per cycle.

Figure 2 shows the speedup that the customized merge tree accel-
erator on the AWS F1 FPGA instance achieves over a 32-thread Intel
Xeon CPU implementation (the absolute performance of baseline is
0.21 GB/s). Part of this large efficiency gain arises from the common
advantages of DSAs as discussed in Section 1: e.g., (i) specialized
data type and operations: the hardware mergers support any key
and value width up to 512 bits; (ii) massive data parallelism: the cus-
tomized merge tree is able to merge 64 input streams concurrently
and output 32 integer elements every cycle; (iii) optimized on-chip
memory: we optimize each input buffer preparing the input stream
to its corresponding tree leaf to have enough space to hide the
DRAM access latency. However, there are additional FPGA-related
features that enable us to achieve the high efficiency.
‚Ä¢ Design choices tailored to hardware constraints: We can
tailor the customizable merge-tree-based architecture according
to the given workload and operating environment. For example,
since the available DRAM bandwidth on AWS F1 is 32 GB/s for
concurrent read and write, we tune the tree root throughput to
be the same amount and select the maximum number of tree
leaves that fit into the on-chip resources. This minimizes the
number of passes needed for merging.

‚Ä¢ Flexible reconfigurability: FPGAs have a unique feature to
support hardware reconfigurability at the runtime. When sorting
multiple large datasets from different domains, one may pre-
generate multiple merge-tree configurations, each optimized
for its own use case (e.g., data size, key/data width). Then, we
reprogram the FPGA at the run time to adapt our accelerator
to the changing sorting demands. Also, when sorting terabyte-
scale data stored on SSDs, the merge sort needs to go through
two phases: 1) to merge the data up to the DRAM size, 2) to
finally merge the data onto the SSD. We show that designers
can reconfigure the FPGA to switch between these two phases
efficiently in [18].

2.2 DNN Accelerators

Our second example is FPGA-based acceleration of deep neural
networks (DNNs). DNNs have been widely used for many artificial
intelligence (AI) applications ranging from computer vision and

1-M1-M1-M1-M1-M1-M2-MInputBuffersCouplerOutput Buffer1-M1-M1-M1-M1-M1-M2-M4-MCouplerDemocratizing Domain-Specific Computing

CACM‚Äô22, 2022, USA

Code 1: HLS C code snippet of CNN. The scop pragma will
be used for systolic compilation in Section 4.1.1

1 void CNN(float In[B][I][H+P-1][W+Q-1],
float W[O][I][P][Q],
2
float Out[B][O][H][W]) {
3
4

// Use the pragma below to annotate the start of the code region to be mapped

5
6
7
8
9
10
11
12
13
14
15
16

17

to a systolic array in AutoSA

#pragma scop
for (int b = 0; b < B; b++)

for (int o = 0; o < O; o++)

for (int h = 0; h < H; h++)

for (int w = 0; w < O; w++) {

Out[b][o][h][w] = 0;
for (int i = 0; i < I; i++)

for (int p = 0; p < P; p++)

for (int q = 0; q < O; q++)

Out[b][o][h][w] += W[o][i][p][q] * In[b][i][h+p][w+q];

}

// Use the pragma below to annotate the end of the code region to be mapped

to a systolic array in AutoSA

#pragma endscop

architecture for various DNN models without expensive hard-
ware updates. This gives FPGAs a distinct advantage over ASICs
in terms of the costs and development cycles, as today‚Äôs deep
learning models are still evolving.

‚Ä¢ Low system overhead latency: Part of the FPGA fabric sup-
ports efficient packet processing schemes, allowing data to be
offloaded onto the accelerator with extremely low latency: e.g.,
the FPGAs are programmed to process network packets from
remote servers with little overhead and feed the data into the
Brainwave NPU on the same die at the line rate. Such low-latency
data offloading ensures that the users can get the acceleration
result in real time, whether the data are from the edge or the
cloud. It also allows a large DNN model to be decomposed and
implemented on multiple FPGAs with very low latency.

Table 1: Hardware specifications and inference performance
comparison (GRUs and LSTMs) on FPGAs and GPUs [21].
Intel S10 NX

Nvidia V100

Hardware

Nvidia T4

Peak FP16 TOPS
Peak INT8 TOPS
On-Chip Mem. (MB)
Process

Speedup at batch-3
Speedup at batch-6
Speedup at batch-32
Speedup at batch-256

65
130
10

143
143
16

125
63
16
TSMC 12nm TSMC 12nm Intel 14nm
2.4√ó
2.1√ó
2.5√ó
2.3√ó

22.3√ó
24.2√ó
5.0√ó
1.6√ó

1√ó
1√ó
1√ó
1√ó

The recent effort by Amazon on advanced query acceleration of
its Redshift database [24] is another good example of FPGA acceler-
ation in datacenters. However, wider adoption of FPGA acceleration
has been constrained by the difficulty of FPGA programming, which
is the focus of this paper.

3 FPGA Programming Challenges

So far it has not been easy for a typical performance-oriented CPU
programmer to create their own DSAs on an FPGA to achieve the
performance gain demonstrated in the preceding section, despite
the recent progress on high-level synthesis (HLS).

HLS allows a designer to start with C/C++ behavior description
instead of the low-level cycle-accurate register-transfer level (RTL)
description to carry out FPGA designs, which significantly shortens

Figure 2: Comparison of the sorting performance using
CPUs and FPGA accelerators.

speech recognition to robotics, due to their greatly improved ac-
curacy and efficiency. One of the earliest, also probably the most
cited FPGA-based deep learning accelerator is the one published
in early 2015 [19]. It was developed using HLS to accelerate multi-
layer convolution neural networks (CNN). The whole system was
implemented on a single Xilinx Virtex-7 485T FPGA chip with a
DDR3 DRAM. It demonstrated close to 5√ó speedup and 25√ó energy
reduction compared to a 16-thread CPU implementation. Utilizing
HLS, it was able to explore over 1,000 accelerator design choices
based on the roofline model and converged to a solution that is opti-
mized both for computation and communication. Several graduate
students carried out this implementation in less than six months
and completed it almost two years ahead of Google‚Äôs announcement
of TPU [9], which was done by a much larger design team.

Microsoft also designed an FPGA-based DNN accelerator, named
Brainwave Neural Processing Unit (NPU) [20], at a much larger
scale and has widely deployed Brainwave NPUs in its cloud produc-
tion. Table 1 summarizes the hardware specifications and bench-
mark results of Brainwave NPU re-implemented on Intel S10 NX
and NVIDIA GPUs [21]. The results show that FPGAs can not
only achieve an order of magnitude better performance than GPUs
for low-batch inferences, but also compete in high-batch infer-
ence cases. On the other hand, although there is a performance
gap between FPGA-based NPUs and hardened NPUs [22], the re-
configurable nature of FPGAs allows designers to quickly adapt
their designs to the emerging deep learning algorithms. The advan-
tages of FPGA-based DNN accelerators are listed below:
‚Ä¢ Customized data type: Deep neural networks are highly com-
pressible in data types since using a low-precision customized
floating point format has negligible impact on accuracy. For
example, the Brainwave NPU employs a narrow floating point
which contains 1-bit sign, 5-bit exponent and 2-bit mantissas.
Although some customized bit-width (e.g., 16-bit and 8-bit) sup-
port is added in the latest GPUs, the FPGAs demonstrate the
flexibility to go down to ultra-low bit width (1-2 bits) with dy-
namic customization [23], as such narrow-precision data types
can be mapped efficiently onto LUTs on FPGAs.

‚Ä¢ Synthesizable parameters: The Brainwave NPU allows for
several parameters to be changed such as data type, vector size,
number of data lanes, and size of the matrix-vector tile engine
during the synthesis. As a result, designers can specialize its

1GB2GB4GB8GB16GB32GBData Size05101520253035Normalized Sorting Performance1.31.21.31.21.213528.429.12928.929.3CPUFPGACACM‚Äô22, 2022, USA

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong

the turnaround times and reduces the FPGA development cycle [25‚Äì
27]. As a result, most FPGA vendors have commercial HLS tools, e.g.,
Xilinx Vitis [28] and Intel FPGA SDK for OpenCL [29]. However,
even though HLS is increasingly employed by hardware designers,
software programmers still find it challenging to use the existing
HLS tools. For example, Code 1 shows the C code for one layer of
CNN. When synthesized with the Xilinx HLS tool, the resulting
microarchitecture is, in fact, 108√ó slower than a single-core CPU.
As explained in [30], this is because the derived microarchitecture
has the following inefficiencies which limit its performance:
‚Ä¢ Inefficient off-chip communication: Although the band-
width of the off-chip memory can support fetching 512-bit data
at a time, the HLS solution uses only 32 bits of the available bus.
This is because the input arguments of the function in Code 1
(lines 1-3), which create interfaces to the off-chip memory, use
32-bit floating-point data type.

‚Ä¢ No data caching: Lines 10 and 14 of Code 1 access the off-chip
memory directly. Although this type of memory access will be
cached automatically on a CPU, they will not be cached on an
FPGA by default. Instead, the designer must explicitly specify
which data need to be reused using the on-chip memories (BRAM,
URAM, or LUT).

‚Ä¢ No overlap between communication and computation:
A load-compute-store pipelined architecture is necessary to
achieve good computation efficiency as done in most CPUs. How-
ever, this is not created automatically by the HLS tool based on
the input C code.

‚Ä¢ Sequential loop scheduling: The HLS tools require the de-
signer to use synthesis directives in the form of pragmas to
specify where to apply parallelization and/or pipelining. In their
absence, everything will be scheduled sequentially.

‚Ä¢ Limited number of ports for on-chip buffers: The default
on-chip memory (i.e., BRAM) has one or two ports. Without
proper array partitioning, it can greatly limit the performance
since it restricts the amount of parallel accesses to the on-chip
buffers, such as the Out buffer in Code 1.
Fortunately, these shortcomings are not fundamental limitations.
They only exist because the HLS tools are designed to generate the
architecture based on specific C/C++ code patterns. As such, we
can resolve all of them and get to a 9,676√ó speedup. To achieve this,
we first saturate the off-chip memory‚Äôs bandwidth by packing 16
elements and creating 512-bit data for each of the interface argu-
ments. We then explicitly define our caching mechanism and create
load-compute-store stages to decouple the computation engine and
the data transfer steps, so that the compute engine works only
with on-chip memory. Finally, we exploit UNROLL and PIPELINE
pragmas to define the parallelization opportunities. We also use
ARRAY_PARTITION pragmas as needed, which brings the total num-
ber of pragmas to 28 and the lines of codes to 150, to enable parallel
accesses to the on-chip memory by creating more memory banks
(ports). Table 2 compares the two microarchitectures in terms of
their number of resources, global memory bandwidth, and perfor-
mance when we map them to Xilinx Virtex Ultrascale+ VCU1525.

Architecture

BRAM DSP

Used DRAM BW (Bits)

Baseline HLS
Manually Optimized HLS

0.1%
47.2%
Table 2: Microarchitecture comparison for the naive CNN
code (Code 1) and its optimized version.

2.1%
78.0%

32
512

Speedup
1√ó
9, 676√ó

4 DSA Design Automation Beyond HLS

To overcome the FPGA programming challenges discussed in
the preceding section, in this section, we highlight our software-
programmer-friendly compilation solutions for FPGAs. Figure 3
details our overall compilation flow from C, C++, or DSLs to
FPGA acceleration. Our solutions include: using architecture-
guided optimization (Section 4.1), such as systolic array or sliding
window-based architecture for stencil applications, automated de-
sign space exploration (Section 4.2), and domain-specific language
(Section 4.3).

Figure 3: Overview of our approaches.

4.1 Architecture-Guided Optimization

One of the challenges of existing HLS tools is that many prag-
mas are needed to specify a complex microarchitecture, such as
a systolic array, for efficient implementation. Instead, we allow
the programmer to simply mark the section of code suitable for a
certain microarchitecture pattern and let the tool to automatically
generate complex and highly optimized HLS code for the intended
microarchitecture. This is called architecture-guided optimization. In
this section, we showcase two examples ‚Äì compilations for systolic
arrays and stencil computations.

4.1.1 Automated systolic arrays compilation: Systolic array archi-
tecture consists of a grid of simple and regular processing elements
(PE) which are linked through local interconnects. With the mod-
ular design and local interconnects, we can easily scale out this
architecture to deliver a high performance while achieving a high
energy efficiency at the same time. One of the representative ex-
amples of this architecture is the Google TPU [9]. It implements

C/C++DSL (e.g., Halide, TensorFlow)Frontend CompilerSystolic array(AutoSA) Stencil Comp( SODA) Architecture-guided OptimizationFPGA AcceleratorHLS + RTL Synthesis & ImplementationAutoDSEGeneral Microarchitecture OptimizationHeteroCLIntermediate Representation‚Ä¶Section 4.1Section 4.2Section 4.3Democratizing Domain-Specific Computing

CACM‚Äô22, 2022, USA

systolic arrays as the major compute unit to accelerate the matrix
operations in the machine learning applications.

On the downside, designing a high-performance systolic array
can be a challenging task. It requires the expert knowledge of both
the target application and the hardware. Specifically, designers need
to identify the systolic array execution pattern from the applica-
tion, transform the algorithm to describe a systolic array, write
the hardware code for the target platform, and tune the design to
achieve the optimal performance. Each step will take significant
efforts, raising the bar to reap the benefits of such an architecture.
For example, a technical report from Intel [31] mentioned that such
a development process will take months of efforts even for industry
experts.

feature maps ùêºùëõ are reused in-between PEs vertically, weights ùëä
are reused across PEs horizontally, and output feature maps ùëÇùë¢ùë°
are computed inside each PE and drained out through the on-chip
I/O network. For the same CNN example as shown in Section 3,
AutoSA-generated systolic array achieved a 2.3√ó speedup over the
HLS-optimized baseline. This is accomplished by a higher resource
utilization and computation efficiency. The regular architecture and
local interconnects make systolic array scalable to fully utilize the
on-chip resource. Furthermore, this architecture exploits a high
level of data reuse from the application that balances the computa-
tion and communication, resulting in a high computation efficiency.
Table 3 compares the details of the AutoSA-generated design with
the HLS-optimized baseline. Moreover, two high-level pragmas in
Code 1 replaces 28 low-level pragmas in the manual HLS design.
Such significant low-level HLS pragma reduction is consistently
observed with the tools introduced in the later sections as well.

Figure 5: Architecture of a 2D systolic array for CNN.

Figure 4: Compilation flow of AutoSA.

To cope with this challenge, we propose an end-to-end compila-
tion framework, AutoSA [32], to generate systolic arrays on FPGA.
Figure 4 depicts the compilation flow of AutoSA. AutoSA takes a C
code as the input that describes the target algorithm to be mapped
to the systolic arrays. This code is then lowered to the polyhedral IR.
AutoSA uses the polyhedral model [33], which is a mathematical
compilation framework for loop nest optimization. AutoSA checks
if the input program can be mapped to a systolic array (Legality
Check). After that, it applies a sequence of hardware optimizations
to construct and optimize the PEs (in the stage of Computation
Management) and the on-chip I/O networks for transferring the
data between PEs and the external memory (Communication Man-
agement). AutoSA introduces a set of tuning knobs that can either
be changed manually or set by an auto-tuner. The output of this
compiler is a systolic array design described in the target hardware
language. At present, we support four different hardware back-
ends, including Xilinx HLS C, Intel OpenCL, Catapult HLS, and
TAPA [34].

With the help of AutoSA, we could now easily create a high-
performance systolic array for CNN as mentioned in the previous
section. AutoSA requires minimal code changes to compile. De-
signers only need to annotate the code region to be mapped to
systolic arrays with two pragmas (Lines 5 and 17 in Code 1). Fig-
ure 5 shows the architecture of the systolic array with the best
performance generated by AutoSA. For CNN, AutoSA generates
a 2D systolic array by mapping the output channel ùëú and the im-
age height ‚Ñé to the spatial dimensions of the systolic array. Input

Speedup
1.0√ó
2.3√ó

Architecture

BRAM DSP

Used DRAM BW (Bits)

Manually Optimized HLS
AutoSA

47.2%
90.1%
Table 3: Micro-architecture comparison for manually-
optimized HLS design and AutoSA-generated design.

78.0%
93.3%

512
512

Automatic systolic array synthesis has been an important re-
search topic for decades. AutoSA represents the state-the-of-art
effort along this direction. At present, AutoSA targets only FPGAs.
One recent work, Gemmini [35], generates systolic arrays for deep
learning applications on ASICs. Gemmini framework employs a
fixed hardware template for generating systolic arrays for matrix
multiplication. The general mapping methodology based on the
polyhedral framework in AutoSA can be applied to Gemmini to
help further improve the applicability of Gemmini framework.

4.1.2 Automated stencil compiler: Our second example for
architecture-guided optimization is for the stencil computation,
which utilizes a sliding window over the input array(s) to produce
the output array(s). Many areas, such as image processing and sci-
entific computing, widely use such a pattern. While the sliding
window pattern seems regular and simple, it is non-trivial to op-
timize the stencil computation kernels for performance given its
low computation-to-communication ratio along and complex data
dependency patterns. Even worse, a stencil computation kernel
can be composed of many stages or iterations concatenated with
each other, which further complicates data dependency and makes
communication optimizations harder to achieve. To overcome these
challenges, we developed a stencil compiler named SODA [36, 37]
with the following customized optimization for the stencil microar-
chitecture:

Model ExtractionCPolyhedral IRLegality CheckXilinx HLS CComputation ManagementCommunication ManagementCode GenerationAuto TunerUser-Specified OptimizationManualIntel OpenCLCatapult HLS CTAPAPEPEPEPEWInOutDRAMùëú‚ÑéCACM‚Äô22, 2022, USA

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong

‚Ä¢ Parallelization support. The stencil computation has a large
degree of inherent parallelism, including both spatial parallelism,
i.e., parallelism among multiple spatial elements within a stage,
and temporal parallelism, i.e., parallelism among multiple tempo-
ral stages. SODA exploits both fine-grained spatial parallelism
and coarse-grained temporal parallelism with perfect data reuse,
by instantiating multiple processing elements (PE) in each stage
and concatenating multiple stages together on-chip, respectively.
Figure 6 illustrates the microarchitecture overview of a SODA
DSA with three fine-grained PEs per stage and two coarse-
grained stages. Within each stage, multiple (three in Figure 6)
PEs can read necessary inputs from the reuse buffers and produce
outputs in parallel, exploiting spatial parallelism. Meanwhile,
Stage 1 can directly send its outputs to Stage 2 as inputs, further
exploiting temporal parallelism.

‚Ä¢ Data reuse support. The sliding window pattern makes it pos-
sible to reuse input data for multiple outputs and reduce memory
communication. CPUs (and to a less extent GPUs) are naturally
at a disadvantage for such data reuse due to their hardware-
managed, application-agnostic cache systems. FPGAs and ASICs,
however, can customize their data paths to reduce the memory
traffic and achieve the optimal data reuse (in terms of the least
amount of off-chip data movement and the smallest possible
on-chip memory size) without sacrificing the parallelism as seen
in [36]. Figure 6 shows the reuse buffers in a SODA DSA, which
read three new inputs in parallel, keep those inputs that are still
needed by the PEs in the pipeline, and discard the oldest three in-
puts which are no longer required. Thus, the SODA DSA accesses
both the inputs and outputs in a streamed fashion, achieving the
least amount of off-chip data movement. Moreover, SODA can
generate reuse buffers for any number of PEs with the smallest
possible on-chip memory size, independent of the number of
PEs used, ensuring high scalability.

‚Ä¢ Computation reuse support. Computation operations are of-
ten redundant and can be reused for stencil kernels. As an exam-
ple, for a 17√ó17 kernel utilized in calcium image stabilization [38],
we can dramatically reduce the number of multiplication opera-
tions from 197 to only 30, while yielding the same throughput.
However, computation reuse is often under-explored, since most
stencil compilers are designed for instruction-based processors
where the parallelization and communication reuse have more
impact on the performance, while the computation reuse is often
just a by-product [15]. For DSAs, we can fully decouple the com-
putation reuse from parallelization and data reuse via data path
customization. SODA algorithms: 1) optimal reuse by dynamic
programming (ORDP) that can fully explore the trade-off be-
tween computation and storage when the stencil kernel contains
up to 10 inputs, 2) heuristic search‚Äìbased reuse (HSBR) for larger
stencils to find near-optimal design points within limited time
and memory constraints.
We developed the SODA compiler to automatically generate a
DSA to consider optimizations in these three dimensions. SODA
utilizes a simple domain-specific language (DSL) as input. As an
example, Code 2 shows a blur filter written in SODA DSL. We
will discuss later in Section 4.3 that higher-level domain-specific
languages can generate the SODA DSL, further reducing the thresh-
old for software programmers and domain experts to benefit from

Figure 6: SODA microarchitecture overview.

DSAs. SODA can automatically explore a large solution space for
a stencil DSA, including unroll factor, iterate factor, tile sizes, and
computation reuse.

Code 2: Blur filter written in SODA [36].

1 kernel: blur
2 unroll factor: 16
3 iterate factor: 1
4 input float: image(3000, *) # Tile size, which decides reuse buffer size
5 local float: blur_x(0, 0) = (image(0, 0) + image(1, 0) + image(2, 0)) / 3
6 output float: blur_y(0, 0) = (blur_x(0, 0) + blur_x(0, 1) + blur_x(0, 2)) / 3

# Spatial parallelism
# Temporal parallelism

Experimental results show that SODA, as an automated acceler-
ator design framework for stencil applications with scalable par-
allelization, full communication-reuse, and effective computation
reuse, achieves a 10.9√ó average speedup over the CPU and a 1.53√ó
average speedup over the GPU [37]. This is achieved by over 200√ó
lines of HLS code generated from SODA DSL, with 34% of the lines
of code being pragmas. Such extensive amount of code required by
the domain-specific architectural optimizations was only possible
with a fully automated DSA compiler framework. Compared to
SODA, a very recent work named ClockWork [39] achieves less
resource consumption by compiling the entire application into one
flat statically scheduled module, but at the cost of long compilation
time and scalability to the whole chip. This presents an interesting
trade-off. A common limitation of the approach taken by both SODA
and ClockWork is that both cannot create a quickly reconfigurable
overlay accelerator to accommodate different stencil kernels. Both
have to generate different accelerators for different stencil patterns,
which may take many hours. It will be interesting to investigate to
see if it is possible to come up with a stencil-specific programmable
architecture to allow runtime reconfiguration for acceleration of
different stencil applications.

4.2 Automated Program Transformation and

Pragma Insertion

For general applications that do not match the predefined computa-
tion patterns (such as systolic arrays and stencil computations), we
carry out an automated local program transformation and pragma
insertion based on the input C/C++ code. The recently developed
Merlin Compiler 1 [40] addresses this need partially by providing
higher-level pragmas. The programming model of the Merlin Com-
piler is similar to that of OpenMP [41], which is commonly used
for multi-core CPU programming. Like in OpenMP, it optimizes
the design by defining a small set of compiler directives in the form
of pragmas. Codes 3 and 4 show the similarity of the programming
structure between the Merlin Compiler and OpenMP.

1Recently open-sourced by Xilinx at https://github.com/Xilinx/merlin-compiler

DRAMReuse BuffersFIFOCompute UnitsPEPEPEFIFOFIFOFFFIFOFFFIFOFIFOStage 1Reuse BuffersFIFOCompute UnitsPEPEPEFIFOFIFOFFFIFOFFFIFOFIFOStage 1Reuse BuffersFIFOCompute UnitsPEPEPEFIFOFIFOFFFIFOFFFIFOFIFOStage 2Reuse BuffersFIFOCompute UnitsPEPEPEFIFOFIFOFFFIFOFFFIFOFIFOStage 2Democratizing Domain-Specific Computing

CACM‚Äô22, 2022, USA

Code 3: OpenMP

Code 4: Merlin Compiler

1 #pragma omp parallel for num_threads(16)
2 for (int i = 0; i < N; i++) {
3

c[i] += a[i] * b[i]; }

1 #pragma ACCEL parallel factor = 16
2 for (int i = 0; i < N; i++) {
3

c[i] += a[i] * b[i]; }

Table 4 lists the Merlin pragmas for the design architecture trans-
formations. Using these pragmas, the Merlin Compiler will apply
source-to-source code transformations and generate the equivalent
HLS code with proper HLS pragmas inserted. The fg PIPELINE
refers to the case where fine-grained pipelining is applied by pipelin-
ing the loop and unrolling all its inner loops completely. In contrast,
the cg PIPELINE pragma applies coarse-grained pipelining by cre-
ating double buffers automatically between the pipelined tasks.

Table 4: Merlin pragmas with architecture structures.
Available Options Architecture Structure

Keyword

PARALLEL
PIPELINE
TILING

factor=<int>
mode=cg/fg
factor=<int>

CG & FG parallelism
CG or FG pipeline
Loop Tiling

CG: Coarse-grained; FG: Fine-grained

By introducing a reduced set of high-level pragmas and generat-
ing the corresponding HLS code automatically, the Merlin Compiler
can make FPGA programming significantly easier. For example, we
can optimize the Advanced Encryption Standard (AES) kernel from
the MachSuite benchmark [42] by adding only 2 Merlin pragmas
and achieve a 470√ó speedup compared to when the kernel (without
any changes) is synthesized using the Vitis HLS tool. However, the
manually optimized HLS code utilizes 37 pragmas and 106 more
lines of code to get to the same performance.

Although the Merlin Compiler greatly reduces the solution space
when optimizing a kernel, it still requires the programmer to man-
ually insert the pragmas at the right place with the right option,
which can still be challenging. To further reduce the DSA design
effort, we have developed a push-button design space exploration
(DSE), called AutoDSE [43], on top of the Merlin Compiler. AutoDSE
is designed to automatically insert Merlin pragmas to optimize the
design based on either performance, area, or a trade-off of the two.
As depicted in Figure 7, AutoDSE takes the C kernel as the input
and identifies the design space by analyzing the kernel abstract
syntax tree (AST) to extract the required information such as loop
trip-counts and available bit-widths. It then encodes the valid space
in a grid structure that marks the invalid pragma combinations.
As the design parameters have a non-monotonic impact on both
the performance and area, AutoDSE partitions the design space to
reduce the chance of it getting trapped in a locally optimal design
point. Each partition explores the design space from a different
starting design point so that AutoDSE can search different parts of
the solution space. Once the DSE is finished, AutoDSE will output
the design point with the highest quality of results (QoR).

AutoDSE is built on a bottleneck-guided optimization that mim-
ics the manual optimization approach to perform iterative improve-
ments. At each iteration, AutoDSE runs the Merlin Compiler to get
the detailed performance breakdown of the current design solution
(estimated after HLS synthesis). Then, it identifies a set of perfor-
mance bottlenecks, sorted by decreasing latency, and marks each
entry as computation or communication-bounded. Guided by this

Figure 7: The AutoDSE framework as in [43].

list of bottlenecks, at each iteration, AutoDSE applies the appropri-
ate Merlin pragma to the code section with the most-promising im-
pact for performance improvement. The experimental results show
that using the bottleneck-optimizer, AutoDSE is able to achieve
high-performance design points in a few iterations. Compared to
a set of 33 HLS kernels in Xilinx Vitis libraries [44], which are
optimized manually, AutoDSE can achieve the same performance
while utilizing 26.38√ó fewer optimization pragmas for the same
input C programs, resulting in less than one optimization pragma
per kernel2. Therefore, in combination with the Merlin Compiler,
AutoDSE greatly simplifies the DSA design effort on FPGAs, mak-
ing it much more accessible by software programmers who are
familiar with CPU performance optimization.

Schafer et al. [45] provided a good survey of the prior DSE works
up to 2019. They either invoke the HLS tool for evaluating a design
point, as in AutoDSE, or develop a model to mimic the HLS tool.
Employing a model can speed up the DSE process since we can
assess each point in milliseconds instead of several minutes to even
hours. However, as pointed out in [45], directly using the HLS tool
results in a higher accuracy. AutoDSE has shown to outperform
the previous state-of-the-art works. Nevertheless, relying on the
HLS tool does slow down the search process considerably and limit
the scope of exploration. To speedup the design space exploration
process, we are developing a single graph neural network (GNN)-
based model for performance estimation to act as a surrogate of the
HLS tool across different applications. Initial experimental results
show that a GNN-based model can estimate the performance and
resource usage of each design point with high accuracy in millisec-
onds [46, 47]. We are excited by the prospect of applying machine
learning techniques to DSA synthesis.

4.3 Further Raising the Level of Design

Abstraction

While architecture-guided optimizations (Section 4.1) and auto-
mated program transformation (Section 4.2) make it a lot easier to

2the rest of the pragmas consist of STREAM and DATAFLOW which are not included in
the Merlin‚Äôs original pragmas. We will add them in the future. Note that the Merlin
Compiler can directly work with Xilinx HLS pragmas as well.

ExplorerBottleneck OptimizerCache Hit CheckingCode TransformationBottleneck AnalysisResult CommittingEstimatorDesign Config. Waiting QueueC KernelDesign Space Generator + PartitionerProfiler and Seed GenerationDesign Space PartitionDesign Space PartitionDesign Space PartitionDesign Space PartitionDesign Space PartitionRepresentative Design SpaceResult DatabaseC Kernel w. Optimized Design Config.Execution FlowResult QueryMulti-Armed BanditReinforcement LearningHLS w. Vendor ToolsGNN ModelEvaluatorCACM‚Äô22, 2022, USA

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong

achieve high-performance DSA designs from C/C++ programs, the
software community has introduced various domain-specific lan-
guages (DSLs) for better design productivity in certain application
domains. One good example is Halide [48], a widely-used image
processing DSL, which has the advantageous property of decou-
pling the algorithm specification from performance optimization
(via scheduling statements). This is very useful for image process-
ing applications, because it is difficult and time-consuming for a
designer to write image processing algorithms while parallelizing
execution and optimizing for data locality and performance at the
same time, due to the large number of processing stages and the
complex data dependency. However, the plain version of Halide
only supports CPUs and GPUs. There is no way to easily synthesize
the vast number of Halide programs to DSAs on FPGAs. The direct
and traditional way is to rewrite programs in hand-optimized RTL
code or HLS C code, which is very time-consuming. Our goal is to
develop an efficient Halide-based compiler to DSA implementations
on FPGAs.

Our approach is to leverage the recently developed Hete-
roCL [49] language as an intermediate representation (IR). As a
heterogeneous programming infrastructure, HeteroCL provides
a Python-based DSL with a clean programming abstraction that
decouples algorithm specification from three important types of
hardware customization in compute, data types, and memory ar-
chitectures. HeteroCL further captures the interdependence among
these different customizations, allowing programmers to explore
various performance/area/accuracy trade-offs in a systematic and
productive way. In addition, HeteroCL produces highly efficient
hardware implementations for a variety of popular workloads by
targeting spatial architecture templates (Section 4.1) including sys-
tolic arrays (Section 4.1.1) and stencil (Section 4.1.2). HeteroCL
allows programmers to explore the design space efficiently in both
performance and accuracy by combining different types of hard-
ware customization and targeting spatial architectures, while keep-
ing the algorithm code intact.

On top of HeteroCL, we developed HeteroHalide [50], an end-to-
end system for compiling Halide programs to DSAs. As a superset
of Halide, HeteroHalide leverages HeteroCL [49] as an intermedi-
ate representation (IR) to take advantage of its vendor neutrality,
great hardware customization capability, and the separation of al-
gorithms and scheduling. The multiple heterogeneous backends
(spatial architectures) supported by HeteroCL makes HeteroHalide
able to generate efficient hardware code according to the type of
applications. Figure 8 shows the overall workflow of HeteroHalide.

Figure 8: HeteroHalide [50] overall workflow.

HeteroHalide greatly simplifies the migration effort from plain
Halide, since the only requirement is moderate modifications on the
scheduling part, not algorithm. HeteroHalide automatically gener-
ates HeteroCL [49] code, making use of both algorithm and schedul-
ing information specified in a Halide program. Code 5 demonstrates

a blur filter written in HeteroHalide. By changing only the sched-
uling part of the Halide code, HeteroHalide can generate highly
efficient FPGA accelerators that outperform 28 CPU cores by 4.15√ó
on average on 10 real-world Halide applications, including 8 applica-
tions using the stencil backend, 1 using the systolic-array backend,
and 1 using the general backend [50]. HeteroHalide has achieved
this not only by taking advantage of HeteroCL, but also by adding
hardware-specific scheduling primitives to plain Halide. For exam-
ple, when we compile plain Halide, the scheduling is applied directly
at IR level using immediate transformation (Line 9). This may result
in loss of information during such transformations, which could
prevent lower-level compilers from applying optimizations. We
create extensions on Halide schedules, allowing some schedules to
be lowered with annotations, using lazy transformation (Line 7). By
adding this extension to Halide, HeteroHalide can generate specific
scheduling primitives at the HeteroCL backend level, thus emitting
more efficient accelerators, and an image-processing domain expert
will be able to leverage DSAs by just changing the scheduling part
of their existing Halide code. For the following sample Halide code,
HeteroHalide is able to generate 1455 lines of optimized HLS C code
with 439 lines of pragmas, achieving 3.89√ó speedup over 28 CPU
cores using only one memory channel of the AWS F1 FPGA [50].
Using all four memory channels, HeteroHalide can outperform the
Nvidia A100 GPU (that is 2.5√ó more expensive on AWS) by 1.1√ó
using schedules generated by the Li2018 Halide auto-scheduler [51].
We expect to see more gains when sparse computation is involved.
In general, HeteroHalide demonstrated a promising flow of com-
piling high-level DSLs via HeteroCL to FPGAs for efficient DSA
implementation. In addition, the newly emerged MLIR compilation
framework [52] is also promising as an alternative intermediate
representation, which we plan to explore in the future. More oppor-
tunities to improve HLS are discussed in a recent keynote invited
paper in [27].

Code 5: Blur filter written in HeteroHalide [48].

// Algorithm, same as plain Halide
Func blur_x("blur_x"), blur_y("blur_y");
blur_x(x, y) = (image(x, y) + image(x+1, y) + image(x+2, y)) / 3;
blur_y(x, y) = (blur_x(x, y) + blur_x(x, y+1) + blur_x(x, y+2)) / 3;

if (for_hardware) {

blur_x.lazy_unroll(x, 16); // Schedule for hardware, added for HeteroHalide

} else {

blur_x.unroll(x, 16);

// Schedule for software, same as plain Halide

}

1
2
3
4
5
6
7
8
9
10

5 Concluding Remarks

In this article, we show that with architecture-guided optimizations,
automated design space exploration for code transformation and
optimization, and support of high-level DSLs, we can provide a
programming environment and compilation flow that is friendly
to software programmers and empower them to create their own
DSAs on FPGAs with efficiency and affordability. This is a critical
step towards democratization of customized computing.

The techniques presented in this article are not limited to existing
commercially available FPGAs, which were heavily influenced by
communication, signal processing, and other industrial applications
that dominated the FPGA user base in the early days. To address the
growing needs for computing acceleration, a number of features

Xilinx HLSHeteroHalideHalideAlgorithmScheduleHeteroCLAlgorithmScheduleIntel OpenCLSystolic Array (AutoSA)Stencil (SODA)General Backend (Merlin/AutoDSE)Democratizing Domain-Specific Computing

CACM‚Äô22, 2022, USA

have been added, such as the dedicated floating processing units
in the Intel‚Äôs Arria-10 FPGA family and the latest AI processor
engines in the Xilinx‚Äôs Versal FPGA family. We expect this trend
will continue, for example, to possibly incorporate the concept
of coarse-grained reconfigurable arrays (CGRAs) [53] to reduce
the overhead of fine-grained programmability, and greatly reduce
the long physical synthesis time suffered by existing FPGAs. Our
compilation and optimization techniques are readily extensible to
such coarse-grained programmable fabrics. For example, we have
an ongoing project of applying our systolic array compile to the
array of AI engines of the Versal FPGA architecture and adding
CGRA overlays to exisitng FPGAs [54].

In their 2018 Turing Award lecture entitled ‚ÄúA golden age for
computer architecture‚Äù, Hennessy and Patterson concluded that
‚Äúthe next decade will see a Cambrian explosion of novel computer
architectures, meaning exciting times for computer architects in
academia and in industry‚Äù [55], with which we fully agree. Our
research aims at broadening the participation of this exciting jour-
ney, so that not only computer architects, but also a large number
of performance-oriented software programmers can create their
own customized architectures and accelerators on programmable
fabrics to achieve a significant performance and energy efficiency
improvement. We hope that this article can stimulate more research
in this direction.

Acknowledgments

We would like to thank Marci Baun for editing the paper. This
work is supported in part by CRISP, one of six centers in JUMP, a
Semiconductor Research Corporation (SRC) program co-sponsored
by DARPA, the CAPA award jointly funded by NSF (CCF-1723773)
and Intel (36888881), and CDSC industrial partners (https://cdsc.
ucla.edu/partners/).

References

[1] R. H. Dennard, F. H. Gaensslen, H.-N. Yu, V. L. Rideout, E. Bassous, and A. R.
LeBlanc, ‚ÄúDesign of ion-implanted mosfet‚Äôs with very small physical dimensions,‚Äù
vol. 9, pp. 256‚Äì268, 1974.

[2] S. Borkar and A. A. Chien, ‚ÄúThe future of microprocessors,‚Äù Communications of

the ACM, vol. 54, no. 5, pp. 67‚Äì77, 2011.

[3] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger, ‚ÄúDark
silicon and the end of multicore scaling,‚Äù in 2011 38th Annual international sym-
posium on computer architecture (ISCA), pp. 365‚Äì376, IEEE, 2011.

[4] J. Cong, V. Sarkar, G. Reinman, and A. Bui, ‚ÄúCustomizable domain-specific com-

puting,‚Äù IEEE Design & Test of Computers, vol. 28, no. 2, pp. 6‚Äì15, 2010.

[5] N. C. Thompson and S. Spanuth, ‚ÄúThe decline of computers as a general purpose

technology,‚Äù Commun. ACM, vol. 64, p. 64‚Äì72, Feb. 2021.

[6] J. Cong, M. A. Ghodrat, M. Gill, B. Grigorian, and G. Reinman, ‚ÄúCharm: A com-
posable heterogeneous accelerator-rich microprocessor,‚Äù in Proceedings of the
2012 ACM/IEEE international symposium on Low power electronics and design,
pp. 379‚Äì384, 2012.

[7] B. Reagen, Y. S. Shao, G.-Y. Wei, and D. Brooks, ‚ÄúQuantifying acceleration: Pow-
er/performance trade-offs of application kernels in hardware,‚Äù in International
Symposium on Low Power Electronics and Design (ISLPED), pp. 395‚Äì400, IEEE,
2013.

[8] W. J. Dally, Y. Turakhia, and S. Han, ‚ÄúDomain-specific hardware accelerators,‚Äù

Communications of the ACM, vol. 63, no. 7, pp. 48‚Äì57, 2020.

[9] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates,
S. Bhatia, N. Boden, A. Borchers, et al., ‚ÄúIn-datacenter performance analysis of a
tensor processing unit,‚Äù in Proceedings of the 44th annual international symposium
on computer architecture, pp. 1‚Äì12, 2017.

[10] J. Cong, M. A. Ghodrat, M. Gill, B. Grigorian, K. Gururaj, and G. Reinman,
‚ÄúAccelerator-rich architectures: Opportunities and progresses,‚Äù in 2014 51st
ACM/EDAC/IEEE Design Automation Conference (DAC), pp. 1‚Äì6, IEEE, 2014.

[11] McKinsey, ‚ÄúSemiconductor design and manufacturing: Achieving leading-edge

capabilities,‚Äù 2020.

[12] Amazon EC2 F1 Instance, ‚Äúhttps://aws.amazon.com/ec2/instance-types/f1/.‚Äù
[13] NIMBIX - Accelerate your workflows with Xilinx Alveo Accelerator Cards in the

Cloud, ‚Äúhttps://www.nimbix.net/alveo.‚Äù

[14] J. Wang, X. Xie, and J. Cong, ‚ÄúCommunication optimization on gpu: A case
study of sequence alignment algorithms,‚Äù in 2017 IEEE International Parallel and
Distributed Processing Symposium (IPDPS), pp. 72‚Äì81, IEEE, 2017.

[15] T. Zhao, M. Hall, P. Basu, S. Williams, and H. Johansen, ‚ÄúExploiting Reuse and
Vectorization in Blocked Stencil Computations on CPUs and GPUs,‚Äù in SC, 2019.
[16] J. Cong, ‚ÄúFrom parallelization to customization - challenges and opportunities,‚Äù
in 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS),
(Los Alamitos, CA, USA), pp. 682‚Äì682, IEEE Computer Society, may 2021.
[17] N. Samardzic, W. Qiao, V. Aggarwal, M.-C. F. Chang, and J. Cong, ‚ÄúBonsai: High-
performance adaptive merge tree sorting,‚Äù in 2020 ACM/IEEE 47th Annual Inter-
national Symposium on Computer Architecture (ISCA), pp. 282‚Äì294, 2020.
[18] W. Qiao, J. Oh, L. Guo, M. F. Chang, and J. Cong, ‚ÄúFANS: fpga-accelerated
near-storage sorting,‚Äù in 29th IEEE Annual International Symposium on Field-
Programmable Custom Computing Machines, FCCM 2021, Orlando, FL, USA, May
9-12, 2021, pp. 106‚Äì114, IEEE, 2021.

[19] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, ‚ÄúOptimizing fpga-based
accelerator design for deep convolutional neural networks,‚Äù in Proceedings of the
2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,
FPGA ‚Äô15, p. 161‚Äì170, Association for Computing Machinery, 2015.

[20] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massengill, M. Liu, D. Lo, S. Alkalay,
M. Haselman, L. Adams, M. Ghandi, et al., ‚ÄúA configurable cloud-scale dnn pro-
cessor for real-time ai,‚Äù in 2018 ACM/IEEE 45th Annual International Symposium
on Computer Architecture (ISCA), pp. 1‚Äì14, IEEE, 2018.

[21] A. Boutros, E. Nurvitadhi, R. Ma, S. Gribok, Z. Zhao, J. C. Hoe, V. Betz, and M. Lang-
hammer, ‚ÄúBeyond peak performance: Comparing the real performance of ai-
optimized fpgas and gpus,‚Äù in 2020 International Conference on Field-Programmable
Technology (ICFPT), pp. 10‚Äì19, 2020.

[22] D. Abts, J. Ross, J. Sparling, M. Wong-VanHaren, M. Baker, T. Hawkins, A. Bell,
J. Thompson, T. Kahsai, G. Kimmell, J. Hwang, R. Leslie-Hurd, M. Bye, E. Creswick,
M. Boyd, M. Venigalla, E. Laforge, J. Purdy, P. Kamath, D. Maheshwari, M. Beidler,
G. Rosseel, O. Ahmad, G. Gagarin, R. Czekalski, A. Rane, S. Parmar, J. Werner,
J. Sproch, A. Macias, and B. Kurtz, ‚ÄúThink fast: A tensor streaming processor
(tsp) for accelerating deep learning workloads,‚Äù in 2020 ACM/IEEE 47th Annual
International Symposium on Computer Architecture (ISCA), pp. 145‚Äì158, 2020.

[23] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, ‚ÄúFracBNN: Accurate
and FPGA-Efficient Binary Neural Networks with Fractional Activations,‚Äù The
2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,
2021.

[24] Amazon, ‚Äúhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-

aqua.html.‚Äù

[25] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vissers, and Z. Zhang, ‚ÄúHigh-
level synthesis for fpgas: From prototyping to deployment,‚Äù in TCAD, vol. 30,
pp. 473‚Äì491, 2011.

[26] Z. Zhang, Y. Fan, W. Jiang, G. Han, C. Yang, and J. Cong, ‚ÄúAutopilot: A platform-

based esl synthesis system,‚Äù in High-Level Synthesis, pp. 99‚Äì112, 2008.

[27] J. Cong, J. Lau, G. Liu, S. Neuendorffer, P. Pan, K. Vissers, and Z. Zhang, ‚ÄúFpga
hls today: Successes, challenges, and opportunities,‚Äù ACM Transactions on Recon-
figurable Technology and Systems (TRETS), vol. 15, no. 4, pp. 1‚Äì42, 2022.

[28] Xilinx Vitis Platform, ‚Äúhttps://www.xilinx.com/products/design-tools/vitis/vitis-

platform.html.‚Äù

[29] Intel SDK for OpenCL Applications, ‚Äúhttps://software.intel.com/en-us/intel-

opencl.‚Äù

[30] J. Cong, P. Wei, C. H. Yu, and P. Zhang, ‚ÄúAutomated accelerator generation and
optimization with composable, parallel and pipeline architecture,‚Äù in 2018 55th
ACM/ESDA/IEEE Design Automation Conference (DAC), pp. 1‚Äì6, IEEE, 2018.
[31] H. Rong, ‚ÄúProgrammatic control of a compiler for generating high-performance

spatial hardware,‚Äù arXiv preprint arXiv:1711.07606, 2017.

[32] J. Wang, L. Guo, and J. Cong, ‚ÄúAutosa: A polyhedral compiler for high-
performance systolic arrays on fpga,‚Äù in Proceedings of the 2021 ACM/SIGDA
international symposium on Field-programmable gate arrays, 2021.

[33] S. Verdoolaege, ‚Äúisl: An integer set library for the polyhedral model,‚Äù in Interna-

tional Congress on Mathematical Software, pp. 299‚Äì302, Springer, 2010.

[34] Y. Chi, L. Guo, J. Lau, Y.-k. Choi, J. Wang, and J. Cong, ‚ÄúExtending high-level
synthesis for task-parallel programs,‚Äù in 2021 IEEE 29th Annual International
Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 204‚Äì
213, IEEE, 2021.

[35] H. Genc, A. Haj-Ali, V. Iyer, A. Amid, H. Mao, J. Wright, C. Schmidt, J. Zhao, A. Ou,
M. Banister, et al., ‚ÄúGemmini: An agile systolic array generator enabling system-
atic evaluations of deep-learning architectures,‚Äù arXiv preprint arXiv:1911.09925,
2019.

[36] Y. Chi, J. Cong, P. Wei, and P. Zhou, ‚ÄúSODA: Stencil with Optimized Dataflow

Architecture,‚Äù in ICCAD, 2018.

CACM‚Äô22, 2022, USA

Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, Jason Cong

[37] Y. Chi and J. Cong, ‚ÄúExploiting Computation Reuse for Stencil Accelerators,‚Äù in

Design Automation Conference, pp. 1347‚Äì1350, 2022.

DAC, 2020.

[38] Z. Chen, H. T. Blair, and J. Cong, ‚ÄúLANMC: LSTM-Assisted Non-Rigid Motion

Correction on FPGA for Calcium Image Stabilization,‚Äù in FPGA, 2019.

[39] D. Huff, S. Dai, and P. Hanrahan, ‚ÄúClockwork: Resource-Efficient Static Scheduling
for Multi-Rate Image Processing Applications on FPGAs,‚Äù in FCCM, 2021.
[40] J. Cong, M. Huang, P. Pan, D. Wu, and P. Zhang, ‚ÄúSoftware infrastructure for
enabling fpga-based accelerations in data centers,‚Äù in ISLPED, pp. 154‚Äì155, 2016.
[41] L. Dagum and R. Menon, ‚ÄúOpenmp: an industry standard api for shared-memory
programming,‚Äù IEEE computational science and engineering, vol. 5, no. 1, pp. 46‚Äì55,
1998.

[42] B. Reagen, R. Adolf, Y. S. Shao, G.-Y. Wei, and D. Brooks, ‚ÄúMachsuite: Benchmarks
for accelerator design and customized architectures,‚Äù in 2014 IEEE International
Symposium on Workload Characterization (IISWC), pp. 110‚Äì119, IEEE, 2014.
[43] A. Sohrabizadeh, C. H. Yu, M. Gao, and J. Cong, ‚ÄúAutodse: Enabling software
programmers to design efficient fpga accelerators,‚Äù ACM Transactions on Design
Automation of Electronic Systems (TODAES), vol. 27, no. 4, pp. 1‚Äì27, 2022.

[44] Xilinx Vitis Libraries, ‚Äúwww.github.com/Xilinx/Vitis_Libraries.‚Äù
[45] B. C. Schafer and Z. Wang, ‚ÄúHigh-level synthesis design space exploration: Past,
present, and future,‚Äù IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, vol. 39, no. 10, pp. 2628‚Äì2639, 2020.

[46] A. Sohrabizadeh, Y. Bai, Y. Sun, and J. Cong, ‚ÄúAutomated accelerator optimization

aided by graph neural networks,‚Äù pp. 55‚Äì60, 2022.

[47] Y. Bai, A. Sohrabizadeh, Y. Sun, and J. Cong, ‚ÄúImproving gnn-based accelerator
design automation with meta learning,‚Äù in Proceedings of the 59th ACM/IEEE

[48] J. Ragan-Kelley, A. Adams, S. Paris, M. Levoy, S. Amarasinghe, and F. Durand, ‚ÄúDe-
coupling Algorithms from Schedules for Easy Optimization of Image Processing
Pipelines,‚Äù in SIGGRAPH, 2012.

[49] Y.-H. Lai, Y. Chi, Y. Hu, J. Wang, C. H. Yu, Y. Zhou, J. Cong, and Z. Zhang,
‚ÄúHeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined
Reconfigurable Computing,‚Äù in FPGA, 2019.

[50] J. Li, Y. Chi, and J. Cong, ‚ÄúHeteroHalide: From Image Processing DSL to Efficient

FPGA Acceleration,‚Äù in FPGA, 2020.

[51] T.-M. Li, M. Gharbi, A. Adams, F. Durand, and J. Ragan-Kelley, ‚ÄúDifferentiable
Programming for Image Processing and Deep Learning in Halide,‚Äù ToG, vol. 37,
no. 4, 2018.

[52] C. Lattner, M. Amini, U. Bondhugula, A. Cohen, A. Davis, J. Pienaar, R. Riddle,
T. Shpeisman, N. Vasilache, and O. Zinenko, ‚ÄúMlir: A compiler infrastructure for
the end of moore‚Äôs law,‚Äù arXiv preprint arXiv:2002.11054, 2020.

[53] L. Liu, J. Zhu, Z. Li, Y. Lu, Y. Deng, J. Han, S. Yin, and S. Wei, ‚ÄúA survey of
coarse-grained reconfigurable architecture and design: Taxonomy, challenges,
and applications,‚Äù ACM Computing Surveys (CSUR), vol. 52, no. 6, pp. 1‚Äì39, 2019.
[54] S. Liu, J. Weng, D. Kupsh, A. Sohrabizadeh, Z. Wang, L. Guo, J. Liu, M. Zhulin,
R. Mani, L. Zhang, J. Cong, and T. Nowatzki, ‚ÄúOvergen: Improving fpga usabil-
ity through domain-specific overlay generation,‚Äù 2022 55th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO), 2022.

[55] J. L. Hennessy and D. A. Patterson, ‚ÄúA new golden age for computer architecture,‚Äù

Communications of the ACM, vol. 62, no. 2, pp. 48‚Äì60, 2019.

