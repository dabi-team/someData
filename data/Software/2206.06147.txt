1

A DSEL for High Throughput and Low Latency
Software-Deﬁned Radio on Multicore CPUs
Adrien Cassagne∗, Romain Tajan†, Olivier Aumage‡, Camille Leroux†, Denis Barthou‡ and Christophe J´ego†
∗Sorbonne Universit´e, CNRS, LIP6, F-75005 Paris, France
†IMS Laboratory, UMR CNRS 5218, Bordeaux INP, University of Bordeaux, Talence, France
‡Inria, Bordeaux Institute of Technology, LaBRI/CNRS, Bordeaux, France

2
2
0
2

g
u
A
3

]
L
C
.
s
c
[

2
v
7
4
1
6
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—This article presents a new Domain Speciﬁc Em-
bedded Language (DSEL) dedicated to Software-Deﬁned Radio
(SDR). From a set of carefully designed components, it enables
to build efﬁcient software digital communication systems, able
to take advantage of the parallelism of modern processor archi-
tectures, in a straightforward and safe manner for the program-
mer. In particular, proposed DSEL enables the combination of
pipelining and sequence duplication techniques to extract both
temporal and spatial parallelism from digital communication
systems. We leverage the DSEL capabilities on a real use case:
a fully digital transceiver for the widely used DVB-S2 standard
designed entirely in software. Through evaluation, we show how
proposed software DVB-S2 transceiver is able to get the most
from modern, high-end multicore CPU targets.

Index Terms—DSEL, SDR, Multicore CPUs, Pipeline, Real-

time system, DVB-S2 transceiver

I. INTRODUCTION

D IGITAL communication systems are traditionally im-

plemented onto dedicated hardware (ASIC) to achieve
high throughputs, low latencies and energy efﬁciency. How-
ever, hardware implementations suffer from a long time to
market, are expensive and speciﬁc by nature [1], [2]. New
communication standards such as the 5G are coming with
large speciﬁcations and numerous possible conﬁgurations [3].
Connecting objects that exchange small amounts of data at
low rates will live together with 4K video streaming for mobile
phone games requiring high throughputs and low latencies [4].
To meet such diverse speciﬁcations, transceivers have to
be able to adapt quickly to new conﬁgurations. Flexible, re-
conﬁgurable and programmable solutions are thus increasingly
required, fueling a growing interest for the Software-Deﬁned
Radio (SDR). It consists in processing both the Physical
(PHY) and Medium Access Control (MAC) layers in soft-
ware [5], rather than in hardware. Shorter time to market,
lower design costs, ability to be updated, to support and to
interroperate with new protocols are its main advantages [6].
SDR can be implemented on various targets such as Field
Programmable Gate Arrays (FPGAs) [7]–[12], Digital Signal
Processors (DSPs) [10], [13], [14] or General Purpose Pro-
cessors (GPPs) [15]–[18]. Many SDR elementary blocks have
been optimized for Intel® and ARM® CPUs. High throughput
results have been achieved on GPUs [19]–[23]; latency results
are is still too high however to meet real time constraints and
to compete with CPU implementations [22], [24]–[33]. This is
mainly due to data transfers between the host (CPUs) and the

device (GPUs), and to the nature of GPU designs, which are
not optimized for latency efﬁciency. In this paper, we focus
on the execution of SDR on multicore general purpose CPUs,
similar to the equipment of antennas or transceivers.

Digital communication systems can be reﬁned so that the
transmitter and the receiver parts are decomposed into several
processing blocks connected in a directed graph. This matches
the dataﬂow model [34], [35]: Blocks are ﬁlters and links
between blocks are data exchanges. Speciﬁc dataﬂow models
such as the synchronous dataﬂow [36] and the cyclo-static
dataﬂow [37], [38] allow the expression of a static schedule
for the graph [39]. SDR however requires a parallel task graph
between stateful tasks and a dynamic schedule due to early
exits, conditionals and loop iterations. Maximizing throughput
is the main objective, keeping latency as low as possible is
a secondary objective. This constrains the time taken by data
movements and requires optimizing for parallelism. For a SDR
operating in antennas or transceivers, memory footprint is not
an issue and all tasks run on multicore CPUs.

This paper includes the following contributions:
• A DSEL based on C++ to build parallel dataﬂow graphs
for SDR signal processing, supporting loops, condition-
als, pipeline and fork/join parallelism.

• A set of micro-benchmarks to analyze the time taken by

the different constructs;

• A complete, real-life example using the DSEL, of a DVB-
S2 transceiver running on both x86 and ARM CPUs.
Section II discusses related works. The proposed DSEL is
presented in Section III. Section IV details scheduling and
parallelism supports. Section V experiments with a DVB-S2
implementation built on the DSEL.

II. RELATED WORKS

Many languages dedicated to streaming applications have
been introduced [40]–[46]. These languages are often variants
of the cyclo-static dataﬂow model and propose automatic
parallelization techniques such as pipelining and forks/joins.
In [47], authors proposed a full compilation chain for SDR,
based on LLVM on heterogeneous MPSoCs. This is promising
but it differs from our approach. Indeed, we chose to integrate
our language into C++, making the DSEL compatible with any
C++11 compilers. Works are also tackling OS and hardware
aspects of SDR [48]–[50]. However, the studied SDR systems
are much simpler than those addressed in this paper.

 
 
 
 
 
 
Few solutions speciﬁcally target SDR sub-domain so far.
GNU Radio [51] is the most famous one. It is open source
and largely adopted by the community. It comes bundled with
a large variety of digital communication techniques used in
real life systems. The last version of GNU Radio (3.9) can
take advantage of multi-core CPUs. One thread is spawn per
block and the scheduling is directly managed by the operating
system. While sufﬁcient on uniform memory access (UMA)
architectures [52], this does not take into account non uniform
memory access (NUMA) architectures with many cores. A
drawback of assigning a block per thread is that the designed
SDR system is strongly linked to the parallelism strategy.
Depending on the CPU architecture,
it can be necessary
to change the parallelism strategy while keeping the same
SDR system description. GNU Radio designers are currently
working on a proof of concept scheduler (newsched) for the
future GNU Radio version 4 [53]. They introduced the concept
of workers that can execute more than one block on a physical
this new version of
core. To the best of our knowledge,
GNU Radio breaks the compatibility with the existing systems
designed with GNU Radio and is not yet fully implemented.
However,
this new version goes in the same direction as
what we propose and we hope that some contributions of this
paper could help the GNU Radio project. To the best of our
knowledge, GNU Radio does not implement the duplication
mechanism presented in Section III-C and we show this is a
key mechanism for high throughputs and scalability. Besides,
a new construct in the next section (cf. the switcher module
in Section III-A) allows the design loops and conditions for
SDR systems, not yet supported by GNU Radio where only
static directed acyclic graph can be managed.

Some other works are focusing particularly on an SDR
implementation for a DVB-S2 transceiver. Hereafter are the
projects we have identiﬁed:

• leansdr. A standalone open source project [54]. A low-
density parity-check (LDPC) bit-ﬂipping decoder [55] is
chosen. The project does not support multi-threading.
• gr-dvbs2rx. An open source out-of-tree module [56]
for GNU Radio. One of the motivation of this project
is to increase the throughput compared to leansdr. The
project is open-source and we were able to perform a fair
comparison. The results are presented in Section V-E.
• Grayver and Utter. In a recently published paper [18],
they succeed in building a 10 Gb/s DVB-S2 receiver on
a cluster of server-class CPUs. The implementation is
closed sources, making fair comparisons difﬁcult.

III. DESCRIPTION OF THE PROPOSED DOMAIN SPECIFIC
EMBEDDED LANGUAGE
This section introduces a DSEL working on sets of symbols
(aka frames). It implements a form of the dataﬂow model,
single rate, tailored to the relevant characteristics of digital
communication chains with channel coding. The language
deﬁnes elementary and parallel components.

A. Elementary Components

Four elementary components are deﬁned: sequence, module,
task and socket. The task is the fundamental component. It can

2

be an encoder, a decoder or a modulator for instance and is
a single-threaded code function. It is designated as ﬁlter in
the standard dataﬂow model. Though unlike a dataﬂow ﬁlter,
a task can have an internal state and a private memory to
store temporary data. Additionally, a set of tasks can share a
common internal/private memory. In that case, multiple tasks
are grouped into a single module. The main problem with
internal memory is that tasks cannot be executed safely by
several threads in parallel because of data races. However, in
many cases the expression of a task or a set of tasks can be
simpliﬁed by allowing stateful tasks and modules.

A task can consume and produce public data through the
input and/or output sockets it exposes. Connecting the sockets
of different tasks is called binding. An input socket can only
be bound to one output socket, while an output socket can be
bound to multiple input sockets. A task can only be executed
once all its input sockets are bound.

Tasks can be grouped into a sequence. A sequence corre-
sponds to a static schedule of tasks. To create a sequence, the
designer speciﬁes the ﬁrst tasks and the last tasks to execute.
Then the connected tasks are analyzed and a sequence object is
built. The analysis is a depth-ﬁrst traversal of the task graph
and independent tasks are ordered according to the binding
order of their inputs. The principle is to add a task to the array
of function pointers when all the input sockets are visited in
the depth ﬁrst traversal of the tasks graph. After that, the output
sockets of the current task are followed to reach new tasks.
The order in which the tasks have been traversed is memorized
in the sequence. When the designer calls the exec method on
a sequence, the tasks are executed successively according to
this statically scheduled order.

(a) Simple chain sequence.

(b) Sequence with multiple ﬁrst and last
tasks.

Fig. 1. Example of task sequences.

Fig. 1 shows two examples of task sequences. Fig. 1a is a
simple chain of tasks. The designer only needs to specify its
ﬁrst task (t1); the sequence analysis then follows the binding
until the last task (t4). In Fig. 1b, bound tasks exist before
and after the current sequence, which also has two ﬁrst tasks
(t1 and t3) and two last tasks (t5 and t6). In this case, the

𝑡1𝑡2𝑡3𝑡4SequenceTaskModuleOutputsocketInputsocket𝑀1𝑀2𝑀3𝑀4𝑡1𝑡2𝑡3𝑡4𝑡5𝑡6Sequence𝑀1𝑀2𝑀3𝑀4𝑀53

(a) Decomposition of the loop in modules.

(b) Decomposition of the loop in sub-sequences.

(c) Execution graph.

Fig. 2. Example of a sequence with a while loop.

and depends on the runtime data. It is also possible to model
the for loop behavior by ignoring the t2 input data and by
adding an internal state to M2, namely the loop counter. If tcom
receives a 0, then the internal path of the switcher will be 0.
Then t3, t4 and t5 tasks will be executed and tsel will select the
t5 output instead of the t1 output, and so on. Fig. 2b shows how
the tasks are regrouped into sub-sequences. It enables to build
the execution graph illustrated in Fig. 2c. The corresponding
pseudo code of the presented loop is shown in Alg. 1. One
can note that in the proposed DSEL there is no limitation to
include nested loops schemes. The loop pattern is common in
iterative demodulation/decoding. This is why it is required in
a DSEL dedicated to SDR. As an exception rule in the graph
construction, the select task is added to the graph when its
last input socket is visited (while all the other tasks require all
input sockets to be visited).

(a) Decomposition of the switch in sub-sequences.

designer has to explicitly specify that t1 and t3 are ﬁrst tasks.
If t1 is sequentially deﬁned before t3 then t1 will be executed
ﬁrst and t3 after. The analysis starts from t1 and continue
to traverse new tasks if possible. In this example, t2 can be
executed directly after t1, but t4 cannot because it depends
on t3. So the analysis stops after t2 and then restarts from t3.
Actually, the index i of the ti task represents the execution
order. The t5 and t6 last tasks have to be explicitly speciﬁed
because their output sockets are bound: the analysis cannot
guess the end of the sequence.

In targeted SDR applications, processing is continuously
repeated on batches of frames as long as the system is on. A
sequence is thus executed in a loop. When the last sequence
task is executed, the next task is the ﬁrst one on the next
frame. The designer can control whether the sequence should
restart by supplying a condition function to the sequence exec
method. The boolean returned by the function conditions
whether the sequence is repeated. A task of a sequence
may also raise an abort exception upon some condition, to
immediately stop the current sequence execution and start the
ﬁrst task of the sequence. In Fig. 1a if the t3 task raises the
abort exception then the next executed task is t1.

Some digital communication systems include schemes that
require a loop or a conditional. A sequence of tasks is executed
one or more times depending on a condition task (or a control
task). To build loops and conditionals, we introduce a switcher
module composed of two control ﬂow tasks. The select task
selects one among several exclusive input paths. The commute
task creates two or more exclusive output paths.

Algorithm 1 Pseudo code of a loop (corresponding to Fig. 2)

execute SS1;
while execute SS2 and not t2.out:

execute SS3;

execute SS4;

Fig. 2 illustrates a loop. To build a loop structure (a while
loop in the example), the select and commute tasks (in the
given order) of a common switcher module are used (see
Fig. 2a). By convention, in a switcher module, the selected
path is initialized to the highest possible path (here 1). So, at
the ﬁrst tsel execution, the t1 output will be selected. Then the
t2 control task will send 0 or 1 as a control socket to tcom.
Here the t2 loop control task is based on the tsel output socket.
As a consequence, the path selection (0 or 1) is dynamic

Fig. 3. Example of a sequence with a switch.

(b) Execution graph.

Fig. 3 illustrates a switch structure. The same switcher
module presented in while loop structures is necessary. The
number of output/input sockets in resp. tcom/tsel tasks is 3
instead of 2 in the while loop example. Also, the position
of these tasks has been swapped, in the current example tcom
is executed before tsel. t2 is a control task that depends on
the output of t1. The t2 task output can be 0, 1 or 2. The

𝑡1𝑡sel𝑡2𝑡com𝑡3𝑡4𝑡5𝑡6𝑀1Switcher𝑀2𝑀3𝑀4𝑀5𝑀6𝑡1𝑡sel𝑡2𝑡com𝑡3𝑡4𝑡5𝑡6𝑆𝑆1𝑆𝑆sel𝑆𝑆2𝑆𝑆com𝑆𝑆3𝑆𝑆4𝑆𝑆1𝑆𝑆sel𝑆𝑆2𝑆𝑆com𝑆𝑆3𝑆𝑆41010𝑡1𝑡2𝑡com𝑡3𝑡4𝑡5𝑡6𝑡7𝑡8𝑡sel𝑡9𝑆𝑆1𝑆𝑆com𝑆𝑆2𝑆𝑆3𝑆𝑆4𝑆𝑆sel𝑆𝑆5𝑆𝑆1𝑆𝑆com𝑆𝑆2𝑆𝑆3𝑆𝑆4𝑆𝑆sel𝑆𝑆5012012TABLE I
EXECUTION OF 1 125 000 C TASKS OF 4 µs EACH. THEORETICAL EXECUTION TIME IS 4500 MS FOR EACH MICRO-BENCHMARK.

C tasks

Ssel tasks

Scom tasks

I tasks

Other

Overhead

Label

Seq. exec.

Run time (ms)

Exec.

Time (ms)

Exec.

Time (ms)

Exec.

Time (ms)

Exec.

Time (ms)

Time (ms)

M B1
M B2
M B3
M B4

375000
37500
37500
562500

4656.45
4744.08
4777.03
4784.88

1125000
1125000
1125000
1125000

151.86
151.86
151.86
151.86

–
412500
562500
562500

–
24.75
33.75
33.75

–
412500
562500
562500

–
33.00
45.00
45.00

–
412500
562500
562500

–
28.88
39.38
39.38

4.59
5.59
7.04
14.89

4

Algorithm 2 Pseudo code of a switch (corresponding to Fig. 3)

execute SS1;
switch t2.out:

case 0: execute SS2;
case 1: execute SS3;
case 2: execute SS4;

execute SS5;

switch exclusive path is determined dynamically depending
on the runtime data. Fig. 3a shows the decomposition of
the tasks in sub-sequences and Fig. 3b presents the resulting
execution graph. Alg. 2 gives the corresponding pseudo code.
The switch pattern is useful
in many SDR contexts. For
instance, depending on the signal to noise ratio (SNR), the
receiver can select a different path adapted to the signal quality.

B. Performance Evaluation on Micro-benchmarks

In this section, an estimation of the DSEL overhead is
measured from four micro-benchmarks: a simple chain (see
Fig. 1a) denoted M B1, a single for loop (Fig. 2) denoted
M B2, a system of two nested for loops denoted M B3, and
a system with a switch (Fig. 3) denoted M B4. In M B1,
M B2 and M B3 three computational tasks are chained. In
M B2, the loop performs 10 iterations. In M B3, the inner loop
performs 5 iterations, the outer loop performs 2 iterations.
In M B4, three computational tasks are chained in the ﬁrst
path, two in the second path and a single in the last path.
Moreover, an iterate task is conﬁgured to perform a cyclic path
selection (0,1,2,0,1,2,...). In each computational task, an active
wait of the same amount of time is performed. Four types of
tasks are used: computational tasks C, select and commute
switcher module tasks Ssel and Scom resp., and iterate tasks I
to determine paths in loops and switches (I = control task).
Evaluations ran on a single core of an Intel® Core™ i5-
8250U @ 1.60 GHz. The Turbo Boost mode has been disabled.
This processor has a 15-Watt TDP that matches embedded
system constraints. Though duration of a C task is controlled
by the programmer, we measured a constant 135 ns over-
head due to the DSEL and to the system call behind the
std::chrono::steady_clock::now() function. We
measured Ssel tasks around 60 ns, Scom tasks around 80 ns,
and I tasks around 70 ns. Later on, Ssel, Scom and I tasks are
reported as overhead. both Ssel and Scom tasks are copy-less,
thus for a given conﬁguration, their execution time is constant.
Tab. I reports the execution time of 1 125 000 C tasks for
each case. Column Seq. exec. gives the number of sequence

executions required to run 1 125 000 C tasks. Theoretical
time is computed directly from the number of C tasks and
the duration of the active waiting in each task: Ttheoretical =
1125000 × 4 µs = 4500 ms. Run time column reports the
measured execution time. Remaining columns report
task
counts and overheads per task types. Last column Other
reports the residual time that does not come from the tasks
execution. In each benchmark, a stop condition is evaluated at
the end of the sequence. The condition checks that the current
number of executions is lower than the one given in the Seq.
exec. column. This comes with an extra cost because of an
additional function call for each sequence execution. This is
also why the execution time of M B4 is higher than M B3,
there is signiﬁcantly more sequence executions in M B4.

Fig. 4. Global overhead of the proposed DSEL on 4 micro-benchmarks.

Fig. 4 shows the overhead depending on the granularity of
the C tasks. It results that from 4 µs tasks, the proposed DSEL
has an acceptable overhead. For tasks longer than 4 µs the
overhead is negligible. This shows that the proposed DSEL
matches the low latency requirements of SDR systems.

C. Parallel Components

A sequence can be duplicated, to let several threads execute
in parallel, see Fig. 5. The number t of duplicates is
it
a parameter of its constructor. There is no synchronization
between sequence duplicates. Each threaded sequence can be
executed on one dedicated core and the public data transfers
remain on this core for the data reuse in the caches. By default,
modules have no duplication mechanism (see next section).

In some particular cases such as in the signal synchroniza-
tion processing, the tasks can have a dependency on them-
selves. It is then impossible to duplicate the sequence because

0.51.02.04.08.016.005101520253035TimeperCtask(𝜇𝑠)Globaloverhead(%)Simplechain(𝑀𝐵1)Forloop(𝑀𝐵2)Nestedforloops(𝑀𝐵3)Switch(𝑀𝐵4)5

(a) Description of a pipeline: tasks creation, tasks
binding and sequences/stages deﬁnition (with the
corresponding number of threads).

(b) Automatic parallelization of a pipeline description: sequence duplica-
tions, 1 to n and n to 1 adaptors creation and binding.

Fig. 6. Example of a pipeline description and the associate transformation
with adaptors.

module::M1 m1_obj(/* ... */); // ’M1’ class & ’t1’ task
module::M2 m2_obj(/* ... */); // ’M2’ class & ’t2’ task

. . .

module::M5 m5_obj(/* ... */); // ’M5’ class & ’t5’ task

// 2) binding of the tasks
m2_obj[sck::t2::in] = m1_obj[sck::t1::out];
m3_obj[sck::t3::in] = m2_obj[sck::t2::out];

. . .

m5_obj[sck::t5::in] = m4_obj[sck::t4::out];

// 3) creation of the pipeline (= sequences and pipeline
//
tools::Pipeline pipeline(

analyses)

// first task of the sequence (for validation purpose)
m1_obj[tsk::t1],
// description of the sequence decomposition in
// 3 pipeline stages
{

{ { m1_obj[tsk::t1] },

{ m1_obj[tsk::t1] } }, // last

{ { m2_obj[tsk::t2] },

{ m3_obj[tsk::t3] } }, // last

{ { m4_obj[tsk::t4] },

// first tasks of stage 1
tasks of stage 1
// first tasks of stage 2
tasks of stage 2
// first tasks of stage 3
tasks of stage 3

{ m5_obj[tsk::t5] } }, // last

},
// number of threads per stage (4 sequence duplications
// in stage 2)
{ 1,4,1 }, /* ... */
// explicit pinning of the threads
{

1 }, // stage 1: thread

{
{ 3,4,5,6 }, // stage 2: threads ’1-4’ to cores ’3-6’
{

8 }, // stage 3: thread

to core

to core

’8’

’1’

’1’

’1’

});

// 4) execution of the pipeline, it is indefinitely
//
pipeline.exec([]() { return false; });

executed in loop

Fig. 7. C++ DSEL source code of the pipeline described in Fig. 6.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39

Fig. 5. Sequence duplication for multi-threaded execution. The modules are
duplicated with their tasks and data.

of the sequential nature of the tasks. To overcome this issue,
the well-known pipelining strategy can be applied to increase
the sequence throughput up to the slowest task throughput. The
proposed DSEL comes with a speciﬁc pipeline component to
this purpose. The pipeline takes multiple sequences as input.
Each sequence of the pipeline is called a stage, run on one
thread. For instance, a 4-stage pipeline creates 4 threads. A
pipeline stage can be combined with the sequence duplication
strategy. It means that there are nested threads in the current
stage thread. Pipelining comes with an extra synchronization
cost between the stage threads, implementation details are
discussed in the next section.

IV. AUTOMATED PARALLELIZATION TECHNIQUES

A. Sequence Duplication

In a fully dataﬂow-compliant model, there is no need to du-
plicate the sequence because a stateless task is always thread-
safe. In the proposed DSEL with stateful tasks, a clone method
is deﬁned for each module to deal with internal state and
private memory (stored in the module). The clone method is
polymorphic and deﬁned in the Module abstract class. It relies
on the implicit copy constructors and a deep copy protected
method (overridable). It is the responsibility of the ModuleImpl
developer to correctly override the deep copy method and to
make sure the duplication is valid for this module. The deep
copy method deals with pointer and reference members. If
the pointer/reference members are read-only (const), then
the implicit copy constructor copies the memory addresses
automatically. When the current ModuleImpl class owns one
ore more writable references, the module cannot be cloned
and its tasks are sequential. However, for a writable pointer
member, the developer can explicitly allocate a new pointer in
the deep copy method.

B. Pipeline

In this section, the pipeline implementation is illustrated
through a simple example. Fig. 6 shows the difference between

𝑡11𝑡12𝑡13𝑡14𝑡21𝑡22𝑡23𝑡24𝑡1𝑡2𝑡3𝑡4𝑡𝑡1𝑡𝑡2𝑡𝑡3𝑡𝑡4thread𝑀1𝑀2𝑀3𝑀11𝑀12𝑀13𝑀21𝑀22𝑀23𝑀𝑡1𝑀𝑡2𝑀𝑡3SequenceSequence𝑡1𝑡2𝑡3𝑡4𝑡5Stage1Stage2Stage3Pipeline𝑡1push1→𝑛pull1→𝑛𝑡2𝑡3push𝑛→1pull′1→𝑛𝑡′2𝑡′3push′𝑛→1pull′′1→𝑛𝑡′′2𝑡′′3push′′𝑛→1pull′′′1→𝑛𝑡′′′2𝑡′′′3push′′′𝑛→1pull𝑛→1𝑡4𝑡5Stage1Stage2Stage31to𝑛adaptor𝑛to1adaptorPipelineCore1Core2Core3Core4Core5Core6Core7Core8Stage1Stage2Stage3PipelinethreadspinningonamulticoreCPUa pipeline description (see Fig. 6a) and its actual instantiation
(see Fig. 6b). In Fig. 6 we suppose that the t1, t4 and t5
tasks cannot be duplicated (plain boxes). The designer knows
the execution time of the t1 task is higher than the
that
cumulated execution time of tasks t4 and t5. We assume that
the cumulated execution time of t2 and t3 is approximatively
four times higher than t1. This knowledge motivates the
splitting of the stages 1, 2 and 3. There is no need to split the
t4 and t5 tasks in two stages because the overall throughput
is limited by the slowest stage (t1 here). Stage 2 is duplicated
four times to increase its throughput by four as we know that
its latency is approximatively four times that of Stage 1. In
general, a preliminary proﬁling phase of the sequential code
is required to guide the pipeline strategy. Listing 7 presents
the C++ DSEL source code corresponding to the pipeline
description in Fig. 6a. Each task ti is contained (as a method)
in the Mi module (or class). The four main steps are: 1)
Creation of the modules; 2) Binding of the tasks; 3) Creation
of the pipeline strategy; 4) Pipeline execution.

Fig. 6b presents the internal structure of the pipeline. As we
can see, new tasks have been automatically added: push1→n,
pull1→n shared by a 1 to n adaptor module and pushn→1,
pulln→1 shared by a n to 1 adaptor module. The binding
as been modiﬁed to insert the tasks of the adaptors. In the
initial pipeline description, t1 is bound to t2. In a parallel
pipelined execution this is not possible anymore because many
threads are running concurrently: One for stage 1, four for
stage 2 and one for stage 3 in the example. To this purpose, the
adaptors implement a producer-consumer scheme. The yellow
diamonds represent the buffers that are required. The push1→n
and pulln→1 tasks can only be executed by a single thread
while the pull1→n and pushn→1 tasks are thread-safe. The
push1→n task copies its input socket in one buffer each time
it is called. There is one buffer per duplicated sequence. To
guarantee that the order of the input frames is preserved, a
round-robin scheduling has been adopted. On the other side,
the pulln→1 task is copying the data from the buffers to its
output socket, with the same round-robin scheduling.

The size of the synchronization buffers in the adaptors are
deﬁned on creation. The default size is one. During the copy
of the input socket data in one of the buffers, threads cannot
access the data until the copy is ﬁnished. The synchronization
is automatically managed by the framework. If the buffer is
full, the producer (push1→n and pushn→1 tasks) has to wait.
The same applies for the consumer (pull1→n and pulln→1
tasks) if the buffer is empty. We implemented both active and
passive waiting.

Copies from and to buffers are expensive. These copies are
removed by dynamically re-binding the tasks just before and
just after the push and pull tasks, and casting the tasks into
copyless variants. It is also necessary to bypass the regular
execution in the push1→n, pull1→n, pushn→1 and pulln→1
tasks. This replaces the source code of the data buffer copy by
a simple pointer copy. The pointers are exchanged cyclically.
In Fig. 6b, the pipeline threads are pinned to speciﬁc CPU
cores. This is the direct consequence of the lines 31-35 in
Listing 7. The hwloc library [57] has been used and integrated
in our DSEL to pin the software threads to processing units

6

(hardware threads). In the given example, we assume that the
CPU cores can only execute one hardware thread (SMT off).
The threads pinning is given by the designer. This can improve
the multi-threading performance on NUMA architectures.

Fig. 6 is an example of a simple chain of tasks. More
complicated task graphs can have more than two tasks to
synchronize between two pipeline stages. The adaptor imple-
mentation can manage multiple socket synchronizations. The
key idea is to deal with a 2-dimensional array of buffers.
Another difﬁcult case is when a task t1 is in stage 1 and
possesses an output socket bound to an other task tx which is
located in the stage 4. To work, the pipeline adaptors between
the stages 1 and 2 and the stages 2 and 3 automatically
synchronize the data of the t1 output socket.

V. APPLICATION ON THE DVB-S2 STANDARD

In this section, we present a real use case of our DSEL.
The second generation of Digital Video Broadcasting standard
for Satellite (DVB-S2) [58] is a ﬂexible standard designed
for broadcast applications. DVB-S2 is typically used for the
digital television (HDTV with H.264 source coding). The full
DVB-S2 transmitter and receiver are implemented in a SDR-
compliant system. Two Universal Software Radio Peripherals
(USRPs) N3201 have been used for the analog signal trans-
mission and reception where all the digital processing of the
system have been implemented. The purpose of this section is
not to detail all the implemented tasks extensively, but rather to
expose the system as a whole. Some speciﬁc focuses are given
to describe the main encountered problems and solutions.

A. Transmitter Software Implementation

TABLE II
SELECTED DVB-S2 CONFIGURATIONS (MODCOD).

Conﬁg.

Mod.

Rate R KBCH KLDPC

Interleaver

MODCOD 1
MODCOD 2
MODCOD 3

QPSK
QPSK
8-PSK

3/5
8/9
8/9

9552
14232
14232

9720
14400
14400

no
no
col/row

The DVB-S2 coding scheme rests upon the serial concate-
nation of a Bose, Ray-Chaudhuri & Hocquenghem (BCH)
and a LDPC code. The selected modulation is a Phase-Shift
Keying (PSK). The standard deﬁnes 32 MODulation and
CODing schemes or MODCODs. This work focuses on the
3 MODCODs given in Tab. II. Depending on the MODCOD,
the PSK modulation and the LDPC code rate R vary. In
MODCOD 1 and 2 there is no interleaver, MODCOD 3
uses a column/row interleaver. KBCH or K is the number
of information bits and the input size of the BCH encoder.
NBCH or KLDPC is the output size of the BCH encoder and the
input size of the LDPC encoder. For each selected MODCOD,
NLDPC = 16200. With the pay load header (PLH) and pilots
bits, the frame size (NPLH or N ) contains a total of 16740
bits.

1USRP N320: https://www.ettus.com/all-products/usrp-n320/.

7

Fig. 8. DVB-S2 transmitter software implementation.

(a) Waiting phase and learning phase 1 & 2.

Fig. 9. DVB-S2 receiver software implementation.

(b) Learning phase 3 & transmission phase.

Fig. 8 shows the DVB-S2 transmitter decomposition in
tasks and pipeline stages. Intrinsically sequential tasks are
represented by plain boxes. The DVB-S2 transmitter has been
implemented in software with the proposed DSEL. Out of
conciseness, it is not detailed in this paper as it is much
more simpler than the receiver part of the system in terms of
computational requirement and complexity of the tasks graph.

B. Receiver Software Implementation

Fig. 9 presents the task decomposition of the DVB-S2
receiver software implementation with the ﬁve distinct phases.
The plain tasks are intrinsically sequential and cannot be
duplicated. The ﬁrst phase is called the waiting phase (see
Fig. 9a). It consists in waiting until a transmitter starts to
transmit. The Synchronizer Frame task (tRx
8 ) possesses a frame
detection criterion. When a signal is detected, the learning
phase 1 (see Fig. 9a) is executed during 150 frames. After
that the learning phase 2 (see Fig. 9a) is also executed during
150 frames. After the learning phase 1 and 2,
the tasks
have to be re-bound for the learning phase 3 (see Fig. 9b).
This last learning phase is applied over 200 frames. After
the 500 frames of these successive learning phases, the ﬁnal
transmission phase is established (see Fig. 9b).

In a real life communication systems, the internal clocks
of the radios can drift slightly. A speciﬁc processing has to
be added in order to be resilient. This is achieved by the
Synchronizer Timing tasks (tRx
6 ). Similarly, the radio
transmitter frequency does not perfectly match the receiver
frequency, so the Synchronizer Frequency tasks (tRx
10 and
tRx
11) recalibrate the signal to recover the transmitted symbols.

5 and tRx

3 , tRx

Finally LDPC decoder is a block coding scheme that requires
to know precisely the ﬁrst and last bits of the codeword.
The Synchronizer Frame task (tRx
8 ) uses the PLH and pilots
bits inserted by the transmitter to recover the ﬁrst and last
symbols. The Synchronizer Timing module is composed by two
separated tasks (synchronize or tRx
6 ). This
behavior is different from the other Synchronizer modules. The
synchronize task (tRx
3,4,5) has two output sockets, one for
the regular data and another one for a mask. The regular data
and the mask are then used by the extract task (tRx
5 ) to screen
which data is selected for the next task. The Synchronizer
Timing tasks (tRx
6 ) have a high latency compared to
the others tasks, thus splitting the treatment in two tasks is a
way to increase the throughput of the pipeline.

5 and extract or tRx

5 and tRx

5 or tRx

Besides in some cases the task does not have enough
samples to produce a frame. In such cases, the extract task
raises an abort exception. The exception is caught and the
sequence restarts from the ﬁrst task (tRx

1 ).

During the waiting and learning phases 1 and 2, the Syn-
chronizer Freq. Coarse, the Filter Matched and a part of
the Synchronizer Timing have to work symbol by symbol.
They have been grouped in the Synchronizer Pilot Feedback
task (tRx
3,4,5 also requires a feedback input from the
Synchronizer Frame task (tRx
8 ). This behavior is no longer
necessary in subsequent phases, so the tRx
3,4,5 task has been
split in tRx
5 . Consequently, the feedback from the
tRx
8

second output socket is left unbound.

4 and tRx

3,4,5). tRx

3 , tRx

Fig. 10 shows the frame error rate (FER) decoding per-
formance results of the 3 selected MODCODs. The shapes
represent the channel conditions: Squares stand for a standard

generate(tTx1)scramble(tTx2)encode(tTx3)encode(tTx4)interleave(tTx5)modulate(tTx6)insert(tTx7)scramble(tTx8)filter(tTx9)send(tTx10)SourceBinaryFileScramblerBinaryEncoderBCHEncoderLDPCInterleaverModemPSKFramerPLHScramblerSymbolFilterShapingRadioUSRPStage1Stage2Stage3receive(𝑡Rx1)imultiply(𝑡Rx2)synchronize(𝑡Rx3,4,5)filtersynchronizesynchronizeextract(𝑡Rx6)imultiply(𝑡Rx7)synchronize(𝑡Rx8)USRPRadioMultiplierAGCSynchronizerPilotFeedbackSynchronizerFreq.CoarseFilterMatchedSynchronizerTiming(Gardner)MultiplierAGCSynchronizerFramereceive(𝑡Rx1)imultiply(𝑡Rx2)synchronize(𝑡Rx3)filter(𝑡Rx4)synchronize(𝑡Rx5)extract(𝑡Rx6)imultiply(𝑡Rx7)synchronize(𝑡Rx8)descramble(𝑡Rx9)synchronize(𝑡Rx10)synchronize(𝑡Rx11)remove(𝑡Rx12)estimate(𝑡Rx13)demodulate(𝑡Rx14)deinterleave(𝑡Rx15)decodeSIHO(𝑡Rx16)decodeHIHO(𝑡Rx17)descramble(𝑡Rx18)send(𝑡Rx19)USRPRadioMultiplierAGCSynchronizerFreq.CoarseFilterMatchedSynchronizerTiming(Gardner)MultiplierAGCSynchronizerFrameScramblerSymbolSynchronizerFreq.FineL&RSynchronizerFreq.FineP/FFramerPLHNoiseEstimatorModemPSKInterleaverDecoderLDPCDecoderBCHScramblerBinarySinkBinaryFileStage1Stage2Stage3Stage4Stage5Stage6Stage7Stage8Endoftheacquisitionphase38

TABLE III
TASKS SEQUENTIAL THROUGHPUTS AND LATENCIES OF THE DVB-S2
RECEIVER (TRANSMISSION PHASE, 16288 FRAMES, INTER-FRAME LEVEL
= 16, MODCOD 2, ERROR-FREE SNR ZONE, X86 TARGET) SEQUENTIAL
TASKS ARE REPRESENTED BY BLUE ROWS. THE SLOWEST SEQ. STAGE IS
IN RED WHILE THE SLOWEST OF ALL IS IN ORANGE .

Stages and Tasks

Throughput
(Mb/s)

Latency
(µs)

Time
(%)

Radio - receive (tRx
1 )
Stage 1

Multiplier AGC - imultiply (tRx
2 )
Synch. Freq. Coarse - synchronize (tRx
3 )
Filter Matched - ﬁlter (tRx
4 )
Stage 2

Synch. Timing - synchronize (tRx
5 )
Stage 3

Synch. Timing - extract (tRx
6 )
Multiplier AGC - imultiply (tRx
7 )
Synch. Frame - synchronize (tRx
8 )
Stage 4

Scrambler Symbol - descramble (tRx
9 )
Synch. Freq. Fine L&R - synchronize (tRx
10)
Synch. Freq. Fine P/F - synchronize (tRx
11)
Stage 5

Framer PLH - remove (tRx
12)
Noise Estimator - estimate (tRx
13)
Stage 6

Modem PSK - demodulate (tRx
14)
Interleaver - deinterleave (tRx
15)
Decoder LDPC - decode SIHO (tRx
16)
Decoder BCH - decode HIHO (tRx
17)
Scrambler Binary - descramble (tRx
18)
Stage 7

Sink Binary File - send (tRx
19)
Stage 8

431.83
431.83

367.45
841.32
116.41
80.00

55.42
55.42

281.83
685.51
159.41
88.65

1682.89
1246.85
112.56
97.27

1008.60
550.06
355.94

40.47
1347.25
164.21
6.92
91.11
5.35

1838.31
1838.31

527.32
527.32

619.71
270.66
1956.08
2846.45

4108.52
4108.52

807.97
332.18
1428.51
2568.66

135.31
182.63
2022.98
2340.92

225.77
413.98
639.75

5626.34
169.02
1386.74
32905.37
2499.41
42586.88

123.87
123.87

0.94
0.94

1.11
0.48
3.49
5.08

7.34
7.34

1.44
0.59
2.55
4.58

0.24
0.33
3.61
4.18

0.40
0.74
1.14

10.05
0.30
2.48
58.79
4.47
76.09

0.22
0.22

Total

4.09

55742.37

99.57

2.70 Ghz 8168 CPUs, 24 cores 128GB RAM (denoted as x86).
Turbo Boost mode has been disabled for the reproducibility
of the experiment results. Each core is powered by AVX-
512F SIMD ISA. The second architecture is composed by two
Cavium ThunderX2® 2.00 GHz CN9975 v2.1 CPUs, 28 cores,
256 GB of RAM (denoted as ARM). Each core is powered by
NEON SIMD ISA. In the proposed implementation, the data
are represented by 32-bit ﬂoating-point numbers. Data paral-
lelism level is thus 16 for AVX-512F ISA and 4 for NEON
ISA. For both targets, the GNU C++ compiler version 9.3 has
been used with the following ﬂags: -O3 -march=native.
An high performance LDPC decoder implementation with
the inter-frame SIMD technique is used (the early termination
criterion has been switched on). This choice has the effect
of computing sixteen/four frames at once in each task of the
receiver (depending on the x86 or ARM target). It negatively
latency of the system (by a factor of
affects the overall
sixteen/four). But it is not important in the video streaming
targeted application. The Decoder LDPC task (tRx
16) is the only
one in the receiver to take advantage of the inter-frame SIMD
technique. The other tasks simply process sixteen/four frames
sequentially.

Tab. III presents the tasks throughputs and latencies mea-
sured for a sequential execution of the MODCOD 2 in the
transmission phase (x86 target). The tasks have been regrouped
per stage in order to introduce the future decomposition when

Fig. 10. DVB-S2 frame error rate (FER) decoding performance (LDPC BP
h-layered, min-sum, 10 ite.).

simulated additive white Gaussian noise (AWGN) channel,
triangles are a simulated AWGN channel in which frequency
shift, phase shift and symbol delay have been taken into
account, circles are the real conditions measured performances
with the USRPs. There is a 0.2 dB inaccuracy in the noise
estimated by the tRx
13 task. It is symbolized by the extra hori-
zontal bars over the circles. The MODCOD 1 is represented by
dashed lines, MODCOD 2 by dotted lines and MODCOD 3 by
solid lines. For each MODCOD, the LDPC decoder is based
on the belief propagation algorithm with horizontal layered
scheduling (10 iterations) and with the min-sum node update
rules. Each DVB-S2 conﬁguration has a well-separated SNR
predilection zone.

C. Open Source Integration with AFF3CT Toolbox

The proposed software implementation of the DVB-S2
digital transceiver is open source2. It is described with the
help of the AFF3CT toolbox [59]. AFF3CT is a library
dedicated to the digital communication systems and more
speciﬁcally to the channel decoding algorithms. In this paper,
we extend AFF3CT with the presented DSEL to the SDR
use case while keeping the interoperability, reproducibility
and maintainability philosophy initiated in the toolbox. Some
components are directly used from the AFF3CT library (black
dashed-dotted tasks in Fig. 8 and Fig. 9b) and are optimized
for efﬁciency. For instance, knowing that the LDPC decoding
is one of the most compute intensive task, an existing high
performance SIMD implementation is used, based on the
portable MIPP library [60]. Additional AFF3CT tasks have
been implemented speciﬁcally for this project (blue boxes
in Fig. 8,
9a and 9b). These new tasks mainly address
two areas: signal synchronizations and ﬁlters, and real-time
communications.

D. Evaluation

This section evaluates the receiver part of the system. The
transmitter part as it is not the most compute intensive part
and high throughputs are much more easier to reach. All the
presented results have been obtained on two high-end NUMA
machines. One is composed by two Intel® Xeon™ Platinum

2DVB-S2 digital transceiver repository: https://github.com/aff3ct/dvbs2.

12345678910−410−310−210−1100𝐸𝑏/𝑁0(dB)FrameErrorRateConfig.123AWGNAWGN+Realthe parallelism is applied. The throughputs have been normal-
ized to the number of information bits (K = 14232). This
enables the comparison among all the reported throughputs.

17) that takes 59% of the time. tRx

The stage 7 takes 76% of the time with especially the
Decoder BCH task (tRx
17 should
not take so many time compared to the other tasks. However,
we chose to not spend too much time in optimizing the BCH
decoding process as the stage 7 throughput can easily be
increased with the sequence duplication technique. The second
slower stage in the stage 3. This stage is the main hotspot
of the implemented receiver. The stage 3 contains only one
synchronization task (tRx
5 ). In the current implementation this
task cannot be duplicated (or parallelized) because there is an
internal data dependency with the previous frame (state-full
task). The stage 3 is the real limiting factor of the receiver. If
a machine with an inﬁnite number of cores is considered, the
maximum reachable information throughput is 55.42 Mb/s.

We did not try to parallelize the waiting and the learning
phases. We measured that the whole learning phase (1, 2 and
3) takes about one second. During the learning phase, the
receiver is not fast enough to process the received samples
in real time. To ﬁx this problem, the samples are buffered
in the Radio - receive task (tRx
1 ). Once the learning phase is
done, the transmission phase is parallelized. Thus, the receiver
becomes fast enough to absorb the radio buffer and samples
in real time. During the transmission phase, the receiver is
split into 8 stages as presented in Fig. 9b. This decomposition
has been motivated by the nature of the tasks (sequential
or parallel) and by the sequential measured throughput. The
number of stages has been minimized in order to limit the
pipeline overhead. Consequently, sequential and parallel tasks
have been regrouped in common stages. The slowest sequential
task (tRx
5 ) has been isolated in the dedicated stage 3. The
other sequential stages have been formed to always have a
higher throughput than the stage 3. The sequential throughput
of the stage 7 (5.35 Mb/s) is lower than the throughput of the
stage 3 (55.42 Mb/s). This is why the sequence duplication
has been applied. The stage 7 has been parallelized over 28
threads. This looks overkill but the machine was dedicated to
the DVB-S2 receiver and the throughput of the Decoder LDPC
task (tRx
16) varies depending on the SNR. An early termination
criterion was enabled. When the signal quality is very good,
the Decoder LDPC task runs fast and the threads can spend a
lot of time in waiting. With the passive waiting version of the
adaptor push and pull tasks, the CPU dynamically adapt the
cores charge and energy can be saved. In Tab. III, the presented
Decoder LDPC task throughputs and latencies are optimistic
because we are in a SNR error-free zone. All the threads are
pinned to a single core with the hwloc library. The 28 threads
of the stage 7 are pinned in round-robin between the CPU
sockets. By this way, the memory bandwidth is maximized
thanks to the two NUMA memory banks. The strategy of
the stage 7 parallelism is to maximize the throughput. During
the duplication process (modules clones), the thread pinning
is known and the memory is copied into the right memory
bank (ﬁrst touch policy). All the other pipeline stages (1, 2,
3, 4, 5, 6 and 8) are running on a single thread. Because
of the synchronizations between the pipeline stages (adaptor

9

pushes and pulls), the threads have been pinned on the same
socket. The idea is to minimize the pipeline stage latencies
in maximizing the CPU cache performance. It avoids the
extra-cost of moving the cache data between the sockets. On
the ARM target, the pipeline has been decomposed in 12
sequential stages and 1 parallel stage of 40 threads (stage 7).
The receiver program needs around 1.3 GB of the global
memory when running in sequential while it needs around 30
GB in parallel. The memory usage increases because of the
sequence duplications in the stage 7. The duplication operation
takes about 20 seconds. It is made at the very beginning of
the program (before the waiting phase). It is worth mentioning
that the amount of memory was not a critical resource. So, we
did not try to reduce its overall occupancy.

(a) Data copy (stage throughput is 40 Mb/s).

(b) Copy-less (stage throughput is 55 Mb/s).

Fig. 11. Comparison of the two pipeline implementations in the receiver (x86
target, MODCOD 2).

Fig. 11 presents the repartition of the time in the pipeline
stages on the x86 target (MODCOD 2). The receiver is running
over 35 threads. Fig. 11a shows the pipeline implementation
with data copies. Fig. 11b shows the pipeline implementation
with pointer copies (copy-less). Push wait and Pull wait are
the percentage of time spent in passive or active waiting.
Push copy and Pull copy are the percentage of time spent in
copying the data to and from the adaptors buffers. Standard
tasks is the cumulative percentage of time spent by the tasks
presented in Fig. 9b. In both implementations the pipeline
stage throughput is constraint by the slowest one. In Fig. 11a
the measured throughput per stage is 40 Mb/s whereas in
Fig. 11b the measured throughput is 55 Mb/s. The copy-less
implementation throughput is ≈ 27% higher than the data copy
implementation. Fig. 11a shows that the copy overhead is non-

123456780%20%40%60%80%100%PipelinestagePercentageoftimeinthestageStd.tasksPushwaitPushcopyPullwaitPullcopy123456780%20%40%60%80%100%PipelinestagePercentageoftimeinthestageStandardtasksPushwaitPullwait10

TABLE IV
THROUGHPUTS DEPENDING ON THE SELECTED DVB-S2 CONFIGURATION.

comparable throughput demonstrates the ﬂexibility and the
portability of the proposed framework.

Throughput (Mb/s)

Sequential

Parallel

Latency (ms)

E. Comparison with State-of-the-Art

Conﬁguration

x86

ARM x86

ARM x86

ARM

MODCOD 1
MODCOD 2
MODCOD 3

3.4
4.1
4.0

1.0
1.4
1.1

37
55
80

19
28
42

–
56
–

37
41
51

negligible. A 27% slowdown is directly due to these copies in
the stage 3. It largely justiﬁes the copy-less implementation.
In Fig. 11b and in the stage 3, 100% of time is taken by the
tRx
task. This is also conﬁrmed by the measured throughput
5
(55 Mb/s) which is very close the sequential throughput (55.42
Mb/s) reported in Tab. III.

Tab. IV summarizes sequential and parallel throughputs for
the 3 MODCODs presented in Tab. II. To measure the maxi-
mum achievable throughput, the USRP modules are removed
and samples are read from a binary ﬁle. This is because the
pipeline stages are naturally adapting to the slowest one. It
means that in a real communication, the throughput of the
radio is always conﬁgured to be just a little bit slower than
the slowest stage. Otherwise the radio task has to indeﬁnitely
buffer samples even though the amount of available memory
in the machine is not inﬁnite. The information throughput (K
bits) is the ﬁnal useful throughput for the user. Between the
MODCOD 1 and 2, only the LDPC code rate varies (R = 3/5
and R = 8/9 resp.). In the parallel implementation, it has
a direct impact on the information throughput. Between the
MODCOD 2 and 3, the modulation varies (QPSK and 8-PSK
resp.) and the frames have to be deinterleaved (column/row
interleaver). High order modulation reduces the amount of
samples processed in the Synchronizer Timing task (tRx
5 ): this
results in higher throughput (80 Mb/s for the 8-PSK) in the
slowest stage 3. In the parallel implementation, the pipeline
stage throughputs are adapting to the slowest stage 3. It results
in an important speedup. In the sequential implementation, it
results in a little slowdown. Indeed, the additional time spent
in the deinterleave task (tRx
15) is higher than the time saved in
the Synchronizer Timing task (tRx

5 ).
These results demonstrate the beneﬁt of our parallel im-
plementation. Throughput speedups range from 10 to 20
compared to the sequential
implementation. Selected con-
ﬁgurations each are most efﬁcient in different SNR zones
(as shown in Fig. 10), depending on the signal quality. For
instance, MODCOD 1 is adapted for noisy environments (3
dB). However the information throughput is limited to 37
Mb/s (x86 target). MODCOD 3 is more adapted to clearer
signal conditions (7.5 dB) and the information throughput
reaches 80 Mb/s (x86 target). MODCOD 2 is in-between. The
throughputs obtained on the ARM target are lower than on
the x86 CPUs (by a factor of ≈ 2 when running in parallel).
It can be explained by the limited mono-core performance
of the ThunderX2 architecture: the frequency is lower (2.0
GHz versus 2.7 GHz) and the SIMD width is smaller (128-
bit in NEON versus 512-bit in AVX-512F). However, being
able to run the transceiver on both x86 and ARM CPUs with

a) gr-dvbs2rx: As we said before, to the best of our
knowledge, it is the faster open source implementation at the
time of the writing of the paper. gr-dvbs2rx has been run on
the same x86 target presented before (with the same compiler
and options) and on the MODCOD 2. We ran the code without
the radios, this way only the software part of the receiver is
evaluated. First, a set of IQs have been generated from the
emitter and written on a ﬁle. Then, the receiver has been
executed on it. The set of IQs have been read from the same
ﬁle. The evaluation has been made on error-free SNR zone.
To make a fair comparison, we modiﬁed a little bit the source
code of the receiver to remove “artiﬁcial blocks” that slowed
down the throughput. The throttle block has been removed
as well as some useless (and not optimized) blocks dedicated
to the conversion of the IQs (from 8-bit ﬁxed-point to 32-
bit ﬂoating-point format). The source code modiﬁcations we
made are available on a fork of the project3.

TABLE V
GR-DVBS2RX THROUGHPUTS PER PIPELINE STAGE COMPARED WITH THIS
WORK (ERROR-FREE SNR ZONE, X86 TARGET, MODCOD 2). SAME
COLOR CODES AS IN TAB. III.

GNU Radio

Stage

Block

1
2
3
4
5
6
7
8
9
10

ﬁle source
agc cc
symbol sync cc
rotator cc
plsync cc
ldpc decoder cb
bch decoder bb
bbdescrambler bb
bbdeheader bb
ﬁle sink

Equi.

(tRx

{i})

1
2
4-6
7
3,8-13
14-16
17
18
–
19

Throughput (Mb/s)

gr-dvbs2rx

This work

100.7
24.0
16.9
96.1
45.3
23.0
14.9
225.8
252.5
346.5

431.8
367.5
33.1
685.5
48.4
31.7
6.9
91.1
–
1838.3

Tab. V presents the per block normalized throughputs of
gr-dvbs2rx and of this work (considering the gr-dvbs2rx
pipeline decomposition). For each GNU Radio block the tasks
equivalence with our system is given. We measured an overall
information throughput of 14.9 Mb/s. As GNU Radio pins
each block to a thread, the throughput performance is driven
by the slowest block (here bch decoder bb). gr-dvbs2rx uses
10 threads (same as the number of stages). If we applied the
same decomposition without the duplication of the stage 7
our receiver will have been limited to the throughput of the
BCH decoder (6.9 Mb/s). With the duplication technique the
stage 7 is automatically dispatched on multiple threads and
its throughput is approximately multiplied by the number of
threads. This automatic transformation is not possible with
the GNU Radio implementation. Moreover, still if we only
consider the gr-dvbs2rx pipeline decomposition, our work
will have been limited to the lowest sequential throughput

3gr-dvbs2rx fork (thr benchmark branch): github.com/kouchy/gr-dvbs2rx/

which is 33.1 Mb/s (symbol sync cc block). Thanks to a ﬁner
decomposition into tasks (symbol sync cc = tRx
5 + tRx
6 )
our receiver is able to reach 55 Mb/s (see Tab. IV). As a
result, the throughput of the proposed implementation is 3.7
times faster than gr-dvbs2rx.

4 + tRx

b) Grayver and Utter: On a comparable CPU, we es-
timated that their work is able to double or even triple the
throughput of our implementation. This is mainly due to new
algorithmic improvements in the synchronization tasks. For
instance, they were able to express more parallelism in the
Synchronizer Timing task (tRx
5 ). However, we also tried some
aggressive optimizations in this task but we never succeeded
to measure the same level of FER decoding performance.
It could be interesting to check for any penalty in terms
of decoding performance that may occur and to combine
their optimizations with our DSEL. Unlike our work, their
work focuses on a single DVB-S2 conﬁguration (8-PSK,
N = 64800 and R = 1/2) and a single architecture (x86).
Their implementation looks like an hard-coded solution for
the DVB-S2 standard while our goal is to provide generic
methods and tools for SDR system implementations.

VI. CONCLUSION

In this article, we introduced a new DSEL designed to
satisfy SDR needs in terms of expressiveness and performance
on multicore processors. It allows the deﬁnition of stateful
tasks, early exits and dynamic control. We evaluated it on
micro-benchmarks and showed that its scheduling overhead
is negligible for tasks longer than 4 µs. We evaluated a
full software implementation of the DVB-S2 standard built
with our DSEL and the AFF3CT library for tasks. This
implementation is the fastest open source software solution on
multicore CPUs. It matches satellite real time constraints (30
- 50 Mb/s), which demonstrates the relevance and efﬁciency
of the DSEL. This is the consequence of two main factors:
1) the low overhead achieved by DSEL, 2) an efﬁcient
implementation of the pipeline technique, where one stage,
parallelized, reaches saturation. In future works, we plan to
integrate the parallel features of the DSEL with high level
languages such as Python or MATLAB® typically used in
the signal processing community, often less familiar with
the C++ language. Moreover, automatic parallelization, tuning
of pipelining stages, could be investigated. A proﬁle-guided
optimization, capturing task runtime, has been used to tune
pipeline stages, for instance. A more automatic and integrated
approach could be possible since the analysis of the task graph
is dynamic.

REFERENCES

[1] M. Palkovic, P. Raghavan, M. Li, A. Dejonghe, L. Van der Perre,
and F. Catthoor, “Future software-deﬁned radio platforms and mapping
ﬂows,” IEEE Signal Processing Magazine, vol. 27, no. 2, pp. 22–33,
Mar. 2010.

[2] M. Palkovic, J. Declerck, P. Avasare, M. Glassee, A. Dewilde, P. Ragha-
van, A. Dejonghe, and L. Van der Perre, “DART - a high level software-
deﬁned radio platform model for developing the run-time controller,”
Springer Journal of Signal Processing Systems (JSPS), vol. 69, pp. 317–
327, Mar. 2012.

11

[3] ETSI, “3GPP - TS 38.212 - Multiplexing and channel coding (R.

15),” Aug. 2018. [Online]. Available: https://www.etsi.org/deliver/etsi
ts/138200 138299/138212/15.02.00 60/ts 138212v150200p.pdf

[4] P. Rost, C. J. Bernardos, A. D. Domenico, M. D. Girolamo, M. Lalam,
A. Maeder, D. Sabella, and D. W¨ubben, “Cloud technologies for ﬂexible
5G radio access networks,” IEEE Communications Magazine, vol. 52,
no. 5, pp. 68–76, May 2014.

[5] J. Mitola, “Software radios: Survey, critical evaluation and future direc-
tions,” IEEE Aerospace and Electronic Systems Magazine, vol. 8, no. 4,
pp. 25–36, Apr. 1993.

[6] R. Akeela and B. Dezfouli, “Software-deﬁned radios: Architecture, state-
of-the-art, and challenges,” ACM Computer Communications, vol. 128,
pp. 106–125, 2018.

[7] P. Coulton and D. Carline, “An SDR inspired design for the FPGA im-
plementation of 802.11a baseband system,” in International Symposium
on Consumer Electronics (ISCE).

IEEE, Sep. 2004, pp. 470–475.

[8] K. Skey, J. Bradley, and K. Wagner, “A reuse approach for FPGA-based
SDR waveforms,” in Military Communications Conference (MILCOM).
IEEE, Oct. 2006, pp. 1–7.

[9] P. Dutta, Y. Kuo, A. Ledeczi, T. Schmid, and P. Volgyesi, “Putting the
software radio on a low-calorie diet,” in Workshop on Hot Topics in
Networks (HotNets). ACM, 2010.

[10] S. Shaik and S. Angadi, “Architecture and component selection for SDR
applications,” International Journal of Engineering Trends and Tech-
nology (IJETT), vol. 4, no. 4, pp. 691–694, 2013. [Online]. Available:
http://www.ijettjournal.org/volume-4/issue-4/IJETT-V4I4P236.pdf
[11] M. R. Maheshwarappa, M. Bowyer, and C. P. Bridges, “Software deﬁned
radio (SDR) architecture to support multi-satellite communications,” in
Aerospace Conference (AeroConf).

IEEE, Mar. 2015, pp. 1–10.

[12] R. Nivin, J. S. Rani, and P. Vidhya, “Design and hardware imple-
mentation of reconﬁgurable nano satellite communication system using
FPGA based SDR for FM/FSK demodulation and BPSK modulation,”
in International Conference on Communication Systems and Networks
(ComNet).

IEEE, Jul. 2016, pp. 1–6.

[13] G. Kaur and V. Raj, “Multirate digital signal processing for software de-
ﬁned radio (SDR) technology,” in International Conference on Emerging
Trends in Engineering and Technology (ICETET).
IEEE, Jul. 2008, pp.
110–115.

[14] A. Karlsson, J. Sohl, J. Wang, and D. Liu, “ePUMA: A unique memory
access based parallel DSP processor for SDR and CR,” in Global
Conference on Signal and Information Processing (GlobalSIP).
IEEE,
Dec. 2013, pp. 1234–1237.

[15] D. R. N. Yoge and N. Chandrachoodan, “GPU implementation of a
programmable turbo decoder for software deﬁned radio applications,”
in International Conference on VLSI Design.
IEEE, Jan. 2012, pp.
149–154.

[16] S. Bang, C. Ahn, Y. Jin, S. Choi, J. Glossner, and S. Ahn, “Implemen-
tation of LTE system on an SDR platform using CUDA and UHD,”
Springer Journal of Analog Integrated Circuits and Signal Processing
(AICSP), vol. 78, no. 3, p. 599, Mar. 2014.

[17] S. Meshram and N. Kolhare, “The advent software deﬁned radio: FM
receiver with RTL SDR and GNU radio,” in International Conference on
Smart Systems and Inventive Technology (ICSSIT).
IEEE, Nov. 2019,
pp. 230–235.

[18] E. Grayver and A. Utter, “Extreme software deﬁned radio – GHz in real

time,” in Aerospace Conference (AeroConf).

IEEE, Mar. 2020.

[19] J. Xianjun, C. Canfeng, P. J¨a¨askel¨ainen, V. Guzma, and H. Berg,
“A 122Mb/s turbo decoder using a mid-range GPU,” in International
Wireless Communications and Mobile Computing Conference (IWCMC).
IEEE, Jul. 2013, pp. 1090–1094.

[20] R. Li, Y. Dou, J. Xu, X. Niu, and S. Ni, “An efﬁcient parallel SOVA-
based turbo decoder for software deﬁned radio on GPU,” IEICE Trans-
actions on Fundamentals of Electronics, Communications and Computer
Sciences, vol. 97, no. 5, pp. 1027–1036, 2014.

[21] B. Le Gal, C. J´ego, and J. Crenne, “A high throughput efﬁcient approach
for decoding LDPC codes onto GPU devices,” IEEE Embedded Systems
Letters (ESL), vol. 6, no. 2, pp. 29–32, Jun. 2014.

[22] P. Giard, G. Sarkis, C. Leroux, C. Thibeault, and W. J. Gross, “Low-
latency software polar decoders,” Springer Journal of Signal Processing
Systems (JSPS), vol. 90, pp. 761–775, Jul. 2016.

[23] S. Keskin and T. Kocak, “GPU accelerated gigabit level BCH and LDPC
concatenated coding system,” in High Performance Extreme Computing
Conference (HPEC).
IEEE, Sep. 2017, pp. 1–4.

[24] G. Sarkis, P. Giard, C. Thibeault, and W. J. Gross, “Autogenerating soft-
ware polar decoders,” in Global Conference on Signal and Information
Processing (GlobalSIP).

IEEE, Dec. 2014, pp. 6–10.

12

heterogeneous MPSoCs,” ACM Transactions on Architecture and Code
Optimization (TACO), vol. 13, no. 2, Jun. 2016.

[48] K. Tan, H. Liu, J. Zhang, Y. Zhang, J. Fang, and G. M. Voelker,
“Sora: High-performance software radio using general-purpose multi-
core processors,” ACM Communications, vol. 54, no. 1, p. 99–107, Jan.
2011.

[49] R. Li, Y. Dou, J. Zhou, L. Deng, and S. Wang, “Cusora: Real-time
software radio using multi-core graphics processing unit,” Elsevier
Journal of Systems Architecture (JSA), vol. 60, no. 3, pp. 280–
292, 2014. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S1383762113002002

[50] T. Hussain, M. Khan, M. U. Rehman, W. Akram, K. Anayat, A. Arshad,
A. Akbar, and A. Habib, “A high performance software deﬁned radio
system architecture and development environment for a wide range of
applications,” in International Conference on Computing, Mathematics
and Engineering Technologies (iCoMET).
IEEE, March 2018, pp. 1–5.
[51] T. Rondeau, J. Blum, J. Corgan, S. Koslowski, E. Blossom, M. M¨uller,
T. O’Shea, B. Reynwar, M. Dickens, A. Rode, R. Economos, M. Braun
et al. (2006) GNURadio: the free and open software radio ecosystem.
[Online]. Available: https://github.com/gnuradio/gnuradio

[52] B. Bloessl, M. M¨uller, and M. Hollick, “Benchmarking and proﬁling
the GNU radio scheduler,” in GNU Radio Conference (GRCon), vol. 4,
no. 1, 2019. [Online]. Available: https://pubs.gnuradio.org/index.php/
grcon/article/view/64

breaking

[53] M. M¨uller, “How to evolve the GNU radio scheduler - embracing
Software
in Free
Source
and
Developers’ European Meeting (FOSDEM), Feb. 2020.
[Online].
Available: https://archive.fosdem.org/2020/schedule/event/fsr how to
evolve the gnu radio scheduler/

and Open

legacy,”

[54] pabr. (2016) leansdr: Lightweight, portable software-deﬁned radio.

[Online]. Available: https://github.com/pabr/leansdr

[55] W. Ryan and S. Lin, Channel Codes: Classical and Modern. Cambridge

University Press, Sep. 2009.

[56] I. Freire, R. Economos, and A. Inan. (2022) gr-dvbs2rx: GNU radio
extensions for the DVB-S2 and DVB-T2 standards. [Online]. Available:
https://github.com/igorauad/gr-dvbs2rx

[57] F. Broquedis, J. Clet-Ortega, S. Moreaud, N. Furmento, B. Goglin,
G. Mercier, S. Thibault, and R. Namyst, “hwloc: A generic framework
for managing hardware afﬁnities in HPC applications,” in Euromicro
Conference on Parallel, Distributed and Network-based Processing
(PDP).

IEEE, Feb. 2010, pp. 180–186.

[58] ETSI, “EN 302 307 - digital video broadcasting (DVB); second
generation framing structure, channel coding and modulation systems
interactive services, news gathering and other
for broadcasting,
broadband satellite applications (DVB-S2),” Mar. 2005.
[Online].
Available: https://www.etsi.org/deliver/etsi en/302300 302399/302307/
01.02.01 60/en 302307v010201p.pdf

[59] A. Cassagne, O. Hartmann, M. L´eonardon, K. He, C. Leroux, R. Tajan,
O. Aumage, D. Barthou, T. Tonnellier, V. Pignoly, B. Le Gal, and
C. J´ego, “AFF3CT: A fast forward error correction toolbox!” Elsevier
SoftwareX, vol. 10, p. 100345, Oct. 2019.

[60] A. Cassagne, O. Aumage, D. Barthou, C. Leroux, and C. J´ego, “MIPP:
A portable C++ SIMD wrapper and its use for error correction coding
in 5G standard,” in Workshop on Programming Models for SIMD/Vector
Processing (WPMVP). V¨osendorf/Wien, Austria: ACM, Feb. 2018.

[25] B. Le Gal, C. Leroux, and C. J´ego, “Multi-Gb/s software decoding of
polar codes,” IEEE Transactions on Signal Processing (TSP), vol. 63,
no. 2, pp. 349–359, Jan. 2015.

[26] A. Cassagne, B. Le Gal, C. Leroux, O. Aumage, and D. Barthou,
“An efﬁcient, portable and generic library for successive cancellation
decoding of polar codes,” in International Workshop on Languages and
Compilers for Parallel Computing (LCPC). Springer, Sep. 2015.
[27] G. Sarkis, P. Giard, A. Vardy, C. Thibeault, and W. J. Gross, “Fast
list decoders for polar codes,” IEEE Journal on Selected Areas in
Communications (JSAC), vol. 34, no. 2, pp. 318–328, Feb. 2016.
[28] A. Cassagne, T. Tonnellier, C. Leroux, B. Le Gal, O. Aumage, and
D. Barthou, “Beyond Gbps turbo decoder on multi-core CPUs,” in
International Symposium on Turbo Codes and Iterative Information
Processing (ISTC).

IEEE, Sep. 2016, pp. 136–140.

[29] A. Cassagne, O. Aumage, C. Leroux, D. Barthou, and B. Le Gal,
“Energy consumption analysis of software polar decoders on low power
processors,” in European Signal Processing Conference (EUSIPCO).
IEEE, Aug. 2016, pp. 642–646.

[30] B. Le Gal and C. J´ego, “High-throughput multi-core LDPC decoders
based on x86 processor,” IEEE Transactions on Parallel and Distributed
Systems (TPDS), vol. 27, no. 5, pp. 1373–1386, May 2016.

[31] ——, “Low-latency software LDPC decoders for x86 multi-core de-
vices,” in International Workshop on Signal Processing Systems (SiPS).
IEEE, Oct. 2017, pp. 1–6.

[32] M. L´eonardon, A. Cassagne, C. Leroux, C. J´ego, L.-P. Hamelin, and
Y. Savaria, “Fast and ﬂexible software polar list decoders,” Springer
Journal of Signal Processing Systems (JSPS), vol. 91, pp. 937–952, Jan.
2019.

[33] B. Le Gal and C. J´ego, “Low-latency and high-throughput software turbo
decoders on multi-core architectures,” Springer Annals of Telecommu-
nications, vol. 75, pp. 27–42, Aug. 2019.

[34] J. Dennis, “Data ﬂow supercomputers,” IEEE Computer, vol. 13, no. 11,

pp. 48–56, Nov. 1980.

[35] W. Ackerman, “Data ﬂow languages,” IEEE Computer, vol. 15, pp. 15–

25, Feb. 1982.

[36] E. A. Lee and D. G. Messerschmitt, “Static scheduling of synchronous
data ﬂow programs for digital signal processing,” IEEE Transactions on
Computers (TC), vol. C-36, no. 1, pp. 24–35, Jan. 1987.

[37] M. Engels, G. Bilsen, R. Lauwereins, and J. A. Peperstraete, “Cycle-
static dataﬂow: Model and implementation,” in Asilomar Conference on
Signals, Systems, and Computers (ACSSC), vol. 1.
IEEE, Oct. 1994,
pp. 503–507.

[38] G. Bilsen, M. Engels, R. Lauwereins, and J. A. Peperstraete, “Cyclo-
static data ﬂow,” in International Conference on Acoustics, Speech and
Signal Processing (ICASSP), vol. 5.
IEEE, May 1995, pp. 3255–3258.
[39] T. M. Parks, J. L. Pino, and E. A. Lee, “A comparison of synchronous
and cycle-static dataﬂow,” in Asilomar Conference on Signals, Systems,
and Computers (ACSSC), vol. 1.

IEEE, Oct. 1995, pp. 204–210.

[40] I. Buck, T. Foley, D. Horn, J. Sugerman, K. Fatahalian, M. Houston,
and P. Hanrahan, “Brook for GPUs: Stream computing on graphics
hardware,” ACM Transactions on Graphics (TOG), vol. 23, no. 3, pp.
777–786, Aug. 2004.

[41] S. Amarasinghe, M. l. Gordon, M. Karczmarek, J. Lin, D. Maze, R. M.
Rabbah, and W. Thies, “Language and compiler design for streaming
applications,” Springer International Journal of Parallel Programming
(IJPP), vol. 2, no. 33, pp. 261–278, Jun. 2005.

[42] S.-W. Liao, Z. Du, G. Wu, and G.-Y. Lueh, “Data and computation
transformations for brook streaming applications on multiprocessors,” in
International Symposium on Code Generation and Optimization (CGO).
IEEE, Mar. 2006, pp. 207–219.

[43] D. Black-Schaffer and W. J. Dally, “Block-parallel programming for
real-time embedded applications,” in International Conference on Par-
allel Processing (ICPP).

IEEE, Sep. 2010, pp. 297–306.

[44] C. Glitia, P. Dumont, and P. Boulet, “Array-OL with delays, a domain
speciﬁc speciﬁcation language for multidimensional
intensive signal
processing,” Springer Multidimensional Systems and Signal Processing,
no. 21, pp. 105–131, Mar. 2010.

[45] W. Thies and S. Amarasinghe, “An empirical characterization of stream
programs and its implications for language and compiler design,” in
International Conference on Parallel Architectures and Compilation
Techniques (PACT). ACM/IEEE, Sep. 2010, pp. 365–376.

[46] P. De Oliveira Castro, S. Louise, and D. Barthou, “DSL stream pro-
gramming on multicore architectures,” in Programming Multi-core and
Many-core Computing Systems.

John Wiley and Sons, 2017, ch. 7.

[47] M. Dardaillon, K. Marquet, T. Risset, J. Martin, and H.-P. Charles,
“A new compilation ﬂow for software-deﬁned radio applications on

