2
2
0
2

r
p
A
7
2

]

Y
C
.
s
c
[

1
v
8
5
0
1
0
.
5
0
2
2
:
v
i
X
r
a

Electronic Laboratory Notebook: A lazy approach

Paper Authors

1. Schubotz, Simon 1,2 (schubotz@ipfdd.de);
2. Schubotz, Moritz 3;
3. Auernhammer, G¨unter K. 1 (auernhammer@ipfdd.de);

Paper Author Roles and Aﬃliations

1 Leibniz-Institut f¨ur Polymerforschung Dresden e.V., Hohe Straße 6, Dresden,
01069, Germany
2 Technische Universit¨at Dresden, Helmholtztraße 10, Dresden, 01062, Germany
3 FIZ Karlsruhe, Franklinstraße 11, Berlin, 10587, Germany

Abstract

Good research data management is essential in modern-day lab work. Various
solutions exist that are either highly speciﬁc or need a signiﬁcant eﬀort to be
customized appropriately. This paper presents an integrated solution for indi-
viduals and small groups of researchers in data-driven deductive research. Our
electronic lab book generates itself out of notes and ﬁles, which are generated by
one or several experiments. The generated electronic lab book is then presented
on a Django-based website. The automated gathering of metadata signiﬁcantly
reduces the documentation eﬀort for the lab worker and prevents human error
in the repetitive task of manually entering basic meta-data. The skilled user can
quickly adapt the electronic lab book software to his needs because the software
employs widely used open-source software libraries with active communities and
excellent documentation.

Keywords

Electronic Laboratory Notebook, Data management, Research data manage-
ment, Scientiﬁc software, Automated data processing

Introduction

Good research data management should primarily allow the researcher to navi-
gate her or his data easily. Additionally, it is an essential prerequisite to archiv-
ing and later potential reuse of the data. Large-scale experiments like Cern [1]
or KaTriN [5] have signiﬁcant capabilities in storing and processing data in a
streamlined way. For small-scale experiments, these capabilities are limited.

1

 
 
 
 
 
 
Figure 1: When performing multiple experiments on multiple samples, much data
is produced. This data has to be structured, analyzed, and potentially linked with
some notes or other data. In our approach, we try to streamline this process.

Many scientists still rely on handwritten notes. For the analysis, the notes have
to be manually linked to the data.

However, there are approaches for electronic lab books (also often Electronic
Laboratory Notebook, short ”lab book” in this work), but those are often highly
specialized [2, 4, 8]. However, there are approaches for electronic lab books
(also often Electronic Laboratory Notebook, short ”lab book” in this work),
but those are often highly specialized. Another issue is that it is diﬃcult for
users to modify lab books because they do not use standard software packages.
Those circumstances make it hard to adapt those lab books to speciﬁc needs.
The adaption of the lab book enables a very tight bonding to experiments,
reducing the amount of tedious documentation work. Particularly for experi-
ments without a built-in data management system, this is useful. For example,
in a self-build experimental setup, data management could be done with the lab
book, resulting in a better user experience and possibly reducing the licensing
fees by using our open-source lab book.

One of the key diﬀerences to other lab books, e.g., those mentioned above, is
that we create the lab book directly from the primary data (primarily measure-
ment or simulation data of experiments). The primary data has to be saved in
a particular ﬁle structure from which we can already retrieve some metadata.
The automated retrieval of metadata signiﬁcantly reduces the amount of work,
especially if many ﬁles have to be treated separately.

Some experiments have multiple data streams. We call the data from the
main device ”main data” and data from supporting devices ”sub data” (Fig-
ure 1). For example, one could try to measure the optical properties of a sample
at diﬀerent pressures. In this case, the main data would be the optical data and
the sub data the pressure. The entries of the main- and sub experiments (and
parts of the metadata) can then be automatically generated from the data ﬁles.
This way, we can handle multiple data streams from the main experiment and

2

Figure 2: Interactive representation of some experimental data.

form the supporting experiment, which are generated by multiple independent
devices. In addition, it is always possible to manually add or modify data and
its metadata.

In our research, we are investigating diﬀerent samples of polymer brushes
with diﬀerent properties, like diﬀerent polymer chain length. By performing
experiments on the sample, the properties of the sample, like its wetting prop-
erties, can be changed [7]. For the data interpretation, it is important to keep
track of the history of the samples, i.e., which experiments were performed on
the sample at which time. With this lab book, we can ﬁlter after one sample and
see the samples‘ history, helping us understand why the sample’s properties have
been changed. Currently, our research produces several thousand of these main
data entries per year. To create all these entries in a database manually would
require a lot of time. For a systematic analysis of the experiments, the possibil-
ity to ﬁlter is essential, e.g., ﬁltering according to experimental parameters, the
experiment type, the sample, observations, etc.

From our research, we deduce the following required features of the lab book.

1. It should be easy and fast to load hundreds or even thousands of ﬁles into

the database if they are saved in a structured way.

2. Metadata should be deduced from the structure the ﬁle is saved in and

added automatically to the database.

3. Files that are related should be automatically linked in the database, e.g.,

main and sub data.

4. There should be a report about the ﬁles that are automatically loaded and

linked.

3

Figure 3: Overview of all experiments that have been performed.

5. Experiments that produced data, in a regular table format (pandas 1
compatible) should be shown in form of an interactive plot (Figure 2).
The plot should also be correlated to other data like the sub data.

Implementation and architecture

We chose Django as a framework [3], because it is written in python, which is
the most popular programming language at the moment. Our Django program
consists of 5 Apps: Exp Main, Exp Sub, Analysis, Lab Dash, and Lab Misc.

The primary data is linked to the metadata and gets a persistent identiﬁer
(ID) when loading data. Over this ID, the data can be quickly accessed or
presented in an interactive graph. By joining the primary data and the metadata
in a database, the data becomes more accessible because one can ﬁlter after
speciﬁc criteria in the metadata and thus keep the overview Figure 3. The main
data can be further enhanced by linking the ID to the sub data ID and their
analysis results.

After the experiments the gathered data has to be stored. For large amounts
of data it is useful to organize the data in the ﬁlesystem hierarchy. Building up
on this idea a special folder structure is implemented, which contains the date,
sample name, and the experiments name (a detailed example can be found in
reuse potential section). By using this special structure, it is possible to auto-
matically generate database entries out of primary data that contains metadata
like the date, sample, link to the ﬁle, or basic information that is written into
the ﬁle or ﬁlename Figure 4. That saves time for the lab worker because the in-
formation has only to be entered once and is afterward available in a structured
and searchable way. The metadata retrieved from the folder structure acts as
the backbone of the lab book; further metadata can then be added manually or
via scripts.

The entries of the experiments are automatically given a persistent ID, which
users can use to link these experiments to all kinds of other information. For
example, the sub experiment can be correlated to the main experiment via the

1https://pandas.pydata.org/

4

Figure 4: A signiﬁcant part of the lab book is generated out of the ﬁles of the
experiments. Joining the experiments with the notes written into the lab book
during the experiments gives a good overview of what was done. After running
the data analysis, the user can enter the results and the interpretation into the lab
book. Keeping the data, the analysis, and its interpretation linked helps the ﬁnding
of new scientiﬁc results.

timestamp of the data ﬁles. The linkage of these experiments then happens over
the ID of the corresponding entries. Additionally, other links could be estab-
lished, e.g., to link to the analysis results, other main experiments, observations,
etc. This is a far more elegant and ﬂexible way to establish, even multiple, struc-
tures in the whole data than saving experiments in certain folders, where the
folders themself act as the structure. Relevant information during the experi-
ment can be added by a separate entry and can later be easily correlated to the
experiments over the timestamp and sample name.

Django creates the database out of models that have to be deﬁned in Django.
For the entries of the experiments, the model is chosen in the following way. An
experimental base model inherits its properties from all the speciﬁc experimen-
tal models. The data relevant for all experiments, like the date, name, and
additional metadata, are stored in the base model. The experiments’ speciﬁc
data, like wavelength, exposure time, are stored in a speciﬁc experiments model.
Thus, the experiment models are always an extension of the experimental base
model. That approach simpliﬁes the complexity of the lab book a lot.

Individual analysis scripts can enhance the primary data. The primary data
and the analysis data can be related via their IDs. By including the metadata,
the analysis can be very speciﬁc. Results of the analysis can be fed back to
the metadata. Over a jupyter 1 interface, a script-driven communication with
the database and all the Django integrated functions is possible. This simpliﬁes
the development of analysis routines because all web interface features are also
available for jupyter scripts, resulting in high ﬂexibility. The jupyter extension
could also extract speciﬁc data, e.g., plots or tables, which can be handy for
publications.

For a ﬁrst overview, the data that is stored in the Exp Base model is repre-

1https://jupyter.org/

5

Figure 5: Details of the experiment.

sented in a table-styled way (Figure 3). In a detailed view, special links related
to the experiment can be accessed, for example, the interactive plot, the sup-
porting experiments, and more (Figure 5). In the table view, the entries can be
modiﬁed or deleted. Later, we want to couple the modiﬁcation rights to the user
management system that is integrated into Django. This would then prevent
malicious interference with the entries.

Additionally, users that store their data in (private) GitHub repositorys, can
protect their data by a decentralized trusted timestamping (DTT) mechanism.
This happens the OriginStam.org API. A cryptographic one-way hash of the
data is generated and send to the OriginStamp API. This hash is then com-
bined with other hashes and integrated into the BitCoin Blockchain and thus
immutably stored. With this hash, we can prove that we had the data when
the hash was published, which happens once a day. Only publishing the hash
rather than the entire data set prevents others from making use of the data
before the measurement is ﬁnalized, properly peer-reviewed, and adequately de-
scribed. This method is incredibly viable for users unwilling or unable, e.g.,
due to legal restrictions, to publish their entire data set. GitHub has an option
to require signed commits, and OriginStamp can seamlessly be integrated via a
Webhook with almost no eﬀort [6, 9]. The signed commits options ensures that
the authorship of the data can be claimed with a public-private key method.

For users that can not use GitHubs‘ webkook functionality (or the equivalent
fuinctionaltiy in GitLabs), we recommend to combine the user authentication
with the DTT and integrate the hash generation, signing and backup function-
ality into the lab book. However, this is out of scope for us.

Quality control

For the quality control, we used the Django testing framework. Several unit
tests are performed, and also an integration test. On the tested machines, there
were no error messages during testing.

6

(2) Availability

Operating system

Windows 10

Runs with docker, thus also on unix systems. In windows the functionality

is higher but the installation eﬀort is also larger.

Programming language

Python 3.8.2

Dependencies

django, django-tables2, django-bootstrap3, django-ﬁlter, django-tables2-column-
shifter, django-bootstrap-modal-forms, django-widget-tweaks, django plotly dash,
channels, bootstrap4, dash, dash-renderer, dash-html-components, dash-core-
components, plotly, numpy, pandas, daphne, redis django-redis, channels-redis,
django-mptt, xlrd, dbfread, kaleido, django-datatables-view, jupyter, ipython,
django-extensions and scipy

Software location:

Archive
Name: Electronic-Laboratory-Notebook
Persistent identiﬁer: swh:1:dir:c6b704517578897e3a7feba2489fcd6c9548bf7a
Licence: Apache License 2.0
Publisher: Softwareheritage
Date published: 18/08/2021

Code repository

Name: GitHub
Persistent identiﬁer: ag-gipp/Electronic-Laboratory-Notebook
Licence: Apache License 2.0
Date published: 18/08/2021

Language

English

7

(3) Reuse potential

The program can be used by anyone who wants to access their data in a more
structured way. Because it is open-source, the customizability is extensive. One
could even introduce more users and lift it to a server and thus run collaborative
projects. However, this would require some work of the user who is familiar with
Django. Here, we will introduce the ﬁrst steps for single-user customization.

Add new ﬁle to exsisting model and sample

If we want to add a new ﬁle to the lab book, we can copy it to the folder structure.
One example ﬁle is already present in this commit. The ﬁle is saved at 01_Data/
01_Main_Exp/01_OCA_35_XL/20210201/Probe_BA_01/171700_osz_wasser_laengest.
png From this path, the program extracts the information that the measurement
was recorded on the ﬁrst of February at 17:17:00, and the sample BA 01 was
used in the experiment OCA. Due to performance optimizations, the ﬁle will
be ignored if it is older than ﬁve days or has the wrong ﬁle extension. The user
conﬁgures the allowed ﬁle extensions. To add the example, change the date in
the folder structure to yesterday by renaming the according to folder. Navigate
in the header to ”Generate” and click on ”Main” and then on ”Generate en-
tries”. When going back to the experiments, there should be a new entry with
diﬀerent data. By clicking on the eye and then on Video, the data is displayed
as an image. If you have ﬁles that do not have the time written in front of them,
consider using the give ﬁle times module in Exp Main/Generate.py We decided
not always to generate the time automatically to allow manually link the data
using the exact times.

Add a new sample

To add a new sample, navigate to http://127.0.0.1:8000/admin/ and log
in with the username: admin and password: admin Go to Lab Misc and click
on samplebrushpnipaamsi and then add. It is essential to give each sample a
proper name consisting of two letters and two numbers separated by an under-
score, e.g., CC 01. Then click save. Thereafter, you can use this name in the
folder structure and add a sample as described above. It is probably helpful
to deﬁne your own model for your sample, which inherits Sample Blanks. For
further details about models, visit https://docs.djangoproject.com/en/3.
1/topics/db/models/

Add a new experiment

To add a new experiment, navigate to Exp Main/models.py and add your model,
which inherits ExpBase. For a start, you can follow the example of the OCA
model. The model name should consist of 3 capital letters. After the Django
migration, the model should be visible in the admin. To tell the program where
the ﬁles of this model are stored, navigate in the admin to Exp path and create

8

a new entry with the path ﬁle ending and the abbreviation, which consists of
the same three capital letters as in the model.

If you use a Windows system, it can be beneﬁcial to run Django without
docker, since only with Windows is it possible to open local ﬁles directly from
the web browser. If ﬁles can not be accessed directly by the web browser, users
of Unix systems need to copy the link into the local ﬁle browser manually. The
rest works normally.

Support

If support is needed do not hesitate to use the appropriate tools of GitHub.

Funding statement

Simon Schubotz thanks the Deutsche Forschungsgemeinschaft (DFG) for fund-
ing of project 422852551 (AU321/10-1) within the priority program 2171. G¨unter
K. Auernhammer was funded by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation)–Project-ID No. 265191195–SFB 1194-A02.

Competing interests

The authors declare that they have no competing interests.

References

[1] Johannes Albrecht et al. “A Roadmap for HEP Software and Comput-
ing R&D for the 2020s”. In: Computing and Software for Big Science 3.1
(2019). doi: 10.1007/s41781-018-0018-8.

[2] Angela Bauch et al. “openBIS: a ﬂexible framework for managing and an-
alyzing complex data in biology research”. In: BMC Bioinformatics 12.1
(2011), p. 468. doi: 10.1186/1471-2105-12-468.

[3] Torsten Bronger. juliabase. GitHub. url: http://www.juliabase.org.
[4] Nicolas CARPi, Alexander Minges, and Matthieu Piel. “eLabFTW: An
open source laboratory notebook for research labs”. In: The Journal of
Open Source Software 2.12 (2017), p. 146. doi: 10.21105/joss.00146.
[5] S Chilingaryan et al. “Advanced data extraction infrastructure: Web based
system for management of time series data”. In: Journal of Physics: Con-
ference Series 219.4 (2010), p. 042034. doi: 10.1088/1742-6596/219/4/
042034.

[6] Thomas Hepp et al. “OriginStamp: A blockchain-backed system for de-
centralized trusted timestamping”. In: it - Information Technology 60.5-6
(2018), pp. 273–281. doi: 10.1515/itit-2018-0020.

9

[7] Simon Schubotz et al. “Memory eﬀects in polymer brushes showing co-
nonsolvency eﬀects”. In: Advances in Colloid and Interface Science (2021),
p. 102442. doi: 10.1016/j.cis.2021.102442.

[8] Paul Schultze-Motel. “Helmholtz Open Science Workshop ”Elektronische
Laborb¨ucher” : Braunschweig, 13.-14. September 2018 ; Dokumentation”.
en. In: (2019). doi: 10.2312/OS.HELMHOLTZ.001.

[9] Patrick Wortner et al. “Securing the Integrity of Time Series Data in Open
Science Projects using Blockchain-based Trusted Timestamping”. In: Pro-
ceedings of the workshop on Web Archiving and Digital Libraries (WADL)
held in conjunction with the ACM/IEEE Joint Conference on Digital Li-
braries (JCDL). 2019. url: https : / / www . gipp . com / wp - content /
papercite-data/pdf/wortner2019.pdf.

10

