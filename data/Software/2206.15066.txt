Story-thinking, computational-thinking, programming and software engineering

Austen Rainer
School of Electronics,
Electrical Engineering
& Computer Science
Queen’s University Belfast
a.rainer@qub.ac.uk

Catherine Menon
School of Physics,
Engineering
& Computer Science
University of Hertfordshire
c.menon@herts.ac.uk

Abstract
Working with stories and working with computations require very different modes of thought. We call
the ﬁrst mode “story-thinking” and the second ”computational-thinking”. The aim of this curiosity-
driven paper is to explore the nature of these two modes of thinking, and to do so in relation to program-
ming, including software engineering as programming-in-the-large. We suggest that story-thinking and
computational-thinking may be understood as two ways of attending to the world, and that each both
contributes and neglects the world, though in different ways and for different ends. We formulate two
fundamental problems, i.e., the problem of “neglectful representations” and the problem of oppositional
ways of thinking. We brieﬂy suggest two ways in which these problems might be tackled and identify
candidate hypotheses about the current state of the world, one assertion about a possible future state,
and several research questions for future research.

1. Introduction
The term “story” is widely used in software engineering, e.g., the “user story” (Lucassen, Dalpiaz, Van
Der Werf, & Brinkkemper, 2015) and the “job story” (Lucassen et al., 2018). Related concepts are also
used, e.g., the scenario (Carroll, 2003). But story – in the fullest sense of that word – and algorithm are
very different things. We use the term “story-thinking” to refer to the ways in which we conceptualise
and engage with stories, e.g., how we create them, how we formally analyse and evaluate them, and
how and why we appreciate them as readers. We use the term “computational-thinking” to refer to the
ways in which we think about computations, e.g., how we create algorithms, how we formally analyse
and evaluate them, and how we simulate their execution on real or nominal machines. The contrasts
between story-thinking and computational-thinking stimulate a range of questions, e.g., what are the
ways in which story and algorithm might relate? how might story – it’s writing, telling and re-telling
– complement computational thinking? and what do we we gain, and what do we lose, with these
contrasting ways of thinking?

The aim of this curiosity-driven paper is to explore the nature of these two modes of thinking, and to do
so in relation to programming, including software engineering as programming-in-the-large. The paper
contributes the following:

1. A conceptualisation of some of the issues, through the application of these two modes of thinking

to a six word story.

2. The identiﬁcation of two fundamental problems: the problem of “neglectful representations” and

the problem of oppositional ways of thinking.

3. Brief proposals for how these problems might be tackled, including prospective hypotheses and

research questions.

The remainder of the paper is organised as follows. We start our exploration with a thought experiment in
Section 2. In Section 3, we model the thought experiment computationally. In Section 4, we discuss the
concept of “computational-thinking”, addressing what it means to think computationally. In Section 5,
we then consider computational thinking in the context of software engineering, as programming-in-
the-large. This leads into the consideration of requirements engineering, in Section 6, as a presumed

2
2
0
2

n
u
J

0
3

]
L
C
.
s
c
[

1
v
6
6
0
5
1
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
“bridge” between the humanly-meaningful real world and the computational world. In Section 7, we
brieﬂy consider related perspectives on modes of thinking, a neuro-scientiﬁc perspective and a psycho-
logical perspective. In Section 8, we then summarise the problems, propose ways forward, suggest two
hypotheses, one assertion and several research questions. Finally, with Section 9, we brieﬂy conclude.

2. A thought experiment
Consider the following short story (which has been intentionally modiﬁed slightly from a well-known
story in the literary world):

“For sale. Baby’s shoes. Never worn.”

What is your reaction to this story?

In his book, Once upon an if, Worley (2014) describes his wife’s ﬁrst reaction to this story. “Oh, no!” he
says she responded, an indication that she interpreted the story as one of sadness or tragedy, presumably
relating to the life of a baby. As the reader, you might, similar to Worley’s wife, interpret the story
as a tragedy. Or you might, as Worley says his wife’s friend did, interpret the story as about someone
who buys things for others but those things are not wanted. And, of course, you might have other
reactions to the story. In a recent online presentation of this story to software engineering academics,
the audience was asked for their reactions. Responses included, “love, compassion”, “sadness”, “cute”,
and “nostalgic – story made me reﬂect on that episode of my life”.

Furthermore, the reference to a baby means the phrase “never worn” carries different connotations com-
pared to, for example, an adult having “never worn” the shoes. The connotations of a baby’s shoes never
being worn are tragic because babies are seen as more susceptible to, and “ﬁtting” (in story-terms) for,
tragedy. An adult’s “never worn” shoes may imply the shoes have been worn at least once, for example
to try them on, but then not worn again, e.g., perhaps they didn’t ﬁt.

Whatever your interpretation of the story, and your reaction to it, the story-teller (based on a story
allegedly written by Ernest Hemingway, though this is disputed) has provoked an experience and done
so very efﬁciently. In just six words, the story-teller creates characters (e.g., a baby, and possibly a
parent), a plot (e.g., perhaps someone has lost a baby, or not been able to have a baby) and therefore an
unfolding of events over time, one or more goals and a struggle (e.g., the goal of having a baby, with
the struggle of not having a baby, or of surviving the loss of a baby), an outcome (e.g., the goal was not
attained) and an emotional experience for the reader (e.g., sadness).

The entire story is shorter in length than Cohn’s (2004) well-accepted template for a single user story in
requirements engineering: As a <type of user> , I want <goal>, [so that <some
reason>]. One reason we chose this story is because it is concise and therefore easy to present
completely in an academic publication with restrictions on page length. But we also chose this story
because it is efﬁcient: an extraordinary amount of information and emotion is evoked in only a few
words. This efﬁciency contrasts with the user story. The contrast – between the six-word story and
the template for a single user-story – suggests fundamental differences in the way that stories model
the world and the way that typical software engineering and programming constructs model the world;
and also suggests fundamental differences in the ways that story–thinking and computational-thinking
require us, or encourage us, to attend to the world.

3. Representing the story computationally
An alternative way to think about the six-word story is computationally. And it seems that as soon as we
start to attend to the six-word story computationally it is no longer a story but instead becomes a text.
To illustrate this point we present and consider two forms of computational thinking: user stories and
software designs. There are other forms we might consider too, e.g., we might write an algorithm and
instantiate it as a program. We don’t explicitly consider algorithms or programs here for two reasons.
First, due to space; second, because the two forms we do consider here would typically be prerequisites

to then developing the algorithm and the program. We do consider the transformation of representations
later in this paper when we discuss software engineering.

3.1. Representing the story as user stories
One way to approach the thought experiment is to represent the story as user stories. Table 1 presents
examples of possible user stories, ﬁrst re-stating Cohn’s (2004) template for user stories. These user
stories can only be speculative because they depend on how one interprets the six words.

Example

ID
N/A As a <type of user> , I want <goal>, [so that <some reason>]

1
2
3
4

As a grieving mother, I want to sell my baby’s shoes, so that I can reduce my ﬁnancial losses.
As a grieving parent, I want to sell my baby’s clothing, so that I can recover my ﬁnancial losses.
As the purchaser of an unwanted item of clothing, I want to sell the item, so that I can recover my costs.
As a user, I want to be able to sell items, so that I can make some money.

Table 1 – Example user stories for the thought experiment

The examples in Table 1 illustrate some of the tensions, strengths and weaknesses of computational-
thinking and story-thinking, such as:

1. The story tells us very little explicitly about the actual protagonist, other than the protagonist wants
to sell a pair of baby’s shoes. We are left to infer the characteristics of the protagonist, including
the basis of their goal. In terms of story-thinking, this is an effective rhetorical device. In terms
of computational-thinking, this is problematic. There is a tension between story-thinking’s use of
evocative connotation and computational-thinking’s standards of speciﬁcation and denotation.

2. There is also a tension between story-thinking’s particularity and computational-thinking’s ab-
straction. The users, or personas, deﬁned in User Stories 1 through 3 can all be abstracted to the
user deﬁned in User Story 4. Conversely, whilst the emotional aspects of the users in User Stories
1, 2 and 3 (e.g., sadness, nostalgia, not wanting something) might be representable in a software
system, it is not clear what gain there is for the software system with such a representation. Or
in other words, it is not clear how representing such states in the system helps the user with what
they want to do.

This distinction between the particularity and the generality of the story can be further illustrated
by comparing the version of the story presented at the beginning of Section 2 with the original
story. In the original, the story reads, “For sale. Baby shoes. Never worn.” Adding one apostrophe
together with one letter, the letter s, helps to particularise the story.

3. In the act of preparing User Stories to model the story, we begin to reshape our conception of the
story. We specify, and possibly also abstract, and by doing so we change the story, limit it and
reduce its effect as a story.

3.2. Representing the story as a software design
A second way to think computationally about the story is in terms of software designs. Figure 1 presents
a simple UML object diagram (Figure 1a) and a database table (Figure 1b) for the sale of a product, in
this case a pair of baby’s shoes. We can of course think with and about these models, but notice how the
kind of thinking we do with these models is different to the kind of thinking we do with the story.

With the object diagram we can, for example, infer that the object is an instance of the class Product,
and we can see the forSale attribute is a boolean variable. We can also see that no private or public
methods are stated for this object, at least in the diagram, and we might examine the class Product
for such methods. The database table shows that we can easily begin to design a database for persistent
storage of information taken from the story. Being outputs of our thinking, these models provide insights
into the nature of our computational-thinking.

(a) UML object diagram

(b) Database table

Figure 1 – UML object diagram and database table for the sale of a product

And we can, of course, revise the models. We might change the condition attribute to become an
enumerated variable, perhaps using the value of 1 to represent the condition of “Never worn.” An enu-
merated variable would help us implement other features, e.g., with an SQL database, it becomes easier
to select products that are never worn, e.g., SELECT * WHERE Condition == 1. An enumerated
variable also improves the efﬁciency of computational processes, e.g., an if test of a numeric variable
requires less computational resource than an equivalent test of a string value.

Figure 2 – A UML object diagram for an item to advertise

The object model presented in Figure 1 is not the only model that might be constructed. Figure 2
suggests a different object model. Comparing Figure 1 and Figure 2, the same text is present but the
way we have structured the text is different. Our interpretation of the six words leads to different
computational representations and those representations then support different thinking, e.g., we might
enumerate the category attribute and, with the appropriate database design, select only the sale ad-
verts: the SQL statement, SELECT * WHERE category == 1, now means something different.
Though the model has changed, the fundamental nature of the model hasn’t, and the kind of thinking –
computational thinking – hasn’t changed either.

4. Computational-thinking
Having presented and discussed examples of outputs from computational thinking, we turn now to
consider deﬁnitions of computational thinking. We use these deﬁnitions to show how computational-
thinking is fundamentally different to story-thinking.

Aho (2012) deﬁnes computational thinking as, “. . . the thought processes involved in formulating prob-
lems so their solutions can be represented as computational steps and algorithms.” (emphasis added).
Denning (2009) recognises the representation as more fundamental than the algorithm, since the rep-
resentation needs to be computable (Erwig, 2017) or, on other words, manipulatable by or through
computation. Heineman, Pollice, and Selkow (2008) write that, “Designing efﬁcient algorithms often
starts by selecting the proper data structures in which to represent the problem to be solved.” (emphasis
added).

Priami (2007) highlights a particular feature of the representation: “. . . the basic feature of computational
thinking is abstraction [representation] of reality in such a way that the neglected details in the model
make it executable by a machine.” (emphasis added). This concept of negative selection – i.e., of
what is removed from the model in order to allow or support or enable computation – is particularly
interesting because the focus with abstraction tends to be on what is retained, i.e., on retaining only
those features of the thing to be modelled that are essential (Starﬁeld, Smith, & Bleloch, 1994). In
Section 3, what’s removed from the story is not the literal words but the order to the words. Removing

aProduct: ProductforSale= Truedescription = “Baby’s shoes”condition = “Never worn”IDTypeSubtypeDescriptionConditionSize AF1G20ShoesBabyBaby’s shoesNever wornSmallanAdvert: Advertcategory = Salesitem = “Baby’s shoes”description = “Never worn”the order “dismantles” the story. Haven (2007) presents an example to illustrate how simply varying the
placement of a word effects the meaning of a sentence. Compare the following examples, taken from
Haven’s book, Story Proof (Haven, 2007):

John will marry Elise.
Even John will marry Elise.
John will even marry Elise.
John will marry even Elise.

Notice not just that the meaning of the sentence changes but also, if we dwell on each sentence, we
might start to wonder about the context and the motivation for John, e.g., what might be going on for
John to marry even Elise?

As well as “dismantling” the story, removing the order of the words so as to make a computable repre-
sentation also ‘de-means’ – i.e., reduces the very meaning of – the story for a human whilst, conversely,
formalising the representation to enable computation. At the same time, details are added to the compu-
tational model, e.g., data types, methods, class-object structures. We return to the issue of “de-meaning”
later in the paper when we contrast Bruner’s Logico-scientiﬁc and Narrative modes of thinking. Later in
the paper, we also brieﬂy return to the issue of adding information when we discuss software engineering
and translation of languages.

Removing information in order to enable computation can be illustrated through another story, the
Byzantine Generals Problem (BGP). There are several versions of the BGP reported in the comput-
ing literature. We take what is perhaps the most commonly-cited version, published within the safety
engineering community by Lamport, Shostak, and Pease (1982).

. . . several divisions of the Byzantine army are camped outside an enemy city, each division
commanded by its own general. The generals can communicate with one another only by
messenger. After observing the enemy, they must decide upon a common plan of action.
However, some of the generals may be traitors, trying to prevent the loyal generals from
reaching agreement. The generals must have an algorithm to guarantee that the following
two conditions are met:
#1: All loyal generals decide on the same plan of action [. . . ]
#2: A small number of traitors cannot cause the loyal generals to adopt a bad plan.
(emboldened emphasises added)

The BGP was formulated by computer scientists to illustrate a particular computational problem, i.e.,
ensuring reliable communication in the presence of faulty components. In a previous publication, we
(Menon & Rainer, 2021) critically evaluated the BGP story, concluding that the story ‘fails’ as a story.
For example, the story lacks any humanly-meaningful objective. Generals are expected to agree on a
common plan of action, but it does not matter on what they agree. They can attack or retreat or, in
principle, do anything else they agree on. Their objective is arbitrary. This is odd for a story because
usually in a story a character would have some motive for their objective. To make a more effective story
for the reader (a reader, not a software engineer or a programmer or, more generally, a computational-
thinker), what is missing from the sentence is a ﬁnal clause, i.e., decide to do what? Furthermore,
since the loyal generals’ objective is arbitrary, the traitors’ objectives are also arbitrary: the traitors are
only concerned with preventing an arbitrary legitimate agreement. But note too that not only must the
traitors’ objectives be arbitrary relative to the loyal generals, each of the traitor’s objectives must be
arbitrary relative to all other traitors. In this context, each traitor might be better understood as an agent
of chaos.

The BGP therefore risks misrepresenting the computational problem through the way it presents a story
in terms of human agents. To prevent this risk, to “square the circle” between story-thinking and

computational-thinking, the human agents in the BGP are given odd (for a human) intention. Safety
engineers possess the technical knowledge needed to interpret the story “correctly” for the computa-
tional problem being explored. For safety engineers, as computational-thinkers, the arbitrariness of the
objectives is not just acceptable but essential. This is because the arbitrariness of the objectives allows
for an algorithmic solution that has wide applicability: the algorithmic solution would apply for situa-
tions where generals agree to attack and for situations where generals agree to retreat and for situations
where generals (arbitrarily) agree to do something else. The BGP abstracts the problem so as to provide
a generalised solution. And in abstracting the problem it must necessarily remove a humanly-meaningful
quality of the story, i.e., meaningful human intention.

Overall then, the BGP acts as a kind of mirror example to the six-word story. To go from the six-word
story to the computational representation, we remove something essential. To go from the computational
problem to a BGP story we do not add something essential. In contrasting ways, both neglect. The BGP
is not meaningless – we can still understand the story – but it is not meaningful, in human terms, as a
“good” story.

5. Software engineering: programming-in-the-large
For conciseness, we use Johnson and Ekstedt’s (2016) Tarpit Theory of Software Engineering as our
reference for discussing the nature of software engineering. As part of the summary of their theory,
Johnson and Ekstedt write, “The goal of software development is to create programs that, when executed
by a computer, result in behavior that is of utility to some stakeholder.”

Drawing on the discussion of computational thinking, we might say that software is useful – of utility – to
a stakeholder when the software behaves in a way that solves a representation of a problem experienced
by that stakeholder. Solving a representation of a problem is not the same as solving the problem.
And inferring from Priami’s (2007) assertion earlier, as well as from our discussion of the BGP, some
problems must be neglected in order to make the resulting model computable.

Johnson and Ekstedt (2016) also write that, “Much of software engineering concerns translations; design
speciﬁcations are translated into source code. . . , source code is translated into machine code. . . , etc. In
fact, the whole process of software engineering can be considered as a series of translations. . . (Johnson
and Ekstedt (2016), p. 187). They deﬁne language as “A set of speciﬁcations”, translation as, “An
activity that preserves the semantic equivalence between source and target languages [speciﬁcations],”
and semantic equivalence as, “Two speciﬁcations are semantically equivalent if, when translated to a
common language, they are syntactically identical.” (Notice how semantic equivalence is being deﬁned
in terms of syntactic identity.)

We have shown, with the six-word story and the BGP story, that it is not necessarily feasible to translate
the story from the source language to a target language and maintain semantic equivalence. More than
that, it is unreasonable to expect a translation, in the way that Johnson and Ekstedt deﬁne it, from the
English language of the six-word story to, as examples, the User Story, the object diagram or the database
table. But that is the point. The language used with story-thinking, and therefore the representations
used for story-thinking, are not semantically equivalent to the language and representations used for
computational-thinking.

Johnson and Ekstedt (2016) seem to recognise this problem of non-equivalence, when they write:

This leads us to the major challenge of software engineering, that programs – these for-
mal, static, syntactic compositions – are very different from the oftentimes ﬂuid and
intangible stakeholder experiences they are intended to evoke. A signiﬁcant feat of
imagination is thus required on the part of the developers to predict the effects of a
given syntactic modiﬁcation to the program code on the end-users’s experiences.
An even greater challenge, which we propose as the core task of software endeavors,
is to determine which syntactic manipulation will cause a speciﬁed stakeholder

experience, and then to perform the appropriate informed manipulations, i.e. to make
design decisions.
(Johnson and Ekstedt (2016); italicised emphasis and san serif font are in the original,
the emboldened emphasis has been added)

Clearly, Johnson and Ekstedt (2016) are aware of the “gap” between stakeholders’ experiences and exe-
cutable programs, and of the challenge of bridging this gap. This takes us to requirements engineering,
discussed in the next section.

6. Requirement engineering
In many situations when we use stories, we are not intending to use the stories for requirements gath-
ering or requirements analysis. Requirements engineering has, however, adopted the use of the term
“story”, as well as aspects of stories. Common approaches in requirements engineering are the “user
story” (Lucassen et al., 2015), the “job story” (Lucassen et al., 2018), and the scenario (Carroll, 2003,
1997). Requirements engineering (RE) would therefore seem to provide the bridge between stakehold-
ers’ “ﬂuid and intangible experiences” (Johnson & Ekstedt, 2016) and the preparation of executable
programs.

But as we showed with the User Stories in Section 3.1, this bridge often seems to begin the process
of neglect, of framing the situation as a computable “problem”. In other words (and however uninten-
tionally), user stories, job stories and scenarios all begin the process of removing the non-computable,
humanly-meaningful aspects of the situation.

Hinton (2014) provides a good example of this process. He discusses how he and colleagues interviewed
clients to gather requirements for a software application that would allow users to change power of at-
torney for their assets on the website, rather than having to use a paper form. Interviewees’ recollections
were about “getting a form, ﬁlling it out, and mailing it in.”

Hinton (2014) writes:

So, what was missed in these interviews? Most of what was really important, it turns out.
Because they [the interviewed clients] knew we were working on a website, these users
often unconsciously tried meeting us halfway by framing their answers in terms that they
assumed we needed. (Hinton (2014); emphasis added)

and:

But if we asked without priming the users with that context, their storytelling would be
closer to the raw situation they were in – for example, “I just got remarried, and I want to
be sure this time my spouse and I have ownership sorted out responsibly,” and not “I guess
I’d look for the right form to ﬁll out.” ((Hinton, 2014); emphasise added)

and also:

Meanwhile, the engineers in the company’s IT department wanted to determine the task [the
problem to be solved] and create a linear progression to it. But, we could tell that few people
[clients] would recognize that linear path to the task, because the goals that engineering
assumed people had already decided on were, in most cases, yet to be discovered. ((Hinton,
2014), p. 384)

Hinton’s account helps to illustrate the two modes of thinking, and the tension arising between them.

7. Modes of thinking recognised in other disciplines
Others have recognised differences in modes of thinking, albeit not in the context of programming and
software engineering. We brieﬂy consider two complementary perspectives: McGilchrist’s two ways of
attending, and Bruner’s two modes of thought.

7.1. McGilchrist’s two ways of attending
McGilchrist (2019, 2021) explores differences in thinking between the left-hemisphere and the right-
hemisphere of the brain. To avoid the simpliﬁcations of left-brain, right-brain thinking here (which
McGilchrist also seeks to avoid ) we’ll use the terms “left-mode attending” and “right-mode attending”.
These modes of attending can be illustrated through the example of a bird sitting on a branch with a
berry in its beak. The bird needs to attend to the berry. This way of attending, the left-mode attending,
requires a concentrated, instrumental attention that manipulates the berry-as-thing, a fragment of the
world. But at the same time, the bird also needs to attend to the environment. This way of attending, the
right-mode attending, requires a broad perspective, in which the bird understands itself in relation to the
world as a whole. McGilchrist uses similar examples to explain how these two ways of attending are in
opposition to each other: it is not possible for one system to be both narrowly-focused and also, at the
same time, broadly perceiving. For McGilchrist, this explains, or at least partly explains, an animal’s
need for two separate hemispheres, where each is a (sub)system that can attend independently.

Drawing on McGilchrist’s work, we hypothesise that story-thinking requires right-mode and left-mode
attending, to both create a story and appreciate it as a reader, but that story-thinking predominantly draws
on right-mode attending for its processing and effect.

7.2. Bruner’s two modes of thought
Bruner (2020) also distinguished two modes of thought, summarised in Table 2. The Logico-scientiﬁc
mode doesn’t map cleanly onto computational-thinking, yet there are clear connections with some of
the key characteristics identiﬁed in the table, e.g., Categorical, General, Abstract, De-contextualised,
Non-contradictory, Consistent. Although the Logico-scientiﬁc mode of thinking may be self-evidently
present, and even dominant, in science and technology, Turner (1996) argues for the fundamental neces-
sity of the Narrative mode: “Narrative imagining – story – is the fundamental instrument of thought.”

Criterion
Objective
Central problem To know truth
Strategy

Logico-scientiﬁc
Truth

Method

Key
characteristics

Empirical discovery
guided by reasoned hypothesis
Sound argument
Tight analysis
Reason
Aristotelian logic
Proof
Top-down
Theory driven
Categorical
General
Abstract
De-contextualised
Ahistorical
Non-contradictory
Consistent

Narrative
Verismilitude
To endow experience with meaning
Universal understanding
grounded in personal experience
Good story
Inspiring account
Association
Aesthetics
Intuition
Bottom-up
Meaning centred
Experiential
Particular
Concrete
Context sensitive
Historical
Contradictory
Paradoxical, ironic

Table 2 – Jerome Bruner’s two modes of thought

Drawing on Bruner’s work, we hypothesise that story-thinking predominantly draws on Narrative think-
ing for its effect, whilst computational-thinking is predominantly (if not exclusively) a Logico-scientiﬁc
mode of thinking.

8. A summary of the problems and suggested ways forward
Drawing on the preceding discussion, we identify two speciﬁc problems, brieﬂy suggest ways in which
these problems might be tackled, and then present candidate hypotheses and research questions for
further research.

8.1. A statement of the problems
We formulate two problems:

1. The problem of neglectful representations. Computational thinking neglects and must neglect.
It does this in order to arrive at a representation that is computable. These “neglectful repre-
sentations” inevitably “de-mean”, i.e., reduce humanly-meaningful qualities. (As a provocative
example, consider Turing’s Imitation Game: this reframes intelligence as behaviour, as a repre-
sentation of intelligence, and by doing so “de-means” intelligence.) Neglectful representations
have implications for the impact of computational-thinking, programming and software engineer-
ing on society, the economy and the environment. For example, economic impact is much easier
to align with computational-thinking because economic thinking appears to be a way of thinking
similar in kind to computational-thinking. But humanly-meaningful qualities of society, as well as
the qualities of the environment, are much harder to represent, a point that is increasingly recog-
nised, e.g., with work on values in computing (Ferrario et al., 2017), kind computing (Alrimawi
& Nuseibeh, 2022), compassionate computing (Pomputius, 2020) and responsible software engi-
neering (Schieferdecker, 2020).

2. The opposing differences in the two modes of thinking. The two modes of thinking are not just
fundamentally different, but appear opposing, even incompatible. There is then the problem of
integrating or otherwise synthesising these two modes of thinking. Achieving some kind of inte-
gration would then help to re-balance the ﬁrst problem.

These problems, and the use of story as a possible solution, appear to be implicitly recognised by others
in software engineering. Strøm (2006, 2007) investigates the role and value of stories that include
emotions and conﬂicts in software engineering. Bailin introduced design stories (Bailin, 2003) and
also discussed how features need stories to convey the “missing semantics” that address “ﬁne-grained
questions of context, interface, function, performance, and rationale” (Bailin, 2009). In an unpublished
paper, Stubbleﬁeld et al. (2002) observe that, “In our experience, most software development projects
do not fail for technical reasons. They fail because they do not engage users at the fundamental level of
value, meaning and practice. They solve the wrong problem, or fail to support deeper patterns of work
and social interaction.” (Stubbleﬁeld & Rogers, 2002). Finally, Yilmaz et al. (2016) report a preliminary
study to demonstrate the beneﬁts of story-driven software development: “. . . it is important to capture
and store the excessively valuable tacit knowledge using a rich story-based approach.” Signiﬁcantly,
none of these papers consider the cognitive processes that occur with story; in other words, none of the
papers consider story in the way we approach it here.

8.2. Suggested ways forward
We suggest two directions for future research:

1. Developing methodology that connects stories with algorithms.

In a previous paper,
Rainer (2017), drawing on prior work in law and legal reasoning, proposed a methodology for
identifying arguments and stories from blog articles, extracting them, and then graphically com-
bining them. This methodology might be extended or, alternatively, might provide an example for
a potential methodology to integrate representations used for story-thinking with representations
used for computational-thinking.

2. Changing software practice. With an agile methodology, software practitioners return to the user
story, e.g., its acceptance criteria, to evaluate whether a user story has been delivered. We sug-
gest that practitioners might return to the story, and not just the user story. As a further example,

software practitioners might initiate one or more reading groups that meet regularly to consider
the stories that provide context for the software being developed. For example, a software en-
gineering (SE) team developing a software system for managing information about children in
publicly–funded care homes might establish a reading group to read and discuss Sissay’s memoir,
My Life is Why (Sissay, 2019). Or software practitioners who want to ensure their software is being
equitable might establish a reading group to read and discuss Ford’s memoir, Think Black (Ford,
2021), recounting his father’s experience as the ﬁrst black software engineer at IBM. With these
examples, story-thinking and computational-thinking are combined in subtle, informal, but per-
haps more culturally effective and longer-term, ways.

We also suggest two hypotheses, one assertion and several research questions:

1. For the hypotheses:

(a) That programming requires the kind or kinds of thinking described here as computational-
thinking, logico-scientiﬁc thinking and left-mode thinking. This is a prescriptive hypothesis,
i.e., that programming should be concerned with computational-thinking, logico-scientiﬁc
thinking and left-mode thinking.
In Perlis’ words, “To understand a program you must
become both the machine and the program.” (epigram #23).

(b) That software engineering is currently oriented toward computational-thinking, logico-
scientiﬁc thinking and left-mode thinking. Unlike our hypothesis about programming, this
hypothesis is normative.

2. For the assertion:

(a) That software engineering should comprise a better balance of computational-thinking and
story-thinking. Our hypotheses about programming and software engineering are state-
ments of how the world is. By contrast, our assertion is a statement of how the world
should be. Software engineering should be better balanced, comprising a balance of both
computational-thinking, logico-scientiﬁc thinking and left-mode thinking and also of story-
thinking, narrative thinking and right-mode thinking.

3. For the research questions:

(a) How do we model humanly-meaningful qualities in software engineering in a way that

avoids the problem of neglectful representations?

(b) How do we show that software engineering needs both modes of thinking?

(c) How do we respect and retain both modes of thinking in software engineering practice?

(d) How do we facilitate story-thinking in software engineering?

(e) How do we integrate these oppositional ways of thinking in software engineering?

9. Conclusion
In this paper we have explored two modes of thinking, story-thinking and computational-thinking. Using
a six-word story as our main example, we show how computational-thinking and story-thinking attend
to this story in very different ways. We’ve considered how software engineering and requirements
engineering recognise, at least implicitly, these different modes of thought, as well as the challenges of
integrating these modes. We also brieﬂy reviewed two examples of related work, one from neuroscience
and one from psychology. We then identiﬁed two speciﬁc and fundamental problems (the problem of
“neglectful representations” and the problem of oppositional ways of thinking), brieﬂy suggested ways
in which these problems might be tackled, and proposed hypotheses, an assertion and research questions
for future research.

10. Acknowledgements
We thank the reviewers in advance for their attention to our paper. We also thank participants of the
online presentation for permission to quote them.

11. References
Aho, A. V. (2012). Computation and computational thinking. The Computer Journal, 55(7), 832–835.
Alrimawi, F., & Nuseibeh, B. (2022). Kind computing.
Bailin, S. C. (2003). Diagrams and design stories. Machine Graphics and Vision, 12(1), 17–38.
Bailin, S. C. (2009). Features need stories. In International conference on software reuse (pp. 51–64).
Bruner, J. (2020). Actual minds, possible worlds. In Actual minds, possible worlds. Harvard university

press.

Carroll, J. M. (1997). Scenario-based design. In Handbook of human-computer interaction (pp. 383–

406). Elsevier.

Carroll, J. M. (2003). Making use: scenario-based design of human-computer interactions. MIT press.
Cohn, M. (2004). User stories applied: For agile software development. Addison-Wesley Professional.
Denning, P. J. (2009). The profession of it beyond computational thinking. Communications of the

ACM, 52(6), 28–30.

Erwig, M. (2017). Once upon an algorithm: how stories explain computing. MIT Press.
Ferrario, M. A., Simm, W., Whittle, J., Frauenberger, C., Fitzpatrick, G., & Purgathofer, P.

(2017).
Values in computing. In Proceedings of the 2017 chi conference extended abstracts on human
factors in computing systems (pp. 660–667).

Ford, C. W. (2021). Think black: A memoir. HarperCollins Publishers Inc.
Haven, K. (2007). Story proof: The science behind the startling power of story. Greenwood Publishing

Group.

Heineman, G. T., Pollice, G., & Selkow, S. (2008). Algorithms in a nutshell. O’Reilly Media.
Hinton, A. (2014). Understanding context: Environment, language, and information architecture. "

O’Reilly Media, Inc.".

Johnson, P., & Ekstedt, M. (2016). The tarpit–a general theory of software engineering. Information

and Software Technology, 70, 181–203.

Lamport, L., Shostak, R., & Pease, M. (1982). The byzantine generals problem. ACM Transactions on

Progamming Languages and Systems, 4(3), 382-401.

Lucassen, G., Dalpiaz, F., Van Der Werf, J. M. E., & Brinkkemper, S. (2015). Forging high-quality user
stories: towards a discipline for agile requirements. In 2015 ieee 23rd international requirements
engineering conference (re) (pp. 126–135).

Lucassen, G., van de Keuken, M., Dalpiaz, F., Brinkkemper, S., Sloof, G. W., & Schlingmann, J. (2018).
Jobs-to-be-done oriented requirements engineering: a method for deﬁning job stories. In Inter-
national working conference on requirements engineering: Foundation for software quality (pp.
227–243).

McGilchrist, I. (2019). The master and his emissary. In The master and his emissary. Yale University

Press.

McGilchrist, I. (2021). The matter with things: Our brains, our delusions, and the unmaking of the

world. Perspectiva Press.

Menon, C., & Rainer, A. (2021). Stories and narratives in safety engineering. In Proceedings of the 30th
safety critical systems symposium, volume: Scsc-170. Retrieved from https://scsc.uk/
scsc-170

Pomputius, A.

(2020). Compassionate computing in the time of covid-19: Interview with laurie n.

taylor. Medical Reference Services Quarterly, 39(4), 399–405.

Priami, C. (2007). Computational thinking in biology. In Transactions on computational systems biology

viii (pp. 63–76). Springer.

Rainer, A. (2017). Using argumentation theory to analyse software practitioners’ defeasible evidence,

inference and belief. Information and Software Technology, 87, 62–80.

Schieferdecker, I. (2020). Responsible software engineering. In The future of software quality assurance

(pp. 137–146). Springer, Cham.

Sissay, L. (2019). My name is why. Canongate Books.
Starﬁeld, A. M., Smith, K. A., & Bleloch, A. L. (1994). How to model it: Problem solving for the

computer age. Interaction Book Company.

Strøm, G. (2006). The reader creates a personal meaning: A comparative study of scenarios and human-

centred stories. In People and computers xix—the bigger picture (pp. 53–68). Springer.

Strøm, G.

(2007). Stories with emotions and conﬂicts drive development of better interactions in
In Proceedings of the 19th australasian conference on computer-

industrial software projects.
human interaction: Entertaining user interfaces (pp. 115–121).

Stubbleﬁeld, W. A., & Rogers, K. S.

The micro traveler and the hero’s journey.
Retrieved from https://wmstubblefield.com/wp-content/uploads/2019/06/
narrativePaper.pdf

(2002).

Turner, M. (1996). The literary mind: The origins of thought and language. Oxford University Press.
Worley, P. (2014). Once up an if: the storythinking handbook. Bloomsbury.
Yilmaz, M., Atasoy, B., O’Connor, R. V., Martens, J.-B., & Clarke, P. (2016). Software developer’s

journey. In European conference on software process improvement (pp. 203–211).

