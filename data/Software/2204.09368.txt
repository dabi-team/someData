2
2
0
2

r
p
A
0
2

]
E
S
.
s
c
[

1
v
8
6
3
9
0
.
4
0
2
2
:
v
i
X
r
a

BugListener: Identifying and Synthesizing Bug Reports from
Collaborative Live Chats
Lin Shi1,2, Fangwen Mu1,2, Yumin Zhang1,2, Ye Yang5, Junjie Chen6, Xiao Chen1,2, Hanzhi
Jiang1,2,Ziyou Jiang1,2, Qing Wang1,2,3,4âˆ—
1Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China
3 State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences, Beijing, China
4 Science & Technology on Integrated Information System Laboratory,
Institute of Software Chinese Academy of Sciences, Beijing, China
5 School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA
6 Tianjin University, College of Intelligence and Computing, Tianjin, China
{shilin,fangwen2020,yumin2020,chenxiao2021,hanzhi2021,ziyou2019,wq}@iscas.ac.cn,
yyang4@stevens.edu, junjiechen@tju.edu.cn

ABSTRACT
In community-based software development, developers frequently
rely on live-chatting to discuss emergent bugs/errors they encounter
in daily development tasks. However, it remains a challenging task
to accurately record such knowledge due to the noisy nature of
interleaved dialogs in live chat data. In this paper, we first formulate
the task of identifying and synthesizing bug reports from commu-
nity live chats, and propose a novel approach, named BugListener,
to address the challenges. Specifically, BugListener automates three
sub-tasks: 1) Disentangle the dialogs from massive chat logs by
using a Feed-Forward neural network; 2) Identify the bug-report
dialogs from separated dialogs by leveraging the Graph neural net-
work to learn the contextual information; 3) Synthesize the bug
reports by utilizing Transfer Learning techniques to classify the
sentences into: observed behaviors (OB), expected behaviors (EB),
and steps to reproduce the bug (SR). BugListener is evaluated on
six open source projects. The results show that: for bug report
identification, BugListener achieves the average F1 of 77.74%, im-
proving the best baseline by 12.96%; and for bug report synthesis
task, BugListener could classify the OB, EB, and SR sentences with
the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by
9.32%, 12.21%, 10.91%, respectively. A human evaluation study also
confirms the effectiveness of BugListener in generating relevant and
accurate bug reports. These demonstrate the significant potential of
applying BugListener in community-based software development,
for promoting bug discovery and quality improvement.

âˆ—Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00
https://doi.org/10.1145/3510003.3510108

KEYWORDS
Bug Report Generation, Live Chats Mining, Open Source

ACM Reference Format:
Lin Shi1,2, Fangwen Mu1,2, Yumin Zhang1,2, Ye Yang5, Junjie Chen6, Xiao
Chen1,2, Hanzhi Jiang1,2,Ziyou Jiang1,2, Qing Wang1,2,3,4. 2022. BugListener:
Identifying and Synthesizing Bug Reports from Collaborative Live Chats
. In 44th International Conference on Software Engineering (ICSE â€™22), May
21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3510003.3510108

1 INTRODUCTION
Collaborative communication via live chats allows developers to
seek information and technical support, share opinions and ideas,
discuss issues, and form community development [14, 16], in a
more efficient way compared with asynchronous communication
such as emails or forums [42, 65, 66]. Consequently, collaborative
live chatting has become an integral part of most software develop-
ment processes, not only for open source communities constituting
globally distributed developers, but also for software companies to
facilitate in-house team communication and coordination, esp. in
accommodating remote work due to the COVID-19 pandemic [49].
Existing literature reports that developers are likely to join col-
laborative live chats to discuss problems they encountered during
development [5, 6, 13, 52]. Shi et al. [62] analyzed 749 live-chat
dialogs from eight OSS communities, and found 32% of the dialogs
are reporting unexpected behaviors, such as something does not
work, reliability issues, performance issues, and errors. In fact, these
reporting problems usually imply potential bugs that have not been
found. Fig. 1 illustrates an example slice of collaborative live chats
[1] from the Docker community. In this conversation, developer
David reported a performance bug that Docker took a lot of disk
space, and Lena indeed confirmed Davidâ€™s feedback. Then, Jack
provided a suggestion to help resolve this problem but failed in the
end. Although developers have revealed this bug via collaborative
live chats, the highly dynamic and multi-threading nature of live
chatting makes this bug-report conversation get quickly flooded by
new incoming messages. After several months, Docker developers
call to remembrance this bug with the frustrated comments such
as â€œlost all my system backupsâ€ and â€œitâ€™s a shameâ€, when there
are several formal bug reports (i.e., #30254, #31105, and #32420)

 
 
 
 
 
 
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

are also limited. How to make maximal use of the limited labeled
data to classify the unlabeled chat messages accurately becomes a
critical problem.

In this work, we propose a novel approach, named BugListener,
which can identify bug-report dialogs from massive chat logs and
synthesize complete bug reports from predicted bug-report dialogs.
BugListener employs a deep graph-based network to capture the
complex dialog structure, and a transfer-learning network to syn-
thesize bug reports. Specifically, BugListener addresses the chal-
lenges with three elaborated sub-tasks: 1) Disentangle the dialogs
from massive chat logs by using a Feed-Forward neural network.
2) Identify bug-report dialogs from separated dialogs by modeling
the original dialog to the graph-structured dialog and leveraging
the Graph neural network (GNN) to learn the complex context
representation. 3) Synthesize the bug reports from predicted bug-
report dialogs using Transfer Learning techniques. Specifically, we
use the pre-trained BERT model provided by Devlin et al. [21]
and fine-tune it twice using the external BEE dataset [68] and our
own dataset, respectively. To evaluate the proposed approach, we
collect and annotate 1,501 dialogs from six popular open-source
projects. The experimental results show that our approach signifi-
cantly outperforms all other baselines in both two tasks. For bug
report identification task, BugListener achieves an average F1 of
77.74%, improving the best baseline by 12.96%. For bug report syn-
thesis task, BugListener could classify sentences depicting observed
behavior (OB), expected behavior (EB), and steps to reproduce (SR)
with the F1 of 84.62%, 71.46%, and 73.13%, respectively, improving
the best baseline by 9.32%, 12.21%, and 10.91%, respectively. We also
conduct a human evaluation to assess the correctness and quality of
the generated bug reports, showing that BugListener can generate
relevant and accurate bug reports.

The main contributions and their significance are as follows.
â€¢ We propose an automated approach, named BugListener, based
on a deep graph-based network to effectively identify the bug-
report dialogs, and a transfer-learning network to extensively
synthesize bug reports. We believe that BugListener can facil-
itate community-based software development by promot-
ing bug discovery and quality improvement.

â€¢ We evaluate the BugListener by comparing with state-of-the-art

baselines, with superior performance.

â€¢ Data availability: publicly accessible dataset and source code
[2] to facilitate the replication of our study and its application in
other contexts.

In the remaining of this paper, Sec. 2 defines the problem. Sec. 3
elaborates the approach. Sec. 4 presents the experimental setup. Sec.
5 demonstrates the results and analysis. Sec. 6 describes the human
evaluation. Sec. 7 discusses indications and threats to validity. Sec.
8 introduces the related work. Sec. 9 concludes our work.

2 PROBLEM DEFINITION
To facilitate the problem definition and further discussion, we first
provide some basic concepts and notations used in this study:
â€¢ A chat log (L) corresponds to a sequence of utterances ğ‘¢ğ‘– in

chronological order, denoted by ğ¿ = {ğ‘¢1, ğ‘¢2, ..., ğ‘¢ğ‘› }.

â€¢ An utterance (ğ‘¢ğ‘– ) consists of the timestamp, developer role, and

textual message, denoted by ğ‘¢ğ‘– =< ğ‘¡ğ‘–ğ‘šğ‘’, ğ‘Ÿğ‘œğ‘™ğ‘’, ğ‘¡ğ‘’ğ‘¥ğ‘¡ >.

Fig. 1: An example of identifying and synthesizing a bug re-
port from the Docker collaborative live chats.
reflecting the similar problem that was submitted to the GitHub
bug repository. We can observe that, if the bug discussed in live
chats could be identified and documented in a timely manner, the
bug may have been resolved earlier by the Docker community. Con-
sequently, the Docker community may have the opportunity to
prevent many failure incidents associated with this bug [54].

Although the live chats could be a tremendous data source em-
bedded with bug reports over time, it is quite challenging to mine
massive chat messages due to the following barriers. (1) Entangled
and noisy data. Live chats typically contain entangled, informal
conversations covering a wide range of topics [44]. Moreover, there
exist noisy utterances such as duplicate and off-topic messages
in chat messages that do not provide any valuable information.
Such entangled and noisy nature of live chat data poses a difficulty
in analyzing and interpreting the communicative dialogues. (2)
Understanding complex dialog structure. In complex dialogs,
developers usually either confirm or reject a bug report by replying
to previous utterances. Since the â€œreply-toâ€ relationship is not linear
to the dialog structure, it is necessary to employ more sophisticated
techniques to handle nonlinear dialog structure, in order to learn
precise feedback and reduce the likelihood of introducing false-
positive. For example, the utterance â€œWhen I use the â€˜automation-
Nameâ€™ key, I get an error that it is not a recognized W3C capability.â€
is very likely to be classified as a bug proposal. However, when
examining the dialog, we found that the following-up utterances
pointed out the error was not a valid bug. Instead, it was caused by
the userâ€™s action of importing incorrect packages. (3) Extremely
expensive annotation. The live chats are typically large in size. It
is extremely expensive to annotate bug reports from chat messages
due to the high volume corpus and a low proportion of ground-truth
data. Only a few labeled chat messages are categorized into bug re-
port types. Thus, the labeled resources for synthesizing bug reports

Bug reportDavidDec 30 2016 10:34I think. I have something important to say. It's the second time that this happens with me. I install docker on Linux Mint. After some hours the disk is full. I lost all my system backups. I think I have to report this, because it's a shame. When I delete all docker images and uninstall docker, I have 100GB free... Someone tells me. How this is even possible that docker uses 100GB? Can docker avoid using such huge disk? Please fix that. Until there, I will be using docker on a virtual machine, because I can't trust this software anymore.Dec 30 2016 12:03 great feedbackLenaJackDec 30 2016 22:14  @Jack Doesn't make any sense. After I restart my computer, the disk free space keeps decreasing. Dec 30 2016 21:02  @David it sounds like you performed a                                             instead of a                                                          and ended up filling your disk.docker pull somehugerepositorydocker pull somehugerepository:specifictagDavidDescription:I have something important to say. It's the second time that this happens with me. I lost all my system backups. I think I have to report this, because it's a shame. When I delete all docker images and uninstall docker, I have 100GB free... How this is even possible that docker uses 100GB? Until there, I will be using docker on a virtual machine, because I can't trust this software anymore.Observed Behavior:After some hours the disk is fullExpected Behavior:Can docker avoid using such huge disk?Steps to reproduce:I install docker on Linux MintBugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

â€¢ A developer role (ğ‘Ÿğ‘œğ‘™ğ‘’) in a dialog is defined as either a reporter
or a discussant. A reporter refers to a developer launching a dialog,
while a discussant refers to a developer participating in the dialog.
Denoted by ğ‘Ÿğ‘œğ‘™ğ‘’ âˆˆ {ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ, ğ‘‘ğ‘–ğ‘ ğ‘ğ‘¢ğ‘ ğ‘ ğ‘ğ‘›ğ‘¡ }.

ğ‘˜ }.

â€¢ A dialog (ğ·ğ‘– ) is a sequence of ğ‘˜ utterances ğ‘¢ğ‘– , retaining the
â€œreply-toâ€ relationship among utterances, denoted by ğ·ğ‘– = {ğ‘¢ R1
1 ,
2 , ..., ğ‘¢ Rğ‘˜
ğ‘¢ R2

â€¢ A relational context for utterance ğ‘¢ğ‘– (Rğ‘– ) is a set of undi-
rected "reply-to" relationship identifiers, each identifier corre-
sponding to a message replying to or replied by ğ‘¢ğ‘– . If two utter-
ances share the same superscript, then it implies one replies to
the other. For example, ğ· = {ğ‘¢ğ‘…1,ğ‘…2
3 } represents that
both ğ‘¢2 and ğ‘¢3 reply to ğ‘¢1.
Our work then targets at automatically identifying and synthe-
sizing bug reports from community live chats. We formulate the
task of automatic bug report generation from live chats with three
elaborated sub-tasks:

2 , ğ‘¢ğ‘…2

, ğ‘¢ğ‘…1

1

(1) Dialog disentanglement: Given the historical chat log ğ¿, dis-

entangle it into separate dialogs {ğ·1, ğ·2, ..., ğ·ğ‘› }.

(2) Bug-Report dialog Identification (BRI): Given a separate dialog
ğ·ğ‘– , find a binary function ğ‘“ so that ğ‘“ (ğ·ğ‘– ) can determine whether
the dialog involves bug-reporting messages.

(3) Bug-Report Synthesis (BRS): Assuming that the content of
bug reports is made up of sentences extracted from the reportersâ€™ ut-
terances, given all the reporterâ€™s utterances ğ‘ˆğ‘Ÿ in the predicted bug-
report dialog ğ·ğ‘– , find a function ğ‘” so that ğ‘”(ğ‘ˆğ‘Ÿ ) = {ğ·ğ¸ğ‘†, ğ‘‚ğµ, ğ¸ğµ, ğ‘†ğ‘…},
where ğ·ğ¸ğ‘†, ğ‘‚ğµ, ğ¸ğµ, and ğ‘†ğ‘… represent the collections of sentences
in ğ‘ˆğ‘Ÿ that depict Description, Observed Behavior, Expected Behavior,
and Step to Reproduce.

3 APPROACH
There are five main steps to construct BugListener, as shown in Fig.
2. These include:(1) dialog disentanglement and data augmentation
to prepare the data; (2) utterance embedding to convert utterances
into semantic vectors; (3) graph-based context embedding to con-
struct dialog graph and learn the contextual representation by em-
ploying a two-layer graph neural network; (4) dialog embedding
and classification to learn whether a dialog is a bug-report dialog;
and (5) bug report synthesis to form a complete bug report. Next,
we present details of each step.

3.1 Data Disentanglement and Augmentation
In this step, We first separate dialogs from the interleaved chat
logs using a Feed-Forward network. Then, we augment the original
dialog dataset utilizing a heuristic data augmentation method to
overcome the insufficient labeled resource challenge.
3.1.1 Dialog Disentanglement. Utterances from a single conver-
sation thread are usually interleaved with other ongoing conver-
sations, and therefore need to be divided into individual dialogs
accordingly. To find a reliable disentanglement model, we exper-
iment with four state-of-the-art dialog disentanglement models,
i.e., BILSTM model [29], BERT model [21], E2E model [44], and
FF model, using our manual disentanglement dataset as detailed
in Section 4.1 later. The comparison results from our experiments
show that the FF model significantly outperforms the others on
disentangling developer live chat by achieving the highest scores

on NMI, Shen-F, F1, and ARI metrics. The average scores of these
four metrics are 0.74, 0.81, 0.47, and 0.57 respectively1.

Specifically, the FF model is a Feed-Forward neural network
with 2 layers, 512-dimensional hidden vectors, and softsign non-
linearities. It employs a two-stage strategy to resolve dialog disen-
tanglement. First, the FF model predicts the â€œreply-toâ€ relationship
between every two utterances in the chat log based on averaged
pre-trained word embedding and many hand-engineered features.
Second, it clusters the utterances that can reach each other via the
â€œreply-toâ€ predictions as one dialog. Thus, the FF-model can output
not only the utterances in one dialog but also their â€œreply-toâ€ rela-
tionship, which is essential for constructing the internal network
structure of dialogs.

3.1.2 Data Augmentation. To address the limited annotation and
data imbalance issue, a heuristic data augmentation mechanism is
employed to enlarge the dataset through dialog mutation. The key
to dialog mutation is to alter the utterance forms and retain their
semantics. To achieve that, we mutate a long utterance by replacing
a few words with their synonyms, or mutate a short utterance
by replacing it with another short utterance. Specifically, given
a dialog ğ· = {ğ‘¢1, ğ‘¢2, ..., ğ‘¢ğ‘› }, we generate ğ‘ different mutants by
iterating the following steps ğ‘ times. For each utterance ğ‘¢ğ‘– in a
dialog ğ·, we perform either an utterance-level replacement or a
word-level replacement based on its length, and generate a new
utterance ğ‘¢ğ‘– â€² = Î“(ğ‘¢ğ‘– ):

âˆ€ğ‘¢ğ‘– âˆˆ ğ·, Î“(ğ‘¢ğ‘– ) =

(cid:40)ğ‘¢ğ‘˜
SR(ğ‘¢ğ‘– )

|ğ‘¢ğ‘– | â‰¤ ğœƒ
|ğ‘¢ğ‘– | > ğœƒ

(1)

where |ğ‘¢ğ‘– | denotes the length of ğ‘¢ğ‘– , and ğœƒ is a predefined threshold
(We empirically set ğœƒ = 5 in this study). ğ‘¢ğ‘˜ is the utterance that is
randomly selected from the entire dialog corpus with a length less
than ğœƒ . SR(ğ‘¢ğ‘– ) denotes the synonym-replacement operation that
has been widely used by NLP text augmentation task [76]. After all
utterances in dialog ğ· are processed, we then obtain a new dialog
ğ·ğ‘ğ‘¢ğ‘” = {ğ‘¢1 â€², ğ‘¢2 â€², ..., ğ‘¢ğ‘› â€²}.

To achieve data balancing, for each project, we first augment the
NBR dialogs to a certain number, then we augment BR dialogs to
match the same number. Taking the Angular project as an example,
we first augment the NBR dialogs from 179 to 358 (2 times), then
augment the BR dialogs from 86 to 358 for balancing purposes.

3.2 Utterance Embedding
The utterance embedding aims to encode semantic information of
words, as well as to learn the representation of utterances.

Word encoding. We encode each word in utterances into a
semantic vector by utilizing the deep pre-trained BERT model [21],
which has achieved impressive success in many natural language
processing tasks [47, 72]. The last layer of the BERT model outputs
a 768-dimensional contextualized word embedding for each word.
Utterance encoding. With all the word vectors, we use TextCNN
[77] to learn the utterance representation. TextCNN is a classical
method for sentence encoding by using a shallow Convolution
Neural Network (CNN) [38] to learn sentence representation. It
has an advantage over learning on insufficient labeled data, since

1Due to space, experimental details on evaluation existing disentanglement models
are provided on our website [2].

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

Fig. 2: Overview of BugListener.

it employs a concise network structure and a small number of pa-
rameters. We use four different size convolution kernels with 100
feature maps in each kernel. The convoluted features are fed to a
Max-Pooling layer followed by the ReLU activation [50]. Then, we
concatenate these features and input them into a 100-dimensional
full-connected layer to obtain the 100-dimensional utterance em-
bedding (cid:174)ğ‘¢ğ‘– . After encoding all the utterances of a dialog ğ·, we can
get utterance-embedded dialog ğ· â€² = { (cid:174)ğ‘¢1, (cid:174)ğ‘¢2, .., (cid:174)ğ‘¢ğ‘› }.

where ğ‘Šğ‘’ is a trainable matrix used to perform linear feature trans-
formation on vertex, ğ‘ (ğ‘–,âˆ—) denotes the set of vertices that vertex
ğ‘£ğ‘– points to.

Edge Type. We define the type of the edge ğ‘’ğ‘– ğ‘— as ğ‘¡ğ‘– ğ‘— âˆˆ T , accord-
ing to the developer-role dependency of ğ‘’ğ‘– ğ‘— . Specifically, we consider
four types of edges in this study, i.e., ğ‘Ÿ â†’ ğ‘Ÿ, ğ‘Ÿ â†’ ğ‘‘, ğ‘‘ â†’ ğ‘Ÿ , and
ğ‘‘ â†’ ğ‘‘, where ğ‘Ÿ denotes the reporter, ğ‘‘ denotes the discussant, as
we defined in the previous section.

3.3 Graph-based Context Embedding
This step aims to capture the graphical context of utterances in one
dialog. Given the utterance-embedded dialog ğ· â€² = { (cid:174)ğ‘¢1, (cid:174)ğ‘¢2, .., (cid:174)ğ‘¢ğ‘› }
with the set of â€œreply-toâ€ relationship R, we first construct a dialog
graph ğº (ğ· â€²). Then, we learn the contextual information of ğº (ğ· â€²)
via a two-layer graph neural network, and output ğºğ‘ (ğ· â€²) where
each vertex in ğºğ‘ (ğ· â€²) restores the contextual information of the
corresponding vertex in ğº (ğ· â€²). Finally, We concatenate each vertex
in ğº (ğ· â€²) with its corresponding vertex in ğºğ‘ (ğ· â€²), and output the
sequence of combination as the dialog vector ğ¶ = { (cid:174)ğ‘1, (cid:174)ğ‘2, ..., (cid:174)ğ‘ğ‘› }.
3.3.1 Construct Dialog Graph. Given the utterance-embedded di-
alog ğ· â€² consisting of ğ‘ utterances and the set of â€œreply-toâ€ rela-
tionship R, we construct a directed graph ğº (ğ· â€²) = (V, E, W, T ),
where V is the vertex set, E is the edge set, W is the weight set of
edges, and T is the set of edge types. More specifically:

Vertex. Each utterance is represented as a vertex ğ‘£ğ‘– âˆˆ V. We use
the utterance embedding (cid:174)ğ‘¢ğ‘– to initialize the corresponding vertex
ğ‘£ğ‘– . The ğ‘£ğ‘– will be updated during the graph learning process.

Edge. We construct the edge set E based on the â€œreply-toâ€ re-
lationship. The edge ğ‘’ğ‘– ğ‘— âˆˆ E denotes that there is a â€œreply-toâ€
relationship between ğ‘¢ğ‘– and ğ‘¢ ğ‘— .

Edge Weight. The edge weight ğ‘¤ğ‘– ğ‘— is the weight of the edge
ğ‘’ğ‘– ğ‘— , with 0 â‰¤ ğ‘¤ğ‘– ğ‘— â‰¤ 1, where ğ‘¤ğ‘– ğ‘— âˆˆ W and ğ‘–, ğ‘— âˆˆ [1, 2, ..., ğ‘ ].
ğ‘¤ğ‘– ğ‘— is determined by the similarity of (cid:174)ğ‘¢ğ‘– and (cid:174)ğ‘¢ ğ‘— . Specifically, we
employ pair-wise dot product to compute the similarity score of
pair vertices. Then, we normalize the similarity score and calculate
the edge weight ğ‘¤ğ‘– ğ‘— :

ğ‘¤ğ‘– ğ‘— =

T Â· ğ‘Šğ‘’ (cid:174)ğ‘¢ ğ‘—
(cid:174)ğ‘¢ğ‘–

T Â· ğ‘Šğ‘’ (cid:174)ğ‘¢ğ‘˜

(cid:174)ğ‘¢ğ‘–
(cid:205)
ğ‘˜ âˆˆğ‘ (ğ‘–,âˆ—)

(2)

3.3.2 Embed Dialog Graph Context. Given a dialog graph ğº (ğ· â€²),
we employ a two-layer graph neural network (GNN) [59] to embed
the graph context of dialog structure and developer-role depen-
dency, respectively. We output ğºğ‘ (ğ· â€²) where each vertex restores
graph context information.

Structure-level GNN. In the first layer, a basic GNN [31] is
used to learn the structure-level context for each vertex in a given
graph, including embedding its neighbor vertices via the â€œreply-toâ€
edges, as well as the features contained in the neighbor vertices.

A basic GNN layer can be implemented as follows:

ğ‘£ğ‘–

(ğ‘™+1) = ğœ

(cid:18)
ğ‘Š (ğ‘™)
1

ğ‘£ğ‘–

(ğ‘™) + ğ‘Š (ğ‘™)
2

âˆ‘ï¸

(cid:19)

(ğ‘™)

ğ‘£ ğ‘—

(3)

ğ‘— âˆˆğ‘ (âˆ—,ğ‘– )

where ğ‘ (âˆ—,ğ‘–) denotes the set of neighboring vertices that point
to vertex ğ‘£ğ‘– . ğ‘£ğ‘– (ğ‘™) represents the updated vertex at layer ğ‘™, and
ğ‘£ğ‘– (ğ‘™+1) represents the updated vertex at layer ğ‘™ + 1. ğœ denotes a
non-linear function, such as sigmoid or ReLU, ğ‘Š (ğ‘™)
are
trainable parameter matrices. We introduce the edge weights to
better aggregate the local information. Hence, the updated vertex
ğ‘£ğ‘– (1) of the structure-level GNN layer is calculated as:

and ğ‘Š (ğ‘™)

2

1

ğ‘£ğ‘–

(1) = ğœ

(cid:18)
ğ‘Š (1)
1

(cid:174)ğ‘¢ğ‘– + ğ‘Š (1)

2

âˆ‘ï¸

(cid:19)

ğ‘¤ ğ‘—ğ‘– (cid:174)ğ‘¢ ğ‘—

(4)

ğ‘— âˆˆğ‘ (âˆ—,ğ‘– )

where ğ‘¤ ğ‘—ğ‘– denotes the edge weight from vertex ğ‘£ ğ‘— to vertex ğ‘£ğ‘– .

Role-level RGCN. In the second layer, we further capture the
high-level contextual information by leveraging Relational Graph
Convolutional Networks (RGCN) [60]. RGCN is a generalization
of Graph Convolutional Networks (GCN) [36] which extends the
hierarchical propagation rules and takes the edge types between

â„4â„3â„ğ‘–â„2ğ‘¢2ğ‘¢ğ‘–ğ‘¢4ğ‘¢3XXX3.2. Utterance Embedding3.1 Dialog Disentanglementand AugmentationDialog (D)3.3 Graph-based Context EmbeddingDialog (Dâ€™)Construct Dialog Graph Gğ‘¢1â€¦â„1â€¦BERTTextCNNğ‘¢1ğ‘¢ğ‘–ğ‘¢ğ‘›â€¦â€¦â€¦â€¦ğ‘¢1ğ‘¢ğ‘–ğ‘¢ğ‘›Embed Graph Contextâ€¦ğ‘¢1â„1ğ‘¢ğ‘›â„ğ‘›â€¦Label=NBR3.4 Dialog Embedding and Classification3.5 Bug Reports SynthesisDialog (D)Chat logData AugmentationAugmented Dialog DatasetDialog DatasetDialogDisentanglementâ€¦Dialog embeddingFC LayerSum PoolingSoftmaxğ‘1ğ‘ğ‘›Label = BRMax PoolingDes: XXXXOB: XXXXEB: XXXXSR: XXXXBug ReportOBLarge Annotated5,067 Bug Reports( Externel)ğ‘¼ğ’“â€²ğ‘¼ğ’“EBSRDESBug Report Dialogs( Internel)gEnglish Wikipedia and BooksCorpus( Externel)Prune Reporterâ€™s utterancesFC LayerFine-tuned BERTFC LayerTwice Fine-tuned BERTPre-trained BERTBugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

vertices into account. Since RGCN explicitly models the neighbor-
hood structures, it can better handle multi-relational graph data
like our dialog graph, which contains four edge types. The vertex ğ‘£ğ‘–
is updated by applying the RGCN over the output of the first layer.

knowledge from the pre-trained model [21]. Specifically, we use
a pre-trained BERT and fine-tune it twice using the external BEE
dataset and our BRS dataset, as shown in the dashed box of â€˜3.5â€™ in
Fig. 2.

ğ‘£ğ‘– = ğœ

(cid:18)
ğ‘Š (2)
1

ğ‘£ğ‘–

(1) +

âˆ‘ï¸

âˆ‘ï¸

(cid:19)

(1)

ğ‘Š (2)
ğ‘¡

ğ‘£ ğ‘—

1
ğ‘ğ‘–,ğ‘¡

(5)

ğ‘¡ âˆˆğ‘‡

ğ‘— âˆˆğ‘ ğ‘¡

(âˆ—,ğ‘–)

(âˆ—,ğ‘– )
where ğ‘ ğ‘¡
denotes the set of vertices that point to vertex ğ‘£ğ‘– under
edge type ğ‘¡ âˆˆ T . ğ‘ğ‘–,ğ‘¡ is a normalization constant that can either be
learned or set in advance (such as ğ‘ğ‘–,ğ‘¡ = |ğ‘ ğ‘¡
|). ğœ denotes a non-
linear function, ğ‘Š (ğ‘™)
are trainable parameter matrices,
the latter matrix changes under different edge types. The output
of role-level RGCN is the ğºğ‘ (ğ· â€²) where each vertex ğ‘£ğ‘– restores the
embedded graph context (cid:174)â„ğ‘– for utterance ğ‘¢ğ‘– .

and ğ‘Š (ğ‘™)

(âˆ—,ğ‘–)

2

ğ‘¡

3.3.3 Combined representation. To enrich the utterance represen-
tation, we concatenate each vertex in ğº (ğ· â€²) with its corresponding
vertex in ğºğ‘ (ğ· â€²), and output the sequence of combination as the
dialog vector ğ¶ = { (cid:174)ğ‘1, (cid:174)ğ‘2, ..., (cid:174)ğ‘ğ‘› }, where (cid:174)ğ‘ğ‘– = [ (cid:174)ğ‘¢ğ‘– âŠ• (cid:174)â„ğ‘– ], and âŠ• is the
concatenation operator.

3.4 Dialog Embedding and Classification
This step aims to obtain the representation of an entire dialog and
classify it as either a positive or a negative bug-report dialog.

Dialog Embedding. We input the dialog vector ğ¶ = { (cid:174)ğ‘1, (cid:174)ğ‘2, ..., (cid:174)ğ‘ğ‘› }
to the Sum-Pooling and the Max-Pooling layer respectively. Then,
we concatenate the output vectors to get the dialog embedding (cid:174)ğ‘”:

(cid:174)ğ‘” =

|ğ‘‰ |
âˆ‘ï¸

ğ‘–=1

(cid:174)ğ‘ğ‘– âŠ• Maxpooling( (cid:174)ğ‘1, ..., (cid:174)ğ‘ğ‘›)

(6)

where âŠ• is the concatenation operator, |ğ‘‰ | is the number of the
graphâ€™s vertices.

Dialog Classification. The label is predicted by feeding the
dialog embedding (cid:174)ğ‘” into two Full-Connected (FC) layers followed
by the Softmax function:

P = softmax(ğ¹ğ¶2 (ReLU(ğ¹ğ¶1 ( (cid:174)ğ‘”ğ‘’ ))))
where P is the 2-length vector [ğ‘ƒ (NBR|ğ·), ğ‘ƒ (BR|ğ·)], the ğ‘ƒ (NBR|ğ·)
is the predicted probability of non-bug-report dialog, the ğ‘ƒ (BR|ğ·)
is the predicted probability of bug-report dialog.

(7)

Finally, we minimize the loss through the Focal Loss [43] func-
tion. The Focal Loss improves the standard Cross-Entropy Loss by
adding a focusing parameter ğ›¾ â‰¥ 0. It focuses on training on hard
examples, while down-weight the easy examples.
âˆ‘ï¸
ğ›¼ğ‘– (1 âˆ’ Pğ‘– )ğ›¾ğ‘¦ğ‘– log(Pğ‘– )

ğ¹ ğ¿ = âˆ’

(8)

ğ‘–

where ğ‘¦ğ‘– is the ğ‘–-th element of the one-hot ground-truth label (BR
or NBR), ğ›¼ğ‘– and ğ›¾ are tunable parameters.

3.5 Bug Report Synthesis
Due to the high volume of live chat data and the low proportion of
ground-truth bug-report dialogs, it is difficult to get enough training
data for bug report synthesis task. To address this challenge, we
utilize a twice fine-tuned BERT model, which proves to be effective
to improve performance through more sophisticated transferring

(1) Initial Fine-tuning BERT model. The BERT model is a
bidirectional transformer using a combination of Masked Language
Model and Next Sentence Prediction. It is trained from English Wikipedia
(2,500M words) and BooksCropus (800M words) [79]. The entire
BERT model is a stack of 12 BERT layers with more than 100 million
parameters.

Based on an assumption that the contents of bug reports are
likely from the reportersâ€™ utterances, we perform the initial fine-
tune on the task of classifying bug-report contents into OB, EB, SR,
and Others. First, we select the external BEE dataset proposed by
Song et al. [68] that includes 5,067 bug reports, 11,776 OB sentences,
1,568 EB sentences, and 24,655 SR sentences as the source dataset.
Second, following the previous study [68], we preprocess sentences
in the 5,076 bug reports with lowercase, tokenization, excluding non-
English and overlong (over 200 words) ones. Third, we freeze the
first nine layers of the pre-trained BERT and update the parameters
of the last three layers via the sentences in the 5,076 bug reports. We
take the output of the first token (the [CLS] token) as the sentence
embedding. Finally, we input the sentence embedding into a FC
layer to produce the probabilities of OB (ğ‘ƒğ‘ ), EB (ğ‘ƒğ‘’ ), SR (ğ‘ƒğ‘  ), and
Others (ğ‘ƒğ‘œ ). We apply Cross-Entropy Loss when measuring the
difference between truth and prediction:

ğ¿ğ‘œğ‘ ğ‘  = âˆ’(ğ‘¦ğ‘ğ‘™ğ‘œğ‘”(ğ‘ƒğ‘ ) + ğ‘¦ğ‘’ğ‘™ğ‘œğ‘”(ğ‘ƒğ‘’ ) + ğ‘¦ğ‘ ğ‘™ğ‘œğ‘”(ğ‘ƒğ‘  ) + ğ‘¦ğ‘œğ‘™ğ‘œğ‘”(ğ‘ƒğ‘œ ))

(9)

where ğ‘¦ğ‘ , ğ‘¦ğ‘’ , ğ‘¦ğ‘  , and ğ‘¦ğ‘œ indicate the ground-truth labels of sen-
tences.

(2) Twice fine-tuning BERT model. Given the above fine-
tuned BERT model, we perform the second round of fine-tuning
on our BRS dataset as follows. We first collect all the reporterâ€™s
utterances ğ‘ˆğ‘Ÿ in Dialog ğ· as our inputs. Since ğ‘ˆğ‘Ÿ may contain
trivial contents that are less meaningful for reporting bugs, we
prune the ğ‘ˆğ‘Ÿ into ğ‘ˆ â€²
ğ‘Ÿ if they satisfy the following heuristic rules: (1)
remove the sentence ğ‘  if: (ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘ ) â‰¤ 5) AND (ğ‘  does not contain
[URL], [EMAIL], [HTML], [CODE] or [VERSION]) (2) remove the
string ğ‘ ğ‘¡ğ‘Ÿ from its sentence if: âˆ€ğ‘ ğ‘¡ğ‘Ÿ âˆˆ {â€œHiâ€, â€œHi Allâ€, â€œhey thereâ€, â€œHi
everybodyâ€, â€œhey guysâ€, â€œhi guysâ€, â€œguysâ€, â€œHi thereâ€, â€œthank youâ€,
â€œthanksâ€, â€œthanks anywayâ€, â€œthanks for replayingâ€, â€œok, thanksâ€, etc.}.
Second, we transfer the BERT model previously fine-tuned on the
external bug report dataset for initialization, and replace the origi-
nal FC layer with a new one. Third, the BERT model is fine-tuned
the second time via labeled sentences in ğ‘ˆ â€²
ğ‘Ÿ using a smaller learning
rate.

(3) Bug reports assembling. When generating bug reports,
we assemble sentences that are predicted to the same category in
chronological order. To fully retain the useful information in ğ‘ˆ â€²
ğ‘Ÿ ,
we assemble all the sentences that belong to the â€œOthersâ€ category
as the description paragraph. In the end, we could generate a bug
report with its description, observed behavior, expected behavior,
and step to reproduce according to best practices for bug reporting
[8, 80].

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

4 EXPERIMENTAL DESIGN
To evaluate the proposed BugListener approach, our evaluation
specifically addresses three research questions:

RQ1: How effective is BugListener in identifying bug-report

dialogs from live chat data?

RQ2: How effective is BugListener in synthesizing bug reports?
RQ3: How does each individual component in BugListener con-

tribute to the overall performance?

4.1 Data Preparation
Studied Communities. Many OSS communities utilize Gitter
4.1.1
[27] or Slack [28] as their live communication means. Considering
the popular, open, and free access nature, we select studied com-
munities from Gitter2. Following previous work [51, 63] , we select
popular and active communities as our studied subjects. Specifically,
we select the Top-1 most participated communities from six active
domains, covering front end framework, mobile, data science, De-
vOps, collaboration, and programming language. Then, we collect
the live chat utterances from these communities. Gitter provides
REST API [26] to get data about chatting rooms and post utterances.
In this study, we use the REST API to acquire the chat utterances
of the six selected communities, and the retrieved dataset contains
all utterances as of â€œ2020-12-31â€.

4.1.2 Preprocessing and Disentanglement. For data preprocessing,
we first convert all the words in utterances into lowercase, and
remove the stopwords. We also normalize the contractions in utter-
ances with contractions [37] library and use Spacy [4] for lemmati-
zation. Following previous work [10, 71], we replace the emojis with
specific strings to standard ASCII strings. Besides, we detect low
frequency tokens such as URL, email address, code, HTML tag, and
version number with regular expressions, and substitute them into
[URL], [EMAIL], [HTML], [CODE], and [VERSION] respectively.
Then, we use the FF model [39] to divide the processed data into
individual dialogs as introduced in Sec. 3.1.1. The detailed statistic
is shown in the â€œEntire Populationâ€ column of Table 1.

Sampling and Filtering. After dialog disentanglement, the
4.1.3
number of individual chat dialogs remains quite large. Limited by
the human resource of labeling, we randomly sample 100 dialogs
from each community. The sample population accounts for about
1.1% of the entire population. Although the ratio is not large, we
consider the selected dialogs are representative because they are
randomly selected from six diverse communities. The details of
sampling results are shown in the â€œSample Populationâ€ column of
Table 1.

Since BugListener relies on natural language processing to un-
derstand the dialog, dialogs that have too much noise or do not
contain enough information are almost incomprehensible and thus
cannot decide a bug report. Following the data cleaning procedures
of previous studies [51, 63], we excluded noisy dialogs by apply-
ing the following exclusion criteria: 1) Dialogs that are written in
non-English languages; 2) Dialogs where the code or stack traces ac-
counts for more than 90% of the entire chat content; 3) Low-quality
dialogs such as dialogs with many typos and grammatical errors.

2In Slack, communities are controlled by the team administrators, whereas in Gitter,
access to the chat data is public

4) Dialogs that involve channel robots which main handle simple
greeting or general information messages.

4.1.4 Ground-truth Labeling. For each sampled dialog obtained in
the previous step, we label ground-truth data from three aspects: (1)
Correct disentanglement results. For each sampled dialog, we man-
ually correct the prediction of the â€œreply-toâ€ relationships between
utterances, as well as the disentanglement results. (2) Label dialogs
with BR and NBR (See the â€œSample Populationâ€ column in Table 1).
For each dialog that has been manually corrected, we manually la-
bel it with a â€œBRâ€ or an â€œNBRâ€ tag, according to whether it discusses
a certain bug that should be reported. (3) Label sentences with OB,
EB, and SR (See the â€œBRS Datasetâ€ column in Table 1). For each
dialog labeled with BR, we first prune all reporterâ€™s utterances ğ‘ˆğ‘Ÿ
to obtain ğ‘ˆ â€²
ğ‘Ÿ as described in Sec. 3.5(2). Then we label each sentence
in ğ‘ˆ â€²
ğ‘Ÿ with observed behavior (OB), expected behavior (EB), and
step to reproduce (SR), according to their contents.

To ensure the labeling validity, we built an inspection team,
which consisted of four PhD students. All of them are fluent Eng-
lish speakers, and have done either intensive research work with
software development or have been actively contributing to open-
source projects. We divided them into two groups. The results from
both groups were cross-checked and reviewed. When a labeled
result received different opinions, we hosted a discussion with all
team members to decide through voting. Based on our observation,
the correctness of automated dialog disentanglement is 79%. The
average Cohenâ€™s Kappa about bug report identification is 0.87, and
the average Cohenâ€™s Kappa about bug report synthesis is 0.84.

4.1.5 Dataset augmentation and balancing. For BRI task, we aug-
ment the dataset as introduced in Sec. 3.1. For each project, we
first augment the NBR data eight times, and then augment the BR
data until BR and NBR data are balanced. The details are shown
in the â€œBRI Datasetâ€ column in Table 1. For BRS task, we apply
EDA[76] techniques to augment OB, EB, SR sentences until their
numbers are balanced. We further incorporate an external dataset
for transfer learning. The external dataset is provided by Song et al.
[68], including 5,067 bug reports with 11,776 OB sentences, 1,568
EB sentences, and 24,655 SR sentences.

4.2 Baselines
The first two RQs require comparison with state-of-the-art baselines.
We employ four common machine-learning-based baselines appli-
cable to both RQ1 and RQ2, including Naive Bayesian (NB) [48],
Random Forest (RF) [41], Gradient Boosting Decision Tree
(GBDT) [34], and FastText [33]. In addition, we employ several
baselines applicable to RQ1 and RQ2, respectively.

Additional Baselines for identifying bug-report dialogs (
RQ1). Furthermore, we also consider some existing approaches
that can identify sentences or mini-stories which are discussing
problems. CNC [32] is the state-of-the-art learning technique to
classify sentences in comments taken from online issue reports.
They proposed a CNN [38]-based approach to classify sentences
into seven categories of intentions: Feature Request, Solution Pro-
posal, Problem Discovery, etc. To achieve better performance of the
CNC baseline, we retrain the CNC model on our BRI dataset. We as-
semble all the utterances in a dialog as an entry, and predict whether

BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Table 1: Our Experiment Dataset. (Part, Dial, Uttr, Sen are short for participating developers, dialog, utterance, and sentence,
respectively. BR and NBR denote bug-report and non-bug-report dialogs. ğ‘ˆğ‘Ÿ denotes sentences in reporterâ€™s utterances, and
ğ‘ˆ â€²
ğ‘Ÿ denotes the pruned ğ‘ˆğ‘Ÿ .)

Entire Population

Part.
22,467
3,979
8,810
8,310
9,260
8,318
61,144

Dial.
79,619
4,906
3,964
27,256
7,452
18,812
142,009

Uttr
695,183
29,039
22,367
252,846
34,147
196,513
1,230,095

Project
Angular
Appium
Docker
DL4J
Gitter
Typescript

Total

Sample Population
BR
NBR

Dial. Uttr. Dial. Uttr
268
179
233
169
185
172
373
178
304
207
85
203
1,448
1,108

1,043
737
916
1,070
813
1,016
5,595

86
84
61
79
63
20
393

BRI Dataset
Augmented NBR Augmented BR
Uttr
1,132
935
1,037
1,781
1,898
1,625
8,408

Uttr.
2,086
1,474
1,832
2,140
1,626
2,032
11,190

Dial.
358
338
344
356
414
406
2,216

Dial.
358
338
344
356
414
406
2,216

BR Dialog
Sen.
Dial.
647
86
596
84
438
61
828
79
733
63
176
20
3418
393

BRS Dataset

Reporter Sen.

BR content

Ur
507
478
367
590
432
138
2,512

Urâ€™
446
397
322
502
369
118
2,154

OB EB
34
177
29
180
35
150
32
184
19
159
12
48
161
898

SR DES
195
40
144
44
105
32
230
56
176
15
50
8
900
195

Table 2: Baseline comparison across the six communities for bug-report dialog identification (%).

Methods

BugListener
NB
GBDT
RF
FastText
CNC
DECA
Casper

Angular
R
79.07
73.26
60.47
59.30
52.33
52.33
45.35
53.49

F1
80.95
65.28
65.82
66.23
62.50
63.38
48.15
59.74

P
82.93
58.88
72.22
75.00
77.59
80.36
51.32
67.65

Appium
R
80.95
66.67
69.05
67.86
72.62
70.24
52.38
85.71

F1
74.73
64.37
67.05
69.94
70.52
68.60
51.76
74.61

P
69.39
62.22
65.17
72.15
68.54
67.05
51.16
66.06

Docker
R
78.69
31.15
54.10
36.07
49.18
62.29
59.02
70.49

P
77.42
65.52
66.00
68.75
56.60
74.51
45.57
60.56

F1
78.05
42.22
59.46
47.31
52.63
67.86
51.43
65.15

P
85.07
62.79
85.00
72.73
74.51
84.44
42.22
82.14

DL4J
R
72.15
34.18
64.56
20.25
48.10
48.10
48.10
58.23

F1
78.08
44.26
73.38
31.68
58.46
61.29
44.97
68.15

Gitter
R
87.30
55.56
82.54
76.19
61.90
71.43
49.20
69.84

P
82.09
72.92
59.77
62.34
67.24
68.18
55.36
73.33

F1
84.62
63.06
69.33
68.57
64.46
69.77
52.10
71.54

Typescript
R
70.00
30.00
65.00
30.00
45.00
65.00
55.00
65.00

F1
70.00
32.43
45.61
40.00
42.86
57.78
30.99
43.33

P
70.00
35.29
35.14
60.00
40.91
52.00
21.57
32.50

Average
R
78.03
48.47
65.95
48.28
54.86
61.57
51.51
67.13

P
77.82
59.60
63.88
68.50
64.23
71.09
44.53
63.71

F1
77.74
51.94
63.44
53.96
58.57
64.78
46.57
63.75

the entry belongs to problem discovery. DECA [69] is the state-of-
the-art rule-based technique for analyzing development emails. It
is used to classify the sentences of emails into problem discovery,
solution proposal, information giving, etc., by using linguistic rules.
We use the twenty-eight linguistic rules [61] for identifying the
â€œproblem discoveryâ€ utterances in a dialog and regard the dialog
containing the â€œproblem discoveryâ€ utterances as the bug-report
dialog. Casper [30] is a method for extracting and synthesizing
user-reported mini-stories regarding app problems from reviews.
Similar to the CNC baseline, we also retrain the Casper model on
the BRI dataset, and apply it to determine bug-report dialogs by
assembling all the utterances in a dialog as one entry.

Additional Baseline for synthesizing bug reports (RQ2).
We investigated seven state-of-the-art approaches for the bug report
synthesis task, including CUEZILLA [80], DeMlBUD [12], iTAPE
[17], S2RMiner [78], infoZilla [9], Euler [11] and BEE [68]. Among
the above approaches, only the replication packages from iTAPE,
S2RMiner, and BEE are available. Since iTAPE and S2RMiner clas-
sify SR sentences, and only BEE share the same target with us,
that is to classify OB, EB, SR, and Other sentences for bug reports.
Therefore, we choose BEE as our additional baselines for bug report
synthesis. BEE comprises three binary classification SVM, which
can tag sentences with OB, EB, or SR labels.

This leads to a total of seven baselines for RQ1, and five baselines

for RQ2.

4.3 Evaluation Metrics
We use three commonly-used metrics to evaluate the performance
of both two tasks, i.e., Precision, Recall, and F1. (1) Precision refers to
the ratio of the number of correct predictions to the total number
of predictions; (2) Recall refers to the ratio of the number of correct
predictions to the total number of samples in the golden test set;
and (3) F1 is the harmonic mean of precision and recall. When

comparing the performances, we care more about F1 since it is
balanced for evaluation.

4.4 Experiment Settings
The experimental environment is a desktop computer equipped
with an NVIDIA GeForce RTX 3060 GPU, intel core i5 CPU, 12GB
RAM, running on Ubuntu OS.

For RQ1, we apply Cross-Project Evaluation on our BRI dataset to
perform the training process. We iteratively select one project as a
test dataset, and the remaining five projects for training. We train
BugListener with 32 batch_size. We choose Adam as the optimizer
with learning_rate=1e-4. To avoid over-fitting, we set dropout=0.5,
and adopt the L2-regularization with ğœ†=1e-5. The ğ›¼ and ğ›¾ of Focal
Loss function are 0 and 2, respectively. When training GBDT, we
set the learning_rate=0.1 and the n_estimators=100; For RF, we set
the min_samples_leaf =10 and the n_estimators=100; We train 100
epochs for FastText, and set the learning_rate=0.1, the window
size of input n-gram as 2; Casper chooses SVM.SVC as the default
function, with rbf as the kernel, 3 as the degree, and 200 as the
cache_size; CNC selects 32 as the batch_size, 128-dimensional word
embedding, four different filter sizes of [2, 3, 4, 5] with 128 filters,
30 training epochs, and dropout=0.5. For these hyper-parameters,
we use greedy search [40] as the parameter selection method to
obtain the best performance.

For RQ2, in the first fine-tune round, we train BugListener on the
external BEE dataset (see Sec. 4.1.5) with 64 batch_size. We set the
warmup proportion of BERT model to 0.1, and the value of gradient
clip to 1.0. We choose Adam as the optimizer with learning_rate=1e-
4 and weight decay rate=0.01. We train BugListener for 13 epochs
and save the best model. In the second fine-tune round, we use
the same parameters while changing the batch_size from 64 to 8,
the epoch from 13 to 70, and the learning_rate from 1e-4 to 1e-6.
We apply a 10-fold partition on the BRS dataset to perform the
secondary fine-tuning, i.e., we use nine folds for fine-tuning, and

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

the remaining one for testing. For NB/GDBT/RF/FastText baselines,
we use the greedy strategy to tune parameters to achieve the best
performance. For the additional baseline BEE, we directly utilize
its open API [67] to predict OB, EB, and SR sentences.

For RQ3, we compare BugListener with its two variants in bug re-
port identification task: 1) BugListener w/o CNN, which removes
the TextCNN. 2) BugListener w/o GNN, which removes the graph
neural network. BugListener with its two variants use the same pa-
rameters when training. We compare BugListener with its variant
without transferring knowledge from the external BEE dataset (i.e.,
BugListener w/o TL) in bug report synthesis task. BugListener
w/o TL has the same network structure with BugListener, but it
does not use the external BEE dataset and is only fine-tuned on our
BRS dataset.

5 RESULTS AND ANALYSIS
5.1 Performance in Identifying Bug Reports
Table 2 shows the comparison results between the performance
of BugListener and those of the seven baselines across data from
six OSS communities, for BRI tasks. The columns correspond to
Precision, Recall, and F1. The highlighted cells indicate the best
performance from each column. Then, we conduct the normality
test and T-test between every two methods. Overall, the data follows
a normal distribution, and BugListener significantly (ğ‘ âˆ’ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ <
0.01) outperforms the seven baselines on F1. Specifically, when
comparing with the best Precision-performer among the seven
baselines, i.e., CNC, BugListener can improve its average precision
by 6.73%. Similarly, BugListener improves the best Recall-performer,
i.e., Casper, by 10.90% for average recall, and improves the best F1-
performer, i.e., CNC, by 12.96% for average F1. At the individual
project level, BugListener can achieve the best F1-score in all six
communities.

For BRI tasks, we believe that the performance advantage of
BugListener is mainly attributed to the rich representativeness of
its internal construction, from two perspectives: (1) BugListener
models the textual dialog as the dialog graph thereby can effec-
tively exploit the graph-structured knowledge. While the structure
information is missing in the baseline methods that treat a dialog
as a linear structure. (2) BugListener leverages a novel two-layer
GNN model with considering the edge types between utterances to
learn a high-level contextual representation. Thus it can capture
the latent semantic relations between utterances more accurately.
Answering RQ1: On average, BugListener has the best preci-
sion, recall, and F1, i.e., 77.82%, 78.03%, and 77.74%, improving the
best F1-baseline CNC by 12.96%. On individual projects, it also out-
performs the other baselines with achieving the best F1-score in all
six communities.

5.2 Performance in Synthesizing Bug Reports
Fig. 3 summarizes the comparison results between the average
performance of BugListener and the five baselines, for BRS task.
We can see that, BugListener can achieve the highest performance
in predicting OB, EB, and SR sentences. It outperforms the six
baselines in terms of F1. For predicting OB sentences, it reaches the
highest F1 (84.63%), improving the best baseline FastText by 9.32%.
For predicting EB sentences, it reaches the highest F1 (71.46%),

Fig. 3: Baseline comparison for bug report synthesis.

improving the best baseline FastText by 12.21%. For predicting SR
sentences, it reaches the highest F1 (73.13%), improving the best
baseline FastText by 10.91%.

Our approach is more effective to classify OB, EB, and SR sen-
tences in live chats than others, mainly due to two reasons: (1) By
leveraging the transfer learning technique, BugListener can obtain
general knowledge from existing bug reports, thus would further
boost the classification performances on the limited resource. (2)
By employing the state-of-the-art BERT model which has a strong
ability to learn semantics via the transformer structure, BugListener
can capture richer semantic features in word and sentence vectors.
We notice that FastText achieve the second performances. These
results are mainly due to that, FastText can better understand the
context by capturing the neighbor words using a fixed-size window
when embedding words. we also notice that BEE performs the worst
on predicting EB (average F1 is only 7%). These results are mainly
due to that, BEE is trained from the external normal bug reports
dataset, and the expression style for EB sentences is quite different
between those in normal bug reports and those in live conversations.
The EB sentences in bug reports are likely expressed in a declarative
tone that state the reporterâ€™s expectation as an objective fact, e.g., â€œI
wish docker can save disk usageâ€. While in live chats, EB sentences
are more likely expressed in an interrogative tone that the reporters
inquiry or ask for a reply, e.g., â€œCan docker avoid using such huge
disk?â€. Therefore, it is difficult for BEE to predict EB sentences
correctly on live chat data.

Answering RQ2: BugListener outperforms the six baselines
in predicting OB, EB, and SR sentences in terms of F1. The three
categoriesâ€™ average Precision, Recall, and F1 are 75.57%, 77.70%, and
76.40%, respectively.

5.3 Effects of Main Components
Fig. 4 (a) presents the performances of BugListener and its two vari-
ants for BRI task. We can see that, the F1 performance of BugLis-
tener is higher than all two variants across all the six communities.
When compared with BugListener and BugListener w/o GNN, re-
moving the GNN component will lead to a dramatic decrease of the
average F1 (by 17.22%) across all the communities. This indicates
that the GNN is an essential component to contribute to BugLis-
tenerâ€™s high performances. When compared with BugListener and
BugListener w/o CNN, removing the TextCNN component will
lead to the average F1 declines by 13.85%. It is mainly because the
TextCNN model can capture the intra-utterance semantic features,
which improves the classification performance.

0102030405060708090100BugListenerFastTextRFGBDTNBBEEOBEBSR73%85%71%75%59%62%70%56%54%72%54%49%72%53%52%69%7%14%BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

of the communities that they are familiar with. Each bug report
is evaluated by three participants. For each bug report, each par-
ticipant has the following information available: (1) the associated
open source community; (2) the original textual dialogs from Gitter;
(3) the bug report generated by BugListener.

The survey contains three questions: (1) Correctness: Whether
the dialog is discussing a bug that should be reported at that mo-
ment (Yes or No)? (2) Quality: How would you rate the quality of
Description, Observed Behavior, Expected Behavior, and Step to
Reproduce in the bug report (using a five-level Likert scale [18])?
(3) Usefulness: How would you rate the usefulness of BugListener
(using a 5-level Likert scale)?

Results. To validate the correctness of bug reports identified
by BugListener, we ask each participant to determine whether it is
a real bug report and aggregate group decision based on the ma-
jority vote from the three participants. To validate the quality and
usefulness of each identified bug report, we ask each participant
to rate using a scheme from 1-10 and use the average score of the
three evaluations as the final score. Fig. 5(a) shows the bar and
pie chart depicting the correctness of BugListener. Among the 31
bug reports identified by BugListener, 24 (77%) of them are correct,
while 7 (23%) of them are incorrect. The correctness is in line with
our experiment results (80% precision of bug report identification).
The bar chart shows the correctness distributed among the five
communities. The correctness ranges from 63% to 100%. The per-
ceived correctness indicates that BugListener is likely generalized
to other open source communities with a relatively good and sta-
ble performance. Fig. 5(b) shows an asymmetric stacked bar chart
depicting the perceived quality and usefulness of BugListenerâ€™s
bug reports, in terms of description, observed behavior, expected

(a) Correctness

(b) Quality and usefulness

Fig. 5: Results of human evaluation

(a) The BRI performance

(b) The BRS performance

Fig. 4: The component analysis.

Fig. 4 (b) shows the performance of BugListener and its variant
without transferring knowledge from the external BEE dataset for
BRS task. We can see that, without the knowledge transferred
from the external BEE dataset, the F1 will averagely decrease by
2.52%, 6.06%, 3.13% for OB, EB, and SR prediction, respectively. This
indicates that incorporating the transferred external knowledge can
largely increase the performance on EB prediction, while slightly
increase the performance on OB and SR prediction.

Answering RQ3: The GNN, TextCNN, and Transfer Learning
technique adopted by BugListener are helpful for bug report identi-
fication and synthesis.
6 HUMAN EVALUATION
To further demonstrate the generalization and usefulness of our
approach, we apply BugListener on recent live chats from five new
communities: Webdriverio, Scala, Materialize, Webpack, and Pan-
das (note that these are different from our studied communities
so that all data of these communities do not appear in our train-
ing/testing data). Then we invite nine human annotators to assess
the correctness, quality, and usefulness of the bug reports generated
by BugListener.

Human Annotators. We recruit nine participants, including
two PhD students, two master students, three professional devel-
opers and two senior researchers, all familiar with the five open
source communities. They all have at least three years of software
development experience, and four of them have more than ten years
of development experience.

Procedure. First, we crawl the recent one-month (July 2021 to
August 2021) live chats of the five new communities from Gitter,
which contain 3,443 utterances. Second, we apply BugListener to
disentangle and construct the live chats into about 562 separated
dialogs. Among them, BugListener identifies 31 potential bug re-
ports in total3. For each participant, we assign 9-11 bug reports

3Limited by the space, we list the details about the 31 bug reports on our website [2].

25354555657585AngularAppiumDockerDL4JGitterTypescriptBugListenerBugListener w/o CNNBugListener w/o GNN405060708090   SREBOBBugListener w/o TL BugListener73%70%71%65%85%82%116232430246810121416WebdriverioMaterializePandasScaleWebpackCorrectIncorrect     7(23%)   24(77%)-100-75-50-2502550751004%13%33%21%4%71%58%46%62%83%13%25%21%21%25%Percentage(%)DesOBEBSRUsefulnessQualityDissatisfiedSomewhat disstisfiedNot dissatisfiedSomewhat satisfiedSatisfiedICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

behavior, and step to reproduce. We can see that, the high quality
of bug report description is highly admitted, 85% of the responses
agree that the bug report description is satisfactory (i.e., â€œsomewhat
satisfiedâ€ or â€œsatisfiedâ€). The high quality of OB, EB, and S2R are
also moderately admitted (62%, 46%, and 58% on aggregated cases,
respectively). In addition, the usefulness bar chart shows that 71%
of participants agree that BugListener is useful. We will further
discuss where does BugListener perform unsatisfactorily in Sec.7.2.

7 DISCUSSION
Encouraged by the significant advantages of BugListener as shown
in Sec.6, we believe that our approach could facilitate the bug dis-
covering process and software quality improvement. In this section,
we propose potential usage scenarios as well as improvement op-
portunities for future work.

7.1 Potential Usage Scenario
Software Engineering Bots are widely known as convenient ways
for workflow streamlining and productivity improvement [3, 22, 35].
BugListener can be easily incorporated into a collaborative bot on
Gitter, following the basic implementation ideas: first, the OSS
repository owner or core team members who care about the po-
tential bugs could subscribe to their interesting chat rooms via
BugListener; then, BugListener will monitor the corresponding
chat rooms and send potential bug reports periodically; and finally,
for the bug reports that are confirmed by subscribers, BugListener
could automatically pull them to code repositories such as Github or
Gitlab that are well integrated with Gitter. We believe that BugLis-
tener could enhance individual and team productivity as well as
improving software quality.

7.2 Improvement Opportunities
As reported in Sec. 6, 7 out of 31 bug reports are incorrectly labeled
by BugListener. To identify further improvement opportunities
for follow-up studies, we summarized the following special cases
based on examining the human evaluation results that necessitates
further studies to improve the performance of BugListener.

(1) Dialogs with a few or no feedback. We found that 5 out of
the 7 incorrect cases are related to insufficient feedback, i.e., three
monologues, and the other two with less than five utterances in
total. When deciding whether a dialog contains a bug or not, the
feedback provided by other developers is important. For example,
feedback such as â€œit is still not workingâ€ and â€œcould you please
file an issueâ€ likely indicate the discussing bug should be reported.
Therefore, it is difficult for BugListener to predict dialogs with
insufficient feedback. In the future, follow-up research can enrich
the bug report classification by adding different confidence levels:
High and Normal. â€œHighâ€ refers to the bug reports that the reporter
or the discussants have confirmed, and â€œNormalâ€ refers to the bug
reports that have the potential.

(2) Dialogs reflecting user misuse/mistake. We observed
that 2/7 incorrect bug reports are actually associated with instal-
lation and version-update due to the usersâ€™ mistake or negligence.
The difference between â€œBugsâ€ and â€œuser misuse/mistakeâ€ is subtle.
Both of them might contain negative complaints, error stack traces,
and similar keywords such as â€œI get errorsâ€, â€œnot addressed at allâ€,

etc. In the future, follow-up studies are needed to incorporate priori
knowledge (e.g., dialogs discussing installation, updating, or build-
ing issues are likely not reporting bugs.) to better distinguish the
two categories.

7.3 Threats to Validity
The first threat is generalizability. BugListener is only evaluated
on six open-source projects, which might not be representative of
closed-source projects or other open-source projects. The results
may be different if the model is applied to other projects. However,
our dataset comes from six different fields. The variety of projects
relatively reduce this threat.

The second threat may come from the results of automated dia-
log disentanglement. In this study, we manually inspect and correct
the disentanglement results to ensure high-quality inputs for evalu-
ating BugListener. The average correctness is 79% in our inspection.
However, for the fully automatic usage of BugListener, the trade-off
option would be directly adopting the automated disentanglement
results. Thus, in real-world application scenarios without manual
correction, a slight drop in performance might be observed. To
alleviate the threat, four state-of-the-art disentanglement models
are selected and experimented on live chat data. We adopt the best
performing model among the four models, the FF model, to disen-
tangle the live chat. The results of human evaluation study show
that BugListener can achieve 77% precision without manual correc-
tion, and the performance only slightly declined by 3% compared
with BugListener taking the corrected dialogue as input. Therefore,
we believe this can serve as a good foundation for BugListenerâ€™s
fully automatic usage.

The third threat relates to the construct of our approach. First,
we hypothesize that the contents of bug reports likely consist of
reportersâ€™ utterances, which occasionally results in missing context
information. To alleviate the threat, we thoroughly analyzed where
our approach performs unsatisfactorily in Sec. 7.2, and planned
future work for improvement. Second, we enlarge our BRI dataset
by using a heuristic data augmentation, which may alter the se-
mantics of the original dialog. To alleviate the threat, we employ
the utterance mutation from two dimensions (utterance-level and
word-level), which has been commonly used in augmenting the
datasets for NLP tasks [23, 76]. It could reduce semantic changes of
the overall dialogs to a minimum.

The fourth threat relates to the suitability of evaluation metrics.
We utilize precision, recall, and F1 to evaluate the performance.
We use the dialog labels and utterance labels manually labeled
as ground truth when calculating the performance metrics. The
threats can be largely relieved as all the instances are reviewed
with a concluding discussion session to resolve the disagreement in
labels based on majority voting. There is also a threat related to our
human evaluation. We cannot guarantee that each score assigned
to every bug report is fair. To mitigate this threat, each bug report
is evaluated by 3 human evaluators, and we use the average score
of the 3 evaluators as the final score.

8 RELATED WORK
Identifying Bug Reports. Identifying bug reports from user feed-
back timely and precisely is vital for developers to update their

BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

applications. Many approaches have been proposed to identify
bugs or problems from app reviews [24, 25, 30, 45, 46, 58, 74, 75],
mailing lists [69, 70], and issue requests [7, 32, 53, 55, 73]. For ex-
ample, Vu et al. [74] detected emerging mobile bugs and trends
by counting negative keywords based on Google Play. Maalej et
al. [45, 46] leveraged natural language processing and sentiment
analysis techniques to classify app reviews into bug reports, fea-
ture requests, user experiences, and ratings. Scalabrino et al. [58]
developed CLAP to classify user reviews into bug reports, feature
requests, and non-functional issues based on a random forest classi-
fier. Di Sorbo et al. [69, 70] classified sentences in developer mailing
lists into six categories: feature request, opinion asking, problem
discovery, solution proposal, information seeking, and information
giving. Huang et al. [32] addressed the deficiencies of Di Sorbo et
al.â€™s taxonomy by proposing a convolution neural network (CNN)-
based approach. Our work differs from existing researches in that
we focus on identifying bug reports from collaborative live chats,
which pose different challenges as chat messages are interleaved,
unstructured, informal, and typically have insufficient labeled data
than the previously analyzed documents.

Synthesizing Bug Reports. Several efforts have been made to
synthesize bug reports by utilizing heuristic rules automatically
[8, 9, 20, 80]. As heuristic approaches often fail to capture the diverse
discourse in bug reports, learning-based approaches have been pro-
posed [11, 17, 68, 78]. Song et al. [68] proposed a tool that integrates
three SVM models to identify the observed behavior, expected be-
havior, and S2R at the sentence level in bug reports. Zhao et al. [78]
proposed an SVM-based approach that automatically extracts the
textual description of steps to reproduce (S2R) from bug reports.
Chaparro et al. [11] proposed a sequence-labeling-based approach
that automatically assesses the quality of S2R in bug reports. Chen
et al. [17] proposed a seq2seq-based approach that automatically
generates titles regarding the textual bodies written in bug reports.
Most of these methods focus on structuring or synthesizing bug re-
ports from textual descriptions that depicting bugs in a single-party
style, while our approach targets to automatically structure and syn-
thesize bug reports from multi-party conversations, complementing
the existing studies on a novel resource.

Knowledge Extraction from Collaborative Live Chats. Re-
cently, more and more work has realized that collaborative live chats
play an increasingly significant role in software development, and
are a rich and untapped source for valuable information about the
software system [13, 14, 42]. Several studies are focusing on extract-
ing knowledge from collaborative live chats. Chatterjee et al. [15]
automatically collected opinion-based Q&A from online developer
chats. Shi et al. [64] proposed an approach to detect feature-request
dialogues from developer chat messages via the deep siamese net-
work. Qu et al. [56] utilized classic machine learning methods to
predict user intent with an average F1 of 0.67. Rodeghero et al. [57]
presented a technique for automatically extracting information rel-
evant to user stories from recorded conversations. Chowdhury and
Hindle [19] filtered out off-topic discussions in programming IRC
channels by engaging Stack Overflow discussions. The findings of
previous work motivate the work presented in this paper. Our study
is different from the previous work as we focus on identifying and
synthesizing bug reports from massive chat messages that would
be important and valuable information for software evolution. In

addition, our work complements the existing studies on knowledge
extraction from developer conversations.

9 CONCLUSION
In this paper, we proposed a novel approach, named BugListener,
which can automatically identify and synthesize bug reports from
live chat messages. BugListener leverages a novel graph neural net-
work to model the graph-structured information of dialog, thereby
effectively predicts the bug-report dialogs. BugListener also adopts
a twice fine-tuned BERT model by incorporating the transfer learn-
ing technique to synthesize complete bug reports. The evaluation
results show that our approach significantly outperforms all other
baselines in both BRI and BRS tasks. We also conduct a human
evaluation to assess the correctness and quality of the bug reports
generated by BugListener. We apply BugListener on recent live
chats from five new communities and obtain 31 potential bug re-
ports in total. Among the 31 bug reports, 77% of them are correct.
71% of human evaluators agree that BugListener is useful. These
results demonstrate the significant potential of applying BugLis-
tener in community-based software development, for promoting
bug discovery and quality improvement.

ACKNOWLEDGMENTS
We deeply appreciate anonymous reviewers for their constructive
and insightful suggestions towards improving this manuscript. This
work is supported by the National Key Research and Development
Program of China under Grant No. 2018YFB1403400, the National
Science Foundation of China under Grant No. 61802374, 62002348,
62072442, 614220920020 and Youth Innovation Promotion Associa-
tion Chinese Academy of Sciences.

REFERENCES
[1] 2016. Chats in Docker Community.

5866382e058ca96737a943e7/.

https://gitter.im/docker/docker?at=

[2] 2021. BugListener. https://github.com/BugListener/BugListener2022/.
[3] Ahmad Abdellatif, Khaled Badran, and Emad Shihab. 2020. MSRBot: Using Bots
to Answer Questions from Software Repositories. Empir. Softw. Eng. 25, 3 (2020),
1834â€“1863.

[4] Explosion AI. 2019. Spacy. https://spacy.io/.
[5] Rana Alkadhi, Teodora Lata, Emitza Guzman, and Bernd Bruegge. 2017. Rationale
in Development Chat Messages: An Exploratory Study. In Proceedings of the
14th International Conference on Mining Software Repositories, MSR 2017. IEEE
Computer Society, 436â€“446.

[6] Rana Alkadhi, Manuel Nonnenmacher, Emitza Guzman, and Bernd Bruegge.
2018. How do Developers Discuss Rationale?. In 25th International Conference
on Software Analysis, Evolution and Reengineering, SANER 2018. IEEE Computer
Society, 357â€“369.

[7] Deeksha Arya, Wenting Wang, Jin L. C. Guo, and Jinghui Cheng. 2019. Analysis
and Detection of Information Types of Open Source Software Issue Discussions.
In Proceedings of the 41st International Conference on Software Engineering, ICSE
2019, Montreal, QC, Canada, May 25-31, 2019. 454â€“464.

[8] Nicolas Bettenburg, Sascha Just, Adrian SchrÃ¶ter, Cathrin Weiss, Rahul Premraj,
and Thomas Zimmermann. 2008. What Makes a Good Bug Report?. In Proceedings
of the 16th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 308â€“318. https://doi.org/10.1145/1453101.1453146

[9] Nicolas Bettenburg, Rahul Premraj, Thomas Zimmermann, and Sunghun Kim.
2008. Extracting Structural Information from Bug Reports. In Proceedings of the
2008 international working conference on Mining software repositories. 27â€“30.
[10] Isabelle Boutet, Megan LeBlanc, Justin A Chamberland, and Charles A Collin.
2021. Emojis Influence Emotional Communication, Social Attributions, and
Information Processing. Computers in Human Behavior 119 (2021), 106722.
[11] Oscar Chaparro, Carlos Bernal-CÃ¡rdenas, Jing Lu, Kevin Moran, Andrian Marcus,
Massimiliano Di Penta, Denys Poshyvanyk, and Vincent Ng. 2019. Assessing
the Quality of the Steps to Reproduce in Bug Reports. In Proceedings of the ACM
Joint Meeting on European Software Engineering Conference and Symposium on

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Lin Shi et al.

the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
August 26-30, 2019. ACM, 86â€“96. https://doi.org/10.1145/3338906.3338947
[12] Oscar Chaparro, Jing Lu, Fiorella Zampetti, Laura Moreno, Massimiliano Di Penta,
Andrian Marcus, Gabriele Bavota, and Vincent Ng. 2017. Detecting Missing
Information in Bug Descriptions. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering. 396â€“407.

[13] Preetha Chatterjee, Kostadin Damevski, Nicholas A. Kraft, and Lori L. Pollock.
2020. Software-related Slack Chats with Disentangled Conversations. In MSR â€™20:
17th International Conference on Mining Software Repositories. ACM, 588â€“592.

[14] Preetha Chatterjee, Kostadin Damevski, Lori Pollock, Vinay Augustine, and
Nicholas A Kraft. 2019. Exploratory Study of Slack Q&A Chats as a Mining
Source for Software Engineering Tools. In Proceedings of the 16th International
Conference on Mining Software Repositories. IEEE Press, 490â€“501.

[15] Preetha Chatterjee, Kostadin Damevski, and Lori L. Pollock. 2021. Automatic Ex-
traction of Opinion-based Q&A from Online Developer Chats. In 43rd IEEE/ACM
International Conference on Software Engineering, ICSE. IEEE, 1260â€“1272. https:
//doi.org/10.1109/ICSE43902.2021.00115

[16] Preetha Chatterjee, Minji Kong, and Lori L. Pollock. 2020. Finding Help with
Programming Errors: An Exploratory Study of Novice Software Engineersâ€™ Focus
in Stack Overflow Posts. J. Syst. Softw. 159 (2020).

[17] Songqiang Chen, Xiaoyuan Xie, Bangguo Yin, Yuanxiang Ji, Lin Chen, and
Baowen Xu. 2020. Stay Professional and Efficient: Automatically Generate Ti-
tles for Your Bug Reports. In 35th IEEE/ACM International Conference on Auto-
mated Software Engineering, ASE 2020. 385â€“397. https://doi.org/10.1145/3324884.
3416538

[18] Peter M Chisnall. 1993. Questionnaire Design, Interviewing and Attitude Mea-

surement. Journal of the Market Research Society 35, 4 (1993), 392â€“393.

[19] Shaiful Alam Chowdhury and Abram Hindle. 2015. Mining StackOverflow to

Filter Out Off-Topic IRC Discussion. (2015), 422â€“425.

[20] Steven Davies and Marc Roper. 2014. Whatâ€™s in a Bug Report?. In 2014 ACM-IEEE
International Symposium on Empirical Software Engineering and Measurement,
ESEM â€™14, Maurizio Morisio, Tore DybÃ¥, and Marco Torchiano (Eds.). ACM, 26:1â€“
26:10.

[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171â€“4186.
https://doi.org/10.18653/v1/n19-1423

[22] Linda Erlenhov, Francisco Gomes de Oliveira Neto, and Philipp Leitner. 2020. An
Empirical Study of Bots in Software Development: Characteristics and Challenges
from a Practitionerâ€™s Perspective. In ESEC/FSE â€™20: 28th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering, Virtual Event. ACM, 445â€“455.

[23] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,
Teruko Mitamura, and Eduard H. Hovy. 2021. A Survey of Data Augmentation
Approaches for NLP. In Findings of the Association for Computational Linguistics:
ACL/IJCNLP 2021, Online Event, August 1-6, 2021, Vol. ACL/IJCNLP 2021. 968â€“988.
https://doi.org/10.18653/v1/2021.findings-acl.84

[24] Cuiyun Gao, Jichuan Zeng, Michael R. Lyu, and Irwin King. 2018. Online App
Review Analysis for Identifying Emerging Issues. In Proceedings of the 40th
International Conference on Software Engineering, ICSE 2018. ACM, 48â€“58.
[25] Cuiyun Gao, Wujie Zheng, Yuetang Deng, David Lo, Jichuan Zeng, Michael R.
Lyu, and Irwin King. 2019. Emerging App Issue Identification from User Feedback:
Experience on WeChat. In Proceedings of the 41st International Conference on
Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2019. IEEE /
ACM, 279â€“288.

[26] Gitter. 2020. REST API. https://developer.gitter.im/docs/rest-api.
[27] Google. 2020. Gitter. https://gitter.im/.
[28] Google. 2020. Slack. https://slack.com/.
[29] Gaoyang Guo, Chaokun Wang, Jun Chen, Pengcheng Ge, and Weijun Chen.
2019. Who is answering whom? Finding "Reply-To" relations in group chats
with deep bidirectional LSTM networks. 22, Suppl 1 (2019), 2089â€“2100. https:
//doi.org/10.1007/s10586-018-2031-4

[30] Hui Guo and Munindar P. Singh. 2020. Caspar: Extracting and Synthesizing User
Stories of Problems from App Reviews. In ICSE â€™20: 42nd International Conference
on Software Engineering. ACM, 628â€“640.

[31] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation Learning

on Graphs: Methods and Applications. arXiv preprint arXiv:1709.05584 (2017).

[32] Qiao Huang, Xin Xia, David Lo, and Gail C. Murphy. 2018. Automating Intention

Mining. IEEE Transactions on Software Engineering PP, 99 (2018), 1â€“1.

[33] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, HÃ©rve JÃ©gou,
and Tomas Mikolov. 2016. FastText.zip: Compressing text classification models.
arXiv preprint arXiv:1612.03651 (2016).

[34] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting
Decision Treef. In Advances in Neural Information Processing Systems. 3146â€“3154.

[35] Chaiyakarn Khanan, Worawit Luewichana, Krissakorn Pruktharathikoon, Ji-
rayus Jiarpakdee, Chakkrit Tantithamthavorn, Morakot Choetkiertikul, Chaiy-
ong Ragkhitwetsagul, and Thanwadee Sunetnanta. 2020. JITBot: An Explainable
Just-In-Time Defect Prediction Bot. In 35th IEEE/ACM International Conference
on Automated Software Engineering, ASE 2020. IEEE, 1336â€“1339.

[36] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations (ICLR).

[37] kootenpv. 2019. Contractions. https://github.com/kootenpv/contractions/.
[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet Classifica-
tion with Deep Convolutional Neural Networks. Advances in neural information
processing systems 25 (2012), 1097â€“1105.

[39] Jonathan K Kummerfeld, Sai R Gouravajhala, Joseph Peper, Vignesh Athreya,
Chulaka Gunasekara, Jatin Ganhotra, Siva Sankalp Patel, Lazaros Polymenakos,
and Walter S Lasecki. 2018. A Large-Scale Corpus for Conversation Disentangle-
ment. arXiv preprint arXiv:1810.11118 (2018).

[40] Mingyang Li, Lin Shi, Ye Yang, and Qing Wang. 2020. A Deep Multitask Learning
Approach for Requirements Discovery and Annotation from Open Forum. In
35th IEEE/ACM International Conference on Automated Software Engineering,
ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE, 336â€“348. https:
//doi.org/10.1145/3324884.3416627

[41] Andy Liaw, Matthew Wiener, et al. 2002. Classification and Regression by ran-

domForest. R news 2, 3 (2002), 18â€“22.

[42] Bin Lin, Alexey Zagalsky, Margaret-Anne D. Storey, and Alexander Serebrenik.
2016. Why Developers Are Slacking Off: Understanding How Software Teams
Use Slack. In Proceedings of the 19th ACM Conference on Computer Supported
Cooperative Work and Social Computing. 333â€“336.

[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.
Focal Loss for Dense Object Detection. In Proceedings of the IEEE international
conference on computer vision. 2980â€“2988.

[44] Hui Liu, Zhan Shi, Jia-Chen Gu, Quan Liu, Si Wei, and Xiaodan Zhu. 2020. End-
to-End Transition-Based Online Dialogue Disentanglement. In Proceedings of the
Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020,
Christian Bessiere (Ed.). 3868â€“3874. https://doi.org/10.24963/ijcai.2020/535
[45] Walid Maalej, Zijad Kurtanovic, Hadeer Nabil, and Christoph Stanik. 2016. On
the Automatic Classification of App Reviews. Requir. Eng. 21, 3 (2016), 311â€“331.
https://doi.org/10.1007/s00766-016-0251-9

[46] Walid Maalej and Hadeer Nabil. 2015. Bug Report, Feature Request, or Simply
Praise? on Automatically Classifying App Reviews. In 2015 IEEE 23rd international
requirements engineering conference (RE). IEEE, 116â€“125.

[47] Harish Tayyar Madabushi, Elena Kochkina, and Michael Castelle. 2020. Cost-
Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data.
arXiv preprint arXiv:2003.11563 (2020).

[48] Andrew McCallum, Kamal Nigam, et al. 1998. A Comparison of Event Models
for Naive Bayes Text Classification. In AAAI-98 workshop on learning for text
categorization, Vol. 752. Citeseer, 41â€“48.

[49] Courtney Miller, Paige Rodeghero, Margaret-Anne D. Storey, Denae Ford, and
Thomas Zimmermann. 2021. "How Was Your Weekend?" Software Development
Teams Working From Home During COVID-19. CoRR abs/2101.05877 (2021).
[50] Vinod Nair and Geoffrey E Hinton. 2010. Rectified Linear Units Improve Restricted

Boltzmann Machines. In Icml.

[51] Shengyi Pan, Lingfeng Bao, Xiaoxue Ren, Xin Xia, David Lo, and Shanping Li.
[n.d.]. Automating Developer Chat Mining. In 36th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia,
November 15-19, 2021. 854â€“866. https://doi.org/10.1109/ASE51524.2021.9678923
[52] Esteban Parra, Ashley Ellis, and Sonia Haiduc. 2020. GitterCom: A Dataset of
Open Source Developer Communications in Gitter. In MSR â€™20: 17th International
Conference on Mining Software Repositories. ACM, 563â€“567.

[53] Quentin Perez, Pierre-Antoine Jean, Christelle Urtado, and Sylvain Vauttier. 2021.
Bug or not bug? That is the Question. In 29th IEEE/ACM International Conference
on Program Comprehension, ICPC 2021, Madrid, Spain, May 20-21, 2021. IEEE,
47â€“58. https://doi.org/10.1109/ICPC52881.2021.00014

[54] AntÃ´nio Mauricio Pitangueira, Paolo Tonella, Angelo Susi, Rita Suzana Pi-
tangueira Maciel, and MÃ¡rcio de Oliveira Barros. 2017. Minimizing the Stake-
holder Dissatisfaction Risk in Requirement Selection for Next Release Planning.
Information & Software Technology 87 (2017), 104â€“118.

[55] Jantima Polpinij. 2021. A Method of Non-bug Report Identification from Bug
Report Repository. Artif. Life Robotics 26, 3 (2021), 318â€“328. https://doi.org/10.
1007/s10015-021-00681-3

[56] C. Qu, L. Yang, W. B. Croft, Y. Zhang, J. Trippas, and M. Qiu. 2019. User Intent

Prediction in Information-seeking Conversations. In CHIIR â€™19.

[57] Paige Rodeghero, Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. De-
tecting User Story Information in Developer-client Conversations to Generate
Extractive Summaries. In Proceedings of the 39th International Conference on
Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017. 49â€“59.
[58] Simone Scalabrino, Gabriele Bavota, Barbara Russo, Massimiliano Di Penta, and
Rocco Oliveto. 2019. Listening to the Crowd for the Release Planning of Mobile
Apps. IEEE Trans. Software Eng. 45, 1 (2019), 68â€“86. https://doi.org/10.1109/TSE.
2017.2759112

BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats

ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

[59] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The Graph Neural Network Model. IEEE transactions on neural
networks 20, 1 (2008), 61â€“80.

[60] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling Relational Data with Graph Convolu-
tional Networks. In European semantic web conference. Springer, 593â€“607.
[61] s.e.a.l. 2017. UZH-s.e.a.l.-Development Emails Content Analyzer (DECA). https:

//www.ifi.uzh.ch/en/seal/people/panichella/tools/DECA.html.

[62] Lin Shi, Xiao Chen, Ye Yang, Hanzhi Jiang, Ziyou Jiang, Nan Niu, and Qing Wang.
2021. A First Look at Developersâ€™ Live Chat on Gitter. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 391â€“403.

[63] Lin Shi, Ziyou Jiang, Ye Yang, Xiao Chen, Yumin Zhang, Fangwen Mu, Hanzhi
Jiang, and Qing Wang. [n.d.]. ISPY: Automatic Issue-Solution Pair Extraction
from Community Live Chats. In 36th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19,
2021. 142â€“154. https://doi.org/10.1109/ASE51524.2021.9678894

[64] Lin Shi, Mingzhe Xing, Mingyang Li, Yawen Wang, Shoubin Li, and Qing Wang.
2020. Detection of Hidden Feature Requests from Massive Chat Messages via
Deep Siamese Network. In ICSE â€™20: 42nd International Conference on Software
Engineering. ACM, 641â€“653.

[65] Emad Shihab, Zhen Ming Jiang, and Ahmed E. Hassan. 2009. On the Use of
Internet Relay Chat (IRC) Meetings by Developers of the GNOME GTK+ Project.
In Proceedings of the 6th International Working Conference on Mining Software
Repositories, MSR 2009 (Co-located with ICSE), Vancouver, BC, Canada, May 16-17,
2009, Proceedings. 107â€“110.

[66] Emad Shihab, Zhen Ming Jiang, and Ahmed E. Hassan. 2009. Studying the Use
of Developer IRC Meetings in Open Source Projects. In 25th IEEE International
Conference on Software Maintenance (ICSM 2009), September 20-26, 2009, Edmonton,
Alberta, Canada. 147â€“156.

[67] Yang Song and Oscar Chaparro. 2020. BEE. http://bugreportchecker.ngrok.io/

api/.

[68] Yang Song and Oscar Chaparro. 2020. BEE: A Tool For Structuring and Analyzing
Bug Reports. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
1551â€“1555.

[69] Andrea Di Sorbo, Sebastiano Panichella, Corrado Aaron Visaggio, Massim-
iliano Di Penta, Gerardo Canfora, and Harald C. Gall. 2015. Development
Emails Content Analyzer: Intention Mining in Developer Discussions (T). In
30th IEEE/ACM International Conference on Automated Software Engineering, ASE
2015, Lincoln, NE, USA, November 9-13, 2015. 12â€“23.

[70] Andrea Di Sorbo, Sebastiano Panichella, Corrado Aaron Visaggio, Massimil-
iano Di Penta, Gerardo Canfora, and Harald C. Gall. 2016. DECA: Development

Emails Content Analyzer. In Proceedings of the 38th International Conference on
Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016 - Companion
Volume. ACM, 641â€“644. https://doi.org/10.1145/2889160.2889170

[71] Chanchal Suman, Sriparna Saha, Pushpak Bhattacharyya, and Rohit Shyamkant
Chaudhari. 2021. Emoji Helps! A Multi-modal Siamese Architecture for Tweet
User Verification. Cognitive Computation 13, 2 (2021), 261â€“276.

[72] Cong Sun and Zhihao Yang. 2019. Transfer Learning in Biomedical Named Entity
Recognition: An Evaluation of BERT in the PharmaCoNER task. In Proceedings
of The 5th Workshop on BioNLP Open Shared Tasks. 100â€“104.

[73] Pannavat Terdchanakul, Hideaki Hata, Passakorn Phannachitta, and Kenichi
Matsumoto. 2017. Bug or Not? Bug Report Classification Using N-Gram IDF. In
2017 IEEE International Conference on Software Maintenance and Evolution, ICSME
2017, Shanghai, China, September 17-22, 2017. IEEE Computer Society, 534â€“538.
https://doi.org/10.1109/ICSME.2017.14

[74] Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, and Tung Thanh Nguyen.
2015. Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach
(T). In 30th IEEE/ACM International Conference on Automated Software Engineering,
ASE 2015. IEEE Computer Society, 749â€“759.

[75] Phong Minh Vu, Hung Viet Pham, Tam The Nguyen, and Tung Thanh Nguyen.
2016. Phrase-based Extraction of User Opinions in Mobile App Reviews. In
Proceedings of the 31st IEEE/ACM International Conference on Automated Software
Engineering, ASE 2016. ACM, 726â€“731.

[76] Jason W. Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for
Boosting Performance on Text Classification Tasks. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
2019, Hong Kong, China, November 3-7, 2019. 6381â€“6387. https://doi.org/10.18653/
v1/D19-1670

[77] Ye Zhang and Byron C. Wallace. 2015. A Sensitivity Analysis of (and Practitionersâ€™
Guide to) Convolutional Neural Networks for Sentence Classification. CoRR
abs/1510.03820 (2015).

[78] Yu Zhao, Kye Miller, Tingting Yu, Wei Zheng, and Minchao Pu. 2019. Automati-
cally Extracting Bug Reproducing Steps from Android Bug Reports. In Reuse in
the Big Data Era - 18th International Conference on Software and Systems Reuse,
ICSR. 100â€“111. https://doi.org/10.1007/978-3-030-22888-0_8

[79] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards
Story-Like Visual Explanations by Watching Movies and Reading Books. In The
IEEE International Conference on Computer Vision (ICCV).

[80] Thomas Zimmermann, Rahul Premraj, Nicolas Bettenburg, Sascha Just, Adrian
Schroter, and Cathrin Weiss. 2010. What Makes a Good Bug Report? IEEE
Transactions on Software Engineering 36, 5 (2010), 618â€“643.

