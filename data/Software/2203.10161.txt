Prepared for submission to Snowmass, CompF4

2
2
0
2

r
a

M
2
2

]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[

2
v
1
6
1
0
1
.
3
0
2
2
:
v
i
X
r
a

Collaborative Computing Support for Analysis
Facilities Exploiting Software as Infrastructure
Techniques

Maria Acosta Flechas,a Garhan Attebury,b Kenneth Bloom,b Brian Bockelman,c
Lindsey Gray,a Burt Holzman,a Carl Lundstedt,b Oksana Shadura,b Nicholas Smith,a
John Thiltgesb

aFermi National Accelerator Laboratory, USA
bUniversity of Nebraska-Lincoln, USA
cMorgridge Institute for Research, USA

Abstract: Prior to the public release of Kubernetes it was diﬃcult to conduct joint de-
velopment of elaborate analysis facilities due to the highly non-homogeneous nature of
hardware and network topology across compute facilities. However, since the advent of
systems like Kubernetes and OpenShift, which provide declarative interfaces for building
fault-tolerant and self-healing deployments of networked software, it is possible for multiple
institutes to collaborate more eﬀectively since resource details are abstracted away through
various forms of hardware and software virtualization. In this whitepaper we will outline
the development of two analysis facilities: “Coﬀea-casa” at University of Nebraska Lincoln
and the “Elastic Analysis Facility” at Fermilab, and how utilizing platform abstraction
has improved the development of common software for each of these facilities, and future
development plans made possible by this methodology.

 
 
 
 
 
 
1

Introduction

The analysis facilities discussed in this whitepaper were designed from the start to use
a container-based infrastructure. Containers provide ﬂexibility, portability and isolation
without the additional overhead of virtual machines. Sites that deploy this infrastructure
at a wide scale make it easier to add elasticity to the analysis facility; servers for a diﬀerent
purpose (e.g. batch worker nodes) can be reprovisioned on the ﬂy for scheduling analysis
tasks. The orchestration tool of choice for containers is Kubernetes. It provides a uniﬁed
declarative description and conﬁguration language, conﬁguration management, service dis-
covery, service load balancing, automated rollouts and rollbacks, and other features key to
providing stable services.

Kubernetes was originally designed for cloud computing, which adopts a single-tenant
model: one user creates and owns an entire cluster. Since its original public release, it
has been extended with role-based access controls, policy primitives, and a conﬁgurable
programmable ﬁlter module in front of the API. Pure Kubernetes is a good ﬁt for facilities
such as Coﬀea-casa which are designed to serve a single experiment. For multi-tenant
facilities there is Red Hat’s open-source OKD platform, which is a superset of Kubernetes.
OKD incorporates additional security and isolation, adds operations-centric tools, a user-
friendly GUI, and additional storage and network orchestration components, making OKD
a good choice for the Elastic Analysis Facility.

Coﬀea-casa is an analysis facility prototype built on a combination of Nebraska’s multi-
tenant Kubernetes resource and their HTCondor pool dedicated for CMS Tier-2 Nebraska
computing. Kubernetes provides a service orchestration layer, allowing the administrative
team to programmatically deploy cyberinfrastructure such as custom pods, network con-
nectivity, and block devices. For example, Kubernetes hosts a JupyterHub instance that
authenticates users and, upon login, provisions a dedicated “analysis pod” for the user. In
this case a Kubernetes “pod” is the atomic unit of compute and service management (po-
tentially consisting of several containers). For Coﬀea-casa, the analysis pod consists of a
Jupyter notebook, a Dask scheduler, and a Dask worker container (ﬁgure 1). The Dask
worker container provides immediate execution resources for user tasks, providing the user
with the feel of near-instant responsiveness of the system [1].

Fermilab’s Elastic Analysis Facility uses OKD [2] as the container orchestration engine.
As mentioned above, increased security and network isolation provided by OKD are key
for guaranteeing compliance with both Laboratory and DOE policies. The project started
in 2018 as an eﬀort to create a dynamic, user-oriented analysis facility capable of elastic
resource allocation and dynamic scaling. The facility was designed to be a multi-VO service
in preparation for high capacity data analysis demands for CMS with HL-LHC and other
upcoming experiments with similar demands such as DUNE.

As a common goal, both facilities incorporate elasticity into their design: they can
dispatch work not only to resources within the facility, but also to an external co-located
computing platform (e.g. university and laboratory batch systems). They dynamically
provision worker instances using the high throughput computing scheduler HTCondor [3].
For Coﬀea-casa, Kubernetes not only allows programmatic service deployment but

– 2 –

Figure 1. CMS analysis facility at the University of Nebraska.

provides “infrastructure-as-code” where the entire infrastructure can be expressed as static
(YAML) ﬁles that can be shared and used by others. The facility utilizes a GitOps philos-
ophy, keeping all ﬁles in a source code repository and allowing an agent (Flux) to manage
synchronization between the repository and the Kubernetes cluster. Multiple agents can
deploy from the same repository, allowing separate production and development instances.
Finally, having infrastructure-as-code allowed easily packages the core infrastructure (e.g.,
removing the site-speciﬁc passwords and secret keys) as a Helm chart, a format used by the
Kubernetes community for application packaging [4].

2 Providing Reproducible, Interactive Computing Environments

2.1 Container images

The Coﬀea-casa analysis facility at Nebraska was initially conceived to support the Coﬀea
analysis framework [5]. Coﬀea-casa builds and uses custom containers to facilitate the
integration of such a complex application. Although the team builds and maintains these
images, the versatility of Kubernetes allow for the drop-in replacement of other custom
Jupyter notebook containers, and the Jupyterhub instance can be conﬁgured to allow user
selection of supported images.

The Coﬀea-casa container also integrates a custom Coﬀea-casa extension to the Dask
JobQueue class [6, 7]. The JobQueue API allows for Dask tasks to be easily deployed into
a batch system such as Slurm or HTCondor. The customization made by the Coﬀea-casa
team allow for site speciﬁc parameters, such as queue name and submitter host, to be easily
obscured from the user thereby making utilization of batch resources transparent to end
users.

In a similar way, Fermilab’s Elastic Facility (EAF) uses containers as means to provide
reproducible and reliable environments for multiple users, experiments and diverse data
science requirements. However, image structuring is done in a layered pattern where “base”
images are curated by the project with minimal software and common conﬁgurations. Later

– 3 –

on, more complex, experiment-speciﬁc images are built from said common base, allowing
for the use of caches at build, creating lightweight images, tailored for each and every use
case within our user community. GPU images with speciﬁc CUDA versions and TensorFlow
distributions are also derived from common bases, guaranteeing that all environment ﬂavors
contain the same (or very similar) analysis libraries and tools.

2.2 Binderhub

Binderhub is a Kubernetes-based cloud service that can launch a repository of code (from
GitHub, GitLab, and others) in a browser window such that the code can be executed and
interacted with in the form of a Jupyter notebook. Binder, the product behind mybinder.org
[8] as a user interface, is also useful for reproducibility because the code needs to be version
controlled and the computational environment needs to be documented in order to beneﬁt
from the functionality of Binder [9].

When ﬁrst deploying BinderHub on Fermilab EAF, OKD’s security features blocked
a number of operations, including pod deletion permissions and no service accounts for
performing API calls. Given that BinderHub deploys arbitrary user code on our clusters,
it needs to be carefully monitored to avoid misuse or potentially malicious users to take
advantage of the resource. The Binder code currently deployed on EAF has been modiﬁed
at the source to ensure proper limitations on API calls, use and speciﬁcation of Service
Accounts, SELinux options and other key elements. The patches are in place to protect the
underlying infrastructure and have been contributed upstream to BinderHub developers.

The EAF BinderHub instance is also conﬁgured as a JupyterHub service which in
turn allows secure calls to JupyterHub’s oauth APIs, guaranteeing consistency across ap-
plications with a central source for user data, which, in the EAF case is the Laboratory’s
LDAP server. Binderhub integration is planned to be delivered as a part of Coﬀea-casa AF
deployment and will be tested in Q2 2022.

2.3 Helm Charts

The analysis facility model adopted by both the Fermilab EAF and Coﬀea-casa seeks to
be modular and exportable. To this end, each facility writes Helm charts for the various
modules they write. These modiﬁcations can then be incorporated into any speciﬁc de-
ployment and persisted for history or any rollbacks required. Helm charts and declarative
conﬁguration in general has been key for building the EAF and ensuring high availability
and reliability of services. Deploying applications in OKD using Helm and Openshift Tem-
plates allows for faster iterations and reduces operational load when dealing with cluster
downtimes and other unforeseen events damaging or disabling the underlying infrastruc-
ture. Even though the EAF ecosystem is complex, it takes almost no time and eﬀort to
re-deploy it completely and re-establish service using any available OKD installation.

2.4 GitOps

GitOps is deﬁned as a model for operating Kubernetes clusters or cloud-native applications
using the Version Control System Git as the single source of truth [10]. One of the features

– 4 –

GitOps envisions declarative descriptions of an environment to be stored as infrastructure-
as-code in a Git repository.

Coﬀea-casa is deployed to the Nebraska Kubernetes resource via Flux [11]. Flux allows
for site-speciﬁc conﬁgurations to be stored and managed in git so that administration of
the analysis facility can be handled via a collaborative group of administrators and in a
deterministic manner. Flux allows for a portion of the site’s conﬁguration to be public and
shareable while portions of the deployment can be retained as private. A similar deployment
is in development at the Fermilab EAF, using ArgoCD [12].

2.4.1 Repeatability, Reliability, and Scalability through GitOps

Continuous integration, delivery, and deployment are software development industry prac-
tices that enable organizations to frequently and reliably release new features and products
cicdreview. They allow for rapid collaboration, better quality control, and automation as
code goes through formal review processes and is audited and tested on a regular basis.

The GitOps framework recommends managing code changes via merge/pull requests in
code management systems, and implementing continuous practices for faster deployments.
Both analysis facilities have implemented peer-reviewed code reviews as well as automated
services for identifying new library versions and building releases, running dependency
checks, and auditing images for known vulnerabilities.

An important beneﬁt of GitOps is faster error recovery and reduced chances for failure
modes. When conﬁgurations and application states are kept in tracked ﬁles, the repository
becomes a history of all changes and deployment states, acting in a similar way to audit
and transaction logs [13]. GitOps tools such as ArgoCD and Flux used by both analysis
facilities provide ways to quickly perform deployments and rollbacks when pods become
unhealthy, applications fail, or infrastructure failures occur. This reduction in the mean-
time-to-recovery represents an operational advantage and allows tasks which traditionally
take many hours, manpower and speciﬁc expertise, to be done in a few minutes and with
lower operational risk.

3 Summary

The analysis facilities at Nebraska and Fermi National Laboratory represent initial successes
of leveraging on-premises computing and cloud computing platform management to fun-
damentally change the user experience with large scale scientiﬁc computing. The facilities
have both demonstrated using Kubernetes to provide a platform for the scalable utilza-
tion of computing resources from a customized Juypter environment. Further development
and reﬁnement of the packaging will deliver a portable platform that could be reasonably
deployed at many other sites.

References

[1] Matous Adamec, Garhan Attebury, Kenneth Bloom, Brian Bockelman, Carl Lundstedt,

Oksana Shadura, and John Thiltges. Coﬀea-casa: an analysis facility prototype. In EPJ Web
of Conferences, volume 251, page 02061. EDP Sciences, 2021.

– 5 –

[2] Red Hat Inc. Okd.io, 2018. URL https://www.okd.io/about/.

[3] Douglas Thain, Todd Tannebaum, and Miron Livny. Distributed computing in practice: The

condor experience. Concurrency and Computation: Practice and Experience, 17(2-4):
323–356, 2005. doi: http://dx.doi.org/10.5281/zenodo.2579447.

[4] Helm Authors for the Cloud Native Computing Foundation. Helm, 2022. URL

https://helm.sh/.

[5] Nicholas Smith, Lindsey Gray, Matteo Cremonesi, Bo Jayatilaka, Oliver Gutsche, Allison

Hall, Kevin Pedro, Maria Acosta Flechas, Andrew Melo, Stefano Belforte, and Jim Pivarski.
Coﬀea - columnar object framework for eﬀective analysis. CoRR, abs/2008.12712, 2020.
URL https://arxiv.org/abs/2008.12712.

[6] Dask Development Team. Dask: Library for dynamic task scheduling, 2016. URL

https://dask.org.

[7] Dask jobqueue Development Team. Dask-jobqueue, 2018. URL

http://jobqueue.dask.org/en/latest/.

[8] The binder project, 2022. URL https://mybinder.org/.

[9] The Turing Way Community. The Turing Way: A handbook for reproducible, ethical and

collaborative research, November 2021. URL https://doi.org/10.5281/zenodo.5671094.

[10] Florian Beetz and Simon Harrer. Gitops: The evolution of devops? IEEE Software, 2021.

[11] Cloud Native Computing Foundation. Flux, 2022. URL https://fluxcd.io/.

[12] Argo Project. Argo cd, 2022. URL https://argoproc.github.io/cd.

[13] Thomas A. Limoncelli. Gitops: A path to more self-service it: Iac pr = gitops. Queue, 16(3):

13–26, jun 2018. ISSN 1542-7730. doi: 10.1145/3236386.3237207. URL
https://doi.org/10.1145/3236386.3237207.

– 6 –

