Adaptive Edge Ofﬂoading for Image Classiﬁcation
Under Rate Limit

Jiaming Qiu, Ruiqi Wang, Ayan Chakrabarti, Roch Gu´erin, Chenyang Lu
Dept. of Computer Science & Engineering, Washington University in St. Louis.
{qiujiaming,ruiqi.w,guerin,lu}@wustl.edu, ayan.chakrabarti@gmail.com

1

2
2
0
2

l
u
J

1
3

]

C
D
.
s
c
[

1
v
5
8
4
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—This paper considers a setting where embedded
devices are used to acquire and classify images. Because of
limited computing capacity, embedded devices rely on a par-
simonious classiﬁcation model with uneven accuracy. When
local classiﬁcation is deemed inaccurate, devices can decide to
ofﬂoad the image to an edge server with a more accurate
but resource-intensive model. Resource constraints, e.g., network
bandwidth, however, require regulating such transmissions to
avoid congestion and high latency. The paper investigates this
ofﬂoading problem when transmissions regulation is through a
token bucket, a mechanism commonly used for such purposes.
The goal
is to devise a lightweight, online ofﬂoading policy
that optimizes an application-speciﬁc metric (e.g., classiﬁcation
accuracy) under the constraints of the token bucket. The paper
develops a policy based on a Deep Q-Network (DQN), and
demonstrates both its efﬁcacy and the feasibility of its deployment
on embedded devices. Of note is the fact that the policy can
handle complex input patterns, including correlation in image
arrivals and classiﬁcation accuracy. The evaluation is carried
out by performing image classiﬁcation over a local testbed using
synthetic traces generated from the ImageNet image classiﬁca-
tion benchmark. Implementation of this work is available at
https://github.com/qiujiaming315/edgeml-dqn.

Index Terms—embedded machine learning, edge computing,
image classiﬁcation, deep reinforcement learning, token bucket

Fig. 1. System overview and connectivity.

I. INTRODUCTION

Recent years have witnessed the emergence of Artiﬁcial
Intelligence of Things (AIoT), a new paradigm of embedded
systems that builds on two important advances. First, through
progress in embedded hardware [1]–[3], machine learning
models can now run on embedded devices, even if resource
constraints limit them to relatively weak models [4]–[6] that
trade accuracy for resource efﬁciency. Second, edge servers
accessible through shared local networks are increasingly com-
mon, providing access to additional compute resources [7].
Those edge servers are powerful enough to run strong(er),
more complex models that are more accurate, therefore supple-
menting the weak local models running on embedded devices.
Of relevance in our setting is that independent of the edge
compute resources, the large amount of input data (e.g., im-
ages) acquired by embedded devices and the limited bandwidth
of the shared network call for judicious decisions on what to
ofﬂoad to edge servers and when. In particular, bandwidth
constraints call for rate limiting transmissions from embedded
devices. In this work and following common practice, we
employ a standard token bucket [8, Section 18.4.2] to regulate
ofﬂoading trafﬁc. A token bucket (sometimes called a leaky

bucket) provides a simple and ﬂexible mechanism that speci-
ﬁes both a long-term transmission rate and a maximum num-
ber of consecutive transmissions (bucket size). It has become
the de facto standard for limiting user transmissions in both
wired and wireless networks, with implementations available
across commercial router/switch products, cloud providers
offerings, and all major operating systems and programming
languages. As a result, the ﬁndings of the paper should have
applicability beyond the speciﬁc environment it considers.

Fig. 1 offers a representative example of the type of edge
computing setting we consider. We use image classiﬁcation as
our target application, although the framework may be gener-
alized to other types of classiﬁcation or inference applications.
Cameras distributed across an area share a network connect-
ing them to an edge server. They are responsible for capturing
images and classifying them according to the category to
which they belong. As is common [9], this is done using
a deep learning model. The limited computational resources
available in the cameras impose the use of what we term a
weak model in contrast to the strong model available on the
edge server that boasts greater compute resources. The primary
difference between the two models is the conﬁdence metric of

 
 
 
 
 
 
2

their outputs, with the strong model outperforming the weak
one. In many instances, the weak model returns a satisfactory
(of sufﬁcient conﬁdence) answer, but it occasionally falls short.
In those cases, the embedded device has the option to send
its input to the edge server for a higher conﬁdence answer.
However, network bandwidth constraints call for regulating
such ofﬂoading decisions through a token bucket mechanism,
with each image transmission consuming a token. The chal-
lenge is to devise a policy that meets those constraints while
maximizing classiﬁcation accuracy (the metric of interest).

Ofﬂoading decisions inﬂuence both immediate and future
“rewards” (improvements in classiﬁcation accuracy). Ofﬂoad-
ing an image generates an immediate reward from the higher
(expected) accuracy of the edge server classiﬁcation. However,
the token this transmission consumes may be better spent on
a future higher reward image. This trade-off depends on both
future image arrivals and how the classiﬁers would perform
on those images. Neither aspect is likely to follow a simple
image capture may be triggered by
pattern. For example,
external events (e.g., motion detectors), with the resulting
arrival process exhibiting complex variations. Similarly, the
accuracy of the weak classiﬁer may be inﬂuenced by weather
and lighting conditions or the type of objects in the images.
This may in turn introduce correlation in the accuracy of
consecutive images classiﬁcations.

Examples of real-world image classiﬁcation applications
that may exhibit such complex input patterns include automatic
check-out in retail stores, wildlife monitoring, or AI-powered
robots that classify waste in recycling plants. In all those
settings, external factors, e.g., store layout, animals behavior,
or how items are stacked in recycling bins, can produce
complex input sequences to the classiﬁer.

This paper presents a general solution capable of handling
arbitrary input sequences while making efﬁcient ofﬂoading
decisions on embedded devices. The solution is built on a
Deep Q-Network (DQN) framework that can learn an efﬁcient
ofﬂoading policy given a training sequence of representative
inputs, i.e., based on a history of consecutive images, classi-
ﬁcation outputs, ofﬂoading rewards, and token bucket states.
More speciﬁcally, the paper makes the following contributions:
• A DQN-based policy that optimizes ofﬂoading decisions
under variable image arrival patterns and correlation in
the accuracy of consecutive images classiﬁcations, while
accounting for token bucket constraints;

• An implementation and benchmarking of the policy in
an edge computing testbed demonstrating its efﬁciency
on embedded devices;

• A comprehensive evaluation using a wide range of image
illustrating its

sequences from the ImageNet dataset,
beneﬁts over competing alternatives.

II. BACKGROUND AND MOTIVATION

As mentioned in Section I, embedded devices can now run
deep learning models. The co-location of data and processing
offers signiﬁcant beneﬁts in leveraging distributed compute
resources and timeliness of execution. For example, as we
report in Section VI-B, local execution can return an image

classiﬁcation answer in about 20ms vs. over 50ms if performed
on an edge server after transmission over a local WiFi network.
This gain in timeliness, however, comes at a cost, as the
weak(er) models running in embedded devices can under-
perform the stronger models that edge servers can run. Of
interest though is the fact that differences in image classi-
ﬁcation accuracy are not systematic or even common. Those
differences vary depending on the classiﬁers (weak and strong)
used, but broadly fall in three categories: (a) images that both
classiﬁers accurately classify, (b) images that both classiﬁers
struggle to classify accurately, and (c) images that the strong
classiﬁer can handle but not the weak classiﬁer.

The relative fraction of images in each category can vary,
but for typical combinations of classiﬁers many images are in
(a), a small fraction of images are in (b), and the remainder
are in (c). For example, using the model of [10] with a
computational footprint of 595MFlops as the strong classiﬁer,
and a 16-layer VGG-style model as the weak classiﬁer, we ﬁnd
that across the ILSVRC validation set 70.00% of images are
in (a), 4.47% are in (b), and the remaining 25.53% images are
in (c) (Fig. 2 shows sample images from all three categories).
To improve overall classiﬁcation accuracy, images in (c)
should be ofﬂoaded, while ofﬂoading images in (a) or (b) is a
waste of network bandwidth and edge resources. Any solution
must, therefore, ﬁrst identify images in (c), and then ensure
that as many of them can be transmitted under the constraints
imposed by the rate control mechanism (token bucket). This
is difﬁcult because of the often unpredictable nature of the
arrival pattern of images in (c). Developing a policy capable of
handling this complexity is one of the challenges the solution
developed in this paper addresses.

III. RELATED WORK

A. Edge Computing for Deep Learning Applications

Three general approaches have been explored to address
bandwidth constraints in edge computing systems running
deep neural network (DNN) models. We brieﬂy review them.
Input Adaptation: In this approach the deep learning
model is only deployed on the edge server, and the embedded
devices ofﬂoad all inputs to the edge server for inference. A
variety of application-speciﬁc techniques have been exploited
to reduce the size of the input data, including compression
based on regions of interest (RoI) for object detection [11],
[12], adaptation of video frame size and rate [13], exploiting
motion vector for object tracking [12], face cropping and
contrast enhancement for emotion recognition [14], and DNN-
driven feedback regions for video streaming [15]. The key idea
is to adapt the input as a function of the inference tasks towards
preserving its accuracy. None of these solutions exploit the
capabilities of modern embedded hardware to execute machine
learning models locally.

Split Computing: This approach takes advantage of the
computing capability of embedded devices by splitting the
inference task between the device and the server, with each
side completing part of the computation. The deep learning
model is partitioned into head and tail models deployed on
the device and the server, respectively. Early works [16], [17]

3

(a)

(b)

(c)

Fig. 2. Image samples from the ILSVRC validation set for which classiﬁcation is (a) accurate for both classiﬁers, (b) hard for both classiﬁers, and (c) accurate
for the strong classiﬁer but not the weak one.

partition the original DNN to minimize bandwidth utilization.
More recent techniques [18], [19] modify the original DNN
structure by injecting a bottleneck autoencoder that ensures
a lightweight head model. Other works [20], [21] apply
knowledge distillation techniques to train an autoencoder that
serves as its head model and performs part of the inference
task in addition to compressing the input. In all these solutions,
the ofﬂoading rate is ﬁxed once the splitting is selected.

Model Cascade and Early Exiting: The cascade of models
framework [22], [23] relies on a cascade of models of in-
creasing complexity and accuracy to achieve fast and accurate
inference with deep learning models. A weak (and fast) model
is used ﬁrst, with stronger but computationally more expensive
models invoked only if the weak model is not sufﬁciently
conﬁdent of its output. In an edge computing setting, this
naturally suggests deploying a pair of weak and strong models
on embedded devices and servers, respectively [24], [25].
Distributed Deep Neural Networks (DDNN) [26] have a
similar focus but rely on early exiting to avoid redundant
inferences. Intermediate exits (i.e., sub-branches) added to the
DNN model allow inference queries to exit once conﬁdence
exceeds a threshold. As with the cascade framework, this
readily maps to an edge computing setting by assigning early
exit layers to the embedded device and the remaining layers
to the edge server [27], [28]. Of particular relevance is [27]
that seeks to select exit points based on network conditions.
However, none of those works focus on enforcing explicit rate
limits as imposed by token buckets.

B. Computation Ofﬂoading Algorithms in Edge Computing

Devising effective ofﬂoading policies is a fundamental
problem in edge computing1; one that has received signiﬁcant
attention. In most works, the ofﬂoading problem is formulated
as an optimization problem that aims to minimize a metric
such as latency and/or energy consumption, with, as in this
paper, deep Q-learning often the solution method of choice
when dealing with dynamic and high-dimensionality inputs.

Focusing on a few representative examples, [30] considers a
mobile edge computing setup with sliced radio access network
and wireless charging and relies on a double DQN approach
to maximize a utility function that incorporates latency and
energy consumption. Similarly, [31] investigates a scenario
where energy harvesting IoT devices make ofﬂoading deci-
sions across multiple edge servers and use DQN to optimize

1Lin et al. [29] provides a comprehensive review.

ofﬂoading rate and edge server selection. Finally, [32] consid-
ers a wireless powered mobile edge computing system, and
uses DQN to make real-time ofﬂoading and wireless resource
allocation decisions that adapt to channel conditions.

there are several

In spite of their reliance on DQN for ofﬂoading decisions
in an edge computing setting,
important
differences with this paper. The ﬁrst is that those papers aim
to optimize general system or computational metrics rather
than an application-speciﬁc metric (classiﬁcation accuracy)
that depends on both local and edge performance. In addition,
although they also target an optimization under constraints,
e.g., energy constraints [30]–[32], those give rise to different
state representations and, therefore, problem formulation than
the token bucket constraint we consider.

The problem of optimizing ofﬂoad decisions to maximize
inference accuracy under token bucket constraint, which we
consider, was ﬁrst introduced in [33] based on the cascade of
models framework. The work formulated the ofﬂoading deci-
sion problem as a Markov Decision Process (MDP) assuming
that the inputs to the classiﬁer are periodic and independent
and identically distributed (i.i.d.). It generalized the ﬁxed
ofﬂoading threshold model of the cascade framework [22],
[23], [26], [27] to account for the token bucket constraints by
adopting an ofﬂoading policy that, for every token bucket state,
learned a threshold based on the local classiﬁer conﬁdence
score. As alluded to in Section I, the periodic and i.i.d. assump-
tions may apply in some settings, but they are overly restrictive
and unlikely to hold in many real-world applications. Devising
policies capable of handling more complex image sequences
is the focus and main contribution of this paper.

IV. PROBLEM FORMULATION

Recalling the system of Fig. 1, images captured by cameras
are classiﬁed by the local (weak) classiﬁer and an ofﬂoading
decision is made based on that classiﬁer’s conﬁdence and the
token bucket state. This ofﬂoading policy can be formulated
as an online constrained optimization problem that accounts
for (i) the image arrival process, (ii) the output of the (weak)
classiﬁer, (iii) the token bucket state, and (iv) the metric to
optimize (classiﬁcation accuracy).

In the rest of this section, we review our assumptions along
each of those dimensions before formulating our optimization,
with Section V introducing a possible solution suitable for the
limited computational resources of embedded devices.

4

A. Input Process

The ﬁrst aspect affecting ofﬂoading decisions is how inputs
arrive at each device, both in terms of their frequency (rate)
and temporal patterns. Our goal is to accommodate as broad a
set of scenarios as possible, and we describe next our model
for the input arrival process at each device.

For modeling sake, we assume a discrete time system with
an underlying clock that determines when images can arrive.
Image arrivals follow a general inter-arrival time process with
an arbitrary distribution F (t). This distribution can be chosen
to allow both renewal and non-renewal inter-arrival times. This
includes i.i.d. arrival processes that may be appropriate when
images come from a large set of independent sources, as well
as non-renewal arrival processes, e.g., MAP [34], that may be
useful to capture environments where image arrivals follow
alternating periods of high and low intensity.

In general, a goal of our solution will be to learn the speciﬁc
structure of the image arrival process, as captured by F (t), and
incorporate that knowledge into ofﬂoading decisions.

B. Classiﬁer Output

The weak and strong classiﬁers deployed in the devices and
the edge server are denoted as W and S respectively. For a
given image x they provide classiﬁcation outputs W(x) and
S(x) in the form of probability distributions over the (ﬁnite)
set of possible classes Y. Given the ground truth class y and
the classiﬁer output z for an input image x, an application-
speciﬁc loss (error) function L(z, y) is deﬁned that measures
the mis-classiﬁcation penalty (e.g., 0 if y is among the k
most likely classes according to z and 1 otherwise, when
the application is “top-k”). Loss is, therefore, dependent on
whether or not an image is ofﬂoaded, and for image x denoted
as L(W(x), y) if it is not ofﬂoaded, and L(S(x), y) otherwise.
Note that at (ofﬂoading) decision time both S(x) and y
are unknown so that neither L(W(x), y) nor L(S(x), y) can
be computed. As a result and as discussed in Section IV-D,
the policy’s goal is instead to maximize an expected reward
(decrease in loss) from ofﬂoading decisions. This reward is
affected not just by the input arrival process, but also by the
classiﬁer output process. In particular, dependencies in the
classiﬁer outputs, e.g., caused by changes in environmental
conditions, can result in sequences of high or low conﬁdence
outputs that need to be accounted for by the policy’s decisions.

C. Token Bucket

As mentioned, it is necessary to regulate the ofﬂoading rate
of devices to control the network load. This is accomplished
through a two-parameters token bucket (r, b) in each device,
which controls both short and long-term ofﬂoading rates.

Speciﬁcally, tokens are replenished at a rate of r ≤ 1,
(fractional) tokens per unit of time, and can be accumulated up
to a maximum value of b. Every ofﬂoading decision requires
the availability of and consumes a full token. Consequently,
the token rate, r, upper-bounds the long-term rate at which
images can be ofﬂoaded, while the bucket depth, b, limits the
number of successive such decisions that can be made.

Reusing the notation of [33], the behavior of the token
bucket system can be captured by tracking the evolution of
the token count n[t] in the bucket over time, as follows:

n[t + 1] = min(b, n[t] − a[t] + r),

(1)

where a[t] the ofﬂoading action at t, which is 1 if an image
arrives and is ofﬂoaded (this needs n[t] ≥ 1), and 0 otherwise.
Again as in [33], we assume that both r and b are rational so
that r = N/P and b = M/P for some integers N ≤ P ≤ M .
We can then scale up the token count by a factor of P and
express it as ¯n:

¯n[t + 1] = min(M, ¯n[t] − P × a[t] + N ),

(2)

which ensures that ¯n[t] is an integer in the set {N, N +
1, · · · , M }, with images ofﬂoaded only when ¯n[t] ≥ P .

D. Ofﬂoading Reward and Decisions

The ofﬂoading policy seeks to “spend” tokens on images
that maximize an application-speciﬁc metric (classiﬁcation
accuracy) while conforming to the token bucket constraints.

Suppose at time unit t the image x[t] with ground truth
category y[t] arrives, so that, as deﬁned earlier, the loss of the
classiﬁcation predictions of the weak and strong classiﬁers are
L(W(x[t]), y[t]) and L(S(x[t]), y[t]), respectively. We deﬁne
the ofﬂoading reward R[t] as the reduction in loss through
ofﬂoading the image to the edge:

R[t] = L(W(x[t]), y[t]) − L(S(x[t]), y[t]).

(3)

Under the assumption of a general input process, a policy
π making an ofﬂoading decision a[t] at time t may need to
account for the entire input history up to time t as well as the
scaled token count ¯n[t], namely,

a[t] = π(X[t], ¯n[t]),

(4)

where X[t] is the input history from time 0 to time t that
accounts for past image arrivals and classiﬁcation outputs.

As alluded to in Section IV-B, we seek an ofﬂoading policy
π∗ that maximizes the expected sum of rewards over an inﬁnite
horizon with a discount factor γ ∈ [0, 1). In other words,

π∗ = arg max

E

π

∞
(cid:88)

t=0

γta[t]R[t].

(5)

Note that, when no image arrives at time t, we implicitly
assume that x[t] is null and that correspondingly so is the
classiﬁcation output. The ofﬂoading action a[t] and reward
R[t] are then both 0. This ensures that the input history X[t]
incorporates information on past image inter-arrival times and
the classiﬁcation outputs following each image arrival, with
the policy only making decisions at image arrival times.

V. SOLUTION
We now describe the approach we rely on to derive π∗. The
policy assumes a given pair of image classiﬁers W, S, access
to representative training data, and seeks to specify actions
that maximize an expected discounted reward as expressed in
Eq. (5). There are several challenges in realizing π∗.

The ﬁrst is that, to improve classiﬁcation accuracy by taking
advantage of the edge server’s strong classiﬁer, we need to
identify images with a positive ofﬂoading reward (i.e., images
in (c) as described in Section II). Based on Eq. (3), the reward
associated with an input x(t) depends on the outputs of both
the weak and strong classiﬁers, W(x[t]) and S(x[t]), and
knowledge of the true class y(t) of the input. Unfortunately,
neither S(x[t]) nor y(t) are available at the time an ofﬂoading
decision needs to be made. We address this challenge through
an approach similar to that of [33] that relies on an ofﬂoading
metric m (x), which learns an estimate of the ofﬂoading
reward R[t]. We brieﬂy review this approach in Section V-A.
The second more signiﬁcant challenge is that, as reﬂected in
Eq. (4), policy decisions may need the entire history of inputs
(and associated metrics) to accurately capture dependencies
in arrival patterns and classiﬁcation outputs. The size of the
resulting state space can translate into signiﬁcant complexity,
which we address through a deep reinforcement
learning
approach based on Q-values as in [35]. We expand on this
approach in Section V-B.

In summary, the processing pipeline for each image in an
embedded device has following steps: (1) The weak classiﬁer
classiﬁes the image and produces an output W(x); (2) Using
W(x) the ofﬂoading metric m(x) is computed as an estimate
of the reward R; (3) Q-values are then computed based on the
current state (which includes a history of ofﬂoading metrics
and input inter-arrival times, and the token bucket state) and
an ofﬂoading decision is made. Of note is that Q-values rely
only on current and local information, which allows for timely
ofﬂoading decisions independent of the edge server.

A. Ofﬂoading Metric

As mentioned, each time an image x arrives,

the only
information available after its local processing is the output
of the weak classiﬁer W(x). The ofﬂoading metric m(x)
represents then an estimate for the corresponding ofﬂoading
reward R. We compute m(x) following the approach outlined
in [33, Section 4.1], which uses a training set of K represen-
tative image samples to generate a mapping from the entropy
h (W(x)) of the weak classiﬁer output to the expected reward.
The entropy h(z) of a classiﬁcation output z is given by:
(cid:88)

h(z) = −

zy log zy,

y∈Y

which captures the classiﬁer’s conﬁdence in its result (recall
that the classiﬁer’s output is in the form of a probability
distribution over the set of possible classes). This entropy is
then mapped to an expected ofﬂoading reward using a standard
radial basis function kernel:
(cid:80)K

f (¯h) =

k=1 σ(¯h, hk) × Rk
(cid:80)K
k=1 σ(¯h, hk)

,

(6)

where ¯h = h(z) for classiﬁcation output z, σ(¯h, hk) =
exp (cid:0)−λ(¯h − hk)2(cid:1), and Rk is the reward from the kth sample
in the training set with hk its entropy.

By setting m(x) = f (h(W(x))), we choose an expected
reward that is essentially a weighted average over the entire

5

training set of K images of reward values for training set
inputs with similar entropy values, where images with entropy
values closer to that of image x are assigned higher weights.

B. A Deep Q-Learning Policy

With the metric m(x) of image x in hand, the policy’s
goal is to decide whether to ofﬂoad it given also the system
state as captured in X(t) and ¯n(t), the past history of image
arrivals, classiﬁcation outputs, and the token bucket state. The
potential sheer size of the underlying state space makes a direct
approach impractical. This leads us to exploring the use of
deep Q-learning proposed in [35]. In the remainder of this
section, we ﬁrst provide a brief overview of deep Q-learning
before discussing its mapping to our problem and articulating
its use in learning from our training data set an ofﬂoading
policy that seeks to maximize the expected ofﬂoading reward.
1) Background: Q-learning is a standard Reinforcement
Learning approach for devising policies that maximize a
discounted expected reward summed over an inﬁnite horizon
as expressed in Eq. (5). It relies on estimating a Q-value,
Q(s, a) as a measure of this reward, assuming that the current
state is s and the policy takes action a. As mentioned above, in
our setting, s consists of the arrival and classiﬁcation history
X and the token count ¯n, while a is the ofﬂoading decision.
Estimating Q-values relies on a Q-value function, which in
deep Q-learning is in the form of a deep neural network, or
Deep Q-Network (DQN). Denoting this network as Q, it learns
Q-values during a training phase through a standard Q-value
update. Speciﬁcally, denoting the current DQN as Q− let

Q+(s, a) = R(s, a, s(cid:48)) + γ max

a(cid:48)

Q−(s(cid:48), a(cid:48)),

(7)

where s(cid:48) is the state following action a at state s, R(s, a, s(cid:48))
is the reward from this transition (available during the training
phase) with γ the discount factor of Eq. (5), and both a
and a(cid:48) are selected from the set of feasible actions in the
corresponding states s and s(cid:48).

The value Q+(s, a) is used as the “ground-truth”, with the
difference between Q+(s, a) and Q−(s, a) representing a loss
function to minimize, which can be realized by updating the
weights of the DQN through standard gradient descent. The
approach ultimately computes Q-values for all combinations of
inputs (state s) and possible actions a, and the resulting policy
greedily takes the action with maximum Q-value in each state:

π(s) = arg max

Q(s, a).

(8)

a

The challenges in learning the policy of Eq. (8) are the size
of the state space and the possibility of correlation and non-
stationary input distributions, which can all affect convergence.
Deep Q-learning introduced two additional techniques to ad-
dress those challenges:
Experience replay: The Q-value updates of Eq. (7) rely on
a (s, a, R, s(cid:48)) tuple, where we recall that the state s may
include the entire past history of the system, e.g., the tuple
(X, ¯n) of Eq. (4) in our case. Deep Q-learning generates
(through simulation2) a set of (s, a, R, s(cid:48)) tuples, stores them

2As we shall see shortly, our setting mostly avoids simulations.

6

in a so-called replay buffer, which it then randomly samples
to perform Q-value updates. This shufﬂes the order of the
collected tuples so that the learned Q-values are less likely to
diverge because of bias from groups of consecutive tuples.
Target network: A Q-value update changes the weights of the
DQN and consequently its Q-value estimates in subsequent
updates. Deep Q-learning makes a separate copy of the DQN,
known as the target network, Qtarget, which it then uses across
multiple successive updates. Speciﬁcally, the Q-value update
of Eq. (7) is modiﬁed to use:

Q+(s, a) = R(s, a, s(cid:48)) + γ max

a(cid:48)

Qtarget(s(cid:48), a(cid:48)).

(9)

Weights of the current DQN are still modiﬁed using gradient
descent after each update, but subsequent values continue to
be computed using Qtarget. The two networks are eventually
synchronized, i.e., Qtarget is updated to the current DQN,
but limiting the frequency of such updates has been shown to
improve learning stability.

2) DQN Setup: This section introduces the architecture
and setup of the DQN used to estimate Q-values for making
efﬁcient ofﬂoading decisions based on the structure of the in-
put process, dependencies in the classiﬁcation output, and the
token bucket state. Aspects of relevance to our DQN include
its inputs and outputs, as well as its internal architecture.

Our system state consists of the input X (image arrivals
and classiﬁcation history) and the (scaled) token count ¯n, i.e.,
s = (X, ¯n). For computational efﬁciency, rather than using
raw images, we instead rely on the ofﬂoading metrics m(x)
to estimate Q-values3. The input history X therefore reduces
to (I, m), i.e., the history of image inter-arrival times and
ofﬂoading metrics. As mentioned earlier, the state space is in-
dependent of the strong classiﬁer, so that ofﬂoading decisions
can be made immediately based only on local information.

With this state deﬁnition, Q-values are produced for each
combination of (X, ¯n, a), where a is a (feasible) ofﬂoading
decision. This suggests (X, ¯n, a) as our input to the DQN.
Such a selection is, however, relatively inefﬁcient; both from a
runtime and a training standpoint. From a runtime perspective,
it calls for multiple passes through the DQN, one for each
possible action. More importantly, a different choice of input
can signiﬁcantly improve training efﬁciency.

In particular, token states are a deterministic function of
ofﬂoading actions and our inputs (and metrics) are statistically
independent of actions. This allows the parallel computation of
Q-values across possible actions, and computing (and updating
during the training phase) Q-values for all token bucket states
¯n at the same time without resampling training data based on
policy, i.e., avoid doing proper reinforcement learning. This
can signiﬁcantly improve training efﬁciency. As a result, we
select X as our system input, with our DQN producing a set of
2M −P −N +2 outputs (Q-values), one for each combination
of token bucket states ¯n and ofﬂoading actions a ∈ {0, 1}.

3Using raw images would add a component of complexity comparable
to the weak classiﬁer itself, which is undesirable. An alternative is to
use intermediate features extracted from the weak classiﬁer. This is still
challenging, especially when considering a history of such metrics, as the
dimensionality of these features remains much higher than the ofﬂoading
metric (a scalar), and would likely require a more complex model architecture.

Many recent works in deep reinforcement learning involve
relatively complex deep convolutional neural networks (CNN)
to handle high-dimensional inputs such as raw images, or rely
on more sophisticated algorithms than DQN, e.g., Proximal
Policy Optimization (PPO) [36] or Rainbow [37]. Initial ex-
periments with CNNs did not yield meaningful improvements
over a lightweight multi-layer perceptron (MLP), possibly
from our state space relative low dimensionality. As a result,
given our focus on a light computational footprint, we opted
for a simple MLP architecture with 5 layers and 64 units in
each layer4, and the relative simplicity of the DQN algorithm.
Exploring the feasibility and beneﬁts of more sophisticated RL
algorithms and more complex architectures such as recurrent
neural networks (RNN) is a topic we leave to future work.

3) DQN Learning Procedure: As our inputs X are indepen-
dent of actions and the token state is a deterministic function
of action, we can limit ourselves to generating a sequence of
image arrivals and corresponding the ofﬂoading metrics and
rewards as our training set, which we store in our replay buffer.
During training, the replay buffer is randomly sampled, each
time extracting a ﬁnite history window (segment) of length
T , which is assumed sufﬁcient to allow learning the joint
distribution of inter-arrival times and classiﬁcation outputs.
Segments sampled from the beginning of the image sequence
are zero-padded to ensure a window size of T for all segments.
For each segment, we create an input tuple X = (I, m) that
consists of the ﬁrst T − 1 image inter-arrival times and the
corresponding ofﬂoading metrics. Conversely, the tuple X (cid:48)
includes the same information but for the last T − 1 entries
in the segment, and represents our next “input state”. We can
then adapt the Q-value update expression of Eq. (9) as follows:

Q+(X, ¯n; a) = a · R + γ max

a(cid:48)∈{0,1}

Qtarget(X (cid:48), ¯n(cid:48); a(cid:48)),

(10)

where ¯n is the token state when the current image (last entry
in X) arrives, R is the reward from ofﬂoading it, a is the
ofﬂoading decision for that image (a is 0 when ¯n < P ), and
¯n(cid:48) is the updated token state following action a. Note that
since no additional images can be ofﬂoaded until the next one
arrives, ¯n(cid:48) can be readily computed from ¯n, a, and the last
inter-arrival time IT in X (cid:48), namely,

¯n(cid:48) = min(M, ¯n − P × a + N × IT ),

This also means that for any pair (X, X (cid:48)) from a given seg-
ment in our replay buffer, we can simultaneously update all Q-
values associated with different token states. This signiﬁcantly
speeds-up convergence of our learning process.

VI. EVALUATION

Our goal

is to demonstrate that

the DQN-based policy
(i) estimates Q-values efﬁciently with negligible overhead in
embedded devices, and (ii) can learn complex input structures
to realize ofﬂoading decisions that outperform state-of-the-art
solutions. To that end, we implemented a testbed emulating
a real-world edge computing setting, and,
in addition to
simulations, ran extensive experiments to evaluate the policy’s

4The performance impact of different choices is discussed in Section VI-C4.

7

Image Selection Process:

image inter-arrival time, I1 and I2, with each state having
a given probability tprobi, i = 1, 2, of transitioning to the
other state. Given our discrete-time setting, up to one image
arrives in each time slot, and the two states emulate alternating
periods of high and low image arrival rates. Of interest is the
extent to which DQN recognizes when it enters a state with
a lower/higher image arrival rate and adjusts its ofﬂoading
decisions based not only on the token bucket state but also its
estimate on when the next images might arrive.
In the simplest

im-
ages are selected randomly from the ImageNet dataset. This
results in classiﬁcation outputs with metrics randomly dis-
tributed across the ImageNet distribution. As mentioned in
Section IV-B, this may not be reﬂective of many practical sit-
uations. To create patterns of correlated conﬁdence outputs, we
rank-order the ImageNet dataset by images’ ofﬂoading metric,
and sample it using a simple two-parameter model based on
a sampling spread sp and a location reset probability rprob.
The reset probability rprob determines the odds of jumping to
a new random location in the rank-ordered ImageNet dataset,
while the spread sp identiﬁes a range of images, and therefore
metrics, from which to randomly select once at a location.
Correlation in the metrics of successive classiﬁcation outputs
can then be varied by adjusting sp and rprob.

instance,

3) DQN Conﬁguration: We use the ofﬁcial ILSVRC valida-
tion set with 50000 images (1000 categories with 50 images
each). We evenly split the validation set into three subsets;
two are used as training sets and the third as test set. Given a
token bucket conﬁguration and sequence generator settings, we
generate a training sequence of 108 images from the training
sets along with corresponding inter-arrival times and metrics.
This sequence is stored in the replay buffer from which we
randomly sample (with replacement) input history segments
with a ﬁxed length history window of T = 97 to train
DQN. The effect of the history window length T on DQN’s
performance is investigated in Section VI-C4. Throughout the
training procedure, we synchronize the target network with
DQN every 214 segments, and perform 4000 synchronizations,
for a total of 4000 × 214 ≈ 6.55 × 107 segments for Q-
value updates. The DQN policy is then evaluated with test
sequences of 107 images from the test set sampled using the
same sequence generator settings.

4) Evaluation Scenarios: In evaluating DQN, we vary im-
age arrival patterns, classiﬁcation output correlation, and token
bucket parameters, and compare DQN to several benchmarks.
The ﬁrst is a lower bound that corresponds to a setting
where the weak classiﬁer is limited to only ofﬂoading a ﬁxed
fraction of images based on its token rate r (i.e., images with
ofﬂoading metrics above the (1 − r)th percentile), but it is
not constrained by the bucket size (equivalent to an inﬁnite
bucket size). This lower bound is often not feasible, but barring
knowing an optimal policy, it offers a useful reference.

We also compare DQN to two practical policies. The ﬁrst
is the MDP policy introduced in [33]. It is oblivious to any
structure in either the image arrival process or the classiﬁer
output (it assumes that they are i.i.d.), but is cognizant of the
token bucket state and attempts to adapt its decisions based on
the number of available tokens and its estimate of the long-

Fig. 3. Mapping (red curve) from entropy of weak classiﬁer output to
ofﬂoading metric, with actual rewards for training set images (purple dots).

runtime efﬁciency on embedded devices and its performance
for different conﬁgurations. Section VI-A reviews our ex-
perimental setup. Section VI-B presents our implementation
and empirical evaluation of runtime efﬁciency in embedded
systems. Finally, Section VI-C evaluates our policy’s efﬁcacy
in making ofﬂoading decisions for different input structures.

A. Experimental Setup

1) Classiﬁcation Task: We rely on the standard task of
image classiﬁcation with 1000 categories from the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) to eval-
uate the classiﬁcation performance of our ofﬂoading policy.

Our classiﬁcation metric is the top-5 loss (or error). It
assigns a penalty of 0 if the image is in the ﬁve most likely
classes returned by the classiﬁer and 1 otherwise. The strong
classiﬁer in our edge server is that of [10] with a computational
footprint of 595MFlops. Our weak classiﬁer is a “home-
grown” 16 layers model acting on low-resolution 64 × 64
images with 13 convolutional layers (8 with 1 × 1 kernels
and 5 with 3 × 3 kernels) and 3 fully connected layers.

i.e.,

in most

Given our classiﬁers and the top-5 loss metric, the function
f (h) of Eq. (6) that maps the entropy5 of the weak classiﬁer
output to the ofﬂoading rewards across the training set is
reported in Fig. 3. We note that the relatively low prediction
accuracy of our weak qualiﬁer results in a monotonic mapping
from entropy to metric,
instances where the
weak classiﬁer is very uncertain about its decision, the strong
classiﬁer can provide a more conﬁdent (and accurate) output.
2) Image Sequence Generation: The other main aspect
of our experimental setup is our “image generators.” They
determine both the image arrival process and how those images
are sampled from the ImageNet dataset. The former affects
temporal patterns in image arrivals at the weak classiﬁer, while
the latter determines potential similarities among successive
classiﬁcation outputs. To test our solution’s ability to infer
such patterns, distinct sequence generators separately control
image arrivals and similarities in classiﬁcation outputs.

Image Arrival Process: We rely on a simple two-state
Markov-Modulated mechanism to create variable image arrival
patterns. Each state is associated with a different but ﬁxed

5Prior to computing the entropy, we calibrate the predictions of the weak

classiﬁer using temperature-scaling as outlined in [38].

0123456Entropy h(x)1.00.50.00.51.0Metric m(x) / Reward R(x,y)RewardMetric8

Fig. 4. Traces of ofﬂoading metrics, token bucket states, and time spent in the image classiﬁcation pipeline in a representative experiment.

TABLE I
TIME SPENT ACROSS COMPONENTS IN THE IMAGE CLASSIFICATION PIPELINE

Time

Absolute: mean(std) (ms)

Weak Classiﬁer
20.62(0.57)

DQN
0.25(0.07)

Transmission
40.97(9.30)

Strong Classiﬁer
11.64(5.79)

Relative:

(not ofﬂoaded)
(ofﬂoaded)

98.78%
27.96%

1.22%
0.33%

−
55.84%

−
15.87%

term image arrival rate. The second, denoted as Baseline, is a
ﬁxed threshold policy commonly adopted by many works in
the model cascade framework [22], [23], [26], [27]. Baseline
uses the same threshold as lower bound, i.e., attempting to
ofﬂoad images with ofﬂoading metrics above the (1 − r)th
percentile, but in contrast to lower bound, it needs to conform
to the token bucket constraint at run time. Further, unlike
DQN, it is oblivious to the token bucket state and any structure
in either the arrival process or the classiﬁcation output.

B. Runtime Efﬁciency

To evaluate the feasibility of our DQN-based policy, we
implemented it on a testbed consisting of an embedded device
and an edge server connected over WiFi, and quantiﬁed its
overhead by comparing its runtime execution time on the
embedded device to the time spent in other components in an
end-to-end classiﬁcation task. Next, we brieﬂy describe our
testbed and measurement methodology.

1) Testbed Conﬁguration: Our testbed comprises a Rasp-
berry Pi 4 Model B 8GB that costs ∼$75 as the embedded
device and a server equipped with an Intel(R) Core(TM) i7-
10700K CPU @ 3.80GHz and Nvidia GeForce RTX 3090
GPU as the edge server. The pair of weak and strong classiﬁers
of Section VI-A are deployed on the embedded device and the
edge server, respectively. To further accelerate the inference
speed of the weak classiﬁer, we convert the weak classiﬁer
to an 8-bit quantized TensorFlow Lite model and accelerate

the inference with a Coral USB accelerator. The DQN is also
converted to a ﬂoat16 TensorFlow Lite model. The Raspberry
Pi and the edge server communicate over a WiFi network using
the 802.11/n mode from the 2.4GHz frequency band.

We resize the ILSVRC validation images to 236 × 236 in
the pre-processing stage to unify the input images size to
1.34 × 106 bits, and set the image arrival rate to 5 images/sec.
To introduce correlation in consecutive classiﬁcations, we use
sp = 0.1 and rprob = 0.1 for the classiﬁer output process.

The token bucket

is conﬁgured with a rate r = 0.1
(i.e., a long-term ofﬂoading rate of one out of 10 images
or 0.67 Mbps) and a bucket size b = 4 (i.e., allowing the
ofﬂoading of up to 4 consecutive images). We note that while
the rate of 0.67 Mbps is well below the bandwidth of the
WiFi network, that bandwidth would in practice be shared
among many embedded devices, so that rate controlling their
individual transmissions, as we do, would be required.

2) Computation Cost: To quantify the overhead that DQN
imposes, we measure where time is spent across the differ-
ent components of the classiﬁcation pipeline. The embedded
device ﬁrst classiﬁes every image using its weak classiﬁer,
and then executes the DQN model to estimate the Q-values
before making an ofﬂoading decision that accounts for the
current token bucket state. Ofﬂoaded images are transmitted
to the edge server over the network and ﬁnally classiﬁed by the
strong classiﬁer. Hence, a full classiﬁcation task includes four
main stages, (i) weak classiﬁer inference, (ii) DQN inference,
(iii) network transmission, and (iv) strong classiﬁer inference,

0.02.55.07.510.012.515.017.520.00.00.20.40.6Offloading Metric0.02.55.07.510.012.515.017.520.001234Token Count0.02.55.07.510.012.515.017.520.0Image Arrival Time (s)0204060Time (ms)12.412.612.813.013.22022offloadednot offloadedoffloadednot offloadedstrongtransmissiondqnweakwhich all contribute to how long it takes to complete.

The bottom section of Fig. 4 plots those respective time
contributions for a representative experiment involving a se-
quence of 100 images, with the two other sections of the ﬁgure
reporting the metrics computed by DQN for each image (top)
and the corresponding token counts (middle) and ofﬂoading
decisions. As we detail further in the rest of the section, the
results illustrate how DQN takes both the ofﬂoading metric
of each image and the token bucket state into account when
making ofﬂoading decisions.

As shown in Table I, DQN only takes 0.25 ms on average.
This is just over 1% of the time spent in the weak classiﬁer,
and for ofﬂoaded images, it is less than a third of a percent of
the total classiﬁcation pipeline time. This demonstrates that the
beneﬁts DQN affords impose a minimal overhead. Quantifying
those beneﬁts is the focus of the next section.

C. Policy Performance

In this section, we evaluate DQN’s performance across a
range of scenarios, which illustrate its ability to learn complex
input structures and highlight how this affects its ofﬂoading
decisions. To that end we proceed in three stages. In the ﬁrst
two, we introduce complexity in only one dimension of the
input structure, i.e., correlation is present in either classiﬁ-
cation outputs or image arrivals. This facilitates developing
insight into how such structure affects DQN’s decisions. In
the third stage, we create a scenario with complexity in both
classiﬁcation outputs and image arrivals, and use it to demon-
strate DQN’s ability to learn policies when complexity spans
multiple dimensions. Finally, as a sanity check, we evaluate
how different choices of model parameters, including history
window length T , number of hidden layers, and number of
units in each layer, affect the performance of DQN.

1) Deterministic Image Arrivals and Correlated Classiﬁ-
cation Outputs: To explore DQN’s ability to learn about the
presence of correlation in classiﬁcation outputs, we ﬁrst ﬁx the
token bucket parameters to r = 0.1 and b = 4, and vary the
two hyper-parameters of our sequence generator to realize dif-
ferent levels of classiﬁcation output correlation: The sampling
spread sp is varied from 0 (single image) to 1 (full dataset and,
therefore, no correlation), while the reset probability rprob is
varied from 10−3 to 1 (no correlation). Fig. 5 reports the top-5
loss for DQN and our three benchmarks.

As expected, when either sp or rprob are large so that
classiﬁcation output correlation is minimal, both DQN and
MDP perform similarly and approach the performance of the
lower bound. However, when classiﬁcation output correlation
is present, DQN consistently outperforms MDP (and the Base-
line). As correlation increases, performance degrades when
compared to the lower bound, but this is not surprising given
the token bucket constraints. Correlation in the classiﬁcation
output means that sequences of either high or low metrics are
more likely, which are harder to handle under token bucket
constraints. A sequence of high metric images may rapidly
deplete a ﬁnite token bucket, so that it may not be possible to
ofﬂoad all of them, irrespective of how forward looking the
policy is. Conversely, a sequence of low metric images may

9

Fig. 5. Ofﬂoading policies performance as a function of classiﬁer output
correlation. Correlation decreases as spread sp (Top) or location resetting
probability rprob (Bottom) increase. Token bucket: r = 0.1, b = 4.

Fig. 6. Ofﬂoading policies performance for different token bucket conﬁgura-
tions under correlated classiﬁcation outputs (sp = 0.1 and rprob = 0.1).

result in wasted tokens (the bucket ﬁlls up) even if, as we shall
see, the DQN policy is able to mitigate this by recognizing
that it has entered such a period and adapting its behavior.

This is illustrated in the top portion of Fig. 7 that reports
traces of classiﬁcation outputs and policy decisions for a
sample conﬁguration of Fig. 5 (sp restricts classiﬁcation output
metrics to a range of 10% of the full set, while rprob results
in an average of 100 images consecutively sampled from that
range). When compared to MDP, DQN recognizes when it
enters periods of low metrics and proceeds to ofﬂoad some low
metric images while MDP does not. Conversely, both policies
perform mostly similarly during periods of high metric.

Fig. 5 relied on a single token bucket conﬁguration, (r, b) =
(0.1, 4). Fig. 6 extends this by still relying on a particular
pattern of classiﬁcation output correlation (sp = 0.1 and
rprob = 0.1), but now for different token bucket conﬁg-
urations. Speciﬁcally, we select three different token rates,
r = 0.05, 0.1, 0.25 and for each vary the token bucket depth b
from 1 to 20. The ﬁgure demonstrates that DQN consistently
outperforms MDP and Baseline, even if the difference dimin-
ishes as either r or b increases. This is expected. A larger
token rate lowers the cost of missed ofﬂoading opportunities
because of wasted tokens, while a larger bucket depth makes
ofﬂoading decisions less dependent on accurately predicting
classiﬁcation output correlation in successive images.

We illustrate the latter in Fig. 7, where we again plot traces
of the decisions that the DQN and MDP policies make for
a scenario with correlated output metrics (sp = 0.1 and
rprob = 0.01) and two different bucket depths, b = 4 (Top)
and b = 20 (Bottom). We note that the value rprob = 0.01

0.00.20.40.60.81.0Sub-Sequence Spread sp0.240.250.260.270.28Average Top-5 Lossrprob = 0.10.00.20.40.60.81.0Sub-Sequence Spread sp0.240.250.260.270.280.29rprob = 0.010.00.20.40.60.81.0Sub-Sequence Spread sp0.240.250.260.270.280.290.30rprob = 0.0013210Log (base 10) ScaledLocation Reset Probability rprob0.240.250.260.270.280.29Average Top-5 Lossspread = 0.13210Log (base 10) ScaledLocation Reset Probability rprob0.240.250.260.270.28spread = 0.23210Log (base 10) ScaledLocation Reset Probability rprob0.2400.2450.2500.2550.2600.265spread = 0.5DQNMDPBaselineLower Bound5101520Bucket depth b0.2700.2750.2800.2850.290Average Top-5 LossRate r = 0.055101520Bucket depth b0.240.250.260.270.28Rate r = 0.15101520Bucket depth b0.180.200.220.240.26Rate r = 0.25DQNMDPBaselineLower Bound10

Fig. 9. Ofﬂoading policies performance for different token bucket conﬁgura-
tions under correlated image arrivals (I1 = 1, I2 = 3 and tprob1 = 0.001,
tprob2 = 0.0005).

inter-arrival times of I1 = 1 and I2 = 3, i.e., images in
every time slot versus every three time slots. We set the ratio
of the transition probabilities out of each state to two, i.e.,
tprob1/tprob2 = 2 so that the low intensity state lasts twice
as long, and vary the state transition probability out of state 1,
tprob1, from 10−3 to 10−0.5. Conﬁguration 2 uses I1 = 1
and I2 = 6, i.e., images again in every time slot in the high
intensity state, but only every six time slots in the low intensity
state, with tprob1/tprob2 = 4, i.e., a low intensity state that
now lasts four times as long. As with the ﬁrst conﬁguration,
we vary tprob1 from 10−3 to 10−0.5.

The results are in Fig. 8, which reports the average top-5
loss for DQN and our three benchmarks for conﬁgurations 1
(Left) and 2 (Right). DQN’s ability to learn the structure
of the arrival process improves performance (lower Top-5
loss) over both MDP and Baseline, with those improvements
diminishing6 as correlation in the arrival process decreases
(increased transition probabilities out out each state).

To better understand how learning about the arrival process
affects DQN’s ofﬂoading decisions, we again use a sample
trace showing the decisions of both DQN and MDP for a
sequence of image arrivals. To illustrate DQN’s ability to
“recognize” rate transitions, the trace explicitly includes one.
The results are reported in the top portion of Fig. 10 for conﬁg-
uration 1 with state transition probabilities of tprob1 = 0.001,
tprob2 = 0.0005. The transition from high to low arrival
intensity is indicated by a vertical line in the ﬁgure.

In the high arrival rate state (left of the dividing line),
DQN is more conservative than MDP with slightly fewer
ofﬂoading decisions. This is, however, offset by its ability to
ofﬂoad some higher metric images than MDP whose more
aggressive behavior resulted in an empty token bucket when
those images arrived. Conversely, once DQN recognizes that it
has transitioned to a state with a lower image arrival rate (right
of the dividing line), it proceeds to be more aggressive and
selects more lower metric images as it knows that the lower
image arrival rate means that tokens will be replenished faster
relative to image arrivals. In contrast, MDP ends-up wasting
tokens it could have used during periods of lower arrival rate.
Next, we investigate the extent to which the results of Fig. 8
remain under different token bucket conﬁgurations. For that
purpose, we select conﬁguration 1 with I1 = 1, I2 = 3
and tprob1 = 0.001, tprob2 = 0.0005. Fig. 9 reports the
performance (top-5 loss) of DQN and our three benchmarks

6As mentioned in Section VI-A2, changes in tprobi, i = 1, 2 affect the
image arrival rate. Hence, the changes in the lower bound as they increase.

Fig. 7. Ofﬂoading decisions of DQN and MDP for token bucket depths of
b = 4 (Top) and b = 20 (Bottom) under correlated classiﬁcation outputs
(sp = 0.1 and rprob = 0.01).

Fig. 8. Ofﬂoading policies performance as function of image arrivals
correlation. Correlation decreases as transition probabilities tprob1, tprob2
increase. Token bucket: r = 0.1, b = 4.

i.e., rprob = 0.1. The
differs from that used in Fig. 6,
motivation is visual clarity, as the lower rprob value stretches
the periods during which classiﬁcation output metrics are
sampled from a given range, which ampliﬁes differences in
policy decisions. Comparing the Top and Bottom parts of the
ﬁgures, we see that when b is larger, DQN recognizes that
the odds of wasting tokens during periods of low metrics are
lower, which results in fewer ofﬂoading decisions during those
times. This is especially so after periods of high metrics, e.g.,
after t ≈ 500, when the token bucket count is low.

2) Markov-modulated Image Arrivals and I.I.D. Classiﬁ-
cation Outputs: Next, we proceed to demonstrate that DQN
can also learn variations in the structure of the image arrival
process, and in particular changes in the arrival rate that extend
over long enough periods of time to affect ofﬂoading decisions.
As the focus is on variations in the image arrival process, we
rely on a simple i.i.d. structure for the classiﬁer outputs.

As in the previous section, we chose r = 0.1, b = 4, as our
base token bucket conﬁguration, and evaluate ofﬂoading per-
formance under Markov-modulated image arrival processes.
We rely on two base conﬁgurations. Conﬁguration 1 alternates
between high and low intensity states with constant image

02004006008001000Time0.00.10.20.30.40.50.6Offloading Metricr = 0.1, b = 4, sp = 0.1, rprob = 0.0102004006008001000Time0.00.10.20.30.40.50.6Offloading Metricr = 0.1, b = 20, sp = 0.1, rprob = 0.01offloaded by DQNoffloaded by MDPoffloaded by bothnot offloaded3.02.52.01.51.00.5Log (base 10) ScaledTransition Probability tprob10.1950.2000.2050.2100.2150.2200.225Average Top-5 LossI1 = 1, I2 = 33.02.52.01.51.00.5Log (base 10) ScaledTransition Probability tprob10.140.160.180.20I1 = 1, I2 = 6DQNMDPBaselineLower Bound5101520Bucket depth b0.2450.2500.2550.2600.2650.270Average Top-5 LossRate r = 0.055101520Bucket depth b0.200.210.220.230.240.25Rate r = 0.15101520Bucket depth b0.100.120.140.160.18Rate r = 0.25DQNMDPBaselineLower Bound11

we carried out an extensive set of experiments where we varied
the image arrival process, classiﬁcation output correlation,
and token bucket parameters. The goal was to offer a broad
coverage of different conﬁgurations and evaluate DQN’s per-
formance across them. The results of those experiments were
essentially similar to those found in earlier sections, with DQN
outperforming the MDP and Baseline benchmarks across all
conﬁgurations with differences of similar magnitude. Because
those results do not offer much additional insight, we omit
them and instead focus on the analysis of a trace that helps
shed some light on how DQN translates what it learns about
the structure of its inputs into policy decisions.

This trace is reported in Fig. 11. It consists of image arrivals
generated according to a Markov-modulated process similar to
that of Fig. 10, namely, I1 = 1, I2 = 3, tprob1 = 0.001, and
tprob2 = 0.0005, with classiﬁcation outputs that exhibit the
same correlation structure as in Fig. 7, namely, sp = 0.1 and
rprob = 0.01. In other words, the trace captures a sequence of
inputs with both correlated arrivals and classiﬁcation outputs.
As in most previous experiments, token bucket parameters
were set to r = 0.1 and b = 4.

The ﬁgure reports (using again dots of different colors)
the ofﬂoading decisions of two versions of the DQN policy.
Both were trained using the same sequences of image arrivals,
but their training sequences differed in the structure of the
corresponding classiﬁcation outputs. The ﬁrst version, DQN−,
was trained with i.i.d. classiﬁcation outputs, while the second,
DQN+, was trained with the correlated classiﬁcation outputs
present in the trace of Fig. 11 (the trace involves one transition
in the arrival rate at t = 711 and multiple periods that span
different ranges of ofﬂoading metrics on each side of that
transition). The purpose is to illustrate how learning about
classiﬁcation output correlation affects DQN’s decisions given
that it already knows about arrival correlation. In other words,
Sections VI-C1 and VI-C2 demonstrated DQN’s ability to
learn and use structure in either the image arrival process or
the classiﬁcation output. The intent is to show it can learn both
by comparing two DQN versions that differ in the information
available to them regarding classiﬁcation outputs.

As in Fig. 10, both policies are aware of possible changes in
image arrival rates. Hence, they each ofﬂoad more aggressively
(lower metrics) after detecting a transition to a state with
lower image arrival rate (a decrease of 20.27% and 19.34% for
DQN+ and DQN−, respectively, in the metrics of ofﬂoaded
images between before and after t = 711). The more interest-
ing aspect though is in the differences in decisions between the
two policies as correlation in classiﬁcation outputs produces
periods with different correlation ranges, a phenomenon that
was not incorporated in the training of DQN−. This is best
seen by focusing on two speciﬁc regions for which this is
more visually apparent, namely, a period of relatively high
metrics in the interval t ∈ [222, 480] and conversely a period
of relatively low metrics in the interval t ∈ [691, 963].

During the high metrics interval, DQN+ becomes aware
that it can expect consistently higher metrics, and so adjusts
its decisions to be more conservative. Both policies ofﬂoad
roughly the same number of images, 27 and 29 for DQN+
and DQN−, respectively, but the average metric of images

Fig. 10. Ofﬂoading decisions of DQN and MDP for token bucket depths of
b = 4 (Top) and b = 20 (Bottom) under correlated image arrivals (I1 = 1,
I2 = 3, tprob1 = 0.001, tprob2 = 0.0005).

across a range of token bucket conﬁgurations, namely, token
rates of r = 0.05, 0.1, 0.25, and token bucket depths that vary
from b = 1 to 20. The ﬁgure illustrates that DQN continues
to outperform MDP across all conﬁgurations, even if, as with
Fig. 6, the differences are smaller than between MDP and
the Baseline. The latter ignores the token bucket state, which
remains the main contributor to differences in performance.

Towards better understanding factors that inﬂuence differ-
ences between DQN and the MDP policy in the presence of
arrival correlation, the bottom part of Fig. 10 reports a trace
of image arrivals (I1 = 1, I2 = 3, tprob1 = 0.001, tprob2 =
0.0005) and policy decisions that parallels that of the top
part of the ﬁgure, but for a different token bucket depth,
i.e., b = 20 versus b = 4. The bigger bucket depth means
that MDP’s overly aggressive behavior during periods of high
arrival rate (it still assumes the lower long-term rate) has less
of an impact, as the larger bucket makes it easier to sustain
least for a period of time).
the higher ofﬂoading rate (at
This is illustrated by the fewer policy decision differences
between MDP and DQN in the bottom part of the ﬁgure’s left-
hand-side. Conversely, the larger bucket also means that DQN
needs not be as aggressive during periods of lower arrival rate
since the larger bucket reduces the odds of wasting tokens by
not ofﬂoading enough images. This is reﬂected in the higher
metrics used by DQN in its ofﬂoading decisions in the right-
hand-side of the bottom part of Fig. 10.

information about

3) Markov-modulated Image Arrival and Correlated Classi-
ﬁcation Outputs: This sub-section demonstrates DQN’s ability
the structure of both image
to extract
arrivals and classiﬁcation outputs, and to use it in its of-
ﬂoading decisions. For that purpose, we combine the corre-
lated image sequence generator of Section VI-C1 with the
Markov-modulated input process of Section VI-C2 to produce
a sequence of arrivals with both variable arrival rate and
correlation in the classiﬁcation accuracy of successive outputs.
In keeping with the structure of Sections VI-C1 and VI-C2,

02004006008001000120014001600Time0.00.20.40.6Offloading Metricr = 0.1, b = 4, I1 = 1, I2 = 302004006008001000120014001600Time0.00.20.40.6Offloading Metricr = 0.1, b = 20, I1 = 1, I2 = 3offloaded by DQNoffloaded by MDPoffloaded by bothnot offloadedstate transition12

Fig. 11. Ofﬂoading decisions of DQN (trained on the combined correlation) and DQN (trained on correlated arrivals with i.i.d. classiﬁer outputs) on a
combined correlation setting. For the token bucket conﬁguration of r = 0.1, b = 4, we consider the image arrival I1 = 1, I2 = 3, tprob1 = 0.001,
tprob2 = 0.0005, combined with correlated classiﬁer output of sp = 0.1, rprob = 0.01. We compare the ofﬂoading decisions of DQN trained on this
combined correlation setting, and DQN trained on the given arrival process, but

i.i.d. classiﬁer outputs.

(and MDP7) for different values of log2 T . The lowest value
(T = 2) corresponds to a setting where DQN uses only the
current ofﬂoading metric and image inter-arrival time, while
the largest setting of T = 128 offers enough samples for
DQN to learn the correlation structure in both arrivals and
classiﬁcation outputs.

The results display relatively limited sensitivity to the choice
of T even if some variations are present. Of note is the fact
that even in the absence of any history (T = 2), DQN still out-
performs MDP because it can use its knowledge of the current
inter-arrival time to make better policy decisions (MDP only
has access to the current ofﬂoading metric and token bucket
state). As T increases and more history information becomes
available, DQN quickly stabilizes at its best performance and
remains insensitive to T over a wide range. Performance
eventually starts to decrease as T becomes too large. This is
likely because its simple architecture (a 5 × 64 MLP) does not
contain a sufﬁciently large number of parameters to interpret
all the information within the high-dimensionality input.

We also performed a grid search on the model parameters by
varying the number of hidden layers from 3 to 8, and the base 2
logarithm of the number of units in each layer from 4 to 8. As
when varying the history window length T , we observed only
small variations (within 1%) in the relative difference between
the best and the worst performance for the top-5 loss. This
indicates limited sensitivity of the model to these choices.

VII. CONCLUSION

The paper investigates a distributed image classiﬁcation
problem in an edge-assisted AIoT setting, where classiﬁcation
accuracy is improved by dynamically ofﬂoading some images
to an edge server subject to network bandwidth constraints.
Managing access to the shared network is regulated through
a token bucket that constrains ofﬂoading decisions. The paper
devises and evaluates a policy that manages ofﬂoad decisions
from devices under such constraints while optimizing classiﬁ-
cation accuracy. Because image arrival patterns and classiﬁca-

Fig. 12. DQN’s performance as a function of the log (base 2) of the history
window length T on a representative setting that combines correlation in both
image arrivals (I1 = 1, I2 = 3, tprob1 = 0.001, tprob2 = 0.0005) and
classiﬁcation outputs (sp = 0.1, rprob = 0.01), and token bucket parameters
of r = 0.1 and b = 4.

ofﬂoaded by DQN+ is 0.580 versus 0.572 for DQN−, a small
but meaningful difference, especially given the narrow range of
metrics sampled during that period (between 0.529 and 0.585).
Conversely, during the low metrics interval, DQN+ realizes
it will be getting images with relatively low metrics
that
for some time, and consequently lowers its expectations and
ofﬂoads lower metric images to avoid wasting tokens. This
results in DQN+ ofﬂoading 24 images during that period
versus only one image for DQN−. Those differences highlight
how the DQN+ policy leverage the additional information it
learned about the structure of classiﬁcation outputs. In turn,
those resulted in improved performance with average top-5
losses of 0.247 and 0.253 for DQN+ and DQN−, respectively.

4) DQN Modeling Parameters: In this last sub-section we
investigate how the DQN’s parameters, including the history
window length T , the number of layers, and the number of
units in each layer, impact performance of our policy. We
report results for a setting that combines both variable image
arrival rate and correlated classiﬁcation output, as it represents
a more complex environment for which the choice of window
length can, therefore, be anticipated to have a greater impact.

Fig. 12 reports the performance (average top-5 loss) of DQN

7MDP is included only to show that DQN outperforms it for all T values.

02004006008001000120014001600Time0.00.10.20.30.40.50.6Offloading Metricr = 0.1, b = 4, sp=0.1, rprob=0.01, I1 = 1, I2 = 3DQN trained on correlated outputsDQN trained on i.i.d. outputsoffloaded by bothnot offloadedstate transition1234567Log (base 2) Scaled History Window Length T0.2460.2480.2500.252Average Top-5 LossDQNMDP13

[21] Y. Matsubara, D. Callegaro, S. Baidya, M. Levorato, and S. Singh,
“Head network distillation: Splitting distilled deep neural networks for
resource-constrained edge computing systems,” IEEE Access, vol. 8,
2020.

[22] X. Wang, Y. Luo, D. Crankshaw, A. Tumanov, F. Yu, and J. E. Gonzalez,
“IDK cascades: Fast deep learning by learning not to overthink,” in Proc.
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2018.
[23] A. Kouris, S. I. Venieris, and C.-S. Bouganis, “CascadeCNN : Pushing
the performance limits of quantisation in convolutional neural networks,”
in Proc. 28th Intl. Conference on Field Programmable Logic and
Applications (FPL), 2018.

[24] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen, “DeepDecision: A mobile
deep learning framework for edge video analytics,” in Proc. IEEE Intl.
Conference on Computer Communications (INFOCOM), 2018.

[25] J. Wang, Z. Feng, Z. Chen, S. George, M. Bala, P. Pillai, S.-W. Yang,
and M. Satyanarayanan, “Bandwidth-efﬁcient live video analytics for
drones via edge computing,” in Proc. IEEE/ACM Symposium on Edge
Computing (SEC), 2018.

[26] S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Distributed deep
neural networks over the cloud, the edge and end devices,” in Proc.
IEEE 37th Intl. Conference on Distributed Computing Systems (ICDCS),
2017.

[27] S. Laskaridis, S. I. Venieris, M. Almeida, I. Leontiadis, and N. D.
Lane, “SPINN: Synergistic progressive inference of neural networks
over device and cloud,” in Proc. 26th Annual Intl. Conference on Mobile
Computing and Networking (MOBICOM), 2020.

[28] L. Zeng, E. Li, Z. Zhou, and X. Chen, “Boomerang: On-demand
cooperative deep neural network inference for edge intelligence on the
industrial internet of things,” IEEE Network, vol. 33, no. 5, 2019.
[29] H. Lin, S. Zeadally, Z. Chen, H. Labiod, and L. Wang, “A survey
on computation ofﬂoading modeling for edge computing,” Journal of
Network and Computer Applications, vol. 169, 2020.

[30] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized
computation ofﬂoading performance in virtual edge computing systems
via deep reinforcement learning,” IEEE Internet of Things Journal,
vol. 6, no. 3, 2019.

[31] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “Learning-
based computation ofﬂoading for iot devices with energy harvesting,”
IEEE Trans. Vehicular Technology, vol. 68, no. 2, 2019.

[32] L. Huang, S. Bi, and Y.-J. A. Zhang, “Deep reinforcement learning
for online computation ofﬂoading in wireless powered mobile-edge
computing networks,” IEEE Trans. Mobile Comp., vol. 19, no. 11, 2020.
[33] A. Chakrabarti, R. Gu´erin, C. Lu, and J. Liu, “Real-time edge classi-
ﬁcation: Optimal ofﬂoading under token bucket constraints,” in Proc.
IEEE/ACM Symposium on Edge Computing (SEC), 2021.

[34] D. M. Lucantoni, K. S. Meier-Hellstern, and M. F. Neuts, “A single-
server queue with server vacations and a class of non-renewal arrival
processes,” Advances in Applied Probability, vol. 22, no. 3, 1990.
[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing Atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[36] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[37] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski,
W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow:
Combining improvements in deep reinforcement learning,” in Proc. 32nd
AAAI conference on artiﬁcial intelligence (AAAI), 2018.

[38] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of
modern neural networks,” in Proc. 34th Intl. Conference on Machine
Learning (ICML), 2017.

tion results can be arbitrary, the policy needs to accommodate
complex input sequences. To that end, we investigate the use
of DQN to realize such a policy, and demonstrate its ability
to effectively “learn” effective policy decisions. Experiments
demonstrate both the efﬁcacy of the DQN-based ofﬂoading
policy and its runtime efﬁciency on embedded devices with
limited computational resources.

REFERENCES

[1] M. Almeida, S. Laskaridis, I. Leontiadis, S. I. Venieris, and N. D. Lane,
“EmBench: Quantifying performance variations of deep neural networks
across modern commodity devices,” in Proc. 3rd Intl. Workshop on Deep
Learning for Mobile Systems and Applications, 2019.

[2] A. Ignatov, R. Timofte, A. Kulik, S. Yang, K. Wang, F. Baum, M. Wu,
L. Xu, and L. Van Gool, “AI benchmark: All about deep learning on
smartphones in 2019,” in Proc. IEEE/CVF Intl. Conference on Computer
Vision Workshop (ICCVW), 2019.

[3] S. Wang, A. Pathania, and T. Mitra, “Neural network inference on mobile

SoCs,” IEEE Design & Test, vol. 37, no. 5, 2020.

[4] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“MobileNetV2: Inverted residuals and linear bottlenecks,” in Proc. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[5] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
neural network with pruning, trained quantization and Huffman coding,”
in Proc. Intl. Conference on Learning Representations (ICLR), 2016.
[6] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam,
and D. Kalenichenko, “Quantization and training of neural networks for
efﬁcient integer-arithmetic-only inference,” in Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.

[7] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision
and challenges,” IEEE Internet of Things Journal, vol. 3, no. 5, 2016.
[8] D. Medhi and K. Ramasamy, Network Routing, 2nd ed. Boston, MA:

Morgan Kaufmann, 2018.

[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation
with deep convolutional neural networks,” in Proc. Advances in Neural
Information Processing Systems (NeurIPS), 2012.

[10] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once for all: Train
one network and specialize it for efﬁcient deployment,” in Proc. Intl.
Conference on Learning Representations (ICLR), 2020.

[11] J. Ren, Y. Guo, D. Zhang, Q. Liu, and Y. Zhang, “Distributed and
efﬁcient object detection in edge computing: Challenges and solutions,”
IEEE Network, vol. 32, no. 6, 2018.

[12] L. Liu, H. Li, and M. Gruteser, “Edge assisted real-time object detection
for mobile augmented reality,” in Proc. 25th Annual Intl. Conference on
Mobile Computing and Networking (MOBICOM), 2019.

[13] Q. Liu and T. Han, “Dare: Dynamic adaptive mobile augmented reality
with edge computing,” in Proc. IEEE 26th Intl. Conference on Network
Protocols (ICNP), 2018.

[14] G. Muhammad and M. S. Hossain, “Emotion recognition for cognitive
edge computing using deep learning,” IEEE Internet of Things Journal,
vol. 8, no. 23, 2021.

[15] K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, and
J. Jiang, “Server-driven video streaming for deep learning inference,” in
Proc. of the Annual conference of the ACM Special Interest Group on
Data Communication on the applications, technologies, architectures,
and protocols for computer communication (SIGCOMM), 2020.
[16] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and
L. Tang, “Neurosurgeon: Collaborative intelligence between the cloud
and mobile edge,” SIGARCH Comput. Archit. News, vol. 45, no. 1, 2017.
[17] A. E. Eshratifar, M. S. Abrishami, and M. Pedram, “JointDNN: An
training and inference engine for intelligent mobile cloud
efﬁcient
computing services,” IEEE Trans. Mobile Comp., vol. 20, no. 2, 2021.
[18] A. E. Eshratifar, A. Esmaili, and M. Pedram, “BottleNet: A deep learning
architecture for intelligent mobile cloud computing services,” in Proc.
IEEE/ACM Intl. Symposium on Low Power Electronics and Design
(ISLPED), 2019.

[19] J. Shao and J. Zhang, “BottleNet++: An end-to-end approach for feature
compression in device-edge co-inference systems,” in Proc. IEEE Intl.
Conference on Communications Workshops (ICC Workshops), 2020.

[20] Y. Matsubara, S. Baidya, D. Callegaro, M. Levorato, and S. Singh, “Dis-
tilled split deep neural networks for edge-assisted real-time systems,” in
Proc. Workshop on Hot Topics in Video Analytics and Intelligent Edges,
2019.

