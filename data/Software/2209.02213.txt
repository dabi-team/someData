Deep Neural Network Augmented Wireless Channel
Estimation on System on Chip

Syed Asrar ul haq, Abdul Karim Gizzini, Shakti Shrey, Sumit J. Darak, Sneh Saurabh and Marwa Chaﬁi

1

2
2
0
2

p
e
S
6

]

R
A
.
s
c
[

1
v
3
1
2
2
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Reliable and fast channel estimation is crucial for
next-generation wireless networks supporting a wide range of
vehicular and low-latency services. Recently, deep learning (DL)
based channel estimation is being explored as an eﬃcient al-
ternative to conventional least-square (LS) and linear minimum
mean square error (LMMSE) based channel estimation. Unlike
LMMSE, DL methods do not need prior knowledge of channel
statistics. Most of these approaches have not been realized on
system-on-chip (SoC), and preliminary study shows that their
complexity exceeds the complexity of the entire physical layer
(PHY). The high latency of DL is another concern. This paper
considers the design and implementation of deep neural network
(DNN) augmented LS-based channel estimation (LSDNN) on
Zynq multi-processor SoC (ZMPSoC). We demonstrate the gain
in performance compared to the conventional LS and LMMSE
channel estimation schemes. Via software-hardware co-design,
word-length optimization, and reconﬁgurable architectures, we
demonstrate the superiority of the LSDNN architecture over the
LS and LMMSE for a wide range of SNR, number of pilots,
preamble types, and wireless channels. Further, we evaluate the
performance, power, and area (PPA) of the LS and LSDNN
application-speciﬁc integrated circuit (ASIC) implementations
in 45 nm technology. We demonstrate that the word-length
optimization can substantially improve PPA for the proposed
architecture in ASIC implementations.

Index Terms—Channel estimation, Deep learning, Least-
square, Linear minimum mean square error, System on chip,
FPGA, ASIC, hardware software co-design

I. Introduction
Accurate channel state information estimation is crucial for
reliable wireless networks [1]–[4]. Conventional least square
(LS) based channel estimation is widely used in long-term
evolution (LTE) and WiFi networks due to its low complexity
and latency. However, it performs poorly at a low signal-to-
noise ratio (SNR) [5]. Other techniques like linear minimum
mean square error (LMMSE) oﬀer better performance than LS
but need prior knowledge of noise, and second-order channel
statistics [6]. Furthermore, it is computationally intensive [6],
[7]. The limited sub-6 GHz spectrum has led to the co-
existence of heterogeneous devices with limited guard bands.
This has resulted in poor SNR due to high noise ﬂoor
and adjacent-channel interference. This has led to signiﬁcant
interest in improving the performance of the conventional
LS and LMMSE channel estimation approaches [8] [9]. With
the integration of vehicular and wireless networks, the radio
environment has become increasingly dynamic, posing serious

This work is supported by the funding received from core research grant

(CRG) awarded to Dr. Sumit J. Darak from DST-SERB, GoI.

Syed Asrar ul haq, Shakit Shrey, Sumit J. Darak and Sneh Saurabh are
with Electronics and Communications Department, IIIT-Delhi, India-110020
(e-mail: {syedh,shakti20323,sumit,sneh}@iiitd.ac.in)

Abdul Karim Gizzini is with ETIS, UMR8051, CY Cergy Paris Universit´e,

ENSEA, CNRS, France (e-mail: abdulkarim.gizzini@ensea.fr).

Marwa Chaﬁi

is with the Engineering Division, New York Uni-
versity (NYU) Abu Dhabi, 129188, UAE,
and NYU WIRELESS,
NYU Tandon School of Engineering, Brooklyn, 11201, NY (e-mail:
marwa.chaﬁi@nyu.edu).

challenges in establishing reliable communication links. Fur-
thermore, high-speed ultra-reliable services demand ultra-low
latency of the order of milliseconds [10].

Recent advances in artiﬁcial intelligence, machine and deep
learning (AI-MDL) have been explored to improve the per-
formance of wireless physical layer such as channel estima-
tion, symbol recovery, link adaptation, localization, etc. [11]–
[19]. Various studies have shown that data-driven AI-MDL
approaches have potential to signiﬁcantly improve the perfor-
mance and oﬀer increased robustness towards imperfections,
non-linearity, and dynamic nature of the wireless environment
and radio front-end [10], [20]–[23]. In the June 2021 3GPP
workshop, various industry leaders presented the framework
for the evolution of intelligent and reconﬁgurable wireless
physical layer (PHY), emphasizing the commercial potential of
the AI-MDL based academic research in the wireless domain.
Though intelligent and reconﬁgurable PHY may take a few
years to get accepted commercially, some of these initiatives
are expected to be included in the upcoming 3GPP Rel-18.

Few works have replaced the LS, and LMMSE channel
estimation with computationally complex deep learning (DL)
architectures [24]–[28]. However, most existing works have
not been realized on system-on-chip (SoC). In this context, this
work focuses on improving the performance of the channel
estimation in wireless PHY, where we design and imple-
ment deep neural network (DNN) augmented LS estimation
(LSDNN) on Xilinx ZCU111 from Zynq multi-processor
SoC (ZMPSoC) family comprising of qual-core ARM Cortex
A53 processor and ultra-scale ﬁeld-programmable gate array
(FPGA). Compared to existing works, we augment the LS with
a Fully Connected Feedforward DNN instead of replacing it
with computationally intensive DL networks. We demonstrate
the performance gain over the conventional LS and LMMSE
approaches. We provide an extensive study of the eﬀects of
training SNR on the performance of the LSDNN channel
estimation.

Through software-hardware co-design, word-length opti-
mization, and reconﬁgurable architectures, we demonstrate the
superiority of the proposed LSDNN architecture over LS and
LMMSE architectures for a wide range of SNRs, number of
pilots, preamble types, and wireless channels. Experimental
results show that the proposed LSDNN architecture oﬀers
lower complexity and a huge improvement in latency over
the LMMSE. Next, we evaluate the performance, power, and
area (PPA) of the LS and LSDNN on application-speciﬁc
integrated circuit (ASIC) implementations in 45 nm technol-
ogy. We demonstrate that the word-length optimization can
substantially improve PPA for the proposed architecture in
ASIC implementations. The AXI-compatible hardware IPs and
PYNQ-based graphical user interface (GUI) demonstrating
the real-time performance comparison on the ZMPSoC are
physical deliverables of this work. Please refer to [29] for

 
 
 
 
 
 
source codes, datasets, and hardware ﬁles used in this work.
The rest of the paper is organized as follows. Section II re-
views the work related to applications of AI-MDL approaches
for wireless PHY. In Section III, we present the system model
and introduce LS, and LMMSE-based channel estimation
approaches. The proposed LSDNN algorithm and simulation
results comparing the performance of LS, LMMSE, and LS-
DNN are given in Section IV followed by their architectures in
Section V. The functional performance and complexity results
of ﬁxed-point architectures are analyzed in Section VI. The
ASIC implementation and results are presented in Section VII.
Section ?? concludes the paper along with directions for future
works.

Notations: Throughout the paper, vectors are deﬁned with
lowercase bold symbols x whose k-th element is x[k]. Matrices
are written as uppercase bold symbols X.

II. Literature Review: DL in Wireless PHY
Various studies and experiments have demonstrated that
conventional frameworks such as Shannon theory [30], de-
tection theory [31], and queuing theory [32] based wireless
PHY suﬀer from performance degradation due to randomness
and diversity of wireless environments. Instead of exploring
approximate frameworks, recent advances in AI-MDL ap-
proaches and their ability to address hard-to-model problems
oﬀer a good alternative for making the wireless PHY robust
[33] [34]. Numerous works have shown that traditional ML
techniques have been successful in combating complex wire-
less environments and hardware non-linearities [21]. How-
ever, they need manual feature selection. The DL approaches
oﬀer the capability to extract features from the data itself,
thereby eliminating the need for manual feature extraction.
This has lead to numerous AI-MDL based approaches for
various problems such as modulation classiﬁcation [35], [36],
direction-of-arrival estimation [37] [38], spectrum sensing
[39], multiple-input multiple-output (MIMO) high-resolution
channel feedback [40] [41], mobility and beam management
[42], link adaptation [18], localization [19]. With the evolution
of heterogeneous large-scale networks, existing frameworks
suﬀer from scaling issues due to the high cost of exhaustive
searching and iterative heuristic algorithms [43] [44]. Such
scaling issues can be handled more eﬃciently using AI-
MDL approaches [45] [46]. To bring DL-aided intelligent
and reconﬁgurable wireless PHY into reality, issues such as
potential use cases, achievable gain and complexity trade-oﬀ,
evaluation methodologies, dataset availability, and standard
compatibility impact need to be addressed [10], [20]–[23].

The DL-based PHY can be categorized in two approaches:
1) Complete transmitter and receiver are replaced with respec-
tive DL architectures [27], [47], 2) Independent DL blocks for
each sub-block or combination of sub-blocks in PHY [24]–
[26], [28], [36], [48]. The drawback of the ﬁrst approach is that
it may not provide intermediate outputs such as channel status
indicator, precoding, and channel rank feedback, making them
incompatible with existing standards. Furthermore, existing
works can not support high throughput requirements. In this
paper, we focus on the second approach addressing the channel
estimation task in wireless PHY.

ChannelNet [25] treats the LS estimated values at pilot sub-
carriers in an OFDM frame as a low-resolution 2D image
and develops a CNN-based image enhancement technique to
increase the resolution to obtain accurate channel estimates at

2

data sub-carriers. Its performance is sensitive to the SNR con-
ditions, which demand SNR-speciﬁc models resulting in high
reconﬁgurable time and large on-chip memory. This model is
further enhanced [26] by replacing the two CNN models with
a single deep residual neural network (ReEsNet) to reduce
its complexity and improve performance. Interpolation-ResNet
in [48] replaces the transposed convolution layer of ReEsNet
with a bilinear interpolation block to reduce the complexity
further and make the network ﬂexible for any pilot pattern. The
computational complexity and memory requirements of these
models are high, even higher than the complexity of PHY. In
this work, we focus on augmenting the conventional channel
estimation with DL instead of DL-based channel estimation
[49], [50].

The validation on synthetic datasets is another concern
in DL-based approaches. This has been addressed in some
of the recent works, such as [28] where DL-based channel
estimation is performed using real radio signals, demonstrating
the feasibility in practical deployment. This works provides a
detailed process and addresses the concerns regarding dataset
creation from the RF environment and training and testing
the models using real radio signals. The authors in [47] used
the concept of transfer learning in [51] to devise a two-
phase training strategy to train a DL-based PHY using real-
world signals and demonstrated over-the-air communication.
It oﬀered similar performance as conventional PHY validating
the feasibility of DL-based approaches in a real-radio environ-
ment. However, limited eﬀorts have been made on hardware
realization of the DL-based wireless PHY [52]–[54]. Hence,
there is limited knowledge of their performance on ﬁxed-point
hardware, latency, and computation complexity. The impact of
the hardware-speciﬁc constraints such as high cost and area of
on-chip memory and a limited number of memory ports on the
training approaches have not been highlighted in the literature
yet. The work presented in this paper addresses these issues
and oﬀers innovative solutions at algorithm and architecture
levels for channel estimation in wireless PHY.

III. System Model
This paper considers orthogonal frequency-division multi-
plexing (OFDM) based transceivers with frame structure based
on IEEE 802.11p standard shown in Fig. 1. The transmitted
frame consists of a preamble header including ten short train-
ing sequences and two long training symbols (LTS), followed
by the signal and data ﬁelds. The LTS consists of predeﬁned
ﬁxed symbols known to both transmitter and receiver and used
for channel estimation at the beginning of the frame. There
are total K = 64 sub-carriers with the sub-carrier spacing of
156.25 KHz, and the corresponding transmission bandwidth
is 10 MHz. Each LTS consists of 64 sub-carriers in a single
OFDM symbol, of which 12 are NULL sub-carriers, and 52
are sub-carriers carrying BPSK modulated preamble sequence.
Each data symbol carries 48 data and 4 pilot sub-carriers. In
this work, we assume that the channel is static over the course
of frame transmission, and hence, only the preamble-based
channel estimated is discussed.

The block diagram of the IEEE 802.11p PHY transceiver
is shown in Fig. 2. The data to be transmitted is modulated
using an appropriate modulation scheme such as QPSK/QAM,
followed by sub-carrier mapping. The set of 64 sub-carriers
is passed through OFDM waveform modulation comprising
IFFT and cyclic preﬁx addition. The length of cyclic preﬁx

3

In this context,

the LS channel estimation [6] can be

expressed as

ˆ˜HLS[k, p] =

(cid:80)Kp

q=1 Y[k, q]
Kp D[k, p]

(3)

The LS estimation is easy to implement in hardware and
does not require prior channel and noise information. On the
other hand, the LMMSE channel estimation [6] needs prior
knowledge of the second order channel and noise statistics.
Mathematically, the estimated channel gain at the k-th sub-
carrier can be expressed as
(cid:32)

(cid:33)−1

(cid:33)

(cid:32)

ˆ˜HLMMSE[k, p] = Rh p

Rh p +

I

ˆ˜HLS[k, p],

(4)

KN0
E p

Fig. 1: IEEE 802.11p frame structure and sub-carrier mapping.

where Rh p is the channel auto-correlation matrix, and E p
denotes the power per preamble symbol. It can be observed
that the LMMSE improves the performance of the LS, and the
improvement depends on the prior knowledge of the channel
and noise statistics.

IV. Deep Neural Networks Based Channel Estimation
In this section, we present LSDNN design details and
simulation results using ﬂoating-point arithmetic comparing
various channel estimation approaches.

A. LSDNN based Channel Estimation

Fig. 2: Building blocks of the IEEE 802.11p based OFDM transmit-
ter and receiver PHY.

is Kcp = 16. The scheduler inserts the preamble OFDM
symbols at the beginning of each frame, comprising a certain
number of data symbols. Next, the complete OFDM frame
is transmitted through the wireless channel. At the receiver,
OFDM symbols containing the LTS preamble are detected and
extracted, followed by OFDM demodulation. Then, channel
estimation and equalization operations are performed.

The frequency-domain input-output relation between the
transmitted and the received OFDM frame at the receiver can
be expressed as follows

Y[k, i] = H[k, i]X[k, i] + V[k, i].

(1)

Here, X[k, i], H[k, i], Y[k, i], and V[k, i] denote the trans-
mitted OFDM frame, the frequency domain channel gain, the
received OFDM frame, and the addictive white Gaussian noise
(AWGN) with zero mean and variance, N0. k and i denote
the sub-carrier and OFDM symbol indices within the received
frame, respectively.

Recall that the channel estimation is performed once per
frame using the received preamble symbols, therefore (1) can
be rewritten as

Y[k, p] = H[k, p] D[k, p] + V[k, p],
(2)
where D[k, p] = dp [k] denote the p-th predeﬁned transmitted
preamble symbol, where 1 ≤ p ≤ Kp and Kp is the total
number of the transmitted preamble symbols within the frame.

Deep Neural Networks (DNN) is a subset of DL that tries to
mimic how information is passed among biological neurons.
A fully-connected neural network is shown in Fig. 3. It has a
layered structure, with each layer feeding to the next layer
in a forward direction. The ﬁrst
layer is called an input
layer representing the input to the DNN, and the last layer
is called an output layer from which the outputs are taken.
The layers between the input layer and output layer are called
hidden layers. The output and hidden layers contain parallel
processing elements called neurons, which take the inputs from
the previous layer and send output to the next layer. A neuron
performs the summation of weighted inputs followed by a non-
linear activation function, as shown in Fig.3. These functions
include Sigmoid, tanh, ReLU, leaky ReLU [55]. For DNN
with L layers, including input and output layers, the output of
j-th neuron in layer l, 1 < l < L is given as


yl
j

= f (l)

(cid:16)


Nl(cid:88)


i=0

i, j yl−1
wl

i, j

(cid:17) + bl



j

,

(5)

Fig. 3: Fully connected Neural Network.

Sub-carriers (K)SymbolsPilot Sub-carrierst1t2t3t4t5t6t7t8t9t10Data FieldGIGILTS1LTS2GISIGNALLTSSTSPreambleSignal Field...GIDATAGIDATAData Sub-carriersNull Sub-carriersK = +31K = +26K = +21K = +7 K = 0  K = -7 K = -21K = -26K = -32RECEIVERTRANSMITTERTransmitDataSub-carrierMappingPilotInsertionIFFTCP additionPreambleInsertionWirelesschannelCP RemovalFFTEqualizationChannelEstimationde-mappingBERCalculationLTSDataLS/MMSEEstimationComplex toRealDNNReal toComplexSub-carrierDe-MappingBERCalculationSub-carrierDe-MappingReferenceLTSCP additionIFFTInput  LayerHidden  LayersOutput  Layerw1w2wnx1x2xnf(.)yINPUTOUTPUTNeuronBias4

where Nl is the total number of neurons in l-th layer, bl
j is
the bias of j-th neuron of l-th layer, wi, j is the weight from
i-th neuron of (l − 1)-th layer to j-th neuron of l-th layer, and
f (l) (.) is the activation function of l-th layer.

ˆ˜HLS

The proposed DNN augmented LS estimation approach
processes the output of the LS estimator using the DNN,
thereby reducing the eﬀect of noise on LS estimates. The
DNN is trained to reduce the loss function JΩ,b( ˜H, yL
) over
one long training symbol, where H is the original channel
impulse response and ˆ˜HLS is the channel estimated using
the LS approach. Training is an iterative process where the
model parameters are updated using optimization functions to
minimize the loss over time. Various optimization functions
such as stochastic gradient descent, root mean square prop,
and adaptive moment estimation (ADAM) can be used [56].
Hyper-parameters for the DNN architecture is chosen man-
ually. Multiple iterations were performed with diﬀerent hyper-
parameters until the one with the best accuracy and low latency
was found. We restricted our search to only a few hidden
layers to reduce the complexity of the overall design. For
illustrations, we consider two DNN architectures: 1) LSDNN1:
DNN with a single hidden layer of size Kon, and 2) LSDNN2:
DNN with two hidden layers of size 2 × Kon. Each model is
trained for 500 epochs with Mean Square Error (MSE) as a
loss function and ADAM as an optimizer. Table I summarises
the speciﬁcations of neural network. After training, the next
step is inference, in which a trained DNN model is tested on
new data to evaluate its performance. The hardware realiza-
tions of DNN focus on accelerating the inference phase.

B. Simulation Results on Floating Point Arithmetic

For the system model discussed in Section III, we have used
six channel models considering vehicle-to-vehicle (V2V) and
roadside-to-vehicle (RTV) environments as discussed in [57].
We have considered two performance metrics: 1) Normalized
mean square error (NMSE) and 2) Bit-error-rate (BER) to

TABLE I: DNN Parameters

Parameters
LSDNN1 (hidden layers ; neurons per layer)
LSDNN2 (hidden layers ; neurons per layer)
Activation Function
Epochs
Optimizer
Loss Function

Values
(1 ; Kon)
(2; 2 × Kon )
ReLU
500
ADAM
MSE

analyze the performance of channel estimation and wireless
PHY, respectively.

In Fig. 4(a), we compare the average NMSE of LS,
LMMSE, and LSDNN architectures (LSDNN1 and LSDNN2)
for a wide range of SNR, and the corresponding BER results
on end-to-end wireless transceiver are shown in Fig. 4(b). We
assume Rayleigh fading channel with AWGN. We consider an
LSDNN architecture trained on a single SNR referred to as
training SNR. Here, the training SNR is selected as 10 dB, and
the testing SNR range is from -50 dB to 50 dB. Please refer
to Section VI-C for more details on the selection of training
SNR.

As shown in Fig. 4, the performance of the conventional
LS and LMMSE architectures matches with their analytical
results, and the error for the LMMSE estimator is less than
that of the LS estimator. It can be observed that the LMMSE
improves the performance of the LS, and the improvement de-
pends on the prior knowledge of the channel and noise statis-
tics. When prior channel statistics are not known accurately,
the LMMSE performance degrades, and the NMSE becomes
worse than LS at high SNR, as shown in Fig. 4. The BER
of the LSDNN1 and LSDNN2 are close to that of the ideal
channel estimation approach and signiﬁcantly outperforms the
LS and LMMSE at all SNRs. The NMSE performance of the
LSDNN improves with the increase in SNR until the training
SNR of 10 dB. Even though LSDNN1 has less complexity,
it oﬀers superior performance than LSDNN2. The NMSE is
also a function of the number of training samples; hence, the
dataset should be suﬃciently large. Similar results are also
observed for diﬀerent wireless channels and preamble types.
Corresponding plots are not included to avoid the repetition
of results.

The improved performance of the LSDNN can be attributed
to the capability of the DNN to extract channel features
and learn a meaningful representation of the given dataset.
Speciﬁcally, it accurately learns to distinguish between the
AWGN noise and the channel gain. The LS estimation does
not consider noise, resulting in noisy estimates at low SNRs.
As SNR increases, the noise content in the LS estimation
reduces, resulting in improved performance. This is evident
from Fig. 5 where we have analyzed the plots of estimated
channel gain magnitudes at diﬀerent OFDM sub-carriers for
three diﬀerent SNRs. It can be observed that LSDNN and
LMMSE performance is close to ideal channel estimation even
at low SNR, with the former better than the latter at all sub-
carriers. The erroneous knowledge of channel parameters leads
to degradation in LMMSE performance, as shown in Fig. 5.
Such prior knowledge is not needed in the LSDNN.

Fig. 4: (a) NMSE and (b) BER comparisons of various channel estimation approaches using ﬂoating point arithmetic.

-50-40-30-20-1001020304050SNR(dB)10-610-3100103106NMSE-50-40-30-20-1001020304050SNR(dB)10-610-410-2100BERPerfect ChannelLSMMSEErroneous MMSELSDNN1LSDNN22425261.41.61.822.22.4(a)(b)5

Fig. 5: Magnitude plot for channel estimation corresponding to subcarriers in an OFDM symbol for (a) 0dB SNR, (b) 20dB
SNR, (c) 45dB SNR.

V. Channel Estimation on SoC
In this section, we discuss the algorithm to architecture
mapping of three channel estimation approaches, LS, LSDNN,
and LMMSE, on FPGA (hardware) part of the ZMPSoC.
The software implementation of these algorithms on an ARM
processor is straightforward, and corresponding results are
given in Section VI. We begin with the LSDNN architecture,
which includes LS estimation as well.

A. LSDNN Architecture

At the receiver, the LTS samples are extracted from the data
obtained after the OFDM demodulation, as shown in Fig. 2.
LSDNN uses these symbols for channel estimation, and it
involves ﬁve tasks: 1) Data extraction, 2) LS-based channel
estimation, 2) Pre-processing, 3) DNN, and 4) Post-processing,
as shown in Fig. 6. In this section, we present the architectures
to accomplish these tasks.

The serial data with complex samples received after OFDM
demodulation are extracted with independent real and imag-
inary parts. For the chosen system model with an OFDM
symbol comprising of 52 sub-carriers, the input to LS esti-
mation is a vector with 104 real samples. Another input to
LS estimation is a reference LTS sequence of the same size.
The LS estimation involves the complex division operation of
the received (yp) and reference (xp) versions of the LTS. The
complex division can be mathematically expressed as shown
below, and the corresponding architecture is shown in Fig. 6

ˆ˜HLS =

yp
xp

= x r × y r + x i × y i
x r2 + x i2

i
(6)
where x r, x i, y r, and y i denote the real and imaginary
parts of received and reference versions of the LTS, respec-
tively.

+ x r × y i + x i × y r
x r2 + x i2

The LS estimation of each sub-carrier requires six real
multiplications, one real division, three additions, and one
subtraction operation. We have explored various architectures
of simultaneous calculation of the LS estimation of multiple
sub-carriers by appropriate memory partitioning to have par-
allel access to data and pipelining to improve the utilization
eﬃciency of hardware resources. Please refer to Section VI
for more details. Further optimizations can be carried out
depending on the modulation type of the LTS sequence. For
instance, the LS estimation of QPSK modulated LTS sequence
needs only one complex multiplication operation. In the case
the LTS and pilot symbols are BPSK
of IEEE 802.11p,

modulated. The LS estimation is reduced to choosing either
the received complex value or its two’s compliment based on
whether the LTS is +1 or -1, respectively.

The output of the LS channel estimation is further pro-
cessed using DNN architecture. Before DNN, pre-processing
is needed to normalize the DNN input to have zero mean and
unit standard deviation. The estimation of mean and standard
deviation during inference is not feasible due to limited on-
chip memory and latency constraints. Hence, we use the pre-
calculated values of the mean and standard deviation using the
training dataset. Similarly, the DNN output is de-normalized
so that transmitted data can be demodulated for BER-based
performance analysis of the end-to-end transceiver. At the
architecture level, pre and post-processing incur additional
costs in terms of multipliers and adders, as shown in Fig. 6.
The DNN architecture mainly consists of one or more
hidden layers followed by the output layer, and it is based
on a fully connected neural network framework. This means
each layer consists of multiple processing elements (PE), also
referred to as neurons, where each PE in a layer communicates
the output to all the PEs in the subsequent layer. There is no
communication and dependency between the various PEs of
a given layer. Also, the data communications between layers
happen only in the forward direction, thereby allowing the
parallel realization of all PEs of a layer.

As shown in Eq. 5 and Fig. 6, each PE consists of a mul-
tiplier and adder unit to perform element-wise multiplications
between outputs of all PEs of the previous layer and layer-
speciﬁc weights stored in internal memory. In the end, the
output is added with the layer-speciﬁc bias term. This means
each PE needs to store Nl−1 weights and one bias value in
its memory, where Nl−1 is the number of PEs in the previous
layer. The output of each layer is stored in memory which
is completely partitioned to allow parallel access from the
PEs in the subsequent layer. The PE functionality is realized
sequentially using the counter and multiplexers. Though the
parallel realization of all multiplication and addition operations
inside the PE is possible due to memory partitioning at PE
this signiﬁcantly increases resource utilization and
output,
power consumption. ReLU block is added at
the end of
each hidden layer to introduce non-linearity. It is a hardware-
friendly activation function that replaces the negative PE
the output of the
output with zero. As shown in Fig. 7,
ﬁrst PE of the previous layer is processed by all PEs in
the layer simultaneously, and the output is stored in their
respective buﬀers. Next, the output of the second PE of the
previous layer is processed, followed by accumulation with
the previous intermediate output in the buﬀer. This process is
repeated until the outputs of all PEs in the previous layer have

1020304050Preamble subcarriers0.511.522.5MagnitudeLSLMMSEErroneous LMMSELSDNNReal Channel Impulse Response1020304050Preamble subcarriers0.40.60.81Magnitude1020304050Preamble subcarriers0.80.911.11.2Magnitude(a) SNR = 0 dB(b) SNR = 20 dB(c) SNR = 45 dB6

Fig. 6: Various building blocks of the LSDNN architecture.

the inverse of the Rh matrix is performed. This is followed by
various matrix multiplication and addition operations. We have
modiﬁed Xilinx’s existing matrix multiplication and matrix
inversion reference examples to support the complex number
arithmetic since the baseband wireless signal is represented
using complex samples. The well-known lower-upper (LU)
decomposition method is selected for matrix inversion. We
parallelize individual operations like element-wise division
and Matrix Multiplication on the FPGA. Every element in the
matrix is parallelly processed to compute division, and every
row column dot product in matrix multiplication is performed
in parallel to speed up the computation. In the end, multiple
instances of these IPs are integrated to get the desired LMMSE
functionality, as shown in Fig. 8.

C. Hardware Software Co-design and Demonstration

In Fig. 9, various building blocks of the channel estima-
tion tasks such as LS, LMMSE, LSDNN, scheduler, DMA,
interrupt, and GUI controller is shown. For illustration, we
have shown all channel estimation approaches on the FPGA.
To enable this, we have developed AXI-stream compatible
hardware IPs and interconnected them with PS via direct-
memory access (DMA) in the scatter-gather mode for eﬃcient
data transfers. Later in Section VI, we considered various
architectures via hardware-software co-design by moving the
blocks between ARM Processor and FPGA. We have deployed
PYNQ based operating system on the ARM processor, which
takes care of various scheduling and controlling operations. It
also enables graphical user interface (GUI) development for
real-time demonstration. As shown in Fig. 9, GUI allows the
user to choose various parameters such as channel estimation
schemes, SNRs, and word-length.

Fig. 7: Scheduling of operations inside the single DNN layer.

been processed. After bias addition and ReLU operation, the
subsequent layer is activated.

In a completely parallel DNN architecture, all the PEs in
a layer are realized using the dedicated hardware resource,
and hence the output of all PEs is computed simultaneously.
On the other hand, in a completely serial architecture, all PEs
in a layer share the hardware resources; hence, the outputs of
PEs are computed serially. Depending on resource and latency
constraints, various architectures such as fully serial, fully
parallel, and serial-parallel architectures for various layers
have been explored.

B. LMMSE Architecture

The LMMSE channel estimation requires prior knowledge
of the channel correlation matrix (Rh) and SNR in addition
to LTS symbols. The LMMSE architecture is shown in Fig. 8
and is based on Eq. 4. In the beginning, Rh is separated into
real and imaginary matrices, and the term (1/SNR) is added
with each diagonal element of the real matrix of Rh. Then,

Serial to ParallelConversionLS Estimation persub-carrierLS Estimation persub-carrierReal and ImaginaryParts ExtractionSerial to ParallelConversionReal and ImaginaryParts ExtractionReal and ImaginaryParts CombinationPre-Processing persub-carrierPre-Processing persub-carrierPost-Processingper sub-carrierPost-Processingper sub-carrierDNNm_r  v_r  m_i  v_i  Xv_r  m_r  +X+v_i  m_i  [1x52]64b[1x104]64b[1x104]32b[1x104]32b[1x104]32b[1x52]64b64bSerial datafrom FFTReferenceLTS fromMemory[1x104]32bx_rx_iy_iy_ro_ro_io_ro_iXXXX+x_r  y_r  x_i  y_i  x_r  x_r  x_i  x_i  x_r  y_i  x_i  y_r  XX+Hidden Layer 1 PE #3Hidden Layer 1 PE #2KReLU #3ReLU #2KHidden Layer M PE #3Hidden Layer M PE #2KReLU #3ReLU #2KOutput Layer PE #3Output Layer PE #2KX+WeightMemoryMod 2KCounterI2KRegister+Bias MemoryPE_enI1M U XM U X=2KPE_en0>00M U XRegisterM U XPE1PE2PENW11W12W13W1KW21W22W23W2KWN1WN2WN3WNKO1 =W11 PE1O2 =W12 PE1O3 =W13 PE1OK =W1K PE1=PE1PE2PENW11W12W13W1KW21W22W23W2KWN1WN2WN3WNKO1 =O1+WN1 PENO2 =O2+WN2 PENO3 =O3+WN3 PENOK =OK+WNK PENO1 =O1+W21 PE2O2 =O2+W22 PE2O3 =O3+W23 PE2OK =OK+W2K PE2=PE1PE2PENW11W12W13W1KW21W22W23W2KWN1WN2WN3WNK=Active Layerwith K PEsActive Layerwith K PEsActive Layerwith K PEsPrevious Layerwith N PEsPrevious Layerwith N PEsPrevious Layerwith N PEsPE1PE2PEN===7

Fig. 8: Architecture of the LMMSE based channel estimation.

Fig. 9: Proposed architecture on ZMPSoC and screenshot of Pynq based GUI depicting the real-time demonstration of channel estimation.

VI. Performance and Complexity Analysis

A. Word Length Impact on Channel Estimation Performance

This section presents the performance analysis of diﬀerent
channel estimation architectures implemented on the ZSoC
for a wide range of SNRs, word-length, and wireless pa-
rameters such as channels, preamble type, etc. The resource
utilization, execution time, and power consumption of various
architectures obtained via word length optimization, hardware-
software co-design, and reconﬁgurability are compared. The
need for reconﬁgurable architectures and challenges in training
DNN for wireless applications are highlighted at the end.

In Section IV-B, we have analyzed the performance of
various channel estimation approaches on ﬂoating-point arith-
metic in Matlab. To optimize the execution time and resource
utilization without compromising the functional accuracy, we
have explored the architectures with various WLs and realized
them on ZSoC. In Fig. 10(a) and (b), we compare the NMSE
and BER performance of the various LSDNN architectures,
respectively, for six diﬀerent WLs along with the single-
precision ﬂoating point (SPFP) architecture of the LS and
double precision ﬂoating point (DPFP) architecture of the
LMMSE. The architectures with the half-precision ﬂoating

Fig. 10: (a) NMSE and (b) BER comparisons of various WL architectures of the LSDNN on the ZSoC.

[52x52]64b[52x52]64bRhSNR Addition toDiagonal ElementsSNR[52x52]64bMatrix InversionMatrix InversionMatrix InversionMatrix InversionMatrixMultiplicationReal and ImaginaryParts ExtractionComplexMatrixMultiplicationElement-WiseDivisionComplexMatrix-VectorMultiplication[52x1]64b[52x52]64b[52x52]64b[52x52]64bRhSerial to ParallelConversionReference LTSfrom MemorySerial to ParallelConversionSerial Datafrom FFTLUPDecomposition[52x52]64b[52x52]64bComplex Matrix InversionPYNQ OSARM (PS)A X IXAI DDR MemoryFPGA (PL)DDR Controller UART SD Card Ethernet UARTSD  CardMonitor SchedulerWireless PHYDMA ControllerPerformance AnalysisInterrupt ControllerGUI ControllerAXI3 TextText DMAAXI LiteAXI MM   DMAAXI LiteAXI MM   DMAAXI LiteAXI MM  LSDNN  (Fig. 6)CNN Layer 3LMMSE (Fig. 8)LS  (Fig. 6)-50-40-30-20-1001020304050SNR(dB)10-610-3100103106NMSELSLMMSELSDNN (SPFP)LSDNN (HPFP)LSDNN (FP 32)LSDNN (FP 24)LSDNN (FP 20)LSDNN (FP 18)-50-40-30-20-1001020304050SNR(dB)10-610-410-2100BER(a)(b)point (HPFL) and ﬁxed-point architectures with WL below
24 lead to signiﬁcant degradation in the NMSE. However,
degradation in NMSE may not correspond to degradation in
the BER due to data modulation in the wireless transceiver.
This is evident from Fig. 10(b), where the architecture with
HPFL oﬀers the BER performance same as that of the SPFL
architecture. On the other hand, the BER of the ﬁxed-point
architecture with WL below 24 leads to signiﬁcant degradation
in the performance.

The WL selection is architecture-dependent; hence, the WL
for the LS may not be the same as that of LSDNN. Based
the LS architecture
on the detailed experimental analysis,
with ﬁxed-point WL of 18 bits or higher oﬀers the same
performance as its SPFL architecture, as shown in Fig 11(a).
In the case of LMMSE, ﬁxed-point architecture is not feasible.
Even SPFL architecture fails to oﬀer the desired performance,
especially at high SNRs; hence, DPFL is needed. Further
analysis revealed that only the matrix inverse sub-block needs
DPFL WL while the rest of the sub-blocks can be realized in
SPFL WL as shown in Fig. 11(b). Such analysis highlights

Fig. 11: NMSE Comparison of various WL architectures of
(a) LS and (b) LMMSE on the ZSoC.

8

the importance of experiments on the SoC for diﬀerent WL
since such results can not be obtained using simulations.

B. Resource, Power and Execution Time Comparison

In this section, we compare the resource utilization, exe-
cution time, and power consumption of various architectures
on the ZSoC platform. In Table II, we consider three diﬀerent
WL architectures of the LS and LMMSE and six diﬀerent WL
architectures of the LSDNN. It is assumed that the complete
architecture is realized in the FPGA (PL) part of the SoC.
The execution time of the LS architecture is the lowest,
while the execution time of the LMMSE architecture is the
highest. The execution time of the LSDNN is signiﬁcantly
lower than that of LMMSE and is around 1.5-2 times that of
the LS architecture. Similarly, resource utilization and power
consumption of the LSDNN are higher than that of the LS.
This is the penalty paid to gain signiﬁcant improvement in
channel estimation performance. However, the LSDNN oﬀers
a signiﬁcantly lower execution time than the LMMSE, and
resource utilization is also lower, especially in terms of BRAM
and DSP units which are limited in numbers on FPGA. The use
of ﬁxed-point architectures leads to signiﬁcant improvement in
all three parameters. In case of LSDNN, we also considered
two additional architectures: 1) Serial-parallel architecture
where 2 PEs share the same hardware resources, and 2) Fully
serial architecture where all PEs share the same hardware
resources. As we move from a completely parallel architecture
to these two architectures, we can gain signiﬁcant savings
in the number of resources, especially DSPs. However, the
execution time increases, which is still signiﬁcantly lower than
the LMMSE. Further improvement is possible if the input data
type of the architecture is changed to ﬁxed-point so that ﬁxed-
to-ﬂoating point conversion overhead at the input and output
can be removed.

Next, we consider various LSDNN architectures obtained
via hardware-software co-design. For this purpose, LSDNN
is divided into three blocks – LS (B1), normalization and

TABLE II: Resource utilization and latency comparison of LS, LMMSE, and DNN-Augmented LS channel estimation for diﬀerent word
length implementations.

Sr. No
1
2
3
4
5
6
7
8
9
10

11

12

Architectures

LS

MMSE

LSDNN

(104 Parallel PE)

LSDNN
(52 Parellel PE)
LSDNN

(1 PE)

Word Length
SPFL
HPFL
FP (18,9)
DPFL
SPFL
DP(INV)-SP
SPFL
HPFL
FP (32,8)
FP (24,8)

FP (24,8)

FP (24,8)

Execution time (ms)
0.0133
0.0146
0.0155
248.41
214.7
219.75
0.0266
0.0298
0.0186
0.0179

0.125

0.236

BRAMs
14
5
7
191.5
99.5
139.5
88
32
88
11

20

22.5

DSPs
96
72
24
522
194
338
322
260
520
260

32

17

LUTs
14721
8371
14665
44444
24249
36225
62895
30436
157130
85155

FFs
15094
10369
16649
43485
24778
34898
58064
32286
151544
82721

PL Power (W)
0.438
0.296
0.373
1.158
0.621
0.918
1.078
0.569
2.46
1.318

23534

23455

0.465

15450

16025

0.373

SoC Power (W)
1.969
1.827
1.904
2.689
2.152
2.449
2.647
2.138
4.029
2.849

1.996

1.904

TABLE III: Execution times for diﬀerent Hw/Sw codesign approaches for DNN Augmented LS Estimation

Sr. No
1
2
3
4

Blocks in PS
B1,B2,B3
B1,B2
B1
NA

Blocks in PL
NA
B3
B2,B3
B1,B2,B3

Execution time (us)
558
49
30
26

Acceleration factor
1
11
18
21

BRAM DSP

LUT

FF

PL Power(W)

SoC Power (W)

NA

80
82
88

130
130
322

26622
26456
62895

25807
28136
58064

0.453
0.453
1.078

2.021
2.022+
2.647

-50-2502550SNR (dB)10-410-2100102104NMSELS (DPFP)LS (SPFP)LS (FP 18)LS (FP 16)LS (FP 14)MMSE (DPFP)MMSE SPFLMMSE DP+SPFLTABLE IV: Resource utilization and latency comparison of LSDNN for wireless PHY with diﬀerent FFT size

Sr. No
1
2
3

FFT Size
64
128
256

Execution time (us)
26
48
85

Acceleration factor
21
28
31

BRAM DSP
322
88
512
195
832
339

LUT
62895
83193
119043

FF
58064
80679
118606

PL Power(W)
1.078
1.474
2.206

SoC Power (W)
2.647
3.005
3.737

9

denormalization (B2), and DNN (B3). In Table III, we consider
four architectures: 1) B1, B2, B3 in PS, 2) B1, B2 in PS,
and B3 in PL, 3) B1 in PS, and B2, B3 in PL, and 4) B1,
B2, B3 in PL (Same as Table II). Moving DNN to PL
gives 11× improvement in execution time with respect to PS
implementation as FPGA can exploit inherent parallelism in
DNNs, as explained in section V-A, to speed up the processing.
Normalization and denormalization can also be performed in
parallel as there is no data dependency for these operations
and thus can be accelerated by moving to PL. Execution time
can further be reduced by moving LS to PL and processing
multiple subcarriers in parallel,
thus achieving an overall
acceleration factor of 21× when all the data processing blocks
are implemented in PL compared to complete PS implemen-
tation. Reducing execution time by moving blocks to PL and
introducing parallelism increases the FPGA fabric’s resource
utilization and power consumption. Thus, when choosing a
particular architecture, there is a trade-oﬀ between latency and
resource utilization/power consumption.

With the evolution of wireless networks, the transmission
bandwidth and hence, the FFT size of the OFDM PHY is
also increasing. The increase in FFT size leads to an increase
in the complexity of the channel estimation architecture,
which in turn demands eﬃcient acceleration via FPGA on
the SoC. In Table IV, we compare the eﬀect on execution
time and corresponding acceleration factor for diﬀerent FFT
sizes. It can be observed that the gain in acceleration factor
increases with the increase in the FFT size mainly due to
more opportunities for parallel arithmetic operations. Thus,
LSDNN architecture has the potential to oﬀer a signiﬁcant
improvement in performance and execution time for next-
generation wireless PHY.

C. Eﬀect of Training SNR

For the results presented till now, the LSDNN model is
obtained using the training SNR of 10 dB. Training a DNN
on a single SNR allows it
to converge faster and model
the channel better as the DNN does not have to encounter
multiple noise distributions during the training phase, thus
reducing training complexity. However, faster training should

not compromise the functional performance of the LSDNN.
For in-depth performance analysis, we analyze the eﬀect of
training SNR on the performance of the LSDNN model.

We initially consider various architectures of the LSDNN
model trained using the dataset comprising all SNR samples
ranging from -50 dB to 50 dB. As shown in Fig.12, the
NMSE and BER performance is poor due to the reasons
mentioned before. However, the selection of training SNR is
not trivial since optimal training SNR may vary depending
on the testing SNR, i.e., the deployment environment. For
instance, as shown in Fig. 13 (a) and (b), single training SNR
aﬀects the performance of LSDNN considerably, especially
at high testing SNRs. To address this issue, we consider the
selection of training SNR based on the testing SNR range,
and corresponding results are shown in Fig. 14. It shows the
NMSE for diﬀerent testing SNRs plotted against the selected
training SNR. For each testing SNR, the NMSE ﬁrst decreases
as the training dataset SNR is increased till it reaches its
minimum value and then starts increasing. The best training
SNR is where the NMSE is minimum, e.g., for the testing
SNR range of 0 dB - 20dB, the lowest NMSE corresponds
to a training SNR of 10dB. Similarly, for testing SNR of -10
dB to 10 dB, the optimal training SNR is 0 dB. Note that

Fig. 12: BER performance of the LSDNN architecture trained
with dataset containing samples from complete SNR range of
-50dB to 50dB.

Fig. 13: (a) NMSE and (b) BER comparisons of various LSDNN architectures trained on diﬀerent training SNRs.

-50-40-30-20-1001020304050SNR(dB)10-610-410-2100BERIdealLSMMSELSDNN-50-40-30-20-1001020304050SNR(dB)10-610-3100103106NMSELSLMMSELSDNN (10 dB)LSDNN (-10 dB)LSDNN  (30 dB)LSDNN (-30 dB)-50-40-30-20-1001020304050SNR(dB)10-610-410-2100BER(a)(b)10

TABLE V: Comparison of various reconﬁgurable LSDNN architec-
tures

External DDR
Memory

Execution time (ms)
LUT
FF
BRAM
DSP
PL power (W)
SoC power (W)

0.257
65926
70300
92
300
0.854
2.423

On-chip BRAM

4 models
0.0264
68424
63802
221.5
324
1.099
2.668

8 models
0.029
70634
63814
253.5
324
1.121
2.69

of the selected LSDNN models are transferred to the PL
via DMA. Such architecture suﬀers from high reconﬁguration
and execution time due to external memory. However, on-
ﬁeld reconﬁgurability is possible since future models can
be added in DRAM using PS without needing hardware
reconﬁguration. In the second approach, all model parameters
are stored in the BRAM of the PL, resulting in lower execution
and reconﬁguration time. Due to limited BRAM, such an
approach limits the number of supported models, and adding
new models requires FPGA conﬁguration. Note that FPGA
reconﬁguration can still be done remotely without requiring
product recall. As part of future works, the second architecture
can be enhanced to optimize BRAM utilization via dynamic
partial reconﬁguration, thereby making the BRAM utilization
independent of the number of models. Such reconﬁgurable
architecture also needs intelligence to detect the change in
environment and reconﬁgure the hardware. The design of
intelligent and reconﬁgurable channel estimation architecture
is an important research direction.

VII. ASIC Implementation
In addition to FPGA, ASIC is another popular hardware
platform for wireless PHY. In this section, we assess the PPA
delivered by various architectures in the ASIC implementation
using 45 nm CMOS technology. First, we synthesize the
Verilog code used for FPGA implementation, using suitable
constraints, to obtain the gate level netlist. We verify the
functional correctness of the design using a combinational
equivalence checker and ensure its temporal safety using static
timing analysis. The validated gate-level netlist is converted
into the ﬁnal Graphic Data System (GDS) using the physical
design steps such as ﬂoor-planning, placement, clock-tree
synthesis, and routing. Finally, we carry out various sign-oﬀ
checks on the ASIC implementation using physical veriﬁcation
and timing analysis tools. We report the PPA of the imple-
mentations with various channel estimation architectures in

Fig. 14: Comparison of eﬀect of training SNR on the NMSE for
various ranges of the testing SNR.

training SNR is irrelevant at low SNRs due to the signiﬁcant
impact of noise on the received signal, and DNN is unable to
learn the channel properties, resulting in high NMSE and BER.
Such dependence on the testing SNR demands reconﬁgurable
architecture that can switch between various LSDNN models
depending on the given testing SNR range. Such architecture
is presented in the next section.

D. Reconﬁgurable Architecture

Though the LSDNN approach oﬀers better performance
than LMMSE without the need of prior channel knowledge,
LSDNN performance depends on the eﬃcacy of the train-
ing dataset with respect to the validation environment. For
instance,
the performance of LSDNN may degrade if the
parameters such as testing SNR, preamble type, pilot type,
etc., of the training and validation environment do not match.
For instance, the performance of the LSDNN trained on model
M1 degrades signiﬁcantly on two diﬀerent models, M2 and
M3, as shown in Fig. 15. This is expected because for a DNN
to perform satisfactorily, the training and testing conditions
should remain the same. To address this challenge, we need
a reconﬁgurable architecture for LSDNN so that an on-the-ﬂy
switch between various LSDNN models can be enabled.

We propose a reconﬁgurable architecture for LSDNN where
the parameters of various LSDNN trained models are pre-
computed and stored in memory. Thus, the same hardware
architecture can be used across multiple testing environments.
In Table V, we consider two architectures based on the type of
memory on the SoC. In the ﬁrst architecture, external DRAM
is used to store the model parameters, and the parameters

Fig. 15: (a) NMSE and (b) BER comparisons of various channel estimation approaches on diﬀerent models. The LSDNN
model is trained on model M1.

-50-40-30-20-1001020304050Training SNR(dB)10-610-3100NMSETesting SNR: -20dB to 0dBTesting SNR: -20dB to 20dBTesting SNR: -10dB to 10dBTesting SNR: 0dB to 20dBTesting SNR: 10dB to 50dB-50-40-30-20-1001020304050SNR(dB)10-610-3100103106NMSELSLMMSELSDNN M1LSDNN M2LSDNN M3-50-40-30-20-1001020304050SNR(dB)10-610-410-2100BER(b)(a)11

and LMMSE. The AXI-compatible hardware IPs and PYNQ-
based graphical user interface (GUI) demonstrating the real-
time performance comparison on the ZMPSoC are a physical
deliverable of this work.

Future works include an extension of the proposed approach
for multi-antenna systems and integration with the RFSoC
platform for validation in a real-radio environment. We would
also like to explore the application of the proposed approach
for upcoming standards such as IEEE 802.11 ad/ay/ax, in
which reference signals are spread over the data frame.

References

[1] J. M. Hamamreh, H. M. Furqan, and H. Arslan, “Classiﬁcations and
applications of physical layer security techniques for conﬁdentiality:
A comprehensive survey,” IEEE Communications Surveys & Tutorials,
vol. 21, no. 2, pp. 1773–1828, 2019.

[2] X. Wang, L. Gao, S. Mao, and S. Pandey, “Csi-based ﬁngerprinting for
indoor localization: A deep learning approach,” IEEE Transactions on
Vehicular Technology, vol. 66, no. 1, pp. 763–776, 2017.

[3] S. Zeadally, M. A. Javed, and E. B. Hamida, “Vehicular communica-
tions for its: Standardization and challenges,” IEEE Communications
Standards Magazine, vol. 4, no. 1, pp. 11–17, 2020.

[4] S. Mumtaz, V. G. Menon, and M. I. Ashraf, “Guest editorial: Ultra-
low-latency and reliable communications for 6g networks,” IEEE
Communications Standards Magazine, vol. 5, no. 2, pp. 10–11, 2021.

[5] Y. Liu, Z. Tan, H. Hu, L. J. Cimini, and G. Y. Li, “Channel estimation
for ofdm,” IEEE Communications Surveys & Tutorials, vol. 16, no. 4,
pp. 1891–1908, 2014.

[6] K. K. Nagalapur, F. Br¨annstr¨om, E. G. Str¨om, F. Undi, and K. Mahler,
“An 802.11p cross-layered pilot scheme for time- and frequency-varying
channels and its hardware implementation,” IEEE Transactions on
Vehicular Technology, vol. 65, no. 6, pp. 3917–3928, 2016.

[7] J.-J. Van De Beek, O. Edfors, M. Sandell, S. K. Wilson, and P. O.
Borjesson, “On channel estimation in ofdm systems,” in 1995 IEEE
45th Vehicular Technology Conference. Countdown to the Wireless
Twenty-First Century, vol. 2, pp. 815–819, IEEE, 1995.

[8] L. Sanguinetti, A. L. Moustakas, and M. Debbah, “Interference manage-
ment in 5g reverse tdd hetnets with wireless backhaul: A large system
analysis,” IEEE Journal on Selected Areas in Communications, vol. 33,
no. 6, pp. 1187–1200, 2015.

[9] Y. Xu, G. Gui, H. Gacanin, and F. Adachi, “A survey on resource
allocation for 5g heterogeneous networks: Current research, future
trends, and challenges,” IEEE Communications Surveys & Tutorials,
vol. 23, no. 2, pp. 668–695, 2021.

[10] F. Restuccia and T. Melodia, “Deep learning at

the physical
layer: System challenges and applications to 5g and beyond,” IEEE
Communications Magazine, vol. 58, no. 10, pp. 58–64, 2020.

[11] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, “Deep
learning-based channel estimation,” IEEE Communications Letters,
vol. 23, no. 4, pp. 652–655, 2019.

[12] Y. Yang, F. Gao, X. Ma, and S. Zhang, “Deep learning-based channel
estimation for doubly selective fading channels,” IEEE Access, vol. 7,
pp. 36579–36589, 2019.

[13] X. Wei, C. Hu, and L. Dai, “Deep learning for beamspace channel esti-
mation in millimeter-wave massive mimo systems,” IEEE Transactions
on Communications, vol. 69, no. 1, pp. 182–193, 2020.

[14] X. Ma and Z. Gao, “Data-driven deep learning to design pilot and
channel estimator for massive mimo,” IEEE Transactions on Vehicular
Technology, vol. 69, no. 5, pp. 5677–5682, 2020.

[15] A. K. Gizzini, M. Chaﬁi, A. Nimr, and G. Fettweis, “Enhancing least
square channel estimation using deep learning,” in 2020 IEEE 91st
Vehicular Technology Conference (VTC2020-Spring), pp. 1–5, IEEE,
2020.

[16] A. K. Gizzini, M. Chaﬁi, A. Nimr, and G. Fettweis, “Deep learning
based channel estimation schemes for ieee 802.11 p standard,” IEEE
Access, vol. 8, pp. 113751–113765, 2020.

[17] J. Pan, H. Shan, R. Li, Y. Wu, W. Wu, and T. Q. Quek, “Channel esti-
mation based on deep learning in vehicle-to-everything environments,”
IEEE Communications Letters, vol. 25, no. 6, pp. 1891–1895, 2021.

[18] R. C. Daniels, C. M. Caramanis, and R. W. Heath, “Adaptation in
convolutionally coded mimo-ofdm wireless systems through supervised
learning and snr ordering,” IEEE Transactions on Vehicular Technology,
vol. 59, no. 1, pp. 114–126, 2010.

[19] X. Sun, C. Wu, X. Gao, and G. Y. Li, “Fingerprint-based localization for
massive mimo-ofdm system with deep convolutional neural networks,”
IEEE Transactions on Vehicular Technology, vol. 68, no. 11, pp. 10846–
10857, 2019.

Fig. 16: Comparative analysis of PPA in ASIC implementations of
channel estimation techniques (Vdd = 1V, technology = 45 nm)

Fig. 16. To obtain the maximum operable frequency for a given
architecture, we have carried out an ASIC implementation ﬂow
with multiple timing constraints. We gradually increased the
target frequency until no more improvement was possible.

We found that for 32-bit implementation, LS achieves the
operating frequency of 350 MHz, while the LSDNN achieves
the operating frequency of 200 MHz, and the LSDNN imple-
mentation consumes 4.8× more area and 2.5× higher power
compared to the LS implementation at their corresponding
frequencies of operation. These results are expected because
LSDNN architecture performs more computation than the
corresponding LS implementation.

Further, we study the eﬀect of word-length optimizations
on the ASIC implementations of diﬀerent channel estimation
techniques. We found that upon word-length optimization of
LS from 32 bits to 18 bits, the 18-bits LS achieves 1.5×
higher operating frequency compared to 32-bits LS (from 350
MHz. to 500 MHz.). Moreover, the 18-bits LS takes 3.3× less
area and consumes 1.8× less power than the 32-bits LS at
their peak operating frequencies. Likewise, upon word-length
optimization of LSDNN from 32 bits to 24 bits, the 24-bits
LSDNN achieves 1.4× higher operating frequency compared
to 32-bits LSDNN (from 200 MHz to 285.7 MHz). Moreover,
the 24-bits LSDNN takes 1.8× less area and 1.2× less power
compared to the 32-bits LSDNN at
their peak operating
frequencies. Thus, the word-length optimization delivers PPA
improvements in ASIC implementations also, both for the LS
and LSDNN architectures.

VIII. Conclusions and Future Directions

In this work, we proposed a deep neural network (DNN)
augmented least-square (LSDNN) based channel estimation
for wireless physical layer (PHY). We have compared the
performance, resource utilization, power consumption, and ex-
ecution time of LSDNN with conventional LS and linear min-
imum mean square estimation (LMMSE) approach on Zynq
multiprocessor system-on-chip (ZMPSoC) and application-
speciﬁc integrated circuits (ASIC) platforms. Numerous ex-
periments validate the superiority of the LSDNN over LS

12

[45] X. Zhang, H. Zhao, J. Xiong, X. Liu, L. Zhou, and J. Wei, “Scal-
able power control/beamforming in heterogeneous wireless networks
with graph neural networks,” in 2021 IEEE Global Communications
Conference (GLOBECOM), pp. 01–06, IEEE, 2021.

[46] Y. Yu, T. Wang, and S. C. Liew, “Deep-reinforcement learning multiple
access for heterogeneous wireless networks,” IEEE Journal on Selected
Areas in Communications, vol. 37, no. 6, pp. 1277–1290, 2019.
[47] S. D¨orner, S. Cammerer, J. Hoydis, and S. t. Brink, “Deep learning
based communication over the air,” IEEE Journal of Selected Topics in
Signal Processing, vol. 12, no. 1, pp. 132–143, 2018.

[48] D. Luan and J. Thompson, “Low complexity channel estimation with
neural network solutions,” in WSA 2021; 25th International
ITG
Workshop on Smart Antennas, pp. 1–6, 2021.

[49] A. K. Gizzini, M. Chaﬁi, A. Nimr, and G. Fettweis, “Joint trﬁ and
deep learning for vehicular channel estimation,” in 2020 IEEE Globecom
Workshops (GC Wkshps, pp. 1–6, 2020.

[50] A. K. Gizzini, M. Chaﬁi, A. Nimr, and G. Fettweis, “Enhancing least
square channel estimation using deep learning,” in 2020 IEEE 91st
Vehicular Technology Conference (VTC2020-Spring), pp. 1–5, 2020.

[51] S. J. Pan and Q. Yang, “A survey on transfer

learning,” IEEE
Transactions on Knowledge and Data Engineering, vol. 22, no. 10,
pp. 1345–1359, 2010.

[52] P. K. Chundi, X. Wang, and M. Seok, “Channel estimation using deep
learning on an fpga for 5g millimeter-wave communication systems,”
IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 69,
no. 2, pp. 908–918, 2022.

[53] F. Restuccia and T. Melodia, “Deep learning at

the physical
layer: System challenges and applications to 5g and beyond,” IEEE
Communications Magazine, vol. 58, no. 10, pp. 58–64, 2020.

[54] Y. Cheng, D. Li, Z. Guo, B. Jiang, J. Geng, W. Bai, J. Wu, and Y. Xiong,
“Accelerating end-to-end deep learning workﬂow with codesign of
data preprocessing and scheduling,” IEEE Transactions on Parallel and
Distributed Systems, vol. 32, no. 7, pp. 1802–1814, 2021.

[55] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, “Eﬃcient processing of
deep neural networks: A tutorial and survey,” Proceedings of the IEEE,
vol. 105, no. 12, pp. 2295–2329, 2017.

[56] S. Ruder, “An overview of gradient descent optimization algorithms,”

arXiv preprint arXiv:1609.04747, 2016.

[57] G. Acosta-Marum and M. A. Ingram, “Six time- and frequency- selective
empirical channel models for vehicular wireless lans,” IEEE Vehicular
Technology Magazine, vol. 2, no. 4, pp. 4–11, 2007.

[20] D. G¨und¨uz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R. Murthy,
and M. van der Schaar, “Machine learning in the air,” IEEE Journal
on Selected Areas in Communications, vol. 37, no. 10, pp. 2184–2199,
2019.

[21] T. Wang, C.-K. Wen, H. Wang, F. Gao, T. Jiang, and S. Jin, “Deep
layer: Opportunities and challenges,”

learning for wireless physical
China Communications, vol. 14, no. 11, pp. 92–111, 2017.

[22] S. Zhang, J. Liu, T. K. Rodrigues, and N. Kato, “Deep learning
techniques for advancing 6g communications in the physical layer,”
IEEE Wireless Communications, vol. 28, no. 5, pp. 141–147, 2021.
[23] K. Mei, J. Liu, X. Zhang, N. Rajatheva, and J. Wei, “Performance anal-
ysis on machine learning-based channel estimation,” IEEE Transactions
on Communications, vol. 69, no. 8, pp. 5183–5193, 2021.

[24] H. Ye, G. Y. Li, and B.-H. Juang, “Power of deep learning for
channel estimation and signal detection in ofdm systems,” IEEE Wireless
Communications Letters, vol. 7, no. 1, pp. 114–117, 2018.

[25] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, “Deep
learning-based channel estimation,” IEEE Communications Letters,
vol. 23, no. 4, pp. 652–655, 2019.

[26] L. Li, H. Chen, H.-H. Chang, and L. Liu, “Deep residual learning meets
ofdm channel estimation,” IEEE Wireless Communications Letters,
vol. 9, no. 5, pp. 615–618, 2020.

[27] T. O’Shea and J. Hoydis, “An introduction to deep learning for the
physical layer,” IEEE Transactions on Cognitive Communications and
Networking, vol. 3, no. 4, pp. 563–575, 2017.

[28] Y. Zhang, A. Doshi, R. Liston, W.-T. Tan, X. Zhu, J. G. Andrews,
and R. W. Heath, “Deepwiphy: Deep learning-based receiver design
and dataset for ieee 802.11ax systems,” IEEE Transactions on Wireless
Communications, vol. 20, no. 3, pp. 1596–1611, 2021.

[29] S. A. U. Haq, “Source code and datasets.” Link, 2022. [Online; accessed

5-August-2022].

[30] C. E. Shannon, “A mathematical theory of communication,” The Bell

system technical journal, vol. 27, no. 3, pp. 379–423, 1948.

[31] S. M. Kay, “Fundamentals of statistical signal processing. detection

theory, volume ii,” Printice Hall PTR, vol. 1545, 1998.

[32] M. K. Karray and M. Jovanovic, “A queueing theoretic approach to
the dimensioning of wireless cellular networks serving variable-bit-rate
calls,” IEEE Transactions on Vehicular Technology, vol. 62, no. 6,
pp. 2713–2723, 2013.

[33] N. Singh, S. S. Santosh, and S. J. Darak, “Toward intelligent reconﬁg-
urable wireless physical layer (phy),” IEEE Open Journal of Circuits
and Systems, vol. 2, pp. 226–240, 2021.

[34] S. J. Darak and M. K. Hanawal, “Multi-player multi-armed bandits for
stable allocation in heterogeneous ad-hoc networks,” IEEE Journal on
Selected Areas in Communications, vol. 37, no. 10, pp. 2350–2363,
2019.

[35] R. Zhou, F. Liu, and C. W. Gravelle, “Deep learning for modulation
recognition: A survey with a demonstration,” IEEE Access, vol. 8,
pp. 67366–67376, 2020.

[36] S. Zheng, S. Chen, and X. Yang, “Deepreceiver: A deep learning-based
intelligent receiver for wireless communications in the physical layer,”
IEEE Transactions on Cognitive Communications and Networking,
vol. 7, no. 1, pp. 5–20, 2021.

[37] O. J. Famoriji, O. Y. Ogundepo, and X. Qi, “An intelligent deep learning-
based direction-of-arrival estimation scheme using spherical antenna
array with unknown mutual coupling,” IEEE Access, vol. 8, pp. 179259–
179271, 2020.

[38] H. Huang, J. Yang, H. Huang, Y. Song, and G. Gui, “Deep learning for
super-resolution channel estimation and doa estimation based massive
mimo system,” IEEE Transactions on Vehicular Technology, vol. 67,
no. 9, pp. 8549–8560, 2018.

[39] J. Gao, X. Yi, C. Zhong, X. Chen, and Z. Zhang, “Deep learning for
spectrum sensing,” IEEE Wireless Communications Letters, vol. 8, no. 6,
pp. 1727–1730, 2019.

[40] T. Wang, C.-K. Wen, S. Jin, and G. Y. Li, “Deep learning-based csi
feedback approach for time-varying massive mimo channels,” IEEE
Wireless Communications Letters, vol. 8, no. 2, pp. 416–419, 2018.

[41] C.-K. Wen, W.-T. Shih, and S. Jin, “Deep learning for massive mimo
csi feedback,” IEEE Wireless Communications Letters, vol. 7, no. 5,
pp. 748–751, 2018.

[42] P. Zhou, X. Fang, X. Wang, Y. Long, R. He, and X. Han, “Deep
learning-based beam management and interference coordination in
dense mmwave networks,” IEEE Transactions on Vehicular Technology,
vol. 68, no. 1, pp. 592–603, 2019.

[43] Q. Shi, M. Razaviyayn, Z.-Q. Luo, and C. He, “An iteratively weighted
mmse approach to distributed sum-utility maximization for a mimo
interfering broadcast channel,” IEEE Transactions on Signal Processing,
vol. 59, no. 9, pp. 4331–4340, 2011.

[44] K. Lin, C. Li, J. J. Rodrigues, P. Pace, and G. Fortino, “Data-driven
joint resource allocation in large-scale heterogeneous wireless networks,”
IEEE Network, vol. 34, no. 3, pp. 163–169, 2020.

