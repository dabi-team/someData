2
2
0
2

r
a

M
0
3

]

R
C
.
s
c
[

1
v
0
7
3
6
1
.
3
0
2
2
:
v
i
X
r
a

𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏: Comparing and selecting cryptography libraries
(long version of EICC 2022 publication)

Jan Wohlwender∗
Hochschule Darmstadt
Darmstadt, Germany

Andreas Heinemann
Hochschule Darmstadt
Darmstadt, Germany

Rolf Huesmann∗
Hochschule Darmstadt
Darmstadt, Germany

Alexander Wiesmaier
Hochschule Darmstadt
Darmstadt, Germany

ABSTRACT
Selecting a library out of numerous candidates can be a labori-
ous and resource-intensive task. We present the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index, a
tool for decision-makers to choose the best fitting cryptography
library for a given context. To define our index, 15 library attributes
were synthesized from findings based on a literature review and
interviews with decision-makers. These attributes were afterwards
validated and weighted via an online survey. In order to create the
index value for a given library, the individual attributes are assessed
using given evaluation criteria associated with the respective at-
tribute. As a proof of concept and to give a practical usage example,
the derivation of the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 values for the libraries Bouncy Castle
and Tink are shown in detail. Overall, by tailoring the weighting of
the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 attributes to their current use case, decision-makers
are enabled to systematically select a cryptography library fitting
best to their software project at hand in a guided, repeatable and
reliable way.

CCS CONCEPTS
• Security and privacy → Information-theoretic techniques;
• Human-centered computing → Accessibility systems and
tools; • Social and professional topics → Software selection
and adaptation.

KEYWORDS
Cryptography library selection, comparative index creation, at-
tributes for library evaluation, evaluation criteria for library assess-
ment, Tink, Bouncy Castle

1 INTRODUCTION AND OUTLINE
In computer science, it is common to outsource often used func-
tionalities to libraries. These are then included in software projects
if required. A particular advantage of cryptography libraries is the
professional implementation and maintenance of cryptographic
functionalities by cryptography experienced developers. Non-cryp-
tography experienced developers can use these libraries without
having to deal with the rather complicated mathematical fundamen-
tals and other obstacles of cryptography. Currently, there are nu-
merous cryptography libraries1 to choose from. This work defines
the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index to compare and select cryptography libraries

∗Both authors contributed equally to this research.
1https://en.wikipedia.org/wiki/Comparison_of_cryptography_libraries Retrieved
03 Jan 2022

based on a guided evaluation of weighted attributes. This index
can be used efficiently by decision-makers to select a cryptography
library for their software project.

We first present related work in Section 2 that also describes
contributions to library selection. Then, attributes and associated
evaluation criteria that allow a rating to be made are identified. The
attributes are first compiled in Section 3.1 based on a literature re-
search and afterwards deepened in Section 3.2 based on interviews
with decision-makers of software projects. In Section 3.3 these at-
tributes are then synthesized into a manageable set. Section 3.4
describes the conducted validation of the synthesized set through
an online survey. The 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index is introduced in Section 4
and described in detail in Section 5. The calculated index values for
the libraries Bouncy Castle and Tink are presented and discussed
in Section 6. We discuss our results in Section 7 before Section 8
concludes the paper and lists further open topics.

2 RELATED WORK
The work of Scheller et. al. [17] identifies the usability of libraries as
an important quality attribute and proposes a method to evaluate it.
Surpassing this limitation to one single attribute (usability) we
present a quantitative comparison based on multiple weighted
attributes in the work at hand.

Wilde and Amundsen [18] describe in their work, in principle
libraries can be compared and contrasted in any case. Their work
focuses on decentralized systems rather than on libraries as we
do. However, they mention that ultimately there can be no single
perfect choice. We agree on that, but we want to provide a way to
make a choice tailored to a given context.

Gao et. al. [5] addresses library recommendations in their work.
Based on a selected library, they search for other suitable libraries.
With the help of the libraries of previous similar software projects,
the authors propose alternative libraries to consider. Different from
the work at hand, they do not use an index for their library selection.
The work of Xie et. al. [19] deals with an algorithm for ranking
different libraries for combined use. The approach is not appropriate
for individual library evaluation and is solely based on the library
description. Our work proposes a method to derive a quantitative
index value for individual libraries through which they can be
directly compared.

The German Federal Office for Information Security (BSI) carried
out a comparison of several existing cryptographic libraries [9]. The
goal of the comparison was to find a library for further development.
The rating criteria were generated from the technical guideline

 
 
 
 
 
 
BSI-TR-02102 [3]. This comparison is very specific to the project
objective. The safety aspects from BSI-TR-02102 can serve as a basis
for the evaluation of security. Our 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index, in contrast, is
customizable to any project.

In the following section, attributes for our library index are
obtained from additional related work not considered in this section.

3 ATTRIBUTES COLLECTION
In order to collect attributes known to the scientific community and
possibly identify new relevant ones for our library evaluation, the
following two methods were used: Literature review and interviews
with decision-makers. Section 3.1 describes from which scientific
sources attributes were identified by literature research. Subse-
quently, Section 3.2 describes how additional attributes were iden-
tified through interviews with decision-makers. These attributes
confirm or complement the attributes identified from the literature
review.

3.1 Attributes identified by a literature survey
The literature search was conducted in the digital publication li-
braries of IEEE, ACM, and Springer. From these sources, seven
papers were identified that defined or described attributes for li-
brary evaluation. These seven papers are briefly presented in the
remainder of this section. The attributes described by the respective
authors can be taken from Table 5 with reference to their source.
For example, the attribute Role Expressiveness (ID: 1.2.1) is acquired
from paper [4], which is presented next.

Clarke’s Cognitive Dimensions Framework [4] presents twelve
factors by which developers are influenced when working with a
library. The attributes were created by the authors with the use of
Visual Basic classes in their minds. All twelve attributes have been
adopted by us.

The authors Zghidi et. al. extend Clarke’s framework in their
paper [20] by another eleven attributes that were collected in a
study in a software company that is not described further. In their
study stakeholders brought a stronger technical perspective to the
process. All eleven attributes were included in further consideration
by us.

In the work of Grill et al. [7] heuristics are used to identify
usability obstacles. The heuristics were determined by a study. All
14 attributes from this work were adopted by us.

Myers et al. [15] describe the usability of libraries using a user-
centered design process. The work provides a platform as well
as a cross-purpose guide to improving library usability and lists
approaches to validating a library for this purpose. All ten attributes
from this work were adopted by us.

Ease of learning libraries is of highest relevance for the authors of
[10]. They derive so-called “learning barriers” from studies carried
out in their work. We derived five attributes relevant for library
evaluation from these “learning barriers” for further use in our
work.

In [2], Bloch shows why a good library design is important for
usability and how it can be considered in development. From this
work, 21 attributes are extracted and used by us.

Using the method of categorization by [12], additional attributes
were identified with the work of Acar et. al. [1]. Although [1]

Wohlwender and Huesmann, et al.

focuses on the applicability of Python libraries, they define several
points that can be used for general scoring. Five attributes were
adopted by us.

The papers presented above [1, 2, 4, 7, 10, 15, 20] focus on the us-
ability of libraries. The works predominantly pay attention to good
applicability and the necessary comprehensibility of the library. It is
noticeable that they demand extensive and detailed documentation,
preferably with many examples. The quality of library documenta-
tion is mentioned most frequently as an attribute and thus enjoys
high relevance among the attributes.

In conclusion, the literature search produced a set of 78 unique
attributes that are considered in our further process. Some attributes
were discovered in several different works as is documented in
Table 5.

Except for [1], the works presented here did not consider cryp-
tography libraries. Accordingly, cryptography specific attributes
were additionally identified through the interviews described in
the following section.

3.2 Attributes identified by interviews with

decision-makers

Additional attributes were identified via interviews with decision-
makers. The interviews intended to examine the attributes found in
the literature for their actuality and additionally to identify further
relevant attributes specific to cryptography libraries.

The interviews were conducted with individuals who are in
positions of making technical decisions in software projects in
their professional environment. Decision-makers such as technical
directors, CTOs, or autonomous developers were explicitly invited
to the interviews. It is important that these were people who were
involved in cryptography library selection in the past or will be in
the near future.

A total of 5 interviews were conducted. 4 people had already
selected a cryptography library at that point. The average work
experience is 8.1 years. The people were recruited from companies
and the social circle of our university. The interviews were con-
ducted in compliance with the applicable General Data Protection
Regulation (GDPR) and in accordance with §24 of the Hessian Data
Protection and Freedom of Information Act (HDSIG)2.

They were performed via different online conferencing systems.
In the process, the audio was recorded with the consent of the sub-
jects. Subsequently, the recordings were anonymized by verbatim
transcription. Qualitative content analysis using a categorization
method described in [11] was used to extract attributes from the
interviews. Since the interviews were conducted independently
from the literature review, we refrained from defining a grouping
with justified evaluation aspects before categorizing. Therefore,
categorization grouping was performed using Mayring’s inductive
category formation method [14].

The interviewer followed the interview guide (Appendix B) to
ensure consistent conditions.
In order to reduce initial contact
inhibitions and to be able to better assess the people, demographic
information and previous experience in projects were shortly dis-
cussed at the beginning of each interview. Subsequent questions
addressed the library decision-making process used in the company

2Hessisches Datenschutz- und Informationsfreiheitsgesetz

𝑐𝑟 𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

or project in which the people were involved. Through the ques-
tions formulated in Appendix B, the interview aimed to find out
whether there is a decision-making process for choosing libraries
and how it is formulated or lived.

This allowed us to generate attributes of interest for scoring from
possible existing specifications imposed on people in their projects.
Likewise, the composition of the team or the decision-making body
is interesting; such as areas of expertise and interests of the people
involved. Possibly, evaluation criteria result from organizational
dependencies.

To identify more attributes, the interviewer asked how this group
of people find a decision and what sources are used in this process.
At the end of the interview, the people were given the opportunity
to ask further questions or to express themselves freely.

From the interviews, through a qualitative content analysis of
the transcription, 50 attributes were identified. These are tagged in
Table 5 with an “I” for interview and the number of mentions across
all interviews, for example “(I:3)”. In addition to the collection of
attributes for scoring, it is obvious from the interviews that there
is a strong personal opinion or preference among the people about
the procedure. These preferences come into effect differently at
evaluating and making decisions about libraries. For people without
a given fixed decision-making process in the company or the project,
personal opinion shapes relevant decision criteria.

3.3 Synthesis of Section 3.1 and Section 3.2
The 78 attributes obtained from the literature review and 50 from
the interviews can be considered complementary sources, as they
were collected using independent methods. The 128 attributes were
categorized by the authors based on their descriptions or the con-
text in which they are placed. In this process, the same, similar or
related attributes were grouped under a meaningful attribute and
structured hierarchically. In Table 5 this is represented by levels
1 − 4 and the ID.
In the further course of this work, level 1 is
referred to as attribute. Since the respective levels 2 − 4 are more
finely granular than the superordinate level 1, they are suitable as
evaluation criteria and will be referred to as such in the following.
This process resulted in 15 attributes which are listed in Table 1.

The description of the respective attributes listed there includes
an explanation of the term. These were generated from the de-
scription of the attributes in the literature or the contexts in the
interviews. The description of the attributes is intended to help
developers later when weighting the attributes.

3.4 Validation of the synthesis via survey
By an online survey, the results in Table 1 ere confirmed. Partic-
ipation in the survey was voluntary and in compliance with the
currently applicable data protection regulations GDPR and §24 HD-
SIG. People acquisition was conducted via direct messaging, social
media, and email. A total of 36 people participated in the survey.
Twelve people performed them completely. Appendix C lists the
questionnaire. The average programming experience of the people
is 7.88 years. Figure 1 shows the frequency with which people make
decisions about libraries. 24% of the people have already selected
a cryptography library.

Table 1: The index attributes

Name
Ease of Use

Scalability

Testability

Extendability

Functional
Complete-
ness
Data Types

Description
How much information can you extract intuitively
while using?
Is the work sequence running synchronous and par-
allel and what data sizes are handleable?
Is the API easily testable and debuggable, can you look
up the system status at any time, are errors caught,
shown, and logged?
Is the functionality extendable and how much effort
do these changes of the API make?
Is the API functionally complete, does it pack all fea-
tures needed, and is it purposefully?

What data types are being used, are they intuitive, do
parameters and return values fit the functions and are
they ordered consistently?

Code Quality Does the code stick to standards and conventions?
Cost

What costs are caused by the API and which licenses
are available?

Requirements What are the requirements of the API and does the

Complexity

Maintained

Spread

Performance
Impact
Security
Documenta-
tion

API cause dependencies that need to be solved?
How complex is the API and how flexible can it be
used and configured? What is the design aesthetic
and how much boilerplate code (same code that needs
to be repeated many times with no changes) needs to
be written?
How maintained is the API, meaning is it being devel-
oped further and is support being provided?
How widespread is the API and how big is the com-
munity? What are the opinions on the API, how is its
reputation, are there successful stories or recommen-
dations?
How does the API impact performance and latency?

Are the API and the procedures used secure?
Is the API documented thoroughly and are best prac-
tices being followed or examples are given?

Figure 1: Evaluation of the question 2: How often do you
select an API?

weekly

several times per month

once per month

several times per year

once per year

never

12%

16%

16%

16%

20%

20%

0%

5%

10%

15%

20%

25%

People

Figure 2: Evaluation of question 4 in blue: "Does the descrip-
tion match the term?" and question 5 in red: "Is the term
suitable for the rating of an API (not in context of own
projects)?" with mean and standard deviation (SD).

Wohlwender and Huesmann, et al.

Table 2: Evaluation of the question 6: personal assessment
of the relevance of the attributes. (Sorted in descending or-
der from Rank 15 = as most important, to Rank 1 = least
important).

m oderately

(3)

fairly
(4)

extraordinarily

(5)

Ra Attribute (Elected by x people to this rank)

15

Ease of Use (4); Security (4); Functional Completeness (1); Code
Quality (1); Requirements (1); Documentation (1)

14 Documentation (3); Functional Completeness (2); Security (2);
Ease of Use (1); Scalability (1); Testability (1); Extendability (1);
Spread (1)

Ease of Use

Scalability

Testability

Extendability

Functional Com-
pleteness

Data Types

Code Quality

Cost

Requirements

Complexity

Maintained

Spread

Performance Im-
pact

Security

Documentation

(mean: 4,28; SD: 0,77)

(mean: 4,38; SD: 0,94)

(mean: 3,94; SD: 0,69)

(mean: 4,23; SD: 0,9)

(mean: 4,5; SD: 0,82)

(mean: 4,46; SD: 0,96)

(mean: 4,17; SD: 0,75)

(mean: 4,23; SD: 0,9)

(mean: 4,06; SD: 0,77)

(mean: 4,38; SD: 0,94)

(mean: 4,19; SD: 0,8)

(mean: 4,31; SD: 0,92)

(mean: 4,5; SD: 0,88)

(mean: 4,08; SD: 0,89)

(mean: 4,25; SD: 0,81)

(mean: 4,08; SD: 0,85)

(mean: 4,12; SD: 0,76)

(mean: 4,31; SD: 0,92)

(mean: 3,73; SD: 0,71)

(mean: 3,92; SD: 0,81)

(mean: 4,53; SD: 0,86)

(mean: 4,23; SD: 0,9)

(mean: 4,29; SD: 0,8)

(mean: 3,85; SD: 0,79)

(mean: 4,12; SD: 0,76)

(mean: 4,54; SD: 0,98)

(mean: 4,5; SD: 0,88)

(mean: 4,62; SD: 1,00)

(mean: 4,65; SD: 0,88)

(mean: 4,38; SD: 0,94)

12

13 Maintained (2); Security (2); Scalability (1); Extendability (1);
Functional Completeness (1); Code Quality (1); Requirements (1);
Spread (1); Performance Impact (1); Documentation (1)
Functional Completeness (3); Documentation (3); Ease of Use (1);
Scalability (1); Testability (1); Extendability (1); Code Quality (1);
Requirements (1)
Testability (3); Functional Completeness (2); Ease of Use (1); Ex-
tendability (1); Code Quality (1); Requirements (1); Maintained (1);
Performance Impact (1); Security (1)

11

9

10 Maintained (3); Requirements (2); Complexity (2); Ease of Use (1);
Scalability (1); Cost (1); Spread (1); Performance Impact (1)
Extendability (2); Code Quality (2); Cost (2); Scalability (1); Testa-
bility (1); Data Types (1); Requirements (1); Complexity (1); Per-
formance Impact (1)
Cost (3); Testability (2); Performance Impact (2); Extendability (1);
Data Types (1); Code Quality (1); Complexity (1); Spread (1)
Scalability (2); Complexity (2); Security (2); Ease of Use (1); Ex-
tendability (1); Functional Completeness (1); Requirements (1);
Spread (1); Performance Impact (1)

7

8

5

4

6 Maintained (3); Scalability (2); Code Quality (2); Ease of Use (1);
Functional Completeness (1); Cost (1); Complexity (1); Perfor-
mance Impact (1)
Requirements (3); Ease of Use (1); Testability (1); Data Types (1);
Cost (1); Complexity (1); Maintained (1); Spread (1); Security (1);
Documentation (1)
Complexity (4); Performance Impact (2); Documentation (2); Scal-
ability (1); Testability (1); Data Types (1); Maintained (1)
Data Types (3); Extendability (2); Cost (2); Spread (2); Performance
Impact (2); Code Quality (1)
Data Types (4); Testability (2); Spread (2); Ease of Use (1); Cost (1);
Maintained (1); Documentation (1)
Spread (2); Code Quality (2); Scalability (1); Extendability (1); Func-
tional Completeness (1); Data Types (1); Cost (1); Requirements (1)

1

3

2

In Figure 2, the questions 4 and 5 are answered by people using a
five-point scale (not at all = 1, hardly = 2, moderately = 3, fairly = 4,
extraordinarily = 5 [16]). It can be seen from the blue dots (question
no. 4) that all descriptions were rated by them as matching the
attributes with a tendency to fairly.

In question no. 5, people were asked whether they felt the
attributes were suitable for an index. All attributes were rated as
tending to be fairly suitable for an index. In Figure 2 this can be
seen by the red squares.

With the question 6, the people were asked to rate the relevance
of the attributes according to their personal assessment based on
a sorting of the attributes. Rank 15 means that this attribute is
the most relevant. The lower the rank, the less important the at-
tribute. In Table 2, people rankings were evaluated by the mean
rank method [8]. The attributes can be placed in multiple rows by
varying people’s votes. Number of votes in brackets.

From these results, a preference of the people is deduced. It
becomes clear that Ease of Use and Security are the most relevant
attributes for cryptography libraries from the people’s point of
view. For both attributes received the most votes (four) in 15th
place. Using the preferences from Table 2, a reference weighting
for the index is derived in the next section.

𝑐𝑟 𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

4 INDEX DEFINITION
In order to create a comparable consistent weighting for the in-
dex, the number of mentions of the attributes from the literature,
the interviews and the weighting from Table 2 are put together.
The number of mentions in the literature, the interviews, and the
rankings from Table 2 are each normalized by the rank method [8].
The average of these three sources (column no. VI in Table 3) is
then ranked again (in column no. VII). Based on the rank in column
no. VII, the possible values for (𝑔𝑖 ) (column no. VIII in Table 3)
1.5; 1.25; 1; 0.75; 0.5 are assigned as following: the ranks 15 - 13
get 1.5 points each, the ranks 12 - 10 get 1.25 points each and so
on. This procedure is shown in Table 3 with the intermediate re-
sults and yields the reference weighting (𝑔𝑖 ) of the attributes in
column no. VIII.

The weighting of the attributes can be chosen differently for
each software project. This is realized by individual adjustments to
the weighting in Table 3 column no. VIII. The sum of the individual
weights should correspond to the number of attributes. This is to
prevent excessive weighting.

The 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of a cryptography library is calculated from
the sum of all evaluation criteria of an attribute divided by the
number of evaluation criteria. This sum is multiplied by the weight
of the attribute and the sum of all attributes gives the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏
index. Thus, the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index is defined as follows:

𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index =

𝑏𝑖 𝑗

(cid:205)𝑚𝑖
𝑗=1
𝑚𝑖

𝑛
∑︁

𝑖=1

· 𝑔𝑖

Where 𝑏𝑖 𝑗 stands for evaluation criterion 𝑗 of attribute 𝑖; 𝑚𝑖 = num-
ber of 𝑏 𝑗 in attribute 𝑖; 𝑛 = number of attributes and 𝑔𝑖 stands for the
weighting of the respective attribute 𝑖 (see Table 3 column no. VIII).
We construct the formula in this way because, this index treats
attributes with many evaluation criteria, the same as attributes
with few once. This allows evaluation criteria to be easily added
(or omitted) in a possible new version of this index. Multiple evalu-
ation criteria for a single attribute ensure that the evaluation of the
attribute will be more fine-grained and accurate. The weighting of
the respective attribute has more influence on the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index
than the number of evaluation criteria of an attribute. This makes
the index flexibly adaptable to the needs of software projects.

5 DEFINITION OF THE 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 INDEX

EVALUATION CRITERIA

The evaluation criteria defined in Appendix A are used for library
assessments. The assessment results are used to create the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏
index described in the previous section. In general, all evaluation
criteria are normalized to the scale [−2, −1, 0, +1, +2]. For many
evaluation criteria, the scale is not fully indicated within its textual
description. In such cases, the intermediate values can be inter-
polated linearly. Evaluation criteria without explicit description
of normalization are presented as follows via numerical values in
percent. This rating is referred to as the default rating in the further
text.

+2 if at least 90% of the condition of the evaluation criterion is

met.

+1 if at least 75% of the condition is met.
0 if at least 50% of the condition is met.

-1 if at least 25% of the condition is met.
-2 if less than 25% of the condition is met.

The evaluation criteria are listed and described in detail in Appen-
dix A.

6 EXEMPLARY USE
In the previous section, the evaluation criteria of the 15 attributes
were introduced. These were used to evaluate the Java versions of
Google’s Tink3 library and the Bouncy Castle4 library. The two
libraries were chosen because they appeal to the similar target group
of developers. They are both developed and maintained by an open
source community. Bouncy Castle is established. Tink is relatively
new (since 2017) and claims to be easy to use for developers without
cryptographic knowledge5.

Table 4 shows the individual points achieved for the evaluation
criteria of both libraries. The attributes are the average of the eval-
uation criteria. These values were multiplied with the reference
weighting from Table 3 column no. VIII and the results summed up
resulting in the index value for each. The 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of Bouncy
Castle is 7.08. Tink reaches an index of 16.75. With the reference
weighting used, a minimum index of −29 points and a maximum in-
dex of 29 points is achievable. Two detailed assessments are shown
below as examples. They are based on the respective criteria in An-
nex A. For the evaluation criterion 1b Default Settings, the Bouncy
Castle library was given a score of 0, since obsolete algorithms
can be used due to the flexible constructors. One example is the
constructor for the base AES keygen KeyGen (int) where the user
can input any value. The Tink library has been assigned a score of
+2, because safe default values are used. For the evaluation crite-
rion 11a Release-frequency, the Bouncy Castle library was given a
score of +1, because the current6 release cycle is every four months
on average. The Tink library has achieved a rating of +1 as well
because currently6 releases are being released quarterly.

These examples, and Table 4 shows that the selected evaluation
criteria are applicable. With the reference weighting generated
from the online survey preferences in Section 3.4, the Tink library
gets a better 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index. When compared in detail, the Tink
library performs better in each of the three attributes weighted
highest at times 1.5: Ease of Use, Code Quality, and Documentation.
Moreover, with the exception for Testability, this is the same case for
the attributes with 1.25-fold weights Complexity and Security. Thus,
according to the reference weighting, the Tink library is preferable
to the Bouncy Castle library.

Since each software project has different conditions and re-
quirements, the weighting can be adjusted individually. This leads
to 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index results adapted to the corresponding software
project.

7 DISCUSSION AND LIMITATIONS
It is generally difficult to get experts for interviews. Since only 5 peo-
ple participated in the interviews, it is possible that not all relevant

3Version 1.6.1, https://github.com/google/tink Retrieved 28 Oct 2021
4Tag r1rv69, https://github.com/bcgit/bc-java Retrieved 28 Oct 2021
5https://developers.google.com/tink Retrieved 22 Oct 2021
627 Jan 2021

Table 3: The calculation of the reference weight (𝑔𝑖 ) for the index. The number of mentions is listed in the brackets as the basis
for the rank.

Wohlwender and Huesmann, et al.

I
No Attribut

II

III
Rank Literature
(number of
mentions)

1
2
3
4
5

Ease of Use
Scalability
Testability
Extendability
Functional
Completeness

Code Quality
Cost
Requirements

6 Data Types
7
8
9
10 Complexity
11 Maintained
Spread
12
13
Performance Impact
Security
14
15 Documentation

15 (33)
6,5 (2)
14 (9)
8 (3)
10,5 (4)

10,5 (4)
10,5 (4)
2,5 (0)
2,5 (0)
13 (8)
2,5 (0)
2,5 (0)
6,5 (2)
5 (1)
10,5 (4)

IV
Rank
Interviews
(number of
mentions)
12 (10)
8 (4)
5 (1)
2,5 (0)
2,5 (0)

2,5 (0)
10 (5)
13 (11)
8 (4)
6 (3)
11 (8)
15 (26)
2,5 (6)
8 (4)
14 (19)

V
Rank
Questionnaire
(Table 2)

15
9
11
10
12

7
10
10
8
9
13
6
11
15
14

VI
⊘

14
7,83
10
6,83
8,33

7,67
10,17
8,5
6,17
9,33
8,83
7,83
6,67
9,33
12,83

VII
Rank
Total

VIII
Weight
(𝑔𝑖 )

15
5,5
12
3
7

4
13
8
1
10,5
9
5,5
2
10,5
14

1,5
0,75
1,25
0,5
1,0

0,75
1,5
1,0
0,5
1,25
1,0
0,75
0,5
1,25
1,5
(cid:205) 15

properties for cryptographic libraries were mentioned. Due to tech-
nical advancements, some of the evaluation criteria might change
their relevance. They should periodically be reviewed, therefore, to
ensure that they are up to date. New criteria should be added to a
new version of the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index, if necessary. For example, the
authors are not aware of any useful evaluation criterion applicable
for the attribute Performance Impact. This might change over time.
In the same way, individual evaluation criteria show potential for
improvement. For example, the evaluation criterion 12b Repositories
is difficult to implement for commercial projects, since these are
rarely developed on a public repository. Other evaluation criteria
are better suited for libraries written in object-oriented languages.
For example, the criteria 3b Exceptions, 4a Public, and 4b Interfaces.
This represents a bias against procedural languages.

Overall, the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index described in this work is a first step
to compare cryptography libraries with each other. The individually
adaptable weighting supports the project-specific customization to
meet the needs of the software project. The reference weighting
provided in the work at hand enables libraries to be compared
independently of specific projects, leading to a general quantitative
comparison of crypto libraries.

seen that some evaluation criteria do not equally fit to all pro-
gramming languages. Furthermore, it is not always easy or even
possible to find the information needed to assess the evaluation cri-
teria. More surveys may provide new attributes or new evaluation
criteria of existing attributes for our index.

Although it takes effort to evaluate many libraries using our
index, in the end there will be a benefit for software quality. We
envision an open access database where the 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of
many libraries is stored and actively maintained by a committee of
experts. The ranking of individual libraries based on the reference
weighting could be visualized in a graph over time to show the
evolution of each library. Decision-makers could then just adapt
the weightings of the attributes to fit their particular project.

ACKNOWLEDGMENTS
This research work has been funded by the German Federal Min-
istry of Education and Research and the Hessian Ministry of Higher
Education, Research, Science and the Arts within their joint sup-
port of the National Research Center for Applied Cybersecurity
ATHENE.

8 CONCLUSION AND OUTLOOK
This is a first attempt to create a 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index based on litera-
ture and interviews. By evaluating the two libraries Bouncy Castle
(𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of 7.08) and Tink (𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of 16.75) as
examples, we have shown that this index is applicable.

Our reference weighting emphasizes that Ease of Use, Code Qual-
ity, and Documentation are the most relevant attributes. We have

REFERENCES
[1] Yasemin Acar, Michael Backes, Sascha Fahl, Simson Garfinkel, Doowon Kim,
Michelle L. Mazurek, and Christian Stransky. 2017. Comparing the Usability of
Cryptographic APIs. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE,
New York, USA, 154–171. https://doi.org/10.1109/SP.2017.52

[2] Joshua Bloch. 2006. How to design a good API and why it matters. In Proc.
21st ACM SIGPLAN Conference (OOPSLA). ACM, Portland, Oregon, 506–507.
https://doi.org/10.1145/1176617.1176622

[3] BSI. 2021.
sellängen.

Kryptographische Verfahren: Empfehlungen und Schlüs-
Technical Report BSI TR-02102-1. BSI, Bonn, Germany.

1109/ICWS.2015.64

[6] google. 2015. Google Java Style Guide. Google.

styleguide/javaguide.html

https://google.github.io/

[7] Thomas Grill, Ondrej Polacek, and Manfred Tscheligi. 2012. Methods towards API
usability: a structural analysis of usability problem categories. In International
conference on human-centred software engineering. Springer, Berlin, 164–180.
[8] Jürgen Hedderich and Lothar Sachs. 2016. Angewandte Statistik. Springer, Berlin.
[9] Rohde & Schwarz GmbH & Co. KG. 2015. Sichere Implementierung einer all-
gemeinen Kryptobibliothek: Arbeitspaket 1: Sichtung und Analyse bestehender
Kryptobibliotheken. https://media.frag-den-staat.de/files/foi/89304/Analyse_
geschNANAMEErzt_Vorblatt.pdf

[10] Andrew J Ko, Brad A Myers, and Htet Htet Aung. 2004. Six learning barriers in
end-user programming systems. In 2004 IEEE Symposium on Visual Languages-
Human Centric Computing. IEEE, New York, USA, 199–206.

[11] Udo Kuckartz. 2018. Qualitative Inhaltsanalyse: Methoden, Praxis, Computerunter-

stützung (4. auflage ed.). Beltz Juventa, Weinheim Basel.

[12] Jonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser. 2017. Research Methods
in Human-Computer Interaction. Morgan Kaufmann, Burlington, Massachusetts.
Google-Books-ID: hbkxDQAAQBAJ.

[13] R.C. Martin. 2009. Clean Code: A Handbook of Agile Software Craftsmanship.
Prentice Hall, Upper Saddle River, New Jersey. https://books.google.de/books?
id=dwSfGQAACAAJ

[14] Philipp Mayring. 2000. Qualitative Inhaltsanalyse. In Forum Qualitative Sozial-
forschung/Forum: Qualitative Social Research, Vol. 1. Institut für Qualitative
Forschung, Berlin.

[15] Brad A Myers and Jeffrey Stylos. 2016. Improving API usability. Commun. ACM

59, 6 (2016), 62–69.

[16] B. Rohrmann. 1978. Empirische Studien zur Entwicklung von Antwortskalen
für die sozialwissenschaftliche Forschung. Zs. für Sozialpsychologie 9 (1978),
222–245.

[17] Thomas Scheller and Eva Kuehn. 2015. Automated measurement of API usability:
The API Concepts Framework. Information and Software Technology 61 (02 2015).
https://doi.org/10.1016/j.infsof.2015.01.009

[18] Erik Wilde and Mike Amundsen. 2019. The Challenge of API Management:
API Strategies for Decentralized API Landscapes. In Companion Proceedings of
The 2019 World Wide Web Conference. ACM, San Francisco, USA, 1327–1328.
https://doi.org/10.1145/3308560.3320089

[19] Fenfang Xie, Jianxun Liu, Mingdong Tang, Dong Zhou, Buqing Cao, and Min Shi.
2016. Multi-relation Based Manifold Ranking Algorithm for API Recommendation.
In Advances in Services Computing. Springer, Cham, 15–32. https://doi.org/10.
1007/978-3-319-49178-3_2

[20] A. Zghidi, I. Hammouda, B. Hnich, and E. Knauss. 2017. On the Role of Fitness
Dimensions in API Design Assessment - An Empirical Investigation. In 2017
IEEE/ACM 1st International Workshop on API Usage and Evolution (WAPI). IEEE,
New York, USA, 19–22.

𝑐𝑟 𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

Table 4: Evaluation criteria (withe background), Attributes
(gray background) and resulting 𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index of the li-
braries Bouncy Castle and Tink.

Nr
1
1a
1b
1c
2
2a
3
3a
3b
4
4a
4b
5
5a
6
6a
6b
7
7a
7b
7c
8
8a
8b
9
9a
10
10a Atomic Setting
10b Boilerplatecode
11 Maintained
11a Release-frequency
11b
11c
12
12a
12b Repositories
13
14
14a
14b Certificated
15
15a
15b Examples

Bouncy Castle Tink
Attribute
+0.33
Ease of Use
Readability
0
Default Settings
0
Naming Conventions
+1
0
Scalability
Concurrency
0
+1
Testability
Testability
0
Exceptions
+2
+2
Extendability
Public
+2
Interfaces
+2
Functional Completeness +1
Purposefulness
+1
+2
Data Types
Returnvalues
+2
Ordering
+2
−0.67
Code Quality
Bugs
+2
Vulnerability
−2
Code Smell
−2
+2
Cost
Cost
+2
Licence
+2
+1
Requirements
Dependencies
+1
+0.5
Complexity
+1
0
0.33
+1
−1
+1
+1
+1
+1
−
−0.5
−1
0
−0.5
+1
−2
7.08

+1
−1
+2
+2
0
0
+0.5
0
+1
+2
+2
+2
+2
+2
+2
+2
+2
+1.67
+1
+2
+2
+2
+2
+2
+2
+2
+1
0
+2
0
+1
0
−1
+0.5
−1
+2
−
0
+2
−2
+2
+2
+2
16.75

Patch frequency
Support
Spread
Successful Stories

Performance Impact
Security
Standards

Documentation
Function documentation

𝑐𝑟𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/
TechnischeRichtlinien/TR02102/BSI-TR-02102.pdf
[4] Steven Clarke. 2004. Measuring API Usability.
windows/measuring-api-usability/184405654

http://www.drdobbs.com/

[5] Wei Gao, Liang Chen, Jian Wu, and Honghao Gao. 2015. Manifold-Learning
Based API Recommendation for Mashup Creation. In 2015 IEEE International
Conference on Web Services. IEEE, New York, USA, 432–439. https://doi.org/10.

A EVALUATION CRITERIA

(1) Ease of Use:

(a) Readability: The length of the function calls or the number
of their parameters in a library are crucial for intuitive
use [15]. Function calls with more than three parameters
should be well justified and still avoided if possible [13].
The default evaluation determines how many functions
have niladic or monadic (less than or equal to two param-
eters).

(b) Default Settings: Are there default values for cryptographic
procedures and are they secure according to the current
status and the recommendations[3] of the German Federal
Office for Information Security (BSI)?
+2 if the default value corresponds to the specifications.
-1 if no or bad default values are suggested.

(c) Naming Conventions: A naming system is used consis-
tently. For example, for Java libraries the Google Java Style
7 are used. The eval-
Guides [6] or Oracle Codeconventions
uation of this criterion is based on the default evaluation.
(d) Regularity: Attention is paid throughout to the symme-
try of names [2] for the same functionality, pairs such as
connect() and disconnect().
+2 when naming is used symmetrically where possible.
0 when naming is used symmetrically for central func-

tions.

-2 when naming does not follow any (recognizable) sys-

tem.

(e) Self-describing function names: The function names are
understandable, intuitive, and reflect the functionality [2].
The evaluation of this criterion is based on the default
evaluation.

(2) Scalability:

(a) Concurrency: Parallel execution of library functions, e.g.
in multiple threads or by clusters or load balancers, is
generally supported. Functionalities that are inherently
sequential, e.g. encrypting after signing, are excluded from
this consideration.
+2 Yes
0 if it is possible via workarounds. For example, by split-
ting data for encryption into several parts and thus
processing them in a distributed manner.

-2 No
(3) Testability:

are offered.
+2 if test functions are supplied, for example, ready-made
test classes, default tests, or test examples in the docu-
mentation.

-2 if no test functions or examples are provided in the

documentation.

(b) Exceptions: Error handling is actively performed by the

library.
+2 when customized error handling routines with error

descriptions are used.

7https://www.oracle.com/java/technologies/javase/codeconventions-contents.html Re-
trieved 16. Feb 2021

(a) Testability: There are test recommendations e.g. test classes/functions

Wohlwender and Huesmann, et al.

0 when (standard) error handling routines are processed.
-2 if there are no error handling routines.

(4) Extendability:

(a) Public: Classes and functions relevant for extended func-
tionality are publicly declared and thus inheritable.
+2 when classes and functions relevant for extended func-

tionality are declared publicly.

0 if partial aspects of the implementation are declared as

public.

-2 if only bundling to no functions and classes are declared

public.

(b) Interfaces: Interfaces are used.

+2 for the use of interfaces on all classes that the user is

to use.

0 if there are isolated interfaces.
-2 for the lack of interfaces in the library.

(5) Functional Completeness:

(a) Purposefulness: The library fulfills only its core mission.

+2 when the library fulfills only its core mission.
0 for libraries that provide additional features that are not

necessary but useful.

-2 in libraries where the actual purpose is obscured by

non-purposeful features.

(6) Data Types:

(a) Return values: Functions have return values to ensure that
they have been executed successfully. The evaluation of
this criterion is based on the default evaluation.

(b) Ordering: The order of parameters is consistent. The eval-
uation of this criterion is based on the default evaluation.
(7) Code Quality: Through a SonarCube instance, the code qual-
ity is analyzed automatically. The index score is composed
of U.S. school grades (A through E correspond to 2 through
-2) for the following three analyses:
(a) Bugs
(b) Vulnerability
(c) Code Smell

(8) Cost:

(a) Cost: The library usable without fees.

+2 for libraries free of cost.
-2 for libraries with a fee.

(b) Licence: Under which license is the library offered?

+2 for licenses that allow unrestricted commercial use.
0 for licenses that allow free use for non-commercial pur-

poses or allow commercial use against payment.

-2 for licenses that do not provide for commercial use and

still require payment.

(9) Requirements:

(a) Dependencies: There must be other software, software

packages, or files installed.
+2 the library automatically installs all dependencies if it

has any (possibly via a package manager).

-2 the dependent software must be installed manually.

(10) Complexity:

(a) Atomic Setting: Precise or fine adjustments can be made.
+2 if an API specifies settings and these can be changed.
0 wenn eine API Einstellungen vorgibt und diese nicht

verändert werden können.

𝑐𝑟 𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

0 if an API specifies settings and these cannot be changed.
-2 when parameters must all be selected manually.
(b) Boilerplatecode: Recurring code overhead must be written
by developers in order to use certain functionalities of the
API.
+2 for APIs that provide dedicated methods instead of

recurring code.

0 for APIs where the largest blocks are simplified.
-2 when the user has to write the same code over and over

again.

(11) Maintained:

(a) Release-frequency: How often is a library updated or a
new major version released? The question refers to the
last three major versions of a library.
+2 there is a fixed release schedule of the library.
0 there is no fixed release schedule, but eventually, the

next release will come.

-2 it is not clear if there will be another release.

(b) Patch frequency: Security gaps are quickly fixed by patches.
+2 if patches have been released within 908 days in the

past.

+1 when patches are delayed for more than 90 days.
0 when it is not clear if patches will be provided.
-2 if no patches are delivered.

(c) Support: The library has an official support channel (e.g.,

a forum, wiki, or mailing lists).
+2 if the project offers free support for the product.
0 with an official paid support.
-2 when there are no official information channels and

sources.

(12) Spread:

(a) Successful Stories: There are articles on successful stories
of using the library in other projects. Successful stories are
articles, contributions or scientific papers on successful
use, which, for example, attest to the added value of the
library or describe experiences.
+2 in the case of several articles or field reports in rep-

utable trade journals such as heise.de9.

0 with no significant mentions in professional journals.
-2 for purchased posts on blogs or similar formats.
(b) Repositories: The project is very popular on repositories
(such as GitHub, GitLab, SourceForge, Bitbucket or a repos-
itory managed by the project). This can be read off from
"‘likes"’, "‘stars"’ or similar. The basis for the rating is the
most popular Java library on GitHub in English language
at the time of the work (December 2020) (Mindustry with
7557 stars10). The evaluation of this criterion is based on
the default evaluation.

(13) Performance Impact
(14) Security

(a) Standards: Are used exclusively algorithms that are on the

white list [3].

8According to the Google "‘Project Zero"’ deadline. https://en.wikipedia.org/wiki/
Project_Zero Retrieved 28 Oct 2021
9https://www.heise.de/ Retrieved 28 Oct 2021
10https://github.com/trending/java?since=monthly&spoken_language_code=en Re-
trieved 23 Dec 2020

+2 when algorithms and standards meet (or exceed) the

current state of the art.

0 if the library meets the standards exactly.
-2 if a sub-aspect is obsolete or is not considered safe

enough.

(b) Certificated: Has the library been certified?

+2 Yes, multiple.
0 Yes.
-2 No.
(15) Documentation:

(a) Function documentation: Each method or function is doc-

umented.
The evaluation of this criterion is based on the default
evaluation.

(b) Examples: There is an example of the correct application
for each method or function. The evaluation of this crite-
rion is based on the default evaluation.

B INTERVIEW
This is the schedule of interviews with decision makers presented
in Section 3.2.

(1) What is your position or role in projects?
(2) How long have you had experience as in this position or

role?

(3) Have you ever worked with a crypto API?

(a) From now on, assume you need to choose a Crypto API

for your next project.

(4) Do you have a decision process for selecting a (crypto) API

in your projects?

(a) What does this process look like?

(5) How do you plan to proceed?
(6) Do you have to adhere to specifications?
(7) Who is involved in this procedure? (How many?)
(a) How are the roles distributed in the team?

(8) Do you get help from outside the team, consultants, or ex-

ternal collaborators?

(a) What information do you get from them?
(9) What are your steps for making the decision?
(10) Where do you get the information from?
(11) What criteria do you use?

(a) Where do they come from?
(b) Why do you attach importance to these topics?
(c) How do you evaluate the criteria?
(d) How do you weight the criteria?

(i) Are the weightings regularly reviewed and adjusted?

(e) Are the criteria given to you?
(12) Do you have to justify your decision?
(13) Do you have any questions yourself or would you like to

share something?

C QUESTIONNAIRE
The questionnaire from the online survey presented in Section 3.4.

(1) For how long have you worked as a developer in years?

(number)

Wohlwender and Huesmann, et al.

(2) How often do you need to choose an API? (checkbox: never,
once a year, several times a year, once a month, several times
a month, weekly)

(3) Have you used a crypto-API before? (yes/no)
(4) Does the description match the term? (Answer options: not
at all, barely, mediocrally, fairly, exceptionally) List with the
attributes and descriptions from table 1.

(5) Is the term suitable for the rating of an API (not in con-
text of own projects)? (Answer options: not at all, barely,
mediocrally, fairly, exceptionally) List with attributes from
table 1.

(6) Sorting question: Sort the following attributes according to
their relevance for choosing a suitable (crypto-)API for your
last project in descending order. Drack-and-drop list with
the attributes from table 1.

(7) Do you have any Remarks, Feedback or suggestions concern-

ing the questions or survey?

𝑐𝑟 𝑦𝑝𝑡𝑜𝑙𝑖𝑏 index

D STRUCTURE OF ATTRIBUTES

Table 5: Hierarchical structure of attributes obtained from literature and interviews. The notation “I” represents the source of
an interview with the number of the mention in parentheses. Otherwise, the source is given from the literature.

ID
1
1.1
1.2
1.2.1
1.3
1.4
1.5
1.6
1.6.1
1.7
1.7.1
1.7.1.1
1.7.1.2
1.7.1.3
1.7.1.4
1.7.2
1.7.3
1.7.3.1
1.7.4
1.8
1.8.1
1.9
1.10
1.11
1.12
1.12.1
1.12.2
1.13
1.13.1
1.14
1.15
2
2.1
2.2
2.3
3
3.1
3.1.1
3.2
3.3
3.3.1
3.3.2
3.4
4
4.1
4.2
5
5.1
5.2
5.3
6
6.1
6.2

Level 1
Ease of Use ([20], I:3)

Level 2

Level 3

Level 4

Flexibility and efficiency of Use ([15], I:1)
Integrierbarkeit ([2], I:3)

Role Expressiveness ([4])

Reusability ([2])
Combinability (I:1)
Domain Correspondence ([4])
Compatibility ([20])

Substitutibility ([2])

Guidance ([1])

Conceptual Correctness ([7])

Self-Describing Functions ([2])
Recognition ([15])
Naming Consistency ([2], I:2)
Match between system and real world ([15])

Error Prevention ([15])
Predictable ([2])

Safe and Secure Defaults ([1])

Caller’s Perspective ([7])

Readability ([2])

Naming ([7])

Learning curve (I:2)
Penetrability ([4])
Learning Style ([4])
Work-Step Unit ([4])

Leftovers for Client Code ([7])
Short Chain of References ([7])

Premature Commitment ([4])

API Elaboration ([4])

Consistency, standards and Conventions ([2, 4, 7, 10, 15])
User Control and Freedom ([15])

Scalability (I:2)

Synchrony ([20])
Handleable Datasize (I:2)
Concurrency ([7])

Testability ([20], I:1)

Error handling and visibility of system status ([7, 10, 15])

Error Checking and Responsiveness ([20])

Progressive evaluation ([4])
Error-Reporting ([2])

Exception Indication ([2])
Helpful error messaging and logging ([15])

Debbuging ([10], I:1)

Extendibility ([2], I:1)

API Evolvability ([20])
API Viscosity ([4])

Functional Completeness ([20])
Features ([1])
Purposefulness/Light Footprint ([2, 2])
Functionality ([2])

Data Types ([7])

Method Parameters and Return Types ([2, 7, 10])
Consistent Parameter Ordering (I:1)

Continue on next page

Table 5: Continued: Hierarchical structure of attributes obtained from the literature and interviews.

Wohlwender and Huesmann, et al.

ID
7
7.1
7.2
8
8.1
9
9.1
10
10.1
10.1.1
10.2
10.3
10.4
10.5
10.6
10.6.1
11
11.1
12
12.1
12.1.1
12.2
12.3
12.3.1
13
13.1
14
14.1
14.2
14.3
15
15.1
15.2

Level 1
Level 2
Code Quality (I:1)

Level 3

Level 4

Cost (I:7)

Standards (I:4)
Patterns/interfaces ([7])

Copyright/License (I:4)

Requirements (I:3)

Dependency (I:2)

Complexity ([7, 20], I:2)

Aesthetic and minimalist design ([15])

Information Hiding ([2])

Working Framework ([4])
Abstraction Level ([4])
Atomic Setting ([20])
Boilerplate Code ([2])
Simplicity ([1, 2], I:1)

Single way to do one thing ([7])

Maintained (I:6)

Support (I:2)

Spread (I:16)

Community (I:2)

Stackoverflow (I:2)

Reputation(I:3)
Recommendations(I:1)

Success Stories(I:4)

Performance Impact ([2])
Latency ([20])

Security ([20])

Security (I:1)
Data Governance (I:1)
Used process (I:2)
Documentation ([1, 2, 7, 10, 15], I:14)
Best Practices (I:1)
Examples (I:4)

