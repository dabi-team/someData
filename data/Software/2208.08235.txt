Input Repair via Synthesis and Lightweight
Error Feedback

Lukas Kirschner
Saarland University, Germany
s8lukirs@stud.uni-saarland.de

Rahul Gopinath
University of Sydney, Australia
rahul.gopinath@sydney.edu.au

Ezekiel Soremekun
University of Luxembourg, Luxembourg
ezekiel.soremekun@uni.lu

Andreas Zeller
CISPA Helmholtz Center for Information Security, Germany
zeller@cispa.de

2
2
0
2

g
u
A
7
1

]
E
S
.
s
c
[

1
v
5
3
2
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract— Oftentimes, input data may ostensibly conform to
a given input format, but cannot be parsed by a conforming
program, for instance, due to human error or data corruption.
In such cases, a data engineer is tasked with input repair, i.e.,
she has to manually repair the corrupt data such that it follows
a given format, and hence can be processed by the conforming
program. Such manual repair can be time-consuming and error-
prone. In particular, input repair is challenging without an input
speciﬁcation (e.g., input grammar) or program analysis.

In this work, we show that incorporating lightweight failure
feedback (e.g., input incompleteness) to parsers is sufﬁcient to
repair any corrupt input data with maximal closeness to the
semantics of the input data. We propose an approach (called
FSYNTH) that leverages lightweight error-feedback and input
synthesis to repair invalid inputs. FSYNTH is grammar-agnostic
and it does not require program analysis. Given a conforming
program, and any invalid input, FSYNTH provides a set of repairs
prioritized by the distance of the repair from the original input.
We evaluate FSYNTH on 806 (real-world) invalid inputs using
four well-known input formats, namely INI, TinyC, SExp, and
cJSON. In our evaluation, we found that FSYNTH recovers
91% of valid input data. FSYNTH is also highly effective and
efﬁcient in input repair: It repairs 77% of invalid inputs within
four minutes. It is up to 35% more effective than DDMax,
the previously best-known approach. Overall, our approach
addresses several limitations of DDMax, both in terms of what
it can repair, as well as in terms of the set of repairs offered.

I. INTRODUCTION

Input data is prone to unintended errors. Such errors may
be introduced when the data is being created (by humans
or by buggy programs), modiﬁed (by external actors) or
transmitted (via ﬂawed networks) [1]. For instance, several
leading to invalid inputs [2].
inputs are created by hand,
Invalid input data may also be caused by disagreements
among data sources on the format speciﬁcation which leads
to different implementations of the speciﬁcation. For example,
JSON libraries implement slightly different deﬁnitions of the
JSON formats [3], [4], different database systems support
slightly different SQL formats [5], and various C compilers
provide slightly different interpretation of the C language.
These problems lead to invalid inputs that cannot be processed
by their conforming programs or consumed by end-users (e.g.,
developers).

Given such invalid inputs that are almost but not quite
parsable, developers are saddled with the task of input repair.
Input repair is particularly important due to the high prevalence
of invalid inputs in software practice [6]. It can be challenging
to recover the valid portion of invalid inputs automatically [7],
and developers often have to manually repair such inputs [6],
which is time-consuming and error-prone.

Given a formal grammar for inputs, grammar-based input
repair approaches such as error-correcting parsers [8]–[10],
can repair invalid inputs. The main idea of such parsers is
to generate a universal grammar that captures any mutation of
the base grammar. Any parse of the input string that employs
a mutation is penalized, and the parse with minimal mutation
is chosen as the best parse, and the corresponding mutation
the best repair. The limitation of this approach is that
it
assumes the existence of a formal grammar in the ﬁrst place,
which completely captures the intended structure. However,
this assumption may not hold in practice due to several
reasons. Firstly, certain input formats (e.g., URL standard from
WHATWG [11]) may not have an ofﬁcial formal grammar
speciﬁcation. Even when the base grammar is context-free
the serializer and deserializer may implement common subsets
beyond the base grammar (e.g., comments and unquoted keys
in JSON) which may contain important information. Hence,
grammar-based input repair approaches are suboptimal for
ﬁxing invalid inputs in practice.

Black-box and language-agnostic input repair methods (e.g.,
lexical DDMax [6]) repair invalid inputs without a base
grammar or program analysis. DDMax works similar to Delta
Debugging [12], but in reverse. The idea is that given a corrupt
input which induces a parse error, and a way to decompose
the input into independent fragments (called deltas (δ)), one
can successively minimize the parse-error inducing part of
the input resulting in a maximal parsing input. Although
DDMax works very well for most inputs, we observe it has
certain limitations which inhibit it from completely repairing
any invalid inputs. Table I and Section II illustrate these
limitations with simpliﬁed invalid inputs. Notably, two major
DDMax limitations include its (1) limited repair operation
(only deletion), and (2) inability to completely repair rich input

 
 
 
 
 
 
structures (e.g., multiple faults) due to inherent assumptions.
Our approach addresses these limitations of DDMax, which
we now discuss in detail.

Firstly, DDMax’s repair is restricted to deletion, which
makes it sub-optimal for repairing some invalid inputs. While
DDMax can repair invalid inputs due to spurious insertions, it
fails to repair input invalidity that is due to changed or deleted
fragments. Consider the sample invalid input in row one of
Table I (i.e., { "name": "Dave" "age": 42 })
which is invalid because of a missing comma separator
between the two values in the JSON object. Due to the limited
repair options of DDMax (i.e., only deletion), its resulting
42 ) is sub-optimal leading to a huge
repair (i.e.,
(75%) data loss (21 out of 28 bytes). This work addresses
this challenge by supporting the synthesis of missing input
fragments.

Secondly, DDMax is modeled after DDMin, where one
of the unstated assumptions is that the δ fragments that are
not contributing to the failure observed can be independently
removed without affecting the failure observed. When this
assumption is not met, (i.e. where the inputs have a rich struc-
ture) the minimal fragment produced by DDMin can be sub-
optimal. Similarly, DDMax assumes each non-failure-inducing
fragment can be added to the passing subset without inducing
a failure. When this assumption is not met (as in the case of
inputs with a rich structure such as conforming to a grammar)
the repairs produced can be suboptimal as the examples show.
As an example, consider the invalid input in row three of
Table I (i.e., {"ABCD":[*"1,2,3,4,5,6"]*}). Due
to the multiple faults (two *s) in this input, the resulting
DDMax repair (i.e., 123456) is sub-optimal causing a 77%
data loss (20 out of 26 bytes).

This paper introduces the FSYNTH approach1 to address
the limitations of DDMax via lightweight error-feedback and
input synthesis. The key insight of this approach is to complete
semantic repair of invalid inputs using lightweight failure
feedback such as the validity, incompleteness, and incorrect-
ness checks of input fragments. Given an invalid input and
a conforming program, FSYNTH performs test experiments
of input subsets and candidate insertions to provide a set
of repairs prioritized by the distance of the repair from the
original input. Unlike DDMax, FSYNTH does not have the
implied assumption of independence of fragments. Hence, it
does not face any limitation when given invalid inputs that
should conform to a rich structure, and does not stumble when
faced with multiple faults. Finally, it provides both deletion
and synthesis as repair options making it an optimal algorithm
in the toolbox of data engineers.

For instance, consider the invalid input

in row one of
Table I (i.e., { "name": "Dave" "age": 42 })
which is invalid because of a missing comma separa-
tor between the two values in the JSON object. While
DDMax could not completely repair this input, FSYNTH
could complete a repair via input synthesis. Section III de-

1FSYNTH denotes “Feedback-driven Input SYNTHesis”

the " is deleted ﬁrst, but

insertion of characters in order.

scribes the full steps of FSYNTH for this input. Speciﬁ-
cally, FSYNTH ﬁrst ﬁnds the maximal parsable preﬁx to be
{ "name": "Dave" , and determines that the parse
boundary, i.e., the shift from incomplete to incorrect happens
at boundary { "name": "Dave" ". Next, FSYNTH
applies deletion, or
In
the resulting input
this case,
{ "name": "Dave" age": 42} does not increase
the maximum parsable preﬁx. Hence, FSYNTH next attempts
to insert a character, eventually determining that only space
characters and comma (,) can be inserted here, resulting in
an increase of the maximal parse preﬁx. Out of these two
insertion candidates, inserting comma results in the maximum
advancement of the parse boundary, resulting in the valid
repair: { "name": "Dave" ,"age": 42 }.

In this work, we show that invalid inputs with rich structure
can be repaired adequately if the input processor can provide
at least some indication of progress. That is, as with DDMax,
we require the input processor to indicate if the input is valid.
However, when given an invalid input, we require the input
processor to indicate whether the input is merely incomplete
(that is, the input is a preﬁx of a valid input) or is incorrect
(that is, no sufﬁx to this input will result in a valid input).2
We note that satisfying the requirements for FSYNTH does
not require using a formal grammar for parsing, and indeed,
there are several systems that implement handwritten parsers
that satisfy FSYNTH constraints [14].

To the best of our knowledge, FSYNTH is the ﬁrst approach
to effectively repair invalid rich inputs without an input spec-
iﬁcation or program analysis. This paper makes the following
contributions:

• Repair via Input Synthesis. FSYNTH expands the reper-
toire for input repair to deletion, insertion, and modiﬁca-
tion. Unlike DDMax, which is limited to deletion of input
fragments, FSYNTH can repair errors due to omission.
• Lightweight Failure Feedback. FSYNTH is the ﬁrst
technique to demonstrate how to use lightweight parser-
error-feedback (e.g., incomplete checks) for input repair.
• Repairing Rich Input Structures. We show that our
FSYNTH technique can repair inputs with rich structure,
including multiple faults and large spans. This is a major
limitation of the state-of-the-art DDMax.

• Empirical Evaluation. We evaluate FSYNTH using 806
(real-world) invalid inputs belonging to four well-known
input formats (e.g., cJSON and TinyC). Our evaluation
results show that FSYNTH has a high (91%) data recov-
ery rate. It is up to 35% more effective than DDMax.

this paper

The remainder of

is structured as follows:
Section II highlights the limitations of the state-of-the-art
input repair method (DDMax) and illustrates how FSYNTH
overcomes these limitations. In Section III we describe how

2This requirement is provided by most parsers. In the cases where parsers
do not provide this information, it can be obtained by external instrumentation
as demonstrated by Bj¨orn et al. [13], or by modifying the parser to provide
such failure feedback, as demonstrated in this work.

TABLE I
DDMax VS. FSYNTH: EXAMPLES SHOWING LIMITATIONS OF DDMax AND THE STRENGTHS OF FSYNTH

Example
{ " n a m e " : " D a v e " " a g e " : 4 2 }
{ " i t e m " : " A p p l e " , " p r i c e " : * * * 3 . 4 5 }
{ " A B C D " : [ * " 1 , 2 , 3 , 4 , 5 , 6 " ] * }

DDMax repair

4 2

3 . 4 5

1 2 3 4 5 6

FSYNTH repair
{ " n a m e " : " D a v e " , " a g e " : 4 2 }

{ " i t e m " : " A p p l e " , " p r i c e " : 3 . 4 5 }

{ " A B C D " : [ " 1 , 2 , 3 , 4 , 5 , 6 " ] }

DDMax limitation
Limited repair options (deletion)
Rich structure (spans)
Rich Structure (multiple-faults)

FSYNTH conducts input synthesis, and Section IV describes
the FSYNTH algorithm. We describe our experimental setup
and ﬁndings in Section V and Section VI. We address the
limitations of this work in Section VII, and discuss related
work in Section VIII. Finally, we conclude this paper with the
discussion of future work in Section IX.

II. RICH INPUT STRUCTURES
Let us illustrate the limitations of the state-of-the-art input
repair method (DDMax) and how our approach (FSYNTH)
addresses these limitations. Figure 1 provides a modiﬁed
version of the lexical DDMax algorithm presented in Kirschner
et al. [6]. A major limitation of lexical DDMax is that it results
in sub-optimal repairs when the invalid input contains rich
structures, e.g., multiple faults. In the following, we discuss
these limitations.

9) Increase granularity: 4 < 4 (cid:56)

10) The solution is ∅.

That is, DDMax is unable to optimally repair inputs of this
kind which contains multiple errors. While in this example, the
data loss that occurred may seem somewhat limited, this need
not always be the case. A similar example is given in Table I.
Here, {"ABCD":[*"1,2,3,4,5,6"]*} contains two
distinct corruptions. As in the previous case, DDMax attempts
to ﬁx this input by dividing it
into smaller and smaller
fragments, none of which isolates an error that when removed,
results in the solution 123456 with signiﬁcant data loss,
including the loss of structure and change in input fragment
type from string to number. Hence, DDMax cannot effectively
repair inputs containing multiple faults.

A. Limitations due to multiple faults

B. Effect of input decomposition

A pattern of failure of DDMax occurs when DDMax is given
an input with multiple errors. For example, consider the JSON
input [*]+. Here, the JSON string is invalid because of two
invalid characters that are non-contiguous. The operation of
DDMax (Figure 1) proceeds as follows:

1) The operation starts with DDMax2(∅, 2)
2) |c(cid:56) − ∅| (cid:54)= 1. Hence, the base case does not apply
3) Increase to complement:

c(cid:56) − ∆1= ]+ (cid:56)
c(cid:56) − ∆2= [* (cid:56)

4) Increase to subset:
∅ ∪ ∆1=[* (cid:56)
∅ ∪ ∆2= ]+ (cid:56)

Unfortunately DDMax can produce non-optimal results even
when the errors are contiguous, and hence considered single
by DDMax. The problem happens when the corruption in the
input interacts with the fragment decomposition algorithm of
DDMax. As an example, consider a variant of the previous
input: [*+]. The JSON string is invalid here because it
contains two invalid characters which are contiguous. The
operation of DDMax (Figure 1) is as follows:
1) The operation starts with DDMax2(∅, 2)
2) |c(cid:56) − ∅| (cid:54)= 1. Hence, the base case does not apply
3) Increase to complement:

c(cid:56) − ∆1= +] (cid:56)
c(cid:56) − ∆2= [* (cid:56)

5) Increase granularity: n < |c(cid:56) − c(cid:48)

(cid:52)| which is 2 < |c(cid:56) − ∅|

(cid:52)
Hence the next iteration is: DDMax2

(cid:0)∅, 4)(cid:1)

6) |c(cid:56) − ∅| (cid:54)= 1. Hence, the base case does not apply.
7) Increase to complement:

c(cid:56) − ∆1= *]+ (cid:56)
c(cid:56) − ∆2= []+ (cid:56)
c(cid:56) − ∆3= [*+ (cid:56)
c(cid:56) − ∆4= [*] (cid:56)

8) Increase to subset:
∅ ∪ ∆1=[ (cid:56)
∅ ∪ ∆2=* (cid:56)
∅ ∪ ∆3=] (cid:56)
∅ ∪ ∆4=+ (cid:56)

4) Increase to subset:
∅ ∪ ∆1=[* (cid:56)
∅ ∪ ∆2= +] (cid:56)

5) Increase granularity: n < |c(cid:56) − c(cid:48)

(cid:52)| which is 2 < |c(cid:56) − ∅|

(cid:52)
Hence the next iteration is: DDMax2

(cid:0)∅, 4)(cid:1)

6) |c(cid:56) − ∅| (cid:54)= 1. Hence, the base case does not apply.
7) Increase to complement:

c(cid:56) − ∆1= *+] (cid:56)
c(cid:56) − ∆2= [+] (cid:56)
c(cid:56) − ∆3= [*] (cid:56)
c(cid:56) − ∆4= [*+ (cid:56)

8) Increase to subset:
∅ ∪ ∆1=[ (cid:56)
∅ ∪ ∆2=* (cid:56)

Maximizing Delta Debugging Algorithm

Let test and c(cid:56) be given such that test(∅) = (cid:52) ∧ test(c(cid:56)) = (cid:56) hold.
The goal is to ﬁnd c(cid:48)
(cid:52) = DDMax(c(cid:56)) such that c(cid:48)
The maximizing Delta Debugging algorithm DDMax(c) is

(cid:52) ⊂ c(cid:56), test(c(cid:48)

(cid:52)) = (cid:52), and ∆ = c(cid:56) − c(cid:48)

(cid:52) is 1-minimal.

(cid:52)



DDMax(c(cid:56)) = DDMax2(∅, 2) where
c(cid:48)
DDMax2(c(cid:56) − ∆i, 2)
(cid:0)c(cid:48)
DDMax2
(cid:0)c(cid:48)
DDMax2
c(cid:48)




(cid:52), n) =

(cid:52)

DDMax2(c(cid:48)

(cid:52) ∪ ∆i, max(n − 1, 2)(cid:1)
(cid:52), min(|c(cid:56)−c(cid:48)

(cid:52)|, 2n)(cid:1)

if |c(cid:56) − c(cid:48)

(cid:52)| = 1 (“base casea”)

else if ∃i ∈ {1, . . . , n} · test(c(cid:56) − ∆i) = (cid:52) (“increase to complement”)
else if ∃i ∈ {1, . . . , n} · test(c(cid:48)
else if n < |c(cid:56) − c(cid:48)
otherwise (“done”).

(cid:52) ∪ ∆i) = (cid:52) (“increase to subset”)

(cid:52)| (“increase granularityb”)

(cid:52) = ∆1 ∪ ∆2 ∪ · · · ∪ ∆n, all ∆i are pairwise disjoint, and ∀∆i · |∆i| ≈ |c(cid:56) − c(cid:48)

where ∆ = c(cid:56) − c(cid:48)
The recursion invariant (and thus precondition) for DDMax2 is test(c(cid:48)
a: Bugﬁx: This base case is necessary to ensure that repairing JSON input 1*1 does not violate the invariant
n ≤ |∆|.
b: Bugﬁx: We should look for minimum of the remaining so that invariant n ≤ |∆| is not violated for JSON input
{*"":2}.

(cid:52)) = (cid:52) ∧ n ≤ |∆|.

(cid:52)|/n holds.

Fig. 1. Modiﬁed Maximizing Lexical Delta Debugging algorithm, extended from Kirschner et al. [6]

∅ ∪ ∆3=+ (cid:56)
∅ ∪ ∆4=] (cid:56)

9) Increase granularity: 4 < 4 (cid:56)

10) The solution is ∅.

That

JSON

invalid

repaired

particular

is,
cannot

by DDMax. As

string
this
be
the
also
previous case,
the data loss can be severe. Consider
{ "item": "Apple", "price": ***3.45 }
in Table I which is similar to Kirchner et al. [6, Figure 1] but
3.45,
with an extra *. DDMax repairs this input to
resulting in data loss.

in

The problem here is that the successive partitions attempted
by DDMax fails to isolate the failure causing fragment even
though the fragment is contiguous. That is, no single inde-
pendent fragment is found, the removal of which results in
removal of the error. Hence, DDMax keeps searching for
smaller and smaller fragments discarding larger and larger
chunks of data.

C. Discussion

Why does DDMax fail to repair these inputs? A major
limitation of DDMax is that it is modeled on DDMin, which
is an effective tool for minimization of failure inducing inputs.
Given a failure inducing input, the idea of DDMin is to suc-
cessively partition the input into smaller and smaller chunks,
remove one chunk at a time and check whether the remaining
chunks are sufﬁcient to reproduce the failure. As this implies, a
key assumption of DDMin is that we can actually remove such
chunks independently. That is, if a chunk does not contribute
to the observed failure, it can be removed without affecting the
failure observed. Secondly, if multiple chunks independently

cause the same failure, only one chunk will be chosen, and
minimized further.

The deﬁnition of DDMax is a mirror of DDMin. DDMax
starts with an empty input that is assumed to be passing. Then,
it partitions the input into chunks, and tries to concatenate
any of these chunks to the passing input, producing a larger
passing input. If after dividing the input into n chunks, none
of the chunks could produce a passing input, it tries again by
dividing the input into 2n chunks.

As in the case of DDMin, the unstated assumption here
is that
if a chunk was not responsible for the observed
failure, it can be extracted independently of other chunks
and added to the passing input fragment without changing
the semantics. This particular assumption need not hold when
we are dealing with inputs that have a rich structure. That
is, "1,2,3,4,5" is very different from 12345 even
though a signiﬁcant portion of the raw characters from ﬁrst
is preserved in the second. Further, once such a semantically
changed fragment forms the seed of the passing fragment,
due to the constraints in the input structure, the remaining
fragments from the original will likely not combine with the
seed fragment, resulting in further data loss.

Although DDMax will have no problems maximizing any
inputs if the input processor conforms to this constraint, we
note that this can be a rather strong constraint in practice.

D. Updates to DDMax deﬁnition

While evaluating DDMax, we noticed two cases where the
formal deﬁnition of DDMax was underspeciﬁed. These are
noted in Figure 1. Speciﬁcally, (1) DDMax requires the base
case when |c(cid:56)−c(cid:52)| = 1. If not, DDMax can go into unbounded
recursion on inputs such as the JSON input: 1*1. (2) when
increasing granularity, the size of the remaining input should
be considered rather than the size of the entire text. Not

doing this would cause an invariant fail for inputs such as
{*"":2}.

is deleted ﬁrst, resulting in:
{ "name": "Dave" age": 42}.

E. Repair of rich inputs with FSYNTH

One of the strengths of FSYNTH is that it can effectively
handle inputs with rich structure. Consider the input to the
JSON processor: {"ABCD":[*"1,2,3,4,5,6"]*}.
(For ease of explanation, let us consider only deletion as the
operation used.) Here, the procedure is as follows:

1) FSYNTH starts by executing a binary search for the
boundary where the input preﬁx changes from incomplete
to incorrect. This is obtained at index 10, providing the
incomplete substring {"ABCD":[.

2) FSYNTH then appends the next character * to the input,
resulting in {"ABCD":[* and observes the result. In
this case, the JSON processor returns incorrect.

3) Hence, the newly added character is discarded, and the
character at
index is appended, resulting in
{"ABCD":[". This results in JSON processor re-
sponding incomplete.

the next

4) FSYNTH now appends the character in the next index,
resulting in {"ABCD":[1" which again results in
incomplete from JSON processor.

5) Proceeding in this fashion the input reaches
{"ABCD":["1,2,3,4,5,6"]*
which
point, we again have the response incorrect
from
discard
the
this
resulting in
character, and try the next character,
{"ABCD":["1,2,3,4,5,6"]}.
6) The JSON processor responds with complete.

JSON processor. Hence, we

at

This completes the repair of the given input. This demonstrates
that FSYNTH has no problem repairing rich inputs containing
multiple errors.

III. INPUT SYNTHESIS

3) The JSON parser responds with incorrect for this input.
4) FSYNTH next attempts to insert a character. Say we tried

to insert 1. This results in:
{ "name": "Dave" 1"age": 42}.

5) The JSON parser responds with incorrect for this input.
6) Indeed, only space characters and comma (,) can be
inserted here, resulting in incomplete from the JSON
parser.

7) Inserting the , results in a new input:

{ "name": "Dave" ,"age": 42 }

8) This is accepted as a valid repair.
That is, the ability of FSYNTH to synthesize characters for

repair can lead to more effective repairs.

IV. FSYNTH

The main strength of FSYNTH is its repertoire of repairs.
The input string can be modiﬁed by deleting any character
(Listing 1), or inserting a character at any index (Listing 1).
In the listings below, Repair is a tuple that allows accessing
its ﬁrst argument with inputstr and its second argument
with boundary.

We deﬁne a few terms before we start:

valid substring. The valid substring of a string is the maxi-

mal preﬁx where the parser returns incomplete.

boundary. The boundary or parse boundary is the index of
the ﬁrst character after the valid substring that results in
incorrect response from the parser, or one past the end if
the input is incomplete.

repair. A repair is a single modiﬁcation (deletion or insertion

of a single character) made on an input string.

repair thread. A repair thread is is a set of repairs made on
an input string. A repair thread has a single boundary and
a corresponding valid preﬁx.

input

The second major limitation of DDMax is that the only
fragments.

operation in its toolbox is deletion of
Consider
{ "name": "Dave" "age": 42 } Here, there is
a missing quote in the key. DDMax repair of this string will
42. The problem is that, deletion of fragments
result in
alone can lead to signiﬁcant corruption of information. In this
instance, the availability of insertion could have repaired the
input string to
{ "name": "Dave", "age": 42 }.
Unfortunately, because DDMax is unable to synthesize
any framents, opportunities for repair can be missed.

1

2

3

4

The FSYNTH algorithm on the other hand, follows in this

fashion.

1) FSYNTH algorithm starts with the corrupted input and
quickly ﬁnds the maximal parsable preﬁx using a binary
search:
{ "name": "Dave" .

2) At this point, FSYNTH applies deletion, insertion, or
modiﬁcation of characters in order. In this case, the "

is observed,

the character
the boundary value. That

The deletion algorithm is simple. When a parse er-
is
ror
present at
is, we know that
inputstr[:boundary] parses correctly. Hence, for dele-
the
tion, we produce a string without
boundary.

that caused the error

the character at

def apply_delete(item):

Listing 1. FSYNTH repairs

inp = item.inputstr[:item.boundary] +

item.inputstr[item.boundary + 1:]

return extend_deleted_item(Repair(inp, item.boundary))

For insertion, the algorithm is more involved. The prob-
lem is that we need to handle corruption in inputs such as
"12345mystring" which is used as input to a JSON parser.
Consider what happens when the ﬁrst quote is deleted. In
this case, we will get the ﬁrst parse error at m. But plainly,
the best repair is before the ﬁrst parse error. The problem is
that insertions at random points in the preﬁx is very costly,
resulting in |α| × |S| modiﬁcations where |α| is the number of
alphabets (characters) the language has, and |S| is the length

of the preﬁx. Hence, we allow the user to choose to toggle
this option with LAST_INSERT_ONLY. If the option is false,
we will attempt repairs at any point in the preﬁx. If it is true,
we will only attempt insertions at the end of the preﬁx.

2

3

4

5

6

def insert_at(item, k, i, suffix)

v = (item.inputstr[:k] + i +

Listing 2. FSYNTH repairs

item.inputstr[k:item.boundary] + suffix)

7

8

9

10

new_item = Repair(v, k, mask=’%s_I%d’ % (item.mask, k))
ie = extend_inserted_item(new_item)
if ie.boundary > k:

return ie

return None

def insert_char(item, i):

suffix = item.inputstr[item.boundary:]
return_lst = []
if LAST_INSERT_ONLY:

v = insert_at(item, item.boundary, i, suffix)
if v is not None: return_lst.append(v)

else:

for k in range(item.boundary):

v = insert_at(item, k, i, suffix)
if v is not None: return_lst.append(v)

return return_lst

def apply_insert(item):
new_items = []
for i in CHARACTERS:

items = item.insert_char(i)
if items:

new_items.extend(items)

return new_items

Listing 3 shows how FSYNTH is invoked. The input string

is passed to repair().

Listing 3. FSYNTH initial search

def repair(inp):

boundary = binary_search(inp)
return find_fixes(inp, boundary)

return value

This function does a binary search (Listing 4) on the
the parse boundary where
argument string looking for
to incor-
the
if binary_search returns an index n,
rect. That
then inputstr[:boundary] is a valid preﬁx, and
inputstr[boundary] is the error causing character if one
exists or the inputstr is incomplete.

from incomplete

changes

is,

1

2

3

4

next_items = [Repair(inputval, boundary)]
while True:

current_items, next_items = next_items, []
completed = []
for item in sample_items_by_mask(current_items)

for i in repair_and_extend(item)

next_items.append(i)
if i.is_complete(): completed.append(i)

if completed: return completed

The function find_fixes() takes in the input string and
the parse boundary. It then generates a set of repair threads,
out of which a few are chosen for continuation (Listing 6).
Then, each repair thread is processed individually. On each
thread, a set of repairs (Listing 1, Listing 2) are applied. If
the repair succeeds, the string can use more characters from
the pending sufﬁx resulting in a larger valid preﬁx string, but
with a larger edit distance. This procedure is repeated until
the preﬁx string is marked complete by the input processor.

The sampling procedure attempts to discard redundant re-
pairs so that the number of simultaneous threads we have
to maintain does not grow unbounded. While doing that, it
ensures that no unique repairs are discarded. For example,
given a JSON fragment [1,2 a repair of [1,23, [1,24
and [1,25 are redunant but [1,2, is unique because the
last repair represents a change in semantics for the parser. The
key insight here is to look at the extension of a given string
to classify whether it is unique or not. That is, given a string
[1,2"x"], after repair of [1,23, the next extension of the
string is likely to be a comma or a digit insertion. However,
after [1,2, the pending sufﬁx can be used to extend the
string resulting in [1,2,"x"]. Hence, we use the kind of
repairs conducted on a string, the length of the preﬁx, as well
as the last character added as the uniqueness indicator.

Unfortunately, using all repairs even after eliminating all
redundant repairs can be rather time consuming. If this
is the case, one can limit
the repair threads to the best
performing ones in terms of the parse boundary by using
filter_best().

def sample_items_by_mask(items):

Listing 6. Sampling

masks = {}
for i in items:

def binary_search(inputstr, left=0, right=len(inputstr)-1):

Listing 4. binary search

5

6

7

if not inputstr: return left
if is_incomplete(Repair(inputstr, right)):

return len(inputstr)-1

while left + 1 < right:

middle = (left + right) // 2
if is_incomplete(Repair(inputstr, middle)):

left = middle

else:

right = middle

return left

8

9

10

11

12

13

14

15

16

17

18

The boundary value is then passed to find_fixes()
(Listing 5).

19

20

21

1

def find_fixes(inputval, boundary):

Listing 5. ﬁnd ﬁxes

key = (i.mask, i.boundary, i.inputstr[i.boundary-1])
if i.mask not in masks: masks[key] = []
masks[key].append(i)

sampled = []
for key in masks:

if len(masks[key]) < MAX_NUM_PER_MASK:

res = masks[key]

else:

res = random.sample(masks[key], MAX_NUM_PER_MASK)

sampled.extend(res)
return filter_best(sampled)

def filter_best(items):

if MAX_SIMULTANIOUS_CORRECTIONS < 0: return items
boundaries = unique_sorted_boundaries(items)
return [i for i in items if i.boundary in

boundaries[:MAX_SIMULTANIOUS_CORRECTIONS]]

After each repair, the new string is checked to see if the
string results in incomplete rather than incorrect. If the string

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

1

2

3

1

2

3

4

5

6

7

8

9

10

11

Find parse
boundary
(Search)

Apply
Repairs

1

2

Select Threads
4

Extend Threads Valid Input?

3

(cid:52)

Return input

5

Fig. 2. Work ﬂow of FSYNTH

results in incomplete, a new parse boundary is found by
repeatedly extending the string with a pending character from
the remaining sufﬁx in the input string. The small complication
here is that different search algorithms are more suitable for
deletion and insertion. With deletion, we have removed the
error causing character, so the next parse error may be far
away. Hence, we use binary search to ﬁnd the next parse error
(Listing 7).

1

2

Listing 7. Extend the boundary

def extend_deleted_item(item):

return bsearch_extend_item(item)

def bsearch_extend_item(item):

bs = binary_search(item.inputstr, left=item.boundary)
if bs >= len(item.inputstr):

item.boundary = bs
return item
item.boundary = bs
return item

In the case of insert, however, in most cases, the character
being inserted, and the character that originally caused a parse
error can still cause a parse error. Hence, we apply linear
search instead (Listing 8).

Listing 8. Extend the boundary

def extend_inserted_item(item):

return lsearch_extend_item(item, nxt=1)

def lsearch_extend_item(item, nxt=1):

while True:

if (item.boundary + nxt) > len(item.inputstr):
item.boundary = item.boundary + nxt - 1
return item

s = Repair(item.inputstr, item.boundary + nxt)
if is_incomplete(s):

nxt += 1
continue

if is_incorrect(s):

item.boundary = item.boundary + nxt - 1
return item

If this succeeds, the parse boundary after the repair would
be larger than the old parse boundary. Hence, if the parse
boundary has increased (Listing 9), then the repair is saved.
If not, the repair is discarded.

Listing 9. Repair and extend

TABLE II
SUBJECT PROGRAMS USED IN THE EVALUATION

Name
INI
cJSON
SExpParser
TinyC

LOC
382
3062
656
375

Lang.
C
C
C
C

1st Commit
Jul 2009
Aug 2009
Sep 2016
2001

Last Commit
Jan 2022
Jan 2022
Sep 2016
Apr 2018

TABLE III
DETAILS OF INVALID INPUTS

Number of Invalid Inputs

Type of Invalid Inputs
Real-World Inputs
Single Mutation
Multiple Mutations
Total
(806)

INI
101
50
50
201

cJSON
107
50
50
207

SExp
50
50
50
150

TinyC
148
50
50
248

def repair_and_extend(item):

return [apply_delete(item)] + apply_insert(item)

The output from find_fixes() (Listing 5) is a set of repair
threads with the least number of repairs from the passed input
string.

FSYNTH assumes the parser correctly signals incomplete
for incomplete inputs, incorrect for other invalid inputs, and
complete if the input was valid. Given this, FSYNTH algorithm
works as follows (Figure 2):

1) Binary search. Given any corrupt input, FSYNTH starts
by a binary search of the input to determine the parse
boundary. This then is used to construct the ﬁrst repair
thread, with boundary set to the binary search result, and
repairs set to empty.

2) Repair. Starting with any existing repair thread, FSYNTH
applies deletion and insertion repairs. A single deletion
results in a single repair thread which is an extenion of
the original repair thread. An insertion however, results
in multiple repair threads corresponding to the number of
characters.

3) Extention. For each thread that results from repair, extend
the thread until
the new parse boundary. For threads
resulting from insertion, keep only those threads that
results in a diffrent parse boundary from the old.

4) Selection. We remove any redundant repair threads, and
choose the best threads if a selection criterion is supplied.
5) Final Fix. We then iteratively choose each thread and
continue making repairs until the parser output at the
boundary changes from incomplete to complete.

The output of the algorithm is a set of repair threads each of
which ﬁxes the input using a set of repairs. The repair threads
are sorted in the order of least number of repairs required to
ﬁx the input.

V. EXPERIMENTAL SETUP

This section describes the experimental setup of this work.
Research Questions: We investigate the data recovery (RQ1),
effectiveness (RQ2), and efﬁciency (RQ3) of our approach us-
ing several well-designed experiments. We also examine how

1

2

3

4

5

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

TABLE IV
DATA RECOVERY (FILE SIZE DIFFERENCE) AND DATA LOSS (LEVENSHTEIN DISTANCE) OF FSYNTH VS. LEXICAL (LEX.) DDMax AND SYNTACTIC
(SYN.) DDMax. THE HIGHEST DATA RECOVERY AND LOWEST DATA LOSS ARE IN BOLD, PERCENTAGE IMPROVEMENT (IMPR.) OF FSYNTH OVER THE
BEST BASELINE WHICH ARE SIGNIFICANT (I.E., GREATER THAN FIVE PERCENT (>5%)) ARE ALSO IN BOLD.

Techniques
Lex. ddmax
Syn. ddmax
FSYNTH
Impr. vs. Lex.
Impr. vs. Syn.

ALL
Average
81%
74%
91%
12%
23%

% Data Recovered by Approach

Input Format

SExp
99.2%
97.0%

cJSON
82.2%
81.5%

TinyC
INI
47.7%
85.8%
70.6%
47.0%
85.9% 98.5% 100.6% 82.1%
72.1%
1.4%
3.7% 74.7%

19.8%
21.6% 20.8%

0.1%

Average Data Loss

ALL
INI
Average
10.2
18.0
258.1
119.6
9.5
10.6
70%
8%
1030% 2629%

cJSON
61.6
82.7
28.7

Input Format
TinyC
SExp
6.2
7.5
33.3
76.6
1.6
7.4
115% -16%
357%
188% 936% 1930%

is

(runtime)

efﬁciency

FSYNTH compares to four state-of-the-art techniques, namely
the built-in repair of the programs (baseline), error-recovery
of ANTLR, as well as lexical DDMax and syntactic DDMax.
Speciﬁcally, we pose the following research questions:
RQ1: Data Recovery and Data Loss. How much input data
is recovered by FSYNTH, and how much data is lost? Does
FSYNTH recover as much data as the state-of-the-art methods?
RQ2: Effectiveness. How effective is FSYNTH in ﬁxing in-
valid inputs? Is it as effective as the state-of-the-art methods?
RQ3: Efﬁciency. What
of
the
FSYNTH? Is it as efﬁcient as the state-of-the-art techniques?
Subject Programs: We used four input formats and their cor-
responding programs. These are INI (INI), JSON (cJSON), S-
Expressions (SExpParser) and TinyC (TinyC). Each program is
moderately large (between 375 LOC to 3062 LOC), relatively
mature (6 to 21 years old), and written in C. Further details
are provided in Table II.
Test inputs: Table III provides details of the number of real-
world invalid inputs and mutated invalid inputs employed in
our experiment. We evaluate our approach using 806 invalid
input ﬁles. As test inputs, we crawled a large corpus of valid
and invalid real-world ﬁles from GitHub using the GitHub
crawling API [15]. In addition to real invalid inputs, for each
format, we introduced a set of 100 artiﬁcially mutated (invalid)
ﬁles from 50 randomly selected valid real-world ﬁles. Half
of those mutated ﬁles contain a single mutated byte and the
other half contains multiple (two to 16) mutated bytes. Each
mutation can either be a byte-ﬂip, an insertion or a deletion to
resemble real-world corruptions as close as possible (e.g., bit
rot on hard disks or transmission errors in network protocols).
Metrics and Measures: We employ the following metrics and
measures to evaluate repair quality:
a.) Number of Repaired Inputs: We count the number of ﬁles
repaired before a four minute timeout for each repair method.
b.) File Size Difference: To determine the amount of data
recovered by each approach, we evaluate the difference in ﬁle
size of the recovered inputs and the original valid input.
c.) Edit Distance: This is measured as the number of characters
that differs between the corrupt input and the repaired input.
d.) Runtime is the time taken for input repair for each method.
e.) Number of Program Runs: To evaluate efﬁciency, we track
the number of times the parser is executed by each approach.
State-of-the-art: In this work, we compare the performance
of FSYNTH to the following techniques: (1) Baseline: The

built-in error-recovery technique of the subject programs.
(2) ANTLR: This is the inbuilt error recovery strategy of
the ANTLR parser generator when equipped with an input
grammar specifying the allowed input structure [16, Automatic
Error Recovery Strategy]. (3) DDMax: Kirschner et al. [6]
proposed two variants of the maximizing variant of the delta
debugging algorithm (called DDMax), namely lexical DDMax
and syntactic DDMax.
Implementation Details: We implemented the test infras-
tructure in about 12k LOC of Java code, FSYNTH was
implemented in 765 lines of Java code. We also slightly mod-
ify subject programs to provide the required incompleteness
feedback (see Table II).
Platform: All experiments were conducted on an ASRock
X470D4U with six physical CPU cores and 32GB of RAM,
with an AMD Ryzen 5600X @ 3.70GHz, 12 virtual cores,
running Debian GNU/Linux.
Research Protocol: We ﬁrst collect a large corpus of real-
world ﬁles which we split into a set of (50) valid ﬁles and
invalid ﬁles. We create additional mutated invalid ﬁles by
injecting single and up to 16 multiple mutations (random
insertions, deletions and byte-ﬂips) into the valid ﬁles (Ta-
ble III). Then, we run each repair technique on each ﬁle
and collect the required data, i.e. the run time, number of
oracle runs, Levenshtein distance and repair status of each ﬁle.
As repair techniques, we employ Baseline, ANTLR, lexical
DDMax, syntactic DDMax and FSYNTH. All experiments
were conducted within a timeout of four minutes per input,
for each repair technique. The maximum time budget was
empirically determined in our preliminary experiments to
ensure a balanced evaluation for all techniques. We found
that four minutes is a sufﬁcient time budget to evaluate all
techniques on most inputs. It is also a reasonable maximum
repair time for an end-user. In our experiments, a longer
timeout did not result in a signiﬁcant increase in repairs for
all techniques.

VI. EXPERIMENTAL RESULTS

This section discusses the results of our experiments.

RQ1 Data Recovery and Data Loss: Let us investigate the
amount of data recovered and lost by FSYNTH), in comparison
to the best-performing state-of-the-art method (DDMax).
Data Recovery: We examine 474 inputs that were completely
repaired by lexical DDMax, syntactic DDMax and FSYNTH,

excluding empty ﬁles (and white spaces). Table IV highlights
the amount of data recovered and lost by each approach.3

Results show that FSYNTH has a very high data recovery
rate. It recovered 91% of input data, on average. This is
23% more than the most effective baseline (syntactic DDMax).
For all input formats, FSYNTH recovered up to (75%) more
data than both variants of DDMax. Consider TinyC, where
FSYNTH recovered up to 75% more input data than DDMax
(see Table IV). These results show that FSYNTH is more
effective in recovering valid input data than DDMax.
Data Loss: To measure data loss, we compute the Levenshtein
distance between repaired and invalid ﬁles using 327 com-
pletely repaired inputs where edit distance could be computed
within a threshold of 750 edit distances (about 30 seconds).

We found that FSYNTH achieves a low data loss of about
11 edit distances, on avarage, which is up to 10 times lower
than (syntactic) DDMax. Table IV shows that for almost
all input formats, FSYNTH achieved a lower data loss than
syntactic and lexical DDMax (except for SExp). This better
performance is attributed to the lightweight failure feedback
of FSYNTH, which allows it to distinguish between incomplete
and incorrect input fragments.

FSYNTH has a high data recovery rate (91%, on average)
and its data loss is up to 26 times lower than that of
ddmax (e.g., INI).

RQ2 Effectiveness: In this experiment, we measure the to-
tal number of ﬁles repaired by our approach. In addition,
we compare the performance of FSYNTH to four state-of-
the-art methods containing both language-agnostic input re-
pair approaches and grammar-based input repair techniques.
Language-agnostic input repair approaches include the built-
in error-recovery of the subject program (called baseline) and
lexical ddmax (see Table V). For comparison to grammar-
based input repair methods, we employ the built-in error-
recovery strategy of the ANTLR parser generator, and syntactic
ddmax (see Table VI). Figure 3 also highlights the effective-
ness of FSYNTH in contrast to the state-of-the-art methods.
Repair Effectiveness: Our approach is very effective in re-
pairing invalid inputs, it repaired almost four out of every
ﬁve invalid input withing a four minute timeout, i.e., about
77% (624 out of 806) of invalid inputs. In comparison to
language-agnostic approaches, FSYNTH is up to 18 times
more effective than the built-in error recovery strategy of the
subject programs (33 vs. 624 repairs), and up to 35% more
effective than the best performing language-agnostic approach
for certain input formats (cJSON and TinyC). Overall, Table V
shows that FSYNTH is eight percent more effective than the
best language-agnostic repair technique (i.e., lexical DDMax).
Additionally,
results show that FSYNTH outperforms the
grammar-based input repair approaches by up to 28%. Despite
zero knowledge of the input format, Table VI shows that

3Note that due to input synthesis (i.e., insertion of input elements), FSYNTH
may report recovering more than 100% of the input ﬁle (e.g., 100.6% for SExp
in Table IV).

TABLE V
NUMBER OF REPAIRED INPUTS WITHIN A FOUR MINUTE TIMEOUT FOR
FSYNTH VS. LANGUAGE-AGNOSTIC STATE-OF-THE-ART, I.E., LEXICAL
DDMax AND BASELINE. THE HIGHEST NUMBER OF REPAIRED INPUTS ARE
IN BOLD, AS WELL AS SIGNIFICANT PERCENTAGE IMPROVEMENT
(IMPROV.) OF FSYNTH OVER THE BEST BASELINE GREATER THAN FIVE
PERCENT (>5%).

Techniques
Baseline
Lex. ddmax
FSYNTH
Improv.

ALL
Total (%)
33 (4%)
578 (72%)
624 (77%)
8%

Number of Repaired Inputs
Input Format
SExp
cJSON
0
6
131
132
158
121
20% −8%

INI
27
201
191
−5%

TinyC
0
114
154
35%

TABLE VI
NUMBER OF REPAIRED INVALID INPUTS WITHIN A FOUR MINUTE
TIMEOUT FOR FSYNTH VS. GRAMMAR-BASED INPUT REPAIR
APPROACHES, I.E., ANTLR AND SYNTACTIC DDMax. THE HIGHEST
NUMBER OF REPAIRED INPUTS ARE IN BOLD, AS WELL AS SIGNIFICANT
PERCENTAGE IMPROVEMENT (IMPROV.) OF FSYNTH OVER THE BEST
BASELINE GREATER THAN FIVE PERCENT (>5%).

Techniques
ANTLR
Syn. ddmax
FSYNTH
Improv.

ALL
Total (%)
344 (43%)
614 (76%)
624 (77%)
2%

Number of Repaired Inputs
Input Format
SExp
cJSON
96
69
149
144
158
121
10% −19%

INI
133
201
191
−5%

TinyC
46
120
154
28%

our approach outperformed the grammar-based input repair
approaches: FSYNTH is almost twice as effective as the error-
recovery strategy of ANTLR (see Figure 3), and slightly (2%)
more effective than the best-performing grammar-based ap-
proach (i.e., syntactic DDMax). The effectiveness of syntactic
DDMax is due to its knowledge of the input grammar. These
results suggest that the combination of failure feedback and
input synthesis is vital for the effective repair of invalid inputs.

FSYNTH is very effective in repairing invalid input ﬁles: It
repaired four out of ﬁve (77% of) invalid inputs and it is
8% more effective than the best language-agnostic method
(lexical DDMax).

Complementarity to DDMax: We further inspect the unique
repairs achieved by each approach to understand the com-
plementarity of our approach to the state-of-the-art methods.
In this experiment, we inspect the number of unique repairs
achieved solely by a single approach (e.g., only FSYNTH),
or two or more approaches (e.g., all approaches). Figure 4
highlights our ﬁndings.

We found that about one in ten repairs could be solely
completed by FSYNTH, which is three times and ﬁve times
as many as lexical and syntactic DDMax repairs, respectively
(see Figure 4). FSYNTH solely completed 9% (61/712) of all
repaired inputs, all of which DDMax could not repair. For
lexical DDMax, our approach solely repaired 5× as many
invalid ﬁles as lexical DDMax (61 vs. 12), while it repairs
thrice as many ﬁles as syntactic DDMax (DDmaxG) (61 vs.
23). Overall, we observe that FSYNTH complements DDMax,

614

624

578

TABLE VII
EFFICIENCY OF FSYNTH VS. DDMax, LOWEST RUNTIME AND SMALLER
NUMBER OF PROGRAM RUNS ARE IN BOLD.

TinyC
SExpParser
cJSON
INI

344

600

400

200

0

33

Baseline

ANTLR Lex.ddmax Syn.ddmax

FSynth

Fig. 3. Number of ﬁles repaired by each approach

FSYNTH

61

25

50

12

488

53

23

DDMax DDmaxG
Fig. 4. Venn Diagram showing the number of invalid inputs repaired (solely)
by a (combinations of) technique(s)

a combination of these approaches completes 88% of repairs
(712), which is 14% and 23% more than the repairs completed
solely by FSYNTH or syntactic DDMax, respectively. On
inspection, we found that the types of repairs completed by
FSYNTH (but not by DDMax) were mostly insertions. We
found that the insertions performed by FSYNTH were mostly
(missing) syntactic elements of the input like curly braces
and colons for JSON or line breaks for INI. In some cases,
FSYNTH inserted alphanumeric characters. For instance, if a
grammar expects a char where there is a missing char.
Overall, we attribute the unique repair achieved by FSYNTH
to the synergistic combination of failure feedback and input
synthesis.

FSYNTH is complementary to the state-of-the-art methods
(lexical and syntactic DDMax). It solely completes 9% of
all repairs, which is up to ﬁve times as much as DDMax.

RQ3 Efﬁciency: Let us evaluate the time performance of
our approach. For a balanced evaluation, we analyse a set
of 480 invalid inputs that were completely repaired by all
three approaches within the four minutes timeout, without data
collection and experimental analysis time. Table VII reports
the efﬁciency of all three approaches.
FSYNTH is considerably efﬁcient

in input repair: It

is

Techniques
Lex. ddmax
Syn. ddmax
FSYNTH

Runtime (s)
Total
Avg.
1653
3.4
982
2.0
4938
10.2

#Prog. Runs
Avg.
3260
2075
11537

Total
1564781
995802
5537601

reasonably fast, it takes about 10 seconds to repair an invalid
input, on average. However, it is about ﬁve times slower than
DDMax (two to three seconds, on average). Table VII shows
that the execution time of FSYNTH is much higher than that
of syntactic and lexical DDMax. This result is due to the
number of program runs required by FSYNTH, especially due
to its additional repair operation (input synthesis) and its extra
oracle checks (incompleteness and parser boundary). FSYNTH
is reasonably efﬁcient, similar to DDMax,
the number of
program runs is its main performance bottleneck.

FSYNTH is reasonably fast (10 seconds), but it is 5×
slower than DDMax because it requires additional
operations and oracle checks.

VII. LIMITATIONS AND THREATS TO VALIDITY

Our approach (FSYNTH) and empirical evaluations may be

limited by the following:
External Validity: This refers to the generalizability of our
approach, i.e., the threat that FSYNTH may not generalize to
other programs and input formats. To mitigate this threat, we
have employed four well-known input formats with varying
complexity, their corresponding programs also have varying
sizes (375 to 3062 LOC) and maturity (6 to 21 years old).
Internal Validity: This concerns the correctness of our im-
plementation and evaluation, especially if we have correctly
implemented FSYNTH and the baselines. We mitigate this
threat by testing our implementation of all approaches on small
and simple test inputs to ensure the they behave as described.
Construct Validity: This concerns the test oracle and failure
feedback employed in our evaluation. To ensure all subjects
provide the expected incomplete and (in)correct feedback, we
tested the programs on sample invalid inputs and modiﬁed the
subject program, if needed.
Limited to Data Repair: The repair produced by our ap-
proach aim to ensure maximal data recovery, but it does not
ensure that the intended user information is preserved. Hence,
FSYNTH is limited to repair of the input data, but not the
intended information.
Input Constraints and Semantics: FSYNTH does not address
concerns about recovering or preserving the input constraints
or intended semantics of the input data. For instance, repairing
an invalid checksum or hash requires such information, and
FSYNTH will be limited for this use cases. However, it would
provide repair candidates that allow end-users to debug such
semantic issues.
In addition, inputs with signiﬁcant semantic
corruption may not be effectively repaired by FSynth. Even

though FSynth is effective in ﬁxing structural parts of invalid
inputs, when the corruption is in the semantic part, the missing
data becomes difﬁcult to recover. Examples include corrupted
numbers in calculations, and dates.
Repair via Input Synthesis: Firstly, the repair via synthesis
approach of FSYNTH is exhaustive, thus it can quickly become
computationally expensive. Secondly, the repair via synthesis
operation of our approach poses the risk of introducing input
elements with unintended semantic consequences. Speciﬁcally,
insertion operations may lead to data corruption and infor-
mation distortion. To mitigate this threat, FSYNTH provides
several valid candidate repairs ranked by the edit distance
of each repair candidate from the original input. Hence, we
encourage end-users to select the best semantically-ﬁt repair
from the potential repair candidates provided by FSYNTH.

VIII. RELATED WORK

Black-box Input Repair: A few techniques have been pro-
posed to analyze input data without program analysis, albeit
with the aim of understanding and localizing faults in the
program. The earliest works were either focused on simpli-
fying failure-inducing inputs [12], [17], or isolating fault-
revealing input fragments [18], [19]. Notably, the minimizing
delta debugging algorithm (DDMin) is focused on reducing
failure-inducing inputs in order to diagnose and localize faults
in the program. More recently, Kirschner et al. [6] proposed a
maximizing variant of the delta debugging algorithm (DDMax)
to repair invalid inputs to subsets via deletion, we compare
FSYNTH to DDMax in this work. In contrast to DDMax,
FSYNTH also synthesizes input elements to complete input
repair.
White and Grey-box Input Repair: Some techniques employ
program analysis to ﬁx invalid inputs. As an example, docov-
ery [20] applies symbolic execution to change broken inputs
to take error-free paths in the subject program. Similarly,
Ammann and Knight [21] proposed a method to transform
invalid inputs into valid inputs by analyzing the region of the
input causing the fault and changing those regions to avoid the
fault. Unlike these methods, FSYNTH is black-box, it relies on
the failure feedback of the subject program.
Constraint-based Input Repair: These approaches auto-
matically learn input constraints then enforce the learned
constraints to repair invalid inputs [22], [23] These constraints
are often extracted from valid inputs [24], [25], speciﬁed
with predicates [26], model-based systems [27], goal-directed
reasoning [28], dynamic symbolic execution [22] or invari-
ants [23]. For instance, S-DAGs [29] enforce constraints on in-
valid inputs in a semi-automatic way. Unlike these approaches,
FSYNTH does not learn input constraints, it employs input
synthesis and failure feedback to ﬁx invalid inputs.
Parser-directed Input Repair: This refers to the input repair
schemes of parsers, interpreters and compilers [8]–[10], [30],
[31]. These techniques employ operations such as insertion,
deletion and replacement of symbols [32]–[34], extending
forward or backwards from a parser error [35], [36], or
more general methods of recovery and diagnosis [8], [37].

In this work, we compare FSYNTH to the recovery scheme of
the ANTLR parser generator [10] which leverages the input
grammar to guide input repair. Unlike FSYNTH which aims
to ﬁx an invalid input, these schemes need an input grammar,
and aim to ensure the compiler does not halt while parsing.

IX. CONCLUSION

This paper presents FSYNTH, an input repair approach that
employs input synthesis and lightweight failure feedback (i.e.,
incomplete and (in)correct checks) to repair invalid inputs. Our
approach is black-box, does not require program analysis or
an input grammar. We evaluate the data recovery,effectiveness,
and efﬁciency of our approach, in comparison to the state-
of-the-art methods. We show that FSYNTH has a very high
data recovery rate,
is
also very effective and efﬁcient in input repair—it completes
the repair of about four in ﬁve invalid inputs (77%) within
four minutes. Furthermore, we demonstrate that our approach
is up to 35% more effective than the best best baseline—
(syntactic) DDMax, without using an input grammar. In
summary, this work demonstrates that combining lightweight
failure feedback and input synthesis are important for the
effective repair of invalid inputs, especially in the absence of
an input speciﬁcation and program analysis.

it recovered 91% of input data. It

In the future, we plan to investigate how to improve the
performance of our approach by learning input semantics and
constraints. We also provide our implementation, data and
experimental results for easy replication, scrutiny and reuse:

https://github.com/vrthra/fsynth-artifact

REFERENCES

[1] C. Scafﬁdi and M. Shaw, “Accommodating data heterogeneity in uls
systems,” in Proceedings of the 2nd international workshop on Ultra-
large-scale software-intensive systems, 2008, pp. 15–18.

[2] K. Mus¸lu, Y. Brun, and A. Meliou, “Preventing data errors with
continuous testing,” in Proceedings of the 2015 International Symposium
on Software Testing and Analysis, 2015, pp. 373–384.

[3] N. Harrand, T. Durieux, D. Broman, and B. Baudry, “The behavioral

diversity of java json libraries,” 2021.

[4] N. Seriot, “Parsing json is a mineﬁeld,” https://seriot.ch/projects/parsing

json.html, 2016.

[5] T. Arvin, “Comparison of different sql implementations,” https://troels.

arvin.dk/db/rdbms/, 2018.

[6] L. Kirschner, E. Soremekun, and A. Zeller, “Debugging inputs,”
in Proceedings of
the ACM/IEEE 42nd International Conference
on Software Engineering, ser. ICSE ’20. New York, NY, USA:
Association for Computing Machinery, 2020, p. 75–86.
[Online].
Available: https://doi.org/10.1145/3377811.3380329

[7] C. Scafﬁdi, B. Myers, and M. Shaw, “Topes,” in 2008 ACM/IEEE 30th
IEEE, 2008, pp.

International Conference on Software Engineering.
1–10.

[8] A. V. Aho and T. G. Peterson, “A minimum distance error-correcting
parser for context-free languages,” SIAM Journal on Computing, vol. 1,
no. 4, pp. 305–312, 1972.

[9] L. Diekmann and L. Tratt, “Don’t panic! better, fewer, syntax errors
for LR parsers,” in 34th European Conference on Object-Oriented
Programming, ECOOP 2020, November 15-17, 2020, Berlin, Germany
(Virtual Conference), ser. LIPIcs, R. Hirschfeld and T. Pape, Eds., vol.
166. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2020, pp. 6:1–
6:32. [Online]. Available: https://doi.org/10.4230/LIPIcs.ECOOP.2020.6
[10] T. Parr and K. Fisher, “Ll (*) the foundation of the antlr parser

generator,” ACM Sigplan Notices, vol. 46, no. 6, pp. 425–436, 2011.
[11] WHATWG, “Url living standard,” https://url.spec.whatwg.org/, 2022.

[12] A. Zeller and R. Hildebrandt, “Simplifying and isolating failure-
inducing input,” IEEE Trans. Softw. Eng., vol. 28, no. 2, pp. 183–200,
Feb. 2002. [Online]. Available: http://dx.doi.org/10.1109/32.988498
[13] B. Mathis, R. Gopinath, M. Mera, A. Kampmann, M. H¨oschele, and
A. Zeller, “Parser-directed fuzzing,” in Proceedings of the 40th ACM
SIGPLAN Conference on Programming Language Design and Imple-
mentation, ser. PLDI 2019.
New York, NY, USA: Association for
Computing Machinery, 2019, p. 548–560.

[14] P. Eaton, “Parser generators vs. handwritten parsers: surveying ma-
jor language implementations in 2021,” https://notes.eatonphil.com/
parser-generators-vs-handwritten-parsers-survey-2021.html, 2021.
[15] G. Inc. (2018) Rest api v3. [Online]. Available: https://developer.github.

com/v3/

[16] T. Parr, The deﬁnitive ANTLR 4 reference. Pragmatic Bookshelf, 2013.
[17] J. Clause and A. Orso, “Penumbra: Automatically identifying failure-
relevant inputs using dynamic tainting,” in Proceedings of the eighteenth
international symposium on Software testing and analysis. ACM, 2009,
pp. 249–260.

[18] G. Misherghi and Z. Su, “HDD: Hierarchical delta debugging,” in Pro-
ceedings of the 28th international conference on Software engineering.
ACM, 2006, pp. 142–151.

[19] C. D. Sterling and R. A. Olsson, “Automated bug isolation via program
chipping,” Software: Practice and Experience, vol. 37, no. 10, pp. 1061–
1086, 2007.

[20] T. Kuchta, C. Cadar, M. Castro, and M. Costa, “Docovery: Toward
generic automatic document recovery,” in International Conference on
Automated Software Engineering (ASE 2014), 9 2014, pp. 563–574.

[21] P. E. Ammann and J. C. Knight, “Data diversity: An approach to
software fault tolerance,” Ieee transactions on computers, no. 4, pp.
418–425, 1988.

[22] I. Hussain and C. Csallner, “Dynamic symbolic data structure repair,” in
Software Engineering, 2010 ACM/IEEE 32nd International Conference
on, vol. 2.

IEEE, 2010, pp. 215–218.

[23] B. Demsky, M. D. Ernst, P. J. Guo, S. McCamant, J. H. Perkins,
and M. Rinard, “Inference and enforcement of data structure
consistency speciﬁcations,” in Proceedings of the 2006 International
Symposium on Software Testing and Analysis, ser. ISSTA ’06. New
York, NY, USA: ACM, 2006, pp. 233–244.
[Online]. Available:
http://doi.acm.org/10.1145/1146238.1146266

[24] F. Long, V. Ganesh, M. Carbin, S. Sidiroglou, and M. Rinard,

“Automatic input rectiﬁcation,” in Proceedings of the 34th International
Conference on Software Engineering, ser. ICSE ’12.
Piscataway,
NJ, USA:
[Online]. Available:
http://dl.acm.org/citation.cfm?id=2337223.2337233

IEEE Press, 2012, pp. 80–90.

[25] M. C. Rinard, “Living in the comfort zone,” in Proceedings of
the 22Nd Annual ACM SIGPLAN Conference on Object-oriented
Programming Systems and Applications, ser. OOPSLA ’07. New
York, NY, USA: ACM, 2007, pp. 611–622.
[Online]. Available:
http://doi.acm.org/10.1145/1297027.1297072

[26] B. Elkarablieh and S. Khurshid, “Juzi: A tool for repairing complex
data structures,” in Proceedings of the 30th international conference on
Software engineering. ACM, 2008, pp. 855–858.

[27] B. Demsky and M. Rinard, “Automatic detection and repair of errors
in data structures,” SIGPLAN Not., vol. 38, no. 11, pp. 78–95, Oct.
2003. [Online]. Available: http://doi.acm.org/10.1145/949343.949314

[28] ——, “Data structure repair using goal-directed reasoning,” in Proceed-
ings. 27th International Conference on Software Engineering, 2005.
ICSE 2005., May 2005, pp. 176–185.

[29] J. Scheffczyk, U. M. Borghoff, P. R¨odig, and L. Schmitz, “S-dags:
Towards efﬁcient document repair generation,” in Proc. of the 2nd Int.
Conf. on Computing, Communications and Control Technologies, vol. 2,
2004, pp. 308–313.

[30] K. Hammond and V. J. Rayward-Smith, “A survey on syntactic error
recovery and repair,” Computer Languages, vol. 9, no. 1, pp. 51–67,
1984.

[31] R. C. Backhouse, Syntax of programming languages: theory and prac-

tice. Prentice-Hall, Inc., 1979.

[32] S. O. Anderson and R. C. Backhouse, “Locally least-cost error recovery
in earley’s algorithm,” ACM Transactions on Programming Languages
and Systems (TOPLAS), vol. 3, no. 3, pp. 318–347, 1981.
[33] C. Cerecke, “Locally least-cost error repair in LR parsers,” 2003.
[34] S. O. Anderson, R. C. Backhouse, E. H. Bugge, and C. Stirling, “An
assessment of locally least-cost error recovery,” The Computer Journal,
vol. 26, no. 1, pp. 15–24, 1983.

[35] M. Burke and G. A. Fisher Jr, A practical method for syntactic error

diagnosis and recovery. ACM, 1982, vol. 17, no. 6.

[36] J. Mauney and C. N. Fischer, “A forward move algorithm for LL and
LR parsers,” ACM SIGPLAN Notices, vol. 17, no. 6, pp. 79–87, 1982.
[37] T. Krawczyk, “Error correction by mutational grammars,” Information

Processing Letters, vol. 11, no. 1, pp. 9–15, 1980.

