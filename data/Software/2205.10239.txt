2
2
0
2

y
a
M
0
2

]
E
S
.
s
c
[

1
v
9
3
2
0
1
.
5
0
2
2
:
v
i
X
r
a

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

1

AGA: An Accelerated Greedy Additional
Algorithm for Test Case Prioritization

Feng Li, Jianyi Zhou, Yinzhu Li, Dan Hao, Lu Zhang

Abstract—In recent years, many test case prioritization (TCP) techniques have been proposed to speed up the process of fault
detection. However, little work has taken the efﬁciency problem of these techniques into account. In this paper, we target the Greedy
Additional (GA) algorithm, which has been widely recognized to be effective but less efﬁcient, and try to improve its efﬁciency while
preserving effectiveness. In our Accelerated GA (AGA) algorithm, we use some extra data structures to reduce redundant data
accesses in the GA algorithm and thus the time complexity is reduced from O(m2n) to O(kmn) when n > m, where m is the number
of test cases, n is the number of program elements, and k is the iteration number. Moreover, we observe the impact of iteration
numbers on prioritization efﬁciency on our dataset and propose to use a speciﬁc iteration number in the AGA algorithm to further
improve the efﬁciency. We conducted experiments on 55 open-source subjects. In particular, we implemented each TCP algorithm with
two kinds of widely-used input formats, adjacency matrix and adjacency list. Since a TCP algorithm with adjacency matrix is less
efﬁcient than the algorithm with adjacency list, the result analysis is mainly conducted based on TCP algorithms with adjacency list.
The results show that AGA achieves 5.95X speedup ratio over GA on average, while it achieves the same average effectiveness as GA
in terms of Average Percentage of Fault Detected (APFD). Moreover, we conducted an industrial case study on 22 subjects, collected
from Baidu, and ﬁnd that the average speedup ratio of AGA over GA is 44.27X, which indicates the practical usage of AGA in
real-world scenarios.
Note: This is a preprint of the accepted paper “Feng Li, Jianyi Zhou, Yinzhu Li, Dan Hao, and Lu Zhang. AGA: An Accelerated
Greedy Additional Algorithm for Test Case Prioritization. IEEE Transactions on Software Engineering, 2021”, which can be
accessed at https://ieeexplore.ieee.org/document/9662236.

Index Terms—Test Case Prioritization, Additional Strategy, Acceleration

(cid:70)

1 INTRODUCTION

Test case prioritization (abbreviated as TCP) [1], [2], [3], [4],
[5], [6], is proposed to schedule the execution order of test
cases so as to detect faults as early as possible. To address
this problem, a large number of TCP techniques have been
proposed in the literature.

Among these TCP techniques, the Greedy Additional
(GA) algorithm has received much attention since it was
proposed in 1999 [5] due to its widely recognized effect-
iveness [7], [8], [9], [10]. In particular, the GA algorithm
iteratively selects the next test case which covers the largest
number of elements (e.g., methods, branches, statements)
that have not been covered by previously selected test
cases. When the selected test cases cover all elements, this
GA algorithm deals with the remaining unselected test
cases with any prioritization technique (e.g., Greedy Total
algorithm [5], which schedules these test cases based on the
descendent order of the number of total covered program
elements). Later in 2002, Elbaum et al. [3] slightly modiﬁed
this algorithm by reordering the remaining test cases with
the GA strategy again after resetting all the elements to be

•

Feng Li, Jianyi Zhou, Dan Hao, and Lu Zhang are with the Institute of
Software, School of Computer Science, Peking University, Beijing, China
and Key Laboratory of High Conﬁdence Software Technologies (Peking
University), MoE. Dan Hao is the corresponding author.
E-mail: {lifeng2014, zhoujianyi, haodan, zhanglucs}@pku.edu.cn

• Yinzhu Li is with the Baidu Online Network Technology (Beijing) Co.,

Ltd.
E-mail: liyinzhu@baidu.com

978-1-6654-4407-1/21/$31.00 ©2021 IEEE

“uncovered”. This GA algorithm repeats the GA strategy
until all the test cases are selected and thus its effectiveness
is no worse than that of the original GA algorithm [5]. There-
fore, the GA algorithm proposed by Elbaum et al. [3] is taken
as the default GA algorithm by most researchers in TCP and
in this paper1. Moreover, the original GA algorithm is called
the GA-ﬁrst algorithm for distinction. Note that we target
GA rather than GA-ﬁrst in this paper because the former
is more widely used in the literature. Although researchers
have put dedicated efforts in TCP and have proposed a large
number of TCP techniques since then, the GA approach [3]
remains one of the most effective strategies in terms of fault-
detection rate [7], [8], [10], which is usually measured by
the average percentage of faults detected (abbreviated as
APFD). In other words, none of the existing TCP techniques
can always outperform GA [3] in terms of effectiveness.

Besides effectiveness, time cost is widely recognized
as another important issue inﬂuencing the application of
an approach [11] [12], [13], [14], especially considering the
limited available time. In particular, the time cost of TCP,
called TCP efﬁciency in this paper, refers to how much time
a TCP approach consumes. As reported, Google [15] runs
800K builds and 150M tests every day (the same tests are
run many times). If a TCP approach consumes much more
time on prioritization, the time left for test running will
be reduced to a large extent. Furthermore, software modi-
ﬁcation occurs dramatically frequently so that regression

1 Without further clariﬁcation, the GA algorithm used in this paper refers

to the one proposed by Elbaum et al. [3].

 
 
 
 
 
 
TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

2

testing consumes about 80% testing cost [16]. For example,
Google developers modify source code one time per second
on average [15]. To improve the efﬁciency of regression
testing, it is necessary to apply TCP more than once because
frequent code modiﬁcation may hamper the effectiveness
of TCP [17]. That is, considering the practical application
of TCP, including the GA algorithm, both effectiveness and
efﬁciency are important.

However, existing TCP approaches, including the GA
algorithm, suffer from the efﬁciency problem, e.g., the
previous work shows that most existing TCP approaches
cannot deal with large-scale application scenarios [13], [15],
[18]. Furthermore, some work [13], [15], [18] points out
that the GA algorithm spends dramatically long time on
prioritization. Note that in the 20-year history of GA, there
is no approach proposed to improve its efﬁciency while
preserving the high effectiveness.

In this paper, we make the ﬁrst attempt to accelerate the
GA algorithm and maintain the effectiveness. In particular,
we analyze the efﬁciency problem of the GA algorithm
and propose to accelerate the GA algorithm through two
enhancements. The proposed algorithm is called the Accel-
erated Greedy Additional (abbreviated as AGA) algorithm.
First, many redundant data accesses occur during priorit-
ization in GA. Whenever a test case is selected, the GA
algorithm scans the coverage information of all test cases
to mark elements covered by this selected test case and
calculates the number of unmarked elements covered by
each unselected test case. Such scanning is less efﬁcient and
may contain many redundant data accesses. Therefore, we
design some extra data structures (e.g., indices) to summar-
ize the coverage information of each test case in the AGA
algorithm. Supposed that m, n, k are the number of test
cases, the number of elements and the number of iterations
to repeat GA strategy (which is called iteration number in
this paper), and given n > m (which is true in most cases),
the time complexity of our AGA algorithm is O(kmn),
while the time complexity of the GA algorithm is O(m2n).
The value of k determines to what extent the former is
superior to the latter. In practice, k is usually much smaller
than m, and in our approach, k is ﬁxed as a constant (by the
second part below), so, our O(kmn) is superior to O(m2n).
Second, the GA algorithm proposed by Elbaum et al. [3]
repeats the GA strategy multiple times in TCP and thus the
iteration number is usually larger than 1. Intuitively, when
an element is covered for enough times, the probability that
it still contains faults is low, so the remaining iterations may
not contribute to the effectiveness but only decrease TCP
efﬁciency. Therefore, we investigated their relation empiric-
ally and applied it to modify the GA algorithm to improve
efﬁciency but preserving effectiveness. To sum up, our AGA
algorithm consists of two parts, time complexity reduction
and iteration number reduction. Note that theoretical im-
provement is rather important and gives clear assurance for
high-efﬁciency under any situations (especially in the ﬁrst
part of AGA). Also, our simple technique with theoretical
improvement is meaningful in practice and can illustrate
the simple nature of the problem.

We conducted controlled experiments by using 55 open-
source projects from GitHub (whose total lines of code
are from 1,621 to 177,546). Because the algorithm input

(program coverage) has two kinds of format, adjacency
matrix and adjacency list, we conduct our experiments on
both of them, which is discussed in Section 2.1. In the
experiments, we studied the contributions of the two parts
of AGA separately, and found that both of them improve the
efﬁciency to a large extent. Furthermore, we investigated the
effectiveness and efﬁciency of AGA by comparing it with
GA. The results showed that on average the speedup ratio
of AGA over GA is 5.95X and 27.72X on two input formats,
which is a very large improvement. We also ﬁnd that the
average APFD of AGA and GA is the same, and Analysis of
Covariance (ANCOVA) [19] shows no signiﬁcant difference
between them. Moreover, the effect size (Cohen’s d) also
indicates small effect.

We also empirically compared AGA with FAST [18],
which focuses on the TCP efﬁciency problem. As FAST [18]
targets a different problem, improving the time efﬁciency
by sacriﬁcing effectiveness, such a comparison in terms
of efﬁciency may be a bit unfair to our AGA approach.
Surprisingly, the results showed that the average speedup
ratio of AGA over FAST is 4.29X (with signiﬁcant difference
and medium effect), which means AGA even outperforms
the technique that sacriﬁces effectiveness to achieve high
efﬁciency. Also, the average APFD difference that AGA
exceeds FAST is 0.1702, and ANCOVA shows that the dif-
ference is statistically signiﬁcant. Moreover, the effect size
(Cohen’s d) also indicates huge effect.

We further performed an industrial case study in Baidu,
a famous Internet service provider with over 600M monthly
active users. In particular, we compared the performance
of AGA and GA in 22 subjects of Baidu. In this industrial
case study, the average speedup ratio of AGA over GA is
44.27X and 61.43X on two input formats, which indicates
the usefulness of AGA in real-world large-scale scenarios.
Also, AGA is faster than FAST on all 22 subjects and
achieves 4.58X speedup ratio on average, and the difference
is statistically signiﬁcant with very large effect. Due to the
commercial constraints, we cannot access the source code
of these projects, and the developers in Baidu also do not
record the fault positions in the history, which are necessary
to calculate the APFD results. So, we did not compare the
effectiveness of these approaches in this study.

The contributions of this work are summarized as below.
• The ﬁrst attempt to improve the efﬁciency of GA while
preserving its effectiveness, since GA is believed to
have high effectiveness. In particular, we resolve the ef-
ﬁciency issue of GA through theoretical improvement,
which gives clear assurance for high-efﬁciency under
any situations.

• An approach to accelerating the widely-known GA al-
gorithm through two parts, including time complexity
reduction and iteration number reduction. With the
former, the complexity is reduced from O(m2n) to
O(kmn) given n > m, which is theoretically proved;
with the latter, the corresponding AGA algorithm is
more efﬁcient and can be as competitive as GA regard-
ing to effectiveness, which is empirically shown. In fact,
although it seems like an easy-to-implement algorithm,
in the broad literature, nobody realizes this optimiza-
tion and the subsequent reduction of complexity. There-
fore, this paper is the ﬁrst to systematically analyze this

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

3

problem and propose and evaluate the optimization
approach, which is helpful for the community.

• Large scale experiments on 55 open-source projects
demonstrating the effectiveness and efﬁciency of our
AGA approach, compared with the GA algorithm.

• An empirical comparison of AGA with FAST, which
improves time efﬁciency but decreases effectiveness.
• An industrial case study on 22 subjects from Baidu,
which indicates the practical usage of AGA in real-
world scenarios.

2 TIME COMPLEXITY REDUCTION

In this section, we review the Greedy Additional (GA)
algorithm by an example (in Section 2.1). By analyzing its
time complexity (in Section 2.2), we propose to accelerate
GA through extra-deﬁned data structures (in Section 2.3).
Such modiﬁcation improves the efﬁciency of GA so that the
time complexity becomes O(kmn) (given n > m), whereas
the complexity of GA is O(m2n), where n is the number
of program elements (e.g., statements, branches, methods)
covered by the test suite, m is the number of test cases in
the test suite, and k is the iteration number.

2.1 Example

Table 1 presents an example showing the coverage inform-
ation of a test suite. This test suite consists of ﬁve test
cases (i.e., T1, T2, . . . , and T5) and the test suite covers
ﬁve program elements (i.e., E1, E2, . . . , and E5). A common
representation form of coverage information is adjacency
matrix, which is shown in Table 1(a). (cid:13) represents that the
test case covers the corresponding program element, while
× represents the opposite. Another representation form of
coverage information is adjacency list, which is shown in
Table 1(b). In our example, the two forms represent totally
the same information.

Table 1: An Example

(a) Adjacency Matrix

Cover or Not

E1

Elements
E3

E4

E2

E5

Test Cases

(cid:13)

(cid:13)

(cid:13) × ×
T1
T2 × × (cid:13)
(cid:13)
(cid:13)
(cid:13) × × ×
T3
T4 × × (cid:13)
(cid:13) ×
T5 × × × × (cid:13)

(cid:13)

(b) Adjacency List

Test Cases Covered Elements

T1
T2
T3
T4
T5

E3
E5

E2
E4
E2
E4

E1
E3
E1
E3
E5

If we take the adjacency matrix as input, the GA al-
gorithm runs as follow. First, no element has been covered
before and this algorithm scans the whole table to calculate
the number of elements covered by each test case. Then
it chooses T1 or T2 since both of them cover the most

elements. Supposed that this algorithm chooses T1, then
T2, T3, T4, and T5 remain unselected. As the selected test
case T1 covers elements E1, E2, and E3, the rest elements E4
and E5 remain uncovered. The algorithm scans the whole
table again to ﬁnd that T2, T3, T4, and T5 covers 2, 0, 1,
and 1 of the 2 uncovered elements, respectively. So, the GA
algorithm chooses T2 as the next test case. Now, all elements
have been covered and the GA algorithm [3] starts another
iteration by resetting all elements to “uncovered”. Finally,
the test execution sequence produced by the GA algorithm
is “T1, T2, T3, T4, T5”. On the other hand, provided the
adjacency list as input, GA runs similarly and produces the
same output.

2.2 Analysis of the GA Algorithm

In this section, we analyze the time complexity of the GA
algorithm through its general implementation. Suppose the
coverage information is recorded in a table like Table 1(a),
the GA algorithm ﬁrst scans the whole table to ﬁnd the line
with the most “(cid:13)” entries and selects the corresponding
test case into the prioritized sequence. When a test case
is selected and added to the sequence, the GA algorithm
scans the whole table to ﬁnd the “(cid:13)”s whose corresponding
element is covered by the latest selected test case. These
“(cid:13)”s are replaced by “×”s. The GA algorithm repeats the
proceeding process until all the entries in the table are “×”s
or all the test cases have been selected. In the latter case
the termination condition is satisﬁed and the GA algorithm
ends by producing a prioritized test suite; otherwise, GA
reuses the initial table by replacing “(cid:13)”s with “×”s for each
selected test case and repeats the proceeding process again.
Supposed that there are m test cases in the given test
suite to be prioritized and n program elements are covered
by the test suite, the GA algorithm needs to scan the whole
table for m times and thus the time complexity is O(m2n),
as shown by previous work [3], [7], [8]. However, lots of
accesses of the table are redundant. First and the most
importantly, every time the coverage table is updated, the
GA algorithm recalculates the total “(cid:13)” entries of each
unselected test case, without reusing previous calculation.
Second, none of the accesses to “×”s in the table is necessary
because the GA algorithm does not want to update them in
the process. Third, in order to ﬁnd the elements covered
by the latest selected test case, the GA algorithm scans all
elements in the table, which is also unnecessary. Let us
illustrate the preceding redundant accesses by the example.
When T1 is selected ﬁrst, the GA algorithm scans Row T1
and ﬁnds three “(cid:13)”s. Among the ﬁve accesses (i.e., E1, E2,
. . ., E5), the accesses of E4 and E5 are redundant. Then, the
GA algorithm changes the state of E1, E2, and E3 in other
four test cases from “(cid:13)” to “×”. During this process, it is
also not necessary to access the state “×”. Then, the GA
algorithm scans the whole table to select the next test case,
but this process can be optimized by analyzing updated
columns and the previous calculation on total number of
“(cid:13)” covered by each test case. To sum up, due to such a
large number of redundant accesses in the GA algorithm, it
is possible to reduce its time cost and improve its efﬁciency.
If we take the adjacency list as input, similar analysis
can be done. First, the accesses of “×”s to ﬁnd covered

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

4

elements in one row is reduced, while more time is spent on
ﬁnding all test cases that cover a speciﬁc element (through
scanning of the whole list). As a result, the overall time
complexity remains O(m2n). Second, lots of accesses of the
list are redundant, too. Following our previous analysis,
the time efﬁciency can be improved through reducing the
unnecessary operations.

2.3 Improvement of Time Complexity

To reduce such redundant accesses, we propose the AGA C
approach that deﬁnes extra data structures, which reuse
previous information collected during its execution. In par-
ticular, we use a list to record the total number of elements
covered by each test case and dynamically update it during
prioritization, in order to alleviate the scanning of the cover-
age table. We also use forward and inverted indices to save
the data accesses of “×” entries in the table.

Our AGA C algorithm is shown in Algorithm 1. Line 1
initializes several data structures. T C is a list of length
m recording the number of elements covered by each test
case. In our example, T C is [3, 3, 2, 2, 1] from Table 1 by
deﬁnition. HS is a list of length n recording whether each
test case has been selected. HC is a list of length n record-
ing whether each element has been covered by previous
test cases. F I are forward indices that index all elements
covered by each test case, while II are inverted indices that
index all test cases that cover each element. From Table 1,
in our example, F I records that T1 covers [E1, E2, E3], T2
covers [E3, E4, E5], etc. II records that E1 is covered by [T1,
T3], E2 is covered by [T1, T3], etc. Line 2 initializes P as
the empty list. Then, in Line 3 to Line 21, the algorithm
selects m test cases in turn. First, it chooses the largest
value in T C whose test case t is marked unselected in HS.
The algorithm adds t to the prioritized list P and marks
it in HS. In our example, in the ﬁrst loop, T1 is selected
(since it covers the most program elements), marked in HS,
and added to P. Then, for every element j in F I[t] that is
marked uncovered in HC, the algorithm marks it as covered
and for every test case i in II[j], the algorithm substracts
T C[i] by 1. In our example, in the ﬁrst loop, E1 and E2
are marked covered and the updated T C is [0, 2, 0, 1, 1]
Finally, the algorithm continues to select the next test case
by repeating the process. As shown from Line 5 to Line 9,
if all elements have been covered by selected test cases,
the algorithm completes current iteration and restores the
original T C to start the next iteration. In our example, after
T1 and T2 are selected, the original T C is restored. The total
number of iterations is called iteration number.

Furthermore, we analyze the time complexity of our
AGA C algorithm. All initialization operations consume
O(mn) time. Each calculation of maximum value in T C
consumes O(m) time, which leads to O(m2) time in total.
The number of times to update T C is equal to the elements
in F I (also equal to the test cases in II) in an iteration,
which is the number of “(cid:13)” entries in the coverage mat-
rix. So, in each iteration, the algorithm updates T C for
up to O(mn) times, and the total time for updating T C
is O(kmn), where k is the iteration number. Generally
speaking, the number of elements is often larger than the
number of test cases, which means n > m. So, according

Algorithm 1: AGA C algorithm

Input: Coverage information M;
Output: Prioritized test cases P;

1 Initialize T C, HS, HC, F I, and II from M, t = 0;
2 Set P as empty list;
3 while t < m do
4

Find the largest value in T C that the
corresponding test case t has not been selected
(take the use of HS);
if No test case can be selected then

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Change T C to the original value;
Change HC to the original value;
Continue;

end
Add t into P;
Mark t as selected in HS;
forall j in F I[t] do

if HC[j] is “uncovered” then

Mark j as “covered” in HC;
forall i in II[j] do

Decrease T C[i] by 1;

end

end

end
t = t + 1;

20
21 end
22 Return P;

to the deﬁnition of Big O notation, the total time complex-
ity O(kmn + m2) can be simpliﬁed as O(kmn + m2) =
O(kmn + mn) = O((k + 1)mn) = O(kmn), where k is the
iteration number. Note that in most cases, n > m obviously
holds, and can also be veriﬁed by the subject statistics in this
paper (given by Table 6). For other special cases, the original
time complexity O(kmn + m2) is still a large improvement.
In addition, in our algorithm, we use more storage space
to maintain the extra data structures in order to improve
time complexity. So, it is necessary to analyze the space
complexity, too. In GA, the coverage information (adjacency
matrix/list) takes O(mn) space, and additional O(1) space
is used to store temporary variables in the algorithm, which
means the overall space complexity of GA is O(mn). In
AGA, the same O(mn) space is used to store the coverage
table, while T C, HS, HC, F I, and II need O(m), O(m),
O(n), O(mn), and O(mn) spaces, respectively. So, the over-
all space complexity of AGA is O(mn), which is the same
as GA, with the only difference lying in the constant factor.

3 ITERATION NUMBER REDUCTION

From Section 2, we obtain a new approach with time com-
plexity O(kmn), where k is the iteration number. In practice,
k is often much smaller than m in most projects because
usually many test cases are needed to cover all elements in
an iteration. However, in the worst case, k may be equal
to m, indicating the worst time complexity of our AGA
algorithm is still the same as that of the GA algorithm.

To further improve the efﬁciency of the GA algorithm,
especially in the worst case, in this section, we discuss

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

5

the impact of the iteration number and introduce another
modiﬁcation adopted in our AGA algorithm. Finally, we
present an experiment to evaluate the impact of iteration
number on the GA algorithm.

3.1 Modiﬁcation with Iteration Number Reduction

Let us re-examine the deﬁnition of “an iteration”. In
this paper, the process of selecting some test cases from
covering 0 element to covering all possible elements and
then resetting them to be “uncovered” is called “an iter-
ation”. Intuitively, the iteration number may have large
impact on time cost of the GA algorithm. The difference
between the GA-ﬁrst algorithm [5] and the GA algorithm [3]
also indicates the inﬂuence of such an iteration number.
Moreover, between these two algorithms, there exist many
other potential algorithms, depending how many times
the GA strategy is used (i.e., iteration number of the GA
strategy) and what strategy is used to deal with the remain-
ing unselected test cases (e.g., Greedy Total strategy, which
schedules test cases based on the descendent order of the
number of total covered program elements).

n = k × l

(1)

Here, we deﬁne the average number of test cases selected
in one iteration as l, so we can deduce Formula (1). Accord-
ing to Formula (1), if a large number of test cases are selected
in one iteration, the total iteration number of this project is
small; if few test cases are selected in one iteration, the total
iteration number of this project is large. As our goal is to
improve efﬁciency while preserving effectiveness, projects
with small iteration number have already been efﬁcient
enough, and the time complexity O(kmn) can be reduced to
O(mn). For those projects with large iteration number, it is
necessary to optimize the iteration number to some extent.
In fact, everytime a program element is covered, the
probability that it still contains faults decreases. After many
iterations, all elements have been covered for enough times.
On one hand, if all faults have been revealed after these
iterations, the remaining iterations are useless for detecting
faults but only increase the time cost. On the other hand,
if there are still several faults existed after many iterations,
they are supposed to be hard to reveal and the remaining
iterations may only reveal them by chance, intuitively. So,
we conjecture that after some iterations, the effectiveness of
GA just ﬂuctuates along with the remaining iterations.

Based on the above reasoning, we introduce another
the proposed AGA algorithm, AGA I.
component of
AGA I reduces the time cost by reducing the iteration
number. Different from the GA algorithm, AGA I does not
repeat applying the GA strategy until all the test cases are
prioritized, but stops when the speciﬁed iteration number is
achieved. Regarding to the remaining unselected test cases,
AGA I applies other less costly techniques (e.g., the Greedy
Total technique (GT) [5], which is usually used in previous
work and also in this paper). Take Table 1 as an example, the
original iteration number is 2. If we reduce it to be 1, AGA I
does not repeat the additional strategy after selecting T1 and

T2 and prioritizes the remaining test cases using GT.
3.2 Experiment

We conjecture that AGA I does not inﬂuence the effective-
ness (e.g., APFD) much but can improve efﬁciency (i.e., time
cost) a lot. To verify our conjecture, we design an experiment
to investigate how the iteration number impacts TCP in
terms of both effectiveness and efﬁciency.

Speciﬁcally, we use the same setup as the comprehensive
experiments in Section 5. More details about the subjects,
faults, implementation and supporting tools, and measure-
ment are given in Section 5.

We applied the GA algorithm to all subjects, and recor-
ded the total number of iterations the GA strategy is applied
during the process for each project, which is denoted as k.
Then we applied to each project k modiﬁed GA algorithms,
each of which is denoted as algorithm algoi (1 ≤ i ≤ k),
recording their APFD values and time spent during pri-
oritization. In particular, algorithm algoi repeats the GA
strategy i times and prioritizes the remaining unselected test
cases by the Greedy Total strategy [5]. Note that algorithm
algo1 is actually the GA-ﬁrst algorithm, whereas algorithm
algok is actually the GA algorithm.

Due to space limit, we only present some statistics of the
experimental results in Table 2, that is minimum, maximum,
average, quartiles (Q1, Q2, Q3), and the detailed results
are given on the website of this project. From the eighth
column, the average iteration number among all open-
source subjects is 29.20. The ninth to the fourteenth columns
present the ratio between the time cost of the GA approach
and that of the GA-ﬁrst approach [5]. The big gap between
the maximal and minimal time ratio indicates the inﬂuence
of the iteration number. To better analyze the relationship
between iteration number and time cost, we put detailed
results in Appendix A. We draw a line chart of iteration
number and time cost for each project. Note that in order to
see the trend, we only present the projects whose iteration
number is no less than 20 (k ≥ 20). The plots also support
our claim that the iteration number contributes much to
the time cost. As k is the coefﬁcient of time complexity, it
largely determines the actual efﬁciency in practice, so, we
think there is a large space to reduce time complexity.

The last six columns in Table 2 present the APFD ranges
of each project with different iteration numbers, that is,
the highest APFD value minus the lowest APFD value.
From the quartiles, we conclude that although some outliers
exist, most of the APFD ranges are very small. And the
average APFD range is only 0.0085 among all open-source
subjects, indicating that little ﬂuctuation of APFD occurs as
the iteration number varies.

To sum up, we have two main observations. First, along
with the increase of the iteration number, the time cost also
increases, indicating that the iteration number contributes
much to the time cost. Second, the APFD value varies a little
when the iteration number varies, which means a too large
iteration number contributes little to the APFD value. These
two observations also verify our conjectures in Section 3.

As we discuss in Section 3, projects with small iteration
numbers are efﬁcient enough by using AGA C, so, we need
to decide a proper reduced iteration number for projects
with a large iteration number. In fact, this reduced iteration
number is not ﬁxed, which means it can be adjusted for

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

6

Table 2: Statistics of the Impact of Iteration Number

Subjects

#Projects

Iteration Number

Time GA / Time GA-ﬁrst *

min. Q1 Q2 Q3 max.

ave. min. Q1

Q2

Q3 max.

ave. min.

Q1

APFD range **
Q2

Q3

max.

ave.

Open-Source

55

1

6

10

16.5

679

29.20

1.00

1.21

1.57

1.84

17.56

2.14

0.0000

0.0004

0.0013

0.0039

0.1328

0.0085

* The time ratio between the GA algorithm and the GA-ﬁrst algorithm.
** The highest APFD subtracts the lowest APFD among all iteration numbers.

speciﬁc usage. In this papar, we determine this value from
some heuristics. On one hand, although we conjecture that
there is no need to conduct too many iterations to detect
faults, we still prefer to choose a relatively high value to en-
sure the effectiveness. On the other hand, if we assume that
every time an element is covered, the probability that it still
contains faults decreases to half of the original probability,
given that the initial probability is 1, we need to cover an
element 10 times to reduced the probability to be less than
1‰ ((1/2)10 = 1/1024). As a result, in the remaining of this
paper, we implement our AGA approach by using 10 as the
reduced iteration number.

Finding: The iteration number has large inﬂuence
on the efﬁciency of the GA algorithm, while it im-
pacts little on effectiveness. In this paper we set the
iteration number to be 10 in implementing the AGA
approach.

Note that this ﬁnding is conﬁrmed on our dataset empir-
ically and may have bias considering the diversity of differ-
ent datasets. However, the constraint on k does reduce the
overall time complexity from O(kmn+m2) to O(mn+m2).
When n > m, which is general in most cases, the reduction
is from O(kmn) to O(mn).

3.3 Discussion on the Chosen Iteration Number

In this paper, we set the iteration number to be 10 in im-
plementing AGA through some heuristics. Here we discuss
the inﬂuence of this choice. First, we analyzed the APFD
results of the GA algorithm with various iteration number
(i.e., algorithm algo1 (1 ≤ i ≤ k) in Section 6.1). In partic-
ular, for each project we recorded the highest APFD value
(denoted as APFDmax) among these algorithms, and found
the smallest iteration number r whose corresponding APFD
value is no smaller than AFPDmax ∗ 99%. Surprisingly, the
smallest iteration number r for all projects are no larger than
10, which indicates that only several iterations is enough
for maintaining original effectiveness, even in projects with
the iteration number up to 679. Second, although we set the
iteration number to be 10 in this paper, it may not be the best
choice. We respectively applied algo8, algo9, algo10, algo11,
and algo12 to all projects with k > 8 as Section 6.1, and
found that the gap between the maximum and minimum
APFD value of these algos is 0.0006 on average, which
means that there might be many possible choices of the
reduced iteration number in practice. In other words, the
value of k in our evaluation is decided by reasoning, but
it can have various values, depending on the choices of
developers. For example, they can use historical faults or
seeded faults to empirically decide the value of k.

4 RESEARCH METHOD

To investigate the performance of our proposed AGA ap-
proach, we design comprehensive experiments. In this sec-
tion, we brieﬂy introduce each component of our experi-
ments and their intentions.

1) The main experiment of this paper is designed to
conﬁrm the contributions of our approach, and thus we in-
vestigate the improvement of AGA and its component (i.e.,
AGA I and AGA C) over the GA algorithm. In particular,
this experiment is conducted on 55 open-source subjects.
Details of this experiment are referred to Sections 5 and 6.
Note that the experiment in Section 3.2 also shares the same
setup and RQ1 complements the experiment in Section 3.2.
This part of experiment can show the superiority of AGA in
widely-used open-source subjects.

2) Although we aim to improve the efﬁciency of GA, we
are also curious about how AGA performs compared with
other TCP techniques. Speciﬁcally, FAST targets the TCP
efﬁciency problem and its goal is close to ours. Therefore,
we ﬁrst compare AGA with FAST, and then with other
representative TCP techniques, including ART-D, GA-S, and
GE. This experiment is in Section 7. This part of experiment
can show that AGA even outperforms techniques that aim
to reduce TCP time cost while sacriﬁcing effectiveness.

3) To show the practical usage of our approach, we
conduct an industrial case study on Baidu, which is a
famous Internet service providers with over 600M monthly
active users. Speciﬁcally, we compare AGA with GA, FAST,
ART-D, GA-S, and GE, respectively and the experiment is
in Section 8. This part of experiment can show that AGA
works also well in real-world industrial applications and
we receive positive feedback from Baidu.

5 EVALUATION DESIGN

We conducted experiments to evaluate our AGA approach.
The experiments was performed on a server whose CPU is
Intel(R) Xeon(R) E5-2683 2.10GHz with 132GB memory and
whose operating system is Ubuntu 16.04.5 LTS. To make a
fair comparison of time cost, we conducted all experiments
on a single thread without parallel execution.

In order to make our results more reliable and let readers
reuse the artefacts, we share our data, analysis scripts, and
detailed data tables online. They are publicly available on
our website: https://github.com/Spiridempt/AGA, and also
on ﬁgshare: https://ﬁgshare.com/s/cf8cc6ba9259c0e0754d.

5.1 Research Questions

As our AGA approach consists of two parts, time com-
plexity reduction (AGA C) and iteration number reduction
(AGA I), the ﬁrst two research questions are to investig-
ate their impacts, separately. Note that the ﬁrst research
question also complements the experiment in Section 3.2.

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

7

The third research question is designed to investigate the
performance of the whole AGA approach by comparing
it with the GA algorithm. To investigate the inﬂuence of
coverage type, the fourth research question is designed to
investigate whether AGA can also improve the efﬁciency of
GA with method coverage.

To sum up, this experiment is to answer the following

three research questions.
RQ1: How does our reduction of iteration number perform
compared with the GA algorithm in terms of efﬁciency?
RQ2: How does our reduction of the time complexity per-
form compared with the GA algorithm in terms of efﬁ-
ciency?
RQ3: How does our AGA approach perform compared with
the GA algorithm in terms of effectiveness and efﬁciency?
RQ4: Can our AGA approach also improve the efﬁciency
when method coverage is used?

5.2 Subjects and Faults

Subjects. In this work, we use 55 open-source projects in
total. Among these projects, 33 are widely used in prior
work [17], [20], [21], the others are the most popular subjects
selected from GitHub according to the number of stars.
Speciﬁcally, we target Github subjects whose primary pro-
gramming language is Java and order them according to
the number of stars in Jan 2019. Then, we check the ﬁrst
100 subjects and keep only the ones that are code repository
and the required tools (e.g., Maven, Clover, PIT, which is
explained in Section 5.3) could work. All the open-source
projects used in this work are written in Java, whose number
of lines of code is from 1,621 to 254,284. Each of these pro-
jects has a test suite written in JUnit Testing Framework. The
detailed information is given in Appendix B (Table 6). It is
worth noting that compared with the experimental dataset
used in recent TCP work [18], [22], [23], our dataset is larger
and contains more large-scale projects, which can make our
experimental results more reliable and convincing.
Faults. As existing work [24], [25], [26] have demonstrated
mutation faults to be suitable for software testing exper-
imentation and mutation faults are widely used in prior
work [7], [17], [22], [27], [28], [29], [30], [31] to evaluate
test case prioritization, we use a widely-used mutation
testing tool PIT [32] to generate mutants for all open-source
subjects. In particular, for each subject, ﬁrst, we generate all
mutants. Second, we keep the mutants that are killed by at
least one failing test case2. Third, we construct one mutation
group for each subject by containing all the remaining muta-
tion faults, which is also consistent with previous work [12].

branches. Additionally, we also design a research question
to investigate whether AGA still improves efﬁciency in the
scenario of method coverage. The implementation code and
all scripts used in this work are written with Python.

In prior work on coverage based test case prioritization,
some takes the adjacency matrix as input [12], while some
uses the adjacency list [18]. In this work, in order to make
a more general comparison, on one hand, we utilize the
GA implementation in [18], which is a relatively efﬁcient
implementation and uses adjacency list as input, and we
implement AGA based on adjacency list. On the other hand,
we implement GA and AGA based on adjacency matrix, too.
Due to the space limit, in the experimental results, we only
report the results based on adjacency list [18], which can be
more reliable, and the detailed results based on adjacency
matrix are put on the website.

It is worth mentioning that in our experiments, when ties
happen (i.e., more than one test case has the same number
of covered elements), AGA/GA selects the topmost test case
in the test-list (given by developers).

5.4 Compared Prioritization Approaches

Besides the proposed AGA approach and the GA ap-
proach [3], in this study we also implemented the GA-
ﬁrst approach proposed by Rothermel et al. [5]. The GA-
ﬁrst approach [5] applies the greedy additional strategy
only in the ﬁrst iteration, and deals with the remaining test
cases by other prioritization approach, e.g., the Greedy Total
approach in this paper, which schedules these test cases
based on the descendent order of the number of covered
program elements.

5.5 Measurement

In this study, similar to existing work [3], [5], we used the
Average Percentage of Fault Detected (APFD) to measure
the effectiveness of TCP approaches. Formula (2) presents
how to calculate APFD values for a subject with n tests
and m faults. Typically, T Fi represents the ﬁrst test case’s
position in the test suite that detects the ith fault.

APFD = 1 −

T F1 + T F2 + ... + T Fm
nm

+

1
2n

(2)

Besides, we used the total time spent during the TCP
process to measure the efﬁciency of a TCP approach. For
fair comparison, we included the preparation time for a
TCP approach, i.e., the time spent in constructing extra data
structures in the AGA approach.

5.3 Implementation and Supporting Tools

We used Clover [33] to collect code coverage information
including both statement coverage and method coverage for
each open-source subject. In this work, most experiments are
conducted on statement coverage because it is the mostly
studied test case prioritization granularity and its low-
efﬁciency problem is severe. In other word, the number
of statements is larger than the number of methods and

2 That is, the subject and the mutant produce different outputs on at least

one test case

5.6 Threats

The internal threats to validity mainly lie in the imple-
mentation of studied approaches and scripts used in the
experiments. To reduce this threat, the ﬁrst two authors
reviewed all the implementation and scripts used in this
work. Also, to improve the reliability of our work, we reuse
some implementation code in previous work [18] to reduce
the threats.

The external threats to validity mainly lie in the subjects
and faults. To reduce the former threat, we used 55 widely
used open-source subjects in our study, which consist of 33

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

8

previously used subjects [17], [20] and 22 popular subjects
selected from GitHub. At the same time, as AGA is a general
approach, it is not biased towards the chosen projects. Note
that because the second part of our approach (iteration
number reduction) is empirically veriﬁed on our dataset,
the large dataset itself also addresses the threat that our
approach may be biased. Also, some prior work [34], [35]
shows that the relative performance of different test case pri-
oritization techniques on mutation faults may not strongly
correlate with the performance on real faults, depending
upon the attributes of the studied subjects, but we follow
the common practice to use mutation faults for open-source
projects following the preceding TCP work [7], [17], [22],
[27], [28], [29], [30], [31]. Additionally, to complement this
experiment, in Section 7, we also evaluate our approach on
real faults. In the future, we plan to conduct an extensive
study by using more projects with more real faults. In
addition, in this paper, we only target the GA algorithm and
compare AGA with it. On one hand, it is widely accepted
that GA remains one of the most effective strategies in terms
of fault-detection rate [7], [8], [10]. On the other hand, the
results of a recent work [12] shows that other black-box
techniques that do not use coverage information (e.g., [36],
[37]) are often less effective than GA. At the same time,
we also design another experiment in Section 7 to compare
AGA with other representative prioritization techniques.
Additionally, most of our experiments are conducted on
statement coverage because its wide usage and severe low-
efﬁciency problem. In fact, our analysis of AGA is regardless
of the scale of coverage matrix, and our theoretical improve-
ment is general for all types of coverage. We also include
RQ4 to empirically verify our improvement on method
coverage. Another minor threat is induced by the diversity
of used subjects, which may lead to misleading statistics
of our results. To address this threat, besides reporting the
mean and median values, we also draw violin plots to learn
the data distribution, which are shown on our website.

6 RESULTS AND ANALYSIS

In this section, we analyze the experimental results on open-
source projects and answer the four research questions.

6.1 RQ1: Efﬁciency of Iteration Number Reduction

In this section, we further investigate the efﬁciency im-
provement of the iteration number reduction. According
to Section 3.2, we implement our approach with iteration
number reduction alone by setting k = 10 and call this
implementation AGA I. In other words, in this subsection,
we assess the contribution of iteration number reduction
alone (without the time complexity reduction).

The results on the 55 open-source projects are given in
Table 7 (Appendix C)3, where the projects are sorted in
ascending order of source lines of code (SLOC) and the
ﬁrst two columns present the results for RQ1. TimeGA
presents the time cost of the GA approach, whereas TimeI
represents that of AGA I. The speedup ratio of AGA I over
GA is 1.08X. It is apparent that most subjects have a small

3 Due to the space limit, we put the results of several research questions

into one table and put the table in Appendix C.

iteration number in GA (less than or slightly more than
10). Therefore, AGA I does not improve the efﬁciency much
for them. However, for those subjects with a large iteration
number, AGA I could reduce their time cost.

To statistically check the differences between AGA I and
GA, we adopt hypothesis test. We ﬁrst use Shapiro-Wilk
test [38] to check the normality of residuals, and the p-
value in AGA I and GA is 9.416 ∗ 10−16 and 5.239 ∗ 10−16,
which reject the hypothesis that they are normally distrib-
uted. Therefore, we need to adopt a non-parametric test.
As we need to include project size as a control variable,
Wilcoxon rank sum test [39] cannot be used. We seek for
the proportional odds regression [40], which is a class of
generalized linear models and is equivalent to Wilcoxon
rank sum test when there is a single binary covariate.
We introduce a variable “group” representing AGA I and
GA and take project size as a control variable. The results
show that the p-value of “group” is 1.380 ∗ 10−6, indicating
signiﬁcant difference between AGA I and GA, and the effect
size (Cohen’s d [41]) is 0.274 (medium effect). Here, because
statistical tests of normality (e.g., Shapiro-Wilk test) might
be impacted by characteristics of the data, we draw the
normal probability plots additionally and put them on our
website. Note that this applies to all normality checks in the
following of the paper.

6.2 RQ2: Efﬁciency of Time Complexity Reduction

The extra data structures deﬁned in our AGA C approach
do not affect the prioritization results, but reduce the time
complexity of prioritization. In this section, we compared
AGA C with GA only in terms of time cost. Note that we
did not implement AGA I in this research question.

The results are given by the ﬁrst ﬁve columns (except
the third column) of Table 7 (Appendix C), where TimeGA
presents the time cost of the GA approach and TimeC
represents that of AGA C. Moreover, we mark the results
of TimeC with (cid:88) only if TimeC < TimeGA.

The last row summarizes the total number of subjects
where AGA C outperforms the GA approach. The res-
ults show that in most projects (48 out of 55 open-source
subjects), the time cost of AGA C is lower than the GA
approach [3], which conﬁrms our previous theoretical ana-
lysis in Section 2. As we can see, in smaller subjects, the
differences between GA and AGA C are very small, which
may be caused by precision errors resulting from calculation
or the operating system. In larger subjects, their differences
are very large, which indicates the efﬁciency of AGA C.

In order to make our experiments comprehensive, we
compared AGA C with the GA-ﬁrst approach, whose time
cost is given by the fourth column TimeGAF of Table 7
(Appendix C). In 36 open-source subjects, AGA C is even
more efﬁcient than the GA-ﬁrst approach, which applies the
time-consuming additional strategy for only one iteration.

In general, the efﬁciency improvement of AGA C is usu-
ally very large. In particular, if we deﬁne TimeGA/TimeC
as the speedup ratio of AGA C over GA for a project, the
average speedup ratio is 4.37X. As small time cost may yield
biased speedup ratio, also in order to show the perform-
ance of AGA in projects with different sizes, we classify
all 55 projects into small-size, middle-size, and large-size,

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

9

according to the SLOC. The small-size projects (S1 to S22
in Table 7 (Appendix C)) all have less than 5,000 SLOC, the
middle-size projects (S23 to S41) all have 5,000-20,000 SLOC,
and the other large-size projects have more than 20,000
SLOC. The results show that the average speedup ratio in
the three categories is 2.16X, 4.65X, and 7.44X, respectively.
So, the reduction of time complexity (AGA C) performs
well, especially in projects with large sizes. In order to
give a more deep view into the distribution and variation
of speedup ratios, we further present the violin plot with
included box plot in Figure 1. The X-axis represents all
projects and projects in three categories, respectively. We
put the violin plots and box plots together to better present
the distributions. From the plots, the speedup ratio of large-
size projects tends to be slightly larger than that of small-
size projects. Moreover, from the plot of large-size projects,
several projects have very large speedup ratio because their
scale is also large.

Figure 1: Speedup Ratios Distribution of AGA C over GA
on Open-Source Projects

To statistically check the differences between AGA C
and GA, we perform hypothesis testing similar to the above.
We ﬁrst use Shapiro-Wilk test [38] to check the normality of
residuals, and the p-value in AGA C and GA is 4.207∗10−15
and 5.239 ∗ 10−16, which reject the hypothesis that they
are normally distributed. We also use the proportional odds
regression [40] and include project size as a control variable.
The results show that the p-value of “group” is 0.038, indic-
ating signiﬁcant difference between AGA C and GA, and
the effect size (Cohen’s d [41]) is 0.234 (medium effect).

Besides, we also calculate the speedup ratios of AGA C
over GA-ﬁrst for a more complete comparison. The average
speedup ratio is 3.01X, and the average speedup ratio in the
three categories is 1.26X, 3.31X, and 5.35X, respectively. This
shows our AGA approach is also superior to GA-ﬁrst.

To statistically check the differences between AGA C
and GA-ﬁrst, we perform the similar procedure as above.
We ﬁrst use Shapiro-Wilk test to check the normality of
residuals, and the p-value in AGA C and GA-ﬁrst is 4.207 ∗
10−15 and 3.828 ∗ 10−16, which reject the hypothesis that
they are normally distributed. We also use the proportional
odds regression [40] and include project size as a control

variable. The results show that the p-value of “group” is
0.399, indicating no signiﬁcant difference between AGA C
and GA-ﬁrst, and the effect size (Cohen’s d) is 0.208 (me-
dium effect).

Provided adjacency matrix as input, we also implemen-
ted GA and AGA C, and the detailed results are on our
website. Speciﬁcally, the average speedup ratio of AGA C
over GA is 24.18X, and the average speedup ratio in the
three categories is 5.47X, 28.16X, and 48.19X, respectively.

Besides, the speedup ratios of the AGA C approach
vary a lot in different projects. On 19 open-source subjects
AGA C is less efﬁcient than GA-ﬁrst. On the one hand, the
iteration numbers of these projects are high so that AGA C
becomes a bit costly. On the other hand, in the only iteration
of GA-ﬁrst, few test cases are needed to cover all statements
and they are selected fast so that GA-ﬁrst is efﬁcient on these
projects.

To sum up, AGA C addresses the high-complexity prob-
lem of GA well and successfully reduces its time complexity.
For any project, any scale of coverage matrix, our approach
could improve the efﬁciency a lot.

Conclusion to RQ2: The time complexity reduction
strategy used in our AGA approach demonstrates
great efﬁciency improvement compared to GA. Spe-
ciﬁcally, the average speedup ratio of AGA C over
GA is 4.37X/24.18X on two types of input.

6.3 RQ3: Comparison with Greedy Additional Ap-
proaches

In this section, we compare the effectiveness and efﬁciency
between the proposed AGA approach and two Greedy
Additional approaches (including both GA and GA-ﬁrst),
whose results are given by the ﬁrst ninth columns (except
the third and ﬁfth column) of Table 7 (Appendix C), where
APFDAGA and TimeAGA represent the APFD results and
time cost of the AGA approach whose iteration number
is set to be 10. Moreover, when the GA approach [3] does
not outperform the corresponding AGA approach [3], i.e.,
APFDAGA ≥ APFDGA or TimeAGA < TimeGA, the corres-
ponding results of the AGA approach is marked with (cid:88).

6.3.1 Effectiveness

The proposed AGA approach has the same or better APFD
performance as the GA approach in 51 out of 55 open-source
subjects, and the average APFD value of AGA is 0.8870,
which is the same as GA. On some subjects (e.g., the open-
source project whose ID is S44), the AGA approach does
not outperform the GA approach, but their APFD difference
is usually very small (e.g., 0.0021 for this subject). We also
make extra comparisons of AGA and GA-ﬁrst and ﬁnd that
AGA has the same or better APFD performance as GA-
ﬁrst in 45 out of 55 open-source subjects and their average
APFD values are the same. On 14 projects, neither the
AGA approach nor the GA approach outperforms the GA-
ﬁrst approach, but their differences are small. Through our
analysis, we suspect that after the ﬁrst iteration, although all
elements have been covered, the numbers of times that each
element is covered still differ. This means test cases with a

010203040Totaln=55Small−sizen=22Middle−sizen=19Large−sizen=14Categories of different sizesSpeedup_ratioTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

10

small number of times being covered should have higher
priority, but in later iterations, this information is ignored.

Moreover, we statistically analyze whether the AGA
approach and the Greedy Additional approaches have sig-
niﬁcant difference on their APFD values. First, we conduct
the Shapiro-Wilk test to check the normality of residuals.
The p-value of AGA, GA, and GAF is 0.328, 0.298, and 0.283,
indicating we cannot reject the hypothesis that they are
normally distributed. We additionally perform Shapiro-Wilk
test to check the normality of residuals, and the p-value of
AGA, GA, and GAF is 0.328, 0.298, and 0.283, indicating we
cannot reject the hypothesis that they are normally distrib-
uted. Therefore, we can use parametric test in the following.
We use Bartlett’s test [42] to check the homogeneity of
variance, and the p-value is 0.880, indicating we cannot reject
the hypothesis that they have equal variance. Then, as we
need to take project size as a control variable (covariate), we
use Analysis of Covariance (ANCOVA) [19], a parametric
test that works on two or more groups to check whether
different groups have the same means. The p-value is 0.641,
indicating we cannot reject that they have the same means.
Then, pairwise ANCOVA tests show that the p-values of
AGA vs. GA, AGA vs. GAF, and GA vs. GAF are 0.981,
0.427, and 0.414. In other words, the probability that AGA
is as competitive as GA is more than 98%. Then, we employ
Cohen’s d [41] to compute the effect size (ES), and the results
in AGA vs. GA, AGA vs. GAF, and GA vs. GAF are 0.005,
0.151, and 0.156, which are all small effects. Furthermore, we
conduct Tukey’s range test [43] to check the 95% conﬁdence
intervals for all pairwise differences, and the results are
[-0.022, 0.022], [-0.030, 0.015], and [-0.030, 0.014].

6.3.2 Efﬁciency

According to Table 7 (Appendix C), in almost all subjects
(i.e., 44 out of 55), the time cost of AGA is much lower than
the GA approach. On average, the speedup ratio of AGA
over GA is 5.95X. Moreover, the speedup ratios in small-size,
middle-size, large-size projects are 2.26X, 6.69X, and 10.76X,
respectively. To learn the distribution of speedup ratios in
small-size, middle-size, large-size projects, we also present
the violin plot with included box plot in Figure 2. From this
ﬁgure, most medium-size and large-size projects achieve
higher speedup ratios than small-size projects. Moreover,
AGA achieves very large speedup ratios on some large-
size projects. So, AGA scales up well in large-size projects.
Furthermore, we compared the time cost of the AGA ap-
proach with the GA-ﬁrst approach, which requires less time
than the GA approach, and ﬁnd that the AGA approach
even outperforms the GA-ﬁrst approach in 37 open-source
subjects. The average speedup ratio is 3.95X, and the av-
erage speedup ratio in the three categories is 1.36X, 4.39X,
and 7.44X, respectively. Here, we notice that the speedup
ratio of AGA over other approaches is sometimes less than
1 (e.g., S3, S4, S7). In fact, the overall time complexity
analysis is meaningful only when the parameters are large
enough. In our dataset, some projects have a relatively small
m value. In this case, although O(mn) seems to be small, its
coefﬁcient is not negligible compared to m. In other words,
the preliminary data structure setup consumes much time
and it impacts the overall running time in some cases. This is
also consistent with the empirical results that AGA performs

better on large projects. On the other hand, the adjacency
lists in some projects are very dense, which takes much time
in the preparation of data structure, and further leads to a
large coefﬁcient. For example, S7 and S42 have relatively
small m values (45 and 34) and dense adjacency lists.

Figure 2: Speedup Ratios Distribution of AGA over GA on
Open-Source Projects

Provided adjacency matrix as input, we also implemen-
ted GA and AGA. The average speedup ratio of AGA over
GA is 27.72X, and the average speedup ratio in the three
categories is 5.84X, 35.47X, and 51.59X, respectively.

To sum up, not surprisingly, the speedup ratio of AGA
is higher than AGA C and AGA I. After combining AGA I
and AGA C, our whole AGA approach obtains more ef-
ﬁcient results while preserving high effectiveness. At the
same time, the proposed AGA approach is demonstrated
to be efﬁcient especially on large-scale projects. In fact,
the surprisingly high efﬁciency of the AGA approach also
indicates the existence of many redundant accesses of data
and it is ubiquitous in most projects.

Conclusion to RQ3: The AGA approach requires
much less time in prioritization than the GA ap-
proach and the average speedup ratio is 5.95X and
27.72X on two types of input. Also, AGA is as
competitive as the latter in terms of APFD values
(with no signiﬁcant difference). This means that we
achieve our goal in this paper and it has promising
use in practice.

6.4 RQ4: Performance on Method Coverage

In previous research questions, we focus on statement-
level coverage because it is the mostly studied coverage
criterion and its low-efﬁciency problem is more severe than
other granularities. In this section, we collect the method-
level coverage for each of our 55 subjects and compare the
efﬁciency of AGA and GA. The results are shown in Table 3.
For each subject, we report the running time (in seconds) of
GA and AGA.

According to Table 3, in almost all subjects, the time
cost of AGA is much lower than GA. On average, the

0204060Totaln=55Small−sizen=22Middle−sizen=19Large−sizen=14Categories of different sizesSpeedup_ratioTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

11

speedup ratio of AGA over GA is 6.02X. Moreover, the
speedup ratios in small-size, middle-size, large-size projects
are 2.28X, 7.32X, and 10.13X, respectively. Compared to the
results on statement coverage in Section 6.3, the speedup
ratios are almost the same for all projects and projects in
different sizes. This conﬁrms that AGA also works well on
method coverage.

In fact, the complexity analysis of GA and our AGA
approach is based on a general (0,1) matrix, regardless of
the meaning behind it. In other words, the type of program
element (e.g., statement, method) does not affect any aspect
of AGA, which means our approach works on any coverage
and has a stable improvement.

Conclusion to RQ4: The AGA approach also works
on method-level coverage. Speciﬁcally, the average
speedup ratio of AGA over GA is 6.02X.

7 EMPIRICAL COMPARISON WITH REPRESENTAT-
IVE PRIORITIZATION TECHNIQUES
In this section, we present an experiment comparing AGA
with some representative prioritization techniques. In par-
ticular, as FAST targets the TCP efﬁciency problem and
thus is closet to our goal, we ﬁrst present the comparison
study with FAST in Section 7.1. Then we present the com-
parison study with other representative TCP techniques in
Section 7.2.

7.1 Comparison with FAST

In this section, we investigate the performance of AGA
with its most related work FAST [18]. In particular, FAST
is proposed as a TCP approach to address the general
TCP efﬁciency problem by sacriﬁcing the TCP effective-
ness, and it is shown to be more efﬁcient than other TCP
techniques [18]. Note that there is no other work in the
literature focusing on the same objective as ours, and thus
we compare AGA against FAST. However, AGA and FAST
target at slightly different goals: FAST approach focuses on
the efﬁciency problem of test prioritization, not speciﬁc to
GA approaches. Although FAST targets a different goal, it is
still interesting to learn how AGA performs compared with
FAST in terms of time cost since both AGA and FAST can be
viewed as addressing the efﬁciency problem. However, as
FAST improves efﬁciency while sacriﬁces effectiveness, the
comparison in terms of time cost is a bit “unfair” for AGA.
In this study, we compare the performance of AGA and
FAST on both the 55 open-source projects used in Section 5
and Defects4J [44], which is the largest real-fault benchmark
(i.e., a set of projects with reproducible real bugs) widely
used in test case prioritization [35], [45], [46], [47], [48]
and fault localization [49], [50], [51], [52], [53]. For ease of
understanding, we present the results of the former subjects
with seeded faults and the results of the latter subjects with
real faults separately.

The FAST approach borrows algorithms commonly used
in the big data domain to ﬁnd similar items and con-
tains a family of similarity-based test case prioritization
approaches. In general, the authors proposed two categories
of FAST, While-box (WB) and Black-box (BB). BB approaches
take test code as input, while WB approaches take program

coverage as input. As WB approaches have the same input
as us and are much faster than BB approaches, we compare
our work with WB approaches [18]. WB approaches include
ﬁve algorithms FAST-pw, FAST-all, FAST-1, FAST-log, and
FAST-sqrt, whose difference lies in how many test cases are
randomly selected for prioritization at a time. In this section,
we implemented this family, and for each subject, we com-
pared the best results of this family with AGA. Speciﬁcally,
according to prior work [18], none of the algorithms in FAST
family always performs the best. Therefore, to show the
superiority of our approach, we run all FAST algorithms
and select the best one for each project. In other words,
when comparing APFD, we keep the highest APFD, and
when comparing time cost, we keep the lowest time cost.
Moreover, due to the randomness in FAST, for each subject
we applied each of these approaches 10 times and used their
median effectiveness and efﬁciency results. Regarding the
time cost, the same as Section 5, we measure the efﬁciency
of a TCP approach by including its preparation time, i.e., the
preparation time used in FAST4.

7.1.1 FAST Results on Seeded Faults

The results of FAST are shown by the tenth and twelfth
columns in Table 7 (Appendix C). Due to space limit, we
do not present the results of all the ﬁve FAST algorithms,
but the largest APFD value and smallest time cost among
them for each subject. Note that usually a FAST algorithm
cannot achieve both the largest APFD value and the smallest
time cost. As the APFD results and time cost of AGA is
already given by the eighth and ninth columns, we use
column WinAPFD and column WinTime to show whether
APFDAGA ≥ APFDFAST and TimeAGA < TimeFAST,
respectively.

Regarding to APFD values, the AGA approach is much
better than FAST in all subjects. More speciﬁcally, the differ-
ences between them are from 0.0456 to 0.3039, and 0.1702 on
average. To statistically check their differences, we follow
the similar procedure as above. We ﬁrst use Shapiro-Wilk
test to check the normality of residuals, and the p-value in
AGA and FAST is 0.328 and 0.137, which cannot reject the
hypothesis that they are normally distributed. Then, taken
project size as a control variable, the Analysis of Covariance
(ANCOVA) shows that p-value < 2 ∗ 10−16, indicating the
statistically signiﬁcant difference between AGA and FAST.
Moreover, the effect size (Cohen’s d) is 2.96 (huge effect) and
Tukey’s range test shows that the 95% conﬁdence interval
of their difference is [0.149, 0.192]. To sum up, AGA signi-
ﬁcantly outperforms FAST in terms of APFD because FAST
algorithms are designed to sacriﬁce prioritization accuracy
to achieve high efﬁciency by using hash signatures.

Regarding to the time cost, the time cost of AGA outper-
forms FAST on 52 out of 55 open-source subjects, and the
speedup ratio of AGA over FAST is 4.29X. To statistically

4 The previous work FAST [18] separated their total running time into
preparation time and prioritization time in their evaluation. However,
preparation happens only once in BB approaches while not in WB
approaches, because the input of BB approaches is test code. Given
updated source code but out-of-date coverage information (from the
previous version), we need not prioritize again and TCP results will
not change. Otherwise, with updated coverage information, the whole
process (including preparation) has to be repeated.

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

12

Table 3: Results of Open-Source Subjects (Method-Level)

Project TimeGA TimeAGA Project TimeGA TimeAGA Project TimeGA TimeAGA

S1
S4
S7
S10
S13
S16
S19
S22
S25
S28
S31
S34
S37
S40
S43
S46
S49
S52
S55

0.0030
0.0021
0.0007
0.0145
0.0068
0.0106
0.2907
0.0111
1.1046
0.1945
0.4390
0.8556
0.7977
0.6570
0.2057
0.0182
0.0230
0.2423
190.9669

0.0031
0.0116
0.0012
0.0081
0.0048
0.0024
0.1007
0.0055
0.1654
0.0347
0.0495
0.0824
0.0594
0.0963
0.0211
0.0044
0.0183
0.0652
2.9963

S2
S5
S8
S11
S14
S17
S20
S23
S26
S29
S32
S35
S38
S41
S44
S47
S50
S53

0.0011
0.0051
0.0032
0.0158
0.0158
0.0741
0.0024
0.0502
0.0131
1.2958
0.3495
0.2642
0.5507
0.3203
1.3687
0.0183
0.0011
0.1631

0.0010
0.0014
0.0028
0.0080
0.0076
0.0146
0.0023
0.0147
0.0043
0.3499
0.0589
0.0330
0.0445
0.0463
0.1088
0.0045
0.0007
0.0369

S3
S6
S9
S12
S15
S18
S21
S24
S27
S30
S33
S36
S39
S42
S45
S48
S51
S54

0.0014
0.0019
0.2587
0.0027
0.0036
0.0047
0.0098
0.0035
0.0041
0.0624
0.1714
31.6469
6.6268
0.0011
0.0202
0.0010
1.0812
15.7745

0.0013
0.0007
0.0504
0.0020
0.0008
0.0013
0.0064
0.0031
0.0020
0.0177
0.0162
3.8975
0.3601
0.0006
0.0034
0.0003
0.0883
1.1856

check their differences, we follow the similar procedure as
above. We ﬁrst use Shapiro-Wilk test to check the normality
of residuals, and the p-value in AGA and FAST is 4.92∗10−15
and 4.05 ∗ 10−15, which reject the hypothesis that they are
normally distributed. Therefore, we use the proportional
odds regression [40] and include project size as a control
variable. The results show that the p-value of “group” is
4.250 ∗ 10−4, indicating signiﬁcant difference between AGA
and FAST, and the effect size (Cohen’s d) is 0.286 (medium
effect). That is, the proposed AGA is more efﬁcient to
FAST (with 4.29X speedup ratio). This is a surprising result
because AGA can even be faster than a technique that
is designed to sacriﬁce effectiveness to reduce time cost.
We also present the violin plot with included box plot in
Figure 3. On larger projects, the speedup ratios are smaller,
which means FAST also scales up well on large-size projects,
whereas it is less efﬁcient than AGA.

7.1.2 FAST Results on Real Faults

Besides, as FAST is evaluated by some subjects of De-
fects4J [44] in the previous work [18], we apply the AGA
approach to these subjects by reusing their artifact package
(including subjects and code) for fair comparison. Moreover,
we add the experiment on Mockito, which is also in De-
fects4J but does not appear in the experiment of FAST. De-
fects4J is the largest real-fault benchmark, so this experiment
complements the previous experiments on seeded faults and
can evaluate AGA on real faults. The comparison results
are given by Table 4, where WinAPFD and WinTime show
whether the proposed AGA approach outperforms FAST in
terms of APFD and time cost, respectively. From this table,
AGA is more effective than FAST algorithms on 5 out of 6
projects and it achieves better time efﬁciency on all 6 projects
(with 5.24X as average speedup ratio), which indicates the
superiority of AGA. Also, from this experiment, we show
that AGA is superior on real faults, too.

Figure 3: Speedup Ratios Distribution of AGA over FAST on
Open-Source Projects

Actually, it is worth pointing out that as the authors
of FAST [18] stated, no single FAST algorithm can be the
best, which means the most effective algorithm in FAST may
lead to somewhat higher time cost and the most efﬁcient al-
gorithm in FAST may lead to somewhat lower APFD value.
That is, the results of FAST in Table 7 (Appendix C) and
Table 4 are not results of one FAST algorithm, but the best
results of all FAST algorithms. Moreover, even compared
with these results, AGA is still promising considering both
effectiveness and efﬁciency.

Considering the advantageous of AGA over FAST, it is
interesting to analyze the secrets behind the observation.
FAST approach achieves the efﬁciency improvement by
using the algorithms used in big data domain to summar-
ize the key information in coverage, but suffers from the
effectiveness loss to some extent because some information

0246Totaln=55Small−sizen=22Middle−sizen=19Large−sizen=14Categories of different sizesSpeedup_ratioTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

13

Table 4: Results of Some Defects4J Projects

FAST

AGA

APFD

0.5219
0.5471
0.5627
0.5463
0.5264
0.5197

Time APFD WinAPFD

Time WinTime

11.7302
5.2600
0.4280
2.8633
5.1456
2.7311

0.4347
0.6992
0.6094
0.5469
0.7128
0.5975

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

5

2.0408
0.9586
0.1513
0.5042
0.9030
0.4537

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

6

Projects

Closure
Math
Lang
Time
Chart
Mockito

Total

APFD range

GAF

APFD

Time

0.0006
0.0000
0.0000
0.0034

0.4354
0.6992
0.6094
0.5436
NA 0.7128
0.5961

0.0014

9.4078
7.4710
0.2150
0.9870
5.0767
2.7539

is missing in the summarization. AGA consists of two parts,
time complexity reduction and iteration number reduction.
In particular, the former part is to use some extra data struc-
tures (e.g., indices) to summarize the coverage information
of each test case, i.e., the statements covered by each test.
With these data structures, AGA does not need to scan the
coverage table whenever a test case is selected, and thus the
time cost of AGA reduces but its effectiveness maintains. To
sum up, FAST suffers from effectivness loss because it uses
simpliﬁed information, while AGA does not because it uses
the same information as before but in an easy-to-access way.
Moreover, similar to Table 2, we compute the gaps
between the highest and lowest APFD among all iteration
numbers for Defects4J subjects, which are shown in Column
“APFD range” of Table 4. The range of “Chart” is marked
as “NA” because it has only one iteration. As we can
see, the gaps are extremely small, which also conﬁrms the
conclusion in Section 3.

Additionally, Column “GAF” of Table 4 shows the res-
ults of GA-ﬁrst. AGA is much more efﬁcient than GAF
while achieves larger APFD, which is consistent with the
conclusion in Section 6.3.

Conclusion: Surprisingly, AGA can achieve 4.29X
speedup ratio compared to FAST, which targets im-
proving time efﬁciency while sacriﬁcing effective-
ness. At the same time, the experimental results
show that AGA is signiﬁcantly better than FAST in
terms of APFD values, and the average difference
between them is 0.1702.

largest number of uncovered elements among those in
the “spanning set”. Here, an element subsumes another
if covering the former guarantees covering the latter:
The notion of a spanning set denotes the subset of non-
subsumed elements.

• GE [8] is a genetic algorithm, which is a represent-
ative of search-based prioritization techniques and is
evaluated to be effective. In each iteration, it uses a
ﬁtness function to select individuals and then applies
crossover and mutation operators to generate new in-
dividuals. Speciﬁcally, an individual (a sequence) is
encoded as an array where each value indicates the
position of a test case; The ﬁtness function is deﬁned by
Baker’s linear ranking algorithm [55]; The crossover op-
erator selects two parents and each of the two offspring
is formed by combining the ﬁrst several values in one
parent and the remaining values in the other parent;
The mutation operator randomly selects two values in
an individual and exchanges their positions.

In this section, we reuse the implementation of ART-D,
GA-S, and GE in [18], [22] and compare them with AGA on
the 55 open-source projects. Considering the randomness
of these techniques, each of them is run 10 times. The
remaining setting of this experiment is the same as Section 5.
Due to the space limit of Table 7 (Appendix C), we put the
results in Table 5. In Table 5, each row represents one project,
and the running time and APFD of AGA, ART-D, GA-S, and
GE are shown separately.

7.2 Comparison with other TCP Techniques

Although only FAST has a close goal to ours, to better evalu-
ate AGA, we also compare it with more representative TCP
techniques. In particular, in this study we use the following
TCP techniques whose input is only coverage information
and which have been widely used in the literature [18], [31].
• ART-D [10] is a family of adaptive random-based TCP
techniques guided by coverage information. At each
iteration, a candidate set is dynamically created by
randomly picking test cases from the set of not-yet-
prioritized test cases as long as they can increase cover-
age. The test case in the candidate set that is the farthest
away from the set of prioritized test cases is selected.
• GA-S (Additional Spanning) [54] is a variant of GA
that at each iteration picks the test case that covers the

The average speedup ratio of AGA over ART-D is
144.58X. Moreover, in all 55 projects, the APFD values of
AGA are larger than ART-D, and the average APFD dif-
ference is 0.1384. That is, AGA always outperforms ART-D
in terms of both effectiveness and efﬁciency. The average
speedup ratio of AGA over GA-S is 182.27X. In 54 out of 55
projects, the APFD values of AGA is larger than GA-S, and
the average APFD difference is 0.0708. The average speedup
ratio of AGA over GE is 285.91X. In 50 out of 55 projects,
the APFD values of AGA are larger than GE, and the
average APFD difference is 0.0459. That is, compared with
the three TCP techniques, our proposed AGA achieves both
effectiveness and efﬁciency. Moreover, the time cost and
APFD values of the compared TCP techniques distribute
in a larger range than AGA, indicating that the latter can
achieve stably promising performance.

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

14

Conclusion: As AGA aims to largely improve
the TCP efﬁciency while preserving the high-
effectiveness of GA, it outperforms ART-D, GA-S,
and GE in terms of both efﬁciency and effectiveness.

8 INDUSTRIAL CASE STUDY
To show the practical usage of our approach, we then
conducted an industrial case study as follows.

Baidu is a famous Internet service provider with over
600M monthly active users. In their regression testing infra-
structure, test case prioritization is frequently needed and
they have been adopting Greedy Additional (GA) strategy
for a long time because of its simple idea and relatively
high effectiveness. However, they often complain about the
long running time of GA, which deviates from the original
intention of test case prioritization, that is to accelerate the
process of detecting faults.

To check the performance of our AGA approach in real-
world scenarios, we collected 22 versions of ﬁve industrial
projects from Baidu, each of which is taken as a subject
in this study. More speciﬁcally, these subjects are collected
from Dec. 2017 to Feb. 2018 and Oct. 2018 to Nov. 2018,
and all of them are written in C. As shown in the ﬁrst three
columns in Table 8 (Appendix D), we summarize the SLOC
and number of test cases of each subject. The SLOCs range
from 20K to 500K while the numbers of test cases range
from 202 to 4,246. Besides, we used C-Cover [56] to collect
statement coverage for each industrial subject.

In Table 8 (Appendix D), we report the time cost of GA,
AGA, and FAST, respectively. When the time cost of AGA
is less than GA, we mark it with (cid:88). As we can see, in all 22
subjects, the time cost of AGA is much lower than that of
GA, and the speedup ratio is 44.27X on average. In general,
our AGA approach is demostrated to be efﬁcient on indus-
trial subjects from Baidu. For example, for the subject I1,
its original prioritization time is larger than 29,000 seconds,
which may be unbearable in practice. However, through
AGA, the prioritization time is reduced to less than 360
seconds. On the other hand, the surprisingly high efﬁciency
of AGA also indicates the ubiquitous existence of many
redundant accesses of data in industrial projects. We also
present the violin plot with included box plot in Figure 4 to
show the distribution and variation. As we can see, on most
projects, AGA has a large improvement compared to GA.

Provided adjacency matrix as input, the average spee-

dup ratio of AGA over GA is 61.43X.

After we report the results, developers in Baidu veriﬁed
(1) the time cost of our implementation of GA is close
to their inner implementations, and (2) the speedup ratio
is signiﬁcant and our technique improves their prioritiza-
tion efﬁciency, because their implementation only works on
small projects, not large projects.

In addition, we also compared our approach with FAST,
and the experimental setup is the same with Section 7. Sur-
prisingly, AGA outperforms FAST again. Speciﬁcally, on all
22 subjects, AGA is faster than FAST and the average spee-
dup ratio is 4.58X. To statistically check their differences, we
follow the similar procedure as above. We ﬁrst use Shapiro-
Wilk test to check the normality of residuals, and the p-value

Figure 4: Speedup Ratios Distribution of AGA over GA and
FAST on Industrial Projects

in AGA and FAST is 5 ∗ 10−4 and 2.5 ∗ 10−3, which reject
the hypothesis that they are normally distributed. Similary
to the above, proportional odds regression [40] is used and
we introduce a variable “group” representing AGA and
FAST and take project size as a control variable. The results
show that the p-value of “group” is 1.35 ∗ 10−6, indicating
signiﬁcant difference between AGA and FAST, and the effect
size (Cohen’s d) is 1.28 (very large effect). We also present
the violin plot with included box plot in Figure 4. Again, we
ﬁnd that on most projects, AGA has a large improvement
than FAST. In [18], the authors proposed FAST to solve
the scalability problem of TCP techniques with the decrease
of effectiveness. Their approach is evaluated to be efﬁcient
when the project size grows up rapidly. However, our AGA
approach is even more efﬁcient than FAST, and this means
AGA may scale up better and is practical in real-world
scenarios. Also, recall that when we compare AGA with
FAST on open-source projects, the p-value is larger and the
effect size is smaller than here, and we conjecture that this
is due to the relatively small sizes of open-source projects.

Additionally, besides FAST, which targets the TCP ef-
ﬁciency problem, we also compare AGA with other more
general TCP techniques as we have done in Section 7.
Speciﬁcally, we run ART-D, GA-S, and GE on the 22 subjects.
Considering the randomness of these techniques, each of
them is run 10 times. The results are shown in Table 8
(Appendix D). As we can see, these techniques are much
slower than AGA, even GA. The average speedup ratio of
AGA over ART-D, GA-S, and GE is 993.37X, 4230.53X, and
123.25X, respectively.

It is worth noting that on one hand, Baidu is sensitive
to the positions of detected faults in history, thus these
positions are not available to us. On the other hand, they
only provide coverage data after desensitization and we
do not have access to the source code of these subjects (to
create mutants) due to the conﬁdential policy. As a result, we
cannot compare the effectiveness of the three approaches in
terms of APFD in industrial subjects.

020406080AGA over GAn=22Speedup_ratio3.03.54.04.55.0AGA over FASTn=22Speedup_ratioTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

15

Table 5: Comparison with Other TCP Techniques on Open-Source Subjects

Project

AGA
Time APFD

ART-D
Time APFD

GA-S

GE

Time APFD

Time APFD

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S20
S21
S22
S23
S24
S25
S26
S27
S28
S29
S30
S31
S32
S33
S34
S35
S36
S37
S38
S39
S40
S41
S42
S43
S44
S45
S46
S47
S48
S49
S50
S51
S52
S53
S54
S55

0.0157
0.0058
0.0222
0.0789
0.0089
0.0046
0.0076
0.0180
0.3363
0.0247
0.0553
0.0106
0.0385
0.0385
0.0055
0.0152
0.1317
0.0226
0.6995
0.0469
0.0248
0.0215
0.0962
0.0187
1.0579
0.0294
0.0303
0.1132
2.3900
0.1804
0.3170
0.1270
0.0921
0.7903
0.3631
23.7849
0.3582
0.2794
2.2078
0.5942
0.1562
0.0287
0.2558
0.8465
0.0597
0.0858
0.0629
0.0025
0.1120
0.0047
1.1036
0.4052
0.2298
2.6648
18.1040

0.9070
0.8380
0.8848
0.8509
0.8527
0.8101
0.9059
0.8898
0.9144
0.9518
0.8766
0.8864
0.8615
0.9188
0.8582
0.8031
0.9183
0.9028
0.9033
0.8013
0.8328
0.8642
0.8198
0.9858
0.8401
0.8339
0.9614
0.9164
0.9490
0.9617
0.9426
0.8911
0.8662
0.9328
0.9467
0.9371
0.8507
0.8657
0.9545
0.9244
0.9106
0.8569
0.8924
0.9240
0.8464
0.8656
0.8750
0.7939
0.8009
0.8517
0.8671
0.9542
0.8710
0.9089
0.9544

0.0832
0.0224
0.0196
0.0478
0.1361
0.0957
0.0150
0.0752
53.9033
0.5895
0.4818
0.0708
0.1763
0.6426
0.1225
0.3687
7.8831
0.1431
34.4428
0.2180
0.4520
0.2999
4.8919
0.0420
70.1140
0.4066
0.0254
11.6980
130.3359
6.9994
46.5734
22.8351
11.0712
120.8907
32.4553
2,397.2400
33.6262
115.8419
1,114.6899
118.3193
19.2647
0.0123
32.1075
162.8027
0.8793
3.7066
0.7825
0.0095
0.8459
0.0164
203.2114
26.1336
13.8199
3,373.5142
55,120.6523

0.8440
0.7508
0.7681
0.5933
0.7405
0.6909
0.7711
0.7459
0.8821
0.8009
0.7640
0.7589
0.7816
0.7800
0.7285
0.6578
0.8316
0.7783
0.7553
0.7021
0.7156
0.7660
0.6979
0.9213
0.8099
0.6026
0.7695
0.7642
0.8254
0.8572
0.8342
0.7165
0.6612
0.8271
0.7277
0.8207
0.6876
0.7753
0.7931
0.7621
0.7195
0.7409
0.7331
0.8437
0.6822
0.6834
0.6884
0.6455
0.6463
0.5108
0.6595
0.8454
0.6956
0.7578
0.8613

0.5878
0.3312
0.1593
0.1079
0.6358
0.2560
0.1116
2.7672
33.7447
0.9681
10.6250
0.3043
2.0406
0.8556
0.2513
0.9745
7.8195
1.5687
374.3919
36.2190
1.5459
0.8994
4.2931
12.3507
387.4319
0.3047
0.2441
4.6810
7955.7540
28.4835
15.9675
3.1369
5.5717
128.1646
20.0292
1,976.4397
23.6568
35.4853
265.5048
17.4111
4.2101
0.2401
38.5073
147.3191
7.4841
17.5147
1.0181
0.0386
5.6348
0.0585
505.1426
200.9616
32.9077
252.3093
4536.0047

0.8812
0.7870
0.8108
0.6555
0.7662
0.6800
0.8255
0.8153
0.8995
0.9017
0.7950
0.8375
0.7667
0.8779
0.7117
0.6509
0.8893
0.8039
0.8381
0.7293
0.7676
0.8151
0.7782
0.9857
0.8426
0.6927
0.8579
0.8734
0.9180
0.9202
0.9191
0.8231
0.7356
0.8861
0.8694
0.8597
0.7621
0.7747
0.9289
0.8672
0.8454
0.8176
0.7915
0.9090
0.7811
0.7494
0.7588
0.7089
0.7600
0.6855
0.8080
0.8889
0.8101
0.8478
0.9292

0.0650
0.0660
0.0900
0.4440
0.9430
0.9140
0.0970
0.1160
38.4580
0.8060
0.4070
0.2970
0.2000
0.6880
0.7360
3.0660
2.7440
0.3120
4.1710
0.3410
1.5660
0.5080
3.6190
0.1230
3.5190
2.6380
0.2200
8.5760
3.7060
1.6830
11.7620
53.4990
23.6020
21.5730
5.1690
11.6880
1,979.6000
337.0780
168.0300
112.3900
40.8410
0.1300
24.0450
34.0820
4.6150
4.0400
3.6900
0.0790
4.3070
0.1830
61.9450
20.1360
33.7620
13,384.3880
13,516.8050

0.8747
0.8373
0.8705
0.8061
0.8310
0.7935
0.8855
0.8985
0.8949
0.9172
0.8501
0.8667
0.7783
0.9011
0.8470
0.7760
0.8785
0.8950
0.8528
0.8185
0.7899
0.8570
0.7866
0.9860
0.8283
0.7833
0.9501
0.8534
0.9131
0.9285
0.9065
0.7696
0.7568
0.8780
0.9105
0.8570
0.7165
0.8072
0.7733
0.8156
0.8116
0.8649
0.8208
0.8864
0.7918
0.8003
0.8225
0.7873
0.7884
0.8559
0.7261
0.9233
0.7768
0.7902
0.8747

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

16

Conclusion: Our AGA approach achieves 44.27X
speedup ratio compared to GA. AGA even outper-
forms FAST in terms of time efﬁciency (4.58X), and
the difference is statistically signiﬁcant. This indic-
ates that AGA is practical in real-world scenarios.

9 DISCUSSION
Space comparison. From the space complexity analysis in
Section 2, AGA consumes at most twice more space than
GA, which is acceptable in practice. Moreover, AGA does
not require high performance servers, e.g., the time cost of
AGA on the two largest open-source projects (i.e., commons-
math & camel-core) is only 153.42s and 187.82s (on a per-
sonal computer whose Intel Core-i5 with 8GB memory),
almost the same as Table 7 (Appendix C)).
Impact of seeded faults and real faults. Previous work [24],
[57], [58] has explored the relationship between seeded
faults and real faults and they may have different charac-
teristics, which has potential inﬂuence on the evaluation on
test case prioritization, fault localization, etc. In this paper,
we evaluate our approach on both 55 open-source subjects
with seeded faults and Defects4J dataset with real faults. The
high performance of AGA on both of them can illustrate its
superiority well.
Discussion on other TCP approaches. Researchers have put
dedicated efforts in TCP and have proposed a large number
of TCP techniques since then. Many approaches take other
information rather than coverage information (e.g., test in-
puts, test outputs, mutants) as input, so they are in different
dimensions. However, even taken all kinds of approaches
into consideration, the GA approach remains one of the
most effective strategies in terms of fault-detection rate [7],
[8], [10], [18]. So, we target GA in this paper and AGA can
be better than other approaches.

10 RELATED WORK
Test case prioritization attracts much attention since this
problem was raised at the end of the 20th century, and the
work on test case prioritization can be classiﬁed into prior-
itization algorithms [8], [10], [59], [60], [61], [62], coverage
criteria used in prioritization [3], [5], [28], [63], [64], [65],
[66], [67], [68], measurement used to estimate prioritization
effectiveness [2], [5], [69], and empirical studies [1], [3], [5],
[12], [31], [70], [71], [72], [73], [74]. Moreover, a number of
surveys on test case prioritization are also given in the liter-
ature [75], [76], [77]. For example, Catal et al. [76] conducted
a systematic study of TCP techniques in 2001-2011 including
120 papers published in that time period. Due to the space
limit, we do not list all the prioritization work here, but
introduce some very recently published work. Di et al. [78]
proposed Hypervolume-based Genetic Algorithm to prior-
itize test cases using multiple test coverage criteria. Azizi
et al. [79] proposed a graph-based framework to map the
prioritization problem to a graph traversal algorithm. Chen
et al. [80] gave an adaptive random sequences approach
based on clustering techniques using black-box information.
Different from them, our work targets the effective GA
algorithm and attempts to solve its efﬁciency problem.

Moreover, some researchers noticed the efﬁciency prob-
lem of TCP and began to work on it. Henard et al. [12]
said, “if prioritization takes too long, then it eats into the
time available to run the prioritized test suite.” That is, for
large software, it is necessary to take the scalability of TCP
into consideration. Marijan et al. [81] proposed ROCKET to
prioritize test cases based on historical failure data, test ex-
ecution time and domain-speciﬁc heuristics to improve the
efﬁciency in the scenario of continuous integration. Knauss
et al. [82] proposed to analyze the correlation between test
failures and source code changes to rapidly prioritize test
cases. Elbaum et al. [13] introduced two techniques that use
readily available test execution history data to determine
what test cases are worth executing and execute them with
higher priority. Recently, Miranda et al. [18] introduced
the FAST techniques to provide similarity-based test case
prioritization techniques with scalable improvements. Our
work is related to the above work because all of them
target TCP efﬁciency problem. However, the above work
either does not take advantage of the coverage information
which results in lower effectiveness or addresses the efﬁ-
ciency problem alone without balancing or even sacriﬁcing
the effectiveness. That is, to our best knowledge, none of
the existing work can improve the efﬁciency of GA while
maintaining its widely-recognized effectiveness. Our work
achieves this goal and AGA is particularly advantageous for
large-scale industrial projects.

11 CONCLUSIONS

In this paper, we make a deep analysis of the Greedy
Additional algorithm (GA) for test case prioritization (TCP)
problem and propose AGA to improve its efﬁciency while
preserving effectiveness. On one hand, we ﬁnd the redund-
ant data accesses in GA and take the use of extra data
structures to cut down them, which leads to an optimized
time complexity from O(m2n) to O(kmn) given n > m,
where m is the number of test cases, n is the number of
program elements, and k is the iteration number. On the
other hand, we notice the impacts of iteration numbers on
the effectiveness and efﬁciency of GA and propose to reduce
it to a relatively small value to improve efﬁciency while
preserving effectiveness. Overall, we achieve an O(mn)
algorithm for prioritization.

We performed comprehensive experiments on 55 open-
source projects to show the effectiveness and efﬁciency of
AGA. On one hand, AGA can achieve the same average
effectiveness as the GA approach, whose performance is
considered to be high, and at the same time, the efﬁciency
of AGA is much higher than GA. Speciﬁcally, our AGA
approach can achieve 5.95X/27.72X speedup ratio over GA
on average on two input formats. On the other hand,
compared with FAST, which was recently proposed to solve
the TCP efﬁciency problem while sacriﬁcing effectiveness to
some extent, AGA achieves 0.1702 higher APFD values on
average and surprisingly the average speedup ratio of AGA
over FAST is 4.29X.

Additionally, we conducted an industrial case study on
22 industrial subjects, collected from Baidu, which is a
famous Internet service provider with over 600M monthly

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

17

active users. The experimental results show that the av-
erage speedup ratios of AGA over GA and FAST are
44.27X/61.43X and 4.58X (with signiﬁcant difference and
very large effect), respectively.

To the best of our knowledge, this is the ﬁrst attempt
to alleviating the efﬁciency problem of the Greedy Addi-
tional TCP approach while maintaining its effectiveness.
It is worth noting that the efﬁciency of TCP algorithm is
especially important when software becomes larger, that
is to say, in real-world scenarios. Our empirical evidence
indicates that AGA is particularly more advantageous for
large-scale industrial projects.

ACKNOWLEDGMENTS

The authors would like to thank all the reviewers for their
valuable comments and suggestions. This work was sup-
ported by the National Natural Science Foundation of China
under Grant No. 61872008.

REFERENCES

[2]

[1] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and
Mary Jean Harrold. Prioritizing test cases for regression testing.
IEEE Transactions on Software Engineering, 27(10):929–948, 2001.
Sebastian Elbaum, Alexey Malishevsky, and Gregg Rothermel.
Incorporating varying test costs and fault severities into test case
prioritization. In Proceedings of the 23rd International Conference on
Software Engineering, pages 329–338. IEEE Computer Society, 2001.
Sebastian Elbaum, Alexey G Malishevsky, and Gregg Rothermel.
IEEE
Test case prioritization: A family of empirical studies.
Transactions on Software Engineering, 28(2):159–182, 2002.

[3]

[4] Xiao Qu, Myra B Cohen, and Gregg Rothermel. Conﬁguration-
aware regression testing: an empirical study of sampling and
In Proceedings of the 2008 International Symposium
prioritization.
on Software Testing and Analysis, pages 75–86. ACM, 2008.

[5] Gregg Rothermel, Roland H Untch, Chengyun Chu, and
Mary Jean Harrold. Test case prioritization: An empirical study.
In Proceedings of the 1999 IEEE International Conference on Software
Maintenance, pages 179–188. IEEE, 1999.

[6] W Eric Wong, Joseph R Horgan, Saul London, and Hiralal
Agrawal. A study of effective regression testing in practice.
In Proceedings of the Eighth International Symposium On Software
Reliability Engineering, pages 264–274. IEEE, 1997.

[7] Lingming Zhang, Dan Hao, Lu Zhang, Gregg Rothermel, and
Hong Mei. Bridging the gap between the total and additional test-
case prioritization strategies. In Proceedings of the 2013 International
Conference on Software Engineering, pages 192–201. IEEE Press, 2013.
Search al-
gorithms for regression test case prioritization. IEEE Transactions
on Software Engineering, 33(4):225–237, 2007.
Shen Lin. Computer solutions of the traveling salesman problem.
Bell System Technical Journal, 44(10):2245–2269, 1965.

[8] Zheng Li, Mark Harman, and Robert M Hierons.

[9]

[10] Bo Jiang, Zhenyu Zhang, Wing Kwong Chan, and TH Tse. Ad-
aptive random test case prioritization. In Proceedings of the 2009
IEEE/ACM International Conference on Automated Software Engineer-
ing, pages 233–244. IEEE Computer Society, 2009.

[11] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and

Clifford Stein. Introduction to algorithms. MIT press, 2009.

[12] Christopher Henard, Mike Papadakis, Mark Harman, Yue Jia,
and Yves Le Traon. Comparing white-box and black-box test
prioritization. In 2016 IEEE/ACM 38th International Conference on
Software Engineering, pages 523–534. IEEE, 2016.

[13] Sebastian Elbaum, Gregg Rothermel, and John Penix. Techniques
for improving regression testing in continuous integration devel-
opment environments. In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering,
pages 235–245. ACM, 2014.

[14] Mika V M¨antyl¨a, Bram Adams, Foutse Khomh, Emelie Engstr ¨om,
and Kai Petersen. On rapid releases and software testing: a case
study and a semi-systematic literature review. Empirical Software
Engineering, 20(5):1384–1425, 2015.

[15] Atif Memon, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric
Nickell, Rob Siemborski, and John Micco. Taming google-scale
continuous testing. In Proceedings of the 39th International Conference
on Software Engineering: Software Engineering in Practice Track, pages
233–242. IEEE Press, 2017.

[16] Ashish Kumar. Development at the speed and scale of google.

QCon San Francisco, 2010.

[17] Yafeng Lu, Yiling Lou, Shiyang Cheng, Lingming Zhang, Dan
Hao, Yangfan Zhou, and Lu Zhang. How does regression test
prioritization perform in real-world software evolution? In 2016
IEEE/ACM 38th International Conference on Software Engineering,
pages 535–546. IEEE, 2016.

[18] Breno Miranda, Emilio Cruciani, Roberto Verdecchia, and Antonia
Bertolino. Fast approaches to scalable similarity-based test case
prioritization. In Proceedings of the 40th International Conference on
Software Engineering, pages 222–232. ACM, 2018.

[19] Ronald Aylmer Fisher. Statistical methods for research workers.

In Breakthroughs in Statistics, pages 66–70. Springer, 1992.

[20] Qi Luo, Kevin Moran, Lingming Zhang, and Denys Poshyvanyk.
How do static and dynamic test case prioritization techniques per-
form on modern software systems? an extensive study on github
IEEE Transactions on Software Engineering, 45(11):1054–
projects.
1080, 2018.

[21] Jianyi Zhou, Junjie Chen, and Dan Hao. Parallel test prioritization.
ACM Transactions on Software Engineering and Methodology, 31(1):1–
50, 2021.

[22] Junjie Chen, Yiling Lou, Lingming Zhang, Jianyi Zhou, Xiaoleng
Wang, Dan Hao, and Lu Zhang. Optimizing test prioritization via
test distribution analysis. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, pages 656–667. ACM,
2018.

[23] Song Wang, Jaechang Nam, and Lin Tan. Qtep: quality-aware test
case prioritization. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, pages 523–534. ACM, 2017.
[24] Ren´e Just, Darioush Jalali, Laura Inozemtseva, Michael D Ernst,
Reid Holmes, and Gordon Fraser. Are mutants a valid substitute
In Proceedings of the 22nd
for real faults in software testing?
ACM SIGSOFT International Symposium on Foundations of Software
Engineering, pages 654–665. ACM, 2014.

[25] James H Andrews, Lionel C Briand, and Yvan Labiche. Is mutation
an appropriate tool for testing experiments? In Proceedings of the
27th International Conference on Software Engineering, pages 402–411.
ACM, 2005.

[26] Hyunsook Do and Gregg Rothermel. On the use of mutation faults
in empirical assessments of test case prioritization techniques.
IEEE Transactions on Software Engineering, 32(9):733–752, 2006.
[27] Yiling Lou, Dan Hao, and Lu Zhang. Mutation-based test-case
prioritization in software evolution. In 2015 IEEE 26th International
Symposium on Software Reliability Engineering, pages 46–57. IEEE,
2015.

[28] Hong Mei, Dan Hao, Lingming Zhang, Lu Zhang, Ji Zhou, and
Gregg Rothermel. A static approach to prioritizing junit test cases.
IEEE Transactions on Software Engineering, 38(6):1258–1275, 2012.

[29] Md Junaid Arafeen and Hyunsook Do. Test case prioritization us-
ing requirements-based clustering. In 2013 IEEE Sixth International
Conference on Software Testing, Veriﬁcation and Validation, pages 312–
321. IEEE, 2013.

[30] Hyunsook Do, Siavash Mirarab, Ladan Tahvildari, and Gregg Ro-
thermel. The effects of time constraints on test case prioritization:
A series of controlled experiments. IEEE Transactions on Software
Engineering, 36(5):593–617, 2010.

[31] Qi Luo, Kevin Moran, and Denys Poshyvanyk. A large-scale
empirical comparison of static and dynamic test case prioritization
In Proceedings of the 2016 24th ACM SIGSOFT Inter-
techniques.
national Symposium on Foundations of Software Engineering, pages
559–570. ACM, 2016.

[32] Pit mutation testing. http://pitest.org/, 2021. Accessed: 2021.
[33] atlassian / clover – bitbucket. https://bitbucket.org/atlassian/

clover/src/default/, 2021. Accessed: 2021.

[34] Rahul Gopinath, Carlos Jensen, and Alex Groce. Mutations: How
In 2014 IEEE 25th International
close are they to real faults?
Symposium on Software Reliability Engineering, pages 189–200. IEEE,
2014.

[35] Qi Luo, Kevin Moran, Denys Poshyvanyk, and Massimiliano
Di Penta. Assessing test case prioritization on real faults and

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

18

mutants.
tenance and Evolution, pages 240–251. IEEE, 2018.

In 2018 IEEE International Conference on Software Main-

[36] Mike Papadakis, Christopher Henard, and Yves Le Traon.
Sampling program inputs with mutation analysis: Going beyond
In 2014 IEEE Seventh Inter-
combinatorial interaction testing.
national Conference on Software Testing, Veriﬁcation and Validation,
pages 1–10. IEEE, 2014.

[37] Justyna Petke, Shin Yoo, Myra B Cohen, and Mark Harman.
Efﬁciency and early fault detection with lower and higher strength
combinatorial interaction testing. In Proceedings of the 2013 9th Joint
Meeting on Foundations of Software Engineering, pages 26–36. ACM,
2013.

[38] Samuel Sanford Shapiro and Martin B Wilk. An analysis
of variance test for normality (complete samples). Biometrika,
52(3/4):591–611, 1965.

[39] Henry B Mann and Donald R Whitney. On a test of whether one
of two random variables is stochastically larger than the other. The
Annals of Mathematical Statistics, pages 50–60, 1947.

[40] Peter McCullagh. Regression models for ordinal data. Journal of
the Royal Statistical Society: Series B (Methodological), 42(2):109–127,
1980.

[41] Jacob Cohen. Statistical power analysis for the behavioral sciences.

Academic press, 2013.

[42] Maurice Stevenson Bartlett. Properties of sufﬁciency and statistical
tests. Proceedings of the Royal Society of London. Series A-Mathematical
and Physical Sciences, 160(901):268–282, 1937.

[43] John W Tukey. Comparing individual means in the analysis of

variance. Biometrics, pages 99–114, 1949.

[44] Ren´e Just, Darioush Jalali, and Michael D Ernst. Defects4j: A
database of existing faults to enable controlled testing studies for
java programs. In Proceedings of the 2014 International Symposium
on Software Testing and Analysis, pages 437–440. ACM, 2014.
[45] David Paterson, Gregory M Kapfhammer, Gordon Fraser, and Phil
McMinn. Using controlled numbers of real faults and mutants
to empirically evaluate coverage-based test case prioritization.
In Proceedings of the 13th International Workshop on Automation of
Software Test, pages 57–63, 2018.

[46] Md Abu Hasan, Md Abdur Rahman, and Md Saeed Siddik.
Test case prioritization based on dissimilarity clustering using
historical data analysis. In International Conference on Information,
Communication and Computing Technology, pages 269–281. Springer,
2017.

[47] Tanzeem Bin Noor and Hadi Hemmati. A similarity-based ap-
proach for test case prioritization using historical failure data.
In 2015 IEEE 26th International Symposium on Software Reliability
Engineering, pages 58–68. IEEE, 2015.

[48] Alireza Haghighatkhah, Mika M¨antyl¨a, Markku Oivo, and Pasi
In Inter-
Kuvaja. Test case prioritization using test similarities.
national Conference on Product-Focused Software Process Improvement,
pages 243–259. Springer, 2018.

[49] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. Deepﬂ:
Integrating multiple fault diagnosis dimensions for deep fault
localization. In Proceedings of the 28th ACM SIGSOFT International
Symposium on Software Testing and Analysis, pages 169–180, 2019.

[50] Xia Li and Lingming Zhang. Transforming programs and tests in
tandem for fault localization. Proceedings of the ACM on Program-
ming Languages, 1(OOPSLA):1–30, 2017.

[51] Spencer Pearson, Jos´e Campos, Ren´e Just, Gordon Fraser, Rui
Abreu, Michael D Ernst, Deric Pang, and Benjamin Keller. Evalu-
In 2017 IEEE/ACM 39th
ating and improving fault localization.
International Conference on Software Engineering, pages 609–620.
IEEE, 2017.

[52] Jeongju Sohn and Shin Yoo. Fluccs: Using code and change metrics
In Proceedings of the 26th ACM
to improve fault localization.
SIGSOFT International Symposium on Software Testing and Analysis,
pages 273–283. ACM, 2017.

[53] Mengshi Zhang, Xia Li, Lingming Zhang, and Sarfraz Khurshid.
Boosting spectrum-based fault localization using pagerank.
In
Proceedings of the 26th ACM SIGSOFT International Symposium on
Software Testing and Analysis, pages 261–272, 2017.

[54] Martina Marr´e and Antonia Bertolino. Using spanning sets
IEEE Transactions on Software Engineering,

for coverage testing.
29(11):974–984, 2003.

[55] James Edward Baker. Adaptive selection methods for genetic
algorithms. In Proceedings of an International Conference on Genetic
Algorithms and Their Applications, volume 1. Hillsdale, New Jersey,
1985.

[56] Bullseye testing technology. http://www.bullseye.com/, 2021.

Accessed: 2021.

[57] Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae.
Are mutation scores correlated with real fault detection? a large
scale empirical study on the relationship between mutants and
In 2018 IEEE/ACM 40th International Conference on
real faults.
Software Engineering, pages 537–548. IEEE, 2018.

[58] Murial Daran and Pascale Th´evenod-Fosse. Software error ana-
lysis: A real case study involving real faults and mutations. ACM
SIGSOFT Software Engineering Notes, 21(3):158–171, 1996.

[59] Gordon Fraser and Franz Wotawa. Test-case prioritization with

model-checkers. In 25th conference on IASTED International, 2007.

[60] Shin Yoo, Mark Harman, Paolo Tonella, and Angelo Susi. Clus-
tering test cases to achieve effective and scalable prioritisation
In Proceedings of the Eighteenth
incorporating expert knowledge.
International Symposium on Software Testing and Analysis, pages 201–
212. ACM, 2009.

[61] Ripon K Saha, Lingming Zhang, Sarfraz Khurshid, and De-
wayne E Perry. An information retrieval approach for regres-
In 2015
sion test prioritization based on program changes.
IEEE/ACM 37th IEEE International Conference on Software Engineer-
ing, volume 1, pages 268–279. IEEE, 2015.

[62] Zengkai Ma and Jianjun Zhao. Test case prioritization based on
In 2008 15th Asia-Paciﬁc Software

analysis of program structure.
Engineering Conference, pages 471–478. IEEE, 2008.

[63] Sebastian Elbaum, Alexey G Malishevsky, and Gregg Rothermel.

Prioritizing test cases for regression testing, volume 25. ACM, 2000.

[64] Hyunsook Do, Gregg Rothermel, and Alex Kinneer. Empirical
studies of test case prioritization in a junit testing environment.
In 15th International Symposium on Software Reliability Engineering,
pages 113–124. IEEE, 2004.

[65] James A Jones and Mary Jean Harrold. Test-suite reduction and
IEEE
prioritization for modiﬁed condition/decision coverage.
Transactions on Software Engineering, 29(3):195–209, 2003.

[66] Lingming Zhang, Ji Zhou, Dan Hao, Lu Zhang, and Hong Mei.
Prioritizing junit test cases in absence of coverage information. In
2009 IEEE International Conference on Software Maintenance, pages
19–28. IEEE, 2009.

[67] Bogdan Korel, Luay Ho Tahat, and Mark Harman. Test prioritiza-
tion using system models. In 21st IEEE International Conference on
Software Maintenance, pages 559–568. IEEE, 2005.

[68] Lijun Mei, Zhenyu Zhang, WK Chan, and TH Tse. Test case
prioritization for regression testing of service-oriented business
applications. In Proceedings of the 18th International Conference on
World Wide Web, pages 901–910. ACM, 2009.

[69] Gregory M Kapfhammer and Mary Lou Soffa. Using coverage
In Proceedings
effectiveness to evaluate test suite prioritizations.
of the 1st ACM International Workshop on Empirical Assessment of
Software Engineering Languages and Technologies: held in conjunction
with the 22nd IEEE/ACM International Conference on Automated
Software Engineering, pages 19–20. ACM, 2007.

[70] Donghwan Shin, Shin Yoo, Mike Papadakis, and Doo-Hwan
Bae. Empirical evaluation of mutation-based test case prioritiz-
ation techniques. Software Testing, Veriﬁcation and Reliability, 29(1-
2):e1695, 2019.

[71] Hyunsook Do, Siavash Mirarab, Ladan Tahvildari, and Gregg
Rothermel. An empirical study of the effect of time constraints
on the cost-beneﬁts of regression testing. In Proceedings of the 16th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering, pages 71–82. ACM, 2008.

[72] Dan Hao, Lu Zhang, and Hong Mei. Test-case prioritization:
Frontiers of Computer Science,

achievements and challenges.
10(5):769–777, 2016.

[73] Michael G Epitropakis, Shin Yoo, Mark Harman, and Edmund K
Burke. Empirical evaluation of pareto efﬁcient multi-objective
regression test case prioritisation. In Proceedings of the 2015 Inter-
national Symposium on Software Testing and Analysis, pages 234–245.
ACM, 2015.

[74] Dan Hao, Lu Zhang, Lei Zang, Yanbo Wang, Xingxia Wu, and
IEEE
Tao Xie. To be optimal or not in test-case prioritization.
Transactions on Software Engineering, 42(5):490–505, 2015.

[75] Shin Yoo and Mark Harman. Regression testing minimization,
selection and prioritization: a survey. Software Testing, Veriﬁcation
and Reliability, 22(2):67–120, 2012.

[76] Cagatay Catal and Deepti Mishra. Test case prioritization: a
systematic mapping study. Software Quality Journal, 21(3):445–478,
2013.

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

19

Dan Hao is an associate professor at School
of Computer Science, Peking University,
P.R.China. She received her Ph.D. in Computer
Science from Peking University in 2008, and
the B.S. in Computer Science from the Harbin
Institute of Technology in 2002. She was a
program co-chair of ASE 2021 and SANER
2022, a general co-chair of SPLC 2018,
the program committees of many prestigious
conferences (e.g., ICSE, FSE, ASE, and ISSTA).
Her current research interests include software

testing and debugging.

[77] Sanjukta Mohanty, Arup Abhinna Acharya, and Durga Prasad
Mohapatra. A survey on model based test case prioritization.
International Journal of Computer Science and Information Technologies,
2(3):1042–1047, 2011.

[78] Dario Di Nucci, Annibale Panichella, Andy Zaidman, and Andrea
De Lucia. A test case prioritization genetic algorithm guided
IEEE Transactions on Software
by the hypervolume indicator.
Engineering, 2018.

[79] Maral Azizi and Hyunsook Do. Graphite: A greedy graph-based
technique for regression test case prioritization. In 2018 IEEE In-
ternational Symposium on Software Reliability Engineering Workshops,
pages 245–251. IEEE, 2018.

[80] Jinfu Chen, Lili Zhu, Tsong Yueh Chen, Dave Towey, Fei-Ching
Kuo, Rubing Huang, and Yuchi Guo. Test case prioritization for
object-oriented software: An adaptive random sequence approach
based on clustering. Journal of Systems and Software, 135:107–125,
2018.

[81] Dusica Marijan, Arnaud Gotlieb, and Sagar Sen. Test case priorit-
ization for continuous regression testing: An industrial case study.
In 2013 IEEE International Conference on Software Maintenance, pages
540–543. IEEE, 2013.

[82] Eric Knauss, Miroslaw Staron, Wilhelm Meding, Ola S ¨oder, Ag-
neta Nilsson, and Magnus Castell. Supporting continuous integra-
tion by code-churn based test selection. In Proceedings of the Second
International Workshop on Rapid Continuous Software Engineering,
pages 19–25. IEEE Press, 2015.

Lu Zhang is a professor at School of Computer
Science, Peking University, P.R. China. He re-
ceived both Ph.D. and BSc in Computer Sci-
ence from Peking University in 2000 and 1995
respectively. He was a postdoctoral researcher
in Oxford Brookes University and University of
Liverpool, UK. He served on the program com-
mittees of many prestigious conferences, such
as FSE, OOPSLA, ISSTA, and ASE. He was a
program co-chair of SCAM 2008 and a program
co-chair of ICSME 2017. He has been on the
editorial boards of Journal of Software Maintenance and Evolution:
Research and Practice and Software Testing, Veriﬁcation and Reliability.
His current research interests include software testing and analysis,
program comprehension, software maintenance and evolution, software
reuse and component-based software development, and service com-
puting.

Feng Li received his B.S. degree from Peking
University in 2018. He is currently a Ph.D. can-
didate in School of Computer Science at Peking
University. His research interests include soft-
ware testing and analysis.

Jianyi Zhou received his B.S. degree in 2014,
and M.S. degree in 2017, both from Beihang
University. He is currently a Ph.D. candidate in
School of Computer Science at Peking Univer-
sity. His research interests include software test-
ing and analysis.

Yinzhu Li received her M.S. degree in Computer
Science and Technology in 2012 from Tianjin
Normal University. She is now an employee at
Baidu Online Network Technology (Beijing) Co.,
Ltd., mainly working on automation testing. Her
research interest is intelligent testing, including
test case selection, test case generation, and
fault localization.

     TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

20

APPENDIX A
CHARTS OF ITERATION NUMBER AND TIME COST

To better analyze the relationship between iteration number
and time cost, we put detailed results in Section 6.1 here.
We draw a line chart of iteration number and time cost for
each project. Note that in order to see the trend, we only
present the projects whose iteration number is no less than
20 (k ≥ 20). As we can see, all projects follow a similar
trend. In some projects, the ﬁrst several iterations cost more
time than other iterations. It is reasonable because along
with the decrease of the number of remaining test cases (n),
prioritization also becomes faster. The plots also support
our claim that the iteration number contributes much to
the time cost. As k is the coefﬁcient of time complexity, it
largely determines the actual efﬁciency in practice, so, we
think there is a large space to reduce time complexity.

lines of code (SLOC), test lines of code (TLOC), number of
test cases (#Test cases), and number of mutants (#Mutants),
respectively. The projects are sorted in ascending order of
source lines of code.

APPENDIX C
RESULTS OF OPEN-SOURCE SUBJECTS
Due to space limit, we show the complete results on open-
source subjects in Table 7. The subjects are sorted in ascend-
ing order of source lines of code (SLOC). The ﬁrst three
columns present the results for RQ1, the ﬁrst ﬁve columns
present the results for RQ2, the ﬁrst nine columns present
the results of RQ3, and the last four columns present the
comparison results with FAST. The detailed analysis can be
found in Sections 6 and 7.

APPENDIX D
RESULTS OF INDUSTRIAL SUBJECTS

Due to space limit, we present the complete results on
industrial subjects in Table 8. For each subject, we present
its SLOC, #Test cases, and the time cost of GA, AGA, FAST,
ART-D, GA-S, and GE, respectively. The detailed analysis
can be found in Section 8.

APPENDIX B
BASIC INFORMATION OF OPEN-SOURCE SUBJECTS
Table 6 shows some basic information of our 55 open-source
subjects. Speciﬁcally, for each subject, we present the source

1.52.02.53.03.5013263952Iteration Numbercommons−mathTime(s)0.200.250.300.350.400.4507142128Iteration Numberla4j−newTime(s)0.51.01.5015304560Iteration NumberjspritTime(s)0.60.70.808162432Iteration NumberjsoupTime(s)203040010203040Iteration Numberrome−1.5.0Time(s)0.51.01.52.02.50169338507676Iteration Numberassertj−coreTime(s)1234024487296Iteration Numberla4jTime(s)0.30.40.50.609182736Iteration Numbercommons−dbcpTime(s)0.100.120.140.160.185101520Iteration NumberbluefloodTime(s)15202530016324864Iteration Numbercamel−coreTime(s)0.30.40.507142128Iteration Numberjopt−simpleTime(s)0.80.91.01.11.209182736Iteration NumberlanguagetoolTime(s)2345016324864Iteration Numbermapdb−mapdb−1.0.9Time(s)TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

21

Table 6: Basic Information for Open-Source Subjects

ID

Subjects

SLOC

TLOC

#Test Cases

#Mutants

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S20
S21
S22
S23
S24
S25
S26
S27
S28
S29
S30
S31
S32
S33
S34
S35
S36
S37
S38
S39
S40
S41
S42
S43
S44
S45
S46
S47
S48
S49
S50
S51
S52
S53
S54
S55

DiskLruCache
gson-ﬁre
gson-ﬁre-v2
jumblr
java-apns
jasmine-maven-plugin
java-uuid-generator
gdx-artemis-master
jopt-simple
protoparser
jackson-datatype-guava
jackson-datatype-guava-v2
JActor
spring-retry
scribe-java
metrics-core
javapoet
low-gc-membuffers
lambdaj-master
LastCalc-0.1
stream-lib
webbit
commons-pool
redline-smalltalk-master
la4j
redline-smalltalk
nv-websocket-client
joss
raml-java-parser-master
raml-java-parser
la4j-v2
commons-io
streamex
jsoup
commons-dbcp
rome-1.5.0
assertj-core
vraptor-archive
mapdb-mapdb-1.0.9
RoaringBitmap
blueﬂood
lanterna
jackson-core
jsprit
hivemall
asterisk-java
asterisk-java-v2
restcountries
chukwa
ews-java-api
languagetool
OpenTripPlanner-otp-0.20.0
hbase-1.2.2
commons-math
camel-core

780
895
1,178
1,489
1,503
1,671
1,790
1,851
1,924
2,153
2,217
2,366
2,542
2,765
2,808
2,835
2,986
3,184
3,634
4,522
4,835
4,914
5,206
5,648
7,086
7,212
7,351
8,078
8,696
8,788
9,272
9,980
10,427
10,507
11,592
11,647
13,361
16,910
17,589
17,807
19,517
20,682
21,320
23,073
28,569
30,495
31,074
31,324
32,654
45,313
47,589
64,718
66,630
86,748
120,248

1,030
726
952
1,243
1,724
1,931
2,388
1,492
5,903
3,227
1,035
1,327
4,418
3,419
2,536
2,194
4,399
9,782
4,914
581
3,806
8,463
8,232
480
4,050
2,414
657
6,035
3,005
5,061
4,035
19,189
7,906
12,037
8,752
2,705
53,059
16,213
35,873
21,494
15,774
7,724
10,924
18,373
3,975
4,263
4,258
468
8,051
1,328
20,778
14,207
17,385
90,798
134,036

61
36
47
103
87
102
45
35
727
171
73
80
65
185
99
150
332
51
265
32
141
131
272
43
625
240
73
531
192
197
799
1,081
450
666
560
475
2,470
1,130
1,776
1,148
961
34
376
1,250
150
217
217
40
131
90
719
379
434
5,082
5,623

152
520
202
167
412
561
346
961
1,677
864
845
320
56
351
563
1,656
973
780
3,399
2,499
3,811
349
633
3,450
5,023
833
277
1,289
4,506
1,288
3,141
7,773
3,958
3,157
2,601
4,929
4,571
7,245
876
21,319
1,854
344
6,215
12,350
6,557
3,226
921
113
569
1,782
26,662
7,325
1,781
84,476
13,005

Total

912,045

633,085

31,454

262,295

TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. X, NO. X, MONTH YEAR

22

RQ1

Table 7: Results of Open-Source Subjects

RQ2

RQ3

Comparison with FAST

Project TimeGA
0.0197
0.0072
0.0074
0.0140
0.0314
0.0115
0.0045
0.0202
1.7455
0.0871
0.0975
0.0243
0.0383
0.0876
0.0221
0.0723
0.3706
0.0323
1.9204
0.0655
0.0942
0.0413
0.3202
0.0239
6.9852
0.0786
0.0095
0.5833
8.5669
0.4128
2.0359
1.0693
0.6033
5.7624
1.5128
210.0549
5.2729
3.6650
42.3862
4.0109
0.9968
0.0034
1.4103
7.2241
0.1297
0.2842
0.1310
0.0025
0.1545
0.0071
8.5532
1.5933
1.0478
100.2525
1,288.1519

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S20
S21
S22
S23
S24
S25
S26
S27
S28
S29
S30
S31
S32
S33
S34
S35
S36
S37
S38
S39
S40
S41
S42
S43
S44
S45
S46
S47
S48
S49
S50
S51
S52
S53
S54
S55

TimeI TimeGAF

0.0197
0.0072
0.0074
0.0140
0.0314
0.0115
0.0045
0.0202
1.7105
0.0871
0.0975
0.0243
0.0383
0.0870
0.0221
0.0723
0.3693
0.0323
1.9201
0.0655
0.0942
0.0413
0.3202
0.0217
2.7593
0.0786
0.0095
0.5819
8.5669
0.4128
1.9270
1.0649
0.6033
5.6923
1.3176
123.6547
3.2186
3.6650
35.0646
4.0064
0.9890
0.0034
1.4103
5.9892
0.1297
0.2842
0.1310
0.0025
0.1545
0.0070
8.4018
1.5887
1.0459
98.7254
1,236.9016

0.0069
0.0030
0.0038
0.0058
0.0163
0.0149
0.0027
0.0092
1.7891
0.0471
0.0464
0.0090
0.0158
0.0327
0.0117
0.0357
0.2144
0.0167
1.5941
0.0336
0.0416
0.0217
0.2099
0.0079
1.0409
0.0397
0.0041
0.3976
5.0644
0.2461
0.9671
0.8031
0.5780
6.0388
0.6824
32.9429
4.6264
3.9989
17.5059
3.0029
0.7100
0.0035
1.4142
4.4672
0.1095
0.2118
0.0754
0.0016
0.0867
0.0029
8.6768
1.5922
0.9088
78.0955
734.9618

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

TimeC APFDGAF APFDGA APFDAGA TimeAGA APFDFAST WinAPFD TimeFAST WinTime
0.0157 (cid:88)
0.0157 (cid:88)
0.0058 (cid:88)
0.0058 (cid:88)
0.0222
0.0222
0.0789
0.0789
0.0089 (cid:88)
0.0089 (cid:88)
0.0046 (cid:88)
0.0046 (cid:88)
0.0076
0.0076
0.0180 (cid:88)
0.0180 (cid:88)
0.3363 (cid:88)
0.5288 (cid:88)
0.0247 (cid:88)
0.0247 (cid:88)
0.0553 (cid:88)
0.0553 (cid:88)
0.0106 (cid:88)
0.0108 (cid:88)
0.0385
0.0385
0.0385 (cid:88)
0.0407 (cid:88)
0.0055 (cid:88)
0.0055 (cid:88)
0.0152 (cid:88)
0.0152 (cid:88)
0.1317 (cid:88)
0.1357 (cid:88)
0.0226 (cid:88)
0.0226 (cid:88)
0.6995 (cid:88)
0.7017 (cid:88)
0.0469 (cid:88)
0.0469 (cid:88)
0.0248 (cid:88)
0.0248 (cid:88)
0.0215 (cid:88)
0.0215 (cid:88)
0.0962 (cid:88)
0.0962 (cid:88)
0.0187 (cid:88)
0.0198 (cid:88)
1.0579 (cid:88)
4.1901 (cid:88)
0.0294 (cid:88)
0.0294 (cid:88)
0.0303
0.0331
0.1132 (cid:88)
0.1179 (cid:88)
2.3900 (cid:88)
2.3900 (cid:88)
0.1804 (cid:88)
0.1804 (cid:88)
0.3170 (cid:88)
0.4384 (cid:88)
0.1270 (cid:88)
0.1415 (cid:88)
0.0921 (cid:88)
0.0921 (cid:88)
0.7903 (cid:88)
0.8250 (cid:88)
0.3631 (cid:88)
0.5911 (cid:88)
23.7849 (cid:88)
46.6052 (cid:88)
0.3582 (cid:88)
2.4364 (cid:88)
0.2794 (cid:88)
0.2794 (cid:88)
2.2078 (cid:88)
5.1276 (cid:88)
0.5942 (cid:88)
0.6129 (cid:88)
0.1562 (cid:88)
0.1797 (cid:88)
0.0287
0.0287
0.2558 (cid:88)
0.2558 (cid:88)
0.8465 (cid:88)
1.7918 (cid:88)
0.0597 (cid:88)
0.0597 (cid:88)
0.0858 (cid:88)
0.0858 (cid:88)
0.0629 (cid:88)
0.0629 (cid:88)
0.0025
0.0025
0.1120 (cid:88)
0.1120 (cid:88)
0.0047 (cid:88)
0.0049 (cid:88)
1.1036 (cid:88)
1.2770 (cid:88)
0.4052 (cid:88)
0.4241 (cid:88)
0.2298 (cid:88)
0.2427 (cid:88)
2.6648 (cid:88)
3.5015 (cid:88)
18.1040 (cid:88)
32.4581 (cid:88)

0.9070 (cid:88)
0.8380 (cid:88)
0.8848 (cid:88)
0.8509 (cid:88)
0.8527 (cid:88)
0.8101 (cid:88)
0.9059 (cid:88)
0.8898 (cid:88)
0.9144 (cid:88)
0.9518 (cid:88)
0.8766 (cid:88)
0.8864 (cid:88)
0.8615 (cid:88)
0.9188 (cid:88)
0.8582 (cid:88)
0.8031 (cid:88)
0.9183 (cid:88)
0.9028 (cid:88)
0.9033 (cid:88)
0.8013 (cid:88)
0.8328 (cid:88)
0.8642 (cid:88)
0.8198 (cid:88)
0.9858 (cid:88)
0.8401
0.8339 (cid:88)
0.9614 (cid:88)
0.9164 (cid:88)
0.9490 (cid:88)
0.9617 (cid:88)
0.9426 (cid:88)
0.8911 (cid:88)
0.8662 (cid:88)
0.9328 (cid:88)
0.9467
0.9371
0.8507 (cid:88)
0.8657 (cid:88)
0.9545 (cid:88)
0.9244 (cid:88)
0.9106 (cid:88)
0.8569 (cid:88)
0.8924 (cid:88)
0.9240
0.8464 (cid:88)
0.8656 (cid:88)
0.8750 (cid:88)
0.7939 (cid:88)
0.8009 (cid:88)
0.8517 (cid:88)
0.8671 (cid:88)
0.9542 (cid:88)
0.8710 (cid:88)
0.9089 (cid:88)
0.9544 (cid:88)

0.0717
0.0348
0.0228
0.0140
0.0542
0.0315
0.0163
0.1102
1.8445
0.1326
0.3315
0.0452
0.1346
0.1315
0.0381
0.0793
0.7061
0.1115
3.6103
0.3444
0.1313
0.1019
0.4346
0.0919
5.7828
0.0435
0.0201
0.5634
14.0143
0.9181
1.5359
0.3410
0.4881
4.2321
1.5940
129.8628
1.0458
1.2374
11.6840
1.6539
0.3880
0.0267
1.5863
4.2414
0.2205
0.5231
0.1188
0.0064
0.2295
0.0069
5.8874
1.8314
0.8469
7.8017
88.3705

0.8809
0.8369
0.8868
0.8505
0.8527
0.8101
0.9045
0.8913
0.9144
0.9514
0.8741
0.8854
0.8500
0.9188
0.8582
0.8010
0.9114
0.9026
0.9003
0.8014
0.8353
0.8642
0.8189
0.9850
0.7135
0.8322
0.9614
0.9153
0.9482
0.9620
0.9511
0.8889
0.8659
0.9277
0.9163
0.8644
0.8508
0.8657
0.8679
0.9198
0.9040
0.8574
0.8913
0.9159
0.8466
0.8648
0.8751
0.7939
0.7997
0.8476
0.8571
0.9530
0.8673
0.9089
0.9516

0.9070
0.8380
0.8848
0.8509
0.8527
0.8101
0.9059
0.8898
0.9144
0.9518
0.8766
0.8864
0.8615
0.9188
0.8582
0.8031
0.9183
0.9028
0.9033
0.8013
0.8328
0.8642
0.8198
0.9858
0.8450
0.8339
0.9614
0.9164
0.9490
0.9617
0.9426
0.8911
0.8662
0.9328
0.9473
0.9418
0.8507
0.8657
0.9545
0.9244
0.9106
0.8569
0.8924
0.9261
0.8464
0.8656
0.8750
0.7939
0.8009
0.8517
0.8671
0.9542
0.8710
0.9089
0.9544

0.8164
0.7164
0.6916
0.7183
0.7285
0.6731
0.7531
0.6771
0.8688
0.7639
0.6758
0.6693
0.7584
0.6149
0.7052
0.6293
0.7128
0.7688
0.6955
0.6636
0.6847
0.7375
0.6742
0.9143
0.7500
0.6079
0.6707
0.7159
0.7234
0.7599
0.6473
0.7007
0.6105
0.7515
0.7872
0.8092
0.6925
0.7608
0.8279
0.6993
0.7181
0.6954
0.6681
0.7696
0.6643
0.6642
0.7217
0.6446
0.6620
0.7722
0.6025
0.7708
0.6630
0.7524
0.8269

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Total

48

0.8870

0.8870

51

48

55

52

Table 8: Results of Industrial Subjects

Subject*

Basic Information
SLOC** #Test Cases

I1
I2
I3
I4
I5
I6
I7
I8
I9
I10
I11
I12
I13
I14
I15
I16
I17
I18
I19
I20
I21
I22

>500K
>200K
>200K
>200K
>200K
>500K
>500K
>200K
>500K
>200K
>500K
>500K
>500K
>200K
>500K
>20K
>200K
>20K
>500K
>200K
>20K
>500K

4,246
2,546
2,566
2,550
2,556
4,123
4,139
2,529
4,134
2,542
4,133
4,137
4,128
2,234
2,201
202
2,216
299
3,993
2,206
281
4,034

Time cost (s)

GA

29,278.9102
3,018.6473
3,228.2772
2,833.4841
3,289.5958
22,118.0296
21,963.5968
4,250.2729
22,057.8564
3,238.5423
23,749.9149
22,194.6776
22,545.8684
571.9417
6,517.1065
7.4382
599.1948
11.6980
21,482.4772
586.5093
8.0470
24,446.3671

AGA

FAST
359.9679 (cid:88) 1,860.1473
89.9239 (cid:88)
398.8814
86.0066 (cid:88)
417.8356
80.5940 (cid:88)
383.9494
94.0641 (cid:88)
428.5125
329.4710 (cid:88) 1,439.6848
336.3432 (cid:88) 1,600.3634
89.2680 (cid:88)
446.4625
335.8682 (cid:88) 1,450.5679
96.6740 (cid:88)
418.7254
348.1437 (cid:88) 1,531.0934
342.6023 (cid:88) 1,466.4241
362.5389 (cid:88) 1,470.3869
22.2583 (cid:88)
85.0108
190.7537 (cid:88)
926.5795
3.5816 (cid:88)
9.7204
16.0822 (cid:88)
85.3307
2.2721 (cid:88)
10.5942
335.6216 (cid:88) 1,750.2093
18.7069 (cid:88)
87.0280
1.8397 (cid:88)
9.1955
335.9041 (cid:88) 1,778.7107

ART-D

GA-S

GE

543,106.2852
32,938.6045
30,458.8672
24,944.4404
31,799.7539
402,039.4240
411,725.1937
36,610.5757
28,328.2207
769,960.0223
398,946.3216
398,254.6365
446,056.7049
4,999.5140
71,541.1456
87.4167
12,411.9608
83.9378
444,997.4857
6,905.6778
34.1523
466,512.4680

2,680,036.2615
315,888.7090
304,555.6710
265,881.1139
366,902.7648
1,766,206.5003
2,390,410.2541
461,096.5509
2,091,910.4123
265,087.9653
2,537,854.1564
2,016,031.3451
2,018,768.3295
37,081.3254
513,769.5738
601.2848
32,268.7012
988.3095
2,295,089.0447
75,574.2453
610.4776
2,636,222.8890

54,969.0830
13,887.3160
14,124.1435
19,345.1543
8,424.3798
49,274.3782
54,897.2351
3,857.2345
37,817.4141
7,134.1514
39,417.0345
38,741.9410
49,287.1451
487.0905
19,481.4108
42.7104
7,015.4581
38.6094
64,510.4519
1,048.8951
19.9627
52,941.8715

Total

>6,860K

61,995

22

* We hide project names for the conﬁdential policy.
** We report rough scale of SLOC due to the conﬁdential policy.

