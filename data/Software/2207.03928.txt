2
2
0
2

l
u
J

7
2

]

G
L
.
s
c
[

2
v
8
2
9
3
0
.
7
0
2
2
:
v
i
X
r
a

GT4SD: GENERATIVE TOOLKIT FOR SCIENTIFIC DISCOVERY

Matteo Manica1, Joris Cadow1, Dimitrios Christoﬁdellis1, Ashish Dave1, Jannis Born1, Dean Clarke1, Yves Gaetan
Nana Teukam1, Samuel C. Hoffman1, Matthew Buchan1, Vijil Chenthamarakshan1, Timothy Donovan1, Hsiang Han
Hsu1, Federico Zipoli1, Oliver Schilter1, Giorgio Giannone1, Akihiro Kishimoto1, Lisa Hamada1, Inkit Padhi1, Karl
Wehden1, Lauren McHugh1, Alexy Khrabrov1, Payel Das1, Seiji Takeda1, and John R. Smith1

1IBM Research

ABSTRACT

With the growing availability of data within various scientiﬁc domains, generative models hold
enormous potential to accelerate scientiﬁc discovery at every step of the scientiﬁc method. Perhaps
their most valuable application lies in the speeding up of what has traditionally been the slowest
and most challenging step of coming up with a hypothesis. Powerful representations are now
being learned from large volumes of data to generate novel hypotheses, which is making a big
impact on scientiﬁc discovery applications ranging from material design to drug discovery. The
GT4SD [Team, 2022] (https://github.com/GT4SD/gt4sd-core) is an extensible open-source
library that enables scientists, developers and researchers to train and use state-of-the-art generative
models for hypothesis generation in scientiﬁc discovery. GT4SD supports a variety of uses of
generative models across material science and drug discovery, including molecule discovery and
design based on properties related to target proteins, omic proﬁles, scaffold distances, binding energies
and more.

Keywords Generative Models · Scientiﬁc Discovery · Accelerated Discovery · Open Source

Humanity’s progress has been characterised by a delicate balance between curiosity and creativity. Science is no
exception with its long evolution through trial and error. While remarkably successful, the scientiﬁc method can be a
slow iterative process that can be inadequate when faced with important and pressing needs, e.g., the need to swiftly
develop drugs and antibiotics or design novel materials and processes to mitigate climate change effects. Indeed, it
can take almost a decade to discover a new material and cost upwards of $10–$100 million. One of the most daunting
challenges in materials discovery is hypothesis generation, where it is extremely challenging to identify and select novel
and useful candidates in search spaces that are overwhelming in size, e.g., the chemical space for drug-like molecules is
estimated to contain 1033 structures [Polishchuk et al., 2013].

To overcome this problem, in recent years, generative models have emerged as an effective approach to design and
discover molecules with desired properties. Generative models more efﬁciently and effectively navigate and explore vast
search spaces that are learned from data based on user-deﬁned criteria. Starting from a series of seminal works [Gómez-
Bombarelli et al., 2018, Segler et al., 2018, Jin et al., 2018, You et al., 2018, Prykhodko et al., 2019], research has covered
wide variety of applications of generative models, including design and discovery of: sugar and dye molecules [Takeda
et al., 2020], ligands for speciﬁc targets [Zhavoronkov et al., 2019, Chenthamarakshan et al., 2020, Born et al., 2021a],
anti-cancer hit-like molecules [Méndez-Lucio et al., 2020, Born et al., 2021b] and antimicrobial peptides [Das et al.,
2021].

At the same time, we have witnessed growing community efforts for developing software packages to evaluate
and benchmark generative models and their application in material science. Initial efforts for generic frameworks
implementing popular baselines and metrics such as GuacaMol [Brown et al., 2019] and Moses [Polykovskiy et al.,
2020] paved the way for domain-speciﬁc generative model software that is gaining popularity in the space of drug
discovery such as TDC [Huang et al., 2021].

In this landscape there is a growing need for libraries and toolkits that can lower the barrier for using generative models.
This need is becoming signiﬁcantly more pressing given the growing trends for larger and larger models and companion
signiﬁcant requirements on large computational resources for training them. This effectively excludes large parts of the
scientiﬁc community from the ability to achieve signiﬁcant progress and concentrates the power on a small, privileged
group of researchers in well-funded institutions, thus impeding the principles of open, collaborative and fair science.

 
 
 
 
 
 
GT4SD - PREPRINT

As a remedy, we here introduce the Generative Toolkit for Scientiﬁc Discovery (GT4SD), a python library that aims
at bridging this gap by developing a framework that eases the process of training, running and developing generative
models to accelerate scientiﬁc discovery Figure 1.

GT4SD offers a set of capabilities for generating novel hypotheses (inference pipelines) and for ﬁne-tuning domain-
speciﬁc generative models (training pipelines). It is designed to be compatible and inter-operable with existing popular
libraries, including PyTorch [Paszke et al., 2019], PyTorch Lightning [Falcon and The PyTorch Lightning team, 2019],
Hugging Face Transformers [Wolf et al., 2020a], GuacaMol [Brown et al., 2019], Moses [Polykovskiy et al., 2020],
TorchDrug [Zhu et al., 2022] and MoLeR [Maziarz et al., 2021]. It includes a wide range of pre-trained models and
applications, ranging from materials science to drug discovery.

GT4SD provides simple interfaces to make generative models easily accessible to users who want to deploy them
with just a few lines of code. The library provides an environment for researchers and students interested in applying
state-of-the-art models in their scientiﬁc research, allowing them to experiment with a wide variety of pre-trained
models spanning a broad spectrum of material science and drug discovery applications. Furthermore, GT4SD provides
a standardised Command Line Interface (CLI) and APIs for inference and training, without compromising on the ability
to specify an algorithm’s ﬁner grained parameters.

1 Results

Figure 1: GT4SD overview. The library implements pipelines for inference and training of generative models. GT4SD
offers utilities for algorithm versioning and sharing for broader usage in the community. The standardised interface
enables algorithm instantiation and run for generating samples with less than ﬁve lines of code (left panel). The CLI
tools ease the run of a full discover pipeline in the terminal (right panel).

Arguably, the largest potential for accelerating scientiﬁc discovery lies in the ﬁeld of de novo molecular design, in
particular in material and drug discovery. With several (pre)clinical trials underway [Jayatunga et al., 2022], it is a
matter of time until the ﬁrst AI-generated drug will receive FDA approval and reach the market. In a seminal study
by Zhavoronkov et al. [2019], a deep reinforcement learning model (GENTRL) was utilized for the discovery of potent
DDR1 inhibitors, a prominent protein kinase target involved in ﬁbrosis, cancer and other diseases [Hidalgo-Carcedo
et al., 2011]. Six molecules were synthesised, four were found active in a biochemical assay and one compound (in the
following called gentrl-ddr1) demonstrated favourable pharmacokinetics in mice.

As an exemplary case study in molecular discovery, we consider a contrived task of adapting the hit-compound
gentrl-ddr1 to a similar molecule with an improved drug-likeness [Bickerton et al., 2012]. Drug-likeness (or QED) is
an in-silico property of a molecule that ranges between 0 and 1 (gentrl-ddr1: 0.38) and comprises a notion of chemical
aesthetics for medicinal chemistry applications. This task requires to explore the local chemical space around the hit

2

GT4SDInferencepipelinesTrainingpipelinesAlgorithmversioningGT4SD - PREPRINT

(i.e., gentrl-ddr1) in order to ﬁnd an optimized lead compound. A summary of how this task can be addressed using the
GT4SD is shown in Figure 2.

In the ﬁrst step, a rich set of pre-trained molecular generative models is accessed with the harmonised interface of the
GT4SD. Two main model classes are available. A ﬁrst category is represented by graph generative models, such as
MoLeR [Maziarz et al., 2022] or models from the TorchDrug library, speciﬁcally a graph-convolutional policy network
and a ﬂow-based autoregressive model (GraphAF; [Shi et al., 2020]). The second model class are chemical language
models (CLM) which treat molecules as text (SMILES [Weininger, 1988] or SELFIES [Krenn et al., 2020] sequences).
Most of the chemical language models in the GT4SD are accessed via the libraries MOSES [Polykovskiy et al., 2020]
or GuacaMol [Brown et al., 2019]; in particular a VAE [Gómez-Bombarelli et al., 2018], an adversarial autoencoder
(AAE; [Kadurin et al., 2017]) or an objective-reinforced GAN model (ORGAN; [Guimaraes et al., 2017]). In the
ﬁrst step, we randomly sample molecules from the learned chemical space of each model. Assessing the Tanimoto
similarity of the generated molecules to gentrl-ddr1 reveals that this approach, while producing many molecules with
satisfying QED, did not sufﬁciently reﬂect the similarity constraint to the seed molecule (cf. Figure 2, bottom left). This
is expected because the investigated generative models are unconditional.

Figure 2: Case study using the GT4SD for molecular discovery. Starting from a compound designed using
generative models by Zhavoronkov et al. [2019] (gentrl-ddr1), we show how GT4SD can be used to swiftly design
molecules with desired properties using a battery of algorithms available in the library in two settings: unconditional
(bottom left) and conditional (bottom right).

As a more reﬁned approach, the GT4SD includes conditional generative models that can be primed with continuous prop-
erty constraints or molecular substructures (e.g., scaffolds) such as MoLeR [Maziarz et al., 2022], REINVENT [Blaschke
et al., 2020] or even with both simultaneously (Regression Transformer; RT [Born and Manica, 2022]). The molecules
obtained from those models, in particular MoLeR and RT, largely respected the similarity constraint and produced
many molecules with a Tanimoto similarity > 0.5 to gentrl-ddr1. The QED-constraint was best reﬂected by the RT
which generated models with an improved QED up to a similarity of 0.82 (cf. Figure 2, right). In a realistic discovery
scenario, the molecules generated with the described recipes could be manually reviewed by medicinal chemists and
selectively considered for synthesis and screening.

2 Discussion

The GT4SD serves as a ﬁrst step toward a harmonised generative modelling environment for accelerated scientiﬁc
discovery.

3

Context: This molecule was proposed by GENTRL (a deep generative model) as DDR1-inhibitor. It showed favorablepharmacokinetics in mice.For details see Zhavoronkovet al. (2019)Task: Find a similarmolecule (measured by Tanimotosimilarity) with an improved drug-likeness (QED) score.Step 1: Investigate the chemical space of molecular generative modelsStep 2: Investigate conditionalgenerative models thatcan be primed with propertiesor substructures(scaffolds)GT4SD –case study on a molecular discovery taskGraph models•GCPN •Graph AF•MoLeRLanguage models•VAE•AAE•ORGAN•GuacaMol& MOSESC1(C=O)cc(OC)c(O)cc1Models•REINVENT•MoLeR•Regression TransformerMoLeRREINVENTTanimoto: 0.59  QED: 0.47RegressionTransformerTanimoto: 0.82  QED: 0.39Tanimoto: 0.12  QED: 0.72QED: 0.38GT4SD - PREPRINT

For the future, we plan to expand application domains (e.g., climate, weather [Ravuri et al., 2021], sustainability,
geo-informatics and human mobility [Yan et al., 2017]) and integrate novel algorithms, ideally with the support of a
steadily growing open-science community.

Future developments will focus on two main components: expanding model evaluation and sample properties pre-
dictions; developing an ecosystem for sharing models built on top the functionalities exposed via the existing CLI
commands for model lifecycle management.

For the ﬁrst aspect, we will expand the currently integrated metrics from GuacaMol and Moses and explore bias
measures to better analyse performance in the light of the generated examples and their properties.

Regarding the sharing ecosystem, we believe GT4SD will further beneﬁt from an intuitive application hub that facilitates
distribution of pre-trained generative models (largely inspired by the Hugging Face model hub [Wolf et al., 2020b]) and
enables users to easily ﬁne-tune models on custom data for speciﬁc applications.

We anticipate GT4SD to democratise generative modelling in the natural sciences and to empower the scientiﬁc
community to access, evaluate and reﬁne large-scale pre-trained models across a wide range of applications.

3 Methods

GT4SD layout The GT4SD library follows a modular structure (Figure 3) where the main components are: (i)
algorithms for serving models in inference mode following a standardised API; (ii) training pipelines sharing a
common interface with algorithm families-speciﬁc implementations; (iii) domain-speciﬁc utilities shared across various
algorithms; (iv) a property prediction interface to evaluate generated samples (currently covering small molecules
and proteins); (v) frameworks implementing support for complex workﬂows, e.g., granular for training mixture of
generative and predictive models or enzeptional for enzyme design. Besides the core components, in the top-level there
are sub-modules for conﬁguration, handling the cloud object storage-based cache and error handling.

Figure 3: GT4SD structure. The library provides algorithms for inference, a CLI utility, target domains, interfaces
and implementations of generative modelling frameworks, a property prediction interface, and training pipelines. For
the inference algorithms (left box) and training pipelines (right box) we provide a representative sample of available
frameworks and methodologies.

GT4SD inference pipelines The API implementation underlying the inference pipelines has been designed to
support various generative models types: generation, conditional generation, controlled sampling and plain prediction

4

algorithmsclidomainsframeworkstraining_pipelines gt4sdconditional_generationmaterials controlled_samplinggenerationpredictionenzeptional granular torch guacamol moses paccmann language_modeling torchdrug inference saving training upload conditional_generationguacamolkey_bertpaccmann_rlregression_transformerreinventcontrolled_samplingadvanced_manufacturingpaccmann_gpgenerationhugging_facemolerpgtpolymer_blocksmosesaaevaeorganpaccmannrl_biased_vaevaetorchdruggcpngraphafguacamolsmiles_lstm_pposmiles_lstm_mhcsmiles_gagraph_gagraph_mctsgranularpropertiesmolecules proteins GT4SD - PREPRINT

algorithms. All the algorithms implemented in GT4SD follow a standard contract that guarantees a standardised way
to call an algorithm in inference mode. The speciﬁc algorithm interface and applications are responsible of deﬁning
implementation details and loading the model ﬁles from a cache that is synced with a a cloud object storage hosting
their versions.

GT4SD training pipelines Training pipelines follow the same philosophy adopted in the implementation of the
inference pipelines. A common interface offers the possibility to implement algorithm family-speciﬁc classes with
an arbitrary customisable training method that can be conﬁgured using a set of data-classes. Each training pipeline is
associated to a class implementing the actual training process and a triplet of conﬁguration data-classes that control
arguments for: model hyper-parameters, training parameters and data parameters.

GT4SD CLI commands To ease consumption of the pipelines and models implemented in GT4SD, a series of CLI
endpoints are available alongside the package: (i) gt4sd-inference, to inspect and run pipelines for inference; (ii)
gt4sd-trainer, to list and conﬁgure training pipelines; (iii) gt4sd-saving, to persist in a local cache a model
version trained via GT4SD for usage in inference mode; (iv) gt4sd-upload, to upload model versions trained via
GT4SD on a model hub to share algorithms with other users. The CLI commands allow to implement a complete
discover workﬂow where, starting from a source algorithm version, users can retrain it on custom datasets and make a
new algorithm version available in GT4SD.

4 Code Availability

GT4SD source code is available on GitHub: https://github.com/GT4SD/gt4sd-core. The repository also
contains exemplary notebooks and examples for users, including the presented case study. The complete documentation
is available at: https://gt4sd.github.io/gt4sd-core/.

5

GT4SD - PREPRINT

References

G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins. Quantifying the chemical beauty of drugs.

Nature chemistry, 4(2):90–98, 2012.

T. Blaschke, J. Arús-Pous, H. Chen, C. Margreitter, C. Tyrchan, O. Engkvist, K. Papadopoulos, and A. Patronov.
Reinvent 2.0: an ai tool for de novo drug design. Journal of Chemical Information and Modeling, 60(12):5918–5922,
2020.

J. Born and M. Manica. Regression transformer: Concurrent conditional generation and regression by blending
numerical and textual tokens. arXiv preprint arXiv:2202.01338, 2022. Spotlight talk at ICLR workshop on Machine
Learning for Drug Discovery.

J. Born, M. Manica, J. Cadow, G. Markert, N. A. Mill, M. Filipavicius, N. Janakarajan, A. Cardinale, T. Laino, and M. R.
Martínez. Data-driven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2.
Machine Learning: Science and Technology, 2(2):025024, 2021a.

J. Born, M. Manica, A. Oskooei, J. Cadow, G. Markert, and M. R. Martínez. PaccMannRL: De novo generation of
hit-like anticancer molecules from transcriptomic data via reinforcement learning. Iscience, 24(4):102269, 2021b.
N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher. Guacamol: benchmarking models for de novo molecular

design. Journal of chemical information and modeling, 59(3):1096–1108, 2019.

V. Chenthamarakshan, P. Das, S. Hoffman, H. Strobelt, I. Padhi, K. W. Lim, B. Hoover, M. Manica, J. Born, T. Laino,
et al. Cogmol: target-speciﬁc and selective drug design for covid-19 using deep generative models. Advances in
Neural Information Processing Systems, 33:4320–4332, 2020.

P. Das, T. Sercu, K. Wadhawan, I. Padhi, S. Gehrmann, F. Cipcigan, V. Chenthamarakshan, H. Strobelt, C. Dos Santos,
P.-Y. Chen, et al. Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations.
Nature Biomedical Engineering, 5(6):613–623, 2021.

W. Falcon and The PyTorch Lightning team.
PyTorchLightning/pytorch-lightning.

PyTorch Lightning, 3 2019. URL https://github.com/

R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla,
J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using
a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.

G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objective-reinforced
generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.
C. Hidalgo-Carcedo, S. Hooper, S. I. Chaudhry, P. Williamson, K. Harrington, B. Leitinger, and E. Sahai. Collective cell
migration requires suppression of actomyosin at cell–cell contacts mediated by ddr1 and the cell polarity regulators
par3 and par6. Nature cell biology, 13(1):49–59, 2011.

K. Huang, T. Fu, W. Gao, Y. Zhao, Y. Roohani, J. Leskovec, C. W. Coley, C. Xiao, J. Sun, and M. Zitnik. Therapeutics
data commons: Machine learning datasets and tasks for drug discovery and development. Proceedings of Neural
Information Processing Systems, NeurIPS Datasets and Benchmarks, 2021.

M. K. Jayatunga, W. Xie, L. Ruder, U. Schulze, and C. Meier. Ai in small-molecule drug discovery: a coming wave?

Nat. Rev. Drug Discov, 21:175–176, 2022.

W. Jin, R. Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph generation.

In

International conference on machine learning, pages 2323–2332. PMLR, 2018.

A. Kadurin, A. Aliper, A. Kazennov, P. Mamoshina, Q. Vanhaelen, K. Khrabrov, and A. Zhavoronkov. The cornucopia
of meaningful leads: Applying deep adversarial autoencoders for new molecule development in oncology. Oncotarget,
8(7):10883, 2017.

M. Krenn, F. Häse, A. Nigam, P. Friederich, and A. Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A
100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, nov 2020.
doi: 10.1088/2632-2153/aba947.

K. Maziarz, H. Jackson-Flux, P. Cameron, F. Sirockin, N. Schneider, N. Stieﬂ, M. Segler, and M. Brockschmidt.

Learning to extend molecular scaffolds with structural motifs. arXiv preprint arXiv:2103.03864, 2021.

K. Maziarz, H. Jackson-Flux, P. Cameron, F. Sirockin, N. Schneider, N. Stieﬂ, M. Segler, and M. Brockschmidt. Learning
to extend molecular scaffolds with structural motifs. In International Conference on Learning Representations, ICLR,
2022.

O. Méndez-Lucio, B. Baillif, D.-A. Clevert, D. Rouquié, and J. Wichard. De novo generation of hit-like molecules

from gene expression signatures using artiﬁcial intelligence. Nature communications, 11(1):1–10, 2020.

6

GT4SD - PREPRINT

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/
paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

P. G. Polishchuk, T. I. Madzhidov, and A. Varnek. Estimation of the size of drug-like chemical space based on gdb-17

data. Journal of computer-aided molecular design, 27(8):675–679, 2013.

D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov,
V. Aladinskiy, M. Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models.
Frontiers in pharmacology, 11:1931, 2020.

O. Prykhodko, S. V. Johansson, P.-C. Kotsias, J. Arús-Pous, E. J. Bjerrum, O. Engkvist, and H. Chen. A de novo
molecular generation method using latent vector based generative adversarial network. Journal of Cheminformatics,
11(1):1–13, 2019.

S. Ravuri, K. Lenc, M. Willson, D. Kangin, R. Lam, P. Mirowski, M. Fitzsimons, M. Athanassiadou, S. Kashem,
S. Madge, et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672–677,
2021.

M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller. Generating focused molecule libraries for drug discovery with

recurrent neural networks. ACS central science, 4(1):120–131, 2018.

C. Shi, M. Xu, Z. Zhu, W. Zhang, M. Zhang, and J. Tang. Graphaf: a ﬂow-based autoregressive model for molecular

graph generation. In International Conference on Learning Representations, ICLR, 2020.

S. Takeda, T. Hama, H.-H. Hsu, V. A. Piunova, D. Zubarev, D. P. Sanders, J. W. Pitera, M. Kogoh, T. Hongo,
Y. Cheng, et al. Molecular inverse-design platform for material industries. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pages 2961–2969, 2020.

G. Team. GT4SD (Generative Toolkit for Scientiﬁc Discovery), 2 2022. URL https://github.com/GT4SD/

gt4sd-core.

D. Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.

Journal of chemical information and computer sciences, 28(1):31–36, 1988.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison,
S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush.
Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, Oct. 2020a. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al.
Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical
methods in natural language processing: system demonstrations, pages 38–45, 2020b.

X.-Y. Yan, W.-X. Wang, Z.-Y. Gao, and Y.-C. Lai. Universal model of individual and population mobility on diverse

spatial scales. Nature communications, 8(1):1–9, 2017.

J. You, B. Liu, Z. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular

graph generation. Advances in neural information processing systems, 31, 2018.

A. Zhavoronkov, Y. A. Ivanenkov, A. Aliper, M. S. Veselov, V. A. Aladinskiy, A. V. Aladinskaya, V. A. Terentiev, D. A.
Polykovskiy, M. D. Kuznetsov, A. Asadulaev, et al. Deep learning enables rapid identiﬁcation of potent ddr1 kinase
inhibitors. Nature biotechnology, 37(9):1038–1040, 2019.

Z. Zhu, C. Shi, Z. Zhang, S. Liu, M. Xu, X. Yuan, Y. Zhang, J. Chen, H. Cai, J. Lu, et al. Torchdrug: A powerful and

ﬂexible machine learning platform for drug discovery. arXiv preprint arXiv:2202.08320, 2022.

7

