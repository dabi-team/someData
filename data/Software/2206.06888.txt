2
2
0
2

n
u
J

4
1

]
E
S
.
s
c
[

1
v
8
8
8
6
0
.
6
0
2
2
:
v
i
X
r
a

CERT: Continual Pre-Training on Sketches for Library-Oriented Code
Generation

Daoguang Zan1,2‚àó , Bei Chen3 , Dejian Yang3 , Zeqi Lin3 , Minsu Kim4‚àó ,
Bei Guan2,5 , Yongji Wang2,5,6 , Weizhu Chen7 , Jian-Guang Lou3
1Cooperative Innovation Center, Institute of Software, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
3Microsoft Research Asia
4Korea University
5Integrative Innovation Center, Institute of Software, Chinese Academy of Sciences
6State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences
7Microsoft Azure AI
{daoguang@, guanbei@, ywang@itechs.}iscas.ac.cn; minsu@korea.ac.kr
{beichen, deyang, zeqi.lin, wzchen, jlou}@microsoft.com

Abstract

Code generation is a longstanding challenge, aim-
ing to generate a code snippet based on a natu-
ral language description. Usually, expensive text-
code paired data is essential for training a code
generation model. Recently, thanks to the success
of pre-training techniques, large language models
are trained on large-scale unlabelled code corpora
and perform well in code generation.
In this pa-
per, we investigate how to leverage an unlabelled
code corpus to train a model for library-oriented
code generation. Since it is a common practice
for programmers to reuse third-party libraries, in
which case the text-code paired data are harder to
obtain due to the huge number of libraries. We ob-
serve that library-oriented code snippets are more
likely to share similar code sketches. Hence, we
present CERT with two steps: a sketcher gener-
ates the sketch, then a generator Ô¨Ålls the details
in the sketch. Both the sketcher and the generator
are continually pre-trained upon a base model using
unlabelled data. Furthermore, we craft two bench-
marks named PandasEval and NumpyEval to evalu-
ate library-oriented code generation. Experimental
results demonstrate the impressive performance of
CERT. For example, it surpasses the base model
by an absolute 15.67% improvement in terms of
pass@1 on PandasEval. Our work is available at
https://github.com/microsoft/PyCodeGPT.

1 Introduction
Code generation, aiming to generate a code snippet for a
given natural language description, is a longstanding chal-
lenge in the artiÔ¨Åcial intelligence community. Usually, to

Figure 1: An example in Python: multiple code snippets using Pan-
das may have the same sketch after anonymizing user-deÔ¨Åned terms.

train a code generation model with good performance, the
massive amount of code snippets paired with natural language
descriptions are indispensable [Sun et al., 2019; Lu et al.,
2021]. However, it is costly and time-consuming to anno-
tate such a dataset. To alleviate this problem, inspired by
GPT-3‚Äôs powerful zero-shot natural language generation abil-
ity [Brown et al., 2021], recent years have witnessed a trend
to train large language models using large-scale code corpora
(e.g., GitHub), and expect these models to work well directly
on code generation tasks, without Ô¨Åne-tuning on expensive
text-code pairs. For example, Codex shows that a 12B param-
eters language model can solve 28.8% of standalone Python
programming problems1.

In this paper, we focus on investigating whether and how

‚àóWork done during the internship at Microsoft Research Asia.

1It is measured on HumanEval [Chen et al., 2021a] with pass@1.

importpandasaspd'''Read in the csv using pandas'''tweets_data= pd.read_csv('./cookbook.csv')# Count value on the column 'label'tweets_data["label"].value_counts(normalize=False)# randomly sample data and get its 'tweet' valuetweets_data["tweet"].sample().values[0]importpandasaspd# Using pandas to read the file anonyijcai22.csvdf_ijcai= pd.read_csv('./anonyijcai22.csv')# In the column title,countthe number of values# Note that the argument normalize is set to Truedf_ijcai["title"].value_counts(normalize=True)# Randomly select a row, return 'abstract' valuedf_ijcai["abstract"].sample().values[0]importpandasaspdvariable= pd.read_csv(string)variable[string].value_counts(normalize=bool)variable[string].sample().values[number]‚Ä¶Code snippetCode snippet The same sketch1ùëõ 
 
 
 
 
 
language models pre-trained on code corpora (without Ô¨Åne-
tuned on pairwise labelled data) can generate library-oriented
code snippets rather than standalone ones. During software
development, it is a common practice for programmers to
reuse third-party libraries (e.g., Pandas and NumPy) to imple-
ment needed functionalities. It is not easy for programmers
to learn how to use these libraries properly. For example, ac-
cording to our statistics, more than 40% of StackOverÔ¨Çow
questions with ‚ÄúPython‚Äù tag also have at least one library tag.
Moreover, for library-oriented code generation, the necessity
of training the model without pairwise labelled data is raised,
as programmers usually need to reuse different libraries in
different scenarios, and it is extremely costly to label sufÔ¨Å-
cient text-code pairs that cover most of these libraries.

Compared to standalone code snippets, library-oriented
code snippets are more likely to share similar sketches.
Sketch is the code structure after anonymizing the user-
deÔ¨Åned terms in the code, such as variable names, method
names, constants, etc., which has also been identiÔ¨Åed as an
API usage pattern in previous research litterateurs on soft-
ware data mining [Zhong et al., 2009; Wang et al., 2013;
Niu et al., 2017]. An example is shown in Figure 1. After
anonymizing variables and constants, multiple code snippets
using the Pandas APIs may have the same (or similar) sketch.
Based on this observation, a natural idea to improve library-
oriented code generation is to decompose this task into two
subtasks: generating the sketch and then Ô¨Ålling in the details.
Many methods based on this idea have been proposed in dif-
ferent code generation tasks (e.g., Coarse-to-Fine [Dong and
Lapata, 2018] and PLOTCODER [Chen et al., 2021b]) and
have shown that this idea can effectively improve the quality
of generated code snippets. However, these methods are pro-
posed for the Ô¨Åne-tuning process, in which high-quality text-
code pairs are required to derive supervision signals for the
two-step generation. Therefore, in our scenario that no pair-
wise labelled data is provided, a research question arises: how
to leverage the insight of sketching to enhance the language
model pre-training on unlabelled code corpora, thus improv-
ing the quality of generated library-oriented code snippets?

To meet the challenge, we propose CERT (for sketCher
and gEneRaTor), a continual pre-training approach on
sketches for library-oriented code generation.
In CERT, a
sketcher Ô¨Årstly focuses on predicting a sketch, which omits
user-deÔ¨Åned details; then, a generator uses the sketch as a
prompt to generate the complete code. Both the sketcher and
the generator are continually pre-trained based on a base lan-
guage model for code, using unlabelled code corpora rather
than pairwise labelled data. In addition, we craft two evalua-
tion benchmarks for Python libraries, called PandasEval and
NumpyEval, each including 101 programming problems us-
ing Pandas and NumPy, respectively. We perform extensive
experiments on CERT. Results indicate that CERT has su-
perior performance on library-oriented code generation. We
further draw several insights via thorough analysis.

Figure 2: Two examples of programming problems from the Panda-
sEval and NumpyEval benchmarks. Context is shown with a white
background and the target code with a gray background.

to solve a programming problem: generate target code based
on context. Context contains natural language problem de-
scription in the form of code comments, and a code snippet
that includes statements such as import, function header and
variable deÔ¨Ånition; target code is a code snippet that solves
the programming problem described in the context. For-
mally, let x = (x1, x2, ¬∑ ¬∑ ¬∑, xN ) denote the context, where
each xn can be either a code token or a natural language to-
ken. Given x, the code generation model can be formulated
as y = M(x), where y = (y1, y2, ¬∑ ¬∑ ¬∑, yM ) denotes the target
code and each ym is a code token.

For standalone code generation, the programming prob-
lem is expected to be solved by a code snippet without us-
ing third-party libraries; conversely, for library-oriented code
generation, the target code y contains library API calls. Two
examples of library-oriented programming problems can be
found in Figure 2. Note that carefully labelled context and tar-
get code pairs are indispensable for model Ô¨Åne-tuning, while
our proposed approach only requires continual pre-training
on unlabelled code corpora.

3 Methodology
In this section, we introduce our base models, followed by the
details of our proposed approach CERT.

3.1 Base Models
Codex [Chen et al., 2021a] is a milestone pre-trained model
that can generate decent code, but it is not publicly avail-
able. Several attempts have been made to reproduce Codex‚Äôs
powerful code generation capability, e.g., CodeClippy2 and
CodeParrot3, but their performance in Python are not sat-
isfactory. To this end, we present PYCODEGPT 4, a pre-
trained language model, which has the ability to generate
pretty good standalone Python code, for example, achieving
8.33% pass@1 on HumanEval [Chen et al., 2021a]. Spe-
cially, PYCODEGPT is a 110M parameters model based on
GPT-Neo [Black et al., 2021]. We collected 60.6M raw
python Ô¨Åles with a total size of 330GB. After a series of
data pre-processing strategies, such as de-duplicating python
Ô¨Åles, cleaning and formatting the contents, etc., the Ô¨Ånal pre-
training corpus contains about 13.0M high-quality python

2 Task Formulation

Before diving into the details of our proposed approach, we
start with a formal description of the task. Code generation is

2https://github.com/CodedotAl/gpt-code-clippy
3https://huggingface.co/transformersbook/codeparrot
4More details about PYCODEGPT are in Appendix.

import pandas as pddef normalize(df):    # Normalization using pandas    # We simply subtract the mean and divide by standard deviation       on df.iloc[:,0,-1] obj with axis is zero.    # Return the normalized dataframe    func_ = lambda x: (x-x.mean()) / x.std()    df.iloc[:,0:-1] = df.iloc[:,0:-1].apply(func_, axis=0)    return dfimport numpy as npx = np.array([[1], [2], [3]])# Numpy Vector (N,1) dimension -> (N,) dimension conversionout = x.reshape(3,)Figure 4: The pre-deÔ¨Åned symbols in sketcher.

training process of the sketcher and generator for this library.

Sketcher. Given the library-oriented sub-corpus D, we per-
form the sketching operation on each Ô¨Åle d ‚àà D. An example
is shown in the upper part of Figure 5. The sketching opera-
tion is used to anonymize the user-deÔ¨Åned terms in the code
Ô¨Åle with our pre-deÔ¨Åned symbols. The Ô¨Åle after sketching is
denoted as ¬Ød. We design three different types of sketching op-
erations: 1) only anonymizing the user-deÔ¨Åned constants (De-
fault CERT); 2) only anonymizing the user-deÔ¨Åned names,
including function names, class names, and variable names
(CERT-N); and 3) anonymizing both the user-deÔ¨Åned con-
stants and names (CERT-NC). For example, in Figure 5, the
constant ‚Äòuser 1‚Äô is anonymized with the pre-deÔ¨Åned sym-
bol ‚Äòstring‚Äô. The details of pre-deÔ¨Åned symbols are shown
in Figure 4. Then, we continually pre-train the base model on
the library-oriented corpus after sketching, and we obtain the
sketcher model. The pre-training objective is the same as that
of the base model. We pre-train the model for 100K steps on
a cluster of 8 NVIDIA V100 GPUs with 32GB memory.

Generator.
In order to prepare the pre-trained corpus
for the generator, we Ô¨Årstly split the original Ô¨Åle d and
the sketching Ô¨Åle ¬Ød into K blocks6, and obtain d =
(d1, d2, ¬∑ ¬∑ ¬∑, dK) and ¬Ød = ( ¬Ød1, ¬Ød2, ¬∑ ¬∑ ¬∑, ¬ØdK). Each block is
a relatively complete code snippet, such as a function or
a class. Note that before splitting, we remove the natural
language code comments from the sketching Ô¨Åle ¬Ød. Then,
the two Ô¨Åles are cross-merged to give a merged Ô¨Åle ÀÜd =
( ¬Ød1, d1, ¬Ød2, d2, ¬∑ ¬∑ ¬∑, ¬ØdK, dK). This is to mimic the process of
having a sketch as a prompt for each block. An example is
shown in the lower part of Figure 5. Then, the base model
is continually pre-trained on all the merged Ô¨Åles ÀÜd and we
obtain the generator model. As with the sketcher model, we
continually pre-train for 100K steps.

4 Benchmark Construction
Third-party libraries are widely used in reality, while little
work has been done to evaluate library-oriented code gen-
eration. To meet this challenge, we craft PandasEval and
NumpyEval, two benchmarks for library-oriented code gen-
eration in Python. Each sample in the benchmarks is a pro-
gramming problem consisting of context and target code. The
programming problems are solved using libraries, where Pan-
das is for PandasEval, and NumPy is for NumpyEval. The
benchmarks are expected to be diverse, authentic, high qual-
ity, moderately difÔ¨Åcult, and unseen during pre-training.

Figure 3: Overview of CERT: a sketcher and a generator.

Ô¨Åles with the size of 96GB. PYCODEGPT is pre-trained for
200K steps and 100B tokens on a cluster of 16 NVIDIA V100
GPUs with 32GB memory. The pre-training time is about
2 days. We summarize the three key points that make PY-
CODEGPT powerful: 1) a large amount of carefully cleaned
data for pre-training; 2) a newly trained tokenizer, which is
specialized in python; and 3) a resampling strategy that pri-
oritizes high-quality data. Besides PYCODEGPT, we also
regard CODEGEN (MONO 350M) [Nijkamp et al., 2022] as
one of our base models, which is by far the best performing
publicly available model on HumanEval5.

3.2 CERT
As mentioned in Section 2, code generation is to generate
target code y based on context x. Since we observe that
library-oriented code snippets are more likely to share similar
sketches, we present a novel approach CERT and decompose
the code generation model M into two modules: a sketcher
MS and a generator MG. Figure 3 shows the overview of
CERT with a concrete example in Pandas. Given x as the
input, the sketcher predicts s, which is the sketch of the target
code y. The sketcher generates multiple candidate sketches
(200 in our experiments) and we choose the one that appears
the most. Then, the input of the generator is the concatenation
of s and x. Formally, the process of CERT can be written as
s = MS(x) and y = MG([s; x]). Note that if the sketch s
is already a complete code snippet without anonymous sym-
bols, we directly take it as the Ô¨Ånal prediction instead of using
the generator; and if the sketch s is an empty sequence, we di-
rectly feed x into the generator.

We build the sketcher and generator on the top of the
base model (PYCODEGPT or CODEGEN) by continual pre-
training. At Ô¨Årst, we extract the python Ô¨Åles that use a spe-
ciÔ¨Åc library (e.g., Pandas) from the whole pre-training corpus
(13.0M Ô¨Åles mentioned in Section 3.1), and obtain the sub-
corpus denoted by D. Then, we will detail the continual pre-

5CODEGEN was released during the review period of this paper.

6We use pip-tools: autopep8, docformatter and redbaron.

df = pd.read_csv('anonyijcai22.csv', names=['user_1', 'user_2', 'user_3'])Sketcherdf = pd.read_csv(string, names=[string, string, string])ContextSketchSketch+ContextTargetCodedf = pd.read_csv(string, names=[string, string, string])import pandas as pd# reading data from anonyijcai22.csv into DataFrame w/the argument `names`: a list ['user_1', 'user_2', 'user_3']ConcatenatingGeneratorimport pandas as pd# reading data from anonyijcai22.csv into DataFrame w/the argument `names`: a list ['user_1', 'user_2', 'user_3'] Type/Pre-defined symbol (user-defined constants)int/number;string/string; float/float; boolean/bool; complex/complexbinary_string/binarystring; interpolated_string/interstringinterpolated_raw_string/interrawstring; float_exponent/floatexponantraw_string/rawstring; unicode_string/unicodestringdef/func; class/AnClass; variable/variableType/Pre-defined symbol (user-defined names)Figure 5: Training data preparation for sketcher and generator with an example in Pandas.

In order to craft programming problems using libraries, we
refer to StackOverÔ¨Çow7, a Q&A website for programmers.
There are plenty of real-world programming problems posted
by real users, which helps us to improve the authenticity of
our data. SpeciÔ¨Åcally, we search for posts using the library
tag on StackOverÔ¨Çow, and select those with high votes. To
ensure quality, we only refer to posts with accepted answers.
We go through a post‚Äôs question and its accepted answer, then
manually organize them into the form needed for our bench-
marks, containing both context and target code. We also pol-
ish all programming problems so that the problem descrip-
tions are clear and the codes are correct. Note that we keep
the intentions and the descriptions of the programming prob-
lems consistent with the posts to the maximum extent. Fi-
nally, two programmers with more than three years of coding
experience in the library are invited to act as code generation
models and check the quality of the data.

As a result, we craft 101 programming problems for Pan-
dasEval and NumpyEval, respectively. Each programming
problem is equipped with test cases for evaluation. For the
programming problems in the form of a function, such as the
bottom one in Figure 2, we create 20 test cases for each of
them. For the others that contain no functions, such as the
top one in the Figure 2, we provide 1 test case to check the
correctness of predicted variable (e.g., out in Figure 2). In
total, 64% programming problems in PandasEval and 30%
in NumpyEval are equipped with 20 test cases. In addition,
we craft programming problems that refer to StackOverÔ¨Çow
rather than GitHub, and also carefully organize and polish the
problems, so that we can ensure they are unseen by the pre-
trained models.

5 Experiments

In this section, we evaluate CERT on PandasEval and
NumpyEval to verify its effectiveness.

Evaluation Metrics. We use pass@k as the metrics. When
k code samples are generated per problem, pass@k indicates
the fraction of correct ones. But computing pass@k in this
way may have high variance. Hence, we follow Chen et

pass@1 Model

Model
GPT-Neo 125M
AlphaCode 89M
Codex 42M
Codex 2.5B
PYCODEGPT 110M

0.75
4.30
5.06
21.36
8.33

pass@1
4.79
GPT-Neo 1.3B
3.80
CodeParrot 110M
8.22
Codex 85M
Codex 12B
28.81
CODEGEN-MONO 350M 12.76

Table 1: The pass@1 (%) results on HumanEval benchmark. We
omit CodeT5 (220M), CodeGPT-Adapted (124M), and CodeClippy
(125M) as their pass@1 = 0.

al. [2021a] to generate n ‚â• k code samples per problem
(n = 200 in our experiments) and count the number of cor-
rect samples c. If n ‚àí c < k, then pass@k = 1; otherwise,
pass@k = 1 ‚àí (cid:81)n
i=n‚àíc+1(1 ‚àí k/i). Note that a predicted
code is correct if it can pass all the test cases.
Implementation Details. We implement our approach us-
ing PyTorch [Paszke et al., 2019], HuggingFace‚Äôs transform-
ers library [Wolf et al., 2019], and DeepSpeed8. In the train-
ing phase of PYCODEGPT, we set the batch size to 10, the
window size to 1024, the learning rate to 5e-4, the gradi-
ent accumulation steps to 4 and the weight decay to 0.1.
The settings of sketcher and generator are the same as PY-
CODEGPT. We use the mixed-precision of FP16 to accelerate
the pre-training. In inference phase, we set the temperature to
one of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]. The best
performance is reported across the above hyper-parameters.

5.1 Main Results
Before evaluating CERT, we would like to evaluate our
base model PYCODEGPT on HumanEval [Chen et al.,
2021a] compared to several advanced pre-trained models. As
shown in Table 1, PYCODEGPT (110M) achieves compet-
itive 8.33% pass@1.
It largely exceeds other models with
comparable parameters, e.g., AlphaCode (89M) [Li et al.,
2022], CodeClippy (125M), and CodeParrot (110M), and
also is better than the larger model GPT-Neo (1.3B).

Then, our proposed CERT is evaluated on PandasEval and
NumpyEval. We train CERT on two base models, includ-
ing PYCODEGPT and CODEGEN, named PYCODEGPT-
CERT and CODEGEN-CERT, respectively. For each bench-
mark, we extract corresponding library-oriented Ô¨Åles to train

7https://stackoverÔ¨Çow.com

8https://github.com/microsoft/DeepSpeed

import pandas as pd# reading data from anonyijcai22.csv into DataFrame w/the argument `names`: a list ['user_1', 'user_2', 'user_3']df = pd.read_csv('anonyijcai22.csv', names=['user_1', 'user_2', 'user_3'])import pandas as pd# reading data from anonyijcai22.csv into DataFrame w/the argument `names`: a list ['user_1', 'user_2', 'user_3']df = pd.read_csv(string, names=[string, string, string])import pandas as pddf = pd.read_csv(string, names=[string, string, string])import pandas as pd# reading data from anonyijcai22.csv into DataFrame w/the argument `names`: a list ['user_1', 'user_2', 'user_3']df = pd.read_csv('anonyijcai22.csv', names=['user_1', 'user_2', 'user_3'])Base ModelPandas fileSketchingConcatenatingContinual Pre-trainingRemoving commentsConcatenatingSketching fileMerged fileBase ModelSketcherGeneratorBenchmark Model

Pandas
Eval

Numpy
Eval

CodeT5 (220M)
CodeGPT-Adapted (124M)
CodeClippy (125M)
CodeParrot (110M)
CODEGEN (350M)
CODEGEN-XL
CODEGEN-CERT
PYCODEGPT (110M)
PYCODEGPT-XL
PYCODEGPT-CERT
- PYCODEGPT-CERT-N
- PYCODEGPT-CERT-NC
- PYCODEGPT-CERTg
CodeT5 (220M)
CodeGPT-Adapted (124M)
CodeClippy (125M)
CodeParrot (110M)
CODEGEN (350M)
CODEGEN-XL
CODEGEN-CERT
PYCODEGPT (110M)
PYCODEGPT-XL
PYCODEGPT-CERT
- PYCODEGPT-CERT-N
- PYCODEGPT-CERT-NC
- PYCODEGPT-CERTg

pass@1
0.00
0.62
0.14
3.21
14.24
21.07
26.40(cid:78)12.16
12.75
19.80
28.42(cid:78)15.67
23.66
19.07
20.58
0.00
1.59
0.08
8.42
19.31
27.33
32.00(cid:78)12.69
18.04
20.50
31.47(cid:78)13.43
24.91
19.88
16.55

pass@10
0.00
2.65
0.92
13.62
30.71
37.67
46.49(cid:78)15.78
37.80
46.80
48.04(cid:78)10.24
41.73
41.50
42.61
0.10
4.17
0.59
21.46
40.89
44.75
49.45(cid:78)8.56
38.13
43.40
46.42(cid:78)8.29
42.88
41.64
44.07

pass@100
0.00
4.95
1.92
33.27
46.04
49.07
58.16(cid:78)12.12
59.65
60.04
60.96(cid:78)1.31
55.08
54.82
56.00
0.74
8.54
1.24
45.94
60.58
63.39
67.82(cid:78)7.24
63.37
56.06
66.41(cid:78)3.04
54.02
55.82
56.80

Table 2: The pass@k (%) results on PandasEval and NumpyEval.
The absolute improvements of CERT over the base model are high-
lighted in red. Also, we report the performance of different sketch-
ing operations (CERT-N and CERT-NC) and the performance of
CERTg trained for general code generation.

Figure 6: The pass@1 result with respect to the number of APIs.

CERT. The Ô¨Åle numbers are about 0.61M for Pandas and
2.62M for NumPy. Baselines include our base models PY-
CODEGPT and CODEGEN; PYCODEGPT-XL and CODE-
GEN-XL, which are continual pre-trained PYCODEGPT and
CODEGEN on the extracted library-oriented Ô¨Åles; and ad-
vanced pre-trained models for code, like CodeT5 [Wang et
al., 2021], CodeGPT [Lu et al., 2021], CodeClippy and
CodeParrot. Table 2 summarizes the performance. CERT
consistently outperforms all the baselines by a large mar-
gin. The absolute improvements over PYCODEGPT and
CODEGEN are shown in red, which are signiÔ¨Åcant, for ex-
ample, 12.69% pass@1 for CODEGEN-CERT and 13.43%
pass@1 for PYCODEGPT-CERT on NumpyEval. The re-
sults demonstrate the effectiveness of CERT with the idea of
leveraging sketches for library-oriented code generation.

Additionally, we would like to investigate the performance
of CERT with respect to the number of API calls involved
in the target code. We divided the programming problems
in each benchmark into four parts based on the number of
APIs. As shown in Figure 6, compared to PYCODEGPT, PY-
CODEGPT-CERT has a steady improvement on each part. It
indicates that CERT can improve the performance of library-
oriented code generation of varying difÔ¨Åculties.

Figure 7: The exact match accuracy of sketches. The sketcher refers
to the one in PYCODEGPT-CERT.

Model
PYCODEGPT
PYCODEGPT-CERTg

pass@1
8.33
8.25

pass@10
13.36
14.12

pass@100
19.13
20.41

Table 3: The pass@k (%) results of PYCODEGPT and PY-
CODEGPT-CERTg on HumanEval.

5.2 Closer Analysis
We conduct some closer analyses to provide more insights.
Different Types of Sketching. As mentioned in Sec-
tion 3.2, we propose three types of sketching operations. By
default, CERT only anonymizes user-deÔ¨Åned constants. The
other two types include CERT-N, which anonymizes only
user-deÔ¨Åned names, and CERT-NC, which anonymizes both
user-deÔ¨Åned constants and names. As shown in Table 2,
CERT with default setting achieves the best performance.
This observation may be related to the inherent character-
istics of Pandas and NumPy. They are commonly used in
data statistics and analysis, often involving manipulation of
the data constants. Thus, it is necessary to anonymize user-
deÔ¨Åned constants. Anonymizing both user-deÔ¨Åned constants
and names would probably make the sketches too abstract.
Quality of Generated Sketches.
Intuitively, it is easier to
generate a sketch than a complete code. Thus, we would like
to evaluate the quality of sketches generated by the sketcher
of CERT. We use exact match accuracy as the metric and in-
clude PYCODEGPT and PYCODEGPT-XL for comparison.
For PYCODEGPT and PYCODEGPT-XL, we anonymize the
user-deÔ¨Åned constants in the predicted code to obtain the
sketch. As shown in Figure 7, our sketcher surpasses base-
lines by 15.20% and 14.21% on PandasEval and NumpyEval,
respectively. It indicates that the sketcher can generate high-
quality sketches, and such sketches further beneÔ¨Åt the gener-
ator. Additionally, the generator does not necessarily require
an exactly correct sketch, as the sketch is just a prompt (A
case will be discussed in Section 5.3).
CERT for General Code Generation. Technically speak-
ing, CERT can also be used for general code generation
tasks, not just the library-oriented ones. Concretely, fol-
lowing the procedure in Figure 5, we can continually pre-
train PYCODEGPT using the whole 13.0M python corpus
instead of the extracted library-oriented Ô¨Åles, and obtain the
model we called CERTg. We evaluate CERTg for gen-
eral code generation on HumanEval compared to the base

The number of APIspass@10%10%20%30%40%1234+PandasEval/PyCodeGPTPandasEval/PyCodeGPT-CERTNumpyEval/PyCodeGPTNumpyEval/PyCodeGPT-CERTExact Match Accuracy of Sketches0%5%10%15%20%25%30%PyCodeGPTPyCodeGPT-XLSketcherPandasEvalNumpyEvalFigure 8: Three library-oriented code generation cases.

Benchmark Model

Pandas
Eval

Numpy
Eval

PYCODEGPT-CERT
CODEGEN-CERT
GPT-3
Codex
PYCODEGPT-CERT
CODEGEN-CERT
GPT-3
Codex

pass@1
Size
110M 28.42
350M 26.40
12.97
175B
12B
18.88
110M 31.47
350M 32.00
175B
16.25
34.42
12B

pass@10
48.04
46.49
20.54
43.05
46.42
49.45
22.15
55.75

pass@100
60.96
58.16
25.43
64.37
66.41
67.82
27.38
71.74

Table 4: GPT-3 and Codex on PandasEval and NumpyEval.

model PYCODEGPT. As shown in Table 3, they have similar
pass@k results. This observation veriÔ¨Åes our assumption that
library-oriented code snippets are more likely to share simi-
lar sketches, so it is beneÔ¨Åcial to use sketches as prompts in
this situation. But in the general case, it is not useful. Mean-
while the results of CERTg on PandasEval and NumpyEval
are in Table 2. CERTg is inferior to CERT, suggesting that
extracting library-oriented Ô¨Åles is essential for CERT to learn
the knowledge of library-oriented sketches.
Evaluation of GPT-3 and Codex. We evaluate GPT-3 and
Codex to see how these extremely large models perform on
PandasEval and NumpyEval. As shown in Table 4, CERT
is competitive with only 110M parameters. Such observa-
tion proves CERT‚Äôs powerful code generation capability in
library-oriented programming problems.

5.3 Case Study
For a more comprehensive comparison, we show three
the
cases in Figure 8. We show in turn the context,
golden target code, the predicted code of PYCODEGPT and
PYCODEGPT-XL, the sketch generated by PYCODEGPT-
CERT and the predicted code of PYCODEGPT-CERT. Case
1 is from PandasEval, both PYCODEGPT-CERT‚Äôs sketcher
and generator reach the correct results, while the baselines do
not. It reveals that sketcher and generator can work well to-
gether. Case 2 is from NumpyEval, the sketcher predicts the
correct sketch, which has no anonymous symbols, then this
sketch is the Ô¨Ånal predicted code. It indicates that the sketcher
has the ability to predict code without user-deÔ¨Åned constants.
At last, in Case 3, the sketcher makes a wrong prediction
pd.Series([[number*2]*3]), while the correct sketch
is pd.Series([[number*2], number, [[number*2],
number]*2]). But PYCODEGPT-CERT‚Äôs generator recti-
Ô¨Åes it and Ô¨Ånally generates the correct code. Since the sketch

acts only as a prompt, it is not necessarily to be perfectly cor-
rect, which endows the generator with solid robustness.

6 Related Work
The most related work is the line of large pre-trained mod-
els for code. As for the encoder-style pre-trained models,
they cannot be employed directly to generate code, such as
CuBERT [Kanade et al., 2020], CodeBERT [Feng et al.,
2020], and GraphCodeBERT [Guo et al., 2020]. As for the
decoder-style or encoder-decoder-style ones, they are trained
on large unlabelled code corpora and can work directly on
code generation task, such as CodeT5 [Wang et al., 2021],
CodeGPT [Lu et al., 2021], PLBART [Ahmad et al., 2021],
PolyCoder [Xu et al., 2022], CODEGEN [Nijkamp et al.,
2022], AlphaCode [Li et al., 2022], and Codex [Chen et al.,
2021a]. All of them focus on generating standalone code,
while we investigate library-oriented code generation. Also,
similar to our idea, there are several works leveraging code
sketches, for example, Coarse-to-Fine [Dong and Lapata,
2018], BAYOU [Murali et al., 2018], SKETCHADAPT [Nye
et al., 2019], and PLOTCODER [Chen et al., 2021b]. How-
ever, they require labelled text-code paired data for Ô¨Åne-
tuning, while our models continually pre-train on unlabelled
code corpora. For code generation benchmarks, there are
few works, including APPS [Hendrycks et al., 2021], Hu-
manEval [Chen et al., 2021a], and PlotCoder‚Äôs dataset [Chen
et al., 2021b]. The former two ones focus on evaluating the
capability of generating standalone code, and the last one is
primarily devoted to generating plotting APIs and visualiza-
tion code. PandasEval and NumpyEval are dedicated to eval-
uating the performance of library-oriented code generation.

7 Conclusion
In this paper, we propose a novel approach CERT for library-
oriented code generation. It leverages the code sketches and
consists of a sketcher and a generator. The sketcher and gen-
erator are continually pre-trained upon a base model using
unlabelled code corpora. Also, we carefully craft two bench-
marks to evaluate library-oriented code generation, namely
PandasEval and NumpyEval. Experimental results and thor-
ough analysis show the effectiveness of CERT. In future
work, we are interested in code generation for private libraries
with fewer data.

import pandas as pd# creating a Series from a list [56, 24, 421, 90]my_series = pd.Series([56, 24, 421, 90]) pd.Series(list(range(56, 24, 421))) pd.Series(range(56, 24, 421), name='my_ser ies') pd.Series([number, number, number, number]) pd.Series([56, 24, 421, 90])The given contextPyCodeGPTimport numpy as npA = np.array([[1, 2], [3, 0]])# How can I know the (row, column) index of the minimum of a numpy array/matrix?# Use unravel_index()out = np.unravel_index(A.argmin(), A.shape) np.argmin(A, axis=0) np.unravel_index(A, (2, 2)) np.unravel_index(A.argmin(), A.shape)‚àö‚àö‚àö√ó√ó√ó√óGolden target codePyCodeGPT-XLPyCodeGPT-CERT-SketcherPyCodeGPT-CERT-GeneratorCase 1Case 2import numpy as np# create a numpy array composed of a list [[[8, 7], 2], [[5, 6], 1], [[8, 2], 6]]array = np.array([[8, 7], 2, [[5, 6], 1], [[8, 2], 6]])np.array([[8, 7], [2, 5], [5, 6], [8, 2], [5, 6], [8, 2], [2, 6]])[[8, 7], [5, 6], [8, 2]]np.array([[number, number], [number, number], [number, number]])np.array([[8, 7], 2, [[5, 6], 1], [[8, 2], 6]])‚àö√ó√ó√óCase 3Acknowledgements

We thank all reviewers and our colleagues, Qian Liu, Yanlin
Wang and Shi Han, for their constructive comments.

References

[Ahmad et al., 2021] Wasi Ahmad, Saikat Chakraborty,
Baishakhi Ray, and Kai-Wei Chang. UniÔ¨Åed pre-training
for program understanding and generation. In North Amer-
ican Chapter of the Association for Computational Lin-
guistics, pages 2655‚Äì2668, 2021.

[Black et al., 2021] Sid Black, Gao Leo, Phil Wang, Connor
Leahy, and Stella Biderman. GPT-Neo: Large Scale Au-
toregressive Language Modeling with Mesh-TensorÔ¨Çow.
In Zenodo, March 2021.

[Brown et al., 2021] Tom B Brown, Benjamin Mann, Nick
Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, et al. Language models are few-shot
learners. In Neural Information Processing Systems, 2021.

[Chen et al., 2021a] Mark Chen, Jerry Tworek, Heewoo Jun,
Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021.

[Chen et al., 2021b] Xinyun Chen, Linyuan Gong, Alvin
Cheung, and Dawn Song. Plotcoder: Hierarchical decod-
ing for synthesizing visualization code in programmatic
In Association for Computational Linguistics,
context.
pages 2169‚Äì2181, 2021.

[Dong and Lapata, 2018] Li Dong and Mirella Lapata.
In
Coarse-to-Ô¨Åne decoding for neural semantic parsing.
Association for Computational Linguistics, pages 731‚Äì
742, 2018.

[Feng et al., 2020] Zhangyin Feng, Daya Guo, Duyu Tang,
Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. CodeBERT: A pre-
trained model for programming and natural languages. In
Findings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1536‚Äì1547, 2020.

[Gao et al., 2020] Leo Gao, Stella Biderman, Sid Black,
Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al.
The pile: An 800gb dataset of diverse text for language
modeling. arXiv preprint arXiv:2101.00027, 2020.

[Guo et al., 2020] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin
Feng, Duyu Tang, et al. GraphCodeBERT: Pre-training
code representations with data Ô¨Çow. In International Con-
ference on Learning Representations, 2020.

[Hendrycks et al., 2021] Dan Hendrycks, Steven Basart,
Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan
Guo, et al. Measuring coding challenge competence with
apps. In Neural Information Processing Systems Datasets
and Benchmarks Track, 2021.

[Kanade et al., 2020] Aditya Kanade,

Petros Maniatis,
Gogul Balakrishnan, and Kensen Shi.
Learning and
In
evaluating contextual embedding of source code.
International Conference on Machine Learning, pages
5110‚Äì5121, 2020.

[Li et al., 2022] Yujia Li, David Choi, Junyoung Chung,
Nate Kushman, Julian Schrittwieser, R¬¥emi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. Competition-level code generation with alphacode.
arXiv preprint arXiv:2203.07814, 2022.

[Lu et al., 2021] Shuai Lu, Daya Guo, Shuo Ren, Junjie
Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, et al. CodeXGLUE: A machine learning bench-
mark dataset for code understanding and generation. arXiv
preprint arXiv:2102.04664, 2021.

[Murali et al., 2018] Vijayaraghavan Murali, Letao Qi, et al.
Neural sketch learning for conditional program generation.
In International Conference on Learning Representations,
2018.

[Nijkamp et al., 2022] Erik Nijkamp, Bo Pang, Hiroaki
Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, et al. A
arXiv
conversational paradigm for program synthesis.
preprint arXiv:2203.13474, 2022.

[Niu et al., 2017] Haoran Niu, Iman Keivanloo, and Ying
Zou. API usage pattern recommendation for software de-
velopment. Journal of Systems and Software, 129:127‚Äì
139, 2017.

[Nye et al., 2019] Maxwell Nye, Luke Hewitt,

Joshua
Tenenbaum, and Armando Solar-Lezama. Learning to in-
fer program sketches. In International Conference on Ma-
chine Learning, pages 4861‚Äì4870, 2019.

[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco
Massa, Adam Lerer, et al. PyTorch: An imperative style,
high-performance deep learning library. In Neural Infor-
mation Processing Systems. 2019.

[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon
Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners.
OpenAI blog, 1(8):9, 2019.

[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and
Alexandra Birch. Neural machine translation of rare words
with subword units. In Association for Computational Lin-
guistics, pages 1715‚Äì1725, 2016.

[Sun et al., 2019] Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei
Xiong, Ge Li, and Lu Zhang. A grammar-based structural
In Association for the
cnn decoder for code generation.
Advancement of ArtiÔ¨Åcial Intelligence, number 01, pages
7055‚Äì7062, 2019.

[Wang and Komatsuzaki, 2021] Ben Wang and Aran Ko-
matsuzaki. GPT-J-6B: A 6 Billion Parameter Autore-
gressive Language Model. https://github.com/kingoÔ¨Çolz/
mesh-transformer-jax, May 2021.

[Wang et al., 2013] Jue Wang, Yingnong Dang, Hongyu
Zhang, Kai Chen, Tao Xie, and Dongmei Zhang. Min-
ing succinct and high-coverage API usage patterns from

source code. In Mining Software Repositories, pages 319‚Äì
328, 2013.

[Wang et al., 2021] Yue Wang, Weishi Wang, ShaÔ¨Åq Joty,
and Steven CH Hoi. CodeT5:
IdentiÔ¨Åer-aware uniÔ¨Åed
pre-trained encoder-decoder models for code understand-
ing and generation. In Empirical Methods in Natural Lan-
guage Processing, pages 8696‚Äì8708, 2021.

[Wolf et al., 2019] Thomas Wolf, Lysandre Debut, Victor
Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R¬¥emi Louf, Morgan
Funtowicz, et al. Huggingface‚Äôs transformers: State-
arXiv preprint
of-the-art natural language processing.
arXiv:1910.03771, 2019.

[Xu et al., 2022] Frank F Xu, Uri Alon, Graham Neubig, and
Vincent Josua Hellendoorn. A systematic evaluation of
large language models of code. In Deep Learning for Code
Workshop, 2022.

[Zhong et al., 2009] Hao Zhong, Tao Xie, Lu Zhang, Jian
Pei, and Hong Mei. MAPO: Mining and recommending
API usage patterns. In European Conference on Object-
Oriented Programming, pages 318‚Äì343. Springer, 2009.

A PYCODEGPT: A Democratizing Code

Generation Model in Python

Large pre-trained language models, e.g., Codex [Chen et
al., 2021a] and AlphaCode [Li et al., 2022], have recently
achieved surprisingly promising results on modeling source
code in several programming languages. However, most of
the state-of-the-art models are not publicly available, hinder-
ing the progress of related research topics and applications.
To this end, we propose a publicly available code pre-trained
model for Python, named PYCODEGPT, to reproduce Codex
with medium size.

A.1 Data Construction
It is well-known that large language models require extremely
large amounts of data to exhibit its performance well. In this
section, we present the process of data collection and the data
pre-processing strategies to ensure data quality.
Data Collection We Ô¨Årst crawl 7.6M repository pages
hosted on GitHub. Then we consider the language distribu-
tion tags in each page to Ô¨Ålter the repositories without Python
Ô¨Åles. As a result, we obtain 1.2M Python-related reposi-
tory URLs. With the Ô¨Åltered repository URLs, we download
all the contents of each repository from GitHub. Following
Codex, we remove Ô¨Åles over 1MB, as experienced develop-
ers usually avoid creating large source code Ô¨Åles to maintain
good readability. As a result, we get 60.6M raw Python Ô¨Åles
under 1MB, with a total size of 330GB. Among these Ô¨Åles, we
further Ô¨Ålter out duplicated Ô¨Åles, which has been recognized
as an important step by CodeParrot. Finally, the number of
unique Ô¨Åles is reduced to 13.0M, with a total size of 96GB.
Data Pre-processing The data pre-processing strategies
are summarized in three aspects.

‚Ä¢ According to the strategies applied to Codex and Code-
Parrot, we consider each source code as text and focus

Hyper-parameter

Learning Rate
Optimizer
Adam Œ≤
Adam (cid:15)
Weight Decay
Warmup Steps
Learning Rate Decay
Batch Size (tokens)
Training Steps
Context Window

Value
5 √ó 10‚àí4
AdamW
0.9, 0.95
10‚àí8
0.1
100K
Cosine
480K
200K steps, 100B tokens
1024

Table 5: Hyper-parameters in pre-training.

on line length limit and alphanumeric rate9. In detail,
we Ô¨Ålter the Ô¨Åles which do not meet the four conditions:
lines of code ‚â• 5, average line length ‚â§ 100, maximum
line length ‚â§ 1000, and alphanumeric rate ‚â• 0.98. Note
that, the fourth condition is applied after removing com-
ments with alphanumeric rate < 0.5, which is one of the
following strategies.

‚Ä¢ We also remove the automatically generated or mean-
.py,
ingless Ô¨Åles, for example, Ô¨Åles with the name
setup.py or pb2.py, because these Ô¨Åles can mislead the
model during training. In addition, we remove some use-
less contexts from the Python Ô¨Åles. SpeciÔ¨Åcally, we re-
move the contexts of license description and comments
with alphanumeric rate < 0.5, where license description
appears as a comment in the head of the code.

init

‚Ä¢ To ensure the training quality, we design two methods to
perform Python syntax checking. The Ô¨Årst method is to
use Python‚Äôs built-in module ast to check the correct-
ness of the syntax. This strategy Ô¨Ålter out non-Python
Ô¨Åles largely, even if the Ô¨Åle name ends with .py. The
second method applies pattern matching to leave Ô¨Åles
containing more than two typical Python keywords (e.g.,
def, if, return, and for).

A.2 Model Training
We use GPT-Neo [Black et al., 2021] as our base model,
which is comparable to GPT-3 [Brown et al., 2021] and has
already been pre-trained on the Pile dataset [Gao et al., 2020].
In this section, we present the details of model training, in-
cluding tokenization, resampling, and hyper-parameters.

Tokenization The tokenizer of GPT-Neo models is based
on GPT-2 [Radford et al., 2019] tokenizer, which applies the
byte-level version of Byte Pair Encoding (BPE) [Sennrich et
al., 2016].The tokenizer is trained on the Pile dataset with
a vocabulary of 50K. Since the distribution of source code
words differ from that of natural text, the original GPT-Neo
tokenizer is not very effective for encoding Python source
code. Thus, we follow CodeParrot to train a new byte-level
BPE tokenizer from scratch on our collected code data. Fi-
nally, we set the vocabulary size to 32K, which allows us to
encode code using approximately 40% fewer tokens.

9It makes our model only target the English version.

Model

Params

pass@k

k=1

k=10

k=100

Parameter Scale: ‚àº100M

TabNine
GPT-Neo
CodeParrot
PolyCoder
Codex
AlphaCode
PYCODEGPT (Ours)

7.59%
4.35%
2.58%
‚Äì
1.88%
2.97%
0.75%
125M
6.57% 12.78%
3.80%
110M
2.13%
4.88%
3.35%
160M
8.22% 12.81% 22.40%
85M
89M
20.0%
12.2%
4.3%
110M 8.33% 13.36% 19.13%

Parameter Scale: ‚àº500M

PolyCoder
Codex
Codex
AlphaCode
AlphaCode
CODEGEN-MONO

2.96%

400M
5.29% 11.59%
300M 13.17% 20.37% 36.27%
25.7% 40.95%
679M 16.22%
31.8%
18.8%
11.6%
302M
685M
38.8%
24.4%
14.2%
350M 12.76% 23.11% 35.19%

GPT-Neo
CodeParrot
AlphaCode

Parameter Scale: ‚àº1B
4.97%
3.58%
17.1%

1.3B
1.5B
1.1B

Parameter Scale: ‚àº2B

7.47% 16.30%
8.03% 14.96%
45.3%
28.2%

GPT-Neo
PolyCoder
Codex
CODEGEN-MONO

6.41% 11.27% 21.37%
2.7B
5.59%
9.84% 17.68%
2.7B
2.5B
21.36% 35.42% 59.50%
2.7B 23.70% 36.64% 57.01%

Parameter Scale: ‚àº6B

GPT-J
CODEGEN-MONO

6B

11.62% 15.74% 27.74%
6.1B 26.13% 42.29% 65.82%

Parameter Scale: >10B

Codex
CODEGEN-MONO

12B

28.81% 46.81% 72.31%
16.1B 29.28% 49.86% 75.00%

Table 6: Experimental results of models under different parameter
scales on the HumanEval Dataset. For AlphaCode, we report the
pre-trained decoder-only results from their paper. For TabNine, we
do know the parameter scale, so we put it to the Ô¨Årst scale group.

Resampling Since different Ô¨Åles have different importance
for training, we design a data resampling strategy to make
high-quality Ô¨Åles appear more often, while keeping a balance
to make each Ô¨Åle appear at least once throughout the training
process. In detail, we evaluate the quality of a Python Ô¨Åle
in two aspects: repository star count and unit test function
rate. The repository star count is determined by GitHub users
and is the main factor in measuring repository quality. We
do not use the star count directly for Ô¨Åltering data because
most Ô¨Åles have very few stars. The unit test function rate
is the number of unit test functions divided by the number
of functions. We introduce it to reduce the weight of Ô¨Åles
used for testing, because test Ô¨Åles often contain a lot of user-
deÔ¨Åned numeric constants or string constants.

Hyper-parameters PYCODEGPT shares the same conÔ¨Åg-
urations with original GPT-Neo 125M model except for the
vocabulary size. It is trained on 16 V100 (32GB) GPUs for
about 2 days. Table 5 lists the hyper-parameters.

A.3 Experiments

We conduct experiments on HumanEval and CodeXGLUE to
show the effectiveness of PYCODEGPT.

Model

Params

LSTM
Transformer
GPT-2
CodeGPT
CodeGPT-Adapted
CodeParrot

PYCODEGPT (Ours)

‚Äì
‚Äì
117M
124M
124M
110M

110M

Token Level

Line Level

Accuracy

EM

ES

58.00%
73.26%
74.22%
74.93%
75.11%
77.22%

17.93% 50.05%
36.65% 67.51%
38.55% 68.94%
39.11% 69.69%
39.65% 69.84%
42.10% 71.07%

80.24%

44.77% 73.09%

Table 7: Experimental results on CodeXGLUE Code Comple-
tion task dataset PY150. Results of LSTM, Transformer, GPT-2,
CodeGPT and CodeGPT-Adapted are from Lu et al. [2021]. Exact
match and edit similarity are abbreviated as EM and ES.

Results on HumanEval HumanEval [Chen et al., 2021a]
contains 164 hand-written code generation problems. As
mentioned in Section 5, the evaluation metric is pass@k. We
use the same parameters as those used by Codex, except for
the stop sequences. The stop sequences include \nclass,
\ndef, \n#, \n@, \nprint, and \nif. We generate 200
programs and apply nucleus sampling using p = 0.95. As in
previous work, we try various temperatures (from 0.1 to 1.0
with the interval of 0.1) and report the best result for each k.
Table 6 shows the experimental results. Even though the
original GPT-Neo models (125M, 1.3B and 2.7B) and GPT-J
6B [Wang and Komatsuzaki, 2021] are trained on the dataset
containing GitHub Ô¨Åles, they do not achieve satisfactory per-
formance on HumanEval compared to Codex. For the open-
source CodeParrot models (110M and 1.5B), as we men-
tioned before, they outperform the corresponding ones of
GPT-Neo but still not enough to compare with Codex. Al-
phaCode [Li et al., 2022] also evaluates their decoder-only
model on HumanEval, and the performance is slightly worse
than Codex. PolyCoder [Xu et al., 2022] is pre-trained on
several programming languages, and it shows even worse per-
formance than CodeParrot. However, our pre-trained 110M
model can obtain comparable performance to Codex 85M and
outperform CodeParrot 110M by 4.53% on pass@1, 6.79%
on pass@10 and 6.35% on pass@100. During our paper re-
view, CODEGEN [Nijkamp et al., 2022] also provided code
generation models of various sizes from 350M to 16.1B and
obtained good performance. We omit CodeT5, CodeGPT,
and CodeClippy, since they all get 0% on pass@1, pass@10,
and pass@100.
Results on CodeXGLUE CodeXGLUE [Lu et al., 2021]
is a benchmark for programming language understanding and
generation tasks. In total, CodeXGLUE provides datasets for
14 tasks, in which the dataset PY150 is for code completion
task. Table 7 shows the experimental results. Note that we
Ô¨Åne-tuned PYCODEGPT and CodeParrot on PY150 train-
ing dataset. The results show that CodeGPT is worse than
the Ô¨Åne-tuned CodeParrot. However PYCODEGPT wins the
CodeParrot with 3.02% higher score on token level comple-
tion accuracy. On line level completion, PYCODEGPT also
achieves 2.67% and 2.02% higher accuracy on exact match
and edit similarity, respectively.

