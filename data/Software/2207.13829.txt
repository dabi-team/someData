2
2
0
2

l
u
J

7
2

]
E
S
.
s
c
[

1
v
9
2
8
3
1
.
7
0
2
2
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

We Do Not Understand What It Says – Studying
Student Perceptions of Software Modelling

Shalini Chakraborty and Grischa Liebel

Abstract—Background: Despite the potential beneﬁts of software modelling, developers have shown a considerable reluctance
towards its application. There is substantial existing research studying industrial use and technical challenges of modelling. However,
there is a lack of detailed empirical work investigating how students perceive modelling. Aim: We investigate the perceptions of
students towards modelling in a university environment. Method: We conducted a multiple case study with 5 cases (5 courses from 3
universities) and two units of analysis (student and instructor). We collected data through 21 semi-structured interviews, which we
analysed using in-vivo coding and thematic analysis. Results: Students see some beneﬁts of modelling, e.g., using models for
planning and communicating within the group. However, several factors negatively inﬂuence their understanding of modelling, e.g.,
assignments with unclear expectations, irregular and insufﬁcient feedback on their models, and lack of experience with the problem
domains. Conclusions: Our ﬁndings help in understanding better why students struggle with software modelling, and might be
reluctant to adopt it later on. This could help to improve education and training in software modelling, both at university and in industry.
Speciﬁcally, we recommend that educators try to provide feedback beyond syntactical issues, and to consider using problem domains
that students are knowledgeable about.

Index Terms—Software Modelling, Case Study, UML, Education.

(cid:70)

1 INTRODUCTION

Despite the potential beneﬁts, software modelling is not
widely adopted in industry [1]. The reasons for a lack of
adoption have been studied in depth, revealing issues such
as poor quality code generation, lack of tool support, and
lack of guidance or training [2], [3], [4], [5], [6], [7].

In the educational domain, substantial work exists on
how to teach modelling in a university curriculum, e.g.,
[8], [9], [10], [11], [12], [13], [14]. Existing work typically
comes in the form of suggested course designs, e.g., [8], [15],
experience reports that discuss challenges or good practices,
e.g., [9], [10], [11], [12], or papers that report quantitative
studies of student opinions, e.g., [13], [14]. This body of
work forms a rich source of experience and inspiration for
teaching modelling.

However, there is a lack of in-depth studies that aim to
understand how students perceive modelling and why this
is the case. Experience reports commonly suffer from vari-
ous biases, as they rely on individual experiences and lack
rigorous methods of data collection and analysis. Similarly,
existing quantitative studies aim to provide a broad picture
and can, therefore, not provide detailed explanations [16].
Since students will after graduation become professionals,
it is important to understand their perceptions and compare
them to the industrial state of practice.

To address this gap, this paper aims to investigate stu-
dents’ perceptions of modelling, both in terms of beneﬁts of
and challenges with modelling. We pose the following three
research questions (RQs) to address our aim:

• RQ1: What are students’ perceptions about mod-

elling?

•

S. Chakraborty and G. Liebel are with Reykjavik University.

Manuscript received April 19, 2005; revised August 26, 2015.

• RQ2: What are the main challenges students face

while modelling?

• RQ3: How do student perceptions compare to indus-

try challenges in modelling?

To answer these questions, we conducted a qualitative
case study, interviewing a total of 21 subjects from 5 uni-
versity courses that cover modelling. We interviewed both
students and instructors, and analysed course assignments.
Our ﬁndings show that several of the perceived beneﬁts
and challenges align directly with industry, e.g., a beneﬁcial
use of models for communication and handling complexity,
or the difﬁculty of obtaining good and fast feedback on a
model. However, we note a major difference to industry,
namely the lack of technical skills and domain knowledge
in the student population, which prevents students from
connecting their models and judging their quality to any
known domain. This latter ﬁnding highlights the impor-
tance to carefully tailor modelling courses to the inexperi-
enced audience, and consider the role of modelling courses
in the curriculum.

The rest of the paper is structured as follows. In Section 2
we discuss the related work on modelling in education and
industry, and the importance of taking students’ perception
into account. In Section 3, we present the method we applied
for our case study, followed by the results and a discussion
therefore in Section 4. Finally we conclude the paper in
Section 5 with a summary and plans for future work.

2 RELATED WORK
In the following, we describe related work that investigates
the use of models in industry, and work on modelling
education. Finally, we describe work that aims to connect
industry and education.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2.1 Models in Industry

There is substantial work on the adoption and the challenges
of software modelling in industry, e.g., [4], [5], [6], [7], [17],
[18], [19], [20], [21], [22].

In an early study at Motorola, Baker et al. [22] ﬁnd that
models increase productivity and reduce defects. However,
they also ﬁnd that modelling tools are insufﬁcient and lack
interoperability, and that MBE does not scale sufﬁciently.

Mohagheghi et al. [5], [21] highlight the potential of
using models for simulation and testing, while also men-
tioning tool issues and model complexity as challenges of
adopting MBE.

Hutchinson et al. [4], [17], [18], [19] conduct several stud-
ies assessing the state of practice of MBE in industry. The
authors conﬁrm that modelling tools and model complexity
are problematic, but also highlight organisational factors.
Among others, they ﬁnd that a lack of training hinders the
adoption of modelling.

In a survey in the Italian software developing industry,
Torchiano et al. [20] ﬁnd that developers mainly use models
for informal sketches and communication. They further
report that inexperience among developers limits the use
of models.

In a survey among systems engineering professionals,
we ﬁnd that models provide substantial beneﬁts, e.g., in
terms of increases in productivity [7]. However, several
challenges in technical and non-technical areas remain, e.g.,
lack of training and guidance, tool shortcomings, and a
lack of tool interoperability. Following up with an in-depth
qualitative study at two automotive companies, we report
that models are used primarily for communication and to
handle complexity. Stakeholders further prefer sketches and
informal models [6].

2.2 Models in Education

Research on models in education exists primarily in three
forms: (i) solution proposals on how to teach modelling, (ii)
experience reports on modelling education, and (iii) surveys
among students.

An example of the ﬁrst category is the practical approach
to teaching model-driven software development proposed
by Schmidt et al. [15]. In the proposed course, students
develop a code generator using standard software develop-
ment tools. Similarly, Westphal [8] describes the design of an
SE course that focuses heavily on modelling. In both cases,
evaluation of the proposed course design relies on student
feedback.

Numerous experience reports exist related to modelling
education, e.g., [10], [11], [23]. Akayama et al. [23] share their
experience and opinion on tool use in software modelling
education. The paper describes different approaches taken
by the individual authors and provides a discussion of
factors such as modelling tools vs pen and paper, the conﬂict
between the concepts of design and programming, and how
to measure the quality of models. Paige et al. [11] discuss
what they consider to be bad practices of teaching mod-
elling. They name bad practices such as covering a too broad
range of modelling-related topics, and focusing on syntax
instead of semantics. Similarly, Kolovos and Cabot [10]
present a corpus of use cases for courses teaching MBE. The

2

authors state that modelling courses regularly suffer from
the use of uninteresting or irrelevant examples, and there-
fore propose a list of use cases suited to teach modelling.
Ciccozzi et al. [24] present a survey among educators on
how modelling is taught. The authors ﬁnd that educators
see the focus on tools critical, as it might prevent students
from understanding the core principles of modelling.

Finally, several empirical studies exist on modelling
education, e.g., [13], [25], [26], [27], [28]. Reuter et al. [25]
study students’ problems with UML diagrams over two
modelling courses. As a result, the authors present a cat-
alogue of problems with UML diagrams. In a similar di-
rection, Stikkolorum et al. [13] study student problems and
modelling strategies in UML Class diagrams by presenting
students with a specialised UML editor that incorporates
feedback mechanisms. The results reveal four distinct strate-
gies, and a number of problems such as choosing the right
syntax elements. Agner et al. [26] conduct a survey on
modelling tool use among 117 students. The authors report
issues such as a lack of feedback, difﬁculties in drawing
the diagrams and tool complexity. Hammouda et al. [28]
compare the use of modelling CASE tools to pen and paper
use. Using a survey, they evaluate students’ perceptions
of the differences, ﬁnding no clear advantage for either
approach. Finally, we conducted several case studies on
tool use in modelling education [14], [27]. We ﬁnd that
students can use industrial modelling tools successfully, but
require substantial coaching in how to use the tools, with a
dedicated tool champion present in the course. We further
ﬁnd that the tools’ inability to provide adequate feedback
impacts the tool acceptance.

In an attempt to connect industry practice to education,
Whittle and Hutchinson [12] present essential differences
between industry practice and education concerning soft-
ware modelling. The authors ﬁnd that modelling education
is more UML-centric, whereas industry is placing more
emphasis on abstract models. Furthermore, in industry it
is more common to use a bottom-up approach to adoption,
whereas modelling is typically taught in a top-down fash-
ion.

3 RESEARCH METHOD

Our goal is to investigate students’ perceptions, especially
their challenges in learning and applying software mod-
elling in a university environment. To do so, we conducted
a multiple-case study. Runeson at al. [29] deﬁne a case study
as an “empirical investigation of a software engineering
phenomenon within its real-life context” where evidence
can be drawn from multiple sources, more importantly
from real-life experiments involving human participants
[30]. The phenomenon we study is the experience of stu-
dents learning software modelling at university. In total, we
interviewed 16 students from 3 different universities and 5
different courses. Additionally, we interviewed instructors
of the 5 courses we considered. Three courses were at
bachelor level and two were from masters level. In addition
to the interviews, we consulted assignment details for the
courses.

Whether students can or should be used as study sub-
jects is a long-debated question in SE [31], [32], [33], [34]. In

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

particular, students can be valid study subjects under certain
conditions, e.g., if novice software engineers are studied
[34]. In our case, we aim to study how students perceive
modelling, in contrast to existing studies that investigate
industrial cases. Therefore, the choice of students as study
subjects is valid.

The study setup is described in detail in the following

sub-sections.

3.1 Case Study Design

3.1.1 Pilot Study

Before starting the actual study, we conducted a pilot study
with three interviewees, two doctoral students and one
bachelor student. Of the two doctoral students, one has
prior knowledge and experience of software modelling. The
other student has no experience in software modelling.
We selected the combination because we wanted to check
how our questions would be received by different persons
with various software modelling background, as we are
dealing with students from different countries, courses and
backgrounds. Then we conducted another interview with a
bachelor student who had taken a modelling course at our
university. Our focus was to check the interview guide. Also,
the pilot study helped us to estimatee the overall interview
time.

3.1.2 Cases and Recruitment
We designed a multiple embedded case study [29] with ﬁve
cases, each case is represented by a course-university pair.
Table 1 shows the cases and the interviewees per case
(using anonymous identiﬁers). Initially, we contacted the
instructors of the courses based on our own contacts, and
once they agreed, we advertised the study to students of
that course. C1 was selected through convenience sampling,
as it is a course at our home university. It is a typical
“UML course” in the sense that basic UML diagrams are
introduced. The teacher is not anyhow involved in mod-
elling research. C2 was selected as a comparative case, as
it was a basic UML course taught by the same instructor
I2 as C1, at the same university. C3 to C5 were selected as
revelatory cases, as they exhibit one or several differences to
C1 and C2. Participation in the interviewees was voluntary
and instructors were not aware of who participated in the
interviews. We asked interested students to contact the
researchers via mail, and once students initiated the contact,
we sent them an interview meeting invitation and a consent
form. All interviews were online. To appreciate students’
participation, we offered them movie vouchers or did a
donation to charity on their behalf.

3.2 Data Collection

Based on the pilot study, we decided to conduct inter-
views for approximately 40 minutes. The student interviews
were divided into two set of questions: introductory ques-
tion and model questions. In the introductory questions,
we added questions about a student’s background, pre-
vious work experience with software models. The model
questions includes students’ personal experiences during
courses, challenges in learning modelling, completing as-
signments and overall perception of software modelling.

3

The instructor questionnaire includes details about
the
course structure, assignments, feedback from students and
the instructor’s challenges with the course. All interviews
conducted remotely through Zoom or Microsoft Teams,
and their build-in recording features. Before recording, we
asked permission from each interviewee and received their
signed consent form. Interviewees could additionally give
their consent that we would publish their anonymised in-
terview transcript. 13 interviewees consented to publishing.
The interview guides and the transcripts can be found on
Zenodo [35].

3.3 Data Analysis

The data analysis process is depicted in Figure 1. After con-
ducting the interviews (steps 1a and 1b in Figure 1), we tran-
scribed the interviews using transcription services1. In case
of automated transcriptions, we post-processed the tran-
scripts to improve their quality. The two researchers then
applied in-vivo coding separately on each interviewee tran-
script. We used in-vivo coding [36], as it helps to highlight
participants’ opinions by using the actual spoken words.
In our case, interviewees were from different countries and
cultural backgrounds, and thus used varying vocabulary
to describe their personal experiences. After conducting
the ﬁrst ﬁve interviews (C1 S1, C1 S2, C1 S3, C1 S4 and
C2 S1), we coded and then jointly discussed the resulting
codes, grouping them into initially 10 themes. These themes
related to beneﬁts and challenges of modelling, as well as
general categories refering to course feedback and concerns.
We reﬁned the themes as analysis progressed and more
interviews were added.

Once we ﬁnalised the themes based on student data,
we coded the instructor data and compared it with the
existing themes. We used the instructor interviewees for
three purposes, namely (a) to understand the course context,
(b) to better understand the student perceptions and to
check whether the instructors had the same views and (c) to
obtain ideas for potential best practices. For sorting quotes
and themes, we used the online whiteboard tool Miro4.

As a second data source for our analysis, we used course
assignments as a second data source to cross-check themes
and quotes that related to, e.g., assignments, work load, or
grading. We contacted the instructors after their respective
interviews for assignment details, which included the as-
signment structure, time, instructions, and point distribu-
tion. Three instructors (C1/I1, C3/I3 and C4/I4) responded.
We checked the assignments for the problem motivation,
the actual tasks, the required problem domain knowledge,
clarity of instructions and the total time dedicated to mod-
elling tasks. We then compared these aspects to statements
made during the interviews.

The total time of data collection and analysis was ap-
proximately one year. The resulting themes related to mod-
elling beneﬁts are listed in Table 2, with example quote for
each theme. Similarly, Table 3 lists the themes related to
modelling challenges.

1. We used Konch2 and Go Transcript3.
4. https://miro.com/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

TABLE 1
Case Summary

Code
C1
C2
C3
C4
C5

Level
Bachelor
Bachelor
Bachelor
Master
Master

Course Description
Software analysis and prototype design using user interviews
Software requirement analysis and design
Software requirement speciﬁcation and design
System speciﬁcation, design and testing
Software architecture design and quality analysis

Students
C1 (S1, S2, S3, S4, S5)
C2 (S1, S2, S3)
C3 (S1, S2)
C4 (S1, S2, S3)
C5 (S1, S2, S3)

Instructors
I1, I2
I2
I3
I4
I5

Fig. 1. Data Analysis Process. Rounded rectangles denote data sources, regular rectangles denote activities, rectangles with wavy bottom line
denote artefacts, and arrows denote information ﬂow.

TABLE 2
Final themes related to modelling beneﬁts, with a short description and a sample quote associated with each theme.

Theme
B: Same Page

B: Better Planning

Maintenance/

B: Better Understand-
ing of Code
B:
Documentation
B: Personal Prefer-
ences

Description
Beneﬁts of modelling in communication and co-
ordination within teams.
Modelling helps in planning ahead, for under-
standing the requirements, potential design alter-
natives, but also to plan implementation.
Beneﬁts of modelling to help understanding a
code base without reading the code in detail.
Beneﬁts of modelling as a way to help maintain-
ing a system and to document what it does.
Beneﬁts related to individual preferences.

B: Doubtful Beneﬁts

Doubts regarding any beneﬁts of modelling.

Example statement
I think it’s important for documentation, as well as getting
everyone on the same page
But the thing I liked this is when you feel like you have a plan
and you know what are you gonna do

I really like the idea of seeing it as a language to talk about code,
because I will never read someone’s code if they ask for feedback-
Usually, it’s other people who will maintain software, so they
need to read what you make if they need to understand.
Yeah, yeah we used that because it was um..prettier, than this
is the whole perfectionism thing with um, it was prettier than
coding
[..] ]you can’t be mastering everything and [I am] more into
implementation rather than modelling things

3.4 Validity threats

We conducted an exploratory [29] case study, where we
primarily collected data through interviews. For the anal-
ysis, we leaned towards interpretivism. An interpretivist
approach means that “humans construct knowledge as they
interpret their experiences of and in the world; rejecting the
objectivist notion that knowledge is simply there to be identiﬁed
and collected” [37], [38]. In our work, adopting to interpre-
tivism is suitable as we seek answers for our RQs through
the interviewees’ perspective, based on their knowledge of
the addressed subject and cultural background. The under-
standing of our knowledge is therefore relative to the person
and their personal experience.

Following the work of Petersen et al. [39], we present
the validity threats of our work and the measures we have
taken to mitigate them in the following.

3.4.1 Transferability

Transferability describes to what extent results from the
study can be transferred to cases that resemble the case
under study [39]. Cultural differences, the inﬂuence of
teachers, and the course subjects limit transferability in our

study. We aim to ensure a substantial level of transferability
by basing our analysis on ﬁve cases. Nevertheless, all ﬁve
cases teach modelling for analysis tasks on a high level
of abstract, i.e., for requirements, architecture and design
purposes. Speciﬁcally, none of the courses covers model
transformation or other tasks that require formal models.
Furthermore, all ﬁve courses are located in Northern Euro-
pean Universities, thus potentially limiting the applicability
to other countries.

To further allow for transferability, we conducted a sub-
stantial number of interviews (22). We reached a saturation
point in our themes after the 16 student interviews, and
hence stopped including further cases or interviewees.

3.4.2 Credibility

Credibility describes to what extent ﬁndings have been
distorted by the researchers [39].

We tried to avoid distortion of the reported ﬁndings by
grounding the analysis in in-vivo codes directly taken from
the verbatim interview transcripts. Furthermore, we per-
formed member checking to get feedback on the extracted
themes from our interviewees. Only few interviewees an-

Student interviews (n=16)Instructor interviews (n=6)Assignment textsTranscriptionOrganise into Excel sheetsIn-vivo coding (1st part)Whiteboard/oﬄine analysisIn-vivo coding (2nd part)Oﬄine and Miro analysisInitial ThemesFinal ThemesJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

TABLE 3
Final themes related to modelling challenges, with a short description and a sample quote associated with each theme.

Theme
C: Unclear Expecta-
tions

C: Irregular and Un-
clear Feedback

C: Lack of Expertise
in the Problem Do-
main

C: Time and Repeti-
tion

C: Notation

C: Tooling

C: Lack of Coopera-
tion

Description
Confusion on what the purpose of modelling is,
often as expectations are not clearly communi-
cated.
Feedback is provided too seldom, is not clear, or
is restricted to formalities, such as the diagram
syntax.
Students are provided with assignments in a do-
main they are unfamiliar with. Therefore, they
struggle to relate their learning to something
known.

Modelling consumes time, and requires repetition
to master. These two factors are often in conﬂict
in university courses, and students struggle to see
the value of repeatedly improving their models..
Struggles with the complexity of the UML nota-
tion.
Modelling tools can cause numerous difﬁculties,
such as poor usability.
Challenges due to difﬁculties in collaborating in
teams, and due to lack of professionalism in stu-
dents’ attitudes.

Example statement
At the end it’s like ’Oops, maybe I should have designed’, but
then I wouldn’t know how to design it.

Writing for hours and then a teacher be like no this is wrong

we didn’t have much experience in programming and designing
and modeling the app, I think the biggest challenge was in the
ﬁrst week that we were just ﬂowing with our ideas how the app
should look like without actually knowing how the end picture
of the class diagrams should look like
Later on, we had to ﬁx it at least 10 times. After writing the
app, we had to go back to our diagrams and redo them how our
ﬁnal vision was

One thing that comes up is with the state diagram and activity
diagram, which one is supposed to do what
I hate PlantUML. I can’t read the diagram that comes out of it.
It’s all over the place
[..] one or two would make the model and the rest maybe don’t
understand. Somehow, I don’t know how to solve this problem.

swered this call, but those conﬁrmed the credibility of the
found themes.

All interviews were recorded, and data analysis per-
formed on the verbatim transcripts. Additionally, we pub-
lish the anonymised transcripts of the 13 interviewees who
consented to this. This should ensure credibility of the
ﬁndings.

3.4.3 Conﬁrmability

Conﬁrmability describes the extent to which conclusions
made by researchers follow from the observed data [39].

To allow for conﬁrmability, we presented the way of
coding in depth. Furthermore, we give example quotes for
each theme, and publish anonymised student transcripts.
Note that this does not necessarily ensure reliability, i.e.,
that analyses conducted by other researchers would yield
precisly the same results. While we did try to reach a level
of consensus among the two researchers in our analysis, we
nevertheless follow an interpretivist approach to research,
in which the subjective impressions of the researchers play
a major role.

Fig. 2. Beneﬁts as supported by the student interviewees. Numbers over
the bars represent the total amount of students mentioning the beneﬁt,
and the number of cases in which students mentioned the beneﬁt in
parenthesis.

4 RESULTS AND DISCUSSION
In this section we report and discuss the ﬁndings of our case
study.

4.1 Student Perception of Software Modelling (RQ1)

Based on the interview data, we observe that students see
speciﬁc beneﬁts of modelling, primarily for areas where
informal models might be sufﬁcient, such as obtaining a
system overview or conceptual understanding of the prob-
lem domain. However, many are skeptical as to what value
modelling has for detailed system design. In the following,
we discuss the students’ perceptions and provide adequate
interview quotes that support the categories. An overview
of how many students mentioned the perceived beneﬁts is
depicted in Figure 2.

4.1.1 Being on the Same Page
Several students ﬁnd that modelling helps their groups
communicate better with each other. Students mention mod-
elling helps them to express their ideas, share work and
make decisions as a group. For example, students stated:

“I think it’s important for documentation, as well as getting
everyone on the same page” — C2 S1
“It’s a lot easier to work together when you have the diagrams.”
— C1 S5
“It helps every group member to know what exactly they want”
— C3 S2

4.1.2 Better Planning
Students ﬁnd modelling helpful in planning the develop-
ment, i.e., designing systems. Students appreciate that they
can visually map the system and plan the development
through modelling.

6 (3)9 (4)6 (3)2 (2)7 (5)6 (4)Modelling Benefits According to Students (n=16)Being on the same pageBetter planningBetter understanding of codeMaintenance/DocumentationPersonal PreferencesDoubtful benefitsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

“But the thing I liked this is when you feel like you have a plan
and you know what are you gonna do” — C1 S2
“Class diagrams are really, really helpful. Before starting your
project...because you can see and have the view of your classes”
— C5 S3

“It’s the same experience that I have when people talk about this
course, they’re asking, ’Is anyone using it?”’ — I2
We further discuss the challenges with modelling in the

next section.

4.1.3 Better Understanding of Code

In relation to programming, some students found models
helpful to get an overview of the code on a higher level of
abstraction. That is, they stated:

“I really like the idea of seeing it as a language to talk about code,
because I will never read someone’s code if they ask for feedback”
— C3 S1
“If you have a diagram for it, we would get a better understanding
of the code itself.” — C2 S3
An aspect of this is a top-down modelling approach,
where a better understanding of the system is obtained
through breaking down the system in steps, as noted by
some of our interviewees.

“If you are making a program that has more than 200 lines of code
then you probably gonna have to model it.” — C1 S1
“that was a very nice experience of seeing how you start with a
kind of vague idea and then start to break it down more concrete
classes” — C3 S1

4.1.4 Maintenance/Documentation

Students ﬁnd modelling to be helpful to prepare for future
tasks, such as maintaining and documenting the product.
However, in our interview data only students with industry
background expressed this beneﬁt.

“Usually, it’s other people who will maintain software, so they need
to read what you make if they need to understand.” — C1 S3
“Unless you don’t have the documentation, you will end up with
just a total mess” — C4 S2

4.1.5 Personal Preferences

In addition to the beneﬁts stated above, some students use
models in terms of personal preference. For example:

“Yeah, yeah we used that because it was um..prettier, than this
is the whole perfectionism thing with um, it was prettier than
coding” — C1 S2
“It helps me basically get my idea out there a lot better. Basically,
when I’m starting a project, I like modeling the higher level and
just going deeper and deeper and deeper.” — C2 S1

4.1.6 Doubtful Beneﬁts

Despite experiencing beneﬁts, several students are doubtful
whether the beneﬁts of modelling outweigh the issues, and
if they will apply models in the future. We received several
statements of students saying they will most likely not apply
models in the future.

“I think it’s a great experience that we can have this now. Not in
the future, in our jobs” — C3 S2
“[..] ]you can’t be mastering everything and [I am] more into
implementation rather than modelling things” — C4 S2

Interestingly, both statements above were made by students
with industrial software development experience. The same
students however admitted that in their experience they
were tasked with implementation only, and not with high-
level tasks such as system design or requirements engineer-
ing.

One of the instructors conﬁrmed that many of their
students would question the application of modelling in
industry:

4.2 Modelling Challenges (RQ2)

Despite beneﬁts observed by the majority of the students,
they experience several challenges related to modelling.
These relate, among others, to tooling, how to choose the
right notation, what to express in the models, and how to
apply it to unfamiliar domains. We will discuss these chal-
lenges in the following, focusing on the student lens, and
complementing their views with the instructor perspective.
Overall, we extracted 8 types of challenges. The support
by the different interviewees is depicted in Figure 3 and
Figure 4.

Fig. 3. Student challenges as supported by the interviewees. Numbers
over the bars represent the total amount of students mentioning the
challenge, and the number of cases in which students mentioned the
challenge in parenthesis.

Fig. 4. Student challenges as supported by the instructor interviewees.
Numbers over the bars represent the total amount of instructors men-
tioning the challenge, and the number of cases in which instructors
mentioned the challenge in parenthesis.

11 (5)8 (4)2 (2)12 (5)7 (4)2 (2)6 (4)Modelling Challenges According to Students (n=16)Unclear ExpectationIrregular FeedbackLack of Expertise in Problem DomainTiming and RepetitionNotationToolingLack of Cooperation3 (3)3 (3)1 (1)0 (0)3 (3)2 (2)0 (0)Modelling Challenges According to Instructors (n=5)Unclear ExpectationIrregular FeedbackLack of Expertise in Problem DomainTiming and RepetitionNotationToolingLack of CooperationJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4.2.1 Unclear Expectations

In university modelling courses, students typically receive
assignments to create models. However, they often struggle
to understand what is expected of them, and how to create
the models.

“We started coding and at the end of the day, we didn’t know what
we did” — C4 S3
In particular, this is caused by a lack of knowledge
in programming, software architecture, and other practical
skills necessary to envision a “good” design:

“At the end it’s like ’Oops, maybe I should have designed’, but
then I wouldn’t know how to design it. ” — C3 S1
Additionally, assignments are often formulated in a way
that is in contrast to how they are assessed, e.g., by ask-
ing students to create a prescriptive model, which is then
assessed by how closely it resembles the ﬁnal code, i.e., in
a descriptive way. Students quickly pick up this discrepancy
in grading and optimise their efforts towards the grading.
That is, they take shortcuts initially, and simply update the
model later to reﬂect their actual code.

“we had to, we had to basically just create a class diagram out of
the code that they wrote, instead of writing a code out of the class
diagram” — C1 S1

4.2.2 Irregular and Unclear Feedback

Students are dissatisﬁed with the type of feedback they
receive. As modelling is often a qualitative task, teachers
often revert to giving feedback on objective things such as
the diagram syntax. However, this is not perceived as useful
feedback.

“So no one can say anything to us. It’s good or not. Just we should
follow some rules about the diagram, for example, what is solid
line, what is dashed line.” — C5 S1
Instead, students would like more “holding hands” with

regular feedback.

“Maybe because it’s the ﬁrst year, this is the ﬁrst introduction to
the subject, so people miss a lot of points, but if it had another level,
teach people more, and take their hand more to do these diagrams
themselves, that would be good” — C1 S3
In particular, since there is no automated feedback as,
e.g., in programming, students require timely and regular
feedback.

“writing for hours and then a teacher be like no this is wrong” —
C1 S1
“Reply to emails more than ﬁve or six hours” — C4 S1
The regularity and quality of feedback was further com-

plicated by Covid-19, as all 5 cases used remote teaching.

4.2.3 Lack of Expertise in the Problem Domain

Deep knowledge of the problem domain is necessary to
perform many software engineering tasks in a satisfactory
manner, e.g., programming [40] or program understand-
ing [41]. If this domain knowledge is lacking or entirely
missing, software engineering tasks cannot be performed
well. Allowing students to work with a known problem
increases their interest and participation. Several students
stated that, in addition to just learning modelling, they were
also lacking domain knowledge and programming experi-
ence. Therefore, they were unable to relate their models to
familiar concepts.

“we didn’t have much experience in programming and designing
and modeling the app, I think the biggest challenge was in the ﬁrst
week that we were just ﬂowing with our ideas how the app should

7

look like without actually knowing how the end picture of the class
diagrams should look like” — C1 S4
Compared to other introductory topics, such as pro-
this makes modelling particularly complex:
gramming,
Students are expected to learn a new concept
(mod-
elling), neither having problem domain nor software de-
sign/architecture experience. In addition, models do not
provide any kind of automated feedback, such as com-
piler/runtime error messages in programming. This leads
to feelings of “just drawing something” in students, without
an anchor to connect their models to.

The choice of the problem domain and the task plays a
major role here. For instance, instructors often use unrealistic
domains [10] or let students decide which system to design.
Both the approach have their advantage/disadvantages.
While unrealistic domains are just that, unrealistic, they
can also expose students to a known domain, or at least
a low level of complexity due to the unrealistic nature of
the domain. However, it suffers from a lack of real-life
relevance. This lack of relevance was also noted by one
student.

“we got to know all the basics and the whole idea and how it’s
supposed to look like, but not how to use it in the future on the
bigger picture and the scale of the app” — C1 S4

When selecting a problem to work with, students can choose
a domain they know. However, they run the risk of selecting
a domain which is particularly unsuited given the course
assignments. Furthermore, experienced students can domi-
nate the work in cases of group work.

4.2.4 Time and Repetition

Students complain that modelling takes time, and courses
are tightly scheduled. Students feel pressure due to the bulk
of assignments, especially when they are not fully aware of
what is expected from them. Given that pressure, they try
to prioritise and minimise effort where possible. However,
arguably, learning how to model will require the students
to make many mistakes and correct them in iterations. To-
gether with the lack of feedback mentioned above, students
perceive these iterative improvements as a waste of time.

“You run it once and then probably change. It’s just a waste” —
C3 S1
“we designed something, it didn’t work, so we needed to change it
” — C2 S1
“Later on, we had to ﬁx it at least 10 times. After writing the app,
we had to go back to our diagrams and redo them how our ﬁnal
vision was” — C1 S4
Interestingly, these students did not perceive that itera-
tive changes helped them understand the problem domain
better, but rather saw it as a burden with unclear beneﬁt.

4.2.5 Notation

Keng et al. [42] reported different learning challenges with
UML over a decade ago. In our cases, we observe similar
challenges. Sudents ﬁnd it hard to remember the syntax of
different diagrams and their purpose, pointing to a lack of
prior knowledge of UML.

“many different types of diagrams to represent the same thing” —
C1 S1
“The hardest part about drawing UML diagrams is always remem-
bering what exactly the syntax is” — C2 S1
Speciﬁcally, our interviewees are most often pointing out

UML state machine and activity diagrams as confusing.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

“I have this image in my head where the state diagram is not the
UI actions. I can’t wrap my head around it.” — C2 S3

“One thing that comes up is with the state diagram and activity
diagram, which one is supposed to do what. ” — C1 S5

The instructors conﬁrm this view.

“here’s a bit of struggle in between the concepts that they express
in a diagram and the particular diagram types” — I3

“they’ve been mixing those pretty well up [state and activity
diagrams]” — I2

One instructor offered the explanation that this relates
to a lack of experience with object-oriented programming in
general.

“they don’t have a lot of experience using object-oriented program-
ming.” — I2

This quote reinforces our discussion above, that students are
exposed to various unknown activities when learning how
to model, and therefore lack knowledge they can anchor
their modelling experiences to.

4.2.6 Tooling

None of the 5 cases we investigated had any requirements
for using a particular modelling tool. Correspondingly, stu-
dents did not voice any tool challenges. Students used
multiple tools for their assignments, e.g., Draw.io, Plan-
tUML, and Lucidchart. Also, some students used plain pen
and paper for drawing and communicating their diagrams.
Students appreciate this choice.

“I think actually giving people the freedom to do what they want
with the tools is really nice, because some people actually liked
drawing it on an iPad or something” — C2 S1

In particular, some students also showed initiative at

choosing different tools depending on the situation.

“When I did the prototype of my app, I used the Figma app on the
internet. It’s really good to visualize the prototype. Then we used
the Lucidchart and draw.io, I think it’s diagrams.net now. This is
for the diagrams. Then we just used the whiteboard to draw it if we
needed to because we just gathered together and we’d just throw it
on the whiteboard and then we just translated it into diagrams or
Lucidchart” — C1 S4

While offering choice in tooling clearly addresses many
tool issues, it has the limitation that it only works if the
course and the coure assignments do not rely on speciﬁc
tool features, such as code generation or simulation capa-
bilities. This is a limitation in our study, as all our cases
introduced UML only as a means to document, plan, and to
communicate, without any automated processing of models
in the form of model transformation or other facilities.

4.2.7 Lack of Cooperation

In all 5 cases, students had to cooperate in teams. In fact,
modelling was in all cases also intended as a way to facilitate
communication in the group. However, in practice they
struggled to cooperate and to appreciate the use of models
for communication purposes.

“We are six, for example, but one or two would make the model
and the rest maybe don’t understand. Somehow, I don’t know how
to solve this problem.” — C1 S3

“Some people don’t like to draw., but we’re supposed to work
together.” — C5 S1

4.2.8 Summary

Overall, we observe that students struggle with various
aspects of models (as depicted in Figures 3 and 4). Apart
from classic issues such as learning an unknown notation
(be it modelling or not), we ﬁnd that students lack a con-
nection to previous knowledge. That is, due to a lack of
domain, design and programming knowledge, and a lack
of automated feedback from modelling tools, they cannot
create an anchor. Essentially, they produce a model that they
cannot judge on any other level than notation. One student
summarised this sentiment as follows:

“The diagram is supposed to speak for yourself. But we didn’t
understand what it was saying” — RU G S1

4.3 Comparison with Industry (RQ3)

While this study focused purely on the educational context,
it is interesting to discuss potential similarities and differ-
ences to industrial practice.

First, we observe that models are appreciated for com-
munication purposes and to handle system complexity in
our cases, something that is also reported in industry [1],
[7]. Other beneﬁts reported in industry, such as simulation
or veriﬁcation capabilities do not apply in our cases, since
all ﬁve courses used modelling only on an informal level
to express models for planning, communication, and coor-
dination. Similarly, it is hard to reason about improvements
reported from industry, e.g., in terms of productivity [21],
[22].

Tooling issues are a frequent topic in industry, e.g.,
reported in [3], [3], [4], [6], [7], [17], [18]. In contrast, none of
our interviewees raised tool issues. One explanation for this
observation is clearly that none of our courses mandated a
CASE tool for modelling, but instead left that choice to the
students. Similarly, while models were often assessed for
semantic correctness, the speciﬁed purposes of the models
did not require syntactically or semantically correct models.
Together with a lack of requirements for interoperability
between tools, this removes most of the issues contemporary
modelling CASE tools have. Nevertheless, it is positive to
observe that our interviewees did not ﬁnish their courses
on modelling with the perception that modelling tools are
bad, as is commonly reported in literature on modelling
education, e.g., in [14], [23]. Such a negative perception
could lead to a reduced uptake of modelling in industry.

Our interviewees raise several challenges that relate to
a lack of guidance, feedback, and clarity when it comes to
modelling. While they primarily perceive this as an issue
in how assignments are set up and how the courses are
designed, the challenges resonate with those in industry.
For instance, a lack of training and guidance is raised in
several empirical studies on modelling in industry, e.g., [1],
[7], [20]. Whittle et al. [3] highlight that organisational and
process factors play a major role in the use of MDE. Similar
issues are raised by our interviewees in the educational
context, namely that modelling assignments need suitable
processes that allow for iterations and quick feedback loops.
This is especially important as models were not used for
automated tasks in any of our cases, i.e., there was no
automatic feedback generated.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

An important difference of the educational setting to
industry is that students often lack both the technical back-
ground, e.g.,
in terms of programming experience, and
the domain knowledge required for the example domains.
Therefore, they struggle to contextualise the value and the
quality of their models, often leading to a perception of
doing a meaningless drawing task. Publications on mod-
elling education sometimes argue for the use of realistic
examples and caution against toy examples, e.g., [10]. Given
our ﬁndings in this study, we would advise to use primarily
example domains the students have sufﬁcient knowledge
of. However, the choice of problem domain is tricky, as
a too simplistic domain might make the use of modelling
superﬂuous.

Finally, we observe that several beneﬁts and challenges
voiced by our interviewees are directly reﬂected in indus-
trial surveys on the use of models, e.g., the usefulness of
models for communication [1], [7], [20], or the sentiment
that models are not worth the effort [1], [20]. While we are
not able to answer this based on our study, we would like
to highlight the possibility that these opinions are shaped
during university education and then maintained later on.
That is, addressing these issues while educating modellers
could lead to a higher uptake and appreciation of modelling
in industry.

5 CONCLUSION

We conducted a case study investigating the perceptions
students have of software modelling. To do so, we con-
ducted 21 interviews with students and instructors in ﬁve
courses at 3 universities, and consulted assignment descrip-
tions of theses courses. The results offer rich insights into
how students perceive modelling. while related work on
the topic exists, it is usually in the form of survey studies,
opinion and experience papers, and industrial case studies.
Several ﬁndings from related work are conﬁrmed in
our study, e.g., that while seeing the beneﬁt of modelling
for planning or communication, many students perceive
modelling as not worth the effort or as an outright waste of
time. They struggle with the fast pace of courses and a lack
of repetition and in-depth feedback that goes beyond the
diagram syntax. However, we additionally ﬁnd differences
to existing literature, or deeper explanations of phenomena
observed in related work. For instance, our interviewees
reported no tooling-related challenges, potentially as they
were allowed to choose their tool of choice in all cases, and
as none of the ﬁve courses used any model-based techniques
that would require a speciﬁc CASE tool. Finally, we ﬁnd
that students struggle to understand modelling, as they are
often at the same time lacking knowledge in areas related to
the course projects, i.e., the problem domain, the intended
task (such as design or architecture), object orientation, and
programming skills. As there is often no automatic feedback
from modelling tools, this means students cannot anchor
their models to any known domain.

Our study is of interest to university researchers, educa-
tors and professionals. Speciﬁcally for educators, it conﬁrms
many existing beliefs, but also highlights new details that
can be considered when teaching modelling. Our study
can therefore help explain the students’ perceptions of

9

modelling and trigger changes in modelling curricula. For
modelling researchers, it can open up new directions on
how to improve guidance and training in modelling, some-
thing that also professionals regularly report as challenging.
Finally, professionals can use our ﬁndings to better un-
derstand what experiences university graduates have with
modelling, which beneﬁts they perceive, and why this is the
case.

ACKNOWLEDGEMENT

We would like to thank the interviewees for participating in
the study.

REFERENCES

[1] T. Gorschek, E. Tempero, and L. Angelis, “On the use of software
design models in software development practice: An empirical
investigation,” Journal of Systems and Software, vol. 95, pp. 176–193,
2014.

[3]

[2] A. Forward, O. Badreddin, and T. C. Lethbridge, “Perceptions
of software modeling: a survey of software practitioners,” in 5th
workshop from code centric to model centric: evaluating the effectiveness
of MDD (C2M: EEMDD), 2010.
J. Whittle, J. Hutchinson, M. Rounceﬁeld, H. Burden, and R. Hel-
dal, “Industrial adoption of model-driven engineering: Are the
tools really the problem?” in International Conference on Model
Driven Engineering Languages and Systems.
Springer, 2013, pp.
1–17.
J. Whittle, J. Hutchinson, and M. Rounceﬁeld, “The state of prac-
tice in model-driven engineering,” IEEE Software, vol. 31, no. 3, pp.
79–85, 2014.

[4]

[5] P. Mohagheghi, W. Gilani, A. Stefanescu, M. A. Fernandez,
B. Nordmoen, and M. Fritzsche, “Where does model-driven en-
gineering help? experiences from three industrial cases,” Software
and Systems Modeling, vol. 12, no. 3, pp. 619–639, 2013.

[6] G. Liebel, M. Tichy, and E. Knauss, “Use, potential, and showstop-
pers of models in automotive requirements engineering,” Software
and Systems Modeling, 2018.

[7] G. Liebel, N. Marko, M. Tichy, A. Leitner, and J. Hansson, “Model-
based engineering in the embedded systems domain: an industrial
survey on the state-of-practice,” Software and Systems Modeling,
vol. 17, no. 1, pp. 91–113, 2018.

[8] B. Westphal, “Teaching software modelling in an undergraduate
introduction to software engineering,” in 2019 ACM/IEEE 22nd
International Conference on Model Driven Engineering Languages and
Systems Companion (MODELS-C), 2019, pp. 690–699.

[9] L. Burgue ˜no, A. Vallecillo, and M. Gogolla, “Teaching uml and ocl
models and their validation to software engineering students: an
experience report,” Computer Science Education, pp. 1–19, 04 2018.
[10] D. S. Kolovos and J. Cabot, “Towards a corpus of use-cases
for model-driven engineering courses.” in EduSymp/OSS4MDE@
MoDELS, 2016, pp. 14–18.

[11] R. F. Paige, F. A. Polack, D. S. Kolovos, L. M. Rose, N. D. Ma-
tragkas, and J. R. Williams, “Bad modelling teaching practices.” in
EduSymp@ MoDELS, 2014, pp. 1–12.

[12] J. Whittle and J. Hutchinson, “Mismatches between industry
practice and teaching of model-driven software development,” in
International Conference on Model Driven Engineering Languages and
Systems. Springer, 2011, pp. 40–47.

[13] D. R. Stikkolorum, T. Ho-Quang, and M. R. Chaudron, “Revealing
students’ uml class diagram modelling strategies with webuml
and logviz,” in 2015 41st Euromicro Conference on Software Engineer-
ing and Advanced Applications, 2015, pp. 275–279.

[14] G. Liebel, R. Heldal, and J.-P. Stegh ¨ofer, “Impact of the use of
industrial modelling tools on modelling education,” in 2016 IEEE
29th International Conference on Software Engineering Education and
Training (CSEET), 2016, pp. 18–27.

[15] A. Schmidt, D. Kimmig, K. Bittner, and M. Dickerhof, “Teaching
model-driven software development: Revealing the” great mira-
cle” of code generation to students,” in Proceedings of the Sixteenth
Australasian Computing Education Conference-Volume 148, 2014, pp.
97–104.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

[36] J. Salda ˜na, The coding manual for qualitative researchers.

SAGE

Publications, 2015.

[37] J. Hiller, Epistemological foundations of objectivist and interpretivist

research. Barcelona Publishers, 2016.

[38] C.-M. Pascale, Cartographies of knowledge: Exploring qualitative epis-

temologies. Sage Publications, 2010.

[39] K. Petersen and C. Gencel, “Worldviews, research methods, and
their relationship to validity in empirical software engineering re-
search,” in 2013 joint conference of the 23rd international workshop on
software measurement and the 8th international conference on software
process and product measurement.

IEEE, 2013, pp. 81–89.

[40] K. Oliveira, A. Regina, C. Rocha, G. Travassos, and C. Menezes,
“Using domain-knowledge in software development environ-
ments,” CiteSeerX, Tech. Rep., 05 2000. [Online]. Available: https:
//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.2007
[41] S. Rugaber, “The use of domain knowledge in program under-
standing,” Annals of Software Engineering, vol. 9, no. 1, pp. 143–192,
2000.

[42] K. Siau and P.-P. Loo, “Identifying difﬁculties in learning uml,”
Information Systems Management, vol. 23, no. 3, pp. 43–51, 2006.

[16] K.-J. Stol and B. Fitzgerald, “The abc of software engineering re-
search,” ACM Transactions on Software Engineering and Methodology
(TOSEM), vol. 27, no. 3, pp. 1–51, 2018.

[17] J. Hutchinson, J. Whittle, M. Rounceﬁeld, and S. Kristoffersen,
“Empirical assessment of mde in industry,” in 2011 33rd Interna-
tional Conference on Software Engineering (ICSE), 2011, pp. 471–480.
[18] J. Hutchinson, M. Rounceﬁeld, and J. Whittle, “Model-driven engi-
neering practices in industry,” in 2011 33rd International Conference
on Software Engineering (ICSE), 2011, pp. 633–642.

[19] J. Hutchinson, J. Whittle, and M. Rounceﬁeld, “Model-driven
engineering practices in industry: Social, organizational and man-
agerial factors that lead to success or failure,” Science of Computer
Programming, vol. 89, pp. 144–161, 2014, special issue on Success
Stories in Model Driven Engineering.

[20] M. Torchiano, F. Tomassetti, F. Ricca, A. Tiso, and G. Reggio,
“Relevance, beneﬁts, and problems of software modelling and
model driven techniques—a survey in the italian industry,” Journal
of Systems and Software, vol. 86, no. 8, pp. 2110–2126, 2013.

[21] P. Mohagheghi and V. Dehlen, “Where is the proof? - a review of
experiences from applying mde in industry,” in Model Driven Ar-
chitecture - Foundations and Applications, ser. Lecture Notes in Com-
puter Science, I. Schieferdecker and A. Hartman, Eds.
Springer
Berlin Heidelberg, 2008, vol. 5095, pp. 432–443.

[22] P. Baker, S. Loh, and F. Weil, “Model-driven engineering in a
large industrial context—motorola case study,” in International
Conference on Model Driven Engineering Languages and Systems.
Springer, 2005, pp. 476–491.

[23] S. Akayama, B. Demuth, T. C. Lethbridge, M. Scholz, P. Stevens,
and D. R. Stikkolorum, “Tool use in software modelling educa-
tion.” in EduSymp@ MoDELS, 2013.

[24] F. Ciccozzi, G. Taentzer, A. Vallecillo, M. Wimmer, M. Famelis,
L. Lambers, S. Mosser, R. Paige, A. Pierantonio, A. Rensink,
and R. Salay, “How do we teach modelling and model-driven
engineering? a survey,” in Proceedings of the 21st ACM/IEEE inter-
national conference on model driven engineering languages and systems:
Companion proceedings, 2018, pp. 122–129.

[25] R. Reuter, T. Stark, Y. Sedelmaier, D. Landes, J. Mottok, and
C. Wolff, “Insights in students’ problems during uml modeling,”
in 2020 IEEE Global Engineering Education Conference (EDUCON),
2020, pp. 592–600.

[26] L. T. Agner, T. C. Lethbridge, and I. W. Soares, “Student experience
with software modeling tools,” Software and Systems Modeling,
vol. 18, no. 5, p. 3025–3047, oct 2019.

[27] G. Liebel, O. Badreddin, and R. Heldal, “Model driven software
engineering in education: A multi-case study on perception of
tools and uml,” in 2017 IEEE 30th Conference on Software Engineer-
ing Education and Training (CSEE T), 2017, pp. 124–133.

[28] I. Hammouda, H. Burden, R. Heldal, and M. R. Chaudron, “Case
tools versus pencil and paper,” in ACM/IEEE 17th Int. Conf. on
Model Driven Engineering Languages and Systems–Educators Sympo-
sium, 2014.

[29] P. Runeson, M. H ¨ost, A. Rainer, and B. Regnell, Case Study Research
John Wiley &

in Software Engineering – Guidelines and Examples.
Sons Inc., 02 2012.

[30] C. Wohlin, “Case study research in software engineering—it is a
case, and it is a study, but is it a case study?” Information and
Software Technology, vol. 133, p. 106514, 2021.

[31] P. Runeson, “Using students as experiment subjects - an analysis
on graduate and freshmen student data,” Proceedings of the 7th
International Conference on Empirical Assessment in Software Engi-
neering, 01 2003.

[32] I. Salman, A. T. Mısırlı, and N. Juristo, “Are students represen-
tatives of professionals in software engineering experiments?”
in 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 1, 2015, pp. 666–676.

[33] D. I. Sj ¨oberg, B. Anda, E. Arisholm, T. Dyba, M. Jorgensen,
A. Karahasanovic, E. F. Koren, and M. Vok´ac, “Conducting realistic
experiments in software engineering,” in Proceedings international
symposium on empirical software engineering.
IEEE, 2002, pp. 17–26.
[34] D. Falessi, N. Juristo, C. Wohlin, B. Turhan, J. M ¨unch, A. Jedl-
itschka, and M. Oivo, “Empirical software engineering experts on
the use of students and professionals in experiments,” Empirical
Software Engineering, vol. 23, 02 2018.

[35] S. Chakraborty and G. Liebel, “Dataset: We Do Not Understand
What
It Says – Studying Student Perceptions of Software
Modelling,” Jul. 2022. [Online]. Available: https://doi.org/10.
5281/zenodo.6913780

