2
2
0
2

r
p
A
7

]

V
C
.
s
c
[

1
v
8
3
7
3
0
.
4
0
2
2
:
v
i
X
r
a

BankNote-Net: Open dataset for assistive universal currency recognition

FELIPE OVIEDO, Microsoft AI for Good Research Lab, USA
SRINIVAS VINNAKOTA, Microsoft, USA
EUGENE SELEZNEV, Microsoft Seeing AI, USA
HEMANT MALHOTRA, Microsoft, USA
SAQIB SHAIKH, Microsoft Seeing AI, USA
JUAN LAVISTA FERRES, Microsoft AI for Good Research Lab, USA

Millions of people around the world have low or no vision. Assistive software applications have been developed for a variety of

day-to-day tasks, including optical character recognition, scene identification, person recognition, and currency recognition. This

last task, the recognition of banknotes from different denominations, has been addressed by the use of computer vision models for

image recognition. However, the datasets and models available for this task are limited, both in terms of dataset size and in variety

of currencies covered. In this work, we collect a total of 24,826 images of banknotes in variety of assistive settings, spanning 17

currencies and 112 denominations. Using supervised contrastive learning, we develop a machine learning model for universal currency

recognition. This model learns compliant embeddings of banknote images in a variety of contexts, which can be shared publicly

(as a highly-compressed vector representation), and can be used to train and test specialized downstream models for any currency,

including those not covered by our dataset or for which only a few real images per denomination are available (few-shot learning). We

deploy a variation of this model for public use in the last version of the Seeing AI app developed by Microsoft. We share our encoder

model and the embeddings as an open dataset in our BankNote-Net repository.

CCS Concepts: • Human-centered computing → Accessibility technologies.

Additional Key Words and Phrases: assistive technology, vision impairment, low vision, blindness, computer vision, machine learning,

contrastive learning

ACM Reference Format:

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres. 2021. BankNote-Net:

Open dataset for assistive universal currency recognition. 1, 1 (April 2021), 13 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION

According to recent data [4], 43 million individuals worldwide are estimated to suffer blindness and over 200 million

people suffer moderate or severe vision impairment. 55% of people in this group are women and 89% live in low and

middle-income countries. Due to population growth and higher incidence of certain pathologies [4], the number of

Authors’ addresses: Felipe Oviedo, felipe.oviedo@microsoft.com, Microsoft AI for Good Research Lab, 1 Microsoft Way, Redmond, WA, USA, 98052;
Srinivas Vinnakota, Microsoft, 1 Microsoft Way, Redmond, WA, USA, 98052; Eugene Seleznev, Microsoft Seeing AI, 1 Microsoft Way, Redmond, WA, USA,
98052; Hemant Malhotra, Microsoft, 1 Microsoft Way, Redmond, WA, USA, 98052; Saqib Shaikh, Microsoft Seeing AI, 1 Microsoft Way, Redmond, WA,
USA, 98052; Juan Lavista Ferres, Microsoft AI for Good Research Lab, 1 Microsoft Way, Redmond, WA, USA, 98052.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2021 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

people affected by blindness and moderate to severe vision loss is expected to increase to over 61 million and 235 million

people by 2050, respectively.

Given the magnitude of the problem, the software community has developed numerous mobile applications and

wearable technologies to aid visually impaired individuals in a multitude of tasks [8, 14, 26], including navigation,

public transport use, scene understanding, person recognition, currency note recognition, etc. Seeing AI [15], a major

application in the space developed by Microsoft, is a pioneer of applying AI to assist individuals with low and no vision

and currently has 100 thousand average monthly users in 80 countries, supporting vision-to-speech in 19 languages.

One common task of assistive applications consists in the recognition (i.e. multi-class classification) of banknotes of

different denominations for a specific currency. Machine learning (ML) is often applied for this task. Several ML models

have been develop for particular currencies [21], including Indian Rupee [12, 19, 27, 28], US dollar [7, 9, 18], Canadian

dollar [18], Euro [20], Bangladesh Taka [11], Thailand Baht [25], Egyptian Pound [23], Pakistan Rupee [10], among

others [21]. The techniques employed in previous works include both classical computer vision and machine learning

approaches, such as LDA [19], feature transformations and template-matching [7, 20, 23], support vector machines [27],

and modern deep learning approaches [10, 11, 25, 28].

Either for training or for testing purposes, these models rely on a relatively large dataset of labelled banknote images

spanning all the denominations of certain currency. If the model is going to be deployed in a mobile assistive application,

as is common for the banknote recognition task, production quality models require well-curated datasets in a variety of

real assistive scenarios, including images with poor focus, various backgrounds, occlusions, blur, varying orientation

and perspective, etc. The lack of comprehensive public datasets has made limited the development of these models to a

few particular countries and currencies.

In this work, our goal is to facilitate the training of machine learning models for the recognition of any banknote in

assistive settings. For this purpose, we collect, curate and label BankNote-Net: the largest open dataset of banknote

images for accessibility to date, composed of a total of 24,826 images of front and back banknote faces across 17

currencies and 112 denominations. To approximate its use by an individual with low or no vision, the images are

captured in a variety of orientations, lightning, background, occlusion and image quality conditions. Using supervised

contrastive learning [13], we use this dataset to train a universal deep learning model for multi-class classification

spanning the 17 currencies and 224 classes (2 faces per each one of the 112 denominations). As the reproduction of

high-resolution currency images is regulated in some jurisdictions, such as as England [16] or Japan [17], we use

this model to learn highly compressed and compliant embeddings for the images. Using leave-one-group-out cross

validation, we demonstrate these embeddings are useful for pre-training models for currencies beyond those covered by

our dataset for both the abundant data setting and the few-shot learning case. Figure 1 summarizes our approach. These

embeddings, along with the pre-trained encoding model, are shared as a public dataset in our BankNote-Net repository.

In addition, we use our dataset for successful validation and deployment of a variation of our model in production, as

part of the most recent version of the Seeing AI app [15].

2 DATA COLLECTION AND CURATION

Deploying reliable ML models for banknote recognition requires images of banknotes in real assistive scenarios, i.e.

varying background, illumination, blur, orientation, occlusion and focus conditions. Although, high quality scans of

multiple currencies are available in banknote collectors’ websites such as [3, 6], abundant data of real usage scenarios

for assistive applications is very limited. In consequence, we design our data collection process with this accessibility

application in mind.

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

3

Fig. 1. Our approach: we use a convolutional neural network (MobileNet V2) as an encoder for our assistive banknote images. Using
supervised contrastive learning, the models learns an highly-compressed and descriptive embedding for each image. We use these
embeddings for downstream tasks, such as recognition of Yen banknotes, as shown in the figure. Our final dataset is composed of the
learned embeddings for 17 currencies and 224 classes. "Dense Classifier" corresponds to a fully-connected neural network, and "GAP"
corresponds to a Global Average Pooling layer.

The final data collection process consisted of the following steps:

(1) Currency selection: we selected the particular currencies to collect according to population and geographic

diversity, and according to areas with high number of users of the Microsoft Seeing AI app.

(2) Recruitment: for each specific currency, we on-boarded consenting volunteers or Amazon Mechanical Turk [2]

workers to perform data collection.

(3) Training: each worker or volunteer was provided with a brochure of requirements for the desired pictures along

with example pictures. This brochure is included in Appendix A.

(4) Data collection: each worker or volunteer captured and labelled up around 100 images for each orientation

(front or back) of each denomination of a specific currency. Each picture was captured with a square aspect

ratio to facilitate preprocessing. The images of each currency were captured exclusively by 1 or 2 volunteers

or workers located in the country where a specific currency is used; this decision was made to improve data

completeness across all denominations of a given currency.

Manuscript submitted to ACM

4

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

(5) Data validation and preparation: the collected data was validated manually by the authors to check for

inconsistencies in labelling or limited image diversity. Then, the file formats were standardized and all images

were resized to 224 x 224 pixels.

In total, 24,826 images were collected from 17 currencies. Figure 2a presents the distribution of collected images

across currencies, the distribution of images per class, and multiple examples of the collected images. The images span

112 denominations and 224 classes (each denomination maps to two classes, corresponding to the front and back faces

of the banknote). The mean number of images per class is 110, as shown in Figure 2. For banknotes that have more

than a one version in circulation, we defined each banknote version as a different class. Appendix B has a list of all

denominations and classes in our dataset.

3 DATA REPRESENTATION AND EVALUATION

The BankNote-Net dataset can be used for various computer vision tasks, including fine-tuning of recognition models

for those currencies included in the dataset, robust validation of recognition models, and transfer learning and few-shot

learning to novel currencies. For this purpose, we encode all BankNote-Net images as embeddings and make them

publicly available. In the following subsections, we present our encoder model and demonstrate the utility of our

embeddings.

3.1 Learning banknote representations

We are interested in learning a compact and compliant representation of banknote images that can be used for

downstream image recognition tasks. For this purpose, we train a neural network image encoder using a supervised

contrastive learning loss [13]. The supervised contrastive learning loss combines a contrastive loss with a cross entropy

loss to learn improved discriminative representations across all classes. Since most assistive applications are deployed in
mobile devices, we use the MobileNet V2 architecture [22] (𝛼 = 1.3, global average pooling for feature extraction, Image-
Net weights, temperature for supervised contrastive loss T=0.05, ADAM optimizer with lr = 5𝑒−5 and epochs = 150) as
the encoder backbone, as shown in Figure 1. The encoder outputs a 256-dimensional vector embedding 𝑣 ∈ R1×256.
During training with the supervised contrastive loss, we find beneficial to freeze the bottom layers in the backbone
architecture up to layer 𝑛 = 96. 𝑛, along all other discussed hyper-parameters, are chosen by hyperparameter tuning of
the mean AUC across all classes on a 10% random validation set, using the Optuna package [1]. During training, we use

random rotation, cropping and channel shift to augment the data.

For each image in our dataset, this model returns a highly-informative and compact embedding, which can be used

for downstream tasks, such as multi-class classification, transfer learning or few-shot learning. Our machine learning

model is summarized in Figure 1.

3.2 Leave-currency-out cross validation

We perform a leave-one-group-out cross validation procedure to confirm the predictive power of our embeddings. For

this, we hold out one of the 17 currencies in the dataset at a time, and train our encoder with the remaining currencies.

For each held-out currency, we split data according to two kinds of splits that we called: full split and few-shot split.

The full split corresponds to a 80%-20% random train-test split, representing a setting with 500-1500+ images per

denomination. The few-shot split corresponds to a 10%-90% train-test split, representing a few-shot learning setting

with 5-20 training images per class.

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

5

Fig. 2. Overview of BankNote-Net: a) Total number of images per currency. b) Distribution of number of images per class
(combination of denomination and front/back side, e.g. "5 USD - Front Side" is a class). c) Example of images in the dataset, collected
in diverse accessibility scenarios.

For each one these held-out data splits, we train an encoder model and use it as a feature extractor for the testing

currency. On top of this model, we train a classification head consisting of a global average pooling layer and 2 fully

Manuscript submitted to ACM

6

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

connected layers (dropout rate = 0.5, Adam optimizer, lr = 1𝑒 − 03, epochs = 50). For each test currency, we average the
results over 5 random seeds. It is important to emphasize that the test currency is never used during the training and

hyperparameter tuning of the encoder. We evaluate three different encoder initialization scenarios:

• BankNote-Net: Pre-training encoder with Banknote-Net embeddings of all training currencies, leaving out

testing currency. Frozen encoder, training and testing of top layers on test currency.

• ImageNet: Pre-training encoder with ImageNet. Frozen encoder, training and testing of top layers on test

currency.

• Random: Training of encoder and top layers from scratch, all layers trainable.

Figure 3 compares the accuracy, recall and precision for the three initialization scenarios and the two kinds of data

splits. We observe that the Banknote-Net embeddings lead to superior and less variable performance in both the full

and few-shot splits. In the few-shot split, we observe a notable boost in performance, confirming the generalization

power of our approach to unseen currencies, even when data is scarce. We note that the accuracy and recall using

BankNote-Net weights outperform those models that use other initialization, including ImageNet. The precision for the

BankNote-Net initialization is comparable to ImageNet, and is significantly superior to training the model from scratch.

The performance on unseen currency data is comparable to models trained exclusively on a single currency, such as

[10–12, 18].

3.3 Public embeddings

As discussed, the reproduction of high-resolution banknote images is regulated in some jurisdictions, such as as England

[16] or Japan [17]. Thus, we make our dataset public in the form of embeddings, which are distilled and compliant

representations of the banknote images. Although our original images are only 224x224 pixels in size and are not high

resolution scans of banknotes, we provide the following analysis for the benefit of the reader.

Our embeddings can be seen as a lossy hash of the collected images. To empirically demonstrate this, we use Shannon
coding theorem [5, 24]. According to the theorem, the maximum lossless compressed image size 𝐶 using pixel-wise
8-bit compression is:

𝐶 ≤ 𝑆

𝐻

8

(1)

Here, S is the original image size (in bytes) and H is the Shannon entropy of an image.

Modern lossless image compression algorithms, such as LZMA and DEFLATE, exploit the local structure in a image

to surpass the limit defined by Equation 1 [5]. If the image compression algorithm is allowed to be lossy, as in JPEG, it

can be further compressed; however, the image will not be able to be decompressed without resolution losses. Our

embedding technique compresses the image significantly way beyond commonly used lossy compression algorithms,

which in turn makes high resolution reconstruction of an image from the embedding infeasible.

We illustrate this fact in Figure ??. In the top figure we present the t-SNE representation for the embeddings of 5

major currencies in the dataset. The points naturally cluster around currencies, which confirms the discriminative

power of our embeddings. In the bottom figure, we present the original file size (in kilobytes (KB), before downsizing to

224x224 pixels) of each image in the dataset according to its entropy. The Shannon coding theorem sets a theoretical

limit for the compressed data points as described in Equation 1. As discussed, lossless JPEG surpasses this limit, specially

for cases with low image entropy. The 50% lossy JPEG image improves on the lossless JPEG compression, with most

images in the dataset located below the Shannon coding bounds. Our BankNote-Net embeddings surpass the best

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

7

Fig. 3. Distribution of accuracy, recall and precision for all test currencies after leave-currency-out cross validation. We use a particular
initialization (BankNote-Net, ImageNet, Random) and a specific train-test split (Full-split or Few-Shot split).

compression ratio by 1-2 orders of magnitude. This significant compression level, combined with the initial image

resizing step, makes lossless and high resolution reconstructions of the images in the dataset infeasible.

4 OPEN DATASET AND PRODUCTION DEPLOYMENT

Originally, the Seeing AI app supported recognition of 7 currencies that were deployed and serviced as independent

models for each currency. With Banknote-Net, we substantially expanded the currencies covered by the Seeing AI app,

by using the dataset to train and validate a variation of the universal model for all 17 supported currencies. In this case,

we perform further hyperparameter tuning to maximize the model precision for predictions with confidence of 0.99 or

above. We choose this approach to minimize false positives during assistive use. This single universal model is deployed

in the last version of the Seeing AI mobile app.

The embeddings, labels and the pre-trained MobileNet V2 encoder model developed during work are available in the

BankNote-Net repository.

Manuscript submitted to ACM

8

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

Fig. 4. Top: t-SNE representation of the embeddings of 5 major currencies in the BankNote-Net dataset. The embeddings naturally
cluster individual currencies together, aiding predictive performance of models learned on the BankNote-Net data. Other currencies
in the dataset are omitted for plot readability. Bottom: Compressed size (in Kilobytes) for each image in the dataset as a function of
entropy. Our encoded (BankNote-Net) representation is several orders of magnitude smaller than lossy and lossless compression
algorithms, making high-resolution reconstruction infeasible.

5 DISCUSSION AND CONCLUSIONS

In this work, we collect and make public a large dataset of banknote images for assistive recognition. To share the

dataset, we use supervised contrastive learning to get learn compliant and highly-compressed embeddings for each of

the 24,826 images and the 17 currencies in the dataset. We demonstrate the power of these embeddings to improve

the predictive performance of currency recognition models for assistive applications, including the few-shot scenario.

Additionally, we demonstrate that the embeddings cannot be feasibly decompressed to high-resolution images.

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

9

Our dataset has several limitations to consider. In particular, most of the images of a given currency were collected

by 1-2 volunteers or workers. Although the workers were clearly aiming to optimize diversity of settings, backgrounds

and orientations for the images, the final diversity of the dataset can be limited by bias during data collection, inherent

specifications of the mobile camera employed, limited background for image capture, etc. This constitutes an important

trade-off we made for the data collection process, and we chose to perform it in this way to maximize completeness

across the denominations within a given currency. Regardless, we show that the learned embeddings have substantial

generalization power, including to currencies not covered by the BankNote-Net dataset. This makes the dataset very

useful for future applications, for instance to extend the model to non-supported currencies or to include new versions

of banknotes that were not considered in the dataset. In the future, we may consider expanding the dataset to achieve a

broader diversity of data collectors.

Another important consideration is that the deployment of the model in production may require the optimization of

particular metric of interest beyond precision and recall, e.g. the high confidence precision mentioned in Section 4. In

this case, the dataset user may benefit from re-training or fine-tuning the encoder on a representative validation dataset

to achieve good performance on the custom metric.

ACKNOWLEDGMENTS

The authors thank Ria Sankar and Heila Precel for helpful discussion. We acknowledge the contributions of all data

collectors across the world.

6 DATA AND CODE AVAILABILITY

The BankNote-Net dataset, along with an encoder, labels and usage examples is available as an open dataset under the

MIT and CDLA-Permissive-2.0 licenses in the BankNote-Net repository.

REFERENCES

[1] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A next-generation hyperparameter optimization

framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2623–2631.

[2] Amazon. 2022. Amazon Mechanical MTurk. https://www.mturk.com/.
[3] BankNote.sw. 2022. baknote.ws. http://www.banknote.ws/.
[4] Rupert Bourne, Jaimie D Steinmetz, Seth Flaxman, Paul Svitil Briant, Hugh R Taylor, Serge Resnikoff, Robert James Casson, Amir Abdoli, Eman
Abu-Gharbieh, Ashkan Afshin, et al. 2021. Trends in prevalence of blindness and distance and near vision impairment over 30 years: an analysis for
the Global Burden of Disease Study. The Lancet Global Health 9, 2 (2021), e130–e143.

[5] Xinghua Cheng, Zhilin Li, et al. 2020. How does Shannon’s source coding theorem fare in prediction of image compression ratio with current

algorithms? (2020).

[6] IBNS Database. 2022. IBNS. https://www.theibns.org/.
[7] Shan Gai, Guowei Yang, and Minghua Wan. 2013. Employing quaternion wavelet transform for banknote classification. Neurocomputing 118 (2013),

171–178.

[8] Lilit Hakobyan, Jo Lumsden, Dympna O’Sullivan, and Hannah Bartlett. 2013. Mobile assistive technologies for the visually impaired. Survey of

ophthalmology 58, 6 (2013), 513–528.

[9] Faiz M Hasanuzzaman, Xiaodong Yang, and YingLi Tian. 2012. Robust and effective component-based banknote recognition for the blind. IEEE

Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, 6 (2012), 1021–1030.

[10] Muhammad Imad, Farhat Ullah, Muhammad Abul Hassan, et al. 2020. Pakistani Currency Recognition to Assist Blind Person Based on Convolutional

Neural Network. Journal of Computer Science and Technology Studies 2, 2 (2020), 12–19.

[11] Md Islam, Mohiuddin Ahmad, Akash Shingha Bappy, et al. 2021. Real-Time Bangladeshi Currency Recognition Using Faster R-CNN Approach for

Visually Impaired People. In Communication and Intelligent Systems. Springer, 147–156.

[12] Abhishek Jain, Paras Jain, and Vikas Tripathi. 2021. Automated Currency Recognition Using Neural Networks. In Research in Intelligent and

Computing in Engineering. Springer, 835–841.

Manuscript submitted to ACM

10

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

[13] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.

Supervised contrastive learning. arXiv preprint arXiv:2004.11362 (2020).

[14] Roberto Manduchi and Sri Kurniawan. 2018. Assistive technology for blindness and low vision. CRC Press.
[15] Microsoft. 2022. Seeing AI app. https://www.microsoft.com/en-us/ai/seeing-ai.
[16] Bank of England. 2022. Using images of banknotes | Bank of England. https://www.bankofengland.co.uk/banknotes/using-images-of-banknotes.
[17] Bank of Japan. 2022. Laws concerning Banknote Reproduction : Bank of Japan. https://www.boj.or.jp/en/note_tfjgs/note/security/gizo0410a.htm/.
[18] Nektarios Paisios, Alex Rubinsteyn, Vrutti Vyas, and Lakshminarayanan Subramanian. 2011. Recognizing currency bills using a mobile phone: an
assistive aid for the visually impaired. In Proceedings of the 24th annual ACM symposium adjunct on User interface software and technology. 19–20.
[19] Shafin Rahman, Prianka Banik, and Shujon Naha. 2014. LDA based paper currency recognition system using edge histogram descriptor. In 2014 17th

International Conference on Computer and Information Technology (ICCIT). IEEE, 326–331.

[20] Vishwas Raval and Apurva Shah. 2018. HORBoVF—A Novel Three-Level Image Classifier Using Histogram, ORB and Dynamic Bag of Visual

Features. In International Conference on ISMAC in Computational Vision and Bio-Engineering. Springer, 895–902.

[21] Vishwas Raval and Apurva Shah. 2019. A Comparative analysis on currency recognition systems for iCu e and an improved grab-cut for android

devices. In Emerging Technologies in Data Mining and Information Security. Springer, 279–286.

[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear

bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4510–4520.

[23] Noura A Semary, Sondos M Fadl, Magda S Essa, and Ahmed F Gad. 2015. Currency recognition system for visually impaired: Egyptian banknote as

a study case. In 2015 5th International Conference on Information & Communication Technology and Accessibility (ICTA). IEEE, 1–6.

[24] Claude Elwood Shannon. 2001. A mathematical theory of communication. ACM SIGMOBILE mobile computing and communications review 5, 1

(2001), 3–55.

[25] Fumiaki Takeda, Lalita Sakoobunthu, and Hironobu Satou. 2003. Thai banknote recognition using neural network and continues learning by DSP

unit. In International Conference on Knowledge-Based and Intelligent Information and Engineering Systems. Springer, 1169–1177.

[26] Ruxandra Tapu, Bogdan Mocanu, and Titus Zaharia. 2020. Wearable assistive devices for visually impaired: A state of the art survey. Pattern

Recognition Letters 137 (2020), 37–52.

[27] Anand Upadhyay, Shashank Shukla, and Verlyn Pinto. 2020. Indian Currency Recognition Using Radial Basis Function and Support Vector Machine.

In International Conference on Intelligent Computing and Smart Communication 2019. Springer, 1049–1056.

[28] Venkataramana Veeramsetty, Gaurav Singal, and Tapas Badal. 2020. Coinnet: platform independent application to recognize Indian currency notes

using deep learning techniques. Multimedia Tools and Applications 79 (2020), 22569–22594.

A APPENDIX A

The data collection instructions and image examples shared with data collectors are included below.

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

11

Manuscript submitted to ACM

12

Felipe Oviedo, Srinivas Vinnakota, Eugene Seleznev, Hemant Malhotra, Saqib Shaikh, and Juan Lavista Ferres

Manuscript submitted to ACM

BankNote-Net: Open dataset for assistive universal currency recognition

13

B APPENDIX B

List of classes in the BankNote-Net dataset, composed (in order) of a currency Forex abbreviation, denomination, face

(front or back) and, if applicable, variation of banknote.

Manuscript submitted to ACM

