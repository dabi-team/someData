2
2
0
2

r
p
A
5
1

]
T
E
.
s
c
[

1
v
9
2
4
7
0
.
4
0
2
2
:
v
i
X
r
a

Experimentally realized memristive memory augmented
neural network

Ruibin Mao1, Bo Wen1, Yahui Zhao1, Arman Kazemi2,3, Ann Franchesca Laguna3, Michael
Neimier3, X. Sharon Hu3, Xia Sheng2, Catherine E. Graves2,*, John Paul Strachan4,5,*, and Can
Li1,*

1Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong
SAR, China
2Hewlett Packard Labs, Hewlett Packard Enterprise, Milpitas, CA, USA
3Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN,
USA
4Peter Gr¨unberg Institut (PGI-14), Forschungszentrum J¨ulich GmbH, J¨ulich, Germany
5RWTH Aachen University, Aachen, Germany
*canl@hku.hk, j.strachan@fz-juelich.de, catherine.graves@hpe.com

Abstract

Lifelong on-device learning is a key challenge for machine intelligence, and this requires
learning from few, often single, samples. Memory augmented neural network has been pro-
posed to achieve the goal, but the memory module has to be stored in an off-chip memory due
to its size. Therefore the practical use has been heavily limited. Previous works on emerging
memory-based implementation have difﬁculties in scaling up because different modules with
various structures are difﬁcult to integrate on the same chip and the small sense margin of the
content addressable memory for the memory module heavily limited the degree of mismatch
calculation. In this work, we implement the entire memory augmented neural network ar-
chitecture in a fully integrated memristive crossbar platform and achieve an accuracy that
closely matches standard software on digital hardware for the Omniglot dataset. The suc-
cessful demonstration is supported by implementing new functions in crossbars in addition to
widely reported matrix multiplications. For example, the locality-sensitive hashing operation
is implemented in crossbar arrays by exploiting the intrinsic stochasticity of memristor de-
vices. Besides, the content-addressable memory module is realized in crossbars, which also
supports the degree of mismatches. Simulations based on experimentally validated mod-
els show such an implementation can be efﬁciently scaled up for one-shot learning on the
Mini-ImageNet dataset. The successful demonstration paves the way for practical on-device
lifelong learning and opens possibilities for novel attention-based algorithms not possible in
conventional hardware.

1

 
 
 
 
 
 
Introduction

Deep neural networks (DNN) have achieved massive success in data-intensive applications but
fail to tackle tasks with a limited number of examples. On the other hand, our biological brain
can learn patterns from rare classes at a rapid pace, which could relate to the fact that we can
recall information from an associative, or content-based addressing, memory.
Inspired by our
brain, recent machine learning models, such as memory augmented neural networks (MANN)1,
have adopted a similar concept, where explicit external memories are applied to store and retrieve
the learned knowledge. While those models have shown the ability to generalize from rare cases,
they are struggled to scale up2, 3. This is because, the entire external memory module needs to
be accessed from the memory to recall the learned knowledge, which immensely increases the
memory overhead. The performance is thus bottlenecked by the memory bandwidth and capacity
issues4–6 in hardware with the traditional von-Neumann computing architecuture7, especially when
they are deployed in edge devices, where the energy sources are limited.

Emerging non-volatile memories, e.g., memristors8, have been proposed and demonstrated
to solve the bandwidth and memory capacity issues in various computing workloads, including
deep neural networks9–14, signal processing15, 16, scientiﬁc computing17, 18, solving optimization
problems19, 20, and more. Those solutions are based on the memristor’s ability to directly process
analog signals at the location where the information is stored. Most existing demonstrations men-
tioned above, however, mainly focus on executing matrix multiplications for accelerating deep
neural networks with crossbar structures8–12, 17, 18, 21, whose experience cannot be directly applied
to the models with explicit external memories in MANNs. Recently, several pioneering works aim
to solve the problem with memristor-based hardware. One promising solution is to exploit the
hyperdimensional computing paradigm 22. But the high dimension vectors, as required to main-
tain the quasi-orthogonal nature, lead to excessive memory requirements. The recent prototype
of this framework uses a large number of phase-change memory devices to solve the important
yet simple Omniglot problem, the most commonly used handwritten dataset for few-shot image
classiﬁcation.23. Although a close to software-equivalent accuracy was achieved, the number of
devices required by the dimension of the binary feature vectors will increase signiﬁcantly for more
complex problems. Ferroelectric device based ternary content addressable memory (TCAM) has
been proposed to be used as the hardware to calculate the similarity directly in the memory24, 25,
but it is only suitable for degree of mismatch up to a few bits. Besides, the locality sensitive hash-
ing (LSH) function that enables the degree of mismatch search in the TCAM was implemented in
software, and the experimental demonstration was limited to a 2×2 TCAM array. More recently,
a 2T-2R TCAM associative memory was used to demonstrate few-shot learning by calculating L1

2

distance 26. In this work a 2-bit readout scheme is employed (requires 64 cycles per row) which
incurs high energy and latency overheads, and feature extraction is again relegated to a digital
processor. The experimental demonstration of the entire MANN concept still remains a high risk,
since the imperfections in such analog hardware, such as device variation, ﬂuctuation, state drift,
and readout noise during the parallel matrix multiplication operations, have challenged the feasi-
bility of the promising hardware.

In this work, we experimentally demonstrate one- and few-shot learning with the entire
MANN fully implemented in our integrated memristor hardware. To achieve this goal, we im-
plement different functionalities in crossbars that are different from the widely reported matrix
multiplication operations. One enabler is the locality-sensitive hashing (LSH) function in crossbars
by fully exploiting the intrinsic stochasticity of memristor devices. This is different from crossbars
for matrix multiplications, where the stochasticity needs to be minimized. Another innovation is
implementing the search function and using the crossbar as a ternary content addressable memory
(TCAM). In addition to what’s possible with conventional TCAMs, the proposed scheme can also
measure the degree of mismatch reliably, which is crucial for few-shot learning implementation.
Since the requirements for those functions are different from conventional matrix multiplications,
here we introduce several hardware-software co-optimization methods, including the introduction
of the wildcard ‘X’ bit in the crossbar-based LSH, and the careful choice of conductance range
according to the device statistics.

Finally, with a fully integrated memristor computing system, we are able to experimentally
demonstrate the few-shot learning with a complete MANN model, including a ﬁve-layer convo-
lutional neural network (CNN), the hashing function, and the similarity searches. Taking into
consideration all imperfections in the emerging system, our hardware achieves 94.9% ± 3.7% ac-
curacy in the 5-way 1-shot task with the Omniglot dataset27, a popular benchmark for few-shot
image classiﬁcation, and 74.9% ± 2.4% accuracy in 25-way 1-shot task, which is close to the
software baseline (95.2% ± 2.6% for 5-way 1-shot and 76.0% ± 2.7% for 25-way 1-shot). Our
experimentally-validated model also shows that the proposed method is scalable with a 58.7 % ac-
curacy to recognize rare cases (5-way 1-shot) for the Mini-ImageNet dataset28, where each image
is a color (RGB) image of size 84×84 pixels which is nine times larger than the size (28×28) of
images in the Omniglot dataset. This accuracy is only 1.3% below the software baseline. Owing
to the in-memory analog processing capability and the massive parallelism, our hardware achieves
more than 4,500× improvement in latency and 2, 900× improvement in energy consumption on
crossbar arrays as compared to the general-purpose graphic processing unit (GPGPU) (Nvidia
Tesla P100).

3

Memory augmented neural networks in crossbars

The MANN architecture commonly includes a controller, an explicit memory, and a content-based
attention mechanism between the two. A controller is usually a traditional neural network structure
such as a convolutional neural network, a recurrent neural network (RNN), or the combination of
different neural networks. The explicit memory stores the extracted feature vectors as the key-
value pairs so that the model can identify the values based on the similarity or distances between
the keys. The access of the explicit memory is the performance bottleneck for models that run
on conventional hardware, such as the general-purpose graphic processing unit (GPGPU). It is
because the similarity search requires accessing all the content in the memory; thus the repeated
data transfer process delays the readout process and consumes abundant energy, especially when
the memory needs to be placed in a separate chip.

We implement the entire network in our memristor-based hardware system to address the
expensive data transfer issue in conventional digital hardware. The system aims to perform both
matrix multiplication, for neural network controllers, and the similarity search, for the explicit
memory, directly in the memristor crossbars, where the data are stored. Memristors have demon-
strated great success in accelerating matrix multiplications in neural networks, which is also the
controller part of the MANN, owing to their extreme density29, the compatibility with CMOS
integration9, 30, and the capability to process analog information directly in its memory. But, the
demonstrations for explicit memories are still limited to small-scale experiments24, 25 or simulated
multiplication-and-add with readout conductances23.

Fig. 1a illustrates how we implement the MANN in the crossbars. First, a regular crossbar-
based convolutional neural network is used to extract the real-valued feature vector, and the method
implementing this step has been widely reported previously11, 31, 32. After that, distances are cal-
culated between the extracted feature vector and those stored in a memory. Cosine distance (CD)
is one of the most widely used distance metrics in the explicit memory of various MANN im-
plementations, but it is not straightforward to implement with memristor-based crossbars. On the
other hand, the cosine distance between two real-valued vectors can be well approximated by the
Hamming distance (HD) using locality sensitive hashing (LSH) codes of the two vectors24, 33, 34.
Accordingly, in this work, instead of being stored in a dynamic random access memory (DRAM)
for future distance calculations, the features are hashed into binary/ternary signatures in a crossbar
with randomly distributed conductance at each crosspoint exploiting the stochasticity of memristor
devices. Those signatures are then searched against those previously stored in another crossbar that
acts as a content-addressable memory enabled by a newly proposed coding method, from which
we can also calculate the degree of mismatch that approximates the cosine distance of the original

4

real-valued vector.

The idea is experimentally demonstrated in our integrated memristor chip. One of the tiled
64 × 64 memristor crossbars in our integrated chip that we used to experimentally implement the
network is shown in Fig. 1b. The peripheral control circuits, including the driving, sensing,
analog-to-digital conversions, and the access transistors, are implemented with a commercial 180
nm technology integrated chip (Fig. 1c). 50 nm×50 nm Ta/TaOx/Pt memristors are integrated with
back-end-of-the-line (BEOL) processing on top of the control peripheral circuits (1d). The fab-
rication details, the device characteristics and peripheral circuit designs were reported previously
elsewhere35, 36, and the picture of our test chip and platform is shown in Supplementary Fig. 1.

5

Figure 1: Memory augmented neural networks in crossbar arrays. a, The schematic of a crossbar-
based MANN architecture. The expensive data transfer in a von Neumann architecture can be alleviated by
performing analog matrix multiplication, locality sensitive hashing, and nearest neihgbor searching directly
in memristor crossbars, where the data is stored. b, Optical image of a 64×64 crossbar array in a fully
integrated memristor chip. c, Cross-section of the memristor chip, where complementary metal-oxide-
silicon (CMOS) circuits at the bottom, inter-connection in the middle, and metal vias on the surface for
memristor integration with back-end processes. d, Top view of four 50 nm×50 nm integrated cross-point
memristors. Animal ﬁgures in a are taken from www.flaticon.com

6

XB 1XB 2XB 3XB 4XB 5CNN controllerReal-valued feature vectorsD dimensions00110X0011100011X00100011Ternary key memoryM memory entriesK-bits ternary vectorRow muxTIA, S&H, ADCWriteSearch5-way 1-shotSupport setQuery imageLabel?Ternary signature embeddingsCat !a+-+-+-Hashing vectorReal-valued vectorCat !GonGoffNumber of mismatches:1101 bit2bits0bit2bits2bitsbdc64 rows64 columns1 μm1 μmXB: Memristor crossbar arrayAnalog Matrix MultiplicationLocality Sensitive HashingNearest Neighbor SearchTernarysignatures20 µmLocality sensitive hashing in crossbar array

We ﬁrst introduce the idea of performing locality sensitive hashing (LSH) directly in crossbars
employing intrinsic stochasticity, and validate that the experimental implementation in memristor
crossbars can approximate cosine distance. LSH 37–39 is a hashing scheme that generates the same
hashing bits with a high probability for the inputs that are close to each other. One of the hashing
functions among the LSH family is implemented by random signed projections, i.e. , applying a
set of hyperplanes to divide the Hilbert space into multiple parts such that similar inputs project
to the same partition with a high probability (Fig. 2a). This random projection is mathematically
expressed by a dot product of the input vector (cid:126)a and a random normal vector (cid:126)n, so that ‘1’ is
generated if (cid:126)a ·(cid:126)n > 0, and ‘0’ otherwise. Accordingly, LSH bits can be calculated by Equation 1 in
a matrix form,

(cid:126)h = H ((cid:126)a N)

(1)

where the (cid:126)h is the binarized hashing vector, (cid:126)a the input real-valued feature vector, N the random
projection matrix with each column a random vector, and H the Heaviside step function.

The random projection matrix can be constructed physically by exploiting the stochastic
programming dynamics of the memristor devices or the initial randomness after the fabrication
(Fig. 2b). But it is still challenging to generate random vectors (cid:126)n with a zero mean value, as
required by the LSH algorithm, because the conductance of the memristor device can only be
positive values. Our solution is to take the difference between devices in the adjacent columns40
in the crossbar array. The devices from the columns, assuming no interference in between, are
independent of each other. Therefore, the distribution of the conductance difference will also be
uncorrelated and random. In this way, the random normal vector (cid:126)n in the original equation can be
/k. The zero mean
represented by the difference of two conductance vectors, i.e. (cid:126)n =
value of the vector(cid:126)n is guaranteed as long as the distribution of the memristor conductance vectors
( (cid:126)g+, (cid:126)g−) have the same mean value. k is a scaling factor, which can be set as an arbitrary value
because we only need to determine if the output is larger than zero or otherwise.

(cid:16) (cid:126)g+ − (cid:126)g−(cid:17)

Here, we experimentally program all devices in a crossbar array to the same target conduc-
tance state. The programming process of memristor devices is stochastic9 as the thinnest part of
the conductive ﬁlament can be only a few atoms wide41. Accordingly, the ﬁnal conductance values
follow a random distribution with the mean roughly matching the target conductance. To lower
the output current and thus the energy consumption, we reset all devices close to the highest re-
sistance state that we can achieve (17 nS at the read voltage of 0.2 V) from arbitrary initial states
using a few pulses (see Methods for details). After programming, as expected, most devices are
programmed to a conductance state near the highest resistance state (Fig. 2c), and the difference

7

between devices from adjacent columns follows a random distribution with a zero mean value (Fig.
2d, e). Hashing bits for an input feature vector are generated efﬁciently by performing multipli-
cations with the randomly conﬁgured memristor crossbar array. After converting the real-valued
input vector into the analog voltage vector, the dot product operations are conducted by applying
the voltage vectors to the row wires of the crossbar and reading the current from the column wires.
Thus, the hashing operation is completed by comparing the current amplitude from the adjacent
columns (Fig. 2b) in one step, regardless of the vector dimension.

Imperfections in emerging memristive devices, such as conductance relaxation and ﬂuctua-
tion, limit experimental performance. This is mainly because the device conductance ﬂuctuations
incur instability of hashing planes implemented as adjacent column pairs in crossbar arrays. This
causes hashing bits for input vectors that are close to hyperplanes to ﬂip between 0 and 1 over
time (Supplementary Fig. 4) and therefore leads to an inaccurate approximation of the cosine dis-
tance. It should be emphasized that this problem only becomes apparent when performing matrix
multiplication in arrays, rather than multiplying with the readout conductances reported in other
works23.

To mitigate bit-ﬂipping, we propose a software-hardware co-optimized ternary locality sen-
sitive hashing scheme (TLSH). The scheme introduces a wildcard ‘X’ value to the hashing bits
(Fig. 2a), in addition to ‘0’ and ‘1’ in the original hashing scheme. As the name implies, the Ham-
ming distance between the wildcard ‘X’ and any values will always be zero. The ‘X’ is generated
when the absolute value of the output current difference is smaller than a threshold value, i.e. Ith.
The value for Ith is chosen to be small and close to the transient analog computing error from the
crossbar, such that any bit-ﬂipping minimally impacts the similarity search.

We validate the proposed approach by conducting experiments in our integrated memristor
crossbars. The hash outputs for 500 64-dimensional real-valued random vectors are computed in
our memristor crossbars for binary and ternary hashing vectors. The Ith representing the threshold
of the ‘X’ wildcard is set to 4 µA in the ternary hashing implementation (TLSH). Fig. 2f shows
that the cosine distance is closely correlated with the Hamming distance between the hashed vec-
tors with 128 hashing bits in total, regardless of whether the hashing codes are generated by a
32-bit ﬂoating-point digital processor (“software LSH” in Fig. 2f), the analog crossbar (“hardware
LSH”), or the proposed co-designed ternary hashing codes by the crossbar (“hardware TLSH”).
Note that the Hamming distance of the ternary hashing codes is smaller than that of the binary
codes because the distance to a wild card ‘X’ is always zero. The effectiveness of the method is
evaluated quantitatively by the linear correlation coefﬁcient, as shown in Fig. 2g. The result shows
that the proposed ternary hashing narrows the already small gap between the digital software ap-

8

proach and our analog hardware approach. The performance improvement results from signiﬁ-
cantly reduced unstable bits, which is experimentally demonstrated by the comparison shown in
Supplementary Fig. 4. The results demonstrate that crossbar arrays, utilizing the proposed ternary
scheme, can effectively and efﬁciently perform hashing operations, taking advantage of intrinsic
stochasticity and massively parallel in-memory computing.

9

Figure 2: Robust ternary locality sensitive hashing in analog memristive crossbars. a, Illustration of the
Locality Sensitive Hashing (LSH) and the Ternary Locality Sensitive Hashing (TLSH) concept. b, The LSH
or TLSH implemented in memristor crossbars. Each adjacent column pair represents one hashing plane.
So, crossbars with N + 1 columns can generate N hashing bits with this method. Greyscale colors on the
memristor symbol represent random conductance states. c, A random memristor conductance distribution
in a 64×129 crossbar after applying ﬁve RESET pulses to each device. The intrinsic stochastic behavior in
memristor devices results in a lognormal-like distribution near 0 µS. d, The distribution of the memristor
conductance difference for devices in adjacent columns. The differential conductance distribution is random
with zero-mean, matching the requirements of our hashing scheme. e, The conductance difference map of
size 64×128 (including three crossbar arrays each of size 64×64). f, The correlation between cosine distance
and Hamming distance with different hashing representations shows that the Hamming distance generated
by both hardware and software can well approximate the cosine distance. IQR, interquartile range. g, The
linear correlation coefﬁcient between Hamming distance and cosine distance increases with the number of
total hashing bits. The hardware TLSH approach shows a higher correlation coefﬁcient than the hardware
LSH approach due to the reduced number of unstable bits, as detailed in Supplementary Fig. 4.

10

abcfgd0                                      1abcLSH0             X       X            1abcTLSHeConductance difference [µS]10-105-5011646432128Conductance [µS]00244008001200120016006810μ = 2.933 μSσ = 5.432 μSCountμ = 0.017 μSσ = 3.097 μSConductance difference [µS]040080012001600-10105-50Countμ = 0.017 μSσ = 3.097 μS25% - 75%Range within 1.5 IQRMedian line+-+-+-Hashing vectorReal-valued vector(1)(2)(3)(N)(N+1)software LSHhardware TLSHhardware LSH80.40.50.60.70.80.9163264128Linear correlation coefficient between CD and HD# of hashing bits640.820.840.860.880.900.92128Hamming distanceCosine distanceCosine distanceCosine distancehardware TLSHhardware LSHsoftware LSH00.51.01.500.51.01.500.51.01.5020406080100Median line25% - 75%Range withiMedian line25% - 75%Range within 1.5 IQRsoftware LSHhardware TLSHhardware LSHRandom conductancesTCAM in crossbars with ability to output degree of mismatches

Following the LSH step, the binary or ternary hashing signatures will be searched against the
hashed signatures previously stored in a memory to calculate the similarity and thus ﬁnd the k-
closest match. As mentioned earlier, this is an extremely time- and energy-consuming step on
conventional hardware such as GPUs. Content addressable memories (CAM) or the ternary ver-
sion (TCAM) are direct hardware approaches that can ﬁnd the exact match in the memory in one
step. Still, existing static random access memory (SRAM) based CAM/TCAM implementations
limit the available memory capacity and incur high power consumption. CAMs/TCAMs based on
non-volatile memories have been developed recently, including those based on memristor/ReRAM
(e.g. 2T-2R26, 42, 2.5T-1R43), ﬂoating gate transistor (e.g. 2Flash44), ferroelectric transistors (e.g.
2FeFET24, 25), etc. Although these studies demonstrated good energy efﬁciency, they are limited to
at most a few bits mismatches which have difﬁculties to serve as attentional memory modules for
scaled-up MANNs24–26.

We implement the TCAM functionality directly in an analog crossbar with the additional
ability to output the degree of mismatch based on the Hamming distance, rather than only a binary
match/mismatch. In contrast to conventional TCAM implementations which sense a mismatch by
a dis-charged match-line, our crossbar-based TCAM searches through a simple encoding and a set
of dot product operations computed in the output currents. Fig. 3a shows a schematic on how this
scheme works. First, the query signature is encoded to use a pair of voltage inputs for 1-ternary-bit,
so that one column wire is driven to a high voltage (i.e. Vsearch), while the other is grounded. The
corresponding memristor conductances that store previous signatures are encoded with one device
set to a high conductance state (i.e.
,
Goff ≈ 0). In this way, for a ‘match’ case, the high voltage will be applied to the device in the low
conductance state, and therefore, very little current is added to the row wires. In a ‘mismatch’ case,
the high voltage applied to the device in the high conductance state will contribute Vsearch×Gon
to the output current of the column wires. The wildcard ‘X’ in the ternary implementation will
be naturally encoded as two low voltages as input or two low conductance devices so that they
contribute zero or very little current and thus always yield ‘match’. In this way, the degree of
mismatch, the Hamming distance, between the query signature and all words stored in the crossbar
is computed in a constant time-step by sensing the column currents from the crossbar (see Fig. 3b).

, Gon) and the other to the lowest conductance state (i.e.

We have experimentally implemented the above TCAM for measuring Hamming distance
in memristive crossbars. First, eight different binary signatures, each has eight bits but different
number of ‘1’s (from one ‘1’ to eight ‘1’s), are encoded into conductance values as shown in Fig.
3c. The conductance values are then programmed to a crossbar with an iterative write-and-verify

11

method (see ref.35 and Methods for details), with the readout conductance matrix after successful
programming shown in Fig. 3b. We choose 150 µS as the Gon for a higher on/off conductance ratio
and minimal relaxation drift (Supplementary Fig. 2). Fig. 3d shows both the distribution of Gon
and Goff after programming.

After conﬁguring the memory to store the previously generated signatures, 100 ternary sig-
nature vectors as queries are randomly chosen and the corresponding encoded voltages are applied
to the column wires of the crossbar (Fig. 3c), to perform the search operation. The search voltage
(Vsearch) is chosen to be 0.2 V in this work, so each mismatched bit will contribute approximately
30 µA (=0.2 V×150 µS) to the output current. In the experiment, however, results are deteriorated
by non-ideal factors. For example, the memristor in a low conductance state still contributes a small
current (Vsearch×Goff(cid:54)=0) in a match case, imperfect programming of Gon results in deviations in
output current for each ‘mismatch’ case, etc. Our device exhibits a large enough on/off ratio35, but
for devices with a lower conductance on/off ratio, such as MRAM45 , the problem would be more
signiﬁcant. For such cases, we propose a 3-bit encoding that is discussed in detail in Supplemen-
tary Fig. 5. Additionally, non-zero wire resistances cause voltage drop along wires, lowering the
output current from what would be expected ideally. Despite these factors, the output current in
our experiments exhibits a linear dependence on the number of mismatch bits, i.e. ternary Ham-
ming distance (Fig. 3e). Fig. 3f shows separated distributions where each distribution represents a
distinct number of mismatch bits ranging from 0 to 8. We have thus experimentally demonstrated
a robust capability to store patterns, search patterns, and obtain the degree of mismatch which
will enable determining the closest match that is stored in an array by simply comparing output
currents.

12

Figure 3: TCAM implemented in crossbar array capable of conducting Hamming distance calcula-
tion. a, Illustration of the basic principle for using dot product to distinguish ‘match’ and ‘mismatch’ cases.
b, The schematic of calculating Hamming distance in a crossbar. The ﬁgure shows three 3-dimensional
ternary key vectors stored in a 3 × 6 crossbar with a differential encoding. Differential voltages representing
ternary bits in search vectors are applied to the source line and the output current from the bit line can rep-
resent the THD between the search vector and keys stored in the memory. c, The readout conductance map
after eight binary vectors experimentally stored in the crossbar as memory. In the experiment, we set Gon as
150 µS and Vsearch as 0.2 V. d, Distribution of Gon and Goff. e, Ouput current shows a linear relation with
Hamming distance measuring the degree of mismatches. IQR, interquartile range. f, Current distributions
are separated from each other through which we can obtain the number of mismatch bits (i.e. , Hamming
distance).

13

bacfedDegree of mismatches (Hamming distance)Current [µA]0123456782520507510012515017520025% - 75%Range within 1.5 IQRMedian lineMean   1 bit mismatch   2 bits mismatch255075100125150175200mismatchmismatch001020304050607080Current [µA]CountmatchGonGoffConductance [μS]Count020406080100120140160051015202530GonGoff10Xmatch1 bit mismatch2 bits mismatchSearch vector10001X1X0Stored key vectors‘1’Input: ‘1’0Match‘1’Input: ‘0’V×GonMismatch‘1’Input: ‘X’0MatchConductance map in a crossbar as a TCAM24610812141612345678Conductance [µS]Row #Col #1X000101XOne- and few-shot learning experiment fully implemented in memristor hardware

We implemented a complete MANN demonstrating one-shot and few-shot learning in crossbars.
To evaluate and compare the performance of our method, we chose the Omniglot dataset27, a
commonly used benchmark for few-shot learning tasks. In this dataset, there are 1,623 different
handwritten characters (classes) from various alphabets. Each character contains 20 handwritten
samples from different people. Samples from 964 randomly chosen classes are used to train the
CNN controller and the remaining 659 are used for one-shot and few-shot learning experiments. In
an N-way K-shot learning experiment, the model should be able to learn to classify new samples
from N different characters (classes) after being shown K handwritten images from each character
(support set). The accuracy is evaluated by classifying an unseen sample (query set) after learning
from the limited number of samples (only one sample each for the 1-shot problem) from each class.

In our experiment, the memristor CNN controller ﬁrst extracts the feature vector from an
image. Note that the CNN weights don’t need to be updated after the meta-training process which
is done in software ofﬂine. The CNN consists of four convolutional layers, two max-pooling lay-
ers, and one fully connected layer (Fig. 4a). There are nearly 65,000 weights in convolutional
layers altogether that are represented by 130,000 memristors, with the conductance difference of
two memristors representing one weight value. The weights of convolutional layers are ﬂattened
and concatenated ﬁrst (see Supplementary Fig. 8 and Methods for details) and then programmed
to crossbar arrays with an iterative write-and-verify method. Limited by the available array size,
we divide larger matrices into 64×64 tiles and reprogram the same arrays when needed to accom-
plish all convolutional operations in the crossbar. Experimental conductance maps (36 matrices
of conductance values) for the CNN layers after each programming of an array are shown in Sup-
plementary Fig. 9. The repeated programming of memristor arrays demonstrated good reliability
of the memristor devices within crossbars. After programming the convolutional weights into the
crossbar, the fully connected layer is retrained to adapt to the hardware defects.

Before the few-shot learning, the explicit memory stored in the crossbar-based TCAM is
initialized with all ‘0’s. During the few-shot learning, the feature vectors computed by the memris-
tor CNN next hashed into 128 binary or ternary signatures and searched against the entries in the
crossbar-based TCAM for the closest match, with the methods described before. The label of the
closest match will be the classiﬁcation result. If correct, the nearest neighbor entry will be updated
based on the new input query vector (see Methods). Otherwise, the signature along with the label
is written to a new location in the TCAM using differential encoding. After learning K images
from the support set, The conductance map that is stored in the crossbar-based TCAM is shown in
Supplementary Fig. 10. Note that the CNN controller stays the same for all four few-shot learning

14

Figure 4: End-to-end experimental inference with memristive crossbar arrays. a, Schematic of CNN
structure implemented in the memristor crossbar array. The conductance shows the weight mapping of
CNN kernels. The format of dimension representations in the ﬁgure follows the Output channel (O), Input
channel (I), Height (H), and Width (W). The conductance maps representing the whole CNN kernels are
shown in Supplementary Fig. 9. b, Linear relationship between the sensing current from the crossbar-
based TCAM and the number of mismatch bits during the search operations. c, Classiﬁcation accuracy
with cosine similarity, software-based LSH with 128 bits, and end-to-end experimental results on crossbar
arrays. We provide 5 experimental data points for each task. Software LSH shows experimental variation
due to different initializations of the hashing planes in each experiment. d, e, Simulations of classiﬁcation
accuracy of 5-way 1-shot problem (d) and 25-way 1-shot problem (e) as a function of device ﬂuctuations in
the memristor model for both TLSH and LSH. Fluctuations from nearly zero to 1 µS are shown. The actual
experimental ﬂuctuation level is shown with an arrow.15

abcde32x3x332x28x2832x32x3x332x28x28  max pooling  max pooling64x32x3x364x14x1464x64x3x364x14x143136x1Retrained FC layer02550025500255002550Conductance [μS]Conductance [μS]Conductance [μS]Conductance [μS]64x15-way 1-shot5-way 5-shot25-way 1-shot25-way 5-shotClassification accuracy (%)020408060100Accuracy (%)607080901005-way 1-shot problemTLSH + TCAMLSH + TCAM0.010.11Conductance fluctuation [μS]25-way 1-shot problemAccuracy (%)304050607080TLSH + TCAMLSH + TCAM0.010.11Conductance fluctuation [μS]Our device fluctuationOur device fluctuationCosine similaritySoftware LSHCrossbar TLSH + TCAMStandard deviationMax and Min boundExperimental data point00200246840424446481400120025% - 75%Range within 1.5 IQRMedian lineCurrent [μA]# mismatch bits020002468404244464814001200Current [μA]# mismatch bitstasks, a key feature to support lifelong learning, which is different from the previously reported
few-shot learnings based on high-dimensional computing23. The memory is the only part need to
be updated during lifelong learning. Even for the memory module, the update is not frequent (1.3
times per bit for 20-shot) throughout the learning process, as demonstrated in Supplementary Fig.
11. Considering a common statistical endurance (about 106) for a relative high switching window
reported previously46, 47, we estimate about 4000-years lifetime of a face recognition system that
sees 10 times each face in one day which is much longer than the human life.

Accuracy is evaluated experimentally in classifying new samples after few-shot learning with
four standard tasks: 5-way 1-shot, 5-way 5-shot, 25-way 1-shot, and 25-way 5-shot, respectively.
We found that the experimental sensing currents during few-shot learning experiment are highly
linear with the number of mismatch bits, i.e. hamming distance, as shown in Fig. 4b. This
is partly enabled by the introduced wildcard ‘X’ from our TLSH method, as discussed in detail
in Supplementary Fig. 15 and Supplementary Note 1. The classiﬁcation results shown in Fig.
4c demonstrate that for 5-way problems, our full crossbar hardware-based MANN achieves an
accuracy very close to the software baseline implemented in digital hardware with cosine similarity
as the distance metric. For 25-way problems, we ﬁnd no difference between results from our
analog hardware and that from the digital hardware implementing the same LSH plus Hamming
distance algorithm. Though there exists some accuracy drop compared to the cosine baseline, the
performance can be improved to match the baseline accuracy by increasing the number of hashing
bits from 128 to 512 (see Supplementary Fig. 6). The experimental results demonstrate that the
MANN fully implemented in crossbar arrays can achieve similar accuracy as software for this task.

Device imperfections analysis

The accuracy can be affected by many non-idealities in emerging memory devices, among which
the two most prominent are conductance ﬂuctuations and relaxation. We noted that the conduc-
tance of memristor devices ﬂuctuates up and down (see Supplementary Fig. 3a, b) even within a
very small period (at the scale of nanoseconds). The ﬂuctuation leads to frequent changes in con-
volutional kernels, hyperplane locations in LSH operations, and stored signature values in TCAMs,
which negatively impact the accuracy. The data (shown in Supplementary Fig. 3c) measured from
our integrated array shows that the degree of device ﬂuctuation increases with the conductance
value. This behavior is consistent with previous reports on single device measurement35, 48. In
addition to the conductance ﬂuctuations, the programmed value may also change permanently (re-
laxation) over time, which is characterized in detail in Supplementary Fig. 2c. From these results,
we ﬁnd that conductance relaxation is larger when device conductance is programmed to a certain
range (from around 25 µS to 75 µS). Therefore, in our implementation, we try to avoid this range

16

as much as possible to achieve the software equivalent accuracy. For example, in the LSH part,
we chose lower conductance levels to minimize both conductance ﬂuctuation and relaxation. In
the TCAM part, we chose 0 µS and 150 µS as the low and high conductance levels to minimize the
impact of conductance relaxation. In the CNN part, we observe that most weight values are very
small (near zero), so with the differential encoding method (details described in Methods) we can
guarantee that most memristor conductance values are below the range with higher relaxation.

To analyze our software-competitive accuracy results and evaluate if our method is scalable,
we built an empirical model describing experimental conductance-dependent ﬂuctuation behavior
and deviation after programming. With the experimental calibrated model (see Methods and Sup-
plementary Fig. 3c, d for more details), we can match the simulation results with the experiments
in few-shot learning on Omniglot handwritten images. The detailed comparison is shown in Sup-
plementary Fig. 12a). The simulation also enables us to analyze how different device ﬂuctuations
impact classiﬁcation accuracy. We conducted simulations assuming the device conductance ﬂuc-
tuation spanning from nearly no ﬂuctuation to 1 µS which is about ten times larger than our device
behavior. The results in Fig. 4e, f show that with the experimental ﬂuctuation value, the accuracy
stays almost the same as the software equivalent value, but the accuracy will sharply drop if the
ﬂuctuation is above three-times larger than our experimental value. The results also show that our
proposed TLSH method exhibits better performance compared to the conventional LSH, especially
for more signiﬁcant device ﬂuctuation scenarios (Fig. 4e, f and Supplementary Fig. 13). In addi-
tion to the higher tolerance to the device ﬂuctuation, the comparison shown in Supplementary Fig.
12b, c also demonstrates the TLSH’s advantages in search energy. These simulations, with experi-
mental calibration, elucidate the experimentally observed defect tolerance and software-equivalent
accuracy. Though there exist other defects such as stuck-at-fault, I/V nonlinearities for high re-
sistance states, and device-to-device variation in active conductance range, we found these have
negligible impact on the ﬁnal performance. With this tool, we are able to analyze scaling up to
more complex real-world problems.

Scaled-up MANN for Mini-ImageNet

The methodology of crossbar-based TLSH and TCAM can be applied in many ﬁelds of deep
learning that require the distance calculation and attention mechanism. To show the scalability of
our fully crossbar-based MANN, we conducted simulations based on our experimentally-calibrated
model for one-shot learning using the Mini-ImageNet dataset28. This dataset is derived from the
ImageNet dataset with 100 classes of 600 images of size 84 × 84 color pixels per class. The
task is known to be much more difﬁcult than that of the Omniglot handwritten dataset. A more
sophisticated ResNet-18 model is used as the controller following the state-of-the-art structure in

17

few-shot learning models49, which has more than 11 million weights, 44 times larger than the
controller to classify images from the Omniglot dataset.

A challenge for this network is the required crossbar sizes (512 in one dimension), and
thus the voltage drops along the wire would signiﬁcantly reduce the computing accuracy. This
is solved by partitioning large arrays (for hyperplanes and memories) into smaller 256×256 tiled
crossbars (Fig. 5a) to accommodate the model. In the simulation, we consider the experimental
device ﬂuctuations (see Supplementary Table 2) and use the same threshold current (4 µA) for
the TLSH approach as in the smaller Omniglot problem. The result in Fig. 5b shows that the
classiﬁcation accuracy for the 5-way 1-shot problem increases with the number of hashing bits, and
reaches 58.7% with 4,096 hashing bits, only 1.3% smaller than the model implemented in digital
hardware with cosine similarity as the distance metric. We also explored the performance with
different partitioned array sizes in Supplementary Fig. 14a, b, which achieves nearly equivalent
performance with arrays smaller than or equal to 256×256 and drops slightly with the 512×512
array. Encouragingly, the TLSH function can be implemented with a larger array (512×512)
because of lower conductance and smaller voltage drops along the wires. From these results, we
can see that the performance of our crossbar-based MANN can scale up effectively to at least
Mini-ImageNet problems.

Discussion

Compared to conventional von Neumann based implementations, the key advantage of crossbar-
based MANN is lower latency and higher energy efﬁciency through co-located computing and
memory, energy-efﬁcient analog operations, and intrinsic stochasticity. To evaluate the strength of
the approach, we run the same 5-way 1-shot problem with Omniglot and Mini-ImageNet datasets
on a digital graphic processing unit (GPU) (Nvidia Tesla P100). The time required to classify
a single image increases dramatically after the size of the MANN’s external memory capacity
reaches a certain threshold (only several MB) because of the repeated off-chip data movement
(see Fig. 5c). This problem on conventional hardware has been the major bottleneck prevent-
ing the widespread adoption of few-shot learning. The approach of directly computing in the
In the crossbar-
memory, or crossbar, provides a plausible solution to address this bottleneck.
based MANNs, the matrix multiplication in the convolutional layer, the hashing in TLSH, and the
searching operation in TCAM are all computed with single-step current readout operations. With
our current proof-of-concept experimental system, readouts take about 100 ns, but can be reduced
to 10 ns. We also considered the time latency for the peripherals (2.5 ns) including the full-adders
for tiled crossbar arrays50. With these conservative forecasts, we compared the latency for GPU
and our analog in-memory hardware. The results shown in Fig. 5d indicate a latency improvement

18

(4,540× and 11,280×) when the memory size (number of entries) is 8,192, for both Omniglot and
Mini-ImageNet datasets. Additionally, our approach also offers high energy efﬁciency compared
with the conventional GPU (2,857× and 50,970×). Detailed analysis about energy and latency
estimation can be found in Supplementary Note 2.

In summary, we have, for the ﬁrst time, experimentally implemented a complete MANN
architecture, from the controller to distance calculation, in an analog in-memory platform with
proven high robustness and scalability. We utilize the analog behavior of memristor devices to per-
form convolution operations for CNNs and exploit the inherent stochasticity of devices to perform
hashing functions. A novel hardware-friendly hashing function (TLSH) is developed to provide
better analog computing error tolerance and lower power consumption. In addition, a differential
encoding method for crossbar-based TCAM is applied to adapt to the ternary Hamming distance
calculations requirements. In our experiment, all dot-product operations are performed in physi-
cal crossbars, which exhibit experimental imperfections, such as device state ﬂuctuations, device
nonlinearities, voltage drops due to wire resistance, and peripheral circuits. The fully hardware-
implemented MANNs delivered similar accuracy compared to software on few-shot learning with
the widely used the Omniglot dataset. Simulation results on Mini-ImageNet show the ability of
crossbar-based MANN to execute real-world tasks, with much-improved latency and energy con-
sumption. We demonstrate that analog in-memory computing with memristive crossbars efﬁciently
supports many different tasks, including convolution, hashing, and content-based searching. The
successful demonstration of these functions opens possibilities with other machine learning algo-
rithms, such as attention-based algorithms, or reaching scales that are currently prohibited by con-
ventional hardware (e.g, Fig. 5c). Additionally, there are many opportunities for future software-
hardware co-optimization to improve the accuracy and efﬁciency results further.

19

Figure 5: Experiment validated simulation results of Mini-ImageNet dataset. a, The architecture of
TLSH and TCAM for the scaled-up MANN. Both matrices for hashing and external memory are partitioned
into tiled memristor crossbar arrays (H×W) to mitigate the voltage drop problem in large crossbars and
to increase the utilization rate. b, The accuracy performance from our experiment-validated models on
Mini-ImageNet dataset. The error bar shows the 95% conﬁdence interval among 100 repeated inference
experiments. c, The execution time of search operations per inference on a GPU drastically increases when
external memory size reaches a threshold, conﬁrming the operation is memory intensive. d, The comparison
of the search latency and energy consumption for 5-way 1-shot learning on both the Omniglot and Mini-
ImageNet datasets. For GPU, the models for both datasets stores the same number of entries (8,192), but
Mini-ImageNet uses a larger memory capacity due to higher dimension (64 vs. 512) of feature vectors,
leading to even better improvement on latency and energy efﬁciency. The number of hashing bits used in
crossbar arrays is 128 and 4096 for Omniglot and Mini-ImageNet, respectively.

20

abcd2628210212214216218220222Memory sizeExecution time per search [ms]10-11001014MB4MBMini-ImageNetOmniglotRow muxTIA, S&H, ADC(1)(1)(2)(2)(3)(256)(256)DifferentialCodingHamming distanceLabelRegisters+AddRegisters+AddFeature vectorsHashing matrixMemory matrixHHWW10000102030405060Accuracy (%)# of hashing bits Execution time per search [s]Energy comsumption per search [J]10-810-710-610-510-410-310-1210-1110-1010-910-810-710-610-5OmniglotMini-ImageNetGPUCrossbarMemory size = 81924.5x1032.9x1031.1x1045.1x104Methods

Memristor integration The memristors are monolithically integrated on CMOS fabricated in a
commercial foundry in a 180 nm technology node. The integration starts with the removal of
native oxide on the surface metal with reactive ion etching (RIE) and a buffered oxide etch (BOE)
dip. Chromium and platinum are then sputtered and patterned with e-beam lithography as the
bottom electrode, followed by reactively sputtered 2 nm tantalum oxide as the switching layer and
sputtered tantalum metal as the top electrode. The device stack is ﬁnalized by sputtered platinum
for passivation and improved electrical conduction.

Iterative write-and-verify programming method In this work, we use the iterative write-and-
verify method to program memristor devices to the target conductance value. First, we set a target
conductance matrix and the corresponding tolerant programming error range. After that, succes-
sive SET and RESET pulses are applied to the target devices followed by conductance readout with
READ pulses. If the device conductance is below the target conductance minus the tolerant error,
a SET pulse is applied. A RESET pulse is applied for conductance above the tolerant values, while
the device has been programmed within the tolerant values are skipped to pertain the state. For the
crossbar-based MANN in this work, we apply the write-and-verify method to map the weights of
the CNN controller and memories in the TCAM structure. During the programming process, we
gradually increase the programming voltage and gate voltage as shown in Supplementary Table 1.
The pulse width for both the SET and RESET process is 1 µs. The tolerant range we set is 5 µS
above or below the target conductance value.

Adjacent Column Method We apply the Adjacent Column Method (ACM)40 to map the conduc-
tance of memristors in crossbar arrays to weights in hashing planes. ACM subtracts the neighbor-
ing columns as shown in Fig. 2b to generate the hash codes. Hence, for a crossbar array with N+1
columns, the output of differential encoding contains N values which immensely saves the area.
The mathematical representation is as follows: Provided that we get a random conductance map
after programming which is:

Gmap =









G1,1 G1,2
G2,1 G2,2
...
...
GN,1 GN,2









· · · G1,N
· · · G2,N
...
. . .
· · · GN,N

(2)

21

then the ACM method is equivalent to multiplying Gmap by a transformation matrix:

Ghash = Gmap ×














· · ·
0
0
1
· · ·
−1
0
1
0 −1 · · ·
0
...
...
...
. . .
· · · −1
0
0
· · ·
0
0

0
0
0
...
1
0 −1






















=

G1,1 − G1,2 G1,2 − G1,3
G2,1 − G2,2 G2,2 − G2,3

...

...

GN,1 − GN,2 GN,2 − GN,3

· · · G1,N−1 − G1,N
· · · G2,N−1 − G2,N
. . .
· · · GN,N−1 − GN,N

...









(3)

Memristor model and simulations We build a memristor model to simulate the conductance
ﬂuctuation, which is the most dominant non-ideality of our crossbar-based MANN. The behavior
of conductance ﬂuctuation is assumed to be a Gaussian nature which is as follows:

G = G0 + σ · N (0, 1)

(4)

where G0 is the conductance after programming, σ describes the standard deviation of the ﬂuctu-
ation range. In the simulation we assume that the device only ﬂuctuates for different VMM pro-
cesses since in the real experiment the execution time of one VMM is very small (10 ns) which is
negligible compared to time between succesive input vectors (1 µs). After considering the device-
to-device variations and ﬁtting the parameters (see Supplementary Table 2) to experimental mea-
surements (see Supplementary Fig. 3), Equation 4 becomes:

G = G0 + exp(a · ln(G0) + b + s · N (0, 1)) · N (0, 1)

(5)

with N (µ, σ 2) being the normal distribution with mean µ and standard deviation σ . In the simu-
lation, we also consider the program error to the initial conductance G0 which is shown as:

G0 = Gt + N (0,

˜G2

err)

where Gt is the target conductance we want to program to and ˜Gerr is the program error which we
set to 5 µS in the simulation.

To get the parameters of the memristor model in terms of the effect of device ﬂuctuation, we
SET 4,096 devices to 16 distinct analog states and READ each device for 1,000 times. The relation
between the mean value and standard deviation of 1,000 reads is shown in Supplementary Fig. 3a
and 3b. We further analyze the standard deviation distribution for each conductance state from
5 µS to 50 µS, plot the distributions in logarithmic scale, and ﬁt them with Gaussian distribution.
The results are shown in Supplementary Fig. 3c. The mean value of s for each distribution gives us

22

the parameter for the model. In addition, we ﬁt a linear curve with conductance states and standard
deviation in a log-log regime of measurements (see Supplementary Fig. 3d). The ﬁtted parameters
a and b are used in the simulation.

Ternary locality sensitive hashing Ternary locality-sensitive hashing introduces a wildcard ’X’
to the hashing vector to alleviate the analog computing error from nonideal factors. We have
demonstrated that this modiﬁed hashing scheme can achieve software-equivalent performance
(LSH with the same hashing bits) on our crossbar arrays. The threshold current Ith applied in
the experiment should be carefully chosen according to the typical value of the computing error
caused by device ﬂuctuation. The value we chose throughout the experiment is 4 µA. We also
show the dependence of classiﬁcation accuracy on different threshold currents in Supplementary
Fig. 7.

For the simulation results in Fig. 4d and 4e, where the device ﬂuctuation varies, we chose
different threshold currents Ith according to the ﬂuctuation levels. Speciﬁcally, for our memristor
model which can be described by Equation 4, we empirically set the threshold current to be 5σ ·Vin
where Vin is the maximum input voltage to the row line when performing VMM. The Vin is chosen
to be 0.2 V in our experiment.

To generate random hashing planes in crossbar arrays (Fig. 2c), we RESET the devices from
an arbitrary high conductance state to near 0 µS, where the conductance is ultimately decided by the
intrinsic stochastic switching process. Regardless of the initial states, we use 5 RESET pulses with
an amplitude of 1.5 V and a width of 20 ns. The RESET voltage is carefully controlled to protect
memristor devices because larger voltages may cause devices to be stuck at low conductance states.

CNN architecture The convolutional neural network (CNN)s in crossbars is applied as the con-
troller in the MANN to extract features from incoming images.

The CNN structure for the Omniglot dataset is composed of:

• 2 convolutional layers, each with 32 channels of shape 3x3

• A 2x2 max-pooling layer

• 2 convolutional layers, each with 64 channels of shape 3x3

• A 2x2 max-pooling layer

• A fully connected layer with 64 outputs

23

Each convolutional layer is followed by a rectiﬁed linear unit (ReLU) activation layer.

The ResNet-18 for the Mini-ImageNet dataset is composed of 8 residual blocks. Each resid-
ual block has two 3x3 convolutional layers with size [layer channel, input channel, 3, 3] and
[layer channel, layer channel, 3, 3] respectively. Each convolutional layer is followed by a batch
normalization layer and a ReLU layer. The overall architecture for ResNet-18 is:

• 1 convolutional layer with 64 channels of shape 3x3

• 2 residual blocks with 64 channels and stride 1

• 2 residual blocks with 128 channels and stride 2

• 2 residual blocks with 256 channels and stride 2

• 2 residual blocks with 512 channels and stride 2

• 1x1 adaptive average pooling layer

We map the weights of convolitional layers in the CNN to the conductance of memristor
devices using differential encoding31. To elaborate, in a differential column pair, we program the
positive weight to the left column and the absolute value of negative weights to the right column,
while keeping the other at the low conductance state. The weight-to-conductance ratio we set in
our experiment is 1:50 µS. The feature maps collected from the output current in Fig. 4a are
converted into voltages and then sent to another crossbar array corresponding to the subsequent
convolutional layer. The fully connected layer is retrained after mapping convolutional layers on
crossbar arrays and it’s computed in the digital domain.

Memory update rules In an N-way K-shot learning, the memory module is updated based on the
N × K images in the support set. If the label of the new input image label doesn’t match the label of
the nearest neighbor entry, we simply ﬁnd a new location in the memory and write the input image
to that location. Conversely, if the input image label matches, we need to update the memory of
the nearest neighbor. In the GPU, we assign cosine distance as the metric to identify the label of
input images and update the real-valued vectors at the same location33. However, in this work,
we use ternary Hamming distance as our metric, and we apply the following rules to update the
ternary vectors in the external memory: We introduce a scoring vector to evaluate the majority of
’1’ and ’0’ for each bit of each memory vector. An element-wise mapping function is applied to
each ternary vector stored in the memory module:

f : {1, 0, X}D → {1, −1, 0}D

24

where D is the dimension of storing vectors. For example, vector (1, 0, 1, X, 0) is mapped to
(1, −1, 1, 0, −1). We assume the scoring vector for each storing vector as: si = f (ai), i = 1, 2, 3, . . . , M,
where si is the scoring vector, ai is the hashing vector stored in TCAM and M is the total number
of memory entries. When there is a match case happening at memory location k, we ﬁrst update
its scoring vector as below:

s∗
k = sk ⊕ f (v)

(6)

where s∗
add operation. Then we update the memory at the same location using the following rules:

k is the new scoring vector, v is the hashing vector of the new image, ⊕ is element-wise

a∗
i = L(ai); L(x) =






x > 0
1
X x = 0
x < 0
0

(7)

where a∗
i is the updated memory and L is an element-wise operator. Therefore, bits of the memory
stored in TCAM are decided by the majority of ’1’ and ’0’ of incoming vectors which match the
storing vectors.

Omniglot training The Omniglot data set contains 1623 different handwritten characters from
50 different alphabets. Each character was drawn online by 20 different people using Amazon’s
Mechanical Turk. In the experiment, we augment the 964 different characters in the training set
to 3856 through rotation. The character types in the test set remain unchanged at 659. There is
no overlap between the training set and the test set. We use the episode training method during
the training process. Episode training is to select N x M instances from the training set during
each training, where N represents different classes and M represents the number of instances in
each class. The purpose of episode training is to enable the learned model to focus on the common
parts, ignoring tasks, so as to achieve the purpose of learning to learn. The speciﬁc settings in the
training process are as follows: memory size is 2048, batch size is 16, episode width is 5, episode
length is 30; The length of the output key is 128, and the validation set is used for veriﬁcation
every 20 times.

Mini-ImageNet training The Mini-ImageNet dataset contains 100 classes randomly chosen from
the ImageNet dataset. We randomly split the dataset into a training set, a validation set, and a test
set containing 64, 16, and 20 classes, respectively. We take the pre-trained model in ref49 and
ﬁne-tuned it using cosine distance as the meta-training metric. Once the meta-training process is
done, the weights for the controller won’t be updated. We use the ResNet-18 model as the CNN
controller and the output feature vector of the CNN is 512-dimensional.

25

Data availability

The data supporting plots within this paper and other ﬁndings of this study are available with
reasonable requests made to the corresponding author.

Code availability

The code used to generate the results of this study is available with reasonable requests made to
the corresponding author.

26

References

1. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D. & Lillicrap, T. Meta-learning with
memory-augmented neural networks. In Balcan, M. F. & Weinberger, K. Q. (eds.) Proceedings
of The 33rd International Conference on Machine Learning, vol. 48 of Proceedings of Machine
Learning Research, 1842–1850 (PMLR, New York, New York, USA, 2016). URL http:
//proceedings.mlr.press/v48/santoro16.html.

2. Stevens, J. R., Ranjan, A., Das, D., Kaul, B. & Raghunathan, A. Manna: An accelerator for
memory-augmented neural networks. In Proceedings of the 52nd Annual IEEE/ACM Interna-
tional Symposium on Microarchitecture, 794–806 (2019).

3. Rae, J. W. et al. Scaling memory-augmented neural networks with sparse reads and writes

(2016). 1610.09027.

4. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning
in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin-
guistics, 3645–3650 (Association for Computational Linguistics, Florence, Italy, 2019). URL
https://www.aclweb.org/anthology/P19-1355.

5. Li, D., Chen, X., Becchi, M. & Zong, Z. Evaluating the energy efﬁciency of deep convolutional
neural networks on cpus and gpus. In 2016 IEEE international conferences on big data and
cloud computing (BDCloud), social computing and networking (SocialCom), sustainable com-
puting and communications (SustainCom)(BDCloud-SocialCom-SustainCom), 477–484 (IEEE,
2016).

6. Ranjan, A. et al. X-MANN: A Crossbar based Architecture for Memory Augmented Neural

Networks 1–6 (2019).

7. Von Neumann, J. First draft of a report on the edvac. IEEE Annals of the History of Computing

15, 27–75 (1993).

8. Chua, L. Memristor-the missing circuit element. IEEE Transactions on circuit theory 18, 507–

519 (1971).

9. Xia, Q. & Yang, J. J. Memristive crossbar arrays for brain-inspired computing. Nature materials

18, 309–323 (2019).

10. Li, C. et al. Long short-term memory networks in memristor crossbar arrays. Nature Machine

Intelligence 1, 49–57 (2019).

27

11. Wang, Z. et al. In situ training of feed-forward and recurrent convolutional memristor net-

works. Nature Machine Intelligence 1, 434–442 (2019).

12. Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on

metal-oxide memristors. Nature 521, 61–64 (2015). 1412.0611.

13. Ambrogio, S. et al. Equivalent-accuracy accelerated neural-network training using ana-
logue memory. Nature 558, 60–67 (2018). URL http://dx.doi.org/10.1038/
s41586-018-0180-5.

14. Chen, W. H. et al. CMOS-integrated memristive non-volatile computing-in-memory for AI
edge processors. Nature Electronics 2, 420–428 (2019). URL http://dx.doi.org/10.
1038/s41928-019-0288-0.

15. Li, C. et al. Analogue signal and image processing with large memristor crossbars. Nature

electronics 1, 52–59 (2018).

16. Sheridan, P. M. et al. Sparse coding with memristor networks. Nature Nanotechnology 12,

784–789 (2017).

17. Le Gallo, M. et al. Mixed-precision in-memory computing. Nature Electronics 1, 246–253

(2018).

18. Zidan, M. A. et al. A general memristor-based partial differential equation solver. Nature

Electronics 1, 411–420 (2018).

19. Yang, K. et al. Transiently chaotic simulated annealing based on intrinsic nonlinearity of

memristors for efﬁcient solution of optimization problems. Science Advances 6 (2020).

20. Cai, F. et al. Power-efﬁcient combinatorial optimization using intrinsic noise in memristor
Hopﬁeld neural networks. Nature Electronics 3, 409–418 (2020). URL http://dx.doi.
org/10.1038/s41928-020-0436-6.

21. Hu, M. et al. Dot-product engine for neuromorphic computing: Programming 1t1m crossbar
to accelerate matrix-vector multiplication. In 2016 53nd ACM/EDAC/IEEE Design Automation
Conference (DAC), 1–6 (IEEE, 2016).

22. Karunaratne, G. et al. In-memory hyperdimensional computing. Nature Electronics 3 (2020).

URL http://dx.doi.org/10.1038/s41928-020-0410-3. 1906.01548.

28

23. Karunaratne, G. et al. Robust high-dimensional memory-augmented neural networks. Nature

communications 12, 1–12 (2021).

24. Ni, K. et al. Ferroelectric ternary content-addressable memory for one-shot learning. Nature

Electronics 2, 521–529 (2019).

25. Laguna, A. F., Yin, X., Reis, D., Niemier, M. & Hu, X. S. Ferroelectric FET based in-memory
computing for few-shot learning. Proceedings of the ACM Great Lakes Symposium on VLSI,
GLSVLSI 373–378 (2019).

26. Li, H. et al. Sapiens: A 64-kb rram-based non-volatile associative memory for one-shot learn-

ing and inference at the edge. IEEE Transactions on Electron Devices (2021).

27. Lake, B., Salakhutdinov, R., Gross, J. & Tenenbaum, J. One shot learning of simple visual
concepts. In Proceedings of the annual meeting of the cognitive science society, vol. 33 (2011).

28. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K. & Wierstra, D. Matching networks

for one shot learning. arXiv preprint arXiv:1606.04080 (2016).

29. Pi, S. et al. Memristor crossbar arrays with 6-nm half-pitch and 2-nm critical dimension.

Nature nanotechnology 14, 35–39 (2019).

30. Cai, F. et al. A fully integrated reprogrammable memristor–cmos system for efﬁcient

multiply–accumulate operations. Nature Electronics 2, 290–299 (2019).

31. Yao, P. et al. Fully hardware-implemented memristor convolutional neural network. Nature

577, 641–646 (2020).

32. Shaﬁee, A. et al. Isaac: A convolutional neural network accelerator with in-situ analog arith-

metic in crossbars. ACM SIGARCH Computer Architecture News 44, 14–26 (2016).

33. Kaiser, Ł., Nachum, O., Roy, A. & Bengio, S. Learning to remember rare events. arXiv

preprint arXiv:1703.03129 (2017).

34. Datar, M., Immorlica, N., Indyk, P. & Mirrokni, V. S. Locality-sensitive hashing scheme based
on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational
geometry, 253–262 (2004).

35. Sheng, X. et al. Low-conductance and multilevel cmos-integrated nanoscale oxide memristors.

Advanced Electronic Materials 5, 1800876 (2019).

29

36. Li, C. et al. CMOS-integrated nanoscale memristive crossbars for CNN and optimization
acceleration. 2020 IEEE International Memory Workshop, IMW 2020 - Proceedings 2, 1–4
(2020).

37. Gionis, A., Indyk, P., Motwani, R. et al. Similarity search in high dimensions via hashing. In

Vldb, vol. 99, 518–529 (1999).

38. Shinde, R., Goel, A., Gupta, P. & Dutta, D. Similarity search and locality sensitive hash-
ing using ternary content addressable memories. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data, 375–386 (2010).

39. Huang, Q., Feng, J., Zhang, Y., Fang, Q. & Ng, W. Query-aware locality-sensitive hashing for
approximate nearest neighbor search. Proceedings of the VLDB Endowment 9, 1–12 (2015).

40. Kazemi, A. et al. A device non-ideality resilient approach for mapping neural networks to
crossbar arrays. In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1–6 (IEEE,
2020).

41. Niu, G. et al. Geometric conductive ﬁlament conﬁnement by nanotips for resistive switching

of hfo 2-rram devices with high performance. Scientiﬁc reports 6, 1–9 (2016).

42. Li, J., Montoye, R. K., Ishii, M. & Chang, L. 1 mb 0.41 µm2 2t-2r cell nonvolatile tcam with
two-bit encoding and clocked self-referenced sensing. IEEE Journal of Solid-State Circuits 49,
896–907 (2013).

43. Lin, C.-C. et al. 7.4 a 256b-wordlength reram-based tcam with 1ns search-time and 14×
improvement in wordlength-energyefﬁciency-density product using 2.5 t1r cell. In 2016 IEEE
International Solid-State Circuits Conference (ISSCC), 136–137 (IEEE, 2016).

44. Fedorov, V. V., Abusultan, M. & Khatri, S. P. An area-efﬁcient ternary cam design using
In 2014 IEEE 32nd International Conference on Computer Design

ﬂoating gate transistors.
(ICCD), 55–60 (IEEE, 2014).

45. Apalkov, D., Dieny, B. & Slaughter, J. Magnetoresistive random access memory. Proceedings

of the IEEE 104, 1796–1830 (2016).

46. Zhao, M. et al. Characterizing endurance degradation of incremental switching in analog rram
In 2018 IEEE International Electron Devices Meeting (IEDM),

for neuromorphic systems.
20–2 (IEEE, 2018).

30

47. Zhao, M. et al. Endurance and retention degradation of intermediate levels in ﬁlamentary

analog rram. IEEE Journal of the Electron Devices Society 7, 1239–1247 (2019).

48. Ambrogio, S. et al. Statistical ﬂuctuations in hfo x resistive-switching memory: part i-set/reset

variability. IEEE Transactions on electron devices 61, 2912–2919 (2014).

49. Wang, Y., Chao, W.-L., Weinberger, K. Q. & van der Maaten, L. Simpleshot: Revisiting
nearest-neighbor classiﬁcation for few-shot learning. arXiv preprint arXiv:1911.04623 (2019).

50. Ghadiry, M., Nadi, M. & A’Ain, A. K. Dlpa: Discrepant low pdp 8-bit adder. Circuits,

Systems, and Signal Processing 32, 1–14 (2013).

Acknowledgement

Author contribution C.L., C.G., J.P.S contributed to the conception of the idea. R.M. performed the
experiments and analyzed data under the supervision of C.L.. R.M., Y.Z. and A.K. performed simulations.
X.S. integrated the memristors. R.M., Y.H, and C.L. wrote the manuscript with input from all authors.

Competing Interests The authors declare that they have no competing interests.

Correspondence Correspondence and requests for materials should be addressed to canl@hku.hk

31

Supplemental Information

Experimentally realized memristive memory augmented
neural network

Ruibin Mao1, Bo Wen1, Yahui Zhao1, Arman Kazemi2,3, Ann Franchesca Laguna3, Michael
Neimier3, X. Sharon Hu3, Xia Sheng2, Catherine E. Graves2,*, John Paul Strachan4,5,*, and Can
Li1,*

1Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong
SAR, China
2Hewlett Packard Labs, Hewlett Packard Enterprise, Milpitas, CA, USA
3Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN,
USA
4Peter Gr¨unberg Institut (PGI-14), Forschungszentrum J¨ulich GmbH, J¨ulich, Germany
5RWTH Aachen University, Aachen, Germany
*canl@hku.hk, j.strachan@fz-juelich.de, catherine.graves@hpe.com

Contents

1 Supplementary Figures

2 Supplementary Tables

3 Supplementary Notes

4 Supplementary References

2

15

16

22

1

1 Supplementary Figures

Supplementary Figure 1: CMOS-integrated crossbar test platform and experimental setup. a, Circuit
schematic of a crossbar for matrix multiplication with peripheral circuits. It consists of a 1T1R crossbar, row
muxes, column muxes, transimpedence ampliﬁers (TIA), sample and hold (S&H) and ADC. b, Picture of
a wire-bonded integrated memristor chip. It constains three 64×64 1T1R crossbar arrays. c, Measurement
board with the integrated memristor chip that connects to a general purpose computer through a microcon-
troller (MCU).

Supplementary Figure 2: Memristor conductance relaxation statistics in a crossbar array. a, Cumula-
tive distribution function on 16 distinct programmed conductance states at different time period. We run the
test on a 64x64 array where we divide it into 4x4 blocks and program them to 16 conductance states with
iterative write-and-verify programming scheme. b, Relation between mean conductance at different time
periods and target conductance. c, Relation between the standard deviation at different time periods and the
target conductance.

2

(a)(b)(c)Input vector: voltages (Vi)ADC (shared across 16 columns)16:1 muxTIAS&HMatrix values1T1M cellConductance (Gij)Output vector: currents (Ij = ∑ Gij • Vi)MCUAnalogreferencesAnalog test probesMemristor ChipDigital test probesPower00252550507575100100125125150150Gtarget [μS]Gmean [μS]0255075100125150Gtarget [μS]Gstd [μS]04812162000.00.20.40.60.81.020406080100120140160Gtarget [μS]CDFabcSupplementary Figure 3: Memristor conductance ﬂuctuations in a crossbar array. a, Relation between
the standard deviation of 1000 readout conductance and the mean value at different conductance states. We
test the whole 64x64 array. b, Boxplot of the standard deviation with respect to different conductance states.
c, Distribution of logarithm of standard deviation at different conductance levels. The distributions show
that the device-to-device variation in terms of conductance ﬂuctuation exhibits a lognormal distribution. d,
Mean value of the logarithm of standard deviation exhibits a linear dependence on the conductance state at
low conductance range (< 50 µS) and tends to maintain within a certain range at higher conductance range.
The coefﬁcient of the variation brought by the read ﬂuctuation decreases as the conductance increases.

3

0024681020304050607080Mean conductance [μS]s.d. of readout conductance [μS]0051015201020304050607080Mean conductance [μS]s.d. of readout conductance [μS]acdbGmean ~ 5μSGmean ~ 30μSGmean ~ 35μSGmean ~ 40μSGmean ~ 45μSGmean ~ 50μSGmean ~ 10μSGmean ~ 15μSGmean ~ 20μSGmean ~ 25μS-220-220-220-220-2200.00.20.40.00.20.4log(σ)PDF25% - 75%Range within 1.5 IQRMedian lineMean conductance [μS]Standard deviation of conductance [μS]Coefficient of variation (σ/μ)Supplementary Figure 4: Number of unstable bits using hardware TLSH and LSH. Ideally, we expect
no unstable bits when performing hardware LSH so that it’s equal to software LSH. However, due to many
device nonidealities, there will be many unstable bits mainly caused by the conductance ﬂuctuation. In this
work, we apply TLSH to solve this problem. Here, we use the same set of vectors in the main text and repeat
the hashing process 100 times with the same conductance map of size 64 × 129. We then count the number
of unstable bits 100 times for both TLSH and LSH. The result shows that by applying TLSH we can reduce
the number of unstable bits, thus increasing the robustness of our hardware in terms of hashing process.

4

1020304050020406080# of unstable bitscounthardware LSHhardware TLSHSupplementary Figure 5: 3-bit encoding method for TCAM using crossbar array. a, TCAM storage and
input encoding method of 3-bit encoding. b, The schematic of experimental set-up for THD calculation by
using dot product. Three bits are stored in crossbar array using 3-bit encoding. Search bits are successively
applied to the row line and the output current from column line exhibits the THD between input and memory.
c, Eight binary vectors stored in crossbar as memory after 3-bit encoding. d, 100 different ternary vectors
sent to TCAM to perform THD calculation in a parallel way. e, Distribution of Gon and Goff. f, Ouput current
exhibits a linear dependence on the THD. IQR, interquartile range. g, Current distributions are separated
from each other through which we can obtain the number of mismatch bits, i.e. , THD.

5

ab0   V   0X  0  1Input bit stream000V*(Gon-Goff)V*(Gon-Goff)0000cdgfe00110101011X1010111101101110010101100X0110001111Search stream(cid:22220)100 inputs0-110-111-101-100-111-100-111-100-111-101-100001-100-111-100-111-101-101-101-100-111-101-100-111-101-101-100-110-111-100-111-100-111-101-100-110-110000-111-101-100-110-110-111-101-101-101-10Encoded input(cid:22220)100 inputsoutput current*V0   0   V0  -V  -V00000001000000110000011100001111000111110011111101111111111111111 2 3 4 5 6 7 8Conductance [µS]programProgrammed TCAMTCAM storage369151218212412345678StateMemoryCrossbar memory1(0, 0, 1)0(1, 0, 0)X(0, 0, 0)(Goff, Goff,Gon)(Gon, Goff,Goff)(Goff, Goff, Goff)StateInputCrossbarinput1(1, -1, 0)0(0, -1, 1)X(0, 0, 0)(V, -V, 0)(0, -V, V)(0, 0, 0)GonGoffConductance [μS]Count02040608010012014016001020304050Ternary hamming distanceCurrent [µA]0123456782520507510012515017520025% - 75%Range within 1.5 IQRMedian lineMean255075100125150175200001020304050607080Current [µA]Count   1 bit mismatch   2 bits mismatchmatch‘1’‘0’‘X’GoffGonSupplementary Figure 6: Robustness of hardware TLSH on few-shot learning. a, b, c, d, To demon-
strate the robustness of hardware TLSH which exploits the intrinsic stochasticity of memristors, we ran-
domly choose 20 different conductance maps to perform the hashing process. The ﬁgure shows the classi-
ﬁcation accuracy with hardware TLSH and software LSH based on the experimentally generated hashing
codes. The result exhibits the robustness of our memristor crossbar array served as hashing vector generator
whose behavior is nearly the same as software LSH. Furthermore, we can obtain the same accuracy as cosine
similarity on few-shot learning tasks if we scale the hashing bits up to 512 which is feasible in the prevailing
crossbar architectures.

6

hardware TLSHsoftware LSHcosine similarityhardware TLSHsoftware LSHcosine similarityhardware TLSHsoftware LSHcosine similarityhardware TLSHsoftware LSHcosine similarityacdb5-way 1-shot8163264128256512# of hashing bits5060708090100Accuracy (%)cosine similaritycosine similaritysoftware LSHsoftware LSHhardware cosine similaritycosine similaritysoftware LSHsoftware LSHhardware TLSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSHsoftware LSH98.9581632641282565125-way 5-shot# of hashing bits5060708090100Accuracy (%)cosine similaritysoftware LSHsoftware LSHhardware hardware cosine similaritysoftware LSHsoftware LSHhardware TLSHhardware TLSH98.678163264128256512# of hashing bits25-way 1-shot20406080100Accuracy (%)cosine similaritysoftware LSHsoftware LSHhardware cosine similaritysoftware LSHsoftware LSHhardware TLSH85.478163264128256512# of hashing bits25-way 5-shot20406080100Accuracy (%)cosine similaritysoftware LSHsoftware LSHhardware cosine similaritysoftware LSHsoftware LSHhardware TLSH94.40Supplementary Figure 7: Impact of the threshold current for hardware TLSH on the classiﬁcation
accuracy. a, b, As expected, the inﬂuence of the threshold current for hardware TLSH should be positive
ﬁrst and negative afterward. Because introducing the wildcard ’X’ in LSH can mitigate the defect of device
nonidealities but can also cast away the information of original input vectors. Here, we experimentally
demonstrate the classiﬁcation accuracy of 25-way problems with different threshold currents. The result
shows that for our system, the performance reaches a peak with a threshold current around 0.4 µA. The zero
thresholds current case denotes hardware LSH. Each task is repeated 100 times.

7

0.000.250.500.751.001.251.501.752.007273747576777879Threhold current for TLSH [μA]Accuracy (%)25-way 1-shot problemhardware LSHhardware TLSH0.000.250.500.751.001.251.501.752.00788082848688Threhold current for TLSH [μA]Accuracy (%)25-way 5-shot problemhardware LSHhardware TLSHabSupplementary Figure 8: Convolutional layers mapped to crossbar arrays a, A 3 × 3 convolutional
kernel is ﬂattened ﬁrst and mapped to conductance using differential pair. b, N ﬁlters with each containing M
channels are concatenated together and mapped to crossbar arrays. Different ﬁlters perform the convolution
in a parallel way.

8

flattenbiasdifferential pairnormalizationconductance [μS]3X3 filter3 x 3M channels9 x M rows3 x 3M channels9 x M rows3 x 3M channels9 x M rowsN filtersConcatenateN columnsabbias9
Supplementary Figure 9: Experimental conductance map of convolutional layers

1st layer 32x1x3x32nd layer 32x32x3x33rd layer 64x32x3x34th layer 64x64x3x3conductance [μS]Supplementary Figure 10: Memory stored in the crossbar-based TCAM after TLSH operation and
binary update process

10

5-way 1-shot5-way 5-shot25-way 1-shot25-way 5-shotConductance [μS]5 keys5 keys25 keys39 keys128 dimensionsSupplementary Figure 11: Update times of the memory stored in crossbar-based TCAM of multi-shot
learning. The majority of the bits are updated only once which is when writing the newly input vector into
the new location of the memory. Unlike the conventional memory update method which need to update
every value in the real-valued vectors, our proposed binary update method only updates very few bits in the
memory which is suitable for the life-long learning given the endurance of memristors. a, Statistical view
of the update times of bits during standard 5-shot learning. b, To analyze the life-long learning properties,
we perform the multi-shot learning on the 25-way task from 1-shot to 20-shot. It shows that nearly 80% of
the bits stored don’t need to be updated once written to store the new input vector. There are only smaller
than 5% of bits that need to be updated higher than 3 times throughout the 20-shot learning.

11

73.6%10.2%10.6%3.6%2.0%86.4%8.6%3.9%0.8%0.3%12345# of updatesProportion00.20.40.60.8(a)(b)Supplementary Figure 12: Simulation results on Omniglot dataset accord with experimental results
a, Classiﬁcation accuracy of derived memristor device model, along with cosine similarity, software-based
LSH with 128 bits, and end-to-end experimental results on crossbar arrays. b, Comparison over LSH and
TLSH with both simulation and experiment on few shot learning tasks. c, Average energy consumption of
each TCAM search operation with TLSH and LSH, respectively.

12

bac5-way 1-shot5-way 5-shot25-way 1-shot25-way 5-shotEnergy [pJ]TLSH+TCAMLSH+TCAM2.02.22.42.62.83.03.23.45-way 1-shot5-way 5-shot25-way 1-shot25-way 5-shotClassification accuracy (%)60657075808590951005-way 1-shot5-way 5-shot25-way 1-shot25-way 5-shotClassification accuracy (%)6065707580859095100Supplementary Figure 13: Impact of device variation on classiﬁcation accuracy. a, b The experimental
device-to-device variation is shown in Supplementary Fig. 3c. In simulation, we increase the variation to
explore the inﬂuence of it on classiﬁcation accuracy. The results show that for both 5-way 1-shot and 25-way
1-shot problem, the classiﬁcation accuracy drops by 10% when conductance variation is about 50% larger
than the experiemntal data. The value of conductance variation is the value of parameter s in Supplementary
Table 2

13

abSupplementary Figure 14: Performance of scaled-up MANNs with different array size a, TLSH per-
formance with different array size. The sneak path problem has no inﬂuence on the TLSH part because most
of the devices are at the OFF state and the hyperplanes are random. b, Classiﬁcation accuracy with different
array size. The performance degradation is found with a bigger array size and a small number of hashing
bits. This is because the sneak path problem affects the performance of TCAM, resulting in miscalculating
of Hamming distances. However, the accuracy remains nearly the same for different partitioned array sizes
when the number of bits is large.

14

10010001000# of hashing bitsLinear correlation coefficient# of hashing bits0.650.700.750.800.850.900.955565604535304050Accuracy (%)ab2 Supplementary Tables

Supplementary Table 1: Iterative write-and-verify method

Program parameter
SET voltage
RESET voltage
SET gate voltage
RESET gate voltage

V1/V V2/V ∆V/V
2.5
1.0
3.5
0.5
2.0
1.0
5.5
5.0

0.1
0.05
0.1
0.1

Supplementary Table 2: Derived values of memristor model parameters

Parameter Description
G0
a
b
s

Initial conductance after program -
Linear ﬁtting parameter
Linear ﬁtting parameter
Device-to-device variation

Value

0.782
-2.168
0.983

15

3 Supplementary Notes

Supplementary Note 1. Sensing margin in crossbar-based TCAM

The sensing margin between the match and mismatch cases in conventional TCAM measures
the reliability of the hardware under extreme conditions. For TCAM circuits based on emerging
memory devices, a higher sensing margin also provides better tolerance to device variation and
thus a smaller bit error rate.

In this work, our crossbar-based TCAM does not only distinguish between match and mis-
match cases but also returns the number of mismatches. For few-shot learning with MANN, what
is required is distinguishing between the closest match and the next close match. Here we describe
the sensing margin β for our crossbar-based TCAM in detecting the closest match in Equation 1.

β =

Im+1 − Im
Im

=

Im+1
Im

− 1

(1)

where Im is the largest possible current (or voltage after the sensing ampliﬁer) of the closest match,
and Im+1 is the smallest possible current (or voltage after the sensing ampliﬁer) of the next closest
match. A higher sensing margin β indicates a fewer error in detecting the closest match, which is
a function of a variety of parameters, including the memristor device ON/OFF conductance ratio
r = GON/GOFF, word length (the width of the array) N, the number of mismatches of the closest
match M, the number of wildcard ‘X’ in query vector/memory entry K, etc. The following analyses
aim to reveal the trade-off relationship among the parameters. The effect of device variation and
wire resistance is not taken into consideration for simplicity.

• Case 1

We start with the simplest case, where there is no wild card ’X’ in the query vector, and we
want to detect the exact match between the query and keys with a word length N. The sense
margin can be written as:

β =

Im+1
Im

− 1 =

GonVs + (N − 1)Go f f Vs
NGo f f Vs

− 1 =

r − 1
N

(2)

where Vs is the search voltage, and other parameters are deﬁned previously.

One ﬁnds that the sensing margin increases with memristor ON/OFF ratio r and decreases
with the word length N. On the other hand, if we want the sense margin to be at least β0,
the word length needs to be smaller than r−1
. For example, if the ON/OFF ratio (r) is 100,
β0
and the sense margin should be at least 0.5, the maximum word length Nmax is limited to
100−1
0.5 = 198.

16

• Case 2

Instead of detecting the exact match, here in this case study, we detect the closest match
(with M mismatch bits). Wildcard ‘X’ is also ignored in this case. The sense margin can be
written as:

β =

Im+1
Im

− 1 =

(M + 1)GonVs + (N − M − 1)Go f f Vs
MGonVs + (N − M)Go f f Vs

− 1

=

(M + 1)r + N − M − 1
Mr + N − M
(cid:19)
N
r − 1

M +

(cid:18)

= 1/

− 1

(3)

(4)

(5)

In addition to the trade-offs revealed in Case 1, one also ﬁnds that the sensing margin β
also decreases with the number of mismatch bits M. So, fewer number of mismatch bits is
preferred for a given application.

• Case 3

Here, we consider the case that query hash code has K wildcard bit ‘X’ from the TLSH step,
and and we want to detect the closest distance with M mismatch bits between the query and
keys with word length of N. The sense margin can be written as:

β =

Im+1
Im

− 1 =

(M + 1)GonVs + (N − M − K − 1)Go f f
MGonVs + (N − M − K)Go f f Vs

− 1

=

(M + 1)r + N − M − K − 1
Mr + N − M − K
N − K
r − 1

M +

(cid:18)

(cid:19)

= 1/

− 1

(6)

(7)

(8)

It is expected because introducing the wild card equivalently reduces the word length (ig-
noring the wire resistance effect). From Equation 8, one concludes that the sense margin for
the closest match increases with decreasing mismatch bits M and wordlength N, and rising
number of wildcard K and device ON/OFF ratio r. In addition, when we introduce ‘X’ in
hashcodes of the query, the number of mismatch bits M of the nearest neighbor is usually
reduced, further increasing the sensing margin.

From the above case study, we conclude that both decreasing the number of mismatch bits
M and increasing the number of the wild card bits K improve the sense margin and robustness of
the search operation. The ternary locality sensitive hashing (TLSH) that we proposed in this work

17

achieves both goals at the same time while maintaining the same accuracy. The main reason behind
this is that hashcodes generated by random projections have some redundancy bits that can be cast
away by assigning them to the wildcard bit ‘X’. Here we show the statistical results extracted from
the few-shot classiﬁcation to demonstrate the idea.

We ﬁrst analyze the statistics of the mismatch bits numbers (or hamming distance) of the
closest match M during the few-shot learning experiments. The distribution of the hamming dis-
tance is shown in Supplementary Fig. 15a. The result shows that by introducing ’X’ in the query
hashcodes, the number of mismatch bits M of the closest match decreases signiﬁcantly. The exper-
imental sensing margin is then extracted from the output current of the closest match (the smallest
current I0) and the next close match (the 2nd smallest current I1), by the equation: β = I1
− 1. As
I0
expected, the experimental sensing margin is inversely proportial to the the hamming distance, i.e.
β ∝ 1
M as is shown in Supplementary Fig. 15b. Though the sense margin here is not the worst
sense margin characterized by Equation 8, we still see a similar trend. As a result, the sensing mar-
gin is improved by introducing the wildcard ‘X’ from the TLSH because of the reduced hamming
distance of the closest match. The conclusion is clearly demonstrated in the experimental sensing
margin distribution shown in Supplementary Fig. 15c. Speciﬁcally, the probability that the sense
margin is larger than two with TLSH is more than two times higher than that using LSH. In light of
the above analysis, we can conclude that the reduced mismatch bit number from our TLSH scheme
improves the sense margin of the crossbar-based TCAM for the closest match search.

Similar phenomena can also be observed in a scaled problem for few-shot learning with
Mini-ImageNet dataset. The result in Supplementary Fig. 16a shows that the sensing margin
decreases with the word length N, and the sensing margin with TLSH is always higher than that
with LSH, because of the introduced wildcard bit ‘X’. Supplementary Fig. 16b also show that the
introduction of the wildcard ‘X’ also reduces the number of mismatch bits, further improving the
sensing margin.

Note that our TLSH method can also combine with other TCAM structures like 2T-2R1, 2,
2Flash3 2FeFET4, 5 which have the similar characteristic, but use latched voltage signal as the
output. As mentioned, the sense margin of our crossbar-based TCAM can be further improved
with the 3-bits encoding method, as discussed in Supplementary Fig. 5.

Supplementary Note 2. Energy and Latency estimation

The latency and energy consumption estimation for end-to-end MANN on the Omniglot
dataset is based on the experimental data extracted from our integrated crossbar system. For the
latency estimation in CNN layers, we considered channel-wise parallelism, where every channel in

18

Supplementary Figure 15: Sense margin analysis of the few-shot classiﬁcation on the Omniglot
dataset. a, Distribution of the mismatch bits of the closest match between query and keys in 4 different
tasks. We can see that by using TLSH in the hashing operation, we can signiﬁcantly decrease the number of
mismatch bits. b, Relationship between the sense margin and the number of mismatch bits M of the closest
match measured in 4 tasks. The sense margin decreases as the M increases. c, Distribution of the sense
margin for TLSH and LSH. The probability that the sense margin is larger than 2 using TLSH method is
twice as high as that using LSH. This veriﬁes that our TLSH method can signiﬁcantly increase the sense
margin.

19

0010203040510152025# mismatch bitsSense margin00501001502000.51.01.5>2Sense margin# Counts# mismatch bits# CountsAverage number of ‘X’ in query = 17(a)(b)(c)5-way 1-shot25-way 1-shot25-way 5-shot5-way 5-shot25% - 75%Range within 1.5 IQRMedian lineSupplementary Figure 16: Sense margin analysis of the 5-way 1-shot classiﬁcation on the Mini-
ImageNet dataset. a, Average sense margin drops as the word length increases. Sense margin on the
hashcodes generated by TLSH method is always higher than that by LSH method. b, Reduction of mis-
match bits grows with the word length and follows the similar trend as the number of ’X’ in the query.

a convolutional layer is working in a parallel way. Therefore, the latency of CNN layers for every
input image is depended on the number of input vectors and read time through column wires.
Each read time for vector-matrix-multiplication in the latency estimation is 10 ns. From the CNN
structure we can get the total time latency for CNN layers:

T = tread × Ninput = (784 + 784 + 196 + 196) × 10 = 19.6 µs

For the latency in TLSH and TCAM, since we need to predeﬁne the memory size for the few-shot
learning task, i.e.
, 256 entries for Omniglot, we still need to partition the hashing matrix and
memory matrix into multiple 64 × 64 arrays, which is the size in the experiment. In this case, we
still need to consider the time latency of full adders for aggregating the results from partitioned
arrays. We use the 2.5 ns latency of conventional 16T 8-bit adder reported in ref6. Therefore, we
get the time latency for TLSH and TCAM: 12.5 ns + 12.5 ns = 25 ns

The energy consumption estimation for Omniglot is done by reading the conductance matrix

out throughout the experiment. The total energy is calculated by:

E = ∑
i, j

V 2

jiGitpulse

where Vji is the jth input voltage vector to ith conductance matrix, Gi is the ith readout conductance
matrix and tpulse is the pulse width of the input. The ﬁnal energy consumption of the entire MANN
per image for the 5-way 1-shot problem is 1.26 µJ where search energy only consumes 13 pJ (in-
cluding TLSH). We acknowledge that the energy and latency overhead of peripheral circuits which

20

Word length1282565121024204840960.120.130.140.150.160.170.18Average sense margin(a)(b)Word length128256512102420484096020406080100120140# Countsneed to realize the entire MANN is not taken into account yet since it’s not fully optimized and
it’s difﬁcult to estimate. The numbers here mainly give an estimation for the implementation in
crossbar arrays.

For the estimation for the Mini-ImageNet dataset, we mainly focus on the overhead in the
search operation which is the key point in this work. For latency estimation, according to our ar-
chitecture for scaled-up networks, time won’t increase since the TLSH and TCAM are all working
in a parallel way which is 25 ns based on our aforementioned assumption. For the energy con-
sumption for Mini-ImageNet on the 5-way 1-shot task, we follow the methods above while we use
the simulated conductance matrix according to our measurement. The ﬁnal energy consumption
for a single image search across the entire memory module is 594 pJ.

In summary, we estimated about 1.26 µJ per Omniglot image inference (5-way 1-shot) with
our full crossbar-based MANN with >1,000× improvement over GPU (1.38 mJ on GPU). Dif-
ferent from MANN implemented on a conventional GPU backed by DRAM, which spends most
energy and time on memory transfer in the memory search, both the energy consumption and time
latency in crossbar arrays for MANN mainly comes from the CNN controller (19.6 µs for time
latency and 12.5 µJ for energy consumption). If we consider the search operation only, the con-
sumed energy for one search operation is only 2.1 pJ for the Omniglot dataset (128 bits), 2857×
improvement over GPU. For Mini-ImageNet dataset, we achieve 82.4 pJ per search (4,096 bits),
over 5.1 × 104 improvement to GPU on the same task7.
The GPU latency numbers reported
here are acquired using PyTorch Proﬁler8, and the energy consumption numbers from the NVIDIA
System Management Interface (nvidia-smi).

Those numbers can be further improved by choosing smaller input voltages and optimizing
peripheral circuits with more advanced technology nodes. Here, we demonstrate that energy
consumption can be improved by simply voltage scaling. In the experiment, we reduce the search
voltage by a factor of 10: from 0.2 V to 0.02 V. In this case, the power consumption is lowered
100× with minor accuracy loss (from 76.8% to 77.5% for the 25-way 1-shot problem). It is note-
worthy that the efﬁciency is not necessarily improving with larger crossbars9. We found the size
of 64×64 or 128×128 is a sweet spot when considering the output current, matrix utilization rate,
etc. Quantitively, we compared search energy in the distance search operation with different ap-
proaches and listed them in the table below, clearly showing our approach’s beneﬁt and scalability.
The result of the search energy is averaged over 11,875 search operations in the 25-way 1-shot
task based on the experimentally measured data. To make a fair comparison, we report the search
energy per bit of our approach and achieve the lowest energy consumption over previous work.

21

Supplementary Table 3: Energy consumption per bit per search operation of TCAM

Energy consumption per bit per search/fJ

4 Supplementary References

Ref.10 Ref.4 Our work
2.5

0.17

0.4

1. Li, J., Montoye, R. K., Ishii, M. & Chang, L. 1 mb 0.41 µm2 2t-2r cell nonvolatile tcam with
two-bit encoding and clocked self-referenced sensing. IEEE Journal of Solid-State Circuits
49, 896–907 (2013).

2. Li, H. et al. Sapiens: A 64-kb rram-based non-volatile associative memory for one-shot learn-

ing and inference at the edge. IEEE Transactions on Electron Devices (2021).

3. Fedorov, V. V., Abusultan, M. & Khatri, S. P. An area-efﬁcient ternary cam design using
ﬂoating gate transistors. In 2014 IEEE 32nd International Conference on Computer Design
(ICCD), 55–60 (IEEE, 2014).

4. Ni, K. et al. Ferroelectric ternary content-addressable memory for one-shot learning. Nature

Electronics 2, 521–529 (2019).

5. Laguna, A. F., Yin, X., Reis, D., Niemier, M. & Hu, X. S. Ferroelectric FET based in-memory
computing for few-shot learning. Proceedings of the ACM Great Lakes Symposium on VLSI,
GLSVLSI 373–378 (2019).

6. Ghadiry, M., Nadi, M. & A’Ain, A. K. Dlpa: Discrepant low pdp 8-bit adder. Circuits,

Systems, and Signal Processing 32, 1–14 (2013).

7. Kazemi, A. et al. A ﬂash-based multi-bit content-addressable memory with euclidean squared
distance. In 2021 IEEE/ACM International Symposium on Low Power Electronics and Design
(ISLPED), 1–6 (IEEE, 2021).

8. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Ad-

vances in neural information processing systems 32 (2019).

9. Wan, W. et al. Edge ai without compromise: Efﬁcient, versatile and accurate neurocomputing

in resistive random-access memory. arXiv preprint arXiv:2108.07879 (2021).

22

10. Karunaratne, G. et al. Robust high-dimensional memory-augmented neural networks. Nature

communications 12, 1–12 (2021).

23

