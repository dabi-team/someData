A Test for FLOPs as a Discriminant for Linear
Algebra Algorithms

Aravind Sankaran
IRTG-MIP
RWTH Aachen University
Aachen, Germany
aravind.sankaran@rwth-aachen.de

Paolo Bientinesi
Department of Computer Science
Ume˚a Universitet
Ume˚a, Sweden
pauldj@cs.umu.se

2
2
0
2

p
e
S
7

]
F
P
.
s
c
[

1
v
8
5
2
3
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Linear algebra expressions, which play a central role
in countless scientiﬁc computations, are often computed via a
sequence of calls to existing libraries of building blocks (such as
those provided by BLAS and LAPACK). A sequence identiﬁes
a computing strategy, i.e., an algorithm, and normally for one
linear algebra expression many alternative algorithms exist.
those algorithms might
Although mathematically equivalent,
exhibit signiﬁcant differences in terms of performance. Several
high-level
languages and tools for matrix computations such
as Julia, Armadillo, Linnea, etc., make algorithmic choices by
minimizing the number of Floating Point Operations (FLOPs).
However, there can be several algorithms that share the same
(or have nearly identical) number of FLOPs; in many cases,
these algorithms exhibit execution times which are statistically
equivalent and one could arbitrarily select one of them as the
best algorithm. It is however not unlikely to ﬁnd cases where
the execution times are signiﬁcantly different from one another
(despite the FLOP count being almost the same). It is also
possible that the algorithm that minimizes FLOPs is not the
one that minimizes execution time. In this work, we develop
a methodology to test the reliability of FLOPs as discriminant
for linear algebra algorithms. Given a set of algorithms (for an
instance of a linear algebra expression) as input, the methodology
ranks them into performance classes; algorithms in the same
class are statistically equivalent in performance. To this end, we
measure the algorithms iteratively until the changes in the ranks
converge to a value close to zero. FLOPs are a valid discriminant
for an instance if all the algorithms with minimum FLOPs are
assigned the best rank; otherwise, the instance is regarded as an
anomaly, which can then be used in the investigation of the root
cause of performance differences.

Index Terms—Performance Analysis, Linear algebra algo-
rithms, Algorithm ranking, Mathematical software performance

I. INTRODUCTION

One of the major performance bottlenecks for countless
computational problems is the evaluation of linear algebra ex-
pressions, i.e., expressions involving operations with matrices
and/or vectors. Libraries such as BLAS and LAPACK provide
a small set of high performance kernels to compute some stan-
dard linear algebra operations [1], [2]. However, the mapping
of linear algebra expressions on to an optimized sequence of
standard operations is a task far from trivial; the expressions

Financial support from the Deutsche Forschungsgemeinschaft (German
Research Foundation) through grants GSC 111 and IRTG 2379 is gratefully
acknowledged.

can be computed in many different ways—each corresponding
to a speciﬁc sequence of library calls—which can signiﬁcantly
differ in performance from one another. Unfortunately, it has
been found that the mapping done by most popular high level
programming languages such as Matlab, Eigen, TensorFlow,
PyTorch, etc., is still suboptimal [3], [4].

Linear algebra expressions can be manipulated using math-
ematical properties such as associativity, distributivity, etc., to
derive different mathematically equivalent variants (or algo-
rithms). For instance, consider the following expression that
evaluates the product of four matrices:

X = ABCD

(1)

where A ∈ Rm×n, B ∈ Rn×k, C ∈ Rk×l and D ∈ Rl×q are
all dense matrices. An instance of Expression 1 is identiﬁed by
the tuple (m, n, k, l, q). Because of associativity of the matrix
product, Expression 1 can be computed in many different
ways, each identiﬁed by a speciﬁc parenthesization. Although
different parenthesizations evaluate to the same mathematical
result, they require different number of FLOPs, and might
exhibit different performance. For Expression 1, ﬁve possible
parenthesizations and their approximate associated costs are
shown in Figure 1. At least six algorithms can be implemented
from the ﬁve variants; note that the evaluation of (AB)(CD)
can correspond to two different implementations, which differ
in the order of instructions, i.e., AB can be computed either
before or after CD. A simple strategy to select the fastest
algorithm is to select a parenthesization that performs the
least ﬂoating point operations (FLOPs). However, it has been
observed that the algorithm with the lowest FLOP count is not
always the fastest algorithm; such instances are referred to as
anomalies [5].
Consider

1:
(331, 279, 338, 854, 497), which was observed as an anomaly
in [5]; there, the algorithms were implemented in C, linked
against Intel Math Kernel library1 and measurements were
conducted on a Linux-based system using 10 cores of an
Intel Xeon processor. The measurement of each algorithm
was repeated 10 times and the median was used to compare
algorithms. Now, in a comparable compute environment, we

of Expression

following

instance

the

1MKL version 2019.0.5

1

 
 
 
 
 
 
Fig. 1: Variants for Matrix chain of length 4. The cost indicates
the approximate FLOP count divided by 2.

the algorithms in Julia2, where the possible
re-implement
inﬂuence of the library overheads on execution times can
be greater than that of the equivalent C implementations.
We link against the same Intel MKL versions and measure
the algorithms. The box-plot of the measurements from two
independent runs are shown in Figure 2a and 2b. The red
line in the box-plot of each algorithm represents the median
execution time; the range of the grey box indicates from 25th
to 75th quantile, and the length of this box is the Inter-Quartile
Region(IQR); the dotted lines are the “whiskers” that extend
to the smallest and largest observations that are not outliers
according to the 1.5IQR rule [6]. The difference in FLOP
count of an algorithm (algi) from the one that computes the
least FLOPs is quantiﬁed by the Relative FLOPs score (RFi):

RFi =

Fi − Fmin
Fmin

(2)

where Fi is the FLOPs computed by algi and Fmin is the cost
corresponding to the algorithm that computes the least FLOPs.
The relative FLOPs scores are shown in Table I.

Rank

Run 1

Run 2

1

2

3

4

5

6

algorithm1
(0.0)
algorithm2
(0.04)

algorithm2
(0.04)
algorithm1
(0.0)

algorithm3
(0.11)
algorithm0
(0.0)

algorithm5
(0.32)
algorithm4
(0.27)

algorithm4
(0.27)
algorithm3
(0.11)

algorithm0
(0.0)
algorithm5
(0.32)

TABLE I: Algorithms are ranked according to increasing
median execution time. The relative FLOP score of every
algorithm is indicated in the parenthesis.

(a) Run 1

(b) Run 2

Fig. 2: Two independent runs consisting of 10 measurements
of each algorithm for the instance (331, 279, 338, 854, 497) of
Expression 1. The algorithms in (a) and (b) are sorted based
on increasing median execution times from bottom to top.

It is well known that execution times are inﬂuenced by
many factors, and that repeated measurements, even with
the same input data and compute environment, often result
in different execution times [7], [8]. Therefore, comparing
the performance of any two algorithms involves comparing
two sets of measurements. In common practice, time mea-
surements are summarized to statistical estimates (such as
minimum or median execution time, possibly in combination
with standard deviations or quantiles), which are then used

2Julia version 1.3.0

to compare and rank algorithms [5], [7]. However, when the
turbo boost settings or inter-kernel cache effects begin to
have a signiﬁcant impact on program execution, the common
statistical quantities cannot reliably capture the proﬁle of
the time measurements [9]; as a consequence, when time
measurements are repeated, the ranking of algorithms would
most likely change and this makes the development of reliable
performance models for automatic algorithm comparisons
difﬁcult.

Consider again the examples in Figure 2. It can be seen that

2

the ranking of algorithms based on medians are completely
different for the two runs. Moreover, the algorithms are not
ranked based on the increasing FLOP counts; in the ﬁrst run,
algorithm0, which is one of the best algorithms in terms of
FLOP count (i.e., RF1 = 0.0) is ranked last, and in the second
run, algorithm2, which is not among the best algorithms
in terms of FLOPs (i.e., RF2 (cid:54)= 0.0) is ranked ﬁrst. The
lack of consistency in ranking stems from not considering
the possibility that two algorithms can be equivalent when
comparing their performances. The box-plots in Figure 2
show that the underlying distribution of measurements of the
algorithms largely overlap. This indicates that the algorithms
with least FLOPs, even though not ranked ﬁrst, could have
simply been ranked as being equivalent to (as good as) the
algorithm chosen as the fastest based on median execution
times. Hence, we elaborate on the deﬁnition of anomalies. Let
SF be the set of all algorithms with the least FLOP count.
In order to classify an instance as an anomaly, the following
conditions are checked one after the other:

1) There should exist an algorithm that is not in SF , but
exhibits noticeably better performance than those in SF ;
that is, we ﬁrst check if SF is a valid representative of
the fastest algorithms.

2) If an instance is not classiﬁed as an anomaly according
to (1), then it is checked if one of the algorithms in
SF exhibit noticeable difference in performance from
the rest in SF ; this implies, even though SF is a valid
representative of the fastest algorithms, it is not possible
to randomly choose an algorithm from SF as the best
algorithm.

In order for one algorithm to be faster (or slower) than
another, there should be noticeable difference in the distri-
bution of their time measurements; for example, consider
another instance of Expression 1: (75, 75, 8, 75, 75). The mea-
surements of the variant algorithms in Julia are shown in
Figure 3. One could visually infer that algorithms 0 and 1
are equivalent and belong to the same (and best) performance
class, while algorithms 2 and 3 have noticeable difference
in performance from algorithms 0 and 1, hence belong to
a different performance class. The expected ranks for the
algorithms are shown in Table II. To this end, the comparison
of any two algorithms should be able to yield one of the
three outcomes: faster, slower, or equivalent. In this paper,
we deﬁne a three-way comparison function, and develop a
methodology that uses this three-way comparison to sort a set
of algorithms into performance classes by merging the ranks
of algorithms whose distribution of time measurements are
signiﬁcantly overlapping with one another.

Algorithm

algorithm0
(0.0)

algorithm1
(0.0)

algorithm2
(2.78)

algorithm3
(2.78)

algorithm4
(5.59)

algorithm5
(5.59)

Expected rank

1

1

2

2

3

3

TABLE II: Expected ranks for the Algorithms based on the
measurements in Figure 3. The relative FLOP score of every
algorithm is indicated in the parenthesis.

Fig. 3: 20 measurements of each algorithm for the instance
(75, 75, 8, 75, 75) of Expression 1.

In practice, linear algebra expressions can have 100s of
possible variants. A Statistically sound algorithm comparison
requires several repetition of measurements for each variant.
As it would be time-consuming to measure all the variants
several times, the following approach is employed:

1) After a small warm up to exclude library overheads, all

the algorithms are measured exactly once.

2) The difference in the execution time of algi from the
algorithm with the lowest execution time is quantiﬁed
by the Relative Time score (RTi):

RTi =

Ti − Tmin
Tmin

(3)

where Ti is the execution time of algi and Tmin is the
minimum observed execution time.

3) A set of candidates (S) is created by ﬁrst shortlisting
all the algorithms with minimum FLOP count. Then, to
this set, are added all the algorithms that perform more
FLOPs, but have relative execution time within a user-
speciﬁed threshold.

4) An initial hypothesis is formed by ranking the candidates

in S based on the single-run execution times.

5) Each candidate in set S is measured M times (where
M is small; e.g., 2 or 3) and ranks of the candidates
are updated or merged using the three-way comparison
function.

6) Step 5 is repeated until the changes in ranks converge
or the maximum allowed measurements per algorithm
(speciﬁed by the user) has been reached.

If FLOPs are a valid discriminant for a given instance of an
expression, then all the algorithms with the least amount of
FLOPs would obtain the best rank. Otherwise, the instance
would be classiﬁed as an anomaly. The identiﬁed anomalies
can be used to investigate the root cause of performance
differences, which would in turn help in the development of
meaningful performance models to predict the best algorithm
without executing them.

3

Organization: In Sec. II, we present related works. In
Sec. III, we introduce the methodology to rank algorithms
using the three-way comparison. The working of our method-
ology and the experiments are explained in Sec. IV. Finally,
in Sec. V, we draw conclusions.

II. RELATED WORKS

The problem of mapping one target linear algebra expres-
sion to a sequence of library calls is known as the Linear Alge-
bra Mapping Problem (LAMP) [3]; typical problem instances
have many mathematically equivalent solutions, and high-level
languages and environments such as Matlab, Julia etc., ideally
should select the fastest one. However, it has been shown
that most of these languages choose algorithms that are sub-
optimal in terms of performance [3], [4]. A general approach
to identify the fastest algorithm is by ranking the solution
algorithms according to their predicted performance. For linear
algebra computations, a common performance metric to be
used as performance predictor is the FLOP count; however, it
has been observed that the number of FLOPs is not always
a direct indicator of the fastest code, especially when the
computation is bandwidth-bound or executed in parallel [10],
[11]. For selected bandwidth-bound operations, Iakymchuk et
al. developed analytical performance models based on memory
access patterns [12], [13]; while those models capture the
program execution accurately, their construction requests not
only a deep understanding of the processor, but also of the
details of the implementation.

There are many examples where an increase in FLOPs
count results in a decrease in execution time (anomalies);
[14]–[16] expose some speciﬁc mathematical operations for
which the need for complex performance models (that mostly
require measuring the execution times) are justiﬁed. However,
that does not mean that FLOPs counts are ineffectual for
general purpose linear algebra computations that are targeted
by the high-level languages. For instance, in [4], Sankaran
et al. expose optimization opportunities in Tensorﬂow and
PyTorch to improve algorithm selection by simply calculating
the FLOPs count and applying linear algebra knowledge. In
order to justify the need for complex performance models for
algorithm selection, it is important to quantify the presence
of anomalies. In [5], Lopez et al. estimate the percentage
of anomalies for instances of Expression 1 on certain single
node multi-threaded setting to be 0.4 percent; in other words,
for that case, complex performance models are pointless
unless they achieve an accuracy greater than 99.6 percent.
It was indicated that the percentage of anomalies increases
for more complex expressions. However, in that study, the
algorithms were compared using the median execution time
from 10 repetition of measurements, and because of this, their
comparisons may not be consistent when the experiments are
repeated.

Performance metrics that are a summary of execution times
(such as minimum, median etc.) lack in consistency when
the measurements of the programs are repeated; this is due
to system noise [7], cache effects [8], behavior of collective

4

communications [17] etc., and it is not realistic to eliminate
the performance variations entirely [18]. The distribution of
execution times obtained by repeated measurements of a
program is known to lack in textbook statistical behaviours
and hence specialized methods to quantify performance have
been developed [19]–[22]. However, approximating statistical
distributions require executing the algorithms several times. In
this work, we develop a strategy to minimize the number of
measurements.

The performance of an algorithm can be predicted using
regression or machine learning based methods; this requires
careful formulation of an underlying problem. A wide body of
signiﬁcant work has been done in this direction for more than a
decade [23]–[25]. Peise et al in [23] create a prediction model
for individual BLAS calls and estimates the execution time for
an algorithm by composing the predictions from several BLAS
calls. In [25], Barve et al predict performance to optimize
resource allocation in multi-tenant systems. Barnes et al in [24]
predict scalability of parallel algorithms. In those approaches,
the performances are quantiﬁed in an absolute term. Instead,
in this work, we quantify performance of algorithms relatively
to one another using pairwise comparisons. In [26], Sankaran
et al. discuss an application of algorithm ranking via relative
performance in an edge computing environment to reduce
energy consumption in devices. They compare algorithms by
randomly bootstrapping measurements. Instead, we present an
approach that compares algorithms by comparing the quantiles
from the measurements.

III. METHODOLOGY

instance, consider

Let A be a set of mathematically equivalent algorithms.
The algorithms alg1, . . . , algp ∈ A are ordered according to
decreasing performance based on some initial hypothesis. Let
h0 : (cid:104)algi(1), . . . , algi(j), . . . algi(p)(cid:105) be an initial ordering.
Here, i(j) is the index of the algorithm at position j in
h0. For
the equivalent algorithms in
Figure 3: {alg0, alg1, alg2, alg3, alg4, alg5}. If the initial
hypothesis is formed based on the increasing minimum
execution times observed for each algorithm, then the initial
ordering would be h0 : (cid:104)alg2, alg1, alg0, alg4, alg3, alg5(cid:105).
the index of the algorithm in the ﬁrst position is
Here,
i(1) = 2, the second position is i(2) = 1, and so on. The
execution time of each algorithm in h0 is measured N
times, and based on the additional empirical evidence, the
algorithms are re-ordered to produce a sequence consisting
of tuples s : (cid:104)(algs(1), rank1), . . . , (algs(p), rankp)(cid:105), where
rankj is the rank of the algorithm at position j in s and
rankj ∈ {1, . . . k} with k ≤ p (i.e., several algorithms
can share the same rank). Here, s(j) is the index of
the algorithm at jth position in s. For instance, for the
experiment
the sorted sequence would be
(cid:104)(alg1, 1), (alg0, 1), (alg2, 2), (alg3, 2), (alg4, 3), (alg5, 3)(cid:105).
to one another
Algorithms that evaluate to be equivalent
are assigned the same rank. To this end, we ﬁrst deﬁne
the procedure to compare two algorithms that
takes into
account the equivalence of the algorithms. Then, we sort the

in Figure 3,

algorithms using the three-way comparison function to update
and merge ranks.

Algorithm comparison (Procedure 1): Procedure 1 takes
in as input any two sets of N measurements ti, tj ∈ RN
from algorithms algi, algj respectively, and a speciﬁc quantile
range (qlower, qupper). The procedure compares the quantiles of
the two algorithms. If the qupper of algi is less than the qlower of
algj, then algi is “better” than (<) algj. Otherwise, if qupper
of algj is less than the qlower of algi, then algi is “worse” than
(>) algj. Otherwise, both the algorithms are “equivalent”(∼).

Procedure 1 CompareAlgs (algi, algj, qlower, qupper)

Inp: algi, algj ∈ A

qlower, qupper ∈ (0, 100)

qupper > qlower

Out: {“algi < algj”, “algi > algj”, “algi ∼ algj”}

(cid:46) ti ∈ RN
(cid:46) tj ∈ RN

(cid:46) ti
(cid:46) ti

low ∈ R
up ∈ R

(cid:46) tj
(cid:46) tj

low ∈ R
up ∈ R

low ← Value of the qlower quantile in ti
up ← Value of the qupper quantile in ti

low ← Value of the qlower quantile in tj
up ← Value of the qupper quantile in tj

1: ti = get measurements(algi)
2: tj = get measurements(algj)
3:
4: ti
5: ti
6:
7: tj
8: tj
9:
10: if ti
11:
12: else if tj
13:
14: else
15:

up < tj
return “algi < algj”
low then
return “algi > algj”

return “algi ∼ algj”

low then

up < ti

Sorting procedure (Procedure 2): The inputs to Pro-
cedure 2 are the initial sequence h0 and a quantile range
(qlower, qupper). The output is a sorted sequence set s. To this
end, the bubble-sort procedure [27] is adapted to work with the
three-way comparison function. Starting from the left most el-
ement in the initial sequence, the procedure compares adjacent
algorithms and swaps their positions if an algorithm occurring
later in the sequence is better (according to Procedure 1) than
the previous algorithm, and then ranks are updated. When
the comparison of two algorithms results to be equivalent as
each other, both are assigned with the same rank, but their
positions are not swapped. In order to illustrate the rank update
rules in detail, we consider the illustration in Figure 4, which
shows the intermediate steps while sorting an initial sequence
h0 : (cid:104)alg1, alg2, alg3, alg4(cid:105). All possible update rules that
one might encounter appear in one of the intermediate steps
of this example.

1) Both positions and ranks are swapped : In the ﬁrst
pass of bubble sort, pair-wise comparison of adja-
cent algorithms are done starting from the ﬁrst el-
the sequence is
ement
(cid:104)alg1, alg2, alg3, alg4(cid:105). As a ﬁrst step, algorithms alg1
and alg2 are compared, and alg2 ends up being faster.
As the slower algorithm should be shifted towards the

in the sequence. Currently,

5

Fig. 4: Bubble Sort with the three-way comparison function.

end of the sequence, alg1 and alg2 swap positions (line
9 in Procedure 2). Since all the algorithms still have
unique ranks, alg2 and alg1 also exchange their ranks,
and no special rules for updating ranks are applied. So,
alg2 and alg1 receive rank 1 and 2, respectively.
2) Positions are not swapped but the ranks are merged:
Next, algorithm alg1 is compared with its successor
alg3; since they are just as good as one another, no
swap takes place. Now, the rank of alg3 should also
indicate that it is as good as alg1; so alg3 is given the
same rank as alg1 and the rank of alg4 is corrected by
decrementing by 1. (line 12-14 in Procedure 2). Hence
alg1 and alg3 have rank 2, and alg4 is corrected to
rank 3.

3) Both positions and ranks are swapped: (This is the
same rule applied in Step 1). In the last comparison of
the ﬁrst sweep of bubble sort, Algorithm alg4 results
to be faster than alg3, so their positions and ranks are
swapped. This completes the ﬁrst pass of bubble-sort.
At this point, the sequence is (cid:104)alg2, alg1, alg4, alg3(cid:105).
4) Swapping positions with algorithms having same rank:
In the second pass of bubble sort, the pair-wise com-
parison of adjacent algorithms, except the “right-most”
algorithm in sequence, is evaluated (note that the right-
most algorithm can still have its rank updated depending
upon the results of comparisons of algorithms occurring
earlier in the sequence). The ﬁrst two algorithms alg2
and alg1 were already compared in Step 1. So now,
the next comparison is alg1 vs. alg4. Algorithm alg4
results to be faster than alg1, so their positions are
swapped as usual. Now, the rank of alg4 remains the

same, but the rank of all the algorithms successive to
alg4 is incremented by 1. Therefore, the rank of alg1
and alg3 is incremented by 1 (line 10-11 in Procedure
2). This completes the second pass of bubble sort and the
two slowest algorithms have been pushed to the right.
5) Positions are not swapped but the ranks are merged:
(This is the same rule applied in Step 2). In the third
and ﬁnal pass, we again start from the ﬁrst element
on the left of the sequence and continue the pair-wise
comparisons until the third last element. This leaves only
one comparison to be done, alg4 vs. alg2. Algorithm
alg4 is evaluated to be as good as alg2, so both are
given the same rank and the positions are not swapped.
The ranks of algorithms occurring later than alg4 in the
sequence are decremented by 1. Thus, the ﬁnal sequence
is (cid:104)alg2, alg4, alg1, alg3(cid:105). Algorithms alg2 and alg4
obtain rank 1, and alg1 and alg3 obtain rank 2.

Procedure 2 SortAlgs (h0, qlower, qupper)

Input: h0 : (cid:104)algi(1), . . . , algi(p)(cid:105)
qlower, qupper ∈ (0, 100)

qupper > qlower
Output: s : (cid:104)(algs(1), r1), . . . , (algs(p), rp)(cid:105)

1: for j = 1, . . . , p do
Initialize rj ← j
2:
Initialize s(j) ← i(j)
3:

(cid:46) Initialize Alg rank
(cid:46) Initialize Alg order

4:
5: for k = 1, . . . , p do
6:
7:

for j = 0, . . . , p-k-1 do

8:
9:
10:
11:

12:
13:
14:

ret = CompareAlgs(algs(j), algs(j+1), qlower, qupper)
if algs(j+1) is faster than algs(j) then
Swap indices s(j) and s(j + 1)
if rj+1 = rj then

Increment ranks rj+1, . . . , rp by 1

else if algs(j+1) is as good as algs(j) then

if rj+1 (cid:54)= rj then

Decrement ranks rj+1, . . . , rp by 1
else if algs(j) is faster than algs(j+1) then

15:
16:
17: s ← (cid:104)(algs(1), r1), . . . , (algs(p), rp)(cid:105)
18: return s

Leave the ranks as they are

and alg4, alg5 obtain rank 3; these are the ranks one might
expect according to the visual inference of the box-plots in
Figure 3. For a smaller range, (q35, q65), alg2 and alg3 obtain
different ranks as alg2 is slightly shifted towards the right of
alg3 despite signiﬁcant overlap. Therefore, ranks from isolated
quantile ranges does not accurately quantify the underlying
performance characteristics of the algorithms.

alg1

alg0

alg3

alg2

alg4

alg5

(q5, q95)
(q10, q90)
(q15, q85)
(q20, q80)
(q25, q75)
(q30, q70)
(q35, q65)
Mean rank

1
1
1
1
1
1
1
1.0

1
1
1
1
1
1
1
1.0

1
2
2
2
2
2
2
1.86

1
2
2
2
2
2
3
2.0

1
2
2
3
3
3
4
2.57

1
2
2
3
3
3
4
2.57

TABLE III: Ranks calculated for the data in Figure 3 on
different quantile ranges. The mean ranks of the algorithms
across the quantile ranges are also shown.

In order to estimate a reliable metric, we repeat Procedure 2,
and compute ranks with different quantile ranges, and compute
the mean rank (mr) for each algorithm. The ranks from a
speciﬁc quantile range can be compared with the mean ranks
to get a better understanding of the performance. We choose
(q25, q75) as default, as this range is considered as a default
for statistical outlier detections [6]. For (q25, q75), both alg2
and alg3 obtain the same rank; however, as the mean rank for
alg2 is slightly greater than alg3, this indicates that according
to the available empirical data, alg3 is better than alg2.

Procedure 3 MeanRanks(h0, q)

Input: h0 : (cid:104)algi(1), . . . , algi(p)(cid:105)

q : [(q1low, q1up), . . . , (qHlow, qHup)]

Output: s[25,75], [(alg1, mr1), . . . , (algp, mrp)]

1: Initialize mri ← 0
2: for qlow, qup in q do
3:
4: mri ← Mean rank of algi over all the quantiles in s.
5: return s[25,75], [(alg1, mr1), . . . , (algp, mrp)]

s[low,up] ← SortAlgs(h0, qlow, qup)

(cid:46) i ∈ {1, . . . , p}

Mean rank calculation (Procedure 3): The results of
the sorting procedure depend on the chosen quantile ranges
(qlower, qupper). For the example in Figure 3, the estimated
ranks for different quantile ranges are shown in Table III.
For large quantile ranges that cover the tail ends of the time
distribution, such as (q5, q95),
to be
equivalent to one another more often than the small ranges.
For instance, in our example, all the algorithms are estimated
to be equivalent (i.e., rank 1) for the quantile range (q5, q95).
As the quantile ranges become smaller and smaller, the tails
of the distributions are curtailed, and the overlaps estimated
among the algorithms become lesser and lesser. Thus, for
(q25, q75), alg0, alg1 obtain rank 1, alg2, alg3 obtain rank 2,

the algorithms result

Convergence (Procedure 4): The calculation of ranks
requires measurements of execution times for each algorithm.
Starting with an empty measurement set (i.e., N = 0),
M measurements (typically only a few; e.g., 2 or 3) of
each algorithm are iteratively added and the mean ranks are
computed using Procedure 3. The iteration stops as the mean
ranks converge. We estimate the convergence of a rank as
follows: Let x be an ordered list of mean ranks. Then the
changes in mean rank between adjacent algorithms in the list
(dx) is computed as:

dx = convolution(x, [1, −1], step = 1)

6

where [1,-1] is the convolution ﬁlter. In simple words, dx
is the difference in the mean ranks between the adjacent
algorithms in x. Let dx and dy be the changes in mean ranks
over the lists from iteration j and j − 1 respectively. Then, the
stopping criterion for the iteration is

(cid:107)dx − dy(cid:107)L2
p

< eps

where (cid:107).(cid:107)L2 is the L2 norm and p is the number of algorithms
being compared. The iterations stop when the stopping crite-
rion becomes less than eps or if the number of measurements
per algorithm reaches a user speciﬁed maximum value (max).
For illustration, follow the example in Table III. The list
of mean ranks is x = [1, 1, 1.86, 2.0, 2.57, 2.57], and the
changes in mean ranks is dx = [0, 0.86, 0.14, 0.57, 0]. Now,
consider the ranks at (q35, q65), the ranks of the algorithms are
(cid:104)(alg1, 1), (alg0, 1), (alg3, 2), (alg2, 3), (alg4, 4), (alg5, 4)(cid:105).
iteration,
Let
in
alg2
i.e.,
obtain
(cid:104)(alg1, 1), (alg0, 1), (alg3, 2), (alg2, 2), (alg4, 3), (alg5, 3)(cid:105),
then
be
y = [1, 1, 1.86, 1.86, 2.43, 2.43], and the changes in mean rank
would be dy = [0, 0.86, 0, 0.57, 0]. Then (cid:107)dy−dx(cid:107)L2
= 0.028.

following
same

iteration would

the mean

assume

the
the

alg3

rank;

rank

next

that

and

for

us

5

Procedure 4 MeasureAndRank(h0, M, eps, max)

Input: h0 : (cid:104)algi(1), . . . , algi(p)(cid:105)

M, eps, max ∈ R
Output: s[25,75], mr(cid:48)

(cid:46) norm ∈ R

(cid:46) j ∈ {1, . . . , p − 1}

1: Set norm (cid:29) eps
2: q ← Deﬁne a set of quantile ranges.
3: N = 0
4: Initialize dyj ← 1
5: while norm ≥ eps and N < max do
Measure every algorithm M times.
6:
N = N + M
7:
s[25,75], mr(cid:48) ←MeanRanks(h0, q)
8:
9:

10:
11:

(cid:46) On N Measurements
xi ← Mean rank of algi from mr(cid:48) (cid:46) i ∈ {1, . . . , p}
(cid:46) x ∈ Rp
dx ← convolution(x, [1, −1], step = 1)
norm ← (cid:107)dx−dy(cid:107)L2
(cid:46) dx ∈ Rp−1
dy ← dx
h0 ← (cid:104)algs(1), . . . , algs(p)(cid:105) (cid:46) Ordering from s[25,75]

p

12:
13:
14:
15: return s[25,75], mr(cid:48)

IV. INTERPRETATION

In this section, we explain the working of our methodology.
For our experiments, we use the Linnea framework [11], which
accepts linear algebra expressions as input, and generates a
family of mathematically equivalent algorithms (in the form
of sequential Julia code) consisting of (mostly, but not limited
to) sequences of BLAS and LAPACK calls. The experiments
are run on a Linux based Intel Xeon machine with turbo-boost
enabled and number of threads set to 10.

We consider the following instances of Expression 1:

7

(a) Instance A

(b) Instance B

Fig. 5: h0 is the Initial hypothesis. s is the updated sequence.
The ranks at (q25, q75) and the mean ranks (in brackets) are
shown.

• Instance A: (1000, 1000, 500, 1000, 1000)
• Instance B: (1000, 1000, 1000, 1000, 1000)

For each instance, we ﬁrst execute all the algorithms once.
Then, the initial hypothesis (h0) is formed by ranking the
algorithms in the increasing order of their single-run execution
times. The Procedure 4 is applied with the parameters M = 3,
eps = 0.03 and max = 30. We deﬁne the quantile ranges
same as those in Table III. The experiments are run on two
different settings. In the ﬁrst setting, the experiments are run
on a node, whose unused processing power and memory can
be shared among other processes. In the second setting, the
experiments are run on a node where exclusive access is
granted. The execution times of the algorithms on the shared
node is expected to have more ﬂuctuations than the exclusive
node. Before every iteration of the mean ranks computation
in Procedure 4, the M measurements from every algorithm
are shufﬂed to enable fair comparison. The results of the
experiments are shown in Figure 5. The tables show the
updated sequences, estimated ranks at (q25, q75) and the mean
ranks. The shades of the cells indicate the relative FLOPs
counts of the algorithms (see Equation 2); a darker shade
indicates a higher relative FLOPs. The algorithms in white
cells compute the least FLOPs.

• Instance A (Figure 5a): The minimum FLOPs algorithms
(alg0 and alg1) obtain the best rank in both settings. In
the shared setting, the mean rank of alg0 is greater than
alg1, which indicates that the underlying distribution of
alg0 is slightly shifted towards the right of alg1 (see
Figure 6a);
this indicates that higher execution times
are observed in some samples of alg0 than in alg1.

All the other algorithms obtain the same rank despite
having different FLOP counts, which indicates signiﬁcant
overlap of distributions. However, the algorithms with
the highest FLOP count (alg4 and alg5) are slightly
shifted to the right of alg2 and alg3, indicating relatively
worse performance; this is quantiﬁed by the higher mean
rank scores of alg4 and alg5 than alg2 and alg3. On
the other hand, in the exclusive setting, alg4 and alg5
obtain a higher rank than alg2 and alg3, which indicates
not only the rightward-shift but also a non-signiﬁcant
overlap of the underlying distributions (see Figure 6c).
The iterations in Procedure 4 stop after 21 and 24
measurements per algorithm in the shared and exclusive
settings respectively.

• Instance B (Figure 5b): All

the algorithms compute
comparable FLOPs and they all obtain the same rank in
both the settings. In the shared mode, 15 measurements
per algorithm were made, while the exclusive mode took
27 measurements per algorithm for the mean ranks to
converge.

Effect of Turbo boost: It can be noticed that in the exclusive
setting, more measurements were made than in the shared
setting. The scatter plots of algorithms in Figure 6b and 6c
show multi-modal distribution of measurements (especially bi-
modal with two clusters of data points at the two ends of the
distribution). This is because the processor operated at multiple
frequency levels due to turbo boost settings, thereby resulting
in signiﬁcantly different execution times for the same algo-
rithm. As the measurements of algorithms were sufﬁciently
shufﬂed, the probability that a particular algorithm executes
in just one frequency mode—thereby resulting in a biased
comparison—is minimized. However, for the quantile ranges
we considered (from Table III), the algorithms alg4, alg3,
alg0, alg1, alg2 obtain the same mean rank scores (see exclu-
sive mode in Figure 5b) even though the relative shifts among
their distributions can be visually observed in Figure 6c.
In order to compare algorithms based on the measurements
taken during the fast frequency modes of the processor (i.e.,
measurements towards the left end of the distribution), we
modify the quantiles set in Procedure 4 and consider the fol-
lowing ranges: [(q5, q50), (q15, q45), (q20, q40), (q25, q35)] and
recalculate the mean ranks. The results are shown in Figure 7a.
Now, alg5 obtains the best rank. The relative shifts among the
algorithms based on the left-part of the distributions are now
quantiﬁed by the mean ranks.

Test for FLOPs as a discriminant for the best algorithm:
Consider the algorithms for instance B again. If the algorithms
are to be executed in the compute node that operates at
multiple frequency levels, then according to our methodology,
at (q25, q75), all the algorithms are considered equivalent, as
they all obtain the best rank. The mean rank of alg5 is better
than the rest, but the methodology does not consider them
statistically signiﬁcant. Hence, one would not lose signiﬁcantly
in performance by randomly choosing one of the minimum
FLOPs algorithm. However, if one is interested only in the

(a) Instance A: Shared

(b) Instance A: Exclusive

(c) Instance B: Exclusive

Fig. 6: Measurements for Instance A and Instance B. The
algorithms are ordered according to the increasing order of
their ranks from bottom to top.

8

the algorithms are merged if they have signiﬁcant overlaps in
the distribution of measurements. The rank estimates quantify
the relative performances of the algorithms from one another.
We showed that our methodology can be used to interpret
and analyse performance even in compute nodes that operates
at multiple frequency levels (e.g., machines that have turbo-
boost enabled). We used our methodology to develop a test
for FLOPs as a discriminant for linear algebra algorithms.
The Python implementation of the methodology is available
online3.

Recall our proposition (from Sec. I) that high-level lan-
guages such as Julia, Matlab, TensorFlow, etc., choose al-
gorithms that are sub-optimal in performance. The argument
for the sub-optimality can be two folds: First, the languages
do not fully apply the linear algebra knowledge to explore
all possible alternate algorithms (this issue was not discussed
in this paper). Second, they select a sub-optimal algorithm
from a given set of alternatives; because these high-level
languages make algorithmic choices by minimizing FLOPs
and it had been pointed out (e.g., in [5]) that the algorithm
with the lowest FLOP count is not always the fastest (such
instances were referred to as anomalies). In order to tackle
the second argument, performance models that facilitate better
algorithm selection strategy have to be developed; that is, those
performance models should be able to perform better than
what FLOPs can already do. To this end, it is important to
verify that, for a considered use case, there exists an abundance
of anomalies that cannot be discriminated using FLOP counts.
Our methodology can be used to detect the presence of anoma-
lies. The anomalies can be used for further investigations to
ﬁnd the root-causes of performance differences.

REFERENCES

[1] J. J. Dongarra, J. D. Croz, S. Hammarling, and R. J. Hanson, “A proposal
for an extended set of fortran basic linear algebra subprograms,” ACM
Signum Newsletter, vol. 20, no. 1, pp. 2–18, 1985.

[2] J. Demmel, “Lapack: A portable linear algebra library for high-
performance computers,” Concurrency: Practice and Experience, vol. 3,
no. 6, pp. 655–666, 1991.

[3] C. Psarras, H. Barthels, and P. Bientinesi, “The linear algebra mapping
problem. current state of linear algebra languages and libraries.” ACM
Transactions on Mathematical Software, May 2022, accepted. [Online].
Available: https://arxiv.org/pdf/1911.09421v2.pdf

[4] A. Sankaran, N. Alashti, C. Psarras, and P. Bientinesi, “Benchmarking
the linear algebra awareness of tensorﬂow and pytorch,” in 2022 IEEE
International Parallel and Distributed Processing Symposium Workshops
(IPDPSW); iWAPT, Mar. 2022, pp. 924–933.

[5] F. Lopez, L. Karlsson, and P. Bientinesi, “Flops as a discriminant
the 51st
for dense linear algebra algorithms,” in Proceedings of
International Conference on Parallel Processing (ICPP 2022), Aug.
2022. [Online]. Available: https://arxiv.org/pdf/2207.02070.pdf

[6] V. Hodge and J. Austin, “A survey of outlier detection methodologies,”

Artiﬁcial intelligence review, vol. 22, no. 2, pp. 85–126, 2004.

[7] T. Hoeﬂer, T. Schneider, and A. Lumsdaine, “Characterizing the in-
ﬂuence of system noise on large-scale applications by simulation,” in
SC’10: Proceedings of the 2010 ACM/IEEE International Conference
for High Performance Computing, Networking, Storage and Analysis.
IEEE, 2010, pp. 1–11.

[8] E. Peise and P. Bientinesi, “A study on the inﬂuence of caching:
Sequences of dense linear algebra kernels,” in International Conference
on High Performance Computing for Computational Science. Springer,
2014, pp. 245–258.

3https://github.com/as641651/AlgorithmRanking.

(a) Instance B

(b) Anomaly

[(q5, q50), (q15, q45), (q20, q40), (q25, q35)]
Fig. 7: Quantiles:
The ranks at (q45, q15) and the mean ranks (in brackets) are
shown.

Recall

performance at the high frequency modes of the processors,
then alg5 shows signiﬁcantly better performance than the
other algorithms. In this case, FLOPs fail to discriminate the
algorithms and the instance will be considered as an anomaly.
the instance (331, 279, 338, 854, 497) which was
observed as an anomaly in [5] and discussed in Sec. I. When
the different frequency modes of the processors are not taken
into account, then all the algorithms are equivalent as they all
obtain rank 1. However, when focusing on the fast frequency
modes, alg2 (which is not the best algorithm based on FLOPs)
shows signiﬁcantly better performance than the algorithms
with minimum FLOPs (see Figure 7b). Now, according to the
methodology, this instance will be considered as an anomaly.
Expression 1 consisted of only 6 variants and we measured
all of them in order to explain the working of the methodology.
However, in practise, compilers such as Linnea generate 100s
of variant algorithms for a given linear algebra expression.
Then, one could ﬁlter the initial set of algorithms and create
a subset consisting of only the potential algorithms before
taking further measurements. In order to test if FLOPs are
a valid discriminant for a given instance of an expression, the
set of potential candidates could be all the algorithms with the
least FLOP count and those algorithms whose relative times
based on single-run execution times (calculated according to
Equation 3) are less than certain threshold (say, 1.5). Then,
the Procedure 4 can be applied on the reduced set, consisting
of only the potential algorithms.

V. CONCLUSION

In this work, we developed a methodology to rank a set of
equivalent algorithms into performance classes. The input to
the methodology is a set of algorithms ranked based on an
initial hypothesis such as FLOP count or single measurement
of execution time of each algorithm. We take further mea-
surements of the algorithms in small steps incrementally, and
update the ranks accordingly. The process of measurements
stops as the updates to ranks converge. To this end, we
developed a strategy to sort algorithms by comparing the
quantiles of the execution time measurements; the ranks of

9

[9] T. Hoeﬂer and R. Belli, “Scientiﬁc benchmarking of parallel comput-
ing systems: twelve ways to tell the masses when reporting perfor-
mance results,” in Proceedings of the international conference for high
performance computing, networking, storage and analysis, 2015, pp. 1–
12.

[10] E. Konstantinidis and Y. Cotronis, “A practical performance model for
compute and memory bound gpu kernels,” in 2015 23rd Euromicro
International Conference on Parallel, Distributed, and Network-Based
Processing.

IEEE, 2015, pp. 651–658.

[11] H. Barthels, C. Psarras, and P. Bientinesi, “Linnea: Automatic
generation of efﬁcient linear algebra programs,” ACM Transactions on
Mathematical Software (TOMS), vol. 47, no. 3, pp. 1–26, Jun. 2021.
[Online]. Available: https://arxiv.org/pdf/1912.12924.pdf

[12] R.

Iakymchuk and P. Bientinesi,

“Execution-less performance
modeling,” in Proceedings of
the second international workshop
on Performance modeling, benchmarking and simulation of high
performance computing systems, 2011, pp. 11–12.

[13] ——,

“Modeling

through memory-stalls,” ACM
SIGMETRICS Performance Evaluation Review, vol. 40, no. 2,
pp. 86–91, 2012.

performance

[14] C. Bischof and C. Van Loan, “The wy representation for products
of householder matrices,” SIAM Journal on Scientiﬁc and Statistical
Computing, vol. 8, no. 1, pp. s2–s13, 1987.

[15] C. Bischof, X. Sun, and B. Lang, “Parallel tridiagonalization through
IEEE Scalable High

two-step band reduction,” in Proceedings of
Performance Computing Conference.

IEEE, 1994, pp. 23–27.

[16] A. Buttari, J. Langou, J. Kurzak, and J. Dongarra, “Parallel tiled qr fac-
torization for multicore architectures,” Concurrency and Computation:
Practice and Experience, vol. 20, no. 13, pp. 1573–1590, 2008.
[17] S. Agarwal, R. Garg, and N. K. Vishnoi, “The impact of noise on
the scaling of collectives: A theoretical approach,” in International
Conference on High-Performance Computing. Springer, 2005, pp. 280–
289.

[18] J. P. S. Alcocer and A. Bergel, “Tracking down performance variation
against source code evolution,” ACM SIGPLAN Notices, vol. 51, no. 2,
pp. 129–139, 2015.

[19] T. Chen, Q. Guo, O. Temam, Y. Wu, Y. Bao, Z. Xu, and Y. Chen,
“Statistical performance comparisons of computers,” IEEE Transactions
on Computers, vol. 64, no. 5, pp. 1442–1455, 2014.

[20] J. Chen and J. Revels, “Robust benchmarking in noisy environments,”

arXiv preprint arXiv:1608.04295, 2016.

[21] T. Hoeﬂer, T. Schneider, and A. Lumsdaine, “Loggopsim: simulating
large-scale applications in the loggops model,” in Proceedings of the
19th ACM International Symposium on High Performance Distributed
Computing, 2010, pp. 597–604.

[22] D. B¨ohme, M. Geimer, L. Arnold, F. Voigtlaender, and F. Wolf, “Identi-
fying the root causes of wait states in large-scale parallel applications,”
ACM Transactions on Parallel Computing (TOPC), vol. 3, no. 2, pp.
1–24, 2016.

[23] E. Peise, D. Fabregat-Traver, and P. Bientinesi, “On the performance
prediction of blas-based tensor contractions,” in International Workshop
on Performance Modeling, Benchmarking and Simulation of High
Performance Computer Systems. Springer, 2014, pp. 193–212.
[24] B. J. Barnes, B. Rountree, D. K. Lowenthal, J. Reeves, B. De Supinski,
and M. Schulz, “A regression-based approach to scalability predic-
tion,” in Proceedings of the 22nd annual international conference on
Supercomputing, 2008, pp. 368–377.

[25] Y. D. Barve, S. Shekhar, A. Chhokra, S. Khare, A. Bhattacharjee,
Z. Kang, H. Sun, and A. Gokhale, “Fecbench: A holistic interference-
aware approach for application performance modeling,” in 2019 IEEE
International Conference on Cloud Engineering (IC2E).
IEEE, 2019,
pp. 211–221.

[26] A. Sankaran and P. Bientinesi, “Performance comparison for scientiﬁc
computations on the edge via relative performance,” in 2021 IEEE
International Parallel and Distributed Processing Symposium Workshops
(IPDPSW).

IEEE, 2021, pp. 887–895.

[27] O. Astrachan, “Bubble sort: an archaeological algorithmic analysis,”

ACM Sigcse Bulletin, vol. 35, no. 1, pp. 1–5, 2003.

10

