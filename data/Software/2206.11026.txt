Test Case Prioritization Using Partial Attention

Quanjun Zhang, Chunrong Fang∗, Weisong Sun, Shengcheng Yu, Yutao Xu, Yulei Liu

State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210093, China

2
2
0
2

n
u
J

2
2

]
E
S
.
s
c
[

1
v
6
2
0
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Test case prioritization (TCP) aims to reorder the regression test suite with a goal of increasing the fault de-
tection rate. Various TCP techniques have been proposed based on diﬀerent prioritization strategies. Among
them, the greedy-based techniques are the most widely-used TCP techniques. However, existing greedy-based
techniques usually reorder all candidate test cases in prioritization iterations, resulting in both eﬃciency and
eﬀectiveness problems. In this paper, we propose a generic partial attention mechanism, which adopts the
previous priority values (i.e., the number of additionally-covered code units) to avoid considering all can-
didate test cases. Incorporating the mechanism with the additional-greedy strategy, we implement a novel
coverage-based TCP technique based on partition ordering (OCP). OCP ﬁrst groups the candidate test cases
into diﬀerent partitions and updates the partitions on the descending order. We conduct a comprehensive
experiment on 19 versions of Java programs and 30 versions of C programs to compare the eﬀectiveness and
eﬃciency of OCP with six state-of-the-art TCP techniques: total-greedy, additional-greedy, lexicographical-
greedy, unify-greedy, art-based, and search-based. The experimental results show that OCP achieves a better
fault detection rate than the state-of-the-arts. Moreover, the time costs of OCP are found to achieve 85% –
99% improvement than most state-of-the-arts.

Keywords: Software testing, Regression testing, Test case prioritization, Greedy algorithm

1. Introduction

During software maintenance and evolution, soft-
ware engineers usually perform code modiﬁcation
due to the ﬁxing of detected bugs, the adding of new
functionalities, or the refactoring of system architec-
ture [1, 2]. Regression testing is conducted to en-
sure that the code modiﬁcation does not introduce
new bugs. However, regression testing can be very
time-consuming because of a large number of reused
test cases [3, 4, 5, 6]. For example, Rothermel et
al. [7] report that it takes seven weeks to run the en-
tire test suite for an industrial project. Besides, with

∗Corresponding author
Email addresses: quanjun.zhang@smail.nju.edu.cn

(Quanjun Zhang), fangchunrong@nju.edu.cn (Chunrong
Fang), weisongsun@smail.nju.edu.cn (Weisong Sun),
yusc@smail.nju.edu.cn (Shengcheng Yu),
MF21320170@smail.nju.edu.cn (Yutao Xu),
1515999248@qq.com (Yulei Liu)

Preprint submitted to The Journal of Systems and Software

the practices of rapid release [8] and continuous in-
tegration [9], the available time for test execution re-
cently keeps decreasing. For example, Memon et
al. [10] report that Google performs an amount of
800K builds and 150M test runs on more than 13K
projects every day, consuming a lot of computing re-
sources.

To address the overhead issues of regression test-
ing, test case prioritization (TCP) has become one
of the most extensively investigated techniques [11,
12]. Generally speaking, TCP reschedules the execu-
tion sequence of test cases in the entire test suite with
the goal of detecting faults as early as possible. Tra-
ditional TCP techniques [13, 14, 15] usually involve
an elementary topic, prioritization strategies, which
incorporate test adequacy criteria (e.g., code cover-
age) to represent diﬀerent behaviours of test cases.
In previous work, the most widely-investigated pri-
oritization strategies are greedy-based strategies [7]

June 23, 2022

 
 
 
 
 
 
(i.e., the total-greedy and additional-greedy strate-
gies), which are generic for diﬀerent coverage cri-
teria. Given a coverage criterion (e.g., statement
or method coverage), the total-greedy strategy se-
lects the next test case yielding the highest cover-
age, whereas the additional-greedy strategy selects
the next test case covering the maximum code units
not covered in previous iterations. The recent em-
pirical results show that although conceptually sim-
ple, the additional-greedy technique has been widely
recognized as one of the most eﬀective TCP tech-
niques on average in terms of fault detection rate
[16, 17, 18, 19, 20, 21].

Compared with the total-greedy strategy,

the
additional-greedy strategy empirically performs out-
standingly due to its feedback mechanism, where the
next test case selection takes into account the eﬀect
of already prioritized test cases [22, 23]. However,
there also exists a shortcoming in the additional-
greedy strategy. Given a regression test suite T with
n test cases, when selecting the i-th test case, the re-
maining n − i + 1 candidate test cases need to be up-
dated. Speciﬁcally, for each candidate test case, all
not-yet-covered code units are examined, of which
those covered by the candidate test case are identi-
ﬁed. The priority values of candidate test cases need
to be measured based on the feedback binary states
of each statement (i.e., covered or not covered). As a
result, the priority values of the candidate test cases
in the previous iterations are lost and need to be re-
calculated in current iteration.

However, due to considering all candidate test
cases in each iteration, the additional-greedy strategy
may suﬀer from the eﬃciency problem. For exam-
ple, consider 3 candidate test cases, expressed as t1,
t2 and t3, in a certain iteration, covering 4, 3 and 2 ad-
ditional statements, respectively. After the test case
t1 is selected, t2 and t3 need to be updated in the next
iteration. Ideally, the remaining test cases can cover
a maximum of 3 additional statements in the next it-
eration, and only test case t2 potentially satisﬁes the
hypothesis. If not, a further hypothesis that both test
cases t2 and t3 have a maximum of 2 additionally-
covered statements is considered, and so on. As a re-
sult, the test cases that cover more statements in the
previous iteration are more likely to maintain the ad-
vantage in the next iteration, as the test cases cannot

cover more statements in the next iteration than in
the previous iteration. For example, updating the test
cases covering no code units in previous iterations is
unnecessary until the prioritization process repeats.
Thus, the additional-greedy strategy, which consid-
ers all candidate test cases at once in each iteration,
may bring redundant calculation in eﬃciency.

Besides, there is a high possibility of tie-occurring
when considering all candidate test cases, may lead
to a decrease performance in the eﬀectiveness. In the
above example, a tie may occur if both t2 and t3 are
considered at once (i.e., t2 and t3 has the highest cov-
erage of statements not yet covered). When facing a
tie, the additional-greedy strategy implicitly assumes
that all remaining candidates are equally important,
and selects one randomly. However, previous em-
pirical studies [23] have shown that the probabil-
ity of ties is relatively high in the additional-greedy
strategy, and the random tie-breaking strategy can be
ineﬀective.
It can be observed that due to consid-
ering all candidate test cases in each iteration, the
additional-greedy strategy suﬀers from the both eﬃ-
ciency and eﬀectiveness problems.

In this paper,

to address the issues mentioned
above, we propose a generic concept, partial atten-
tion mechanism, to avoid considering all candidate
test cases with previous priority values (i.e., the num-
ber of additionally-covered code units) . We also ap-
ply the concept to the additional-greedy strategy and
implement a novel coverage-based TCP technique
based on the notion of partition ordering (OCP). Our
technique pays attention to the partial test cases in-
stead of the whole candidate test set with the help of
priority values calculated in the previous prioritiza-
tion iteration. The key idea of our technique is as fol-
lows: the priority values of the candidate test cases
in the previous iteration can be regarded as a refer-
ence in the next iteration, so as to avoid considering
all candidates at the same time. To implement this
idea, all candidate test cases are classiﬁed into diﬀer-
ent partitions based on their previous priority values.
Then among the candidates that have the highest pri-
ority value in the previous iteration, the one with the
unchanged coverage of not-yet-covered code units is
selected. Likewise, if no test case meets the selection
criterion, test cases with the second highest priority
values are considered, and so on.

2

We perform an empirical study to compare OCP
with six state-of-the-art TCP techniques in terms of
testing eﬀectiveness and eﬃciency on 19 versions
of four Java programs, and 30 versions of ﬁve real-
world Unix utility programs. The empirical results
demonstrate that OCP can outperform state-of-the-
arts in terms of fault detection rate. OCP is also
observed to have much less prioritization time than
most state-of-the-arts (except the total-greedy strat-
egy, a low bound control TCP technique) and the
improvement can reach 85% - 99% on average. We
view our proposed technique as an initial framework
to control the balance of full prioritization and par-
tial prioritization during TCP, and believe more tech-
niques can be derived based on our technique.

In particular, the contributions of this paper are as

follows:

• We propose the ﬁrst notion of the partial atten-
tion mechanism that uses previous priority val-
ues to avoid considering all candidate test cases
in TCP.

• We apply the partial attention mechanism to
the additional-greedy strategy, leading a novel
coverage-based TCP technique based on parti-
tion ordering (OCP).

• We conduct an empirical study to investigate
the eﬀectiveness and eﬃciency of the proposed
technique compared to six state-of-the-art TCP
techniques.

• We release the relevant materials (including
source code, subject programs, test suites and
mutants) used in the experiments for replication
and future research [24].

The rest of this paper is organized as follows. Sec-
tion 2 reviews some background information and
presents a motivation example. Section 3 introduces
the proposed approach. Section 4 presents the re-
search questions, and explains details of the empiri-
cal study. Section 5 provides the detailed results of
the study and answers the research questions. Sec-
tion 6 discusses some related work, and Section 7
discusses the threats to validity of our experiments.
Section 8 presents the conclusions and discusses fu-
ture work.

2. Background & Motivation

In this section, we provide some background infor-
mation about test case prioritization and a motivating
example.

2.1. Test Case Prioritization

Test case prioritization (TCP) [7] aims to reorder
the test cases to maximize the value of an objective
function (e.g., exposing faults earlier [25] or reduc-
ing the execution time cost [26, 27]). TCP problem
is formally deﬁned as follows:

Deﬁnition 1. Test Case Prioritization: Given a test
suite T , PT is the set of its all possible permutations,
and f is an object function deﬁned to map PT to real
numbers R. The problem of TCP [7] is to ﬁnd P(cid:48) ∈
PT , such that ∀P(cid:48)(cid:48), P(cid:48)(cid:48) ∈ PT (P(cid:48)(cid:48) (cid:44) P(cid:48)),
f (P(cid:48)) ≥
f (P(cid:48)(cid:48)).

However, it is infeasible to obtain the fault de-
tection capability of test cases before test execution.
Therefore, some alternative metrics (e.g., structural
coverage), which are in some way correlated with the
fault detection rate, are adopted to guide the priori-
tization process instead [7, 28]. Among all metrics,
code coverage is the most widely used one [16, 29].
Intuitively, once a criterion is chosen, a speciﬁc pri-
oritization strategy is used to order the test cases ac-
cording to the chosen criterion, such as the greedy-
based strategies [25], search-based strategies [30],
and art-based strategies [31].

2.2. A Motivating Example

To better illustrate the details of OCP, Figure 1
shows a piece of code with a fault in line s5, which
can be detected by the test case t4. The code is a
method that computes the greatest common divisor
using the subtract-based version of Euclid’s algo-
rithm [32, 33]. The source code is on the left and
four test cases with their statement coverage infor-
mation are on the right.

Before explaining the details of OCP, we ﬁrst re-
view the steps that the additional-greedy strategy
takes to prioritize the four test cases. In the ﬁrst it-
eration, the additional-greedy strategy chooses the
test case t2 with the maximum coverage. To con-
tinue, in the second iteration, the additional-greedy

3

strategy selects the test case with the maximum cov-
erage of not-yet-covered statements, e.g., s2 and s5.
The additional-greedy strategy updates the coverage
states for all remaining test cases and faces a tie
where both t3 and t4 cover one of the not-yet-covered
In such a case, a random one (e.g., t3
statements.
or t4) will be selected.
In the third iteration, the
additional-greedy strategy searches for the test case,
which yields the maximum coverage of statements
that the ﬁrst and the second test case have not cov-
ered, and t4 or t3 will be selected. In other words,
in each iteration, the additional-greedy strategy se-
lects the test case that provides the maximum cover-
age for the not-yet-covered statements. In the fourth
iteration, the last test case t1 is selected and this pro-
cedure continues until the ordering is complete. As
a result, the test sequence of the additional-greedy
strategy is < t2, t3, t4, t1 > or < t2, t4, t3, t1 > with the
APFD values ranging from 0.375 to 0.625. How-
ever, as discussed in Section 1 the additional-greedy
strategy needs to consider all candidate test cases at
each iteration, which may result in suboptimal per-
formance in eﬀectiveness and eﬃciency. For exam-
ple, in the second iteration, we need to update all the
remaining test cases and also perform a random tie-
breaking.

Figure 1: A motivating example

If OCP is applied to the example, the ﬁrst selected
test case is t2, which is the same as the additional-
greedy strategy. However, in the second iteration,
when facing a tie, OCP prefers the fault-revealing
test case t4 as it covers more statements than t3 in
In the third iteration, t3 is up-
the ﬁrst iteration.
dated ﬁrst as it covers more statements than t1 in the
last iteration and found to cover one not-yet-covered

4

statement s2. As no statement is covered by t1 in
the second iteration, we can observe t1 cannot cover
more statements in the next iteration. Thus, t3 is se-
lected without updating t1, which leads to fewer cal-
culations. As a result, the test sequence of OCP is
< t2, t4, t3, t1 > with the APFD value reaching 0.625,
which may result in a higher fault detection rate
with lower prioritization time. In recent years, the
sizes of the regression test suites of modern indus-
trial systems grow at a fast pace, and existing TCP
techniques (e.g., the additional-greedy strategy and
its follow-ups) have become inadequate in eﬃciency
[16, 34]. However, there exist little work to improve
the additional-greedy strategy eﬃciency while pre-
serving the high eﬀectiveness.

3. Approach

In this section, we introduce the details of test case

prioritization by the partial attention mechanism.

3.1. Partial Attention Mechanism

Although the additional-greedy strategy empiri-
cally performs outstandingly in terms of fault detec-
tion rate, there is a weakness in the feedback mecha-
nism. As discussed in Section 1, considering all can-
didate test cases in each prioritization iteration may
result in redundant calculations and a high probabil-
ity of tie-occurring.
In fact, only the binary states
of the code units (i.e., covered or not covered) are
fed back to the next iteration, and some valuable in-
formation(e.g., the previous priority values) is dis-
carded. In other words, the candidate test cases are
independent of each other before each iteration, and
the loss of previous priority values may lead to a
decrease performance in the eﬀectiveness and eﬃ-
ciency of TCP techniques. As a result, the priority
values of all candidate test cases need to be updated
based on huge calculations.

Thus, to address the problem of considering all
candidate test cases in each iteration, we attempt to
adopt the feedback information from another per-
spective. Speciﬁcally, the priority values in previ-
ous iterations are adopted to pay attention to partial
candidate test cases. The critical insight is that the
number of additionally-covered code units is non-
monotonically decreasing, as it cannot cover more

ID Program Test cases 𝑡!	(0,0) 𝑡"	(0,1) 𝑡#	(1,0) 𝑡$	(1,1) 𝑡%	(0,2) 𝑡&	(2,1) 𝑠! if (a == 0)  √ √ √ √ √ √ 𝑠"     return b; √ √ × × √ × 𝑠# while ( b != 0 ) { × × √ √ × √ 𝑠$     if (a > b) × × × √ × √ 𝑠%         a = a - b; // b=b-a; × × × × × √ 𝑠&     else × × × √ × × 𝑠’         b = b - a;} × × × √ × × 𝑠( return a; × × √ √ × × Result Pass Pass Pass Pass Pass Fail   ID Program Test cases 𝑡!	(1,0) 𝑡"	(1,1) 𝑡#	(0,2) 𝑡$	(2,1) 𝑠! if (a == 0)  √ √ √ √ 𝑠"     return b; × × √ × 𝑠# while (b != 0) { √ √ × √ 𝑠$     if (a > b) × √ × √ 𝑠%         a = a - b; // b = a-b; × × × √ 𝑠&     else    b = b - a;} × √ × × 𝑠’ return a; √ √ × × Result Pass Pass Pass Fail    code units in the next iteration. Then, the prior-
ity values of candidate test cases can be stored in
a well-designed structure (i.e., partition in Section
3.2). Meanwhile, the structure can be maintained in
the next iteration, such that the more important test
cases can be given more attention without additional
calculations. As a result, we propose a concept of the
partial attention mechanism and apply the concept to
state-of-the-art, the additional-greedy strategy.

Suppose that test case ti and t j covers sik and s jk
not-yet-covered statements (sik > s jk) at k-th iter-
ation, respectively. Thus, at the next iteration, ti
is ﬁrst updated and is found to cover si(k+1) state-
ments. There may exist two possible situations: (1)
si(k+1) ≤ s jk: t j needs to be updated and the number of
covered statements is s j(k+1). If si(k+1) equals s j(k+1),
ti is preferred as it covers more statements than t j
in the i−th iteration. Otherwise, the test case cov-
ering more statements in j-th iteration is selected,
which is identical to the additional-greedy strategy.
ti is selected without updating t j.
(2) si(k+1) > s jk:
Suppose the covered statements of selected test case
at k-th iteration and ti are S = {s1, s2, ..., sn} and S (cid:48) =
m}. If S ∪ S (cid:48) = ∅, we have s jk = s jk, oth-
{s(cid:48)
erwise s jk > s jk. Thus, we observe that as the selec-
tion steps iterate, the number of covered statements
should be non-monotonically decreasing (s jk ≥ s jl).
In such case, we can conform si(k+1) > s j(k+1) from
si(k+1) > s jk and s jk ≥ s jl without updating t j.

2, ..., s(cid:48)

1, s(cid:48)

In conclusion, before covering all code units, if
test case ti covers more code units than another test
case t j, ti is more likely to have a higher priority
value. Thus, instead of considering both ti and t j, the
more important one ti should be updated ﬁrst. If ti
covers more code units than the theoretical best pri-
ority value of t j (i.e, the priority value at last itera-
tion), ti will be selected. Otherwise, the remaining t j
is updated and compared with ti.

3.2. Partition Ordering based Prioritization

In our work, we view the partial attention mech-
anism as a general concept that can be applied to
diﬀerent prioritization strategies using diﬀerent cov-
erage criteria. For example, in the lexicographical-
greedy strategy, the test cases covering the code units
fewer covered in the previous iteration should be pre-
ferred, as they have a higher probability to cover

Algorithm 1 Pseudocode of OCP
Input: T :

{t1, t2, · · · , tn} is a set of unordered test cases with size n; U:

{u1, u2, · · · , um} is a set of code units with size m in the program P

(cid:46) the highest priority value

if Cover[i, j] and not UnitCover[ j] then

end if

end for

temp num ← temp num + 1

Candidates ← Candidates (cid:31) (cid:104)ti, priority(cid:105)

temp num ← 0
for each j (1 ≤ j ≤ m) do

end if
if temp num = priority then

Output: S : a set of prioritized test cases
1: S ← ∅
2: Candidates ← ∅
3: priority ← m
4: for each j (1 ≤ j ≤ m) do
5:
UnitCover[ j] ← f alse
6: end for
7: for each i (1 ≤ i ≤ n) do
8:
9: end for
10: while |Candidates| > 0 do
11:
maximum ← −1
12:
for each (cid:104)ti, temp priority(cid:105) (cid:15) Candidates do
if temp priority == priority then
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
end if
43: end while
44: return S

for each j (1 ≤ j ≤ m) do
UnitCover[ j] ← f alse

end for
if priority > 0 then

temp Candidates (cid:31) (cid:104)ti, priority(cid:105)

if |temp Candidates| > 0 then

priority ← priority − 1

UnitCover[ j] ← f alse

end for

end for

end if

end if

end if

else

else

(cid:104)tk, priority(cid:105) ← T ieS elect(temp Candidates)
S ← S (cid:31) (cid:104)tk(cid:105)
Candidates ← S \ (cid:104)tk, priori(cid:105)
for each j (1 ≤ j ≤ m) do

if Cover[i, j] and not UnitCover[ j] then

these code units in the next iteration. As greedy-
based strategies are the most widely-adopted prior-
itization strategies [7, 22], and the additional-greedy
strategy is considered to be one of the most eﬀec-
tive TCP techniques in terms of fault detection ca-
pacity [16, 17, 21, 35, 36]. We apply the partial at-
tention mechanism to the additional-greedy strategy
and implement a simple greedy strategy to instanti-
ate the function based on partition ordering. Gen-
erally speaking, all candidate test cases are grouped
into diﬀerent partitions based on their previous pri-
ority values and higher partitions are updated prefer-
entially.

5

Given a program under test U = {u1, u2, · · · , um}
containing m code units, and a test suite T =
{t1, t2, · · · , tn} containing n test cases, Algorithm 1
describes the pseudocode of our proposed method.
At each iteration, all the candidate test cases are
sorted based on the priority values (i.e., the num-
ber of additionally-covered code units) and the ones
with the same priority value are adjacent to each
other. We then group the candidate test cases ac-
cording to their priority values into p partitions, in-
dexed from left to right by 1, 2, ..., p, such that all
the candidate test cases within a partition have the
same priority value. Based on these partitions, we
form the vector v = [v1, v2, ..., vp] where vi indicates
priority value in the i − th partition. In the next iter-
ation, we then update the candidate test cases in par-
tition p and group them into new partitions accord-
ing to their updated priority values (i.e., the number
of additionally-covered code units in the next itera-
tion).
If there exists a test case from the partition
p that falls into a new partition j (v j > vp−1), the
test case is selected. Because the test cases from the
partition i (i ≤ p − 1) can not be updated into a par-
tition with a higher index j (v j > vp−1). Otherwise,
we update the partition from right to left according
to the value vi until a test case is selected.
In the
worst case, we may update all the partitions if no test
case is selected, which is identical to the additional-
greedy strategy.
Speciﬁcally,

array
use
Cover[i, j] (1 ≤ i ≤ n, 1 ≤ j ≤ m) to iden-
the test case ti covers the code
tify whether
unit u j or not. We use another Boolean array
UnitCover[ j] (1 ≤ j ≤ m) to denote whether
the code unit u j has been covered by the already
selected test cases or not. We set the value of
UnitCover[ j] (1 ≤ j ≤ m) to be f alse. Similarly, we
use a variable priority to denote the largest priority
value for all candidate test cases. Meanwhile, we set
the value of priority to be m. Besides, we use a set
Candidates to denote all remaining test cases and
their corresponding priority values. Initially, we add
the whole test suite to Candidates with the default
priority value m (i.e., the number of code units).

a Boolean

we

to ﬁnd a test case with the given priority value and
add it to the prioritized test set S . In particular, lines
12-24 calculate the sum of units covered by the test
cases with the highest previous priority value and add
the ones that maintain the advantage into the candi-
date test set temp Candidates. Before choosing the
next test case, our approach examines whether or not
there are any code units that are not covered by the
test cases in S . If all code units have been covered,
the remaining candidate test cases are prioritized by
restarting the previous process (lines 39-41). Other-
wise, we select the test case in temp Candidate with
highest previous priority values as the next one and
update the cover status of all code units (lines 26-
37). If no test case is selected, we further update the
second partition, and so on (line 36). This process is
repeated until all test cases in Candidates have been
added to S . It is worth noting that although a par-
tial attention mechanism is adopted in our approach,
there is also a small possibility that a tie occurs,
e.g., more than one test case in temp Candidate with
highest previous priority values. In such a case, sim-
ilar to the additional-greedy strategy, our approach
performs a random tie-breaking.

4. Experiment

In this section, we present our empirical study in
detail, including the research questions, some vari-
ables, subject programs and the experimental setup.

4.1. Research Questions

The empirical study is conducted to answer the

following research questions.

RQ1 How does the eﬀectiveness of OCP compare
in terms of

techniques,

with state-of-the-art
fault detection rate?

RQ2 How does the granularity of code coverage im-
pact the comparative eﬀectiveness of OCP?

RQ3 How does the granularity of test cases impact

the comparative eﬀectiveness of OCP?

In Algorithm 1, lines 1-9 perform initialization,
and lines 10–43 prioritize the test cases. In the main
loop from line 10 to line 43, each iteration attempts

RQ4 How does the eﬃciency of OCP compare with
state-of-the-art techniques, in terms of execu-
tion time?

6

4.2. Independent Variables
4.2.1. Prioritization techniques

Although the proposed generic strategies can work
with any coverage criteria, we implement OCP based
on basic structural coverage criteria due to their pop-
ularity [17, 22, 29, 31, 35]. We select the six state-
of-the-art coverage-based TCP techniques that have
been widely used in previous TCP studies [16, 17,
35]: total-greedy [7], additional-greedy [7], uniﬁed-
greedy [22, 37], lexicographical-greedy [23], art-
based [31], and search-based [30].

The total-greedy technique prioritizes test cases
based on the descending number of code units cov-
ered by those test cases. The additional-greedy tech-
nique chooses each test case from the candidate test
set such that it covers the largest number of code
units not yet covered by the previously selected test
cases. Similarly, the uniﬁed-greedy technique selects
the test case with the highest sum of the probabili-
ties that units covered by the test case contain unde-
tected faults, while the lexicographical-greedy tech-
nique selects the test case with the maximum cover-
age of one-time-covered code units. Likewise, if a
tie occurs, code units that are covered twice are con-
sidered, and so on. The art-based technique selects
each test case from a random candidate test set such
that it has the greatest maximum distance from the
already selected test cases. Finally, the search-based
technique considers all permutations as candidate so-
lutions, and uses a meta-heuristic search algorithm
to guide the search for a better test execution order
[30]. Depending on prioritization strategies, these
TCP techniques are grouped into three categories and
the details are presented in Table 1.

For the total-greedy, additional-greedy, art-based
and search-based techniques, we directly use the
source code released by existing work [20, 38].
the uniﬁed-
the implementation of
Meanwhile,
greedy technique is not publicly available and the
lexicographical-greedy technique is implemented in
other language (i.e., Matlab). Thus, we implement
the uniﬁed-greedy and lexicographical-greedy tech-
niques according to their paper carefully. For the
uniﬁed-greedy technique, we select the basic model
(i.e., Algorithm 1 in [22]) in our work, as the ex-
tended model requires multiple coverage of code
units by given test cases, which is beyond the scope

of our work. We also select the default conﬁguration
(i.e., Algorithm 2 in [23]) for the lexicographical-
greedy technique, as it achieve a great balance be-
tween fault detection rate and prioritization time.

Table 1: Studied TCP techniques

Mnemonic Description
TCPtot
TCPadd
TCPunif
TCPlexi
TCPart
TCPsearch
TCPocp

Category
total-greedy test prioritization
greedy-based
additional-greedy test prioritization
greedy-based
uniﬁed-greedy test prioritization
greedy-based
lexicographical-greedy test prioritization greedy-based
art-based test prioritization
search-based test prioritization
our proposed technique OCP

Reference
[7]
[7]
[22, 37]
[23]
similarity-based [31]
[30]
search-based
This study
greedy-based

4.2.2. Code Coverage Granularity

In traditional TCP studies [16, 22], the coverage
granularity is generally considered to be a constituent
part of the prioritization techniques. To enable suf-
ﬁcient evaluations, we attempt to investigate generic
prioritization strategies with various structural cover-
age criteria (i.e., the statement, branch, and method
coverage granularities).

4.2.3. Test Case Granularity

For the subject programs written in Java, we con-
sider the test case granularity as an additional factor
in the prioritization techniques. Test case granularity
is at either the test-class or the test-method granular-
ity. Speciﬁcally, given a Java program, a JUnit test
class ﬁle refers to a test case at test-class granularity,
while each test method in the ﬁle refers to a test case
at test-method granularity. In other words, a test case
at the test-class granularity generally involves a num-
ber of test cases at the test-method granularity. For C
subject programs, the actual program inputs are the
test cases.

4.3. Dependent Variables

To evaluate the eﬀectiveness of diﬀerent TCP
techniques, we adopt the widely-used APFD (av-
erage percentage faults detected) as the evaluation
metric for fault detection rate [7]. Given a test suite
T , with n test cases, P(cid:48) is a permutation of T . Then
the APFD value for P(cid:48) is deﬁned by the following
formula:

APFD = 1 −

(cid:80)m

i=1 T Fi
n ∗ m

+ 1
2n

(1)

7

where, m denotes the total number of detected faults
and T Fi denotes the position of ﬁrst test case that
reveals the fault i.

4.4. Subject Programs, Test Suites and Faults

To enable suﬃcient evaluations, we conduct our
study on 19 versions of four Java programs (i.e.,
eight versions of ant, ﬁve versions of jmeter, three
versions of jtopas, and three versions of xmlsec),
which are obtained from the Software-artifact Infras-
tructure Repository (SIR) [39, 40]. Meanwhile, 30
versions of ﬁve real-life Unix utility programs writ-
ten in C language (six versions of ﬂex, grep, gzip,
make and sed) are also adopted, which are down-
loaded from the GNU FTP server [41]. Both the Java
and C programs have been widely utilized as bench-
marks to evaluate TCP tchniques [22, 23, 31, 42].
Table 3 lists all the subject programs and the detailed
statistical information. In Table 3, for each program,
columns 3 to 6 summarize the version, size, number
of branches, number of methods, respectively.

Each version of the Java programs has a JUnit test
suite that is developed during the program’s develop-
ment. These test suites have two levels of test-case
granularity: the test-class and the test-method. The
numbers of JUnit test cases are shown in the #Test
column: The data is presented as x (y), where x is the
number of test cases at test-method granularity, and
y is the number of test cases at test-class granularity.
The test suites for the C programs are collected from
the SIR [39, 40]. The number of tests cases in each
suite is also shown in the #Test column of Table 3.

The faults contained in each version of the pro-
grams are produced based on mutation analysis [43,
44]. Although some seeded faults of programs are
available from SIR, previous research has conﬁrmed
that the seeded ones are easily detected and small
in size. Meanwhile, mutation faults have previously
been identiﬁed as suitable for simulating real pro-
gram faults [45, 46, 47, 48, 49] and have been widely
applied to various TCP evaluations [7, 16, 17, 22, 25,
35, 42]. Thus, for both C and Java programs, muta-
tion faults are introduced to evaluate the performance
of the diﬀerent techniques. The details of these op-
erators are presented in Table 2. For C programs,
we obtain the mutants from previous TCP studies
[42, 50], which are produced using seven mutation

Table 2: Statistics on Mutation Operators

Language

Java

C

Operators
CB
IC
IN
MA
NC
VM
ER
FR
TR
NR
PR
SD
UI
CR
AR
LR
BR
RR

Descriptions
conditionals boundary
increments
invert negatives
math
negate conditionals
void method calls
empty returns
false returns
true returns
null returns
primitive returns
statement deletion
unary insertion
constant replacement
arithmetic operator replacement
logical operator replacement
bitwise logical operator replacement
relational operator replacement

operators. For Java programs, we use eleven muta-
tion operators from the “NEW DEFAULTS” group
of the PIT mutation tool [51] to generate mutants.
Speciﬁcally, we generate mutants (i.e., faulty ver-
sions) by seeding all mutation operators into the sub-
ject programs automatically. Then we run the avail-
able test suite against each mutant. The mutant is
killed if there exist any test that produces inconsis-
tent test outcomes between the original and faulty
version, otherwise the mutant is lived. We select all
killed mutants to evaluate the fault detection rate of
TCP techniques.

Meanwhile, according to existing studies [38, 42],
the subsuming mutants identiﬁcation (SMI) tech-
nique [52] is adopted to remove the duplicate and
subsuming mutants from all killed mutants. The
number of subsuming mutants used in our experi-
ment is presented in the #Subsuming Mutant col-
umn. It’s worth noting that the subsuming faults are
classiﬁed as test-class level and test-method level for
the Java programs.

4.5. Framework

Figure 2 presents the overall experimental frame-
work of the proposed technique. (1) We collect the
coverage information for the Java program using the
FaultTracer tool [53, 54], which uses on-the-ﬂy
bytecode instrumentation without any modiﬁcation
of the target program based on the ASM bytecode
manipulation and analysis framework [55]. For C
program, there are six versions of each program P:
PV0, PV1, PV2, PV3, PV4, and PV5. Version PV0 is

8

Language Program

Version KLoC #Branch #Method #Class

Table 3: Subject program details
#Test Case

Java

C

ant v1
ant v2
ant v3
ant v4
ant v5
ant v6
ant v7
ant v8
jmeter v1
jmeter v2
jmeter v3
jmeter v4
jmeter v5
jtopas v1
jtopas v2
jtopas v3
xmlsec v1
xmlsec v2
xmlsec v3
f lex v0
f lex v1
f lex v2
f lex v3
f lex v4
f lex v5
grep v0
grep v1
grep v2
grep v3
grep v4
grep v5
gzip v0
gzip v1
gzip v2
gzip v3
gzip v4
gzip v5
make v0
make v1
make v2
make v3
make v4
make v5
sed v0
sed v1
sed v2
sed v3
sed v4
sed v5

v1 9
1.4
1.4.1
1.5
1.5.2
1.5.3
1.6 beta
1.6 beta2
v1 7 3
v1 8
v1 8 1
v1 9 RC1
v1 9 RC2
0.4
0.5.1
0.6
v1 0 4
v1 0 5D2
v1 0 71
2.4.3
2.4.7
2.5.1
2.5.2
2.5.3
2.5.4
2.0
2.2
2.3
2.4
2.5
2.7
1.0.7
1.1.2
1.2.2
1.2.3
1.2.4
1.3
3.75
3.76.1
3.77
3.78.1
3.79
3.80
3.01
3.02
4.0.6
4.0.8
4.1.1
4.2

25.80
39.70
39.80
61.90
63.50
63.60
80.40
80.40
33.70
33.10
37.30
38.40
41.10
1.89
2.03
5.36
18.30
19.00
16.90
8.96
9.47
12.23
12.25
12.38
12.37
8.16
11.99
12.72
12.83
20.84
58.34
4.32
4.52
5.05
5.06
5.18
5.68
17.46
18.57
19.66
20.46
23.13
23.40
7.79
7.79
18.55
18.69
21.74
26.47

5,240
8,797
8,831
11,743
141,76
141,68
17,164
17,746
3,815
3,799
4,351
4,484
4,888
519
583
1,491
3,534
3,789
3,156
2,005
2,011
2,656
2,666
2,678
2,680
3,420
3,511
3,631
3,709
2,531
2,980
1,468
1,490
1,752
1,610
1,663
1,733
4,397
4,585
4,784
4,845
5,413
5,032
676
712
1,011
1,017
1,141
1,412

2,511
3,836
3,845
5,684
5,802
5,808
7,520
7,524
2,919
2,838
3,445
3,536
3,613
284
302
748
1,627
1,629
1,398
138
147
162
162
162
162
119
104
109
113
102
109
81
81
98
93
93
97
181
181
190
216
239
268
66
65
65
66
70
98

228
342
342
532
536
536
649
650
334
319
373
380
389
19
21
50
179
180
145
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

#T Class
34 (34)
52 (52)
52 (52)
102 (100)
105 (103)
105 (102)
149 (149)
149 (149)
26 (21)
29 (24)
33 (27)
33 (27)
37 (30)
10 (10)
11 (11)
18 (16)
15 (15)
15 (15)
13 (13)

#T Method
137 (135)
219 (214)
219 (213)
521 (503)
557 (543)
559 (537)
877 (866)
879 (867)
78 (61)
80 (74)
78 (77)
78 (77)
97 (83)
126 (126)
128 (128)
209 (207)
92 (91)
94 (94)
84 (84)

500
500
500
500
500
500
144
144
144
144
144
144
156
156
156
156
156
156
111
111
111
111
111
111
324
324
324
324
324
324

#Mutant

#Subsuming Mutant

#All
6,498
11,027
11,142
14,834
17,826
17,808
22,171
22,138
8,850
8,777
9,730
10,187
10,459
704
774
1,906
5,501
5,725
3,833
–
13,873
14,822
775
14,906
14,922
–
23,896
24,518
17,656
17,738
17,108
–
7,429
7,599
7,678
7,838
8,809
–
36,262
38,183
42,281
48,546
47,310
–
2,506
5,947
5,970
6,578
7,761

#Detected #SM Class

59
90
92
192
211
92
284
226
38
37
47
47
53
29
34
57
32
33
27

1,332
2,677
2,661
6,585
6,230
6,255
9,094
9,068
573
867
1,667
1,703
1,651
399
446
1,024
1,198
1,204
1,070
–
6,177
6,396
420
6,417
6,418
–
3,229
3,319
3,156
3,445
3492
–
639
659
547
548
210
–
5,800
5,965
6,244
6,958
7,049
–
1,009
1,048
450
470
628

#SM Method
32
47
47
88
91
91
119
119
20
22
25
25
29
9
10
16
12
12
10

–
32
32
20
33
32
–
56
58
54
58
59
–
8
8
7
7
7
–
37
29
28
29
28
–
16
18
18
19
22

compiled using gcc 5.4.0 [56], and then the cover-
age information is obtained using the gcov tool [57].
(2) After collecting the code coverage information,
we implement all TCP techniques in Java, and apply
them to each program version under study. Speciﬁ-
cally, OCP ﬁrst divides test cases into diﬀerent par-
titions and updates test cases in the highest partition.
If there exist an updated test case satisﬁes the selec-
tion criteria, the test case will be added to the priori-
tized test sequence. Because the approaches contain

randomness, each execution is repeated 1,000 times
independently. This results in, for each testing sce-
nario, 1,000 test sequences for each TCP technique.
(3) To evaluate the fault detection rate, we construct
the faulty programs by mutation faults. Speciﬁcally,
we generate mutants by seeding all mutation opera-
tors (presented in Table 2) and consider each mutant
as a faulty program with only one mutation fault.
We then execute all test cases against each faulty
program and remove the mutants that any test case

9

Figure 2: OCP’s Framework.

(4) Besides, we calculate the APFD
cannot kill.
values and prioritization time for all test sequences
based on the record information (e.g., the mutation
detected results and time cost of each TCP tech-
nique) (5) To further test whether there is a statis-
tically signiﬁcant diﬀerence between OCP and other
TCP techniques, we perform the unpaired two-tailed
Wilcoxon-Mann-Whitney test, at a signiﬁcance level
of 5%, following previously reported guidelines for
inferential statistical analysis involving randomized
algorithms [58, 59]. To identify which technique is
better, we also calculate the eﬀect size, measured by
the non-parametric Vargha and Delaney eﬀect size
measure [60], ˆA12, where ˆA12(X, Y) gives probability
that the technique X is better than technique Y. The
statistical analyses are performed using R language
[61].

4.6. Experimental Setup

The experiments are conducted on a Linux 5.15.0-
25-generic cloud server with eight virtual cores of
Intel(R) Xeon(R) Silver 4116 CPU (2.10 GHz) and
32 GBs of virtual RAM.

5. Results and analysis

This section presents the experimental results to
answer the research questions. We investigate the ef-
fectiveness of OCP to answer RQ1, and perform im-
pact analysis to investigate the inﬂuences caused by

10

the code coverage granularity to answer RQ2. Be-
sides, we also perform analysis to investigate the in-
ﬂuences caused by and test case granularity on OCP
to answer RQ3. Finally, we analyze the time cost of
OCP to answer RQ4.

To answer RQ1 to RQ3, Figures 3 to 6 present
box plots of the distribution of the APFD values
achieved over 1,000 independent runs. Each box
plot shows the mean (square in the box), median
(line in the box), and upper and lower quartiles
(25th and 75th percentile) of the APFD values for
all the TCP techniques.
Statistical analyses are
also provided in Tables 4 to 5 for each pairwise
APFD comparison between OCP and the other
TCP techniques. For ease of illustration, we denote
the mentioned TCP techniques as TCPtot, TCPadd,
TCPlexi, TCPuni f , TCPart, TCPsearch and TCPocp,
respectively. For example, for a comparison be-
tween two methods TCPocp and M, where M ∈
{TCPtot, TCPadd, TCPlexi, TCPuni f , TCPart, TCPsearch},
the symbol (cid:52) means that TCPocp is better (p-value
is less than 0.05, and the eﬀect size ˆA12(TCPocp, M)
the symbol (cid:54) means that
is greater than 0.50);
M is better (the p-value is less than 0.05, and
ˆA12(TCPocp, M) is less than 0.50); and the symbol
(cid:109) means that there is no statistically signiﬁcant
diﬀerence between them (i.e., the p-value is greater
than 0.05).

To answer RQ4, Table 6 provides comparisons of
the execution times for the diﬀerent TCP techniques.

sourcecodecoverage reporttest suitefaultseedingmutation reporttestpartitiontestselectiontestsequencepartition updatinginstrumentationfaultdetectionûüûüûüûüûüüûüüûüûüüûûüûücoveragematrixmeasurementtestexecutionexecutiontime5.1. RQ1: Eﬀectiveness of OCP

In this section, we evaluate the eﬀectiveness of dif-
ferent TCP techniques by fault detection rate. We
provide the APFD results for OCP with diﬀerent
code coverage criteria and test case granularities.
Figures 3 to 5 show the APFD results for the C pro-
grams, the Java programs at the test-method granu-
larity and the test-method granularity, respectively.
Each sub-ﬁgure in these ﬁgures has the seven TCP
techniques across the x-axis, and corresponding to
the APFD values on the y-axis. Table 4 presents the
corresponding statistical comparisons. Each row de-
notes the statistical results for the corresponding pro-
gram under diﬀerent coverage criteria. Column ”C
Programs”, ”Java-M Programs”, ”Java-C Programs”
and”All Programs” are calculated based on all APFD
values for C programs, Java programs at the test-
method granularity, Java programs at the test-class
granularity and all programs.

5.1.1. C Subject Programs

Based the results on Figure 3 and Table 4, we make

the following observations:

When comparing TCPocp with the greedy-based
strategies, our proposed TCPocp approach has much
better performance than TCPtot, TCPadd, TCPuni f y
and TCPlexi for all programs and code coverage gran-
ularities, except for make with branch coverage (for
which TCPocp has very similar, or better perfor-
mance). The maximum diﬀerence in mean and me-
dian APFD values between TCPocp and TCPtot is
more than 40%, while between TCPocp and TCPadd,
it is about 10%.

Our proposed technique TCPocp has similar or bet-
ter APFD performance than TCPart and TCPsearch
f lex and gzip),
for some subject programs (e.g.,
with all code coverage granularities, but has slightly
worse performance for some others (e.g., make and
sed). However, the diﬀerence in mean and me-
dian APFD values between TCPocp and TCPart or
TCPsearch is less than 5%.

Furthermore, the statistical results support the box
plot observations. All p-values for the compar-
isons between TCPocp and the greedy-based strate-
gies (i.e., TCPtot or TCPadd) are less than 0.05 (ex-
cept for make with branch coverage), indicating that
their APFD scores are signiﬁcantly diﬀerent. The

ˆA12 values are also much greater than 0.50, ranging
from 0.51 to 1.00. However, although all p-values
between TCPocp and TCPart or TCPsearch are also less
than 0.05, their ˆA12 values are much greater than 0.50
in some cases, but much less than 0.50 in others.
Nevertheless, considering all the C programs, not
only does TCPocp have signiﬁcantly diﬀerent APFD
values to the other six TCP techniques, but it also
has better performances overall (except for TCPart
and TCPsearch).

5.1.2. Java Programs at Test-Method Granularity

Based on Figure 5 and Table 4, we have the fol-

lowing observations:

Compared with the greedy-based strategies,
TCPocp performs much better than TCPtot, regard-
less of subject program and code coverage granu-
larity, with the maximum mean and median APFD
diﬀerences reaching about 12%. TCPocp also has
very similar performance to TCPadd, with the mean
and median APFD diﬀerences approximately equal
to 1%. However, none of the other two techniques
(TCPlexi and TCPuni f y) is either always better or al-
ways worse than TCPocp, with TCPocp sometimes
performing better for some programs, and sometimes
worse.

TCPocp also performs better than TCPart and
TCPsearch at most cases (except jtopas at statement
and branch coverage) There is a statistically signif-
icant diﬀerence between TCPocp and TCPart, which
supports the above observations.

Furthermore, the statistical results support the box
plot observations. Considering all Java programs,
TCPocp performs better than TCPtot, TCPart and
TCPsearch, as most p-values are less than 0.05, and
the relevant eﬀect size ˆA12 ranges from 0.58 to 0.98.
However, OCP has very a similar (or slightly worse)
performance to TCPadd, with ˆA12 values of either
0.48 or 0.50.

5.1.3. Java Programs at Test-Class Granularity

Based on Figure 4 and Table 4, we have the fol-

lowing observations:

OCP achieves higher mean and median APFD val-
ues than TCPtot for most cases, except jmeter. OCP
has a very similar performance to TCPadd, with their
mean and median APFD diﬀerences at around 1%.

11

(a) statement coverage

(b) branch coverage

(c) method coverage

Figure 3: APFD results for C programs

(a) statement coverage

(b) branch coverage

(c) method coverage

Figure 4: APFD results for Java programs at test-method granularity

OCP has a competitive performance with TCPuni f y
and TCPlexi for all programs with diﬀerent code
coverage granularities. OCP achieves much higher
mean and median APFD values than TCPart for most
cases, for all programs with all code coverage gran-
ularities, with the maximum diﬀerences reaching ap-
proximately 10%. Other than for a few cases (e.g.,
jtopas), OCP usually has better performance than
TCPsearch.

Furthermore, the statistical analysis supports the
above box plots observations.
Considering all
Java programs together, OCP proforms better than
TCPtot, TCPuni f y, TCPsearch, TCPart, and TCPsearch
on the whole. Most p-values are less than 0.05,
indicating that their diﬀerences are signiﬁcant; and
the eﬀect size ˆA12 values range up to 1.00, which
means that TCPocp is better than the other ﬁve TCP
techniques. Finally, while the p-values for compar-
isons between TCPocp and TCPadd are less than 0.05
(which means that the diﬀerences are insigniﬁcant),
the ˆA12 values range from 0.49 to 0.51, indicating
that they are very similar.

Answer to RQ1: Overall, our analysis on the fault
detection eﬀectiveness that (1) For C programs,
OCP has signiﬁcantly better performance than all
greedy-based strategies and maintaining the com-
parable performance with TCPart and TCPsea.
(2) For Java programs at test-method granularity,
OCP has better performance than TCPtot, TCPart
and TCPsea, while has similar performance with
TCPadd, TCPuni f y and TCPlexi. (3) For Java pro-
grams at test-class granularity, OCP has better
or similar performance with TCPadd, TCPart and
TCPsea for all programs, while has comparable per-
formance with TCPtot, TCPuni f y and TCPlexi.

5.2. RQ2: Impact of Code Coverage Granularity

In our study, three basic structural coverage crite-
ria (i.e., statement, branch and method) are adopted
to evaluate the performance of proposed TCP tech-
niques.
Previous empirical studies have demon-
strated that diﬀerent code coverage granularities may
aﬀect the APFD results [16, 38]. Thus, in this sec-
tion, we examine how the selection of code coverage
granularity inﬂuences the eﬀectiveness of OCP.

12

TotAddLexUniARTSeaOCP0.50.60.70.80.91.0APFDTotAddLexUniARTSeaOCP0.50.60.70.80.91.0APFDTotAddLexUniARTSeaOCP0.50.60.70.80.91.0APFDTotAddLexUniARTSeaOCP0.40.50.60.70.80.9APFDTotAddLexUniARTSeaOCP0.30.40.50.60.70.80.9APFDTotAddLexUniARTSeaOCP0.40.50.60.70.80.9APFD(a) statement coverage

(b) branch coverage

(c) method coverage

Figure 5: APFD results for Java programs at test-class granularity

Table 4: Statistical eﬀectiveness comparisons of APFD for all programs.

Program Name

Statement Coverage

Branch Coverage
TCPadd TCPunify TCPlexi

Method Coverage
TCPadd TCPunify TCPlexi

–

TCPtot

TCPart

TCPart

TCPsearch

TCPsearch

TCPadd TCPunify TCPlexi

gnu f lex
gnu make
gnu grep
gnu gzip
gnu sed
C Programs
ant
jtopas
jmeter
xmlsec

TCPtot
TCPsearch
(cid:52) (1.00) (cid:52) (0.52) (cid:52) (0.70) (cid:52) (0.87) (cid:52) (0.86) (cid:52) (0.73) (cid:52) (1.00) (cid:52) (0.66) (cid:54) (0.22) (cid:52) (0.90) (cid:52) (0.72) (cid:52) (0.57) (cid:52) (1.00) (cid:52) (0.70) (cid:52) (0.71) (cid:52) (0.85) (cid:54) (0.28) (cid:54) (0.47)
(cid:52) (0.85) (cid:52) (0.61) (cid:52) (0.62) (cid:52) (0.82) (cid:54) (0.30) (cid:54) (0.29) (cid:52) (1.00) (cid:109) (0.51) (cid:52) (0.77) (cid:52) (0.87) (cid:54) (0.28) (cid:54) (0.42) (cid:52) (0.58) (cid:52) (0.68) (cid:52) (0.68) (cid:52) (0.58) (cid:54) (0.22) (cid:54) (0.24)
(cid:52) (1.00) (cid:52) (0.57) (cid:52) (0.79) (cid:52) (1.00) (cid:54) (0.48) (cid:52) (0.64) (cid:52) (1.00) (cid:52) (0.54) (cid:52) (0.67) (cid:52) (1.00) (cid:109) (0.50) (cid:52) (0.66) (cid:52) (1.00) (cid:52) (0.71) (cid:52) (0.99) (cid:52) (1.00) (cid:54) (0.21) (cid:54) (0.25)
(cid:52) (0.80) (cid:52) (0.66) (cid:52) (0.78) (cid:52) (0.58) (cid:54) (0.43) (cid:54) (0.36) (cid:52) (0.84) (cid:52) (0.70) (cid:52) (0.83) (cid:109) (0.49) (cid:54) (0.37) (cid:54) (0.32) (cid:52) (0.54) (cid:52) (0.53) (cid:52) (0.54) (cid:52) (0.53) (cid:54) (0.47) (cid:54) (0.47)
(cid:52) (1.00) (cid:52) (0.54) (cid:52) (0.64) (cid:52) (1.00) (cid:54) (0.15) (cid:54) (0.37) (cid:52) (1.00) (cid:52) (0.55) (cid:52) (0.64) (cid:52) (1.00) (cid:54) (0.14) (cid:54) (0.37) (cid:52) (1.00) (cid:52) (0.90) (cid:52) (0.97) (cid:52) (0.97) (cid:54) (0.22) (cid:54) (0.29)
(cid:52) (0.91) (cid:52) (0.54) (cid:52) (0.60) (cid:52) (0.74) (cid:54) (0.44) (cid:54) (0.47) (cid:52) (0.93) (cid:52) (0.55) (cid:52) (0.60) (cid:52) (0.77) (cid:54) (0.43) (cid:54) (0.47) (cid:52) (0.81) (cid:52) (0.61) (cid:52) (0.66) (cid:52) (0.74) (cid:54) (0.37) (cid:54) (0.41)
(cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.42) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.40) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.22) (cid:52) (0.98) (cid:52) (0.96) (cid:52) (0.95)
(cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.36) (cid:54) (0.39) (cid:54) (0.00) (cid:54) (0.44) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.38) (cid:52) (1.00) (cid:54) (0.00) (cid:54) (0.44) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.03) (cid:52) (0.66) (cid:52) (0.56) (cid:109) (0.50)
(cid:52) (1.00) (cid:109) (0.50) (cid:109) (0.50) (cid:54) (0.38) (cid:52) (1.00) (cid:52) (0.62) (cid:52) (1.00) (cid:54) (0.48) (cid:52) (0.61) (cid:109) (0.51) (cid:52) (1.00) (cid:52) (0.69) (cid:52) (0.86) (cid:109) (0.50) (cid:54) (0.47) (cid:54) (0.45) (cid:52) (0.98) (cid:52) (0.58)
(cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.39) (cid:52) (1.00) (cid:52) (1.00) (cid:109) (0.49) (cid:52) (1.00) (cid:109) (0.49) (cid:54) (0.13) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (0.77) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.36) (cid:52) (0.99) (cid:52) (1.00) (cid:52) (0.55)
Java-M Programs (cid:52) (0.98) (cid:109) (0.50) (cid:54) (0.48) (cid:52) (0.67) (cid:52) (0.70) (cid:52) (0.65) (cid:52) (0.94) (cid:109) (0.50) (cid:54) (0.45) (cid:52) (0.70) (cid:52) (0.64) (cid:52) (0.66) (cid:52) (0.98) (cid:109) (0.50) (cid:54) (0.39) (cid:52) (0.71) (cid:52) (0.77) (cid:52) (0.64)
(cid:52) (0.83) (cid:109) (0.50) (cid:52) (0.52) (cid:52) (0.70) (cid:52) (0.99) (cid:52) (0.60) (cid:52) (0.89) (cid:109) (0.50) (cid:52) (0.51) (cid:52) (0.69) (cid:52) (1.00) (cid:52) (0.54) (cid:52) (0.94) (cid:109) (0.50) (cid:54) (0.48) (cid:52) (0.71) (cid:52) (1.00) (cid:52) (0.57)
(cid:109) (0.50) (cid:109) (0.50) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (0.93) (cid:109) (0.50) (cid:109) (0.50) (cid:109) (0.50) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (0.91) (cid:109) (0.50) (cid:54) (0.33) (cid:109) (0.50) (cid:109) (0.50) (cid:54) (0.33) (cid:52) (0.94) (cid:109) (0.50)
(cid:54) (0.36) (cid:109) (0.50) (cid:54) (0.46) (cid:54) (0.36) (cid:52) (1.00) (cid:109) (0.50) (cid:54) (0.40) (cid:54) (0.47) (cid:52) (0.52) (cid:54) (0.40) (cid:52) (1.00) (cid:52) (0.52) (cid:54) (0.31) (cid:109) (0.50) (cid:54) (0.48) (cid:54) (0.32) (cid:52) (1.00) (cid:109) (0.49)
(cid:52) (1.00) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (1.00) (cid:52) (1.00) (cid:109) (0.50) (cid:52) (0.97) (cid:54) (0.48) (cid:54) (0.40) (cid:52) (0.96) (cid:52) (1.00) (cid:109) (0.50) (cid:52) (1.00) (cid:109) (0.51) (cid:52) (0.60) (cid:52) (1.00) (cid:52) (1.00) (cid:52) (0.58)
Java-C Programs (cid:52) (0.63) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (0.55) (cid:52) (0.98) (cid:52) (0.53) (cid:52) (0.65) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (0.56) (cid:52) (0.97) (cid:52) (0.51) (cid:52) (0.67) (cid:109) (0.50) (cid:109) (0.50) (cid:52) (0.58) (cid:52) (0.98) (cid:52) (0.52)
(cid:52) (0.71) (cid:52) (0.51) (cid:52) (0.51) (cid:52) (0.56) (cid:52) (0.57) (cid:52) (0.51) (cid:52) (0.73) (cid:52) (0.51) (cid:52) (0.51) (cid:52) (0.57) (cid:52) (0.55) (cid:52) (0.51) (cid:52) (0.69) (cid:52) (0.52) (cid:52) (0.51) (cid:52) (0.57) (cid:52) (0.57) (cid:109) (0.50)

ant
jtopas
jmeter
xmlsec

All Programs

d
o
h
t
e
m

TCPart

TCPtot

s
s
a
l
c

Figure 6 presents the APFD results of OCP for the
three types of code coverage, according to the sub-
ject programs’ language or test case granularity. The
language or test case granularity is shown on the x-
axis and the APFD scores on the left y-axis. It can be
observed that for C programs, statement and branch
coverage are very considerable, and are more eﬀec-
tive than method coverage. However, for Java pro-
grams, they have similar performance.

Table 5 presents a comparison of

the mean

Figure 6: Eﬀectiveness: APFD results with diﬀerent code cov-
erage and test case granularities for all programs

13

and median APFD values, and also shows the p-
values/eﬀect size ˆA12 for the diﬀerent code coverage
granularity comparisons. Column ”C”, ”Java (test-
method)”, ”Java (test -class)” and ”All” is calculated
based on all APFD values for C programs, Java pro-
grams at the test-method granularity, Java programs
at the test-class granularity and all programs. It can
be observed that the APFD values are similar, with
the maximum mean and median value diﬀerences be-
ing less than 3%, and less than 8%, respectively. Ac-
cording to the statistical comparisons, there is no sin-
gle best code coverage type for OCP, with each type
sometimes achieving the best results. Nevertheless,
branch coverage appears slightly more eﬀective than
statement and method coverage for OCP.

Answer to RQ2: Overall, our analysis on the code
coverage granularity reveals that the code cover-
age granularity may only provide a small impact
on OCP testing eﬀectiveness, with branch cover-
age possibly slightly outperforming statement and
method coverage.

TotAddLexUniARTSeaOCP0.350.400.450.500.550.600.65APFDTotAddLexUniARTSeaOCP0.350.400.450.500.550.600.65APFDTotAddLexUniARTSeaOCP0.350.400.450.500.550.60APFDCJava (test-method)Java (test-class)0.40.50.60.70.80.91.0APFDstatementmethodbranchTable 5: Statistical eﬀectiveness comparisons of APFD between diﬀerent coverage granularities for OCP.
Median

Comparison

Mean

Metric

APFD

Language

C
Java (test-method)
Java (test-class)
All

Statement Branch Method Statement Branch Method Statement vs Branch Statement vs Method Branch vs Method
0/0.65
1.39E-137/0.58
1.67E-94/0.56
2.2E-113/0.54

1.65E-15/0.48
7.13E-97/0.43
4.34E-6/0.43
1.8E-24/0.48

0/0.62
2.04E-3/0.51
4.34E-06/0.49
6.71E-34/0.52

0.87
0.69
0.54
0.70

0.89
0.73
0.54
0.75

0.88
0.69
0.54
0.72

0.87
0.68
0.54
0.71

0.90
0.69
0.55
0.73

0.89
0.68
0.54
0.72

5.3. RQ3: Impact of Test Case Granularity

5.4. RQ4: Eﬃciency of OCP

In our study, the Java programs have two granular-
ities of test cases (i.e., the test-class and test-method).
Following to previous studies [22, 38], we also con-
sider the test case granularity as a factor in the eval-
uation. Thus, in this section, we also investigate how
the test case granularity inﬂuence the eﬀectiveness of
OCP.

The comparisons are presented in Figure 6. OCP
usually has signiﬁcantly lower average APFD values
for prioritizing test cases at the test-class granularity
than at the test-method granularity.

Table 5 presents the statistical eﬀectiveness com-
parisons of APFD between diﬀerent granularities for
OCP. Each cell in the Mean, Median, and Compar-
ison columns represents the mean APFD value, the
median value, and the p-values/eﬀect size ˆA12 for the
diﬀerent code coverage granularity comparisons, re-
spectively. Considering all the Java programs, as can
be seen in Table 5, the mean and median APFD val-
ues at the test-method granularity are much higher
than at the test-class granularity with all code cov-
erage granularities. In fact, the test case at the test-
class granularity consists of a number of test cases at
the test-method granularity. For example, there exist
1000 test cases at the test-class granularity and more
than 5000 test cases at test-method granularity for
Java programs at Table 3, resulting in a much larger
number of test cases at the test-method granularity.
Thus, the permutation space of candidate test cases
at the test-method granularity may be greater, which
leads to a better fault detection rate [22].

Answer to RQ3: Overall, our analysis on the test
case granularity reveals that OCP has better eﬀec-
tiveness performance when prioritizing test cases
at the test-method granularity than at the test-class
granularity in terms of fault detection rate.

14

In this section, to evaluate the eﬃciency of OCP,
we calculate the execution time for all TCP tech-
niques. Table 6 presents the statistics about time
costs (i.e., the preprocessing time and prioritization
time) for all subject programs and studied TCP tech-
niques.

Speciﬁcally, the preprocessing time contains the
compilation time for executing the program and the
instrumentation time for collecting the coverage in-
formation. Thus the preprocessing time of the sub-
ject programs is the same for diﬀerent TCP tech-
niques and is not presented. Apart from the ﬁrst two
columns that display the program name and the pro-
gramming language it belongs to, each cell in the ta-
ble shows the mean prioritization time over the 1,000
independent runs using each TCP technique.

As discussed in Section 4, the Java programs have
each version individually adapted to collect the code
coverage information, with diﬀerent versions using
diﬀerent test cases. Thus, the prioritization time is
collected for each Java program version. In contrast,
each PV0 version of the C programs is compiled and
instrumented to collect the code coverage informa-
tion for each test case, and all program versions use
the same test cases. Thus, each C program version
has the same prioritization time. As a result, we
present the time costs for each Java program version
and C Program. Furthermore, because all the studied
TCP techniques prioritize test cases after the cover-
age information is collected, they are all deemed to
have the preprocessing time.

Based on the time costs, we have the follow-
ing observations:
(1) As expected, the time costs
for all TCP techniques (including OCP) are lowest
with method coverage, followed by branch, and then
statement coverage for all programs. As shown in
Table 3, the number of methods is much smaller than
the number of branches, which in turn is smaller than
the number of statements. Thus, the coverage infor-

p
c
o
P
C
T

h
c
r
a
e
s
P
C
T

t
r
a
P
C
T

i
n
u
P
C
T

x
e
l
P
C
T

d
d
a
P
C
T

t
o
t
P
C
T

p
c
o
P
C
T

h
c
r
a
e
s
P
C
T

t
r
a
P
C
T

i
n
u
P
C
T

x
e
l
P
C
T

d
d
a
P
C
T

t
o
t
P
C
T

p
c
o
P
C
T

h
c
r
a
e
s
P
C
T

t
r
a
P
C
T

i
n
u
P
C
T

x
e
l
P
C
T

d
d
a
P
C
T

t
o
t
P
C
T

e
g
a
r
e
v
o
C
d
o
h
t
e

M

e
g
a
r
e
v
o
C
h
c
n
a
r
B

e
g
a
r
e
v
o
C

t
n
e
m
e
t
a
t
S

.
s
e
u
q
i
n
h
c
e
t
P
C
T

t
n
e
r
e
ﬀ
i
d

r
o
f

s
d
n
o
c
e
s
i
l
l
i

m
n
i

s
t
s
o
c

n
o
i
t
u
c
e
x
e

f
o

s
n
o
s
i
r
a
p
m
o
C

:
y
c
n
e
i
c
ﬃ
E

:
6
e
l
b
a
T

m
a
r
g
o
r
P

e
g
a
u
g
n
a
L

7
8
.
1
0
1

9
0
.
4
3
4
5
6

3
2
.
4
9
5
1
1
1

8
3
.
1
1
4

6
3
.
8
1
1
1

7
9
.
2
5
3

1
4
.
4

6
5
.
1
6

6
1
.

9
7
5
4
0
1

.

1
8
3
1
1
9
1

.

6
1
6
6
6

.

2
6
6
4
9
1

.

7
6
3
3
5

9
7
7

.

.

7
0
7
7
1

.

5
8
6
6
6
3
0
4

.

8
8
1
1
5
1
3
5

.

4
9
0
7
4
2

7
7
.
8
2
9
6

9
4
.
7
3
9
1

6
2
.
0
3

5
7
.
8
2
1

4
1
.
3
8
2
5
7

5
8
.
9
9
1
6
3
1

8
.
0
0
5

5
3
.
9
2
3
1

1
7
.
4
2
4

1
1
.
5

1
3
.
1
7

2
8
.

3
9
1
5
1
1

.

4
0
0
5
0
2
2

.

1
8
5
9
7

.

6
6
1
3
2

.

6
1
8
4
6

8
5
8

.

.

4
6
0
1
2

.

2
7
1
0
6
2
6
4

.

8
8
5
1
4
1
0
7

.

5
8
0
8
9
2

4
9
.
9
5
2
8

8
.
9
3
3
2

1
5
.
3
3

4
0
.
5
2
1

7
5
.
7
2
8
3
7

2
.
6
0
8
7
2
1

7
.
8
8
4

7
.
4
9
2
1

1
8
.
7
1
4

8
9
.
4

9
1
.
0
7

3
5
.

7
6
9
2
0
1

.

7
4
8
5
8
8
1

.

2
8
9
7
7

.

2
1
1
7
2
2

.

4
5
5
3
6

4
0
9

.

.

7
2
4
0
2

.

1
0
9
9
7
2
6
4

.

9
4
3
8
0
5
7
6

.

7
9
9
8
2

8
3
.
6
5
0
8

2
2
.
1
6
2
2

8
9
.
2
3

4
8
.
6
8
4

9
3
.
3
5
2
5
3
1

9
4
.
6
9
2
7
9
6

5
4
.
3
1
9
1

9
5
.
3
9
7
4

9
6
.
7
7
5
1

4
8
.
2
1

6
3
.
2
2
2

9
1
.

1
8
2
2
0
3

.

6
7
0
7
2
3
0
1

.

6
2
0
3
0
3

.

9
2
6
7
2
8

.

7
9
4
4
2

4
5
1
2

.

.

8
8
0
6

.

2
1
9
8
5
3
4
2
1

.

5
6
4
2
7
2
8
5
3

.

2
1
9
1
7
0
1

9
3
.
3
6
8
8
2

3
8
.
7
4
4
6

9
3
.
5
7

6
6
.
9
7
5

5
7
.
7
5
2
5
7
1

4
3
.
1
2
4
1
0
7

8
0
.
1
1
9
1

7
8
.
2
3
8
4

3
2
.
7
3
6
1

3
1
.
2
1

8
6
.
5
2
2

8
2
.

0
1
0
6
5
2

.

4
5
4
3
4
0
1

.

9
2
5
0
3

.

4
6
2
7
3
8

.

4
5
8
9
1

1
1
0
2

.

.

9
3
7
3
6

.

6
4
2
2
5
2
2
1

.

9
5
8
4
7
8
8
5
3

.

5
1
5
4
6
0
1

1
2
.
1
7
0
9
2

7
8
.
6
7
5
6

5
1
.
1
7

8
7
.
2
1

2
2
.
1
0
0
3

5
.
3
5
3

6
2
.
1
3

7
8
.
5
4

1
3
.
0
6

3
7
.
2
1

7
.
4
2
4
2

5
5
.
3
1
2

4
5
.
7
1

7
2
.
7
2

4
9
.
8
3

0
0
.
2

6
6
.
1

1
0
.
4

2
2
.
2

4
2
.
8

1
4
.
8

4
8
.
9
4
3

3
5
.
5
0
2

2
7
.
6

7
.
5

6
1
.
6
6
7
1

2
4
.
4
8

8
7
.
8
1
0
9

9
8
.
4
2
1
1

5
8
.
1

5
2
.
2

5
2
.
7

9
6
.
8

1
8
.
1

4
3
.
2

1
7
.
2

5
2
.
1

9
7
.
2
1

9
1
.
5
1

9
4
.
8
2

2
1
.
7

9
7
.
8
6
0
1
2

3
3
.
5
1
3
6

2
9
.
2
3

5
9
.
3
0
1

7
0
.
2
3

8
.
2
3
8
5
1

8
1
.
1
8
2
6

3
8
.
2
3

1
0
.
4
0
1

6
3
.
2
3

0
6
.
0

5
4
.
0

6
0
.
0

6
0
.
0

6
1
.
0

9
2
.
0

8
7
.
0

4
8
.
0

9
.
6

9
.
7

6
3
.
2

7
7
.
6

5
8
.
6

3
9
.
4

8
6
.
6
0
4

2
4
6
4

.

6
7
3
1

.

8
3
4
1

.

9
6
.
2
4
4

.

6
1
6
0
1

5
9
5
2

.

1
9
7
2

.

3
8
.
9
4
9
1

.

3
6
9
5
8

1
7
8
3

.

9
5
1
7

.

6
7
.
2
5
8
8

.

6
5
6
5

9
0
2
1

.

5
4
2
4

.

4
1
.
2
8
3
6
2

.

6
8
5
4
6
1

.

6
1
5

.

4
3
4
6
1

5
0
4
4

.

5
.
3
2
6
0
2

.

4
4
8
1
6
1

7
4
0
5

.

.

8
0
4
6
1

3
4
5
4

.

1
7
5

.

1
9
8

.

.

0
7
3

7
1
9

.

6
2
0

.

5
7
0

.

9
4
0

.

9
4
0

.

4
3
1

.

1
2
1

.

4
3
8

.

.

9
8
3
4

5
2
3
1

.

.

3
2
6
5
5

9
4
3
6

.

.

7
7
1
0
1

8
7
2
1

.

.

4
6
2
5
9
1

.

4
5
7
9

5
8
6

.

.

9
5
0
9
4
9
2

.

1
8
5
8
1
6

.

1
2
2

6
3
0
4

.

7
7
6
7

.

1
0
7
5

.

8
6
.
9
1

8
7
.
0
4

9
2
.
8

7
0
.
3
1

3
9
.
7
1
1

4
5
.
6
4

2
5
.
8
5
1

4
0
.
0
4

9
4
.
0

2
8
.
0

5
7
.
0

5
7
.
1

7
1
2
2

.

.

6
8
5
7
3
5
3

.

8
9
0
2
1
7
3

.

2
5
0
2
2

3
0
.
8
1
6

5
9
.
9
7
1

9
.
4

3
0
2
2

.

.

2
3
7
1
2
5
3

.

6
5
5
7
5
6
3

.

6
6
7
1
2

8
0
.
6
1
6

4
6
.
1
8
1

8
9
.
4

3
.
9
7

9
6
.

6
4
8
3

.

1
2
6
7
4
7

.

6
0
5
6
3

.

8
4
5
6
5

.

3
8
4
7
2

6
6
2

.

.

9
6
8
3
1

.

3
2
9
8
1
4

.

7
7
3
9
7

.

8
9
7
8
6

3
0
.
8
2
0
1

5
.
8
6
3

1
3
.
4

x
e
l

f

u
n
g

9
3
.
1
4

7
3
.

9
7
9
2

.

9
3
2
4
4
5

.

9
0
6
3
2

.

6
7
9
9
3

.

3
3
1
6
1

2
0
2

.

8
1
8
4

.

.

7
0
4
3
8
2

.

9
7
9
7
8
3

.

4
4
2
1
3

.

2
9
6
4

6
0
.
4
6
1

5
.
2

p
e
r
g

u
n
g

1
v

t
n
a

2
v

t
n
a

3
v

t
n
a

4
v

t
n
a

5
v

t
n
a

6
v

t
n
a

7
v

t
n
a

8
v

t
n
a

p
i
z
g

u
n
g

e
k
a
m
u
n
g

d
e
s

u
n
g

C

6
3
.
5
3

2
6
.
3
2
2
8
1

4
7
.
1
2
2
2
4

6
.
9
2
1

1
8
.
7
3
3

7
1
.
1
1
1

4
2
.
1

7
4
.
0
2

2
3
.

1
3
4
0
3

.

3
8
7
1
5
7

.

5
8
1
2
2

.

1
0
1
1
6

.

1
8
6
6
1

4
2
2

.

1
7
4
5

.

.

5
4
4
2
9
6
1
1

.

4
9
4
8
0
2
2
2

.

6
0
4
6
7

5
4
.
1
9
0
2

2
.
5
0
5

8
9
.
1

4
5
.
2

6
7
.
6

4
4
.
0

6
8
.
0

7
9
.
0

1
0
.
1

0
2
.
1

6
1
.
1

0
3
.
1

3
0
.
1

3
3
.
0

2
7
.
0

0
7
.
0

0
9
.
2

2
1
.
3

2
0
.
3

8
4
.
7

7
1
.
7

4
0
.
0

5
0
.
0

3
1
.
0

9
0
.
0

8
1
.
0

5
2
.
0

5
2
.
0

9
2
.
0

0
1
.
0

1
1
.
0

8
0
.
0

6
2
.
8
1
1
3

1
6
.
8
2
5

8
7
.
0
4
5
8

8
6
.
6
7
5

4
.
3

9
.
3

5
.
4
9
9
2
3

4
1
.
5
5
9
3

7
7
.
6
1

2
6
.
3
1
0
5

2
1
.
8
6

9
3
.
4
7
3
7

6
6
.
5
8
1

2
.
8
3
9
2

8
7
.
5
9
2

2
8
.
4
3
9
1

6
5
.
3
0
3

6
6
.
1
0
0
2

7
7
.
7
7
3

6
3
.
5
3
4
1

6
8
.
9
3
3

8
0
.
3
6
3
1

3
9
.
5
7
3

4
2
.
0
5
0
1

3
.
9
5
2

2
6
.
2
2
9

2
.
5
2

5
5
.
6
7
3
2

0
2
.
1
1
1

9
4
.
5
5
7
2

2
1
.
9
0
1

1
0
.
1

2
.
2

1
5
.
3

5
6
.
3

7
4
.
4

6
0
.
3

4
3
.
3

5
4
.
2

3
7
.
0

3
4
.
2

9
2
.
2

8
2
.
2
1

4
4
.
3
1

2
.
4
5

8
8
.
4

6
.
0
1

1
6
.
5
1

4
3
.
3

4
7
.
3

3
9
.
6
1

9
8
.
0

9
0
.
2

4
9
.
2

6
6
.
5
1

3
.
3

2
3
.
8
1

1
5
.
4
1

6
2
.
5
1

9
6
.
3

5
3
.
3

4
5
.
3

3
0
.
2
1

7
.
2

6
8
.
3

6
2
.
1
1

8
9
.
0
1

2
6
.
0

8
7
.
1

9
7
.
1

7
2
.
0
1
1
3
1

6
6
.
5
6
4
1

4
8
.
5
1

8
9
.
4
6

6
5
.
3
1

1
.
2
2
8
3
1

5
9
.
6
5
6
1

5
9
.
6
1

6
9
.
0
7

1
0
.
6
1

6
2
.
0
9
3
5
1

5
4
.
7
8
5
1

4
6
.
6
1

5
3
.
9
6

7
2
.
3
1

2
6
.
2
5
6
2
3

9
5
.
7
7
2
7

8
6
.
3
5

8
2
.
6
9
1

8
4
.
2
4

2
4
.
0
0
4
1
4

9
9
.
1
6
2
7

8
1
.
3
5

6
8
.
5
9
1

7
6
.
1
4

1
8
.
7
7
6

9
.
8
9
8

9
.
4
1
5
1

3
5
.
4
5
2
1

1
9
.
2
7
2
1

3
5
.
0

8
7
.
0

2
0
.
3

6
3
.
3

6
6
.
6

9
.
5
1
9
1

7
0
.
4
1

8
0
.
7
6
1
2

8
4
.
4
1

2
0
.
5
7
4
2

3
8
.
9
1

6
0
.
5
7
8

4
5
.
7
3
9

4
9
.
6
0
7

9
7
.
2

2
7
.
2

8
.
1

5
0
.
0

8
0
.
0

2
2
.
0

8
1
.
0

4
3
.
0

3
5
.
0

4
5
.
0

6
6
.
0

6
1
.
0

8
1
.
0

4
1
.
0

2
4
.
0

8
4
.
0

6
3
.
1

9
1
.
1

6
0
.
2

7
3
.
3

4
4
.
3

9
1
.
4

3
.
1

2
2
.
1

1
0
.
1

5
0
.
0

5
0
.
0

6
1
.
0

6
1
.
0

6
2
.
0

4
4
.
0

2
4
.
0

3
5
.
0

6
1
.
0

4
1
.
0

1
1
.
0

4
1
.
0

6
1
.
0

3
4
.
0

4
1
.
0

5
2
.
0

9
1
.
0

1
2
.
0

2
4
.
0

5
2
.
0

8
1
.
0

4
1
.
0

0
1
.
0

9
1
.
0

8
2
.
0

2
2
.
1

9
.
0

9
8
.
0

0
0
.
2

7
9
.
1

2
0
.
0

2
0
.
0

7
0
.
0

4
0
.
0

5
0
.
0

8
0
.
0

9
0
.
0

8
0
.
0

4
0
.
0

6
0
.
0

4
0
.
0

0
4
.
0

1
0
.
1

6
0
.
1

4
9
.
4

4
3
.
5

4
1
.
5

9
1
.
0

0
2
.
0

8
4
.
0

0
1
.
0

4
1
.
0

6
1
.
0

7
1
.
0

4
4
.
0

2
6
.
0

7
.
0

7
6
.
0

7
7
.
0

0
2
.
1

4
5
.
1

7
.
9
5
2
5

8
6
5
2

.

2
1
.
5
2
8
6

1
2
7
4

.

3
8
.
3
3
8
4

6
8
8
5

.

1
6
.
1
8
4
1

7
0
9
5

.

8
9
.
7
4
3
1

.

5
1
7

5
5
.
8
6
3
1

.

1
3
8
1
1

7
9
.
3
9
5
1

.

2
9
0
5
1

8
2
.
1

3
0
.
4
1
3
1

.

2
9
1
1
1

5
7
.
8
7
1
1

4
0
5
3

.

7
3
.
6
8
6
3

.

9
6
1
7
1

2
0
.
1
7
3
4

.

1
0
0
7
1

3
2
1

.

7
0
2

.

9
3
2

.

4
2

.

4
8
2

.

2
5
5

.

3
0
7

.

9
6
5

.

1
0
1

.

4
6
3

.

8
5
3

.

6
1
6

.

4
6
9

.

5
8
0
1

.

2
0
1

.

3
7
1

.

3
9
1

.

6
8
0
1

.

1
2

.

7
6
2
1

.

.

3
4
2

8
5
8
2

.

9
5
4
2

.

1
7
5

.

6
6
7
1

.

2
5
7
1

.

7
4
2

.

9
6
4

.

7
3
6

.

2
1
5

.

2
8
0

.

3
9
2

.

7
8
2

.

7
6
.
1
1

5
2
.
6
1
5
4
5

.

2
2
8
4
8
5

7
0
0
9

.

.

7
8
4
4
3

5
7
9
6

.

4
8
.
1
1

7
.
2
8
9
7
5

.

1
7
4
3
9
5

0
.
3
9
0
2

9
.
5
2
6
2

9
2
2

.

5
9
2

.

4
7
.
9
0
5
6

3
7
1
1

.

8
1
.
0

8
1
.
0

4
1
.
0

2
8
.
6
7
5

4
.
9
6
2
1

4
6
.
1
2
5

0
2
.
0

9
4
.
8
8
0
2

1
5
.
2
0
6
1

8
5
.
5
5
6
1

8
9
.
4
6
4
1

6
4
.
8
3
2
2

6
6
3

.

1
1
6

.

4
5
9

.

4
4
9

.

.

7
1
1

5
1
4

.

1
3
4

.

3
9
2

.

.

6
0
9

8
2
0

.

7
3
0

.

8
8
0

.

9
1
0

.

8
2
0

.

4
4
0

.

1
3
0

.

1
5
0

.

3
0

.

2
3
0

.

6
2
0

.

.

6
6
4
3

8
3
0
7

.

3
7
1

.

8
9
1

.

3
6
5

.

3
3
1

.

5
8
1

.

7
5
2

.

9
5
2

.

2
0
3

.

4
1
2

.

7
4
2

.

1
0
2

.

0
2
0

.

2
2
0

.

2
7
0

.

5
1
0

.

4
2
0

.

3
0

.

8
2
0

.

6
3
0

.

4
2
0

.

5
2
0

.

1
2
0

.

4
7
.
3
1
4
9
1

.

9
5
3
3
9
1

6
7
6
2

.

.

2
3
9
1
1

7
4
2
2

.

9
5
.

7
6
8
0
2

.

8
9
3
1
2

9
8
8
2

.

.

3
1
1
3
1

1
7
4
2

.

4
0
.
6
6
9
1
2

.

4
5
7
1
0
2

7
6
7
2

.

.

9
2
8
2
1

.

4
2
2

6
6
0

.

7
6
0

.

4
3
2

.

8
1
0

.

4
2
0

.

6
1
0

.

6
1
0

.

3
2
0

.

7
4
0

.

4
5
0

.

6
3
0

.

3
1
0

.

4
4
0

.

7
2
0

.

7
5
1

.

2
7
1

.

4
8
1

.

0
6
3

.

1
6
3

.

6
0
0

.

6
0
0

.

9
1
0

.

6
0
0

.

7
0
0

.

7
0
0

.

6
0
0

.

7
0
0

.

6
0
0

.

9
0
0

.

6
0
0

.

8
5
1

.

2
1
3

.

8
7
3

.

1
8
3

.

2
4
4

.

0
2
4

.

7
9
4

.

5
4
4

.

4
4
1

.

5
6
3

.

5
8
3

.

0
5
0

.

5
5
0

.

8
3
1

.

4
4
0

.

9
7
0

.

2
0
1

.

5
0
1

.

0
3
1

.

5
6
0

.

4
6
0

.

.

8
3
8
2
1
1
3

.

9
5
6
4
3

.

8
3
2
4
9
5
2

.

4
2
3
6
0
1

.

5
0
2
4
5
7
1

.

8
7
0
8
3
1

.

6
6
1
6
7
1
1

.

8
8
2
2
4
1

.

6
4
9
1
1
3
1

.

4
1
1
0
8
1

.

7
0
2
9
3
0
1

.

1
4
5
6
3
2

.

1
7
8
8
1
1
1

.

6
3
0
7
2

8
1
6

.

3
8
3
1

.

1
6
7
1

.

4
0
8
1

.

1
2
1
2

.

4
6
2
2

.

.

1
6
2

9
7
.
7
2

8
2
.
8
6

.

6
1
8

2
4
.
3
8

1
5
.
6
9

6
1
.
9
9

4
9
.
4

9
9
.
0
1

1
.
4
1

6
0
.
6
1

5
3
.
8
1

6
0
.
2
2

3
6
.
8
0
1

6
0
.
4
2

.

2
9
8
2
1
0
1

.

2
8
8
9
0
2

6
2
0
2

.

.

6
1
7
5
4
6

.

6
5
3
3
1

.

6
0
9
8
6
8
1

.

9
7
8
2
6

.

3
3
2
8
5
9
1

.

7
2
0
3
6

3
3
4

.

6
2
4
1

.

8
2
4
1

.

2
3
.
9
8

7
6
.
9
1

1
6
.
4
6

0

.

4
6

8
7
.
8
1

9
6
.
3

7
5
.
1
1

7
3
.
1
1

4
.
1

7
5
0

.

.

4
0
9
1
4

.

7
7
8
6
2
8

.

1
2
3
1
7

6
1
6

.

7
6
7

.

.

1
7
4
8
0
7
2

7
8
1
3

.

.

8
8
0
6
5
6

6
6
6
1

.

.

8
7
5
9
5
0
1

4
8
8
3

.

.

5
0
0
3
3
3
1

7
9
4
6

.

.

5
8
6
6
3
3
1

9
2
7
6

.

.

9
3
1
6
8
0
1

5
9
3
9

.

.

5
3
5
1
3
6

.

3
0
4
0
1
6

6
8
8
1

.

5
0
9
1

.

4
8
2
1

.

4
7
0

.

6
8
0

.

5
6
2

.

9
8
0

.

1
8
1

.

7
5
2

.

4
6
2

.

8
2
3

.

1
2
1

.

4
2
1

.

8
9
0

.

6
6
.
4

4
3
.
5

2
6
.
5
1

9
9
.
5

7
2
.
2
1

2
5
.
6
1

4
9
.
6
1

5
9
.
0
2

5
2
.
8

3
5
.
8

9
6

.

2
6
.
0

1
6
.
0

2
1
.
2

7
7
.
0

2
3
.
1

7
9
.
1

8
9
.
1

3
4
.
2

2
0
.
1

1
0
.
1

7
7
.
0

5
8
7
1

.

.

9
0
3
8
1
2
0
1

.

4
2
3
0
0
9

.

4
5
1
0
1

2

.

0
2
4

4
1
9
1

.

.

9
6
0
0
9
6
1
1

.

3
7
8
7
0
0
1

.

1
6
9
0
1

8

.

0
6
4

6
9
.
6
7

5
9
.
8
8

4
6
8
1

.

.

9
2
9
5
6
6
1
1

.

8
5
0
7
9

.

3
4
7
0
1

9
0
.
1
5
4

4
7
.
2
8

6
7
.
5

4
4
.
6

5
6
.
6

9
3
9
3

.

.

7
9
3
8
3
6
1

.

7
7
0
3
5
1
3
1

.

4
6
6
2
3

8
9
.
3
9
1
1

1
0
.
9
4
2

8
.
2
1

9
7
9
3

.

.

2
3
6
7
2
6
5
1

.

4
8
4
1
8
2
3
1

.

4
4
5
2
3

1
0
.
6
9
1
1

4
2
.
7
4
2

8
6
.
2
1

4
4
.
0

2
1
.
1

9
3
.
1

2
1
.
1

4
2
.
1

3
1
.
1

1
2
.
1

1
2
.
1

2
4
.
0

6
3
.
1

5
1
.
0

6
1
.
0

9
4
.
0

2
.
0

1
3
.
0

6
3
.
0

6
3
.
0

3
4
.
0

1
2
.
0

2
.
0

7
1
.
0

7
7
.
7

1
v

r
e
t
e
m

j

2
v

r
e
t
e
m

j

3
v

r
e
t
e
m

j

4
v

r
e
t
e
m

j

5
v

r
e
t
e
m

j

1
v

c
e
s
l
m
x

2
v

c
e
s
l
m
x

3
v

c
e
s
l
m
x

1
v

t
n
a

2
v

t
n
a

3
v

t
n
a

4
v

t
n
a

5
v

t
n
a

6
v

t
n
a

7
v

t
n
a

8
v

t
n
a

1
v

s
a
p
o
t
j

2
v

s
a
p
o
t
j

3
v

s
a
p
o
t
j

1
v

r
e
t
e
m

j

2
v

r
e
t
e
m

j

3
v

r
e
t
e
m

j

4
v

r
e
t
e
m

j

5
v

r
e
t
e
m

j

1
v

c
e
s
l
m
x

2
v

c
e
s
l
m
x

3
v

c
e
s
l
m
x

a
v
a
J

)
s
s
a
l
c
-
t
s
e
t
(

e
g
a
r
e
v
A

15

6
8
.
3

1
9
.
3

1
5
.
0
4
9
2
2

.

2
6
4
5
0
2

.

1
8
1

4
6
6
5

.

6
7
3
1

.

6
3
.
5
7
6
7
4

.

9
7
0
7
1
2

2
1
9
1

.

.

5
9
5

9
4
5
1

.

3
2
.
4
1

6
6
.
1
6
7
0
6

.

5
0
9
7
6
4
1

4
3
8
7

.

.

5
8
7
5
2

7
6
9
5

.

1
9
6
3

.

.

6
8
0
1
4
5
6

.

2
2
2
8
5
7
5

.

2
3
8
2
2

9
3
.
4
1
7

4
6
.
6
6
1

8
9
.
6

3
v

s
a
p
o
t
j

9
7
0
1

.

.

3
3
3
5
5
9
4

.

5
2
0
9
5
5

.

3
1
1

.

6
4
9
4
2
9
5

.

9
3
8
6
0
6

8
0
2
5

.

8
0
5
5

.

8
9
.
7
5
1

8
0
.
7
3

9
9
.
4
6
1

6
1
.
9
3

5
0
.
2

7
9
.
1

1
v

s
a
p
o
t
j

2
v

s
a
p
o
t
j

a
v
a
J

)
d
o
h
t
e
m

-
t
s
e
t
(

mation representing the related test cases is smaller,
requiring less time to compute the priority value. (2)
It is also expected that (for the Java programs) pri-
oritization at the test-method granularity would take
longer than at the test-class granularity, regardless of
code coverage granularities. As shown in Table 3,
the number of test cases to be prioritized at the test-
method granularity is large than those at the test-class
granularity, which requires more prioritization itera-
tions.

(3) OCP requires much less time to prioritize
test cases than most studied TCP techniques (e.g.,
TCPadd, TCPlexi, TCPuni, TCPart and TCPsearch) ir-
respective of subject program, and code coverage
and test case granularities. Meanwhile, OCP can im-
prove the costs of TCPadd by 85% on average. Con-
sidering that TCPadd remains state-of-the-art, the de-
creases in costs can achieve slightly better or compa-
rable fault detection eﬀectiveness and thus are valu-
It should be noted that TCPtot has
able actually.
a much faster prioritization rate than TCPocp, as it
does not use feedback information during the prior-
itization process. However, TCPtot performs worst
among all TCP techniques and is usually considered
as a low bound control TCP technique [22].

Answer to RQ4: Overall, our analysis on the ef-
ﬁciency reveals that except TCPtot, the APFD val-
ues of which is much lower than OCP, TCPocp has
much less time to prioritize test cases than TCPadd,
TCPlexi, TCPuni, TCPart and TCPsearch.

6. Related Work

A considerable amount of research has been con-
ducted to improve regression testing performance on
various issues [29, 62, 63, 64, 65, 66, 67]. We focus
on the coverage-based TCP techniques and summa-
rize the existing work from the following categories.

6.1. Prioritization Strategies

Despite the large body of research on coverage-
based TCP [66, 68, 69, 70], the total-greedy and
additional-greedy greedy strategies remain the most
widely investigated prioritization strategies [7]. In
addition to the above greedy-based strategies, re-
searchers have also investigated other generic strate-
gies [30, 31]. For example, Li et al. [30] transform

the TCP problem into a search problem and propose
two search-based prioritization strategies (i.e., a hill-
climbing strategy and a genetic strategy). Further-
more, inspired by the advantages of adaptive random
testing (ART) in replacing random testing (RT) [71],
Jiang et al.
[31] investigate ART to improve ran-
dom test case prioritization and propose an art-based
strategy based on the distribution of test cases across
the input domain.

Researchers have also proposed some alternative
strategies in previous studies to take further advan-
tage of the code coverage information (e.g., the cov-
ered times of code units). For example, Eghbali et al.
[23] propose an enhanced additional-greedy strategy
for breaking ties using the notion of lexicographical
ordering (i.e., the lexicographical-greedy strategy),
where fewer covered code units should have higher
priority value for coverage. Speciﬁcally, unlike tradi-
tional greedy-based strategies [25], Eghbali et al. do
not categorize all code units into two distinct groups,
(i.e., covered and not covered). At each iteration,
huge calculations are required to calculate the pri-
ority values of all code units based on the number
of times they are covered, and then each one in the
remaining test cases is lexicographically compared
against others. Similarly, Zhang et al. [22] propose
a variant of the additional-greedy strategy to unify
the total-greedy strategy and the additional-greedy
strategy. Each time a code unit is covered by a test
case, the probability that the code unit contains un-
detected faults is reduced by some ratio between 0%
(as in the total-greedy strategy) and 100% (as in the
additional-greedy strategy), which can also be con-
sidered as an eﬀective strategy to break tie cases. The
above techniques attempt to make use of more accu-
rate coverage information obtained by additional cal-
culations. For example, the lexicographical-greedy
strategy needs to rank all code units based on the
number of times they are covered, while the uniﬁed-
greedy strategy needs to calculate the value of fault
detection probability for each code unit. Most re-
cently, Zhou et al. [34] propose eight parallel test pri-
oritization techniques, which are adapted from four
typical sequential test prioritization techniques (in-
cluding total-greedy, the additional-greedy strategy
, the search-based strategy, and the art-based strat-
egy). Diﬀerent from traditional sequential TCP tech-

16

niques, it aims at generating a set of test sequences,
each of which is allocated in an individual computing
resource. Thus, we do not include it in this work.

It can be observed that most existing TCP strate-
gies tend to consider the whole candidate set in pri-
oritization iterations. To address this limitation, we
pay attention to partial candidate test cases with the
aid of previous priority values, resulting in a better
performance in both eﬀectiveness and eﬃciency.

6.2. Coverage criteria

In principle, TCP techniques can use any test ad-
equacy criterion as the underlying coverage crite-
rion [72, 73]. Among various criteria, structural
coverage has been widely adopted in in previous
TCP research, such as statement coverage[7, 74],
branch coverage[31], method coverage [22, 28],
block coverage[30] and modiﬁed condition/decision
coverage [75]. Elbaum et al. [25] also propose a
fault-exposing-potential (FEP) criterion based on the
probability of the test case detecting a fault. Fang
et al. [73] use logic coverage for TCP, where high
coverage of logic expressions implies a high prob-
ability of detecting faults. Recently, Chi et al. [19]
demonstrate that basic structural coverage may not
be enough to predict fault detection capability and
propose a dynamic relation-based coverage based on
[28] detect
method call sequences. Wang et al.
fault-prone source code by existing code inspection
techniques and then propose a quality-aware TCP
technique (i.e., QTEP) by considering the weighted
source code in terms of fault-proneness. However,
such techniques require not only coverage informa-
tion but also other source code information (e.g., the
defect prediction results and method call sequences)
and thus are not considered in our work. In this work,
we investigate how the basic structure coverage cri-
teria inﬂuence the performance of TCP techniques.

6.3. Empirical studies

As an eﬀective regression testing technique, TCP
has been extensively studied in the literature from
both academic and industrial perspectives. Recently,
researchers also performed a large number of empiri-
cal studies to investigate TCP from diﬀerent aspects.
For example, several studies usually focus on the per-
formance of the traditional dynamic test prioritiza-

tion regarding some eﬀectiveness and eﬃciency cri-
teria (e.g., APFD, APFDC, and prioritization time)
[7, 25, 76, 77]. Meanwhile, Lu et al. [17] were the
ﬁrst to investigate how real-world software evolution
impacts the performance of prioritization strategies:
They reported that source code changes have a low
impact on the eﬀectiveness of traditional dynamic
techniques, but that the opposite was true when con-
sidering new tests in the process of evolution. Cit-
ing a lack of comprehensive studies comparing static
and dynamic test prioritization techniques, Luo et
al. [16, 35] compared static TCP techniques with dy-
namic ones. Henard et al. [42] compared white-box
and back-box TCP techniques. In this work, we fo-
cus on the coverage-based TCP techniques and con-
duct an extensive study to evaluate OCP with six
state-of-the-art TCP techniques.

7. Threats to validity

To facilitate the replication and veriﬁcation of our
experiments, we have made the relevant materials
(including source code, subject programs, test suites,
and mutants) available at [24]. Despite that, our
study may still face some threats to validity.

7.1. Internal validity

The implementation of our experiment may intro-
duce threats to internal validity. First, randomness
might aﬀect the reliability of conclusions. To address
this, we repeat the prioritization process 1,000 times
and used statistical analysis to assess the strategies.
Second, the data structures used in the prioritization
algorithms, and the faults in the source code, may in-
troduce noise when evaluating the eﬀectiveness and
eﬃciency. To minimize these threats, we use data
structures that are as similar as possible, and care-
fully reviewed all source code before conducting the
experiment. Third, to assess the eﬀectiveness of TCP
techniques, the most widely used metric APFD is
adopted in our experiment. However, APFD only
reﬂects the rate at which faults are detected, ignoring
the time and space costs. Our future work will in-
volve additional metrics (e.g., APFDC) that can mea-
sure other practical performance aspects of prioriti-
zation strategies.

17

7.2. External validity

The main threat to external validity lies in the se-
lection of the subject programs and faults. First, al-
though 19 Java and 30 C program versions with var-
ious sizes are adopted in our experiment, the results
may not generalize to programs written in other lan-
guages (e.g., C++ and Python). Meanwhile, the rel-
ative performances of TCP techniques on the used
mutants may not be generalizable to the real faults,
despite the fact that mutation testing have argued to
be an appropriate approach for assessing TCP per-
formance [45, 39, 48], To mitigate these threats, ad-
ditional studies will be conducted to investigate the
performance of TCP on programs with real faults and
other languages in the future.

8. Conclusion

In this paper, we have introduced a generic par-
tial attention mechanism that adopts priority values
of previously selected test cases to avoid consider-
ing all test cases. We also apply the concept to the
additional-greedy strategy and implement a novel
coverage-based TCP technique, partition ordering
based prioritization (OCP). Results from our empir-
ical study have demonstrated that OCP can achieve
better fault detection rate than six state-of-the-arts
(i.e., total-greedy, additional-greedy, uniﬁed-greedy,
lexicographical-greedy, art-based, and search-based
TCP techniques). OCP is also found to have much
less prioritization time to prioritize test cases than
most state-of-the-arts (except the total-greedy strat-
egy) and the improvement can achieve 85% - 99%
on average.

In the future, we plan to continue reﬁning the
generic partial attention mechanism and extend it
to other TCP techniques (e.g., the lexicographical-
greedy strategy). We will also launch an exten-
sive eﬀort on understanding the impact of the pro-
posed technique for other application domains of
TCP research [78, 79, 80], such as conﬁguration
testing[21, 81] and combinatorial testing [82, 83].

Acknowledgments

The authors would like to thank the anony-
mous reviewers for insightful comments. This re-

search is partially supported by the National Natu-
ral Science Foundation of China (No. 61932012,
62141215) and Science, Technology and In-
novation Commission of Shenzhen Municipality
(CJGJZD20200617103001003).

References

[1] D. Elsner, F. Hauer, A. Pretschner, S. Reimer, Empiri-
cally evaluating readily available information for regres-
sion test optimization in continuous integration, in: Pro-
ceedings of the 30th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis (ISSTA’21),
2021, pp. 491–504.

[2] W. Lam, A. Shi, R. Oei, S. Zhang, M. D. Ernst,
T. Xie, Dependent-test-aware regression testing tech-
niques, in: Proceedings of the 29th ACM SIGSOFT In-
ternational Symposium on Software Testing and Analysis
(ISSTA’20), 2020, p. 298–311.

[3] W. E. Wong, J. R. Horgan, S. London, H. Agrawal, A
study of eﬀective regression testing in practice, in: Pro-
ceedings of the 8th International Symposium on Software
Reliability Engineering, 1997, pp. 264–274.

[4] M. Gligoric, L. Eloussi, D. Marinov, Practical regression
test selection with dynamic ﬁle dependencies, in: Pro-
ceedings of the 24th International Symposium on Soft-
ware Testing and Analysis (ISSTA’15), 2015, pp. 211–
222.

[5] L. Zhang, Hybrid regression test selection, in: Proceed-
ings of the 40th International Conference on Software En-
gineering (ICSE’18), 2018, pp. 199–209.

[6] E. Cruciani, B. Miranda, R. Verdecchia, A. Bertolino,
Scalable approaches for test suite reduction, in: 2019
IEEE/ACM 41st International Conference on Software
Engineering (ICSE’19), 2019, pp. 419–429.

[7] G. Rothermel, R. H. Untch, , M. J. Harrold, Test case pri-
oritization: An empirical study, in: Proceedings of the
15th IEEE International Conference on Software Mainte-
nance (ICSM’99), 1999, pp. 179–188.

[8] M. V. M¨antyl¨a, B. Adams, F. Khomh, E. Engstr¨om, K. Pe-
tersen, On rapid releases and software testing: a case
study and a semi-systematic literature review, Empirical
Software Engineering 20 (5) (2015) 1384–1425.

[9] S. Elbaum, G. Rothermel, J. Penix, Techniques for im-
proving regression testing in continuous integration devel-
opment environments, in: Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE’14), 2014, pp. 235–
245.

[10] A. Memon, Z. Gao, B. Nguyen, S. Dhanda, E. Nickell,
R. Siemborski, J. Micco, Taming google-scale continu-
ous testing, in: 2017 IEEE/ACM 39th International Con-
ference on Software Engineering: Software Engineering
in Practice Track (ICSE-SEIP’17), 2017, pp. 233–242.

[11] Z. Sadri-Moshkenani, J. Bradley, G. Rothermel, Sur-
vey on test case generation, selection and prioritization

18

for cyber-physical systems, Software Testing, Veriﬁcation
and Reliability 32 (1) (2022) e1794.

[12] J. A. do Prado Lima, S. R. Vergilio, A multi-armed ban-
dit approach for test case prioritization in continuous in-
tegration environments, IEEE Transactions on Software
Engineering 48 (2) (2022) 453–465.

[13] W. E. Wong, J. R. Horgan, S. London, A. P. Mathur, Eﬀect
of test set minimization on fault detection eﬀectiveness,
Software: Practice and Experience 28 (4) (1998) 347–
369.

[14] M. Khatibsyarbini, M. A. Isa, D. N. Jawawi, R. Tumeng,
Test case prioritization approaches in regression testing:
A systematic literature review, Information and Software
Technology 93 (2018) 74–93.

[15] S. Yoo, M. Harman, Regression testing minimization, se-
lection and prioritization: a survey, Software Testing, Ver-
iﬁcation and Reliability 22 (2) (2012) 67–120.

[16] Q. Luo, K. Moran, D. Poshyvanyk, A large-scale empiri-
cal comparison of static and dynamic test case prioritiza-
tion techniques, in: Proceedings of the 24th ACM SIG-
SOFT International Symposium on Foundations of Soft-
ware Engineering (ESEC/FSE’16), 2016, pp. 559–570.

[17] Y. Lu, Y. Lou, S. Cheng, L. Zhang, D. Hao, Y. Zhou,
L. Zhang, How does regression test prioritization perform
in real-world software evolution?, in: Proceedings of the
38th International Conference on Software Engineering
(ICSE’16), 2016, pp. 535–546.

[18] Q. Luo, K. Moran, D. Poshyvanyk, M. Di Penta, Assess-
ing test case prioritization on real faults and mutants, in:
Proceedings of the 34th IEEE International Conference on
Software Maintenance and Evolution (ICSME’18), 2018,
pp. 240–251.

[19] J. Chi, Y. Qu, Q. Zheng, Z. Yang, W. Jin, D. Cui,
T. Liu, Test case prioritization based on method call se-
quences, in: Proceedings of the 42nd IEEEAnnual Com-
puter Software and Applications Conference (COMP-
SAC’18), Vol. 01, 2018, pp. 251–256.

[20] J. Chen, Y. Lou, L. Zhang, J. Zhou, X. Wang, D. Hao,
L. Zhang, Optimizing test prioritization via test distribu-
tion analysis, in: Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE’18), 2018, pp. 656–667.

[21] R. Cheng, L. Zhang, D. Marinov, T. Xu, Test-case prior-
itization for conﬁguration testing, in: Proceedings of the
30th ACM SIGSOFT International Symposium on Soft-
ware Testing and Analysis (ISSTA’21), New York, NY,
USA, 2021, p. 452–465.

[22] L. Zhang, D. Hao, L. Zhang, G. Rothermel, H. Mei,
Bridging the gap between the total and additional test-
case prioritization strategies,
in: Proceedings of the
2013 International Conference on Software Engineering
(ICSE’13), 2013, pp. 192–201.

[23] S. Eghbali, L. Tahvildari, Test case prioritization using
lexicographical ordering, IEEE Transactions on Software
Engineering 42 (12) (2016) 1178–1195.

[24] The

project

website,
QuanjunZhang/OCP, accessed 10 March 2022.

https://github.com/

[25] S. Elbaum, A. G. Malishevsky, G. Rothermel, Prioritizing
test cases for regression testing, in: Proceedings of the
8th ACM SIGSOFT International Symposium on Soft-
ware Testing and Analysis (ISSTA’00), 2000, pp. 102–
112.

[26] L. Zhang, J. Zhou, D. Hao, L. Zhang, H. Mei, Prioritizing
JUnit test cases in absence of coverage information, in:
Proceedings of the 25th IEEE International Conference
on Software Maintenance (ICSM’09), 2009, pp. 19–28.

[27] H. Mei, D. Hao, L. Zhang, L. Zhang, J. Zhou, G. Rother-
mel, A static approach to prioritizing junit test cases,
IEEE Transactions on Software Engineering 38 (6) (2012)
1258–1275.

[28] S. Wang, J. Nam, L. Tan, QTEP: Quality-aware test case
prioritization, in: Proceedings of the 11th Joint Meeting
on Foundations of Software Engineering (ESEC/FSE’17),
2017, pp. 523–534.

[29] Y. Lou, J. Chen, L. Zhang, D. Hao, A survey on regression
test-case prioritization, in: Advances in Computers, Vol.
113, Elsevier, 2019, pp. 1–46.

[30] Z. Li, M. Harman, R. M. Hierons, Search algorithms for
regression test case prioritization, IEEE Transactions on
Software Engineering 33 (4) (2007) 225–237.

[31] B. Jiang, Z. Zhang, W. K. Chan, T. Tse, Adaptive ran-
dom test case prioritization, in: Proceedings of the 24th
IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE’09), 2009, pp. 233–244.

[32] W. Weimer, T. Nguyen, C. Le Goues, S. Forrest, Auto-
matically ﬁnding patches using genetic programming, in:
Proceedings of the 31st International Conference on Soft-
ware Engineering (ICSE’09), 2009, p. 364–374.

[33] L. Gazzola, D. Micucci, L. Mariani, Automatic software
repair: A survey, IEEE Transactions on Software Engi-
neering 45 (1) (2017) 34–67.

[34] J. Zhou, J. Chen, D. Hao, Parallel test prioritization, ACM
Transactions on Software Engineering and Methodology
31 (1) (2022) 1–50.

[35] Q. Luo, K. Moran, L. Zhang, D. Poshyvanyk, How do
static and dynamic test case prioritization techniques per-
form on modern software systems? An extensive study
on GitHub projects, IEEE Transactions on Software En-
gineering 45 (11) (2019) 1054–1080.

[36] F. Li, J. Zhou, Y. Li, D. Hao, L. Zhang, Aga: An accel-
erated greedy additional algorithm for test case prioritiza-
tion, IEEE Transactions on Software Engineering (2021).
[37] D. Hao, L. Zhang, L. Zhang, G. Rothermel, H. Mei, A
uniﬁed test case prioritization approach, ACM Transac-
tions on Software Engineering and Methodology 24 (2)
(2014) 10:1–10:31.

[38] R. Huang, Q. Zhang, D. Towey, W. Sun, J. Chen, Regres-
sion test case prioritization by code combinations cover-
age, Journal of Systems and Software 169 (2020) 110712.
[39] H. Do, S. Elbaum, G. Rothermel, Supporting controlled
experimentation with testing techniques: An infrastruc-

19

ture and its potential impact, Empirical Software Engi-
neering 10 (4) (2005) 405–435.

[40] Software-artifact

Infrastructure Repository

(SIR),

https://sir.csc.ncsu.edu/portal/index.php,
accessed 10 March 2022.

[41] GNU FTP Server, http://ftp.gnu.org/, accessed 10

March 2022.

[42] C. Henard, M. Papadakis, M. Harman, Y. Jia, Y. Le
Traon, Comparing white-box and black-box test priori-
tization, in: Proceedings of the 38th IEEE/ACM Inter-
national Conference on Software Engineering (ICSE’16),
2016, pp. 523–534.

[43] M. Papadakis, M. Kintis, J. Zhang, Y. Jia, Y. Le Traon,
M. Harman, Mutation testing advances: an analysis and
survey, in: Advances in Computers, Vol. 112, Elsevier,
2019, pp. 275–378.

[44] J. Zhang, L. Zhang, M. Harman, D. Hao, Y. Jia, L. Zhang,
Predictive mutation testing, IEEE Transactions on Soft-
ware Engineering 45 (9) (2019) 898–918.

[45] J. H. Andrews, L. C. Briand, Y. Labiche, Is mutation an
appropriate tool for testing experiments?, in: Proceedings
of the 27th International Conference on Software Engi-
neering, (ICSE’05), 2005, pp. 402–411.

[46] F. Belli, C. J. Budnik, W. E. Wong, Basic operations
for generating behavioral mutants, in: Second Workshop
on Mutation Analysis (Mutation 2006-ISSRE Workshops
2006), 2006, pp. 9–9.

[47] H. Do, G. Rothermel, A controlled experiment assessing
test case prioritization techniques via mutation faults, in:
Proceedings of the 21st IEEE International Conference on
Software Maintenance (ICSM’05), 2005, pp. 411–420.

[48] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes,
G. Fraser, Are mutants a valid substitute for real faults in
software testing?, in: Proceedings of the 22nd ACM SIG-
SOFT International Symposium on Foundations of Soft-
ware Engineering (ESEC/FSE’14), 2014, pp. 654–665.

[49] F. Belli, C. J. Budnik, A. Hollmann, T. Tuglular, W. E.
Wong, Model-based mutation testing—approach and case
studies, Science of Computer Programming 120 (2016)
25–48.

[50] J. H. Andrews, L. C. Briand, Y. Labiche, A. S. Namin, Us-
ing mutation analysis for assessing and comparing testing
coverage criteria, IEEE Transactions on Software Engi-
neering 32 (8) (2006) 608–624.

[51] H. Coles, T. Laurent, C. Henard, M. Papadakis, A. Ven-
tresque, Pit: A practical mutation testing tool for java
(demo), in: Proceedings of the 25th International Sympo-
sium on Software Testing and Analysis (ISSTA’16), 2016,
p. 449–452.

[52] M. Papadakis, C. Henard, M. Harman, Y. Jia, Y. Le Traon,
Threats to the validity of mutation-based test assessment,
in: Proceedings of the 25th International Symposium
on Software Testing and Analysis (ISSTA’16), 2016, pp.
355–365.

[53] L. Zhang, M. Kim, S. Khurshid, FaultTracer: A change
impact and regression fault analysis tool for evolving Java

programs, in: Proceedings of the 20th ACM SIGSOFT
Symposium on the Foundations of Software Engineering
(ESEC/FSE’12), 2012, p. 40.

[54] L. Zhang, M. Kim, S. Khurshid, FaultTracer: A spectrum-
based approach to localizing failure-inducing program ed-
its, Journal of Software: Evolution and Process 25 (12)
(2013) 1357–1383.

[55] ASM: An all purpose Java bytecode manipulation and
analysis framework, http://asm.ow2.org/, accessed
10 March 2022.

[56] gcc: The GNU Compiler Collection, https://gcc.

gnu.org/, accessed 10 March 2022.

[57] gcov: A Test coverage program, https://gcc.gnu.
org/onlinedocs/gcc/Gcov.html, accessed 10 March
2022.

[58] A. Arcuri, L. Briand, A hitchhiker’s guide to statistical
tests for assessing randomized algorithms in software en-
gineering, Software Testing, Veriﬁcation and Reliability
24 (3) (2014) 219–250.

[59] M. Gligoric, A. Groce, C. Zhang, R. Sharma, M. A.
Alipour, D. Marinov, Guidelines for coverage-based com-
parisons of non-adequate test suites, ACM Transactions
on Software Engineering and Methodology 24 (4) (2015)
22:1–22:33.

[60] A. Vargha, H. D. Delaney, A critique and improvment of
the CL common language eﬀect size statistics of mcgraw
and wong, Journal of Education and Behavioral Statistics
25 (2) (2000) 101–132.

[61] R: The R project for statistical computing, https://
www.r-project.org/, accessed 10 March 2022.
[62] R. Pan, M. Bagherzadeh, T. A. Ghaleb, L. Briand, Test
case selection and prioritization using machine learning:
a systematic literature review, Empirical Software Engi-
neering 27 (2) (2022) 1–43.

[63] S. Yu, C. Fang, Z. Cao, X. Wang, T. Li, Z. Chen, Pri-
oritize crowdsourced test reports via deep screenshot un-
derstanding, in: 2021 IEEE/ACM 43rd International Con-
ference on Software Engineering (ICSE’21), IEEE, 2021,
pp. 946–956.

[64] C. ai Sun, B. Liu, A. Fu, Y. Liu, H. Liu, Path-directed
source test case generation and prioritization in metamor-
phic testing, Journal of Systems and Software 183 (2022)
111091.

[65] S. Mondal, R. Nasre, Hansie: Hybrid and consensus re-
gression test prioritization, Journal of Systems and Soft-
ware 172 (2021) 110850.

[66] C. Fang, Z. Chen, K. Wu, Z. Zhao, Similarity-based test
case prioritization using ordered sequences of program
entities, Software Quality Journal 22 (2) (2014) 335–361.
[67] A. Haghighatkhah, M. M¨antyl¨a, M. Oivo, P. Kuvaja, Test
prioritization in continuous integration environments,
Journal of Systems and Software 146 (2018) 80–98.
[68] B. Miranda, E. Cruciani, R. Verdecchia, A. Bertolino, Fast
approaches to scalable similarity-based test case prioriti-
zation, in: Proceedings of the 40th International Confer-
ence on Software Engineering (ICSE’18), 2018, pp. 222–

20

sion: Using similarity to generate and prioritize t-wise test
conﬁgurations for software product lines, IEEE Transac-
tions on Software Engineering 40 (7) (2014) 650–670.

[83] H. Wu, C. Nie, J. Petke, Y. Jia, M. Harman, An empirical
comparison of combinatorial testing, random testing and
adaptive random testing, IEEE Transactions on Software
Engineering 46 (3) (2020) 302–320.

232.

[69] M. G. Epitropakis, S. Yoo, M. Harman, E. K. Burke, Em-
pirical evaluation of pareto eﬃcient multi-objective re-
gression test case prioritisation, in: Proceedings of the
23rd International Symposium on Software Testing and
Analysis (ISSTA’15), 2015, pp. 234–245.

[70] Q. Peng, A. Shi, L. Zhang, Empirically revisiting and
enhancing ir-based test-case prioritization, in: Proceed-
ings of the 29th ACM SIGSOFT International Sympo-
sium on Software Testing and Analysis (ISSTA’20), 2020,
pp. 324–336.

[71] R. Huang, W. Sun, Y. Xu, H. Chen, D. Towey, X. Xia, A
survey on adaptive random testing, IEEE Transactions on
Software Engineering 47 (10) (2019) 2052–2083.
[72] D. Hao, L. Zhang, L. Zang, Y. Wang, X. Wu, T. Xie, To
be optimal or not in test-case prioritization, IEEE Trans-
actions on Software Engineering 42 (5) (2016) 490–505.
[73] C. Fang, Z. Chen, B. Xu, Comparing logic coverage crite-
ria on test case prioritization, Science China Information
Sciences 55 (12) (2012) 2826–2840.

[74] D. Di Nucci, A. Panichella, A. Zaidman, A. De Lucia, A
test case prioritization genetic algorithm guided by the hy-
pervolume indicator, IEEE Transactions on Software En-
gineering 46 (6) (2020) 674–696.

[75] J. A. Jones, M. J. Harrold, Test-suite reduction and prior-
itization for modiﬁed condition/decision coverage, IEEE
Transactions on Software Engineering 29 (3) (2003) 195–
209.

[76] H. Do, G. Rothermel, A. Kinneer, Prioritizing JUnit test
cases: An empirical assessment and cost-beneﬁts analy-
sis, Empirical Software Engineering 11 (1) (2006) 33–70.
[77] H. Do, S. Mirarab, L. Tahvildari, G. Rothermel, The ef-
fects of time constraints on test case prioritization: A
series of controlled experiments, IEEE Transactions on
Software Engineering 36 (5) (2010) 593–617.

[78] Z. Wang, H. You, J. Chen, Y. Zhang, X. Dong, W. Zhang,
Prioritizing test inputs for deep neural networks via mu-
tation analysis, in: 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE’21), IEEE,
2021, pp. 397–409.

[79] J. Chen, Z. Wu, Z. Wang, H. You, L. Zhang, M. Yan, Prac-
tical accuracy estimation for eﬃcient deep neural network
testing, ACM Transactions on Software Engineering and
Methodology (TOSEM) 29 (4) (2020) 1–35.

[80] A. Sharif, D. Marijan, M. Liaaen, Deeporder: Deep
learning for test case prioritization in continuous integra-
tion testing, in: 2021 IEEE International Conference on
Software Maintenance and Evolution (ICSME’21), IEEE,
2021, pp. 525–534.

[81] X. Sun, R. Cheng, J. Chen, E. Ang, O. Legunsen, T. Xu,
Testing conﬁguration changes in context to prevent pro-
duction failures, in: 14th USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI’20),
2020, pp. 735–751.

[82] C. Henard, M. Papadakis, G. Perrouin, J. Klein, P. Hey-
mans, Y. Le Traon, Bypassing the combinatorial explo-

21

