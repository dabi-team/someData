2
2
0
2

p
e
S
3
2

]

C
O
.
h
t
a
m

[

3
v
5
6
0
7
0
.
4
0
2
2
:
v
i
X
r
a

A decomposition method for lasso problems with
zero-sum constraint

Andrea Cristofari∗

∗Department of Civil Engineering and Computer Science Engineering,
University of Rome “Tor Vergata”, Via del Politecnico, 1, 00133 Rome, Italy
E-mail: andrea.cristofari@uniroma2.it

Abstract. In this paper, we consider lasso problems with zero-sum constraint, commonly
required for the analysis of compositional data in high-dimensional spaces. A novel algorithm
is proposed to solve these problems, combining a tailored active-set technique, to identify the
zero variables in the optimal solution, with a 2-coordinate descent scheme. At every iteration,
the algorithm chooses between two diﬀerent strategies: the ﬁrst one requires to compute the
whole gradient of the smooth term of the objective function and is more accurate in the
active-set estimate, while the second one only uses partial derivatives and is computationally
more eﬃcient. Global convergence to optimal solutions is proved and numerical results are
provided on synthetic and real datasets, showing the eﬀectiveness of the proposed method.
The software is publicly available.

Keywords. Nonlinear programming. Convex programming. Constrained lasso. Decompo-
sition methods. Active-set methods.

1 Introduction

In this paper, we address the following optimization problem:

1
2

min

n

kAx − yk2 + λkxk1

xi = 0,

i=1
X

(1)

where k·k denotes the Euclidean norm, k·k1 denotes the ℓ1 norm, x ∈ Rn is the variable
vector and A ∈ Rm×n, y ∈ Rm, λ ∈ R are given, with λ ≥ 0.

Problem (1) is an extension of the well known lasso problem [43], imposing the zero-sum
n
constraint
i=1 xi = 0. This constraint is required in some regression models for composi-
tional data, i.e., for data representing percentages, or proportions, of a whole. Applications
with data of this type frequently arise in many diﬀerent ﬁelds, such as geology, biology, ecol-
ogy and economics. For example, in microbiome analysis, datasets are usually normalized
and result in compositional data [22, 41].

P

Let us brieﬂy review the role of the zero-sum constraint in regression analysis for com-
positional data. Assume we are given a response vector y ∈ Rm and an m × n matrix Z of

1

 
 
 
 
 
 
A decomposition method for lasso problems with zero-sum constraint

covariates, where every row of Z is a sample. By deﬁnition, compositional data are vectors
whose components are non-negative and sum to 1, so we can assume each row of Z belong
n
to the positive simplex ∆+
i=1 zi = 1, z > 0}. Due to the constrained
form of the sample space, standard statistical tools designed for unconstrained data cannot
P
be applied to build a regression model. To overcome this issue, log-contrast models were
proposed in [1, 2], using the following log-ratio transformation of data:

n−1 = {z ∈ Rn :

Wij = log

,

i ∈ {1, . . . , m},

j ∈ {1, . . . , n} \ {r},

Zij
Zir

(cid:16)

(cid:17)

where the rth component is referred to as reference component. Denoting by W\r ∈ Rm×(n−1)
the matrix with entries Wij, a linear log-contrast model is then obtained as follows:

y = W\r x\r + ε,

x1

∈ Rn−1 is the vector of regression coeﬃcients
where x\r =
and ε ∈ Rm is a vector of independent noise with mean zero and ﬁnite variances. Using the
deﬁnition of W\r, we can write

. . . xr−1 xr+1

. . . xn

(cid:3)

(cid:2)

T

yi =

xj log (Zij) −

xj log(Zir) + εi,

i = 1, . . . , m.

Xj6=r
Hence, introducing xr = −
metric form:

Xj6=r

j6=r xj, we can express the above model in the following sym-

P
y = Ax + ε,

n

subject to

xi = 0,

(2)

where A ∈ Rm×n is the matrix with Aij = log(Zij) in position (i, j). Note that the zero-
sum constraint has a central role, since it allows the response y to be expressed as a linear
combination of log-ratios.

i=1
X

Starting from (2), an ℓ1-regularized least-square formulation with zero-sum constraint
was ﬁrst considered in [32] for variable selection in high-dimensional spaces, leading to the
optimization problem (1). The resulting estimator was shown in [32] to be model selection
consistent as well as to ensure scale invariance, permutation invariance and selection invari-
ance. Moreover, in [3] it was shown that model (1) is proportional reference point insensitive,
allowing to overcome some issues related to the choice of a reference point in molecular
measurements.

Well established algorithms can be used to solve (1), exploiting the peculiar structure of
the problem. In this fashion, an augmented Lagrangian scheme with the subproblem solved
by cyclic coordinate descent was proposed in [32], while a coordinate descent strategy based
on random selection of variables was proposed in [3]. Moreover, considering more general
forms of constrained lasso, an approach based on quadratic programming and an ADMM
method were analyzed in [20], a semismooth Newton augmented Lagrangian method was
proposed in [17] and path algorithms were designed in [20, 28, 45].

In this paper we propose a decomposition algorithm, named Active-Set Zero-Sum-Lasso
(AS-ZSL), to eﬃciently solve problem (1) in a large-scale setting. The ﬁrst ingredient of our

2

Andrea Cristofari

approach is an active-set technique to identify the zero variables in the optimal solution, which
is expected to be sparse due to the ℓ1 regularization. The second ingredient is a 2-coordinate
descent scheme to update the variables estimated to be non-zero in the ﬁnal solution. In
more detail, we deﬁne two diﬀerent strategies: the ﬁrst one uses the whole gradient of the
smooth term of the objective function and is more accurate in the identiﬁcation of the zero
variables, while the second one only needs partial derivatives and is computationally more
eﬃcient. To balance computational eﬃciency and accuracy, in the algorithm we use a simple
rule to choose between the two strategies, based on the progress in the objective function.
The proposed method is proved to converge to optimal solutions and is shown to perform well
in the numerical experiments, compared to other approaches from the literature. The AS-ZSL
software is available at https://github.com/acristofari/as-zsl.

The rest of the paper is organized as follows. In Section 2 we introduce the notation and
give some optimality results for problem (1), in Section 3 we present the AS-ZSL algorithm,
in Section 4 we establish global convergence of AS-ZSL to optimal solutions, in Section 5
we show numerical results on synthetic and real datasets, in Section 6 we ﬁnally draw some
conclusions. Moreover, in Appendix A we report some details on the subproblem we need to
solve in the algorithm.

2 Preliminaries and optimality results

In this section, we introduce the notation and give some optimality results for problem (1).
First note that we did not include an intercept term in model (2), since it can be omitted if
the vector y and the columns of A have mean zero.

2.1 Notation

n
We indicate by e the vector made of all ones (so that the zero-sum constraint
i=1 xi = 0
can be rewritten as eT x = 0). We denote the Euclidean norm, the ℓ1 norm and the sup norm
of a vector v by kvk, kvk1 and kvk∞, respectively. The maximum between a vector v and
0, denoted by max{v, 0}, is understood as a vector whose ith entry is max{vi, 0}. Given a
matrix M , the element in position (i, j) is indicated with Mij, while M i is the ith column of
M .

P

Moreover, we deﬁne

ϕ(x) =

1
2

kAx − yk2,

so that the objective function of problem (1) can be expressed as

f (x) = ϕ(x) + λkxk1.

The gradient of ϕ is denoted by ∇ϕ.

2.2 Optimality conditions

Let us ﬁrst rewrite (1) as a smooth problem with non-negativity constraints, using a well
known variable transformation [20, 39, 43]. More speciﬁcally, we can introduce the variables

3

A decomposition method for lasso problems with zero-sum constraint

x+, x− ∈ Rn in order to split x into positive and negative components, i.e., x = x+ − x−,
with x+ = max{x, 0} and x− = max{−x, 0}. We obtain the following reformulation:

min ϕ(x+ − x−) + λeT (x+ + x−)
eT (x+ − x−) = 0
x+ ≥ 0
x− ≥ 0.

(3)

Note that the number of variables has doubled, i.e., problem (3) has 2n variables. It is easy
to obtain the following equivalence between problems (1) and (3).

Lemma 2.1. Let x∗ be an optimal solution of problem (1). Then, (x∗
− = max{−x∗, 0}.
solution of problem (3), where x∗

+ = max{x∗, 0} and x∗

+, x∗

−) is an optimal

Vice versa, if (x∗

+, x∗

−) is an optimal solution of problem (3), then x∗ = x∗

+ − x∗

− is an

optimal solution of problem (1).

Since problem (3) is convex and all constraints are linear, we can use KKT conditions
−) is an optimal solution of problem (3) if and only if there exist

+, x∗

and state that a point (x∗
KKT multipliers σ∗

+, σ∗

− ∈ Rn and µ∗ ∈ R such that

+ − x∗
+ − x∗

−) + λe − µ∗e − σ∗
−) + λe + µ∗e − σ∗

+ = 0,
− = 0,

∇x+ϕ(x∗
∇x−ϕ(x∗
eT (x∗
x∗
+ ≥ 0,
σ∗
+ ≥ 0,
+)T x∗
(σ∗

+ − x∗
−) = 0,
x∗
− ≥ 0,
σ∗
− ≥ 0,

+ = 0,

(σ∗

−)T x∗

− = 0.

From Lemma 2.1 and the fact that ∇x+ϕ(x+ − x−) = −∇x−ϕ(x+ − x−), it follows that
− ∈ Rn and

a point x∗ is an optimal solution of problem (1) if and only if there exist σ∗
µ∗ ∈ R such that

+, σ∗

+ = 0,
− = 0,

∇ϕ(x∗) + λe − µ∗e − σ∗
∇ϕ(x∗) − λe − µ∗e + σ∗
eT x∗ = 0,
σ∗
+ ≥ 0,
+)i x∗
(σ∗
−)i x∗
(σ∗

σ∗
− ≥ 0,
i = 0,
i = 0,

∀i : x∗
∀i : x∗

i > 0,
i < 0.

(4a)

(4b)

(4c)

(4d)

(4e)

(4f)

The next theorem provides equivalent optimality conditions for problem (1).

Theorem 2.2. Let x∗ be a feasible point of problem (1). The following statements are
equivalent:

(a) x∗ is an optimal solution of problem (1);

4

Andrea Cristofari

(b) A scalar µ∗ (the same as the one appearing in (4)) exists such that

∇iϕ(x∗) − µ∗ = λ,
∇iϕ(x∗) − µ∗ = −λ,
|∇iϕ(x∗) − µ∗| ≤ λ,




i : x∗
i : x∗
i : x∗

i < 0,
i > 0,
i = 0;


(c) ηmin(x∗) ≥ ηmax(x∗), where

min

η

max

η

(x) = min

i=1,...,n

(x) = max
i=1,...,n

∇iϕ(x) +

2 min{sign(xi), 0} + 1

λ

,

(cid:8)

∇iϕ(x) +

(cid:0)

(cid:1)
2 max{sign(xi), 0} − 1

(cid:9)
λ

.

Proof. We show the following implications.

(cid:8)

(cid:0)

(cid:1)

(cid:9)

• (a) ⇒ (b). If x∗ is an optimal solution, then (4) holds.
we have (σ∗
arguments, if x∗
i > 0 we have (σ∗
(4b) and (4d) we have −λ ≤ ∇iϕ(x∗) − µ∗ ≤ λ.

i < 0, from (4d)–(4f)
−)i = 0 and, from (4b), it follows that ∇iϕ(x∗) − µ∗ = λ. By the same
i = 0, from (4a),

+)i = 0 and ∇iϕ(x∗) − µ∗ = −λ. If x∗

If x∗

• (b) ⇒ (a). If (b) holds, then the KKT system (4) is satisﬁed by setting σ∗
λe − µ∗e and σ∗

− = −∇ϕ(x∗) + λe + µ∗e, implying that x∗ is an optimal solution.

+ = ∇ϕ(x∗)+

• (b) ⇒ (c). Let us rewrite ηmin(x∗) and ηmax(x∗) as follows:

ηmin(x∗) = min

n
ηmax(x∗) = max

min
i : x∗
i ≥0

max
i : x∗
i ≤0

∇iϕ(x∗) + λ, min
i : x∗
i <0
∇iϕ(x∗) − λ, max
i >0

i : x∗

∇iϕ(x∗) − λ

,

o
∇iϕ(x∗) + λ

.

o

If (b) holds, we have

n

(5a)

(5b)

and

∇iϕ(x∗) + λ 


≥ ∇iϕ(x∗) − λ = µ∗,
= µ∗,
≥ µ∗,



∇iϕ(x∗) − λ 


= µ∗,
≤ ∇iϕ(x∗) + λ = µ∗,
≤ µ∗,

i : x∗
i : x∗
i : x∗

i < 0,
i > 0,
i = 0,

i : x∗
i : x∗
i : x∗

i < 0,
i > 0,
i = 0.

Therefore, ηmin(x∗) ≥ µ∗ ≥ ηmax(x∗).



• (c) ⇒ (b). Let us consider ηmin(x∗) and ηmax(x∗) written as in (5). If (c) holds, let
µ∗ be any number in [ηmax(x∗), ηmin(x∗)]. If {i : x∗

i < 0} 6= ∅, we can write

min
i : x∗
i <0

∇iϕ(x∗) − λ ≤ max
i : x∗
i <0

∇iϕ(x∗) − λ ≤ max
i ≤0

i : x∗

∇iϕ(x∗) − λ

≤ ηmax(x∗) ≤ µ∗ ≤ ηmin(x∗) ≤ min
i : x∗
i <0

∇iϕ(x∗) − λ.

5

A decomposition method for lasso problems with zero-sum constraint

Therefore, all the inequalities in the above chain are actually equalities, implying that
µ∗ = mini : x∗

i <0 ∇iϕ(x∗) − λ = maxi : x∗

i <0 ∇iϕ(x∗) − λ. Namely,
∀i : x∗

i < 0,

µ∗ = ∇iϕ(x∗) − λ,

and the ﬁrst condition of (b) is satisﬁed. Similarly, if {i : x∗

max
i : x∗
i >0

∇iϕ(x∗) + λ ≥ min
i : x∗
i >0

∇iϕ(x∗) + λ ≥ min
i : x∗
i ≥0

i > 0} 6= ∅, we can write
∇iϕ(x∗) + λ

≥ ηmin(x∗) ≥ µ∗ ≥ ηmax(x∗) ≥ max
i >0

i : x∗

∇iϕ(x∗) + λ,

implying that

i > 0,
and the second condition of (b) is satisﬁed. Finally, if {i : x∗

µ∗ = ∇iϕ(x∗) + λ,

∀i : x∗

i = 0} 6= ∅, we can write

min
i : x∗
i =0

∇iϕ(x∗) + λ ≥ min
i : x∗
i ≥0
≥ max
i : x∗
i ≤0

∇iϕ(x∗) + λ ≥ ηmin(x∗) ≥ µ∗ ≥ ηmax(x∗)

∇iϕ(x∗) − λ ≥ max
i =0

i : x∗

∇iϕ(x∗) − λ,

implying that

∇iϕ(x∗) − λ ≤ µ∗ ≤ ∇iϕ(x∗) + λ,

∀i : x∗

i = 0,

and the third condition of (b) is satisﬁed.

Note that, by point (c) of Theorem 2.2, we can deﬁne ηmax(x) − ηmin(x) as a measure
of optimality violation. Moreover, now we derive an expression for the KKT multiplier µ∗
appearing in Theorem 2.2, which will be used in Section 3 to deﬁne an active-set estimate.

Theorem 2.3. If x∗ 6= 0 is an optimal solution of problem (1), then the corresponding
multiplier µ∗ appearing in point (b) of Theorem 2.2 is given by

|x∗

i |p

∇iϕ(x∗) + λ sign(x∗
i )

i : x∗
µ∗ = X

i 6=0

(cid:0)

|x∗

i |p

(cid:1)

,

∀p ≥ 0.

Xi : x∗
i 6=0

Proof. Consider the KKT multiplier µ∗ appearing in (4), which is the same as the one
appearing in point (b) Theorem 2.2. From (4a) and (4b), we obtain

+ = ∇ϕ(x∗) + λe − µ∗e,
σ∗
− = −∇ϕ(x∗) + λe + µ∗e.
σ∗

Using (4d)–(4f),
dimensional strictly convex problem, for all p ≥ 0:

it follows that µ∗ is the unique optimal solution of the following one-

min
µ

1
2 "

(∇iϕ(x∗) + λ − µ)2(x∗

i )p +

(−∇iϕ(x∗) + λ + µ)2(−x∗

i )p

.

Xi : x∗
i >0

Xi : x∗
i <0

6

(cid:21)

To compute µ∗ as the minimizer of the above univariate function, we have to set its derivative
to 0, that is,

(x∗

i )p(∇iϕ(x∗) + λ) +

(−x∗

i )p(∇iϕ(x∗) − λ)

Andrea Cristofari

µ∗ = Xi : x∗
i >0

(x∗

i )p +

Xi : x∗
i <0

(−x∗

i )p

,

leading to the expression given in the assertion.

i : x∗
X

i >0

i : x∗
X

i <0

Let us conclude this section by showing a regularity property of the non-zero feasible
points of problem (1), which will be useful in the global convergence analysis of the proposed
algorithm.

Proposition 2.4. Let ¯x be a feasible point of problem (1) such that ¯xj 6= 0 for an index
j ∈ {1, . . . , n}. If there exists I ⊆ {1, . . . , n} such that

f (¯x) = min
ξ∈R

f (¯x + ξ(ei − ej)),

∀i ∈ I,

then there exists ¯µ ∈ R such that

∇iϕ(¯x) − ¯µ = λ,
∇iϕ(¯x) − ¯µ = −λ,
|∇iϕ(¯x) − ¯µ| ≤ λ,




i ∈ I : ¯xi < 0,
i ∈ I : ¯xi > 0,
i ∈ I : ¯xi = 0.

(6)

Proof. For any point x ∈ Rn and a (non-zero) direction d ∈ Rn, let f ′(x; d) be the directional
derivative of f at x along d. We have



f ′(x; d) = lim
ε→0+

f (x + εd) − f (x)
ε

= ∇ϕ(x)T d + λ lim
ε→0+

kx + εdk1 − kxk1
ε

= ∇ϕ(x)T d + λ

n

i=1
X

sign(xi) di +

.

|di|

!

i : xi=0
X

(7)

Since f (¯x) = minξ∈R f (¯x + ξ(ei − ej)) for all i ∈ I by hypothesis, from the convexity of f we
get

f ′(¯x; ei − ej) ≥ 0 and f ′(¯x; ej − ei) ≥ 0,

∀i ∈ I.

(8)

Without loss of generality, we assume that ¯xj > 0 (the proof for the case ¯xj < 0 is identical,
except for minor changes). Using (7), we have

f ′(¯x; ei − ej) =

f ′(¯x; ej − ei) =

∇iϕ(¯x) − ∇jϕ(¯x) − 2λ,
∇iϕ(¯x) − ∇jϕ(¯x),

(

∇jϕ(¯x) − ∇iϕ(¯x) + 2λ,
∇jϕ(¯x) − ∇iϕ(¯x),

(

if ¯xi < 0,
if ¯xi ≥ 0;

if ¯xi ≤ 0,
if ¯xi > 0.

(9)

(10)

7

 
A decomposition method for lasso problems with zero-sum constraint

Therefore, in view of (8), for all i ∈ I such that ¯xi 6= 0 we can write

∇iϕ(¯x) =

∇jϕ(¯x) + 2λ,
∇jϕ(¯x),

(

i ∈ I : ¯xi < 0,
i ∈ I : ¯xi > 0.

So, we can set ¯µ = ∇jϕ(¯x) + λ and the ﬁrst two conditions of (6) are satisﬁed. Moreover,
with this choice of ¯µ we have |∇iϕ(¯x) − ¯µ| = |∇iϕ(¯x) − ∇jϕ(¯x) − λ|. So, to show that also
the last condition of (6) holds, we have to show that

0 ≤ ∇iϕ(¯x) − ∇jϕ(¯x) ≤ 2λ,

i ∈ I : ¯xi = 0.

This follows from (8), (9) and (10).

Remark 1. In Proposition 2.4, if I = {1, . . . , n}, then ¯x is an optimal solution of prob-
lem (1), according to point (b) of Theorem 2.2.

3 The algorithm

In this section we propose a decomposition algorithm, named Active-Set Zero-Sum-Lasso
(AS-ZSL), to eﬃciently solve problem (1).

The underlying assumptions motivating our approach are that the optimal solutions are
sparse, due to the sparsity-promoting ℓ1 regularization, and that the problem dimension is
large. To face these issues, the proposed method relies on two main ingredients:

(i) an active-set technique to estimate the zero variables in the optimal solution,

(ii) a 2-coordinate descent scheme to update only the variables estimated to be non-zero

in the optimal solution.

In the ﬁeld of constrained optimization, several active-set techniques were proposed to
identify the active (or binding) constraints, see, e.g., [4, 6, 12, 13, 15, 18, 19, 25, 26, 38,
40]. Active-set techniques were successfully used also to identify the zero variables in ℓ1-
regularized problems [7, 16, 30, 42, 49, 50] and in ℓ1-constrained problems [14]. Moreover,
screening rules were proposed to identify variables that can be discarded in lasso-type prob-
lems [21, 44, 51, 52, 48] and shrinking techniques have been widely used in some algorithms
for machine learning problems to ﬁx subsets of variables [8, 27, 29, 53].

Our approach makes use of the so-called multiplier functions, which will be deﬁned later,
adapting the active-set technique originally proposed in [18] for non-linearly constrained
problems.

To explain our active-set technique, let us ﬁrst deﬁne the following index sets for any

optimal solution x∗ of problem (1):

¯A(x∗) = {i : x∗
i = 0},
¯N (x∗) = {1, . . . , n} \ ¯A(x∗) = {i : x∗

i 6= 0}.

We say that ¯A(x∗) and ¯N (x∗) represent the active set and the non-active set, respectively,
at x∗. In any point xk produced by the algorithm, we get estimates of ¯A(x∗) and ¯N (x∗)

8

Andrea Cristofari

exploiting point (b) of Theorem 2.2. Namely, we set the vector πk as an approximation of
∇ϕ(xk) − µ∗e and deﬁne

Ak = {i : xk
i = 0, |πk
N k = {1, . . . , n} \ Ak,

i | ≤ λ},

(11a)

(11b)

as estimates of ¯A(x∗) and ¯N (x∗), respectively.

Once Ak and N k have been computed, we want to move only the variables estimated to
be non-zero in the ﬁnal solution, i.e., the variables in N k, thus working in a lower dimensional
space. Moreover, to eﬃciently address large-scale problems, a 2-coordinate descent scheme is
used for the variable update, that is, we move two coordinates at a time (this is the minimum
number of variables we can move to maintain feasibility, due to the equality constraint).

Given a feasible point xk 6= 0 produced by the algorithm, in the sequel we deﬁne two
possible strategies to compute πk in (11) and to update the variables. The ﬁrst strategy
uses the whole gradient ∇ϕ and is more accurate in the active-set estimate, while the second
strategy only uses partial derivatives and is computationally more eﬃcient. The rationale
is trying to balance accuracy and computational eﬃciency, in order to calculate the whole
gradient vector ∇ϕ only when needed. In particular, in our algorithm we never compute
the matrix AT A, since this is impractical for large dimensions. So, if the residual is known,
O(m) operations are needed to compute a single partial derivative, while O(mn) operations
are needed to compute ∇ϕ.

3.1 Strategy MVP

In this ﬁrst strategy, we need to compute the whole gradient ∇ϕ(xk). Then, to obtain
πk in (11), we use an approximation of the KKT multiplier µ∗ by means of the so called
multiplier functions. More precisely, given a neighborhood X of an optimal solution x∗, we
say that µ : X → R is a multiplier function if it is continuous in x∗ and such that µ(x∗) = µ∗.
A class of multiplier functions can be straightforwardly obtained from Theorem 2.3. Namely,
for all x 6= 0, we can deﬁne

n

|xi|p

∇iϕ(x) + λ sign(xi)

µ(x) =

i=1
X

(cid:0)

n

,

(cid:1)

p > 0.

(12)

|xi|p

i=1
X

Once ∇ϕ(xk) and µ(xk) have been computed, we can set

πk = ∇ϕ(xk) − µ(xk)e.

(13)

With this choice of πk we have the following identiﬁcation property, ensuring that, if we
are suﬃciently close to an optimal solution x∗, then i ∈ Ak ⇒ x∗
i = 0, while the inverse
implication (i.e., x∗

i = 0 ⇒ i ∈ Ak) holds if |∇iϕ(x∗) − µ∗| < λ.

9

A decomposition method for lasso problems with zero-sum constraint

Proposition 3.1. Let Ak and N k be deﬁned as in (11), with πk computed as in (13) and
µ(xk) computed as in (12). Then, for any optimal solution x∗ of problem (1), there exists a
neighborhood B(x∗) such that

¯A+(x∗) ⊆ Ak ⊆ ¯A(x∗),

∀xk ∈ B(x∗),

where ¯A+(x∗) = {i : x∗

i = 0, |∇iϕ(x∗) − µ∗| < λ} and µ∗ is the KKT multiplier.

Proof. The inclusion Ak ⊆ ¯A(x∗) is trivial, since x∗
borhood of x∗, we have xk
i
the continuity of ∇ϕ(x) and the continuity of µ(x), recalling that µ(x∗) = µ∗.

i 6= 0 implies that, for all xk in a neigh-
6= 0 and then i /∈ Ak. The inclusion ¯A+(x∗) ⊆ Ak follows from

To update the variables in N k, we use a 2-coordinate descent scheme based on the so called
maximal violating pair (MVP), i.e., we move the two variables that most violate an optimality
In the literature, similar choices were considered for singly linearly constrained
measure.
problems when the objective function is smooth, using proper optimality measures (see,
e.g., [5, 29, 31, 35, 36]). In our case, using point (c) of Theorem 2.2, a maximal violating
pair in N k is deﬁned as any pair (ˆı, ˆ) such that

ˆı ∈ Argmin

i∈N k

∇iϕ(xk) +

2 min{sign(xk

i ), 0} + 1

λ

,

(cid:8)
ˆ ∈ Argmax

j∈N k

(cid:0)
∇jϕ(xk) +

2 max{sign(xk

(cid:1)
j ), 0} − 1

(cid:9)
λ

.

(cid:8)

(cid:0)

(cid:1)

(cid:9)

(14a)

(14b)

The next feasible point xk+1 is then obtained by minimizing f with respect to xˆı and xˆ,

keeping all the other variables ﬁxed in xk, that is,

xk+1 ∈ Argmin{f (x) : eT x = 0, xh = xk

h, h ∈ {1, . . . , n} \ {ˆı, ˆ}}.

Equivalently,

xk+1 = xk + ξ∗(eˆı − eˆ),
ξ∗ ∈ Argmin

{f (xk + ξ(eˆı − eˆ))}.

ξ∈R

(15)

3.2 Strategy AC2CD

This second strategy does not require to compute the whole gradient ∇ϕ. In (11) we simply
set πk = πk−1 and then we update the variables in N k by means of an almost cyclic rule. In
particular, we extend a 2-coordinate descent method, named AC2CD, proposed in [10] and
further analyzed in [11]. Considering singly linearly constrained problems with lower and
upper bound on the variables, the method proposed in [10] chooses two variables at a time,
such that one of them must be “suﬃciently far” from the lower and the upper bound in some
points produced by the algorithm, while the other one is picked cyclically. In order to adapt
this approach to our setting, we ﬁrst have to choose a variable index j(k) such that

|xk

j(k)| ≥ τ kxkk∞,

j(k) ∈ N k,

(16)

10

Andrea Cristofari

where τ ∈ (0, 1] is a ﬁxed parameter. Namely, we require xk
j(k) to be “suﬃciently diﬀerent”
from zero. Then, we start a cycle of inner iterations where we update two variables at a time.
|N k| of N k, then we
In particular, ﬁrst we set zk,1 = xk and choose a permutation pk
i at a time in a cyclic fashion, with i = 1, . . . , |N k|, and consider the index
select one index pk
pair (pk
and zj(k), keeping
all the other variables ﬁxed in zk,i. Namely,

i , j(k)). We compute zk,i+1 by minimizing f (z) with respect to zpk

1, . . . , pk

i

zk,i+1 ∈ Argmin{f (z) : eT z = 0, zh = zk,i

h , h ∈ {1, . . . , n} \ {pk

i , j(k)}}.

Equivalently,

zk,i+1 = zk,i + ξ∗(epk
ξ∗ ∈ Argmin

i

{f (zk,i + ξ(epk

i

− ej(k)),

ξ∈R

− ej(k)))}.

(17)

After producing the points zk,1, zk,2, . . . , zk,|N k|+1, we set the next feasible point xk+1 =
zk,|N k|+1.

3.3 Choosing between the two strategies

We have seen that, one the one hand, Strategy AC2CD does not need the expensive compu-
tation of the whole gradient ∇ϕ, but, on the other hand, we expect the active-set estimate
used in Strategy MVP to be more accurate in a neighborhood of an optimal solution, ac-
cording to Proposition 3.1. In order to balance computational eﬃciency and accuracy, we
want to use Strategy MVP only when we judge it is worthwhile to compute a new gradient
and a new πk in (11). This occurs when we observe no suﬃcient progress in the objective
function. In particular, given a parameter θ ∈ (0, 1], at iteration k we use Strategy MVP if
both f (xk−1) − f (xk) ≤ θ max{f (xk−1), 1} and Strategy MVP was not used in xk−1, oth-
erwise we use Strategy AC2CD. As to be shown below, eventually the algorithm alternates
between the two strategies.

The scheme of the proposed AS-ZSL method is reported in Algorithm 1.

Remark 2. Both Strategy MVP and Strategy AC2CD require to solve a subproblem for every
variable update, given in (15) and (17), respectively. We see that each subproblem consists
in an exact minimization of f over a direction of the form ±(ei − ej). A ﬁnite procedure for
this minimization is described in Appendix A.

4 Convergence analysis

In this section, we show the convergence of AS-ZSL to optimal solutions, using some results
on the proposed active-set estimate and standard arguments on block coordinate descent
methods [23, 33, 46].

First, we note that most results stated in the above sections require xk 6= 0.

Indeed,

AS-ZSL ensures that

xk 6= 0,

∀k ≥ 1,

11

A decomposition method for lasso problems with zero-sum constraint

Algorithm 1 Active-Set Zero-Sum-Lasso (AS-ZSL)

0 Given θ ∈ (0, 1] and τ ∈ (0, 1], set x0 = 0 and k = 0
1 If x0 is not optimal, compute x1 such that f (x1) < f (x0), set k = 1 and go to line 4

2 While xk is not optimal

If f (xk−1) − f (xk)
max{f (xk−1), 1}

≤ θ and Strategy MVP was not used for xk−1

Strategy MVP
Compute Ak and N k as in (11), with πk computed as in (13)
Compute a maximal violating pair (ˆı, ˆ) as in (14)
Compute xk+1 as in (15)

Else

Strategy AC2CD
Compute Ak and N k as in (11), with πk = πk−1
Choose a variable index j(k) satisfying (16)
Set zk,1 = xk
Choose a permutation {pk
For i = 1, . . . , |N k|

|N k|} of N k

1 , . . . , pk

Compute zk,i+1 as in (17)

End for
Set xk+1 = zk,|N

k

|+1

End if

3

4

5

6

7

8

9

10

11

12

13

14

15

16

Set k = k + 1

17
18 End while

since x0 = 0 and

f (xk+1) ≤ f (xk) < f (x0) = f (0),

∀k ≥ 1.

(18)

As a consequence, for all k ≥ 1,

• the multiplier function µ(x) given in (12) is well deﬁned at xk;

• N k 6= ∅, according to (11).

Since f is continuous and coercive, also the following result follows from (18), ensuring

the convergence of {f (xk)} and the existence of limit points for {xk}.

Lemma 4.1. Let {xk} be an inﬁnite sequence of points produced by AS-ZSL. Then,

(i) lim
k→∞

f (xk) = f ∗ ∈ R, with f ∗ > 0;

12

Andrea Cristofari

(ii) every subsequence {xk}K⊆N has limit points, each of them being feasible and diﬀerent

from zero.

Moreover, for any iteration k where Strategy AC2CD is used, from the instructions of

the algorithm we have

f (xk+1) = f (zk,|N k|+1) ≤ f (zk,|N k|) ≤ . . . ≤ f (zk,1) = f (xk).

So, still using (18) and the fact that f is continuous and coercive, we have the following
result. It ensures, over a subsequence of points produced by Strategy AC2CD, the existence
of limit points and the convergence of the objective function.

Lemma 4.2. Let {xk}K⊆N be an inﬁnite subsequence of points produced by AS-ZSL such that
Strategy AC2CD is used for all k ∈ K. Then, for every ﬁxed i ∈ {1, . . . , min
k∈K

|N k| + 1},

(i) lim
k→∞
k∈K

f (zk,i) = lim
k→∞

f (xk) = f ∗ ∈ R, with f ∗ > 0;

(ii) the subsequence {zk,i}K has limit points, each of them being feasible and diﬀerent from

zero.

Now, we show that AS-ZSL cannot select Strategy MVP or Strategy AC2CD for an
arbitrarily large number of consecutive iterations. In particular, provided {xk} is inﬁnite,
eventually the algorithm alternates between the two strategies.

Proposition 4.3. Let {xk} be an inﬁnite sequence of points produced by AS-ZSL. Then, there
exists ¯k such that Strategy MVP is used for k = ¯k, ¯k + 2, ¯k + 4, . . . and Strategy AC2CD is
used for k = ¯k + 1, ¯k + 3, ¯k + 5, . . ..

Proof. From point (i) of Lemma 4.1 and (18), the sequence {f (xk)} converges to a value f ∗
such that f (xk) ≥ f ∗ for all k ≥ 0. It follows that

f (xk−1) − f (xk)
θ max{f (xk−1), 1}

lim
k→∞

≤ lim
k→∞

f (xk−1) − f (xk)
θ max{f ∗, 1}

= 0.

Therefore, according to the test at line 3 of Algorithm 1, the algorithm alternates between
Strategy MVP and Strategy AC2CD inﬁnitely for suﬃciently large k.

4.1 Global convergence to optimal solutions

Without loss of generality, for every index pair (i, j) selected by Strategy MVP or Strategy
AC2CD, now we require f to be strictly convex along the directions ±(ei − ej).
In Ap-
pendix A.1, we show how this can be easily guaranteed. Essentially, we just have to remove
variables from the problem when we ﬁnd identical columns in the matrix A. Using this
non-restrictive requirement, the following results show that kxk+1 − xkk → 0.

13

A decomposition method for lasso problems with zero-sum constraint

Proposition 4.4. Let {xk}K1⊂N and {xk}K2⊂N be two inﬁnite subsequences of points pro-
duced by AS-ZSL, such that Strategy MVP is used for all k ∈ K1 and Strategy AC2CD is used
for all k ∈ K2. Then,

kxk+1 − xkk = 0,

lim
k→∞
k∈K1

lim
k→∞
k∈K2

kxk+1 − xkk = lim
k→∞
k∈K2

|N k|

i=1
X

kzk,i+1 − zk,ik = 0.

(19a)

(19b)

Proof. By contradiction, assume that (19a) does not hold. It follows that there exists a sub-
sequence {xk}K3⊆K1 such that lim inf k→∞, k∈K3kxk+1 − xkk > 0. By point (ii) of Lemma 4.1,
we can assume that both {xk}K3 and {xk+1}K3 converge to feasible points (passing into a
further subsequence if necessary), so that

lim
k→∞
k∈K3

xk = x′ 6= x′′ = lim
k→∞
k∈K3

xk+1.

(20)

Since the set of variable indices is ﬁnite, we can also assume that the maximal violating
pair (ˆı, ˆ) is the same for all k ∈ K3 (passing again into a further subsequence if necessary).
Using (15), (20) and the continuity of f , we obtain

x′′ = x′ + ξ∗(eˆı − eˆ), where

ξ∗ ∈ Argmin

ξ∈R

{f (x′ + ξ(eˆı − eˆ))}.

Namely, x′ and x′′ belong to the line {x′ + ξ(eˆı − eˆ), ξ ∈ R}. As said at the beginning of this
subsection, without loss of generality here we require f to be strictly convex along ±(eˆı − eˆ)
(in Appendix A.1 it is shown how this can be easily guaranteed). Since x′ 6= x′′, it follows
that

x′ + x′′
2

f

(cid:16)

(cid:17)

1
2

1
2

<

f (x′) +

f (x′′).

(21)

Moreover, using again (15) we can write

f (xk+1) ≤ f

1
2

xk +
(cid:16)

(xk+1 − xk)
(cid:17)

= f

(cid:16)

xk + xk+1
2

≤

f (xk)
2

+

f (xk+1)
2

,

(cid:17)

where the last inequality follows from the convexity of f . Since f (xk+1) ≤ f (xk), we obtain

f (xk+1) ≤ f

≤ f (xk).

(22)

xk + xk+1
2

(cid:16)

(cid:17)

By (20) and point (i) of Lemma 4.1, using the continuity of f we have that

f (x′) = lim
k→∞
k∈K3

f (xk) = lim
k→∞
k∈K3

f (xk+1) = f (x′′).

Therefore, taking the limits in (22), we get f

x′ + x′′
2

f

(cid:16)

=

(cid:17)

= f (x′) = f (x′′), that is,

f (x′′),

x′ + x′′
2

(cid:16)
1
f (x′) +
2

(cid:17)
1
2

14

Andrea Cristofari

contradicting (21).

To show (19b), from the instructions of the algorithm we can write xk+1−xk =

zk,i) for all k ∈ K2, implying that

|N k|
i=1 (zk,i+1−

P

|N k|

kxk+1 − xkk ≤

kzk,i+1 − zk,ik,

∀k ∈ K2.

i=1
X

So, to prove the desired result, we just have to show that

|N k|

kzk,i+1 − zk,ik = 0.

lim
k→∞
k∈K2

i=1
X

Arguing by contradiction, assume that there exist an index ¯ı ∈ {1, . . . , n} and a subsequence
{zk,¯ı}K4⊆K2 such that lim inf k→∞, k∈K4kzk,¯ı+1 − zk,¯ık > 0 for all k ∈ K4. By point (ii)
of Lemma 4.2, we can assume that both {zk,¯ı}K and {zk,¯ı+1}K converge to feasible points
(passing into a further subsequence if necessary). Thus, we get a contradiction by the same
arguments used to prove (19a).

Proposition 4.5. Let {xk} be an inﬁnite sequence of points produced by AS-ZSL. Then,

kxk+1 − xkk = 0.

lim
k→∞

Proof. From Proposition 4.3, two inﬁnite subsequences {xk}K1⊂N and {xk}K2⊂N exist such
that Strategy MVP is used for all k ∈ K1 and Strategy AC2CD is used for all k ∈ K2, with
K1 ∪ K2 = N. Then, the desired result follows from Proposition 4.4.

We are ﬁnally ready to show global convergence of AS-ZSL to optimal solutions.

Theorem 4.6. Let {xk} be an inﬁnite sequence of points produced by AS-ZSL. Then, {xk}
has limit points and every limit point is an optimal solution of problem (1).

Proof. From point (ii) of Lemma 4.1, {xk} has limit points, each of them being feasible and
diﬀerent from zero. Let x∗ be any of these limit points and let {xk}K⊆N be a subsequence
In view of Proposition 4.5, x∗ is also a limit point of {xk+1}K and of
converging to x∗.
{xk−1}K . Namely,

lim
k→∞
k∈K

xk = lim
k→∞
k∈K

xk+1 = lim
k→∞
k∈K

xk−1 = x∗ 6= 0.

(23)

Using Proposition 4.3, without loss of generality we can assume that Strategy AC2CD is
used for all k ∈ K. (In particular, if Strategy AC2CD is used for inﬁnitely many k ∈ K, we
can simply discard from {xk}K the indices k where Strategy AC2CD is not used. On the
contrary, if ¯k exists such that Strategy MVP is used for all k ≥ ¯k, k ∈ K, then we can consider
the subsequence {xk+1}K instead. This subsequence still converges to x∗ by (23) and, for all
suﬃciently large k ∈ K, Strategy AC2CD is used for all k + 1 in view of Proposition 4.3.)

15

A decomposition method for lasso problems with zero-sum constraint

Since the set of variable indices is ﬁnite, for all k ∈ K we can assume that Ak, N k, j(k)

and pk

i are the same, that is,

Ak = A,

N k = N ,

j(k) = ¯,

pk
i = pi,

∀i ∈ {1, . . . , |N |}

(passing into a further subsequence if necessary) and, by point (ii) of Lemma 4.2, that

zk,i = ¯zi,

∀i ∈ {1, . . . , |N | + 1}

(24)

lim
k→∞
k∈K

(passing again into a further subsequence if necessary).

By continuity of f , we can take the limits in (17) and, using (24), for all i ∈ {1, . . . , |N |}

we have

¯zi+1 = ¯zi + ξ∗(epi − e¯),
ξ∗ ∈ Argmin

{f (¯zi + ξ(epi − e¯))}.

ξ∈R

Now, we show that

¯zi = x∗,

∀i ∈ {1, . . . , |N | + 1}.

(25)

(26)

From the instructions of the algorithm we have zk,1 = xk, so we can write kzk,i − xkk ≤

i−1
h=1kzk,h+1 − zk,hk for all i ∈ {1, . . . , |N | + 1}. Using (19b), it follows that

P

kzk,i − xkk = 0,

lim
k→∞
k∈K

∀i ∈ {1, . . . , |N | + 1}.

(27)

Since kzk,i − x∗k ≤ kzk,i − xkk + kxk − x∗k, from (23) and (27) we get

kzk,i − x∗k = 0,

lim
k→∞
k∈K

∀i ∈ {1, . . . , |N | + 1}.

Using (24), we thus obtain (26).

Therefore, from (25) and (26) it follows that

f (x∗) = min
ξ∈R

f (x∗ + ξ(ei − e¯)),

∀i ∈ N .

(28)

Using (23), a real number η > 0 exists such that kxkk∞ ≥ η/τ for all suﬃciently large
¯ | ≥ τ kxkk∞ (see line 9
¯ | ≥ η, and then,

k ∈ K, where τ ∈ (0, 1] is the parameter used in AS-ZSL such that |xk
of Algorithm 1). Consequently, for all suﬃciently large k ∈ K, we have |xk

So, using (28), (29) and Proposition 2.4, there exists µ∗ ∈ R such that

x∗
¯ 6= 0.

∇iϕ(x∗) − µ∗ = λ,
∇iϕ(x∗) − µ∗ = −λ,
|∇iϕ(x∗) − µ∗| ≤ λ,

i ∈ N : x∗
i ∈ N : x∗
i ∈ N : x∗

i < 0,
i > 0,
i = 0.

16






(29)

(30)

Andrea Cristofari

Now, from the active-set estimate (11a) we observe that i ∈ A ⇒ xk

i = 0 for all k ∈ K.

Since {xk}K → x∗ from (23), it follows that

Taking into account (30) and (31), according to point (b) of Theorem 2.2 we thus have to
show that

A ⊆ {i : x∗

i = 0}.

(31)

(32)
in order to prove that x∗ is optimal and conclude the proof. Note that, from (30) and (31),
we can write |x∗

i |pµ∗, i = 1, . . . , n. This implies that

∇iϕ(x∗) + λ sign(x∗
i )

∀i ∈ A,

= |x∗

|∇iϕ(x∗) − µ∗| ≤ λ,

i |p

(cid:0)

n

(cid:1)

∇iϕ(x∗) + λ sign(x∗
i )

n

= µ∗

|x∗

i |p.

|x∗

i |p

i=1
X

(cid:0)

(cid:1)

i=1
X

So, using the deﬁnition of the multiplier function µ(x) given in (12), we get

µ∗ = µ(x∗).

(33)

Moreover, Proposition 4.3 ensures that, for suﬃciently large k ∈ K, Strategy MVP is used
at xk−1, implying that πk = πk−1 = ∇ϕ(xk−1) − µ(xk−1)e (see lines 8 and 4 of Algorithm 1).
Therefore, from the active-set estimate (11a), for all suﬃciently large k ∈ K we can write

A = {i : xk

i = 0, |∇iϕ(xk−1) − µ(xk−1)| ≤ λ}.

Since {xk}K and {xk−1}K converge to x∗ from (23), using the continuity of ∇ϕ, the continuity
of the multiplier function µ(x) and (33), we can take the limits for k → ∞, k ∈ K, and we
ﬁnally get (32).

5 Numerical results

In this section, we report the numerical results obtained on synthetic and real datasets with
compositional data. We implemented AS-ZSL in C++, using a MEX ﬁle to call the algorithm
from Matlab. The AS-ZSL software is available at https://github.com/acristofari/as-zsl.
In our experiments, we use p = 1 for the multiplier functions deﬁned in (12) and τ = 1.
Moreover, θ is set to 10−2 at the beginning of the algorithm and is gradually decreased to
10−6. All tests were run on an Intel(R) Core(TM) i7-9700 with 16 GB RAM memory.

We compared AS-ZSL with the following algorithms:

• compCL [32], which solves (1) by the method of multiplier minimizing the augmented La-
grangian function by cyclic coordinate descent. The code was written in C++ and called
from R. It was downloaded from https://cran.r-project.org/package=Compack as
part of the R package Compack.

• QP [20], which solves the quadratic reformulation (3). Since n > m, a ridge term
10−4kxk2 was added to the original objective function, as suggested in [20]. The code
was downloaded from https://github.com/Hua-Zhou/SparseReg, it builds the prob-
lem via Matlab and uses the Gurobi Optimizer (version 9.5) [24] for the minimization.

17

A decomposition method for lasso problems with zero-sum constraint

• zeroSum [3], which uses an extension of the random coordinate descent method to
solve (1). The code was written in C++ and called from R. It was downloaded from
https://github.com/rehbergT/zeroSum as part of the R package zeroSum.

These algorithms were run with their default parameters and options, except those spec-
iﬁed above. We observe that both compCL and zeroSum use a (block) coordinate descent
approach and were speciﬁcally designed for regression problems with zero-sum constraint,
while QP uses the default algorithm implemented in Gurobi for quadratic programs, i.e., the
barrier algorithm.

The results obtained from the comparisons are described in Subsection 5.1 and 5.2. Fi-
nally, in Subsection 5.3 we show how a warm start strategy can be used in AS-ZSL to solve
a sequence of problems with decreasing regularization parameters.

5.1 Synthetic datasets

In the ﬁrst experiments, we generated some synthetic datasets for log-contrast model as
suggested in [32], using the function comp Model implemented in the Compack package. More
speciﬁcally, a matrix M ∈ Rm×n was ﬁrst generated from a multivariate normal distribution
N (ω, Σ). To model the presence of ﬁve major components in the composition, the vector ω
has all zeros except for ωi = log(0.5n), i = 1, . . . , 5. The matrix Σ has 0.5|i−j| in position
(i, j). Then, the log-contrast model (2) was obtained by setting the matrix A such that
Aij = log(Zij) in position (i, j), with

,

i = 1, . . . , m,

j = 1, . . . , n,

Zij =

eMij
n

eMih

h=1
X

the vector of regression coeﬃcients x = (1, −0.8, 0.6, 0, 0, −1.5, −0.5, 1.2, 0, . . . , 0)T and the
noise terms in ε were generated from a normal distribution N (0, 0.52). We see that x has six
non-zero coeﬃcients, with three of them being among the ﬁve major components.

We considered problems with dimensions m = 2000 and n ∈ {m, 2m, 5m}. For every pair
(m, n) we generated 10 diﬀerent datasets and, for each of them, we used λ ∈ {λ1, . . . , λ5}
such that λ1, . . . , λ5 are logarithmically equally spaced between 10λ1 and 10λ5 , where

λ1 = 0.95λmax, λ5 = 10−3λmax

and λmax is such that x∗ = 0 is an optimal solution of problem (1) if and only if λ ≥ λmax.
We can easily compute λmax from point (c) of Theorem 2.2 (it also follows from Corollary 1
of [28]):

λmax =

max
j=1,...,n

[(AT )y]j − min
i=1...,n

[(AT )y]i

2

.

In
In Table 1, we show the results obtained with the diﬀerent problem dimensions.
particular, for each considered λ, we report the average values over the 10 runs in terms
of ﬁnal objective value and CPU time. We see that AS-ZSL always took less 2 seconds on
average to solve all the considered problems, being much faster than the other methods

18

Table 1: Results on synthetic datasets for log-contrast models with six non-zero regression
coeﬃcients. The ﬁnal objective value is indicated by f ∗, while the CPU time in seconds is
indicated by time.

Andrea Cristofari

m = 2000, n = 2000

λ1

f ∗

AS-ZSL 4.70e+04
compCL 4.70e+04
4.70e+04
zeroSum 4.71e+04

QP

λ2

f ∗

1.61e+04
1.61e+04
1.61e+04
4.71e+04

time

0.03
1.44
7.06
2.54

λ3

f ∗

4.71e+03
4.71e+03
4.71e+03
2.79e+04

time

0.04
1.44
6.40
2.55

λ4

f ∗

1.59e+03
1.59e+03
1.59e+03
4.61e+03

time

0.20
5.02
7.47
2.59

λ5

f ∗

5.21e+02
5.37e+02
5.21e+02
2.01e+03

time

0.38
5.89
7.14
3.31

time

0.02
2.10
7.38
2.53

m = 2000, n = 4000

λ1

λ2

λ3

λ4

λ5

f ∗

time

f ∗

time

f ∗

time

f ∗

time

f ∗

time

AS-ZSL 5.41e+04
compCL 5.41e+04

0.04
4.30

5.66e+02
1.22
5.85e+02 11.49
5.41e+04 104.77 1.84e+04 96.41 5.19e+03 80.44 1.79e+03 105.43 5.66e+02 97.74
3.85e+04 10.03 5.51e+03 10.02 2.16e+03 11.47

1.79e+03
1.79e+03

1.84e+04
1.84e+04

5.19e+03
5.19e+03

5.42e+04

0.36
9.98

0.09
2.82

0.09
2.77

9.93

9.97

zeroSum 5.42e+04

QP

λ1

λ2

f ∗

time

f ∗

AS-ZSL 6.50e+04
2.18e+04
compCL 6.50e+04 11.30 2.18e+04

0.10

m = 2000, n = 10000

λ3

f ∗

5.81e+03
5.81e+03

time

0.24
7.08

time

0.19
6.86

λ4

λ5

f ∗

time

f ∗

time

2.04e+03
1.64
2.04e+03 26.21 6.54e+02 29.84

6.30e+02

0.79

QP

6.50e+04 3239.67 2.18e+04 2537.57 5.81e+03 2364.73 2.04e+03 2738.23 6.30e+02 2745.07

zeroSum 6.51e+04 61.90 6.51e+04 61.60 6.45e+04 61.92 7.22e+03 61.50 2.28e+03 63.73

and also achieving the lowest objective function value. In particular, AS-ZSL is one or two
orders of magnitude faster than the other coordinate descent based methods, i.e., compCL
and zeroSum.

Next, we used the same datasets described above, but with the vector of regression
coeﬃcients x containing 5% of randomly chosen non-zero entries, which were generated from
a uniform distribution in (−1, 1).

The results are shown in Table 2. Also in this case, we see that AS-ZSL achieves the
lowest objective function value and is the fastest method, except for the largest problems
with λ5, where compCL took less time, but it returned a higher objective function value.

5.2 Real datasets

Now we show the results obtained on real microbiome data, which are generally regarded
as compositional in the literature (see, e.g., [9, 22, 34, 41]). We used three datasets con-
sidered in [37], downloaded from https://github.com/nphdang/DeepCoDA and containing
data from [47] suitably adjusted to deal with log-contrast model by proper replacement of
the zero features (which are not allowed in such models). The considered datasets are for
binary classiﬁcation (then, responses are in {0, 1}) and are described in Table 3.

19

A decomposition method for lasso problems with zero-sum constraint

Table 2: Results on synthetic datasets for log-contrast models with 5% of non-zero regression
coeﬃcients. The ﬁnal objective value is indicated by f ∗, while the CPU time in seconds is
indicated by time.

λ1

f ∗

AS-ZSL 3.46e+04
compCL 3.46e+04
3.46e+04
zeroSum 3.46e+04

QP

time

0.03
4.18
6.85
2.43

m = 2000, n = 2000

λ2

λ3

λ4

λ5

f ∗

time

f ∗

time

f ∗

0.12

1.85e+04
1.43e+03
6.03e+03
1.94e+04 10.21 8.39e+03 10.64 3.40e+03
1.43e+03
6.03e+03
1.85e+04
1.49e+03
7.71e+03
2.02e+04

9.69
3.95

6.74
3.08

0.27

time

0.55
9.83
6.50
5.18

f ∗

time

1.48
2.71e+02
9.16
1.52e+03
2.71e+02
6.25
2.73e+02 40.79

m = 2000, n = 4000

λ1

λ2

λ3

λ4

λ5

f ∗

time

f ∗

time

f ∗

time

f ∗

time

f ∗

time

0.05

AS-ZSL 8.52e+04
5.84
3.22e+04
compCL 8.52e+04 17.26 5.51e+04 19.75 3.50e+04 20.39 1.48e+04 18.10 4.89e+03 17.81
8.52e+04 96.86 5.36e+04 103.67 3.22e+04 95.91 1.14e+04 99.98 2.33e+03 96.76
6.75e+04 12.09 4.45e+04 16.19 1.19e+04 22.96 2.35e+03 149.59

zeroSum 8.52e+04

2.33e+03

1.14e+04

5.36e+04

0.69

0.31

2.20

9.78

QP

m = 2000, n = 10000

λ1

λ2

λ3

λ4

λ5

f ∗

time

f ∗

time

f ∗

time

f ∗

time

f ∗

time

AS-ZSL 1.73e+05
1.40e+04 18.53 2.86e+03 95.71
4.85e+04
compCL 1.73e+05 34.30 1.21e+05 46.58 5.41e+04 53.06 2.01e+04 40.07 5.68e+03 30.30

1.18e+05

3.52

0.91

0.15

QP

1.73e+05 2622.73 1.18e+05 2890.54 4.85e+04 2963.40 1.40e+04 3216.94 2.86e+03 3586.99
zeroSum 1.73e+05 61.94 1.23e+05 92.17 5.47e+04 206.69 1.52e+04 700.20 3.24e+03 1930.67

For each dataset, ﬁrst we applied a log transformation and then we chose λ by a 5-fold
cross validation. The ﬁnal results are shown in Table 4. We see that AS-ZSL took less than 1
second on all the problems, still being the fastest method and achieving the lowest objective
function value.

5.3 Optimizing over a grid of regularization parameters

In the previous experiments, we have shown the performances of AS-ZSL on several instances
of problems (1) for a speciﬁc value of the regularization parameter λ. Now, we want to analyze
a simple warm start strategy for AS-ZSL to solve a sequence of problems with decreasing
regularization parameters. This can be useful in practice when a suitable value of λ is not

Table 3: Microbiome datasets from [37, 47].

Dataset

m

n

Class 1

Class 2

1
2
3

2070
404
408

3090
3090
3090

Gastro
Stool
Subgingival

Oral
Tongue
Supragingival

20

Table 4: Results on microbiome datasets. The ﬁnal objective value is indicated by f ∗, while
the CPU time in seconds is indicated by time.

Andrea Cristofari

Dataset 1

Dataset 2

Dataset 3

f ∗

AS-ZSL
compCL
QP
zeroSum

3.38e+01
3.38e+01
3.38e+01
2.56e+02

time

0.48
13.86
95.63
7.27

f ∗

2.84e+01
2.95e+01
2.84e+01
7.35e+01

time

0.20
2.61
57.50
1.77

f ∗

7.18e+01
7.18e+01
7.18e+01
1.05e+02

time

0.08
4.39
81.98
0.97

Table 5: Comparison of CPU time required by AS-ZSL with and without warm start over 10
values of λ. All results are in seconds.

λ1

λ2

λ3

λ4

λ5

λ6

λ7

λ8

λ9

λ10

time with warm start
time without warm start

7.83 12.34 15.60 19.45
0.14 0.46 0.99 1.77 2.95 4.87
0.14 0.44 0.80 1.29 2.79 5.24 10.94 24.69 52.16 104.07

cumulative time with warm start

0.14 0.60 1.59 3.36 6.31 11.18 19.01 31.35 46.95 66.40
cumulative time without warm start 0.14 0.58 1.38 2.67 5.46 10.69 21.63 46.32 98.48 202.55

known in advance and a parameter selection procedure must be carried out.

We generated 10 synthetic datasets as explained in Subsection 5.1, with m = 2000,
n = 10000 and 5% of non-zero entries in the vector of regression coeﬃcients. For each
dataset, we considered 10 diﬀerent parameters λ1, . . . , λ10 logarithmically equally spaced
between 10λ1 and 10λ10 , where λ1 = 0.95λmax, λ10 = 10−3λmax and λmax was computed as
explained in Subsection 5.1.

For λ = λi, i = 2, . . . , 10, the warm start strategy simply consists in setting the starting

point of AS-ZSL as the optimal solution computed with λ = λi−1.

In Table 5, we report the average results obtained with and without warm start. From
the third and the fourth row of the table, we observe that the warm start strategy allowed
us to complete the whole optimization process in 66 seconds, while it took more than 200
seconds without warm start. From the second and the third row of the table, we also note
that running AS-ZSL with one small λ took more than running AS-ZSL several times using
decreasing regularization parameters with warm start. For example, AS-ZSL took 104 seconds
for λ = λ10 without warm start, but it took a total of 66 seconds when it was run ten times
with λ = λ1, . . . , λ10 using the warm start strategy. This suggests that the warm start
strategy might be used to further speed up the algorithm when problem (1) must be solved
with small values of λ.

6 Conclusions

In this paper we proposed AS-ZSL, a 2-coordinate descent method with active-set estimate to
solve lasso problems with zero-sum constraint. At every iteration, AS-ZSL chooses between
two strategies: Strategy MVP uses the whole gradient of the smooth term of the objective
function and is more accurate in the active-set estimate, while Strategy AC2CD only needs
partial derivatives and is computationally more eﬃcient. A suitable test is used to choose

21

A decomposition method for lasso problems with zero-sum constraint

between the two strategies, considering the progress in the objective function. A theoretical
analysis was carried out, showing global convergence of AS-ZSL to optimal solutions. We
performed numerical experiments on synthetic and real datasets, showing the eﬀectiveness
of the proposed method compared to other algorithms from the literature. In particular, the
good numerical results are due both to the identiﬁcation of the zero variables carried out
by the proposed active-set estimate and to the possibility of choosing, at each iteration, be-
tween Strategy MVP and Strategy AC2CD, in order to balance accuracy and computational
eﬃciency.

We ﬁnally outlined a warm start strategy for AS-ZSL to solve a sequence of problems
with decreasing regularization parameters, which may be useful when a parameter selection
procedure must be carried out and many problems with diﬀerent regularization parameters
must be solved.

Possible directions for future research might include the extension of these results to a
more general class of problems, for example considering other loss functions, other regular-
ization terms and other types of constraints.

Appendix A. The subproblem

According to (15) and (17), every variable update in AS-ZSL requires the resolution of a sub-
problem, both when using Strategy MVP and when using Strategy AC2CD. As highlighted
in Remark 2, each subproblem consists in the minimization of f along a direction of the form
±(ei − ej). In the next proposition, we give an equivalent expression of f along ±(ei − ej)
as an univariate function, which will be useful to compute the minimizer.

Proposition A.1. Let ¯x be any feasible point of problem (1). For any i 6= j, we have

f (¯x + ξ(ei − ej)) = f i,j

¯x (¯xi + ξ),

∀ξ ∈ R,

where f i,j

¯x : R → R is the function deﬁned as follows:

f i,j
¯x (u) =

1
2

αu2 − βu + λ(|u| + |u − ¯xi − ¯xj|) + c,

with

α = kAi − Ajk2,
β = α¯xi − ∇iϕ(¯x) + ∇jϕ(¯x),

c =

1
2

ky − A¯x + (Ai − Aj)¯xik2 + λ

|¯xt|.

Proof. First, let us write the function f as follows:

f (x) =

1
2

m

Xh=1(cid:18)

Ahixi + Ahjxj +

Ahtxt − yh

+ λ

|xi| + |xj| +

|xt|

.

Xt6=i,j

22

(cid:19)

(cid:18)

Xt6=i,j

(cid:19)

Xt6=i,j

2

Andrea Cristofari

Now, choose any ξ ∈ R and let u = ¯x + ξ(ei − ej). To prove the desired result, we have to
show that

f (u) = f i,j

¯x (ui) =

αu2

i − βT ui + λ(|ui| + |ui − ¯xi − ¯xj|) + c.

(34)

1
2

Clearly, ut = ¯xt for all t 6= i, j. Since eT ¯x = 0 from the feasibility of ¯x, it follows that eT u = 0
and uj = −

t6=i,j ¯xt − ui = ¯xi + ¯xj − ui. Then,

t6=i,j ut − ui = −

P

f (u) =

1
2

m

P
(Ahi − Ahj)ui +

(Aht − Ahj)¯xt − yh

h=1(cid:18)
X

t6=i,j
X
|ui| + |ui − ¯xi − ¯xj| +

+ λ

(cid:18)

|¯xt|

.

(cid:19)

t6=i,j
X

2

+

(cid:19)

Now, let us deﬁne the vector ρ ∈ Rm as

ρh = yh −

(Aht − Ahj)¯xt,

h = 1, . . . , m,

Xt6=i,j

so that

f (u) =

=

1
2

1
2

k(Ai − Aj)ui − ρk2 + λ

|ui| + |ui − ¯xi − ¯xj| +

|¯xt|

(cid:18)

α2u2

i − ρT (Ai − Aj)ui +

kρk2 + λ

1
2

(cid:18)

Xt6=i,j
|ui| + |ui − ¯xi − ¯xj| +

(cid:19)

|¯xt|

.

(cid:19)

Xt6=i,j

For all h = 1, . . . , m, we can write

ρh = yh −

Aht ¯xt + Ahj

¯xt = yh − (A¯x)h + Ahi ¯xi + Ahj ¯xj + Ahj

¯xt,

t6=i,j
X

t6=i,j
X

= yh − (A¯x)h + (Ahi − Ahj)¯xi,

t6=i,j
X

where the last inequality follows from the fact that ¯xj = −
Therefore,

ρ = y − A¯x + (Ai − Aj)¯xi

t6=i,j ¯xt − ¯xi, since eT ¯x = 0.

P

(35)

and we obtain

f (u) =

1
2

αu2

i − ρT (Ai − Aj)ui + λ

|ui| + |ui − ¯xi − ¯xj|

+ c.

(cid:18)

(cid:19)

Finally, since (A¯x − y)T Ai = ∇iϕ(¯x) and (A¯x − y)T Aj = ∇jϕ(¯x), from (35) it follows that
ρT (Ai − Aj) = kAi − Ajk2 ¯xi − ∇iϕ(¯x) + ∇jϕ(¯x) = α¯xi − ∇iϕ(¯x) + ∇jϕ(¯x) = β, thus
proving (34).

23

A decomposition method for lasso problems with zero-sum constraint

Appendix A.1. Ensuring strict convexity

In the convergence analysis of AS-ZSL, we require f to be strictly convex along ±(ei − ej) for
every index pair (i, j) selected by Strategy MVP or Strategy AC2CD (see Subsection 4.1).
Using Proposition A.1, it is now easy to show that this requirement is satisﬁed if and only if
Ai 6= Aj.

Corollary A.2. Let ¯x be a feasible point for problem (1). For any i 6= j, we have that f is
strictly convex over the line {¯x + ξ(ei − ej)), ξ ∈ R} if and only if Ai 6= Aj.

Proof. The result follows from Proposition A.1, observing that f i,j
is strictly convex if and
¯x
only if Ai 6= Aj (from the expression of α) and that any pair of distinct points x′, x′′ over the
line {¯x + ξ(ei − ej)), ξ ∈ R} can be expressed as x′ = ¯x + ξ′(ei − ej) and x′′ = ¯x + ξ′′(ei − ej),
for some ξ′ 6= ξ′′.

When Ai = Aj, in the next proposition we show the variable xi can be safely removed

from the problem.

Proposition A.3. Assume that Ai = Aj for some i 6= j. If x∗ is an optimal solution of
min{f (x) : eT x = 0, xi = 0}, then x∗ is an optimal solution of problem (1) as well.

Proof. By contradiction, assume that x∗ is not an optimal solution of problem (1). Then,
there exists a feasible point x′ for problem (1) such that f (x′) < f (x∗). Now, let us deﬁne
x′′ as follows:

x′
h,
0,
i + x′
x′
j,

x′′
h = 


if h ∈ {1, . . . , n} \ {i, j},
if h = i,
if h = j.

Clearly, eT x′′ = eT x′ = 0. Since Ai = Aj, we have Ax′′ = Ax′ and, using the triangular
inequality, kx′′k1 ≤ kx′k1. It follows that f (x′′) ≤ f (x′) < f (x∗), contradicting the fact that
x∗ is an optimal solution of min{f (x) : eT x = 0, xi = 0}.



In conclusion, Proposition A.3 and Corollary A.2 suggest a simple procedure to ensure
that, after a ﬁnite number of iterations, f is strictly convex along ±(ei−ej) for every index pair
(i, j) selected by Strategy MVP or Strategy AC2CD. Namely, if we ﬁnd two identical columns
Ai and Aj, we can simply ﬁx xi = 0 and remove this variable from problem (1) (together with
the column Ai). We note that checking if two columns Ai and Aj are identical does not require
additional computational burden because, as explained below, kAi − Ajk must be computed
anyway (in order to calculate the coeﬃcients α and β appearing in Proposition A.1).

Appendix A.2. Computing the optimal solution

Given a feasible point ¯x of problem (1) and an index pair (i, j), with i 6= j, now we show
how to compute

ˆx = ¯x + ξ∗(ei − ej),
ξ∗ ∈ Argmin

ξ∈R

{f (¯x + ξ(ei − ej))}.

24

Andrea Cristofari

According to (15) and (17), a computation of this form is needed for the variable update both
when using Strategy MVP and when using Strategy AC2CD. Note that ˆx can be equivalently
obtained as an optimal solution of the following problem:

min f (x)
x ∈ {¯x + ξ(ei − ej), ξ ∈ R}.

(36)

So, in view of Proposition A.1, we can calculate

u∗ ∈ Argmin f i,j

¯x (u)

and set

ˆxh = 

To compute u∗, let us ﬁrst recall that, from Proposition A.1, we have


t6=j ˆxt,

P

u∗,
¯xh,
−

if h = i,
if h 6= i, j,
if h = j.

f i,j
¯x (u) =

1
2

αu2 − βu + λ(|u| + |u − ¯xi − ¯xj|) + c,

(37)

where α = kAi − Ajk2, β = α¯xi − ∇iϕ(¯x) + ∇jϕ(¯x) and c is a constant. Since f i,j
¯x is coercive,
then it has a minimizer. If α = 0 (i.e., if Ai = Aj), we explained above that we can simply
ﬁx xi = 0 and remove this variable from the problem. So, here we only focus on α > 0. In
this case, observe that f i,j
¯x

is strictly convex, has a unique minimizer u∗ and we can write

1
2 αu2 − βu + 2λu − λ(¯xi + ¯xj) + c,
1
2 αu2 − βu − 2λu + λ(¯xi + ¯xj) + c,
1
2 αu2 − βu + λ(¯xi + ¯xj) + c,
1
2 αu2 − βu − λ(¯xi + ¯xj) + c,

if u ≥ 0 and u ≥ ¯xi − ¯xj,
if u ≤ 0 and u ≤ ¯xi − ¯xj,
if u ≥ 0 and u < ¯xi − ¯xj,
if u ≤ 0 and u > ¯xi − ¯xj.

f i,j
¯x (u) =






Hence, we can ﬁrst seek a stationary point of f i,j
¯x where the function is diﬀerentiable, i.e.,
in R \ {0, ¯xi + ¯xj}. If such a stationary point exists, then it will be the desired minimizer
u∗. Otherwise, u∗ will be a point of non-diﬀerentiability, that is, either 0 or ¯xi + ¯xj. This
procedure is reported in Algorithm 2.

Note that, in addition to ∇iϕ(¯x) and ∇jϕ(¯x), in Algorithm 2 we only have to compute

kAi − Ajk2 to get α and β, with a cost of O(m) operations.

25

A decomposition method for lasso problems with zero-sum constraint

Algorithm 2 to compute the minimizer u∗ of (37)

seek a stationary point
β − 2λ
α

0 Set ¯u =
1 If ¯u > max{¯xi + ¯xj, 0}
2
3 Else

Set u∗ = ¯u and EXIT (stationary point found )

4

5

6

7

8

9

10

11

Set ¯u =

β + 2λ
α
If ¯u < min{¯xi + ¯xj, 0}

Set u∗ = ¯u and EXIT (stationary point found )

Else

Set ¯u =

β
α
If ¯u(¯u − ¯xi − ¯xj) < 0

Set u∗ = ¯u and EXIT (stationary point found )

End if

End if

12
13 End if

stationary point not found, the minimizer is a point of non-diﬀerentiability

¯x (0) ≤ f i,j
14 If f i,j
15 Else, set u∗ = ¯xi + ¯xj

¯x (¯xi + ¯xj), then set u∗ = 0

26

Andrea Cristofari

References

[1] J. Aitchison. The statistical analysis of compositional data. Journal of the Royal Sta-

tistical Society: Series B (Methodological), 44(2):139–160, 1982.

[2] J. Aitchison and J. Bacon-Shone. Log contrast models for experiments with mixtures.

Biometrika, 71(2):323–330, 1984.

[3] M. Altenbuchinger, T. Rehberg, H. Zacharias, F. St¨ammler, K. Dettmer, D. Weber,
A. Hiergeist, A. Gessner, E. Holler, P. J. Oefner, and R. Spang. Reference point insen-
sitive molecular data analysis. Bioinformatics, 33(2):219–226, 2017.

[4] M. Andretta, E. G. Birgin, and J. M. Mart´ınez. Practical active-set Euclidian trust-
region method with spectral projected gradients for bound-constrained minimization.
Optimization, 54(3):305–325, 2005.

[5] A. Beck. The 2-coordinate descent method for solving double-sided simplex constrained
minimization problems. Journal of Optimization Theory and Applications, 162(3):892–
919, 2014.

[6] E. G. Birgin and J. M. Mart´ınez. Large-scale active-set box-constrained optimization
method with spectral projected gradients. Computational Optimization and Applica-
tions, 23(1):101–125, 2002.

[7] R. H. Byrd, G. M. Chin, J. Nocedal, and F. Oztoprak. A family of second-order methods
for convex ℓ1-regularized optimization. Mathematical Programming, 159(1):435–467,
2016.

[8] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM

Transactions on Intelligent Systems and Technology (TIST), 2(3):1–27, 2011.

[9] P. L. Combettes and C. L. M¨uller. Regression models for compositional data: General
log-contrast formulations, proximal optimization, and microbiome data applications.
Statistics in Biosciences, 13(2):217–242, 2021.

[10] A. Cristofari. An almost cyclic 2-coordinate descent method for singly linearly con-
strained problems. Computational Optimization and Applications, 73(2):411–452, 2019.
[11] A. Cristofari. Active-set identiﬁcation with complexity guarantees of an almost cyclic
2-coordinate descent method with Armijo line search. SIAM Journal on Optimization,
32(2):739–764, 2022.

[12] A. Cristofari, M. De Santis, S. Lucidi, and F. Rinaldi. A Two-Stage Active-Set Algorithm
for Bound-Constrained Optimization. Journal of Optimization Theory and Applications,
172(2):369–401, 2017.

[13] A. Cristofari, M. De Santis, S. Lucidi, and F. Rinaldi. An active-set algorithmic frame-
work for non-convex optimization problems over the simplex. Computational Optimiza-
tion and Applications, 77:57–89, 2020.

[14] A. Cristofari, M. De Santis, S. Lucidi, and F. Rinaldi. Minimization over the ℓ1-ball
using an active-set non-monotone projected gradient. Computational Optimization and
Applications, 2022.

[15] A. Cristofari, G. Di Pillo, G. Liuzzi, and S. Lucidi. An augmented Lagrangian method
exploiting an active-set strategy and second-order information. Journal of Optimization
Theory and Applications, 2022.

[16] M. De Santis, S. Lucidi, and F. Rinaldi. A Fast Active Set Block Coordinate Descent

27

A decomposition method for lasso problems with zero-sum constraint

Algorithm for ℓ1-Regularized Least Squares. SIAM Journal on Optimization, 26(1):
781–809, 2016.

[17] Z. Deng, M.-C. Yue, and A. M.-C. So. An Eﬃcient Augmented Lagrangian-Based
Method for Linear Equality-Constrained Lasso.
In ICASSP 2020 - 2020 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
5760–5764. IEEE, 2020.

[18] F. Facchinei and S. Lucidi. Quadratically and superlinearly convergent algorithms for
the solution of inequality constrained minimization problems. Journal of Optimization
Theory and Applications, 85(2):265–289, 1995.

[19] F. Facchinei, J. J´udice, and J. Soares. An active set Newton algorithm for large-scale
nonlinear programs with box constraints. SIAM Journal on Optimization, 8(1):158–186,
1998.

[20] B. R. Gaines, J. Kim, and H. Zhou. Algorithms for ﬁtting the constrained lasso. Journal

of Computational and Graphical Statistics, 27(4):861–871, 2018.

[21] L. E. Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination for the lasso and
sparse supervised learning problems. Paciﬁc Journal of Optimization, 8(4):667–698,
2010.

[22] G. B. Gloor, J. M. Macklaim, V. Pawlowsky-Glahn, and J. J. Egozcue. Microbiome
datasets are compositional: and this is not optional. Frontiers in Microbiology, 8:2224,
2017.

[23] L. Grippo and M. Sciandrone. On the convergence of the block nonlinear Gauss–Seidel
method under convex constraints. Operations Research Letters, 26(3):127–136, 2000.
[24] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL

https://www.gurobi.com.

[25] W. W. Hager and D. A. Tarzanagh. A Newton-type active set method for nonlinear
optimization with polyhedral constraints. arXiv preprint arXiv:2011.01201, 2020.
[26] W. W. Hager and H. Zhang. A new active set algorithm for box constrained optimization.

SIAM Journal on Optimization, 17(2):526–557, 2006.

[27] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordi-
nate descent method for large-scale linear SVM. In Proceedings of the 25th International
Conference on Machine Learning, pages 408–415, 2008.

[28] J.-J. Jeon, Y. Kim, S. Won, and H. Choi. Primal path algorithm for compositional data

analysis. Computational Statistics & Data Analysis, 148:106958, 2020.

[29] T. Joachims. Making large-scale support vector machine learning practical.

In
B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods –
Support Vector Learning, B, pages 169–184. MIT Press, 1999.

[30] N. Keskar, J. Nocedal, F. ¨Oztoprak, and A. Waechter. A second-order method for
convex ℓ1-regularized optimization with active-set prediction. Optimization Methods
and Software, 31(3):605–621, 2016.

[31] C.-J. Lin. On the convergence of the decomposition method for support vector machines.

IEEE Transactions on Neural Networks, 12(6):1288–1298, 2001.

[32] W. Lin, P. Shi, R. Feng, and H. Li. Variable selection in regression with compositional

covariates. Biometrika, 101(4):785–797, 2014.

[33] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method for

28

Andrea Cristofari

convex diﬀerentiable minimization. Journal of Optimization Theory and Applications,
72(1):7–35, 1992.

[34] A. Mishra and C. L. M¨uller. Robust regression with compositional covariates. Compu-

tational Statistics & Data Analysis, 165:107315, 2022.

[35] L. Palagi and M. Sciandrone. On the convergence of a modiﬁed version of SVM light

algorithm. Optimization Methods and Software, 20(2-3):317–334, 2005.

[36] J. C. Platt. Sequential Minimal Optimization: A Fast Algorithm for Training Support
Vector Machines. In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in
Kernel Methods – Support Vector Learning, pages 185–208. MIT Press, 1998.

[37] T. Quinn, D. Nguyen, S. Rana, S. Gupta, and S. Venkatesh. Deepcoda: personalized
interpretability for compositional health data. In International Conference on Machine
Learning, pages 7877–7886. PMLR, 2020.

[38] K. Scheinberg. An eﬃcient implementation of an active set method for SVMs. Journal

of Machine Learning Research, 7:2237–2257, 2006.

[39] M. Schmidt, G. Fung, and R. Rosales. Fast optimization methods for l1 regularization:
In European Conference on Machine

A comparative study and two new approaches.
Learning, pages 286–297. Springer, 2007.

[40] A. L. Schwartz and E. Polak. Family of projected descent methods for optimization
problems with simple bounds. Journal of Optimization Theory and Applications, 92(1):
1–31, 1997.

[41] P. Shi, A. Zhang, and H. Li. Regression analysis for microbiome compositional data.

The Annals of Applied Statistics, 10(2):1019–1040, 2016.

[42] S. Solntsev, J. Nocedal, and R. H. Byrd. An algorithm for quadratic ℓ1-regularized
optimization with a ﬂexible active-set strategy. Optimization Methods and Software, 30
(6):1213–1237, 2015.

[43] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

[44] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. J. Tibshirani.
Strong rules for discarding predictors in lasso-type problems. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 74(2):245–266, 2012.

[45] R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. The Annals

of Statistics, 39(3):1335–1371, 2011.

[46] P. Tseng. Convergence of a block coordinate descent method for nondiﬀerentiable min-
imization. Journal of Optimization Theory and Applications, 109(3):475–494, 2001.
[47] P. Vangay, B. M. Hillmann, and D. Knights. Microbiome Learning Repo (ML Repo):
A public repository of microbiome regression and classiﬁcation tasks. Gigascience, 8(5),
2019.

[48] J. Wang, J. Zhou, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projec-

tion. Advances in Neural Information Processing Systems, 26, 2013.

[49] Z. Wen, W. Yin, D. Goldfarb, and Y. Zhang. A fast algorithm for sparse reconstruc-
tion based on shrinkage, subspace optimization, and continuation. SIAM Journal on
Scientiﬁc Computing, 32(4):1832–1857, 2010.

[50] Z. Wen, W. Yin, H. Zhang, and D. Goldfarb. On the convergence of an active-set method
for ℓ1 minimization. Optimization Methods and Software, 27(6):1127–1146, 2012.

29

A decomposition method for lasso problems with zero-sum constraint

[51] Z. Xiang, H. Xu, and P. J. Ramadge. Learning sparse representations of high dimensional
data on large scale dictionaries. Advances in Neural Information Processing Systems,
24, 2011.

[52] Z. J. Xiang and P. J. Ramadge. Fast lasso screening tests based on correlations. In 2012
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 2137–2140. IEEE, 2012.

[53] G.-X. Yuan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. A comparison of optimization
methods and software for large-scale l1-regularized linear classiﬁcation. Journal of Ma-
chine Learning Research, 11:3183–3234, 2010.

30

