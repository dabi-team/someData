2
2
0
2

n
u
J

8
1

]
E
S
.
s
c
[

1
v
6
0
2
9
0
.
6
0
2
2
:
v
i
X
r
a

Fusing Industry and Academia at GitHub (Experience
Report)

PATRICK THOMSON, GitHub, Inc., United States
ROB RIX, GitHub, Inc., Canada
NICOLAS WU, Imperial College London, United Kingdom
TOM SCHRIJVERS, KU Leuven, Belgium

42

GitHub hosts hundreds of millions of code repositories written in hundreds of different programming languages.
In addition to its hosting services, GitHub provides data and insights into code, such as vulnerability analysis
and code navigation, with which users can improve and understand their software development process.
GitHub has built Semantic, a program analysis tool capable of parsing and extracting detailed information
from source code. The development of Semantic has relied extensively on the functional programming
literature; this paper describes how connections to academic research inspired and informed the development
of an industrial-scale program analysis toolkit.

Additional Key Words and Phrases: effects, Haskell, data types, industry

1 INTRODUCTION
GitHub is a service that provides storage for repositories of source code tracked with the Git
distributed version control system. It is the largest such service in the world, supporting over 65
million users and storing petabytes of source code across hundreds of millions of repositories. The
size of the corpus of code on GitHub means that analyzing that code is a source of significant
business value. While GitHub boasts a large engineering staff, we report the experience of Semantic
Code, a team formed in 2015 to create tools that analyze the corpus of open-source and proprietary
code stored on GitHub. One of these tools is a framework called Semantic, a program analysis tool
that supports diffing, code navigation, and abstract interpretation. Semantic is implemented in the
functional programming language Haskell and is available as open-source software.1

In order to extract up-to-date data from a user’s codebase, code analysis services such as Seman-
tic must operate whenever a user uploads a code change to GitHub. This means that code analysis
must be able to handle tens of thousands of requests per minute, with thousands of simultaneous
connections, while producing useful data in a timely fashion. Such systems present significant
engineering and scaling problems. Industrial approaches to the development of such systems are
sometimes ad-hoc, but ad-hoc approaches often suffer in terms of performance and comprehen-
sibility, both of which are hard requirements at GitHub’s scale. In order to avoid these pitfalls,
the Semantic Code team heavily draws on the literature associated with functional programming
research, including algebraic and scoped effects, data types à la carte, recursion schemes, abstract
definitional interpreters and generalized LR parsing.

We have found that FP allows us to find mistakes, test our assumptions, build prototypes, and
to experiment within a given problem domain. By leveraging techniques from this literature, the
Semantic Code team both solved pressing business problems and ended up with production-tested

1https://github.com/github/semantic

Authors’ addresses: Patrick Thomson, GitHub, Inc., United States, patrickt@github.com; Rob Rix, GitHub, Inc., Canada,
robrix@github.com; Nicolas Wu, Imperial College London, United Kingdom, n.wu@imperial.ac.uk; Tom Schrijvers, KU
Leuven, Belgium, tom.schrijvers@kulueven.be.

2020. 2475-1421/2020/1-ART42 $15.00
https://doi.org/

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

 
 
 
 
 
 
42:2

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

libraries that we were able to release as open-source software. This illustrates the bidirectional
nature of exchange between industry and academia: by drawing on academic techniques, industry
can contribute to software ecosystems at large.

In this paper, we describe the history of the Semantic project over the seven years since the
establishment of the Semantic Code team and it initial prototype of the system (Section 2), the
techniques we used to scale this prototype so that it could cope with production traffic (Section 3),
and the production applications powered by Semantic (Section 4). We then discuss the specific
techniques for modelling effects that we have employed and refined (Section 5), the varying levels
of success we have had with a range of functional programming techniques (generalized LR parsing,
algebraic effects, data types à la carte, recursion schemes) and some of the lessons we have learned
along the way (Section 6). Finally, we conclude (Section 7). We hope that this experience report
provides perspective on the scale of industrial problems, illustrates the process of iterating on
solutions within a given problem space, elucidates real-world applications and connections to
academic research, and affirms that the academic community’s work is worthwhile and relevant to
the challenges faced in industrial software development.

2 THE BEGINNING
Semantic has its roots in the realm of diffing, a process of determining a representation of a
change to some source code. A diff is a representation, often textual, of a set of changes within a
given software project. The process of computing, applying, and displaying diffs is a foundational
responsibility of Git and other version control systems. However, diffs of the sort emitted by the
git-diff program are not always the most readable rendition of a given change. Many diffs can
span more than tens of thousands of lines, with large diffs sometimes numbering in the hundreds
of thousands; as they increase in length, they tend to decrease in readability. The Semantic project
began in 2015, with a prototype of a new diffing algorithm, intended to produce a more readable and
informative diff than the basic git-diff program—a semantic diff that is aware of the structural
and syntactic qualities of programming languages. Such a diff would use information derived from
syntax trees to recognise structural changes in a program, such as moving a function definition from
one file to another, rather than registering these changes as purely textual. (Note that a semantic
diff does not involve the denotational or operational semantics of a program.) The fact that diffing
is a core capability of version control systems, and of the GitHub web application itself, motivated
us to explore whether semantic diffing could provide business value for users.

2.1 Parsing
Though GitHub hosts code written in thousands of different programming languages, manpower
constraints led the Semantic Code team to target a subset of most popular programming languages
on GitHub: Python, Ruby, JavaScript, TypeScript, PHP, and Go. In order for Semantic to operate on
this diverse set of programming languages, we required a comprehensive approach to parsing and
analyzing source code. Real-world programming languages use varied techniques and algorithms for
parsing source code: the Ruby programming language uses a LALR(1) parser generated with GNU
Bison, whereas the Python language generates its own parser out of a parsing expression grammar
(PEG). The choice of parsing algorithm becomes critical when considering syntactic structures that
require capabilities beyond that of some parsing algorithms. An example of this is Python’s with
statement, which requires multiple tokens of lookahead to distinguish parenthesized expressions
from multi-line grouped expressions in its argument. CPython originally used an LL(1) parser,
supporting only one token of lookahead, which led to deficiencies in the implementation2 that were

2https://bugs.python.org/issue12782

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:3

only remedied when the parser was rewritten in PEG style. We needed a parsing toolkit that provided
a consistent approach and application programming interface (API) across several languages and
also provided sufficient expressive power to parse these languages correctly, regardless of the
capabilities of their canonical parsers.

Our chosen approach was built around the Tree-sitter parser generator [Brunsfeld 2018]. Tree-
sitter, a toolkit originally developed at GitHub to provide syntax highlighting for the Atom text
editor, uses the generalized LR algorithm (GLR), first described by Lang [1974] and first implemented
in 1984 by Tomita [1986]. The GLR algorithm can recognise any context-free grammar, including
ambiguous grammars and those requiring arbitrary token lookahead. These capabilities allow
Tree-sitter and its grammars to serve as a lingua franca for the world of programming languages:
regardless of the language under discussion, Tree-sitter is powerful enough to recognise it, and
its API is consistent across languages. Programmers use a JavaScript domain-specific language
(DSL) to express Tree-sitter grammars, which generates a dependency-free C program, compilable
to machine code or WebAssembly, that any editor or programming tool can use to yield a syntax
tree from program text. By virtue of choosing Tree-sitter to power Semantic’s parsing support,
we were confident we could extend chosen approaches to any language, as long as that language
has a Tree-sitter grammar. The success of Tree-sitter parsers in other applications and problem
domains, such as GitHub’s syntax highlighting service, the Neovim text editor, and the Radare
reverse engineering toolkit, made us confident that the parsers themselves could correctly handle
languages as syntactically complex as TypeScript and Ruby. Additionally, Tree-sitter parsers are
tolerant of syntax errors: should a source file contain invalid syntax, the error condition will be
confined only to that point in the syntax tree, and the remaining syntactically-valid code will
be present and accessible. This is a hard requirement, given that we cannot assume all code in a
repository is well-formed.

2.2 An Initial Prototype
Satisfied with our solution to the difficulties of cross-language parsing, we wrote our initial prototype
of a semantic executable in a beta version of the Swift programming language3. The Semantic
Code team had prior experience with Objective-C, Swift’s predecessor, which made it an attractive
platform given its additional type safety atop familiar Objective-C APIs. While the prototype
worked, it suffered from poor performance and poor developer experience. Though the diffing
algorithm was clearly not optimized yet, there was too much friction in writing deployment-ready
code in Swift: given its beta status Swift was evolving rapidly and its rapid language changes and
sometimes-unstable toolchains distracted from larger engineering goals. Since our code was written
in a functional style, we looked for more established languages that would preserve the functional
style without compromising performance or readability. Haskell seemed an appropriate choice,
given its decades of history, its mature, native-code Glasgow Haskell Compiler (GHC), and its
success in other industrial settings such as Meta’s Sigma anti-spam system [Marlow 2015; Marlow
et al. 2014]. Moreover, Haskell’s foreign-function interface (FFI) made it easy to link to Tree-sitter
generated parsers.

We were astonished by the speed with which we converted our Swift code to Haskell4. After
less than a day’s engineering effort, our Haskell implementation outperformed the Swift code by a
factor of two; this was doubly remarkable given that this was the team’s first experience writing
Haskell in industry. GHC’s FFI support allowed us to operate on syntax trees via the C-based

3Accessible at commit d23d646 of the Semantic repository.
4Accessible at commit 95c2850.

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:4

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

Tree-sitter API, and we generated Haskell data types suitable for diffing and analysis by passing
these syntax trees as the seed value to an anamorphism [Meijer et al. 1991].

3 SCALING THE PROTOTYPE WITH FUNCTIONAL TECHNIQUES
Buoyed by the ease of implementing this prototype with Tree-sitter and with Haskell, we turned
to the next issue facing us: how were we to take this prototype and build something capable of
handling GitHub’s stringent engineering requirements and considerable production traffic? Scaling
a prototype is not just a matter of performance: it also involves planning to keep code complexity
under control. A single programming language is complicated enough to implement and analyze;
we feared that the complexity associated with a naïve approach to implementing a multi-language
analyzer would impair the development of any interesting analysis features.

3.1 Syntactic Sharing
Both empirical studies [Haefliger et al. 2008] and the team’s anecdotal experience indicate that
code reuse is an effective method for managing complexity. The team decided to achieve a degree
of code reuse by sharing the representation of ASTs across our target programming languages.
As an example of this, many languages share syntactic features that are relatively similar, such as
simple arithmetic operations, functions, assignment statements, and comments. Manually defining
a Comment syntax type for each language would be both tedious and complicated. Other language
variants overlap more substantially, such as in the case of TypeScript, which is a superset of
JavaScript: given their degree of shared semantics, it was our goal to reuse analyses written for
TypeScript on JavaScript codebases.

Our chosen solution for syntactic sharing would let us compose a language’s syntax types out
of smaller, shared parts. We decided to use a data types à la carte [Swierstra 2008] methodology,
which meant defining ASTs as a coproduct of endofunctors, each representing the shape of a kind
of node (a string literal, a function call, etc.), tied into a recursive shape with a simple fixpoint.
Another fixpoint, defining recursive positions as either a copy, insertion, deletion, or replacement
of ASTs, gave us a representation for diffs. All syntax types were functors parameterized in terms
of an additional data type representing a node’s annotation; this polymorphism allowed us to track
source locations and to annotate, in the case of diffing, whether a node was added or removed.
Syntax errors in the tree were represented with a shared error type.

import Data.Sum (Sum)
import Control.Comonad.Cofree (Cofree)
import qualified Syntax
import qualified Syntax.Literal as Literal

type JSONSyntax =
[ Literal.Null,
Literal.List,
Literal.Boolean,
Literal.Hash,
Literal.Decimal,
Literal.KeyValue,
Literal.TextElement,
Syntax.Error -- for ill-formed nodes

]

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:5

17

type JSONTerm = Cofree (Sum JSONSyntax)

A representation of JSON syntax written with data types à la carte.

An advantage of representing syntax trees as fixed points of coproducts of endofunctors is that
such a representation is compatible with recursion schemes [Meijer et al. 1991]. The team employed
recursion schemes whenever the shape of data permitted it, as in our experience operations
written with recursion schemes are often more flexible, readable, and type-safe than those written
with explicit recursion. We used catamorphisms and anamorphisms to implement operations that
generated, manipulated, and serialized syntax trees, and used paramorphisms for operations such
as term rewriting and dead code elimination that required additional context.

Every engineering decision comes with trade-offs, and choosing data types à la carte provided
us with compositionality and fluent recursion but sacrificed a degree of type safety: because these
types are functorial with respect to their children, we cannot constrain the types of the children
without giving up functorial map. Another downside of this representation was that it required a
preprocessing stage, dubbed assignment, an unparser that converted a Tree-sitter syntax tree to a
sum-of-products Haskell type, based on an anamorphism implemented once per desired language.
This code resembled a construct dual to the Tree-sitter specification of the parser, but had to be
written manually. Despite these drawbacks, we grew comfortable with the use of coproducts of
functors to represent ASTs, turning then to improving the performance of the program itself.

3.2 Algorithmic Improvement
Our algorithm was initially simple, essentially treating a given syntax node as a tuple, a list,
or a dictionary. We diffed tuples—nodes with a fixed set of children—in 𝑂 (𝑛) time by diffing
corresponding members. Lists represented nodes with arbitrarily many children, and were diffed
with an approach based on the classic shortest edit script (SES) problem [Myers 1986], initially using
a naïve, compare-everything-to-everything-else algorithm running in 𝑂 (𝑛2) time. Dictionaries,
mapping keys to values, were diffed via set reconciliation of the keys. All of this was wrapped
up in a small DSL implemented using a free monad, giving us a high-level vocabulary capable of
understanding and executing diff scripts generated automatically or by hand, supporting all of our
target languages. Breaking syntax down into sums of small syntax functors eventually allowed
us to define what we called sub-structural diffing, where a piece of syntax can nominate some
sub-syntax as mediating its identity; this allowed us to diff functions by comparing their identifiers.
In 2016, performance concerns and a desire to eventually detect code moves and renames, and
other non-minimal features of readable diffs, led us to borrow parts of the RWS-Diff algorithm [Finis
et al. 2013], which applies techniques from computer vision to the problem of comparing trees.
While we found this suboptimal due mainly to its use of a pseudorandom number generator and its
consequent unpredictability, it did improve our algorithm’s efficiency, particularly when combined
with other approaches. For example, we decided to improve the naive 𝑂 (𝑛2) diffing algorithm that
we had written originally for variable-arity branches, and eventually as a preflight pass applied
before RWS. The diffing system was improved by implementing an 𝑂 (𝑛𝑑) time algorithm by Myers
[1986] where 𝑑 is the size of the difference, which was extremely efficient, particularly in the
expected case that more has remained the same than has changed. Fixed-arity branches continued
using sequential diffing, running in linear time.

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:6

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

4 PRODUCTION APPLICATIONS
Having achieved acceptable performance, our next course of action was to replace the diffs shown
on the github.com website, particularly on pull request views, with syntactic diffs emitted from Se-
mantic. This section explains why we had to abandon this approach for non-technical reasons, and
how our goal shifted instead to applications related to source code navigation and comprehension.

4.1 Production Showstoppers for Semantic’s Diffs
We had come to a point where Semantic was able to output its diffs in a format compatible with
git. We could therefore integrate these diffs into the github.com interface as a drop-in replacement
for files in supported languages. However, non-technical factors led us to abandon this course of
action: specifically, the fact that diffs on GitHub would differ from those generated by the standard
git-diff program was deemed an insurmountable barrier to widespread adoption. Although Git
itself can be configured to use syntactic diffs emitted from a semantic binary, it would require
users to download, install, and understand an extra tool, one less battle-tested than git-diff itself.
We then turned to another feature. With diffs represented as syntax trees where some nodes
have been replaced by patches, we had all the information we needed to compute summaries
of the patches occurring within a diff in a high-level, readable description of what had changed.
Unfortunately, while collecting the changes was a matter of a trivial fold, producing a high-level,
intelligible description from these proved to be much harder, especially given that these were
changes to source code which itself is difficult to summarize. It turns out to be quite a challenge to
do better than just presenting the changes verbatim, at least for a user base largely consisting of
experienced programmers. We arrived at no formulation that substantially improved on the act of
reading a textual diff.

4.2 Table-of-Contents Analysis
Our subsequent engineering efforts focused on
a table of contents feature. The table of con-
tents associated with a given diff provides a
summary of the files and functions changed in
that diff. This feature was shipped to the pub-
lic [Rix 2017]. A remote-procedure call (RPC)
server called semiotic, written in Go, listened
for requests for a table of contents, executed
the semantic binary out-of-band to analyze
that diff, and returned the table of contents
data over the wire, encoded in Google Protocol
Buffers format [Google 2008]. This information
was then decoded by the Ruby on Rails appli-
cation that powers github.com and rendered
appropriately in its interface (Figure 1). Though
this feature performed well and provided users
with useful information, it did not see wide user
adoption, possibly due to its limited space in
the pull request UI. GitHub now has a side-bar
view, inspired by the table of contents feature,
that is displayed on all pull requests.

Fig. 1. The GitHub table of contents feature as ren-
dered to users.

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:7

The fact that this service yielded useful customer data at GitHub levels of scale considerably
increased the project’s momentum. One of the Semantic Code team’s central goals is to build solu-
tions that require zero configuration from users: GitHub already provides a powerful analysis tool
in CodeQL, and supports custom analysis tools in continuous-integration pipelines implemented
with GitHub Actions, but these solutions require manual configuration for each repository in order
to integrate with users’ build processes. At this point, our goal shifted from diffs and diff analysis
to the navigation and comprehension of source code itself, without requiring the repository owner
to perform any work to yield its benefits.

4.3 Code Navigation
Our first effort after the table of contents feature was to improve an internal experimental prototype
code navigation system. The archetypal code navigation feature, common to many text editors
and IDEs, is jump to definition. This allows a user, when faced with an unfamiliar function or
variable, to travel immediately to its definition. Dual to this feature is find all references, which,
given the definition of a function or variable, locates all parts of the program that reference or
invoke that entity. Given that GitHub users often use the github.com web interface to browse code,
we anticipated that code navigation as a GitHub feature would be profoundly helpful to those
trying to navigate and understand large or unfamiliar codebases.

The prototype code navigation system was used internally by GitHub staff, and proved a helpful
tool in the maintenance of the large and complicated Ruby on Rails codebase that underlies GitHub
itself. While this prototype worked well enough for internal purposes, there were concerns that
it would not scale to a production system visible to all GitHub users. The original system was
written in Ruby and extracted data from source code by invoking and parsing the output of the
external ctags(1) Unix utility. We wanted to bring reliability, performance, and breadth of utility
to this service, without the user having to configure any aspect of the system, and decided to
build a prototype of this feature atop our Haskell codebase. We considered an approach built on
the code navigation capabilities of external language tooling, over the Language Server Protocol
(LSP) standard, but discarded it due to the operational difficulty associated with deploying a
separate software stack for each targeted language, as well as the impedance mismatch between
LSP capabilities and our tree-oriented view of the world.

Though sharing the syntax tree provided a
high degree of reusability when implementing
a ctags analysis for several different program-
ming languages, the complexity of these lan-
guages reared its head again. An example of
this is Ruby’s support for allowing function
calls that omit parentheses in parameter lists,
similar syntactically to invocation of Unix shell
commands. This syntactic quirk means that it
is not possible to distinguish syntactically be-
tween a zero-argument function call and a ref-
erence to a variable. Analyses must keep tables of local variable declarations and use these tables
to disambiguate them from zero-argument function calls.

Fig. 2. The GitHub jump to definition user interface.

Implementing code navigation was a matter of defining a type class that emitted tag information
and implementing this class for every relevant node. This was made available through semanticd,
an HTTP-based RPC service, written with the Servant web API library [Servant 2014] and linking
in Semantic as a library, that wrote tag information to a MySQL database upon pushes to GitHub
repositories and fetched that information based on user queries. The resulting system performed

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:8

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

admirably when deployed to production, handling tens of thousands of requests per minute,
deployed and scaled via the Kubernetes [Google 2014] container management system. semanticd
proved reliable at GitHub scale: the primary source of crashes was a vendor-specific hardware bug
that the GHC maintainers promptly remedied.5

4.4 Semantic Analysis
With the infrastructure for simple code analysis such as diffing and table of contents in place, the
next goal of the project was to perform more sophisticated code analysis on repositories to drive
features for end users. The goal was to have multiple forms of analysis over multiple different
languages. Without a principled approach to modularity, this problem would easily become an
engineering nightmare: if supporting 𝑚 features for 𝑛 languages requires 𝑚 × 𝑛 amount of work,
it becomes increasingly untenable to support new features on languages with a small team, and
completely unreasonable to expect the same from third-party contributors. Focusing our attention
on the top five most-used languages is already a problem, as general-purpose languages are often
quite large, and language size is an important factor in the effort required.

An optimal solution would reduce this to an 𝑚 + 𝑛 problem where each new feature or program-
ming language need only be implemented once. Now the effort required by the team is much more
tractable, and third-party contributors can more reasonably add support for new languages.

Further, since both our team and teams likely to be clients of these services lacked prior experience
with program analysis, our solution needed to be approachable; we expected to serve a variety of
different end-user–facing products, and so we needed it to scale to multiple analyses; and we knew
we would need to be able to experiment to tune both performance and precision of the results to
suit the constraints of each use case.

Finally, while we could already serve some needs by means of syntactic analyses, typically
implemented as folds over syntax trees and diffs, properties which depended on any of a program’s
dynamics, for example computing the set of exception types that can be thrown through a given call
site, or collecting the set of instructions unreachable from a given set of inputs, remained outside
of our reach. Therefore, we knew we needed a truly semantic analysis, justifying for the first time
our toolkit and team’s heretofore aspirational names.

Serendipitously, it was at about this time that the team recognised that work by Darais et al.
[2017] on Abstracting Definitional Interpreters (ADI) was relevant to these problems. All ADI
requires of us is an evaluator written in an open-recursive style against a set of capabilities based
on the work of Van Horn and Might [2010]. Implementing an evaluator for an entire language is a
non-trivial task, but allows us to support the vast majority of analyses, and hence end-user features,
without additional effort, reducing our 𝑚 × 𝑛 problem to 𝑚 + 𝑛 at a stroke. Furthermore, motivated
third parties such as communities of language users can contribute to an evaluator, just like they
already do with parsers. This reduces the burden on our team, while often also allowing bugfixes
to be provided with a faster turnaround than could otherwise be the case.

Further, ADI gives us a large number of levers to control the performance and precision of
analyses. Conveniently, these are the two metrics by which we tune an analysis. For example,
the monadic nature of the evaluator’s targeted interface allows semantics to be altered simply
by switching the ordering of the handlers of the various components of the abstract machine.
(Note that in the paper, these are implemented via monad transformers, while our implementation
employs algebraic and scoped effects [Wu et al. 2014].)

To some degree, this began as a solution in search of a (specific) problem: it was clear that we
would need this suite of capabilities for future developer productivity features, but it wasn’t at all

5https://gitlab.haskell.org/ghc/ghc/-/issues/18033

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:9

obvious what they should be. At one time, we planned on implementing code navigation using
abstract interpretation, but in the end that implementation was pursued in a Rust service using
a notion of stack graphs [Creager 2021] inspired by scope graphs [van Antwerpen et al. 2018],
which allowed us to express scope-aware code navigation without the time and effort associated
with developing full abstract interpretation for all targeted languages. This finer-grained solution
proved more expedient for us given our time constraints, and by implementing it as an addition
to tree-sitter allowed us to avoid the overhead of serializing between Tree-sitter and Semantic.
However, we continue to explore definitional interpreters for analyses more sophisticated than
those required for code navigation.

5 EVOLVING APPROACHES
Semantic’s successful production deployment confirmed our belief that academic research was an
effective source of techniques and approaches. However, working in a problem space as complex
and diverse as analysis of real-world programming languages presented confounding factors to
which we had to adjust. Some of these factors stemmed from runtime performance issues, some
from compile-time performance, and some from our desire to manage complexity more fluently.

5.1 Effective Haskell
As our grasp of syntax became more precise, our need to express the capabilities of operations on
these types grew: we wanted to delineate what operations could and could not happen where. We
found that the complexity associated with rigorously expressing capabilities with the Haskell type
system was justified, given the clarity that these types provided and the errors that they prevented.
We found that one of the appeals of Haskell in an industrial setting is that it is clear in showing
what you can and cannot do.

Haskell programmers most commonly use a finally-tagless approach [Carette et al. 2007] when
expressing the capabilities of code in the type system using the monad transformer library Mtl,
to express computational effects [Jones 1995]. This approach suffers in that it requires a separate
monad transformer for each effect’s interpretation, and each monad transformer must implement
an instance per effect, which became infeasibly complex given that our most complicated analyses
sometimes involved invocations dozens of effects deep. We reached instead for algebraic effects
[Plotkin and Power 2001], a family of techniques that, instead of using type class instances in the
manner of Mtl, represent effects with data constructors and handlers that interpret invocations
of these constructors. We forked and modified an existing effect system developed by Dev and
King [2016]. Our successful experience with the Kiselyov and Ishii formulation of algebraic effects
established it as a foundational tool for subsequent work in research and in production systems.
We saw an early example of the utility of algebraic effects during the development of our diffing
algorithm. During the debugging process, we needed to dump the state of the 𝑂 (𝑛𝑑) algorithm
during its operation. In order to do this, we implemented effect handlers that both performed
the diffing algorithm and emitted its intermediate states as SVG (Scalable Vector Graphics) files.
Effect handlers made it straightforward to implement the algorithm and allowed us to customize
its interpretation when we needed it to emit additional data.

5.2 Effects and Interpretations
Several issues led us to conclude that we needed an effect system which could represent not only
algebraic effects, but also effects which themselves contain effectful actions.

Some effectful actions take effectful computations as parameters. One example is the Reader
effect, which is equipped with two primitive actions: ask returns the “environment” value and
local 𝑓 𝑝 modifies the environment value, using the function 𝑓 , but this modification is local to

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:10

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

the computation 𝑝. The standard algebraic effects approach implements ask as a constructor and
local as an interpreter.

As pointed out by Wu et al. [2014], when combined with other effects, implementing local
and similar operations as interpreters leads to surprising, and often undesired behavior. We ran
into this hard-to-debug pitfall on multiple occasions. This convinced us of the need for an effect
system capable of modelling not just algebraic effects, but also non-algebraic scoped effects—effects
which delimit the scope within which some behaviour—locally shadowing a variable, catching an
exception, forking a computation using cooperative multitasking, etc.—will be applied.

Though Mtl is the de facto effect system for developing Haskell applications, it was unsuitable
for implementing the system described by Darais et al. [2017], as the implementation relies on the
ability to have multiple state types, with multiple interpretations, in a given monadic computation.
Mtl prohibits this approach due to the fact that its effect type classes, to aid in type inference,
prohibit multiple instances of these classes for a given monad transformer. The recommended
solution for this is to create more monad classes, and to implement these classes for all monad
transformers, which is the 𝑂 (𝑛2)-instances problem discussed previously. Despite this drawback,
Mtl performs very well, orders of magnitude better than a freer-monad approach, due to the fact
that the GHC optimizer is eager to inline type class methods [Peyton-Jones and Marlow 2002],
avoiding the overhead of constructing type class dictionaries. However, GHC is generally reluctant
to inline recursive code, and the act of handling algebraic effects is inherently recursive, given
that one handler might need to delegate to another. Our solution needed to introduce as few
performance changes as possible, both out of performance requirements and ability to implement
effects such as telemetry-based performance monitoring: adding effects needed to avoid altering
the performance characteristics of the code under examination. As such, we had to recover a degree
of the inlining characteristics of Mtl.

5.3 Fused-Effects
The aforementioned deficiencies in our freer-monad effect system caused us to look at alternatives.
We looked, again, to the literature to help us overcome these problem. Work by Wu et al. [2014]
provided a formulation of effect handlers capable of handling scoped operators. Wu and Schrijvers
[2015] demonstrated that GHC’s reluctance to inline recursive effect handlers is remedied by
expressing these handlers as type class methods. Additionally, work by Schrijvers et al. [2019]
allowed us, by limiting ourselves to modular effects, to remove occurrences of the freer monad
entirely, instead using the well-attested and GHC-friendly monad transformer approach. We
implemented Fused-Effects, an effect system based on these techniques, and immediately observed
considerable speedups in analysis benchmarks. The Fused-Effects support for scoped effects with
multiple interpretations rendered it trivial to implement effects such as telemetry. We have released
Fused-Effects as open-source software6, as well as associated packages providing integration
with common libraries such as haskeline and advanced approaches to data manipulation such as
profunctor optics [Pickering et al. 2017]. With more than 10,000 downloads to date, Fused-Effects
has seen substantial community adoption and industrial use outside GitHub.

6 LESSONS LEARNED
We consider functional programming to have been a clear win for Semantic. In practice, some
techniques applied better than others; nevertheless, we learned a great deal in the process of
applying and evaluating a variety of approaches.

6https://github.com/fused-effects/fused-effects

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:11

6.1 GLR Parsing
Tree-sitter and the generalized LR algorithm it provides have proven rock-solid at industrial scale
and sufficiently expressive to parse even languages, such as Ruby and TypeScript, that display
tremendous syntactic complexity. Tree-sitter’s JavaScript-based DSL is sufficiently expressive to
cover most aspects of PL syntax, and for languages with complex lexing rules, such as Ruby’s
support for nested heredoc strings, Tree-sitter allows hand-written lexers in C or C++. On the
occasions we had to extend Tree-sitter’s capabilities, such as to emit information about a grammar’s
syntax types for code generation (see Section 6.4), doing so proved straightforward. The Tree-sitter
ecosystem continues to expand, thanks to its significant community adoption, and we anticipate that
it will continue to serve as a fundamental building block for the Semantic Code team’s engineering
efforts. The fact that language communities themselves can maintain their language’s Tree-sitter
grammar allows GitHub to provide useful code intelligence features, without having to commit
GitHub engineering time to language support itself. Given the plethora of programming languages
hosted on GitHub, this is an elegant solution to manpower and time constraints, one that also
benefits language communities.

6.2 Algebraic Effects
From the project’s inception, our use of algebraic effects was pervasive and profound. We conjecture
that Semantic is among the largest Haskell projects developed without any direct dependency on
Mtl. We found algebraic effects an elegant, effective, and expressive way of writing effectful code
in Haskell.

The monadic computations described in Darais et al. [2017] require multiple State and Reader
types in their transformer stacks, and the paper’s artifacts accomplish this by defining a bespoke
monad transformer library, alongside macros to ease the composition of monad stacks. Expressing
these computations using Mtl would be tedious, as every duplicated State or Reader effect
would need to be wrapped in its own finally-tagless monadic interface, alongside a concrete
transformer type that would require instances for all associated monad transformer classes. This
is the classic 𝑂 (𝑛2) instances problem at work. Because Fused-Effects’s effect invocations are
more polymorphic than those provided by the Mtl interface, this posed no difficulty, allowing us
to port the aforementioned interpreters from the Racket implementation in Darais et al. [2017] to
Haskell with a minimum of fuss and effort. Though Mtl yields better type inference thanks to the
functional dependencies of its effect classes, GHC’s support for visible type applications [Eisenberg
et al. 2016] allowed us a pleasing syntax for disambiguating any ambiguous invocations.

An example of the utility and flexibility of algebraic effects is an effect we developed to extract
telemetry data from our production Haskell systems. A hard requirement for production systems is
that they emit data about the state of the system, in order for their maintainers to have insight
into the behavior of a program during the operating and debugging process. Examples of such
include log messages and associated key-value data; remote metrics such as counters, timers, and
statistical distributions; and execution traces that describe the control flow paths taken by a given
request or action. These data are sent to internal services that store, aggregate, categorize, and
display them in a manner that aids comprehension. Algebraic effects provided us a rich vocabulary
for aggregating and collecting these data, as well as the ability for this aggregation and collection
to operate differently in different contexts.

Timing the execution of a given code block is an example of a scoped effect: the yielded telemetry
data are limited to the code around which the timing invocation is wrapped. However, sometimes
we wanted different behavior, especially during local development: data aggregators are intended
for production, rather than development, but the statistics generated are still useful and relevant

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:12

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

during the development cycle. In a production context, we wanted these data to be uploaded to
an aggregator; in a development context we wanted to see them reported on the command-line;
and when running automated tests, we wanted to discard them entirely. Furthermore, our solution
needed to introduce as few performance changes as possible, lest the act of measuring some code’s
execution time alter the performance characteristics of the code under examination. With Fused-
Effects, it was trivial to define a Telemetry effect and associated interpreters to handle the three
above cases, thanks to Fused-Effects’s support for reinterpreting scoped effects. A simpler effect
system that did not support such reinterpretation would not have sufficed.

6.3 Data Types à la Carte
Our initial experience with data types à la carte was positive. Expressing a language’s syntax with
a type-level list (see Section 3.1), thanks to GHC’s support for type-level programming, and then
parameterizing the associated type with that list, provided an elegant and uniform interface to
querying and analyzing syntax trees. The fact that we shared common syntax nodes, like Comment
and Integer types, between languages allowed us to generalize functions and analyses across said
languages. Performance problems associated with a recursively defined notion of subsumption7 led
to us developing a library called Fastsum, released as open-source software8, that unrolled this
recursive loop within the Haskell type system, which bypassed performance problems associated
with large languages such as TypeScript, which contains hundreds of distinct syntax nodes.

The utility of the data types à la carte approach was ultimately limited due to the variance in
semantics between languages with what superficially appeared to be the same kinds of syntax.
For example, object-oriented programming languages that support implementation inheritance
provide a construct that instructs the language to invoke a superclass method. In Python, Java,
and Ruby, this construct is written super(), suggesting that they should be modeled by a single
syntax type. However, Ruby provides an unusual variant of super(), known as “zsuper”, short
for “zero-argument super” and written without trailing parentheses. When invoked in a method
taking one or more arguments, zsuper implicitly forwards all arguments in scope to its superclass’s
implementation. We defined a Ruby-specific zsuper syntax functor, and made similar compromises
when dealing with other quirks of real-world programming languages: ultimately, this problem
domain is sufficiently complicated to, at times, actively resist abstraction.

Additionally, an à la carte approach is ultimately unityped: even though syntax trees generally
permit type invariants on their subtrees, our approach discarded this information. Though this
made it easy to represent nodes that can appear anywhere, such as comments or parenthesized
expressions, it complicated tree rewrites, as we had to anticipate the possibility of such occurrences
at any given position in the syntax tree. For example, an operation that requires portions of a syntax
tree may be valid in a case without comments present in the syntax tree, but if comments can appear
at any point in a syntax tree, that operation must account for the presence of a comment as well as
preserve its position in the syntax tree. Correctly accounting for edge cases such as these involved
many runtime type checks, which obviated many of the gains from choosing a strongly typed
implementation language. Furthermore, the assignment stage, the language-specific anamorphism
mapping a Tree-sitter tree to a coproduct of functors, proved a significant maintenance burden:
any update in a language grammar required modifications to the assignment stage. Assignment
code had to be written manually, held us back from upgrading language grammars regularly, was
difficult to debug, and proved to be a reliable source of bugs.

7https://gitlab.haskell.org/ghc/ghc/-/issues/8095
8https://github.com/patrickt/fastsum

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

Fusing Industry and Academia at GitHub (Experience Report)

42:13

6.4 From à la Carte to Code Generation
The team ultimately chose to move away from an à la carte representation of syntax trees, electing
instead to generate Haskell data types directly from type information emitted by the Tree-sitter
parser generator. This approach, described and implemented by Nadeem [2020], proved superior
in several aspects: it retained the type information that the à la carte representation lacked, the
generated types required no maintenance by hand, and it eliminated the need to implement an
assignment stage altogether! Because syntax trees yielded from Tree-sitter may not be well-formed,
the fields of syntax types are wrapped in an Err functor isomorphic to Maybe.

-- Terminal nodes and their textual contents
data True a = True {ann :: a, text :: Text}
data False a = True {ann :: a, text :: Text}
data Number a = Number {ann :: a, text :: Text}
data Null a = Number {ann :: a, text :: Text}
data String a = Number {ann :: a, text :: Text}

-- Nodes with children
data Array a = Array {ann :: a, children :: [Err (Value a)]}
data Pair a = Pair {ann :: a,

key :: Err ((Number :+: String) a),
value :: Err (Value a)}
data Object a = Object { ann :: a, children :: [Err (Pair a)]}

-- Toplevel choice node representing JSON values
newtype Value a = Value

((True :+: False :+: Number :+: Null :+: String :+: Array :+: Object) a)

A simplified representation of the syntax types generated from the Tree-sitter grammar for JSON.

Ultimately, the utility of reusing grammar descriptions outweighed the utility of reusing hand-
maintained syntax types. Though a code-generation approach creates more syntax data types than
does an à la carte representation, the elision of the complicated and error-prone assignment stage
made this a sensible tradeoff in light of our engineering requirements. Despite à la carte syntax
proving ultimately unsuitable, it showed that code reuse is more effective when it integrates with
high-level data descriptions, such as the JavaScript DSL that specifies a grammar description.

6.5 Recursion Schemes
Recursive operations are a fundamental building block of idiomatic Haskell, and as such a toolkit
providing a generalized vocabulary for many different types of recursion proved helpful for a
wide array of tasks. The Haskell recursion-schemes library provides a sophisticated vocabulary
for catamorphisms, anamorphisms, paramorphisms, and histomorphisms, alongside an API that
integrates well both with standard Haskell data structures and with our hand-written syntax
types, which, being functorial with respect to their subterms, were already compatible with the
standard fixpoint encoding of codata in Haskell. An example of recursion schemes’ applicability
came during the prototyping of a DSL for term rewriting; expressing the DSL’s interpreter with a
paramorphism elided all explicit recursion and generalized the DSL to any data type compatible
with recursion-schemes. Even the more exotic recursion schemes proved themselves useful, such
as histomorphisms during our experiments with diff summaries and hylomorphisms as part of our
implementation of RWS-Diff.

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:14

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

The problems with recursion schemes emerged most notably when transitioning to the generated
syntax types described in Section 6.4. These generated types are functorial in terms of their
annotation type rather than their subterms, and as such the traditional encoding of recursion
schemes via a fixed point of functors does not apply. There exist approaches that generalize the data
types à la carte to mutually-recursive, well-typed trees [Bahr and Hvitved 2011], but the incidental
complexity is high, and the sheer size of syntax trees like those of TypeScript precluded traditional
encodings of sums and subsumptions (hence our development of Fastsum). Defining a higher-order
equivalent of the traditional Traversable type class, as well as the required code to derive these
instances generically [Magalhães et al. 2010] recovered a degree of expressivity when recursing
into subterms, though at the cost of flexibility when compared to the à la carte formulation.

7 EPILOGUE
This section summarizes Semantic’s current situation and future plans, and then concludes.

Current Situation. While the Haskell implementation of code navigation was a success, it was
later replaced by a domain-specific language for querying syntax trees. This decision did not
stem from deficiencies in Semantic itself: rather, our manpower constraints precluded the use of
heavy-duty language stacks. Due to the considerable number of programming langauges in use, we
needed to enable external contributors to maintain their own code navigation rules without having
to write Haskell code. Haskell’s considerable ecosystem, large compiler, and learning curve made it
an unsuitable choice for language maintainers unfamiliar with Haskell. (The same is true of Rust,
Tree-sitter’s implementation language). Our goal was to eliminate all possible barriers to entry.
Because the query language we developed is useful in other contexts, we implemented it directly in
Tree-sitter, adding to its existing suite of tools. Our production systems now invoke the Tree-sitter
code directly, rather than being mediated by Semantic. The lowered barrier to entry afforded by the
use of the domain-specific query language allowed the Elixir programming language community
to contribute and maintain their own rules for code navigation, and we anticipate future external
contributions from language communities.

Future Plans. Semantic continues development at GitHub, where we are using it to explore
future product features that require high-level program analysis and abstract interpretation. We
have defined compatibility interfaces to bridge the output of Tree-sitter DSL operations to perform
analyses that would be inconvenient to express in the DSL or without the Haskell ecosystem. We
are now experimenting with abstract interpretation to track exceptions.

Further work remains to generalize the hardcoded Err functor (see Section 6.4) wrapping child
nodes (to handle the fact that Tree-sitter parsers’ error-tolerance means that some subterms may
be syntactically invalid) to a type parameter, rendering them types of kind (★ → ★) → ★. This
approach, thanks to the flexibility associated with customizing the shape of contained data, can
recover the convenience and flexibility of the unityped version without compromising type safety.
For example, though generated syntax nodes have no fields representing comments, parameterizing
these nodes with a functor containing comment text will allow us to associate them with comments.
Using a three-valued functor (usually known as These) allows us to recover diffing capabilities by
associating nodes with additions, subtractions, or unchanged sections in a given diff.

Conclusion. Even though Semantic is no longer part of the code navigation pipeline, we consider
it a successful application of FP techniques, which allowed us to iterate quickly on a solution, draw
from well-researched avenues of thought, and express complex thoughts concisely, all the while
retooling our approach in the face of evolving business requirements and use cases. We are pleased

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

Fusing Industry and Academia at GitHub (Experience Report)

42:15

both with the utility of Semantic as a tool at GitHub’s disposal and as a source of open-source
software and community interest.

ACKNOWLEDGEMENTS
The authors would like to thank all the contributors to Semantic over its lifetime, including Ayman
Nadeem, Timothy Clem, Rick Winfrey, Josh Vera, Max Brunsfeld, Doug Creager, and Charlie
Somerville. Many thanks also to the anonymous ICFP 2022 reviewers for their helpful feedback.

REFERENCES
Patrick Bahr and Tom Hvitved. 2011. Compositional Data Types. In Proceedings of the Seventh ACM SIGPLAN Workshop
on Generic Programming (Tokyo, Japan) (WGP ’11). Association for Computing Machinery, New York, NY, USA, 83–94.
https://doi.org/10.1145/2036918.2036930

Max Brunsfeld. 2018. tree-sitter/tree-sitter: v0.20.4. https://doi.org/10.5281/zenodo.5894991
Jacques Carette, Oleg Kiselyov, and Chung-chieh Shan. 2007. Finally Tagless, Partially Evaluated. In Programming Languages
and Systems, 5th Asian Symposium, APLAS 2007, Singapore, November 29-December 1, 2007, Proceedings (Lecture Notes in
Computer Science, Vol. 4807), Zhong Shao (Ed.). Springer, 222–238. https://doi.org/10.1007/978-3-540-76637-7_15
Douglas Creager. 2021. Introducing stack graphs | The GitHub Blog. https://github.blog/2021-12-09-introducing-stack-

graphs/

David Darais, Nicholas Labich, Phúc C. Nguyen, and David Van Horn. 2017. Abstracting Definitional Interpreters (Functional

Pearl). Proc. ACM Program. Lang. 1, ICFP, Article 12 (aug 2017), 25 pages. https://doi.org/10.1145/3110256

Allele Dev and Alexis King. 2016. freer-simple. https://github.com/lexi-lambda/freer-simple
Richard A. Eisenberg, Stephanie Weirich, and Hamidhasan G. Ahmed. 2016. Visible Type Application. In Programming

Languages and Systems, Peter Thiemann (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 229–254.

Jan P. Finis, Martin Raiber, Nikolaus Augsten, Robert Brunel, Alfons Kemper, and Franz Färber. 2013. RWS-Diff: Flexible and
Efficient Change Detection in Hierarchical Data. In Proceedings of the 22nd ACM International Conference on Information
and Knowledge Management (San Francisco, California, USA) (CIKM ’13). Association for Computing Machinery, New
York, NY, USA, 339–348. https://doi.org/10.1145/2505515.2505763

Google. 2008. Protocol Buffers. https://developers.google.com/protocol-buffers
Google. 2014. Kubernetes. https://kubernetes.io
Stefan Haefliger, Georg von Krogh, and Sebastian Spaeth. 2008. Code Reuse in Open Source Software. Management Science

54, 1 (2008), 180–193. https://doi.org/10.1287/mnsc.1070.0748 arXiv:https://doi.org/10.1287/mnsc.1070.0748

Mark P. Jones. 1995. Functional programming with overloading and higher-order polymorphism. In Advanced Functional

Programming, Johan Jeuring and Erik Meijer (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 97–136.

Bernard Lang. 1974. Deterministic Techniques for Efficient Non-Deterministic Parsers. In Automata, Languages and

Programming, Jacques Loeckx (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 255–269.

José Magalhães, Atze Dijkstra, Johan Jeuring, and Andres Löh. 2010. A Generic Deriving Mechanism for Haskell. Proceedings
of the ACM SIGPLAN International Conference on Functional Programming, ICFP 45, 37–48. https://doi.org/10.1145/
1863523.1863529

Simon Marlow. 2015. Fighting spam with Haskell. https://engineering.fb.com/2015/06/26/security/fighting-spam-with-

haskell/

Simon Marlow, Louis Brandy, Jonathan Coens, and Jon Purdy. 2014. There is No Fork: An Abstraction for Efficient,
Concurrent, and Concise Data Access. SIGPLAN Not. 49, 9 (aug 2014), 325–337. https://doi.org/10.1145/2692915.2628144
Erik Meijer, Maarten Fokkinga, and Ross Paterson. 1991. Functional programming with bananas, lenses, envelopes and
barbed wire. In Functional Programming Languages and Computer Architecture, John Hughes (Ed.). Springer Berlin
Heidelberg, Berlin, Heidelberg, 124–144.

Eugene W. Myers. 1986. An O(ND) Difference Algorithm and Its Variations. Algorithmica 1 (1986), 251–266.
Ayman Nadeem. 2020. CodeGen: Semantic’s improved language support system | The GitHub Blog. https://github.blog/2020-

08-04-codegen-semantics-improved-language-support-system/

Simon Peyton-Jones and Simon Marlow. 2002. Secrets of the Glasgow Haskell Compiler Inliner. J. Funct. Program. 12, 5 (jul

2002), 393–434. https://doi.org/10.1017/S0956796802004331

Matthew Pickering, Jeremy Gibbons, and Nicolas Wu. 2017. Profunctor Optics: Modular Data Accessors. The Art, Science,

and Engineering of Programming 1 (03 2017). https://doi.org/10.22152/programming-journal.org/2017/1/7

Gordon Plotkin and John Power. 2001. Semantics for Algebraic Operations. Electronic Notes in Theoretical Computer Science

45 (07 2001). https://doi.org/10.1016/S1571-0661(04)80970-8

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

42:16

Patrick Thomson, Rob Rix, Nicolas Wu, and Tom Schrijvers

Rob Rix. 2017. Quickly review changed methods and functions in your pull requests | The GitHub Blog. https://github.

blog/2017-07-26-quickly-review-changed-methods-and-functions-in-your-pull-requests/

Tom Schrijvers, Maciej Piróg, Nicolas Wu, and Mauro Jaskelioff. 2019. Monad transformers and modular algebraic effects:

what binds them together. 98–113. https://doi.org/10.1145/3331545.3342595

Servant. 2014. https://github.com/haskell-servant/servant
Wouter Swierstra. 2008. Data types à la carte. Journal of Functional Programming 18, 4 (2008), 423–436. https://doi.org/10.

1017/S0956796808006758

Masaru Tomita. 1986. Efficient parsing for natural language: A fast algorithm for practical systems. Kluwer Academic.
Hendrik van Antwerpen, Casper Poulsen, Arjen Rouvoet, and Eelco Visser. 2018. Scopes as types. Proceedings of the ACM

on Programming Languages 2 (10 2018), 1–30. https://doi.org/10.1145/3276484

David Van Horn and Matthew Might. 2010. Abstracting Abstract Machines. In Proceedings of the 15th ACM SIGPLAN
International Conference on Functional Programming (Baltimore, Maryland, USA) (ICFP ’10). Association for Computing
Machinery, New York, NY, USA, 51–62. https://doi.org/10.1145/1863543.1863553

Nicolas Wu and Tom Schrijvers. 2015. Fusion for Free - Efficient Algebraic Effect Handlers. In Mathematics of Program
Construction - 12th International Conference, 2015. Proceedings (Lecture Notes in Computer Science, Vol. 9129), Ralf Hinze
and Janis Voigtländer (Eds.). Springer, 302–322. https://doi.org/10.1007/978-3-319-19797-5_15

Nicolas Wu, Tom Schrijvers, and Ralf Hinze. 2014. Effect Handlers in Scope. In Proceedings of the 2014 ACM SIGPLAN
Symposium on Haskell (Gothenburg, Sweden) (Haskell ’14). Association for Computing Machinery, New York, NY, USA,
1–12. https://doi.org/10.1145/2633357.2633358

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 42. Publication date: January 2020.

