NEPTUNE: Network- and GPU-aware Management
of Serverless Functions at the Edge
Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, Luca Terracciano
Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano
Milan, Italy
{name.surname}@polimi.it

2
2
0
2

y
a
M
9

]
E
S
.
s
c
[

1
v
0
2
3
4
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Nowadays a wide range of applications is constrained by low-
latency requirements that cloud infrastructures cannot meet. Multi-
access Edge Computing (MEC) has been proposed as the reference
architecture for executing applications closer to users and reduc-
ing latency, but new challenges arise: edge nodes are resource-
constrained, the workload can vary significantly since users are
nomadic, and task complexity is increasing (e.g., machine learning
inference). To overcome these problems, the paper presents NEP-
TUNE, a serverless-based framework for managing complex MEC
solutions. NEPTUNE i) places functions on edge nodes according to
user locations, ii) avoids the saturation of single nodes, iii) exploits
GPUs when available, and iv) allocates resources (CPU cores) dy-
namically to meet foreseen execution times. A prototype, built on
top of K3S, was used to evaluate NEPTUNE on a set of experiments
that demonstrate a significant reduction in terms of response time,
network overhead, and resource consumption compared to three
well-known approaches.

CCS CONCEPTS
â€¢ Theory of computation â†’ Scheduling algorithms; â€¢ Comput-
ing methodologies â†’ Distributed computing methodologies; â€¢ Com-
puter systems organization â†’ Distributed architectures.

KEYWORDS
serverless, edge computing, gpu, placement, dynamic resource allo-
cation, control theory

ACM Reference Format:
Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, Luca Terrac-
ciano. 2022. NEPTUNE: Network- and GPU-aware Management of Server-
less Functions at the Edge. In 17th International Symposium on Software
Engineering for Adaptive and Self-Managing Systems (SEAMS â€™22), May
18â€“23, 2022, PITTSBURGH, PA, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3524844.3528051

1 INTRODUCTION
Multi-access Edge Computing (MEC) [30] has emerged as a new
distributed architecture for running computations at the edge of the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9305-8/22/05. . . $15.00
https://doi.org/10.1145/3524844.3528051

network and reduce latency compared to cloud executions. Differ-
ently from cloud computing, which is characterized by a virtually
infinite amount of resources placed on large data centers, MEC
infrastructures are based on geo-distributed networks of resource-
constrained nodes (e.g., 5G base stations) that serve requests and
process data close to the users.

The rise of edge computing [17], also fostered by the advent of
5G networks, enables the creation of applications with extremely
low latency requirements like autonomous driving [28], VR/AR [10]
and mobile gaming [49] systems. According to Li et al. [23], the
average network delay from 260 locations to the nearest Amazon
EC2 availability zone is approximately 74ms. This makes meeting
tight response time requirements in the cloud nearly impossible. In
use-cases like obstacle detection, response times of a few hundreds
of milliseconds are required [25] and thus the network delay must be
lower than the one offered by cloud-based solutions. Many mobile
devices would allow these computations to be executed on the
device itself, but this is not always possible given the inherent
complexity of some tasks (e.g., machine learning-based ones) and
the need for limiting resource consumption (e.g., to avoid battery
draining).

An important challenge of edge computing is that clients usu-
ally produce highly dynamic workloads since they move among
different areas (e.g., self-driving vehicles) and the amount of traffic
in a given region can rapidly escalate (e.g., users moving towards
a stadium for an event). To tackle these cases, solutions that scale
resources (i.e., virtual machines and containers [14]) automatically
according to the workload have been extensively investigated in
the context of cloud computing, ranging from approaches based
on rules [12, 48] and machine learning [27, 51] to those based on
time-series analysis [35]. These solutions assume that (new) re-
sources are always available and that nodes are connected through
a low-latency, high-bandwidth network. At the edge, these assump-
tions are not valid anymore and some ad-hoc solutions have been
presented in the literature. For example, Ascigil et al. [1] and Wang
et al. [44] propose a solution for service placement on resource-
constrained nodes, while Poularakis et al. [32] focus on request
routing and load balancing at the edge.

Approaches that focus on service placement or request routing
for MEC aim to maximize the throughput of edge nodes, but com-
prehensive solutions that address placement, routing, and minimal
delays at the same time are still work in progress. In addition, the
tasks at the edge, like AI-based computations, are becoming heavier
and heavier. The use of GPUs [9] can be fundamental to acceler-
ate these computations, but they are seldom taken into account
explicitly [6, 20, 39].

 
 
 
 
 
 
SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

To tame all these problems, this paper presents NEPTUNE, a com-
prehensive framework for the runtime management of large-scale
edge applications that exploits placement, routing, network delays,
and CPU/GPU interplay in a coordinated way to allow for the con-
current execution of edge applications that meet user-set response
times. NEPTUNE uses the serverless paradigm [19] to let service
providers deploy and execute latency-constrained functions without
managing the underlying infrastructure. NEPTUNE uses Mixed In-
teger Programming (MIP) to allocate functions on nodes, minimize
network delays, and exploit GPUs if available. It uses lightweight
control theoretical planners to allocate CPU cores dynamically to
execute the remaining functions.

NEPTUNE is then holistic and self-adaptive. It addresses edge
nodes, function placement, request routing, available resources, and
hardware accelerators in a single and coherent framework. Users
only provide functions and foreseen response times, and then the
system automatically probes available nodes as well as the locality
and intensity of workloads and reacts autonomously. While other
approaches (see Section 5) only focus on a single or few aspects and
they can only be considered partial solutions, NEPTUNE tackles
all of them and oversees the whole lifecycle from deployment to
runtime management.

We built a prototype that extends K3S1, a popular tool for con-
tainer orchestration at the edge, to evaluate NEPTUNE on a set
of experiments executed on a geo-distributed cluster of virtual
machines provisioned on Amazon Web Services (AWS). To pro-
vide a comprehensive and meaningful set of experiments, we as-
sessed NEPTUNE against three realistic benchmark applications
â€”including one that can be accelerated with GPUs. The compari-
son revealed 9.4 times fewer response time violations, and 1.6 and
17.8 times improvements as for resource consumption and network
delays, respectively.

NEPTUNE builds up from PAPS [4] from which inherits part of
the design. Compared to PAPS, this paper provides i) a new place-
ment and routing algorithm that exploits resources more efficiently
and minimizes service disruption, ii) the support of GPUs, and iii)
an empirical evaluation of the approach (PAPS was only simulated).
The rest of the paper is organized as follows. Section 2 explains
the problem addressed by NEPTUNE and provides an overview of
the solution. Section 3 presents how NEPTUNE tackles placement,
routing, and GPU/CPU allocation. Section 4 shows the assessment
we carried out to evaluate NEPTUNE. Section 5 presents some re-
lated work, and Section 6 concludes the paper.

2 NEPTUNE
The goal of NEPTUNE is to allow for the execution of multiple con-
current applications on a MEC infrastructure with user-set response
times and to optimize the use of available resources. NEPTUNE must
be able to take into consideration the main aspects of edge com-
puting: resource-constrained nodes, limited network infrastructure,
highly fluctuating workloads, and strict latency requirements.

A MEC infrastructure is composed of a set ğ‘ of distributed nodes
that allow clients to execute a set ğ´ of applications. Edge clients
(e.g., self-driving cars, mobile phones, or VR/AR devices) access
applications placed on MEC nodes. Each node comes with cores,

1https://k3s.io

Figure 1: MEC Topology.

memory, and maybe GPUs, along with their memory. Because of
user mobility, the workload on each node can vary frequently, and
resource limitations do not always allow each node to serve all the
requests it receives; some requests must be outsourced to nearby
nodes.

Given a request ğ‘Ÿ for an application ğ‘ âˆˆ ğ´, its response time
ğ‘…ğ‘‡ is measured as the time required to transmit ğ‘Ÿ from the client
to the closest node ğ‘–, execute the request, and receive a response.
More formally, ğ‘…ğ‘‡ is defined as ğ‘…ğ‘‡ = ğ¸ + ğ‘„ + ğ·, where ğ¸ (execution
time) represents the time taken for running ğ‘, ğ‘„ (queue time) is the
time spent by ğ‘Ÿ waiting for being managed, and ğ· is the network
delay (or network latency). In particular, as shown in Figure 1, ğ· is
the sum of the (round trip) time needed by ğ‘Ÿ to reach ğ‘– (ğ‘¦ğ‘– ) and, if
needed, the (round trip) time needed to outsource the computation
on a nearby node ğ‘— (ğ›¿ğ‘–,ğ‘— ). NEPTUNE handles requests once they
enter the MEC topology and assumes ğ‘¦ğ‘– be optimized by existing
protocols [11].

NEPTUNE is then location aware since it considers the geograph-
ical distribution of both nodes and workloads. It stores a represen-
tation of the MEC network by measuring the inter-node delays and
it monitors where and how many requests are generated by users.
Furthermore, NEPTUNE allows users to put a threshold (service
level agreement) on the response time provided by each application
(ğ‘…ğ‘‡ ğ‘…
ğ‘ ).

2.1 Solution overview
NEPTUNE requires that an application ğ‘ be deployed as a set ğ¹ğ‘
of functions â€”as prescribed by the serverless paradigm. Develop-
ers focus on application code without the burden of managing the
underlying infrastructure. Each function covers a single function-
ality and supplies a single or a small set of REST endpoints. The
result is more flexible and faster to scale compared to traditional
architectures (e.g., monoliths or microservices).

(with ğ‘…ğ‘‡ ğ‘…

Besides the functionâ€™s code, NEPTUNE requires that each func-
tion be associated with a user-provided required response time
ğ‘ ) and the memory ğ‘šğ¶ğ‘ƒğ‘ˆ
ğ‘…ğ‘‡ ğ‘…
needed to properly
ğ‘“
execute it. In case of GPU-accelerated functions, the GPU memory
ğ‘šğºğ‘ƒğ‘ˆ
must also be specified. Then, NEPTUNE manages both its
ğ‘“

ğ‘“ â‰¤ ğ‘…ğ‘‡ ğ‘…

ğ‘“

Î´1,2Î´1,3Î´3,4Î´2,4Î´2,3Î´1,3Mobile appSelf-drivingVR/ARy3y1y2y4Edge clientsMEC Node 1MEC Node 2MEC Node 3MEC Node 4NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

deployment by provisioning one or more function instances, and
its operation with the goal of fulfilling the set response time.

NEPTUNE manages functions through a three-level control hier-

archy: Topology, Community, and Node.

Since function placement is NP-hard [32], the main goal of the
Topology level is to tackle the complexity for the lower levels by
splitting the topology into communities of closely-located nodes.
Each community is independent of the others and a request can
only be managed within the community of the node that received
it. If this is not possible, then the community is undersized and the
Topology level must reconfigure the communities.

The Topology level employs a single controller based on the
Speaker-listener Label Propagation Algorithm (SLPA) â€” proposed by
Xie et al. [47] â€” to create the communities. SLPA has a complexity
of ğ‘‚ (ğ‘¡ âˆ— ğ‘ ) where ğ‘ is the number of distributed nodes and ğ‘¡ is the
user-defined maximum number of iterations. Since the complexity
scales linearly with the number of nodes, this solution has proven
to be suitable also for large clusters [4].

Given a maximum community size ğ‘€ğ¶ğ‘† and the maximum al-
lowed network delay Î”, SLPA splits the topology in a set of com-
munities with a number of nodes that is lower than ğ‘€ğ¶ğ‘† and inter-
node delays smaller than Î” (i.e., ğ›¿ğ‘–,ğ‘— â‰¤ Î” for all nodes ğ‘– and ğ‘— in the
community). SLPA could potentially assign a node to multiple com-
munities, but to avoid resource contention, NEPTUNE re-allocates
shared nodes â€”if neededâ€” to create non-overlapping communities.
At Community level, each community is equipped with a MIP-
based controller in charge of managing function instances. The
controller places function instances on the different nodes of the
community by first considering those that could exploit GPUs, if
available. The goal is to minimize network delay by dynamically
deploying function instances close to where the demand is gener-
ated and at the same time to minimize the time spent to forward
them when needed.

The Community level computes routing policies i) to allow each
node to forward part of the workload to other close nodes, and ii) to
prioritize computation-intensive functions by forwarding requests
to GPUs up to their full utilization, and then send the remaining
requests to CPUs. To avoid saturating single nodes, the Community
level can also scale function instances horizontally, that is, it can
replicate them on nearby nodes. For example, if a node ğ‘– cannot offer
enough resources to execute an instance of ğ‘“ to serve workload
ğœ†ğ‘“ ,ğ‘– , the Community level creates a new instance (of ğ‘“ ) on a node ğ‘—
close to ğ‘–, and the requests that cannot be served by ğ‘– are forwarded
to ğ‘—.

While the first two control levels take care of network latency
(ğ·), once the requests arrive at the node that processes them, the
Node level ensures that function instances have the needed amount
of cores to meet set response times. Each function instance is man-
aged by a dedicated Proportional Integral (PI) controller that pro-
vides vertical scaling, that is, it adds, or removes, CPU cores to
the function (container). Unlike other approches [2, 34], NEPTUNE
can reconfigure CPU cores without restarting function instances,
that is, without service disruption. Figure 2 shows that, given a set
point (i.e., the desired response time), the PI controller periodically
monitors the performance of its function instance and dynamically
allocates CPU cores to optimize both execution time ğ¸ and queue
time ğ‘„.

Figure 2: NEPTUNE.

Also, the controllers at Node level are independent of each other:
each controller is not aware of how many cores the others would
like to allocate. Therefore, since the sum of the requests might
exceed the capacity of a node, a Contention Manager solves resource
contentions by proportionally scaling down requested allocations.
Every control level is meant to work independently of the others,
and no communication is required between controllers operating
at the same level. This means that NEPTUNE can easily scale.

The three control levels operate at different frequencies but yet
cohesively to eliminate potential interference. Thanks to fast PI
controllers and vertical scaling, the Node level operates every few
seconds to handle workload bursts, whereas the Community level
computes functions and routing policies in the order of minutes,
and allows the Node level to fully exploit the underlying resources.
The Topology level runs at longer intervals, but it can react faster
when there are changes in the network, such as when a node is
added or removed.

A real-time monitoring infrastructure is the only communication
across the levels. It allows NEPTUNE to gather performance metrics
(e.g., response times, core allocations per function instance, net-
work delays) needed by the three control levels to properly operate.
Note that the controllers at Community and Node levels measure
device performance in real-time without any a-priori assumption.
This means that NEPTUNE can also manage heterogeneous CPUs
with different performance levels (e.g., different types of virtual
machines).

Figure 2 shows a MEC topology controlled by the three-level
hierarchical control adopted by NEPTUNE. Control components are
depicted in grey. First of all, the Topology level splits the network
into four communities. Each community is handled by its Commu-
nity level controller, responsible for function placement and request

MEC TopologyMonitoring InfrastructureCommunity 1Node LevelCommunity 2...faContention ManagerPIPICommunity 3...Community 4...Routingfa    0.7   N1 fa    0.3   N3...Î´2,3GPURouting...TopologyLevelSLPARouting...Î´1,2Î´1,3fdNode N3CommunityLevelMIPfafcNode N1fbfdNode N2SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

routing. The figure reports a detailed view of Community 1. We
can observe that i) a set of functions (ğ‘“ğ‘, ğ‘“ğ‘ , ğ‘“ğ‘ , ğ‘“ğ‘‘ ) is placed across
three nodes and, ii) each node is provided with its set of routing
policies. In particular, the routing policies for Node N1 enforce that
70% of the requests for ğ‘“ğ‘ are served by the node itself, while the
remaining 30% is forwarded to the instance, running on Node N3,
that exploits a GPU. Finally, the figure shows the materialization of
Node level on Node N3. Each function instance (ğ‘“ğ‘, ğ‘“ğ‘‘ ) is vertically
scaled by a dedicated PI controller, and resource contentions are
solved by the Contention Manager.

3 PLACEMENT, ROUTING, AND

ALLOCATION

As explain above, the Topology level partitions the network in
a set of communities using algorithm SLPA. Each community is
controlled independently from the others. The main goal of the
Community level controller is to dynamically place function exe-
cutions as close to where the workload is generated as possible. A
trivial solution, which can drastically reduce network delay, would
be to replicate the whole set of functions on each node. Since nodes
have limited resources, this approach is often not feasible.

An effective placement solution should not saturate nodes and
should allow for stable placement, that is, it should avoid the disrup-
tion implied by keeping migrating functions among nodes2. At the
same time, the placement cannot be fully static since users move
and requirements change.

Each placement effort should also consider graceful termination
periods and cold starts. The former is the user-defined amount of
time we must wait whenever a function instance is to be deleted
to let it finish serving pending requests. The latter is a delay that
can range from seconds to minutes and that affects newly created
instances [45]. Before a function instance starts serving requests,
a container must be started and the execution environment be
initialized. Diverse approaches, like container pooling [24, 37], can
mitigate cold starts, but their efficiency depends on the functions at
hand and they cannot always reduce cold starts in a significant way.
This is why NEPTUNE does not exploit these solutions natively.

To address these problems, the Community level adopts two
similar instances of a 2-step optimization process â€”based on Mixed
Integer Programmingâ€” to allocate GPUs first and then CPUs. In
both cases, the first step aims to find the best function placement
and routing policy that minimize the overall network delay. Then,
since diverse placements with a network delay close to the optimal
one may require different changes in function deployment, the
second step is in charge of choosing a placement that minimizes
deployment actions, that is, disruption.

Table 1 summarizes the inputs NEPTUNE requires to the user,
the characteristics of used nodes, the values gathered by the moni-
toring infrastructure, and the decision variables adopted in the MIP
formulation.

3.1 Function placement
Each time the Community level is activated, the 2-step optimization
process is executed twice. The first execution aims to fully utilize

Table 1: Inputs, data, and decision variables.

Inputs
ğ‘šğ¶ğ‘ƒğ‘ˆ
ğ‘“
ğ‘šğºğ‘ƒğ‘ˆ
ğ‘“
ğœ™ ğ‘“

Memory required by function ğ‘“
GPU memory required by function ğ‘“
Maximum allowed network delay for function ğ‘“

Infrastructure data
ğ‘€ğ¶ğ‘ƒğ‘ˆ
ğ‘—
ğ‘€ğºğ‘ƒğ‘ˆ
ğ‘—
ğ‘ˆ ğ¶ğ‘ƒğ‘ˆ
ğ‘—
ğ‘ˆ ğºğ‘ƒğ‘ˆ
ğ‘—

Memory available on node ğ‘—
GPU memory available on node ğ‘—
CPU cores on node ğ‘—
GPU cores on node ğ‘—

Monitored data

ğ›¿ğ‘–,ğ‘—
ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡
ğœ†ğ‘“ ,ğ‘–
ğ‘¢ğ¶ğ‘ƒğ‘ˆ
ğ‘—
ğ‘¢ğºğ‘ƒğ‘ˆ
ğ‘—

Network delay between nodes ğ‘– and ğ‘—
Objective function value found after step 1
Incoming ğ‘“ requests to node ğ‘–
Average CPU cores used by node ğ‘— per single ğ‘“ request
Average GPU cores used by node ğ‘— per single ğ‘“ request

Decision variables
ğ‘¥ğ¶ğ‘ƒğ‘ˆ
ğ‘“ ,ğ‘–,ğ‘—
ğ‘ğ¶ğ‘ƒğ‘ˆ
ğ‘“ ,ğ‘—
ğ‘¥ğºğ‘ƒğ‘ˆ
ğ‘“ ,ğ‘–,ğ‘—
ğ‘ğºğ‘ƒğ‘ˆ
ğ‘“ ,ğ‘—
ğ‘€ğº ğ‘“
ğ¶ğ‘…ğ‘“
ğ·ğ¿ğ‘“

Fraction of ğ‘“ requests sent to CPU instances from node ğ‘– to ğ‘—
1 if a CPU instance of ğ‘“ is deployed on node ğ‘—, 0 otherwise
Fraction of ğ‘“ requests sent to GPU instances from node ğ‘– to ğ‘—
1 if a GPU instance of ğ‘“ is deployed on node ğ‘—, 0 otherwise
Number of ğ‘“ migrations
Number of ğ‘“ creations
Number of ğ‘“ deletions

GPU resources, while the second only considers CPUs and the
remaining workload to be handled.

Since the two executions are similar, the formulation presented
herein is generalized. Some of the employed data are resource-
specific (e.g., ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— , ğ‘šğ‘“ ): Table 1 differentiate them with a ğ¶ğ‘ƒğ‘ˆ or
ğºğ‘ƒğ‘ˆ superscript, while in the rest of this section the superscripts
are omitted for simplicity.

Network delay minimization. The first step aims to place function
instances and to find routing policies that minimize the overall
network delay ğ· in a given community ğ¶ âŠ† ğ‘ .

The formulation employs two decision variables: ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— and ğ‘ ğ‘“ ,ğ‘— .
The former (ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆˆ [0 : 1]) represents the amount of incoming ğ‘“
requests3 (ğœ†ğ‘“ ,ğ‘– ) that node ğ‘– forwards to node ğ‘— (i.e., routing policies).
The latter (ğ‘ ğ‘“ ,ğ‘— ) is a boolean variable that is ğ‘¡ğ‘Ÿğ‘¢ğ‘’ if an ğ‘“ instance is
deployed onto node ğ‘— (i.e., placement).

The objective function (Formula 1) minimizes the overall net-
work delay of the incoming workload in ğ¶. Starting from the in-
coming ğ‘“ requests to each node ğ‘– and the measured delay between
nodes ğ‘– and ğ‘—, it computes the fractions of outsourced requests to
minimize the overall network delay:

ğ‘šğ‘–ğ‘›

ğ¹
âˆ‘ï¸

ğ¶
âˆ‘ï¸

ğ¶
âˆ‘ï¸

ğ‘“

ğ‘–

ğ‘—

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆ— ğœ†ğ‘“ ,ğ‘– âˆ— ğ›¿ğ‘–,ğ‘—

(1)

2

NEPTUNE does not handle application state migration.

3For the sake of brevity, we use ğ‘“ request to mean a request generated for function ğ‘“ ,
and ğ‘“ instance to refer to an instance of function ğ‘“ .

NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

If we only considered inter-node delays (ğ›¿ğ‘–,ğ‘— ), we would minimize
the overall network delay only if incoming requests were distributed
evenly (i.e., each node manages the same amount of requests). Since
workloads can be very different, the addition of per-node incoming
requests (ğœ†ğ‘“ ,ğ‘– ) gives a more appropriate formulation of the problem.
Intuitively, the higher the workload in a specific area is, the more
important the minimization of network delay becomes.

In addition to the function to minimize, we must add some con-
straints. First, requests cannot be forwarded too far from where
they are generated. Each function is characterized by parameter ğœ™ ğ‘“ ,
which sets the maximum allowed network delay of each ğ‘“ request:

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆ— ğ›¿ğ‘–,ğ‘— â‰¤ ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆ— ğœ™ ğ‘“

âˆ€ğ‘–, ğ‘— âˆˆ ğ¶, âˆ€ğ‘“ âˆˆ ğ¹

(2)

Second, the nodes that receive forwarded requests must have a

function instance that can serve them:

ğ‘ ğ‘“ ,ğ‘— = ğ‘– ğ‘“ (

ğ¶
âˆ‘ï¸

ğ‘–

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— > 0) 1 ğ‘’ğ‘™ğ‘ ğ‘’ 0 âˆ€ğ‘— âˆˆ ğ¶, âˆ€ğ‘“ âˆˆ ğ¹

(3)

Third, the overall memory required by the functions (ğ‘šğ‘“ ) placed

on a node must not exceed its capacity ğ‘€ğ‘— :

ğ¹
âˆ‘ï¸

ğ‘“

ğ‘ ğ‘“ ,ğ‘— âˆ— ğ‘šğ‘“ â‰¤ ğ‘€ğ‘— âˆ€ğ‘— âˆˆ ğ¶

(4)

Fourth, to avoid resource contentions, routing policies must
consider the overall amount of GPU or CPU cores available in the
node (ğ‘ˆ ğ‘— ) and the average GPU or CPU cores consumption for each
ğ‘“ request processed on node ğ‘— (ğ‘¢ğ‘“ ,ğ‘— ):

ğ¶
âˆ‘ï¸

ğ¹
âˆ‘ï¸

ğ‘–

ğ‘“

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆ— ğœ†ğ‘“ ,ğ‘– âˆ— ğ‘¢ğ‘“ ,ğ‘— â‰¤ ğ‘ˆ ğ‘— âˆ€ğ‘— âˆˆ ğ¶

(5)

Fifth, routing policies must be defined for all the nodes in the

community and all the functions of interest:

ğ¶
âˆ‘ï¸

ğ‘—

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— = 1 âˆ€ğ‘– âˆˆ ğ¶, âˆ€ğ‘“ âˆˆ ğ¹

(6)

Note that when ğ‘– = ğ‘—, ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— gives the fraction of ğ‘“ requests

executed locally, that is, on ğ‘– itself.

This optimization problem finds the best placement with the
minimum network delay. However, each iteration (execution of the
optimization problem) may suggest a placement that requires many
disruptive operations (i.e., deletions, creations, and migrations)
with respect to the previous placement (iteration). For this reason,
a second step is used to minimize service disruption and ameliorate
the result.

Disruption minimization. The second step searches for a function
placement that minimizes function creation, deletion, and migration
with an overall network delay close to the optimal one found by
the first step.

This means that the second step keeps the constraints defined

in Formulae 2- 6 and adds:

ğ¹
âˆ‘ï¸

ğ¶
âˆ‘ï¸

ğ¶
âˆ‘ï¸

ğ‘“

ğ‘–

ğ‘—

ğ‘¥ ğ‘“ ,ğ‘–,ğ‘— âˆ— ğœ†ğ‘“ ,ğ‘– âˆ— ğ›¿ğ‘–,ğ‘— â‰¤ ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡ âˆ— (1 + ğœ–)

(7)

to impose that the final placement must be in the interval [ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡ ,
ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡ âˆ—(1+ğœ–)], where ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡ is the smallest network delay found after
the first step, and ğœ– is an arbitrarily small parameter that quantifies
the worsening in terms of network overhead. For example, ğœ– = 0.05
means a worsening up to 5%.

We also consider the number of created, deleted, and migrated
ğ‘“ instances between two subsequent executions of the 2-step opti-
mization process, that is, between the to-be-computed placement
(ğ‘ ğ‘“ ,ğ‘– ) and the current one (ğ‘ğ‘œğ‘™ğ‘‘
). ğ·ğ¿ğ‘“ and ğ¶ğ‘…ğ‘“ denote, respectively,
ğ‘“ ,ğ‘–
the maximum between 0 and the removed (added) ğ‘“ instances be-
tween the two iterations:

ğ·ğ¿ğ‘“ =

ğ¶ğ‘…ğ‘“ =

ğ¶
âˆ‘ï¸

ğ‘–
ğ¶
âˆ‘ï¸

ğ‘–

ğ‘šğ‘ğ‘¥ (ğ‘ğ‘œğ‘™ğ‘‘

ğ‘“ ,ğ‘– âˆ’ ğ‘ ğ‘“ ,ğ‘–, 0) âˆ€ğ‘“ âˆˆ ğ¹

ğ‘šğ‘ğ‘¥ (ğ‘ ğ‘“ ,ğ‘– âˆ’ ğ‘ğ‘œğ‘™ğ‘‘

ğ‘“ ,ğ‘– , 0) âˆ€ğ‘“ âˆˆ ğ¹

(8)

The number of migrations (in the new placement) that represents
the number of instances that have been moved from one node to
another is computed as the minimum between instance creations
ğ¶ğ‘…ğ‘“ and instance deletions ğ·ğ¿ğ‘“ :

ğ‘€ğº ğ‘“ = ğ‘šğ‘–ğ‘›(ğ¶ğ‘…ğ‘“ , ğ·ğ¿ğ‘“ ) âˆ€ğ‘“ âˆˆ ğ¹
The new objective function is then defined as:

ğ‘šğ‘–ğ‘›

ğ¹
âˆ‘ï¸

ğ‘“

ğ‘€ğº ğ‘“ +

1
ğ·ğ¿ğ‘“ + 2

âˆ’

1
ğ¶ğ‘…ğ‘“ + 2

(9)

(10)

The goal of the objective function is to minimize the number
of migrations (ğ‘€ğº ğ‘“ ), since deletions and creations are necessary,
1
to avoid over- and under-provisioning. Factors
ğ¶ğ‘…ğ‘“ +2 ,
which are always lower than 1, allow us to discriminate among so-
lutions with the same amount of migrations, but a different number
of creations and deletions.

ğ·ğ¿ğ‘“ +2 and

1

This formulation ensures close-to-optimal network delays, along
with the minimum number of instances, to serve the current work-
load. The controllers at Node level are then entitled to scale execu-
tors vertically as needed (see next Section).

3.2 CPU allocation
The Node level is in charge of minimizing ğ‘„ğ¸, that is, the handling
time, defined as the sum of the execution time ğ¸ and the queue time
ğ‘„; the network delay ğ· is already minimized by the Community
level. ğ‘„ğ¸ can vary due to many factors, such as variations in the
workload or changes in the execution environment and we aim
to control it by changing the amount of CPU cores allocated to
function instances. If this is not enough, the problem is lifted up
to the Community level that re-calibrates the number of function
instances.

Control theoretical approaches have proven to be an effective
solution for the self-adaptive management of these resources [6, 15].
The Node level comprises a lightweight Proportional-Integral (PI)

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

controller for each function instance to scale allocated cores dy-
namically. PI controllers support fast control periods, have constant
complexity, and provide formal design-time guarantees.

Each function instance is equipped with an independent PI con-
troller. The control loop monitors the average value of ğ‘„ğ¸, computes
the allocation, and actuates it. More formally, given a desired set
point ğ‘„ğ¸ğ‘“ ,ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘ , the controller periodically measures the current
value of ğ‘„ğ¸ğ‘“ ,ğ‘— (controlled variable) â€” the actual value of ğ‘„ğ¸ğ‘“ on
node ğ‘— â€” and computes the delta between desired and actual value.
Note that, since the controllers will strive to keep ğ‘„ğ¸ğ‘“ ,ğ‘— close to
the set point ğ‘„ğ¸ğ‘“ ,ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘ , this value should be set to a lower value
than the desired ğ‘…ğ‘‡ ğ‘…
ğ‘“

The controller reacts to the error and recommends the new
amount of cores that the function should use. Algorithm 1 describes
the computation: Line 2 computes error ğ‘’ğ‘Ÿğ‘Ÿ as the difference be-

.

Algorithm 1 Node level CPU core allocation.
1: procedure ComputeInstanceCores(ğ‘“ , ğ‘—)
2:

;

ğ‘„ğ¸ğ‘“ ,ğ‘—

âˆ’ 1

1
ğ‘„ğ¸ğ‘“ ,ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘

ğ‘’ğ‘Ÿğ‘Ÿ :=
ğ‘ğ‘ğ‘¢ := ğ‘”ğ‘’ğ‘¡ğ¶ğ‘ƒğ‘ˆ ğ´ğ‘™ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘“ , ğ‘—);
ğ‘–ğ‘›ğ‘¡ğ‘œğ‘™ğ‘‘ := ğ‘ğ‘ğ‘¢ âˆ’ ğ‘”ğ‘–ğ‘›ğ‘¡ âˆ— ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘™ğ‘‘ ;
ğ‘–ğ‘›ğ‘¡ := ğ‘–ğ‘›ğ‘¡ğ‘œğ‘™ğ‘‘ + ğ‘”ğ‘–ğ‘›ğ‘¡ âˆ— ğ‘’ğ‘Ÿğ‘Ÿ ;
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘™ğ‘‘ := ğ‘’ğ‘Ÿğ‘Ÿ ;
ğ‘ğ‘Ÿğ‘œğ‘ := ğ‘’ğ‘Ÿğ‘Ÿ âˆ— ğ‘”ğ‘ğ‘Ÿğ‘œğ‘ ;
ğ‘ğ‘ğ‘¢ := ğ‘–ğ‘›ğ‘¡ + ğ‘ğ‘Ÿğ‘œğ‘;
ğ‘ğ‘ğ‘¢ := ğ‘šğ‘ğ‘¥ (ğ‘ğ‘ğ‘¢ğ‘šğ‘–ğ‘›, ğ‘šğ‘–ğ‘›(ğ‘ğ‘ğ‘¢ğ‘šğ‘ğ‘¥ , ğ‘ğ‘ğ‘¢));

3:

4:

5:

6:

7:

8:

9:
10: end procedure

tween the inverse of ğ‘„ğ¸ğ‘“ ,ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘ and ğ‘„ğ¸ğ‘“ ,ğ‘— . To compute the Integral
contribution, the current core allocation (ğ‘ğ‘ğ‘¢) of the function in-
stance is retrieved at line 3. The previous integral contribution
ğ‘–ğ‘›ğ‘¡ğ‘œğ‘™ğ‘‘ is computed at line 4 by using the allocation, the integral
gain ğ‘”ğ‘–ğ‘›ğ‘¡ (i.e., a tuning parameter), and the prior error ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘™ğ‘‘ . The
integral component ğ‘–ğ‘›ğ‘¡ is computed by multiplying the current
error ğ‘’ğ‘Ÿğ‘Ÿ times the integral gain ğ‘”ğ‘–ğ‘›ğ‘¡ , and by adding ğ‘–ğ‘›ğ‘¡ğ‘œğ‘™ğ‘‘ (line 5).
The previous error ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘™ğ‘‘ is then updated at line 6.

The proportional contribution is computed by using ğ‘’ğ‘Ÿğ‘Ÿ and
the proportional gain ğ‘”ğ‘ğ‘Ÿğ‘œğ‘ at line 7. Finally, the new allocation
is calculated as the sum of the two contributions (line 8) and then
adjusted according to the maximum and minimum allowed core
allocations ğ‘ğ‘ğ‘¢ğ‘šğ‘ğ‘¥ and ğ‘ğ‘ğ‘¢ğ‘šğ‘–ğ‘›, respectively.

Being independent of the others, these controllers are not aware
of available CPU cores and of the allocations computed by the other
controllers. Therefore, the computed allocations (line 9) are not
immediately applied since they could exceed the allowed capacity.
The allocations of the function instances deployed on a node are
processed by a Contention Manager (one per node), which is in
charge of computing a feasible allocation. If the sum of suggested
allocations fits the allowed capacity, they are applied without any
modification. Otherwise, they are scaled down proportionally. The
Contention Manager can easily be extended and embed other, non-
proportional heuristics to manage resource contention.

4 EVALUATION

Implementation. We implemented a prototype4 of NEPTUNE
built on top K3S, a popular distribution of Kubernetes5 optimized
for edge computing. Each control level is materialized in a dedi-
cated component that exclusively uses native K3s APIs to manage
deployed applications. Conversely to existing approaches (see Sec-
tion 5), the prototype is capable of performing in-place vertical
scaling of containers, that is, it can dynamically update the CPU
cores allocated to the different containers without restarting the
application.

The stable version of K3S does not allow one to change allo-
cated resources without restarting function instances, a process
that sometimes can take minutes. This could decrease the capability
of the Node level to handle bursty workloads. For this reason, the
prototype augments K3S with the Kubernetes Enhancement Proposal
6 and allows re-
1287 that implements In-Place Pod Vertical Scaling
sources to be changed without restarts. This enables faster control
loops and better control quality.

To provide an effective usage of GPUs, the prototype uses nvidia-
7, a container runtime that enables the use of GPUs within
docker
containers. However, by default, GPU access can only be reserved to
one function instance at a time. This prevents the full exploitation
of GPUs and limits the possible placements produced at Community
8
level. To solve this problem, the prototype employs a device plugin
developed by Amazon that enables the fractional allocation of GPUs.
In particular, the plugin makes use of the Nvidia Multi Process
9 (MPS), a runtime solution designed to transparently allow
Service
GPUs to be shared among multiple processes (e.g., containers).

Research questions. The solution adopted at Topology level has
been largely covered by PAPS [4]. The experiments in the paper
focus on evaluating Community and Node level. The conducted
evaluation addresses the following research questions:
RQ1 How does NEPTUNE handle workloads generated by mobile

users at the edge?

RQ2 How does NEPTUNE perform compared to other state-of-

the-art approaches?

RQ3 How does NEPTUNE use GPUs to speed up response times?

4.1 Experimental setup

Infrastructure. We conducted the experiments on a simulated
MEC topology with nodes provisioned as a cluster of AWS EC2 geo-
distributed virtual machines distributed across three areas. Each
area corresponds to a different AWS region: Area A to eu-west, Area
B to us-east, and Area C to us-west. Since communities are indepen-
dent, our experiments focused on evaluating different aspects of
NEPTUNE within a single community that included the three areas.
Figure 3 shows the average network delays between each pair
of areas and nodes computed as the round trip times of an ICMP
[31] (Internet Control Message Protocol) packet. Note that nodes

4Source code available at https://github.com/deib-polimi/edge-autoscaler.
5https://kubernetes.io/.
6https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1287-in-
place-update-pod-resources
7https://github.com/NVIDIA/nvidia-docker
8https://github.com/awslabs/aws-virtual-gpu-device-plugin
9https://docs.nvidia.com/deploy/mps/index.html

NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Table 2: Characteristics of deployed functions.

Name

Language Memory

ğ‘…ğ‘‡ ğ‘…
ğ‘“

Cold start

Simple stateless function

primes

Rust

âˆ¼15 MB

200ms

<5s

Complex application

200ms

âˆ¼360 MB 300ms âˆ¼100s
Java
carts-post
âˆ¼360 MB 200ms âˆ¼100s
Java
carts-delete
âˆ¼360 MB 200ms âˆ¼100s
Java
carts-util
âˆ¼15 MB
Go
catalogue
âˆ¼400 MB 600ms âˆ¼100s
Java
orders
âˆ¼15 MB
50ms
Go
payment
âˆ¼350 MB 50ms
Java
shipping
100ms
âˆ¼15 MB
login
Go
200ms
âˆ¼15 MB
registration Go
Go
50ms
âˆ¼15 MB
user
Machine Learning inference
Python

<5s
âˆ¼100s
<5s
<5s
<5s

âˆ¼500 MB 550ms âˆ¼100s

resnet

<5s

the procedure described in Section 4.5). The set points used by PI
controllers were set to half of the value of ğ‘…ğ‘‡ ğ‘…
ğ‘“

.

We used Locust

12, a distributed scalable performance testing tool,
to feed the system, and mimicked service demand ğœ†ğ‘“ ,ğ‘– through dif-
ferent realistic, dynamic workloads. Each experiment was executed
five times to have (more) consistent results.

Collected metrics. For each experiment, we collected the average
(ğœ‡) and standard deviation (ğœ) of the following metrics: i) response
time (ms) as defined in Section 2, ii) response time violation rate
(% of requests) defined as the percentage of requests that are not
served within ğ‘…ğ‘‡ ğ‘…
considering the 99th percentile of the measured
ğ‘“
response times, iii) network time rate (%) as the percentage of time
spent to forward requests in the network over the total response
time (ğ·/ğ‘…ğ‘‡ ), and iv) allocated cores (millicores or thousandths of a
core) to measure the resources consumed by function instances.

13 (KN), and OpenFaaS

Competitors. Our experiments compare NEPTUNE against three
14 (OF).
well-known approaches: K3S, Knative
K3S is one of the most popular solutions for container orchestration
at the edge. It manages the full lifecycle of containerized applica-
tions deployed in a topology and adopts a fair placement policy, that
is, it schedules containers to keep the resource utilization of nodes
15 to horizontal
equal. K3S exploits the Horizontal Pod Autoscaler
scale applications. KN and OF add serverless functionalities to K3S
and a set of custom components to perform request routing and
horizontal scaling.

To achieve consistent and statistical relevant results, all experi-

ments described in this section were run 5 times.

12https://locust.io/
13https://knative.dev/docs/.
14https://www.openfaas.com/.
15https://rancher.com/docs/rancher/v2.5/en/k8s-in-rancher/horitzontal-pod-
autoscaler/.

Figure 3: Network delay between areas.

of the same area were deployed onto different AWS availability
zones to obtain significant network delays. Each area contained
three worker nodes, and one in Area A was GPU-empowered. These
nodes were deployed as c5.xlarge instances (4 vcpus, 8 GB memory);
the one with GPU that used a g4dn.xlarge instance (4 vcpus, 16 GB
memory, 1 GPU). The master node (not depicted in the figure) was
deployed on a c5.2xlarge instance (8 vcpus, 16 GB memory).

NEPTUNE control periods. Node controllers were configured with
a control period of 5 seconds. Faster control loops can be used but
they may lead to inconsistent resource allocation updates since K3S
resource states are stored in a remote database. Function placement
and routing policies were recomputed by Community controllers
each 1 minute while Topology controller was triggered every 10
minutes.

Applications. To work on a reasonable set of experiments, we
used the three applications summarized in Table 2: we created
the first function, and we borrowed the other two from the liter-
ature [33, 36]. These applications are written using multiple pro-
gramming languages (e.g., Rust, Java, Go) and have different mem-
ory requirements (ranging from 15MB to 500MB) and cold start
times (from a bunch of seconds to minutes). The first application is
primes, a stateless and CPU-heavy function that counts all the prime
numbers less than a given input number. As exemplar complex ap-
10 that implements an e-commerce
plication we employed sock-shop
platform. The application uses a microservice architecture; we fur-
ther decomposed it into smaller functions to make it suitable for a
serverless platform11. For example, microservice carts was divided
into three smaller units: carts-post, carts-delete and cart-util. Finally,
to also evaluate GPU-accelerated tasks (e.g machine learning in-
ference), we used Resnet [43], a neural network model for image
classification, implemented using TensorFlow Serving. For each
function Table 2 also reports the memory requirements, the cold
start times and the desired response times (obtained by applying

10https://github.com/microservices-demo/microservices-demo
11The source code of the function-based version of sock-shop is available at https:
//github.com/deib-polimi/serverless-sock-shop

Community-0Area A 12ms95ms85ms2msnode-A-2node-A-0node-A-1Area B 1msnode-B-2node-B-0node-B-1Area C 1msnode-C-2node-C-0node-C-1SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

(a) Geo-dynamic workload shape (users)

(b) Resource allocation (millicores)

(c) Average response time (ms)

(d) Networking time rate (%)

Figure 4: Behavior of NEPTUNE with moving workloads.

4.2 RQ1: Moving workload
The first experiments evaluate the performance of NEPTUNE when
users move between Area A and Area B within the same community.
We used a cluster of four worker nodes: two nodes in Area A (not
equipped with GPU) and two in Area B. Each run lasted 60 minutes
and used application primes with ğ‘…ğ‘‡ ğ‘…
ğ‘ğ‘Ÿğ‘–ğ‘šğ‘’ğ‘  set to 200ms and the set
point of PI controllers to 100ms. User migration happened twice
per run and consisted in moving 100 users from one area to another
in less than 10 minutes.

Figure 4 shows the behavior of application primes when man-
aged by NEPTUNE. Since the multiple runs executed for this set of
experiments had similar behavior, the figure illustrates how work-
loads, resources, and performance varied over time during one of
these runs. Figure 4a shows how the workload changed in each area.
In particular, the workload was generated by users close to node
Node-A-0 for Area A and Node-B-0 for Area B. Figure 4b presents the
resources allocated to each node over time. Since communities are
independent, at least one instance per function is always allocated

(if possible) to minimize cold starts. Thus, the overall allocation
is always greater than zero. Conversely, if a node ğ‘– has 0 cores
allocated at time ğ‘¡ for function ğ‘“ , it means ğ‘“ is not running on ğ‘– at
ğ‘¡ (e.g., from second 0 to 1250 for Node-B-0).

The chart shows that if one node in an area cannot manage gen-
erated load, the Community level detects this issue and instantiates
a new function instance on another node as close to the work-
load generator as possible. This behavior can be observed close to
second 600 when the workload in Area A reaches the peak and
a new replica is created on Node-A-1. Similarly, at second 1500 a
new replica is deployed on Node-B-1 when the workload in Area
B increases. In contrast, when the workload decreases instances
are deleted as shown close to second 2700 on Node-B-0. Moreover,
the experiment clearly shows how NEPTUNE is able to migrate
function instances when users move to keep the network delay
minimized. For example, close to second 1000, users move from
Area A to Area B, and right after the function is migrated on node
Node-B-0 to handle the workload in the proximity of users.

Thanks to NEPTUNE, function primes never violates the set re-
sponse time: the average response time ğ‘…ğ‘‡ğ‘“ in Figure 4c is always
significantly lower than the threshold (200 ms). The control loops
are able to keep the response time very close to the set point. Con-
trol theoretical controllers behave very well when they operate
with high-frequency control loops, enabled by the in-place vertical
scaling feature [5, 6]. In fact, the response time only deviates from
the set point when the instances are replicated (scaled horizontally),
at seconds 600, 1600 and 2600, since the action requires more time
than re-configuring containers. However, note that the response
time always returns close to the set point, and this shows that
NEPTUNE can recover from multiple types of perturbations (e.g,
creation and deletion of replicas, fluctuating workloads).

Figure 4d shows that NEPTUNE is able to keep the network
overhead extremely low. The only peaks in the chart (seconds 1100
and 2200) are caused by users who change location and by the fact
that routing policies are not updated immediately.

When users start to migrate to another area, replicas cannot al-
ways be created immediately on nodes with the minimum network
delay, as depicted in the chart close to second 1100: the workload
on Node-B-0 increases and an instance is created on Node-B-1.
This behavior occurs because the two-step optimization process
evaluates the placement on B-0 or B-1 to be extremely comparable
(within ğœ–) since they handle a small portion of the traffic compared
to the nodes in Area A. However, NEPTUNE migrates the function
instance directly to Node-B-0 node as soon as the workload in Area
B increases (close to second 1200).

4.3 RQ2: Comparison with other approaches
We compared our solution against the three approaches described
in Section 4.1 by means of application sock-shop. Note that some
of the functions of this application must invoke other functions.
For example, function order invokes function user to retrieve userâ€™s
address and payment card information, function catalogue to re-
trieve product details, function payment to ensure the creation of
the invoice and, finally, in case of success, function carts-delete to
empty the cart. We took these dependencies into account by setting
adequate response times as shown in Table 2: from 50ms, for simple

0500100015002000250030003500Time (s)0255075100# UsersNode-A-0Node-B-00500100015002000250030003500Time (s)05001000150020002500MillicoresNode-A-0Node-A-1Node-B-0Node-B-10500100015002000250030003500Time (s)050100150200RT (ms)Avg RTRTRfQEf,desired0500100015002000250030003500Time(s)0.012.525.037.550.0NTR %NTR %NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Table 3: Results of the comparison with other approaches.

Function

carts-delete

carts-post

carts-util

catalogue

orders

payment

shipping

login

registration

user

ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ
ğœ‡
ğœ

Response time (ms)

NEPT
66.7
3.4
110.6
7.6
57.4
3.0
53.3
2.7
211.6
12.1
10.4
0.7
15
1.1
30.3
1.5
46.4
2.7
21.8
0.7

K3S
64.6
10.9
175.9
64.2
95.4
31.0
54.6
5.2
418.9
86.7
50.2
9.8
75
23.2
72.5
12.3
57.7
6.3
66.4
6.4

KN
60.6
1.6
73.8
2.0
54.6
1.2
163.1
35.4
505.1
165.7
27.9
0.4
28.6
0.6
73.2
12.1
65
4.4
177
35.1

OF
100.3
27.3
184.3
73.7
45.6
1.8
39.2
1.4
485.2
126.4
23.6
1.3
88
32.8
46
0.7
34.9
1.3
93.4
20.3

Response time violation (%)
NEPT K3S
0.3
0.0
3.5
2.7
1.7
1.4
0.1
~0
16.6
8.2
2.8
0.4
5.9
1.6
2.8
0.7
0.1
0.1
7.8
0.4

KN
0
0
0.1
0.1
0
0
17.7
2.1
16.5
3.0
1.2
0.7
1.5
0.6
11.1
7.0
2.6
1.4
46.7
24.2

OF
2
1.3
3.4
2.6
0.1
~0
0
0
16
9.0
0.4
~0
8.5
3.5
0.2
~0
0
0
16.8
5.3

0.1
0.1
0.1
0.1
0
0.1
0
0
0
0
0
0
2.6
1.1
0
0
0
0
0.5
0.5

Network time rate (%)
OF
72.9
18.2
69
26.4
70.3
2.2
71.2
1.6
25.2
8.2
98.9
3.9
92.7
20.3
63.2
1.1
81.7
1.8
76.8
16.3

NEPT K3S
63.9
16.5
68
22.4
78.5
19.6
74
11.9
15.8
7.5
98.7
9.5
96.2
16.9
70.1
14.4
80.9
10.4
77.9
10.1

KN
92.3
1.2
78.3
0.9
92.8
1.0
41.9
6.3
44.2
25.0
98.4
1.3
95.6
1.7
77.9
1.2
87.9
1.6
31.5
5.9

3.5
2.1
3.7
2.9
2.6
1.2
1.6
0.6
4.1
1.7
8.2
6.1
6.4
2.5
2.6
0.9
1.4
0.4
7.1
1.0

Core allocation (millicores)
OF
597.5
2.6
597.3
2.1
4306.1
180.3
458
3.1
597.8
1.2
443.1
4.3
596.9
0.9
452.2
6.6
453.4
6.2
463.2
1.9

K3S
1921.1
429.6
615.5
31.3
689.3
162.2
197.6
23.9
4484.5
407.2
101.8
13.2
888.5
202.8
94.2
15.6
105.3
12.0
681.8
91.0

KN
596
2.1
597.4
2.6
596.5
2.2
65.2
13.0
1040.7
294.5
49.7
0.2
597.4
1.0
54.1
6.1
53.6
6.1
355.3
166.8

NEPT
631.3
149.9
722.8
178.9
516.3
83.2
102.7
4.1
1114.8
273.0
795
438.9
416.5
132.3
76.7
13.9
71.6
9.8
153.2
23.3

functions with no dependencies, to 600ms, assigned to the more
complex ones.

Each run had a duration of 20 minutes and used a workload
that resembles a steep ramp with an arrival rate ğœ†ğ‘“ ,ğ‘– designed to
suddenly increase over a short period of time. The workload started
with 10 concurrent users, and we added one additional user every
second up to 100. We considered a network of 6 nodes in Area B
and C.

Table 3 reports the statistical results obtained during the ex-
periments with each approach and with function of application
sock-shop. The results show that NEPTUNE provided in most of the
cases the lowest response time compared to the other approaches.
The obtained response times were consistent across multiple runs:
the standard deviation ranged between 3% and 7% of the average.
Other approaches presented higher standard deviation values: in
the worst case, KN obtained a standard deviation equal to 32.8%
of the average, while K3S (36.5%) and OF (40%) were even more
inconsistent.

NEPTUNE reported few violations of the required response time.
For most functions the amount of violations was lower or equal to
0.1%, while it was 2.6% and 0.5% for functions shopping and user,
respectively. Other solutions obtained significantly higher viola-
tions. In the worst case, K3S failed to meet the foreseen response
time 16.6% of the requests, while OF and KN reported violations
for 16.9% and 46.8%, respectively. This can be explained because
other approaches, compared to NEPTUNE, do not employ precise
routing policies, do not perform an adequate resource allocation,
and do not solve resource contentions on nodes.

We can also observe how NEPTUNE routing policies helped
meet set response times. The percentage of time spent by routing
requests ranges from 1.4% to 8.2% of the total response time, and,
on average, only 4.1% of the time is spent in the network. On the
other hand, routing policies of other solutions do not consider

node utilization, network delay, and applications performance. K3S
reported a network time rate ranging from 15.8% to 98.7% of the
response time, with an overall average of 72.4%. Similarly, OF and
KN obtained an average network time rate of 72.2% and 74.1%,
respectively.

Finally, as for the resources allocated by each approach for each
function, NEPTUNE allocated on average 4600 millicores, while
K3S and OF used about twice that amount, 9780 and 8960 mil-
licores, respectively. KN uses fewer resources than NEPTUNE on
average (4500 millicores) but it also suffers from a high number
of response time violations. This means that KN usually allocates
fewer resources than needed (e.g., for function catalogue).

Differently from NEPTUNE, other solutions do not adopt any
resource contention mechanism to provide a fair allocation of re-
sources. For example, K3S allocated most of the resources, 4480
millicores, to function orders, while other functions could not get
the resources to work properly. This creates an imbalance among
functions that prevents applications to be properly scaled and leads
to more response time violations.

4.4 RQ3: GPU Management
The third set of experiments was carried out to assess the transpar-
ent GPU management provided by NEPTUNE for computationally
intensive functions. To provide a heterogeneous environment, ex-
periments were conducted using the three nodes in Area A (Node-
A-2 is equipped with a GPU).

We used two functions, called resnet-a and resnet-b, both embed
the ResNet neural network in inference mode. The instances of the
two functions deployed on Node-A-2 were set to share the same
GPU.

Each run lasted 20 minutes and used the same workload de-
scribed in Section 4.3 with a number of concurrent users starting
from 10 and up to 30 (increased by one every second).

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

Nevertheless, the CPU-only replicas of resnet-a and resnet-b
can serve 98.3% and 100% of requests within the set response time,
respectively. Moreover, GPU instances handle 70% of requests while
the remaining part was routed to CPU instances. As a result, the
total number of violations of both functions is close to 0.

4.5 Threats to validity
We conducted the experiments using twelve functions (three appli-
cations) showing that NEPTUNE is able to minimize the network
delay, to reduce response times, and to efficiently allocate resources
compared to other three well-known approaches. However, we
must highlight threats that may constrain the validity of obtained
results [46]:

Internal Threats. The experiments were run with synthetic
workloads that may introduce bias. Workloads have a ramp-shape
to simulate an incremental growth or reduction of connected users.
We used the following procedure to retrieve the maximum con-
current users in each experiment. First, we fixed the amount and
types of nodes the topology was composed of. The maximum con-
current users of each experiment was retrieved by observing how
many users were required to generate enough workload to require
consistently at least 70% of the clusterâ€™s resources.

The three applications were not provided with a given required
response time for each function (ğ‘…ğ‘‡ ğ‘…
was computed using
ğ‘“
an iterative process. Starting from 50ğ‘šğ‘  and with 50ğ‘šğ‘  increments,
ğ‘…ğ‘‡ ğ‘…
was set to be able to serve at least 50% of requests in an amount
ğ‘“
of time equal to ğ‘…ğ‘‡ ğ‘…

). ğ‘…ğ‘‡ ğ‘…
ğ‘“

ğ‘“ /2.

External Threats. Some of our assumptions may limit the gen-

eralization of the experiments.

Consistently with the serverless paradigm, NEPTUNE assumes
functions to either be stateless (e.g., without session) or depend
on an external database. Currently, interactions with databases are
only partially modeled by NEPTUNE. The time to read from and
write on a database is modeled at the Node level as non-controllable
stationary disturbance of the response time (e.g., a Gaussian noise).
Thus, during our experiments, databases were deployed on dedi-
cated and properly sized machines.

Results show that NEPTUNE is able to efficiently control func-
tions that depend on a database (e.g., orders, carts-post) with a
precision similar to the ones without dependencies (e.g., payment,
user).

Construct and Conclusion Threats. The experiments demon-
strate the validity of our claim, that is, that NEPTUNE is able to
efficiently execute multiple functions deployed on a distributed
edge topology. All experiments have been executed five times and
obtained results are statistically robust and show small variance.

5 RELATED WORK
The management of edge topologies is a hot and widely addressed
topic by both industry and academia [7, 18]. To the best of our
knowledge, NEPTUNE is the first solution that provides: an easy
to use serverless interface, optimal function placement and rout-
ing policies, in-place vertical scaling of functions, and transparent
management of GPUs and CPUs. The relevant related works we
are aware of only focus on specific aspects of the problem.

(a) Response time.

(b) Distributions.

Figure 5: Resnet-a: CPU and GPU executions.

Figure 5 illustrates one run of the experiments. Figure 5a shows
the average response time of function resnet-a when executed on
both CPUs and GPUs. Function resnet-b obtained similar results
that are not reported here for lack of space. GPU executions ob-
tained an almost constant response time and never violated the set
response time.

At the beginning of the experiment, all the requests were routed
to the GPU and after some 50 seconds the GPU was fully utilized.
To avoid degradation of the response time, the Community level
quickly reacted by updating the routing policies and allowing part
of the workload to be handled by function instances running on
CPUs. The mean response time of CPU instances shows a peak
at the beginning of the experiment (with some brief violations of
the response time) that is caused by the cold start. After that, the
Node level comes into play and dynamically adjusts the CPU cores
allocated to the replicas to keep the response time close to the set
point.

The box plot of Figure 5b shows the distribution of response
times for both functions resnet-a and resnet-b on GPU, on CPU,
and the aggregated result. The interquartile range (IQR) is set to 1.5,
and the rectangle shows the distribution between the 25th and 75th
percentiles. Both GPU instances of resnet-a and resnet-b are able to
keep response times quite far from the set threshold, and thus no
violations. In particular, the mean response times of resnet-a and
resnet-b are 180ms and 183ms, respectively, which is three times
smaller than the threshold.

The distribution of response times on CPUs is wider compared
to GPUs. CPU containers are managed by PI controllers that have a
transient period to adjust the initial core allocation to an adequate
value to reach the desired set point; this does not happen with GPU
instances.

020040060080010001200Time (s)0200400600800Time (ms)RTRfQEf,desiredGPU avg RTCPU avg RTresnet-aGPUresnet-bGPUresnet-aCPUresnet-bCPUresnet-aTotalresnet-bTotal0200400600Time (ms)RTRfNEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Wang et al. [44] propose LaSS, a framework for latency-sensitive
edge computations built on top of Kubernetes and Openwhisk16.
LaSS models the problem of resource allocation using a M/M/c
FCFS queuing model. They provide a fair-share resource alloca-
tion algorithm, similar to NEPTUNEâ€™s Contention Manager, and
two reclamation policies for freeing allocated resources. LaSS is
the most similar solution to NEPTUNE, but it lacks network over-
head minimization and GPU support. Furthermore, the approach
is not fully compatible with the Kubernetes API. Kubernetes is
only used to deploy OpenWhisk. Functions run natively on top of
the container runtime (e.g., Docker 17) and resources are vertically
scaled by bypassing Kubernetes. This approach, also adopted in
cloud computing solutions [6, 34], is known to create state repre-
sentation inconsistencies between the container runtime and the
orchestrator [3].

Ascigir et al. [1] investigate the problem for serverless functions
in hybrid edge-cloud systems and formulate the problem using
Mixed Integer Programming. They propose both fully-centralized
(function orchestration) approaches, where a single controller is
in charge of allocating resources, and fully-decentralized (function
choreography) ones, where controllers are distributed across the
network and decisions are made independently. Compared to NEP-
TUNE, they focus on minimizing the number of unserved requests
and they assume that each request can be served in a fixed amount
of time (single time-slot). However, this assumption is not easy to
ensure in edge computing: nodes may be equipped with different
types of hardware and produce different response times. This is
naturally considered in NEPTUNE with the help of GPUs.

Multiple approaches in the literature focus on placement and
routing at the edge [8, 16, 32]. One of the most used techniques,
also employed by NEPTUNE, is to model the service placement and
workload routing as an Integer or a Mixed Integer Programming
problem.

Notably, Tong et al. [42] model a MEC network as a hierarchical
tree of geo-distributed servers and formulate the problem as a two-
steps Mixed Nonlinear Integer Programming (MNIP). In particular,
their approach aims to maximize the amount of served requests by
means of optimal service placement and resource allocation. The
effectiveness of their approach is verified using formal analysis and
large-scale trace-based simulations. They assume that workloads
follow some known stochastic models (Poisson distribution), and
that arrival rates are independent and identically distributed. This
may not be true in the context of edge computing where workloads
are often unpredictable and may significantly deviate from the
assumed distribution. NEPTUNE does not share these assumptions
and uses fast control-theoretical planners to mitigate volatility and
unpredictability in the short term.

To cope with dynamic workloads, Tan et al. [41] propose an
online algorithm for workload dispatching and scheduling with-
out any assumption about the distribution. However, since their
approach only focuses on routing requests, they cannot always
minimize network delays, especially when edge clients move from
one location to another.

16https://openwhisk.apache.org/
17https://www.docker.com

Mobile workloads are addressed, for example, by Leyva-Pupo
et al. [22], who present a solution based on an Integer Linear Pro-
gramming (ILP) problem with two different objective functions one
for mobile users and one for static ones. Furthermore, since the
problem is known to be NP-hard, they use heuristic methods to
compute a sub-optimal solution. Sun et al. [40] propose a service
migration solution based on Mixed Integer Programming to keep
the computation as close as possible to the user. In particular, they
consider different factors that contribute to migrations costs (e.g.,
required time and resources). However, the two aforementioned
solutions exploit virtual machines and they are known for their
large image sizes and long start-up times, making service migra-
tion a very costly operation. NEPTUNE, as other approaches in the
literature[29, 44, 50], uses containers that are lighter and faster to
scale.

Only a few solutions have been designed for GPU management
in the context of edge computing. For example, Subedi et al. [39]
mainly focuses on enabling GPU accelerated edge computation
without considering latency-critical aspects such as placing appli-
cations close to the edge clients.

6 CONCLUSIONS AND FUTURE WORK
The paper proposes NEPTUNE, a serverless-based solution for man-
aging latency-sensitive applications deployed on geo-distributed
large scale edge topologies. It provides smart placement and rout-
ing to minimize network overhead, dynamic resource allocation to
quickly react to workload changes, and transparent management
of CPUs and GPUs. A prototype built on top of K3S, a popular
container orchestrator for the edge, helped us demonstrate the
feasibility of the approach and interesting results with respect to
similar state-of-the-art solutions.

Our future work comprises the improvement of adopted sched-
uling and resource allocation solutions by exploiting function de-
pendencies [26] and workload predictors to anticipate future de-
mand [21]. As a further extension, we will consider Bayesian op-
timization approaches [13, 38] to find optimal response times au-
tomatically. State migration and data consistency approaches can
also be integrated to manage stateful applications.

7 ACKNOWLEDGEMENTS
This work has been partially supported by the SISMA national
research project (MIUR, PRIN 2017, Contract 201752ğ¸ğ‘ğ‘Œ ğµ).

REFERENCES
[1] Onur Ascigil, Argyrios Tasiopoulos, Truong Khoa Phan, Vasilis Sourlas, Ioan-
nis Psaras, and George Pavlou. 2021. Resource Provisioning and Allocation in
Function-as-a-Service Edge-Clouds (Early Access). IEEE Transactions on Services
Computing (2021), 1â€“14.

[2] David Balla, Csaba Simon, and Markosz Maliosz. 2020. Adaptive scaling of Kuber-
netes pods. In Proceedings of the IEEE/IFIP Network Operations and Management
Symposium, NOMS 2020. IEEE, 1â€“5.

[3] Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano.
2021. KOSMOS: Vertical and Horizontal Resource Autoscaling for Kubernetes. In
Proceedings of the 19th International Conference on Service-Oriented Computing,
ICSOC 2021 (Lecture Notes in Computer Science, Vol. 13121). Springer, 821â€“829.
[4] Luciano Baresi, Danilo Filgueira MendonÃ§a, and Giovanni Quattrocchi. 2019.
PAPS: A Framework for Decentralized Self-management at the Edge. In Proceed-
ings of the 17th International Conference on Service-Oriented Computing, ICSOC
2019 (Lecture Notes in Computer Science, Vol. 11895). Springer, 508â€“522.

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

Luciano Baresi, Davide Yi Xian Hu, Giovanni Quattrocchi, and Luca Terracciano

[5] Luciano Baresi and Giovanni Quattrocchi. 2018. Towards Vertically Scalable
Spark Applications. In Euro-Par 2018: Parallel Processing Workshops - Euro-Par
2018 International Workshops, Turin, Italy, August 27-28, 2018, Revised Selected
Papers (Lecture Notes in Computer Science, Vol. 11339). Springer, 106â€“118.
[6] Luciano Baresi and Giovanni Quattrocchi. 2020. COCOS: A Scalable Architecture
for Containerized Heterogeneous Systems. In Proceedings of the IEEE International
Conference on Software Architecture, ICSA 2020. IEEE, 103â€“113.

[7] Julian Bellendorf and ZoltÃ¡n ÃdÃ¡m Mann. 2020. Classification of optimization
problems in fog computing. Future Gener. Comput. Syst. 107 (2020), 158â€“176.
[8] David Bermbach, Jonathan Bader, Jonathan Hasenburg, Tobias Pfandzelter, and
Lauritz Thamsen. 2021. AuctionWhisk: Using an Auction-Inspired Approach for
Function Placement in Serverless Fog Platforms (Early Access). Software: Practice
and Experience (2021), 1â€“49.

[9] Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure, David
VÃ¡zquez, and Antonio M. LÃ³pez. 2016. GPU-based Pedestrian Detection for
Autonomous Driving. In Proceedings of the International Conference on Compu-
tational Science 2016, ICCS 2016 (Procedia Computer Science, Vol. 80). Elsevier,
2377â€“2381.

[10] Junguk Cho, Karthikeyan Sundaresan, Rajesh Mahindra, Jacobus E. van der
Merwe, and Sampath Rangarajan. 2016. ACACIA: Context-aware Edge Comput-
ing for Continuous Interactive Applications over Mobile Networks. In Proceedings
of the 12th International on Conference on emerging Networking EXperiments and
Technologies, CoNEXT 2016. ACM, 375â€“389.

[11] Thomas Heide Clausen and Philippe Jacquet. 2003. Optimized Link State Routing

Protocol (OLSR). RFC 3626 (2003), 1â€“75.

[12] Xavier Dutreilh, Nicolas Rivierre, AurÃ©lien Moreau, Jacques Malenfant, and Isis
Truck. 2010. From Data Center Resource Allocation to Control Theory and Back.
In Proceedings of the IEEE International Conference on Cloud Computing, CLOUD
2010. IEEE, 410â€“417.

[13] NicolÃ² Felicioni, Andrea Donati, Luca Conterio, Luca Bartoccioni, Davide Yi Xian
Hu, Cesare Bernardis, and Maurizio Ferrari Dacrema. 2020. Multi-Objective
Blended Ensemble For Highly Imbalanced Sequence Aware Tweet Engagement
Prediction. In Proceedings of the Recommender Systems Challenge 2020, RecSys
Challenge 2020. ACM, 29â€“33.

[14] Wes Felter, Alexandre Ferreira, Ram Rajamony, and Juan Rubio. 2015. An updated
performance comparison of virtual machines and Linux containers. In IEEE
International Symposium on Performance Analysis of Systems and Software, ISPASS
2015. IEEE, 171â€“172.

[15] Domenico Grimaldi, Valerio Persico, Antonio PescapÃ¨, Alessandro Salvi, and
Stefania Santini. 2015. A Feedback-Control Approach for Resource Management
in Public Clouds. In Proceedings of the IEEE Global Communications Conference
2015, GLOBECOM 2015. IEEE, 1â€“7.

[16] Songtao Guo, Bin Xiao, Yuanyuan Yang, and Yang Yang. 2016. Energy-efficient
dynamic offloading and resource scheduling in mobile cloud computing. In Pro-
ceedings of the 35th Annual IEEE International Conference on Computer Communi-
cations, INFOCOM 2016. IEEE, 1â€“9.

[17] Akhil Gupta and Rakesh Kumar Jha. 2015. A Survey of 5G Network: Architecture

and Emerging Technologies. IEEE Access 3 (2015), 1206â€“1232.

[18] Congfeng Jiang, Xiaolan Cheng, Honghao Gao, Xin Zhou, and Jian Wan. 2019.
Toward Computation Offloading in Edge Computing: A Survey. IEEE Access 7
(2019), 131543â€“131558.

[19] Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-che Tsai, Anurag Khan-
delwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Jayant
Yadwadkar, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica, and David A. Pat-
terson. 2019. Cloud Programming Simplified: A Berkeley View on Serverless
Computing. CoRR abs/1902.03383 (2019), 1â€“35.

[20] Patrick Kalmbach, Andreas Blenk, Wolfgang Kellerer, Rastin Pries, Michael
Jarschel, and Marco Hoffmann. 2019. GPU Accelerated Planning and Place-
ment of Edge Clouds. In Proceedings of the International Conference on Networked
Systems 2019, NetSys 2019. IEEE, 1â€“3.

[21] Jitendra Kumar and Ashutosh Kumar Singh. 2018. Workload prediction in cloud
using artificial neural network and adaptive differential evolution. Future Gener.
Comput. Syst. 81 (2018), 41â€“52.

[22] Irian Leyva-Pupo, Alejandro Santoyo-GonzÃ¡lez, and Cristina CervellÃ³-Pastor.
2019. A Framework for the Joint Placement of Edge Service Infrastructure and
User Plane Functions for 5G. Sensors 19, 18 (2019), 3975.

[23] Ang Li, Xiaowei Yang, Srikanth Kandula, and Ming Zhang. 2010. CloudCmp:
comparing public cloud providers. In Proceedings of the 10th ACM SIGCOMM
Internet Measurement Conference, IMC 2010. 1â€“14.

[24] Ping-Min Lin and Alex Glikson. 2019. Mitigating Cold Starts in Serverless

Platforms: A Pool-Based Approach. CoRR abs/1903.12221 (2019), 1â€“5.

[25] Shih-Chieh Lin, Yunqi Zhang, Chang-Hong Hsu, Matt Skach, Md. Enamul Haque,
Lingjia Tang, and Jason Mars. 2018. The Architectural Implications of Au-
tonomous Driving: Constraints and Acceleration. In Proceedings of the 23rd In-
ternational Conference on Architectural Support for Programming Languages and
Operating Systems, ASPLOS 2018. ACM, 751â€“766.

[26] Wei-Tsung Lin, Chandra Krintz, and Rich Wolski. 2018. Tracing Function Depen-
dencies across Clouds. In Proceedings of the 11th IEEE International Conference on
Cloud Computing, CLOUD 20188. IEEE, 253â€“260.

[27] Chunhong Liu, Chuanchang Liu, Yanlei Shang, Shiping Chen, Bo Cheng, and
Junliang Chen. 2017. An adaptive prediction approach based on workload pattern
discrimination in the cloud. J. Netw. Comput. Appl. 80 (2017), 35â€“44.

[28] Shaoshan Liu, Liangkai Liu, Jie Tang, Bo Yu, Yifan Wang, and Weisong Shi.
2019. Edge Computing for Autonomous Driving: Opportunities and Challenges.
Proceedings of the IEEE 107, 8 (2019), 1697â€“1716.

[29] Omogbai Oleghe. 2021. Container Placement and Migration in Edge Computing:

Concept and Scheduling Models. IEEE Access 9 (2021), 68028â€“68043.

[30] Quoc-Viet Pham, Fang Fang, Vu Nguyen Ha, Md. Jalil Piran, Mai Le, Long Bao
Le, Won-Joo Hwang, and Zhiguo Ding. 2020. A Survey of Multi-Access Edge
Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-
of-the-Art. IEEE Access 8 (2020), 116974â€“117017.

[31] Jon Postel. 1981. Internet Control Message Protocol. RFC 777 (1981), 1â€“14.
[32] Konstantinos Poularakis, Jaime Llorca, Antonia Maria Tulino, Ian J. Taylor, and
Leandros Tassiulas. 2019. Joint Service Placement and Request Routing in Multi-
cell Mobile Edge Computing Networks. In Proceedings of the IEEE Conference on
Computer Communications, INFOCOM 2019. IEEE, 10â€“18.

[33] Peter-Christian Quint and Nane Kratzke. 2018. Towards a Lightweight Multi-
Cloud DSL for Elastic and Transferable Cloud-native Applications. In Proceedings
of the 8th International Conference on Cloud Computing and Services Science,
CLOSER 2018. SciTePress, 400â€“408.

[34] Gourav Rattihalli, Madhusudhan Govindaraju, Hui Lu, and Devesh Tiwari. 2019.
Exploring Potential for Non-Disruptive Vertical Auto Scaling and Resource Esti-
mation in Kubernetes. In Proceedings of the 12th IEEE International Conference on
Cloud Computing, CLOUD 2019. IEEE, 33â€“40.

[35] Krzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw Zych, Przemys-
law Broniek, Jarek Kusmierek, Pawel Nowak, Beata Strack, Piotr Witusowski,
Steven Hand, and John Wilkes. 2020. Autopilot: workload autoscaling at Google.
In Proceedings of the 15th EuroSys Conference 2020, EuroSys 2020, Heraklion, Greece,
April 27-30, 2020. ACM, 16:1â€“16:16.

[36] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmark-
ing State-of-the-Art Deep Learning Software Tools. In Proceedings of the 7th
International Conference on Cloud Computing and Big Data, CCBD 2016. IEEE,
99â€“104.

[37] Paulo Silva, Daniel Fireman, and Thiago Emmanuel Pereira. 2020. Prebaking Func-
tions to Warm the Serverless Cold Start. In Proceedings of the 21st International
Middleware Conference, Middleware 2020. ACM, 1â€“13.

[38] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian
Optimization of Machine Learning Algorithms. In Proceedings of the 26th Annual
Conference on Neural Information Processing Systems. NIPS 2012. 2960â€“2968.
[39] Piyush Subedi, Jianwei Hao, In Kee Kim, and Lakshmish Ramaswamy. 2021.
AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and
Dynamic Model Placements on Edge Devices. In Proceedings of the 14th IEEE
International Conference on Cloud Computing, CLOUD 2021. IEEE, 31â€“42.
[40] Xiang Sun and Nirwan Ansari. 2016. PRIMAL: PRofIt Maximization Avatar
pLacement for mobile edge computing. In Proceedings of the IEEE International
Conference on Communications 2016, ICC 2016. IEEE, 1â€“6.

[41] Haisheng Tan, Zhenhua Han, Xiang-Yang Li, and Francis C. M. Lau. 2017. On-
line job dispatching and scheduling in edge-clouds. In Proceedings of the IEEE
Conference on Computer Communications 2017, INFOCOM 2017. IEEE, 1â€“9.
[42] Liang Tong, Yong Li, and Wei Gao. 2016. A hierarchical edge cloud architecture for
mobile computing. In Proceedings of the 35th Annual IEEE International Conference
on Computer Communications, INFOCOM 2016. IEEE, 1â€“9.

[43] Abhishek Verma, Hussam Qassim, and David Feinzimer. 2017. Residual squeeze
CNDS deep learning CNN model for very large scale places image recognition. In
Proceedings of the 8th IEEE Annual Ubiquitous Computing, Electronics and Mobile
Communication Conference, UEMCON 2017, 2017. IEEE, 463â€“469.

[44] Bin Wang, Ahmed Ali-Eldin, and Prashant J. Shenoy. 2021. LaSS: Running
Latency Sensitive Serverless Computations at the Edge. In Proceedings of the 30th
International Symposium on High-Performance Parallel and Distributed Computing,
HPDC 2021. ACM, 239â€“251.

[45] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael M.
Swift. 2018. Peeking Behind the Curtains of Serverless Platforms. In Proceedings of
the USENIX Annual Technical Conference, USENIX ATC 2018. USENIX Association,
133â€“146.

[46] Claes Wohlin, Martin HÃ¶st, and Kennet Henningsson. 2006. Empirical Research
Methods in Web and Software Engineering. In Web Engineering. Springer, 409â€“
430.

[47] Jierui Xie, Boleslaw K. Szymanski, and Xiaoming Liu. 2011. SLPA: Uncovering
Overlapping Communities in Social Networks via a Speaker-Listener Interaction
Dynamic Process. In Proceedings of the IEEE 11th International Conference on Data
Mining Workshops, (ICDMW) 2011. IEEE, 344â€“349.

[48] Lenar Yazdanov and Christof Fetzer. 2014. Lightweight Automatic Resource
Scaling for Multi-tier Web Applications. In Proceedings of the 7th International
Conference on Cloud Computing, CLOUD 2014. IEEE, 466â€“473.

NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge

SEAMS â€™22, May 18â€“23, 2022, PITTSBURGH, PA, USA

[49] Xu Zhang, Hao Chen, Yangchao Zhao, Zhan Ma, Yiling Xu, Haojun Huang, Hao
Yin, and Dapeng Oliver Wu. 2019. Improving Cloud Gaming Experience through
Mobile Edge Computing. IEEE Wirel. Commun. 26, 4 (2019), 178â€“183.

[50] Ao Zhou, Shangguang Wang, Shaohua Wan, and Lianyong Qi. 2020. LMM:
latency-aware micro-service mashup in mobile edge computing environment.
Neural Comput. Appl. 32, 19 (2020), 15411â€“15425.

[51] Qian Zhu and Gagan Agrawal. 2012. Resource Provisioning with Budget Con-
straints for Adaptive Applications in Cloud Environments. IEEE IEEE Transactions
on Services Computing 5, 4 (2012), 497â€“511.

