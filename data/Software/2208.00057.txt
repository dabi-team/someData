ARGONNE NATIONAL LABORATORY
9700 South Cass Avenue
Argonne, Illinois 60439

Compact Representations of Structured BFGS Matrices

J. J. Brust, W. Di, S. Leyﬀer, and C. G. Petra

Mathematics and Computer Science Division

Preprint ANL/MCS-P9279-0120

August 2020

1 This work was supported by the U.S. Department of Energy, Oﬃce of Science, Advanced
Scientiﬁc Computing Research, under Contract DE-AC02-06CH11357 at Argonne National
Laboratory. through the Project ”Multifaceted Mathematics for Complex Energy Systems.”
This work was also performed under the auspices of the U.S. Department of Energy by
Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.

2
2
0
2

l
u
J

9
2

]

C
O
.
h
t
a
m

[

1
v
7
5
0
0
0
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
The submitted manuscript has been created by UChicago Ar-
gonne, LLC, Operator of Argonne National Laboratory (“Ar-
gonne”). Argonne, a U.S. Department of Energy Oﬃce of Science
laboratory, is operated under Contract No. DE-AC02-06CH11357.
The U.S. Government retains for itself, and others acting on
its behalf, a paid-up nonexclusive, irrevocable worldwide license
in said article to reproduce, prepare derivative works, distribute
copies to the public, and perform publicly and display publicly,
by or on behalf of the Government. The Department of En-
ergy will provide public access to these results of federally spon-
sored research in accordance with the DOE Public Access Plan.
http://energy.gov/downloads/doe-public-accessplan

COAP manuscript No.
(will be inserted by the editor)

Compact Representations of Structured BFGS
Matrices

Johannes J. Brust · Zichao (Wendy) Di ·
Sven Leyﬀer · Cosmin G. Petra

Received: date / Accepted: date

Abstract For general large-scale optimization problems compact representa-
tions exist in which recursive quasi-Newton update formulas are represented
as compact matrix factorizations. For problems in which the objective func-
tion contains additional structure, so-called structured quasi-Newton methods
exploit available second-derivative information and approximate unavailable
second derivatives. This article develops the compact representations of two
structured Broyden-Fletcher-Goldfarb-Shanno update formulas. The compact
representations enable eﬃcient limited memory and initialization strategies.
Two limited memory line search algorithms are described and tested on a
collection of problems, including a real world large scale imaging application.

Keywords Quasi-Newton method · limited memory method · large-scale
optimization · compact representation · BFGS method

Submitted to the editors DATE. This material was based upon work supported by the U.S.
Department of Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing Research
(ASCR) under Contract DE-AC02-06CH11347.

J.J. Brust
Argonne National Laboratory, Lemont, IL
E-mail: jbrust@anl.gov

W. Di
Argonne National Laboratory, Lemont, IL
E-mail: wendydi@mcs.anl.gov

S. Leyﬀer
Argonne National Laboratory, Lemont, IL
E-mail: leyﬀer@mcs.anl.gov

C. G. Petra
Lawrence Livermore National Laboratory, Livermore, CA
E-mail: petra1@llnl.gov

1 Introduction

The unconstrained minimization problem is

minimize
∈Rn

),

f (x
¯

(1)

x
¯

where f : Rn → R is assumed to be twice continuously diﬀerentiable. If the
) ∈ Rn×n is unavailable, because it is unknown or diﬃ-
Hessian matrix ∇2f (x
¯
cult to compute, then quasi-Newton approaches are eﬀective methods, which
approximate properties of the Hessian at each iteration, ∇2f (xk+1) ≈ Bk+1
[8]. Arguably, the most widely used quasi-Newton matrix is the Broyden-
Fletcher-Goldfarb-Shanno (BFGS) matrix [4, 12,15,22], because of its desirable
results on many problems. Given sk ≡ xk+1 −xk and yk ≡ ∇f (xk+1)−∇f (xk)
the BFGS recursive update formula is

Bk+1 = Bk −

1
sT
k Bksk

BksksT

k Bk +

1
sT
k yk

ykyT
k .

(2)

For a symmetric positive deﬁnite initialization B0 ∈ Rn×n (2) generates sym-
metric positive deﬁnite matrices as long as sT
k yk > 0 for all k ≥ 0 (see [12,
Section 2]).

1.1 BFGS Compact Representation

Byrd et al. [6] propose the compact representation of the recursive formula
(2). The compact representation has been successfully used for large-scale
unconstrained and constrained optimization [28]. Let the sequence of pairs
{si, yi}k−1
i=0 be given, and let these vectors be collected in the matrices Sk =
· · · , yk−1] ∈ Rn×k. Moreover,
· · · ,
[s0,
k Yk = Lk + Rk, where Lk ∈ Rk×k is the strictly lower triangular ma-
let ST
trix, Rk ∈ Rk×k is the upper triangular matrix (including the diagonal), and
k−1yk−1) ∈ Rk×k is the diagonal part of ST
Dk = diag(sT
k Yk. The
compact representation of the BFGS formula (2) is [6, Theorem 2.3]:

sk−1] ∈ Rn×k and Yk = [y0,

0 y0, · · · , sT

Bk = B0 − [ B0Sk Yk ]

(cid:20)ST

k B0Sk Lk
LT
−Dk
k

(cid:21)−1 (cid:20)ST

k B0
YT
k

(cid:21)

.

(3)

For large optimization problems limited memory versions of the compact rep-
resentation in (3) are used. The limited memory versions typically store only
the last m > 0 pairs {si, yi}k−1
i=k−m when k ≥ m. In limited memory BFGS
(L-BFGS) the dimensions of Sk and Yk are consequently n × m. Usually the
memory parameter is much smaller than the problem size, namely, m (cid:28) n.
A typical range for this parameter is 5 ≤ m ≤ 50 (see Boggs and Byrd in
[2]). Moreover, in line search L-BFGS methods the initialization is frequently
chosen as B0 = ˆσkIn, where ˆσk = yT
k−1yk−1. Such an initializa-
tion enables eﬃcient computations with the formula in (3), and adds extra
information through the parameter ˆσk, which depends on the iteration k.

k−1yk−1/sT

2

1.2 Structured Problems

When additional information about the structure of the objective function is
known, it is desirable to include this information in a quasi-Newton update.
Initial research eﬀorts on structured quasi-Newton methods were in the context
of nonlinear least squares problems. These include the work of Gill and Murray
[14], Dennis et al. [9,10], and Yabe and Takahashi [27]. Recently, Petra et al.
[20] formulated the general structured minimization problem as

minimize
∈Rn

x
¯

),

f (x
¯

f (x
¯

) = (cid:98)k(x
¯

) + (cid:98)u(x
¯

),

(4)

where (cid:98)k : Rn → R has known gradients and known Hessians and (cid:98)u : Rn → R
has known gradients but unknown Hessians. For instance, objective func-
tions composed of a general nonlinear function plus a regularizer or penalty
term are described with (4). Thus, applications such as regularized logistic
regressions [25] or optimal control problems contain structure that may be
exploited, when we assume that the Hessian of the regularizer is known.
We note that nonlinear least squares problems typically do not have the
form as in (4), yet available second derivatives may also be used for this
class of problems after reformulating the quasi-Newton vectors. We will de-
scribe an image reconstruction application in the numerical experiments, Sec-
tion 4. Even though approximating the Hessian of the objective function
in (4) by formula (2) or (3) is possible, this would not exploit the known
parts of the Hessian. Therefore in [20] structured BFGS (S-BFGS) updates
are derived, which combine known Hessian information with BFGS approx-
imations for the unknown Hessian components. At each iteration the Hes-
sian of the objective is approximated as ∇2f (xk+1) ≈ ∇2(cid:98)k(xk+1) + Ak+1,
where Ak+1 approximates the unknown Hessian, that is, Ak+1 ≈ ∇2
(cid:98)u(xk+1).
Given the known Hessian ∇2(cid:98)k(xk+1) ≡ Kk+1 and the gradients of (cid:98)u, let
uk ≡ Kk+1sk + (∇(cid:98)u(xk+1) − ∇(cid:98)u(xk)). One of two structured approximations
from [20] is the structured BFGS-Minus (S-BFGS-M) update

AM

k+1 = BM

k − Kk+1 −

1
k BM
sT
k sk

BM

k sksT

k BM

k +

1
sT
k uk

ukuT
k ,

(5)

k + Kk. By adding Kk+1 to both sides, the update from (5)
k+1 that resembles (2), in which Bk+1, Bk, and yk are
k+1 is symmetric
M
0 as long
k uk > 0 for k ≥ 0. A second formula is the structured BFGS-Plus (S-

where BM
k = AM
implies a formula for BM
replaced by BM
positive deﬁnite given a symmetric positive deﬁnite initialization B
¯
as sT
BFGS-P) update

k , and uk, respectively. Consequently, BM

k+1, BM

AP

k+1 = AP

k −

1
k (cid:98)BP
sT
k sk

(cid:98)BP

k sksT

k (cid:98)BP

k +

1
sT
k uk

ukuT
k ,

(6)

where (cid:98)BP
hand side (BP

k = AP

k + Kk+1. After adding Kk+1 to both sides in (6), the left
k is positive deﬁnite

k+1 + Kk+1) is positive deﬁnite if (cid:98)BP

k+1 = AP

3

k uk > 0. However, in general, (cid:98)BP

and sT
k does not have to be positive deﬁnite,
because the known Hessian, Kk+1, may not be positive deﬁnite. Similar to (2)
the update in (6) is also rank 2. Both of the updates in eqs. (5) and (6) were
implemented in a line search algorithm and compared with the BFGS formula
(2) in [20]. The structured updates obtained better results in terms of iteration
count and function evaluations than did the unstructured counterparts. Unlike
the BFGS formula from (2), which recursively deﬁnes Bk+1 as a rank-2 update
to Bk, the formulas for AM
k+1 in eqs. (5), (6) additionally depend on
the known Hessians Kk+1 and Kk. For this reason the compact representations
of AM
k+1 are diﬀerent from the one for Bk+1 in (3) and have not yet
been developed. The updates in (5) and (6) are dense in general, and hence
neither are suitable for large-scale optimization. Hence, here, we develop ﬁrst
a compact representation of (5) and (6) and then show how to exploit them
to develop structured limited-memory quasi-Newton updates.

k+1 and AP

k+1 and AP

1.3 Article Contributions

k+1 and AP

In this article we develop the compact representations of the structured BFGS
updates AM
k+1 from eqs. (5) and (6) that lead to practical large-
scale limited-memory implementations. Unwinding the update formula in (6)
is challenging, however by using an induction technique we are able to derive
the explicit expression of the compact representation. We propose the limited
memory versions of the compact structured BFGS (L-S-BFGS) matrices and
describe line search algorithms (with slightly modiﬁed Wolfe conditions) that
implement them. We exploit the compact representations in order to com-
pute search directions by means of the Sherman-Morrison-Woodbury formula
and implement eﬀective initialization strategies. Numerical experiments of the
proposed L-S-BFGS methods on various problems are presented.

2 Compact Representations of Structured BFGS Updates

To develop the compact representations of the structured BFGS formulas, we
deﬁne

Uk = [ u0,

· · · , uk−1 ] , ST

k Uk = LU

k + RU
k ,

diag(ST

k Uk) = DU
k ,

(7)

where Uk ∈ Rn×k collects all uk for k ≥ 0 and where LU
lower triangular matrix, RU
the diagonal), and DU

k ∈ Rk×k is a strictly
k ∈ Rk×k is an upper triangular matrix (including

k ∈ Rk×k is the diagonal part of ST

k Uk.

2.1 Compact Representation of AM
k

Theorem 1 contains the compact representation of AM
k .

4

Theorem 1 The compact representation of AM

k in the update formula (5) is

AM

k = BM

0 − Kk − (cid:2) BM

0 Sk Uk

(cid:3)

(cid:20)ST

k BM
(LU

0 Sk LU
k
k )T −DU
k

(cid:21)−1 (cid:20)ST

k (BM
0 )T
UT
k

(cid:21)

,

(8)

where Sk is as deﬁned in (3), Uk, LU

k , and DU

k are deﬁned in (7), and

BM

0 = AM

0 + K0.

Proof. Observe that by adding Kk+1 to both sides of (5) the update formula
of BM

k+1 becomes

BM

k+1 = BM

k −

1
k BM
sT
k sk

BM

k sksT

k BM

k +

1
sT
k uk

ukuT
k .

k+1 is relabeled as Bk+1, BM

This expression is the same as (2) when BM
k is
relabeled as Bk, and uk is relabeled as yk. The compact representation of (2)
is given by (3), and therefore the compact representation of BM
k is given by
(3) with Yk replaced by Uk and B0 replaced by BM
0 . Then (8) is obtained
by subtracting Kk from the compact representation of BM
k , and noting that
BM
0 is
symmetric positive deﬁnite and sT
k uk > 0 for k ≥ 0, the inverse in the right-
hand side of (8) is nonsingular as long as BM
0 is symmetric positive deﬁnite
and sT
(cid:117)(cid:116)

k is symmetric positive deﬁnite as long as BM

0 + K0. Since BM

0 = AM

k uk > 0 for k ≥ 0.

k = (Kk +
k )−1, which is used to compute search directions in a line search algorithm

Corollary 1 describes the compact representation of the inverse HM
AM
(e.g., pM

k = −HM

k ∇f (xk)).

Corollary 1 The inverse HM
tation of AM

k from (8), is given as

k = (cid:0)Kk + AM

k

(cid:1)−1

, with the compact represen-

0 +

HM

k = HM
(cid:2)Sk HM

0 Uk

(cid:3)

where

(cid:20)(TU

k )T (cid:0)DU

k HM

0 Uk

k + UT
-TU
k

(cid:1) TU

k -(TU
k )T
0
¯ k×k

(cid:21) (cid:20)

ST
k
k (HM

0 )T

UT

(cid:21)

,

(9)

and where TU
and (7).

k = (RU

HM

0 = (BM

0 )−1 = (AM
k )−1 with Sk, Uk, DU

0 + K0)−1,
k , and RU

k deﬁned in Theorem 1

Proof. Deﬁne

Ξk ≡ (cid:2) BM

0 Sk Uk

(cid:3) , Mk ≡

5

(cid:20)ST

k BM
(LU

0 Sk LU
k
k )T −DU
k

(cid:21)

,

for the compact representation of AM
expression of HM
k = (cid:0)Kk + AM

HM

(cid:1)−1

0 )−1 then the
k in (8). Let HM
k is obtained by the Sherman-Morrison-Woodbury identity:

0 = (BM

= (cid:0)BM
= (BM

= HM

(cid:1)−1

k
0 − ΞkM−1
k ΞT
k
0 )−1 + (BM
0 )−1Ξk
(cid:20) 0
¯k×k
(RU
(cid:20)(RU

0 Ξk

0 − HM

= HM

0 + HM

0 Ξk

0 )−1Ξk
(cid:21)−1

k (BM

(cid:2)Mk − ΞT
RU
k
k HM
k + UT

k )T DU
k )−T (cid:0)DU

0 Uk
k HM
k )−1

0 Uk

k + UT
−(RU

(cid:3)−1

ΞT

k (BM

0 )−1

ΞT

k HM
0

(cid:1) (RU

k )−T

k )−1 −(RU
0
¯k×k

(cid:21)
ΞT

k HM
0

where the third equality is obtained from applying the Sherman-Morrison-
Woodbury inverse, the fourth equality uses the identity ST
k = RU
k ,
and the ﬁfth equality is obtained by explicitly computing the inverse of the
block matrix. Using (RU
0 Sk Uk]
(cid:117)(cid:116)
yields the expression in (9).

0 )−1Ξk = (BM

k )−1 = TU

k and (BM

k Uk − LU

0 )−1[BM

2.2 Compact Representation of AP
k

Developing the compact representation of (6) is more challenging and requires
an inductive argument. Speciﬁcally, we deﬁne vk ≡ Kk+1sk in addition to the
expressions in (7) and

Vk = [ v0,

k Vk = LV

. . . , vk−1 ] , ST

where Vk ∈ Rn×k collects all vk for k ≥ 0 and where LV
lower triangular matrix, RV
k ∈ Rk×k is the diagonal part of ST
the diagonal), and DV
contains the compact representation of AP
k .

k Vk) = DV
k , (10)
k ∈ Rk×k is the strictly
k ∈ Rk×k is the upper triangular matrix (including
k Vk. Theorem 2

k + RV
k ,

diag(ST

Theorem 2 The compact representation of AP

k in the update formula (6) is

AP

k = AP

0 − [ Qk Uk ]

where

(cid:20)DV

k + LV

k + (LV
(LU

k )T + ST
k )T

k AP

0 Sk LU
k
−DU
k

(cid:21)−1 (cid:20)QT
k
UT
k

(cid:21)

,

(11)

and where Sk, Uk, DU
deﬁned in (10).

k , and LU

Qk ≡ Vk + AP
0 Sk,
k are deﬁned in (2) and Vk, LV

k , and DV

k are

Proof. The proof of (11) is by induction. For k = 1 it follows that
(cid:20)sT

0 v0 + sT

0 AP

0 s0

(cid:3)

AP

1 = AP

0 − (cid:2)v0 + AP

0 s0 u0

(cid:21)−1 (cid:20)(v0 + AP
uT
0

(cid:21)

0 s0)T

−sT

0 u0

= AP

0 −

1
0 (K1 + AP
sT

0 )s0

(K1 + AP

0 )s0sT

0 (K1 + AP

0 )T +

1
sT
0 u0

u0uT
0 ,

6

which shows that (11) holds for k = 1. This expression is the same as AP
1 in
(6), and thus the compact representation holds for k = 1. Next assume that
(11) is valid for k ≥ 1, and in particular let it be represented as

AP

k = AP

0 − [ Qk Uk ]

(cid:20) (Mk)11 (Mk)12
(Mk)T
12 (Mk)22

(cid:21)−1 (cid:20) QT
k
UT
k

(cid:21)

,

(12)

where

(Mk)11 = DV

k + LV

k + (LV

k )T + ST

k AP

0 Sk, (Mk)12 = LU

k , (Mk)22 = −DU
k .

We verify the validity of (12) by substituting it in the update formula (6), and
then seek the representation (12) for k + 1:

AP

k+1 = AP

0 − [ Qk+1 Uk+1 ]

(cid:20) (Mk+1)11 (Mk+1)12
(Mk+1)T
12 (Mk+1)22

First let

(cid:21)−1 (cid:20) QT
UT

k+1

k+1

(cid:21)

.

qk = vk + AP

0 sk, wk = QT

k sk,

rk = UT

k sk,

ξk =

(cid:20) wk
rk

(cid:21)

,

and note that in (6) it holds that

(AP

k + Kk+1)sk = AP

k sk + vk

= AP

0 sk − [ Qk Uk ] [Mk]−1

≡ qk − [ Qk Uk ] [Mk]−1

(cid:21)

+ vk

k sk
k sk

(cid:20) QT
UT
(cid:21)

(cid:20) wk
rk

≡ qk − [ Qk Uk ] [Mk]−1ξk.

Next we deﬁne σP
tation of AP

k = 1/sT
k+1 using (6) and (12):

k (AP

k + Kk+1)sk and obtain the following represen-

AP

k+1 = AP

k − σP

k (AP

k sk + vk)(AP

k sk + vk)T +

1
sT
k uk

ukuT
k

= AP

0 − σP

k [ Qk Uk qk ]

(cid:34) M−1
k
σP
k

+ M−1
−ξT

k ξkξT
k M−1

k

k M−1

k −M−1
k ξk
1

(cid:35) 






QT
k
UT
k
qT
k

+

1
sT
k uk

ukuT
k

= AP

0 − [ Qk Uk qk ]





Mk
k rT
k

(cid:2) wT

(cid:21)

(cid:20) wk
rk
(cid:3) sT
k qk



−1 







 +

QT
k
UT
k
qT
k

1
sT
k uk

ukuT
k .

7

Using the permutation matrix P
¯
sent AP

k+1 as

= [ e1 · · · ek e2k+1 · · · e2k ], we repre-

AP

k+1 = AP

0 − [ Qk Uk qk ] P
¯

P
¯





T

Mk
k rT
k

(cid:2) wT

−1





(cid:21)

(cid:20) wk
rk
(cid:3) sT
k qk

T

P
¯

P
¯









QT
k
UT
k
qT
k

+

1
sT
k uk

ukuT
k

= AP

0 − [ Qk qk Uk uk ]







(Mk)11 wk (Mk)12
sT
k qk
rk
0

wT
k
(Mk)T
12
0
¯

rT
k
(Mk)22
0
¯

0
¯
0
0
¯
−sT
k uk



-1 















.

QT
k
qT
k
UT
k
uT
k

(Mk+1)11 =

Now we verify that the identities hold:
Qk+1 = [ Qk qk ] = (cid:2) Vk + AP
Uk+1 = [ Uk uk ] ,
(cid:20) (Mk)11 wk
sT
wT
k qk
k
(cid:21)
(cid:20) (Mk)12 0
¯
rT
0
k
(cid:20) (Mk)22
0
¯

0
¯
−sT
k uk

(Mk+1)12 =

(Mk+1)22 =

= Lk+1,

= DV

(cid:21)

(cid:21)

= −Dk+1.

0 Sk vk + AP

0 sk

(cid:3) = Vk+1 + AP

0 Sk+1,

k+1 + LV

k+1 + (LV

k+1)T + ST

k+1AP

0 Sk+1,

Therefore we conclude that AP
indices k.

k+1 is of the form (11) with k + 1 replacing the
(cid:117)(cid:116)

2.3 Limited Memory Compact Structured BFGS

The limited memory representations of Eqs. (8) and (11) are obtained by stor-
ing only the last m ≥ 1 columns of Sk, Uk and Vk. By setting m (cid:28) n limited
memory strategies enable computational eﬃciencies and lower storage require-
ments, see e.g., [19]. Updating Sk, Uk and Vk requires replacing or inserting
one column at each iteration. Let an underline below a matrix represent the
represents Sk without its
matrix with its ﬁrst column removed. That is, S
¯k
ﬁrst column. With this notation, a column update of a matrix, say Sk, by a
vector sk is deﬁned as follows.

colUpdate (Sk, sk) ≡

(cid:40)

[ Sk sk ]
[ S
sk ]
¯k

if k < m
if k ≥ m.

Such a column update either directly appends a column to a matrix or ﬁrst
removes a column and then appends one. This column update will be used, for
instance, to obtain Sk+1 from Sk and sk, i.e., Sk+1 = colUpdate(Sk, sk). Next,

8

let an overline above a matrix represent the matrix with its ﬁrst row removed.
T
T
That is, S
¯ k represents S
k U
k Uk without its ﬁrst row. With this notation, a
¯
¯
product update of, say ST
k Uk, by matrices Sk, Uk and vectors sk, uk is deﬁned
as:

prodUpdate (cid:0)ST

k Uk, Sk, Uk, sk, uk

(cid:1) ≡






(cid:34)

(cid:35)

ST
k Uk ST
k uk
k Uk sT
sT
k uk
(cid:17)
(cid:34) (cid:16)
T
T
uk
S
S
k U
¯ k
¯
¯
k
sT
sT
k U
k uk
¯ k

if k < m

(cid:35)

if k ≥ m.

This product update is used to compute matrix products, such as, ST
k+1Uk+1,
with O(2mn) multiplications, instead of O(m2n) when the product ST
k Uk
had previously been stored. Note that a diagonal matrix can be updated in
this way by setting the rectangular matrices (e.g., Sk, Uk) to zero, such that
e.g., DU
k+1 = prodUpdate(DU
, sk, uk). An upper triangular matrix can
, 0
k , 0
¯
¯
be updated in a similar way, e.g., RU
k+1 = prodUpdate(RU
, sk, uk). To
save computations, products with zeros matrices are never formed expliclty.
Section 3 discusses computational and memory aspects in greater detail.

k , Sk, 0
¯

3 Limited-Memory Structured BFGS Line Search Algorithms

This section describes two line search algorithms with limited memory struc-
tured BFGS matrices. The compact representations enable eﬃcient reinitial-
ization strategies and search directions, and we discuss these two components
ﬁrst, before presenting the overall algorithms.

3.1 Initializations

k−1yk−1/sT

0 = (cid:98)σkIn, where (cid:98)σk = yT

For the limited memory BFGS matrix based on (3) one commonly uses the
initializations B(k)
k−1yk−1 (c.f. [6]). Choos-
ing the initialization as a multiple of the identity matrix enables fast com-
putations with the matrix in (3). In particular, the inverse of this matrix
may be computed eﬃciently by the Sherman-Morrison-Woodbury identity.
Because at the outset it is not necessarily obvious which initializations to use
for the limited memory structured-BFGS (L-S-BFGS) matrices based on eqs.
(8) and (11), we investigate diﬀerent approaches. We use the analysis in [1],
which proposed formula (cid:98)σk. Additionally, in that work a second initialization
(cid:98)σ(2)
k = sT
k−1sk−1 was proposed. Because in the S-BFGS methods the
vectors (cid:98)uk and uk are used instead of yk (unstructured BFGS), the initializa-

k−1yk−1/sT

9

tions in this article are the below.



uT
k uk
sT
k uk
(cid:98)uT
k (cid:98)uk
sT
k (cid:98)uk
sT
k uk
sT
k sk
sT
k (cid:98)uk
sT
k sk

Init. 1

(13)

Init. 3

Init. 2

Init. 4

σk+1 =



Note that Init. 1 and Init. 2 are extensions of (cid:98)σk to structured methods.
Instead of using yk these initializations are deﬁned by (cid:98)uk and uk. Init. 3
and Init. 4 extend (cid:98)σ(2)
k . Observe that the vectors (cid:98)uk = ∇(cid:98)u(xk+1) − ∇(cid:98)u(xk)
). In contrast, uk = Kk+1sk +
depend only on gradient information of (cid:98)u(x
¯
(cid:98)uk depends on known second-derivative information, too. Because the initial
M
P
matrices A
0 and A
0 aﬀect the compact representations from Theorems 1 and
¯
¯
2 diﬀerently, we accordingly adjust our initialization strategies for these two
matrices. In particular, for L-S-BFGS-M the compact limited memory formula
for BM

M
k simpliﬁes if we take B
0 as a multiple of the identity matrix:
¯
M
B
0 = A
¯
¯

.
¯ 0 ≡ σkI
¯

M
0 + K

(14)

M
0

the corresponding limited memory matrices B
¯

The advantage of this choice is that it has similar computational complexities
to the L-BFGS formula from (3). However by setting this default initialization
M
for B
k are not equivalent
¯
M
k deﬁned by (5), even when k < m.
anymore to the full-memory matrices B
¯
M
0 is not taken
In Section 3.4.1 computational techniques are discussed when B
¯
P
. This
0 = σkI
as a multiple of the identity matrix. For L-S-BFGS-P we set A
¯
¯
initialization, as long as σk remains constant, implies that the limited memory
compact representations from Theorem 1 and the update formulas from (6)
produce the same matrices when k < m.

3.2 Search Directions

The search directions for line search algorithms, with the structured BFGS
approximations, are computed as

pk = −(Kk + Ak)−1gk,

(15)

k from (11). When AM

where gk = ∇f (xk) and where Ak is either the limited memory version of AM
k
from (8) or AP
k is used, we apply the expression of the
inverse from Corollary 1, in order to compute search directions. In particular,
M
from the preceding section, the
0 = σkI
with the initialization strategy B
¯
¯
search directions (15) are computed eﬃciently by

pM

k = −

gk
σk

−[Sk Uk]





(TU

k )T (cid:0)DU

k Uk

k + 1/σkUT
− TU

k
σk

(cid:1) TU

k − (TU
k )T
σk
0
¯m×m





(cid:18)(cid:20) ST
UT

k gk
k gk

(cid:21)(cid:19)

,

(16)

10

where TU
k is deﬁned in Corollary 1. This computation is done eﬃciently as-
suming that all matrices have been updated before, such as UT
k Uk. Omitting
terms of order m, the multiplication complexity for this search direction is
O(n(4m + 1) + 3m2). In particular, computing pM
k can be done by: two vector
multiplies with the n × 2m matrix [ Sk Uk ] (order 4nm), the scaling gk
(order
σk
n) and a matrix vector product with a structured 2m × 2m matrix. Since TU
k
represents a solve with an m × m upper triangular matrix the vector product
with the middle 2m×2m matrix is done in order 3m2. When AP
k is used, search
directions are computed by solves of the linear system (Kk + AP
k = −gk.
Because of the compact representation of AP
k we can exploit structure in solv-
ing this system, as is described in Section 3.4.2

k )pP

3.3 Algorithms

Similar to Petra et al. [20], we use a strong Wolfe line search in our implemen-
tations of the new limited-memory compact representations. For nonnegative
constants 0 < c1 ≤ c2, the current iterate xk and search direction pk, the
strong Wolfe conditions deﬁne the step length parameter α by two inequalities

f (xk + αpk) ≤ f (xk) + c1α(pT
(cid:12)
(cid:12)pT

k ∇f (xk + αpk)(cid:12)

k ∇f (xk)), and
k ∇f (xk)(cid:12)
(cid:12) .

(cid:12) ≤ c2

(cid:12)
(cid:12)pT

(17)

Because the S-BFGS-M matrix from (8) is positive deﬁnite as long as sT
k uk > 0
for k ≥ 0 (rather than sT
k yk > 0 for k ≥ 0 for L-BFGS), the line searches in
our algorithms include this condition. As in [20, Appendix A] a variant of the
Mor´e-Thuente [18] line search is used. This line search is identical to the one of
Mor´e-Thuente, except for one condition. Speciﬁcally, given a trial step length
¯t, our line search terminates when the conditions in (17) and
αt and trial u
additionally sT
¯t > 0 holds. [20, Proposition 17] ensures the existence of a step
k u
length α that satisﬁes all of the above conditions. Such a line search variant is
straight forward to implement, by adding one additional condition to a Mor´e-
Thuente line search. Moreover, when S-BFGS-M is used, new search directions
are computed by using the inverse from Corollary 1. In contrast, because the S-
BFGS-P matrix from (11) is not necessarily positive deﬁnite even if sT
k uk > 0
for k ≥ 0 (see [20]), our implementation checks whether Kk + AP
k is positive
deﬁnite, before computing a new search direction However, if it is known that
Kk is positive deﬁnite for all k ≥ 0 (which is often the case in applications)
than ensuring that sT
k uk > 0 for k ≥ 0 ensure positive deﬁniteness, in this
case too. If this matrix is positive deﬁnite, then a new search direction is
computed by solving the linear system (Kk + AP
k = −gk. Otherwise the
search direction is computed by solving the system (Kk +AP
k +δIn)pP
k = −gk,
where the scalar δ > 0 ensures that (Kk + AP
k + δIn) (cid:31) 0 (Here δ is chosen as
the the ﬁrst δ = 10j, j = 0, 1, . . . that yields a positive deﬁnite matrix). The
proposed limited memory line search algorithms are listed in Algorithm 1 and
Algorithm 2.

k )pP

11

Algorithm 1 Limited Memory Structured-BFGS-Minus (L-S-BFGS-M)
1: Initialize: k = 0, m > 0, (cid:15) > 0, σk > 0, 0 < c1 ≤ c2, xk, gk = ∇f (xk) = ∇ˆk(xk) +
, Θk =
¯ 0 = (1/σk)I
¯

k )−1 = 0, UT

∇ˆu(xk), Sk = 0, Uk = 0, DU
[Sk H

k = 0, (RU

k Uk = 0, H

¯ 0Uk]
2: while (cid:107)gk(cid:107)∞ > (cid:15) do
Compute:
3:

where

Mk =

pk = −H

¯ 0gk + ΘkMk(ΘT

k gk),

(cid:20) (RU

k )−T (DU

k +UT
−(RU

¯ 0Uk)(RU
k H
k )−1

k )−T
k )−1 −(RU
0
¯

(cid:21)

.

4:

Strong Wolfe line search:

xk+1 = xk + αpk,
where α > 0, xk+1 satisﬁes strong Wolfe conditions (cf. [20] and (17)), sk = xk+1−xk,
sT
k uk > 0.
Updates: gk+1 = ∇f (xk+1), uk = ∇2ˆk(xk+1)sk + (∇ˆu(xk+1) − ∇ˆu(xk))
Sk+1 = colUpdate(Sk, sk)

5:
6:
7: Uk+1 = colUpdate(Uk, uk)
8: RU
9: UT
10: DU
Compute: σk+1
11:
12:
, update Mk+1, Θk+1 using Theorem 1, k = k + 1
¯ 0 = (1/σk+1)I
H
¯
13: end while
14: return xk

k+1 = prodUpdate(RU
k+1Uk+1 = prodUpdate(UT
k+1 = prodUpdate(DU
, 0
¯

k Uk, Uk, Uk, uk, uk)
, sk, uk)

, sk, uk)
k+1, Sk, 0
¯

k , 0
¯

¯ 0 + A
¯

Note that ΘT

(cid:20) ST
k gk
¯ 0(UT
H

(cid:21)
k gk)
M
0 )−1 is needed, when the

so that only one linear solve with H
algorithm does not use a multiple of the identity as the initialization.

k gk on Line 3 in Algorithm 1 is computed as
¯ 0 = (K
Algorithm 2 is expected to be computationally more expensive than Algo-
rithm 1 because it tests for the positive deﬁniteness of Kk + Ak in Line 3 and
it computes search directions by the solve in Line 6. However, the structured
quasi-Newton approximation in Algorithm 2 may be a more accurate approx-
imation of the true Hessian (see [20]), which may result in fewer iterations
or better convergence properties. Note that as in [20, Section 3.1.2] compu-
tational eﬀorts for ensuring positive deﬁniteness may largely reduced by e.g.,
deﬁning δ = max(0, (ε − (uk + vk)T sk)/(cid:107)sk(cid:107)2), for 0 < ε. Unlike Algorithm 2,
Algorithm 1 does not require solves involving large linear systems.

3.4 Large-Scale Computation Considerations

This section discusses computational complexity and memory requirements
of the structured Hessian approximations when the problems are large. In
particular, if n is large the Hessian matrices Kk typically exhibit additional
structure, such as being diagonal or sparse. When Kk is sparse and solves with
it can be done eﬃciently, the compact representation of AM
k can be
exploited to compute inverses of Kk + Ak eﬃciently. Note that Algorithm 1

k and AP

12

Algorithm 2 Limited Memory Structured-BFGS-Plus (L-S-BFGS-P)
1: Initialize: k = 0, m > 0, (cid:15) > 0, σk > 0, 0 < c1 ≤ c2, xk, gk = ∇f (xk) = ∇ˆk(xk) +
k = 0,

∇ˆu(xk), Kk = ∇2ˆk(xk), Sk = 0, Uk = 0, Vk = 0, DU
Ωk = 0, ST

k = 0, DV

k = 0, LU

k = 0, LV

k Sk = 0, Ak = σkI
¯

if

2: while (cid:107)gk(cid:107)∞ > (cid:15) do
3:
4:
5:
6:

end if
Solve:

(Kk + Ak) (cid:54)(cid:31) 0 then
Find δ > 0 such that (Kk + Ak + δIn) (cid:31) 0

(Kk + Ak)pk = −gk

7:

Strong Wolfe line search:

xk+1 = xk + αpk,
where α > 0, xk+1 satisﬁes strong Wolfe conditions (cf. [20] and (17)), sk = xk+1−xk.

8:

Updates: gk+1 = ∇f (xk+1), Kk+1 = ∇2ˆk(xk+1), vk = Kk+1sk, uk = vk +
(∇ˆu(xk+1) − ∇ˆu(xk))
Sk+1 = colUpdate(Sk, sk)

9:
10: Uk+1 = colUpdate(Uk, uk)
11: Vk+1 = colUpdate(Vk, vk)
LU
k+1 = prodUpdate(LU
, Uk, sk, 0
k , 0
12:
)
¯
¯
LV
k+1 = prodUpdate(LV
, Vk, sk, 0
13:
)
k , 0
¯
¯
k+1Sk+1 = prodUpdate(ST
ST
14:
k Sk, Sk, Sk, sk, sk)
k+1 = prodUpdate(DU
15: DU
k , 0
, sk, uk)
, 0
¯
¯
16: DV
k+1 = prodUpdate(DV
, sk, vk)
, 0
k , 0
¯
¯
17:
18:
19:

Compute: σk+1
A
, update Ωk+1 = [ Vk+1 + A
¯ 0 = (1/σk+1)I
¯

¯ 0Sk+1 Uk ]

(cid:20)DV

k+1 + LV

k+1 + (LV
(LU

k+1)T + ST
k+1)T

k+1A

¯ 0Sk+1 LU
k+1
−DU

k+1

(cid:21)−1

ΩT

k+1

Ak+1 = A

¯ 0 − Ωk+1

k = k + 1

20:
21: end while
22: return xk

is directly applicable to large problems, because the formula in (16) does not
use solves with Kk . Nevertheless, observe that the matrices Kk + Ak, (with
limited memory Ak from Theorem 1 or Theorem 2, respectively), have the
form with m (cid:28) n:

Kk + Ak ≡ (cid:98)K0 −









Ξk









(cid:2)Mk

(cid:3)−1(cid:2)

ΞT
k

(cid:3),

(18)

for some (cid:98)K0. If AM
0 and Ξk, Mk correspond
k in (18) then (cid:98)K0 = Kk + AP
to the remaining terms in Theorem 1. Using AP
0
and Ξk, Mk correspond to the remaining terms in Theorem 2. Because of

k is used in (18) then (cid:98)K0 = K0+AM

13

its structure the matrix in (18) can be inverted eﬃciently by the Sherman-
Morrison-Woodbury formula as long as solves with (cid:98)K0 can be done eﬃciently.
Next, L-S-BFGS-M and L-S-BFGS-P are discussed in the situation when solves
with (cid:98)K0 are done eﬃciently. Afterwards we relate these methods to S-BFGS-
M, S-BFGS-P and BFGS, L-BFGS.

3.4.1 Computations for L-S-BFGS-M

The most eﬃcient computations are achieved when (cid:98)K0 is set as a multiple of
(cf. (3.2) with O(n(4m + 1) + 3m2) multiplications).
the identity matrix σkI
¯
This approach however omits the K0 term. Nevertheless, when K0 has ad-
ditional structure such that factorizations and solves with it can be done in,
say nl multiplications, search directions can be computed eﬃciently in this
case, without omitting K0. In particular, the search direction is computed as
pM
k = −(Kk + AM
k )−1gk = −HM
k gk where HM
k is the inverse from (1). The
+ K0)−1. To determine the search direction
initialization matrix is HM
0 = (σkI
¯
two matrix vector products with the n × 2m matrices [ Sk HM
0 Uk ] are re-
quired, at complexity O(4nm + 2nl). The product with the 2m × 2m middle
matrix is done at O(2nm + nl + 2m2). Subsequently, −HM
0 gk is obtained at nl
multiplications. The total complexity is thus O(n(6m+4l)+2m2). Note that if
σk is set to a constant value, say σk = ¯σ, then the complexity can be further re-
duced by storing the matrix ¯U
)−1u
¯i.
¯k−m . . . ¯u
¯i = (K0 + ¯σI
¯
The computational cost in this situation is O(n(4m + l) + 3m2), excluding
the updating cost of the vector ¯u
¯i at order nl. With a constant σk only one
) is required.
factorization of (K0 + ¯σI
¯

¯k−1 ], where ¯u

¯ k = [ ¯u

3.4.2 Computations for L-S-BFGS-P

k is used in (18) with (cid:98)K0 = (Kk + AP

0 ) and (cid:98)Qk = (cid:98)K−1

0 Qk, (cid:98)Uk =

When AP
(cid:98)K−1

0 Uk the inverse has the form

(Kk + AP

k )−1 = (cid:98)K−1
0

(cid:16)

Mk − ΞT

k (cid:98)K−1

0 Ξk

(cid:17)−1

ΞT

k (cid:98)K−1
0

(cid:19)

,

where ΞT

k (cid:98)K−1

0 Ξk =

(cid:20)

QT
UT

k (cid:98)Qk QT
k (cid:98)Qk UT

k (cid:98)Uk
k (cid:98)Uk

and Ξk, Mk are deﬁned in (2). Assum-

k = −(Kk + AP

ing that Mk, Qk, Uk had previously been updated, computing the search
k )−1gk may be done as follows; First, (cid:98)Qk, (cid:98)Uk are
direction pP
computed in O(2nlm) multiplications. Then the 2m × 2m matrix ΞT
0 Ξk
is formed in O(3nm2) multiplications. Combining the former terms and solv-
ing with the (small) 2m × 2m matrix explicitly, the direction pP
k is computed
in O(n(2lm + 3m2 + 4m + 1) + m3) multiplications. Note that this approach
requires an additional 2nm storage locations for the matrices (cid:98)Qk, (cid:98)Uk. Two
additional remarks; ﬁrst, since Qk = Vk + AP
0 Sk, the update of Qk uses
O(nl) multiplications to form a new v
¯k and additional nm multiplications if
AP
. If σk remains constant, say σk = ¯σ, then the update of Qk is done
0 = σkI
¯

k (cid:98)K−1

14

(cid:18)
¯n + Ξk
I
(cid:21)

Table 1 Comparison of computational demands for BFGS,L-BFGS,S-BFGS-M,S-BFGS-
P,L-S-BFGS-M,L-S-BFGS-P, excluding storage of Kk and where solves with Kk are as-
sumed to cost O(nl) multiplications and vector multiplies cost O(l). Terms of order O(m)
or lower are omitted. († The search direction cost for L-S-BFGS-P do not include the identity
regularization with δ from Sec. 3.3, as other techniques are possible.)

Search Direction
O(n2)
O(n(4m + 1) + m2)

Memory
Method
O(n2)
BFGS
O(2nm + 3
L-BFGS ((3), [6])
O(n2)
S-BFGS-M ((5),[20]) O(n2)
O(n2)
O(n2)†
S-BFGS-P ((6),[20])
O(2nm + 3
O(n(4m + 1) + m2)
L-S-BFGS-M ((16))
O(n(6m + 4l) + m2) O(2nm + 3
L-S-BFGS-M ((18))
O(n(2lm + 3m2+
L-S-BFGS-P ((18))
4m + 1) + m3)†

O(4nm + 3m2)

2 m2) O(2nm)

O(n2)
O(n2)
2 m2) O(2nm + l)
2 m2) O(n(m + l))
O(n(3m + l))

Update
O(n2)

at only O(nl) multiplications, because AP
0 Sk does not need to be recomputed
each iteration. Second, if Kk = K0, in other words if Kk is a constant ma-
trix then Theorems 1 and 2 reduce to the same expressions yielding the same
computational complexities.

3.4.3 Memory Usage and Comparison

This section addresses the memory usage of the proposed representations and
relates their computational complexities to existing methods. As an overall
guideline, the representations from (18) use 2nm + 4m2 storage locations,
excluding the (cid:98)K0 term. This estimate is reﬁned if the particular structure of
the matrix Mk is taken into consideration. For example, the matrices TU
k and
DU
k from Theorem 1 are upper triangular and diagonal, respectively. Thus,
k Uk ∈ Rm×m is stored and updated,
when HM
the memory requirement for the limited memory version of HM
k in (1) are
O(2nm + 3
2 m2 + m) locations. We summarize the computational demands
of the diﬀerent methods in a table. Note that when m (cid:28) n and l (cid:28) n L-
BFGS, L-S-BFGS-M and L-S-BFGS-P enable computations with complexity
lower than n2 and therefore allow for large values of n. Moreover, Table 1
shows that the proposed limited-memory BFGS methods have similar search
direction complexity to unstructured L-BFGS, but higher update cost.

, and when the matrix UT
0 = σkI
¯

4 Numerical Experiments

This section describes the numerical experiments for the proposed methods in
Section 3. The numerical experiments are carried out in MATLAB 2016a on
a MacBook Pro @2.6 GHz Intel Core i7, with 32 GB of memory. The experi-
ments are divided into ﬁve parts. In Experiment I, we investigate initialization
strategies. Experiment II compares the limited memory methods with the full-
memory methods. The tests in this experiment are on the same 61 CUTEst
[16] problems as in [20], unless otherwise noted. In Experiment III, we use

15

classiﬁcation data from LIBSVM (a library for support vector machines [7])
in order to solve regularized logistic regression problems with the proposed
methods. We also include an application to PDE constrained optimization.
In Experiment IV, the proposed methods and L-BFGS with the IPOPT [26]
solver are compared. Experiment V, describes a real world application from
image reconstruction.

Extended performance proﬁles as in [17] are provided. These proﬁles are an
extension of the well known proﬁles of Dolan and Mor´e [11]. We compare the
number of iterations and the total computational time for each solver on the
test set of problems, unless otherwise stated. The performance metric ρs(τ )
with a given number of test problems np is

ρs(τ ) =

card {p : πp,s ≤ τ }
np

and πp,s =

tp,s
min tp,i
1≤i≤S,i(cid:54)=s

,

where tp,s is the “output” (i.e., iterations or time) of “solver” s on problem p.
Here S denotes the total number of solvers for a given comparison. This metric
measures the proportion of how close a given solver is to the best result. The
extended performance proﬁles are the same as the classical ones for τ ≥ 1. In
the proﬁles we include a dashed vertical grey line, to indicate τ = 1. In all
experiments the line search parameters are set to c1 = 1 × 10−4 and c2 = 0.9.

4.1 Experiment I

This experiment investigates the initialization strategies from Section 3. To
this end, the problems in this experiment are not meant to be overly challeng-
ing, yet they are meant to enable some variations. Therefore, we deﬁne the
quadratic functions

Qi(x
¯

; φ, r) ≡

1
2

T (φ · I
x
¯
¯

+ Q
¯ i

¯ iQ
D
¯

T
i

,
)x
¯

¯ i ∈ Rr×r is a diag-
with scalar parameters 0 < φ, 1 ≤ r ≤ n and where D
∈ Rn×r has orthonormal gaussian columns. Note that r
onal matrix and Q
¯ i
eigenvalues of the Hessian ∇2Qi are the diagonal elements of φ · I
¯ i, while
+ D
¯
the remaining (n − r) eigenvalues are φ. Therefore, by varying φ, r, and the
¯ i, Hessian matrices with diﬀerent spectral properties are formed.
elements of D
In particular, when r (cid:28) n, the eigenvalues are clustered around φ. In the
experiments of this section we investigate φ = 1. In Appendix A, we include
tests when φ = 1000. The structured objective functions from (4) are deﬁned
by

T g
¯

(cid:98)k(x
¯

) = x
¯

) = Q2(x
¯
We refer to the objective functions f (x
) deﬁned by (19) as
) + (cid:98)u(x
¯
¯
structured quadratics. The problems in this experiment have dimensions n =
j · 100 with corresponding r = j · 10 for 1 ≤ j ≤ 7. Since some of the problem

(cid:98)u(x
¯
) = (cid:98)k(x
¯

+ Q1(x
¯

; φ, r).

; φ, r),

(19)

16

Fig. 1 Comparison of initialization strategies for L-S-BFGS-M on problems with eigenvalues
clustered around 1 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1. Left: number of iterations;
right: time.

data in this experiment is randomly generated (e.g., the orthonormal matrices
Q
), the experiments are repeated ﬁve times for each n. The reported results
¯ i
are of the average values of the ﬁve individual runs. For all solvers we set m = 8
(memory parameter), (cid:15) = 5 × 10−6 ((cid:107)gk(cid:107)∞ ≤ (cid:15)), and maximum iterations to
10,000 This limit was not reached in the experiments.

4.1.1 Experiment I.A: L-S-BFGS-M

Experiment I.A compares the four L-S-BFGS-M initializations on the struc-
tured quadratic objective functions with eigenvalues clustered around 1. In
particular, φ = 1, and the elements of D
¯ i are uniformly distributed in the
interval [0, 999]. The results are displayed in Fig. 1. We observe that in terms
of number of iterations, Init. 4 (red) and Init. 3 (purple) perform similarly and
that also Init. 2 (green) and Init. 1 (blue) perform similarly. Overall, Init. 4
and Init. 3 requirer fewer iterations on the structured quadratics. Moreover,
the solid lines are above the dashed ones for both pairs. This indicates that
including only gradient information in (cid:98)uk and in the initialization strategy, as
opposed to also including 2nd derivative information from uk, may be desirable
for this problem. Init. 1 and Init. 2 are fastest on these problems. Even though
these initializations require a larger number of iterations, they can be faster
because the line searches terminate more quickly.

Next, we compare the four L-S-BFGS-P initializations. As before, experi-
ments on problems with eigenvalues clustered at 1 are done. Experiments with
eigenvalues clustered around 1000 are included in Appendix A. The respective
outcomes are in Figure 2.

We observe that, similar to Figure 1, Init. 3 and Init. 4 do best in iterations,

while Init. 1 does best in time.

To analyze the properties of the scaling factor σk in greater detail, Section

4.1.2 describes experiments that relate σk to eigenvalues.

17

0.5124=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M40.5124=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4Fig. 2 Comparison of initialization strategies for L-S-BFGS-P on problems with eigenvalues
clustered around 1 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1. Left: number of iterations;
right: time.

4.1.2 Experiment I.B: Eigenvalue Estimation

In Experiment I.B we investigate the dynamics of σk in the four initialization
strategies from (13) on a ﬁxed problem as the iteration count k increases.
In particular, we use one representative run from the average results of the
preceding two subsections, where n = 100 and r = 10. In Figure 3 the evolution
of σk of all four initializations for both; L-S-BFGS-M and L-S-BFGS-P is
displayed on a structured quadratic problem with eigenvalues clustered at 1. In
Figure 4 the same quantities are displayed for structured quadratic problems
with eigenvalues clustered at 1000. In green ¯λ1≤n and ¯λ1≤r are displayed,
which correspond to the median taken over the ﬁrst 1, 2, · · · , n (all) and the
ﬁrst 1, 2, · · · , r eigenvalues, respectively. Because in Figure 3 the eigenvalues
are clustered around 1, ¯λ1≤n = 1. In Figure 4 the eigenvalues are clustered
around 1000 and ¯λ1≤r = 1000. In red ¯σk is the average σk value over all
iterations.

Across all plots in Figures 3 and 4 we observe that the dynamics of σk
for L-S-BFGS-M and L-S-BFGS-P are similar. Moreover, the average ¯σk is
higher for Init. 1 and Init. 2 than for Init. 3 and Init. 4. The variability of
Init. 2 appears less than that of Init. 1, while the variability of Init. 4 appears
less than that of Init. 3. We observe that Init. 1 and 2 approximate a large
eigenvalue well, whereas Init. 3 and Init. 4 approximate smaller eigenvalues
better (cf. Figure 3 lower half). Since large σk values typically result in shorter
step lengths (step computations use 1/σk), choosing Init. 1 and Init. 2 result
in shorter step lengths on average. Taking shorter average steps can be a
desirable conservative strategy when the approximation to the full Hessian
matrix is not very accurate. Therefore as a general guideline, Init. 1 and Init.
2 appear more suited for problems in which it is diﬃcult to approximate the
Hessian accurately, and Init. 1 and Init. 2 are more suited for problems in
which larger step sizes are desirable.

18

0.5124=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P40.5124=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4Fig. 3 Eigenvalue estimation with initialization parameter σk. The eigenvalues are clustered
around 1 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1.

4.2 Experiment II

Experiment II compares the limited memory structured formulas with the full-
memory update formulas from Petra et al. [20] on the CUTEst problems from
[20]. The full-memory algorithms from [20], which use Eqs. (5) and (6), are
called S-BFGS-M and S-BFGS-P, respectively. The line search procedures of
the limited memory structured BFGS algorithms (Algorithms 1 and 2) are
the same as for the full memory algorithms. Moreover, the initializations in
P
0 =
the full memory algorithms are set as A
¯
¯σIn for S-BFGS-P, where ¯σ = 10i for the ﬁrst i ≥ 0 that satisﬁes (10iIn +
K0) (cid:31) 0 (usually i = 0). The experiments are divided into two main parts.
Experiment II.A. tests the limited memory structured BFGS-Minus versions
corresponding to Algorithm 1. Experiment II.A. is further subdivided into the
cases in which the memory parameters are m = 8 and m = 50. These values
represent a typical value (m = 8) and a relatively large value (m = 50), cf.
e.g., [2]. Experiment II.B. tests the limited memory structured BFGS-Plus
versions corresponding to Algorithm 2. As before, Experiment II.B. is further
subdivided into the cases in which the memory parameters are m = 8 and
m = 50. For all the solvers, we set (cid:15) = 1 × 10−6 ((cid:107)gk(cid:107)∞ ≤ (cid:15)) and maximum
iterations to 1,000.

M
0 = ¯σIn for S-BFGS-M, and A
¯

19

50100150200k050010001500<kL-S-BFGS-M (Init.1)50100150k050010001500<kL-S-BFGS-P (Init.1)7615r7615n7<k7615r7615n7<k50100150200k0200400600800<kL-S-BFGS-M (Init.2)50100150k0200400600800<kL-S-BFGS-P (Init.2)7615r7615n7<k7615r7615n7<k50100150k0100200300400500600<kL-S-BFGS-M (Init.3)2060100140k0100200300400500600<kL-S-BFGS-P (Init.3)7615r7615n7<k7615r7615n7<k50100150k0100200300400500600<kL-S-BFGS-M (Init.4)2060100140k0100200300400500600<kL-S-BFGS-P (Init.4)7615r7615n7<k7615r7615n7<kFig. 4 Eigenvalue estimation with scaling parameter. The eigenvalues are clustered around
1,000 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1000.

Fig. 5 Comparison of four initialization strategies of L-S-BFGS-M from (13) to the full-
recursive method S-BFGS-M (corresponding to (5)) on all 62 CUTEst problems from [20].
The limited memory parameter is m = 8. Left: number of iterations; right: time.

4.2.1 Experiment II.A: L-S-BFGS-M

In Experiment II.A we compare the limited memory implementations of Algo-
rithm 1 with initialization strategies in (13) with the full-recursive S-BFGS-M
method from (5). The solvers are tested on all 62 CUTEst problems from [20].
Figure 5 contains the results for the limited memory parameter m = 8.

We observe that the full-memory S-BFGS-M (black) does well in terms
of number of iterations and execution time. However, L-S-BFGS-M1 (Init. 1,

20

2468101214k0500100015002000<kL-S-BFGS-M (Init.1)2468101214k0500100015002000<kL-S-BFGS-P (Init.1)7615r7615n7<k7615r7615n7<k51015k02004006008001000<kL-S-BFGS-M (Init.2)24681012k02004006008001000<kL-S-BFGS-P (Init.2)7615r7615n7<k7615r7615n7<k2468101214k050010001500<kL-S-BFGS-M (Init.3)24681012k050010001500<kL-S-BFGS-P (Init.3)7615r7615n7<k7615r7615n7<k51015k02004006008001000<kL-S-BFGS-M (Init.4)24681012k02004006008001000<kL-S-BFGS-P (Init.4)7615r7615n7<k7615r7615n7<k0.512481632=00.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4S-BFGS-M0.51248163264128256=00.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4S-BFGS-MFig. 6 Comparison of four initialization strategies of L-S-BFGS-M from (13) with the full-
recursive method S-BFGS-M (corresponding to (5)) on all 62 CUTEst problems from [20].
The limited memory parameter is m = 50. Left: number of iterations; right: time.

blue), a limited memory version with memory of only m = 8, does compara-
tively well. In particular, this strategy is able to solve one more problem, as
indicated by the stair step at the right end of the plot.

Figure 6 shows the results for the limited memory parameter m = 50.
A larger limited memory parameter makes using limited memory structured
matrices more computationally expensive but is also expected to increase the
accuracy of the quasi-Newton approximations.

Note that the outcomes of S-BFGS-M (black) in Figure 6 are the same as
those in Figure 5, because it does not depend on the memory parameter. For
the limited memory versions we observe that the outcomes of L-S-BFGS-M2
(green) improve notably, whereas the other limited memory versions remain
roughly unchanged. Using the initialization strategies (Init. 1 or Init. 2), lim-
ited memory solvers are able to solve one more problem than the full-memory
method can, as indicated by the highest ending lines in the plot. We suggest
that Init. 1 and Init. 2 (see Section 4.1.2) generate initialization parameters
σk that are on average larger than those generated by Init. 3 or Init. 4. These
larger values in turn result in shorter average step sizes, which appears advan-
tageous on general nonlinear problems.

4.2.2 Experiment II.B: L-S-BFGS-P

In Experiment II.B we compare the versions of Algorithm 2 using the initial-
ization strategies from (13) with the full memory recursive S-BFGS-P method
(6). The solvers are run on 55 of the 62 CUTEst problems from [20] for which
n ≤ 2500. Figure 7 contains the results for the limited memory parameter
m = 8:

We observe that for a relatively small memory parameter m = 8, L-S-
BFGS-M3 (Init. 3, purple) solves the most problems. L-S-BFGS-M4 (Init. 4,
red) requires the fewest iterations, as indicated by the highest circle on the
y-axis in the left panel of Figure 7.

21

0.512481632=00.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4S-BFGS-M0.51248163264128=00.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4S-BFGS-MFig. 7 Comparison of four initialization strategies of L-S-BFGS-P from (13) to the full-
recursive method S-BFGS-P (corresponding to (6)) on 55 CUTEst problems from [20]. The
limited memory parameter is m = 8. Left: number of iterations; right: time.

Fig. 8 Comparison of four initialization strategies of L-S-BFGS-P from (13) to the full-
recursive method S-BFGS-P (corresponding to (6)) on 55 CUTEst problems from [20]. The
limited memory parameter is m = 50. Left: number of iterations; right: time.

Figure 8 shows the results for the limited memory parameter m = 50.
A larger parameter makes using limited memory structured matrices more
computationally expensive but is also expected to increase the accuracy of the
quasi-Newton approximations.

Note that the outcomes of S-BFGS-P in Figure 8 are the same as in Figure
7, because the full-memory solver does not depend on the memory parameter.
For a larger memory m = 50, the outcomes of L-S-BFGS-P2 (green) and
L-S-BFGS-P4 (red) improve notably. Overall, L-S-BFGS-P4 solves the most
problems.

From the experiments in this section, we ﬁnd that initialization strategies
Init.1 and Init. 2 appear most desirable for L-S-BFGS-M, whereas Init. 4 and
Init. 2 appear most desirable for L-S-BFGS-P.

22

0.512481632=00.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4S-BFGS-P0.51248163264128256=00.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4S-BFGS-P0.512481632=00.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4S-BFGS-P0.51248163264128=00.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4S-BFGS-P4.3 Experiment III

This section describes one application of the methods in the context of ma-
chine learning. A 2nd similar application to PDE constrained optimization is
included, too. For all solvers we set m = 8 (memory parameter), (cid:15) = 1 × 10−6
((cid:107)gk(cid:107)∞ ≤ (cid:15)) and maximum iterations to 10,000. Since some of the problems in
this section are large we use the techniques described in Section 3.4 through-
out the experiments. Because some of the problems in this experiment are
very large, the recursive formulas from (5) and (6) (with m = ∞) cannot be
directly used on these problems. However, the limited memory compact repre-
sentations use the memory parameter m to threshold the computational and
memory cost and are therefore applicable.

4.3.1 Experiment III.A: Logistic Regressions

The problems in this section are deﬁned by smooth-structured objective func-
tions from machine learning, as described, for example, in [24]. In particular,
logistic regression problems use smooth objective functions for classiﬁcation
tasks (for instance, [5]), which often depend on a large number of data points
and many variables. The classiﬁcation problems are deﬁned by the data pairs
¯i ∈ Rn may be large, and the
¯i, yi}D
{d
so-called labels yi ∈ {−1, 1} are scalars. In [25] regularized logistic regression
problems are described in which the objective function is composed of two
terms. The optimization problems are formulated as

i=1, where the so-called feature vectors d

minimize
∈Rn

x
¯

λ
2

(cid:107)2
2 +

(cid:107)x
¯

D
(cid:88)

i=1

log (cid:0)1 + exp(−yix
¯

¯i)(cid:1) ,
T d

where λ > 0. The regularization term, λ
(cid:107)2, has a second derivative,λI
, that
¯
is readily available. Therefore, we deﬁne the known and unknown components
for this problem as

2 (cid:107)x
¯

) =

(cid:98)k(x
¯

λ
2

(cid:107)2
2,

(cid:107)x
¯

) =

(cid:98)u(x
¯

D
(cid:88)

i=1

log (cid:0)1 + exp(−yix
¯

¯i)(cid:1) .
T d

(20)

This data was obtained from www.csie.ntu.edu.tw/~cjlin/libsvm/ (re-
trieved on 10/03/19). Ten problems were used, with problem dimensions listed
in Table 2. Some of the problems are large, with n ≥ 5000 and thus we fo-
cus on the computations as described in Section 3.4.1. The regularization pa-
rameter is set as λ = 10−3. For comparison, we include IPOPT [26] with a
L-BFGS quasi-Newton matrix (we use a precompiled Mex ﬁle with IPOPT
3.12.12, MUMPS and MA57). We specify the limited memory BFGS option
for IPOPT using the setting hessian approximation= ‘limited memory’
and tolerances by tol=9.5e-10 and acceptable tol = 9.5e-10. The results
of the experiments are shown in Figure 9. We observe that all solvers, ex-
cept for L-S-BFGS-M2, solve the same total number of problems. Moreover,

23

Table 2 List of dimensions for 10 LIBSVM logistic regression problems. Here D denotes
i=1, and n denotes the number of variables/feature
the number of training pairs {d
weights (the size of the problem).

¯i, yi}D

Problem
rcv1
duke
gisette
colon cancer
leukemia
real sim
madelon
w8a
mushrooms
a9a

D
20242
34
6000
62
38
72309
2000
49749
2000
32561

n
47236
7129
5000
2000
7129
20958
500
300
500
123

Fig. 9 Comparison of L-S-BFGS-M solvers on 10 logistic regression classiﬁcation problems
using data from LIBSVM. Left: number of iterations, right: time.

the structured L-BFGS solvers tend to use fewer iterations and overall less
computational time than IPOPT’s L-BFGS method.

Next, we describe experiments for optimal control problems with similar

structures.

4.3.2 Experiment III.B: Optimal Control Problems

This experiment describes a typical situation in PDE constrained optimiza-
tion. In particular, if the PDE is nonlinear, then we can compute gradients
eﬃciently using the adjoint equation, but Hessians of the unknown part can-
not be computed eﬃciently. Denoting u as the horizontal axis and v as the
vertical axis, then 2D Poisson problems, with an unknown control x(u, v), are
deﬁned by the diﬀerential equation: yuu + yvv = x. The solution y(u, v) has
known boundary values on a box (u, v) ∈ [0, 1]2; in other words, y(0, v), y(1, v),
y(u, 0), and y(u, 1) are known. Discretizing the domain and splitting it into
an interior and boundary part, we get for the optimal control problem

minimize
∈Rn

x
¯

1
2

(cid:110)

(cid:107)x
¯

2 + (cid:13)
(cid:107)2
(cid:13)y
¯

(x
¯

) − y
¯

∗(cid:13)
2
(cid:13)
2

(cid:111)

subject to A
¯

y
¯

= x
¯

,
+ g
¯

24

0.250.51248=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4IPOPT0.250.51248=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4IPOPTFig. 10 Comparison of L-S-BFGS-M solvers on PDE constrained optimal control problems.
The dimensions of the problems are n = (10 × j − 2)2 for j = 2, 3, · · · , 10. Left: number of
iterations, right: time. L-S-BFGS-M0 represents an eﬃcient implementation of Algorithm 1
in which the initialization is the constant σk = 0.

∈ Rn×n
∈ Rn represents a vector with boundary information, A
where g
¯
¯
is a matrix resulting from a 5-point stencil ﬁnite diﬀerence discretization of
∗ are ﬁxed data values. Because the Hessian of
the partial derivatives, and y
¯
the regularization term, 1
(cid:107)2
2 (cid:107)x
2, is straightforward to compute, we deﬁne the
¯
structured objective function by

) =

(cid:98)k(x
¯

1
2

(cid:107)2
2,

(cid:107)x
¯

) =

(cid:98)u(x
¯

1
2

(cid:13)
(cid:13)y
¯

(x
¯

∗(cid:13)
2
(cid:13)
2

) − y
¯

,

(21)

−1(x
¯

(x
¯

) = A
¯

). The number of variables is deﬁned by the formula
+ g
using y
¯
¯
n = (10 × j − 2)2, where j = 2, 3, · · · , 10, which corresponds to discretizations
with 20, 30, · · · , 100 mesh points in one direction. The largest problem has
n = 9604 variables. For comparison we also include the implementation of a
“standard” BFGS method from [20], which uses the same line search as do the
limited memory structured methods and IPOPT’s L-BFGS method.

4.4 Experiment IV

In this experiment the structured solvers are compared to IPOPT [26] with
an L-BFGS quasi-Newton matrix (we use a precompiled Mex ﬁle with IPOPT
3.12.12 that includes MUMPS and MA57 libraries). The objective function is
a structured quartic function

f (x
¯

) = (cid:98)k(x
¯

),
)+ (cid:98)u(x
¯

) =

(cid:98)k(x
¯

1
12

n
(cid:88)

(a2

i x4

i +12xigi),

i=1

) =

(cid:98)u(x
¯

1
2

n
(cid:88)

i=1

qix2

i , (22)

where the data ai, gi and qi are random normal variables with n = j × 100, 1 ≤
j ≤ 7. The starting values are all ones, i.e., x0 = 1
. We specify the limited
¯
memory BFGS option for IPOPT using the setting hessian approximation=
‘limited memory’ and tolerances by tol=9.5e-10 and acceptable tol =
9.5e-10. For all solvers we set m = 8 (memory parameter), and maximum

25

0.250.51240.20.40.60.81s()0.250.512481632640.20.40.60.81s()Fig. 11 Comparison of L-S-BFGS-P on structured objective functions to IPOPT and L-
BFGS-B. Left: number of iterations, right: time.

iterations to 10,000. A solver is regarded to have converged when (cid:107)gk(cid:107)∞ ≤
9.5 × 10−5. The average outcomes of 5 runs of the experiments are in Figure
11.

IPOPT and the L-S-BFGS-P solvers converge to the speciﬁed tolerances
on all problems. The outcomes of the number of iterations (left plot) and
computational times (right plot) in Figure 11 are consistent. In particular, we
observe that the diﬀerences in the number of iterations are roughly reﬂected in
the diﬀerence in the computational times. In this problem the known Hessian
is sensitive to changes in x
, and including second-order information in the
¯
quasi-Newton approximations yields outcomes with fewer iterations.

4.5 Experiment V

This experiment describes the application of the structured compact BFGS
methods on an imaging problem. Large-scale imaging problems are challeng-
ing, because they involve large amounts of data and high-dimensional param-
eter space. Typically, image reconstruction problems are formulated as opti-
mization problems. In [13], eﬃcient gradient-based quasi-Newton techniques
for large-scale ptychographic phase retrieval are described. However, even if
the objective function is not directly formulated as in problem (4) it may still
be possible to exploit known 2nd derivatives. Let z = x + yi ∈ Cn2
be the ob-
ject of interest, and dj ∈ Rm2
be the observed data (or intensities) measured
from the jth probe, where n2 and m2 are the dimensions of the vectorized
object and data resolution images, respectively. A ptychography experiment
is modeled by

dj = |F(Qjz)|2 + (cid:15)j,

j = 1, . . . , N,

(23)

where N is the total number of probes (or scanning positions), F : Cm2
is the two-dimensional discrete Fourier operator, Qj ∈ Cm2×n2
(a diagonal illumination matrix), and (cid:15)j ∈ Rm2

(cid:55)→ Cm2
is the kth probe
is the noise corresponding to

26

0.512481632=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4IPOPT0.250.51248163264=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P4IPOPTthe kth measurement error. The diagonal elements of Qj are nonzero in the
columns corresponding to the pixels being illuminated in the object at scanning
step j. There are diﬀerent ways for formulating the reconstruction problem.
One such formulation is the amplitude-based error metric

minimize
z
¯

f (z) =

1
2

N
(cid:88)

j=1

(cid:13)
(cid:13)|F(Qjz)| − (cid:112)dj
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

=

1
2

N
(cid:88)

j=1

rT
j rj,

(24)

where rj = |F(Qjz)| − (cid:112)dj. Let dj = (cid:112)dj. Here, f : Cn2
(cid:55)→ R is a real-
valued cost function deﬁned on the complex domain, and is therefore not
complex-diﬀerentiable [21]. To overcome the lack of complex-diﬀerentiability,
it is common to employ the notion of CR (Wirtinger) Calculus, where the
derivatives of the real and imaginary parts of z are computed independently
[21,23]. For these real-valued functions, the mere existence of these Wirtinger
derivatives is necessary and suﬃcient for the existence of a stationary point
[3,21,23]. Using Wirtinger calculus (using zj = FQjz), the partial gradients
for (24) can be computed as

∇zrj = Jj = diag(zj/ |zj|)FQj,

∇zf =

N
(cid:88)

j=1

J ∗
j rj =

N
(cid:88)

j=1

Q∗

j F ∗diag(zj/ |zj|) (|zj| − dj) ,

(25)

Hessian. To compute the Hessian matrix, let

T1,j = Q∗

j F ∗diag(dj/ |zj|)FQj,

and

then

=

H
¯

N
(cid:88)

j=1

T2,j = Q∗

j F ∗diag(dj (cid:12) z2

j / |zj|3)FQj,

Q∗





j Qj − Re(T1,j) + Re(T2,j)
1,j) + Im(T ∗

Im(T ∗

2,j)

Im(T1,j) + Im(T2,j)

Q∗

j Qj − Re(T1,j) − Re(T2,j)



 , (26)

where the known 2nd derivatives are K
¯

= (cid:80)N

j=1

(cid:20) Q
¯

∗

j

Q
¯ j

(cid:21)

∗

j

Q
¯

Q
¯ j

and the re-

maining block elements of H
¯

are estimated.

Deﬁning the vectorization from complex to real variables by x
¯

) ≡
= vecR(z
¯
2 ]T , we deﬁne the vectors for the
)), where vec(x1, x2) = [ xT
vec(Re(z
), Im(z
¯
¯
structured BFGS methods by yk = vecR(∇f (zk+1)) − vecR(∇f (zk)), sk =
vecR(zk+1) − vecR(zk), (cid:98)uk = yk − K
¯

sk and

1 xT

uk = (cid:98)uk + K
¯

sk = yk.

Using these vectors, we can form the compact structured BFGS matrices. In
this experiment, we compare a limited memory structured BFGS method (L-
S-BFGS) and limited memory BFGS (L-BFGS) method in Figure 12. The

27

Fig. 12 Comparison of log objective function values on a Ptychographic Phase Retrieval
problem for the ﬁrst 250 iterations of 2 L-BFGS solvers. The L-S-BFGS method, using
, converges faster than the classical L-BFGS method with an
the known derivatives in K
¯
identity initialization. The computational cost for the search directions in this problem
scales according to n with limited memory.

image dimensions are (cid:98)n = 50 so that the total number of real variables is n =
2 · (cid:98)n2 = 5, 000. Moreover, (cid:98)m = 16 so that (cid:98)m2 × (cid:98)n2 = 256 × 5, 000 = 1, 280, 000,
and N = 16. Because of the structure of the known Hessian l = 1 (cf. Table 1),
and solves with this matrix are done on the order of O(n). Because of the size
of this problem, and the corresponding computational/memory requirements
the recursive update formulas from eqs. (5) and (6) are not applicable, yet the
limited memory techniques threshold the required computational resources.

5 Conclusions

In this article we develop the compact representations of the structured BFGS
formulas proposed in Petra et al. [20]. Limited memory versions of the com-
pact representations with four non-constant initialization strategies are imple-
mented in two line search algorithms. The proposed limited memory compact
representations enable eﬃcient search direction computations by the Sherman-
Morrison-Woodbury formula and the use of eﬃcient initialization strategies.
The proposed methods are compared in a collection of experiments, which

28

050100150200250k-20-15-10-50log(f(zk))PtychographicPhaseRetrievalk=mL-S-BFGS(m=8)L-BFGS(m=8)Fig. 13 Comparison of initialization strategies for L-S-BFGS-M on problems with eigenval-
ues clustered around 1,000 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1000. Left: number
of iterations; right: time.

include the original full-memory methods. The structured methods typically
require fewer total iterations than do the unstructured approaches. Among
the four proposed initialization strategies, initializations 1 and 2 appear best
for the structured minus methods (L-S-BFGS-M), whereas initializations 4
and 2 appear robust for the structured plus (L-S-BFGS-P) methods. In an
array of applications, including a large-scale real world imaging problem, the
proposed structured limited memory methods obtain better numerical results
than conventional unstructured methods.

Acknowledgements This work was supported by the U.S. Department of Energy, Oﬃce
of Science, Advanced Scientiﬁc Computing Research, under Contract DE-AC02-06CH11357
at Argonne National Laboratory. through the Project ”Multifaceted Mathematics for Com-
plex Energy Systems.” This work was also performed under the auspices of the U.S. De-
partment of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-
07NA27344.

Appendix A: Initialization Comparison with φ = 1000

In Section 4.1, the four L-S-BFGS-M initializations were compared on struc-
tured quadratic objective functions with eigenvalues clustered around 1, whereas
in this section the eigenvalues are clustered around 1000. In particular, φ =
¯ i are uniformly distributed in the interval [−999, 0].
1000, and the elements of D
The results are displayed in Figure 13. For the large clustered eigenvalues Init.
1 and 3 require the fewest iterations, while Init. 3 appears fastest overall.
For L-S-BFGS-P the computations with φ = 1000 are in Figure 14 In the
comparison of L-S-BFGS-P, Init. 2 and Init. 3 do best in iterations.

References

1. Barzilai, J., Borwein, J.: Two-Point Step Size Gradient Methods.

IMA Journal of

Numerical Analysis 8(1), 141–148 (1988). DOI 10.1093/imanum/8.1.141

29

0.5124=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M40.5124=0.20.40.60.81;s(=)L-S-BFGS-M1L-S-BFGS-M2L-S-BFGS-M3L-S-BFGS-M4Fig. 14 Comparison of initialization strategies for L-S-BFGS-P on problems with eigenval-
ues clustered around 1,000 with 1 ≤ λr ≤ 1000 and λr+1 = · · · = λn = 1000. Left: number
of iterations; right: time.

2. Boggs, P., Byrd, R.: Adaptive, limited-memory bfgs algorithms for unconstrained op-
timization. SIAM Journal on Optimization 29(2), 1282–1299 (2019). DOI 10.1137/
16M1065100. URL https://doi.org/10.1137/16M1065100

3. Brandwood, D.H.: A complex gradient operator and its application in adaptive array
theory. IEE Proceedings F - Communications, Radar and Signal Processing 130(1),
11–16 (1983)

4. Broyden, C.G.: The Convergence of a Class of Double-rank Minimization Algorithms
1. General Considerations. IMA Journal of Applied Mathematics 6(1), 76–90 (1970).
DOI 10.1093/imamat/6.1.76. URL https://doi.org/10.1093/imamat/6.1.76

5. Byrd, R.H., Chin, G.M., Neveitt, W., Nocedal, J.: On the use of stochastic hessian infor-
mation in optimization methods for machine learning. SIAM Journal on Optimization
21, 977–995 (2011). DOI 10.1137/10079923X

6. Byrd, R.H., Nocedal, J., Schnabel, R.B.: Representations of quasi-Newton matrices and

their use in limited-memory methods. Math. Program. 63, 129–156 (1994)

7. Chang, C.C., Lin, C.J.: Libsvm: A library for support vector machines. ACM Trans.
Intell. Syst. Technol. 2(3), 27:1–27:27 (2011). DOI 10.1145/1961189.1961199. URL
http://doi.acm.org/10.1145/1961189.1961199

8. Dennis, J., Mor´e, J.: Quasi-newton methods, motivation and theory. SIAM Review 19,

46–89 (1977)

9. Dennis Jr., J.E., Gay, D.M., Walsh, R.E.: An adaptive nonlinear least-squares algorithm.
ACM Trans. Math. Softw. 7(3), 348–368 (1981). DOI 10.1145/355958.355965. URL
http://doi.acm.org/10.1145/355958.355965

10. Dennis Jr, J.E., Martinez, H.J., Tapia, R.A.: Convergence theory for the structured bfgs
secant method with an application to nonlinear least squares. J. Optim. Theory Appl.
61(2), 161–178 (1989). DOI 10.1007/BF00962795. URL https://doi.org/10.1007/
BF00962795

11. Dolan, E., Mor´e, J.: Benchmarking optimization software with performance proﬁles.

Mathematical Programming 91, 201–213 (2002)

12. Fletcher, R.: A new approach to variable metric algorithms. The Computer Journal
13(3), 317–322 (1970). DOI 10.1093/comjnl/13.3.317. URL https://doi.org/10.1093/
comjnl/13.3.317

13. Fung, S.W., WendyDi, Z.: Multigrid optimization for large-scale ptychographic phase
retrieval. SIAM Journal on Imaging Sciences 13(1), 214–233 (2020). DOI 10.1137/
18M1223915

14. Gill, P.E., Murray, W.: Algorithms for the solution of the nonlinear least-squares prob-

lem. SIAM J. Numer. Anal. 11, 311–365 (2010)

15. Goldfarb, D.: A family of variable-metric methods derived by variational means. Math.
Comp. 24, 23–26 (1970). DOI 10.1090/S0025-5718-1970-0258249-6. URL https://
doi.org/10.1090/S0025-5718-1970-0258249-6

30

0.5124=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P40.5124=0.20.40.60.81;s(=)L-S-BFGS-P1L-S-BFGS-P2L-S-BFGS-P3L-S-BFGS-P416. Gould, N.I.M., Orban, D., Toint, P.L.: CUTEr and SifDec: A constrained and uncon-
strained testing environment, revisited. ACM Trans. Math. Software 29(4), 373–394
(2003)

17. Mahajan, A., Leyﬀer, S., Kirches, C.: Solving mixed-integer nonlinear programs by qp
diving. Technical Report ANL/MCS-P2071-0312, Mathematics and Computer Science
Division, Argonne National Laboratory, Lemont, IL (2012)

18. Mor´e, J.J., Thuente, D.J.: Line search algorithms with guaranteed suﬃcient decrease.

ACM Trans. Math. Softw. 20(3), 286–307 (1994). DOI 10.1145/192115.192132

19. Nocedal, J.: Updating quasi-Newton matrices with limited storage. Math. Comput. 35,

773–782 (1980)

20. Petra, C., Chiang, N., Anitescu, M.: A structured quasi-newton algorithm for optimizing
with incomplete hessian information. SIAM Journal on Optimization 29(2), 1048–1075
(2019). DOI 10.1137/18M1167942. URL https://doi.org/10.1137/18M1167942
21. Remmert, R.: Theory of Complex Functions. Springer-Verlag New York (1991)
22. Shanno, D.F.: Conditioning of quasi-Newton methods for function minimization. Math.
Comp. 24, 647–656 (1970). DOI 10.1090/S0025-5718-1970-0274029-X. URL https:
//doi.org/10.1090/S0025-5718-1970-0274029-X

23. Sorber, L., Barel, M.V., Lathauwer, L.D.: Unconstrained optimization of real functions
in complex variables. SIAM Journal on Optimization 22(3), 879–898 (2012). DOI
10.1137/110832124. URL https://doi.org/10.1137/110832124

24. Sra, S., Nowozin, S., Wright, S.J.: Optimization for Machine Learning. The MIT Press

(2011)

25. Teo, C.H., Vishwanthan, S., Smola, A.J., Le, Q.V.: Bundle methods for regularized risk

minimization. J. Mach. Learn. Res. 11, 311–365 (2010)

26. W¨achter, A., Biegler, L.T.: On the implementation of an interior-point ﬁlter line-search

algorithm for large-scale nonlinear programming. Math. Program. 106, 25–57 (2006)

27. Yabe, H., Takahashi, T.: Factorized quasi-newton methods for nonlinear least squares
problems. Mathematical Programming 11(75) (1991). DOI 10.1007/BF01586927
28. Zhu, C., Byrd, R., Nocedal, J.: Algorithm 778: L-BFGS-B: Fortran subroutines for large-
scale bound-constrained optimization. ACM Transactions on Mathematical Software
23, 550–560 (1997)

31

