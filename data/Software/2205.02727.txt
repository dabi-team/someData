Replicating Data Pipelines with GrimoireLab
Kalvin Eng, Hareem Sahar
{kalvin.eng,hareeme}@ualberta.ca
University of Alberta
Edmonton, Canada

2
2
0
2

y
a
M
5

]
E
S
.
s
c
[

1
v
7
2
7
2
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
In this paper, we present our MSR Hackathon 2022 project that repli-
cates an existing Gitter study [2] using GrimoireLab. We compare
the previous study’s pipeline with our GrimoireLab implementation
in terms of speed, data consistency, organization, and the learning
curve to get started. We believe our experience with Grimoire-
Lab can help future researchers in making the right choice while
implementing their data pipelines over Gitter and Github data.

KEYWORDS
Gitter, developer discussions, GrimoireLab

ACM Reference Format:
Kalvin Eng, Hareem Sahar. 2022. Replicating Data Pipelines with Grimoire-
Lab. In 19th International Conference on Mining Software Repositories (MSR
’22), May 23–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 3 pages.
https://doi.org/10.1145/3524842.3528524

1 INTRODUCTION
Developer chat rooms such as Slack and Gitter are frequently used
for project specific discussions. These discussions contain a wealth
of information that can be leveraged to facilitate better software de-
velopment and management. Gitter is a chat platform that is mainly
centered around Github repositories which can offer a wealth of
information such as informal discussions centered around issues of
a project. The mining of this data surrounding a project is a non-
trivial task as it spans through Github and Gitter with disparate
APIs and schemas.

As such, tools like GrimoireLab [1] have been developed to sup-
port tasks such as data retrieval, processing, and visualization from
software repositories, and other relevant sources. Consequently,
researchers and analysts can retrieve large datasets in an efficient
way, without reinventing the wheel, and at the same time ensuring
easier replicability of their work.

The primary goal of our hackathon project was to replicate the
data pipeline of an existing study by Sahar et al. [2] that investi-
gates chat discussions among developers in the Gitter platform.
Sahar et al. [2] investigated: how frequently issues are mentioned
in Gitter chat; reasons why issues are posted in chat; whether or
not posting issues in chat affects their resolution time; and whether
mentioning issues in chat are correlated with number of Github

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00
https://doi.org/10.1145/3524842.3528524

issue comments. We do not fully replicate their study as some anal-
ysis steps involved manual labelling, rather we investigate whether
the use of GrimoireLab makes the data collection process easier. To
this end, we use several GrimoireLab components to replicate the
pipeline and answer some of the data distribution related questions
from the original paper.

2 PREVIOUS DATA PIPELINE

Figure 1: Methodology of Sahar et al. [2] pipeline.

Sahar et al. [2] gathered data from 24 Github repositories and
their associated Gitter chat rooms. Figure 1 shows their data pipeline:
(1) Data Retrieval - The Gitter API was used to extract chat
logs. The chat logs, which were in JSON format, were parsed
to obtain issue references. The issues and their metadata was
extracted from Github using the Github API.

(2) Data Storage - The chat logs were stored in JSON whereas

issues were stored in CSV files.

(3) Identities Management - The Gitter and Github username
and displayName are compared to resolve aliases. For this,
Levenshtein distance was used and later manual analysis
was done.

(4) Analytics - An analysis was done on the aligned and cleaned
data to answer four research questions, one of which in-
volved open coding to manually label purpose of issue refer-
ences in Gitter.

The data pipeline was implemented using a mix of R, Python,
and bash scripts. The dataset and scripts of this process can be
found on Github. 1
1https://github.com/Hareem-E-Sahar/gitter

Gathering Chat LogsChat logs of 24Gitter chat rooms inJSON formatExtract chat logsfrom 24 chatrooms  Parsing Chat Messages Parse JSON files to extract relevantdata fields fromeach chat message Usernames anddisplaynamesIssue numberRepo nameGitter's RESTAPI  Resolving Aliases Determine namesimilarity using LevenshteinDistance  Manually inspect allnames with 60%similarity   Uniqueidentifiers  for names Analyzing Issues DistributionString matchingon repo names  Issues from 24studied projects  8911 Foreign Issues 3,583 Issues fromrelevant projects 4171 1234 
 
 
 
 
 
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Eng and Sahar

3 GRIMOIRELAB PIPELINE

Figure 2: Components of GrimoireLab used in our study.

To use GrimoireLab, we choose the Docker Compose approach
which orchestrates all the relevant Docker containers needed to
work with GrimoireLab. This approach allows users to deploy Gri-
moireLab without installing any dependencies on the host machine.
In particular, the SirMordred container contains all the tools
needed for data retrieval, data storage, identity management, and
managing dashboards. MariaDB and ElasticSearch containers are
used for data storage in the form of inverted indexes, whereas the
Kibiter container is used to create dashboards and to visualize the
collected data. Figure 2 shows the GrimoireLab components used
in this work:

(1) Data Retrieval - Raw data is collected via Perceval from
the Github API, Gitter API, and Github git repositories and
inserted into an ElasticSearch index.

(2) Data Storage - GrimoireElk enriches the raw data scraped in
the previous step, e.g., identifying the issues and pull requests
in a Gitter message and assigning identities determined in
(3).

(3) Identities Management - SortingHat helps to manage iden-
tities retrieved from different data sources and allows for
similar identities to be merged together. HatStall is used as
the user web interface for viewing and merging the identi-
ties.

(4) Analytics - Kibiter is a fork of the Kibana dashboard that
helps to creates data visualizations from ElasticSearch in-
dices. To enhance Kibiter, premade dashboard panels called
Sigils are used. For backing up and loading panels, Kidash is
used.

SirMordred serves as the main program to orchestrate task execution
among the components. The scripts used to generate the dataset
can be found on Github. 2

3.1 Adaptations
In order to replicate the results of the previous pipeline, some
adaptations were needed to made for GrimoireLab to enrich the
Gitter data correctly and align Github and Gitter identities.

Gitter Data Enrichment. We found that GrimoireElk, which
enriches the raw data of Gitter, did not accurately classify issues
and pull requests in Gitter messages similar to the previous pipeline
implementation. Therefore, we adapt GrimoireElk to identify the is-
sues and pull requests from the Gitter raw data via querying Github.
In line with Sahar et al. [2], we also classify the source repository of
the issue and pull requests into 3 categories: project (repository is
directly related to the Gitter room), parent (repository is related to
the parent organization of the current project discussion in Gitter
room), and foreign (has no relation to the parent organization or
repository in the Gitter Room).

Identity Alignment. Although SortingHat provides matching
techniques for identities, we find that it can be insufficient for iden-
tities that might have similar names or usernames. Therefore, we
implement a detection script that queries the SortingHat identities
that have not been merged yet and find identities that have a nor-
malized Levenshtein distance greater than 0.7 for both names and
usernames. The detected pairs of identities can then be merged
using the HatStall web interface if the end user decides that they
are similar enough.

4 PRELIMINARY RESULTS
Using GrimoireLab we replicate the previous pipeline on 7 out
of the 24 Gitter rooms3: amberframework/amber, aws/aws-sdk-go,
patchthecode/JTAppleCalendar, mailboxer/mailboxer, PerfectlySoft/Pe-
rfect, kriasoft/react-starter-kit, and shuup/shuup.

Figure 3 shows the issue references found by Sahar et al. [2] to
the ones we found via GrimoireLab. We retrieved a more issues
using GrimoireLab per project as it collects data until February
2022 whereas original study used data until November 2019. For our
comparisons, we use data from both pipelines up until November
2019.

Figure 3: Count of Gitter API issues.

We compare the resolution time of issues and pull requests ref-
erenced in Gitter rooms as shown in Figure 4. We can see that the
boxplots are relatively similar with some minor differences which
is attributed to the different second-precision for the calculated
resolution times.

In Figure 5, we show the comments change ratio which is com-
puted by dividing the number of comments found in the Github
issue tracker before being referenced in Gitter and by the num-
ber after within one week. The previous study did not include the
comments posted on the day that the Gitter issue reference was
made. As a result, their results were different than ours which can

2https://github.com/k----n/GrimoireGitter

3The data of 4 Gitter rooms used in the previous study are no longer available.

1234amberaws-sdk-goJTAppleCalendarmailboxerPerfectreact-starter-kitshuup101102Number of Issues (log scale)57952141436265526696114443626656PreviousGrimoireLabReplicating Data Pipelines with GrimoireLab

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

can be accessed via a single ElasticSearch API. Moreover, identities
can be easily managed via a web interface with HatStall or via the
command line with SortingHat.

By comparison, the previous pipeline used a collection of CSV
and JSON files to store the retrieved and processed data. Multiple
scripts were used to extract data, align identities, remove incon-
sistencies, and finally compute results. Having a non-standardized
collection of files with differing schemas makes querying for data
more difficult and could possibly lead to inconsistencies. This be-
came very clear when we computed the after and before comments
ratio in Figure 5 and found differing results. Furthermore, we also
found that resolution times in Figure 4 were not as precise in the
previous pipeline compared to the GrimoireLab data likely due to
dates in the previous pipeline being processed through multiple
scripts. We found that GrimoireLab produced more accurate data
from a single source, as opposed to the data being “lost in transla-
tion” in the previous pipeline from using multiple scripts and files
to produce the data.

Learning Curve. GrimoireLab is a diverse collection of tools
separated into microservices, so it took us a lot of time to become
familiar with the software stack. We had to read extensive docu-
mentation and source code to understand the particular use cases
of the GrimoireLab components. Furthermore, familiarity is also re-
quired in software such as Docker to bootstrap the pipeline quickly
without the need to install software dependencies. ElasticSearch
familiarity is also needed to help query the retrieved data in Gri-
moireLab. Learning the different schemas of ElasticSearch indices
created with GrimoireLab can be quite daunting, but once familiar-
ized with the indices, powerful visualizations can be created in the
Kibiter dashboard to explore data.

In comparison, the previous pipeline’s implementation is a col-
lection scripts that requires no need for orchestration. Instead, R,
Python, and bash scripts and executed in a consequential manner
which makes the implementation much simpler. However, once the
number of projects to analyze grow much larger, we anticipate that
GrimoireLab will be much easier to work with as there is already
built-in support for retrieving data from multiple sources and a
singular API for querying the data. We confirmed this by retrieving
and processing issue comments, which we found was easier with
GrimoireLab.

6 CONCLUSION
In this paper, we describe our experience with using GrimoireLab
to implement an existing pipeline Sahar et al. [2]. Our preliminary
results indicate that the GrimoireLab pipeline is superior compared
to a traditional pipeline in terms of data consistency and organi-
zation, but it offers a steep learning curve. A complete replication
package of our GrimoireLab usage can be found on Github here:
https://github.com/k----n/GrimoireGitter

REFERENCES
[1] Santiago Dueñas, Valerio Cosentino, Jesus M Gonzalez-Barahona, Alvaro
del Castillo San Felix, Daniel Izquierdo-Cortazar, Luis Cañas-Díaz, and Al-
berto Pérez García-Plaza. 2021. GrimoireLab: A toolset for software development
analytics. PeerJ Computer Science 7 (2021), e601.

[2] Hareem Sahar, Abram Hindle, and Cor-Paul Bezemer. 2021. How are issue reports
discussed in Gitter chat rooms? Journal of Systems and Software 172 (2021), 110852.

Figure 4: Resolution time comparison between previous
pipeline and GrimoireLab.

Figure 5: Ratio of number of issue comments in Github one
week after and before issue reference in Gitter.
be seen from the median of boxplots and the outliers. GrimoireLab
precisely computes the dates of comments to a second level gran-
ularity which allowed us to compute results with more precision
than the previous study [2].

5 COMPARISON OF APPROACHES
We anecdotally compare the pros and cons of adopting a traditional
pipeline approach [2] to the GrimoireLab pipeline in terms of speed,
data consistency, organization, and the learning curve.

Speed. We only choose 7 Github repositories for the replication
since they contain at most 4214 issues and pull requests (as of Feb-
ruary 3, 2022) making it manageable to be scraped from Github.
Large Github projects are difficult to scrape with GrimoireLab as it
is designed to scrape all data from Github which requires many to-
kens4. With a few tokens, the time to retrieve large Github projects
with issues and pull requests in the tens of thousands would likely
take days to retrieve. In comparison, Sahar et al. [2] extracted only
specific issues and pull requests mentioned in Gitter, which are
relatively small in number. As a result they were less limited by the
API rate limits and could obtain data from large projects relatively
quickly.

In terms of data processing speed, the GrimoireLab data enrich-
ment process can take a long time due to the sheer volume that it
must process — it took approximately 3 hours to create complete
enriched indices for the 7 repositories. Comparatively, the previous
pipeline can be more selective in data processing, hence it could be
significantly faster.

Data Consistency and Organization. We find that Grimoire-
Lab helps to keep data more organized out of the box by providing
tools to centralize data into ElasticSearch, as well as manage identi-
ties in a MariaDB database. All of the collected and processed data

4Github API allows 5,000 requests per hour against each token.

llllll110100100010000amberaws−sdk−goJTAppleCalendarmailboxerPerfectreact−starter−kitshuupResolution Time in Hours (log scale)PreviousGrimoireLabllllllllllllllllllllllllllllllllllllllll01020amberaws−sdk−goJTAppleCalendarPerfectreact−starter−kitshuupComments Change RatioPreviousGrimoireLab