Automatic Comment Generation via Multi-Pass Deliberation
Xiao Chenâˆ—â€ 
chenxiao2021@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China

Fangwen Muâˆ—â€ 
fangwen2020@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China

Lin Shiâˆ—â€ â€¡
shilin@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China

2
2
0
2

p
e
S
4
1

]
E
S
.
s
c
[

1
v
4
3
6
6
0
.
9
0
2
2
:
v
i
X
r
a

Song Wang
wangsong@eecs.yorku.ca
Lassonde School of Engineering, York
University
Toronto, Canada

Qing Wangâˆ—â€ â€¡Â§
wq@iscas.ac.cn
Institute of Software, Chinese
Academy of Sciences
Beijing, China

ABSTRACT
Deliberation is a common and natural behavior in human daily
life. For example, when writing papers or articles, we usually first
write drafts, and then iteratively polish them until satisfied. In light
of such a human cognitive process, we propose DECOM, which
is a multi-pass deliberation framework for automatic comment
generation. DECOM consists of multiple Deliberation Models and
one Evaluation Model. Given a code snippet, we first extract key-
words from the code and retrieve a similar code fragment from a
pre-defined corpus. Then, we treat the comment of the retrieved
code as the initial draft and input it with the code and keywords
into DECOM to start the iterative deliberation process. At each
deliberation, the deliberation model polishes the draft and gener-
ates a new comment. The evaluation model measures the quality
of the newly generated comment to determine whether to end the
iterative process or not. When the iterative process is terminated,
the best-generated comment will be selected as the target comment.
Our approach is evaluated on two real-world datasets in Java (87K)
and Python (108K), and experiment results show that our approach
outperforms the state-of-the-art baselines. A human evaluation
study also confirms the comments generated by DECOM tend to
be more readable, informative, and useful.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software notations and
tools; â€¢ Computing methodologies â†’ Artificial intelligence.

KEYWORDS
Comment generation, Information retrieval, Deep neural network

âˆ—Also With Laboratory for Internet Software Technologies, Institute of Software, CAS
â€ Also With University of Chinese Academy of Sciences
â€¡Corresponding author
Â§Also With Science & Technology on Integrated Information System Laboratory,
Institute of Software, CAS

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9475-8/22/10.
https://doi.org/10.1145/3551349.3556917

ACM Reference Format:
Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. 2022. Auto-
matic Comment Generation via Multi-Pass Deliberation. In 37th IEEE/ACM
International Conference on Automated Software Engineering (ASE â€™22), Oc-
tober 10â€“14, 2022, Rochester, MI, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3551349.3556917

1 INTRODUCTION
With software growing in size and complexity, developers tend to
averagely spend around 59% of their effort on program compre-
hension during software development and maintenance [32, 62].
Source code comments provide concise natural language descrip-
tions of code snippets, which not only greatly reduce the effort for
developers to understand the code, but also play a vital role in soft-
ware maintenance and evolution. However, manually commenting
code is time-consuming, and code comments are often missing or
outdated in software projects [13, 30]. Therefore, the code com-
ment generation task, which aims at automatically generating a
high-quality comment for a given code snippet, has long attracted
the interest of many researchers.

Most of the existing approaches treat comment generation as
a machine translation task and adopt a one-pass encoder-decoder
process, i.e., first encode the input code into a sequence of seman-
tic features, then decode the features to a natural language com-
ment [26, 28, 34, 64]. Although the encoder-decoder framework has
achieved remarkable performance on the comment generation task,
it still suffers from two major limitations. The first one is that such
models adopt a regular one-pass decoding process that sequentially
generates comments word by word. They directly use the gener-
ated comment as the final output, which results in their inability to
correct the mispredicted words. The words mistakenly predicted in
the early steps may lead to error accumulation under the constraint
of the language model. Taking the auto-generated comments in
Figure 1 as an example, the one-pass model Re2com [58] incorrectly
predicts the sixth word â€œprobabilityâ€ as â€œbytesâ€, which leads the
model to keep exploring the words related to â€œbytesâ€ when predict-
ing the consequential words. As a result, the related words â€œwritten
to this streamâ€ are mistakenly generated, which results in a typical
example of error accumulation. The second limitation is that they
generate comments sequentially. Thus, such sequential one-pass
models cannot leverage the global information of the generated
comment to further polish its local content. As the example shown

 
 
 
 
 
 
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

the state-of-the-art baselines by 8.3%, 6.0%, 13.3%, and 10.5% with
respect to BLEU-4, ROUGE-L, METEOR, and CIDEr on Java dataset.
On Python dataset, DECOM improves the performance on BLEU-
4, ROUGE-L, METEOR, and CIDEr by 5.8%, 3.8%, 6.6%, and 6.3%,
respectively. We also conduct a human evaluation to assess the
generated comments on three aspects: naturalness, informative-
ness, and usefulness, showing that DECOM can generate useful
and relevant comments.

Our main contributions are outlined as follows:

â€¢ Technique: a multi-pass deliberation framework for com-
ment generation, named DECOM, which is inspired by the
human cognitive process, and can effectively generate com-
ments in an iterative way. To the best of our knowledge,
this is the first work that employs multi-pass deliberation to
enhance the performance of comment generation.

â€¢ Evaluation: an experimental evaluation of the performance
of DECOM against state-of-the-art baselines, which shows
that DECOM outperforms all baselines, together with a hu-
man evaluation, which further confirms the readability, in-
formativeness, and usefulness of DECOM.

â€¢ Data: publicly accessible dataset and source code [4] to fa-
cilitate the replication of our study and its application in
extensive contexts.

In the rest of this paper, Section 2 elaborates the approach. Sec-
tion 3 presents the experimental setup. Section 4 demonstrates
the results and analysis. Section 5 describes the human evaluation.
Section 6 discusses indications and threats to validity. Section 7
introduces the related work. Section 8 concludes our work.

2 APPROACH
In this section, we present our DECOM, a multi-pass deliberation
framework that performs an iterative polishing process to refine the
draft to a better comment. Figure 2 illustrates an overview of DE-
COM, which consists of three main stages: (1) Data initialization,
for extracting the keywords from the input code and retrieving the
similar code-comment pair from the retrieval corpus; (2) Model
training, for leveraging a two-step training strategy to optimize
DECOM; and (3) Model prediction, for generating the target com-
ment of the new source code. Below, we provide details for each
stage in DECOM.

2.1 Data Initialization
Given a code snippet ğ‘¥, this stage aims to extract the keywords ğ‘¡
from ğ‘¥, and retrieve the initial draft ğ‘§0 from the retrieval corpus.
Extract keywords from code. A code snippet contains many
different types of tokens, such as reserve words (if, for), identi-
fier names (set_value, SortList), and operators (+, âˆ—). Among them,
identifier names defined by users usually contain more semantic
information that users want to express [10, 48, 53]. For example,
a methodâ€™s name is a typical identifier name, which is used to de-
scribe the overall functionality of the code and can be considered
as a shorter version of its code comment. Thus, to enable the model
to attend more on the identifier names and capture semantic in-
formation from them, we extract these words from code. First, we
use the javalang [3] and tokenize [5] libraries to extract identifier
names from the Java and Python code snippets, respectively. Then,

Figure 1: A motivation example of multi-pass deliberation.

in Figure 1, the one-pass model EditSum [34] generates two consec-
utive prepositional phrases after the word â€œcopâ€. Although either
of them is reasonable in their local contexts (â€œof the given tableâ€
and â€œof the tableâ€), putting them together degrades the comment
fluency and makes the developers hard to understand.

To alleviate these challenges, we introduce the Deliberation mech-
anism [63] in the comment generation task, aiming to further en-
hance the performance. Deliberation is a common and natural
behavior in human daily life. When writing papers or articles, we
usually first write drafts, and then iteratively polish them until
satisfied. Figure 1 illustrates an example of applying multi-pass
deliberation on comment generation. Based on the initial draft
â€œconstructs a new multivariate table from a univariate tableâ€, the
first-pass deliberation will generate the comment â€œcreates a new
copy of the given distributionâ€, and refine it in the second and
the third pass guided by the closeness to the similarity with the
give code snippet. In the end, we could obtain the most satisfying
comment â€œreturns a copy of the probability tableâ€.

In light of such a human cognitive process, we propose a novel
multi-pass deliberation framework for automatic comment genera-
tion, named DECOM, which contains multiple Deliberation Models
and one Evaluation Model. Given a code snippet, initially, we re-
trieve the most similar code from a pre-defined corpus and treat its
comment as the initial draft. We also extract the identifier names
from the input code as keywords, since these user-defined words
usually contain more semantic information that users want to ex-
press [10, 48, 53]. Then, we input the code, the keywords, and the
initial draft into DECOM to start the iterative deliberation process.
At each deliberation, the deliberation model polishes the draft and
generates a new comment. The evaluation model calculates the
quality score of the newly generated comment. This multi-pass
process terminates when (1) the quality score of the new comments
is no longer higher than the previous ones, or (2) the maximum
number of deliberations is reached. To evaluate our approach, we
conduct experiments on two real-world datasets in Java (87K) and
Python (108K), and the results show that our approach outperforms

One-Pass Decoding: Rencos: prunes all tablevalues that have a probabilitylower than the threshold Re2com: returns a copy of thebytes written to this stream Editsum: createsanew copy of thegiven table of the tablepublicCategoricalTablecopy(){Map<Value,Double>newTable=newHashMap<Value,Double>();if(variable ==null){variable =1;}if(table.isEmpty()){return newCategoricalTable(variable);}for(Value v :table.keySet()){newTable.put(v,table.get(v));}returnnewCategoricalTable(variable,newTable);}Ground Truth:returns a copy of the probability tableInitial Draft:constructs a new multivariate table from a univariate tableMulti-Pass Deliberation: First-Pass: createsa new copyof thegiven distribution Second-Pass: returns a copy of the table from this tableThird-Pass: returns a copy of the probability tableCloseness to copy()Closeness to copy()Automatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Figure 2: The overall architecture of DECOM

we further split the extracted identifier names into sub-tokens by
CamelCase or snake_case to obtain the smaller semantic units and
reduce data sparsity. These sub-tokens are treated as the keywords
of the code.

Retrieve the initial draft. To obtain the initial draft, follow-
ing the previous studies [58], we use the lexical similarity-based
retrieval method to identify the top similar code-comment pair for
the given code ğ‘¥. Specifically, we first take the training set of the
benchmark dataset as the retrieval corpus. Then, for each code in
the retrieval corpus, we adopt the BM25 [45] metric to calculate
the similarity between it and the given code ğ‘¥. The BM25 is a bag-
of-words retrieval metric to measure the relevance of documents
to a given search query in IR and is also widely used in code clone
detection and code search tasks [29, 39, 46]. Finally, we extract the
code with the highest similarity score as the retrieved result, and
use the comment of the code as the initial draft ğ‘§0. Since the size of
our training sets is quite large, we leverage the open-source search
engine Lucene [1] to speed up the retrieval process. We follow the
settings of Lucene from Re2Com [58] to run our experiments.

2.2 Model Training
DECOM contains ğ¾ deliberation models and one evaluation model,
where ğ¾ is the maximum deliberation number. To reduce computa-
tion cost and facilitate the sharing of information between models,
all ğ¾ deliberation models share three encoders with others and
share the code encoder and comment encoder with the evaluation
model. Each deliberation model has its own decoder, which can
avoid these models generating highly similar comments. We employ
a two-step training strategy to train DECOM as shown in Figure 2.
In the first step, we locally train the ğ¾ deliberation models: we first

jointly train the first deliberation model and the evaluation model.
Then we freeze the shared encoders and train the other deliberation
models one by one. In the second step, we fine-tune DECOM by
jointly optimizing all trained models.

2.2.1 Deliberation Model. Each deliberation model consists of three
different encoders (i.e. code encoder, keyword encoder, and com-
ment encoder) and a decoder. The details of them are illustrated in
the following.

Figure 3: The detailed structure of the Deliberation model.

2.2 Model TrainingYesğ‘¸ğ’Œâ‰¥ğ‘¸ğ’Œâˆ’ğŸNoComment ğ’›ğ’Œâˆ’ğŸEvaluation Modelğ’™ğ’›ğ’ŒK Deliberation Modelsğ’™ğ’•ğ’›ğ’Œâˆ’ğŸğ‘¸ğ’Œğ‘²>ğ’ŒNoComment ğ’›ğ‘²Yes2.1 Data InitializationStep 1ï¼šLocally train the K Deliberation modelsDeliberation LossEvaluation LossK-1 Deliberation ModelsDeliberation Lossğ’šComment EncoderCode EncoderKeyword Encoder1stDecoderğ’›ğŸğ’•ğ’™ğ’›ğŸ1stDeliberation ModelEvaluation ModelComment EncoderCode EncoderEvaluatorğ’›ğŸğ’™Retrieval  CorpusKeywordsğ’•Similar Code Initial Draft ğ’›ğŸInput Code ğ’™Ground Truth ğ’šInput Code ğ’™(New)Step 2ï¼šGlobally finetune the K deliberation modelsEvaluation ModelKDeliberation ModelsDeliberation LossEvaluation Lossğ’šFreeze three encodersğ’™,  ğ’•, ğ’›ğŸ2.3 Model PredictionIteratively train the K-1 Deliberation modelsğ’šPosition EncodingEmbeddingSelf AttentionAdd & NormFeed ForwardNïƒAdd & NormKeywordsğ’•Position EncodingEmbeddingSelf AttentionAdd & NormFeed ForwardNïƒAdd & NormComment ğ’›ğ’Œâˆ’ğŸKeyword  Encoder.Comment  EncoderCode  EncoderCodeğ’™Position EncodingEmbeddingSelf AttentionAdd & NormFeed ForwardNïƒAdd & NormSelf AttentionAdd & NormFeed ForwardAdd & NormLinear & SoftmaxNïƒğ‘˜ğ‘¡â„DecoderGroun Truthğ’šPosition EncodingEmbeddingCross AttentionAdd & NormCross AttentionAdd & NormComment ğ’›ğ’ŒASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

Encoders. The code encoder, keyword encoder, and comment
encoder aim to encode the source code ğ‘¥, keywords ğ‘¡, and the
previous comment ğ‘§ğ‘˜âˆ’1 as vectors, thus enabling the deliberation
model to obtain the semantic information from both source-side
(code and keywords) and target-side (the past comment). We con-
struct the three encoders by following the structure of the vanilla
Transformer Encoder [54]. As shown in Figure 3, each encoder is
composed of a stack of ğ‘ identical Transformer Encoder blocks.
Each block contains two sub-layers: The first sub-layer is a multi-
head self-attention layer (MHAtt), which employs multiple atten-
tion heads to capture the information from different representation
sub-spaces at different positions. The second sub-layer is a two-
layer Feed-Forward Network (FFN) with a ReLU activation function
in between. The residual connection is employed around the two
sublayers, followed by layer normalization (LayerNorm) [7]. Since
the three encoders have the same structure, we only introduce the
code encoder for simplicity.

Given a code snippet ğ‘¥ = [ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ‘™ (ğ‘¥) ], where ğ‘™ (ğ‘¥) is the
number of words in the code. The code encoder first embeds each
word of the code into a ğ‘‘ dimensional word vector:
âˆ’â†’
ğ‘¥ğ‘– = ğ‘Šğ‘’ T Â· ğ‘¥ğ‘– + ğ‘ƒğ¸ğ‘–

(1)

where ğ‘Šğ‘’ is a trainable embedding matrix, and ğ‘ƒğ¸ğ‘– is the position
encoding of the ğ‘–-th word. Following previous study [54], we use
the ğ‘ ğ‘–ğ‘›ğ‘’ and ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ function of different frequencies to compute
the position encoding:

ğ‘ƒğ¸ğ‘–,2ğ‘— = sin (cid:0)ğ‘—/100002ğ‘—/ğ‘‘ (cid:1)
ğ‘ƒğ¸ğ‘–,2ğ‘—+1 = cos (cid:0)ğ‘—/100002ğ‘—/ğ‘‘ (cid:1)
where ğ‘– is the position of the word and ğ‘— denotes the ğ‘—-th dimension
of the embedding vector.

(2)

(3)

Then, the code encoder inputs the sequence of word embeddings
into ğ‘ identical encoder blocks to calculate the hidden states of the
code. For the ğ‘–ğ‘¡â„
block of the code encoder, suppose that the input
is ğ»ğ‘–âˆ’1, the output ğ»ğ‘– is calculated as follows:

ğ»ğ‘–,1 = LayerNorm

(cid:18)
ğ»ğ‘–âˆ’1 + MHAtt(ğ»ğ‘–âˆ’1, ğ»ğ‘–âˆ’1, ğ»ğ‘–âˆ’1)

(cid:19)

ğ»ğ‘– = LayerNorm

(cid:18)
ğ»ğ‘–,1 + FFN(ğ»ğ‘–,1)

(cid:19)

(4)

(5)

âˆ’â†’
ğ‘¥2, ...,

where ğ»ğ‘–,1 is the hidden states of the first sub-layer. Initially, the
word embedding vectors [âˆ’â†’
âˆ’âˆ’âˆ’â†’
ğ‘¥ğ‘™ (ğ‘¥) ] are fed into the first block,
ğ‘¥1,
and the ğ‘ ğ‘¡â„
block outputs the final hidden states of the input code
ğ» = [â„1, â„2, ..., â„ğ‘™ (ğ‘¥) ]. Similarly, DECOM can encode the keywords
ğ‘¡ and the past comment ğ‘§ğ‘˜âˆ’1 into hidden states ğ‘ƒ and ğ‘…ğ‘˜âˆ’1, respec-
tively.

There are two points worth noting: (1) In the first-pass delib-
eration, DECOM takes the comment of the retrieved code as the
initial draft ğ‘§0, for each turn after this, DECOM uses the comment
generated in the previous turn as the draft. (2) source code ğ‘¥ and
keywords ğ‘¡ do not change in the iterative deliberation process, so
to save computational resources and time, we compute their hidden
states ğ» and ğ‘ƒ only once, and reuse them in subsequent iterations.
Decoder. The decoder aims to improve the quality of the pre-
viously generated comment ğ‘§ğ‘˜âˆ’1 by jointly leveraging its context
and the semantics of the source code ğ‘¥ and the keywords ğ‘¡. As

shown in Figure 3, the decoder is also composed of a stack of ğ‘
identical decoder blocks, and each block consists of four sub-layers.
In addition to the first and the last sub-layers introduced in the
part of Encoders in section 2.2.1, the decoder block inserts two
multi-head cross attention sub-layers in between, which are used
to capture the information from the outputs of the three encoders.
In the ğ‘˜ğ‘¡â„
pass deliberation (ğ‘˜ â‰¥ 1), given the hidden states ğ» ,
ğ‘ƒ, ğ‘…ğ‘˜âˆ’1. The ğ‘–ğ‘¡â„
block of the decoder first gets the hidden states of
the first sub-layer ğ‘†ğ‘–,1 using Eq. (4). Then, in the second sub-layer,
the block separately performs multi-head attention over the hidden
states of the source code ğ» and the keywords ğ‘ƒ:

ğ‘ğ‘– = MHAtt(ğ‘†ğ‘–,1, ğ», ğ» )
ğ‘ğ‘– = MHAtt(ğ‘†ğ‘–,1, ğ‘ƒ, ğ‘ƒ)
Besides, to effectively leverage the information from source-side,
we utilize the gate mechanism [23] to adaptively incorporate the
ğ‘ğ‘– containing source code features and the ğ‘ğ‘– containing keywords
features:

(6)

(7)

ğ›½ = Sigmoid(ğ‘Š T
ğ‘”ğ‘ğ‘¡ğ‘’ [ğ‘ğ‘– ; ğ‘ğ‘– ])
(cid:18)
ğ‘†ğ‘–,1 + ğ›½ Â· ğ‘ğ‘– + (1 âˆ’ ğ›½) Â· ğ‘ğ‘–

ğ‘†ğ‘–,2 = LayerNorm

(cid:19)

(8)

(9)

where ğ›½ is the degree of integration between source code and key-
words, A larger value of the ğ›½ (ranges from 0 to 1) indicates that the
model should pay more attention to the information in the source
code. ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’ is a trainable parameter matrix, [; ] is concatenation op-
eration, and ğ‘†ğ‘–,2 is the hidden states of the second sub-layer. In the
third sub-layer, the block obtains the ğ‘†ğ‘–,3 by performing multi-head
attention over the hidden states of the previous comment ğ‘…ğ‘˜âˆ’1:

ğ‘†ğ‘–,3 = LayerNorm

(cid:18)
ğ‘†ğ‘–,2 + MHAtt(ğ‘†ğ‘–,2, ğ‘…

ğ‘˜âˆ’1, ğ‘…

ğ‘˜âˆ’1)

(cid:19)

(10)

Based on this equation, the decoder can capture the important
clues from the global information of the past comment for further
refinement. Then, according to Eq. (5), the ğ‘–ğ‘¡â„
block uses the ğ‘†ğ‘–,3 to
compute the output of the last sub-layer ğ‘†ğ‘– . After the calculation
of ğ‘ decoder blocks, the decoder gets the hidden states of the last
decoder block ğ‘†. For the ğ‘—-th decoding step, the probability of ğ‘—ğ‘¡â„
token ğ‘§ğ‘˜
state ğ‘  ğ‘— in ğ‘† via a
linear layer followed by a Softmax function.

ğ‘— can be calculated by projecting the ğ‘—ğ‘¡â„

ğ‘ (ğ‘§

ğ‘˜
ğ‘— |ğ‘§

ğ‘˜
1 , ğ‘§

ğ‘˜
2 ,

..., ğ‘§

ğ‘˜
ğ‘—âˆ’1) = Softmax(ğ‘Š T

ğ‘œ Â· ğ‘  ğ‘— + ğ‘ğ‘œ )

(11)

where ğ‘Šğ‘œ is the parameter matrix and ğ‘ğ‘œ is the bias. Ultimately, we
use the Argmax function to generate the new comment ğ‘§ğ‘˜
ğ‘˜
1 ) ; ğ‘ (ğ‘§
= Argmax([ğ‘ (ğ‘§
where the ğ‘™ (ğ‘˜) is the length of the ğ‘˜ğ‘¡â„

ğ‘˜
2 ) ; Â· Â· Â· ; ğ‘ (ğ‘§

generated comment.

ğ‘˜
ğ‘™ (ğ‘˜) )])

(12)

ğ‘§

ğ‘˜

.

2.2.2 Evaluation Model. The evaluation model aims to estimate
the quality of the generated comments and calculate their quality
scores. As shown in Figure 4, the evaluation model contains a shared
code encoder, a shared comment encoder, and an evaluator.

generated by the ğ‘˜ğ‘¡â„

Given the new comment ğ‘§ğ‘˜

deliberation
model, the comment encoder encodes the ğ‘§ğ‘˜
into hidden states ğ‘…ğ‘˜
.
To obtain the representation of the comment, the evaluator first uses
Mean Pooling to average the hidden states ğ‘…ğ‘˜
]
to get the aggregated features ğ‘£ğ‘˜

= [ğ‘Ÿğ‘˜
ğ‘šğ‘’ğ‘ğ‘›. Then, it utilizes a two-layer

1 , ..., ğ‘Ÿğ‘˜

0 , ğ‘Ÿğ‘˜

ğ‘™ (ğ‘˜)

Automatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

from scratch is unstable in practice, which is mainly because of the
cold start [49] problem. To mitigate this problem, we use a two-step
training strategy as shown in Figure 2.

Step 1: Locally train the ğ¾ Deliberation models. We first jointly
train the first deliberation model and the evaluation model by min-
imizing the following loss function:
L (ğœƒ 1
ğ‘‘, ğœƒğ‘’ ) = Lğ‘‘ğ‘’ğ‘™ğ‘–ğ‘ (ğœƒ 1

ğ‘‘ ) + ğ›¼ğ‘’ Lğ‘’ğ‘£ğ‘ğ‘™ (ğœƒğ‘’ )

(18)

where the ğ›¼ğ‘’ is a hyperparameter, which is set to be 0.1 in our exper-
iments to control the weight of the evaluation loss. Then we freeze
the three shared encoders, and iteratively train the subsequent de-
liberation models using the Eq. (16) until the last deliberation model
is trained.

Step 2: Globally train the ğ¾ Deliberation models. One of the draw-
backs of the first-step training is that the deliberation models is
optimized independently and the model components cannot share
the information. To address this, we further fine-tune DECOM by
jointly training all ğ¾ deliberation models and the evaluation model:

L (ğœƒ 1

ğ‘‘, ..., ğœƒ

ğ¾
ğ‘‘ , ğœƒğ‘’ ) =Lğ‘‘ğ‘’ğ‘™ğ‘–ğ‘ (ğœƒ 1

ğ‘‘ ) + ... + Lğ‘‘ğ‘’ğ‘™ğ‘–ğ‘ (ğœƒ

ğ¾
ğ‘‘ )+

ğ›¼ğ‘’ Lğ‘’ğ‘£ğ‘ğ‘™ (ğœƒğ‘’ )

(19)

Note that in this step, all parameters are unfrozen and are updated
at the same time.

deliberation, the ğ‘˜ğ‘¡â„

2.3 Model Prediction
The prediction stage aims to generate a concise and useful comment
for a given code snippet. As shown in Figure 2, given a new code
snippet ğ‘¥, we first perform data initialization introduced earlier
to obtain the keywords ğ‘¡ and the initial draft ğ‘§0. Then, we input
them into DECOM to generate the target comment automatically.
The comment generation process involves multiple deliberation
processes. During the ğ‘˜ğ‘¡â„
deliberation model
polishes the previously generated comment ğ‘§ğ‘˜âˆ’1 and generates a
new comment ğ‘§ğ‘˜
. The evaluation model estimates the quality of
the new comment ğ‘§ğ‘˜
by calculating the cosine similarity between
this and the source code ğ‘¥. The deliberation process is performed
iteratively unless either of the following two conditions is satisfied:
(1) the quality score of the new comment is no longer higher than
the previous ones; (2) a certain number of deliberations ğ¾ > 0 is
reached. In the former case, we adopt the previous comment as the
target comment. In the latter case, the last generated comment is
accepted.

3 EXPERIMENTAL SETUP
3.1 Dataset
Since most of the related studies [6, 12, 16, 64, 65] for comment
generation tasks are evaluated on JCSD [27] and PCSD [9] bench-
mark datasets, in this study, we also select these two datasets in our
experiments. JCSD has 87,136 code-comment pairs collected from
more than 9K Java Github repositories created from 2015 to 2016
with at least 20 stars. It first extracted Java methods and Javadocs,
and treated the first sentence of the Javadoc as the ground-truth
comment of the corresponding code. PCSD contains 108,726 code-
comment pairs collected from open source repositories on GitHub.
It used docstrings (i.e., the string literals that appear right after the
definition of functions) as comments for Python functions.

Figure 4: The detailed structure of the Evaluation model.

feed-forward network (FFN) to map the features into the comment
representation ğ‘£ğ‘˜

.

ğ‘˜

ğ‘£

ğ‘£

ğ‘˜
0 ; ğ‘Ÿ

= FFN(ğ‘£

ğ‘˜
ğ‘šğ‘’ğ‘ğ‘›) = ReLU(ğ‘£

ğ‘˜
ğ‘šğ‘’ğ‘ğ‘› = MeanPooling([ğ‘Ÿ

ğ‘˜
ğ‘˜
1 ; Â· Â· Â· ; ğ‘Ÿ
ğ‘™ (ğ‘˜) ])
ğ‘˜
ğ‘šğ‘’ğ‘ğ‘› Â· ğ‘Š1 + ğ‘1) Â· ğ‘Š2 + ğ‘2
Similarly, the evaluator can obtain the code representation ğ‘£ğ‘¥
using
the Eq. (13) and (14) . Then, we use the cosine similarity metric
to calculate the similarity score ğ‘„ğ‘˜
. A higher
similarity score indicates that the comment ğ‘§ğ‘˜
is more semantically
similar to the source code ğ‘¥.

between ğ‘£ğ‘˜

and ğ‘£ğ‘¥

(14)

(13)

ğ‘˜

ğ‘„

= ğ¶ğ‘œğ‘  (ğ‘£

ğ‘¥

ğ‘˜ ) =

, ğ‘£

ğ‘£ğ‘¥ T Â· ğ‘£ğ‘˜
âˆ¥ ğ‘£ğ‘¥ âˆ¥ Â· âˆ¥ ğ‘£ğ‘˜ âˆ¥

(15)

2.2.3 Two-Step Training. In this section, we describe the training
process and strategies for DECOM. We denote the parameters of the
ğ‘˜ğ‘¡â„
ğ‘‘ and the parameters of the evaluation
model as ğœƒğ‘’ .

deliberation model as ğœƒğ‘˜

Deliberation loss and Evaluation loss. Given the source code
ğ‘¥, the ground truth ğ‘¦, the keywords ğ‘¡, and the previous comment
ğ‘§ğ‘˜âˆ’1, the ğ‘˜ğ‘¡â„
deliberation model can be optimized by maximizing
the probability of ğ‘ (ğ‘¦|ğ‘¥, ğ‘¡, ğ‘§ğ‘˜âˆ’1). The loss function is calculated as:

Lğ‘‘ğ‘’ğ‘™ğ‘–ğ‘ (ğœƒ

ğ‘˜
ğ‘‘ ) =

âˆ‘ï¸

(cid:18)
ğ‘¦ğ‘– |ğ‘¦<ğ‘–, ğ‘¥, ğ‘¡, ğ‘§

(cid:19)

ğ‘˜âˆ’1

âˆ’ log ğ‘

(16)

1â‰¤ğ‘– â‰¤ğ‘™ (ğ‘¦)

where ğ‘™ (ğ‘¦) is the length of ground truth ğ‘¦. For the evaluation model,
we use the Circle Loss function [51] to optimize its parameters ğœƒğ‘’ :

Lğ‘’ğ‘£ğ‘ğ‘™ (ğœƒğ‘’ ) = ğ‘™ğ‘œğ‘”

(cid:18)

1 + ğ‘’

ğœ† (ğ¶ğ‘œğ‘  (ğ‘£ğ‘¥ ,ğ‘£ğ‘˜ )âˆ’ğ¶ğ‘œğ‘  (ğ‘£ğ‘¥ ,ğ‘£ğ‘¦ ))

(cid:19)

(17)

where ğ¶ğ‘œğ‘  () denotes the cosine similarity score, ğ‘£ğ‘¥ , ğ‘£ğ‘˜
the representation vectors of the source code, the ğ‘˜ğ‘¡â„
comment, and the ground truth, respectively.

and ğ‘£ ğ‘¦
are
generated

Two-step Training Strategy. In theory, we can train the frame-
work from random initialization by jointly optimizing all compo-
nents. However, we find that training the multi-pass model directly

Code  EncoderComment  EncoderCodeğ’™Position EncodingEmbeddingSelf AttentionAdd & NormFeed ForwardNïƒAdd & NormQuality Scoreğ‘¸ğ’ŒMean PoolingFeed ForwardCosine SimilarityPosition EncodingEmbeddingSelf AttentionAdd & NormFeed ForwardNïƒAdd & NormComment ğ’›ğ’ŒEvaluatorASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

For the sake of fairness, we preprocess the JCSD and PCSD
strictly following Rencos [64]. Specifically, we first split datasets
into a training set, validation set, and test set in a consistent propor-
tion of 8 : 1 : 1 for the Java dataset and 6 : 2 : 2 for the Python dataset.
We use the javalang [3] and tokenize [5] libraries to tokenize the
code snippet for JCSD and PCSD, respectively. We further split
code tokens of the form CamelCase and snake_case to respective
sub-tokens. In common with [64], we remove the exactly duplicated
code-comment pairs in the test set for JCSD. The specific statistics
of the two preprocessed datasets are shown in Table 1.

Table 1: Statistic of Datasets

Dataset
Train
Validation
Test
Unique tokens in code
Unique tokens in comment
Avg. tokens in code
Avg. tokens in comment
Max. token in code
Max. token in comment

JCSD
69,708
8,714
6,489
230,336
35,535
99.9
17.1
4,842
670

PCSD
65,236
21,745
21,745
481,756
37,111
133.1
9.9
157,116
333

3.2 Evaluation Metrics
We evaluate the performance of different approaches using com-
mon metrics including BLEU [43], ROUGE-L [37], METEOR [8], and
CIDEr [55]. BLEU measures the ğ‘›-gram precision by computing
the overlap ratios of ğ‘›-grams and applying brevity penalty on short
translation hypotheses. BLEU-1/2/3/4 corresponds to the scores of
unigram, 2-grams, 3-grams, and 4-grams, respectively. ROUGE-L
is defined as the length of the longest common subsequence be-
tween generated sentence and reference, and based on recall scores.
METEOR is based on the harmonic mean of unigram precision and
recall, with recall weighted higher than precision. CIDEr considers
the frequency of ğ‘›-grams in the reference sentences by computing
the TF-IDF weighting for each ğ‘›-gram. ğ¶ğ¼ğ·ğ¸ğ‘Ÿğ‘› score for ğ‘›-gram is
computed using the average cosine similarity between the candi-
date sentence and the reference.

3.3 Implementation Details
Following previous studies [64], we set the length limits (in terms
of #words) of code and comment (i.e., 300 and 30 for JCSD, 100
and 50 for PCSD). To save the computing resource, we limit the
maximum vocabulary size of source code and comment to 50K for
both datasets. The out-of-vocabulary words are replaced by â€˜UNKâ€™.
The word embedding size of both code and comment is set to 512.
We set the dimensions of hidden states to 512, the number of heads
to 8, and the number of blocks to 6, respectively. The maximum
deliberation number ğ¾ is set to be 3. We set the mini-batch size
to 32 and train our approach using the Adam [31] optimizer. In
the first-step training, we set the learning rate to 1e-4, and for
the second-step training, we use a smaller learning rate (1e-5) to
fine-tune DECOM. To avoid the over-fitting problem, we apply
dropout [22] with 0.2. The maximum number of epochs is set to
100 for each step of training. We also use the strategy of early

stopping, when the validation performance does not improve for 20
consecutive epochs, the training process will be stopped. To reduce
training time, we use the greedy search to generate comments at
the training stage. During the prediction stage, we use the beam
search [59] and set the beam size to 5 for choosing the best result.
Our approach is implemented based on the Pytorch [2] framework.
The experimental environment is a desktop computer equipped
with an NVIDIA GeForce RTX 3060 GPU, intel core i5 CPU, and
12GB RAM, running on Ubuntu OS.

4 RESULTS
We address the following three research questions to evaluate the
performance of DECOM:

RQ1 : How does the DECOM perform compared to the state-of-

the-art comment generation baselines?

RQ2: How does each individual component in DECOM con-

tribute to the overall performance?

RQ3: Whatâ€™s the performance of DECOM on the data with dif-

ferent code or comment length?

4.1 RQ1: Comparison with Baselines
4.1.1 Baselines. We compare our approach with three categories
of existing work on the comment generation task. We exactly adopt
the hyperparameter settings reported in the original paper for all
baselines. For a fair comparison, we use the same maximum code
and comment length for all approaches, and evaluate their perfor-
mance using the same training/testing datasets.

â€¢ IR-based baselines. LSI [14] is an IR technique to analyze the
latent meaning or concepts of documents. The similarity between
the code and the comment is computed based on the LSI-reduced
vectors and cosine distance, and we set the vector dimension
to be 500. VSM [47] is also a commonly used IR technique in
comment generation tasks. For a given code snippet, we represent
the code as a vector using TF-IDF, and extract the comment of
the most similar code based on cosine similarity. NNGen [38] is
a nearest-neighbors approach for generating commit messages.
It first embeds code into vectors based on the bag of words and
the term frequency. Then, it retrieves the nearest neighbors of
the code. Finally, it outputs the message of the code with the
highest BLEU score.

â€¢ NMT-based approaches. CODE-NN [28] is the first learning-
based model for comment generation. It maps the source code
sequence into word embeddings, then uses the LSTM and the
attention mechanism to generate comments. TL-CodeSum [27]
is a multi-encoder neural model that encodes API sequences
along with code token sequences and generates comments from
source code with transferred API knowledge. Hybrid-DRL [56]
incorporates ASTs and sequential content of code snippets into
a deep reinforcement learning framework.

â€¢ Hybrid approaches. Rencos [64] is a hybrid approach that com-
bines the advantages of both IR-based and NMT-based techniques.
Re2Com [58] is an exemplar-based comment generation ap-
proach that leverages the advantages of three types of methods
based on neural networks, templates, and IR to improve the per-
formance. EditSum [34] is the most recent hybrid approach. It

Automatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Table 2: The results of comparison with baselines, with the improvement compared with the best baselines in percentage.

Method

31.4
LSI
VSM
33.3
33.0
NNGen
CODE-NN
23.9
TL-CodeSum 29.9
32.4
Hybrid-DRL
Re2com
33.7
Rencos
37.5
34.1
EditSum
DECOM
40.4

BLEU-1/2/3/4
19.3
22.5
21.1
24.4
20.9
24.4
8.6
12.8
18.1
21.3
16.3
22.6
19.0
23.6
23.4
27.9
19.5
24.3
25.2
30.2

17.3
19.0
18.7
6.3
16.1
13.3
16.3
20.6
16.9
22.3

JCSD

PCSD

ROUGE-L METEOR CIDEr
1.803
1.983
1.933
0.978
1.660
1.656
1.807
2.209
1.865
2.442

34.8
36.6
36.3
28.9
33.2
26.5
38.1
42.0
38.6
44.5

14.4
15.4
15.0
9.1
13.7
13.5
15.1
17.3
15.2
19.6

BLEU-1/2/3/4
20.1
23.6
22.1
26.1
20.1
23.8
10.7
15.4
12.5
16.5
19.5
26.2
17.4
22.3
24.2
29.5
18.2
23.1
25.5
31.4

17.6
19.3
17.4
8.1
10.4
15.0
14.5
20.7
15.6
21.9

36.3
38.9
36.5
30.8
31.1
41.1
36.6
43.1
37.7
45.6

ROUGE-L METEOR CIDEr
1.982
2.216
1.967
1.229
1.335
2.042
1.813
2.449
1.894
2.603

17.2
19.0
17.1
13.4
13.6
17.9
17.0
21.1
17.1
22.5

40.0
42.7
40.2
35.1
35.3
42.2
40.8
47.5
42.0
49.3

Table 3: RQ2 Ablation study on the multi-pass deliberation and evaluation model.

Variants

DECOM w/o Multi-pass Deliberation
DECOM w/o Evaluation Model
DECOM

38.9
39.5
40.4

BLEU-1/2/3/4
23.5
28.5
24.3
29.3
25.2
30.2

20.8
21.5
22.3

JCSD

PCSD

ROUGE-L METEOR CIDEr
2.274
2.338
2.442

43.1
43.7
44.5

18.8
19.0
19.6

BLEU-1/2/3/4
23.8
29.3
24.3
30.3
25.5
31.4

20.4
20.6
21.9

43.5
44.6
45.6

ROUGE-L METEOR CIDEr
2.424
2.478
2.603

47.5
48.6
49.3

21.1
21.6
22.5

first retrieves the most similar code snippet, and treats the corre-
sponding comment as a prototype. Then, it combines the pattern
in the prototype and semantic information of the input code to
generate the target comment.

4.1.2 Results. Table 2 shows the comparison results between the
performance of DECOM and other baselines, and the best perfor-
mance is highlighted in bold. Overall, our approach achieves the
best performance on all evaluation metrics, followed by Rencos, Ed-
itSum, and Re2com. On the Java dataset, DECOM achieves 22.3, 44.5,
19.6, and 2.442 points on BLEU-4, ROUGE-L, METEOR, and CIDEr.
Compared with the best baseline (Rencos), DECOM improves the
performance of BLEU-4, ROUGE-L, METEOR, and CIDEr by 8.3%,
6.0%, 13.3%, and 10.5%, respectively. On the Python dataset, DECOM
achieves 21.9, 49.3, 22.5, and 2.603 points on BLEU-4, ROUGE-L,
METEOR, and CIDEr. Compared with the best baseline (Rencos),
DECOM also achieves 5.8%, 3.8%, 6.6%, and 6.3% improvements on
BLEU-4, ROUGE-L, METEOR, and CIDEr, respectively .

Answering RQ1: DECOM outperforms the state-of-the-art base-
lines in terms of all seven metrics on both two datasets. Compared
to the best baseline Rencos, DECOM improves the performance
of BLEU-4, ROUGE-L, METEOR, and CIDEr by 8.3%, 6.0%, 13.3%,
and 10.5% on JCSD dataset, by 5.8%, 3.8%, 6.6%, and 6.3% on PCSD
dataset, respectively.

4.2 RQ2: Component Analysis
4.2.1 Variants. To evaluate the contribution of core components,
we obtain two variants: (1) DECOM w/o Multi-pass Delibera-
tion, which removes the multi-pass deliberation and adopts the
one-pass process to generate comments. (2) DECOM w/o Evalu-
ation Model, which removes the evaluation model and takes the
comment generated by the last (ğ¾ğ‘¡â„
) deliberation model as the
result. We train the two variants with the same experimental setup
as DECOM and evaluate their performance on the test sets of JCSD
and PCSD, respectively.

4.2.2 Results. Table 3 presents the performances of DECOM and
its two variants. We can see that, removing the two components
makes the performance degrade substantially. Specifically, when
comparing DECOM and DECOM w/o Multi-pass Deliberation, re-
moving the multi-pass deliberation will lead to a dramatic decrease
in the average BLEU-4 (by 6.8%), ROUGE-L (by 3.4%), METEOR (by
5.2%), and CIDEr (by 6.9%) across both datasets. When comparing
DECOM and DECOM w/o Evaluation Model, we find that remov-
ing the evaluation model will lead to the performance decline in
the average BLEU-4 (by 4.8%), ROUGE-L (by 1.6%), METEOR (by
3.5%), and CIDEr (by 4.5%). We can also observe that, removing the
multi-pass deliberation will lead to a larger degree of performance
decline than removing the evaluation model.

Answering RQ2: Both the multi-pass deliberation and the eval-
uation model components have positive contributions to the perfor-
mance of DECOM, where the multi-pass deliberation component
contributes more to increasing the performance.

4.3 RQ3: Performance for Different Lengths
4.3.1 Methodology. To answer this question, we analyze the per-
formance of DECOM and best three baselines (i.e. Re2com, Rencos,
and EditSum) on different lengths (i.e., number of tokens) of code
and comments. We calculate the BLEU-1 score of each sample on
the test set of both datasets and average the scores by the length of
code and comments, respectively. (Note that, based on our obser-
vations, all the seven evaluation metrics show similar trends. For
simplicity, we show BLEU-1 only).

4.3.2 Results. Figure 5 presents the performance of DECOM and
the three baselines on JCSD and PCSD datasets with code and
comments of different lengths, where the red lines denote the per-
formance of DECOM. Overall, we can observe that the performance
of DECOM generally outperforms the three baselines with different
code and comment lengths on both datasets. Specifically, as the
length of the input code increases, DECOM almost keeps a stable

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

Figure 5: BLEU-1 scores for different code and comment lengths.

Figure 6: The results of human evaluation.

improvement over the other three approaches. The performance
of DECOM is nearly the best on all the lengths of Java and Python
code snippets. In particular, DECOM can achieve much higher per-
formance than others when the length of the Java code snippet is
over 200 words. This shows that DECOM can better understand the
semantics of the long code snippets by sharing the information be-
tween deliberation models and the evaluation model. For the output
comments, we can see that when the output comments are becom-
ing complicated with a relatively long length, the performance of
all the approaches decrease, which indicates that the longer the
comment, the harder to generate it completely. However, DECOM
still has a substantial improvement over the other baselines (as
shown in Figure 5(c)), showing that our approach has the ability to
generate long and concise comments.

Answering RQ3: DECOM generally outperforms the best three
baselines on different lengths of the input code snippets and the
output comments, indicating its robustness. In particular for Java,
DECOM can achieve much higher performance than others when
the code snippets and comments are long.

5 HUMAN EVALUATION
Although the evaluation metrics (i.e., BLEU, ROUGE-L, METEOR,
and CIDEr) can measure the lexical gap between the generated
comments and the references, it can hardly reflect the semantic gap.
Therefore, we perform a human evaluation to further assess the
quality of comments generated by different approaches.

5.1 Procedure
We recruited six participants, including three Ph.D. students, one
master student, and two senior researchers, who are not co-authors

of this paper. They all have at least three years of both Java and
Python development experience, and four of them have more than
six years of development experience. We randomly select 100 code
snippets from the test dataset (50 from JCSD and 50 from PCSD).
By applying the best three baselines (i.e., Re2com, Rencos, and Ed-
itSum) and DECOM, we obtain a total of 400 generated comments.
The 400 code-comment pairs are divided into three groups, and
each group is used to create a questionnaire. We randomly list
the code-comment pairs on the questionnaire and remove their
labels to ensure that the participants are not aware of where the
comments are generated from. Each questionnaire is evaluated by
two participants, and the final result of a generated comment is
the average of two participants. Each participant is asked to rate
each generated comment from the three aspects: (1) Naturalness
reflects the fluency of generated comments from the perspective
of grammar; (2) Informativeness reflects the information rich-
ness of generated comments; and (3) Usefulness reflects how can
generated comments help developers. All three scores are integers,
ranging from 1 to 5 (1 for poor, 2 for marginal, 3 for acceptable, 4
for good, and 5 for excellent).

5.2 Results
Figure 6 exhibits the results of human evaluation by showing the
violin plots depicting the naturalness, informativeness, and useful-
ness of different models, and Table 4 shows the statistic results. Each
violin plot contains two parts, i.e., the left and right parts reflect the
evaluation results of models on the JCSD dataset and PCSD dataset.
The box plots in the violin plots present the distribution of data
and the red triangles mean the average scores of the three aspects.
Overall, DECOM is better than all baselines in three aspects. The

(a) Naturalness(b) Informativeness(c) UsefulnessJavaPythonAutomatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Table 4: The statistic results of human evaluation.

Naturalness

Informativeness

Usefulness

Approach Avg. Median
Re2com
3.7
3.9
Rencos
4.0
EditSum
DECOM
4.1
Re2com
2.7
3.0
Rencos
2.6
EditSum
DECOM
3.2
Re2com
2.6
2.9
Rencos
2.4
EditSum
DECOM
3.1

4.0
4.0
4.0
4.5
2.5
3.0
2.0
3.0
2.0
2.5
2.0
3.0

Std.
1.1
1.0
0.9
0.8
1.3
1.3
1.3
1.2
1.4
1.4
1.3
1.3

average score for naturalness, informativeness, and usefulness of
our approach are 4.24, 3.43, and 3.25, respectively, on the JCSD
dataset. On the PCSD dataset, our approach gets the average score
of 4.05, 2.96, and 2.87 in terms of naturalness, informativeness, and
usefulness. We can see that, the comments generated in the PCSD
dataset receive lower scores in human evaluation, while receiving
higher scores in evaluation metrics (see Table 2). This is mainly
because the PCSD dataset contains shorter comments (see Table 1),
thus mistakenly generating fewer keywords may lead to a lower
degree of human satisfaction. While the shorter comments are more
probable with these N-gram matching metrics [44].

Specifically, in terms of naturalness, our approach achieves aver-
age scores above 4 on both JCSD and PCSD datasets, which shows
that DECOM can generate fluent and readable comments. Besides,
in terms of informativeness and usefulness, DECOM is the only
approach with an average score of more than 3 points on the JCSD
dataset. It indicates that the comments generated by DECOM tend
to be more informative and useful than other baselines.

6 DISCUSSION
6.1 Qualitative Analysis
For qualitative analysis of our approach, we present two cases
generated by the best three baselines together with DECOM. The
cases are selected from the test sets of Java and Python datasets
respectively, as shown in Figure 7 . Overall, the comments generated
by DECOM tend to be more accurate and more readable than the
other three baselines. In case 1, the aim of the Java code is to
display the contents of an index. The three baselines mistakenly
predict the keyword â€œdisplaysâ€ as â€œlocatesâ€, â€œwritesâ€, and â€œlocatesâ€,
respectively, resulting in the semantics of the generated comments
being different from the ground truth. In contrast, the comment
generated by DECOM is exactly the same as the ground truth,
indicating that our approach can understand the intention of code
concisely. In case 2, we can see that, our approach also performs
better than other baselines, and the comment generated by DECOM
has a high semantic similarity with the ground truth.

We believe that the performance advantage of DECOM mainly
comes from two aspects: (1) DECOM can observe the entire previ-
ously generated comment and leverage its global information to
polish it. While other baselines can only leverage the previously
generated words. (2) DECOM employs an evaluation model that can

Figure 7: Examples of qualitative analysis.

determine the opportunity when the deliberation process should
end, as well as learn the semantic relationship between source code
and target comments. Besides, the evaluation model shares its two
encoders with the deliberation models, which facilitates the infor-
mation sharing among these models, and enables DECOM to learn
a better representation for code and comments.

6.2 Parameter Analysis
Figure 8 illustrates the impact of the maximum number of delib-
erations ğ¾ on the performance of DECOM trained on the PCSD
dataset, as well as the time cost (Note that, since the JCSD dataset
has quite similar results to the PCSD dataset, we only exhibit the
results on the PCSD dataset).

We can see that enlarging the maximum deliberation number ğ¾
generally increases the performance of DECOM. When enlarging
ğ¾ from 1 to 5, the BLEU-1 score increases by 6.0%. We also note
that DECOM with ğ¾ = 2 substantially outperforms DECOM with
ğ¾ = 1 (i.e one-pass model), which indicates that the deliberation
process can greatly improve the comment quality by polishing the
previously generated comment. Moreover, for ğ¾ larger than 3, the

929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986AutomaticCommentGenerationviaMulti-PassDeliberationASE2022,10-14October,2022,UnitedStates987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044degreeofhumansatisfaction.WhiletheshortercommentsaremoreprobablewiththeseN-grammatchingmetrics[38].Specifically,intermsofnaturalness,ourapproachachievesaver-agescoresabove4onbothJCSDandPCSDdatasets,whichshowsthatDECOMcangeneratefluentandreadablecomments.Besides,intermsofinformativenessandusefulness,DECOMistheonlyapproachwithanaveragescoreofmorethan3pointsontheJCSDdataset.ItindicatesthatthecommentsgeneratedbyDECOMtendtobemoreinformativeandusefulthanotherbaselines.6DISCUSSION6.1QualitativeAnalysisForqualitativeanalysisofourapproach,wepresenttwocasesgeneratedbythebestthreebaselinestogetherwithDECOM.ThecasesareselectedfromthetestsetsofJavaandPythondatasetsrespectively,asshowninTable5.Overall,thecommentsgeneratedbyDECOMtendtobemoreaccurateandmorereadablethantheotherthreebaselines.Incase1,theaimoftheJavacodeistodisplaythecontentsofanindex.Thethreebaselinesmistakenlypredictthekeywordâ€œdisplaysâ€asâ€œlocatesâ€,â€œwritesâ€,andâ€œlocatesâ€,respectively,resultinginthesemanticsofthegeneratedcommentsbeingdifferentfromthegroundtruth.Incontrast,thecommentgeneratedbyDECOMisexactlythesameasthegroundtruth,indicatingthatourapproachcanunderstandtheintentionofcodeconcisely.Incase2,wecanseethat,ourapproachalsoperformsbetterthanotherbaselines,andthecommentgeneratedbyDECOMhasahighsemanticsimilaritywiththegroundtruth.WebelievethattheperformanceadvantageofDECOMmainlycomesfromtwoaspects:(1)DECOMcanobservetheentirepreviously-generatedcommentandleverageitsglobalinformationtopolishit.Whileotherbaselinescanonlyleveragethepreviouslygeneratedwords.(2)DECOMemploysanevaluationmodelthatcandeterminetheopportunitywhenthedeliberationprocessshouldend,aswellaslearnthesemanticrelationshipbetweensourcecodeandtargetcomments.Besides,theevaluationmodelsharesitstwoencoderswiththedeliberationmodels,whichfacilitatestheinformationsharingamongthesemodels,andenablesDECOMtolearnabetterrepresentationforcodeandcomments.6.2ParameterAnalysisFigure7illustratestheimpactofthemaximumnumberofdelib-erationsğ¾ontheperformanceofDECOMtrainedonthePCSDdataset,aswellasthetimecost(Notethat,sincetheJCSDdatasethasquitesimilarresultstothePCSDdataset,weonlyexhibittheresultsonthePCSDdataset).Wecanseethatenlargingthemaximumnumberofiterationsğ¾generallyincreasestheperformanceofDECOM.Whenenlargingğ¾from1to5,theBLEU-1scoreincreasesby6.0%.WealsonotethatDECOMwithğ¾=2substantiallyoutperformsDECOMwithğ¾=1(i.eone-passmodel),whichindicatesthatthedeliberationprocesscangreatlyimprovethecommentqualitybypolishingthepreviouslygeneratedcomment.Moreover,forğ¾largerthan3,theperformancesslowlyincreasebutthetimecostrisesexponentially.Forexample,whenenlargingğ¾from3to5,theBLEU-1scoreincreasesby1%(0.5points),whilethetrainingtimeincreasesbyTable5:Examplesofqualitativeanalysis.Case1(Java):publicvoiddumpIndex(booleanshowBounds)throwsIOException{byteixRecord[]=newbyte[SPATIAL_INDEX_RECORD_LENGTH];intrecNum=0;if(shpFileName==null){return;}else{recNum++;intoffset=readBEInt(ixRecord,0);intlength=readBEInt(ixRecord,4);}ssx.close();}GroundTruth:displaysthecontentsofthisindex.Re2com:writesinthespatialindexfileforintersections.Rencos:locatesrecordsintheshapefile.thespatialindexissearchedforintersections.EditSum:locatesrecordsintheshapefile.DECOM:displaysthecontentsofthisindex.Case2(Python):@pytest.mark.django_dbdeftest_make_naive_use_tz_false(settings):settings.USE_TZ=Falsedatetime_object=datetime(2016,1,2,21,52,25,tzinfo=pytz.utc)asserttimezone.is_aware(datetime_object)naive_datetime=make_naive(datetime_object)asserttimezone.is_aware(naive_datetime)GroundTruth:testsdatetimesareleftintactifuse_tzisnotineffect.Re2com:testsdatetimesaremadenaiveconfigured.Rencos:testsdatetimesaremadeawareoftheconfiguredtimezone.EditSum:testsdatetimesaremadeawareintactiftimezonesisnotinadmin.DECOM:testsdatetimesareleftintactiftimezonesisnotineffect.Figure7:Performanceandtimecostinvaryingthemaximumnumberofdeliberationsğ¾.65%(26hours).Thus,weconsiderğ¾=3tobeatrade-offchoicebetweeneffectivenessandefficiency.6.3ThreatstoValidityTherearethreemainthreatstothevalidityofourapproach.Thefirstthreattovalidityisthedatasetsweuse.WeonlyevaluateDECOMontheJavaandPythondatasets.However,DECOMuses9ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

7 RELATED WORK
7.1 Automatic Comment Generation
The automatic comment generation task is now a rapidly-growing
research topic in the community of software engineering and natu-
ral language processing.

Early studies typically utilize template-based approaches and
information retrieval (IR) based approaches to generate comments.
The basic idea of the template-based approach [40, 41, 50] is to
extract the keywords from the code snippets and fill them into
the predefined templates. Due to the limitations of manually de-
signing templates, these methods are usually time-consuming and
have poor generalization. The IR-based approaches [14, 15, 19, 20,
38, 47, 60, 61] aim to use IR techniques to extract keywords from
the source code and compose them into term-based comments for
a given code snippet. For example, Wong et al. [60] generated a
comment for a given code snippet by retrieving the replicated code
samples from software repositories with clone detection techniques.
However, the IR-based approaches ignore the semantic relation-
ship between source code and natural language, so the comments
they generate are poorly readable. Recently, many learning-based
methods have been proposed, which train the neural models from
a large-scale code-comment corpus to automatically generate com-
ments [11, 26â€“28, 33, 34, 56â€“58, 64]. Iyer et al. [28] first treated the
comment generation task as an end-to-end translation problem and
introduced NMT techniques into code comment generation. Hu et
al. [26] converted the Java methods into AST sequence to learn the
structural information, and applied a seq2seq model to generate
comments. Wei et al. [58] proposed an exemplar-based comment
generation method that utilized the comment of the similar code
snippet as an exemplar to assist in generating the target comment.
Zhang et al. [64] proposed a seq2seq approach that retrieved two
similar code snippets for a given code to improve the quality of
the generated comment. Further, Li et al. [34] treated the comment
of the similar code retrieved from a parallel corpus as a prototype.
Based on the semantic differences between input code and similar
code, they proposed a seq2seq network to update the prototype
and generate comments.

Different from the existing research, we propose a novel frame-
work for automatic comment generation, which performs multiple
deliberation processes to iteratively polish the generated comments.
DECOM also contains an evaluation model that not only determines
whether to end the deliberation process, but also learns the seman-
tic relationship between source code and target comments. The
experimental results also prove the superiority of our approach.

7.2 Deliberation Networks
The Deliberation mechanism aims to refine the existing results for
further improvement. It has been successfully applied to various do-
mains, such as machine translation [17, 21, 35], question generation
[42], image captioning [36], speech recognition [24, 25, 52].

Xia et al. [63] first proposed a deliberation network for sequence
generation tasks, which consists of two decoders: a first-pass de-
coder for generating a draft, and a second-pass decoder for polishing
the generated draft to a better sequence. Geng et al. [17] proposed a
novel architecture to introduce the deliberation mechanism into the
neural machine translation model. It leveraged the policy network

Figure 8: Performance and time cost in varying the maxi-
mum number of deliberations ğ¾.

performances slowly increase but the time cost rises exponentially.
For example, when enlarging ğ¾ from 3 to 5, the BLEU-1 score
increases by 1% (0.5 points), while the training time increases by
65% (26 hours). Thus, we consider ğ¾ = 3 to be a trade-off choice
between effectiveness and efficiency.

6.3 Threats to Validity
There are four main threats to the validity of our approach.

The first threat to validity is that DECOM uses the lexical simi-
larity based method to retrieve the top similar code-comment pair,
which may cause the retrieved comment (initial draft) to be seman-
tically different from the target comment. However, the threats can
be largely relieved as DECOM generates the target comment by iter-
atively polishing the previous comments. Specifically, DECOM can
correct and refine the retrieved comment in subsequent iterations
by leveraging its global information and semantic features of the
source code. Thus, even though the dissimilar comment is retrieved,
DECOM still can guarantee its performance is not affected.

The second threat to validity is the datasets we use. We only
evaluate DECOM on the Java and Python datasets. However, DE-
COM uses language-agnostic features that can be easily extracted
from any programming language. Therefore, we believe that our
approach has good generalizability and can perform well on the
datasets of other programming languages, such as C# and Ruby.

The third threat relates to the suitability of evaluation metrics.
First, recent researchers have raised concern over the use of BLEU
[18], warning the community that the way BLEU is used and inter-
preted can greatly affect its reliability. To mitigate that threat, we
also adopt other metrics, i.e., ROUGE, METEOR, and CIDEr, when
evaluating performance. Second, there is also a threat related to our
human evaluation. We cannot guarantee that each score assigned
to every generated comment is fair. To mitigate this threat, each
comment is evaluated by six human evaluators, and we use the
average score of the two evaluators as the final score.

The fourth threat relates to the errors in the implementation
of baselines. To mitigate this issue, we directly use the publicly
available code of CODE-NN, TL-CodeSum, Hybrid-DRL, Rencos,
and Re2com to implement baselines. However, the code of EditSum
[34] is not available, so we tried our best to understand the paper and
re-implement the approach carefully. While we have verified our
implementation can achieve similar results as the original EditSum
on the same dataset used in its paper.

Automatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

to determine whether to end the translation process adaptively.
Nema et al. [42] utilized the deliberation network to address the au-
tomatic question generation task. They proposed a novel approach
called Refine Network, which contains two decoders. The second
decoder used dual attention to capture information from both (i)
the original passage and (ii) the question (initial draft) generated
by the first decoder, thereby refining the question generated by
the first decoder to make it more correct and complete. Lian et
al. [36] proposed a universal two-pass decoding framework for
the image captioning task, which contains a drafting model and
a deliberation model. The drafting model first generated a draft
caption according to an input image, and a deliberation model then
refined the draft caption to a better image description. Hu et al.
[25] employed the deliberation network for the speech recogni-
tion task. They combined acoustics and first-pass text hypotheses
for second-pass decoding based on the deliberation network and
obtained significant improvements.

The findings of previous work motivate the work presented in
this paper. Our study is different from the previous work as we focus
on enhancing the performance of the comment generation task by
incorporating its own characteristics into the deliberation network.
Specifically, we combine the two characteristics of the comment
generation task into the deliberation network: (1) since code reuse is
widespread in software development, we use retrieval techniques to
retrieve the most similar comment to provide an explicit hint about
the comment expression; (2) since user-defined identifier names
usually contain semantic information, we extract the keywords
from the source code to strength the semantic features of the source
code. To the best of our knowledge, this is the first work that
treats the comment generation process as the process of writing
and polishing, and utilizes multi-pass deliberation automatically
generate comments.

8 CONCLUSION
In this paper, we propose a novel multi-pass deliberation frame-
work for automatic comment generation, named DECOM, which is
inspired by human cognitive processes. DECOM relies on multiple
deliberation models and one evaluation model to iteratively per-
form the deliberation process. For each process, the deliberation
model refines the previously generated comment into a better one.
The evaluation model estimates the quality of the new generated
comment, and compares its quality score to the previous one to
determine whether to end the iterative process. We use a two-step
training strategy to train our framework. The evaluation results
show that our approach significantly outperforms all other base-
lines on both Java and Python datasets. A human evaluation study
also confirms the comments generated by DECOM tend to be more
readable, informative, and useful. In future work, we plan to incor-
porate the reinforcement learning techniques (e.g. policy network)
into the framework to adaptively choose the suitable deliberation
processes, thereby enhancing the performance.

ACKNOWLEDGMENTS
We sincerely appreciate anonymous reviewers for their construc-
tive and insightful suggestions for improving this manuscript. This
work is supported by the National Key Research and Development

Program of China under Grant No. 2018YFB1403400, the National
Science Foundation of China under Grant No. 61802374, 62002348,
62072442, 614220920020 and Youth Innovation Promotion Associa-
tion Chinese Academy of Sciences.

REFERENCES
[1] 2016. Lucene. https://lucene.apache.org/.
[2] 2016. Pytorch Framework. https://pytorch.org/.
[3] 2022. Javalang. https://pypi.org/project/javalang.
[4] 2022. Project Website. https://github.com/ase-decom/ASE22_DECOM.
[5] 2022. Tokenize. https://docs.python.org/2/library/tokenize.html.
[6] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020. 4998â€“5007.

[7] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-

tion. CoRR abs/1607.06450 (2016). http://arxiv.org/abs/1607.06450

[8] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT
Evaluation with Improved Correlation with Human Judgments. In Proceedings of
the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-
tion and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005,
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association
for Computational Linguistics, 65â€“72. https://aclanthology.org/W05-0909/
[9] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of
python functions and documentation strings for automated code documentation
and code generation. arXiv preprint arXiv:1707.02275 (2017).

[10] Simon Butler, Michel Wermelinger, Yijun Yu, and Helen Sharp. 2010. Exploring
the Influence of Identifier Names on Code Quality: An Empirical Study. In 14th
European Conference on Software Maintenance and Reengineering, CSMR 2010,
15-18 March 2010, Madrid, Spain. IEEE Computer Society, 156â€“165. https://doi.
org/10.1109/CSMR.2010.27

[11] Qingying Chen and Minghui Zhou. 2018. A neural framework for retrieval and
summarization of source code. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ASE 2018, Montpellier, France,
September 3-7, 2018, Marianne Huchard, Christian KÃ¤stner, and Gordon Fraser
(Eds.). ACM, 826â€“831. https://doi.org/10.1145/3238147.3240471

[12] Junyan Cheng, Iordanis Fostiropoulos, and Barry W. Boehm. 2021. GN-
Transformer: Fusing Sequence and Graph Representation for Improved Code
Summarization. CoRR abs/2111.08874 (2021). arXiv:2111.08874 https://arxiv.org/
abs/2111.08874

[13] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and KÃ¡thia MarÃ§al de Oliveira.
2005. A study of the documentation essential to software maintenance. In Pro-
ceedings of the 23rd Annual International Conference on Design of Communication:
documenting & Designing for Pervasive Information, SIGDOC 2005, Coventry, UK,
September 21-23, 2005, Scott R. Tilley and Robert M. Newman (Eds.). ACM, 68â€“75.
https://doi.org/10.1145/1085313.1085331

[14] Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas,
and Richard A. Harshman. 1990. Indexing by Latent Semantic Analysis. J. Am.
Soc. Inf. Sci. 41, 6 (1990), 391â€“407.

[15] Brian P. Eddy, Jeffrey A. Robinson, Nicholas A. Kraft, and Jeffrey C. Carver. 2013.
Evaluating source code summarization techniques: Replication and expansion.
In IEEE 21st International Conference on Program Comprehension, ICPC 2013, San
Francisco, CA, USA, 20-21 May, 2013. IEEE Computer Society, 13â€“22. https:
//doi.org/10.1109/ICPC.2013.6613829

[16] Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, and Xin Xia.
2021. Code Structure Guided Transformer for Source Code Summarization. CoRR
abs/2104.09340 (2021). arXiv:2104.09340 https://arxiv.org/abs/2104.09340
[17] Xinwei Geng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2018. Adaptive Multi-pass
Decoder for Neural Machine Translation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -
November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junâ€™ichi
Tsujii (Eds.). Association for Computational Linguistics, 523â€“532. https://doi.
org/10.18653/v1/d18-1048

[18] David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to Com-
ment "Translation": Data, Metrics, Baselining & Evaluation. In 35th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2020, Melbourne,
Australia, September 21-25, 2020. 746â€“757.

[19] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program
comprehension with source code summarization. In Proceedings of the 32nd
ACM/IEEE International Conference on Software Engineering - Volume 2, ICSE 2010,
Cape Town, South Africa, 1-8 May 2010, Jeff Kramer, Judith Bishop, Premkumar T.
Devanbu, and SebastiÃ¡n Uchitel (Eds.). ACM, 223â€“226. https://doi.org/10.1145/
1810295.1810335

[20] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the
Use of Automated Text Summarization Techniques for Summarizing Source Code.

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang

In 17th Working Conference on Reverse Engineering, WCRE 2010, 13-16 October
2010, Beverly, MA, USA, Giuliano Antoniol, Martin Pinzger, and Elliot J. Chikofsky
(Eds.). IEEE Computer Society, 35â€“44. https://doi.org/10.1109/WCRE.2010.13

[21] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark,
Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William
Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin,
Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dong-
dong Zhang, Zhirui Zhang, and Ming Zhou. 2018. Achieving Human Parity on
Automatic Chinese to English News Translation. CoRR abs/1803.05567 (2018).
arXiv:1803.05567 http://arxiv.org/abs/1803.05567

[22] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of
feature detectors. CoRR abs/1207.0580 (2012). arXiv:1207.0580 http://arxiv.org/
abs/1207.0580

[23] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. 9, 8 (1997), 1735â€“1780. https://doi.org/10.1162/neco.1997.9.8.1735
[24] Ke Hu, Ruoming Pang, Tara N. Sainath, and Trevor Strohman. 2021. Transformer
Based Deliberation for Two-Pass Speech Recognition. In IEEE Spoken Language
Technology Workshop, SLT 2021, Shenzhen, China, January 19-22, 2021. IEEE, 68â€“74.
https://doi.org/10.1109/SLT48900.2021.9383497

[25] Ke Hu, Tara N. Sainath, Ruoming Pang, and Rohit Prabhavalkar. 2020. Delib-
eration Model Based Two-Pass End-To-End Speech Recognition. In 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020,
Barcelona, Spain, May 4-8, 2020. IEEE, 7799â€“7803.
https://doi.org/10.1109/
ICASSP40776.2020.9053606

[26] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment
generation. In Proceedings of the 26th Conference on Program Comprehension, ICPC
2018, Gothenburg, Sweden, May 27-28, 2018, Foutse Khomh, Chanchal K. Roy, and
Janet Siegmund (Eds.). ACM, 200â€“210. https://doi.org/10.1145/3196321.3196334
[27] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
Source Code with Transferred API Knowledge. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July
13-19, 2018, Stockholm, Sweden. 2269â€“2275.

[28] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for
Computer Linguistics. https://doi.org/10.18653/v1/p16-1195

[29] He Jiang, Liming Nie, Zeyi Sun, Zhilei Ren, Weiqiang Kong, Tao Zhang, and Xiapu
Luo. 2019. ROSF: Leveraging Information Retrieval and Supervised Learning
for Recommending Code Snippets. IEEE Trans. Serv. Comput. 12, 1 (2019), 34â€“46.
https://doi.org/10.1109/TSC.2016.2592909

[30] Mira Kajko-Mattsson. 2005. A Survey of Documentation Practice within Correc-
tive Maintenance. Empir. Softw. Eng. 10, 1 (2005), 31â€“55. https://doi.org/10.1023/B:
LIDA.0000048322.42751.ca

[31] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980

[32] Amy J. Ko, Brad A. Myers, Michael J. Coblenz, and Htet Htet Aung. 2006. An
Exploratory Study of How Developers Seek, Relate, and Collect Relevant Infor-
mation during Software Maintenance Tasks. IEEE Trans. Software Eng. 32, 12
(2006), 971â€“987. https://doi.org/10.1109/TSE.2006.116

[33] Alexander LeClair, Aakash Bansal, and Collin McMillan. 2021. Ensemble Models
for Neural Source Code Summarization of Subroutines. In IEEE International
Conference on Software Maintenance and Evolution, ICSME 2021, Luxembourg, Sep-
tember 27 - October 1, 2021. IEEE, 286â€“297. https://doi.org/10.1109/ICSME52107.
2021.00032

[34] Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin. 2021. EditSum: A
Retrieve-and-Edit Framework for Source Code Summarization. In 36th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2021, Melbourne,
Australia, November 15-19, 2021. IEEE, 155â€“166. https://doi.org/10.1109/ASE51524.
2021.9678724

[35] Yangming Li and Kaisheng Yao. 2020. Rewriter-Evaluator Framework for Neural
Machine Translation. CoRR abs/2012.05414 (2020). arXiv:2012.05414 https:
//arxiv.org/abs/2012.05414

[36] Zheng Lian, Yanan Zhang, Haichang Li, Rui Wang, and Xiaohui Hu. 2021. Cross
Modification Attention Based Deliberation Model for Image Captioning. CoRR
abs/2109.08411 (2021). arXiv:2109.08411 https://arxiv.org/abs/2109.08411
[37] Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries.

In Text summarization branches out. 74â€“81.

[38] Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation:
how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. 373â€“384.

[39] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey
Svyatkovskiy. 2022. ReACC: A Retrieval-Augmented Code Completion Frame-
work. CoRR abs/2203.07722 (2022). https://doi.org/10.48550/arXiv.2203.07722

arXiv:2203.07722

[40] Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Sum-
marization of Context for Java Methods. IEEE Trans. Software Eng. 42, 2 (2016),
103â€“119. https://doi.org/10.1109/TSE.2015.2465386

[41] Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L. Pollock,
and K. Vijay-Shanker. 2013. Automatic generation of natural language summaries
for Java classes. In IEEE 21st International Conference on Program Comprehension,
ICPC 2013, San Francisco, CA, USA, 20-21 May, 2013. IEEE Computer Society,
23â€“32. https://doi.org/10.1109/ICPC.2013.6613830

[42] Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan
Srinivasan, and Balaraman Ravindran. 2019. Letâ€™s Ask Again: Refine Network
for Automatic Question Generation. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun
Wan (Eds.). Association for Computational Linguistics, 3312â€“3321. https://doi.
org/10.18653/v1/D19-1326

[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:
a Method for Automatic Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics, July
6-12, 2002, Philadelphia, PA, USA. ACL, 311â€“318. https://doi.org/10.3115/1073083.
1073135

[44] Ehud Reiter. 2018. A structured Review of the Validity of BLEU. Computational

Linguistics 44, 3 (2018), 393â€“401.

[45] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance
Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333â€“389.
https://doi.org/10.1561/1500000019

[46] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In Proceedings of the
2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages, MAPL@PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018, Justin
Gottschlich and Alvin Cheung (Eds.). ACM, 31â€“41. https://doi.org/10.1145/
3211346.3211353

[47] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A Vector Space Model

for Automatic Indexing. Commun. ACM 18, 11 (1975), 613â€“620.

[48] Andrea Schankin, Annika Berger, Daniel V. Holt, Johannes C. Hofmeister, Till
Riedel, and Michael Beigl. 2018. Descriptive compound identifier names improve
source code comprehension. In Proceedings of the 26th Conference on Program
Comprehension, ICPC 2018, Gothenburg, Sweden, May 27-28, 2018. ACM, 31â€“40.
https://doi.org/10.1145/3196321.3196332

[49] Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar, and David M. Pennock.
2002. Methods and metrics for cold-start recommendations. In SIGIR 2002: Pro-
ceedings of the 25th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, August 11-15, 2002, Tampere, Finland,
Kalervo JÃ¤rvelin, Micheline Beaulieu, Ricardo A. Baeza-Yates, and Sung-Hyon
Myaeng (Eds.). ACM, 253â€“260. https://doi.org/10.1145/564376.564421

[50] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-
Shanker. 2010. Towards automatically generating summary comments for Java
methods. In ASE 2010, 25th IEEE/ACM International Conference on Automated
Software Engineering, Antwerp, Belgium, September 20-24, 2010, Charles Pecheur,
Jamie Andrews, and Elisabetta Di Nitto (Eds.). ACM, 43â€“52. https://doi.org/10.
1145/1858996.1859006

[51] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao
Wang, and Yichen Wei. 2020. Circle Loss: A Unified Perspective of Pair Similar-
ity Optimization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. Computer Vision
Foundation / IEEE, 6397â€“6406. https://doi.org/10.1109/CVPR42600.2020.00643

[52] Tzu-Wei Sung, Jun-You Liu, Hung-yi Lee, and Lin-Shan Lee. 2019. Towards End-
to-end Speech-to-text Translation with Two-pass Decoding. In IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton,
United Kingdom, May 12-17, 2019. IEEE, 7175â€“7179. https://doi.org/10.1109/
ICASSP.2019.8682801

[53] Armstrong A. Takang, Penny A. Grubb, and Robert D. Macredie. 1996. The
effects of comments and identifier names on program comprehensibility: an
experimental investigation. J. Program. Lang. 4, 3 (1996), 143â€“167.
http://
compscinet.dcs.kcl.ac.uk/JP/jp040302.abs.html

[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA. 5998â€“6008.

[55] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr:
Consensus-based image description evaluation. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. IEEE
Computer Society, 4566â€“4575. https://doi.org/10.1109/CVPR.2015.7299087
[56] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving automatic source code summarization via deep

Automatic Comment Generation via Multi-Pass Deliberation

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Confer-
ence on Automated Software Engineering, ASE 2018, Montpellier, France, September
3-7, 2018. 397â€“407.

[57] Haoye Wang, Xin Xia, David Lo, Qiang He, Xinyu Wang, and John Grundy. 2021.
Context-aware Retrieval-based Deep Commit Message Generation. ACM Trans.
Softw. Eng. Methodol. 30, 4 (2021), 56:1â€“56:30. https://doi.org/10.1145/3464689

[58] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine:
Exemplar-based Neural Comment Generation. In 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia,
September 21-25, 2020. 349â€“360.

[59] Sam Wiseman and Alexander M. Rush. 2016. Sequence-to-Sequence Learning as
Beam-Search Optimization. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, No-
vember 1-4, 2016, Jian Su, Xavier Carreras, and Kevin Duh (Eds.). The Association
for Computational Linguistics, 1296â€“1306. https://doi.org/10.18653/v1/d16-1137
[60] Edmund Wong, Taiyue Liu, and Lin Tan. 2015. CloCom: Mining existing source
code for automatic comment generation. In 22nd IEEE International Conference
on Software Analysis, Evolution, and Reengineering, SANER 2015, Montreal, QC,
Canada, March 2-6, 2015. IEEE Computer Society, 380â€“389. https://doi.org/10.
1109/SANER.2015.7081848

[61] Edmund Wong, Jinqiu Yang, and Lin Tan. 2013. AutoComment: Mining question
and answer sites for automatic comment generation. In 2013 28th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2013, Silicon
Valley, CA, USA, November 11-15, 2013. IEEE, 562â€“567. https://doi.org/10.1109/

ASE.2013.6693113

[62] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and Shan-
ping Li. 2018. Measuring program comprehension: a large-scale field study
with professionals. In Proceedings of the 40th International Conference on Soft-
ware Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, Michel
Chaudron, Ivica Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 584.
https://doi.org/10.1145/3180155.3182538

[63] Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-
Yan Liu. 2017. Deliberation Networks: Sequence Generation Beyond One-
Pass Decoding. In Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy
Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (Eds.). 1784â€“1794.
https://proceedings.neurips.cc/paper/2017/hash/
c6036a69be21cb660499b75718a3ef24-Abstract.html

[64] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In ICSE â€™20: 42nd International
Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020,
Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 1385â€“1397. https://doi.org/
10.1145/3377811.3380383

[65] Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, and
Harald C. Gall. 2021. Adversarial Robustness of Deep Code Comment Generation.
CoRR abs/2108.00213 (2021). arXiv:2108.00213 https://arxiv.org/abs/2108.00213

