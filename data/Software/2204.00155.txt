How heated is it? Understanding GitHub locked issues

Isabella Ferreira
Polytechnique Montréal
Montréal, Canada
isabella.ferreira@polymtl.ca

Bram Adams
Queen’s University
Kingston, Canada
bram.adams@queensu.ca

Jinghui Cheng
Polytechnique Montréal
Montréal, Canada
jinghui.cheng@polymtl.ca

2
2
0
2

r
p
A
1

]
E
S
.
s
c
[

1
v
5
5
1
0
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
Although issues of open source software are created to discuss and
solve technical problems, conversations can become heated, with
discussants getting angry and/or agitated for a variety of reasons,
such as poor suggestions or violation of community conventions.
To prevent and mitigate discussions from getting heated, tools
like GitHub have introduced the ability to lock issue discussions
that violate the code of conduct or other community guidelines.
Despite some early research on locked issues, there is a lack of
understanding of how communities use this feature and of potential
threats to validity for researchers relying on a dataset of locked
issues as an oracle for heated discussions. To address this gap, we
(i) quantitatively analyzed 79 GitHub projects that have at least
one issue locked as too heated, and (ii) qualitatively analyzed all
issues locked as too heated of the 79 projects, a total of 205 issues
comprising 5,511 comments. We found that projects have different
behaviors when locking issues: while 54 locked less than 10% of
their closed issues, 14 projects locked more than 90% of their closed
issues. Additionally, locked issues tend to have a similar number of
comments, participants, and emoji reactions to non-locked issues.
For the 205 issues locked as too heated, we found that one-third do
not contain any uncivil discourse, and only 8.82% of the analyzed
comments are actually uncivil. Finally, we found that the locking
justifications provided by maintainers do not always match the
label used to lock the issue. Based on our results, we identified
three pitfalls to avoid when using the GitHub locked issues data
and we provide recommendations for researchers and practitioners.

CCS CONCEPTS
• Human-centered computing → Empirical studies in collab-
orative and social computing; • Software and its engineering
→ Open source model.

KEYWORDS
github locked issues, heated discussions, incivility, civility

1 INTRODUCTION
In open source software (OSS) development, community members
use Issue Tracking Systems (ITSs) (e.g., Jira, Bugzilla, and GitHub
Issues) to discuss various topics related to their projects. Such ITSs
provide a set of features that streamline communication and col-
laboration by promoting discussions around bug reports, requests
of new features or enhancements, questions about the community
and the project, and documentation feedback [1, 12].

Although issue reports play a crucial role in software develop-
ment, maintenance, and evolution, issue discussions can get heated
(or “uncivil”), resulting in unnecessarily disrespectful conversa-
tions and personal attacks. This type of unhealthy, and sometimes

disturbing or harmful behavior can be the result of a variety of
reasons. For example, even though diversity has many benefits for
open source communities [29, 30], the mix of cultures, personali-
ties, and interests of open source contributors can cause a clash of
personal values and opinions [2]. Furthermore, as a social-technical
platform, ITSs sometimes host social context discussions, such as
conversations about the black lives matter and me too movements,
which can increase the chances of conflicts and arguments. Those
discussions seek for a more anti-oppressive software terminology,
such as renaming the branch master to main, whitelist/blacklist to
allowlist/blocklist and gender-neutral pronouns. Finally, the increas-
ing level of stress and burnout among OSS contributors can also
cause unhealthy interactions [18]. In fact, the amount of requests
that OSS maintainers receive is overwhelming and the aggressive
tone of some OSS interactions drains OSS developers [18].

Such heated interactions can have many negative consequences
for OSS projects. Ferreira et al. [10] have found that both main-
tainers and developers often discontinue further conversation and
escalate the uncivil communication in code review discussions.
Egelman et al. [9] also found that interpersonal conflicts in code
review can trigger negative emotions in developers. These com-
munication styles might also hinder OSS communities’ ability to
attract, onboard, and retain contributors.

To help OSS projects deal with some of the aforementioned
challenges, in June 2014, GitHub released a feature that allows
project owners to lock issues, pull requests, and commit conversa-
tions [13], basically prohibiting further comments. The main goal
of this feature is to smooth out too heated conversations that violate
the community’s code of conduct [15, 28] or GitHub’s community
guidelines [5]. However, conversations can also be locked for other
reasons, such as off-topic, resolved, or spam. Contributors might also
choose to lock issues without providing a reason.

Since locked issues have been manually tagged by community
experts rather than by researchers or classifiers, this data of locked
issues provides a potentially valuable dataset for software engi-
neering researchers aiming to understand how OSS communities
handle possibly harmful conversations. A few very recent previous
studies have used this dataset, in particular the subset of too heated
locked issues, as an oracle to detect toxicity in software engineering
discussions [18], and to understand when, how, and why toxicity
happens on GitHub locked issues [17]. However, to the best of our
knowledge, none of these studies have performed an in-depth in-
vestigation of the nature of GitHub locked issues in general and the
validity of the too heated locked issues in particular as a potential
oracle.

Hence, in this paper, we adopt a mixed-methods approach and
aim at assessing the characteristics of GitHub locked issues. First,
we quantitatively analyzed 1,272,501 closed issue discussions of 79
open source projects hosted on GitHub that have at least one issue

 
 
 
 
 
 
locked as too heated. This analysis is aimed at identifying the overall
characteristics of GitHub locked and non-locked issues. Then, we
qualitatively examined all 205 issues locked as too heated in the
analyzed projects, and their 5,511 comments, to assess the extent
to which the issue discussions locked as too heated were, in fact,
uncivil. For this, we identified the tone-bearing discussion features
(TBDFs) of issue discussions [10], i.e., “conversational characteristics
demonstrated in a written sentence that convey a mood or style of
expression.” We then identified heated discussions based on the
presence of uncivil TBDFs, i.e., “features of discussion that convey an
unnecessarily disrespectful tone.” Additionally, we assessed the topics
being discussed by too heated locked issues and the justifications
given by maintainers for locking such issues.

In summary, we make the following contributions:
• To the best of our knowledge, this is the first study shedding
light on the usage patterns of the GitHub locking conversa-
tions feature;

• We found that projects have different behaviors to lock issues,
that the locking justifications given by maintainers do not
always match the label on the GitHub platform, and that not
all issues locked as too heated are uncivil;

• We identified three pitfalls and provided a set of recommen-
dations of what researchers should do and not do when using
this dataset;

• We provide three recommendations for practitioners and de-

signers of ITSs;

• We make a replication package1 available that contains (i)
the codebooks used in the qualitative coding, (ii) the manu-
ally tagged dataset of issues locked as too heated, containing
sentences coded with TBDFs, the topics of discussion, and
the justifications given by maintainers, and (iii) the scripts to
analyze the data.

2 RELATED WORK: UNHEALTHY

INTERACTIONS IN OSS

Our work is related to a few very recent studies [3, 10, 17, 18, 23]
that investigate unhealthy interactions, mostly in the form of inci-
vility and toxicity, that happen in OSS contexts. Ferreira et al. [10]
investigated incivility (i.e., “heated discussions that involve unneces-
sary disrespectful comments and personal attacks”) in code review
discussions of rejected patches of the Linux Kernel Mailing List.
They found that incivility was present in 66% of the non-technical
code review emails of rejected patches; they also identified that
frustration, name calling, and impatience were the most common
types of incivility. Miller et al. [17] investigated toxicity, which
is defined as “rude, disrespectful, or unreasonable language that is
likely to make someone leave a discussion” in 100 open source issue
discussions (including 20 GitHub issues locked as too heated). The
authors found that entitlement, insults, and arrogance were the
most common types of toxicity in issue discussions.

Because the nature of discussions is different, previous work
found different causes of unhealthy interactions in code review
and issue discussions. In code review discussions, incivility was
found to be mainly caused by the violation of community con-
ventions, inappropriate solutions proposed by the developers, and

1Replication package: https://doi.org/10.6084/m9.figshare.18848765.v2

Ferreira et al.

characteristics of the maintainers’ feedback [10]. However, in is-
sue discussions, users have been found to write toxic comments
when having problems using the software, holding different points
of view about technical topics, or being in a disagreement about
politics and ideology (e.g., OSS philosophy) [17].

These studies also identified patterns of reactions from OSS com-
munities to unhealthy interactions. In code reviews, maintainers
often discontinue further conversation or escalate uncivil commu-
nication [10]. In issue discussions, although maintainers and other
members may try to engage in a constructive conversation after
toxicity happens, the discussion might still escalate to more toxic-
ity [17]. However, these discussions seemed to be localized; only
in a few cases, the author who posted toxic comments would open
another toxic issue [17]. Additionally, when maintainers invoke
the code of conduct [28], the author of the toxic comments usually
did not comment anymore. These previous results indicate that
locking issues might be effective to stop toxicity. Our study is built
upon these recent works to focus on understanding (i) how GitHub
locked issues are used in practice and (ii) to what extent GitHub
issues locked as too heated are indeed uncivil.

Researchers have also worked on building classifiers to detect
unhealthy interactions. Recently, Cheriyan et al. [3] have proposed
a machine learning-based method to detect and classify offensive
comments into swearing and profanity. The classifier is trained
with a dataset including conversations in GitHub Issues, Gitter,
Slack, and Stack Overflow. Raman et al. [18] created a classifier
to automatically detect toxicity on GitHub. The authors manually
validated GitHub issues locked as too heated to train the classifier;
their classifier obtained a satisfiable precision but low recall.

Finally, Sarker et al. [23] analyzed how different toxicity detectors
perform on software engineering data, finding that none of the
analyzed detectors achieved a good performance. However, the
design of such detectors often lacks an in-depth understanding of
the characteristics of the dataset. We contribute to filling this gap by
investigating the reliability and pitfalls of using the GitHub locked
issues data, in particular too heated locked issues, to train machine
learning detectors of incivility.

3 STUDY DESIGN
3.1 Study goal and research questions
The general goal of this study is to understand the nature of GitHub
locked issues and to identify the common pitfalls that could pose a
threat to validity when using a (sub)set of GitHub locked issues as
an oracle for uncivil communication. Our specific goals are to (i)
quantitatively characterize GitHub locked issues in comparison to
non-locked issues and (ii) qualitatively assess the actual discussion
tones in issues locked as too heated. Based on these goals, we con-
structed four main research questions to guide our study, which
we present below along with their motivations.

RQ1. What are the characteristics of GitHub locked issues?
To the best of our knowledge, only the study conducted by Miller
et al. [17] has studied GitHub locked issues, in particular focusing
on when, how, and why toxicity happens on a sample of 20 locked
issues. In our quantitative study, we aim to conduct a broader anal-
ysis of locked issues to identify their overall characteristics. It is

How heated is it? Understanding GitHub locked issues

essential to gain this understanding to make informed decisions
about mining this kind of data and to understand how different
open source projects use this feature. To this end, we assess how
often projects lock issues, as well as how different locked issues are
in comparison to issues that are not locked, in terms of the number
of comments, the number of people participating in the discussion,
and the number of emoji reactions.

RQ2. What are the justifications given in the comments by
project maintainers when locking issues as too heated? Project
maintainers can choose predefined reasons (e.g., too heated or spam)
to lock an issue on GitHub. However, these predefined reasons are
abstract and sometimes difficult to interpret. Maintainers often
need to communicate the specific justifications of locking a too-
heated issue with the community in the issue comments in order
to explain their actions, educate the community members, and/or
maintain their authority. In this RQ, we aim at understanding how
maintainers communicate these justifications.

RQ3. What are the topics being discussed in issues locked
as too heated? Earlier work by Ferreira et al. [10] did not find any
correlation between the topics of code review discussions and the
presence of incivility. However, this finding might not necessarily
apply to issue discussions because code review and issues discus-
sions have different focuses (the former on the solution space while
the latter on the problem space) and participant dynamics (there is a
smaller power distance between maintainers and other discussants
in issue discussions than in code reviews). Thus, in this RQ, we aim
at examining the topics of discussion in issues locked as too heated
in order to analyze the presence of potentially provocative topics.

RQ4. To what extent are issues locked as too heated uncivil?
In this RQ, we aim at investigating the extent to which issues locked
as too heated do, in fact, involve heated interactions. We used the
characterization of incivility of Ferreira et al. [10] to identify the
uncivil tones in issue discussions as a concrete measure of heated
discussions. Our analyses were split into four sub-RQs.

First, we aim at answering RQ4.1. What are the features of
discussion in issues locked as too heated? This is done by identi-
fying the tone-bearing discussion features (TBDFs) of the sentences
of these issues. TBDFs capture the “conversational characteristics
demonstrated in a written sentence that convey a mood or style of
expression” [10]. As an example of a TBDF, the following sentence
shows that a speaker is frustrated: “I’m fed up the whole framework
is filled with this crap. Why you even do double way binding if works
half the needed cases? Don’t even make a framework at this point.”
(project angular/angular), demonstrating therefore a mood or
style of expression. Because the original TBDF framework was cre-
ated through analysis of code reviews, we adapted this framework
to the context of issue discussions, then used this adapted frame-
work to identify the TBDFs in each sentence of the issue discussions
in our sample.

Then, we answer RQ4.2. How uncivil are issues locked as
too heated?. For this, we use the notion of uncivil TBDFs, which
are “features of discussion that convey an unnecessarily disrespectful
tone” [10]. We consider an issue or comment as technical if none
of its sentences demonstrate any TBDF in RQ4.1, uncivil if at least

one sentence demonstrates an uncivil TBDF, and civil if at least
one sentence demonstrates a TBDF but none of these TBDFs are
uncivil. Conceptually, uncivil issues and comments correspond to
inappropriate and unhealthy discussions.

Finally, we use the aforementioned categorization of issues and
comments to assess correlations between (1) the TBDFs demon-
strated in an issue discussion and (2) justifications given by project
contributors for locking an issue and the topics of the too heated
issues. Thus we ask: RQ4.3. How are the observed discussion
feature types distributed across the justifications given by
project contributors when locking too heated issues? and RQ4.4.
How are the observed discussion feature types distributed across
the different discussion topics in too heated issues?

3.2 Data selection
Our research questions require open source projects with a suffi-
cient number and variety of locked issues. Since no pre-built dataset
exists, we used a two-pronged approach. First of all, we selected
projects from 32 different GitHub collections2, which are curated
lists of diverse and influential GitHub projects. We selected projects
that (i) have more than 1,000 issues, (ii) have code commits later
than November 2020 (six months before our data collection), and
(iii) have at least one issue locked as too heated until 2021-06-03.
This resulted in 29 GitHub projects.

Second, we also added projects open-sourced by three large, well-
known software companies (Apple, Google, and Microsoft), since
we hypothesized that such projects might encounter more polar-
ized discussions. For this, we mined the companies’ corresponding
GitHub organization, but also added open-sourced projects not
hosted within the organization (such as Bazel and Kubernetes).
This resulted in 3,931 projects,including 86 projects from Apple,
1,240 projects from Google, and 2,605 projects from Microsoft. Filter-
ing out projects without issues locked as too heated until 2021-06-03
resulted in 50 projects, i.e., 1 project from Apple, 23 from Google,
and 26 from Microsoft.

Then, we collected all closed issues from the resulting sample
of 79 projects until 2021-06-03 using the GitHub REST API3. We
chose to analyze only closed issues in order to observe complete
issue discussions. For each closed issue, we recorded whether the
issue has been locked [7], the issue comments, the emoji reactions
to each comment, all the events related to the issue (e.g., when the
issue was locked), the dates that the issue was opened, closed, and
locked (if so), and the contributors who performed these actions.

3.3 Quantitative analysis on locked issues
To answer RQ1, we considered two independent groups: locked
and non-locked issues, as well as three dependent variables. We
discuss the hypothesis related to each dependent variable below.

Number of comments: Ferreira et al. found that review discus-
sions with arguments tend to be longer and have more uncivil than
civil emails [10]. We thus hypothesize that locked issue discussions
have more comments than non-locked discussions (H1).

Number of participants: Previous research found that discus-
sions involving people with different backgrounds and cultures

2https://github.com/collections
3https://docs.github.com/en/rest/reference/issues

are more likely to be uncivil [22]. Since OSS projects are highly
diverse, we hypothesize that locked issues have more participants
than non-locked issues (H2).

Number of emoji reactions: Previous work has shown that
emoji reactions reduce unnecessary commenting in pull request
discussions, leading to fewer conflicts [24]. We thus hypothesize
that locked issues have fewer emoji reactions than non-locked
issues (H3).

We aggregate the dependent variables per issue for each indepen-
dent group. Following the central limit theorem [14] and because
of the large number of issues in our dataset (about 1.3 million),
we used unpaired t-tests [33] to determine if there is a significant
difference in the means of each dependent variable between the two
independent groups. Then, we computed the effect size between
the means of the two groups for each dependent variable using
Cohen’s d [20].

3.4 Qualitative analysis on locked issues
To answer RQ2, RQ3, and RQ4, we conducted a qualitative analy-
sis [25, 26] on the GitHub issues locked as too heated.

Identifying the justifications to lock GitHub issues. In RQ2, we
3.4.1
aim at investigating the justifications given by community members
themselves when locking issues as too heated. We approached the
coding through the following steps. First, we read the title and
the first comment (description) of the issue to understand what
the issue is about. Then, we read the last comment of the issue. If
the last comment mentions that the issue is being locked and it
is clear why it is the end of the discussion, we then coded for the
justification given by the community member. If the last comment
does not mention that the issue is being locked or it is not clear
why, we then searched for other comments justifying the reason
for locking the issue, as follows.

(1) We searched for the keywords “locking”, “locked”, “closing”,
“heated”. If we found a comment mentioning one of these key-
words, we read the comment and check if it mentions the justi-
fication for locking the issue. If not, we executed step (2).
(2) We read the entire issue thread looking for a justification to lock
the issue. If we did not find any justification, we then coded the
justification as “no reason mentioned”. We coded the respective
justification, otherwise.

The first author started with an inductive coding [27] on all
issues locked as too heated, a total of 205 issues. During the coding
process, we added the identified justifications in a codebook [21]
with the name of the code, a definition, and one or more examples.
The codebook containing all manually identified justifications was
improved during discussions with two other authors. Then, we
conducted axial coding and grouped the identified justifications
into themes [32].

To guarantee that the codebook can be replicated, the second
author deductively coded 20% [4, 11] of the issues (41 issues). Af-
terwards, the first and third authors discussed the disagreements,
improving our coding schema. We computed Cohen’s Kappa to
evaluate the inter-rater reliability of our coding schema [16]. The
average Kappa score between the two raters across all the identi-
fied justifications is 0.85, ranging from 0.64 to 1.00, demonstrating

Ferreira et al.

an almost perfect agreement [31]. The complete codebook can be
found in our replication package4.

Identifying the topic of the discussion. For RQ3, we first in-
3.4.2
ductively coded the issue title and the content being discussed at
the issue level. We then did axial coding to group our codes into
categories. A codebook was created with the axial codes and their
definitions. Two authors discussed and iteratively improved the
codebook, which can be found in our replication package5.

Identifying tone-bearing discussion features (TBDFs). We iden-
3.4.3
tify the discourse characteristics of issues locked as too heated (RQ4)
through tone-bearing discussion features (TBDFs) [10]. To initiate
the coding, the first author directly used the framework and the
codebook of 16 TBDFs proposed by Ferreira et al. [10]. To adapt
the coding schema in the issue discussion context, we added new
codes or adjusted the coding criteria of certain codes when appro-
priate. The coding was conducted at the sentence level of each issue
comment written in English and visible on GitHub (a total of 5,511
comments from the 205 issues locked as too heated). We also took
into consideration the context of the previous comments on the
same issue to identify the TBDF of a particular sentence.

The updated codebook was iteratively discussed and improved
with all authors. In the end, we added four TBDFs and adjusted the
coding criteria of eight TBDFs in the original codebook. The third
author then deductively coded 20% [4, 11] of the comments coded
with at least one TBDF by the first author (145 out of 718 comments).
We measured the inter-rater reliability, and the average Cohen’s
Kappa score between the two raters and across all the identified
TBDFs was 0.65, ranging from 0.43 to 0.91, showing substantial
agreement between the two raters [31]. The final codebook can be
found in our replication package5.

4 RESULTS
4.1 RQ1. Characteristics of GitHub locked

issues

Among the 1,272,501 closed issues of the 79 analyzed projects, we
identified three types of projects: 14 projects (17.72%) locked
more than 90% of their closed issues, 54 projects (68.35%)
locked less than 10% of their closed issues; the remaining 11
projects (13.92%) locked between 54% and 88% of their closed
issues with an average of 73% of locked issues. Figures 1 and 2
present the distribution of (non-)locked issues per project as well as
per locking reason mentioned on the GitHub platform, respectively.
Furthermore, we found that 313,731 (61.61%) locked issues
have been automatically locked by a bot (e.g., due to inactiv-
ity), 195,409 (38.38%) by an organization, and 52 (0.01%) by a
user. Interestingly, 16 projects (20.25%) had most of their issues
locked with no reason mentioned and 9 projects (11.4%) had most
of their issues locked as resolved. These 25 projects, specifically,
have 313,494 (62.13% of their locked issues) issues locked by a bot.
Concerning the length of locked and non-locked issues, an un-
paired t-test revealed that non-locked issues had a statistically
significantly larger number of comments (𝑚𝑒𝑎𝑛 = 5.72, 𝑆𝐷 = 10.54)
than locked issues (𝑚𝑒𝑎𝑛 = 5.05, 𝑆𝐷 = 8.23), 𝑡 = −40.587, 𝑝 < 0.001.

4Replication package: https://doi.org/10.6084/m9.figshare.18848765.v2

How heated is it? Understanding GitHub locked issues

Figure 1: Distribution of percentages of (non-)locked issues
per project.

Figure 2: Distribution of percentages of (non-)locked issues
per project according to locking reasons labeled on GitHub.

However, the difference between the means of these two variables
is negligible (Cohen’s 𝑑 = 0.07), rejecting H1. Similarly, we found
that non-locked issues involved a statistically significantly larger
number of participants (𝑚𝑒𝑎𝑛 = 2.92, 𝑆𝐷 = 2.48) than locked issues
(𝑚𝑒𝑎𝑛 = 2.86, 𝑆𝐷 = 3.21), 𝑡 = −11.559, 𝑝 < 0.001. However, Co-
hen’s 𝑑 = 0.02 indicated that this difference is negligible, rejecting
H2. Finally, we found that non-locked issues involved a statistically
significantly larger number of reactions (𝑚𝑒𝑎𝑛 = 0.13, 𝑆𝐷 = 0.85)
than locked issues (𝑚𝑒𝑎𝑛 = 0.07, 𝑆𝐷 = 0.60), 𝑡 = −40.631, 𝑝 < 0.001,
but this difference is again negligible (Cohen’s 𝑑 = 0.07), rejecting
H3.

Summary RQ1: We found three types of projects in terms
of issue locking behaviors: (1) 14 projects locked more than
90% of their closed issues, (2) 54 locked less than 10%, and (3)
the remaining 11 locked between 54% and 88% of their closed
issues. Furthermore, 31.65% of the projects have the majority
of issues locked by a bot. Finally, locked issues tended to
have a similar number of comments, participants, and emoji
reactions to non-locked issues.

4.2 RQ2. Justifications for locking GitHub

issues as too heated

In 70 issues (34.15% of the 205 too heated locked issues), the jus-
tification for locking the issue was not explicitly mentioned by
the project contributors and not clear from the discussion. In the

remaining 135 issues, we found ten categories of justifications
that project contributors gave when locking the issues as too
heated.

Inappropriate/unhealthy interaction was the justification for
locking 52 issues (25.37%). Contributors locked these issues by call-
ing out behaviors violating the code of conduct, asking other con-
tributors to keep the discourse civil, or mentioning that the kind
of behavior will not be tolerated by the community. Furthermore,
project contributors locked issues because the conversation was
starting to become uncivil, and in a few cases, the offensive com-
ments were even hidden. As an example: “This is not the place to post
this kind of message @[username], I’m closing the topic. Please follow
the contributing guidelines if you want to post anything constructive.”
(project angular/angular).

Off-topic. Project contributors explicitly mentioned that they
were locking 23 issues (11.22%) because the discussion was getting
off-topic. This includes cases where the issues were not actionable
or unrelated to the project goals, and/or people were discussing
the implications related to other issues but not the issue being
discussed. E.g., “Thanks for creating this issue. We think this issue
is unactionable or unrelated to the goals of this project. Please follow
our issue reporting guidelines.” (project microsoft/vscode).

Issue will not be addressed. The justification for locking 16 is-
sues (7.80%) was that the team decided not to address the issue. This
can be due to various reasons such as different motivations between
the team and the users, disagreement about licensing, geopolitical
or racial concerns, or project contribution process problems. E.g.,
“We discussed this issue on the SIG-arch call of 20200130, and have
unanimously agreed that we will keep the current naming for the
aforementioned reasons.” (project kubernetes/kubernetes).

Issue/PR status. The reason for locking 11 issues (5.37%) was
due to the issue or pull request status: duplicated, merged, not
mergeable, inactive, stale, fixed, abandoned, etc. In most cases in
this category, project contributors locked the issue instead of closing
it. E.g., “This PR is being closed because golang.org/cl/281212 has been
abandoned.” (project golang/go).

Wrong communication channel was the justification for lock-
ing 7 issues (3.41%). In this case, the contributor mentioned that
the problem should be discussed in another channel, such as the
mailing list or the IRC channel. E.g., “The community site is where we
are moving conversations about problems happening on travis-ci.com
or travis-ci.org. Thanks in advance for posting your questions over
there.” (project travis-ci/travis-ci).

Issue cannot be addressed. Project contributors locked 7 issues
(3.41%) because it was not feasible to address the issue under dis-
cussion. The underlying reason could be that the discussion did not
provide a reasonable way to address the problem, the issue is related
to another project or a dependency, or the project does not have
enough resources (such as personnel or infrastructure) to address
the issue. In most cases, issues in this category were locked as too
heated instead of closed. E.g., “I’m going to close this issue (and edit
the OP for fast reference) since the issue is deeper in the OS and the
Flutter framework can’t resolve it.” (project flutter/flutter).

Work prioritization. Project contributors locked 6 issues (2.93%)
to save time answering discussions and requests, or to focus on
other work aspects. E.g., “Closing this conversation to prevent more

0255075100LockedNon−locked% issues per project110100No reason mentionedOff−topicResolvedSpamToo heated% issues per project (in log10)“ETA requested” responses (which actually take time from feature
work)” (project firebase/FirebaseUI-Android).

Google Summer of Code, discussing about project financing, making
announcements about the end of the project, etc.

Ferreira et al.

Not following community rules. Project contributors locked
6 issues (2.93%) because contributors were not following the com-
munity rules. For example, contributors used +1 comments instead
of the emoji reaction +1, discussed too many problems in one issue,
or continued the discussion on issues with a similar topic. E.g.,
“I’ve locked this to contributors for now. Adding +1 comments is too
noisy. For future reference, add a reaction to the issue body, and don’t
comment.” (project ansible/ansible).

Address the issue in the future. Project contributors justified
the reason for locking 5 issues (2.44%) being that the problem will
be addressed in the future. That is, the contributor mentioned that
the bug will be triaged with other bugs, the issue needs further
investigation, or the bug will be addressed in the future. E.g., “As
always, thanks for reporting this. We’ll definitely be triaging this with
the rest of our bug fixes. In the meantime, however, please keep the
discourse civil.” (project microsoft/terminal).

Communication problems. There were 2 issues (0.97%) locked
because the discussion was not being productive or people could
not reach a consensus. E.g., “I’m going to close this thread, as the
conversation isn’t really productive after [link to a comment], I’m
afraid.” (project flutter/flutter).

Summary RQ2: We identified ten justifications that project
contributors gave when locking issues as too heated. In the
majority of the issues (74.63%), the justifications were not
related to the conversation being uncivil.

4.3 RQ3. Topics of discussions in issues locked

as too heated

In 12 issues (5.85%) that were locked as being too heated, we were
not able to identify the discussion topic from the issue title or the
comments. In the remaining 193 issues, we found 13 topics:

Source code problems was the topic of 78 issues (38.05%). Exam-
ples of such problems include deprecated functionality, encoding
problems, code warning, and installation problems.

User interface is the topic of 35 issue discussions (17.07%). Con-
tributors were concerned with the interface colors, the user inter-
face crashing or not responsive, or the need for changing icons.

Renaming. There were 15 issues (7.32%) discussing about re-
naming the software, the API, or certain terminologies due to racial
concerns, such as renaming master to main and whitelist to allowlist.
New feature. There were 15 issues (7.32%) locked as too heated
that were discussing the implementation of a new feature. In some
cases, the project was asking the community about feature ideas
for the next releases. In other cases, people were requesting to add
social features (such as Instagram and Twitter) to the terminal and
to consider specific syntax in the source code.

Community/project management. We found 12 issues (5.85%)
discussing topics related to the community and project management.
More specifically, contributors were asking the project’s owner to
give more privileges to other people to review and merge code,
criticizing censorship when removing comments and locking issues,
asking questions about coding programs such as freeCodeCamp and

Lack of accordance. Contributors expressed their opinion to
not work for a specific company or their opinion about a tool or
programming language in 9 issues (4.39%).

Data collection/protection. We found 7 issues (3.41%) discussing
data collection or data protection. More specifically, contributors
were concerned about the security, privacy, and ethical issues re-
lated to data collection, storage, and sharing.

Documentation. The topic of 6 issues (2.93%) was related to doc-
umentation, such as errors, missing information, or inappropriate
content (e.g., political banners) in the documentation.

Performance is the topic of 5 issues (2.44%). Contributors were
discussing the performance problems between two releases, the
application or the download of the application was very slow, and
interactivity with the webpage was very slow.

Error handling. 4 issues (1.95%) discussed compilation errors,

tool errors, or errors after a version update.

Translation. We found 3 issues (1.46%) discussing a problem
with the listing of languages in the software (e.g., listing Chinese
(China), and Chinese (Taiwan) separately), or regarding languages
from the Google Translator (e.g., Scottish Gaelic).

Versioning. There were 2 issues (0.98%) about requests to change
the version number of the tool or complaints about the tool not
following semantic versioning.

License. There were 2 issues (0.98%) about making changes in

the license file.

Summary RQ3: We found 13 topics being discussed in issues
locked as too heated, with source code problems, user interface,
and renaming being the most frequent topics.

4.4 RQ4. Incivility in issues locked as too

heated

In this RQ, we aim at assessing to what extent issues locked as too
heated feature uncivil discourse. We present the results of each
sub-RQ below.

4.4.1 RQ4.1. What are the features of discussion in issues locked
as too heated? We identified 20 tone-bearing discussion fea-
tures (TBDFs) in issue discussions locked as too heated, four
of which have not been found by previous work. In total,
1,212 distinct sentences were coded with a TBDF (a sentence
can be coded with more than one TBDF). We present below the de-
scription, an example, and the frequency of the four TBDFs uniquely
identified in the analyzed issue discussions. For the 16 TBDFs iden-
tified by Ferreira et al. [10], we present the frequency and the
conditions that were added to code such TBDFs when different
from previous work. For replication purposes, the description and
the example of TBDFs not described here can be found in our repli-
cation package5.

Positive features. Surprisingly, 151 sentences (12.46%) in issue
discussions locked as too heated actually expressed a posi-
tive tone. Considerateness is the most frequent positive feature

5Replication package: https://doi.org/10.6084/m9.figshare.18848765.v2

How heated is it? Understanding GitHub locked issues

(𝑁 = 61), followed by Appreciation and excitement (𝑁 = 58),
and Humility (𝑁 = 32). Contributors also expressed Appreciation
and excitement towards the project in issue discussions; e.g., “You
have a super reliable, and extendable set of tools. wcf provides the best
tools for building enterprise applications. Without it, you simply end
up rebuilding it.” (project dotnet/wcf).

Neutral features appear in 115 sentences (9.49%) of issue discus-
sions locked as too heated. In the context of issue discussions, we
have identified Expectation (𝑁 = 44) and Confusion (𝑁 = 12) as
two new neutral TBDFs. Additionally, Sincere apologies (𝑁 = 31),
Friendly joke (𝑁 = 25), and Hope to get feedback (𝑁 = 3) also
appear in issue discussions, similar to code review discussions [10].
Expectation is a new TBDF we identified in the issue discussions
and is the most frequent neutral feature in our dataset (𝑁 = 44).
This TBDF is expressed when the speaker expects to add a feature
in the future, to fix a specific problem, or that the feature should
do something specific. It is also expressed when the speaker is
expecting someone to resolve a problem or that the community will
work on a particular problem. E.g., “As a consumer of your product,
I expect it to work as advertised.” (project jekyll/jekyll).

We also identified Confusion, which is expressed when the
speaker is unable to think clearly or understand something (𝑁 = 12).
E.g., “I am confused because I add this ‘mixins‘ to as own which should
not affect any updates from Bootstrap.” (project twbs/bootstrap).

Negative features. There were 196 distinct sentences (16.17%)
demonstrating negative features. While indicating a negative
mood, these TBDFs do not involve a disrespectful tone. In our
dataset, we found Commanding (𝑁 = 61), Sadness (𝑁 = 40), and
Oppression (𝑁 = 9), which were already identified by previous
work [10]. However, different from code reviews, in issue discus-
sions contributors also expressed Sadness when the community
is going to lose something or someone (e.g., “It would be a great
loss for the .NET community if you’d stop contributing.” project
dotnet_runtime), and Oppression when a person of power rein-
forces their standpoints (e.g., “bro I am an Open Source author and
maintainer so don’t try lecturing me about being against "off-putting
towards the open-source community.” project dotnet/maui). Ad-
ditionally, we identified two new negative TBDFs: Dissatisfaction
(𝑁 = 75) and Criticizing oppression (𝑁 = 17)

Dissatisfaction appears when a simple change requires a lot of
discussions and the change is not accepted, when someone wants
to stop contributing because things never get resolved, or the com-
munity does not acknowledge the problem or is not willing to fix
the problem. E.g., “At this point I am discouraged to report more
because nothing ever seems to get fixed (usually because “it’s com-
plicated").” (project dotnet/roslyn). Additionally, contributors
might express dissatisfaction with the framework, tool, or process.
Criticizing oppression happens when someone of a lesser power
(e.g., developer) does not accept what someone (usually a person
of a higher power) says or how the person behaves. E.g., “Your
heavy-handed and dismissive approach to moderation diminishes the
project and the whole community.” (project nodejs/node).

Uncivil features. Uncivil features are those that convey an un-
necessarily disrespectful tone [10]. We identified 790 sentences

(65.18%) featuring at least one uncivil TBDF. Annoyance and
Bitter frustration is the most common uncivil feature (𝑁 = 288).
Issue discussions also demonstrate Name calling (𝑁 = 222), Mock-
ing (𝑁 = 194), Irony (𝑁 = 64), Impatience (𝑁 = 55), Vulgarity
(𝑁 = 51), and Threat (𝑁 = 18). Although contributors express
these uncivil TBDFs in both code review discussions [10] and issue
discussions, we found that several TBDFs had new interpretations
in the context of issue discussions, which we describe below.

Contributors tend to express Annoyance and Bitter frustra-
tion in issue discussions when they use capital letters to emphasize
something in a frustrating way, when someone is using abusive
language to express their opinion, when injustice makes the other
person feel unable to defend herself/himself, and when the speaker
is strongly irritated by something impossible to do in the speaker’s
opinion. Contributors might also mention that they are “fed up”,
“pissed off”, “sick”, and “tired” of something. E.g., “I’m not just an
angry fool, a lot of people are fed up with it and it actually is a
half-baked crippled tool.” (project angular/angular).

Name calling was expressed in issue discussions by mentioning
“you”, the name or identification of someone on GitHub (usually ex-
pressed by @username), or the name of a company in a sentence that
has a negative connotation. E.g., “Obviously you didn’t take my hint
on length to heart but don’t come to conclusions about my character
because you don’t know me at all.” (project flutter/flutter).
Contributors expressed Mocking in issue discussions by making
fun of the community rules or mimicking the way someone speaks.
E.g., “People can express their opinions but the mods are just gonna lock
it away “cause y’all can’t behave” and “the discussion is getting out of
hand and is not productive anymore”.” (project angular/angular).
Contributors expressed Impatience when other community
members asked them to work on a bug even if they do not have
enough resources, when they are unhappy with the situation that
exists for a long time, and when someone comments before reading
and understanding the message. E.g., “As I have stated multiple times
you can definitely work with Atom while offline (or with network
requests blocked).” (project atom/atom).

Threat is demonstrated in issue discussions when someone men-
tions that a person will be punished if they do not follow the code
of conduct, when contributors threaten to stop using a product, or
when someone is challenging someone else. E.g., “I have the urge to
drop Microsoft products and suggest to the company I work for that
we do the same wherever we can.” (project dotnet/roslyn).

Summary RQ4.1: We identified 20 TBDFs in issues locked
as too heated, four of which have never been found by previ-
ous work. Annoyance and bitter frustration, name calling, and
mocking are the most common TBDFs in the analyzed issues.

4.4.2 RQ4.2. How uncivil are issues locked as too heated? Building
on the sentence-level TBDF coding of RQ4.1, we then consider the
overall issue or comment as civil if it contains sentences coded
with positive, neutral, and/or negative features. An issue/comment is
considered uncivil if it contains sentences coded with at least one
uncivil TBDF. Finally, an issue/comment is considered technical if
none of its sentences are coded with a TBDF, i.e., the issue discussion
is focused only on technical aspects.

Ferreira et al.

Figure 4: Position of uncivil comments in uncivil issues.

Figure 5: Number of issues per justifications given by main-
tainers when locking issues as too heated.

shows that, in this case, the lack of justification given by the
project contributors is not a reliable reason to filter out such
data, since they still contain a high number of uncivil issues.
As expected, 96.15% of the issues locked with a justification of
inappropriate/unhealthy interaction included one or more uncivil
comments, while none were civil, and two issues (3.85%) were techni-
cal. These findings might be due to the fact that project contributors
can delete or hide heated comments, which we did not consider
in our analysis. When analyzing the off-topic justification for too
heated locked issues, we found that 69.57% (16) of those issues were
uncivil, 21.74% (5) were civil, and 8.70% (2) were technical. In this
case, the latter seven issues should have been locked by the project
contributor using the off-topic option, instead of too heated.

Issues locked due to inappropriate/unhealthy interaction
often demonstrate annoyance and bitter frustration, mock-
ing, and name calling. Figure 6 presents the frequency of TBDFs
per identified justification given by maintainers when locking issues
as too heated. Issues locked with no reason mentioned often demon-
strate annoyance and bitter frustration, mocking, and name calling.
Interestingly, too heated issues locked because the conversation
was off-topic often demonstrate mocking. This is because most of
the issues in this category are from the project microsoft/vscode
and are related to the Santagate event [19]. To celebrate the holiday

Figure 3: Distribution of the frequency of the three types of
comments across issues.

Discussions locked as too heated can still have civil com-
ments or even involve civil or technical comments only. From
the 205 issues locked as too heated, 138 (67.32%) of them are uncivil,
45 (21.95%) are technical, and 22 (10.73%) issues are civil. From the
5,511 comments part of the 205 issues, 4,793 (86.97%) of them are
technical, 486 (8.82%) are uncivil, and 232 (4.21%) are civil. We also
observe that the median numbers of technical, uncivil, and civil
comments in issues locked as too heated are 9, 1, and 0, respectively.
Figure 3 presents the distribution of the number of comments per
issue of the three types of comments.

Inspired by the coding framework of Miller et al. [17], we in-
vestigate where the uncivil comments are positioned in uncivil
issues. Particularly, we considered three locations: (1) in the issue
description, (2) in the first comment, and (3) in later comments
(i.e., emerged from the discussion). For each one of the 138 issues
that included at least one uncivil comment, the combination of the
above three locations resulted in seven conditions of where the
uncivil comments were positioned: (i) only in the issue description,
(ii) only in the first comment, (iii) only in the issue description and
the first comment, (iv) in the issue description and it emerged from
the discussion, (v) in the first comment and it emerged from the dis-
cussion, (vi) in the issue description, first comment, and it emerged
from the discussion, or (vii) only emerged from the discussion.

As shown in Figure 4, uncivil comments emerged from the
discussion in 88 issues (63.77%), incivility was present on the
issue description and it emerged from the discussion in 16 issues
(11.59%), and it was present on the first comment and emerged from
the discussion in 10 issues (7.25%).

Summary RQ4.2: Contrary to expectations, 32.68% of issues
locked as too heated are either technical or civil, and only
8.82% of the comments in issues locked as too heated are un-
civil. Additionally, uncivil comments emerge from the actual
discussion in 63.77% of the uncivil issues.

4.4.3 RQ4.3. How are the observed TBDF types distributed across
the locking justifications? As observed in Figure 5, although project
contributors did not mention a justification for locking 70 issues,
60% of those issues (42 issues) were uncivil; the remaining 28 issues
(40%) were either technical (22 issues) or civil (6 issues). This result

1101001000civiltechnicaluncivilComment code#comments per issue (in log10)10116118857Only first commentDescription and first commentDescription, first comment, and emerging from discussionOnly issue descriptionFirst comment and emerging from discussionDescription and emerging from discussionEmerging from discussion0255075100#uncivil issuesPosition of uncivil comment0204060No reasonInappropriate interactionOff−topicIssue will not be addressedIssue/PR statusWrong communication channelIssue cannot be addressedWork prioritizationNot following community rulesAddress the issue in the futureCommunication problemsJustifications for locking issues# issuesIssue codeciviltechnicaluncivilHow heated is it? Understanding GitHub locked issues

Figure 6: Justifications given by maintainers when locking
issues as too heated per TBDF.

season, the VS Code team added a Santa hat to the settings gear.
A user wrote an issue complaining that the Santa hat was very
offensive to him. After that, the VS Code team changed the icon to
a snowflake. However, many users got frustrated with that change
and started writing a lot of issues and comments filled with mocking.

Summary RQ4.3: Although all identified justifications for
too heated issues contain some proportion of issues with inci-
vility, contributors only called out for uncivil communication
in 25.37% of all issues, representing 37.68% of the uncivil
issues. Furthermore, 60% of the issues in which project con-
tributors did not mention a justification for locking the issue
are actually uncivil.

4.4.4 RQ4.4. How are the observed TBDF types distributed across the
discussion topics? Figure 7 shows the number of civil, technical, and
uncivil issues per topic of discussion. Although 56 issues (71.79%)
discussing source code problems are uncivil, 22 issues (28.25%) are
either technical (13 issues) or civil (9 issues). A similar pattern is
observed for the topics user interface and renaming.

Interestingly, none of the issues discussing community/project

management, data collection/protection, translation, and topic
not clear exhibit civility, i.e., issues discussing the aforemen-
tioned topics are either uncivil or technical. All issues discussing
versioning and license are uncivil, and issues discussing per-
formance and translation are more technical and civil than
uncivil (6 technical/civil issues (75%) against 2 uncivil issues (25%)).
Figure 8 presents the frequency of topics per TBDF. We found
that issues discussing source code problems often demonstrate
annoyance and bitter frustration and name calling.

Figure 7: Number of issues per topics of issues locked as too
heated.

Figure 8: Topics of issues locked as too heated per TBDF.

Summary RQ4.4: All too heated issues discussing versioning
and license were uncivil, and issues discussing performance
and translation tended to be more technical or civil.

5 DISCUSSION AND RECOMMENDATIONS
We present below a discussion about (i) how projects are using
the GitHub locking conversations feature, (ii) how incivility is ex-
pressed in issues locked as too heated, and (iii) recommendations
for researchers and practitioners.

5.1 How are projects using the GitHub locking

conversations feature?

We found that projects have different behaviors when locking their
issues. In fact, 14 projects locked more than 90% of their closed

6524121225110282181021213613123321312179219114151113211215101461311333167148221522243192810252141133111411125145576226142371711824623032354128181123854431711261124659151224130113241positiveneutralnegativeuncivilAddress the issue in the futureCommunication problemsInappropriate interactionIssue cannot be addressedIssue will not be addressedIssue/PR statusNo reasonNot following community rulesOff−topicWork prioritizationWrong communication channelAppreciation and ExcitementConsideratenessHumilityConfusionExpectationFriendly jokeHope to get feedbackSincere apologiesCommandingCriticizing oppressionDissatisfactionOppressionSadnessAnnoyance and Bitter frustrationImpatienceIronyMockingName callingThreatVulgarityJustifications for locking issuesTBDFs0255075100125150#sentences020406080Code problemsUser interfaceRenamingNew featureTopic not clearCommunity/project managementLack of accordanceData collection/protectionDocumentationPerformanceError handlingTranslationVersioningLicenseTopics# issuesIssue codeciviltechnicaluncivil752122417386111723028131361133211211115412211116191321923345815611111416231252531174122184132227115227331214314463411115912120111316114325312158112128415342324139322349107132182442111237854261641253positiveneutralnegativeuncivilCommunity/project managementData collection/protectionDocumentationError handlingLack of accordanceLicenseNew featurePerformanceRenamingSource code problemsTopic not clearTranslationUser interfaceVersioningAppreciation and ExcitementConsideratenessHumilityConfusionExpectationFriendly jokeSincere apologiesHope to get feedbackCommandingDissatisfactionOppressionSadnessCriticizing oppressionAnnoyance and Bitter frustrationImpatienceIronyMockingName callingVulgarityThreatTopicsTBDFs0255075100125150#sentencesissues, while 54 locked less than 10% of their closed issues. Fur-
thermore, the overall percentage of locked issues (40.02%) was
surprising to us; this might be due to the fact that 61.61% of the
locked issues were automatically locked by a bot (e.g., due to in-
activity). We also found that project maintainers give a variety of
justifications to the community when locking issues as too heated,
while most of the justifications are not related to the issue being
uncivil.

In fact, it seems that issues locked because the conversation was
off-topic should have been locked as “off-topic” instead, and issues
locked with other justifications (such as Issue/PR status, Issue will
not be addressed, or Issue cannot be addressed) should have been
closed normally instead of locked. Furthermore, we found that is-
sues locked as too heated often focused on discussing topics related
to source code problems, renaming, and user interface. For discus-
sions about source code problems, more specifically, if participants
abstain from demonstrating annoyance and bitter frustration, name
calling, and mocking, the conversation would be more civil. Some
topics seem to not trigger incivility though, such as performance
and translation.

5.2 How is incivility expressed in issues locked

as too heated?

Similar to code review discussions on LKML [10], we found that
issue discussions often demonstrate annoyance and bitter frustra-
tion and name calling. Furthermore, we found that issue discussions
feature expectation, confusion, dissatisfaction, and criticizing oppres-
sion, which have not been found in code review discussions of the
LKML [10]. That said, confusion was investigated in code review
discussions of Android, Facebook, and Twitter projects [8]. These
differences are most likely a result of the distinct ways in which is-
sues and code reviews are discussed (i.e., issues focus frequently on
the problem space while code reviews focus on the solution space)
as well as the communication style of different OSS communities.
Finally, we found that in most (63.77%) of the issues that included
an uncivil comment, those comments emerged during the discus-
sion, instead of appearing at the beginning of the issue. This might
happen due to a variety of reasons. In code review discussions,
uncivil comments emerge from the discussion when maintainers
do not immediately answer or developers do not solve the problem
with the provided information [17], or the maintainer’s feedback is
inadequate, there is a violation of community conventions, or poor
code quality [10]. However, the causes of incivility in locked issues
are still unknown and we encourage further studies to investigate
this direction.

5.3 Recommendations
Based on our results, we provide a set of recommendations for both
researchers and practitioners.

For researchers. We have identified three potential pitfalls
5.3.1
for researchers that use GitHub locked issues data “out of the box”,
and we provide recommendations to mitigate these problems.

Ferreira et al.

About one-third (31.65%) of the projects in our sample had the
majority (62.13%) of closed issues locked by a bot (e.g., the Lock
Threads bot6 that locks issues due to inactivity), instead of a main-
tainer. Thus, using the GitHub locked issues data as it is might be
misleading, since issues locked as resolved do not necessarily mean
that the issue is indeed resolved. In fact, the GitHub guidelines [7]
recommend that issues should be locked when the conversation
is not constructive or violates the project’s code of conduct or
GitHub’s community guidelines, while the bots often lock issues
for reasons other than these.

Recommendations: Don’t use all locked issues assuming that
the project is following the GitHub’s guidelines. Do use the GitHub
events [6] to verify if the issues have been locked by a bot. If so, the
real reason why the issue is locked should be examined (manually).

Pitfall 2: Issues locked as too heated may not contain uncivil
expressions.

According to our analysis of incivility, 32.68% of the issues locked
as too heated are either technical or civil; i.e., they do not contain
any uncivil comment. We hypothesize that maintainers locked
such issues as too heated to prevent the discussion from becoming
heated. As a result, researchers should not assume that uncivil
expressions would necessarily appear in issues locked as too heated.
In fact, we found that the majority of the comments (91.18%) in
those issues are not heated/uncivil at all. Thus, blindly using this
dataset (e.g., directly feeding it to a machine learning model to
detect inappropriate behavior) might lead to unreliable results.

Recommendations: Don’t blindly use the issues locked as too
heated assuming that they all include uncivil or too heated dis-
cussions. Do manually inspect the data to identify heated/uncivil
comments and construct manually annotated datasets for auto-
mated techniques. To reduce this effort, benchmarks could be built
and reused.

Pitfall 3: The justification given by maintainers may not
match the label they used to lock the issues or reflect the true
locking reason.

For all the issues locked with a too heated label, maintainers only
explicitly justified 25.37% of them with a comment on inappropri-
ate/unhealthy interaction. For the other 74.63%, the justifications
given were, among others, off-topic, issue will not be addressed, or
issue/PR status. Furthermore, among the 70 issues to which the
maintainers did not explicitly provide a justification for locking, we
found that the majority (60%) in fact include an uncivil comment.
There are different explanations of why maintainers provide jus-
tifications other than the reason they used to lock the issue. They
might have locked the issue before it gets too heated as a preventive
measure, provided a nebulous justification to avoid confrontation,
or simply chosen the wrong reason for locking. Researchers should
consider these factors when using the labels or the explicitly given
justifications.

Recommendations: Don’t blindly accept either the justifica-
tion label and/or text as the true oracle for why an issue is locked.

Pitfall 1: Bots automatically lock issues.

6https://github.com/dessant/lock-threads

How heated is it? Understanding GitHub locked issues

Do scrutinize factors such as the discussion topic, the context of
the discussion, and the presence/absence of unhealthy interaction.

For practitioners. We suggest the following recommenda-
5.3.2
tions for practitioners and designers of issue tracking systems
(ITSs).

Recommendation 1: Projects should have clear and explicit
guidelines for maintainers to lock issues according to each
locking reason.

We identified three types of projects that locked issues, i.e.,
projects that locked (i) more than 90% of their closed issues, (ii)
less than 10% of their closed issues, and (iii) between 54% and 88%
of their closed issues. Hence, having explicit guidelines would not
only guarantee consistency amongst the maintainers of a given OSS
project but would also ensure transparency to the entire community.

Recommendation 2: Projects should not abuse the locking
issue feature (e.g., locking instead of closing issues).

According to the GitHub guidelines [5], conversations should
only be locked when they are not constructive. However, we found
that 17.72% of projects locked more than 90% of their closed issues.
This could have an adverse effect on the project since OSS contrib-
utors might assume at first sight that the community is uncivil.

Recommendation 3: ITSs should provide features that (i)
allow projects to add custom locking reasons, (ii) allow main-
tainers to select more than one locking reason (e.g., spam and
too heated), and (iii) encourage maintainers to add a justifica-
tion of why the issue is being locked.

We found that maintainers give different justifications when
locking issues and that such justifications do not match the label
on the GitHub platform in 74.63% of the issues locked as too heated.
Furthermore, maintainers did not mention a justification for locking
70 issues as too heated, out of which we could not observe signs of
incivility in 28 issues. Finally, some too heated issues are also spam,
such as issues related to the Santagate event. New features in ITSs
are necessary to mitigate the aforementioned problems.

6 THREATS TO VALIDITY
Construct validity. We used incivility, particularly Ferreira et al.’s
framework of TBDFs [10], as a proxy to identify and measure heated
discussions and expressions. While this is the most appropriate
framework we found adaptable to our context, incivility defined in
this framework may not completely overlap with the concept of
heated discussions.

Internal validity. First, our qualitative coding can lead to in-
consistencies due to its subjectiveness. To minimize this threat,
our codebooks were interactively improved with all three authors.
Additionally, we validated our codings with a second rater, reach-
ing an almost perfect agreement for the coding of justifications
given by maintainers and a substantial agreement for the TBDFs.
Second, we only coded for comments that are visible on the GitHub
platform and were not able to analyze hidden or deleted comments.

So it is possible that issues coded as technical or civil actually in-
cluded hidden or deleted comments that were uncivil. Third, we
only qualitatively analyzed issues locked as too heated, but since
adding a reason to lock issues is optional on the GitHub platform,
we might have missed heated issues that were not explicitly labeled
by a maintainer with a reason or that were labeled with another
reason.

External validity. Although the projects were carefully selected
and filtered based on a set of criteria, our results are based on a sam-
ple of 79 projects that have at least one issue locked as too heated
in the analyzed period. Hence, our results cannot be generalized to
projects that do not have any issues locked as too heated. To mini-
mize this threat, we compared locked and non-locked issues in our
quantitative analyses and we qualitatively assessed all comments
in all issues locked as too heated. Furthermore, the selected projects
might not be the ones with the largest number of locked issues.
Although this is a threat to the study validity, we could still find
interesting insights in the analyzed sample.

7 CONCLUSION
In this paper, we focus on an empirical study aimed at understand-
ing the characteristics of the locked issues on GitHub, particularly
those locked as too heated. In our sample of 79 projects, we found
that projects have different behaviors when it comes to locking
issues, and that locked issues tend to have a similar number of
comments, participants, and emoji reactions to non-locked issues.
Through an analysis of the 205 issues locked as too heated in our
dataset, we found that the locking justifications provided by the
maintainers in the comments do not always match the label used to
lock the issue. The topics covered by those issues are also diverse.
Leveraging a framework capturing uncivil behavior in software
engineering discussions, we identified that about one-third of the
issues locked as too heated do not contain any uncivil comments.
Our analysis also revealed patterns in how the civil and uncivil
discussion features are distributed across the explicit justifications
and the discussion topics. Together, our results provide a detailed
overview of how GitHub’s locking conversations feature is used
in practice, and we suggest concrete pitfalls and recommendations
for software engineering researchers and practitioners using this
information.

ACKNOWLEDGEMENTS
The authors thank the Natural Sciences and Engineering Research
Council of Canada for funding this research through the Discovery
Grants Program [RGPIN-2018-04470].

REFERENCES
[1] Deeksha Arya, Wenting Wang, Jin LC Guo, and Jinghui Cheng. 2019. Analysis
and detection of information types of open source software issue discussions.
In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
IEEE, 454–464.

[2] Jinghui Cheng and Jin LC Guo. 2019. Activity-based analysis of open source
software contributors: Roles and dynamics. In 2019 IEEE/ACM 12th International
Workshop on Cooperative and Human Aspects of Software Engineering (CHASE).
IEEE, 11–18.

[3] Jithin Cheriyan, Bastin Tony Roy Savarimuthu, and Stephen Cranefield. 2021.
Towards offensive language detection and reduction in four Software Engineering
communities. In Evaluation and Assessment in Software Engineering. 254–259.

Ferreira et al.

[30] Bogdan Vasilescu, Daryl Posnett, Baishakhi Ray, Mark GJ van den Brand, Alexan-
der Serebrenik, Premkumar Devanbu, and Vladimir Filkov. 2015. Gender and
tenure diversity in GitHub teams. In Proceedings of the 33rd annual ACM confer-
ence on human factors in computing systems. 3789–3798.

[31] Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver

agreement: the kappa statistic. Fam med 37, 5 (2005), 360–363.

[32] Maike Vollstedt and Sebastian Rezat. 2019. An introduction to grounded theory
with a special focus on axial coding and the coding paradigm. Compendium for
early career researchers in mathematics education 13 (2019), 81–100.

[33] Elise Whitley and Jonathan Ball. 2002. Statistics review 6: Nonparametric methods.

Critical care 6, 6 (2002), 1–5.

[4] Marcia W DiStaso and Denise Sevick Bortree. 2012. Multi-method analysis of
transparency in social media practices: Survey, interviews and content analysis.
Public Relations Review 38, 3 (2012), 511–514.

[5] GitHub Docs. [n. d.]. GitHub Community Guidelines. https://docs.github.com/
en/github/site-policy/github-community-guidelines Accessed on Jan. 02, 2022.
[6] GitHub Docs. [n. d.]. GitHub event types. https://docs.github.com/en/developers/
webhooks-and-events/events/github-event-types Accessed on Jan. 02, 2022.
[7] GitHub Docs. [n. d.]. Moderation - Locking conversations. https://docs.github.
com/en/communities/moderating-comments-and-conversations/locking-
conversations Accessed on Jan. 02, 2022.

[8] Felipe Ebert, Fernando Castor, Nicole Novielli, and Alexander Serebrenik. 2019.
Confusion in code reviews: Reasons, impacts, and coping strategies. In 2019 IEEE
26th international conference on software analysis, evolution and reengineering
(SANER). IEEE, 49–60.

[9] Carolyn D. Egelman, Emerson Murphy-Hill, Elizabeth Kammer, Margaret Morrow
Hodges, Collin Green, Ciera Jaspan, and James Lin. 2020. Predicting Developers’
Negative Feelings about Code Review. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering (Seoul, South Korea) (ICSE ’20).
Association for Computing Machinery, New York, NY, USA, 174–185. https:
//doi.org/10.1145/3377811.3380414

[10] Isabella Ferreira, Jinghui Cheng, and Bram Adams. 2021. The" Shut the f** k up"
Phenomenon: Characterizing Incivility in Open Source Code Review Discussions.
Proceedings of the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1–35.
[11] Armstrong Foundjem. 2019. Release synchronization in software ecosystems. In
2019 IEEE/ACM 41st International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion). IEEE, 135–137.

[12] Petra Heck and Andy Zaidman. 2013. An analysis of requirements evolution in
open source projects: Recommendations for issue trackers. In Proceedings of the
2013 International workshop on principles of software evolution. 43–52.

[13] Zach Holman. 2014. Locking conversations. https://github.blog/2014-06-09-

locking-conversations/

[14] Mohammad Rafiqul Islam. 2018. Sample size and its role in Central Limit Theorem
(CLT). Computational and Applied Mathematics Journal 4, 1 (2018), 1–7.
[15] Renee Li, Pavitthra Pandurangan, Hana Frluckaj, and Laura Dabbish. 2021. Code
of Conduct Conversations in Open Source Software Projects on Github. Proceed-
ings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–31.
[16] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and
inter-rater reliability in qualitative research: Norms and guidelines for CSCW
and HCI practice. Proceedings of the ACM on Human-Computer Interaction 3,
CSCW (2019), 1–23.

[17] Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu, and Christian
Kästner. 2022. Did You Miss My Comment or What? Understanding Toxicity in
Open Source Discussions. (5 2022).

[18] Naveen Raman, Minxuan Cao, Yulia Tsvetkov, Christian Kästner, and Bogdan
Vasilescu. 2020. Stress and burnout in open source: Toward finding, understand-
ing, and mitigating unhealthy interactions. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering: New Ideas and Emerging Results.
57–60.

[19] David Ramel. 2019. Santa Hat Icon in vs code creates ’Santagate,’ locks Down
Repository. https://visualstudiomagazine.com/articles/2019/12/20/santagate.
aspx

[20] Marnie E Rice and Grant T Harris. 2005. Comparing effect sizes in follow-up
studies: ROC Area, Cohen’s d, and r. Law and human behavior 29, 5 (2005),
615–620.

[21] Johnny Saldaña. 2015. The coding manual for qualitative researchers (3rd ed.).

Sage.

[22] Joni Salminen, Sercan Sengün, Juan Corporan, Soon-gyo Jung, and Bernard J
Jansen. 2020. Topic-driven toxicity: Exploring the relationship between online
toxicity and news topics. PloS one 15, 2 (2020), e0228723.

[23] Jaydeb Sarker, Asif Kamal Turzo, and Amiangshu Bosu. 2020. A Benchmark Study
of the Contemporary Toxicity Detectors on Software Engineering Interactions. In
2020 27th Asia-Pacific Software Engineering Conference (APSEC). IEEE, 218–227.
[24] Teyon Son, Tao Xiao, Dong Wang, Raula Gaikovina Kula, Takashi Ishio, and
Kenichi Matsumoto. 2021. More Than React: Investigating The Role of EmojiRe-
action in GitHub Pull Requests. arXiv preprint arXiv:2108.08094 (2021).

[25] Anselm Strauss and Juliet Corbin. 1990. Open coding. Basics of qualitative
research: Grounded theory procedures and techniques 2, 1990 (1990), 101–121.
[26] Anselm L Strauss. 1987. Qualitative analysis for social scientists. Cambridge

university press.

[27] David R Thomas. 2003. A general inductive approach for qualitative data analysis.

(2003).

[28] Parastou Tourani, Bram Adams, and Alexander Serebrenik. 2017. Code of conduct
in open source projects. In 2017 IEEE 24th international conference on software
analysis, evolution and reengineering (SANER). IEEE, 24–33.

[29] Bogdan Vasilescu, Vladimir Filkov, and Alexander Serebrenik. 2015. Perceptions
of diversity on git hub: A user survey. In 2015 IEEE/ACM 8th International Work-
shop on Cooperative and Human Aspects of Software Engineering. IEEE, 50–56.

