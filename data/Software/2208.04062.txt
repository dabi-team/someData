2
2
0
2

g
u
A
8

]
E
S
.
s
c
[

1
v
2
6
0
4
0
.
8
0
2
2
:
v
i
X
r
a

Testing of Machine Learning Models with Limited Samples: An
Industrial Vacuum Pumping Application

Ayan Chatterjee
Department of Mathematics and Computer Science,
Karlstad University, Universitetsgatan 2, Karlstad, 651 88,
Sweden
ayan.chatterjee@kau.se

Bestoun S. Ahmed
Department of Mathematics and Computer Science,
Karlstad University, Universitetsgatan 2, Karlstad, 651 88,
Sweden
bestoun@kau.se

Erik Hallin
Uddeholms AB, Uvedsvägen, Hagfors, 683 33, Värmlands
län, Sweden
erik.hallin@uddeholm.com

Anton Engman
Uddeholms AB, Uvedsvägen, Hagfors, 683 33, Värmlands
län, Sweden
anton.engman@uddeholm.com

ABSTRACT
There is often a scarcity of training data for machine learning (ML)
classification and regression models in industrial production, espe-
cially for time-consuming or sparsely run manufacturing processes.
Traditionally, a majority of the limited ground-truth data is used
for training, while a handful of samples are left for testing. In that
case, the number of test samples is inadequate to properly evaluate
the robustness of the ML models under test (i.e., the system under
test) for classification and regression. Furthermore, the output of
these ML models may be inaccurate or even fail if the input data
differ from the expected. This is the case for ML models used in the
Electroslag Remelting (ESR) process in the refined steel industry
to predict the pressure in a vacuum chamber. A vacuum pumping
event that occurs once a workday generates a few hundred sam-
ples in a year of pumping for training and testing. In the absence
of adequate training and test samples, this paper first presents a
method to generate a fresh set of augmented samples based on
vacuum pumping principles. Based on the generated augmented
samples, three test scenarios and one test oracle are presented to
assess the robustness of an ML model used for production on an
industrial scale. Experiments are conducted with real industrial
production data obtained from Uddeholms AB steel company. The
evaluations indicate that Ensemble and Neural Network are the
most robust when trained on augmented data using the proposed
testing strategy. The evaluation also demonstrates the proposed
method’s effectiveness in checking and improving ML algorithms’
robustness in such situations. The work improves software test-
ing’s state-of-the-art robustness testing in similar settings. Finally,
the paper presents an MLOps implementation of the proposed ap-
proach for real-time ML model prediction and action on the edge

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE 2022, 14 - 18 November, 2022, Singapore
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $xx.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

node and automated continuous delivery of ML software from the
cloud.

KEYWORDS
data augmentation, software testing, vacuum pumping, data de-
composition, machine learning, mlops

ACM Reference Format:
Ayan Chatterjee, Bestoun S. Ahmed, Erik Hallin, and Anton Engman. 2022.
Testing of Machine Learning Models with Limited Samples: An Industrial
Vacuum Pumping Application. In Proceedings of The 30th ACM Joint Euro-
pean Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE 2022). ACM, City, State, Country, 11 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Machine learning (ML) software has grown in popularity in recent
years due to the availability of massive amounts of training data
and application size. When training an ML software, in general,
the conventional approach is to divide the data into training and
testing for regression or classification. A large portion of the ground
truth is often used for training, leaving a fraction of the samples
for test, usually 25% or less. A problem arises when insufficient
data is available, which takes practically all the ground truth for
training and leaves a few samples for testing using the conven-
tional approach. For example, when an event is performed ‘N’ (N <
10) times a day during the workday, such as a manufacturing/pro-
duction process, it adds up to roughly 300N different events per
year. Delivering stable decisions over time is a critical issue in the
industrial application. Therefore, testing the robustness of such a
software product is an essential quality assurance check. Robust-
ness is a non-functional quality attribute of ML software. Testing
the robustness of ML software under limited data conditions is un-
derexplored in the literature. The question remains whether these
limited data are sufficient to render a robust ML model using the
conventional training-test split strategy.

An instance where there are insufficient data available is in the
refined steel industry, specifically vacuum pumping in the Elec-
troslag Remelting (ESR) process [2, 15, 23]. The process includes
furnace cleaning [18], which requires vacuum pumping or extract-
ing oxygen from the furnace. As part of the ESR process, when a
vacuum pumping event occurs, the sensors attached to the chamber

 
 
 
 
 
 
ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Chatterjee A, et al.

collect data on the total pressure inside the chamber. The pump-
ing process is terminated when the desired pressure threshold is
reached. Predicting the minimum pressure of a vacuum chamber
early in the pumping process can potentially identify issues such
as pump failures and leaks. However, a pumping event occurs once
a day on average and generates a few hundred events in a year to
split between training and test. In this case, for an ML model, there
are only a few hundred occurrences to divide between training
and test data. It becomes difficult to obtain the appropriate test
data to determine whether the ML model for pressure prediction is
overfitting [20].

Theoretically, to avoid overfitting, one of the following steps is
taken [4, 8, 21]: (i) removing features such as random forest, (ii)
increasing the amount of data available for training, (iii) performing
cross-validation, or with (iv) regularisation, which includes cali-
brating the hyperparameters for the given data. Data augmentation
[11, 27] is one such technique for generating slightly altered copies
of existing data or creating new synthetic data and boosting the
number of training samples available. We use data augmentation in
a new industrial application in the ESR process. The data augmenta-
tion technique and the testing approach discussed in this paper are
based on the general principles of vacuum engineering. Vacuum
pumping is used across multiple domains, including fusion power
plants [9] and EUV lithography for global silicon manufacturing
[29]. This paper focused on ESR due to the availability of pumping
data from the Uddeholms AB steel company. However, in principle,
this augmentation and testing technique can be applied in multiple
disciplines where vacuum pumping is part of the manufacturing
process.

This study offers a fresh perspective on data augmentation for
vacuum pumping from the software testing point of view. The
approach presented in this paper first decomposes the data using
domain knowledge and then generates random and fresh samples
utilizing the concepts of vacuum pumping. The newly generated
augmented data are then used to assess ML models’ robustness on
an industrial scale. Here, careful test scenarios were designed to
determine the end-to-end functionality of the ML systems. Each
scenario is combined with a test oracle to determine the pass and
fail cases during robustness testing. The oracle has been designed
from a combination of theory and domain-knowledge background
in industrial production. Therefore, ML models are subjected to
robustness testing, an important non-functional testing property
of ML software [30]. Here, robustness testing is used to ensure
that they work as expected with known data (ground truth or GT),
remain within physical constraints, and, in theory, are resilient to
future unknown data. To assess the robustness in the presence of
limited data, this paper makes the following contributions:

• A data augmentation approach for ESR vacuum pumping is
developed that uses domain knowledge to generate synthetic
data.

• A strategy is developed to evaluate the robustness of an ML
model based on real and augmented data to compensate for
insufficient ground truth test data.

• The paper used real data from steel production and ML mod-

els on an industrial scale.

• An MLOps implementation of the proposed approach.

The remainder of the paper is structured as follows. Section 2
gives an overview of ML training with small samples and vacuum
pumping theory to describe the principles underlying the data
decomposition and pumping speed dictionary mentioned in the
proposed work. The subsequent section 3 describes (i) an efficient
data augmentation approach for vacuum pumping and (ii) tests to
evaluate the robustness of a trained ML model. Section 4 shows
how the proposed approach is used in industrial production, where
training, testing, and real-time prediction and actions of the ML
software are carried out automatically, followed by Section 5 which
contains experiential results. Section 6 concludes the paper and
provides future directions for continuing this study.

2 BACKGROUND AND RELATED WORK
2.1 Small sample ML and data augmentation
Small samples have low statistical power, making it difficult for the
ML software to train robustly [26]. It is difficult to accurately pre-
dict performance from a small set of measured variants, especially
if those features interact. An approach to statistical learning perfor-
mance prediction that considers variability is presented in the paper
by Guo et al. [10]. The method uses a series of random samples
to detect feature interactions step by step without additional ef-
fort. The method investigates and exploits the correlation between
feature selections and performance to predict performance in a con-
figurable software system. This correlation can be easily revealed by
measuring the performance of all software system configurations
and then providing direct answers (e.g., which configuration is the
fastest). In addition, small sample ML requires a robust validation
method. The simulations in Vabalas et al.’s paper [25] show that
the k-fold cross-validation produces strongly biased performance
estimates with small sample sizes, and the bias is still apparent with
sample sizes of 1000.

Increasing the training sample size in a principled manner is a
constant demand for ML software. Increasing training data with
data augmentation improved accuracy in various areas. Two key
categories are defined in the survey paper by Wen et al. [27] on time-
series data augmentation: basic and advanced. Basic approaches
include the time domain, the frequency domain, and the time-
frequency domain, which include augmentation approaches such
as noise injection, flipping, and jittering. Advanced methods in-
clude decomposition, statistical generative models, and learning.
The survey’s primary focus is on class imbalance and data augmen-
tation for members of the minority class. Among other methods,
Generative adversarial networks (GANs) have been shown to be
able to produce artificial data that are similar to the real data. When
faced with the problem of unbalanced data classification, GANs
can provide an alternative solution. By generating artificial data
that are similar to the original data, and thus augmenting the train-
ing dataset, GANs can be used for data augmentation. GANs, for
example, in the papers by Shao et al.[22], Ortego et al. [19], and
more recently in Jain et al. [12] are intended to produce realistic
synthesized signals with labels for further use in machine fault di-
agnosis. A limitation of such methods is that all augmented samples
produced may not be physically plausible and may show unrealistic
artifacts. A recent study by Lee and Lee [17] presents a unique deep

Testing of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

learning methodology for evaluating the interior noise in automo-
biles. The method uses domain knowledge for data augmentation
and for model development, has seen substantial increase in ML
software accuracy. Thus, unlike other approaches„ this paper ex-
pands on data augmentation in ESR and proposes a new method
that combines domain knowledge and ML to test the robustness of
such software products.

2.2 ESR Vacuum Pumping
The ESR process involves remelting the consumable electrode as
the steel is refined and solidified [14]. Figure 1 depicts the schematic
diagram of the ESR, where the region above the slag requires near-
vacuum conditions to avoid contaminants during the process. As
a result, the vacuum pumps attached to the ESR pump out excess
oxygen or air from the chamber.

Figure 1: Schematic sketch of ESR.

Theoretically, vacuum pumps in the ESR process reduce the
number of gas molecules in a vacuum system by creating a volume
flow determined by the rate at which the pressure in a vacuum
system of constant volume decreases over time [13]. When pumping
occurs, the overall pressure inside a chamber is directly proportional
to the amount of gas extracted by the pump, and therefore there is
less leakage and flow through the inner surfaces of the chamber.
The amount of gas pumped out and the amount of gas that leaks in
are constantly changing over time. Dmitrieva et al. [6] describe the
total pressure inside a vacuum system at a time ‘T’ as ‘𝑃𝑇 ’ with an
initial pressure of ‘𝑃0’, the volume of the vacuum pump chamber
‘𝑉𝑐 ’ the average pumping speed ‘𝑆’, the leakage flow ‘𝑄𝑙 ’ and the
flow from the inner surfaces ‘𝑄𝑖 ’, which is:

𝑃𝑇 =

𝑄𝑙 + 𝑄𝑖
𝑆

+ (𝑃0 −

𝑄𝑙 + 𝑄𝑖
𝑆

)𝑒− 𝑇 𝑆

𝑉𝑐

(1)

In the above equation 1, 𝑄𝑙 depends on the total conductance
that determines the aggregate bandwidth of all leaks. And, 𝑄𝑖 is
proportional to the surface area of the vacuum chamber, a degassing
coefficient, and a constant that depends on the material and rough-
ness of the vacuum chamber. However, in practice, a vacuum pump
can only lower the pressure to a certain point; and the time taken
is referred to as the pump-down time. An approximate model [5]
with effective pumping speed ‘𝑆𝑇 ’ at pump-down time ‘T’ is given
as:

𝑆𝑇 =

𝑉𝑐
𝑇

𝑙𝑛(

𝑃𝑇
𝑃0

)

(2)

The pumping event is not a common occurrence in real pro-
duction. For example, in a steel company such as Uddeholms AB
in Sweden, for which this study is conducted, the pumping event
occurs once a day on average for approximately twenty minutes
to pump out oxygen from the vacuum chamber. After pumping
for twenty minutes, experts inspect the chamber to ensure that it
meets quality standards before moving on to the next phase, such
as checking internal pressure. If the quality requirements are not
achieved, the chamber is inspected and re-pumped for an addi-
tional twenty minutes. The role of ML in this situation is to observe
the first few seconds or a minute of pumping and to forecast the
minimum pressure. In this case, accurate pressure forecasts save
valuable time and increase production efficiency. However, with
the limited occurrence of the event, a few hundred samples can be
generated in a year of pumping for training and testing that will
affect the robustness of the implemented ML model.

3 THE PROPOSED APPROACH
This paper presents two concepts for the ESR vacuum pumping
application with limited training data: (i) data decomposition and
augmentation to generate new vacuum pumping data, and (ii) tests
for evaluating the robustness of a trained model using the generated
augmented data and new test scenarios.

3.1 Augmented Sample Generation
A three-step strategy is designed to generate augmented data and
address the scarcity of training data for vacuum pumping. The
first step is to decompose the ground truth and interpret the initial
pressure and pump-down time. Second, a dictionary of pumping
speeds is constructed that is independent of the initial pressure.
The final step generates sparse representations for the dictionary
and produces a fresh set of augmented samples. The following
subsections address these steps.

3.1.1 Decomposing the ground truth data. Each pumping event is a
separate and distinct occurrence. For a given vacuum chamber (i.e.,
a constant volume), equation 2 mandates that one must understand
‘𝑆𝑇 ’, ‘𝑇 ’, and the ‘𝑃0’ in order to interpret ‘𝑃𝑇 ’. At minimum pres-
sure, all three mentioned variables are scalars. However, a vector
is formed for 𝑆𝑇 when we look at the entire pumping period (t =
0 to T). We take the approach to decompose the ground-truth by
interpreting the 𝑃0 and 𝑇 distributions and extracting the 𝑆𝑇 into a
dictionary or a matrix containing only the unique 𝑆𝑇 vectors. As a
result, we use the central limit theorem and the law of large num-
bers [24] to approximate the initial pressure and pump-down time
as a Gaussian distribution. Although future pumping occurrences
are unknown, this assumption will, in principle, assist a regression/-
pressure prediction model in learning about the different pumping
events in a proactive manner. The mean and standard deviation of
the distributions for 𝑃0 and 𝑇 are computed using the maximum
likelihood estimation from ground-truth data.

3.1.2 Extracting a dictionary of pumping speeds. After obtaining
the 𝑃0 and 𝑇 distributions, the next step is to obtain the pumping

IngotLiquid PoolSlagVVacuumElectrodeESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Chatterjee A, et al.

speeds 𝑆 per unit volume at regular intervals. Because the amount
of gas coming in and going out changes over time, we extracted
all 𝑆 from Equation 2. The collected 𝑆 was then reduced to the
independent pumping speeds to a dictionary 𝐷𝑆 ∈ R𝑋 ×𝑌 (𝐷𝑆 ⊂
𝑆) assuming a simplex and linear mixing. Mathematically, for an
acceptable residual error 𝜖 and a representation 𝜓 , it is given as:

min
𝐷𝑆 ∈R

||𝐷𝑆 ||𝑟𝑜𝑤,0 s.t. ||𝑆 − 𝐷𝑆𝜓 ||2 ≤ 𝜖

(3)

3.1.3 Generating augmented samples. Once the parameters 𝑃0 and
𝑇 from section 3.1.1, and a dictionary 𝐷𝑆 mentioned in section 3.1.2
for the vacuum chamber and pump setup are determined, the next
step is to generate as many augmented samples M (𝑀 ≫ 𝑁 ) as are
required. For each of the 𝑀 samples, a sparse vector 𝜓 (𝜓 ∈ R𝑌 ×1)
is generated at random, resulting in a unique pump speed that is
a fractional combination of historical events. The mathematical
conditions of 𝜓 are:

∀𝜓𝑦 ⊂ 𝜓 , 𝜓𝑦 ≥ 0 and ||𝜓 ||1 = 1
(4)
After that, a random initial pump speed 𝑃0 and pump-down time
𝑇 are generated for their respective distributions, and a convolution
operation with equation 2 creates a unique augmented sample.
This augmented sample differs from the ground truth data while
remaining within the constraints of vacuum pumping’s physical
capabilities. The procedure is repeated M times to yield M unique
samples. A diagram of the proposed data augmentation is depicted
in Figure 2.

Figure 2: A diagram showing the proposed data augmenta-
tion technique for ESR vacuum pumping.

3.2 Tests to Evaluate Robustness of a Trained

ML Model

Once an ML model is trained using augmented data, it must be
tested to ensure that it is robust. We propose three test scenarios
to evaluate the robustness of the model. Each of the three test
scenarios addresses a unique concern, which are as follows:

• Feasibility test scenario: is to see if the trained model re-

mains within the physical limits of vacuum pumping.

• Ground truth testing: examines the trained model’s accu-
racy and its performance when applied to known data.
• Volume enclosed testing: under a given residual error ex-
amines how well the trained model should operate under
the constraints of vacuum pumping (e.g., in case of a drift in
the data or changes in the pumping conditions).

Depending on the application’s sensitivity, the results from the
scenarios are collected into a test oracle that determines if an applied
robustness test of the machine learning model is passed or failed.

Feasibility Test Scenario. The feasibility test scenario ensures
3.2.1
that a trained model stays within the physical limits of vacuum
pumping. This scenario warrants that no direction from initial
pressure and pump-down time is impractical, i.e., the predicted 𝑃𝑇
is always positive when 𝑃0 and 𝑇 are positive. Mathematically, it is:

∀𝑃0 > 0 and 𝑇 > 0, 𝑃𝑇 > 0

(5)

3.2.2 Testing the ground truth. After the feasibility test, we have a
scenario that uses the ground-truth and a scenario that uses aug-
mented data. The second test scenario evaluates the residual error
of the data from the ground-truth test data. The mean absolute
error (MAE, equation 6), also known as the Manhattan distance or
the ℓ1 norm of the residual error divided by the number of observa-
tions, and the goodness-of-fit measure: R-squared (𝑅2, Equation 7)
value. An additional ℓ∞ with the real and augmented data shows
the maximum expected prediction error for the given data.

𝑀𝐴𝐸 (𝑎, 𝑏) =

(cid:205)𝑛

𝑖=1 |𝑎𝑖 − 𝑏𝑖 |
𝑛

𝑅2 (𝑎, 𝑏) = 1 −

(cid:205)𝑛
𝑖 (𝑎𝑖 − 𝑏𝑖 )2
(cid:205)𝑖 (𝑎𝑖 − ¯𝑎)2

, where ¯𝑎 =

1
𝑛

𝑛
∑︁

𝑖=1

𝑎𝑖

(6)

(7)

3.2.3 Volume enclosed under a given residual error. The third test
scenario involves estimating the volume of occupied space within
a specified residual error threshold. A higher volume indicates a
more robust method for the given data and threshold. A graphical
illustration of the concept is shown in Figure 3.

This test retrieves augmented samples whose residual error is less
than a specified threshold, ‘t’. Afterward, the linearly independent
columns are located. In the end, the determinant method [28] is
used to compute the volume, which is:

|𝑉 (𝑃)| =

√︁

det( ¯𝑃𝑇 ¯𝑃)
(𝑑 − 1)!

(8)

Where, ¯𝑃 = [𝑃1 − 𝑃𝑑, 𝑃2 − 𝑃𝑑, ... , 𝑃𝑑−1 − 𝑃𝑑 ]

ExtractDS(Dic�onary)GroundTruthAugmentedDataAugmentedDataGenerateΨ(SparseRepresenta�on)ExtractP0andTDistributionsNewP0andTTesting of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

For example, for a given threshold of ‘𝑡’ for the MAE, the
pass-fail criteria is:

𝑀𝐴𝐸

(cid:26) ≥ 𝑡
< 𝑡

,
,

Pass
Fail

(10)

Figure 3: A two-dimensional illustration of the area en-
closed by the residual error (under a desired threshold) of
pumping measurements.

3.2.4 Test Oracle. In the final phase, the results/output of the three
test scenarios are fed into three sub-oracles and then to the main
oracle. Figure 4 displays the flow diagram of the sub and main
oracles.

Figure 4: Robustness testing test oracle for the ESR vacuum
pumping

• Oracle 1: The first sub-oracle (or oracle 1) is to identify
whether a vacuum pump pressure prediction ML model pre-
dicts positive values with the augmented samples. If any of
the augmented samples as input returns a negative pressure,
the ML model under test is considered a failure of the test.
Mathematically, it is given by:
(cid:26) 1
0

∑︁ 𝑃𝑇 > 0 =

Pass
Fail

(9)

,
,

• Oracle 2: The second sub-oracle gets its input from sce-
nario 2 (or ground-truth testing). The sub-oracle compares
the predicted values/scores against acceptable thresholds
determined by field experts. Selecting thresholds is about
determining how pure each block of steel should remain.
In principle, a higher minimum pressure means that more
gas molecules remain in the chamber, reacting with the steel
during subsequent production processes after vacuum pump-
ing. The output of this sub-oracle determines whether a ML
model meets the threshold requirements of the application.

• Oracle 3: The third sub-oracle inputs two volume measure-
ments from the volume enclosed testing scenario: the volume
of the augmented sample space in its entirety (𝑉𝑡𝑜𝑡 ) and the
volume of the augmented data space (𝑉𝑡 ), which is under
the field expert determined threshold. The ratio of the two
volumes is calculated and the pass-fail criteria is given by a
threshold ‘𝑡𝑣’ as:
𝑉𝑡
𝑉𝑡𝑜𝑡

(cid:26) ≥ 𝑡𝑣
< 𝑡𝑣

Pass
Fail

(11)

,
,

Once an ML model under test passes the sub-oracle, the 𝑉𝑡
value is passed on to the main oracle.

• Main Oracle: The main oracle takes input from all three
sub-oracles and determines the robustness of the ML model.
A robust model must stay within the physical boundary (i.e.,
passed the oracle 1), meet the business requirements (i.e.,
passed the oracle 2), and be able to adapt to slight changes
in the ground truth data (i.e., passed oracle 3). However,
suppose that an ML model under test passes oracles 1 and 2
only. In that case, it suggests that the ML model fairs well
with the ground truth data, but any deviation or drift may
produce unreliable predictions or be less robust. For this
reason, the main oracle considers an ML model under test a
robust model when it has passed all three sub-oracles. Finally,
among several ML models under test that passed the main
oracle; the most robust ML model is the one with the largest
volume, demonstrating resilience towards changes in the
ground truth while delivering the required accuracy in the
current data.

4 MLOPS IMPLEMENTATION
Continuous and automated ML software development, also known
as MLOps, is becoming increasingly popular as a result of the
success of continuous software development, DevOps in particular.
This paper describes an early-stage implementation to continuously
deliver ML software in the ESR for the proposed data augmentation
and testing approach. The architecture outlined in figure 5 shows
the proposed implementation with services and microservices split
between cloud and edge node. Services like ML software training
and retraining and the persistent storing of pumping data are exe-
cuted in the cloud. Microservices at the edge operate in real-time,
making decisions and taking action automatically.

4.1 Real-time prediction and action
Automated and real-time decisions in production are made possible
by containerized microservices on the Edge node. A great tool for
creating containerized applications is Docker1 [1]. During vacuum
pumping, the ‘delivery and collection’ microservice receives real-
time data from pressure sensors attached to the ESR. The real-time
streaming data pipelines and applications built with Apache Kafka2

1https://www.docker.com/
2https://kafka.apache.org/

theoretical pumping possibilities (augmented data)observed ground truthpumping data where prediction is under acceptable thresholdScenario 1 (Feasibility Test)Scenario 2 (Ground TruthTesting)Scenario 3 (Volume EnclosedTesting)Oracle 1Oracle 2Oracle 3Main OracleESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Chatterjee A, et al.

Figure 5: Data pipeline for MLOps implementation of the proposed approach in industrial setting enabled by a combination
of (i) sensor, actuator, and Edge microservices (light blue) for real-time analysis/prediction and action, and (ii) Cloud services
(light green) for offline ML software training and testing.

is an example of ‘delivery and collection’. After ‘T’ seconds after
pumping, delivery and collection microservice transmits the col-
lected data to the ‘vacuum pump pressure prediction’ microservice,
where the ML model trained by our proposed approach is run. The
ML model predicts whether or not the desired minimum pressure
will be reached and sends the predicted minimum value to the next
microservice – ‘action’. The ‘action’ microservice signals the actua-
tor to stop pumping when the predicted minimum does not meet
the expected requirements in a fully autonomous setup. Alterna-
tively, or in addition, the ‘action’ microservice sends a signal to
display the predicted minimum for a human field operator to take
appropriate steps. The ‘action’ microservice is customizable based
on the business needs.

4.2 Continuous and automated ML software

training and deployment

In order to provide continuous delivery of the ML vacuum pump
prediction software, data flows from the sensors attached to ESR to
the "delivery and collection" microservice on the edge. The pumping
data is subsequently forwarded to the cloud for long-term ‘persis-
tent’ storage. The API layer in the cloud offers an interface between
microservices on the edge, cloud services, and persistent storage.
The ML software with the proposed approach is then trained with
the historical pumping data, and when an ML model passes our
proposed main oracle, a new version of the ML prediction software
container is automatically deployed to the Edge. At the current
stage, the ML model is trained periodically after a few months;

however, future work in the architecture will involve the research
on additional microservices to detect and classify model degrada-
tion and trigger a re-training automatically.

5 EXPERIMENTAL EVALUATION AND

RESULTS

5.1 Dataset and ML Methods
The experiments carried out in this paper are using proprietary
data from Uddeholms AB on two furnaces of equal volume and
manufacturing specifications, termed ‘furnace M’ and ‘furnace S.’
From January to October 2021, Furnace M has 203 events (subfigure
(a) in figure 6), and Furnace S has 107 events (subfigure (c) in figure
6). For this experiment, the goal of the pressure prediction model
is to predict the minimum pressure (in mbar) reached after twenty
minutes of pumping based on the data collected during the first
minute of pumping (subfigure (b) in Figure 6).

Three machine learning approaches are evaluated in this paper
using the default settings in MATLAB’s: (i) Optimizable Ensemble3
(Opt Ensemble), (ii) a single-layered Neural Network (NN)4, and (iii)
Optimizable Gaussian Process Regression5 (or Opt GPR). Bayesian
optimization6 was used to optimize the models’ hyperparameters
during run-time. Similar optimizers exist in other platforms, such

3https://www.mathworks.com/help/stats/fitensemble.html
4https://www.mathworks.com/help/stats/fitrnet.html
5https://www.mathworks.com/help/stats/fitrgp.html
6https://www.mathworks.com/help/stats/hyperparameter-optimization-in-
regression-learner-app.html

Microservices on Edge NodeDelivery andCollection ServicesVacuum PumpPressure PredictionAction ServicePersistent StorageAPI LayerVacuum Pump Pressure Prediction ML Model Training ServiceAutomatedDeploymentActuator / DisplayOutputESR Vacuum PumpPressure SensorData AugmentationML Model TrainingML Model TestingCloud or OfflineReal-time or OnlineTesting of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

(a) ESR M

(b) First minute of (a) taken as input for prediction

(c) ESR S

Figure 6: Two ESR pumps are depicted in this figure: pump M in (a) and pump S in (c). Subfigure (b) displays pumping data
from the first minute that is utilized as input to a ML model for predicting the minimum pressure.

as Hyperopt in Python [16] and the paper by Bergstra et al. [3],
which use random search and two new greedy sequential methods
based on the expected improvement criterion to optimize hyper-
parameters. The hyperparameters are mentioned in table 1.

Table 1: Table showing the optimized Ensemble and GPR hy-
perparameters and the default hyperparameters of the sin-
gle layered NN.

Model

Opt Ensemble

NN

Opt GPR

Hyperparameters
Method: Bootstrap aggregation (bagging)
Number of learners: 496
Minimum leaf size: 5
Number of predictors to sample: 59
Layer size: 10
Activation: ReLU
Standardize data: Yes
Sigma: 7.2e-02
Basis: Zero (empty matrix)
Kernel function: Nonisotropic Exponential
Standardize data: Yes

pumping speeds (𝑆𝑇 ) are then calculated using equation 2. After
that, the GT data is converted from temporal domain to a time-
independent sparse domain. In other words, all individual pumping
data with their own length from t = 0 to the pump-down time (T) are
interpolated to the same length using cubic spline interpolation7.
With an average pump-down time of 333s, we selected a dictionary
resolution or length of 500 (≈1.5x) to capture minute changes in
pump speeds. The collection of all pumping speeds is then subset to
the independent pumping speeds with a greedy dictionary learning
approach presented in algorithm 1.

Algorithm 1 Pseudocode for the extraction of the pumping speed
dictionary

1: Input: S
2: while stopping criteria for 𝜖 do
3:

𝜓 ← 𝑓 (𝐷𝑠, 𝑆)
𝜖 ← 𝑆 − 𝐷𝑠 × 𝜓
𝑆𝑚 ← arg max ||𝜖 ||𝑝
𝐷𝑠 ← 𝐷𝑠 (cid:208) 𝑆𝑚

4:

5:

6:
7: end while
8: Output: 𝐷𝑠

⊲ Representation
⊲ Residual error
⊲ 𝑆𝑚: element with max 𝜖
⊲ Add selected 𝑆𝑚 to 𝐷𝑠

Our proposed data augmentation method and the classical/tra-
ditional approaches under test are both trained on like-for-like
conditions with MATLAB’s Bayesian hyperparameter optimizer
in this paper. Using the conventional 80%-20% training-test split
approach (referred to as ‘classic’ in tables and figures) and 100,000
augmented samples (referred to as ‘aug’ in tables and figures), all
three approaches were trained using furnace M data. Classic meth-
ods use 80% of GT data for training, while aug approaches train
with the augmented samples. After training, the models were tested
on data from furnace S.

5.2 Data decomposition and augmentation
The first stage in data augmentation of the pump M data is using
maximum likelihood to estimate the distributions of initial pres-
sure (𝑃0) and pump-down time (𝑇 ). The distributions of 𝑃0 and 𝑇 ,
respectively, are depicted in subfigures (a) and (b) in figure 7. The

In principle, the representation is an 𝐿0 minimization problem
which is NP-hard[7], is often approximated to 𝐿1 or in this case,
we choose the greedy approach, which is 𝐷𝑇
𝑠 × 𝑆. This approach
iteratively selects the pump speed with the highest residual that
is orthogonal to the previously selected pumping speeds. With a
small residual error of 1e-03 for 𝜖, the dictionary selects 40 different
pumping speeds. The output dictionary is shown in figure 7c.

After decomposing ESR M data, a random sparse representation
and a random initial pressure (𝑃0) and pump-down time (𝑇 ) are
generated for augmented data generation. 𝑃0 and 𝑇 are generated
with Pearson system random numbers8 within the limits of the GT
data. Figure 8 shows three examples of augmented samples.

7https://www.mathworks.com/help/matlab/ref/spline.html
8https://www.mathworks.com/help/stats/pearsrnd.html

020040060080010001200Time(s)020040060080010001200Pressure(mbar)0102030405060Time(s)20030040050060070080090010001100Pressure(mbar)020040060080010001200Time(s)020040060080010001200Pressure(mbar)ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Chatterjee A, et al.

(a) Initial pressure (𝑃0)

(b) Pump-down time (T)

(c) Pumping speed dictionary

Figure 7: Figure showing the distributions of the ESR M’s initial pressure in (a) with a mean and standard deviation 1e+03 ±
16.84, and pump-down time in (b) with a mean and standard deviation of 333.59 ± 262.52. Subfigure (c) shows the extracted
pumping speed dictionary.

Figure 8: Three examples of augmented samples, each with a pump-down time of 218, 869, and 222 seconds, respectively (top
to bottom) and with initial pressures of 1007.9, 1012.3, and 996.8 mbar respectively.

The procedure is repeated for 100k augmented samples in this
experiment. The augmented data of all samples is shown in Figure
9.

5.3 Testing the ML models
Feasibility test. When all six models (three with classic and
5.3.1
three with aug) are run through the feasibility test, both Opt En-
semble and Opt GPR pass the test. In contrast, the classic NN failed
the test, predicting a negative pressure from the augmented data.
Despite the fact that five out of six models predicted only positive
values.

5.3.2 Testing the ground truth. If a model passes the feasibility
test, this scenario calculates the residual error with ground-truth
data. With 60 seconds of input data, the residual error between the
predicted minimum pressure and the actual minimum pressure is
calculated. Table 2 shows mixed MAE outcomes between furnace
M and S individually between classic and aug training. However,
there is an increase in the 𝑅2 score and consistency in the results
between bias and variance when trained with augmented samples.
Figure 10 illustrates the improvements, showing that training with
augmented data surpasses the classical technique, in addition to
spikes and deviations.

9409609801000102010401060Pressure(mbar)0102030405060708090ESRM(groundtruth)Estimateddistribution15045075010501350Time(s)020406080100120140160ESRM(groundtruth)Estimateddistribution00.10.20.30.40.50.60.70.80.91-5-4-3-2-101Pumpingspeedperunitvolume051015202530354000.050.10.150.205010015020025002004006008001000051015202530354000.10.20.3010020030040050060070080090002004006008001000051015202530354000.20.40.60.805010015020025002004006008001000#DictionaryelementTime(s)Pressure(mbar)RepresentationTesting of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Table 2: Residual error of the six models in furnace M, furnace S, and the maximum error observed in the augmented data.
Since aug models were trained using augmented samples, they use 100% of GT data for testing, unlike classic approaches that
use only 20% of GT data for testing.

Model
Opt Ensemble (classic)
Opt Ensemble (aug)
NN (classic)
NN (aug)
Opt GPR (classic)
Opt GPR (aug)

furnace M
𝑅2
0.95
0.93
0.48
0.80
0.58
0.75

ℓ∞
14.76
13.56
2.34
13.21
8.80
14.17

MAE
1.03
1.20
1.72
1.45
1.21
1.23

furnace S
𝑅2
0.94
0.98
0.47
0.91
0.83
0.93

ℓ∞
22.40
21.02
19.69
21.11
25.29
22.12

MAE
1.38
1.00
2.08
1.29
1.28
1.16

aug MAE
ℓ∞
|S-M|
0.35
9.04
0.20
6.37
0.36
124.54
0.16
7.63
0.07
15.39
0.07
8.04

(a) 100k samples of augmented data

(a) Opt Ensemble

(b) GT and augmented data differences

Figure 9: Augmented pumping data from decomposed ESR
M GT.

(b) NN

Figure 10: A figure showing the prediction of Opt Ensemble
and NN models with both classic and aug training.

5.3.3 Volume enclosed under a given residual error. The last test
estimates the volume of the trained model in temporal space whose
residual MAE error is below the acceptable error threshold. For this

20030040050060070080090010001100Pressure(mbar)at10s020040060080010001200Pressure(mbar)at270sAugmenteddataGroundtruthAugmenteddatafillingthegaps0102030405060708090100#PumpingEventMinimumPressure(mbar)GroundTruth(furnaceS)Opt.Ensemble(classic)Opt.Ensemble(aug)0102030405060708090100#PumpingEventMinimumPressure(mbar)GroundTruth(furnaceS)NN(classic)NN(aug)ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Chatterjee A, et al.

experiment, a threshold of 1 mbar determined by Uddeholm field
experts is selected. When the data is collected once per minute, we
have 60 temporal dimensions with the first minute of pumping data
as input.

Table 3 shows that, compared to classic models, the enclosed
capacity for aug models has increased dramatically. However, all
six models cover less than 0.11% of the whole augmented sample
space, necessitating further research into a deep learning model for
vacuum pumping predictions.

Table 3: Volume enclosed by augmented samples for 60 sec-
onds of pumping data whose prediction residual error is be-
low the threshold of 1 mbar. The volume of augmented data
in this space is 1.1331e-17.

Model
Opt Ensemble (classic)
Opt Ensemble (aug)
NN (classic)
NN (aug)
Opt GPR (classic)
Opt GPR (aug)

Volume
3.46e-45
6.39e-31
9.86e-52
1.93e-33
1.25e-35
7.48e-21

Increase %

1.85e+16

1.96e+20

5.98e+16

5.3.4 The Test Oracle. Finally, the output from the feasibility test
and the findings from Tables 2 and 3 are input into the test oracle
to measure the robustness of the six ML models under test. The
thresholds for the pass-fail criteria for each sub-oracle will vary
according to the application’s sensitivity. For this experiment, we
used the following thresholds determined by field experts: MAE of
1.5 mbar, 𝑅2 threshold of 0.8, ℓ∞ threshold of 25 mbar, and volume
below the residual error threshold of 1.0e-35. Two of the six models
pass the three sub-oracles and the main oracle, Opt Ensemble and
NN, were both trained using augmented samples. And, based on the
volume enclosed, Opt Ensemble (aug) covers ≈330x more volume
than NN (aug), making Opt Ensemble trained with augmented
samples the most robust of the six models.

6 CONCLUSION
In this paper, we show that the traditional 80-20 split between
training and testing in software testing is inadequate for machine
learning with small samples. Furthermore, this paper improves on
the state-of-the-art software testing practices in robustness testing
and presents a data augmentation technique that comprises newly
developed test scenarios and oracles to determine a trained model’s
robustness in the absence of sufficient training and test data. The
early experiments suggest that adding augmented data generated
by the proposed approach to ML models’ training improves the
test data prediction accuracy and, more importantly, reduces the
bias and variance difference between different vacuum pumping
production data for ESR applications. Additionally, we identified
that the six ML models under test do not capture events where
the minimum pressure deviates substantially from the training
distribution mean, which will be investigated in future work and
require the development of a bespoke model for vacuum pumping
application. The augmentation and testing approach is designed to

aid future model development and how to design a robust testing
methodology when working with small sample ML.

In the future, further model investigation will need to be car-
ried out, including research into deep learning and the integra-
tion and use of several trained models simultaneously. This paper
presents our best practice to date in addressing quality assurance
challenges in an MLOps workflow. Although the paper focuses
on vacuum pumping in the steel production industry, robustness
testing methodology and general quality assurance workflow can
be applied across multiple disciplines.

ACKNOWLEDGMENTS
This work has been funded by the Knowledge Foundation of Swe-
den (KKS) through the Synergy Project AIDA - A Holistic AI-
driven Networking and Processing Framework for Industrial IoT
(Rek:20200067).

REFERENCES
[1] Muhammad Alam, Joao Rufino, Joaquim Ferreira, Syed Hassan Ahmed, Nadir
Shah, and Yuanfang Chen. 2018. Orchestration of Microservices for IoT Using
Docker and Edge Computing.
IEEE Communications Magazine 56, 9 (2018),
118–123. https://doi.org/10.1109/MCOM.2018.1701233

[2] B ARH et al. 2016. ELECTROSLAG REMELTING: A PROCESS OVERVIEW.

Materiali in tehnologije 50, 6 (2016), 971–979.

[3] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms
for Hyper-Parameter Optimization. In Advances in Neural Information Processing
Systems, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger
(Eds.), Vol. 24. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf

[4] Gavin C. Cawley and Nicola L. C. Talbot. 2007. Preventing Over-Fitting during
Model Selection via Bayesian Regularisation of the Hyper-Parameters. Journal
of Machine Learning Research 8, 31 (2007), 841–861. http://jmlr.org/papers/v8/
cawley07a.html

[5] Vishal D Chaudhari and Avinash D Desai. 2011. Performance evaluation of vac-
uum system: Pump-down time. International Journal of Scientific and Engineering
Research 2, 11 (2011).

[6] VV Dmitrieva, GP Averyanov, and SE Ulin. 2017. Simulation modelling in vacuum
engineering. In Journal of Physics: Conference Series, Vol. 798. IOP Publishing,
012155.

[7] Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. 2011. A note on the complexity of L

p minimization. Mathematical programming 129, 2 (2011), 285–299.

[8] Benyamin Ghojogh and Mark Crowley. 2019. The Theory Behind Over-
fitting, Cross Validation, Regularization, Bagging, and Boosting: Tutorial.
arXiv:stat.ML/1905.12787

[9] Thomas Giegerich and Christian Day. 2014. The KALPUREX-process – A new
vacuum pumping process for exhaust gases in fusion power plants. Fusion Engi-
neering and Design 89, 7 (2014), 1476–1481. https://doi.org/10.1016/j.fusengdes.
2014.03.082 Proceedings of the 11th International Symposium on Fusion Nuclear
Technology-11 (ISFNT-11) Barcelona, Spain, 15-20 September, 2013.

[10] Jianmei Guo, Krzysztof Czarnecki, Sven Apel, Norbert Siegmund, and Andrzej
Wąsowski. 2013. Variability-aware performance prediction: A statistical learning
approach. In 2013 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE). 2013 28th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 301–311. https://doi.org/10.1109/ASE.2013.6693089
[11] Alex Hernández-García and Peter König. 2018. Further advantages of data
augmentation on convolutional neural networks. In International Conference on
Artificial Neural Networks. Springer, 95–103.

[12] Saksham Jain, Gautam Seth, Arpit Paruthi, Umang Soni, and Girish Kumar. 2022.
Synthetic data augmentation for surface defect detection and classification using
deep learning. Journal of Intelligent Manufacturing 33, 4 (01 Apr 2022), 1007–1020.
https://doi.org/10.1007/s10845-020-01710-x

[13] Xiang ji Yue, Ying li Zhang, Ze hao Su, De chun Ba, Guang yu Wang, and Zhen
hou Zhang. 2017. CFD-based analysis of gas flow in dry scroll vacuum pump.
Vacuum 139 (2017), 127–135. https://doi.org/10.1016/j.vacuum.2017.02.019
[14] Zhou Hua Jiang, Jia Yu, Fu Bin Liu, Xu Chen, and Xin Geng. 2017. Application
of Mathematical Models for Different Electroslag Remelting Processes. High
Temperature Materials and Processes 36, 4 (2017), 411–426. https://doi.org/10.
1515/htmp-2016-0146

[15] Abdellah Kharicha, Ebrahim Karimi-Sibaki, Menghuai Wu, Andreas Ludwig, and
Jan Bohacek. 2018. Review on Modeling and Simulation of Electroslag Remelting.

Testing of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

steel research international 89, 1 (2018), 1700100. https://doi.org/10.1002/srin.
201700100 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/srin.201700100
[16] Brent Komer, James Bergstra, and Chris Eliasmith. 2019. Hyperopt-sklearn. In

Automated Machine Learning. Springer, Cham, 97–111.

[17] Hyeongwoon Lee and Jongsoo Lee. 2021. Neural network prediction of sound
quality via domain Knowledge-Based data augmentation and Bayesian approach
with small data sets. Mechanical Systems and Signal Processing 157 (2021), 107713.
[18] Yu Liu, Xijie Wang, Guangqiang Li, Qiang Wang, Zhao Zhang, and Baokuan Li.
2018. Role of vacuum on cleanliness improvement of steel during electroslag
remelting. Vacuum 154 (2018), 351–358. https://doi.org/10.1016/j.vacuum.2018.
05.032

[19] Patxi Ortego, Alberto Diez-Olivan, Javier Del Ser, and Basilio Sierra. 2020. Data
augmentation for industrial prognosis using generative adversarial networks. In
International Conference on Intelligent Data Engineering and Automated Learning.
Springer, 113–122.

[20] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz
Hardt, John Miller, and Ludwig Schmidt. 2019. A Meta-Analysis of Overfitting
in Machine Learning. In Advances in Neural Information Processing Systems,
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(Eds.), Vol. 32. Curran Associates, Inc.

[21] Miriam Seoane Santos, Jastin Pompeu Soares, Pedro Henrigues Abreu, Helder
Araujo, and Joao Santos. 2018. Cross-Validation for Imbalanced Datasets: Avoid-
ing Overoptimistic and Overfitting Approaches [Research Frontier]. IEEE Com-
putational Intelligence Magazine 13, 4 (2018), 59–76. https://doi.org/10.1109/MCI.
2018.2866730

[22] Siyu Shao, Pu Wang, and Ruqiang Yan. 2019. Generative adversarial networks
for data augmentation in machine fault diagnosis. Computers in Industry 106

(2019), 85–93.

[23] Ewa Sjöqvist Persson, Andrey Karasev, Alec Mitchell, and Pär G. Jönsson. 2020.
Origin of the Inclusions in Production-Scale Electrodes, ESR Ingots, and PESR
Ingots in a Martensitic Stainless Steel. Metals 10, 12 (2020). https://doi.org/10.
3390/met10121620

[24] Stefan Thurner, Rudolf Hanel, and Peter Klimek. 2018. Introduction to the theory

of complex systems. Oxford University Press.

[25] Andrius Vabalas, Emma Gowen, Ellen Poliakoff, and Alexander J. Casson. 2019.
Machine learning algorithm validation with a limited sample size. PLOS ONE 14,
11 (11 2019), 1–20. https://doi.org/10.1371/journal.pone.0224365

[26] Caspar J Van Lissa. 2020. Small sample meta-analyses: Exploring heterogeneity

using MetaForest. In Small Sample Size Solutions. Routledge, 186–202.

[27] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang,
and Huan Xu. 2021. Time Series Data Augmentation for Deep Learning: A
Survey. In Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence, IJCAI-21, Zhi-Hua Zhou (Ed.). International Joint Conferences on
Artificial Intelligence publisher, 4653–4660. https://doi.org/10.24963/ijcai.2021/
631 Survey Track.

[28] Ruiyuan Wu, Wing-Kin Ma, Yuening Li, Anthony Man-Cho So, and
Nicholas D. Sidiropoulos. 2021. Probabilistic Simplex Component Analysis.
arXiv:eess.SP/2103.10027

[29] Anthony Yen. 2018. Continued Scaling in Semiconductor Manufacturing with

EUV Lithography. In 2018 EUVL Workshop P, Vol. 3. 11–14.

[30] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine Learning Test-
ing: Survey, Landscapes and Horizons. IEEE Transactions on Software Engineering
48, 01 (Feb 2020), 1–37. https://doi.org/10.1109/TSE.2019.2962027

