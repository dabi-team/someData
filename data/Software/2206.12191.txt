Computational Complexity Evaluation of Neural
Network Applications in Signal Processing

Pedro J. Freire, Sasipim Srivallapanondh, Antonio Napoli, Jaroslaw E. Prilepsky, Sergei K. Turitsyn

1

2
2
0
2

n
u
J

4
2

]
P
S
.
s
s
e
e
[

1
v
1
9
1
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—In this paper, we provide a systematic approach for
assessing and comparing the computational complexity of neural
network layers in digital signal processing. We provide and link
four software-to-hardware complexity measures, deﬁning how
the different complexity metrics relate to the layers’ hyper-
parameters. This paper explains how to compute these four
metrics for feed-forward and recurrent layers, and deﬁnes in
which case we ought to use a particular metric depending on
whether we characterize a more soft- or hardware-oriented ap-
plication. One of the four metrics, called ‘the number of additions
and bit shifts (NABS)’, is newly introduced for heterogeneous
quantization. NABS characterizes the impact of not only the
bitwidth used in the operation but also the type of quantization
used in the arithmetical operations. We intend this work to serve
as a baseline for the different levels (purposes) of complexity
estimation related to the neural networks’ application in real-time
digital signal processing, aiming at unifying the computational
complexity estimation.

Index Terms—Neural network, Computational complexity,

Hardware estimation, Signal processing.

I. INTRODUCTION

O VER the last few decades, neural networks (NNs) have

begun to ﬁnd widespread usage in a wide range of signal
processing applications: ﬁltering, parameter estimation, signal
detection, system identiﬁcation, pattern recognition, signal
reconstruction, time series analysis, signal compression, signal
transmission, etc. [1]–[4]. Audio, video, image, communica-
tion, geophysical and radar scanning data, are the examples of
important signal types that typically undergo various forms of
signal processing [5]–[7]. The key capabilities of NNs in signal
processing are: performing distributed processing, emulating
nonlinear transformations and processes, self-organizing, and
enabling high-speed processing communication applications
[8]–[10] . With these properties, NNs can provide a very
powerful means of solving many signal processing tasks, par-
ticularly in the areas related to the nonlinear signal processing,
real-time signal processing, adaptive signal processing, and
blind signal processing [5], [11]–[13].

Real-time signal processing, as an example, is a ﬁeld that
enables technological breakthroughs by effectively incorpo-
rating signal processing in hardware: real-time and onboard

This paper was supported by the EU Horizon 2020 program under the
Marie Sklodowska-Curie grant agreement 813144 (REAL-NET) and 956713
(MENTOR). JEP is supported by Leverhulme Trust, Grant No. RP-2018-063.
SKT acknowledges the support of the EPSRC project TRANSNET.

Pedro J. Freire, Sasipim Srivallapanondh, Jaroslaw E. Prilepsky and Sergei
K. Turitsyn are with Aston Institute of Photonic Technologies, Aston Univer-
sity, United Kingdom, p.freiredecarvalhosouza@aston.ac.uk.

Antonio Napoli is with Inﬁnera R&D, Sankt-Martin-Str. 76, 81541, Munich,

Germany.

Manuscript received xxx 19, zzz; revised January 11, yyy.

signal processing is the key to the evolution of phones and
watches into smartphones/smartwatches. To the best of our
knowledge, one of the ﬁrst real-time applications of NNs was
discussed in 1989 [14], and numerous works since then have
deliberated the challenges of implementing such solutions in
hardware exploiting the notion of computational complexity
[15]–[24].

From a computer science perspective, computational com-
plexity analysis is almost always attributed to the Big-O
notation of the algorithm [25]–[27]. In general, the Big-O
notation is used to express an algorithm’s complexity while
assessing its efﬁciency, which means that we are interested
in how effectively the algorithm scales with the size of the
dataset in terms of running time [28]–[30]. However, from the
engineering standpoint, the Big-O is often an oversimpliﬁed
measure that cannot be immediately translated into the hard-
ware resources required to realize the algorithm (NNs) in a
hardware platform [16].

Due to this problem that refers to the absence of some
“universal” measure, various works started to present com-
plexity in terms of multiply and accumulate (MAC) [15]–[18],
Kolmogorov complexity [19], the number of bit-operations
(BOP) [20], [21], the number of real multiplications (RM)
[22]–[24]. However, it is not always clear when to use each
speciﬁc metric, and, more importantly, none of the metrics
mentioned above shows the beneﬁts of using different strate-
gies of quantization for saving the complexity of implementing
the multipliers.

As far as we know, no work has so far uniﬁed the computa-
tional metrics itemized above such that we have no universal
metrics to compare the complexity when different types of
quantization are applied for NN structures. In this paper, we
solve this issue by carrying out a systematic computational
complexity analysis for a zoo of NN layer types. In addition,
we introduce a new useful metric: we coined ‘the number
of additions and bit shifts’ (NABS). This metric takes into
account the impact of the weights’ quantization type on the
reduction of the multipliers’ implementation complexity used
in an NN layer. Overall, we intend our work to give largely
universal measures of complexity to establish a comparison
baseline depending on whether the application is software- or
hardware-based.

The paper is organized as follows. In Sec. II we de-
scribe the details of different computational metrics from
soft- and hardware implementation levels. Sec. III presents
how to compute the computational complexity of different
NN layers. Sec. IV describes the results of our evaluation
of how complexity grows against the design parameters of

 
 
 
 
 
 
2

Fig. 1: Diagram of computational complexity metrics illustrating the various levels of complexity measurement from software
to hardware.

each NN layer. We also address the impact of quantization on
different computational complexity metrics. Our ﬁndings are
summarized in the conclusion.

an acceptable comparative estimate to qualitatively assess the
complexity against some existing benchmarks (e.g. against the
DSP operations for optical channel equalization tasks [32]).

II. FOUR METRICS OF COMPUTATIONAL COMPLEXITY

Accurate computational complexity evaluation is critical in
the design of digital signal processing (DSP) devices to better
understand the implementation feasibility and bottlenecks for
each device’s structure. With this in mind, we summarize
the four most commonly used different criteria for assessing
the computational complexity, from the software level to the
hardware level, in Fig. 1.

The ﬁrst, most software-oriented, level of estimation tra-
ditionally deals only with counting the number of real mul-
tiplications of the algorithm [31], [32] (quite often deﬁned
per one processed element, say a sample or a symbol). This
metric is the number of real multiplications (RM). When
comparing computational complexity, the purpose of this high-
level metric is to consider only the multipliers required,
ignoring additions, because the implementation of the latter
in hardware or software is initially considered cheap, while
the multiplier is generally the slowest element in the system
and consumes the largest chip area [31], [33]. This ignoring
the additions can also be easily understood by looking at the
Big-O analysis of multiplier versus adder. When multiplying
two integers with n digits, the computational complexity of
the multiplication instance is O(n2), whereas the addition of
the same two numbers has a computational complexity of
Θ(n) [34]1. As a result, if you are dealing with ﬂoat values
with 16 decimal digits, multiplication is by far the most time-
consuming part of the implementation procedure. Therefore,
when comparing solutions that use ﬂoating-point arithmetic
with the same bitwidth precision, the RM metric provides

1The Big-O notation represents the worst case or the upper bound of the
time required to perform the operation, Big Omega (Ω) shows the best case or
the lower bound whereas the Big Theta (Θ) notation deﬁnes the tight bound of
the amount of time required, in other words, f (n) is claimed to be Θ(g(n))
if f (n) is O(g(n)) and f (n) is Ω(g(n)).

When moving to ﬁxed-point arithmetic, the second metric
known as the number of bit-operations (BOP) must be adopted
to understand the impact of changing the bitwidth precision on
the complexity. The BOP metric provides a good insight into
mixed-precision arithmetic performance, since we can forecast
the BOP needed for fundamental arithmetic operations like ad-
dition and multiplication, given the bitwidth of two operands.
In a nutshell, the BOP metric aims to generalize ﬂoating-point
operations (FLOPs) to heterogeneously quantized NNs, as far
as the FLOPs cannot be efﬁciently used to evaluate integer
arithmetic operations [21], [35]. For the BOP metric, we have
to include the complexity contribution of both multiplications
and additions, since now we evaluate the complexity in terms
of the most common operations in NNs: the multiply-and-
accumulate operations (MACs) [21], [35], [36]. However, the
BOP accounts for the scaling of the number of multipliers with
the bitwidth of two operands, and the scaling of the number of
adders with the accumulator bitwidth. Note that since most real
DSP implementations use dedicated logic macros (e.g. DSP
slice in Field Programmable Gate Arrays [FPGA] or MAC
in Application Speciﬁc Integrated Circuit [ASIC]), the BOP
metric ﬁts as a good complexity estimation metric inasmuch
as the BOP also accesses the MAC taking into account the
particular bitwidth of two operands.

TABLE I: Capacity ranges for XC4000 Series CLB Resources
given in Ref. [37].

CLB Resource

Logic Gate Range

Gate range per 4-input LUT (2 per CLB)
Gate range per 3-input LUT
Gate range per ﬂip-ﬂop (2 per CLB)
Total gate range per CLB
Estimated typical number of gates per CLB

1 to 9
1 to 6
6 to 12
15 to 48
28.5

HardwareLevelNumber of Real MultiplicationsNumber of BitOperationsNumber of Shift and Add OperationsNumber of Hardware Logic Gates×10110ΣABSoftwareLevelNumber of float point multiplications which the NN model operatesNumber of multiply-accumulate operations with bitwidthconsiderationwhere the bit precision of input, weights and activation function is taken into accountNumber of fixed-point operationswhen only shifts and adders are used to implement all arithmetic operations of the NN modelNumber of logic gates requiredin hardware when implementing the NN modelThe progress in the development of new advanced NN
quantization techniques [38]–[41] allowed implementing the
ﬁxed point multiplications participating in NNs efﬁciently,
namely with the use of a few bit-shifters and adders [42]–
[44]. Since the BOP lacks the ability to properly assess the
effect of different quantization strategies on the complexity, a
new, more sophisticated metric is required there. We introduce
the third complexity metric that counts the number of total
equivalent additions to represent the multiplication operation,
called the number of additions and bit shifts (NABS). The
number of shift operations can be neglected when calculating
the computational complexity because, in the hardware, the
shift can be performed without extra costs in constant time
with the O(1) complexity. Even though the cost of bit shifts
can be ignored due to the aforementioned reasons, and only
the total number of adders has to be accounted for to measure
the computational complexity, we prefer to keep the full name
“number of additions and bit shifts” to highlight that the
multiplication is now represented as shifts and adders.

Finally, the metric which is closest to the hardware level is
the number of logic gates (NLG) that is used for our evaluating
method’s hardware (e.g. ASIC or FPGA) implementation. It
is different from the NABS metric, as now the true cost of
implementation is to be presented. In this case, the activation
function cost, represented by look-up tables (LUT) is also
taken into account. Additionally, other metrics like the number
of ﬂip-ﬂops (FFs) or registers, the number of logic blocks
used for general logic and memory blocks, or other special
functional macros used in the design, are also relevant. As it
is clear from this explanation, there will be no straightforward
equation to convert the NABS to NLG as the latter depends
on the circuit design adopted by the developer. Tools such as
Synopsys Synthesis [45] for ASIC implementation can provide
this kind of information. However, with regard to the FPGA
design, it is harder to get a correct estimate of the gate count
from the report of the FPGA tools [46].

In this paper, we advocate that the NLG metric should be
applied to count the number of logic gates used to implement
the hardware piece, similar to the concept of the Maximum
Logic Gates metric for FPGA devices [37]. The Maximum
Logic Gates metric is utilized to approximate the maximum
number of gates that can be realized in the FPGA for a design
consisting of only logic functions2. Additionally, this metric
is based on an estimate of the typical number of usable gates
per conﬁgurable logic block (CLB) or logic cell multiplied by
the total number of such blocks or cells [37]. With regard to
the correspondence between CLB and logic gates number, see
Table I.

It should be noted that Table I is based on an older, now
obsolete, 4-input LUT architecture [37]. Newer FPGA families
now feature a 6-input LUT architecture, and to address the
resource consumption for the new generation of devices, a
reasonable approximation would be to increase the ‘maximum
gate range equivalent per LUT’ ﬁgure used in [37] by 50%.
Note that the gate equivalence ﬁgures for FF’s (registers)
still hold true for the 6-input architecture. It is also worth

2On-chip memory capabilities are not factored into this metric.

3

noting that the CLB architecture has changed substantially
since Ref. [37] was published, such that we include Table II
linking CLB-gates with more up-to-date 6-input architecture.

TABLE II: Estimated capacity ranges for 6 input LUT based
CLB Resources.

CLB Resource

Logic Gate Range

Gate range per 6-input LUT (8 per CLB)
Gate range per ﬂip-ﬂop (16 per CLB)
Total gate range per CLB

6 to 15
6 to 12
144 to 312

To conclude, we comment on universal metrics between
the FPGA and the ASIC implementations. We emphasize
that calculating an ASIC gate equivalent to an FPGA DSP
slice is not a straightforward task because not all features
are necessarily required when implementing the speciﬁc arith-
metic function in an ASIC. However, utilizing the estimation
approach laid out
in Ref. [37], a ﬁgure can be obtained.
Using the Xilinx Ultrascale + DSP48E2 slice basic multiplier
functionality as an example (see Xilinx UG579 Fig. 1-1 in
Ref. [47]) and pipelining it for maximum performance, it is
possible to estimate the number of FFs and adders required for
such an ASIC equivalence. Taking into account the structure
of the multiplication of a m-bit number by a n-bit number,
implemented using an array multiplier architecture, it is equiv-
alent to m × n AND gates, n half adders, and (m − 2) × n
full adders3. For example, the ASIC equivalence of a 27×18
multiplier in an FPGA would have 486 AND gates, 18 half
adders, 450 full adders, and 90 Flip Flops.

III. MATHEMATICAL COMPLEXITY FORMULATION

In this section, we provide a brief introduction to various
types of NN: dense layer, Convolutional Neural Networks
(CNN), Vanilla Recurrent Neural Networks (RNN), Long
Short-Term Memory Neural Networks (LSTM), Gated Re-
current Units (GRU), and Echo State Networks (ESN). We
investigate the computational complexity for each network in
terms of RM, BOP and NABS. In this work, the computational
complexity is formulated per layer, and the output layer is not
taken into account for the complexity calculation to eliminate
redundant computations if multiple layers or multiple NN
types are combined. Table III in the section’s end summarizes
the formulas for the RM, BOP, and NABS for all NN types
studied.

A. Dense Layer

A dense layer, also known as a ‘fully connected layer’, is a
layer in which each neuron is connected with all the neurons
from the previous layer with a speciﬁc weight wij. The input
vector is mapped to the output vector in a nonlinear manner
by the dense layer, due to the participation of a non-linear
activation function. Dense layers can be combined to form a
Multi-Layer Perceptron (MLP), which is a class of a feed-
forward deep NN.

3Note that a half adder is equivalent to 1 AND gate + 1 XOR gate, and a

full adder is equal to 2 AND gates + 2 XOR gates + 1 OR gate

The output vector y of a dense layer given x as an input

vector, is written as:

y = φ(W x + b),

(1)

where y is the output vector, φ is a nonlinear activation
function, W is the weight matrix, and b is the bias vector.
Writing explicitly the matrix operation inside the activation
function:

W x + b =









w11
w21
...

w12
w22
...
wnn1 wnn2









. . . w1ni
. . . w2ni
...

. . .
. . . wnnni









x1
x2
...
xni









+









b1
b2
...
bnn





,




(2)

where ni is the number of features in the input vector and nn
represents the number of neurons in the layer, we can readily
see that the RM of a dense layer can be computed according
to the simple well-known formula:

RMDense = nnni.

(3)

Now we calculate the BOP of a dense layer, taking into
account the bitwidth of two operands, to approximate the
computational complexity of NNs when the mixed-precision
arithmetic is used. The bitwidth, also known as the precision,
is the number of bits used to represent a certain element; for
example, each weight in the weight matrix can be represented
with bw bits. Fig. 2 illustrates the data ﬂow of the MAC
operations for a neuron of a dense layer with ni input features
and bi as input bitwidth. The multiplication of the input
vector and the weights for one neuron can be mathematically
represented as follows:

yMUL, one neuron =

ni(cid:88)

n=1

wnxn.

(4)

Initially, the ni multiplications of input vector elements and
weights for one neuron take place. When the multiplication of
two operands is performed, the resulting bitwidth is the sum
of the bitwidths of two operands (bw + bi) as it is shown in
the ﬁrst row of Fig. 2.

After that, ni − 1 additions need to be made, and the
resulting number of bits can be deﬁned as follows. Considering
that the result of the addition of two operands has the bitwidth
of the bigger operand plus one bit, we start adding the
multiplication results pairwise until only one element remains.
In this case, the second row of Fig. 2 shows the ﬁrst level of
pairwise additions, with resulting bitwidth of bw + bi + 1, and
since this pairwise addition process is repeated for (cid:100)log2(ni)(cid:101)
levels (until we have just a ﬁnal single number), the total
bitwidth of it is given by bw + bi + (cid:100)log2(ni)(cid:101), i.e. it is the
bitwidth required to perform the overall MAC process. Then,
the addition of the bias vector is performed. In this work,
for all types of networks, we assume that the size of the
accumulator deﬁned by the multiplication of the weight matrix
and the input vector, is dominant; thereby, the assumption for
the bias bitwidth bb is as follows: bb < bw+bi+(cid:100)log2(ni)(cid:101), and
the addition of bias, in the end, will not result in the overﬂow.

4

Finally, the bitwidth of the resulting number is truncated to
ba, where ba is the bitwidth of the activation function [16].

When calculating the BOP for a dense layer, the costs
of both multiplications and additions need to be included.
Then, the BOP formula takes the form of the sum of two
constituents, BOPMul and BOPBias, corresponding to vector-
matrix multiplication and bias addition:

BOPMul = nn

(cid:2)nibwbi + (ni − 1)(bw + bi + (cid:100)log2(ni)(cid:101))(cid:3), (5)

BOPBias ≈ nn(bw + bi + (cid:100)log2(ni)(cid:101)).

(6)

Eq. (5) shows the cost of the number of one-bit full
adders calculated from the dot product of ni-dimensional input
vector and weight matrix, as in Refs. [21], [48]. The cost
takes into account the bitwidths of the weights and input,
bw and bi. To compute the product of the two operands, we
have to use ninn multiplications and nn(ni − 1) additions.
The multiplication cost can be calculated by the number of
multiplications multiplied by bwbi, which is related to the
bit operation, and the number of additions multiplied by the
accumulator bitwidth required to do the operation. The ﬁnal
BOP is the contribution of multiplication and the addition of
bias of the dense layer. For the convenience of the forthcoming
presentation, let us deﬁne the short notations:
Mult(ni, bw, bi) = nibwbi + (ni −1)(cid:0)bw +bi + (cid:100)log2(ni)(cid:101)(cid:1),
and

Acc(ni, bw, bi) = bw + bi + (cid:100)log2(ni)(cid:101).

The Acc expression represents the actual bitwidth of the
accumulator required for MAC operation, as shown in Fig.
2. Then, the BOP of the dense layer expressed through the
layer parameters becomes:

BOPDense = BOPMul + BOPBias

≈ nnni
≈ nnni

(cid:2)bwbi + (bw + bi + (cid:100)log2(ni)(cid:101))(cid:3)
(cid:2)bwbi + Acc(ni, bw, bi)(cid:3).

(7)

Now, we note that with the advancement in NN quantiza-
tion techniques, there arises the opportunity to approximate
multiplication by using shift and few add operations only
while still maintaining a good processing accuracy, since the
NNs can diminish the approximation error that the quantized
approximation introduces4 [43], [49]. As mentioned in Sec. II,
the number of shifts can be neglected compared to the con-
tribution of adders. The number of adders is different for
different types of quantization. To be more speciﬁc, let X
represent the number of adders required, at most, to perform
the multiplication and let b be the bitwidth of the quantized
matrix. For uniform quantization, we have: X = b − 1. And,
for example, when the weight matrix with bitwidth of bw, is
quantized, we have Xw = bw − 1 as the number of adders
we need at most to perform the multiplication of the weights

4Note that using the shifts and adders to perform multiplications can cause
some quantization noise/error since we are converting from a ﬂoat-point
representation to a ﬁxed-point representation with some deﬁned quantized
level of values. However, in NNs, this noise can be partially mitigated by
including those quantized weights in the NN training process as in Refs. [39]–
[41]

5

Fig. 2: Data path of a neuron in a quantized dense layer where x is the input vector with size ni, w is the weight matrix, bw
is the weight bitwidth and bi is the input bitwidth, ba is the activation bitwidth and bb is the bias bitwidth.

5. In the case of Power-of two (PoT) quantization, we have:
X = 0, because each multiplication costs just a shift [42], [53].
Lastly, for the Additive Powers-of-Two (APoT) quantization,
we have: X = n, where n denotes the number of additive
terms. In APoT, the sum of n PoT terms is used to represent
each quantization level [38]. Eventually, the NABS of a dense
layer can be derived from its BOP equation, Eq. (7):

NABSDense ≈ nnni

(cid:2)XwAcc(ni, bw, bi)+Acc(ni, bw, bi)(cid:3)

≈ nnni(Xw + 1)Acc(ni, bw, bi).

(8)

(8),

the multiplication term bwbi

As in Eq.
(7)
is converted into the number of adders needed to operate
the multiplication times the accumulator bitwidth required:
XwAcc(ni, bw, bi).

in Eq.

B. Convolutional Neural Networks

In CNN, we apply the convolutions with different ﬁlters to
extract the features and convert them into a lower-dimensional
feature set, but still preserve the original properties. CNNs
can be used in 1D, 2D, or 3D networks depending on the
applications. In this paper, we focus on 1D-CNNs, which are
applicable to processing sequential data [3]. For simplicity of
understanding, the 1D-CNN processing with padding equal to
0, dilation equal to 1, and stride equal to 1, can be summarized

5Note that we can consider other techniques for the representation of such
ﬁxed-point multiplication to reduce its complexity e.g. the double-base number
system where each multiplication with b bits, at worst, needs no more than
b/log(b) additions [50]. For the Canonical Signed Digit (CSD) representation,
in the worst-case scenario, we have (b + l)/2 nonzero bits and on average it
tends asymptotically to (3b + l)/9 [51], [52]

as follows:



yf
i = φ



ni(cid:88)

nk(cid:88)

n=1

j=1



i+j−1,n · kf
xin

j,n + bf

 ,

(9)

where yf
i denotes the output, known as a feature map, of
a convolutional layer built by the ﬁlter f in the i-th input
element, nk is the kernel size, ni is the size of the input vector,
xin represents the raw input data, kf
j denotes the j-th trainable
convolution kernel of the ﬁlter f and bf is the bias of the ﬁlter
f .

In the general case, when designing the CNN, the param-
eters like padding, dilation, and stride also affect the output
size of the CNN. It can be formularized as:

OutputSize =

(cid:20) ns + 2 padding−dilation(nk − 1)−1
stride

(cid:21)
,

+1

(10)

where ns is the input time sequence size.

The RM of a 1D-convolutional layer can be computed as

follows:

RMCNN = nf nink · OutputSize,

(11)

where nf is the number of ﬁlters, also known as the output
dimension. As in Eq. (11), there are nink multiplications per
sliding window, and the number of times that sliding window
process needs to be repeated is equal to the output size. Then,
the procedure is executed repeatedly for all nf ﬁlters.

The BOP for a 1D-convolutional layer, after taking into con-
sideration the multiplications and additions, can be represented
as:

BOPCNN = OutputSize · nf Mult(nink, bw, bi)

+nf Acc(nink, bw, bi).

(12)

𝑤1×…Multiplication: 𝑏𝑤+𝑏𝑖Addition: 𝑏𝑤+𝑏𝑖+1Addition: 𝑏𝑤+𝑏𝑖+2Addition: 𝑏𝑤+𝑏𝑖+log2𝑛𝑖bias++++TruncateTruncate: 𝑏𝑎bits…𝑏𝑏bits…+Operation:Resulting Bits××××𝑏𝑖bits𝑏𝑤bits𝑤2𝑤3𝑤4𝑤𝑛𝑖+Output (Activation Function)𝑥𝑛𝑖𝑥4𝑥2𝑥3𝑥1Eq. (12) is derived from Eq. (9) and Eq. (11). The ﬁrst
term is associated with the convolution operation between the
ﬂattened input vector and the sliding windows, and the latter
term corresponds to the addition of the bias.

The procedure to derive the NABS is similar to that de-
scribed in details in the case of a dense layer, Sec. III-A. The
NABS of a 1D-convolutional layer is given by:

NABSCNN = OutputSize · nf

(cid:2)nink(Xw + 1) − 1(cid:3)

· Acc(nink, bw, bi)
+nf Acc(nink, bw, bi).

(13)

To obtain the 1D-convolutional layer’s NABS, the multipli-
cation in Eq. (12) is represented by the number of adders
required, at most,
to perform the multiplication times the
accumulator bitwidth.

C. Vanilla Recurrent Neural Networks

Vanilla RNN is different from MLP and CNN in terms of
its ability to handle the memory, which is quite beneﬁcial for
time series data. RNNs take into account the current input
and the output that the network has learned from the prior
input. Even though the RNNs introduced the efﬁcient memory
handling, it still suffers from the inability to capture the long-
term dependencies because of the vanishing gradient issue
[54]. The equations for the vanilla RNN given a time step
t are as follows:

ht = φ(W xt + U ht−1 + b),

(14)

where φ is, again, the nonlinear activation functions, xt ∈
Rni is the ni-dimensional input vector at time t, ht ∈ Rnh
is a hidden layer vector of the current state with size nh,
W ∈ Rnh×ni and U ∈ Rnh×nh represent the trainable weight
matrices, and b is the bias vector. For more explanations on
the vanilla RNN operation, see Ref. [55]. The RM of a vanilla
RNN is:

RMRNN = nsnh(ni + nh),

(15)

where nh notes the number of hidden units. From Eq. (15),
the RM for a time step is nh(ni +nh). It can be separated into
two terms; the nhni term corresponds to the multiplication of
the input vector xt and the weight matrix, and the n2
h term
arises because of the multiplication to the prior cell output
ht−1. Finally, ns, which denotes the number of time steps
in the layer, should be taken into account, as the process is
repeated ns times.

The BOP for a vanilla RNN is given as:

BOPRNN = nsnhMult(ni, bw, bi)
+nsnhMult(nh, bw, ba)
+2nsnhAcc(nh, bw, ba).

(16)

From Eq. (16), the ﬁrst term is associated with the input
vector multiplied by the weight matrix, and the second term
corresponds to the multiplications of the recurrent cell outputs.
The ﬁnal term is the contribution of the addition between
W xt + U ht−1 and the addition of the bias vector in Eq. (14);
one can see that the size of the accumulator used in this
is due to the assumption that
term,

is Acc(nh, bw, ba). It

6

Acc(nh, bw, ba) is dominant because it should be greater than
Acc(ni, bw, bi) as a result of the inequality nh > ni.

As in the case of a dense layer, the NABS of vanilla RNN
can be calculated from its BOP equation by converting the
multiplication to the number of adders needed at most (X)
depending on the quantization scheme and the accumulator
size:

NABSRNN = nsnh
+nsnh

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)nh(Xw + 1) + 1(cid:3)Acc(nh, bw, ba).

(17)

D. Long Short-Term Memory Neural Networks

LSTM are an advanced type of RNNs. Although RNNs
suffer from short-term memory issues, the LSTM network
has the ability to learn long-term dependencies between time
steps (t), insofar as it was speciﬁcally designed to address
the gradient issues encountered in RNNs [56], [57]. There
are three types of gates in an LSTM cell: an input gate (it), a
forget gate (ft), and an output gate (ot). More importantly, the
cell state vector (Ct) was proposed as a long-term memory to
aggregate the relevant information throughout the time steps.
The equations for the forward pass of the LSTM cell given a
time step t are as follows:

it = σ(W ixt + U iht−1 + bf ),
ft = σ(W f xt + U f ht−1 + bi),
ot = σ(W oxt + U oht−1 + bo),
Ct = ft (cid:12) Ct−1 + it (cid:12) φ(W cxt + U cht−1 + bc),
ht = ot (cid:12) φ(Ct),

(18)

where φ is usually the “tanh” activation functions, σ is
the sizes of each
usually the sigmoid activation function,
variable are xt ∈ Rni, ft, it, ot ∈ (0, 1)nh , Ct ∈ Rnh and
ht ∈ (−1, 1)nh . The (cid:12) symbol represents the element-wise
(Hadamard) multiplication.

The RM of a LSTM layer is:

RMLSTM = nsnh(4ni + 4nh + 3),

(19)

where nh is the number of hidden units in the LSTM cell.
Similarly to RNNs, the RM can be calculated from the term
associated with the input vector xt and the term corresponding
to the prior cell output ht−1; however, each term occurs four
times, as we can see in Eq. (18). Therefore, we have 4nhni
and 4n2
h, respectively. Moreover, we also need to include the
element-wise product that is operated three times in Eq. (18),
which costs 3nh. Finally, the process is repeated ns times,
hence, ns is multiplied to the overall number.

The BOP for a LSTM layer is computed based on Eq. (19),
but also includes the bitwidth of the operands and the number
of additions. As a result, the BOP can be represented as:

BOPLSTM = 4nsnhMult(ni, bw, bi)
+4nsnhMult(nh, bw, ba)
+3nsnhb2
a
+9nsnhAcc(nh, bw, ba).

(20)

To give more details on the expression, the ﬁrst two terms in
Eq. (20) are the contribution of the input vector multiplications

and the recurrent cell output association, respectively. The term
3nsnhb2
a refers to 3 times of the element-wise product of two
operands with ba bitwidth, see Eq. (18). For each time step,
there are nh elements in each vector needed to be multiplied.
The last term is for all the additions, since we assume that
Acc(nh, bw, ba) gives the dominant contribution, as described
in Sec. III-C. Finally, the process is then restarted ns times.
The NABS of a LSTM layer is derived from the Eq. (20)
by replacing the multiplications with the shifts and adders
including their cost, as mentioned in Sec. II that the shifts
would not be included. The number of adders depends on the
quantization technique. The NABS would be as follows:
(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)nh(Xw + 1) + 1(cid:3)Acc(nh, bw, ba)

NABSLSTM = 4nsnh
+4nsnh
+6nsnhba.

(21)

The ﬁrst and second terms are the results of input vector
multiplications and the recurrent cell operation combined with
all addition operations, respectively. In this case, the third term
comes from 3nsnh(ba + ba). Due to the element-wise product
of two operands with bitwidth ba,
the resulting bitwidth
becomes ba + ba as mentioned in Fig. 2.

E. Gated Recurrent Units

Like LSTM, the GRU network was created to overcome
the short-term memory issues of RNNs. However, GRU is
less complex, as it has only two types of gates: reset (rt)
and update (zt) gates. The reset gate is used for short-term
memory, whereas the update gate is responsible for long-term
memory [58]. In addition, the candidate hidden state (h(cid:48)
t) is
also introduced to state how relevant the previous hidden state
is to the candidate state. The GRU for a time step t can be
formalized as:

zt = σ(W zxt + U zht−1 + bz),
rt = σ(W rxt + U rht−1 + br),
t = φ(W hxt + rt (cid:12) U hht−1 + bh),
h(cid:48)
ht = zt (cid:12) ht−1 + (1 − zt) (cid:12) h(cid:48)
t,
where φ is typically the “tanh” activation function and the rest
of designations are the same as in Eq. (18).

(22)

The RM of the GRU is calculated in the same way as we
did for the LSTM in Eq. (19), but the number of operations
with the input vector xt and with the previous cell output
ht−1 is reduced from four (LSTM) to three times as shown in
Eq. (22). Thus, the expression for the RM becomes:

RMGRU = nsnh(3ni + 3nh + 3).

(23)

The BOP for the GRU can be calculated in the same
manner as we did for the LSTM in Eq. (20). However, now
the expression is slightly different in the number of matrix
multiplications as the number of gates is now lower. The BOP
number can be represented as:

BOPGRU = 3nsnhMult(ni, bw, bi)
+3nsnhMult(nh, bw, ba)
+3nsnhb2
a
+8nsnhAcc(nh, bw, ba).

(24)

7

The explanation for each line here is identical
Eq. (20).

to that

in

The NABS of the GRU is derived similarly to the LSTM

case:

NABSGRU = 3nsnh
+nsnh
+6nsnhba.

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)3nh(Xw + 1) + 5(cid:3)Acc(nh, bw, ba)

(25)

Again, the explanation for each term in this expression is
identical to Eq. (21).

F. Echo State Networks

ESN belongs to the class of recurrent layers, but more
speciﬁcally, to the reservoir computing category. ESN was
proposed to relax the training process, while being efﬁcient
and simple to implement. The ESN comprises three layers:
an input layer, a recurrent layer, known as a reservoir, and
an output layer, which is the only layer that is trainable.
The reservoir with random weight assignment
is used to
replace back-propagation in traditional NNs to reduce the
computational complexity of training [59]. We notice that the
reservoir of the ESNs can be implemented in two domains:
digital and optical [60]. With the optical implementation of
the reservoir, the computational complexity dramatically falls,
however, the degradation of the performance due to the change
of domain is noticeable [61]. In this work, we only examine
the digital domain implementation. Moreover, we focus on the
leaky-ESN, as it is believed to often outperform standard ESNs
and is more ﬂexible due to time-scale phenomena [62], [63].
The equations of the leaky-ESN for a certain time step t are
given as:

at = φ (cid:0)W rst−1 + W inxt
st = (1 − µ)st−1 + µat,
yt = W ost + bo,
where st represents the state of the reservoir at time t, W r
denotes the weight of the reservoir with the sparsity parameter
sp, W in is the weight matrix that shows the connection
between the input layer and the hidden layer, µ is the leaky
rate, W o denotes the trained output weight matrix, and yt is
the output vector.

(27)

(28)

(26)

(cid:1) ,

The RM of an ESN is given by

RMESN = nsNr(ni + Nrsp + 2 + no),

(29)

where Nr is the number of internal hidden neuron units of the
reservoir and no denotes the number of output neurons. From
Eq. (29), the Nrni multiplications occur from the input vector
operations, and the term N 2
r sp is included due to the reservoir
layer; to be more speciﬁc, the latter term is multiplied with
the sparsity parameter sp which indicates the ratio of zero
values in the matrix. Eq. (27) results in 2Nr multiplications.
Unlike the other network types, now we have to include the
contribution of the output layer explicitly because it contains
the trainable weight matrix, and this layer contributes Nrno
the process is repeated for ns
multiplications. Eventually,
times.

TABLE III: Summary of the three computational complexity metrics per layer (the number of real multiplications, the number
of bit operations, the number of additions and bit shifts) for a zoo of neural network layers as a function of their designing
hyper-parameters; the number of neurons (nn), the number of features in the input vector (ni), the number of ﬁlters (nf ), the
kernel size (nk), the input time sequence size (ns), the number of hidden units (nh), the number of internal hidden neuron
units of the reservoir (Nr), sparsity parameter (sp), the number of output neurons (no), weight bitwidth (bw), input bitwidth
(bi), activation bitwidth (ba) and the number of adders required at most to represent the multiplication (Xw)

8

Network type

Real multiplications (RM)

MLP

nnni

Number of bit-operations (BOP)
(cid:2)bwbi + Acc(ni, bw, bi)(cid:3)

nnni

1D-CNN

nf nink · OutputSize

OutputSize · nf Mult(nink, bw, bi)
+nf Acc(nink, bw, bi)

Vanilla RNN

nsnh(ni + nh)

LSTM

nsnh(4ni + 4nh + 3)

GRU

nsnh(3ni + 3nh + 3)

ESN

nsNr(ni + Nrsp + 2 + no)

nsnhMult(ni, bw, bi)
+nsnhMult(nh, bw, ba)
+2nsnhAcc(nh, bw, ba)
4nsnhMult(ni, bw, bi)
+4nsnhMult(nh, bw, ba)
+3nsnhb2
a
+9nsnhAcc(nh, bw, ba)
3nsnhMult(ni, bw, bi)
+3nsnhMult(nh, bw, ba)
+3nsnhb2
a
+8nsnhAcc(nh, bw, ba)
nsNrMult(ni, bw, bi)
+nsNrspMult(Nr, bw, ba)
+nsNrMult(no, bw, ba)
+2nsNrb2
a
+4nsNrAcc(Nr, bw, ba)

Number of additions and bit shifts(NABS)

nnni(Xw + 1)Acc(ni, bw, bi)

OutputSize · nf

(cid:2)nink(Xw + 1) − 1(cid:3)

·Acc(nink, bw, bi)
+nf Acc(nink, bw, bi)

nsnh
+nsnh

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)nh(Xw + 1) + 1(cid:3)Acc(nh, bw, ba)

4nsnh
+4nsnh

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)nh(Xw + 1) + 1(cid:3)Acc(nh, bw, ba)

+6nsnhba

3nsnh
+nsnh

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)3nh(Xw + 1) + 5(cid:3)Acc(nh, bw, ba)

+6nsnhba

+nsNr

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
nsNr
(cid:2)sp(NrXw + Nr − 1(cid:3) + 4)Acc(Nr, bw, ba)
(cid:2)no(Xw + 1) − 1(cid:3)Acc(no, bw, ba)

+nsNr

+4nsNrba

The BOP number for an ESN can be represented as:

BOPESN = nsNrMult(ni, bw, bi)

+nsNrspMult(Nr, bw, ba)
+nsNrMult(no, bw, ba)
+2nsNrb2
a
+4nsNrAcc(Nr, bw, ba).

(30)

In Eq. (30), the ﬁrst term is the input vector contribution, the
second one is contributed by the reservoir layer, the third term
refers to the output layer multiplications, and the fourth term
stems from the multiplications in Eq. (27). Eventually, all the
addition operations are accounted for by the ﬁnal term.

The NABS of an ESN, which can be calculated in a similar

way as in the LSTM case in Sec. III-D, is:

NABSESN = nsNr
+nsNr
+nsNr
+4nsNrba.

(cid:2)ni(Xw + 1) − 1(cid:3)Acc(ni, bw, bi)
(cid:2)sp(NrXw + Nr − 1(cid:3) + 4)Acc(Nr, bw, ba)
(cid:2)no(Xw + 1) − 1(cid:3)Acc(no, bw, ba)

(31)

By changing the multiplication terms in Eq. (30) to the number
of adders required at most, we obtain the ESN’s NABS.
The input vector multiplication contributes the ﬁrst term. The
reservoir layer and all the addition operations result in the
second term. The third term comes from the output layer of
the ESN. The last term is the contribution of Eq. (27).

IV. COMPARATIVE ANALYSIS OF THE COMPLEXITIES FOR
EACH NN STRUCTURE

The comparison of the complexity in terms of RM is
illustrated in Fig. 3 for feed-forward NNs and in Fig. 4 for
recurrent networks. In feed-forward networks, we ﬁrst address
the computational complexity for a dense layer. In order to
reach over 2 × 106 real multiplications, which we use as a
threshold (highlighted by a maroon color), we can have up
to around 1500 input features (ni) and 1500 neurons (nn)
which is clearly a high value for a single dense layer. The
nnni term in Eq. (3) forms a hyperbolic curve, as can be seen
in Fig. 3a. For the 1D-convolutional layer, as predicted by
Eq. 11, we reach a high complexity (maroon) region using
less input features than the dense layer case because now
the complexity growth depends on more than 2 variables
(e.g. to reach the complexity threshold, the number of time
steps ns can be set to 275 with the kernel size nk equal
to 150, and we ﬁxed ni = 100, no = 1, padding = 0,
dilation = 1, and stride = 1). According to the exemplary
chosen parameters above, the nk ≤ ns condition derived
from Eq. (10) must be satisﬁed in order to obtain at least
the output size equal to 1; therefore, in Fig. 3b, the white
region corresponds to unavailable output and the heat-map has
a hyperbolic form because only ns and nk parameters vary and
the other parameters are kept constant.

The RNN-based networks apparently have higher complex-
ity than the feed-forward NNs. The vanilla RNN in Fig. 4a
shows the least complexity among the RNN-based networks
studied, while the LSTM’s complexity growth is the fastest,

9

(a) Dense layer with parameters; number of features in the input
vector ni and number of neurons in the layer nn.

(b) CNN with parameters; kernel size nk and number of time
steps ns.

Fig. 3: Number of real multiplications (RM) of feed-forward layers.

which can be seen from the size of the maroon area in Fig.
4b. The GRU in Fig. 4c shows slightly lower complexity
than the LSTM because it has a lower number of gates in its
architecture. If we look at the equations of GRU and LSTM
in Table III, the LSTM has a multiplier of 4 for the ni and nh,
whereas for the GRU the multiplier is 3. The ESN with ﬁxed
no = 100 and sp = 0.5 in Fig. 4d has higher complexity than
the vanilla RNN, but less complexity than the GRU, because
the ESN by design has a less complex architecture due to
the use of the reservoir [64]. For all RNN-based networks,
we readily infer that the number of hidden units nh, or Nr
for the ESN, plays the most crucial role in deﬁning the layer’s
computational complexity in terms of RM metric. In Fig.4, we
observe that the top face of all cubes which corresponds to the
highest number of nh (nh = 100) has the largest maroon areas.
In terms of the effect on the complexity behaviour, the second
most important quantity is the number of time steps ns; we
can see in Fig. 4 that the right face of the cubes, referring to
the highest number of ns (ns = 100), has the second-largest
maroon areas. Finally, the dimensions of the input vector ni
have the least impact on the RM, as shown by the left face
(ni = 100) of all cubes in Fig. 4 with the smallest maroon
areas compared to other faces. See Eqs. (3), (11), (15), (19),
(23), and (29) for the exact dependencies.

to highlight

Furthermore,

the computational complexity
trend over those different recurrent
layers, we plotted the
RM versus the number of hidden units (nh for vanilla RNN,
LSTM, and GRU, or Nr in ESN) in a scenario where all
other hyper-parameters are constant. Fig. 5a depicts the result
of this analysis when ns = 100, ni = 100 for all networks,
and no = 100, sp = 0.5 for the ESN. By considering
those parameters as ﬁxed, the complexity of all four recurrent
layers scales quadratically with the number of hidden units
(n2
h), which is traditionally interpreted as having the same
O(n2). However, Fig. 5a brings an important fact that there
are signiﬁcant differences in the computational complexity in

terms of RM between all four recurrent layers. Ultimately, this
means that the Big-O notation is not sensitive enough to assess
the complexity of the NNs in digital signal processing. We can
spot that the LSTM complexity escalates the fastest followed
by the GRU, ESN, and RNN, respectively. These differences
result mainly from the scaling terms on the n2
h of each RM
complexity expression for these layers. Note that for the ESN
(Eq. (29)), the complexity increases more steadily, as far as,
Nr is multiplied to the sparsity parameter sp. Moreover, as
noted in Sec. III-F, the reservoir can be implemented in the
optical domain, so the complexity can be reduced further at
the expense of performance trade-off.

We notice that not only the hyper-parameters like nh and ns,
affect the computational complexity, but also the bitwidth or
precision of each parameter can impact the complexity when
we quantify it in terms of the BOP. The effects produced by
bitwidth value of weight (bw), input (bi), and activation (ba) are
examined in this study. The full-precision or 32-bit precision
can be considered over-redundant because 8-bit or less is often
enough to provide comparable performance, as stated by many
works in the ﬁeld [65]–[68]. Here, we did not provide the study
of the BOP versus different hyper-parameters and bitwidth,
because we believe that this is a straightforward analysis that
approximately follows what we already studied with the RM
metric. Instead, we focus on answering the following question:
which variable bitwidth (bw, bi, or ba) produces the highest
saving in the BOP complexity when its precision is reduced?.
This question can guide the design of a low complexity NN
structure by identifying which parameter of the NN is the key
to producing the higher saving in complexity. To address this
question, we compare the reduction of the BOP when using
8-bit precision versus the 4-bit precision for each parameter
(bw, bi, and ba) in different network types; the results of
the comparison are shown in Fig. 5b. The bitwidth of the
weight matrix bw is the most signiﬁcant parameter to consider
when trying to reduce the layer’s complexity, as the BOP

500100015002000nn25050075010001250150017502000ni0.250.500.751.001.251.501.752.001e650100150200250300ns50100150200250300nk0.000.250.500.751.001.251.501.752.001e610

(a) Vanilla RNN.

(b) LSTM.

(c) GRU.

(d) ESN.

Fig. 4: Number of real multiplications (RM) of recurrent layers with respect to different values of the number of features in
the input vector ni, the number of time steps ns and the number of hidden units nh or Nr in ESN.

is decreased by around 40% for all network types when we
halved the precision of bw. For the dense and 1D-convolutional
layers, the precision for input bi is as important as the bw,
while reducing the bitwidth of the bias vector ba does not
have a noticeable impact on the BOP. In the RNN-based
networks, converting from 8-bit to 4-bit precision for bi and ba
shows a nearly equivalent reduction in the BOP, except for the
ESN case, where decreasing the ba precision results in more
reduction in the BOP than when we reduce the bi precision.

Lastly, we analyze the NABS metric considering various
quantization techniques: uniform, PoT and APoT quantiza-
tion, as described in Sec. III-A. Note that each technique
needs a different number of shifts and adders to perform
the multiplication. As mentioned before, the shifts incur no
extra cost in hardware implementation; therefore, we focus
on evaluating the NABS metric of each layer for certain
quantization techniques versus the number of adders required
at most to perform the multiplication, denoting it as X. More
speciﬁcally, if the weight matrix has bw as its bitwidth and
the uniform quantization is utilized, the number of adders
required at most (Xw) is equal bw − 1. In the case of PoT,
Xw = 0, and for APoT, Xw varies between 1 and bw − 2, in
this case. Fig. 5c shows the NABS versus Xw analysis when
considering that: bw, bi, ba = 8 for all networks, ni = 1000

and nn = 2000 for a dense layer, ni = 100, ns = 300, no = 1,
padding = 0, dilation = 1, stride = 1 and nk = 100 for a
1D-convolutional layer, ni = 100, ns = 100 and nh = 100
for all RNN-based networks, and no = 100, sp = 0.5 for the
ESN.

As it is shown in Fig. 5c, for all types of networks, when
the PoT quantization is used, the NABS can drop around 8
times lower compared to the NABS when using the uniform
quantization. Since APoT is a quantization scheme represented
by a sum of PoT terms, APoT provides a smooth transition
between PoT and uniform quantization. In various works,
PoT was claimed to have very low complexity because the
multiplications are replaced by just shifts [53], [69], [70].
However, when we consider that the multiplication in the
uniform quantization can be represented by shifts and adders,
and we have a fair metric like NABS to compare between
different quantization techniques, the NABS when applying
PoT is only around an order of magnitude lower than the
NABS when using the uniform quantization. To be more
speciﬁc, even though PoT converts all multipliers into bit
shifters, we still have a number of adders coming from the
sum operations which are not related to the multipliers, but
are they are key for the operational structure of the NN
layers. Therefore, the NABS metric can provide a reliable

ni020406080100ns020406080100nh0204060801000.0000.2220.4440.6670.8891.1111.3331.5561.7782.0001e6ni020406080100ns020406080100nh0204060801000.0000.2220.4440.6670.8891.1111.3331.5561.7782.0001e6ni020406080100ns020406080100nh0204060801000.0000.2220.4440.6670.8891.1111.3331.5561.7782.0001e6ni020406080100ns020406080100Nr0204060801000.0000.2220.4450.6670.8891.1111.3331.5561.7782.0001e611

(a)

(b)

(c)

Fig. 5: (a) Complexity comparison between recurrent-based networks when ns = 100, ni = 100 for all networks and no = 100,
sp = 0.5 for ESN; (b) Reduction of BOP in percentage when reducing bitwidth of each parameter; weight bidtwidth bw, input
bidtwidth bi, and activation bidtwidth ba by half or from 8 bits to 4 bits; (c) Comparison of the number of additions and bit
shifts (NABS) with different quantization techniques and different network types, assume bw = 8. Note that Xw is the number
of adders required to represent a multiplier.

assessment of the computational complexity of NNs before
their implementation in hardware, where the NLG will be
the ultimate metric. Note that we intentionally increased the
values of the hyper-parameters for the feed-forward NNs in
order to compare them in the same graph as the RNN-based
networks. For the complexity with regard to NABS, the LSTM
apparently needs the highest number of shifts and adders. In
conclusion, the three matrices of complexity: the RM, the BOP,
and the NABS, have the same trend, meaning that the LSTM
requires the most computational resources, followed by the
GRU. However, the complexity depends on the particular NN
design and can be reduced if we can tolerate a more accurate
trade-off: varying the values of hyper-parameters can affect
the accuracy, but, simultaneously, work in favour to reduce
the computational complexity.

V. CONCLUSION
In this work, we described the systematic approach for
how to evaluate the computational complexity in terms of
the three metrics (noticing that the fourth one, the number of
logic gates, is hardware dependent and cannot be calculated
without referring to a particular setup): the number of real
multiplications (RM), the number of bit operations (BOP), and
the number of additions and bit shifts (NABS); this itemization
implies that we are gradually changing from the software
to hardware level. The introduction of such detailed metrics
has offered us an opportunity to establish a baseline for the
complexity calculation in a more consistent way, depending
on the purpose. We investigated the computation of RM, BOP,
and NABS in feed-forward and recurrent layers, addressing,
namely, a dense layer, a 1D-convolutional layer, a vanilla
RNN, LSTM, GRU, and ESN architectures in rather general
form.

First, we evaluated the RM metric showing how complexity
evolves when changing different hyper-parameters of each NN
layer. Speciﬁcally, for the recurrent layers, as it was actually
expected, the LSTM has the highest complexity among the
examined NNs, because of RNN’s architecture featuring dif-
ferent types of gates; moreover, the LSTM complexity grows

dramatically with the rise of the number of hidden units. The
least complex recurrent architecture is the vanilla RNN. For
all recurrent networks, the most complexity-impactful hyper-
parameter is the number of hidden units, followed by the
number of time steps; the size of the input vector is the least
inﬂuential. Here, we note that the dense layer is the NN layer
type that is the cheapest in terms of complexity, because it
is just a matrix multiplication, while all other layers imply a
more complex processing procedure.

Our paper also shows the importance of bitwidth (or the
precision) for deﬁning the complexity expressed in terms of
BOP, i.e. when we descend closer to the hardware level. With
the two times reduction in bitwidth, especially in the bitwidth
of the weights, the BOP gets drastically reduced, becoming
around 40% lower for all types of NNs. This fact shows that
when designing the NN solutions, we should prioritize low
precision bit in the NN weights to achieve a better reduction
in complexity.

Finally, this paper introduces the new metric named NABS
to highlight the effects of different quantization techniques:
uniform, PoT, and APoT. Different from the other papers
claiming that the complexity can be reduced drastically when
applying the PoT, our current work shows that by using a
fair complexity metric like the NABS, the true complexity
the NABS metric) gets reduced by only around one
(i.e.
order of magnitude when using the PoT compared to the
uniform quantization for all NN layers. Thus, we claim that
the new metric NABS identiﬁes the true complexity level (or
the reduction level) better than the previously used RM or
BOP.

REFERENCES

[1] A. Miller, B. Blott et al., “Review of neural network applications
in medical imaging and signal processing,” Medical and Biological
Engineering and Computing, vol. 30, no. 5, pp. 449–464, 1992.

[2] L. A. Feldkamp and G. V. Puskorius, “A signal processing framework
based on dynamic neural networks with application to problems in adap-
tation, ﬁltering, and classiﬁcation,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2259–2277, 1998.

020406080100Number of hidden units02468Real multiplication (RM)1e6RNNLSTMGRUESNbwbiba010203040% of reduction in BOP when bitwidth is halfDenseCNNRNNLSTMGRUESN0bw1Xw108109NABSPoTAPoTUniformDenseCNNRNNLSTMGRUESN[3] S. Kiranyaz, O. Avci, O. Abdeljaber, T. Ince, M. Gabbouj, and D. J.
Inman, “1d convolutional neural networks and applications: A survey,”
Mechanical systems and signal processing, vol. 151, p. 107398, 2021.
[4] H. Dahrouj, R. Alghamdi, H. Alwazani, S. Bahanshal, A. A. Ahmad,
A. Faisal, R. Shalabi, R. Alhadrami, A. Subasi, M. T. Al-Nory, O. Kit-
taneh, and J. S. Shamma, “An overview of machine learning-based
techniques for solving optimization problems in communications and
signal processing,” IEEE Access, vol. 9, pp. 74 908–74 938, 2021.
[5] S.-i. Amari and A. Cichocki, “Adaptive blind signal processing-neural
network approaches,” Proceedings of the IEEE, vol. 86, no. 10, pp.
2026–2048, 1998.

[6] K. Burse, R. N. Yadav, and S. Shrivastava, “Channel equalization using
neural networks: A review,” IEEE Transactions on Systems, Man, and
Cybernetics, Part C (Applications and Reviews), vol. 40, no. 3, pp. 352–
357, 2010.

[7] M. Kahrs and K. Brandenburg, Applications of digital signal processing
to audio and acoustics. Springer Science & Business Media, 1998.
[8] R. Govil, “Neural networks in signal processing,” Fuzzy Systems and

Soft Computing in Nuclear Engineering, pp. 235–257, 2000.

[9] B. Lusch, J. N. Kutz, and S. L. Brunton, “Deep learning for universal
linear embeddings of nonlinear dynamics,” Nature Communications,
vol. 9, no. 1, pp. 1–10, 2018.

[10] H. Huttunen, “Deep neural networks: A signal processing perspective,”
Springer, 2019, pp. 133–

in Handbook of signal processing systems.
163.

[11] M. A. Jarajreh, E. Giacoumidis, I. Aldaya, S. T. Le, A. Tsokanos,
Z. Ghassemlooy, and N. J. Doran, “Artiﬁcial neural network nonlin-
ear equalizer for coherent optical ofdm,” IEEE Photonics Technology
Letters, vol. 27, no. 4, pp. 387–390, 2015.

[12] F.-L. Luo and R. Unbehauen, Applied neural networks for signal

[13] M.

Ibnkahla,

processing. Cambridge university press, 1998.
“Applications

digital
of
communications–a survey,” Signal processing, vol. 80, no. 7, pp.
1185–1215, 2000.

networks

neural

to

[14] D. Malkoff, “A neural network for real-time signal processing,” Ad-
vances in Neural Information Processing Systems, vol. 2, 1989.
[15] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, “Efﬁcient processing of
deep neural networks: A tutorial and survey,” Proceedings of the IEEE,
vol. 105, no. 12, pp. 2295–2329, 2017.

[16] P. Gysel, M. Motamedi, and S. Ghiasi, “Hardware-oriented ap-
convolutional neural networks,” arXiv preprint

proximation of
arXiv:1604.03168, 2016.

[17] B. Li and T. N. Sainath, “Reducing the computational complexity of
two-dimensional lstms.” in INTERSPEECH, 2017, pp. 964–968.
[18] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo-
lutional neural networks using energy-aware pruning,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2017,
pp. 5687–5695.

[19] J. L. Balc´azar, R. Gavalda, and H. T. Siegelmann, “Computational
power of neural networks: A characterization in terms of kolmogorov
complexity,” IEEE Transactions on Information Theory, vol. 43, no. 4,
pp. 1175–1183, 1997.

[20] M. Van Baalen, C. Louizos, M. Nagel, R. A. Amjad, Y. Wang,
T. Blankevoort, and M. Welling, “Bayesian bits: Unifying quantiza-
tion and pruning,” Advances in neural information processing systems,
vol. 33, pp. 5741–5752, 2020.

[21] C. Baskin, N. Liss, E. Schwartz, E. Zheltonozhskii, R. Giryes, A. M.
Bronstein, and A. Mendelson, “Uniq: Uniform noise injection for
non-uniform quantization of neural networks,” ACM Transactions on
Computer Systems (TOCS), vol. 37, no. 1-4, pp. 1–15, 2021.

[22] S. Deligiannidis, C. Mesaritakis, and A. Bogris, “Performance and
complexity analysis of bi-directional recurrent neural network models
versus volterra nonlinear equalizers in digital coherent systems,” Journal
of Lightwave Technology, vol. 39, no. 18, pp. 5791–5798, 2021.
[23] O. Sidelnikov, A. Redyuk, and S. Sygletos, “Equalization performance
and complexity analysis of dynamic deep neural networks in long haul
transmission systems,” Optics Express, vol. 26, no. 25, pp. 32 765–
32 776, 2018.

[24] P. J. Freire, Y. Osadchuk, B. Spinnler, A. Napoli, W. Schairer, N. Costa,
J. E. Prilepsky, and S. K. Turitsyn, “Performance versus complexity
study of neural network equalizers in coherent optical systems,” Journal
of Lightwave Technology, vol. 39, no. 19, pp. 6085–6096, 2021.
[25] W. Maass, “On the computational complexity of networks of spiking
neurons,” Advances in neural information processing systems, vol. 7,
1994.

12

[26] R. Alizadeh, J. K. Allen, and F. Mistree, “Managing computational
complexity using surrogate models: a critical review,” Research in
Engineering Design, vol. 31, no. 3, pp. 275–298, 2020.

[27] S. Wiedemann, K.-R. M¨uller, and W. Samek, “Compact and com-
putationally efﬁcient representation of deep neural networks,” IEEE
transactions on neural networks and learning systems, vol. 31, no. 3,
pp. 772–785, 2019.

[28] A. M. Amin, R. R. Mahmood, and A. I. Khan, “Analysis of pattern
recognition algorithms using associative memory approach: a compar-
ative study between the hopﬁeld network and distributed hierarchical
graph neuron (dhgn),” in 2008 IEEE 8th International Conference on
Computer and Information Technology Workshops.
IEEE, 2008, pp.
153–158.

[29] S. N. Kerr, “A big-o experiment: which function is it?” in Proceedings
of the 43rd annual Southeast regional conference-Volume 1, 2005, pp.
317–318.

[30] V. D. Blondel and J. N. Tsitsiklis, “A survey of computational com-
plexity results in systems and control,” Automatica, vol. 36, no. 9, pp.
1249–1274, 2000.

[31] E. Jacobsen and P. Kootsookos, “Fast, accurate frequency estimators
[dsp tips & tricks],” IEEE Signal Processing Magazine, vol. 24, no. 3,
pp. 123–125, 2007.

[32] B. Spinnler, “Equalizer design and complexity for digital coherent
receivers,” IEEE Journal of Selected Topics in Quantum Electronics,
vol. 16, no. 5, pp. 1180–1192, 2010.

[33] S. Mirzaei, A. Hosangadi, and R. Kastner, “Fpga implementation of
high speed ﬁr ﬁlters using add and shift method,” in 2006 International
Conference on Computer Design.

IEEE, 2006, pp. 308–313.

[34] S. Jahani, “Zot-mk: a new algorithm for big integer multiplication,”
MSc MSc, Department of Computer Science, Universiti Sains Malaysia,
Penang, 2009.

[35] B. Hawks, J. Duarte, N. J. Fraser, A. Pappalardo, N. Tran, and
Y. Umuroglu, “Ps and qs: Quantization-aware pruning for efﬁcient low
latency neural network inference,” arXiv preprint arXiv:2102.11289,
2021.

[36] J. Wu, Y. Wang, Z. Wu, Z. Wang, A. Veeraraghavan, and Y. Lin, “Deep
k-means: Re-training and parameter sharing with harder cluster assign-
ments for compressing deep convolutions,” in International Conference
on Machine Learning. PMLR, 2018, pp. 5363–5372.

[37] X. Staff, “Gate count capacity metrics for fpgas,” Xilinx Corp., San Jose,

CA, Application Note XAPP, vol. 59, 1997.

[38] Y. Li, X. Dong, and W. Wang, “Additive powers-of-two quantization: An
efﬁcient non-uniform discretization for neural networks,” arXiv preprint
arXiv:1909.13144, 2019.

[39] T. Koike-Akino, Y. Wang, K. Kojima, K. Parsons, and T. Yoshida,
“Zero-multiplier sparse dnn equalization for ﬁber-optic qam systems
with probabilistic amplitude shaping,” in 2021 European Conference
on Optical Communications (ECOC).

IEEE, 2021, pp. 1–4.

[40] M. Elhoushi, Z. Chen, F. Shaﬁq, Y. H. Tian, and J. Y. Li, “Deepshift:
Towards multiplication-less neural networks,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 2359–2368.

[41] H. You, X. Chen, Y. Zhang, C. Li, S. Li, Z. Liu, Z. Wang, and
Y. Lin, “Shiftaddnet: A hardware-inspired deep network,” arXiv preprint
arXiv:2010.12785, 2020.

[42] P. Gentili, F. Piazza, and A. Uncini, “Efﬁcient genetic algorithm design
for power-of-two ﬁr ﬁlters,” in 1995 International conference on acous-
tics, speech, and signal processing, vol. 2.
IEEE, 1995, pp. 1268–1271.
[43] J. B. Evans, “Efﬁcient ﬁr ﬁlter architectures suitable for fpga imple-
mentation,” IEEE Transactions on Circuits and Systems II: Analog and
Digital Signal Processing, vol. 41, no. 7, pp. 490–493, 1994.

[44] W. R. Lee, V. Rehbock, K. L. Teo, and L. Caccetta, “Frequency-
response masking based ﬁr ﬁlter design with power-of-two coefﬁcients
and suboptimum pwr,” Journal of Circuits, Systems, and Computers,
vol. 12, no. 05, pp. 591–599, 2003.

[45] P. Kurup and T. Abbasi, Logic synthesis using Synopsys®.

Springer

Science & Business Media, 2012.

[46] H. Li and W. Ye, “Efﬁcient implementation of fpga based on vivado
high level synthesis,” in 2016 2nd IEEE International Conference on
Computer and Communications (ICCC), 2016, pp. 2810–2813.

[47] X. Inc., “Ultrascale architecture dsp slice,” 2021. [Online]. Available:

https://docs.xilinx.com/v/u/en-US/ug579-ultrascale-dsp

[48] N. Tran, B. Hawks, J. M. Duarte, N. J. Fraser, A. Pappalardo, and
Y. Umuroglu, “Ps and qs: Quantization-aware pruning for efﬁcient low
latency neural network inference,” Frontiers in Artiﬁcial Intelligence,
vol. 4, p. 94, 2021.

13

[49] S. V. Padmajarani and M. Muralidhar, “Fpga implementation of multi-
plier using shift and add technique,” International Journal of Advances
in Electronics and Computer Science-IJAECS, vol. 2, no. 9, pp. 1–5,
2015.

[50] V. S. Dimitrov, K. U. Jarvinen, and J. Adikari, “Area-efﬁcient multi-
pliers based on multiple-radix representations,” IEEE Transactions on
Computers, vol. 60, no. 2, pp. 189–201, 2010.

[51] R. I. Hartley, “Subexpression sharing in ﬁlters using canonic signed digit
multipliers,” IEEE Transactions on Circuits and Systems II: Analog and
Digital Signal Processing, vol. 43, no. 10, pp. 677–688, 1996.

[52] V. Dimitrov, L. Imbert, and A. Zakaluzny, “Multiplication by a con-
stant is sublinear,” in 18th IEEE Symposium on Computer Arithmetic
(ARITH’07).

IEEE, 2007, pp. 261–268.

[53] D. Przewlocka-Rus, S. S. Sarwar, H. E. Sumbul, Y. Li, and B. De Salvo,
“Power-of-two quantization for low bitwidth and hardware compliant
neural networks,” arXiv preprint arXiv:2203.05025, 2022.

[54] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependen-
cies with gradient descent is difﬁcult,” IEEE Transactions on Neural
Networks, vol. 5, no. 2, pp. 157–166, 1994.

[55] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of
recurrent neural networks for sequence learning,” arXiv preprint
arXiv:1506.00019, 2015.

[56] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[57] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget:
Continual prediction with lstm,” Neural computation, vol. 12, no. 10,
pp. 2451–2471, 2000.

[58] R. Dey and F. M. Salem, “Gate-variants of gated recurrent unit (gru)
neural networks,” in 2017 IEEE 60th international midwest symposium
on circuits and systems (MWSCAS).

IEEE, 2017, pp. 1597–1600.

[59] Q. Wu, E. Fokoue, and D. Kudithipudi, “On the statistical challenges
of echo state networks and some potential remedies,” arXiv preprint
arXiv:1802.07369, 2018.

[60] M. Sorokina, S. Sergeyev, and S. Turitsyn, “Fiber echo state network

analogue for high-bandwidth dual-quadrature signal processing,” Optics
express, vol. 27, no. 3, pp. 2387–2395, 2019.

[61] S. S. Mosleh, L. Liu, C. Sahin, Y. R. Zheng, and Y. Yi, “Brain-
inspired wireless communications: Where reservoir computing meets
mimo-ofdm,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 29, no. 10, pp. 4694–4708, 2018.

[62] C. Sun, M. Song, S. Hong, and H. Li, “A review of designs and
applications of echo state networks,” arXiv preprint arXiv:2012.02974,
2020.

[63] H. Jaeger, M. Lukoˇseviˇcius, D. Popovici, and U. Siewert, “Optimization
and applications of echo state networks with leaky-integrator neurons,”
Neural networks, vol. 20, no. 3, pp. 335–352, 2007.

[64] E. L´opez, C. Valle, H. Allende-Cid, and H. Allende, “Comparison
of recurrent neural networks for wind power forecasting,” in Mexican
Conference on Pattern Recognition. Springer, 2020, pp. 25–34.
[65] H. Yu, H. Li, H. Shi, T. S. Huang, G. Hua et al., “Any-precision deep
neural networks,” arXiv preprint arXiv:1911.07346, vol. 1, 2019.
[66] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, “Scalable methods
for 8-bit training of neural networks,” Advances in neural information
processing systems, vol. 31, 2018.

[67] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Haq: Hardware-aware
automated quantization with mixed precision,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 8612–8620.

[68] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry, “Accurate
post training quantization with small calibration sets,” in International
Conference on Machine Learning. PMLR, 2021, pp. 4466–4475.
[69] M. Marchesi, G. Orlandi, F. Piazza, and A. Uncini, “Fast neural networks
without multipliers,” IEEE transactions on Neural Networks, vol. 4,
no. 1, pp. 53–62, 1993.

[70] S.-E. Chang, Y. Li, M. Sun, W. Jiang, R. Shi, X. Lin, and Y. Wang, “Msp:
an fpga-speciﬁc mixed-scheme, multi-precision deep neural network
quantization framework,” arXiv preprint arXiv:2009.07460, 2020.

