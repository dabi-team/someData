2
2
0
2

y
a
M
7
1

]
E
S
.
s
c
[

1
v
9
8
5
8
0
.
5
0
2
2
:
v
i
X
r
a

Hierarchical Distribution-Aware Testing of Deep Learning

WEI HUANG, University of Liverpool, U.K.
XINGYU ZHAO, University of Liverpool, U.K.
ALEC BANKS, Defence Science and Technology Laboratory, U.K.
VICTORIA COX, Defence Science and Technology Laboratory, U.K.
XIAOWEI HUANG, University of Liverpool, U.K.

With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns
regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Despite recent
efforts made in detecting Adversarial Examples (AEs) via state-of-the-art attacking and testing methods, they
are normally input distribution agnostic and/or disregard the perception quality of AEs. Consequently, the
detected AEs are irrelevant inputs in the application context or unnatural/unrealistic that can be easily noticed
by humans. This may lead to a limited effect on improving the DL modelâ€™s dependability, as the testing budget
is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations.

In this paper, we propose a new robustness testing approach for detecting AEs that considers both the input
distribution and the perceptual quality of inputs. The two considerations are encoded by a novel hierarchical
mechanism. First, at the feature level, the input data distribution is extracted and approximated by data
compression techniques and probability density estimators. Such quantified feature level distribution, together
with indicators that are highly correlated with local robustness, are considered in selecting test seeds. Given
a test seed, we then develop a two-step genetic algorithm for local test case generation at the pixel level,
in which two fitness functions work alternatively to control the quality of detected AEs. Finally, extensive
experiments confirm that our holistic approach considering hierarchical distributions at feature and pixel
levels is superior to state-of-the-arts that either disregard any input distribution or only consider a single
(non-hierarchical) distribution, in terms of not only the quality of detected AEs but also improving the overall
robustness of the DL model under testing.

CCS Concepts: â€¢ Software and its engineering â†’ Software testing and debugging; Software reliability;
â€¢ Computing methodologies â†’ Machine learning.
Additional Key Words and Phrases: Deep learning robustness, adversarial examples detection, natural pertur-
bations, distribution-aware testing, robustness growth, safe AI

ACM Reference Format:
Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang. 2022. Hierarchical Distribution-Aware
Testing of Deep Learning. 1, 1 (May 2022), 26 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Deep Learning (DL) is being explored to provide transformational capabilities to many industrial
sectors including automotive, healthcare and finance. The reality that DL is not as dependable as
required now becomes a major impediment. For instance, key industrial foresight reviews identified

Authorsâ€™ addresses: Wei Huang, University of Liverpool, U.K., w.huang23@liverpool.ac.uk; Xingyu Zhao, University of
Liverpool, U.K., Xingyu.Zhao@liverpool.ac.uk; Alec Banks, Defence Science and Technology Laboratory, U.K., abanks@dstl.
gov.uk; Victoria Cox, Defence Science and Technology Laboratory, U.K., vcox@dstl.gov.uk; Xiaowei Huang, University of
Liverpool, U.K., Xiaowei.Huang@liverpool.ac.uk.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/5-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: May 2022.

 
 
 
 
 
 
2

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

that the biggest obstacle to gaining benefits of DL is its dependability [18]. There is an urgent need
to develop methods to enable the dependable use of DL, for which great efforts have been made in
recent years in the field of DL Verification and Validation (V&V) [15, 44].

DL robustness is arguably the property in the limelight. Informally, robustness requires that
the decision of the DL model is invariant against small perturbations on inputs. That is, all inputs
in a small input region (e.g., a norm ball defined in some ğ¿ğ‘ -norm distance) should share the same
prediction label by the DL model. Inside that region, if an input is predicted differently to the
given label, then this input is normally called an Adversarial Example (AE). Most V&V methods
designed for DL robustness are essentially about detecting AEs, e.g., adversarial attack based
methods [10, 22] and coverage-guided testing [8, 14, 21, 23, 28, 39].

As recently noticed by the software engineering community, emerging studies on systematically
evaluating AEs detected by aforementioned state-of-the-arts have two major drawbacks: (i) they
do not take the input data distribution into consideration, therefore it is hard to judge whether
the identified AEs are meaningful to the DL application [3, 7]; (ii) most detected AEs are of poor
perception quality that are too unnatural/unrealistic [11] to be seen in real-life operations. That
said, not all AEs are equal nor can be eliminated given limited resources. A wise strategy is to
detect those AEs that are both being â€œdistribution-awareâ€ and with natural/realistic pixel-level
perturbations, which motivates this work.

Prior to this work, a few decent attempts at distribution-aware testing for DL have been made.
Broadly speaking, the field has developed two types of approaches: Out-Of-Distribution (OOD)
detector based [2, 7] and feature-only based [5, 29]. The former can only detect anomalies/outliers,
rather than being â€œfully-awareâ€ of the distribution. While the latter is indeed generating new test
cases according to the learnt distribution (in a latent space), it ignores the pixel-level information due
to the compression nature of generative models used [49]. To this end, our approach is advancing
in this direction with the following novelties and contributions:

a) We provide a â€œdivide and conquerâ€ solutionâ€”Hierarchical Distribution-Aware (HDA) testingâ€”
by decomposing the input distribution into two levels (named as global and local) capturing how the
feature-wise and pixel-wise information are distributed, respectively. At the global level, isolated
problems of estimating the feature distribution and selecting best test seeds can be solved by
dedicated techniques. At the local level where features are fixed, the clear objective is to precisely
generate test cases considering perceptual quality. Our extensive experiments show that such
hierarchical consideration is more effective to detect high-quality AEs than state-of-the-art that
either disregards any data distribution or only considers a single (non-hierarchical) distribution.
Consequently, we also show the DL model under testing exhibits higher robustness after â€œfixingâ€
the high-quality AEs detected.

b) At the global level, we propose novel methods to select test seeds based on the approximated
feature distribution of the training data and predictive robustness indicators, so that the norm balls of
the selected seeds are both from the high-density area of the distribution and relatively unrobust
(thus more cost-effective to detect AEs in later stages). Notably, state-of-the-art DL testing methods
normally select test seeds randomly from the training dataset without any principled rules. Thus,
from a software engineering perspective, our test seed selection is more practically useful in the
given application context.

c) Given a carefully selected test seed, we propose a novel two-step Genetic Algorithm (GA) to
generate test cases locally (i.e. within a norm ball) to control the perceptual quality of detected AEs.
At this local level, the perceptual quality distribution of data-points inside a norm ball requires
pixel-level information that cannot be sufficiently obtained from the training data alone. Thus, we
innovatively use common perceptual metrics that quantify image quality as an approximation of
such local distribution. Our experiments confirm that the proposed GA is not only effective after

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

3

being integrated into HDA (as a holistic testing framework), but also outperforms other pixel level
AE detectors in terms of perception quality when applied separately.

d) We investigate black-box (to the DL model under testing) methods for the main tasks at both
levels. Thus, to the best of our knowledge, our HDA approach provides an end-to-end, black-box
solution, which is the first of its kind and more versatile in software engineering practice.

e) A publicly accessible tool of our HDA testing framework with all source code, datasets, DL

models and experimental results.

2 PRELIMINARIES AND RELATED WORK
In this section, we first introduce preliminaries and related work on DL robustness, together with
formal definitions of concepts adopted in our HDA approach. Then existing works on distribution-
aware testing are discussed. Since our HDA testing also considers the naturalness of detected AEs,
some common perception quality metrics are introduced. In summary, we present Fig. 1 to show
the stark contrast of our proposed HDA testing (the green route) to other related works (the red
and amber routes).

Fig. 1. Comparison between our proposed Hierarchical Distribution-Aware (HDA) testing and related works.

2.1 DL Robustness and Adversarial Examples
We denote the prediction output of DL model as the vector ğ‘“ (ğ‘¥) with size equal to the total number
of labels. The predicated label Ë†ğ‘“ (ğ‘¥) = arg maxğ‘– ğ‘“ğ‘– (ğ‘¥) where ğ‘“ğ‘– (ğ‘¥) is the ğ‘–ğ‘¡â„ attribute of vector ğ‘“ (ğ‘¥).
DL robustness requires that the decision of the DL model Ë†ğ‘“ (ğ‘¥) is invariant against small pertur-
bations on input ğ‘¥. That is, all inputs in an input region ğœ‚ have the same prediction label, where ğœ‚ is

, Vol. 1, No. 1, Article . Publication date: May 2022.

4

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

usually a small norm ball (defined with some ğ¿ğ‘ -norm distance1) around an input ğ‘¥. If an input ğ‘¥ â€²
inside ğœ‚ is predicted differently to ğ‘¥ by the DL model, then ğ‘¥ â€² is called an Adversarial Example (AE).
DL robustness V&V can be based on formal methods [16, 26] or statistical approaches [35, 37],
and normally aims at detecting AEs. In general, we may classify two types of methods (the two
branches in the red route of Fig. 1) depends on how the test cases are generated: (i) Adversarial
attack based methods are normally optimised for the DL prediction loss to find AEs, which include
white-box attack methods like Fast Gradient Sign Method (FGSM) [10] and Projected Gradient
Descent (PGD) [22], as well as black-box attacks [1, 38] using GA with gradient-free optimisation.
(ii) Coverage-guided testing are optimised for certain coverage metrics on the DL modelâ€™s internal
structure, which is inspired by the coverage testing for traditional software. Several popular test
metrics, like neuron coverage [21, 23], modified condition/decision coverage [28] for CNNs and
temporal coverage [8, 14] for RNNs are proposed. While it is argued that coverage metrics are
not strongly correlated with DL robustness [11, 40], they are seen as providing insights into the
internal behaviours of DL models and hence may guide test selection to find more diverse AEs [14].
Without loss of generality, we reuse the formal definition of DL robustness in [35, 36] in this

work:

Definition 1 (Local Robustness). The local robustness of the DL model ğ‘“ (ğ‘¥), w.r.t. a local region

ğœ‚ and a target label ğ‘¦, is:

Rğ‘™ (ğœ‚, ğ‘¦) :=

âˆ«

ğ‘¥ âˆˆğœ‚

ğ¼ (ğ‘¥)ğ‘ğ‘™ (ğ‘¥ | ğ‘¥ âˆˆ ğœ‚) dğ‘¥

(1)

where ğ‘ğ‘™ (ğ‘¥ | ğ‘¥ âˆˆ ğœ‚) is the local distribution of region ğœ‚ which is precisely the â€œinput modelâ€ used by
both [35, 36]. ğ¼ (ğ‘¥) is an indicator function, and ğ¼ (ğ‘¥) = 1 when

Ë†ğ‘“ (ğ‘¥) = ğ‘¦, ğ¼ (ğ‘¥) = 0 otherwise.

To detect as many AEs as possible, normally the first question isâ€”which local region shall we
search for those AEs? I.e. how to select test seeds? To be cost-effective, we want to explore unrobust
regions, rather than regions where AEs are relatively rare. This requires the local robustness of a
region to be known a priori, which may imply a paradox (cf. Remark 3 later). In this regard, we can
only predict the local robustness of some regions before doing the actual testing in those regions.
We define:

Definition 2 (Local Robustness Indicator). Auxiliary information that strongly corre-
lated with Rğ‘™ (ğœ‚, ğ‘¦) (thus can be leveraged in its prediction) is named as a local robustness indicator.

We later seek for such indicators (and empirically show their correlation with the local robustness),
which forms one of the two key factors considered in selecting test seeds in our method.

Given a test seed, we search for its AEs in a local region ğœ‚ that with different labels. This involves

the question on what size of ğœ‚ should be, for which we later utilise the property of:

Remark 1 (ğ‘Ÿ -separation). For real-world image datasets, any data-points with different ground
truth labels are at least distance 2ğ‘Ÿ apart in the input (pixel) space, with ğ‘Ÿ being estimated case by case
depends on the dataset.

The ğ‘Ÿ -separation property was first observed by [41]: intuitively it says, there is a minimum distance
between two real-world objects of different labels.

Finally, not all AEs are equal in terms of the â€œstrength of being adversarialâ€ (stronger AEs may

lead to greater robustness improvement in, e.g., adversarial training [33]), for which we define:

1ğ‘ = 0, 1, 2 and âˆ. ğ¿âˆ norm is more commonly used.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

5

Definition 3 (Prediction Loss). Given a test seed ğ‘¥ with label ğ‘¦, the prediction loss of an input

ğ‘¥ â€² to the test seed is defined as:

J (ğ‘“ (ğ‘¥ â€²), ğ‘¦) = max
ğ‘–â‰ ğ‘¦

(ğ‘“ğ‘– (ğ‘¥ â€²) âˆ’ ğ‘“ğ‘¦ (ğ‘¥ â€²))

(2)

where ğ‘“ğ‘– (ğ‘¥ â€²) returns the probability of label ğ‘– after input ğ‘¥ â€² being processed by the DL model ğ‘“ .
Note, J â‰¥ 0 implies arg maxğ‘– ğ‘“ğ‘– (ğ‘¥) â‰  ğ‘¦ and thus ğ‘¥ â€² is an AE of ğ‘¥.

Next, to measure the DL modelsâ€™ overall robustness across the whole input domain, we introduce
a notion of global robustness. Being different to some existing definitions where local robustness
are treated equally [30, 31], ours is essentially a â€œweighted sumâ€ of the local robustness of local
regions where each weight is the probability of the associated region on the input data distribution.
Defining global robustness in such a â€œdistribution-awareâ€ manner aligns with our motivationâ€”as
revealed later by empirically estimated global robustness, our HDA appears to be more effective in
supporting the growth of the overall robustness after â€œfixingâ€ those distribution-aware AEs.

Definition 4 (Global Robustness). The global robustness of the DL model ğ‘“ (ğ‘¥) is defined as:
(3)

ğ‘ğ‘” (ğ‘¥ âˆˆ ğœ‚)Rğ‘™ (ğœ‚, ğ‘¦)

Rğ‘” :=

âˆ‘ï¸

ğœ‚ âˆˆX

where ğ‘ğ‘” (ğ‘¥ | ğ‘¥ âˆˆ ğœ‚) is the global distribution of region ğœ‚ (i.e., a pooled probability of all inputs in the
region ğœ‚ğ‘§) and Rğ‘™ (ğœ‚, ğ‘¦) is the local robustness of region ğœ‚ to the label ğ‘¦.

The estimation of Rğ‘”, unfortunately, is very expensive that requires to compute the local robust-
ness Rğ‘™ of a large number of regions. Thus, from a practical standpoint, we adopt an empirical
definition of the global robustness in our later experiments, which has been commonly used for DL
robustness evaluation in the adversarial training [22, 32, 33, 43].

Definition 5 (Empirical Global Robustness). Given a DL model ğ‘“ and a validation dataset
ğ·ğ‘£, we define the empirical global robustness as Ë†Rğ‘” : (ğ‘“ , ğ·ğ‘£,ğ‘‡ ) â†’ [0, 1] where T denotes a given type
of AE detection method and Ë†Rğ‘” is the weighted accuracy on AEs obtained by conducting T on âŸ¨ğ‘“ , ğ·ğ‘£âŸ©.
To be â€œdistribution-awareâ€, the synthesis of ğ·ğ‘£ should conform to the global distribution while
locally AEs are searched by ğ‘‡ according to the local distribution. Consequently, the set of AEs may
represent the input distribution.

2.2 Distribution-Aware Testing for DL
There are increasing amount of DL testing works developed towards being distribution-aware
(as summarised in the amber route of Fig. 1). Deep generative models, such as Variational Auto-
Encoders (VAE) and Generative Adversarial Networks (GAN), are applied to approximate the
training data distribution, since the inputs (like images) to Deep Neural Network (DNN) are usually
in a high dimensional space. Previous works heavily rely on OOD detection [2, 7] or synthesising
new test cases directly from latent spaces [4, 5, 24, 29]. The former not really considers the whole
distribution, rather flags outliers, thus a more pertinent name of it should be out-of-distribution-
aware (OODA) testing. While for both types of methods, another problem arises that the distribution
encoded by generative models only contain the feature-wise information and filter out the pixel-wise
perturbations [49]. Consequently, directly searching and generating test cases from the latent space
of generative models may only perturb features, thus called Feature-Only Distribution-Aware (FODA)
in this paper (while also named as semantic AEs in some literature [13, 48]). Our approach, the
green route in Fig. 1, differs from aforementioned works by considering both the global (feature
level) distribution in latent spaces and the local (pixel level) perceptual quality distribution in the
input space.

, Vol. 1, No. 1, Article . Publication date: May 2022.

6

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

2.3 Perception Quality of Images
Locally, data-points (around the selected seed) sharing the same feature information may exhibit
differently in terms of naturalness. To capture such distribution, some perceptual quality metric
can be utilised to compare the perceptual difference between the original images and perturbed
images. Some common metrics for perceptual quality include:

â€¢ Mean Square Error (MSE) between the original image and perturbed image.
â€¢ Peak Signal-to-Noise Ratio (PSNR) [9] defined as 20âˆ—ğ‘™ğ‘œğ‘”10

, where ğ‘€ğ´ğ‘‹ is the maximum

ğ‘€ğ´ğ‘‹
âˆš
ğ‘€ğ‘†ğ¸

possible pixel value of the image.

â€¢ Structural Similarity Index Measure (SSIM) [34] that considers image degradation as perceived

change in structural information.

â€¢ FrÃ©chet Inception Distance (FID) [12] that compares the distribution between a set of original

images and a set of perturbed images by squared Wasserstein metric.

Notably, all these metrics are current standards for assessing the quality of images, as widely

used in the experiments of aforementioned related works.

3 THE PROPOSED METHOD

Fig. 2. An example of Hierarchical Distribution Aware Testing

We first present an overview of our HDA testing, cf. the green route in Fig. 1, and then dive into

details of how we implement each stage by referring to an illustrative example in Fig. 2.

3.1 Overview of HDA Testing
The core of HDA testing is the hierarchical structure of two distributions. We formally define the
following two levels of distributions:

Definition 6 (Global Distribution). The global distribution captures how feature level infor-

mation is distributed in some (low-dimensional) latent space after data compression.

Definition 7 (Local Distribution). Given a data-point sampled from the latent space, we
consider its norm ball in the input pixel space. The local distribution is a conditional distribution
capturing the perceptual quality of all data-points within the norm ball.

Latent space is the representation of compressed data, in which data points with similar features
are closer to each other. DNNs, e.g. encoder of VAEs, map any data points in the high dimensional
input space to the low dimensional latent space. It infers that input space can be divided into
distinct regions, and each region corresponds to a data point in latent space. By fitting a global
distribution in the latent space, we actually model the distribution of distinct regions over the input

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

7

space. The local distribution is defined as a conditional distribution within each region, sharing the
same features. Thus, we propose the following remark.

Remark 2 (Decompose one distribution into two levels). Given the definitions of global and
local distributions, denoted as ğ‘ğ‘” and ğ‘ğ‘™ respectively, we may decompose a single distribution over the
entire input domain X as:

âˆ«

ğ‘ (ğ‘¥) =

ğ‘ğ‘™ (ğ‘¥ |ğ‘¥ âˆˆ ğœ‚ğ‘§)ğ‘ğ‘” (ğ‘¥ âˆˆ ğœ‚ğ‘§) dğ‘§

(4)

where variable ğ‘§ represents a set of features while ğœ‚ğ‘§ represents a region in the input space that â€œmapsâ€
to the ğ‘§ point in the latent space.

Intuitively, compared to modelling a single distribution, our hierarchical structure of distributions
is superior in that the global distribution guides for which regions of the input space to test, while
the local distribution can be leveraged to precisely control the perceptual quality of test cases. The
green route in Fig. 1 shows our HDA testing process, which appears as three stages:

Stage 1: Explicitly Approximate the Global Distribution. We first extract feature-level information
from the given dataset by using data compression techniquesâ€”the encoder of VAEs in our case,
and then explicitly approximate the global distribution in the latent-feature space, using Kernel
Density Estimator (KDE).

Stage 2: Select Test Seeds Based on the Global Distribution and Local Robustness Indicators. Given the
limited testing budget, we want to test in those local input regions that are both more error-prone
and representative of the input distribution. Thus, when selecting test seeds, we consider two
factorsâ€”the local robustness indicators (cf. Definition 2) and the global distribution. For the former,
we propose several auxiliary information with empirical studies showing their correlation with the
local robustness, while the latter has already been quantified in the first stage via KDE.

Stage 3: Generate Test Cases Around Test Seeds Considering the Local Distribution and Prediction
Loss of AEs. When searching for AEs locally around a test seed given by the 2nd stage, we develop
a two-step GA in which the objective function is defined as a fusion of the prediction loss (cf.
Definition 3) and the local distribution (modelled by common perceptual quality metrics). Such
fusion of two fitness functions allows the trade-off between the â€œstrength of being adversarialâ€ and
the perceptual quality of the detected AEs. The optimisation is subject to the constraint of only
exploring in a norm ball whose central point is the test seed and with a radius smaller than the
ğ‘Ÿ -separation distance (cf. Remark 1).

While our chosen technical solutions are effective and popular, alternatives may also suffice for

the purpose of each stage.

3.2 Approximation of the Global Distribution
Given the training dataset D, the task of approximating the input distribution is equivalent to
estimating a Probability Density Function (PDF) over the input domain X given D. Despite this
is a common problem with many established solutions, it is hard to accurately approximate the
distribution due to the relatively sparse data of D, compared to the high dimensionality of the
input domain X. So the practical solution is to do dimensionality reduction and then estimate the
global distribution, which indeed is the first step of all existing methods of distribution-aware DL
testing.

, Vol. 1, No. 1, Article . Publication date: May 2022.

8

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

Specifically, we choose VAE-Encoder+KDE2 for their simplicity and popularity. Assume D
contains ğ‘› samples and each ğ‘¥ğ‘– âˆˆ D is encoded by VAE-Encoder as a Gaussian distribution ğ‘§ğ‘– in
the latent space, we can estimate the PDF of ğ‘§ (denoted as ğ‘ƒğ‘Ÿ (ğ‘§)) based on the encoded D. The
ğ‘ƒğ‘Ÿ (ğ‘§) conforms to the mixture of Gaussian distributions, i.e., ğ‘§ âˆ¼ N (ğœ‡ğ‘§ğ‘– , ğœğ‘§ğ‘– ). Notably, this mixture
of Gaussian distributions nicely aligns with the gist of adaptive KDE [20], which uses the following
estimator:

ğ‘ğ‘” (ğ‘¥ âˆˆ ğœ‚ğ‘§) âˆ ğ‘ƒğ‘Ÿ (ğ‘§) â‰ƒ

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ¾â„ğ‘– (ğ‘§ âˆ’ ğœ‡ğ‘§ğ‘– )

(5)

That is, when choosing a Gaussian kernel for ğ¾ in Eqn. (5) and adaptively setting the bandwidth
parameter â„ğ‘– = ğœğ‘§ğ‘– (i.e., the standard deviation of the Gaussian distribution representing the
compressed sample ğ‘§ğ‘– ), the VAE-Encoder and KDE are combined â€œseamlesslyâ€. Finally, our global
distribution ğ‘ğ‘” (ğ‘¥ âˆˆ ğœ‚ğ‘§) (a pooled probability of all inputs in the region ğœ‚ğ‘§ that corresponds to a
point ğ‘§ in the latent space) is proportional to the approximated distribution of ğ‘§ with the PDF ğ‘ƒğ‘Ÿ (ğ‘§).
Running Example: The left diagram in Fig. 2 depicts the global distribution learnt by KDE,
after projected to a two-dimensional space for visualisation. The peaks3 are evaluated with highest
probability density over the latent space by KDE.

3.3 Test Seeds Selection
Selecting test seeds is actually about choosing which norm balls (around the test seeds) to test for
AEs. To be cost-effective, we want to test those with higher global probabilities and lower local
robustness at the same time. For the latter requirement, there is potentially a paradox:

Remark 3 (A Paradox of Selecting Unrobust Norm Balls). To be informative on which norm
balls to test for AEs, we need to estimate the local robustness of candidate norm balls (by invoking
robustness estimators to quantify Rğ‘™ (ğœ‚, ğ‘¦), e.g., [35]). However, local robustness evaluation itself is
usually about sampling for AEs (then fed into statistical estimators) that consumes the testing resources.

To this end, instead of directly evaluating the local robustness of a norm ball, we can only
indirectly predict it (i.e., without testing/searching for AEs) via auxiliary information that we call
local robustness indicators (cf. Definition 2). In doing so, we save all the testing budget for the later
stage when generating local test cases.

Given a test seed ğ‘¥ with label ğ‘¦, we propose two robustness indicators (both relate to the
vulnerability of the test seed to adversarial attacks)â€”the prediction gradient based score (denoted as
ğ‘†grad) and the score based on separation distance of the output-layer activation (denoted as ğ‘†sep):

ğ‘†grad = ||âˆ‡ğ‘¥ J (ğ‘“ (ğ‘¥), ğ‘¦)||âˆ
||ğ‘“ (ğ‘¥) âˆ’ ğ‘“ ( Ë†ğ‘¥)||âˆ
ğ‘†sep = min
Ë†ğ‘¥

s.t. ğ‘¦ â‰  Ë†ğ‘¦

(6)

These allow prediction of a whole norm ballâ€™s local robustness by the limited information of
its central point (the test seed). The gradient of a DNNâ€™s prediction with respect to the input is a
white-box metric, that widely used in adversarial attacks, such as FGSM [10] and PGD [22] attacks.
A greater gradient calculated at a test seed implies that AEs are more likely to be found around it.
The activation separation distance is regarded as a black-box metric and refers to the minimum
ğ¿âˆ norm between the output activations of the test seed and any other data with different labels.
Intuitively, a smaller separation distance implies a greater vulnerability of the seed to adversarial

2We only use the encoder of VAEs for feature extraction, rather than generate new data from the decoder, which is different
to other methods mentioned in Section 2.2.
3Most training data lie in this region or gather around the region.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

9

attacks. We later show empirically that indeed these two indicators are highly correlated with the
local robustness.

After quantifying the two required factors, we combine them in a way that was inspired by [46].
In [46], the DL reliability metric is formalised as a weighted sum of local robustness where the
weights are operational probabilities of local regions. To align with that reliability metric, we do
the following steps to select test seeds:

(i) For each data-point ğ‘¥ğ‘– in the test set, we calculate its global probability (i.e., ğ‘ƒğ‘Ÿ (ğ‘§ğ‘– ) where ğ‘§ğ‘– is
its compressed point in the VAE latent space) and one of the local robustness indicators (either
white-box or black-box, depending on the available information).

(ii) Normalise both quantities to a same scale.
(iii) Rank all data-points by the product of their global probability and local robustness indicator.
(iv) Finally we select top-ğ‘˜ data-points as our test seeds, and ğ‘˜ depends on the testing budget.
Running Example: In the middle diagram of Fig. 2, we add in the local robustness indicator
results of the training data which are represented by a scale of coloursâ€”darker means lower
predicted local robustness while lighter means higher predicated local robustness. By our method,
test seeds selected are both from the highest peak (high probability density area of the global
distribution) and relatively darker ones (lower predicated local robustness).

3.4 Local Test Cases Generation
Not all AEs are equal in terms of the â€œstrength of being adversarialâ€, and stronger AEs are associated
with higher prediction loss (cf. Definition 3). Detecting AEs with higher prediction loss may benefit
more when considering the future â€œdebugingâ€ step, e.g., by adversarial retraining [33]. Thus, at
this stage, we want to search for AEs that are both â€œstrongly being adversarialâ€ and â€œless likely to
be noticed by humans". That is, the local test case generation can be formulated as the following
optimisation given a seed (ğ‘¥, ğ‘¦):

J (ğ‘“ (ğ‘¥ â€²), ğ‘¦) + ğ›¼ Â· ğ‘ğ‘™ (ğ‘¥ â€²|ğ‘¥ â€² âˆˆ ğœ‚ğ‘§ğ‘¥ )

max
ğ‘¥ â€²
s.t. ||ğ‘¥ âˆ’ ğ‘¥ â€²||âˆ â‰¤ ğ‘Ÿ

(7)

where J is the prediction loss, ğ‘ğ‘™ (ğ‘¥ â€²|ğ‘¥ â€² âˆˆ ğœ‚ğ‘§ğ‘¥ ) is the local distribution (note, ğ‘§ğ‘¥ represents the
latent features of test seed ğ‘¥), ğ‘Ÿ is the ğ‘Ÿ -separation distance, and ğ›¼ is a coefficient to balance the
two terms. As what follows, we note two points on Eqn. (7): why we need the constraint and how
we quantify the local distribution.

The constraint in Eqn. (7) determines the right locality of local robustnessâ€”the â€œneighboursâ€ that
should have the same ground truth label ğ‘¦ as the test seed. We notice the ğ‘Ÿ -separation property
of real-world image datasets (cf. Remark 1) provides a sound basis to the question. Thus, it is
formalised as a constraint that the optimiser can only search in a norm ball with a radius smaller
than ğ‘Ÿ , to guarantee the detected AEs are indeed â€œadversarialâ€ to label ğ‘¦.

While the feature level information is captured by the global distribution over a latent space,
we only consider how the pixel level information is distributed in terms of perceptual quality to
humans. Three common quantitative metricsâ€”MSE, PSNR and SSIM introduced in Section 2.3â€”are
investigated. We note, those three metrics by no means are the true local distribution representing
perceptual quality, rather quantifiable indicators from different aspects. Thus, in the optimisation
problem of Eqn. (7), replacing the local distribution term with them would suffice our purpose. So,
we redefine the optimisation problem as:

max
ğ‘¥ â€²

J (ğ‘“ (ğ‘¥ â€²), ğ‘¦) +ğ›¼ Â·L (ğ‘¥, ğ‘¥ â€²),

s.t. ||ğ‘¥ âˆ’ğ‘¥ â€²||âˆ â‰¤ ğ‘Ÿ

(8)

, Vol. 1, No. 1, Article . Publication date: May 2022.

10

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

where L (ğ‘¥, ğ‘¥ â€²) represents those perceptual quality metrics correlated with the local distribution of
the seed ğ‘¥. Certainly, implementing L (ğ‘¥, ğ‘¥ â€²) requires some prepossessing, e.g., normalisation and
negation, depending on which metric is adopted.

Considering that the second term of the objective function in Eqn. (8) may not be differentiable
and/or the DL modelâ€™s parameters are not always accessible, we propose a black-box approach to
solve the optimisation problem that generates local test cases. It is based on a GA with two fitness
functions to effectively and efficiently detect AEs, as shown in Algorithm 1.

Algorithm 1 Two-Step GA Based Local Test Cases Generation
Input: Test seed (ğ‘¥, ğ‘¦), neural network function ğ‘“ (ğ‘¥), local perceptual quality metric L (ğ‘¥, ğ‘¥ â€²),

population size ğ‘ , maximum iterations ğ‘‡ , norm ball radius ğ‘Ÿ , weight parameter ğ›¼.

Output: A set of ğ‘š test cases T

T [ğ‘–] = ğ‘¥ + uniform(âˆ’ğ‘Ÿ, +ğ‘Ÿ )

1: ğ¹1 = J (ğ‘“ (ğ‘¥ â€²), ğ‘¦), ğ¹2 = J (ğ‘“ (ğ‘¥ â€²), ğ‘¦) + ğ›¼ Â· L (ğ‘¥, ğ‘¥ â€²)
2: for ğ‘– = 1, ..., ğ‘ do
3:
4: end for
5: while ğ‘¡ < ğ‘‡ or max(fit_list2) does not converge do
6:
7:
8:
9:
10:
11:

fit_list1 = cal_fitness(ğ¹1, T )
fit_list2 = cal_fitness(ğ¹2, T )
if majority(fit_list1 < 0) then

parents = selection(fit_list1, T )

parents = selection(fit_list2, T )

else

12:
end if
T = crossover (parents, ğ‘ )
13:
T = mutation(T ) âˆª parents
14:
ğ‘¡ = ğ‘¡ + 1
15:
16: end while
17: fit_list2 = cal_fitness(ğ¹2, T )
18: ğ‘–ğ‘‘ğ‘¥ = arg max(ğ‘“ ğ‘–ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡2) [: ğ‘š]
19: T = T [ğ‘–ğ‘‘ğ‘¥]
20: return test set T

Algorithm 1 presents the process of generating a set of ğ‘š test cases T from a given seed
ğ‘¥ with label ğ‘¦ (denoted as (ğ‘¥, ğ‘¦)). At line 1, we define two fitness functions (the reason behind
it will be discussed next). We initialise the population by adding uniform noise in range (âˆ’ğ‘Ÿ, +ğ‘Ÿ )
to the test seed, at line 2-4. At line 5-16, the population is iteratively updated by evaluating the
fitness functions, selecting the parents, conducting crossover and mutation. At line 17-20, the best
ğ‘š fitted individuals in population are chosen as test cases. The crossover and mutation are regular
operations in GA based test cases generation for DL models [1], while two fitness functions work
alternatively to guide the selection of parents.

The reason why we propose two fitness functions is because, we notice that there is a trade-
off between the two objectives J and L in the optimisation. Prediction loss J is related to the
adversarial strength, while L indicates the local distribution. Intuitively, generating the test cases
with high local probability tends to add small amount of perturbations to the seed, while a greater
perturbation is more likely to induce high prediction loss. To avoid the competition between the
two terms that may finally leads to a failure of detecting AEs, we define two fitness functions to

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

precisely control the preference at different stages:

ğ¹1 = J (ğ‘“ (ğ‘¥ â€²), ğ‘¦),

ğ¹2 = J (ğ‘“ (ğ‘¥ â€²), ğ‘¦) + ğ›¼ Â· L (ğ‘¥, ğ‘¥ â€²)

11

(9)

At early stage, ğ¹1 is optimised to quickly direct the generation of AEs, with ğ¹1 > 0 meaning the
detection of an AE. When most individuals in the population are AEs, i.e., majority(fit_list1 â‰¥ 0),
the optimisation moves to the second stage, in which ğ¹2 is replaced by ğ¹1 to optimise the local
distribution indicator as well as the prediction loss. It is possible4 that the prediction loss of most
individuals again become negative, then the optimisation will go back to the first stage. With such
a mechanism of alternatively using two fitness functions in the optimisation, the proportion of
AEs in the population is effectively prevented from decreasing.

Algorithm 1 describes the process for generating ğ‘š local test cases given a single test seed.
Suppose ğ‘› test seeds are selected earlier and in total ğ‘€ local test cases are affordable, we can
allocate, for each test seed ğ‘¥ğ‘– , the number of local test cases ğ‘šğ‘– , according to the ğ‘› (re-normalised)
global probabilities, which emphasises more on the role of distribution in our detected set of AEs.
Running Example: The right diagram in Fig. 2 plots the local distribution using MSE as its
indicator, and visualises the detected AEs by different testing methods. Unsurprisingly, all AEs
detected by our proposed HDA testing are located at the high density regions (and very close to
the central test seed), given it considers the perceptual quality metric as one of the optimisation
objectives in the two-step GA based test case generation. In contrast, other methods (PGD and
coverage-guided) are less effective.

4 EVALUATION
We evaluate the proposed HDA method by performing extensive experiments to address the
following research questions (RQs):

RQ1 (Effectiveness): How effective are the methods adopted in the three main stages of
HDA?. Namely, we conduct experiments to i) examine the accuracy of combining VAE-Encoder+KDE
to approximate the global distribution; ii) check the correlation significance of the two proposed lo-
cal robustness indicators with the local robustness; iii) investigate the effectiveness of our two-step
GA for local test cases generation.

RQ2 (AE Quality): How is the quality of AEs detected by HDA?. Comparing to conventional
attack-based and coverage-guided methods and more recent distribution-aware testing methods
of OODA and FODA, we introduce a comprehensive set of metrics to evaluate the quality of AEs
detected by HDA and others.

RQ3 (Sensitivity): How sensitive is HDA to the DL models under testing? We carry out
experiments to assess the capability of HDA applied on DL models (adversarially trained) with
different levels of robustness.

RQ4 (Robustness Growth): How useful is HDA to support robustness growth of the DL
model under testing? We examine the global robustness of DL models after â€œfixingâ€ the AEs
detected by various testing methods.

4.1 Experiment Setup
In RQ1 and RQ2, we consider five popular benchmark datasets and five diverse model architectures
for evaluation. Details of the datasets and trained DL models under testing are listed in Table 1. The
norm ball radius ğ‘Ÿ is calculated based on the ğ‘Ÿ -separation distance (cf. Remark 1) of each dataset.

4Especially when a large ğ›¼ is used, i.e., with preference on detecting AEs with high local probability than with high
adversarial strength, cf. the aforementioned trade-off.

, Vol. 1, No. 1, Article . Publication date: May 2022.

12

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

In RQ3, we add the comparison on DL models, enhanced by PGD-based adversarial training,
for sensitivity analysis. Table 1 also records the accuracy of these adversarially trained models.
Adversarial training trades the generalisation accuracy for the robustness as expected (thus a
noticeable decrement of the training and testing accuracy) [43]. In RQ4, we firstly sample 10000
data points from the global distribution as validation set and detect AEs around them by different
methods. Then, we fine-tune the normally trained models with training dataset augmented by these
AEs. 10 epochs are taken along with â€˜slow start, fast decayâ€™ learning rate schedule [17] to reduce
the computational cost while improve the accuracy-drop and robustness. To empirically estimate
the global robustness on validation set, we find another set of AEs according to local distribution,
different from the fine-tuning data. These validating AEs are miss-classified by normally trained
models. Thus, empirical global robustness of normally trained models is set to 0 as the baseline.

For readersâ€™ convenience, all the metrics used in RQ2, RQ3 and RQ4 for comparisons are listed
in Table 2. The metrics are introduced to comprehensively evaluate the quality of detected AEs and
the DL models from different aspects.

Table 1. Details of the datasets and DL models under testing.

Dataset

Image Size

ğ‘Ÿ

DL Model

Normal Training

Adversarial Training
Train Acc. Test Acc. Train Acc. Test Acc.

MNIST

1 Ã— 32 Ã— 32
Fashion-MNIST 1 Ã— 32 Ã— 32
3 Ã— 32 Ã— 32
3 Ã— 32 Ã— 32
3 Ã— 64 Ã— 64

SVHN
CIFAR-10
CelebA

LeNet5
AlexNet
VGG11
ResNet20

0.1
0.08
0.03
0.03
0.05 MobileNetV1

1.000
0.952
0.945
0.994
0.953

0.991
0.910
0.944
0.900
0.918

0.992
0.899
0.882
0.748
0.877

0.988
0.882
0.889
0.724
0.853

Table 2. Evaluation metrics for the quality of detected AEs and DL models

Metrics
AE Prop.
Pred. Loss
ğ‘ğ‘”
Rğ‘™
Ë†Rğ‘”
FID
ğœ–
% of Valid AEs

Meanings
Proportion of AEs in the set of test cases generated from selected test seeds
Adversarial strength of AEs as formally defined by Definition 3
Normalised global probability density of test-seeds/AEs
Local robustness to the correct classification label, as formally defined by Definition 1
Empirical global robustness of DL models over input domain as defined in Definition 5
Distribution difference between original images (test seeds) and perturbed images (AEs)
Average perturbation distance between test seeds and AEs
Percentage of â€œin-distirbutionâ€ AEs in all detected AEs

All experiments were run on a machine of Ubuntu 18.04.5 LTS x86_64 with Nvidia A100 GPU and
40G RAM. The source code, DL models, datasets and all experiment results are publicly available at
https://github.com/havelhuang/HDA-Testing.

4.2 Evaluation Results and Discussions
4.2.1 RQ1. There are 3 sets of experiments in RQ1 to examine the accuracy of technical solutions
in our tool-chain, corresponding to the 3 main stages respectively.

First, to approximate the global distribution, we essentially proceed in two stepsâ€”dimensionality
reduction and PDF fitting, for which we adopt the VAE-Encoder+KDE solution. Notably, the
VAE trained in this step is for data-compression only (not for generating new data). To reflect
the effectiveness of both aforementioned steps, we (i) compare VAE-Encoder with the Principal

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

13

Component Analysis (PCA), and (ii) then measure the FID between the training dataset and a set
of random samples drawn from the fitted global distribution by KDE.

PCA is a common approach for dimensionality reduction. We compare the performance of
VAE-Encoder and PCA from the following two perspectives. The quality of latent representation
can be measured by the clustering and reconstruction accuracy. To learn the global distribution
from latent data, we require that latent representations should group together data-points based
on semantic features and can be decoded to reconstruct the original images with less information
loss. Therefore, we apply K-means clustering to the latent data and calculate the Completeness
Score (CS), Homogeneity Score (HS) and V-measure Score (VS) [25] for measuring the ability of
clustering. While, the reconstruction loss is calculated based on the MSE. As is shown in Table 3,
VAE-Encoder achieves higher CS, HS, VS scores and less reconstruction loss than PCA. In other
words, the latent representations encoded by VAE-Encoder is more significant in terms of capturing
features than that of PCA.

Table 3. Quality of Latent Representation in PCA & VAE-Encoder

PCA

VAE-Encoder

Dataset

MNIST

CS
0.505
F.-MNIST 0.497
0.007
0.084
0.112

SVHN
CIFAR-10
CelebA

Clustering
HS
0.508
0.520
0.007
0.085
0.092

VS
0.507
0.508
0.007
0.085
0.101

Recon. Loss

44.09
55.56
65.75
188.22
764.94

Clustering
HS
0.566
0.601
0.012
0.105
0.150

CS
0.564
0.586
0.013
0.105
0.185

VS
0.565
0.594
0.013
0.105
0.166

Recon. Loss

27.13
23.72
66.21
168.44
590.54

Dataset
MNIST
Fashion-MNIST
SVHN
CIFAR-10
CelebA

Global Dist. Uni. Dist.

0.395
0.936
0.875
0.285
0.231

13.745
90.235
143.119
12.053
8.907

Fig. 3 & Table 4. Samples drawn from the approximated global distribution by KDE and a uniform distribution
over the latent feature space (Figure); and FID to the ground truth based on 1000 samples (Table).

To evaluate the accuracy of using KDE to fit the global distribution, we calculate the FID between
a new dataset (with 1000 samples) based on the fitted global distribution by KDE and the training
dataset. The FID scores are shown in Table 4. As a baseline, we also present the results of using
a uniform distribution over the latent space. As expected, we observe that all FID scores based
on approximated distributions are significantly smaller (better). We further decode the newly
generated dataset for visualisation in Fig. 3, from which we see the generated images by KDE keep
high fidelity while the uniformly sampled images are not human-perceptible.

Answer to RQ1 on HDA stage 1: The combination of VAE-Encoder+KDE may accurately
approximate the global distribution.

, Vol. 1, No. 1, Article . Publication date: May 2022.

14

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

Table 5. Pearson correlation coefficients (in absolute values) between the local robustness & its two indicators.

Dataset
MNIST

ğ‘†grad
0.672
Fashion-MNIST 0.872
0.848
0.832
0.699

SVHN
CIFAR-10
CelebA

ğ‘†sep
0.379
0.716
0.612
0.646
0.468

Move on to stage 2, we study the correlations between a norm ballâ€™s local robustness and its two
indicators proposed earlierâ€”the prediction gradient based score and the score based on separation
distance of output-layer activation (cf. Eq. 6).

We invoke the tool [35] for estimating the local robustness Rğ‘™ defined in Definition 1. Based
on 1000 randomly selected data-points from the test set as the central point of 1000 norm balls,
we calculate the local robustness of each norm ball5 as well as the two proposed indicators. Then,
we do the scatter plots (in log-log scale6), as shown in Fig. 4. Apparently, for all 5 datasets, the
indicator based on activation separation distance is negatively correlated (1st row), while the
gradient indicator is positively correlated with the estimated local robustness (2nd row). We further
quantify the correlation by calculating the Pearson coefficients, as recorded in Table 5. We observe,
both indicators are highly correlated with the local robustness, while the gradient based indicator
is stronger. This is unsurprising, because the activation separation distance is a black-box metric
which is usually weaker than the white-box gradient information.

Fig. 4. Scatter plots of the local robustness evaluation vs. its two indicators, based on 1000 random norm balls.

Answer to RQ1 on HDA stage 2: The two proposed local robustness indicators are signifi-
cantly correlated with the local robustness.

For the local test case generation in stage 3, by configuring the parameter ğ›¼ in our two-step GA,
we may do trade-off between the â€œstrength of being adversrialâ€ (measured by prediction loss J )
and the perceptual quality (measured by a specific L), so that the quality of detected AEs can be
optimised.

5Radius ğ‘Ÿ is usually small by definition (cf. Remark 1), yielding very small ğ‘™ğ‘œğ‘” (1 âˆ’ Rğ‘™ ).
6There are dots collapsed on the vertical line of ğ‘™ğ‘œğ‘” (1 âˆ’ ğ‘…) = âˆ’70, due to a limitation of the estimator [35]â€”it terminates
with the specified threshold when the estimation is lower than that value. Note, the correlation calculated with such noise
is not undermining our conclusion, rather the real correlation would be even higher.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

15

Fig. 5. The prediction loss (red) and the three quantified local distribution indicators (blue) of the best fitted
test case during the iterations of our two-step GA based local test case generation.

Fig. 6. Comparison between regular GA and two-step GA.

In Fig. 5, we visualise the changes of the two fitness values as the iterations of the GA. As
shown in the first plot, only the prediction loss J is taken as the objective function (i.e., ğ›¼ = 0)
during the whole iteration process. The GA can effectively find AEs with maximised adversarial
strength, which is observed from that the prediction loss of best fitted test case in the population
converges after hundreds of iterations. From the second to the last plot, the other fitness function
L representing the local distribution information is added to the objective function (i.e., ğ›¼ > 0),

, Vol. 1, No. 1, Article . Publication date: May 2022.

16

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

they are MSE, PSNR and SSIM. Intuitively, higher local probability density implies smaller MSE
and greater PSNR and SSIM.

Thanks to the two-step setting of the fitness functions, the prediction loss J of best fitted
test case goes over 0 quickly in less than 200 iterations, which means it detects a first AE in the
population. The J of the best fitted test case is always quite close to the rest in the population, thus
we may confidently claim that many AEs are efficiently detected by the population not long after
the first AE was detected. Then, the optimisation goes to the second stage, in which the quantified
local distribution indicator L is pursued. The J and L finally converge and achieve a balance
between them. If we configure the coefficient ğ›¼, the balance point will change correspondingly. A
greater ğ›¼ (e.g., ğ›¼ = 1.1 in the plots) detects more natural AEs (i.e., with higher local probability
density), and the price paid is that the detected AEs are with weaker adversarial strength (i.e., with
smaller but still positive prediction loss).

Fig. 7. AEs detected by our two-step GA (last 3 columns) & other methods

We further investigate the advantages of our 2-step GA over the regular GA (using ğ¹2 as the
objective function). In Fig. 6, as ğ›¼ increases, the proportion of AEs in the population exhibits a
sharp drop to 0 when using the regular GA. In contrast, the two-step GA prevents such decreasing
of the AE proportion while preserving it at a high-level of 0.6, even when ğ›¼ is quite large. Moreover,
larger ğ›¼ represents the situations when the AEs are more naturalâ€”as shown by the blue curves7,
the local distribution indicator (SSIM in this case) is only sufficiently high when ğ›¼ is big enough.
Thus, compared to the regular GA, we may claim our novel 2-step GA is more robust (in detecting
AEs) to the choices of ğ›¼ and more suitable in our framework for detecting AEs with high local
probabilities.

7The blue dashed line stops earlier as there is no AEs in the population when ğ›¼ is big.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

17

Fig. 7 displays some selected AEs from the five datasets. Same as the PGD and the coverage-guided
testing, if we only use the prediction loss J as the objective function in the GA, the perturbations
added to the images can be easily told. In stark contrast, AEs generated by our two-step GA (with
the 3 perceptual quality metrics in the last 3 columns) are of high quality8 and indistinguishable
with human-eyes from the original images (first column).

Answer to RQ1 on HDA stage 3: Two-step GA based local test case generation can effectively
detect AEs with high perception quality.

4.2.2 RQ2. We compare our HDA with state-of-the-art AE detection methods in two sets of
experiments. In the first set, we focus on comparing with the adversarial attack and coverage-
guided testing (i.e., the typical PGD attack and neuron coverage metric for brevity, while the
conclusion can be generalised to other attacks and coverage metrics). Then in the second set of
experiments, we show the advantages of our HDA over other distribution-aware testing methods.
In fact, both PGD attack and coverage-guided testing do not contribute to test seeds selection.
They simply use randomly sampled data from the test set as test seeds, by default. Thus, we only
need to compare the randomly selected test seeds with our â€œglobal distribution probability9 plus
local robustness indicatedâ€ test seeds, shown as â€œâ€˜ğ‘ğ‘” + Rğ‘™ â€ in Table 6. Specifically, for each test seed,
we calculate two metricsâ€”the local robustness Rğ‘™ of its norm ball and its corresponding global
probability ğ‘ğ‘”. We invoke the estimator of [35] to calculate the former (ğ‘™ğ‘œğ‘”(1 âˆ’ Rğ‘™ ), to be exact).
To reduce the sampling noise, we repeat the test seed selection 100 times and present the averaged
results in Table 6.

Table 6. Comparison between randomly selected test seeds and our â€œğ‘ğ‘” + Rğ‘™ indicatedâ€ test seeds (averaging
over 100 test seeds).

Dataset

MNIST
Fashion-MNIST
SVHN
CIFAR-10
CelebA

Random Test Seeds
ğ‘™ğ‘œğ‘”(1 âˆ’ Rğ‘™ )
-48.7
-21.9
-22.1
-23.3
-36.3

ğ‘ğ‘”
0.0049
0.0074
0.0055
0.0101
0.0069

ğ‘ğ‘” + Rğ‘™ Test Seeds
ğ‘™ğ‘œğ‘”(1 âˆ’ Rğ‘™ )
ğ‘ğ‘”
0.0835
-45.6
0.0632
-18.4
0.0804
-21.2
0.3439
-19.8
0.1272
-32.7

From Table 6, we observe: (i) test seeds selected by our method have much higher global proba-
bility density, meaning their norm balls are much more representative of the data distribution; (ii)
the norm balls of our test seeds have worse local robustness, meaning it is more cost-effective to
detect AEs in them. These are unsurprising, because we have explicitly considered the distribution
and local robustness information in the test seed selection.

Finally, the overall evaluation on the generated test cases and the detected AEs by them are
shown in Table 7. The results are presented in two dimensionsâ€”3 types of testing methods versus
2 ways of selecting test seeds, yielding 6 combinations (although by default, PGD and Coverage-
guided methods are using random seeds, while our method is using the â€œğ‘ğ‘” + Rğ‘™ â€ seeds). For each
combination, we study 4 metrics (cf. Table 2 for meanings behind them): (i) the AE proportion; (ii)

8All 3 perceptual quality metrics perform good in grey-scale images, while SSIM performs the best in colourful images and
tends to add noise to the background.
9Refer to Section 3.2 for the calculation. The value of probability density is further normalised by training dataset for a
better presentation.

, Vol. 1, No. 1, Article . Publication date: May 2022.

18

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

the average prediction loss; (iii) the FID10 of the test set quantifying the image quality; and (iv)
the computational time (and an additional coverage rate for coverage-guided testing). We note the
observations on these 4 metrics in the following paragraphs.

Table 7. Evaluation of the generated test cases and detected AEs by PGD Attack, coverage-guided testing
and the proposed HDA testing (all results are averaged over 100 seeds)

PGD Attack

Coverage Guided Testing

Seed

Dataset

AE Prop. Pred. Loss

Random
Seeds

ğ‘ğ‘”+Rğ‘™
Seeds

MNIST
F.-MNIST
SVHN
CIFAR-10
CelebA
MNIST
F.-MNIST
SVHN
CIFAR-10
CelebA

0.205
0.957
0.866
1.000
0.979
0.986
1.000
0.992
1.000
0.998

7.01
19.62
2.81
39.74
119.95
11.95
26.24
2.91
40.08
120.51

FID Time(s) Cov. Rate AE Prop. Pred. Loss
0.46
1.89
95.81
87.51
78.53
0.21
0.69
87.50
83.35
74.83

1.48
2.15
0.09
3.32
12.09
1.37
2.41
0.09
3.98
8.72

0.859
0.936
0.976
0.988
0.992
0.873
0.950
0.979
0.989
0.988

0.001
0.228
0.004
0.196
0.052
0.076
0.322
0.038
0.221
0.067

0.48
0.44
11.11
11.22
12.64
0.47
0.44
11.44
12.74
11.54

FID Time(s) AE Prop. Pred. Loss
0.76
3.65
98.49
93.32
84.42
0.82
1.33
93.05
87.05
80.49

Hierarchical Distribution-Aware Testing
Time(s)
51.63
63.36
156.46
221.29
233.78
53.05
62.61
156.39
221.38
233.97

187.92
131.47
343.47
542.73
931.65
187.73
132.39
336.64
543.32
939.78

FID
0.16
0.11
95.22
75.59
69.39
0.01
0.03
83.02
70.27
67.77

0.600
0.999
0.922
1.000
1.000
1.000
1.000
1.000
1.000
1.000

1.48
6.08
2.37
37.79
96.03
3.59
9.73
2.12
33.45
93.48

Regarding the AE proportion in the set of generated test cases, the default setting of our proposed
approach is clearly the best (with score 1) among the 6 combinations. Both our novel test seed
selection and two-step GA local test case generation methods contribute to the win. This can be
told from the decreased AE proportion when using random seeds in our method, but still the result
is relatively higher than most combinations. PGD, as a white-box approach using the gradient
information, is also quite efficient in detecting AEs, especially when paired with our new test seed
selection method. On the other hand, coverage-guided testing is comparatively less effective in
detecting AEs (even with high coverage rate), yet our test seed selection method can improve it.

As per the results of prediction loss, PGD, as a gradient-descent based attacking method, unsur-
prisingly finds the AEs with the largest prediction loss. With better test seeds considering local
robustness indicators selected by our method, the prediction loss of PGD can be even higher. Both
coverage-guided and our HDA testing are detecting AEs with relatively lower prediction loss,
meaning the AEs are with â€œweaker adversarial strengthâ€. The reason for the low prediction loss of
AEs detected by our approach is that our two-step GA makes the trade-off and sacrifices it for AEs
with higher local probabilities (i.e., more natural). This can be seen through the small FID of our
test set. PGD, on the other hand, has relatively high FID scores, as well as coverage-guided testing.
On the computational overheads, we observe PGD is the most efficient, given it is by nature
a white-box approach using the gradient-descent information. While, our approach is an end-to-
end black-box approach (if without using the gradient based indicator when selecting test seeds)
requiring less information and being more generic, at the price of being relatively less efficient. That
said, the computational time of our approach is still acceptable and better than coverage-guided
testing.

Answer to RQ2 on comparing with adversarial attack and coverage-guided testing: HDA
shows advantages over adversarial attack and coverage-guided testing on test seeds selection
and generation of high perception quality AEs.

Next, we try to answer the difference between our HDA testing method and other distribution-
aware testing as summarised earlier (the amber route of Fig. 1). We not only study the common

10To show how close the perturbed test cases are to the test seeds in the latent space, we use the last convolutional layer of
InceptionV3 to extract the latent representations of colour images for FID. InceptionV3 is a well-trained CNN and commonly
used to show FID that captures the perturbation levels, e.g., in [12]. While InceptionV3 is used for colour images, VAE is
used for grey-scale datasets MNIST and Fashion-MNIST.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

19

evaluation metrics in earlier RQs, but also the input validation method in [7], which flags the
validity of AEs according to a user-defined reconstruction probability threshold.

Table 8. Evaluation of AEs detected by OODA, FODA and our HDA testing methods (based on 100 test seeds).

Dataset

Tool

ğ‘ğ‘”

MNIST

SVHN

OODA 0.0055
FODA 0.0030
HDA 0.0835
OODA 0.0046
FODA 0.0021
HDA 0.0804

% of
Valid AEs
29
100
100
21
100
100

ğœ–

FID

0.81
0.73
0.05
0.82
0.31
0.03

2.29
0.47
0.01
128.84
110.73
83.02

As shown in Table 8, HDA can select test seeds from much higher density region on the global
distribution and find more valid AEs than OODA. The reason behind this is that OODA aims at
detecting outliersâ€”only AEs have lower reconstruction probabilities (from the test seed) than
the given threshold will be marked as invalid test cases. While, HDA explicitly explores the high
density meanwhile error-prone regions by combining the global distribution and local robustness
indicators. In other words, HDA does priority ordering (according to the global distribution and
local robustness) and then selects the best, while OODA rules out the worst. As expected, FODA
performs similarly bad as OODA in terms of ğ‘ğ‘”, since both are using randomly selected seeds.
While, FODA has high proportion of valid AEs since the test cases are directly sampled from the
distribution in latent space.

Regarding the perceptual quality of detected AEs, HDA can always find AEs with small pixel-level
perturbations (ğœ–) in consideration of the ğ‘Ÿ -separation constraint, and with small FID thanks to
the use of perceptual quality metrics (SSIM in this case) as objective functions. While OODA only
utilises the reconstruction probability (from VAE) to choose AEs, and FODA directly samples test
cases from VAE without any restrictions (thus may suffer from the oracle problem, cf. Remark 4
later). Due to the compression nature of generative modelsâ€”they are good at extracting feature
level information but ignore pixel level information [49], AEs detected by OODA and FODA are all
distant to the original test seeds, yielding large ğœ– and FID scores. Notably, the average distance ğœ–
between test seeds and AEs detected by OODA and FODA are much (7âˆ¼28 times) greater than the
ğ‘Ÿ -separation constraints (cf. Table 1), leading to the potential oracle issues of those AEs, for which
we have the following remark:

Remark 4 (Oracle Issues of AEs Detected by OODA and FODA). AEs detected by OODA
and FODA are normally distant to the test seeds with a perturbation distance even greater than the
ğ‘Ÿ -separation constraint. Consequently, there is the risk that the perturbed image may not share the
same ground truth label of the test seed, and thus hard to determine the ground truth label of the
â€œAEâ€11.

To visualise the difference between AEs detected by HDA, FODA and OODA, we present 4
examples in Fig. 8. We may observe the AEs detected by HDA are almost indistinguishable from
the original images. Moreover, the AEs by FODA is a set of concrete evidence for Remark 4â€”it is
actually quite hard to tell what is the ground truth label of some perturbed image (e.g., the bottom
left one), while others appear to have a different label of the seed (e.g., the bottom right one should
be with a label â€œ1â€ instead of â€œ7â€).

11In quotes, because the perturbed image could be a â€œbenign exampleâ€ with a correct predicted label (but different to the test
seed).

, Vol. 1, No. 1, Article . Publication date: May 2022.

20

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

Fig. 8. Example AEs detected by different distribution-aware testing methods.AEs detected by our HDA are
indistinguishable from the original images, while AEs detected by FODA and OODA are of low perceptual
quality and subject to the oracle issues noted by Remark 4.

Answer to RQ2 on comparing with other distribution-aware testing: Compared to OODA
and FODA, the proposed HDA testing can detect more valid AEs, free of oracle issues, with
higher global probabilities and perception quality.

4.2.3 RQ3. In earlier RQs, we have varied the datasets and model architectures to check the
effectiveness of HDA. In this RQ3, we concern HDAâ€™s sensitivity to DL models with different
levels of robustness. Adversarial training may greatly improve the robustness of DL models and is
normally used as the defence to adversarial attack. To this end, we apply HDA on both normally and
adversarially trained models (by [22] to be exact), and then compare with three most representative
attacking methodsâ€”the most classic FGSM, the most popular PGD, and the most advanced one
AutoAttack [6]. Experimental results are presented in Table 9.

Table 9. Evaluation of AEs generated by FGSM, PGD, AutoAttack and HDA on normally and adversarially
trained DL models (all results are averaged over 100 test seeds).

Model

Normally
Trained

Adversarially
Trained

Dataset

MNIST

SVHN
CIFAR10
CelebA
MNIST

ğ‘ğ‘”
0.0099
F.-MNIST 0.0109
0.0041
0.0115
0.0090
0.0100
F.-MNIST 0.0112
0.0043
0.0137
0.0096

SVHN
CIFAR10
CelebA

FGSM
AE Prop.
0.34
0.78
0.77
0.93
0.81
0.09
0.29
0.45
0.46
0.37

FID
1.085
3.964
114.65
112.32
99.226
0.993
4.187
127.89
122.74
108.56

ğ‘ğ‘”
0.0099
0.0109
0.0042
0.0114
0.0090
0.0100
0.0112
0.0040
0.0136
0.0095

PGD
AE Prop.
0.53
1.00
0.97
1.00
0.97
0.08
0.34
0.50
0.50
0.39

FID
0.639
2.611
107.04
101.92
89.413
0.728
3.492
121.25
118.55
105.96

AutoAttack
AE Prop.
0.92
1.00
0.99
1.00
1.00
0.11
0.74
0.63
0.55
0.43

FID
0.954
4.505
108.41
108.15
91.591
0.634
2.888
120.97
93.779
106.72

ğ‘ğ‘”
0.0100
0.0109
0.0042
0.0115
0.0091
0.0105
0.0122
0.0054
0.0139
0.0097

ğ‘ğ‘”
0.0835
0.0635
0.0804
0.3442
0.1285
0.1944
0.0632
0.0821
0.3263
0.2007

HDA
AE Prop.
1.00
1.00
1.00
1.00
1.00
0.62
0.81
0.71
0.64
0.49

FID
0.011
0.013
79.21
67.13
67.71
0.049
0.297
83.88
55.47
71.06

As expected, after the adversarial training by [22], the robustness of all five DL models are greatly
improved. This can be observed from the metric of AE Prop.: For all four methods, the proportion
of AEs detected in the set of test case is sharply decreased for adversarially trained models, while
HDA still outperforms others. Since the rationales behind the three adversarial attacks are without
considering the input data distribution nor perception quality, it is unsurprising that their two sets
of results for normally and adversarially trained models are quite similar, in terms of the metrics ğ‘ğ‘”
and FID. On the other hand, the FID scores of AEs detected by HDA get worse but still better than
others. The measured ğ‘ğ‘” on AEs detected by HDA also changed due to the variations of robustness
indicators before and after the adversarial training, and yet much higher than all attacking methods.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

21

Answer to RQ3: HDA is shown to be capable and superior to common adversarial attacks
when applied on DL models with different levels of robustness.

4.2.4 RQ4. The ultimate goal of developing HDA testing is to improve the global robustness of
DL models. To this end, we refer to a validation set of 10000 test seeds. We fine-tune [17] the DL
models with AEs detected for validation set from different methods. Then, we calculate the train
accuracy, test accuracy and empirical global robustness before and after the adversarial fine-tuning.
Empirical global robustness is measured on a new set of on-distribution AEs for validation set,
different from the fine-tuning data. Fine-tuning requires to know the ground truth label of the AEs,
which cannot be satisfied due to the potential oracle issues of OODA and FODA (cf. Remark 4).
Thus, we omit the comparison with OODA and FODA, while other results are presented in Table 10.

Table 10. Evaluation of DL modelsâ€™ train accuracy, test accuracy, and empirical global robustness (based on
10000 on-distribution AEs) after adversarial fine-tuning.

Dataset

MNIST

F.-MNIST

SVHN

CIFAR-10

No. of
Test Cases
300
3000
30000
300
3000
30000
300
3000
30000
300
3000
30000

PGD Attack
Train Acc. Test Acc.
97.64%
98.05%
98.65%
91.27%
87.34%
85.06%
93.63%
88.94%
80.14%
85.60%
84.10%
82.99%

98.26%
99.10%
99.94%
97.41%
89.48%
86.67%
95.01%
88.81%
78.72%
92.39%
88.78%
88.26%

Rğ‘”
49.27%
84.09%
99.88%
68.04%
88.78%
95.00%
48.84%
75.83%
80.14%
46.88%
76.46%
94.47%

HDA Testing
Train Acc. Test Acc.
98.77%
98.72%
98.88%
91.35%
89.96%
89.70%
87.78%
92.66%
91.91%
86.78%
86.42%
86.13%

99.98%
99.95%
100.00%
98.92%
94.49%
93.54%
89.26%
92.96%
92.81%
93.38%
92.07%
91.62%

Rğ‘”
90.94%
99.28%
100.00%
70.00%
92.39%
97.89%
62.93%
83.78%
94.91%
48.96%
92.92%
97.58%

Coverage Guided Testing
Rğ‘”
34.83%
47.91%
71.10%
47.05%
63.30%
84.71%
16.79%
37.59%
66.58%
0.03%
0.48%
13.43%

Train Acc. Test Acc.
98.93%
98.99%
98.77%
90.93%
84.94%
85.56%
90.95%
84.06%
84.18%
86.22%
84.40%
84.30%

100.00%
100.00%
100.00%
97.62%
88.06%
88.60%
97.25%
87.21%
87.32%
95.56%
93.22%
93.46%

We first observe that adversarial fine-tuning is effective to improve the DL modelsâ€™ empirical
global robustness, measured by the prediction accuracy on AEs for normally trained models, while
compromising the train/test accuracy as expected (in contrast to normal training in Table 1). In
most cases, DL models enhanced by HDA testing suffers least from the drop of generalisation. The
reason behind this is that HDA testing targets at AEs from high density regions on distributions,
usually with small prediction loss, shown in Fig. 5. Thus, eliminating AEs detected by HDA testing
requires relatively minor adjustment to DLâ€™s models, the generalisation of which can be easily
tampered during the fine-tuning with new samples.

In terms of empirical global robustness, HDA testing detects AEs around test seeds from the high
global distribution region, which are more significant to the global robustness improvement. It can
be seen that with 3000 test cases generated by utilising 1000 test seeds, the HDA testing can improve
empirical global robustness to nearly or over 90%, very closed to the fine-tuning with 30000 test
cases from 10000 test seeds. This means the distribution-based test seeds selection is more efficient
than random test seeds selection. Moreover, even fine-tuning with 30000 test cases, leveraging
all the test seeds in the validation set, HDA is still better than PGD attack and coverage-guided
testing, due to the consideration of local distributions (approximated by naturalness). We notice
that PGD-based adversarial fine-tuning minimises the maximum prediction loss within the local
region, which is also effective to eliminate the natural AEs, but sacrificing more train/test accuracy.
DL models fined-tuned with HDA testing achieve the best balance between the generalisation and
global robustness.

, Vol. 1, No. 1, Article . Publication date: May 2022.

22

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

Answer to RQ4: Compared with adversarial attack and coverage-guide testing, HDA con-
tributes more to the growth of global robustness, while mitigating the drop of train/test
accuracy during adversarial fine-tuning.

5 THREATS TO VALIDITY
5.1 Internal Validity
Threats may arise due to bias in establishing cause-effect relationships, simplifications and assump-
tions made in our experiments. In what follows, we list the main threats of each research question
and discuss how we mitigate them.

5.1.1 Threats from HDA Techniques. In RQ1, both the performance of the VAE-Encoder and KDE
are threats. For the former, it is mitigated by using four established quality metrics (in Table 3)
on evaluating dimensionality reduction techniques and compared to the common PCA method. It
is known that KDE performs poorly with high-dimensional data and works well when the data
dimension is modest [19, 27]. The data dimensions in our experiments are relatively low given
the datasets have been compressed by VAE-Encoder, which mitigates the second threat. When
studying the local robustness indicators, quantifying both the indicators and the local robustness
may subject to errors, for which we reduce them by carefully inspecting the correctness of the
script on calculating the indicators and invoking a reliable local robustness estimator [35] with fine-
tuned hyper-parameters. For using two-step GA to generate local test cases, a threat arises by the
calculation of norm ball radius, which has been mitigated by ğ‘Ÿ -separation distance presented in the
paper [41]. Also, the threat related to estimating the local distribution is mitigated by quantifying
its three indicators (MSE, PSNR and SSIM) that are typically used in representing image-quality by
human-perception.

5.1.2 Threats from AEsâ€™ Quality Measurement. A threat for RQ1, RQ2 and RQ3 (when examining
how effective our method models the global distribution and local distribution respectively) is the
use of FID as a metric, quantifying how â€œsimilarâ€ two image datasets are. Given FID is currently
the standard metric for this purpose, this threat is sufficiently mitigated now and can be further
mitigated with new metrics in future. RQ2 includes the method of validating AEs developed in
[7], which utilises generative models and OOD techniques to flag valid AEs with reconstruction
probabilities greater than a threshold. The determination of this threshold is critical, thus poses a
thread to RQ2. To mitigate it, we use same settings across all the experiments for fair comparisons.

5.1.3 Threats from Adversarial Training and Fine-Tuning. In RQ3 and RQ4, the first threat rises
from the fact that adversarial training and adversarial fine-tuning will sacrifice the DL modelâ€™s
generalisation for robustness. Since the training process is data-driven and of black-box nature, it is
hard to know how the predication of a single data-point will be affected, while it is meaningless to
study the robustness of an incorrectly predicted seed. To mitigate this threat when we compare the
robustness before and after adversarial training/fine-tuning, we select enough number of seeds and
check the prediction of each selected seed (filtering out incorrect ones if necessary) to make sure
test seeds are always predicted correctly. For the global robustness computation in RQ4, we refer to
a validation dataset, where a threat may arise if the empirical result based on the validation dataset
cannot represent the global robustness. To mitigate it, we synthesise the validation set with enough
dataâ€”10000 inputs sampled from global distribution. We further attack the validation dataset to
find an AE per seed according to the local distribution. Thus, DL modelsâ€™ prediction accuracy on
this dataset empirically represents the global robustness as defined. For the training/fine-tuning

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

23

to be effective, we need a sufficient number of AEs to augment the training dataset. A threat may
arise due to a small proportion of AEs in the augmented training dataset (the DL model will be
dominated by the original training data during the training/fine-tuning). To mitigate such a threat,
we generate a large proportion of AEs in our experiments.

5.2 External Validity
Threats might challenge the generalisability of our findings, e.g. the number of models and datasets
considered for experimentation; thus we mitigate these threats as follows. All our experiments are
conducted on 5 popular benchmark datasets, covering 5 typical types of DL models, cf. Table 1.
Experimental results on the effectiveness of each stage in our framework are all based on averaging
a large number of samples, reducing the random noise in the experiments. In two-step GA based
test case generation, a wide range of the ğ›¼ parameter has been studied showing converging trends.
Finally, we enable replication by making all experimental results publicly available/reproducible on
our project website to further mitigate the threat.

6 CONCLUSION & FUTURE WORK
In this paper, we propose a HDA testing approach for detecting AEs that considers both the
data distribution (thus with higher operational impact assuming the training data statistically
representing the future inputs) and perceptual quality (thus looks natural and realistic to humans).
The key novelty lies in the hierarchical consideration of two levels of distributions. To the best
of our knowledge, it is the first DL testing approach that explicitly and collectively models both
(i) the feature-level information when selecting test seeds and (ii) pixel-level information when
generating local test cases. To this end, we have developed a tool chain that provides technical
solutions for each stage of our HDA testing. Our experiments not only show the effectiveness of
each testing stage, but also the overall advantages of HDA testing over state-of-the-arts. From a
software engineeringâ€™s perspective, HDA is cost-effective (by focusing on practically meaningful
AEs), flexible (with end-to-end, black-box technical solutions) and may effectively contribute to the
robustness growth of the DL software under testing.

The purpose of detecting AEs is to fix them. Although existing DL retraining/repairing techniques
(e.g. [17] used in RQ4 and [33, 42]) may satisfy the purpose to some extent, bespoke â€œdebuggingâ€
methods with more emphasise on the feature-distribution and perceptual quality can be integrated
into our framework in a more efficient way. To this end, our important future work is to close the
loop of â€œdetect-fix-assessâ€ as depicted in [47] and then organise all generated evidence as safety
cases [45]. Finally, same as other distribution-aware testing methods, we assume the input data
distribution is same as the training data distribution. To relax this assumption, we plan to take
distribution-shift into consideration in future versions of HDA.

ACKNOWLEDGMENTS
This work is supported by the U.K. DSTL (through the project of Safety Argument for Learning-
enabled Autonomous Underwater Vehicles) and the U.K. EPSRC (through End-to-End Conceptual
Guarding of Neural Architectures [EP/T026995/1]). Xingyu Zhao and Alec Banksâ€™ contribution
to the work is partially supported through Fellowships at the Assuring Autonomy International
Programme. This project has received funding from the European Unionâ€™s Horizon 2020 research
and innovation programme under grant agreement No 956123.

This document is an overview of U.K. MOD (part) sponsored research and is released for informa-
tional purposes only. The contents of this document should not be interpreted as representing the
views of the U.K. MOD, nor should it be assumed that they reflect any current or future U.K. MOD

, Vol. 1, No. 1, Article . Publication date: May 2022.

24

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

policy. The information contained in this document cannot supersede any statutory or contractual
requirements or liabilities and is offered without prejudice or commitment.

REFERENCES
[1] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui Hsieh, and Mani B Srivastava. 2019.
Genattack: Practical black-box attacks with gradient-free optimization. In Proceedings of the Genetic and Evolutionary
Computation Conference. 1111â€“1119.

[2] David Berend. 2021. Distribution Awareness for AI System Testing. In 43rd IEEE/ACM Int. Conf. on Software Engineering:

Companion Proceedings, ICSE Companion 2021, Madrid, Spain, May 25-28, 2021. IEEE, 96â€“98.

[3] David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun Zhao. 2020. Cats Are Not Fish: Deep
Learning Testing Calls for out-of-Distribution Awareness. In Proc. of the 35th IEEE/ACM Int. Conference on Automated
Software Engineering (ASEâ€™20). ACM, New York, NY, USA, 1041â€“1052. https://doi.org/10.1145/3324884.3416609
[4] Taejoon Byun and Sanjai Rayadurgam. 2020. Manifold-based Test Generation for Image Classifiers. In ICSE â€™20:
42nd Int. Conference on Software Engineering, Workshops, Seoul, Republic of Korea, 27 June - 19 July, 2020. ACM, 221.
https://doi.org/10.1145/3387940.3391460

[5] Taejoon Byun, Abhishek Vijayakumar, Sanjai Rayadurgam, and Darren Cofer. 2020. Manifold-based Test Generation
for Image Classifiers. In Int. Conf. On Artificial Intelligence Testing (AITest). IEEE, Oxford, UK, 15â€“22. https://doi.org/
10.1109/AITEST49225.2020.00010

[6] Francesco Croce and Matthias Hein. 2020. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In Proc. of the 37th Int. Conf. on Machine Learning (ICMLâ€™20), Vol. 119. PMLR, 2206â€“2216.
[7] Swaroopa Dola, Matthew B. Dwyer, and Mary Lou Soffa. 2021. Distribution-Aware Testing of Neural Networks Using
Generative Models. In IEEE/ACM 43rd Int. Conference on Software Engineering (ICSEâ€™21). IEEE, Madrid, Spain, 226â€“237.
[8] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deepstellar: Model-based quantitative
analysis of stateful deep learning systems. In Proc. of the 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 477â€“487.
[9] Rafael C. Gonzalez and Richard E. Woods. 1992. Digital image processing. 793 pages.
[10] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples.
In 3rd Int. Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings.

[11] Fabrice Harel-Canada, Lingxiao Wang, Muhammad Ali Gulzar, Quanquan Gu, and Miryung Kim. 2020. Is Neuron
Coverage a Meaningful Measure for Testing Deep Neural Networks?. In Proc. of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM, New York, NY, USA,
851â€“862.

[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained
by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA.
6626â€“6637.

[13] Hossein Hosseini and Radha Poovendran. 2018. Semantic Adversarial Examples. In 2018 IEEE Conference on Computer
Vision and Pattern Recognition Workshops, CVPR Workshops 2018, Salt Lake City, UT, USA, June 18-22, 2018. IEEE
Computer Society, 1614â€“1619.

[14] Wei Huang, Youcheng Sun, Xingyu Zhao, James Sharp, Wenjie Ruan, Jie Meng, and Xiaowei Huang. 2021. Coverage

Guided Testing for Recurrent Neural Networks. IEEE Tran. on Reliability (2021).

[15] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, and et al. 2020. A survey of safety and trustworthiness of deep neural
networks: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review 37 (2020),
100270.

[16] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety verification of deep neural networks. In

Computer Aided Verification (LNCS, Vol. 10426). Springer International Publishing, Cham, 3â€“29.

[17] Ahmadreza Jeddi, Mohammad Javad Shafiee, and Alexander Wong. 2021. A simple fine-tuning is all you need: Towards
robust deep learning via adversarial fine-tuning. In Workshop on Adversarial Machine Learning in Real-World Computer
Vision Systems and Online Challenges (AML-CV) @ CVPRâ€™21. 1â€“5.

[18] David Lane, David Bisset, Rob Buckingham, Geoff Pegman, and Tony Prescott. 2016. New foresight review on robotics

and autonomous systems. Technical Report No. 2016.1. LRF. 65 pages.

[19] Han Liu, John Lafferty, and Larry Wasserman. 2007. Sparse nonparametric density estimation in high dimensions

using the rodeo. In Artificial Intelligence and Statistics. PMLR, 283â€“290.

[20] S. H. Lokerse, L. P. J. Veelenturf, and J. G. Beltman. 1995. Density Estimation Using SOFM and Adaptive Kernels. In
Neural Networks: Artificial Intelligence and Industrial Applications - Proceedings of the Third Annual SNN Symposium on

, Vol. 1, No. 1, Article . Publication date: May 2022.

Hierarchical Distribution-Aware Testing of Deep Learning

25

Neural Networks, Nijmegen, The Netherlands, September 14-15, 1995. Springer, 203â€“206.

[21] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu,
et al. 2018. Deepgauge: Multi-granularity testing criteria for deep learning systems. In Proce. of the 33rd ACM/IEEE Int.
Conference on Automated Software Engineering (ASEâ€™18). 120â€“131.

[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep
Learning Models Resistant to Adversarial Attacks. In 6th Int. Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.

[23] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated Whitebox Testing of Deep
Learning Systems. In Proceedings of the 26th Symposium on Operating Systems Principles, Shanghai, China, October
28-31, 2017. ACM, 1â€“18.

[24] Vincenzo Riccio and Paolo Tonella. 2020. Model-based exploration of the frontier of behaviours for deep learning
system testing. In ESEC/FSE â€™20: 28th ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, Prem Devanbu, Myra B. Cohen, and
Thomas Zimmermann (Eds.). ACM, 876â€“888. https://doi.org/10.1145/3368089.3409730

[25] Andrew Rosenberg and Julia Hirschberg. 2007. V-Measure: A Conditional Entropy-Based External Cluster Evaluation
Measure. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic. ACL, 410â€“420.
[26] Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. 2018. Reachability Analysis of Deep Neural Networks with
Provable Guarantees. In Proceedings of the Twenty-Seventh Int. Joint Conference on Artificial Intelligence, IJCAI-18. Int.
Joint Conferences on Artificial Intelligence Organization, 2651â€“2659.

[27] David W Scott. 1991. Feasibility of multivariate density estimates. Biometrika 78, 1 (1991), 197â€“205.
[28] Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, and Rob Ashmore. 2019. DeepConcolic:
testing and debugging deep neural networks. In Proceedings of the 41st Int. Conference on Software Engineering:
Companion Proceedings, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019. IEEE / ACM, 111â€“114.

[29] Felipe Toledo, David Shriver, Sebastian Elbaum, and Matthew B Dwyer. 2021. Distribution Models for Falsification and

Verification of DNNs. In IEEE/ACM Int. Conf. on Automated Software Engineering (ASEâ€™21).

[30] Benjie Wang, Stefan Webb, and Tom Rainforth. 2021. Statistically robust neural network classification. In Uncertainty

in Artificial Intelligence. PMLR, 1735â€“1745.

[31] Jingyi Wang, Jialuo Chen, Youcheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun, and Peng Cheng. 2021. Robot:
robustness-oriented testing for deep learning systems. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE). IEEE, 300â€“311.

[32] Jingyi Wang, Jialuo Chen, Youcheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun, and Peng Cheng. 2021. RobOT:
Robustness-Oriented Testing for Deep Learning Systems. In 2021 IEEE/ACM 43rd Int. Conf. on Software Engineering
(ICSEâ€™21). 300â€“311. https://doi.org/10.1109/ICSE43902.2021.00038

[33] Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. 2019. On the Convergence and
Robustness of Adversarial Training. In Proceedings of the 36th Int. Conference on Machine Learning, ICMLâ€™19, Vol. 97.
PMLR, Long Beach, California, USA, 6586â€“6595.

[34] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. 2004. Image quality assessment: from error

visibility to structural similarity. IEEE Trans. Image Process. 13, 4 (2004), 600â€“612.

[35] Stefan Webb, Tom Rainforth, Yee Whye Teh, and M. Pawan Kumar. 2019. A statistical approach to assessing neural

network robustness. In ICLRâ€™19. New Orleans, LA, USA.

[36] Lily Weng, Pin-Yu Chen, Lam Nguyen, Mark Squillante, Akhilan Boopathy, Ivan Oseledets, and Luca Daniel. 2019.
PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach. In ICMLâ€™19, Vol. 97. PMLR, 6727â€“
6736.

[37] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. 2018.
Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach. In 6th Int. Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
[38] Chenwang Wu, Wenjian Luo, Nan Zhou, Peilan Xu, and Tao Zhu. 2021. Genetic Algorithm with Multiple Fitness
Functions for Generating Adversarial Examples. In IEEE Congress on Evolutionary Computation, CEC 2021, KrakÃ³w,
Poland, June 28 - July 1, 2021. IEEE, 1792â€“1799.

[39] Xiaofei Xie, Tianlin Li, Jian Wang, Lei Ma, Qing Guo, Felix Juefei-Xu, and Yang Liu. 2022. NPC: Neuron Path Coverage
via Characterizing Decision Logic of Deep Neural Networks. ACM Trans. Softw. Eng. Methodol. 31, 3 (April 2022).
https://doi.org/10.1145/3490489

[40] Shenao Yan, Guanhong Tao, Xuwei Liu, Juan Zhai, Shiqing Ma, Lei Xu, and Xiangyu Zhang. 2020. Correlations between
Deep Neural Network Model Coverage Criteria and Model Quality. In Proc. of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020). ACM,
New York, NY, USA, 775â€“787.

, Vol. 1, No. 1, Article . Publication date: May 2022.

26

Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, and Xiaowei Huang

[41] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. 2020. A
Closer Look at Accuracy vs. Robustness. In Advances in Neural Information Processing Systems (NeurIPSâ€™20, Vol. 33),
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.). Curran Associates, Inc., 8588â€“8601.

[42] Bing Yu, Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, and Jianjun Zhao. 2021. DeepRepair: Style-Guided
Repairing for Deep Neural Networks in the Real-World Operational Environment. IEEE Tran. on Reliability (2021),
1â€“16.

[43] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. 2019. Theoretically
principled trade-off between robustness and accuracy. In International conference on machine learning. PMLR, 7472â€“
7482.

[44] J. M. Zhang, M. Harman, L. Ma, and Y. Liu. 2020. Machine Learning Testing: Survey, Landscapes and Horizons. IEEE

Tran. on Software Engineering (2020). https://doi.org/10.1109/TSE.2019.2962027 Early access.

[45] Xingyu Zhao, Alec Banks, James Sharp, Valentin Robu, David Flynn, Michael Fisher, and Xiaowei Huang. 2020. A
Safety Framework for Critical Systems Utilising Deep Neural Networks. In Computer Safety, Reliability, and Security
(LNCS, Vol. 12234), AntÃ³nio Casimiro, Frank Ortmeier, Friedemann Bitsch, and Pedro Ferreira (Eds.). Springer Int.
Publishing, Cham, 244â€“259.

[46] Xingyu Zhao, Wei Huang, Alec Banks, Victoria Cox, David Flynn, Sven Schewe, and Xiaowei Huang. 2021. Assessing
the Reliability of Deep Learning Classifiers Through Robustness Evaluation and Operational Profiles. In AISafetyâ€™21
Workshop at IJCAIâ€™21, Vol. 2916.

[47] Xingyu Zhao, Wei Huang, Sven Schewe, Yi Dong, and Xiaowei Huang. 2021. Detecting Operational Adversarial
Examples for Reliable Deep Learning. In 51th Annual IEEE-IFIP Int. Conf. on Dependable Systems and Networks (DSNâ€™21),
Vol. Fast Abstract.

[48] Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating Natural Adversarial Examples. In 6th Int. Conference
on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net.

[49] Yue Zhong, Lizhuang Liu, Dan Zhao, and Hongyang Li. 2020. A generative adversarial network for image denoising.

Multimedia Tools and Applications 79, 23 (2020), 16517â€“16529.

, Vol. 1, No. 1, Article . Publication date: May 2022.

