2
2
0
2

p
e
S
2
1

]

R
A
.
s
c
[

1
v
8
0
1
6
0
.
9
0
2
2
:
v
i
X
r
a

1

Bit-Line Computing for CNN Accelerators
Co-Design in Edge AI Inference

Marco Rios, Flavio Ponzina, Alexandre Levisse, Giovanni Ansaloni, David Atienza

Abstract—By supporting the access of multiple memory words at the same time, Bit-line Computing (BC) architectures allow the
parallel execution of bit-wise operations in-memory. At the array periphery, arithmetic operations are then derived with little additional
overhead. Such a paradigm opens novel opportunities for Artiﬁcial Intelligence (AI) at the edge, thanks to the massive parallelism
inherent in memory arrays and the extreme energy efﬁciency of computing in-situ, hence avoiding data transfers. Previous works have
shown that BC brings disruptive efﬁciency gains when targeting AI workloads, a key metric in the context of emerging edge AI
scenarios. This manuscript builds on these ﬁndings by proposing an end-to-end framework that leverages BC-speciﬁc optimizations to
enable high parallelism and aggressive compression of AI models. Our approach is supported by a novel hardware module performing
real-time decoding, as well as new algorithms to enable BC-friendly model compression. Our hardware/software approach results in a
91% energy savings (for a 1% accuracy degradation constraint) regarding state-of-the-art BC computing approaches.

Index Terms—Edge Artiﬁcial Intelligence, In-Memory Computing, Hardware/Software Co-Design, Convolutional Neural Networks,
Low-power Software Optimization.

(cid:70)

1 INTRODUCTION

T HANKS to their ability to extract abstract information

from raw data acquisitions, Machine Learning (ML)
algorithms such as Convolutional Neural Networks (CNNs)
are fostering a revolution in multiple and diverse ﬁelds,
ranging from personal mobility to health-care. Neverthe-
less, the increased accuracy of recent CNN models comes
at the cost of massive memory requirements and intense
workloads [1].

These downsides are particularly important for edge
devices running artiﬁcial intelligence algorithms, a scenario
named edge AI in literature [2]. Computational efﬁciency
is key in edge AI because applications often have to abide
to real-time constraints. Such constraints have to be met
within tight computing and energy budgets, commonly
orders-of-magnitude lower at the edge when compared to
the cloud, thus requiring careful optimization of hardware
and software. The main avenues towards the optimization
of ML workloads leverage the high level of parallelism and
robustness of these algorithms.

In the ﬁeld of CNNs, parallelism is enabled by
their structured and repetitive computing patterns, based
on Multiply-ACcumulate (MAC) instructions, millions of
which are employed to implement their convolutional and
fully connected layers. Indeed, a high degree of data reuse
is present both when convolving ﬁlters with activations
(in convolutional layers) and when executing matrix-vector
products (in fully connected ones). This characteristic can,
as we do in this papers, be effectively harnessed by Single
Instruction, Multiple Data (SIMD) computation strategies to
increase efﬁciency and performance [3].

• The authors are with the Embedded Systems Laboratory, ´Ecole Polytech-
nique F´ed´erale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne,
Switzerland.
E-mail: {marco.rios, ﬂavio.ponzina, alexandre.levisse, giovanni.ansaloni,
david.atienza}@epﬂ.ch.

Moreover, due to their robustness, CNNs can be op-
timized with very little, or no, accuracy drop, by either
reducing the amount of required MAC operations or simpli-
fying their computation. Quantization approaches advocate
the use of ﬁxed-point formats in contrast to ﬂoating-point,
enabling the use of only few bits to represent the parameters
(weights) and intermediate values (activations). Pruning
strategies focus at a coarser granularity, seeking to skip
weights and MAC computations that have little impact on
the output quality. As detailed in Section 2, pruning and
quantization are often combined in state-of-the-art edge AI
strategies.

Besides software optimization, the rise of edge AI has
motivated the computer architecture and hardware research
community to introduce dedicated designs. Approaches
range from processors-based solutions, such as the ultra-
low-power PULP multi-core in Rossi et al. [4], to custom
accelerators, e.g., Eyeriss in Chen et al. [3]. In this context,
In-Memory Computing (IMC) architectures are particularly
appealing, as computation inside memory avoids energy-
expensive data movements in-between processing and stor-
age components, while the parallelism made available by
the regular structure of memory banks presents a good
opportunity to support the SIMD patterns in CNNs.

A promising implementation of the IMC paradigm, that
we focus on in this manuscript, is that of Bit-line Computing
(BC) [5]–[9]. Based on conventional high-density SRAM,
BC architectures can be seamless integrated in CMOS tech-
nology and target different levels in memory hierarchies.
Moreover, BC supports a very high level of SIMD paral-
lelism through the use of multiple subarrays at the same
time. Finally, BC implementations require very little area
overhead at the memory periphery to implement shift-add
operations, which can then be chained to compute MACs.
The above-mentioned approach is particularly beneﬁcial
when small-bitwidth operands are considered, as those re-

 
 
 
 
 
 
quire a reduced number of shift-adds.

the

addresses

This work

fundamental

hard-
ware/software co-design challenge by providing a holistic
framework for the optimization, deployment, and execution
of CNN models on a BC architecture for edge computing.
We combine a novel BC-aware CNN optimization strategy
with a highly optimized BC architecture. Both support ﬁne-
grained quantization and pruning in fully connected and
convolutional layers. Moreover, leveraging the statistical
distribution of weight values in CNNs, our methodology
features a novel weight encoding strategy both during CNN
optimization and in the BC hardware implementation. The
strategy, named Generic Convolutional Weights (GCW)
encoding, uses few bits to encode weight values that appear
more frequently, and a higher number of bits for those
that are only rarely used, reducing model sizes by up to
4x in our experiments. A dedicated pipeline is in charge
of decompressing the model representation at run time,
without any impact on performance, converting it to a
sequence of BC operations. Operations are then executed
in parallel on multiple memory subarrays, greatly reducing
run-time.

In summary, the contributions of this paper are:
• We present a synergic hardware and software frame-
work that employs ﬁne-grained bitwidths, data com-
pression, and in-memory parallel computing to support
edge AI applications with a very high degree of energy
efﬁciency.

• We detail the characteristics of a novel BC circuit im-
plementation, able to effectively execute parallel in-
memory MAC operations among operands of varying
bitwidth.

• We introduce a strategy to automatically map on BC
subarrays the parameters of convolutional and fully
connected layers, maximizing operations parallelism
and data reuse.

• We present a CNN model compression, called GCW
coding, to compress CNN models losslessly. Also, we
describe a corresponding decoding circuit operating at
run-time, with little area and no timing overhead.
The remainder of the paper is organized as follows.
Section 2 discusses related works on CNN optimiza-
tion/compression and on IMC architectures. Section 3 in-
troduces an overall view of the proposed framework. Sec-
tion 4 focuses on the software optimization by detailing
the CNN optimization methodology and the compression
approach. Next, Sections 5 and 6 present the proposed BC
memory array architecture and the pipeline circuit for GCW
decoding, respectively. Section 7 describes the mapping of
convolutional and fully connected layers of CNNs on the BC
architecture. Section 8 provides details on the experimental
setup, while our achieved results are presented in Section 9.
Finally, Section 10 concludes the paper.

2 BACKGROUND

2.1 Convolutional Neural Networks

CNNs process input data employing a layer-based structure,
where increasingly more abstract features are extracted in
deeper layers. The compute-intense workload and large

memory requirements of CNNs are mostly due to convo-
lutional (CONV) and fully connected (FC) layers. For both
layer types, the number of MAC operations is related to the
size of input and output features, with several millions of
MACs being common requirements in recent CNNs [10].

2

(named ﬁlters),

convolved over

In CONV layers, three-dimensional matrices of CNN
three-
are
weights
input feature maps, producing one output
dimensional
element for each ﬁlter position in the input. Conversely,
FC layers (which are usually included after convolutional
ones) compute linear transformations, multiplying the input
feature vectors by the weight matrixes. FC and CONV
layers have different data access patterns. In contrast to
convolutions, weights in fully connected layers are used
only once during an inference, because each column of the
weight matrix multiplies the input vector to produce one
output element. This characteristic is taken into account in
the data mapping strategy discussed in Section 2.3. Recently,
few other works have proposed methods to map large CNN
layers in constrained IMC resources. As opposed to our
focus on BC architectures, they consider crosspoint arrays
[11], [12], and provide solutions only for convolutional
layers [13].

2.2 CNN models compression

Pruning and quantization are the most common approaches
exploiting the inherent redundancy in CNNs to reduce
their complexity, hence supporting their deployment in con-
strained devices. In pruning, either individual weights [14]
or entire convolutional ﬁlters [15] are removed, achieving
model compression higher than 10x [16]. Instead, in quanti-
zation techniques, the weights and activations comprising
CNNs models are represented using low-bitwidths inte-
ger data representations instead of ﬂoating-point numbers
[17]. Quantized CNN models have a smaller memory foot-
print than ﬂoating-point ones. Furthermore, they use less
complex integer hardware for computing MAC operations,
which also reduces energy requirements. It has been shown
that 8-bits quantization can be usually implemented without
affecting the initial CNN accuracy [18].

A further approach to compress CNN models is that of
weight encoding. It is often applied after quantization, as
low-bitwidth representations constrain the set of admissible
values. Encoding can be implemented employing different
strategies. Codebook-based strategies limit the number of
unique weights and store them in small look-up tables
(i.e., named codebooks), encoding the baseline model into
a set of binary indexes that address speciﬁc code-words
[19]. Another weight encoding approach is to leverage the
statistical distribution of weight values to compress their
representation, employing shorter code-words for the most
used values and longer code-words for rarer ones [20], [21].
Since encoding weight values does not change CNN
models, but only operates on data representation, it does
not degrade accuracy. On the other side of the coin, run-time
decoding may introduce overheads in time, area, and energy
requirements. The authors of [20] use Huffman codes to
index a codebook storing a constrained set of CNN weight
values. While our GCW strategy (detailed in Section 4) has
some similarities with respect to Huffman coding, it does

Table 1. Assignment of operands for fully connected and
convolutional layers

3

Layer type

Operand
Multiplicand
In-Memory Operand (IMO)
Multiplier
Broadcasted Operand (BO)

CONV

FC

Activations

Weights

Weights

Activations

decrease the performance or imply large area overheads.
Recent works [5], [6] have shown that such operations can
be reliably performed at high clock frequencies if the acti-
vated WLs belong to different subdivisions of SRAM arrays,
termed Local Groups (LGs). Only memory cells in the same
LG share short-distance vertical connections (local bit-lines,
LBLs). During a read access, cell values are propagated
via LBLs to the LG Periphery (LGP) circuit which features
sense ampliﬁers and read/write ports. In-memory bit-line
operations are then performed after those sense ampliﬁers,
effectively protecting memory cells from data corruption.

Further logic is then employed at the array periphery
to derive additions from bit-wise operations. Finally, multi-
plication instructions are performed as a sequence of shift-
adds between two memory words storing a) one of the
two multiplication operands and b) the accumulated partial
results (details are provided in Section 5.3).

This strategy differentiates between the two inputs of
a multiplication, as only one of them (In-Memory Operand,
IMO) resides in the memory array, while the other (Broad-
casted Operand, BO) is streamed in the memory one bit at
a time. Indeed, a single BO can be streamed to multiple
memory subarrays at once to perform multiplications in
parallel sharing one operand (hence its name). As shown
in Table 1, due to the differences in access patterns and to
maximize data reuse, in CONV layers we consider weights
as BOs and activations as IMOs, while for FC layers we
operate the opposite choice.

Although shift-adds require several cycles to execute
multiply or MAC instructions, high performance can be
achieved in BC arrays by (i) word-level parallelism inside
a single subarray, (ii) partitioning the SRAM into subarrays
to perform parallel operations, (iii) workload optimization
to reduce the bitwidth of streamed BO operands, reducing
the cycle-count of multiplications. The effectiveness of (i)
and (iii) is highly inﬂuenced by quantization and pruning,
as they determine the number of shift-adds required by a
multiplication. Nonetheless, current works do not explore
this interdependence in detail. As an example, [17] adopts
a ﬁxed 16-bits representation for activations and an 8-bits
one for weights. We instead show that important efﬁciency
gains can be harnessed when employing more aggressive
stances such as heterogeneous per-layer schemes, and that
these can be supported with little hardware overhead.

3 FRAMEWORK OVERVIEW

Figure 2 provides a bird’s eye view of our framework.
Starting from a (ﬂoating-point, non-optimized) description
of a CNN the framework provides a pathway to co-optimize
the CNN implementation and the BC computing array
executing it. It also optimizes the mapping of software onto
hardware.

Figure 1. Bit-line computing concept. Two word-lines are ac-
tivated in the same clock cycle. The discharge of the bit-lines
result in the bit-wise operations AND and NOR between the
accessed words.

not require explicit look-up tables, minimizing the cost of
its hardware implementation.

2.3 IMC architectures

In-Memory Computing (IMC) relies on smart memories to
perform computing tasks in a highly parallel way. IMC is es-
pecially attractive for applications that are computationally
and memory-intensive, while presenting regular patterns
and few (or no) data-dependent branches or loop-carried
dependencies. CNNs do present these characteristics, and
are therefore well-suited for IMC execution.

IMC architectures can be broadly divided into two cate-
gories. Crosspoint architectures employ meshes of variable
resistances to encode weights and arrays of analog-to-digital
converters to read-back results [22]. They can efﬁciently per-
form matrix-vector multiplications, but are prone to noise
due to manufacturing variability, temperature, and voltage
ﬂuctuations [23], [24]. Moreover, they require the challeng-
ing co-integration of digital, analog, and non-volatile tech-
nologies.

A second strategy, that we focus on in this paper, is
that of Bit-line Computing (BC) [25]. This approach can
be readily integrated into existing SRAM memories and
hierarchies by adding the capability to concurrently activate
multiple memory words and employing sense ampliﬁers to
perform bit-wise operations. As the name suggests, bit-line
Computing relies on the behavior of the bit-lines (BL and
BL) when two Word-Lines (WLs) are activated in the same
cycle clock.

Such a behavior is exempliﬁed in Figure 1, which shows
two SRAM bit-cells (each realized as two cross-coupled
inverters) connected to the same BLs. When the two WLs
of the cells are activated, if either one or both of the cells
store the value ’0’, the voltage on the BL wire discharges to
the ground through one or both of the cells access transistor
(M0) and the NMOS of their inverter. Notice that the only
case in which BL remains at Vdd is when both cells store the
value ’1’. Thus, the BL connection behaves as the logic AND
gate. Dually, the negated BL signal (BL) only retains a high
voltage if both cells store the value ’0’ (hence Q0 and Q1 are
both ’1’), implementing the functionality of a NOR gate.

This setup can induce data-corruption due to unde-
sirable currents ﬂowing between the bit-cells. In order to
address this issue, [25] reduces the operating frequency and
[26] adopts 10T bit-cells. However, these solutions either

(cid:31)(cid:30)(cid:30)(cid:29)(cid:30)(cid:28)0(cid:31)(cid:30)(cid:29)(cid:30)(cid:27)1(cid:31)(cid:30)(cid:30)(cid:29)(cid:30)(cid:27)0(cid:31)(cid:30)(cid:30)(cid:29)(cid:30)(cid:28)1(cid:26)(cid:25)(cid:30)(cid:29)(cid:30)(cid:31)(cid:27)(cid:30)(cid:24)(cid:30)(cid:31)(cid:28)(cid:23)(cid:25)(cid:27)(cid:23)(cid:25)(cid:28)(cid:31)1(cid:31)0(cid:26)(cid:25)(cid:26)(cid:25)(cid:26)(cid:25)(cid:30)(cid:29)(cid:30)(cid:31)(cid:27)(cid:22)(cid:30)(cid:31)(cid:28)(cid:21)(cid:20)(cid:20)(cid:30)(cid:30)(cid:30)(cid:30)(cid:30)(cid:19)(cid:18)(cid:20)VDD     GND(cid:27)(cid:27)(cid:28)(cid:28)(cid:27)(cid:28)(cid:27)(cid:28)(cid:27)(cid:27)(cid:27)(cid:28)(cid:28)(cid:27)(cid:27)(cid:27)(cid:17)(cid:27)(cid:17)(cid:28)(cid:17)(cid:27)(cid:17)(cid:28)4

Figure 2. Overall view of our HW-SW co-design framework,
showing algorithmic optimizations (left), mapping, and execu-
tion on BC hardware (right).

Application-level optimizations, detailed in Section 4,
combine non-uniform quantization schemes and encod-
ing methods to obtain an optimized model that can be
efﬁciently executed in-memory. In more detail, we ﬁrst
include a resource-aware CNN quantization strategy that
reduces workload and memory requirements (Figure 2.A).
This stage consists in an iterative approach, where the
bitwidth of weights and activations in convolutional and
fully connected layers is optimized while abiding by an ac-
curacy threshold. Then, due to the characteristic distribution
of CNN weights, GCW encoding further compresses the
quantized CNNs (Figure 2.B). GCW uses smaller bitwidths
for the weight values appearing more frequently and larger
bitwidths for those appearing only sporadically.

Because of the typically large size of intermediate fea-
tures in CNNs, we decompose convolutions and the matrix-
vector operations in FC layers into smaller computing
blocks (Figure 2.C). This tilling process, focus of Section 7,
allows large CNN models to be accelerated in limited-sized
memories, minimizing the number of data transfers while
exploiting parallelism to increase performance.

When processing convolutional layers, weights are de-
coded at run-time during execution, with minimal overhead
(Figure 2.D). We describe the circuit performing weights
decoding in Section 6. The decoder ﬁrst translates param-
eters to their two’s complement representations, and then
converts them to a sequence of BC instructions, which
govern the execution of our memory arrays (Figure 2.E). The
design of the BC arrays and their features, including support
for heterogeneous quantization, are detailed in Section 5.

4 ALGORITHMIC-LEVEL CNN OPTIMIZATION

Algorithmic optimizations (Figure 2.A) aim at decreasing
the workload and memory requirements of a CNN model
with BC-aware transformations. The optimization process
consists of two stages: ﬁrst, a heterogeneous quantization
and pruning step reduce the bitwidth and the number of
weights and activations in convolutional and fully con-
nected layers. Next, the quantized weights are encoded
using variable-bitwidth codes, further reducing memory
requirements.

Figure 3. Workload-aware quantization and pruning method-
ology (left). Running example (right).

4.1 Heterogeneous Quantization

Per-layer quantization enables aggressive model compres-
sions. Indeed, layers in CNN may (and, usually, do) have
different degrees of robustness, with quantization-sensitive
layers requiring larger bitwidth for activations and param-
eters. Heterogeneous schemes have therefore the potential
to reach better trade-offs between accuracy and model size.
However, they also expose a much larger design space than
uniform alternatives.

To navigate it, we introduce an iterative process that
reduces the size of in-memory and broadcasted operands
(IMOs and BOs, as deﬁned in Table 1), according to the
scheme in Figure 3. As a running example, the ﬁgure con-
siders in its rightmost part a running example referring to
a CNN with two convolutional and one fully connected
layers, which we will follow in the rest of this section.

The input models in our optimization ﬂow are homo-
geneously quantized CNNs, employing 16-bits IMOs and
8-bits BOs (Figure 3(a)). As shown in [17], CNNs at these
quantization levels have indistinguishable accuracies with
respect to ﬂoating-point implementations. In the ﬁrst opti-
mization step (Figure 3(b)), the bitwidth of the BOs is inde-
pendently tailored for each layer. To this end, we attempt to
reduce bitwidths starting from the layer having the highest
number of MAC operations (and, therefore, the highest
potential for savings). Then, we retrain the network for a
small number of epochs and check the obtained accuracy of
the new conﬁguration. If the accuracy degradation exceeds
a user-deﬁned threshold, we backtrack (we considered 1%
and 5% maximum degradations for the experiments in
Section 9) . In a similar fashion, we then iteratively target the
layers having the second, third, etc. most numerous MAC
operations. Once all layers have been processed once, we
further try to reduce the bitwidth of the BOs in the highest-
workload layer from which we haven’t previously back-
tracked. The iteration continues until no further bitwidth
reductions are possible in the BOs of any layer.

This phase is followed by a ﬁlter-level optimization,

Run-time inferenceOffline CNN OptimizationCNN Weights Encoding(Section 4.2)Heterogeneous Quantization (Section 4.1)CNN Weights Decoding(Section 6)ADIn-Memory Execution(Section 5)CNN Layers Mapping(Section 7)BCE•Broadcasted operands: 8 bits•In-memory operands: 16 bits•Layer-based quantization•Target: broadcasted operands•2 ≤ N≤ 8 quant. bits•Filter-based Optimization•Redundant bits removal•Layer-based quantization•Target: in-memory operands•N=8 or N=16 quant. bitsd) In-memory operands optimizationa) Uniform Quantizationc) Filter-level optimizationLayerWeightsActivationsCONV816CONV816FC168CNN Structureb) Broadcasted Operandsoptimization LayerWeightsActivationsCONV416CONV516FC168CNN StructureLayerWeightsActivationsCONVMixed16CONVMixed16FC163CNN StructureLayerWeightsActivationsCONVMixed8CONVMixed16FC83CNN Structure5

Figure 4. Weights distribution in the three convolutional layers
of LeNet-5.

which focuses on convolutional layers only. We observe
that a large amount of CNN ﬁlters do not use the entire
value range when representing weights, especially after the
above-mentioned aggressive quantization. As illustrated in
Figure 3(c), we hence drop, without loss of accuracy, the
most signiﬁcant bits (MSbs) on a per-ﬁlter base if allowed
by weight ranges, correspondingly scaling convolution out-
puts. For example, if the value range of the BOs in a ﬁlter is
⊂ [−0.25, 0.25), 2 MSbs can be dropped, and outputs should
be divided by 2Dropped bits = 4. Additionally, in this stage,
ﬁlters having all weights equal to zero are entirely deleted.
The last step of the optimization ﬂow (Figure 3(d)) per-
forms the tailoring of the IMOs bitwidths. It leverages
the word-level parallelism supported by the BC arrays (as
described in Section 5.4). Similarly to the approach followed
for BOs, we attempt to reduce the bitwidth of IMOs on a per-
layer basis. However, quantization steps are coarser in this
case, as they must abide to the sub-word formats supported
in hardware. In our experiments, we admit 1x16-bit and
2x8bit sub-words, with the latter resulting in 2x reduction
in execution time1.

4.2 Generic Convolutional Weights Encoding

GCW compression further reduces the memory required to
represent parameters in convolutional layers. Our strategy
leads to an efﬁcient implementation of run-time decoding,
which we describe in Section 6. It takes advantage of the
limited set of values that can be assumed by quantized
weights, as well as their characteristic statistical distribution.
We observe that weights in quantized CONV layers
predominantly assume the value ”0”, even after removing
ﬁlters only containing zero values. This trend is illustrated
in Figure 4, which provides as examples the weights distri-
bution in the three convolutional layers of LeNet-5 2.

Additionally, narrow distributions centered around zero
indicate that small (quantized) weight values appear much
more frequently than larger-magnitude ones. These ﬁndings
open the path for an encoding scheme that, similarly to
Huffman coding, employs code-words of variable length.
Highly occurring values are coded using low-bitwidth rep-
resentations, while less frequent values are mapped to

1. While in principle our approach could be extended to 4 bits or 2
bits per word, such settings would incur in unacceptably large accuracy
degradations.

2. We obtained similar results for the other benchmark CNNs in

Section 9.

Figure 5. Generic Convolutional Weights coding scheme, where
N indicates the quantization level values before coding. The
code uses fewer bits to represent the most frequent symbols.

higher bitwidth codes. Given the distributions of CNN
weight values, this choice translates into employing the
minimal bitwidth (1 bit) for the value ”0”, small bitwidths
for the values close to zero and large bitwidths for values of
high magnitude.

The GCW encoding scheme is illustrated in Figure 5,
with N being the number of quantized bits. Weights are
assumed to be normalized in the range [−1, 1). They are
divided into ﬁve intervals, symmetric with respect to zero
(as shown in the leftmost column). Values close to zero
(other than zero itself) are represented with 5-bit code-
words. A 1-bit preﬁx is appended to the two’s complement
4-bit representation of the corresponding value.

Other, seldomly used, code-words are derived by ap-

pending a ﬁxed 5-bit preﬁx to their N -bit representation.

For N = 8, 13 bits are required to represent large-
magnitude values, but only 5 bits for low-magnitude ones
(and only 1 bit for the value ’0’). Note that, for N < 5,
long code-words are never generated and the coding only
generates different code-words lengths for zero and non-
zero weights.

5 BIT-LINE COMPUTING ARCHITECTURE

5.1 BC array organization

As commonly done in standard SRAMs, the BC array ar-
chitecture is divided into several subarrays. Each subar-
ray is capable of performing a MAC operation between
a broadcasted operand (which must be the same for all
subarrays) and an in-memory operand, local to each sub-
array. Figure 6(a) shows the subarray organization. The
Word-Lines (WLs) are evenly separated into Local Groups
(LGs), which contain the memory cells and the LG Periph-
ery (LGP). The role of the LGP is to electrically isolate
SRAM cells from each other, allowing in-memory opera-
tions among words in different local groups at high speed
without data corruption. LGPs logic also performs bit-wise
negation and shift operations, as detailed in the following.
At the bottom of the subarray, the Bit-line Computing Unit

DecimalValue (A) CodeSize[-9, 2        ]10000 & bin    (A)N+51 &  bin    (A)50011 &  bin    (A)510000 &  bin    (A)N+5GCW CodingNN44(N-1)2(N-1)[ -1,  -8 ]2(N-1)[ 1,  7 ]2(N-1)[ 8, 2        -1](N-1)2(N-1)6

(BCU) ripples a carry signal among bit columns, allowing
the implementation of additions. Subarray operations are
governed by a small controller, global to the entire array,
which decodes BC operations and properly activate WLs.

Figure 6(b) highlights a single bit-column of the subar-
ray, in which bit-cells are organized in Local Groups (LGs).
Moreover, words in a LG are arranged with way interleav-
ing (i.e., the same bit of multiple words are placed next to
each other)3. Bit-cells belonging to the same local group and
the same way share, via access transistors, share the same
Local Bit Lines (LBLs). LBLs in a LG are connected to a mul-
tiplexer that acts as a way selector, allowing to connect LBLs
to the sense ampliﬁers of the LG periphery and, through its
read ports, to global bit-lines (GBLs). Finally, GBLs interface
with the BCU, which, besides standard SRAM read and
write ports, implements a 1-bit adder, rippling the carry in
and out from/to neighboring bit-columns.

Our design based on way interleaving allows the mem-
ory array to be designed with high-density push-rule 6T
bit-cells, while still maintaining enough clearance for the
design of the LGP and BCU blocks, which must be vertically
aligned with storage cells.

5.2 In-Memory Operations

The design illustrated above supports both standard read-
write and in-memory operations, as detailed next.

5.2.1 Write

Write operations are performed by setting up the data on the
GBLs through the write ampliﬁer, which chooses data from
two sources: the external input (DATA IN in Figure 6) or the
result of a performed BC operation. Thereon, the LGP write
port 4 transfers the data from the GBLs to the LBLs. Finally,
the WL is activated, switching on the access transistor and
allowing the data to be written into the target SRAM bit-cell.

5.2.2 Read
Both LBLs and GBLs are pre-charged to Vdd before reading
a word from the SRAM memory. Then, the WL is activated,
switching on the access transistor and allowing one of the
LBLs to discharge to the ground. Conversely, the other holds
its charge, depending on the value stored in the bit-cell. The
LBL Sense Ampliﬁer (LBL-SA) consequently asserts a logic
value and this signal is connected to the read port. GBLs
assume the voltage value of the LBLs connected to the read
ports, and the GBL Sense Ampliﬁer (GBL-SA) asserts such
value and outputs the read word.

5.2.3 BC Instructions
BC instructions always assume the format of add(OP1,
OP2), with the operands being allowed to be individually
shifted and/or negated. OP2 can be set to zero if only
shift/negation of one operand is required. The operation
outcome can be either output from the memory or writ-
ten back, which requires an additional cycle. shift and
negation operations are performed on each operand in
the LGP by the read ports, allowing the operands to be indi-
vidually shifted/negated before the addition is performed.

3. For clarity, in Figure 6(b) we show an example with only two ways
4. Not depicted in Figure 6

Figure 6. (a) Bit-line Computing subarray. (b) Highlight of one
bit-column depicting the Local Group circuitry and Bit-line
Computing Unit. (c) 8-bits two-ways interleaving.

(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:30)(cid:23)(cid:22)(cid:26)(cid:21)(cid:20)(cid:26)(cid:19)(cid:18)(cid:17)(cid:16)(cid:29)(cid:30)(cid:15)(cid:17)(cid:24)(cid:30)(cid:27)(cid:27)(cid:14)(cid:24)(cid:13)(cid:30)(cid:24)(cid:12)(cid:11)(cid:27)(cid:18)(cid:15)(cid:14)(cid:13)(cid:30)(cid:24)(cid:12)(cid:11)(cid:27)(cid:18)(cid:15)(cid:14)(cid:10)(cid:28)(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)(cid:31)(cid:4)(cid:31)(cid:31)(cid:4)(cid:31)(cid:8)(cid:7)(cid:6)(cid:5)(cid:31)(cid:4)(cid:31)(cid:31)(cid:4)(cid:31)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:31)(cid:3)(cid:26)(cid:2)(cid:14)(cid:24)(cid:18)(cid:22)(cid:1)(cid:14)(cid:24)(cid:127)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:4)(cid:31)(cid:26)(cid:129)(cid:23)(cid:27)(cid:17)(cid:18)(cid:22)(cid:27)(cid:14)(cid:141)(cid:14)(cid:24)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:4)(cid:31)(cid:26)(cid:129)(cid:23)(cid:27)(cid:17)(cid:18)(cid:22)(cid:27)(cid:14)(cid:141)(cid:14)(cid:24)(cid:3)(cid:4)(cid:31)(cid:3)(cid:4)(cid:31)(cid:10)(cid:19)(cid:9)(cid:3)(cid:4)(cid:31)(cid:11)(cid:8)(cid:6)(cid:143)(cid:26)(cid:19)(cid:18)(cid:17)(cid:144)(cid:157)(cid:20)(cid:144)  €(cid:2)(cid:24)(cid:14)(cid:144)(cid:1)(cid:28)(cid:24)(cid:25)(cid:14)(cid:20) (cid:7)(cid:6)(cid:20)‚(cid:6)‚‚(cid:7)ƒ(cid:6)‚„(cid:24)(cid:18)(cid:17)(cid:14)(cid:26)(cid:6)(cid:129)(cid:22)(cid:3)(cid:4)(cid:31)(cid:3)(cid:4)(cid:31)‚(cid:6)€(cid:6)…(cid:157)(cid:20)(cid:4)(cid:144) (cid:31)(cid:4)(cid:31)(cid:11)(cid:8)(cid:6)(cid:2)(cid:24)(cid:14)(cid:144)(cid:1)(cid:28)(cid:24)(cid:25)(cid:14)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)(cid:31)(cid:3)(cid:26)(cid:2)(cid:14)(cid:24)(cid:18)(cid:22)(cid:1)(cid:14)(cid:24)(cid:127)(cid:7)ƒ(cid:6)‚(cid:2) (cid:7)€(cid:7)ƒ(cid:6)‚(cid:2) (cid:7)€(cid:6)†(cid:6)‡(cid:6)ˆ(cid:6)‰(cid:6)Š(cid:6)‹(cid:6)Œ(cid:6)Ž(cid:4)†(cid:4)‡(cid:4)ˆ(cid:4)‰(cid:4)Š(cid:4)‹(cid:4)Œ(cid:4)Ž(cid:6)†(cid:6)‡(cid:6)ˆ(cid:6)‰(cid:6)Š(cid:6)‹(cid:6)Œ(cid:6)Ž(cid:4)†(cid:4)‡(cid:4)ˆ(cid:4)‰(cid:4)Š(cid:4)‹(cid:4)Œ(cid:4)Ž(cid:10)(cid:29)(cid:9)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:30)(cid:23)(cid:22)(cid:26)(cid:5)(cid:4)(cid:18)(cid:17)(cid:11)(cid:27)(cid:18)(cid:15)(cid:14)(cid:26)(cid:144)(cid:30)(cid:129)(cid:22)(cid:23)(cid:17)(cid:18)(cid:15)(cid:25)(cid:26) (cid:15)(cid:18)(cid:17)(cid:26)(cid:10)(cid:4)(cid:144) (cid:9)(cid:8)(cid:7)(cid:6)(cid:5)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:3)(cid:24)(cid:30)(cid:23)(cid:22)(cid:26)(cid:5)(cid:31)(cid:4)(cid:31)(cid:31)(cid:4)(cid:31)(cid:8)(cid:7)(cid:6)(cid:5)(cid:31)(cid:4)(cid:31)(cid:31)(cid:4)(cid:31)„(cid:28)(cid:127)(cid:26)(cid:21)„(cid:28)(cid:127)(cid:26)(cid:143)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:3)(cid:24)(cid:30)(cid:23)(cid:22)(cid:26)(cid:21)„(cid:28)(cid:127)(cid:26)(cid:21)„(cid:28)(cid:127)(cid:26)(cid:143)7

Figure 8. Binary integer multiplication example, between the
IMO (001001102 = 3810) and the BO (100112 = 1910), the multi-
plication is decomposed in shift-add instructions. LSh opera-
tions are left-shifts.

while each bit of the BO dictates which operation should be
done at each step.

5.3.2 Multiplication in the BC array

The approach to multiplication illustrated above can not be
executed as-is in the BC architecture. First, in Figure 8, the
product (10110100102 = 72210) requires more bits (10) com-
pared to both of the operands (5 and 8-bits), which would
result in an overﬂow in our BC scenario. Second, no support
is provided for representing negative numbers, common in
AI applications.

These two issues are addressed by supporting MAC
operations in ﬁxed-point (instead of integer) format, among
signed numbers encoded in two’s complement. We repre-
sent values using 1 bit for the integer part, and n bits for
the fractional part (a format commonly indicated as Q1.n).
Thus, the values are always in the range [−1, 1). Since
two’s complement is used, the most signiﬁcant bit is ‘1’ for
negative numbers and ‘0’ for positive ones.

Multiplication on the BC architecture can then be per-
formed by employing sequences composed by the following
two instructions:

(i) Addition of two operands, both arithmetically right-
shifted (i.e., with sign extension) by one bit. One
operand is the partial product ACC, while the second
is either 0 or the IMO, depending on the BO bit.

(ii) Addition between two operands, where one operand is
the partial product ACC and the other is either 0 or the
2’s complement of the IMO.
Instruction (i) is performed at every iteration step from
bit 0 to bit N − 1 of the BO, while operation (ii) is used for
bit N of BO (its most signiﬁcant bit).

Figure 9(a) shows an example of a multiplication among
two two’s complement numbers in the Q1.7 and Q1.4 for-
mats, with the result in Q1.7. Binary digits are the same
as the ones depicted in Figure 8, however, in Figure 9 the
associated values are now IMO = 001001102 = 0.29687510
and BO = 100112 = -0.812510.

Figure 9(b) shows the operations required to perform the
multiplication, in the case of only supporting shifts of 1 bit
(NES = 1). Notice that the bitwidth of the partial products
does not increase at each iteration, and no overﬂow occurs.
Instead, truncations induce small errors in the computed
product (0.4% in the example).

Crucially, all the listed iterations include operations
which are supported in the BC array, and each iteration can
be executed in a single clock cycle. As presented in Figure 7,

Figure 7. (a) Local Group Periphery circuit with extended read
port, enabling shift and negation. (b) Data path of embedded
shift.

Figure 7(a) presents a schematic of the read port circuitry.
It offers two paths to discharge the GBLs to the ground,
where each path is composed of two NMOS in series.
Conventional read operation uses the path controlled by the
Read Enable (RD EN) signal and the LBL-SA output of the
same bit. Shifts instead rely on the path controlled by the
Shift Enable (SH EN) signal and the LBL-SA output of a
neighbour bit. This operation is deﬁned as Embedded Shift
(ES), since it is performed inside the LGP. Indeed, single-
cycle, multiple-shifts operations can be supported in read
ports by adding additional discharge paths from LBL-SA of
farther bit columns, accomplishing design with a varying
Number of Embedded Shifts (NES).

The LGP also embeds 2-to-1 multiplexers connected to
both outputs of the LBL-SA. This design allows bit-wise
negations to be supported in the LGP, as for this operation
the multiplexers invert the LBL and LBL. At the subarray
level, values can consequently be arithmetically negated
(i.e., in two’s complement representation), by performing
bit-wise negation in the LGP and asserting the carry-in of
the least signiﬁcant bit in the BCU.

5.3 Multiplications

5.3.1 Multiplication among binary numbers with partial
products

In general terms, multiplications of two operands can be
decomposed into partial products, which are summed up to
retrieve the full product. Each partial product is found by
multiplying each digit of the multiplier with the multipli-
cand and shifting the result to the left based on the position
of this digit. In binary systems, the digits of the multiplier
are either zero or one, thus, the partial products are either
zero or the left-shifted multiplicand.

Figure 8 illustrates an example that

shows a bi-
nary multiplication of
the IMO
(001001102 = 3810) and the BO (100112 = 1910). Figure 8 also
shows the required operations, where ACC is the partial
products at each iteration. It can be noticed that operations
are only performed with the IMO and product vector ACC,

two input operands,

RD_ENRD_ENGBL<N>GBL<N>SH_ENSH_ENLocal BL multiplexerLBL<N>       LBL<N>GBL<N-1>GBL<N-1>LG0LG1BCUSH_EN(a)(b)READ PORT(cid:31)(cid:30)(cid:29)1 bit1 bit(cid:31)(cid:30)(cid:30)(cid:29)(cid:31)(cid:28)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:26)(cid:26)(cid:20)(cid:19)(cid:18)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)(cid:20)(cid:19)(cid:18)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)(cid:20)(cid:19)(cid:18)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)(cid:31)(cid:30)(cid:30)(cid:29)(cid:31)(cid:28)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:26)(cid:26)(cid:26)(cid:26)(cid:17)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)(cid:17)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)(cid:31)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:31)(cid:31)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:31)(cid:31)(cid:31)(cid:17)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)(cid:31)(cid:31)(cid:31)(cid:31)(cid:17)(cid:17)(cid:16)(cid:17)(cid:16)(cid:16)(cid:17)(cid:16)(cid:17)(cid:17)(cid:16)(cid:17)(cid:15)(cid:14)(cid:26)(cid:13)(cid:26)(cid:15)(cid:12)(cid:11)(cid:10)(cid:11)(cid:12)(cid:26)(cid:9)(cid:12)(cid:12)(cid:29)(cid:20)(cid:19)(cid:8)(cid:22)(cid:16)(cid:16)(cid:17)(cid:17)(cid:29)(cid:24)(cid:19)(cid:8)(cid:22)(cid:16)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:9)(cid:7)(cid:6)(cid:10)(cid:15)(cid:12)(cid:16)(cid:17)(cid:16)(cid:5)(cid:26)(cid:13)(cid:26)(cid:16)(cid:7)(cid:11)(cid:12)(cid:11)(cid:16)(cid:29)(cid:9)(cid:12)(cid:12)(cid:26)(cid:13)(cid:26)(cid:6)(cid:16)(cid:12)(cid:11)(cid:16)(cid:12)(cid:14)(cid:11)(cid:7)(cid:10)(cid:11)(cid:16)(cid:7)(cid:11)(cid:12)(cid:22)(cid:4)(cid:31)(cid:30)(cid:30)(cid:29)(cid:31)(cid:28)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:26)(cid:26)(cid:20)(cid:19)(cid:18)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)IMOBO8

Figure 9.
(a) Two’s complement ﬁxed-point multiplication
example between the IMO expressed in Q1.7 and the BO
expressed in Q1.4. The BC instructions for (b) NES = 1 and
(c) NES = 3. RSh operations are right-shifts.

right-shifts are provided by read ports in Local Groups,
and can execute concurrently with additions, since the BCU
computes additions at the subarray periphery. Similarly, the
addition of the partial product with the two’s complement
of the multiplicand, also requires one clock cycle as the
bit-wise negation is done in the LG periphery, while the
addition and the assertion of the carry-in is executed in the
BCU.

5.3.3 Using Multiple Embedded Shifts

The support for multiple embedded shifts (NES>1) can
effectively speed-up computation, at the cost of added com-
plexity in the read port. Indeed, in the case of NES = 2, 2
bits of the BO can be processed in one iteration, provided
that the ﬁrst one is equal to zero (“00” and “01”), since
in this case only one addition, or none, is required after
two shifts. For NES = 3, BO sequences of bits with two
leading zeroes (“000” and “001”) can be processed in a
single clock cycle. Figure 9(c) shows the BC instructions to
execute the same multiplication as Figure 9(b), but on a BC
array with NES = 3. In this conﬁguration, only 3 operations
are required (instead of 5 when NES = 1), resulting in a
speed-up of 1.67x. A large number of supported NES can
provide diminishing beneﬁts, as long sequences of zeroes
in the BO can be rare. Overly large NES values may also
be detrimental, as they impact the complexity of LGP read
ports [7].

Figure 10. (a) Word-level parallelism support for 1x16bit and
2x8bit modes, considering NES = 1. (b) H-tree organization.

5.4 In-Memory Parallelism

5.4.1 Word-level Parallelism

The BC architecture allows to store in each memory location
either a single 16-bit word (in Q1.15) or two 8-bit ones (each
in Q1.7) 5. In-memory instructions can hence be executed on
1x16bit or 2x8bit word formats, enabling in the latter case
two multiplications simultaneously in a single subarray.

To implement word-level parallelism, connections be-
tween the 7th and 8th bit-column are conﬁgurable. In par-
ticular, right shift operations on the 7th bit-column can
either be connected to the 8th bit-column (in 1x16bit mode),
or to itself (to implement sign extension in 2x8bit mode).
Similarly, the carry-in of the 8th bit-column can be either
connected to the 7th bit-column (1x16bit mode), or dictated
by the performed operation to implement two’s comple-
ment (2x8bit mode).

Hence, two additional multiplexers are required (H1 and
H2 in Figure 10). The ﬁrst one, at LG peripheries, connects
or disconnects the shift-right signal from the 8th to the 7th
bit-column, while the second, in BCUs, operates similarly
for the carry signal between the 7th and the 8th bit columns.

5.4.2 Array-level Parallelism

Multiple subarrays are connected in an H-tree conﬁguration,
as shown in Figure 10(b). This organization ensures that all
the signals transmitted from/to the array periphery have
the same distance to all the subarrays, equalizing the delays
and minimizing critical paths.

The H-tree interconnect is active before and after compu-
tation to transfer IMO inputs to subarrays, and retrieve re-
sults. During computation, the H-tree broadcasts one BC in-

5. Word-level parallelism does not impact normal (non-BC) reads and

writes.

00100110IMO (Q1.7)2222222201234567-------(-1). 000100110000000000 000010011 000100110000011100000000000000001110000000000000001111101101011100001(cid:31)(cid:30)(cid:31) 11001add(RSh(ACC), RSh(IMO))BC Instructions (NES = 1):add(ACC, 2sComp(IMO))0.296875 - 0.812510011BO (Q1.4) 000100110000000000 000010011 0001001100000011100(cid:31)11001add(RSh(ACC), RSh(IMO))add(2x RSh(ACC), 2sComp(IMO))11011010Final product (Q1.7) :  BOOperations11100001Final product (Q1.7) :  BC Instructions (NES = 3):BOOperations(cid:31)(cid:30)(cid:29)(cid:31)(cid:28)(cid:29)add(RSh(ACC), RSh(IMO))add(RSh(ACC), 0)add(RSh(ACC), 0)add(RSh(ACC), RSh(IMO))- 0.2421875 11100001Product (Q1.7)(0.296875)  X   (- 0.8125) = -0.24121094 (0.4% error)  (cid:30)(cid:31)(cid:30)(cid:31)(cid:30)(cid:31)(cid:30)(cid:31)(cid:30)(cid:31)(cid:31)(cid:27)(cid:29)LSbLSb(cid:31)(cid:30)(cid:29)Htree - 64 subarrays4 subarrays8 subarrays16 subarrays32 subarrays(cid:31)(cid:28)(cid:29)LG0SRAMLGPbit<7>sh<8>C<7>H1BCUC<6>sh<7>Programmable Carry-inC<8>H2BCULGPbit<8>sh<9>LG0SRAMCarryOutCarryInCarryOutCarryInbit<6:0>bit<15:9>9

Figure 11. (a) Block diagram of the pipeline circuit to extract BC operations from the compressed CNN model. (b) GCW decoder
circuit-level design (c) GCW decoding example with N = 6.

struction related to BO bits to all subarrays at each clock cy-
cle, exploiting the reuse of BOs in multiple MAC operations
to achieve a high degree of parallelism. Importantly, this
approach allows a ﬁne-grained ﬂexibility in the bitwidth
of BOs, which may assume arbitrary values, leading to a
ﬁne-grained control of trade-offs between model cost (size,
energy, time-per-inference) and accuracy.

6 REAL-TIME DATA DECOMPRESSION
To reduce the size of CNN models, hence memory require-
ments, convolutional weights are stored in an encoded form,
according to the Generic Convolutional Weights (GCW) rep-
resentation introduced in Section 4.2. At run-time, a pipeline
circuit decodes them, deriving the corresponding BC in-
structions. Figure 11(a) depicts the pipeline stages of the
decoder. First, a shift register reads a memory word contain-
ing multiple GCW-encoded weights, possibly of different
bitwidths. Then, the GCW decoder decompresses weights
into their Q1.n representation. Finally, the BC instruction
decoder converts these values into a set of BC instructions,
which are broadcasted to the subarrays.

6.1 Shift Register

For each convolutional ﬁlter, GCW-encoded weights form a
bit-stream where each value is represented with 1 bit (for
the ”0” value), 5 bits (for values close to 0), or up to 13 bits
(otherwise). Hence, the shift register, which is ﬁlled by 32-
bits memory words at a time, usually holds multiple GCW
code-words. When the decoder requires a new weight, it
examines the ﬁrst 13 bits of the shift register (GCW<12:0>
in Figure 11(a)), determining which bit-ﬁeld blue(ﬁrst bit,
ﬁrst 5 bits, or ﬁrst 5+N bits, where N is the quantization
level) contains the next code-word and advances the shift
register according to the code length. The shift register
is re-ﬁlled when less than 13 bits remain in its buffer,
concatenating memory words. Code-words can hence cross
the boundary of two subsequent memory words, preventing
memory under-utilization.

6.2 GCW Decoder

The GCW decoder decodes the weights values according
to their quantization level N , as illustrated in Figure 5. It
analyzes GCW<12:0>, searching for speciﬁc bit sequences.
The circuit-level design of the GCW is depicted in
Figure 11(b). It comprises two multiplexers 3-to-1 (M1 and
M2), which share the same selection signal (sel<0:1>).
The signal sel<0> is directly connected to GCW<12>,
while sel<1> is the output of a 4-input NOR gate
(GCW<11:8>). When sel<0> = 0, the weight value is
zero, encoded using a 1-bit code. Instead, whensel<0> =
1, sel<1> controls the output of the multiplexers. If the
GCW<11:8> bits are different from “0000”, they represent
a small-magnitude weight encoded using a 5-bit code. The
corresponding value is GCW<11:8>, sign-extended to N
bits (SigExt). Finally, if GCW<11:8> = “0000”, the code-
word represents a large-magnitude weight, whose value is
encoded in the GCW<7:8-N> bits using a variable code
length that varies from 9 to 13.

Figure 11(c) illustrates an example of run-time decod-
ing considering N = 6. In the ﬁrst one, GCW<12> = 0.
Consequently, the weight value is zero, encoded using 1
bit. Thus, the shift register shifts one position, allowing the
next weight to be decoded. Next, in the second example,
GCW<12:8> = ”10110”. The GCW decoder extracts the last
4 bits and concatenates them with “00” to form a 6-bits
binary value. The third example shows a similar case, but,
as the read value is negative, “11” is concatenated. The last
example depicts the case where the GCW decoder ﬁnds
the sequence “10000”. The binary value is then retrieved
in GCW<7:2>.

6.3 BC instruction decoder

The BC instruction decoder converts the weights’ bits into
BC instructions, deﬁning the RightShift, the Add, and
the 2sComp signals. It also governs the write back signal to
store the result of in-memory operations in the BC arrays.

Shift RegisterMemorySubarraycontrollerData<31:0>GCW<12:0>GCWDecoderCode lengthValue<N-1:0>RightShift Add2sCompWrite backClkNew weightClkBC InstructionDecoderGCW<12:0>Value<5:0>0101101100010000000101101100010000011011000100000101110001000001000100010001(a)(b)GCW Decoder (N = 6)N155 + NGCW<12>GCW<11>GCW<10>GCW< 9 >GCW< 8 >bin    (0)SigExt & GCW<11:8>GCW<7:8-N>M1 M2 Code lengthValue<N-1:0>Quantization level (N)sel<0>sel<1>Code length  =  1Code length  =  5Code length  =  5Code length  =  11sel<0:1>0 X1 01 01 1(c)ABCABCsel<0:1>Output M1, M20 XA1 0B1 1CSigExtSigExt10

example of how the layers are split into tiles and assigned
to subarrays. In this example, input activations consist in
a matrix of 8x8x3 that is convoluted by two ﬁlters of
3x3x3. Each output element is generated by computing the
Hadamard product, or element-wise product, between a
ﬁlter and a fraction of the input matrix having the same size,
and summing up all the elements of the output Hadamard
product. Sliding the ﬁlter over the entire input matrix allows
the evaluation of different output elements. Hence, the
convolution of the input features with one ﬁlter results in a
bi-dimensional output (of size 6x6x1 in the example). Then,
multiple ﬁlters compute different convolutions, producing a
three-dimensional output (6x6x2 Figure 12).

To parallelize this computation pattern, a ﬁlter is used to
convolve multiple input memory regions at the same time,
storing the data pertaining to each region as the in-memory
operands of different subarrays. Note that this paralleliza-
tion strategy involves broadcasting the ﬁlter weights to sub-
arrays. A subdivision in four memory regions is depicted
in the example in Figure 12, each computing a quarter of
the output (a 3x3x2 matrix). Due to border effects, memory
regions must partially overlap. In the example, the elements
in grey are transferred to two subarrays, while the elements
in black are transferred to all of them. For practical subarray
sizes, nonetheless, this effect is marginal, even if the size
of tiles reduces when the number of employed subarrays
increases.

For large CONV layer sizes and small subarray size,
even small tiles require more input words to compute a
single output value than the subarray storage capacity. For
example, considering the ﬁrst layer of AlexNet that has 64
ﬁlters of size 11x11x3, requiring 363 input words to calculate
a 1x1 output for all the 64 ﬁlters (1x1x64), which surpasses
the BC implementation detailed in Section 8 (320 16-bits
words per subarray). In these cases, partial convolutions
are performed, as shown in Figure 13. Consequently, ﬁlters
are decomposed in the depth direction. Then, partial con-
volution results are retrieved with BC operations as before.
Finally, the outputs of the partial convolutions are merged
with additional in-memory operations.

7.2 Fully Connected Layers
Fully connected layers compute Y outputs from X inputs
by performing a vector-matrix multiplication with a X × Y
weight matrix W . In contrast to CONV layers, each weight
is only used once at each inference in FC ones. Therefore,
it cannot be used as a broadcasted operand. Instead, inputs
do exhibit data reuse, as all X components are employed in
the computation of each output. Therefore, we store weights
as IMOs in memory subarrays for FC layers, and employ
inputs as BOs.

This mapping is exempliﬁed in Figure 14(a), where the
input vector X has four elements and output Y has three
elements. Consequently, twelve weights are required for
the layer in the example. Figure 14(b-c) depicts how this
layer is mapped into three subarrays to calculate the three
outputs (i.e., one output per subarray). The MAC operations
are performed between the common input X, which is
broadcasted to the subarrays as BC instructions, and the
stored weights W . Thus, all the three outputs are calculated
in parallel.

Figure 12. Convolutional layer activation tilling and transferred
to different subarrays, while weights are converted into BC
operations.

Figure 13. Example of convolution layer inputs that surpass
the capacity of a subarray, this layer is deployed with partial
convolutions, which requires an extra in-memory operation to
reconstruct the output activation.

The sequence of shift-add operations implementing a MAC
is entirely skipped when a ”0” weight value is decoded,
which results in energy and performance gains, as discussed
in Section 9.

7 MAPPING CNNS TO BC ARRAYS
The presented BC architecture effectively accelerates the ex-
ecution of convolutional and fully connected layers, whose
workload dominates the execution of CNN models (e.g.,
they constitute more than 98% of the run-time in the
benchmarks in Section 8). When deploying a CNN layer
onto a BC array of given dimensions, primary goals are
the reduction of data transfers between the subarrays and
the periphery, and the maximization of parallelism when
computing MACs. To this end, we developed an automated
operations scheduler, which distributes workloads to the
BC architecture considering hardware constraints, such as
the number of available subarrays and their size, as well as
application characteristics, including the type of CNN layer
(convolutional or fully connected) and its geometry. Because
large CNN layers may not entirely ﬁt the limited memory
size of the available BC arrays, the operations scheduler
divides each layer into smaller blocks, or tiles, processed
in parallel by each subarray. We detail next the speciﬁc
mapping strategy for either CONV and FC layers.

7.1 Convolutional Layers

As presented in Table 1, for convolutional layers activations
are in-memory operands, while weights are broadcasted to
subarrays in the form of BC instructions. Figure 12 shows an

(cid:31)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:21)(cid:20)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:19)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:18)(cid:31)(cid:30)(cid:29)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:17)(cid:29)(cid:26)(cid:16)(cid:15)(cid:26)(cid:16)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:21)(cid:29)(cid:26)(cid:16)(cid:15)(cid:26)(cid:16)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:19)(cid:29)(cid:26)(cid:16)(cid:15)(cid:26)(cid:16)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:18)(cid:29)(cid:26)(cid:16)(cid:15)(cid:26)(cid:16)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:23)(cid:24)(cid:22)(cid:28)(cid:17)(cid:14)(cid:23)(cid:13)(cid:24)(cid:12)(cid:11)(cid:24)(cid:10)(cid:16)(cid:9)(cid:12)(cid:28)(cid:29)(cid:15)(cid:9)(cid:23)(cid:24)(cid:8)(cid:12)(cid:10)(cid:7)(cid:6)(cid:9)(cid:5)(cid:4)(cid:3)(cid:16)(cid:10)(cid:2)(cid:31)(cid:31)(cid:30)(cid:31)(cid:31)(cid:30)(cid:29)(cid:31)(cid:31)(cid:30)(cid:31)(cid:31)(cid:30)(cid:28)(cid:27)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:26)(cid:25)(cid:31)(cid:31)(cid:30)(cid:31)(cid:31)(cid:30)(cid:29)(cid:27)(cid:26)(cid:25)(cid:31)(cid:31)(cid:30)(cid:31)(cid:31)(cid:30)(cid:29)(cid:27)(cid:26)(cid:25)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:20)(cid:23)(cid:19)(cid:18)(cid:17)(cid:16)(cid:15)(cid:14)(cid:16)(cid:19)(cid:13)(cid:21)(cid:20)(cid:16)(cid:15)(cid:12)(cid:18)(cid:11)(cid:13)(cid:21)(cid:10)(cid:13)(cid:21)(cid:18)(cid:22)(cid:9)(cid:17)(cid:16)(cid:15)(cid:12)(cid:21)(cid:22)(cid:13)(cid:17)(cid:21)(cid:20)(cid:16)(cid:15)(cid:8)(cid:15)(cid:10)(cid:13)(cid:21)(cid:18)(cid:23)(cid:15)(cid:7)(cid:18)(cid:6)(cid:9)(cid:22)(cid:15)(cid:9)(cid:19)(cid:18)(cid:12)(cid:9)(cid:10)(cid:23)(cid:22)(cid:23)(cid:21)(cid:20)(cid:16)(cid:15)(cid:18)(cid:5)(cid:20)(cid:15)(cid:20)(cid:4)(cid:23)(cid:19)(cid:18)(cid:8)(cid:15)(cid:10)(cid:13)(cid:21)(cid:18)(cid:3)(cid:16)(cid:22)(cid:7)(cid:12)(cid:28)(cid:2)(cid:29)(cid:29)(cid:2)(cid:29)(cid:29)(cid:18)(cid:1)(cid:18)(cid:28)(cid:26)(cid:28)(cid:27)(cid:29)(cid:29)(cid:29)(cid:29)(cid:28)(cid:29)(cid:29)(cid:29)(cid:29)(cid:28)(cid:26)(cid:25)(cid:26)(cid:25)(cid:26)(cid:25)++(cid:26)(cid:25)=(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)(cid:29)11

Figure 15. Layout of (a) semi-custom design of the decoder
circuit and, (b) full-custom design of one memory subarray.

Figure 14. Example of FC layer represented in (a) graphical
form and (b) matrix multiplication. (c) Data mapping to execute
this workload on a BC array.

Such FC mapping is not amenable to data encoding
strategies such as GCW, because broadcasted operands are
activation values computed at run-time during inference.
Nonetheless, FC layers exhibit usually smaller workloads
than CONV ones and are not, in general (as in the bench-
marks considered in Section 9), run-time bottlenecks.

8 EXPERIMENTAL SETUP
8.1 Evaluation on CNN benchmarks

We evaluate our architecture and optimization framework
on several edgeAI benchmarks of different complexity to
demonstrate the effectiveness of our approach in a sig-
niﬁcant range of applications: we consider LeNet-5 [27]
on CIFAR-10
[28] and AlexNet [29], VGG16 [30], Mo-
bileNet [31] and Xception [32] on the CIFAR-100 dataset [28].
Accuracy values for various CNNs and optimization levels
are retrieved using PyTorch [33] and the quantization func-
tions described in [34].

CNNs are ﬁrst trained using ﬂoating-point precision for
200 epochs, obtaining accuracies in line with the state-of-the-
art. Similar to [8], models are then homogeneously quan-
tized to 16-bits in-memory operands and 8-bits broadcasted
operands, and reﬁned for 20 additional training epochs.
This conﬁguration, which has no accuracy loss with respect
to employing ﬂoating-point weights and activations, is as-
sumed as the starting point for further optimizations using
the proposed methodology.

To establish a baseline, we iteratively repeat quantization
and retraining to homogeneously reduce, down to 2 bits,
the bitwidth of the operands streamed into the subarrays.
Then, we retrain the models for ﬁve ﬁne-tuning epochs
at each step. Five epochs are also run when applying our
heterogeneous approach, as described in Section 4, after
each BOs and IMOs optimization steps (phases (b) and (d)
in Figure 3).

Figure 16. Average bitwidths achieved in our evaluated bench-
marks by employing a synergic use of heterogeneous quantiza-
tion (blue bars) and GCW encoding (green bars), for maximum
accuracy degradation constraint of 1%.

words. Therefore, the subarray stores 5120 bits (640 bytes).
From a layout perspective (Figure 15(a)), the memory cell
array is organized into 160 rows and 32 columns. It can
store 320 words in 1x16bit mode or 640 words in 2x8bit
mode. Using a methodology analogous to [5], we implement
the BC architecture as a full-custom design and performed
HSpice energy and timing characterization. Targeting a
28nm CMOS technology from TSMC, the architecture can
operate at a maximum frequency of 2.2GHz. The subarray
requires 376pJ for reading a 16-bit word, and 414pJ energy
for writing it. An in-memory shift-add operation requires
381pJ. The subarray has an area of 1240µm2, of which 26.5%
is used for the BCU and the LGP circuits. 6.5% of the total
area is used to implement the negation, embedded shift, and
wordline parallelism. Finally, 67% of the area is ﬁlled up by
the SRAM cells.

The GCW decoder is designed as semi-custom IC, as
shown in Figure 15(b). It is synthesized, placed and routed
(again, in 28nm CMOS technology from TSMC) to extract
its area, timing, and energy requirements. The circuit has
an area of 760µm2, which represents 61% of the area of a
single subarray, and less than 1% of the total area in a 128
subarrays conﬁgurations. The decoder requires 1fJ per cycle
to operate.

We compare performances with the subarray design de-
scribed in [5]. Such architecture has been shown to achieve
3x better performance, and 1.5x increased energy efﬁciency
compared with the ARM NEON SIMD accelerator when
running inferences on benchmark CNNs.

8.2 Circuit-level characterization

As a test vehicle for our experiments, we consider a BC
architecture composed of a varying number of subarrays.
Each subarray contains 5 LGs of 32 rows each, totalling 160
memory rows. Each row is composed of 2 interleaved 16-bits

9 RESULTS
9.1 Accuracy-constrained compression

Figure 16 depicts the average bitwidth (across layers) of
IMOs and BOs in CNN benchmarks applications optimized

(cid:31)(cid:30)(cid:31)(cid:29)(cid:31)(cid:28)(cid:31)(cid:27)(cid:26)(cid:30)(cid:26)(cid:29)(cid:26)(cid:28)(cid:25)(cid:30)(cid:30)(cid:25)(cid:29)(cid:30)(cid:25)(cid:28)(cid:30)(cid:25)(cid:30)(cid:27)(cid:25)(cid:29)(cid:27)(cid:25)(cid:28)(cid:27)(cid:26)(cid:30)(cid:26)(cid:29)(cid:26)(cid:28)(cid:31)(cid:30)(cid:31)(cid:29)(cid:31)(cid:28)(cid:31)(cid:27)(cid:25)(cid:30)(cid:30)(cid:25)(cid:29)(cid:30)(cid:25)(cid:28)(cid:30)(cid:25)(cid:30)(cid:29)(cid:25)(cid:29)(cid:29)(cid:25)(cid:28)(cid:29)(cid:25)(cid:30)(cid:28)(cid:25)(cid:29)(cid:28)(cid:25)(cid:28)(cid:28)(cid:25)(cid:30)(cid:27)(cid:25)(cid:29)(cid:27)(cid:25)(cid:28)(cid:27)(cid:31)(cid:30)(cid:24)(cid:23)(cid:23)(cid:22)(cid:25)(cid:21)(cid:20)(cid:19)(cid:18)(cid:17)(cid:16)(cid:22)(cid:15)(cid:22)(cid:14)(cid:13)(cid:12)(cid:11)(cid:10)(cid:9)(cid:8)(cid:17)(cid:20)(cid:7)(cid:6)(cid:17)(cid:20)(cid:5)(cid:4)(cid:3)(cid:4)(cid:2)(cid:1)(cid:17)(cid:9)(cid:8)(cid:17)(cid:20)(cid:7)(cid:6)(cid:17)(cid:20)(cid:5)(cid:4)(cid:127)(cid:1)(cid:17)(cid:2)(cid:1)(cid:17)(cid:3)(cid:129)(cid:127)(cid:22)(cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22)(cid:157)(cid:26)(cid:30)(cid:26)(cid:29)(cid:26)(cid:28)(cid:31)(cid:30) (cid:22)(cid:31)(cid:29) (cid:22)(cid:31)(cid:28) (cid:22)(cid:31)(cid:27)(cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22)(cid:157)(cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22) (cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22)€(cid:25)(cid:30)(cid:30)(cid:25)(cid:30)(cid:29)(cid:25)(cid:30)(cid:28)(cid:25)(cid:30)(cid:27)(cid:25)(cid:29)(cid:30)(cid:25)(cid:29)(cid:29)(cid:25)(cid:29)(cid:28)(cid:25)(cid:29)(cid:27)(cid:25)(cid:28)(cid:30)(cid:25)(cid:28)(cid:29)(cid:25)(cid:28)(cid:28)(cid:25)(cid:28)(cid:27)(cid:13)(cid:6)(cid:10)(cid:13)(cid:8)(cid:10)(cid:13)(cid:143)(cid:10)(cid:3)(cid:129)(cid:127)(cid:22)(cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22) (cid:3)(cid:129)(cid:127)(cid:22)(cid:141)(cid:1)(cid:143)(cid:6)(cid:144)(cid:144)(cid:6)(cid:12)(cid:22)€BOAverage BitwidthIn-memory Broadcasted 2468481216Baseline   Quantization    Quantization + EncodingLeNet-5      AlexNet       VGG16     MobileNet     Xceptionoperandsoperands4.94.84.24.64.24.21.82.51.51.28.08.014.214.112.1with our proposed methodology. In all cases, results are for
an accuracy threshold of 1% with respect to implementa-
tions having 16-bits IMOs and 8-bits BOs.

Blue bars illustrate the average bitwidth reduction
achieved in convolutional and fully connected layers by
mean of our heterogeneous quantization approach (as de-
tailed in Section 4.1), which results in compression ratios of
CNN models of 76.8% on average.

Then, additional savings are achieved by encoding the
weights of convolutional layers (illustrated in Section 4.2).
Results are shown as green bars in Figure 16. They show that
GCW encoding effectively contributes to the reduction of
storage requirements, resulting in overall model size savings
of 85.3% on average.

Experimental outcomes show that all activations of con-
volutional layers of simpler CNNs (LeNet-5 and AlexNet)
can be effectively reduced to 8-bits while abiding by the
accuracy constraint. Such optimization instead can only be
selectively applied in more complex benchmarks such as in
VGG16, MobileNet and Xception, highlighting the beneﬁt of
a heterogeneous approach.

Moreover, especially high compression ratios are
achieved for larger models, because their size is largely
determined by the footprint of convolutional weights. These
can be effectively compressed by quantization and encoding
(by up to more than 20x for the Xception CNN).

9.2 CNN Inference Cycle-count Reduction

Figure 17 illustrates the accuracy/performance trade-off in
different optimized benchmarks. Black markers report the
accuracy of homogeneously quantized models, while blue
and green lines show the accuracy achieved in the var-
ious steps of the proposed hardware/software co-design
methodology, for accuracy degradation thresholds of 1%
and 5%, respectively, compared to conﬁgurations with 8-bit
BOs and 16-bits IMOs.

In Figure 17, the points (b), (c) and (d) highlight im-
provements obtained in the different stages of the opti-
mization strategy, as illustrated in Figure 3 and detailed in
Section 4. They report performance/accuracy of CNNs after
(b) BOs, (c) convolutional ﬁlters, and (d) IMOs optimization.
Points (e) and (f) report further cycle-count reductions
obtained by hardware optimizations. In (e) up to three
single-cycle bit-shifts (NES = 3 in Section 5.3) are supported.
Additionally, in (f), MAC operations involving zero-valued
broadcasted operands are skipped (Section 6).

When employing 5 bits, baseline uniform quantization
achieves a 33% cycle-count reduction at the cost of an
average 1.3% accuracy degradation, which rapidly increases
for smaller bitwidths. Conversely, signiﬁcantly higher per-
formance improvements are obtained with our approach. In
particular, heterogeneous quantization alone (steps (b)-(d))
enables alone up to 80% cycle-count gains. BC hardware
optimizations are also highly effective (steps (e) and (f)).
Support for NES=3 results in an average cycle-count reduc-
tion of 2.1x, which increases to 2.9x when also skipping mul-
tiplications involving zero-valued broadcasted operands.
Considering all software and hardware optimizations, the
co-design framework achieves an average cycle-count re-
duction of 89.3% (a speed-up of 11.5x) for 8-bits quantized

12

Figure 17. Accuracy and cycle-count reductions in homoge-
neously quantized CNNs (black lines) and optimized CNNs
(blue and green lines) for a 1% and a 5% accuracy drops. Data
refers to single-subarray BC architectures. Vertical dashed lines
mark speed-up levels of 5x and 10x.

baselines for 1% degradation thresholds, and 91.9% average
cycle-count reduction (a speed-up of 15x) for 5% accuracy
degradation.

Performance gains are linearly correlated to energy sav-
ings, as they derive from reductions in shift-adds. Illustrat-
ing this aspect, Table 2 shows the Inferences Per Second
(IPS) and inference energy in 8-bits quantized CNN base-
lines and in optimized models. As an example, the baseline
AlexNet requires a 9.16mJ to perform one inference. Our
optimized hardware and software reduce this cost by 93%,
to just 0.62mJ, when executing on a single subarray.

8Bits7Bits6Bits5Bits4Bits3bits2Bits(b)(c)(d)(e)(f)(b)(c)(d)(e)(f)01020304050607080-100102030405060708090100Accuracy (%)Cycle-Count Reduction (%)Unif. Quant.Opt - 1% dropOpt - 5% drop8Bits7Bits6Bits5Bits4Bits3bits2Bits(b)(c)(d)(e)(f)(b)(c)(d)(e)(f)01020304050607080-100102030405060708090100Accuracy (%)Cycle-Count Reduction (%)8Bits7Bits6Bits5Bits4Bits3bits2Bits(b)(c)(d)(e)(f)(b)(c)(d)(e)(f)01020304050607080-100102030405060708090100Accuracy (%)Cycle-Count Reduction (%)8Bits7Bits6Bits5Bits4Bits3bits2Bits(b)(c)(d)(e)(f)(b)(c)(d)(e)(f)01020304050607080-100102030405060708090100Accuracy (%)Cycle-Count Reduction (%)8Bits7Bits6Bits5Bits4Bits3bits2Bits(b)(c)(d)(e)(f)(b)(c)(d)(e)(f)01020304050607080-100102030405060708090100Accuracy (%)Cycle-Count Reduction (%)LeNet-5AlexNetVGG16MobileNetXceptionCycle-count reduction (%)Cycle-count reduction (%)Cycle-count reduction (%)Cycle-count reduction (%)Cycle-count reduction (%)(5x)(10x)(5x)(10x)(5x)(10x)(5x)(10x)(5x)(10x)Unif. Quant.Our, -1% AccucaryOur, -5% AccucaryUnif. Quant.Our, -1% AccucaryOur, -5% AccucaryUnif. Quant.Our, -1% AccucaryOur, -5% AccucaryUnif. Quant.Our, -1% AccucaryOur, -5% AccucaryUnif. Quant.Our, -1% AccucaryOur, -5% AccucaryTable 2. Inferences-per-second (left) and inference energy (right) in homogeneously quantized implementations executing on the
BC architecture in [5] compared to heterogeneously quantized models executed on optimized BC architecture and employing 1,
32 or 128 subarrays. Optimized implementations are obtained for a 1% accuracy degradation threshold.

13

slight increase in energy consumption is due to leakage
energy (Figure 18(b)) consumed by subarrays during data
transfers, and by the larger and more energy-hungry H-tree
required to connect them. On the other hand, a higher level
of parallelism requires decoding a smaller number of BC
instructions, decreasing the related energy budget, which
nonetheless accounts for less than 1% of the total energy in
all conﬁgurations.

10 CONCLUSION

Edge AI requires the execution of extremely computa-
tional and memory-intensive applications on constrained
platforms. Bit-Line computing is a promising avenue to
cope with this challenge, but demands careful synergic co-
optimization of applications and hardware. To tackle this
problem, in this work we have presented a framework
that comprises hardware-aware application optimizations
as well as novel architectural solutions to effectively harness
them. On the application side, CNNs are compressed by
combining heterogeneous quantization and weights encod-
ing. In turn, the proposed Bit-line Computing (BC) platform
embeds low-overhead hardware features to perform run-
time decoding, support ﬁne-grained quantization of broad-
casted operands, and leverage word-level parallelism of in-
memory operands. Results on a variety of CNN benchmarks
have demonstrated that, for a 1% accuracy degradation
constraint, our compression strategy offers 85% average
memory reductions compared to uniformly quantized CNN
implementations. Using the same constraint, our proposed
BC architecture achieves on average 11x inference speed-
ups and 90% energy savings with respect to state-of-the-art
BC approaches.

ACKNOWLEDGMENTS

This work has been supported by the EC H2020 WiPLASH
(Ga. No. 863337), the EC H2020 FVLLMONTI (Ga. No.
101016776), the ERC Consolidator Grant COMPUSAPIEN
(Ga. No. 725657), and by the Swiss NSF ML-Edge (Ga. No.
182009) projects.

REFERENCES

[1] A. Khan et al., “A survey of the recent architectures of deep con-
volutional neural networks,” Springer Artiﬁcial Intelligence Review,
2020.

[2] X. Wang et al., “Convergence of edge computing and deep learn-
ing: A comprehensive survey,” IEEE Communications Surveys &
Tutorials, 2020.

Figure 18. (a) Cycle and (b) energy breakdown of multiple
subarray architecture for an AlexNet inference with SW-HW
optimization.

9.3 BC Architectures with Multiple Subarrays

As shown in Table 2, the IPS of our evaluated benchmarks
scales up with the number of subarrays, as the workload
is effectively distributed using the CNN mapping strategy
proposed in Section 7. For AlexNet, VGG16, MobileNet,
and Xception, on average a speed-up of 58x is reached
when employing 128 subarrays with respect to a single
one. Being a smaller network, LeNet-5 is less amenable to
parallelization, reaching in this setting a 15x speed-up.

The performance of our solution when varying the num-
ber of subarrays are further detailed in Figure 18(a) for
the Alexnet benchmark. In its top part, the ﬁgure reports
absolute cycle-counts and energy requirements. Below, we
show the proportional breakdowns. These results indicate
that, when the workload is entirely run on a single sub-
array, 99% of the clock cycles are used to perform MAC
operations. However, the use of 32 or 128 subarrays reduces
this percentage to 68% and 35% (respectively) because data-
transfers are performed sequentially, while the parallelism
of BC operations grows linearly with the number of subar-
rays.

Then, the energy results included in Table 2 show that
the use of a high number of subarrays has only a small
impact on energy efﬁciency (while greatly reducing run-
times). For AlexNet, VGG16, MobileNet, and Xception the
difference in energy between 1 and 128 subarray conﬁgu-
rations is only 12% (again, LeNet-5 is an outlier due to its
small dimensions). Indeed, the number of subarrays does
neither inﬂuence the energy cost of BC operations nor the
number of BC operations required by an inference. The

1 sub32 subs128 subs1 sub32 subs128 subsLeNet-5289141522491225450.00350.00070.00120.0045AlexNet0.111.6236739.160.620.640.71VGG160.392.89782012.570.350.350.39MobileNet0.353.76932202.820.270.270.29Xception0.00230.092.185.13432.6911.3411.5412.26Optimized (1% drop)Optimized (1% drop)Inferences Per Second (IPS)Energy (mJ)Simon et al.[5]Simon et al.[5]Cycle breakdownEnergy breakdown(a)(b)0.00E+005.00E-041.00E-03123Series21321281.00.50Energy (mJ)1.00E+071.00E+081.00E+091.00E+10123Series21e101e91e81e7Cycle-Count 132128Num. of Subarrays132128100%80%60%40%20%0%100%90%80%70%0%132128ControlLeakageData TransfersData TransfersNum. of SubarraysNum. of SubarraysNum. of Subarrays[3] Y.-H. Chen et al., “Eyeriss: An energy-efﬁcient reconﬁgurable
accelerator for deep convolutional neural networks,” IEEE JSSC,
2016.

[4] D. Rossi et al., “PULP: A parallel ultra low power platform for next

generation IoT applications,” IEEE HCS, 2015.

[5] A.-W. Simon et al., “BLADE: An in-cache computing architecture

for edge devices,” IEEE Transactions on Computers, 2020.

[6] M. Rios et al., “Running efﬁciently CNNs on the edge thanks to
hybrid SRAM-RRAM in-memory computing,” IEEE/ACM DATE,
2021.

[7] ——, “An associativity-agnostic in-cache computing architecture

optimized for multiplication,” IEEE VLSI-SoC, 2019.

[8] F. Ponzina et al., “A ﬂexible in-memory computing architecture for

heterogeneously quantized CNNs,” IEEE ISVLSI, 2021.

[9] A.

Garofalo

et

al.,

“Darkside

(2021),”

http://asic.ee.ethz.ch/2021/Darkside.html.

[10] S. Bianco et al., “Benchmark analysis of representative deep neural

network architectures,” IEEE Access, 2018.

[11] L. Song et al., “Pipelayer: A pipelined ReRAM-based accelerator

for deep learning,” IEEE HPCA, 2017.

[12] X. Peng et al., “Optimizing weight mapping and data ﬂow for
convolutional neural networks on processing-in-memory architec-
tures,” IEEE TCAS I, 2019.

[13] J. Rhe et al., “VW-SDK: Efﬁcient convolutional weight mapping
using variable windows for processing-in-memory architectures,”
arXiv preprint arXiv:2112.11282, 2021.

[14] S. Han et al., “Learning both weights and connections for efﬁcient

neural network,” NIPS, 2015.

[15] Y. He et al., “Learning ﬁlter pruning criteria for deep convolutional

neural networks acceleration,” Proceedings of the IEEE/CVF, 2020.

[16] P. Molchanov et al., “Pruning convolutional neural networks for
resource efﬁcient inference,” arXiv preprint arXiv:1611.06440, 2016.
[17] B. Reagen et al., “Ares: A framework for quantifying the resilience

of deep neural networks,” IEEE/ACM DAC, 2018.

[18] B. Denkinger et al., “Impact of memory voltage scaling on accuracy
and resilience of deep learning based edge devices,” IEEE Design
& Test, 2019.

[19] A. Jain et al., “Symmetric k-means for deep neural network com-
pression and hardware acceleration on FPGAs,” IEEE JSTSP, 2020.
[20] S. Han et al., “Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding,”
arXiv preprint arXiv:1510.00149, 2015.

[21] J.-H. Lee et al., “Arithmetic coding-based 5-bit weight encoding
and hardware decoder for CNN inference in edge devices,” IEEE
Access, 2021.

[22] D. Ielmini et al., “Device and circuit architectures for in-memory

computing,” Wiley Advanced Intelligent Systems, 2020.

[23] ——, “In-memory computing with resistive switching devices,”

Nature Electronics, 2018.

[24] A. Levisse et al., “Architecture, design and technology guidelines

for crosspoint memories,” IEEE/ACM NANOARCH, 2017.

[25] S. Jeloka et al., “A 28 nm conﬁgurable memory (TCAM/BCAM/
SRAM) using push-rule 6t bit cell enabling logic-in-memory,” IEEE
JSSC, 2016.

[26] K.-C. Akyel et al., “DRC2: Dynamically reconﬁgurable computing

circuit based on memory architecture,” 2016.

[27] Y. Lecun et al., “Gradient-based learning applied to document

recognition,” Proceedings of the IEEE, 1998.

[28] A. Krizhevsky et al., “Learning multiple layers of features from

tiny images,” University of Toronto, 2009.

[29] ——, “Imagenet classiﬁcation with deep convolutional neural

networks,” NIPS, 2012.

[30] K. Simonyan et al., “Very deep convolutional networks for large-

scale image recognition,” arXiv:1409.1556, 2014.

[31] A. G. Howard et al., “Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications,” arXiv:1704.04861, 2017.
[32] F. Chollet, “Xception: Deep learning with depthwise separable

convolutions,” Proceedings of the IEEE/CVF, 2017.

[33] A. Paszke et al., “Pytorch: An imperative style, high-performance

deep learning library,” arXiv:1912.01703, 2019.

[34] B. Jacob et al., “Quantization and training of neural networks
for efﬁcient integer-arithmetic-only inference,” Proceedings of the
IEEE/CVF, 2018.

14

Marco Rios received the M.Sc. degree in
Computer Science and Electronics For Embed-
ded Systems from Universit ´e Grenoble Alpes,
France, in 2018. He is currently a PhD student
at the Embedded Systems Laboratory of EPFL,
Switzerland. His research interests include de-
sign of integrated systems and circuits, in-SRAM
computing and the system impact of emerging
memories.

Italy,

Flavio Ponzina received the M.Sc. degree
in Computer Engineering from Politecnico di
Torino,
in 2018. He is currently a PhD
student at the Embedded Systems Laboratory
(ESL), EPFL. His main research interests in-
clude low power architectures and AI-based sys-
tems optimization.

Alexandre Levisse received his Ph.D. degree in
Electrical Engineering from CEA-LETI, France,
and from Aix-Marseille University, France,
in
2017. From 2018 to 2021, he was a post-
doctoral researcher in the Embedded Systems
Laboratory at
Institute of
Technology Lausanne (EPFL). From 2021, he
works as a scientist in EPFL. His research inter-
ests include circuits and architectures for emerg-
ing memory and transistor technologies as well
as in-memory computing and accelerators.

the Swiss Federal

Giovanni Ansaloni is a researcher at the Em-
bedded Systems Laboratory of EPFL (ESL-
EPFL, Lausanne, CH). He previously worked as
a Post-Doc at the University of Lugano (USI,
CH) between 2015 and 2020, and at EPFL
between 2011 and 2015. He received a Ph.D.
degree in Informatics from USI
in 2011. His
research efforts focus on domain-speciﬁc and
ultra-low-power architectures and algorithms for
edge computing systems,
including hardware
and software optimization techniques.

David Atienza is a full professor of electri-
cal and computer engineering, and head of
the Embedded Systems Laboratory (ESL) at
in
EPFL, Switzerland. He received his Ph.D.
computer science and engineering from UCM,
Spain, and IMEC, Belgium, in 2005. His research
interests include system-level design method-
ologies for multi-processor system-on-chip (MP-
SoC) servers and edge AI architectures. He has
co-authored more than 350 papers, one book,
and 12 patents. Dr. Atienza has received, among
other recognitions, the ICCAD 10-Year Retrospective Most Inﬂuential
Paper Award in 2020, the Most Inﬂuential DAC Under-40 Innovators
Award in 2018, and an ERC Consolidator Grant in 2016. He is an IEEE
Fellow and an ACM Distinguished Member.

