2
2
0
2

r
p
A
4
1

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
5
2
0
7
0
.
4
0
2
2
:
v
i
X
r
a

(Preprint) AAS 21-364

AUTONOMOUS SATELLITE DETECTION AND TRACKING USING
OPTICAL FLOW

David Zuehlke*, Daniel Posada†, Madhur Tiwari‡, Troy Henderson§

In this paper, an autonomous method of satellite detection and tracking in images
is implemented using optical ﬂow. Optical ﬂow is used to estimate the image ve-
locities of detected objects in a series of space images. Given that most objects
in an image will be stars, the overall image velocity from star motion is used to
estimate the image frame-to-frame motion. Objects seen to be moving with veloc-
ity proﬁles distinct from the overall image velocity are then classiﬁed as potential
resident space objects. The detection algorithm is exercised using both simulated
star images and ground-based imagery of satellites. Finally, this algorithm will be
tested and compared using a commercial and an open-source software approach to
provide the reader two different options based on their need.

INTRODUCTION

Space situational awareness (SSA) is of increasing importance due to the ever increasing number
of objects (both controlled and uncontrolled objects such as debris) in orbit.1, 2 In order to guarantee
the safety of space assets and personnel in future launches, advancements are needed in the way
that current SSA work is performed. Optical measurements offer an inexpensive method of space
object tracking and is currently performed by the Space Surveillance Network (SSN). However,
the limits of the space surveillance network lie in available time on existing assets for observing
satellites. Utilizing small optical ground stations provides a way to augment the capabilities of the
SSN. However, robust tracking and identiﬁcation methods for data obtained from small stations is a
prerequisite to their data being deemed useful.3 Common methods of resident space object (RSO)
identiﬁcation in images involve streak detection, or estimating the gross motion of all objects in the
image.4, 5 Image template matching has also been utilized as a method of identifying and tracking
RSOs in optical imagery.6

The common thread through the various detection algorithms is that image properties are used to
identify and track a speciﬁc subset of image features. The computer vision community has further
tools to contribute in this arena for a problem framed in the more general terms of tracking the
movement of objects in an image (i.e. picking out moving satellites in a star-ﬁeld). One of the most
widespread methods used for motion tracking in computer vision research today is optical ﬂow.7
Essentially optical ﬂow seeks to ﬁnd the image plane velocity ﬁeld caused by the motion of the

*PhD Candidate, Aerospace Engineering, Embry-Riddle Aeronautical University, 1 Aerospace Blvd., Daytona Beach
Florida
†PhD Student, Aerospace Engineering, Embry-Riddle Aeronautical University, 1 Aerospace Blvd., Daytona Beach Florida
‡PhD Candidate, Aerospace Engineering, Embry-Riddle Aeronautical University, 1 Aerospace Blvd., Daytona Beach
Florida
§Associate Professor, Aerospace Engineering, Embry-Riddle Aeronautical University, 1 Aerospace Blvd., Daytona Beach
Florida

1

 
 
 
 
 
 
scene or the observer in a sequence of images. Optical ﬂow has been applied to object tracking in
dense object scenes for tracking vehicle motion8 and robot obstacle avoidance and path planning.9
In relation to spacecraft, optical ﬂow has been applied to space-based ground imaging to detect
sandstorms, and other large scale weather phenomenon as observed from earth orbiting satellites.7
Optical ﬂow has also been used to track the movement of ground objects (such as ships in a harbor)
from space10 and to estimate the angular velocity of a spacecraft using images from a star tracker
camera.11 Overall motion of the star-ﬁeld provided a method of determining the inertial spacecraft
angular velocity without any star catalog. However, none of the mentioned research applies optical
ﬂow to the problem of space domain awareness, which is the focus of this paper.

This paper implements an optical ﬂow algorithm based on the Lucas-Kanade method to estimate
the camera frame velocities of all objects contained in a space image.12 A space image can be
loosely deﬁned as an optical image taken of the night sky that contains stars, astronomical objects,
and possibly RSOs. Given that stars make up the bulk of objects in any space image, the overall
image velocity provides a way of identifying which objects are stars in the image without the use of
a star catalog or plate solving with an application such as Astrometry.net.13 The velocity estimate
for the entire image is taken to be the average velocity of all objects, with the effects of outliers
removed. Objects seen to be moving with velocity proﬁles (magnitude and/or direction) distinct
from the overall image motion are classiﬁed as potential RSOs. Once all potential RSOs have been
identiﬁed, the next step in the Space Situational Awareness (SSA) process would be to transform
the pixel coordinates to inertial coordinates (angles such as azimuth and elevation or right-ascension
(RA) and declination (DEC)) for use in an angles-only orbit determination method. In order to
thoroughly test the RSO identiﬁcation algorithm, a star and RSO image simulator was developed.
Simulated imagery provides a testable“ground-truth” scenario. In addition to the simulated imagery,
the algorithm is further tested by the use of ground-based images of satellites.

BACKGROUND AND THEORY

Optical Flow

Some preliminary deﬁnitions are necessary before delving into the mechanics of optical ﬂow.
First, all images can be represented mathematically by a matrix of intensities written as I(x, y).
Where I is the intensity at the point (x, y) in the image. The optical ﬂow between two images
can be thought of as the 2D vector ﬁeld representing the apparent motion between two consecutive
images.14 As a vector ﬁeld, optical ﬂow allows the calculation of the displacement and velocities of
detected objects in a series of images. There are many methods of computing optical ﬂow, but two
of the most common are the methods of Lucas-Kanade and Horn-Schunck.12, 15 In this paper, the
Lucas-Kanade (LK) method will be followed. Optical ﬂow assumes:

1. The pixel intensities of objects are constant between consecutive frames.

2. Pixels in a neighborhood all follow similar paths of motion.

Given these assumptions, a pixel at a time t, is represented by I(x, y, t), then the same pixel will be
displaced a small amount (dx, dy) when time dt has passed between images. Thus the pixel must
obey the relation given in Equation (1).

I(x, y, t) = I(x + dx, y + dy, t + dt)

(1)

2

The optical ﬂow constraint equation is obtained by taking a Taylor series expansion of the right
hand side of Equation (1).7 Performing the expansion we obtain:

Where Ix =

∂I
∂x

and Iy =

∂I
∂y

Ixu + Iyv + It = 0.

are the spatial image derivatives (image gradients), and It =

(2)

∂I
∂t

is the temporal image brightness derivative, and u =

is the horizontal optical ﬂow, and ﬁnally

dy
dt

v =
is the vertical optical ﬂow. In order to solve Equation (2) the LK method takes the original
image and divides it into small sections while assuming a constant velocity in the sections. A
weighted least-squares equation is then solved to obtain the optical ﬂow ﬁt. The ﬁnal solution of the
least-squares solution is given by Equation (3).16

dx
dt

(cid:21)
(cid:20)u
v

=

(cid:20) (cid:80)
(cid:80)

i I 2
xi
i IxiIyi

(cid:80)

i IxiIyi
(cid:80)
i I 2
yi

(cid:21)−1 (cid:20)− (cid:80)
− (cid:80)

i IxiIti
i IyiIti

(cid:21)

(3)

With the optical ﬂow solution, the velocities of each object detected in consecutive images is ob-
tained.

Software Implementation

To test the effectiveness of this algorithm, two different pipelines that use the LK method will be
tested in different development environments. The ﬁrst environment is set up using MATLAB®, the
second one is performed using the open source toolbox OpenCV for Python.

Image Simulator

In order to test the optical ﬂow tracking method on space images, a star and satellite image
simulator was developed. The simulator assumes that the camera frame is tracking at a constant
velocity. Note that this camera frame velocity can be deﬁned to be the rate of star motion, the rate
of RSO motion for a given orbit regime, or simply a constant angular rate. For the purposes of
this simulator, actual star positions were not important, it was desired to simulate the appearance of
stars without having to incorporate actual positions from a star catalog. Star positions are seeded
randomly across the image frame, with star brightness levels also set by a random number generator.
To simulate camera motion, each image frame is propagated forward in time for a user deﬁned
number of samples. During each sample the image is convolved with a Gaussian kernel to simulate
the effect of an optical system’s point spread function (PSF). Image intensities are scaled according
to a user speciﬁed camera bit depth. Intensities that fall above the bit depth are automatically set
to the bit depth to simulate saturated pixels. RSO positions are seeded randomly across the image.
RSO velocities are set to be different from the image velocity for stars (as is the case in real images).
Noise is added to each image frame as white Gaussian noise with a mean level set by a user deﬁned
percentage of the bit depth. Further details into the actual implementation of the star image simulator
will be provided in the full paper. Figure 1 shows an example simulated frame with stars that streak
slightly in the image and RSOs that show up as point sources. In order to provide more realistic
images, Gaussian white noise was added to corrupt the pristine simulated image. White noise was
set to be at a value of 5% of the maximum intensity of the image.

3

Figure 1. Simulated space image.

AUTONOMOUS RSO IDENTIFICATION PROCESS

The RSO identiﬁcation algorithm developed for this research is outlined in Figure 2. The basic

steps shown in Figure 2 are followed by both pipelines.

Figure 2. RSO identiﬁcation process block diagram.

Given a sequence of consecutive space images, the algorithm begins by loading the image at
time t and the image at t + dt, where t + dt is the time of the next image in the sequence. Both
images are then processed to reduce noise and improve detection results. The image processing
applied involves using a Gaussian smoothing ﬁlter to reduce the effects of random image noise, and
a threshold ﬁlter to remove the background noise level.

Application of a Gaussian smoothing ﬁlter suppresses high-frequency content of the image.
Gaussian smoothing works by blurring the original image slightly by convolution with a Gaus-
sian Kernel. Each smoothed pixel is a weighted average of the neighboring pixels, with the mean
weighted towards the central pixels, thus keeping the image in a Gaussian distribution. A typical

4

Gaussian Kernel such as that given in Equation (4). Where (x, y) represent pixel coordinates and σ
is the standard deviation for the Gaussian distribution that is desired. A higher σ value results in an
image that is smoother.

1
2πσ2 exp
An asterisk represents the convolution operation, then the ﬁnal smoothed image is given by Equation
(5).3

G(x, y, σ) =

x2 + y2
2σ2

(4)

−

(cid:18)

(cid:19)

Is(x, y) = G(x, y, σ) ∗ I(x, y)

(5)

Applying a threshold on image intensity based on the mean background intensity level provides
a simple method of removing any remaining noise after the Gaussian smoothing. All pixels below
the chosen threshold level are set to zero intensity, with all remaining illuminated pixels then repre-
senting the stars and possible RSOs in an image. The threshold utilized is shown in Equation (6).
The ﬁnal processed image, denoted If (x, y), is the result of applying the threshold to the image as
shown by Equation (7), where only pixel values above the threshold remain in the ﬁnal processed
image. Processed images are then used for the optical ﬂow step.

threshold = mean(Is(x, y)) + N × std(Is(x, y))

If (x, y) = Is(x, y) > threshold

(6)

(7)

Once the processing is applied to both images I(x, y, t) and I(x, y, t + dt), the optical ﬂow
between the images is calculated using the Lucas-Kanade method. The result is a velocity ﬁeld
estimating the motion of all objects detected in the images. A cropped image example of the optical
ﬂow between two simulated images is shown in Figure 3. Now with the velocity ﬁeld calculated,

Figure 3. Simulated image optical ﬂow crop example.

the next step in the algorithm is to determine the overall image velocity based on the motion of the
most prevalent objects in the image (i.e. stars).This is accomplished by looking at the magnitudes
of the image velocity for all objects detected in the image.

5

MATLAB Pipeline

Once the optical ﬂow between the images has been found, the overall image velocity is calculated
based on the magnitude of velocities for image objects. Objects determined to be moving with ve-
locities near the overall image velocity are classiﬁed as stars, while objects that move with velocities
that are different from the overall image velocity are classiﬁed as potential RSOs.

MATLAB includes a direct function to estimate optical ﬂow based on different methods including

the LK algorithm. These methods17 include the use of functions and objects such as:

• opticalFlowLK: This function creates an object to estimate the direction and speed of a mov-

ing feature.

• estimateFlow: This function takes the opticalFlowLK object and estimates optical ﬂow be-
tween two consecutive video frames. The method runs recursively as the frames in the video
advance.

Utilizing the built in functions, the velocity proﬁle of the image is found. Given the velocity mag-
nitude map, we can “centroid” the magnitude map as an image whose intensities consist of the
individual pixel object velocities. MATLAB’s computation of Optical Flow uses a ﬁxed object size
and containts one parameter for a noise threshold to determine what is considered a pixel object.
By looking at the entire magnitude map then the actual full “object” velocity is found rather than
just the velocity of a small portion of the object. An example of the resulting velocity magnitude
(cropped version) from a simulated image is shown in Figure 4. Velocities of each object are com-

Figure 4. Simulated image optical ﬂow velocity magnitude.

puted as the sum of the “intensity” for the detected object in the velocity magnitude matrix. With the
velocity of all the image objects, now the maximum, minimum, mean, and median velocity objects
can be found. It was found that given a sufﬁcient number of stars to set the mean velocity, then by
looking at the standard deviation of the velocities RSOs can be picked out as shown in Equations
(8) and (9).

i = 1, 2, ..., N

(8)

(9)
Where vrsok is the velocity of the kth rso detected and the index i is the object detected in the current
velocity magnitude map with N as the number of total objects detected.

i = 1, 2, ..., N

vrsok = vmean + 2 σvi
vrsok = vmean − 2 σvi

6

OpenCV Pipeline

Similar to the MATLAB implementation, the output of the LK algorithm is the position of the
different features in terms of pixel location. With the time between frames, the velocity of a matched
pair between frames can be obtained using a simple derivative of pixels per frame time step:

velx =

x2 − x1
f rame time

vely =

y2 − y1
f rame time

(10)

Once the velocities are obtained the magnitude of the velocity of the features can be obtained by
using the l2 norm.

(cid:113)

|vel| =

vel2

x + vel2
y

(11)

Once the magnitudes are obtained, a comparison is done between the different magnitudes to ﬁlter
out the RSOs from the stars. The RSOs have a higher velocity therefore the ﬂow will be higher
making them easier to identify. It is important to remind the reader that OpenCV is a free tool-
box compatible with Python and C/C++ making these approaches completely free for the scientiﬁc
community.

Similar to MATLAB, OpenCV provides multiple functions18 but the main methods for optical

ﬂow estimation are:

• goodFeaturesToTrack(): This function creates an object with features to track.

• calcOpticalFlowPyrLK(): This function takes the features detected and estimates optical ﬂow

between two consecutive video frames.

Using these main functions, an algorithm that follows the process outlined in Figure 2 was created.
The main difference in the OpenCV implementation lies in the calculation of the object image
velocities as mentioned above. Once the image frame velocities are computed then the RSOs can
be found from the stars in the image.

RESULTS

The autonomous RSO detection method was applied to both simulated and actual imagery. Both
the MATLAB and OpenCV pipelines were successfully able to track the RSOs in simulated and
real imagery.

Simulated Images MATLAB Results

First, the MATLAB implementation of the The optical ﬂow tracker is shown to be able to success-
fully track the motion of stars across for both simulated and experimental images. Figure 5 shows
a simulated image star ﬁeld with a group of 5 stationary RSOs.This simulates a GEO constellation
of satellites being tracked. Next, Figure 6 shows the optical ﬂow for the simulated star ﬁeld image
(computed with the consecutive frame). Blue arrows represent the velocity magnitude and direc-
tion estimated for the local pixel regions. Note that the magnitude scale (length of the vectors) is
exaggerated for the sake of display. Figure 7 shows the simulated star ﬁeld with RSOs highlighted.
A cropped result of running the optical ﬂow and RSO identiﬁcation routine is shown in Figure 8.
Note that the stars all have large velocity magnitudes while the detected RSO velocity vectors are
much smaller. The algorithm was able to successfully differentiate the stars from the RSOs for the
simulated image set.

7

Figure 5. Simulated star ﬁeld.

Figure 6. Optical Flow from simulated star ﬁeld.

Figure 7. Simulated RSOs highlighted.

Figure 8. Closeup of simulated image
optical ﬂow showing identiﬁed RSO.

Experimental Images

In this section results from running the optical ﬂow algorithm on ground-based imagery obtained
from Daytona Beach Florida are presented. Figure 9 shows a raw space image and Figure 10 shows
the resulting optical ﬂow calculation and RSO detection step. Blue arrows represent the optical ﬂow
velocities estimated for various objects detected in the image, and the RSO position is denoted by a
red asterisk. This set of images was taken in sidereal tracking mode, so star positions remain nearly
constant while the RSO is seen moving through the frame with a much higher image velocity. The
algorithm was able to autonomously label the correct object in the image as an RSO. Figures (11 -
14) show the results for a longer series of images of the same object. A single RSO was detected
moving from the right center of the image towards the center of the image. Note that detected
objects are shown in red, the RSO in green, and the ﬂow velocity vectors in blue.

8

Figure 9. Raw ground-based image.

Figure 10. MATLAB optical ﬂow RSO detection.

Figure 11. RSO detection frame 10.

Figure 12. RSO detection frame 30.

Figure 13. RSO detection frame 50.

Figure 14. RSO detection frame 70

OpenCV Pipeline Results

Figure 16 shows an initial RSO and ﬂow detection from Figure 15 using the OpenCV pipeline.
This set of images represent a different set of images where the telescope was set to track the
satellite motion (GEO satellites) and stars streaked through the image. The main image motion then
occurs for the stars. Results are comparable to the MATLAB implementation in that detection was

9

reasonably accurate across the image set.

Figure 15. Raw ground-based image.

Figure 16. Optical ﬂow RSO detection
from ground imagery.

OpenCV was able to track successfully 4 out of 5 RSOs, with one in particular not being tracked
due to the low surface brightness of the satellite. In order to prevent this from happening, image
pre-processing similar to the methods employed in the MATLAB pipeline can be implemented.

Figure 17. RSOs (marked in red) identiﬁed with OpenCV moving from left bottom
corner to right upper corner.

CONCLUSION AND FUTURE WORK

The autonomous RSO detection algorithm using optical ﬂow was used to successfully track the
gross motion of stars and RSO objects through various simulated and real image sets. Overall
image-plane velocity, given by star-motion proved a successful discriminatory threshold for deter-
mining candidate RSO objects. Two separate pipelines utilizing commercial (MATLAB) and open
source (OpenCV) implementations of detection algorithms demonstrated promising results for RSO
detection. OpenCV has proved to be an easy and free alternative to commercial applications for this
type of workﬂow, and even possesses some advantages over MATLAB such as giving the user con-
trol of more parameters for optical ﬂow. Future work includes improving image pre-processing
to maximize detection results, standardizing the algorithm between the two pipelines, and running
numerous further image sets to benchmark the detection results. Potential different methods to pre-
process the images include blur removal and artifacts due to noise from different sources such as
sensors, light, atmospheric effects through advanced image ﬁltering.

10

REFERENCES

[1] J. Virtanen, J. Poikonen, T. S¨antti, T. Komulainen, J. Torppa, M. Granvik, K. Muinonen, H. Pentik¨ainen,
J. Martikainen, J. N¨ar¨anen, J. Lehti, and T. Flohrer, “Streak detection and analysis pipeline for space-
debris optical images,” Adv. Space Res., Vol. 57, Apr. 2016, pp. 1607–1623.

[2] T. Schildknecht, “Optical surveys for space debris,” The Astronomy and Astrophysics Review, Vol. 14,

01 2007, pp. 41–111. Copyright - Springer-Verlag 2007; Last updated - 2014-08-22.

[3] D. Zuehlke, “Space Image Processing and Orbit Estimation Using Small Aperture Optical Systems,”

Master’s thesis, Embry-Riddle Aeronautical University, 2019.

[4] G. Nir, B. Zackay, and E. O. Ofek, “Optimal and Efﬁcient Streak Detection in Astronomical Images,”

June 2018.

[5] B. Sease and B. Flewelling, “Polar and spherical image transformations for star localization and RSO

discrimination,” ResearchGate, Jan. 2015.

[6] D. Zuehlke and T. Henderson, “Geostationary Satellite Constellation Tracking and Identiﬁcation Using
Normalized Cross Correlation,” AIAA Scitech 2020 Forum, AIAA SciTech Forum, American Institute
of Aeronautics and Astronautics, Jan. 2020.

[7] D. Fortun, P. Bouthemy, and C. Kervrann, “Optical ﬂow modeling and computation: A survey,” Comput.

Vis. Image Underst., Vol. 134, May 2015, pp. 1–21.

[8] X. Xiang, M. Zhai, N. Lv, and A. El Saddik, “Vehicle counting based on vehicle detection and tracking

from aerial videos,” Sensors, Vol. 18, No. 8, 2018, p. 2560.

[9] E. Dur, “Optical ﬂow-based obstacle detection and avoidance behaviors for mobile robots used in un-
maned planetary exploration,” 2009 4th international conference on recent advances in space technolo-
gies, IEEE, 2009, pp. 638–647.

[10] B. Du, S. Cai, and C. Wu, “Object Tracking in Satellite Videos Based on a Multiframe Optical Flow
Tracker,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, Vol. 12,
Aug. 2019, pp. 3043–3055.

[11] G. Fasano, G. Ruﬁno, D. Accardo, and M. Grassi, “Satellite angular velocity estimation based on star

images and optical ﬂow techniques,” Sensors, Vol. 13, Sept. 2013, pp. 12771–12793.

[12] B. D. Lucas, T. Kanade, and Others, “An iterative image registration technique with an application to

stereo vision,” 1981.

[13] D. Lang, D. W. Hogg, K. Mierle, M. Blanton, and S. Roweis, “Astrometry.net: Blind Astrometric
Calibration Of Arbitrary Astronomical Images,” The Astronomical Journal, Vol. 139, No. 5, 2010,
p. 1782–1800, 10.1088/0004-6256/139/5/1782.

[14] F. Girosi, A. Verri, and V. Torre, “Constraints for the computation of optical ﬂow,” ResearchGate, Apr.

1989.

[15] B. K. P. Horn and B. G. Schunck, “Determining Optical Flow,” Techniques and Applications of Image
Understanding, Vol. 0281, International Society for Optics and Photonics, Nov. 1981, pp. 319–331.
[16] J. L. Barron, D. J. Fleet, and S. S. Beauchemin, “Performance Of Optical Flow Techniques,” Vol. 12,

Feb. 1994, pp. 43–77.

[17] “Optical Flow,” Mathworks. Available at https://www.mathworks.com/help/vision/ref/opticalﬂow.html#

bqi5zaf-1.

[18] “Optical Flow,” OpenCV. Available at https://docs.opencv.org/4.5.1/d4/dee/tutorial optical ﬂow.html.

11

