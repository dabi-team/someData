JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Just Noticeable Difference Modeling for Face
Recognition System

Yu Tian, Zhangkai Ni, Member, IEEE, Baoliang Chen, Shurun Wang, Shiqi Wang, Senior Member, IEEE,
Hanli Wang, Senior Member, IEEE, and Sam Kwong, Fellow, IEEE

2
2
0
2

p
e
S
3
1

]

V
C
.
s
c
[

1
v
6
5
8
5
0
.
9
0
2
2
:
v
i
X
r
a

Abstract‚ÄîHigh-quality face images are required to guarantee
the stability and reliability of automatic face recognition (FR) sys-
tems in surveillance and security scenarios. However, a massive
amount of face data is usually compressed before being analyzed
due to limitations on transmission or storage. The compressed
images may lose the powerful
identity information, resulting
in the performance degradation of the FR system. Herein, we
make the Ô¨Årst attempt to study just noticeable difference (JND)
for the FR system, which can be deÔ¨Åned as the maximum
distortion that the FR system cannot notice. More speciÔ¨Åcally,
we establish a JND dataset including 3530 original images and
137,670 compressed images generated by advanced reference
encoding/decoding software based on the Versatile Video Coding
(VVC) standard (VTM-15.0). Subsequently, we develop a novel
JND prediction model to directly infer JND images for the FR
system. In particular, in order to maximum redundancy removal
without impairment of robust identity information, we apply
the encoder with multiple feature extraction and attention-based
feature decomposition modules to progressively decompose face
identity and
features into two uncorrelated components,
residual features, via self-supervised learning. Then, the residual
feature is fed into the decoder to generate the residual map.
Finally, the predicted JND map is obtained by subtracting the
residual map from the original image. Experimental results have
demonstrated that the proposed model achieves higher accuracy
of JND map prediction compared with the state-of-the-art JND
models, and is capable of saving more bits while maintaining the
performance of the FR system compared with VTM-15.0.

i.e.,

Index Terms‚ÄîJust noticeable distortion, face recognition sys-

tem, deep neural network, image and video coding.

I. INTRODUCTION

R ECENTLY, advances in the Ô¨Åeld of face recognition

(FR) have made it the prominent biometric technique
for identity authentication in many areas [1], [2], [3], [4], [5],
[6], [7], such as public security, Ô¨Ånance, and human-computer
interaction. Since FR systems refer to computing the similarity
of discriminative features between two images, high-quality
face images are of paramount importance for achieving the
superior performance of FR systems. In general, as illustrated

Yu Tian, Baoliang Chen, Shurun Wang and Shiqi Wang are with the
Department of Computer Science, City University of Hong Kong, Hong Kong
999077
blchen6-c@my.cityu.edu.hk;
(e-mail:ytian73-c@my.cityu.edu.hk;
srwang3-c@my.cityu.edu.hk; shiqwang@cityu.edu.hk).

Zhangkai Ni and Hanli Wang are with the Department of Computer
Science and Technology, Key Laboratory of Embedded System and Service
Computing (Ministry of Education), and Shanghai Institute of Intelligent
Science and Technology, Tongji University, Shanghai 200092, P. R. China
(e-mail: eezkni@gmail.com; hanliwang@tongji.edu.cn).

Sam Kwong is with the Department of Computer Science, City University
of Hong Kong, Hong Kong 999077, and also with the City University of
Hong Kong Shenzhen Research Institute, Shenzhen 518057, China (e-mail:
cssamk@cityu.edu.hk).

in Fig. 1(a), FR systems deployed in the cloud server perform
recognition over two images sampled from the gallery set and
probe set, respectively. The gallery set is composed of face
data with known subjects stored in the cloud databases, and
the probe set is collected from various remote devices such
as smartphones, monitors and laptops. In some application
scenarios, the probe set is required to be compressed before
being transmitted to the cloud server to facilitate this remote
computing process. Unfortunately, the FR system might not
correctly recognize the compressed images with incomplete
identity information from corrupted images [8]. Therefore, it
is highly desirable to Ô¨Ånd the optimal solution for balancing
coding bits and the performance of the FR system.

Just noticeable difference (JND), which characterizes the
minimum perceptual threshold of signal change below which
the change cannot be perceived by the ultimate receiver, has
been widely used for improving the coding efÔ¨Åciency of
image/video compression algorithms [9], [10], [11], [12], [13].
Therefore, we consider performing JND prediction before
image compression over the probe set, as shown in Fig. 1(b).
According to different ultimate receivers of images/video sig-
nals, existing JND models can be divided into two categories:
JND models for the human visual system (HVS) and JND
models for machine vision. Regarding JND models for the
HVS, we can summarize them according to the domain over
which the JND threshold is computed into two categories: 1)
pixel-domain JND models [14], [15], [16], [17], [18], which
predict the JND threshold for each pixel of the image; 2) sub-
band domain JND models [19], [11], [20], which predict the
JND threshold for each sub-band by transforming the pixel-
domain image into a speciÔ¨Åc transform domain (e.g., discrete
cosine transformation (DCT), Karhunen-Lo¬¥eve transformation
(KLT)). However, those HVS-oriented JND models are not
proper models to estimate JND for the FR system, as the FR
systems focus on discriminative information for analysis and
recognition instead of luminance and texture information that
most HVS-orientated models rely on.

Regarding JND models for machine vision, Zhang et al.
[21] studied the impact of image/video compression on the
performance of networks for image classiÔ¨Åcation and object
detection tasks. They Ô¨Årstly established a large-scale JND-
annotated dataset including over 340,000 images for image
classiÔ¨Åcation and object detection tasks. The details can be
found in Table I. Subsequently, two JND prediction models
are proposed under no-reference and few-reference circum-
stances, named JND-MV-NR and JND-MV-FR, respectively.
Jin et al. [22] proposed a JND prediction model for the

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

neural network (DNN) based FR algorithms have been widely
studied, which are commonly optimized with a well-designed
loss function to obtain more discriminative and generalizable
feature representation. For example, Taigman et al. [3] trained
the AlexNet [26] with a softmax loss function to propose
the Ô¨Årst DNN-based FR algorithm DeepFace. Schroff et al.
[2] proposed a triplet loss to optimize the face embeddings
extracted by GoogLeNet-24 [27] by minimizing the distance
between an anchor and a positive sample and maximizing the
distance between the anchor and a negative sample. Wen et
al. [28] proposed a center loss to learn the center of each
class and joint it with the softmax loss function to enhance
the discriminative power of LeNet [29].

Recently, a popular line of designing the FR algorithm is
introducing a large margin to separate hard samples strictly.
Liu et al. [30] proposed the angular softmax loss (SphereFace)
to learn the angularly discriminative feature on a hypersphere
manifold. Following the work in [30], Wang et al. [4] proposed
a new large margin loss (LMCL) by introducing an additive
cosine margin penalty, and Deng et al. [5] proposed an
additive angular-based margin loss (ArcFace). Based on the
fact
that easy samples and hard samples are of different
importance during the training phase, Huang et al. [6] de-
veloped a curriculum learning loss (CurricularFace) to adjust
the importance of different samples adaptively. To mitigate the
overÔ¨Åtting problem, Zhong et al. [7] introduced the sigmoid-
constrained hypersphere loss (SFace) to control intra-class and
inter-class distances on the hypersphere manifold. Extending
from the study of SphereFace, Liu et al. [31] incorporated
the multiplicative margin to propose an improved SphereFace
(SphereFaceR).

B. Just Noticeable Difference

According to the Weber‚Äôs law [32], JND can be deÔ¨Åned
as the minimum amount by which stimulus intensity must be
changed to produce a noticeable variation in sensory expe-
rience. In the literature, the majority of pixel- and subband-
domain JND research is based on human perception experi-
ence. Regarding the pixel-domain JND estimation, considering
the effects of the luminance adaption (LA) and contrast mask-
ing (CM) on the threshold sensitivities of the HVS, Chou et al.
[33] proposed a pioneer JND model to estimate the visibility
threshold associated with each pixel of the image. Afterward,
Yang et al. [34] re-formulated the CM model by taking the
edge information into account. Liu et al. [16] further optimized
the CM estimation algorithm by decomposing CM into two
parts, i.e., edge masking (EM) and texture masking (TM).
Since the HVS pays more attention to orderly information than
disorderly information, Wu et al. [17] proposed a JND model
based on the free-energy principle to compute JND thresholds
in terms of the orderly contents and the disorderly contents
of the original image. Inspired by the orientation selectivity
mechanism in the primary visual cortex, Wu et al. [18]
combined the pattern complexity with the luminance contrast
to build a novel pattern masking for the JND prediction.
Shen et al. [13] considered the fact that the JND proÔ¨Åle is
highly related to the local image content of the image and thus

Fig. 1.
proposed JND algorithm in the remote FR process.

(a) Overview of the remote FR process; (b) Application of the

image classiÔ¨Åcation task, which employs a semantic-guided
redundancy assessment strategy to estimate the JND map via
unsupervised learning. It is worth mentioning that the above-
mentioned works investigate the JND on datasets with a large
inter-variance, resulting in producing a large JND value for
each image. Considering that face images shall have a small
inter-class distance, more robust and discriminative features
should be explored in JND prediction for the FR system.

In this paper, we make the Ô¨Årst attempt to study the JND for
the FR system. The major contributions can be summarized
as follows:

‚Ä¢ We establish a JND dataset served for the FR system. In
particular, the proposed dataset consists of 3520 original
images collected from the MegaFace dataset and 137,670
compressed images using VTM-15.0 intra-coding with
39 QP values ranging from 13 to 51. Subsequently, we
conduct extensive recognition tests to search JND images
for the FR system.

‚Ä¢ We develop a novel JND prediction framework for face
recognition systems. The fundamental idea behind the
proposed framework is to perform identity informa-
tion preservation and distortion maximization simultane-
ously when inferring JND images. Therefore, we devise
attention-based feature decomposition (AFD) modules
and employ the self-supervised optimization strategy.
‚Ä¢ Experimental results show that the proposed JND predic-
tion model achieves better performance than the state-of-
the-art methods. Furthermore, compared with VTM-15.0
intra coding, the image coding framework combined with
our proposed model can save more bits while maintaining
the performance of the FR system.

II. RELATED WORKS

A. Deep Face Recognition

FR can be classiÔ¨Åed as face identiÔ¨Åcation and face veriÔ¨Å-
cation, both of which can essentially boil down to measuring
the similarity of two face images. Traditional FR algorithms
mainly use handcrafted features (e.g., Gabor [23], histogram of
oriented gradients [24] and local binary patterns [25] features)
to calculate the similarity of two face images. Due to the
limited representational capacity of handcrafted features, deep

Databases(Gallery Set)Remote DevicesCompressionFR systemRemote DevicesJND predictionFR systemCompression(a)(b) Probe SetDatabases(Gallery Set)Probe SetJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

B. Study of Just Noticeable Difference

Let x and y denote two images sampled from the probe
and gallery sets, respectively. Moreover, ht represents the FR
system, which determines whether two images are of the same
identity at a given threshold t. In particular, ht(x, y) = 1
if the distance between feature representations of x and y
is less than the threshold t, which means x and y are of
the same identity. By contrast, ht(x, y) = 0 if the distance
between feature representations of x and y is not less than the
threshold t, which implies x and y have different identities.
In this study, our purpose is to Ô¨Ånd an image ÀÜx to satisfy
ht(ÀÜx, y) = ht(x, y), where ÀÜx represents the distorted version
of the original image x containing the maximum signal loss
caused by image compression that the FR system does not
perceive.

Fig. 2. Recognition accuracy of different FR algorithms on our proposed
dataset.

proposed a JND proÔ¨Åle inference scheme based on patch-level
structural visibility learning. Regarding the subband-domain
JND estimation, Wei et al. [35] proposed a DCT-based JND
estimation model, which considered the effects of LA, CM
and spatial contrast sensitivity function (CSF). Ma et al. [36]
applied an adaptive block transform including two DCT block
sizes of 16√ó16 and 8√ó8 to improve the accuracy of the JND
estimation model. Bae et al. [19] developed a new texture
complexity metric for estimating the JND, which can reÔ¨Çect
the perceived complexity. Jiang et al. [20] estimated the JND
threshold of the image via the minimum number of spectral
components in KLT.

III. THE PROPOSED DATASET
Our proposed dataset is established with the aim of inves-
tigating the impact of image compression on the performance
of the FR system. Therefore, both original face images and
their corresponding compressed versions are included in our
proposed dataset.

A. Data Collection

We Ô¨Årst collect 3530 face images of 80 identities from
the probe set in the publicly available reÔ¨Åned version [5] of
the MegaFace dataset [37] as original images. Those images
cover various variations such as ages, poses, and styles and
have a Ô¨Åxed resolution of 112√ó112. Subsequently, we apply
the advanced reference encoding/decoding software based on
the Versatile Video Coding (VVC) standard (VTM-15.0) [38]
to encode original face images using different QPs. More
speciÔ¨Åcally, we convert each original face image into YUV
format and compress it with 39 QPs ranging from 13 to 51,
where the larger QP value indicates the more severe signal-
level corruption. The compressed YUV data is collected and
converted back into RGB format to obtain compressed face
images at different distortion levels. As a result, there are 3530
original face images and 137,670 compressed face images in
total for the JND study.

i=1}S

s,i }Ns

To this end, we conduct several recognition tests with
different probe sets. More speciÔ¨Åcally, we Ô¨Årst split all images
in our dataset into 40 probe sets R = {R(k)}39
k=0 according
to different QP values. The k-th probe set R(k) is in the form
of {{r(k)
s=1, where k and S mean the compression
level and the number of identities, respectively, and Ns is
the number of images of the s-th identity. The probe set is
composed of all original (uncompressed) images when k = 0.
Subsequently, in order to establish the gallery set, we check the
original gallery set in the MegaFace dataset and Ô¨Ånd that there
are many blurry face images and even some images without the
human face. Herein, we use a face image quality metric [42]
to evaluate the quality of all images within the original gallery
set and sort them according to their corresponding quality
scores from high to low. The top 10,000 face images with high
recognizability D = {dm}104
m=1 are selected. We combine them
with all original images to establish the Ô¨Ånal gallery set M =
R(0) ‚à™ D of our recognition tests. During the recognition test
on the k-th probe set, each image within the probe set is paired
with images with the same identity within the gallery set M to
form positive pairs P (k) = {{{(r(k)
s=1, and
paired with images with different identities to form negative
pairs G(k) = {{{(r(k)
s=1. Then, we employ
the FR system to verify all positive and negative pairs. After
performing all recognition tests, for the i-th original image of
the s-th identity, we seek to Ô¨Ånd a maximum QP (denoted as
Àúq) such that r(Àúq)

s,i satisÔ¨Åes the following constraints:

s,i , dm)}104

s,j )}Ns‚àí1

j(cid:54)=i }Ns

s,i , r(0)

m=1}Ns

i=1}S

i=1}S

Ns‚àí1
(cid:88)

j(cid:54)=i

ht0(r(Àúq)

s,i , r(0)

s,j ) ‚â•

Ns‚àí1
(cid:88)

j(cid:54)=i

ht0 (r(0)

s,i , r(0)
s,j ),

104
(cid:88)

m=1

ht0 (r(Àúq)

s,i , dm) ‚â§

104
(cid:88)

m=1

ht0(r(0)

s,i , dm),

(1)

(2)

where t0 represents the threshold with respect to 10‚àí4 false
acceptance rate (FAR) on G(0). As a result, Àúq and r(Àúq)
s,i are
the corresponding JND point and JND image of the original
image r(0)

s,i , respectively.

We initially enroll Ô¨Åve FR algorithms trained on different
circumstances (i.e., loss functions and backbone architectures),
both of which are pre-trained algorithms provided by the

01722273237424751QP00.10.20.30.40.50.60.70.80.91AccuracyArcFace-IR100 ArcFace-MobileNetSphereFaceR-IR100 SphereFaceR-SFNet20CurricularFace-IR100JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

TABLE I
COMPARISON OF EXISTING BENCHMARK JND DATABASES FOR MACHINE VISION.

Dataset

# of
original images

Zhang et al. [21]

355,541

Source of
original images
PASCAL VOC [39]
and MS COCO [40]

HEVC [41]

Distortion type

QP

Proposed

3,520

MegaFace [37]

VTM-15.0 [38]

Ground-truth
JND

Avaliable

Machine vision
task
Image classiÔ¨Åcation/
object detection

Avalibale

Face recognition

[18, 22, 27, 32, 38, 41, 43, 45,
47, 49, 51]
[13, 14, 15, 16, 17, 18, 19, 20,
21, 22, 23, 24, 25, 26, 27, 28,
29, 30, 31, 32, 33, 34, 35, 36,
37, 38, 39, 40, 41, 42, 43, 44,
45, 46, 47, 48, 49, 50, 51]

(a)

(b)

Fig. 3. Results of the JND noticeable difference study for the FR system. (a) The JND points distribution of original face images in the proposed dataset;
(b) BPP and ‚ÄúAccuracy‚Äù comparison among JND and different QPs.

corresponding authors. Fig. 2 shows the recognition perfor-
mance of all candidate FR algorithms when the probe images
are compressed at different QP values, where the ‚ÄúAccuracy‚Äù
refers to the true acceptance rate (TAR) of the FR algorithm at
its corresponding threshold t0. From the experimental results,
one can easily observe that the recognition performances of
all candidate FR systems decrease as the QP value increases.
When the loss function is Ô¨Åxed, the FR algorithm employing
the ResNet-100 [37] network achieves higher accuracy and
robustness than the FR algorithm employing other architec-
tures. For example, when the QP value enlarges from 13
to 37, ArcFace-IR100 only reduces 1.3% of accuracy while
ArcFace-MobleNet reduces 7.14% of accuracy. Additionally,
we observe that although there are a variety of loss functions,
FR algorithms with the same backbone architecture show a
signiÔ¨Åcant performance degradation at the same QP value, and
the recognition performance of all algorithms is catastroph-
ically compromised when the QP value rises to 51. Based
on the above observations, we employ ArcFace-IR100 as the
default algorithm of our FR system. Fig. 3(a) is the JND
points distribution of original face images in our proposed
dataset. From the results of Fig. 3(a), we can see that the
distribution of JND points spans a wide range, which implies
that the levels of discriminative information carried by original
images in the proposed dataset are various. Subsequently,
we encode all original images with their corresponding JND

points and different QP points, respectively. The average bits
per pixel (BPP) over the compressed images and the accuracy
performance of the FR system on the compressed images
are shown in Fig. 3(b). It can be clearly Ô¨Ånd that the JND
study sheds light on the optimization of the image coding
framework. In particular, the image coding framework using
JND prediction leads to 81% bit-rate savings at least while
keeping the performance of the FR system, and it achieves 5%
of performance improvement under the same bit-rate level.

IV. THE PROPOSED MODEL

A. Motivation

Let x denote an input face image. The distorted image Àúx can
be represented as the linear combination of x and e, i.e., Àúx =
x+e, where e is the disturbance caused by image compression.
According to the general deÔ¨Ånition of the JND [43], the JND
prediction problem for the FR system can be formulated as
follows,

max e, s.t. (cid:107)xid ‚àí Àúxid(cid:107)2 ‚â§ (cid:15),
where xid and Àúxid represent the identity information of x and
Àúx, respectively. (cid:15) is the threshold of the distance between
xid and Àúxid below which Àúx and x can be recognized as
being from the same subject. Our goal is to design a JND
prediction model for the FR system, which can automatically
infer the JND version of each input face image. To this end, we

(3)

131722273237424751QP050100150200250300350400Number of Images00.250.50.7511.25BPP0.10.20.30.40.50.60.70.80.91AccuracyQPJNDJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Fig. 4. Overview of the proposed JND prediction model. The purpose is to infer the JND map of the input image that satisÔ¨Åes distortion maximization and
identity information preservation. Given a probe image r(0)
s,i , the encoder is applied to extract the multi-level features and decompose them into identity-related
and residual features. To maximize the feature separation, we introduce positive and negative samples. The positive sample r(0)
s,j is sampled from the original
images. The negative sample r(0)
c,a is produced within the training batch. We apply Lid to optimize the learned feature space by maximizing the inter-class
distance and minimizing the intra-class distance. Finally, multi-scale residual features are fed into the decoder to infer the residual map, and the Ô¨Ånal predicted
JND map ÀÜr(0)
s,i closer to the ground-truth JND map r(Àúq)
s,i ,
we employ Lcontent and Lpixel to constrain the predicted result in terms of pixel domain and semantic content, respectively.

s,i can be generated by subtracting the residual map from r(0)

s,i . To make the predicted JND map ÀÜr(0)

design it based on ResNet-50 [5], [44], which consists of a
convolution unit (denoted as EnConv), four basic blocks and
Ô¨Åve AFD modules. EnConv represents a 3√ó3 convolution with
stride of 1 followed by a batch-normalization (BN) layer [45]
and PReLU activation function. The structure of the basic
block is similar to the residual block of ResNet-50 in [44],
which is stacked by cascaded convolutional layers and the
residual units. The human perception process presents the
importance of the attention mechanism [46]. We use the AFD
module [47], [48] to capture the discriminative feature that the
FR system relies on for image understanding and recognition.
The detailed structure of the AFD module is shown in Fig.
5. To be speciÔ¨Åc, the input of the AFD module is the mixed
feature (denoted as F ‚àà RC√óH√óW ) extracted by the previous
module (i.e., EnConv or Basic Block). We Ô¨Årst aggregate
spatial features of each channel through mean and standard
deviation calculations (denoted as Mean and Std), generating
two spatial descriptors (denoted as Favg ‚àà RC√ó1√ó1 and
Fstd ‚àà RC√ó1√ó1). Then, we concatenate Favg and Fstd and
pass the resulted feature through two 1 √ó 1 convolutions with
stride of 1 (denoted as Conv1√ó1) to generate the attention
feature. Subsequently, we fuse the attention feature and the
input feature F to produce the identity feature (denoted as
Fid ‚àà RC√óH√óW ). As suggested in [49], [50], the residual
feature (denoted as Fres ‚àà RC√óH√óW ) can be generated as:
Fres = F ‚àí Fid. Regarding the decoder, it contains multiple
upsampling layers for upsampling the spatial resolution of the
input feature and deconvolutional units (denoted as DeConv)
for reducing the size of channels. In particular,
the input
feature Ô¨Årst passes through the upsampling layer, which is

Fig. 5. The detailed structure of the AFD module.

decompose the JND prediction problem into two sub-problems
and jointly tackles them with two parts: 1) identity information
preservation; 2) distortion maximization. Regarding identity
information preservation, considering the face image can be
jointly represented by the intrinsic identity information and
various face variations, we embed the AFD module in the
encoder to separate the mixed feature into identity and resid-
ual components. In this manner, the decoder can utilize the
residual feature to generate the residual map without losing
the identity information. Regarding distortion maximization,
positive and negative pairs are introduced to optimize the
learning procedure of identity and residual features in a self-
supervised manner. The overview of our proposed framework
is shown in Fig. 4. The details will be described in the
following sub-sections.

B. Network Architecture

As shown in Fig. 4, our proposed model is built upon an
encoder-decoder architecture with skip connections between
the encoder and decoder layers. Regrading the encoder, we

ùëü!,#(%)CCCCEncoderDecoderùìõùíäùíÖùëü!,#(34)ùìõùíÑùíêùíèùíïùíÜùíèùíï+ùìõùíëùíäùíôùíÜùíçConcatCBasic BlockAFDDeConvTransmissionIdentity Feature FlowResidual Feature FlowÃÇùëü!,#(%)ùëü5,6(%)EnConvUpsample646412825651225612864643Encoderùëü!,2(%)64641282565126464128256512EncoderSharewhcMeanStdùë≠CConv7√ó7ReLUSigmoidwhcùë≠ùíäùíÖwcùë≠ùíìùíÜùíîConv7√ó7hwhcMeanStdùë≠CConv7√ó7ReLUwhcùë≠ùíäùíÖwcùë≠ùíìùíÜùíîConv7√ó7hCConv7√ó7JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

implemented by the bilinear interpolation. The upsampled
feature is concatenated to the residual feature of the encoder
the corresponding location via the skip connection. It
at
is worthwhile noting that the skip connections make only
the residual features available for the decoder, avoiding the
introduction of identity features during the process of residual
map generation.

4) Total Loss: By jointly considering identity loss, pixel
loss, and content loss, the Ô¨Ånal loss is deÔ¨Åned as the weighted
sum of these losses, that is,

Ltotal = ŒªidLid + ŒªpixelLpixel + ŒªcontentLcontent,

(7)

where Œªid, Œªpixel, and Œªcontent are weighting parameters to
balance the relative importance of Lid, Lpixel, and Lcontent.

C. Loss Function

1) Identity Loss: The identity information that the FR sys-
tem relies on for recognition is hidden in the feature space and
does not have the corresponding ground-truth representation.
This motivated us to extract
the intrinsic identity feature
in a self-supervised learning manner. BeneÔ¨Åting from the
properties of identity consistency and identity diversity, we
employ self-supervised contrastive learning [51], [52], [53] to
capture inter-person and intra-person correspondences. Given
an anchor image, we Ô¨Årst Ô¨Ånd the appropriate positive and
negative samples with respect to the anchor image. To be
speciÔ¨Åc, considering that the more robust feature can be mined
via the more signiÔ¨Åcant intra-person variation, we use the
positive image with the largest variation to the anchor image.
The negative sample is produced within the training mini-
batch by shufÔ¨Çing training samples of a batch. After that, we
maximize information from identity features within positive
samples while minimizing those within negative samples.
Following [51], [52], the identity loss is formulated as follows:

Lid = log

(cid:16)

1 + e(cid:107)v‚àív+(cid:107)1

(cid:17)

+ log

(cid:16)

1 + e‚àí(cid:107)v‚àív‚àí(cid:107)1

(cid:17)

,

(4)

where v, v+ and v‚àí are identity features of the anchor, positive
and negative images extracted by the Ô¨Åfth AFD module,
respectively. || ¬∑ ||1 denotes (cid:96)1 distance.

2) Pixel Loss: In order to force the predicted JND image to
be closer to the ground-truth JND image, the straightforward
method is to constrain the distance between the two images
in the pixel domain. Herein, the pixel loss is computed as:

Lpixel =

(cid:13)
(cid:13)r(Àúq)
(cid:13)

s,i ‚àí ÀÜr(0)

s,i

(cid:13)
(cid:13)
(cid:13)1

,

(5)

where ÀÜr(0)
s,i
image r(0)
s,i .

is the predicted JND image of the original probe

3) Content Loss: We use the content loss [54], [55] to make
the predicted JND image consistent with the ground-truth JND
image from the aspects of semantic content. The content loss
is calculated as follows,

Lcontent =

K
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)œÜk

(cid:17)

(cid:16)

r(Àúq)
s,i

‚àí œÜk

(cid:16)

ÀÜr(0)
s,i

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

,

(6)

where œÜk indicates the mapping function that project
the
image to the feature representation at the k-th layer of the
VGG space and K is the total number of layers. In this
work, we adopt feature representations from the Relu 1 1,
Relu 2 1, Relu 3 1, Relu 4 1, and Relu 5 1 layers in VGG-
19 network.

V. EXPERIMENT

In this section, we Ô¨Årst present the implementation details
of the proposed model. We then compare our proposed model
with the existing JND models in terms of accuracy perfor-
mance on predicting the JND image. Furthermore, we demon-
strate the effectiveness of the proposed method in optimizing
image coding for the FR system.

A. Implementation Details

We implement our model using PyTorch [57]. The inputs
of our network are resized to 112 √ó 112 √ó 3, and the mini-
batch size is set as 12. The whole network is trained via the
Adam [58] optimizer. The maximum epoch is 100. The leaning
rate is set by 1e-4 at the Ô¨Årst 50 epochs. After 50 epochs, the
learning rate is reduced to its 1/5 every 1 epoch. The weighting
parameters Œªid, Œªpixel, Œªcontent in Eqn. (7) are set as 0.5, 5.0,
and 0.1. In the experiments, to reduce the negative impact of
excessive inter-class distance on the JND study for the FR
system, we remove the original probe images that the FR
system can not recognize correctly from our proposed dataset.
As a result, 3501 original images and 136,539 compressed
images are employed.

B. Comparisons With State-of-the-Art Methods

In this experiment, we aim to explore the prediction ac-
in directly predicting JND
curacy of the proposed model
the FR system. Several classical and state-
images for
of-the-art HVS-oriented JND prediction models,
includ-
ing Zhang05 [56], Yang05 [14], Liu10 [16], Wu17 [18],
Shen21 [13], Jiang22 [20], are employed for performance
comparison. To ensure a fair comparison, all the source codes
of the competing models are obtained from their authors. We
randomly split our dataset into Ô¨Åve subsets to conduct Ô¨Åve-
fold cross-validation. In each trial, four subsets are selected for
training the proposed JND prediction model, and the remaining
one subset
is used for testing. Five objective metrics are
selected as the evaluation criteria. They are peak signal-to-
noise ratio (PSNR), learned perceptual image patch similarity
(LPIPS) metric, similarity distribution distance for face im-
age quality assessment (SDD-FIQA) metric [42], stochastic
embedding robustness for face image quality (SER-FIQ) [59]
evaluation metric and FaceQnet V1 [60],
the latter three
of which are no-reference image quality assessment metrics
speciÔ¨Åcally designed to estimate the recognizability of face
images for the FR systems. Regarding PSNR and LPIPS, we
compute the average similarity yielded between the predicted
JND images and their ground truth JND images over the test
set. Regarding SDD-FIQA, SER-FIQ and FaceQnet V1, we

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

TABLE II
ACCURACY COMPARISON OF THE PROPOSED MODEL AND THE EXISTING JND MODELS IN TERMS OF INFERRING JND IMAGES FOR THE FR SYSTEM.

Index

G1

G2

G3

G4

G5

Average

Measures
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì
PNSR ‚Üë
LPIPS ‚Üì
(cid:52)SDD-FIQA ‚Üì
(cid:52)SER-FIQ ‚Üì
(cid:52)FaceQnet V1 ‚Üì

Yang05 [14]
25.8524
0.2689
16.0740
0.1540
0.0677
25.8467
0.2672
16.2668
0.1584
0.0633
25.7987
0.2697
16.2847
0.1488
0.0664
25.8296
0.2669
16.1128
0.1508
0.0672
25.8668
0.2667
16.1872
0.1564
0.0649
25.8388
0.2679
16.1851
0.1537
0.0659

Zhang05 [56]
27.2942
0.2769
15.4714
0.1548
0.0614
27.3780
0.2761
15.6895
0.1585
0.0570
27.2230
0.2779
15.6275
0.1492
0.0595
27.2780
0.2752
15.5447
0.1509
0.0608
27.3788
0.2754
15.6177
0.1562
0.0593
27.3104
0.2763
15.5901
0.1539
0.0596

Liu10 [16]
25.4615
0.2673
15.9986
0.1534
0.0709
25.4507
0.2658
16.1745
0.1578
0.0657
25.4046
0.2681
16.1983
0.1483
0.0687
25.4390
0.2654
16.0123
0.1504
0.0693
25.4699
0.2652
16.1385
0.1557
0.0674
25.4451
0.2664
16.1044
0.1531
0.0684

Wu17 [18]
25.2887
0.2756
15.6548
0.1503
0.0678
25.3584
0.2741
15.8395
0.1557
0.0630
25.2905
0.2761
15.9101
0.1475
0.0662
25.3124
0.2740
15.5808
0.1468
0.0661
25.3837
0.2729
15.7609
0.1521
0.0655
25.3268
0.2745
15.7492
0.1505
0.0657

Shen21 [13]
27.8275
0.2090
15.1498
0.1421
0.0694
27.8735
0.2068
15.3610
0.1465
0.0646
27.7516
0.2112
15.3728
0.1374
0.0666
27.8100
0.2075
15.2016
0.1405
0.0686
27.8891
0.2058
15.2668
0.1442
0.0656
27.8303
0.2081
15.2704
0.1421
0.0670

Jiang22 [20]
28.1862
0.2548
15.8685
0.1550
0.0623
28.3158
0.2518
16.0774
0.1601
0.0582
28.1192
0.2543
15.9742
0.1504
0.0607
28.1842
0.2523
15.8462
0.1529
0.0613
28.3047
0.2508
16.0163
0.1576
0.0598
28.2220
0.2528
15.9565
0.1552
0.0646

Proposed
30.9292
0.1301
9.3969
0.1302
0.0521
31.0571
0.1283
9.0470
0.1346
0.0466
30.8576
0.1313
9.5549
0.1267
0.0483
30.9456
0.1283
8.9222
0.1301
0.0481
30.9641
0.1286
8.9844
0.1319
0.0462
30.9470
0.1293
9.1811
0.1307
0.0483

compare the average difference between recognizability scores
of the predicted JND images and those of the ground truth
JND images over the test set. The higher values of PSNR
and the lower value of LPIPS, (cid:52)SDD-FIQA, (cid:52)SER-FIQ and
(cid:52)FaceQnet V1 represent better performance, which indicates
that the predicted JND image from the model is closer to the
ground truth JND image. The results are listed in Table II,
where Gi means the i-th trial, the Ô¨Årst-, second- and third-
best performances are highlighted in red, blue and black bold,
respectively.

From the experimental results, one can obviously observe
that compared with the competing JND prediction models,
our proposed model achieves higher PSNR values and lower
LPIPS, (cid:52)SDD-FIQA, (cid:52)SER-FIQ and (cid:52)FaceQnet V1 values
on both G1, G2, G3, G4 and G5. The main reason is the
gap in the perceptual characteristics between the FR system
and the HVS. By considering the properties of the FR system
in recognizing the face image, we combine the low-level
structure and high-level semantic information into the JND
image inference procedure. Therefore, the proposed model can
be more consistent with the perception of the FR system than
the classical and state-of-the-art JND prediction models.

To make a more explicit comparison between these HVS-
orientated JND prediction models and our proposed model,
we provide some examples of the predicted JND and residual

images as shown in Fig. 6 and Fig. 7. Comparing the predicted
results of our proposed model (see Fig. 6(m)) with that of
Zhang05, Wu17 and Shen21 (see Fig. 6(i), (j) and (k)), one
can Ô¨Ånd that Zhang05, Wu17 and Shen21 deem that
the
low luminance area contains more redundant
information.
This is because Zhang05, Wu17 and Shen21 are modeled by
considering the LA mechanism of the HVS. Such luminance-
dependent JND prediction models fail to estimate the accurate
JND map under the low luminance environment. Since the
HVS is sensitive to the foreground regions of ordered textures
and insensitive to the complex textured regions, those HVS-
orientated JND prediction models cannot correctly Ô¨Ånd dis-
criminative regions that the FR system focuses on speciÔ¨Åcally
when estimating the occluded face images. For example, the
complex textured regions (e.g., hair, eyebrows) that might
provide some useful clues about the identity of the subject
are underestimated (see 7(i), (j), (k)), and the irrelevant infor-
mation carried by the occlusion (i.e., the mobile phone in the
Fig. 7(a)) are received overmuch attention (see 7(i) and (l)).
All these reveal that our proposed model can obtain better
consistency with the perception of the FR system and make
an accurate estimation.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

Fig. 6. Visual comparison of JND images and residual images generated by different JND models. (a): A low-brightness original face image selected from
our proposed dataset; (b)&(h): the ground-truth JND image and the residual image; (c)&(i): the JND image and the residual image generated by Zhang05 [56],
the PNSR value between (b) and (c) is 31.4499dB; (d)&(j): the JND image and the residual image generated by Wu17 [18], the PNSR value between (b) and
(d) is 28.3692dB; (e)&(k): the JND image and the residual image generated by Shen21 [13], the PNSR value between (b) and (e) is 27.9426dB; (f)&(l): the
JND image and the residual image generated by Jiang22 [20], the PNSR value between (b) and (f) is 33.4728dB; (g)&(m): the JND image and the residual
image generated by our proposed model, the PNSR value between (b) and (g) is 36.1064dB.

Fig. 7. Visual comparison of JND images and residual images generated by different JND models. (a): An occluded original face image selected from our
proposed dataset; (b)&(h): the ground-truth JND image and the residual image; (c)&(i): the JND image and the residual image generated by Zhang05 [56],
the PNSR value between (b) and (c) is 26.0927dB; (d)&(j): the JND image and the residual image generated by Wu17 [18], the PNSR value between (b) and
(d) is 24.3461dB; (e)&(k) are the corresponding the JND image and the residual image generated by Shen21 [13], the PNSR value between (b) and (e) is
26.7643dB; (f)&(l): the JND image and the residual image generated by Jiang22 [20], the PNSR value between (b) and (f) is 26.8419dB; (g)&(m): the JND
image and the residual image generated by our proposed model, the PNSR value between (b) and (g) is 29.8513dB.

C. Application in Remote FR System

In this subsection, we evaluate the performance of the
proposed JND model in improving the efÔ¨Åciency of image
compression. Here, we denote x and x(q) as the original face
image and its corresponding compressed image with QP equals
to q, respectively, ÀÜx is the predicted JND image produced by
the JND prediction model. As such, we can search the optimal
ÀÜq of the predicted JND image ÀÜx as follows,

ÀÜq = arg max

q

(cid:16)

g

x(q), ÀÜx

(cid:17)

,

(8)

where g (cid:0)x(q), ÀÜx(cid:1) denotes PNSR value of x(q) and ÀÜx. The
optimal ÀÜq is viewed as the predicted JND point of x.

We compare our proposed model with two JND models for
machine vision, i.e., JND-MV-NR [21] and JND-MV-FR [21].

It is worth mentioning that JND-MV-NR and JND-MV-FR are
composed of multiple classiÔ¨Åers, which can directly predict
the optimal QP value for each input image. JND-MV-NR and
JND-MV-FR trained on our proposed dataset are employed
for comparison. We conduct the evaluation experiment by
applying each JND model to VTM-15.0 intra coding frame-
work. More speciÔ¨Åcally, we select the VTM 5.0 intra coding
frameworks with QPs of 24, 32, 35, 36, 37, 40, 41 and 42 as
the anchors. All test images in each trial are compressed by
the VTM-15.0 intra coding with their corresponding optimal
QP values produced by the JND model and the employed
anchor QP values, respectively. The compressed images are
fed into the FR system to examine whether they can be
recognized correctly. The evaluation results in terms of BPP
and ‚ÄúAccuracy‚Äù (denoted ACC.) are reported in Table III, from

(a)(b)(c)(d)(e)(f)(g)(h)(i)(j)(k)(l)(m)(a)(b)(c)(d)(e)(f)(g)(h)(i)(j)(k)(l)(m)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

TABLE III
EVALUATION OF THE PROPOSED MODEL ON IMPROVING THE EFFICIENCY OF IMAGE/VIDEO COMPRESSION.

JND-MV-FR [21] Proposed QP24
1.44

0.38

Index Measures

G1

G2

G3

G4

G5

Average

BPP
ACC.
BPP
ACC.
BPP
ACC.
BPP
ACC.
BPP
ACC.
BPP
ACC.

JND-MV-NR [21]
0.21
88.91%
0.21
90.82%
0.22
91.33%
0.22
90.96%
0.22
90.20%
0.22
90.44%

0.22
91.41%
0.22
88.52%
0.22
90.95%
0.21
88.83%
0.23
92.24%
0.22
90.39%

1.44

0.36

0.41

0.59

0.33

0.38

1.44

QP35
0.42

QP40
0.24

QP32
0.60

QP42
0.20

QP37
0.33

QP41
0.22

QP36
0.37
99.23% 99.75% 99.53% 99.17% 99.00% 98.50% 95.08% 92.09% 87.61%
0.37
98.71% 99.62% 99.26% 98.82% 98.66% 97.99% 94.17% 91.05% 86.13%
0.37
98.85% 99.75% 99.54% 99.03% 98.63% 98.16% 94.98% 92.47% 88.75%
0.37
98.97% 99.64% 99.39% 99.01% 98.75% 98.35% 94.91% 92.31% 88.02%
0.37
98.71% 99.65% 99.53% 99.11% 98.70% 98.38% 94.60% 92.02% 88.25%
0.37
98.90% 99.68% 99.45% 99.03% 98.75% 98.28% 94.86% 91.99% 87.75%

0.34

0.22

0.36

0.33

0.20

0.59

0.20

0.33

0.24

1.43

0.42

0.24

0.22

0.42

0.24

0.59

0.33

1.44

0.20

0.22

0.20

0.20

0.33

0.36

0.24

0.59

0.42

0.22

1.43

0.42

0.24

0.22

0.59

TABLE IV
PERFORMANCE COMPARISON OF AVERAGE Distpos AND Distneg OF THE
PROPOSED FRAMEWORK USING DIFFERENT OPTIMIZATION STRATEGIES
ON G1, G2, G3, G4 AND G5.

Method
Ours w/o Lid
Ours

Distpos
0.8834
0.8807

Distneg
1.4146
1.4147

TABLE V
PERFORMANCE COMPARISONS OF AVERAGE PNSR, LPIPS AND
(cid:52)SDD-FIQA OF DIFFERENT NETWORK ARCHITECTURES ON G1, G2, G3,
G4 AND G5.

Method

PSNR LPIPS (cid:52)SDD-FIQA

Ours w/o AFD and Skip Connection 27.9070 0.2665
Ours w/ AFD, w/o Skip Connection 27.9842 0.2642
30.9470 0.1293

Ours

16.2935
15.1565
9.1811

which one can observe that 1) compared with the anchors
under the similar bit-rate level, JND-MV-NR and JND-MV-FR
degrade the accuracy performance of the FR system; 2) our
proposed coding framework achieves bit-rate savings while
improving the accuracy compared with QP35 of G1, QP36 of
G2, G4 and G5, which means our proposed coding framework
is able to achieve a better balance between the accuracy of
the FR system and bit-rate saving than constant QP coding;
3) compared with QP37 and QP42, where the former is the
point at which the performance of the FR system begins to
decrease signiÔ¨Åcantly while the latter is the one with the
largest distribution of JND points for all original images in the
proposed database, the proposed coding framework achieves
higher accuracy, which means our proposed coding framework
can assign a more reasonable QP for each image. Those results
provide useful evidence that the proposed model has great
potential in image coding for the FR system.

D. Ablation Studies

1) Optimization Strategy Analysis: To investigate the con-
tribution of the self-supervised contrastive learning strategy,
we perform the ablation study based on different optimiza-
tion strategies. Considering that the crucial role of the self-

supervised contrastive learning strategy is to optimize the
learned feature space, we compare the distances between the
identity features of inter-class and intra-class pairs. The results
are listed in Table IV, where Distpos represents the average
(cid:96)1 distance of identity features between all predicted JND
images and their positive samples, and Distneg represents the
average (cid:96)1 distance of identity features between all predicted
JND images and their negative samples. The lower Distpos
and higher Distneg values indicate that the predicted JND
images contain identity information with higher robustness
and higher discriminability, respectively. Ours w/o Lid is the
proposed model without using the contrastive learning strategy,
i.e., the whole framework is only trained with Lpixel and
Lcontent. From the results of Table IV, one can observe
the proposed model using self-supervised contrastive learning
strategy achieves lower Distpos and higher Distneg values.

2) Architecture Analysis: To better understand the individ-
ual contributions of the AFD module and skip connection,
we conduct several comparisons between our proposed model
and its two variants: 1) Ours w/o AFD and Skip Connection:
removing both the AFD modules and skip connections. That
is the feature extracted by each convolutional block (e.g.,
EnConv and Basic Block) of the encoder is directly input to
the next block without feature decomposition, and the decoder
only uses the feature of the last stage of the encoder to
generate the residual map by the decoder. Noted that the Lid
is associated with the AFD module, we also remove the Lid;
2) Ours w/ AFD, w/o Skip Connection: removing the skip
connections. That is an AFD model is embedded between
two convolutional blocks of the encoder, and the input of the
decoder is the residual feature decomposed from the feature of
the last stage of the encoder. The average PNSR, LPIPS and
(cid:52)SDD-FIQA results on G1, G2, G3, G4 and G5 are listed
in Table V, from which one can easily see that compared
with the two variants, our proposed model achieves around
10.89% and 10.59% improvements on PNSR, 51.48% and
51.26%, reductions on LPIPS, 43.65% and 39.42% reductions
on (cid:52)SDD-FIQA. Therefore, our proposed model using the
AFD module and skip connection allows multi-level residual
features learned from the encoder to be embedded in the
it can improve the accuracy of the JND
decoder so that

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

prediction.

VI. CONCLUSION

We have made several efforts in the JND research of FR sys-
tems: 1) establish a JND dataset; 2) propose a JND prediction
model; 3) incorporate the model into the remote FR system.
Regarding the JND dataset, 3530 original images and 137,670
compressed images generated by VTM-15.0 are available for
research usage. Regarding the JND prediction model, the main
novelty is that our model is able to preserve the identity
information and remove the redundancy simultaneously in a
self-supervised manner. Extensive experiments conducted over
our proposed JND prediction model and the state-of-the-art
JND prediction models have clearly demonstrated that the
predicted JND images by our proposed model are much closer
to the ground-truth JND images in terms of low-level and
high-level features. In addition, we also show that the image
coding framework combined with our proposed model can
gain performance improvement. Namely, the image coding
algorithm guided by our proposed model is able to assign
more appropriate QP values to face images, thereby achieving
bit-rate savings while maintaining similar accuracy to the FR
system.

REFERENCES

[1] Y. Sun, Y. Chen, X. Wang, and X. Tang, ‚ÄúDeep learning face repre-
sentation by joint identiÔ¨Åcation-veriÔ¨Åcation,‚Äù Adv. Neural Inf. Process.
Syst., vol. 27, pp. 1988‚Äì1996, 2014.

[2] F. Schroff, D. Kalenichenko, and J. Philbin, ‚ÄúFacenet: A uniÔ¨Åed em-
bedding for face recognition and clustering,‚Äù IEEE Conf. Comput. Vis.
Pattern Recog., pp. 815‚Äì823, 2015.

[3] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, ‚ÄúDeepface: Closing
the gap to human-level performance in face veriÔ¨Åcation,‚Äù IEEE Conf.
Comput. Vis. Pattern Recog., pp. 1701‚Äì1708, 2014.

[4] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu,
‚ÄúCosface: Large margin cosine loss for deep face recognition,‚Äù IEEE
Conf. Comput. Vis. Pattern Recog., pp. 5265‚Äì5274, 2018.

[5] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, ‚ÄúArcface: Additive angular
margin loss for deep face recognition,‚Äù IEEE Conf. Comput. Vis. Pattern
Recog., pp. 4690‚Äì4699, 2019.

[6] Y. Huang, Y. Wang, Y. Tai, X. Liu, P. Shen, S. Li, J. Li, and F. Huang,
‚ÄúCurricularface: Adaptive curriculum learning loss for deep face recog-
nition,‚Äù IEEE Conf. Comput. Vis. Pattern Recog., pp. 5901‚Äì5910, 2020.
[7] Y. Zhong, W. Deng, J. Hu, D. Zhao, X. Li, and D. Wen, ‚ÄúSface: Sigmoid-
constrained hypersphere loss for robust face recognition,‚Äù IEEE Trans.
Image Process., vol. 30, pp. 2587‚Äì2598, 2021.

[8] S. Wang, S. Wang, W. Yang, X. Zhang, S. Wang, S. Ma, and W. Gao,
‚ÄúTowards analysis-friendly face representation with scalable feature and
texture compression,‚Äù IEEE Trans. Multimedia, pp. 1‚Äì1, 2021.

[9] X. Zhang, S. Wang, K. Gu, W. Lin, S. Ma, and W. Gao, ‚ÄúJust-noticeable
difference-based perceptual optimization for jpeg compression,‚Äù IEEE
Signal Process. Lett., vol. 24, no. 1, pp. 96‚Äì100, 2017.

[10] T. Tian, H. Wang, L. Zuo, C.-C. J. Kuo, and S. Kwong, ‚ÄúJust noticeable
difference level prediction for perceptual image compression,‚Äù IEEE
Trans. Broadcast., vol. 66, no. 3, pp. 690‚Äì700, 2020.

[11] H. Liu, Y. Zhang, H. Zhang, C. Fan, S. Kwong, C.-C. J. Kuo, and X. Fan,
‚ÄúDeep learning-based picture-wise just noticeable distortion prediction
model for image compression,‚Äù IEEE Trans. Image Process., vol. 29,
pp. 641‚Äì656, 2020.

[12] M. Zhou, X. Wei, S. Kwong, W. Jia, and B. Fang, ‚ÄúJust noticeable
distortion-based perceptual rate control in hevc,‚Äù IEEE Trans. Image
Process., vol. 29, pp. 7603‚Äì7614, 2020.

[13] X. Shen, Z. Ni, W. Yang, X. Zhang, S. Wang, and S. Kwong, ‚ÄúJust
noticeable distortion proÔ¨Åle inference: A patch-level structural visibility
learning approach,‚Äù IEEE Trans. Image Process., vol. 30, pp. 26‚Äì38,
2021.

[14] X. Yang, W. Lin, Z. Lu, E. Ong, and S. Yao, ‚ÄúMotion-compensated
residue preprocessing in video coding based on just-noticeable-distortion
proÔ¨Åle,‚Äù IEEE Trans. Circuits Syst. Video Technol., vol. 15, no. 6, pp.
742‚Äì752, 2005.

[15] S. Wang, L. Ma, Y. Fang, W. Lin, S. Ma, and W. Gao, ‚ÄúJust noticeable
difference estimation for screen content images,‚Äù IEEE Trans. Image
Process., vol. 25, no. 8, pp. 3838‚Äì3851, 2016.

[16] A. Liu, W. Lin, M. Paul, C. Deng, and F. Zhang, ‚ÄúJust noticeable
difference for images with decomposition model for separating edge
and textured regions,‚Äù IEEE Trans. Circuits Syst. Video Technol., vol. 20,
no. 11, pp. 1648‚Äì1652, 2010.

[17] J. Wu, G. Shi, W. Lin, A. Liu, and F. Qi, ‚ÄúJust noticeable difference esti-
mation for images with free-energy principle,‚Äù IEEE Trans. Multimedia,
vol. 15, no. 7, pp. 1705‚Äì1710, 2013.

[18] J. Wu, L. Li, W. Dong, G. Shi, W. Lin, and C.-C. J. Kuo, ‚ÄúEnhanced
just noticeable difference model for images with pattern complexity,‚Äù
IEEE Trans. Image Process., vol. 26, no. 6, pp. 2682‚Äì2693, 2017.
[19] S.-H. Bae and M. Kim, ‚ÄúA novel generalized dct-based jnd proÔ¨Åle based
on an elaborate cm-jnd model for variable block-sized transforms in
monochrome images,‚Äù IEEE Trans. Image Process., vol. 23, no. 8, pp.
3227‚Äì3240, 2014.

[20] Q. Jiang, Z. Liu, S. Wang, F. Shao, and W. Lin, ‚ÄúTowards top-down just
noticeable difference estimation of natural images,‚Äù IEEE Trans. Image
Process., 2022.

[21] Q. Zhang, S. Wang, X. Zhang, S. Ma, and W. Gao, ‚ÄúJust recognizable
distortion for machine vision oriented image and video coding,‚Äù Int. J.
Comput. Vis., vol. 129, p. 2889‚Äì2906, 2021.

[22] J. Jin, X. Zhang, X. Fu, H. Zhang, W. Lin, J. Lou, and Y. Zhao, ‚ÄúJust
noticeable difference for deep machine vision,‚Äù IEEE Trans. Circuits
Syst. Video Technol., vol. 32, no. 6, pp. 3452‚Äì3461, 2022.

[23] C. Liu and H. Wechsler, ‚ÄúGabor feature based classiÔ¨Åcation using the
enhanced Ô¨Åsher linear discriminant model for face recognition,‚Äù IEEE
Trans. Image Process., vol. 11, no. 4, pp. 467‚Äì476, 2002.

[24] T. I. Dhamecha, P. Sharma, R. Singh, and M. Vatsa, ‚ÄúOn effectiveness
of histogram of oriented gradient features for visible to near infrared
face matching,‚Äù Int. Conf. Pattern Recog., pp. 1788‚Äì1793, 2014.
[25] T. Ahonen, A. Hadid, and M. Pietikainen, ‚ÄúFace description with local
binary patterns: Application to face recognition,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell., vol. 28, no. 12, pp. 2037‚Äì2041, 2006.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù Adv. Neural Inf. Process.
Syst., vol. 25, pp. 1097‚Äì1105, 2012.

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with convolutions,‚Äù
IEEE Conf. Comput. Vis. Pattern Recog., pp. 1‚Äì9, 2015.

[28] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, ‚ÄúA discriminative feature learning
approach for deep face recognition,‚Äù Eur. Conf. Comput. Vis., pp. 499‚Äì
515, 2016.

[29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ‚ÄúGradient-based learning
applied to document recognition,‚Äù IEEE, vol. 86, no. 11, pp. 2278‚Äì2324,
1998.

[30] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, ‚ÄúSphereface: Deep
hypersphere embedding for face recognition,‚Äù IEEE Conf. Comput. Vis.
Pattern Recog., July 2017.

[31] W. Liu, Y. Wen, B. Raj, R. Singh, and A. Weller, ‚ÄúSphereface revived:
Unifying hyperspherical face recognition,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell., 2022.

[32] Weber‚Äôs Law of Just Noticeable Difference, 2017. [Online]. Available:

http://apps.usd.edu/coglab/WebersLaw.html

[33] C.-H. Chou and Y.-C. Li, ‚ÄúA perceptually tuned subband image coder
based on the measure of just-noticeable-distortion proÔ¨Åle,‚Äù IEEE Trans.
Circuits Syst. Video Technol., vol. 5, no. 6, pp. 467‚Äì476, 1995.
[34] X. Yang, W. Ling, Z. Lu, E. Ong, and S. Yao, ‚ÄúJust noticeable distortion
model and its applications in video coding,‚Äù Signal Process. Image
Commun., vol. 20, no. 7, pp. 662‚Äì680, 2005.

[35] Z. Wei and K. N. Ngan, ‚ÄúSpatio-temporal just noticeable distortion
proÔ¨Åle for grey scale image/video in dct domain,‚Äù IEEE Trans. Circuits
Syst. Video Technol., vol. 19, no. 3, pp. 337‚Äì346, 2009.

[36] L. Ma, K. N. Ngan, F. Zhang, and S. Li, ‚ÄúAdaptive block-size trans-
form based just-noticeable difference model for images/videos,‚Äù Signal
Process. Image Commun., vol. 26, no. 3, pp. 162‚Äì174, 2011.

[37] I. Kemelmacher-Shlizerman, S. Seitz, D. Miller, and E. Brossard, ‚ÄúThe
megaface benchmark: 1 million faces for recognition at scale,‚Äù IEEE
Conf. Comput. Vis. Pattern Recog., pp. 4873‚Äì4882, 2016.

[38] Versatile Video Coding, Standard Rec. ITU-T H.266 and ISO/IEC

23090-3, 2020.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

[39] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù Int. J. Comput.
Vis., vol. 88, no. 2, pp. 303‚Äì338, 2010.

[40] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in
context,‚Äù Eur. Conf. Comput. Vis., pp. 740‚Äì755, 2014.

[41] High EfÔ¨Åciency Video Coding, Standard Rec. ITU-T H.265 and ISO/IEC

23008-2, 2014.

[42] F.-Z. Ou, X. Chen, R. Zhang, Y. Huang, S. Li, J. Li, Y. Li, L. Cao,
and Y.-G. Wang, ‚ÄúSsd-Ô¨Åqa: Unsupervised face image quality assessment
with similarity distribution distance,‚Äù IEEE Conf. Comput. Vis. Pattern
Recog., pp. 7670‚Äì7679, 2021.

[43] W. Lin and G. Ghinea, ‚ÄúProgress and opportunities in modelling just-
noticeable difference (jnd) for multimedia,‚Äù IEEE Trans. Multimedia,
2021.

[44] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image

recognition,‚Äù IEEE Conf. Comput. Vis. Pattern Recog., 2016.

[45] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep
network training by reducing internal covariate shift,‚Äù Int. Conf. Learn.
Represent, vol. 37, pp. 448‚Äì456, 2015.

[46] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, ‚ÄúRecurrent models

of visual attention,‚Äù Neural Inf. Process. Syst., pp. 2204‚Äì2212, 2014.

[47] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, ‚ÄúCbam: Convolutional block

attention module,‚Äù Eur. Conf. Comput. Vis., pp. 3‚Äì19, 2018.

[48] Z. Ni, W. Yang, S. Wang, L. Ma, and S. Kwong, ‚ÄúTowards unsupervised
deep image enhancement with generative adversarial network,‚Äù IEEE
Trans. Image Process., vol. 29, pp. 9140‚Äì9151, 2020.

[49] H. Wang, D. Gong, Z. Li, and W. Liu, ‚ÄúDecorrelated adversarial learning
for age-invariant face recognition,‚Äù IEEE Conf. Comput. Vis. Pattern
Recog., June 2019.

[50] Z. Huang, J. Zhang, and H. Shan, ‚ÄúWhen age-invariant face recognition
meets face age synthesis: A multi-task learning framework,‚Äù IEEE Conf.
Comput. Vis. Pattern Recog., pp. 7282‚Äì7291, June 2021.

[51] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman,
A. Trischler, and Y. Bengio, ‚ÄúLearning deep representations by mutual
information estimation and maximization,‚Äù Int. Conf. Learn. Represent.,
2018.

[52] E. H. Sanchez, M. Serrurier, and M. Ortner, ‚ÄúLearning disentangled
representations via mutual information estimation,‚Äù Eur. Conf. Comput.
Vis., 2019.

[53] A. v. d. Oord, Y. Li, and O. Vinyals, ‚ÄúRepresentation learning with
contrastive predictive coding,‚Äù arXiv preprint arXiv:1807.03748, 2019.
[54] J. Justin, A. Alexandre, and F.-F. Li, ‚ÄúPerceptual losses for real-time
style transfer and super-resolution,‚Äù Eur. Conf. Comput. Vis., pp. 694‚Äì
711, 2016.

[55] Z. Ni, W. Yang, S. Wang, L. Ma, and S. Kwong, ‚ÄúUnpaired image
enhancement with quality-attention generative adversarial network,‚Äù in
ACM Int. Conf. Multimedia, 2020, pp. 1697‚Äì1705.

[56] X. Zhang, W. Lin, and P. Xue, ‚ÄúImproved estimation for just-noticeable

visual distortion,‚Äù Signal Process., vol. 85, no. 4, pp. 795‚Äì808, 2005.

[57] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, and Z. L. et al., ‚ÄúPytorch: An imperative style, high-
performance deep learning library,‚Äù Adv. Neural Inform. Process. Syst.,
vol. 32, pp. 8026‚Äì8037, 2019.

[58] D. P. Kingma and J. Ba, ‚ÄúAdam: a method for stochastic optimization,‚Äù

Int. Conf. Learn. Represent., 2015.

[59] P. Terh¬®orst, J. N. Kolf, N. Damer, F. Kirchbuchner, and A. Kuijper, ‚ÄúSer-
Ô¨Åq: Unsupervised estimation of face image quality based on stochastic
embedding robustness,‚Äù IEEE Conf. Comput. Vis. Pattern Recog., pp.
5651‚Äì5660, 2020.

[60] J. Hernandez-Ortega, J. Galbally, J. Fierrez, and L. Beslay, ‚ÄúBiometric
quality: review and application to face recognition with faceqnet,‚Äù arXiv
preprint arXiv:2006.03298v2, 2020.

