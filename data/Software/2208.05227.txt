2
2
0
2

g
u
A
0
1

]
E
S
.
s
c
[

1
v
7
2
2
5
0
.
8
0
2
2
:
v
i
X
r
a

Multi-View Pre-Trained Model for Code
Vulnerability Identiﬁcation

Xuxiang Jiang1, Yinhao Xiao2, Jun Wang1, and Wei Zhang1((cid:66))

1 School of Computer Science and Technology, East China Normal University
zhangwei.thu2011@gmail.com
2 School of Information Science, Guangdong University of Finance & Economics

Abstract. Vulnerability identiﬁcation is crucial for cyber security in
the software-related industry. Early identiﬁcation methods require sig-
niﬁcant manual eﬀorts in crafting features or annotating vulnerable code.
Although the recent pre-trained models alleviate this issue, they over-
look the multiple rich structural information contained in the code it-
self. In this paper, we propose a novel Multi-View Pre-Trained Model
(MV-PTM) that encodes both sequential and multi-type structural in-
formation of the source code and uses contrastive learning to enhance
code representations. The experiments conducted on two public datasets
demonstrate the superiority of MV-PTM. In particular, MV-PTM im-
proves GraphCodeBERT by 3.36% on average in terms of F1 score.

Keywords: Pre-trained model · Vulnerability Identiﬁcation · Contrastive
Learning.

1

Introduction

Code vulnerabilities are a major threat to the software-related industry. It is
reported that the number of vulnerabilities has grown from 4,600 in 2010 to
175,477 by 2022 1. The number of vulnerabilities is still rapidly increasing.

Accordingly, the ﬁeld of vulnerability identiﬁcation is under intensive explo-
ration in academia. In early-stage research, vulnerability identiﬁcation methods
can be categorized into three types: static analysis [22,1], dynamic analysis [10],
and machine learning methods [5,18] based on hand-crafted features. Yet, a
drawback of these methods restrains their performance. That is, they require
vulnerability-related expertise and signiﬁcant manual eﬀorts, yielding them hard
to be deployed and poorly scalable [24].

In later-stage research, researchers applied deep learning to address the afore-
mentioned drawback existing in early-stage research [13]. Some studies [12,24]
leveraged several state-of-the-art deep learning techniques, e.g., LSTM and GGRN.
A common feature of these methods is that they require large amounts of labeled
data to perform supervised training and achieve better performance than conven-
tional methods. Unfortunately, there is currently a lack of data annotated with

1 http://cve.mitre.org/

 
 
 
 
 
 
2

Xuxiang Jiang et al.

vulnerability categories, and manually annotating data is labor-intensive. This
hinders the further development of these methods in vulnerability identiﬁcation.
The emergence of pre-training techniques alleviates the aforementioned prob-
lem. Thanks to the advancement, some pre-trained models for source code have
been proposed, such as CodeBERT [3] and CodeT5 [21]. However, a signiﬁcant
disadvantage of these methods is that they ignores rich structural information
such as abstract syntax and control ﬂow. As such, a natural research question
arises: how to combine multiple structural information with pre-trained models
for vulnerability identiﬁcation.

To tackle this question, we propose a novel Multi-View Pre-Trained Model
(MV-PTM). Based on the pre-trained model, MV-PTM encodes both sequential
information and multi-type structural information of the source code in a uni-
ﬁed framework. Speciﬁcally, it generates representations of code under diﬀerent
structural information constraints. We term these representations as multiple
views of source code. In this work, we use analysis tools to extract the Abstract
Syntax Tree (AST), Control Flow Graph (CFG), and Data Flow Graph (DFG)
of the source code and represent them as adjacency matrices that are taken as
input by Structural-Aware Self-Attention Encoder to produce views containing
diﬀerent semantics. MV-PTM makes vulnerability predictions based on these
views. In addition, MV-PTM uses contrastive learning [15] method for represen-
tation enhancement of structural information.
Our contributions are listed as follows:

– We propose a novel approach based on the pre-trained model that learns
diﬀerent structural information of the source code in a uniﬁed framework,
which endows our model with the capability to represent the semantics of
code more accurately.

– We perform contrastive learning based on multiple views of code to improve
the performance of code representation learning, which is demonstrated to
be better at characterizing code in the experiment.

– MV-PTM outperforms the state of the arts signiﬁcantly with an average of

3.85% higher Accuracy and 6.80% F-1 Score.

2 Methodology

Overview. Fig. 1 shows the overview of MV-PTM. First, we use Tree-sitter 2
to parse the source codes and get the code structural graphs We then convert
these graphs into adjacency matrices to guide the generation of multi-view code
representations based on the Structural-Aware Self-Attention Encoder and pre-
trained model. Afterward, the multi-view representations are fed to a Pooling
Layer and Multi-layer Perceptron (MLP) for identiﬁcation.

2 https://github.com/tree-sitter/tree-sitter-c

Multi-View Pre-Trained Model for Code Vulnerability Identiﬁcation

3

Fig. 1: The architecture of MV-PTM. In the adjacency matrix, 1 denotes there
is an edge between corresponding nodes of code, and 0 otherwise. The layer of
pre-trained model is used to obtain the source code embedding.

2.1 Structural Information

As aforementioned, we ﬁrst obtain diﬀerent structural graphs: CFG, AST, and
DFG. Each node in these graphs represents a program statement, and each edge
represents certain structural information. We use an adjacency matrix M n×n
to represent a certain type of edges in the graph (n is the number of tokens).
We set Mi,j = 1 if the i-th node and the j-th node are connected in the graph;
Otherwise, Mi,j = 0.

CFG is a graphical representation of the paths that are traversed during
the execution of a program. For example, as shown in Fig. 2 (b), when the
program executes the “if (a > 3)” statement, it decides whether “b = a - b” is
executed according to the variable “a”. AST is a tree-structured representation
of the syntax structure of the source codes. Each node on the tree represents a
syntactic structure. We use the subtrees in AST to analyze each statement in
the program. Speciﬁcally, the tokens in the same statement can be connected to
each other. DFG tracks the use of variables during program execution, including
access or modiﬁcation of variables. Take Fig. 2 (d) for instance, the variable “a”
in “b = a - b” comes from “a > 3’.

2.2 Structure-Aware Self-Attention Encoder

We utilize the pre-trained CodeBERT as the backbone in our approach to gen-
erate contextualized token representations, but our approach is ﬂexible to other
pre-trained models. Taking the source code xi as an example, the representations
Zi are obtained by:

Zi = CodeBERT(xi) .

(1)

4

Xuxiang Jiang et al.

(a)

(b)

(c)

(d)

Fig. 2: An example of diﬀerent structural adjacency matrices.

On top of the backbone, we further design Structure-Aware Self-Attention
Encoder (SASA) based on the self-attention mechanism proposed by [20]. SASA
combines the structural information matrix with the scaled dot-product atten-
tion using addition operations, given by:

Attn(Q, K, V, M ) = softmax(

QK T
√
dk

+ M )V ,

(2)

where Q, K and V denote the query, key, and value matrix, respectively, and
are set by Zi. dk is the dimension of K and softmax is Normalized Exponential
Function. M is the adjacency matrix generated according to the speciﬁc struc-
ture information and it can constrain what the i-th token can attend to when
computing attention values. Under the constraints of adjacency matrices, SASA
can generate multiple views containing diﬀerent structural information.

Under our observation, we notice that there are some similar dependencies
between diﬀerent structural information. Inspired by this, we make the node
representation learning on diﬀerent views share the same self-attention head.
Besides, we also have the view-speciﬁc self-attention head. To fuse the represen-
tations learned from shared heads and view-speciﬁc heads, we use linear mapping

Multi-View Pre-Trained Model for Code Vulnerability Identiﬁcation

5

to project them into the same space. As a whole, the SASA attention for one
structural adjacency matrix (one view) is calculated as follows:

SASA(Q, K, V, M ) = Cat(H1, H2)W o ,

(3)

where H1 and H2 correspond to the representation learned from the shared
self-attention head and view-speciﬁc self-attention head, as shown in Eq. 2. Cat
means the concatenation operation.

2.3 Multi-view Contrastive Learning

To enhance the representations learned from diﬀerent structural information, we
regard each type of information as one view and perform Contrastive Learning.
This is motivated by the fact that diﬀerent views of the same piece of code have
some correlations and tend to cluster together in the semantic space.

To realize contrastive learning, we consider diﬀerent views of the same code
as positive pairs and those of diﬀerent codes as negative pairs. The loss function
w.r.t. contrastive learning is expressed as:

LCON T RA = ψast + ψdf g + ψcf g ,

(4)

where ψast takes AST as the Matched Structural view, and it is analogous to
ψdf g and ψcf g. ψ is Normalized Temperature Scaled Cross Entropy Loss [2].

Fig. 3: Diamonds, triangles, pentagons, and squares correspond to code sequence,
DFG, CFG, and AST, respectively. The circle denotes the semantic space.

2.4 Training Loss

We leverage Cross Entropy for training the main task, i.e., vulnerability identi-
ﬁcation, and the total loss for ﬁne-tuning MV-PTM is given by:

L = LCLS + λLCON T RA ,

(5)

6

Xuxiang Jiang et al.

where λ is the hyper parameter and we set λ = 1 in the experiments.

3 Experiments

3.1 Experimental Setup

Datasets. We evaluate our approach on two C-language datasets used in pre-
vious studies [24,21], which contain manually-labeled functions collected from
open-source projects FFmpeg and QEMU.

Since some code snippets in the dataset exceed the length limit of CodeBERT,
we discarded the codes that have more than 512 tokens. For long code segments,
the recognition accuracy of MV-PTM is not ideal.

Table 1: Statistics of the datasets.
FFmpeg
3958
462
499
4919
274.5

QEMU
10903
1378
1319
13600
325.3

Training Set
Validation Set
Test Set
Total
Average Length

Baselines. We choose the following six methods as the baselines since they
represent the most up-to-date vulnerability identiﬁcation mechanisms:

VulDeePecker [12]: It turns the source codes into a token sequence. The

initial embeddings of tokens are trained via Word2Vec [14].

CNN [16]: It models the source codes as natural language and applies CNN
to extract features from the code. The embedding initialization is the same as
that of VulDeePecker.

Devign [24]: It represents the source code with the code property graph
(CPG) which integrates all syntax and dependency semantics. Based on the
graph, it uses Gated Graph Recurrent Network[11] for graph-level classiﬁcation.
SELFATT [20]: Similar to [12], it takes the source code as sequences and

exploits the multi-head attention mechanism for code representation learning.

CodeBERT [3]: It is a pre-trained model for programming language which
has achieved acceptable performance on many code-related tasks such as code
search and code documentation generation.

GraphCodeBERT [6]: It is the ﬁrst pre-trained model that leverages code

structure to learn code representation to improve code understanding.

3.2 Experimental Results

Performance comparison. As shown in Table 2, MV-PTM outperforms all
baseline methods on both two datasets. According to the experimental results,
we summarize the following ﬁndings:

Multi-View Pre-Trained Model for Code Vulnerability Identiﬁcation

7

Table 2: Experimental results on the two datasets.

Methods

FFmpeg

QEMU

Accuracy

F-1 Score

Accuracy

F-1 Score

VulDeePecker[12]
CNN[16]
Devign[24]
SELFATT[20]
CodeBERT[3]
GraphCodeBERT[6]
MV-PTM

0.5622
0.6032
0.5904
0.6152
0.6353
0.6613
0.6874

0.5923
0.6278
0.6015
0.6323
0.6431
0.6724
0.6843

0.5956
0.6482
0.6039
0.6361
0.6907
0.6975
0.7149

0.5644
0.3974
0.3244
0.3701
0.6102
0.6497
0.7049

The local and structural characteristics of the code can improve
the performance of vulnerability identiﬁcation. Comparing CNN with
VulDeePecker, we can ﬁnd that the Accuracy is signiﬁcantly improved in both
two datasets, implying that the local characteristics learned by CNN are indeed
helpful for vulnerability identiﬁcation. GraphCodeBERT outperforms CodeBERT
with an average of 1.64% higher Accuracy and 3.44% F-1 Score.

MV-PTM performs best among all methods. MV-PTM has further
improved its performance based on CodeBERT. It is noteworthy that the F-1
Score of baselines on the QEMU dataset is not ideal, while MV-PTM raised the
F-1 Score to 0.7049.

3.3 Ablation study

In this section, we verify the eﬀectiveness of the three structural information and
contrastive learning methods used in our work according to the results of the
ablation study. Table 3 shows the experimental results.

Table 3: Eﬀect of structural information and contrastive learning.

Methods

FFmpeg

QEMU

Accuracy

F-1 Score

Accuracy

F-1 Score

MV-PTM
MV-PTM w/o CFG
MV-PTM w/o AST
MV-PTM w/o DFG
MV-PTM w/o Contrastive

0.6874
0.6553
0.6573
0.6513
0.6693

0.6843
0.6643
0.6674
0.6683
0.6845

0.7149
0.6983
0.7036
0.6990
0.7005

0.7049
0.6865
0.6845
0.6892
0.6688

Structural information improves the performance of the model. It
can be observed from Table 3 that when any kind of structural information is
removed, the performances of the model on both datasets decrease signiﬁcantly.

8

Xuxiang Jiang et al.

Contrastive Learning can learn a more accurate multi-view repre-
sentation of source code. We ﬁnd that after removing the contrastive learning
module, the performance of the model also decreases to a certain extent. This
phenomenon implies that the contrastive learning method we proposed can make
the model learn diﬀerent code structure information more eﬀectively.

4 Related Work

Vulnerability Identiﬁcation. In academia, there usually are rule-based and
learning-based methods for vulnerability identiﬁcation. Rule-Based methods are
widely explored in academia. SUTURE [23] is a static analysis method, which is
capable of identifying high-order vulnerabilities in OS kernels. Learning-Based
methods are a novel research direction that attracts much attention. VulDeeP-
ecker [12] is an LSTM-based model, which represents the source code as vectors.
Pre-trained Model for Programming Languages. CodeBERT proposed
in [3] is a bimodal model for programming language and natural language trained
by Masked Language Modeling and Replaced Token Detection. GraphCode-
BERT [6] considers the inherent structure of code by Edge Prediction and
Node Alignment to support tasks like code clone detection [17,8,9]. Compared
to CodeBERT, MV-PTM improves the accuracy by an average of 3.81% at the
cost of 15% additional parameters and 25% training time, which is within ac-
ceptable limits.
Contrastive Learning. Contrastive learning is usually conducted in an un-
supervised manner by increasing the similarity between the representations of
positive pairs and decreasing the similarity between the representations of nega-
tive pairs [7,19]. Data augmentation is a commonly-used technique to construct
positive pairs, including rotation, scaling, and cropping in computer vision [2]
and dropout in NLP [4].

5 Conclusion

In this paper, we propose MV-PTM, a pre-trained based model which uses struc-
tural information including AST, DFG, and CFG, to obtain multiple views of the
source code. Besides, we introduce an auxiliary task based on contrastive learn-
ing to improve the performance of code representation. The experiments on two
datasets demonstrate that structural information and contrastive learning are
eﬀective for vulnerability identiﬁcation. In the future, we plan to introduce the
knowledge graph to generate reasonable explanations for the identiﬁed vulnera-
bilities.

Acknowledgment

This study was supported in part by the National Key R&D Program of China
(2019YFB2102600), the National Natural Science Foundation of China (62002067),
and the Guangzhou Youth Talent Program of Science (QT20220101174).

Multi-View Pre-Trained Model for Code Vulnerability Identiﬁcation

9

References

1. Chandramohan, M., Xue, Y., et al.: Bingo: cross-architecture cross-os binary

search. In: SIGSOFT FSE. pp. 678–689 (2016)

2. Chen, T., Kornblith, S., et al.: A simple framework for contrastive learning of visual

representations. In: ICML. pp. 1597–1607 (2020)

3. Feng, Z., Guo, D., et al.: Codebert: A pre-trained model for programming and

natural languages. In: EMNLP. pp. 1536–1547 (2020)

4. Gao, T., Yao, X., Chen, D.: Simcse: Simple contrastive learning of sentence em-

beddings. In: EMNLP. pp. 6894–6910 (2021)

5. Grieco, G., Grinblat, G.L., et al.: Toward large-scale vulnerability discovery using

machine learning. In: CODASPY. pp. 85–96 (2016)

6. Guo, D., Ren, S., et al.: Graphcodebert: Pre-training code representations with

data ﬂow. In: ICLR (2021)

7. He, K., Fan, H., et al.: Momentum contrast for unsupervised visual representation

learning. In: CVPR. pp. 9726–9735 (2020)

8. Huang, C., Zhou, H., et al.: Code clone detection based on event embedding and

event dependency. CoRR (2021)

9. Ji, X., Liu, L., Zhu, J.: Code clone detection with hierarchical attentive graph

embedding. Int. J. Softw. Eng. Knowl. Eng. pp. 837–861 (2021)

10. Li, Y., Chen, B., et al.: Steelix: program-state based binary fuzzing.

In:

ESEC/SIGSOFT FSE. pp. 627–637 (2017)

11. Li, Y., Tarlow, D., et al.: Gated graph sequence neural networks. In: ICLR (2016)
12. Li, Z., Zou, D., et al.: Vuldeepecker: A deep learning-based system for vulnerability

detection. In: NDSS (2018)

13. Lin, G., Wen, S., et al.: Software vulnerability detection using deep neural networks:

A survey. Proc. IEEE pp. 1825–1848 (2020)

14. Mikolov, T., Sutskever, I., et al.: Distributed representations of words and phrases

and their compositionality. In: NeurIPS. pp. 3111–3119 (2013)

15. van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive

predictive coding. CoRR (2018)

16. Russell, R.L., Kim, L.Y., et al.: Automated vulnerability detection in source code

using deep representation learning. In: ICMLA. pp. 757–762 (2018)

17. Sheneamer, A., Roy, S., Kalita, J.: An eﬀective semantic code clone detection
framework using pairwise feature fusion. IEEE Access pp. 84828–84844 (2021)
18. Shin, Y., Meneely, A., et al.: Evaluating complexity, code churn, and developer
activity metrics as indicators of software vulnerabilities. IEEE Trans. Software
Eng. pp. 772–787 (2011)

19. Tang, X., Dong, C., Zhang, W.: Contrastive author-aware text clustering. Pattern

Recognit. 130, 108787 (2022)

20. Vaswani, A., Shazeer, N., et al.: Attention is all you need. In: NeurIPS. pp. 5998–

6008 (2017)

21. Wang, Y., Wang, W., et al.: Codet5: Identiﬁer-aware uniﬁed pre-trained encoder-
decoder models for code understanding and generation. In: EMNLP. pp. 8696–8708
(2021)

22. Xu, Z., Kremenek, T., Zhang, J.: A memory model for static analysis of C programs.

In: ISoLA. pp. 535–548 (2010)

23. Zhang, H., Chen, W., et al.: Statically discovering high-order taint style vulnera-

bilities in OS kernels. In: CCS. pp. 811–824 (2021)

24. Zhou, Y., Liu, S., et al.: Devign: Eﬀective vulnerability identiﬁcation by learning
comprehensive program semantics via graph neural networks. In: NeurIPS. pp.
10197–10207 (2019)

