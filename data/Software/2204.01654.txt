Distributed MPC with ALADIN—A Tutorial

1

Boris Houska, Jiahe Shi

Abstract

2
2
0
2

r
p
A
4

]

C
O
.
h
t
a
m

[

1
v
4
5
6
1
0
.
4
0
2
2
:
v
i
X
r
a

This paper consists of a tutorial on the Augmented Lagrangian based Alternating Direction Inexact Newton
method (ALADIN) and its application to distributed model predictive control (MPC). The focus is—for simplicity
of presentation—on convex quadratic programming (QP) formulations of MPC. It is explained how ALADIN can
be used to synthesize sparse QP solvers for large-scale linear-quadratic optimal control by combining ideas from
augmented Lagrangian methods, sequential quadratic programming, as well as barrier or interior point methods.
The highlight of this tutorial is a real-time ALADIN variant that can be implemented with a few lines of code
yet arriving at a sparse QP solver that can compete with mature open-source and commercial QP solvers in terms
of both run-time as well as numerical accuracy. It is discussed why this observation could have far reaching
consequences on the future of algorithm and software development in the ﬁeld of large-scale optimization and
MPC.

I. INTRODUCTION

The success of MPC [20] in academic and industrial applications [19] relies on high-performance real-time
optimization algorithms. Over the last decades, such real-time optimization algorithms and software have been
developed for small- to medium-scale optimization problems, which can solve these problems online within the
milli- and microsecond range [7], [12]. Numerical inaccuracies or the run-time of such solvers are usually only
a problem if one attempts to implement MPC for a system with a very large number of states or controls, or if
one formulates MPC problems for nonlinear models.

The goal of the present tutorial paper is not only to review the state-of-the-art but also to discuss recent
achievements concerning the development of real-time distributed optimization methods for MPC based on
ALADIN [8], [9]. Here, our focus is, for simplicity of presentation, on linear systems with quadratic stage
cost such that the MPC problem can be formulated as a convex QP. Before being able to understand why this
focus on such a basic MPC setting is of interest—despite the fact that QP solvers for MPC have been developed
over the past 70 years and despite the fact that many generic and tailored QP solvers have reached a high level
of maturity—we need to brieﬂy review existing solution methods for QPs. Namely, there are three big classes of
existing methods: active set methods [22], as implemented in the software packages qpOASES [3], MOSEK [13]
or GUROBI [18], interior point methods [15], as implemented in CVXGEN [12] and OOQP [5], as well as ﬁrst
order methods [4]—nowadays often based on the alternating direction method of multipliers (ADMM) [1]—as,
for example, implemented in the open-source conic solver SCS [17] as well as the sparse QP solver OSQP [21].
Notice that most of the latter references come along with more complete overviews of the history of QP solver
development to which we refer at this point.

In contrast to the above reviewed convex QP solvers, ALADIN has originally been developed for solving
large-scale non-convex optimization problems. For instance, the original ALADIN variant [8] combines ideas
from the ﬁeld of sequential quadratic programming (SQP) [16] and augmented Lagrangian methods [6] in order
to compose a second order method for distributed non-convex optimization. Nevertheless, certain variants of
ALADIN can also be used as ﬁrst order methods for non-differentiable convex optimization problems. In such
a setting global convergence of ALADIN can be established [9].

Highlights

As the present paper consists of a tutorial on how to apply existing variants of ALADIN to develop sparse and
distributed QP solvers for MPC, no new theoretical results are presented. Nevertheless, after a short introduction
to MPC in Section II, the following two new aspects of ALADIN can be considered as highlights of this tutorial.

Boris Houska and Jiahe Shi are with the School of Information Science and Technology, ShanghaiTech University, China; borish,

shijh@shanghaitech.edu.cn.

 
 
 
 
 
 
2

1) Section III introduces a complete and very practical variant of ALADIN for solving sparse and distributed
QPs. This variant is based on a synthesis of active set methods, interior point methods, and ﬁrst order
methods, which leads to a new high-performance sparse QP solver that can be implemented with just a
few lines of code.

2) Section IV reviews ideas from [10] on how to develop real-time distributed variants of the presented sparse
QP solver that are tailored for MPC. An application of this solver to large-scale benchmark MPC problems
illustrates the competitiveness of ALADIN compared to other existing QP solvers.

As much as this tutorial explains how to synthesize a “simple” variant of ALADIN that can compete with
existing sparse QP solvers, our goal is not to develop yet another QP solver software package. On the contrary,
the ultimate goal of this line of research on ALADIN is of a completely different nature: ALADIN has originally
been designed for solving large-scale non-convex optimization and nonlinear MPC problems. At the current
status of research, very early-stage software packages based on ALADIN have appeared [2]. However, the step
from such early-stage implementations to a high-performance large-scale non-convex problem solver will require
much research effort and time investment. Thus, for numerical software developers who wish to work on such
large-scale optimization software, it will be important to assess ﬁrst whether investing time into ALADIN based
solvers has the potential to advance the state-of-the-art. The present tutorial intends to help with the porgress
to come to such an assessment by pointing out that ALADIN works well for solving convex QPs. This can be
interpreted as one possible indicator that further research on the implementation of a non-convex optimization
problem solver could indeed be fruitful. Therefore, Section V will not only summarize the highlights of this
tutorial, but also elaborate on how the numerical observations from this article might impact the future of high-
performance numerical optimization solver development for convex and non-convex programming as well as
distributed MPC.

A. Notation

Let Rn denote the n-dimensional real vector space and

kxk2
Q

def
= x⊺Qx

a weighted Euclidean norm with Q ∈ Rn×n denoting a positive deﬁnite matrix. We occasionally use the notation

Q1 + kx2k2
2 ]⊺ can be a block vector of any dimension and Q1 and Q2 denote positive matrices of the
where x = [x⊺
corresponding dimensions. The symbols 1 and 1 are used to denote, respectively, the unit matrix and a vector
whose components are all equal to one—assuming that it is clear from the context what their dimensions are.

2
kxk
Q1,Q2

1 , x⊺

Q2,

def
= kx1k2

This section reviews linear-quadratic MPC controllers.

II. MODEL PREDICTIVE CONTROL

A. Linear-Quadratic MPC

This paper concerns MPC problems of the form

JN (x0)

def
= min
x,u

N −1

kxkk2

Q + kukk2

R

+ kxN k2
P

k=0
X

(cid:9)

(cid:8)
∀k ∈ {0, 1, . . . , N − 1},
xk+1 = Axk + Buk
c ≤ Cxk + Duk ≤ d .

s.t. 


(1)

Here, xk ∈ Rnx denotes the state and uk ∈ Rnu the control input at time k. The corresponding MPC feedback
law,



µ(x0) = u⋆

0(x0),

3

corresponds to the ﬁrst element, u⋆
0(x0), of the minimizing control input sequence of (1), which depends on the
state measurement x0. Throughout this paper the system matrices A and B as well as the joint state- and control
constraint matrices C and D are assumed to be given. Moreover, for simplicity of presentation, we assume that
the matrices Q, R, and P are positive deﬁnite, although several of the considerations below can be generalized
for positive semi-deﬁnite weights, too. As reviewed in the introduction, there exist many numerical methods for
solving (1). Nevertheless, numerical challenges can arise if we have a system with a very large number of states,
nx ≫ 1, while A and Q (and sometimes also B and R) are sparse matrices. Additionally, in some applications, for
instance, if nu ≪ nx, one needs large prediction horizons N in order to achieve a satisfying control performance.

Remark 1 A historical overview, recent developments, and a more complete discussion on how to write economic
and distributed MPC problems in the form of (1) can be found in the book [20] and the recent overview
article [14].

B. Recursive Feasibility and Stability

For simplicity of presentation, we assume that (A, B) is asymptotically stabilizable such that we can compute

the positive deﬁnite matrix P by solving the Riccati equation

P = A⊺P A + Q − A⊺P B(R + B⊺P B)

−1B⊺P A.

(2)

We additionally assume that c < 0 < d such that all constraints are strictly feasible. Thus, if we choose N
sufﬁciently large, we have JN (x0) = J∞(x0) for all x0 that are in the domain of the inﬁnite horizon cost J∞.
Here, J∞ is a piecewise quadratic and positive deﬁnite function that satisﬁes the stationary Bellman equation
Q + kuk2
kxk2
c ≤ Cx + Du ≤ d .

R + J∞(Ax + Bu)

J∞(x) = minu

s.t.

(3)

Notice that under these assumption the MPC controller is recursively feasible and µ stabilizes the system [20].
However, such stability and recursive feasibility statements only hold if (1) is solved exactly. For real-time MPC
solvers one needs to impose additional requirements on the accuracy of the solver in order to ensure stability;
see Section IV.

C. Tutorial Example

Throughout this paper, we use a chained spring-mass-damper system as a tutorial. The position and velocity

of the i-th wagon, pi and vi, satisfy a recursion of the form
p+
i = pi + hvi
v+
i = vi + h

(pi−1 − 2pi + pi+1) −

ks
m

(cid:20)

kd
m

vi +

ui
m

(cid:21)

for all i ∈ {1, . . . , n}. Here, we formally deﬁne p0 = 0 as well as pn+1 = pn, which means that the ﬁrst wagon
of the chain is attached to a wall while the last wagon is free. Moreover, ui denotes a piecewise constant force
at the i-th wagon. For simplicity of presentation, we set the spring constant, the damping constant and mass to
ks = kd = m = 1. Moreover, we use the Euler discretization parameter h = 0.1. The matrices and vectors

C =

, D =

, and d = −c =

1 0
0 0

0 0
0 1

5 · 1
1

(cid:19)
model simple state- and control constraints. Similarly, we set Q = 1 and R = 1 while P is computed by solving
the above mentioned algebraic Riccati equation.

(cid:18)

(cid:18)

(cid:18)

(cid:19)

(cid:19)

This section explains how ALADIN can be used as a generic sparse QP solver for solving (1).

III. ALADIN

A. Distributed Quadratic Programming

There are several ways to exploit the structure of (1). Similar to the implementation of ADMM in the software
package OSQP [21], we focus in this paper on a generic sparse structure exploitation scheme writing (1) in the
form

F (y) + G(z)

min
y,z
s.t. Ey = z | λ ,

(4)

4

where the auxiliary vector y = [u⊺
λ denotes the dual solution. Here, F is a quadratic function,

1 , . . . , x⊺

1 , u⊺

0 , x⊺

N ]⊺ collects all optimization variables of the MPC problem and

F (y) =

y⊺Qy

def
=

1
2

The sparse matrix E is given by

N −1

k=0
X

(cid:8)

kxkk2

Q + kukk2

R

+ kxN k2

P .

(cid:9)



. . .

A B −1
C D
0

,












B −1
D
0
A B −1
C D
0

E

def
=














where the empty blocks are all equal to 0. Notice that (1) and (4) are equivalent, if we deﬁne the function G as

G(z)

def
=

if z ≤ z ≤ z

0
∞ otherwise

(

)

with

and

z

z

def
= [−(Ax0)⊺, (c − Cx0)⊺, 0⊺, c⊺, . . . , 0⊺, c⊺]⊺

def
= [−(Ax0)⊺, (d − Cx0)⊺, 0⊺, d⊺ . . . , 0⊺, d⊺]⊺ .

(5)

The parametric initial value x0 enters via the vectors z and z while all matrices are constant.

B. Distributed Optimization Algorithm

The main idea of ALADIN is to start with an initial guess (y, z) for the primal solution of (4) as well as an

initial guess λ for its dual solution and repeat the following steps.

1) Choose positive deﬁnite matrices H ≻ 0, Σ ≻ 0, and K ≻ 0 as well as a tuning parameter θ ∈ [0, 1].
2) Solve the decoupled optimization problems

F (v) + λ⊺E⊺v +

min
v

and min

w

G(w) − λ⊺w +

1
2

kv − yk2
Σ

1
2
kw − zk2
K

and denote the minimizers by v and w.

3) Compute the gradient of F and subgradient of G as

∇F (v) = Σ(y − v) − E⊺λ

and ∂G(w) = K(z − w) + λ

at the decoupled minimizers v and w.

(6)

(7)

(8)

(9)

4) Solve the equality constrained consensus QP

min
y+,z+

1
2

y+ − v
z+ − w

2

+

H,K

∇F (v)
∂G(w)

⊺

y+
z+

(cid:19)(cid:13)
(cid:13)
s.t. Ey+ = z+ | λ+ ,
(cid:13)
(cid:13)
denote the primal minimizers by y+ and z+, and denote the dual solution by λ+.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:18)

(cid:19)

(cid:19)

(cid:18)

5

(10)

5) Update the variables y ← y+, z ← z+, as well as λ ← θλ+ + (1 − θ)∂G(y); and go to Step 1.

Notice that, if we set θ = 1, the above algorithm coincides with the derivative-free variant of ALADIN that has
been analyzed in [9]. However, the corresponding global convergence proof can be generalized easily for any
choice of θ ∈ [0, 1]. As such, the above algorithm converges for all convex QPs—even without requiring positive
deﬁniteness of the objective matrices.

Remark 2 The above algorithm is neither equivalent to SQP nor to ADMM, although the introduction of the
decoupled augmented Lagrangian problems in Step 2) is inspired by ADMM, while Step 10) is inspired by SQP.
In fact, if F and G would both be smooth, one could—in complete analogy to SQP methods—set H and K to the
Hessian matrices of F and G in order to obtain a variant of ALADIN that has a locally quadratic convergence
rate [8]. In our context, however, G is a non-smooth function. Thus, the choice of K requires further discussion.

C. Hessian Matrix Updates

In our context, F is a smooth quadratic form. Consequently, we set the Hessian matrix approximation H to

H

def
= Q + ǫ1 · 1 ,

which coincides with the exact Hessian of F apart from a small regularization term that can be adjusted by the
tuning parameter ǫ1 > 0. The reason for introducing this tuning parameter is that, for ǫ1 > 0, it can be shown
that the above outlined variant of ALADIN also converges for LPs, where we have Q = 0, or other types of
degenerate QPs, where Q might be highly ill-conditioned [9].

Next, in order to be able to assign a “Hessian approximation” to the non-smooth function G, one option is to
establish an analogy to traditional interior point methods for convex optimization. In order to elaborate on this
idea, we introduce relaxed log-barrier functions of the form

1
t
with barrier parameter t > 0 and relaxation parameter r ≥ 0, which are deﬁned on the domain ξ < r. An
associated smooth approximation, G(z) ≈ Φ(r, t, z), is then given by

ϕ(r, t, ξ) = −

· log (r − ξ)

Φ(r, t, z)

def
=

( ϕ (r, t, zi − zi) + ϕ (r, t, zi − zi) ) .

This approximation becomes exact for increasing barrier parameters and vanishing relaxation, t → ∞ and r → 0+.
In the following implementation of ALADIN, we use the Hessian matrix approximation K = ∇2
zΦ(r, t, z), which
is motivated by the fact that G ≈ Φ(r, t, ·). An explicit expression for K can be found by using the formula

i
X

∇2

ξ ϕ(r, t, ξ) =

1
t

1

(r − ξ)2 > 0

when evaluating the second order derivative of Φ. Notice that the log-barrier is here merely used for tuning the
matrix K by interpreting this matrix as a Hessian approximation. In contrast to actual interior point methods,
however, the decoupled optimization problem (7) uses the exact function G rather than its relaxed log-barrier
approximation.

6

ALADIN as Sparse QP Solver

• Default Regularization and Initialization:

– Set ǫ1 = 10−6 and ǫ2 = 10−3.
– If the user does not specify any customized initialization, set y = 0 and z = λ = 0.

• Initial Hessian Approximations:

– Set H = Q + ǫ1 · 1, Σ = Q + ǫ2 · 1, and K = 1.

• Main Loop:

For i = 1 : imax do:
1) Set σ ← E⊺λ.
2) Set v ← [Q + Σ]−1(Σy − σ).
3) Set w ← Π(z + K −1λ).
4) If kw − zk∞ ≤ TOL and kQy + σk∞ ≤ TOL, break.
5) Set h ← Σ(y − v) − σ.
6) Set k ← K(z − w) + λ.
7) Set

0 E⊺
H
0 K −1
E −1
0




8) Set y ← y+, z ← z+, and λ ← 3
4 λ+ + 1
9) If log3(i) ∈ N , do the following:

← 

y+
z+
λ+

4 k.








−1











Hv − h
Kw − k
0






a) Attempt to guess the active set based on the current iterate. If successful, perform a single active

set step and return the optimal solution.

b) Set r = 11

10 · kw − zk∞ , set

t =

1
max{kw − zk∞, kQy + σk∞}

,

and update K ← ∇2

zΦ(r, t, z).

End

• Output: Primal and dual solution, y and λ.

Fig. 1. Summary of a version of ALADIN that is tailored for solving sparse QPs. All sparse linear algebra operations can be distributed
by exploiting the sparse block structures of the matrices E and Q.

D. Implementation Details

Notice that almost all steps of the above outlined algorithm involve the solution of equality constrained
convex QPs, which can be solved by using sparse Cholesky factorizations. The only exception is the decoupled
optimization problem (7). Since the above outlined log-barrier based weight matrix generation scheme leads to
a diagonal K, this problem can, however, be solved by a projection onto the box [z, z]. More precisely, if Π
denotes the projection function,

∀i ∈ {1, . . . , nE}, Πi(ξ) = 


zi
ξi
zi

if
if
if

ξi < zi
zi ≤ ξi ≤ zi
zi < ξi ,

the explicit solution for the decoupled variable v is given by v = Π(z +K −1λ). With this, we have all ingredients
to setup a complete sparse QP solution algorithm, as summarized in Figure 1. Notice that the ﬁrst step of this
algorithm computes the matrix-vector product E⊺λ. Steps 2) and 3) set v and w to the explicit optimal solution
of (6) and (7), respectively. As Q + Σ is constant, its decomposition can be pre-computed exploiting the sparse



7

and diagonal block-structure of Q. Moreover, Step 4) evaluates the primal and dual residuums of the current
iterates terminating the loop as soon as a user-speciﬁed termination tolerance TOL > 0 is reached. Next, Steps 5)
and 6) compute the subgradients in (8) and (9) while Step 7) computes the primal and dual solutions of the
equality constrained consensus QP (10). Although the presented ALADIN variant convergences for any value
θ ∈ [0, 1], the value θ = 3

4 , as used by Step 8), has been found to work well on benchmark problems.1

Last but least, since Step 9) is computationally expensive, it is only evaluated whenever the iteration index is an
integer power of 3. In detail, Step 9a) attempts to guess the active set based on the active set of the projection step.
If this yields a solution to the QP, one can directly terminate [21]. Moreover, Step 9b) implements the log-barrier
based Hessian approximation heuristic from the previous section. We use the barrier relaxation r = 11
10 kw−zk∞, a
value that is a bit larger than kw−zk∞, such that the log-barrier is well-deﬁned at the current iterate. Moreover, the
log-barrier parameter t is set to the inverse of the maximum of the primal and dual residuum of the current iterate.
Notice that updating K is expensive in the sense that one needs to update the sparse Cholesky decomposition of
the KKT matrix in Step 7). However, as we will show below, this log-barrier update heuristic leads to signiﬁcant
overall run-time improvements.

Remark 3 Although the above variant of ALADIN has many similarities with ADMM [21], [1], it is not equivalent
to ADMM, not even if we would skip the Hessian updates. It is, however, possible to construct variants of ALADIN
that are equivalent to ADMM [8]. Nevertheless, the unique feature of ALADIN compared to ADMM is that it
offers a natural way of choosing the augmented Lagrangian weights H and K. Namely, we can exploit the
similarity of ALADIN to SQP and interior point methods, which motivates the above log-barrier based scaling
heuristic. As we will see below, the above outlined ALADIN variant performs well for large-scale QPs—even
without using a pre-conditioner.

This section reviews ideas from [10], which can be used to develop a real-time ALADIN solver for distributed
MPC. The performance of the solver from Figure 1 and the performance of its real-time variant are discussed in
Section IV-B.

IV. REAL-TIME VARIANTS

A. Real-Time Parallel MPC

A real-time variant of the ALADIN based QP solver from Figure 1 for the MPC problem (1) can be obtained

by

1) implementing only a ﬁnite number imax of ALADIN iterations per sampling time,
2) sending the approximately optimal input ˆu0 ≈ u⋆
3) skipping Step 9) but still updating K once during every real-time loop using the relaxed log-barrier, and,
4) warm-starting the solver by shifting the solution from the previous time step. Additionally, the initialization

0(x0) to the real process as soon as i = imax,

variables are scaled such that k(y, z, λ)k ≤ γ0kx0kQ for a sufﬁciently large constant γ0 < ∞.

The above real-time ALADIN method has the property [10] that there exist constants γ1 < ∞ and κ < 1 with

kx+

0 − x⋆

1k ≤ γ1κimaxkx0kQ .

(11)

0(x0) denotes the current ALADIN iterate for the ﬁrst control input, u⋆

Here, ˆu0 ≈ u⋆
0(x0) the optimal input,
x+
0 = Ax0 + B ˆu0 the next state of the closed-loop system and x⋆
0 the optimal solution for the
state at the next time instance. Inequality (11) holds because ALADIN converges linearly. Moreover, due to our
warm-start, kx⋆

1kQ scales at most linearly with respect to kx0kQ; that is,

1 = Ax0 + Bu⋆

kx⋆

1kQ ≤ γ2kx0kQ

for a constant γ2 < ∞. Since the inﬁnite horizon cost J∞ is piecewise quadratic, there exist constants γ3, γ4 < ∞
with

J∞(x) − J∞(x′

) ≤ γ3kxkQkx − x′

kQ + γ4kx − x′

k2
Q

1The parameter θ has here been tuned by empirical testing with thousands of randomly generated large-scale QPs. Adjusting this

parameter properly leads to approximately 50% of run-time improvement on average.

8

Fig. 2. Convergence of the ALADIN based sparse QP solver from Figure 1 for the MPC problem (1) with and without log-barrier Hessian
updates. The dotted green line shows the maximum of the primal and dual residuums versus the iteration index i of the main ALADIN
loop for the case that the Hessian matrices are only updated once. In this case, the optimal active set is found after 6561 iterations. The
convergence proﬁle can be compared to the solid blue line, which shows the ALADIN iteration for the case that the Hessian matrix K
is updated by using the log-barrier approach. The “spikes” in the convergence proﬁle around the iterations 27, 81 and 243 are caused by
the Hessian updates, which can eventually lead to a drop of residual accuracy for a couple of iterations before paying out on the long
run. In this example, the ALADIN based algorithm with log-barrier updates detects the optimal active set during iteration 729.

for all x, x′ in the feasible domain of J∞. Next, by starting with the Bellman equation (3) and substituting the
three latter inequalities one ﬁnds that

J∞(x+

0 ) ≤ J∞(x0) − kx0k2

Q + [γ2γ3 + γ1γ4]γ1κimaxkx0k2
Q.

This is a Lyapunov descent condition as long as

imax >

log(γ1[γ2γ3 + γ1γ4])
log(κ−1)

.

Thus, if imax is sufﬁciently large, the above real-time ALADIN variant yields an asymptotically stable feedback
law as long as the iterates do not leave the domain of J∞ [10].

Remark 4 If the iterates of the real-time ALADIN variant leave the domain of J∞, the above estimates are
wrong. As such, the above Lyapunov descent condition only ensures asymptotic stability under the assumption
that recursive feasibility holds. If one is interested in a rigorous guarantee of recursive feasibility of real-time
MPC in the presence of state constraints one needs to use methods from the ﬁeld of rigid robust MPC [11]
in order to pre-compute robustness margins for all state-constraints such that recursive feasibility holds in the
presence of sufﬁciently small numerical errors.

B. Numerical Performance

In order to illustrate numerical performance, we implement the algorithm from Figure 1 for the tutorial case
study from Section II-C. We use n = 50 wagons. This leads to an MPC problem with 100 states and 50 controls.
We additionally set the prediction horizon to N = 100. The corresponding QP is sparse: it has 15000 optimization
variables, 24900 constraints, as well as 124202 total non-zero entries in the QP data matrices and vectors. Figure 2
shows the maximum of the primal and dual residuum versus the iteration index for the current state measurement
x0 = 2 · 1. The log-barrier Hessian updates improve the convergence rate and overall run-time of the algorithm
approximately by a factor 8.

In this case study, we implemented the presented ALADIN method in approximately 200 lines of prototype
Julia code ﬁnding that—for randomly chosen initial values x0—this implementation needs on average 0.9 seconds
to solve the complete QP (without warm starts). On the same computer, OSQP solves the same QP in 0.8 seconds
on average while GUROBI needs more than 1 second on average. The same trend in terms of run-time is conﬁrmed

9

Fig. 3. Relative loss of control performance of closed-loop real-time ALADIN compared to the optimal inﬁnite horizon performance
J ⋆
∞ = J∞(x0) for the particular initial value x0 = 2 · 1 versus the maximum number imax of ALADIN iterations per sampling time.

by running these solvers on randomly generated sparse QPs. We do not elaborate more on this run-time result,
because our goal here is merely to show that a simple implementation of ALADIN can achieve run-times that
have the same order of magnitude as the run-times of existing sparse QP solvers.

∞

Finally, Figure 3 shows the loss of control performance,
J MPC
∞ − J ⋆
∞
J ⋆
∞

with J MPC

∞ =

ℓ

xMPC
k
(cid:16)
where ˜µ denotes the real-time ALADIN based MPC feedback law, xMPC
the associated approximately optimal
k
closed loop trajectory and J ⋆
∞ = J∞(x0) the optimal inﬁnite horizon performance, both for the initial state
x0 = 2 · 1. For imax < 3, the real-time ALADIN iteration happens to lead to an unstable closed-loop feedback
law. However, for instance, for imax = 5 the relative loss of performance is smaller than 0.1%. The run-time of
real-time MPC is in this example less than 10 milliseconds—a run-time improvement of a factor 100 compared
to exact MPC.

xMPC
k

k=0
X

(cid:17)(cid:17)

, ˜µ

(cid:16)

,

V. CONCLUSIONS

This paper has presented a tutorial on how to implement a relatively simple ALADIN variant for solving sparse
large-scale QPs as arising in the context of distributed MPC. It has been explained that this methods combines
ideas from the ﬁeld of sequential quadratic programming, interior point methods and augmented Lagrangian
methods. For instance, in the proposed implementation, a relaxed log-barrier heuristic has been introduced in
order to update certain Hessian matrices, which improves the convergence rate of ALADIN almost by an order
of magnitude.

The numerical results of this paper are relevant for the future of large-scale optimization and MPC solver
development. This is because the presented ALADIN scheme can also be applied to solve more general convex
as well as non-convex optimization problems [8], [2]. The fact that the presented “simple” variant of this method
can directly be used to implement a surprisingly competitive sparse QP solver within just a few lines of code can
be interpreted as a promising indicator that ALADIN has enormous potential to be among the most competitive
algorithms for large scale optimization and MPC. Besides, as pointed out in this tutorial, too, the algorithmic
framework of ALADIN offers a uniﬁed perspective on augmented Lagrangian, SQP, and interior point methods.
This perspective might help to proceed in a systematic way when synthesizing future large-scale non-convex
optimization algorithms and software.

10

REFERENCES

[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction

method of multipliers. Found. & Trends in Machine Learning, 3:1–122, 2011.

[2] A. Engelmann, Y. Jiang, H. Benner, R. Ou, B. Houska, and T. Faulwasser. ALADIN-α—an open-source MATLAB toolbox for

distributed non-convex optimization. Optimal Control Applications & Methods, 43:4–22, 2022.

[3] H.J. Ferreau, C. Kirches, A. Potschka, H.G. Bock, and M. Diehl.

qpoases: a parametric active-set algorithm for quadratic

programming. Mathematical Programming Computation, 6(4):327–363, 2014.

[4] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Res. Log. Q., 3:95–110, 1956.
[5] E.M. Gertz and S.J. Wright. Object-oriented software for quadratic programming. ACM Trans. on Math. Software, 29(1):58–81,

2003.

[6] A. Hamdi and S.K. Mishra. Decomposition methods based on augmented Lagrangian: a survey. In Topics in Nonconvex Optimization.

Mishra, S.K., Chapter 11, pages 175–204, 2011.

[7] B. Houska, H.J. Ferreau, and M. Diehl. An auto-generated real-time iteration algorithm for nonlinear MPC in the microsecond

range. Automatica, 47:2279–2285, 2011.

[8] B. Houska, J. Frasch, and M. Diehl. An augmented Lagrangian based algorithm for distributed non-convex optimization. SIAM

Journal on Optimization, 26(2):1101–1127, 2016.

[9] B. Houska and Y. Jiang. Distributed optimization and control with ALADIN. Recent Advances in Model Predictive Control: Theory,

Algorithms, and Applications, pages 135–163, 2021.

[10] Y. Jiang, J. Oravec, B. Houska, and M. Kvasnica. Parallel MPC for linear systems with input constraints. IEEE Transactions on

Automatic Control, 66(7):3401–3408, 2021.

[11] W. Langson, I. Chryssochoos, S. V. Rakovi´c, and D. Q. Mayne. Robust model predictive control using tubes. Autom., 40(1):125–133,

2004.

[12] J. Mattingley and S. Boyd. CVXGEN: a code generator for embedded convex optimization. Optimization in Engineering, 13(1):1–27,

2012.

[13] MOSEK. The MOSEK optimization toolbox for MATLAB, 2022. (http://www.mosek.com).
[14] M.A. M¨uller and F. Allg¨ower. Economic and distributed model predictive control: Recent developments in optimization-based

control. Journal of Control, Measurement, and System Integration, 10(2):39–52, 2017.

[15] Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Programming. SIAM, Philadelphia, 1994.
[16] J. Nocedal and S.J. Wright. Numerical Optimization. Springer Series in Operations Research and Financial Engineering Springer,

2006.

[17] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting and homogeneous self-dual embedding.

Journal of Optimization Theory and Applications, 169(3):1042–1068, 2016.

[18] Gurobi Optimization. Gurobi optimizer reference manual, 2022. (http://www.gurobi.com).
[19] S.J. Qin and T.A. Badgwell. A survey of industrial model predictive control technology. Con. Eng. Practice, 93(316):733–764,

2003.

[20] J.B. Rawlings, D.Q. Mayne, and M.M. Diehl. Model predictive control: Theory and design. Nob Hill Publishing, 2017.
[21] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd. OSQP: an operator splitting solver for quadratic programs.

Mathematical Programming Computation, 12:637–672, 2020.

[22] P. Wolfe. The simplex method for quadratic programming. Econometrica, 27(3):382–398, 1959.

