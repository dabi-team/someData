Model-Free Deep Reinforcement Learning in
Software-Deﬁned Networks

Luke Borchjes∗,Clement Nyirenda†,Louise Leenen‡
Computer Science Department, University of the Western Cape
1ldborchjes@gmail.com, 3647745@myuwc.ac.za
2cnyirenda@uwc.ac.za
3lleenen@uwc.ac.za
South Africa

2
2
0
2

p
e
S
3

]
I

A
.
s
c
[

1
v
0
9
4
1
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—This paper compares two deep reinforcement learn-
ing approaches for cyber security in software deﬁned network-
ing. Neural Episodic Control to Deep Q-Network has been
implemented and compared with that of Double Deep Q-
Networks. The two algorithms are implemented in a format
similar to that of a zero-sum game. A two-tailed T-test analysis
is done on the two game results containing the amount of turns
taken for the defender to win. Another comparison is done
on the game scores of the agents in the respective games. The
analysis is done to determine which algorithm is the best in game
performer and whether there is a signiﬁcant difference between
them, demonstrating if one would have greater preference over
the other. It was found that there is no signiﬁcant statistical
difference between the two approaches.

Index Terms—Software deﬁned networking, deep reinforce-

ment learning, cyber security

I. INTRODUCTION

Most cyber security approaches are model-based and lack
scalability because they are sample inefﬁcient [1]. Literature
has also shown that there is a lack of networking models
featuring the next generation of networks, such as Software
Deﬁned Networks (SDNs) [2], [1]. Interest in using deep
reinforcement learning (DRL) has grown signiﬁcantly due
to its versatility; One example for DRL approaches is the
Double Deep Q-Learning algorithm [3]. Double Deep Q-
Networks (DDQN) emanates from the combination of Double
Q-Learning and Deep Q-Networks [3]. This algorithm is
aimed at reducing the number of over estimations by splitting
the maximum operation of the target function into two Q-
value functions, action selection and action evaluation. The
target network of the Deep Q-Network (DQN) architecture
replaces the action evaluation function, giving the resulting
DDQN algorithm.

A recent modiﬁcation to Deep Q-Networks has introduced
Neural Episodic Control (NEC) into the algorithm [4]. Neural
Episodic Control (NEC) to Deep Q-Network has an imple-
mentation of similar structure to DDQN, although instead
of double Q-Learning with a DQN it makes use of neural
episodic control and a DQN [3] Neural episodic control
replaces the evaluation network. NEC has greater performance
scores in earlier iterations, whereas DQN outperforms it in
latter iterations [5]. It is this combination that gives it the
shorthand names NEC2DQN and N2D [5]. This paper is
aimed at ﬁnding out whether these algorithms are capable of

managing a network and isolating machines that are infected
and prevent further propagation throughout the network, pre-
venting the intruder from gaining access to the critical server
within the context of software deﬁned networks (SDN).

The rest of the paper is organised as follows. Section I
presents the fundamentals of software deﬁned networking
(SDN), including the deﬁnition, motivation as well as some
literature on SDN. Section II presents the concept of deep
reinforcement learning, focusing on the chosen algorithms,
their deﬁnition, motivation as well as the associated literature.
Section III presents the methodology, which includes the
implementation of the environment as well as agent use of the
algorithms. The results are presented and analysed in Section
IV. The discussion and future work are presented in Sections
V and VI respectively.

II. BRIEF OVERVIEW ON SOFTWARE DEFINED
NETWORKING

Software Deﬁned Networking (SDN) is a three layer net-
work architecture that has been developed and put in practice
as early as of 2013. SDN is composed of the three layers:
(1) application layer made up of applications, delivering
services and communicating their network requirements to
the controller by means of northbound APIs; (2) Control
layer, hosting the SDN controller, translating requirements
into low-level controls sent
through southbound API’s to
the infrastructure layer; (3) infrastructure layer, consisting of
network switches [2]. The major advantage of SDN is that it
separates network control and forwarding functions, allowing
the controller to be programmable to perform various applica-
tion services and tasks [2]. Consequently, network resources
can be conveniently managed, conﬁgured and optimised using
the standardised protocols. Due to its architecture there has
been a good variety of available open-source SDN controller
platforms/frameworks, a few examples being OpenDayLight,
RYU, NOX/POX and Open vSwitch [2].

In this paper, RYU was chosen as it is recommended for
quick prototyping, being consistently updated and well built
with Python. RYU is a component-based software deﬁned
networking framework, providing APIs to create, manage or
control networks [6]. This controller is paired with MiniNet,
a system framework that creates a realistic virtual network,
with a real kernel, switch and application code that can be

 
 
 
 
 
 
run on a single machine either native, through cloud services
or on a virtual machine (VM) [7]. Reinforcement learning
in SDN has been demonstrated as a ﬁtting tool for various
purposes as demonstrated by [2]. A good example is the
implementation of intelligent routing based on reinforcement
learning for software deﬁned networking by [8]. A challenge
SDN faces stems from dynamic trafﬁc patterns, and requires
frequent network reconﬁguration [9]. Consequently, reinforce-
ment learning presents itself as an ideal tool for this task.

III. DEEP REINFORCEMENT LEARNING
FUNDAMENTALS

A. Double Deep Q-Learning

Q-Learning is a reinforcement learning algorithm proposed
to optimally solve Markov Decision Processes (MDPs) [10].
An MDP is deﬁned as (S, A, P, R, GAMMA); S is the
states, A is the set of actions doable in the environment,
P the state transition property, R the reward and ﬁnally
Gamma (γ) is the discount factor [10]. However in stochastic
MDPs Q-Learning performs poorly due to the large over
estimations of the action-values [11]. The algorithm contains
the Q function that calculates the quality of a state-action
combination; Q : S × A → R. The core of the algorithm is a
Bellman equation as a simple value iteration update, using the
weighted average of the old value and the new information
by using

Qnew(st, at) ← Q(st, at)+(rt+maxaQ(st+1, a)−Q(st, at)).
(1)

The Q function works by initialising it to any possibly
arbitrary ﬁxed value. Then at each time t, agent is in state st
and selects an action at, observes a reward rt and enters a
new state st+1, then updates Q. The action selected is linked
to the highest expected value. In doing so, the obvious method
to obtain it, is by approximation of the value by means of the
maximal estimator (maxaQ(st+1, a)) as seen in the temporal
difference (rt +maxaQ(st+1, a)−Q(st, at) in the Q function
in Eq. (1).

The approximation of the value of the next state is done
by maximising over the estimated action values in that state.
To solve the overestimation of action-values the algorithm
Double Q-Learning is proposed. Double Q-Learning is the
implementation of two Q functions: QA and QB. Each Q
function is updated from the other’s next state [11] by using

QA ← QA + (rt + maxaQB − QA),

(2)

where QA = QA(s, a), QB = QB(s(cid:48), a∗), and a∗ =
argmaxaQA(s(cid:48), a).

Algorithm 1 Double Q-Learning (Hasselt et al., 2015)

1: Initialise networks Qθ and Qθ(cid:48)
2: Initialise replay buffer D
3: Initialise τ << 1
4: for each iteration do
5:
6:
7:
8:
9:

for each environment step do

Observe state st and select action at ∼ π(at, st)
Execute at
Observe next state st+1 and reward rt = R(st, at)
Store (st, at, rt, st+1) in replay buffer D

10:
11:
12:
13:
14:
15:

end for
for each update state do

sample et = (st, at, rt, st+1) D
Compute target Q value using (2, 3)
Perform gradient descent step on:
(Q ∗ (st, at) − Qθ(st, at))2
Update target network parameters:
θ(cid:48) ← τ ∗ θ + (1 − τ ) ∗ θ(cid:48)

16:
17:
18:
19: end for

end for

B. Neural Episodic Control to Deep Q-Learning

Neural Episodic Control is an algorithm proposed by Pritzel
et. al. in [5]. NEC is able to execute successful strategies
as soon as they are experienced,
instead of waiting for
optimization to be done, such as stochastic gradient descent
as with Deep Q-Networks in Q-Learning. The algorithm has
three parts, a convolutional neural network for the processing
of images and pictures (used to create a state embedding), a
set of memory modules (one per action) and a ﬁnal network
for converting action readouts into Q(s, a) [5]. The memory
module mentioned is referred to as a differential neural
dictionary (DND) because each memory module contains two
dynamically sized arrays of vectors, Ka and Va. Memory
module Ma exists for each action a ∈ A such that Ma =
(Ka, Va). For array Ka, key h, for h ∈ Ka, is produced
by inputting pixel state s into CNN and is used to lookup
a value from DND Ma yielding the weights w. The key h
is also described as the embedding vector. Array Va holds
the target values v ∈ Va for each action. The DND has two
function operations: lookup and write. When doing a lookup
using key h it returns an output o, the weighted sum of the
values in the DND, deﬁned by
(cid:88)

o =

wivi,

(4)

where wi = k(h,hi)

(cid:80)

j k(h,hi) and

i

k(h, hi) =

1

||h − hi||2

2 + δ

.

(5)

QB ← QB + (rt + maxaQA − QB),

(3)

where QB = QB(s, a), QA = QA(s(cid:48), b∗), and b∗ =
argmaxaQB(s(cid:48), a).

In this paper a modiﬁed version of the algorithm proposed
by Hasselt et. al. [3] is employed; Algorithm 1 shows the
implementation of this algorithm.

Weight wi is given by normalised kernels between vectors
h and hi,
the lookup key and the corresponding key in
memory respectively. The kernel function used for h and hi
is Eq. 5 [4]. The write operation simply appends the keys
and values to their corresponding arrays Ka and Va. Should
a key-value pair already exist it is updated. The values in
the DND are in turn the corresponding Q values to the state

originally having resulted in the key-value pair to be written
to memory. Therefore producing an estimate of Q(s, a) for
any single given action a [4]. To update the values in DND
N-step Q-learning is applied as in [12], thus the N-step Q-
Value estimate is then

Q(N )(st, a) =

N −1
(cid:88)

j=0

γjrt+j + γN maxa(cid:48)Q(st+N , a(cid:48))

(6)

Thus the value in array Va is Qi for:

Qi ← Qi + (Q(N )(s, a) − Qi)

(7)

Neural Episodic Control, however, requires large memory
and a lot of calculation time with its computational space and
time [5]. Thus in [4] a solution is proposed in the form of
the algorithm called Neural Episodic Control to a Deep Q-
Network, or shorthand NEC2DQN (N2D). This algorithm is
created by supplementing NEC with the simplest Deep Q-
learning network. This is possible because with both NEC
and DQN there is always one action-value Q*, thus it is
possible for them to converge on the same Q*. Therefore
the Q-value of one algorithm can be taken as the target
value, making it easier to converge towards a better target
value, approaching Q* faster [4]. Similarly this is seen in
the Double DQN algorithm mentioned previously. By setting,
QA(s, a) = QDQN (s, a), QB(s, a) = QN EC(s, a) we can
modify (2) and (3) such that

QDQN (s, a) ← QDQN (s, a)+(r+QN EC(s(cid:48), a∗)−QDQN (s, a)),

and

QN EC(s, a) ← QN EC(s, a)+(r+QDQN (s(cid:48), b∗)−QN EC(s, a)).

(8)

(9)
Since NEC uses N-step learning we adopt and rewrite the
equations as;
QDQN (s, a) ← QDQN (s, a)+(r+Q(N )

N 2D(s(cid:48), a∗)−QDQN (s, a))

(10)

9:
10:
11:
12:
13:
14:

15:
16:
17:
18:
19:

20:
21:
22:
23:
24:
25:

26:
27:
28:
29:
30:
31:

32:
33:
34:
35:
36:

Algorithm 2 Neural Episodic Control to Deep Q-Learning
(Nishio et al., 2018)

1: Initialise the number of time steps
2: Initialise the change step CS for λ(t)
3: Initialise replay memory D to capacity CD
4: Initialise replay memory E to capacity CE
5: Initialise DND memory Ma capacity CMa
6: Initialise action-value function QN EC and QDQN
7: for each episode do
8:

for t = 1, 2, ..., T do

Receive observation st from environment
if t mod(2) then

Receive QDQN (st, a)
if S < CS then(use NEC)

Receive embedding h and QN EC(st, a).

else

Set QN EC(st, a) to free values.

end if
Calculate QN 2D(st, a)
at ← (cid:15) − greedy policy on QN 2D(st, a)
Take action at, receive reward rt.
Append (st, at, rt) to G.
Train on a random minibatch from D.
S ← S + 1

end if
if not t mod(2) then

Receive QDQN (st, a)
at ← (cid:15) − greedy policy on QDQN (st, a)
Take action at, receive reward rt.
Append (st, at, rt, st+1) to E
Train on random minibatch from E

end if
for t = 1, 2, ..., T do
Calculate yt
Append (st, at, rt) to G
if S < CS then

Append (ht, yt) to Ma

end if

QN EC(s, a) ← QN EC(s, a)+(r+Q(N )

N 2D(s(cid:48), b∗)−QN EC(s, a)),

where Q(N )

N 2D is deﬁned as

end for

37:
38:
39: end for

end for

(11)

QN 2D(st, a) = λ(t)QN EC(st, a) + (1 − λ(t))QDQN (st, a).
(12)

Algorithm 2 shows the implementation of Neural Episodic
Control to Deep Q-Learning. Initially, a Convolutional Neural
Network (CNN) is used because the input states contained
pixels that were processed. Algorithm 2 was then modiﬁed
such that instead of using a CNN to getting the state em-
bedding h, an embedding neural network was used. The
modiﬁcation was necessary because the state provided is a
1-dimensional array of length 80, containing 1’s and 0’s.

The algorithms process information, learn and make the
decision that best maximises the return reward for a chosen
action. The algorithms use Q-values to determine the state-
action value pair that produces the action which maximises
the reward return. They rely on the experiences stored in a

memory buffer. To ﬁrst get these experiences exploration is
introduced, where an action is selected at random from the
action space available. Memory batches are sampled and used
in the training of the model using the algorithm that is built
using the equations explored. As the agent using the model
progresses and adapts, exploitation is used. Exploitation is
when the model makes a prediction based on what it’s learnt,
instead of using random exploration. The shift is gradual and
in this case is implemented using the epsilon-greedy method,
in order to balance exploration vs exploitation.

IV. METHODOLOGY

For the execution of the simulation, all

the work was
done in Ubuntu 20.04 with the programming language of
choice Python3 alongside the Tensorﬂow2, Cuda and Mininet

frameworks as well as the RYU network controller. The
hardware used to run the simulations was an Nvidia RTX
3080Ti, Intel i7 10700F, with 64 Gigabytes of DDR4 RAM.

The algorithms presented in the previous section are im-
plemented in a way that they take in a state, then either by
exploration or exploitation, make a decision on an optimal
action. That action is then implemented into the environment
and in return a new environment state and reward is returned.
As illustrated in Fig 1., two different agent algorithms are
implemented on a turn based system.

Fig. 1. Block diagram of Deep Reinforcement Learning implementation

The agents using these algorithms play two different roles
alternatively. During runs of game 1 the N2D algorithm is
used by the defender and DDQN by the attacker. During the
runs of game 2 the N2D algorithm is used by the attacker and
DDQN is used by the defender. As illustrated in Fig. 2, the
environment makes use of a star topology, with four switches
connected to a central router. Each switch connects to a series
of hosts that forms its own subnet. In total there are 4 four
subnets with subnet 1 having 6 hosts, subnet 2 with 8 hosts as
well as subnets 3 and 4 with 9 hosts each. The environment
is hosted on Ubuntu running MiniNet and RYU natively.

Network restrictions and routes were set in place within the
SDN using MiniNet, such that only certain hosts were visible
to others. Fig. 3 illustrates the network seen by the agents
as well as communication routes. In Fig. 3 hosts that are
coloured red are those that are initially compromised where
as blue indicates the critical server location and the yellow
indicates those which are ordinary hosts.

The ﬂag system commonly used in hackathon competitions
has been implemented into the game. Depending on the role
the agent had, they would have a different set of actions
as described with different win conditions. For the attacking
agent to win, it would have to successfully complete its goal
by compromising the critical server, or drain the defender of
points, such that its score is greater than the defenders. The
attacker can only compromise one host a turn, by injecting a
ﬂag into that compromised host. The defending agent wins
by completely blocking out the attacker, by isolating and
patching all hacked hosts, or by preventing further spread and
thus isolating the remaining network from infected sections,
or by outlasting the attackers assault and ending the game

Fig. 2. Star topology implemented in MiniNet

Fig. 3. Topology showing the allowed visibility and communication links

with a greater score.

The observed state that the agents see is a numpy array
of length 80, containing 1’s and 0’s, representing the hosts
and links. For the hosts 1 represents not compromised and
0 represents compromised. For the links in the network, the
value of 1 represents active and 0 represents inactive. The zero
sum game format was chosen when structuring the format
of the game loop. For scoring the defender will start with
the maximum possible score. The attacker will start at 0. As
the attacker gains ground, the defender loses points and the
attacker gains points. As the defender regains lost ground then
the defender regains those lost points and the attacker loses
them. Initially a game ran at a cap of 1,000,000 turns, giving
each agent 500,000 turns each. Thus for 10 game runs each
agent had a total of 5 million steps. When running at 500,000
turns per agent it was rare that a game run exceeded 50,000

turns, thus the turns were capped at 50,000. Taking this into
consideration, later, 10 game runs were done at 25,000 turns
per agent. Thus each agent got a total of 250,000 steps. Two
games are done where the agents had alternated their roles,
implying in game 1 the agent using the N2D algorithm is the
defender and in game 2 is the attacker. The attacking agent
uses the DDQN algorithm in game 1, and in game 2 is the
defending agent. In all games the attacker goes ﬁrst.

A. Agents

Two agents are implemented. An agent is classiﬁed as red
when it has the role of the attacker and blue when it has the
role of the defender. An agent is an entity trained to make
the most optimal decision by means of the desired algorithm
implemented. Both these agents are each implementing a
different type of deep reinforcement learning algorithm. The
DDQN agent uses the Double Deep Q-Network algorithm
proposed in [3], as presented in Algorithm 1. The N2D agent
uses the NEC2DQN algorithm as proposed by in [4] and
illustrated in Algorithm 2.

The agents alternate in the assignment of roles. In case
A) DDQN is the attacker and N2D is the defender, but in
case B) DDQN becomes the defender and N2D becomes the
attacker. The attacking agent has access to 32 action outputs
and the defending agent has access to 68 action outputs. The
attacker has one action, selecting a host to be hacked and
compromising it, but with 32 hosts, we have 32 available
action outputs. The defender has 4 different types of actions;
(1) Isolating and patching a host; (2) Reconnecting a host
and its respective links; (3) Migrating the critical server to
any of the predetermined backup locations (hosts) and; (4)
doing nothing. The single action rewards are within the set
R = {−1, 0, 1} such that r ∈ R, however for the attacker the
action rewards are r ∈ {−1, 1}.

V. RESULTS AND DISCUSSIONS

no signiﬁcant difference between the algorithms. The large
disparity between the means is likely due to some outlier data
effects.

TABLE I
GAME RUN RESULTS

Runs

1
2
3
4
5
6
7
8
9
10

Game 1: total turns
played by defender
168
356
3066
11120
10258
6154
8726
322
682
740

Game 2: total turns
played by defender
35
68
240
5414
1158
2299
2936
4050
1699
339

In order to ﬁnd out if there was a signiﬁcant difference
in the means, a two-tailed T-test analysis that was conducted
on the two game results presented in Table I, with an alpha
of 0.05. Results in Table II show a P(T ≤ t) two-tail value
is 0.086288326, which is greater than 0.05. Therefore, there
is no signiﬁcant difference between the two algorithms. The
DDQN is, therefore, a better algorithm simply because it is
simple and leads to lower computational overhead.

TABLE II
TWO-TAILED T-TEST ANALYSIS FOR THE TWO GAMES

Game 2 turns
1823.8
3418095.95
10

Mean
Variance
Observations
Pearson Correlation
Hypothesized Mean
Difference
df
t Stat
P(T ≤ t) one-tail
t Critical one-tail
P(T ≤ t) two-tail
t Critical two-tail

Game 1 turns
4159.2
20064225.96
10
0.53
0

9
1.9255316
0.04314416
1.8331129
0.086288326
2.262157158218

The results are presented in two categories: game perfor-
mance results and agent rewards. These results are presented
in the next subsections.

B. Agent Rewards

A. Game Results

Table I shows the results of the two games played. 10
Runs with each agent having 25,000 turns was done. The
number of turns taken for the defending agent to win each
game recorded, and statistically analysed. Statistical analysis
showed the two-tailed p-value equals 0.1449, revealing there
is no signiﬁcant statistical difference between the two algo-
rithms. The calculated mean value for the number of turns
taken for the defending agent using the N2D algorithm is
4,159.20. This implies that on average, it takes the defender
4,159 turns to isolate the intruder, and win the game. The
calculated mean value for the defending agent using the
DDQN algorithm is 1,823.80. This implies that on average
takes the defender 1,823 turns to isolate the intruder,
it
and win the game. Even though, this shows that the better
defender algorithm seems to be DDQN, statistically, there is

The agents need to learn while being deployed, therefore
they need to maximize their received reward, since they may
not reach the state where they no longer need to do exploration
[13]. Figure 4 shows the rewards distribution of each agent’s
algorithm in each game’s run. The defending agent uses
the N2D algorithm and the attacking agent uses the DDQN
algorithm.

Figure 5 shows the rewards distribution of each agent’s al-
gorithm in each game’s run. Defending agent uses the DDQN
algorithm and the attacking agent uses the N2D algorithm. In
both games’ results the defending agent’s scores in the early
and latter runs shows dominance over the attacking agent.
This is likely due to the case of exploration being greater
than exploitation, and with the defending agent being able to
isolate any host, this gives it a much better initial momentum.
Unlike the attacker that is limited to the neighbours of already
compromised hosts. In the mid section group of runs, the

attacker. The current game environment is inherently biased in
favor of the defender, therefore improving on the algorithms
to function better in this environment also opens the door
for the models to become more effective in aggression as
an attacker. Should there be growth in the attacking role it
should be considered that these algorithm may also serve as
good offensive tools within the cyber security space.

REFERENCES

[1] T. T. Nguyen and V. J. Reddi, “Deep reinforcement

learning for
cyber security,” IEEE Transactions on Neural Networks and Learning
Systems, pp. 1–17, 2021. [Online]. Available: https://doi.org/10.1109%
2Ftnnls.2021.3121870

[2] Y. Han, B. I. P. Rubinstein, T. Abraham, T. Alpcan, O. De Vel, S. Erfani,
D. Hubczenko, C. Leckie, and P. Montague, “Reinforcement learning
for autonomous defence in software-deﬁned networking,” 2018. [On-
line]. Available: https://arxiv.org/abs/1808.05770

[3] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” 2015. [Online]. Available: https://arxiv.org/abs/
1509.06461

[4] D. Nishio and S. Yamane, “Faster deep q-learning using neural episodic
control,” in 2018 IEEE 42nd Annual Computer Software and Applica-
tions Conference (COMPSAC), vol. 1.

IEEE, 2018, pp. 486–491.

[5] A. Pritzel, B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis,
D. Wierstra, and C. Blundell, “Neural episodic control,” CoRR, vol.
abs/1703.01988, 2017. [Online]. Available: http://arxiv.org/abs/1703.
01988

[6] R. S. F. Community. Build sdn agilely. [Online]. Available: https:

//ryu-sdn.org/

[7] M. P. Contributors. Mininet an instant virtual network on your laptop

(or other pc). [Online]. Available: http://mininet.org/

[8] D. Casas-Velasco, O. Caicedo Rendon, and N. Fonseca, “Intelligent
routing based on reinforcement learning for software-deﬁned network-
ing,” IEEE Transactions on Network and Service Management, vol. 18,
pp. 870–881, 03 2021.

[9] A. Hakiri, A. Gokhale, P. Berthou, D. Schmidt, and T. Gayraud,
“Software-deﬁned networking: Challenges and research opportunities
for future internet,” Computer Networks, vol. 75, 12 2014.

[10] M. van Otterlo and M. A. Wiering, “Markov decision processes:

Concepts and algorithms,” 2012.

[11] H. Van Hasselt, “Double q-learning.” 01 2010, pp. 2613–2621.
[12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” in Proceedings of The 33rd International Con-
ference on Machine Learning, ser. Proceedings of Machine Learning
Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New York,
New York, USA: PMLR, 20–22 Jun 2016, pp. 1928–1937. [Online].
Available: https://proceedings.mlr.press/v48/mniha16.html

[13] D. P. L. Poole and A. K. Mackworth, Foundations of Computational
Agents 2nd Edition, Nov 2018. [Online]. Available: https://artint.info/
2e/html/ArtInt2e.Ch12.S6.html

Luke D. Borchjes received his Bachelor’s Degree (Honors) in Computer
Science from the University of the Western Cape. He is currently working
on his MSc in Computer Science at the University of the Western Cape. His
research interests are in Deep Reinforcement Learning, Cyber Security and
Software Deﬁned Networks.

Clement N. Nyirenda received his PhD in Computational Intelligence from
Tokyo Institute of Technology in 2011. His research interests are in Com-
putational Intelligence paradigms such as Fuzzy Logic, Swarm Intelligence,
and Artiﬁcial Neural Networks and their applications in Communications.

Louise Leenen Louise Leenen completed her PhD at the University of
Wollongong in Australia in 2009. Her research areas are AI Applications
in Cybersecurity, Ontology Engineering and Mathematical Modelling.

Fig. 4. Game run rewards of the defending agent using the neural episodic
control to deep q-network algorithm, and attacking agent using the double
deep q-network algorithm.

Fig. 5. Game run rewards of the attacking agent using the neural episodic
control to deep q-network algorithm and the attacking agent using the double
deep q-network algorithm.

attacking agent is able to gain ground, this is likely because
at this point the agent has switched to exploitation and is
able to target hosts more efﬁciently, leading to longer in-game
engagement and a higher turn count. In the latter grouping,
the defending agent once again dominates, this could be from
exploitation and the adaptation to the selection patterns used
by the attacker. It is also worth noting that each agent retains
its memory of each run.

VI. CONCLUSION AND FUTURE

This paper presents a comparative evaluation of Neural
Episodic Control to Deep Q-Network (N2D) and Double Deep
Q-Networks for cyber security purposes in software deﬁned
networks (SDNs). The results show that both algorithms are
adequate tools for network defense. There is no statistically
signiﬁcant difference between the approaches, which makes
DDQN more favorable due to its simplicity.

In future, the number of game runs will be increased in
order to provide a greater amount of total steps for each
agent, as well as a greater data pool to analyse. Adversarial
learning will be implemented, in which the attacker does white
and black box causative attacks on the defending agent. The
network topology will also be expanded. Another issue is
that improvement on these algorithms holds the possibility of
diminishing returns as a defender but signiﬁcant growth as an

