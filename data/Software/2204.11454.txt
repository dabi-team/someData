Natural Language to Code Translation with Execution

Freda Shi∗∗♥♦ Daniel Fried♥♠ Marjan Ghazvininejad♥

Luke Zettlemoyer♥

Sida I. Wang♥

Facebook AI Research♥

TTI-Chicago♦ Carnegie Mellon♠

freda@ttic.edu

sida@fb.com

2
2
0
2

r
p
A
5
2

]
L
C
.
s
c
[

1
v
4
5
4
1
1
.
4
0
2
2
:
v
i
X
r
a

Abstract

Generative models of code, pretrained on large
corpora of programs, have shown great suc-
cess in translating natural language to code
(Chen et al., 2021; Austin et al., 2021; Li
et al., 2022, inter alia). While these models do
not explicitly incorporate program semantics
(i.e., execution results) during training, they
are able to generate correct solutions for many
problems. However, choosing a single correct
program from among a generated set for each
problem remains challenging.

In this work, we introduce execution result–
based minimum Bayes risk decoding (MBR-
EXEC) for program selection and show that
it improves the few-shot performance of pre-
trained code models on natural-language-to-
code tasks. We select output programs from a
generated candidate set by marginalizing over
program implementations that share the same
semantics. Because exact equivalence is in-
tractable, we execute each program on a small
number of test inputs to approximate seman-
tic equivalence. Across datasets, execution or
simulated execution signiﬁcantly outperforms
the methods that do not involve program se-
mantics. We ﬁnd that MBR-EXEC consis-
tently improves over all execution-unaware se-
lection methods, suggesting it as an effective
approach for natural language to code transla-
tion.

Figure 1: Illustration of MBR-EXEC on translating nat-
ural language to Python code: we (1) sample programs
from Codex (Chen et al., 2021), (2) execute each pro-
gram on one test case, and (3) select the example with
the minimal execution result–based Bayes risk. Num-
bers around dotted lines denote the 0/1 matching loss
between execution results, while the Bayes risk of a
program is deﬁned by the sum of the loss between it-
self and other examples. In the ﬁgure, either Code #1
or Code #3 can be selected. Ground-truth program out-
put is not needed for selection.

1

Introduction

The recent success of large pretrained language
models (Radford et al., 2019; Brown et al., 2020)
has extended to translating natural language de-
scriptions into executable code (Chen et al., 2021;
Austin et al., 2021; Li et al., 2022, inter alia). After
pretraining on large corpora of code with a simple
language modeling objective, the models demon-
strate the ability to follow few-shot prompts (Rad-
ford et al., 2019; Brown et al., 2020) to translate nat-
ural language to various programming languages.

∗Work done while interning at Facebook AI Research.

While code sampled from such models obtains sur-
prisingly good BLEU scores against ground-truth
programs and relatively high execution accuracies,
it often includes obvious mistakes, and is of much
lower quality than the code written by intermediate-
level human programmers (Li et al., 2022).

In this work, we translate natural language to
executable code with awareness of execution re-
sults on a limited number of test case inputs,
which we require only at inference time. Our ap-
proach is built on the hypothesis that a pretrained
code model spreads probability mass over multi-

4<info> assertadd(1, 2) == 3</info><text> Add twointegers </text><code> defadd(a, b): returna+ b</code>…<info> assertcount("abc1") == 3</info><text> Count lowercase letters </text><code> defcount(s):returnlen(s)</code>Codexdefcount(s):returnlen([cforcinsifc.islower()])</code>defcount(string):cnt= 0forchinstring:ifch.islower():cnt+= 1returncnt</code>count("abc1")=Execute3333Code #3Code #2Code #1110011Bayes Risk112Execution Result MatchingFew-shot     PromptTest Example 
 
 
 
 
 
ple semantically-equivalent code forms that imple-
ment the same functionality. Given a text descrip-
tion of a desired program function, we (1) sample
a set of programs from a pretrained code model
(§3.1) and (2) select a single candidate program
using execution-result-based minimum Bayes risk
(MBR) decoding (§3.2). Intuitively, we score each
sampled program using its agreement to other sam-
ples in terms of execution results, and select a pro-
gram with maximal overall agreement.

Our evaluation focuses on a challenging set-
ting where only a single program can be submit-
ted as the solution to a given problem. We show
that the execution result–based selection method
(i.e., MBR-EXEC) signiﬁcantly outperforms all no-
execution baselines across all considered datasets,
despite having never executed any code during
training and even when it has no access to ground-
truth outputs. In addition, we show that MBR de-
coding with a BLEU-based risk function performs
consistently well across datasets, and can be con-
sidered as a promising alternative when we are not
able to execute.

2 Related Work

2.1 Language to Code with Neural Networks

With the progress of neural network–based lan-
guage modeling and conditioned text generation,
there has been much work exploring natural lan-
guage to code generation with end-to-end neu-
ral model architectures (Xiao et al., 2016; Ling
et al., 2016; Rabinovich et al., 2017; Dong and
Lapata, 2018; Suhr et al., 2018; Xu et al., 2020;
inter alia). Recently,
Lachaux et al., 2021,
large Transformer-based (Vaswani et al., 2017)
pretrained code models have shown surprisingly
strong generation performance across program-
ming languages (Chen et al., 2021; Austin et al.,
2021; Li et al., 2022, inter alia). In this work, we
explore selection (i.e., inference) methods to apply
to these pretrained models, showing that selecting
programs using their execution results can greatly
improve program generation.

Multiple benchmarks have been proposed to
evaluate code model performance (Miceli Barone
and Sennrich, 2017; Yin et al., 2018; Hendrycks
et al., 2021; Lu et al., 2021, inter alia). In this work,
we evaluate on three text-to-code datasets: MBPP
(Python; Austin et al., 2021), Spider (SQL; Yu et al.,
2018) and NL2Bash (Bash; Lin et al., 2018), cov-
ering a range of programming languages.

2.2 Prompting Pretrained Language Models

The GPT-2 (Radford et al., 2019) and GPT-3
(Brown et al., 2020) models have shown strong
prompting performance: after conditioning on a
task-related prompt, the language models are of-
ten able to make accurate output predictions for
unseen inputs. These results lead to prompt-based
approaches for few-shot or zero-shot text classiﬁca-
tion (Shin et al., 2020; Gao et al., 2021; Min et al.,
2021, inter alia), question answering (Khashabi
et al., 2020), machine translation (Radford et al.,
2019), and evaluation of generated text (Yuan et al.,
2021), where no more than a few examples are
used to construct the prompts. Few-shot exam-
ples are usually formatted into natural language
prompts and continuations generated by the mod-
els for these prompts are then converted to task-
speciﬁc predictions. The prompt formatting can
be either manually designed (Jiang et al., 2020) or
automatically learned (Li and Liang, 2021; Lester
et al., 2021). We refer the readers to Liu et al.
(2021) for a comprehensive survey.

We prompt a pretrained code model (Codex;
Chen et al., 2021) in a few-shot setting (§3.1) and
perform execution-based selection over the sam-
ples. We also ﬁnd that the Codex model performs
well with a fairly programming-language-agnostic
prompt formatting (Table 1).

2.3 Minimum Bayes Risk Decoding

In structured prediction, Minimum Bayes risk
(MBR) decoding (Bickel and Doksum, 1977) se-
lects a structured output that minimizes the ex-
pected errors in the structure by introducing an
explicit loss function to the decision criterion. This
method has outperformed the maximum a posteri-
ori (MAP) method on many tasks, including speech
recognition, syntactic parsing (Titov and Hender-
son, 2006; Shi et al., 2019; Zhang et al., 2020),
statistical machine translation (Kumar and Byrne,
2004; Zhang and Gildea, 2008), and neural ma-
chine translation (Eikema and Aziz, 2020, 2021).

In machine translation, MBR decoding is usu-
ally implemented by reranking candidates (Goel
and Byrne, 2000; Kumar and Byrne, 2004; Tromble
et al., 2008, inter alia). Let F denote the input, and
E denote the corresponding ground-truth transla-
) between trans-
tion. Given a loss function (cid:96)(
·
F ), MBR
lations and a probability model P (E

,
·

|

General Template

<info>[INFO]</info>
<text>[TEXT]</text>
<code>[CODE]</code>

(optional)

Instantiation 1: Python
One-Shot Example

<info>assert add(1, 2) == 3</info>
<text>Write a function that adds 2 integers</text>
<code>def add(a, b):

return a + b</code>

Query

<info>assert cat() == "cat"</info>
<text>Write a function that outputs the string "cat"</text>
<code>

Instantiation 2: Bash
One-Shot Example

<text>show the files in the current directory</text>
<code>ls</code>

Query

<text>show the first 5 lines of a.txt</text>
<code>

Table 1: Prompt formatting template for queries to pretrained code models. For instantiation, we substitute
[TEXT] and [CODE] with natural language descriptions and corresponding code snippets respectively. We also
provide compatibility for an optional [INFO] section to provide the model extra information (e.g., the desired
function identiﬁer and example function calls) that helps code generation. In general, we expect the pretrained
code models to generate a </code> token at the end of each code snippet given its pattern following ability
(Brown et al., 2020; Chen et al., 2021), otherwise we truncate the generated code to a maximum of 1024 tokens.

decoding can be formulated as

ˆE = arg min
E(cid:48)∈Eh

(cid:88)

E∈Ee

(cid:96)(E, E(cid:48))P (E

F ),

(1)

|

h is the hypothesis space, and
E

e is the evi-
where
E
dence space: both are sets of possible translations.
We deﬁne execution based MBR loss functions,
and show that they are crucial in the sample selec-
tion processes for natural language to code with a
pretrained large language model.

3 Proposed Approach: MBR-EXEC

Our execution-based framework consists of two
parts: (1) collecting samples from a pretrained code
model (§3.1) and (2) selecting the best candidate
using minimum Bayes risk decoding (§3.2).

3.1 Sample Collection

To obtain the corresponding code, we query the
pretrained code model with few-shot prompts fol-
lowed by the text description, using a uniﬁed
mark-up style few-shot prompting template (Ta-
ble 1).1
In addition to the generated programs
themselves, most existing models also allow us
to have the associated probability of generating
each generated token wi conditioned on the prompt

1While existing work on prompting language models usu-
ally requires a task-speciﬁc design of prompts (Shin et al.,
2020; Zhong et al., 2021; Gao et al., 2021, inter alia), we ﬁnd
that a fairly general pattern (Table 1), which does not involve
any programming language–speciﬁc information, works well
across programming languages on Codex.

tokens C =
generated tokens w1, . . . , wi−1, denoted by P (wi
C, w1, . . . wi−1).

c1, . . . , cn
(cid:104)

and all the previously

(cid:105)

|

3.2 Execution-Based MBR Decoding

Given a problem in its natural language description
N
i=1 using
C, we sample a set of programs
the method in §3.1. We formulate the execution-
based MBR (MBR-EXEC) decoding by selecting

pi
{

=

P

}

ˆp = arg min

MBR(p;

p∈P L

)

P

= arg min
p∈P

(cid:88)

pref∈P

(cid:96)(p, pref)

(2)

;

L

MBR(
·

) denotes the
as the best candidate, where
·
MBR loss of a program conditioned on a set of ref-
erences and (cid:96) is a predeﬁned, execution-based loss
function that examines the discrepancy between
two programs. Intuitively, this ﬁnds a consensus
candidate which has a low loss relative to all other
candidates. The above implementation is an unbi-
ased estimation of Eq (1).

We introduce the following execution result–

based loss function:

(cid:96)(pi, pj) = max
t∈T

1 [pi(t)

= pj(t)] ,

is the set of available test inputs,2 and
where
pi(t) denotes the execution result of program pi

T

2Our MBR-EXEC decoding process does not involve any
ground-truth test case output, nor the ground-truth programs.
This is compatible with many real scenarios, e.g., in a pro-
gramming competition, where valid test input are easier to
access than ground-truth output.

(cid:54)
Method

MBPP

Spider

NL2Bash

Greedy (3-shot)
Sample (3-shot)

47.3 ± 2.5 50.8 ± 2.6 52.8 ± 2.9
47.7 ± 1.5 48.5 ± 2.6 53.0 ± 2.9

MBR-EXEC

58.2 ± 0.3 63.6 ± 0.8 58.5 ± 0.3

Table 2: Comparison between MBR-EXEC and base-
lines without selection process. For both MBR-EXEC
and Sample (3-shot), we collected samples with tem-
perature 0.3. All numbers involve the same set of 125
samples for each case: for greedy and sample baselines,
we report average performance of them all; for MBR-
EXEC, we report the result with 25 examples, averaged
across 5 experiments.

when having t as the input. When a program fails
to execute on a test case, it is considered not equiv-
alent to any other programs, even if they fail to
execute as well. Intuitively, (cid:96) assigns equivalence
(0 loss) if and only if two programs have the same
output on all considered test cases.

There may be multiple programs receiving the
), which are all minima.
;
MBR(
same MBR loss
P
·
We break any ties by selecting the program with
the largest likelihood among them.

L

4 Experiments

We evaluate (§4.3) and analyze (§4.4) the perfor-
mance of MBR-EXEC, starting with introducing
the datasets and evaluation metrics (§4.1), as well
as non-execution-based baselines (§4.2) for MBR-
EXEC. Finally, we show and discuss oracle perfor-
mances on the considered tasks (§4.5).

4.1 Datasets and Evaluation Metrics

We consider three datasets that cover a range of
programming languages: MBPP (Python; Austin
et al., 2021), Spider (SQL; Yu et al., 2018), and
NL2Bash (Bash; Lin et al., 2018).

MBPP. The MBPP dataset (Austin et al., 2021)3
consists of 974 basic Python programming prob-
lems, with 500 of them used for testing and the
rest for training or few-shot prompting. There are
ground-truth program and three assertions (i.e., test
cases with input and ground-truth output) associ-
ated with the description of each problem. When
collecting the samples, we use one assertion as the
extra information ([INFO]; Table 1).4 Programs

3https://github.com/google-research/

google-research/tree/master/mbpp

4The main goal of [INFO] in MBPP is to inform Codex
about the desired function name for easier evaluation – while
the assertions are not a necessary part of prompt, we use them

are evaluated with execution accuracy, where a pro-
gram is considered as passing if all three test cases
are correct.

Spider. The Spider dataset (Yu et al., 2018)5 is
a text-to-SQL dataset, which requires a model to
translate text descriptions into SQL commands.
There are 7,000 examples for training and 1,034
for development. When prompting models to pro-
duce candidate commands, we concatenate the cor-
responding SQL table and column names as the
[INFO]. Commands are evaluated with the execu-
tion accuracy, where a command is considered as
passing if it returns the same result as the ground-
truth command when being executed on the same
database.

NL2Bash. The NL2Bash dataset (Lin et al.,
2018) aims to translate natural language to bash
commands. We do not include [INFO] in the
sample collection process. Because it is difﬁcult to
execute bash commands in a sandbox, we split a
bash command with bashlex,6 a rule-based bash
parser, and use the token-level BLEU-4 score be-
tween commands as the estimation of execution
result similarity. We consider a command to be
unexecutable when bashlex fails to parse it. Fol-
lowing Lin et al. (2018), commands are evaluated
with character-level BLEU-4 score.

Across datasets, we use 15 examples from the
training set for few-shot prompting. A detailed ex-
ample showing prompt formatting can be found in
Appendix A. Unless otherwise speciﬁed, we col-
lect samples by querying Codex with ﬁve different
prompts, each containing 3 examples, using tem-
perature 0.3. We combine the candidates sampled
across the ﬁve prompts to get a set of candidate
samples to use in our selection methods. For execu-
tion on MBPP and Spider, we apply a memory limit
of 128GB and a time limit of 10 seconds on a sin-
gle Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
CPU, and consider the programs that exceed these
limits as inexecutable; unless otherwise speciﬁed,
we only execute each program on the ﬁrst test input
provided for the example, and use the output for
calculating the Bayes risk in the inference process.

as [INFO] for simplicity and compatibility with past work
(Austin et al., 2021).

5https://yale-lily.github.io/spider
6https://pypi.org/project/bashlex/

(a) MBPP

(b) Spider

(c) NL2Bash

Figure 2: Primary evaluation results: performance of the evaluated selection criteria (best viewed in color). For
each sample size, we evaluate the methods on 5 different groups of samples and report the average performance
(lines) and the standard deviations (shaded regions). All samples are collected from Codex with temperature 0.3.

4.2 Baselines

4.3 Primary Results

We compare the most basic baselines with no se-
lection, prompting Codex with three examples in
Table 1 format:

• Greedy decoding. We perform token by token

greedy decoding to generate the output.

• Sampling. We sample the output token by to-
ken with a ﬁxed temperature, where we set the
temperature as 0.3 in our experiments.

In addition, we consider the following baseline
sample selection methods:

• Maximizing likelihood (ML). Given a set of
sampled candidate programs, we select the one
with the largest log likelihood. Formally, we
select

ˆp = arg max
p∈P

np
(cid:89)

i=1

P (wp,i

|

C, wp,1, . . . , wp,i−1),

where np denotes the number of tokens in a gen-
erated program p, and wp,i denotes its i-th token.
• Maximizing average log likelihood (MALL)
across tokens. In order to address the practical
issue that ML typically favors shorter sequences,
we follow Chen et al. (2021) and propose an-
other baseline that uses the average log likelihood
across tokens as the selection criterion.

• BLEU score based MBR (MBR-BLEU). To
study the effect of execution based MBR in sam-
ple selection, we consider BLEU score based
MBR, where the Bayes risk is calculated using
the following risk function:

(cid:96)BLEU(pi, pj) =

BLEU(pi, pj),

−
where BLEU(pi, pj) is the BLEU score of the
two programs. We use character-level or token-
level BLEU-4 in all of our experiments.

We evaluate MBR-EXEC on the three datasets
(§4.1) with dataset-speciﬁc metric, where we use
one test case for each problem. MBR-EXEC outper-
forms all baselines without a selection process by
a signiﬁcant margin (Table 2). In addition, we ﬁnd
that MBR-EXEC outperforms all baseline selection
methods (Figure 2), and is especially effective on
the two datasets (MBPP, Spider) that use execution-
based evaluation.
In addition, the MBR-BLEU
metrics are also strong and robust across datasets,
suggesting the effectiveness of ﬁnding a consensus
candidate that has generally low discrepancy with
other samples.

While more samples lead to better performance
for most methods, MALL consistently performs
worse with a larger sample size, as we ﬁnd that
MALL generally favors programs with unneces-
sary repetitions,7 and a larger sample size generally
leads to a larger chance to have such a sample.

4.4 Analysis

We analyze the performance of MBR-EXEC from
the following perspectives: the effectiveness across
different sample collection temperatures (§4.4.1),
the effectiveness of using groups of 3-shot prompts
(§4.4.2) and the contribution of using execution
results instead of simply checking the executability
of programs (§4.4.3).

4.4.1 Effect of Sample Temperature

We ﬁrst compare sampling with temperature 0.3 to
greedy decoding from the Codex model (Table 3).
When having the same number of examples, MBR-

7This issue has been found in existing open-ended text gen-
eration models, while methods such as unlikelihood training
(Welleck et al., 2020) may help reduce degeneration (i.e., the
generation of unnecessarily repetitive output).

020406080100120#Samples485052545658ExecutionAccuracy(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL020406080100120#Samples404550556065ExecutionAccuracy(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL020406080100120#Samples102030405060char.BLEU(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL(a) MBPP

(b) Spider

(c) NL2Bash

Figure 3: Performance of the evaluated selection criteria across temperatures (best viewed in color). For each
temperature, we perform the methods on 5 different groups of 25 examples and report the average performance
(lines) and the standard deviations (shaded regions).

Dataset

Greedy (τ = 0)

Sample (τ = 0.3)

MBPP
Spider
NL2Bash

56.0
62.1
58.4

58.2 ± 0.3
63.6 ± 0.8
58.5 ± 0.3

Table 3: MBR-EXEC performance on greedily de-
coded and sampled programs: for each problem, we
use 25 groups of 3-shot prompts, decode or sample
one program with each prompt, and use MBR-EXEC
to select the best program. For sampling with temper-
ature 0.3, we repeat the process for 5 times and report
the average performance and standard deviations. The
dataset-speciﬁc metric can be found at §4.1. The best
number in each row is in boldface.

EXEC on sampled candidates with temperature 0.3
consistently reaches competitive or better perfor-
mance than that on greedy decoded candidates.

We plot the performance of MBR-EXEC for var-
ious sampling temperatures (Figure 3). Across
datasets, we ﬁnd that MBR-EXEC with a decoding
temperature lower than 0.5 usually leads to rea-
sonably good performance. When the temperature
approaches 1.0, the results rapidly drop for all con-
sidered selection methods on MBPP and Spider;
however, MALL generally achieves higher perfor-
mance on NL2bash with a higher temperature.

According to the evidences discussed above, we
recommend to use sampling with a low temper-
ature (speciﬁcally, lower than 0.5) for candidate
sample collection, and perform MBR-EXEC for
ﬁnal program selection for better results.

4.4.2 Effect of Different 3-shot Prompts

(a) MBPP

(b) NL2Bash

Figure 4: Performance with different types of prompts,
where groups of 3-shot denotes the prompt formatting
in Table 1, while concatenation of 15 denotes concate-
nating all available 15 examples as prompts for data
collection.

ure 4).8 We allow different orders of the 15 ex-
amples when collecting samples. On both MBPP
and NL2Bash datasets, we ﬁnd that using differ-
ent groups of 3-shot prompts clearly outperforms
concatenating all 15 examples, suggesting that dif-
ferent groups of fewer-shot prompts followed by

We analyze the necessity of choosing multiple
groups of 3-shot instead of simply concatenat-
ing the available 15 examples as the prompt (Fig-

8We only include MBPP and NL2Bash results here as
concatenating 15 Spider examples usually results in exceeding
the token number limit of the pretrained models.

0.20.40.60.81.0Temperature(τ)4045505560ExecutionAccuracy(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL0.20.40.60.81.0Temperature(τ)50556065ExecutionAccuracy(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL0.20.40.60.81.0Temperature(τ)102030405060char.BLEU(×100)MBR-EXECMBR-tokenBLEUMBR-charBLEUMLMALL020406080100120#Samples545658ExecutionAccuracy(×100)groupsof3-shotconcatenationof15020406080100120#Samples56575859char.BLEU(×100)groupsof3-shotconcatenationof15(a) MBPP

(b) Spider

Figure 5: Comparison between applying methods to
all possible candidates vs. applying methods to only
executable candidates (best viewed in color), where
executability-X denotes applying selection criteria X
on executable candidates only. We did not include
MBR-tokenBLEU and MALL and their combination
with executability check in this ﬁgure for clarity – full
analysis on execution vs. executability can be found in
appendix B.

post-hoc decoding may be more effective than us-
ing all available examples for all time.

4.4.3 Executability vs. Execution Results

We perform an ablation study to identify the contri-
bution of execution results vs. program executabil-
ity (Figure 5) on the MBPP and Spider datasets.9
We try to execute all candidates on the test cases,
and perform baseline candidate methods only on
the candidates that successfully execute within the
time limit. On both datasets, we ﬁnd that sim-
ply involving executablity checking signiﬁcantly
helps improve the performance of all non-semantic
feature–based selection methods; on Spider, apply-
ing ML over executable commands even outper-

9We did not include NL2bash since MBR-EXEC does not
really execute the commands. However, the comparison be-
tween MBR-EXEC and MBR-tokenBLEU in Figure 3(c) shows
that using an external bash parser as an executability estimator
leads to more consistent and generally better performance.

Figure 6: Execution accuracies with respect to sample
size on the MBPP dataset, where the number in the
parentheses denotes the number of test cases per prob-
lem used for MBR-EXEC. Best viewed in color.

forms MBR-EXEC across sample sizes.

4.4.4 Soft Loss as the Bayes Risk Function
While all the above evaluations are based on exe-
cuting one test case per problem, more test cases
can lead to more accurate judgments of semantic
equivalence between programs (Zhong et al., 2020).
Therefore, we introduce more test cases, and com-
pare (cid:96) (§3.2) with (cid:96)soft, a soft version of the loss
function, as the Bayes risk function in MBR-EXEC.
We deﬁne (cid:96)soft as follows:

(cid:96)soft(pi, pj) =

1

(cid:88)

|T |

t∈T

1 [pi(t)

= pj(t)] ,

which assesses equivalence based on the number
of test cases that receive the same output. If there
is only one test case available, (cid:96) and (cid:96)soft are equiv-
alent.

We experiment with the MBPP dataset (Fig-
ure 6) as it provides three test cases per prob-
lem. While multiple test cases clearly outperforms
MBR-EXEC with one test case across sample sizes,
we did not ﬁnd signiﬁcant difference between (cid:96)hard
and (cid:96)soft, nor between using two or three test cases.

4.5 Oracle Performance

We report the upper bound performance of all in-
ference methods (Figure 7). Here, we deﬁne the
expected Pass@K on one problem q by

ExPass@K(q)

(cid:20)

=E|P|=K

max
p∈P

min
t∈Tq

(cid:21)
1 [p(t) = G(t)]

,

where G(t) denotes the ground-truth output for test
case input t. Intuitively, to calculate the perfor-
mance upper bound, a problem q is considered to

020406080100120#Samples485052545658ExecutionAccuracy(×100)MBR-EXECexecutability-MBR-charBLEUexecutability-MLMBR-charBLEUML020406080100120#Samples556065ExecutionAccuracy(×100)MBR-EXECexecutability-MLexecutability-MBR-charBLEUMLMBR-charBLEU020406080100120#Samples555657585960ExecutionAccuracy(×100)MBR-EXEC(3)-‘MBR-EXEC(2)-‘MBR-EXEC(3)-‘softMBR-EXEC(2)-‘softMBR-EXEC(cid:54)
(a) MBPP

(b) Spider

(c) NL2Bash

Figure 7: Sample size–oracle performance curves on the considered datasets. We calculate each expected Pass@K
with 5 different sets of candidates for each sample size, while using the same sets to perform MBR-EXEC for fair
comparison.

be solved if there exists one program in the can-
didate sample set P that passes all associated test
cases
q. The dataset-level expected Pass@K is
deﬁned as the average expected Pass@K over all
problems.

T

In addition, we report the supervised perfor-
mance on these datasets, where all available train-
ing data are used for model training or ﬁnetuning:
for MBPP, the results are from Austin et al. (2021),
where they use all 374 training examples to ﬁnetune
their pretrained code model; for Spider, we com-
pare to the current state-of-the-art result (Scholak
et al., 2021); for NL2Bash, we ﬁnetune GPT-2
(Radford et al., 2019) with all training examples
with the same prompting set up as Table 1.

However, it is worth noting that the upper bounds
already outperform the state-of-the-art supervised
performances on all datasets by a signiﬁcant mar-
gin, when a reasonable amount of sample is given.
This further demonstrates the effectiveness of the
pretrained code models, and points out a potential
next step in the direction: while such models are
able to generate correct programs, designing effec-
tive inference algorithm may be a promising way
towards translating natural language to code in real
world applications.

5 Discussion

We presented and systematically analyzed MBR-
EXEC, an execution–based inference algorithm for
pretrained language to code models, on datasets
that cover three representative programming lan-
guages. Our results showed that doing execution,
even with access only to inputs (not outputs) for
test cases, or with only access to an executability
checker, substantially helps improve the quality of
generated programs especially in the settings that
use execution accuracy as the evaluation metric

(MBPP and Spider). Given the consistently strong
performance, we suggest future work on program
synthesis with large pretrained models consider
MBR-EXEC as an effective selection algorithm.
When we are not able to execute programs, or there
are no test inputs available, our results suggest con-
sidering an alternative MBR metric (e.g., MBR-
BLEU) as the selection algorithm.

All selection methods are performed on top of a
frozen pretrained code model (Codex; Chen et al.,
2021), and that incorporating execution informa-
tion into the training or ﬁnetuning process of pre-
trained models may further help improve the perfor-
mance. We leave the exploration of joint execution
and training to future work.

On the other hand, our results are relevant to the
recent discussion between meaning and form in pre-
trained language models (Bender and Koller, 2020;
Li et al., 2021). While Bender and Koller (2020)
argue that language models trained only on form
can never learn meaning,10 we present empirical
evidence that such models have implicitly learned
meaning: while the Codex model is trained with
the next-token prediction objective, i.e., trained
with only the form of code; we ﬁnd that samples
with the correct execution results (i.e., meanings)
are selected more frequently by MBR-EXEC. Re-
call that MBR-EXEC does not involve any com-
parison with ground-truth execution result in the
selection process, and therefore, the results suggest
that Codex implicitly assigns higher probability to
the programs that have desired semantics.11 Dif-

10According to the deﬁnition by Bender and Koller (2020)
and in the context of this paper, a language model learns the
meaning means that it knows the execution result without
relying on an external interpreter or compiler.

11The failure of ML does not necessarily imply that the
model assigns high probability to wrong programs. Programs
with the same semantics may have different form, where

020406080100120#Samples556065707580ExecutionAccuracy(×100)ExPass@KMBR-EXECsupervised020406080100120#Samples606570758085ExecutionAccuracy(×100)ExPass@KMBR-EXECsupervised020406080100120#Samples5560657075char.BLEU(×100)ExPass@KMBR-EXECsupervisedfering from Li et al. (2021), who train supervised
probes to match hand-designed logic forms and text
representations, MBR-EXEC requires less supervi-
sion and does not involve any training. We suggest
that future work may explore using MBR-like ap-
proaches to probe semantics implicitly learned by
generative models of code

References

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
2021. Program synthesis with large language mod-
els. arXiv preprint arXiv:2108.07732.

Emily M. Bender and Alexander Koller. 2020. Climb-
ing towards NLU: On meaning, form, and under-
In Proceedings of the
standing in the age of data.
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 5185–5198, Online. As-
sociation for Computational Linguistics.

Peter J Bickel and Kjell A Doksum. 1977. Mathemat-
ical statistics: basic ideas and selected topics, vol-
umes I-II package. HoldenDay Inc., Oakland, CA,
USA.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Evaluating large lan-
Brockman, et al. 2021.
arXiv preprint
guage models trained on code.
arXiv:2107.03374.

Li Dong and Mirella Lapata. 2018. Coarse-to-ﬁne de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 731–742, Melbourne, Australia. Association
for Computational Linguistics.

Bryan Eikema and Wilker Aziz. 2020. Is MAP decod-
ing all you need? the inadequacy of the mode in neu-
ral machine translation. In Proceedings of the 28th
International Conference on Computational Linguis-
tics, pages 4506–4520, Barcelona, Spain (Online).
International Committee on Computational Linguis-
tics.

Bryan Eikema and Wilker Aziz. 2021. Sampling-based
minimum bayes risk decoding for neural machine
translation. arXiv preprint arXiv:2108.04718.

marginalizing over the execution result may help identify the
true class of programs.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
In Proceedings of the 59th Annual Meet-
learners.
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 3816–3830, Online. Association for Computa-
tional Linguistics.

Vaibhava Goel and William J Byrne. 2000. Minimum
bayes-risk automatic speech recognition. Computer
Speech & Language, 14(2):115–135.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Man-
tas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring coding challenge
competence with APPS. In Thirty-ﬁfth Conference
on Neural Information Processing Systems Datasets
and Benchmarks Track (Round 2).

Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know? Transactions of the Association for
Computational Linguistics, 8:423–438.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-
mat boundaries with a single QA system. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020, pages 1896–1907, Online. As-
sociation for Computational Linguistics.

Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics:
HLT-NAACL 2004, pages 169–176, Boston, Mas-
sachusetts, USA. Association for Computational
Linguistics.

Marie-Anne Lachaux, Baptiste Roziere, Marc
Szafraniec, and Guillaume Lample. 2021. Dobf: A
deobfuscation pre-training objective for program-
ming languages. Advances in Neural Information
Processing Systems, 34.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.
Implicit representations of meaning in neural lan-
In Proceedings of the 59th Annual
guage models.
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 1813–1827, Online. Association for
Computational Linguistics.

Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
In
Optimizing continuous prompts for generation.
Proceedings of
the
the 59th Annual Meeting of
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
4582–4597, Online. Association for Computational
Linguistics.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Competition-level code generation with
alphacode. arXiv preprint arXiv:2203.07814.

Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer,
and Michael D. Ernst. 2018. NL2Bash: A corpus
and semantic parser for natural language interface
In Proceedings of
to the linux operating system.
the Eleventh International Conference on Language
Resources and Evaluation (LREC 2018), Miyazaki,
Japan. European Language Resources Association
(ELRA).

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Koˇciský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
599–609, Berlin, Germany. Association for Compu-
tational Linguistics.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
arXiv preprint arXiv:2107.13586.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, MING GONG, Ming Zhou, Nan Duan, Neel
Sundaresan, Shao Kun Deng, Shengyu Fu, and Shu-
jie LIU. 2021. CodeXGLUE: A machine learning
benchmark dataset for code understanding and gen-
In Thirty-ﬁfth Conference on Neural In-
eration.
formation Processing Systems Datasets and Bench-
marks Track (Round 1).

Antonio Valerio Miceli Barone and Rico Sennrich.
2017. A parallel corpus of python functions and
documentation strings for automated code documen-
In Proceedings of the
tation and code generation.
Eighth International Joint Conference on Natural
Language Processing (Volume 2: Short Papers),
pages 314–319, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2021. Noisy channel language
model prompting for few-shot text classiﬁcation.
arXiv preprint arXiv:2108.04106.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1139–
1149, Vancouver, Canada. Association for Computa-
tional Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog, 1(8):9.

Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. PICARD: Parsing incrementally for
constrained auto-regressive decoding from language
models. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 9895–9901, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen
Livescu. 2019. Visually grounded neural syntax ac-
quisition. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1842–1861, Florence, Italy. Association for
Computational Linguistics.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,
Eric Wallace, and Sameer Singh. 2020. AutoPrompt:
Eliciting Knowledge from Language Models with
In Proceed-
Automatically Generated Prompts.
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4222–4235, Online. Association for Computational
Linguistics.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
pages 2238–2249, New Orleans, Louisiana. Associ-
ation for Computational Linguistics.

Ivan Titov and James Henderson. 2006. Bayes risk
minimization in natural language parsing. Univer-
sity of Geneva technical report.

Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
620–629, Honolulu, Hawaii. Association for Com-
putational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems, 30.

Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic
evaluation for text-to-SQL with distilled test suites.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 396–411, Online. Association for Computa-
tional Linguistics.

Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [MASK]: Learning vs. learning
to recall. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 5017–5033, Online. Association for
Computational Linguistics.

Appendices

A Example Prompts and Codex API

Responses

We include example 3-shot prompts and corre-
sponding Codex responses that we used in our ex-
periments, on the three datasets (Tables 4, 5, 6),
where we format the prompts following the pat-
terns presented in Table 1. Data shown in the tables
are collected with the greedy decoding strategy (i.e.,
temperature = 0), and can be found in the ﬁrst line
of seed 0 in our released data for each test dataset.

B Full Analysis on Executability vs.

Execution Result

We report
the comparison between MBR-
tokenBLEU and MALL vs. their combination with
executability check (Figure 8; in complementary to
Figure 5), where we observe that an executability
checker is an effective ﬁlter to improve execution
accuracies for both datasets (MBPP and Spider).

Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-
ral text generation with unlikelihood training.
In
International Conference on Learning Representa-
tions.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
In Proceedings of the 54th An-
mantic parsing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1341–
1350, Berlin, Germany. Association for Computa-
tional Linguistics.

Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,
Torsten Scholak, Michihiro Yasunaga, Chien-Sheng
Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al.
2022. Uniﬁedskg: Unifying and multi-tasking struc-
tured knowledge grounding with text-to-text lan-
guage models. arXiv preprint arXiv:2201.05966.

Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan
Vasilescu, and Graham Neubig. 2020. Incorporating
external knowledge through pre-training for natural
language to code generation. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 6045–6052, Online. As-
sociation for Computational Linguistics.

Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, and Graham Neubig. 2018. Learning to
mine aligned code and natural language pairs from
In 2018 IEEE/ACM 15th interna-
stack overﬂow.
tional conference on mining software repositories
(MSR), pages 476–486. IEEE.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Irene Li,
Dongxu Wang, Zifan Li, James Ma,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018.
Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911–3921, Brussels, Belgium. Association for
Computational Linguistics.

Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
BARTScore: Evaluating generated text as text gen-
eration. In Advances in Neural Information Process-
ing Systems.

Hao Zhang and Daniel Gildea. 2008. Efﬁcient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of ACL-08: HLT, pages 209–
217, Columbus, Ohio. Association for Computa-
tional Linguistics.

Yu Zhang, Zhenghua Li, and Min Zhang. 2020. Efﬁ-
cient second-order TreeCRF for neural dependency
In Proceedings of the 58th Annual Meet-
parsing.
ing of the Association for Computational Linguistics,
pages 3295–3305, Online. Association for Computa-
tional Linguistics.

MBPP: Prompt

<info>assert camel_to_snake(’GoogleAssistant’) == ’google_assistant’</info>
<text>Write a function to convert camel case string to snake case string by

using regex.</text>

<code>import re
def camel_to_snake(text):

str1 = re.sub(’(.)([A-Z][a-z]+)’, r’\\1_\\2’, text)
return re.sub(’([a-z0-9])([A-Z])’, r’\\1_\\2’, str1).lower()</code>

<info>assert sort_dict_item({(5, 6) : 3, (2, 3) : 9, (8, 4): 10, (6, 4): 12} )

== {(2, 3): 9, (6, 4): 12, (5, 6): 3, (8, 4): 10}</info>

<text>Write a function to sort dictionary items by tuple product of keys for the

given dictionary with tuple keys.</text>

<code>def sort_dict_item(test_dict):

res = {key: test_dict[key] for key in sorted(test_dict.keys(), key = lambda ele

: ele[1] * ele[0])}

return (res)

</code>
<info>assert reverse_list_lists([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12],
[13, 14, 15, 16]])==[[4, 3, 2, 1], [8, 7, 6, 5], [12, 11, 10, 9], [16, 15,
14, 13]]</info>

<text>Write a function to reverse each list in a given list of lists.</text>
<code>def reverse_list_lists(lists):

for l in lists:

l.sort(reverse = True)

return lists </code>

<info>assert remove_Occ(\"hello\",\"l\") == \"heo\"</info>
<text>Write a python function to remove first and last occurrence of a given

character from the string.</text>

<code>

MBPP: Response

def remove_Occ(str1,ch):

return str1[:str1.index(ch)] + str1[str1.rindex(ch)+1:]</code>

Table 4: MBPP example prompt and response from Codex: we use the ﬁrst assertion in the dataset as the extra
information (i.e., [INFO] in Table 1). The content in the last <info>...</info> and <text>...</text>
marks in the prompt corresponds to the test problem.

Spider: Prompt

<info>e_learning | * | Course_Authors_and_Tutors : author_id , author_tutor_ATB ,

login_name , password , personal_name , middle_name , family_name ,

gender_mf , address_line_1 | Students : student_id , date_of_registration ,
date_of_latest_logon , login_name , password , personal_name , middle_name ,
family_name | Subjects : subject_id , subject_name | Courses : course_id ,
author_id , subject_id , course_name , course_description |
Student_Course_Enrolment : registration_id , student_id , course_id ,
date_of_enrolment , date_of_completion | Student_Tests_Taken :
registration_id , date_test_taken , test_result</info>

<text>Which course authors teach two or more courses? Give me their addresses

and author IDs.</text>

<code>SELECT T1.address_line_1 , T2.author_id FROM Course_Authors_and_Tutors AS

T1 JOIN Courses AS T2 ON T1.author_id = T2.author_id GROUP BY T2.author_id
HAVING Count(*) >= 2</code>

<info>flight_1 | * | flight : flno , origin , destination , distance ,

departure_date , arrival_date , price , aid | aircraft : aid , name ,
distance | employee : eid , name , salary | certificate : eid , aid</info>
<text>Show origin and destination for flights with price higher than 300.</text>
<code>SELECT origin , destination FROM Flight WHERE price > 300</code>
<info>driving_school | * | Addresses : address_id , line_1_number_building ,

city , zip_postcode , state_province_county , country | Staff : staff_id ,
staff_address_id , nickname , first_name , middle_name , last_name ,
date_of_birth , date_joined_staff , date_left_staff | Vehicles : vehicle_id ,

vehicle_details | Customers : customer_id , customer_address_id ,

customer_status_code , date_became_customer , date_of_birth , first_name ,
last_name , amount_outstanding , email_address , phone_number ,
cell_mobile_phone_number | Customer_Payments : customer_id , datetime_payment
, payment_method_code , amount_payment | Lessons : lesson_id , customer_id ,
lesson_status_code , staff_id , vehicle_id , lesson_date , lesson_time ,

price</info>

<text>When did the staff member with first name as Janessa and last name as

Sawayn leave the company?</text>

<code>SELECT date_left_staff FROM Staff WHERE first_name = \"Janessa\" AND

last_name = \"Sawayn\";</code>

<info>concert_singer | * | stadium : Stadium_ID , Location , Name , Capacity ,

Highest , Lowest , Average | singer : Singer_ID , Name , Country , Song_Name
, Song_release_year , Age , Is_male | concert : concert_ID , concert_Name ,
Theme , Stadium_ID , Year | singer_in_concert : concert_ID , Singer_ID</info>

<text>How many singers do we have?</text>
<code>

Spider: Response

SELECT COUNT(*) FROM singer;</code>

Table 5: Spider example prompt and response from Codex: following Xie et al. (2022), we use the concatenation
of corresponding table and column names in the dataset as the extra information (i.e., [INFO] in Table 1).. The
content in the last <info>...</info> and <text>...</text> marks in the prompt corresponds to the
test problem.

NL2Bash: Prompt

<text>Print file information of command \"bash\"</text>
<code>echo $(ls -l $(which bash))</code>
<text>Recursively change the owner and group of all files in \"/your/directory/

to/fuel/\" to \"nginx\"</text>

<code>chown nginx:nginx /your/directory/to/fuel/ -R</code>
<text>Copy \"src/prog.js\" and \"images/icon.jpg\" to \"/tmp/package\" keeping

relative path names</text>

<code>rsync -R src/prog.js images/icon.jpg /tmp/package</code>
<text>Adds execution permissions on a script ./etc/bash_completion within

Homebrew home folder path.</text>

<code>

NL2Bash: Response

chmod +x /usr/local/etc/bash_completion</code>

Table 6: NL2Bash example prompt and response from Codex: we did not use any extra information. The content
in the last <text>...</text> marks in the prompt corresponds to the test problem.

(a) MBPP (MBR-tokenBLEU)

(b) MBPP (MALL)

(c) Spider (MBR-tokenBLEU)

(d) Spider (MALL)

Figure 8: Comparison between applying methods to
all possible candidates vs. applying methods to only
executable candidates (best viewed in color), where
executability-X denotes applying selection criteria X
on executable candidates only. We also include the
curves of MBR-EXEC for comparison.

020406080100120#Samples485052545658ExecutionAccuracy(×100)MBR-EXECexecutability-MBR-tokenBLEUMBR-tokenBLEU020406080100120#Samples5052545658ExecutionAccuracy(×100)MBR-EXECexecutability-MALLMALL020406080100120#Samples556065ExecutionAccuracy(×100)MBR-EXECexecutability-MBR-tokenBLEUMBR-tokenBLEU020406080100120#Samples404550556065ExecutionAccuracy(×100)MBR-EXECexecutability-MALLMALL