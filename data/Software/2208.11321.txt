2
2
0
2

g
u
A
4
2

]

G
L
.
s
c
[

1
v
1
2
3
1
1
.
8
0
2
2
:
v
i
X
r
a

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

Mengdi Zhang
mdzhang.2019@phdcs.smu.edu.sg
Singapore Management University
Singapore

Jingyi Wang
wangjyee@zju.edu.cn
Zhejiang University
China

Jun Sun
junsun@smu.edu.sg
Singapore Management University
Singapore

Bing Sun
bing.sun.2020@phdcs.smu.edu.sg
Singapore Management University
Singapore

ABSTRACT
Discrimination has been shown in many machine learning applica-
tions, which calls for sufficient fairness testing before their deploy-
ment in ethic-relevant domains such as face recognition, medical
diagnosis and criminal sentence. Existing fairness testing approaches
are mostly designed for identifying individual discrimination, i.e.,
discrimination against individuals. Yet, as another widely concerning
type of discrimination, testing against group discrimination, mostly
hidden, is much less studied. To address the gap, in this work, we
propose TESTSGD, an interpretable testing approach which sys-
tematically identifies and measures hidden (which we call ‘subtle’)
group discrimination of a neural network characterized by condi-
tions over combinations of the sensitive features. Specifically, given
a neural network, TESTSGD first automatically generates an inter-
pretable rule set which categorizes the input space into two groups
exposing the model’s group discrimination. Alongside, TESTSGD
also provides an estimated group fairness score based on sampling
the input space to measure the degree of the identified subtle group
discrimination, which is guaranteed to be accurate up to an error
bound. We evaluate TESTSGD on multiple neural network mod-
els trained on popular datasets including both structured data and
text data. The experiment results show that TESTSGD is effective
and efficient in identifying and measuring such subtle group dis-
crimination that has never been revealed before. Furthermore, we
show that the testing results of TESTSGD can guide generation of
new samples to mitigate such discrimination through retraining with
negligible accuracy drop.

ACM Reference Format:
Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun. 2022. TESTSGD: In-
terpretable Testing of Neural Networks Against Subtle Group Discrimination
. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

INTRODUCTION

1
Machine learning models, especially neural networks, are becoming
ubiquitous in various real-life applications. For example, they are
used in medical diagnosis [27], self-driving cars [10] and criminal
sentencing [6]. Meanwhile, more and more attention has been paid
to the fairness issues of these machine learning models [5, 12, 17, 18,
37, 38, 44, 50, 51] as discrimination has been discovered in many
applications [19, 37, 42, 49]. For instance, machine learning mod-
els were used to predict recidivism risk for suspected criminals by
computing the likelihood of committing a future crime [6]. Analysis
results show that the prediction model was more likely to mislabel
black defendants as high recidivism risk and mislabel white defen-
dants as low risk. To minimize such ethical risks, it is crucial to
systematically test the fairness of machine learning models, espe-
cially neural networks where such issues are typically ‘hidden’ due
to the lack of interpretability [33, 40].

Recently, multiple efforts have been made in the testing commu-
nity to first search for (and then guide mitigating) discrimination
of machine learning models spanning from traditional ones to neu-
ral networks [17, 42, 49–51]. For instance, state-of-the-art fairness
testing work utilizes gradient information of the input sample to
accelerate search/generation of discriminative samples [4, 50, 51].
Despite being effective, existing research has mostly focused on in-
dividual discrimination, i.e., identifying or generating individual dis-
criminatory instances of a machine learning model [17, 42, 49–51].
Group discrimination, which characterizes a model’s discrimination
against a certain group (whose sensitive features1 satisfy certain
conditions), is another concerning type of discrimination, which
has been widely studied [17, 26, 41, 49]. However, testing against
group discrimination has been much less studied so far. Compared
to testing of individual discrimination, testing a machine learning
model against group discrimination imposes new challenges. First,
it is highly non-trivial to effectively enumerate all combinations of
sensitive features (especially when the sensitive features have multi-
ple or even continuous values). Second, group discrimination can
be hidden, i.e., there might be ‘subtle’ group discrimination against
those groups whose sensitive features satisfy certain unknown condi-
tions, e.g., male-white of certain age group. While a prior work [25]
similarly addresses discrimination against subgroups defined over
conjunctions of protected features in the learning phase, we propose

1we use “feature”/“attribute” interchangeably

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

an automatic testing approach to systematically identify such sub-
groups using interpretable rules and measure such discrimination
before model deployment.

Specifically, in this work, we develop an effective method to
systematically test a given machine learning model against such hid-
den subtle group discrimination, namely TESTSGD. An overview
of TESTSGD is shown in Figure 1, which consists of three main
phases: 1) candidate rule set generation, 2) group fairness identifica-
tion, and 3) discrimination mitigation. In the first phase, TESTSGD
will automatically generate a candidate set of rules concerning mul-
tiple sensitive features. Note that we only consider frequent rule
set with sufficient support (which characterize a sufficiently large
group). In the second phase, the rule set R effectively partitions the
samples into two groups, i.e., 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 which satisfies the rules and
𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 which does not. The key intuition behind is to develop
effective criteria to automatically mine interpretable rules which
are practical and relevant in the real-world applications. Then we
measure if the model suffers from group discrimination (against the
groups partitioned by the rule set) by measuring the group fairness
score. Note that, solely relying on the training samples might not
be enough to accurately measure such a score. We thus propose to
apply a standard data augmentation method, i.e., imposing minor
perturbation on the available seed samples to generate new sam-
ples, and obtain an accurate estimation of the group fairness score
(with bounded errors). The testing results of the first two phases are
thus the identified subtle group discrimination (characterized by the
rules) and their corresponding group fairness score (with bounded
errors). For example, we test the model trained on the Crime [34]
dataset which predicts whether the violent crimes per population
in a specific community is high. The interpretable rule set found
by TESTSGD shows that it discriminates against communities in
which the percentage of Caucasian population is lower than 80% and
the percentage of females who are divorced is higher than 40%, with
a 60.7% group fairness score, i.e., it is 60.7% more likely to predict
high crime rate for such a community. In the last phase (optional
depending whether the identified discrimination is considered to be
harmful), TESTSGD leverages the testing results to mitigate the
identified subtle group discrimination. That is, to improve group
fairness, we generate new samples according to the condition under
which discrimination exists and retrain the original model.

TESTSGD is implemented as an open-source software [43]. We
evaluate our TESTSGD on 8 models trained on widely adopted
datasets including both structured data and text data. The experimen-
tal results show TESTSGD is effective in identifying and measuring
subtle group discrimination. The results also show that subtle group
discrimination does exist in all of these 8 models and sometimes to a
surprising level which has never been revealed before. For instance,
the model trained on the COMPAS [6] dataset is much less likely
to predict Hispanic males older than 40 years old as criminals with
high recidivism risk. Furthermore, our experiments show that the
testing-guided discrimination mitigation is useful. That is, we can
mitigate identified subtle group discrimination for all models without
sacrificing the accuracy.

In a nutshell, we summarize our main contributions as follows.

• We propose a method to automatically generate an interpretable
rule set to identify subtle group discrimination in neural networks,
applicable for both structured and text data;

• We develop a theoretical bound for accurately sampling and esti-

mating the group fairness score against two groups.

• We show that we can generate samples systematically based on
the interpretable rule set to mitigate subtle group discrimination.
The remainder of this paper is structured as follows. Section 2
provides the background on input types and fairness definitions.
Section 3 defines our problem. We present the proposed TESTSGD
framework in Section 4, which is evaluated in Sections 5. Lastly, we
review related work in Section 6 and conclude in Section 7.

2 BACKGROUND
Our goal is to develop a black-box method to identify subtle group
discrimination in a user-provided neural network model. Our method
supports neural networks trained on two different kinds of data, i.e.,
structure data and text data. Our method does not require the inner
details of the neural network. That is, the neural network is viewed
as a function 𝑀 : 𝑅𝑝 → 𝑅𝑞 which maps an input 𝑥 ∈ 𝑅𝑝 to an
output 𝑦 ∈ 𝑅𝑞. Furthermore, we focus on deep feed-forward neural
networks and recurrent neural networks.

Input Type

2.1
First of all, we define two different data, i.e., structure data, text data,
and their corresponding sensitive features which are used to evaluate
the discrimination of the neural networks.

A sample of structured dataset is composed of a set of features,
i.e., a feature vector. A feature can be categorical (i.e., with a fixed
set of values) or continuous (i.e., with a certain value range). We
define the structure data and the corresponding sensitive features as
follows.

Definition 2.1 (Structured Data). A structured data 𝑥 contains
𝑁 features {𝑥1, 𝑥2, · · · , 𝑥𝑁 }, where ∀𝑥𝑖, 𝑥𝑖 ∈ 𝐿𝑖 , where 𝐿𝑖 is a set
of feature values. We write 𝑆 = {𝑠1, 𝑠2, · · · , 𝑠𝑛 } to denote the set of
sensitive features in 𝑥, where 𝑛 < 𝑁 .

The text data is composed of a set of tokens. We define the
sensitive feature of text data based on the presence of sensitive terms.
Note that there could be different categories of sensitive terms, e.g.,
terms referring to race, religion, or ethnicity. We define the text data
and the corresponding sensitive features as follows.

Definition 2.2 (Text Data). A text data 𝑥 contains a sequence of
tokens {𝑥1, 𝑥2, · · · , 𝑥𝑁 }. We write 𝑆 = {𝑠1, 𝑠2, · · · , 𝑠𝑛 } to denote a
set of categories of sensitive terms, where 𝑛 < 𝑁 and 𝑇 to denote a
set of sensitive terms {𝑡1, 𝑡2, · · · , 𝑡𝑘 }, where 𝑡 𝑗 ∈ 𝑠𝑖 for some 𝑖, for
all 𝑗 ∈ [1, 𝑘] and 𝑡 𝑗 ∈ 𝑥 .

2.2 Fairness Definitions
To define our problem, we define fairness and the concept of group
fairness score. There are multiple definitions of fairness [13, 15,
17, 24, 26, 49]. Here, we briefly review two well-studied fairness
definitions, i.e., individual fairness and group fairness.

Individual fairness focuses on specific pairs of individuals. Intu-
itively, individual discrimination (unfairness) occurs when two indi-
viduals that differ by only certain sensitive feature (such as gender

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

Conference’17, July 2017, Washington, DC, USA

Figure 1: An Overview of TESTSGD.

or race) are treated differently, i.e., with a different predicted label.
This notion is widely used to search discriminatory instances which
differ only in those sensitive characteristics [42, 50, 51]. There are
also plenty of works on learning models which are more likly to
avoid individual discrimination [37].

Group fairness, also known as statistical fairness, focuses on sensi-
tive groups such as ethnic minority and the parity across different
groups based on some statistical measurements [8, 17, 26, 41, 49].
A classifier satisfies this metric if the samples in the sensitive group
have a positive classification probability or true positive probability
that is similar with or equal to that of the insensitive group.

In this work, we focus on group fairness for its relevance in
many neural network applications. In the following, we provide a
formal definition of group fairness based on positive classification
rate measurement.

Definition 2.3. Let 𝑀 be a neural network model; 𝑙 be a (favor-
able) prediction; and 𝜉 be a positive constant. Let 𝐺 be a group
identified by certain condition 𝜙 on sensitive features 𝑆. 𝐺 can be
defined as a set of samples {𝑥 |𝑥 ⊨ 𝜙 }, where 𝑥 ⊨ 𝜙 means x satisfies
condition 𝜙. We say 𝑀 satisfies group fairness, with respect to 𝜉 and
𝐺, if and only if

| 𝑃 (𝑀 (𝑥) = 𝑙 | 𝑥 ∈ 𝐺) − 𝑃 (𝑀 (𝑥) = 𝑙 | 𝑥 ∉ 𝐺) | ≤ 𝜉

(1)

Note that, in some cases, the model may be fair overall but unfair
under some specific ‘subtle’ conditions. For example, the model is
fair considering gender attribute if it approves half of the loans from
female or male applicants. However, when we consider both gender
and race, the model may show discrimination. For example, it ap-
proves loans for far less a percentage of Hispanic female individual,
compared to the remaining group. In this setting, we say that the
model discriminates against Hispanic females (if we show that the
testing results have sufficient statistical confidence).

3 PROBLEM DEFINITION
Our problem is to develop a systematic method for identifying subtle
group discrimination. That is, given a neural network model 𝑀 (as
well as a constant threshold 𝜉, we aim to generate a condition 𝜙
such that 𝑀 is unfair with respect to the group identified by 𝜙. The
condition 𝜙 must satisfy the following conditions. (1) It must be
constituted by variables representing sensitive features. (2) It must
be human-interpretable, so that our analysis result can be presented
for human decision making. (3) It must identify a group of non-
trivial size. In addition, our method must support both structured
data as well as text data. Furthermore, we would like our method to
generate results with certain correctness guarantee, e.g., the chance
of reporting non-existing discrimination is low.

Inspired by rule-based models, which are widely used to learn
interpretable models [29, 35], we generate 𝜙 in the form of rules
(a.k.a. constraints) which are understandable by human beings and
also concrete enough to show model prediction differences between
different groups. The rules are constituted by the input features, with-
out relying on any latent variables or representations. We define 𝜙 to
be the conjunction of one or more rules, each of which is constituted
by only one sensitive feature. Furthermore, to limit the search space
as well as to make sure the generated rules are interpretable, we
limit each rule on continuous features to be of the form of a linear
inequality, e.g., 𝑎𝑔𝑒 ≥ 30 is a possible rule but 𝑎𝑔𝑒 𝑖𝑠 𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑒𝑠 𝑜 𝑓 7
is not.

In order to make sure that the discrimination that we discover
is highly likely in the actual system, we propose a sampling based
approach to estimate the probability of predicting certain label within
a given group. Such a method allows us to generate an estimation
with certain level of statistical confidence, i.e., with a bound on the
error. Note that it is not straightforward to adopt existing techniques
such as hypothesis testing [45, 46]. This is because the group fairness
score is the difference between two estimations (i.e., one for the

DatasetRule Generationif𝑠𝑢𝑝𝑝𝑜𝑟𝑡𝑅≥𝑠𝑢𝑝_𝑡ℎ𝑟Rule setNeural Network𝑠𝑒𝑒𝑑𝑠!𝑠𝑎𝑚𝑝𝑙𝑒𝑠!𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬!𝑅𝑠𝑒𝑒𝑑𝑠¬!Sampling𝜙!𝜙¬!𝑖𝑓𝑀𝑎𝑟𝑔𝑖𝑛𝑜𝑓𝑒𝑟𝑟𝑜𝑟≤𝑡ℎ𝑟𝑒sholdFairness scorePhase1:Rule SetGenerationInput SamplingPhase2:Group FairnessIdentificationelseelsePhase3:MitigationEnhancedNNConference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

individual in the group and the other for those not in the group). We
solve this problem by establishing a conservative error bound on the
difference based on the error bounds for the two estimations.

Table 1: Identity Sensitive Terms

Sensitive Features

Identity Terms

4 METHODOLOGY
In this section, we describe the details of our approach. There are
two main steps, i.e., learning a rule set and identifying group dis-
crimination based on the learned rule set. The inputs for our method
include a machine learning model 𝑀, its training set 𝐷, and a set of
sensitive features 𝑆. The output is the subtle group discrimination
represented as a rule set characterizing the discriminated group and
the corresponding group fairness score.

4.1 Generating Frequent Rule Sets
To identify discrimination against certain group, we first need a way
of characterizing a group. In this work, we characterize the groups
based on a set of rules, each of which constrains one sensitive feature.
In the following, we present how we generate rules for sensitive
features of both structured data and text data.

In terms of categorical features 𝑥𝑖 in structured data such as
gender or race, in general, a rule can be defined as a subset of the
possible feature values 𝐿𝑖 . For instance, given the sensitive feature of
race which has five values, i.e., Caucasian, Black, Hispanic, Asian
and other-race, a rule can be any set containing one to four of these
five values.In total, we have 30 rules. For continuous features 𝑥𝑖 in
structure data such as age or percentage, there may be too many
possible values to enumerate, i.e., the domain of 𝐿𝑖 is too large. Thus
we apply techniques such as binning to turn continuous features
into categorical features. Here, we divide the original value range
into 𝐾 intervals with equal width. Then we consider each interval
as a single value and consider a set containing adjacent values as a
rule. We set 𝐾 as 10 in our experiments. For example, we divide age
attribute ranging from 0 to 100 into 10 equal intervals.

For textual dataset, defining rules is not that straightforward. In
this work, we define the rules based on the presence of sensitive
terms 𝑇 (refer to Definition 2.2). For each sensitive category 𝑠, we
define a rule which intuitively means that the text sample contains a
term 𝑡, where 𝑡 ∈ 𝑠. In this work, we use a set of 48 terms created
in [14] as the sensitive terms which can be classified into 4 categories,
i.e., gender, race, religion and age. The sensitive terms are shown in
Table 1. For example, when we consider the gender feature for text
dataset, there are 14 sensitive terms and thus 14 rules are generated.
Once a set of rules are identified, we then characterize a group
based on a rule set. Each element of a rule set is a rule constraining
one sensitive feature. Intuitively, a rule set partitions the input space
of 𝑀 into two disjoint groups, i.e., those who satisfy all the rules
in the rule set and the rest. If these two groups have a significant
different probability of being predicted favorably by the model 𝑀,
we successfully identify a subtle discrimination.

Note that a naive approach is to enumerate all possible rules
based on one sensitive feature and combine them arbitrarily. Such
an approach is both infeasible and undesirable. First, there can be
enormous combinations of the rules. Second, not all rule sets are
interesting. For instance, a rule set may be {𝑎𝑔𝑒 ≥ 100, 𝑔𝑒𝑛𝑑𝑒𝑟 =
𝑀𝑎𝑙𝑒}. A discrimination found against the group identified by this
rule set is likely to be due to the limited data. Furthermore, the

gender

race

religion

age

lesbian, gay, bisexual, transgender, trans,
queer, lgbt, lgbtq, homosexual, straight,
heterosexual, male, female, nonbinary
african, african american, black, white,
european, hispanic, latino, latina, latinx,
mexican, canadian, american, asian, indian,
middle eastern, chinese, japanese
christian, muslim, jewish, buddhist, catholic,
protestant, sikh, taoist, atheist
old, older, young, younger, teenage,
millennial, middle aged, elderly

discrimination is perhaps not as concerning as discrimination against
groups that represent a sizable population.

We thus only consider frequent rule sets among all possible com-
binations of rules. A frequent rule set is a set of rules that are satisfied
by a group with a size more than certain threshold. Formally, given
a rule set 𝑅, the support for 𝑅 is the frequency of the number of
samples that satisfy all rules in rule set 𝑅. Given a support threshold
𝜃 (i.e., a percentage), we say that 𝑅 is frequent if its support is no
less than 𝜃 . In the following, we present how to identify a set of
frequent rule sets for structured and text data.

For each sensitive feature 𝑠, let 𝑅𝑠 be the set of rules concerning
𝑠. A rule set 𝑅 is composed of rules for each sensitive feature, i.e.,
𝑅 = {𝑟𝑠1, 𝑟𝑠2, ..., 𝑟𝑠𝑛 } where 𝑟𝑠𝑖 ∈ {𝑅𝑠𝑖 ∪ ∅} and 𝑅 ≠ ∅. 𝑅 is
frequent if and only if 𝑠𝑢𝑝𝑝𝑜𝑟𝑡 (𝑅) ≥ 𝜃 where 𝑠𝑢𝑝𝑝𝑜𝑟𝑡 (𝑅) is defined
as follows.

support(𝑅) =

#{𝑑 ∈ 𝐷 |∀𝑟 ∈ 𝑅. 𝑑 ⊨ 𝑟 }
#𝐷

(2)

where #𝑋 of a set 𝑋 is the number of elements in 𝑋 ; and 𝑑 ⊨ 𝑟
means that 𝑑 satisfies 𝑟 .

Example Consider the structured dataset Census Income [36]. It has
three sensitive features, i.e., gender, race, and age. Each feature has
a set of values. The following constitutes a rule set

{𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑀𝑎𝑙𝑒, 𝑟𝑎𝑐𝑒 = 𝑊 ℎ𝑖𝑡𝑒, 40 ≤ 𝑎𝑔𝑒 < 60}

□
Rule sets for text data are defined differently. Recall that each rule
is a proposition on whether the text contains certain sensitive term.
Formally, given the set of the categories of sensitive terms 𝑆, a rule
set 𝑅 is then a set of sensitive terms {𝑟1, 𝑟2, · · · , 𝑟𝑚 }, where 𝑟𝑘 ∈ 𝑠𝑖
for some 𝑖, for all 𝑘 ∈ [1, 𝑚] and 𝑚 ≤ 𝑛. The support of 𝑅 is defined
as follows.

support(𝑅) =

#{𝑑 ∈ 𝐷 |∀𝑟 ∈ 𝑅. 𝑐𝑜𝑛𝑡𝑎𝑖𝑛𝑠 (𝑑, 𝑠𝑟 )}
#𝐷

(3)

where 𝑠𝑟 is the sensitive category referring to 𝑟 and 𝑐𝑜𝑛𝑡𝑎𝑖𝑛𝑠 (𝑑, 𝑠𝑟 )
is a proposition which is true if and only if 𝑑 contains at least one
term in the category 𝑠𝑟 .

Example Consider the text dataset Wikipedia Talk Pages [47]. We
have two categories of sensitive terms, e.g., gender and race. For
each category, we have a set of corresponding sensitive terms as

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

Conference’17, July 2017, Washington, DC, USA

Algorithm 1 𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑡𝑅𝑢𝑙𝑒𝑆𝑒𝑡𝑠 (𝐷, 𝑆, 𝑠𝑢𝑝_𝑡ℎ𝑟 ) where 𝐷 is the train-
ing set, 𝑆 is the sensitive attributes, 𝜃 is the support threshold
1: 𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒𝑠 ← {}, 𝑟𝑢𝑙𝑒_𝑠𝑒𝑡𝑠 ← ∅
2: for each 𝑠 in 𝑆 do
3:

𝑟𝑢𝑙𝑒𝑠 ← {𝑟1, 𝑟2, ...}
𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒 [𝑠] = 𝑟𝑢𝑙𝑒𝑠

4:
5: end for
6: 𝑎𝑙𝑙_𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒𝑠 ← {𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒 [𝑠] ∪ ∅} for all 𝑠 ∈ 𝑆
7: 𝑟𝑢𝑙𝑒_𝑠𝑒𝑡𝑠 ← 𝑐𝑜𝑚𝑏𝑖𝑛𝑎𝑡𝑖𝑜𝑛𝑠 (𝑎𝑙𝑙_𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒𝑠)
8: 𝑎𝑙𝑙_𝑟𝑢𝑙𝑒_𝑠𝑒𝑡𝑠 ← {𝑅 𝑓 𝑜𝑟 𝑅 𝑖𝑛 𝑟𝑢𝑙𝑒_𝑠𝑒𝑡𝑠 𝑖 𝑓 𝑠𝑢𝑝𝑝𝑜𝑟𝑡 (𝐷, 𝑅) ≥ 𝜃 }
9: return 𝑎𝑙𝑙_𝑟𝑢𝑙𝑒_𝑠𝑒𝑡𝑠

shown in Table 1. The following constitutes a rule set

{“𝑏𝑖𝑠𝑒𝑥𝑢𝑎𝑙”, “𝐶𝑎𝑢𝑐𝑎𝑠𝑖𝑎𝑛”}

□
Algorithm 1 shows the exact steps in generating all possible rule
sets. At line 1, we first initialize a dictionary 𝑠𝑖𝑛𝑔𝑙𝑒_𝑟𝑢𝑙𝑒𝑠 and an
empty set 𝑟𝑢𝑙𝑒𝑠_𝑠𝑒𝑡𝑠. During the loop from line 2 to 5, we generate
all possible 1-feature rules for each sensitive feature as discussed
above. At line 6, we generate a set of all 1-feature rules. Then, we
generate all possible rule sets at line 7. Lastly, at line 8, we only
keep those rule sets that have a support value no less than 𝜃 .

In general, given a dataset with 𝐾 sensitive features, and at most
𝑁 rules for each sensitive feature, the number of rule sets is 𝑁 𝐾 in
the worse case. For example, we have 2 gender-related single rules, 5
race-related 1-feature rules and 10 age-related 1-feature rules, there
are 17 rule sets when considering one sensitive attribute, 80 rule sets
when considering two sensitive attributes and 100 rule sets when
considering all sensitive attributes. So in total, there are 197 possible
rule sets.

Identifying Group Fairness

4.2
For each group identified by a rule set, we then measure the discrim-
ination against the group. That is, we aim to compute the probability
of predicting certain label by 𝑀 on those samples in the group, and
that on those samples not in the group, and measure the difference.
The score is the group fairness score, which varies from 0 (i.e., no
difference) to 1 (i.e., completely different). Formally,

Definition 4.1 (Group fairness score). Let 𝑅 be a rule set. Let 𝑙 be
a (favorable) label. The group fairness score with respect to 𝑅 and 𝑙
is |𝑝𝑟𝑜𝑏 (𝑅, 𝑙) − 𝑝𝑟𝑜𝑏 (¬𝑅, 𝑙)|, where 𝑝𝑟𝑜𝑏 (𝑅, 𝑙) is the probability of
predicting 𝑙 given samples satisfying 𝑅, ¬𝑅 identifies samples not
satisfying 𝑅.

We remark that this definition is similar to the CV score [13] and
multivariate group discrimination score [17]. However, the former is
limited to binary input types and the latter is limited to categorical
input types. In comparison, our fairness score supports both struc-
tured data and text data.

Example Take a model trained on the (structured) Census Income
dataset as an example. The model predicts whether the income of
an adult is above $50,000 annually, i.e., “True” means above the
threshold and “False” means otherwise. Assume the rule set is

Assume that the model predicts 28% of individuals in this group
with “True”, and 10% of the remaining population with “True”. The
model’s group fairness score, with respect to the rule set and the
□
prediction, is 18%.

Given a rule set, measuring the group fairness score requires us to
measure 𝑝𝑟𝑜𝑏 (𝑅, 𝑙) and 𝑝𝑟𝑜𝑏 (¬𝑅, 𝑙), which is non-trivial since ex-
haustively enumerating all samples is infeasible due to the enormous
input space. On the other hand, measuring it based on a limited
number of samples may yield inaccurate results. In the following,
we propose an approach to compute group fairness scores with a
statistical confidence guarantee. Formally, we would like to measure
the group fairness score 𝑓 within a margin of error 𝜖 under a certain
confidence 𝛿, such that 𝑝𝑟𝑜𝑏 (|𝑓 − ^𝑓 | > 𝜖) < 1 − 𝛿, where ^𝑓 is the
real group fairness score over all possible samples.

Algorithm 2 shows how we measure the group fairness score. We
maintain two sets of samples, i.e., 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 which contains samples
satisfying 𝑅 and 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 which contains samples not satisfying
𝑅. At line 1, we set both 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 and 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 to be empty, error
margin 𝜖 to be infinity and the number of generated samples as
0. During the loop from line 2 to 16, we keep generating samples
and calculating group fairness score until the error margin 𝜖 is no
more than the given error threshold 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 . From line 3 to line 6,
we generate new samples for 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 and 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 respectively
using a function 𝑆𝑎𝑚𝑝𝑙𝑒. We remark that the generated samples
should follow the original data distribution (i.e., that of the training
dataset). We present details on how we sample on structured and
text dataset in the next subsection.

At line 7, we increase 𝑛𝑢𝑚 by 1. After generating a sufficient
number of samples, we check the error margin 𝜖 from line 9 to 15.
We first calculate the probability of predicting 𝑙 at line 9 and 10 for
two sets of samples. Then at line 11, we calculate the error margin 𝜖
on the group fairness score. We explain why it is calculated this way
below. If 𝜖 is less than or equal to 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 , the stopping criteria
is satisfied (as in line 12 and 13). Lastly, at line 17, we return the
absolute difference between 𝜙𝑟 and 𝜙¬𝑟 as the group fairness score.
In the above algorithm, we estimate the error margin of the group
fairness score based on an estimation of 𝑝𝑟𝑜𝑏 (𝑅, 𝑙) and 𝑝𝑟𝑜𝑏 (¬𝑅, 𝑙).
The complication is that both 𝑝𝑟𝑜𝑏 (𝑅, 𝑙) and 𝑝𝑟𝑜𝑏 (¬𝑅, 𝑙) carry cer-
tain error margin, which may magnify the error margin for the group
fairness score. In the following, we prove that line 11 in the above
algorithm allows us to conservatively estimate the error margin of
the group fairness score.

THEOREM 4.2. Assume that 𝜙𝑟 satisfies the following

𝑝𝑟𝑜𝑏 (|𝜙𝑟 − ^𝜙𝑟 | > 𝜖𝑟 ) < 1 − 𝛿𝑟
(4)
where 𝜖𝑟 and 𝛿𝑟 are constants. Similarly, 𝜙¬𝑟 satisfies the following.
𝑝𝑟𝑜𝑏 (|𝜙¬𝑟 − ^𝜙¬𝑟 | > 𝜖¬𝑟 ) < 1 − 𝛿¬𝑟

(5)

Then the following is satisfied.

𝑝𝑟𝑜𝑏 (|𝑓 − ^𝑓 | > 𝜖𝑟 + 𝜖¬𝑟 ) < 1 − 𝛿𝑟 𝛿¬𝑟

(6)

Proof: Since 𝑝𝑟𝑜𝑏 (|𝜙𝑟 − ^𝜙𝑟 | > 𝜖𝑟 ) < 1 − 𝛿𝑟 and 𝑝𝑟𝑜𝑏 (|𝜙¬𝑟 − ^𝜙¬𝑟 | >
𝜖¬𝑟 ) < 1 − 𝛿¬𝑟 , we have:

{𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑀𝑎𝑙𝑒, 𝑟𝑎𝑐𝑒 = 𝑊 ℎ𝑖𝑡𝑒, 40 ≤ 𝑎𝑔𝑒 < 60}

𝑝𝑟𝑜𝑏 (|𝜙𝑟 − ^𝜙𝑟 | ≤ 𝜖𝑟 ) ≥ 𝛿𝑟

Conference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

Algorithm 2 𝐺𝑟𝑜𝑢𝑝𝐹𝑎𝑖𝑟𝑛𝑒𝑠𝑠𝑆𝑐𝑜𝑟𝑒 (𝐷, 𝑀, 𝑅, 𝑠𝑎𝑚𝑝𝑙𝑒_𝑡ℎ𝑟, 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 )
where 𝐷 is the training dataset; 𝑀 is the machine learning model; 𝑅
is a rule set, 𝑠𝑎𝑚𝑝𝑙𝑒_𝑡ℎ𝑟 is the number of generated inputs threshold;
𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 is error margin threshold
1: 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 ← ∅, 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 ← ∅, 𝜖 ← +∞, 𝑛𝑢𝑚 ← 0
2: while 𝜖 > 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 do
𝑥 ← 𝑆𝑎𝑚𝑝𝑙𝑒 (𝐷, 𝑅)
3:
𝑥 ′ ← 𝑆𝑎𝑚𝑝𝑙𝑒 (𝐷, ¬𝑅)
𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 ← 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 ∪ 𝑥
𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 ← 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 ∪ 𝑥 ′
𝑛𝑢𝑚 ← 𝑛𝑢𝑚 + 1
if 𝑛𝑢𝑚 > 𝑠𝑎𝑚𝑝𝑙𝑒_𝑡ℎ𝑟 then

4:

7:

6:

8:

5:

9:

10:

11:

12:

13:

14:

𝜙𝑟 ← #{𝑖 ∈ 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 |𝑀 (𝑖) = 𝑙 }/𝑛𝑢𝑚
𝜙¬𝑟 ← #{𝑖 ∈ 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 |𝑀 (𝑖) = 𝑙 }/𝑛𝑢𝑚
𝜖 ← 𝑧 ×
𝑛𝑢𝑚 + 𝑧 ×
if 𝜖 ≤ 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 then

√︃ 𝜙¬𝑟 (1−𝜙¬𝑟 )
𝑛𝑢𝑚

√︃ 𝜙𝑟 (1−𝜙𝑟 )

break

end if

end if

15:
16: end while
17: return 𝑓 ← |𝜙𝑟 − 𝜙¬𝑟 |

𝑝𝑟𝑜𝑏 (|𝜙¬𝑟 − ^𝜙¬𝑟 | ≤ 𝜖¬𝑟 ) ≥ 𝛿¬𝑟

𝑝𝑟𝑜𝑏 (|(𝜙𝑟 − ^𝜙𝑟 ) − (𝜙¬𝑟 − ^𝜙¬𝑟 )| ≤ 𝜖𝑟 + 𝜖¬𝑟 ) ≥
𝑝𝑟𝑜𝑏 (|𝜙𝑟 − ^𝜙𝑟 | ≤ 𝜖𝑟 ) · 𝑝𝑟𝑜𝑏 (|𝜙¬𝑟 − ^𝜙¬𝑟 | ≤ 𝜖¬𝑟 ) ≥ 𝛿𝑟 𝛿¬𝑟

Hence

and

𝑝𝑟𝑜𝑏 (|(𝜙𝑟 − ^𝜙𝑟 ) − (𝜙¬𝑟 − ^𝜙¬𝑟 )| > 𝜖𝑟 + 𝜖¬𝑟 ) < 1 − 𝛿𝑟 𝛿¬𝑟
𝑝𝑟𝑜𝑏 (|(𝜙𝑟 − 𝜙¬𝑟 ) − ( ^𝜙𝑟 − ^𝜙¬𝑟 )| > 𝜖𝑟 + 𝜖¬𝑟 ) < 1 − 𝛿𝑟 𝛿¬𝑟
According to Definition 4.1, group fairness score 𝑓 = 𝜙𝑟 − 𝜙¬𝑟 . Thus
𝑝𝑟𝑜𝑏 (|𝑓 − ^𝑓 | > 𝜖𝑟 + 𝜖¬𝑟 ) < 1 − 𝛿𝑟 𝛿¬𝑟

□

The above theorem provides a theoretical guarantee on the sta-
tistical confidence for the group fairness score estimation. That is,
based on the Equation 6, the fairness level for fairness score 𝑓 is
𝛿𝑟 𝛿¬𝑟 and the margin of error is the sum of two margin of errors as
𝜖𝑟 + 𝜖¬𝑟 . Each 𝜖 is calculated by:

𝜖 = 𝑧 ×

√︂𝜙 (1 − 𝜙)
𝑛𝑢𝑚

(7)

where 𝑧 is the value from the standard normal distribution for a
certain confidence level 𝛿 (e.g., for a 95% confidence level, 𝑧 = 1.96).
So the final margin of error for fairness score 𝑓 is shown in line 11
of Algorithm 2. Based on the result, we derive the stopping criteria,
as shown in line 12 and 13 of Algorithm 2.

The above shows how we compute the group fairness score for
one rule set. Given multiple rule sets, we systematically compute
the fairness score for each rule set with Algorithm 2, and then rank
the rule sets according to the resultant group fairness score. If the
group fairness score of certain rule set is more than a given tolerable
threshold, we report that discrimination is identified.

Example Take a model trained on the (structured) Census Income
dataset as an example. We fixed the confidence level to 95% and
the corresponding 𝑧-value is 1.96. We set the sampling threshold
𝑠𝑎𝑚𝑝𝑙𝑒_𝑡ℎ𝑟 as 1000 and the error of margin threshold 𝑒𝑟𝑟𝑜𝑟 _𝑡ℎ𝑟 as
0.05. We are given a rule set

{𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑀𝑎𝑙𝑒, 𝑟𝑎𝑐𝑒 = 𝑊 ℎ𝑖𝑡𝑒, 40 ≤ 𝑎𝑔𝑒 < 60}
First, we sample 1000 inputs as 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 using 𝑆𝑎𝑚𝑝𝑙𝑒 function
that represents white males who are older than 40 but younger than
60. Then we sample another 1000 inputs as 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 using 𝑆𝑎𝑚𝑝𝑙𝑒
function that represents the rest individuals. We observe that 283
samples in 𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑟 are labeled as “True”, while only 91 samples
in 𝑠𝑎𝑚𝑝𝑙𝑒𝑠¬𝑟 are labeled as “True”. So 𝜙𝑟 is 28.3% and 𝜙¬𝑟 is 9.1%.
According to Algorithm 2, 𝜖𝑟 is 0.028 and 𝜖¬𝑟 is 0.018. So the
margin of error 𝜖 for fairness score is 0.046. Since 𝜖 is less than 0.05,
we stop sampling. Finally, the group fairness score is computed as
□
19.2% with 90.25% confidence.

Input Sampling

4.3
As discussed above, Algorithm 2 requires us to sample inputs with a
distribution which is similar to the data distribution of the training
dataset. As shown in [20], modern machine learning models mostly
rely on the i.i.d. assumptions. That is, the training and test set are
assumed to be generated independently from an identical distribution.
It is more likely for machine learning models to predict identically
distributed data correctly.

While it is impossible to know the actual data distribution, we
aim to generate additional samples from a distribution as close as
possible to the distribution of the training set. For structured data,
instead of generating feature vectors randomly, we generate new
samples by adding tiny perturbations on original samples uniformly.
The perturbation is added to one randomly selected non-sensitive
attribute with randomly selected perturbation direction and the per-
turbation size is 1 for integer variables or 0.01 for decimal variables.
Formally, given the rule set 𝑅, we first search a seed instance from
the dataset 𝐷 as 𝑠𝑒𝑒𝑑 = {𝑥1, 𝑥2, · · · , 𝑥𝑁 }, where ∀𝑟 ∈ 𝑅. 𝑠𝑒𝑒𝑑 ⊨ 𝑟 .
Then we randomly select a non-sensitive attribute 𝑥𝑘 , where 𝑥𝑘 ∉ 𝑆.
We perturb 𝑥𝑘 as 𝑥 ′
𝑘 = 𝑥𝑘 + 𝑑𝑖𝑟 · 𝑠_𝑝𝑒𝑟𝑡, where 𝑑𝑖𝑟 ∈ [−1, 1] and
𝑠_𝑝𝑒𝑟𝑡 is the perturbation size.

For text data, we generate new samples by replacing sensitive
terms with a different term in the same sensitive term category.
For example, when we test the machine learning model trained on
the Wikipedia Talk Pages dataset, given a rule set {“𝑔𝑎𝑦”}, we
need to generate additional comments containing the term “gay”.
First, we search all comments containing gender-related sensitive
terms such as “lesbian” and “bisexual”, as defined in Table 1. Then
we replace these terms in the original comments with the term
“gay” to generate new comments. That is, we can generate “I am
a gay” from an original comment “I am a lesbian”. The reason
why we use text replacement instead of text perturbation, as in the
case of structured data, is that perturbing texts with synonyms (as
proposed in [39] for adversarial attacks) is ineffective to generate
the texts in the desired group. Our text generation method also has
the benefit of mitigating the influence of data imbalance which
may cause unintended bias [14]. Formally, given the rule set 𝑅 =
{𝑟1, 𝑟2, · · · , 𝑟𝑚 }, we first search a seed instance from the dataset 𝐷 as
𝑠𝑒𝑒𝑑 = {𝑥1, 𝑥2, · · · , 𝑥𝑁 }, where ∀𝑟 ∈ 𝑅. 𝑐𝑜𝑛𝑡𝑎𝑖𝑛𝑠 (𝑠𝑒𝑒𝑑, 𝑠𝑟 ), where

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

𝑠𝑟 is the sensitive category referring to 𝑟 and 𝑐𝑜𝑛𝑡𝑎𝑖𝑛𝑠 (𝑑, 𝑠𝑟 ) is a
proposition which is true if and only if 𝑑 contains at least one term
in the category 𝑠𝑟 . Then we replace the term 𝑥𝑖 to term 𝑟 𝑗 , for all
𝑟 𝑗 ∈ 𝑅 and 𝑥𝑖 ∈ 𝑠𝑟 𝑗 .

5
IMPLEMENTATION AND EVALUATION
We have implemented TESTSGD as a self-contained software
toolkit based on Tensorflow [1] with about 6K lines of Python code.

Experiment Subjects Our experiments are based on 8 models trained
with the following benchmark datasets. These datasets have been
widely used as evaluation subjects in multiple previous studies on
fairness [14, 17, 30, 37, 50, 51].

• Census Income [36]: The dataset contains more than 30,000
samples and is used to predict whether the income of an adult is
above $50,000 annually. The attributes 𝑔𝑒𝑛𝑑𝑒𝑟 , 𝑟𝑎𝑐𝑒 and 𝑎𝑔𝑒 are
sensitive attributes.

• Bank Marketing [32]: The dataset contains 45,000+ samples and
is used to train models for predicting whether the client would
subscribe a term deposit. Its sensitive attribute is 𝑎𝑔𝑒.

• German Credit [22]: This is a small dataset with 600 samples.
The task is to assess an individual’s credit. The sensitive attributes
are 𝑔𝑒𝑛𝑑𝑒𝑟 and 𝑎𝑔𝑒.

• COMPAS [6]: This dataset contains 7,000+ samples. The task is
to predict whether the recidivism risk score for an individual is
high. The sensitive attributes are 𝑔𝑒𝑛𝑑𝑒𝑟 , 𝑟𝑎𝑐𝑒 and 𝑎𝑔𝑒.

• Crime [34]: This dataset contains almost 2,000 data for commu-
nities within the US. The task is to predict whether the violent
crimes per population in a specific community is high. Since this
dataset records population statistics, their sensitive features are
shown in multiple attributes with percentage values. Here, we
extract all gender/race/age related attributes to learn rule sets.
• Law School [7]: This dataset has more than 20,000 application
records and is used to predict whether a student passes the bar
exam. The attributes, 𝑟𝑎𝑐𝑒 and 𝑔𝑒𝑛𝑑𝑒𝑟 are sensitive attributes.
• Wiki Talk Pages [47]: This is a textual dataset containing more
than 100,000 Wikipedia TalkPage comments. The task is to predict
whether a given comment is toxic.

• IMDB [31]: IMDB dataset contains 50,000 movie reviews. The
task is to predict whether a given sentence is a positive review.

For the first six structured datasets, we train a six-layer feed-
forward neural network using the exact same configuration as re-
ported in the previous studies [50, 51]. For the last two textual
datasets, we train a convolutional neural network (CNN) combined
with long short-term memory (LSTM). The details of trained models
are shown in Table 3. The accuracy of the trained models is expect-
edly similar to what is reported in the previous studies. Table 2 shows
the value of parameters used in our experiment to run TESTSGD.
All experiments are conducted on a server running Ubuntu 1804
with 1 Intel Core 3.10GHz CPU, 32GB memory and 2 NVIDIA
GV102 GPU. To mitigate the effect of randomness, all the results
are the average of 3 runs.

We aim to answer multiple research questions as follows.

RQ1: Is our method effective in identifying subtle group discrimina-
tion of a given machine learning model? To answer the question, we
systematically apply our approach to the above-mentioned models

Conference’17, July 2017, Washington, DC, USA

Table 2: Parameters of the Experiments

𝜃

sample_thr
𝛿

Parameters Value
5%
1000
95%
0.05
1.96
1
0.01

error_thr
z
s_pert
s_pert

Discription
support threshold
sampling threshold
confidence level
error margin threshold
z value
perturbation size for integer variables
perturbation size for decimal variables

Table 3: Dataset and Models of Experiments

Dataset
Census Income
Bank Marketing
German Credit
COMPAS
Crime
Law School

Model
Six-layer Fully-connected NN
Six-layer Fully-connected NN
Six-layer Fully-connected NN
Six-layer Fully-connected NN
Six-layer Fully-connected NN
Six-layer Fully-connected NN

Wikipedia Talk Pages CNN Long Short-term memory network
CNN Long Short-term memory network

IMDB

Accuracy
86.13%
91.62%
100%
78.99%
92.52%
95.19%
93.89%
86.68%

and measure the results. The results are summarized in Table 4. It
shows results on the six models trained on structured data and re-
sults on the two models trained on text data. These four columns
show datasets, rule sets, group fairness scores and model accuracies
respectively. The favorable label is “True”, the meaning of which
can be found in the above introduction on the corresponding dataset.
Note that for each model, we rank the identified subtle discrimina-
tion according to the group fairness score and we report the top 3
worst discrimination only.

We can observe that subtle discrimination does exist in these
models, which were never revealed in the previous studies [14, 17,
30, 50, 51]. For instance, the model trained on the Bank Marketing
dataset predicts only 3.3% of the clients who are older than 10 but
younger than 90 would subscribe a term deposit, whilst predicting
41.5% of clients older than 90 would subscribe a term deposit. All
of the top 3 testing results all show the model discriminates against
young clients. We remark that although this is unfair according to
the definition, it may have its underlying reasons and it is still up to
human experts to decide whether it is actual discrimination.

For the models trained on the Census Income dataset, German
Credit dataset and the Law School dataset, they show relatively
mild discrimination. In contrast, the model trained on the COMPAS
dataset shows severe discrimination, with a fairness score of 62.4%.
That is, for Hispanic or other race male individuals who are older
than 40, the model is much less likely to predict the recidivism risk
as high. For the remaining individuals, the model predicts 83.1%
of them have high recidivism risk. Top 2 and top 3 test results also
show severe discrimination against older Hispanic or other race male
individuals. Similarly, the model trained on the Crime dataset also
shows high discrimination. Different from the first five structured
datasets, samples in this dataset have 10 different sensitive features,
each of which is a decimal ranging from 0.0 to 1.0 representing the
percentage of certain population. As shown in the top 1 testing result,
when the percentage of divorced females is above 40% and the per-
centage of Caucasian is below 80%, the model is much more likely

Conference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

Table 4: Rule Sets and Fairness Scores for Neural Networks

Dataset

top 1

Rule Set

Census Income

gender=male, 40≤age<80,
race=White or Asian-Pac-Islander

Bank Marketing

10 ≤ age < 90

German Credit

gendre = female, 60≤age<70

COMPAS

gender = male, age≥40,
race = Hispanic or other race

Law School

gender = male, race = Asian or Black

Crime

FemalePctDiv≥0.4, racePctWhite≤0.8

Wiki Talk Pages

"gay", "taoist"

IMDB

"european", "yong"

Fairness Score
(𝜙𝑟 , 𝜙¬𝑟 )
20.2%
(29.9%, 9.7%)
38.2%
(3.3%, 41.5%)
21.9%
(72.5%, 50.6%)
62.4%
(20.7%, 83.1%)
15.0%
(84.5%, 99.5%)
60.7%
(83.8%, 23.2%)
6.5%
(13.0%, 6.5%)
6.6%
(56.0%, 49.4%)

top 2

Rule Set

gender=male, 40≤age<70,
race=White or Amer-Indian-Eskimo

10 ≤ age < 70

gender = female, 60≤age<80

gender = male, 40≤age<60,
race = Hispanic or other race

gender = female, race = Asian or Black

FemalePctDiv≥0.5, racePctWhite≤0.8

"gay", "protestant"

"white", "older"

Fairness Score
(𝜙𝑟 , 𝜙¬𝑟 )
19.4%

top 3

Rule Set

gender=male, 40≤age<80, race=White,

(28.9%, 9.5%) Asian-Pac-Islander or Amer-Indian-Eskimo

22.8%
(26.6%, 3.8%)
21.8%
(70.5%, 48.7%)
62.3%
(20.3%, 82.6%)
11.1%
(88.8%, 99.9%)
59.6%
(87.0%, 27.4%)
5.4%
(12.9%, 7.5%)
6.6%
(59.1%, 52.6%)

10 ≤ age < 60

gender = male, 40≤age<80

gender = male, 50≤age<60,
race = Hispanic

gender = female, race = Black

FemalePctDiv≥0.5, racePctWhite≤0.6

"gay", "african american"

"lgbtq"

Fairness Score
(𝜙𝑟 , 𝜙¬𝑟 )
18.4%
(26.9%, 8.5%)
20.5%
(4.7%, 25.2%)
15.5%
(52.6%, 47.1%)
62.3%
(19.5%, 81.8%)
10.2%
(89.7%, 99.9%)
59.5%
(94.6%, 35.1%)
5.1%
(12.5%, 7.4%)
6.5%
(7.5%, 14.0%)

to predict that the violent crimes per population in this community
is high. All testing results on the model trained on Crime dataset
suggest that the model discriminates against communities with high
percentage of divorced females and low percentage of Caucasian.

In Table 4, the last two rows show the results on models trained
on the text data. In general, we observe that the models trained on
text dataset show less discrimination. The maximum fairness score
for the model trained on the Wikipedia Talk Pages dataset is 6.5%.
That is, the model predicts 13.0% of comments containing both “gay”
and “taoist” as toxic. For other comments (i.e., those without one
of these two terms or both), the model predicts only 6.5% of them
as toxic. Top 2 and top 3 testing results show that the model dis-
criminates against comments containing both “gay” and “protestant”
and comments containing both “gay” and “african american” respec-
tively. The model trained on the IMDB dataset shows a similar level
of discrimination. It is more likely to predict reviewers containing
“european” and “young” and reviews containing “white” and “older”
as positive. It also shows discrimination against reviews containing
“lgbtq”. Our conjecture on why the level of discrimination is con-
siderably lower on these models is that each sample in these text
datasets often has many features and as a result, the influence of
each term (including sensitive terms) is distributed.

Answer to RQ1: TESTSGD is effective in identifying
subtle group discrimination in neural networks.

RQ2: Is our method efficient? To answer this question, we measure
the amount of time required to identify the subtle discrimination
for each model. The total execution time and the numbers of tested
rule sets are shown in Table 5. For all models, the time required to
identify the subtle discrimination is less than 20 hours. Furthermore,
models trained on structured dataset take considerably less time than
those trained on text dataset. That is, models trained on the Census
Income, Bank Marketing, German Credit, COMPAS and Law
School take less than 16 minutes. One exception is the model trained
on the Crime dataset that takes more than 8 hours. The main reason
is that it has a large number of rule sets, due to a large number of
sensitive features (i.e., 10), all of which are continuous features. In
contrast, both models trained on text dataset take more than 9 hours
to finish. The main reason is that generating additional samples for

Table 5: Time Taken to Identify the subtle discrimination

Dataset
Census Income
Bank Marketing
German Credit
COMPAS
Law School
Crime
Wiki Talk Pages
IMDB

Time (seconds)
869.35
141.52
104.85
908.5
18.46
29150.01
34982.28
69125.16

#rule set
880
34
53
1590
17
13282
732
876

such dataset takes much more time in general. We remark that the
sampling procedure can be easily parallelized and thus we could
significantly reduce the time if it is an issue.

Note that the support threshold 𝜃 is set to be 5% in all the above
experiments. Intuitively, it means that each rule must be relevant
to 5% of the population (although the rule set, which is a con-
junction of multiple rules, may impact a smaller population). This
hyper-parameter largely determines how many rule sets that we must
examine and thus may have an impact on the execution time. We
thus conduct additional experiments with different 𝜃 values, ranging
from 1% to 50%, to evaluate the effect of 𝜃 on the execution time
and the results. The results on two models, i.e., the model on Law
School and the model on COMPAS, are detailed in Table 6.

The table shows the execution time, the number of rule sets and
the worst group fairness score. We can observe that, the larger a 𝜃
we set, the fewer rule sets, the less execution time and the smaller
group fairness score in general. If the threshold 𝜃 is too low, e.g., 1%,
we spend a lot of time on testing a huge number of rule sets, which
may not be interesting (one such example is {𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑀𝑎𝑙𝑒, 𝑎𝑔𝑒 ≥
100}). In contrast, if the threshold 𝜃 is too high, e.g., 20% or 50%,
there may only exists few or even none rule set (as in the case of the
model trained on the COMPAS dataset).

We note that different 𝜃 may result in different discrimination
being identified. For the model trained on Law School, the rule set
shows that the model discriminates against black or Asian males the
most when 𝜃 is 5%. However, when we set 𝜃 to be 1%, the model is
shown to discriminate against black male individuals the most. For
the model trained on the COMPAS dataset, the model discriminates

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

Table 6: Effect of Different 𝜃

#rule sets

Rule Set

Dataset

𝜃

1%

5%

Time
(seconds)

46.71

18.46

Law School

10%

17.83

20%

17.83

50%

6.28

59

17

16

16

2

1%

1175.79

2063

5%

908.50

1590

COMPAS

10%

676.74

1180

gender=male,
race=Black
gender=male,
race=Asian or Black
gender=male,
race=Asian or White
gender=male,
race=Asian or White
gender=male,
race=other race
gender=male, age≥40
race=Hispanic or other race
gender=male, age≥40
race=Hispanic or other race
gender=male, age≥20
race=Hispanic or other race

20%

50%

0

0

0

0

NULL

NULL

Fairness
Score

16.3%

15.0%

1.0%

0.9%

0.3%

62.4%

62.4%

43.9%

NULL

NULL

against Hispanic or other race males who is older than 40 years
old most when we set 𝜃 to be 5%. However, when we set 𝜃 higher
(i.e., 10%), the age range is expanded to be over 20 years in the
identified rule set. Such a result is expected as a large 𝜃 requires us
to find discrimination against a large group. What is considered to
be a reasonable value for 𝜃 is a complicated question, which should
probably be answered by lawmakers.

Answer to RQ2: TESTSGD is reasonably efficient.

RQ3: Can we mitigate subtle discrimination using our testing re-
sults? To further show the usefulness of our approach, we evaluate
whether we can mitigate the identified subtle discrimination using
our testing results. The idea is to mitigate the discrimination by
retraining. We remark that there are alternative approaches for im-
proving fairness as well [25, 38]. Note that we generate additional
instances satisfying the rule set with the sampling approach de-
scribed in Section 4.3. We only select those generated instances with
the opposite label. For example, the model trained on COMPAS
is more likely to predict elderly males who are Hispanic or other
race with “False” label. We can use the 𝑆𝑎𝑚𝑝𝑙𝑒 function to generate
instances satisfying the condition that are labeled as “True” accord-
ing to the original model. Afterward, we retrain the original model
with these additional instances and testing the subtle discrimination
with respect to the same rule set to see the improvement. Note that
we gradually increase the number of additional instances from 50
to 10% of original dataset size to achieve the lowest fairness score
without decreasing the accuracy of the retrained model.

We only consider the top 1 worst rule sets to mitigate the discrim-
ination. The results are shown in Table 7 for six models trained on
additional structured data and two models retrained on additional
textual data. We can observe that all models show reduced subtle
discrimination and almost the same accuracy. The fairness scores
for retrained models on Census Income, German Credit and Law
School decrease by about half. For the most improvement, the model
retrained on the COMPAS dataset shows much less subtle discrim-
ination as the fairness score decreases by more than 10 times, i.e.,

Conference’17, July 2017, Washington, DC, USA

from 57.7% to 4.2%. The fairness score of the model trained on
the Crime dataset decreases from 60.7% to 51.4%. Relatively, the
fairness improvement is not obvious. We believe that it is due to its
many continuous sensitive features and the large number of features
(i.e., each input contains more than 100 attributes). That is, it would
require a lot more additional data to improve fairness. In terms of
CNN models, the fairness score decreases from from 6.5% to 0.4%
for the model retrained on Wikipedia Talk Pages and decreases
from 6.6% to 3.3% for the model retrained on IMDB.

Answer to RQ3: TESTSGD is useful in mitigating the
identified subtle group discrimination through retraining.

Comparison with Baselines We identify the following two baselines
from literature which can potentially identify similar group discrimi-
nation as our work. 1) THEMIS [17] calculates group discrimina-
tion scores over combinations of multiple features (subgroups) by
measuring the difference between the maximum and minimum fre-
quencies of two subgroups on randomly generated samples. Those
subgroups can then be regarded as identified discrimination if the
score is higher than a threshold. 2) FairFictPlay [25] proposed an
in-processing algorithm aiming to improve subgroup fairness. The
subgroups are identified with user-provided constraints in the form
of conjunctions of Boolean attributes, linear threshold functions,
or bounded degree polynomial threshold functions over multiple
protected features.

In Table 8, we show the identified group discrimination with
TESTSGD, Themis and FairFictPlay respectively, along with the fair-
ness scores, on the same models trained on structured data (similarly
to Table 3). We set the timeout as 24 hours. Note that FairFictPlay
uses complex linear functions on all the protected features (which
are hard to interpret) to define discriminatory subgroups, thus we do
not show the exact concrete linear functions in the table. We have
the following observations. 1) Compared to FairFictPlay, TESTSGD
identifies discrimination with higher scores (more discriminating)
while being interpretable. Moreover, TESTSGD automatically iden-
tifies the discriminated subgroups without any prior knowledge. 2)
Similar to TESTSGD, Themis is able to identify discriminated sub-
groups automatically. However, Themis identifies two subgroups
which are maximally different (in terms of being predicted favor-
ably) while TESTSGD identifies subgroups which are predicted
different from the rest. These two approaches thus produce results
that are complementary to each other. Note that Themis does not
support text data.
6 RELATED WORK
Many existing works attempted to test discrimination according to
different fairness definitions and measurements [13, 15]. In [16],
Feldman et al. provide a fairness definition which is measured ac-
cording to demographic parity of model predictions. It measures
how well the sensitive class can be predicted based on classifica-
tion accuracy. In [21], Hardt et al. present an alternate definition of
fairness based on demographic parity. It requires a decision to be
independent of the sensitive attribute. In [28], Kusner et al. define
counterfactual discrimination which focuses on single decisions to-
wards an individual. A prediction is counterfactual fair if it is the
same in the actual group and a different demographic group. In [17],

Conference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

Table 7: Discrimination Mitigation for Neural Networks

Dataset

Rule Set

Census Income

gender=male, 40≤age<80,
race=White or Asian-Pac-Islander

Bank Marketing

10 ≤ age < 90

accuracy

86.1%

91.6%

German Credit

gender = female, 60≤age<70

100.0%

COMPAS

gender = male, age≥40,
race = Hispanic or other race

Law School

gender = male, race = Black

79.0%

95.2%

Crime

FemalePctDiv≥0.4, racePctWhite≤0.8

93.9%

Wiki Talk Pages

"gay", "taoist"

IMDB

"european", "yong"

93.9%

86.7%

Before

After

Fairness Score
(𝜙𝑟 , 𝜙¬𝑟 )
20.2%
(29.9%, 9.7%)
38.2%
(3.3%, 41.5%)
21.9%
(72.3%, 50.6%)
62.4%
(20.7%, 83.1%)
15.0%
(84.5%, 99.5%)
60.7%
(83.8%, 23.2%)
6.5%
(13.0%, 6.5%)
6.6%
(56.0%, 49.4%)

accuracy

86.2%

90.6%

100.0%

78.5%

95.1%

98.1%

95.5%

84. %

Fairness Score
(𝜙𝑟 , 𝜙¬𝑟 )
10.1%
(18.9%, 8.8%)
5.4%
(6.9%, 12.3%)
7.3%
(45.9%, 53.2%)
4.2%
(80.9%, 85.1%)
7.5%
(92.3%, 99.8%)
51.4%
(90.6%, 39.2%)
0.4%
(8.4%, 8.0%)
3.3%
(43.7%, 40.4%)

Table 8: Comparisons Between TESTSGD , Themis and FairFictPlay. ‘-’ means timeout.

Dataset

Census Income

TESTSGD

Rule Set
Gender=male, 40≤age<80,
race=White or Asian-Pac_islander

Bank Marketing

10≤age<90

German Credit

gender=feamle, 60≤age<70

COMPAS

gender=male, age≥40,
race=Hispanic or other race

Fairness Score

20.20%

38.2%

21.9%

62.4%

Law School

gender=male, race=Asian or Black

15.0%

Themis

FairFictPlay

Sensitive Attributes’ values for Max/Min Proportion
[gender=Female, 60≤age<70, race=Asian-Pac_islander] -
[gender=Male, 10≤age<20, race=White]

Fairness Score

Subgroup

Fairness Score

26.6%

Linear Threshold Function

13.9%

[60≤age<70] - [10≤age<20]

8.4%

Linear Threshold Function

[gender=Female, 80≤age<90] -
[gender=Male, 10≤age<20]
[gender=Female, 10≤age<20, race=Native American] -
[gender=Male, 60≤age<70, race=other race]
[gender=Male, race=White] -
[gender=Female, race=Black]

7.6%

7.0%

17.1%

Linear Threshold Function

67.3%

Linear Threshold Function

22.4%

13.5%

Linear Threshold Function

3.7%

Crime

FemalePctDiv≥0.4, racePctWhite≤0.8

60.7%

-

-

Linear Threshold Function

38.8%

Galhotra et al. propose causal discrimination to measure the fraction
of inputs for which model causally discriminates. This definition is
similar to counterfactual fairness, but it takes instances of discrimi-
nation into account. In [25], Kearns et al. proposed an in-processing
algorithm aiming to improve the fairness of given subgroups, where
subgroups are defined as conjunctions of attributes, linear threshold
functions, or bounded degree polynomial threshold functions over
multiple protected features. Most existing works [8, 17, 26] use
positive classification rate as fairness measurement.

Subsequently, many works focus on individual discrimination to
generate individual discriminatory instances [3, 23, 50, 51]. They
tried to generated instances which are classified differently after
changing sensitive attributes. In [3], Agarwal et al. present an au-
tomated testing approach to generate test inputs to find individual
discrimination. In [37], Ruoss et al. propose a fairness representation
framework to generalize individual fairness to multiple notions. It
learns a mapping from similar individuals to latent representations.
However, the testing on individual discrimination cannot provide a
statistical measurement of fairness.

Some other existing works attempted to test model discrimination
with fairness score measurements. In [41], Tramer et al. propose an
unwarranted associations framework to detect unfair, discriminatory
or offensive user treatment in data-driven applications. It identi-
fies discrimination according to multiple metrics including the CV
score, related ratio and associations between outputs and sensitive
attributes. In [26], Kleinberg et al. also test multiple discrimination

scores and compare different fairness metrics. In [17], Galhotra et al.
propose a tool called THEMIS to measure software discrimination.
It tests discrimination with two fairness definitions, i.e., group dis-
crimination score and causal discrimination score. In [2], Adebayo
et al. try to determine the relative significance of a model’s inputs
in determining the outcomes and use it to assess the discriminatory
extent of the model.

Some prior work has been done on fairness for text classification
tasks as well. In [9], Blodgett et al. discuss the impact of unfair
natural language in NLP and show how statistical discrimination
arises in processing applications. In [11], Bolukbasi et al. show
gender bias in the world embedding and provide a methodology
for modifying an embedding to remove gender bias. In [14], Dixon
et al. measure discrimination using a set of common demographic
identity terms and propose a method to mitigate the unintended bias
by balancing the training data.

Compared with all the above-mentioned existing works, we pro-
vide further fairness testing. Instead of measuring the overall dis-
crimination, our approach systematically identifies and measures
subtle discrimination. That is, we not only measure statistical dis-
crimination with a confidence guarantee but also offer interpretable
rule sets to represent subtle discrimination.

This work is remotely related to works on applying rule-based
models for model explanation. In [48], Yang et al. present an al-
gorithm for building probabilistic rule lists with logical IF-THEN
structure. In [29], Lakkaraju et al. propose interpretable decision

TESTSGD: Interpretable Testing of Neural Networks Against
Subtle Group Discrimination

Conference’17, July 2017, Washington, DC, USA

sets to interpret model predictions with high accuracy and high inter-
pretation. Our work leverage such rule-based interpretable structure
to present subtle discrimination in models.
7 CONCLUSION
In this work, we focus on testing neural network models against
subtle group discrimination and propose a framework to systemat-
ically identify interpretable subtle group discrimination based on
group fairness measurement with a certain confidence. Our extensive
evaluation demonstrates that subtle group discrimination in neural
networks is common to a surprising level. We also show that it is pos-
sible to mitigate such discrimination by utilizing our testing results
to generate more data for retraining.

REFERENCES
[1] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Irving, G., Isard, M., et al.: Tensorflow: A system for large-scale
machine learning. In: 12th {USENIX} symposium on operating systems design
and implementation ({OSDI} 16). pp. 265–283 (2016)

[2] Adebayo, J., Kagal, L.: Iterative orthogonal feature projection for diagnosing bias

in black-box models. arXiv preprint arXiv:1611.04967 (2016)

[3] Agarwal, A., Lohia, P., Nagar, S., Dey, K., Saha, D.: Automated test generation to
detect individual discrimination in ai models. arXiv preprint arXiv:1809.03260
(2018)

[4] Aggarwal, A., Lohia, P., Nagar, S., Dey, K., Saha, D.: Black box fairness testing of
machine learning models. In: Proceedings of the ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia. pp. 625–635. ACM
(2019), https://doi.org/10.1145/3338906.3338937

[5] Angell, R., Johnson, B., Brun, Y., Meliou, A.: Themis: Automatically testing soft-
ware for discrimination. In: Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. pp. 871–875 (2018)

[6] Angwin, J., Larson, J., Mattu, S., Kirchner, L.: Machine bias: There’s software used
across the country to predict future criminals. and it’s biased against blacks. ProP-
ublica (2016), https://www.propublica.org/article/machine-bias-risk-assessments-
in-criminal-sentencing

[7] Anthony, L.C., Liu, M.: Analysis of differential prediction of law school perfor-
mance by racial/ethnic subgroups based on the 1996-1998 entering law school
classes. lsac research report series. (2003)

[8] Biswas, S., Rajan, H.: Do the machine learning models on a crowd sourced
platform exhibit bias? an empirical study on model fairness. In: Proceedings of
the 28th ACM joint meeting on European software engineering conference and
symposium on the foundations of software engineering. pp. 642–653 (2020)
[9] Blodgett, S.L., O’Connor, B.: Racial disparity in natural language process-
ing: A case study of social media african-american english. arXiv preprint
arXiv:1707.00061 (2017)

[10] Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P.,
Jackel, L.D., Monfort, M., Muller, U., Zhang, J., et al.: End to end learning for
self-driving cars. arXiv preprint arXiv:1604.07316 (2016)

[11] Bolukbasi, T., Chang, K.W., Zou, J., Saligrama, V., Kalai, A.: Man is to computer
programmer as woman is to homemaker? debiasing word embeddings. arXiv
preprint arXiv:1607.06520 (2016)

[12] Buolamwini, J., Gebru, T.: Gender shades: Intersectional accuracy disparities in
commercial gender classification. In: Conference on fairness, accountability and
transparency. pp. 77–91. PMLR (2018)

[13] Calders, T., Verwer, S.: Three naive bayes approaches for discrimination-free

classification. Data Mining and Knowledge Discovery 21(2), 277–292 (2010)

[14] Dixon, L., Li, J., Sorensen, J., Thain, N., Vasserman, L.: Measuring and mitigating
unintended bias in text classification. In: Proceedings of the 2018 AAAI/ACM
Conference on AI, Ethics, and Society. pp. 67–73 (2018)

[15] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through
awareness. In: Proceedings of the 3rd innovations in theoretical computer science
conference. pp. 214–226 (2012)

[16] Feldman, M., Friedler, S.A., Moeller, J., Scheidegger, C., Venkatasubramanian,
S.: Certifying and removing disparate impact. In: proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining. pp.
259–268 (2015)

[17] Galhotra, S., Brun, Y., Meliou, A.: Fairness testing: testing software for discrimi-
nation. In: Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering. pp. 498–510 (2017)

[18] Ghosh, B., Basu, D., Meel, K.S.: Justicia: A stochastic sat approach to formally

verify fairness. arXiv preprint arXiv:2009.06516 (2020)

[19] Gianfrancesco, M.A., Tamang, S., Yazdany, J., Schmajuk, G.: Potential biases in
machine learning algorithms using electronic health record data. JAMA internal
medicine 178(11), 1544–1547 (2018)

[20] Goodfellow, I.: A research agenda: Dynamic models to defend against correlated

attacks. arXiv preprint arXiv:1903.06293 (2019)

[21] Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning.

arXiv preprint arXiv:1610.02413 (2016)

[22] Hofmann, H.: German credit dataset (1994), https://archive.ics.uci.edu/ml/datasets/

statlog+(german+credit+data)

[23] Huchard, M., Kästner, C., Fraser, G.: Proceedings of the 33rd acm/ieee inter-
national conference on automated software engineering (ase 2018). In: ASE:
Automated Software Engineering. ACM Press (2018)

[24] Joseph, M., Kearns, M., Morgenstern, J., Roth, A.: Fairness in learning: Classic

and contextual bandits. arXiv preprint arXiv:1605.07139 (2016)

[25] Kearns, M., Neel, S., Roth, A., Wu, Z.S.: Preventing fairness gerrymandering:
Auditing and learning for subgroup fairness. In: International Conference on
Machine Learning. pp. 2564–2572. PMLR (2018)

[26] Kleinberg, J., Mullainathan, S., Raghavan, M.: Inherent trade-offs in the fair

determination of risk scores. arXiv preprint arXiv:1609.05807 (2016)

[27] Kononenko, I.: Machine learning for medical diagnosis: history, state of the art
and perspective. Artificial Intelligence in medicine 23(1), 89–109 (2001)
[28] Kusner, M.J., Loftus, J.R., Russell, C., Silva, R.: Counterfactual fairness. arXiv

preprint arXiv:1703.06856 (2017)

[29] Lakkaraju, H., Bach, S.H., Leskovec, J.: Interpretable decision sets: A joint frame-
work for description and prediction. In: Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and data mining. pp. 1675–1684
(2016)

[30] Ma, P., Wang, S., Liu, J.: Metamorphic testing and certified mitigation of fairness
violations in nlp models. In: Proceedings of the 29th International Joint Conference
on Artificial Intelligence (IJCAI). 458–465 (2020)

[31] Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning word
vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Human Language Technologies.
pp. 142–150. Association for Computational Linguistics, Portland, Oregon, USA
(June 2011), http://www.aclweb.org/anthology/P11-1015

[32] Moro, S., Cortez, P., Rita, P.: A data-driven approach to predict the success of
bank telemarketing. Decision Support Systems 62, 22–31 (2014), https://archive.
ics.uci.edu/ml/datasets/bank+marketing

[33] Pei, K., Cao, Y., Yang, J., Jana, S.: Deepxplore: Automated whitebox testing
of deep learning systems. In: proceedings of the 26th Symposium on Operating
Systems Principles. pp. 1–18 (2017)

[34] Redmond, M.: Communities and crime dataset (2009), http://archive.ics.uci.edu/

ml//datasets/Communities+and+Crime)

[35] Rivest, R.L.: Learning decision lists. Machine learning 2(3), 229–246 (1987)
[36] Ronny Kohavi, B.B.: Data mining and visualization (1996), https://archive.ics.uci.

edu/ml/datasets/adult

[37] Ruoss, A., Balunovi´c, M., Fischer, M., Vechev, M.: Learning certified individually

fair representations. arXiv preprint arXiv:2002.10312 (2020)

[38] Salimi, B., Rodriguez, L., Howe, B., Suciu, D.: Interventional fairness: Causal
database repair for algorithmic fairness. In: Proceedings of the 2019 International
Conference on Management of Data. pp. 793–810 (2019)

[39] Sato, M., Suzuki, J., Shindo, H., Matsumoto, Y.: Interpretable adversarial per-
turbation in input embedding space for text. arXiv preprint arXiv:1805.02917
(2018)

[40] Tian, Y., Pei, K., Jana, S., Ray, B.: Deeptest: Automated testing of deep-neural-
network-driven autonomous cars. In: Proceedings of the 40th international confer-
ence on software engineering. pp. 303–314 (2018)

[41] Tramer, F., Atlidakis, V., Geambasu, R., Hsu, D., Hubaux, J.P., Humbert, M., Juels,
A., Lin, H.: Fairtest: Discovering unwarranted associations in data-driven applica-
tions. In: 2017 IEEE European Symposium on Security and Privacy (EuroS&P).
pp. 401–416. IEEE (2017)

[42] Udeshi, S., Arora, P., Chattopadhyay, S.: Automated directed fairness testing.
In: Proceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering. pp. 98–108 (2018)

[43] Unknown: Omitted for anonymity
[44] Verma, S., Rubin, J.: Fairness definitions explained. In: 2018 ieee/acm international

workshop on software fairness (fairware). pp. 1–7. IEEE (2018)

[45] Wald, A.: Sequential tests of statistical hypotheses. The annals of mathematical

statistics 16(2), 117–186 (1945)

[46] Wald, A., Wolfowitz, J.: Optimum character of the sequential probability ratio test.

The Annals of Mathematical Statistics pp. 326–339 (1948)

[47] Wulczyn, E., Thain, N., Dixon, L.: Ex machina: Personal attacks seen at scale.
In: Proceedings of the 26th international conference on world wide web. pp.
1391–1399 (2017)

[48] Yang, H., Rudin, C., Seltzer, M.: Scalable bayesian rule lists. In: International

Conference on Machine Learning. pp. 3921–3930. PMLR (2017)

[49] Zafar, M.B., Valera, I., Rogriguez, M.G., Gummadi, K.P.: Fairness constraints:
Mechanisms for fair classification. In: Artificial Intelligence and Statistics. pp.

Conference’17, July 2017, Washington, DC, USA

Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun

962–970. PMLR (2017)

[50] Zhang, P., Wang, J., Sun, J., Dong, G., Wang, X., Wang, X., Dong, J.S., Dai, T.:
White-box fairness testing through adversarial sampling. Proceedings of the 42rd
International Conference on Software Engineering (ICSE 2020), Seoul, South
Korea (2020)

[51] Zhang, P., Wang, J., Sun, J., Wang, X., Dong, G., Wang, X., Dai, T.,
Dong, J.S.: Automatic fairness testing of neural classifiers through adversar-
ial sampling. IEEE Transactions on Software Engineering pp. 1–1 (2021).
https://doi.org/10.1109/TSE.2021.3101478

