2
2
0
2

n
u
J

3
1

]
E
S
.
s
c
[

1
v
1
6
9
5
0
.
6
0
2
2
:
v
i
X
r
a

A Survey on Automated Driving System Testing: Landscapes
and Trends

SHUNCHENG TANG, ZHENYA ZHANG, YI ZHANG, JIXIANG ZHOU, YAN GUO, SHUANG
LIU, SHENGJIAN GUO, YAN-FU LI, LEI MA, YINXING XUE, and YANG LIU

Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both
academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning
and control, which brings together the latest advances in multiple domains. Despite these achievements,
safety assurance of the systems is still of great significance, since the unsafe behavior of ADS can bring
catastrophic consequences and unacceptable economic and social losses. Testing is an important approach to
system validation for the deployment in practice; in the context of ADS, it is extremely challenging, due to
the system complexity and multidisciplinarity. There has been a great deal of literature that focuses on the
testing of ADS, and a number of surveys have also emerged to summarize the technical advances. However,
most of these surveys focus on the system-level testing that is performed within software simulators, and
thereby ignore the distinct features of individual modules. In this paper, we provide a comprehensive survey
on the existing ADS testing literature, which takes into account both module-level and system-level testing.
Specifically, we make the following contributions: (1) we build a threat model that reveals the potential safety
threats for each module of an ADS; (2) we survey the module-level testing techniques for ADS and highlight
the technical differences affected by the properties of the modules; (3) we also survey the system-level testing
techniques, but we focus on empirical studies that take a bird’s-eye view on the system, the problems due to
the collaborations between modules, and the gaps between ADS testing in simulators and real world; (4) we
identify the challenges and opportunities in ADS testing, which facilitates the future research in this field.

CCS Concepts: • Computer systems organization → Embedded systems; • Software and its engineer-
ing → Software verification and validation; • Security and privacy → Systems security; • Computing
methodologies → Artificial intelligence.

Additional Key Words and Phrases: ADS testing, module-level testing, system-level testing, system security

ACM Reference Format:
Shuncheng Tang, Zhenya Zhang, Yi Zhang, Jixiang Zhou, Yan Guo, Shuang Liu, Shengjian Guo, Yan-Fu Li,
Lei Ma, Yinxing Xue, and Yang Liu. 2022. A Survey on Automated Driving System Testing: Landscapes and
Trends. ACM Forthcoming 1, 1 (June 2022), 51 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION
With the aim of bringing convenient driving experience, increasing driving safety and reducing
traffic congestion, automated driving systems (ADS) have attracted significant attention from both
academia and industry. According to the statistics from a recent report [1], the autonomous car

Authors’ addresses: S. Tang, Y. Zhang, J. Zhou, Y. Guo and Y. Xue are with University of Science and Technology of China,
China, Z. Zhang is with Kyushu University, Japan, S. Liu is with Tianjin University, China, S. Guo is with Baidu Security,
USA, Y. Li is with Tsinghua University, China, L. Ma is with University of Alberta, Canada, and Y. Liu is with Nanyang
Technological University, Singapore.
Y. Xue is corresponding author, email: yxxue@ustc.edu.cn.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/6-ART $15.00
https://doi.org/XXXXXXX.XXXXXXX

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

 
 
 
 
 
 
2

S. Tang et al.

market was valued for 22 billion dollars in 2021. However, the state-of-the-practice ADS are
still vulnerable to numerous safety and security threats, due to either the complicated external
environments or deliberate attacks from various sources. These threats may lead to system failure,
which could bring catastrophic consequences and unacceptable losses. Despite the rapid progress
that has been made so far, safety assurance of ADS is still a major challenge to their full-scale
industrialization. Some recent news, e.g., the report of Tesla’s fatal accident [2], further highlights
the importance of the research in the safety assurance of automated driving.

In general, an ADS is composed of several modules for the functionalities of sensing, perception,
planning and control. The sensing module collects and preprocesses the environmental data using a
number of intelligent sensors, such as camera, radar, LiDAR, etc. The perception module extracts the
information from the sensors to understand the environmental conditions, such as road, obstacles,
traffic signs, etc. Based on the output of the perception module, the planning module generates
the optimal driving trajectories which the ADS are expected to follow. Lastly, the control module
sends the lateral and longitudinal control signals to drive the ADS along the planned trajectories.
In particular, some ADS adopt a special end-to-end design that integrates the perception, planning
and control functionalities in one module. These modules collaborate with each other and jointly
decide the behavior of the ADS; the abnormal function of any module can lead to system failures,
which severely threatens the safety of ADS.

Testing has been demonstrated to be an effective approach to exposing the potential problems
and ensuring the safety of systems. The testing of ADS is known to be extremely challenging,
due to the complexity and multidisciplinarity of ADS. In recent years, there have been a surge
of studies that focus on ADS testing. These published papers span over mainstream venues of
various domains, such as transportation venues (e.g., ITSC, IV), software engineering venues (e.g.,
ICSE, ASE, ISSTA), artificial intelligence venues (e.g., CVPR, AAAI), security venues (e.g., CCS,
USENIX Security), etc., which tackle the challenges in ADS testing from various perspectives (see
a detailed statistics and analysis in §8.1). Numerous testing approaches are proposed for solving
different problems, and numerous bugs and vulnerabilities are reported to facilitate the system
reengineering that repairs the existing problems and ensures the system safety.

To better understand the landscapes in ADS testing, there have been several surveys [3–5] that
summarize the recent advances in this field. In [3], Zhang et al. present a literature review on
the techniques to identify situations with the potential of causing safety risks, which they define
as Critical Scenarios Identification (CSI) methods, and they point out the necessity of combining
different CSI methods for safety assurance of the ADS. In [4], Zhong et al. conduct a review on
works about scenario-based testing in high-fidelity simulation, and discuss the gap between the
virtual environment and the real world. In [5], Jahangirova et al. propose a set of driving quality
metrics and oracles for ADS testing, and demonstrate the effectiveness of combining 26 best metrics
as the functional oracles.

Most of the existing surveys view the system as a whole and investigate the methodologies of
ADS testing from the system perspective. In that case, as a typical problem setting, ADS testing
consists in generating critical scenarios that lead to system failures, such as collision with obstacles;
moreover, because of the high cost of testing ADS in the real world, most of the studies in these
surveys adopt software simulators as the testing environments. While these surveys are useful,
they are not sufficient to show the comprehensive landscape of ADS testing. Indeed, since ADS are
complex and composed of multiple modules that differ from each other in technical design, their
testing should capture the features of different modules and address the challenges in different
domains. Moreover, on the system level, the testing should concern with the problems due to the
collaborations between different modules, and highlight the gaps between simulation-based testing
and real-world testing.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

3

Contributions. To bridge this gap, we conduct a survey on ADS testing that focuses on both
module-level testing and system-level testing. Specifically, on the module level, we reveal the
distinction of the testing techniques for different modules due to their different features; on the
system level, we focus on the challenges introduced by the corporations between different modules
and we also discuss on the different levels of realisticness of the testing environments.

In summary, the main contributions of the paper are listed as follows:

• we build a threat model for the general ADS based on the literature, which reveals the potential

safety and security threats for each module of an ADS;

• we survey the testing techniques for the different modules of ADS, and in particular we highlight
the technical differences in these testing techniques affected by the different properties of the
modules;

• we also survey the system-level testing techniques, but we focus on: i) the empirical studies
that show a bird’s-eye view of the systems without running them; ii) the system-level problems
due to the joint working of multiple modules; iii) the gaps between simulator-based testing and
real-world testing;

• based on our survey, we identify the challenges and potential research opportunities for ADS
testing, which facilitates the future investigation in this field, to strengthen the system safety
and reliability.

To the best of our knowledge, our work is the first one that unveils the intrinsic differences and
challenges in the ADS testing w.r.t. different modules; meanwhile, we give a specific emphasis on
the comparison between the currently popular simulation-based testing and the real-world testing.
Moreover, our analysis and discussion on the challenges and opportunities exhibit the landscapes,
and stimulate future research in this important field.

Paper organization. The rest of the paper is organized as follows: §2 overviews the background of
the ADS; §3 introduces a threat model of ADS; §4 describes the survey methodology, including the
detailed scope, collection process and collection results. The main results of this survey are in §5,
§6 and §7. In §5, we survey the literature of empirical study on ADS testing; in §6, we survey the
literature of techniques on module-level ADS testing; in §7, we survey the literature of techniques
on system-level ADS testing. We then show the statistics and analysis of the works in §8. We
summarize the challenges and potential research directions in §9. Lastly, we conclude this survey
in §10.

2 PRELIMINARIES
In this section, we introduce the background of ADS. We first provide an overview of the catego-
rization of ADS according to the levels of automation, from L0 to L5; then we show the general
architecture of ADS; lastly, we showcase three open-source ADS, with two of L4 and one of L2.

2.1 Overview of Automated Driving Systems
According to the complexity and variety of the ADS, the society of automotive engineers (SAE)
proposed the taxonomy and definitions of driving automation systems, known as J30161, which
has become a classification standard in recent years. As shown in Table 1, it categorizes driving
automation systems into six levels, ranging from no driving automation (Level 0) to full driving
automation (Level 5). These levels are usually referred to as L0 to L5.

The definitions of from L0 to L5 systems are as follows: (1) L0 systems only perform warnings
and momentary interventions, such as Lane Departure Warning (LDW), Automated Emergency
Braking (AEB), etc.; (2) L1 systems support steering or acceleration/deceleration for drivers; example

1https://www.sae.org/standards/content/j3016_202104/

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

4

S. Tang et al.

Table 1. Automation Levels and Definitions by SAE

Level Name

Description

Example

0

1

2

3

4

5

No Driving Automation

Driver Assistance

Drivers perform all of the dynamic driving
task (DDT)

Lane Departure Warn-
ing

The system performs part of the DDT: either
steering or acceleration/deceleration

Lane centering OR
ACC

Partial Driving Automation The system performs part of the DDT: steer-

ing and acceleration/deceleration

Conditional Driving Au-
tomation

Drivers or fallback-ready users need to be
receptive to ADS-issued requests

Lane centering AND
ACC at the same time

Traffic Jam Assist

High Driving Automation

Full Driving Automation

The system performs all of the DDT and
DDT fallback within a specified ODD

Local Driverless taxis

The system performs all of the DDT and
DDT fallback without ODD limitation

Full autonomous vehi-
cles

features include Automated Lane Centering (ALC), Adaptive Cruise Control (ACC), etc.; (3) L2 systems
perform steering and acceleration/deceleration at the same time, and a typical L2 system should
support both ALC and ACC; (4) L3 systems can execute responses to driving conditions within
Operational Design Domain (ODD), which is an operational restriction imposed to the ADS at the
design stage, but these systems require fallback-ready people to handle system failures; an example
is a traffic jam chauffeur; (5) L4 systems can further support the system fallback, and an example is
a local driverless taxi; (6) L5 systems can handle all driving conditions.

A system in L0 to L2 is also known as advanced driver assistance system (ADAS), since it is only in
charge of a part of the dynamic driving tasks (DDT), such as lateral control or longitudinal control,
and the safety of the whole vehicle still relies on drivers. In contrast, a system in L3-L5 performs
all of the DDT and drivers are not expected to interfere during the driving process, so it realizes
the real automated driving.

Note that there exist other identified synonyms of Automated Driving, e.g., autonomous driving,
self-driving, but in this paper, we follow the relevant terminology from the SAE J3016, in which the
term “ADS” refers to Automated Driving System. In literature, e.g. [4], an ADAS is usually referred
to as a system that belongs to L0-L2, while an ADS is referred to as a system that belongs to L3-L5.
In this work, since many testing techniques are independent of the automation levels of the systems
under testing, we sometimes mix the use of the terms and, by ADS, we refer to the systems over all
of the levels of driving automation.

2.2 Architecture of ADS
A common ADS is composed of four functional modules, namely, sensing module, perception
module, planning module and control module, as shown in Fig. 1. In the sensing module, intelligent
sensors (e.g., camera, radar and LiDAR) are used to collect the driving context from the physical
world. The perception module extracts useful environmental information from the sensor data,
and sends it to the planning module for motion planning. Based on the information, the planning
module generates the optimal driving trajectory. Finally, the control module outputs the control
signal to drive the vehicle along the trajectory. In the remainder of this section, we elaborate on
the architecture of a typical ADS.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

5

Fig. 1. The typical architecture of an ADS

Sensing module. By adopting various physical sensors, the sensing module takes charge of col-
lecting and preprocessing driving environment information from the physical world. The common
sensors used by an ADS include Global Positioning Systems (GPS), inertial measurement units (IMU),
cameras, radio detection and ranging (radar) and light detection and ranging (LiDAR). Specifically,
GPS provides the absolute position data (e.g., latitude, longitude and heading angle) while IMU
provides temporal data (e.g., acceleration and angular velocity). The combination of the two sensors
can provide more accurate real-time positioning of the autonomous vehicles. Cameras are used to
record and capture visual information on the driving road for the perception module, and radar
is used to detect obstacles by radio waves. Moreover, LiDAR has become indispensable to several
leading ADS (e.g., Apollo and Autoware) since it collects 3D point cloud data and processes
them with higher measurement accuracy. Compared with the camera sensors that are sensitive to
light conditions (e.g., shadows and bright sunlight), LiDAR sensors are more robust under these
environments and the generated 3D point cloud will be further utilized to build 3D models for the
surrounding objects.

Perception module. With the help of deep learning techniques, the perception module processes
sensor data from the sensing module to accomplish a series of perception tasks such as localization,
detection and prediction.
• Localization provides the real-time location of the ADS during the driving process. Furthermore,
localization is mostly implemented by fusing the data of GPS, IMU and LiDAR. Specifically, the
3D point cloud data of LiDAR is used to match the features stored in a High-Definition (HD) Map,
in order to determine the most likely location.

• Detection includes lane detection, traffic light detection and object detection. The data of camera
are often used for lane detection and traffic light detection, while the data of camera, radar and
LiDAR are often fused by several algorithms (e.g., extended Kalman filters [6] for object detection).
These detection tasks are mostly implemented by using deep neuron networks (DNNs) such as
faster RCNN [7] and Yolo v3 [8].

The prediction task also benefits from the perception module but it is mainly used for trajectory
planning, and thus it will be introduced next in the planning module.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

GPS/IMUCameraLiDARRadarSensingLocalizationDetectionPerceptionPredictionPlanningPlanningControlalgorithmsControlfunctionsControlEnd-to-End module6

S. Tang et al.

Planning module. By using DNNs and planning algorithms, the planning module takes percep-
tion data as input and makes decisions for the control module to control the vehicle. It has two
submodules, namely, the prediction submodule and the planning submodule.
• The prediction submodule estimates the future trajectories of the moving objects (e.g., vehicles
and pedestrians) detected by the perception module. For a given moving object, the possibility of
its path is often evaluated by machine learning algorithms, e.g., LSTM, RNN, etc.

• The planning submodule generates the optimal driving trajectory for ego vehicle based on the
prediction results. Specifically, this module is responsible for three tasks: route planning, behavior
planning and motion planning.
– Route planning selects the optimal path for the vehicle by using path algorithms, such as

Dijkstra, A*, etc.;

– Behavior planning makes decisions for the actions taken by the ADS, such as lane changing,

car following, etc., based on the system requirements and traffic rules;

– Motion planning generates velocity and steering angle plan which is locally optimal and

consider several factors, including safety, efficiency, comfort, etc.

Control module. Based on the trajectories planned by the planning module, the control module
finally takes charge of the longitudinal and lateral control of the vehicle. By using control algorithms
(e.g., proportional integral derivative (PID) control [9] and model predictive control (MPC) [10]), this
module generates appropriate control commands (e.g., steering, throttle and brake) and sends them
to the related hardware, i.e., the electronic control unit (ECU), via the protocol of controller area
network (CAN) bus. Note that, this module is critical for several functionalities provided by the ADS,
including Adaptive Cruise Control (ACC), Lane Keeping Assistance (LKA) and Automatic Emergency
Braking (AEB).

As can be seen from Fig. 1, besides the common modules mentioned above, there exists another
end-to-end design that combines the perception, planning and control process in one module. To be
specific, this module mainly consists of special deep learning models, which could directly output
the current control commands based on the information from the sensing module.

2.3 Three Open-Source Systems
In this section, we introduce two ADS, namely, Apollo, Autoware and one ADAS called OpenPilot,
and they are all commercial-grade open-source systems. Specifically, Apollo and Autoware are
L4 systems, and they are equipped with all the components and functionalities shown in Fig. 1. In
contrast, OpenPilot, as an L2 system, uses radar and cameras as the primary sensors, and several
functionalities, such as localization, in Fig. 1 are not available.

Apollo. Apollo has been a popular open-source ADS developed by Baidu since 2017; by Dec.
2021, it has been updated to version 7.0.0. The hardware platform of Apollo includes camera,
LiDAR, millimeter wave radar, Human-Machine Interface (HMI) device, etc., and currently the
communications over different components are managed by CyberRT2. The functionalities of
Apollo include cruising, urban obstacle avoidance, lane changing, etc.

Autoware. Autoware is another open-source L4 ADS developed by the research group of Nagoya
University in 2015. It is mainly applicable to urban roads, but it also suits highway and other road
conditions. By using the sensors introduced in §2.2, it supports a series of functionalities including
connected navigation, traffic light recognition, object tracking, etc. Unlike Apollo which uses
CyberRT, Autoware adopts ROS3 for the communications over different components.

2https://apollo.auto/cyber.html
3https://www.ros.org/

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

7

Fig. 2. Threat model of ADS

OpenPilot. OpenPilot is a popular open-source L2 ADAS developed by Comma.ai and it has
been updated to version 0.8.12 up to Dec. 2021. OpenPilot supports common L2 features, such as
adaptive cruise control (ACC), Automated Lane Centering (ALC), Forward Collision Warning (FCW),
etc. Unlike other L2 ADAS, OpenPilot has high portability—it can be compatible with more than
120 types of vehicle models by using related hardware set (e.g., Car Harness4 and Comma Two5.

Up to now, the above three systems have been widely adopted for commercial usage in practice.
Moreover, they all have their own project repositories in GitHub, which provides a platform for
developers and users to discuss on the problems they encounter during the development and
deployment. Based on the open issues from their repositories and recent surveys [11, 12], we
present the threat model of general ADS in §3.

3 THREAT MODEL OF ADS
In this section, we introduce the threat model of a typical ADS. Fig. 2 shows the overview of the
threats on each component of an ADS, which is introduced in §3.1.

3.1 The Threat Model
In order to establish the threat model, we collect potential threats to the safety and security of ADS
from various sources. These sources include the issues from the repositories of the open-sourced
ADS and the research outcomes of existing literature. Our threat model is depicted in Fig. 2.

Threats to sensing. Physical sensors are critical hardware used in an ADS for collecting the
information of the external environments. First, harsh weather conditions such as foggy and snowy
weather could reduce the capabilities of the intelligent sensors. Besides that, physical noise or fake
signal data, namely, jamming attack [13–16] and spoofing attack [13, 17–19] (see details in §6.1.2)
may also exist in the driving environment, and could interfere these sensors and harm their normal
functionalities.

4https://comma.ai/shop/products/comma-car-harness
5https://comma.ai/shop/products/two

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

Jamming/Spoofing AttackAdversarial ExamplesCloud ServiceDenial of ServiceImproper localizationImplementationCapabilityInterfaceMismatchUnwanted Maneuver of NPCsEmergency SituationHarsh WeatherThreatsADS ModulesExternalServiceSensingPerceptionSensor DataPlanningPerception DataControlPlanning DataTrojan AttackTraining Database8

S. Tang et al.

Threats to perception. First, one threat comes from adversarial examples that are generated by
adding perturbations to normal images, which can fool the deep learning models in the perception
module to make incorrect predictions, as shown by [20–29]. Second, another type of threat is called
Trojan attack [30–33], in which malicious data are injected into the training data of the deep learning
models. The injected data usually contain specific patterns that can trigger misclassifications of
the deep learning models. Moreover, in the case when the ADS requires a HD map from the cloud
service, Denial of service (DoS) [34] or fake HD map data [35] can interfere the perception tasks
such as localization.

Threats to planning. Having the data coming from the perception module, the planning module
takes charge of several tasks, e.g., object trajectory prediction, path planning. During the process, a
threat comes from the unwanted maneuver of non-player characters (NPCs), which can interfere
the prediction for moving objects and thus lead to an unsafe trajectory plan. Moreover, improper
localization from the perception module can also threaten the accuracy of output trajectories.

Threats to control. The control module calculates the control signal for lateral and longitudi-
nal control of the vehicle, based on the planned trajectories. One major threat comes from the
implementation capability of the system, which concerns about whether the trajectories can be
implemented by the control module in real time. Another threat happens when an emergency
situation is encountered, e.g., when an emergency brake is needed, and the control module may
fail to handle these cases. Moreover, the output signals are sent via the CAN bus to the ECU for
controlling the vehicle. Since this process involves a data transmission between software and
hardware, a potential threat is the interface mismatch, e.g., an inappropriate steering angle rate, in
the practical usage.

3.2 Comparison of the Threats Faced by the Open-Source ADS
For all of the three open-source systems (namely, Apollo, Autoware and OpenPilot) introduced
in §2.3, they face some common threats, such as harsh weather, jamming/spoofing attack, and
unexpected NPCs’ behaviors, etc. Since OpenPilot is a L2 ADAS, while Apollo and Autoware are
typical L4 ADS, they suffer from different threats owing to their different features. On the one hand,
due to the fact that OpenPilot uses cameras and radar as its primary sensors, those threats that
are against specific sensors such as LiDAR, will not threaten OpenPilot. Also, unlike Apollo and
Autoware, OpenPilot does not require cloud service, and thus the threats from the cloud service,
such as DoS or fake GPS data, do not threaten it. On the other hand, OpenPilot suffers from the
threats sourcing from the interface with different vehicle models, such as interface mismatch, more
than Apollo and Autoware, as it is used as an add-on for different vehicle models.

4 PAPER COLLECTION AND REVIEW SCHEMA
This section introduces the scope of the survey, paper collection methodology and the results of
our paper collection.

4.1 Survey Scope
An ADS is composed of both software and hardware component; both of them are closely related
to the safety of the whole system. Hence, the scope of this survey includes the testing of both the
software and the hardware aspects of ADS. We collect papers according to the following inclusion
criteria.
• The papers that perform an empirical study or static analysis related to the safety of ADS;
• The papers that propose a method for testing the modules of ADS or the whole system;

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

9

• The papers that provide a dataset or simulator adopted for testing the modules of ADS or the

whole system;

• The papers that introduce metrics as test oracles or adequacy criteria for testing the modules of

ADS or the whole system.
Specifically, empirical study refers to the literature on the empirical analysis on the system or
the bugs/issues, without running system simulation; test oracle refers to the metrics that measure
whether an ADS or its components misbehaves; test adequacy refers to the criteria that judge
whether a test suite has been sufficient for testing. The detailed studies are presented in §5, §6 and
§7, respectively for empirical study, module-level testing and system-level testing.

4.2 Paper Collection Methodology
We perform the literature collection with the same search string in three electronic databases,
namely, Google Scholar6, DBLP7 and IEEE Xplore8. The search string combines two components,
namely, the identified synonyms of “automated driving (AD)”, and approaches for quality assurance
of ADS. We optimize the search string in an iterative manner, in order to collect as many related
papers as possible. The final search string used during our search process is as follows:

Search string = ∗AD + ∗QA

where ∗AD denotes the identified synonyms of automated driving, such as automated vehicle,
autonomous driving, self-driving vehicle, etc.; ∗QA denotes the identified synonyms of quality
assurance, such as test, validation, evaluation, etc.

Moreover, we take the following two steps to filter out irrelevant papers and acquire more

relevant papers:
Step 1 Abstract analysis. For each paper hit by the search strings, we ensure the relevance to
the safety aspect of autonomous driving. Hence, we perform a manual analysis of the
obtained abstracts to filter the papers. In this process, we adopt the inclusion criteria which
is introduced in §4.1. Moreover, we exclude the papers in which the target systems are other
systems, e.g., unmanned aerial vehicle (UAV), autonomous underwater vehicles (AUV), rather
than automated driving systems.

Step 2 Full-text analysis. We then proceed with downloading the papers and analyzing the full
text. First, we perform snowballing [36] on each of the remaining papers after abstract
analysis. Concretely, we iteratively check the sections of related work in these studies and
perform Step 1 on the new papers, until the closure of the collected papers. Second, we
extract features (e.g., test objective, test methodology) of each paper in our collection. For a
more accurate analysis of these studies, each paper is randomly assigned to 2 authors for
assessment, while each author shares a document with the label of the extracted features.
After all of the authors complete their tasks, we conduct a consensus meeting to examine
the features of each paper in our collection. The details of the extracted features will be
introduced in §6 and §7.

4.3 Paper Collection Results
Table 2 shows the results of our paper collection. Since the papers collected from Google Scholar
and IEEE Xplore are subsets of those from DBLP, we only present the results of DBLP. As a result,

6https://scholar.google.com/
7https://dblp.org/
8https://ieeexplore.ieee.org/

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

10

S. Tang et al.

Table 2. The results of paper collection

AD

QA

Hits

Filter

automated vehicle
automated driving
autonomous car
autonomous vehicle
autonomous driving
self-driving car
self-driving vehicle
driving/driver assistance/assistant
ADS modules

test/attack/verification/
evaluation/validation/bug/analysis

query

snowball

total

238
187
85
161
230
40
24
117
162

-

-

9
5
1
20
11
2
0
1
9

58

45

1244

103

we collected 1244 papers across five research areas till Dec. 2021. By a manual review of the 1244
abstracts, we identified 58 papers relevant to our study. Snowballing on the obtained 58 papers
results in 45 relevant papers, we determine the final list of 103 papers in our collection set.

5 LITERATURE OF EMPIRICAL STUDY ON ADS TESTING
In this section, we provide an overview on the papers that perform empirical study in the field
of ADS testing. By the term of empirical study, we mean that, instead of executing the systems
in a simulated or real-world environment, these studies perform empirical analysis based on
existing database, such as project repositories, public accident reports, etc., related to the ADS. In
general, empirical study is an essential step before the experimental ADS testing, since it provides
experiences and insights in the distribution of potential safety risks.

We classify these studies into three categories, namely, system study, bug/issue study and acci-
dent/disengagement report study. System study, shown in §5.1, mainly analyzes the architectures
of ADS and thus is beneficial for understanding the system behavior before running it. Bug/issue
study, shown in 5.2, focuses on collecting and analyzing the bugs and issues of the ADS, which
are usually raised by users, developers and researchers and published in project repositories. Acci-
dent/disengagement report study, shown in §5.3, refers to the analysis on those real-world accidents
and disengagements reported in various databases (e.g., the accident reports released by California
Department of Motor Vehicles (CADMV) [37]). These reports target at real-world system failures,
and they provide important references for understanding the system reliability in the real world.

5.1 System Study
Because of the high complexities of the system architectures of ADS, it is necessary to have a
comprehensive understanding of the systems before performing their evaluation. The works on
system studies, e.g., [38] on Apollo, build the logical architectures that disclose the connections
over different modules in ADS. As a result, these lines of works can bring insights into the potential
vulnerabilities and suggest useful metrics for system testing.

In [38], Peng et al. investigate on the collaboration between the code and the DNN models in
Apollo; specifically, they study which roles are played by the code and the underlying DNN models,
respectively. They find that the 28 DNN models used in Apollo interact with each other in diverse
ways, e.g., the output of one DNN can be used as the input of another DNN, and the outputs of

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

11

multiple DNNs can be combined as the input of another DNN. Moreover, the code also plays an
important role in the system workflow, e.g., it can be used for filtering out invalid output of DNNs,
and it can complement the imperfect outcome of DNNs.

5.2 Bug/Issue Study
For those open-source ADS, issues and bugs reported in their public repositories (e.g., GitHub)
reflect real problems encountered by users and developers during the development and deployment.
Therefore, systematic analysis [11, 12] on these issues can provide helpful insights into the root
causes of the system failures.

In [11], Garcia et al. present a comprehensive study of bugs in two ADS, namely Apollo and
Autoware. Specifically, they collect bugs from the commits across the Apollo and Autoware
repositories on GitHub and perform a manual analysis on these bugs and commits. As a result, they
obtain 13 root causes (e.g., algorithm, data, memory) for system crash, 20 symptoms (e.g., speed
and velocity control, vehicle trajectory) and 18 bug-related components (e.g., perception, planning,
control), based on their analysis on 499 bugs of the two ADS.

In [12], Tang et al. perform a study on issue analysis for OpenPilot. They collect 235 bugs from
1293 pull requests and 694 issues of the OpenPilot project in GitHub and Discord9. These bugs
are then classified into 5 categories, including (DNN) model bugs, plan/control bugs, car bugs,
hardware bugs and UI bugs. Among these different types of bugs, they find that the car bugs related
to the interface with different car models dominate 31.48%, and plan/control bugs related to the
control of car behaviors account for 25.95%.

5.3 Disengagement/Accident Report Study
The following works [39–46] all perform analysis on the reports from the California Department of
Motor Vehicles (CADMV) [37], which involve disengagement and accident records on public roads.
Specifically, a disengagement refers to a failure that requires human driver to take over the control
of vehicles; an accident refers to a collision with other traffic participants. These empirical studies
investigate the relevant factors, such as the causes, the correlations and the impacts of these system
failures; they also shed a light on the future system developments.

Analysis of disengagement reports. In the works [39, 40, 42], the authors analyze the disen-
gagements and classify them based on different metrics. In [39] and [40], the authors classify the
disengagements according to their root causes, and so the resulting categories include factors, such
as system failure, human factor, weather condition, etc. In contrast, Wang et al. [42] attribute the
disengagements to different modules of ADS, namely, perception issues, planning issues and control
issues. Unlike the works above, Boggs et al. [41] apply the binary logistic regression [47] to catego-
rize the cause of the disengagements in more details. The results show that the planning discrepancy
(e.g., improper localization, motion planning) accounts for 41% of ADS disengagements.

Analysis of accident reports. The following works [43–46] identify the common accident types
from accident reports. In [43], Alambeigi et al. utilize probabilistic topic models to identify common
crash patterns, e.g., driver-initiated transition crashes, sideswipe crashes during left-side overtakes.
Boggs et al. [44] and Leilabadi et al. [45] apply text analysis to the accident reports, and they find
that the accidents mostly occur when vehicles run in the automated mode, and the most frequent
ADS crash type is the rear-end collision. In [46], Song et al. conclude that the most representative
accident pattern is the “collision following ADS stop”, i.e., an ADS stops suddenly and gets hit by

9https://discord.com/

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

12

Table 3. Summary of the papers for empirical study on ADS testing

Category

System Study

Bug/Issue Study

Description

Introducing the interaction between the code and the DNN models in
Apollo

Finding the root causes, symptoms and bug-related components based on
analysis on bugs of Apollo and Autoware
Performing categorization and analysis on bugs of OpenPilot

Disengagement/Accident
Report Study

The analysis and classification of the disengagements based on different
perspectives (e.g., modules)

The identification of the common accident types by different methods (e.g.,
text analysis)

S. Tang et al.

Literature

[38]

[11]

[12]

[39–42]

[43–46]

other vehicles on the road. Moreover, the probability by which a disengagement can be transited to
a collision is 68%.

5.4 Discussion
Table 3 summarizes the collected papers for empirical study on ADS testing. It can be seen that
there exists only system study for Apollo while bug/issue studies cover the three open-source ADS
which we mentioned in §2.3. In addition, more existing works [39–46] perform empirical studies
on the disengagement and accident reports for identifying the root causes or failure types, since
these investigations are critical to understand the ADS safety performances in the real world.

6 LITERATURE OF TECHNIQUES ON MODULE-LEVEL ADS TESTING
In this section, we introduce the works on module-specific testing for ADS. These modules under
test include the ones that have been introduced in §2.2, namely, the sensing module (in §6.1), the
perception module (in §6.2), the planning module (in §6.3), the control module (in §6.4) and the
end-to-end module (in §6.5).

We introduce these studies from three perspectives, namely, test methodology, test oracle and
test adequacy. Concretely, (i) test methodology introduces various methods or technical innovations
for testing; (ii) test oracle defines metrics that can be used to judge whether the module behaves
correctly; (iii) test adequacy proposes coverage criteria that tell if a test suite is sufficient. Note that,
since different modules have different features and face different technical challenges, there exists
the case where, in a specific module, not all of the three perspectives are applicable.

6.1 Sensing Module
The sensing module is the frontier module of an ADS and the performance of the physical sensors
(e.g., camera, radar, LiDAR) in this module is critical to the safety and security of the whole ADS.
Relevant studies on the test methodology of this module can be divided into physical testing (shown
in §6.1.1) and deliberate attacking (shown in §6.1.2). Physical testing aims to test the performance
of the sensors under different physical conditions, while deliberate attacking interferes the input
signals of the sensors to diminish the sensing quality.

6.1.1 Physical Testing. Physical testing [48–50] aims to assess the sensors’ capabilities of handling
specific tasks under different physical environments, such as various harsh weather conditions.
In [48], Kutila et al. perform a detection distance testing of LiDAR in the foggy and snowy conditions.
The results show that the maximum measurable distance by the LiDAR decreases for 20 − 40𝑚
under harsh weather conditions. They also compare the detection capability of LiDAR with different

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

13

wavelengths under harsh weather in their follow-up work [49]. Concretely, they test the detection
accuracy of LiDAR at 905𝑚𝑚 and 1550𝑚𝑚 wavelengths in foggy and rainy weather, and the results
indicate that the LiDAR with a larger wavelength can detect the environment more accurately
when the visibility is low. Another work [50] performs a LiDAR performance testing on detecting
pedestrians under different weather conditions. The results show that the detection failure of LiDAR
sensor increases 33% under rainy weather.

6.1.2 Deliberate Attacking. Unlike physical testing, deliberate attacking refers to the intentional
attacks launched by human attackers. Deliberate attacking on the sensors of ADS can be classified
into jamming attack and spoofing attack, which are introduced below.

Jamming attack. This is a basic type of attack on sensors by generating noises using specific tools
to interfere the sensors and damage their normal functionalities. Some studies [13, 14] use light as
the attack source. Petit et al. [13] present a remote attack on the camera sensors of the ADS. They
show that the camera can be blinded under the emission of intense light. Shin et al. [14] propose
a blinding attack method against LiDAR. They utilize intense light with the same wavelength as
LiDAR, to achieve the attack. Consequently, the LiDAR fails in perceiving objects from the direction
of the light source. Other studies [15, 16] use specific hardware to interfere the sensors. In [15],
Yan et al. present a contactless attack method on different sensors. They utilize a laser to trigger
temperature damages on cameras and an ultrasonic jammer to launch attacks on ultrasonic sensors
and radars. In [16], Lim et al. propose a method to simulate the attacks on ultrasonic sensors, by
placing an ultrasonic sensor opposite to the target sensor. As a result, the target sensor receives
both the echo (i.e., the reflected wave) of itself and the echo of the other sensor, which leads to an
incorrect detection of obstacles.

Spoofing attack. Spoofing attack is performed by injecting fake data to deceive sensors. The
following works [13, 17–19] generate fake signals to the target sensors. In [13], Petit et al. deceive
the LiDAR by resending the original signal again from another position. As a result, the LiDAR
detects two ghost walls in different locations. In [17], Meng et al. propose a GPS spoofing generator,
using a vector tracking-based receiver. In [18], Zeng et al. propose an attack method against road
navigation systems to trigger the fake turn-by-turn navigation for guiding the victim to a wrong
destination. By slightly shifting the GPS location, the fake navigation route matches the shape
of the actual roads. In [19], Nassi et al. propose an attack approach that projects the adversarial
signs on the driving ADS. The results show that Mobileye [51], which is their target ADS, fails to
recognize the spoofed traffic sign as a real traffic sign.

6.1.3 Discussion. Table 4 shows the summary of the papers for the sensing module testing and
the corresponding threats have been shown in Fig. 2. It can be seen that the existing physical
testing works [48–50] mainly focus on testing LiDAR sensors under different weather conditions,
e.g., the foggy weather, the snowy weather. This is because the LiDAR sensor has become a key
component in ADS and its robustness is of great significance to the vehicle’s safety. Besides, we
find more works that perform deliberate attack including jamming attack [13–16] and spoofing
attack [13, 17–19] on physical sensors. With the usage of specific devices, e.g., laser [15], fake GPS
signal generator [17, 18], these two types of attacks have been demonstrated to be effective for
finding abnormal behaviors of the related sensors.

6.2 Perception Module
The perception module receives and processes sensor data; based on which, it perceives external
environments. The literature we collected includes the test methodologies (shown in §6.2.1), the

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

14

S. Tang et al.

Table 4. Summary of the papers for the sensing module testing

Methodology

Description

Literature Test Sensor

Physical test

Testing the detection distance of LiDAR under differ-
ent weather conditions

[48–50]

LiDAR

Deliberate
attack

Intense light blinding attack

Jamming
attack

Spoofing
attack

Using jamming waveform to interfere the sensors

Using a laser to cause temperature damage
Using echo to interfere the sensors
Resending the original signal from another position
Injecting fake GPS signals by a spoofing generator
Projecting adversarial traffic signs

[13]
[14]

[15]

[16]
[13]
[17, 18]
[19]

Camera
LiDAR
Ultrasonic sensor

Camera
Ultrasonic sensors
LiDAR
GPS
Camera

test oracles (shown in §6.2.2), and the test adequacy criteria (shown in §6.2.3) for testing DNN
models used in the perception module of ADS.

6.2.1 Testing Methodology. Adversarial attack is the major approach for testing of the DNN models
used in the perception module, which attempts to generate adversarial examples to trigger wrong
inference results of perception. In general, there are three basic methods for performing adversarial
attack, namely, by solving an optimization problem, by leveraging the generative adversarial networks
(GAN) [52] and by poisoning the training data. In the following, we introduce the literature that
adopts these methods.

Optimization-based attack. We denote by 𝐹 a DNN model, which takes as input a picture 𝑥
and gives as output a label 𝑦. In general, an adversarial attack consists in solving the following
optimization problem:

min 𝛿

𝑠.𝑡 .

𝐹 (𝑥 + 𝛿) = 𝑦∗,

𝑦∗ ≠ 𝑦𝑜

(1)

where 𝛿 is a perturbation added to the picture 𝑥, and 𝑦∗ is a wrong label that is different from the
correct label 𝑦𝑜 . In other words, an adversarial attack involves finding the minimum perturbation
that misleads a DNN model to make a wrong inference. Mostly, the collected literature on adversarial
attack follows this general framework; meanwhile, these papers differ in their applications and
motivations.

Adversarial attacks on camera-based object detectors (e.g., YOLO V3 [8], Faster R-CNN [7],
etc.) have been studied by the works [20, 22]. Chen et al. [20] propose an attack method, called
ShapeShifter, to generate perturbations against the object detector Faster R-CNN [7]. To enhance the
robustness of the perturbations, they adopt the Expectation over Transformation technique [53] that
adds random distortions iteratively, during the optimization process for generating perturbations.
In another work [22], Zhao et al. propose two approaches for generating adversarial perturbations:
one is called hiding attacks that can make object detectors unable to recognize objects; and the
other is appearing attack that can lead the object detectors to making incorrect recognition.

In addition to attacking the camera-based object detectors, there are also works [21, 23, 25] that
focus on attacking the LiDAR-based object detectors. Cao et al. [23] present a white-box attacking
method on a LiDAR-based perception module by adding the spoofed points into the original 3D
point clouds. Sun et al. [25] introduce a black-box attacking method on the LiDAR-based detectors,
and they find that the target models tend to ignore the occlusion information in the LiDAR point

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

15

clouds. Cao et al. [21] propose an attacking method that can generate an adversarial 3D object to
fool both the camera and the LiDAR-based perception algorithms.

Semantic segmentation is another important task of the perception module, and it can also be
attacked by proper adaptations of the optimization-based framework in Eq. 1. Xu et al. [26] perform
an adversarial attack on the popular segmentation model DeepLab-V3+ [54]. The perturbations
generated by [26] are very stealthy since they are small and can be projected to an unnoticed area
in the original image.

While the adversarial attack framework in Eq. 1 is effective in fooling DNN models, it does not
consider the realisticness of the generated perturbed pictures. There is a line of works [24, 27] thus
considering the adversarial attack problem under physical conditions or with the aim of being
realistic. Eykholt et al. [24] propose an attacking method, called Robust Physical Perturbations (𝑅𝑃2),
that makes the object classifiers for road sign recognition produce wrong classification results
under real-world physical conditions, e.g., different viewpoint angles and distances to the signs.
The experiment results show that the attacked classifier misclassifies physical traffic signs with a
success rate of 100% in the lab environment and 84.8% in the real world. Dreossi et al. [27] generate
realistic image perturbations by defining the term of modification space, which imposes constraints
that limit the space of the perturbations.

GAN-based attack. This type of attack [55] generates adversarial perturbations to fool a DNN
model by training a GAN [52]. A GAN consists of two neural network models, namely, a generator
𝐺 and a discriminator 𝐷; specifically, 𝐺 is used to generate perturbations and add them to an input
image, and 𝐷 is used to distinguishing the generated image by 𝐺 and the original image. The
objective of training a generator 𝐺 is to make the perturbed image of 𝐺 indistinguishable by the
discriminator 𝐷; this can be implemented by optimizing a loss function 𝐿𝐺 . For fooling the target
DNN, another loss function 𝐿𝐷 is needed to stimulate the adversarial images produced by the GAN
to be misclassified. As a result, the final objective function is formalized as follows:

𝐿 = 𝛾 · 𝐿𝐺 + 𝐿𝐷

where 𝛾 is a parameter that controls the relative importance of 𝐿𝐺 and 𝐿𝐷 .

Liu et al. [28] propose a GAN-based attack framework called perceptual-sensitive GAN (PS-GAN),
which generates adversarial patches with high visual fidelity. Experiment results show that the
adversarial patches can significantly reduce the classification accuracy of the target DNNs. Xiong
et al. [29] propose a multi-source attack method based on GAN, which generates adversarial
perturbations that can fool both camera-based and LiDAR-based perception models.

Trojan attack. This type of attack [31] is also called poisoning attack or backdoor attack. Specifically,
it works by injecting malicious samples with trigger patterns into the training data of the target
DNN models. Then the models can learn the malicious behaviors and make incorrect predictions
when the inputs contain such triggers. The following works [30, 32, 33] we collected are all based
on this idea.

Trojan attack on CNN classifiers for traffic sign recognition is evaluated in [30], and the success
rate of the attacks by the triggers can reach 92% while the malicious samples only account for 3%
of the training data. Another Trojan attack [32] on CNN classifiers works by introducing physical
perturbations into the training environment, without modifying the training data such as the
ground-truth labels.

Ding et al. [33] propose the Trojan attack for deep generative models such as DeRaindrop
Net [56], which is a GAN-based network for raindrops removal. Experiment results show that the
model could be triggered to misclassify the traffic light or the value on the speed limit sign when it
normally removes the raindrops.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

16

S. Tang et al.

6.2.2 Test Oracle. A test oracle defines a metric used to distinguish the correct behavior and
unexpected behavior of the system under testing. Sometimes, an oracle is obviously identified;
however, that is not always the case. In the perception testing, due to the huge input space (that
involves all the possible input images) of the DNN models, it is a great challenge to specify the
oracles for all the input images. We collect several types of test oracles that have been adopted for
perception testing, namely, ground-truth labeling [57], metamorphic relations [58, 59] and formal
specifications [60, 61] to judge whether a bug exists in the perception module.

Ground-truth labeling. The general approach of testing a DNN in the perception module is, given
an image, to match the inferred label by the DNN with the ground-truth label. Usually, these ground-
truth labels are obtained by manual labeling. For instances, the ground-truth labels in [62, 63]
are produced in this way. However, manual labeling is notoriously expensive and laborious; to
that end, automatically labeling methods are pursued by researchers. Zhou et al. [57] propose an
automatic labeling method to detect the road component in the camera sensored images. Their
method identifies the road component in the 3D point cloud captured by a LiDAR for the same
scene, and project the identified area to the corresponding image. The projected area labels the
road component in the camera sensored images, which can be used for the validation of semantic
segmentation models.

Metamorphic relationship. Metamorphic relationship [64] was introduced by Chen et al. to tackle
the problem when the test oracle is absent in traditional software testing. Consider the testing of a
program 𝑓 that implements the trigonometric function sin. Normally, for any input 𝑥, given the
ground-truth value sin(𝑥) as the oracle for 𝑓 (𝑥), we can assess the correctness of 𝑓 by checking
if 𝑓 (𝑥) = sin(𝑥). However, assume that the ground-truth value sin(𝑥) is unknown. In this case,
testing 𝑓 by checking if 𝑓 (𝑥) = sin(𝑥) is not possible; instead, we can use the metamorphic testing
that tests the program based on a metamorphic relation. For instance, in this case, a metamorphic
relation can be built as 𝑓 (𝑥) = 𝑓 (𝜋 − 𝑥), due to the property sin(𝑥) = sin(𝜋 − 𝑥) held by sin.
Hence, the correctness of 𝑓 can be assessed by metamorphic testing, which consists in checking if
𝑓 (𝑥) = 𝑓 (𝜋 − 𝑥), for any input 𝑥.

Metamorphic testing has been studied for testing the perception module of an ADS; various

metamorphic relations have been proposed, over images [58] and frames in a scenario [59].
• Metamorphic relations over images. Shao et al. [58] introduce a metamorphic relation in object
detection, that is, the detected object in the original images should also be detected in the synthetic
images.

• Metamorphic relations over frames in a scenario. In [59], Ramanagopal et al. propose two meta-
morphic relations, respectively for identifying temporal and stereo inconsistencies that exist in
different frames of a scenario. The temporal metamorphic relation says that an object detected
in a previous frame should also be detected in a later frame; the stereo metamorphic relation is
defined in a similar way, for regulating the spatial consistency of the objects in different frames
of a scenario.

Formal specifications. Recently, temporal logics-based formal specifications are adopted in mon-
itoring of the perception module of ADS. In general, temporal logics are a family of formalism
used to express temporal properties of systems, e.g., an event should always happen during a
system execution; flagship temporal logics include linear temporal logic (LTL) [65], metric temporal
logic (MTL) [66], etc. In [60], Dokhanchi et al. propose an adaptation of temporal logic to express
desired properties of perception; the new formalism is called Timed Quality Temporal Logic (TQTL).
Specifically, TQTL can be used to express temporal properties that should be held by the perception
module during object detection, e.g., “whenever a lead car is detected at a frame, it should also be

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

17

detected in the next frame”. Conceptually, the properties expressed by TQTL are similar to the ones
in [59]; however, by adopting such formal specification to express these properties, one can synthe-
size a monitor that automatically checks the satisfiability of the system execution. TQTL is later
extended to Spatio-Temporal Quality Logic (STQL) [61], which has enriched syntax to express more
refined properties over the bounding boxes used in object detection. The authors also propose an
online monitoring framework, named PerceMon, for monitoring the perception module at runtime
of the ADS.

6.2.3 Test Adequacy. Measuring the adequacy of the testing for DNN models in the perception
module is challenging, due to the complexity of DNN models. Compared to program execution, DNN
inference involves a completely different logical process, which is deemed to be non-interpretable.
In this domain, various metrics, analogous to the test adequacy criteria for programs, have been
proposed; some of the metrics are for general DNN testing, while some are dedicated to ADS testing.
Below we introduce two typical lines of such adequacy criteria for testing of the DNN models in
perception.

Structural coverage. Neuron coverage is proposed in [67], inspired by the structural coverage
used in traditional software testing. In [67], the authors analogize DNN inference to program
execution, and consider the neuron activation as a symbol that indicates whether a neuron is
“covered”. Based on this analogy, they define neuron coverage by what percentage of the neurons
that are activated, as the counterpart of structural coverage in DNN. Inspired by [67], a number of
other neuron coverage criteria are proposed. For instance, k-multisection neuron coverage [68] is
the refined version of neuron coverage that considers not only “activated” neurons but also “not
activated” neurons; surprise adequacy [69] pursues the novelty of an individual test case based on
whether it is out of the distribution of the training data.

Combinatorial coverage. Combinatorial testing [70] utilizes combinatorial coverage for test case
generation, which measures the coverage of the combinations of different system parameters.
The 𝑡-way combination coverage is a typical criterion, which is defined by the number of the
𝑡-wise combinations covered by the test suite, out of the whole number of the possible 𝑡-wise
combinations. For instance, consider a system that has 3 binary parameters 𝑎, 𝑏 and 𝑐. Given a test
suite 𝑇 = {⟨0, 0, 1⟩, ⟨0, 1, 0⟩, ⟨1, 0, 0⟩, ⟨1, 1, 0⟩} that involves 4 test cases, the 2-way combination of
𝑇 is computed by 1/3, which indicates that one combination 𝑎𝑏 is covered by 𝑇 (since 𝑇 involves all
the possible cases 00, 01, 10, 11 of 𝑎𝑏), over all the three possible combinations 𝑎𝑏, 𝑎𝑐, and 𝑏𝑐.

Combinatorial coverage has been used to solve the oracle problem in the testing of the perception
module. In [71], Gladisch et al. characterize the scenarios by using multiple parameters concerning
different features, such as lane types, road types, etc. They then apply combinatorial coverage as a
guidance to generate test cases that can reveal system failures and achieve high coverage. In [72],
Cheng et al. propose 𝑘-projection coverage that aims to reduce the combinatorial explosion during
the test case generation, by incorporating domain expertise.

6.2.4 Discussion. Table 5 summarizes the collected papers for testing the perception module. This
module includes a number of DNN models for understanding the environmental information;
affected by their vulnerabilities, many crashes are caused by this module [11]. It can be seen that
a large number of papers perform adversarial attack on this module including three categories,
namely optimization-based attack, GAN-based attack and Trojan attack. The first two methods
could generate adversarial examples to fool the DNN models in use while the third method focuses
on the training process of the DNN models, as shown in Fig. 2. The types of the tasks cover various
deep learning models for different functions of perception, including object classifiers [24, 28],
semantic segmentation models [26] and camera-/LiDAR-based object detectors [20–23, 25, 29]. Since

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

18

Table 5. Summary of the papers for the perception module testing

Methodology Description

Literature

Task type

S. Tang et al.

Environment
setting

Optimization-
based

GAN-based

Trojan Attack

Replacing true traffic signs with gen-
erated adversarial traffic signs

Generating transferable adversarial
traffic signs and stickers
Adding spoofed points into the origi-
nal 3D point clouds
Pasting generated adversarial stick-
ers on traffic signs
Performing the black-box attack by
the occlusion information
Generating
images
adversarial
against multi-sensor fusion based
perception
Adding perturbation on the unno-
ticed area

Imposing constraints on image per-
turbation

Generating adversarial patches with
high visual fidelity
Proposing a multi-source attack
method

Evaluating the attack effectiveness on
traffic sign recognition task
Introducing physical perturbations
into the training environment
Performing the Trojan attack for
models used for raindrops removal

[20]

[22]

[23]

[24]

[25]

[21]

[26]

[27]

[28]

[29]

[30]

[32]

[33]

Object detector: Faster-RCNN[7]

Real world

Object detectors: Faster-RCNN[7]
and Yolo v3 [8]
A LiDAR-based perception sys-
tem [73]
Classifiers: LISA-CNN [74], GTSRB-
CNN [75] and Inception-v3 [76]
Perception module
in Apollo,
PointRCNN [77]and PointPillars [78]
Perception module in Apollo

Real world

Simulation

Digital
dataset
Digital
dataset
Simulation

Segmentation models:ResNet101 [79]
and MobileNet
[80] based on
DeepLab-V3+[54]
Classifier: SqueezeDet
Yolo [82]

[81] and

Classifiers: VGG16 [83], ResNet [79]
and VY [84]
Semantic segmentation model: VAE-
GAN [85]

Classifier: LeNet-5 [86]

Classifier: ResNet18 [79]

Claffifiers: DeRaindrop Net [56] and
RCAN [87]

Digital
dataset

Digital
dataset

Digital
dataset
Digital
dataset

Digital
dataset
Digital
dataset
Digital
dataset

Oracle

Description

Ground-truth labeling

Generating road label automatically by LiDAR

Metamorphic relationship

The detected object in the original images should also be detected in the synthetic
images
The object detected in a previous frame should also be detected in a later frame

Formal specifications

Adapting TQTL to express desired properties of perception
Adapting STQL to express more refined properties

Adequacy

Description

Structural coverage

Neuron coverage: the percentage of the neurons are activated
K-multisection neuron coverage: considering activated neurons and not activated
neurons
Surprise adequacy: the novelty of a test case based on its range in the training
data distribution

Combinatorial coverage

Characterizing the scenarios by multiple parameters
Incorporating domain expertise to reduce the combinatorial explosion

Literature

[57]

[58]

[59]

[59, 60]
[61]

Literature

[67]
[68]

[69]

[71]
[72]

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

19

the generated adversarial perturbations may not be effective in a noisy physical environment [88],
a number of methods (e.g., Robust Physical Perturbations [24]) are proposed to overcome this
challenge.

As mentioned in §6.2.2, it is also a great challenge to judge the correctness of the output of the
perception module. We collect three types of approaches including ground-truth labeling [57],
metamorphic relations [58, 59] and formal specifications [60, 61] for tackling this problem. One
finding is that the first approach focuses on automatically generating ground-truth labels for single
images, while the other two methods tend to express the properties between continuous frames
and thus are suitable for evaluating the perception module at runtime.

Traditional coverage metrics, e.g., code coverage, are typically not suitable for estimating the
test adequacy of DNN-based models. Structural coverage metrics like neuron coverage [67] have
become a mainstream substitute. In addition, combinatorial testing is another approach for tackling
the test adequacy problem of the perception module.

6.3 Planning Module
The planning module takes as input the information from the perception module and produces a
suitable driving trajectory as a reference for the control module to make decisions. In the planning
module, we introduce the studies on test methodology (shown in §6.3.1), test oracle (shown in §6.3.2)
and test adequacy (shown in §6.3.3).

6.3.1 Test Methodology. Testing of the planning module consists in providing traffic scenarios for
an ADS, and checking if the planner module generates trajectories that satisfy properties such as
safety, comfort, low cost, etc. Note that, the path planning module is usually integrated in the whole
ADS, and highly coherent with other modules: the input of the module comes from the perception,
and the output trajectory is a reference for the control module, rather than the actual one observable
from the system. Therefore, testing the planning module independently is a challenging task.

Due to the above reasons, there exist no large numbers of studies on the testing dedicated
to the planning module. The studies we collected are based either on dedicated path planning
systems [89–91], or on the assumption of the perfection of the perception and control modules [92].
To summarize, search-based testing is the major technique for testing of the planning module, and
it is adopted in most of the works [89–91, 93, 94] for this module.

Search-based testing. According to [95], scenarios are defined on three abstraction levels, namely,
functional scenarios, logical scenarios, and concrete scenarios. A functional scenario has the highest
abstraction level that defines only the basic conditions and participants of a scenario; on top of
a functional scenario, a logical scenario is defined by a set of parameters and their ranges; with
the parameter values fixed in a logical scenario, a concrete scenario is generated. In the context
of scenario generation, search-based testing usually consists in searching in the parameter space
of a logical scenario for a concrete scenario, with specific objectives. Below are the examples of
applying search-based testing to generate concrete scenarios for the testing of the planning module.
The works [89–91] use a dedicated path planning system from their industry collaborator, which
computes the trajectories of the ADS based on several constraints, e.g., safety, traffic regulations. The
aggressiveness of the path planning strategy is decided by a system parameter, named weight. In [89],
the authors define a coverage criterion named weight coverage, which is used to characterize the
testing adequacy of the weight parameter. Later in [90], they propose two search-based techniques,
named single-weight approach and multi-weight approach, that automatically generate testing
scenarios guided by weight coverage. Specifically, the single-weight approach searches for the
scenarios that cover one specific weight of the path planner, while the multi-weight approach
generates scenarios that cover different weights simultaneously using the multi-objective search.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

20

S. Tang et al.

In [91], they consider searching for the driving patterns that are identified by the features appearing
in the planned trajectory, such as forward/lateral acceleration, curvature, etc. They claim that
the driving patterns that take place in a trajectory for a considerable duration are relevant to the
characteristics of the path planner, and thus facilitate engineers in system assessment.

In [93], Althoff et al. propose the notion of drivable area for motion planning algorithms, which
represents a safe solution space in which the ADS can avoid collision. Then they adopt a search
method to generate scenarios that are highly critical in the sense that the drivable area is limited.
In their follow-up work [94], the authors consider the interference of other traffic participants
in the drivable area, in order to increase the complexity of the scenarios. In their experiment,
the evolutionary algorithms [96] they use are demonstrated to be advantageous in finding local
optimum over these complex and diverse scenarios.

6.3.2 Test Oracle. Assessing the correctness of the output of the planning module, i.e., a trajectory,
is a challenging problem, due to the lack of an oracle for that. In this section, we introduce the
studies [97, 98] that define different metrics as the oracles to evaluate the correct functionality of
the planning module.

In [97, 98], Calò et al. define the notion of avoidable collisions, to distinguish them from unavoidable
collisions, in a dedicated path planner. By their definition, a collision is avoidable if it can be
avoided from happening in the same scenario by using a different system configuration of the ADS.
Compared to the unavoidable ones, the avoidable collisions are considered critical, since these
collisions require system reengineering to rectify the unsafe behavior.

6.3.3 Test Adequacy. As mentioned in §6.3.1, the inputs to the planning module involve both
external parameters that identify a scenario and internal parameters of the ADS. Because of the
infinity of the possible combinations of these parameters, generating test cases that are sufficiently
diverse remains a great challenge. In this section, we collect the studies [89, 92] that propose
coverage measurements on the space of the parameters. Namely, the weight coverage criterion
in [89] refers to the coverage of the possible configurations of the path planner under test; the route
coverage criterion in [92] is proposed to measure whether different features of a map have been
explored by the test suite.

In [89], Laurent et al. propose a coverage criterion, named weight coverage, to test a dedicated
path planning system. In their path planner, there is a weight function that consists of 6 weight
parameters, which affect the path planning decisions from different aspects, such as safety, comfort,
etc. In order to cover diverse planning decisions made by the system, the authors use the weight
coverage to guide the exploration in the weight parameter space. Thereby, they manage to generate
scenarios that cover more diverse combinations of the weight parameters.

Tang et al. [92] propose another coverage criterion called route coverage for testing the route
planning functionality of Apollo. Based on a Petri net [99] abstracted from the map, they quantify
the route diversity based on the junction topology feature and route feature. The junction topology
feature describes the relative position and connection relationship of the roads at a junction, while
the route feature describes the action of Apollo to track a selected road. By mutating the test cases,
they achieve a high route coverage ratio and thus obtain a diverse test suite that covers various
features of the map.

6.3.4 Discussion. The summary of the collected papers for testing the planning module is shown
in Table 6. We find that search-based testing is a dominating technique and has been demonstrated
to be effective for revealing faults in the design of the planning module [89–91, 93, 94]. In addition,
several metrics are proposed for facilitating the testing on the planning module, e.g., avoidable
collision [97] for tackling the oracle problem, weight coverage [89] and route coverage [92] for

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

21

Table 6. Summary of the papers for the planning module testing

Methodology Description

Literature

Test Objective

Environment

Search based
testing

Searching for testing scenarios with
weight coverage guarantee
Searching for the specific driving pat-
terns
Searching for the scenarios in which
the drivable area is limited

Oracle

Description

[89, 90]

[91]

A dedicated path planning
system [97]

Simulation

[93, 94]

CommonRoad [100]

Digital dataset

Avoidable collisions

A collision can be avoided from happening in the same sce-
nario by using a different system configuration of the ADS.

Adequacy

Weight coverage

Description

Based on a weight function consisting of weight parameters
which affect planning decisions

Route coverage

Based on the junction topology feature and route feature

Literature

[97, 98]

Literature

[89]

[92]

evaluating the sufficiency of the test suite. Note that since the planning module is dependent on the
localization functionality and most failure scenarios in our collected works are due to the behaviors
of NPCs, we summarize the potential threats as improper localization and unwanted maneuver of
NPCs in Fig. 2.

6.4 Control Module
Based on the trajectories produced by the planning module, the control module takes charge of the
lateral and longitudinal control of the ADS. By using various control algorithms, such as model
predictive control (MPC), proportional integral derivative (PID) control, etc., it generates control
signals, e.g., acceleration, deceleration, and steering angle, to the CAN bus for the control of the
whole system. In this module, we introduce the works on the testing of the control module, from
the perspectives of test methodology (shown in §6.4.1) and test oracle (shown in §6.4.2).

6.4.1 Test Methodology. The control module takes charge of multiple functionalities, such as the
longitudinal control and the lateral control of the ADS. Hence, the testing of this module focuses
on detecting scenarios that lead to longitudinal/lateral collisions.

Fault injection. Fault injection is a method that deliberately injects faults to a system, in order
to assess the fault tolerance of the system. In [101], Uriagereka et al. adopt this technique for
testing the fault tolerance ability of the control module of an ADS. Specifically, they inject faulty
GPS signals into the lateral control function of the ADS, which makes it produce wrong steering
commands. By calculating the fault tolerant time interval, which denotes the duration from the
activation of the fault to the occurrence of unsafe behavior, they find the lateral control system can
tolerate this type of fault for as long as 177𝑚𝑠.

Falsification. Temporal logic-based falsification is applied to ADS testing in [102]. Originally,
falsification refers to a technique for testing of the general cyber-physical systems, guided by the
quantitative semantics of temporal logic specifications, which indicates how far of the system
from being unsafe. In [102], Tuncali et al. propose a falsification-based automatic test generation
framework for testing motion controllers. They utilize a cost function, i.e., the quantitative semantics
of temporal logic specification, as a guidance to searching for the critical scenarios in which the

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

22

Table 7. Summary of the papers for the control module testing

Methodology

Description

Literature

Test Objective

Fault injection

Injecting faulty GPS signals

[101]

Steering Controller [105]

Falsification

Oracle

Utilizing temporal logic-based falsifi-
cation to search critical scenarios

[102, 103]

A vehicle dynamics model [106]

Description

S. Tang et al.

Environment

Simulation

Simulation

Literature

Optimization-based oracle model

The model can generate an oracle area in a given scenario

[104]

relative speed of the two vehicles in the collision is minimal. The obtained scenarios can be taken as
the behavioral boundary that divides the safe and unsafe behaviors. As a follow-up work, Tuncali
et al. [103] utilize the Rapidly-exploring Random Trees (RRT) algorithm for ADS falsification. They
incorporate a new cost function that applies time-to-collision to measure the seriousness of the
collision. As a result, the new method achieves better effectiveness in searching for safety-critical
scenarios, thanks to the exploration brought by the RRT algorithm.

6.4.2 Test oracle. Like the planning module, the control module also faces the oracle problem in
its testing—indeed, it is usually not straightforward to determine whether a control decision is
“correct”. In [104], Djoudi et al. propose a framework to determine whether the control module
makes “the correct decision”. They design a model to generate an oracle area in the given scenario,
which is the closest safe position ahead of the vehicle. A control decision is then considered as “the
right decision”, if it could drive the vehicle close to the oracle area.

6.4.3 Discussion. Table 7 summarizes the collected papers for the testing of the control module.
As with the planning module, there is also no much study on the testing of the control module.
Note that currently most of the control modules of ADS adopt mature control techniques directly,
including PID [9] and MPC [10], which partially explains why this module is not extensively studied.
The collected studies adopt two major techniques for testing the control module, including fault
injection [101] and falsification [102, 103]. To tackle the oracle problem in control module testing,
the framework proposed in [104] could generate an oracle area for judging whether the control
decision is “correct”, and the corresponding threat is denoted as implementation capability in Fig. 2.
However, we do not find related works for handling the test adequacy problem for this module. In
general, the control module deals with dynamics whose state transitions are continuous, so how to
define adequacy criteria for test cases needs further exploration in the future.

6.5 End-to-End Module
The end-to-end module is a special design adopted by some modern ADS, which combines the
functionalities of perception, planning and control together in one DNN-based module. Indeed, it
maps from one end, i.e., the sensing information, directly to another end, i.e., the control decisions
of ADS. For instance in the scenario of steering angle control, the end-to-end DNN takes as input
the sensing information including road condition and the status of other cars, and produces as
output a predicted steering angle for the control of the ADS. In this section, we introduce the
collected studies on the testing of the end-to-end module from the perspectives of test methodology
(shown in §6.5.1), test oracle (shown in §6.5.2), and test adequacy (shown in §6.5.3).

6.5.1 Test Methodology. As mentioned before, an end-to-end DNN model integrates three func-
tionalities, namely perception, planning and control, together in one module. Among these three

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

23

functionalities, perception is most vital as it provides input information to other modules; mean-
while, it is also most vulnerable to external environments, as it essentially involves image recognition
tasks that rely on deep learning. Compared to a DNN just for perception, although an end-to-end
DNN does not directly output the perception information, the control decisions it makes still de-
pend on the perception information. Therefore, like the case in the perception module, generating
adversarial images or scenarios that fool the end-to-end DNN is still the major testing methodology
for testing of the end-to-end modules.

We introduce three approaches, namely, search-based testing, optimization-based adversarial
attack, and GAN-based attack. The first approach has been introduced in §6.3.1; the last two
approaches have been introduced in §6.2.1.

Search-based testing. Search-based testing searches for a target test case in the input space,
guided by certain objectives. One commonly used objective is the coverage of the test suite—
maximizing the cumulative coverage of a test suite can expose more diverse behavior of the system,
and thus allow a better chance of detecting the target test case. In the context of DNN testing,
neuron coverage is proposed in by Pei et al. [67] to analogize the structural coverage in traditional
programs. In their follow-up work [107], Tian et al. propose a coverage-guided testing framework
called DeepTest for DNN testing. In [107], they propose various image operations, e.g., scaling,
shearing, rotating, etc., as the test input (image) mutation methods; then they generate test cases by
applying these operations to seed images, and keep only those mutants that enlarge the cumulative
neuron coverage of a test suite. Experiments are conducted on the ADS end-to-end modules, and
the results show the effectiveness of their method in test case generation.

In addition to coverage, seriousness of the unsafe behavior is another factor that can be used as
the search objective, and this has been considered by Li et al. [108]. In [108], the seriousness of
the unsafe behavior of the end-to-end module is formulated as the deviation of the actual steering
angle made in the test scenario from the expected steering angle. The authors design an objective
function that takes into account both the coverage and the seriousness, such that they can detect
not only diverse but also serious unsafe test cases.

Optimization-based adversarial attack. The optimization-based adversarial attacking frame-
work has been introduced in §6.2.1. In [109], Zhou et al. introduce a framework called DeepBillboard
that can generate adversarial perturbations which are added to billboard. The perturbations they
generate can mislead the steering angles in a series of frames captured by camera sensors during
the driving process, in spite of the physical conditions, such as different distances and different
angles to the billboard. Later in [110], Pavlitskaya et al. extend DeepBillboard with the projected
gradient sign method [111], and they generate adversarial patches for attacking an end-to-end model
DriveNet [112]. Their experiments show that the curved and rainy scenes are more vulnerable to
these adversarial attacks.

In another line of work, Boloor et al. [113] utilize adversarial black lines that are simple to be
painted on the public road to attack the end-to-end module of ADS. Later in [114], they use Bayesian
optimization [115] to generate adversarial black lines, which can lead to a deviation of an ADS from
the original path.

GAN-based attack. GAN has been introduced in §6.2.1, and it is another major approach for
adversarial attacking. In [116], Kong et al. propose a GAN-based approach called PhysGAN which
utilizes 3D tensor, i.e., a slice of video containing hundreds of frames, to generate adversarial
roadside signs that can continuously mislead the end-to-end driving models with high efficacy
and robustness. In another work [117], to generate adversarial images that are realistic, Zhang
et al. propose a GAN-based approach called DeepRoad. They demonstrate that their generated

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

24

S. Tang et al.

adversarial images are realistic under various weather conditions, and effective in detecting unsafe
system behaviors.

6.5.2 Test Oracle. An oracle of the end-to-end module indicates which is the correct control
decision at each moment of a scenario. Although this can be done with the help of human drivers,
it is too expensive and prone to errors. Existing works propose various automatic methods to
solve the oracle problem of the end-to-end module, including metamorphic relations [107, 117],
differential testing [67], and model-based oracle [118].

Metamorphic relations. As introduced in §6.2.2, metamorphic relations are a viable way to
solve the oracle problem. In the testing of end-to-end driving models, there is a few works that
leverage metamorphic relations to define the test oracles, such as DeepTest [107] and DeepRoad [117].
The metamorphic relation introduced by DeepTest [107] is that, the steering angle should not
change significantly for the same scenes under different weather and lighting conditions. Similarly,
DeepRoad [117] aims to detect model consistency, which means, for a synthetic image and the
original image, the difference between two predicted steering angles is smaller than a threshold.

Differential testing. Pei et al. [67] apply differential testing to generate scenarios which reveal
the inconsistencies between different DNN models. For the same scenario, they expect that the
DNNs under test should give the same inference result. The violation to this property is considered
as an unexpected behavior.

Model-based oracle. In [118], Stocco et al. propose a so-called self-assessment oracle for the
potential risk prediction of ADS. The self-assessment oracle involves training a probabilistic model
that characterizes the distribution of the potential risks under various real scenes. This model can
be used to monitor the real environment during the execution of the ADS and predict the situation
which the ADS is probably not able to handle.

6.5.3 Test Adequacy. In §6.2.3, we introduce the structural coverage for DNN testing, which
analogizes the structural coverage in traditional program testing. Since the end-to-end module
also relies on DNN models, these structural coverage criteria are also used in the testing of the
end-to-end module. Neuron coverage, which has been introduced in §6.2.3, is used by its authors
in [107] for a coverage-guided testing, as mentioned in §6.5.1. The refined structural coverage
criteria for DNNs, such as k-multisection neuron coverage (KMNC) and neuron boundary coverage
(NBC) proposed in [68], are used in [108], which is also elaborated on in §6.5.1.

6.5.4 Discussion. As with the perception module, the end-to-end module also contains many
DNN-based models; however, these models are not only used for perception, but also for the control
of the vehicles. Consequently, adversarial attack methods used in the perception module testing,
including optimization-based method [109, 110, 113, 114] and GAN-based method [116, 117], are
also adopted as the testing methodologies for this module. One observation is that, compared to
perception module testing that tests DNN models using single images, the works [109, 116] for
end-to-end module testing often use a series of images, i.e., the frames captured by cameras in a
system execution, in testing. Another major testing approach is the coverage-based testing [67,
107, 108, 117], in which the testing is guided by coverage criteria proposed for measuring whether
the system behavior has been sufficiently explored.

Since it is hard to evaluate the correctness of the output steering angle for an input image,
metamorphic relations [107, 117] and differential testing [67] are adopted for tackling this problem.
In addition, we find that other oracle techniques, e.g., model-based oracles [118], are used to solve
the oracle problem for this module.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

25

Table 8. Summary of the papers for the end-to-end module testing

Methodology Description

Literature

Test Objective

Environment

Search-based

Generating transformed images with
high neuron coverage

Optimization-
based

Designing an objective function to
search for the diverse and serious un-
safe test cases

Replacing original billboard with ad-
versarial billboard
Extending attack methods in [109] to
generate adversarial patches

Generating adversarial black lines on
the road

[107]

[108]

[109]

[110]

DNN models: Rambo, Chauffeur and
Epoch

Digital
dataset

DNN models: Daveorig [119], Dave-
dropout [120] and Chauffeur.

Digital
dataset

CNN models: DeepGauge [68] and
Deepmutation [121]
DriveNet [112]

Digital
dataset
Simulation

[113, 114]

Conditional Imitation Learning mod-
els [122]

Simulation

GAN-based

Generating adversarial roadside sign

[116]

Generating realistic adversarial im-
ages

[117]

CNN steering models: Dave-2, Udac-
ity Cg23

DNN models: Autumn, Chauffeur,
and Rwightman

Digital
dataset

Digital
dataset

Oracle

Description

Metamorphic relation

Differential testing

Model-based oracle

The steering angle should not change significantly under dif-
ferent conditions

The DNNs under test should give the same inference result
for the same scenario

Predicting the situation which the ADS is probably not able
to handle

Literature

[107, 117]

[67]

[118]

7 LITERATURE OF TECHNIQUES ON SYSTEM-LEVEL ADS TESTING
In this section, we introduce the research works on the testing ADS from the system level. Different
from the module-level testing, system-level testing focuses on finding the failures that threaten
the safety of the whole vehicle due to the collaborations between modules. In §6, most of the
testing works are done in simulated environments, implemented by various software simulators.
In this section, we introduce not only simulator-based testing in §7.1, but we also introduce the
mixed-reality testing in §7.2 that introduces real hardware in the testing loop.

7.1 System-Level Testing with Simulators
We first introduce system-level testing conducted with the help of software simulators. As in the
way the module-level testing works are introduced, we present these studies from three perspectives,
namely, test methodology (shown in §7.1.1), test oracle (shown in §7.1.2) and test adequacy (shown
in §7.1.3).

7.1.1 Test Methodology. In literature, we find various testing methods for the system-level testing
of ADS, including search-based testing, adaptive stress testing, sampling-based methods, adversarial
attack, etc. In this section, we introduce these testing methods.

Search-based testing. Search-based testing (or a similar concept named fuzzing), is one of the
most widely-adopted methodologies in ADS testing. As introduced in §6.3.1, it consists in searching
in the parameter space for specific parameter values that achieve a testing objective. In this section,
we introduce the works [123–127] to illustrate the ideas.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

26

S. Tang et al.

In [123], Dreossi et al. propose a compositional search-based testing framework, and apply it
for the testing of ADS with machine learning components (i.e., mostly perception). The basic idea
in their work is the cooperative use of the perception input space and the whole system input
space: the constraints on one space can reduce the search efforts in the other space. In this way,
they improve the efficiency of searching for counterexamples. There are also works that aim to
improve the search efficiency by designing better search algorithm. In [124], Abdessalem et al.
combine multi-objective search with decision tree classification for test generation of ADS. In their
framework, the classification checks whether the scenario is a critical one, and accelerate the search
process. In practice, search-based testing has also proved to be effective for industry-level ADS.
In [125], Li et al. propose AVFuzzer used for testing of Apollo, and they show the effectiveness of
AVFuzzer in finding dangerous scenarios.

Search-based testing is usually based on system simulations; however, even with software
simulators, the simulations of ADS can still be expensive and slow. To that end, there is a line of
work that trains surrogate models as the substitute for testing acceleration. In [126], Abdessalem
et al. train a surrogate model that maps the scenario parameters to fitness functions, and use the
surrogate to detect the non-critical parameters for search space reduction. In [127], to search for
more collision scenarios, Beglerovic et al. train a surrogate model by utilizing the critical scenarios
that already exist.

Adaptive stress testing. Stress testing has been widely adopted in various domains of industry,
which performs testing by providing test cases beyond the capability of the system under test.
Adaptive stress testing, literally, performs stress testing in an adaptive manner; namely, it assigns
different priorities to the test cases, and allocates them different testing resources accordingly.
Therefore, specifying the policy of priority assignment is the key in adaptive stress testing. In [128],
the authors apply adaptive stress testing for ADS, and design a priority assignment policy based on
the difference between the expected behavior and the actual behavior. Later in [129], they propose
a new priority assignment policy based on Responsibility-Sensitive Safety (RSS) [130], which defines
the utopian behavior of the cars by which no collision will happen in a scenario. The new policy
in [129] is thus defined according to the distance of the ADS behavior compared to the utopian
cases in the RSS rules.

Sampling. We refer to sampling as the statistical method that samples values from a probability
distribution. One use case in ADS testing is to generate scenarios by sampling in a natural scenario
distribution, in order to make the generated scenario realistic. This has been studied in [131].

Moreover, advanced sampling techniques can be applied to achieve specific goals; for example,
importance sampling [132] is a technique used to handle sampling of rare events. In normal occasions,
unsafe scenarios are indeed rare to happen, so detecting those scenarios is hard and costly. In that
case, importance sampling can be applied to accelerate the testing. In [133], Norden et al. utilize an
importance sampling approach to characterize the probability of the system failure scenarios with
less number of simulations. Zhao et al. [134–137] work extensively in this direction. The main aim
of their work is to spend less simulations to detect more system failures, under various scenarios.
Specifically, in [134–136], they investigate the cut-in lane change scenarios; in [137], they focus on
the car-following scenario.

Adversarial attack. Adversarial attack has been introduced in §6.2.1, in which it is used for testing
the perception module. Here, we introduce two works [138, 139] that also attack the perception
module, but they assess the influence of the attack to the whole system. In [138], Sato et al. generate
attack patches, as a camouflage of dirty roads, that mislead the lateral control functionality of the

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

27

victim ADS to deviate from the lane. In [139], Rubaiyat et al. generate perturbations to camera-
captured images, based on the system-level safety risk analysis, to assess the reliability of OpenPilot
under real-world environmental conditions.

7.1.2 Test Oracle. The oracles of the system-level testing of ADS are usually defined by safety
metrics, such as time-to-collision, which measures how far of the ADS under test is from the
dangerous situations. These metrics can be directly computed by monitoring the system behavior in
the simulator, or expressed as formal specifications, such as signal temporal logic (STL), which can
automatically monitor the system behavior and compute the metric values. Besides, metamorphic
relations are also used in some work for defining the oracle of ADS.

Safety metrics. In system-level ADS testing, the most commonly-used safety metrics include, e.g.,
minimum distance between cars [125], time-to-collision [103], etc., that measure the closeness of
the ego car to collision in the scenario. In [140], Weng et al. propose Model Predictive Instantaneous
Safety Metric (MPrISM) that provides a predicted time-to-collision for measuring the closeness of
the ego car to collision. Specifically, MPrISM computes a predictive time-to-collision based on the
prediction of the ego car’s motion over a certain period in the future, and the authors experimentally
demonstrate the superiority of the proposed metric.

Formal specifications. As introduced in §6.2.2, formal specification uses temporal logic languages
to express the properties which the system should hold during the running; then by specification-
based monitoring, the satisfaction of the system behavior can be automatically decided. On the
system-level testing of ADS, signal temporal logic (STL), which can express the properties over
real-time continuous variables, is the proper selection of specification language. There are a few
works that adopt STL as the specification language, such as [123, 141], in which STL monitors are
synthesized to decide whether the behavior of the ADS satisfy the desired safety properties.

Metamorphic relations. Metamorphic relations have been discussed for the module-level testing
in §6.2.2 and §6.5.2. On the system-level testing, Seymour et al. [142] define a metamorphic relation
that the behavior of ADS should be unchanged regardless of the weather condition, which is then
applied in metamorphic testing to identify violation cases in the systems. Han et al. [143] utilize
metamorphic relations to distinguish between real failure and false alarms. The metamorphic
relation regulates that the behavior of the ADS should be similar in slightly different scenarios;
otherwise, the collision in one of such scenarios is considered avoidable, and thus a real failure.

7.1.3 Test Adequacy. In system-level testing, the adequacy of testing is embodied by the diversity
of the testing scenarios for the ADS. In this section, we introduce two line of works that define
various metrics to characterize the diversity of scenarios.

Scenario coverage. There is a line of work that defines coverage for scenarios. The intuition is
that the testing is sufficient if all different types of scenarios are covered [144]. In [145], Tang et al.
classify the scenarios based on the topological structure of the map. In [146], Majzik et al. propose
a coverage of scenarios based on the properties that are temporal, spatial, or causal, of the real
simulation data. In [147], Kerber et al. define a distance measure over scenarios based on their
Spatiotemporal features, which enable scenario clustering.

Combinatorial coverage. Combinatorial coverage has been introduced in §6.2.3. Unlike the above
coverage criteria defined directly on the features of the scenarios, combinatorial coverage concerns
about the coverage of the combinations of different parameters that identify different scenarios.
In [141], Tuncali et al. propose to use covering array for scenario generation in ADS testing. Covering
array is a specific mechanism in software testing that guarantees the satisfaction of the 𝑡-way

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

28

S. Tang et al.

Table 9. Summary of the papers for simulation-based system-level testing

Methodology Description

Literature

Test Objective

Environment

Search-based
testing

Incorporating the perception input space and the
whole system input space to accelerate search
Searching for unsafe feature interactions

Finding critical scenarios that caused by fusion
errors
Finding safety violations of an ADS in the dy-
namic environment

Searching for the non-critical parameters with
surrogate model

Searching for faulty behaviors with surrogate
model

Adaptive
stress testing

Finding the failure behaviors in the driving sce-
nario

Sampling

Sampling rare events with less simulations

[123]

[124]

[151]

AEB systems [148,
149]
Systems
IEE [150]
OpenPilot

from

Simulation

Simulation

Simulation

[125]

Apollo

Simulation

[126]

[127]

A Pedestrian Detec-
tion Vision based
(PeVi) system
AEB system

Simulation

Simulation

[128, 129]

[133]

[134–137]

Intelligent
Model[152]

OpenPilot

ACC [153] and AEB
system [154]

Driver

Simulation

Simulation

Simulation

Adversarial
attack

Generating attack patches as a camouflage of dirty
roads
Generating adversarial perturbations under dif-
ferent weather

[138]

OpenPilot

Digital dataset

[139]

OpenPilot

Simulation

Oracle

Safety metrics

Description

Model Predictive Instantaneous Safety Metric (MPrISM): a predicted time-to-
collision for measuring the closeness of the vehicle to collision

Literature

[125, 140]

Formal specifications

Adopting STL as the specification language to express the properties over real-
time continuous variables

[123, 141]

Metamorphic relations

The behavior of ADS should be unchanged regardless of the weather condition

The behavior of the ADS should be similar in slightly different scenarios

Adequacy

Description

Scenario coverage

Based on the topological structure of the map
Based on the properties that are temporal, spatial, etc.
Based on Spatiotemporal features

Combinatorial coverage

Applying covering array for scenario generation

[142]

[143]

Literature

[145]
[146]
[147]

[141]

combination coverage of the parameters. See §6.2.3 for more details about 𝑡-way combination
coverage.

7.1.4 Discussion. As shown by Table 9, search-based testing [124, 125, 127, 151] have been widely
adopted for testing the whole ADS. Although simulation-based testing aims to solve the high cost
problem of real-world testing, it may repeatedly simulate the same type of scenarios, which is also
a time-consuming process. Consequently, adaptive stress testing [128, 129] and sampling-based
techniques [131, 133–136] are applied for accelerating the testing process. As in the cases of the
perception and end-to-end modules, adversarial attack [138, 139] has also been adopted for system-
level testing, which aims to detect the vulnerabilities of the perception that affects the safety of the
whole system. System-level testing usually relies on safety metrics, i.e., time-to-collision [103], and

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

29

Fig. 3. Illustration of the HiL, ViL, and SciL approaches

metamorphic relations [142, 143] as the oracles that measure the occurrences of safety violations
during the testing process.

7.2 Mixed-Reality Testing
Due to the expensiveness of the real-world testing of ADS, most approaches in §6 and in §7.1 test
ADS in software simulators. Although modern simulators have achieved high fidelity, simulator-
based testing is not sufficient to reveal all the problems of ADS, because there are still gaps between
simulators and the real world. As a trade-off, mixed-reality testing combines the simulator-based
testing with real-world testing. Specifically, it refers to a family of testing schemes called X-in-the-
loop, which means replacing the component “X” in the simulator by “X” in the real world. In this
section, we introduce hardware-in-the-loop, vehicle-in-the-loop, scenario-in-the-loop, respectively.

7.2.1 Hardware-in-the-Loop (HiL). HiL testing usually introduces the real vehicle ECU hardware
into the testing loop. In HiL testing, except the ECU in the control module under test is real, other
objects including the vehicle, the sensors and the traffic scenes are all simulated. The workflow of
the HiL method is shown in the blue box in Fig. 3. This method is used in the following works.

In [155], Chen et al. present an HiL testing platform that can realize multi-agent system and use
3D modeling and OpenStreetMap [156] to define various traffic scenes, so the platform simulates the
characteristics of complex traffic scenes, which increases the diversity of the testing. Later in [157],
they improve the platform by using a self-designed ECU that makes the offline testing more realistic.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

Data FusionTest BenchVehicle MovementPlanningResultsSimulatedSimulated ScenariosSimulated SensorsVehicle DynamicsModelVehicleStatesSpeedSteering…VehicleStatesVehicleStatesCameraLiDAR…SensorsDataVehicleStatesRoadsRoad Features…TrafficScenariosADSComputing HardwareControl AlgorithmsSignal SimulationECUVirtual CanMessageReal VehiclePlanningResultsSimulated  DataVehicle-in-the-LoopHardware-in-the-LoopTest FieldRealPedestriansObstacles…SimulatedVirtual VehicleVirtual Traffic…Data FusionVehicle MovementScenario-in-the-LoopVehicle States30

S. Tang et al.

In [158], Fu et al. propose a retargetable fault injection framework that can be implemented across
systems with HiL. In [159], Wang et al. adopt dSPACE Automotive Simulation Model (ASM) [160] to
verify the flatness of an model predictive controller for trajectory tracking of autonomous vehicles.
In [161], Brogle et al. present a simulation system based on Carla and robot operating system
(ROS), which achieves high fidelity in vehicle dynamics and sensor data output. In [162], Gao et al.
introduce a real vehicle into HiL testing, and test the AEB algorithm using the Udwadia–Kalaba
(U-K) method [163].

7.2.2 Vehicle-in-the-Loop (ViL). While HiL integrates the ECU into the simulation, it ignores
the the factors of the real road environments. To account for that issue, the ViL testing method
that combines real vehicle and virtual environmental objects, such as roads, traffic scenes, etc., is
proposed. The workflow of ViL is shown in the orange box in Fig. 3. This framework has been
widely adopted in literature, some examples are demonstrated by the following works.

In [164], an open-source programmable traffic simulator called SUMO [165] is adopted to simulate
the traffic testing environment. In [166], Fayazi et al. propose a ViL testing method that can configure
an intersection specific environment to verify the scheduling of the ADS without physical signal
lights. In [167], Solmaz et al. propose a testing framework that allows real-time integration of
simulated traffic scenarios in the ADS testing. In [168], Chen et al. propose a ViL verification method
which can reconstruct scenarios based on OpenStreetMap [156] and inject virtual perception data
and real data into the perception module. In [169], Li et al. propose an ADS testing framework
composed of a test bench implemented in conjunction with road to rig (R2R), which combines
laboratory testing in a controlled environment with actual highway testing. In [170], Diewald et al.
propose a platform that consists of a vehicle testing bench and a digital radar target simulator. The
platform is able to implement a virtual radar simulation system with multi-angle distribution for
ADS validation.

Scenario-in-the-Loop (SciL). In order to enhance the realisticness, the concept of SciL testing is
7.2.3
proposed on the basis of ViL testing. In SciL testing, besides the real vehicle, the dummy objects (e.g.,
pedestrian, cyclist) and the obstacles are arranged in the test site. In contrast, the implementation
of the traffic flow is still virtual. At this time, the data obtained by the sensors come from both real
scene and virtual scene. The workflow of SciL is shown in the black box in Fig. 3, and this method
is adopted in the following works.

In [171], Szalay et al. propose a digital-twin technique for SciL testing, which can conduct
dynamic simulation according to the solid model and the perceived data of the vehicle. In [172],
they then explain the concept of SciL from the perspective of the requirements of testing track
configuration. In [173], Horvath et al. describe the comparison of the implementation objectives
and conditions of ViL and SciL from the perspective of traffic configuration. In [174], Szalai et al.
utilize SUMO [165] to simulate the traffic conditions, with a customized virtual environment by
Unity [175].

7.3 Simulator-Based Testing vs. Real-World Testing
Regarding the efforts in simulator-based testing, a natural question arises that, how far is the
simulator-based testing still from the real-world testing. As an emerging issue, this topic has
attracted increasing research attention; here, we introduce the latest progress from two perspectives,
namely, the realisticness of test cases and the realisticness of simulators.

Realisticness of test cases. One question arises in the simulator-based testing that the virtual
scenarios generated by testing algorithms that lead to system failures may never happen in the real

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

31

world. Indeed, simulators give a high liberty to create traffic participants, of which, nevertheless,
only a subset can really happen.

There is a line of work that aims to bridge this gap and thus to generate natural scenarios for
ADS testing. In [176], Nalic et al. propose a co-simulation framework using two simulation tools
CarMaker (for vehicle dynamics) and VISSIM (for traffic simulation); their framework can generate
scenarios based on calibrated traffic models derived from real-world data. In [177], Bashetty et al.
propose a method called DeepCrashTest that can create the crash scenarios from publicly available
real-world crash videos. They extract the trajectories of the vehicles from the videos and use them
to reconstruct the traffic scenario. These scenarios can be used for fault analysis and rectify the
unsafe behavior of ADS.

Realisticness of simulators. A comparative study on the assessment of testing in different levels
of simulations is performed by Antkiewicz et al. [178]. In their work, the authors study simulator-
based testing, mixed-reality testing, and real-world testing on two scenarios, i.e., car following and
surrogate actor pedestrian crossing. They propose various metrics, e.g., realisticness, costs, agility,
scalability, controllability, and based on these metrics, they compare the different testing schemes
under evaluation. As their conclusion, they quantitatively show the performance difference among
the testing schemes: although real-world testing is better in terms of realisticness, it is more costly,
and less agile, scalable, controllable, etc., compared to simulation-based testing; the performance of
mixed-reality testing is in the middle of them.

Although simulator-based testing cannot achieve the same realisticness as the real-world testing,
to which extent the results of simulation-based testing can benefit real-world testing? This question
is investigated in [179], where the authors perform simulator-based testing to identify critical
scenarios, and map them to a real-world environment. Their key insights involve that, 62.5% of the
unsafe scenarios detected by the simulators translate to real collisions; and 93.3% safe scenarios
with the simulators are also safe in the real world.

8 STATISTICS AND ANALYSIS OF LITERATURE
In this section, we provide the statistics of the literature we have discussed in §5, §6 and §7, and
based on that, we analyze the landscape of the research in ADS testing.

8.1 Statistics and Analysis in Paper Collection
We analyze the collected papers from three perspectives, namely, the publication venues, the
targeted system modules, and the publication years. We show the distribution of the publication
venues of all the papers in Fig. 4a and the distribution of the targeted modules/system in Fig. 4b.
Moreover, we present the number of papers published in different years in Fig. 4c.

Publication venues. By Fig. 4a, we can see that, (i) many of the papers, up to 43%, are published
in transportation venues such as International Conference on Intelligent Transportation Systems
(ITSC) and IEEE Intelligent Vehicles (IV); (ii) 26% of the papers are published in software engineering
venues such as International Conference on Software Engineering (ICSE), International Symposium
on Software Reliability Engineering (ISSRE) and International Conference on Automated Software
Engineering (ASE); (iii) since the ADS and artificial intelligence are closely related, 9% of the papers
are published in artificial intelligence venues such as Computer Vision and Pattern Recognition
Conference (CVPR) and AAAI Conference on Artificial Intelligence (AAAI). (iv) the adversarial attack
methods for vulnerability detection of ADS are related to the security of the systems, and hence
9% are published in the security venues, such as USENIX Security Symposium and IEEE Symposium
on Security and Privacy (S&P).

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

32

S. Tang et al.

(a) Distribution of the publication venues

(b) Distribution of the testing targets

(c) The number of the publications in each year

Fig. 4. The statistical information of the publications in ADS testing

Target modules. By Fig. 4b, we can see that, obviously, the number of the papers on system-level
testing is of the largest percentage, up to 42%. These papers involve the testing techniques that
span over both simulation-based testing and mixed-reality testing. Moreover, the number of the
papers concerning with the perception module is the second largest, up to 24%. The perception
module takes charge of object detection and image segmentation, based on deep learning, which is a
popular research direction. Compared to the perception module, there are fewer papers concerning
with other modules, e.g., the planning module or the control module.

Publication year. By Fig. 4c, we can see that from 2015 to 2020, the number of the relevant papers
increases continuously. This trend shows that the safety and security of ADS are attracting more
and more research attention from researchers. In 2021, the number decreases, partially because
the testing of ADS had been initially established, and researchers are seeking for new challenges;
another reason for this phenomenon is that only partial papers in 2021 have been published by the
time when we collected the papers.

8.2 Timeline of the Publications Related to Apollo and OpenPilot
This section shows the timeline of some milestone publications related to the testing of two
open-source ADS, namely, Apollo and OpenPilot.

Apollo. Research interests in safety analysis of Apollo arose in 2018, when Apollo 3.0 was
released. Cao et al. publish their early research [23], which shows that, by injecting spoofed LiDAR
points, attackers can fool the LiDAR-based perception model of Apollo 3.0 to detect a fake vehicle.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

33

Spoofing attack [23]

(cid:114)
2018.7
v3.0
New Perception
Module

Black-box attack [25]
Formal methods [179]
AV-Fuzzer [125]

(cid:114)
2019.1
v3.5
New Runtime
Framework

MSF attack [21]
Route coverage [92, 145]
Trajectory coverage [182]
VisTA [181]
LiveTCM [180]

(cid:114)
2019.6
v5.0
New Perception and
Planning Algorithm

(a) Apollo

Metamorphic testing [142]
ComOpT [183]

(cid:114)
2020.9
v6.0
New Data Pipeline
Upgraded DL Models

(cid:45)

Fault injection [139]
(cid:114)
2018.2
v0.4.2

Black-box evaluation [133]

(cid:114)
2018.07
v0.5

DRP attack [138]

(cid:114)
2019.12
v0.7
LDW

VulFuzz [184]

(cid:114)
2020.8
v0.7.8
New DM Model

FusionFuzz [151]

(cid:114)
2021.6
v0.8.5
Model-based FCW

(cid:45)

(b) OpenPilot. LDW, DM and FCW refer to Lane Departure Warning, Driver Monitoring

and Forward Collision Warning, respectively.

Fig. 5. Timeline of the Publications Related to the Apollo and OpenPilot

Apollo 3.5 was released in Jan. 2019, equipped with a new runtime framework named Cyber
RT and new perception algorithms. Sun et al. [25] propose a black-box attack method on Apollo
3.5, which targets at more general machine learning models than [23]. Fremont et al. [179] applied
formal scenario-based testing and investigated the gap between testing in simulator and real-world.
A fuzzing method [125] is proposed to test Apollo 3.5, and various safety critical scenarios which
can lead to crashes are found.

Apollo 5.0 was released in Jun. 2019, equipped with new perception and planning algorithms; the
largest number of related studies is found in this version. Cao et al. [21] perform a first multi-sensor
fusion (MSF) attack aiming to make the victim ADS to fail in detecting adversarial 3D objects. Several
coverage criteria, such as route coverage [92, 145] and trajectory coverage [182], are adopted for
the testing of Apollo’s planning algorithms. There are also testing frameworks, e.g., ViSTA [181]
and LiveTCM [180], proposed to support test case generation and automated execution. These
frameworks are evaluated on Apollo 5.0 and they are proved effective in exposing issues and
limitations in the system.

Apollo 6.0 was released in Sep. 2020. Seymour et al. [142] applied metamorphic relations to
identify inconsistencies in Apollo 6.0 and find 58 collision cases in the system. Li et al. [183] utilize
k-way combinatorial testing to increase the testing coverage, and they report two types of collision
accidents with the root causes.

Apollo now has been iterated to version 7.0 with enhanced deep learning models for perception
and prediction, and whether the vulnerabilities (e.g., limitations, issues or vulnerabilities) found in
previous versions are still exists in this version of the system needs to be explored in the future.

OpenPilot. Research efforts for quality assurance of OpenPilot arose in 2018, when OpenPilot
0.4.2 was released. Rubaiyat et al. [139] propose a fault injection framework for the safety evaluation
of OpenPilot and find the weakness of this version’s safety mechanisms in the forward collision
warning. OpenPilot 0.5 was released in July, 2018. Norden et al. [133] utilized adaptive sampling
to perform a black-box testing of this version and made an analysis on the failure scenarios.

The functionality of lane departure warning was added in OpenPilot 0.7 in Dec. 2019, along
with an updated perception model for lane detection. Sato et al. [138] performed a Dirty Road Patch

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

34

S. Tang et al.

(DRP) attack with this version, and the generated patches can mislead the steering angle decisions
and cause collisions.

OpenPilot 0.7.8 was released in Aug. 2020, equipped with an updated driver monitoring model;
OpenPilot 0.8.5 was released in Jun. 2021, equipped with a model-based forward collision warning
functionality. VulFuzz [184] and FusionFuzz [151] were proposed for testing the two versions of
OpenPilot, respectively. These fuzzing-based frameworks have been demonstrated for finding
system failures with a high efficacy and efficiency.

In summary, as an L2 system, OpenPilot evolves faster than Apollo, and each version update
brings improvements to the functionalities and supports for vehicle models. However, the related
research on OpenPilot is much less than that on Apollo. As a lightweight system that may be
earlier adopted by ADS market, it is worth more research attention in the future.

8.3 Datasets and Toolsets for ADS Testing

Datasets. In the context of ADS, deep learning components handle safety-critical tasks, e.g.,
perception and end-to-end control, so it is necessary to validate their robustness under various
scenarios. This process typically relies on data from real-world, which is, however, hard to obtain in
general. Fortunately, there are a collection of datasets publicly available to solve the problem, which
involve quantities of real-world pictures and videos recorded by onboard sensors. For example, the
KITTI dataset [185] contain over 10,000 images of traffic scenarios, collected by a variety of sensors
including high-resolution RGB/grayscale stereo cameras and a 3D laser scanner.

In this section, we summarize the scenario-driven datasets for ADS testing in Table 10. The first
column shows the time when each dataset was released. The next three columns give the name,
brief description and the size of each dataset, and the last column indicates the related works that
adopt these datasets. Note that the datasets for other machine learning testing tasks [186] that have
nothing to do with ADS testing are not listed here; in other words, all the datasets listed here are
dedicated to ADS testing.

As shown by Table 10, we collect 28 datasets released from 2004 to 2021, including popular
ones like KITTI dataset [185] and emerging ones like AUTOMATUM DATA dataset [187]. One
observation is that these datasets span over various physical conditions, e.g., different times of
the day [188], different weather conditions [189, 190], different traffic density [191]; some of the
datasets even cover multiple conditions [192–194]. These datasets also span over various application
scenarios, such as urban street [192, 194, 195], highway [187, 196–198] and intersection [199, 200].
In addition, we find that some of these datasets are specific to a certain task, e.g., pedestrian
detection [201–203], and traffic sign detection [75, 204, 204].

As the column of reference shows, several datasets such as KITTI dataset [185], Udacity dataset [188]

and Cityscapes [195] are frequently used in ADS testing due to the diverse tasks they support, such
as object detection, semantic segmentation, etc. However, we also find that a number of datasets
have not been widely used, affected by their own limitations. For example, the rounD [205] and
Opendd [206] can only be used for validating the behavior planning of ADS in the scenario of
roundabouts; SYNTHIA [207], GTA [208] and Apollo Synthetic [209] contain synthetic images
from virtual environments, which may be not realistic enough for ADS testing.

Toolsets. As mentioned before, simulator-based testing has become an important alternative
approach for real-world testing. Simulators usually provide vehicle dynamics, e.g., longitudinal
and lateral motion of the vehicle, and virtual traffic scenarios for developers to perform testing.
Moreover, simulators can help to generate those extreme scenarios for testing, e.g., harsh weather,
which are rarely encountered in the real world. There have been many advanced simulation
platforms developed for ADS testing in recent years. For example, Carla [210] is an open-source

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

35

simulator for ADS training and testing, which supports various sensor models, environmental
conditions, etc.

In this section, we summarize the simulation platforms usually used for ADS testing in Table 11.
Like Table 10, the first column lists the time when each platform was proposed. The next two
columns show the names and short descriptions of them and the last column indicates the related
works which utilize these platforms in practice.

As shown in Table 11, we collect 28 simulation platforms including classical platforms such as
Matlab Simulink [211] and Carsim [212], and emerging popular simulators such as Carla [210]
and LGSVL [213]. An obvious observation is that these simulation platforms have different charac-
teristics and thus are suited for different testing tasks. For example, CarSim [212], CarMaker [214]
and ADAMS [215] can provide high-fidelity vehicle dynamics; Gazebo [216] is commonly used
in conjunction with ROS as it is a 3D robotic simulator. As the column of reference shows, we
find that Carla [210] and LGSVL [213] have been widely adopted in ADS testing practice. This
can be due to the following reasons: they are both high-fidelity simulators with multiple built-in
features, including 3D environment and traffic simulation, sensor and vehicle dynamics simulation,
flexible API, etc; also, the two simulators support integration with typical ADS, e.g., LGSVL has the
communication bridge interface with Apollo and Autoware, while Carla can be easily connected
with OpenPilot. However, we find that a considerable number of simulation platforms have not
been widely chosen for research. One possibility is that some of them like PreScan [217], NVIDIA
Drive Constellation [218], rFpro [219], and Cognata [220] are not open-sourced, which prevents
them from being widely adopted in research.

8.4 Programming Languages for Scenario Generation
In order to systematically generate test cases, it has become a trend to propose new programming
languages for testing scenario description. In this way, the generation of a new test case boils
down to writing a program that describes the scenario. Also, researchers can make use of existing
coverage criteria for programs, such as the code coverage criteria, to assess the adequacy of the
generated tests.

To define such a programming language, researchers need to formally express the basic elements
in an ADS scenario, e.g., the ego car, other cars, pedestrians, static objects, etc. Since these languages
are usually dependent on existing formats, they vary in ways of expressing those elements, based
on their dependent formats. For instance, Scenic [250], a python-like language, requires users to
define those objects as variables; in contrast, GeoScenario [249] provides users with a graphical
interface where users can drag the icons to describe a scenario. Moreover, these languages usually
do not emerge independently; instead, they come with specific simulators, or even specific ADS.
In this section, we summarize the state-of-the-art programming languages for test case generation
in Table 12, and we introduce their dependent formalism, their bonded simulator, other features,
and their adoption in ADS testing. There exists literature, e.g., [256], that surveys programming
languages for test generation of ADS. Compared to [256], our main aim is to show the ecosystems
and the landscape of the use of these languages in ADS testing, as a reference for the readers
to better understand the testing techniques in §6 and §7. Also, our study includes some latest
achievements, e.g., paracosm [255] and SceGene [254], in this direction.

As shown by Table 12, we collect 8 representative programming languages, including the classic
ones, such as OpenScenario, that have been widely used in different stages of the development of
ADS, and emerging ones, such as paracosm [255]. As our findings, first, different languages are
designed for different purposes and attached with different features, e.g., Scenic [250] allows proba-
bilistic sampling for testing driving systems with machine learning components; SceGene [254]
designs bio-inspired operations, such as crossover, mutation, for scenario generation. Second, some

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

Reference
[94, 133]
-
[21, 25, 29, 59,
60, 109, 116,
141, 177, 177,
223–225]
[24, 28]
[24]
[26, 57, 223,
224]
[67, 108, 109,
116, 117]
-
-

-
-

-

-

36

S. Tang et al.

Table 10. Scenario driven datasets for ADS testing

Time
2004
2011
2012

Dataset
NGSim [221]
NGSIM Drone [222]
KITTI [185]

Description
The intersection of structured roads
Naturalistic vehicle trajectories
Driving scenes captured by a standard station wagon

Volume
videos
174h videos
12919 Images

2013
2015
2016

GTSRB [75]
LISA [204]
Cityscapes [195]

2016

Udacity [188]

2016
2016

2016
2017

SYNTHIA [207]
the
Stanford
Dataset[202]
Comma.ai [198]
RobotCar [194]

Drone

2017

CityPersons [201]

2017

Mapillary vistas [192]

2018

GTA [208]

2018

BDDV [193]

2018
2018

2018
2019

2019
2019

The comma2k19 [196]
The highD [197]

ApolloScape [191]
Apollo Synthetic [209]

ACFR [226]
NuScenes [189]

2019

The INTERACTION [227]

2019

Waymo [63]

2020

The ind [200]

2020

Avdata [190]

2020

The rounD [205]

2020
2021

Opendd [206]
AUTOMATUM DATA [187]

Multiple traffic signs in Germany
A traffic sign dataset containing 47 different road signs
A diverse set of stereo video sequences recorded in street
scenes
Video frames taken from urban road

Multiple categories of virtual city rendering pictures
The movement and dynamics of pedestrians across the
university campus
Several hours video of major highway driving
Various combinations of weather, traffic and pedestrians,
as well as long-term changes such as road engineering.
A dataset with a large proportion of blocked pedestrians
images
Street View of multiple cities under multiple seasons and
weather conditions
Synthetic images of urban traffic scenes collected using
the game engine
Various scene types and weather conditions at different
times of the day
Over 33 hours of commute in California’s 280 highway
Traffic conditions of six different locations obtained by
drone
Images under different conditions and traffic density
A variety of different virtual scenes with high visual fi-
delity
Vehicle traces at 5 Roundabouts
Images under different times of day and weather condi-
tions
A dataset collected under interactive driving scenes with
semantic maps
Including a perception dataset with high-resolution sen-
sor data and labels, and a motion dataset with object tra-
jectory and corresponding 3D map
Naturalistic trajectories of vehicles and vulnerable road
users recorded at German intersections
Multiple seasons, traffic conditions and driving environ-
ments
Naturalistic trajectories of vehicles and vulnerable road
users recorded at German roundabouts
An trajectory dataset covering seven roundabouts
Highway scenes from 30 hours of drone videos

50k images
huge videos
25K Images

im-

101369
ages
213k images
69G videos

80G videos
Huge videos

2975 images

25000 images

80655 images

[59]

277h videos

-

95G videos
147h videos

147k images
273k images

23000 images
40k images

videos

[138]
-

[58]
-

-
-

-

Huge videos

[225]

videos

[228]

4.3G videos

videos

62h vedios
30h videos

-

-

-
-

of these languages provide more user-friendly features; for instance, some of the languages, e.g.,
SceML [252] provide GUI for users to define their scenarios. However, as the column of reference
shows, most of these languages have not been widely adopted in practice. This can be due to several
reasons: one possibility is that some languages are still too specialized for practitioners to adopt
them in their work; also, since many of the languages, such as GeoScenario [249], are designed for
specific systems, they are still ad-hoc and not easily extensible to be adopted in other systems.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

37

Table 11. Simulation platforms for ADS testing

Time
1984

1988
1992

Toolset
Matlab
Simulink [211]
CarSim [212]
Vissim [231]

1995

Amesim [232]

1997
2002
2003
2004
2007
2009
2010
2011
2011

2012
2013

2015
2016
2017
2017

2018
2018

Aimsun [233]
SUMO [165]
dSPACE [160]
Webots [235]
USARSim [236]
VTD [237]
Pro SiVIC [238]
Unity [175]
Dynacar [240]

Gazebo [216]
PreScan [217]

BeamNG.tech [241]
SimMobility [243]
Udacity [188]
Carla [210]

Airsim [246]
rFpro [219]

2018

Cognata [220]

2018

2019
2020

2020

2020
2020

NVIDIA Drive Con-
stellation [218]
Deepdrive [247]
LGSVL [213]

Stu-

SCANeR
dio [248]
ADAMS [215]
CarMaker [214]

Description
Supporting design, simulation and deployment of ADS

Vehicle simulator supporting moving objects and sensors
A microscopic simulation modeling tool based on time interval
and driving behavior
Advanced modeling environment for performing simulation of
engineering systems
Real-time transportation management
Micro traffic simulation software
Supoorting simulation for HiL testing
Open-source multi platform robot simulation software
Supporting high-fidelity simulation of robot and environment
Visual simulation platform for complex traffic scene
Supporting multi-frequency sensor simulation
Supporting real-time 3D simulation of complex scenarios
Supporting real-time simulation for designing, developing and
verifying ADS
Open-source, scalable, flexible and multi-robot 3D simulator
Supporting sensor simulation, traffic modeling, HiL and ViL
testing, etc.
Supporting customized physical engine
Integrating various mobility-sensitive behavioral models
Open-source simulator for training and testing ADS
Open-source simulation platform supporting sensor models,
environmental conditions, etc
Open-source simulator based on Unreal Engine
Supporting driving simulation of ADS and vehicle dynamics
development
An artificial intelligence driven simulation platform for ADS
testing
Scalable and high-fidelity simulation platform for HiL testing

Reference
[102, 103, 124, 127,
167, 229]
[230]
[176]

-

[141]
[167, 171, 234]
-
[177]
-
[224]
-
[171, 239]
[101]

[146, 157, 168]
[108, 124]

[242]
-
[118]
[61, 114, 122, 133,
151, 244, 245]
-
-

-

-

Hardware independent platform
High-fidelity simulator supporting sensor simulation, vehicle
dynamics and control simulation, etc.
Flexible simulator allowing simulation configuration and re-
sults analysis
Supporting the analysis of vehicle dynamics
Platform for model-based design and testing of vehicle dynam-
ics

-
[21, 92, 125, 138, 142,
145, 179–182, 225]
[104]

-
[167, 176]

In conclusion, programming languages are increasingly deemed as powerful weapons for test

case generation in ADS testing, but they are not widely adopted yet in practice.

9 CHALLENGES AND OPPORTUNITIES
As this survey reveals, ADS testing has experienced rapid growth in recent years. Nevertheless,
there still exist many challenges and open questions in its development and deployment. Based on
our analysis of the collected literature and our discussions in each section, we list the challenges
and opportunities in this direction, as shown in Fig. 6. To account for it, there exist several solutions

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

38

S. Tang et al.

Table 12. Programming languages for scenario generation

Language

Dependencies

Supported simulators Other features

Reference

OpenScenario

Unified Modeling
Language (UML),
XML

GeoScenario [249] XML

CARLA, Matlab, Prescan,
etc.

A scenario is described in a “Storyboard” tag in
XML, which includes a series of events

[61]

An Unreal-based driving
simulator

The language is based on open street map stan-
dard. Users can either program by dragging icons,
or code in an XML editor.

-

Scenic [250]

stiEF [251]

SceML [252]

Imperative, object
oriented (Python-
like)

Built-in
Newtonian
Simulator, Carla, Grand
Theft Auto V, LGSVL,
Webots, etc.

It is a probabilistic programming language that
can specify the input distributions of machine
learning components, and use that information
for testing and analysis.

[179]

Domain specific
language

Graph-based mod-
eling framework

Vires VTD

CARLA

CommonRoad [253] XML

SUMO traffic simulator

SceGene [254]

A hierarchical rep-
resentation model

-

paracosm [255]

Reactive program-
ming model

Udacity’s
simulator

self-driving

It supports multilingual representations for sce-
nario description.

It allows information modeling in different depth,
to support scenarios in different abstraction levels.

-

-

It provides a benchmark set that contains scenar-
ios for the study of motion planning.

[94]

It supports scenario generation via bio-inspired
operations, such as crossover, mutation and selec-
tion.

It adopts reactive objects that allow to describe
temporal reactive behavior of entities. It also de-
fines coverage criteria for test case generation.

-

-

Fig. 6. Illustration of challenges and opportunities

to the first four challenges that could be improved, while the last three challenges still lack research
direction and require a long period of research.

Efficient test generation methods. Efficiency is one of the most important objectives in ADS
testing, since system executions, no matter in simulator environments or real world, are too
expensive. There have been many methods that aim to reduce the number of system executions,
e.g., training surrogate models [126], or adopting sampling-based methods [133], as discussed
in §7.1.1. However, there exist several limitations in these methods; for example, the process of
preparing training data in [126] for surrogate models is time-consuming. One potential future

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

39

direction is to explore the application of traditional cost reduction techniques, such as test selection
and test prioritization, to further accelerate the testing process.

Realisticness of test cases. Generating realistic test cases that can really threaten the safety of
ADS in the real world should be another important goal of test case generation. Unrealistic test
cases that cannot happen in the real world are meaningless and not worthy of taking care of.
However, compared to efficiency, this aspect is usually ignored. Generating realistic test cases is a
demand over different modules, and some existing works have paid attention to this problem. For
example, in the perception module testing, RP2 [24] is proposed to generate test cases under real
physical conditions; in the planning module testing, avoidable collision [97] is proposed to filter out
useless test cases; moreover, this is also a major issue in system-level testing, as discussed in §7.3.
In addition to these efforts, the problem is worthy of more attention, in order to find out those
really useful test cases.

Oracle problem for different modules. Although there have been many works that try to
design suitable oracles for different modules of ADS, there still remain many open challenges
in defining oracles regarding different characteristics of different modules. For the perception
module, as discussed in §6.2.4, the automatically labeling method in [57] targets only at semantic
segmentation models, so one future direction is to explore how to automatically generate high-
fidelity ground-truth labels for other types of models in the perception module. For the planning
module, as discussed in §6.3.4, the criteria such as avoidable collision [97] are ad hoc and may not be
generalized to other systems. Metamorphic relations are adopted by works [59, 107, 117, 225, 229]
for different modules, but it may lack sufficiently accuracy and so lead to false positives. Hence, it
remains a challenge to design more accurate and reliable oracles for the testing of different modules
in ADS.

Effective coverage criteria. Coverage criteria are used as a guidance to generate diverse test
cases for testing. As discussed in §6 and §7, various coverage criteria have been proposed for testing
different modules of the ADS, e.g., neuron coverage [67] for perception and end-to-end modules,
weight coverage [89] and route coverage [92] for the planning module. Notably, few coverage criteria
have been proposed for the control module, which indicates a future research direction. In addition,
combinatorial testing [71, 72, 141] is another effective technique that ensures the coverage of test
cases. One problem in the existing studies is that they mainly consider covering the spatial aspects
of the test cases; for instance, neuron coverage [67] is computed based on the activated neurons
in a DNN model. In practice, temporal aspects should also be considered during the execution of
the system; in the case of neuron coverage, it should reason about not only which neurons are
activated but also when the neurons are activated.

Online monitoring. In this work, we mainly see testing techniques for ADS based on the posterior
checking of the system execution; another effective quality assurance scheme is online monitoring
that monitors the system behavior at runtime. As advantages, online monitoring can detect unsafe
behavior during the system execution, and thus warn drivers to take actions to avoid the safety risk.
However, one challenge of this approach is how to develop techniques to automatically monitor the
system. As discussed in §6.2.4, there have been some works, e.g., [61], that rely on formal temporal
specifications to monitor the perception module at runtime. This is indeed an effective approach
that can solve the problem, and more efforts are deserved to further explore in this direction.

Fault analysis of system failure. As this survey shows, the function of an ADS relies on the
collaborative work of different modules; indeed, the wrong function of any module can cause a
failure on the system level. Therefore, one question arises that, when a system failure occurs, which
module should account for the failure. Currently, as discussed in §7.1.4, the research attention is

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

40

S. Tang et al.

mostly focused on failure detection, rather than fault analysis. Moreover, fault analysis of ADS is
challenging in nature, which requires to define the boundary of each module properly and make the
oracles of each module clear; sometimes, the failure of the system is not due to single modules but
the interactions between different modules. Therefore, not only effective fault analysis techniques
should be proposed, but their validation methods are also important.

Simulators vs. real world. Because of the high cost of real-world testing, simulator-based testing
is the most commonly used testing paradigm; however, even with modern high-fidelity simulators
(e.g., Carla and LGSVL), there is still a gap from real world testing. One potential research direction
is to explore how to utilize the results of simulator-based testing to reduce the cost of real-world
testing. Recently, lightweight mixed-reality testing schemes, including hardware-in-the-loop (HiL),
vehicle-in-the-loop (ViL) and scenario-in-the-loop (SciL) (more detail in §7.2), that mix the simulator-
based testing and the real-world testing, also emerge to achieve a trade-off between the two. While
HiL and ViL testing have developed quickly over the years, SciL testing, which is closest to the real
world, is still at a theoretical stage that is not yet widely adopted. As discussed in §7.3, there have
been several works that try to estimate how far is the simulator-based testing from the real-world
testing. Nevertheless, in the case of handling complex traffic scenarios in testing, there are still open
questions, such as the selection between simulator-based testing and real-world testing, and how
to mitigate the weaknesses of the selected testing paradigm, that are seeking for better answers.

10 CONCLUSION
This survey provides a comprehensive overview and analysis of the research works on ADS testing.
These testing works cover both module-level testing and system-level testing of ADS, and we also
include the works on empirical study w.r.t. system testing, mixed-reality testing, and real-world
testing. In the introduction to the testing of each module, we respectively unfold the landscape
of the literature from three perspectives, namely, test methodology, test oracle and test adequacy.
Based on the literature review, we perform statistics and analysis on the landscape of ADS testing,
and propose a number of challenges and research opportunities in this direction.

Our work gives a specific emphasis on the technical difference in the testing of different modules,
and also reveal the gaps between simulation-based testing and real-world testing. Moreover, our
analysis and discussion on the challenges and opportunities based on the literature review point out
the future direction of the research in this field. We hope that this work can inspire and motivate
more contributions in the safety assurance of ADS, and we also hope that ADS can be sufficiently
reliable to be adopted by more people as early as possible.

ACKNOWLEDGMENTS
This work was supported in part by Major Science and Technology Project of Anhui Province
under Grant 202103a05020009, in part by National Nature Science Foundation of China under Grant
61972373, in part by the Basic Research Program of Jiangsu Province under Grant BK20201192
and in part by the National Research Foundation Singapore under its NSoE Programme (Award
Number: NSOE-TSS2019-03). The research of Dr. Xue is also supported by CAS Pioneer Hundred
Talents Program of China.

REFERENCES

[1] Mordor Intelligence Inc. Autonomous (driverless) car market - growth, trends, covid-19 impact, and forecast
(2022 - 2027), 2022. URL https://www.mordorintelligence.com/industry-reports/autonomous-driverless-cars-market-
potential-estimation.

[2] Bay City News. Fatal crash on sb i-680 onramp in san jose, 2022. URL https://www.kron4.com/news/bay-area/fatal-

crash-on-sb-i-680-onramp-in-san-jose/.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

41

[3] Xinhai Zhang, Jianbo Tao, Kaige Tan, Martin Torngren, Jose Manuel Gaspar Sanchez, Muhammad Rusyadi Ramli, Xin
Tao, Magnus Gyllenhammar, Franz Wotawa, Naveen Mohan, et al. Finding critical scenarios for automated driving
systems: A systematic mapping study. IEEE Transactions on Software Engineering, 2022.

[4] Ziyuan Zhong, Yun Tang, Yuan Zhou, Vania de Oliveira Neves, Yang Liu, and Baishakhi Ray. A survey on scenario-
based testing for automated driving systems in high-fidelity simulation. arXiv preprint arXiv:2112.00964, 2021.
[5] Gunel Jahangirova, Andrea Stocco, and Paolo Tonella. Quality metrics and oracles for autonomous vehicles testing.
In 2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST), pages 194–204. IEEE, 2021.
[6] Simon J Julier and Jeffrey K Uhlmann. New extension of the kalman filter to nonlinear systems. In Signal processing,
sensor fusion, and target recognition VI, volume 3068, pages 182–193. International Society for Optics and Photonics,
1997.

[7] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region

proposal networks. Advances in neural information processing systems, 28, 2015.

[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement.
[9] Michael A Johnson and Mohammad H Moradi. PID control. Springer, 2005.
[10] Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer science & business media, 2013.
[11] Joshua Garcia, Yang Feng, Junjie Shen, Sumaya Almanee, Yuan Xia, and Qi Alfred Chen. A comprehensive study of
autonomous vehicle bugs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,
pages 385–396, 2020.

[12] Shuncheng Tang, Zhenya Zhang, Jia Tang, Lei Ma, and Yinxing Xue. Issue categorization and analysis of an open-
source driving assistant system. In 2021 IEEE International Symposium on Software Reliability Engineering Workshops
(ISSREW), pages 148–153. IEEE, 2021.

[13] Jonathan Petit, Bas Stottelaar, Michael Feiri, and Frank Kargl. Remote attacks on automated vehicles sensors:

Experiments on camera and lidar. Black Hat Europe, 11(2015):995, 2015.

[14] Hocheol Shin, Dohyun Kim, Yujin Kwon, and Yongdae Kim. Illusion and dazzle: Adversarial optical channel exploits
against lidars for automotive applications. In International Conference on Cryptographic Hardware and Embedded
Systems, pages 445–467. Springer, 2017.

[15] Chen Yan, Wenyuan Xu, and Jianhao Liu. Can you trust autonomous vehicles: Contactless attacks against sensors of

self-driving vehicle. Def Con, 24(8):109, 2016.

[16] Bing Shun Lim, Sye Loong Keoh, and Vrizlynn LL Thing. Autonomous vehicle ultrasonic sensor vulnerability and
impact assessment. In 2018 IEEE 4th World Forum on Internet of Things (WF-IoT), pages 231–236. IEEE, 2018.
[17] Qian Meng, Li-Ta Hsu, Bing Xu, Xiapu Luo, and Ahmed El-Mowafy. A gps spoofing generator using an open sourced

vector tracking-based receiver. Sensors, 19(18):3993, 2019.

[18] Kexiong Curtis Zeng, Shinan Liu, Yuanchao Shu, Dong Wang, Haoyu Li, Yanzhi Dou, Gang Wang, and Yaling Yang.
All your {GPS} are belong to us: Towards stealthy manipulation of road navigation systems. In 27th {USENIX}
security symposium ({USENIX} security 18), pages 1527–1544, 2018.

[19] Dudi Nassi, Raz Ben-Netanel, Yuval Elovici, and Ben Nassi. Mobilbye: attacking adas with camera spoofing. arXiv

preprint arXiv:1906.09765, 2019.

[20] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. Shapeshifter: Robust physical adversarial
attack on faster r-cnn object detector. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pages 52–68. Springer, 2018.

[21] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang Yang, Qi Alfred Chen, Mingyan Liu, and
Bo Li. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving
under physical-world attacks. In 2021 IEEE Symposium on Security and Privacy (SP), pages 176–194. IEEE, 2021.
[22] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai Chen. Seeing isn’t believing: Towards
more robust adversarial attack against real world object detectors. In Proceedings of the 2019 ACM SIGSAC Conference
on Computer and Communications Security, pages 1989–2004, 2019.

[23] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and
Z Morley Mao. Adversarial sensor attack on lidar-based perception in autonomous driving. In Proceedings of the 2019
ACM SIGSAC conference on computer and communications security, pages 2267–2281, 2019.

[24] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi
Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 1625–1634, 2018.

[25] Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z Morley Mao. Towards robust lidar-based perception in autonomous
driving: General black-box adversarial sensor attack and countermeasures. In 29th {USENIX} Security Symposium
({USENIX} Security 20), pages 877–894, 2020.

[26] Xing Xu, Jingran Zhang, Yujie Li, Yichuan Wang, Yang Yang, and Heng Tao Shen. Adversarial attack against urban
scene segmentation for autonomous vehicles. IEEE Transactions on Industrial Informatics, 17(6):4117–4126, 2020.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

42

S. Tang et al.

[27] Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, and Sanjit A Seshia. Systematic testing of

convolutional neural networks for autonomous driving. arXiv preprint arXiv:1708.03309, 2017.

[28] Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie, and Dacheng Tao. Perceptual-sensitive
gan for generating adversarial patches. In Proceedings of the AAAI conference on artificial intelligence, volume 33,
pages 1028–1035, 2019.

[29] Zuobin Xiong, Honghui Xu, Wei Li, and Zhipeng Cai. Multi-source adversarial sample attack on autonomous vehicles.

IEEE Transactions on Vehicular Technology, 70(3):2822–2835, 2021.

[30] Huma Rehman, Andreas Ekelhart, and Rudolf Mayer. Backdoor attacks in neural networks–a systematic evaluation
on multiple traffic sign datasets. In International Cross-Domain Conference for Machine Learning and Knowledge
Extraction, pages 285–300. Springer, 2019.

[31] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning

attack on neural networks. 2017.

[32] Naman Patel, Prashanth Krishnamurthy, Siddharth Garg, and Farshad Khorrami. Bait and switch: Online training

data poisoning of autonomous driving systems. arXiv preprint arXiv:2011.04065, 2020.

[33] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. Trojan attack on deep generative models in
autonomous driving. In International Conference on Security and Privacy in Communication Systems, pages 299–318.
Springer, 2019.

[34] Men Long, Chwan-Hwa Wu, and John Y Hung. Denial of service attacks on network-based control systems: impact

and mitigation. IEEE Transactions on Industrial Informatics, 1(2):85–96, 2005.

[35] Meital Ben Sinai, Nimrod Partush, Shir Yadid, and Eran Yahav. Exploiting social navigation. arXiv preprint

arXiv:1410.0151, 2014.

[36] Claes Wohlin. Guidelines for snowballing in systematic literature studies and a replication in software engineering.
In Martin J. Shepperd, Tracy Hall, and Ingunn Myrtveit, editors, 18th International Conference on Evaluation and
Assessment in Software Engineering, EASE ’14, London, England, United Kingdom, May 13-14, 2014, pages 38:1–38:10.
ACM, 2014. doi: 10.1145/2601248.2601268. URL https://doi.org/10.1145/2601248.2601268.

[37] Testing of Autonomous Vehicles with a Driver. State of california department of motor vehicles. https://www.dmv.ca.

gov/portal/dmv/detail/vr/autonomous/testing, 2019.

[38] Zi Peng, Jinqiu Yang, Tse-Hsun Chen, and Lei Ma. A first look at the integration of machine learning models in
complex autonomous driving systems: a case study on apollo. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1240–1250, 2020.
[39] Vinayak V Dixit, Sai Chand, and Divya J Nair. Autonomous vehicles: disengagements, accidents and reaction times.

PLoS one, 11(12):e0168054, 2016.

[40] Francesca Favarò, Sky Eurich, and Nazanin Nader. Autonomous vehicles’ disengagements: Trends, triggers, and

regulatory limitations. Accident Analysis & Prevention, 110:136–148, 2018.

[41] Alexandra M Boggs, Ramin Arvin, and Asad J Khattak. Exploring the who, what, when, where, and why of automated

vehicle disengagements. Accident Analysis & Prevention, 136:105406, 2020.

[42] Song Wang and Zhixia Li. Exploring causes and effects of automated vehicle disengagement using statistical modeling

and classification tree based on field test data. Accident Analysis & Prevention, 129:44–54, 2019.

[43] Hananeh Alambeigi, Anthony D McDonald, and Srinivas R Tankasala. Crash themes in automated vehicles: A topic
modeling analysis of the california department of motor vehicles automated vehicle crash database. arXiv preprint
arXiv:2001.11087, 2020.

[44] Alexandra M Boggs, Behram Wali, and Asad J Khattak. Exploratory analysis of automated vehicle crashes in california:
A text analytics & hierarchical bayesian heterogeneity-based approach. Accident Analysis & Prevention, 135:105354,
2020.

[45] Shervin Hajinia Leilabadi and Stephan Schmidt. In-depth analysis of autonomous vehicle collisions in california. In

2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 889–893. IEEE, 2019.

[46] Yu Song, Madhav V Chitturi, and David A Noyce. Automated vehicle crash sequences: Patterns and potential uses in

safety testing. Accident Analysis & Prevention, 153:106017, 2021.

[47] Kenneth E Train. Discrete choice methods with simulation. Cambridge university press, 2009.
[48] Matti Kutila, Pasi Pyykönen, Werner Ritter, Oliver Sawade, and Bernd Schäufele. Automotive lidar sensor development
scenarios for harsh weather conditions. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems
(ITSC), pages 265–270. IEEE, 2016.

[49] Matti Kutila, Pasi Pyykönen, Hanno Holzhüter, Michele Colomb, and Pierre Duthon. Automotive lidar performance
verification in fog and rain. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages
1695–1701. IEEE, 2018.

[50] Li Tang, Yunpeng Shi, Qing He, Adel W Sadek, and Chunming Qiao. Performance test of autonomous vehicle lidar

sensors under different weather conditions. Transportation research record, 2674(1):319–329, 2020.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

43

[51] Mobileye. https://www.mobileye.com/, 2022.
[52] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and

Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.

[53] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In

International conference on machine learning, pages 284–293. PMLR, 2018.

[54] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with
atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer
vision (ECCV), pages 801–818, 2018.

[55] Chaowei Xiao, Bo Li, Jun Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples with
adversarial networks. In 27th International Joint Conference on Artificial Intelligence, IJCAI 2018, pages 3905–3911.
International Joint Conferences on Artificial Intelligence, 2018.

[56] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for
raindrop removal from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 2482–2491, 2018.

[57] Wei Zhou, Julie Stephany Berrio, Stewart Worrall, and Eduardo Nebot. Automated evaluation of semantic segmentation
robustness for autonomous driving. IEEE Transactions on Intelligent Transportation Systems, 21(5):1951–1963, 2019.
[58] Jinyang Shao. Testing object detection for autonomous driving systems via 3d reconstruction. In 2021 IEEE/ACM 43rd
International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pages 117–119. IEEE,
2021.

[59] Manikandasriram Srinivasan Ramanagopal, Cyrus Anderson, Ram Vasudevan, and Matthew Johnson-Roberson.
Failing to learn: Autonomously identifying perception failures for self-driving cars. IEEE Robotics and Automation
Letters, 3(4):3860–3867, 2018.

[60] Adel Dokhanchi, Heni Ben Amor, Jyotirmoy V Deshmukh, and Georgios Fainekos. Evaluating perception systems for
autonomous vehicles using quality temporal logic. In International Conference on Runtime Verification, pages 409–416.
Springer, 2018.

[61] Anand Balakrishnan, Jyotirmoy Deshmukh, Bardh Hoxha, Tomoya Yamaguchi, and Georgios Fainekos. Percemon:
Online monitoring for perception systems. In International Conference on Runtime Verification, pages 297–308. Springer,
2021.

[62] Daniel Kondermann, Rahul Nair, Katrin Honauer, Karsten Krispin, Jonas Andrulis, Alexander Brock, Burkhard
Gussefeld, Mohsen Rahimimoghaddam, Sabine Hofmann, Claus Brenner, et al. The hci benchmark suite: Stereo
and flow ground truth with uncertainties for urban autonomous driving. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, pages 19–28, 2016.

[63] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2446–2454, 2020.

[64] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. Metamorphic testing: a new approach for generating next test

cases. arXiv preprint arXiv:2002.12543, 2020.

[65] Amir Pnueli. The temporal logic of programs. In 18th Annual Symposium on Foundations of Computer Science (sfcs

1977), pages 46–57. ieee, 1977.

[66] Ron Koymans. Specifying real-time properties with metric temporal logic. Real-time systems, 2(4):255–299, 1990.
[67] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of deep learning

systems. In proceedings of the 26th Symposium on Operating Systems Principles, pages 1–18, 2017.

[68] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu,
et al. Deepgauge: Multi-granularity testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, pages 120–131, 2018.

[69] Jinhan Kim, Robert Feldt, and Shin Yoo. Guiding deep learning system testing using surprise adequacy. In 2019

IEEE/ACM 41st International Conference on Software Engineering (ICSE), pages 1039–1049. IEEE, 2019.

[70] Changhai Nie and Hareton Leung. A survey of combinatorial testing. ACM Computing Surveys (CSUR), 43(2):1–29,

2011.

[71] Christoph Gladisch, Christian Heinzemann, Martin Herrmann, and Matthias Woehrle. Leveraging combinatorial
testing for safety-critical computer vision datasets. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops, pages 324–325, 2020.

[72] Chih-Hong Cheng, Chung-Hao Huang, and Hirotoshi Yasuoka. Quantitative projection coverage for testing ml-
enabled autonomous systems. In International Symposium on Automated Technology for Verification and Analysis,
pages 126–142. Springer, 2018.
[73] V. L. Inc. Vlp-16 user manual, 2018.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

44

S. Tang et al.

[74] Andreas Mogelmose, Mohan Manubhai Trivedi, and Thomas B Moeslund. Vision-based traffic sign detection
IEEE Transactions on Intelligent

and analysis for intelligent driver assistance systems: Perspectives and survey.
Transportation Systems, 13(4):1484–1497, 2012.

[75] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine learning algorithms
for traffic sign recognition. Neural Networks, (0):–, 2012. ISSN 0893-6080. doi: 10.1016/j.neunet.2012.02.016. URL
http://www.sciencedirect.com/science/article/pii/S0893608012000457.

[76] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural

networks. Advances in neural information processing systems, 25, 2012.

[77] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from
point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 770–779, 2019.
[78] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 12697–12705, 2019.

[79] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[80] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,
and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861, 2017.

[81] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer. Squeezedet: Unified, small, low power fully convolutional
neural networks for real-time object detection for autonomous driving. In Proceedings of the IEEE conference on
computer vision and pattern recognition workshops, pages 129–137, 2017.

[82] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object
detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788, 2016.
[83] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv

preprint arXiv:1409.1556, 2014.

[84] V Yadav. p2-trafficsigns. https://github.com/vxy10/p2-TrafficSigns.
[85] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. In International conference on machine learning, pages 1558–1566. PMLR, 2016.
[86] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[87] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages
286–301, 2018.

[88] Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. No need to worry about adversarial examples in object

detection in autonomous vehicles. arXiv preprint arXiv:1707.03501, 2017.

[89] Thomas Laurent, Paolo Arcaini, Fuyuki Ishikawa, and Anthony Ventresque. A mutation-based approach for assessing
weight coverage of a path planner. In 2019 26th Asia-Pacific Software Engineering Conference (APSEC), pages 94–101.
IEEE, 2019.

[90] Thomas Laurent, Paolo Arcaini, Fuyuki Ishikawa, and Anthony Ventresque. Achieving weight coverage for an
autonomous driving system with search-based test generation. In 2020 25th International Conference on Engineering
of Complex Computer Systems (ICECCS), pages 93–102. IEEE, 2020.

[91] Paolo Arcaini, Xiao-Yi Zhang, and Fuyuki Ishikawa. Targeting patterns of driving characteristics in testing autonomous
driving systems. In 2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST), pages 295–305.
IEEE, 2021.

[92] Yun Tang, Yuan Zhou, Fenghua Wu, Yang Liu, Jun Sun, Wuling Huang, and Gang Wang. Route coverage testing for
autonomous vehicles via map modeling. In 2021 IEEE International Conference on Robotics and Automation (ICRA),
pages 11450–11456. IEEE, 2021.

[93] Matthias Althoff and Sebastian Lutz. Automatic generation of safety-critical test scenarios for collision avoidance of

road vehicles. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1326–1333. IEEE, 2018.

[94] Moritz Klischat and Matthias Althoff. Generating critical test scenarios for automated vehicles with evolutionary

algorithms. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 2352–2358. IEEE, 2019.

[95] Till Menzel, Gerrit Bagschik, and Markus Maurer. Scenarios for development, test and validation of automated

vehicles. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1821–1827. IEEE, 2018.

[96] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolu-

tionary computation, 1(1):1–23, 1993.

[97] Alessandro Calò, Paolo Arcaini, Shaukat Ali, Florian Hauer, and Fuyuki Ishikawa. Generating avoidable collision
scenarios for testing autonomous driving systems. In 2020 IEEE 13th International Conference on Software Testing,

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

45

Validation and Verification (ICST), pages 375–386. IEEE, 2020.

[98] Alessandro Calò, Paolo Arcaini, Shaukat Ali, Florian Hauer, and Fuyuki Ishikawa. Simultaneously searching and
solving multiple avoidable collisions for testing autonomous driving systems. In Proceedings of the 2020 Genetic and
Evolutionary Computation Conference, pages 1055–1063, 2020.

[99] Carl Adam Petri. Kommunikation mit automaten. 1962.
[100] open source. Commonroad. http://https://commonroad.in.tum.de/.
[101] Garazi Juez Uriagereka, Ray Lattarulo, Joshue Pérez Rastelli, Estibaliz Amparan Calonge, Alejandra Ruiz Lopez, and
Huascar Espinoza Ortiz. Fault injection method for safety and controllability evaluation of automated driving. In
2017 IEEE Intelligent Vehicles Symposium (IV), pages 1867–1872. IEEE, 2017.

[102] Cumhur Erkan Tuncali, Theodore P Pavlic, and Georgios Fainekos. Utilizing s-taliro as an automatic test generation
framework for autonomous vehicles. In 2016 ieee 19th international conference on intelligent transportation systems
(itsc), pages 1470–1475. IEEE, 2016.

[103] Cumhur Erkan Tuncali and Georgios Fainekos. Rapidly-exploring random trees for testing automated vehicles. In

2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 661–666. IEEE, 2019.

[104] Adel Djoudi, Loic Coquelin, and Rémi Regnier. A simulation-based framework for functional testing of automated
driving controllers. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1–6.
IEEE, 2020.

[105] David González and Joshué Pérez. Control architecture for cybernetic transportation systems in urban environments.

In 2013 IEEE Intelligent Vehicles Symposium (IV), pages 1119–1124. IEEE, 2013.

[106] Kun Zhang, Jonathan Sprinkle, and Ricardo G Sanfelice. A hybrid model predictive controller for path planning
and path following. In Proceedings of the ACM/IEEE Sixth International Conference on Cyber-Physical Systems, pages
139–148, 2015.

[107] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated testing of deep-neural-network-driven

autonomous cars. In Proceedings of the 40th international conference on software engineering, pages 303–314, 2018.

[108] Zhong Li, Minxue Pan, Tian Zhang, and Xuandong Li. Testing dnn-based autonomous driving systems under critical
environmental conditions. In International Conference on Machine Learning, pages 6471–6482. PMLR, 2021.
[109] Husheng Zhou, Wei Li, Zelun Kong, Junfeng Guo, Yuqun Zhang, Bei Yu, Lingming Zhang, and Cong Liu. Deepbillboard:
Systematic physical-world testing of autonomous driving systems. In 2020 IEEE/ACM 42nd International Conference
on Software Engineering (ICSE), pages 347–358. IEEE, 2020.

[110] Svetlana Pavlitskaya, Sefa Ünver, and J Marius Zöllner. Feasibility and suppression of adversarial patch attacks on
end-to-end vehicle control. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),
pages 1–8. IEEE, 2020.

[111] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning

models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.

[112] Christian Hubschneider, Andre Bauer, Michael Weber, and J Marius Zöllner. Adding navigation to the equation:
Turning decisions for end-to-end vehicle control. In 2017 IEEE 20th International Conference on Intelligent Transportation
Systems (ITSC), pages 1–8. IEEE, 2017.

[113] Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, and Xuan Zhang. Simple physical adversarial examples
against end-to-end autonomous driving models. In 2019 IEEE International Conference on Embedded Software and
Systems (ICESS), pages 1–7. IEEE, 2019.

[114] Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik, and Xuan Zhang. Attacking
vision-based perception in end-to-end autonomous driving models. Journal of Systems Architecture, 110:101766, 2020.
[115] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions,
with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599,
2010.

[116] Zelun Kong, Junfeng Guo, Ang Li, and Cong Liu. Physgan: Generating physical-world-resilient adversarial examples
for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
14254–14263, 2020.

[117] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid. Deeproad: Gan-based metamorphic
In 2018 33rd IEEE/ACM International

testing and input validation framework for autonomous driving systems.
Conference on Automated Software Engineering (ASE), pages 132–142. IEEE, 2018.

[118] Andrea Stocco, Michael Weiss, Marco Calzana, and Paolo Tonella. Misbehaviour prediction for autonomous driving
systems. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pages 359–371, 2020.
[119] Nvidia-autopilot-keras: End to end learning for self-driving cars. https://github.com/0bserver07/Nvidia-Autopilot-

Keras.

[120] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

46

arXiv:1604.07316, 2016.

S. Tang et al.

[121] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al.
Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE 29th International Symposium on Software
Reliability Engineering (ISSRE), pages 100–111. IEEE, 2018.

[122] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and Alexey Dosovitskiy. End-to-end driving
via conditional imitation learning. In 2018 IEEE international conference on robotics and automation (ICRA), pages
4693–4700. IEEE, 2018.

[123] Tommaso Dreossi, Alexandre Donzé, and Sanjit A Seshia. Compositional falsification of cyber-physical systems with

machine learning components. Journal of Automated Reasoning, 63(4):1031–1053, 2019.

[124] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas Stifter. Testing vision-based control systems using
learnable evolutionary algorithms. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),
pages 1016–1026. IEEE, 2018.

[125] Guanpeng Li, Yiran Li, Saurabh Jha, Timothy Tsai, Michael Sullivan, Siva Kumar Sastry Hari, Zbigniew Kalbarczyk, and
Ravishankar Iyer. Av-fuzzer: Finding safety violations in autonomous driving systems. In 2020 IEEE 31st International
Symposium on Software Reliability Engineering (ISSRE), pages 25–36. IEEE, 2020.

[126] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas Stifter. Testing advanced driver assistance systems
using multi-objective search and neural networks. In Proceedings of the 31st IEEE/ACM international conference on
automated software engineering, pages 63–74, 2016.

[127] Halil Beglerovic, Michael Stolz, and Martin Horn. Testing of autonomous vehicles using surrogate models and
stochastic optimization. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC), pages
1–6. IEEE, 2017.

[128] Mark Koren, Saud Alsaif, Ritchie Lee, and Mykel J Kochenderfer. Adaptive stress testing for autonomous vehicles. In

2018 IEEE Intelligent Vehicles Symposium (IV), pages 1–7. IEEE, 2018.

[129] Anthony Corso, Peter Du, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Adaptive stress testing with reward
augmentation for autonomous vehicle validatio. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
pages 163–168. IEEE, 2019.

[130] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. On a formal model of safe and scalable self-driving

cars. arXiv preprint arXiv:1708.06374, 2017.

[131] Yasuhiro Akagi, Ryosuke Kato, Sou Kitajima, Jacobo Antona-Makoshi, and Nobuyuki Uchida. A risk-index based
sampling method to generate scenarios for the evaluation of automated driving vehicle safety. In 2019 IEEE Intelligent
Transportation Systems Conference (ITSC), pages 667–672. IEEE, 2019.

[132] Surya T Tokdar and Robert E Kass. Importance sampling: a review. Wiley Interdisciplinary Reviews: Computational

Statistics, 2(1):54–60, 2010.

[133] Justin Norden, Matthew O’Kelly, and Aman Sinha. Efficient black-box assessment of autonomous vehicle safety.

arXiv preprint arXiv:1912.03618, 2019.

[134] Ding Zhao, Henry Lam, Huei Peng, Shan Bao, David J LeBlanc, Kazutoshi Nobukawa, and Christopher S Pan. Accel-
erated evaluation of automated vehicles safety in lane-change scenarios based on importance sampling techniques.
IEEE transactions on intelligent transportation systems, 18(3):595–607, 2016.

[135] Zhiyuan Huang, Henry Lam, David J LeBlanc, and Ding Zhao. Accelerated evaluation of automated vehicles using

piecewise mixture models. IEEE Transactions on Intelligent Transportation Systems, 19(9):2845–2855, 2017.

[136] Zhiyuan Huang, Henry Lam, and Ding Zhao. An accelerated testing approach for automated vehicles with background
traffic described by joint distributions. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems
(ITSC), pages 933–938. IEEE, 2017.

[137] Ding Zhao, Xianan Huang, Huei Peng, Henry Lam, and David J LeBlanc. Accelerated evaluation of automated vehicles

in car-following maneuvers. IEEE Transactions on Intelligent Transportation Systems, 19(3):733–744, 2017.

[138] Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jia, Xue Lin, and Qi Alfred Chen. Dirty road can attack: Security of
deep learning based automated lane centering under {Physical-World} attack. In 30th USENIX Security Symposium
(USENIX Security 21), pages 3309–3326, 2021.

[139] Abu Hasnat Mohammad Rubaiyat, Yongming Qin, and Homa Alemzadeh. Experimental resilience assessment of an
open-source driving agent. In 2018 IEEE 23rd Pacific rim international symposium on dependable computing (PRDC),
pages 54–63. IEEE, 2018.

[140] Bowen Weng, Sughosh J Rao, Eeshan Deosthale, Scott Schnelle, and Frank Barickman. Model predictive instantaneous
safety metric for evaluation of automated driving systems. In 2020 IEEE Intelligent Vehicles Symposium (IV), pages
1899–1906. IEEE, 2020.

[141] Cumhur Erkan Tuncali, Georgios Fainekos, Hisahiro Ito, and James Kapinski. Simulation-based adversarial test
generation for autonomous vehicles with machine learning components. In 2018 IEEE Intelligent Vehicles Symposium
(IV), pages 1555–1562. IEEE, 2018.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

47

[142] John Seymour, Quang-Hung Luu, et al. An empirical testing of autonomous vehicle simulator system for urban

driving. In 2021 IEEE International Conference on Artificial Intelligence Testing (AITest), pages 111–117. IEEE, 2021.

[143] Jia Cheng Han and Zhi Quan Zhou. Metamorphic fuzz testing of autonomous vehicles. In Proceedings of the IEEE/ACM

42nd International Conference on Software Engineering Workshops, pages 380–385, 2020.

[144] Florian Hauer, Tabea Schmidt, Bernd Holzmüller, and Alexander Pretschner. Did we test all scenarios for automated
and autonomous driving systems? In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 2950–2955.
IEEE, 2019.

[145] Yun Tang, Yuan Zhou, Yang Liu, Jun Sun, and Gang Wang. Collision avoidance testing for autonomous driving

systems on complete maps. In 2021 IEEE Intelligent Vehicles Symposium (IV), pages 179–185. IEEE, 2021.

[146] István Majzik, Oszkár Semeráth, Csaba Hajdu, Kristóf Marussy, Zoltán Szatmári, Zoltán Micskei, András Vörös,
Aren A Babikian, and Dániel Varró. Towards system-level testing with coverage guarantees for autonomous vehicles.
In 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems (MODELS),
pages 89–94. IEEE, 2019.

[147] Jonas Kerber, Sebastian Wagner, Korbinian Groh, Dominik Notz, Thomas Kühbeck, Daniel Watzenig, and Alois Knoll.
Clustering of the scenario space for the assessment of automated driving. In 2020 IEEE Intelligent Vehicles Symposium
(IV), pages 578–583. IEEE, 2020.

[148] Taeyoung Lee, Kyongsu Yi, Jangseop Kim, and Jaewan Lee. Development and evaluations of advanced emergency
braking system algorithm for the commercial vehicle. In 22nd International Technical Conference on the Enhanced
Safety of Vehicles (ESV) National Highway Traffic Safety Administration, 2011.
[149] Udacity’s self-driving car simulator. https://github.com/udacity/self-driving-car-sim.
[150] International electronics & engineering. https://www.iee.lu/.(2018).
[151] Ziyuan Zhong, Zhisheng Hu, Shengjian Guo, Xinyang Zhang, Zhenyu Zhong, and Baishakhi Ray. Detecting safety

problems of multi-sensor fusion in autonomous driving. arXiv preprint arXiv:2109.06404, 2021.

[152] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observations and microscopic

simulations. Physical review E, 62(2):1805, 2000.

[153] Uwe Kiencke and Lars Nielsen. Automotive Control Systems: For Engine, Driveline and Vehicle. Springer-Verlag, Berlin,

Heidelberg, 1st edition, 2000. ISBN 3540669221.

[154] Christof Gauss et al. Comparative test of advanced emergency braking systems. ADAC Internal Test Report, ADAC

Technik Zentrum, 2012.

[155] Yu Chen, Shitao Chen, Tangyike Zhang, Songyi Zhang, and Nanning Zheng. Autonomous vehicle testing and
In 2018 IEEE Intelligent Vehicles

validation platform: Integrated simulation system with hardware in the loop.
Symposium (IV), pages 949–956. IEEE, 2018.

[156] Microsoft. Openstreetmap. http://www.openstreetmap.org/.
[157] Shitao Chen, Yu Chen, Songyi Zhang, and Nanning Zheng. A novel integrated simulation and testing platform for

self-driving cars with hardware in the loop. IEEE Transactions on Intelligent Vehicles, 4(3):425–436, 2019.

[158] Yuting Fu, Andrei Terechko, Tjerk Bijlsma, Pieter JL Cuijpers, Jeroen Redegeld, and Ali Osman Örs. A retargetable
fault injection framework for safety validation of autonomous vehicles. In 2019 IEEE International Conference on
Software Architecture Companion (ICSA-C), pages 69–76. IEEE, 2019.

[159] Zejiang Wang, Jingqiang Zha, and Junmin Wang. Autonomous vehicle trajectory following: A flatness model
predictive control approach with hardware-in-the-loop verification. IEEE Transactions on Intelligent Transportation
Systems, 2020.

[160] dSPACE. dspace. https://www.dspace.com/en/ltd/home.cfm.
[161] Craig Brogle, Chao Zhang, Kai Li Lim, and Thomas Bräunl. Hardware-in-the-loop autonomous driving simulation

without real-time constraints. IEEE Transactions on Intelligent Vehicles, 4(3):375–384, 2019.

[162] Ying Gao, Zhigang Xu, Xiangmo Zhao, Guiping Wang, and Quan Yuan. Hardware-in-the-loop simulation platform
for autonomous vehicle aeb prototyping and validation. In 2020 IEEE 23rd International Conference on Intelligent
Transportation Systems (ITSC), pages 1–6. IEEE, 2020.

[163] FE Udwadia and RE Kalaba. Analytical dynamics a new approach cambridge university press. New York, NY, 1996.
[164] Tamás Tettamanti, Mátyás Szalai, Sándor Vass, and Viktor Tihanyi. Vehicle-in-the-loop test environment for
autonomous driving with microscopic traffic simulation. In 2018 IEEE International Conference on Vehicular Electronics
and Safety (ICVES), pages 1–6. IEEE, 2018.

[165] Daniel Krajzewicz, Georg Hertkorn, Christian Rössel, and Peter Wagner. Sumo (simulation of urban mobility)-an open-
source traffic simulation. In Proceedings of the 4th middle East Symposium on Simulation and Modelling (MESM20002),
pages 183–187, 2002.

[166] S Alireza Fayazi, Ardalan Vahidi, and Andre Luckow. A vehicle-in-the-loop (vil) verification of an all-autonomous

intersection control scheme. Transportation Research Part C: Emerging Technologies, 107:193–210, 2019.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

48

S. Tang et al.

[167] Selim Solmaz, Martin Rudigier, and Marlies Mischinger. A vehicle-in-the-loop methodology for evaluating automated

driving functions in virtual traffic. In 2020 IEEE Intelligent Vehicles Symposium (IV), pages 1465–1471. IEEE, 2020.

[168] Yu Chen, Shitao Chen, Tong Xiao, Songyi Zhang, Qian Hou, and Nanning Zheng. Mixed test environment-based
vehicle-in-the-loop validation-a new testing approach for autonomous vehicles. In 2020 IEEE Intelligent Vehicles
Symposium (IV), pages 1283–1289. IEEE, 2020.

[169] Hexuan Li, Demin Nalic, Vamsi Makkapati, Arno Eichberger, Xuan Fang, and Tamás Tettamanti. A real-time co-
simulation framework for virtual test and validation on a high dynamic vehicle test bed. In 2021 IEEE Intelligent
Vehicles Symposium (IV), pages 1132–1137. IEEE, 2021.

[170] Axel Diewald, Clemens Kurz, Prasanna Venkatesan Kannan, Martin Gießler, Mario Pauli, Benjamin Göttel, Thorsten
Kayser, Frank Gauterin, and Thomas Zwick. Radar target simulation for vehicle-in-the-loop testing. Vehicles, 3(2):
257–271, 2021.

[171] Zsolt Szalay, Mátyás Szalai, Bálint Tóth, Tamás Tettamanti, and Viktor Tihanyi. Proof of concept for scenario-in-the-
loop (scil) testing for autonomous vehicle technology. In 2019 IEEE International Conference on Connected Vehicles and
Expo (ICCVE), pages 1–5. IEEE, 2019.

[172] Márton Tamás Horváth, Tamás Tettamanti, Balázs Varga, and Zsolt Szalay. The scenario-in-the-loop (scil) automotive
simulation concept and its realisation principles for traffic control. In Proceedings of the 8th Symposium of the European
Association for Research in Transportation, Budapest, Hungray, pages 4–6, 2019.

[173] Márton Tamás Horváth, Qiong Lu, Tamás Tettamanti, Árpád Török, and Zsolt Szalay. Vehicle-in-the-loop (vil) and
scenario-in-the-loop (scil) automotive simulation concepts from the perspectives of traffic simulation and traffic
control. Transport and Telecommunication, 20(2):153–161, 2019.

[174] Mátyás Szalai, Balázs Varga, Tamás Tettamanti, and Viktor Tihanyi. Mixed reality test environment for autonomous
cars using unity 3d and sumo. In 2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics
(SAMI), pages 73–78. IEEE, 2020.
[175] Unity. Unity. https://unity.com/.
[176] Demin Nalic, Arno Eichberger, Georg Hanzl, Martin Fellendorf, and Branko Rogic. Development of a co-simulation
framework for systematic generation of scenarios for testing and validation of automated driving systems. In 2019
IEEE Intelligent Transportation Systems Conference (ITSC), pages 1895–1901. IEEE, 2019.

[177] Sai Krishna Bashetty, Heni Ben Amor, and Georgios Fainekos. Deepcrashtest: Turning dashcam videos into virtual
crash tests for automated driving systems. In 2020 IEEE International Conference on Robotics and Automation (ICRA),
pages 11353–11360. IEEE, 2020.

[178] Michał Antkiewicz, Maximilian Kahn, Michael Ala, Krzysztof Czarnecki, Paul Wells, Atul Acharya, and Sven Beiker.
Modes of automated driving system scenario testing: Experience report and recommendations. SAE International
Journal of Advances and Current Practices in Mobility, 2(2020-01-1204):2248–2266, 2020.

[179] Daniel J Fremont, Edward Kim, Yash Vardhan Pant, Sanjit A Seshia, Atul Acharya, Xantha Bruso, Paul Wells, Steve
Lemke, Qiang Lu, and Shalin Mehta. Formal scenario-based testing of autonomous vehicles: From simulation to the
real world. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1–8. IEEE,
2020.

[180] Yize Shi, Chengjie Lu, Man Zhang, Huihui Zhang, Tao Yue, and Shaukat Ali. Restricted natural language and
model-based adaptive test generation for autonomous driving. In 2021 ACM/IEEE 24th International Conference on
Model Driven Engineering Languages and Systems (MODELS), pages 101–111. IEEE, 2021.

[181] Andrea Piazzoni, Jim Cherian, Mohamed Azhar, Jing Yew Yap, James Lee Wei Shung, and Roshan Vijay. Vista:
a framework for virtual scenario-based testing of autonomous vehicles. In 2021 IEEE International Conference on
Artificial Intelligence Testing (AITest), pages 143–150. IEEE, 2021.

[182] Zhisheng Hu, Shengjian Guo, Zhenyu Zhong, and Kang Li. Coverage-based scene fuzzing for virtual autonomous

driving testing. arXiv preprint arXiv:2106.00873, 2021.

[183] Changwen Li, Chih-Hong Cheng, Tiantian Sun, Yuhang Chen, and Rongjie Yan. Comopt: Combination and optimiza-

tion for testing autonomous driving systems. arXiv preprint arXiv:2110.00761, 2021.

[184] Lama J Moukahal, Mohammad Zulkernine, and Martin Soukup. Vulnerability-oriented fuzz testing for connected

autonomous vehicle systems. IEEE Transactions on Reliability, 2021.

[185] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The

International Journal of Robotics Research, 32(11):1231–1237, 2013.

[186] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. Machine learning testing: Survey, landscapes and horizons. IEEE

Transactions on Software Engineering, 2020.

[187] Paul Spannaus, Peter Zechel, and Kilian Lenz. Automatum data: Drone-based highway dataset for the development
and validation of automated driving software for research and commercial applications. In 2021 IEEE Intelligent
Vehicles Symposium (IV), pages 1372–1377. IEEE, 2021.

[188] Udacity. Udacity. https://fcav.engin.umich.edu/research/failing-to-learn.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

49

[189] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020.

[190] Siddharth Agarwal, Ankit Vora, Gaurav Pandey, Wayne Williams, Helen Kourous, and James McBride. Ford multi-av

seasonal dataset. The International Journal of Robotics Research, 39(12):1367–1376, 2020.

[191] Baidu. Apolloscape. https://github.com/ApolloScapeAuto/dataset-api.
[192] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for
semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages
4990–4999, 2017.

[193] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, and Trevor Darrell. Bdd100k:
A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2(5):6, 2018.
[194] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The oxford robotcar dataset. The

International Journal of Robotics Research, 36(1):3–15, 2017.

[195] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,
Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016.

[196] Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset.

arXiv preprint arXiv:1812.05752, 2018.

[197] Robert Krajewski, Julian Bock, Laurent Kloeker, and Lutz Eckstein. The highd dataset: A drone dataset of naturalistic
vehicle trajectories on german highways for validation of highly automated driving systems. In 2018 21st International
Conference on Intelligent Transportation Systems (ITSC), pages 2118–2125. IEEE, 2018.

[198] Eder Santana and George Hotz. Learning a driving simulator. arXiv preprint arXiv:1608.01230, 2016.
[199] Elias Strigel, Daniel Meissner, Florian Seeliger, Benjamin Wilking, and Klaus Dietmayer. The ko-per intersection
laserscanner and video dataset. In 17th International IEEE Conference on Intelligent Transportation Systems (ITSC),
pages 1900–1901. IEEE, 2014.

[200] Julian Bock, Robert Krajewski, Tobias Moers, Steffen Runde, Lennart Vater, and Lutz Eckstein. The ind dataset: A
drone dataset of naturalistic road user trajectories at german intersections. In 2020 IEEE Intelligent Vehicles Symposium
(IV), pages 1929–1934. IEEE, 2020.

[201] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Citypersons: A diverse dataset for pedestrian detection. In

Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3221, 2017.

[202] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette: Human
trajectory understanding in crowded scenes. In European conference on computer vision, pages 549–565. Springer,
2016.

[203] Dongfang Yang. Dut. https://github.com/dongfang-steven-yang/vci-dataset-dut.
[204] Mark Philip Philipsen, Morten Bornø Jensen, Andreas Møgelmose, Thomas B Moeslund, and Mohan M Trivedi. Traffic
light detection: A learning algorithm and evaluations on challenging dataset. In intelligent transportation systems
(ITSC), 2015 IEEE 18th international conference on, pages 2341–2345. IEEE, 2015.

[205] Robert Krajewski, Tobias Moers, Julian Bock, Lennart Vater, and Lutz Eckstein. The round dataset: A drone dataset
In 2020 IEEE 23rd International Conference on Intelligent

of road user trajectories at roundabouts in germany.
Transportation Systems (ITSC), pages 1–6, 2020. doi: 10.1109/ITSC45102.2020.9294728.

[206] Antonia Breuer, Jan-Aike Termöhlen, Silviu Homoceanu, and Tim Fingscheidt. opendd: A large-scale roundabout
drone dataset. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1–6. IEEE,
2020.

[207] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large
collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3234–3243, 2016.

[208] Manikandasriram. Gta. https://fcav.engin.umich.edu/research/failing-to-learn.
[209] Baidu. Apollo synthetic. https://apollo.auto/synthetic.html.
[210] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving

simulator. In Conference on robot learning, pages 1–16. PMLR, 2017.

[211] MathWorks. Matlab simulink. https://ww2.mathworks.cn/products/simulink.html?s_tid=hp_products_simulink.
[212] Carsim. Carsim. https://www.carsim.com/products/carsim/index.php.
[213] Guodong Rong, Byung Hyun Shin, Hadi Tabatabaee, Qiang Lu, Steve Lemke, M¯artin, š Možeiko, Eric Boise, Geehoon
Uhm, Mark Gerow, Shalin Mehta, et al. Lgsvl simulator: A high fidelity simulator for autonomous driving. In 2020
IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1–6. IEEE, 2020.

[214] IPG. Carmaker. https://ipg-automotive.com/cn/products-solutions/software/carmaker/.
[215] MSC software. Adams. https://www.mscsoftware.com/product/adams.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

50

S. Tang et al.

[216] OSRF. Gazebo. http://gazebosim.org/.
[217] Siemens. Prescan. https://m.tass.plm.automation.siemens.com/cn/prescan-2.
[218] NVIDIA. Nvidia drive constellation. https://developer.nvidia.com/zh-cn/drive/drive-constellation.
[219] rFpro. rfpro. https://www.rfpro.com/.
[220] Cognata. Cognata. https://www.cognata.com/cn/.
[221] Vassili Alexiadis, James Colyar, John Halkias, Rob Hranac, and Gene McHale. The next generation simulation program.

Institute of Transportation Engineers. ITE Journal, 74(8):22, 2004.

[222] Vincenzo Punzo, Maria Teresa Borzacchiello, and Biagio Ciuffo. On the assessment of vehicle trajectory data accuracy
and application to the next generation simulation (ngsim) program data. Transportation Research Part C: Emerging
Technologies, 19(6):1243–1262, 2011.

[223] Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger, and Carsten Rother. Augmented
reality meets computer vision: Efficient data generation for urban driving scenes. International Journal of Computer
Vision, 126(9):961–972, 2018.

[224] Alexander Von Bernuth, Georg Volk, and Oliver Bringmann. Rendering physically correct raindrops on windshields
for robustness verification of camera-based object recognition. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages
922–927. IEEE, 2018.

[225] Deepak Talwar, Sachin Guruswamy, Naveen Ravipati, and Magdalini Eirinaki. Evaluating validity of synthetic data
in perception tasks for autonomous vehicles. In 2020 IEEE International Conference On Artificial Intelligence Testing
(AITest), pages 73–80. IEEE, 2020.

[226] Alex Zyner, Stewart Worrall, and Eduardo M Nebot. Acfr five roundabouts dataset: Naturalistic driving at unsignalized

intersections. IEEE Intelligent Transportation Systems Magazine, 11(4):8–18, 2019.

[227] Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kummerle, Hendrik
Konigshof, Christoph Stiller, Arnaud de La Fortelle, et al. Interaction dataset: An international, adversarial and
cooperative motion dataset in interactive driving scenarios with semantic maps. arXiv preprint arXiv:1910.03088, 2019.
[228] Johannes Bernhard, Mark Schutera, and Eric Sax. Optimizing test-set diversity: Trajectory clustering for scenario-
based testing of automated driving systems. In 2021 IEEE International Intelligent Transportation Systems Conference
(ITSC), pages 1371–1378. IEEE, 2021.

[229] Cumhur Erkan Tuncali, Georgios Fainekos, Danil Prokhorov, Hisahiro Ito, and James Kapinski. Requirements-driven
test generation for autonomous vehicles with machine learning components. IEEE Transactions on Intelligent Vehicles,
5(2):265–280, 2019.

[230] Şükrü Yaren Gelbal, Santhosh Tamilarasan, Mustafa Rıdvan Cantaş, Levent Güvenç, and Bilin Aksun-Güvenç. A
connected and autonomous vehicle hardware-in-the-loop simulator for developing automated driving algorithms. In
2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 3397–3402. IEEE, 2017.

[231] PTV. Vissim. https://www.ptvgroup.com/en/solutions/products/ptv-vissim/.
[232] Tong Duy Son, Ajinkya Bhave, and Herman Van der Auweraer. Simulation-based testing framework for autonomous
driving development. In 2019 IEEE International Conference on Mechatronics (ICM), volume 1, pages 576–583. IEEE,
2019.

[233] Aimsum. Aimsum. https://www.aimsun.com/aimsun-next/.
[234] Balázs Varga, Tamás Tettamanti, and Zsolt Szalay. System architecture for scenario-in-the-loop automotive testing.

Transport and Telecommunication, 22(2):141–151, 2021.

[235] Cyberbotics Ltd. Webots. http://www.cyberbotics.com/#cyberbotics.
[236] Stefano Carpin, Mike Lewis, Jijun Wang, Stephen Balakirsky, and Chris Scrapper. Usarsim: a robot simulator for
research and education. In Proceedings 2007 IEEE International Conference on Robotics and Automation, pages 1400–1405.
IEEE, 2007.

[237] Vires. Vtd. https://vires.mscsoftware.com/vtd-vires-virtual-test-drive/.
[238] ESI. Pro sivic. https://cn.esi-group.com/pro-sivicshou-ce.
[239] Baekgyu Kim, Takato Masuda, and Shinichi Shiraishi. Test specification and generation for connected and autonomous

vehicle in virtual environments. ACM Transactions on Cyber-Physical Systems, 4(1):1–26, 2019.

[240] Dyna. Dynacar. https://www.dynamore.de/en.
[241] BeamNG. Beamng.tech. https://beamng.tech/.
[242] Alessio Gambi, Tri Huynh, and Gordon Fraser. Generating effective test cases for self-driving cars from police reports.
In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, pages 257–267, 2019.

[243] Muhammad Adnan, Francisco C Pereira, Carlos Miguel Lima Azevedo, Kakali Basak, Milan Lovric, Sebastián Raveau,
Yi Zhu, Joseph Ferreira, Christopher Zegras, and Moshe Ben-Akiva. Simmobility: A multi-scale integrated agent-based
simulation platform. In 95th Annual Meeting of the Transportation Research Board Forthcoming in Transportation
Research Record, 2016.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

A Survey on Automated Driving System Testing: Landscapes and Trends

51

[244] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang. Physical adversarial attack on

vehicle detector in the carla simulator. arXiv preprint arXiv:2007.16118, 2020.

[245] Yanan Guo, Christopher DiPalma, Takami Sato, Yulong Cao, Qi Alfred Chen, and Yueqiang Cheng NIO. An adversarial

attack on dnn-based adaptive cruise control systems.

[246] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation

for autonomous vehicles. In Field and service robotics, pages 621–635. Springer, 2018.

[247] Voyage. Deepdrive. https://deepdrive.io/.
[248] Avsimulation. Scaner studio. https://www.avsimulation.com/scaner-studio/.
[249] Rodrigo Queiroz, Thorsten Berger, and Krzysztof Czarnecki. Geoscenario: An open dsl for autonomous driving

scenario representation. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 287–294. IEEE, 2019.

[250] Daniel J Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L Sangiovanni-Vincentelli, and Sanjit A
Seshia. Scenic: a language for scenario specification and scene generation. In Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation, pages 63–78, 2019.

[251] Florian Bock, Christoph Sippl, Aaron Heinz, Christoph Lauer, and Reinhard German. Advantageous usage of textual
domain-specific languages for scenario-driven development of automated driving functions. In 2019 IEEE International
Systems Conference (SysCon), pages 1–8. IEEE, 2019.

[252] Barbara Schütt, Thilo Braun, Stefan Otten, and Eric Sax. Sceml: A graphical modeling framework for scenario-based
In Proceedings of the 23rd ACM/IEEE International Conference on Model Driven

testing of autonomous vehicles.
Engineering Languages and Systems, pages 114–120, 2020.

[253] Matthias Althoff, Markus Koschi, and Stefanie Manzinger. Commonroad: Composable benchmarks for motion

planning on roads. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 719–726. IEEE, 2017.

[254] Ao Li, Shitao Chen, Liting Sun, Nanning Zheng, Masayoshi Tomizuka, and Wei Zhan. Scegene: Bio-inspired traffic
scenario generation for autonomous driving testing. IEEE Transactions on Intelligent Transportation Systems, 2021.
[255] Rupak Majumdar, Aman Mathur, Marcus Pirron, Laura Stegner, and Damien Zufferey. Paracosm: A test framework

for autonomous driving simulations. Fundamental Approaches to Software Engineering, 12649:172, 2021.

[256] Jing Ma, Xiaobo Che, Yanqiang Li, and Edmund M-K Lai. Traffic scenarios for automated vehicle testing: A review of

description languages and systems. Machines, 9(12):342, 2021.

ACM Forthcoming, Vol. 1, No. 1, Article . Publication date: June 2022.

