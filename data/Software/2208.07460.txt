2
2
0
2

g
u
A
5
1

]
E
S
.
s
c
[

1
v
0
6
4
7
0
.
8
0
2
2
:
v
i
X
r
a

A Research Software Engineering Workﬂow for Computational
Science and Engineering

Tomislav Mari´ca,∗, Dennis Gl¨aserb, Jan-Patrick Lehrc, Ioannis Papagiannidisa, Benjamin
Lambied, Christian Bischofc, Dieter Bothea

aMathematical modeling and analysis, Mathematics department, TU Darmstadt, Darmstadt, 64287, Germany
bInstitute for Modelling Hydraulic and Environmental Systems, Department of Hydromechanics and Modelling of
Hydrosystems, University of Stuttgart, Stuttgart, 70569, Germany
cInstitute for Scientiﬁc Computing, Department of Computer Science, TU
Darmstadt, Darmstadt, 64289, Germany
dInstitute for Technical Thermodynamics, TU Darmstadt, Darmstadt, 64287, Germany

Abstract

University research groups in Computational Science and Engineering (CSE) generally lack
dedicated funding and personnel for Research Software Engineering (RSE), which, combined with
the pressure to maximize the number of scientiﬁc publications, shifts the focus away from sustainable
research software development and reproducible results. The neglect of RSE in CSE at University
research groups negatively impacts the scientiﬁc output: research data - including research software
- related to a CSE publication cannot be found, reproduced, or re-used, diﬀerent ideas are not
combined easily into new ideas, and published methods must very often be re-implemented to be
investigated further. This slows down CSE research signiﬁcantly, resulting in considerable losses in
time and, consequentially, public funding.

We propose a RSE workﬂow for Computational Science and Engineering (CSE) that addresses
these challenges, that improves the quality of research output in CSE. Our workﬂow applies estab-
lished software engineering practices adapted for CSE: software testing, result visualization, and
periodical cross-linking of software with reports/publications and data, timed by milestones in the
scientiﬁc publication process. The workﬂow introduces minimal work overhead, crucial for univer-
sity research groups, and delivers modular and tested software linked to publications whose results
can easily be reproduced. We deﬁne research software quality from a perspective of a pragmatic
researcher: the ability to quickly ﬁnd the publication, data, and software related to a published
research idea, quickly reproduce results, understand or re-use a CSE method, and ﬁnally extend
the method with new research ideas.

1. Introduction

Research software engineering - crucial in Computational Science and Engineering - continues
to ﬁnd little application in academia, potentially due to beliefs that the costs outweigh the bene-
ﬁts [1]. However, eﬃcient and long-term CSE research is impossible without sustainable research
software development: without modularity and adequate test coverage, straightforward integration

∗Corresponding author

1

 
 
 
 
 
 
of diﬀerent research ideas, and the ability to ﬁnd and reproduce research results from publications,
many CSE research projects stall.

Ensuring the sustainability of research software is often challenging even for a single researcher
- often a Ph.D. student. Focusing only on an upcoming scientiﬁc publication means focusing on
the next set of computations, often wholly disregarding the impact of changes in the research soft-
ware on the existing code base. Adding new functionality while ensuring that complex existing
CSE methods continue to work as they should is more complicated when multiple researchers work
together on the same code base. Sustainable research software, therefore, must allow the integra-
tion of contributions from diﬀerent researchers and ensuring that previous contributions work as
expected. The software industry has already established principles, workﬂows, and tools to cope
with sustainable software development. These principles and workﬂows are widespread in larger
scientiﬁc projects; however, they ﬁnd little to no application in many smaller projects, particularly
at universities. In research, everything revolves around scientiﬁc publications. There is a need to
Find research software and data related to a publication and Access it. Once results are found and
accessed, there is a need to Interact with them easily - to draw new conclusions, perform a more
detailed analysis, apply the method outside of the original application-range. Finally, to extend or
improve methods, being able to Reuse and/or Reproduce results from speciﬁc milestones is neces-
sary. The requirements mentioned above are the basis of the FAIR principles [2]. For reproducing
results, the speciﬁc software conﬁguration and its environment (dependencies), complete input data,
and a result evaluation workﬂow are necessary. Moreover, structured management of primary data
(e.g. simulation results) and secondary data (e.g. diagrams and tables) is also necessary.

Access to the software and its conﬁguration used in a publication is crucial for result reproduc-
tion. Without access to the CSE software, and its primary and secondary data, the only way to
assess whether a numerical method is working is to re-implement it. Re-implementation of pub-
lished CSE results is extremely diﬃcult - CSE methods are generally complex, and not all details
are available in a scientiﬁc publication. The need to re-implement published work is slowing down
research in CSE and introducing equally signiﬁcant ﬁnancial losses in public research funding.

As for the challenge of developing sustainable research software, we argue that a careful inte-
gration of new features, involving automatic veriﬁcation and validation tests must be done, and
can be done in a relatively straightforward way in the university research environment. Without
integration, research software is often developed in diverging directions, and integrating features
quickly becomes intractable. Computational sciences pose an additional challenge: large-scale tests
that require high-performance computing are often necessary for veriﬁcation and validation of the
research software. This requires careful design of certain parts of the workﬂow.

In this paper, we propose a workﬂow that addresses these challenges mostly based on standard
software development practices applied to research software in CSE. The workﬂow is lightweight
and is focused on small research teams or individual researchers at universities, because univer-
sity researchers seldom have dedicated personnel and resources that support the development of
sustainable research software. In addition, we show at which points the workﬂow can be extended.
Increasing the quality of research software is gaining a lot of traction in the CSE community.
Initiatives such as Better Scientiﬁc Software (BSSw) [11] and its German and British equivalents,
the National Research Data Initiative (NFDI) [12] (with a community-driven knowledge-base [13])
and the Sustainable Software Initiative [14], support sustainable research software development
by organizing workshops and providing best practices to the CSE community. The ”Turing Way”
handbook [15] is a community-driven excellent source of information on increasing the eproducibility
of research results in data science.

2

Publication Branching model TDD Cross-linking CI

(Meta)data standardization

[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]

-
-
1
-
1
1
1
1

-
-
2
-
-
-
-
-

-
-
-
3
-
-
-
-

-
-
-
1
1
5
1
4

1
2
-
3
-
-
4
-

Table 1: A comparison of the proposed workﬂow with other workﬂows. Similarity is expressed subjectively using
numbers 1 (not similar) to 5 (very similar), or ’-’ for an aspect that is not addressed.

A discussion on best practices to increase the quality of scientiﬁc software can be found in
[5], which comprise quality aspects of the source code (e.g. consistent naming and formatting),
quality assurance measures (automated build and test pipelines), design principles (e.g. ”don’t-
repeat-yourself”) and technical aspects such as the use of version control systems.

In [7], the issue of reproducibility is addressed by combining a ”git” version control workﬂow
that includes research data, and Org-mode for the Emacs text editor research documentation. The
workﬂow is minimalistic, but it is tied to the Emacs editor.

A distinction between replicability, reproducibility and reusability of research software is de-
scribed in [4]. According to the authors, replicability of a computational experiment requires suf-
ﬁcient documentation on how to set up and run the program, while the automation and testing of
this procedure is only recommended. Reproducibility of the results requires additional information
on hardware and software dependencies, while reusability further demands modular software and a
licensing model. The workﬂow presented in this work places automation at its core, with the aim
of increasing the reproducibility.

Making software publicly accessible as early as possible, making it easily found, applying a
license compatible with its dependencies and ensuring transparent contribution and communica-
tion channels become relevant when research software attracts collaboration partners and starts
growing into an open-source project [3]. Moreover, we argue that early public access represents a
psychological motivational factor for university research groups - Ph.D. students that are mainly
responsible for new feature development, in our experience, place more focus on code quality if they
are working on a publicly accessible code.

Continuous Integration (CI) for research software on HPC systems was proposed in [10], using a
dedicated Jenkins test server in combination with a Singularity [16] registry to ensure reproducibility
of research results. This approach is very promising, especially regarding the use of Singularity
images that encapsulate the computing environment of the research software. Since then, web-
based version control services such as GitHub and GitLab oﬀer Continuous Integration - online
automatic testing systems, that are easy to set up and make the dedicated Jenkins test server
unnecessary.

Good enough practices in scientiﬁc computing according to [6] focus on data management, soft-
ware, collaboration and project organization. Regarding data management, they recommend the
archiving of raw data while making it easily understandable. Their software recommendations focus
on code quality in terms of comments, function naming, etc., while their collaboration recommen-

3

dations may nowadays be replaced by issue tracking available on git hosting sites. Good enough is
a crucial attribute for small university research groups: a workﬂow must be simple enough to be
quickly adopted by a small team of Ph.D. students and postdocs.

Continuous Benchmarking (CB) is a useful method to make sure that the performance of a
software does not deteriorate over time. A CB workﬂow that includes version control, a modern
build system, automatic testing framework, code reviews, benchmark tests on HPC systems, storage
of performance data and their automatic visualization is presented in [8]. They realized all the
aforementioned steps by relying on available web-based technologies, apart from result visualization:
the authors have developed their own web-based solution for this task.

For many research software projects, performance may be a secondary goal that comes after
ensuring that physically meaningful results are being computed. To this end, Continuous Veriﬁca-
tion and Validation (VV) may be done by comparing test results against previous milestones, other
research software, experiments or analytical solutions where available.

The importance of automatic visualization of results and their communication to team members
is emphasized in [17]. The authors use a custom workﬂow to manage continuous integration of
research software using git hooks that trigger automatic tests and email as means of communication
of test results.

Jupyter [18] represents a powerful CSE environment for literate programming - interleaving
documentation and executable code in Jupyter notebooks makes it possible to combine test doc-
umentation (problem description, method description) with data generation (simulations, experi-
mental data processing) and visualization. In [19], Jupyter notebooks are combined with Binder
[20] - a web service for building custom computing environments that contain the software and all
its dependencies. Focus is placed on using Jupyter Notebooks and literate programming to initial-
ize and perform computational studies and visualize results. This workﬂow is relevant regarding
reproducing results and interleaving computation and documentation.

A tabular summary of the comparison between our proposed workﬂow and those reported so

far in the literature is given in table 1.

Our minimalistic workﬂow applies all elements of the workﬂows mentioned above in a way that
makes it easily adoptable by small university research groups in CSE and applicable also to legacy
research software. We rely entirely on existing web services and open-source software for version
control, automatic testing, computing environments, data management, and visualization. Another
distinctive feature of our workﬂow is the focus on the scientiﬁc publication and the peer-review
process as a source of milestones that mark the points in research that require the reproduction of
results. In an ideal world, integration and testing are continuous, however, conditions for research
software development at universities are far from ideal. Tying integration and testing to writing a
scientiﬁc paper (submission, revisions, acceptance) signiﬁcantly reduces the workload overhead of
university researchers while still ensuring sustainable research software development.

The paper is structured as follows: ﬁrst, we outline an overview of the general workﬂow in
section 2. Sections 3 and 4 outline a basic and full version of our workﬂow presenting their diﬀerent
aspects in detail, that is, which challenge is addressed, which technology is applied, what are
potential impediments, and how to overcome them. We describe a minimal working example
in section 5, while two case studies are presented in section 6 that show the positive impact when
following the suggested workﬂow. Finally, we draw conclusions in section 7 and outline further
activities required.

4

Figure 1: Overview of the proposed workﬂow and the technologies used. Elements in the lower white area show
the minimum workﬂow steps. The elements along the gradient are recommended elements that comprise the full
workﬂow.

2. A workﬂow for increasing the quality of scientiﬁc software

An overview of our workﬂow’s components is shown in ﬁg. 1. The schematic highlights the
aspects that we consider as the minimum workﬂow, while pointing out useful additions. Projects
should establish the minimum aspects ﬁrst, and then increase towards the full workﬂow, as suggested
in [1]. This keeps initial investment manageable and introduces only minimal overhead to the general
organization of the project.

At the very least, we recommend to (1) use a version control system (VCS) for all source ﬁles
of the software, as well as text ﬁles that hold experiment conﬁgurations, (2) use an established
cross-platform build system for the software artifacts, and, (3) perform cross linking of the code,
the scientiﬁc publications and data using the mechanisms provided by the VCS and persistent
identiﬁers (PID), e.g., digital object identiﬁers (DOI).

Additionally, the project can include issue tracking as an improvement, followed by continuous
integration and test-driven development (TDD). As particularly beneﬁcial we found using con-
tainerization for both testing purposes and for reproducible computational experiments. Finally,
the introduction of automatic quantiﬁcation, visualization and evaluation on HPC systems proved
useful to early detect any degradation in performance or numerical quality. The latter beneﬁts
signiﬁcantly from a working continuous integration pipeline being available.

The workﬂow described in the remainder of this paper is built on top of available and widely
adopted open source software, and we refer to the respective software documentation wherever
appropriate. The basic workﬂow is described ﬁrst: section 3.1 covers version control systems,
section 3.2 the use of the build system, and section 3.3 covers cross linking of publication arti-
facts. Thereafter, additional components of the workﬂow are described: issue tracking (section 4.1),
continuous integration (section 4.4), containerization (section 4.3), test-driven development (sec-
tion 4.2), and automatic quantiﬁcation and evaluation using HPC systems (section 4.2.1).

5

Automatic quantiﬁcation, visualization, and evaluation on HPC ContainerizationIssue TrackingContinuous IntegrationTest-Driven DevelopmentIncreasing CommitmentProject BeneﬁtVersion ControlBuild SystemCross-LinkingAutomaticQuantiﬁcationand VisualizationContainerizationIssue TrackingContinuous IntegrationTest-Driven DevelopmentCrossLinkingSoftware QualityVersion ControlBuild SystemIncreasing BeneﬁtProject TraceabilityVersionControlBuildSystemFigure 2: Schematic of the relationship between the feature-based git branching model, the peer-review process, and
the cross-linking of digital research artifacts (cf. section 3.3).

3. Minimal workﬂow

3.1. Version control branching-model for research software

Without version-controlled software, research eﬀorts are doomed to be repeated, which hinders
progress and wastes funding resources. Additionally, the transfer of the reasoning behind software
design decisions is a challenge when the various versions diverge, and non-permanent research
personnel leaves the university. The situation becomes particularly critical whenever new Ph.D.
students continue the work of a former Ph.D. student.

The proposed workﬂow approaches the challenges posed by (re-)integration of diﬀerent research
ideas by adopting a branching model in a version control system that combines version control with
research milestones and the scientiﬁc publication process, to ﬁnally cross-link source code, research
reports and result data, thus ensuring all digital research artifacts related to a scientiﬁc publication
or a research idea are easily found. A commonly used version control system (VCS) is git [21], and
many available web interfaces, e.g., Gitlab [22], Github [23], or Bitbucket [24], simplify its use. For
successful integration of versions, it is crucial to follow a so-called branching model, e.g., Gitﬂow [25]
or Github ﬂow [26], to beneﬁt most from git’s capabilities.

A branching model describes how an individual or a team manage diﬀerent versions. With
a focus on small university research teams, we suggest to use the simplest possible feature-based
branching model while connecting it with the scientiﬁc publication process to simplify result re-
production, schematically shown in ﬁg. 2 for an example research project. A research idea may
require a number of diﬀerent new features, which are possibly developed in parallel and on dif-
ferent branches by diﬀerent members of the research institution. We do not show research-idea
sub-features in ﬁg. 2 for the sake of clarity. We suggest to create one new branch for each research
idea that will contain everything that is needed, from the perspective of the research software, for
a scientiﬁc publication. All newly developed features are merged into the research idea branch, and
the research idea is ﬁnished once the results it delivers are of high enough quality for a scientiﬁc
publication, or, if the idea fails. The software quality - in terms of the quality of research output
- is therefore connected to the quality assurance given by the peer-review process in our workﬂow.

6

mainfeature/research-idea-Afeature/research-idea-Bfailedpublication acceptedbugfixreview revisionsmain contains research ideasthat worked and working sub-algorithms from failedresearch-ideasfeature/research-idea-CTagged commits with cross linksBugfix integrationsDevelopment integrationsOptional mergesJournal SubmissionWhether an idea worked or not, it is beneﬁcial to write up the work carried out in a report, ex-
plaining future researchers what has been done, what worked and what did not. This way, the
generally completely neglected, but extremely valuable, negative scientiﬁc results are documented,
at least in internal, and much better, in publicly available reports. Another reason for an internal
report would be when a researcher leaves the institution and there is unﬁnished work that may be
continued by someone else.

Before writing up the report, any new developments in the main branch should be incorporated
into the research idea branch, and it should be veriﬁed that features that had been already present
in the research software still deliver the same results. If not, it must be investigated if the new
behaviour is an improvement or if errors were introduced. Ensuring that existing features deliver
results of similar quality when integrating new features can be done manually; however, in CSE
context this typically takes many person-hours, and thus, using automatic testing and continuous
integration (see section 4.4) is much more eﬃcient.

At this stage, the research idea branch is mature enough to produce the results for a report,
for instance, for submission to a scientiﬁc journal. We suggest to publish both the report and
the data discussed in it in order to receive persistent identiﬁers (PID) that can be used for cross-
linking (see section 3.3). The software itself, in the state of the research idea branch that was used
to produce the published results, should now also be published on a data repository such as e.g.
Zenodo [27], or TUDatalib [28] at TU Darmstadt. This yields another PID for the software. Next,
a new commit is created on the research idea branch, which contains only changes to the readme
ﬁle of the repository, adding to it the PIDs of the software, data and the report publications. This
makes sure that any published digital asset produced with the software is documented. Finally,
this commit is tagged, following a naming convention established in the group. For instance, tags
may contain the abbreviation of the research idea, the abbreviation of the journal to which the
preprint was submitted, and the state of the review process (e.g. submission, accepted, revision-1,
revision-2, etc.). More details on this and the cross-linking are discussed in section 3.3.

At this point, the research team may decide if they consider the results good enough for the
research idea branch to be merged with the development branch. If preexisting features have been
further developed on the research idea branch, we strongly suggest to merge it such that future
unrelated research ideas can beneﬁt from these improvements. In either case, this branching model
allows the next person to comprehend the developments on the research idea branch, and possibly,
start developing a new research idea based on the one described in the report.

The feature branch model is used further in the same way during the peer-review process of
a manuscript. Review comments may lead to new feature branches and bug ﬁxes, which are all
merged before producing new results. Once the new version of the manuscript becomes satisfactory,
the process described above is repeated for this revision: an archive of the research software is
uploaded as a new version of the existing data item on the data repository and cross-linked with
the publication as outlined in section 3.3, and the branch is tagged again according to the tag naming
convention, suﬃxed with the revision number and maybe information on relevant improvements.
Once the publication is accepted, the research idea branch is merged into the main branch, making
sure that it contains all successful ideas from published peer-reviewed work.

We want to emphasize again that we recommend this workﬂow also for research ideas that failed,
that is, publishing code, data and a report and link them to a speciﬁc tag in the software repository.
The team may decide to keep the code with the failed research idea as a separate branch or merge
it into the development branch in order to maintain these pieces of code, for instance, if it is likely
that this work will be picked up again in near future.

7

The major (tagged) milestones in this git branching model therefore revolve around scientiﬁc
publications - digital artifacts that matter most in university research. Of course, this does not
mean that more granular information is not available: the team can generate additional git tags for
developments they deem relevant. The core idea is to utilise git tags from major publication mile-
stones to increase transparency and help to ensure reproducibility of results for these publications.
Finally, here are some general recommendations for using a VCS that are and should be funda-
mental practice: Write telling messages when you commit changes, such that your project partners
understand what has changed and why it was introduced. Prefer small changes per commit instead
of minimizing the number of commits. Each commit should introduce a single coherent change,
which also makes it easier to enrich that change with a descriptive commit message. Avoid putting
multiple features into one merge request. If a merge request introduces one single feature, then it
is easy to give it a descriptive name. Moreover, if the feature is developed in small and well-named
commits, the code reviewing process is much facilitated, as both the intent of the merge request as
well as the steps required to make it work are obvious. Make sure all tests pass on each commit.
This is especially important for code that needs to be compiled, since commits on which the code
does not build impede the usage of tools like git bisect. In particular, make sure the tests always
pass on the development branch. In order to achieve this, we recommend to rebase branches before
merging them into development, making sure that all tests still pass. This makes sure that there
have not been any changes on the development branch that cause the new tests of the feature
branch to fail after the merge.

3.2. Build system

Adoption of open source software and the complexity of scientiﬁc codes often introduce a rel-
atively large number of dependencies. Using an established build system simpliﬁes handling of
dependencies on diﬀerent platforms. Rules can be deﬁned in the build system that enable the same
source code to be built on diﬀerent systems, where the dependencies used by the scientiﬁc code are
placed in diﬀerent paths. Moreover, build systems handle platform-dependent technical details and
hence, reduce the time required to maintain the build process. We strongly advocate the use of
build systems instead of writing and maintaining platform-speciﬁc build instructions as e.g. GNU
Makeﬁles.

3.3. Cross-linking publications, software, and datasets

This section covers one of the most important aspects of our proposed workﬂow: the cross-
linking of the publication, the software and the data sets, i.e., connecting all digital assets of a
particular publication. We motivate this step from multiple perspectives.

First, the authors of the publication and their respective research groups need easy ways to
retrieve all assets for a given publication, e.g., to continue their work. This may happen at a
signiﬁcantly later point than the publication date, for example, when a previous idea is investigated
further, or new funding is obtained. Since research team members at universities are usually
Ph.D. students and postdocs that leave the group every 1-5 years, cross-linking digital assets of a
publication is crucial for sustainability.

Second, researchers who want to compare their own work to published results, need an easy way
to determine all parameters used to obtain the results. Scientiﬁc publications in CSE generally do
not contain all metadata required to reproduce the results. Often, only the parameters relevant
to the parameter variation are published, for instance, diﬀerent ﬂuid densities and viscosities, or
parameters of numerical schemes investigated in the publication. All other parameters used by the

8

Figure 3: Cross-linked publication, data and source code using Persistent Identiﬁers.

research software that are not part of the parameter variation are not mentioned. However, these
unmentioned parameters may be completely diﬀerent in another CSE research software. Concretely,
a publication may contain the parameters and information about approximating curvature for
multiphase ﬂow simulations, and no information about the solution of the multiphase pressure
equation, since it is not related to the paper’s topic. Reproducing the eﬀect of the curvature model
in another software on the numerical stability of the simulation, however, does require the metadata
about the pressure equation and its discretization. Therefore, archiving and cross-linking input data
(input metadata) with a publication signiﬁcantly increases reproducibility.

Third, funding agencies are starting to require research to be Findable, Accessible, Interoperable,
and Reproducible [2] for a certain number of years. Hence, being able to retrieve snapshots and
container images of software versions, experiment / simulation conﬁgurations, the input and result
data sets, follows the FAIR principles closely [2]. This approach is especially applicable in CSE,
where, compared to experiments, every step in the data processing pipeline is digital.

Eﬀect of cross-linking on the publications. If a publication serves as documentation of a method
implemented in scientiﬁc software, the two should be cross-linked. Extending the cross-linking to
also include the data reported in the publication in the form of diagrams and tables (secondary
data) is the logical next step - especially considering their typically small size. Without archiving
and cross-linking secondary data, other researchers still nowadays resort to scanning diagrams from
publications, which is especially prone to error when logarithmic scales are used. From the point
of view of the research group: once the researcher responsible for the results, and the part of the
source code used to generate them, leaves the group, it quickly becomes impossible to associate
the source code and the datasets used in the publication. Even if all digital assets are archived
locally at the research institution the problem still persists: over the years, many publications, code
versions, and datasets are generated, so the problem of ﬁndability (F in FAIR [2]) grows with time,
if it is not technically handled. From the point of view of an external researcher: the cross-linking
makes it possible to retrieve all (conﬁguration and result) data needed for a direct comparison of
methods for the same simulation problem (R in FAIR [2]).

Eﬀect of cross-linking on the data. The lack of cross-linking severely complicates data re-use. To
draw new conclusions or derive new results (models) from existing data, detailed information about

9

DatasetSoftware ImageRemote gitrepositoryPIDsarticle DOIRepositorySnapshotPIDsgit repo URLgit tagArticleDOILiterature surveyResearcherDataRepositoryFigure 4: Schematic of a (software) project timeline and the proposed workﬂow steps. Software feature and experi-
mentation is developed iteratively until the point of satisfying results. Then the publication about the method and
its results is prepared and ﬁnalized. Thereafter, the next feature is approached.

the method that generated the data is necessary: information usually available in a scientiﬁc publi-
cation and simulation input metadata. A new application may make it necessary to apply a method
beyond the parameter set used to generate the existing data: in this case, knowing acceptable pa-
rameter ranges and ﬁnding the speciﬁc version of the research software is again necessary.

Eﬀect of cross-linking on the software. From the perspective of sustainable scientiﬁc software de-
velopment and increasing scientiﬁc software quality, the issues are the same for the software, like
those mentioned above for the publication and the datasets. For example, having only a speciﬁc
version of the scientiﬁc software available, checking the results becomes problematic if the result
datasets need to be re-computed. This is especially diﬃcult if this involves extensive simulations
that require considerable computational resources and know-how in preparing parameter variations,
performing simulations, and post-processing results. Scientiﬁc methods are improved sequentially:
every research step improves on existing methodology. These improvements often change only some
sub-algorithms of an existing method. Thus, introduced diﬀerences are diﬃcult to discern in the
scientiﬁc code, since even relatively small research software contains many relatively complex sub-
algorithms and, moreover, grows in time. Knowing which method is implemented by the speciﬁc
version of the software becomes relevant as soon as we aim to compare two methods.

The minimum workﬂow cross-linking connects all digital assets related to a scientiﬁc publication,
excluding full simulation results and software containers, and it is very straightforward to apply.
This is important, since an overly complex workﬂow cannot be easily adopted with limited resources
in university research groups. A straightforward cross-linking of publications, scientiﬁc software,
and result data can be achieved using Persistent Identiﬁers (PIDs), as shown in ﬁg. 3. As outlined
in section 3.1, the cross-linking using PIDs starts at the point in the development when a report is
written, that is, a preprint to be submitted to a scientiﬁc journal, a technical report or an internal
report about a failed research idea. In ﬁg. 4 we visualize a timeline of a project and at which points
the diﬀerent parts of the workﬂow are placed. For the remainder of the paper, we use the term
submission to refer to publishing a preprint, submitting to a peer-review process, or preparing an
internal technical report.

At the time of submission, all feature and bugﬁx branches required for the research idea have
been merged into the research idea branch (see section 3.1). This branch now contains the state of
the code with which the results that appear in the report are produced, and it is identiﬁed uniquely

10

CrossLinkSoftwarePublicationDevelop Feature AVCS,Build-SystemVCSDevelop Feature BExperimentationFinalizePublicationVCSProjectTimeVCS,Build-SystemVCSCrossLinkSoftwarePublicationDevelop Feature AVCS,Build-SystemVCSExperimentationPublicationVCSProjectTimeCrossLinkVCS,Build-SystemVCSVCSRepeat ProcessforEvery Featureby its current commit id. At this point, the data produced by running the code in this state are
available and are used to write up the report. After ﬁnishing the report, a snapshot of the code
and the data are published into two diﬀerent datasets of a data repository as e.g. TUdatalib1 [28]
or a similar service. The data repository creates a persistent identiﬁer (PID) (e.g. a DOI) for every
uploaded digital asset, which can now be mentioned in the report such that readers can ﬁnd the
associated data and software. Now, the report itself can be published to a manuscript repository
(e.g. ArXiv [29]), which further produces a PID for the publication. If the report corresponds to
a research idea that has failed, data repositories at universities usually make it possible to publish
the report internally and still obtain a PID for internal use.

At this point there are PIDs for the code, data and the report, but so far only the report
contains cross-links to the other assets. The next step is to mention the report in the descriptions
of the datasets for code and data, and place the PID of the report in the metadata. Afterwards, a
commit and git tag are created in the code repository (see section 3.1), which add the three PIDs
that were obtained up to now to its documentation. Afterwards, this git tag can also be mentioned
in the metadata of all three publications. This makes it possible to retrieve a snapshot of the
source code, but also to locate the version control repository and check out the tag, and continue
researching. Note that git tags are not PIDs and can be changed by the developers, so tags do
not meet the requirements typically posed on long-term availability. This three-way cross-linking
minimum workﬂow is depicted in the ﬁrst part of ﬁg. 4 and enables the convenient retrieval of all
digital assets. The resulting cross-linked assets are shown in ﬁg. 3.

One of the diﬀerences between the minimum and the full workﬂow cross-linking is the type of
data that is published. To clarify: We separate the scientiﬁc data into primary data and secondary
data. Primary data are large: in the context of CSE they are full simulation results. Secondary data
are small datasets that are visualized in the publication as diagrams or tables and are instrumental
in interpreting the results of the method implemented in the scientiﬁc software and described by
the publication. In the minimum workﬂow, only the secondary data are considered.

To facilitate the processing of the published secondary data, scripts that produce the ﬁgures and
tables shown in the publication may be added to the data publication. In this context, Jupyter note-
books [18] have emerged as a convenient way of processing data and documenting data workﬂows.
In our workﬂow, the data generated through simulation is processed in Jupyter notebooks [18] (see
section 4.2.1).

The small secondary data (e.g. CSV ﬁles in ﬁg. 6) are archived together with the Jupyter
notebooks and their HTML exports, the tabular information of the parameter study that connects
case IDs with parameters, and the simulation input ﬁles. In other words, the secondary data archive
contains the complete directory structure from ﬁg. 6, including all parameter studies reported in
a publication, but without the large simulation results. The tabular information of the parameter
study should be saved in a text format (YAML, JSON, CSV, or similar) that can be easily read by
a computer program (I in FAIR [2]), but also easily read by a researcher trying to ﬁnd the ID of
a simulation from a parameter study, that was simulated using a speciﬁc set of parameters. This
archive makes it possible for a researcher to quickly ﬁnd a CSV ﬁle (F in FAIR [2]) that stores the
data used to visualize a speciﬁc diagram in a publication. The secondary data archive generally
has a size below 1GB and can easily be uploaded to and downloaded from the data repository.

It is important to note that although relatively small in size compared to full simulation results,

1TUdatalib is the data repository service provided at TU Darmstadt

11

the datasets that belong to secondary data contain all the information relevant for judging the qual-
ity of the scientiﬁc results, comparing diﬀerent methods, or performing regression tests (ensuring
all existing tests still pass when adding new features).

At this point, the publication may enter the peer-review process that results in reviewer com-
ments. The application of reviewers’ comments and further research of the method lead to the
repetition of the above described process. If the new feature is researched and works well, it is in-
tegrated into the research idea branch, a new git tag is generated (see section 3.1), the source code
archive and the secondary data are updated as new versions on the data repository, the pre-print is
modiﬁed accordingly with new cross-links, a new version of the preprint is submitted to e.g. ArXiv,
and the metadata on the data repository is updated.

Although listing the cross-linking steps here may make the workﬂow seem complicated, applying
the steps actually takes at most a couple of work hours, maximally a workday for an individual
researcher, and even less in a team eﬀort. Compared to the number of person-months (sometimes
person-years) generally necessary to investigate a research idea, half of a workday used to follow the
FAIR principles and increase signiﬁcantly the quality of research results is an excellent investment.

4. Full workﬂow

4.1. Issue tracking

There are two important aspects of research software development at universities that can be

improved signiﬁcantly by issue tracking.

First, the research software developed at university research groups is mainly developed by
personnel that has non-permanent positions and leave after 1 − 5 years, e.g., Ph.D. students or
Postdocs. Often, virtually no overlap period is available: the person responsible for a research
direction often leaves before the new person arrives. In addition, (undergraduate) students work
on projects on a short-term basis. This high turnover rate at university research groups, combined
with contributions from diﬀerent research directions and experience levels, demands documentation
of the project status. Without project status documentation, repeating the predecessor’s steps,
discovering unaddressed bugs again, and the inability to understand the reasoning behind decisions
already made in the project when inheriting an existing project all lead to an enormous waste of
time and resources.

Second, once the software is published, the scientiﬁc community is likely to ﬁnd bugs or alter-
native applications for the software. Simplifying bug reports or feature requests from the outside
world is crucial for increasing research software quality. Hence, keeping track of the status of the
research software is utterly important, to allow the transfer of knowledge about open issues and
potential limitations.

An issue tracking system helps in communicating such aspects clearly to new project members
and people generally interested in the software project. Issue tracking systems are typically freely
available on web-based git services [22, 23, 24], and allow to track the status of the project. Issues
such as bugs, ideas, research cooperations, and peer-review comments are modeled as cards. The
issue cards can be extended with links to the source code and input data and attachments (e.g.,
images from simulations), and they support the chat functionality - placing the discussion about
an issue in the issue itself such that it is easily ﬁndable and understandable.

Defects are reported as bug reports, and ideas are maintained as feature requests. Moreover,

ideas can be broken down into smaller sub-tasks, simplifying the onboarding process.

12

Figure 5: Test-Driven Development (TDD) applied to research software in resource-limited university research setting.
TDD is applied in a top-down approach, starting with the CSE application that should generate research results,
then applying TDD to its sub-algorithms recursively, on a per-need basis.

A straightforward way to present and update issues is the well-known Kanban approach [30].
It consists of a board where the issues are moved between three columns: To Do, In Progress and
Done. The issues can be labelled, e.g., bug, improvement, or others, and they can be grouped into
project milestones. If actively maintained, the straightforward Kanban issue tracking simpliﬁes the
transition of research personnel, as the status of the project, including its current milestones, can
easily be understood on the visual Kanban board. This lightweight issue tracking is already suﬃ-
cient for research software developed within individual Ph.D. / PostDoc projects. More advanced
approaches for collaborative projects that involve many projects and researchers may require a
dedicated issue tracking software and more complex issue tracking workﬂows.

4.2. Test-Driven Development for Research Software

Test-Driven Development (TDD, [31]) is a practice of writing test applications (programs) ﬁrst,
before implementing the algorithms that are supposed to deliver the test results. This forces
programmers to think about the code from the perspective of the user, which typically yields a
cleaner API (Application Programming Interface) - the kind of API, the programmers would like
to use themselves.

TDD consists of three phases: red, green and refactor. The ﬁrst, red, phase begins with the
implementation of the test applications, and, as no algorithmic implementation is available to per-
form the actual computation, the tests fail. The tests deﬁne the return types and arguments of
algorithms used in the tests, thus deﬁning the API of the algorithm library. The algorithm im-
plementation follows, and it is modiﬁed until the tests pass, to enter the green phase of TDD. In
the ﬁnal, refactor phase, the passing tests now allow the developers to refactor the implementation
while using the tests to verify that the algorithmic implementation still works as intended. Refac-
toring is done until a modular (re-usable and extensible) software design is achieved. Then, the
next algorithm is approached, by starting over with writing the tests ﬁrst.

We propose an adaptation of the TDD approach to research software in CSE. Our top-down
TDD for CSE research software is outlined in ﬁg. 5. Numerical methods in CSE consist of dif-
ferent complex sub-algorithms. Some methods are based on rigorous mathematical theory and

13

TDD for the high-levelCSE application Generateprimary dataProcess primary datainto secondary dataVisualize the comparisonof secondary data withexpected resultsTests pass and/or visualcomparison satisfactoryRefactor the applicationor its sub-algorithmsQuantify, automateand run tests onsecondary dataDesignsatisfactorystopnoMain testapplicationSub-algorithmsyesyesnoSelect potential failing sub-algorithm Sub-algorithmrequires TDDSub-algorithmtest applicationSub-sub-algorithmsGenerateprimary data... re-used re-used re-usednewnew re-usedTDD for the sub-algorithmCSE applicationhave provable properties, e.g., convergence, or error estimates, others are based on approximations
whose errors cannot be theoretically estimated. In both cases, extensive and automatic veriﬁcation
and validation are crucial, since proven properties are not equivalent to a correct implementation.
Moreover, if the implementation does not produce the expected results, it is necessary to determine
which part of the method’s implementation causes the problem, which is generally done by testing.
If TDD is applied for the development of research software with the goal of producing the (test)
results for a scientiﬁc publication, the focus is shifted from attempting to implement low-level unit-
tests of all algorithms, to preparing only those veriﬁcation and validation tests, that are relevant for
the subsequent scientiﬁc publication. This way, tests that are written ﬁrst, without providing the
algorithmic implementation, are very high-level CSE applications, for example: partial diﬀerential
equation (PDE) solvers. Existing publications, that are supposed to be outperformed, deﬁne the
test results. Alternatively, the results are deﬁned by experimental data, or by the method of man-
ufactured solutions. Without a single line of algorithmic implementation, it is therefore possible
to deﬁne the type of primary data generated by the CSE high-level test application (e.g., velocity,
pressure, and temperature ﬁelds), and what is expected of them (macroscopic quantities, e.g. drag
resistance), as shown in ﬁg. 5. Furthermore, it is possible to visualize the not-yet-available (empty)
results against expected results - on purpose generating an artiﬁcially failing comparison.

Once the research tests are prepared, the algorithmic implementation follows for the high-level
application, by re-using algorithms from legacy research software, or, if necessary, implementing al-
gorithms from scratch. For algorithms implemented from scratch, the RSE TDD approach branches
oﬀ and cycles through its phases until the algorithms implemented from scratch are refactored, in
a recursive top-down approach, since they also consist of sub-algorithms. Re-used sub-algorithms
from legacy code on every level are assumed to be working correctly, unless the TDD testing shows
otherwise, in which case suspicious sub-algorithms are selected for further TDD, as shown in ﬁg. 5.
When the high-level application is not generating correct results after completed algorithmic im-
plementation, TDD is applied to its sub-algorithm that is selected to be the most likely cause of
the problem during result visualization/analysis. In CSE, algorithms are generally very complex
because they model complex physical phenomena. For example, a PDE discretization that is unit-
tested for a set of Initial and Boundary Value Problems (IBVP), does not necessarily work for a IBV
problem outside of the tested set. Automatic testing and visualization of results in the red phase
help the programmers more easily isolate troublesome sub-algorithms that require more TDD. This
way, ﬁnalizing the green phase for the high-level application does not require unit-testing to be
applied on every sub-algorithm of a CSE application, which is not possible to do in a university
research setting because of the lack of dedicated resources. Only those sub-algorithms that are
not correct in the context of expected research results are further developed with TDD. As work
continues on diﬀerent scientiﬁc publications, the automatic test suite grows with new tests being
added for new publications and the overall test coverage increases along the research roadmap, with
working and useful sub-algorithms from both failing and succeeding research ideas integrated into
the main software version following the RSE version-control branching model section 3.1.

4.2.1. Test qantiﬁcation and visualization

Testing scientiﬁc software involves running parameter variations, so-called studies, as shown in
ﬁg. 6. They often include hundreds of simulations, so called cases, cf. ﬁg. 6. The large number
of test cases makes it important to be able to quickly identify failing tests and re-run them after
changing the algorithm implementation to ﬁx the problem. Hence, the respective data, input and
output, needs to be well organized.

14

Figure 6: Diﬀerent parameter studies are orchestrated by a Study Runner. All studies are implemented using Jupyter
notebooks, which contain a problem description and the visualization of results. This allows for live inspection of
test result data. Structured metadata is exported into ﬁles to be available for subsequent use in publications and
data cross-linking.

The main input of such parameter studies are simulation parameters that are varied across the
components of a parameter vector. This is, typically, performed automatically by a study runner
on an available HPC machine, as the production tests consume a signiﬁcant amount of resources to
execute. In addition to running the respective case, the study runner also builds a mapping, using
unique IDs, from each simulation case to the particular input parameter vector used. Moreover, it
exports structured metadata stored in both human and machine-readable standardized and open
ﬁle format, that can be used subsequently as part of the cross-linking process, cf. section 3.3. ﬁg. 6
shows a possible organization of the data in folders with a naming scheme of simple ascending
integers used as the unique IDs.

Being able to examine test results of the parameter studies run as early as possible is crucial to
catch problematic executions timely, i.e., stop their execution, thus, limit the wasting of compute
resources, reduce the negative impact on the priority on the HPC machine, and speeding up the
research process. Being able to perform real-time analysis of test results is, thus, fundamental.
Manual inspection of tables and diagrams is a natural part of the research process: it is often not
possible or necessary, and sometimes not tractable, to automatically quantify the comparison of
secondary data, resulting in an automatic pass/fail status of the test. A straightforward solution to
the challenge of (almost) real-time manual test result analysis is the use of Jupyter notebooks [18].
A Jupyter notebook is written for each individual study, cf. ﬁg. 6. The project notebook, i.e., the
one executing the study runner, is created to link all the other notebooks. This eﬀectively links
together all the tests used to verify / validate the scientiﬁc software in a single place.

Each Jupyter notebook is used to document its parameter study. The test documentation covers
the mathematical description of the CSE problem that is solved by the test. Such descriptions
include which equations are solved, the initial and the boundary conditions, as well as physical
parameters that are investigated. The Jupyter notebooks then process secondary data generated
by the simulation (or a post-processing utility that runs alongside the simulation), and store them
as ASCII CSV ﬁles in ﬁg. 6. The central project notebook can be viewed live locally in a browser,
after the Jupyter server is started remotely on the HPC cluster. This straightforward use of Jupyter
notebooks makes it possible to check simulation results of many studies / cases that may run for a
long time on a cluster, and stop those early that are not generating satisfactory results. Additionally,
since Jupyter notebooks are used for sequential processing of secondary data they use as input, they
are always executed anew, and do not require information on the sequence of execution of their
cells (provenance).

15

An important beneﬁt of this visualization process is that diagrams and related secondary data
processed by the notebooks are used directly in the scientiﬁc publication. They are stored by the
notebooks in dedicated folders, that can be easily synchronized with the publication folder, as soon
as the results are satisfactory. Moreover, the solution is well suited for cross-linking of digital
research artifacts (cf. section 3.3), containerization (cf. section 4.3), and continous integration of
research software (section 4.4).

A Ph.D. or a postdoctoral researcher focusing on a CSE methodology will not generally have
the resources needed to alter the ﬁle format of primary data from a legacy CSE research software,
which is not the case for secondary data.

On the other hand, secondary data is precious: It is the basis for acceptance or rejection of
scientiﬁc publications, decisions on whether milestones have been reached, and conclusions about
one method outperforming all others. There are two challenges in handling secondary data that our
workﬂow addresses: handling metadata and interoperability. We propose a very straightforward
solution. Secondary data is minuscule compared to primary data. Storing secondary data using
an ASCII CSV format while saving metadata alongside result data directly in the columns solves
both the issue of handling metadata and the interoperability of secondary data. However, storing
metadata next to data in columns introduces information repetition. For example, one can think of
secondary data as a vector (row) uniquely identiﬁed in a parameter variation by another vector - a
vector of parameters. In parameter variations, other parameters are held constant as one parameter
varies. Established data formats (e.g., HDF5 [32]) rely on a tree structure to model parameter
variations without repetition of values. Contrary to this approach, for secondary data, we repeat
non-varying parameter values alongside varying parameter values and store both as secondary-
data columns. This way, all secondary data, and metadata are stored in a single table in ASCII
CSV. This approach makes our secondary data completely interoperable, as any software that can
process tabular data can process our secondary data. The information about which column stores
metadata information and which secondary data becomes apparent if the column names follow the
nomenclature from the scientiﬁc publication. Furthermore, this approach trivializes the inclusion of
metadata into data analysis: using metadata for sub-set selection and other statistical or sensitivity
analyses. Not relying on complex ﬁle formats simpliﬁes the processing of secondary data immensely
and saves time, with a negligible overhead caused by (meta)data duplication. A concrete example
of storing metadata together with secondary data in tabular form is shown for hyperparameter
tuning of an artiﬁcial neural network (NN) in table 2. Fixing one parameter and varying others
causes data duplication, which can be ignored for secondary data because of its small size, and
potentially used in further data analysis to better understand, in this case, the training behavior
of the NN model.

HIDDEN LAYERS OPTIMIZER STEP MAX ITERATIONS DELTA X

EPOCH

TRAINING MSE

10,10,10,10
10,10,10,10
10,10,10,10
10,10,10,10
...
10,10,10,10
10,10,10,10
10,10,10,10
10,10,10,10
...

0.0001
0.0001
0.0001
0.0001
...
0.001
0.001
0.001
0.001
...

3000
3000
3000
3000
...
3000
3000
3000
3000
...

0.0625
0.0625
0.0625
0.0625
...
0.0625
0.0625
0.0625
0.0625
...

1
2
3
4
...
1
2
3
4
...

1.091560
1.082970
1.077200
1.072650
...
0.992354
0.991959
0.995102
0.996143
...

Table 2: Storing metadata (hyperparameters) together with secondary data (epoch, MSE) in tabular form.

16

4.3. Containerization

The reproducibility of results is a fundamental principle of research. In addition, being able to
use software in a productive environment is highly relevant. However, the multitude of platforms
provides a challenge, given the usually limited number of personnel in academic research groups, es-
pecially when required on top of the demanded FAIR principles [2]. An approach for cross-platform
interoperability is the encapsulation of the software environment in so-called containers. Contain-
ers, e.g., Singularity containers [16, 33], make it possible to execute software in any environment,
given that it provides a suitable container runtime. Hence, results can be reproduced more easily
if the researcher provides containers that include the software and conﬁguration ﬁles used for a
speciﬁc publication. Large input data sets should not be included in the container but should be
published separately in a data repository. This keeps the container size as small as possible, for
instance, for users that want to use the software on their own data set. For the reproduction of
results, the input data can be downloaded on-the-ﬂy from the data repository.

How the container is built is speciﬁed in recipe ﬁles. They list all required dependencies, the
software, how it is built and which data to copy into the container. However, note that copying data
into the container makes the recipe itself not comply with the FAIR principles [2] as one cannot
reproduce the building process without that data. Therefore, it is beneﬁcial to only use data from
online resources with a guarantee of availability, for instance, from dedicated data repositories. The
same holds also for code: one should prefer to pull used sources from release pages or ”git clone”
into a repository with a speciﬁc commit or tag in the build recipe instead of copying source code
into the container. In general, git repositories have no guarantee to exist over long time periods, as
they could in principle be moved or removed. Therefore, if available, one can clone into the mirrors
hosted at softwareheritage.org/ which are guaranteed to be conserved over a long time frame.

Finally, the recipe could deﬁne a set of commands that can be invoked on the container, e.g., to

reproduce a speciﬁc result.

4.4. Continuous integration of research software

Continuous Integration (CI) [34] is the practice of frequent integration of changes in a shared
version of the software.
In this section, a straightforward CI workﬂow is presented for research
software, that increases its quality without introducing a signiﬁcant workload overhead for the
researchers.

To understand CI, and its beneﬁts, it is beneﬁcial to brieﬂy reiterate the connected branching
model, cf. section 3.1. A new feature is developed using a so-called feature branch (research idea),
while the current stable version of the software is maintained on the main branch.
In a more
traditional sense, CI recommends frequent, e.g., daily, integration from features onto the main
branch. Daily integration of changes is not tractable for scientiﬁc software, because the features
are driven by scientiﬁc research:
it is often not immediately clear if the developed methodology
will work. Therefore, this integration is performed when the generated results are suitable for
the submission of a scientiﬁc publication to a peer-review process. On the other hand, bug ﬁxes
and potential improvements to the main branch can be integrated into the feature branch. To
anticipate diverging individual feature branches, this integration should be done frequently, e.g.,
weekly. This ensures that improvements to common, shared components are integrated into the
individual feature branches. Moreover, integrating from main to feature branch on a weekly basis,
makes sure that the developments achieved in a successful project are integrated by everyone in
the research group, while the responsible developer is still working in the research group. Hence,
no working software artifact is lost.

17

Figure 7: Continuous integration workﬂow: the merge request triggers the CI pipeline to execute. This builds the
software, runs all tests and produces the visualizations. These are published for inspection. If tests pass, the merge
request is accepted and the changes are added to the main repository. If the tests fail, the developer inspects the
results and re-initiates the process.

Even though the integration itself may not always be continuous, there are aspects of CI that
help to increase code quality. ﬁg. 7 shows a CI workﬂow example. Feature development happens
in the user’s repository. When the feature is implemented together with its tests, the user opens a
merge request (MR), i.e., a request to integrate their changes into the main branch. This triggers
the CI pipeline, where the build system, CMake in the example, is used to check the compilation
of the software on diﬀerent platforms. Once the software passes this stage of so-called build tests,
the actual tests are executed on the GitLab runner. Besides small-scale unit tests, this involves the
execution of parameter studies and populates the respective folder structure with secondary data,
as shown in ﬁg. 6.

The automatic execution of tests as part of the CI pipeline is an important aspect to maintained
software quality, and simpliﬁes re-integration. A merge request triggers the execution of the CI
pipeline, and the MR should only be accepted if all tests have passed. This ensures that the
integration of all changes, i.e., feature branch and master branch, do not lead to errors in parts of
the software.

We explain an important additional detail of our CI pipeline, geared towards the needs in

18

BuildRun TestsTests passed?PythonYesNoGitLab CI PipelineRuns on GitLab RunnerCMakeTabular data (Pandas CSV files)Jupyter notebooks (.ipynb, HTML, PDF)Diagrams (PDF, PNG)Tables (LaTex) Secondary dataas GitLabartifacts readyfor downloadProcess andvisualizeresultsResearcherResearcher's repositoryTeam's repositoryPerform Research(Jupyter notebooks,project source code)PushchangesMergerequestCreatepublication /insert diagramsand tablesPublish  data, andsource code on adata repository(DOIs)Cross-linkresearchartifactsSubmit the publicationto peer reviewAutomatic StepManual stepResearch TeamAccept mergerequestDownloadDownloadNot necessaryfor interpretedlanguagesCSE. Before the quantiﬁcation of tests causes tests to either fail or pass, results are processed that
show why the tests failed / succeeded. Often, the textual output of the test environment is not
suﬃcient to diagnose test failure. Especially in the context of many CSE disciplines, test results
require visualization in the form of diagrams or 2D/3D visualization to determine the cause of an
error. For this purpose, the proposed CI pipeline from ﬁg. 7 contains jobs that export the Jupyter
notebooks as HTML. The result is the complete visualization of test results, uniquely identiﬁed
with a commit.

Provided all tests pass, the acceptance of the MR in ﬁg. 7 is decided by the responsible team
member, who can also provide feedback on code quality. A web interface to git, such as GitLab,
simpliﬁes the code-review process within the research group.

4.5. Cross-linking publications, software, datasets, and software images

While the minimum workﬂow cross-linking ensures the source code, the publication, and the
secondary data are cross-linked, it can be extended further with primary data and containers (see
section 4.3). Publishing and cross-linking primary data enables readers of the publication to gain
further insights into the results and draw new conclusions. Adding software containers to the cross-
linking workﬂow enables results of the scientiﬁc software to be reproduced more easily and across
diﬀerent computing platforms. Here the additional steps are covered, cf. ﬁg. 1, that are used to
achieve these improvements.

The primary data is archived in a slightly diﬀerent way than the secondary data because of its
size. Each parameter study is stored separately to enable the researcher to only download a speciﬁc
parameter study, which saves time and network bandwidth. The directory structure, shown in ﬁg. 6
is used similar to archiving secondary data, the only diﬀerence being that each parameter study is
stored in a separate archive. This makes it possible for the researchers to access subsets of results,
for example, ”STUDY A” or ”STUDY B” in ﬁg. 6, with respective Jupyter notebooks, their HTML
exports, and the parameter study tabular information. Datasets that are generated for subsets of
the primary data are uploaded to a data repository and their PIDs are used for cross-linking as
described for the minimum workﬂow above.

Containers in this respect can be seen as single image ﬁles that contain everything necessary to
execute user-speciﬁed commands. These commands are deﬁned when the container is created, and,
since containers are immutable, will produce the same result when invoked. In our workﬂow we use
Singularity containers [33], as the technology is supported by our HPC center, and the images are
single ﬁles that can easily be uploaded to a data repository. It is important to publish the recipe
for building the image alongside with the actual image, as this is an important documentation of
the requirements and dependencies of the published research software. A problem with the code
publication of the minimum workﬂow is that it only contains the research software itself. However,
that may depend on several other software packages, speciﬁc compiler versions, etc. This can
make it diﬃcult for interested peers to get the software running on their system, even if these
dependencies are explicitly stated. Moreover, the software may only run with speciﬁc versions
of these dependencies or possibly produces diﬀerent results with other versions. The recipe for
building the image explicitly documents how the environment of the researcher who produced the
results published in the report can be instantiated and the image enables users to directly spin up
a container with this environment without having to install anything. In order for this recipe to
yield the same results over time, it is important that it uses a version-pinned base image and only
installs pinned versions of the dependencies.

19

5. Minimal working example

We have prepared a Minimal Working Example (MWE) - a template CSE research project - that
can serve as basis for adopting the proposed workﬂow. The workﬂow MWE contains a numerical
C++ application implemented as a version-controlled open-source project [35], that generates a
small data set, visualized by a Jupyter notebook, referenced in a minimal example of a scientiﬁc
report (publication) [36].

The MWE research software is a simple C++ application that evaluates numerical derivatives
of a polynomial using ﬁnite diﬀerences in a 1D interval. The ﬁnite diﬀerences are compared against
exact values for a prescribed polynomial. The MWE research software is version-controlled (sec-
tion 3.1), and built using the CMake build system (section 3.2). The example data sets, the
repository snapshot, the active repository, and the minimal report are all cross-linked (section 3.3).
The MWE is archived at the TUdatalib data repository. The research data [37] and the snapshot
of the research code [38] are archived, and cited in the example Research Report [36], along with
the URL of the ”live” code repository. Once the research report [36] is uploaded to the repository,
the metadata of the other data items are updated, by adding ’dc.relation.isreferencedby’ element,
that denotes the data item is referenced by another item. This way the full metadata record of
a data item on the repository stores the cross-linking information, that can be updated as data-
items evolve. For example, as the peer-review process progresses or new milestones are reached,
new sub-versions of data items obtain new DOIs that denote sub-versions, eg. https://doi.org/
10.48328/tudatalib-921.2 for a new version of the Research Report. This makes it possible to
continue updating the Report, Data, Code Snapshot with the feedback from the peer-review process
or during the development of new and improved methods.

A minimal CI pipeline is conﬁgured for the GitLab [22] MWE source code repository [35],
demonstrating the use of Jupyter notebooks for data processing and visualization. The research
data linked with the report [36] is the artifact generated by the CI pipeline [37]. For realistic
CSE tests, Jupyter notebooks contain detailed information about the test setup: geometry of the
problem, initial and boundary conditions, model parameters.

6. Case studies

This section presents two software projects that follow the minimum workﬂow and apply, in
addition, parts or all of the full workﬂow. We highlight which parts of the workﬂow were introduced
at what time of the project and how it helped to improve the software or its development process.

6.1. geophase

The geophase library [39] implements geometrical algorithms and models for intersecting non-
convex polyhedral volumes with non-planar faces in the C ++ programming language. Those algo-
rithms are used in [40] for the positioning of a piecewise linear ﬂuid interface in an unstructured
geometrical Volume-of-Fluid method for simulating two-phase ﬂows [41, 42]. The development of
the Consecutive Cubic Spline (CCS) interface positioning algorithm [40] in the geophase library
serves as a case study for the proposed workﬂow for increasing the quality of scientiﬁc software.

20

6.1.1. Minimum workﬂow

The git VCS is used for version control (section 3.1) and the geophase library [39] is actively
developed on TUGitlab [43]. To simplify cross-platform installation and handling of dependencies
the CMake [44] build system is used (section 3.2). For the submission of the CCS algorithm [40],
the cross-linking of the preprint, the source code and the resulting data was done (section 3.3).
Applying the proposed minimal workﬂow in the peer-review process increases the transparency,
because the reviewers and the rest of the scientiﬁc community can access the preprint, the source
code and the data.

6.1.2. Full workﬂow

Issue tracking (section 4.1) is used to track the status of the project on GitLab using Kanban
boards. The Continuous Integration (CI) is used for ”geophase” as described in section 4.4. As
shown in ﬁg. 7, this also includes publishing Jupyter notebooks and secondary data as CI artifacts,
available for download from the GitLab/Hub web interface.

For geophase, the results from successful CI pipelines are also publicly available [45]. This makes
it possible to quickly isolate possible negative eﬀects between diﬀerent versions, before quantifying
them and testing them inside the CI pipeline. For the submission, a Singularity [33] container
is created (section 4.3) that contains both the geophase repository and its software enviornment.
The Singularity container is conﬁgured in a way that enables execution of diﬀerent tasks with
a single command: (1) the geophase library can be cloned (2) the source code of the geophase
library can be built using the dependencies stored in the container, (3) all tests can be executed
and re-started, (4) the jupyter notebook in the container can be served from within a container to
visualize results. Currently, the generation of the Singularity container (although straightforward) is
still done manually, because the delivery of the container is reserved for a submission. Alternatively,
the creation of the Singularity container can easily be included into the CI pipeline, which would, in
turn, result in Continuous Delivery across diﬀerent platforms. The geophase library was developed
using TDD (section 4.2), the tests have been implemented using the GoogleTest testing framework
and categorized using CTest.

A more complex future step extends the use of the Singularity container to run performance
tests on the HPC cluster as a part of the CI pipeline. Once the production test execution on the
HPC cluster is integrated in the workﬂow shown in ﬁg. 7, the next step is to automate the publishing
of the data sets using the API of the data repository. It is important to note, however, that it is
not necessary to automate every aspect of the workﬂow, because new features are developed very
slowly in scientiﬁc software, and satisfactory results are obtained sometimes after many months or
years of work.

6.2. PIRA

PIRA2[46, 47] is a performance proﬁler for C /C ++ applications. It consists of diﬀerent modules,
implemented in diﬀerent languages, that are maintained across multiple repositories, and various
other external dependencies. Hence, a particular version of PIRA consists of particular versions of
various software projects. When applied to a target application, the user speciﬁes a conﬁguration ﬁle
that determines various parameters. In particular, how the target program is executed and where
to store proﬁling results. This conﬁguration ﬁle is, in its nature, similar to simulation parameters.

2PIRA: https://github.com/tudasc/pira.

21

6.2.1. Minimum workﬂow

As suggested in the proposed workﬂow, the project uses the git VCS. PIRA includes its diﬀerent
components as git submodules, i.e., it tracks a speciﬁc version of theses modules. The project is
implemented in Python and some of its dependencies are implemented in C /C ++. The build
system CMake[44] was used from the start for all native code modules. For its publications, the
respective version of PIRA was tagged, which also includes the speciﬁc version of the submodules.
The conﬁguration ﬁles are stored in a separate git repository, and a branch is created for each
publication. Cross-linking, as proposed in this paper, was not performed for the initial PIRA
publications as it was still unclear which services to use.

6.2.2. Additional workﬂow components in PIRA

PIRA introduced a particular branching model later in the project. The motivation for its
introduction was twofold: the release of PIRA as open source software, and multiple students
working on PIRA. To simplify contributing, and guarantee a certain level of quality, the Gitﬂow [25]
model was adopted. The introduction of the Gitﬂow model helped with stabilizing the project’s
public repository signiﬁcantly.

In addition to the introduction of the Gitﬂow model, the issue tracker and continuous integration
were introduced to improve software quality further. The issue tracker is used to report bugs
and suggest new features or usability improvements. Student assistants contribute by taking over
responsibility for a certain issue. Following the Gitﬂow model, they create a new feature branch,
implement the feature, and re-integrate it using a merge request. Since the adoption of the Gitﬂow
branching model, the issue tracker and continuous integration, the public repository contained only
one partly dysfunctional commit, i.e., a speciﬁc version in which some of the unit tests were failing.
The CI helps signiﬁcantly with the development of features in the software.

A sometimes overlooked aspect of a working CI environment is that it serves greatly as a
reference of what should work. Especially in a university setting, e.g., students on-boarding for
a Bachelor or Master thesis, being able to point to a working conﬁguration and environment of
the software is beneﬁcial. The on-boarding of students, before the introduction of CI, consisted of
multiple meetings to set up the environment and install required dependencies. The introduction
of CI mandated to improve this aspect. Moreover, students start with the CI conﬁguration, and
only speciﬁc questions of the setup are asked and resolved in personal (or virtual) meetings.

Lately, the containerization of PIRA, to simplify the CI environment, and gain ﬁrst experiences

with using PIRA in the cloud [48] was started and is being explored further.

7. Conclusions

The proposed Research Software Engineering workﬂow for Computational Science and Engi-
neering signiﬁcantly increases the quality of scientiﬁc results with a minimal workload overhead,
which makes it applicable in a university research setting with limited dedicated resources for RSE.
Placing the focus of the workﬂow on scientiﬁc publications makes the workﬂow attractive for re-
searchers that already mainly develop CSE research software to generate scientiﬁc publications as
their primary scientiﬁc output.

The minimal workﬂow combines an established build system with a simple feature-based version-
control branching model adapted to the peer-review process. Submission, revision and acceptance

22

of scientiﬁc publications are the main milestones in this branching model, using git-tags and cross-
links between data, software, and publications. This ensures that the FAIR principles [2] are applied
to a large extent in a challenging university research setting with an extremely low work overhead.
The complete workﬂow further increases the sustainability of research software. Next to the
known practice of issue tracking, our version of Test-Driven Development - adapted to CSE research
software - gradually increases the test coverage of research software, allowing researchers to keep
the focus on the subsequent scientiﬁc publication without introducing broad unit-testing unrelated
to the research roadmap. Automatic test quantiﬁcation and result visualization using Jupyter
notebooks in a data-processing pipeline provide a basis for discussions and data analysis that
help in quickly identifying sources of errors. Automatic testing combined with containerization and
minimal workﬂow makes it possible to reproduce research results from any milestone across diﬀerent
computing platforms. Finally, the use of Continuous Integration for automating scientiﬁc workﬂows
enables the results to be reproduced automatically from a git Web interface. Granted, for researchers
unfamiliar with the tools and techniques from the complete workﬂow, a non-negligible learning
investment is expected. However, this investment has a very high return rate. Increasing testing
coverage using our version of TDD as well as automatic test quantiﬁcation and visualization quickly
pay oﬀ even for individual researchers and very small research groups. Continuous Integration
increases research output more gradually once the time comes to combine existing research into
new ideas.

Acknowledgements

This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foun-
dation) – Project-ID 265191195 – SFB 1194, and by the Hessian LOEWE initiative within the
Software-Factory 4.0 project. The authors would also like to thank the Federal Government and
the Heads of Government of the L¨ander, as well as the Joint Science Conference (GWK), for their
funding and support within the framework of the NFDI4Ing consortium. Funded by the German
Research Foundation (DFG) - project number 442146713. Benchmarks were conducted on the
Lichtenberg cluster at TU Darmstadt.

The authors are grateful to Moritz Schwarzmeier for his work on the NFDI4Ing Knowledgebase
[13], and to Moritz Schwarzmeier and Tobias Tolle for providing valuable constructive feedback on
the RSE workﬂow and this manuscript.

References

[1] M. A. Heroux, E. Gonsiorowski, R. Gupta, R. Milewicz, J. D. Moulton, G. R. Watson, J. Wil-
lenbring, R. J. Zamora, E. M. Raybourn, Lightweight software process improvement using pro-
ductivity and sustainability improvement planning (psip), in: G. Juckeland, S. Chandrasekaran
(Eds.), Tools and Techniques for High Performance Computing, Springer International Pub-
lishing, Cham, 2020, pp. 98–110.

[2] FAIR principles, https://www.go-fair.org/fair-principles/, 2022. Accessed: 2022-08-03.

[3] R. C. Jim´enez, M. Kuzak, M. Alhamdoosh, M. Barker, B. Batut, M. Borg, S. Capella-Gutierrez,
N. Chue Hong, M. Cook, M. Corpas, M. Flannery, L. Garcia, J. L. Gelp´ı, S. Gladman, C. Goble,
M. Gonz´alez Ferreiro, A. Gonzalez-Beltran, P. C. Griﬃn, B. Gr¨uning, J. Hagberg, P. Holub,

23

R. Hooft, J. Ison, D. S. Katz, B. Leskoˇsek, F. L´opez G´omez, L. J. Oliveira, D. Mellor, R. Mos-
bergen, N. Mulder, Y. Perez-Riverol, R. Pergl, H. Pichler, B. Pope, F. Sanz, M. V. Schnei-
der, V. Stodden, R. Suchecki, R. Svobodov´a Vaˇrekov´a, H. A. Talvik, I. Todorov, A. Treloar,
S. Tyagi, M. van Gompel, D. Vaughan, A. Via, X. Wang, N. S. Watson-Haigh, S. Crouch, Four
simple recommendations to encourage best practices in research software, F1000Research 6
(2017) 1–15. doi:10.12688/f1000research.11407.1.

[4] J. Fehr, J. Heiland, C. Himpe, J. Saak, Best practices for replicability, reproducibility and
reusability of computer-based experiments exempliﬁed by model reduction software, AIMS
Mathematics 1 (2016) 261–281. doi:10.3934/Math.2016.3.261, arXiv: 1607.01191.

[5] G. Wilson, D. A. Aruliah, C. T. Brown, N. P. Chue Hong, M. Davis, R. T. Guy, S. H.
Haddock, K. D. Huﬀ, I. M. Mitchell, M. D. Plumbley, B. Waugh, E. P. White, P. Wilson, Best
Practices for Scientiﬁc Computing, PLoS Biology 12 (2014). doi:10.1371/journal.pbio.
1001745, arXiv: 1210.0530.

[6] G. Wilson, J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, T. K. Teal, Good enough practices
in scientiﬁc computing, PLOS Computational Biology 13 (2017) e1005510. URL: https://
journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510. doi:10.
1371/journal.pcbi.1005510, publisher: Public Library of Science.

[7] L. Stanisic, A. Legrand, V. Danjean, An Eﬀective Git And Org-Mode Based Workﬂow For
Reproducible Research, ACM SIGOPS Operating Systems Review 49 (2015) 61–70. URL:
https://doi.org/10.1145/2723872.2723881. doi:10.1145/2723872.2723881.

[8] H. Anzt, Y.-C. Chen, T. Cojean, J. Dongarra, G. Flegar, P. Nayak, E. S. Quintana-Ort´ı,
Y. M. Tsai, W. Wang, Towards Continuous Benchmarking: An Automated Performance
Evaluation Framework for High Performance Software,
in: Proceedings of the Platform for
Advanced Scientiﬁc Computing Conference, PASC ’19, Association for Computing Machinery,
New York, NY, USA, 2019, pp. 1–11. URL: https://doi.org/10.1145/3324989.3325719.
doi:10.1145/3324989.3325719.

[9] M. Riesch, T. D. Nguyen, C. Jirauschek, bertha: Project skeleton for scientiﬁc software,
PLOS ONE 15 (2020) e0230557. URL: https://journals.plos.org/plosone/article?
id=10.1371/journal.pone.0230557. doi:10.1371/journal.pone.0230557, publisher: Pub-
lic Library of Science.

[10] Z. Sampedro, A. Holt, T. Hauser, Continuous Integration and Delivery for HPC: Using Singu-
larity and Jenkins, in: Proceedings of the Practice and Experience on Advanced Research Com-
puting, PEARC ’18, Association for Computing Machinery, New York, NY, USA, 2018, pp.
1–6. URL: https://doi.org/10.1145/3219104.3219147. doi:10.1145/3219104.3219147.

[11] Better Scientiﬁc Software (BSSw), https://bssw.io/, 2022. Accessed: 2022-08-03.

[12] National Research Data Initiative, https://nfdi4ing.de/, 2022. Accessed: 2022-08-03.

[13] NFDI4Ing Knowledge Base, https://nfdi4ing.pages.rwth-aachen.de/knowledge-base/, 2022.

Accessed: 2022-08-03.

[14] Software Sustainability Institute, https://software.ac.uk/, 2022. Accessed: 2022-08-03.

24

[15] T. T. W. Community, The Turing Way: A handbook for reproducible, ethical and collabora-
tive research, 2022. URL: https://doi.org/10.5281/zenodo.6909298. doi:10.5281/zenodo.
6909298.

[16] Singularity, https://singularity.lbl.gov/, 2022. Accessed: 2022-08-03.

[17] S. Khuvis, Z. Q. You, H. Na, S. Brozell, E. Franz, T. Dockendorf, J. Gardiner, K. Tomko,
in: ACM International

A continuous integration-based framework for software management,
Conference Proceeding Series, 2019. doi:10.1145/3332186.3332219.

[18] Jupyter, https://jupyter.org/, 2022. Accessed: 2022-08-03.

[19] M. Beg, J. Taka, T. Kluyver, A. Konovalov, M. Ragan-Kelley, N. M. Thiery, H. Fangohr,
Using Jupyter for Reproducible Scientiﬁc Workﬂows, Computing in Science and Engineering
23 (2021) 36–46. doi:10.1109/MCSE.2021.3052101.

[20] Binder, https://mybinder.org/, ???? Accessed: 2022-05-05.

[21] S. Chacon, B. Straub, Pro git, Apress, 2014.

[22] GitLab, https://gitlab.com, 2022. Accessed: 2022-08-03.

[23] GitHub, https://github.com, 2022. Accessed: 2022-08-03.

[24] Bitbucket, https://bitbucket.org, 2022. Accessed: 2022-08-03.

[25] Gitﬂow, https://nvie.com/posts/a-successful-git-branching-model/, 2022. Accessed:

2022-08-03.

[26] Github ﬂow, https://guides.github.com/introduction/flow/, 2022. Accessed: 2022-08-

03.

[27] Zenodo, https://zenodo.org/, 2022. Accessed: 2022-08-03.

[28] TUdatalib, https://tudatalib.ulb.tu-darmstadt.de/, 2022. Accessed: 2022-08-03.

[29] arXiv, https://arxiv.org/, 2022. Accessed: 2022-08-03.

[30] Y. Sugimori, K. Kusunoki, F. Cho, S. UCHIKAWA, Toyota production system and kanban
system materialization of just-in-time and respect-for-human system, The international journal
of production research 15 (1977) 553–564.

[31] K. Beck, Test-driven development: by example, Addison-Wesley Professional, 2003.

[32] The HDF Group, Hierarchical data format version 5, 2000-2010. URL: http://www.hdfgroup.

org/HDF5.

[33] G. M. Kurtzer, V. Sochat, M. W. Bauer, Singularity: Scientiﬁc containers for mobility of

compute, PloS one 12 (2017) e0177459.

[34] M. Fowler, M. Foemmel, Continuous integration, 2006.

[35] Minimal Working Example Repository, 2022. URL: https://gitlab.com/cse-ci-examples/

cse-rse-workflow-code/-/tree/2022-07-21-preprint, accessed: 2022-07-21.

25

[36] T. Maric, Computational science and engineering - research software engineering workﬂow :

Research report, 2022. URL: https://doi.org/10.48328/tudatalib-921.2.

[37] T. Maric, Computational science and engineering - research software engineering workﬂow :

Research data, 2022. URL: https://doi.org/10.48328/tudatalib-910.2.

[38] T. Maric, Computational science and engineering - research software engineering workﬂow :
Research code, 2022. URL: https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/
3524.5.

[39] Git repository of the geophase library, https://git.rwth-aachen.de/leia/geophase, 2022. Ac-

cessed: 2022-08-03.

[40] T. Mari´c, Iterative volume-of-ﬂuid interface positioning in general polyhedrons with consecutive

cubic spline interpolation, 2020. arXiv:2012.02227.

[41] T. Mari´c, H. Marschall, D. Bothe, An enhanced un-split face-vertex ﬂux-based vof method,

Journal of computational physics 371 (2018) 967–993.

[42] T. Mari´c, D. B. Kothe, D. Bothe, Unstructured un-split geometrical volume-of-ﬂuid methods–a

review, Journal of Computational Physics 420 (2020) 109695.

[43] TUGitLab, https://git.rwth-aachen.de/, 2022. Accessed: 2022-08-03.

[44] CMake, https://cmake.org, 2022. Accessed: 2022-08-03.

[45] CI results repository of the geophase library, https://leia.pages.rwth-aachen.de/geophase-ci-

results/, 2022. Accessed: 2022-08-03.

[46] J.-P. Lehr, A. H¨uck, C. Bischof, Pira: Performance instrumentation reﬁnement automation,
in: 5th ACM SIGPLAN Intl. Workshop on Artiﬁcial Intelligence and Empirical Methods for
Software Engineering and Parallel Computing Systems, AI-SEPS 2018, ACM, 2018, pp. 1–10.
URL: http://doi.acm.org/10.1145/3281070.3281071. doi:10.1145/3281070.3281071.

[47] J.-P. Lehr, A. Calotoiu, C. Bischof, F. Wolf, Automatic instrumentation reﬁnement for empir-
ical performance modeling, in: 2019 IEEE/ACM Intl. Workshop on Programming and Perfor-
mance Visualization Tools (ProTools), IEEE, 2019, pp. 40–47. doi:10.1109/ProTools49597.
2019.00011.

[48] D. Sokolowski, J.-P. Lehr, C. Bischof, G. Salvaneschi, Leveraging hybrid cloud hpc with mul-
titier reactive programming, in: 2020 IEEE/ACM International Workshop on Interoperability
of Supercomputing and Cloud Technologies (SuperCompCloud), 2020. To appear.

26

