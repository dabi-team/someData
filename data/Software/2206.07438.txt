2
2
0
2

l
u
J

7
2

]

G
L
.
s
c
[

2
v
8
3
4
7
0
.
6
0
2
2
:
v
i
X
r
a

Multi-Objective Hyperparameter Optimization â€“ An
Overview
FLORIAN KARLâˆ—, Fraunhofer Institut fÃ¼r integrierte Schaltungen, Germany
TOBIAS PIELOKâˆ—, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
JULIA MOOSBAUER, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
FLORIAN PFISTERER, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
STEFAN COORS, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
MARTIN BINDER, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
LENNART SCHNEIDER, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
JANEK THOMAS, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
JAKOB RICHTER, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany
MICHEL LANG, Technische UniversitÃ¤t Dortmund, Germany
EDUARDO C. GARRIDO-MERCHÃN, Universidad Pontificia Comillas, Spain
JUERGEN BRANKE, Warwick Business School, Germany
BERND BISCHL, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany

Hyperparameter optimization constitutes a large part of typical modern machine learning workflows. This
arises from the fact that machine learning methods and corresponding preprocessing steps often only yield
optimal performance when hyperparameters are properly tuned. But in many applications, we are not only
interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be
considered when determining an optimal configuration, resulting in a multi-objective optimization problem.
This is often neglected in practice, due to a lack of knowledge and readily available software implementations
for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-
objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide
an extensive survey of existing optimization strategies, both from the domain of evolutionary algorithms
and Bayesian optimization. We illustrate the utility of MOO in several specific ML applications, considering
objectives such as operating conditions, prediction time, sparseness, fairness, interpretability and robustness.

CCS Concepts: â€¢ Computing methodologies â†’ Supervised learning; â€¢ Theory of computation â†’ Evo-
lutionary algorithms; â€¢ Applied computing â†’ Multi-criterion optimization and decision-making.
Additional Key Words and Phrases: Multi-Objective Hyperparameter Optimization, Neural Architecture Search,
Bayesian Optimization

1 INTRODUCTION
With the immense popularity of machine learning (ML) and data-driven solutions for many do-
mains [229], the demand for automating the creation of suitable ML pipelines has strongly in-
creased [126]. Automated machine learning (AutoML) and automated hyperparameter optimization
(HPO) promise to simplify the ML process by enabling less experienced practitioners to optimally
configured ML models for a variety of tasks - reducing manual effort and improving performance at

âˆ—Both authors contributed equally to this research.

Authorsâ€™ addresses: Florian Karl, florian.karl@iis.fraunhofer.de, Fraunhofer Institut fÃ¼r integrierte Schaltungen, Germany;
Tobias Pielok, tobias.pielok@stat.uni-muenchen.de, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Julia Moosbauer,
Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Florian Pfisterer, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Ger-
many; Stefan Coors, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Martin Binder, Ludwig-Maximilians-UniversitÃ¤t
MÃ¼nchen, Germany; Lennart Schneider, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Janek Thomas, Ludwig-
Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Jakob Richter, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany; Michel
Lang, Technische UniversitÃ¤t Dortmund, Germany; Eduardo C. Garrido-MerchÃ¡n, Universidad Pontificia Comillas, Spain;
Juergen Branke, Warwick Business School, Germany; Bernd Bischl, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Germany.

 
 
 
 
 
 
2

Karl and Pielok, et al.

the same time [30, 109, 126, 184, 264]. HPO is often classified as a black-box optimization problem,
a type of optimization problem where only the outputs of the function to be optimized can be
observed and no analytic expression of the underlying function is known. Furthermore, these
optimization problems are often noisy and expensive [12]. Another challenge in HPO is that there
is rarely a clear-cut, obvious, single performance metric, which is in stark contrast to the setup
often described in many algorithm-centric research papers and introductory books [109, 126, 181].
Nowadays, models and pipelines are held to a high standard, as the ML process (as currently
realized in many businesses and institutions) comes with a number of different stakeholders. While
predictive performance measures are still decisive in most cases, models must be reliable, robust,
accountable for their decisions, efficient for seamless deployment, and so on. For example, in
many internet-of-things applications, an ML model is deployed on edge devices like smartphones,
watches, and embedded systems [117]. Power consumption, memory capacity, and latency can be
limiting factors when deploying models in such settings, and obvious trade-offs between these
factors and predictive performance exist. The current state-of-the-art models in computer vision
and natural language processing with millions or billions of weights are not applicable in such
settings [24]. Even putting aside secondary-order objectives like efficiency, interpretability etc.,
expressing just predictive performance as a single metric can frequently be challenging. The most
well known example of this is arguably in medical applications: For a diagnostic test, solely looking
at misclassification rates is ill-advised: Misclassifying a sick patient as healthy (false negative) has
usually much more severe consequences than classifying a healthy person erroneously as sick (false
positive), i.e., different misclassification costs, which are often unknown or hard to quantify, have
to be considered [87]. Of course, multiple objectives can in principle be aggregated into a single
metric, which converts a multi-objective optimization (MOO) problem to a single-objective one
(SOO). However, it is often unclear how a trade-off between different objectives should be defined a
priori, i.e., before possible alternative solutions are known [41]. In this paper, we argue that there is
substantial merit in directly approaching a multi-objective HPO (MOHPO) problem as such and give
a comprehensive review of methods, tools and applications; preliminary work [29, 116] supports
our view. Furthermore, other prominent work in ML research - aside from HPO - has advocated for
a multi-objective perspective [128, 135, 190]. For example, as argued in Mierswa [190], many ML
and data mining applications inherently concern trade-offs and thus should be approached via an
MOO formulation and MOO methods. And even if the main interest lies in a single objective it still
might be advantageous to approach the problem via MOO methods since they have the potential to
reduce local minimas in this case [151]. MOO algorithms seek to approximate the set of efficient or
Pareto-optimal solutions. These solutions have different trade-offs, but it is not possible to improve
any objective without degrading at least one other objective. This set of Pareto optimal solutions
can then be analyzed by domain experts in a post-hoc manner, and an informed decision can be
made as to which trade-off should be used in the application, without requiring the user to specify
this a priory [135, 137].

This paper provides a comprehensive review on the topic of MOHPO, explains the most pop-
ular algorithms, discusses main challenges and opportunities, and surveys existing applications.
We restrict the scope of this paper to the realm of supervised ML. Unsupervised ML, in contrast,
entails a different set of metrics to the scenario studied in our manuscript and is largely governed
by custom, usecase-specific measures [86, 203] and sometimes even visual inspection of results.
The rest of the paper is structured as follows. First we define the MOHPO problem in Section 2.
Section 3 presents the theoretical foundations of MOO, i.e., how to evaluate sets of candidate
solutions. Then, we introduce several important MOHPO methods in Section 4, such as variants
of Bayesian Optimization (BO) and also Hyperband. Finally, Section 5 introduces a number of

Multi-Objective Hyperparameter Optimization â€“ An Overview

3

applications for MOHPO and their associated objectives. We will categorize these applications
through exploring three overarching perspectives on the ML process: (1) Performance metrics,
(2) metrics that measure costs and restrictions at deployment like efficiency, and (3) metrics that
enforce reliability and interpretability.

2 HYPERPARAMETER OPTIMIZATION

2.1 The machine learning problem

Taxonomy of black-box problems

Stochasticity

Domain

Codomain

Evaluation cost

deterministic

dimensionality

single-objective â„

cheap

multi-objective
â„ğ‘š, ğ‘š âˆˆ {2, 3, 4}

many objective
â„ğ‘š, ğ‘š â‰¥ 5

expensive

stochastic,
homoscedastic

stochastic,
heteroscedastic

low dim ğ‘‘

high dim ğ‘‘

type

numerical

purely categorical

mixed numerical
and categorical

hierarchical /
structured

Fig. 1. Taxonomy of common black-box optimization problems. Attributes that are related to MOHPO - and
therefore a substantial focus in this paper - are highlighted.

The fundamental ML problem can be defined as follows. Let D be a dataset with ğ‘› input-output
pairs (cid:0)x(ğ‘–), ğ‘¦ (ğ‘–) (cid:1) âˆˆ X Ã— Y, which are independent and identically distributed (i.i.d.) from a data-
generating distribution â„™ğ‘¥ ğ‘¦. An ML algorithm I (Â·, ğ€) configured by hyperparameters ğ€ âˆˆ Î› maps a
dataset D to a model ğ‘“ : X â†’ â„ğ‘” in the hypothesis space H via I : (ğ”» Ã— Î›) â†’ H, where ğ”» is the
set of all finite datasets. With a slight abuse of notation we will also write Iğ€ if the hyperparameter
ğ€ is fixed, i.e., Iğ€ (D) = I (D, ğ€). What we would like to optimize is the expected generalization
performance ğºğ¸ of our model, when trained on D, on new unseen data (x, ğ‘¦) âˆ‰ D, i.e.,

GE(I, ğ€, ğ‘›, ğ¿) = ğ”¼ [ğ¿(ğ‘¦, Iğ€ (D)(x))]

(1)

with the expectation taken over the random data D of size ğ‘› and a fresh test sample (x, ğ‘¦), both
independently sampled from â„™ğ‘¥ ğ‘¦. Since the data generating distribution â„™ğ‘¥ ğ‘¦ is usually unknown,
also GE has to be estimated. To do so, the data D is split into ğµ training and test sets by a resampling

4

Karl and Pielok, et al.

method Dğ‘
train and Dğ‘
error can then be computed as

test such that D = Dğ‘

train (cid:164)âˆª Dğ‘

test, ğ‘ = 1, . . . , ğµ. The expected generalization

(cid:99)GE (I, J, ğ¿, ğ€) :=

1
ğµ

ğµ
âˆ‘ï¸

ğ‘=1

1
|Dğ‘

test|

âˆ‘ï¸

ğ¿

(cid:16)
ğ‘¦, Iğ€ (Dğ‘

train)(x)

(cid:17)

(x,ğ‘¦) âˆˆ Dğ‘
test

(2)

where J is the set of train-test-splits. Often, just a regularized error on the training data is minimized
with a with point-wise loss function ğ¿ (ğ‘¦, ğ‘“ (x)) : Y Ã— â„ğ‘” â†’ â„.

Ë†ğ‘“ = arg min

ğ‘“ âˆˆH

Remp(ğ‘“ ) + ğœ‚ ğ½ (ğ‘“ ), with Remp(ğ‘“ ) :=

âˆ‘ï¸

(x,ğ‘¦) âˆˆD

ğ¿ (ğ‘¦, ğ‘“ (x)) ,

(3)

The regularizer ğ½ (ğ‘“ ) expresses a preference (or prior in the Bayesian sense) for simpler models
and is usually formulated as some kind of norm on the parameter vector, e.g., the L2 norm for a
ridge penalty, and its strength is controlled by an HP ğœ‚. 1 Most learning algorithms are configured
by a possibly large number of hyperparameters ğ€ that control the hypothesis space or the fitting
procedure. Usually, the generalization error (cid:99)GE (I, J, ğ¿, ğ€) critically depends on the choice of ğ€.
In most cases, the analytical relationship of hyperparameters and generalization error is unknown,
and even to experts it is not clear how to choose optimal hyperparameter values. Hyperparameter
optimization aims to minimize the estimated generalization error (cid:99)GE for a given dataset D using
(cid:99)GE (I, ğ€) , where ËœÎ›, a bounded subspace of the
the hyperparameter configuration (HPC) ğ€: min
ğ€ âˆˆ ËœÎ›
hyperparameter space Î›, is the so-called search-space or domain in the context of the above
optimization problem. Note that it is important to not use any test data during training or HPO as
this could lead to an optimistic bias. Instead, nested resampling techniques should be applied [31]:
The data is split into an optimization set Doptim and a test set Dtest. During the optimization process,
the model performance should only be assessed by an (inner) resampling technique, e.g., cross-
validation, on Doptim, whereas Dtest should only be used for the final assessment of the chosen
model(s). A simple alternative is simply splitting Doptim into two datasets Dtrain and Dval, which
leads to the widely known train/val/test-split. Afterwards, a resulting non-dominated solution set
can be determined on Dtest based on the solution candidates found on Doptim. In practice, it is very
important to have no hidden information leakage from test set to training, which may for example
happen when preprocessing (such as missing value imputation or feature selection) is done on the
combined set. The estimate based on a single train-test-split can have a high variance, for example
because of a strong dependence on the data split. To counter this, an (outer) resampling strategy
like cross-validation is usually used, and the average of the estimated generalizations of the quality
indicator on (D (ğ‘˜)

test) with ğ‘˜ âˆˆ â„• is reported.

optim, D (ğ‘˜)

2.2 Black box optimization
As there is generally no analytical expression of the general hyperparameter optimization problem,
it forms a black-box function. Black-box functions can be characterized according to different at-
tributes (see Figure 1), which influence the difficulty of the problem. For further fundamental details
on SOO for HPO see [30]. In an ML scenario, we are often interested in the generalization error with
respect to more than one loss as well as further relevant criteria like robustness, interpretability,
sparseness, or efficiency of a resulting model [135]. Therefore, we introduce a generalized definition

1This can already be seen as a simple scalarization of a multi-objective problem, as we combine the different measures
for empirical risk and regularization into a weighted sum. Mierswa [190] derives the two conflicting objectives from
this scalarized version and presents an explicit multi-objective formulation of the optimization problem (minimizing the
regularized empirical risk).

Multi-Objective Hyperparameter Optimization â€“ An Overview

5

of the hyperparameter optimization problem that takes into account ğ‘š criteria: Given a number of
evaluation criteria ğ‘1 : Î› â†’ â„, . . . , ğ‘ğ‘š : Î› â†’ â„ with ğ‘š âˆˆ â„•, we define ğ‘ : Î› â†’ â„ğ‘š to assign an
ğ‘š-dimensional cost vector to a HPC ğ€. The estimated generalization error (cid:99)GE is one evaluation
criterion that is commonly used, but many further criteria will be discussed in this paper. The
general multi-objective hyperparameter optimization problem can be defined as

min
ğ€ âˆˆÎ›

ğ‘ (ğ€) = min
ğ€ âˆˆÎ›

(ğ‘1(ğ€), ğ‘2(ğ€), . . . , ğ‘ğ‘š (ğ€)) .

(4)

Without loss of generality, we assume that all criteria are minimized. The domain Î› of the
problem is called numerical if only numeric HPs ğ€ are optimized. By including additional discrete
hyperparameters, like the type of kernel used in a support sector machine (SVM), the search space
becomes mixed numerical and categorical. Mixed search spaces already require adaption of some
optimization strategies, such as BO, which we will discuss in Section 4.3. It can also be necessary to
introduce further conditional hierarchies between hyperparameters. For example, when optimizing
over different kernel types of an SVM, the ğ›¾ kernel hyperparameter is only valid if the kernel
type is set to Radial Basis Function (RBF), while for a polynomial kernel, a hyperparameter for the
polynomial degree must be specified. These conditional hierarchies can become highly complicated
- especially when moving from pure HPO to optimizing over full ML pipelines, i.e., AutoML, or over
neural network architectures, referred to as neural architecture search (NAS) [88, 200, 215]. The
dimensionality of the search space dim( ËœÎ›) also directly influences the difficulty of the problem:
While it might be desirable to include as many hyperparameters as possible (since it is often
unknown which hyperparameters are actually important), this increases the complexity of the
optimization problem and requires an increasingly larger budget of (expensive) evaluations. The
co-domain, also called objective space, i.e., â„ğ‘š, of the problem is characterized by the number of
objectives ğ‘š. Objective functions ğ‘ğ‘–, ğ‘– âˆˆ {1, 2, ..., ğ‘š} can be characterized by their evaluation cost and
stochasticity. Some evaluation criteria are deterministic, such as the required memory of a model on
hard disk. Other criteria can only be measured with some additional noise, e.g., the generalization
performance discussed above. We generally assume that all objective functions are black-boxes,
i.e., analytical information about ğ‘ğ‘– is not available, even though there are exceptions to this (e.g.,
number of parameters in some models). To record the evaluated hyperparameter configurations and
their respective scores, we introduce the so-called archive A = ((ğ€ (1), ğ‘ (ğ€ (1) )), (ğ€ (2), ğ‘ (ğ€ (2) )), . . . ),
with A [ğ‘¡ +1] = A [ğ‘¡ ] âˆª (ğ€+, ğ‘ (ğ€+)) if a single configuration is presented by an algorithm that
iteratively proposes hyperparameter configurations.

Example 1. Consider a standard scenario in which we would like to train a deep neural network for
object detection on images. Assume that ğ‘š = 2 criteria are relevant: While predictive performance is one
criterion (ğ‘1) to be optimized, we also optimize for memory consumption (ğ‘2) of the network at prediction
time, as the model is deployed on a mobile phone. Both criteria depend on the hyperparameters and
architectural choices of the network Î› = (ğœ†1, ğœ†2), and they are potentially conflicting: Memory-efficient
models are usually less complex and may use special types of convolutions [117]. An insufficient
capacity of the model may result in bad generalization. The trade-off between the two objectives is
a priori unknown, because neither the range of generalization performance nor the userâ€™s hardware
constraints are known (we explore scenarios like this one in Section 5.2). Ideally, we would like to have
a set of solutions with different optimal trade-offs in the two objectives. We will formalize this notion
in Section 3.2.

2.3 Multi-objective machine learning
A concept closely related but different to MOHPO is multi-objective machine learning. To un-
derstand the difference, it is important to differentiate between first level model parameters (e.g.,

6

Karl and Pielok, et al.

weights of a neural network or learned decision rules) and second order hyperparameters (HPs)
(e.g., neural network architectures and optimizers) [30]. Model parameters are fixed by the ML
algorithm at training time in accordance to one or multiple metrics, whereas HPs are chosen by
the ML practitioner before training and influence the behavior of the learning algorithm and the
structure of its associated hypothesis space. We define multi-objective ML methods as those that
focus on learning first level parameters (sometimes together with second level hyperparameters).
Our work, in contrast, concentrates on hyperparameter optimization for ML algorithms, i.e., tuning
second level HPs. An example to further illustrate this distinction can be found in Suttorp and
Igel [241]: The authors examine both tuning model parameters and hyperparameters in order
to improve performance of a support vector machine (SVM). To provide a clearer picture and to
enable a better distinction, we try to give an overview of existing approaches and inherent pros
and cons in multi-objective ML before we continue. While this section focuses on summarizing
key ideas, a more comprehensive summary and case study of such approaches was conducted
in Jin and Sendhoff [137]. Broadly, multi-objective ML approaches try to internally balance for
improved generalization, multiple error metrics or improved interpretability to obtain more diversity
in ensemble members and many other criteria (c.f. Jin and Sendhoff [137] for a list of existing
approaches). Another potential advantage of internally optimizing multiple objectives is that it
may help the ML algorithm to escape local optima, thus improving the accuracy of the ML model.
To provide an example, Iglesia et al. [127] use a variant of NSGA-II (c.f. Section 4.2.3) to learn
association rules that are accurate, simple and diverse. However, trade-offs are not exposed as
hyperparameters to be tuned, but instead the method yields a set of rules, which approximate the
Pareto set. Feature selection is a topic that borders MOHPO and multi-objective ML and is often
handled in a multi-objective manner [18, 29, 191]. We consider feature selection as closely related
to HPO and will therefore dedicate large portions of Section 5.6 to this topic. However, it should be
noted that feature selection is often grouped with multi-objective ML, as e.g., in Jin [135].

3 FOUNDATIONS OF MULTI-OBJECTIVE OPTIMIZATION

3.1 Objectives and constraints
In the context of this paper, objectives refer to the evaluation criteria of the ML model ğ‘1 : ËœÎ› â†’
â„, . . . , ğ‘ğ‘š : ËœÎ› â†’ â„ with ğ‘š âˆˆ â„•. While oftentimes we simply aim to minimize these objectives, in
real-world applications, we may well face a constrained HPO problem of the form:

minimize
ğ€ âˆˆ ËœÎ›

ğ‘ (ğ€)

subject to ğ‘˜1(ğ€) = 0, . . . , ğ‘˜ğ‘› (ğ€) = 0
Ë†ğ‘˜1(ğ€) â‰¥ 0, . . . , Ë†ğ‘˜ Ë†ğ‘› (ğ€) â‰¥ 0

(equality constraints),

(inequality constraints),

where ğ‘ : ËœÎ› â†’ â„ğ‘š as before. It is the task of an ML practitioner to translate a real-world problem
into an ML task - and therefore objectives and constraints - to measure the quality and feasibility
of a given model. Depending on the use case it has to be carefully considered whether to frame a
requirement to an ML model as an objective or a constraint. For example, is it important to have
a model as memory-efficient as possible or is memory requirement limited by a hard constraint?
Finding high performing and efficient deep learning architectures has emerged as a prominent
task recently, coining the term hardware-aware NAS (HW-NAS) [24]. Successful approaches exist
that frame the HW-NAS problem as a constrained optimization problem [44, 244] or a MOO
problem [78, 182]. It should be noted that constrained optimization comes with its own set of
challenges that are orthogonal to the multi-objective aspect we focus on in this paper. We therefore

Multi-Objective Hyperparameter Optimization â€“ An Overview

7

exclude constrained optimization from our work - only mentioning it in absolutely crucial parts
and giving helpful references to the reader when appropriate.

3.2 Pareto optimality
A HPC ğ€ âˆˆ ËœÎ› (Pareto-)dominates another configuration ğ€ â€², written as2 ğ€ â‰º ğ€ â€², if and only if

âˆ€ğ‘– âˆˆ {1, ..., ğ‘š} : ğ‘ğ‘– (ğ€) â‰¤ ğ‘ğ‘– (ğ€ â€²) âˆ§
âˆƒğ‘— âˆˆ {1, ..., ğ‘š} : ğ‘ ğ‘— (ğ€) < ğ‘ ğ‘— (ğ€ â€²) .

(5)

In other words: ğ€ dominates ğ€ â€², if and only if there is no criterion ğ‘ğ‘– in which ğ€ â€² is superior to ğ€,
and at least one criterion ğ‘ ğ‘— in which ğ€ is strictly better. For example, if the number of false positive
and number of false negative classifications are the two criteria of interest, a HPC configuration ğ€
dominates ğ€ â€² if the model trained by I (Â·, ğ€) shows (at most) as many false positives as the model
trained by I (Â·, ğ€ â€²), but produces less false negatives at prediction time.

Fig. 2. Illustration for a two-dimensional MOO problem with two objectives ğ‘1 and ğ‘2. The left plot shows
the search space ËœÎ›, and the right plot shows the objective space â„2. Configurations for which no other
configuration ğ€ has lower objective values in both objectives - the estimated Pareto set (left) - and their
mapping to the co-domain - the estimated Pareto front (right) - are highlighted.

We say ğ€ weakly dominates ğ€ â€², written as ğ€ âª¯ ğ€ â€², if and only if ğ€ â‰º ğ€ â€² or âˆ€ğ‘– âˆˆ {1, . . . , ğ‘š} ğ‘ğ‘– (ğ€) =
ğ‘ğ‘– (ğ€ â€²). A configuration ğ€âˆ— is called non-dominated or (Pareto) optimal if and only if there is no
other ğ€ âˆˆ ËœÎ› that dominates ğ€âˆ—. Pareto dominance defines only a partial order over ËœÎ›, i.e., two
configurations ğ€ and ğ€ â€² can also be incomparable. This situation arises if there exist ğ‘–, ğ‘— âˆˆ {1, . . . , ğ‘š}
for which ğ‘ğ‘– (ğ€) < ğ‘ğ‘– (ğ€ â€²) but also ğ‘ ğ‘— (ğ€ â€²) < ğ‘ ğ‘— (ğ€). Hence, in contrast to single-objective optimization,
there is in general no unique single best solution ğ€âˆ—, but a set of Pareto optimal solutions that are
pairwise incomparable with regard to â‰º. This set of solutions is referred to as the Pareto (optimal)
set or efficient frontier [10] and defined as

P := (cid:8)ğ€ âˆˆ ËœÎ› | (cid:154) ğ€ â€² âˆˆ ËœÎ› s.t. ğ€ â€² â‰º ğ€(cid:9) .

(6)

2In some literature, the direction of the domination relationship is reversed, i.e., they write ğ€â€² â‰º ğ€ if ğ€ dominates ğ€â€². We
choose our notation because it naturally fits the minimization perspective taken in this paper.

âˆ’0.6âˆ’0.30.00.30.6âˆ’0.6âˆ’0.30.00.30.6l1l2Pareto Set (search space)âˆ’0.50.00.51.00.00.40.8c1(l)c2(l)Pareto Front (objective space)âˆ’0.6âˆ’0.30.00.30.6âˆ’0.6âˆ’0.30.00.30.6l1l2Pareto Set (search space)âˆ’0.50.00.51.00.00.40.8c1(l)c2(l)Pareto Front (objective space)8

Karl and Pielok, et al.

The image of P under ğ‘, written as ğ‘ (P), is referred to as the Pareto front (see Figure 2). The
goal of a multi-objective optimizer that solves (4) is not to find a single best configuration ğ€âˆ—, but
rather a set of configurations Ë†P that approximates the Pareto set P well.

Example 2. Continuing from Example 1, we compare two potential neural networks: network A
and network B. Network A with configuration ğ€ğ´ has an accuracy of ğ‘1(ğ€ğ´) = 0.87 and memory
consumption of ğ‘2(ğ€ğ´) = 127, while network B with ğ€ğµ has the same accuracy ğ‘1(ğ€ğµ) = 0.87 but
lower memory consumption ğ‘2(ğ€ğµ) = 112. Consequently, network A is dominated by network B as
ğ€ğµ â‰º ğ€ğ´. Considering a third network, network C, with ğ‘1(ğ€ğ¶ ) = 0.89 and ğ‘2(ğ€ğ¶ ) = 180, we can see
that both ğ€ğ¶ and ğ€ğµ are non-dominated, and thus the current best approximation of the Pareto set is
Ë†P = {ğ€ğµ, ğ€ğ¶ }.

3.3 Evaluation
The result of a multi-objective algorithm is Ë†P, the set of points of the estimated Pareto front. In
order to evaluate this set or compare it to other sets, one must define what it means for a Pareto
front to be better than another. Usually, this comparison is quantitatively based on so-called quality
indicators.
3.3.1 Comparing solution sets. Let Ë†Î› A and Ë†Î› B be two non-dominated solution sets - i.e., within
each set, no configuration is dominated by another configuration. The associated approximated
Pareto fronts are denoted by A and B respectively, i.e., ğ‘ ( Ë†Î› A) = A and ğ‘ ( Ë†Î› B) = B. According
to Zitzler et al. [271], A is said to weakly dominate B, denoted as A âª¯ B, if for every solution
ğ€ğ‘ âˆˆ Ë†Î› B there is at least one solution ğ€ğ‘ âˆˆ Ë†Î› A which weakly dominates ğ€ğ‘. A is furthermore said
to be better than B, denoted as A âŠ² B, if A âª¯ B, but not every solution of Ë†Î› A is weakly dominated
by any solution in Ë†Î› B, i.e., A âª¯Ì¸ B. This represents the weakest form of superiority between two
approximations of the Pareto front. Note that these order relationships defined for A and B can
directly be transferred to the associated solution sets Ë†Î› A and Ë†Î› B. How well a single solution set
represents the Pareto front can be divided into four qualities [166]:
Convergence The proximity to the true Pareto front
Spread The coverage of the Pareto front
Uniformity The evenness of the distribution of the solutions
Cardinality The number of solutions
The combination of spread and uniformity is also referred to as diversity. One way of comparing
these qualities is to visualize the solution sets. For bi-objective optimization problems, this can
be straightforwardly done. However, for a higher number of objectives, the visualization and
decision-making process based on this visualization can become substantially more challenging.
TuÅ¡ar and FilipiÄ [251] offer a review of existing visualization methods.

3.3.2 Quality indicators. An objective measurement of the quantitative difference between solution
sets is clearly desirable for comparing algorithms. Therefore, many quality indicators ğ¼ - which map
the approximation of the Pareto front to a real number representing the quality of a set of solutions -
were proposed. An extensive overview of these indicators can be found in Li and Yao [166]. Quality
indicators that focus on all four qualities listed above can be divided into distance-based, which
require the knowledge of the true Pareto front or a suitable approximation of it, and volume-based,
which measure the volume between the approximated Pareto Front and a method-specific point.
Common distance-based quality indicators are the inverted generational distance [57], Dist2 [60]
and the ğœ–-indicator [271]. Common volume-based indicators are hypervolume indicator [270], the
R class of indicators [106], and the integrated preference functional [47]. The most popular quality
indicator is the hypervolume indicator [270], also called dominated hypervolume or S-metric,

Multi-Objective Hyperparameter Optimization â€“ An Overview

9

since it does not require any prior knowledge of the Pareto front. The hypervolume, HV, of an
approximation of the Pareto front A can be defined as the combined volume of the dominated
hypercubes domHCğ’“ of all solution points ğ€ğ‘ âˆˆ Ë†Î› A regarding a reference point ğ’“, i.e.,

(cid:216)

HVğ’“ (A) := ğœ‡ (cid:169)
(cid:173)
ğ€ğ‘ âˆˆ Ë†Î›A
(cid:171)

,

domHCğ’“ (ğ€ğ‘)(cid:170)
(cid:174)
(cid:172)

where ğœ‡ is the Lebesgue measure and the dominated hypercube

domHCğ’“ (ğ€ğ‘) := {ğ’– âˆˆ â„ğ‘š | ğ‘ğ‘– (ğ€ğ‘) â‰¤ ğ’–ğ‘– â‰¤ ğ’“ğ‘– âˆ€ğ‘– âˆˆ {1, . . . , ğ‘š}}.
HV is illustrated in Figure 3 (left). The hypervolume indicator is strictly Pareto compliant [269],

i.e., for all solution sets Ë†Î› A and Ë†Î› B, it holds that

B âŠ² A â‡’ HVğ’“ (B) < HVğ’“ (A).

Hence for the true Pareto front, the HV reaches its maximum. In practice, the nadir point, which is
constructed from the worst objective values, is often used as the reference point. For a guideline on
how to set a reference point, see [129]. Based on a quality indicator estimate, MOO strategies can

Fig. 3. The plot shows the hypervolume indicator (the area of the blue shaded region) regarding the reference
point (1, 1) (marked in red).

be evaluated over different benchmark datasets by using the Nemenyi post hoc test [71], where the
strategies are compared pairwise to find the best-performing one under a significance level of ğ›¼.

4 MULTI-OBJECTIVE OPTIMIZATION METHODS
This work in general and the following section in particular, focus on a posteriori methods, i.e.,
those that return a set of configurations Ë†P that tries to approximate the true Pareto set P as
well as possible. We will also explore some a priori methods, i.e., those that will only return one
configuration depending on preferences set before optimization. Finally, we will discuss integration
of user preferences to customize certain methods in Section 4.5.1. While a multitude of very specific
methods for MOO exist across various domains, we try to mainly focus on those that are actually
applied to MOHPO.

lllll0.000.250.500.751.000.000.250.500.751.00c1(l)c2(l)10

Karl and Pielok, et al.

4.1 Naive approaches

Scalarization. Scalarization transforms a multi-objective goal into a single-objective one,
4.1.1
i.e., it is a function ğ‘  : â„ğ‘š Ã— T â†’ â„ that maps ğ‘š criteria to a single criterion to be optimized,
configured by scalarization hyperparameters ğœ âˆˆ T . Having only one objective often simplifies
the optimization problem [192]. However, there are two main drawbacks to using scalarization for
MOO [137]: The scalarization hyperparameters ğœ must be chosen sensibly, such that the single-
objective represents the desired relationship between the multiple criteria â€“ which is not trivial,
especially without extensive prior knowledge of the optimization problem. We will outline three
popular scalariztaion techniques:

Weighted sum approach. One of the most well-known scalarization techniques, where one looks

for the optimal solution to:

ğ‘š
âˆ‘ï¸

ğ‘–=1

min
ğ€ âˆˆ ËœÎ›

ğœğ‘–ğ‘ğ‘– (ğ€).

(7)

It can be shown [77] that for a solution Ë†ğ€ of (7), it holds that if

ğœğ‘– â‰¥ 0,

(8)
Additionally, it holds for convex ËœÎ› and convex functions ğ‘ğ‘–, ğ‘– = 1, . . . , ğ‘š that for every non-
dominated solution Ë†ğ€ there exist ğœğ‘– â‰¥ 0,
ğ‘– = 1, . . . , ğ‘š, such that Ë†ğ€ is the respective solution
of (7); however the weighted sum might perform poorly for nonconvex problems [77].

ğ‘– = 1, . . . , ğ‘š â‡’ Ë†ğ€ is non-dominated in ËœÎ›.

Tchebycheff approach. The weighted Tchebycheff problem is formulated as:

min
ğ€ âˆˆ ËœÎ›

max
ğ‘–=1,...,ğ‘š

[ğœğ‘– |ğ‘ğ‘– (ğ€) âˆ’ ğ‘§âˆ—

ğ‘– |],

(9)

ğ‘– = ğ‘šğ‘–ğ‘›

where ğ‘§âˆ—
ğ€ âˆˆ ËœÎ›ğ‘ğ‘– (ğ€) ğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, . . . , ğ‘š defines the (ideal) reference point. For every optimal
solution ğ€âˆ— there exists one combination of weights Ë†ğœ, so that ğ€âˆ— is the optimal solution to the
optimization problem 9 defined through Ë†ğœ.

ğœ–-constraint approach. As outlined in Section 3.1, in some instances it might be feasible to
formulate one or more of the objectives as constraints. A MOO problem can also be reduced to a
single scalar optimization problem by turning all but one objective constraints, as in the ğœ–-constraint
method: Given ğ‘š âˆ’ 1 constants (ğœ–2, . . . , ğœ–ğ‘š) âˆˆ Rğ‘šâˆ’1,

ğ‘“1(ğ€), subject to ğ‘“2(ğ€) â‰¤ ğœ–1, . . . , ğ‘“ğ‘š (ğ€) â‰¤ ğœ–ğ‘š.

(10)

min
ğ€ âˆˆ ËœÎ›

However, much like parameters in the weighted sum method, constraints must be sensibly chosen,
which can prove to be challenging a priori and without sufficient domain knowledge.

These techniques (e.g., weighted sum method, Tchebycheff method) are the same ones used in
the MOEA/D Evolutionary Algorithm as outlined in Section 4.2.3 or the BO technique ParEGO as
outlined in Section 4.3.2, where several scalar optimization problems are created in place of the
multi-objective one.

4.1.2 Random and grid search. Random and grid search are very basic and robust algorithms for
single-objective HPO [30]. Random search is generally preferred, as it is an anytime algorithm
and scales better in the case of low effective dimensionality of an HPO problem with low-impact
parameters [26]; it often provides a surprisingly competitive baseline. Modifying random and grid
search for MOO is trivial: as all points are independently spawned and evaluated, one simply returns
all non-dominated solutions from the archive. Random and grid search can serve as reasonable

Multi-Objective Hyperparameter Optimization â€“ An Overview

11

Fig. 4. Basic loop of an evolutionary algorithm.

baselines when introducing more sophisticated optimization methods - similar to the single-
objective case. This procedure is adopted in a number of MOHPO works [128, 207, 230]

4.2 Evolutionary algorithms
Evolutionary algorithms (EAs) are general black-box optimization heuristics inspired by principles
of natural evolution. This section provides a brief introduction, followed by some prominent
examples of multi-objective EAs. EAs haven been used in HPO and AutoML applications, such as
in the popular AutoML framework TPOT [201]. Swarm Intelligence methods are related to and
share many of the advantages and disadvantages of EAs. They can be utilized for multi-objective
optimization [89] and have been examined for MOHPO and feature selection in select works
(e.g., Xue et al. [260], Bacanin et al. [13]).

Fundamentals of evolutionary algorithms. Evolutionary algorithms (EAs) are population-
4.2.1
based, randomized meta-heuristics, inspired by principles of natural evolution. Historically, different
variants have been independently developed such as genetic algorithms, evolution strategies and
evolutionary programming. These are now generally subsumed under the term â€œevolutionary
algorithm". EAs start by (often randomly) initializing a population of solutions and evaluating them.
Then, in every iteration (often called generation), the better solutions are (often probabilistically)
selected as parents and used to generate new solutions, so-called offspring. The two main operators
to generate new solutions are crossover, which tries to recombine information from two parents into
an offspring, and mutation, which randomly perturbs a solution. The resulting offspring solutions
are then evaluated and inserted into the population. Since the population size is kept constant, some
solutions have to be removed (survival selection), and once again usually the better solutions are
chosen to survive with a higher probability. If the stopping criterion reached, the best encountered
solution is returned, otherwise the next iteration starts. A schematic overview of a generation
can be found in Figure 4. EAs are popular optimization methods for the following reasons: (i) no
specific domain knowledge is necessary (at least for baseline results) [3], (ii) ease of implementa-
tion [3], (iii) low likelihood of becoming trapped in local minima [252], (iv) general robustness and
flexibility [101], and (v) straightforward parallelization [3]. They are widely employed in practice
and known for successfully dealing with complex problems and complex search spaces where other
optimizers may fail [109]. Furthermore, because they are population-based, EAs are particularly

PopulationParentsOffspringSelectionCrossover & MutationInsertionGeneration12

Karl and Pielok, et al.

well suited for multi-objective optimization, as they can simultaneously search for a set of solutions
that approximate the Pareto front.

4.2.2 Multi-objective evolutionary algorithms (MOEAs). When applying EAs to a multi-objective
problem, the only component that needs changing is the selection step (selecting parents and
selecting survivors to the next generation). Whereas in single-objective optimization the objective
function can be used to rank individuals and select the better ones, as discussed in Section 3.2,
in multi-objective optimization there are many Pareto-optimal solutions with different trade-offs
between the objectives, and we are interested in finding a good approximation of the Pareto front.
Recent works categorize MOEAs into three classes [80]:

â€¢ Pareto dominance-based algorithms use two-levels for ranking: On the first level, Pareto
dominance is used for a coarse ranking (usually non-dominated sorting, see below), while on
the second level usually some sort of diversity measure is used to refine the ranking of the
first level.

â€¢ Decomposition-based algorithms utilize scalarization (see Section 4.1.1) to decompose the
original problem into a number of single-objective subproblems with different parametriza-
tions, which are then solved simultaneously.

â€¢ Indicator-based algorithms use only a single metric, such as the hypervolume indicator, and

selection is governed by a solutionâ€™s marginal contribution to the indicator.

In the following, we will provide a prominent example for each category.

4.2.3 Prominent MOEAs.

NSGA-II (Pareto dominance-based). The non-dominated sorting genetic algorithm (NSGA-II) [68]
is still one of the most popular MOEAs. In many benchmark studies in the field, it serves as a
popular baseline [207, 252]. Being Pareto dominance based, it first uses non-dominated sorting to
obtain an initial coarse ranking of the population. This iteratively determines the non-dominated
solutions, assigns them to the next available class, and removes them from consideration. Among
the solutions in each obtained class, the extreme solutions (best in each objective) are ranked
highest, and the remaining solutions are ranked according to the crowding distance, the sum of
differences between an individualâ€™s left and right neighbor, in each objective, where large crowding
distances are preferred. While this works very well for problems with two objectives, it breaks down
in case of a larger number of objectives, as the non-dominated sorting becomes less discriminative,
and the left and right neighbor in each objective are often different solutions. NSGA-III [67, 131]
has been developed as an alternative for problems with a higher number of objectives, but shares
many similarities with decomposition-based algorithms.

MOEA/D (Decomposition-based). The Multi-objective Evolutionary Algorithm based on Decom-
position (MOEA/D) decomposes the multi-objective problem into a finite number ğ‘ of scalar
optimization problems that are then optimized simultaneously [266]. Each single optimization
problem usually uses a Tschebyscheff scalarization, see Section 4.1.1. In theory, each solution to
such a scalar problem should be a point on the Pareto front of the original multi-objective problem.
The distribution of solutions on the Pareto front is thus governed by the set of scalarizations chosen,
and it is challenging to identify scalarizations without a good knowledge of the Pareto frontier.
Rather than solving the different scalarized problems independently, the idea is to solve them
simultaneously, and allow the different search processes to influence each other. In a nutshell,
the population comprises of the best solution found so far for each of the sub-problems. In every
generation, a new offspring is created for each sub-problem by randomly selecting two parents
from the sub-problemâ€™s neighborhood, performing crossover and mutation, and re-inserting the

Multi-Objective Hyperparameter Optimization â€“ An Overview

13

individual into the population. The new individual replaces all individuals in the population for
which it is better with respect to the corresponding sub-problem. In effect, this means mating is
restricted to among individuals from the same region of the non-dominated frontier, and diversity
in the population is maintained implicitly by the definition of the different sub-problems.

SMS-EMOA (Indicator-based). The S metric selection evolutionary multi-objective optimization
algorithm (SMS-EMOA) [79] also uses the non-dominated sorting algorithm from NSGA-II for an
initial coarse ranking, but then uses an individualâ€™s marginal hypervolume contribution for as a
secondary criterion. The marginal hypervolume of an individual ğ‘– is the difference in hypervolume
between the population R including individual ğ‘–, and excluding individual ğ‘–:

Î”ğ»ğ‘‰ (ğ‘–, R) := ğ»ğ‘‰ (R) âˆ’ ğ»ğ‘‰ (R \ ğ‘–)
Different from the other two algorithms above, SMS-EMOA only produces one offspring per
generation, adds it to the population and then discards the worst solution based on the ranking
just described.

(11)

For more details on the above MOEAs, different MOEAs, and according applications, please refer
to Coello et al. [56], Abraham and Jain [3], Deb [65], and Branke et al. [39]. The main disadvantage of
many evolutionary algorithms is their relatively slow convergence (compared to other optimization
methods) and the need for many evaluations; for more computationally expensive ML problems, a
multi-objective HPO can become very costly when tackling it with these methods [65]. To alleviate
this problems, MOEAs have been combined with surrogate-modelling techniques (e.g.,[181]) or
gradient-based local search (e.g., [158]). EAs can further be found in several multi-objective AutoML
solutions, such as TPOT [200] or FEDOT [216].

4.2.4 Relevant software and implementations. Two very established packages which offer EMOAs
are PlatEMO3 and pymoo4. They both offer a wide range of EMOAs and can be generally recom-
mended. The mle-hyperopt5 package offers directly MOHPO via NSGA-II using internally the
nevergrad6 package.

4.3 Model-based optimization
4.3.1 Bayesian Optimization. In the following, the basic concepts of Bayesian optimization (BO)
are shown. For more detailed information, see [30]. BO has become increasingly popular as a global
optimization technique for expensive black-box functions, and specifically for HPO [125, 139, 239].
BO is an iterative algorithm with the key strategy of modelling the mapping ğ€ â†¦â†’ ğ‘ (ğ€) based on
observed performance values found in the archive A via (non-linear) regression. This approximating
model is called a surrogate model. Typical choices for the surrogate model are Gaussian processes
(GPs) or random forests. BO starts on an archive A of evaluated configurations, typically sampled
randomly, using e.g., Latin Hypercube Sampling or Sobol sampling [34]. BO then uses the archive to
fit the surrogate model, which for each ğ€ produces both an estimate of performance Ë†ğ‘ (ğ€) as well as
an estimate of prediction uncertainty Ë†ğœ (ğ€), which then gives rise to a predictive distribution for each
possible HPC. Based on the predictive distribution, BO computes a cheap-to-evaluate acquisition
function ğ‘¢ (ğ€) that encodes a trade-off between exploitation and exploration where exploitation
favours solutions with high predicted performance, while exploration favours solutions with high
uncertainty of the surrogate model because the surrounding area has not been explored sufficiently
yet. Instead of directly looking for the optimum of the expensive objective, the acquisition function

3https://github.com/BIMK/PlatEMO
4https://github.com/anyoptimization/pymoo
5https://github.com/mle-infrastructure/mle-hyperopt
6https://github.com/facebookresearch/nevergrad

14

Karl and Pielok, et al.

ğ‘¢ (ğ€) is optimized in order to identify a new candidate ğ€+ for evaluation. The true objective value
ğ‘ (ğ€+) of the proposed HPC ğ€+ â€“ generated by optimization of ğ‘¢ (ğ€) â€“ is finally evaluated and added
to the archive A. The surrogate model is updated, and BO iterates until a predefined termination
criterion is reached.

Simple acquisition functions. A very popular acquisition function is the expected improvement (EI)
[139]). EI was introduced in connection with GPs that have a Bayesian interpretation, expressing
the posterior distribution of the true performance value given already observed values as a Gaussian
random variable ğ¶ (ğ€) with ğ¶ (ğ€) âˆ¼ N ( Ë†ğ‘ (ğ€), Ë†ğœ (ğ€)2). A further, very simple acquisition function is
the lower confidence bound (LCB) [138]. The LCB treats local uncertainty as an additive bonus at
each ğ€ to enforce exploration, which can be controlled with a control parameter ğœ….
4.3.2 Multi-objective Bayesian Optimization. The previously presented BO framework can be
extended to simultaneously optimize a set of possibly conflicting black-boxes. In particular, the
multi-objective extensions to the BO framework (MO-BO) can be categorized into two categories
as shown in Figure 5.

Surrogate based
on scalarization

ParEGO

. . .

Multi-objective model-
based optimization

Surrogate for each output

Aggregating
Acquisition Function

Multiple
Acquisition Functions

Information-Theoretic

SMS-EGO

Multi-EGO

PESMO

ğœ–-EGO

MOEA/D-EGO

MESMO

. . .

. . .

EHI

. . .

Fig. 5. Different multi-objective model-based optimization approaches.

A simple approach is to create a new objective by scalarizing the multi-dimensional output of
ğ‘ (ğ€) to a single value (see Section 4.1.1). Some MO-BO methods utilize one or more scalarizations of
the MOO problem to fit surrogates in order to approximate the Pareto front. Algorithms that do not
use scalarization of the outcomes instead train independent surrogates for each output dimension.
By having a prediction for each output dimension, we can either obtain an acquisition function for
each dimension and use a multi-objective optimizer to obtain a set of promising configurations,
or we can build an acquisition function that aggregates the predictions for each dimension into
a single-objective acquisition function. In the following, we discuss some of these approaches in
more detail.

Scalarization and ParEGO . ParEGO is a scalarization-based extension of BO to MOO problems
proposed by Knowles [149]. This method is built on the assumption that the Pareto front can be
approximated by solving a number of single-objective problems that are scalarized variants of
the ğ‘š objective functions (see Section 4.1.1). In ParEGO, we proceed as follows: First, we must

Multi-Objective Hyperparameter Optimization â€“ An Overview

15

determine a set of scalarization weights that will be used throughout all BO-iterations. This set of
weights should ensure that the Pareto front is explored evenly. Therefore, we create the set of all
possible weight vectors using the following rule:

(cid:40)

ğœ¶ = (ğ›¼1, ğ›¼2, ..., ğ›¼ğ‘š)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘š
âˆ‘ï¸

ğ‘—=1

ğ›¼ ğ‘— = 1 âˆ§ ğ›¼ ğ‘— =

(cid:41)

, ğ‘™ âˆˆ {0, 1, ..., ğ‘ }

,

ğ‘™
ğ‘ 

(12)

which generates (cid:0)ğ‘ +ğ‘šâˆ’1
(cid:1) different weight vectors. Second, the output space of each of our ğ‘š
ğ‘šâˆ’1
objectives is normalized to [0, 1]. Finally, in each iteration of ParEGO, we create the scalarized
objective using the so-called augmented Tchebycheff function [149]:
(cid:0)ğ›¼ ğ‘—ğ‘ ğ‘— (ğ€)(cid:1) + ğœŒ [ğœ¶ Â· ğ‘ (ğ€)],

ğ‘ğœ¶ (ğ€) = max

(13)

ğ‘— âˆˆ {1,...,ğ‘š }

where ğœŒ is a small positive constant and ğœ¶ is a weight vector drawn uniformly from the set in (12).
Instead of only using the linear second term, the first term with the Tschebycheff norm is added
to ensure that we are able to find a solution in the non-convex parts of the Pareto front. In each
iteration, a surrogate model is fitted on the design with scalarized outcomes (cid:110)(cid:16)
(cid:17)(cid:111)
,
ğ‘–
and the EI is optimized on this model to propose a new HPC. ParEGO can easily be extended to
parallel batch proposals. In order to propose a batch of size ğ‘, Horn et al. [116] suggest to sample
ğ‘ different weight vectors per iteration in a stratified manner. New design points are proposed
by fitting ğ‘ surrogates to the ğ‘ differently scalarized outcomes in the design and optimizing the
acquisition function on each of the ğ‘ surrogate models in parallel. One concern with ParEGO and
scalarizing-based solutions in general is that the uniformly sampled weights do not necessarily
result in the best distribution of nondominated points. An advantage of ParEGO is that it can be
easily adapted to focus the search on one objective by limiting the maximum weights of the others.

ğ€ (ğ‘–), ğ‘ğœ¶ (ğ€ (ğ‘–) )

EHI. (or EHVI) Emmerich et al. [82] propose to use the expected improvement over the S-
metric (i.e., hypervolume) as an acquisition function for MO-BO. Here, a surrogate model for each
objective is fitted individually. The EHI is then calculated as the expectation of the hypervolume
improvement over the distribution of outcomes as predicted by the surrogate models. A drawback
of hypervolume-based BO is that a non-trivial multidimensional integral must be evaluated to
calculate the expectation of hypervolume improvement. It is possible to use Monte-Carlo-based
approximations [81], and the KMAC method [262] to efficiently calculate the EHI criterion with
complexity ğ‘‚ (ğ‘› log ğ‘›) in three dimensions and ğ‘‚ (ğ‘› âŒŠğ‘š/2âŒ‹) in ğ‘š dimensions. Additionally, Emmerich
et al. [83] propose a more efficient means of calculating the EHI with a complexity of ğ‘‚ (ğ‘› log ğ‘›) for
ğ‘š = 2. However, these methods are significantly more complex than the other presented MO-BO
infill criteria. To obtain batch-proposals, Yang et al. [263] propose dividing the objective space into
several sub-objective spaces and then search for the optimal solutions in each sub-objective space
by using a truncated EHI.

SMS-EGO. The S-Metric Selection-based Efficient Global Optimization (SMS-EGO) algorithm
is another popular extension of MO-BO. This method was proposed by Ponweiser et al. [217]
and extends the idea of the EHI by employing an infill criterion: In each BO iteration, SMS-EGO
approximates each of the ğ‘š objectives with a separate surrogate model. For each objective, we
compute the LCB and denote the resulting ğ‘š-dimensional outcome with ğ’–LCB. The desirability of
a configuration ğ€ is derived from the increment of the dominated hypervolume when ğ’–LCB(ğ€) is
added to the current Pareto front approximation Ë†P:

ğ‘¢SMS(ğ€) = HVğ’“

(cid:16) Ë†P âˆª ğ’–LCB (ğ€)

(cid:17)

âˆ’ HVğ’“ ( Ë†P) âˆ’ ğ‘,

(14)

16

Karl and Pielok, et al.

with a penalty ğ‘ and with the reference point ğ’“ chosen as max( Ë†P) + 1ğ‘š (under the assumption
that all objectives are to be minimized). The penalty ğ‘ increases the worse ğ‘ (ğ€) is compared to the
current Pareto front. To prevent the selection of configurations close to Ë†P, SMS-EGO assigns also a
penalty to points within an ğœ–-range of Ë†P. However, the penalty is in this case only influenced by the
objectives inferior to the approximated Pareto front. If ğ’–LCB (ğ€) is a dominated solution, ğ‘¢SMS (ğ€)
would be zero without the penalty term ğ‘, making the optimization of the acquisition function
more challenging. Therefore, to guide the search towards non-dominated solutions in areas of
dominated solutions, a penalty ğ‘ is added for each point that dominates the solution candidate.
Otherwise, if ğ’–LCB (ğ€) is a non-dominated solution, the penalty is zero.

Multi-EGO. In each BO iteration, Multi-EGO approximates each of the ğ‘š objectives with a
separate surrogate model from which a single-objective acquisition function is obtained. Jeong
and Obayashi [133] use Gaussian processes for the surrogates and EI as the individual acquisition
functions. The ğ‘š acquisition functions are optimized jointly as an MOO problem itself using a
multi-objective genetic algorithm (in principal any MOEA can be used here), resulting in a set of
candidates with non-dominated acquisition function values. From this set, Multi-EGO then selects
multiple points to be evaluated. This naturally lends to parallelization, as there are always multiple
proposals generated in each iteration.

MESMO and PESMO. Common information-theoretic acquisition functions are multi-objective
maximum entropy search (MESMO) and predictive entropy searc (PESMO) proposed by Belakaria
et al. [21] and HernÃ¡ndez-Lobato et al. [110] respectively. Information theoretic multi-objective
acquisition functions model each black-box with an independent surrogate model. The resulting
Pareto set is modelled as a random variable. Hence, we can compute the entropy of the location
of the Pareto set. The lower this entropy is, the more we know about the location of the Pareto
set. These acquisition functions represent the expected reduction of entropy if a point is evaluated.
Information theoretic acquisition functions are linear combinations of the expected entropy of the
whole predictive distributions of every black-box. Hence, the utility of every point represents a
global measure of the uncertainty of the input space and not a local and heuristic measure of the
particular point, as in other acquisition functions. Thus, in theory they should explore the search
space more efficiently [21, 110].

4.3.3 Extensions and open challenges of Bayesian Optimization. So far, we have seen the classical
multi-objective scenario where a set of black-box functions is simultaneously optimized. However,
there are extensions of BO to a wider range of MOO scenarios. MOO scenarios can, for example,
be constrained. Such constraints, as mentioned in the previous section, might also be black-boxes
that can be approximated by surrogate models. An example of such a scenario is the simultaneous
optimization of the prediction error and time of prediction of a deep neural network constrained to
some particular physical storage size required for its implementation on a chip. Another interesting
and unsolved problem is to model the dependencies of various black-boxes. Intuitively, if expert
knowledge suggests that there is a correlation between the black-boxes, we could infer the value
of one black box by knowing values of another black-box. For example, if we are simultaneously
optimizing the prediction error and prediction speed of a deep neural network, we can hypothesize
that a negative correlation exists between them. By knowing a result with a low prediction error, it
may be likely that prediction speed is slow. This could possibly allow forgoing evaluation of that
black-box, especially if it will incur a significant loss of computational (or other) resources. Modeling
independent surrogates for every black-box does not take into account potential correlations. We
could model these dependencies by using a multi-output GP and a specific acquisition function that
considers the information computed by the multi-output GP [172, 196]. Furthermore, the structure

Multi-Objective Hyperparameter Optimization â€“ An Overview

17

of the search space of HPO problems raises challenges to be addressed by optimizers. In particular
in AutoML and NAS, the search spaces are often characterized by high dimensionality, the presence
of both numerical and categorical variables, and hierarchies between hyperparameters. Modeling
the relationship between HPs and model performance is particularly challenging in such scenarios.
These issues have been increasingly discussed for single-objective HPO. To efficiently optimize
over high-dimensional spaces, Wang et al. [257] propose random embeddings. To accommodate
mixed-hierarchical spaces, there are several approaches reaching from GPs with special kernel
functions [162] to other model classes that are inherently capable of representing mixed-hierarchical
hyperparameters by linear models [64], random forests [125], or kernel density estimation [25].
These challenges equally apply to the MOO setting, and need to be addressed accordingly in future
research. Modelling the causal relations of the input space variables in MOO potentially reduces the
size of the input space [4]. Some optimization problems involve a mix of expensive black-boxes and
cheap ones. In order to solve these scenarios, a hybrid methodology between decoupled MO-BO
and metaheuristics can be suggested. In particular, cheaper black-boxes could be optimized with
metaheuristics and more expensive ones with BO. The noise of the black-boxes can also vary across
the input space. Other ideas are to extend automatic BO, i.e., approaches which try to adapt the
control parameters of BO itself automatically, to the multi-objective setting [186] or implement
asynchronous multi-objective BO to never leave any black-box or resource idle. Finally, non-myopic
BO can be extended to the multi-objective setting [134].

4.3.4 Relevant software and implementations. Table 1 compares BO frameworks with multi objective
capabilities. We identified as suitable well-established frameworks Dragonfly [142], HyperMap-
per [197], Openbox [168], Ax7, trieste8, BoTorch [15] and GPflowOpt [152]. We checked if they offer
scalarization-based approaches (e.g., ParEGO), EHVI-based approaches (e.g., EHVI, QNEHVI) or
MESMO, and if they can handle constraints and use computational resources in parallel.

Table 1. BO frameworks which offer MO support

Dragonfly HyperMapper OpenBox Ax trieste BoTorch GPflowOpt

Scalarization-based
EHVI-based
MESMO
Constraints
Parallel

âœ“
Ã—
Ã—
Ã—
âœ“

âœ“
Ã—
Ã—
âœ“
âœ“

âœ“
âœ“
âœ“
âœ“
âœ“

âœ“
âœ“
Ã—
âœ“
âœ“

âœ“
âœ“
Ã—
âœ“
âœ“

âœ“
âœ“
Ã—
âœ“
âœ“

Ã—
âœ“
Ã—
Ã—
Ã—

4.4 Multi-fidelity optimization
Assuming the existence of cheaper approximate functions, multi-fidelity optimization is a popular
choice for expensive-to-evaluate black-box optimization target functions in many application
domains like aerodynamics [93] or industrial design [119]. These approximations tend to be noisy
or less accurate in general, but present a cheaper option to estimate an evaluation of the original
function, when a multitude of evaluations are infeasible due to the incurred computational cost.
Multi-fidelity optimization has been established for single-objective hyperparameter tuning [164]
and has become a desirable mode of optimization especially for deep learning models, that are
very costly to fully train and evaluate [85, 164, 165]. Instead of optimizing configuration selection,

7https://ax.dev/
8https://secondmind-labs.github.io/trieste/

18

Karl and Pielok, et al.

Fig. 6. Exemplary bracket run (figure inspired by Hutter et al. [126]). Faint lines represent future performance
of HPCs that were discarded early.

as in model-based optimization, multi-fidelity methods for HPO aim at optimizing configuration
evaluation, i.e., allocate resources to the different configurations chosen for evaluation in an efficient
manner. In contrast to other applications, where one might have access to only one or few fidelities
(e.g., one cheap computer simulation for an expensive aerodynamics optimization problem), ML
presents a continuum of fidelities to choose from. Typical examples for this are the number of
epochs a deep learning model is trained, how much training data is provided for model training or
resolution of input images in computer vision. The idea is to allocate more resources (i.e., additional
epochs, increased training data etc.) to promising configurations while discarding worse performing
configurations without sacrificing a lot of resources. This is achieved by starting all configurations
on a lower fidelity (i.e., less resources) and then promoting only the well-performing configurations
to a higher fidelity. This idea is shown in Figure 6. Essentially, multi-fidelity approaches for single-
criteria HPO were originally intended to build on top of the already quite competitive random
search with improved resource allocation. These approaches have since been enhanced by the
use of model-based optimization for drawing configurations instead of random sampling [85] and
asynchronous execution [148]. The same holds true in the multi-objective setting as recent works
have shown [230, 231]. Multi-fidelity extensions like Hyperband can be carried over to the MO
setting by simply defining a suitable performance indicator to decide where increased resource
allocation is desirable, which could be achieved in numerous ways. Existing approaches have done
this via scalarization using random weights [230] or non-dominated sorting [227, 231]. Taking
multi-fidelity one step further, it can also be combined with BO, which remains one of the open

Multi-Objective Hyperparameter Optimization â€“ An Overview

19

challenges for MOO: This has been done in single-objective HPO, where configurations are no
longer sampled randomly (as in traditional Hyperband), but via BO [85]. Therefore, instead of
enhancing random search, multi-fidelity methods are used to enhance BO in this case [85].

4.5 Further issues

Focusing optimization through user preferences. A growing body of literature is exploring
4.5.1
the integration of decision maker (DM) preferences into multi-objective optimization [36, 256].
Depending on when the user preferences are elicited relative to the optimization process, these
methods are generally divided into a priori (before optimization), progressive (during optimization),
and a posteriori (after optimization) approaches [37]. The majority of literature on multi-objective
optimization aims to find a good approximation of the entire Pareto front, providing the decision
maker with a variety of alternatives to choose from after optimization, so falls into the a posteriori
category. There are, however, at last three reasons for taking preference information into account
earlier [36]:

(1) It will allow to provide the DM with a more relevant sample of Pareto optimal alternatives.
This could either be a smaller set of only the most relevant (to the DM) alternatives, or a
more fine-grained resolution of the most relevant parts of the Pareto frontier.

(2) By focusing the search onto the relevant part of the search space, we expect the optimization
algorithm to find these solutions more quickly. This is particularly important in computa-
tionally expensive applications such as hyperparameter optimization.

(3) As the number of objectives increases, it becomes more and more difficult to create an
approximation to the complete Pareto optimal frontier. This is partly because of the increasing
number of Pareto optimal solutions, but also because with an increasing number of objectives,
almost all solutions in a random sample of solutions become non-dominated [130], rendering
dominance as selection criterion less useful. DM preference information can re-introduce the
necessary order relation.

Several ways to specify preferences have been proposed, including reference points (an "ideal"
solution), constraints (minimum acceptable qualities for each objective), maximal/minimal trade-
offs (how much is the DM willing to sacrifice at most in one criterion to improve another criterion
by one unit) and desirability functions (non-linear scaling of each objective to [0, 1]). Rather than
asking the DM to specify preferences explicitly, preferences can also be learned [95] by asking the
DM to rank pairs or small sets of solutions, or to pick the most preferred solution from a set. This
has the advantage that the DM just has to compare solutions, which is something they should be
comfortable doing. Finally, people have observed that DMs often select a solution from the Pareto
frontier that â€œsticks out" in the sense that improving it slightly in either objective would lead to a
significant deterioration in the other. These solutions are often called "knees", and it is possible to
specifically search for them without having to ask the DM anything (e.g., [38]).

4.5.2 Noisy environments. If an MOO problem is noisy, we do not have direct access to the objective
values ğ‘1, . . . , ğ‘ğ‘š, but instead we only have measurements Ëœğ‘1, . . . , Ëœğ‘ğ‘š with

Ëœğ‘ğ‘– = ğ‘ğ‘– + ğœ–ğ‘– âˆ€ğ‘– = 1, . . . , ğ‘š
where ğœ–ğ‘– is the observational noise in the ğ‘–-th objective modeled as a random variable. Noise in
the evaluation plays a major role also in most MOHPO problems, e.g., because generalization
error estimates in Eq. (2) are based on a finite dataset sampled from a much larger universe, the
random sequence of the data as it is presented during training has an impact on the performance,
or a stochastic optimizer is used during training. Clearly, noise is challenging for optimization,
whether single-objective or multi-objective. It may lead to false performance comparisons such

(15)

20

Karl and Pielok, et al.

as for example an incorrectly inferred dominance relationship between two HPCs. This may
mean some dominated solutions are classified as non-dominated and thus incorrectly returned
by the algorithm, while some Pareto-optimal solutions are discarded incorrectly because they
are perceived as being dominated. It may also lead to an over-optimistic estimate of the Pareto
frontier, as most solutions returned as non-dominated were â€œlucky" in their evaluation. One simple
way to reduce the effect of noise is to evaluate each solution multiple times and optimize based
on mean values. While this reduces the standard error, it is computationally very expensive
and may be impractical for HPO. Researchers in the evolutionary computation community have
developed a wealth of methods to cope with noisy evaluations, including the use of statistical tests
(e.g., Syberfeldt et al. [243], Park and Ryu [205]), the use of surrogate models (e.g., Branke et al. [40]),
probabilistic dominance [122], or integrating statistical ranking and selection techniques [161]. A
relatively simple yet effective method seems to be the rolling tide EA [91] which alternates between
sampling new candidates and refining the archive, i.e., re-evaluating promising HPCs, in each
optimization iteration. For BO, the noise can be accounted for by using re-interpolation [153] or
in a straightforward way by choosing GP regression (rather than interpolation) and appropriate
acquisition functions, see, e.g., Astudillo and Frazier [11], Daulton et al. [62], HernÃ¡ndez-Lobato
et al. [110], Horn et al. [115], Knowles et al. [150], Rojas Gonzalez et al. [223], Rojas-Gonzalez
and Van Nieuwenhuyse [224]. Also heteroscedastic models to deal with input space noise in the
presence of several objectives like the models presented in Villacampa-Calvo et al. [253] can be
used for BO. A recent survey on multi-objective optimization methods under noise with a provable
convergence to a local non-dominated set has been provided by Hunter et al. [123], older surveys
with a somewhat broader scope can be found in Gutjahr and Pichler [103], Jin and Branke [136].

4.5.3 Realistic evaluation of multi-objective optimization methods. When applying MOHPO in
a real world setting it is crucial to understand how a solution will behave on unseen data after
deployment. In standard HPO this is achieved via a 3-way split of the data in train, validation and
test sets, or, more generally, as nested resampling (c.f. Section 3). In Section 3.3.2 different measures
for the quality of multi-objective optimization have been introduced. While these measures are
generally useful to evaluate the performance over the whole objective space, decision makers are
ultimately only interested in a single solution (selected from the Pareto set) for deployment and
later use. This usually implies a human-in-the-loop. For a single train/validation/test split, this is
easily achievable: The decision maker needs to look at the Pareto front computed on the validation
set, choose the configuration they would like to use and then evaluate its performance on the
test set. Extending this approach to nested resampling, multiple Pareto fronts are generated, one
for each outer fold. Here, for a drill-down to to a single solution would imply that the decision
maker needs to make these choices for each outer loop, which can become impractical and can
make larger benchmark studies difficult to conduct efficiently. What can be implemented in an
automatic fashion, is the evaluation of each generated Pareto front on its associated outer test
set in an unbiased fashion. Here, similarly as in nested resampling in single objective HPO, each
Pareto set candidate would be trained on the joint training and validation set, and evaluated on the
test set. This results in a new unbiased Pareto front, for each outer iteration, and measures like
hypervolume can also be calculated from the outer results in an unbiased fashion. Furthermore,
attainment surfaces [92] have been introduced to study the performance of general multi-objective
noisy optimization by visualizing the Pareto fronts of multiple runs simultaneously. They can be
used to either visualize the set of all returned fronts evaluated on the validation sets, or, as described
above, on the outer test sets. In general, proper MOHPO evaluation is understudied and an open
challenge for further research.

Multi-Objective Hyperparameter Optimization â€“ An Overview

21

4.6 Relevant benchmarks and results
In general, relevant benchmarks comparing different multi-objective optimizers for HPO mostly
were conducted in the context of new optimizers being proposed (see, e.g., Guerrero-Viu et al. 99,
HernÃ¡ndez-Lobato et al. 110, Schmucker et al. 230). We start by giving a brief summary of interesting
findings in the literature. The experiments conducted in Horn [113] reveal the weaknesses of
utilizing grid search in the context of multi-objective HPO when tuning hyperparameters of
an SVM for binary classification (classification error and training time as targets) compared to
sequential model-based optimization techniques in the sense that grid search fails to provide
a good Pareto front approximation. Random search on the contrary has shown good results
and even outperformed some model-based optimization methods (i.e., ParEGO, SMS-EGO and
PESMO) when applied to a fairness-related multi-objective HPO task [230]. While an exhaustive
benchmark comparing the performance of random and grid search on multi-objective HPO problems
is missing at this point in time, random search can generally be recommended as a baseline when
conducting experiments [230]. Horn and Bischl [114] found that ParEGO and SMS-EGO were able
to outperform an NSGA-II variant and latin hypercube-sampling when tuning over multiple models
with hierarchical structure in binary classification tasks (9 datasets) where the objectives have
been FPR and FNR. In Horn et al. [115] the authors investigated the performance of noise-resistant
SMS-EGO variants, the rolling tide EA (RTEA) and approaches based on random search while
tuning an SVM on 9 binary classification tasks with FPR and FNR as objectives. Here, RTEA and a
repeated variant of SMS-EGO were ranked best among all competitors. HernÃ¡ndez-Lobato et al.
[110] compared PESMO to EHVI, ParEGO and SMS-EGO when tuning the hyperparameters of
a feed-forward neural network on the MNIST dataset with classification error and prediction
time as objectives. Here, PESMO outperformed all competitors, especially if few evaluations were
available. Guerrero-Viu et al. [99] conducted a NAS+HPO benchmark tuning the architecture and
hyperparameters of a CNN on classification tasks using the Oxford Flowers and MNIST Fashion
datasets. They propose an EMOA extended by successive halving, a multi-objective BOHB variant,
an EHVI variant, a multi-objective extension of BANANAS [258] and a BULK & CUT optimizer
which first tries to find large and accurate models in the beginning and subsequently prunes
them. In general, the EHVI variant and BULK & CUT optimizer slightly outperformed the other
approaches [214]. Recent standardized HPO benchmark suites like HPOBench [76] and YAHPO
Gym [214] include some support for multi-objective use cases, which emphasizes the trend towards
MOHPO in the research community. In [214], the authors present YAHPO Gym, a benchmark suite
for HPO, and illustrate the potential of YAHPO Gymâ€™s MOHPO benchmark problems by comparing
seven multi-objective optimizers on the YAHPO-MO benchmark (v1.0), a collection of 25 MOHPO
problems given by tuning hyperparameters of an elastic net, random forest, gradient boosting tree,
decision tree, funnel-shaped MLP net, or an ML pipeline on various OpenML tasks. The number
of objectives ranges from two to four. Objectives are given by at least one performance metric
(e.g., accuracy), and interpretability metrics (such as the number of features used or the interaction
strength of features as introduced in Section 5) or metrics regarding computational efficiency such
as memory required to store the model. The optimization budget (number of evaluations) is scaled
with respect to the search space dimensionality and ranges from 77 to 267 objective evaluations.
[214] compare random search, random search (x4), ParEGO, SMS-EGO, EHVI, Multi-EGO and a
mixed integer evolution strategy (MIES; Li et al. 167) relying on Gaussian and discrete uniform
mutation and uniform crossover. All model-based optimizers use random forests as surrogate
models, for more details see [214]. Random search (x4) samples four configurations uniformly at
random (in parallel) at each optimizer iteration. Regarding performance evaluation the anytime
hypervolume indicator (computed on normalized objectives) was considered. Looking at mean ranks

22

Karl and Pielok, et al.

Application Domains

Prediction
performance

Computational
efficiency

Fairness

Interpretability

Robustness

Sparseness via
feature selection

ROC Analysis

Energy
Consumption

Equalized Odds

Sparseness

Distribution Shift

Number of
Features

Natural Language
Processing

Memory
Consumption

Equality of
Opportunity

Complexity of
Main Effects

Adversarial
Examples

Stability of
Feature Seletion

Object Detection

Prediction and
Training Time

Calibration

Interaction
Strength

Perturbations

. . .

FAT-ML related objectives

Fig. 7. Overview of application scenarios for MOHPO.

over all benchmark problems, random search (x4) outperforms all competitors after having used
25% of the optimization budget (which was expected due to random search (x4) having evaluated
four times more configurations in total compared to the competitors). However, with respect to
final performance, Multi-EGO, ParEGO and MIES are on par with or outperform random search
(x4), while EHVI or SMS-EGO fail to consistently outperform the vanilla random search. Looking at
each benchmark problem separately, results indicate strong performance differences of optimizers
with respect to the hypervolume indicator. Especially MIES performs exceptionally well on some
benchmark problems but only shows average performance on others. While this benchmark study
is a first step towards an exhaustive multi-objective benchmark, plenty of work still needs to be
done to be able to give general recommendations regarding when to use which multi-objective
optimizer.

5 OBJECTIVES AND APPLICATIONS
In the following, we will give an overview of relevant metrics for ML models, interesting use cases
and application domains for MOHPO. We organize this section by examining three aspects of ML
model evaluation:

Prediction performance. In most cases, prediction performance is of primary importance. Which
performance metric aligns best with the goals and costs associated with an ML task is not always
readily apparent, especially if misprediction costs are hard to quantify or even unknown. We discuss
the case where multiple performance metrics with unquantifiable trade-offs are relevant by the
example of ROC analysis in classification.

Computational efficiency. Computational efficiency is a prime example for the difficulty of finding
an appropriate metric. The desire for a memory or energy efficient model is often difficult to
operationalize because it is not as straightforward to measure model efficiency. We will examine
various possible ways and present a usecase where efficiency and prediction performance are
optimized simultaneously.

Fairness, interpretability, robustness and sparseness. Many applications areas require ML models
to fulfill higher standards than only high predictive performance. First, we explore Fairness and
Interpretability as objectives. We further examine Robustness of ML models to domain shift, per-
turbations and adversarial attacks as a prerequisite/objective, as robust models inspire a higher
degree of trust. Finally we will venture into the topic of sparse ML models, where the number of

Multi-Objective Hyperparameter Optimization â€“ An Overview

23

features is minimized in some manner. Sparseness itself is not necessarily a desirable quality, but
may be a suitable proxy for model complexity/interpretability, data acquisition cost and/or even
performance. The term FAT-ML (Fairness, Accountability and Transparency in Machine Learning)
has been coined and has gained some traction recently9. There is considerable overlap between the
objectives examined here and FAT-ML, but FAT-ML also entails external characteristics, such as
e.g., responsibility which is encompassed in the accountability aspect.

A visual representation of the remaining sections can be found in Figure 7. It should be noted
that interactions between different objectives are worth studying. Objectives may be positively
correlated, for example a model with high predictive accuracy might be expected to also perform
well measured in other performance metrics like AUC [49]. Similarly, a sparse model is expected to
be more efficient as well as more interpretable. Objectives might also be conflicting. A more complex
model might for example perform better in terms of accuracy, but might be less interpretable. It is
hard to quantify these relationships without a comprehensive benchmark, but we will try to shed
light on them wherever relevant.

5.1 Prediction performance
HPO has traditionally been focusing on optimizing for predictive performance measured by a single
performance metric. It can, however, also be beneficial to optimize multiple different prediction
metrics simultaneously; particularly, if a trade-off between different metrics cannot be specified
a priori. An abundance of prediction metrics have been defined for various ML tasks and the
appropriate choice depends on the specific use case and available data and we will therefore
not provide a comprehensive overview. Good examples for the diversity in metrics can be found
for classification, one of the most widely utilized ML tasks, in Figure 3.1 of Japkowicz and Shah
[132] and for time series forecasting, a much more specific application, in Figure 2.3 of Svetunkov
[242]. Different performance metrics may penalize prediction errors in distinct ways, and therefore
prediction metrics are more or less correlated [49, 268]. These correlations have been extensively
studied for some (widely used) metric pairs [58, 63, 121]. Here, as in general MOHPO, we argue it
may be of merit to apply MOHPO in a case where the user is interested in two or more, possibly
conflicting prediction metrics and/or trade-offs.

5.1.1 ROC analysis. Many binary classification models predict scores or probabilities that are
then converted to predicted classes by applying a decision threshold. Different decision threshold
values will result in different trade-offs between performance measures. A higher decision threshold
may for example reduce the number of false positives, but may also reduce the number of true
positives at the same time. This trade-off is often visualized by the receiver operating curve (ROC),
where true positive rate (TPR) is plotted against false positive rate (FPR) for different threshold
values. Similarly, the precision-recall curve visualizes TPR versus positive predicted value (PPV),
respectively. Because improving one classification metric by varying the decision threshold is
typically associated with deteriorating performance with respect to the other, choosing a decision
threshold based on a ROC analysis a fundamentally multi-objective problem, where the decision
threshold can be viewed as an additional hyperparameter. As the decision threshold has no impact
on the model itself, it can be optimized post-hoc and tuning is therefore fairly cheap [30]. An
approach to address this optimization problem is to formulate it as a single-objective problem by
e.g., aggregating elements of the confusion matrix (F-Measure) or the ROC curve (AUC) into a
single metric. As a result, by reducing the natural two-objective optimization problem to a single
one, one only optimizes a classifierâ€™s general ability to discriminate both classes. However, not

9See e.g., https://facctconference.org/ or https://www.fatml.org/.

24

Karl and Pielok, et al.

Fig. 8. Scheme showing two intersecting, non-dominant ROC curves with similar AUC in green and one
ROC curve that is fully dominated in blue. The resulting Pareto front is marked through a dashed black line.

all information is preserved. An example can be seen in Figure 8, where two ROC curves lead to
a similar AUC but present quite distinct shapes. It may be desirable to consider the ROC curves
for different HP configurations with different ROC curves for a final solution. One can follow the
approaches in e.g., LÃ©vesque et al. [163], Chatelain et al. [52], and Bernard et al. [27] and combine
the information from each iteration in one ROC front that displays all Pareto-optimal trade-offs
between TPR and FPR. This preserves all relevant information from individual evaluations and
allows for decisions in accordance to user- or case-specific preferences. In our example in Figure 8,
the final combined model would then show the same performance, i.e., trade-offs, as the dashed
black line. A similar approach has been introduced for regression: The Regression Error Character-
istic (REC) Curves, which examines trade-offs between error tolerance and percentage of points
within the tolerance [28].

Extending these ideas to a multi-class setting, where each class has unique associated misclas-
sification costs, brings new challenges. The number of different misclassification errors grows
quadratically with the number of classes ğ‘”, leading to increasingly high-dimensional surfaces.
Naturally, one wants to minimize all the different misclassification errors simultaneously. An
in-depth exploration of this challenge and how to solve it can be found in Everson and Fieldsend
[84]: The authors define the ROC surface for a ğ‘”-class classification problem and try to optimize for
the respective ğ‘”(ğ‘” âˆ’ 1) misclassification rates by an EA. Multi-class ROC problems are therefore
generally seen as a MOO problem and closely related to the core topic of this work [84, 90]. They
are generally solved through (i) single-model approaches, where one classifier is identified, and
once the costs are known at prediction time, a suitable trade-off on that classifierâ€™s ROC surface
is then found, or (ii) multi-model approaches that produce a pool of suitable classifiers that are
available at prediction time [27]. The optimal ROC surface can be viewed as a Pareto front; in this
case the search space is the space of classifiers, and every classifier corresponds to a ROC curve.
A classifier is non-dominated if any part of its ROC curve is non-dominated and the length of

Multi-Objective Hyperparameter Optimization â€“ An Overview

25

the non-dominated part of the ROC can be used as the crowding distance. It should be noted that
the principle of AUC as a metric of the classifierâ€™s general ability to discriminate both classes in
the binary case does not carry over to the multi-class setting [75]. Hand and Till [105] attempt to
generalize and adapt AUC from a binary setting to a meaningful metric for the multi-class setting.
In binary classification, the ROC curve is generated through varying the decision threshold, which
is quite cheap and can be done after each iteration in a hyperparameter optimization problem. In
a ğ‘”-class setting during prediction, the classifier will generally output a vector of probabilities or
some measure of confidence that a sample belongs to the corresponding class ğ‘ğ‘– Bernard et al. [27]:

â„(ğ‘¥) = [â„(ğ‘1|ğ‘¥), â„(ğ‘2|ğ‘¥), . . . , â„(ğ‘ğ‘” |ğ‘¥)]

To generate a decision rule, a vector of weights ğ‘¤ = (ğ‘¤1, ğ‘¤2, ..., ğ‘¤ğ‘”) is applied, and the highest
value is chosen. To obtain a ROC surface similar to the ROC curve in binary classification, a large
number of different weight vectors would have to be evaluated, which incurs larger additional
costs and makes it no longer trivial to obtain post-hoc for each configuration [27].

Applications and exemplary use case. An illustrative example is given by Chatelain et al. [52],
who attempt to identify well-performing configurations of SVMs in a binary classification setting
with unknown misclassification costs. Their use-case is based on digit recognition from incoming
handwritten mail document images. While shown to be effective at similar tasks [198], SVMs are
notorious for extreme effects of hyperparameters on performance [52]. Chatelain et al. [52] introduce
two parameters ğ¶âˆ’ and ğ¶+ as penalties for misclassifying the respective classes (namely, digit vs.
non-digit) and use them during training of the SVM. They tune for these two hyperparameters along
with ğ›¾, the kernel parameter for the radial basis function kernel. Using NSGA-II (see Section 4.2.3),
they evolve a pool of non-dominated hyperparameter configurations, thus approximating the
Pareto optimal set. Another application can be found in Horn and Bischl [114], where ParEGO
and SMS-EGO (see Section 4.3) have been applied to jointly minimize the false-negative rate and
false-positive rate of an SVM on a variety of binary classification tasks from different domains.

5.1.2 Other applications and examples for multiple prediction performance metrics. This section
outlines a few examples where trade-offs between several prediction performance metrics are
examined and MOHPO is applied accordingly.

Natural language processing. Language and human speech in general are quite complex and in
turn natural language processing (NLP) tasks are notoriously hard to evaluate. One such example
is the recent subfield of natural language generation (NLG): The quality of the resulting texts has
to be quantified, but may depend on the use case and different aspects such as semantics, syntax,
lexical overlap, fluency, coherency [143]. Sai et al. [225] show that across several popular metrics
for NLG tasks, no single metric correlates with all desirable aspects. In a related application of
MOHPO, Schmucker et al. [231] use successive halving to optimize perplexity and word error rate
for transformer based language models as one of their application examples.

Object detection. Given an image, object detection is used to determine whether there are instances
of a given type and where they are located [174]. This introduces aspects of both regression (how
close is the proposed bounding box of the object compared to the ground truth?) and classification
(are all objects identified correctly?) into the task. Widely used metrics include precision and
recall, but this naturally only focuses on one aspect of the task, as one needs to formerly define
when something is a true prediction or a false prediction. Liddle et al. [170] propose to specifically
examine two aspects of object detection in a multi-objective manner during evaluation: (i) detection
# falsely reported objects
# objects in the image . Aside from these
rate ğ·ğ‘… =

and (ii) false alarm rate ğ¹ğ´ğ‘… =

# correctly located objects
# objects in the image

26

Karl and Pielok, et al.

prediction performance metrics, detection speed is often crucial, which makes evaluation with a
single metric even more challenging [174].

5.2 Computational efficiency
Technical constraints have always limited ML research and application, but with the increasing
prominence of deep learning, efficiency in ML has become an important topic [228, 244, 245, 255].
This section will not address efficiency of the actual optimization process â€“ be it model search,
hyperparameter optimization or NAS [14, 173, 245], which is an active research area in its own
right. In the context of this work, we see efficiency as a desirable quality of an ML model with a
given HP configuration in terms of computational effort needed for training or prediction10. When
taking efficiency into account, a common scenario is the existence of resource limitations which
have to be respected, e.g., the memory consumption of the model has to be below the available
memory to allow for deployment. In these scenarios it may be more useful to the practitioner to
formulate a constrained (single-objective or multi-objective, depending on the remaining objectives)
optimization problem. Another approach to interpret efficiency in the context of an ML model is
Feature Efficiency, which will be addressed in Section 5.6. When looking at hardware implementation,
we can roughly differentiate between energy-efficient and memory-efficient models. In the following,
we will introduce several metrics that can be used to measure a modelâ€™s efficiency.

5.2.1 Efficiency metrics and approaches. We will in the following give an introduction to three
broad approaches to quantifying efficiency in the context of machine learning models and present
one related use case each. A comprehensive overview of publications applying MOHPO with at
least one objective related to efficiency can be found in Appendix A11.

Energy consumption and computational complexity. Limiting computational complexity reduces
the number of operations performed in a model and therefore generally leads to rather energy-
efficient models. Lu et al. [182] give an overview of suitable measures for computational complexity
of deep learning models: number of active nodes, number of active connections between the nodes,
number of parameters and number of floating-point operations (FLOPs). From experiments, they
conclude that FLOPs are the ideal metric and move on to optimize for it in a multi-objective
NAS along with accuracy for image classification architectures. This matches with other research
papers; FLOPs have long been used in ML â€“ especially in deep learning publications over the last
decade â€“ to describe the complexity or even size of a model, for example, in the introductions
of ResNet by He et al. [108] or ShuffleNet by Zhang et al. [267]. An alternative to FLOPs is to
use Multiply-Accumulate (MAC) operations, but the relationship to FLOPs is roughly linear [118].
Another approach to measure energy consumption is through the use of an appropriate simulator
like Aladdin as introduced in Shao et al. [235]. It is designed to simulate the energy consumption of
NNs given the right information (C code describing the operations performed by the NN) and is
used for evaluation of architectures [111, 222].
Example Application - Wang et al. [254] aim to tackle the issue of complex deep learning models
for computer vision and the challenges these extremely deep architectures pose for deployment on

10The optimization process can become more efficient if it focuses on efficient candidates during optimization. An example of
this correlation is the expected improvement per second acquisition function [239] in BO, which often prefers configurations
that are quick to evaluate.
11While some applications of MOHPO such as prediction performance have too many publications to provide a reasonable
and comprehensive overview in the scope of this work, others like e.g., interpretability have only recently been introduced
and only few related publications exist. Efficiency in MOHPO is established and at the same time still somewhat novel.
We therefore believe this comprehensive review â€“ to the best of our knowledge the first of its kind â€“ to be a worthwile
contribution.

Multi-Objective Hyperparameter Optimization â€“ An Overview

27

e.g., edge devices. They employ a multi-objective approach to identify models suitable for image
classification that are not only highly accurate, but also cope with a minimal amount of FLOPs.
Convolutional Neural Networks [160] are a popular choice of model for image classification today,
and many complex architectures are used for various computer vision tasks. One such example is
DenseNet [120] which consists of several blocks of dense layers connected via convolutional layers
and pooling layers. Wang et al. [254] uses the the hyperparameters of its dense blocks as the search
space for the architecture search. This includes the number of layers and the growth for the dense
blocks as well as typical deep learning hyperparameters, such as maximum epochs or learning
rate. From this search space, a population is initiated, and Particle Swarm Optimization [237] is
used to find a Pareto front. In experiments on the CIFAR-10 dataset, some of the identified models
outperform DenseNet-121 and other DenseNet configurations while being less complex with a
smaller number of FLOPs.

Model size and memory consumption. Especially in deep learning models, model size and efficiency
will often go hand in hand, as employing more parameters (i.e., greater model size) generally results
in more FLOPs. The parameters are mostly weights, and their number can be straightforwardly
derived in deep learning architectures. Several publications have used the number of parameters
as proxy for the efficiency of a deep learning model. For example Howard et al. [117] introduced
MobileNets, which are specifically designed for efficient deployment on edge devices. They in-
troduce separable convolutions to reduce the number of parameters needed for a top-performing
Convolutional Neural Network.
Example Application - Loni et al. [177] present their framework DeepMaker to identify efficient and
accurate models for embedded devices. The objectives are classification accuracy and model size
(number of trainable network weights); they also discovered a high correlation between the latter
and prediction time. NSGA-II is used to search the space of discrete hyperparameters (activation
function, number of condense blocks, number of convolution layers per block, learning rate, kernel
size and optimizer). Their approach has some similarity to multi-fidelity optimization, as during
optimization sampled architectures are only trained for 16 epochs to decide for the optimal choice,
but the final performance is reported after training the selected architecture for 300 epochs. The
method is tested on MNIST; CIFAR-10 and CIFAR-100.

Prediction and training time. The model is usually trained upon deployment, or the training is
not as time-critical, so we are mostly interested in minimizing prediction time. However, some
applications and deployment strategies require frequent retraining of the model. In which case,
training time can be a crucial factor as well. While often a crucial metric, prediction time is very
hard to measure reliably due to various differences in the computing environment [182]. Prediction
time may also correlate strongly with energy efficiency metrics, Rajagopal et al. [220] use FLOPs as
a proxy for inference latency.
Example Application - PESMO is introduced in HernÃ¡ndez-Lobato et al. [110] and applied to several
MOO problems, one of which is finding fast and accurate NNs for image classification on MNIST.
They tune a variety of hyperparameters: The number of hidden units per layer (between 50 and 300),
the number of layers (between 1 and 3), the learning rate, the amount of dropout, and the level of â„“1
and â„“2 regularization [110]. The objectives are prediction error and prediction time - measured here
as the time it takes for the network to predict 10.000 images. A ratio is then computed with the time
the fastest network in the search space requires for this task. PESMO is compared against SMS-EGO,
ParEGO and EHI among others and shows superior performance in terms of hypervolume. Their
follow-up paper [111] deals with a similar use case.

Dong et al. [72] examine bi-objective NAS for image classification optimizing for classification

28

Karl and Pielok, et al.

accuracy and FLOPs, the number of parameters and prediction time respectively. It may sometimes
be relevant to optimize a model with two efficiency metrics along with a primary performance
metric; examples for this can be found in Chu et al. [55], Elsken et al. [78], Lu et al. [180]. Chu
et al. [55] present a framework that allows incorporation of constraints with respect to the three
objectives in a task of the super-resolution domain: peak signal-to-noise-ratio or structural sim-
ilarity index as performance metrics, FLOPs, and the number of model parameters. Note, that
this section already delves heavily into the topic of HW-NAS, but only from a multi-objective
point of view. Single-objective and constrained approaches [24] are also widespread in addressing
these challenges. Benmeziane et al. [24] provide a comprehensive survey on HW-NAS for a reader
interested in this subtopic.

In context of the full software/hardware stack, Lokhmotov et al. [176] tunes the hyperparameters

of MobileNets for test accuracy and prediction time on an image classification task.

5.3 Fairness
When algorithmic decisions made by ML models impact the lives of humans, it is important to
avoid introducing bias that adversely affects sub-groups of the population [19, 112]. As an illustrat-
ing example, consider a bankâ€™s decision-making process for loan applications. Ethical and legal
requirements dictate that a model should not discriminate applicants because of protected attributes,
such as gender or race. Simultaneously, the bank wishes to prioritize predicting loan defaults as
accurately as possible in order to maximize profits. Jointly optimizing those objectives can provide
additional clarity to decision-makers by informing them of possible trade-offs while yielding models
that satisfy fairness metrics, allowing them to make an informed decision regarding which models
to deploy and enables the use of fair models.

For a survey on different fairness perspectives and metrics, we refer to two excellent surveys
by Mehrabi et al. [189] and Pessach and Shmueli [212], as well as a benchmark by [94] for a more
comprehensive take on fair ML than is possible in the scope of this paper. The general goal of
fair ML is to detect and potentially mitigate biases arising due to the use of ML models. For this
reason, several fairness metrics as well as debiasing methods have been proposed. The choice of the
correct metric depends on the context a decision is made in [272], see e.g., [226] who propose a
Fairness Tree that maps applicable metrics to different problem settings. While a variety of causal
and individual fairness notions exist, so-called (statistical) group fairness metrics are widely used
in practice, since they are easy to implement and do not require access to the underlying (causal)
data generating mechanism. Given a protected attribute ğ´ (e.g., race), an outcome ğ‘‹ , and a binary
predictor ğ‘Œğ‘, several criteria can be derived [19], and we provide three widely used examples:

â€¢ Equalized Odds (Independence):

ğ‘ƒğ‘Ÿ (ğ‘Œğ‘ = 1|ğ´ = 1, ğ‘Œ = ğ‘¦) = ğ‘ƒğ‘Ÿ (ğ‘Œğ‘ = 1|ğ´ = 0, ğ‘Œ = ğ‘¦), ğ‘¦ âˆˆ {0, 1}

Fulfilling this essentially requires equal true positive and false positive rates between sub-
populations. More generally, independence on the conditional ğ‘Œ is required. Equalized odds
for a binary predictor is the most relevant example of such an independence measure [107].

â€¢ Equality of Opportunity (Sufficiency):

ğ‘ƒğ‘Ÿ (ğ‘Œğ‘ = 1|ğ´ = 1, ğ‘Œ = 1) = ğ‘ƒğ‘Ÿ (ğ‘Œğ‘ = 1|ğ´ = 0, ğ‘Œ = 1)

This criterion is a relaxation of equalized odds, as only independence on the event ğ‘Œ = 1,
which denotes the advantageous outcome (e.g. getting approved for a loan) here, is required.
It therefore constitutes equality of opportunity [107].

Multi-Objective Hyperparameter Optimization â€“ An Overview

29

â€¢ Calibration is another desirable criterion for classifiers, especially in the context of fairness,
where calibrated probabilities in all groups may be required. For a classifier â„(ğ‘¥) that yields
predicted probability, calibration requires that:
âˆ€
ğ‘ âˆˆ {0,1}

ğ‘ƒğ‘Ÿ (ğ‘Œğ‘ = 1|ğ´ = ğ‘, â„(ğ‘¥) = ğ‘) = ğ‘.

âˆ€
ğ‘ âˆˆ [0;1]

Many fairness metrics metrics operate on a Fairness Tensor [145] and essentially measure

differences in performance metrics such as ğ¹ ğ‘ƒğ‘… or ğ¹ğ‘‚ğ‘… between protected groups.

Applications and exemplary use case. Existing approaches towards improving a ML-algorithmâ€™s
fairness mainly focus on decreasing discrepancies in classifier performance between sub-groups or
individuals (e.g., in [19, 156]). This is achieved e.g., by pre-processing the data (e.g., [141]), imposing
fairness constraints during model fits [156] or by post-processing model predictions [5, 107].
Those methods in turn often come with hyperparameters that can be further tuned in order to
emphasize fairness during training [230, 231]. Several approaches towards optimizing (model-)
hyperparameters for fairness in a multi-objective fashion have been proposed. While Pfisterer
et al. [213] propose multi-objective BO to jointly optimize fairness criteria as well as prediction
accuracy, [211] propose a constrained BO approach, where the fairness metric is constrained to a
small deviation from optimal fairness while predictive accuracy is optimized. Pelegrina et al. [210]
use a MOEA to optimize simultaneously for fairness metrics and performance in an attempt to
find a fair PCA. Yet another approach is introduced in Martinez et al. [188], where each sensitive
group risk is a separate objective, also leading to a MOHPO problem for choosing a classifier. It is
interesting to note that fairness can be heavily influenced not only by parameters of the debiasing
method, but also by the choice of ML algorithm and hyperparameters [213]. One popular dataset
studied in the context of Fairness is the COMPAS (Correctional Offender Management Profiling
for Alternative Sanctions) data [8]. The goal of COMPAS is to predict the risk that a criminal
defendant will re-offend. The goal is to obtain a prediction that is accurate but simultaneously not
biased towards individuals of any race. In the following example, the latter quantity is measured
(optimal for ğœğ¹ ğ‘ƒğ‘… = 1, where ğ¹ ğ‘ƒğ‘…ğ‘† is the false positive rate on group ğ‘† with ğ‘†0
via ğœğ¹ ğ‘ƒğ‘… =
and ğ‘†1 is the advantaged and disadvantaged group, respectively). The effect of applying several
debiasing strategies â€“ interpolating between no debiasing and full debiasing â€“ is shown in Figure 9.
A random forest (ğ‘…ğ¹ ) model is trained with different debiasing techniques: Reweighing [141],
Equalized Odds [107], and Natural Language Processing [5]. As can be observed, these different
strategies and debiasing strengths lead to different trade-offs, thus resulting in a MOO problem. It
is important to note that existing approaches (including the ones we cover here) often propagate a
solely technological solution to a socio-technical problem [112]. While MOO for fairness can solve
the technological aspects of this problem, it cannot be forgotten that the social aspects must also be
addressed [232]. Furthermore, practitioners must not only take into account the model itself, but
also the data used to train the algorithm [202, 248], the process behind the collection and labeling
of such data, and eventual feedback loops arising from use of potentially biased models.

ğ¹ ğ‘ƒğ‘…ğ‘†0
ğ¹ ğ‘ƒğ‘…ğ‘†1

5.4 Interpretability
In order to make an ML modelâ€™s decisions more transparent, different methods that aim at providing
human-understandable explanations have been proposed (c.f. [194]). Requirements for interpretabil-
ity generally range from models derived from fully interpretable model classes to interpretability
via post-hoc interpretation techniques. For example, the former is required to satisfy regulatory
constraints in the banking sector [42, 50] and can be thought of as a constraint on the search space
when selecting a model. The latter is often used to understand functional relationships between
features and target variables in an ML context (i.e., understanding the model) or to understand

30

Karl and Pielok, et al.

Fig. 9. Effect of 3 different debiasing strategies on the fairness-accuracy trade-off on the COMPAS dataset
measured on test data. Figure obtained from [5]

single decisions made by a trained algorithm [194]. In addition, interpretability techniques can be
helpful to debug errors made by models (e.g., errors stemming from mislabeled data or spuriously
drawn correlations [155]). While many explanations can be either directly derived from the model
class (e.g., decision trees and generalized linear models), interpretability techniques either focus on
a single model class [204, 233] or are model-agnostic [9, 183]. The latter is especially desirable, as this
allows the user to explain arbitrary models resulting from tuning over various model classes. On the
other hand, interpretability methods can produce misleading results if a model is too complex or the
explainability technique is unreliable due to them e.g., using additional features [147, 194]. Quan-
tifying interpretability, i.e., determining how complex the predictive decisions of a given model are,
could be a first step towards obtaining models that provide reliable explanations and interpretable
models. Quantifying interpretability of a model is not straightforward, as terms like interpretability,
explainability, and complexity are highly subjective expressions [171, 194, 195]. First approaches
to metrics that can be used as a proxy for interpretability on tabular data have been proposed, as
shown e.g., in Molnar et al. [195]. While explainability for non-tabular application domains may
be equally interesting, no methods that allow quantifying the interpretability of e.g., computer
vision models have been proposed to date. A popular proxy for model complexity (and therefore
interpretability) is sparseness, i.e., the number of features. We explore this concept further in the
context of MOHPO in Section 5.6. Molnar et al. [195] propose 2 additional metrics aside from
sparseness to quantify interpretability on tabular data, which we will briefly summarize here12:

â€¢ Complexity of main effects Molnar et al. [195] propose to determine the average shape
complexity of Accumulated Local Effects (ALE) [9] main effects by the number of parameters
needed to approximate the curve with linear segments. The Main Effect Complexity (MEC)
is defined as:

ğ‘€ğ¸ğ¶ =

1
ğ‘—=1 ğ‘‰ğ‘—

(cid:205)ğ‘

ğ‘
âˆ‘ï¸

ğ‘—=1

ğ‘‰ğ‘— Â· ğ‘€ğ¸ğ¶ ğ‘—,

(16)

12A detailed derivation and explanation can be found in Molnar [194], Molnar et al. [195]

0.580.600.620.640.70.80.91.0Ï„FPRaccuracyModelRFRF-Equalized OddsRF-NLPRF-ReweighingMulti-Objective Hyperparameter Optimization â€“ An Overview

31

(cid:205)ğ‘›

ğ‘–=1(ğ‘“ğ‘—,ğ´ğ¿ğ¸ (ğ‘¥ (ğ‘–) ))2 and ğ‘€ğ¸ğ¶ ğ‘— = ğ¾ + (cid:205)ğ¾

where ğ‘‰ğ‘— = 1
ğ‘˜=1 Iğ›½1,ğ‘˜ >0 âˆ’ 1. ğ¾ is the number of linear
ğ‘›
segments needed for a good-enough approximation, ğ‘ is the number of features, ğ›½1, ğ‘˜ the
slope of the ğ‘˜ âˆ’ ğ‘¡â„ linear segment, and ğ‘› the number of samples.

â€¢ Interaction Strength Quantifying the impact of interaction effects is relevant when expla-
nations are required, as most interpretability techniques use linear relationships to obtain
explanations. Interaction strength ğ¼ğ´ğ‘† can be measured as the fraction of variance that cannot
be explained by main effects:

ğ¼ğ´ğ‘† =

E(ğ¿(ğ‘“ , ğ‘“ğ´ğ¿ğ¸1ğ‘ ğ‘¡ ))
E(ğ¿(ğ‘“ , ğ‘“0))

â‰¥ 0,

(17)

where ğ‘“ is the prediction function, ğ‘“ğ´ğ¿ğ¸1ğ‘ ğ‘¡ the sum of first order ALE effects, and ğ‘“0 the mean
of predictions.

Applications and exemplary use case. Molnar et al. [195] aim to find an accurate and interpretable
model to predict the quality of white wines (on a scale from 0 to 10) [59]. In order to accomplish this,
they define a large search space of several models (SVM, gradient boosted trees, and random forest,
among others) with a number of tunable hyperparameters and optimize for four objectives: cross-
validated mean absolute error, number of features used, main effect complexity, and interaction
strength. They conduct the optimization with ParEGO (see Section 4.3.2) over 500 iterations to
find good trade-offs between these objectives. Carmichael et al. [48] perform MOHPO for deep
learning architectures to find optimal trade-offs between accuracy and introspectability for image
classification tasks on ImageNet-16-120, CIFAR-10 and MNIST.

5.5 Robustness
In many use cases, it is highly desirable to identify and deploy robust ML models, i.e., models that
are able to maintain good performance despite variation in the input data [66, 96]. Robustness
in the context of ML is only loosely defined. We broadly distinguish between robustness of the
training procedure and robustness of the fitted model, which have both been described in literature.
We focus on the latter, i.e., the susceptibility of a trained model to shifts of the data in the prediction
step. While most research into robustness focuses on images, we aim to look at a general case.
We mostly consider a classification setting in the following sections, but we also aim to provide
information as to how this differs for regression where appropriate.

5.5.1 Robustness metrics and approaches. While generally seen as important and relevant, there
are no tried and proven metrics to assess the robustness of ML models, nor a proper taxonomy.
The taxonomy in [247] provides an overview of possible changes to the input data. While centered
around image data, many ideas can be carried over to other types of data. They distinguish between
natural and synthetic changes to the input data. Natural changes equate to changes to the dataset
relying solely on unmodified data points, whereas synthetic changes allow for modification (e.g.,
perturbations) of data points. We differentiate between three types of changes:

Distribution shift. Our notion of a distribution shift refers to changes of either the marginal
distribution of the target or the distribution of the features (conditional on the target) on a macro
level. This concept is often associated with domain adaption (which is becoming increasingly
popular as a research area, see Zhang et al. [265] for an introduction). Several typical real-world
examples would be the increase in temperature of a manufacturing environment that leads to a
change in sensor readings, or an image classification task previously trained and tested on only
well-lit pictures suddenly exposed to darker pictures, or different weather conditions. Typical
metrics used to assess robustness in the context of distribution shifts are effective robustness

32

Karl and Pielok, et al.

ğœŒ (ğ‘“ ) = ğ‘ğ‘ğ‘2(ğ‘“ ) âˆ’ ğ›½ (ğ‘ğ‘ğ‘1(ğ‘“ )), where ğ‘ğ‘ğ‘1, ğ‘ğ‘ğ‘2 are the accuracies pre- and post- domain shift, and
ğ›½ is the chosen baseline accuracy on the shifted test set. Duchi and Namkoong [74] show that
optimizing a model for robustness in the context of distribution shift equates to optimizing it
for performance on the tails. They then propose to reformulate the optimization problem from
optimizing for average performance to instead using a metric that upweights the regions with the
highest loss. They dub this reformulated optimization problem the distributionally robust problem.
Indeed, the notion of distributionally robust optimization has existed for some time and has been
applied to both data-driven and ML problems [70, 219].

Adversarial examples. Adversarial examples have recently generated substantial interest from the
visual deep learning community [6, 98, 249], because a model that is very susceptible to adversarial
attacks (the use of adversarial examples) is not as trustworthy. Prominent examples from image
data present pictures with small perturbations â€“ often undetectable to a human observer â€“ that
"trick" the ML model into false classification, whereas the original image was classified correctly.
Adversarial examples are well-known in image data [259] but have been shown in other types
of data, such as text [124], sequence [53], or tabular data [17]. Oftentimes, adversarial examples
generated through perturbation are in focus, but different adversarial attacks generally lead to
different measures [208]. A popular metric to assess robustness in the context of adversarial
examples is adversarial accuracy [185, 187, 250]. Adversarial accuracy measures the percentage
of samples that are (still) correctly classified after the adversarial attack [250]. In practice the
user would conduct the adversarial attack and then calculate adversarial accuracy from the new
predictions. For adversarial perturbations in an ğœ€-ball around each point, a typical adversarial attack
in classification, it can be defined as:

E[ğŸ™(ğ‘“ (ğ‘¥ âˆ—) = ğ‘ğ‘¥ )], ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘¥ âˆ— = arg max
ğ‘‘ (ğ‘¥ â€²,ğ‘¥) â‰¤ğœ–

ğ¿(ğ‘¥ â€², ğ‘ğ‘¥ ),

(18)

where ğ‘ğ‘¥ is the respective class label. Two metrics for adversarial robustness in computer vision
were originally defined in Bastani et al. [20] and have also been employed by Buzhinsky et al. [43]:
adversarial frequency and adversarial severity. Adversarial frequency ğœ™ is measured as the accuracy
on a worst-case input in an â„“ğ‘ ğœ€-ball around each point ğ‘¥âˆ—:

ğœ™ (ğ‘“ , ğœ€) (cid:66) ğ‘ƒ (ğœŒ (ğ‘“ , ğ‘¥âˆ—) â‰¤ ğœ€),
where ğœŒ (ğ‘“ , ğ‘¥âˆ—) is the minimum distance Ë†ğœ€ for some well-defined metric ğ‘‘, so that âˆƒ ğ‘¥, ğ‘‘ (ğ‘¥, ğ‘¥âˆ—) â‰¤
Ë†ğœ€ : ğ‘“ (ğ‘¥) â‰  ğ‘“ (ğ‘¥âˆ—) with ğ‘“ as a classifier. Adversarial severity ğœ‡ is defined as the expected minimum
distance to an adversarial example from the input for some ğœ€ [20]:

ğœ‡ (ğ‘“ , ğœ€) (cid:66) ğ¸ [ğœŒ (ğ‘“ , ğ‘¥âˆ—) | ğœŒ (ğ‘“ , ğ‘¥âˆ—) â‰¤ ğœ€],
with ğœŒ, ğ‘“ , ğ‘¥âˆ— as before. While Bastani et al. [20] deem adversarial frequency to be generally the more
important metric of the two, significant work centers around the minimum distance to creating an
adversarial example, especially for neural networks [209]. In Rauber et al. [221], this subsequently
prompted the unification of various adversarial attacks and measurement of robustness through
minimum required perturbation.

Perturbations. Perturbations are oftentimes strongly linked to the construction of adversarial
examples in deep learning [159]. This can be seen as a logical connection, as one would often look
for an adversarial example within some ğœ–-ball (see above). Laugros et al. [159] argue and show that
robustness in the context of common perturbations and adversarial robustness are independent
attributes of a model. Their work focuses on image data and includes some transformations â€“ like
a change in brightness â€“ that we would classify as a domain shift. One of the most common ways
to perturb data is the addition of Gaussian noise, which is also included in their work. A simple

Multi-Objective Hyperparameter Optimization â€“ An Overview

33

measure for robustness regarding such a perturbation is presented in Pfisterer et al. [213]. Adding
random Gaussian noise ğ‘ (0, ğœ–) 13 to input data ğ‘‹ is a typical way to introduce perturbations. One
can then compare the loss derived from a relevant measure ğ¿ on the changed input data to the loss
on unperturbed data:

|ğ¿(ğ‘‹, ğ‘Œ ) âˆ’ ğ¿(ğ‘‹ + ğ‘ (0, ğœ–), ğ‘Œ )|.
Of course, the same procedure can be applied to other types of perturbations. In Gilmer et al. [97], the
authors argue and provide evidence that susceptibility to adversarial examples and perturbations
â€“ at least for image data â€“ are actually two symptoms of the same underlying problem, and
thus, optimizing for robustness against adversarial attacks and more general corruption through
perturbations should go hand-in-hand. In general, perturbations and adversarial examples are
closely related, and â€“ as seen above â€“ substantial research in the computer vision community
focuses on adversarial perturbations [6, 43, 97]. Niu et al. [199] give a comprehensive overview
on perturbations to data in machine translation (mainly synthetic misspellings and letter case
changing) and define a variety of suitable measures for model robustness in the context of those
perturbations.

5.5.2 A note on robustness and uncertainty quantification. Uncertainty quantification, especially
in the context for deep learning, has become a heavily researched topic and is often mentioned
in conjunction with robustness of ML models [157]. To the best of our knowledge, optimizing an
ML modelâ€™s configuration for proper uncertainty quantification or minimal uncertainty is not an
established method â€“ and integrating a respective metric into MOHPO is even less so. We will
therefore restrict this brief section to exploring the connection between uncertainty quantification
and robustness as well as other relevant objectives. The total uncertainty of a model can be divided
into aleatoric uncertainty and epistemic uncertainty. While the former is inherent to observations,
the latter accounts for uncertainty in the model parameters and can be explained away with
additional data [144]. Models that are better calibrated with respect to uncertainty tend to suffer
less from adversarial examples [157, 238]. Along the same lines, robustness regarding domain shift
and predictive performance on out-of-distribution samples are closely linked to (and even used as
a measure of) predictive uncertainty [157]. As perturbations are often used to model noise in the
data, a connection between this type of robustness and aleatoric uncertainty is easily conjectured.
Indeed, Kendall and Gal [144] show that using the quantification of aleatoric uncertainty has a
beneficial effect on the robustness in the context of noise, i.e., perturbations. Finally, uncertainty
quantification can be relevant to the interpretability of an ML model. As argued in Kendall and Gal
[144], a proper quantification of the modelâ€™s uncertainty adds an extra layer of explainability and
might aid in the prevention of critical mistakes, thus increasing the trustworthiness of the model.

5.5.3 Application and exemplary use case. The main goal of Guo et al. [102] is to identify network
architectures that are robust with respect to adversarial attacks. To this end, the authors employ
one-shot NAS [23]. Essentially, this entails defining a search space with several operations: An
architecture is specified through hyperparameters ğ›¼ = {ğ›¼ (ğ‘–,ğ‘—)
| ğ›¼ (ğ‘–,ğ‘—)
âˆˆ {0, 1}, ğ‘–, ğ‘— = 1, ..., ğ‘ , ğ‘˜ =
ğ‘˜
1, ..., ğ‘›} that represents the inclusion (ğ›¼ (ğ‘–,ğ‘—)
= 0) of a transformation from a
pool of ğ‘› transformations14 between nodes ğ‘–, ğ‘— of a graph representing a computational cell in the
network. After initial training of a supernet, i.e., all hyperparameters set to 1, architectures are drawn
through random search, fine-tuned and evaluated. While the optimization is not strictly multi-
objective, this is not as important when conducting what is essentially a random search, because
subsequent trials are not influenced by prior ones. The authors then examine different architectures

= 1) or exclusion (ğ›¼ (ğ‘–,ğ‘—)

ğ‘˜

ğ‘˜

ğ‘˜

13ğœ– is generally 0.001 âˆ’ 0.01 times the range of the numerical feature
143 Ã— 3-separable convolution, identity, and zero in this case.

34

Karl and Pielok, et al.

Fig. 10. Overview and classification of several architectures examined by Guo et al. [102] with respect to
objectives number of parameters and adversarial accuracy.

and compare the desirable qualities of robustness in the context of adversarial examples and model
size to identify suitable models. This is illustrated for a few select architectures in Figure 10.

5.6 Sparseness via Feature Selection
In ML, we often face high-dimensional tasks with a large number of features. However, frequently
only a small fraction of all features is informative. Sparse models, i.e., models which use relatively
few features, are often desirable. If too many features are considered by a model, . . .

â€¢ . . . the relationship between features and model prediction may be hard to understand and

interpret [104] (see Section 5.4),

â€¢ . . . the cost for model fitting or inference may increase, either in terms of storage, computation,

or in terms of actual costs for measuring or acquiring the data [193],

â€¢ . . . the predictive performance of a model might even suffer because of the the curse of

dimensionality [22].

The severity of the effects depends on the machine learning algorithm used; some algorithms
scale worse than others with regards to the number of features, and some (e.g., k-nearest neighbors
algorithm) suffer more from the curse of dimensionality than others. Because of these potential
drawbacks, it may often be desirable to perform feature selection before or during training. Feature
selection is inherently an MOO problem, as model performance and sparsity tend to be conflicting
objectives; a lower number of features often means a lower performance due to reduced information.
However, for a certain model and configuration there will be a specific desirable quantity of features,
that is probably not the maximum number of features [154]: Including less features will likely
decrease the performance due to lack of information, whereas including too many features will
create an abundance of information and the configuration suffers increasingly from the curse
of dimensionality. In the following, we will provide an overview of common feature selection
techniques with an emphasis on their connection and possible combination with MOHPO. Feature
selection approaches are generally categorized into three sections in accordance to how the evaluate
feature configurations [7]:

Embedded methods. methods perform feature selection as part of the model fitting process. For
example, empirical risk minimization with L0 or L1 regularization shrinks irrelevant coefficients

Multi-Objective Hyperparameter Optimization â€“ An Overview

35

towards zero, and can therefore automatically produce sparse models during training. For exam-
ple, Louizos et al. [179] train sparse neural networks via L0 penalization. As another example, trees
and tree-based methods can inherently produce sparse models by limiting the number of used
features through the maximal tree depth. As feature selection is performed as part of model fitting,
no separate search technique must be used. The drawback of embedded methods is that they are
specific to the model in use.

Filter methods. approaches use proxies to rank feature subsets by their likely explanatory power
independent of the learning algorithm. Single measures â€“ for instance, information theoretic
measures â€“ correlation measures, distance measures, or consistency measures [61] are calculated
and used for evaluation of different feature subsets. Filtering is usually applied before model
training.

Wrapper methods. search over the space of selected features to optimize model performance [154].
Because they take the learner algorithm performance into account directly, they often yield better
performance than filter methods. However, they are computationally more expensive because model
performance evaluations are generally noisy and relatively expensive. Additionally, exhaustively
trying all possible combinations is usually computationally infeasible. Because wrapper methods
can use general optimization algorithms, they are the most amenable to extensions to MOO [7].
Hybrid approaches can also be utilized, as in Bommert et al. [33] and Binder et al. [29], where hybrid
filter-wrapper approaches are proposed to limit the complexity of the search spaces. This is desirable
as BO surrogate models for feature selection must be modeled on a potentially high-dimensional
and combinatorial search space (see Eq. (19)).

Sparseness Metrics. Aside from the simplest form, namely minimizing the number of used
5.6.1
features, it may be of interest to formulate other objectives in relation to sparsity by e.g., weighting
the features:

â€¢ (Weighted) Number of Features included in the model

ğ‘
âˆ‘ï¸

ğ‘—=1

ğ‘¤ ğ‘—ğ‘  ğ‘— .

If features have different costs, e.g., because they have different acquisition costs if a model is
intended to be applied in real life, different weights can be introduced for this purpose.
â€¢ Stability of Feature Selection: In some applications the main goal of the analysis is the
identification of important features. In bioinformatics, for example, performance plays a
subordinate role in the analysis of omics data. The aim is to identify important genes for later
examination in the laboratory. Besides predictive performance and the number of selected
features, a third objective quantifying the stability of feature selection is introduced e.g., by
comparing sets of selected features resulting from different resampling iterations.

We argue that it is reasonable and efficient to consider feature selection and hyperparameter
optimization in a joint step and will briefly mention some approaches that do so: Binder et al.
[29] examine an approach including feature selection and hyperparameter optimization of ML
models and benchmark EAs as well as BO methods. Bouraoui et al. [35] propose a similar approach,
unifying model and feature selection, restricted to SVMs. Several MOEAs are used for conjoint
MOHPO and feature selection in Sopov and Ivanov [240] with application to emotion recognition.
Liuliakov and Hammer [175] proposed a combined feature selection HPO AutoML tool based on
TPOT.

36

Karl and Pielok, et al.

5.6.2 Applications and exemplary use case. In general, the task of feature selection implies search
over a binary space {0, 1}ğ‘ with ğ‘ ğ‘– = I[feature ğ‘– is included]. To simultaneously perform hyperpa-
rameter optimization, we formulate the joint search space of feature and hyperparameter configu-
rations

{0, 1}ğ‘ Ã— ËœÎ›.

(19)

The dimensionality of this space grows exponentially with the number of available features,
which makes the optimization problem particularly challenging. Additionally, there may be com-
plex interactions between features and hyperparameter configurations. Expensive evaluation like
wrapper evaluation techniques raise the need for efficient search methods. Evolutionary algorithms
(see Section 4.2) are widely used for feature selection due to their ability to handle complex search
spaces, [1, 261], but recent works have presented BO methods (see Section 4.3) as a more efficient
alternative [29, 33]. Aside from exploring one exemplary use case, we will highlight a few select
examples of feature selection in the context of MOHPO. A comprehensive review with additional
examples can be found in Al-Tashi et al. [7] and specifically for Evolutionary Computation methods
in Xue et al. [261]15. We highlight two works in the scope of multi-objective feature selection.
Bommert et al. [32] state feature selection as a MOHPO problem with three objectives: predictive
performance, number of features selected, and stability of feature selection. Furthermore, they
provide a comprehensive comparison of stability measures. They tune hyperparameters of the
underlying ML pipelines (feature filter plus classification learner) that are relevant to the sparseness
objectives via random search to identify desirable trade-offs. All classification pipelines have been
tuned w.r.t. the aforementioned three criteria. In the next step, all configurations that are not within
a 5% tolerance of the best predictive performance identified so far are discarded. The remaining con-
figurations are shown in Figure 11 for an exemplary dataset. The figure demonstrates one takeaway
message of the benchmark: In comparison with the single-criterion tuning approach (marked by the
triangle in Figure 11), the multi-objective approach additionally reveals hyperparameter settings
that yield models with comparable or even better performance. At the same time, these additional
identified model configurations require fewer features, and the feature selection is more stable.
Binder et al. [29] recently showed that model-based optimization - even more than evolutionary
approaches - offer great performance enhancement compared to random search, which was used
in Bommert et al. [32].

6 DISCUSSION AND OPEN CHALLENGES
This paper has presented an overview of the concepts, methods and applications of MOHPO - as well
as related concepts - and provide a guide to the ML practitioner delving into this particular topic. It is
evident there is merit to formulating ML problems in a multi-objective manner, as many application
examples support. Single-objective ML tasks, tuned to a pure prediction performance metric, are no
longer in keeping with the state-of-the-art for many ML applications, as models have to meet certain
standards with respect to secondary goals. While this presents new challenges to the ML expert with
regard to optimization and algorithm selection, proper methods can provide the user with a suite of
Pareto optimal trade-offs to choose a suitable model. As the topic of MOHPO and multi-objective
pipeline creation and model selection is not fully established, the available software (not for MOO,
but specifically MOHPO) is limited, but good implementations exist for several standard methods.
Finally, we want to emphasize the lack of proper and extensive benchmarks for the field of MOHPO,
which could shed some further light on strengths and weaknesses of different methods on a variety

15This work mainly includes works that reduce feature selection to a single-objective problem, but also several multi-objective
approaches.

Multi-Objective Hyperparameter Optimization â€“ An Overview

37

Fig. 11. Excerpt from results in Bommert et al. [32]: Different combinations of feature filters and classification
learners tuned on the eating dataset (OpenML-ID 1233). Shown are points from the 3D Pareto front (projected
into 2D space of stability measure vs. mean number of selected features) that are within the 5% threshold
of the best predictive performance identified so far on the training set. Points that appear to be dominated
in this 2D representation are in fact non-dominated due to higher accuracy. All three visualized criteria are
evaluated on independent test sets.

of MOHPO tasks. The discrepancy in widespread vs. sparse use of MOHPO depending on the
application at hand may also appear striking. ROC analysis and multi-objective feature selection
are established and well researched areas; the body of literature for MOHPO including efficiency
objectives has grown rapidly in the past few years with the ascent of deep learning and HW-
NAS. With the recent trends to integrate FAT-ML related standards into the ML process, MOHPO
with applications to interpretability and fairness is currently becoming increasingly relevant, but
few works have been published that deal with these objectives. Integrating user preferences in a
meaningful way either a priori or during the optimization process (see Section 4.5.1) remains a
challenge that could help efficiency and transparency of MOHPO. It should be noted that MOHPO -
compared to single-objective HPO - already improves in terms of transparency, simply by not having
to reduce to a single metric and the result being a collection of trade-offs and not only a single HP
configuration. Another challenge is MOHPO beyond supervised learning: We have focused in this
work on supervised learning, but unsupervised methods like anomaly detection or clustering also
depend heavily on HPs. As in single-objective HPO [30], the difficulty of performance evaluation
and lack of standardized metrics complicate the application of the presented methods. Aside from
"typical" performance measures, other objectives like e.g., efficiency can still be concretely evaluated
and may be included in (MO)HPO of unsupervised methods.

ACKNOWLEDGMENTS
This work was supported by the Bavarian Ministry of Economic Affairs, Regional Development and
Energy through the Center for Analytics â€“ Data â€“ Applications (ADA-Center) within the framework

0.6750.7000.7250.750405060Mean Number of Selected Features (train set)1 âˆ’ Stability (train set)0.620.630.640.65Mean ClassificationAccuracy (Train)Best performance on train setParetoâˆ’optimal multiâˆ’crit and within 5% performance threshold38

Karl and Pielok, et al.

of BAYERN DIGITAL II (20-3410-2-9-8) as well as the German Federal Ministry of Education and
Research (BMBF) under Grant No. 01IS18036A.

REFERENCES

[1] Nadia Abd-Alsabour. 2014. A review on evolutionary feature selection. In 2014 European Modelling Symposium. IEEE,

20â€“26. https://doi.org/10.1109/EMS.2014.28

[2] Majid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, and Svetha Venkatesh. 2019. Multi-objective Bayesian
optimisation with preferences over objectives. In Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett
(Eds.). 12214â€“12224. https://dl.acm.org/doi/10.5555/3454287.3455383

[3] Ajith Abraham and Lakhmi Jain. 2006. Evolutionary Multiobjective Optimization: Theoretical Advances and Applications.

Springer London. https://books.google.de/books?id=KHOQu7R_POoC

[4] Virginia Aglietti, Xiaoyu Lu, Andrei Paleyes, and Javier GonzÃ¡lez. 2020. Causal Bayesian Optimization. In International

Conference on Artificial Intelligence and Statistics. PMLR, 3155â€“3164.

[5] Ashrya Agrawal, Florian Pfisterer, Bernd Bischl, Jiahao Chen, Srijan Sood, Sameena Shah, Francois Buet-Golfouse,
Bilal A Mateen, and Sebastian Vollmer. 2020. Debiasing classifiers: is reality at variance with expectation?
arXiv:2011.02407

[6] Naveed Akhtar and Ajmal Mian. 2018. Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey.

arXiv:1801.00553

[7] Qasem Al-Tashi, Said Jadid Abdulkadir, Helmi Md Rais, Seyedali Mirjalili, and Hitham Alhussian. 2020. Approaches
to Multi-Objective Feature Selection: A Systematic Literature Review. IEEE Access 8 (2020), 125076â€“125096. https:
//doi.org/10.1109/ACCESS.2020.3007291

[8] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kichner. 2016. Machine Bias. https://www.propublica.org/article/

machine-bias-risk-assessments-in-criminal-sentencing

[9] Daniel W. Apley. 2016. Visualizing the effects of predictor variables in black box supervised learning models.

arXiv:1612.08468

[10] Jasbir S. Arora. 2004. Multiobjective Optimum Design Concepts and Methods. In Introduction to Optimum Design

(Second Edition) (second edition ed.), Jasbir S. Arora (Ed.). Academic Press, San Diego, 543 â€“ 563.

[11] Raul Astudillo and Peter Frazier. 2017. Multi-attribute Bayesian optimization under utility uncertainty. In NIPS

workshop on Bayesian Optimization, Vol. 172.

[12] Charles Audet and Warren Hare. 2017. Derivative-Free and Blackbox Optimization. Springer International Publishing.
[13] Nebojsa Bacanin, Timea Bezdan, Eva Tuba, Ivana Strumberger, and Milan Tuba. 2020. Optimizing Convolutional
Neural Network Hyperparameters by Enhanced Swarm Intelligence Metaheuristics. Algorithms 13 (03 2020), 67.
https://doi.org/10.3390/a13030067

[14] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. 2017. Accelerating neural architecture search using

performance prediction. arXiv:1705.10823

[15] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and
Eytan Bakshy. 2020. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. arXiv:1910.06403 [cs.LG]
[16] Maria Baldeon-Calisto and Susana K. Lai-Yuen. 2020. AdaResU-Net: Multiobjective adaptive convolutional neural

network for medical image segmentation. Neurocomputing 392 (2020), 325â€“340.

[17] Vincent Ballet, Xavier Renard, Jonathan Aigrain, Thibault Laugel, Pascal Frossard, and Marcin Detyniecki. 2019.

Imperceptible Adversarial Attacks on Tabular Data. arXiv:1911.03274

[18] Mohua Banerjee, Sushmita Mitra, and Ashish Anand. 2006. Feature Selection Using Rough Sets. In Multi-Objective

Machine Learning, Yaochu Jin (Ed.). Studies in Computational Intelligence, Vol. 16. Springer, 3â€“20.

[19] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2018. Fairness and Machine Learning. fairmlbook.org. http:

//www.fairmlbook.org.

[20] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Criminisi.

2016. Measuring Neural Net Robustness with Constraints. arXiv:1605.07262

[21] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. 2019. Max-value Entropy Search for Multi-Objective
Bayesian Optimization. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett (Eds.). 7823â€“7833.
https://proceedings.neurips.cc/paper/2019/hash/82edc5c9e21035674d481640448049f3-Abstract.html

[22] Richard E. Bellman. 2015. Adaptive Control Processes. Princeton University Press, Princeton. 94 pages. https:

//doi.org/10.1515/9781400874668

Multi-Objective Hyperparameter Optimization â€“ An Overview

39

[23] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. 2018. Understanding and

simplifying one-shot architecture search. In International Conference on Machine Learning. PMLR, 550â€“559.

[24] Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, SmaÃ¯l Niar, Martin Wistuba, and Naigang Wang.
2021. A Comprehensive Survey on Hardware-Aware Neural Architecture Search. arXiv:2101.09336 https://arxiv.org/
abs/2101.09336

[25] James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, and BalÃ¡zs KÃ©gl. 2011. Algorithms for Hyper-Parameter Optimization.
In Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and
K. Q. Weinberger (Eds.). Curran Associates, Inc., 2546â€“2554.

[26] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of machine

learning research 13, 2 (2012).

[27] Simon Bernard, ClÃ©ment Chatelain, SÃ©bastien Adam, and Robert Sabourin. 2016. The multiclass roc front method for

cost-sensitive classification. Pattern Recognition 52 (2016), 46â€“60.

[28] Jinbo Bi and Kristin P. Bennett. 2003. Regression error characteristic curves. In Proceedings of the 20th international

conference on machine learning (ICML-03). 43â€“50.

[29] Martin Binder, Julia Moosbauer, Janek Thomas, and Bernd Bischl. 2020. Multi-Objective Hyperparameter Tuning
and Feature Selection Using Filter Ensembles. In Proceedings of the 2020 Genetic and Evolutionary Computation
Conference (CancÃºn, Mexico) (GECCO â€™20). Association for Computing Machinery, New York, NY, USA, 471â€“479.
https://doi.org/10.1145/3377930.3389815

[30] Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, Theresa Ullmann,
Marc Becker, Anne-Laure Boulesteix, Difan Deng, and Marius Lindauer. 2021. Hyperparameter Optimization:
Foundations, Algorithms, Best Practices and Open Challenges. arXiv:2107.05847 [stat.ML]

[31] Bernd Bischl, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012. Resampling methods for meta-model
validation with recommendations for evolutionary computation. Evolutionary computation 20, 2 (2012), 249â€“275.
[32] Andrea Bommert, JÃ¶rg RahnenfÃ¼hrer, and Michel Lang. 2017. A Multicriteria Approach to Find Predictive and Sparse
Models with Stable Feature Selection for High-Dimensional Data. Computational and Mathematical Methods in
Medicine 2017 (2017), 1â€“18. https://doi.org/10.1155/2017/7907163

[33] Andrea Bommert, Xudong Sun, Bernd Bischl, JÃ¶rg RahnenfÃ¼hrer, and Michel Lang. 2020. Benchmark for filter methods
for feature selection in high-dimensional classification data. Computational Statistics & Data Analysis 143 (March
2020), 106839. https://doi.org/10.1016/j.csda.2019.106839

[34] Jakob Bossek, Carola Doerr, and Pascal Kerschke. 2020. Initial Design Strategies and Their Effects on Sequential
Model-Based Optimization: An Exploratory Case Study Based on BBOB. In Proceedings of the 2020 Genetic and
Evolutionary Computation Conference (GECCO â€™20). Association for Computing Machinery, New York, NY, USA,
778â€“786. https://doi.org/10.1145/3377930.3390155

[35] Amal Bouraoui, Salma Jamoussi, and Yassine BenAyed. 2018. A multi-objective genetic algorithm for simultaneous
model and feature selection for support vector machines. Artificial Intelligence Review 50, 2 (2018), 261â€“281.
[36] JÃ¼rgen Branke. 2016. MCDA and Multiobjective Evolutionary Algorithms. Springer New York, New York, NY, 977â€“1008.
[37] JÃ¼rgen Branke and Kalyanmoy Deb. 2005. Integrating user preferences into evolutionary multi-objective optimization.

In Knowledge incorporation in evolutionary computation. Springer, 461â€“477.

[38] JÃ¼rgen Branke, Kalyanmoy Deb, Henning Dierolf, and Matthias Osswald. 2004. Finding knees in multi-objective

optimization. In Parallel Problem Solving from Nature (LNCS, 3242). Springer, 722â€“731.

[39] Juergen Branke, Kalyan Deb, Kaisa Miettinen, and Roman SlowiÅ„ski. 2008. Multiobjective Optimization: Interactive

and Evolutionary Approaches. Springer. https://books.google.de/books?id=N-1hWMNUa2EC

[40] JÃ¼rgen Branke, Christian Schmidt, and Hartmut Schmeck. 2001. Efficient Fitness Estimation in Noisy Environments.
In Genetic and Evolutionary Computation Conference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,
243â€“250.

[41] Carla E. Brodley, Umaa Rebbapragada, Kevin Small, and Byron Wallace. 2012. Challenges and opportunities in applied

machine learning. Ai Magazine 33, 1 (2012), 11â€“24.

[42] Michael BÃ¼cker, Gero Szepannek, Alicja Gosiewska, and Przemyslaw Biecek. 2020. Transparency, Auditability and

eXplainability of Machine Learning Models in Credit Scoring. arXiv:2009.13384

[43] Igor Buzhinsky, Arseny Nerinovsky, and Stavros Tripakis. 2020. Metrics and methods for robustness evaluation of

neural networks with generative models. arXiv:2003.01993 [cs.LG]

[44] Han Cai, Ligeng Zhu, and Song Han. 2019. ProxylessNAS: Direct Neural Architecture Search on Target Task and
Hardware. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net.

[45] Maria Baldeon Calisto and Susana K. Lai-Yuen. 2020. AdaEn-Net: An ensemble of adaptive 2Dâ€“3D Fully Convolutional

Networks for medical image segmentation. Neural Networks 126 (2020), 76â€“94.

40

Karl and Pielok, et al.

[46] Maria G. Baldeon Calisto and Susana K. Lai-Yuen. 2021. EMONAS-Net: Efficient multiobjective neural architecture
search using surrogate-assisted evolutionary algorithm for 3D medical image segmentation. Artif. Intell. Medicine 119
(2021), 102154. https://doi.org/10.1016/j.artmed.2021.102154

[47] Matt Carlyle, John W. Fowler, Esma Gel, and Bosun Kim. 2003. Quantitative Comparison of Approximate Solution
Sets for Bi-criteria Optimization Problems. Decision Sciences 34 (02 2003). https://doi.org/10.1111/1540-5915.02254
[48] Zachariah Carmichael, Tim Moon, and Sam A. Jacobs. 2021. Learning Interpretable Models Through Multi-Objective

Neural Architecture Search. arXiv:2112.08645

[49] Rich Caruana and Alexandru Niculescu-Mizil. 2004. Data mining in metric space: an empirical analysis of supervised
learning performance criteria. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery
and data mining. 69â€“78.

[50] Diogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. 2019. Machine learning interpretability: A survey on

methods and metrics. Electronics 8, 8 (2019), 832.

[51] Akshay Chandrashekaran and Ian Lane. 2016. Automated optimization of decoder hyper-parameters for online

LVCSR. In 2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 454â€“460.

[52] ClÃ©ment Chatelain, SÃ©bastien Adam, Yves Lecourtier, Laurent Heutte, and Thierry Paquet. 2010. A multi-model
selection framework for unknown and/or evolutive misclassification cost problems. Pattern Recognition 43, 3 (2010),
815â€“823.

[53] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh. 2020. Seq2Sick: Evaluating the Robustness
of Sequence-to-Sequence Models with Adversarial Examples. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February
7-12, 2020. AAAI Press, 3601â€“3608. https://aaai.org/ojs/index.php/AAAI/article/view/5767

[54] Ting-Wu Chin, Ari S. Morcos, and Diana Marculescu. 2021. Joslim: Joint Widths and Weights Optimization for
Slimmable Neural Networks. In Machine Learning and Knowledge Discovery in Databases. Research Track - European
Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part III (Lecture Notes in Computer
Science, Vol. 12977), Nuria Oliver, Fernando PÃ©rez-Cruz, Stefan Kramer, Jesse Read, and JosÃ© A. Lozano (Eds.). Springer,
119â€“134. https://doi.org/10.1007/978-3-030-86523-8_8

[55] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. 2020. Multi-objective Reinforced Evolution in Mobile Neural Architecture
Search. In Computer Vision - ECCV 2020 Workshops - Glasgow, UK, August 23-28, 2020, Proceedings, Part IV (Lecture
Notes in Computer Science, Vol. 12538), Adrien Bartoli and Andrea Fusiello (Eds.). Springer, 99â€“113. https://doi.org/10.
1007/978-3-030-66823-5_6

[56] Carlos A. Coello Coello, Gary B. Lamont, and David A. van Veldhuizen. 2007. Evolutionary Algorithms for Solving

Multi-Objective Problems. Springer US. https://books.google.de/books?id=rXIuAMw3lGAC

[57] Carlos A. Coello Coello and Margarita R. Sierra. 2004. A Study of the Parallelization of a Coevolutionary Multi-
objective Evolutionary Algorithm. In MICAI 2004: Advances in Artificial Intelligence, Third Mexican International
Conference on Artificial Intelligence, Mexico City, Mexico, April 26-30, 2004, Proceedings (Lecture Notes in Computer
Science, Vol. 2972), RaÃºl Monroy, Gustavo Arroyo-Figueroa, Luis Enrique Sucar, and Juan Humberto Sossa Azuela
(Eds.). Springer, 688â€“697.

[58] Corinna Cortes and Mehryar Mohri. 2003. AUC optimization vs. error rate minimization. Advances in neural

information processing systems 16 (2003), 313â€“320.

[59] Paulo Cortez, AntÃ³nio Cerdeira, Fernando Almeida, Telmo Matos, and JosÃ© Reis. 2009. Modeling wine preferences by
data mining from physicochemical properties. Decis. Support Syst. 47, 4 (2009), 547â€“553. https://doi.org/10.1016/j.dss.
2009.05.016

[60] Piotr CzyzÅ¼ak and Adrezej Jaszkiewicz. 1998. Pareto simulated annealingâ€”a metaheuristic technique for multiple-

objective combinatorial optimization. Journal of Multi-Criteria Decision Analysis 7, 1 (1998), 34â€“47.

[61] Manoranjan Dash and Huan Liu. 1997. Feature selection for classification.

Intelligent Data Analysis 1, 3 (1997),

131â€“156.

[62] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. 2021. Parallel Bayesian Optimization of Multiple Noisy
Objectives with Expected Hypervolume Improvement. In Advances in Neural Information Processing Systems.
[63] Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. In Proceedings of

the 23rd international conference on Machine learning. 233â€“240.

[64] Erik A. Daxberger, Anastasia Makarova, Matteo Turchetta, and Andreas Krause. 2020. Mixed-Variable Bayesian
Optimization. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020,
Christian Bessiere (Ed.). ijcai.org, 2633â€“2639. https://doi.org/10.24963/ijcai.2020/365

[65] Kalyanmoy Deb. 2001. Multi-Objective Optimization using Evolutionary Algorithms. Wiley. https://books.google.de/

books?id=OSTn4GSy2uQC

Multi-Objective Hyperparameter Optimization â€“ An Overview

41

[66] Kalyan Deb and Himanshu Gupta. 2006. Introducing Robustness in Multi-Objective Optimization. Evolutionary

computation 14 (2006), 463â€“94.

[67] Kalyanmoy Deb and Himanshu Jain. 2014. An Evolutionary Many-Objective Optimization Algorithm Using Reference-
Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints. Evolutionary Compu-
tation, IEEE Transactions on 18 (08 2014), 577â€“601. https://doi.org/10.1109/TEVC.2013.2281535

[68] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. 2002. A fast and elitist multiobjective genetic

algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation 6, 2 (2002), 182â€“197.

[69] Dwyer S. Deighan, Scott E. Field, Collin D. Capano, and Gaurav Khanna. 2021. Genetic-algorithm-optimized neural
networks for gravitational wave classification. Neural Computing and Applications 33, 20 (2021), 13859â€“13883.
[70] Erick Delage and Yinyu Ye. 2010. Distributionally Robust Optimization Under Moment Uncertainty with Application
to Data-Driven Problems. Operations Research 58 (06 2010), 595â€“612. https://doi.org/10.1287/opre.1090.0741
[71] Janez DemÅ¡ar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets. Journal of Machine Learning

Research 7, 1 (2006), 1â€“30. http://jmlr.org/papers/v7/demsar06a.html

[72] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. 2018. PPP-Net: Platform-aware Progressive
Search for Pareto-optimal Neural Architectures. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net. https://openreview.
net/forum?id=B1NT3TAIM

[73] Jin-Dong Dong, A. Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. 2018. PPP-Net: Platform-aware Progressive Search

for Pareto-optimal Neural Architectures. In ICLR.

[74] John Duchi and Hongseok Namkoong. 2020. Learning Models with Uniform Performance via Distributionally Robust

Optimization. arXiv:1810.08750 [stat.ML]

[75] Darrin C. Edwards, Charles E. Metz, and Robert M. Nishikawa. 2005. The hypervolume under the ROC hypersurface
of" near-guessing" and" near-perfect" observers in N-class classification tasks. IEEE transactions on medical imaging
24, 3 (2005), 293â€“299.

[76] Katharina Eggensperger, Philipp MÃ¼ller, Neeratyoy Mallik, Matthias Feurer, RenÃ© Sass, Aaron Klein, Noor H. Awad,
Marius Lindauer, and Frank Hutter. 2021. HPOBench: A Collection of Reproducible Multi-Fidelity Benchmark
Problems for HPO. CoRR abs/2109.06716 (2021).

[77] Matthias Ehrgott. 2005. Multicriteria Optimization (2. ed.). Springer. https://doi.org/10.1007/3-540-27659-9
[78] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Efficient Multi-Objective Neural Architecture Search
via Lamarckian Evolution. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net.

[79] Michael T. M. Emmerich, Nicola Beume, and Boris Naujoks. 2005. An EMO algorithm using the hypervolume measure
as selection criterion. In International Conference on Evolutionary Multi-Criterion Optimization. Springer, 62â€“76.
[80] Michael T. M. Emmerich and AndrÃ© H. Deutz. 2018. A tutorial on multiobjective optimization: fundamentals and

evolutionary methods. Natural computing 17, 3 (2018), 585â€“609.

[81] Michael T. M. Emmerich, AndrÃ© H. Deutz, and Jan Willem Klinkenberg. 2011. Hypervolume-based expected improve-
ment: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC).
IEEE, 2147â€“2154.

[82] Michael T. M. Emmerich, Kyriakos C. Giannakoglou, and Boris Naujoks. 2006. Single-and multiobjective evolutionary
optimization assisted by Gaussian random field metamodels. IEEE Transactions on Evolutionary Computation 10, 4
(2006), 421â€“439.

[83] Michael T. M. Emmerich, Kaifeng Yang, AndrÃ© H. Deutz, Hao Wang, and Carlos M. Fonseca. 2016. A multicriteria
generalization of bayesian global optimization. In Advances in Stochastic and Deterministic Global Optimization.
Springer, 229â€“242.

[84] Richard M. Everson and Jonathan E. Fieldsend. 2006. Multi-class ROC analysis from a multi-objective optimisation

perspective. Pattern Recognition Letters 27, 8 (2006), 918â€“927.

[85] Stefan Falkner, Aaron Klein, and Frank Hutter. 2018. BOHB: Robust and Efficient Hyperparameter Optimization at
Scale. In Proceedings of the 35th International Conference on Machine Learning, ICML (Proceedings of Machine Learning
Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 1436â€“1445.

[86] Xinjie Fan, Yuguang Yue, Purnamrita Sarkar, and YX Rachel Wang. 2020. On hyperparameter tuning in general

clustering problemsm. In International Conference on Machine Learning. PMLR, 2996â€“3007.

[87] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern recognition letters 27, 8 (2006), 861â€“874.
[88] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. 2015.
Efficient and robust automated machine learning. In Advances in neural information processing systems. 2962â€“2970.
[89] Jonathan E. Fieldsend. 2009. Optimizing Decision Trees Using Multi-objective Particle Swarm Optimization. Springer

Berlin Heidelberg, Berlin, Heidelberg, 93â€“114.

42

Karl and Pielok, et al.

[90] Jonathan E. Fieldsend and Richard M. Everson. 2005. Formulation and comparison of multi-class ROC surfaces.

(2005).

[91] Jonathan E. Fieldsend and Richard M. Everson. 2015. The Rolling Tide Evolutionary Algorithm: A Multiobjective
Optimizer for Noisy Optimization Problems. IEEE Transactions on Evolutionary Computation 19, 1 (2015), 103â€“117.
https://doi.org/10.1109/TEVC.2014.2304415

[92] Carlos M. Fonseca and Peter J. Fleming. 1996. On the performance assessment and comparison of stochastic
multiobjective optimizers. In International Conference on Parallel Problem Solving from Nature. Springer, 584â€“593.
[93] Alexander I. J. Forrester, AndrÃ¡s SÃ³bester, and Andy J. Keane. 2007. Multi-fidelity optimization via surrogate modelling.
Proceedings of the royal society a: mathematical, physical and engineering sciences 463, 2088 (2007), 3251â€“3269.
[94] Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P. Hamilton, and Derek
Roth. 2019. A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the
Conference on Fairness, Accountability, and Transparency, FAT* 2019, Atlanta, GA, USA, January 29-31, 2019, danah
boyd and Jamie H. Morgenstern (Eds.). ACM, 329â€“338. https://doi.org/10.1145/3287560.3287589

[95] Johannes FÃ¼rnkranz and Eyke HÃ¼llermeier. 2010. Preference learning. Springer.
[96] AntÃ³nio Gaspar-Cunha and JosÃ© A. Covas. 2006. Robustness using Multi-Objective Evolutionary Algorithms. In

Applications of Soft Computing. Springer Berlin Heidelberg, Berlin, Heidelberg, 353â€“362.

[97] Justin Gilmer, Nicolas Ford, Nicholas Carlini, and Ekin Cubuk. 2019. Adversarial Examples Are a Natural Consequence
of Test Error in Noise (Proceedings of Machine Learning Research, Vol. 97). PMLR, Long Beach, California, USA,
2280â€“2289. http://proceedings.mlr.press/v97/gilmer19a.html

[98] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples.
In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6572

[99] Julia Guerrero-Viu, Sven Hauns, Sergio Izquierdo, Guilherme Miotto, Simon Schrodi, Andre Biedenkapp, Thomas
Elsken, Difan Deng, Marius Lindauer, and Frank Hutter. 2021. Bag of Baselines for Multi-objective Joint Neural
Architecture Search and Hyperparameter Optimization. arXiv:2105.01015

[100] Ayla GÃ¼lcÃ¼ and Zeki KuÅŸ. 2021. Multi-objective simulated annealing for hyper-parameter optimization in convolutional

neural networks. PeerJ Computer Science 7 (2021), e338.

[101] Vassil Guliashki, Hristo Toshev, and Chavdar Korsemov. 2008. Survey of Evolutionary Algorithms used in multiobjec-

tive optimization. 60 (11 2008).

[102] Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua Lin. 2020. When nas meets robustness: In search of robust
architectures against adversarial attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 631â€“640.

[103] Walter J. Gutjahr and Alois Pichler. 2016. Stochastic multi-objective optimization: a survey on non-scalarizing

methods. Annals of Operations Research 236, 2 (2016), 475â€“499.

[104] Isabelle Guyon and AndrÃ© Elisseeff. 2003. An Introduction to Variable and Feature Selection. Journal of Machine

Learning Research 3 (2003), 1157â€“1182. http://jmlr.org/papers/v3/guyon03a.html

[105] David Hand and Robert Till. 2001. A Simple Generalisation of the Area Under the ROC Curve for Multiple Class

Classification Problems. Hand, The 45 (11 2001), 171â€“186. https://doi.org/10.1023/A:1010920819831

[106] Michael P. Hansen and Andrzej Jaszkiewicz. 1998. Evaluating the quality of approximations to the non-dominated

set.

[107] Moritz Hardt, Eric Price, Nati Srebro, et al. 2016. Equality of opportunity in supervised learning. In Advances in neural

information processing systems. 3315â€“3323.

[108] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition. 770â€“778.

[109] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2020. AutoML: A Survey of the State-of-the-Art. arXiv:1908.00709 [cs.LG]
[110] Daniel HernÃ¡ndez-Lobato, JosÃ© Miguel HernÃ¡ndez-Lobato, Amar Shah, and Ryan P. Adams. 2016. Predictive entropy
search for multi-objective bayesian optimization. In International Conference on Machine Learning. 1492â€“1501.
[111] JosÃ© Miguel HernÃ¡ndez-Lobato, Michael A. Gelbart, Brandon Reagen, Robert Adolf, Daniel HernÃ¡ndez-Lobato, Paul N.
Whatmough, David Brooks, Gu-Yeon Wei, and Ryan P. Adams. 2016. Designing neural network hardware accelerators
with decoupled objective evaluations. In NIPS workshop on Bayesian Optimization. 10.

[112] Kenneth Holstein, Jennifer Wortman Vaughan, Hal DaumÃ© III, Miroslav DudÃ­k, and Hanna M. Wallach. 2019. Improving
Fairness in Machine Learning Systems: What Do Industry Practitioners Need?. In Proceedings of the Conference on
Human Factors in Computing Systems, CHI 2019, Stephen A. Brewster, Geraldine Fitzpatrick, Anna L. Cox, and Vassilis
Kostakos (Eds.). ACM, 600. https://doi.org/10.1145/3290605.3300830

[113] Daniel Horn. 2010. Multi-objective analysis of machine learning algorithms using model-based optimization techniques.

Ph. D. Dissertation. Technische UniversitÃ¤t Dortmund.

Multi-Objective Hyperparameter Optimization â€“ An Overview

43

[114] Daniel Horn and Bernd Bischl. 2016. Multi-objective parameter configuration of machine learning algorithms using

model-based optimization. In 2016 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 1â€“8.

[115] Daniel Horn, Melanie Dagge, Xudong Sun, and Bernd Bischl. 2017. First Investigations on Noisy Model-Based
Multi-objective Optimization. Lecture Notes in Computer Science 10173, 298â€“313. https://doi.org/10.1007/978-3-319-
54157-0_21

[116] Daniel Horn, Tobias Wagner, Dirk Biermann, Claus Weihs, and Bernd Bischl. 2015. Model-based multi-objective
optimization: taxonomy, multi-point proposal, toolbox and benchmark. In International Conference on Evolutionary
Multi-Criterion Optimization. Springer, 64â€“78.

[117] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,
and Hartwig Adam. 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications.
arXiv:1704.04861

[118] Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping Chou, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu
Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng Juan. 2018. Monas: Multi-objective neural architecture search using
reinforcement learning. arXiv preprint arXiv:1806.10332 (2018).

[119] Deng Huang, Theodore T. Allen, William I. Notz, and R. Allen Miller. 2006. Sequential kriging optimization using

multiple-fidelity evaluations. Structural and Multidisciplinary Optimization 32, 5 (2006), 369â€“382.

[120] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. 2017. Densely connected convolutional

networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4700â€“4708.

[121] Jin Huang and Charles X. Ling. 2005. Using AUC and accuracy in evaluating learning algorithms. IEEE Transactions

on knowledge and Data Engineering 17, 3 (2005), 299â€“310.

[122] Evan J. Hughes. 2001. Evolutionary multi-objective ranking with uncertainty and noise. In International Conference

on Evolutionary Multi-Criterion Optimization. Springer, 329â€“343.

[123] Susan R. Hunter, Eric A. Applegate, Viplove Arora, Bryan Chong, Kyle Cooper, Oscar RincÃ³n-Guevara, and Carolina

Vivas-Valencia. 2019. An Introduction to Multiobjective Simulation Optimization. 29, 1 (2019).

[124] Aminul Huq and Mst. Tasnim Pervin. 2020. Adversarial Attacks and Defense on Texts: A Survey. arXiv:2005.14108
[125] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2011. Sequential model-based optimization for general
algorithm configuration. In International conference on learning and intelligent optimization. Springer, 507â€“523.
[126] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2018. Automated Machine Learning: Methods, Systems,

Challenges. Springer. In press, available at http://automl.org/book..

[127] Beatriz Iglesia, M.S. Philpott, Anthony Bagnall, and Victor Rayward-Smith. 2004. Data mining rules using multi-

objective evolutionary algorithms. 1552 â€“ 1559 Vol.3. https://doi.org/10.1109/CEC.2003.1299857

[128] Md Shahriar Iqbal, Jianhai Su, Lars Kotthoff, and Pooyan Jamshidi. 2020. FlexiBO: Cost-Aware Multi-Objective

Optimization of Deep Neural Networks. CoRR abs/2001.06588 (2020).

[129] Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. 2018. How to Specify a Reference Point in Hypervolume
Calculation for Fair Performance Comparison. Evolutionary Computation 26, 3 (2018), 411â€“440. https://doi.org/10.
1162/evco_a_00226

[130] Hisao Ishibuchi, Noritaka Tsukamoto, and Yusuke Nojima. 2008. Evolutionary many-objective optimization: A short
review. In 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence). IEEE,
2419â€“2426.

[131] Himanshu Jain and Kalyanmoy Deb. 2014. An Evolutionary Many-Objective Optimization Algorithm Using Reference-
Point Based Nondominated Sorting Approach, Part II: Handling Constraints and Extending to an Adaptive Approach.
Evolutionary Computation, IEEE Transactions on 18 (08 2014), 602â€“622. https://doi.org/10.1109/TEVC.2013.2281534
[132] Nathalie Japkowicz and Mohak Shah (Eds.). 2011. Evaluating Learning Algorithms: A Classification Perspective.

Cambridge University Press.

[133] Shinkyu Jeong and Shigeru Obayashi. 2005. Efficient global optimization (EGO) for multi-objective problem and data

mining. In 2005 IEEE congress on evolutionary computation, Vol. 3. IEEE, 2138â€“2145.

[134] Shali Jiang, Henry Chai, Javier GonzÃ¡lez, and Roman Garnett. 2019. Efficient nonmyopic Bayesian optimization and

quadrature. arXiv:1909.04568

[135] Yaochu Jin. 2006. Multi-objective machine learning. Vol. 16. Springer Science & Business Media.
[136] Yaochu Jin and JÃ¼rgen Branke. 2005. Evolutionary optimization in uncertain environments-a survey. IEEE Transactions

on Evolutionary Computation 9, 3 (2005), 303â€“317.

[137] Yaochu Jin and Bernhard Sendhoff. 2008. Pareto-based multiobjective machine learning: An overview and case studies.
IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38, 3 (2008), 397â€“415.
[138] Donald R. Jones. 2001. A Taxonomy of Global Optimization Methods Based on Response Surfaces. Journal of Global

Optimization 21, 4 (2001), 345â€“383. https://doi.org/10.1023/A:1012771025575

[139] Donald R. Jones, Matthias Schonlau, and William J. Welch. 1998. Efficient Global Optimization of Expensive Black-Box
Functions. Journal of Global Optimization 13, 4 (Dec. 1998), 455â€“492. https://doi.org/10.1023/A:1008306431147

44

Karl and Pielok, et al.

[140] Chia-Feng Juang and Chia-Hung Hsu. 2014. Structure and parameter optimization of FNNs using multi-objective
ACO for control and prediction. In 2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE). IEEE, 928â€“933.
[141] Faisal Kamiran and Toon Calders. 2012. Data Preprocessing Techniques for Classification without Discrimination.

Knowl. Inf. Syst. 33, 1 (Oct. 2012), 1â€“33. https://doi.org/10.1007/s10115-011-0463-8

[142] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff
Schneider, BarnabÃ¡s PÃ³czos, and Eric P. Xing. 2020. Tuning Hyperparameters without Grad Students: Scalable and
Robust Bayesian Optimisation with Dragonfly. J. Mach. Learn. Res. 21 (2020), 81:1â€“81:27.

[143] Marvin Kaster, Wei Zhao, and Steffen Eger. 2021. Global Explainability of BERT-Based Evaluation Metrics by

Disentangling along Linguistic Factors. arXiv:2110.04399

[144] Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesian deep learning for computer vision?

arXiv:1703.04977

[145] Joon Sik Kim, Jiahao Chen, and Ameet Talwalkar. 2020. FACT: A Diagnostic for Group Fairness Trade-offs. In
Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research,
Vol. 119), Hal DaumÃ© III and Aarti Singh (Eds.). PMLR, 5264â€“5274. http://proceedings.mlr.press/v119/kim20a.html
[146] Ye-Hoon Kim, Bhargava Reddy, Sojung Yun, and Chanwon Seo. 2017. Nemo: Neuro-evolution with multiobjective

optimization of deep neural network for speed and accuracy. In ICML 2017 AutoML Workshop.

[147] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T SchÃ¼tt, Sven DÃ¤hne, Dumitru
Erhan, and Been Kim. 2019. The (un) reliability of saliency methods. In Explainable AI: Interpreting, Explaining and
Visualizing Deep Learning. Springer, 267â€“280.

[148] Aaron Klein, Louis C. Tiao, Thibaut Lienart, Cedric Archambeau, and Matthias Seeger. 2020. Model-based Asynchro-

nous Hyperparameter and Neural Architecture Search. arXiv:2003.10865

[149] Joshua Knowles. 2006. ParEGO: A Hybrid Algorithm with on-Line Landscape Approximation for Expensive Multiob-

jective Optimization Problems. 10, 1 (2006), 50â€“66. https://doi.org/10.1109/TEVC.2005.851274

[150] Joshua Knowles, David Corne, and Alan Reynolds. 2009. Noisy Multiobjective Optimization on a Budget of 250
Evaluations. In Evolutionary Multi-Criterion Optimization, Matthias Ehrgott, Carlos M. Fonseca, Xavier Gandibleux,
Jin-Kao Hao, and Marc Sevaux (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 36â€“50.

[151] Joshua Knowles, Richard Watson, and David Corne. 2001. Reducing Local Optima in Single-Objective Problems by

Multi-objectivization. Lecture Notes in Computer Science. https://doi.org/10.1007/3-540-44719-9_19

[152] Nicolas Knudde, Joachim van der Herten, Tom Dhaene, and Ivo Couckuyt. 2017. GPflowOpt: A Bayesian Optimization

Library using TensorFlow. arXiv:1711.03845

[153] Patrick Koch, Tobias Wagner, Michael T. M. Emmerich, Thomas BÃ¤ck, and Wolfgang Konen. 2015. Efficient multi-

criteria optimization on noisy machine learning problems. Applied Soft Computing 29 (2015), 357â€“370.

[154] Ron Kohavi and George H. John. 2002. Wrappers for feature subset selection. Artificial Intelligence 97, 1-2 (2002),

273â€“324.

[155] Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. Principles of explanatory debugging
to personalize interactive machine learning. In Proceedings of the 20th international conference on intelligent user
interfaces. 126â€“137.

[156] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. 2020.
Fairness without Demographics through Adversarially Reweighted Learning. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/07fc15c9d169ee48573edd749d25945d-Abstract.html

[157] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates,
Inc.

[158] Adriana Lara, Gustavo Sanchez, Carlos A. Coello Coello, and Oliver Schutze. 2010. HCS: A New Local Search Strategy
for Memetic Multiobjective Evolutionary Algorithms. IEEE Transactions on Evolutionary Computation 14, 1 (2010),
112â€“132. https://doi.org/10.1109/TEVC.2009.2024143

[159] Alfred Laugros, Alice Caplier, and Matthieu Ospici. 2019. Are adversarial robustness and common perturbation
robustness independant attributes?. In Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops.

[160] Yann LeCun, Patrick Haffner, LÃ©on Bottou, and Yoshua Bengio. 1999. Object recognition with gradient-based learning.

In Shape, contour and grouping in computer vision. Springer, 319â€“345.

[161] Loo-Hay Lee, Chew Ek Peng, Teng Suyan, and Li Juxin. 2009. Application of evolutionary algorithms for solving
multi-objective simulation optimization problems. In Multi-objective memetic algorithms. Springer, 91â€“110.
[162] Julien-Charles Levesque, Audrey Durand, Christian GagnÃ©, and Robert Sabourin. 2017. Bayesian optimization for
conditional hyperparameter spaces. In 2017 International Joint Conference on Neural Networks, IJCNN 2017, Anchorage,

Multi-Objective Hyperparameter Optimization â€“ An Overview

45

AK, USA, May 14-19, 2017. IEEE, 286â€“293. https://doi.org/10.1109/IJCNN.2017.7965867

[163] Julien-Charles LÃ©vesque, Audrey Durand, Christian GagnÃ©, and Robert Sabourin. 2012. Multi-objective evolutionary
optimization for generating ensembles of classifiers in the ROC space. In Proceedings of the 14th annual conference on
Genetic and evolutionary computation. 879â€“886.

[164] Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2017. Hyperband: A
Novel Bandit-Based Approach to Hyperparameter Optimization. J. Mach. Learn. Res. 18 (2017), 185:1â€“185:52. http:
//jmlr.org/papers/v18/16-558.html

[165] Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin
Recht, and Ameet Talwalkar. 2020. A System for Massively Parallel Hyperparameter Tuning. In Proceedings of Machine
Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020, Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
and Vivienne Sze (Eds.). mlsys.org.

[166] Miqing Li and Xin Yao. 2019. Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey. Comput.

Surveys 52 (03 2019), 1â€“38. https://doi.org/10.1145/3300148

[167] Rui Li, Michael T. M. Emmerich, Jeroen Eggermont, Thomas BÃ¤ck, Martin SchÃ¼tz, Jouke Dijkstra, and Johan H. C.
Reiber. 2013. Mixed Integer Evolution Strategies for Parameter Optimization. Evolutionary Computation 21, 1 (2013),
29â€”-64.

[168] Yang Li, Yu Shen, Wentao Zhang, Yuanwei Chen, Huaijun Jiang, Mingchao Liu, Jiawei Jiang, Jinyang Gao, Wentao
Wu, Zhi Yang, Ce Zhang, and Bin Cui. 2021. OpenBox: A Generalized Black-box Optimization Service. In KDD â€™21:
The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,
2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 3209â€“3219. https://doi.org/10.1145/3447548.3467061
[169] Jason Liang, Elliot Meyerson, Babak Hodjat, Dan Fink, Karl Mutch, and Risto Miikkulainen. 2019. Evolutionary neural
automl for deep learning. In Proceedings of the Genetic and Evolutionary Computation Conference. 401â€“409.
[170] Thomas Liddle, Mark Johnston, and Mengjie Zhang. 2010. Multi-objective genetic programming for object detection.

In IEEE Congress on Evolutionary Computation. IEEE, 1â€“8.

[171] Zachary C. Lipton. 2018. The mythos of model interpretability. Commun. ACM 61, 10 (2018), 36â€“43.

https:

//doi.org/10.1145/3233231

[172] Haitao Liu, Jianfei Cai, and Yew-Soon Ong. 2018. Remarks on multi-output Gaussian process regression. Knowl.

Based Syst. 144 (2018), 102â€“121. https://doi.org/10.1016/j.knosys.2017.12.034

[173] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2018. Hierarchical

Representations for Efficient Architecture Search. In International Conference on Learning Representations.

[174] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti PietikÃ¤inen. 2020. Deep
learning for generic object detection: A survey. International journal of computer vision 128, 2 (2020), 261â€“318.
[175] Aleksei Liuliakov and Barbara Hammer. 2021. AutoML Technologies for the Identification of Sparse Models. In

International Conference on Intelligent Data Engineering and Automated Learning. Springer, 65â€“75.

[176] Anton Lokhmotov, Nikolay Chunosov, Flavio Vella, and Grigori Fursin. 2018. Multi-objective autotuning of mobilenets
across the full software/hardware stack. In Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament
on Co-designing Pareto-efficient Deep Learning. 1.

[177] Mohammad Loni, Sima Sinaei, Ali Zoljodi, Masoud Daneshtalab, and Mikael SjÃ¶din. 2020. DeepMaker: A multi-
objective optimization framework for deep neural networks in embedded systems. Microprocess. Microsystems 73
(2020), 102989. https://doi.org/10.1016/j.micpro.2020.102989

[178] Mohammad Loni, Ali Zoljodi, Sima Sinaei, Masoud Daneshtalab, and Mikael SjÃ¶din. 2019. NeuroPower: Designing
Energy Efficient Convolutional Neural Network Architecture for Embedded Systems. In Artificial Neural Networks and
Machine Learning - ICANN 2019: Theoretical Neural Computation - 28th International Conference on Artificial Neural
Networks, Munich, Germany, September 17-19, 2019, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 11727),
Igor V. Tetko, Vera KurkovÃ¡, Pavel Karpov, and Fabian J. Theis (Eds.). Springer, 208â€“222. https://doi.org/10.1007/978-
3-030-30487-4_17

[179] Christos Louizos, Max Welling, and Diederik P. Kingma. 2017. Learning Sparse Neural Networks through L0

Regularization. arXiv:1712.01312

[180] Zhichao Lu, Kalyanmoy Deb, and Vishnu Naresh Boddeti. 2020. Muxconv: Information multiplexing in convolutional
neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12044â€“12053.
[181] Zhichao Lu, Kalyanmoy Deb, Erik Goodman, Wolfgang Banzhaf, and Vishnu Naresh Boddeti. 2020. Nsganetv2:
Evolutionary multi-objective surrogate-assisted neural architecture search. In European Conference on Computer
Vision. Springer, 35â€“51.

[182] Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, and Wolfgang Banzhaf.
2019. Nsga-net: neural architecture search using multi-objective genetic algorithm. In Proceedings of the Genetic and
Evolutionary Computation Conference. 419â€“427.

46

Karl and Pielok, et al.

[183] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in
Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (Eds.). Curran Associates, Inc., 4765â€“4774. http://papers.nips.cc/paper/7062-a-unified-approach-to-
interpreting-model-predictions.pdf

[184] Gang Luo. 2016. A review of automatic selection methods for machine learning algorithms and hyper-parameter

values. Network Modeling Analysis in Health Informatics and Bioinformatics 5, 1 (2016).

[185] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards
Deep Learning Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https:
//openreview.net/forum?id=rJzIBfZAb

[186] Gustavo Malkomes and Roman Garnett. 2018. Automating Bayesian optimization with Bayesian optimization. In

Advances in Neural Information Processing Systems. 5984â€“5994.

[187] Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. 2019. Metric learning for adversarial

robustness. Advances in Neural Information Processing Systems 32 (2019).

[188] Natalia Martinez, Martin Bertran, and Guillermo Sapiro. 2020. Minimax Pareto Fairness: A Multi Objective Perspective.
In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event
(Proceedings of Machine Learning Research, Vol. 119). PMLR, 6755â€“6764. http://proceedings.mlr.press/v119/martinez20a.
html

[189] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and

fairness in machine learning. ACM Computing Surveys (CSUR) 54, 6 (2021), 1â€“35.

[190] Ingo Mierswa. 2009. Non-Convex and Multi-Objective Optimization in Data Mining. Ph. D. Dissertation. Technische

UniversitÃ¤t Dortmund.

[191] Ingo Mierswa and Michael Wurst. 2006. Information preserving multi-objective feature selection for unsupervised

learning. In Proceedings of the 8th annual conference on Genetic and evolutionary computation. 1545â€“1552.

[192] Kaisa Miettinen. 2012. Nonlinear Multiobjective Optimization. Springer US. https://books.google.de/books?id=

bnzjBwAAQBAJ

[193] Fan Min, Qinghua Hu, and William Zhu. 2014. Feature selection with test cost constraint. International Journal of

Approximate Reasoning 55, 1 (2014), 167â€“179.

[194] Christoph Molnar. 2019. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.
[195] Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. 2019. Quantifying Interpretability of Arbitrary Machine

Learning Models Through Functional Decomposition. arXiv:1904.03867

[196] Pablo Moreno-MuÃ±oz, Antonio ArtÃ©s, and Mauricio Alvarez. 2018. Heterogeneous multi-output gaussian process

prediction. In Advances in neural information processing systems. 6711â€“6720.

[197] Luigi Nardi, David Koeplinger, and Kunle Olukotun. 2019. Practical Design Space Exploration. In 27th IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2019, Rennes,
France, October 21-25, 2019. IEEE Computer Society, 347â€“358. https://doi.org/10.1109/MASCOTS.2019.00045
[198] Renata F. P. Neves, Alberto N. G. Lopes Filho, Carlos A. B. Mello, and Cleber Zanchettin. 2011. A SVM based off-line
handwritten digit recognizer. In 2011 IEEE International Conference on Systems, Man, and Cybernetics. IEEE, 510â€“515.
[199] Xing Niu, Prashant Mathur, Georgiana Dinu, and Yaser Al-Onaizan. 2020. Evaluating Robustness to Input Perturbations

for Neural Machine Translation. arXiv:2005.00580 [cs.CL]

[200] Randal S. Olson and Jason H. Moore. 2016. TPOT: A tree-based pipeline optimization tool for automating machine

learning. In Workshop on automatic machine learning. PMLR, 66â€“74.

[201] Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore. 2016.
Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 â€“
April 1, 2016, Proceedings, Part I. Springer International Publishing, Chapter Automating Biomedical Data Science
Through Tree-Based Pipeline Optimization, 123â€“137. https://doi.org/10.1007/978-3-319-31204-0_9

[202] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social data: Biases, methodological

pitfalls, and ethical boundaries. Frontiers in Big Data 2 (2019), 13.

[203] Julio-Omar Palacio-NiÃ±o and Fernando Berzal. 2019. Evaluation metrics for unsupervised learning algorithms. arXiv

preprint arXiv:1905.05667 (2019).

[204] Anna Palczewska, Jan Palczewski, Richard Marchese Robinson, and Daniel Neagu. 2014. Interpreting random forest
classification models using a feature contribution method. In Integration of reusable systems. Springer, 193â€“218.
[205] Taejin Park and Kwang Ryel Ryu. 2011. Accumulative Sampling for Noisy Evolutionary Multi-Objective Optimization.
In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation (Dublin, Ireland) (GECCO â€™11).
Association for Computing Machinery, New York, NY, USA, 793â€“800. https://doi.org/10.1145/2001576.2001684
[206] Maryam Parsa, Aayush Ankit, Amirkoushyar Ziabari, and Kaushik Roy. 2019. Pabo: Pseudo agent-based multi-
objective bayesian hyperparameter optimization for efficient neural accelerator design. In 2019 IEEE/ACM International

Multi-Objective Hyperparameter Optimization â€“ An Overview

47

Conference on Computer-Aided Design (ICCAD). IEEE, 1â€“8.

[207] Maryam Parsa, John P. Mitchell, Catherine D. Schuman, Robert M. Patton, Thomas E. Potok, and Kaushik Roy. 2020.
Bayesian multi-objective hyperparameter optimization for accurate, fast, and efficient neural network accelerator
design. Frontiers in neuroscience 14 (2020), 667.

[208] Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, and Nassir Navab. 2018. Generalizability vs. Robustness:

Adversarial Examples for Medical Imaging. arXiv:1804.00504 [cs.CV]

[209] Jonathan Peck, Joris Roels, Bart Goossens, and Yvan Saeys. 2017. Lower bounds on the robustness to adversarial
perturbations. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 804â€“813. http://papers.nips.cc/paper/6682-
lower-bounds-on-the-robustness-to-adversarial-perturbations.pdf

[210] Guilherme D. Pelegrina, Renan DB Brotto, Leonardo T. Duarte, Romis Attux, and JoÃ£o M. T. Romano. 2020. A novel
multi-objective-based approach to analyze trade-offs in Fair Principal Component Analysis. arXiv:2006.06137
[211] Valerio Perrone, Michele Donini, Krishnaram Kenthapadi, and CÃ©dric Archambeau. 2020. Fair Bayesian Optimization.

arXiv:2006.05109 [stat.ML]

[212] Dana Pessach and Erez Shmueli. 2020. Algorithmic fairness. arXiv:2001.09784
[213] Florian Pfisterer, Stefan Coors, Janek Thomas, and Bernd Bischl. 2019. Multi-Objective Automatic Machine Learning

with AutoxgboostMC. arXiv:1908.10796 [stat.ML]

[214] Florian Pfisterer, Lennart Schneider, Julia Moosbauer, Martin Binder, and Bernd Bischl. 2021. YAHPO Gym - Design
Criteria and a new Multifidelity Benchmark for Hyperparameter Optimization. CoRR abs/2109.03670 (2021).
[215] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. 2018. Efficient neural architecture search via

parameter sharing. arXiv:1802.03268

[216] Iana S Polonskaia, Nikolay O Nikitin, Ilia Revin, Pavel Vychuzhanin, and Anna V Kalyuzhnaya. 2021. Multi-objective
evolutionary design of composite data-driven models. In 2021 IEEE Congress on Evolutionary Computation (CEC).
IEEE, 926â€“933.

[217] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze. 2008. Multiobjective Optimization on a
Limited Budget of Evaluations Using Model-Assisted S-Metric Selection. In Parallel Problem Solving from Nature -
PPSN X (Lecture Notes in Computer Science). Springer, Berlin, Heidelberg, 784â€“794. https://doi.org/10.1007/978-3-
540-87700-4_78

[218] Hao Qin, Takahiro Shinozaki, and Kevin Duh. 2017. Evolution strategy based automatic tuning of neural machine

translation systems. In Proc. 14th Int. Workshop Spoken Language Transl. 120â€“128.

[219] Hamed Rahimian and Sanjay Mehrotra. 2019.

Distributionally Robust Optimization: A Review.

arXiv:1908.05659 [math.OC]

[220] Aghila Rajagopal, Gyanendra Prasad Joshi, A. Ramachandran, R. T. Subhalakshmi, Manju Khari, Sudan Jha, K. Shankar,
and Jinsang You. 2020. A Deep Learning Model Based on Multi-Objective Particle Swarm Optimization for Scene
Classification in Unmanned Aerial Vehicles. IEEE Access 8 (2020), 135383â€“135393. https://doi.org/10.1109/ACCESS.
2020.3011502

[221] Jonas Rauber, Wieland Brendel, and Matthias Bethge. 2018. Foolbox: A Python toolbox to benchmark the robustness

of machine learning models. arXiv:1707.04131 [cs.LG]

[222] Brandon Reagen, JosÃ© Miguel HernÃ¡ndez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei,
and David Brooks. 2017. A case for efficient accelerator design space exploration via bayesian optimization. In 2017
IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). IEEE, 1â€“6.

[223] Sebastian Rojas Gonzalez, Hamed Jalali, and Inneke Van Nieuwenhuyse. 2020. A multiobjective stochastic simulation
optimization algorithm. European Journal of Operational Research 284, 1 (2020), 212â€“226. https://doi.org/10.1016/j.
ejor.2019.12.014

[224] Sebastian Rojas-Gonzalez and Inneke Van Nieuwenhuyse. 2020. A survey on kriging-based infill algorithms for

multiobjective simulation optimization. Computers & Operations Research 116 (2020), 104869.

[225] Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, and Mitesh M. Khapra. 2021. Perturbation checklists

for evaluating NLG evaluation metrics. arXiv:2109.05771

[226] Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. 2018.

Aequitas: A Bias and Fairness Audit Toolkit. arXiv:1811.05577

[227] David Salinas, Valerio Perrone, Olivier Cruchant, and CÃ©dric Archambeau. 2021. A multi-objective perspective on

jointly tuning hardware and hyperparameters. arXiv:2106.05680

[228] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern
recognition. 4510â€“4520.

[229] Iqbal H. Sarker. 2021. Machine learning: Algorithms, real-world applications and research directions. SN Computer

Science 2, 3 (2021), 1â€“21.

48

Karl and Pielok, et al.

[230] Robin Schmucker, Michele Donini, Valerio Perrone, Muhammad Bilal Zafar, and Cedric Archambeau. 2020. Multi-
Objective Multi-Fidelity Hyperparameter Optimization with Application to Fairness. NeurIPS Workshop on Meta-
Learning (2020).

[231] Robin Schmucker, Michele Donini, Muhammad Bilal Zafar, David Salinas, and CÃ©dric Archambeau. 2021. Multi-

objective Asynchronous Successive Halving. arXiv:2106.12639

[232] Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and
abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency.
59â€“68.

[233] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE
international conference on computer vision. 618â€“626.

[234] Amar Shah and Zoubin Ghahramani. 2016. Pareto frontier learning with expensive correlated objectives. In Interna-

tional conference on machine learning. PMLR, 1919â€“1927.

[235] Yakun Sophia Shao, Brandon Reagen, Gu-Yeon Wei, and David Brooks. 2014. Aladdin: A pre-rtl, power-performance
accelerator simulator enabling large design space exploration of customized architectures. In 2014 ACM/IEEE 41st
International Symposium on Computer Architecture (ISCA). IEEE, 97â€“108.

[236] Takahiro Shinozaki, Shinji Watanabe, and Kevin Duh. 2020. Automated Development of DNN Based Spoken Language

Systems Using Evolutionary Algorithms. In Deep Neural Evolution. Springer, 97â€“129.

[237] Margarita R. Sierra and Carlos A. Coello Coello. 2005. Improving PSO-based multi-objective optimization using
crowding, mutation and epsilon-dominance. In International conference on evolutionary multi-criterion optimization.
Springer, 505â€“519.

[238] Lewis Smith and Yarin Gal. 2018. Understanding measures of uncertainty for adversarial example detection.

arXiv:1803.08533

[239] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian Optimization of Machine Learning
Algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information
Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, Peter L.
Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, LÃ©on Bottou, and Kilian Q. Weinberger (Eds.). 2960â€“2968.
[240] Evgenii Sopov and Ilia Ivanov. 2015. Self-Configuring Ensemble of Neural Network Classifiers for Emotion Recognition
in the Intelligent Human-Machine Interaction. In IEEE Symposium Series on Computational Intelligence, SSCI 2015,
Cape Town, South Africa, December 7-10, 2015. IEEE, 1808â€“1815. https://doi.org/10.1109/SSCI.2015.252

[241] Thorsten Suttorp and Christian Igel. 2006. Multi-Objective Optimization of Support Vector Machines. In Multi-
Objective Machine Learning, Yaochu Jin (Ed.). Studies in Computational Intelligence, Vol. 16. Springer, 199â€“220.
[242] Ivan Svetunkov. 2022. Forecasting and Analytics with ADAM. Retrieved May 18, 2008 from https://openforecast.org/

adam/

[243] Anna Syberfeldt, Amos Ng, Robert John, and Philip Moore. 2010. Evolutionary optimisation of noisy multi-objective
problems using confidence-based dynamic resampling. European Journal of Operational Research 204 (08 2010),
533â€“544. https://doi.org/10.1016/j.ejor.2009.11.003

[244] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew G. Howard, and Quoc V. Le. 2019.
Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2820â€“2828.

[245] Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In

International Conference on Machine Learning. PMLR, 6105â€“6114.

[246] Tomohiro Tanaka, Takafumi Moriya, Takahiro Shinozaki, Shinji Watanabe, Takaaki Hori, and Kevin Duh. 2016.
Automated structure discovery and parameter tuning of neural network language model based on evolution strategy.
In 2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 665â€“671.

[247] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. 2020. Measuring

Robustness to Natural Distribution Shifts in Image Classification. arXiv:2007.00644 [cs.LG]

[248] Antonio Torralba and Alexei A. Efros. 2011. Unbiased look at dataset bias. In CVPR 2011. 1521â€“1528.

https:

//doi.org/10.1109/CVPR.2011.5995347

[249] Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. Ensemble

adversarial training: Attacks and defenses. arXiv:1705.07204

[250] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. 2019. Robustness
May Be at Odds with Accuracy. In 7th International Conference on Learning Representations, ICLR. OpenReview.net.
https://openreview.net/forum?id=SyxAb30cY7

[251] Tea TuÅ¡ar and Bogdan FilipiÄ. 2014. Visualization of Pareto Front Approximations in Evolutionary Multiobjective
Optimization: A Critical Review and the Prosection Method. IEEE Transactions on Evolutionary Computation 19 (01
2014), 1â€“1. https://doi.org/10.1109/TEVC.2014.2313407

Multi-Objective Hyperparameter Optimization â€“ An Overview

49

[252] Vimal Vachhani, Vipul Dabhi, and Harshadkumar Prajapati. 2015. Survey of Multi Objective Evolutionary Algorithms.

https://doi.org/10.1109/ICCPCT.2015.7159422

[253] Carlos Villacampa-Calvo, Bryan Zaldivar, Eduardo C. Garrido-MerchÃ¡n, and Daniel HernÃ¡ndez-Lobato. 2020. Multi-

class Gaussian Process Classification with Noisy Inputs. arXiv:2001.10523

[254] Bin Wang, Yanan Sun, Bing Xue, and Mengjie Zhang. 2019. Evolving Deep Neural Networks by Multi-objective

Particle Swarm Optimization for Image Classification. arXiv:1904.09035 [cs.NE]

[255] Chunnan Wang, Hongzhi Wang, Guocheng Feng, and Fei Geng. 2020. Multi-Objective Neural Architecture Search

Based on Diverse Structures and Adaptive Recommendation. arXiv:2007.02749

[256] Handing Wang, Markus Olhofer, and Yaochu Jin. 2017. A mini-review on preference modeling and articulation in
multi-objective optimization: current status and challenges. Complex & Intelligent Systems 3, 4 (2017), 233â€“245.
[257] Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando de Freitas. 2016. Bayesian Optimization in a
Billion Dimensions via Random Embeddings. J. Artif. Intell. Res. 55 (2016), 361â€“387. https://doi.org/10.1613/jair.4806
[258] Colin White, Willie Neiswanger, and Yash Savani. 2021. BANANAS: Bayesian Optimization with Neural Architectures

for Neural Architecture Search. In AAAI.

[259] Rey Reza Wiyatno, Anqi Xu, Ousmane Dia, and Archy de Berker. 2019. Adversarial Examples in Modern Machine

Learning: A Review. arXiv:1911.05268

[260] Bing Xue, Mengjie Zhang, and Will N. Browne. 2012. Particle swarm optimization for feature selection in classification:

A multi-objective approach. IEEE transactions on cybernetics 43, 6 (2012), 1656â€“1671.

[261] Bing Xue, Mengjie Zhang, Will N Browne, and Xin Yao. 2015. A survey on evolutionary computation approaches to

feature selection. IEEE Transactions on Evolutionary Computation 20, 4 (2015), 606â€“626.

[262] Kaifeng Yang, Michael T. M. Emmerich, AndrÃ© H. Deutz, and Thomas BÃ¤ck. 2019. Efficient computation of expected
hypervolume improvement using box decomposition algorithms. Journal of Global Optimization 75, 1 (2019), 3â€“34.
[263] Kaifeng Yang, Pramudita Satria Palar, Michael T. M. Emmerich, Koji Shimoyama, and Thomas BÃ¤ck. 2019. A Multi-
Point Mechanism of Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Global Optimization.
In Proceedings of the Genetic and Evolutionary Computation Conference. ACM, Prague Czech Republic, 656â€“663.
https://doi.org/10.1145/3321707.3321784

[264] Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Hu Yi-Qi, Li Yu-Feng, Tu Wei-Wei, Yang Qiang, and Yu
Yang. 2018. Taking human out of learning applications: A survey on automated machine learning. arXiv:1810.13306
[265] Kun Zhang, Bernhard SchÃ¶lkopf, Krikamol Muandet, and Zhikun Wang. 2013. Domain Adaptation Under Target and
Conditional Shift. In Proceedings of the 30th International Conference on International Conference on Machine Learning
- Volume 28 (Atlanta, GA, USA) (ICMLâ€™13). JMLR.org, IIIâ€“819â€“IIIâ€“827. http://dl.acm.org/citation.cfm?id=3042817.
3043028

[266] Qingfu Zhang and Hui Li. 2007. MOEA/D: A multiobjective evolutionary algorithm based on decomposition. IEEE

Transactions on evolutionary computation 11, 6 (2007), 712â€“731.

[267] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. 2018. Shufflenet: An extremely efficient convolutional
neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition.
6848â€“6856.

[268] Yangming Zhou and Yangguang Liu. 2014. Correlation analysis of performance metrics for classifier. 487â€“492.

https://doi.org/10.1142/9789814619998_0081

[269] Eckart Zitzler, Dimo Brockhoff, and Lothar Thiele. 2006. The Hypervolume Indicator Revisited: On the Design of
Pareto-compliant Indicators Via Weighted Integration. In Evolutionary Multi-Criterion Optimization, 4th International
Conference, EMO 2007, Matsushima, Japan, March 5-8, 2007, Proceedings (Lecture Notes in Computer Science, Vol. 4403),
Shigeru Obayashi, Kalyanmoy Deb, Carlo Poloni, Tomoyuki Hiroyasu, and Tadahiko Murata (Eds.). Springer, 862â€“876.
https://doi.org/10.1007/978-3-540-70928-2_64

[270] Eckart Zitzler and Lothar Thiele. 1998. Multiobjective Optimization Using Evolutionary Algorithms - A Comparative
Case Study. In Parallel Problem Solving from Nature - PPSN V, 5th International Conference, Amsterdam, The Netherlands,
September 27-30, 1998, Proceedings (Lecture Notes in Computer Science, Vol. 1498), A. E. Eiben, Thomas BÃ¤ck, Marc
Schoenauer, and Hans-Paul Schwefel (Eds.). Springer, 292â€“304. https://doi.org/10.1007/BFb0056872

[271] Eckart Zitzler, Lothar Thiele, Marco Laumanns, Carlos M. Fonseca, and Viviane Grunert da Fonseca. 2003. Performance
assessment of multiobjective optimizers: an analysis and review. IEEE Trans. Evol. Comput. 7, 2 (2003), 117â€“132.
https://doi.org/10.1109/TEVC.2003.810758

[272] IndrË™e Å½liobaitË™e. 2017. Measuring discrimination in algorithmic decision making. Data Mining and Knowledge Discovery

31, 4 (2017), 1060â€“1089.

50

Karl and Pielok, et al.

A LIST OF PUBLICATIONS APPLYING MOHPO WITH AT LEAST ONE OBJECTIVE

RELATED TO EFFICIENCY

Designing neural network hardware accelerators with decoupled objective evaluations [111].
â€¢ Optimization Method: PESMO with decoupled evaluations
â€¢ Objectives:
(1) energy consumption
(2) prediction error
â€¢ Task: image classification (MNIST)
â€¢ Domain: computer vision

Pabo: Pseudo agent-based multi-objective bayesian hyperparameter optimization for efficient neural

accelerator design [206].

â€¢ Optimization Method: Bayesian Optimization (Gaussian process) for each objective
â€¢ Objectives:
(1) energy consumption
(2) classification error
â€¢ Task: image classification (CIFAR-10 and flower17)
â€¢ Domain: computer vision

A case for efficient accelerator design space exploration via bayesian optimization [222].
â€¢ Optimization Method: PESMO
â€¢ Objectives:
(1) energy consumption
(2) classification error
â€¢ Task: image classification (MNIST)
â€¢ Domain: computer vision

Monas: Multi-objective neural architecture search using reinforcement learning [118].
â€¢ Optimization Method: scalarization + reinforcement learning
â€¢ Objectives:
(1) energy consumption/MACs
(2) accuracy
â€¢ Task: image classification (CIFAR-10)
â€¢ Domain: computer vision

Nsga-net: neural architecture search using multi-objective genetic algorithm [182].
â€¢ Optimization Method: surrogate-assisted (BO) NSGA-II
â€¢ Objectives:
(1) FLOPs
(2) classification error
â€¢ Task: image classification (CIFAR-10)
â€¢ Domain: computer vision

Multi-objective simulated annealing for hyper-parameter optimization in convolutional neural

networks [100].

â€¢ Optimization Method: Multi-Objective simulated annealing
â€¢ Objectives:
(1) FLOPs
(2) classification accuracy

Multi-Objective Hyperparameter Optimization â€“ An Overview

51

â€¢ Task: image classification (CIFAR-10)
â€¢ Domain: computer vision

Nsganetv2: Evolutionary multi-objective surrogate-assisted neural architecture search [181].

Application 1

â€¢ Optimization Method: surrogate assisted (adaptive selection from BO, MLP among others)

NSGA-II
â€¢ Objectives:
(1) number of MAdds
(2) classification accuracy
â€¢ Task: image classification (CIFAR-10, CIFAR-100, MNIST)
â€¢ Domain: computer vision

Application 2

â€¢ Optimization Method: (scalarization +) surrogate assisted (adaptive selection from BO, MLP

among others) NSGA-II

â€¢ Objectives:
(1) number of MAdds
(2) classification accuracy
â€¢ Task: image classification (SINIC-10, STL-10, Flowers102, Pets, DTD, Aircraft)
â€¢ Domain: computer vision

Application 3

â€¢ Optimization Method: surrogate assisted (adaptive selection from BO, MLP among others)

NSGA-II
â€¢ Objectives:
(1) number of MAdds
(2) number of parameters
(3) CPU latency
(4) GPU latency
(5) classification accuracy
â€¢ Task: image classification (CIFAR-10, CIFAR-100, MNIST)
â€¢ Domain: computer vision

Pareto frontier learning with expensive correlated objectives [234].
â€¢ Optimization Method: EIPV - Bayesian Optimization (multi-output Gaussian process with

correlated objectives) expected improvement in hypervolume

â€¢ Objectives:
(1) memory consumption and training time (combined)
(2) accuracy
â€¢ Task: classification (boston housing)

Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks [54].
â€¢ Optimization Method: MO-BO (Gaussian processes) upper confidence bound and random

scalarization

â€¢ Objectives:
(1) memory consumption or FLOPS
(2) cross entropy loss
â€¢ Task: image classification (CIFAR-10, CIFAR-100, ImageNet)
â€¢ Domain: computer vision

52

Karl and Pielok, et al.

Automated optimization of decoder hyper-parameters for online LVCSR [51].
â€¢ Optimization Method: scalarization + random search or BO (Gaussian process) or TPE or EA
â€¢ Objectives:
(1) peak memory consumption
(2) word error rate
(3) real time factor
â€¢ Task: large vocabulary continuous speech recognition
â€¢ Domain: natural language processing

AdaEn-Net: An ensemble of adaptive 2Dâ€“3D Fully Convolutional Networks for medical image

segmentation [45].

â€¢ Optimization Method: EA based on MOEA/D
â€¢ Objectives:
(1) model size (number of trainable parameters)
(2) segmentation accuracy (Dice similarity coefficient)
â€¢ Task: image segmentation
â€¢ Domain: medical

AdaResU-Net: Multiobjective adaptive convolutional neural network for medical image segmenta-

tion [16].

â€¢ Optimization Method: EA based on MOEA/D
â€¢ Objectives:
(1) model size (number of trainable parameters)
(2) segmentation accuracy (Dice similarity coefficient)
â€¢ Task: image segmentation
â€¢ Domain: medical

EMONAS-Net: Efficient multiobjective neural architecture search using surrogate-assisted evolution-

ary algorithm for 3D medical image segmentation [46].

â€¢ Optimization Method: SaMEA: random forest surrogate assisted EA based on MOEA/D
â€¢ Objectives:
(1) number of parameters
(2) expected segmentation error
â€¢ Task: image segmentation
â€¢ Domain: medical

NeuroPower: Designing Energy Efficient Convolutional Neural Network Architecture for Embedded

Systems [178].

â€¢ Optimization Method: SPEA2
â€¢ Objectives:
(1) number of parameters
(2) classification accuracy
â€¢ Task: image classification (CIFAR-10, CIFAR-100, MNIST)
â€¢ Domain: computer vision

Evolutionary neural automl for deep learning [169].

Application 1

â€¢ Optimization Method: CoDeepNEAT (MOEA)
â€¢ Objectives:

Multi-Objective Hyperparameter Optimization â€“ An Overview

53

(1) number of parameters
(2) classification accuracy
â€¢ Task: binary text classification (Wikidetox)
â€¢ Domain: natural language processing

Application 1

â€¢ Optimization Method: CoDeepNEAT (MOEA)
â€¢ Objectives:
(1) number of parameters
(2) AUROC
â€¢ Task: image classification
â€¢ Domain: medical

Genetic-algorithm-optimized neural networks for gravitational wave classification [69].
â€¢ Optimization Method: scalarization + EA variants
â€¢ Objectives:
(1) network size
(2) accuracy
â€¢ Task: gravitational wave classification
â€¢ Domain: physics

Automated Development of DNN Based Spoken Language Systems Using Evolutionary Algo-

rithms [236].

â€¢ Optimization Method: MO CMA-ES
â€¢ Objectives:
(1) network size
(2) word error rate
â€¢ Task: speech recognition
â€¢ Domain: natural language processing

Structure and parameter optimization of FNNs using multi-objective ACO for control and predic-

tion [140].

â€¢ Optimization Method: MO-RACACO (Ant colony optimization)
â€¢ Objectives:
(1) network size (number of rule nodes)
(2) RMSE or SAE
â€¢ Task: time sequence prediction and nonlinear control problems
â€¢ Domain: control design

Multi-Objective Optimization of Support Vector Machines [241].
â€¢ Optimization Method: NSGA-II
â€¢ Objectives:
(1) number of support vectors
(2) false positive rate
(3) false negative rate
â€¢ Task: pattern recognition
â€¢ Domain: autonomous driving

Multi-objective Bayesian optimisation with preferences over objectives [2].

54

Karl and Pielok, et al.

â€¢ Optimization Method: Bayesian Optimization (Gaussian processes) with (preference based)

EHI as acquisition function

â€¢ Objectives:
(1) prediction time
(2) prediction error
â€¢ Task: image classification (MNIST)
â€¢ Domain: computer vision

Predictive entropy search for multi-objective bayesian optimization [110].
â€¢ Optimization Method: PESMO
â€¢ Objectives:
(1) prediction time
(2) prediction error
â€¢ Task: image classification (MNIST)
â€¢ Domain: computer vision

Efficient multi-criteria optimization on noisy machine learning problems [153].
â€¢ Optimization Method: SMS-EGO variant - hypervolume-based expected improvement as

acquisition function

â€¢ Objectives:
(1) training time
(2) prediction accuracy
â€¢ Task: binary classification (Sonar from UCI repository)

Automated structure discovery and parameter tuning of neural network language model based on

evolution strategy [246].

â€¢ Optimization Method: MO CMA-ES
â€¢ Objectives:
(1) training time
(2) word error rate
â€¢ Task: speech recognition
â€¢ Domain: natural language processing

Evolution strategy based automatic tuning of neural machine translation systems [218].
â€¢ Optimization Method: MO CMA-ES
â€¢ Objectives:
(1) validation time
(2) translation performance (BLEU)
â€¢ Task: machine translation
â€¢ Domain: natural language processing

A Deep Learning Model Based on Multi-Objective Particle Swarm Optimization for Scene Classifica-

tion in Unmanned Aerial Vehicles [220].

â€¢ Optimization Method: Particle Swarm optimization
â€¢ Objectives:
(1) inference latency
(2) classification accuracy
â€¢ Task: image/scene classification
â€¢ Domain: unmanned aerial vehicles

Multi-Objective Hyperparameter Optimization â€“ An Overview

55

Mnasnet: Platform-aware neural architecture search for mobile [244].

Application 1

â€¢ Optimization Method: weighted product method + reinforcement learning
â€¢ Objectives:
(1) inference latency
(2) accuracy
â€¢ Task: image classification (ImageNet)
â€¢ Domain: computer vision

Application 1

â€¢ Optimization Method: weighted product method + reinforcement learning
â€¢ Objectives:
(1) inference latency
(2) mAP
â€¢ Task: object detection (COCO)
â€¢ Domain: computer vision

Max-value Entropy Search for Multi-Objective Bayesian Optimization [21].
â€¢ Optimization Method: MESMO
â€¢ Objectives:
(1) prediction time
(2) classification accuracy
â€¢ Task: image classification (MNIST)
â€¢ Domain: computer vision

Nemo: Neuro-evolution with multiobjective optimization of deep neural network for speed and

accuracy [146].

â€¢ Optimization Method: NSGA-II
â€¢ Objectives:
(1) prediction speed
(2) classification accuracy
â€¢ Task: image classification (MNIST, CIFAR-10)
â€¢ Domain: computer vision

Multi-objective Asynchronous Successive Halving [231].

Application 1

â€¢ Optimization Method: Asynchronous successive halving
â€¢ Objectives:
(1) latency
(2) accuracy
â€¢ Task: image classification (NAS-Bench-201)
â€¢ Domain: computer vision

Application 2

â€¢ Optimization Method: Asynchronous successive halving
â€¢ Objectives:
(1) prediction time
(2) perplexity
(3) word error rate
â€¢ Task: language modeling

56

Karl and Pielok, et al.

â€¢ Domain: natural language processing

PPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural Architectures [73].
â€¢ Optimization Method: Bayesian Optimization (Recurrent Neural Network)
â€¢ Objectives:
(1) number of parameters
(2) FLOPs
(3) prediction time
(4) error rate
â€¢ Task: image classification (CIFAR-10)
â€¢ Domain: computer vision

