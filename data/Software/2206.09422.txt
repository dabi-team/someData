Phantom Artifacts & Code Review Coverage
in Dependency Updates

Nasif Imtiaz
simtiaz@ncsu.edu
North Carolina State University
North Carolina, USA

Laurie Williams
lawilli3@ncsu.edu
North Carolina State University
North Carolina, USA

ABSTRACT
As modern software extensively uses free open source packages as
dependencies, developers have to regularly pull in new third-party
code through frequent updates. However, without a proper review
of every incoming change, vulnerable and even malicious code can
sneak into the codebase through these dependencies. The goal of
this study is to aid developers in securely accepting dependency
updates by measuring if the code changes in an update have passed
through a code review process. We implement DepDive, an update
audit tool for packages in Crates.io, npm, PyPI, and RubyGems
registry. DepDive first (i) identifies the files and the code changes
in an update that cannot be traced back to the package’s source
repository, i.e., phantom artifacts; and then (ii) measures what
portion of changes in the update, excluding the phantom artifacts,
has passed through a code review process, i.e., code review coverage.
Using DepDive, we present an empirical study across the latest
ten updates of the most downloaded 1000 packages in each of the
four registries. Our study unveils interesting insights while also
providing an evaluation of our proposed approach. We find that
phantom artifacts are not uncommon in the updates (20.1% of the
analyzed updates had at least one phantom file). The phantoms
can appear either due to legitimate reasons, such as in the case of
programmatically generated files, or from accidental inclusion, such
as in the case of files that are ignored in the repository. However,
without provenance tracking, we cannot audit if the changes in
these phantom artifacts were code-reviewed or not.

Regarding code review coverage (CRC), we find the updates are
typically only partially code-reviewed (52.5% of the time). Further,
only 9.0% of the packages had all their updates in our data set fully
code-reviewed, indicating that even the most used packages can
introduce non-reviewed code in the software supply chain. We also
observe that updates either tend to have very high CRC or very low
CRC, suggesting that packages at the opposite end of the spectrum
may require a separate set of treatments.

2
2
0
2

n
u
J

9
1

]
E
S
.
s
c
[

1
v
2
2
4
9
0
.
6
0
2
2
:
v
i
X
r
a

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ACM Reference Format:
Nasif Imtiaz and Laurie Williams. 2022. Phantom Artifacts & Code Review
Coverage in Dependency Updates. In Proceedings of ACM Conference (Con-
ference’17). ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

1 INTRODUCTION
Modern software extensively uses free open source packages as
dependencies [34]. However, the use of open source has opened
up new attack vectors, as vulnerable and even malicious code can
sneak into software through these third-party dependencies [31].
Further, practitioners are recommended to keep dependencies up
to date with the latest version [2], resulting in developers pulling
in new code through frequent updates [8], often automatically and
without a security review [30].

Recent times have seen popular open source packages be compro-
mised and push malicious updates, such as the attack through ua-
parser-js, an npm package used by major software corporations [21].
Attacks through dependencies, such as the preceding example, are
categorized as “supply chain attacks” by security practitioners [18].
While the existence of malicious packages in different ecosystems
is well-known [31], popular packages from reliable sources can also
be compromised, such as through: (1) hijacking of a maintainer’s ac-
count; (2) a maintainer going rogue; (3) account handover through
social engineering; and (4) build system compromise. [31, 40]

Therefore, practitioners are now recommended to review depen-
dency updates before merging them into the codebase [3, 39], as
the responsibility of security lies on the consumer when using free
open-source code [38]. However, manually reviewing all the code
changes in each update may not be a practical solution, as projects
may have hundreds of direct and transitive dependencies [24, 34].
Further, actively maintained packages get frequent updates, over-
burdening any project that would employ such strict measures.
Therefore, we propose that dependency updates should go through
automated security and quality checks before being merged, which
can act as the first line of defense and can aid developers in securely
accepting dependency updates.

Recently, the software industry has proposed multiple frame-
works to define the compliance standards for using open source
packages, such as the “Supply chain Levels for Software Artifacts
(SLSA)” [10] framework. Specifically, SLSA provides a checklist of
standards and controls to prevent tampering, improve integrity, and
secure packages and infrastructure. Among others, SLSA requires
a dependency package to employ a two-person code review [13].
However, while projects like the “Security Scorecards” [11] exist to
aid in dependency selection, little research has been done yet on
employing automated checks during each subsequent update [37].

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Nasif Imtiaz and Laurie Williams

The goal of this study is to aid developers in securely accepting
dependency updates by measuring if the code changes in an update
have passed through a code review process.

The check for code review ensures that each line of code change
in the update can be traced back to at least two persons. Such a
requirement can guard against the threat scenarios where (1) only
one maintainer’s account has been hijacked by the attacker(s); a sin-
gle maintainer has gone rogue; or (2) attackers have compromised
the publishing infrastructure to inject (unowned) malicious code.
Further, prior research has shown that code review helps ensure
higher code quality [29] and can prevent the introduction of new
security vulnerabilities [17]. Ladisa et al [26] have developed an
attack taxonomy for open source supply chain attacks, where they
have mentioned code review as a safeguard against the attack vector
inject into sources of legitimate package.

However, the feasibility of employing an automated check for
the code review requirement during dependency updates has not
been studied yet. Packages get bundled and distributed in different
ways and may use different code hosting platforms and code review
tooling. Such differences in the maintenance and the distribution
of open source packages may create challenges in reliably auditing
the changes in each new update. Further, no empirical study exists
on the code review practices among top open source packages to
understand the practicality of employing this SLSA requirement
currently in practice. This paper aims to address these gaps in the
research of securely using open source dependencies.

We implement DepDive which measures the code review cov-
erage (CRC) [28] in a dependency update. We define CRC as the
proportion of the code changes in an update that have gone through
a code review process. DepDive works for four package registries,
namely Crates.io for Rust, npm for JavaScript, PyPI for Python,
and RubyGems for Ruby. We choose these registries because they
follow a similar package distribution model, where the maintainers
upload the package code to the registry. We further scope our im-
plementation to GitHub repositories as we leverage the platform to
determine if certain code changes have been reviewed or not.

In summary, DepDive maps code changes between two versions
of a package uploaded in the registry to the corresponding commits
in the package’s source repository that made these changes and
identifies if there was a reviewer for the mapped commits through
four GitHub-based checks. Along the process, DepDive also iden-
tifies the files and lines of code that cannot be mapped from the
registry to the source repository, which we refer to as phantom
artifacts, following the definition in prior work [37]. While one ap-
proach can be to consider the phantom artifacts as non-reviewed in
the denominator of the CRC measurement, these artifacts may exist
in the form of binaries or programmatically-generated files, as will
be shown in this paper. Therefore, they will require a provenance
tracking mechanism to audit if changes in them were reviewed or
not. With DepDive, we first filter out the phantom artifacts and
output them separately from the CRC measurement.

For an empirical evaluation, we run DepDive over the latest ten
releases of the most downloaded 1000 packages in Crates.io, npm,
PyPI, and RubyGems. Based on DepDive’s output, we answer the
following two research questions: RQ1: To what extent do phantom
artifacts exist in the updates of the most downloaded packages?;

and RQ2: Excluding phantom artifacts, what is the code review
coverage (CRC) in the updates of the most downloaded packages?
Besides the answer to the above two research questions, the
contributions of our work include a working tool, DepDive, that
outputs details on the phantom artifacts and the code review data
for a dependency update. The code and data for this paper are
anonymously available at https://tinyurl.com/depdive.

2 BACKGROUND
Package Source Code Repository: Repository is a cloud file host-
ing service with a versioning system to store the source code of a
software project. In a git repository, the most granular unit to track
a revision of the source code is called a commit that is identified
through a unique commit hash.

Package Registry: Package registries are centralized package
hosting services to store and distribute packages. In this paper, we
work with Crates.io, npm, PyPI, and RubyGems registry. These
registries follow a similar package distribution model where devel-
opers can upload their package source code to the registry alongside
any required artifacts such as data files, pre-compiled binaries, etc.
in a registry-specific distribution bundle (e.g., wheel for PyPI). The
client software can then download and install packages from these
registries. Note that, while it is expected that the same source code
in the repository is distributed via the registry, the registry contains
its own copy of the package code which may not be identical to the
one in the repository.

Code Review: Code review is a manual review process of code
changes by any developer(s) other than the author. While the his-
tory of code review is not tracked by git, reviews are generally
performed using a tool, e.g., Gerrit [5]. GitHub offers a pull-based
development model that integrates native code review tooling. A
developer can open a pull request (PR) on GitHub to submit code
changes and ask for reviews from other developers.

3 DEPDIVE
In this section, we describe the implementation of the update audit
tool, DepDive. Figure 1 shows a high-level workflow of the tool.

DepDive takes four arguments as input: (1) registry name; (2) pack-

age name; (3) current version; and (4) update version. DepDive
works with four package registries, namely Crates.io, npm, PyPI,
and RubyGems. The input can be an update from any version to an-
other version available in the registry. DepDive begins the analysis
by identifying the source code repository of the given package from
metadata available on the registry. We scope DepDive’s implemen-
tation to GitHub repositories, as we rely on data available through
the GitHub platform to determine if a commit was code-reviewed,
as will be explained in Section 3.5.

DepDive then collects package code both from the registry and
the repository and compares them to identify the phantom artifacts.
Phantom artifacts in the registry do not map to any corresponding
artifact in the repository, and therefore, cannot be audited for code
review without tracking the provenance of these artifacts first.
Hence, DepDive outputs data on phantom artifacts separately from
code review analysis. Algorithm 1 lists pseudocode for determining
phantom files and lines in an update.

Phantom Artifacts & Code Review Coverage
in Dependency Updates

Conference’17, July 2017, Washington, DC, USA

Figure 1: DepDive workflow

Afterward, DepDive maps each line of code changed in the up-
date to its corresponding commit in the repository that made the
change. Algorithm 2 lists the pseudocode for identifying the code
changes between two versions and mapping them to their corre-
sponding commits. DepDive then determines if the commit was
code-reviewed through four GitHub-based checks. In the following
subsections, we explain each step in detail.

3.1 Collect package code
3.1.1 Download package code from the registry: DepDive down-
loads package code for both the current and the update version
from the respective registries. While Crates.io, npm, and RubyGems
provide every package in a uniform format, PyPI packages can be
available in multiple formats in the registry. When multiple formats
are available, we prefer the default wheel distribution.

3.1.2 Locate package repository: We collect the repository of a
package from the metadata provided by the registry. Afterward, we
locate the directory path of the package within the repository.

A repository can contain source code for multiple packages.
Therefore, we need to know the directory path of a package within
the repository for an accurate one-to-one mapping of files be-
tween the registry and the repository. For example, the filepath
CHANGELOG.md in the Rust package tokio maps to the filepath
tokio/CHANGELOG.md in the repository.

For Crates.io, npm, and RubyGems, DepDive identifies the di-
rectory path by locating the manifest file (Cargo.toml, package.json,
and Gemspec) of the package in the repository. While PyPI pack-
ages do not contain a uniform manifest file, we locate the directory
by matching the filepaths in the registry with the filepaths in the
repository. Through directory path locating, DepDive also validates
the retrieved repository for a package. Further, a repository can
contain submodules that point to different repositories. We obtain
commit history recursively for all the submodules in the repository.

3.1.3 Map file path from the registry to the repository: For each
file in the registry, we obtain the repository filepath by combining
the package directory and the registry filepath. We follow this
simple heuristic of filepath matching based on the assumption that
packages are bundled by maintaining the same directory structure
in the repository 1. Further, we also resolve the cases where a
filepath contains a symbolic reference to another file within the
repository.

3.2 Identify release commit
To compare the package code in the registry and in the repository,
and to identify the commits between two versions in the repository,
we need to identify the head commit from which a certain version
was built and uploaded in the registry. We refer to this commit as
the release commit for a package version.

DepDive identifies the release commit through the git tags in
the corresponding repository. Tagging the release commit with the
version number is a recommended developmental practice [1] and
was followed in prior research work [22, 23]. DepDive identifies
the release tag and the associated commit for a version through a
regular expression match of the tag name with the version number
and the package name. If the repository does not contain a sin-
gle tag to match the given version, DepDive fails to analyze the
corresponding update.

While not every repository annotates the release commit via git
tags or may do it inaccurately, the alternative way is to compare
the repository code with the registry code at all the commits in the
history and take the commit with the smallest difference (registry
code may contain phantom artifacts which will not be present in
the release commit, and therefore may not return an exact match
with any single commit). Vu et al. developed LastPyMile [37] to
identify phantom artifacts in a PyPI package by comparing the
registry code with every commit in the repository. However, their
approach cannot identify the specific release commit for a given

1We have found this assumption to be generally true in our study.

Package registryLocate and validatesource repositoryPackage code for version XPackage code for version YDownload package codeRelease commit for  version XRelease commit for  version YIdentify release commitfrom repository tagsPhantom filesPhantom linesOutput artifacts that arepresent in the registry,  but not in the repositoryComparepackage codeand repositorycodeIdentify code deltabetween version X and YMap code delta tocommitsGitHub Pull Request Author and committerDetermine if commitwas code-reviewedGerrit, ProwCode review CoverageReviewed commitsOutput code review dataNon-reviewed commitsRegsitry, EPackage, AInput: Update toversion Y  from version XVersion, XVersion, YIdentify lines of code (LOC)added in version Y  from version XIdentify lines of code (LOC)removed from version X  in version YGitHubrepositoryMap added LOC to commit  via git blame Map removed LOC to commitvia reverse git blameConference’17, July 2017, Washington, DC, USA

Nasif Imtiaz and Laurie Williams

version. Therefore, we take the simple heuristic to identify release
commits via repository tags.

3.3 Identify phantom artifacts
DepDive then compares the code diff between the package code
of the current and the update version fetched from the registry,
and the code diff between the two versions’ corresponding release
commits in the repository. Here, by code diff, we refer to the diff
between two codebases obtained by git-diff. However, files and
lines of code changes can be present in the registry, but not present
within the two release commits in the repository. We refer to such
files and code changes as phantom artifacts, following the definition
of the term in prior work [37]. DepDive outputs phantom artifacts
in two categories, phantom files, and phantom lines:
3.3.1 Phantom file: We define phantom files in an update as files
that are present in the update version in the registry, but not present
in the corresponding repository filepath at the release commit (line
8-14 in Algorithm 1).

3.3.2 Phantom line: We define phantom lines in an update as
the lines of code changes that are present in the code diff between
the current and the update version, but not present in the code diff
in the repository between the corresponding release commits (line
15-24 in Algorithm 2).

Step 1 Identify Phantom artifacts in an update
Require: registry name: 𝐸
Require: package name: 𝑃
Require: current version: 𝑋
Require: update version: 𝑌

1: 𝑃𝑋 = 𝐷𝑜𝑤𝑛𝑙𝑜𝑎𝑑𝑃𝑎𝑐𝑘𝑎𝑔𝑒𝐶𝑜𝑑𝑒𝐹𝑟𝑜𝑚𝑅𝑒𝑔𝑖𝑠𝑡𝑟𝑦 (𝐸, 𝑃, 𝑋 )
2: 𝑃𝑌 = 𝐷𝑜𝑤𝑛𝑙𝑜𝑎𝑑𝑃𝑎𝑐𝑘𝑎𝑔𝑒𝐶𝑜𝑑𝑒𝐹𝑟𝑜𝑚𝑅𝑒𝑔𝑖𝑠𝑡𝑟𝑦 (𝐸, 𝑃, 𝑌 )
3: 𝐹𝑌 𝑃 = 𝐺𝑒𝑡𝐴𝑙𝑙𝑃𝑎𝑐𝑘𝑎𝑔𝑒𝐹𝑖𝑙𝑒𝑝𝑎𝑡ℎ𝑠 (𝑃𝑌 )
4: 𝑅, 𝐷 = 𝐿𝑜𝑐𝑎𝑡𝑒𝑅𝑒𝑝𝑜𝑠𝑖𝑡𝑜𝑟𝑦𝐴𝑛𝑑𝐷𝑖𝑟𝑒𝑐𝑡𝑜𝑟𝑦 (𝐸, 𝑃)
5: 𝐶𝑋 = 𝐼𝑑𝑒𝑛𝑡𝑖 𝑓 𝑦𝑅𝑒𝑙𝑒𝑎𝑠𝑒𝐶𝑜𝑚𝑚𝑖𝑡 (𝑅, 𝑋 )
6: 𝐶𝑌 = 𝐼𝑑𝑒𝑛𝑡𝑖 𝑓 𝑦𝑅𝑒𝑙𝑒𝑎𝑠𝑒𝐶𝑜𝑚𝑚𝑖𝑡 (𝑅, 𝑌 )
7: 𝐹𝑌 𝑅 = 𝐺𝑒𝑡𝐴𝑙𝑙𝑃𝑎𝑐𝑘𝑎𝑔𝑒𝐹𝑖𝑙𝑒𝑝𝑎𝑡ℎ𝑠𝐴𝑡𝐺𝑖𝑣𝑒𝑛𝐶𝑜𝑚𝑚𝑖𝑡 (𝑅, 𝐷, 𝐶𝑌 )
8: Set of phantom files, 𝐻𝑃 𝐹 = ∅
9: for all 𝑓 ∈ 𝐹𝑌 𝑃 do
10:

𝑓𝑟 = 𝐺𝑒𝑡𝑅𝑒𝑝𝑜𝑠𝑖𝑡𝑜𝑟𝑦𝐹𝑖𝑙𝑒𝑃𝑎𝑡ℎ(𝑓 , 𝐷)
if 𝑓𝑟 ∉ 𝐹𝑌 𝑅 then
𝐻𝑃 𝐹 = 𝐻𝑃𝐹 ∪ 𝑓

13:
end if
14: end for
15: Map of phantom lines to files: 𝑀𝑃𝐿
16: for all 𝑓 ∈ 𝐹𝑌 𝑃 − 𝐻𝑃𝐹 do
17:

𝑓𝑟 = 𝐺𝑒𝑡𝑅𝑒𝑝𝑜𝑠𝑖𝑡𝑜𝑟𝑦𝐹𝑖𝑙𝑒𝑃𝑎𝑡ℎ(𝑓 , 𝐷)
Code changes in the registry, 𝐷𝑃 = 𝐷𝑖 𝑓 𝑓 (𝑓 , 𝑃𝑋 , 𝑃𝑌 )
Code changes in the repository, 𝐷𝑅 = 𝐷𝑖 𝑓 𝑓 (𝑓𝑟 , 𝑅, 𝐶𝑋 , 𝐶𝑌 )
phantom lines, 𝑃𝑓 = 𝐷𝑃 − 𝐷𝑅
if 𝑃𝑓 ≠ ∅ then

𝑀𝑃𝐿 .𝑖𝑛𝑠𝑒𝑟𝑡 (𝑓 , 𝑃𝑓 )

23:
end if
24: end for
25: return Phantom files and phantom lines, 𝐻𝑃𝐹 , 𝑀𝑃𝐿

11:

12:

18:

19:

20:

21:

22:

Step 2 Map code delta to corresponding commits in the repository
Require: Release commit of current version: 𝐶𝑋
Require: Release commit of update version: 𝐶𝑌
Require: Files changed in update: 𝐹𝑋𝑌

1: Commits between 𝐶𝑋 and 𝐶𝑌 , 𝐿𝑋𝑌 = 𝑔𝑖𝑡_𝑙𝑜𝑔(𝐶𝑋 ..𝐶𝑌 )
2: Common ancestor, 𝐶𝐴 = 𝑃𝑎𝑟𝑒𝑛𝑡𝑂 𝑓 (𝑂𝑙𝑑𝑒𝑠𝑡 (𝐿𝑋𝑌 )
3: Map of added lines to commit: 𝐴𝑋𝑌
4: Map of removed lines to commit: 𝑅𝑋𝑌
5: for all 𝑓 ∈ 𝐹𝑋𝑌 do
Git blame: 𝐵𝑌
6:
for all (𝑙, 𝑐) ∈ 𝐵𝑌
if 𝑐 ∈ 𝐿𝑋𝑌 then

𝑓 = 𝑔𝑖𝑡_𝑏𝑙𝑎𝑚𝑒 (𝑓 , 𝐶𝑌 )

𝑓 do

7:

8:

𝐴𝑋𝑌 .𝑖𝑛𝑠𝑒𝑟𝑡 (𝑓 , (𝑙, 𝑐))

end if
end for
Reverse git blame: 𝑅𝐵𝑋

𝑓 = 𝑔𝑖𝑡_𝑏𝑙𝑎𝑚𝑒 (𝑓 , 𝐶𝐴, 𝑟𝑒𝑣𝑒𝑟𝑠𝑒 = 𝑇𝑟𝑢𝑒)

for all (𝑙, 𝑐) ∈ 𝑅𝐵𝑋
if 𝑐 ≠ 𝐶𝑌 then

𝑓 do

𝑟𝑐 = 𝑓 𝑖𝑛𝑑𝑅𝑒𝑚𝑜𝑣𝑎𝑙𝐶𝑜𝑚𝑚𝑖𝑡 (𝑓 , 𝑐, 𝑙)
if 𝑟𝑐 ∈ 𝐿𝑋𝑌 then

𝑅𝑋𝑌 .𝑖𝑛𝑠𝑒𝑟𝑡 (𝑓 , (𝑙, 𝑟𝑐))

end if

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

end if
end for

20:
21: end for
22: return Code changes to commit map, 𝐴𝑋𝑌 , 𝑅𝑋𝑌

𝐶𝑋

𝐶𝑌

𝐶𝐴

𝐶𝑋

𝐶𝑌

(a) 𝐶𝑋 and 𝐶𝑌 are
on the same branch
where 𝐶𝑋 is the com-
mon ancestor for both
𝐶𝑋 and 𝐶𝑌

(b) 𝐶𝑋 and 𝐶𝑌 are on
different branches, di-
verging from the com-
mon ancestor commit,
𝐶𝐴

Figure 2: The relative position of the release commit of ver-
sion X (𝐶𝑋 ) and the release commit of version Y (𝐶𝑌 ) in the
repository commit history.

3.4 Map code delta to commits
Besides phantom files and lines, the rest of the code diff between
two versions in the registry maps to the code diff between the
corresponding release commits in the repository. However, not
every line of change in the code diff between two commit points
can be mapped to a specific commit. To explain why, consider the
two commit history graphs shown in Figure 2, where 𝐶𝑋 and 𝐶𝑌
refer to the release commit for version X and Y, respectively. In
Figure 2a, both 𝐶𝑋 and 𝐶𝑌 lie on the same development branch,
where 𝐶𝑋 is a direct ancestor of 𝐶𝑌 . In this case, each line of change
in the diff between 𝐶𝑋 and 𝐶𝑌 can be mapped to a single commit
between the two commits. Contrarily, in Figure 2b, 𝐶𝑋 and 𝐶𝑌 lie

Phantom Artifacts & Code Review Coverage
in Dependency Updates

on two different branches with a common ancestor at commit 𝐶𝐴.
In this case, a line of code that was added in 𝐶𝑋 , but was not present
in 𝐶𝐴 and subsequently in 𝐶𝑌 , will be shown as a deleted line in
the diff between 𝐶𝑋 and 𝐶𝑌 (and between the two versions fetched
from the registry). However, that line of code was not deleted in
any commit, rather just did not appear between 𝐶𝐴 and 𝐶𝑌 .

Therefore, we define code delta in an update as code changes
that can be mapped to a commit that is present in the update version,
but not present in the current version. Following this definition,
we only consider the commits between 𝐶𝐴 and 𝐶𝑌 in Figure 2b
as the commits between 𝐶𝑋 and 𝐶𝑌 (double-dot commit range as
per git syntax). The rationale behind this approach is that when
analyzing an update, DepDive assumes that the current version is
already trusted by the user, and therefore, focuses analysis only
on the code changes between the current and the update versions.
Consequently, in Figure 2b, we assume that if 𝐶𝑋 is trusted by a user,
any ancestor commit of 𝐶𝑋 is also trusted by the user. Therefore,
when updating from 𝐶𝑋 to 𝐶𝑌 , we only focus on the code changes
between their common ancestor 𝐶𝐴 and the update version 𝐶𝑌 . In
the simpler case, where both 𝐶𝑋 and 𝐶𝑌 lie on the same branch as
shown in Figure 2a, 𝐶𝑋 itself is the common ancestor of the two
versions.

Algorithm 2 shows DepDive’s implementation of identifying
the code delta between two versions and mapping each line of
code in the delta to a corresponding commit in the repository. To
identify the newly-added lines in the update version, DepDive runs
git blame on each package file at commit 𝐶𝑌 to obtain the commits
that added each line in the file. If a line was added in a commit
that lies between 𝐶𝑋 and 𝐶𝑌 , we take the line as a newly-added
line in the update version (line 6-10 in Algorithm 2). To identify
the lines that have been removed in the update, we run reverse git
blame on each package file at commit 𝐶𝐴, to obtain the last commit
where each line in the file was present. If a line was still present in
𝐶𝑌 , the line was not removed in the update version. For the rest of
the removed lines, we identify the removal commit by traversing
forward from the blamed commit up to 𝐶𝑌 . If the removal commit
lies between 𝐶𝑋 and 𝐶𝑌 , we take the line as a removed line in the
update (line 12-20 in Algorithm 2). 2

Along the process, we also obtain the commit mapping for each
newly added and removed line of code in the update. Note that,
in this and the preceding step, DepDive also handles corner cases
where a file has been renamed; a file that already existed in the
repository but was newly included in the registry; and a file within
a submodule pointing to a different remote repository.

3.5 Determine if a commit was code-reviewed
After obtaining the commits involved in an update, we need to
determine if the commit was code-reviewed. Git does not store
data on code-review for a commit. Therefore, we focus on the
projects that use GitHub as their development platform and look

2In the case where the two release commits lie on the same branch (Figure 2a), the
code delta in the update will be equal to or greater than the code diff between the
versions fetched from the registry. The code delta can be greater in cases where a line
was modified after the release commit of the current version but was reverted before
the update version. Such a line will not appear in the code diff between versions from
the registry, but will appear in the code delta calculated by DepDive.

Conference’17, July 2017, Washington, DC, USA

for evidence on GitHub to determine if a commit had been code-
reviewed. Specifically, we apply four checks to look for evidence
of code review on GitHub. Note that, we adopt these checks from
“Security Scorecards” [11], a tool that analyzes the last 30 commits
of a GitHub project and generates a score for a project’s adherence
to code review. Below, we explain the four checks:

(1) GitHub review: We check if a commit belongs to a GitHub
pull request and if the pull request was reviewed through
GitHub’s native code review tooling.

(2) Different merger: We check if a commit belongs to a GitHub
pull request and if the pull request was opened and merged
by two different GitHub accounts. We assume the action of
merging as an implicit approval of the commit by the merger.
(3) Different committer: We check if a commit was authored
and committed by two different accounts. We assume the
commit was approved by the committer.

(4) Third-party tools: Besides GitHub’s native code review tool-
ing, a project can use external code review tools, e.g., Ger-
rit [5], or custom bots to handle code-review, e.g., Prow [7].
When a commit is reviewed on Gerrit, the commit message
contains metadata for the Gerrit review. When a pull request
on GitHub is handled by Prow, Prow adds a label to the pull
request to indicate if it was code-reviewed. Following Secu-
rity Scorecards’ implementation, we also check for evidence
for Gerrit or Prow review in DepDive.

We determine a commit as code-reviewed if any one of the four
checks is met. When checking for different merger or committer,
we do not consider a commit to be code-reviewed if both the author
and the reviewer are bot accounts on GitHub. We further exclude
cases where GitHub’s own bots are involved (e.g., GitHub actions).
However, in cases where a pull request has been opened by a bot,
e.g., Dependabot [4], and merged by a non-bot account, we consider
the commit as code-reviewed.

4 DATASET
In this section, we explain the packages and their updates we se-
lected for this study, and the updates that DepDive could success-
fully analyze based on which we complete our empirical analysis.

4.1 Package Selection
We select the latest ten updates of the most downloaded 1000 pack-
ages in each of the four package registries, namely Crates.io, npm,
PyPI, and RubyGems. For Crates.io and RubyGems, we download
the official data dumps that are updated daily and then select the
top packages in the order of the total download count across all the
versions. For PyPI, we use the dataset from [36] that is updated on
a monthly basis. We collected the data for these three registries at
the end of December 2021. For NPM, we use the dataset from [40]
that was constructed in August 2021. While the download count
can be inflated in different ways, including through CI/CD tooling,
sampling the most downloaded packages to study a package ecosys-
tem is an established approach [16, 37] and provides an estimation
of the most used packages in a registry.

We then collect the list of the available version releases on the
registry for each of the selected packages at the end of December
2021. DepDive can run on an update from any version to another

Conference’17, July 2017, Washington, DC, USA

Nasif Imtiaz and Laurie Williams

Registry

Crates.io
npm
PyPI
RubyGems
Total

No. of
Selected Packages
990
989
992
991
3,962

No. of
Selected Updates
8,434
8,158
8,657
8,646
33,895

No. of Packages
analyzed by DepDive
833 (84.1%)
919 (92.9%)
788 (79.4%)
674 (68.0%)
3,214 (81.1%)

No. of Updates
analyzed by DepDive
6,326 (75.0%)
7,178 (88.0%)
6,089 (70.3%)
5,351 (61.9%)
24,944 (73.6%)

Table 1: Updates successfully analyzed by DepDive from the selected packages

(e.g., 1.8.3 → 1.8.4, 1.5.1 → 1.9.0, 1.0.0 → 3.0.3 etc.). However, for this
empirical study, for each version, we consider an update only from
its prior version according to SemVer [12] ordering. For example, for
tokio@1.8.4, we consider an update from tokio@1.8.3 to tokio@1.8.4
to pass as input to DepDive. We choose this approach to avoid
any data duplication, that is, the same code changes and commits
to appear in multiple updates of a package selected in this study.
Further, we exclude any pre-release version in our data set.

We also restrict our data set to only the most recent ten updates
for each of the packages. The rationale behind this is that DepDive
makes multiple GitHub API calls for each commit in an update for
code-review checking. However, GitHub limits API calls to 5000/hr
for each user, which puts a constraint on how much analysis we
can do within an hour. However, choosing at least ten updates will
provide us the data to measure the consistency of a package in its
code review coverage in each update. In the case where a package
has less than ten updates, we consider all of them in our study.

Out of the 4000 packages, 26 packages had only one release
listed on the respective registry. Further, 12 packages had only one
regular release available, while the other available versions were
pre-releases. We exclude these 38 packages as there were no updates
available to analyze. Finally, we select 33,895 updates from 3,962
packages to run DepDive on. Table 1 shows a breakdown of these
initially selected updates across the four registries.

4.2 DepDive Analysis
Out of the selected 33,895 updates, DepDive successfully analyzed
24,944 (73.6%) updates. In Section 5, we present our empirical anal-
ysis based on DepDive’s output for these 24,944 updates. Table 1
shows a breakdown of the dataset of this study. While DepDive
outputs details on each commit, phantom artifact, and lines of code
changed in an update, we collect the following metrics for each
update to conduct our empirical analysis: (1) No. of phantom files,
(2) No. of files with phantom lines, (3) No. of added phantom lines 3,
and (4) Code Review Coverage (CRC).

Note that, we measure the CRC of an update as the proportion of
the update’s code delta that has been code-reviewed. Here, the code
delta, as explained in Section 3.4. is the code changes in an update
that can be mapped to a commit in the repository and therefore,
can be classified if code-reviewed or not

3While DepDive also outputs phantom removal, we observe that the removed lines
are often also phantom lines from the old version.

4.3 Why DepDive failed
As shown in Table 1, DepDive only successfully analyzed 73.6% of
the initially selected updates. The primary reason that DepDive
could not analyze an update was the absence of git tags in the repos-
itory pointing to the release commit of a version, which was the
case for 5,747 updates (17.0%) in our dataset. The next major reason
is DepDive’s inability to either locate or validate the repository of
a package, which was the case for 2,669 of all the updates (7.9%).
For 127 updates (0.4%), the listed repository was not on GitHub. For
the rest of 1.1% of the updates, DepDive failed for various reasons,
including not being able to read a file containing non-Unicode char-
acters, a private submodule within the repository, and version code
not being available on the registry.

If DepDive were to be deployed in a CI/CD pipeline, our empirical
evaluation shows that the tool could fail in 26.4% of the cases.
However, the failure rate may be reduced if the package repositories
followed the best developmental practices, such as annotating a git
tag for each released version. We recommend that package manager
tools add a feature to include metadata on the repository, package
directory, and release commit in the package bundle during the
build process to aid in a third-party audit of each update.

5 EMPIRICAL ANALYSIS
Based on DepDive’s output, we answer the following two research
questions for the four studied registries: RQ1: To what extent do
phantom artifacts exist in the updates of the most downloaded
packages? and RQ2: Excluding phantom artifacts, what is the code
review coverage in the updates of the most downloaded packages?
In the following two subsections, we present our findings:

5.1 Phantom Artifacts
Table 2 shows the occurrence of phantom files, and Table 3 shows
the occurrence of phantom lines in updates across the four studied
registries. Overall, 24.9% of the updates contained either a phantom
file or a phantom line (3.2%, 44.3%, 38.5%, and 9.1% in the case of
Crates.io, npm, PyPI, and RubyGems updates, respectively). Below,
we discuss our findings:

5.1.1 Phantom files: We find that 20.1% of the analyzed updates
and 27.4% of the analyzed packages had at least one phantom file.
We find that npm and PyPI updates are more likely to contain a
phantom file (34.7% and 34.3%, respectively) than Crates.io and Ruby
Gems updates (1.9% and 6.0%, respectively). Overall, we identified
306,940 phantom files across all the updates. Below, we provide a
crude characterization of the phantom files that we identified:

Phantom Artifacts & Code Review Coverage
in Dependency Updates

Conference’17, July 2017, Washington, DC, USA

Registry

Crates.io
npm
PyPI
RubyGems
Total

Total
packages
833
919
788
674
3214

Total
Updates
6,326
7,178
6,089
5,351
24944

Packages with
phantom files
42 (5.0%)
413 (44.9%)
355 (45.1%)
71 (10.5%)
881 (27.4%)

Updates with
phantom files
123 (1.9%)
2,489 (34.7%)
2,088 (34.3%)
323 (6.0%)
5023 (20.1%)

Median phantom
file count

3.25
2.00
1.50
2.00
2.00

Table 2: Updates with phantom files, i.e., files present in the registry but not in the repository.

Registry

Crates.io
npm
PyPI
RubyGems
Total

Total
packages
833
919
788
674
3,214

Total
Updates
6,326
7,178
6,089
5,351
24,944

Packages with
phantom lines
63 (7.6%)
265 (28.8%)
157 (19.9%)
105 (15.6%)
590 (18.4%)

Updates with
phantom lines
87 (1.4%)
1,733 (24.1%)
471 (7.7%)
185 (3.5%)
2,476 (9.9%)

Median file count
with phantom lines
1.0
1.0
1.0
1.0
1.0

Median count of
added phantom lines
2.0
1.0
3.0
2.0
2.0

Table 3: Updates with phantom lines, i.e., changes in a file in the registry but not in the repository

Across all the npm updates, we identified 123,000 phantom files.
We observed that for some npm packages, the code in the registry
can be a transpiled version of the source code in the repository. For
example, the source code may be written in TypeScript, whereas
the package contains transpiled JavaScript code. Similarly, npm
packages can also contain minified JavaScript, or JavaScript tran-
spiled through transcompilers such as babel. Overall, 89.4 % of the
phantom files in the npm updates are JavaScript files (.js, .d.ts, .cjs,
.mjs, .min.js, .map, .flow files) which may be code transpiled during
the package build process. Besides transpiled code, 9.5% of the npm
phantom files are .json files which are either data or configuration
files, and possibly were git-ignored in the repository through the
.gitignore file.

We identified 146,564 phantom files across all the PyPI updates.
The majority of these files (46.1%) are machine-generated header
files for C/C++ (.h, .hpp, .inc files), while 7.4% of the phantom files
are compiled binaries (.so, .jar, .dylib). The core engine of many
Python packages, e.g., tensorflow, are written in languages like
C/C++, while the Python files only provide a front-end to commu-
nicate with the engine. While the repositories of these packages
contain the source code for the engine written in different lan-
guages, the Python packages, in their default wheel distribution,
only contain the compiled binaries and the machine-generated
header files alongside the Python source files. The header files and
the binaries come from 118 distinct packages in our dataset (15.0%
of the analyzed PyPI packages). Moreover, we find 16.3% of the
phantom files to be Python files, where one-third of them (32.1%)
are __init__.py files which are presumably machine-generated. Fi-
nally, in PyPI updates, we have also observed non-Python code files
(7.1% are .js, .ts files), data files (5.0% are .dat files), and git-ignored
files (e.g., .pyc, .pyi, .py files) as phantom files.

Across all the RubyGems updates, we found 7,483 phantom files.
However, 83.2% of these files come from 7 packages where the
package either (i) bundled its own Ruby dependency packages,
(ii) bundled non-Ruby dependency code, or (iii) included log files

that were git-ignored in the repository. Across Crates.io updates,
we identified 29,893 phantom files. However, 84.4% of these files
come from a single package that contained the source code for its
project website hosted in a repository branch. Overall, across all
the four registries, we have observed files that are git-ignored in the
repository, to be included in the registry (e.g., .DS_Store, .npmignore,
.editorconfig, setup.cfg files).

Accidental vs legitimate phantom files: Broadly, we can char-
acterize the phantom files into two categories: (i) phantom files
due to legitimate reasons such as compiled binaries in PyPI wheel
distribution, transpiled JavaScript, auto-generated files, and third-
party dependency code; and (ii) phantom files presumably put in
mistakenly, such as the git-ignored files.

For the legitimate phantom files, we need to track their prove-
nance, and then determine if changes in the origin files have been
code-reviewed or not, and incorporate that into the overall CRC of
the update. We discuss current research efforts and our recommen-
dations on handling legitimate phantom files in Section 7. On the
contrary, we recommend package maintainers issue a new clean
release in case of accidental phantom files, as the last mile between
the repository and the registry has been used as an attack vector to
sneak in malicious code in the past [31, 37].

Overall, while there may be legitimate reasons behind phantom
files, we cannot audit the changes in these files through our pro-
posed approach. Package users may manually review these files
before accepting an update, especially when the current version
does not have any phantom files, but the new update does. In our
dataset, 461 packages (52.1% of all the packages with phantom files)
had phantom files only in a subset of all their updates. We presume
that these packages may have accidentally put in phantom files in
some of their updates.

5.1.2 Phantom lines: We find that 9.9% of the analyzed updates
across 18.4% of the packages had code changes in an update that
were not present in the changes of the corresponding files in the
repository. We find that phantom lines are generally small changes,

Conference’17, July 2017, Washington, DC, USA

Nasif Imtiaz and Laurie Williams

Registry

Crates.io
npm
PyPI
RubyGems
Total

Total
Package
830
918
780
672
3,200

Total
Updates
6,293
7,147
5,861
5,309
24,610

Median LOC changes
in updates

75.75
25.00
76.75
61.25
51.00

Median code review
coverage (CRC) in updates
42.9%
5.9%
52.7%
31.0%
27.2%

Updates with
100% CRC
915 (14.5%)
87 (1.2%)
1,425 (24.3%)
269 (5.1%)
2,696 (11.0%)

Updates with
0% CRC
2,068 (32.9%)
3,497 (48.9%)
1,760 (30.0%)
1,663 (31.3%)
8,988 (36.5%)

Table 4: Code review coverage of the analyzed updates

Registry

Crates.io
npm
PyPI
RubyGems
Total

Total
Packages

830
918
780
672
3,200

Packages with
all updates
having 100% CRC
99 (11.9%)
5 (0.5%)
162 (20.8%)
23 (3.4%)
289 (9.0%)

Packages with
all updates
having 0% CRC
217 (26.1%)
386 (42.0%)
185 (23.7%)
161 (24.0%)
949 (29.7%)

Table 5: Packages with all updates fully code-reviewed, or
not code-reviewed at all.

Figure 3: Violin plot of code review coverage across updates

with 2 added lines in 1 file at the median. We also find that npm
updates are more likely to contain phantom lines than updates in
the other three studied registries.

We identified phantom lines in 1,733 (24.1%) npm updates, where
93.9% of these updates contained phantom lines in the manifest
file, package.json, that was presumably dynamically generated with
added data such as the release commit. Further, as explained when
discussing phantom files, the npm package code in the registry can
be transpiled from the code in the repository. Therefore, the same
filepath may have different code content in the registry and the
repository, resulting in many updates with phantom lines.

Across PyPI packages, we identified 471 updates (7.7%) with
phantom lines, where 51.6% of the updates had phantom lines in a
file named _version.py that was presumably dynamically generated
during the package build process with added data such as the release
commit, the build date, and the version number. Further, __init__.py
files can also be generated dynamically during the build process
and may differ in content from the repository copy, which resulted
in phantom lines in 117 updates (24.8%). For both Crates.io and

RubyGems, we do not find any common pattern in the identified
phantom lines. However, false positives may appear in the case
where a release commit was inaccurately tagged.

While some files can be dynamically generated during the pack-
age build process and incorporate phantom lines, overall, we find
that phantom lines are less likely to occur due to legitimate rea-
sons in Crates.io, PyPI, and RubyGems updates, and package users
should manually review these lines before accepting an update.

5.2 Code Review Coverage
In this section, we present our findings on code review coverage
(CRC) for the analyzed updates. Note that, we exclude phantom
artifacts in CRC measurement, and only consider the changes in
the package code that can be mapped to the repository. Table 4
presents the median lines of code (LOC) changes and the median
CRC across updates in the four registries. We excluded 334 updates
from the 24,944 analyzed updates as they contained zero code delta
as measured by DepDive. These updates either only made changes
in the non-package files in the repository, or only made changes
in the phantom files. Further, in our dataset, we identified 110,657
commits as code-reviewed, of which 60.1% were GitHub review,
31.5% were Different merger, and 8.4% were Different committer,
as per the checks explained in Section 3.5.

The table also shows the portion of the updates with 100% and
0% CRC across the four registries. We find that 11.0% of the updates
were fully code-reviewed, while 36.5% of the updates were not
code-reviewed at all. The rest of the 52.5% of the updates were
only partially code-reviewed. Further, we find that the median CRC
across all the analyzed updates stands at 27.2%.

We find the npm packages to have the lowest median CRC. In
Section 5.1, we explained that npm packages may contain transpiled
JavaScript, resulting in many phantom artifacts. In these cases, De-
pDive’s CRC measurement would be limited to only a subset of the
package files. For example, for the no-case package, DepDive could
only audit the package manifest files for code review. To address
this limitation, we looked at the CRC for the 3,947 npm updates that
did not contain any phantom artifacts with a presumption that such
updates are less likely to contain transpiled code. However, we still
observe a low median CRC of 6.1% for these updates. While many
npm packages are small in size and maintained by a few maintain-
ers resulting in many code changes remaining non-reviewed, our
analysis in this study may be an under-approximation for npm, as
the packages that contain transpiled JavaScript, e.g., packages with
source code in TypeScript, may also be more likely to follow devel-
opmental best practices such as code review. We have observed npm
packages from reputed organizations like babel and facebook to

Crates.ionpmPyPIRubyGemsAllRegistry020406080100code review coverage(%)Phantom Artifacts & Code Review Coverage
in Dependency Updates

develop their source code in TypeScript and adhere to code review
in the majority of the commits in the corresponding repositories.
However, DepDive could only audit a subset of the files in these
packages, and therefore, we may have got an under-approximated
CRC measurement.

For Crates.io, PyPI, and RubyGems packages, we find the median
CRC to be 42.9%, 52.7%, and 31.0% respectively. Figure 3 shows a
violin chart of the CRC across the updates. We observe an hourglass
shape in the violin chart, suggesting that updates tend to have
either very high CRC or very low CRC. Open source packages
are often maintained by a small group of maintainers. While the
contributions from an outsider get code-reviewed by one of the
maintainers, the code changes from the maintainers themselves
may remain non-reviewed, therefore resulting in low-to-medium
CRC overall. On the contrary, we have found updates that can have
very high CRC but are not fully code-reviewed. For example, we
have found 2,442 updates (9.9%) that are larger than 51 LOC changes
(overall median) and have greater than 90% but less than 100% CRC.
While most of the commits in these updates were code-reviewed,
we found some commits did not go through the code review pro-
cess, possibly due to the following two primary reasons as per our
observation: (i) Non-critical changes: the commit only changed con-
figuration or documentation files; (ii) Cherry-picked commits: the
commit cherry-picked a commit from a different branch where it
was code-reviewed either to backport a fix or to import changes to
a release branch from the master branch. Note that, SLSA requires
context-specific approval during a code review, which means the
cherry-picked commits require their own separate review. Nonethe-
less, this phenomenon shows that non-reviewed code changes may
sneak in even in packages that attempt to adhere to code review.
A few notable packages where we found high CRC but not 100%
include numpy, ansible-core in PyPI, nokogiri in Ruby, openssl in
Crates.io, etc.

Table 5 shows packages for whom all the analyzed updates in our
data set were measured to have 100% CRC or all the updates were
measured to have 0% CRC. We find that only 9.0% of the packages
in our dataset had all their updates fully code-reviewed, most of
them coming from PyPI and Crates.io. We observed that packages
from reputed organizations such as google, Azure, rust-lang, tokio-rs,
and aws are likely to consistently have fully code-reviewed updates.
On the contrary, 29.7% of the packages had none of their updates
code-reviewed at all. We have observed that these packages are
typically maintained by a small group of maintainers who do not
review each other’s code.

6 LIMITATIONS
DepDive design decisions: We adopt multiple heuristics in de-
signing DepDive that may result in false-positive outputs. Firstly,
we map package files in the registry to a file in the repository
through filepath matching. However, package maintainers may
choose a build process where the filepaths will be altered in the
bundled package without altering the content of the file, in which
case DepDive will output them as phantom files. Similarly, we may
fail to validate a correct repository and locate the package directory
when the directory structure in the registry code and the repository
do not match. While an alternate approach could be a pairwise

Conference’17, July 2017, Washington, DC, USA

comparison of all the files, our simpler heuristic is based on a real-
istic assumption that bundled packages typically follow the same
directory structure as that in the repository (unless the files are
programmatically generated). Secondly, we leverage repository tags
to determine the release commit for a package version, inaccuracy
in which may incorrectly output phantom artifacts. We explained
this design decision in Section 3.2 and recommend that packages
themselves should contain the metadata for the release commit.

Overall, we chose our heuristics to keep the design simple while
minimizing the possibility of false negatives. In this regard, our
empirical analysis of the phantom artifacts may represent an over-
approximation. Similarly, DepDive scopes its code-review checking
to the GitHub platform, and therefore, may output false positives
in the case where a project is hosted on the GitHub platform but
uses an external or an atypical code review tool. While we only
chose packages that list a GitHub repository in their provided meta-
data for the empirical study, our findings on code review coverage
may represent an under-approximation in such cases. We aim to
incorporate more checks in the future for code review and recom-
mend maintainers list their code review tooling within the package
metadata.

Generalizability threat: Our work studies the most down-
loaded packages in four package registries. However, as shown
in Table 4, we failed to analyze 26.4% of the initially selected up-
dates. The failure may introduce unknown sampling biases in our
dataset for the empirical analysis. Further, the threat to the CRC
analysis for the npm ecosystem due to the presence of transpiled
code has been explained in Section 5.2. Nonetheless, we believe our
empirical findings provide an evaluation of our proposed approach
and offer many insights into the existence of phantom artifacts and
CRC among the most downloaded packages.
7 DISCUSSION
7.1 DepDive Design Philosophy
One driving philosophy behind DepDive is that every line of code in
the software supply chain should be traceable to at least two owners.
To achieve that, we need to map the package code downloaded from
the registry to the commits in the package’s repository and then
reliably determine the author and the reviewers of the mapped
commits. However, we face two challenges in the process:

Provenance of the legitimate phantom artifacts: We find
that legitimate phantom artifacts, such as the programmatically
generated files, can appear in a package. Especially, transpiled
JavaScript code in the npm packages and compiled binaries in the
PyPI packages make DepDive’s audit incomplete without ensur-
ing the integrity of the origin of these files. Downstream projects
can address this issue by retrieving package code directly from the
source repository and then building the package by themselves.
PyPI already has a feature to provide a source distribution alongside
the default wheel distribution [6] that contains the source code to
compile the required binaries and machine-generated files on the
client’s end. Clients then can run DepDive on the source distribu-
tion for a complete CRC measurement. We recommend npm also
consider this distribution model.

Another way is to divide the update audit into two parts: (i)
have a framework, such as the in-toto [35] and the Reproducible
Builds [9], to verify that the package has indeed come from a certain

Conference’17, July 2017, Washington, DC, USA

Nasif Imtiaz and Laurie Williams

commit point in the repository, and (ii) measure CRC directly from
the repository (without the step in Algorithm 1). However, these
approaches have their own pros and cons. We recommend future
research to investigate designing a package distribution model that
minimizes the efforts required from the stakeholders while ensuring
the integrity of the packages.

Quality of code review: While we pitch DepDive as an audit
tool, a rogue/hijacked maintainer account can easily bypass Dep-
Dive checks by creating a pull request from a sock account and then
reviewing and accepting that pull request. SLSA requirement [13]
states that two trusted persons should be involved in a code re-
view. Therefore, a tool like DepDive should also consider the digital
identity of the involved developer accounts and flag any suspicious
review. Similarly, the diversity and the expertise of the reviewers
should also indicate the overall quality of a code review. We con-
sider providing a quality rating for each code review and flagging
suspicious reviews as future work for DepDive.

Difference with other tools: There are existing tools that ei-
ther (i) measure the code review adherence of a project, or (ii)
identify phantom artifacts in a package. "Security Scorecard" [11]
gives a score for a GitHub project’s code review adherence by look-
ing at the branch protection rules and the review history of the last
30 commits. While such a one-time check can help in selecting a de-
pendency, we aim to measure the code review coverage during each
update, which presents an added challenge of mapping the code
changes from the registry to the repository commits. Further, Vu et
al. [37] have developed LastPyMile to identify phantom artifacts in
a PyPI package by comparing the package code with every commit
in the repository. While DepDive’s approach has methodological
differences with LastPyMile as explained in Section 3.2, both the
tools should return similar results (for phantom artifacts in PyPI
updates). In a way, DepDive brings the above two tools’ objectives
into a single workflow and provides an isolated audit only for the
changes in an update. Such an audit will help package users focus
their review effort on the incremental changes in each update.

7.2 The State of the Package Ecosystems
Through our empirical evaluation of DepDive, we also present our
findings on the code review coverage in the recent updates of the
most downloaded packages. Below, we discuss some implications
of our findings:

The hourglass phenomenon: We have seen that packages
either tend to have very high CRC or very low CRC, as depicted by
an hourglass shape in Figure 3. The packages with high CRC should
enforce strict branch protection in the GitHub project to reject any
non-reviewed commits, or not include any non-production files
that have loose restrictions on code-review in the package. On the
other end of the spectrum, packages with low CRC may be in need
of more manpower. Future research may look at recommending
reviewers for packages that are highly used but maintained by a
small group of maintainers.

Post-release code vetting: The trade-off that comes with code
review is a slowed-down development, while maintainers of some
packages may not welcome reviewers in their projects. Further,
multiple maintainers can collude in developing a package and wait
until the package becomes sufficiently popular before pushing in a

backdoor as part of a long-term cyberattack. An alternate approach
to code review can be the post-release crowd-vetting of the code
changes in a new update. Package registries can provide a system
where developers from all around the world can review and approve
each new release of a package, while the client projects can wait a
certain period until an update has garnered enough approval before
accepting it in their codebase.

8 RELATED WORK
Supply chain security: Recent works have focused on the secure
use of open source dependencies as part of the software supply
chain [24, 31, 40, 41]. Duan et al. have proposed static and dynamic
analysis approaches to detect malicious packages for the interpreted
languages [19], while Sejfia et al. have proposed machine learning
models to detect malicious npm packages [33]. Further, Ferreira et
al. have proposed a permission-based protection mechanism for
malicious npm updates [20]. Our work differs from these prior
works in the way that we do not explicitly aim to detect malicious
updates, rather propose an automated check for each update to aid
developers in reviewing the security and the quality of the incoming
changes before accepting them into the codebase.

Ecosystem wide analysis: Recent works have done ecosystem-
wide analysis to understand the state of different software supply
chain networks. Zimmermann et al. [41] and Liu et al. [27] have
looked at the vulnerability propagation in the npm ecosystem, while
Alfadel et al. [14] have investigated the PyPI ecosystem. Similarly,
Zahan et al. [40] and Bommarito et al. [16] have looked at various
quality issues in the npm and PyPI ecosystems, respectively. Further,
Imtiaz et al. [23] have looked at how packages release security fixes
with an ecosystem-wide analysis for seven package registries.

Code review: There is a rich body of literature establishing
the benefits of code review in software development [25, 28, 32].
Research has shown that code review can help both software quality
and security [17, 29], while also helping disseminate the knowledge
of the codebase [15].
9 CONCLUSION
We implement DepDive, a dependency update audit tool for Crates.io,
npm, PyPI, and RubyGems packages, that first (i) identifies the files
and the code changes in an update that cannot be traced back to
the package’s source repository, i.e., phantom artifacts, and then (ii)
measures what portion of the changes in the update excluding the
phantom artifacts has passed through a code review process, i.e.,
code review coverage. DepDive can help package users in focusing
their review effort on the phantom artifacts and the non-reviewed
code when pulling in a new update, while also providing a qual-
ity estimate of the incoming changes. We ran DepDive over the
latest ten updates of the most downloaded 1000 packages in each
of the four above-mentioned registries, of which DepDive could
successfully analyze 73.6% of the updates.

Overall, from our empirical evaluation over 24,944 updates across
3,214 packages, we present interesting insights regarding the stud-
ied packages ecosystems. We find that phantom artifacts are not
uncommon in the updates, either due to legitimate reasons, such as
in the case of programmatically generated files, or from presumably
accidental inclusion, such as in the case of git-ignored files. While
phantom artifacts are rare in Crates.io and RubyGems updates, we

Phantom Artifacts & Code Review Coverage
in Dependency Updates

Conference’17, July 2017, Washington, DC, USA

matter?. In 2015 IEEE international conference on software maintenance and evolu-
tion (ICSME). IEEE, 111–120.

[26] Piergiorgio Ladisa, Henrik Plate, Matias Martinez, and Olivier Barais. 2022. Tax-
onomy of Attacks on Open-Source Software Supply Chains. arXiv preprint
arXiv:2204.04008 (2022).

[27] Chengwei Liu, Sen Chen, Lingling Fan, Bihuan Chen, Yang Liu, and Xin Peng.
2022. Demystifying the Vulnerability Propagation and Its Evolution via Depen-
dency Trees in the NPM Ecosystem. arXiv preprint arXiv:2201.03981 (2022).
[28] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E Hassan. 2014.
The impact of code review coverage and code review participation on software
quality: A case study of the qt, vtk, and itk projects. In Proceedings of the 11th
Working Conference on Mining Software Repositories. 192–201.

[29] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E Hassan. 2016.
An empirical study of the impact of modern code review practices on software
quality. Empirical Software Engineering 21, 5 (2016), 2146–2189.

[30] Samim Mirhosseini and Chris Parnin. 2017. Can automated pull requests en-
courage software developers to upgrade out-of-date dependencies?. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 84–94.

[31] Marc Ohm, Henrik Plate, Arnold Sykosch, and Michael Meier. 2020. Backstabber’s
knife collection: A review of open source software supply chain attacks. In
International Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment. Springer, 23–43.

[32] Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern code review: a case study at google. In Proceedings of
the 40th International Conference on Software Engineering: Software Engineering
in Practice. 181–190.

[33] Adriana Sejfia and Max Schäfer. 2022. Practical Automated Detection of Malicious

npm Packages. arXiv preprint arXiv:2202.13953 (2022).

[34] Synopsys. 2021.

2021 Open source security and risk analysis re-
port. https://www.synopsys.com/software-integrity/resources/analyst-reports/
open-source-security-risk-analysis.html.

[35] Santiago Torres-Arias, Hammad Afzali, Trishank Karthik Kuppusamy, Reza Curt-
mola, and Justin Cappos. 2019. in-toto: Providing farm-to-table guarantees for bits
and bytes. In 28th USENIX Security Symposium (USENIX Security 19). 1393–1410.
[36] Hugo van Kemenade and Richard Si. 2022. hugovk/top-pypi-packages: Release

2022.01. https://doi.org/10.5281/zenodo.5812615

[37] Duc-Ly Vu, Fabio Massacci, Ivan Pashchenko, Henrik Plate, and Antonino Sabetta.
2021. LastPyMile: identifying the discrepancy between sources and packages.
In Proceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 780–792.
[38] Jon Washburn. 2017. What is FOSS, and why should I be worried about
it? https://www.stoelprivacyblog.com/2017/10/articles/software/what-is-foss-
and-why-should-i-be-worried-about-it/.

[39] Jeong Yang, Young Lee, and Arlen P McDonald. 2021. SolarWinds Software
Supply Chain Security: Better Protection with Enforced Policies and Technolo-
gies. In International Conference on Software Engineering, Artificial Intelligence,
Networking and Parallel/Distributed Computing. Springer, 43–58.

[40] Nusrat Zahan, Laurie Williams, Thomas Zimmermann, Patrice Godefroid, Bren-
dan Murphy, and Chandra Maddila. 2021. What are Weak Links in the npm
Supply Chain? arXiv preprint arXiv:2112.10165 (2021).

[41] Markus Zimmermann, Cristian-Alexandru Staicu, Cam Tenny, and Michael Pradel.
2019. Small world with high risks: A study of security threats in the npm
ecosystem. In 28th {USENIX} Security Symposium ({USENIX} Security 19). 995–
1010.

find that npm and PyPI updates can commonly have phantom arti-
facts in the form of transpiled JavaScript code, compiled binaries,
and other machine-generated files.

Regarding code review coverage, we find that updates are typi-
cally only partially code-reviewed (52.5% of the time). Further, only
9.0% of the packages in our dataset had all their updates fully code-
reviewed, highlighting the fact that even the most used packages
ship non-reviewed code. We also observe that updates tend to have
either very high CRC or very low CRC, indicating that packages at
the opposite end of the spectrum require different treatments. Over-
all, this paper provides an empirical evaluation of our proposed
approach to auditing a dependency update and an ecosystem-level
analysis of code review coverage among the latest updates of the
most downloaded packages.

REFERENCES
[1] 2018. A Tutorial for Tagging Releases in Git. https://dev.to/neshaz/a-tutorial-

for-tagging-releases-in-git-147e.

[2] 2019. 9 Reasons for keeping software dependencies up to date. https://nullbeans.

com/9-reasons-for-keeping-software-dependencies-up-to-date/.

[3] 2020.

Shifting supply chain security left with dependency review.

https://github.blog/2020-12-17-shifting-supply-chain-security-left-with-
dependency-review/.

[4] 2021. Dependabot. https://github.com/dependabot.
[5] 2021. Gerrit Code Review. https://www.gerritcodereview.com/.
[6] 2021. An Overview of Packaging for Python. https://packaging.python.org/en/

latest/overview/.

[7] 2021. Prow. https://github.com/kubernetes/test-infra/tree/master/prow.
[8] 2021. Push and pull: when and why to update your dependencies. https://

pythonspeed.com/articles/when-update-dependencies/.
[9] 2021. Reproducible Builds. https://reproducible-builds.org/.
[10] 2021. Safeguarding artifact integrity across any software supply chain. slsa.dev.
[11] 2021. Security Scorecards. https://github.com/ossf/scorecard.
[12] 2021. Semantic Versioning 2.0.0. https://semver.org/.
[13] 2021. SLSA Requirements. https://slsa.dev/spec/v0.1/requirements.
[14] Mahmoud Alfadel, Diego Elias Costa, and Emad Shihab. 2021. Empirical anal-
ysis of security vulnerabilities in python packages. In 2021 IEEE International
Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE,
446–457.

[15] Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In 2013 35th International Conference on Software
Engineering (ICSE). IEEE, 712–721.

[16] Ethan Bommarito and Michael Bommarito. 2019. An empirical analysis of the

python package index (pypi). arXiv preprint arXiv:1907.11073 (2019).

[17] Amiangshu Bosu, Jeffrey C Carver, Munawar Hafiz, Patrick Hilley, and Derek
Janni. 2014.
Identifying the characteristics of vulnerable code changes: An
empirical study. In Proceedings of the 22nd ACM SIGSOFT international symposium
on foundations of software engineering. 257–268.

[18] W Edwards Deming. 2020. For Good Measure. USENIX PATRONS (2020), 83.
[19] Ruian Duan, Omar Alrawi, Ranjita Pai Kasturi, Ryan Elder, Brendan Saltaformag-
gio, and Wenke Lee. 2020. Towards measuring supply chain attacks on package
managers for interpreted languages. arXiv preprint arXiv:2002.01139 (2020).
[20] Gabriel Ferreira, Limin Jia, Joshua Sunshine, and Christian Kästner. 2021. Con-
taining malicious package updates in NPM with a lightweight permission system.
In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).
IEEE, 1334–1346.
[21] Ryan Flowers. 2021.

SUPPLY CHAIN ATTACK: NPM LIBRARY
https:

USED BY FACEBOOK AND OTHERS WAS COMPROMISED.
//hackaday.com/2021/10/22/supply-chain-attack-npm-library-used-by-
facebook-and-others-was-compromised/.

[22] Pronnoy Goswami, Saksham Gupta, Zhiyuan Li, Na Meng, and Daphne Yao. 2020.
Investigating The Reproducibility of NPM Packages. In 2020 IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE, 677–681.
[23] Nasif Imtiaz, Aniqa Khanom, and Laurie Williams. 2021. Open or Sneaky? Fast or
Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages.
arXiv preprint arXiv:2112.06804 (2021).

[24] Nasif Imtiaz, Seaver Thorn, and Laurie Williams. 2021. A comparative study of
vulnerability reporting by software composition analysis tools. In Proceedings of
the 15th ACM/IEEE International Symposium on Empirical Software Engineering
and Measurement (ESEM). 1–11.

[25] Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin Cao, and Michael W
Godfrey. 2015. Investigating code review quality: Do people and participation

