2
2
0
2

r
p
A
6

]
E
N
.
s
c
[

1
v
2
4
9
2
0
.
4
0
2
2
:
v
i
X
r
a

A Design Methodology for Fault-Tolerant Computing using
Astrocyte Neural Networks

Murat Isik∗
mci38@drexel.edu
Drexel University
Philadelphia, PA, USA

M. Lakshmi Varshika∗
lm3486@drexel.edu
Drexel University
Philadelphia, PA, USA

Ankita Paul∗
ap3737@drexel.edu
Drexel University
Philadelphia, PA, USA

Anup Das
anup.das@drexel.edu
Drexel University
Philadelphia, PA, USA

ABSTRACT
We propose a design methodology to facilitate fault tolerance of
deep learning models. First, we implement a many-core fault-tolerant
neuromorphic hardware design, where neuron and synapse cir-
cuitries in each neuromorphic core are enclosed with astrocyte
circuitries, the star-shaped glial cells of the brain that facilitate self-
repair by restoring the spike firing frequency of a failed neuron
using a closed-loop retrograde feedback signal. Next, we introduce
astrocytes in a deep learning model to achieve the required degree
of tolerance to hardware faults. Finally, we use a system software to
partition the astrocyte-enabled model into clusters and implement
them on the proposed fault-tolerant neuromorphic design. We eval-
uate this design methodology using seven deep learning inference
models and show that it is both area- and power-efficient.

CCS CONCEPTS
• Hardware → Neural systems; • Computer systems orga-
nization → Dependable and fault-tolerant systems and net-
works.

KEYWORDS
astrocyte, neuromorphic computing, fault tolerance

ACM Reference Format:
Murat Isik, Ankita Paul, M. Lakshmi Varshika, and Anup Das. 2022. A
Design Methodology for Fault-Tolerant Computing using Astrocyte Neural
Networks. In CF’22: ACM International Conference on Computing Frontiers,
May 17–19, 2022, Turin, Piedmont, Italy. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/1122445.1122456

∗Authors contributed equally to this research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CF’22, May 17–19, 2022, Turin, Piedmont, Italy
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Modern embedded systems are embracing neuromorphic devices to
implement spiking-based deep learning inference applications [3].
A neuromorphic device is designed as a many-core hardware, where
each core consists of silicon circuitries to implement neurons and
synapses [18]. Although technology scaling has provided a steady
increase of performance, increased power densities (hence temper-
atures) and other scaling effects create an adverse impact on the
reliability by increasing the likelihood of transient, intermittent,
and permanent faults in the neuron and synapse circuitries [13, 14].
Hardware faults introduce errors in a trained deep learning model
implemented on those circuitries, compromising inference quality
(assessed using the accuracy metric). Therefore, providing fault
tolerance is a critical requirement for neuromorphic devices.

Recent efforts to this end include software solutions such as
model replication [9] and error prediction coding [7], and hardware
solutions such as approximation [12] and redundant mapping [20].
For FPGA-based neuromorphic designs, fault tolerance can also
be addressed using periodic scrubbing [11, 19]. In this work, we
propose a complimentary approach to fault tolerance. We exploit
the self-repair capability of the brain, which copes with damaged
neurons using astrocytes, the star-shaped glial cells of the brain [8].
Astrocytes generate an indirect retrograde feedback signal, which
helps to restore the spike firing frequency of a failed neuron [6].

We propose a design methodology for fault-tolerant neuromor-
phic computing, which consists of the following three components.

• We propose a many-core neuromorphic design where neurons
in each core are enclosed with astrocytes to facilitate self-
repair of errors caused by logic and memory faults.

• We introduce astrocytes in a deep learning model to achieve

a desired degree of tolerance to hardware faults.

• We propose a system software to partition an astrocyte-enabled
inference model into clusters and implement them on the pro-
posed fault-tolerant neuromorphic cores of the hardware.

We evaluate our design methodology using seven deep learning
inference models. Results show that the proposed design methodol-
ogy is both area- and power-efficient, yet providing a high degrees
of fault tolerance to randomly injected faults.

 
 
 
 
 
 
2 ASTROCYTE NEURAL NETWORKS
Figure 1 illustrates how an astrocyte regulates the neuronal activity
at a synaptic site using a closed-loop feedback mechanism.

Astrocyte causes a transient increase of intracellular calcium
(𝐶𝑎2+) levels, which serves as the catalyst for self-repair. 𝐶𝑎2+-induced
𝐶𝑎2+ release (CICR) is the main mechanism to regulate 𝐶𝑎2+ in the
healthy brain. CICR is triggered by inosital 1,4,5-triphosphate (𝐼 𝑃3),
which is produced upon astrocyte activation. To describe the opera-
tion of the astrocyte, let 𝛿 (𝑡 − 𝜏) be a spike at time 𝜏 from the neuron
𝑛𝑖 . This spike triggers the release of 2-arachidonyl glycerol (2-AG),
a type of endocannabinoid responsible for stimulating the cytosolic
calcium 𝐶𝑎2+ (cyt). The quantity of 2-AG produced is governed by
the ordinary differential equation (ODE)

𝑑𝐴𝐺
𝑑𝑡

=

−𝐴𝐺
𝜏𝐴𝐺

+ 𝑟𝐴𝐺 · 𝛿 (𝑡 − 𝜏),

(1)

where 𝐴𝐺 is the quantity of 2-AG, 𝜏𝐴𝐺 is the rate of decay and 𝑟𝐴𝐺
is the rate of production of 2-AG.

Figure 1: Operation of an astrocyte (gray blocks).

On one pathway, the cytosolic calcium is absorbed by the en-
doplasmic reticulum (ER) via the Sarco-Endoplasmic-Reticulum
𝐶𝑎2+-ATPase (SERCA) pumps, and on the other pathway, the cy-
tosolic calcium enhances the Phospholipase C (PLC) activation
process. This event increases 𝐼 𝑃3 production and ER intracellular
calcium release via the CICR mechanism.

The intracellular astrocytic calcium dynamics control the gluta-

mate (Glu) release from the astrocyte, which is governed by
−𝐺𝑙𝑢
𝜏𝐺𝑙𝑢

+ 𝑟𝐺𝑙𝑢 (𝑡 − 𝑡𝐶𝑎),

𝑑𝐺𝑙𝑢
𝑑𝑡

=

(2)

where 𝜏𝐺𝑙𝑢 is the rate of decay and 𝑟𝐺𝑙𝑢 is the rate of production
of glutamate, and 𝑡𝐶𝑎 is time at which 𝐶𝑎2+ crosses the release
threshold. The glutamate generates e-SP, the indirect signal to the
synaptic site. e-SP is related to Glu using the following ODE
−𝑒𝑆𝑃
𝜏𝑒𝑆𝑃

𝑚𝑒𝑆𝑃
𝜏𝑒𝑆𝑃

𝑑𝑒𝑆𝑃
𝑑𝑡

𝐺𝑙𝑢 (𝑡 ),

(3)

=

+

(a) Original network.
(b) Astrocyte-modulated network.
Figure 2: Inserting an astrocyte in a neural network.

(Eq. 4). Therefore, the PR increases (Eq. 5) along with an increase
of the spike firing frequency at the synaptic site.

Figure 3 illustrates the self-repair mechanism. The input neuron
𝑛𝑖 is excited with Poisson spike events having a mean spike rate
of 60Hz. We interrupt the input at around 50 sec. We observe that
the firing frequency at the synaptic site connected to 𝑛𝑖 drops to 0.
This is indicated with the label output (fault). Using astrocyte, the
firing frequency can be restored partially as illustrated using the
label output (astrocyte).

Figure 3: Self-repair mechanism of an astrocyte.

3 PROPOSED DESIGN METHODOLOGY
3.1 Novel Hardware With Astrocyte Circuitries
Figure 4 shows the architecture of a many-core neuromorphic hard-
ware (left sub-figure). We take the example of two recent designs –
DYNAPs [5], where each core consists of an N ×N crossbar with 𝑁
pre-synaptic neurons connected to 𝑁 post-synaptic neurons (middle
sub-figure), and 𝜇Brain [18], where each core consists of neurons
that are organized in three layers with 𝑁 neurons in layer 1, 𝑀
neurons in layer 2, and 𝑃 neurons in layer 3 (right sub-figure).

where 𝜏𝑒𝑆𝑃 is the decay rate of e-SP and 𝑚𝑒𝑆𝑃 is a scaling factor.

Finally, there exists a direct signaling pathway (DSE) from neu-

ron 𝑛𝑖 to the synaptic site. The DSE is given by

𝐷𝑆𝐸 = −𝐾𝐴𝐺 · 𝐴𝐺 (𝑡 ),

(4)

Figure 4: Baseline architecture of a neuromorphic hardware.

where 𝐾𝐴𝐺 is a constant. Overall, the synaptic transmission proba-
bility (PR) at the synaptic site is

Figure 5 illustrates our proposed changes to a baseline crossbar

(left sub-figure) and a baseline 𝜇Brain (right sub-figure) design.

𝑃𝑅 (𝑡 ) = 𝑃𝑅 (0) + 𝑃𝑅 (0)

(cid:18) 𝐷𝑆𝐸 (𝑡 ) + 𝑒𝑆𝑃 (𝑡 )
100

(cid:19)

(5)

In the brain, each astrocyte encloses multiple synapses connected
to a neuron. Figure 2a shows an original network of neurons, while
Figure 2b shows these neurons enclosed using an astrocyte.

To understand the self-repair mechanism, consider neuron 𝑛𝑖 in
Fig. 1 fails to fire a spike. Without the astrocyte, the spike firing
rate at the synaptic site would decreases. However, because of the
astrocyte, 2-AG production reduces (Eq. 1). This increases the DSE

Figure 5: Proposed design of crossbar (left) and 𝜇Brain
(right).

20406080100Time(sec)0255075100SpikeFrequency(Hz)input(nofault)input(fault)output(nofault)output(fault)output(astrocyte)C1C2C3C5C6C7C9C10C11C13C14C15C0C4C8C12• Brian 2 [16]: for astrocyte modeling.
• ARES [10]: for fault simulations.
• Xilinx Vivado: for FPGA synthesis.

4.1 Astrocyte Area and Power
We implemented the astrocyte design, the baseline 𝜇Brain and cross-
bar designs on Xilinx VCU128 development board (see Table 1). We
observe that although an astrocyte circuitry is smaller than the size
of a 𝜇Brain (336 neurons) and a crossbar (256 neurons), it is in fact,
significantly larger and consumes significantly higher power than
a single neuron circuitry. Furthermore, an astrocyte circuitry uses
more flip flops (FF), slices, and lookup tables (LUTs) than the two
baseline designs. The higher area of the two baseline designs are
due to the use of more block RAMs (BRAMs). The power consump-
tion of an astrocyte design is shown in Figure 6, distributed into
clocks, signals, logic, DSP, BRAM, MMCM, and I/O.

Table 1: Implementation of an astrocyte and the baseline
𝜇Brain [18] and crossbar [5] designs on Xilinx VCU128.

Neurons
Synapses
Operating Frequency
BRAM
DSP
FF
Slice
LUT
FPGA Utilization
Power

𝜇Brain [18] Crossbar [5] Astrocyte
256
16,384
100MHz
32
0
86
78
76
40%
4.53W

336
17,408
100MHz
48
0
129
117
114
49%
4.64W

–
–
100MHz
4
4
2,368
670
1,345
12%
0.538 W

3.2 Software Mapping Framework
A single neuromorphic core can implement only a limited number
of neurons and synapses. A 128 × 128 crossbar core consists of 128
input and 128 output neurons, while a 𝜇Brain core consists of 256
neurons in layer 1, 64 neurons in layer 2, and 16 neurons in layer
3. We use a distance-based heuristic [18] to partition an inference
model into clusters, where each cluster can be implemented on a
core of the hardware.1 It sorts all neurons of a model based on their
distances from output neurons. For 𝜇Brain (crossbar) mapping, it
groups all neurons with distance less than or equal to 2 (1) into
clusters considering the resource constraint of a core. In the next
iteration, it removes already clustered neurons from the model,
recalculates neuron distances, and groups remaining neurons to
generate the next set of clusters. The process is repeated until all
neurons are clustered. By incorporating hardware constraints, we
ensure that a cluster can fit onto the target core architecture.

3.3 Astrocyte-Enabled Inference Model
We introduce the following notations.
𝐺𝑀 (𝐶, 𝐸) = Inference model with 𝐶 clusters and 𝐸 edges
𝐺𝐴 (𝐶𝐴, 𝐸) = Astrocyte-enabled model with 𝐶𝐴 clusters and 𝐸 edges

𝐿 = Layers of a core. 𝐿 = {𝐿𝑥 , 𝐿𝑦 } (crossbar) and 𝐿 = {𝐿𝑥 , 𝐿𝑦, 𝐿𝑧 } (𝜇Brain)

Algorithm 1 shows the pseudo-code to insert astrocytes in clus-
ters of an inference model 𝐺𝑀 . First, it organizes the neurons of a
cluster into two (for crossbar) or three (for 𝜇Brain) layers (line 2).
Next, for each layer it uses the ARES framework [10] to insert 𝑁𝑟
random errors, one at a time and record the corresponding accuracy
(line 5). If the minimum accuracy 𝑎𝑚𝑖𝑛 is lower than a threshold
𝑎𝑡ℎ, it adds an astrocyte to the layer (lines 6-8). Otherwise, it exits
and analyzes the next layer (lines 8-9). In allocating astrocytes to a
layer, if more than one astrocytes are needed, then its distributes
neurons of the layer equally amongst the astrocytes. 𝑁𝑟 and 𝑎𝑡ℎ
are user defined parameters and they are empirically set to 10,000
and 𝑎𝑜 , respectively, where 𝑎𝑜 is the baseline accuracy of the model
without error. Finally, the astrocyte-enabled model (𝐺𝐴) is returned.

Algorithm 1: Inserting astrocytes in clusters of a model.

Input: 𝐺𝑀 = (C, E)
Output: 𝐺𝐴 = (C𝐴, E)

1 for 𝐶𝑘 ∈ 𝐶 do
𝐶𝑘 = {𝐶𝑥
2
𝑘

, 𝐶 𝑦
𝑘

, 𝐶𝑧

𝑘 };

/* For each cluster in 𝐶 */
/* arrange neurons & synapses of 𝐶𝑘 into three

layers for 𝜇Brain core. For crossbar mapping, 𝐶𝑘 = {𝐶𝑥
𝑘
for 𝐶𝑖

/* For each layer in 𝐶𝑘 */
/* Run until all neurons of the layer are

𝑘 ∈ 𝐶𝑘 do
while (true) do

, 𝐶 𝑦

𝑘 }. */

protected against randomly injected errors */

Insert 𝑁𝑟 random errors using ARES and evaluate the minimum

accuracy 𝑎𝑚𝑖𝑛 ;
if 𝑎min < 𝑎𝑡ℎ then
*/

𝑘 = 𝐶𝑖
𝐶𝑖

𝑘 ∪ A;

else

exit;

/* Min accuracy is less than threshold.

/* Add an astrocyte. */

3

4

5

6

7

8
9

4 EVALUATION
Our simulation framework consists of the following.

• QKeras: to train 2-bit quantized deep learning models.
• PyCARL[1]: to generate spiking inference models.

1Apart from distance-based heuristic, recently heuristic graph partitioning approaches
are also proposed in literature [2, 4, 15].

Figure 6: Power consumption of astrocyte, distributed into
clocks, signals,BRAMs,DSPs,MMCM, and I/Os.

4.2 Fault Tolerance
Figure 7 plots the accuracy, normalized to the replication technique,
of each evaluated model for 10%, 20%, and 50% of parameters in error.
These errors are injected randomly using the ARES framework [10]
and the reported results are average of 10 runs. With 10% error
rate, there are only a few errors per cluster. Therefore, most errors
can be masked by astrocytes that are inserted into each model
cluster. So, we see no accuracy drop. With higher error rates, the
accuracy is lower. This is because of the increase in parameter
errors in each cluster. Errors in multiple neurons of an enclosed
astrocyte impact its ability to restore the spike frequency, causing
a significant amount of accuracy drop. On average, the accuracy is
23% and 54% lower for error rate of 20% and 50%, respectively.

sys_diff_clockreset_1xlconstant_1Constantdout[0:0]xlconstant_2Constantdout[0:0]clk_wizClocking WizardCLK_IN1_Dresetclk_out1lockedtop_0top_v1_0mult_out[31:0]answer[31:0]c_counter_binary_0Binary CounterCLKUPQ[0:0]c_counter_binary_1Binary CounterCLKUPQ[0:0]c_addsub_0Adder/SubtracterA[31:0]B[31:0]CLKS[31:0]memory_pot_0memory_pot_v1_0resetclkenmemory_potential[10:0]xlconstant_3Constantdout[0:0]xlconstant_4Constantdout[0:0]PiP_new_0PiP_new_v1_0M_CLKi_rstbclrvcc_in_check[10:0]in_lfsr[31:0]x1[41:0]y1[41:0]rst_clk_wiz_100MProcessor System Resetslowest_sync_clkext_reset_inaux_reset_inmb_debug_sys_rstdcm_lockedmb_resetbus_struct_reset[0:0]peripheral_reset[0:0]interconnect_aresetn[0:0]peripheral_aresetn[0:0]opu_0opu_v1_0in_1[41:0]in_2[41:0]dac_out[15:0]clkresetila_0ILA (Integrated Logic Analyzer)clkprobe0[10:0]probe1[31:0]probe2[41:0]probe3[41:0]probe4[15:0]xadc_wiz_0XADC Wizards_drpdaddr_in[6:0]den_indi_in[15:0]do_out[15:0]drdy_outdwe_inVp_Vndclk_inreset_inchannel_out[4:0]eoc_outalarm_outeos_outbusy_outchannel_out_0[4:0]Figure4:Blockdesignsofanastrocyte.On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (a).On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (b).On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O                     On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (c).Figure5:Insertinganastrocyteinaneuralnetwork.Figure 7: Normalized accuracy for different error rates.

4.3 Design Tradeoffs
Figure 8 shows the area of a 𝜇Brain-based design normalized to
the replication technique for three error rates – 10%, 20%, and 30%.
The accuracy constraint is set as the accuracy without error. This
accuracy constraint is achieved for 10% error rate using our baseline
design. So there is no area overhead. For 20% and 50% error rates,
more astrocytes are needed to achieve the accuracy constraint. On
average, the proposed design requires 28% and 49% higher area for
20% and 50% error rate, respectively.

Figure 8: Normalized area for different error rates.

4.4 Model Area
Table 2 reports the design area for each of the evaluated deep
learning inference models using 1) model replication technique [9],
2) redundant mapping technique [20], and 3) the proposed design
methodology. Design areas are reported for both the 𝜇Brain-based
core [18] and the crossbar-based core [5]. All results are normalized
to the 𝜇Brain-based design implementing the LeNet model using
the model replication technique. We make three key observations.

Table 2: Design area compared to model replication [9] and
redundant mapping [20].

Model
Redundant
Mapping [20]
Replication [9]
𝜇Brain crossbar 𝜇Brain crossbar 𝜇Brain crossbar

Proposed
Design

LeNet
AlexNet
VGGNet
ResNet
DenseNet
MobileNet
Xception

1.0
79.0
62.9
1.1
13.5
4.4
40

0.8
68.5
54.6
0.9
11.7
3.8
34.7

–
–
–
–
–
–
–

0.7
54.8
43.7
0.8
9.4
3.0
27.7

0.5
39.2
31.2
0.6
6.7
2.2
19.9

0.4
33.1
26.4
0.5
5.7
1.8
16.8

First, design area is larger for models with higher number of
parameters. This is because models with more parameters require
more clusters (cores), which increases the design area. Second,
the redundant mapping technique is only applicable to crossbar-
based designs. Therefore, results for the 𝜇Brain-based design are
not provided. Third, for the 𝜇Brain-based design, the proposed de-
sign methodology results in 50% lower area than the replication
technique. For the crossbar-based design, it results in 51.6% lower
area than the replication technique and 39.5% lower area than the
redundant mapping technique. These improvements are because
implementing a few astrocytes in a baseline 𝜇Brain and crossbar
designs is area-efficient than 1) replicating model clusters, which re-
quires more cores to implement a model, and 2) redundant mapping,
which requires larger crossbars to implement each cluster.

4.5 Model Power
Figure 9 reports the power for each evaluated model on a crossbar-
based design using the three evaluated approaches. Power numbers

for each core is calculated based on the static power of the de-
sign and the activation of the synaptic weights in the core [17].
We make two key observations. First, power is higher for models
such as AlexNet, VGGNet, and Xception due to higher number of
model parameters. Second, on average, power using the proposed
design methodology is 60% lower than replication technique and
50% lower than redundant mapping technique. For 𝜇Brain-based
design (not shown here for space limitations), power using the
proposed methodology is 60% lower than the replication technique.

Figure 9: Power consumption.

5 CONCLUSIONS
We propose a design methodology for fault-tolerant neuromorphic
computing. First, we propose a novel design, where a core con-
sists of neuron, synapse, and astrocyte circuitries. Each astrocyte
encloses multiple neurons to facilitate self-repair of a failed neu-
ron. Next, we insert astrocytes in an inference model to achieve
the desired degree of fault tolerance. Finally, we propose a system
software framework to map astrocyte-enabled inference model to
the proposed fault-tolerant many-core design. We evaluate the pro-
posed design methodology using several deep learning models on
the fault-tolerant implementation of two baseline neuromorphic
designs. We show that the proposed design methodology is both
area and power-efficient, yet providing similar degrees of fault
tolerance compared to existing approaches.

ACKNOWLEDGMENTS
This work is supported by the National Science Foundation Faculty
Early Career Development Award CCF-1942697 (CAREER: Facili-
tating Dependable Neuromorphic Computing: Vision, Architecture,
and Impact on Programmability).

REFERENCES
[1] A. Balaji et al., “PyCARL: A PyNN interface for hardware-software co-simulation

of spiking neural network,” in IJCNN, 2020.

[2] A. Balaji et al., “Mapping spiking neural networks to neuromorphic hardware,”

TVLSI, 2020.

[3] Y. Cao et al., “Spiking deep convolutional neural networks for energy-efficient

object recognition,” IJCV, 2015.

[4] P. K. Huynh et al., “Implementing spiking neural networks on neuromorphic

architectures: A review,” arXiv, 2022.

[5] S. Moradi et al., “A scalable multicore architecture with heterogeneous mem-
ory structures for dynamic neuromorphic asynchronous processors (DYNAPs),”
TBCAS, 2017.

[6] S. Nadkarni et al., “Modeling synaptic transmission of the tripartite synapse,”

Physical Biology, 2007.

[7] S. Park et al., “Low-cost prediction-based fault protection strategy,” in CGO, 2020.
[8] V. Parpura et al., “Glutamate-mediated astrocyte–neuron signalling,” Nature, 1994.
[9] F. Ponzina et al., “E2CNNs: Ensembles of convolutional neural networks to
improve robustness against memory errors in edge-computing devices,” TC, 2021.
[10] B. Reagen et al., “Ares: A framework for quantifying the resilience of deep neural

networks,” in DAC, 2018.

[11] R. Santos et al., “Criticality-aware scrubbing mechanism for SRAM-based FPGAs,”

in FPL, 2014.

[12] A. Siddique et al., “Exploring fault-energy trade-offs in approximate DNN hard-

ware accelerators,” in ISQED, 2021.

[13] S. Song et al., “A case for lifetime reliability-aware neuromorphic computing,” in

MWSCAS, 2020.

LeNetAlexNetVGGNetResNetDenseNetMobileNetXceptionAVERAGE01Norm.Acc.Errorrate=10%20%50%LeNetAlexNetVGGNetResNetDenseNetMobileNetXceptionAVERAGE01Norm.AreaErrorrate=10%20%50%LeNetAlexNetVGGNetResNetDenseNetMobileNetXception0255075100Norm.Power1.00.80.40.51.20.90.50.6ReplicationRedundantProposed[14] S. Song et al., “Improving dependability of neuromorphic computing with non-

[18] M. L. Varshika et al., “Design of many-core big little 𝜇Brains for energy-efficient

volatile memory,” in EDCC, 2020.

embedded neuromorphic computing,” in DATE, 2022.

[15] S. Song et al., “DFSynthesizer: Dataflow-based synthesis of spiking neural net-

[19] S. Venkataraman et al., “A bit-interleaved embedded hamming scheme to correct

works to neuromorphic hardware,” TECS, 2021.

single-bit and multi-bit upsets for SRAM-based FPGAs,” in FPL, 2014.

[16] M. Stimberg et al., “Modeling neuron–glia interactions with the Brian 2 simulator,”

[20] G. Yuan et al., “Improving DNN fault tolerance using weight pruning and differ-

in Computational Glioscience, 2019.

ential crossbar mapping for ReRAM-based edge AI,” in ISQED, 2021.

[17] T. Titirsha et al., “On the role of system software in energy management of

neuromorphic computing,” in CF, 2021.

