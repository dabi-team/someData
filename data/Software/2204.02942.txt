2
2
0
2

r
p
A
6

]
E
N
.
s
c
[

1
v
2
4
9
2
0
.
4
0
2
2
:
v
i
X
r
a

A Design Methodology for Fault-Tolerant Computing using
Astrocyte Neural Networks

Murat Isikâˆ—
mci38@drexel.edu
Drexel University
Philadelphia, PA, USA

M. Lakshmi Varshikaâˆ—
lm3486@drexel.edu
Drexel University
Philadelphia, PA, USA

Ankita Paulâˆ—
ap3737@drexel.edu
Drexel University
Philadelphia, PA, USA

Anup Das
anup.das@drexel.edu
Drexel University
Philadelphia, PA, USA

ABSTRACT
We propose a design methodology to facilitate fault tolerance of
deep learning models. First, we implement a many-core fault-tolerant
neuromorphic hardware design, where neuron and synapse cir-
cuitries in each neuromorphic core are enclosed with astrocyte
circuitries, the star-shaped glial cells of the brain that facilitate self-
repair by restoring the spike firing frequency of a failed neuron
using a closed-loop retrograde feedback signal. Next, we introduce
astrocytes in a deep learning model to achieve the required degree
of tolerance to hardware faults. Finally, we use a system software to
partition the astrocyte-enabled model into clusters and implement
them on the proposed fault-tolerant neuromorphic design. We eval-
uate this design methodology using seven deep learning inference
models and show that it is both area- and power-efficient.

CCS CONCEPTS
â€¢ Hardware â†’ Neural systems; â€¢ Computer systems orga-
nization â†’ Dependable and fault-tolerant systems and net-
works.

KEYWORDS
astrocyte, neuromorphic computing, fault tolerance

ACM Reference Format:
Murat Isik, Ankita Paul, M. Lakshmi Varshika, and Anup Das. 2022. A
Design Methodology for Fault-Tolerant Computing using Astrocyte Neural
Networks. In CFâ€™22: ACM International Conference on Computing Frontiers,
May 17â€“19, 2022, Turin, Piedmont, Italy. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/1122445.1122456

âˆ—Authors contributed equally to this research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CFâ€™22, May 17â€“19, 2022, Turin, Piedmont, Italy
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Modern embedded systems are embracing neuromorphic devices to
implement spiking-based deep learning inference applications [3].
A neuromorphic device is designed as a many-core hardware, where
each core consists of silicon circuitries to implement neurons and
synapses [18]. Although technology scaling has provided a steady
increase of performance, increased power densities (hence temper-
atures) and other scaling effects create an adverse impact on the
reliability by increasing the likelihood of transient, intermittent,
and permanent faults in the neuron and synapse circuitries [13, 14].
Hardware faults introduce errors in a trained deep learning model
implemented on those circuitries, compromising inference quality
(assessed using the accuracy metric). Therefore, providing fault
tolerance is a critical requirement for neuromorphic devices.

Recent efforts to this end include software solutions such as
model replication [9] and error prediction coding [7], and hardware
solutions such as approximation [12] and redundant mapping [20].
For FPGA-based neuromorphic designs, fault tolerance can also
be addressed using periodic scrubbing [11, 19]. In this work, we
propose a complimentary approach to fault tolerance. We exploit
the self-repair capability of the brain, which copes with damaged
neurons using astrocytes, the star-shaped glial cells of the brain [8].
Astrocytes generate an indirect retrograde feedback signal, which
helps to restore the spike firing frequency of a failed neuron [6].

We propose a design methodology for fault-tolerant neuromor-
phic computing, which consists of the following three components.

â€¢ We propose a many-core neuromorphic design where neurons
in each core are enclosed with astrocytes to facilitate self-
repair of errors caused by logic and memory faults.

â€¢ We introduce astrocytes in a deep learning model to achieve

a desired degree of tolerance to hardware faults.

â€¢ We propose a system software to partition an astrocyte-enabled
inference model into clusters and implement them on the pro-
posed fault-tolerant neuromorphic cores of the hardware.

We evaluate our design methodology using seven deep learning
inference models. Results show that the proposed design methodol-
ogy is both area- and power-efficient, yet providing a high degrees
of fault tolerance to randomly injected faults.

 
 
 
 
 
 
2 ASTROCYTE NEURAL NETWORKS
Figure 1 illustrates how an astrocyte regulates the neuronal activity
at a synaptic site using a closed-loop feedback mechanism.

Astrocyte causes a transient increase of intracellular calcium
(ğ¶ğ‘2+) levels, which serves as the catalyst for self-repair. ğ¶ğ‘2+-induced
ğ¶ğ‘2+ release (CICR) is the main mechanism to regulate ğ¶ğ‘2+ in the
healthy brain. CICR is triggered by inosital 1,4,5-triphosphate (ğ¼ ğ‘ƒ3),
which is produced upon astrocyte activation. To describe the opera-
tion of the astrocyte, let ğ›¿ (ğ‘¡ âˆ’ ğœ) be a spike at time ğœ from the neuron
ğ‘›ğ‘– . This spike triggers the release of 2-arachidonyl glycerol (2-AG),
a type of endocannabinoid responsible for stimulating the cytosolic
calcium ğ¶ğ‘2+ (cyt). The quantity of 2-AG produced is governed by
the ordinary differential equation (ODE)

ğ‘‘ğ´ğº
ğ‘‘ğ‘¡

=

âˆ’ğ´ğº
ğœğ´ğº

+ ğ‘Ÿğ´ğº Â· ğ›¿ (ğ‘¡ âˆ’ ğœ),

(1)

where ğ´ğº is the quantity of 2-AG, ğœğ´ğº is the rate of decay and ğ‘Ÿğ´ğº
is the rate of production of 2-AG.

Figure 1: Operation of an astrocyte (gray blocks).

On one pathway, the cytosolic calcium is absorbed by the en-
doplasmic reticulum (ER) via the Sarco-Endoplasmic-Reticulum
ğ¶ğ‘2+-ATPase (SERCA) pumps, and on the other pathway, the cy-
tosolic calcium enhances the Phospholipase C (PLC) activation
process. This event increases ğ¼ ğ‘ƒ3 production and ER intracellular
calcium release via the CICR mechanism.

The intracellular astrocytic calcium dynamics control the gluta-

mate (Glu) release from the astrocyte, which is governed by
âˆ’ğºğ‘™ğ‘¢
ğœğºğ‘™ğ‘¢

+ ğ‘Ÿğºğ‘™ğ‘¢ (ğ‘¡ âˆ’ ğ‘¡ğ¶ğ‘),

ğ‘‘ğºğ‘™ğ‘¢
ğ‘‘ğ‘¡

=

(2)

where ğœğºğ‘™ğ‘¢ is the rate of decay and ğ‘Ÿğºğ‘™ğ‘¢ is the rate of production
of glutamate, and ğ‘¡ğ¶ğ‘ is time at which ğ¶ğ‘2+ crosses the release
threshold. The glutamate generates e-SP, the indirect signal to the
synaptic site. e-SP is related to Glu using the following ODE
âˆ’ğ‘’ğ‘†ğ‘ƒ
ğœğ‘’ğ‘†ğ‘ƒ

ğ‘šğ‘’ğ‘†ğ‘ƒ
ğœğ‘’ğ‘†ğ‘ƒ

ğ‘‘ğ‘’ğ‘†ğ‘ƒ
ğ‘‘ğ‘¡

ğºğ‘™ğ‘¢ (ğ‘¡ ),

(3)

=

+

(a) Original network.
(b) Astrocyte-modulated network.
Figure 2: Inserting an astrocyte in a neural network.

(Eq. 4). Therefore, the PR increases (Eq. 5) along with an increase
of the spike firing frequency at the synaptic site.

Figure 3 illustrates the self-repair mechanism. The input neuron
ğ‘›ğ‘– is excited with Poisson spike events having a mean spike rate
of 60Hz. We interrupt the input at around 50 sec. We observe that
the firing frequency at the synaptic site connected to ğ‘›ğ‘– drops to 0.
This is indicated with the label output (fault). Using astrocyte, the
firing frequency can be restored partially as illustrated using the
label output (astrocyte).

Figure 3: Self-repair mechanism of an astrocyte.

3 PROPOSED DESIGN METHODOLOGY
3.1 Novel Hardware With Astrocyte Circuitries
Figure 4 shows the architecture of a many-core neuromorphic hard-
ware (left sub-figure). We take the example of two recent designs â€“
DYNAPs [5], where each core consists of an N Ã—N crossbar with ğ‘
pre-synaptic neurons connected to ğ‘ post-synaptic neurons (middle
sub-figure), and ğœ‡Brain [18], where each core consists of neurons
that are organized in three layers with ğ‘ neurons in layer 1, ğ‘€
neurons in layer 2, and ğ‘ƒ neurons in layer 3 (right sub-figure).

where ğœğ‘’ğ‘†ğ‘ƒ is the decay rate of e-SP and ğ‘šğ‘’ğ‘†ğ‘ƒ is a scaling factor.

Finally, there exists a direct signaling pathway (DSE) from neu-

ron ğ‘›ğ‘– to the synaptic site. The DSE is given by

ğ·ğ‘†ğ¸ = âˆ’ğ¾ğ´ğº Â· ğ´ğº (ğ‘¡ ),

(4)

Figure 4: Baseline architecture of a neuromorphic hardware.

where ğ¾ğ´ğº is a constant. Overall, the synaptic transmission proba-
bility (PR) at the synaptic site is

Figure 5 illustrates our proposed changes to a baseline crossbar

(left sub-figure) and a baseline ğœ‡Brain (right sub-figure) design.

ğ‘ƒğ‘… (ğ‘¡ ) = ğ‘ƒğ‘… (0) + ğ‘ƒğ‘… (0)

(cid:18) ğ·ğ‘†ğ¸ (ğ‘¡ ) + ğ‘’ğ‘†ğ‘ƒ (ğ‘¡ )
100

(cid:19)

(5)

In the brain, each astrocyte encloses multiple synapses connected
to a neuron. Figure 2a shows an original network of neurons, while
Figure 2b shows these neurons enclosed using an astrocyte.

To understand the self-repair mechanism, consider neuron ğ‘›ğ‘– in
Fig. 1 fails to fire a spike. Without the astrocyte, the spike firing
rate at the synaptic site would decreases. However, because of the
astrocyte, 2-AG production reduces (Eq. 1). This increases the DSE

Figure 5: Proposed design of crossbar (left) and ğœ‡Brain
(right).

20406080100Time(sec)0255075100SpikeFrequency(Hz)input(nofault)input(fault)output(nofault)output(fault)output(astrocyte)C1C2C3C5C6C7C9C10C11C13C14C15C0C4C8C12â€¢ Brian 2 [16]: for astrocyte modeling.
â€¢ ARES [10]: for fault simulations.
â€¢ Xilinx Vivado: for FPGA synthesis.

4.1 Astrocyte Area and Power
We implemented the astrocyte design, the baseline ğœ‡Brain and cross-
bar designs on Xilinx VCU128 development board (see Table 1). We
observe that although an astrocyte circuitry is smaller than the size
of a ğœ‡Brain (336 neurons) and a crossbar (256 neurons), it is in fact,
significantly larger and consumes significantly higher power than
a single neuron circuitry. Furthermore, an astrocyte circuitry uses
more flip flops (FF), slices, and lookup tables (LUTs) than the two
baseline designs. The higher area of the two baseline designs are
due to the use of more block RAMs (BRAMs). The power consump-
tion of an astrocyte design is shown in Figure 6, distributed into
clocks, signals, logic, DSP, BRAM, MMCM, and I/O.

Table 1: Implementation of an astrocyte and the baseline
ğœ‡Brain [18] and crossbar [5] designs on Xilinx VCU128.

Neurons
Synapses
Operating Frequency
BRAM
DSP
FF
Slice
LUT
FPGA Utilization
Power

ğœ‡Brain [18] Crossbar [5] Astrocyte
256
16,384
100MHz
32
0
86
78
76
40%
4.53W

336
17,408
100MHz
48
0
129
117
114
49%
4.64W

â€“
â€“
100MHz
4
4
2,368
670
1,345
12%
0.538 W

3.2 Software Mapping Framework
A single neuromorphic core can implement only a limited number
of neurons and synapses. A 128 Ã— 128 crossbar core consists of 128
input and 128 output neurons, while a ğœ‡Brain core consists of 256
neurons in layer 1, 64 neurons in layer 2, and 16 neurons in layer
3. We use a distance-based heuristic [18] to partition an inference
model into clusters, where each cluster can be implemented on a
core of the hardware.1 It sorts all neurons of a model based on their
distances from output neurons. For ğœ‡Brain (crossbar) mapping, it
groups all neurons with distance less than or equal to 2 (1) into
clusters considering the resource constraint of a core. In the next
iteration, it removes already clustered neurons from the model,
recalculates neuron distances, and groups remaining neurons to
generate the next set of clusters. The process is repeated until all
neurons are clustered. By incorporating hardware constraints, we
ensure that a cluster can fit onto the target core architecture.

3.3 Astrocyte-Enabled Inference Model
We introduce the following notations.
ğºğ‘€ (ğ¶, ğ¸) = Inference model with ğ¶ clusters and ğ¸ edges
ğºğ´ (ğ¶ğ´, ğ¸) = Astrocyte-enabled model with ğ¶ğ´ clusters and ğ¸ edges

ğ¿ = Layers of a core. ğ¿ = {ğ¿ğ‘¥ , ğ¿ğ‘¦ } (crossbar) and ğ¿ = {ğ¿ğ‘¥ , ğ¿ğ‘¦, ğ¿ğ‘§ } (ğœ‡Brain)

Algorithm 1 shows the pseudo-code to insert astrocytes in clus-
ters of an inference model ğºğ‘€ . First, it organizes the neurons of a
cluster into two (for crossbar) or three (for ğœ‡Brain) layers (line 2).
Next, for each layer it uses the ARES framework [10] to insert ğ‘ğ‘Ÿ
random errors, one at a time and record the corresponding accuracy
(line 5). If the minimum accuracy ğ‘ğ‘šğ‘–ğ‘› is lower than a threshold
ğ‘ğ‘¡â„, it adds an astrocyte to the layer (lines 6-8). Otherwise, it exits
and analyzes the next layer (lines 8-9). In allocating astrocytes to a
layer, if more than one astrocytes are needed, then its distributes
neurons of the layer equally amongst the astrocytes. ğ‘ğ‘Ÿ and ğ‘ğ‘¡â„
are user defined parameters and they are empirically set to 10,000
and ğ‘ğ‘œ , respectively, where ğ‘ğ‘œ is the baseline accuracy of the model
without error. Finally, the astrocyte-enabled model (ğºğ´) is returned.

Algorithm 1: Inserting astrocytes in clusters of a model.

Input: ğºğ‘€ = (C, E)
Output: ğºğ´ = (Cğ´, E)

1 for ğ¶ğ‘˜ âˆˆ ğ¶ do
ğ¶ğ‘˜ = {ğ¶ğ‘¥
2
ğ‘˜

, ğ¶ ğ‘¦
ğ‘˜

, ğ¶ğ‘§

ğ‘˜ };

/* For each cluster in ğ¶ */
/* arrange neurons & synapses of ğ¶ğ‘˜ into three

layers for ğœ‡Brain core. For crossbar mapping, ğ¶ğ‘˜ = {ğ¶ğ‘¥
ğ‘˜
for ğ¶ğ‘–

/* For each layer in ğ¶ğ‘˜ */
/* Run until all neurons of the layer are

ğ‘˜ âˆˆ ğ¶ğ‘˜ do
while (true) do

, ğ¶ ğ‘¦

ğ‘˜ }. */

protected against randomly injected errors */

Insert ğ‘ğ‘Ÿ random errors using ARES and evaluate the minimum

accuracy ğ‘ğ‘šğ‘–ğ‘› ;
if ğ‘min < ğ‘ğ‘¡â„ then
*/

ğ‘˜ = ğ¶ğ‘–
ğ¶ğ‘–

ğ‘˜ âˆª A;

else

exit;

/* Min accuracy is less than threshold.

/* Add an astrocyte. */

3

4

5

6

7

8
9

4 EVALUATION
Our simulation framework consists of the following.

â€¢ QKeras: to train 2-bit quantized deep learning models.
â€¢ PyCARL[1]: to generate spiking inference models.

1Apart from distance-based heuristic, recently heuristic graph partitioning approaches
are also proposed in literature [2, 4, 15].

Figure 6: Power consumption of astrocyte, distributed into
clocks, signals,BRAMs,DSPs,MMCM, and I/Os.

4.2 Fault Tolerance
Figure 7 plots the accuracy, normalized to the replication technique,
of each evaluated model for 10%, 20%, and 50% of parameters in error.
These errors are injected randomly using the ARES framework [10]
and the reported results are average of 10 runs. With 10% error
rate, there are only a few errors per cluster. Therefore, most errors
can be masked by astrocytes that are inserted into each model
cluster. So, we see no accuracy drop. With higher error rates, the
accuracy is lower. This is because of the increase in parameter
errors in each cluster. Errors in multiple neurons of an enclosed
astrocyte impact its ability to restore the spike frequency, causing
a significant amount of accuracy drop. On average, the accuracy is
23% and 54% lower for error rate of 20% and 50%, respectively.

sys_diff_clockreset_1xlconstant_1Constantdout[0:0]xlconstant_2Constantdout[0:0]clk_wizClocking WizardCLK_IN1_Dresetclk_out1lockedtop_0top_v1_0mult_out[31:0]answer[31:0]c_counter_binary_0Binary CounterCLKUPQ[0:0]c_counter_binary_1Binary CounterCLKUPQ[0:0]c_addsub_0Adder/SubtracterA[31:0]B[31:0]CLKS[31:0]memory_pot_0memory_pot_v1_0resetclkenmemory_potential[10:0]xlconstant_3Constantdout[0:0]xlconstant_4Constantdout[0:0]PiP_new_0PiP_new_v1_0M_CLKi_rstbclrvcc_in_check[10:0]in_lfsr[31:0]x1[41:0]y1[41:0]rst_clk_wiz_100MProcessor System Resetslowest_sync_clkext_reset_inaux_reset_inmb_debug_sys_rstdcm_lockedmb_resetbus_struct_reset[0:0]peripheral_reset[0:0]interconnect_aresetn[0:0]peripheral_aresetn[0:0]opu_0opu_v1_0in_1[41:0]in_2[41:0]dac_out[15:0]clkresetila_0ILA (Integrated Logic Analyzer)clkprobe0[10:0]probe1[31:0]probe2[41:0]probe3[41:0]probe4[15:0]xadc_wiz_0XADC Wizards_drpdaddr_in[6:0]den_indi_in[15:0]do_out[15:0]drdy_outdwe_inVp_Vndclk_inreset_inchannel_out[4:0]eoc_outalarm_outeos_outbusy_outchannel_out_0[4:0]Figure4:Blockdesignsofanastrocyte.On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (a).On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (b).On-Chip Power XC6VLX240-1 0% 10% 36% 18% 12% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O                     On-Chip Power XC7VX485T-2 1% 10% 37% 17% 11% 12% 12% Clocks Signals Logic BRAM DSP MMCM I/O On-Chip Power XCVU37P-L2 0% 10% 12% 44% 14% 11% 9% Clocks Signals Logic BRAM DSP MMCM I/O (c).Figure5:Insertinganastrocyteinaneuralnetwork.Figure 7: Normalized accuracy for different error rates.

4.3 Design Tradeoffs
Figure 8 shows the area of a ğœ‡Brain-based design normalized to
the replication technique for three error rates â€“ 10%, 20%, and 30%.
The accuracy constraint is set as the accuracy without error. This
accuracy constraint is achieved for 10% error rate using our baseline
design. So there is no area overhead. For 20% and 50% error rates,
more astrocytes are needed to achieve the accuracy constraint. On
average, the proposed design requires 28% and 49% higher area for
20% and 50% error rate, respectively.

Figure 8: Normalized area for different error rates.

4.4 Model Area
Table 2 reports the design area for each of the evaluated deep
learning inference models using 1) model replication technique [9],
2) redundant mapping technique [20], and 3) the proposed design
methodology. Design areas are reported for both the ğœ‡Brain-based
core [18] and the crossbar-based core [5]. All results are normalized
to the ğœ‡Brain-based design implementing the LeNet model using
the model replication technique. We make three key observations.

Table 2: Design area compared to model replication [9] and
redundant mapping [20].

Model
Redundant
Mapping [20]
Replication [9]
ğœ‡Brain crossbar ğœ‡Brain crossbar ğœ‡Brain crossbar

Proposed
Design

LeNet
AlexNet
VGGNet
ResNet
DenseNet
MobileNet
Xception

1.0
79.0
62.9
1.1
13.5
4.4
40

0.8
68.5
54.6
0.9
11.7
3.8
34.7

â€“
â€“
â€“
â€“
â€“
â€“
â€“

0.7
54.8
43.7
0.8
9.4
3.0
27.7

0.5
39.2
31.2
0.6
6.7
2.2
19.9

0.4
33.1
26.4
0.5
5.7
1.8
16.8

First, design area is larger for models with higher number of
parameters. This is because models with more parameters require
more clusters (cores), which increases the design area. Second,
the redundant mapping technique is only applicable to crossbar-
based designs. Therefore, results for the ğœ‡Brain-based design are
not provided. Third, for the ğœ‡Brain-based design, the proposed de-
sign methodology results in 50% lower area than the replication
technique. For the crossbar-based design, it results in 51.6% lower
area than the replication technique and 39.5% lower area than the
redundant mapping technique. These improvements are because
implementing a few astrocytes in a baseline ğœ‡Brain and crossbar
designs is area-efficient than 1) replicating model clusters, which re-
quires more cores to implement a model, and 2) redundant mapping,
which requires larger crossbars to implement each cluster.

4.5 Model Power
Figure 9 reports the power for each evaluated model on a crossbar-
based design using the three evaluated approaches. Power numbers

for each core is calculated based on the static power of the de-
sign and the activation of the synaptic weights in the core [17].
We make two key observations. First, power is higher for models
such as AlexNet, VGGNet, and Xception due to higher number of
model parameters. Second, on average, power using the proposed
design methodology is 60% lower than replication technique and
50% lower than redundant mapping technique. For ğœ‡Brain-based
design (not shown here for space limitations), power using the
proposed methodology is 60% lower than the replication technique.

Figure 9: Power consumption.

5 CONCLUSIONS
We propose a design methodology for fault-tolerant neuromorphic
computing. First, we propose a novel design, where a core con-
sists of neuron, synapse, and astrocyte circuitries. Each astrocyte
encloses multiple neurons to facilitate self-repair of a failed neu-
ron. Next, we insert astrocytes in an inference model to achieve
the desired degree of fault tolerance. Finally, we propose a system
software framework to map astrocyte-enabled inference model to
the proposed fault-tolerant many-core design. We evaluate the pro-
posed design methodology using several deep learning models on
the fault-tolerant implementation of two baseline neuromorphic
designs. We show that the proposed design methodology is both
area and power-efficient, yet providing similar degrees of fault
tolerance compared to existing approaches.

ACKNOWLEDGMENTS
This work is supported by the National Science Foundation Faculty
Early Career Development Award CCF-1942697 (CAREER: Facili-
tating Dependable Neuromorphic Computing: Vision, Architecture,
and Impact on Programmability).

REFERENCES
[1] A. Balaji et al., â€œPyCARL: A PyNN interface for hardware-software co-simulation

of spiking neural network,â€ in IJCNN, 2020.

[2] A. Balaji et al., â€œMapping spiking neural networks to neuromorphic hardware,â€

TVLSI, 2020.

[3] Y. Cao et al., â€œSpiking deep convolutional neural networks for energy-efficient

object recognition,â€ IJCV, 2015.

[4] P. K. Huynh et al., â€œImplementing spiking neural networks on neuromorphic

architectures: A review,â€ arXiv, 2022.

[5] S. Moradi et al., â€œA scalable multicore architecture with heterogeneous mem-
ory structures for dynamic neuromorphic asynchronous processors (DYNAPs),â€
TBCAS, 2017.

[6] S. Nadkarni et al., â€œModeling synaptic transmission of the tripartite synapse,â€

Physical Biology, 2007.

[7] S. Park et al., â€œLow-cost prediction-based fault protection strategy,â€ in CGO, 2020.
[8] V. Parpura et al., â€œGlutamate-mediated astrocyteâ€“neuron signalling,â€ Nature, 1994.
[9] F. Ponzina et al., â€œE2CNNs: Ensembles of convolutional neural networks to
improve robustness against memory errors in edge-computing devices,â€ TC, 2021.
[10] B. Reagen et al., â€œAres: A framework for quantifying the resilience of deep neural

networks,â€ in DAC, 2018.

[11] R. Santos et al., â€œCriticality-aware scrubbing mechanism for SRAM-based FPGAs,â€

in FPL, 2014.

[12] A. Siddique et al., â€œExploring fault-energy trade-offs in approximate DNN hard-

ware accelerators,â€ in ISQED, 2021.

[13] S. Song et al., â€œA case for lifetime reliability-aware neuromorphic computing,â€ in

MWSCAS, 2020.

LeNetAlexNetVGGNetResNetDenseNetMobileNetXceptionAVERAGE01Norm.Acc.Errorrate=10%20%50%LeNetAlexNetVGGNetResNetDenseNetMobileNetXceptionAVERAGE01Norm.AreaErrorrate=10%20%50%LeNetAlexNetVGGNetResNetDenseNetMobileNetXception0255075100Norm.Power1.00.80.40.51.20.90.50.6ReplicationRedundantProposed[14] S. Song et al., â€œImproving dependability of neuromorphic computing with non-

[18] M. L. Varshika et al., â€œDesign of many-core big little ğœ‡Brains for energy-efficient

volatile memory,â€ in EDCC, 2020.

embedded neuromorphic computing,â€ in DATE, 2022.

[15] S. Song et al., â€œDFSynthesizer: Dataflow-based synthesis of spiking neural net-

[19] S. Venkataraman et al., â€œA bit-interleaved embedded hamming scheme to correct

works to neuromorphic hardware,â€ TECS, 2021.

single-bit and multi-bit upsets for SRAM-based FPGAs,â€ in FPL, 2014.

[16] M. Stimberg et al., â€œModeling neuronâ€“glia interactions with the Brian 2 simulator,â€

[20] G. Yuan et al., â€œImproving DNN fault tolerance using weight pruning and differ-

in Computational Glioscience, 2019.

ential crossbar mapping for ReRAM-based edge AI,â€ in ISQED, 2021.

[17] T. Titirsha et al., â€œOn the role of system software in energy management of

neuromorphic computing,â€ in CF, 2021.

