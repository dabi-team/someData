2
2
0
2

g
u
A
1
3

]
E
S
.
s
c
[

1
v
9
9
7
4
1
.
8
0
2
2
:
v
i
X
r
a

Predicting Flaky Tests Categories using Few-Shot
Learning

Amal Akli
Ecole Nationale Sup´erieure d’Informatique
Algeria
ha akli@esi.dz

Guillaume Haben
University of Luxembourg
Luxembourg
guillaume.haben@uni.lu

Sarra Habchi
Ubisoft
Canada
sarra.habchi@ubisoft.com

Mike Papadakis
University of Luxembourg
Luxembourg
michail.papadakis@uni.lu

Yves Le Traon
University of Luxembourg
Luxembourg
yves.letraon@uni.lu

Abstract—Flaky tests are tests that yield different outcomes
when run on the same version of a program. This non-
deterministic behaviour plagues continuous integration with false
signals, wasting developers’ time and reducing their trust in
test suites. Studies highlighted the importance of keeping tests
ﬂakiness-free. Recently, the research community has been push-
ing forward the detection of ﬂaky tests by suggesting many static
and dynamic approaches. While promising, those approaches
mainly focus on classifying tests as ﬂaky or not and, even
when high performances are reported, it remains challenging
to understand the cause of ﬂakiness. This part is crucial for
researchers and developers that aim to ﬁx it. To help with the
comprehension of a given ﬂaky test, we propose FlakyCat, the
ﬁrst approach for classifying ﬂaky tests based on their root cause
category. FlakyCat relies on CodeBERT for code representa-
tion and leverages a Siamese network-based Few-Shot learning
method to train a multi-class classiﬁer with few data. We train
and evaluate FlakyCat on a set of 343 ﬂaky tests collected from
open-source Java projects. Our evaluation shows that FlakyCat
categorises ﬂaky tests accurately, with a weighted F1 score
of 70%. Furthermore, we investigate the performance of our
approach for each category, revealing that Async waits, Unordered
collections and Time-related ﬂaky tests are accurately classiﬁed,
while Concurrency-related ﬂaky tests are more challenging to
predict. Finally, to facilitate the comprehension of FlakyCat’s
predictions, we present a new technique for CodeBERT-based
model interpretability that highlights code statements inﬂuencing
the categorization.

Index Terms—Software Testing, Flaky Tests, CodeBERT, Few-

Shot learning, Siamese Networks.

I. INTRODUCTION

Continuous Integration (CI) plays a key role in nowadays’
software development life cycle [1], [2]. CI ensures the quick
application of changes to a main code base by automatically
running a variety of tasks. Those changes can be responsible
for building the program and its dependencies, performing
checks (e.g., static analysis), and running test suites to main-
tain code integrity and correctness. An important assumption
for practitioners is that tasks are deterministic, i.e., regardless
of the context of the execution of a same task, results need to
be persistent.

Unfortunately, in practice, this is not always the case. Pre-
vious research has identiﬁed test ﬂakiness as one of the main
issues in the application of automated software testing [3],
[4]. A ﬂaky test is a test that passes and fails when executed
on the same version of a program. Flakiness hinders CI
cycles and prevents automatic builds due to false signals,
resulting in undesirable delays. Furthermore, surveys [5]–[7]
show that ﬂakiness affects developers’ productivity as they
spend a considerable time and effort investigating the nature
and causes of ﬂaky tests.

To alleviate this issue, researchers have proposed tools
that help detect ﬂaky tests. In particular, IDFlakies [8] and
Shaker [9] detect ﬂakiness in test suites by running tests in
different setups. However, rerunning tests, especially for a
large number of times, is resource-intensive and might not
be a scalable solution. For this reason, researchers recently
suggested alternative approaches to detect ﬂaky tests based
on features that do not require any test execution [10]–
[12]. Although promising, these approaches mainly focus on
classifying tests as ﬂaky or not without any additional explana-
tion. Unfortunately, the absence of any additional information
about the cause of ﬂakiness is insufﬁcient to comprehend and
investigate the cause of the (ﬂaky) failures. This means that
additional investigation is required to understand the nature of
ﬂakiness and identify the culprit code elements that need to
be ﬁxed [6].

Another important

line of research in the area regards
automated approaches that aim at helping to locate the root
causes and suggest potential ﬂakiness ﬁxes [13]–[15]. How-
ever, research on automatically ﬁxing ﬂakiness is still at an
early stage: tools often focus on one category of ﬂakiness
and with few examples. For instance, iFixFlakies [16] and
ODRepair [17] focus only on dealing with test order dependen-
cies, which is one of the main causes of test ﬂakiness. Flex [18]
automatically ﬁxes ﬂakiness due to algorithmic randomness in
machine learning algorithms.

We believe that both developers and researchers could
beneﬁt from additional information that could assist them in

 
 
 
 
 
 
gaining a better understanding of ﬂaky tests, once they have
been detected. Therefore, we propose FlakyCat, a learning-
identiﬁes the
based ﬂakiness categorization approach that
key reason/category of the test failures. FlakyCat relies on
CodeBERT [19] to build appropriate code representations and
allow static test ﬂakiness predictions, i.e., predictions based
solely on test code.

Another limitation of previous work, relying on supervised
learning, regards the need for large volumes of available data.
Unfortunately, debugged ﬂaky test data is scarce, inhibiting the
application of learning-based methods. To deal with this issue
we leverage the Few-Shot learning capabilities of Siamese net-
works, which we combine with the CodeBERT representations
to learn ﬂakiness categories from a limited set of data (ﬂaky
tests).

To evaluate FlakyCat, we collect a set of 343 ﬂaky tests
from open-source Java projects and categorize them based on
ﬂakiness ﬁxing commit messages, that are applied on both
tests and projects’ source code. We then use this annotated
ﬂaky test set to train and evaluate FlakyCat and compare it
with some relevant baselines.

In particular, our empirical evaluation aims at answering the

following three research questions:

• RQ1: How effective is FlakyCat compared to approaches

based on traditional supervised learning?
Findings: Our results show that FlakyCat is capable of
predicting ﬂakiness categories with an F1 score of 70%,
outperforming classiﬁers based on traditional supervised
machine learning that was used in previous work.

• RQ2: How effective is FlakyCat in predicting each one

of the considered ﬂakiness categories?
Findings: FlakyCat classiﬁes accurately ﬂaky tests re-
lated to Async waits, Unordered collection, and Time,
with an average F1 score of 70%. However, the approach
shows difﬁculty in classifying Concurrency-related ﬂaky
tests, since these cases are related to the interaction of
threads and processes and are easily confused with Async
waits.

• RQ3: How do statements of the test code inﬂuence the

predictions of FlakyCat?
Findings: We found that statements that lead the model to
predict the categories of Time and Unordered collections
tend to enclose information related to the corresponding
ﬂakiness cause. On another hand, it is more challenging to
identify a correlation between the statements inﬂuencing
the categorisation of the Async waits and Concurrency
categories.

In summary, our contributions can be summarised as fol-

lows:

• Dataset We collected 343 ﬂaky tests alongside their

category of test ﬂakiness.

• Model We present FlakyCat, a new approach based on
Few-Shot Learning and CodeBERT to classify ﬂaky tests
with regard to their ﬂakiness category.

• Interpretability We introduce a technique to explain
what information is learnt by models using CodeBERT
as code representation.

To enable reproducibility and replicability of our work, we
make the dataset used to evaluate FlakyCat and the scripts
publicly available in our replication package 1.

The paper is organized as followed: Related works are
presented in section II. Section III presents how we designed
and implemented FlakyCat. Section IV introduces our inter-
pretability technique. Section V describes how we collected
our dataset and evaluated our study. Section VI presents the
results of our study. We further discuss different use cases in
Section VII. Finally, section VIII gives details about the threats
to the validity of this study.

II. RELATED WORK

Recently, practitioners from the industry reported struggling
with ﬂakiness and highlighted the need to ﬁnd solutions to
the problem [3], [20]–[23]. Consequently, researchers from
academia started to draw their attention to the matter. Luo et
al. presented the ﬁrst empirical study to understand and
categorize the root causes of ﬂakiness, they analyzed 201
ﬂaky tests and identiﬁed 10 root causes of ﬂakiness,
the
top ones being Asynchronous waits, concurrency, and test
order dependency. Using the same taxonomy deﬁned by Luo
[24], Eck et al. [25] classiﬁed 200 ﬂaky tests and identiﬁed
four new causes of ﬂakiness. Over the years, several surveys
were carried on to identify the sources, impacts and existing
strategies to mitigate ﬂakiness by interrogating developers
and practitioners [5]–[7], [26]. Parry et al. presented the
state of the art of academic research in another survey [27].
Researchers presented different tools and approaches to detect
ﬂaky tests in a more efﬁcient way. Notably, DeFlaker [28],
IDFlakies [8], Shaker [9] and NonDex [29] attempt to facilitate
the detection of ﬂaky tests compared to exhaustive reruns.
Because the cost of running tests is viewed as expensive,
researchers also sought to suggest static alternatives for the
detection. Different approaches relying on machine learning
were introduced. Pinto et al. [30] and following replication
studies [31], [32] presented a vocabulary-based model using
elements from the test code to classify tests as ﬂaky or
not. Others investigated the use of test smells [12] and code
metrics [33] for predicting ﬂaky tests. Trying to outperform
the performances of existing approaches, others relied on a
mix of static and dynamic features, like FlakeFlagger [34]
or Flake16 [35]. Fixing ﬂakiness is also an aspect that has
recently been investigated. Shi et al. introduced iFixFlakies
[16] to ﬁx order-dependent ﬂaky tests. At Google, Ziftci et
al. suggested using coverage differences, between passing
and failing executions of ﬂaky tests to guide developers to
understand the underlying problem. Coverage information is
also used by FlakyLoc [15], which leverages spectrum-based
fault localization to locate the root cause of ﬂakiness in web
apps. Logs are also frequently considered to be a useful source

1https://anonymous.4open.science/r/FlakyCat-5033/

of information in understanding root causes of ﬂakiness [14].
Closer to our work, Flakify [36] used CodeBERT [19] as a
pre-trained language-based model for their predictor. As their
goal is to classify tests as ﬂaky or not, we are the ﬁrst to
focus on predicting the category of ﬂakiness for each test.
Few-Shot Learning is widely used in computer vision [37]. In
software engineering though, fewer studies used this approach
for their task. Notably, studies suggested using this model for
vulnerability detection [38] and code clone detection [39], but
none were carried out for ﬂakiness. About those pre-trained
language models, Wan et al. [40] investigated their ability to
capture the syntax structure of source code, and report that
they are efﬁcient for code processing tasks.

III. FLAKYCAT

In this section, we present the design and implementation
of FlakyCat. Figure 1 presents an overview of the main steps
of FlakyCat, code transformation and classiﬁcation.

a) Inputs: CodeBERT is able to process both source code
and natural language, e.g., comments and documentation. In
our case, we did not exploit the possibility of using comments,
because the input
length is limited. The input passed to
CodeBERT is the concatenation of two segments with a special
separator token, namely:

[CLS], w1, w2, ...wn, [SEP ], c1, c2, ..., cm, [EOS].

Where Wi is a sequence of natural language text, and Ci is a
sequence of code elements. [CLS] is a special token placed in
front of the two segments, whose ﬁnal hidden representation is
considered as the representation of the whole sequence, used
for the classiﬁcation. To make the source code of a test case
match these expected inputs, we start by ﬁltering it from extra
spaces such as line breaks and tabs. Then, we tokenize the
sequence and add the special tokens: CLS at the beginning and
SEP at the end. We convert the sequence of tokens into IDs as
each token is assigned an ID that corresponds to its index in
the vocabulary ﬁle of CodeBERT. This sequence is passed to
the CodeBERT model, which returns a vector representation.

A. Step 1: Test transformation

Figure 2 illustrates this process.

1) Scope: We rely on the test code to assign ﬂaky tests
to different categories. Previous studies showed that ﬂakiness
ﬁnds its root causes in the test in more than 70% of the
cases [24], [41]. Hence, focusing on the test code allows us
to capture the nature of ﬂakiness while minimizing the overall
cost of FlakyCat. Indeed, considering the code under test
would require running the tests and collecting the coverage,
which entails additional requirements and costs.

2) Test vectorization: In order to perform a source code
classiﬁcation task, we ﬁrst need to transform the code into a
suitable representation that will be fed to the classiﬁcation
model. Previous ﬂakiness studies relied on predeﬁned fea-
tures [10], [34], test smells [12], [34], and code vocabulary
[11], [31], [32] to transform code into vectors. Test smells
convey limited information, i.e., smell presence, and may not
be sufﬁcient to capture the nature of ﬂakiness. In the same
vein, vocabulary-based approaches do not grasp the semantics
of the code [42] and tend to overﬁt to the vocabulary present
in speciﬁc projects [12], [31].

Recently, code embeddings from pre-trained language mod-
els were also considered for source code representation [36],
[43]. Pre-trained language models allow the encoding of code
semantics and are intended for general-purpose tasks such
as code completion, code search, and code summarization.
Considering these beneﬁts, we use the pre-trained language
model CodeBERT [44] to generate source code embeddings.
CodeBERT can learn the syntax and semantics of the code
and doesn’t require any predeﬁned features [40]. It supports
both programming language and natural language. It has been
developed with a multi-layer transformer architecture [45],
and trained on over six million pieces of code involving six
programming languages (Java, Python, JavaScript, PHP, Ruby,
and Go). In the following, we explain how we used CodeBERT
to vectorize the test code.

b) Outputs: CodeBERT output includes two represen-
tations, the ﬁrst contains a contextual vector representation
of each token in the sequence, and the second is the CLS
representation having a size of 768, which is used to represent
the whole sequence. In the case of FlakyCat, we are interested
in the CLS representation of the test code, which is used as a
vector representation of the test cases.

B. Step 2: Test categorization

Unlike traditional machine learning classiﬁers that attempt
to learn how to match an input x to a probability y by training
the model in a large training dataset and then generalizing to
unseen examples, Few-Shot learning (FSL) classiﬁers learn
what makes the elements similar or belonging to the same
class from only a few data. Facing the scarcity of data about
ﬂaky tests in general, selecting a Few-Shot classiﬁer seems
then to be a promising choice. A ﬂaky test can be part of the
category Async wait just because it uses only one statement
of explicit wait, FSL can focus on this statement and learn
that it’s more important than the others for the classiﬁcation
of this category.

In FSL, we call the item we want to classify a query, and the
support set is a small set of data containing few examples for
each class used to help the model to make classiﬁcations based
on similarity. To classify ﬂaky tests according to their ﬂakiness
category, we compute the similarity between the query and all
examples of each ﬂakiness category in our Support Set and
assign the label having the maximum similarity with the query.
This classiﬁcation is obviously performed in a space where all
elements of the same class are similar or close to each other.
This is achieved by a model called Siamese network. Its task is
to transform the data and project it into a space where all the
elements of a same class are close to each other, and then we
can simply classify the elements by computing their similarity.

Fig. 1. An overview of FlakyCat

Fig. 2. The process of converting the code source of a test case to a vector using CodeBERT, going through tokenization, then converting to IDs and applying
the CodeBERT model to get the CLS vector

Since CodeBERT has no knowledge of the characteristics of
ﬂaky tests and only generates a general representation of the
source code, the vectors produced are all similar. However, the
Siamese network learns which characteristics in these vectors
are shared by tests of the same class, and thus allows to project
vectors into a space that groups tests of the same ﬂakiness
category. After this step, it becomes possible to classify them
with a simple similarity computation.

a) Siamese networks training: Siamese networks have
two identical sub-networks, each sub-network processes the
input vector and performs transformations. Both sub-networks
are trained by calculating the similarity between the two
inputs and using the similarity difference as a loss function.
Accordingly, the weights are adjusted to have a high similarity
if the inputs belong to the same class. For the architecture
of the sub-networks, we used a dense layer of 512 neurons
and a normalization layer as shown in Figure 4. We also
performed a linear transformation to keep relations learnt
by CodeBERT using attention mechanism introduced in the
transformer architecture [46]. This model is trained using a
Triplet Loss function, based on the calculation of similarity
difference.

Fig. 3. Visualization of data before and after training of the Siamese network
with the triplet loss, which brings together the elements of the same class

1) Vector transformation with Siamese networks: The
Siamese network has knowledge of the similarity of elements
of the same class. It processes two vectors in input and
applies transformations that allow minimizing the distance
between the two vectors if they share similar characteristics.
Figure 3 shows an example of the visualization of ﬂaky
test vectors before and after the Siamese network is applied.

As FlakyCat uses the CodeBERT representation of tests as
input, using the previously mentioned techniques would not
give understandable features. Thus, we decide to introduce a
new technique to interpret CodeBERT-based models. We aim
at understanding what information is learned by FlakyCat.

Our technique is inspired by delta debugging algorithms.
Delta debugging can help to isolate failing unit tests [50]. In
our case, we are interested in code statements linked with
the most inﬂuential information for the model’s decision. To
achieve this, we classify test cases and select only the ones that
were correctly predicted (TP) as they contain information that
was useful in the model’s decision. We create new versions of
each test. Each version is a copy of the original test minus one
statement that was removed. Next, we feed the new versions
to FlakyCat. Among all new versions for one test, we keep the
one for which the similarity score with the correct category
endured the biggest drop. We consider the statement removed
in this version as the most inﬂuential one. When ﬂakiness
categorisation results are presented to developers, the most
inﬂuential statement can be highlighted to explain the rationale
behind the assigned category. We can also envision scenarios
where multiple inﬂuential statements are highlighted gradually
based on their contribution to the classiﬁcation result.

V. EVALUATION

In this section, we explain our evaluation setting for Flaky-
Cat. First, we describe our data curation process, then, we
present our approach for answering each of the three research
questions.

A. Data curation

1) Collection: For our study, we had to collect a set of
ﬂaky tests containing their source code and their ﬂakiness
category. We focused our collection efforts on one program-
ming language, as training a classiﬁer using code and tokens
from different programming languages is more challenging.
For the language choice, we opted for Java, which is the most
common language in existing ﬂakiness datasets. Nevertheless,
as existing sets do not contain enough data about ﬂakiness
categories, we also built a new set of ﬂaky tests from GitHub
that we classify manually.

a) Existing datasets: There is no large public dataset of
ﬂaky tests labelled according to their category of ﬂakiness.
Most of the existing data are split into ﬂaky and non-ﬂaky
tests and are used for binary classiﬁcation such as Flake-
Flagger [34] and DeFlaker [51]. There is also the Illinois’
dataset2, which is partially classiﬁed into order-dependent
and implementation-dependent ﬂaky tests. Regarding the data
classiﬁed by ﬂakiness categories deﬁned by Luo et al. [24] and
Eck et al. [25], there is only limited data available used for
analysis in previous empirical studies about ﬂakiness. Luo et
al. [24] analyzed 200 commits and classiﬁed 135 commits
into 10 ﬂakiness categories. Using the same taxonomy, Eck et
al. [25] classiﬁed 200 tests into 10 categories, including four

2https://mir.cs.illinois.edu/ﬂakytests/

Fig. 4. The architecture of the Siamese network used for our model and
trained using the triplet Loss function

Let the Anchor A be the reference input (it can be any
input), the positive example P is an input that has the same
class as the Anchor, the negative example N is an input
that has a different class than the Anchor, s() is the cosine
similarity function, and m is a ﬁxed margin. The idea behind
the Triplet Loss function is that we maximize the similarity
between A and P , and minimize the similarity between A
and N , so ideally s(A, P ) is large and s(A, N ) is small. The
formula for this loss function is:

Loss = max(s(A, N ) − s(A, P ) + m, 0)

m is an additional margin as we do not want s(A, P ) to be
very close to s(A, N ), which would lead to a zero loss.

To train the Siamese network with the triplet loss, we give
as input batches of pairs with the same classes, and any other
pair of a different class can be used as a negative example.
We select the closest negative example to the anchor, such
as s(A, N ) (cid:39) s(A, P ), which generates the largest loss and
constitutes a challenge for the model learning.

IV. INTERPRETABILITY TECHNIQUE

Model interpretability refers to one’s ability to interpret the
decisions, recommendations, or in our case the predictions,
of a model. Interpretability is a crucial step to increase trust
in using a machine learning model. Indeed, it allows model
creators to investigate potential biases in the learning processes
and better assess the overall performance of their models.
On top of that, providing users, who are the developers in
this case, with information about how the model came to its
prediction can enhance the model adoption [47].

Flakiness prediction approaches often rely on Information
Gain to explain what features in the model yield the most
information [12], [30], [34]. In the case of tree models, the
reported information gain is given by the Gini importance (also
known as Mean Decrease in Impurity) [48]. Parry et al. [35]
used SHapley Additive explanations (SHAP), which is another
popular technique for model interpretability [49].

new categories of ﬂakiness. We retrieved 135 classiﬁed tests
from the dataset of Luo et al., however, we were not able to
access the set of Eck et al.. We also identiﬁed a new dataset
of classiﬁed ﬂaky tests3, from which we recovered 114 tests.
b) New dataset: To expand our dataset, we explore
GitHub projects and search for ﬂakiness-ﬁxing commits that
mention a ﬂakiness category in their messages. In this search,
we use ﬂakiness-related keywords such as Flaky and Intermit
in the commit messages. To ensure that the commit refers
to a ﬂakiness category, we further ﬁlter commits by speciﬁc
keywords related to each category: thread, concurrence, dead-
lock, race condition for Concurrency, time, hour, seconds, date
format, local date for Time, port, server, network, http, socket
for Network and rand for Random. After the search, we rely
on the developers’ explanation in the commit message and on
the provided ﬁx to classify tests into the different ﬂakiness
categories listed in the literature. This collection allowed us
to obtain 214 categorized tests. To ensure the correctness of
our manual classiﬁcation, the ﬁrst two authors of the paper
performed a double-check on the whole dataset to identify.

2) Filtering: The previous step allowed us to collect 214
categorized ﬂaky tests. In this step, we ﬁlter out tests that
are not adequate for our study. In particular, some data points
in the existing datasets were missing attributes necessary for
data extraction, such as the name of the test method or the
download link. As we were unable to obtain the missing ﬁelds,
we ﬁltered out these points.

The ﬁltering reduced the number of test cases from 135 to
79 for the dataset of Luo et al., and from 114 to 50 in the
new identiﬁed dataset. For the data we collected ourselves, we
accounted for the ﬁlters from the beginning, so we retain the
number of 214 successfully collected and extracted test cases,
leaving us with a set of 343 categorized ﬂaky tests.

3) Processing: After ﬁlling all the necessary attributes: the
test case name, ﬂakiness category, test ﬁle name, and project
URL, we download the code ﬁles and extract test methods
using the spoon library4. At this stage, all comments have
been deleted from the source code to restrict CodeBERT to
code statements.

4) Data selection: The ﬁnal dataset contains 343 ﬂaky tests
distributed over 10 ﬂakiness categories. Table I illustrates this
repartition.

The collected ﬂaky tests are not distributed evenly across
categories of ﬂakiness. Just as shown in past empirical stud-
ies [24], [52], some categories, such as Async waits, are more
prevalent than others. Our approach uses Few-Shot learning to
learn from limited datasets. Still, it requires a certain amount
of examples to learn common patterns from each category.
We decided to have at least 30 tests in a category to consider
it. This number is commonly accepted by statisticians as a
threshold to have representativeness [53]. Also, we decided
to discard the category of Test order dependency as it is a
particular case where the ﬂakiness is speciﬁc to the order of

3https://github.com/Test-Flaky/TSE22
4https://github.com/INRIA/spoon

the test suite. This leaves us with four categories, highlighted
in grey in the table: Async waits, Unordered collections,
Concurrency, and Time.

5) Data augmentation: Facing the challenge of learning
from few data, we over-sampled our dataset in a similar way to
SMOTE [54]. We duplicated tests by mutating only the code
elements that have no inﬂuence on ﬂakiness. This includes
variable names and constants such as strings, numbers and
booleans. We used the Spoon library for the detection of
these elements, and we replaced them with randomly generated
signiﬁcant words. As a result, the total number of tests after
data augmentation is 639.

TABLE I
FINAL DATASET. THE HIGHLIGHTED ROWS ARE THE DATA USED TO TRAIN
AND TEST THE MODEL. THE ORIGINAL DATA REFERS TO THE DATA WE
COLLECTED, AND THE AUGMENTED DATA IS THE ADDITIONAL DATA WE
CREATED AFTER OVERSAMPLING

Class

Data

Async waits
Unordered collections
Concurrency
Time
Test order dependency
Network
Randomness
Test case timeout
Resource leak
Platform dependency
Too restrictive range
I/O

Original
89
45
36
35
33
16
13
9
5
2
2
2

Augmented
285
136
113
105
99
51
40
29
17
7
7
6

B. Experimental design

1) Baseline:

The automatic classiﬁcation of ﬂaky tests according to their
category is a task that has not already been performed
automatically. Previous studies analyzing the categories of
ﬂakiness relied on a manual classiﬁcation of tests. Hence,
as a baseline, we compare FlakyCat with existing approaches
used to classify tests as ﬂaky or not, including the vocabulary-
based approach [11], and the smell-based approach [12]. Our
motivation is to determine whether it is possible to make
this classiﬁcation based on limited data, and which classiﬁer
and code representation are the most suitable for ﬂakiness
classiﬁcation.

More speciﬁcally, we compare our FSL-based approach
with traditional classiﬁers from the Scikit-learn library [55]
used by previous studies on ﬂakiness prediction [11], [12],
[32]: Random Forest (RF), Support Vector Machine (SVM),
Decision Tree (DT), K-Nearest Neighbour (KNN). We also
compare the code representation used in our classiﬁcation
approach with the different source code representations used
in the context of ﬂaky tests, in particular test smells and
vocabulary.

For the classiﬁcation based on test smells, we use the 21
smells detected by tsDetect [56], to generate vectors indicating
in the
the presence of each smell detected by the tool,

same way as in the study of Camara et al. [12]. As for
the vocabulary-based classiﬁcation, we use token occurrence
vectors, as in the article by Pinto et al. [11]. We tokenize the
code and apply standard pre-processing like stemming, then
calculate occurrences of each token.

We use a 4-fold stratiﬁed cross-validation to assess the
predictive performances of our model. FlakyCat relies on a
Siamese network. It is trained with combinations of data by
indicating whether these data are similar or not so that the
model can learn what makes them similar. Since we train with
combined data, the balancing of data is not required, because
it is automatically over-sampled.

As the augmented samples in our dataset are very similar to
the original ones, it was important to keep them in the same
sets for training, so that no augmented samples are leaked
in the test set. For the support set used for classiﬁcation, we
select the most centred examples to represent each class.

2) Parameters:

For FlakyCat, we select the parameters by testing combina-
tions of the most important parameters that have a direct
impact on the model performance, which include the similarity
margin used in the triplet loss function and the number of data
pairs used to train the Siamese model. As shown in Figure 5,
we choose a margin of 0.3 and we ﬁx the number of data
pairs at 10,000. We select the standard value lr = 0.01 for the
learning rate. The change of this value had no impact on the
results. For baseline classiﬁers, we keep the standard values
used by previous works. We varied the number of trees in the
Random Forest classiﬁer, we tested values from 100 to 1000
with a step of 100. We observed that this does not make much
difference regarding the F1 score (¡1%), and we identiﬁed 200
as the number giving the best results.

3) Evaluation metrics:

We use the standard evaluation metrics to compare between
classiﬁers, including precision, recall, Matthews correlation
coefﬁcient (MCC), F1-score, and Area under the ROC curve
(AUC). These metrics have been used to evaluate the perfor-
mance of classiﬁers, including binary classiﬁcation of ﬂaky
tests [11], [12], [36].

Fig. 5. F1-score for different values of similarity margin and combinations

C. RQ1: How effective is FlakyCat compared to approaches
based on traditional supervised learning?

This question aims to evaluate FlakyCat and compare it to
relevant baselines. In the following, we explain the choice of
these baselines and our selection of parameters and evaluation
metrics.

D. RQ2: How effective is FlakyCat in predicting each one of
the considered ﬂakiness categories?

This question aims to evaluate FlakyCat’s ability to classify
the different categories of ﬂakiness. To perform this, we split
the dataset into four sets following the categories: Async waits,
Unordered collections, Concurrency, and Time. Then, we use
the same settings as for RQ1 to tune the Siamese network,
train it, and evaluate it for each category.

E. RQ3: How do statements of the test code inﬂuence the
predictions of FlakyCat?

We applied the technique we proposed in section IV
for CodeBERT-based model interpretability to FlakyCat. We
collected the most inﬂuential statement for each ﬂaky test
correctly classiﬁed. We then proceed to investigate those state-
ments. First, we regroup statements by category of ﬂakiness
(according to the test they belong to). Then, we want to give
details on what type of statements FlakyCat found useful. To
do so, we look through the list of statements and attempt
to identify recurring code statements and categorise them.
The process of identifying statement types is subjective and
inspired by qualitative research. The idea is to ﬁnd statement
types of interest for our speciﬁc case of ﬂakiness classiﬁcation.
We identiﬁed 8 types of statements that are related to some
aspects of ﬂakiness in each category:

• Control ﬂow: Groups statements of loops and conditions.
• Asserts: All types of assertions in tests.
• Threads: All thread control statements.
• Constants: Constant values of numbers, strings, and

booleans.

• Waits: All explicit wait statement.
• Time-related: Statements that perform operations on

time values, dates.

• External/API calls: Groups network / API calls, and
manipulation of external resources such as ﬁles and
databases.

• New instances: Statements creating new instances or

objects.

In this question, we investigate the prevalence of these state-
ment types in each ﬂakiness category.

VI. RESULTS

A. RQ1: How effective is FlakyCat compared to approaches
based on traditional supervised learning?

Following the outlined experimental design, we trained and
tested FlakyCat and the four traditional classiﬁers, using the
three source code representations, the vectors obtained from
CodeBERT, the vectors based on vocabulary, and the ones on

tests smells. The obtained results are presented in Table II.
The results show that FlakyCat achieves the best performance
for all evaluation metrics. It obtained an average weighted F1
score of 70% and a precision of 71%. We get an MCC of 0.58
(bounds for this metric are between -1 and 1), being close to
1 means a perfect classiﬁcation. We have an AUC of 0.78,
which shows that the model is highly distinguishing between
classes.

a) Representation effect: Regarding the three code repre-
sentations, CodeBERT achieves the best performance among
all classiﬁers, with an F1 score between 0.45 and 0.7 for the
ﬁve classiﬁers. By using the vocabulary vectors, all classiﬁers
do not exceed an F1 score of 0.6. The representation based
on test smells yields the worst result, with the best F1 score
being 0.26. This means that CodeBERT is able to learn more
information about the code and the ﬂaky test behaviours.

b) Classiﬁer effect: Regarding the choice of classiﬁer,
we ﬁnd that
the Few-Shot classiﬁer based on similarity
achieves the best performance using the representations
based on CodeBERT and vocabulary. Among traditional
classiﬁers, Random Forest obtains the best results, as reported
in previous ﬂaky test classiﬁcation studies [30], [31]. While
for the smells-based representation, none of the classiﬁers
showed promising results. Using this code representation, the
SVM classiﬁer turned out to achieve the best F1 score: 0.26.
We conclude that the information present in the test smells
was not helpful to learn how to predict ﬂaky tests category.

Overall, our results show that it is possible to automat-
ically classify ﬂaky test categories with limited dataset
size. CodeBERT is the best approach to represent ﬂaky
test source code and Few-Shot learning performs better
than traditional machine learning classiﬁers.

B. RQ2: How effective is FlakyCat in predicting each one of
the considered ﬂakiness categories?

Table III shows performances achieved by FlakyCat for
each of the four ﬂakiness categories that we have selected.
Figure 6 shows that the category Unordered collections is the
easiest for the model to classify, with a precision of 0.84 and
an F1 score of 0.85 (average across folds). The category Async
waits and Time respectively have a precision of 0.73 and 0.65.
Concurrency performances are lower with a precision of 0.48.
We suspect that concurrency issues happen in many cases
in the code under test. As FlakyCat only relies on the test
source code, this would indeed explain why performances are
lower in this case. Another supposition is that concurrency
issues and asynchronous waits are sometimes closely related.
For example, a thread incorrectly waiting for another one to
ﬁnish might be considered both a concurrency issue and an
asynchronous wait.

Fig. 6. F-score per ﬂakiness category using FlakyCat

While the three ﬂakiness categories Unordered collec-
tions, Async waits and Time show good ability to be
detected automatically, Concurrency remains difﬁcult
to detect by relying only on the test case code.

C. RQ3: How do statements of the test code inﬂuence the
predictions of FlakyCat?

Table IV reports the prevalence (%) of the different types
of statements among all inﬂuential statements per ﬂakiness
category, e.g., 100% Asserts in the time category would mean
that all inﬂuential statements for the time category contain
assert statements.

The results in Table IV show that

the Control ﬂow,
Constants, and New instances statements are almost evenly
distributed. We conclude that those statement types are not
correlated to the speciﬁcity of ﬂakiness categories under study.
Compared to other ﬂakiness categories,
the percentage of
assertions in the inﬂuential statements of Time and Unordered
collections is high, 55% and 50% respectively. Based on our
analysis, this includes in particular assertions that perform
exact comparisons, such as assertEquals(), between
constant values and collection items, or dates for example.
43% of inﬂuential statements in the Concurrency category
include some thread manipulation, and 17% for the Async
Waits category, while the rest of the categories have none.
Statements containing explicit waits represent respectively
17% and 25% for Async Waits and Concurrency categories,
but zero for the others. Statements containing time values are
most common in the Time category with 85%. We note that
they appear as well in a small proportion, 12.8% and 12.5%
respectively, for Async Waits and Concurrency. Statements
from the external/API calls group are mainly found in the
Async waits and Concurrency categories, this includes network
calls and manipulation of external resources.

The results show that FlakyCat

is able to differentiate
between the features that are important
to each ﬂakiness
category by considering the correlation between the types of
statements and ﬂakiness categories. This also suggests that

TABLE II
COMPARING PERFORMANCES OF FLAKYCAT (CODEBERT AND FEW-SHOT LEARNING) WITH TRADITIONAL MACHINE LEARNING CLASSIFIERS

Model

SVM
KNN
DT
RF
FSL

Precision
0.19
0.10
0.17
0.15
0.11

Smells-based
MCC
0.00
0.0 1
-0.07
-0.08
-0.01

Recall
0.44
0.2
0.36
0.31
0.31

F1
0.26
0.10
0.22
0.20
0.16

AUC
0.50
0.51
0.46
0.45
0.50

Precision
0.48
0.44
0.43
0.62
0.63

Vocabulary-based
Recall
0.47
0.43
0.47
0.6
0.63

MCC
0.18
0.12
0.24
0.42
0.46

F1
0.35
0.37
0.45
0.53
0.60

AUC
0.54
0.55
0.62
0.66
0.71

Precision
0.19
0.50
0.51
0.63
0.71

CodeBERT-based
Recall
0.43
0.50
0.49
0.63
0.70

MCC
0.00
0.23
0.28
0.46
0.58

F1
0.26
0.45
0.49
0.59
0.70

AUC
0.50
0.59
0.64
0.70
0.78

TABLE III
PERFORMANCES BY CATEGORIES USING FEW-SHOT CLASSIFIER

Category
Async waits
Concurrency
Time
Unordered collections

Precision
0.73
0.48
0.65
0.84

Recall Weighted F1-score
0.75
0.48
0.68
0.80

0.74
0.46
0.66
0.85

number of examples in these categories (less than 30), which
does not allow the classiﬁer to learn the similarities between
tests in these categories, and even if it learns, the features are
still not mineralizable.

CodeBERT is able to grasp some semantics from the test code.

Our analysis of the most inﬂuential statements shows
the statements inﬂuencing the predictions of
that
FlakyCat are correlated to ﬂakiness categories. By
highlighting these statements, our interpretability tech-
nique can help developers understand ﬂaky tests and
their categories.

VII. DISCUSSION

A. The effect of adding additional categories

Our results showed that ﬂakiness categories can be classiﬁed
automatically. We carried out our main experiments with four
categories of ﬂakiness for which we had a reasonable number
of tests. Still, we believe that one interesting aspect of our
study is understanding the impact of adding other categories to
FlakyCat. For this, we investigate the performance of FlakyCat
in each category (similarly to RQ2), but we consider two
more categories, Network and Random. These are the next
two categories with the most samples in our dataset with 16
and 13 ﬂaky tests, respectively. The F1 scores and the accuracy
obtained for each category are presented in ﬁgure 7.

Compared to the results previously reported in Table III,
we observe that the performances of each category are slightly
impacted. The Async waits category is the most impacted one.
Indeed, after adding two categories, we get an overall F1 score
of 0.57, where the added categories get the worst results. This
performance drop is caused by multiple factors. The ﬁrst one
is the increase in the number of classes for the classiﬁer from
4 to 6. The discrimination between four classes is much easier
than six. Indeed, the top four categories become more difﬁcult
to distinguish, which means that the added categories have
common characteristics with them. Secondly, the overall F1
score is affected by the poor performances observed in the
new two categories. These performances can be a result of the

Fig. 7. F-score and precision per category when considering six ﬂakiness
categories

To further understand these results, we inspect a test that
was misclassiﬁed by FlakyCat. The test is presented in the
listing 85. Based on the dataset of Luo et al. [24], the original
ﬂakiness label of this test is Network, because the test fails
intermittently due to a lack of online dependency. However,
FlakyCat assigns it ﬁrst to the category Concurrency, then the
second label is Async waits, and the third one is Network.
Analyzing the whole test, we can notice that it uses multiple
threads, unordered collections, and asynchronous wait for a
ﬁxed time, so the test may be ﬂaky for other reasons. Hence,
statically multiple categories of ﬂakiness can be assigned to
it. In this case, the model can’t make much of a distinction,
because it works with similarities, and the test is similar to
more than one category.

B. Correlation between the statements inﬂuencing FlakyCat
and ﬂakiness causes

The results of RQ3 highlight the type of code statements
on which our model relies to make predictions. Looking at
these results, we can notice that, according to the model,
Async waits and Concurrency categories have similar types of
statements behind them. These statements are related to the use
of threads, wait statements with ﬁxed time values, constants
and external calls. This means that the two categories have

5https://github.com/apache/hbase/commit/e89712d29dd91be4

TABLE IV
PREVALENCE OF THE DIFFERENT TYPES OF STATEMENTS IN EACH FLAKINESS CATEGORY

Async Waits
Concurrency
Time
Unordered collections

NB statements
47
16
20
36

Control ﬂow
2.1%
0,0%
10,0%
8,3%

Asserts
17.0 %
6,3%
55,0%
50,0%

Threads
17.0 %
43,8%
0%
0%

Constants
66.0 %
56,3%
60,0%
66,7%

Waits
17.0 %
25,0%
0%
0%

Time Related
12.8%
12,5%
85,0%
0%

External/API calls
23.4%
31,3%
5,0%
8,3%

New instance
8.5%
12,5%
15,0%
13,9%

Similarly,

complex and can as well be subjective. To ensure the quality
of the data, the ﬁrst two authors reviewed the collected ﬂaky
tests and conﬁrmed their belonging to the assigned category.
types in RQ3
the identiﬁcation of statement
required a manual analysis of the most inﬂuential statements.
Hence, the identiﬁed types can be subjective and the assign-
ment of statements is prone to human errors. To mitigate this
risk, we kept the statement types factual, e.g., control ﬂow
and asserts. This allows us to avoid assignment ambiguities
and intersections between the different statement types.

b) External validity: The ﬁrst threat to external validity
is the generalizability of our approach. In this study, we
train a model to recognize ﬂaky tests from four of the most
prevalent categories, but we are not sure of the performances in
other categories. We discussed the addition of two categories
(Network and Randomness), and retrieved that the number of
examples is one of the inﬂuencing factors.

c) Construct validity: One potential threat to construct
validity regards the metrics used for the evaluation study.
To alleviate this threat, we report MCC, F1-score, and AUC
metrics in addition to the commonly-used precision and recall.
As our data is not evenly distributed across the different
categories, we report the weighted F1 score.

IX. CONCLUSION

Test ﬂakiness is considered as a major issue in software
testing as it disrupts CI pipelines and breaks trust in regression
testing. Detecting ﬂaky tests is resourceful as it can require
many reruns to reproduce failures. To facilitate the detection,
more and more studies suggest static and dynamic approaches
to predict if a test is ﬂaky or not. However, detecting ﬂaky tests
constitutes only a part of the challenge since it remains difﬁcult
for developers to understand the root causes of ﬂakiness. Such
understanding is vital for addressing the problem, i.e., ﬁxing
the cause of ﬂakiness. At the same time, researchers would
gain more insights based on this information. So far, only a
few automated ﬁxing approaches were suggested and these are
focusing on one category of ﬂakiness. Knowing the category
of ﬂakiness for a given ﬂaky test is thus a key information.

With our work, we propose a new approach to this problem
that aims at classifying previously identiﬁed ﬂaky tests in
their corresponding category. We present FlakyCat, a Siamese
network-based multi-class classiﬁer that relies on CodeBERT’s
code representation. FlakyCat addresses the problem of data
scarcity in the ﬁeld of ﬂakiness by leveraging the Few-Shot
learning capabilities of Siamese networks to allow the learning
of ﬂakiness categories from small sets of ﬂaky tests. As part

Fig. 8. A ﬂaky test from the Category Network taken from the project HBase.

common characteristics. Moreover, external calls, including
network calls which constitute a separate ﬂakiness category,
are also frequent in the Async waits and Concurrency cate-
gories. In addition, time-related statements, which are mainly
present in the Time ﬂakiness category, occur also in the Async
waits and Concurrency categories. This implies that several
deﬁned ﬂakiness categories can intersect (or be included in
one another). The results may also draw our attention to the
fact that even some elements of the code that may seem
important when debugging ﬂakiness problems may be part
of the problem. As table IV shows, constants and asserts are
among the important elements for the classiﬁcation of the
categories Time and Unordered collection. This is because
ﬂaky tests that are caused by unordered collection try to assert
that elements of an unordered collection are exactly equal to
constant values with a ﬁxed order. The same issue occurs in
the Time category as tests compare time values with different
precision. This means that developers and tools should be
careful with exact equality assertions to avoid these categories
of ﬂakiness.

Future research studies may further investigate the precise
causes of ﬂakiness and make clear distinctions between cate-
gories. The application of machine learning to determine the
causes of ﬂakiness is promising and should receive attention
while considering that different categories of ﬂakiness may be
included in a single test.

VIII. THREATS TO VALIDITY

a) Internal validity: One threat to the internal validity is
related to the dataset we used in our study. Flaky tests were
gathered from different sources, as explained in section V-A.
It is possible that ﬂaky tests were assigned to the wrong
label, which would impact the training and evaluation of our
model. Certifying the category based on the test source code is

of our evaluation of FlakyCat, we collect and make available a
dataset of 343 ﬂaky tests with information about their category
of ﬂakiness.

Our empirical evaluation shows that FlakyCat performs the
best compared to other code representations and traditional
classiﬁcation models used by previous ﬂakiness prediction
studies. In particular, we reach a weighted F1 score of 70%.
We also analysed the performances with respect
to each
category of ﬂakiness. We found that ﬂaky tests belonging to
Async waits, Unordered collections and Time are the easiest
to classify, whereas ﬂaky tests from the Concurrency cate-
gory are more challenging to predict. Finally, we present a
new technique to explain CodeBERT-based machine learning
models which is inspired by delta-debugging. This technique
helps in explaining what code elements are learnt by models
and could give useful information to developers who wish to
understand ﬂakiness’s root causes.

REFERENCES

[1] M. Shahin, M. A. Babar, and L. Zhu, “Continuous integration, delivery
and deployment: a systematic review on approaches, tools, challenges
and practices,” IEEE Access, vol. 5, pp. 3909–3943, 2017.

[2] M. Rehkopf, “What is continuous integration — atlassian,” https://www.
atlassian.com/continuous-delivery/continuous-integration, (Accessed on
01/12/2021).

[3] J. Micco, “The State of Continuous Integration Testing Google,” 2017.
[4] A. Memon, Z. Gao, B. Nguyen, S. Dhanda, E. Nickell, R. Siem-
borski, and J. Micco, “Taming google-scale continuous testing,” in 2017
IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track (ICSE-SEIP).
IEEE, 2017,
pp. 233–242.

[5] S. Habchi, G. Haben, M. Papadakis, M. Cordy, and Y. L. Traon, “A
qualitative study on the sources, impacts, and mitigation strategies of
ﬂaky tests,” International Conference on Software Testing (ICST), 2022.
[6] M. Eck, M. Castelluccio, F. Palomba, and A. Bacchelli, “Understanding

Flaky Tests: The Developer’s Perspective,” arXiv, pp. 830–840, 2019.

[7] M. Gruber and G. Fraser, “A survey on how test ﬂakiness affects
developers and what support they need to address it,” in Proceedings of
the 15th IEEE International Conference on Software Testing, Veriﬁcation
and Validation, ser. ICST ’22, 2022.

[8] W. Lam, R. Oei, A. Shi, D. Marinov, and T. Xie, “IDFlakies: A
framework for detecting and partially classifying ﬂaky tests,” Proceed-
ings - 2019 IEEE 12th International Conference on Software Testing,
Veriﬁcation and Validation, ICST 2019, pp. 312–322, 2019.

[9] D. Silva, L. Teixeira, and M. D’Amorim, “Shake It! Detecting Flaky
Tests Caused by Concurrency with Shaker,” Proceedings - 2020 IEEE
International Conference on Software Maintenance and Evolution, IC-
SME 2020, pp. 301–311, 2020.

[10] T. M. King, D. Santiago, J. Phillips, and P. J. Clarke, “Towards a
Bayesian Network Model for Predicting Flaky Automated Tests,” 2018
IEEE International Conference on Software Quality, Reliability and
Security Companion (QRS-C), pp. 100–107, 2018.

[11] G. Pinto, B. Miranda, S. Dissanayake, M. d’Amorim, C. Treude, and
A. Bertolino, “What is the vocabulary of ﬂaky tests?” in Proceedings
of the 17th International Conference on Mining Software Repositories,
2020, pp. 492–502.

[12] B. Camara, M. Silva, A. Endo, and S. Vergilio, “On the use of test smells
for prediction of ﬂaky tests,” in Brazilian Symposium on Systematic and
Automated Software Testing, 2021, pp. 46–54.

[13] C. Ziftci and D. Cavalcanti, “De-ﬂake your tests : Automatically locating
root causes of ﬂaky tests in code at google,” in 2020 IEEE International
Conference on Software Maintenance and Evolution (ICSME), 2020, pp.
736–745.

[14] W. Lam, P. Godefroid, S. Nath, A. Santhiar, and S. Thummalapenta,
“Root Causing Flaky Tests in a Large-Scale Industrial Setting,” in
Proceedings ofthe 28th ACM SIGSOFT International Symposium on
Software Testing and Analysis (ISSTA ’19). Beijing, China: ACM Press,
2019, pp. 101–111.

[15] J. Mor´an, C. Augusto, A. Bertolino, C. de la Riva, and J. Tuya,
“Flakyloc: Flakiness
in web
applications,” J. Web Eng., vol. 19, no. 2, pp. 267–296, 2020. [Online].
Available: https://doi.org/10.13052/jwe1540-9589.1927

localization for

reliable test

suites

[16] A. Shi, W. Lam, R. Oei, T. Xie, and D. Marinov, “iFixFlakies : A
Framework for Automatically Fixing Order-Dependent Flaky Tests,”
in 27th ACM Joint European Software Engineering Conference and
Symposium on the Foundations ofSoftware Engineering (ESEC/FSE
’19), 2019.

[17] C. Li, C. Zhu, W. Wang, and A. Shi, “Repairing order-dependent ﬂaky
the 44th International

tests via test generation,” in Proceedings of
Conference on Software Engineering - ICSE ’22.

ICSE, 2022.

[18] S. Dutta, A. Shi, R. Choudhary, Z. Zhang, A. Jain, and S. Mis-
ailovic, “Detecting ﬂaky tests in probabilistic and machine learning
applications,” ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis, pp. 211–
224, 2020.

[19] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming
and natural languages,” arXiv preprint arXiv:2002.08155, 2020.
[20] H. Jiang, X. Li, Z. Yang, and J. Xuan, “What causes my test alarm?
automatic cause analysis for test alarms in system and integration
the 39th International Conference on
testing,” in Proceedings of
Software Engineering, ser. ICSE ’17.
IEEE Press, 2017, p. 712–723.
[Online]. Available: https://doi.org/10.1109/ICSE.2017.71

[21] M. contributors, “Test veriﬁcation - mozilla — mdn,” https://
developer.mozilla.org/en-US/docs/Mozilla/QA/Test Veriﬁcation, March
2019, (Accessed on 01/12/2021).

[22] M. Harman and P. O’Hearn, “From Start-ups to Scale-ups: Opportunities
and Open Problems for Static and Dynamic Program Analysis,” in
2018 IEEE 18th International Working Conference on Source Code
Analysis and Manipulation (SCAM).
IEEE, sep 2018, pp. 1–23.
[Online]. Available: https://ieeexplore.ieee.org/document/8530713/

[23] J.

Palmer,
and

“Test
dealing

ing
neering,”
test-ﬂakiness-methods-for-identifying-and-dealing-with-ﬂaky-tests/,
November 2019, (Accessed on 01/12/2021).

identify-
engi-
https://engineering.atspotify.com/2019/11/18/

ﬂakiness
with

methods
:

for
Spotify

ﬂaky

tests

–

[24] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An empirical analysis
of ﬂaky tests,” in Proceedings of the ACM SIGSOFT Symposium on the
Foundations of Software Engineering, vol. 16-21-November-2014, nov
2014, pp. 643–653.

[25] M. Eck, F. Palomba, M. Castelluccio, and A. Bacchelli, “Understanding
ﬂaky tests: The developer’s perspective,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering,
ser. ESEC/FSE 2019. New York, NY, USA: Association for
Computing Machinery, 2019, p. 830–840. [Online]. Available: https:
//doi-org.sndl1.arn.dz/10.1145/3338906.3338945

[26] O. Parry, G. M. Kapfhammer, M. Hilton, and P. McMinn, “Surveying the
developer experience of ﬂaky tests,” in Proceedings of the International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP), 2022.

[27] O. Parry, “A Survey of Flaky Tests,” ACM transactions on software

engineering and methodology, vol. 31, no. 1, 2021.

[28] J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov,
“DeFlaker: Automatically Detecting Flaky Tests,” in Proceedings of the
40th International Conference on Software Engineering - ICSE ’18.
New York, New York, USA: ACM Press, 2018, pp. 433–444. [Online].
Available: http://dl.acm.org/citation.cfm?doid=3180155.3180164
[29] A. Gyori, B. Lambeth, A. Shi, O. Legunsen, and D. Marinov, “Nondex:
A tool for detecting and debugging wrong assumptions on java api speci-
ﬁcations,” in Proceedings of the 2016 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering, 2016, pp. 993–997.
[30] G. Pinto, B. Miranda, S. Dissanayake, M. D’Amorim, C. Treude, and
A. Bertolino, “What is the Vocabulary of Flaky Tests?” Proceedings
- 2020 IEEE/ACM 17th International Conference on Mining Software
Repositories, MSR 2020, pp. 492–502, 2020.

[31] G. Haben, S. Habchi, M. Papadakis, M. Cordy, and Y. Le Traon, “A
Replication Study on the Usability of Code Vocabulary in Predicting
Flaky Tests,” Proceedings of the International Conference on Mining
Software Repositories (MSR), 2021.
[32] B. Camara, M. Silva, A. T. Endo,

“What
the vocabulary of ﬂaky tests? an extended replication,” in

and S. Vergilio,

is

Conference on Software Testing, Veriﬁcation and Validation, ICST 2021,
pp. 148–158, 2021.

[53] R. V. Krejcie and D. W. Morgan, “Determining sample size for research
activities,” Educational and psychological measurement, vol. 30, no. 3,
pp. 607–610, 1970.

[54] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
synthetic minority over-sampling technique,” Journal of artiﬁcial intel-
ligence research, vol. 16, pp. 321–357, 2002.

[55] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” the Journal of machine
Learning research, vol. 12, pp. 2825–2830, 2011.

[56] A. Peruma, K. Almalki, C. D. Newman, M. W. Mkaouer, A. Ouni,
and F. Palomba, “Tsdetect: An open source test smells detection
tool,” in Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ser. ESEC/FSE 2020. New York, NY, USA:
Association for Computing Machinery, 2020, p. 1650–1654. [Online].
Available: https://doi.org/10.1145/3368089.3417921

(ICPC).

2021 2021 IEEE/ACM 29th International Conference on Program
Comprehension (ICPC)
IEEE
Computer Society, may 2021, pp. 444–454.
[Online]. Available:
https://doi.ieeecomputersociety.org/10.1109/ICPC52881.2021.00052
[33] V. Pontillo, F. Palomba, and F. Ferrucci, “Toward static test ﬂakiness
prediction: A feasibility study,” in Proceedings of the 5th International
Workshop on Machine Learning Techniques for Software Quality Evo-
lution, 2021, pp. 19–24.

Los Alamitos, CA, USA:

[34] A. Alshammari, C. Morris, M. Hilton, and J. Bell, “Flakeﬂagger:
Predicting ﬂakiness without rerunning tests,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE), 2021, pp.
1572–1584.

[35] O. Parry, G. M. Kapfhammer, M. Hilton, and P. McMinn, “Evalu-
ating features for machine learning detection of order-and non-order-
dependent ﬂaky tests,” in 2022 IEEE Conference on Software Testing,
Veriﬁcation and Validation (ICST).

IEEE, 2022, pp. 93–104.

[36] S. Fatima, T. A. Ghaleb, and L. Briand, “Flakify: A Black-
for Flaky Tests,” arXiv
[Online]. Available:

Box, Language Model-based Predictor
preprint arXiv:2112.12331, pp. 1–12, 2021.
http://arxiv.org/abs/2112.12331

[37] X. Sun, B. Wang, Z. Wang, H. Li, H. Li, and K. Fu, “Research progress
on few-shot learning for remote sensing image interpretation,” IEEE
Journal of Selected Topics in Applied Earth Observations and Remote
Sensing, vol. 14, pp. 2387–2402, 2021.

[38] M. Khajezade, F. H. Fard, and M. S. Shehata, “Evaluating few shot and
contrastive learning methods for code clone detection,” arXiv preprint
arXiv:2204.07501, 2022.

[39] Y. He, W. Wang, H. Sun, and Y. Zhang, “Vul-mirror: a few-shot
learning method for discovering vulnerable code clone,” EAI Endorsed
Transactions on Security and Safety, vol. 7, no. 23, p. e4, 2020.
[40] Y. Wan, W. Zhao, H. Zhang, Y. Sui, G. Xu, and H. Jin, “What do they
capture?–a structural analysis of pre-trained language models for source
code,” arXiv preprint arXiv:2202.06840, 2022.

[41] W. Lam, K. Mus¸lu, H. Sajnani, and S. Thummalapenta, “A study
on the lifecycle of ﬂaky tests,” in Proceedings of
the ACM/IEEE
42nd International Conference on Software Engineering, ser. ICSE ’20.
New York, NY, USA: Association for Computing Machinery, 2020,
p. 1471–1482.
[Online]. Available: https://doi.org/10.1145/3377811.
3381749

[42] S. Chakraborty, R. Krishna, Y. Ding, and B. Ray, “Deep learning
based vulnerability detection: Are we there yet,” IEEE Transactions on
Software Engineering, 2021.

[43] X. Zhou, D. Han, and D. Lo, “Assessing generalizability of codebert,”
in 2021 IEEE International Conference on Software Maintenance and
Evolution (ICSME).
IEEE, 2021, pp. 425–436.

[44] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,
B. Qin, T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained
model for programming and natural languages,” in Findings of the
Association for Computational Linguistics: EMNLP 2020. Online:
Association for Computational Linguistics, Nov. 2020, pp. 1536–1547.
[Online]. Available: https://aclanthology.org/2020.ﬁndings-emnlp.139

[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems, vol. 30, 2017.
[46] ——, “Attention is all you need,” Advances in neural

information

processing systems, vol. 30, 2017.

[47] D. V. Carvalho, E. M. Pereira, and J. S. Cardoso, “Machine learning
interpretability: A survey on methods and metrics,” Electronics, vol. 8,
no. 8, p. 832, 2019.

[48] “Feature importances with a forest of trees — scikit-learn 1.1.1 docu-
mentation,” https://scikit-learn.org/stable/auto examples/ensemble/plot
forest importances.html, (Accessed on 06/24/2022).

[49] S. Lundberg, “Shap documentation,” https://shap.readthedocs.io/, 2018,

(Accessed on 06/23/2022).

[50] A. Zeller and R. Hildebrandt, “Simplifying and isolating failure-inducing
input,” IEEE Transactions on Software Engineering, vol. 28, no. 2, pp.
183–200, 2002.

[51] J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov,
“Deﬂaker: Automatically detecting ﬂaky tests,” in 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE), 2018, pp.
433–444.

[52] M. Gruber, S. Lukasczyk, F. Krois, and G. Fraser, “An Empirical Study
of Flaky Tests in Python,” Proceedings - 2021 IEEE 14th International

