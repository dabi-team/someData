2
2
0
2

r
p
A
0
1

]

A
N
.
h
t
a
m

[

2
v
2
9
2
2
1
.
3
0
2
2
:
v
i
X
r
a

Eﬃcient distributed matrix-free multigrid methods
on locally reﬁned meshes for FEM computations

Peter Munch∗†

Timo Heister‡

Laura Prieto Saavedra§

Martin Kronbichler¶

April 12, 2022

Abstract

This work studies three multigrid variants for matrix-free ﬁnite-element computations on locally reﬁned
meshes: geometric local smoothing, geometric global coarsening, and polynomial global coarsening. We have
integrated the algorithms into the same framework—the open-source ﬁnite-element library deal.II—, which
allows us to make fair comparisons regarding their implementation complexity, computational eﬃciency, and
parallel scalability as well as to compare the measurements with theoretically derived performance models.
Serial simulations and parallel weak and strong scaling on up to 147,456 CPU cores on 3,072 compute
nodes are presented. The results obtained indicate that global coarsening algorithms show a better parallel
behavior for comparable smoothers due to the better load balance particularly on the expensive ﬁne levels.
In the serial case, the costs of applying hanging-node constraints might be signiﬁcant, leading to advantages
of local smoothing, even though the number of solver iterations needed is slightly higher.

Key words. multigrid, ﬁnite element computations, linear solvers, matrix-free method

1

Introduction

Many solvers for ﬁnite element methods (FEM) rely on eﬃcient solution methods for second-order partial
diﬀerential equations (PDEs), e.g., for the Poisson equation:

−∆u = f,

where u is the solution variable and f is the source term. Poisson-like problems also frequently occur as
subproblems, e.g., in computational ﬂuid dynamics [1–3] or in computational plasma physics [4]. Eﬃcient
realizations often rely on adaptively reﬁned meshes to resolve geometries or features in the solution itself and
on robust iterative solvers for such meshes.

Multigrid methods are among the most competitive solvers for such problems [5]. The three basic steps of
a two-level algorithm are 1) presmoothing, in which the high-frequency error components in the initial guess
are removed with a presmoother, 2) coarse-grid correction, in which the given problem is solved on a coarse
grid, requiring intergrid transfer operators and a coarse-grid solver, and 3) postsmoothing, in which the high-
frequency error components introduced during interpolation are removed with a postsmoother. Nesting two-level
algorithms recursively gives a multigrid algorithm. In library implementations, these steps are generally hidden
behind operators. The latter can be generally chosen and/or conﬁgured by the user and strongly depend on the
multigrid variant selected. This publication discusses massively parallel multigrid variants for locally reﬁned
meshes and the eﬃcient implementation of their operators.

∗Institute of Material Systems Modeling, Helmholtz-Zentrum Hereon, Max-Planck-Str.

1, 21502 Geesthacht, Germany

(peter.muench@hereon.de).

†Institute for Computational Mechanics, Department of Mechanical Engineering, Technical University of Munich, Boltz-

mannstr. 15, 85748 Garching b. M¨unchen, Germany (peter.muench@tum.de).

‡Clemson University, South Carolina, USA (heister@clemson.edu).
§Department of Chemical Engineering, Ecole Polytechnique de Montreal, PO Box 6079, Stn Centre-Ville, H3C 3A7, Montreal,

QC, Canada (laura.prieto-saavedra@polymtl.ca).

¶Department of Information Technology, Uppsala University, Box 337, 75105 Uppsala, Sweden (martin.kronbichler@it.uu.se).

1

 
 
 
 
 
 
1.1 Multigrid variants

Which multigrid approach to choose depends on the way the mesh is generated and on the underlying ﬁnite-
element space. If the mesh is generated by globally reﬁning each cell on a coarse grid recursively, it is a natural
choice to apply geometric multigrid (abbreviated here as h-multigrid), which uses the levels of the resulting
mesh hierarchy as multigrid levels. Alternatively, in the context of high-order ﬁnite elements, it is possible to
create levels by reducing the polynomial order of the shape functions p of the elements, while keeping the mesh
the same, as done by polynomial multigrid (abbreviated as p-multigrid). For a very ﬁne, unstructured mesh
with low-order elements, it is not as trivial to explicitly construct enough multigrid levels and one might need
to fall back to non-nested multilevel algorithms [6, 7] or algebraic multigrid (AMG; see the review by St¨uben
[8]). These basic multigrid strategies can be nested in hybrid multigrid solvers [9–15] and, in doing so, one can
exploit the advantages of all of them regarding robustness. Most common are hp-multigrid, which combines h-
and p-multigrid, and AMG as black-box coarse-grid solver of geometric or polynomial multigrid solvers.

All the above-mentioned multigrid variants are applicable to locally reﬁned meshes. However, local reﬁne-
ment comes with additional options (local vs. global deﬁnition of the multigrid levels) and with additional
challenges, which are connected, e.g., with the presence of hanging-node constraints.

1.2 Related work

Some authors of this study have been involved in various publications in the ﬁeld of multigrid methods in the
past. Their implementations will be used and extended in this work. In [9], an eﬃcient hybrid multigrid solver
for discontinuous Galerkin methods (DG) for globally reﬁned meshes was presented. It relies on auxiliary-space
approximation [16], i.e., the transfer into a continuous space, as well as on sequential execution of p-multigrid,
h-multigrid, and AMG. In [17], that solver was extended to leverage locally reﬁned meshes. In [18–21], matrix-
free implementations of parallel geometric local-smoothing algorithms for CPU and GPU were investigated and
comparisons with AMG were conducted.

1.3 Our contribution

In this publication, we will consider three well-known multigrid algorithms for locally reﬁned meshes for con-
tinuous higher-order matrix-free FEM: geometric local smoothing, geometric global coarsening, and polynomial
global coarsening. We have implemented them into the same framework, which allows us to compare their
implementation complexity and performance for a large variety of problem sizes. This has not been done in
an extensive way in the literature, often using only one of them [18, 22]. Furthermore, we rely on matrix-free
operator evaluations, which are optimal, state-of-the-art implementations in terms of node-level performance
on modern hardware [21], and hence embed the methods in a challenging context in terms of communication
costs where diﬀerences are most pronounced.

The algorithms presented in this publication have been integrated into the open-source ﬁnite-element library
deal.II [23] and are mostly part of its 9.3 release [24]. Their implementation has been used in [17] to simulate
the ﬂow through a lung geometry and is applied in the ExaDG incompressible Navier–Stokes solver [3]. All
results of this publication have been obtained with small benchmark programs leveraging on the infrastructure
of deal.II. The programs are available on GitHub under https://github.com/peterrum/dealii-multigrid.
The results obtained in this publication for continuous FEM are transferable to the DG case, where one
does not have to consider hanging-node constraints but ﬂuxes between diﬀerently reﬁned cells. In the case of
auxiliary-space approximation [17], this diﬀerence only involves the ﬁnest level and the rest of the multigrid
algorithm could be as described in this publication.

The remainder of this work is organized as follows.

In Section 2, we give a short overview of multigrid
variants applicable to locally reﬁned meshes. Section 3 presents implementation details of our solver, and
Section 4 discusses relevant performance models. Sections 5 and 6 demonstrate performance results for geometric
multigrid and polynomial multigrid, and in Section 7 the solver is applied to a challenging Stokes problem.
Finally, Section 8 summarizes our conclusions and points to further research directions.

2

Algorithm 1: Multigrid V-cycle x ← MultigridVCycle(b) including the copy of b to and of x from
the multigrid level(s).

1 [b(0), . . . ,b(L)] ← b;
2 VCycleLevel(L);
3 return x ← [x(0), . . . ,x(L)] ;

/* Algorithm 2 w.

/* copy to multigrid level(s) */
input/output [b(0)/x(0), . . . , b(L)/x(L)] */
/* copy from multigrid level(s) */

Algorithm 2: Actual multigrid V-cycle VCycleLevel(l) called recursively on each level l. It operates
on vectors of vectors [b(0), . . . , b(L)] and [x(0), . . . , x(L)], which are ﬁlled/read in Algorithm 1, and uses
the level operators A(l), smoothers, intergrid operators, and coarse-grid solvers, which are set up on or
between the multigrid levels. In the case of local smoothing, we distinguish between interior DoFs (not
labeled specially) and DoFs on the internal boundaries (x(l)
E ) as well as decompose the level operator
ES, and A(l)
A(l) into A(l)
EE (see the explanation in Subsection 2.1). For global coarsening,
A(l) = A(l)
SS.
1 if l = 0 then

SE, A(l)

SS, A(l)

x(0) ← CoarseGridSolver(A(0)

SS , b(0)) ;

2
3 else

4

5

6

7

8

9

10

11

SS x(l), −A(l)
(cid:16)

ES x(l)(cid:17)
(cid:17)
;

r(l), r(l)
E

x(l) ← Smoother(A(l)
(cid:17)
(cid:16)
r(l), r(l)
E

(cid:16)
b(l) − A(l)

←

SS , 0, b(l)) ;

b(l−1) ← b(l−1) + Restrictor
VCycleLevel(l − 1) ;
(cid:17)
(cid:16)
x(l), x(l)
E
if local smoothing then
b(l) ← b(l) − A(l)
x(l) ← Smoother(A(l)

SE x(l)
E ;
SS , x(l), b(l)) ;

← x(l) + Prolongator (cid:0)x(l−1)(cid:1) ;

;

/* coarse-grid solver: A(0)
SS

!
= A(0) */

/* presmoothing */

/* compute residual */

/* restrict residual */

/* recursion */

/* prolongation */

/* edge */

/* postsmoothing */

2 Multigrid methods for locally reﬁned meshes

Algorithms 1 and 2 present the basic multigrid algorithm to solve an equation system of the form Ax = b (A is
the system matrix, b is the right-hand side vector containing the source term f and the boundary conditions, and
x is the solution vector). It is general enough for meshes obtained by global as well as by local reﬁnement. In the
ﬁrst step, data is transferred to the multigrid levels, after which a multigrid cycle (in this study, a V-cycle with
the steps: presmoothing, computation of the residual, restriction, solution on the coarser grid, prolongation, and
postsmoothing) is performed. Then, the result is copied back from the multigrid levels. The steps to copy data
from and to the multigrid levels are not strictly needed in all cases, however, are required by local smoothing
and can be used to switch from double to single precision to reduce the costs of multigrid if it is used as a
preconditioner [25]. The multigrid algorithm is complemented with the algorithms of a pre-/postsmoother (e.g.,
Chebyshev smoother [26]) and of a coarse-grid solver. We use the term “coarse-grid solver” also in the case
that it is applied to a grid that could be coarsened further. This term is rather an indication that the recursion
is terminated. Instead of using multigrid as a solver, we choose to precondition a conjugate-gradient solver [27]
with one multigrid cycle per iteration as this is often more robust. This algorithm is not presented here.

The various multigrid algorithms for locally reﬁned meshes diﬀer in the construction of the levels and the
concrete details in the implementations of the multigrid steps. We will consider two types of geometric multigrid
methods: geometric local smoothing in Subsection 2.1 and geometric global coarsening in Subsection 2.2.
Figure 1 gives a visual comparison of them and points out the issues resulting from the local or global deﬁnition
of the levels, which will be discussed extensively in the following. Furthermore, we will detail polynomial global
coarsening in Subsection 2.3.

AMG can be used for the solution on locally reﬁned meshes as well. Since the levels are constructed
recursively via the Galerkin operator A(c) := R(c,f )A(f )P (f,c) with the restriction matrix R(c,f ) and the
prolongation matrix P (f,c) constructed algebraically, no distinction regarding the local or global deﬁnition of
the levels is possible. Since we will use AMG in the following only as a coarse-grid solver, we refer to the
literature for more details: Clevenger et al. [18] present a scaling comparison between AMG and a matrix-free
version of local smoothing for a Laplace problem with Q2, showing the advantages of matrix-free multigrid

3

mg level 2

mg level 1

mg level 0

local smoothing:

reﬁnement edge

“active” level

64 cells

64 cells

16 cells

(cid:80) =144 cells

hanging node

global coarsening:

112 cells

28 cells

16 cells

(cid:80)=156 cells

Figure 1: Visual comparison of geometric multigrid methods for locally reﬁned meshes. Top: (geometric) local
smoothing; bottom: (geometric) global coarsening. Local smoothing only considers cells strictly on the same
reﬁnement level. This typically introduces an internal boundary (at the reﬁnement edge) when the cells do not
cover the whole computational domain. Only if they do (here, for level 1 and 0, not for level 2), one can switch
to a coarse-grid solver. Instead, global coarsening considers the whole domain and typically introduces hanging
nodes on the multigrid levels. Global coarsening tends to have more cells in total compared to local-smoothing
algorithms, but often reduces the number of cells per multigrid level quicker on the ﬁner levels. The gray
shading indicates active cells.

methods on modern computing systems.

2.1 Geometric local smoothing

Geometric local-smoothing algorithms [18, 28–36] use the reﬁnement hierarchy also for multigrid levels and
perform smoothing reﬁnement level by reﬁnement level: cells of less reﬁned parts of the mesh on the ﬁnest level
are skipped (see Figure 1) so that hanging-node constraints do not need to be considered during smoothing.
Authors in [30, 37–39] also have investigated a version of local smoothing in which smoothing is also performed
on a halo of a single coarse cell so that hanging-node constraints need to be applied here as well. We will not
consider this form of local smoothing in the following.

The fact that domains on each level might not cover the whole computational domain results in multiple
issues. Data needs to be transferred in Algorithm 1 to and from all multigrid levels that are active, i.e., have
cells that are not reﬁned. Furthermore, internal interfaces (also known as reﬁnement edges, abbreviated as
edges) might result; they need special treatment. For details, interested readers are referred to [19, 36]. In the
following, we will only summarize key aspects relevant for our investigations.

For the purpose of explanation, let us split the degrees of freedom (DoFs) associated with the cells on an
E so that the associated

S and the ones at the reﬁnement edges x(l)

arbitrary level l into the interior ones x(l)
matrix system A(l)x(l) = b(l) has the following block structure:

(cid:32)

A(l)
A(l)

SS A(l)
ES A(l)

SE

EE

(cid:33) (cid:32)

x(l)
S
x(l)
E

(cid:33)

(cid:32)

=

(cid:33)

b(l)
S
b(l)
E

For presmoothing on level l, only the contributions from A(l)
SS are considered. This is equivalent to applying
homogeneous Dirichlet boundary conditions at the reﬁnement edges, i.e., A(l)
EE = I.
However, when switching to a ﬁner or coarser level, the coupling matrices need to be considered. The residual

ES = 0, and A(l)

SE = 0, A(l)

4

to be restricted becomes:

(cid:32)

r(l)
S
r(l)
E

(cid:33)

(cid:32)

=

b(l)
S
b(l)
E

(cid:33)

(cid:32)

−

A(l)
A(l)

SS A(l)
ES A(l)

SE

EE

(cid:33) (cid:32)

x(l)
S
x(l)
E

(cid:33)

x(l)
E =0
=

(cid:18) 0
b(l)
E

(cid:19)

+

(cid:18) b(l)
S
0

(cid:32)

(cid:19)

−

(cid:33)

A(l)
SS
A(l)
ES

x(l)
S

.

(1)

(cid:124)

(cid:123)(cid:122)
∗

(cid:125)

Since b(l)
E has been transferred to the coarser level by Algorithm 1 already, one only has to restrict the result of
term ∗ to the coarser level. During postsmoothing, a modiﬁed right-hand side b(l)
E needs to
be considered, which is equivalent to applying an inhomogeneous Dirichlet boundary condition with boundary
values prescribed by the coarser level.

S := b(l)

S − A(l)

SEx(l)

A natural choice to partition the multigrid levels for local-smoothing algorithms is to partition the active
level and assign cells on the lower reﬁnement levels recursively to the process of their children. A simple
variant of it is the “ﬁrst-child policy” [18]:
it recursively assigns the parent cell the rank of its ﬁrst child
cell. Since generally in adaptive FEM codes the parents of locally owned and ghost cells are already available
on processes due to tree-like data-structure storage of adaptively reﬁned meshes [40, 41], no additional data
structures need to be constructed and saved, but the storage of an additional ﬂag (multigrid rank of the cell) is
enough, leading to low memory consumption. Furthermore, intergrid transfer operations are potentially cheap
as data is mainly transferred locally. A disadvantage—besides of having to consider the edge constraints—is
the potential load imbalance on the levels, as discussed in [18]. This load imbalance could be alleviated by
partitioning each level for itself. Such alternative partitioning algorithm would lead to similar problems as in
the case of global-coarsening algorithms (discussed next) and, as a result, the needed data structures would
become more complex. This would object to the claimed simplicity of the data structures of this method; hence,
we will consider geometric-local smoothing only with “ﬁrst-child policy” in this publication.

Furthermore, the fact that the transfer to and from the multigrid levels involves all active levels prevents an
early switch to a coarse-grid solver, i.e., one can only switch to this solver on levels that are indeed not locally
!= A(l). On the other hand, the lack of hanging nodes allows the usage of such types
reﬁned anymore, i.e., A(l)
SS
of smoothers that have been developed for uniformly reﬁned meshes, e.g., patch smoothers [42, 43].

Since geometric local smoothing is the only local-smoothing approach we will consider here, we will call it

simply—as common in the literature—local smoothing in the following.

2.2 Geometric global coarsening

Geometric global-coarsening algorithms [37, 44] coarsen all cells simultaneously, translating to meshes with
hanging nodes also on coarser levels of the multigrid hierarchy (see Figure 1). The computational complexity—
i.e., the total number of cells to be processed—is slightly higher than in the case of local smoothing and might
be non-optimal for some extreme examples of meshes [29, 34].

The fact that all levels cover the whole computational domain has the advantages that no internal interfaces
have to be considered and the transfer to/from the multigrid levels becomes a simple copy operation to/from
the ﬁnest level (b(L) ← b, x ← x(L)). However, hanging nodes have to be considered during the application
of the smoothers on the levels. This is normally not a problem, since codes supporting adaptive meshes will
already have the right infrastructure for this (at least for the active level). However, the operator evaluation and
the applicable smoothers might be more expensive per cell than in the case of uniformly reﬁned meshes, since
the application of hanging-node constraints is not free [45]. On the other hand, global-coarsening approaches
show—for comparable smoothers—a better convergence behavior, which improves with the number of smoothing
iterations [46, 47].

As the work on the levels generally increases compared to local smoothing, it is a valid option to repartition
each level separately. On the one hand, this implies higher pressure on the transfer operators, since they need
to transfer data between independent meshes1, requiring potentially complex internal data structures, which
describe the connectivities, and involved setup routines2. On the other hand, it opens the possibility to control

1In deal.II, one needs to create a sequence of grids for global coarsening (each with its own hierarchical description). This is
generally acceptable, since repartitioning of each level often leads to non-overlapping trees so that a single data structure containing
all geometric multigrid levels would have little beneﬁt for reducing memory consumption.

2Sundar et al. [22] present a two-step setup routine: the original ﬁne mesh is coarsened and the resulting “surrogate mesh” is

repartitioned. For space-ﬁlling-curve-based partitioning, this approach turns out to be highly eﬃcient.

5

the load balance between processes and the minimal granularity of work per process (by removing processes on
the coarse level in a controlled way, allowing to switch to subcommunicators [22]) and also to apply a coarse-grid
solver on any level. Furthermore, the construction of full multigrid solvers, which visit the ﬁnest level only a
few times, is easier.

2.3 Polynomial global coarsening

Polynomial global-coarsening algorithms [13, 22, 48–76] are based on keeping the mesh size h constant on all
levels, but reducing the polynomial degree p of shape functions, e.g., to p = 1. Hence, the multigrid levels in
this case have the same mesh but diﬀerent polynomial orders. There are various strategies to reduce the order
of the polynomial degree [9, 77]: the most common is the bisection strategy, which repeatedly halves the degree
p(c) = (cid:98)p(f )/2(cid:99). This strategy reduces the number of DoFs in the case of a globally reﬁned mesh similarly to
the geometric multigrid strategies and is a good compromise between the number of iterations and the cost of
a V-cycle [9].

The statements made in Section 2.2 about geometric global coarsening are also valid for polynomial global
coarsening. However, in contrast to geometric global coarsening, the levels here do not need to be partitioned
for themselves in order to obtain a good load balance, since the number of unknowns is reduced uniformly in
each cell. This leads to a transfer operation that mainly works on locally owned DoFs.

In the following, we call geometric global coarsening simply global coarsening and polynomial global coars-

ening polynomial coarsening.

3 Implementation details

In this section, we will detail eﬃcient implementations of the multigrid ingredients listed in Algorithm 2 and
needed for the multigrid variants for locally reﬁned meshes, which have been considered in Section 2. We start
with the handling of constraints. Then, we proceed with matrix-free evaluations of operator A, which is needed
on the active and the multigrid levels, as well as with smoothers and coarse-grid solvers. The discussion of
matrix-free transfer operators concludes this section.

3.1 Handling constraints

Constraints need to be considered—with slight diﬀerences—in the case both of local-smoothing and global-
coarsening algorithms. First, we impose Dirichlet boundary conditions in a strong form and express them as
constraints. Secondly, hanging-node constraints, which force the solution representation of the reﬁned side to be
matching the polynomial representation of the coarse side, need to be considered to maintain H 1 regularity of
the tentative solution [78]. In a general way, these constraints can be expressed as xi = (cid:80)
j cijxj + bi, where xi
is a constrained DoF, xj a constraining DoF, cij the coeﬃcient relating the DoFs, and bi a real value, which can
be used to consider inhomogeneities. We do not eliminate constraints, but use a condensation approach [79, 80].
We will continue to talk about constraints and their eﬃcient application in the context of matrix-free loops in
Subsections 3.2 and 3.5, where we discuss operators that indeed need to apply constraints.

3.2 Matrix-free operator evaluation

Instead of assembling the system matrix A and performing matrix-vector multiplications of the form Ax,
the matrix-free operator evaluation computes the underlying ﬁnite-element integrals to represent A(x). The
structure of a matrix-free operator evaluation in the context of continuous ﬁnite elements generally is:

v = A(x) =

(cid:88)

e

e ◦ CT
GT

e ◦ ˜S T

e ◦ Qe ◦ Se ◦ Ce ◦ Ge ◦ x.

(2)

This structure is depicted in Figure 2. For each cell e, cell-relevant values are gathered with operator Ge,
constraints are—as discussed in Subsection 3.1—applied with Ce, and quantities—like values, gradients, or
Hessians—are computed with Se at the quadrature points. These quantities are processed by a quadrature-
point operation Qe; the result is integrated and summed into the result vector v by applying ˜S T
e , and GT
e .
In this publication, we consider symmetric (self-adjoint) PDE operators with ˜Se = Se.

e , CT

6

n
o
i
t
a
u
l
a
v
e

.
p
o

n
o
i
t
a
g
n
o
l
o
r
p

.
y
l
o
p

evaluation

Ce ◦ Ge

[45]

e ◦ G(c)
C(c)

e

[45]

coarse (c)

Qe

˜ST
e

Se

integration

e ◦ CT
GT
e

P (f,c)
e

Fig. 3

W (f )
e

Fig. 4

S(f )
e

ﬁne (f)

Figure 2: Basic steps of a matrix-free operator evaluation according to (2) and of a matrix-free polynomial
prolongation according to (7) for a single cell e.

In the literature, there are both GPU [19, 25, 81–83] and CPU [4, 81, 84–86] implementations for operations
as expressed in (2), which require their own hardware-speciﬁc optimizations. For tensor-product (quadrilateral
and hexahedral) elements, a technique known as sum factorization [87, 88] is often applied, which allows to
replace full interpolations from the local solution values to the quadrature points by a sequence of 1D ones. In
the context of CPUs, it is an option to vectorize over multiple elements [85, 86], i.e., perform (S T ◦ Q ◦ S)e for
multiple elements at the same time. However, in order to be able to do this, the data already needs to be laid
out in a struct-of-arrays fashion. The reshuﬄing of the data from array-of-structs format to struct-of-arrays
format and back can be done, e.g., by Ge, while looping through all elements [85]. We would like to point out
that we do not perform Ce ◦ Ge ◦ x in two steps, but apply the constraints right away, while we are setting
up the value of a DoF. For the application of hanging-node constraints, we use the special-purpose algorithm
introduced in [45], which is based on the update of the DoF map Ge and applies in-place sum factorization
for the interpolation during the application of edge and face constraints as well. Even though the application
of hanging-node constraints uses state-of-the-art algorithms with small overhead for high-order ﬁnite elements
(< 20%), the additional steps are not free particularly for linear elements and might lead to load imbalances in
a parallel setting if there is a process with a disproportionately high number of cells with hanging nodes. For a
quantitative analysis of this problem, see the discussion in [45].

Even though we are using matrix-free algorithms, some of the multigrid ingredients (e.g., smoother and
coarse-grid solver) need an explicit representation of the linear operator in the form of a matrix or a part of it
(e.g., of its diagonal). In a naive approach relying only on the matrix-free kernels (2), one could compute the
matrix by simply applying a unit base vector ei globally to the operator A(:, i) = A(ei) and hereby reconstructing
the matrix column by column. One can use the same approach also on the cell level:

Ae(:, i) = GT

e ◦ Qe ◦ Se ◦ ei

(3)

and assemble the resulting element matrix, as usual, also applying the constraints. Computing the diagonal is
slightly more complex if one does not want to store the complete element matrix and to apply the constraints
during assembly, as described above.
Instead, we choose to compute the j-th entry of the locally relevant
diagonal contribution via

de(j) =

(cid:88)

i

(cid:2)CT

e Ae(:, i)(cid:3) Ce(i, j) ∀j ∈ {j | Ce,ji (cid:54)= 0},

(4)

i.e., we apply the local constraint matrix Ce from the left to the i-th column of the element matrix (computed
via (3)) and apply Ce again from the right. This approach needs as many basis vector applications as there are

7

shape functions per cell. The local result can be simply added to the global diagonal via d =

(cid:88)

e

GT

e ◦ de. For

cells without constrained DoFs, (4) simpliﬁes to de(i) = Ae(i, i) so that one can use (3) to compute the element
matrix column by column and only store the diagonal entries.

3.3 Smoother

For demonstration purposes, we use Chebyshev iterations around a point-Jacobi method [26], since the con-
struction of eﬃcient and robust smoothers for locally reﬁned meshes is out of the scope of this publication. The
smoother we use only needs an operator evaluation and its diagonal representation, for which we can use the
algorithms described in Subsection 3.2. It is run on the levels either globally or locally, depending on whether
we use a global-coarsening or a local-smoothing multigrid algorithm. Interior boundaries separating the current
level from a coarser one are treated as homogeneous Dirichlet boundaries. As a consequence, the constraint
matrix does not have to consider hanging-node constraints anymore, and the algorithms signiﬁcantly simplify—
with the result that smoother applications have a higher throughput in the local-smoothing case than in the
case of global coarsening.

3.4 Coarse-grid solver

The algorithms described in Subsection 3.2 allow to set up traditional coarse-grid solvers, e.g., a Jacobi solver,
a Chebyshev solver, direct solvers, but also AMG. In this publication, we mostly apply AMG as a coarse-grid
solver, since we use it either on very coarse meshes (in this case, it falls back to a direct solver) or for problems
discretized with linear elements, for which AMG solvers are very competitive.

3.5 Transfer operator

The prolongation operator P (f,c) prolongates the result x from a coarse space to a ﬁne space (this includes
prolongation from a coarse grid to a ﬁne grid and from a coarser polynomial degree to a ﬁner degree):

According to the literature [22, 46], this can be done in three steps:

x(f ) = P (f,c) ◦ x(c)

x(f ) = W (f ) ◦ ˜P (f,c) ◦ C(c) ◦ x(c)

(5)

with C(c) setting the values of constrained DoFs on the coarse mesh, particularly resolving the hanging-node
constraints, ˜P (f,c) performing the prolongation on the discontinuous space as if no hanging nodes were existing,
and the weighting operator W (f ) zeroing out the DoFs constrained on the ﬁne mesh.

In order to derive a matrix-free implementation, one can express (5) for nested meshes as loops over all cells

(see also Figure 2):

x(f ) =

(cid:88)

S (f )
e

◦ W (f )
e

◦ P (f,c)
e

e∈{cells}

◦ C(c)

e ◦ G(c)

e ◦ x(c)

(6)

e

e ◦G(c)

Here, C(c)
e gathers the cell-relevant coarse DoFs and applies the constraints just as in the case of matrix-free
operator evaluations (2). P (f,c)
performs the prolongation onto the ﬁne space for the given (coarse) cell and
S (f )
sums the result back to a global vector. Since multiple elements could add to the same global entry of the
e
vector x(f ) during the cell loop, the values to be added have to be weighted with the inverse of the valence of
the corresponding DoF. This is done by W (f )
, which also ignores constrained DoFs (zero valence) in order to be
consistent with (5). Figure 4 shows, as an example, the values of W (f ) for a simple mesh for a scalar Lagrange
element of degree 1 ≤ p ≤ 3.

e

We construct the element prolongation matrices via:

(cid:16)

P (f,c)
e

(cid:17)

ij

(cid:16)

=

M (f )
e

(cid:17)−1 (cid:16)

φ(f )
i

, φ(c)
j

(cid:17)

Ωe

8

with

(cid:16)

M (f )
e

(cid:17)

ij

(cid:16)

=

φ(f )
i

, φ(f )
j

(cid:17)

Ωe

reﬁnement type

element types

example 1:

example 2:

without reﬁnement

global coarsening

polynomial coarsening

FEc

FEf

with reﬁnement

Q1

Q1

Q1

Q2

local smoothing

Figure 3: Left: Construction of the element prolongation P (f,c)
, based on the reﬁnement-type (with/without
reﬁnement) and element-types pair (coarse and ﬁne FE). Right: Examples for prolongation with and without
reﬁnement for equal-degree and diﬀerent-degree ﬁnite elements (p(c) = p(f ) = 1 vs. p(c) = 1, p(f ) = 2). Relevant
prolongation types for local smoothing, global coarsening, and polynomial coarsening are highlighted. Note that,
in the case of global coarsening, two types of prolongation (categories) are needed.

e

with φ(c) being the shape functions deﬁned on the coarse cell and φ(f ) being the ones deﬁned on the “ﬁne cell”.
Instead of treating each “ﬁne cell” on its own, we group direct children of the coarse cells together and deﬁne
the ﬁne shape functions on the appropriate subregions. As a consequence, the “ﬁnite element” on the ﬁne
mesh depends both on the actual ﬁnite-element type (like on the polynomial degree and continuity) and on the
reﬁnement type, as indicated in Figure 3. In the case of local smoothing, the ﬁnite-element type remains the
same and cells are only reﬁned by deﬁnition, since cells without reﬁnements are ignored during smoothing. In
the case of polynomial global coarsening, the mesh stays the same and only the polynomial degree is uniformly
increased for all cells. In the case of geometric global coarsening, cells are either reﬁned or not, but the element
and its polynomial degree stay the same. This means that—while in the case of local smoothing and polynomial
global coarsening a single P (f,c)
is enough—one needs two variants in the case of geometric global coarsening
(note: for non-reﬁned cells, P (f,c)
is an identity matrix). We deﬁne the set of all coarse-ﬁne-cell pairs connected
via the same element prolongation matrix as category C.

e

e

Since P (f,c)

= P (f,c)

e

C(e) , i.e., all cells of the same category C(e) have the same element prolongation matrix, and
in order to be able to apply them for multiple elements in one go in a vectorization-over-elements fashion [85]
as in the case of matrix-free loops (2), we loop over the cells type by type so that (6) becomes:

x(f ) =

(cid:88)

(cid:88)

S (f )
e

◦ W (f )
e

◦ P (f,c)
c

c

e∈{e|C(e)=c}

◦ C(c)

e ◦ G(c)

e ◦ x(c).

(7)

We choose the restriction operator as the transpose of the prolongation operator

R(c,f ) =

(cid:16)

P (f,c)(cid:17)T

↔ R(c,f )

e

=

(cid:16)

P (f,c)
e

(cid:17)T

,

which implies that the element restriction matrix is the transpose of the cell prolongation matrix as well.

We conclude this subsection with discussing the appropriate data structures for a transfer operator suitable
both for global geometric and for polynomial coarsening. For details on local smoothing, see [18, 19] and the
documentation of MGTransferMatrixFree class in deal.II [89]. Since global-coarsening algorithms smoothen
on the complete computational domain, data structures only need to be able to perform two-level transfers (7)
independently between arbitrary ﬁne (f) and coarse (c) grids. C(c)
is identical to Ce ◦ Ge in the matrix-free
loop (2) so that specialized algorithms and data structures [45] can be applied and reused. S (f )
e needs the indices
of DoFs for the given element e in order to be able to scatter the values, and W (f )
stores the weights of DoFs
C(e) (and R(c,f )
(or of geometric entities - see also the argumentation in Figure 4) for the given element e. P (f,c)
C(e) )

e ◦ G(c)
e

e

9

p = 1

p = 2

p = 3

W (f )
i

:

0

1

1/2

1/4

Figure 4: Example for entries of W (f ) for a mesh with two coarse cells, of which one is reﬁned, and Dirichlet
boundary at the left face for a scalar continuous Lagrange element of degree 1 ≤ p ≤ 3. In our implementation,
constrained DoFs do not contribute to the valence of constraining DoFs, which results in valences of one for
DoFs inside constraining (coarse) edges/faces. In the case that p > 2 and all DoFs of a geometric entity are
constrained in the same way, it is enough to store the valence per geometric entity. For eﬃcient access to the
information during the cell loop, one would store the information for all entities of a “ﬁne cell” (9 integers in
2D and 27 in 3D, just like in the p = 2 case).

need to be available for each category. We regard them as general operators and choose the most eﬃcient way
of evaluation based on the element types: simple dense-matrix representation vs. some substructure with sum
factorization based on smaller 1D prolongation matrices. The category of each cell has to be known for each
cell.

e ◦ G(c)
e

Alongside these process-local data structures, one needs access to all constraining DoFs on the coarse level
required during C(c)
and to the DoFs of all child cells on the ﬁne level during the process-local part of
S (f )
, which is concluded by a data exchange. If external vectors do not allow access to the required DoFs, we
e
copy data to/from internal temporal global vectors with appropriate ghosting [85]. We choose the coarse-side
identiﬁcation of cells due to its implementation simplicity and structured data access at the price of more ghost
transfer.3 For setup of the communication pattern, we use consensus-based sparse dynamic algorithms [90, 91].
For the sake of separation of concerns, one might create three classes to implement a global-coarsening
transfer operator as we have done in deal.II. The relation of these classes is shown in Figure 5: the multigrid
transfer class (MGTransferGlobalCoarsening) delegates the actual transfer tasks to the right two-level imple-
mentation (MGTwoLevelTransfer), which performs communications needed as well as evaluates (7) for each
category and cell by using category-speciﬁc information from the third class (MGTransferSchemes).

4 Performance modeling

In Section 3, we have presented eﬃcient implementations of the operators in Algorithms 1 and 2 for local smooth-
ing, global coarsening, and polynomial coarsening. Most of the discussion was independent of the multigrid
variant chosen, highlighting the similarities from the implementation point of view. The main diﬀerences arise
naturally from the local or global deﬁnition of levels. E.g., one might need to consider—possibly expensive—
hanging-node constraints during matrix-free loops when doing global coarsening or polynomial coarsening. Local
smoothing, on the other hand, has the disadvantage of performing additional steps: 1) global transfer to/from
multigrid levels and 2) special treatment of edges during smoothing, computation of the residual, and modiﬁ-
cation of the right-hand side vector for postsmoothing. In the following sections, we will quantify the inﬂuence
of the costs of the potentially more expensive operator evaluations and of the additional operator evaluations,
related to the choice of the multigrid level deﬁnition.

3Sundar et al. [22] showed that, by assigning all children of a cell to the same process, one can easily derive an algorithm
that allows to perform the cell-local prolongation/restriction on the ﬁne side, potentially reducing the amount of data to be
communicated during the transfer. Since we allow levels to be partitioned arbitrarily in our implementation, we do not use this
approach. Furthermore, one should note that the algorithm proposed there does not allow to apply the constraints C(c)
during a
single cell loop as in (7), but needs a global preprocessing step as in (5), potentially requiring additional sweeps through the whole
data with access to the slow main memory.

e

10

(cid:28)interface(cid:29)
MGTransferBase

prolongate and add(level, dst, src) = 0
restrict and add(level, dst, src) = 0

copy to mg(dst, src) = 0
copy from mg(dst, src) = 0
interpolate to mg(dst, src) = 0

MGTransferMatrixFree

prolongate and add(level, dst, src)
restrict and add(level, dst, src)

MGTransferGlobalCoarsening

copy to mg(dst, src)
copy from mg(dst, src)
interpolate to mg(dst, src)

prolongate and add(level, dst, src)
restrict and add(level, dst, src)

copy to mg(dst, src)
copy from mg(dst, src)
interpolate to mg(dst, src)

MGTwoLevelTransfer

category ptrs
level dof indices coarse (G)
constraints coarse (C)
weights ﬁne (W)
level dof indices ﬁne (S)

L + 1

ghosted vector coarse (G - optional)
ghosted vector ﬁne (S - optional)

prolongate and add(vec ﬁne, vec coarse)
restrict and add(vec coarse, vec ﬁne)
interpolate(vec coarse, vec ﬁne)

MGTransferScheme

prolongation matrix
prolongation matrix 1D

interpolation matrix
interpolation matrix 1D

#C

Figure 5: UML diagram of the global-coarsening transfer operator MGTransferGlobalCoarsening in deal.II.
It implements the base class MGTransferBase, which is also the base class of MGTransferMatrixFree (local
smoothing), and delegates its prolongation/restriction/interpolation tasks to the right MGTwoLevelTransfer in-
stance. Each of these instances is deﬁned between two levels and is responsible for looping over categories/cells
and for evaluating (7) by using the prolongation matrices from the correct MGTransferScheme object. Further-
more, it is responsible for the communication, for which it has two optional internal vectors with appropriate
ghosting. MGTwoLevelTransfer objects can be initialized for geometric or polynomial global coarsening—with
the consequence that MGTransferGlobalCoarsening can handle (global) h-, p-, and hp-multigrid.

Our primary goal is to minimize the time to solution.

It consists of setup costs and the actual solve
time, which is the product of the number of iterations times the time per iteration. We will disregard
the setup costs, since they normally amortize in time-dependent simulations, where one does not remesh every
time step, and ways to optimize the setup of global-coarsening algorithms are known in the literature [22]. The
time of the solution process strongly depends on the choice of the smoother, which inﬂuences the number of
iterations and, as a result, also the time to solution. Since diﬀerent iteration numbers might distort the view on
the performance of the actual multigrid algorithm, we will also consider the value of the time per iteration as
an important indicator of the computational performance of multigrid algorithms particularly to quantify the
additional costs in an iteration.

In order to get a ﬁrst estimate of the beneﬁts of an algorithm compared to another, one can derive following

metrics purely from geometrical information:

• The serial workload can be estimated as the sum of the number of cells on all levels Ws = (cid:80)

l Cl. This
metric is based on the assumption that all cells have the same costs, which is not necessarily true in the
context of hanging nodes [45].

l maxp Cp

• The parallel workload can be estimated as the sum of the maximum number of cells owned by any
process on each level: Wp = (cid:80)
l , i.e., the critical path of the cells. In the ideal case, one would
expect that Cp
l = Cl/p and therefore Wp = Ws/p. However, due to potential load imbalances, the
work might not be well distributed on the levels, i.e., maxp (Cp
l ) ≥ Cs/p. Since one can theoretically only
proceed to process the next level once all processes have ﬁnished a level, load imbalances will result in some
processes waiting at some imaginary barriers. We say “imaginary barriers” as level operators generally
do not have any barriers, but only rely on point-to-point communication between neighboring processes.
Nevertheless, this simpliﬁed point of view is acceptable, since multigrid algorithms are multiplicative
Schwarz methods between levels, inherently leading to a serial execution of the levels. We deﬁne parallel
workload eﬃciency as Ws/(Wp · p), as has been also done in [18].

• We deﬁne horizontal communication eﬃciency as 50% of the number of ghost cells accumulated over
all ranks and divided by the total number of cells. The division by two is necessary to take into account
that only one neighbor is updating the ghost values. As such, this ratio can be seen as a proxy of how much
information needs to be communicated, when computing residuals and updating ghost values. As this
number counts cells, it is independent of the polynomial degree of the element chosen. The element degree
used determines the absolute amount of communication necessary. Note that in reality, only degrees of
freedom located at the interface have to be exchanged such that the fraction of the solution that needs to
be communicated is less than the fraction of those cells.

• Vertical communication eﬃciency is the share of ﬁne cells that have the same owning process as
their corresponding coarse cell (parent). This quantity gives an indication on the eﬃciency of the transfer

11

L = 4

L = 5

octant

#cells

%HN

#dofs

p = 1

p = 4

#cells

%HN

shell

#dofs

p = 1

p = 4

1.2e+2
7.0e+2
4.7e+3
3.5e+4
2.7e+5
2.1e+6
1.7e+7
1.3e+8
1.1e+9
8.6e+9
-

2.2e+2
31%
1.0e+3
37%
5.7e+3
23%
12%
3.8e+4
6.2% 2.8e+5
3.1% 2.2e+6
1.6% 1.7e+7
0.8% 1.4e+8
0.4% 1.1e+9
0.2% 8.9e+9

-

-

9.3e+3
5.1e+4
3.2e+5
2.3e+6
1.8e+7
1.4e+8
1.1e+9
8.6e+9
6.9e+10
-
-

-
-
1.2e+3
6.8e+3
3.7e+4
2.7e+5
2.2e+6
1.7e+7
1.4e+8
1.1e+9
8.9e+9

-
-
-
-
2.0e+3
69%
9.8e+3
78%
4.8e+4
70%
3.2e+5
38%
2.3e+6
19%
10%
1.8e+7
4.8% 1.4e+8
2.4% 1.1e+9
1.2% 8.9e+9

-
-
9.3e+4
5.1e+5
2.6e+6
1.9e+7
1.4e+8
1.1e+9
8.9e+9
7.1e+10
5.7e+11

L

3
4
5
6
7
8
9
10
11
12
13

L = 6

L = 7

Figure 6: Cross section at the center of the geometries of the octant (top) and the shell (bottom) simulation.
Additionally, the number of cells, the share of cells with hanging-node constraints, and the number of DoFs (for
a scalar Lagrange element with p = 1 and p = 4) are given for each reﬁnement case. Note that we only consider
3D geometries in this publication.

operator and on how much data has to be sent around. A small number indicates that most of the data has
to be completely permuted, involving a large volume of communication. This metric has been considered
in [18] as well.

• Increasing number of (multigrid) levels leads to additional synchronization points and communication

steps and, as a result, might lead to increased latency.

Furthermore, memory consumption of the grid class is a metric we will consider.4 A common argu-
ment supporting the usage of local-smoothing algorithms is that no space is needed for potentially diﬀerently
partitioned meshes and complex data structures providing the connectivity between them, since the multigrid
algorithm can simply reuse the already existing mesh hierarchy also for the multigrid levels [18].

Examples

In the experimental sections 5 and 6, we will consider two types of static 3D meshes, as has been also done
in [18]. They are obtained by reﬁning a coarse mesh consisting of a single cell deﬁned by [−1, 1]3 according to
one of the following two solution criteria:

• octant: reﬁne all mesh cells in the ﬁrst octant [−1, 0]3 L times and

• shell: after L − 3 uniform reﬁnements, perform three local reﬁnement steps with all cells whose center c

is |c| ≤ 0.55, 0.3 ≤ |c| ≤ 0.43, and 0.335 ≤ |c| ≤ 0.39.

These two meshes are relevant in practice, since similar meshes occur in simulations of ﬂows with far ﬁelds and
of multi-phase ﬂows with bubbles [2] or any kind of interfaces. All the reﬁnement procedures are completed by
a closure after each step, ensuring one-irregularity in the sense that two leaf cells may only diﬀer by one level if
they share a vertex. Figure 6 shows the considered meshes and provides numbers regarding the cell count for
3 ≤ L ≤ 13. All meshes are partitioned along space-ﬁlling curves [40, 41] with the option to assign cells weights.
Tables 1 and 2 give—as examples—evaluated numbers for geometrical metrics of the two considered meshes
for a single process and for 192 processes with cells constrained by hanging nodes weighted with the factor of 2
for partitioning, compared to the rest of the cells. For a single process, only workload and memory consumption
are shown.

Starting with the octant case, one can see that the serial workload in the case of local smoothing and global
coarsening is similar, with local smoothing having consistently less work. The workload of each level is depicted

4We use the memory-consumption output provided by deal.II. No particular eﬀorts have been put in reducing the memory

consumption of the triangulations in the case of global coarsening.

12

Table 1: Geometrical multigrid statistics for the octant test case for diﬀerent numbers of reﬁnements (wl:
serial/parallel workload, wl-eﬀ: parallel workload eﬃciency, v-eﬀ: vertical communication eﬃciency; h-eﬀ:
horizontal communication eﬃciency; mem: memory consumption in bytes).

L

3
4
5
6
7
8
9

L

5
6
7
8
9
10

1 process

192 processes

local smoothing

global coarsening

local smoothing

global coarsening

wl

mem

wl

mem

wl

wl-eﬀ

v-eﬀ

h-eﬀ

mem

wl

wl-eﬀ

v-eﬀ

h-eﬀ

mem

1.4e+2
8.0e+2
5.4e+3
4.0e+4
3.1e+5
2.4e+6
1.9e+7

6.6e+4
3.3e+5
2.0e+6
1.4e+7
1.1e+8
8.6e+8
6.8e+9

1.4e+2
8.4e+2
5.6e+3
4.0e+4
3.1e+5
2.4e+6
1.9e+7

8.7e+4
4.2e+5
2.5e+6
1.7e+7
1.3e+8
9.9e+8
7.8e+9

1.8e+1
2.2e+1
7.2e+1
4.0e+2
2.7e+3
2.0e+4
1.6e+5

3%
18%
38%
51%
58%
62%
64%

1.3e+6
89% 60%
1.2e+7
85% 56%
5.3e+7
88% 58%
1.3e+8
96% 65%
4.2e+8
99% 75%
99% 84%
1.8e+9
99% 91% 1.0e+10

2.5e+1
3.3e+1
6.7e+1
2.8e+2
1.7e+3
1.3e+4
1.0e+5

3%
13%
43%
76%
92%
96%
98%

2.7e+6
17% 62%
1.5e+7
58%
4%
6.8e+7
59%
1%
2.0e+8
6%
66%
6.2e+8
13% 75%
23% 84%
2.4e+9
38% 91% 1.3e+10

Table 2: Geometrical multigrid statistics for the shell test case for diﬀerent numbers of reﬁnements.

1 process

192 processes

local smoothing

global coarsening

local smoothing

global coarsening

wl

mem

wl

mem

wl

wl-eﬀ

v-eﬀ

h-eﬀ

mem

wl

wl-eﬀ

v-eﬀ

h-eﬀ

mem

1.4e+3
7.8e+3
4.2e+4
3.1e+5
2.5e+6
-

6.0e+5
3.2e+6
1.7e+7
1.2e+8
8.9e+8
-

1.8e+3
9.2e+3
4.9e+4
3.4e+5
2.5e+6
-

·106

9.5e+5
4.4e+6
2.2e+7
1.4e+8
1.1e+9
-

3.1e+1
1.6e+2
7.6e+2
4.7e+3
3.5e+4
2.7e+5

22%
26%
28%
34%
36%
38%

3.2e+7
80% 55%
7.7e+7
89% 59%
1.5e+8
96% 66%
4.4e+8
99% 76%
1.8e+9
99% 85%
99% 91% 1.0e+10

4.5e+1
1.0e+2
4.3e+2
2.3e+3
1.5e+4
1.1e+5

21%
47%
58%
75%
86%
92%

4.7e+7
2%
56%
1.3e+8
12% 59%
3.0e+8
36% 65%
7.7e+8
78% 75%
2.7e+9
93% 84%
97% 91% 1.3e+10

Local smoothing

·106

Global coarsening

d
a
o
l
k
r
o

W

d
a
o
l
k
r
o

W

2.0

1.0

0.0

0

1

2

3

4

5

6

7

8

2.0

1.0

0.0

0

1

2

3

4

5

6

7

8

Level

Level

Figure 7: Workload of an octant simulation with a single process.

·104

Local smoothing

·104

Global coarsening

min max

avg

min max

avg

1.0

0.5

0.0

0

1

2

3

4

5

6

7

8

1.0

0.5

0.0

0

1

2

3

4

5

6

7

8

Level

Level

Figure 8: Workload of an octant simulation with 192 processes.

13

in Figure 7. The behavior of the memory consumption is similar to the one of the workload: global coarsening
has a slightly higher memory consumption, since it explicitly needs to store the coarser meshes as well; however,
the second ﬁnest mesh has already approximately one eighth of the size of the coarsest triangulation so that
the overhead is small. In the parallel case, the memory consumption diﬀerences are higher: this is related to
the fact that overlapping ghosted forests of trees need to be saved. In contrast, the workload in the parallel
case is much lower in the case of global coarsening: while global coarsening is able to reach eﬃciencies higher
than 90%, the eﬃciency is only approximately 50-60% in the case of local smoothing. The high value in the
global-coarsening case was to be expected, since we repartition each level during construction. The low value in
the case of local smoothing can be explained by taking a look at Figure 8, where the minimum, maximum, and
average workload are shown for each level. The workload on the ﬁnest level is optimally distributed between
processes with cells, however, there are a few processes without any cells, i.e., any work, on that level. On the
second level, most processes can reduce the number of cells nearly optimally by a factor of 8, but processes
idle on the ﬁnest level start to participate in the smoothing process: since they have not participated in the
coarsening yet, the number of cells for those processes is much higher and approximately the same as other
processes had on the ﬁnest level. This pattern of discrepancy of minimum and maximum number of cells on the
lower levels continues and, as a consequence, the maximum workload per level is higher, increasing the overall
parallel workload. Figure 8 might also give the impression that there is a load imbalance in the case of global
coarsening, since the minimum workload on the ﬁnest level is the half of the maximum value. This is related to
the way how we partition—penalizing of cells that have hanging nodes with a weight of 2—and to the fact that
a lot of cells with hanging nodes are clustered locally. The actually resulting load imbalance is small, as shown
in Figure 11. The diﬀerence between local smoothing and global coarsening in the parallel workload comes
at a price. While the vertical communication eﬃciency is—by construction—high in the local-smoothing case,
this is not true in the case of global coarsening: 20% and less is not uncommon, requiring the permutation of
data during transfer. The horizontal communication costs are similar in the case of local smoothing and global
coarsening. Assuming that 1) pre- and postsmoothing are the most time-consuming steps, 2) the workload is
the relevant metric, and 3) the transfer between levels is not dominant in this case, the values indicate that
global coarsening might be twice as fast as local smoothing. However, if the transfer is the bottleneck, the
picture might look diﬀerently so that a conclusive statement only based on geometrical metrics is not possible
in this case.

The observations made for the octant case are also valid, but even more pronounced in the shell case.
Here, the vertical communication is more favorable in the case of global coarsening and the workload eﬃciency
is signiﬁcantly worse in the local-smoothing case so that one can expect a noticeable speedup when using global
coarsening.

In summary, one can state the following:

in the serial case, global coarsening has to process at least as
many cells as local smoothing so that one can expect that the latter has—with the assumption that the number
of iterations is the same—an advantage regarding throughput, particularly since no hanging-node constraints
have to be applied. With increasing number of processes, the workload might not be well distributed anymore
in the case of local smoothing with the result that parallel eﬃciency drops strongly. In contrast, this is—by
construction—not an issue in the global-coarsening case, since the work is simply redistributed between the
levels. The price is that one might need to send around a lot of data during restriction and prolongation. The
number of levels in the case of local smoothing and of global coarsening is the same, leading to potentially
same scaling limits O(L). However, with appropriate partitioning of the levels one could decrease the number
of participating processes on the coarser levels and switch to a coarse-grid solver at an earlier stage in the
global-coarsening case, leading to a better scaling limit. In the following, we will show experimentally that the
above statements can be veriﬁed and will take a more detailed look at the signiﬁcance of a good load balance
and of a cheap transfer, which are mutually contradicting requirements.

Making deﬁnite general conclusions is diﬃcult as they depend on the number of processes as well as on
the type of the coarse mesh and of the reﬁnement. While the two meshes considered here are prototypical for
many problems we have encountered in practice, it is clear that the statements made in this publication cannot
hold in all cases, since it is easy to construct examples that favor one over the other. However, the two meshes
demonstrate clearly, which aspects are dominating at which problem sizes; the actual crossover point, however,
might be problem- and hardware-speciﬁc.

14

Table 3: Speciﬁcation of the hardware system used for evaluation. Memory bandwidth is according to the
STREAM triad benchmark (optimized variant without read for ownership transfer involving two reads and one
write), and GFLOP/s are based on the theoretical maximum at the AVX-512 frequency. The dgemm performance
is measured for m = n = k = 12,000 with Intel MKL 18.0.2. We measured a frequency of 2.5 GHz with AVX-512
dense code for the current experiments. The empirical machine balance is computed as the ratio of measured
dgemm performance and STREAM bandwidth from RAM memory.

cores
frequency base (max AVX-512 frequency)
SIMD width
arithmetic peak (dgemm performance)
memory interface
STREAM memory bandwidth
empirical machine balance
L1-/L2-/L3-/MEM size
compiler + compiler ﬂags

Intel Skylake Xeon Platinum 8174
2 × 24
2.7 GHz
512 bit
4147 GFLOP/s (3318 GFLOP/s)
DDR4-2666, 12 channels
205 GB/s
14.3 FLOP/Byte
32kB (per core)/1MB (per core)/66MB (shared)/96GB(shared)
g++, version 9.1.0, -O3 -funroll-loops -march=skylake-avx512

5 Performance analysis: h-multigrid

In the following, we solve a 3D Poisson problem with homogeneous Dirichlet boundary conditions and a constant
right-hand side as well as compare the performance of local-smoothing and global-coarsening algorithms for the
octant and shell test cases, as introduced in Section 4. The results for a more complex setup with non-
homogeneous Dirichlet boundary conditions are shown in Table 6 in Appendix A.

We will use a continuous Lagrange ﬁnite element, which is deﬁned as the tensor products of 1D ﬁnite elements
with degree p. For quadrature, we consider the consistent Gauss–Legendre quadrature rule with (p + 1)3 points.
We start with investigating the serial performance, proceed with parallel execution with moderate numbers
of processes, and ﬁnally analyze the parallel behavior on the large scale (150k processes). We conclude this
section with investigation of an alternative partitioning scheme for global coarsening.

In order to obtain the best performance, the experiments are conﬁgured in the following way:

• Cells with hanging-node constraints are weighted by the factor of 2.

• The conjugate-gradient solver is run until a reduction of the l2-norm of the unpreconditioned residual by
104 is obtained. We choose a rather coarse tolerance, since this is a common value for the solution of
time-dependent problems, such as the Navier–Stokes equations, where good initial guesses can be obtained
by projection and extrapolation without the need to converge multigrid to many digits. Similarly, coarse
tolerances also indicate the costs of solving in a full-multigrid scenario with the ﬁnest level only correcting
against the next coarser one.

• The conjugate-gradient solver is preconditioned by a single V-cycle of either a local-smoothing or a global-

coarsening multigrid algorithm.

• All operations in the multigrid V-cycle are run with single-precision ﬂoating-point numbers, while the

conjugate-gradient solver is run in double precision [25].

• We use a Chebyshev smoother of degree 3 on all levels.

• As coarse-grid solver, we use two V-cycles of AMG (double-precision, ML [92] with parameters shown in

Appendix C).

The results of performance studies leading to the decision on the conﬁguration described above are presented
in Tables 7-11 in Appendix A. All experiments have been conducted on the SuperMUC-NG supercomputer. Its
compute nodes have 2 sockets (each with 24 cores of Intel Xeon Skylake) and the AVX-512 ISA extension so
that 8 doubles or 16 ﬂoats can be processed per instruction. A detailed speciﬁcation of the hardware is given
in Table 3. The parallel network is organized into islands of 792 compute nodes each. The maximum network
bandwidth per node within an island is 100GBit/s=12.5GB/s5 via a fat-tree network topology.
Islands are
connected via a pruned-tree network architecture (pruning factor 1:4).

5https://doku.lrz.de/display/PUBLIC/SuperMUC-NG, retrieved on February 26, 2022.

15

CG (2.91s)
MG (10.1s)
CG↔MG (0.93s)

CG (2.92s)
MG (10.1s)
CG↔MG (0.32s)

(a) Local smoothing (per it.: 13.94s).

(b) Global coarsening (per it.: 13.34s).

Figure 9: Time per iteration spent for conjugate-gradient solver (CG), multigrid preconditioner (MG), as well
as transfer between solver and preconditioner (CG↔MG) in a serial shell simulation with k = 4 and L = 9.

For local smoothing and global coarsening, we use diﬀerent implementations of the transfer operator from
deal.II (see Subsection 3.5).
In order to demonstrate that they are equivalent and results shown in the
following are indeed related to the deﬁntion of multigrid levels and the resulting diﬀerent algorithms, we present
in Table 12 in Appendix A a performance comparison of these implementations for uniformly reﬁned meshes of
a cube, for which both algorithms are equivalent.

Note: For global coarsening, we have investigated the possibility to decrease the number of participating
processes and to switch to the coarse-grid solver earlier. For our test problems, we could not see any obvious
beneﬁts for the time to solution so that we do not use these features of global coarsening, but defer their
investigation to future work.

5.1 Serial runs: overview

Figure 9 gives an overview of the time shares during the solution process in a serial shell simulation for local
smoothing and global coarsening. Without going into details of the actual numbers, one can see that most of
the time is spent in the multigrid preconditioner in the case both of local smoothing and of global coarsening
(72%/76%). It is followed by the other operations in the outer conjugate gradient solver (21%/22%). The least
time is spent for transferring data between preconditioner and solver (7%/2%). It is well visible that more time
(factor of approximate 3) is spent for the transfer in the local-smoothing case. This is not surprising, since the
transfer involves all multigrid levels sharing cells with the active level. Since the overwhelming share of solution
time is taken by the multigrid preconditioner, all detailed analysis in the remainder of this work concentrates
on the multigrid V-cycle.

One should note that spending 72%/76% of the solution time within the multigrid preconditioner is already
low, particularly taking into account that the outer conjugate-gradient solver also performs its operator evalua-
tions (1 matrix-vector multiplication per iteration) in an eﬃcient matrix-free fashion. The low costs are related
to the usage of single-precision ﬂoating point numbers and to the low number of pre-/postsmoothing steps,
which results in a total of 6-7 matrix-vector multiplications per level and iteration.

5.2 Serial run

Tables 4 and 5 show the number of iterations and the time to solution for the octant and the shell test cases
run serially with local smoothing and global coarsening as well as with the polynomial degrees p = 1 and p = 4.
It is well visible that local smoothing has to perform at least as many iterations as global coarsening, with
the diﬀerence in iterations limited to 1 in the examples considered. This diﬀerence is not surprising, since global
coarsening does at least as much work as local smoothing, by smoothing over the whole computational domain.
Note that global coarsening also beneﬁts from the simple setup of a smooth solution with artiﬁcial reﬁnement,
see Table 6 in the appendix for a somewhat more realistic test case. As already seen in Tables 1 and 2, the
serial workload is higher in the case of global coarsening; this is also visible in the times of a single V-cycle (not
shown). Nevertheless, the fewer number of iterations leads to a smaller time to solution in the case of global
coarsening in some instances. Generally, the costs of a global-coarsening V-cycle are relatively more expensive
in the case of the shell simulation with linear elements: This is not surprising due to the higher number of cells
with hanging-node constraints (see also Figure 6) in the shell case and the higher overhead of linear elements
for application of hanging-node constraints, as analyzed by [45].

16

Table 4: Number of iterations and time to solution for local smoothing (LS) and global coarsening (GC) with
1 process and 192 processes for the octant simulation case.

p = 1

p = 4

1 process

192 processes (4 nodes)

p = 1

p = 4

LS

GC

LS

GC

LS

GC

LS

GC

L #i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

3
4
5
6
7
8
9

4
4
4
4
4
4
4

5.0e-4
1.8e-3
8.6e-3
5.1e-2
3.6e-1
2.8e+0
2.2e+1

4
4
4
4
3
3
3

6.0e-4
2.5e-3
1.2e-2
6.5e-2
3.2e-1
2.3e+0
1.8e+1

4
4
4
4
4
4
-

3.8e-3
1.8e-2
1.1e-1
8.8e-1
6.9e+0
5.3e+1
-

4
4
3
3
3
3
-

4.3e-3
2.0e-2
8.7e-2
6.5e-1
5.0e+0
3.8e+1
-

4
4
4
4
4
4
4

1.2e-3
2.6e-3
5.3e-3
5.6e-3
1.3e-2
3.9e-2
2.3e-1

4
4
4
4
3
3
3

1.0e-3
1.7e-3
2.6e-3
4.1e-3
5.7e-3
2.1e-2
1.3e-1

4
4
4
4
4
4
-

2.5e-3
4.3e-3
6.9e-3
1.7e-2
8.4e-2
7.2e-1
-

4
4
3
3
3
3
-

2.5e-3
4.1e-3
5.0e-3
1.0e-2
5.0e-2
4.3e-1
-

Table 5: Number of iterations and time to solution for local smoothing (LS) and global coarsening (GC) with
1 process and 192 processes for the shell simulation case.

p = 1

p = 4

1 process

192 processes (4 nodes)

p = 1

p = 4

LS

GC

LS

GC

LS

GC

LS

GC

L

5
6
7
8
9
10

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

5
5
5
5
5
-

3.6e-3
1.5e-2
7.3e-2
5.1e-1
3.7e+0
-

4
4
4
4
4
-

7.1e-3
2.8e-2
1.3e-1
7.3e-1
4.3e+0
-

4
4
4
4
4
-

3.1e-2
1.9e-1
1.0e+0
7.8e+0
5.8e+1
-

4
4
4
4
4
-

4.5e-2
2.2e-1
1.2e+0
7.9e+0
5.6e+1
-

5
5
5
5
5
5

6.6e-3
6.8e-3
9.2e-3
1.7e-2
6.3e-2
3.9e-1

4
4
4
4
4
4

3.1e-3
4.1e-3
5.6e-3
1.2e-2
3.5e-2
1.9e-1

4
4
4
4
4
-

6.4e-3
1.0e-2
2.3e-2
1.1e-1
8.5e-1
-

4
4
4
4
4
-

6.3e-3
9.2e-3
1.8e-2
6.9e-2
5.2e-1
-

Figure 10 shows the distribution of times spent on each multigrid level and in each multigrid stage for the
octant case with L = 8 and p = 4. While the times spent on each multigrid level show similar trends in the
case of local smoothing and global coarsening, distinct (and expected) diﬀerences are visible for the stages:
The restriction and prolongation steps take about the same time in both cases. The presmoothing,
residual, and postsmoothing steps are slightly more expensive in the global-coarsening case, which is related
to the observation that the evaluation of the level operator A(l) (2/1/3-times) is the dominating factor and the
presence of hanging-node constraints makes its evaluation more expensive. The observation that the residual
evaluation is not more expensive in the local-smoothing case is related to the fact that the computation of
A(l)
S is a side product of the application of A(l). The only visible additional cost of local smoothing is
A(l)
E (edge step). However, its evaluation is less expensive than the application of A(l), since only DoFs in
proximity to the interface are updated.

ESx(l)
SEx(l)

5.3 Moderately parallel runs

For the discussion of moderately parallel runs, we have run simulations with 192 processes (on 4 nodes). Table 1
and 2 have shown that imbalance in the workload leads to parallel workload eﬃciencies of 40–50% in the case
of local smoothing. The imbalance is lower in the case of global coarsening at the price of a permutation during
prolongation and restriction.

Tables 4 and 5 conﬁrm the signiﬁcance of a good workbalance for the time to solution. The number of
processes does not inﬂuence the number of iterations due to the chosen smoother. Speedups—compared to
local smoothing—are reached: up to 2.3/1.7 for the octant (p = 1/p = 4) and up to 2.1/1.6 for the shell
case if the diﬀerent iteration numbers are considered. Normalized per solver iteration, the advantage of global
coarsening in these four cases is 2.0, 1.3, 1.7 and 1.6, respectively. Figure 11 gives an indication on this behavior
by showing the distribution of the minimum/maximum/average times spent on each multigrid level and each
multigrid stage for the octant case with L = 8. In the case of global coarsening, it is well visible that the
load is equally distributed and the time spent on the levels is signiﬁcantly reduced level by level for the higher
levels. For the ﬁnest ones, local smoothing shows a completely diﬀerent picture. On the ﬁnest level, there are
processes with hardly any work, but nevertheless the average work is close to the maximum value, indicating
that the load is well-balanced among the processes with work. However, on the second ﬁnest level, a signiﬁcant

17

Local smoothing

Global coarsening

]
s
[

e
m

i
t

e
v
i
s
u
l
c
x
E

8.00

6.00

4.00

2.00

0.00

4.00

]
s
[

e
m
T

i

2.00

0

1

2

3

4

5

6

7

8

Level

8.00

6.00

4.00

2.00

0.00

4.00

2.00

0

1

2

3

4

5

6

7

8

Level

0.00

pres m oothing

residual

restriction
coarse-grid

solver
prolongation

edge

posts m oothing

0.00

pres m oothing

residual

restriction
coarse-grid

solver
prolongation

edge

posts m oothing

Figure 10: Proﬁle of a V-cycle of an octant simulation with a single process for L = 8 and p = 4

Local smoothing

Global coarsening

min max

avg

0

1

2

3

4

5

6

7

8

Level

min max

avg

]
s
[

e
m

i
t

e
v
i
s
u
l
c
x
E

0.10

0.08

0.06

0.04

0.02

0.00

]
s
[

e
m
T

i

0.04

0.02

min max

avg

0

1

2

3

4

5

6

7

8

Level

min max

avg

0.10

0.08

0.06

0.04

0.02

0.00

0.04

0.02

0.00

pres m oothing

residual

restriction
coarse-grid

solver
prolongation

edge

posts m oothing

0.00

pres m oothing

residual

restriction
coarse-grid

solver
prolongation

edge

posts m oothing

Figure 11: Proﬁle of a V-cycle of an octant simulation with 192 processes for L = 8 and p = 4

workload imbalance—of factor 2.8—is visible also among the processes with work. This leads to the situation
that the maximum time spent on the second ﬁnest level is just slightly less than the one on the ﬁnest level,
contradicting our expectation of a geometric series and leading to the observed increase in the total runtime.

5.4 Large-scale parallel run

Figures 12 and 13 show results of scaling experiments starting with 1 compute node (48 processes) up to 3,072
nodes (147,456 processes). Besides the times of a single V-cycle, we plot the normalized throughput (DoFs per
process and time per iteration) against the time per iteration. The throughput in case of p = 4 is signiﬁcantly—
by approximatly a factor of 3—higher than for p = 1. This is expected and related to the used matrix-free
algorithms and their node-level performance, which improves with the polynomial order [21]. Just as in the
moderately parallel case (see Subsection 5.3), we can observe better timings in the case of global coarsening for

18

L = 7

L = 8

L = 9

L = 10

L = 11

LS

GC

p = 1

p = 4

id

eal

1

4

16

64

256

1k

3k

100

10−1

10−2

10−3

id

eal

1

4

16

64

256

1k

3k

Nodes (× 48 CPUs)

Nodes (× 48 CPUs)

·106

·106

6

4

2

0

10−2

10−1

sec / it

10−2

10−1

sec / it

100

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

]
c
e
s
×
c
o
r
p
[
/
]
t
i

×
s
F
o
D

[

2

1

0

Figure 12: Strong-scaling comparison of local smoothing (LS) and global coarsening (GC) for octant for p = 1
and p = 4.

L = 7

L = 8

L = 9

L = 10

L = 11

L = 12

LS

GC

p = 1

p = 4

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

]
c
e
s
×
c
o
r
p
[
/
]
t
i

×
s
F
o
D

[

2

1

0

id

eal

1

4

16

64

256

1k

3k

100

10−1

10−2

10−3

id

eal

1

4

16

64

256

1k

3k

Nodes (× 48 CPUs)

Nodes (× 48 CPUs)

·106

·106

6

4

2

0

10−2

10−1

sec / it

10−2

10−1

sec / it

100

Figure 13: Strong-scaling comparison of local smoothing (LS) and global coarsening (GC) for shell for p = 1
and p = 4.

19

a large range of conﬁgurations (max. speedup: octant 1.9/1.4 for p = 1/p = 4, shell: 2.4/2.4). The number of
iterations of local smoothing is 4 for both cases and all reﬁnement numbers. The number of iterations of global
coarsening is 4 for the sphere case and decreases from 4 to 2 with increasing number of reﬁnements in the case
of quadrant so that the actual speedups reported above are even higher. The high speedup numbers of global
coarsening in the shell simulation case are particularly related to its high workload and vertical eﬃciency, as
shown in Table 5.

The normalized plots give additional insights. Apart from the obvious observations that, with increasing
number of reﬁnements, the minimal time to solution increases (left bottom corner of the plots in Figures 12
and 13) and global coarsening starts with higher throughputs, one can see that the decrease of parallel eﬃciency
is more moderate in the case of global coarsening. This is quite astonishing and means, e.g., for the octant
case with p = 1/L = 10, that one can increase the number of processes by a factor of 16 and still have a
throughput per process that is higher than the one in the case of single-node computations of local smoothing,
which normally shows a kink in eﬃciency at early stages (particularly visible in the shell case, which matches
the ﬁndings made in [18]). A further observation in the shell case is that the lines for global coarsening overlap
far from the scaling limit, i.e, the throughput is independent of the number of processes and the number of
reﬁnements. This is not the case for local smoothing, where the throughput deterioriates with the number of
levels, indicating load-balance problems. The simulations with p = 4 show similar trends, but the lines are not
as smooth, possibly due to the decreased granularity for higher orders.

5.5 First-child policy as alternative partitioning strategy for global coarsening

In Subsection 5.3, we have discussed that local smoothing with ﬁrst-child policy might suﬀer from deteriorated
reduction rates of the maximimum number of cells on each level; in particular, there might be processes without
any cells, i.e., any work, increasing the critical path, although the vertical eﬃciency is optimal. In this section,
we consider the ﬁrst-child policy, which we use in the context of local smoothing as an alternative for partitioning
of the levels for global coarsening.

Figure 14 shows the timings of large-scale octant simulations for 1) local smoothing, 2) global coarsening
with default partitioning, and 3) global coarsening with ﬁrst-child policy for p = 1/p = 4. The timings of the
latter show similar trends as global coarsening with default partitioning and are lower than the ones of local
smoothing. To explain this counterintuitive observation, Figure 14 shows the maximum number of cells on each
multigrid level of the three approaches for L = 11 on 256 nodes. Since global coarsening with ﬁrst-child policy
does not perform any repartitioning, better reduction trends as in the local-smoothing case on all levels can not
be expected, however, one can observe that, for the local section of the reﬁnement tree, the number of cells on
the ﬁnest levels can be reduced nearly as well as in the case of the default partitioner, i.e., the parallel workload
and the parallel eﬃciency are not much worse, nonetheless, with higher vertical eﬃciency. This is not possible
for the lower levels, but the behavior of the ﬁnest levels dominates the overall trends, since they take the largest
time of the computation. One should note that local smoothing has access to the same cells, but simply skips
them during smoothing of a given level, missing the opportunity to reduce the problem size locally. In our
experiments, it is not clear whether an optimal reduction of cells is crucial for all conﬁgurations: for p = 1,
global coarsening with default partitioning is faster for most conﬁgurations (up to 20%) and for p = 4, global
coarsening with ﬁrst-child policy is faster (up to 10%). We could trace this diﬀerence back to the diﬀerent costs
of the transfer, where for p = 4 a reduced data transfer (both within the compute node and across the network)
and for p = 1 a better load balance is beneﬁcial.

In the shell case (Figure 15), we observe that both global coarsening partitioning strategies result in very
similar reduction rates, which can be traced back to the fact that both strategies lead to comparable partitionings
of the levels (see also Table 5, which shows a very high vertical eﬃciency of the default partitioning) and—as
a consequence—to very similar solution times (±10%). Local smoothing only reduces the maximum number
of cells per level optimally once all locally reﬁned cells have been processed and the levels that have been
constructed via global reﬁnement have been reached. This case stresses the issue of load imbalances related to
reduction rates signiﬁcantly diﬀering between processes.

20

LS

GC (default)

GC (fcp)

L = 8

L = 9

L = 10

L = 11

LS/GC (default)

GC (fcp)

p = 1

p = 4

l
e
v
e
l

r
e
p

s
l
l
e
c

.
x
a
M

105

104

103

102

101

100

÷ 2

÷ 8

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

id

e

al

id

e

al

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

0

2

4

6

Level

8

10

1

4

16

64

256

512

1

4

16

64

256

512

Nodes (× 48 CPUs)

Nodes (× 48 CPUs)

Figure 14: Left: Maximum number of cells per level (L = 11, 256 nodes) and right: strong scaling of global
coarsening with ﬁrst-child policy (fcp) in comparison to local smoothing (LS) and global coarsening (GC) with
default policy for octant.

LS

GC (default)

GC (fcp)

L = 9

L = 10

L = 11

L = 12

LS/GC (default)

GC (fcp)

p = 1

p = 4

106

104

102

100

l
e
v
e
l

r
e
p

s
l
l
e
c

.
x
a
M

const

÷ 8

÷ 8

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

id

e

al

id

e

al

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

100

10−1

10−2

10−3

0

2

4

6

Level

8

10

12

1

4

16

64

256

512

1

4

16

64

256

512

Nodes (× 48 CPUs)

Nodes (× 48 CPUs)

Figure 15: Left: Maximum number of cells per level (L = 12, 256 nodes) and right: strong scaling of global
coarsening with ﬁrst-child policy (fcp) in comparison to local smoothing (LS) and global coarsening (GC) with
default policy for shell.

21

L = 7

L = 8

L = 9

L = 10

LS/GC

p-mg

L = 8, p-mg

L = 9, p-mg

L = 8, GC part

L = 9, GC part

100

10−1

10−2

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

10−3

1

ideal

4

16

64

256

Nodes (× 48 CPUs)

100

10−1

10−2

]
s
[

n
o
i
t
a
r
e
t
i

r
e
p

e
m
T

i

10−3

1

ideal

4

16

64

256

Nodes (× 48 CPUs)

Figure 16: Strong scaling of p-multigrid for octant for p = 4. p-multigrid switches to a global-coarsening coarse-
grid solver immediately once linear elements have been reached. Left: Comparison with local smoothing (LS)
and global coarsening (GC) for p = 4. Right: Comparison with global coarsening (GC) for p = 1 (coarse-grid
problem).

6 Performance analysis: p-multigrid

In this section, we consider p-multigrid. The settings are as described in Section 5. On the coarsest level (p = 1,
ﬁne mesh with hanging nodes), we run a single V-cycle of either a local-smoothing or a global-coarsening
In Appendix B, we present a comparison with state-of-the-art
geometric multigrid solver in an hp context.
AMG solvers [92, 93] as coarse-grid solvers of p-multigrid on 16 nodes. The results show better timings in favor
of geometric multigrid, which also turned out to be more robust with a single V-cycle.

Figure 16 presents a strong-scaling comparison of h- and p-multigrid versions of the global-coarsening algo-
rithm for the octant case for p = 4. The measurements for local smoothing as coarse-grid solver are skipped
here, since the trends are similar and the values for local smoothing are only a few percentage higher than the
ones for global coarsening.6 As we use a bisection strategy in the context of p-multigrid, the overall multigrid
algorithm has two additional levels compared to pure h-multigrid (the same ﬁne mesh, but with p = 1 and
p = 2). In our experiments, we have observed that p-multigrid needs at least as many iterations as the pure
(global-coarsening) h-multigrid algorithm (with small diﬀerences—max 1). One can see that one cycle of p-
multigrid is 10-15% faster than h-multigrid for moderate numbers of processes. The reason for this is that the
smoother application on the ﬁnest level is equally expensive, however, the transfer between the two ﬁnest levels
is cheaper: due to the same partitioning of these levels, the data is mainly transferred between cells that are
locally owned on both the coarse and the ﬁne level. At the scaling limit (not shown), p-multigrid falls behind
h-multigrid regarding performance. This is related to the increased number of levels.

For the sake of completeness, Figure 16 also shows global-coarsening timings for p = 1 for L = 8/L = 9
exemplarily, since it is the coarse-grid problem of the p-multigrid solver. It is well visible that the coarse-grid
problem is negligible for a wide range of nodes, but becomes noticeable at the scaling limit.

7 Application: variable viscosity Stokes ﬂow

We conclude this publication by presenting preliminary results of a practical application from Geosciences by
integrating the global-coarsening framework into the mantle convection code ASPECT [94, 95] and compare it
against the existing local-smoothing implementation [20].
We consider the variable-viscosity Stokes problem

6The raw data for L = 9 is provided in Appendix B.

−∇ · (2ηε(u)) + ∇p = f

∇ · u = 0

22

#DoFs [1e6] #it

solve [s]

V-cycle [s] #it

global coarsening

local smoothing
solve [s]

V-cycle [s]

10.0
20.7
43.2
88.0
178.0
355.0
715.7
1441.4
2896.7
5861.9

25
25
25
22
25
26
27
28
28
29

1.38
1.61
2.26
2.39
4.62
6.47
10.65
22.17
37.38
77.94

0.003
0.006
0.009
0.013
0.020
0.039
0.069
0.141
0.266
0.515

25
25
26
27
28
28
27
29
26
26

1.15
1.84
2.63
4.15
7.87
13.66
23.67
50.85
99.07
193.19

0.006
0.008
0.013
0.022
0.047
0.109
0.214
0.436
0.971
2.060

L

5
6
7
8
9
10
11
12
13
14

Figure 17: Stokes ﬂow in a spherical shell. Left: Visualization of the solution with the high viscosity sinkers
(red), velocity vector ﬁeld in purple, and adaptive mesh in the background. Right: Performance comparison
with 7168 processes.

with a Q2-Q1 Taylor-Hood discretization of velocity (cid:126)u, pressure p, and viscosity η(x). The resulting linear
system

(cid:18)A BT
0

B

(cid:19) (cid:18)U
P

(cid:19)

(cid:19)

(cid:18)F
0

=

is solved with a Krylov method (in our tests using IDR(2), see [20]) preconditioned using a block preconditioner

P −1 =

(cid:19)−1

(cid:18)A BT
0 −S

where the Schur complement S = BA−1BT is approximated using a mass matrix weighted by the viscosity.
The inverses of the diagonal blocks of A and the Schur complement approximation ˆS are each approximated
by applying a single V-cycle of geometric multigrid using a Chebyshev smoother of degree 4 and implemented
in a matrix-free fashion. Each IDR(2) iteration consists of 3 matrix-vector products and 3 preconditioner
applications. For a detailed description see [20].

We consider a 3D spherical shell benchmark problem called “nsinker spherical shell” that is part of ASPECT.
A set of 7 heavy sinkers are placed in a spherical shell with inner radius 0.54 and outer radius 1.0. The ﬂow
is driven by the density diﬀerence of the sinkers and the gravity of magnitude 1. The viscosity is evaluated
in the quadrature points of each cell on the ﬁnest level, averaged using harmonic averaging on each cell, and
then interpolated to the coarser multigrid levels using the multigrid transfer operators (see also the function
interpolate to mg() in Figure 5) for use in the multigrid preconditioner.

The initial mesh consisting of 96 coarse cells, 4 initial reﬁnement steps and a high-order manifold description
is reﬁned adaptively using a gradient jump error estimator of the velocity ﬁeld roughly doubling the number of
unkowns in each step, see Figure 17 (left).

We compare number of iterations, time to solution, and time for a single V-cycle of the A block in Figure 17

(right). Computations are done on TACC Frontera7 on 7168 processes (128 nodes with 56 cores each).

While iteration numbers are very similar, the simulation with global-coarsening is about twice as fast in
total. Each V-cycle is up to four times faster, which is not surprising, since the mesh has—for high number
of reﬁnements—a workload eﬃciency of approximately 20%. The results suggest that the ﬁndings regarding
iteration numbers and performance obtained in Section 5 for a simple Poisson problem are also applicable for
this non-trivial problem.

8 Summary and outlook

We have compared geometric local-smoothing and geometric global-coarsening multigrid implementations for
locally reﬁned meshes. They are based on optimal node-level performance through matrix-free operator evalua-
tion and have been integrated into the same ﬁnite-element library (deal.II) in order to enable a fair comparison

7Using deal.II master f07477f502 with 64bit indices, Intel 19.1.0.20200306 with -O3 and -march=native

23

regarding implementation complexity and performance.

From the implementation point of view, the two multigrid versions are—except for some subtle diﬀerences—
similar, requiring special treatment either of reﬁnement edges or of hanging-node constraints during the appli-
cation of the smoother and the transfer operator. For the latter, one can usually rely on existing infrastructure
for the application of constraints, already available from the context of matrix-free operator evaluations [45, 85].
During transfer, global coarsening needs to transfer between cells that are reﬁned or not. To be able to vec-
torize the cell-local transfer, we categorize cells within the transfer operator and process cells with the same
category in one go. In the case of local smoothing, it is possible but not common to repartition the multigrid
levels; instead, one uses local strategies for partitioning. For global coarsening, we investigated two partitioning
strategies: one that optimally balances workload during smoothing via repartitioning each level and one that
minimizes the data to be communicated during the transfer phase.

In a large number of experiments, we have made the observation that, for serial simulations, geometric
local smoothing is faster than geometric global coarsening (if the number of iterations is the same), since the
total number of cells to perform smoothing on is less and the need for evaluating hanging-node constraints
on each level might be noticeable, particularly for linear shape functions. For parallel simulations, an equally
distributed reduction of cells is beneﬁcial.
Is this not given, load imbalance is introduced and the critical
In the case of local smoothing, there might be a non-
path of cells, i.e., the time to solution, is increased.
negligible number of processes without cells, i.e. work, naturally introducing load imbalance already on the
ﬁnest—computationally most expensive—levels. Global coarsening with repartitioning alleviates this problem
and reaches optimal parallel workload, however, comes with the disadvantage of expensive transfer steps due
to permutation of the data. We made the observation that global coarsening even if levels are not partitioned
optimally for smoothing allows—for the examples considered—to reduce the number of cells on the ﬁrst levels
surprisingly well, introducing only a small load imbalance, but allowing to perform transfers locally. We could
not make a deﬁnite statement on the choice of partitioning strategies for global coarsening, since it very much
depends on whether the transfer or the load imbalance is the bottleneck for the given problem.

We have also considered polynomial global coarsening (p-multigrid).

Its implementation is conceptually
similar to the one of geometric global coarsening so that the data structures of geometric global coarsening can
be reused and only the setup diﬀers. Far from the scaling limit, p-multigrid shows better timings per iteration,
which can be contributed to the cheaper intergrid transfer. At the scaling limit, the introduction of additional
multigrid levels (when combining h and p multigrid) is noticeable and leads to slower times due to latency.

Global-coarsening algorithms also give the possibility to simply remove ranks from MPI communicators,
allowing to increase the granularity of the problem on each level and once the problem size can not be reduced
by a suﬃcient degree anymore, one can simply switch to the coarse-grid solver even on a level with hanging
nodes. Furthermore, h and p global coarsening could be done in one go: while this might lead to an overly
aggressive coarsening in many cases and, as a result, to a deterioration of the convergence rate, in other cases, the
reduced number of levels could result in a reduced latency and, as a consequence, in an improved strong-scaling
behavior. The investigation of these topics is deferred to future work.

In this publication, we focused on geometrically reﬁned meshes consisting only of hexahedral shaped cells,
where all cells have the same polynomial degree p. However, the presented algorithms work both for p- and for
hp-adaptive problems, where the polynomial degree varies for each cell, as well as for simplex or mixed meshes,
as the implementation in deal.II [24] shows. Moreover, they are—as demonstrated in [17]—applicable also for
auxiliary-space approximation for DG.

9 Acknowledgements

The authors acknowledge collaboration with Maximilian Bergbauer, Thomas C. Clevenger, Ivo Dravins, Niklas
Fehn, Marc Fehling, and Magdalena Schreter as well as the deal.II community.

This work was supported by the Bayerisches Kompetenznetzwerk f¨ur Technisch-Wissenschaftliches Hoch-
und H¨ochstleistungsrechnen (KONWIHR) through the projects “Performance tuning of high-order discontin-
uous Galerkin solvers for SuperMUC-NG” and “High-order matrix-free ﬁnite element implementations with
hybrid parallelization and improved data locality”. The authors gratefully acknowledge the Gauss Centre for
Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the
GCS Supercomputer SuperMUC-NG at Leibniz Supercomputing Centre (LRZ, www.lrz.de) through project
id pr83te.

24

Timo Heister was partially supported by the National Science Foundation (NSF) Award DMS-2028346,
OAC-2015848, EAR-1925575, by the Computational Infrastructure in Geodynamics initiative (CIG), through
the NSF under Award EAR-0949446 and EAR-1550901 and The University of California – Davis, and by
Technical Data Analysis, Inc. through US Navy STTR Contract N68335-18-C-0011. Clemson University is
acknowledged for generous allotment of compute time on Palmetto cluster.

A APPENDIX TO SECTION 5

Table 6: Number of iterations and time to solution for local smoothing (LS) and global coarsening (GC)
for the octant simulation case with given analytical solution with 768 processes (16 nodes): u((cid:126)x) =
(cid:16) 1
exp (cid:0)−||(cid:126)x − (cid:126)x0||/α2(cid:1) with (cid:126)x0 = (−0.5, −0.5, −0.5)T and α = 0.1. Right-hand-side function f ((cid:126)x)
√
α

(cid:17)3

2π

and inhomogeneous Dirichlet boundary conditions have been selected appropriately.

p = 1

p = 4

LS

GC

LS

GC

L

3
4
5
6
7
8
9
10

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

4
4
4
4
4
5
5
5

1.4e-3
2.8e-3
4.0e-3
5.7e-3
8.2e-3
2.2e-2
8.8e-2
6.1e-1

4
4
4
4
4
4
4
4

1.0e-3
1.7e-3
2.7e-3
4.2e-3
6.1e-3
1.2e-2
5.2e-2
3.8e-1

4
4
4
4
4
5
5
-

2.9e-3
4.7e-3
6.9e-3
1.2e-2
2.8e-2
2.8e-1
2.5e+0
-

4
4
4
4
4
4
4
-

2.9e-3
4.2e-3
6.5e-3
1.0e-2
2.4e-2
1.7e-1
1.6e+0
-

Table 7: Number of iterations and time to solution for local smoothing, global coarsening, and AMG with 768
processes (16 nodes) for the octant simulation case. For local smoothing and global coarsening, results are
shown for Chebyshev smoothing degrees of k = 3 and k = 6.

local smoothing

k = 3

k = 6

p = 1

global coarsening

k = 3

k = 6

AMG

local smoothing

k = 3

k = 6

global coarsening

k = 3

k = 6

p = 4

L

3
4
5
6
7
8
9
10

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

4
4
4
4
4
4
4
4

1.3e-3
2.8e-3
4.3e-3
5.8e-3
8.2e-3
1.7e-2
7.1e-2
5.0e-1

3
3
3
3
3
3
3
3

1.4e-3
2.9e-3
4.6e-3
6.4e-3
9.3e-3
2.1e-2
8.6e-2
6.1e-1

4
4
4
4
3
3
3
2

1.0e-3
1.7e-3
2.6e-3
4.3e-3
4.5e-3
9.1e-3
3.8e-2
1.9e-1

2
2
2
2
2
2
2
2

7.0e-4
1.3e-3
2.3e-3
3.4e-3
4.9e-3
1.0e-2
4.3e-2
3.0e-1

1
1
5
8
6
6
7
7

6.0e-3
2.5e-2
4.2e-3
1.5e-2
1.2e-2
3.0e-2
2.1e-1
1.6e+0

4
4
4
4
4
4
4
-

2.6e-3
4.3e-3
6.1e-3
9.9e-3
2.7e-2
2.3e-1
2.0e+0
-

3
3
3
3
3
3
3
-

2.5e-3
4.6e-3
6.6e-3
1.1e-2
3.0e-2
2.6e-1
2.3e+0
-

4
4
3
3
3
3
3
-

2.5e-3
4.0e-3
4.9e-3
7.2e-3
1.8e-2
1.4e-1
1.2e+0
-

2
2
2
2
2
2
2
-

1.9e-3
3.0e-3
5.1e-3
7.1e-3
1.7e-2
1.4e-1
1.3e+0
-

Table 8: Number of iterations and time to solution for local smoothing (LS) and global coarsening (GC) for
the octant simulation case with 768 processes (16 nodes). All operations in the outer CG solver are run with
double-precision ﬂoating-point numbers and in the multigrid V-cycle are run with the following multigrid
number types: single- or double-precision ﬂoating-point numbers.

p = 1

p = 4

p = 1

p = 4

LS

GC

LS

GC

LS

GC

LS

GC

double

ﬂoat

L

3
4
5
6
7
8
9
10

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

4
4
4
4
4
4
4
4

1.3e-3
2.8e-3
4.0e-3
7.8e-3
8.9e-3
2.1e-2
9.9e-2
7.7e-1

4
4
4
4
3
3
3
2

9.0e-4
1.6e-3
2.6e-3
4.0e-3
4.9e-3
1.0e-2
5.1e-2
2.8e-1

4
4
4
4
4
4
4
-

2.6e-3
4.5e-3
6.6e-3
1.1e-2
3.8e-2
4.0e-1
3.2e+0
-

4
4
3
3
3
3
3
-

2.7e-3
4.1e-3
5.1e-3
9.6e-3
2.7e-2
2.4e-1
1.9e+0
-

4
4
4
4
4
4
4
4

1.3e-3
2.7e-3
4.0e-3
6.0e-3
8.3e-3
1.7e-2
7.1e-2
5.0e-1

4
4
4
4
3
3
3
2

1.0e-3
1.7e-3
2.6e-3
4.1e-3
4.8e-3
9.2e-3
3.9e-2
1.9e-1

4
4
4
4
4
4
4
-

2.4e-3
4.1e-3
6.1e-3
1.1e-2
2.7e-2
2.3e-1
2.0e+0
-

4
4
3
3
3
3
3
-

2.4e-3
4.1e-3
4.8e-3
7.3e-3
1.8e-2
1.4e-1
1.2e+0
-

25

Table 9: Number of iterations for local smoothing (LS) and global coarsening (GC) for the octant simulation
case and for diﬀerent global relative solver tolerances with 768 processes (16 nodes).

10−4

10−6

10−8

10−10

p = 1

p = 4

p = 1

p = 4

p = 1

p = 4

p = 1

p = 4

LS GC

LS GC

LS GC

LS GC

LS GC

LS GC

LS GC

LS GC

4
4
4
4
4
4
4
4

4
4
4
4
3
3
3
2

4
4
4
4
4
4
4
-

4
4
3
3
3
3
3
-

6
6
6
6
6
6
6
6

5
6
5
5
5
4
4
4

6
6
6
6
6
6
6
-

6
5
5
5
4
4
4
-

7
8
8
8
8
8
8
8

6
7
7
7
6
6
6
6

8
8
8
8
8
8
8
-

7
7
7
6
6
6
6
-

9
10
10
10
10
9
9
9

8
9
8
8
8
8
7
7

10
10
10
10
10
10
10
-

9
9
8
8
8
7
7
-

L

3
4
5
6
7
8
9
10

Table 10: Time to solution for global coarsening for the octant simulation case and diﬀerent cell weights
for cells near hanging nodes compared to regular cells with 768 processes (16 nodes).

L/w

1.0

1.5

3
4
5
6
7
8
9
10

1.1e-3
1.8e-3
2.5e-3
4.0e-3
4.8e-3
1.1e-2
5.3e-2
2.3e-1

1.0e-3
1.8e-3
2.7e-3
4.1e-3
4.8e-3
1.0e-2
4.4e-2
1.9e-1

p = 1
2.0

1.0e-3
1.8e-3
2.7e-3
4.1e-3
4.7e-3
9.1e-3
3.8e-2
1.7e-1

2.5

3.0

1.0

1.5

1.0e-3
1.7e-3
2.6e-3
4.2e-3
4.4e-3
8.5e-3
3.5e-2
1.5e-1

1.0e-3
1.7e-3
2.8e-3
4.1e-3
5.4e-3
8.2e-3
3.5e-2
1.5e-1

2.7e-3
3.9e-3
4.4e-3
7.3e-3
1.8e-2
1.1e-1
9.0e-1
-

2.6e-3
3.9e-3
4.7e-3
7.1e-3
1.7e-2
1.0e-1
8.2e-1
-

p = 4
2.0

2.6e-3
3.9e-3
4.6e-3
7.0e-3
1.6e-2
1.0e-1
7.9e-1
-

2.5

3.0

2.5e-3
3.9e-3
4.5e-3
7.1e-3
1.7e-2
1.0e-1
7.8e-1
-

2.5e-3
3.9e-3
4.6e-3
7.0e-3
1.8e-2
1.0e-1
7.9e-1
-

Table 11: Time to solution for global coarsening for the octant simulation case and diﬀerent cell weights
for cells near hanging nodes compared to regular cells with 24,576 processes (512 nodes).

L/w

1.0

1.5

3
4
5
6
7
8
9
10
11

1.4e-3
2.1e-3
2.9e-3
4.5e-3
5.9e-3
7.6e-3
1.2e-2
1.8e-2
7.6e-2

1.3e-3
2.2e-3
2.9e-3
4.1e-3
4.8e-3
9.0e-3
1.1e-2
1.5e-2
6.8e-2

p = 1
2.0

1.4e-3
2.1e-3
2.9e-3
4.1e-3
5.7e-3
7.9e-3
1.1e-2
1.4e-2
5.7e-2

2.5

3.0

1.0

1.5

1.6e-3
2.0e-3
2.9e-3
4.2e-3
5.1e-3
9.2e-3
1.1e-2
1.3e-2
5.4e-2

1.4e-3
2.1e-3
2.9e-3
4.1e-3
5.6e-3
8.9e-3
1.0e-2
1.3e-2
5.3e-2

3.1e-3
4.3e-3
4.6e-3
7.0e-3
1.1e-2
1.5e-2
4.9e-2
3.7e-1
-

3.0e-3
4.4e-3
5.0e-3
7.0e-3
1.1e-2
1.5e-2
4.2e-2
3.4e-1
-

p = 4
2.0

3.0e-3
4.3e-3
4.6e-3
7.1e-3
1.1e-2
1.6e-2
4.3e-2
3.4e-1
-

2.5

3.0

2.9e-3
4.3e-3
5.1e-3
6.9e-3
1.1e-2
1.5e-2
4.2e-2
3.4e-1
-

2.9e-3
4.3e-3
4.7e-3
6.6e-3
1.0e-2
1.4e-2
4.2e-2
3.4e-1
-

Table 12: Number of iterations and time to solution for local smoothing (LS) and global coarsening (GC) for a
uniformly reﬁned mesh of a cube (without hanging nodes) with 768 processes (16 nodes).

p = 1

p = 4

LS

GC

LS

GC

L #i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

3
4
5
6
7
8
9

4
4
4
4
4
4
4

1.80e-3
2.50e-3
3.30e-3
4.60e-3
9.00e-3
4.18e-2
3.00e-1

4
4
4
4
4
4
4

1.80e-3
2.40e-3
3.20e-3
5.40e-3
9.00e-3
4.21e-2
2.98e-1

4
4
4
4
4
4
-

2.90e-3
3.90e-3
5.90e-3
1.61e-2
1.29e-1
1.11e+0
-

4
4
4
4
4
4
-

3.20e-3
4.00e-3
6.40e-3
1.69e-2
1.29e-1
1.10e+0
-

26

B APPENDIX TO SECTION 6

Table 13: Number of iterations and time to solution for local smoothing (LS), global coarsening (GC), and AMG
as coarse-grid solver of p-multigrid with 768 processes (16 nodes) for the octant simulation case for p = 4.
For AMG, diﬀerent numbers of V-cycles #v are investigated. AMG parameters used are shown in Appendix C.

LS

GC

#v=1

#v=2

#v=3

#v=4

AMG (ML)

AMG (BoomerAMG)
#v=1

L #i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

3
4
5
6
7
8
9

4
4
4
4
4
4
4

2.60e-3
5.10e-3
7.60e-3
1.11e-2
2.35e-2
1.81e-1
1.53e+0

4
4
4
4
4
4
4

2.20e-3
3.90e-3
6.10e-3
9.20e-3
2.11e-2
1.72e-1
1.51e+0

4
4
4
5
6
7
9

1.70e-3
5.40e-3
1.17e-2
2.37e-2
4.70e-2
3.26e-1
3.54e+0

4
4
4
4
5
5
7

2.00e-3
8.30e-3
1.95e-2
3.24e-2
5.47e-2
2.65e-1
2.97e+0

4
4
4
4
4
4
6

2.20e-3
1.11e-2
2.74e-2
4.45e-2
5.59e-2
2.37e-1
2.75e+0

4
4
4
4
4
4
5

2.50e-3
1.39e-2
3.52e-2
5.96e-2
6.72e-2
2.63e-1
2.46e+0

4
4
4
5
6
7
8

1.70e-3
4.90e-3
1.17e-2
3.60e-2
1.04e-1
4.76e-1
3.83e+0

Table 14: Number of iterations and time of a single iteration for h-multigrid (local smoothing (LS), global
coarsening (GC)) and p-multigrid (local smoothing or global coarsening as coarse-grid solver) for L = 9 and
p = 4.

h-mg

p-mg

LS

GC

LS

GC

#nodes #i

t[s]

#i

t[s]

#i

t[s]

#i

t[s]

8
16
32
64
128
256
512

4
4
4
4
4
4
4

6.30e-1
4.99e-1
2.40e-1
1.39e-1
5.97e-2
3.17e-2
1.54e-2

3
3
3
3
3
3
3

4.95e-1
4.03e-1
2.03e-1
1.05e-1
4.90e-2
2.57e-2
1.37e-2

4
4
4
4
4
4
4

4.75e-1
3.83e-1
1.87e-1
9.57e-2
4.49e-2
2.35e-2
1.38e-2

4
4
4
4
4
4
4

4.66e-1
3.77e-1
1.84e-1
9.25e-2
4.37e-2
2.05e-2
1.24e-2

C AMG Parameters

Teuchos : : ParameterList parameter_list ;
ML_Epetra : : SetDefaults ( ”SA” , parameter_list ) ;

Listing 1: ML [92] (Trilinos 12.12.1)

t y p e ” , ”ILU” ) ;

t y p e ” , coarse_type ) ;

parameter_list . set ( ” smoother :
parameter_list . set ( ” c o a r s e :
parameter_list . set ( ” i n i t i a l i z e random s e e d ” ,
parameter_list . set ( ” smoother :
parameter_list . set ( ” c y c l e a p p l i c a t i o n s ” , 2 ) ;
parameter_list . set ( ” p r e c t y p e ” , ”MGV” ) ;
parameter_list . set ( ” smoother : Chebyshev a l p h a ” , 1 0 . ) ;
parameter_list . set ( ” smoother :
parameter_list . set ( ” a g g r e g a t i o n :
parameter_list . set ( ” c o a r s e : max s i z e ” , 2 0 0 0 ) ;

t h r e s h o l d ” , 1e −4) ;

i f p a c k o v e r l a p ” , 0 ) ;

sweeps ” , 1 ) ;

t r u e ) ;

PCHYPRESetType ( pc , ”boomeramg” ) ;

Listing 2: BoomerAMG [93] (PETSc 3.14.5)

set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g a g g n l ” , ” 2 ” ) ;
set _o pt ion _ val ue ( ”−pc hypre boomeramg max row sum ” , ” 0 . 9 ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g s t r o n g t h r e s h o l d ” , ” 0 . 5 ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g r e l a x t y p e u p ” , ”SOR/ J a c o b i ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g r e l a x t y p e d o w n ” , ”SOR/ J a c o b i ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g r e l a x t y p e c o a r s e ” , ” Gaussian−e l i m i n a t i o n ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g g r i d s w e e p s c o a r s e ” , ” 1 ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g t o l ” , ” 0 . 0 ” ) ;
set _o pt ion _ val ue ( ”−p c h y p r e b o o m e r a m g m a x i t e r ” , ” 2 ” ) ) ;

27

References

[1] Michel O. Deville, Paul F. Fischer, and Ernest H. Mund. High-order methods for incompressible ﬂuid ﬂow,

volume 9. Cambridge University Press, 2002.

[2] Martin Kronbichler, Ababacar Diagne, and Hanna Holmgren. A fast massively parallel two-phase ﬂow solver
for microﬂuidic chip simulation. International Journal of High Performance Computing Applications, 32
(2):266–287, 2018. doi: 10.1177/1094342016671790.

[3] Daniel Arndt, Niklas Fehn, Guido Kanschat, Katharina Kormann, Martin Kronbichler, Peter Munch,
Wolfgang A. Wall, and Julius Witte. ExaDG: High-order discontinuous Galerkin for the exa-scale.
In
Hans-Joachim Bungartz, Severin Reiz, Benjamin Uekermann, Philipp Neumann, and Wolfgang E. Nagel,
editors, Software for Exascale Computing - SPPEXA 2016-2019, pages 189–224, Cham, 2020. Springer
International Publishing.

[4] Peter Munch, Katharina Kormann, and Martin Kronbichler. hyper.deal: An eﬃcient, matrix-free ﬁnite-
element library for high-dimensional partial diﬀerential equations. ACM Transactions on Mathematical
Software, 47(4):1–34, 2021. doi: 10.1145/3469720. URL https://doi.org/10.1145/3469720.

[5] Amir Gholami, Dhairya Malhotra, Hari Sundar, and George Biros. FFT, FMM, or Multigrid? A compara-
tive Study of State-Of-the-Art Poisson Solvers for Uniform and Nonuniform Grids in the Unit Cube. SIAM
Journal on Scientiﬁc Computing, 38(3):C280–C306, 2016. ISSN 1064-8275. doi: 10.1137/15M1010798.

[6] Marco L. Bittencourt, Craig C. Douglas, and Ra´ul A. Feij´oo. Nonnested multigrid methods for linear
problems. Numerical Methods for Partial Diﬀerential Equations: An International Journal, 17(4):313–331,
2001.

[7] James H. Bramble, Joseph E. Pasciak, and Jinchao Xu. The analysis of multigrid algorithms with nonnested

spaces or noninherited quadratic forms. Mathematics of Computation, 56(193):1–34, 1991.

[8] Klaus St¨uben. A review of algebraic multigrid. Journal of Computational and Applied Mathematics, 128

(1-2):281–309, 2001. ISSN 03770427. doi: 10.1016/S0377-0427(00)00516-1.

[9] Niklas Fehn, Peter Munch, Wolfgang A. Wall, and Martin Kronbichler. Hybrid multigrid methods for
high-order discontinuous galerkin discretizations. Journal of Computational Physics, 415:109538, 2020.
doi: 10.1016/j.jcp.2020.109538. URL https://doi.org/10.1016/j.jcp.2020.109538.

[10] Johann Rudi, Omar Ghattas, A. Cristiano I. Malossi, Tobin Isaac, Georg Stadler, Michael Gurnis, Peter
W. J. Staar, Yves Ineichen, Costas Bekas, and Alessandro Curioni. An extreme-scale implicit solver
In Proceedings of the International Conference for High Performance Computing,
for complex PDEs.
ISBN
Networking, Storage and Analysis on - SC ’15, pages 1–12, New York, USA, 2015. ACM Press.
9781450337236. doi: 10.1145/2807591.2807675.

[11] Benedict O’Malley, J´ozsef K´oph´azi, Richard P. Smedley-Stevenson, and Monroe D. Eaton. Hybrid Multi-
level solvers for discontinuous Galerkin ﬁnite element discrete ordinate diﬀusion synthetic acceleration of
radiation transport algorithms. Annals of Nuclear Energy, 102(April):134–147, 2017. ISSN 0306-4549. doi:
10.1016/j.anucene.2016.11.048.

[12] Cao Lu, Xiangmin Jiao, and Nikolaos Missirlis. A Hybrid Geometric+Algebraic Multigrid Method
with Semi-Iterative Smoothers. Numerical Linear Algebra with Applications, 21(2):221–238, 2014. ISSN
10991506. doi: 10.1002/nla.1925.

[13] Benedict O’Malley, J´ozsef K´oph´azi, Richard P. Smedley-Stevenson, and Monroe D. Eaton. P-multigrid
expansion of hybrid multilevel solvers for discontinuous Galerkin ﬁnite element discrete ordinate (DG-
FEM-SN) diﬀusion synthetic acceleration (DSA) of radiation transport algorithms. Progress in Nuclear
Energy, 98:177–186, 2017. ISSN 01491970. doi: 10.1016/j.pnucene.2017.03.014.

[14] J¨org Stiller. Nonuniformly weighted Schwarz smoothers for spectral element multigrid. Journal of Scientiﬁc

Computing, 72:81–96, 2016. doi: 10.1007/s10915-016-0345-z.

28

[15] Hari Sundar, George Biros, Carsten Burstedde, Johann Rudi, Omar Ghattas, and Georg Stadler. Parallel
geometric-algebraic multigrid on unstructured forests of octrees. In Proceedings of the International Con-
ference on High Performance Computing, Networking, Storage and Analysis, SC’12, page 43, Salt Lake
City, UT, USA, 2012. IEEE Computer Society Press.

[16] Paola F Antonietti, Marco Sarti, Marco Verani, and Ludmil T Zikatanov. A uniform additive schwarz pre-
conditioner for high-order discontinuous galerkin approximations of elliptic problems. Journal of Scientiﬁc
Computing, 70(2):608–630, 2017.

[17] Martin Kronbichler, Niklas Fehn, Peter Munch, Maximilian Bergbauer, Karl-Robert Wichmann, Carolin
Geitner, Momme Allalen, Martin Schulz, and Wolfgang A Wall. A next-generation discontinuous Galerkin
ﬂuid dynamics solver with application to high-resolution lung airﬂow simulations. In Proceedings of the
International Conference for High Performance Computing, Networking, Storage and Analysis, SC’21,
pages 1–15, St. Louis, MO, USA, 2021. Association for Computing Machinery (ACM).

[18] Thomas C. Clevenger, Timo Heister, Guido Kanschat, and Martin Kronbichler. A ﬂexible, parallel, adaptive
geometric multigrid method for fem. ACM Transactions on Mathematical Software (TOMS), 47(1):1–27,
2020.

[19] Martin Kronbichler and Karl Ljungkvist. Multigrid for matrix-free high-order ﬁnite element computations

on graphics processors. ACM Transactions on Parallel Computing (TOPC), 6(1):1–32, 2019.

[20] Thomas C. Clevenger and Timo Heister. Comparison between algebraic and matrix-free geometric multigrid
for a stokes problem on adaptive meshes with variable viscosity. Numerical Linear Algebra with Applications,
28(5):e2375, 2021. doi: 10.1002/nla.2375.

[21] Martin Kronbichler and Wolfgang A Wall. A performance comparison of continuous and discontinuous
galerkin methods with fast multigrid solvers. SIAM Journal on Scientiﬁc Computing, 40(5):A3423–A3448,
2018.

[22] Hari Sundar, Georg Stadler, and George Biros. Comparison of multigrid algorithms for high-order continu-
ous ﬁnite element discretizations. Numerical Linear Algebra with Applications, 22(4):664–680, 2015. ISSN
10991506. doi: 10.1002/nla.1979.

[23] Daniel Arndt, Wolfgang Bangerth, Denis Davydov, Timo Heister, Luca Heltai, Martin Kronbichler,
Matthias Maier, Jean-Paul Pelteret, Bruno Turcksin, and David Wells. The deal.II ﬁnite element li-
brary: Design, features, and insights. Computers & Mathematics with Applications, 81:407–422, 2021.
ISSN 0898-1221. doi: 10.1016/j.camwa.2020.02.022. URL https://arxiv.org/abs/1910.13247.

[24] Daniel Arndt, Wolfgang Bangerth, Bruno Blais, Marc Fehling, Rene Gassm¨oller, Timo Heister, Luca
Heltai, Uwe K¨ocher, Martin Kronbichler, Matthias Maier, Peter Munch, Jean-Paul Pelteret, Sebastian
Proell, Konrad Simon, Bruno Turcksin, David Wells, and Jiaqi Zhang. The deal.II library, version 9.3.
Journal of Numerical Mathematics, 29(3):171–186, 2021. doi: 10.1515/jnma-2021-0081. URL https:
//doi.org/10.1515/jnma-2021-0081.

[25] Karl Ljungkvist. Matrix-free ﬁnite-element operator application on graphics processing units. In European

Conference on Parallel Processing, pages 450–461. Springer, 2014.

[26] Mark Adams, Marian Brezina, Jonathan Hu, and Ray Tuminaro. Parallel multigrid smoothing: polynomial

versus gauss–seidel. Journal of Computational Physics, 188(2):593–610, 2003.

[27] Magnus Rudolph Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear systems,

volume 49. NBS Washington, DC, 1952.

[28] Achi Brandt. Multi-level adaptive solutions to boundary-value problems. Mathematics of computation, 31

(138):333–390, 1977.

[29] Peter Bastian and Christian Wieners. Multigrid methods on adaptively reﬁned grids. Computing in Science

& Engineering, 8(6):44–54, 2006.

29

[30] Stephen F. McCormick. Multilevel adaptive methods for partial diﬀerential equations. SIAM, 1989.

[31] Mario Storti, N. Nigro, and Sergio Idelsohn. Multigrid methods and adaptive reﬁnement techniques in
elliptic problems by ﬁnite element methods. Computer methods in applied mechanics and engineering, 93
(1):13–30, 1991.

[32] S Lopez and Raﬀaele Casciaro. Algorithmic aspects of adaptive multigrid ﬁnite element analysis. Interna-

tional journal for numerical methods in engineering, 40(5):919–936, 1997.

[33] Anton Sch¨uller. Portable Parallelization of Industrial Aerodynamic Applications (POPINDA): Results of

a BMBF Project, volume 71. Springer Science & Business Media, 2013.

[34] Guido Kanschat. Multilevel methods for discontinuous galerkin fem on locally reﬁned meshes. Computers

& structures, 82(28):2437–2445, 2004.

[35] Jean-Christophe Jouhaud, Marc Montagnac, and Lo¨ıc P. Tourrette. A multigrid adaptive mesh reﬁnement
strategy for 3d aerodynamic design. International journal for numerical methods in ﬂuids, 47(5):367–385,
2005.

[36] B¨arbel Janssen and Guido Kanschat. Adaptive multilevel methods with local smoothing for hˆ1-and hˆcurl-
conforming high order ﬁnite element methods. SIAM Journal on Scientiﬁc Computing, 33(4):2095–2114,
2011.

[37] Roland Becker and Malte Braack. Multigrid techniques for ﬁnite elements on locally reﬁned meshes.

Numerical linear algebra with applications, 7(6):363–379, 2000.

[38] Oleg Iliev and Dimitar Stoyanov. Multigrid-adaptive local reﬁnement solver for incompressible ﬂows. In

International Conference on Large-Scale Scientiﬁc Computing, pages 361–368. Springer, 2001.

[39] Haijun Wu and Zhiming Chen. Uniform convergence of multigrid v-cycle on adaptively reﬁned ﬁnite element
meshes for second order elliptic problems. Science in China Series A: Mathematics, 49(10):1405–1429, 2006.

[40] Wolfgang Bangerth, Carsten Burstedde, Timo Heister, and Martin Kronbichler. Algorithms and data
structures for massively parallel generic adaptive ﬁnite element codes. ACM Transactions on Mathematical
Software, 38(2):1–28, 2011. ISSN 00983500. doi: 10.1145/2049673.2049678.

[41] Carsten Burstedde, Lucas C. Wilcox, and Omar Ghattas. p4est : Scalable Algorithms for Parallel Adaptive
Mesh Reﬁnement on Forests of Octrees. SIAM Journal on Scientiﬁc Computing, 33(3):1103–1133, 2011.
ISSN 1064-8275. doi: 10.1137/100791634.

[42] Douglas Arnold, Richard Falk, and Ragnar Winther. Preconditioning in h(div) and applications. Mathe-

matics of computation, 66(219):957–984, 1997.

[43] Guido Kanschat and Youli Mao. Multigrid methods for hdiv-conforming discontinuous galerkin methods

for the stokes equations. Journal of Numerical Mathematics, 23(1):51–66, 2015.

[44] Roland Becker, Malte Braack, and Thomas Richter. Parallel multigrid on locally reﬁned meshes. In Reactive

Flows, Diﬀusion and Transport, pages 77–92. Springer, 2007.

[45] Peter Munch, Karl Ljungkvist, and Martin Kronbichler. Eﬃcient application of hanging-node constraints
for matrix-free high-order FEM computations on CPU and GPU. ISC High Performance 2022 (accepted
for publication), 2022.

[46] Eugenio Aulisa, Giacomo Capodaglio, and Guoyi Ke. Construction of h-reﬁned continuous ﬁnite element
spaces with arbitrary hanging node conﬁgurations and applications to multigrid algorithms. SIAM Journal
on Scientiﬁc Computing, 41(1):A480–A507, 2019.

[47] Eugenio Aulisa, Sara Calandrini, and Giacomo Capodaglio. An improved multigrid algorithm for n-irregular
meshes with subspace correction smoother. Computers & Mathematics with Applications, 76(3):620–632,
2018.

30

[48] Einar M. Rønquist and Anthony T. Patera. Spectral element multigrid. I. Formulation and numerical

results. Journal of Scientiﬁc Computing, 2(4):389–406, 1987. ISSN 08857474. doi: 10.1007/BF01061297.

[49] Yvon Maday and Rafael Munoz. Spectral element multigrid. II. Theoretical justiﬁcation. Journal of

Scientiﬁc Computing, 3(4):323–353, 1988. ISSN 0885-7474. doi: 10.1007/BF01065177.

[50] Ning Hu and I. Norman Katz. Multi-P Methods: Iterative Algorithms for the P-Version of the Finite

Element Analysis. SIAM Journal on Scientiﬁc Computing, 16(6):1308–1332, 1995.

[51] Ning Hu, Xian-Zhong Guo, and I. Norman Katz. Multi-p Preconditioners. SIAM Journal on Scientiﬁc

Computing, 18(6):1676–1697, 1997.

[52] Xian-Zhong Guo and I. Norman Katz. Performance enhancement of the multi-p preconditioner. Computers

& Mathematics with Applications, 36(4):1–8, 1998.

[53] Xian-Zhong Guo and I. Norman Katz. A Parallel Multi-p Method. Computers & Mathematics with

Applications, 39(9-10):115–123, 2000.

[54] Harold L. Atkins and Brian T. Helenbrook. Numerical Evaluation of P-Multigrid Method for the Solution of
Discontinuous Galerkin Discretizations of Diﬀusive Equations. In 17th AIAA Computational Fluid Dynam-
ics Conference, pages 1–11, Toronto, Ontario Canada, 2005. 17th AIAA Computational Fluid Dynamics
Conference; 6-9 Jun. 2005, American Institute of Aeronautics and Astronautics. ISBN 978-1-62410-053-6.
doi: 10.2514/6.2005-5110.

[55] Brian T. Helenbrook, Dimitri Mavriplis, and Harold L. Atkins. Analysis of “p”-Multigrid for Continuous
and Discontinuous Finite Element Discretizations. In 16th AIAA Computational Fluid Dynamics Confer-
ence, Orlando, Florida, 2003. American Institute of Aeronautics and Astronautics. ISBN 978-1-62410-086-4.
doi: 10.2514/6.2003-3989.

[56] Brian T. Helenbrook and Harold L. Atkins. Application of p-Multigrid to Discontinuous Galerkin Formu-

lations of the Poisson Equation. AIAA Journal, 44(3):566–575, 2006. doi: 10.2514/1.15497.

[57] Brian T. Helenbrook and Harold L. Atkins. Solving Discontinuous Galerkin Formulations of Poisson’s
Equation using Geometric and p Multigrid. AIAA Journal, 46(4):894–902, 2008. ISSN 0001-1452. doi:
10.2514/1.31163.

[58] Brian T. Helenbrook and Brendan S. Mascarenhas. Analysis of Implicit Time-Advancing p-Multigrid
Schemes for Discontinuous Galerkin Discretizations of the Euler Equations. In 46th AIAA Fluid Dynamics
Conference, Washington, D.C., 2016. American Institute of Aeronautics and Astronautics. ISBN 978-1-
62410-436-7. doi: 10.2514/6.2016-3494.

[59] Brendan S. Mascarenhas, Brian T. Helenbrook, and Harold L. Atkins. Application of p-Multigrid to
Discontinuous Galerkin Formulations of the Euler Equations. AIAA Journal, 47(5):1200–1208, 2009. ISSN
0001-1452. doi: 10.2514/1.39765.

[60] Brendan S. Mascarenhas, Brian T. Helenbrook, and Harold L. Atkins. Coupling p-multigrid to geometric
multigrid for discontinuous Galerkin formulations of the convection-diﬀusion equation. Journal of Compu-
tational Physics, 229(10):3664–3674, 2010. ISSN 10902716. doi: 10.1016/j.jcp.2010.01.020.

[61] J¨org Stiller. Robust multigrid for high-order discontinuous Galerkin methods: A fast Poisson solver suitable
for high-aspect ratio Cartesian grids. Journal of Computational Physics, 327:317–336, 2016. doi: 10.1016/
j.jcp.2016.09.041.

[62] J¨org Stiller. Robust Multigrid for Cartesian Interior Penalty DG Formulations of the Poisson Equation
In M. Bittencourt, editor, Spectral and High Order Methods for Partial Diﬀerential Equations
in 3D.
ICOSAHOM 2016. Lecture Notes in Computational Science and Engineering, volume 119, pages 189–
201. Springer, Cham, 2017. ISBN 978-3-319-65869-8 (print); 978-3-319-65870-4 (online). doi: 10.1007/
978-3-319-65870-4 12.

31

[63] Francesco Bassi and Stefano Rebay. Numerical Solution of the Euler Equations with a Multiorder Dis-
continuous Finite Element Method. In S.W. Armﬁeld, editor, Computational Fluid Dynamics 2002, pages
199–204. Springer Berlin Heidelberg, Berlin, Heidelberg, 2003. ISBN 978-3-642-59334-5 (online); 978-3-
642-63938-8 (print). doi: 10.1007/978-3-642-59334-5 27.

[64] Koen Hillewaert, P Wesseling, E O˜nate, J P´eriaux, Jean-Fran¸cois Remacle, Nicolas Cheveaugeon, Paul-
Emile Bernard, and Philippe Geuzaine. Analysis of a hybrid p-multigrid method for the discontinuous
Galerkin discretisation of the Euler equations. In Pieter Wesseling, editor, Proceedings of the European
Conference on Computational Fluid Dynamics, Egmond aan Zee, Netherlands, 2006. ECCOMAS CFD
2006. doi: 90-9020970-0.

[65] Chunlei C. Liang, Ravishekar Kannan, and Z.J. Wang. A p-multigrid spectral diﬀerence method with
explicit and implicit smoothers on unstructured triangular grids. Computers & Fluids, 38(2):254–265,
2009. ISSN 00457930. doi: 10.1016/j.compﬂuid.2008.02.004.

[66] Hong Luo, Joseph D. Baum, and Rainald L¨ohner. Fast p-multigrid discontinuous galerkin method for
compressible ﬂows at all speeds. AIAA Journal, 46(3):635–652, 2008. doi: 10.2514/1.28314. URL https:
//doi.org/10.2514/1.28314.

[67] Hong Luo, Joseph D. Baum, and Rainald L¨ohner. A p-multigrid discontinuous Galerkin method for the
Euler equations on unstructured grids. Journal of Computational Physics, 211(2):767–783, 2006. ISSN
00219991. doi: 10.1016/j.jcp.2005.06.019.

[68] David Darmofal and Krzysztof Fidkowski. Development of a Higher-Order Solver for Aerodynamic Ap-
plications.
In 42nd AIAA Aerospace Sciences Meeting and Exhibit, number January, pages 1–12, Re-
ston, Virigina, 2004. American Institute of Aeronautics and Astronautics. ISBN 978-1-62410-078-9. doi:
10.2514/6.2004-436.

[69] Francesco Bassi, Antonio Ghidoni, Stefano Rebay, and P Tesini. High-order accurate p-multigrid discon-
tinuous galerkin solution of the euler equations. International journal for numerical methods in ﬂuids, 60
(8):847–865, 2009.

[70] Cristian R. Nastase and Dimitri J. Mavriplis. High-order discontinuous Galerkin methods using an hp-
multigrid approach. Journal of Computational Physics, 213(1):330–357, 2006. ISSN 00219991. doi: 10.
1016/j.jcp.2005.08.022.

[71] Sachin Premasuthan, Chunlei Liang, Antony Jameson, and Zhi Wang. A p-Multigrid Spectral Diﬀerence
Method For Viscous Compressible Flow Using 2D Quadrilateral Meshes. In 47th AIAA Aerospace Sciences
Meeting including The New Horizons Forum and Aerospace Exposition, Orlando, Florida, 2009. American
Institute of Aeronautics and Astronautics. ISBN 978-1-60086-973-0. doi: 10.2514/6.2009-950.

[72] Krzysztof J. Fidkowski, Todd A. Oliver, James Lu, and David L. Darmofal. p-Multigrid solution of
high-order discontinuous Galerkin discretizations of the compressible Navier-Stokes equations. Journal of
Computational Physics, 207(1):92–113, 2005. ISSN 00219991. doi: 10.1016/j.jcp.2005.01.005.

[73] A. Ghidoni, A. Colombo, F. Bassi, and S. Rebay. Eﬃcient p -multigrid discontinuous Galerkin solver for
complex viscous ﬂows on stretched grids. International Journal for Numerical Methods in Fluids, 75(2):
134–154, 2014. ISSN 02712091. doi: 10.1002/ﬂd.3888.

[74] Khosro Shahbazi, Dimitri J. Mavriplis, and Nicholas K. Burgess. Multigrid algorithms for high-order dis-
continuous Galerkin discretizations of the compressible Navier-Stokes equations. Journal of Computational
Physics, 228(21):7917–7940, 2009. ISSN 00219991. doi: 10.1016/j.jcp.2009.07.013.

[75] Zhenhua Jiang, Chao Yan, Jian Yu, and Wu Yuan. Practical aspects of p-multigrid discontinuous Galerkin
solver for steady and unsteady RANS simulations. International Journal for Numerical Methods in Fluids,
78(11):670–690, 2015. ISSN 10970363. doi: 10.1002/ﬂd.4035.

[76] Jed Brown. Eﬃcient nonlinear solvers for nodal high-order ﬁnite elements in 3D. Journal of Scientiﬁc

Computing, 45(1-3):48–63, 2010. ISSN 08857474. doi: 10.1007/s10915-010-9396-8.

32

[77] Brian T Helenbrook and Harold L. Atkins. Solving discontinuous galerkin formulations of poisson’s equation

using geometric and p multigrid. AIAA journal, 46(4):894–902, 2008.

[78] Mark S Shephard. Linear multipoint constraints applied via transformation as part of a direct stiﬀness

assembly process. International Journal for Numerical Methods in Engineering, 20(11):2107–2112, 1984.

[79] Wolfgang Bangerth and Oliver Kayser-Herold. Data structures and requirements for hp ﬁnite element

software. ACM Transactions on Mathematical Software (TOMS), 36(1):1–31, 2009.

[80] Pamela Zave and Werner C. Rheinboldt. Design of an adaptive, parallel ﬁnite-element system. ACM Trans.

Math. Softw., 5(1):1–17, 1979.

[81] Robert Anderson, Julian Andrej, Andrew Barker, Jamie Bramwell, Jean-Sylvain Camier, Jakub Cerveny,
Veselin Dobrev, Yohann Dudouit, Aaron Fisher, Tzanio Kolev, Will Pazner, Mark Stowell, Vladimir Tomov,
Ido Akkerman, Johann Dahm, David Medina, and Stefano Zampini. MFEM: A modular ﬁnite element
methods library. Computers & Mathematics with Applications, 81:42–74, 2021. doi: 10.1016/j.camwa.2020.
06.009. URL https://doi.org/10.1016/j.camwa.2020.06.009.

[82] Karl Ljungkvist. Matrix-free ﬁnite-element computations on graphics processors with adaptively reﬁned

unstructured meshes. In SpringSim (HPC), 2017.

[83] Tzanio Kolev, Paul Fischer, Misun Min, Jack Dongarra, Jed Brown, Veselin Dobrev, Tim Warburton,
Stanimire Tomov, Mark S Shephard, Ahmad Abdelfattah, et al. Eﬃcient exascale discretizations: High-
order ﬁnite element methods. The International Journal of High Performance Computing Applications, 35
(6):527–552, 2021.

[84] Steﬀen M¨uthing, Marian Piatkowski, and Peter Bastian. High-performance implementation of matrix-free

high-order discontinuous galerkin methods. arXiv preprint arXiv:1711.10885, 2017.

[85] Martin Kronbichler and Katharina Kormann. A generic interface for parallel cell-based ﬁnite element
operator application. Computers & Fluids, 63:135–147, 2012. ISSN 00457930. doi: 10.1016/j.compﬂuid.
2012.04.012.

[86] Martin Kronbichler and Katharina Kormann. Fast matrix-free evaluation of discontinuous galerkin ﬁnite
element operators. ACM Transactions on Mathematical Software, 45(3):1–40, 2019. doi: 10.1145/3325864.
URL https://doi.org/10.1145/3325864.

[87] J. Markus Melenk, Klaus Gerdes, and Christoph Schwab. Fully discrete hp-ﬁnite elements: fast quadra-
ture. Computer Methods in Applied Mechanics and Engineering, 190(32):4339–4364, 2001.
ISSN 0045-
7825. doi: 10.1016/S0045-7825(00)00322-4. URL http://www.sciencedirect.com/science/article/
pii/S0045782500003224.

[88] Steven A. Orszag. Spectral methods for problems in complex geometries. Journal of Computational Physics,
ISSN 00219991. doi: 10.1016/0021-9991(80)90005-4. URL http://linkinghub.

37(1):70–92, 1980.
elsevier.com/retrieve/pii/0021999180900054.

[89] Daniel Arndt, Wolfgang Bangerth, Denis Davydov, Timo Heister, Luca Heltai, Martin Kronbichler,
Matthias Maier, Jean-Paul Pelteret, Bruno Turcksin, and David Wells. The deal. ii library, version 8.5.
Journal of Numerical Mathematics, 25(3):137–145, 2017.

[90] Torsten Hoeﬂer, Christian Siebert, and Andrew Lumsdaine. Scalable communication protocols for dynamic

sparse data exchange. ACM Sigplan Notices, 45(5):159–168, 2010.

[91] Daniel Arndt, Wolfgang Bangerth, Bruno Blais, Thomas C. Clevenger, Marc Fehling, Alexander V.
Grayver, Timo Heister, Luca Heltai, Martin Kronbichler, Matthias Maier, Peter Munch, Jean-Paul Pel-
teret, Reza Rastak, Ignacio Tomas, Bruno Turcksin, Zhuoran Wang, and David Wells. The deal.II library,
version 9.2. Journal of Numerical Mathematics, 28(3):131–146, 2020. doi: 10.1515/jnma-2020-0043. URL
https://doi.org/10.1515/jnma-2020-0043.

33

[92] Michael W Gee, Christopher M Siefert, Jonathan J Hu, Ray S Tuminaro, and Marzio G Sala. Ml 5.0
smoothed aggregation user’s guide. Technical report, Technical Report SAND2006-2649, Sandia National
Laboratories, 2006.

[93] Robert D Falgout, Jim E Jones, and Ulrike Meier Yang. The design and implementation of hypre, a library
In Numerical solution of partial diﬀerential equations on

of parallel high performance preconditioners.
parallel computers, pages 267–294. Springer, 2006.

[94] M. Kronbichler, T. Heister, and W. Bangerth. High accuracy mantle convection simulation through modern
numerical methods. Geophysical Journal International, 191:12–29, 2012. doi: 10.1111/j.1365-246X.2012.
05609.x. URL http://dx.doi.org/10.1111/j.1365-246X.2012.05609.x.

[95] Timo Heister, Juliane Dannberg, Rene Gassm¨oller, and Wolfgang Bangerth. High accuracy mantle con-
vection simulation through modern numerical methods. II: Realistic models and problems. Geophysical
Journal International, 210(2):833–851, 2017. doi: 10.1093/gji/ggx195. URL https://doi.org/10.1093/
gji/ggx195.

34

