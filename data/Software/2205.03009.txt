2
2
0
2

y
a
M
6

]

R
C
.
s
c
[

1
v
9
0
0
3
0
.
5
0
2
2
:
v
i
X
r
a

Watching the watchers: bias and vulnerability in remote proctoring software

Ben Burgess
Princeton University

Avi Ginsberg
Georgetown Law

Edward W. Felten
Princeton University

Shaanan Cohney
University of Melbourne

Abstract
Educators are rapidly switching to remote proctoring and
examination software for their testing needs, both due to
the COVID-19 pandemic and the expanding virtualiza-
tion of the education sector. State boards are increasingly
utilizing these software for high stakes legal and medical
licensing exams. Three key concerns arise with the use
of these complex software: exam integrity, exam proce-
dural fairness, and exam-taker security and privacy.

We conduct the ﬁrst technical analysis of each of these
concerns through a case study of four primary proctoring
suites used in U.S. law school and state attorney licensing
exams. We reverse engineer these proctoring suites and
ﬁnd that despite promises of high-security, all their anti-
cheating measures can be trivially bypassed and can pose
signiﬁcant user security risks.

We evaluate current facial recognition classiﬁers
alongside the classiﬁer used by Examplify, the legal
exam proctoring suite with the largest market share, to
ascertain their accuracy and determine whether faces
with certain skin tones are more readily ﬂagged for
cheating. Finally, we offer recommendations to improve
the integrity and fairness of the remotely proctored exam
experience.

1

Introduction

The rapid adoption of proctoring suites (software for
monitoring students while they take exams remotely) is
a phenomenon precipitated by the pandemic [27]. Sub-
sequently, we ﬁnd high stakes exams being administered
remotely. Recent media coverage provides anecdotal evi-
dence that these proctoring suites introduce security con-
cerns, harm procedural fairness, and pose risk to exam
integrity [28, 32]. In this work, we aim to investigate the
anecdotal claims and systematize the study of these soft-
ware. We evaluate the proctoring software on its tech-
nical merits using traditional reverse engineering tech-
niques and perform a fairness analysis.

Historically, computerized test taking beneﬁted from
in-person proctors and institution controlled hardware.
In the remote test-taking setting such proctors are un-
available so institutions resort to utilizing proctoring
suites. Proctoring suites attempt to mitigate the in-
creased risks of cheating associated with the remote en-
vironment and student hardware by installing pervasive,
highly privileged services on students’ computers. Such
software attempts to reduce academic misconduct during
exams by limiting access to online resources and system
functions, such as the ability to paste in pre-written ma-
terials. Considerable privacy concerns arise since a stu-
dent’s laptop is not generally a single use device that only
contains class related material. A student using their own
hardware faces the risk of their personal information be-
ing misused or leaked by the proctoring software.

Initially,

the remote proctoring software was mar-
keted and designed for tertiary institutions, however, the
software has recently been adopted for medicine and
law licensing exams.
Inherent societal costs to illegit-
imately passing students are magniﬁed in both profes-
sions, where an inept lawyer can put an individual’s lib-
erty at stake and an incompetent physician can cause sig-
niﬁcant trauma to patients. The time and monetary bur-
den of professional education and licensing places ex-
treme pressure on students and this, along with the ben-
eﬁts of passing, increases the extent to which students
may be willing to risk cheating. Maintaining conﬁdence
that degrees earned and licenses obtained ensure a min-
imum degree of competency and knowledge is impera-
tive. Equally important is the conﬁdence that no indi-
vidual who merits entrance into a profession has been
blocked due to false cheating allegations. Applied re-
search into whether remote exam proctoring puts either
of these interests in jeopardy is lacking in current liter-
ature and merits attention from the security and privacy
community.

We conduct the ﬁrst systematic, technical analysis of
the remote proctoring ecosystem of law school and state

 
 
 
 
 
 
bar exam boards. By limiting the scope of our investiga-
tion to the single regulated profession of law, we can map
out how the software is used across the entire profession,
increasing conﬁdence in our proposed recommendations.
We do not, however, have reason to suspect that apply-
ing our methodology to other examination settings would
yield substantially different results.

Through public data aggregation and survey, four
exam proctoring suites–—Examplify, ILG Exam360,
Exam4, and Electronic Blue Book—–utilized in 93% of
U.S. law schools and 100% of remote state bar exams
are identiﬁed. Reverse engineering of these four proctor-
ing suites reveals vulnerabilities of varying complexity
that compromise the purportedly secure testing environ-
ments. We evaluate suites in the context of three potential
adversaries: a law student; a law student with computer
science experience; and an experienced reverse engineer
and discuss vulnerabilities identiﬁed with each. We ﬁnd
the majority of proctoring suites analyzed install a highly
privileged system service that has full access to a user’s
activities. Highlighting the privacy trade-off of this ser-
vice, we ﬁnd that system logs created before an exam
begins are nonetheless transmitted by the software to the
vendor’s servers during the exam.

We determine that Examplify implements a facial
recognition classiﬁer to authenticate a student against a
pre-existing photograph prior to starting an exam. The
classiﬁer is then re-run repeatedly during the exam de-
pending on the settings selected by the exam administra-
tor. At each interval, the classiﬁer attempts to determine
whether the student initially authenticated is the student
who continues taking the exam. We extract the name of
the facial recognition system Examplify is using, ‘face-
api.js’, and note they employ the pretrained models that
are publicly available on the face-api’s GitHub. We test
these models against current off-the-shelf state-of-the-
art (SOTA) classiﬁers across several different datasets to
evaluate whether one is more accurate than the other but
ﬁnd signiﬁcant accuracy concerns across the board.

We then evaluate the classiﬁers across subjects of dif-
ferent races and ﬁnd concerning variability. We investi-
gate proctoring suite terms of service and user interface
to shed light on whether a user is giving informed con-
sent to all the monitoring.

We conclude with privacy protecting recommenda-
tions for remote exam proctoring administrators and stu-
dents and provide vendors with suggestions to improve
exam integrity while lessening the student privacy im-
pact.

Contributions

• We survey the top 180 law schools and all U.S. state
bar associations to determine their remote proctor-

ing practices (dataset to be released upon publica-
tion) (Section 3.3).

• We reverse engineer four exam proctoring suites
and identify the security the proctoring suite pro-
vides the institution and its student privacy ramiﬁ-
cations (Section 4).

• We release a tool for automatically and rapidly ex-
tracting key security and privacy properties of exam
suite software.

• We perform a detailed evaluation of racial biases
in the facial recognition model used in the software
with the dominant market-share for remote proctor-
ing in law schools (Section 5).

While we acknowledge that the attack techniques we
use are in general, well known, their application to the
particular high-stakes context merits a comprehensive
analysis.
Research Ethics & Limitations

While our survey involved contacting law schools and
state bar associations to determine what platforms they
use and how they use them, our work was exempt from
IRB review as we did not collect data pertaining to indi-
viduals.

Our analysis of facial recognition systems used a data
set containing images of incarcerated individuals who
were not given an opportunity by the original researcher
to consent to its use for research. We consulted with an
applied ethicist independent from the project and deter-
mined that while we are unable to rectify the consent
issue to fully uphold the principle of autonomy, use of
these images is nonetheless appropriate as our work ful-
ﬁlls the principle of beneﬁcence in the following manner:

• Our work aims to aid marginalized groups and
dis-empowered individuals by evaluating the pri-
vacy/bias concerns of software that powerful orga-
nizations require students to use.

• Our work does not cause additional harm to the in-
dividuals in our dataset, beyond perpetuating its use
in academic literature.

Thus, while we are unable to uphold the principle of
autonomy to the greatest extent, we believe our research
is nonetheless appropriate.

Our analysis of facial recognition systems focuses on
racial biases in algorithms, despite evidence that system
performance is more closely tied to skin-tone with race
as a proxy. However, as our very large reference data
sets are racially coded rather than coded by skin tone,
a more sophisticated distinguishing analysis was outside
our scope.

2

We restrict ourselves to the law and regulated profes-
sions sub-sector of education, centering our work on a
limited set of products to more comprehensively cover
our chosen area and produce timely research in light of
the current social need.

We intentionally refrain from evaluating server side
components and functionality of the software packages
to avoid the accompanying legal and ethical concerns.
We conducted responsible disclosure to the proctoring
suite vendors to make them aware of the vulnerabilities
we found along with potential remediations they could
take. We believe this mitigates any potential harm dis-
closing these vulnerabilities may have on exam integrity.

2 Related Work

Several previous studies [22, 38] have discussed the
different threat model remote proctoring solutions face
and provided recommendations for security features that
could mitigate these new vulnerabilities such as im-
proved authentication measures or 360 degree cameras.
Slusky [34] extended this by investigating the security
feature claims of 20 different exam proctoring suites,
noting their strengths and weaknesses against various
threat models. Teclehaimanot, E T A L. [36] studied eight
John Madison University professors and determined that
a signiﬁcant number of their students seemingly gained
an advantage on remotely proctored exams despite the
use of a proctoring suite. Cohney, E T A L. [9] performed
a multidisciplinary analysis of the risks faced by institu-
tions as they rapidly adopt EdTech in light of COVID-19.
A few studies attempted to quantify how a student’s
perceived stress and performance varies between remote
and in person exam environments. Karim, E T A L. [20]
studied 582 subjects taking a cognitive test in each en-
vironment and found the scores were similar between
the two groups but that subjects in the remote setting in-
dicated a higher perceived stress level. Teclehaimanot,
E T A L. [36] performed a meta analysis of test scores
recorded in proctored and unproctored environments,
ﬁnding a 0.20 STD variation between the two sets.

Teclehaimanot, E T A L. [36] conducted a survey of
eight experts from different universities with previous
experience using remote exam proctoring solutions and
found the perceived trust of the vendor and the security
of the offering to be the primary factors inﬂuencing their
solution adoption decision. The recent work of Balash,
E T A L. [4] presented an in depth user-study of student
responses to remote proctoring software in light of the
pandemic. Barrett [5] discusses the risks of adopting re-
mote proctoring software in terms of bias, test-taker anx-
iety, and student privacy.

Singh, E T A L. [33] extended this by creating targeted
attacks to cause false positives by the classiﬁer. We do
not investigate these attacks in the context of Examplify’s
classiﬁer but anticipate no reason they would not be ap-
plicable. Nagpal, E T A L. [25] evaluated several differ-
ent machine learning classiﬁers using the Multi-PIE and
Morph-II datasets and found signiﬁcant racial biases. We
closely structure our methodology for detecting biases in
Examplify’s classiﬁer to this work. The National Insti-
tute of Standards and Technology (NIST) conducted the
Face Recognition Vendor Test (FRVT) to quantify face
recognition accuracy at a very large scale [17], providing
methodological expertise that guides our work.

Perhaps more importantly, teachers and students have
documented their concerns in increasing number since
the start of the pandemic. Students have performed their
own small scale experiments testing various proctoring
suites [14, 19] while teachers have voiced misgivings
[35] about use of facial recognition.

3 Preliminaries

Computerized exams coupled with the need for remote
testing have prompted increased reliance on proctor-
ing software, inclusive of facial recognition classiﬁers,
bringing concerns over privacy, security, racial bias, and
fairness to the forefront.

3.1 Exam Software

Computerized exam software generally consists of a user
interface that displays multiple choice questions with an-
swers and/or text boxes for student answer entry, cou-
pled with a series of cheating detection and deterrence
tools tailored to meet the assumed threat model. The
general assumption: students cheat by searching the in-
ternet, opening documents/programs on their computers,
or consulting a device, person, or printed material during
the exam. To this end, exam software generally block ac-
cess to the internet and non-approved applications on the
user’s machine; perform audio recordings to detect ver-
bal communication with another person; and run facial
recognition to ensure the appropriate individual takes the
entire exam and does so without looking away to consult
another information source.

Facial recognition is a particularly problematic aspect
of exam software given general concerns with the accu-
racy of such models across different skin-tones and face
structures. As a result, individuals from certain groups
may be ﬂagged more frequently for potential cheating—
a classic case of ‘disparate impact’.

Previous studies have demonstrated biases in the dif-
ferent components of facial recognition systems [23, 39].

As a highly regulated industry, law pays particular at-
tention to ethical and professional standards such that

3

when a student cheats within law school or during a li-
censing exam, there is a high likelihood that the accredit-
ing organization will bar them (potentially permanently)
from practice. We infer that these high standards within
the profession affect decisions made regarding exam ad-
ministration and the choice of what remote proctoring
software platform to use, if any.

3.2 Legal Education

When we model the threats to law school and bar exams,
we take into account the structure of testing in the le-
gal profession and the nature of the exams themselves—
which differ substantially from many other ﬁelds. Law
school course grades depend primarily on a single,
tightly curved ﬁnal exam usually worth at least 85% of
the course grade. Law school grades, and thus exams, are
closely tied to job prospects (more-so than many other
ﬁelds) [6] and bar exam scores are a determining fac-
tor in lawyer licensing. These factors, combined with
the high-debt burden associated with U.S. legal educa-
tion, place extreme pressure on students, often provid-
ing strong motivation for unscrupulous students to cheat,
even by means of paying outside individuals for mate-
rials, technical bypasses, or cheating tools. Some law
courses utilize multiple choice exams, but more often
law exams take the format of a story riddled with legal
issues that a student must identify and analyze. Exam
answers often consist of several pages of written text so
that cheating by simply copying another student’s answer
would be painfully obvious. Student cheating by attain-
ing/predicting exam content in advance to pre-write an-
swers or using messenger apps during the exam to con-
sult friends comprise the likely, albeit more difﬁcult to
detect, form of dishonesty.

COVID-19 has caused bar associations and schools
to rapidly adopt an at home remote testing model, forc-
ing administrators to look for additional assurances that
cheating attempts will be detected.

3.3 Software Usage Survey

To determine the targets for our technical work, we
survey software usage across bar associations and law
schools. We identify the remote proctoring software used
by the top 180 law schools by scraping public facing
websites and making private inquiries to law school ad-
ministrations. Table 1 depicts our survey results. We
repeat this process for every state bar association’s li-
censing exam and illustrate the results in Figure 1. Our
data set with the individual school and state bar re-
mote proctoring suite adoption choices is available at
github.com/WWP22/ProctoringSuiteAdoption. Through
this process we ﬁnd that Examplify, ILG Exam360,

Exam Proctoring Suite

Schools Percentage

Examplify
Exam4
Electronic Blue Book
Canvas
MyLaw
ILG Exam360
Other

99
52
13
4
3
1
5

55%
29%
7.2%
2.2%
1.7%
0.56%
4.5%

Table 1: Examplify leads in market share followed by
Exam4 and Electronic Blue Book. Two schools did not
respond to inquiries and one closed down.

Exam 4, and Electronic Blue Book comprise the four
primary exam software suites used by over 93% of law
schools and 100% of bar exam associations. We select
these for analysis in the remainder of this work.

4 Cheating

The uncontrolled setting and hardware of the remote en-
vironment makes for a broad threat model. To evaluate
the security provided by these exam proctoring suites we
present a reverse engineering methodology and propose
three theoretical but realistic adversaries to a remotely
proctored law exam. Practical compromises to the se-
curity features we identify along with potential attacks
from each of these adversaries are discussed.

4.1 Methodology

The four exam suites analyzed are not open source,
so we reverse engineer the binaries using existing
static/dynamic analysis tools. We focus on three ques-
tions: (1) Do the exam suites provide the security guar-
antees they promise? (2) What privacy and security is
the user required to give up to achieve these guarantees?
and (3) Are the exam integrity checks performed fairly
across all examinees?

We isolate suspected critical functions using com-
mon reverse engineering methods such as system log in-
spection and recording of user interface dialogues. We
then use traditional disassembly and reverse engineering
workﬂows to manually inspect binary areas of interest.
We employed dynamic analysis to identify and describe
functionality that was not apparent through the static
analysis, disabling anti-debugging functionality when
present in the software.

To evaluate the integrity of exams in transit, we use
standard network interposition techniques to evaluate
whether the connection to retrieve the exam is over an
encryption TLS connection or in plaintext. If the connec-

4

conﬁgure hardware devices, modify drivers, use custom
encryption tools, and extract encryption keys from and
modify the control ﬂow of the binary is assumed. While
we expect the number of students with this background
is low, such individuals may sell their services or cracked
versions of software.

4.3 Reverse Engineering Findings

In this subsection we detail our ﬁndings regarding how
proctoring suites operate and discuss each suite’s secu-
rity features and implementation.

Exam proctoring suites use a few distinct components
to ensure exam integrity is maintained: computer mon-
itoring, exam content protection, and identity veriﬁ-
cation and authentication. We categorize our ﬁndings
based on these components for the following four proc-
toring suites: Examplify, Exam 4, Electronic Blue Book,
and ILG Exam360.

4.3.1 Computer Monitoring

The computer monitoring components in an exam suite
aim to prevent a student from accessing unauthorized re-
sources during the exam and may even restrict the stu-
dent from beginning the exam if certain parameters are
not met. A summary of the features used by each suite
is provided in Appendix A. The features we identify as
comprising the computer monitoring component of exam
integrity are detailed with each exam suite’s feature im-
plementation outlined:
Virtual Machine Detection A student running the proc-
toring software inside a virtual machine environment can
easily exit the environment with a hotkey and completely
evade any monitoring the exam proctoring suite hoped to
provide. To prevent this, most suites feature virtual ma-
chine detection to detect and prevent attempts to run the
software inside a virtualized container. The most com-
mon implementation of this is a simple check of the com-
puter’s CPU manufacturer to see if the string matches
known virtual machine software vendors. An additional
check of CPU temperature for constant values or known
presets can be run since a virtual CPU does not often pass
through the real value from the actual CPU.

All of the exam suites we examined implement a vir-
tual machine check by comparing the CPU vendor ﬁeld
against a list of known virtual machine vendors. Exam-
plify extends this by retrieving the CPU temperature and
ﬂagging a device if it reports a CPU temperature of 100C
as this is the typical default value virtual machine ven-
dors use. Electronic Blue Book also checks the com-
puter, hard drive, network adapter, and bios vendor in-
formation to see if any ﬁelds contain the string ‘virtual’.
If a virtual machine is detected, they log the attempt and

Figure 1: The adoption of remote exam proctoring suites
by state bar exam associations across the United States.

tion is over TLS, we evaluate the program’s response to
being served: (1) a valid certiﬁcate that has an incorrect
common name; and (2) a self signed certiﬁcate that is not
recognized by any certiﬁcate authority but that bears the
correct common name. We also attempt to force a down-
grade by blocking access to the port before the handshake
can occur and then watching for further plaintext retries
on a separate port.

For readability, we defer exam fairness methodology

to Section 5 due to its increased depth.

4.2 Threat Model for Exam Proctoring

We informally model three adversaries likely to interfere
with the fair and secure operation of a remotely proctored
law exam. We emphasize that in reality, many students
may be appropriately modelled as a more sophisticated
adversary than their background would suggest by hir-
ing help. Attack budgets ranging from a few to several
thousands of dollars are feasible, given the hundreds-of-
thousands of dollars spent on law school. We deﬁne three
adversaries:
Law Student An individual able to adjust basic sys-
tem and ﬁle settings, conﬁgure simple hardware devices,
and use known passwords but with no experience in re-
verse engineering software, programming, modifying bi-
nary settings, or extracting encryption keys.
Law Student with CS Background An individual with
signiﬁcant programming experience, familiarity with ba-
sic system administration, and the knowledge to extract
keys from the binary but with no ability to modify any
portion of the binary and without extensive experience
reverse engineering software.
Experienced Reverse Engineer An individual with
all the prior capabilities with additional experience ana-
lyzing and rebuilding binaries, disassembling software,
and applying patches. Familiarity with advanced sys-
tem administration and the ability to: adjust any setting,

5

prompt the user to run the software outside of a virtual
machine. Appendix B provides a summary of the popu-
lar virtual machine software blocked by each proctoring
suite.
Virtual Webcam/Microphone Detection Virtual web-
cam and microphone programs exist that allow a user to
generate a virtual device which either returns data from
a remote endpoint or from a ﬁle. Adversaries could by-
pass exam suite identity veriﬁcation by returning video
of themselves in another location while someone takes
the exam for them or by returning a prerecorded video of
them taking the exam. Exam proctoring suites attempt
to mitigate this threat by checking the device vendor and
bus location against a list of ﬂagged vendors. If one of
these blocked devices is detected, the software will either
ﬂag the exam for further review or prevent the student
from beginning to take it.

Examplify and ILG Exam360 detect virtual webcams
and microphones by retrieving the operating system’s de-
vice list and comparing it to known virtual device ven-
dors. Exam4 and Electronic Blue Book do not use the
computer’s webcam or microphone so they don’t im-
plement a check. Appendix C provides an overview of
the popular virtual webcam/microphone vendors on the
blocked list.
Clipboard Management Students copying pre-written
text into an exam is a major concern, especially in the
ﬁeld of law where exam essay answers may require
lengthy summaries of law and/or analysis that can be pre-
pared before the exam. Exam proctoring suites attempt
to prevent this by either clearing the clipboard before the
exam or logging its contents for later analysis. During
the exam the clipboard generally can be used inside the
proctoring suite but copying from outside apps is prohib-
ited or logged using a similar method.

The exam proctoring suites implement clipboard pro-
tection by calling the system clear function. The content
is not captured before the clear operation by any of the
suites. Exam4 and ILG Exam360 implement a custom
restricted clipboard for use inside the test environment
which limits what can be copied.
Screenshot Capture Exam proctoring suites may of-
fer screenshots of the student’s screen during the exam
to allow a proctor to retroactively review the exam ses-
sion to determine if unauthorized resources were ac-
cessed on the computer. These screenshots are normally
captured using a highly privileged system level service
which leads to potential privacy issues when an exam is
not in progress.

ILG Exam360 is the only exam suite we analyzed that
provides screenshot captures of the student’s computer
during an exam. The screenshot is captured by calling
their highly privileged system service using a Javascript
call which uploads the screenshot to Exam360.

Process/Application Restrictions Process restrictions
are normally used to limit what applications a student
can access during an exam. These are generally imple-
mented using a process allow list that contains processes
speciﬁcally allowed by the exam in addition to critical
processes the operating system needs to maintain the
computer’s function. A weaker implementation involves
using process block lists that prevent certain processes
such as web browser activation from being started. Both
approaches are implemented using the exam proctoring
suite’s highly privileged system service which starts a
watchdog service that forcibly kills unauthorized pro-
cesses.

Examplify and ILG Exam360 compare the processes
currently running on the system to a list of processes they
and the exam itself allow, continuously monitoring for
and forcibly killing any processes that are running but not
included on the list. Exam4 and Electronic Blue Book
have a list of disallowed services which they kill upon
exam commencement.
Network Interception Closed book exams require the
software to limit a student’s ability to search for informa-
tion on the internet. Approaches to block internet access
that we found included: dropping internet trafﬁc, insert-
ing an active man in the middle to capture trafﬁc, or redi-
recting the trafﬁc to the vendor’s servers. The simplest
approach is dropping the trafﬁc using a routing rule.

Examplify restricts network trafﬁc by inserting a null
route into the default operating system routing table.
Exam4 disables the network adapter during the exami-
nation. ILG Exam360 inserts a null DNS entry into the
network conﬁguration causing domain name resolution
failure. Electronic Blue Book does not implement any
network restrictions other than blocking access to com-
mon websites through their process block list. None of
the implementations inspected capture browser trafﬁc or
redirect it to a site controlled by the suite vendor.

4.3.2 Exam Content Protection

Exams are often downloaded to student computers be-
fore the actual exam begins making security during tran-
sit between proctoring suite vendor servers and students
paramount since trafﬁc can be easily intercepted using
off the shelf solutions. Exam protection during the down-
load and while it sits on the student’s computer is vital to
prevent early or unauthorized access. We detail below
our ﬁndings for each of the exam suites for both encryp-
tion at rest and in transit.
Encryption In Transit Examplify and ILG Exam360
use transport layer security (TLS) for all of their connec-
tions. The certiﬁcate chain, expiration date, and common
name are correctly veriﬁed which mitigates active man
in the middle attacks. The connection is never down-

6

graded to plaintext HTTP even if the software is unable
to successfully complete the handshake. Examplify in-
cludes their own certiﬁcate store inside the software to
prevent potentially using a modiﬁed system certiﬁcate
store. Exam4 and Electronic Blue Book allow the in-
dividual institution to select their transport layer security
settings. Electronic Blue Book allows each institution to
conﬁgure whether to use TLS or not, and the conﬁgura-
tions are available publicly. We found several that failed
to enable TLS. Exam4 similarly features per-school con-
ﬁguration, but the conﬁgurations are not public. How-
ever, the school from whom we obtained the Exam4 bi-
nary did not have TLS enabled.
Encryption At Rest
Examplify, ILG Exam360, and
Exam4 use AES-256 for encrypting exams at rest on stu-
dent computers. ILG Exam360 and Exam4 use SHA1
to derive the AES key from a password. Examplify uses
10,000 iterations of the PBKDF2 function to derive the
exam password. The exam manifest which contains the
main exam password salt, exam information, allowed re-
sources, and monitoring settings is encrypted separately
using a static key stored in the Examplify binary. Elec-
tronic Blue Book allows the institution to choose be-
tween 3DES and AES for exam encryption and uses
1,000 iterations of SHA1 by default for password deriva-
tion, but the institution can conﬁgure the iteration count.
The password salt is statically stored in the Electronic
Blue Book binary and is set to ‘Kosher’.

4.3.3 Identity Veriﬁcation and Authentication

Exam suites all implement some form of user authenti-
cation to ensure the test taker matches the individual to
be assessed.
Logins Exam4, ILG Exam360, and Electronic Blue
Book implement standard single factor logins. Exam-
plify implements a similar single factor login. OAuth
is not supported by any of these solutions so institu-
tions cannot easily add more extensive identity veriﬁca-
tion measures such as two factor veriﬁcation.
General Interaction Fingerprinting General interac-
tion ﬁngerprinting analyzes the pattern of a student’s key
strokes and mouse movements against the class average
for anomalies and if present, ﬂags the exam for human
proctor review. This poses the risk of potentially un-
fairly ﬂagging students with disabilities or those who le-
gitimately deviate from the class average pattern. While
none of the suites we analyzed use this, it is used by Proc-
torio, a common proctoring suite outside our scope.
Facial Recognition As an analog to students showing
ID upon entering an exam hall, some exam proctoring
suites employ facial recognition for identity veriﬁcation
to counter another person taking the exam for a student.
The distance of the facial feature vectors of a student’s

image are compared against those of the student’s trusted
reference image and if below a certain threshold, the stu-
dent is considered veriﬁed.

ILG Exam360 offers facial recognition but also em-
ploys a remote human veriﬁcation method before exam
initiation utilizing a webcam to connect a student with
a human proctor who conducts the ﬁnal veriﬁcation. In
the end, the process resembles that of an exam hall. Ex-
amplify’s veriﬁcation implementation relies on an auto-
mated facial recognition classiﬁer. Our research quan-
tiﬁes bias introduced by Examplify’s process which we
detail in Section 5.3. Exam4 and Electronic Blue Book
do not offer facial recognition.

4.4

Impact Analysis

To evaluate the impact remote exam proctoring has on
exam integrity, we must determine whether its secu-
rity features are effective against expected adversaries;
whether privacy concerns are generated by these fea-
tures; and whether these features introduce bias into the
exam taking process.

4.4.1 Security Feature Vulnerabilities

Proctoring suite security features have various vulnera-
bilities depending on the expertise of the adversary. We
validate the attacks referenced in this section against the
proctoring suite in a simulated test environment to ensure
they can actually be conducted.
Virtual Machines Virtual machine software allows a
guest operating system to be run inside the primary envi-
ronment, bypassing any monitoring the proctoring suite
could hope to achieve. The CPU vendor check imple-
mented by all of the exam suites can be easily bypassed
using common virtual machine software since this ﬁeld
is generally conﬁgurable. The CPU temperature check
Examplify conducts is defeated by conﬁguring the vir-
tual machine to pass through the CPU temperature of the
actual computer’s CPU vendor or a random string not on
the block list the exam proctoring suites are using. A
law student with a CS background, capable of installing
a virtual machine and conﬁguring the CPU vendor, could
readily complete this attack.
Virtual Webcams/Microphones Virtual webcam and
microphone devices can allow adversaries to take exams
outside an appropriate physical context by replaying a
prerecorded ﬁle or piping a connection to a separate de-
vice. The only mitigation employed to prevent virtual de-
vices is a driver list search which looks for known virtual
device vendors along with the running process restric-
tions. A student can evade detection easily by installing a
virtual driver not on the known vendor list or more skill-
fully by renaming the driver of a blocked virtual device

7

since the signing key of the driver is not checked by the
exam proctoring suite. All these attacks are well within
the capabilities of a student with a CS background. A
student able to create a custom virtual device that mas-
queraded as a legitimate driver could redistribute this to
other students.
Clipboard Protection
Exam proctoring suites pro-
tect against clipboard content being carried into the exam
by calling the operating system’s clear function before
the exam begins. This does not preclude the use of an
external hardware connected clipboard such as a KVM
or a built in keyboard macro which allows note storage.
The majority of these hardware connected devices sim-
ply present as standard hardware interface devices which
don’t require any additional drivers. Exam proctoring
suites could attempt to protect against clipping and past-
ing clipboard content by ﬁngerprinting the input rate of
a student’s keystrokes. Purchasing a hardware device ca-
pable of maintaining an external clipboard is an attack
most law students could perform. We do not investigate
the Mac version of the exam proctoring suites but collo-
quial evidence suggests that the iCloud clipboard shar-
ing might bypass these protections by loading informa-
tion from the phone’s clipboard into the Mac keyboard
through the service during the exam.
Process Restrictions
Process restrictions are imple-
mented either with a list of allowed processes or with a
list of disallowed processes that are killed upon starting
the exam. To subvert either restriction, a student with
CS background could recompile a piece of open source
software to report a different process name. As an ex-
ample, Chromium could be recompiled to report as ‘ex-
plorer.exe’ which is allowed by every testing suite we
looked at since it is a critical user interface component
for Windows based systems. For suites using block lists,
most law students would be capable of ﬁnding a process
not present on the disallowed list through trial and error.

4.4.2 Student Privacy Concerns

Continuing to evaluate the impact remote exam proctor-
ing software use has on exam integrity, two major privacy
questions arise: (1) Is the user appropriately informed of
the information being collected upon engaging with the
remote exam software? and (2) Does the potential for
pervasive monitoring after the student is no longer ac-
tively taking an exam exist? To this end, we develop an
analysis tool to assist other researchers in identifying re-
mote exam software privacy issues.
Informed Consent An examinee cannot provide mean-
ingful consent to the activities performed by the exam
proctoring software if they are not informed of the spe-
ciﬁc data being collected or the surveillance mechanisms
performed. Examinees are prohibited from reverse engi-

neering the software to discover such information and at-
tempts to glean this information by reading privacy poli-
cies, end user license agreements, or similar documents
will be met with vague and sometimes conﬂicting ver-
biage. As an example, ExamSoft’s privacy policy notes,
“in order to secure the exam taker’s device, ExamSoft
must access and, in some instances, modify device sys-
tem ﬁles.” This broad statement provides no substantive
limitation on what the Examplify software may do. Other
privacy policies contain conﬂicting statements about the
software’s activities. For example, the Extegrity Exam4
privacy policy states, “Exam4 does not access any data
on the laptop other than the data created by the act of
taking an exam” and “Exam4 monitors activity the stu-
dent engages in while taking an exam and creates an en-
crypted log report for evidentiary purposes.” It is not
possible for Exam4 to monitor activity the student en-
gages in if it does not access any data other than that cre-
ated by the act of taking an exam. These types of state-
ments thwart a student’s ability to meaningfully consent.
Furthermore, even if an examinee was fully aware of the
software privacy implications, meaningful consent is still
lacking given the examinee’s lack of meaningful alterna-
tives. Faced with accepting and using the software as-is
or refraining from taking the exam, most law students
would opt for the former as the latter coincides with an
inability to become a licensed lawyer. Such a choice is
not a choice. While legal agreements offer a degree of li-
ability cover for the exam software companies, they fail
to meet conventional ethical standards for consent [3].
Post Exam Monitoring
Examplify installs a highly
privileged system service that is constantly running on
the computer even if Examplify isn’t open. Currently
running applications on a user’s computer are logged to
a debugging ﬁle that is uploaded periodically once the
Examplify application is open. The service also regu-
larly reaches out to the Examplify server to check for
and install updates for the service or the binary. Exam4
and ILG Exam360 also implement a system service but
stop it when the exam is terminated gracefully. Elec-
tronic Blue Book directly hooks into the Windows sys-
tem service with their binary to provide their monitoring
features, guaranteeing no additional background moni-
toring is being performed once the binary is closed.

4.5 Automating Impact Analysis

We created an analysis tool based on the RADARE2
and Ghirda frameworks [8] to simplify the reverse en-
gineering process for researchers who want to quickly
analyze the privacy impact of other exam proctor-
ing solutions. We release the tool publicly at
github.com/WWP22/ProctoringSuiteAnalysisTool. The
approach we use in this paper using traditional tools like

8

Dataset

Source

# of subjects # of faces

In-the-wild White Asian Black Hispanic Indian

MORPH-II Government Data
Multi-PIE
LFWA+
VGGFace2

Study Capture
LFW
Image Search

14K
750K
6K
9K

55K
337
13K
3M

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)†

(cid:88)
(cid:88)
(cid:88)
(cid:88)†

(cid:88)
(cid:88)
(cid:88)
(cid:88)†

(cid:88)
(cid:88)†

Race Annotation

Table 2: Summary of datasets used for the facial recognition accuracy section.

IDA work well for in depth studies, but they are not well
suited for providing a quick summary of an exam proc-
toring suite’s privacy impact. Our tool requires a user to
simply run the Python script on the binary they want to
analyze and a high level overview of the application will
be provided.

4.5.1 Design

The analysis tool ﬁrst loads all of the shared object ﬁles
the binary uses and then performs auto analysis using
RADARE2’s built in analysis suite. This attempts to
locate the segments and functions to generate a control
ﬂow graph. From this control ﬂow graph, we extract
cross references to lines in any part of the code which
allows us to more easily establish where certain data el-
ements are being used.

We are able to detect privacy sensitive calls such as
calls to a microphone, webcam, or video driver by ﬁn-
gerprinting common vendor and system library names.
We also attempt to extract information about the security
features the exam proctoring suite implements including
whether it detects virtual machines, uses a secure con-
nection to reach the back-end server, and whether it en-
crypts on disk content. If on disk encryption is found, we
display the cipher suite being used then attempt to extract
the encryption key and initialization vector by searching
for keys of the correct bit length in a user deﬁnable win-
dow around any data references found in the encryption
function. For a more complete analysis, the tool can be
run with the live memory option which initializes the bi-
nary, attaches GDB, and runs to a user deﬁned breakpoint
then performs the analysis. This allows a more complete
analysis of libraries and code segments which are stored
encrypted at rest or loaded from a remote endpoint. A
user can view a summary of the binary’s security and pri-
vacy properties or opt for a more in depth analysis featur-
ing control ﬂow graphs and decompilations with Ghirda
of code segments the tool ﬁnds relevant.

4.5.2 Evaluation

We evaluate the automated analysis tool on the four
suites to determine whether it accurately identiﬁes rele-

vant security and privacy information. The tool is evalu-
ated without using the live memory analysis option since
we believe this would be the most commonly run conﬁg-
uration of the tool due to the relatively large performance
and memory overhead of searching the entire live mem-
ory space multiple times on consumer hardware.

We ﬁnd the tool able to correctly identify camera and
microphone usage in the all cases aside from a false posi-
tive triggered when analyzing Electronic Bluebook. This
false positive is due to the inclusion of a large English
dictionary in the binary which incorrectly triggers one of
the vendor searches we run. The relevant function con-
trol ﬂow graph and decompilation is presented to the user
which would allow the user to trivially identify it as ir-
relevant and subsequently disregard.

The tool performs similarly well with virtual machine
detection, insecure connections, and on disk encryption
with results mirroring the results we obtained in our man-
ual analysis. The encryption key Examplify uses is suc-
cessfully extracted and presented to the user along with
a few false positives. We write a simple Python script to
test the encryption keys and initialization vectors the tool
outputs and are able to successfully decrypt the on disk
libraries for Examplify in under one minute.

5 Automated Identity Veriﬁcation

Traditionally, human veriﬁcation through simple recog-
nition of a student and identiﬁcation card checks have
been used to ensure exam integrity. While these checks
could still be conducted in the remote setting these exam
software suites operate in, there is a large incentive for
exam proctoring vendors to move to a fully automated
model using an automated facial recognition system due
to reduced stafﬁng cost and an increased capacity for
the number of students that can be veriﬁed. When fa-
cial recognition classiﬁers are used to determine who can

†VMER uses more speciﬁc race annotations than the current fed-
eral government standards for collecting and presenting data on race.
We group these more speciﬁc annotations as follows using the fed-
eral standards and the NIST FRVT as a guideline: Caucasian Latin→
White; African American→ Black; East Asian→ Asian; and Asian
Indian→ Indian

9

MORPH-II

Multi-PIE

LFWA+

VGGFace2

Classiﬁer

FNMR FMR

FNMR FMR

FNMR

FMR

FNMR FMR

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

7.5% 14%
4.1% 78%
5.7% 72%
12% 2.2%
0.66% 21%

9.2% 50%
1.9% 83%
8.6% 77%
11% 48%
9.2% 19%

2.9%
14%
68%
2.8%
12%
62%
24% 0.54%
22% 0.69%

2.0%
17%
42%
3.4%
63%
11%
23%
1.2%
14% 0.42%

Table 3: FNMR and FMR averages across various datasets using both state of the art algorithms and Face-API.js

take an exam or whether a student is ﬂagged for cheating,
it is critical to the fairness of the examination to insure
that the system is accurate and fair.

We evaluate the overall accuracy of current state of
the art facial recognition systems along with the facial
recognition classiﬁer used by Examplify, ‘face-api.js’,
over several datasets we believe accurately reﬂect the real
world images these systems would be operating on. The
classiﬁers we select for comparison against ‘face-api.js’
are based on their accuracy using Labeled Faces in the
Wild (LFW) [31], the current dataset used most promi-
nently in the literature for benchmarking the fairness of
facial recognition algorithms [21]. This results in a selec-
tion containing FaceNet [30], VGG-Face [26], OpenFace
[1], and ArcFace [12, 18, 13, 11].

Given that the expertise of remote proctoring ﬁrms is
outside AI/ML and that in the current business environ-
ment such ﬁrms are unlikely to develop and train their
own models (an assumption borne out by our analysis of
the leading market product), it is reasonable to select pre-
trained, off-the-shelf models for comparison. Finally, we
conduct an analysis of the error rates based on the sub-
ject’s race which allows us to look at the fairness of the
classiﬁer across subjects from different races. We release
our image selections, data processing code, and classiﬁer
performance results at github.com/WWP22/AIVerify.

5.1 Accuracy

Using the knowledge gleaned from our analysis of Ex-
amplify’s identity veriﬁcation system, we separate the
facial recognition steps an exam proctoring suite would
need to perform into two steps: the initial veriﬁcation
against a student’s identiﬁcation photo to bootstrap their
identity; and the continuous veriﬁcation that insures the
student who veriﬁed their identity initially continues to
take the entire exam.

Dataset Selection We select MORPH-II [29], Multi-PIE
[16], LFWA+ [24] and VGGFace2 [7] as our datasets to
evaluate the performance of the facial recognition classi-
ﬁer using images that are very similar to the ones likely

to be encountered during real world use. A full descrip-
tion of the datasets can be found in Table 2.
Metric Selection We focus on the false non match rate
(FNMR) and the false match rate (FMR) of the facial
recognition systems for our evaluation. We select these
metrics as they allow us to determine the rate at which the
facial recognition system would fail to verify a student
(FNMR) and at what rate the system would falsely verify
a different person as the student (FMR). These metrics
are also used in other key studies on facial recognition
accuracy such as the NIST FRVT [17] which allows our
results to be easily comparable.

5.1.1

Initial Veriﬁcation

Initial identity veriﬁcation in a remote exam setting re-
lies on comparing a known image of the subject (likely a
school ID or driver’s license) to a capture of the subject
trying to take the exam. This can present background,
lighting, and age progression challenges for the facial
recognition classiﬁer.
Image Selection We select images from each of the
above datasets to create sub-sampled datasets for our
FNMR and FMR evaluation. We create our FNMR
datasets by selecting the earliest capture of each subject
in the dataset as our reference image. We use up to 50
images from subsequent captures of the same subject to
compare against. The extended length of time between
subsequent captures in the Morph-II, LFWA+, and VG-
GFace2 datasets make them useful for the initial iden-
tity veriﬁcation comparison since institutions commonly
use the student’s driver’s license which has an average
photo refresh time of 5.7 years [37] as the reference im-
age. For the Multi-PIE dataset we select subjects who
attended multiple sessions, using each subject’s ﬁrst at-
tended session as the reference image and up to three
of their subsequent session images as comparison im-
ages. Use of these Multi-PIE captures provides insight
on whether more recent reference images signiﬁcantly
improve the performance of the facial recognition clas-
siﬁer We create the datasets we use for evaluating the
FMR by selecting the earliest subject capture as the ref-

10

erence image and 50 earliest captures of other subjects
as comparison images. Appendix D summarizes these
sub-sampled datasets.
Classiﬁer Performance High FNMRs occur across all
of the datasets evaluated for both the SOTA classiﬁers
and ‘face-api.js’. When run on datasets that are in-the-
wild versus those with more constrained capture condi-
tions, signiﬁcantly elevated FNMRs occur across all of
the classiﬁers except VGG-Face. A signiﬁcant differ-
ence in the FNMRs between the Morph-II and Multi-PIE
datasets was not noted which suggests that the time be-
tween the reference image and the comparison image is
not a major factor in classiﬁer performance. Overall, we
see an inverse relationship between the FNMR and the
FMR such that an algorithm less likely to fail at identi-
fying a correct student would be more likely to verify an
imposter (hired test-taker). We summarize these ﬁndings
in Table 3.
Analysis These high FNMRs make any of the classiﬁers
we evaluated inappropriate for use in an automated facial
recognition setting due to the extreme cost to a student
unable to be veriﬁed. Exam proctoring vendors can at-
tempt to reduce the error rate by selecting classiﬁers with
the lowest FNMR, however, these classiﬁers all present
much higher FMRs suggesting they provide little more
than security theatre. Using more recent trusted refer-
ence images had minimal impact on the FNMR when
compared to using images captured over longer time pe-
riods minimizing an institution’s ability to mitigate the
high FNMR by collecting more recent reference images.

5.1.2 Continuous Veriﬁcation

Continuous identity veriﬁcation works to ensure the ini-
tially veriﬁed student takes the entire exam and is not
replaced by another person. The student’s recent ref-
erence image veriﬁed during initial veriﬁcation is com-
pared to subsequent captures taken silently at random in-
tervals during the exam. These unprompted captures cre-
ate challenges for the facial recognition classiﬁer since
the student’s facial tilt, pose, and lighting may all vary
signiﬁcantly from the prompted reference image taken in
a controlled fashion.
Image Selection We create subsets from the Multi-
PIE dataset for evaluating FNMR based on varying facial
rotations, facial expressions, and lighting. Appendix E
describes each dataset.
Classiﬁer Performance The performance between the
SOTA classiﬁers and ‘face-api.js’ is roughly equivalent
up to a 30 degree head or facial rotation but diverges
thereafter with ‘face-api.js’ maintaining a relatively low
FNMR up to 60 degrees. The SOTA classiﬁers begin
failing to verify the majority of subjects once the rotation
reaches 60 degrees while ‘face-api.js’ still veriﬁes over a

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

100

)

%

(

e
t
a
R
h
c
t
a

M
-
n
o
N
e
s
l
a
F

80

60

40

20

0

−90 −75 −60 −45 −30 −15 0

15

30

45

60

75

90

Facial Tilt (°)

Figure 2: False Non Match Rate (FNMR) of subjects at
various facial rotations ranging from -90° to 90° from the
camera across various classiﬁers..

75 degree rotation. Figure 2 shows the relationship be-
tween facial rotation and classiﬁer performance. There
is minimal variation from the average FNMR discussed
in Section 5.1.1 when the subject changes their facial ex-
pression. Classiﬁer performance falls off sharply as the
lighting conditions differ more signiﬁcantly from the ref-
erence image. ‘Face-api.js’ signiﬁcantly underperforms
VGG-Face when evaluating images taken under varying
lighting conditions but outperforms all of the other SOTA
classiﬁers. Appendix F provides a full description of the
FNMR performance on these datasets.
Analysis The classiﬁers exhibit signiﬁcantly increased
FNMRs once they are tasked with comparing images
in challenging conditions such as varied lighting or
head/facial rotation. Students taking an exam in good
faith over long exam periods have images that reason-
ably exhibit lighting and head/face position variation es-
pecially since captures are taken without notice. The
FNMR exhibited even in the less extreme conditions sug-
gest that a signiﬁcant number of students would fail to
be veriﬁed correctly. A majority of students would fail
to be validated in the more extreme conditions we tested.
The variance in lighting seen was on the more extreme
side but not outside the realm of real world exam envi-
ronments where exams can start in daylight and end after
dark.

5.2 Fairness

When forms of identity veriﬁcation are used as part of
examination procedure, it is critical to the fairness of the
examination that every effort to minimize bias is made.

11

MORPH-II

LFWA+

VGGFace2

Classiﬁer

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

k
c
a
l
B

e
t
i
h
W

n
a
i
s
A

c
i
n
a
p
s
i
H

k
c
a
l
B

e
t
i
h
W

n
a
i
s
A

n
a
i
d
n
I

k
c
a
l
B

e
t
i
h
W

n
a
i
s
A

n
a
i
d
n
I

6.8% 9.7% 15% 10%
3.6% 5.2% 11% 7.4%
6.0% 4.6% 9.5% 3.8%
11% 13% 23% 11%
0.57% 1.1% 0.0% 0.1%

15% 15% 9.5% 14%
1.4% 2.9% 2.7% 0.58%
13% 12% 10% 18%
30% 24% 15% 27%
23% 22% 18% 21%

26% 17% 17% 15%
1.9% 3.6% 3.1% 3.4%
18% 10% 14% 7.1%
34% 21% 27% 19%
20% 13% 14% 11%

Table 4: FNMR averages across various datasets using both state of the art algorithms and Face-API.js

Dataset Selection
We select MORPH-II, LFWA+
and VGGFace2 as they either provide labels of a sub-
ject’s race in the original dataset or have labels available
through additional studies such as VGG-Face2 Mivia
Ethnicity Recognition (VMER) [15]. Multi-PIE is not
included in this section due to the limited number of sub-
jects in some of the races we analyze.
Metric Selection The FNMR metric, which potentially
ﬂags a student for cheating, is utilized in this section
since it reﬂects the most negative impact on the student.
Classiﬁer Performance On the Morph-II dataset all
classiﬁers except OpenFace ﬂag ‘White’ subjects at a
slightly higher rate than ‘Black’ subjects. Signiﬁcant
outliers in the Facenet and Arcface classiﬁers for ‘Asian’
subjects are evident. Across races, ‘Asian’ subjects are
ﬂagged at signiﬁcantly higher rates in all classiﬁers ex-
cept FaceAPI. The LFWA+ dataset shows a much wider
distribution of FNMRs within a racial grouping across
classiﬁers and higher FNMR values for ‘Black’ subjects
versus ‘White’ subjects occur for all of the classiﬁers ex-
cept VGG-Face and Facenet. Compared to other races,
‘Asian’ subjects have lower FNMR values across all clas-
siﬁers run on LFWA+ except VGG-Face. Much like clas-
siﬁer performance on the LFWA+ dataset, all classiﬁers
run on VGGFace2 except VGG-Face yield higher FNMR
values for subjects labelled as ‘Black’ versus ‘White’.
Additionally, ‘Asian’ and ‘Indian’ subjects also fair bet-
ter than ‘Black’ subjects on all classiﬁers except VGG-
Face on the VGGFace2 dataset. While lower FNMRs
are seen on ‘Asian’ subjects compared to ‘White’ sub-
jects with LFWA+, elevated FNMRs occur on the ‘Asian’
group compared to the ‘White’ group using the VG-
GFace2 dataset except with the Facenet and VGG-Face
classiﬁers.
‘Indian’ subjects have the lowest FNMRs
across all races using all classiﬁers except VGG-Face on
the VGGFace2 dataset. A summary of these results can
be see in Table 4.
Analysis We see signiﬁcant variations in the perfor-
mance of the different classiﬁers depending on the race
of the subject being analyzed. In the large VGGFace2

and LFWA+ datasets, we see signiﬁcantly higher FN-
MRs for subjects labelled as ‘Black’ versus ‘White’.
Similar disadvantage for subjects tagged as ‘Asian’ com-
pared to those tagged as ‘White’ occurs on the VG-
GFace2 dataset. This demonstrates the propensity for
minority groups to be ﬂagged at higher rates than other
groups depending on the capture conditions. Addition-
ally, we see major variance in FNMR performance be-
tween subjects of different races based on the dataset,
demonstrating the difﬁculty of correcting for this in the
software using the classiﬁer.

5.3 Evaluating Real World Implementa-

tion

Examplify uses the ‘face-api.js’ classiﬁer for facial
recognition with a pretrained publicly available model
inside the exam proctoring suite. We see in Section 5.1
that ‘face-api.js’ was quite inaccurate overall with an
average FNMR of 11.47% when evaluated on datasets
that accurately reﬂect images it would be running on in
a testing environment. In Examplify’s implementation,
the initial veriﬁcation step acts as a gate keeper for the
student to be able to take the exam. The high average
FNMR suggests that this would disadvantage numerous
students attempting to take their exam. We see simi-
larly concerning performance on the continuous veriﬁca-
tion task which runs in the background during an exam
if enabled by the institution. Realistic variations in the
capture conditions such as head/face rotation or lighting
changes could cause the image to fail to be veriﬁed, ﬂag-
ging the student’s exam for human review. Depending
on how this is presented to the proctor for human review,
this may unfairly bias the proctor against the student with
a presumption of guilt.

We see signiﬁcant variations in the performance of the
‘face-api.js’ classiﬁers depending on the race of the sub-
ject with some minority groups being disadvantaged in
LFWA+ and VGGFace2 datasets. This suggests that the
automated facial recognition system may be unfairly dis-

12

advantaging certain students based on their race by not
allowing them to take their exam or by presenting their
exam to a human proctor for review at a higher rate than
other non-minority students in certain cases. Given the
variability and bias in the facial recognition steps, we be-
lieve a human based veriﬁcation model is a signiﬁcantly
fairer approach to insuring exam integrity.

If an automated classiﬁer is to be used, we recommend
training models on a dataset that contains a balanced
sampling of subjects from different races versus using
pretrained default models. We also recommend evaluat-
ing the performance of the classiﬁes on datasets that re-
alistically represent the use case of the system. Classiﬁer
performance cannot be accurately assessed just using an
overall performance metric without looking at that per-
formance metric across subjects from different races to
determine whether the classiﬁer is acting in a fair man-
ner.

6 Discussion

Impact on Marginalized Groups Minorities and other
marginalized groups are traditionally underrepresented
in the legal profession. Exam software with the built-in
skin-tone biases creates an invisible barrier. This occurs
both in the intuitive case where minorities are ﬂagged for
cheating at higher rates, but also where they are ﬂagged
at substantially lower rates. Opponents of afﬁrmative ac-
tion have erroneously argued that minorities are not able
to cope with the rigor of law school. By using software
that ﬂags minority students substantially less, such op-
ponents will continue to cast doubt on the validity and
competence of minority students. Thus, substantial bias
in either direction can perpetuate systemic racism. This
may further impair the willingness of minorities to enter
the legal profession. Law schools, bar associations, and
other educational/licensing institutions must investigate
and commission research into inherent bias in the exam
software they utilize to prevent discrimination in exam
administration.
Fundamental Challenges of Remote Examination
Remote exam proctoring suites suffer from fundamental
limitations in the threat model they can protect against
since they run on untrustworthy student hardware versus
exam hall hardware. The student has full administrative
access and will always be able to bypass security fea-
tures and monitoring scheme protections given enough
time. Exam proctoring suite vendors can attempt to in-
crease the time and skill level necessary to compromise
the exam by adding complexity to the process through
obfuscation and active anti-debugging measures.

In order to create a truly secure remote exam proc-
toring suite, a vendor needs to establish a trusted envi-
ronment on the device that restricts the student’s ability

to extract or modify part of the exam suite. Intel SGX
and other similar trusted execution environments could
potentially be employed for this, however, it may intro-
duce signiﬁcant usability and availability concerns due
to a much more complex bootstrapping procedure. Fur-
thermore, Intel has recently announced the deprecation
of SGX in consumer chips—as a result of ongoing secu-
rity issues with the technology [10].
Risks of Privileged Software All the software we eval-
uated require privileged system access. Operating sys-
tems increasingly restrict such access as it is a common
source of malfeasance. Buggy but well-intentioned code
given such access substantially broadens the attack sur-
face of the OS and serves as a glaring target. Experts
urge against granting such access even to third-party an-
tivirus software [2]! Compounding the problem, students
are likely to be unaware that privileged system services
from the proctoring packages do not uninstall automati-
cally and persist after the exam is over [4], putting them
at long term risk.

Privacy, Surveillance, and Ethical Concerns
At-
tempting to meet their design goals, platforms engage
in sweeping surveillance. Keylogging, screen captures,
drive access, process monitoring, and A/V feeds give
their software access to personal data stored on the de-
vice. Outside of this context, these features appear only
in malware, highlighting the unusual capabilities of these
software. Some binaries also include anti-debugging
mechanisms in their code, further limiting the ability of
student advocates to assess the security and safety of the
software.

The context in which students are required to install
proctoring software mitigates their ability to meaning-
fully consent to the substantial impositions on their pri-
vacy and security. The veneer of legitimacy of the plat-
form conveyed by the institutional backing of the school
or testing company compounds this. This dynamic is par-
tially captured in the results of Balash, E T A L. [4] who
ﬁnd that trust in institutions substantially reduces the ex-
tent to which students are willing to object to remote
proctoring. Even if they did, students are not provided
with a reasonable alternative.
Recommendations The strongest recommendation we
offer is that where allowable, educators should design
assessments that minimize the possible advantage to be
gained by cheating. Project work, open book essays, or
other similar modes with unrestricted access to resources
feature fewer opportunities to gain unfair advantage.

Where re-imagining assessment is not possible, stu-
dents should be offered a meaningful chance to opt-out
of digital testing on their own hardware and should be
given the choice of either using provided hardware or
paper-copy and live proctoring. Furthermore, schools put
substantial efforts into helping students install proctoring

13

software and should match this with equal efforts to help
them uninstall the software while advising them of the
risks of retaining it.

Given the substantial fairness concerns with facial
recognition systems, our primary recommendation is to
avoid using them. Where infeasible, it is vital that hu-
man review remain a central component and that such
review be conducted by multiple diverse individuals to
reduce human biases as well. Lastly, until such time
as more advanced facial recognition technology is devel-
oped, if these current classiﬁers are going to see contin-
ued use, candid conversation regarding generalized dif-
ferences in facial features between racial groups needs
to be addressed by programmers through dialogue with
racially diverse focus groups and accounted for in cali-
bration settings in an attempt to reduce the incidence of
false identiﬁcation.

References

[1] Amos, B., Bartosz, L., and Satyanarayanan, M.
OpenFace: A general-purpose face recognition li-
brary with mobile applications. Technical report,
CMU-CS-16-118, CMU School of Computer Sci-
ence, 2016 (cited on page 10).

[2] Anthony, S.

It might be time to stop us-
ing antivirus. Ars Technica, 2017. https :
/ / arstechnica . com / information -
technology / 2017 / 01 / antivirus - is - bad/
(cited on page 13).

[3] Appelbaum, P. S., Lidz, C. W., and Meisel, A. In-
formed consent: legal theory and clinical practice,
1987 (cited on page 8).

[4] Balash, D. G., Kim, D., Shaibekova, D.,
Fainchtein, R. A., Sherr, M., and Aviv, A. J. Ex-
amining the Examiners: Students’ Privacy and Se-
curity Perceptions of Online Proctoring Services.
USENIX Symposium on Usable Privacy and Secu-
rity, 2021 (cited on pages 3, 13).

[5] Barrett, L. Rejecting test surveillance in higher
education. SSRN Electronic Journal, Jan. 2021
(cited on page 3).

[9] Cohney, S., Teixeira, R., Kohlbrenner, A.,
Narayanan, A., Kshirsagar, M., Shvartzshnaider,
Y., and Sanﬁlippo, M. Virtual classrooms and
real harms. USENIX Symposium on Usable Pri-
vacy and Security, 2021 (cited on page 3).

[10] Corporation, I. 12th generation intel® core™ pro-

cessors datasheet (cited on page 13).

[11] Deng, J., Guo, J., Niannan, X., and Zafeiriou,
S. Arcface: additive angular margin loss for
deep face recognition. In CVPR, 2019 (cited on
page 10).

[12] Deng, J., Guo, J., Yuxiang, Z., Yu, J., Kotsia, I.,
and Zafeiriou, S. Retinaface: single-stage dense
face localisation in the wild. In arxiv, 2019 (cited
on page 10).

[13] Deng, J., Roussos, A., Chrysos, G., Ververas, E.,
Kotsia, I., Shen, J., and Zafeiriou, S. The menpo
benchmark for multi-pose 2d and 3d facial land-
mark localisation and tracking. IJCV, 2018 (cited
on page 10).

[14] Feathers, T. Proctorio is using racist algorithms to
detect faces. VICE, 2021. https://www.vice.
com / en / article / g5gxg3 / proctorio - is -
using - racist - algorithms - to - detect -
faces (cited on page 3).

[15] Greco, A., Percannella, G., Vento, M., and Vig-
ilante, V. Benchmarking deep network architec-
tures for ethnicity recognition using a new large
face dataset. Machine Vision and Applications,
2020 (cited on page 12).

[16] Gross, R., Matthews, I., Cohn, J., Kanade, T., and
Baker, S. Multi-pie. Image and vision computing,
28(5):807–813, 2010 (cited on page 10).

[17] Grother, P. J., Grother, P. J., and Ngan, M. Face
recognition vendor test (frvt). US Department of
Commerce, National Institute of Standards A N D
Technology, 2014 (cited on pages 3, 10).

[18] Guo, J., Deng, J., Xue, N., and Zafeiriou, S.
Stacked dense u-nets with dual transformers for
robust face alignment. In BMVC, 2018 (cited on
page 10).

[6] Be prepared: law school doesn’t even resemble

[19]

your college experience (cited on page 4).

[7] Cao, Q., Shen, L., Xie, W., Parkhi, O. M., and Zis-
serman, A. Vggface2: a dataset for recognising
faces across pose and age, 2018 (cited on page 10).

[8] Cheng, E. Binary Analysis and Symbolic Ex-
ecution with angr. PhD thesis, PhD thesis, The
MITRE Corporation, 2016 (cited on page 8).

Johnson, E. Hey @proctorio @artfulhacker how
do you explain this? Twitter, 2020. http://web.
archive.org/web/20210715164139/https:
/ / twitter . com / ejohnson99 / status /
1303121786637373443 (cited on page 3).

[20] Karim, M. N., Kaminsky, S. E., and Behrend,
T. S. Cheating, reactions, and performance in re-
motely proctored testing: an exploratory experi-
mental study. Journal of Business and Psychol-
ogy, 29(4):555–572, 2014 (cited on page 3).

14

[32] Singer, N. Online Cheating Charges Upend Dart-
mouth Medical School. NYTimes, 2021 (cited on
page 1).

[33] Singh, R., Agarwal, A., Singh, M., Nagpal, S., and
Vatsa, M. On the robustness of face recognition
algorithms against attacks and bias. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intel-
ligence, volume 34 of number 09, 13583–13589,
2020 (cited on page 3).

[34] Slusky, L. Cybersecurity of online proctoring sys-
tems. Journal of International Technology and In-
formation Management, 29(1):56–83, 2020 (cited
on page 3).

[35] Swauger, S. MIT Technology Review, 2020.
https : / / www . technologyreview . com /
2020 / 08 / 07 / 1006132 / software -
algorithms - proctoring - online - tests -
ai-ethics/ (cited on page 3).

[36] Teclehaimanot, B., You, J., Franz, D. R., Xiao, M.,
and Hochberg, S. A. Ensuring academic integrity
in online courses: a case analysis in three testing
environments. The Quarterly Review of Distance
Education, 12(1):47–52, 2018 (cited on page 3).

[37] Tefft, B. C. Driver license renewal policies and fa-
tal crash involvement rates of older drivers, united
states, 1986–2011. Injury epidemiology, 1(1):1–
11, 2014 (cited on page 10).

[38] Turani, A. A., Alkhateeb, J. H., and Alsewari,
A. A. Students online exam proctoring: a case
study using 360 degree security cameras. In 2020
Emerging Technology in Computing, Communica-
tion and Electronics (ETCCE), 1–5. IEEE, 2020
(cited on page 3).

[39] Wu, W., Protopapas, P., Yang, Z., and Michalatos,
P. Gender classiﬁcation and bias mitigation in fa-
cial images. In 12th ACM Conference on Web Sci-
ence, 106–114, 2020 (cited on page 3).

[21] Kemelmacher-Shlizerman, I., Seitz, S. M., Miller,
D., and Brossard, E. The megaface benchmark: 1
million faces for recognition at scale. In Proceed-
ings of the IEEE conference on computer vision
and pattern recognition, 4873–4882, 2016 (cited
on page 10).

[22] Langenfeld, T.

Internet-based proctored assess-
ment: security and fairness issues. Educational
Measurement: Issues and Practice, 39(3):24–27,
2020 (cited on page 3).

Understanding bias
[23] Leslie, D.
arXiv
recognition
technologies.
arXiv:2010.07023, 2020 (cited on page 3).

in facial
preprint

[24] Liu, Z., Luo, P., Wang, X., and Tang, X. Deep
learning face attributes in the wild. In Proceedings
of International Conference on Computer Vision
(ICCV), 2015 (cited on page 10).

[25] Nagpal, S., Singh, M., Singh, R., and Vatsa, M.
Deep learning for face recognition: pride or prej-
udiced? arXiv preprint arXiv:1904.01219, 2019
(cited on page 3).

[26] Parkhi, O., Vedaldi, A., and Zisserman, A. Deep
face recognition. university of oxford, 2015 (cited
on page 10).

[27] Raman, R., Sairam, B., Veena, G., Vachharajani,
H., and Nedungadi, P. Adoption of online proc-
tored examinations by university students during
covid-19: innovation diffusion study. Education
and Information Technologies:1–20, 2021 (cited
on page 1).

[28] Reed, A. Online Bar Exams Come With Face
Scans, Bias Concerns. Bloomberg Law, 2020.
https://news.bloomberglaw.com/health-
law - and - business / online - bar - exams -
come - with - face - scans - discrimination -
concerns (cited on page 1).

[29] Ricanek, K., and Tesafaye, T. Morph: a lon-
gitudinal image database of normal adult age-
progression. In, volume 2006, 341–345, May
2006 (cited on page 10).

[30] Schroff, F., Kalenichenko, D., and Philbin, J.
Facenet: a uniﬁed embedding for face recognition
and clustering. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition,
815–823, 2015 (cited on page 10).

[31] Serengil, S. I., and Ozpinar, A. Lightface: a hy-
brid deep face recognition framework. In 2020 In-
novations in Intelligent Systems and Applications
Conference (ASYU), 23–27. IEEE, 2020 (cited on
page 10).

15

A Proctoring Suite Feature Summary

We provide a summary of the various security and privacy features each exam proctoring suite we analyzed currently
offers. We categorize the features based on whether they have a direct relation to a user’s privacy. If a feature wasn’t
implemented by the exam proctoring suite, we mark the feature as not implemented (N/I) for that suite.

A.1 Security

Security Feature

Examplify

Exam4

EBB

ILG Exam360

Encryption at Rest
Encryption in Transit
Virtual Machine Protection
Virtual Device Detection
Clipboard Management
Screenshot Capture
Process Restrictions
Network Access Restrictions

AES-256
HTTPS
Block List
Block List
Integrated
N/I
Allow List
Route Table Adapter Disable

AES-256
HTTP
Block List
N/I
Cleared
N/I
Block List

3DES
HTTP
Block List
N/I
Cleared
N/I
N/I
N/I

AES-256
HTTPS
Block List
Block List
Integrated
App Window
Null DNS
Null DNS

This table provides an overview of the security features provided by Examplify, Exam4, Electronic Blue Book, and
ILG Exam360. All of the exam suites use strong encryption algorithms to protect exam content at rest. Only two exam
suites, Examplify and ILG Exam360, use HTTPS for their encryption in transit while the other exam suites rely on the
content being protected using the same keys they protect the exam at rest with. Virtual machines are blocked by all of
the exam suites using block lists which have common vendors and virtual machine properties listed in them. Virtual
device detection is only implemented by Examplify and ILG Exam360. Since Exam4 and Electronic Blue Book do
not use the microphone or camera, they do not implement any virtual device detection. Clipboard management is
implemented by all of the exam proctoring suites by either implementing a custom restricted clipboard or by using the
system clipboard with a function call to clear it before the exam begins. ILG Exam360 was the only proctoring suite
that implemented a form of screenshot capture to see what was on the student’s screen during the exam. The processes
a student can run are restricted on all of the exam suites either through an allow list which allows certain processes
to be run by the student or a block list which speciﬁcally prevents certain processes from being run by the student.
Network access restrictions are offered in Examplify, Exam4, and ILG Exam360 while Electronic Blue Book does not
offer a way to restrict internet access aside from the process restrictions.

A.2 Privacy

Privacy Related Feature

Examplify

Exam4

EBB

ILG Exam360

Initial Identity Veriﬁcation
Continuous Identity Check
System Service
Device Identiﬁers

Automated
Automated
Always Running
App List/OS/Hardware

N/I
N/I

N/I
N/I
App Running App Running App Running
OS/Hardware
N/I

Human
Human

N/I

This table provides an overview of the privacy related features in Examplify, Exam4, Electronic Blue Book, and ILG
Exam360. Initial identity veriﬁcation is only offered by Examplify and ILG Exam360. Exam4 and Electronic Blue
Book do not offer a way to identify a student using any biometrics. Similarly, continuous identity veriﬁcation is only
offered by Examplify and ILG Exam360. For both identity veriﬁcation steps, Examplify uses an automated method
while ILG Exam360 uses a human to complete the veriﬁcation step. All of the proctoring suites implement some form
of system service to allow a signiﬁcant portion of their functionality. Examplify’s monitoring service was the only
service we found to continue running after the application was closed. Device identiﬁers are sent by Examplify and
ILG Exam360 to a centralized server while Exam4 and Electronic Blue Book do not collect any device metadata.

16

B Virtual Machine Block List

Virtual Machine

Examplify Exam4 EBB ILG Exam360

VirtualBox
VMWare Workstation
VMWare Fusion
Parallels
Hyper-V
QEMU

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

This table provides a summary of popular virtual machine software that is blocked by the exam proctoring suites
we evaluated. VirtualBox and VMWare Workstation are blocked by all of the proctoring suites. VMWare Fusion
which is only available on Mac was blocked solely by Examplify even though ILG Exam 360 and Exam4 offer Mac
versions. Parallels was detected by Examplify, Exam4 and ILG Exam360 but not by Electronic Blue Book. Hyper-V
and QEMU were only detected by Examplify.

C Virtual Webcam/Microphone Block List

Virtual Webcam/Microphone

Examplify Exam4 EBB ILG Exam360

ManyCam
YouCam
MyCam
Logitech Capture
OBS Studio

(cid:88)
(cid:88)
(cid:88)

N/I
N/I
N/I
N/I
N/I

N/I
N/I
N/I
N/I
N/I

(cid:88)
(cid:88)
(cid:88)

(cid:88)

This table provides an overview of the popular virtual webcam and microphones that were detected or blocked
by the Examplify or ILG Exam360 proctoring suites. Exam4 and Electronic Blue Book do not use the webcam or
microphone during exams so they did not implement speciﬁc security features to target virtual webcams or micro-
phones. ManyCam, YouCam, and MyCam are blocked by both Examplify and ILG Exam360 while OBS Studio is
only detected by ILG Exam360. Logitech Capture is undetected by all of the proctoring suites.

D Initial Veriﬁcation Image Selection

FNMR

FMR

Classiﬁer

# of subjects

# of comparisons

# of subjects

# of comparisons

MORPH-II
Multi-PIE
LFWA+
VGGFace2

13K
264
1.7K
9.1K

42K
81K
7.4K
457K

14K
264
5.7K
9.1K

681K
172K
287K
457K

This table provides a summary of the datasets we use for our evaluation of the facial recognition classiﬁer’s per-
formance in an initial veriﬁcation setting. The number of subjects for the FNMR datasets is lower than the FMR
dataset since we require subsequent images of the same subject so if a subject only has a single image, they would be
excluded. We select the ﬁrst image of the subject in the dataset as the reference image then compare against up to 50
subsequent images of the same subject for the FNMR datasets. We compare the subject against 50 other subjects at
random for the FMR datasets.

17

E Continuous Veriﬁcation Image Selection

Multi-PIE Dataset

# of subjects # of comparisons

Session
Facial Rotation
Lighting
Facial Expression

264
337
337
337

81K
151K
131K
104K

This table summarizes the datasets we use for our evaluation of the facial recognition classiﬁer’s performance in a
continuous veriﬁcation setting. The ‘Session’ dataset uses the session image of a subject as the reference image and
images from all subsequent sessions as comparison images. We use ﬁve different lighting conditions to increase the
number of samples with each reference image being compared against a comparison image using the same lighting
condition. The number of subjects for the Multi-PIE Session dataset is lower than the overall number of subjects in
the Multi-PIE dataset since subjects were only included in this dataset if they attended multiple sessions and only a
subset of the total subjects attended multiple session. The ‘Facial Rotation’ dataset uses an image of the subject facing
directly into the camera as the reference image with each comparison image being another image of the subject at
a different rotation away from being centered and facing towards the camera. The ‘Lighting’ dataset uses a well lit
image of the subject as the reference image then four comparison images with different degrees of lighting which cause
partial feature occlusion. The ‘00’ and ‘16’ lighting conditions are the most extreme variation from the reference image
while the ‘04’ and ‘12’ lighting conditions are much less extreme. The ‘Facial Expression’ dataset uses an image of
the subject with a neutral expression as the reference image then comparison images with the subject either smiling,
showing surprise, squinting, showing disgust, or screaming.

F Continuous Veriﬁcation Performance

We provide a summary of the FNMR performance of the facial recognition classiﬁers across sub-sampled datasets we
create in order to test the classiﬁer’s performance in a continuous veriﬁcation setting.

F.1 Average FNMR

Multi-PIE Dataset

Classiﬁer

Session

Facial Rotation Lighting

Facial Expression

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

9.2%
1.9%
8.6%
11%
9.2%

48%
26%
39%
58%
22%

20%
2.3%
11%
14%
8.7%

9.4%
2.1%
7.7%
11%
9.6%

This table provides a summary of the overall FNMR performance across all of the sub-sampled datasets we created.
We see the ‘Session’, ‘Lighting’, and ‘Facial Expression’ datasets exhibiting signiﬁcantly reduced average FNMR
values compared to the ‘Facial Rotation’ dataset. This is likely due to the fact that the average number of facial
features being occluded by any of these variations is signiﬁcantly less than when the subject rotates their head away
from being centered with the camera. When the facial recognition classiﬁer is unable to ﬁnd these features they inﬂate
the FNMR since the classiﬁer is calculating the distance between features it locates on the face then comparing the
distance to a set threshold in order to determine if a face is veriﬁed.

18

F.2 Facial Rotation

Rotation

Classiﬁer

-90

-75

-60

-45

-30

-15

15

30

45

60

75

90

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

9.0% 7.2% 19% 47% 69% 73% 73%
73% 72% 69% 45% 21%
4.5% 3.2% 5.0% 22% 41% 37% 45%
35% 49% 44% 17% 6.2%
5.0% 4.2% 9.9% 42% 50% 63% 73%
72% 50% 57% 36% 11%
91% 90% 85% 53% 19%
9.9% 7.4% 19% 58% 85% 90% 91%
80% 32% 7.3% 1.6% 2.8% 0.84% 1.2% 5.8% 2.8% 9.5% 42% 79%

This table provides the complete FNMR performance for each facial rotation we tested across all of the different
classiﬁers. We see the FNMR increase as the facial rotation increases most likely due to more facial features being
hidden in the image. We see FaceNet and OpenFace begin failing to verify the majority of subjects at 60 degrees
of facial rotation. ArcFace begins to fail to verify subjects at 45 degrees of facial rotation while VGG-Face has its
performance slowly fall off as the angle increases but it never begins failing to verify a majority of the subjects.
FaceAPI starts failing to verify the majority of subjects at around 90 degrees. While these face rotations may seem
high, it is important to remember even a relatively small FNMR can lead to a signiﬁcant number of students being
ﬂagged for cheating as they take these very high stakes exams.

F.3 Session

Classiﬁer

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

Session Gap

1

2

3

10% 9.2% 8.0%
2.1% 1.8% 1.8%
9.3% 8.4% 7.7%
12% 10% 9.2%
10% 8.9% 8.3%

This table provides the FNMR performance of the classiﬁers across images taken in different capture sessions. The
session gap refers to the number of sessions between the reference image and the comparison image. We would expect
to see the FNMR increase as the session gap increased but we do not see that pattern with any of the classiﬁers. This
suggests that the time between captures is less important than other factors such as the subject’s facial hair or hairstyle.

F.4 Lighting

Lighting

Classiﬁer

00

04

12

16

Facenet
VGG-Face
OpenFace
ArcFace
FaceAPI

34% 15% 17% 13%
1.1% 2.7% 2.4% 3.0%
13% 12% 12% 8.4%
15% 15% 15% 13%
17% 7.2% 8.2% 2.9%

This table provides the FNMR performance of the different classiﬁers across the various lighting conditions we test.
We see the FNMR performance degrade when compared against one of the worst lighting conditions, ‘00’. However,
we do not see a similar degradation when testing against the mirrored condition, ‘16’, suggesting that the performance
in bad lighting conditions is very variable across most of the classiﬁers. VGG-Face is the only classiﬁer we tested
which performed consistently well across all of the lighting conditions.

19

