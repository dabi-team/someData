Sockeye 3: Fast Neural Machine Translation with PyTorch

Felix Hieber∗, Michael Denkowski∗, Tobias Domhan∗, Barbara Darques Barros,
Celina Dong Ye, Xing Niu, Cuong Hoang, Ke Tran, Benjamin Hsu, Maria Nadejde,
Surafel Lakew, Prashant Mathur, Anna Currey, Marcello Federico
Amazon

2
2
0
2

g
u
A
2

]
L
C
.
s
c
[

4
v
1
5
8
5
0
.
7
0
2
2
:
v
i
X
r
a

Abstract

Sockeye 3 is the latest version of the Sock-
eye toolkit for Neural Machine Translation
(NMT). Now based on PyTorch, Sockeye 3
provides faster model implementations and
more advanced features with a further stream-
lined codebase. This enables broader experi-
mentation with faster iteration, efﬁcient train-
ing of stronger and faster models, and the ﬂex-
ibility to move new ideas quickly from re-
search to production. When running compa-
rable models, Sockeye 3 is up to 126% faster
than other PyTorch implementations on GPUs
and up to 292% faster on CPUs. Sockeye
3 is open source software released under the
Apache 2.0 license.

1 Introduction

Sockeye1 provides a fast, reliable, and extensible
codebase for Neural Machine Translation (NMT).
As of version 3, Sockeye is based on PyTorch2
(Paszke et al., 2019), offering researchers a famil-
iar starting point for implementing their ideas
and running experiments. Sockeye’s distributed
mixed-precision training and quantized inference
also enable users to quickly build production-
ready NMT systems. Inference benchmarks show
that Sockeye is up to 126% faster than other Py-
Torch implementations on GPUs and up to 292%
faster on CPUs. Sockeye supports a range of ad-
vanced NMT features including source and target
factors, source and target preﬁxes, lexical short-
lists, and fast hybrid decoders. Sockeye powers
Amazon Translate3 and has been used in numer-
ous scientiﬁc publications.4 Sockeye is developed

*Corresponding authors:

{fhieber,mdenkows,domhant}@amazon.com
1https://github.com/awslabs/sockeye
2https://pytorch.org/
3https://aws.amazon.com/translate/
4https://github.com/awslabs/sockeye#

research-with-sockeye

as open source software on GitHub, where com-
munity contributions are welcome.

In the following sections, we describe Sock-
eye 3’s scalable training (§2), optimized inference
(§3), and advanced features (§4). We then share
the results of a PyTorch NMT benchmark (§5) and
case studies that use Sockeye features to imple-
ment formality and verbosity customization (§6).
We conclude with a discussion of Sockeye’s de-
velopment philosophy (§7) and include a minimal
usage example in an appendix (§A).

2 Training

Sockeye 3 implements optimized mixed precision
training that scales to any number of GPUs and
any size of training data.

2.1 Parallel Data Preparation

Sockeye provides an optional preprocessing step
that splits training data into random shards, con-
verts the shards to a binary format, and writes
them to disk. During training, the shards are se-
quentially loaded and unloaded to enable training
on arbitrarily large data with a ﬁxed memory bud-
get. Sockeye 3’s data preparation step supports
datasets of any size (subject to disk space) and
runs in parallel on any number of CPUs. See Sec-
tion A.2 for instructions on how to run data prepa-
ration.

2.2 Distributed Mixed Precision Training

By default, Sockeye training runs in FP32 on a sin-
gle GPU. Activating mixed precision mode runs
some or all of the model in FP16.5 This yields a
direct speedup from faster calculations and an indi-
rect speedup from ﬁtting larger batches into mem-

5PyTorch AMP runs a mix of FP16 and FP32 opera-
tions to balance speed and precision: https://pytorch.
org/docs/stable/amp.html. Apex AMP (O2) runs
the entire model in FP16 to maximize speed: https://
nvidia.github.io/apex/amp.html.

 
 
 
 
 
 
GPUs Precision

1
1
8
8

FP32
FP16 & FP32
FP32
FP16 & FP32

Tokens/Sec
8,451
28,287
65,280
218,688

Table 1: WMT14 En-De big transformer training
benchmark on a p3.16xlarge EC2 instance using the
large batch recipe described by Ott et al. (2018).

ory. Turning on distributed mode enables scal-
ing to any number of GPUs by launching separate
training processes that use PyTorch’s distributed
data parallelism6 to synchronize updates.
In all
cases, Sockeye traces the full encoder-decoder
model with PyTorch’s optimizing JIT compiler.7
Shown in Table 1, activating mixed precision
yields over 3X training throughput. Scaling to 8 lo-
cal GPUs yields 7.7X throughput, demonstrating
96.6% GPU efﬁciency.

3 Inference

Inference beneﬁts from previous development
for static computation graphs, avoiding dynamic
shapes and data-dependent control ﬂow as much
as possible. As such, we are able to trace various
components of the model with PyTorch’s JIT com-
piler (encoder, decoder, and beam search).

3.1 Quantization

By default, Sockeye runs inference with FP32
model weights. Quantizing these weights to FP16
or INT8 can increase translation speed and reduce
the model’s memory footprint. Enabling FP16
quantization for GPUs casts the entire model to
FP16 at runtime. Enabling INT8 quantization for
CPUs activates PyTorch’s dynamic quantization8
that runs linear layers (feed-forward networks) in
INT8 while keeping the rest of the model in FP32.
Both quantization strategies typically have mini-
mal impact on quality and are recommended for
most translation scenarios.

3.2 Efﬁcient Greedy Search

Work on high performance NMT reports that cer-
tain types of models produce adequate translations

6https://pytorch.org/docs/stable/

notes/ddp.html

7https://pytorch.org/docs/stable/jit.

html

8https://pytorch.org/tutorials/
recipes/recipes/dynamic_quantization.
html

Translation Speed (Sent/Sec) ↑
Baseline Greedy

GPU FP16
+Lexical Shortlist
CPU FP32
+Quantize INT8
+Lexical Shortlist

11.9
12.0
2.6
4.5
7.2

13.1
13.9
2.7
4.9
7.6

Table 2: Benchmark comparing Sockeye’s beam search
with size 1 (baseline) to greedy search for a big trans-
former (WMT17 En-De) with batch size 1. GPU in-
ference runs on a g4dn.xlarge EC2 instance and CPU
inference runs on a c5.2xlarge EC2 instance.

without beam search (Junczys-Dowmunt et al.,
2018). For such cases, Sockeye provides a dedi-
cated implementation of greedy search that does
not have the computational overhead of maintain-
ing hypotheses in a beam. Table 2 compares Sock-
eye’s beam search with a beam size of 1 to the
greedy implementation. Greedy search improves
translation speed by 16% on GPUs and 6% on
CPUs for a model that is already optimized for
speed (quantized weights and lexical shortlists).

4 Advanced Features

(Domhan et al., 2020)

Sockeye 3 migrates Sockeye 2’s advanced NMT
features
from MXNet
(Chen et al., 2015) to PyTorch. Sockeye 3 also in-
troduces new features that are exclusive to the Py-
Torch version.

4.1 Migrated Sockeye 2 Features

additional

representations

Source Factors (Sennrich and Haddow, 2016):
(factors)
Combine
with source word embeddings before running
the ﬁrst encoder layer. Factors can encode any
pre-computable token level information such as
original case or BPE type. This approach enables
combining the advantages of a smaller normalized
vocabulary (more examples of each type in the
training data) and a larger ﬁne-grained vocabulary
(distinguish between types with the same normal-
ized form but different original forms).

Lexical Shortlists (Devlin, 2017): When translat-
ing an input sequence, limit the target vocabulary
to the top k context free translations of each source
token. This can signiﬁcantly increase translation
speed by reducing the size of the output softmax
that runs at each decoding step. Sockeye provides
tools for generating shortlists from the training

big 6:6 transformer

big 20:2 transformer+SSRU

baseline

En-De +SF

+SF+TF
baseline

Ru-En +SF

+SF+TF

newstest
35.64
35.52
35.18
32.99
32.82
33.18

newstest-UP newstest

25.94
28.85
33.47
24.47
25.87
31.39

34.48
34.62
35.12
33.53
33.16
33.60

newstest-UP
24.97
27.37
32.82
25.99
26.72
31.49

Table 3: BLEU scores of different models on newstest and its all-uppercased version (newstest-UP). Using target
case factors (+SF+TF) achieves signiﬁcantly higher BLEU than using source case factors alone (+SF) and the
baseline for translating all-uppercased inputs. The training data is augmented with 1% all-uppercased pairs.

data with fast align9 (Dyer et al., 2013) and
uses a default value of k = 200.

4.2 SSRU Decoder

Sockeye supports replacing self-attention layers
in the decoder with Simpler Simple Recurrent
Units (SSRUs), which are shown to substantially
improve translation throughput (Kim et al., 2019).
An SSRU simpliﬁes the LSTM cell by removing
the reset gate and replacing the tanh non-linearity
with ReLU:

ft = σ(Wtxt + bf )
ct = ft ⊙ ct−1 + (1 − ft) ⊙ Wxt
ht = ReLU(ct)

Only the cell state ct requires sequential computa-
tions while other parts of the SSRU can be com-
puted in parallel.

4.3 Target Factors

Factored models have been used to enrich phrase-
based MT and NMT with linguistic features
(Koehn and Hoang, 2007; Garc´ıa-Mart´ınez et al.,
2016). They reduce the output space by decompos-
ing surface words y on different dimensions, such
as lemma and morphological tags, and maximize
P(yt|y<tx) = Qn

i |y<t, x).

i=1 P(f t

When target factors are enabled, Sockeye 3 pre-
dicts target words (f1) and attributes (f2...n) with
independent output layers, and the embeddings
of the word and attributes are combined for the
next decoder step. It incorporates the dependency
between words and attributes by time-shifting at-
tributes so that attributes at time t are actually pre-
dicted at time t + 1.

Following Niu et al. (2021), we test

the ef-
fectiveness of using target case factors in trans-
lating all-uppercased inputs. We use the same

9https://github.com/clab/fast_align

train/dev/test data processing procedures as in
other sections, except we (1) uppercase 1% train-
ing pairs and add them back to the training; (2)
truecase data and deduct case factors as detailed
in Niu et al. (2021), and (3) additionally evaluate
on all-uppercased newstest sets.

Results in Table 3 show that, with sub-optimal
data augmentation, using target case factors
(+SF+TF) achieves signiﬁcantly higher BLEU
scores than using source case factors alone (+SF)
and the baseline for translating all-uppercased in-
puts.

4.4 Fine-Tuning with Parameter Freezing

When ﬁne-tuning models, freezing some or most
of the parameters can increase training through-
put, avoid overﬁtting on small in-domain data, and
yield compact parameter sets for multitask or mul-
tilingual systems (Wuebker et al., 2018; Fan et al.,
2021). Sockeye supports freezing any model pa-
rameter by name as well as presets for freezing
everything except a speciﬁc part of the model (de-
coder, output layer, embeddings, etc.). When up-
dating only the decoder, Sockeye turns off au-
tograd for the encoder and skips its backward
pass. This yields faster training updates and low-
ers memory usage, which enables larger batch
sizes.

4.5 Source and Target Preﬁxes

Adding artiﬁcial source and target
tokens has
become a staple technique for NMT with ap-
ranging from multilingual models
plications
(Johnson et al., 2017) to output length customiza-
tion (Lakew et al., 2022). While these tokens
can be added to training data with simple pre-
processing scripts, adding them during inference
requires extended support
in the NMT toolkit.
Sockeye 3 enables users to specify arbitrary pre-

ﬁxes (sequences of tokens) on both the source and
target sides for any input. Source preﬁxes are au-
tomatically added to the beginning of each input.
When inputs are split into multiple chunks,10 the
source preﬁx is included at the beginning of each
chunk. When a target preﬁx is speciﬁed, Sockeye
3 forces the decoder to generate the N preﬁx to-
kens as the ﬁrst N decoder steps before continuing
the translation normally. Because target preﬁxes
have diverse use cases, Sockeye 3 allows users to
choose whether to apply preﬁxes when translating
all input chunks and whether to strip preﬁxes out
of the translation output. For instance, multilin-
gual NMT requires special tokens to be added to
each chunk to identify the output language, but re-
moves these artiﬁcial tokens from the ﬁnal trans-
lation. By contrast, continuing partial translations
requires that the preﬁx is only added to the ﬁrst
chunk and includes that preﬁx as part of the trans-
lation.

As an example of leveraging special tokens, let
us consider a multilingual NMT model where the
output language is speciﬁed on the source side. Us-
ing this model to translate into German requires
adding the token <2DE> to the beginning of each
input. Such source preﬁxes can be speciﬁed using
Sockeye’s JSON input format:

{"text": "The boy ate the waff@@
le .", "source_prefix": "<2DE>"}

This adds <2DE> to the beginning of each source
chunk. If the model uses special target tokens to
determine output language, a target preﬁx can be
speciﬁed:

{"text": "The boy ate the waff@@
le .", "target_prefix": "<2DE>"}

This forces the decoder to generate <2DE> as its
ﬁrst target token. Finally, Sockeye 3 supports
adding source and target preﬁx factors. For exam-
ple:

{"text": "The boy ate the waff@@
le .", "target_prefix": "<2DE>",
"target_prefix_factors": ["O O B"
]}

Here <2DE> is force-decoded as the ﬁrst target to-
ken and aligns with target factor O. The next two
target tokens after <2DE> are assigned target fac-
tors O and B.

10During Sockeye inference, inputs that exceed the maxi-
mum sequence length set during training are split into smaller
“chunks” that are translated independently.

4.6 Neural Vocabulary Selection

Instead of selecting the target vocabulary out of
context as in lexical shortlisting (§4), Neural Vo-
cabulary Selection (NVS) (Domhan et al., 2022)
uses the encoder’s hidden representation to pre-
dict the set of target words and is learned jointly
with the translation model. Similarly, it results in
lower translation latency via reduced computation
per decoder step. The advantage of NVS lies in
its simplicity, as no external alignment model is
required and predictions are made in context, re-
sulting in a higher recall of target words for given
target vocabulary size.

5 Benchmark

We conduct a reproducible benchmark of PyTorch-
based neural machine translation toolkits that in-
cludes Sockeye, Fairseq11 (Ott et al., 2019), and
OpenNMT12 (Klein et al., 2017). For each toolkit,
we train a standard big transformer model and run
inference on GPUs and CPUs. The scripts used to
conduct this benchmark are publicly available.13

5.1 Training

We select two translation tasks for which pre-
processed data sets are available: WMT17
English-German (5.9M sentences) and Russian-
English (25M sentences).14 We further pro-
cess the data by applying byte-pair encoding15
(Sennrich et al., 2016) with 32K operations and
ﬁltering out sentences longer than 95 tokens.
We use each toolkit to train a big transformer
model (Vaswani et al., 2017) on 8 local GPUs
(p3.16xlarge EC2 instance) using the large batch
recipe described by Ott et al. (2018). Models are
trained for either 25K updates (En-De) or 70K up-
dates (Ru-En) with checkpoints every 500 updates.
The 8 best checkpoints are averaged to produce the
ﬁnal model weights.

We use the fastest known settings for each
toolkit that do not change the model architecture or
training recipe. This includes enabling NVIDIA’s
Apex16 extensions for PyTorch and running the en-
tire model in FP16. Shown in Table 4, Sockeye

11https://github.com/pytorch/fairseq
12https://github.com/OpenNMT/OpenNMT-py
13https://github.com/awslabs/sockeye/

tree/arxiv_sockeye3/arxiv

14https://data.statmt.org/wmt17/

translation-task/preprocessed

15https://github.com/rsennrich/

subword-nmt

16https://github.com/NVIDIA/apex

WMT17 En-De

WMT17 Ru-En

Training Time (Hours) ↓ BLEU ↑ Training Time (Hours) ↓ BLEU ↑

Sockeye
Fairseq
OpenNMT

9.9
10.0
13.7

35.3
35.3
35.2

28.1
28.0
39.4

33.1
33.0
32.2

Table 4: Big transformer training benchmark using 8 GPUs on a p3.16xlarge EC2 instance. Models are trained
using the large batch recipe described by Ott et al. (2018) for either 25K (En-De) or 70K updates (Ru-En).

Translation Speed (Sent/Sec) ↑
Sockeye Fairseq OpenNMT

GPU FP16 Batch 64
+Lexical Shortlist
GPU FP16 Batch 1
+Lexical Shortlist
CPU FP32 Batch 1
+Quantize INT8
+Lexical Shortlist

67.8
76.0
8.4
9.5
1.2
2.4
4.7

66.1
–
3.2
–
1.1
–
–

47.8
–
4.2
–
1.2
–
–

Table 5: Big transformer WMT17 En-De inference benchmark. GPU inference runs on a g4dn.xlarge EC2 instance
and CPU inference runs on a c5.2xlarge EC2 instance. All reported values are averages over 3 runs. The listed
techniques do not signiﬁcantly impact translation quality; BLEU scores for all settings are within 0.2 of the FP32
baseline.

WMT17 En-De

WMT17 Ru-En

Training Time (Hours) ↓ BLEU ↑ Training Time (Hours) ↓ BLEU ↑

Big 6:6
Big 20:2
Big 20:2 SSRU

9.9
14.7
15.6

35.3
34.7
34.9

28.1
41.2
44.2

33.1
33.5
33.0

Table 6: Sockeye model architecture training benchmark using 8 GPUs on a p3.16xlarge EC2 instance. Models are
trained using the large batch recipe described by Ott et al. (2018) for either 25K updates (En-De) or 70K updates
(Ru-En). Model checkpoints are saved every 500 updates and the 8 best checkpoints are averaged.

Translation Speed (Sent/Sec) ↑
Big 6:6 Big 20:2 Big 20:2 SSRU

GPU FP16 Batch 64
GPU FP16 Batch 1
CPU INT8 Batch 1

73.8
9.9
4.5

116.3
17.4
7.8

142.8
18.5
9.5

Table 7: Sockeye model architecture WMT17 En-De inference benchmark. GPU inference runs on a g4dn.xlarge
EC2 instance and CPU inference runs on a c5.2xlarge EC2 instance. All conﬁgurations use lexical shortlists. All
reported values are averages over 3 runs.

and Fairseq are fastest, training models with com-
parable BLEU scores in comparable time.

6 Case Studies

6.1 Formality Control

5.2

Inference

We benchmark inference on GPUs (g4dn.xlarge
EC2 instance) and CPUs (c5.2xlarge EC2 instance
with 4 physical cores). Shown in Table 5, Sockeye
matches or outperforms other toolkits on GPUs
and CPUs with and without batching. When ac-
tivating NMT optimizations that are only natively
supported by Sockeye (lexical shortlists and CPU
INT8 quantization17), Sockeye is fastest across the
board: +15% for batched GPU inference, +126%
for non-batched GPU inference, and +292% for
CPU inference.

5.3 Alternate Model Architectures

Domhan et al. (2020) report that transformer mod-
els with deep encoders and shallow decoders
(20:2) can translate signiﬁcantly faster than stan-
dard models (6:6) with similar quality (±1 BLEU).
The speedup can be attributed to better paralleliza-
tion in the encoder (sequence-level operations ver-
sus per-step operations) and fewer calculations per
encoder layer (no encoder-decoder cross-attention
and a single forward pass versus beam search).

We benchmark three versions of Sockeye’s
transformer: (1) the standard big 6:6 model from
Section 5.1, (2) a big 20:2 model, and (3) a big
20:2 model that replaces decoder self-attention
with SSRUs as described in Section 4.2. Shown
in Tables 6 and 7, moving from a 6:6 model to a
20:2 model yields up to a 76% inference speedup
and moving to SSRUs yields up to a 23% addi-
tional speedup (87%-111% faster than the base-
line). These models do take longer to train. The
20:2 transformers have substantially more parame-
ters (46 sub-layers versus 30) and SSRUs do not
parallelize as well as self-attention during train-
ing. These trade-offs make models with deep
encoders, shallow decoders, and SSRUs a good
match for tasks where decoding time and costs
dominate. This includes experiments that translate
large amounts of data (e.g., back-translation) and
applications where NMT models are deployed for
large volume translation.

We present a case study on using Sockeye 3 trans-
former models with deep encoders and shallow
SSRU decoders (20:2) introduced in Sections 4.2
and 5.3, and the source preﬁx feature introduced in
Section 4.5. We train unconstrained baseline and
formality controlled models for 6 language pairs
for the 2022 IWSLT shared task on Formality Con-
trol for Spoken Language Translation.18 The base-
line models and ﬁne-tuning instructions are pub-
licly available.19

The English-German and English-Spanish mod-
els were trained on 20M pairs sampled from
ParaCrawl v9 (Ba˜n´on et al., 2020), using WMT
newstest for development. The English-Japanese
model was trained on all 10M pairs
from
JParaCrawl v2 (Morishita et al., 2020) using the
IWSLT17 development set. The English-Hindi
model was trained on all 15M pairs from CCMa-
trix (Schwenk et al., 2021), using the WMT news-
dev2014 for development and newstest2014 for
testing.

For evaluating generic quality, we used the
WMT newstests20 as well as the MuST-C test
sets (Di Gangi et al., 2019). To train and evaluate
formality-controlled models we use the CoCoA-
MT dataset and benchmark (N˘adejde et al., 2022).
We replicate the experiments in N˘adejde et al. us-
ing Sockeye 3 models: we ﬁne-tune the generic
baseline MT model on labeled contrastive trans-
lation pairs augmented by an equal number of
randomly sampled unlabeled generic training data.
The contrastive translation pairs are labeled using
a special source preﬁx that speciﬁes the formality
level of the target:

src: <FORMAL> ‘Are you tired?‘
trg: ‘Sind Sie muede?‘
src: <INFORMAL> ‘Are you tired?‘
trg: ‘Bist du muede?‘

At inference time, we use the source preﬁx to con-
trol the formality level in the output. We report
evaluation results in Table 8 showing formality-
controlled models have high targeted accuracy
while preserving generic quality.

18https://iwslt.org/2022/formality
19https://github.com/amazon-research/

contrastive-controlled-mt/tree/main/
IWSLT2022/models

17At the time of writing, activating OpenNMT’s INT8

20We used newstest 2020 for German, 2014 for Spanish,

mode does not appear to have any impact.

2014 for Hindi, 2020 for Japanese

M-ACC - CoCoA-MT test

BLEU

Lang.

EN-DE

EN-ES

EN-HI

EN-JA

System
generic
controlled
generic
controlled
generic
controlled
generic
controlled

F
-
97.8
-
89.1
-
96.3
-
68.8

I
-
45.0
-
47.8
-
36.7
-
83.2

Avg.
-
71.4
-
68.4
-
66.5
-
76.0

WMT
42.1
41.4
35.1
35.0
10.0
9.9
21.7
22.2

TED
32.7
32.1
36.7
36.9
-
-
14.3
14.3

Table 8: Accuracy of generic baseline and formality-controlled models on the CoCoA-MT test set. The TED test
sets are MuST-C for EN-DE,ES and IWSLT for EN-JA. For controlled models, M-Acc (F)/(I) scores are computed
using formal/informal translations respectively, resulting in performance upper bounds of 100%.

Lang. Pair

Test set

En-De

En-Fr

En-Es

MuST-C

IMT

MuST-C

IMT

MuST-C

IMT

System
Baseline
VC
Baseline
VC
VC+Rank
Baseline
VC
Baseline
VC
VC+Rank
Baseline
VC
Baseline
VC
VC+Rank

BERTScore
0.837
0.834
0.757
0.757
0.743
0.867
0.860
0.778
0.778
0.772
0.846
0.845
0.802
0.799
0.789

LC
41.3
56.6
51.5
56.5
63.5
38.7
53.6
39.5
58.0
65.5
60.0
66.7
59.0
62.5
64.0

BERTScore×LC
34.6
47.2
39.0
42.8
47.2
33.6
46.1
30.7
45.1
50.6
50.8
56.4
47.3
50.0
50.5

Table 9: Results comparing a standard NMT model (Baseline), NMT with output verbosity control (VC), and VC
with N-best re-ranking (VC+Rank) on the Ted Talks MuST-C test set released for the isometric MT (IMT) shared
task. Models are evaluated using BERTScore and length compliance within ±10% (LC), and the ﬁnal system
ranking metric (BERTScore×LC).

6.2

Isometric MT

We present another case study on Isometric MT
where the task is to generate translations similar
in length to the source text.
In this setup, we
experiment with the verbosity control (VC) work
of Lakew et al. (2019), speciﬁcally the length to-
ken approach using Sockeye’s source preﬁx im-
plementation (Section 4.5). We train and evalu-
ate models using data from the constrained set-
ting in the 2022 IWSLT shared task on Isomet-
ric Spoken Language Translation.21 Evaluation is
done on MuST-C v1.2 (Cattoni et al., 2021) and
a blind set released as part of the shared task.22
All models are evaluated on three language pairs:
English-German, English-French, and English-
Spanish using BERTScore (Zhang et al., 2020).
There is also a length compliance (LC) met-
ric (Lakew et al., 2022) which measures whether
the translation is within ±10% of the source length

21https://iwslt.org/2022/isometric
22https://github.com/amazon-research/

isometric-slt/tree/main/dataset

and a ﬁnal system ranking metric that combines
BERTScore and LC. For preprocessing, we lever-
age SentencePiece (Kudo and Richardson, 2018)
with 16.5K operations. Models are trained with
the transformer base (6:6) architecture on 8 GPUs
(p3.16xlarge instances). At training time, we ap-
ply <short>, <normal>, and <long> preﬁxes
to the source side of the parallel training data
based on the length compliance of the target side.
During inference, we add the <normal> preﬁx
to generate translations that are similar in length
to source. Following (Lakew et al., 2019), we also
run an ablation study for the blind set where we re-
rank the N -best list to ﬁnd the best translation in
terms of translation quality and length. We report
results in Table 9 that show improvements when
adding verbosity control (VC) to baseline models
and further improvements when applying N -best
re-ranking (VC+Rank).

7 Development

Sockeye is developed as open source software
under the Apache 2.0 license and hosted on
GitHub. All contributions are publicly reviewed
using GitHub’s pull request system. Sockeye is
written in PEP 8 compatible Python 3 code. Func-
tions are documented with Sphinx-style docstrings
and include type hints for static code analysis.
Sockeye includes an extensive suite of unit, in-
tegration, and system tests covering the toolkit’s
core functionality and advanced features. New
code is required to pass all tests (and add new tests
to cover new functionality) plus type checking and
linting in order to be merged. Sockeye 3 retires
some older features such as lexical constraints. We
welcome pull requests from community members
interested in porting these features from Sockeye
2.

8 Acknowledgements

We would like to thank Vincent Nguyen for help-
ing us conﬁgure OpenNMT for our benchmark.

References

Marta Ba˜n´on, Pinzhen Chen, Barry Haddow, Ken-
neth Heaﬁeld, Hieu Hoang, Miquel Espl`a-Gomis,
Mikel L. Forcada, Amir Kamran, Faheem Kirefu,
Philipp Koehn, Sergio Ortiz Rojas, Leopoldo
Pla Sempere, Gema Ram´ırez-S´anchez, Elsa
Sarr´ıas, Marek Strelec, Brian Thompson, William
Waites, Dion Wiggins, and Jaume Zaragoza. 2020.
ParaCrawl: Web-scale acquisition of parallel corpora.
In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages
4555–4567, Online. Association for Computational
Linguistics.

Mattia A. Di Gangi, Roldano Cattoni, Luisa Ben-
tivogli, Matteo Negri, and Marco Turchi. 2019.
MuST-C: a Multilingual Speech Translation Corpus.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 2012–
2017, Minneapolis, Minnesota. Association for
Computational Linguistics.

Tobias Domhan, Michael Denkowski, David Vilar,
Xing Niu, Felix Hieber, and Kenneth Heaﬁeld. 2020.
The sockeye 2 neural machine translation toolkit at AMTA 2020.
In Proceedings of the 14th Conference of the As-
sociation for Machine Translation in the Americas
(Volume 1: Research Track), pages 110–115, Vir-
tual. Association for Machine Translation in the
Americas.

Tobias Domhan, Eva Hasler, Ke Tran, Jonay Trenous,
Bill Byrne, and Felix Hieber. 2022. The devil is in
the details:on the pitfalls of vocabulary selection in
In Proceedings of the
neural machine translation.
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Seattle, USA. Associ-
ation for Computational Linguistics.

A.

Victor

Smith.

Dyer,
Noah

Chahuneau,
2013.

Chris
and
A simple, fast, and effective reparameterization of IBM model 2.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 644–648, Atlanta, Georgia. Association for
Computational Linguistics.

Angela Fan, Shruti Bhosale, Holger Schwenk,
Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,
Mandeep Baines, Onur Celebi, Guillaume Wen-
zek, Vishrav Chaudhary, Naman Goyal, Tom
Sergey Edunov,
Birch, Vitaliy Liptchinsky,
Michael Auli,
2021.
Beyond english-centric multilingual machine translation.
Journal of Machine Learning Research, 22(107):1–
48.

and Armand

Joulin.

Roldano Cattoni, Mattia Antonino Di Gangi, Luisa
Bentivogli, Matteo Negri, and Marco Turchi. 2021.
Mercedes
Must-c: A multilingual corpus for end-to-end speech translation.
rault,
Computer Speech & Language, 66:101155.
Factored neural machine translation architectures.
In Proceedings of the 13th International Workshop
on Spoken Language Translation, Seattle, US.

Garc´ıa-Mart´ınez,
and

Lo¨ıc
Bougares.

Bar-
2016.

Fethi

Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,
Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan
Zhang, and Zheng Zhang. 2015. Mxnet: A ﬂexi-
ble and efﬁcient machine learning library for hetero-
geneous distributed systems. In Neural Information
Processing Systems, Workshop on Machine Learn-
ing Systems.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda Vi´egas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google’s multilingual neural machine translation system: Enabling zero-shot translation.
Transactions of the Association for Computational
Linguistics, 5:339–351.

Jacob

Devlin.

2017.
Sharp models on dull hardware: Fast and accurate neural machine translation decoding on the CPU.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2820–2825, Copenhagen, Denmark. Association for
Computational Linguistics.

Junczys-Dowmunt,
Hieu
and

Heaﬁeld,
kiewicz,
Marian: Cost-effective high-quality neural machine translation in C++.

Kenneth
Grund-
2018.

Hoang,
Anthony

Roman
Aue.

Marcin

In Proceedings of the 2nd Workshop on Neural Ma-
chine Translation and Generation, pages 129–135,
Melbourne, Australia. Association for Computa-
tional Linguistics.

In Findings of the Association for Computational
Linguistics: NAACL 2022, Seattle, USA. Associa-
tion for Computational Linguistics.

Young Jin Kim, Marcin Junczys-Dowmunt, Hany
Hassan, Alham Fikri Aji, Kenneth Heaﬁeld, Ro-
man Grundkiewicz, and Nikolay Bogoychev. 2019.
From research to production and back: Ludicrously fast neural machine translation.
In Proceedings of the 3rd Workshop on Neural Gen-
eration and Translation, pages 280–288, Hong
Kong. Association for Computational Linguistics.

Myle

Ott,

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
fairseq: A fast, extensible
Michael Auli. 2019.
In Proceedings of
toolkit for sequence modeling.
NAACL-HLT 2019: Demonstrations.

and

Sergey

Michael

Edunov,

David
Grangier,
2018.
Scaling neural machine translation. In Proceedings
of the Third Conference on Machine Translation:
Research Papers, pages 1–9, Brussels, Belgium.
Association for Computational Linguistics.

Auli.

Guillaume Klein, Yoon Kim, Yuntian Deng,
2017.

Jean Senellart,
and Alexander Rush.
OpenNMT: Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada. Association
for Computational Linguistics.

Philipp

and

Hieu

Koehn

Hoang.

2007.
In Proceedings of
Factored translation models.
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868–876, Prague, Czech Republic. Associa-
tion for Computational Linguistics.

Taku

Kudo

and

John

Richardson.

2018.

Surafel Melaku

Lakew, Mattia Di Gangi,
2019.

Federico.

Marcello

and
Controlling the output length of neural machine translation.
In Proceedings of the 16th International Conference
on Spoken Language Translation, Hong Kong.
Association for Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. 2019.
Pytorch: An imperative style, high-performance deep learning library.
In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc.

Sergey
mand
CCMatrix: Mining billions of high-quality parallel sentences on the web.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
6490–6500, Online. Association for Computational
Linguistics.

SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.

Wenzek,
Ar-
2021.

Schwenk,
Edunov,

Grave,
Fan.

Guillaume

Edouard

Angela

Holger

Joulin,

and

Surafel Melaku Lakew, Yogesh Virkar, Prashant
Isometric
Mathur, and Marcello Federico. 2022.
MT: neural machine translation for automatic dub-
In Proceedings of the 2022 IEEE Interna-
bing.
tional Conference on Acoustics, Speech and Signal
Processing.

Rico

Sennrich

and

Barry Haddow.

2016.

Linguistic input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation: Volume 1, Research Papers, pages 83–
91, Berlin, Germany. Association for Computational
Linguistics.

Jun
Nagata.

Morishita,
Masaaki

Makoto
and
JParaCrawl: A large scale web-based English-Japanese parallel corpus.
In Proceedings of The 12th Language Resources
and Evaluation Conference, pages 3603–3609,
Marseille, France. European Language Resources
Association.

and
Neural machine translation of rare words with subword units.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1715–1725, Berlin, Germany.
Association for Computational Linguistics.

Suzuki,
2020.

Alexandra

Birch.

Haddow,
2016.

Sennrich,

Barry

Rico

Xing

Niu,

Georgiana

Dinu,

and

Mathur,
Faithful target attribute prediction in neural machine translation.
CoRR, abs/2109.12105.

Currey.

Anna

Prashant
2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Maria N˘adejde, Anna Currey, Benjamin Hsu, Xing
Niu, Marcello Federico, and Georgiana Dinu. 2022.
CoCoA-MT: A dataset and benchmark for Con-
trastive Controlled MT with application to formality.

Joern

Wuebker,
and

Patrick
DeNero.

Simi-
2018.

aner,
Compact personalized models for neural machine translation.

John

In Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing,
pages 881–886, Brussels, Belgium. Association for
Computational Linguistics.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kil-
and Yoav Artzi. 2020.

ian Q. Weinberger,
Bertscore: Evaluating text generation with bert.
In International Conference on Learning Represen-
tations.

A Installation and Usage

A.1 Installation

The easiest way to install Sockeye is via pip:

> pip3 install sockeye

This section covers a minimal example of using
Sockeye’s CLI tools. For a step-by-step tutorial
on training a standard transformer model on any
size of data, see the WMT 2014 English-German
example23 on GitHub.

is

included

installed,
Once Sockeye
you can use
command-line
the
train
tools
to
(sockeye-train),
models
data
translate
(sockeye-translate), and more.
If you
plan to extend or modify the code, you can install
Sockeye from source:

> git clone https://github.com/
awslabs/sockeye.git
> cd sockeye
> pip3 install --editable ./

Using the editable ﬂag means that changes to
the code will apply directly without needing to re-
install the package.

A.2 Sample Usage

Training a Sockeye model requires parallel (source
and target) training and validation data. You can
use raw training data directly, though we recom-
mend using sockeye-prepare-data to pre-
pare the data ahead of time. This reduces memory
consumption and data loading time during train-
ing:

> sockeye-prepare-data \

-s [source training data] \
-t [target training data] \
-o [output directory]

To

train

a model

from scratch,

run
sockeye-train with the prepared train-
ing data directory, validation source and target
data ﬁles, model output directory, and at least one
stopping criteria such as number of training steps:

> sockeye-train \

-d [prepared training data] \
-vs [source validation data] \
-vt [target validation data] \
-o [output directory] \
--max-updates [training steps]

To ﬁne-tune an existing model on new data (e.g.,
for domain adaptation), run sockeye-train
with the new data and specify a checkpoint from
the existing model with the --params argument.
Once you have trained a Sockeye model, you

can use it to translate inputs by running:

> sockeye-translate \
-m [model directory]

23https://github.com/awslabs/sockeye/

blob/main/docs/tutorials/wmt_large.md

