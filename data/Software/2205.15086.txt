2
2
0
2

y
a
M
0
3

]
E
S
.
s
c
[

1
v
6
8
0
5
1
.
5
0
2
2
:
v
i
X
r
a

Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

ARTICLE

Retrieving and Ranking Relevant JavaScript Technologies from Web
Repositories

Hernan C. Vazquez1 | J. Andres Diaz Pace1 | Claudia Marcos2 | Santiago Vidal*1

1ISISTAN-CONICET, Tandil, Buenos
Aires, Argentina
2ISISTAN-CIC, Tandil, Buenos Aires,
Argentina

Correspondence

Santiago Vidal, ISISTAN-CONICET.

Email: svidal@exa.unicen.edu.ar

Summary

The selection of software technologies is an important but complex task. We consider

developers of JavaScript (JS) applications, for whom the assessment of JS libraries has

become diﬃcult and time-consuming due to the growing number of technology options

available. A common strategy is to browse software repositories via search engines (e.g.,

NPM, or Google), although it brings some problems. First, given a technology need, the

engines might return a long list of results, which often causes information overload issues.

Second, the results should be ranked according to criteria of interest for the developer.

However, deciding how to weight these criteria to make a decision is not straightforward.

In this work, we propose a two-phase approach for assisting developers to retrieve and

rank JS technologies in a semi-automated fashion. The ﬁrst-phase (ST-Retrieval) uses a

meta-search technique for collecting JS technologies that meet the developer’s needs. The

second-phase (called ST-Rank), relies on a machine learning technique to infer, based on

criteria used by other projects in the Web, a ranking of the output of ST-Retrieval. We

evaluated our approach with NPM and obtained satisfactory results in terms of the accuracy

of the technologies retrieved and the order in which they were ranked.

KEYWORDS:
JavaScript, Information overloading, Web repositories

1

INTRODUCTION

Software technologies are an essential part of current software development practices. The use of software technologies, such as libraries and

frameworks, can greatly improve developers’ productivity. Nonetheless, an inappropriate technology selection can negatively aﬀect the software

product being built, and also the business goals of the organization 1. In this context, the task of selecting a software technology that fulﬁlls the

speciﬁc needs of a development task is generally a complex and time-consuming decision-making process. One of the reasons for this complexity

is the availability of a large number of software technologies in the market, in response to the growing demand from software companies. Keeping

up-to-date with technological developments is challenging for developers.

In particular, this situation is very common in JavaScript (JS) development, as developers have to regularly search, evaluate and compare candi-

date JS libraries/frameworks for their applications. This process can be perceived by developers as a “technological fatigue"1. This phenomenon is

due to the extensive number of JS technologies available in Web repositories, such as NPM2 (Node Package Manager), which promote reuse by

lowering production costs and speeding up software delivery 2.

1https://medium.com/@ericclemmons/javascript-fatigue-48d4011b6fc4
2https://www.npmjs.com/

 
 
 
 
 
 
2

H.C. Vazquez et al

We argue that one of the reasons for JS technological fatigue is the lack of precision in the search engines for JS repositories. As a consequence,

developers often resort to general-purpose search engines (e.g., Google or Bing) with the hope of having better results. However, the downside

of such engines is that they tend to return long lists of documents, and then developers have to navigate within each result to ﬁnd possible JS

technologies. This leads to information overload issues. Furthermore, once the developer identiﬁes a set of candidate technologies for her applica-

tion, she must analyzed each technology to decide which one better ﬁts her needs. Normally, this decision is driven by features of the technology,

such as: popularity in the community, or number of downloads, among others. Assigning weights to these features for comparison purposes is not

straightforward. For instance, NPM uses the AHP (Analytic Hierarchy Process) 3 technique to support comparisons of JS technologies.

The problem of information overload has been studied in various disciplines 4, and Web search engines are one of the main tools to face the

problem 5. The usage of search engines for Web-based software repositories has received some attention in the literature, but there are still

challenges for ﬁnding technologies being relevant to a particular development (or technological) need 6. Previous works on software technology

selection have addressed the problem from diﬀerent perspectives 7 8 9 10 11. Nonetheless, most works focus on the evaluation of candidate tech-

nologies based on speciﬁc characteristics, departing from a given group of technologies, and they leave aside the problem of searching for relevant

candidates. In addition, the characteristics for decision making are often manually assessed by developers.

In this article, we propose an approach to assist developers in searching and ranking JS technologies. The approach works in two phases
called ST-Retrieval and ST-Rank. Given a developer’s query expressing a JS technological need, ST-Retrieval applies a meta-search strategy 12, which
combines the search results of both a JS-speciﬁc engine and general-purpose engines. Based on the technologies recovered by ST-Retrieval, ST-
Rank generates a ranking of those technologies for the developer by means of a pair-wise learning to rank method. The ranking is based on an
(automated) analysis of technology features extracted from JS projects on the Web (e.g., number of stars in the repository, number of releases in

the last year, number of contributors, etc.). To assess the relevance of the projects, we employ a popularity metric called CDSel. CDSel measures

the relationship between the number of projects in which the technology was selected and how popular are those projects.

We have evaluated the approach on the NPM repository for JS, using a predeﬁned set of queries and 1000 popular projects from GitHub. For

ST-Retrieval, our experiments reported an average precision improvement of 20%, and we were able to recover a larger number of relevant JS
technologies than using the default search engine provided by NPM. Regarding ST-Rank, we observed improvements of at least 20% on average
when compared to the default ranking strategy followed by NPM. Based on these initial results, our approach makes two contributions: (i) it can

boost the performance of the NPM search system by leveraging on results provided by multiple search engines, and (ii) it helps developers by

ranking ﬁrst those technologies being widely used in the JS community.

The rest of the article is structured as follows. In Section 2, we provide a brief description of the software technology selection problem, along

with a motivational scenario in JS development. In Section 3, we describe the two phases of the approach in detail. Section 4 presents the evaluation

of ST-Retrieval and ST-Rank, and discusses their pros and cons. Section 5 covers related work. Finally, Section 6 gives the conclusions and outlines
future lines of work.

2

SOFTWARE TECHNOLOGY SELECTION

From a development perspective, the selection of software technologies has an inﬂuential role in both the development process and the quality

of the ﬁnal product 13. The successful application of a given technology (e.g., a JS package) means that its usage for a particular task produces a

desired objective 7. This success also depends on contextual features (e.g., alignment between the developer’s requirement and the chosen package,

maintenance of the package, type of license, or package usability, among others).

As a motivational example of the technology selection problem for JS development, let us consider a JS developer that needs to extract a

barcode from an image, with the goal of automating a process for extracting codes from a series of image ﬁles. The application context for this

functionality is a Web browser. Initially, she goes to the NPM package repository and submits the query “extract barcode from image” to its search
engine, which returns only the package bytescout3 as output. Bytescout is a JS client for a cloud service of the same name. When reading about
Bytescout, our developer ﬁnds out that it is a paid service and that the JS client is not open-source. Also, when she looks at the package description,

NPM reports that Bytescout has been downloaded 40 times in the last month, which for a JS package might indicate that it is not very popular in

the community. Let us assume that our developer is not convinced with these features, or they are not aligned with the standards of her project.

However, bytescout is the only technology returned by NPM. In this context, our developer has several options, namely: (i) adopt the package

despite her disagreement with its features, (ii) implement a solution from scratch to read the barcodes, (iii) try a modiﬁed query with the hope of

getting more results from NPM, or (iv) use alternative information sources (e.g. Google, or NPMSearch, among other engines) to ﬁnd alternative

technologies. Let us suppose here that she goes for the third option and re-phrases the query as “barcode reader", which makes NPM return now

3https://bytescout.com/

H.C. Vazquez et al

3

FIGURE 1 Overview of the two-phase approach.

16 results. After inspecting each result, our developer is still unconvinced of using any of the technologies, since they do not seem to be very

popular nor receive enough maintenance. The scenario so far shows the current limitations of JS-speciﬁc search engines, like NPM.

Next, let us assume that our developer decides to go for the fourth option, and she submits the query “extract barcode from image javascript

package" to Google (the last phrase intend to prevent Google form returning results for libraries in other languages). This query returns a list of Web

pages, and our developer then inspects each page in order to check whether some JS technologies are mentioned. In doing so, she realizes that a
technology called QuaggaJS4 is referenced in 3 results from the top-10 pages of the list. As she is not aware of this technology, she goes back to
the NPM repository and ﬁnds that QuaggaJS is more popular than bytescout, it is open-source, and is well-maintained by the community. At this

point, our developer can pick QuaggaJS for her development need, or keep looking for other technologies. This scenario illustrates the challenge

of using general-purpose search engines for JS, but also the issues related to the comparison of technologies.

The bottom line of the example has two implications. First, the search and comparison of JS technologies should take advantage of diﬀerent

information sources. Second, the developer’s manual analysis of technologies (e.g., by looking at Web sites) should be minimized, in the sense

that semi-automated rankings could be created from criteria or feedback provided by other projects. Based on these ideas, we developed the

ST-Retrieval and ST-Rank components of our approach.

3

APPROACH

For retrieving and ranking relevant JS technologies, we propose two-phase approach as shown in Figure 1. The ﬁrst phase, called ST-Retrieval,
takes a query given by the JS developer and returns a list of candidate technologies matching the query. The query is written in natural language

and speciﬁes a technological requirement (e.g. “extract barcode from image"). The goal of this phase is to leverage on various search engines.
Afterwards, the second phase, called ST-Rank, is fed with the outputs of ST-Retrieval along with an application context5 (i.e. Web browser, Node.js,
etc.) provided by the developer, and generates a ranking of JS technologies based on their relevance. The goal of this phase is to infer a ranking by

“learning" from technology features and decisions made in other projects. To do so, a machine learning (ML) model is built. Each phase internally

involves diﬀerent steps. Gray boxes in the ﬁgure correspond to steps or artifacts provided at setup, while white boxes refer to steps performed

every time a new query is entered by the developer.

3.1

ST-Retrieval

We model the search and retrieval of JS technologies as a meta-search problem 12. In meta-search, the original query is sent in parallel to a set of

search engines, each one returning an ordered list of items that satisfy the query. The meta-search system combines all the lists into a new one

that is expected to keep the “best" items of the individual lists.

4https://serratus.github.io/quaggaJS/
5Since some technologies only work in a speciﬁc context, our approach needs to be informed of the application context in which the developer is going

to apply the technology

ST-RetrievalST-RankJS DeveloperProcess queryExtract technologiesMerge technology results (1) QueryRepository of technologiesCreate datasetBuild ML ModelRank technologiesTraining datasetRanking model(3) Application context(2) Candidate technologies(4) Ranking of candidate technologiesCollect data4

H.C. Vazquez et al

The meta-search strategy for ST-Retrieval works in three steps (Figure 1), as follows. First, the input query is sent to both domain-speciﬁc engines
(e.g. NPM) and general-purpose engines (e.g., Google, Bing). The domain-speciﬁc engines directly return a list of JS technologies. However, the

general-purpose engines produce a list of Web pages, which might include names of JS technologies. Second, these Web pages are processed in

order to extract a list of JS technologies. Third, the lists from each search engine are merged into one single list, which is passed on to the second

phase.

The technical aspects of each step are explained below.

3.1.1

Process query

In this step, we aim at leveraging on the capabilities of existing search engines. Speciﬁcally, we use four engines, namely: NPM, NPMSearch6,
Google, and Bing7. In general, the possible engines envisioned in our approach fall into two types: (i) domain-speciﬁc (DS), and (ii) general-purpose
(GP). The former are especially designed for the search of JS technologies (NPM and NPMSearch), while the latter are often used for general queries
(Google and Bing).

Both types of engines allow developers to enter queries in natural language. Developers’ queries can express functional or non-functional

requirements that should be met by a JS technology. Stop-words (e.g., articles, pronouns, prepositions, etc.) that do not provide information to the

search engine are removed from the query. For GP engines, it also augment the query to avoid results from other domains (e.g., technologies in a
language incompatible with the project). Since we are retrieving JS technologies, the query is expanded with the suﬃx "javascript package". Once

the query was expanded, it is ready for execution in the corresponding engine. The result of this execution is a set of Web pages (or documents)

in HTML, XML or JSON format.

3.1.2

Extract technologies

This step is concerned with obtaining the technologies named in the documents being returned by a query. Technologies are represented as a tuple
< name, repository−url, home−url, description > where:

• name: corresponds to the name of the technology in the repository (e.g., “quagga”).

• repository_url: is the technology URL on the repository site (e.g., “https://www.npmjs.com/package/quagga”).

• home_url: is the URL of the technology site (e.g., “https://serratus.github.io/quaggaJS/”).

• description: is the description of the technology (e.g., “An advanced barcode-scanner written in JavaScript”).

On one hand, for DS engines, each result will map one-to-one with existing technologies in the repository via its name and repository_url. On
the other hand, for GP engines, the search results are Web documents, which might refer to zero, one or many technologies. For this reason, those
names of the technologies should be recovered. This information extraction task can be seen as a Named Entity Recognition (NER) problem. NER 14

aims at classifying entities found in a given text into predeﬁned categories (e.g., people, organizations, places, time expressions, among others).

In our work, the named entity category is "software technology", and we employ a rule-based strategy for string-matching 15. By means of these

rules, all the technologies whose name or address (home_url, repository_url) match the category are extracted.

This step requires a repository of JS technologies as input (Figure 1), which includes the information of the tuples. We generated this repository

in advance via a process of Web crawling on the NPM site8. Figure 2 shows an example of results and technologies obtained from the NPM and

Google engines, along with their mappings for the query “extract barcode from image". In the example, NPM returned a single result (bytescout)

matching the name of a technology in the repository. Unlike NPM, Google returned an HTML document, whose text was parsed for matches of

names or addresses of technologies. In particular, the technology name in the resulting page (QuaggaJS) did not match the technology name in the

repository (quagga) but the address of the site did match (home_url, https://serratus.github.io/quaggaJS/).

3.1.3

Merge technology results

Based on the set of named technologies, this step creates an ordered list of technologies per search engine. For DS engines, the sorting function is
straightforward, and each technology is assigned to the position of the result where it is named9. For GP engines, a result can be related to many

6https://npmsearch.com
7https://www.bing.com
8In order to complement the information found in NPM, we only selected technologies whose repositories were publicly available in GitHub
9this is because each result corresponds unequivocally with a technology

H.C. Vazquez et al

5

FIGURE 2 Matching search results with technologies for diﬀerent engines.

TABLE 1 Borda Fuse aggregation example ([points] name).

NPM

Google

Bing

Candidate list

[4] bytescout

[4] quagga

[4] quagga

[8] quagga

[3] bcreader

[3] bc-js

[6] bytescout

[2] bytescout

[1] jaguar

[2] bwip-js
[1] bcreader

[4] bcreader

[3] bc-js

[2] bwip-js
[1] jaguar

named technologies. In this case, the sorting function is based on the order in which the technologies were named. If two technologies are named

within the same result, the ﬁrst one named will appear in the list before than the one named afterwards. For example, if the ﬁrst result contains

the text "You can use Quagga or Barcode-Reader" and the next result contains the text "You should use Bytescout", then the candidates will be

ordered as 1) Quagga, 2) Barcode- Reader, and 3) Bytescout.

After all search engines produced their individual lists, they are merged into a single list. In meta-search, this process is known as a ranking

aggregation function 16. Aggregation functions can be classiﬁed into two main types 17: (i) those that use similarity scores returned by the search

engine, and (ii) those that use ranking positions. When engines do not expose their similarity scores for their results, as in the cases of Google or

Bing, ranking positions can be used instead. For this reason, we decided to use Borda Fuse in our approach. Borda Fuse is a positional aggregation
function that is computationally simple to implement 16, and it performs well in the context of Web searching 12.

In the Borda Fuse method, each search engine is considered as a voter. Each voter presents a list of n ordered candidates (i.e., technologies). For
each list, the best ﬁrst candidate receives n points, the second candidate receives n-1 points, and so on. Then, the points awarded by the diﬀerent
voters are added and the candidates are ranked in descending order according to the total of points obtained. Table 1 shows an aggregation example

using the Borda Fuse method. Each list gives a maximum of 4 points (equal to the maximum length of individual lists) and decreases by one to each
position of the list, i.e. the second position gives 3 points, the third position gives 2 points, and the last position gives 1 point. The scores are added

and a ﬁnal list from highest to lowest scores is created. In our example, the most relevant results, quagga and bytescout, are in the top-ranked

positions of the ﬁnal list.

3.2

ST-Rank

ST-Retrieval outputs a set of candidate technologies for the developer’s query. Since these technologies have the same goals, the developer has to
scrutinize them to ﬁnd the technology that best fulﬁlls her needs. This analysis often involves indicators provided by the repository (e.g., the number

of downloads, dependent projects, or contributors, among others) and also searching for developers’ opinions in blogs and forums. In this context,

extract barcode from imageNPM*resultsGoogle**resultsbytescoutname: bytescoutrepository_url: .../bytescouthome_url: nulldescription: ByteScout Cloud API extracts datafrom PDF, images, barcodes, spreadsheets andother functions.name: quaggarepository_url: .../quaggahome_url: https://serratus.github.io/quaggaJS/description: An advanced barcode-scannerwritten in JavaScript.Repository oftechnologies......html doc<p>There is <a href="https://serratus.github.io/quaggaJS/"rel="noreferrer">QuaggaJS</a> library (open source) for readingbarcodes, all done in JavaScript.</p>MatchNot MatchMatch* Domain-Speciﬁc Search Engine (type(i))** General Purpose Search Engine (type(ii)) 6

H.C. Vazquez et al

(a) Collect data

(b) Build dataset

FIGURE 3 Example of derivation of a training dataset.

ST-Rank assists developers to select the “best" technology by creating a ranking based on the choices made by other developers. The assistance
consists of four steps (Figure 1), as follows. First, information about popular technologies used by other developers in open-source repositories is

collected. Second, a dataset is created by taking into account the previous information and the application context of each technology (e.g., Web

browser, Node.js, etc.). A training dataset is derived from the dataset, and we apply a supervised machine learning algorithm to build a ranking

model for JS technologies. These three steps happen during the setup of the approach. At last, the machine learning model predicts a ranking for

the technologies given by ST-Retrieval, according to the “patterns" inferred from the training dataset.

The technical aspects of each step are explained below.

3.2.1

Collect data

The input of this step is a group of popular open-source JS projects. We are interested in the dependencies of every project and their features. In
NPM, these dependencies are in the package.json of each project10.

The processing of each dependency involves three tasks. The ﬁrst task is to look for alternatives to the technologies identiﬁed. For example,
project Chart.js11 depends on moment12, which is a library to manipulate dates (Figure 3). Thus, alternative technologies solving the same need
are sought, such as date-fns13 and momentjs14. We implemented this search by automatically scraping the website of NPMCompare15. For a given
technology, the website gives a list of related packages.

The second task is to assess the “popularity" of the dependency (or JS technology). To do so, we propose a metric called CDSel (Community
Degree of Selection) for a technology. CDSel is intended to capture the relationship between the number of projects in which the technology was
selected and the relevance of those projects. Let n be the number of reference projects, rel(Pi) the relevance of project i, and s = 1 when the
technology was selected in Pi (0 otherwise), then CDSel is computed as follows:

The logarithm serves as an attenuating factor in the ranking, so as to produce a controlled reduction in the values of the metric. For example,

in our dataset we obtained a CDSel value of 396.192 for moment, 15.646 for date-fns, and 1.791 for momentjs, which means that moment is most

CDSel =

n
(cid:88)

i=1

s ∗

rel(Pi)
log2(i + 1)

10Since both the “reference projects" and their dependencies are JS projects themselves, for the sake of clarity we refer to the former as “projects" and

to the latter as “technologies"

11https://www.npmjs.com/package/chart.js
12https://www.npmjs.com/package/moment
13https://www.npmjs.com/package/date-fns
14https://www.npmjs.com/package/momentjs
15https://npmcompare.com

Chart.jsmomentmomentjsdate-fnsTechnologyCDSelhas_tests#releases#maintainersmomentmomentjsdate-fns396.1921.79115.6461105ProjectDependencyAlternative0000132TechnologyRankingmomentmomentjsdate-fns123Technologyhas_tests#releases#maintainersmomentmomentjsdate-fns10.771000010.41, 0.77, 1,1, 0.77, 1,    0, 1, 0.4,0, 0, 0,  Training rankingNormalized featuresTraining instancesT#1T#2Label0, 0, 0,110 0, 1, 0.4,Training datasetH.C. Vazquez et al

7

often selected by relevant repositories than date-fns and momentjs (Figure 3). Regarding rel(Pi), it is computed as follows:

rel(P )i = z − (rankP osition(Pi))

where z is the length of the ranking and rankP osition(Pi) is the position in the ranking of project (Pi). The ranking is based on the stars of each
project in GitHub.

The third task is to access to the NPM and GitHub repositories for retrieving features that characterize each technology. We focus on features

that can be used as criteria for decision making, such as: number of developers that maintain the technology, #daily downloads, or #dependencies,

among others. More than 40 features are taken into account16. It should be noticed that all the technologies are represented with the same set of

features. It is up to the machine learning algorithm to decide which features are relevant. Our rationale is that, if a technology T was chosen in a
project (over other available options), then there should be a criterion over some technology features, upon which T was considered more relevant
than the other options. Thus, being able to learn the selection criteria depends on the features collected from the technologies.

3.2.2

Create dataset

This step assembles the dataset to be used for building the machine learning (ranking) model. We refer to this dataset as the training dataset (Figure
1), and it contains a set of training instances. A given training instance captures a pair of technologies.

Initially, a training ranking is computed for each technology according to its CDSel value. In our example, moment will be ranked ﬁrst since its CDSel
value is higher than the CDSel value of date-fns and momentjs. Then, each technology is represented as a feature vector [F Ti1, F Ti2, ..., F Tin]
where F T is a particular feature and n is the total number of features. Each vector is normalized via feature scaling 18, so that its values are between
0 and 1. For example, in Figure 3 vector (1, 10, 5) for moment becomes (1, 0.77, 1) after normalization.

At last, a set of training instances is created. Speciﬁcally, for each possible pair of technologies in a training ranking, a vector is created by
concatenation of their normalized feature vectors. A label of 1 is added to this vector when the ﬁrst technology is more relevant than the second
one, or 0 otherwise. In our example, the pair (moment,date-fns) is labeled to 1 because moment is ranked before date-fns.

3.2.3

Build ML model

This step applies a “learning-to-rank" 19 technique on the training instances of the dataset. A pairwise supervised variant is used, since the order

of two given technologies is not aﬀected by other technologies in the list. For example, moment is more relevant than date-fns regardless of other
alternative technologies. Among the various learning-to-rank algorithms reported in the literature 20, we chose GBRank 21 due to its eﬀectiveness

in Web search ranking 22,23,24. GBRank is based on a gradient boosting method for minimizing wrong preference predictions.

3.2.4

Rank technologies

Given two technologies, the ML model (above) is able to predict an order for them according to their relevance. Finally, this step takes all possible

pairs of technologies (from the Merge technology results step) and runs them through the ML model, in order to generate a ranking. The pairs are
ﬁltered according to the application context deﬁned by the JS developer. Those technologies being most relevant (i.e., popular) should be ranked

ﬁrst by the ML model.

4

EVALUATION

We performed an empirical evaluation of the ST-Retrieval and ST-Rank components using datasets sampled from JS projects. The following research
questions were addressed:

• RQ#1: How does the performance of ST-Retrieval compare to that of existing search engines?

• RQ#2: What is the quality of the rankings proposed by ST-Rank with respect to the rankings from NPM and NPMCompare?

The experiments to answer RQ#1 and RQ#2 are reported in sub-sections 4.1 and 4.2 respectively.

16The full list of features can be found in the Supplementary Material zip ﬁle at https://bit.ly/2w9sOzV.

8

H.C. Vazquez et al

TABLE 2 Reference Queries.

Queries

check valid email address

download web videos

send sms

quick sort algorithm

ﬁlter adult content images

user authentication

extract barcode from image

convert data formats

download free music

convert typewritten image to text

sentiment analysis

third party authentication

convert text to speech

calculate word similarity

translate english to spanish

credit card validation

health tracker

captcha authentication

detect text language

rank aggregation algorithms

mobile app framework

DOM manipulation utils

lightweight 3D graphic library

mathematical functions

scraper

4.1

Evaluation of ST-Retrieval

The main goal is to determine whether the performance of ST-Retrieval for retrieving a set of JS technologies is better than the performance of DS
and GP search engines.

4.1.1

Experimental Design and Operation

We used NPM as the basis for the evaluation, as NPM is the de-facto package repository for JS technologies. First, we downloaded the technology

registry from the NPM repository. With this information, we built the repository of technologies (Figure 1) to a given date (08/28/17), as described

in sub-section 3.1.2. Then, we asked two senior JS developers to record any search (i.e., queries) for technologies in NPM that they would make,

during 2 weeks, in their normal projects. After ﬁltering some of their search results (in order to remove very similar queries), we obtained a reference
set of 25 queries (Table 2) representing a variety of technological needs. We ran ST-Retrieval 25 times on this set (once for each query) and stored
the lists of outputted technologies. The length of the queries was selected as a normal distribution over the most usual length 25. The search

engines were: NPM, NPMSearch, Google, and Bing. We also ran a retrieval eﬀectiveness test for search engines 26.

During the Process query step, we only took into account the ﬁrst 20 documents from the list of results. This was because users searching the
Web (e.g., using Google) are very likely to consider only the ﬁrst results 27. These documents were recorded in order each time we ran the step. As

part of the Extract technologies step, we stored all the technologies identiﬁed. At the end, we retrieved a total of 2760 JS technologies (in general,
multiple technologies were extracted from the documents returned by Google and Bing). Then, in order to establish a baseline, we asked 12 JS
senior developers (diﬀerent from the ones that produced the reference set of queries) to assess the technology results. Each developer was given

all the technologies for a query in random order, and was asked to judge the technology relevance using a binary scale (yes/no). The developers did

not know which search engine produced a particular result, and duplicate results (i.e. results returned by more than one engine) were presented to

H.C. Vazquez et al

9

them only once. In particular, 11 developers analyzed the results of 2 queries and 1 developer analyzed the results of 3 queries. On average, each

JS developer analyzed 230 technologies.

4.1.2

Metrics

Information Retrieval metrics were used for gauging performance 28, such as: precision, recall, MAP (Mean Average Precision) and DCG (Discounted

Cumulative Gain). We leveraged on the reference set of queries and the technology baseline described previously.

In our domain, precision is the ratio between the number of relevant technologies recovered and the total number of technologies recovered.
The closer the precision value is to 1, the greater the number of relevant technologies recovered with ST-Retrieval.Recall is the ratio between the
number of relevant technologies recovered and the total relevant technologies known. A recall of 1 indicates that all relevant technologies have
been recovered. In addition, MAP 29 measures, for a set of queries, the average precision for each query. MAP values close to 1 mean that the
relevant technologies are within the top-ranked positions. At last, DCG 30 measures the quality of rankings, in terms of the utility (gain) of a result
based on its relevance and position in the ranking. DCG can be divided by the maximum value taken from among all the queries, and then, the

values for each query can be averaged to measure the average quality of the rankings. This variant of DCG is called Normalized DCG (nDCG).

4.1.3

Analysis of Results

Recall and Precision were calculated in diﬀerent positions of the ranking in order to establish the value of the metric in that position. In addition,
the reported results are the arithmetic mean of the values for each query.

FIGURE 4 Hits, Precision, and Recall at K position (ST-Retrieval).

Figure 4 shows the results for the four search engines and the Borda Fuse function considering each position (k) of the ranking (k = 1, ﬁrst
position). The ﬁrst chart shows the average number of hits (Hits). A result is said to be a hit if it is relevant to the query. The other charts show the
results for Precision, and Recall. As it can be observed, ST-Retrieval presents high values in all positions for all the metrics. Google and NPM behave
similarly to ST-Retrieval until around the third position. From that position on, ST-Retrieval exhibits a considerable improvement. We argue that this
improvement is due to the aggregation of results, and also to the ability of Borda Fuse to ﬁnd the global relevance of a result based on its position
in the results relative to the diﬀerent search engines. The maximum precision obtained by ST-Retrieval was 0.72 (8% increase when compared to
the search engines) and the maximum recall was of 0.77 (34% more than the search engines). This diﬀerence in performance is also observed in

the nDCG and MAP metrics for ranking quality metrics.

Table 3 shows the values for nDCG (nD) and MAP (M) for ranking sizes of 5, 10, and 20. Table 3 also shows the values of Hits (H) and Precision (P) in
positions 5, 10, and 20, respectively. Interestingly, Borda Fuse shows the best performance along all positions. In the case of the search engines, NPM
obtained the best results. For this reason, we compared Borda Fuse against NPM. For instance, NPM is outperformed by the BordaFuse technique
in 19.06% (in average) for all positions in Hits. This means that around position 5 Borda Fuse is able to return one more relevant technology than
NPM. Thus, the precision of NPM is outperformed by Borda Fuse by 19.06% (in average) over all positions.

Regarding the ranking quality, Borda Fuse obtained a nDCG of 22.04%, 20.5% and 25.9% higher than the same metric for NPM, for ranking
lengths of 5, 10 and 20 respectively. When it comes to MAP, the improvements of Borda Fuse with respect to NPM were of 27.37%, 25.16%, and
21.78% for ranking lengths of 5, 10 and 20 respectively. The improvement in ranking quality can be related to the improvements in precision and

recall. On one hand, by increasing the precision in the top-ranked positions, the relevant results near the top of the ranking increase and so do the

values of nDCG and MAP (since both metrics reward results in the top-ranked positions). On the other hand, by increasing recall, the amount of

relevant results that add up along the nDCG and MAP calculations also increases.

                                N                            U H F D O O 5 H F D O O  D W  N 1 S P M V 1 S P V H D U F K * R R J O H % L Q J 6 7  5 H W U L H Y D O  % R U G D ) X V H 10

H.C. Vazquez et al

TABLE 3 Metrics for search engines (ST-Retrieval).

Method/Metric H@1

H@5

H@10 H@20

P@5

P@10

P@20

nD(r=5)

nD(r=10)

nD(r=20) M(r=5) M(r=10) M(r=20)

NPM

0.600

2.680

4.760

NPMSearch

0.320

1.240

2.480

0.640

2.480

3.480

0.560

1.920

2.280

7.680

4.280

4.040

2.320

0.536

0.476

0.384

0.635

0.248

0.248

0.214

0.322

0.496

0.348

0.202

0.720

0.384

0.228

0.116

0.662

0.639

0.333

0.736

0.675

0.660

0.361

0.750

0.677

0.632

0.348

0.712

0.638

0.620

0.336

0.704

0.639

0.615

0.325

0.695

0.639

0.720

3.240

5.480

9.400

0.648

0.548

0.470

0.775

0.770

0.831

0.805

0.776

0.749

Google

Bing

BordaFuse

After observing that the performance values of ST-Retrieval were higher than those of the search engines,we tested the statistical signiﬁcance
of the results using the non-parametric Wilcoxon Signed Rank Test 31 with a signiﬁcance level α = 0.05. We stated the null hypothesis (H10)
as: “The metric values from ST-Retrieval are equal to those from other search engines". The alternative hypothesis (H11) states that there is a
diﬀerence between these metrics. Table 4 shows the p-values obtained for each metric (treatment) reported in Table 3 after running the tests.
With the exception of the number of hits at ﬁrst position (H@1), all p-value values are less than 0.05. This means that we can reject H10 for all the
metrics (except H@1), and conclude that the diﬀerences between ST-Retrieval and the existing search engines are statistically signiﬁcant. Finally,
we successfully answer RQ#1 by saying that ST-Retrieval does improve the search results.

TABLE 4 Comparison on the retrieval metrics using Wilcoxon signed rank test (ST-Retrieval).

ST-Retrieval vs H@1 H@5

H@10 H@20

P@5

P@10

P@20

nD(r=5)

nD(r=10)

nD(r=20) M(r=5) M(r=10) M(r=20)

NPM

NPMSearch

Google

Bing

.317

<.05

.317

.248

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.05

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.05

<.05

<.05

<.001

<.05

<.001

<.05

<.001

<.05

<.001

<.05

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

<.001

4.2

Evaluation of ST-Rank

The main goal is to determine whether ST-Rank improves the rankings generated by NPM.

4.2.1

Experimental Design and Operation

In order to obtain a “group of popular projects" for the Collect data step, we selected the top-1000 most popular projects according to GitHub,
from the repository of technologies (Figure 1) used in the ST-Retrieval experiment. NPM and NPMCompare were employed for obtaining features
and alternatives for each technology. In the Create dataset step, we then created around 250 training rankings of between 2 and 6 technologies
each. In total, more than 1000 training instances were created17.

To assess the rankings produced by ST-Rank, we deﬁned a test set by randomly removing 20% of the training rankings (along with their training
instances). Speciﬁcally, we shuﬄed the order of each training ranking. Then, we presented the technologies of each ranking to two senior JS
developers and asked them to produce reference rankings by sorting the technologies in descending order of relevance18. The remaining 80% of the
training instances were divided to perform a k-fold cross-validation 32 with k = 5, in order to train the ML model and ﬁnd the best conﬁguration of
hyper-parameters for GBRank. Speciﬁcally, we used random search 33 to explore the possible values of the hyper-parameters, and the values close
to the best random conﬁguration achieved by a grid search 34 were applied. In the end, GBRank was run with parameters learning_rate = 0.004,
max_depth = 50, min_samples_split = 50, and min_samples_leaf = 10.

For the application contexts, we manually classiﬁed the JS technologies based on their execution environment. We considered three possible

environments, namely: (i) technologies running in a Web browser (Web), (ii) technologies to be used in the Node.js execution environment (Node),

17The training dataset can be found in the Supplementary Material zip ﬁle at https://bit.ly/2w9sOzV.
18The reference rankings can be found in the Supplementary Material zip ﬁle at https://bit.ly/2w9sOzV.

H.C. Vazquez et al

11

and (iii) technologies that do not require a speciﬁc environment (No context). From these application contexts, we conﬁgured 5 possible scenarios,
as follows: (i) All - the developer makes no distinction among application contexts (i.e. the three execution environments are considered), (ii) Web
- the developer needs a technology for a Web browser (i.e. Web and No context are considered), (iii) Node - the developer needs a technology
for Node.js (Node and No context are considered), (iv) OnlyWeb - the developer needs a technology speciﬁcally developed for the Web browser
(only Web is considered), and (v) OnlyNode - the developer needs a technology speciﬁcally developed for Node.js (only Node is considered). Based
on these scenarios, we need ST-Rank to classify the inputted technologies and the training rankings. Thus, we ran ST-Rank 5 times, once for each
scenario. At last, we compared our results with the default ranking techniques supported by NPM and NPMCompare, which are the Analytic

Hierarchy Process (AHP) 3 and the Weighted Average Method (WAM) 35, respectively.

4.2.2

Metrics

We use three metrics to evaluate the performance of the rankings, namely: MAP (see Section 4.1.2), SRCC (Spearman Rank Correlation Coeﬃ-
cient) 36, and MRR (Mean Reciprocal Rank) 37. SRCC measures the correlation between the rankings created by ST-Rank (and also by AHP and WAM)
against the reference rankings created by the senior JS developers. In particular, we calculated SRCC for each ranking and then averaged them to

obtain a value that summarizes the “stability" of each technique. MRR, in turn, evaluates if the highest-ranked items are relevant. The closer the

MRR value is to 1, the greater the number of relevant technologies in the highest positions.

4.2.3

Analysis of Results

TABLE 5 Ranking results for diﬀerent scenarios (ST-Rank).

All

Web

Node

OnlyWeb

OnlyNode

Technique M@3 M@5 SRCC MRR M@3 M@5 SRCC MRR M@3 M@5 SRCC MRR M@3 M@5 SRCC MRR M@3 M@5 SRCC MRR

AHP

WAM

GBRank

0.517 0.541 0.454 0.654 0.756 0.762 0.731 0.865 0.541 0.561 0.365 0.667 0.785 0.725 0.621 0.880 0.461 0.478 0.310 0.672

0.813 0.798 0.656 0.863 0.744 0.729 0.669 0.836 0.793 0.806 0.668 0.869 0.928 0.928 0.878 0.952 0.822 0.796 0.750 0.883

0.925 0.914 0.788 0.915 0.910 0.864 0.835 0.942 0.874 0.889 0.886 0.932 0.952 0.962 0.950 0.964 0.933 0.909 0.907 0.966

Table 5 shows the metric values for the scenarios. In the case of MAP, we computed it for diﬀerent ranking lengths (M@3 and M@5 means MAP

considers a ranking of 3 and 5 technologies respectively). The values in bold in the columns correspond to the best value for the metric along that

column. As it can be observed, GBRank shows the best results for all the metrics. For example, for the All scenario, when considering MAP, GBRank
outperforms AHP and WAM by around 44% and 12% respectively. Similarly, in the case SRCC, GBRank improves the values of AHP and WAM

by around 42% and 17%. MRR shows smaller improvements of GBRank, 28% and 6% for AHP and WAM respectively. The diﬀerences between

the techniques are similar for the other scenarios. On average, the improvements of ST-Rank over AHP and WAM are of 10%, 20% at least, and
5% for MAP, SRCC, and MRR, respectively. The smallest improvement was that of MRR, although this result was expected. MRR only takes into

account the ﬁrst element of the ranking. That is, if the ﬁrst element of the reference ranking is in the ﬁrst position of the ranking being evaluated,

this ranking has a maximum value of MRR (although the other elements are disordered). However, in the problem of ranking of technologies, it

is important that the other elements are also ordered. If for some reason (e.g., due to a technical limitation) the ﬁrst technology cannot be used,

the rest of the technologies should be as orderly as possible. This might happen if the JS developer needs to develop a proof-of-concept, and she

should be able to ﬁnd the right technology as quickly as possible.

TABLE 6 Results of Wilcoxon signed rank test (ST-Rank).

ST-Rank vs

M@3

M@5

SRCC

MRR

AHP

WAM

4.41e−12
5.52e−05

1.31e−13
1.01e−06

2.35e−16
1.67e−10

4.27e−12
5.79e−04

To analyze if the results are statistically signiﬁcant, we tested the results using the non-parametric Wilcoxon Signed Rank Test with a signiﬁcance
level α = 0.05. We stated the null hypothesis (H20) as: “The metric values from ST-Rank are equal to those provided by the other techniques (i.e.

12

H.C. Vazquez et al

AHP and WAM)”. The alternative hypothesis (H21) states that there is a diﬀerence between these metrics. Table 6 shows the p-values obtained
for each metric reported in Table 5 after running the tests. As it can be seen, all p-value values are less than 0.05. This means that we can reject
H20 for all the metrics. Thus, we can conclude that the diﬀerences between ST-Rank and AHP and WAM are statistically signiﬁcant. In summary,
we successfully answer RQ#2 by saying that the ranking proposed by ST-Rank are better than the ranking generated by the existing engines.

4.3

Threats to Validity

A threat to construct validity has to do with the queries (reference set) and technology searches (baseline) used in the experiments. We tried to rely

on queries and searches being representative of real-world JS development. To this end, we extracted a dataset from the NPM repository using

the public JS package registry. While we used only 25 queries, they returned 2760 JS technologies that were manually analyzed. Since the analysis

of query results from search engines takes a substantial amount of time from experts, we preferred not to do a detailed query analysis and leave

this for future work.

A threat to internal validity is the usage of Borda Fuse to order the list of technologies in ST-Retrieval, which might have biased the results, and
also aﬀected the outputs of ST-Rank. Alternative aggregation methods could return diﬀerent orderings and should be explored in future works.
Similarly, for ST-Rank we used a particular learning-to-rank technique, but other techniques could have led to diﬀerent results.

Another threat is that the training rankings ST-Rank were based on the CDSel metric. This metric was chosen as a proxy for the popularity of JS
packages, but it might not correctly represent the relevance of the packages for the community. Thus, a better validation of this metrics should be

pursued.

To mitigate threats to external validity, we considered queries with diﬀerent sizes, purposes and domains. However, our dataset might not be

representative of all kinds of JS projects, and further experimentation and surveys of JS projects are necessary. Furthermore, the oﬀ-line evaluation

showed that the computations of ST-Retrieval sometimes take a considerable time, which might be a problem in an online search engine. We have
not consider yet the computation time of the search engines as a comparison factor.

5

RELATED WORK

Several approaches have been developed to support selection of software technologies 10. In general, these approaches are based on creating a list

of technologies that are compared and presented to developers, so they can decide which ones to apply to their projects. Some works have focused

on the evaluation of a set of pre-established candidate technologies and do not deal with the problem of searching/retrieving the technologies

from (Web) repositories. For example, given a set of predeﬁned candidates, Ernst et al. 38 proposes a scorecard to help developers to select a given

technology. The scorecard is based on performance, maintenance, and community criteria.

Software repositories 6 are one of the main sources for search/retrieval of candidate technologies. However, existing repositories have not been

very successful for this task, despite improvements in their underlying technology, such as the Web 6. One of the reasons is the performance of the

search engines, which sometimes fail to produce the desired results. Based on the above, several works have tried to improve the search oﬀered

by the repositories. As far as we know, no works about meta-search targeted to software technologies have been proposed. However, there are a

few works that bear similarities with our approach. Agora 39 is a research prototype that intended to replace the idea of software repositories by

creating a global database of JavaBeans and CORBA components, which was automatically generated and indexed. However, a major drawback

was its reliance on the syntactic interface of JavaBeans and CORBA to carry out its search function. In addition, Agora did not use search engines

for software repositories. Another recent work is Dolphin 40, which indexes open-source projects from version control repositories (e.g., OpenHub,

SourceForge) and ranks them according to their inﬂuence in discussions of forum communities (e.g., StackOverﬂow, OSChina). Our main diﬀerence

with this work is that Dolphin searches open-source code from version control repositories, while ST-Retrieval focuses on software released and
stored in repositories (e.g., NPM). Another diﬀerence is that Dolphin does not use general-purpose search engines.

LibFinder 41 is a search-based recommendation system for Java that uses multi-objective optimization to recommend software libraries, that

combine GitHub and Maven repositories. However, it does not allow user queries to ﬁnd technologies. Instead, it is based on the source code to

recommend libraries that can replace pieces of source code made by hand. LibFinder does not use general-purpose search engines. Soliman et.

al. 42 developed an approach to retrieve architectural design decisions and solution alternatives. The approach uses StackOverﬂow as an example

of an online repository of architecture knowledge. However, this approach is based on a mapping between text and a “de-facto" ontology, and its

applicability to other contexts is yet to be determined.

Regarding the order in which the technologies are created, there are works that address the problem from diﬀerent perspectives 8,7,9. However,

most of these works create ranking strategies manually, based on speciﬁc candidate characteristics. For example, Franch and Carvallo 43 propose

the adoption of a structured quality model to evaluate software packages. This model provides a taxonomy of software quality characteristics and

H.C. Vazquez et al

13

metrics to calculate its value for a given domain. Jadhav et. al 13 classiﬁes package ranking strategies into two groups, those using AHP and those

using WAM. Then, an expert system is used. This approach is diﬃcult to implement, since it depends on experts to construct rules in a manual

fashion. Grande et al. 10 deﬁne the selection problem as a multi-objective optimization problem and develop a framework for its treatment. They

apply genetic algorithms to solve the multi-objective problem. One limitation of this approach is the creation and maintenance of the repository

of technologies and their characteristics.

6

CONCLUSIONS

In this article we proposed an approach targeted to JS technologies based on two complementary phases, called ST-Retrieval and ST-Rank. ST-
Retrieval helps developers to search and retrieve JS technologies using a meta-search strategy. ST-Rank assists developers in ranking the candidate
JS technologies identiﬁed by the previous phase. To achieve this goal, ST-Rank uses a machine learning strategy that learns how to create rankings
of technologies from previous choices made by the development community in popular open-source projects.

An initial evaluation of the approach using the NPM repository showed satisfactory results. In the case of ST-Retrieval, precision and recall values
of around 80% were obtained, improving other search engines by approximately 20%. Regarding ST-Rank, improvements of around 5% for MRR,
10% for MAP, and 20% for SRCC, were obtained in the ranking metrics. Despite these results, the approach still presents some drawbacks. First,

the technology extraction function using string-matching in ST-Retrieval can be expensive in computational terms. This might limit the application
of the approach for online searching. For this reason, we will analyze the possibility of building an extraction function using other NER techniques

and exploring their inﬂuence on the ST-Retrieval results. Second the CDSel metric to measure the degree of selection of a technology in the
community in ST-Rank still needs further validation. Although reasonable results were obtained during the evaluations with CDSel using 1000
projects, considering alternative parameters in this metric can have an impact on the quality of the rankings.

As future work, in the case of ST-Retrieval, we will apply aggregation functions other than Borda Fuse. In the case of ST-Rank, we will explore
alternatives to GBRank for the learning-to-rank task. In addition, we plan to conduct a study with subjects (JS developers as users of our approach)

in order to corroborate our ﬁndings. Further work can extend our approach to technology repositories for other programming languages (e.g., Ruby,

Python or Java, among others). Finally, following ideas of XAI (eXplainable AI) 44, it would be interesting to add support for generating explanations

of the rankings, so that the comparison strategies are easier to understand by developers.

References

1. Lin HY, Hsu PY, Sheen GJ. A fuzzy-based decision-making procedure for data warehouse system selection. Expert systems with applications

2007; 32(3): 939–953.

2. Wittern E, Suter P, Rajagopalan S. A look at the dynamics of the JavaScript package ecosystem. In: ACM. ; 2016: 351–361.

3. Saaty TL. Decision making with the analytic hierarchy process. International journal of services sciences 2008; 1(1): 83–98.

4. Eppler MJ, Mengis J. The concept of information overload: A review of literature from organization science, accounting, marketing, MIS, and

related disciplines. The information society 2004; 20(5): 325–344.

5. Berghel H. Cyberspace 2000: Dealing with information overload. Communications of the ACM 1997; 40(2): 19–24.

6. Clayton N, Biddle R, Tempero E. A study of usability of Web-based software repositories. In: ; 2000: 51-58

7. Birk A. Modelling the application domains of software engineering technologies. In: IEEE. ; 1997: 291–292.

8. Basili VR, Rombach HD. Support for comprehensive reuse. Software engineering journal 1991; 6(5): 303–316.

9. Klein J, Gorton I. Design assistant for NoSQL technology selection. In: ACM. ; 2015: 7–12.

10. Grande ADS, Rodrigues RDF, Dias-Neto AC. A Framework to Support the Selection of Software Technologies by Search-Based Strategy. In:

IEEE. ; 2014: 979–983.

11. Vegas S, Basili V. A characterisation schema for software testing techniques. Empirical Software Engineering 2005; 10(4): 437–466.

12. Aslam JA, Montague M. Models for metasearch. In: ACM. ; 2001: 276–284.

14

H.C. Vazquez et al

13. Jadhav AS, Sonar RM. Framework for evaluation and selection of the software packages: A hybrid knowledge based system approach. Journal

of Systems and Software 2011; 84(8): 1394–1407.

14. Nadeau D, Sekine S. A survey of named entity recognition and classiﬁcation. Lingvisticae Investigationes 2007; 30(1): 3–26.

15. Navarro G. A guided tour to approximate string matching. ACM computing surveys (CSUR) 2001; 33(1): 31–88.

16. Dwork C, Kumar R, Naor M, Sivakumar D. Rank aggregation methods for the web. In: ACM. ; 2001: 613–622.

17. Liu TY. Learning to rank for information retrieval. Springer Science & Business Media . 2011.

18. Jain YK, Bhandare SK. Min max normalization based data perturbation method for privacy protection. International Journal of Computer &

Communication Technology 2011; 2(8): 45–50.

19. Li H. Learning to rank for information retrieval and natural language processing. Synthesis Lectures on Human Language Technologies 2011; 4(1):

1–113.

20. Li P, Wu Q, Burges CJ. Mcrank: Learning to rank using multiple classiﬁcation and gradient boosting. In: ; 2008: 897–904.

21. Zheng Z, Zha H, Zhang T, Chapelle O, Chen K, Sun G. A general boosting method and its application to learning ranking functions for web

search. In: ; 2008: 1697–1704.

22. Bai J, Diaz F, Chang Y, Zheng Z, Chen K. Cross-market model adaptation with pairwise preference data for web search ranking. In: Association

for Computational Linguistics. ; 2010: 18–26.

23. Kanungo T, Ghamrawi N, Kim KY, Wai L. Web search result summarization: title selection algorithms and user satisfaction. In: ACM. ; 2009:

1581–1584.

24. Bian J, Liu Y, Agichtein E, Zha H. Finding the right facts in the crowd: factoid question answering over social media. In: ACM. ; 2008: 467–476.

25. Taghavi M, Patel A, Schmidt N, Wills C, Tew Y. An analysis of web proxy logs with query distribution pattern approach for search engines.

Computer Standards & Interfaces 2012; 34(1): 162–170.

26. Lewandowski D. Evaluating the retrieval eﬀectiveness of Web search engines using a representative query sample. Journal of the Association

for Information Science and Technology 2015; 66(9): 1763–1775.

27. Höchstötter N, Lewandowski D. What users see–Structures in search engine results pages. Information Sciences 2009; 179(12): 1796–1812.

28. Büttcher S, Clarke CL, Cormack GV. Information retrieval: Implementing and evaluating search engines. Mit Press . 2016.

29. Caruana R, Niculescu-Mizil A. An empirical comparison of supervised learning algorithms. In: ACM. ; 2006: 161–168.

30. Järvelin K, Kekäläinen J. IR evaluation methods for retrieving highly relevant documents. In: ACM. ; 2000: 41–48.

31. Lavrenko V, Croft WB. Relevance-based language models. In: . 51. ACM. ; 2017: 260–267.

32. Refaeilzadeh P, Tang L, Liu H. Cross-validation. Encyclopedia of database systems 2009: 532–538.

33. Bergstra J, Bengio Y. Random search for hyper-parameter optimization. Journal of Machine Learning Research 2012; 13(Feb): 281–305.

34. Bergstra JS, Bardenet R, Bengio Y, Kégl B. Algorithms for hyper-parameter optimization. In: ; 2011: 2546–2554.

35. Perez M, Rojas T. Evaluation of Workﬂow-type software products: a case study. Information and software technology 2000; 42(7): 489–503.

36. Zwillinger D, Kokoska S. CRC standard probability and statistics tables and formulae. Crc Press . 1999.

37. Chapelle O, Metlzer D, Zhang Y, Grinspan P. Expected reciprocal rank for graded relevance. In: ACM. ; 2009: 621–630.

38. Ernst N, Kazman R, Bianco P. Component Comparison, Evaluation, and Selection: A Continuous Approach. In: IEEE; 2019.

39. Seacord RC, Hissam SA, Wallnau KC. Agora: A search engine for software components. IEEE Internet computing 1998; 2(6): 62.

H.C. Vazquez et al

15

40. Zhan Y, Yin G, Wang T, Yang C, Li Z, Wang H. Dolphin: A search engine for OSS based on crowd discussions across communities. In: IEEE. ;

2016: 599–605.

41. Ouni A, Kula RG, Kessentini M, Ishio T, German DM, Inoue K. Search-based software library recommendation using multi-objective

optimization. Information and Software Technology 2017; 83: 55–75.

42. Soliman M, Galster M, Riebisch M. Developing an Ontology for Architecture Knowledge from Developer Communities. In: IEEE. ; 2017: 89–92.

43. Franch X, Carvallo JP. A quality-model-based approach for describing and evaluating software packages. In: IEEE. ; 2002: 104–111.

44. Gunning D. Explainable artiﬁcial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web 2017.

How to cite this article: H. Vazquez, J.A. Diaz Pace, C. Marcos, and S. Vidal (2020), An Automated Machine Learning Approach for Retrieving and
Ranking Relevant JavaScript Technologies from Web Repositories, Software Evolution and Process, 2017;00:1–6.

