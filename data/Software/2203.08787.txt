Exploring Variational Graph Auto-Encoders for Extract Class
Refactoring Recommendation

Pritom Saha Akash
University of Illinois at Urbana-Champaign
pakash2@illinois.edu

2
2
0
2

r
a

M
6
1

]
E
S
.
s
c
[

1
v
7
8
7
8
0
.
3
0
2
2
:
v
i
X
r
a

Abstract

The code smell is a sign of design and develop-
ment ﬂaws in a software system that reduces
the reusability and maintainability of the sys-
tem. Refactoring is done as an ongoing prac-
tice to remove the code smell from the pro-
gram code. Among different code smells, the
God class or Blob is one of the most common
code smells. A god class contains too many
responsibilities, violating object-oriented pro-
gramming design’s low coupling and high co-
hesiveness principles. This paper proposes an
automatic approach to extracting a God class
into multiple smaller classes with more spe-
ciﬁc responsibilities. To do this, we ﬁrst con-
struct a graph of methods (as nodes) for the
concerning god class. The edge between any
two methods is determined by their structural
similarity, and the feature for each method is
initialized using different semantic represen-
tation methods. Then, the variational graph
auto-encoder is used to learn a vector repre-
sentation for each method. Finally, the learned
vectors are used to cluster methods into differ-
ent groups to be recommended as refactored
classes. We assessed the proposed framework
using three different class cohesion metrics
on sixteen actual God Classes collected from
two well-known open-source systems. We
also conducted a comparative study of our ap-
proach with a similar existing approach and
found that the proposed approach generated
better results for almost all the God Classes
used in the experiment.

1

Introduction

Code smells are deﬁned as the coding practices
that reﬂect ﬂaws in design and implementation
phases in computer programming. Code smells
deteriorate the quality, understandability, and main-
tainability of a source code in the long run, though
there is no technical inaccuracy caused by any code
smells. It violates one of the fundamental rules
of Object-Oriented Programming (OOP) concept

that is, a class should focus on a speciﬁc respon-
sibility (high cohesion) with limited dependency
on other classes (low coupling). However, during
software evolution, the system’s internal structure
undergoes many changes and manipulations. For
which, the structure of a class may go away from
its original design and become complex by incorpo-
rating additional responsibilities. This type of code
smell is called God class (GC) or Blob (Brown
et al., 1998). As a GC violates the programming
principles by increasing coupling and decreasing
cohesion within classes and eventually makes it
difﬁcult for the developers to maintain the system,
it is necessary to refactor a GC into smaller, more
speciﬁc classes. And the refactoring process for
this smell is referred to as Extract class refactoring
(Fowler, 2018).

Numerous studies have been conducted in the
area of extract class refactoring.
(Bavota et al.,
2010) proposed an approach by following two steps
where the method chains in a class are extracted
from cohesion within methods. For this, both struc-
tural and semantic similarities between methods
are considered. Then, using this between methods
similarity matrix, a graph is built, and the edges are
cut based on a predeﬁned threshold. A follow-up
study was conducted to empirically evaluate the ef-
fectiveness of their tool on real GCs from existing
open-source systems (Bavota et al., 2014). An-
other related work based on the MaxFlow-MinCut
algorithm was proposed to split a low cohesive
class into two classes with higher cohesion. How-
ever, this approach can only refactor a GC into
two classes. In (Bavota et al., 2014), an approach
for extracting a GC into multiple classes was pro-
posed by exploiting both structural and semantic
similarity between methods. In this work, the se-
mantic similarity between classes was calculated
with Latent Semantic Indexing (Deerwester et al.,
1990). However, there is the problem of sparsity in
the vector representation of a code snippet (i.e., a

 
 
 
 
 
 
method in a class).

Different from (Bavota et al., 2014), recently,
(Akash et al., 2019) proposed an approach by us-
ing topic modeling where each method in a class
is regarded as a document and a topic distribution
is generated using the Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) model for each class.
Finally, cosine similarity between two methods’
topic distribution is used as their semantic simi-
larity. One major limitation of this method is that
the topic model naturally needs a large amount of
textual content to train the model. However, a class
generally does not have that much textual (both
code blocks and in-code documentation) content to
train the topic model successfully. Therefore, the
learned topic distribution may not be appropriate
to represent the semantic of a method in a class.

On the other hand, one common behavior of
the above approaches for refactoring is that they
use a simple weighted combination of different
similarities between methods. To be more speciﬁc,
the structural and semantic similarities between the
two methods are combined by taking a weighted
sum. However, this combination of two different
similarities in the same space is not intuitive. There
should be a more complex relationship between the
structural and semantic similarity between the two
methods in the actual scenario.

Therefore, to solve this problem, we leverage
the graph structure of a class consisting of its
methods as nodes and edges between them. More
speciﬁcally, we use the structural similarity within
methods to construct the graph topology and ex-
plore the different semantic representations of each
method to initialize its conceptual attribute. Now,
to learn the latent representation of each method in
embedding space, we apply a neural graph-based
framework called variational graph autoencoder
(VGAE) (Kipf and Welling, 2016b). This model
uses a graph-convolutional network (GCN) (Kipf
and Welling, 2016a) encoder and a simple inner
product decoder to learn the embedding. The GCN
can encode both the graph’s structure and the fea-
tures of the nodes, thus combining both structural
and semantic relatedness between methods in a
class.

2 Proposed Framework

The proposed framework consists of two main
steps. The ﬁrst step is the method by method simi-
larity matrix calculation, where each entry of the

matrix represents how much proximate two meth-
ods are to be included in the same class. The sec-
ond step is to cluster the methods into different
groups based on the calculated similarity matrix in
the previous step. The overview of the proposed
framework is shown in Figure 1.

2.1 Method-by-Method Similarity Matrix

Calculation

For calculating the method-by-method similarity
matrix, we ﬁrst construct a graph encoding the
structural similarity among methods in the con-
cerning class. After that, we initialize a feature
representation for each method based on its seman-
tic proprieties. After that, the constructed graph
and the feature matrix are used as input to varia-
tional graph auto-encoders (VGAE) to learn each
method’s lower-dimensional latent representation
(vector). Finally, the learned vector representation
of each method is used to calculate the method-by-
method similarity matrix based on cosine similarity.
The following sections describe the details of each
step.

2.1.1 Class Graph Construction

For constructing a graph representing the structure
of a class, we need to compute an adjacency matrix.
Here, the nodes are methods in the class, and the
edge between two nodes represents whether there
is a predeﬁned minimum structural similarity be-
tween two nodes (i.e., methods). For this, we ﬁrst
need to calculate the structural similarity between
the two methods. We use two well-known met-
rics to capture the structural similarity between
methods, and they are Structural Similarity be-
tween Methods (SSM) (Gui and Scott, 2006), and
Call-based Dependence between Methods (CDM)
(Bavota et al., 2011). Let mi and mj are two meth-
ods, then SSM between these methods is calculated
as follows:

SSMi,j =

(cid:40) |Vi∩Vj |
|Vi∪Vj |
0

if |Vi ∪ Vj| (cid:54)= 0
otherwise.

(1)

where Vi and Vj refers to the instance variables ac-
cessed/used both methods mi and mj respectively.
The higher value of SSM means two methods are
structurally similar to be included in the same class.
On the other hand, the CDM indicates the value
of how two methods in a class are related by
method calls from each other. Two are structurally
proximate if they call each other frequently. The

Figure 1: Overview of the system architecture

2.1.2 Method Feature Initialization

Only capturing the structural properties of methods
in a class is not enough to thoroughly understand
the meaning and characteristics of methods. A
method itself contains rich information. For exam-
ple, as a computer programmer or expert, we can
naturally understand what a method is responsible
for by looking through its code snippets, includ-
ing text documentation and comments appearing in
it. This textual information is essential to get the
semantic meaning of the method and helps under-
stand the relationship between any two methods.
An example of a method snippet from an open-
source project named Xerces1 is shown in Figure
2. This ﬁgure shows the codes, the text documen-
tation, and comments in a speciﬁc method of a
class. However, the computer does not understand
a text by itself, and for this, we need to represent
that in machine-readable vector form. For this pur-
pose, we use this code snippet for each method to
turn into a bag of words. Then this bag of words is
used to get a vector representation from various em-
bedding methods (described in Section 3.4). This
vector representation for each method is used as its
feature to be utilized in the next step.

2.1.3 Latent Representation of Methods

Now, we have a graph of the given GC representing
its structural properties with the relation between
its methods. We also have a semantic feature for
each method calculated using textual information
from code snippet of that method. As we said be-
fore, we are interested in combining both structural
and semantic properties of methods in a class to
represent its latent meaning. By doing so, we will

1https://github.com/apache/xerces2-
j/tree/trunk/src/org/apache/xerces/parsers

Figure 2: A partial method snippet from DOMParser-
Impl.java class from an opensource project Xerces

CDM between two methods mi and mj is calcu-
lated as below:

CDMi→j =

(cid:40) calls(mi,mj )
callsin(mj )
0

if callsin(mj) (cid:54)= 0
otherwise.

(2)
where calls(mi, mj) denotes the number of times
method mi called the method mi and callsin(mj)
is the total number of incoming calls to method mj.
Finally, overall CDM between methods mi and mj
is calculated as:

CDMi,j = max{CDMi→j, CDMj→i}

(3)

Now, two structural similarity metrics SSM and
CDM are combined with equal weight to determine
whether there should be an edge between the two
methods. More speciﬁcally, if there is a non-zero
structural similarity score (combination of SSM
and CDM) between the two methods, we draw an
edge between these two methods. Therefore, we
have a graph constructed on methods as nodes and
edges between them by structural similarity.

be able to calculate the similarity between meth-
ods for deciding whether the methods should be
extracted to be included in a new refactored class.
For this purpose, some previous work (Akash et al.,
2019; Bavota et al., 2014; Jeba et al., 2020) con-
sider linearly combining structural and semantic
similarity using weights. However, this simple lin-
ear weighted combination is not appropriate for
capturing complex relationships between structural
and semantic properties of methods in a method.
Rather, considering a class as an attributed network
represented by a graph topology constructed by the
structural similarity between methods and node at-
tributes calculated considering semantic properties
of methods and using this attributed network to
learn a latent representation of each method make
more sense to encode the complex relationships
between two methods in that class.

For the above purpose, we utilize a recent
framework named variational graph autoencoder
(VGAE) for unsupervised learning on attributed
graph-structured data based on the variational
auto-encoder (VAE) (Kingma and Welling, 2013;
Rezende et al., 2014). VGAE learns interpretable
latent vector representations of undirected graphs
using latent variables. Similar to the original paper,
in our study, we use a convolutional graph network
(GCN) (Kipf and Welling, 2016a) based encoder
to encode the attributed network (graph from struc-
tural similarity and semantic feature from previ-
ous steps) nodes into a latent lower-dimensional
representation and a simple inner product decoder
to reconstruct the network from the latent lower-
dimensional representation. The overview of the
VGAE process is shown in Figure 1 where we can
see the similarity matrix generated adjacency ma-
trix and semantic features of each node are fed
into a GCN encoder to get a latent representation
matrix Z, which is then used to reconstruct the
graph based on a Dot Product decoder. Finally,
this latent lower-dimensional representation matrix
of methods ( Z) is used to calculate the method-
by-method similarity matrix based on the cosine
similarity score.

2.2 Clustering

ture) clustering algorithm (Ankerst et al., 1999) to
cluster the methods. The ﬁrst reason for using OP-
TICS for clustering is that it can ﬁnd the clusters
without pre-specifying the number of clusters be-
forehand. On the other hand, we can deﬁne the
minimum number of methods that should be in-
cluded in a cluster. This requirement is important,
while we do not want the extracted class with very
few methods. Finally, the cluster of methods in a
class is used as extracted classes as a recommenda-
tion to be refactored.

System

God Class

LOC #Methods

AbstractDOMParser
AbstractSAXParser
BaseMarkupSerializer
CoreDocumentImpl
DeferredDocumentImpl
DOMNormalizer
DOMParserImpl
DurationImpl
NonValidatingConﬁguration
XIncludeHandler

GanttOptions
GanttProject
GanttGraphicArea
GanttTaskPropertiesBean
ResourceLoadGraphicArea
TaskImpl

2658
2418
1968
2871
2159
2043
1393
2021
807
3082

1016
1137
366
707
1060
1210

Xerces

GanttProject

45
55
61
119
76
31
17
45
18
111

68
90
43
27
29
46

Table 1: God classes used in the experiment

3 Experimental Setup

In this section, we empirically evaluate the perfor-
mance of the proposed framework of extract-class
refactoring.

3.1 Research Questions

To guide the empirical evaluation of the proposed
framework, we seek to answer the following re-
search questions:

• (RQ1) Does VGAE improve the results
of class refactoring from simple weighted
combination-based baselines?

• (RQ2) What is the overall best performing
model considering the cohesion and coupling
of the newly extracted classes?

• (RQ3) Do sub-classes after refactoring show
better class properties than the original class?

In this step, we use the computed method-by-
method similarity matrix to cluster the methods
into different groups; each is denoted as a refac-
tored subclass. For this purpose, we use OPTICS
(Ordering Points To Identify the Clustering Struc-

3.2 Dataset

To evaluate the effectiveness of the proposed frame-
work with existing ones, we applied the frame-
work to refactor the actual sixteen GCs from two

(a)

(b)

(c)

(d)

Figure 3: Comparative results (average LCOM) between WC and VGAE in 16 classes using four different feature
initialization.

(a)

(b)

(c)

(d)

Figure 4: Comparative results (average MPC) between WC and VGAE in 16 classes using four different feature
initialization.

well-known open-source systems called Xerces and
GanttProject. Table 1 summarizes the properties
of each class, i.e., the line of codes (LOC) and the
number of methods in a class.

3.3

Implementation Details

There are some parameters in both baselines and
the proposed models we have to set. For example,
to combine two structural and semantic similari-
ties in WC-based baselines, we use equal weights
(1/3) for all three similarities (SSM, CDM, and
CSM). Similarly, in VGAE based methods, to com-
bine two structural similarities (SSM and CDM)
while constructing the class graph, we use equal
weights of 0.5. We follow the same pipeline to
preprocess text and code in each method for a fair
comparison. More speciﬁcally, we ﬁrst tokenize
the text in source code based on space, camel case,
underscore, special character, and numeric. Then,
we remove stop words, including programming
language-speciﬁc keywords in the source code. Af-
ter that, we lemmatize each token using Spacy Nat-
ural Language Processing 2. Finally, all the tokens

2https://spacy.io/

are converted to lower-case.

3.4 Models

• WC + LSI. This baseline is designed fol-
lowing an existing approach of God class
refactoring in (Bavota et al., 2014). This ap-
proach combines structural similarity using
SSM and CDM with LSI-based semantic sim-
ilarity with equal weights. For instance, we
get three similarity matrices MSSM , MCDM
and MCSM corresponding SSM, CDM and
LSI based CSM. These three matrices are
combined with equal weights to get the ﬁ-
nal method by method similarity matrix M .
For calculation of MCSM , each method in
the class is represented by an LSI generated
vector using the code snippet of that method
alongside comments and code documentation.
Finally, the similarity M is used to cluster the
methods into multiple classes.

• WC + LDA. This baseline is also an existing
approach (Akash et al., 2019). It has a simi-
lar overall framework to the previous baseline.
Rather than using LSI, this baseline uses LDA

Classes

GanttGraphicArea
GanttOptions
GanttProject
GanttTaskPropertiesBean
ResourceLoadGraphicArea
TaskImpl
AbstractDOMParser
AbstractSAXParser
BaseMarkupSerializer
CoreDocumentImpl
DeferredDocumentImpl
DOMNormalizer
DOMParserImpl
DurationImpl
NonValidatingConﬁguration
XIncludeHandler

WC+
LSI

94.33
149.67
174.86
48.0
57.0
1039.6
33.67
191.5
128.8
808.8
171.67
372.0
126.17
53.25
36.5
188.62

WC+
LDA

WC+
BERT

WC+
CodeBERT

VGAE+
LSI

VGAE+
LDA

VGAE+
BERT

VGAE+
CodeBERT

158.5
1183.0
264.17
53.0
20.67
957.2
28.0
90.6
108.0
810.4
124.0
357.0
214.2
270.0
37.5
177.38

96.67
335.5
179.67
141.5
153.0
1156.0
38.0
213.75
489.5
826.4
109.5
370.0
324.0
99.33
157.0
574.4

241.0
1059.0
238.33
152.0
153.0
825.0
37.33
221.75
477.5
1095.67
120.33
150.0
514.67
58.75
13.0
201.22

46.5
43.5
64.33
17.0
11.33
178.33
6.75
33.0
41.17
96.92
36.0
65.0
64.29
18.6
13.0
55.91

80.33
85.0
73.44
38.67
15.33
267.4
6.75
43.5
71.0
129.9
26.62
73.83
46.12
13.6
6.0
49.1

21.6
42.62
39.7
21.0
12.0
167.3
43.0
42.38
45.0
65.45
31.5
34.86
74.86
15.67
13.67
67.0

656.0
76.71
70.0
37.0
152.0
206.0
6.75
57.88
18.83
175.7
97.33
71.67
2249.0
37.4
156.0
89.8

Table 2: Average LCOM results after refactoring (smaller is better)

Classes

GanttGraphicArea
GanttOptions
GanttProject
GanttTaskPropertiesBean
ResourceLoadGraphicArea
TaskImpl
AbstractDOMParser
AbstractSAXParser
BaseMarkupSerializer
CoreDocumentImpl
DeferredDocumentImpl
DOMNormalizer
DOMParserImpl
DurationImpl
NonValidatingConﬁguration
XIncludeHandler

WC+
LSI

12.67
15.17
29.29
42.33
19.0
15.0
55.33
16.0
20.4
25.4
13.33
40.67
7.83
13.25
8.5
17.88

WC+
LDA

18.5
41.5
30.33
42.67
13.0
18.2
38.6
10.6
18.8
22.6
10.25
40.33
10.0
17.5
9.0
18.75

WC+
BERT

WC+
CodeBERT

VGAE+
LSI

VGAE+
LDA

VGAE+
BERT

VGAE+
CodeBERT

12.67
20.0
32.67
59.0
35.0
20.75
55.67
16.0
35.0
25.2
11.5
49.0
12.0
17.33
17.0
22.6

18.0
39.5
30.5
59.5
35.0
16.8
57.0
14.25
45.5
36.0
13.0
35.75
15.33
13.25
5.67
18.0

9.5
11.38
24.67
31.75
13.33
10.78
40.25
9.88
17.0
13.0
9.0
25.83
6.43
9.4
5.67
16.64

12.33
11.86
23.56
41.33
12.67
10.8
42.25
11.0
19.8
12.3
7.38
26.0
5.75
7.8
4.25
20.3

8.8
10.75
21.6
34.5
13.33
9.3
48.0
8.75
15.17
14.09
8.38
22.57
6.86
10.83
6.33
17.1

33.0
12.43
24.56
38.0
34.0
11.0
43.5
9.12
17.83
15.7
8.17
28.33
42.0
9.6
16.0
19.7

Table 3: Average MPC results after refactoring (smaller is better)

for semantically representing each method in
a class. More speciﬁcally, this method uses
textual information from the methods to ﬁnd
a set of topics and their distribution using the
LDA topic modeling algorithm (Blei et al.,
2003) by considering each method as a doc-
ument. Then, the topic distribution for each
method is used as the semantic representation
of that method and used to calculate the cosine
similarity between any two methods.

• WC + BERT. This baseline also uses the
weighted combination of structural and se-
mantic similarity. For calculating semantic
similarity, each method is represented in a
vector space using sentence BERT (Reimers
and Gurevych, 2019). Then cosine similarity
between two corresponding vectors is calcu-
lated.

• WC + CodeBERT. It is precisely similar to

the WC+BERT model, except instead of using
BERT for method representation, CodeBERT
(Feng et al., 2020) is used to get vector repre-
sentation of each method in the class. Code-
Bert is a pre-trained language model speciﬁed
for programming codes, is used to encode a
code snippet (i.e., a method in a class) into a
vector space.

• VGAE + LSI. This is the proposed frame-
work where the structural similarity combin-
ing SSM and CDM is used to construct the
class graph. For the initial node feature, each
method’s LSI-based vector representation is
used. Then, VGAE (Kipf and Welling, 2016b)
is used to learn a latent semantic representa-
tion of each method. Finally, the learned vec-
tors of methods are used to cluster them into
groups that are recommended as refactored
classes.

• VGAE + LDA. This proposed framework is
similar to the previous. However, unlike the
previous one, in this approach, instead of us-
ing LSI-based vector representation for the
initial node feature, we used LDA-based vec-
tor representation (same representation used
in WC+LDA).

• VGAE + BERT. In this VGAE based frame-
work, we use vector representation of each
method by BERT pre-trained language model
as initial node feature representation (same
representation used in WC+BERT). The rest
of the framework is similar to the previous
one.

• VGAE + CodeBERT. In this VGAE based
framework, we use vector representation of
each method by CodeBERT pre-trained lan-
guage model for programming language code
as initial node feature representation (same
representation used in WC+CodeBERT). The
rest of the framework is similar to the previous
one.

3.5 Evaluation Metrics

To assess the performance of proposed models, we
use two evaluation metrics named Lack of Cohe-
sion of Methods (LCOM) (Li and Henry, 1993) and
Message Passing Coupling (MPC) (Li and Henry,
1993). LCOM counts the difference between the
number of pairs of methods that do not share any in-
stance variables and the number of pairs where two
methods share at least one instance variable. Hence,
the lower value of LCOM is better and means there
is high cohesion in the class. On the other hand,
MPC measures the number of method calls de-
ﬁned in methods of a class to methods in other
classes. Again, lower MPC is better because the
class should have lower coupling with other classes
according to the object-oriented design principle.

4 Results and Discussion

4.1 Performance of VGAE over Weighted

Combining (WC)

RQ1: To answer the ﬁrst research question, we
compare the average LCOM and average MPC
scores of extracted classes over 16 GCs between
WC and VGAE based methods.

Figures 3 and 4 show the result of LCOM and
MPC respectively. From ﬁgure 3, for the cases of
almost all of the 16 GCs and four different feature

initialization, we can see the average LCOM scores
over the extracted classes by VGAE based methods
are far lower than that of WC-based methods. This
result means that VGAE based methods extract
GCs to sub-classes with more cohesiveness among
the methods in the sub-classes compared to the
simple WC-based methods.

Similarly, from ﬁgure 4, we can observe that
the average MPC score over the extracted classes
by VGAE based methods is considerably lower
than that of WC-based methods in almost of all of
the cases. It means that the coupling among the
extracted classes by VGAE based methods is lower
than that of sim WC-based methods.

As per the object-oriented design principles, we
know that it is desirable to have higher cohesion
among methods in a class and lower coupling
among classes. From the above result, we can see
that the VGAE based methods can achieve higher
cohesion and lower coupling among the extracted
classes than the simple WC-based methods. There-
fore, we can easily say that the use of VGAE to
combine structural and semantic properties of meth-
ods in a class has considerable improvement over
the simple weighted combination in terms of per-
formance.

4.2 Overall Best Performing Model

RQ2: To answer the second research question, we
compare the results of six models over 16 GCs us-
ing the two metrics LCOM and MPC. More specif-
ically, we calculate the average LCOM and MPC
over the extracted classes for each of the sixteen
GCs by six compared models.

Table 2 shows the comparative results using av-
erage LCOM score for six models. From the Table,
we can see the almost all the best performing mod-
els in terms of average LCOM are from the VGAE
based models. Among VGAE based models, no sin-
gle method marginally outperforms others in most
of the sixteen GCs. However, considering over-
all LCOM scores, we can see the VGAE+BERT
and VGAE+LDA models attain the lowest average
LCOM for 5 out of 16 GCs by each. On the other
hand, Table shows the comparative results using the
average MPC score for six models. In this Table,
we again see the best performing are the VGAE
based models. Among the VGAE based models,
we observe that the VGAE+BERT obtains lower
average MPC scores in 7 out of 16 GCs, where the
VGAE+LDA is the second-best performing model

System

Class Names

Before
Refactoring

After Refactoring

GanttProject

Average

Xerces

LCOM MPC #splits

LCOM

MPC

GanttGraphicArea
GanttOptions
GanttProject
GanttTaskPropertiesBean
ResourceLoadGraphicArea
TaskImpl

657.0
2626.0
3632.0
419.0
153.0
7491.0

34
62
147
97
35
48

2496.33

70.50

AbstractDOMParser
AbstractSAXParser
BaseMarkupSerializer
CoreDocumentImpl
DeferredDocumentImpl
DOMNormalizer
DOMParserImpl
DurationImpl
NonValidatingConﬁguration
XIncludeHandler

4.0
2308.0
1004.0
6589.0
987.0
1729.0
2250.0
716.0
157.0
4790.0

130
38
64
76
20
102
43
23
17
83

5
8
10
4
3
10

4
8
6
11
8
7
7
6
3
10

64.0, 10.0, 0, 13.0, 21.0
218.0, 26.0, 8.0, 20.0, 9.0, 9.0, 12.0, 39.0
323.0, 16.0, 0, 21.0, 0, 9.0, 15.0, 3.0, 10.0, 0
13.0, 0, 0, 71.0
8.0, 28.0, 0
1568.0, 6.0, 10.0, 15.0, 36.0, 0, 30.0, 0, 0, 8.0

12, 7, 11, 2, 12
64, 0, 17, 0, 4, 0, 0, 1
45, 32, 6, 8, 26, 14, 16, 47, 4, 18
68, 23, 2, 45
7, 13, 20
38, 5, 0, 0, 9, 3, 11, 1, 26, 0

65.98

15.43

172.0, 0, 0, 0
339.0, 0, 0, 0, 0, 0, 0, 0
250.0, 10.0, 10.0, 0, 0, 0
333.0, 0, 0, 20.0, 0, 66.0, 151.0, 10.0, 10.0, 120.0, 10.0
235.0, 7.0, 10.0, 0, 0, 0, 0, 0
174.0, 7.0, 10.0, 4.0, 0, 28.0, 21.0
433.0, 4.0, 10.0, 10.0, 55.0, 9.0, 3.0
27.0, 0, 0, 21.0, 36.0, 10.0
28.0, 2.0, 11.0
639.0, 0, 0, 11.0, 1.0, 0, 0, 19.0, 0, 0

27, 47, 48, 70
28, 2, 18, 1, 7, 7, 3, 4
64, 0, 1, 3, 15, 8
48, 7, 4, 24, 4, 44, 19, 4, 0, 1, 0
19, 2, 12, 5, 9, 17, 2, 1
39, 82, 3, 23, 11, 0, 0
8, 16, 0, 0, 0, 0, 24
30, 9, 11, 2, 9, 4
6, 3, 10
81, 3, 5, 5, 33, 8, 18, 8, 10, 0

Average

2053.4

59.6

47.51

14.8

Table 4: Cohesion (LCOM) and Coupling (MPC) results obtained before and after refactoring the 16 God Classes.

in terms of average MPC score.

Therefore, considering both LCOM and MPC,
we can say that overall, the VGAE+BERT model
is the best performing model to extract GCs into
sub-classes with higher cohesion and lower cou-
pling. Interestingly, the VGAE+CodeBERT model
does not achieve as good performance in extract-
ing GCs as that of the VGAE+BERT model. The
initial motivation of using CodeBERT was that as
CodeBERT is a pre-trained LM dedicated to pro-
gramming source code, it will be better to encode
the semantic meaning of source code alongside
syntactic information than BERT. However, this
intuition may not be correct as we can not see the
reﬂection of that intuition in the result. The reason
is that while representing a method semantically,
the only important thing that matters is the text in
the code rather than its syntax. Therefore, BERT is
better to capture the semantic meaning of a method
than CodeBERT.

4.3

Improvement over Original Class

RQ3: To answer the third research question, we
compare the result of LCOM and MPC on each
of the original 16 GCs before and after refactoring
by the best performing model VGAE+BERT. The
result is shown in Table 4. From the Table, it is
clearly observable that for all the 16 GCs, the co-
hesion is increased among extracted classes after
refactoring(by lower LCOM), and the coupling (by
lower MPC) decreased among the extracted classes
It indicates that the proposed
after refactoring.
refactoring model for extracting GCs is effective
in ﬁnding sub-classes with speciﬁc responsibilities.
For instance, the LCOM and MPC values of Gantt-

GraphicArea were 657 and 34, respectively. The
proposed approach refactors the class into ﬁve sub-
classes. Each of the LCOM scores (64, 10, 0, 13,
and 21) of all newly generated classes is substan-
tially lower than the original class. Similarly, each
of the MPC scores of newly generated classes is
also lower than the original one (12,7,11,2 and 12).
Therefore, this result indicates that the sub-classes
after refactoring using the proposed method show
better class properties than the original one.

5 Conclusion

This paper proposes a new framework for refac-
toring a God class into sub-classes. After refactor-
ing using the proposed framework, the extracted
sub-classes obtain higher cohesion and lower cou-
pling. The proposed approach combines the struc-
tural and semantic properties of methods in a God
class based on the variational graph auto-encoder
(VGAE) model. We conducted an empirical exper-
iment over 16 actual GCs from two open-source
systems to evaluate the performance of the pro-
posed framework. The experimental result shows
that the proposed framework outperforms the exist-
ing and well-designed baseline with better cohesion
and coupling score. One limitation of the proposed
study is that it can only give suggestions of extract-
ing a predeﬁned GC into sub-classes. It can’t detect
whether a class is a God class or not. As a future
research direction, we plan to propose an automatic
approach to detecting and extracting GCs.

Thomas N Kipf and Max Welling. 2016a.

Semi-
supervised classiﬁcation with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Thomas N Kipf and Max Welling. 2016b. Vari-
arXiv preprint

ational graph auto-encoders.
arXiv:1611.07308.

Wei Li and Sallie Henry. 1993. Maintenance metrics
In [1993] Pro-
for the object oriented paradigm.
ceedings First International Software Metrics Sym-
posium, pages 52–60. IEEE.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
networks. arXiv preprint arXiv:1908.10084.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014.
Stochastic backpropagation and
approximate inference in deep generative models.
In International conference on machine learning,
pages 1278–1286. PMLR.

References

Pritom Saha Akash, Ali Zafar Sadiq, and Ahmedul
Kabir. 2019. An approach of extracting god class
exploiting both structural and semantic similarity.

Mihael Ankerst, Markus M Breunig, Hans-Peter
Kriegel, and Jörg Sander. 1999. Optics: Ordering
points to identify the clustering structure. ACM Sig-
mod record, 28(2):49–60.

Gabriele Bavota, Andrea De Lucia, Andrian Marcus,
and Rocco Oliveto. 2014. Automating extract class
refactoring: an improved method and its evaluation.
Empirical Software Engineering, 19(6):1617–1664.

Gabriele Bavota, Andrea De Lucia, and Rocco Oliveto.
2011. Identifying extract class refactoring opportu-
nities using structural and semantic cohesion mea-
sures. Journal of Systems and Software, 84(3):397–
414.

Gabriele Bavota, Rocco Oliveto, Andrea De Lu-
cia, Giuliano Antoniol, and Yann-Gaël Guéhéneuc.
2010. Playing with refactoring: Identifying extract
In 2010
class opportunities through game theory.
IEEE International Conference on Software Mainte-
nance, pages 1–5. IEEE.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

William H Brown, Raphael C Malveau, Hays W" Skip"
McCormick, and Thomas J Mowbray. 1998. An-
tiPatterns: refactoring software, architectures, and
projects in crisis. John Wiley & Sons, Inc.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
Indexing by latent semantic analysis. Jour-
1990.
nal of the American society for information science,
41(6):391–407.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. arXiv preprint arXiv:2002.08155.

Martin Fowler. 2018. Refactoring: improving the de-
sign of existing code. Addison-Wesley Professional.

Gui Gui and Paul D Scott. 2006. Coupling and cohe-
sion measures for evaluation of component reusabil-
ity. In Proceedings of the 2006 international work-
shop on Mining software repositories, pages 18–21.

Tahmim Jeba, Tarek Mahmuda, Pritom S Akashb, and
Nadia Naharb. 2020. God class refactoring recom-
mendation and extraction using context based group-
ing.

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114.

