2
2
0
2

n
u
J

1
1

]
E
S
.
s
c
[

1
v
7
5
4
5
0
.
6
0
2
2
:
v
i
X
r
a

Testing Ocean Software with Metamorphic Testing

Quang-Hung Luu
Swinburne University of Technology
Hawthorn, Victoria, Australia
hluu@swin.edu.au

Tsong Yueh Chen
Swinburne University of Technology
Hawthorn, Victoria, Australia
tychen@swin.edu.au

Huai Liu
Swinburne University of Technology
Hawthorn, Victoria, Australia
hliu@swin.edu.au

Hai L. Vu
Monash University
Clayton, Victoria, Australia
hai.vu@monash.edu

ABSTRACT
Advancing ocean science has a signiﬁcant impact to the develop-
ment of the world, from operating a safe navigation for vessels
to maintaining a healthy and diverse ocean ecosystem. Various
ocean software systems have been extensively adopted for diﬀer-
ent purposes, for instance, predicting hourly sea level elevation
across shorelines, simulating large-scale ocean circulations, as well
as integrating into Earth system models for weather forecasts and
climate projections. Regardless of their signiﬁcance, guaranteeing
the trustworthiness of ocean software and modelling systems is a
long-standing challenge. The testing of ocean software suﬀers a
lot from the so-called oracle problem, which refers to the absence
of test oracles mainly due to the nonlinear interactions of multi-
ple physical variables and the high complexity in computation. In
the ocean, observed tidal signals are distorted by non-deterministic
physical variables, hindering us from knowing the “true” astro-
nomical tidal constituents existing in the timeseries. In this paper,
we present how to test tidal analysis and prediction (TAP) soft-
ware based on metamorphic testing (MT), a simple yet eﬀective
testing approach to the oracle problem. In particular, we construct
metamorphic relations from the periodic property of astronomical
tide, and then use them to successfully detect a real-life defect in
an open-source TAP software. We also conduct a series of exper-
iments to further demonstrate the applicability and eﬀectiveness
of MT in the testing of TAP software. Our study not only justiﬁes
the potential of MT in testing more complex ocean software and
modelling systems, but also can be expanded to assess and improve
the quality of a broader range of scientiﬁc simulation software sys-
tems.

CCS CONCEPTS
• Applied computing → Earth and atmospheric sciences; •
Software and its engineering → Software testing and debug-
ging.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MET’22, May 9, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9307-2/22/05. . . $15.00
https://doi.org/10.1145/3524846.3527341

KEYWORDS
Metamorphic testing, metamorphic relation, ocean software, scien-
tiﬁc simulation, tidal analysis and prediction

ACM Reference Format:
Quang-Hung Luu, Huai Liu, Tsong Yueh Chen, and Hai L. Vu. 2022. Testing
Ocean Software with Metamorphic Testing. In 7th International Workshop
on Metamorphic Testing (MET’22), May 9, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 8 pages. https://doi.org/10.1145/3524846.3527341

1 INTRODUCTION
Our Earth is actually a world of water as over 70% of its surface
is covered by the ocean. Ocean science, also being referred to as
marine science or oceanography, is an important discipline of the
Earth sciences, covers a wide range of topics, from ocean circula-
tion, waves, storm surge, sea level rise, heat content, water trans-
port to ecosystem dynamics, ocean acidiﬁcation and plate tectonics
of the sea bed [38]. In the current computing age, various software
systems have been developed to support the ocean modelling and
computations, which help our scientists simulate, model and pre-
dict ocean conditions, and thus assist the relevant industries and
human activities as well as contribute to advance other disciplines
[8].

Being a dynamic of the ocean, astronomical tide is the periodic
variability of sea levels on the Earth exerted by gravitational forces
of the Moon and the Sun. We are hence unable to observe the pure
astronomical tide in sea level elevations. Instead, sea level eleva-
tions are a result of co-existence and interaction between tides
and other non-deterministic variables such as surface wind, atmo-
spheric pressure and ocean currents at multiple spatio-temporal
scales [23]. In other words, it is a great challenge to correctly obtain
the “pure” astronomical tidal constituents from sea level records.

Tidal analysis and prediction (TAP) software is an ocean soft-
ware and modelling system. It is developed to decompose tidal con-
stituents from timeseries (i.e., series of data points arranged in the
time order) of the sea level elevation, and adopt them to predict
future sea level [7, 12, 28]. Having correct astronomical tidal con-
stituents is a critical task because it contributes to assure a safe ves-
sel navigation, validate complex ocean modelling systems [21, 27],
estimate the rate of sea level rise [23, 37, 39], and even support the
claims on oceanographic territories [24]. On one hand, it is diﬃ-
cult to isolate pure astronomical tidal constituents. On the other
hand, the TAP software is prone to defects and bugs. Because of
both physical and computational reasons, it is diﬃcult to deter-
mine whether the outcome from the TAP software is correct.

 
 
 
 
 
 
MET’22, May 9, 2022, Pittsburgh, PA, USA

Q.-H. Luu et al.

It is a prevalent problem in many ocean software and modelling
systems, including TAP software, that there does not exist a sys-
tematic mechanism, namely the oracle, to check the correctness of
the computation result given any program input. Such a problem
is termed as the oracle problem in the context of software testing.
Among all testing techniques, metamorphic testing (MT) [2, 5, 34]
has been demonstrated as very eﬀective in addressing the oracle
problem which has helped reveal many longstanding real-life bugs
in a wide variety of software systems. The core component of MT
is a set of metamorphic relations (MRs), which are basically the
necessary properties of the target algorithm, represented as the
relationships among multiple inputs and their corresponding ex-
pected outputs. Numerous studies [5, 34, 35] have shown that if
well-deﬁned, even a simple MR could help detect non-trivial bugs
in a complicated system that suﬀers from the oracle problem.

Several studies have been conducted to use MT to test the soft-
ware for scientiﬁc computations and simulations. For instances,
in testing simulations, MT has been successfully adopted to test
agent-based (ABM) and discrete-event (DES) simulations [26], hy-
brid ABM and DES systems [11], health care simulation [25], web-
enabled simulation [1], and simulator platform for self-driving cars
[36, 41]. In testing scientiﬁc software, MT is an eﬀective technique
to detect faults in simulation programs for designing nuclear power
plants [14], bioinformatics programs [4], epidemiological models
[31, 33], chemical reaction networks for prototyping nano-scale
molecular devices [13], matrix calculation programs [32], solvers
for partial diﬀerential equations [3], multiple linear regression soft-
ware [22], ocean modelling [16], storm water management model
systems [19], machine learning-based hydro-logical models [40],
Monte-Carlo computational programs [10, 30], serverless scientiﬁc
applications [20], as well as other types of scientiﬁc software [9, 18,
19, 29]. For example, He et al. [14] has found 33 bugs in simulation
programs that are used to design and analyze nuclear power plants
in a study that adopts MT.

In this paper, we demonstrate how to test TAP software using
MT. Seven MRs are identiﬁed and applied to test UTide, a real-life
and open-source TAP software. The paper has the following three
contributions:

• Based on the fundamental features of TAP, we propose two
strategies for identifying MRs that are particularly suitable
for TAP software.

• The testing based on our MRs has revealed a real-life defect

in UTide.

• A series of experimental studies have been conducted to
demonstrate the applicability and eﬀectiveness of MT in the
testing of TAP software, and more broadly, ocean software.

The rest of the paper is organized as follows. In Section 2, we
introduce the background information of the present work, includ-
ing the basic concepts of MT and TAP. In Section 3, we present two
strategies of identifying MRs for TAP software, based on which, we
deﬁne seven MRs for this study. The settings for our experimental
studies are presented in Section 4, while the experimental results
are analyzed in Section 5. We further discuss the results in Section 6
and the related work in Section 7. Finally, the paper is summarized
in Section 8.

2 BACKGROUND
2.1 Metamorphic Testing
Metamorphic testing (MT) was ﬁrst proposed in 1990s as an ap-
proach for test case generation [6]. MRs are leveraged to transform
some existing test cases into new test cases. The basic steps for MT
are as follows:

(1) Identify necessary properties of the target program and rep-
resent them in the form of MRs, that is, the relations among
multiple inputs and their corresponding expected outputs.
(2) Generate source test cases using other test case generation

techniques.

(3) Execute the source test cases.
(4) Construct follow-up test cases based on MRs and the source
test cases (and even their test results obtained in Step (3)).

(5) Execute the follow-up test cases.
(6) Check the test results in Steps (3) & (5) against the corre-

sponding MRs.

Note that MT makes use of MRs, instead of a test oracle, to verify
the test results, as shown in the above Step (6). If an MR is violated
in Step (6), we can say that a defect has been revealed. In other
words, MRs provide an alternative mechanism to the oracle for test
result veriﬁcation, thus addressing the oracle problem.

Let us look at the following example to further illustrate how
MT works. Suppose that a program 𝑃 calculates the shortest path
between two nodes 𝑎 and 𝑏 in an undirected graph 𝐺. One possi-
ble MR can be deﬁned as follows: Given a node 𝑐 that appears in
the shortest path between 𝑎 and 𝑏, the length of the shortest path
between 𝑎 and 𝑏 should be equal to the sum of the length of the
shortest paths between 𝑎 and 𝑐 and that between 𝑐 and 𝑏. Assume
that we already have the source test case (𝐺, 𝑎, 𝑏). We execute this
test case and select a node 𝑐 from the execution result 𝑃 (𝐺, 𝑎, 𝑏)
(that is, the shortest path composed of a series of nodes). Then, we
can construct two follow-up test cases, (𝐺, 𝑎, 𝑐) and (𝐺, 𝑐, 𝑏), the ex-
ecution results of which are 𝑃 (𝐺, 𝑎, 𝑐) and 𝑃 (𝐺, 𝑐, 𝑏), respectively.
Finally, we check whether the relation |𝑃 (𝐺, 𝑎, 𝑏)| = |𝑃 (𝐺, 𝑎, 𝑐)| +
|𝑃 (𝐺, 𝑐, 𝑏)| holds or not. If the relation does not hold, it means that
the MR is violated, which, in turn, implies the detection of a defect.

2.2 Tidal Analysis and Prediction
Consider a time series of sea level observations 𝑦 (𝑡) where 𝑡 =
𝑡1, 𝑡2, . . . , 𝑡𝑀 and 𝑀 is an odd integer. Assume the sea level is a
combination of 𝑁 astronomical tidal constituents with frequencies
𝜎1, 𝜎2, . . . , 𝜎𝑁 , each of which is constructed from a unique com-
bination of six Doodson numbers. These numbers are associated
with the mean longitudes of the Moon, the Sun, the lunar perigee
and the solar perigee, together with the negative of the longitude
of the Moon’s ascending node and the mean lunar time [12]. Tidal
response model equation [12, 28] is as follows:

ˆ𝑦(𝑡) = ˆ𝑎0 + ˆ𝑎1𝑡 +

𝑁

Õ
𝑘=1

ˆ𝑏𝑘𝑒𝑖𝜎𝑘𝑡 + ˆ𝑏−𝑘𝑒−𝑖𝜎𝑘𝑡

(1)

Testing Ocean Software with Metamorphic Testing

MET’22, May 9, 2022, Pittsburgh, PA, USA

where ˆ𝑏−𝑘 and ˆ𝑏−𝑘 are unknown (complex) amplitude and 𝑖 is the
imaginary unit. Its conjunction form in real numbers is as follows:

where

ˆ𝑦(𝑡) = ˆ𝑎0 + ˆ𝑎1𝑡 +

𝑁

Õ
𝑘=1

ˆ𝐵𝑘 cos 𝜎𝑘𝑡 + ˆ𝐶𝑘 sin 𝜎𝑘𝑡 (cid:17)

(cid:16)

(2)

X(𝑡) =

= 𝑖 ( ˆ𝑏𝑘 − ˆ𝑏−𝑘 )/2. Alternatively, the
where ˆ𝐵𝑘
response model can also be represented by the following equation:

= ( ˆ𝑏𝑘 + ˆ𝑏−𝑘 )/2 and ˆ𝐶𝑘

ˆ𝑦 (𝑡) = ˆ𝑎0 + ˆ𝑎1𝑡 +

𝑁

Õ
𝑘=1

ˆ𝐴𝑘 cos (𝜎𝑘𝑡 + ˆ𝜙𝑘 )

(3)

where the amplitude ˆ𝐴𝑘 and the phase ˆ𝜙𝑘 of a tidal constituent 𝑘
are deﬁned in terms of ˆ𝐵𝑘 and ˆ𝐶𝑘 as follows:

ˆ𝐴𝑘
ˆ𝜙𝑘

= q ˆ𝐵𝑘 + ˆ𝐶𝑘
= arctan (− ˆ𝐶𝑘/ ˆ𝐵𝑘 )

(4)

After the equation is solved, some corrections can be applied.
First, the computed phases ˆ𝜙𝑘 (𝑘 = 1, 2, . . . , 𝑁 ) are referenced to
the “Greenwich phase”, which is deﬁned as the phase of the equi-
librium response at 0◦ longitude (the Greenwich meridian). Second,
nodal or satellite corrections are applied if a latitude is speciﬁed in
the conﬁgurable input; otherwise, it will use a default value.

A process for TAP consists of two modes. In the analysis mode,
the TAP software takes the input of raw timeseries of sea level
elevation 𝑦 (𝑡) at time steps 𝑡 = 𝑡1, 𝑡2, . . . , 𝑡𝑀 . It will also use other
inputs such as tidal frequencies, conﬁguration for which algorithm
to be used for ﬁtting, the latitude of tide gauge or the adoption of a
trend. If any of these inputs is not provided, it will be automatically
determined by the TAP software, through either assigning a de-
fault value or computing it using relevant algorithms. For instance,
frequencies of 𝑁 astronomical tidal constituents 𝜎1, 𝜎2, . . . , 𝜎𝑁 can
be either given by users or determined by the TAP itself, at the dis-
cretion of the users. The output of analysis mode consists of tidal
amplitudes ˆ𝐴1, ˆ𝐴2, . . . , ˆ𝐴𝑁 and phases ˆ𝜙1, ˆ𝜙2, . . . , ˆ𝜙𝑁 together with
the intercept ˆ𝑎1 and the trend ˆ𝑎2. In the prediction mode, the TAP
software uses the determined tidal constants ˆ𝐴1, ˆ𝐴2, . . . , ˆ𝐴𝑁 and
phases ˆ𝜙1, ˆ𝜙2, . . . , ˆ𝜙𝑁 as well as ˆ𝑎1 and ˆ𝑎2 to predict the sea level
elevation ˆ𝑦 (𝑡) at a given time 𝑡 according to Equation 3.

3 METAMORPHIC RELATIONS FOR TAP

SOFTWARE

Considering the unique features of TAP software, we design two
strategies to derive MRs. The ﬁrst approach is to construct MRs
based on the core mathematical framework for the TAP, namely
regression. The second approach is to construct MRs based on the
properties of tidal waves.

3.1 Regression-based MRs
We can reformulate Equation 2 in the matrix form as follows:

1
𝑡1


cos 𝜎1𝑡1


sin 𝜎1𝑡1


. . .


cos 𝜎𝑁 𝑡1


sin 𝜎𝑁 𝑡1



ˆ𝑎0

(cid:2)
ˆ𝑦(𝑡1)
(cid:2)

1
𝑡2
cos 𝜎1𝑡2
sin 𝜎1𝑡2
. . .
cos 𝜎𝑁 𝑡2
sin 𝜎𝑁 𝑡2
ˆ𝐶1
ˆ𝐵1
ˆ𝑦(𝑡2)
. . .

ˆ𝑎1

. . .
. . .
. . .
. . .
. . .
. . .
. . .
ˆ𝐶𝑁 (cid:3)

;

1
𝑡3
cos 𝜎1𝑡3
sin 𝜎1𝑡3
. . .
cos 𝜎𝑁 𝑡3
sin 𝜎𝑁 𝑡3
ˆ𝐵𝑁

. . .

ˆ𝑦 (𝑡𝑀 )

(cid:3)

1
𝑡𝑀
cos 𝜎1𝑡𝑀
sin 𝜎1𝑡𝑀
. . .
cos 𝜎𝑁 𝑡𝑀
sin 𝜎𝑁 𝑡𝑀

;
















(6)

ˆ𝛽∗ =
ˆy(𝑡) =

This equation can be solved using the ordinary least square (OLS)

regression, which gives us the unique solution

ˆ𝛽∗ = (XX𝑇 )−1Xy(𝑡)

(7)

where

y(𝑡) =

𝑦 (𝑡2)
(8)
In the analysis, the output coeﬃcient ˆ𝛽 is commonly used as the

𝑦 (𝑡1)
(cid:2)

𝑦 (𝑡𝑀 )

. . .

(cid:3)

ˆ𝛽 =

output which is deﬁned as follows
ˆ𝜙1
where the coeﬃcients of ˆ𝛽 are derived from the coeﬃcients of ˆ𝛽∗
according to Equation 4 and Equation 6.

ˆ𝜙𝑁 (cid:3)

ˆ𝑎0
(cid:2)

ˆ𝐴𝑁

ˆ𝐴1

. . .

ˆ𝑎1

(9)

2, 𝑦𝑠

𝑀, 𝑦𝑠

1), (𝑡𝑠

2), . . . , (𝑡𝑠

Hereby, we reuse some MRs that have been developed in our
previous study for the multiple linear regression (MLR) [22]. Given
the source test case 𝐼𝑠 = {(𝑡𝑠
1, 𝑦𝑠
𝑀 )} and the
corresponding output 𝑂𝑠 = { ˆ𝛽𝑠 }. It should be noted that regres-
sion and TAP software are designed to solve diﬀerent problems
though they are based on the same mathematical approach. There-
fore, they have some common MRs but also some diﬀerent MRs.
For example, we have an MR related to swapping or rotating two
timeseries (i.e., the data for two variables) in the regression, but
such kind of MRs cannot be deﬁned for the TAP software. In this
study, ﬁve MRs are reused to the TAP software as described as fol-
lows.

MR1: Inserting a predicted point. Assume (𝑡𝑝, 𝑦𝑝 ) is a predicted
point using Equation 2. The follow-up test case can be constructed
=
by 𝐼 𝑓
{ ˆ𝛽𝑓 }. Thus, this MR should have the relation ˆ𝛽𝑓

(𝑡𝑝, ˆ𝑦𝑝 ), and the follow-up output should be 𝑂 𝑓

= ˆ𝛽𝑠 .

= 𝐼𝑠

Ð

1

= − ˆ𝛼0

= − ˆ𝛼1

𝑗 and ˆ𝛼 𝑓

= −𝐼𝑠, then ˆ𝛼 𝑓
0

MR2: Reﬂecting the sea level. The follow-up test case can be con-
structed by having 𝐼 𝑓
𝑗 . When
there is only one astronomical tidal constituent (𝑁 = 1), the rela-
tion should also consist of ˆ𝐴𝑓
= ˆ𝐴𝑠
1.
1
MR3: Shifting the sea level. The follow-up test case can be con-
= 𝐼𝑠 + ℎ, where ℎ is a real number, and then
structed by having 𝐼 𝑓
the intercept is shifted accordingly whilst other components of ˆ𝛽
are unchanged; that is, ˆ𝛼 𝑓
= ˆ𝜙𝑠
0
𝑘
for 𝑘 = 1, 2, . . . , 𝑁 .

0 +ℎ and ˆ𝛼 𝑓

𝑘, ˆ𝜙 𝑓

1, ˆ𝐴𝑓

= ˆ𝐴𝑠

= ˆ𝛼𝑠

= ˆ𝛼𝑠

𝑘

𝑘

1

MR4: Scaling the sea level. The follow-up test case can be con-

= 𝛾𝐼𝑠 , where 𝛾 is a real number, and then the amplitude-

structed by 𝐼 𝑓
related components of the follow-up output is scaled proportion-
ally, that is, ˆ𝛼 𝑓
1, ˆ𝐴𝑓
0
𝑘
MR5: Swapping samples. If we swap two points in the input,

𝑘 for 𝑘 = 1, 2, . . . , 𝑁 .

0, ˆ𝛼 𝑓

= 𝛾 ˆ𝐴𝑠

= 𝛾 ˆ𝛼𝑠

= 𝛾 ˆ𝛼𝑠

1

ˆy(𝑡) = X𝑇 (𝑡) ˆ𝛽∗

(5)

then the follow-up output is the same, that is, ˆ𝛽𝑓

= ˆ𝛽𝑠 .

MET’22, May 9, 2022, Pittsburgh, PA, USA

Q.-H. Luu et al.

3.2 Domain-speciﬁc MRs
MR6: Cancellation of signals. Consider the sea level elevation in-
duced by a single tidal constituent component without a trend, that
is, we have

ˆ𝑦(𝑡) = ˆ𝑎0 + ˆ𝐴1 cos (𝜎1𝑡 + ˆ𝜙1)
In the follow-up test case, the signal is cancelled by the predicted
1 cos (𝜎1𝑡 + ˆ𝜙𝑠
component, 𝑦 𝑓 = 𝑦𝑠 − ˆ𝐴𝑠
1). Then, we should have
ˆ𝐴𝑓
= 0. MR6 comes from the intuition that if we suppress the
1
oscillation of a tidal constituent, then this wave component is no
longer part of the timeseries.

(10)

MR7: Periodic shift of signals. Consider the sea level elevation

induced by a single tidal constituent with a trend, that is

(11)

ˆ𝑦 (𝑡) = ˆ𝑎0 + ˆ𝑎1𝑡 + ˆ𝐴1 cos (𝜎1𝑡 + ˆ𝜙1)
2 , . . . , 𝑡 𝑓

1 , 𝑡 𝑓
In the follow-up test case, the timeseries 𝑡 𝑓
𝑀 are all shifted
by a magnitude of 2𝜋/𝜎1, that is, 𝑡 𝑓
𝑗 + 2𝜋/𝜎1 for 𝑗 = 1, 2, . . . , 𝑀.
𝑗
The follow-up output is then expected to be ˆ𝑎𝑓
0 −2𝜋 ˆ𝑎𝑠
1/𝜎1, and
0
ˆ𝑎𝑓
1. The rationale for MR7 is the periodic
1
property of a tidal wave which requires the tide to be the same
after a full period.

1, ˆ𝐴𝑓

1, ˆ𝜙 𝑓

= ˆ𝐴𝑠

= ˆ𝜙𝑠

= ˆ𝑎𝑠

= ˆ𝑎𝑠

= 𝑡𝑠

1

1

4 EXPERIMENTS
4.1 Research questions
In this study, we attempt to answer the following three research
questions (RQs):

RQ1: Can MT help reveal any real-life defect? We will examine
whether or not the proposed MRs are able to detect any issue in
the system under test (SUT).

RQ2: Is MT applicableto detect diﬀerent types of faults? We will
carry out the mutation analysis to further study the eﬀectiveness
of MT.

RQ3: Is there any diﬀerence in the eﬀectiveness of MRs? Its answer

may hint us for the selection of the best MRs for testing.

RQ1 will be addressed by testing the original program; whilst

RQ2 and RQ3 will be answered based on the mutation analysis.

4.2 System under test and mutation analysis
UTide (Uniﬁed Tidal Analysis and Prediction) software developed
by Wesley Bowman (https://github.com/wesleybowman/UTide) was
used as our system under test. Its Python version has 2940 lines of
code (LOC), taking the raw input of timeseries of sea level, and pro-
ducing the output in the form of either tidal harmonic constants
(in the analysis mode) or timeseries of tidal elevation (in the predic-
tion mode). Having the latest version (v0.2.6) published one year
ago, the software is stable after 4 years of development, receiving
GitHub stars from 75 users and being forked by 44 developers. It
was conﬁgured to run on iMac M1 computer with Python 3 under
an Anaconda environment.

Mutation analysis is a commonly used method to examine the
eﬀectiveness of testing approaches [17]. In the mutation analysis,
the original program will be modiﬁed by seeding a fault into a se-
lected statement. Each modiﬁed version is referred to as a mutant,
which should be unique to avoid double counts. It contains the
code and behaviour that are not the same as the original program.

If the fault seeded in a mutant can be detected using a test suite, it
is said that the mutant is “killed” by the test suite. The eﬀectiveness
of the test suite (and hence the technique that generates the test
suite) is measured by the percentage of mutants that they kill out
of all considered mutants. The advantage of mutation analysis is
to generate a wide range of faults to challenge the eﬀectiveness of
the newly developed testing method. In this study, we adopted the
mutant-generating tool developed by Luu et al. [22], and modiﬁed
it to suit our purpose of testing Python-based programs.

Our focus is to test the core component of UTide. Hence, for the
mutation analysis, we generated mutants from the core analysis
component of the TAP. It contains 206 LOC (in “_solve.py” from
line 232 to line 438). Among mutation operators deﬁned in [22]
for C/C++ programs, we used ﬁve sets to generate mutants for the
Python program, as presented in Table 1. We then applied a ﬁlter-
ing process to eliminate the equivalent mutants (which refers to
the mutants that have the same execution behaviors as the origi-
nal program): If a mutant always delivers the same output as the
UTide on a set of randomly generated test cases, that mutant would
be removed. We also manually inspected the remaining mutants
to ensure that the non-equivalent mutants are unique. Finally, we
obtained 38 non-equivalent mutants that are non-trivial (i.e., not
killed by the Python interpreter) to be used for testing (Table 1).

4.3 Experimental setup
In this study, we generated diﬀerent timeseries of the principal lu-
nar semi-diurnal (M2) tidal constituent with random noises and
conﬁgurations. The period of M2 astronomical tidal constituent is
about 12 hours and 25.2 minutes [15]. Only one tidal constituent
was used so that all MRs are applicable. The length of timeseries
varies randomly from 1 week to 1 month at one-hour intervals.
The amplitudes and phases were randomly chosen in the range of
[0.1, 3] metres and [0◦, 360◦), respectively. We generated and used
100 diﬀerent random datasets (meaning 100 source test cases).

The input to UTide is the timeseries of tide data and the conﬁgu-
ration for the analysis. We conﬁgured the program in the “automa-
tion” mode using the OLS ﬁtting algorithm with a trend for MR1,
MR2, MR3, MR4 and MR5 and MR7. For MR6, the program was
customized with the given tidal constituent (M2) without a trend
in accordance with Equation 10. Unless preﬁxing the output with
a single tidal constituent, the program returns the output which
consists a list of tidal amplitudes and phases that have signiﬁcant
impacts to the signals. We mainly used these outputs to determine
whether the MRs are satisﬁed or violated. For each MR, we ran an
experiment twice, one with the source (random) test case and an-
other with the follow-up ones. We repeated the experiments 100
times using 100 diﬀerent datasets generated above.

4.4 Assessment
To determine the violation or satisfaction of an MR, we examined
the relation between the source and follow-up outputs. It involves
two conditions.

(1) For the ﬁrst condition, we determined whether or not the
tidal constituents are consistent with the conﬁguration. All
MRs require that the tidal constituents in both the source
and follow-up outputs are the same. If they diﬀer by a tidal
constituent, the MRs will be considered as violated.

Testing Ocean Software with Metamorphic Testing

MET’22, May 9, 2022, Pittsburgh, PA, USA

Table 1: Summary of generated mutants and mutants used for testing, being classiﬁed by mutation operators.

Mutation category mutation operators
array_index
condition_if
logic_comparison
logic_value
math_operator

Description
Misplaced array index
[], [1+], [1-]
Illogical branching condition
if(), if None, if False, if True
! =, <, >, <=, >=, ==
Mistaken logical operators
True, False, None, not None Mistaken logical condition
+, -, *, /

Wrong use of mathematical operators
Total

Number of used mutants
7
10
3
1
17
38

(2) For the second condition, we continued to examine the rela-
tion between source and follow-up outputs if the ﬁrst condi-
tion is satisﬁed. For example, in MR1, we checked whether
= ˆ𝜙𝑠 are satisﬁed
or not both conditions ˆ𝐴𝑓
within a small round-oﬀ error (which was set as 0.01 in this
study). If one of them is not satisﬁed, the MR will be consid-
ered as violated.

= ˆ𝐴𝑠 and ˆ𝜙𝑓

To measure the eﬀectiveness of our method, we used the muta-

tion score 𝑀𝑆, which is deﬁned as

𝑀𝑆 = 𝑀𝐾
𝑀𝑁

× 100%

(12)

where 𝑀𝐾 is the total number of mutants that are killed by an MR,
𝑀𝑁 is the total number of non-equivalent mutants.

5 RESULT
5.1 RQ1: Detection of real-life defect
The proposed MRs were ﬁrst applied to test the UTide. We ob-
served that MR1 is able to reveal the real-life defect in the software
under test.

With MR1, we found that adding a new point to the timeseries
will cause the TAP to produce a totally diﬀerent output. The detec-
tion of error is described as follows. Assume ˆ𝐴𝑠 and ˆ𝜙𝑠 are the am-
plitude and the phase of a tide component in the timeseries consist-
ing of 𝑀 points 𝐼𝑠 = {(𝑡1, 𝑦1), (𝑡2, 𝑦2), . . . , (𝑡𝑀 , 𝑦𝑀 )}, respectively.
= {(𝑡1, 𝑦1),
The follow-up test case consists of 𝑀 + 1 points 𝐼 𝑓
(𝑡2, 𝑦2), . . . , (𝑡𝑀 , 𝑦𝑀 ), (𝑡𝑀+1, ˆ𝑦𝑀+1)} where ˆ𝑦𝑀+1 is obtai-ned from
the prediction mode using Equation 3 with the known ˆ𝐴𝑠, ˆ𝜙𝑠 and
𝑡𝑀+1. The follow-up output consists of ˆ𝐴𝑓 and ˆ𝜙𝑓 . As required by
our MR, the follow-up output should be the same for both ampli-
= ˆ𝜙𝑠 . However, the follow-
= ˆ𝐴𝑠 and 𝜙𝑓
tude and phase, that is ˆ𝐴𝑓
up phase ˆ𝜙𝑓 turned out to look like a random number (and hence
not equal to ˆ𝜙𝑠 ), and related to the choice of 𝑡𝑀+1, which should
not be the case.

To make sure that there is no human error in the data processing,
we have rigorously tracked down every step in the data pipeline.
It could be conﬁrmed that the transformation that does not alter
the length of timeseries (e.g., scaling, shifting) have not led to any
similar abnormal output. The adoption of other MRs (MR2, MR3,
MR4, MR5, MR6 and MR7) did not detect any violation. We were
yet to determine the root cause of the fault. We anticipated that it

could be attributed to how the program converts the computed co-
eﬃcients (e.g., ˆ𝐵𝑘 and ˆ𝐶𝑘 ) to the phase ( ˆ𝜙𝑘 ), or the mistake in han-
dling the last time step for determining the correct phase. Further
investigation will be conducted to locate the buggy statements.

5.2 RQ2: Applicability of MT
Since the defect happens only when we modify the length of time-
series, the UTide is arguably still usable for the mutation analysis.
The violation of MR1 is determined from examining only the re-
lation among amplitudes. For other six MRs, both amplitudes and
phases were used to derive the mutation scores. Table 2 presents
the summary of experimental results on the mutation analysis.

It is found that all MRs are able to kill mutants. In other words,
all MRs are eﬀective in detecting buggy programs. The mutation
scores range from 13% to 24% (Table 2), subject to the MRs and
test cases used. The adoption of domain-speciﬁc MRs (MR6 and
MR7) also allowed us to detect the mutants (𝑎𝑝 = 𝑛𝑝.ℎ𝑠𝑡𝑎𝑐𝑘 ((𝑚 [:
𝑛𝑁 𝑅];𝑚 [2∗𝑛𝑁 𝑅 : 2+𝑛𝑁 𝑅 +𝑛𝑅])) and 𝑌𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 −𝑎𝑚)) that
are unable to be killed by the regression-based MRs (as shown in
Table 3). Whilst our MRs can detect mutants in the 𝑚𝑎𝑡ℎ_𝑜𝑝𝑒𝑟𝑎𝑡𝑜𝑟
and 𝑐𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛_𝑖 𝑓 categories (Table 2), some mutants associated
with 𝑙𝑜𝑔𝑖𝑐_𝑐𝑜𝑚𝑝𝑎𝑟𝑖𝑠𝑜𝑛 and 𝑎𝑟𝑟𝑎𝑦_𝑖𝑛𝑑𝑒𝑥 have yet been killed in
these experiments (Table 1).

5.3 RQ3: Eﬀectiveness of MRs
We also found that the eﬀectiveness of each MR is not the same.
MR1 has the best performance among all MRs: It could kill about
one quarter of the mutants (Table 2). It is associated with inserting
the predicted point into the analysis. The second best MRs include
MR2 and MR4, which are related to the reﬂecting and scaling of
sea level, respectively. They had the same mutation score of 16%
(Table 2).

Most detected mutants were about the modiﬁcation of coeﬃ-
cients through the arithmetic operators (+, −, ∗, /) in preparation
for the ﬁtting (Table 3). There are three mutants (𝑜𝑝𝑡 [“𝑟𝑚𝑖𝑛”]/(24+
𝑙𝑜𝑟 );, 𝑖 𝑓 𝑇𝑟𝑢𝑒 : (Line 302), and 𝑋𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)) that were
killed only by MR1. The results suggest that the diversity of MRs
is required for detecting diﬀerent types of faults.

The wide range of eﬀectiveness indicates that some MRs are
better than others in detecting faults. As a result, selecting the most
eﬀective MR is one key issue for the testing of the TAP software,
similar to other types of software.

MET’22, May 9, 2022, Pittsburgh, PA, USA

Q.-H. Luu et al.

Table 2: Mutation scores after 100 test cases.

Mutation score 𝑀𝑆

Dataset MR1 MR2 MR3 MR4 MR5 MR6 MR7
13%

16%

11%

24%

11%

16%

11%

Table 3: List of non-equivalent mutants killed and the numbers of times they are killed over the total 100 random test cases.

Mutation category Line Original
math_operator
condition_if
math_operator

242
302
338

math_operator
math_operator
math_operator
math_operator
math_operator
math_operator
condition_if
condition_if

342
342
343
342
343
343
357
367

Mutant
𝑜𝑝𝑡 [“𝑟𝑚𝑖𝑛”]/(24 + 𝑙𝑜𝑟 );
𝑜𝑝𝑡 [“𝑟𝑚𝑖𝑛”]/(24 ∗ 𝑙𝑜𝑟 );
𝑖 𝑓 𝑛𝑜𝑡 𝑜𝑝𝑡 [“𝑛𝑜𝑡𝑟𝑒𝑛𝑑”] :
𝑖 𝑓 𝑇𝑟𝑢𝑒 :
𝑎𝑝 = 𝑛𝑝.ℎ𝑠𝑡𝑎𝑐𝑘 ((𝑚 [: 𝑛𝑁 𝑅];
𝑎𝑝 = 𝑛𝑝.ℎ𝑠𝑡𝑎𝑐𝑘 ((𝑚 [: 𝑛𝑁 𝑅];
𝑚 [2 ∗ 𝑛𝑁 𝑅 : 2 + 𝑛𝑁 𝑅 ∗ 𝑛𝑅])) 𝑚 [2 ∗ 𝑛𝑁 𝑅 : 2 + 𝑛𝑁 𝑅 + 𝑛𝑅]))
𝑋𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 + 𝑎𝑚)
𝑋𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 + 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑖 𝑓 𝑜𝑝𝑡 [“𝑡𝑤𝑜𝑑𝑖𝑚”] :
𝑖 𝑓 𝑜𝑝𝑡 [“𝑛𝑜𝑡𝑟𝑒𝑛𝑑”] :

𝑋𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑋𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 ∗ 𝑎𝑚)
𝑌𝑢 = 𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 − 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 + 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝 ∗ 𝑎𝑚)
𝑌𝑢 = −𝑛𝑝.𝑟𝑒𝑎𝑙 (𝑎𝑝/𝑎𝑚)
𝑖 𝑓 𝑇𝑟𝑢𝑒 :
𝑖 𝑓 𝑇𝑟𝑢𝑒 :

MR1 MR2 MR3 MR4 MR5 MR6 MR7
0
0

0
0

0
0

0
0

0
0

0
0

2
1

0
85
89
0
86
86
100
100
100

0
0
100
0
14
100
100
100
100

0
0
0
0
85
96
0
100
100

0
0
100
0
88
82
100
100
100

0
0
0
0
93
93
0
100
100

100
0
81
100
0
0
0
100
0

100
0
0
0
0
22
23
100
100

6 DISCUSSION
To further improve the eﬀectiveness and robustness of testing, we
can consider the following tasks in our future work:

• Improvements: Bugs in the SUT should be detected and cor-
rected before the mutation analysis. We should also need to
include more code in the mutation analysis, and at the same
time, enhance the diversity of mutants. A better reﬁnement
and ﬁltering of non-equivalent mutants are of necessity.
• Extensions: New MRs are necessary to improve the eﬀec-
tiveness of testing. In addition, the adoption of realistic tide
records instead of random data is useful to measure the per-
formance of the method. More TAP programs (other than
UTide) are needed to examine the eﬀectiveness of MT.

Looking forward, we suggest that MT can be adopted in diﬀer-

ent ways as follows:

• Veriﬁcation of the TAP software: Our method is applicable
to test the TAP software, either well-established or newly
developed ones.

• Validation of the TAP data quality and output: It is very com-
mon that a short (1-day to two-week) timeseries of sea level
are used. Our MRs can be organized to construct suitable
metrics for evaluating the reliability of used data and the
conﬁdence of results.

• Veriﬁcation and validation of high-resolution tide models: In
practice, tidal simulations are mainly validated by means of
root mean square errors and correlation coeﬃcients. Since
tidal waves simulated in ocean modelling systems are peri-
odic, the domain-speciﬁc MRs can be extended to verify and
validate them in a similar way.

7 RELATED WORKS
In ocean science, Hiremath et al. [16] conducted the ﬁrst study to
adopt MT to validate ocean systems. They developed a machine
learning algorithm that is able to identify MRs automatically by op-
timizing a predeﬁned cost function. They have adopted the method
to test a simpliﬁed model for computing the kinetic energy of the
sea level elevation.

In the hydraulic ﬁeld, MT has been successfully applied to test
the storm water management model (SWMM) systems [19] and
dynamical hydraulic systems model [40]. Lin et al. [19] have pro-
posed four MRs related to the change of correlation coeﬃcients
(𝑅2), which help detect real-life defects in the SWMM system that
is used to simulate the dynamic rainfall-runoﬀ models for estimat-
ing the storm water runoﬀs in urban areas. The work of Yang and
Chui [40] focuses on testing machine learning models which pre-
dict ﬂood events in Germany. They have proposed three MRs and
adopted them to test 13 machine learning models. They then pro-
posed that MT can be further extended to test the general process-
based models.

In summary, whilst MT has been shown to be applicable to ocean
science and related ﬁelds, to the best of our knowledge, no compre-
hensive work has been carried out to systematically apply MT into
the testing of realistic ocean software and modelling systems.

8 CONCLUSION
In this study, we illustrated how to use MT to test the ocean soft-
ware through a case study of TAP software. In doing so, we have
deﬁned a set of MRs, either by adopting MRs derived from its math-
ematical framework, or by identifying new MRs based on TAP’s

Testing Ocean Software with Metamorphic Testing

MET’22, May 9, 2022, Pittsburgh, PA, USA

innate properties. The experiment showed that our MRs were ef-
fective in both revealing the real-life fault as well as killing non-
equivalent mutants. We also discussed on the future directions to
improve and extend the work. The success of our study points out
a promising direction for the testing of complex ocean modelling
systems, including Ocean General Circulation Models (OGCM) and
real-time/operational Ocean Forecast Systems (OFS).

In addition to demonstrating the potentials of MT in testing
ocean software, this study also highlights the vision of MT’s ap-
plication into various scientiﬁc and industrial simulation systems.
In the underwater research, submarine simulators (e.g., UWSim,
NAUTIS Maritime Simulator) are used to test the underwater robots
including Autonomous Underwater Vehicles (AUVs) for a better
development and operation of a submarine. On one hand, subma-
rine simulation might require the inputs from ocean and other
simulations; on the other hand, it has features and functions for
navigating AUVs. Traﬃc simulations, as another example, involve
complex mathematical formulas and rules for predicting potential
traﬃc ﬂows and events. The work in testing diﬀerent traﬃc simu-
lators (such as SUMO and PTV Vissim) and autonomous driving
simulators and platforms (e.g., Apollo, Autoware, CARLA, SVL)
not only helps improve the safety and the road users’ experience,
but also may promote the reliability of other vehicular systems in-
cluding submarines and aeroplanes. It would be worthwhile to in-
vestigate how to deﬁne MRs (similar to our regression-based MRs).
Other application domains of MT in scientiﬁc simulations include
power distribution systems and even nuclear plants, where a se-
ries of MRs can be constructed and used to test the correctness of
simulation results such that potential safety-critical errors can be
detected and avoided before the real systems are deployed.

REFERENCES
[1] John Ahlgren, Maria Berezin, Kinga Bojarczuk, Elena Dulskyte, Inna Dvortsova,
Johann George, Natalija Gucevska, Mark Harman, Maria Lomeli, Erik Meijer,
Silvia Sapora, and Justin Spahr-Summers. 2021. Testing Web Enabled Simula-
tion at Scale Using Metamorphic Testing. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP).
140–149. https://doi.org/10.1109/ICSE-SEIP52600.2021.00023

[2] Tsong Yueh Chen, S. C. Cheung, and Siu-Ming Yiu. 1998. Metamorphic Testing:
A New Approach for Generating Next Test Cases. Technical Report HKUST-CS98-
01, Hong Kong University of Science and Technology (1998).

[3] T. Y. Chen, J. Feng, and T. H. Tse. 2002. Metamorphic testing of programs on par-
tial diﬀerential equations: a case study. In Proceedings 26th Annual International
Computer Software and Applications. 327–333.

[4] Tsong Yueh Chen, Joshua WK Ho, Huai Liu, and Xiaoyuan Xie. 2009.
testing bioinformatics programs using
BMC Bioinformatics 10, 1 (19 Jan 2009), 24.

An innovative
metamorphic testing.
https://doi.org/10.1186/1471-2105-10-24

approach for

[5] T. Y. Chen, F. C. Kuo, H. Liu, P.L. Poon, D. Towey, T. H. Tse, and Z. Q. Zhou.
2018. Metamorphic Testing: A review of challenges and opportunities. Comput.
Surveys 51, 1 (2018). https://doi.org/10.1145/3143561

[6] T. Y. Chen and M. F. Lau. 1998. A new heuristic for test suite reduction. Infor-

mation and Software Technology 40, 5 (1998), 347–354.

[7] D.L. Codiga. 2011. MUniﬁed Tidal Analysis and Prediction Using the UTide
Matlab Functions. Technical Report 2011-01. Graduate School of Oceanography,
University of Rhode Island, Narragansett, RI (2011).

[8] Benoit Cushman-Roisin and Jean-Marie Beckers. 2011. Introduction to Geophys-
ical Fluid Dynamics: Physical and Numerical Aspects (2 ed.). Academic Press.
[9] Junhua Ding, XinChuan Li, and Xin-Hua Hu. 2019. Testing Scientiﬁc Soft-
ware with Invariant Relations: A Case Study. In 2019 IEEE 19th Interna-
tional Conference on Software Quality, Reliability and Security (QRS). 406–417.
https://doi.org/10.1109/QRS.2019.00057

[10] Junhua Ding, Tong Wu, Dianxiang Wu, Jun Q. Lu, and Xin-Hua Hu. 2011. Meta-
morphic Testing of a Monte Carlo Modeling Program. In Proceedings of the 6th
International Workshop on Automation of Software Test (Waikiki, Honolulu, HI,

USA) (AST ’11). Association for Computing Machinery, New York, NY, USA, 1–7.
https://doi.org/10.1145/1982595.1982597

[11] Mohammed Farhan, Caroline Krejci, Megan Olsen,

nak. 2021.
In 2021 Annual Modeling and Simulation Conference
https://doi.org/10.23919/ANNSIM52504.2021.9552058

and M S Rau-
Metamorphic Testing for Hybrid Simulation Validation.
(ANNSIM). 1–12.

[12] M. G. G. Foreman and R. F. Henry. 1989. The harmonic analysis of tidal
Advances in Water Resources 12, 3 (1989), 109–120.

model time series.
https://doi.org/10.1016/0309-1708(89)90017-1

[13] Michael C. Gerten, James I. Lathrop, Myra B. Cohen, and Titus H. Klinge. 2020.
ChemTest: An Automated Software Testing Framework for an Emerging Para-
digm. Association for Computing Machinery, New York, NY, USA, 548–560.
https://doi.org/10.1145/3324884.3416638

[14] Xiao He, Xingwei Wang, Jia Shi, and Yi Liu. 2020. Testing High Performance
Numerical Simulation Programs: Experience, Lessons Learned, and Open Issues.
In Proceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA 2020). Association for Computing Machinery, New
York, NY, USA, 502–515. https://doi.org/10.1145/3395363.3397382

[15] Steacy Dopp Hicks. 2006. Understanding tides. NOAA National Ocean Service.
[16] J. Dilip Hiremath, Martin Claus, Wilhelm Hasselbring, and Willi Rath. 2021. To-
wards Automated Metamorphic Test Identiﬁcation for Ocean System Models. In
2021 IEEE/ACM 6th International Workshop on Metamorphic Testing (MET). 42–46.
https://doi.org/10.1109/MET52542.2021.00014

[17] Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of
Mutation Testing. IEEE Transactions on Software Engineering 37, 5 (2011), 649–
678. https://doi.org/10.1109/TSE.2010.62

[18] Upulee Kanewala and Tsong Yueh Chen. 2019. Metamorphic Testing: A Simple
Yet Eﬀective Approach for Testing Scientiﬁc Software. Computing in Science
Engineering 21, 1 (2019), 66–72. https://doi.org/10.1109/MCSE.2018.2875368
[19] X. Lin, M. Simon, and N. Niu. 2018. Hierarchical Metamorphic Relations for Test-
ing Scientiﬁc Software. In Proceedings of the International Workshop on Software
Engineering for Science (Gothenburg, Sweden) (SE4Science ’18). ACM, New York,
NY, USA, 1–8. https://doi.org/10.1145/3194747.3194750

[20] Xuanyi Lin, Michelle Simon, and Nan Niu. 2021. Scientiﬁc Software Testing
Goes Serverless: Creating and Invoking Metamorphic Functions. IEEE Software
38, 1 (2021), 61–67. https://doi.org/10.1109/MS.2020.3029468

[21] Q. H. Luu, Kosuke I., Yoichi I., and Toshiyuki A. 2011. Tidal transport through
the Tsugaru Strait—Part I: Characteristics of the major tidal ﬂow and its residual
current. Ocean Science Journal 46, 4 (2011), 273–288.

[22] Quang-Hung Luu, Man F. Lau, Sebastian P.H. Ng, and Tsong Yueh
Testing multiple linear regression systems with metamor-
Journal of Systems and Software 182 (2021), 111062(1–21).

Chen. 2021.
phic testing.
https://doi.org/10.1016/j.jss.2021.111062

[23] Q. H. Luu, P. Tkalich, and T. W. Tay. 2015. Sea level trend and variability around

Peninsular Malaysia. Ocean Science 11, 4 (2015), 617–628.

[24] Y. Lyons, Luu Q. H., and P. Tkalich. 2018. Determining high-tide features (or
islands) in the South China Sea under Article 121 (1): a legal and oceanography
perspective. In The South China Sea Arbitration. Edward Elgar Publishing.
[25] Christian Murphy, M. S. Raunak, Andrew King, Sanjian Chen, Christopher Im-
briano, Gail Kaiser, Insup Lee, Oleg Sokolsky, Lori Clarke, and Leon Osterweil.
2011. On Eﬀective Testing of Health Care Simulation Software. In Proceedings of
the 3rd Workshop on Software Engineering in Health Care (Waikiki, Honolulu, HI,
USA) (SEHC ’11). Association for Computing Machinery, New York, NY, USA,
40–47. https://doi.org/10.1145/1987993.1988003

[26] M. Olsen and M. Raunak. 2019.

Through Metamorphic Testing.
2019), 91–108. https://doi.org/10.1109/TR.2018.2850315

Increasing Validity of Simulation Models
IEEE Transactions on Reliability 68, 1 (March

[27] Jeﬀrey D. Paduan and Libe Washburn. 2013. High-Frequency Radar Observa-
tions of Ocean Surface Currents. Annual Review of Marine Science 5, 1 (2013),
115–136. https://doi.org/10.1146/annurev-marine-121211-172315
[28] Rich Pawlowicz, Bob Beardsley, and Steve Lentz. 2002.

tidal harmonic

cal
including error
analysis
Computers & Geosciences
using T_TIDE.
https://doi.org/doi:10.1016/S0098-3004(02)00013-4

estimates
28,

8 (2002),

Classi-
in MATLAB
929–937.

[29] Zedong Peng, Upulee Kanewala, and Nan Niu. 2021. Contextual Understanding
and Improvement of Metamorphic Testing in Scientiﬁc Software Development.
In Proceedings of the 15th ACM / IEEE International Symposium on Empirical Soft-
ware Engineering and Measurement (ESEM). Association for Computing Machin-
ery, New York, NY, USA. https://doi.org/10.1145/3475716.3484188

[30] Daniel Pesu, Zhi Quan Zhou, Jingfeng Zhen, and Dave Towey. 2018. A Monte
Carlo Method for Metamorphic Testing of Machine Translation Services. In 2018
IEEE/ACM 3rd International Workshop on Metamorphic Testing (MET). 38–45.
[31] L.L. Pullum and O. Ozmen. 2012. Early Results from Metamorphic Testing of Epi-
demiological Models. In 2012 ASE/IEEE International Conference on BioMedical
Computing (BioMedCom). 62–67. https://doi.org/10.1109/BioMedCom.2012.17

[32] Karishma Rahman and Upulee Kanewala. 2018.

Predicting Metamorphic
Relations for Matrix Calculation Programs. In Proceedings of the 3rd In-
ternational Workshop on Metamorphic Testing (Gothenburg, Sweden) (MET

MET’22, May 9, 2022, Pittsburgh, PA, USA

Q.-H. Luu et al.

’18). Association for Computing Machinery, New York, NY, USA, 10–13.
https://doi.org/10.1145/3193977.3193983

[33] Arvind Ramanathan, Chad A. Steed, and Laura L. Pullum. 2012.

Ver-
iﬁcation of Compartmental Epidemiological Models Using Metamor-
phic Testing, Model Checking and Visual Analytics.
In 2012 ASE/IEEE
International Conference on BioMedical Computing (BioMedCom). 68–73.
https://doi.org/10.1109/BioMedCom.2012.18

[34] S. Segura, G. Fraser, A.B. Sanchez, and A. Ruiz-Cortes. 2016. A survey on Meta-
IEEE Transactions on Software Engineering 42, 9 (Sept 2016),

morphic Testing.
805–824. https://doi.org/10.1109/TSE.2016.2532875

[35] S. Segura, D. Towey, Z. Q. Zhou, and T. Y. Chen. 2020. Metamorphic Testing:

Testing the Untestable. IEEE Software 37 (2020), 46–53. Issue 3.

[36] John Seymour, Dac-Thanh-Chuong Ho, and Quang-Hung Luu. 2021. An Empiri-
cal Testing of Autonomous Vehicle Simulator System for Urban Driving. In 2021
IEEE International Conference on Artiﬁcial Intelligence Testing (AITest). 111–117.

https://doi.org/10.1109/AITEST52744.2021.00031

[37] P. Tkalich, P. Vethamony, Q. H. Luu, and M.T. Babu. 2013. Sea level trend and

variability in the Singapore Strait. Ocean Science 9 (2013), 293–300.

[38] Robert Ellis Tom Garrison. 2016. Oceanography: An Invitation to Marine Science

(9 ed.). Cengage Learning.

[39] Thomas Wahl, Shaleen Jain, Jens Bender, Steven D. Meyers, and Mark E. Luther.
2015.
Increasing risk of compound ﬂooding from storm surge and rainfall
for major US cities. Nature Climate Change 5, 12 (01 Dec 2015), 1093–1097.
https://doi.org/10.1038/nclimate2736

[40] Yang Yang and Ting Fong May Chui. 2021.

Reliability Assessment of
Machine Learning Models in Hydrological Predictions Through Metamor-
phic Testing.
Water Resources Research 57, 9 (2021), e2020WR029471.
https://doi.org/10.1029/2020WR029471

[41] Z. Q. Zhou and L. Sun. 2019. Metamorphic Testing of Driverless Cars. Commu-
nications of ACM 62, 3 (Feb. 2019), 61–67. https://doi.org/10.1145/3241979

