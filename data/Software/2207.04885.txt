2
2
0
2

l
u
J

8

]
L
F
.
s
c
[

1
v
5
8
8
4
0
.
7
0
2
2
:
v
i
X
r
a

Global Cellular Automata GCA – A
Massively Parallel Computing Model

Rolf Hoﬀmann
Technische Universit¨at Darmstadt, Germany

July 12, 2022

Abstract

The “Global Cellular Automata” (GCA) Model is a generalization
of the Cellular Automata (CA) Model. The GCA model consists of
a collection of cells which change their states depending on the states
of their neighbors, like in the classical CA model. In generalization of
the CA model, the neighbors are no longer ﬁxed and local, they are
variable and global. In the basic GCA model, a cell is structured into
a data part and a pointer part. The pointer part consists of several
pointers that hold addresses to global neighbors. The data rule deﬁnes
the new data state, and the pointer rule deﬁne the new pointer states.
The cell’s state is synchronously or asynchronously updated using the
new data and new pointer states. Thereby the global neighbors can
be changed from generation to generation. Similar to the CA model,
only the own cell’s state is modiﬁed. Thereby write conﬂicts cannot
occur, all cells can work in parallel which makes it a massively parallel
model. The GCA model is related to the CROW (concurrent read
owners write) model, a speciﬁc PRAM (parallel random access ma-
chine) model. Therefore many of the well-studied PRAM algorithms
can be transformed into GCA algorithms. Moreover, the GCA model
allows to describe a large number of data parallel applications in a
suitable way. The GCA model can easily be implemented in software,
eﬃciently interpreted on standard parallel architectures, and synthe-
sized/conﬁgured into special hardware target architectures. This ar-
ticle reviews the model, applications, and hardware architectures.

Keywords: Global Cellular Automata Model GCA, Parallel Pro-
gramming Model, Massively Parallel Model, GCA Hardware Architec-
tures, GCA Algorithms, Synchronous Firing, Dynamic Neighborhood,
Dynamic Topology, Dynamic Graphs.

1

 
 
 
 
 
 
CONTENTS

Contents

1 Introduction

2

4

2 The Global Cellular Automata Model GCA

6
2.1 The Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
. . . . . . . . . . . . . . . . . . . .
2.2 The GCA Model Variants
8
8
2.2.1 Basic Model with Stored Pointers . . . . . . . . . . . .
2.2.2 General Model with Address Modiﬁcation . . . . . . . 13
2.2.3 Plain Model . . . . . . . . . . . . . . . . . . . . . . . . 15

3 Relations to Other Models

20
3.1 Relation to the CROW Model
. . . . . . . . . . . . . . . . . . 20
3.2 Relation to Parallel Pointer Machines . . . . . . . . . . . . . . 21
3.3 Relation to Random Boolean Networks . . . . . . . . . . . . . 21

4 GCA Algorithms

22
4.1 What is a GCA Algorithm? . . . . . . . . . . . . . . . . . . . 23
4.2 Basic Model Examples . . . . . . . . . . . . . . . . . . . . . . 24
4.2.1 Distribution of the Maximum . . . . . . . . . . . . . . 24
4.2.2 Vector Reduction . . . . . . . . . . . . . . . . . . . . . 25
4.2.3 Preﬁx Sum, Horn’s Algorithm . . . . . . . . . . . . . . 27
4.3 General Model Examples . . . . . . . . . . . . . . . . . . . . . 28
4.3.1 Bitonic Merge . . . . . . . . . . . . . . . . . . . . . . . 28
2D XOR with Dynamic Neighbors . . . . . . . . . . . . 31
4.3.2
. . . . . . . . . . . 34
4.3.3 Time-Dependent XOR Algorithms
Space-Dependent XOR Algorithms
4.3.4
. . . . . . . . . . . 36
1D XOR Rule with Dynamic Neighbors . . . . . . . . . 37
4.3.5
4.4 Plain Model Example . . . . . . . . . . . . . . . . . . . . . . . 37
4.5 A New Application: Synchronous Firing . . . . . . . . . . . . 40
Synchronous Firing Using a Wave . . . . . . . . . . . . 40
Synchronous Firing with Spaces . . . . . . . . . . . . . 43
Synchronous Firing with Pointer Jumping . . . . . . . 45

4.5.1
4.5.2
4.5.3

5 GCA Hardware Architectures

49
5.1 Fully Parallel Architecture . . . . . . . . . . . . . . . . . . . . 52
5.2 Sequential with Parallel Memory Access
. . . . . . . . . . . . 53
. . . . . . . . . . . . . . . . . . 56
5.3 Partial Parallel Architectures
5.3.1 Data Parallel Architecture with Pipelining . . . . . . . 56
5.3.2 Generation of a Data Parallel Architecture . . . . . . . 59
5.3.3 Multisoftcore . . . . . . . . . . . . . . . . . . . . . . . 61

CONTENTS

6 Conclusion

3

63

7 Appendix 0: Programs for the 1D Basic and General Model 64
7.1 Basic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
7.2 General Model with Address Modiﬁcation . . . . . . . . . . . 66

8 Appendix 1: Program for Synchronous Firing within Two

Rings

68

9 Appendix 2: First Paper [1] Introducing the GCA Model

70
9.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
. . . . . . . . . . . . . . . . . . . . . . . . . 71
9.2 The GCA model
. . . . . . . . . . . . . 73
9.3 Mapping problems on the GCA model
9.3.1 Example 1: Firing Squad Problem . . . . . . . . . . . . 74
9.3.2 Example 2: Fast Fourier Transformation . . . . . . . . 75
9.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
9.5 References of First Paper (Appendix 2) . . . . . . . . . . . . . 78

10 References of Sections 1 – 6

79

1 INTRODUCTION

1

Introduction

4

Since the beginning of parallel processing a lot of theoretical and practical
work has been done in order to ﬁnd a parallel programming model 1 (for short
parallel model ) that fulﬁlls the following properties, amongst others

• User-friendly: Applications are easy to model and to program.

• Platform-independent: The parallel model can easily programmed, com-
piled and executed on standard sequential and parallel platforms.

• Eﬃcient: Applications can eﬃciently be interpreted on many diﬀerent

parallel target architectures.

• System-design-friendly: Parallel target architectures supporting the ex-
ecutions of the model (including application-speciﬁc processing hard-
ware) are easy to design, to implement, and to program.

In the following sections such a parallel model, the Global Cellular Au-
tomata (GCA) model, is described, and how it can be implemented and used.
GCA is a model of parallel execution, and at the same time it is a simple
and direct programming model. A programming model is the way how the
programmer has to think in order to map an algorithm to a certain model
which ﬁnally is interpreted by a machine. In our case, the programmer has to
keep in mind, that a machine exists which interprets and executes the GCA
model.

This model was introduced in [1] (attached, Appendix 2, Sect. 9) and
then further investigated, implemented, and applied to diﬀerent problems.
This article is partly based on the former publications [1]–[31].

A wide range of applications can easily be modeled as a GCA, and eﬃ-
ciently be executed on standard or tailored hardware platforms, for instance

• Graph algorithms [5], like Hirschberg’s algorithm computing the con-

nected cycles of a graph [17, 18], dynamic graphs

• Vector and matrix operations [16, 18, 20], vector reduction (Sect. 4.2.2),

permutations, perfect shuﬄe operations and algorithms

• Sorting and merging (Sect. 4.3.1, Sect. 9), sorting with pointers [4]

• Diﬀusion with exchange of distant particles [19]

1Diﬀerent parallel programming models are reviewed in the survey [43].

1 INTRODUCTION

5

• Fast Fourier Transformation [1] (Sect. 9)

• PRAM (Parallel Random Access Machine (Sect. 3.1) algorithms with-
out concurrent write, converted into GCA algorithms, like the Preﬁx
Sum (Sect. 4.2.3)

• N-body simulation [21]

• Traﬃc simulation [26, 27]

• Multi-agent simulation [24, 25, 28, 29], logic simulation [32],

• Hypercube algorithms2 , combinatorics, communication networks, and

neural networks

• Synchronization related to the Firing Squad Synchronization Problem

[61]–[64], a new application described in Sect 4.5.

This article is organized as follows:

1. (The Global Cellular Automata Model GCA, Sect. 2): the idea using
pointers and pointer rules in the cells, and the three model variants
basic, general and plain

2. (Relations to Other Models, Sect. 3): the relations to the CROW

PRAM model, Parallel Pointer Machines and Boolean Networks

3. (GCA Algorithms, Sect. 4): examples for the three GCA variants and

a novel application (Synchronous Firing)

4. (GCA Hardware Architectures, Sect. 5): fully parallel, sequential, and

partial parallel architectures

5. (Appendix 0, Sect. 7): Pascal program code for the 1D basic and

general model

6. (Appendix 1, Sect. 8): Pascal program code for synchronous ﬁring

7. (Appendix 2, Sect. 9): ﬁrst paper introducing the GCA Model.

2Sanjay Ranka and Sartaj Sahni: Hypercube Algorithms. Eds. Dogramaci, ¨Ozay et al.

Bilkent University Lecture Series, Springer (1990)

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

6

2 The Global Cellular Automata Model GCA

The classical Cellular Automata (CA) model consists of an array of cells
arranged in an n-dimensional grid. Each cell is connected to its neigh-
bors belonging to a local neighborhood. For instance, the von-Neumann-
Neighborhood of a cell under consideration (also called the Center Cell )
contains its nearest neighbors in the North, East, South, and West. The
next state of the center cell is deﬁned by a local rule f residing in each cell:
C ← f (C, N, E, S, W ). At discrete time t (or “at time-step t”), all cells are
applying the same rule synchronously and thereby a new generation of cell
states (a conﬁguration) for the next time t + 1 is computed.

As each cell changes only its own state (only self-modiﬁcation is allowed),
no write conﬂicts can occur. The model is inherently parallel, powerful and
simple. Many applications with local communication can smartly be de-
scribed as CA, and CAs can easily be simulated in software or realized in
parallel hardware.

The GCA model is a generalization of the CA model using a dynamically
computed global neighborhood. In order to get a ﬁrst impression of the model,
the reader may read the original paper [1] ﬁrst, attached as Appendix 2 (Sect.
9).

2.1 The Idea

The motivation to propose the GCA model was to allow a more ﬂexible
communication between cells by enhancing the CA model.

Flexible communications is obtained by (i) selecting neighbors dynami-
cally through rule computed links and (ii) by allowing any cell of the whole
array to be a direct neighbor, a so-called global neighbor. Whereas in prin-
ciple feature (i) can also be realized in classical CA, feature (ii) is a major
paradigm shift from local data access to global data access. Thereby parallel
algorithms which need instant direct communication can easily be modeled.
Global access even to the most distant cell is the extreme case of the
so-called long range or remote access. Long range access can also be called
“long-range wiring”. The term “conﬁgurable wiring” can be used when the
wiring can be changed before runtime.

In our model we allow not only a ﬁxed global wiring before processing
but also a dynamic wiring / access during runtime that can change from
generation to generation. It is important to notice that write-conﬂicts cannot
appear, because each cell modiﬁes locally its own state only. Therefore all
new cell states can be computed in parallel, and that is why we attribute the
model as “massively parallel ”. Nevertheless we have to realize that global and

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

7

dynamic neighborhood are more costly than the local and ﬁxed neighborhood
of standard CA.

In order to minimize or limit the cost of the communication network, one
can (i) implement only the communication links (the access pattern) used by
the application, or (ii) restrict the set of possible neighborhoods (the possible
links), locally or in number. In the case (ii), the algorithm for the application
has to be adjusted to the available neighborhoods.

A GCA can informally be described as follows: A GCA consists of an
array C = (c0, c1, . . . , cn−1) of cells ci, and each cell stores a state qi which
implies an array of states Q = (q0, q1, . . . , qn−1). The cell’s state qi = (di, Pi)
consists of a data part di and a pointer part Pi = (p1
i ) which con-
tains m pointers to neighbors. The pointers deﬁnes the connections (links)
to the actual neighbors which are now dynamic. The local rule does not only
update the data part but also the pointer part, and so we use two rules, the
data rule and the pointer rule. Thereby the m neighbors can be changed
from generation to generation. As shown in Fig. 1 a cell can change its
neighbors between generations.

i , . . . , pm

i , p2

Figure 1: In generation t each cell is connected to m neighbors, and it com-
putes its new neighbors. Then, in generation t + 1, each cell is connected to
its new neighbors. In this example with m = 2, cell i = 6 has the neighbors
i = 1, 8 at time-step t, and i = 3, 12 at t + 1.

All cell states of the array together constitute a conﬁguration Q(t) at a
certain time-step t. A GCA is initialized by an initial conﬁguration Q(t = 0).
The result of the computation is the ﬁnal conﬁguration Q(tf inal).

Some notions that will be used in the sequel:

• Cell Index: The index that identiﬁes a cell.

• Address: (Absolute) A cell index. (Relative) An oﬀset to the cell’s own

index.

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

8

• Pointer: An address pointing to a cell.

• Index Notation: We are mainly using subscripts or superscripts for in-
dexing. Alternatively we may use square brackets to denote indexing
instead of subscripts (e.g. qpointer = Q[pointer]). We prefer to use
square brackets when dynamic addressing by pointers shall be empha-
sized.

2.2 The GCA Model Variants

Three model variants are distinguished, the basic model, the general model
and the plain model. They are closely related and can be transformed into
each other to a large extent. It depends on the application or the implemen-
tation which one will be preferred. The model variants mainly diﬀer in the
way how addresses to the neighbors are stored and computed:

• Basic Model

Pointers are part of the cell’s state which deﬁne the global neighbors.
The are computed at the previous time-step t − 1 and used at the
current time-step t.

• General Model

Pointers are available as in the basic model.
In addition, they can
further be modiﬁed / speciﬁed at the current time-step t before access.

• Plain Model

The state is not structured into ﬁelds, the actual pointers are derived
from the current state before access.

The GCA model can easily be programmed. A compilable PASCAL
program is given in Section 7 (Appendix 0) that simulates the 1D XOR rule
with two dynamic neighbors. The basic model is used in Sect. 7.1, and the
general model with a common address base is used in Sect. 7.2.

2.2.1 Basic Model with Stored Pointers

The basic model [1, 2] was the ﬁrst one deﬁned in order to facilitate the
description of cell-based algorithms with dynamic long-range interactions.
([1] is attached as Appendix 2, Section 9). The cell’s state consists of two
parts, a data part d, and a pointer part P with m pointers (p1, p2, . . . , pm).
The pointers deﬁne directly the global neighbors. They are computed in the

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

9

Figure 2: Basic GCA model, with two pointers. The cell state is a com-
position of a data state d and the pointer states (p1, p2).
(Step 1a) Two
global cell states are accessed by the pointers and dynamically linked to the
cell. (Step 1b) The new data state d(cid:48) and the new pointer states (p1(cid:48), p2(cid:48)) are
computed by the data rule f and the pointer rules G = (g1, g2). (Updating)
The new state (d(cid:48), p1(cid:48), p2(cid:48)) is copied to the state (d, p1, p2). Remark: In this
ﬁgure the cell’s index i of the items was omitted.

previous generation t − 1 to be used in the current generation t. Usually they
store relative addresses to neighbors, but absolute addresses are allowed, too.
A basic GCA is an array C = (c0, c1, . . . , cn−1) of dynamically intercon-

nected cells ci. Each cell is composed of storage elements and functions:
i, P (cid:48)

i, fi, Gi) = ((di, Pi), (d(cid:48)
ci = (qi, q(cid:48)
i ), fi, Gi).
For a formal deﬁnition we use the elements
(I, A, D, f, G, q, q(cid:48), m, u)
as explained in the following:

• I is a ﬁnite index set. A unique index (or label, or absolute address)
from this set is assigned to each cell. In the following deﬁnitions we
want to use only a simple one-dimensional indexing scheme with cell
indexes i ∈ I = {0, 1, . . . , n − 1}. For modeling graph algorithms, we
can interpret an index as a label of a node. For modeling problems in
discrete space, we can map each point in space to a unique index, or
we may use a multi-dimensional array and a corresponding indexing
scheme.

• m is the number of pointers to dynamic neighbors, and n is the num-

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

10

ber of cells, where 1 ≤ m < n. We call a GCA with m arms/pointers
“m-armed GCA”.

• qi = (di, Pi) ∈ Q is the cell’s state and q(cid:48)

i = (d(cid:48)

i, P (cid:48)

i ) is its new state.

• Q = D × Am is the set of cell states.

• di ∈ D is the data state, where D is a ﬁnite set of data states.

• A is the address space. p ∈ A is an address used to access a global
neighbor. It can be relative (to the cell’s index i) or absolute. Such an
address is also called eﬀective address.

A = I = {0, . . . , n − 1}, is the address space for absolute address-
ing, or

A = R = {−n/2, . . . , (n − 1)/2}, is the address space for relative
addressing, where “/” means integer division. That is,

R =

(cid:40)

if n even
{−n/2, . . . , +(n − 2)/2}
{−(n − 1)/2, . . . , +(n − 1)/2} if n odd

• Pi is a vector of pointers, the pointer part of the cell’s state.

Pi = (p1

i , p2

i , . . . , pm

i ), where pk

i ∈ A.

• fi is the data rule.

fi : I × Q × Qm → D
It is called uniform, if it is index-independent (∀i : fi = f ).

• Gi is the pointer rule (also called neighborhood rule).

It computes m pointers pointing to the new neighbors at the next time
t + 1 depending on the cell’s state and the neighbors’ states at the
current time t.
Gi : I × Q × Qm → Am
It is called uniform, if it is index-independent (∀i : Gi = G).
We can split the whole neighborhood rule into a vector of single neigh-
borhood rules each responsible for a single pointer:
Gi = (g1

: I × Q × Qm → Am.

i ) where gj=1..m

i , . . . , gm

i

• d(cid:48)

i = fi is the new data state at time-step t after computation stored
temporarily in a memory.

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

11

• P (cid:48)

i = Gi is the new vector of pointers (or the new neighborhood ) at

time-step t after computation stored temporarily in a memory.

• u ∈ {synchronous, asynchronous} is the updating method.

u = synchronous
(Phase 1) Each cells computes its new state q(cid:48)

i = (d(cid:48)

i, P (cid:48)

i ).

i ], Q[p2

i = (Q[p1
i = (Q[i + p1

– (Step 1a) The neighbors’ states Q∗
i ]) if pj
i ], . . . Q[i + pm

Q∗
Q∗
where Q is the array of cell states:
Q = (Q[0], Q[1], . . . , Q[n − 1]) = (q0, q1, . . . , qn−1)

i ], . . . Q[pm

i ], Q[i + p2

i ]) if pj

i are accessed. 3
i is an absolute address,

i is a relative address,

– (Step 1b) The new data state and the new neighborhood are com-

puted by the rules fi and Gi and stored temporarily.
d(cid:48)
i ← fi(qi, Q∗
i )
i ← Gi(qi, Q∗
P (cid:48)
i )

i).

(Phase 2) For all cells, the new state is copied to the state memory
(qi ← q(cid:48)
The order of computations during Phase 1, and the order of updates
during Phase 2 does not matter, but the two phases must be separated.
Parallel computations and parallel updates within each phase are al-
lowed, as it is typically the case for synchronous hardware with clocked
registers.

u = asynchronous
(Only one Phase) Each cells computes its new state q(cid:48)
which then is copied immediately to qi.

i = (d(cid:48)

i, P (cid:48)
i )

– (Step 1a) The neighbors’ states are accessed,

like in the syn-

chronous case.

– (Step 1b) The new data state and the new neighborhood are com-
puted by the rules e and g and stored temporarily, like in the
synchronous case.

– (Step 1c) The computed new state is immediately stored in the

state variable.
(di, Pi) ← (d(cid:48)

i, P (cid:48)

i ).

3In the case that the actual access index is outside its range, it is mapped to it by the

modulo operation. Q[i + p]) (cid:55)→ Q[i + p mod n])

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

12

Every selected cell computes its new state and immediately updates its
state. Cells are usually processed in a certain sequential order (includ-
ing random). It is possible to process cells in parallel if there is no data
dependency between them.

Relative and Absolute Addressing. We have the option to use either
relative or absolute addressing. Our understanding is that a pointer pj
i holds
an eﬀective address (either relative or absolute), that is ready to access a
neighbor. In the case of absolute addressing, the neighbor’s state is Q[pj
i ],
and in the case of relative addressing, the neighbor’s state is Q[i ⊕ pj
i ] where
’⊕’ means addition mod n.

This means, that in the case of relative addressing, the cell’s index has to
be added to the pointer in order to access the array of states by an absolute
address. Another way is to use an index-aware access network (or method)
that automatically takes into account the cell’s position, for instance by an
adequate wiring. For instance multiplexers can be used where input 0 is
connected to the cell i itself, input 1 to the next cell i + 1, and so on in cyclic
order. The multiplexer can then directly be addressed by relative addresses
(mapped to positive increments that identify the inputs of the multiplexers).
Usually relative addressing is the ﬁrst choice, it is more convenient for
applications because (i) the initial pointer connections are easier to deﬁne
and often in a uniform way, and (ii) the initial pointer connections often do
not depend on the size n of the array, and (iii) pointer modiﬁcations are
easier to conduct.

Further Dependencies. In some applications, the rules shall further
depend on the current time t (counted in every cell, or supplied by a central
control), or on the states W (i) of some additional ﬁxed local neighbors as it
is standard in classical CA. Then we can extend the parameter list of the
data and pointer rule by (t, W (i)), or more general by (i, t, W (i, t)).

GCA Implementation Complexity.

• Memory Capacity. The data part of a cell needs a constant number
of bits bit(D) where bit(D) = δ is the number of bits needed to store
the data state D. The pointer part needs the capacity m · log2 n, it
depends on n because the larger the number of cells, the larger becomes
the address space. So the whole memory capacity is

2n · V (n, m) , where V (n, m) = δ + m · log2 n is the word length of the
cell state.

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

13

• Data and Pointer Rule. The data rule has m + 1 inputs of word length

V and δ output bits.

The whole pointer rule has the same number of inputs bits as the data
rule, but m · log2 n output bits. We assume that the internal wiring is
included in the rules. Then the complexity of the rules is in

O(nV × V ) = O((n + 1) · V (n, m)).

• Communication Network.

– Interconnections. The number of links between cells is n·m(n−1)
because each cell can have m(n − 1) neighbors. The average link
length is n/4 × space-unit for a ring layout structure. Each link is
V (n, m) bit wide. Then we get for the overall eﬀort (considering
wire length and bit width capacity) O(mn3 × V (n, m)).

– Switches. In addition, mn switches or multiplexers are necessary
for selecting the neighbors. Each multiplexer has n inputs and one
output with a word length of V bits. For each bit of V , a simple
one-bit multiplexer with a complexity of O(V ) is needed. So a
word multiplexer has the complexity O(nV ). The complexity for
all nm multiplexers is then O(mn2 × V (n, m)).

In order to keep the eﬀort for the communication network low, the
number m of pointers/arms should be small, especially equal to one,
and the really used neighbors by the algorithm should be analyzed in
order to identify unused links. The eﬀort for the communication net-
work can be reduced by implementing only the required access pattern
for a certain set of applications, or one could restrict the set of possible
neighborhoods (links to neighbors) in advance per design and then use
only the available links for programming the algorithm. 4 In principle,
any network with an aﬀordable complexity can be used that allows to
read information from remote locations, not necessarily in one time-
step. – The problem of GCA wiring was partially addressed in [32, 33].

2.2.2 General Model with Address Modiﬁcation

Now we add to the basic model (Sect. 2.2.1) an address modiﬁcation function
and call this model general model.
In the basic model, the pointers store
eﬀective addresses that are directly used to access the neighbors, and they

4For instance, only hypercube connections could be supplied. Then hypercube
algorithms can directly be implemented, and other algorithms have to be trans-
formed/programmed into a “pseudo” hypercube algorithm, if possible.

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

14

Figure 3: General GCA model, with address modiﬁcation. Example
with two eﬀective addresses. Address modiﬁcation functions e1, e2 are added
to the basic model that allow to modify the addresses before access, at the
beginning of the current time-step.

are computed and ﬁxed in the preceding generation t − 1.
In the general
model, the former stored pointer values pk=1..m get a diﬀerent meaning, they
represent now address bases that will undergo additional modiﬁcations into
real eﬀective addresses ˆpk=1..m. The eﬀective addresses ˆpk are computed at
the beginning of each time-step t by an extra address modiﬁcation function
ek for each address k = 1 . . . m:
i , p2

i = ek(p1
ˆpk
Further parameters may be taken into account, like the cell index i, the
current time t, or the current state of additional locally ﬁxed neighbors
W (i, t). Then we yield the more general formula

i , . . . , pm

i , di).

i (t), p2

i (t), . . . , pm

i (t), di(t), i, t, W (i, t)).

i (t) = ek(p1
ˆpk
Usually, only a subset of all possible arguments will be used, for instance
ˆpk
i (t) = ek(pk
ˆpk
i (t) = ek(pk
ˆpk
i (t) = ek(pk
ˆpk
i (t) = ek(pk
ˆpk
i (t) = ek(pk
Compared to the basic model, the general model has the advantage that a
GCA algorithm can immediately (in the same time-step, without a one-step

i (t), di(t), i, t, W (i, t)), not depending on pi(cid:54)=k
i (t), di(t)), not depending on pi(cid:54)=k
, i, t, W
i (t), di(t), W (i, t)), not depending on pi(cid:54)=k
i (t), i), not depending on pi(cid:54)=k
i (t), t), not depending on pi(cid:54)=k

, di, t, W , index-dependent
, di, i, W , time-dependent.

, i, t

i

i

i

i

i

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

15

delay) specify its global neighbors, for instance depending on the states of
local neighbors. To summarize, an eﬀective address is (i) partly computed
in the preceding generation (in particular as address base in the same way
as pointers are computed in the basic model), and then (ii) further speciﬁed
by an address modiﬁcation function in the current generation.

Examples. We assume relative addressing and one pointer only (single-
arm GCA). The used operator ⊕ denotes an addition mod n where the result
is mapped into the deﬁned relative address space, a ⊕ b = (a + b) mod n −
(cid:98)n/2(cid:99). Examples for address modiﬁcations:

• The eﬀective address depends on the current data state.

if di = 0 then ˆpi = pi else ˆpi = pi ⊕ 1

• The eﬀective address depends on the current time.

if odd(t) then ˆpi = pi ⊕ (+1) else ˆpi = pi ⊕ (−1)

• The eﬀective address depends on the current data state of the left
and right neighbor, which are additional ﬁxed neighbors as we have in
classical CA.

if (di−1 = 0) and (di+1 = 0) then ˆpi = pi ⊕ 1 else ˆpi = pi

• The eﬀective address depends on the current pointer states of the left

and right neighbor, which are ﬁxed neighbors.

ˆpi = pi−1 ⊕ pi+1

Variant of the General Model with a Common Address Base.
Instead of using m separate address bases, it is possible to combine them
into one common pi only. Then pi can be termed “common address base” or
neighborhood address information. All m eﬀective addresses are then derived
i = ek(pi, di) for k = 1 . . . m. This variant
from this common address base: ˆpk
can save storage capacity if only a few special neighborhoods are used by the
algorithm.

2.2.3 Plain Model

In the plain GCA model, the pointers are encoded in the cell’s state and
therefore must be decoded before neighbors can be accessed. The cell’s state
is not structured into separate parts (data, pointer) as in the basic and the
general model. (The plain model was also called condensed GCA model in a
former publication [7].)

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

16

Figure 4: Plain GCA model. Example with two eﬀective addresses. They
are computed by the pointer functions h1, h2 at the beginning of the current
time-step before accessing the neighbors.

A plain GCA is an array C = (c0, c1, . . . , cn−1) of dynamically intercon-

nected cells ci. Each cell i is composed of storage elements and functions:

i, fi, Hi).

ci = (qi, q(cid:48)
For a formal deﬁnition we use the elements (I, Q, qi, q(cid:48)

i, m, A, Pi, Hi, fi, u)

as explained in the following:

• I is a ﬁnite index set which supplies to each cell a unique index i

(label, absolute address) .

i ∈ I = {0, 1, . . . , n − 1}.

• Q is a ﬁnite set of states. They are not separated into data and pointer

states.

• qi ∈ Q is the cell’s state and q(cid:48)

i ∈ Q is its new state. Storage elements
(memories, registers) are provided that can store the cell’s state and
its new state.

• m is the number of pointers to dynamic neighbors, and n is the

number of cells, where 1 ≤ m < n.

• A is the address space. p ∈ A is an address used to to access a global

neighbor. It can be relative (to the cell’s index i) or absolute.

A = I = {0, . . . , n − 1} is the address space for absolute address-
ing, or

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

17

A = R = {−n/2, . . . , (n − 1)/2} is the address space for relative
addressing, where “/” means integer division. That is,

R = {−n/2, . . . , +(n − 2)/2} if n even, or

R = {−(n − 1)/2, . . . , +(n − 1)/2} if n odd.

• Pi is a vector of pointers, Pi = (p1

i , . . . , pm
The pointers are deﬁned by the pointer function

i , p2

i ), where pk

i ∈ A.

Pi = Hi

(∀k = 1..m : pk

i = hk

i ), explained next.

• Hi = (h1

i , h2

i , . . . , hm

i ) is the pointer function (also called neighbor-
hood selection function, addressing function). It computes m pointers
(relative or absolute eﬀective addresses) pointing to the current neigh-
bors depending on the cell’s state q at the current time t before access.
Hi : I × Q → Am
It is called uniform, if it is index-independent (∀i : Hi = H).
We can split the whole pointer function into a vector of single pointer
functions, each responsible for a single pointer separately:
Hi = (h1

i ) where hk=1..m

: I × Q → A.

i , . . . , hm

i

• fi is the cell rule, taking the states of its global neighbors Q∗

i ∈ Qm

into account.
fi : I × Q × Qm → Q
It is called uniform, if it is index-independent (∀i : fi = f ).

• u ∈ {synchronous, asynchronous} is the updating method.

u = synchronous
(Phase 1) Each cells computes its new state q(cid:48)

i = fi.

i ], Q[h2

i = (Q[h1
i = (Q[i + h1

– (Step 1a) The neighbors’ states are accessed. 5
i ], . . . Q[hm

Q∗
Q∗
i ], Q[i + h2
where Q is the vector of cell states:
Q = (Q[0], Q[1], . . . , Q[n − 1]) = (q0, q1, . . . , qn−1)

i ]) if hj
i ], . . . Q[i + hm

i ]) if hj

i is an absolute address,

i is a relative address,

5In the case that the actual access index is outside its range, it is mapped to it by the

modulo operation. Q[i + p]) (cid:55)→ Q[i + p mod n])

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

18

– (Step 1b) The new state is computed by the cell rule fi and stored

temporarily.
i ← fi(qi, Q∗
q(cid:48)
i )

i).

(Phase 2) For all cells the new state is copied to the state memory
(qi ← q(cid:48)
The order of computations during Phase 1 and the order of updates
during Phase 2 does not matter, but the phases must be separated.
Parallel computations and parallel updates within each phase are al-
lowed, as it is typically the case in synchronous hardware with clocked
registers.

u = asynchronous
(Only one Phase) Each cell computes its new state c(cid:48)
immediately copied to ci.

i which is then

– (Step 1a) The neighbors’ states are accessed,

like in the syn-

chronous case.

– (Step 1b) The new cell state is computed by the rules fi and stored

temporarily, like in the synchronous case.

– (Step 1c) The computed new state is immediately copied to the

state variable.
qi ← q(cid:48)
i.

Every selected cell computes its new state and updates immediately its
state. Cells are usually processed in a certain sequential order (includ-
ing random). It may be possible to process some cells states in parallel
if there is no data dependence between them.

In some applications the rules and functions may further depend on the
current time t (counted in each cell or in a central control), or on the states
W (i) of some additional ﬁxed local neighbors. Then we can extend the
parameter list of the cell rules by (t, W (i)).

A typical application modeled by GCA needs only one or two pointers,
and the set of really addressed cells during the run of a GCA algorithm
(Sect. 4) – the access pattern – is often quite limited. This means that the
neighborhood address space needed by a speciﬁc algorithm is only a subset
of the full address space. Then the cost to store the address information and
for the communication network can be kept low. Therefore whole GCA can
be designed / minimized / conﬁgured with regard to a speciﬁc application
or a class of applications.

2 THE GLOBAL CELLULAR AUTOMATA MODEL GCA

19

Is a GCA an array of automata as CA are? Yes, because we can use a
CA with a global neighborhood (ﬁxed connections to every cell) and embedd
a GCA. We can also construct a digital synchronous circuit as for example
shown in Fig. 5.

Figure 5: Plain GCA model, single-arm. (a) Each cell i can select any
other cell as its actual neighbor. (b) A possible implementation in hardware,
absolute addressing. All cell states are inputs to a multiplexer. The actual
cell is selected by the pointer pi = hi(qi). The rule fi(qi, Q[pi]) computes the
new state q(cid:48)
i.

Single-arm. For many applications it is suﬃcient to use one neighbor

only. Then we have
i := fi(qi, q∗
q(cid:48)

i ) where

q∗
i = Q[pi] for absolute addressing, and
q∗
i = Q[i ⊕ pi] for relative addressing,
where pi = hi(qi) with the declaration hi = h1

i and pi = p1
i .

The principal structure of such a single-arm GCA is shown in Fig. 5. All
cell states are inputs to a multiplexer. The actual neighbor is selected by the
pointer pi = hi(qi). Then the rule fi(qi, Q[pi]) computes the new state.

3 RELATIONS TO OTHER MODELS

20

3 Relations to Other Models

3.1 Relation to the CROW Model

The GCA model is related to the CROW (concurrent read, owner write)
model [34, 35, 36, 44], a variant of the PRAM (parallel random access ma-
chine) models.

The PRAM is a set of random access machines (RAM), called proces-
sors, that execute the instructions of a program in synchronous lock-step
mode and communicate via a global shared memory. Each PRAM instruc-
tion takes one time unit regardless whether it performs a local or a global
(remote) operation. Depending on the access of global variables, variants of
the models are distinguished, CRCW (concurrent read, concurrent write),
CREW (concurrent read, exclusive write), EREW (exclusive read, exclusive
write), and CROW.

The CROW model consists of a common global memory and P proces-
sors, and each memory location may only be written by its assigned owner
processor. In contrast, the GCA model consists of P cells, each with its lo-
cal state memory (data and pointer part) and its local rule (together acting
as a small processing unit updating the data and pointer state). Thus the
GCA model is (i) “cell based ”, meaning that the state and processing unit
are distributed and encapsulated, similar to objects as in the object oriented
paradigm, and (ii) the cells are structured into (data ﬁelds, pointer ﬁelds,
data and pointer rules (for the basic and general model)) according to the
application. A processing unit of a GCA can be seen as special conﬁgured
ﬁnite state automaton, having just the processing features which are needed
for the application. On the other hand, the CROW model is “processor
based ”, it uses universal processors with a standard instruction set indepen-
dent of the application. Furthermore, in the GCA the data and pointer state
are computed in parallel through the deﬁned rules in one time-step, whereas
in the PRAM model several instructions (and time-steps) of a program have
to be executed to realize the same eﬀect.

There is a lot of literature about PRAM models, algorithms and their
computational properties, like [39, 40, 41, 43]. The models EROW (exclusive
read, owner write) [42] and OROW (owner read, owner write) [37, 38] may
also be of interest in this context.

In this paper we will not investigate the computational properties such as
complexity classes for time and space of the GCA model. Nevertheless we can
see a close relationship to the CROW model, because we can (i) distribute
the global memory cells with “owner’s write” property to distinct GCA cells,
and (ii) we can translate a CROW algorithm with several instructions to

3 RELATIONS TO OTHER MODELS

21

a GCA algorithm with a few data and pointer rules. When we want to
compare these models in more depth we have to specify whether we allow
an unbounded number of processors and global memory vs. the number of
GCA cells and their local memory size.

3.2 Relation to Parallel Pointer Machines

The term “Parallel Pointer Machines” is ambiguous and stands for diﬀerent
models using processors and memory cells linked by pointers. Among them
are the KUM (Kolmogorov-Uspenskii machine 1953, 1958) and the SMM
(Storage Modiﬁcation Machine, Sch¨onhage 1970, 1980). While the KUM
operates on an undirected graph with bounded degree, the SMM operates on
a directed graph of bounded out-degree but possibly unbounded in-degree.
Another model similar to SMM is the Linking Automaton (Knuth, The Art
of Computer Programming, Vol. 1: Fundamental Algorithms, 1968, 1973).
More details about parallel pointer machines are given in [45]– [50].

These models were mainly deﬁned in the context of graph manipulation.
The HMM model [46] uses a global memory with exclusive write similar to
the CROW model with n processors and with dynamic links between them.
Our GCA model diﬀers in the way how the pointers are stored, interpreted
and manipulated. It comes along in three variants, it is cell-based without a
common memory, and it is an easy understandable extension of the classical
CA.

3.3 Relation to Random Boolean Networks

Random Boolean Networks (RBN) were originally proposed by Kauﬀmann in
1969 [51, 52] as a model of genetic regulatory networks. A RBN consists of N
nodes storing a binary state s ∈ {0, 1}, where each node i ∈ {0 . . . N −1} = I
receives K states sij (at time t) from the connected nodes ij∈{1...K} and
computes its next state (valid at time t + 1) by a boolean function fi:

∀i : si(t + 1) = fi(si1(t), si2(t), . . . sik(t)) .
Considered as a directed graph, each node is a computing node that re-
ceives K inputs via the arcs from the connected source nodes. In other words,
the fan-in (in-degree) of a node is K, equal to the number of arrows pointing
to that node, the head ends adjacent with that node. Arcs can be seen as
data-ﬂow connections from source nodes to computing nodes. There can be
deﬁned some special nodes dedicated for data input and output. The network
graph can also be called “wiring diagram”. In terms of CA, a node is a cell
that can have read-connections to any other cell. In RBN, the connections

4 GCA ALGORITHMS

22

and functions are ﬁxed during the dynamics, but randomly chosen. If the
connections and functions are designed / conﬁgured for a special application,
then the network is called Boolean Network (BN). So a RBN is a randomly
conﬁgured BN. RBN are often considered as large sets of diﬀerent conﬁgured
instances which then are used for statistical analysis. Normally the fan-in
K is much smaller than N , but in the extreme case a node can be aﬀected
by all others. Usually the number K is constant for all nodes, but it can be
node dependent (non-uniform), too.

The GCA model described in the following sections is a more general
model that includes BN. In the GCA model, nodes are called cells and source
nodes sij are called neighbors. A cell can point to any global neighbor, and
the pointers can be changed dynamically by pointer rules. Pointers in a GCA
graph represent the actual read-access to a neighbor, whereas in a BN graph
the pointers are inverted and represent the data-ﬂow.

The GCA model provides dynamically computed links, whereas in BN
the links are ﬁxed/static. The rules of GCA tend to be cell/space/index
independent, whereas in BN the boolean functions tend to be node/index
dependent. Another minor diﬀerence is that in the GCA model the own
state si is always available as parameter in the next state function, meaning
that in GCA self-feedback is always available, whereas in BN self-feedback it
intentional by a deﬁned wire (self-loop in the graph).

More information about RBN and BN can be found e.g. in [53]–[60].

4 GCA Algorithms

Several GCA algorithms were already described in [1] (Reprint, Appendix 2,
Sect. 9), and in [2]–[31].

Examples for GCA Algorithms are presented in the following Sections:

4.2.1 (Distribution of the Maximum),
4.2.2 (Vector Reduction),
4.2.3 (Preﬁx Sum, Horn’s Algorithm),
4.3.1 (Bitonic Merge),
4.3.2 (2D XOR with Dynamic Neighbors),
4.3.4 (Space Dependent XOR Algorithms),
4.3.5 (1D XOR Rule with Dynamic Neighbors),
4.4 (Plain Model Example).

New GCA algorithms about synchronization are presented in the Sections

4.5.1 (Synchronous Firing Using a Wave),
4.5.2 (Synchronous Firing with Spaces),

4 GCA ALGORITHMS

23

4.5.3 (Synchronous Firing with Pointer Jumping).

4.1 What is a GCA Algorithm?

We will use the notion “GCA algorithm”, meaning a speciﬁc GCA that com-
putes a sequence of conﬁgurations (global states) that is not constant allover.
As in CA, we start with an initial conﬁguration and expect a dynamic evolu-
tion of diﬀerent conﬁgurations. We distinguish decentralized algorithms from
controlled algorithms. We call a decentralized algorithm also uncontrolled,
autonomous, standalone, or (fully) local. If not further speciﬁed, we mean
with a GCA algorithm a decentralized GCA algorithm.

What is a decentralized GCA algorithm?

• Decentralized GCA algorithm: There is no central control which inﬂu-
ences the cells behavior. The cells decide themselves about their next
state. The only inﬂuence is the central clock that synchronizes par-
allel computing and updating when we are using synchronous mode
and not asynchronous mode. Starting with an initial conﬁguration at
time t = 0, a new generation at t + 1 is repeatedly computed from
the current generation at t. We may require or observe that the global
state converges to an attractor (a ﬁnal conﬁguration or an orbit of
conﬁgurations), or that it changes randomly.

Controlled GCA algorithms. We may enhance our model for more
general applications by adding a central controller that can be a ﬁnite state
automaton. We distinguish three types. The properties of these model types
is a subject of further research.

• With simple control. There is a central control that sends some basic
common control signals to the cells. Typical signals are Start, Stop,
Reset, a global Parameter, the actual time t given by a central
Time-Counter, or a time-dependent Control Code.

• With simple loop control. In addition, the control unit is able to main-
tain simple control structures like loops. There can be several loop
counters and the number of loops may depend on parameters or on the
size n of the cell array. The control unit may send diﬀerent instruction
codes depending on the control state. These codes are interpreted by
the cells in order to activate diﬀerent rules. Not allowed is the feedback
of conditions from the cells back to the control unit.

4 GCA ALGORITHMS

24

• With feedback. In addition to the case before, the cells may send con-
ditions back to the control. Thereby central conditional operations
(if ) and conditional loops (while, repeat) can be realized. A condition
can be translated into diﬀerent instruction codes or used to terminate
a loop. More complex control units may be deﬁned if necessary, pro-
grammable, or supporting the management of subroutines or recursion.

4.2 Basic Model Examples

4.2.1 Distribution of the Maximum

Figure 6: Maximum. (a) Each cell computes the maximum (operator “+”)
of all data elements. The pointer to the neighbor is constant (p = 1), meaning
that here always the right neighbor is taken into account. (b) The data ﬂow.
The algorithm takes n − 1 parallel steps.

All cells shall change their data state into the maximum value of all cells.
The GCA algorithm is rather trivial. The cell’s state is q = (d, p), where d
is an integer and p is a relative pointer. Initially p = 1 for all cells, each cells
points to its right neighbor. The neighbor’s data is d∗ = D[abs(p)], where
abs() maps a relative address to the (absolute) index range {0, · · · , n − 1}.
If it is clear from the context, then abs() may be omitted, and we can simply
write d∗ = D[p], or in “dot-notation” : p.d = d∗ = D[p].

The data rule is d(cid:48) = max(d, d∗), and the pointer rule may be a constant
p(cid:48) = 1. The algorithm takes on the value from the right if it is greater. The
implementation corresponds to a cyclic left shift register, if the data rule

4 GCA ALGORITHMS

25

were d(cid:48) = d∗. The algorithm takes n − 1 steps. In a conventional way we can
write the rules as follows

di(t + 1) = max(di(t), di+pi(t)) = max(di(t), di+1(t))
pi(t + 1) = pi(t) = 1.

We can notice that is algorithm can also be described by a classical CA
because a ﬁxed local neighborhood is used. Indeed, the GCA model includes
the CA model. But we leave the CA model and come to the GCA model
when we make use of the global neighborhood (up to p = n) and use the
dynamic neighborhood feature. Therefore we yield a real GCA algorithm
when we use a “real” GCA pointer rule p(cid:48) = f (p, d, n, ...),

for example

p(cid:48) = p + 1 mod n
p(cid:48) = 2p mod n
p(cid:48) = n/2
p(cid:48) = random.

We will not investigate these alternatives here further, and whether they
perform better or worse for distributing the maximal value. The following
real GCA algorithm can also be used to compute the maximum, and it needs
only log2 n steps.

4.2.2 Vector Reduction

Given a vector D = (d0, d1, . . . dn−1). The reduction function reduce() is

reduce(D) = d0 + d1 + . . . + dn−1

where ’+’ denotes any dyadic reduction operator, like max, min, and, or,
average.

In order to show the principle, we consider the simpliﬁed case where the
number of cells is a power of two, n = 2k. Then the reduction can be de-
scribed as a data parallel algorithm

for t = 1 to k do

parallel for all i

d(cid:48)
i = di + di+2k−1 mod n

end parallel

end for

4 GCA ALGORITHMS

26

Figure 7: Vector Reduction. The algorithm computes the sum of all
In the ﬁrst
elements. Each cell computes the sum in a tree-like fashion.
time-step (t = 0) → (t = 1) each cells adds the data value of its right
neighbor (with relative pointer value +1). In the following generations the
distance to the neighbor is doubled (p = 2, 4, . . .). (a) The cells with their
pointers, dynamically changing. (b) The data ﬂow (inverse to the pointers).

The data elements are accumulated in a tree like fashion and after k =
log2 n steps every cell contains the sum. The algorithm can be modiﬁed if
the number of cells is not a power of two, or if the result shall appear only
in one distinct cell.

We can easily transform the data parallel algorithm into a GCA algo-

rithm:

q = (d, p)
d∗ = D[abs(p)]
d(cid:48) = d + (p (cid:54)= 0) · d∗ data rule, if (p (cid:54)= 0) then add
p(cid:48) = 2p mod n

cell state, p is a relative pointer, initially set to +1
neighbor’s data state

pointer rule, p = 1, 2, 4 . . . , n/2, 0 .

The problem of controlling the algorithm (Initialize, Start, Stop/Halt)
can be implemented diﬀerently. We assume always an initial conﬁguration
at time t = 0 to be given, and we don’t care how it is established. Then we
assume that a hidden or visible central time counter t := t+1 is automatically
incremented generation by generation. In some time-dependent algorithms
the central time counter can be used, or a separate counter is supplied in every

4 GCA ALGORITHMS

27

cell in order to keep the algorithm decentralized. The ﬁnal conﬁguration is
reached when the pointer’s value changes to 0 by the modulo operation.
Then p(cid:48) = p = 0 holds. The algorithm may be further active, but the cell’s
state is not changing any more. The algorithm can halt automatically in a
decentralized way when all cells decide to change into an inactive state when
p = 0.

4.2.3 Preﬁx Sum, Horn’s Algorithm

Figure 8: Horn’s Algorithm. The algorithm computes the preﬁx sum. In
the ﬁrst time-step (t = 0) → (t = 1) each cells i ≥ 1 adds the data value of
its left neighbor (relative pointer value -1). In the following generations, the
distance to the dynamic neighbor is −2, −4, . . ., and the number of active
adding cells is decreased by 1 until n/2. The ﬁgure shows the data ﬂow. The
shaded data elements mark already computed results.

Given a vector D = (d0, d1, . . . dn−1). The preﬁx sum is the vector (si)

where

s0 = d0
s1 = s0 + d1 = d0 + d1
s2 = s1 + d2 = d0 + d1 + d2
. . .
sn−1 = sn−2 + dn−1 .

4 GCA ALGORITHMS

28

The preﬁx sum can be computed in diﬀerent ways. Horn’s algorithm is

a CREW data parallel algorithm for n = 2k elements:

for t = 1 to k do

parallel for i = 1 to n − 1

if i ≥ 2t−1 then d(cid:48)

i = di + di−2t−1

endparallel

endfor .

The number of additions (active processors/cells) decreases step by step,
it is (n − 1, n − 2, n − 4, . . . n/2). The data parallel algorithm can be trans-
formed into the following GCA algorithm straight forward.

q = (d, p)
d∗ = D[abs(p)]
d(cid:48) = d + (i ≥ −p) · d∗ data rule, if (i ≥ −p) then add
p(cid:48) = 2p mod n

cell state, p is a relative pointer, initially -1
neighbor’s data state

pointer rule, p = −1, −2, −4 . . . , −n/2, 0

An advantage of this algorithm is that the number of simultaneous read
accesses (fan-out) is not more than two. There exists another algorithm
where the number of active cells and the maximal fan-out are equal to n/2.

4.3 General Model Examples

4.3.1 Bitonic Merge

The bitonic merge algorithm sorts a bitonic sequence. A sequence of numbers
is called bitonic, if the ﬁrst part of the sequence is ascending and the second
part is descending, or if the sequence is cyclically shifted. Consider a sequence
of length n = 2k. In the ﬁrst step, cells with distance 2k−1 are compared,
Fig. 9. Their data values are exchanged if necessary to get the minimum
to the left and the maximum to the right.
In each of the following steps
the distance between the cells to be compared is halve of the distance of the
preceding step. Also with each step the number of sub-sequences is doubled.
There is no communication between diﬀerent sub-sequences. The number of
parallel steps is k = log2 n.

The cell’ state is a record q = (d, i, p), where d ∈ DataSet, i ∈ I is the
cell’s identiﬁer, and p ∈ 0, 1, 2, ..., 2k−1 is the pointer base, initially set to 2k−1.

4 GCA ALGORITHMS

29

(a)

(b)

Figure 9: (a) Initial at t = 0 a bitonic sequence of length n = 8 is given.
Cells 0, 1, 2, 3 access cells 4, 5, 6, 7 and vice versa. The initial pointer base
is 4 (binary 100), and it is used to mask the cell’s index in order to select
either ˆp = peﬀ = +4 or peﬀ = −4. Iteratively the pointer base is shifted to
the right (division by 2) yielding peﬀ = ±2, 1, 0. If the right neighbor’s value
is smaller, it is copied. If the left neighbor’s value is greater, it is copied.
(b) The data ﬂow. Cells with right neighbors compute the minimum, cells
with left neighbors compute the maximum. The graph also shows which
cells are accessed during the run, the access pattern (the inverted arrows, the
time-evolution of the pointers).

The following abbreviations are used in the description of the GCA rules:

– the data and the pointer base: d = di, p = pi,
– the global neighbor’s data state: d∗ = d∗

i = D[abs( ˆpi)], where ˆpi is the

eﬀective relative address computed from the relative address base.

The address modiﬁcation rule computing the eﬀective address is

ˆp =

(cid:26) +p if (i and p) = 0
−p if (i and p) = 1

.

The data rule is




d∗

if

d(cid:48) =

(i and p = 0) and (d∗ < d)
or (i and p = 1) and (d < d∗)

.



d

otherwise

The pointer rule is p(cid:48) = p/2 .

The algorithm can also be described in the cellular automata language CDL,
as follows.

4 GCA ALGORITHMS

30

end;

begin

distance = infinity;

dneighbor, d: integer;

{neighbor’s and own data}

{global access to any cell}

d: integer; {initialized by a bitonic sequence to be merged}
i: integer; {own position initialized by 0..(2^k)-1}
{p = pointer base to neighbor, mask initialized by 2^(k-1)}
p: integer; {2^(k-1), 2^(k-2) ... 1}

(1) cellular automaton bitonic_merge;
(2) const dimension = 1;
(3)
(4)
(5) type celltype=record
(6)
(7)
(8)
(9)
(10)
(11)
(12) var peff : celladdress; {eff. relative address of global neighbor}
(13)
(14)
(15) #define cell *[0] {the cell’s own state at rel. address 0}
(16)
(17) rule begin
(18) if ((cell.i and cell.p) = 0 ) then
(19)
(20)
(21)
(22)
(23)
(24)
(25)
end
(26)
(27) else
(28)
(29)
(30)
(31)
(32)
(33)
(34)
(35)
(36)
(37) {access-pattern 2^(k-1),...,4,2,1, where n=2^k}
(38) p := p / 2;
(39) end;

{cell id is greater than bit mask / base pointer}
{use the neighbor to the left with distance given by -base}
peff := [-cell.p];
dneighbor := *peff.i; d := cell.i; {data access}
{if neighbor’s data is greater / not in order}
if (dneighbor > d) then cell.d := dneighbor;

{cell id is smaller than bit mask / base pointer}
{use the neighbor to the right with distance given by base}
peff := [cell.p]; {use base address without change}
dneighbor := *peff.i; d := cell.i;
{if neighbor’s data is smaller / not in order}
if (d > dneighbor) then cell.d := dneighbor;

{address modification}

{data access}

begin

end;

The general algorithm can be transformed into a basic GCA algorithm.
Then the address calculation has to be performed already in the previous
generation t − 1. Initially the pointers of the left half are +n/2, and −n/2 for
the right half of cells. The pointer rule then needs to compute the requested
access pattern for the next time-step using in principle the method used in
the former address modiﬁcation rule.

Then there arises a principle diﬀerence between the general and basic
GCA algorithm for this application. In the general algorithm, the address

4 GCA ALGORITHMS

31

base is the same for every cell (but time-dependent) and could be supplied
by a central unit. In the basic GCA algorithm, the eﬀective address has to
be stored and computed in each cell because it depends on time and index.

4.3.2

2D XOR with Dynamic Neighbors

CA XOR Rule. Firstly, for comparison, we want to describe the classic CA
2D XOR rule computing the mod 2 sum of their four orthogonal neighbors.
Given is a 2D array of cells

D = array [0 .. n − 1, 0 .. n − 1] of binary, where binary = {0, 1} .

The data state of cell (x, y) is D[x, y] = d(x,y). The data state of a neigh-
bor with the relative address p = (px, py) is d(x,y)+(px,py) = d(x+px,y+py). The
nearest NESW neighbors’ relative addresses are

pN orth = (0, −1), pEast = (1, 0), pSouth = (0, 1), pW est = (−1, 0).

The data rule is (written in diﬀerent notations)

d(cid:48)
(x,y) = d(x,y)+pN orth + d(x,y)+pEast + d(x,y)+pSouth + d(x,y)+pW est mod 2

d(cid:48) = pN orth.d + pEast.d + pSouth.d + pW est.d mod 2

d(cid:48) = dN orth + dEast + dSouth + dW est mod 2 .

GCA Rule with dynamic neighbors. Now we want to use dynamic

neighbors which can change their distance to the center cell.

• cell state

q = (d, p)

where d ∈ D = {0, 1} is the data part, and p is the common address
base (a distance, a relative pointer), initially set to 1.

• eﬀective relative addresses to neighbors 6

pN orth = (0, −p), pEast = (p, 0), pSouth = (0, p), pW est = (−p, 0).

6Remark. The pointer p is used four times in a simple symmetric way, meaning that
we use the general GCA model with the common address base p. If we would prefer to
use the basic model, we had to use the cell state q = (d, pN orth, pEast, pSouth, pW est),
and we would need four pointer rules, just simple variations of each other.

4 GCA ALGORITHMS

32

• neighbors’ data states

dN orth = pN orth.d, dEast = pEast.d, dSouth = pSouth.d, dW est = pW est.d

• data rule

d(cid:48) = dN orth + dEast + dSouth + dW est mod 2

• pointer rule 1, emulating the classical CA rule

p(cid:48) = p = 1

• pointer rule 2, p = (1, 2, 3 . . . , n − 1)∗

p(cid:48) =

(cid:40)

(p + 1)mod n
1

if (p + 1)mod n > 0
if (p + 1)mod n = 0

• pointer rule 3, 4, 5, 6: ∆ = 2, 3, 4, 5; p = (1, 1 + ∆, 1 + 2∆, . . .)∗

p(cid:48) =

(cid:40)

(p + ∆)mod n
1

if (p + ∆)mod n > 0
if (p + ∆)mod n = 0

• pointer rule 7, p = 1, 2, 4, . . . 0

p(cid:48) = 2p mod n

• pointer rule 8, p = 1, 3, 9, . . . 0

p(cid:48) = 3p mod n

Depending on the actual pointer rule, the evolution of conﬁgurations
(patterns) diﬀers. For n = 32, as depicted in Fig. 10, the evolution starts
initially with a cross (5 cells with value 1) in the middle. For all pointer
rules, the evolution converges to a blank (all zero) conﬁguration at a time-
step t ≤ 16. Equal or relative similar pattern can be observed for the diﬀerent
pointer rules, for example look at the following patterns, for

(t = 3, p(cid:48) = 1) ≡ (t = 2, p(cid:48) = p + 1)
(t = 7, p(cid:48) = 1) ≡ (t = 3, p(cid:48) = 2p)
(t = 8, p(cid:48) = p + 2) ≡ (t = 8, p(cid:48) = p + 4) ≡ (t = 8, p(cid:48) = 3p)
(t = 15, p(cid:48) = 1) ≡ (t = 15, p(cid:48) = p + 2) ≡ (t = 4, p(cid:48) = 2p) .

We can conclude from these examples that dynamic neighbors (given by
the pointer rules) can produce more complex patterns. By “complex pattern”
we mean here a pattern that is more diﬃcult to understand (needs more
attention for interpretation) because it contains more diﬀerent subpatterns
compared to the simple CA XOR rule. For example, the pattern (t = 5, p(cid:48) =

4 GCA ALGORITHMS

33

p(cid:48) = 1

p + 1

p + 2

p + 3

p + 4 p + 5

2p

3p

Figure 10: The evolution of the XOR rule with dynamic neighbors. (p(cid:48) = 1, rule 1)
The classical XOR rule with local NESW neighbors. (p + 1, rule 2) The pointer to the
neighbors is incremented by one. (p + ∆, rule 3, 4, 5, 6) The pointer is incremented by
∆ = 2, 3, 4, 5. (2p, rule 7)(3p, rule 8) The pointer is multiplied by 2, 3, respectively.

4 GCA ALGORITHMS

34

(a)

(b)

(c)

Figure 11: Some special patterns evolved by XOR rules with four distant
orthogonal neighbors. n = 65. The initial conﬁguration is a cross like in
Fig. 10. The patterns are of size 130 × 130, by doubling the 65 × 65 pattern
in x- and y- direction in order to exhibit better the inherent structures. (a)
Pointer rule p(cid:48) = p = 3, at t = 57. (b) Pointer rule p(cid:48) = 3p mod n [if 3p mod
n < n] +1 [if 3p mod n = 0], at t = 121; and (c) at t=139.

p = 1) contains 1 sub-patterns (a cross), whereas pattern (t = 5, p(cid:48) = p + 1)
contains 4 sub-patterns (plus their rotations).

Three selected patterns are shown in Fig. 11. The data rule is the XOR
rule with four orthogonal neighbors, as before. The size of the pattern is
65 × 65. The initial conﬁguration is a cross like in Fig. 10. The patterns
shown are of size 130 × 130, by doubling the 65 × 65 pattern in x- and y-
direction in order to exhibit better the inherent structures. The used pointer
rules are (a) p(cid:48) = p = 3, and (b, c) p(cid:48) = 3p mod n [if 3p mod n < n], or
p(cid:48) = 1 [if 3p mod n = 0].

4.3.3 Time-Dependent XOR Algorithms

We want to give an example where the pointer rule depends on the time t.
Either a central or a local clock can be used. In the case of a local clock,
the cell’s state needs to be extended. We use the XOR rule of the preceding
section.

• cell state. p is the address base, a relative pointer, initially set to 1.

q = (d, p)

• eﬀective relative addresses to neighbors

pN orth = (0, −p), pEast = (p, 0), pSouth = (0, p), pW est = (−p, 0).

• data rule

d(cid:48) = dN orth + dEast + dSouth + dW est mod 2

4 GCA ALGORITHMS

35

rule A

B

C

D

E

F

G

H

Figure 12: The evolution of the XOR rule with dynamic neighbors, time and space
dependent. (A) The classical XOR rule with local NESW neighbors for comparison. (B,
C, D, E) The pointer alternates in time. (B) p(cid:48) = (1, 2)∗. (C) p(cid:48) = (1, 3)∗. (D) p(cid:48) = (1, 4)∗.
(E) px(cid:48), py(cid:48) = ((1, 3), (3, 1))∗. The distance to neighbors is diﬀerent in x- and y-direction.
(F, G, H) The pointer is space dependent, diﬀerent neighbors deﬁned by pointers are used
where checkerboard is black or white. Either the orthogonal neighbors or the diagonal
neighbors are used. (F, G, H) pointers to neighbors are px = py = 1, 2, 3.

4 GCA ALGORITHMS

36

• pointer rule A, emulating the classical CA rule, for comparison

p(cid:48) = p = 1

• pointer rule B: p = 1 + t mod 2, p = (1, 2, 1, 2, . . .)

pointer rule C: p = 1 + 2(t mod 2), p = (1, 3, 1, 3, . . .)

pointer rule D: p = 1 + 3(t mod 2), p = (1, 4, 1, 4, . . .)

• pointer rule E: (px, py) = ((1, 3), (3, 1))∗ = ((1, 3), (3, 1), (1, 3), (3, 1), . . .)

where
pN orth = (0, −py), pEast = (px, 0), pSouth = (0, py), pW est = (−px, 0).

px = 1 + 2(t mod 2), py = 1 + 2((t + 1) mod 2).

The evolution of these time dependent XOR rules are shown in Fig. 12
(B, C, D, E). Rule E exhibits more irregular patterns because the distance
to the neighbors is diﬀerent in x- and y-direction, and alternating.

4.3.4 Space-Dependent XOR Algorithms

We want to give an example where the pointer rule depends on the space
given by the two-dimensional cell index (x, y). We use the same XOR rule
and deﬁnitions as in the preceding section.

• pointer rules F, G, H

A checkerboard is considered, where white 0-cells are deﬁned by the
condition [(x + y) mod 2 = 0], and black 1-cells by the condition
[(x + y) mod 2 = 1].

The pointer rules for white cells deﬁnes their orthogonal neighbors:
pN orth = (0, −py), pEast = (px, 0), pSouth = (0, py), pW est = (−px, 0).

The pointer rules for black cells deﬁnes their diagonal neighbors:
pN orth = (px, −py), pEast = (px, py), pSouth = (−px, py),
pW est = (−px, −py).

with px = py = p = 1, 2, 3 for rule F, G, H.
Note that for black cells, pN orth addresses NorthEast, pEast addresses
SouthEast, pSouth addresses SouthWest, and pW est addresses NorthWest.

The space-dependent rules F, G, H (Fig. 12) show diﬀerent patterns
and sub-patterns compared to the time dependent rules B – E. These exam-
ples show that diﬀerent and more complex patterns can be generated if the
neighbors are changed in time or space by an appropriate pointer rule.

4 GCA ALGORITHMS

37

4.3.5

1D XOR Rule with Dynamic Neighbors

Two compilable PASCAL program are given in Section 7 (Appendix 0) that
simulate the 1D XOR rule with two dynamic neighbors. The basic model is
used in Sect. 7.1, and the general model with a common address base is used
in Sect. 7.2.

4.4 Plain Model Example

In the plain GCA model, the cell’s state q is not structured into a data and
pointer part. The pointer(s) are computed from the state. In our example,
we use again the XOR rule with remote NESW neighbors, and the cell’s state
is binary. The distance to the neighbors is directly related to the cell’s state,
here it is deﬁned as

p = (1 − q)A + qB =

(cid:26) A if q = 0

B if q = 1, where 1 ≤ A, B ≤ n/2.

.

The eﬀective relative addresses to the distant NESW neighbors are

pN orth = (0, −p), pEast = (p, 0), pSouth = (0, p), pW est = (−p, 0).

Fig. 13 and Fig. 14 show the evolution of this rule with data dependent
pointers. Fig. 13: The pointer value is p = 9 = A if the cell’s state is 0
(white), and is p = 1 = B if the cell’s state is 1 (black). For t = 7 − 24
we observe small 49 sub-patterns placed regularly at 7 × 7 distinct positions.
The sub-patterns are changing and slowly increasing until they merge. The
density of black cells is roughly increasing during the evolution, but the
pattern does not converge into a full black conﬁguration.

Fig. 14: The pointer value is p = 9 = A if the cell’s state is 0 (white), and
is p = 3 = B if the cell’s state is 1 (black). For t ≥ 35 all cells remain white.
Note that the interesting pattern for t = 34 is not a true checkerboard, the
white areas are squares of two diﬀerent sizes, or rectangles.

4 GCA ALGORITHMS

38

t = 0 − 5

6 − 11

12 − 17

18 − 23

24 − 29

30 − 35

Figure 13: Plain GCA Model, XOR rule with data dependent pointers. The
pointer value is p = 9 if the cell’s state is 0 (white), and is p = 1 if it is 1
(black). For t = 7 − 24 we observe small 49 sub-patterns placed regularly at
7 × 7 distinct positions. The sub-patterns are changing and slowly increasing
until they merge.

4 GCA ALGORITHMS

39

t = 0 − 5

6 − 11

12 − 17

18 − 23

24 − 29

30 − 35

Figure 14: Plain GCA Model, XOR rule with data dependent pointers. The
pointer value is p = 9 if the cell’s state is 0 (white), and is p = 3 if the cell’s
state is 1 (black). For t ≥ 35 all cells stay black.

4 GCA ALGORITHMS

40

4.5 A New Application: Synchronous Firing

Our problem is similar to the Firing Squad Synchronization Problem (FSSP)
that is a well studied classical Cellular Automata Problem [61, 62, 63, 64].
Initially at time t = 0 all cells in a line are “quiescent”, the whole system
is quiescent. Then at t = 1, a dedicated cell (the general ) becomes active
by a special external or internal event. The goal is to design a set of states
and a local CA rule such that, no matter how long the line of cells is, there
exists a time tf ire such that every cell changes into the ﬁring state at that
time simultaneously.

Here we are modifying the problem because we aim at GCA modeling,
allowing pointer manipulation and global access. In order to avoid confusion,
we call our problem “Synchronous Firing” (SF). Applying the GCA model,
the problem becomes easier to solve, although not necessarily simple. We
expect a shorter synchronization time.

The cell’s state is q = (d, p), where d is the data state and p the pointer.
We can easily ﬁnd a trivial solution. The cells (i = 0, 1, . . . , n − 1) are
arranged in a ring, all of them are quiescent soldiers (state S) at time t = 0.
All cells contain a pointer pointing to cell i = 0. At t = 1 a general (state
G) is installed at position i = 0. Now all cells read the state of their global
neighbor which is G for all of them. Then, at t = 2 all cells change into the
Firing state (F). Although trivial, this solution is somehow realistic. The
soldiers observe the general, and when he gives a signal, all of them ﬁre at
the next time-step, for instance after one second.

This solution of the problem is not general enough because the sol-
diers must know the position of the general in advance. We aim at more
general/non-trivial solutions.

4.5.1 Synchronous Firing Using a Wave

As before, all cells are arranged in a ring and initially they are in the quiescent
state S (Soldier). Then, by an external force, any one of the soldiers changes
its state into G (General). Now we want to ﬁnd a solution where all cells
ﬁre simultaneously, independently of the general’s position. Furthermore we
want to allow only one pointer per cell (one-armed GCA) and the initial
values of the relative pointers to be the same.

In the solution we use the data states S, G, and F. Initially all pointers
are set to the value -1, meaning that every cell points to its left neighbor in
the ring.

The GCA algorithm consists of a pointer rule and a data rule. The

following abbreviations are used:

4 GCA ALGORITHMS

41

Figure 15: Synchronous Firing using pointers. At t < 0 the system is quies-
cent. Then, at t = 0, one of the soldiers becomes a general and will produce
a self-loop at t = 1. From t = 1 to t = 5 a wave propagates clockwise.
When it reaches the general, the cells know that they have to ﬁre at the next
time-step. Solid arrows depict pointers, dotted arrows depict pointers that
were modiﬁed.

GCA-ALGORITHM 1
Synchronous Firing using a Wave

pi = −1

di = S

t < 0

t = 0

t > 0

x.1

x.2

A.1

y.1

y.2

pi ← g(pi, di, p∗

i , d∗
i )

t = n + 1

pi = k − i

t = n + 2

pi = −1

dk = G
di ← f (pi, di, d∗
i )

di = F

di = S

initial

∃k ∈ I

∀i

∀i

∀i

Figure 16: At t < 0 the system is quiescent. Then at t = 0 a general is
introduced. From t = 1 to t = n + 1 a wave propagates clockwise. When it
reaches the general each cell knows that it has to ﬁre at the next time-step
t = n + 1.

p = pi, d = di, p∗ = p∗
The pointer rule:7

i = Prel[abs(pi)], d∗ = d∗

i = D[abs(pi)].

p(cid:48) = g =

(cid:26) p ⊕ 1 if (d = S, G) and ((d∗ = G) or (p∗ (cid:54)= −1))

−1

if (d = F )

(1a)
(1b)

.

7 a ⊕ b = a + b mod n

4 GCA ALGORITHMS

42

The data rule:
(cid:26) F
S

d(cid:48) = f =

if (d∗ = G) and ((p (cid:54)= −1) or (p∗ = 0))
if (d = F )

(2a)
(2b)

.

The algorithm works as follows, as shown for n = 4 in Fig. 15:

• t < 0: Initially the conﬁguration is quiescent.

∀i ∈ I : pi = −1, di = S.

• t = 0: A general is assigned.

∃!i ∈ I : di = G

• t = 1: A wave is starting. The ﬁrst soldier in the ring whose left
neighbor is the general forms a self-loop which marks (the head of) the
wave.
(p = p + 1 if (d = S) and (d∗ = G), Rule 1a)

• t = 2, . . . , n + 1: The wave moves clockwise. Cells that recognize
the wave follow it. The cell’s pointer is incremented if the neighbor’s
pointer p∗ does not point to the left anymore.
(p = p + 1 if (p∗ (cid:54)= −1), Rule 1a)

• t = n: The wave has reached the general and all cells point to it
(d∗ = G). This situation signals that all cells shall ﬁre. (Rule 2a).
Then the General and the Soldiers (except one) ﬁre if their pointers
are not equal to -1 (the initial condition). The Soldier to the right of
the General is prevented to ﬁre by the condition p (cid:54)= −1 because the
condition p = −1 is true at the beginning and in the pre-ﬁring state.
Therefore the excluded Soldier needs to be included by an additional
condition p∗ = 0 that detects the self-loop of the General.

• tf ire = n + 1: All cells are in the ﬁring state. The whole system can
be reset into the quiescent state (Rule 2b), or another algorithm could
be started, for instance repeating the same algorithm with a general at
another position.

We can describe this algorithm in a special tabular notation as shown in
Fig. 16. The ﬁrst column shows a numbering scheme. Preconditions and
inputs before starting the algorithm are marked by “x.i ”. The algorithmic
actions are marked by “A.i ”. Predicates and outputs are marked by “y.i ”,
they are no actions. They show intermediate or ﬁnal results of algorithmic
actions and serve also for a better understanding of the algorithm. They

4 GCA ALGORITHMS

43

are not necessary to describe the algorithm, they are optional and may also
be true at another time. In the second column a temporal precondition is
given. We assume that the time proceeds stepwise but we do not give an
implementation for that. There may be a time counter in every cell, or there
may be a central time-counter that can be accessed by any cell. The third
column speciﬁes the change of the pointer according to the pointer rule g.
The fourth column speciﬁes the change of the data according to the data rule
f . The ﬁfth column is reserved for comments or additional assertions.

The classical CA solution of Mazoyer [62] with local neighborhood needs
tf ire = 2n − 1. So the GCA solution is only nearly twice as fast. The purpose
was not ﬁnd the fastest GCA algorithm but to show how a GCA algorithm
can be described and works in principle.

4.5.2 Synchronous Firing with Spaces

Our next solution is based on the former algorithm using a wave as described
in Sect. 4.5.1. Now the number of cells shall be larger than the number of
active cells (General, Soldiers), empty (inactive) cells (spaces) can be placed
at arbitrary positions between them. So an active ring of cells is embedded
into a larger ring of cells. Our algorithm will have the following features:

• Any number of inactive cells can be placed between active cells.

• The ordering scheme used for connecting the active cells by pointers

needs not to follow the indexing scheme.

• Several rings of active cells can be embedded in the space and processed

in parallel.

The algorithm uses two pointers per cell, p1 and p2. Initially active cells
are connected in one or more rings (circular double linked lists). Pointer p2
remains constant, thereby a loop exist always in one direction. Pointer p1 is
variable and is used to mark the wave. Inactive (constant) cells are marked
by self-loops, their pointers are set to zero (p1 = 0 and p2 = 0). (Another
way to code inactive cells were to use an extra data state.)

We associate the index range with a horizontal line of cells, where cell
index 0 corresponds to the leftmost position and index n − 1 to the rightmost
position. In our later example and for explanation we connect initially a cell
to its left neighbor by p1 and to its right neighbor by p2. (The connection
scheme can be arbitrarily as long as the cells are connected in a ring.)
The pointer rule for p2 is p2(cid:48) = p2 (no change after initialization).

4 GCA ALGORITHMS

44

The pointer rule for p1 is

p1(cid:48) = g =






p1

0
p1 ⊕ p1.p2

if not Active
otherwise
if (p1.d = G) and (p1 (cid:54)= 0) and (p1.p1 (cid:54)= 0)
if ((p1 = 0) or (p1.p1 = 0))

(3a)

(3b)
(3c)

The data rule is



d

d(cid:48) = f =



F

if not Active
otherwise
if (p1.d = G) and ((p1 (cid:54)= −p1.p2) or (p1.p1 = 0))

(4a)

(4b)

The algorithm works as follows.

• t < 0: Initialization. All data states are set to di = S. Inactive cells
are represented by (p1 = 0 and p2 = 0). Rings consisting of active cells
to be synchronized are formed. A cell may belong to one ring only,
i.e. rings are mutually exclusive. Neighboring cells cj, ci, and ck of a
ring are connected by pointers. Cell ci points to the “left” cell cj by
p1 and to the “right” cell ck by p2. The conditions ci.p1 = −cj.p2 and
ci.p2 = −ck.p1 are true.

• t = 0: A General is assigned in each ring by setting di(k) = G, where

i(k) is the index of the General in the ring k.

• t = 1: A wave is starting in each ring. The soldier in each ring whose
p1 neighbor is the General forms a self-loop (p1 = 0) which marks the
wave (Rule 3b).

• t > 1: (Rule 3c). The wave move along in the direction of p2. The
pointer p1 is set to p2 (the next position of the wave) when the cell itself
is the head of the wave (self-loop p1 = 0) because then p1 ⊕ p1.p2 = p2.
The pointer p1 follows the wave through p1 ⊕p1.p2 when the p1 neighbor
is the head of the wave (self-loop p1.p1 = 0).

• t(k) = L(k): The wave has reached the General of a ring k, where t =
L(k) is the length of the ring k. This situation signals that all cells shall
ﬁre (Rule 4a). All cells of the ring k point to the General (p1.d = G),
this is the precondition to ﬁre. The Soldiers (except one) ﬁre only if
their pointers are not equal to the initial condition p1 (cid:54)= −p1.p2, which
is an indirect self-loop of length 2. But a self-loop of length 2 is true
for the Soldier S next to the General G via p1 at the beginning and in

4 GCA ALGORITHMS

45

the pre ﬁring state. (G → p2 → S / G ← p1 ← S). So by adding the
condition p1.p1 = 0 (S points via p1 to G showing a self-loop), S will
also ﬁre. The General is allowed to ﬁre when the self-loop of length
2 (p1 = −p1.p2) has changed into a self-loop (p1 = 0), and then the
condition p1 (cid:54)= −p1.p2 holds.

• tf ire(k) = L(k) + 1: All cells of ring k are in the ﬁring state.

Example. The number of cells is n = 9, index i ∈ {0, 1, . . . , 8}. Two
rings with bidirectional links to their neighbors are embedded in the array.
Ring A is the connection of the cells (2, 4, 6). Ring B is the connection of
the cells (1, 3, 5, 7). Cells 0 and 9 are passive cells that can be seen as the
borders of the array. The p1 pointers (relative values) of A are (−5, −2, −2).
The value -5 is the (cyclic) distance from cell 2 to 6. The p2 pointers of A
are (2, 2, 5). The value 5 is the (cyclic) distance from cell 6 to 2. – The p1
pointers of B are (−3, −2, −2, −2). The p2 pointers of B are (2, 2, 2, 3).

4.5.3 Synchronous Firing with Pointer Jumping

Solution 1. The question is whether the synchronization time can be re-
duced by using the pointer jumping (or pointer doubling) technique. This
technique is well-known from PRAM (parallel random access machine) algo-
rithms. It means for the GCA model that an indirect neighbor, a neighbor of
a neighbor, becomes a direct neighbor. This can be accomplished by pointer
substitution (p ← p∗) in the case of absolute pointers, or pointer addition
(p ← p+p∗) in the case of relative pointers (or pointer vectors where the cells
are identiﬁed by their coordinates in the n-dimensional space), or simply by
pointer doubling (p ← 2p) in the case of relative pointers when the cells are
ordered by a consecutive 1D array index. For instance, this technique allows
us to ﬁnd the maximum of data items stored in a line of cells in logarithmic
time.

A ﬁrst algorithm is given in Fig. 18. Initially at t = 0 we assume that
there is one general among all remaining soldiers. Then the following rules
are applied. Pointer Rule:

p(cid:48) = g(p, p∗, n) = p ⊕ p∗ = (p + p∗) mod n

(5)

Alternatively the rule g(p, n) = 2p mod n could be used because p = p∗

holds here. Data Rule:

d(cid:48) = f (p, d, d∗) =






d∗
2
d

if (p (cid:54)= 0) and (d < d∗)
if (p = 0) and (d = 1)
otherwise

(6a)
(6b)
(6c)

.

4 GCA ALGORITHMS

46

Figure 17: Synchronous Firing of two rings embedded in an 1D array. The
cyclic connected cells (2, 4, 6) form ring A, and the cells (1, 3, 5, 7) form ring
B. At t = 0 we can observe the connections by the pointers and one General
for ring B at i = 1, and another at i = 6 for ring A. Then two waves are
starting, one in ring A and one in ring B. Ring A ﬁres at t = 4, 7, 10, 13, . . .
(ﬁring states are represented by black squares), and ring B ﬁres at t =
5, 9, 13, 17, . . .. All cells except the border cells ﬁre at t = 13, 25, . . ..

4 GCA ALGORITHMS

47

The algorithm in tabular form is shown in Fig. 18. The time evolution

of the pointers and the data are shown in the following for n = 8:

GCA-ALGORITHM 2
Synchronous Firing with Pointer Jumping

x.1

x.2

A.1

y.1

y.2

t < 0

t = 0

t > 0

pi = 1

di = S = 0

pi = 1
pi ← g(pi, di, p∗

i , d∗
i )

dk = G = 1
di ← f (pi, di, d∗
i )

t = log2 n

t ≥ 1 + log2 n

pi = 0

pi = 0

di = G = 1

di = F = 2

∀i ∈ I

∃!k ∈ I

∀i

∀i

∀i

Figure 18: The system starts working at t = 0 when one of the soldiers is
assigned to be a general. The information G = 1 is exponentially distributed
among the neighbors by pointer jumping. At t = log2 n all the pointers
become 0, and the data is G everywhere which is the signal to ﬁre. At
t = log2 n + 1 alls cells change into the ﬁring state d = 2.

Pointer

Data
0 1 2 3 4 5 6 7

i= 0 1 2 3 4 5 6 7
t
0
1
2
3
4

>1 1 1 1 1 1 1 1 >1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 1
1 0 0 0 0 1 1 1
>0 0 0 0 0 0 0 0 >1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 >2 2 2 2 2 2 2 2

2 2 2 2 2 2 2 2
4 4 4 4 4 4 4 4

The algorithm works as follows, according to Fig. 18:

• t < 0: Each cell points to its right neighbor in the ring. Every cell is

in state S.

• t = 0: A general is assigned at any position.

• t > 0: The pointer and data rule are applied. The pointer value is
doubled at each step until 0 = 2n mod n is reached (1, 2, ...2n−1, 0).
The data value 1 propagates exponentially to all cells until the system
will be ready to ﬁre.

• t = log2 n: This situation (∀i : (pi = 0) and (di = 1)) signals that all

cells are ready to ﬁre.

4 GCA ALGORITHMS

48

• tf ire = 1 + log2 n: All cells change into the ﬁring state.

There are two shortcomings of this solution. (1) The number n must be
a power of 2. (2) When the General is assigned, the pointers must have the
value +1. So it is not possible to introduce the general at a later time when
the pointers were already changed by the rule. Therefore we look for a more
general solution without these restrictions.

Solution 2. The following solution works for any n, and the General can

be introduced at any time at any position. Pointer Rule:

p(cid:48) = g(p, n) =






1
0
2p mod n otherwise

if p = 0
if p < 0

(7a)
(7b)
(7c)

.

This rule ensures that the pointers run in a cycle with values that are
powers of 2. The cyclic sequence is (1, 2, 4, . . . , N/2, 0) where N is the next
power of 2 boundary for n : 2k−1 < n ≤ N = 2k. Rule (7a) implicates that
the sequence is repeated when 0 is reached. Rule (7c) doubles the pointer
by default. Rule (7b) is used if n is not a power of two. Then, in the last
step of the cycle, zero cannot be the result of pointer doubling. The result
of doubling modulo n would be less then p which is the criterion to force the
pointer to take on the value 0, and so to mark the end of the cycle.

Data Rule:

d(cid:48) = f (p, d, d∗) =





d∗
2
3
d

if (p (cid:54)= 0) and (d < d∗)
if (p = 0) and (d = 1)
if (p = 0) and (d = 2)
otherwise

(8a)
(8b)
(8c)
(8d)

.

The data states are: 0 = S (Soldier), 1 = G (General), 2 = A (Attention),
3 = F (Fire). Rule (8a) is used to propagate exponentially the states 1 and
2. Rule (8b) changes the state into 2 when the last value (0) of the cyclic
pointer sequence is detected. Firing Rule (8c) is applied when all states are
2 at the end of the cycle. Otherwise the state remains unchanged (8d).

Note that the pointers are running in a cycle, the system waits (busy
waiting) for the General to be introduced. This system state can be inter-
preted as a “quiescent state” that is in fact an orbit. After the General was
introduced the algorithm starts working until the system ﬁres.

Compared to the algorithm before, we need now around two cycles instead

of one but the algorithm is much more general.

The maximal ﬁring time is tmax

when the pointers are in the state 00...0. The minimal ﬁring time is tmin

f ire = 2 + 2log2 n if the general is introduced
f ire =

5 GCA HARDWARE ARCHITECTURES

49

2 + log2 n if the general is introduced when the pointers are in the state
11...1.

The time evolution of the pointers and the data are shown in the following

for n = 9:

Data
0 1 2 3 4 5 6 7 8

(b) Pointer

0 1 2 3 4 5 6 7 8

Data
0 1 2 3 4 5 6 7 8

0 1 2 3 4 5 6 7 8

(a) Pointer
i=
t
-1
0
1
2
3
4
5
6
7
-1
9
10

0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1 1 >0 0 0 0 1 0 0 0 0
0 0 0 1 1 0 0 0 0
2 2 2 2 2 2 2 2 2
0 1 1 1 1 0 0 0 0
4 4 4 4 4 4 4 4 4
1 1 1 1 1 0 1 1 1
-1-1-1-1-1-1-1-1-1
>1 1 1 1 1 1 1 1 1
>0 0 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1 1 >2 2 2 2 2 2 2 2 2
2 2 2 2 2 2 2 2 2
2 2 2 2 2 2 2 2 2
2 2 2 2 2 2 2 2 2
4 4 4 4 4 4 4 4 4
2 2 2 2 2 2 2 2 2
-1-1-1-1-1-1-1-1-1
>2 2 2 2 2 2 2 2 2
>0 0 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1 1 >3 3 3 3 3 3 3 3 3

0 0 0 0 0 0 0 0 0
-1-1-1-1-1-1-1-1-1
>0 0 0 0 0 0 0 0 0 >0 0 0 0 1 0 0 0 0
>0 0 0 0 2 0 0 0 0
1 1 1 1 1 1 1 1 1
0 0 0 2 2 0 0 0 0
2 2 2 2 2 2 2 2 2
0 2 2 2 2 0 0 0 0
4 4 4 4 4 4 4 4 4
2 2 2 2 2 0 2 2 2
-1-1-1-1-1-1-1-1-1
>0 0 0 0 0 0 0 0 0 >2 2 2 2 2 2 2 2 2
>3 3 3 3 3 3 3 3 3

1 1 1 1 1 1 1 1 1

On the left (a) a case with tmax

sync is shown, and on the right (b) a case with
tmin
sync. All pointers are equal and they are running permanently in the cycle:
(1, 2, 4, −1, 0)∗.

5 GCA Hardware Architectures

We have to be aware that an architecture ARCH may consists of three parts
ARCH = (FIX, CONF, PROGR) where CONF and PROG are optional.
FIX is the ﬁxed hardware by construction/production, CONF is the conﬁg-
urable part (typically the logic and wiring as in a FPGA (ﬁeld programmable
logical array)), and PROG means programmable, usually by a loadable pro-
gram into a memory before runtime.

There are four possible general types of architectures

Description

Architecture Parts
Type
1
2
3
4

FIX
FIX, CONF
FIX, PROG
FIX, CONF, PROG conﬁg. & progr. processor

special processor
conﬁgurable processor
programmable processor

After conﬁguration and programming the architecture turns into a spe-
In general a “processor” can
cial (conﬁgured & programmed) processor.
be complex and built by interconnected sub processors, like a multicore or
multiprocessor system with a network.

5 GCA HARDWARE ARCHITECTURES

50

Figure 19: CEPRA-S supporting CA and GCA models. 8 data memories,
program memory, temporary memory, computational unit (FPGA1), inter-
pretation and address generator (FPGA2), PCI-Interface.

A variety of architectures can be used or designed to support the GCA
model. In our research group (Fachgebiet Rechnerarchitektur, FB20 Infor-
matik, Technische Universit¨at Darmstadt) we developed special hardware
support using FPGAs, ﬁrstly for the CA model (CEPRA (Cellular Process-
ing Architecture) series, CEPRA-3D 1997, CEPRA-1D 1996, CEPRA-1X
1996, CEPRA-8D 1995, CEPRA-8L 1994, CEPRA-S 2001), and then for the
GCA model (2002–2016) [8]–[31]. The CEPRA-S (Fig. 19) was designed not
only for CA but also for GCA.

There are mainly three fundamental GCA architectures:

• Fully Parallel Architecture. A speciﬁc GCA algorithm is directly
mapped into the hardware using registers, operators and hardwired
links which may also be switched if necessary. The advantage of such
an implementation is a very high performance [12, 17, 18] (Sect. 5.1),

5 GCA HARDWARE ARCHITECTURES

51

Figure 20: Multiprocessor Architecture with cell processors that may oﬀer
GCA support (address modiﬁcation, accessing global neighbors, optimized
network, special GCA instructions).

but the problem size is limited by the hardware resources, and the
ﬂexibility to apply diﬀerent rules is low.

• Data Parallel Architecture with Memory Banks and Pipelin-
ing (DPA). This partial parallel architecture [9, 10, 11, 12, 16, 19,
20, 21] oﬀers a high performance, is scalable and it can process a large
number of cells. The ﬂexibility to cope with diﬀerent and complex
applications is restricted.

• Multiprocessor Architecture. This architecture (Fig. 20) is not
as powerful as the above mentioned, but it has the advantage that
it can be tailored to any GCA problem by programming. It also al-
lows integrating standard or other computational models. Standard
processors can be used, or special ones supporting GCA features, see
[12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31].

Standard multiprocessor platforms, like standard multicores or GPUs,
can also execute eﬃciently the GCA model. In [30] a speedup of 13 for
bitonic merging was reached on an NVIDIA GFX 470 compared to an
Intel Q9550@3GHz with 4 threads, and 150 for a diﬀusion algorithm.

5 GCA HARDWARE ARCHITECTURES

52

Figure 21: Fully parallel implementation. Communication implemented by
a multiplexer in each cell (a). Communication implemented by a common
network (b).

5.1 Fully Parallel Architecture

An important attribute is the degree of parallel processing (the number of
processing/computation units) p 8. In other words, p gives the number of
results that can be computed and stored in parallel. A sequential architecture
is given by p = 1, a fully parallel by p = n, and a partial parallel by n > p > 1.
Fully parallel architecture means that the whole GCA with p = n is com-
pletely implemented in hardware (Fig. 21) for a speciﬁc application. The
question is how many hardware resources are needed. The number of cells is
n. Therefore the logic (computing the eﬀective address and the next state)
and the number of registers holding the cells’ states are proportional to n.
The local interconnections are proportional to n, too. As the GCA gener-
ally allows read-access from each cell to any other cell, the communication
network needs n × (n − 1) global links, where a link consists of V (n, m) bit-
wires/channels. V (n, m) is the word length in bits of the cell’s state. The
length of a global link is not a constant, it depends on the physical distance.
In a ring layout, the average link length of n/4 × (space unit) has to be taken
into account. See considerations about implementation complexity for the
basic model in Sect. 2.2.1 on page 12. Note that the longest distance also
determines the maximal clock rate.

Many applications / GCA algorithms do not require a total intercon-
nection fabric because only a subset of all communications (read accesses)
are required for a speciﬁc application. Therefore the amount of wires and

8In this Sect. 5 about hardware architectures, p stays for the degree of parallelism and

not for pointer.

peffdd*MUXfromallothercellsread-onlycommunicationnetwork(to beoptimizedfortheapplication)peffdd*MUXfromallothercellsread-onlycommunicationnetwork(to beoptimizedfortheapplication)5 GCA HARDWARE ARCHITECTURES

53

switches can be reduced signiﬁcantly for one or a limited set of applications.
In addition, for each global link a switch is required. The switches can be
implemented by a multiplexer in each cell, or by a common switching net-
work (e.g. crossbar). Note that the number of switches of the network can
also be reduced to the number of communication links used by the speciﬁc
application. Another aspect is the multiple read (concurrent read) feature.
In the worst case, one cell is accessed from all the other cells which may cause
a fan-out problem in the hardware implementation.

5.2 Sequential with Parallel Memory Access

Figure 22: Multiport memory. When the computation of a new generation
of cell states is completed, the read and write access are switched.

The goal in this and the next section is to design architectures with
normal memories that work eﬃciently. We assume that the GCA can access
two global cells, k = 2. 9 The cell state structure is (D, L1, L2) where D is
the data part and L1, L2 are the pointers. The array Cell stores the whole set
of cell states, and the array CellNew is needed for buﬀering in synchronous
mode.

The computation of a new cell state at position z needs the following four

steps:

1. (Fetch) The cell’s state a = Cell[z] is fetched.

2. (Get) The remote cell states b = Cell[L1], and c = Cell[L2] are fetched.

3. (Execute) The function y = f (a, b, c) is computed.

4. (Write) The result (new state) is buﬀered CellNew[z] := y.

9In this and the next section the number of pointers/links is denoted by “k ” and not

by m as before.

5 GCA HARDWARE ARCHITECTURES

54

Our ﬁrst design assumes a virtual (or real) multiport memory (Fig. 22)
that can perform all necessary memory accesses in parallel. The internal
read memory R is used to read the actual cell states Cell[i], and the internal
write memory W is used to buﬀer the new cell states CellNew[i]. The read
memory R is a read multiport memory allowing k + 1 parallel read accesses.
The read ports are R1, R2, R3. W is a write memory with one port W 1.

When the computation of a new cell generation CellNew(t) is complete,
it has to function as Cell(t + 1) for the next time-step t + 1. One could alter-
nate/interchange the internal read memory with the internal write memory
(switch, using internal multiplexer hardware). One could also use diﬀerent
In principle one could
pages and change read/write access for the ports.
also copy the arrays Cell ← CellNew to realize the required synchronous
updating.

Figure 23: Sequential architecture with pipelining. The multiport memory
(parallel access) is emulated by the use of 2(k + 1) normal memories. When
a new generation is completed, the read and write memories are switched.

The multiport memory can be implemented using normal memories (Fig.
23). Three read memories R1, R2, R3 and three write memories are used (in
general 2(k+1) memories). Each new state is simultaneously written into the
write memories W 1, W 2, W 3. After switching the read and write memories,
the new states are available in parallel from the read memories for the next
generation.

5 GCA HARDWARE ARCHITECTURES

55

Figure 24: Control algorithm controlling the execution of the pipeline shown
in Fig. 23. (a) Detailed with all register transfer operations. (b) Abstract
representation, where z(cid:48) = z − 3.

Control Algorithm (Fig. 24). The control algorithm for this pipelined

architecture was developed by transformation of a purely sequential one.

• State 1

Fetch1: The cell’s state at position z is fetched and stored in a1.
The counter z is incremented (synchronously).

• State 2
Get1:
Fetch2: The next cell’s state is fetched.

The global states a1.L1 and a1.L2 are fetched and a1 is shifted to a.

• State 3
Exe1:
Get2:
Fetch3: The next cell is fetched.

The data values a, b, c are available and the computation is performed.
For the next already fetched cell, the global cell states are accessed.

• State 4 : Four actions are performed in parallel when the pipeline is fully

working.

Write: The result of cell z − 3 is written.
Exe:
Get:
Fetch: Cell z is fetched.

The result of cell z − 2 is computed.
The global cells’ states, addressed by z − 1, are read.

Computation Time. If the number of cells is large enough, the latency
(time to ﬁll the pipeline in states 0–3) can be disregarded. Then a new
result can be computed within one clock cycle, independently of the num-
ber k of global cells: t(n, k) = nT , where T is the duration of one clock cycle.

5 GCA HARDWARE ARCHITECTURES

56

Implementation Complexity. The number of registers, functions (arith-
metic and logic), and the local wiring according to the layout shown in Fig.
23 is relatively low and constant compared to the required memory capacity
(for a large number of cells). The capacity M1 (in bits) of one memory is

M1(n, k) = n(bit(D) + k · bit(L)) = n(bit(D) + k · log2 n) .

The whole memory capacity for 2(k + 1) memories is

M (n, k) = 2(k + 1)M1 = 2n(k + 1) · (bit(D) + k · log2 n) .
The memory capacity is in O(k2 · n · log n), therefore the number of
pointers needs to be small, usually k = 1 or k = 2 is suﬃcient for most
applications.

5.3 Partial Parallel Architectures

5.3.1 Data Parallel Architecture with Pipelining

Figure 25: Multiport memory that provides p write and p read ports to
banks (address pages), and p · k read ports Sj with the whole address range
for accessing the neighbors. Case k = 1.

We want to design a data parallel architecture (DPA) with pipelining for
the parallel degree p, and with one pointer k = 1. We call the such an archi-
tecture “data parallel ”, because p data elements (cell states) are computed
in parallel. A special multiport memory (real or virtual) is needed (Fig.
25). It contains two sub memories that can be switched to allow alternating
read/write access in order to emulate the synchronous updating scheme. The
sub memories are structured into p banks/pages. Each bank stores n/p cells.
The banks can be accessed via p write ports W0, W1, . . . Wp−1 and p read

5 GCA HARDWARE ARCHITECTURES

57

Figure 26: Data parallel Architecture (DPA) with pipelining for p = 4. Stage
1: p cells’ states are read form the banks of the primary memory. Stage 2:
the global neighbors are accessed. Stage 3: p new cell states are computed.
Stage 4: The new states are written into all associated buﬀer banks (not
shown).

5 GCA HARDWARE ARCHITECTURES

58

ports R0, R1, . . . Rp−1. In addition, the read memory supplies pk = p access
ports S1, S2, . . . Sp−1 with the whole address range, dedicated to access the
global neighboring cells. The working principle for a new generation of cell
states is:

1. for z := 0 to n/p − 1 do

(a) Read p cell states from the p banks in parallel from location z.
(b) Access p · k neighbors via the whole range ports Si=1..p.

(c) Compute p results.

(d) Write the results to the p banks of the write memory.

2. Interchange the read and write memory (switch) before starting a new

generation.

The write operations are without conﬂict, because each of the p cells are
assigned exclusively to a separate bank (like in the owner’s write PRAM
model). The memory capacity needed is just the space for the cells (doubled
for buﬀering) and does not depend on p:

Mmultiport(n, k) = 2n(bit(D) + k · bit(Link)) = 2n(bit(D) + k · log2 n) ,
however we have to be aware that the hardware realization of such a multiport
memory is complex because it would need a special design with a lot of ports
and wiring. Therefore we want to emulate it by using standard memories
(Fig. 26). For explanation we assume the case p = 4 and k = 1. We will use
several bank memories of size n/p.

1. In pipeline stage 1, p cells are fetched from the p banks R0,1,2,3 of the

primary memory R at position z deﬁned by a counter.

2. In stage 2, pk (i.e. 4) global cells are accessed form the secondary
memories Si=0,1,2,3 with the whole address range. Each Si memory is
composed of p banks Si

j=0,1,2,3.

3. In stage 3, p = 4 results (new cell states) are computed.

4. In stage 4, the results yj are transferred to each associated buﬀer bank

(denoted by ∗) at position z − 3
R∗

j [z − 3] ← yj

j [z − 3], S∗i

.

5 GCA HARDWARE ARCHITECTURES

59

After completion of one generation, the buﬀer memory banks and the

used banks are interchanged:
and S i=0,1,2,3
j

↔ S∗ i=0,1,2,3

Rj ↔ R∗
j
After the start-up phase, p new cell states are computed and stored for
every time step. The number of bank memories needed is (kp + 1)p, each
holding n/p cell bits. The whole capacity needed is (kp+1)p·n/p = n(kp+1)
cell bits, to be doubled because of buﬀering.

for all banks j.

j

M = 2n(kp + 1) · (bits(D) + k log2n) .

5.3.2 Generation of a Data Parallel Architecture

The data parallel architecture (DPA) (Sect. 5.3.1) uses p pipelines in order
to process p cell rules in parallel. It was implemented on FPGAs in diﬀerent
variants and for diﬀerent applications up to p = 8 ([12, 17, 18, 19, 20, 21, 31]).
In [19, 20, 21] the whole address space is partitioned into (sub) arrays,
also called “cell objects”.
In our implementation, a cell object represents
either a cell vector or a cell matrix. A cell object is identiﬁed by its start
address, and the cells within it are addressed relatively to the start address.
The destination object D stores the cells to be updated, and the source object
S stores the global cells to be read. Although for most applications D and S
are disjunct, the may overlap or be the same.

The DPA consists of a control unit and p pipelines, only one pipeline is
shown in Fig. 27. In the case of one pipeline only, the cells of S are processed
sequentially using a counter k. In the ﬁrst pipeline stage the cell D[k] is read
from memory R. In the second stage the eﬀective address ea is computed
by h. In the third stage the global cell S[ea] is read. In the fourth stage the
next cell state d is computed. Then the next cell state is stored in the buﬀer
memories R(cid:48) and S(cid:48) at location k. When all cells of the destination object
are processed, the memories (R, S) and (R(cid:48), S(cid:48)) are interchanged.

An application speciﬁc DPA with p pipelines can automatically be gen-
erated out of a high level description in the experimental language GCA-L
[19]. The program (Fig. 28a) describes the Jacobi iteration [20] solving a set
of linear equations.

The most important feature of GCA-L is the foreach D with neighbor
= &S[..] do .. endforeach construct.
It describes the (parallel) iteration
over all cells D[i, j] using the global neighbors &S[h(i,j)]. Our tool generates
Verilog code for the functions h, e, g to be embedded in the pipeline(s). These
functions are also pipelined. In addition control code for the control unit is
generated. The most important control codes are the rule instructions. A

5 GCA HARDWARE ARCHITECTURES

60

Figure 27: Data parallel architecture (DPA) with one pipeline.

Figure 28: (a) GCA-L program for the Jacobi iteration.
(b) Next data
operator e automatically generated out of the progam. It contains 4 ﬂoating
point units and several integer units.

ProgramCounterInstr.MemoryIMObjectMemoryOMstart, rows, columnsof SOURCERSRhe/gki,jki,jki,jki,jkddeaead*SdController+1dreadcellcomputeeffectiveaddressreadglobalneighborcellcomputenextdatanextpointerbuffernextcellcontentspeffCellMemoryGlobal CellMemoryinstructionreadobjectinformation(descriptor)ResultMemoriesphysicaladdressof celllogicalindicesin matrixAdaptedoperatorsof theRULE instructionconfiguredintoh, e, geff. addr. operationnextdata, nextpointeroperationApplicationspecificcontrolcodestart, rows, columnsof DESTIN.CONTROLPIPELINEProgramCounterInstr.MemoryIMObjectMemoryOMstart, rows, columnsof SOURCERSRhe/gki,jki,jki,jki,jkddeaead*SdController+1dreadcellcomputeeffectiveaddressreadglobalneighborcellcomputenextdatanextpointerbuffernextcellcontentspeffCellMemoryGlobal CellMemoryinstructionreadobjectinformation(descriptor)ResultMemoriesphysicaladdressof celllogicalindicesin matrixAdaptedoperatorsof theRULE instructionconfiguredintoh, e, geff. addr. operationnextdata, nextpointeroperationApplicationspecificcontrolcodestart, rows, columnsof DESTIN.CONTROLPIPELINEdneighbor.d*+-/dnextdataoperation=ij<<1subgen+i<columnsddprogramparameterlogN= 3;cellstructure= d; celltypefloatcell= float; neighborhood= neighbor;floatcellX[5]; X.d= 1,2,3,4,5;floatcellA[5][5]; A.d= 15,2,3,4,5, 2,19,4,5,6, 5,4,15,2,1, 1,3,5,18,4, 4,2,3,1,12;floatcellAtemp[5][5]; floatcellB[5]; B.d= 77,132,-60,53,412;centralsubgen;forgen=0 to1000000 doforeachAtempwithneighbor= &A[i,j] dod <= neighbor.d; endforeach;foreachAtempwithneighbor= &X[0,j] doif(i!=j) thend <= d *neighbor.delsed <= d endifendforeach;forsubgen= 0 tologNdoforeachAtempwithneighbor= &Atemp[i+(1<<subgen)%columns,j] doif(i+(1<<subgen)<columns) thend <= d+neighbor.delsed <= d endifendforeachendfor;foreachX withneighbor= &B[i] dod <= neighbor.d; endforeach;foreachX withneighbor=&Atemp[i,0] dod <= d-neighbor.d; endforeach;foreachX withneighbor= &A[i,i] dod <= d / neighbor.d; endforeachendforendprogram(a)(b)dneighbor.d*+-/dnextdataoperation=ij<<1subgen+i<columnsdddneighbor.d*+-/dnextdataoperation=ij<<1subgen+i<columnsddprogramparameterlogN= 3;cellstructure= d; celltypefloatcell= float; neighborhood= neighbor;floatcellX[5]; X.d= 1,2,3,4,5;floatcellA[5][5]; A.d= 15,2,3,4,5, 2,19,4,5,6, 5,4,15,2,1, 1,3,5,18,4, 4,2,3,1,12;floatcellAtemp[5][5]; floatcellB[5]; B.d= 77,132,-60,53,412;centralsubgen;forgen=0 to1000000 doforeachAtempwithneighbor= &A[i,j] dod <= neighbor.d; endforeach;foreachAtempwithneighbor= &X[0,j] doif(i!=j) thend <= d *neighbor.delsed <= d endifendforeach;forsubgen= 0 tologNdoforeachAtempwithneighbor= &Atemp[i+(1<<subgen)%columns,j] doif(i+(1<<subgen)<columns) thend <= d+neighbor.delsed <= d endifendforeachendfor;foreachX withneighbor= &B[i] dod <= neighbor.d; endforeach;foreachX withneighbor=&Atemp[i,0] dod <= d-neighbor.d; endforeach;foreachX withneighbor= &A[i,i] dod <= d / neighbor.d; endforeachendforendprogram(a)(b)5 GCA HARDWARE ARCHITECTURES

61

rule instruction triggers the processing of all cells in a destination object and
applies the so called adapted operators h, e, g coded in the rule. All necessary
application speciﬁc rule instructions are extracted from the source program.
For the Jacobi iteration proigram [20], Fig. 28b shows the generated next
data operation used by a rule instruction. It contains 4 ﬂoating point units
and several integer units. The ﬂoating point operations are internally also
pipelined (+(14 stages), -(14), *(11), /(33)). Our tool generates Verilog code
which is then used further for synthesis with Quartus II for Altera FPGAs.
For p = 8 pipelines, normalized to the amount needed for one pipeline, the
relative increments for the FPGA Altera Stratix II EP2S180 were: 8.3 for the
ALUTs (logic elements), 7.5 for the registers, 4.5 for the memory bits (note
that the required memory bits are theoretically proportional to (p + 1)/2 for
the pipeline architecture). The speedup was 6.8 for 8 pipelines compared to
one. Thus the scaling behavior was very good and almost linear for up to 8
pipelines.

5.3.3 Multisoftcore

Figure 29: Multisoftcore system implemented on an FPGA. A local GCA
cell memory is attached to each NIOS II softcore. Each core can read and
write its own GCA cell memory and read from any other GCA cell memory
via the network.

The basic idea is to use many standard softcores together with speciﬁc
GCA support. Each core is responsible to handle a subset of all cells being
processed in one generation. In our implementation, p NIOS II softcores were
used [22]–[29]. To each processor a GCA cell memory is attached (Fig. 29).
A processor can read via the network the state of a global cell residing in
another cell memory. Only the cells residing in the own cell memory need to

networkNIOS IImemoryGCAcellmemoryNIOS IImemoryGCAcellmemoryreadread, writenetworkNIOS IImemoryGCAcellmemoryNIOS IImemoryGCAcellmemoryreadread, write5 GCA HARDWARE ARCHITECTURES

62

be updated according to the GCA model. No write access via the network
is needed, thereby the network can be simpliﬁed. In case that only a speciﬁc
application has to be implemented, the network can be minimized according
to the communication links used by the application. The machine instruction
set of the NIOS processors was extended (custom instructions), e.g. read a
cell via the network, read/write local cell memory, ﬂoating point operations,
synchronize and copy new cell states into the current cell states.

A tool was developed that can automatically generate C code (extended
by custom instructions) out of a GCA-L program for such a multisoftcore
system. Then this C code is compiled and loaded into the cores of the
system conﬁgured on an FPGA.

6 CONCLUSION

6 Conclusion

63

Global Cellular Automata (GCA) is a new data parallel programming model
related to Cellular Automata (CA). Applications are modeled as a set of
cells which can dynamically connect to any other (global) cell. The global
communication topology is dynamic but locally computed by the cells. In
the basic model, pointers are stored in the cell that point directly to global
neighbors. They are updated by pointer rules taking the states of the cell and
its neighbors into account. In the general model, the pointers are modiﬁed
before access. In the plain model, the state of a cell is not structured into a
data and a pointer part.

The CROW PRAM model is related to the GCA model, therefore CROW
and CREW algorithms can be converted into GCA algorithms. The CROW
model is processor based (n processors with instruction set, common mem-
ory), whereas the GCA model is cell based (state contains pointers, data and
pointer rules, local memories). Boolean Networks can be seen as a special
GCA case where the state is binary and the individual links are ﬁxed.

The range of GCA applications is very wide. Typical applications be-
sides CA applications are graph algorithms, hypercube algorithms, matrix
operations, sorting, PRAM algorithms, particle and multi-agent simulation,
logic simulation, communication networks, pointer structures, and dynamic
topologies. Examples for GCA algorithms were given (maximum, reduction,
preﬁx sum, bitonic merging, diﬀerent XOR rules), and the new application
Synchronous Firing.

GCA algorithms can easily be described in standard languages or in a
special language like GCA-L, and compiled to standard parallel platforms
(like multicores, GPUs), or to special GCA target architectures. GCA target
architectures can relative easily be designed and generated for FPGAs, like
the fully parallel architecture, the data parallel architecture with memory
banks and pipelining, or a multisoftcore architecture.

The eﬀort for the communication network between cells can be reduced
by implementing only the required access pattern of the application, or one
could restrict the set of accessible global neighbors in advance by deﬁnition
(e.g. hypercube or perfect shuﬄe connections) and then use for an algorithm
the allowed connections only.

To summarize, the GCA model is a powerful and easy to use parallel
programming model based on cells with dynamic global neighbors, which can
eﬃciently be executed on standard and special parallel platforms. It fulﬁlls
to a large extent important requirements for a parallel programming model:
user-friendly, platform-independent, eﬃcient, and system-design-friendly.

7 APPENDIX 0: PROGRAMS FOR THE 1D BASIC AND GENERAL MODEL64

7 Appendix 0: Programs for the 1D Basic

and General Model

7.1 Basic Model

The following program can be seen as a prototype for the 1D Basic GCA model. The
cell’s state is (c, p1, p2), where c is the data state and p1, p2 are the pointers. The pointer
rules p1new PointerRule and p1new PointerRule compute the new pointers (multiplying
the current value by 2). The data rule DataRule with Data at Pointers (XOR of left and
right dynamic neighbor) computes the new data state. The classical CA XOR rule can be
emulated by setting the pointer constant to p1 = +1 and p2 = −1.

{5.6.2022 RH. Simple 1D Basic GCA program, XOR with pointers doubled}
program prog_gca_basic_xor;
uses SysUtils;
var

OUT_c, OUT_p1, OUT_p2, OUT_p1eff, OUT_p2eff: textfile;

const BlackSquare=#$E2#$96#$88#$E2#$96#$88;
const OutputZERO=’ ’; OutputONE=’ #’; // BlackSquare; can be used
const N=31; TMAX=5;

// number of cells, max number of generations

type field = array [0..N-1] of integer;
type cell = record c,cnew, p1,p1n2, p2,p2new, p1eff,p2eff : field end;

// cell’s structure, not used here

var

c, cnew: field;
p1, p1new: field;
p2, p2new: field;
t: integer;

// data state, buffered sync operation
// stored relative pointer, buffered sync operation

var
//=========================================================================== FUNCTIONS, PROCEDURES
function modN(a:integer):integer; begin modN:=(a+N)mod N; end;

// time-counter, generation

function p1new_PointerRule(x:integer):integer;
const p1init= +1;
begin

//_____________________ initial set pointer const at t=0
if t=0 then p1new_PointerRule:=p1init;
//_____________________ initial set pointer const at t=0
//_____________________ for t=1,2, ...
if t>0 then begin

p1new_PointerRule:=(p1[x]*2) mod N; // 1,2,4, ...
if p1new_PointerRule=0 then p1new_PointerRule:= p1init; end;//don’t use p=0, instead p1init

//_____________________ for t=1,2, ...

end;

function p2new_PointerRule(x:integer):integer;
const p2init= -1;
begin

//_____________________ initial set pointer const at t=0
if t=0 then p2new_PointerRule:=p2init;
//_____________________ initial set pointer const at t=0
//_____________________ for t=1,2, ...
if t>0 then
begin

p2new_PointerRule:=(p2[x]*2) mod N; // -1,-2,-4, ...
if p2new_PointerRule=0 then p2new_PointerRule:= p2init; //don’t use p=0, instead p2init

end;
//_____________________ for t=1,2, ...

end;
//________________________________________ new Pointer for all cells
procedure p1new_p2new_Apply_PointerRule_at_t_for_tplus1;
var x: integer;
begin

// cell’s index/position

for x:=0 to N-1 do

begin p1new[x]:=p1new_PointerRule(x);

p2new[x]:=p2new_PointerRule(x); end;

end;
//________________________________________ new Pointer for all cells
//________________________________________ data rule at site x
function DataRule_with_Data_at_Pointers(x,p1,p2: integer):integer;

function abs(p_relative:integer): integer;
begin

abs:=modN(x+p_relative);

end;

7 APPENDIX 0: PROGRAMS FOR THE 1D BASIC AND GENERAL MODEL65

begin

// L exor R, abs(p1)=modN(x+p1), c[x] or c[modn(x+1) .. could also be used
// may also depend on cell’s state, fixed neighbors’ states, time t, index x
// new data cnew may depend on: t,x, (c, p1, p2), p1.(c,p1,p2), p2.(c,p1,p2)
DataRule_with_Data_at_Pointers:=( c[abs(p1)]+c[abs(p2)] ) mod 2;

end;
//________________________________________ data rule at site x
//________________________________________ new cells’ data states
procedure cnew_ApplyDataRule;
var x: integer;
begin for x:=0 to N-1 do cnew[x]:=DataRule_with_Data_at_Pointers(x, p1[x], p2[x]); end;
//________________________________________ new cells’ data states
//________________________________________ init data state
procedure c_init(z:integer);
var x:integer;
begin for x:=0 to N-1 do c[x]:=z; end;
procedure c_init_Point_middle(background,color:integer);
begin c_init(background); c[N div 2]:=color; end;
//________________________________________ init data state
//________________________________________ print
procedure c_print;
var x, mid:integer;
begin

mid:=N div 2;
for x:=0 to N-1 do

// show pointers of cell at midddle

case c[x]

of 0: write(OUT_c, OutputZERO); 1: write(OUT_c, OutputONE); otherwise write(OUT_c, ’ ?’); end;

writeln(OUT_c,’ t=’,t:4, ’ at[mid]: ’, ’p1=’, p1[mid]:4,’ p2=’, p2[mid]:4);

end;
procedure p_print(var ff:textfile; pointer:field);
var x:integer; DIGITS:integer=3;
begin if N<10 then DIGITS:=2 else if N<100 then DIGITS:=3 else if N<1000 then DIGITS:=4 else DIGITS:=5;

// p1, p2

for x:=0 to N-1 do

write(ff, pointer[x]:DIGITS );

writeln(ff,’ t=’,t);

end;
//________________________________________ print
// ========================================================================== FUNCTIONS, PROCEDURES
// ========================================================================== MAIN
BEGIN

’OUT_p2.txt’);

rewrite(OUT_p2);

t:=0;

’OUT_c.txt’);
’OUT_p1.txt’);

rewrite(OUT_c);
rewrite(OUT_p1); assign(OUT_p2,

assign(OUT_c,
assign(OUT_p1,
//______________________________________ init data at t=0
c_init_Point_middle(0,1);
//______________________________________ init data at t=0
//______________________________________ init pointer at t=0
p1new_p2new_Apply_PointerRule_at_t_for_tplus1; // init for t=0, see proc!
p1:=p1new; p2:=p2new;
//______________________________________ init pointer at t=0
//______________________________________ output initial at t=0
c_print;
p_print(OUT_p1,p1); p_print(OUT_p2,p2);
//______________________________________ output initial at t=0
for t:=1 to TMAX do
begin

//syncupdate pointer t=0, init

//____________________________________ compute next generation
//# state c and pointers p1,p2 are available (were computed at t-1)
cnew_ApplyDataRule;
p1new_p2new_Apply_PointerRule_at_t_for_tplus1; // 1b. apply pointer rules
c:=cnew;
p1:=p1new; p2:=p2new;
//____________________________________ compute next generation
//____________________________________ output new generation at t after computation
c_print; p_print(OUT_p1,p1); p_print(OUT_p2,p2);
//____________________________________ output new generation at t after computation

// 2a. syncupdate data
// 2b. syncupdate pointer

// 1a. apply data rule

end;
close(OUT_c); close(OUT_p1); close(OUT_p2);

END.
// ========================================================================== MAIN END
output textfile OUT_c:

#

#
#
#
#
#
# # # # # # # # # # # # # # #

#
#
#

#
#

#
#

#

#

#

t=
t=
#
t=
#
t=
#
# t=
#
#
# # # # # # # # # # # # # # # t=

#
#
#

#
#

#
#

#

#

1 p2=
-1
0 at[mid]: p1=
2 p2= -2
1 at[mid]: p1=
4 p2=
-4
2 at[mid]: p1=
3 at[mid]: p1=
8 p2= -8
4 at[mid]: p1= 16 p2= -16
-1
5 at[mid]: p1=

1 p2=

7 APPENDIX 0: PROGRAMS FOR THE 1D BASIC AND GENERAL MODEL66

7.2 General Model with Address Modiﬁcation

The following general GCA program computes the same result as the basic
GCA program before. Two address bases p1 and p2 are used, that store the
same value sequence 1, 2, 4, ... . (Therefore it would be suﬃcient to use one
address base only.) The eﬀective addresses are p1eﬀ = p1 and p2eﬀ = −p2.

{5.6.2022 RH. Simple 1D General GCA program, XOR, address modification}
program prog_gca_gneral_xor;
uses SysUtils;
var

OUT_c, OUT_p1, OUT_p2, OUT_p1eff, OUT_p2eff: textfile;

const BlackSquare=#$E2#$96#$88#$E2#$96#$88;
const OutputZERO=’ ’; OutputONE=’ #’; // BlackSquare; can be used
const N=31; TMAX=5;

// number of cells, max number of generations

type field = array [0..N-1] of integer;
type cell = record c,cnew, p1,p1n2, p2,p2new, p1eff,p2eff : field end;

// cell’s structure, not used here

var

c, cnew: field;
p1, p1new: field;
p2, p2new: field;
p1eff,p2eff: field; // effective addresses, only temp variable
t: integer;

// data state, buffered sync operation
// stored relative pointer, buffered sync operation

// time-counter, generation

var
//=========================================================================== FUNCTIONS, PROCEDURES
function modN(a:integer):integer; begin modN:=(a+N)mod N; end;

//________________________________________ effective address
procedure p1eff_p2eff_EffectiveAddress_at(x:integer);
begin

p1eff[x]:=-1; p2eff[x]:=+1; // fixed nearest neighbors ok ECA, default
// may also depend on cell’s data state, fixed neighbors’ states, time, index x
begin p1eff[x]:= p1[x]; p2eff[x]:= -p2[x]; end; // modified

end;
//________________________________________ effective address
//________________________________________ effective address for all cells
procedure p1eff_p2eff_Apply_EffectiveAddress_at_t_for_t;
var x: integer;
begin for x:=0 to N-1 do p1eff_p2eff_EffectiveAddress_at(x); end;
//________________________________________ effective address for all cells
//________________________________________ new Pointer for all cells
procedure p1new_p2new_Apply_PointerRule_at_t_for_tplus1;
const p1init=1; p2init=1; var
begin

// cell’s index/position

x: integer;

// index

for x:=0 to N-1 do
begin

// new pointer pnew may depend on: t,x, (c, p1, p2), p1.(c,p1,p2), p2.(c,p1,p2)
//............................................................Pointer Rules

//_____________________ initial set pointer const at t=0
if t=0 then p1new[x]:=p1init; p2new[x]:=p2init;
//_____________________ initial set pointer const at t=0
//_____________________ for t=1,2, ...
if t>0 then begin

p1new[x]:=(p1[x]*2) mod N; p2new[x]:=(p2[x]*2) mod N;
if p1new[x]=0 then p1new[x]:=p1init; //don’t use p=0, instead pinit=1
if p2new[x]=0 then p2new[x]:=p2init; end;

//_____________________ for t=1,2, ...

//............................................................Pointer Rules

end; // for x

end;
//________________________________________ new Pointer for all cells
//________________________________________ data rule at site x
function DataRule_with_Data_at_Pointers(x,p1eff,p2eff: integer):integer;

function abs(peff_relative:integer): integer;
begin

abs:=modN(x+peff_relative); end;

begin

// L exor R, abs(p1eff)=modN(x+p1eff), c[x] or c[modn(x+1) .. could also be used
// may also depend on cell’s state, fixed neighbors’ states, time t, index x
// new data cnew may depend on: t,x, (c, p1, p2), p1.(c,p1,p2), p2.(c,p1,p2)
DataRule_with_Data_at_Pointers:=( c[abs(p1eff)]+c[abs(p2eff)] ) mod 2;

end;
//________________________________________ data rule at site x
//________________________________________ new cells’ data states
procedure cnew_ApplyDataRule;
var x: integer;
begin for x:=0 to N-1 do cnew[x]:=DataRule_with_Data_at_Pointers(x, p1eff[x], p2eff[x]); end;
//________________________________________ new cells’ data states
//________________________________________ init data state

7 APPENDIX 0: PROGRAMS FOR THE 1D BASIC AND GENERAL MODEL67

x:integer;

procedure c_init(z:integer);
var
begin for x:=0 to N-1 do c[x]:=z; end;
procedure c_init_Point_middle(background,color:integer);
begin c_init(background); c[N div 2]:=color; end;
//________________________________________ init data state
//________________________________________ print
procedure c_print;
var x, mid:integer;
begin

mid:=N div 2;
for x:=0 to N-1 do

// show pointers of cell at mid

case c[x]

of 0: write(OUT_c, OutputZERO); 1: write(OUT_c, OutputONE);

otherwise write(OUT_c, ’ ?’); end;

writeln(OUT_c,’ t=’,t:4, ’ at[mid]: ’,
’p1=’, p1[mid]:4,’ p2=’, p2[mid]:4,’ p1eff=’,p1eff[mid]:4,’ p2eff=’,p2eff[mid]:4);

end;
procedure p_print(var ff:textfile; pointer:field);
var x:integer;
var DIGITS:integer=3;
begin if N<10 then DIGITS:=2 else if N<100 then DIGITS:=3
else if N<1000 then DIGITS:=4 else DIGITS:=5;
for x:=0 to N-1 do

write(ff, pointer[x]:DIGITS );

//p1,p2,p1eff,p21eff

writeln(ff,’ t=’,t);

end;
//________________________________________ print
//=========================================================================== FUNCTIONS, PROCEDURES
// ========================================================================== MAIN
BEGIN

’OUT_c.txt’);
’OUT_p1.txt’);
’OUT_p2.txt’);

rewrite(OUT_c);
rewrite(OUT_p1);
rewrite(OUT_p2);

assign(OUT_c,
assign(OUT_p1,
assign(OUT_p2,
assign(OUT_p1eff, ’OUT_p1eff.txt’); rewrite(OUT_p1eff);
assign(OUT_p2eff, ’OUT_p2eff.txt’); rewrite(OUT_p2eff);
//______________________________________ init data at t=0
c_init_Point_middle(0,1);
//______________________________________ init data at t=0
//______________________________________ init pointer at t=0
t:=0;
p1new_p2new_Apply_PointerRule_at_t_for_tplus1; // init for t=0, see proc!
p1:=p1new; p2:=p2new;
// peff depends on p init, to be printed at t=0
p1eff_p2eff_Apply_EffectiveAddress_at_t_for_t;
//______________________________________ init pointer at t=0
//______________________________________ output initial at t=0
c_print;
p_print(OUT_p1,p1); p_print(OUT_p2,p2);
p_print(OUT_p1eff,p1eff); p_print(OUT_p2eff,p2eff);
//______________________________________ output initial at t=0
for t:=1 to TMAX do
begin

//syncupdate pointer t=0, init

//____________________________________ compute next generation
//# state c and pointer p are computed
p1eff_p2eff_Apply_EffectiveAddress_at_t_for_t; // 1. compute peff
cnew_ApplyDataRule;
p1new_p2new_Apply_PointerRule_at_t_for_tplus1; // 2b. apply pointer rule
c:=cnew;
p1:=p1new; p2:=p2new;
//____________________________________ compute next generation
//____________________________________ output new generation at t after computation
c_print; p_print(OUT_p1,p1); p_print(OUT_p2,p2);
p_print(OUT_p1eff,p1eff);
//____________________________________ output new generation at t after computation

// 3a. syncupdate data
// 3b. syncupdate pointer

p_print(OUT_p2eff,p2eff);

// 2a. apply data rule

end;
close(OUT_c); close(OUT_p1); close(OUT_p2); close(OUT_p1eff); close(OUT_p2eff)

END.
// ========================================================================== MAIN END
output text file OUT_c:

#

#
#
#
#
#
# # # # # # # # # # # # # # #

#
#
#

#
#

#
#

#

#

#

t=
t=
#
t=
#
t=
#
#
# t=
#
# # # # # # # # # # # # # # # t=

#
#
#

#
#

#
#

#

#

1 p2=
0 at[mid]: p1=
2 p2=
1 at[mid]: p1=
4 p2=
2 at[mid]: p1=
3 at[mid]: p1=
8 p2=
4 at[mid]: p1= 16 p2=
1 p2=
5 at[mid]: p1=

1 p1eff=
2 p1eff=
4 p1eff=
8 p1eff=
16 p1eff=
1 p1eff=

1 p2eff= -1
1 p2eff=
-1
2 p2eff= -2
-4
4 p2eff=
-8
8 p2eff=
16 p2eff= -16

8 APPENDIX 1: PROGRAM FOR SYNCHRONOUS FIRING WITHIN TWO RINGS68

8 Appendix 1: Program for Synchronous Fir-

ing within Two Rings

Figure 30: Pascal program part 1: Main. Synchronous Firing within two
rings in 1D using waves as described in Sect. 4.5.2.

8 APPENDIX 1: PROGRAM FOR SYNCHRONOUS FIRING WITHIN TWO RINGS69

Figure 31: Pascal program part 2: The Rule fssp. Synchronous Firing within
two rings in 1D using waves as described in Sect. 4.5.2.

9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL70

9 Appendix 2: First Paper [1] Introducing

the GCA Model

Global Cellular Automata GCA:
An Universal Extension of the CA Model
Rolf Hoﬀmann, Klaus–Peter V¨olkmann, Stefan Waldschmidt
Darmstadt University of Technology, Germany
(hoffmann,voelk,waldsch)@informatik.tu-darmstadt.de

Abstract

A model called global cellular automata (GCA) will be introduced.
The new model preserves the good features of the cellular automata
but overcomes its restrictions. In the GCA the cell state consists of
a data ﬁeld and additional pointers. Via these pointers, each cell has
read access to any other cell in the cell ﬁeld, and the pointers may
be changed from generation to generation. Compared to the cellular
automata the neighbourhood is dynamic and diﬀers from cell to cell.
For many applications parallel algorithms can be found straight for-
ward and can directly be mapped on this model. As the model is also
massive parallel in a simple way, it can eﬃciently be supported by
hardware. 10

9.1 Motivation

The classical cellular automata model (CA) can be characterized by the fol-
lowing features

• The CA consists of a n–dimensional ﬁeld of cells. Each cell can be

identiﬁed by its coordinates.

• The neighbours are ﬁxed and are deﬁned by relative coordinates.

• Each cell has local read access to the states of its neighbours. Each cell
contains a local rule. The local rule deﬁnes the next state depending
on the cell state and the states of the neighbours.

• The cells are updated synchronously, the new generation of cells (new

cell states) depend on the old generation (old cell states).

• The model is massive parallel, because all next states can be computed

and updated in parallel.

10The section numbering has changed here because the old paper was integrated into

this comprising publication.

9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL71

• Space or time dependent rules can be implemented by the use of special

space or time information coded in the state.

The CA is very well suited to problems and algorithms, which need only
access to their ﬁxed local neighbours [6]. Algorithms with global (long dis-
tance) communication can only indirectly be implemented by CA. In this
case the information must be transported step by step along the line from
the source cell to the destination cell, which needs a lot of time. Therefore
the CA is not an eﬃcient model for global algorithms.

We have searched for a new model, which preserves the good features of
the CA but overcomes the local communication restriction. The new model
shall be still massive parallel, but at the same time suited to any kind of
global algorithm. Thus we will be able to describe a more general class of
algorithms in a more eﬃcient and direct way. We also claim that this model
can eﬃciently be implemented in hardware.

9.2 The GCA model

The model is called global automata model (GCA). The GCA can be charac-
terized by the following features

• A GCA consists of a n–dimensional ﬁeld of cells. Each cell can be

identiﬁed by its coordinates.

• Each cell has n individual neighbours which are variable and may
change from generation to generation. The neighbours are deﬁned by
relative coordinates (addresses, pointers).

• The state of a cell contains a data ﬁeld and n address ﬁelds.

State = (Data, Address1, Address2, ...)

• Each cell has global read access to the states of its neighbours by the

use of the address ﬁelds.

• Each cell contains a local rule. The local rule deﬁnes the next state de-
pending on the cell state and the states of the neighbours. By changing
the state, the addresses may also be changed, meaning that in the next
generation diﬀerent neighbours will be accessed.

• The cells are updated synchronously, the new generation of cells de-

pends on the old generation.

9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL72

• The model is massive parallel, because all next states can be computed

and updated in parallel.

• Space or time dependent rules can be implemented by the use of special

space or time information coded in the state.

A one–dimensional GCA with two address ﬁelds will be deﬁned in a formal

way, using a PASCAL like notation:

1. The cell ﬁeld

Cell = array [0..n-1] of State

2. The State of each cell

State = record

Data: Datatype
Address1: 0..n-1
Address2: 0..n-1

endrecord

3. The deﬁnition of the local rule

function Rule(Self:State, Neighbour1:State,Neighbour2:State)

4. The computation of the next generation

for i=0..n-1 do in parallel

Cell[i]:= Rule(Cell[i], Cell[Address1], Cell[Address2])

endfor

Fig. 32 shows the principle of the GCA model. Cell[i] reads two other
cell states and computes its next state, using its own state and the states of
the two other states in access. In the next state, Cell[i] may point to two
diﬀerent cells.

The above model can be deﬁned in a more general way with respect to

the following features

• The number k of addresses can be 1, 2, 3... If k=1 we call it a one-

handed GCA, if k=2 we call it a two-handed GCA and so forth.

• The number k may vary in time and from cell to cell, in this case it

will be a variable-handed GCA.

9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL73

Figure 32: The GCA model.

• Names could be used for the identiﬁcation of the cells, instead of or-
dered addresses. In this case the cells can be considered as an unordered
set of cells.

• A special passive state may be used to indicate that the cell state shall
not be changed any more. It can be used to indicate the end of the
computation or the deletion of a cell. A cell which is not in the passive
state is called active. An active cell may turn a passive cell to active.

Similar models have been proposed before[3]. Usually they are theoreti-
cally oriented and lack any aspects of applications and implementations[4].

9.3 Mapping problems on the GCA model

The GCA has a very simple and direct programming model. The program-
ming model is the way how the programmer has to think in order to map an
algorithm to a certain model, which is interpreted by a machine. In our case,
the programmer has to keep in mind, that a machine exists which interpretes
and executes the GCA model.

Many problems can easily and eﬃciently be mapped to the GCA model,

e.g.

• sorting of numbers

• reducing a vector, like sum of vector elements

• matrix multiplication

 Cell[i]Cell[Address2]Cell[Address1]Address1Address2.........Rulenext State9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL74

• permutation of vector elements

• graph algorithms

The following examples are written in the cellular programming language
CDL[1]. CDL was designed to facilitate the description of cellular rules based
on a rectangular n-dimensional grid with a local neighbourhood. The locality
of the neighbourhood radius was asserted and controlled by the declaration of
distance=radius. For the GCA the new keyword infinity was introduced
for the declaration of the radius.

In CDL the unary operator * is used (like in C) to dereference the rel-
ative address of a cell in order to obtain the state of the referenced cell.
The following examples are one–handed GCAs, showing how useful unlim-
ited read-access to any other cell is.

9.3.1 Example 1: Firing Squad Problem

This is an implementation of the ﬁring squad algorithm on a one dimen-
sional array. The set of possible states for every cell is described by the type
celltype in lines (7) to (11).

The sequence of soldier cells (kind=soldier) has to be enclosed by edge–
cells (kind=edge) to mark the border of the squad. The wave component of
all cells should be initialised with [-1] which is the relative address, pointing
to the neighbour on the left.

// allows unlimited access

// have an edge on each side
// init with [-1]
// init with false

celltype=record

kind : (soldier,edge);
wave : celladdress;
fire : boolean;

dimension = 1;
distance = infinity;
init=[-1];

(1) cellular automaton firing_1;
(2)
(3) const
(4)
(5)
(6)
(7) type
(8)
(9)
(10)
(11)
(12)
(13) var
(14)
(15) rule begin
(16)
(17)
(18)
(19)
(20)

if (n=init) then

n:celladdress;

end;

n:=(*[0]).wave; // address of the cell our wave points to

if (*n.kind=edge) or (*n.wave!=init) then

// we are in init state

// the wave is just coming

9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL75

*[0].wave:=[0]

if *n.kind=edge then
*[0].fire:=true

else

else

(21)
(22)
(23)
(24)
(25)
(26)
(27) end;

// the wave passed already

// the wave reached the edge

*[0].wave:=[n.%1+1];

// the wave is still rolling

At the beginning (n=[-1]) every soldier is looking to his direct neighbour
on the left. If his neighbour is a edge cell or a soldier which is not in the init
state anymore (line (19)) the soldier himself will leave the init state (line
(21)) and deﬁnes the front of the wave.

Figure 33: The ﬁring squad.

If the wave already passed the soldier (lines (23) to (26)) the variable
n points to the wave front. All soldiers ﬁre when the wave reaches the right
edge, otherwise the wave rolls on one more step.

9.3.2 Example 2: Fast Fourier Transformation

The fast Fourier transformation (FFT) is another, more complex example.
We do not want to explain the algorithm in this paper, it is described in
details in [5]. The example is used to demonstrate that a complex algorithm
can

wave:        -1 -1 -1 -1 -1wave:         0 -1 -1 -1 -1         wave:         1  0 -1 -1 -1wave:         2  1  0 -1 -1wave:         3  2  1  0 -1wave:         5  4  3  2  1wave:         4  3  2  1  0wave:         5  4  3  2  1fire:         *  *  *  *  *kind:   E  E  s  s  s  s  s  E  E012345679 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL76

• easily be mapped onto the GCA model

• concisely be described

• eﬃciently be executed in parallel

Each cell contains a complex number (r,i) which is calculated in every
time step from its own number and the number contained in another cell.
The address of the other cell depends on its own absolute address (position)
and the time step in the way shown in ﬁg. 34.

Figure 34: The FFT access pattern.

For example, the cell at position 2 reads the cell at position 3 in the ﬁrst
step, the cell at position 0 in the next step, and the cell at position 6 in
the last time step. Obviously this access pattern can not be implemented
eﬃciently on a classical cellular automaton using strict locality.

(1) cellular automaton FFT;
(2)
(3) const dimension=1;
(4)
(5)
(6) type celltype=record
(7)
(8)
(9)
(10)
(11)
(12) #define this
(13)
(14) var other:celladdress;

r,i
step
position

end;

distance=infinity;

// global access, radius of neighborhood

// the complex value

: float;
: integer; // initialised with 1
: integer; // init with 0..(2^k)-1

*[0] // the cell’s state, contents(*) of rel. address 0

pos101234567248step9 APPENDIX 2: FIRST PAPER [1] INTRODUCING THE GCA MODEL77

a,wr,wi:float;

(15)
(16)
(17) rule begin
(18)
(19)
(20)
(21)
(22)
(23)
(24)
(25)
(26)
(27)
(28)
(29)
(30)
(31)
(32)
(33)
(34)
(35)
(36)
(37)
(38) end

}
else
{

}

// calculate relative address of other cell
other := [ (this.position exor this.step)-this.position ];

// calculate new values for local r and i
a:= -pi / this.step * (this.position and (this.step-1));
wr:=cos(a);
wi:=sin(a);
if ( other > 0 )
{

// other cell has higher number

this.r := this.r + wr* *other.r - wi* *other.i;
this.i := this.i + wr* *other.i + wi* *other.r;

this.r := *other.r - ( wr* this.r - wi* this.i );
this.i := *other.i - ( wr* this.i + wi* this.r );

// other cell has lower number

this.step
:= 2 * this.step;
this.position:= this.position;

// step=1,2,4,8...
// carry own position

The algorithm is concise and eﬃcient because the address of the neighbour
is calculated (line (19)) and thereby an individual neighbour is accessed
(lines (27) and (28)). The listing of the FFT without using this feature
would at least be twice as long and the calculation would take signiﬁcantly
more time.

9.4 Conclusion

We have introduced a powerful model, called global cellular automata (GCA).
The cell state is composed of a data ﬁeld and n pointers which point to n
arbitrary other cells. The new cell state is computed by a local rule, which
takes into account its own state and the states of the other cells which are
in access via the pointers. In the next generation the pointers may point to
diﬀerent cells. Each cell changes its state independently from the other cells,
there are no write conﬂicts. Therefore the GCA model is massive parallel
meaning that it has a great potential to be eﬃciently supported by hardware.
We plan do implement the GCA model on the CEPRA-S processor [2].

Parallel algorithms can easily be described and mapped onto the GCA.
Compared to the CA model it is much more ﬂexible although it is only a
little more complex.

REFERENCES

78

9.5 References of First Paper (Appendix 2)

References

[1] Christian Hochberger, Rolf Hoﬀmann, and Stefan Waldschmidt. Compilation of CDL
In Viktor Malyshkin, editor, Parallel Computing

for diﬀerent target architectures.
Technologies, pages 169–179, Berlin, Heidelberg, 1995. Springer.

[2] Rolf Hoﬀmann, Bernd Ulmann, Klaus-Peter V¨olkmann, and Stefan Waldschmidt. A
stream processor architecture based on the conﬁgurable CEPRA–S.
In Reiner W.
Hartenstein and Herbert Gr¨unbacher, editors, Field–Programmable Logic and Appli-
cations, pages 822–825, Berlin, Heidelberg, 2000. Springer.

[3] A.N. Kolmogorov and V.A. Uspenskii. On the deﬁnition of an algorithm. In American
Mathematical Society Translations, volume 9 of Series 2, pages 217–245. American
Mathematical Society, 1963.

[4] Arnold Sch¨onhage. Real-time simulation of multidimensional turing machines by stor-
age modiﬁcation machines. SIAM Journal on Computing, 9(3):490–508, August 1980.

[5] Samuel D. Stearns. Digital Signal Analysis. Hayden Book Company, Rochelle Park,

New Jersey, 1975.

[6] T. Toﬀoli and N. Margolus. Cellular Automata Machines. MIT Press, Cambridge

Mass., 1987.

10 REFERENCES OF SECTIONS 1 – 6

79

10 References of Sections 1 – 6

References

Global Cellular Automata GCA

[1] Hoﬀmann, R., V¨olkmann, K.P., Waldschmidt, S. : Global cellular automata GCA:
an universal extension of the CA model. In: ACRI 2000 Conference Proceedings.
“Work in Progress” session, Karlsruhe, Germany, Oct. 4th - 6th. (2000)

[2] Hoﬀmann, R., V¨olkmann, K.P., Waldschmidt, S., Heenes, W. : GCA: Global cel-
lular automata. A ﬂexible parallel model. In Malyshkin, V.E., ed. : Parallel Com-
puting Technologies, 6th International Conference, PaCT 2001, Novosibirsk, Russia,
September 3-7, 2001, Proceedings. Volume 2127 of LNCS, Springer 66-73 (2001)

[3] Hoﬀmann, R., V¨olkmann, K.P., Heenes, W. : Globaler Zellularautomat (GCA): Ein
neues massivparalleles Berechnungsmodell. PARS Workshop, Oct. 8th - 9th 2001,
Munich, PARS Mitteilungen GI (2001)

[4] Hoﬀmann, R., V¨olkmann, K. P., Heenes, W. : GCA: A massively parallel Model.
In Proceedings International Parallel and Distributed Processing Symposium IPDS,
Nice, France. IEEE (2003, April)

[5] Ehrt, C. : Globaler Zellularautomat: Parallele Algorithmen. Diplomarbeit, Technis-

che Universt¨at Darmstadt, FB20. (2005)

[6] Jendrsczok, J., Ediger, P., Hoﬀmann, R. : The Global Cellular Automata Exper-
imental Language GCA-L1. Technical Report RA-1-2007, Technische Universit¨at
Darmstadt (2007)

[7] Hoﬀmann, R. : The massively parallel computing model GCA. In European Confer-
ence on Parallel Processing (pp. 77-84). Springer, Berlin, Heidelberg. (August 2010)

GCA Architectures and Hardware Implementations

[8] Hoﬀmann, R., V¨olkmann, K.P., Heenes, W. : Architekturen f¨ur das massiv-parallele
Rechenmodell GCA (GlobalCellular Automata). Technical Report Informatik Rech-
nerarchitektur RA-1-2002, Technische Universit¨at Darmstadt (2002)

[9] Heenes, W., Hoﬀmann, R., V¨olkmann, K.P. : Architekturen f¨ur den globalen Zellu-
larautomaten, 19th PARS Workshop, Gesellschaft f¨ur Informatik (GI). Basel (March
2003)

[10] Hoﬀmann, R., V¨olkmann, K. P., Heenes, W. : GCA: A massively parallel Model.
In Proceedings International Parallel and Distributed Processing Symposium (pp.
7-pp). IEEE.(2003, April)

[11] Hoﬀmann, R., Heenes, W., Halbach, M. : Implementation of the Massively Parallel

Model GCA, PARELEC 2004, pp. 135–139, IEEE Computer Society (2004)

REFERENCES

80

[12] Heenes, W., Hoﬀmann, R., Kanthak, S. : FPGA implementations of the massively
parallel GCA model. In 19th IEEE International Parallel and Distributed Processing
Symposium (pp. 6-pp). IEEE. (2005, April)

[13] Heenes, W., Jendrsczok, J., Hoﬀmann, R. : Eine massiv parallele Rechnerarchitektur

f¨ur das GCA Modell. In PARS-Workshop, GI, L¨ubeck. (2005)

[14] Heenes, W., Hoﬀmann, R., Jendrsczok, J. : A multiprocessor architecture for the
massively parallel model GCA. In: IPDPS/SMTPS 2006, 25. bis 29. April, Rhodes
Island, Greece, IEEE Proceedings: 20th International Parallel & Distributed Pro-
cessing Symposium, IEEE (2006)

[15] Heenes, W. : Entwurf und Realisierung von massivparallelen Architekturen f¨ur Glob-
ale Zellulare Automaten. Dissertation Technische Universt¨at Darmstadt, D17. (2007)

[16] Jendrsczok, J., Hoﬀmann, R., Ediger, P., Keller, J. : Implementing APL-like data
parallel functions on a GCA machine. In Proc. 21st Workshop Parallel Algorithms
and Computing Systems (PARS) GI. (2007)

[17] Jendrsczok, J., Hoﬀmann, R., Keller, J. . Hirschberg’s Algorithm on a GCA and its
Parallel Hardware Implementation. In European Conference on Parallel Processing
(pp. 815-824). Springer. (2007)

[18] Jendrsczok, J., Hoﬀmann, R., Keller, J.

Implementing Hirschberg’s PRAM-
algorithm for connected components on a global cellular automaton. International
Journal of Foundations of Computer Science, 19(06), 1299-1316. (2008)

:

[19] Jendrsczok, J., Ediger, P., Hoﬀmann, R. : A Scalable Conﬁgurable Architecture for
the Massively Parallel GCA Model. International Journal of Parallel, Emergent and
Distributed Systems (IJPEDS) 24(4) 275-291 (2009), and in 2008 IEEE International
Parallel & Distributed Processing Symposium (pp. 1-8). IEEE Computer Society.
(2008, April)

[20] Jendrsczok, J., Hoﬀmann, R., Ediger, P. : A Generated Data Parallel GCA Machine
for the Jacobi Method. In: 3rd HiPEAC Workshop on Reconﬁgurable Computing
January 25th, 2009, Paphos, Cyprus. 73-82 (2009)

[21] Jendrsczok, J., Hoﬀmann, R., Lenck, T. : Generated Horizontal and Vertical Data
Parallel GCA Machines for the N-Body Force Calculation. In Berekovic, M., M¨uller-
Schloer, C., Hochberger, C., Wong, S., eds. : Architecture of Computing Systems -
ARCS 2009, 22nd International Conference, Delft, The Netherlands, March 10-13,
2009. Proceedings. Volume 5455 of LNCS, Springer 96-107 (2009)

[22] Sch¨ack, C., Heenes, W., Hoﬀmann, R. : A Multiprocessor Architecture with an
Omega Network for the Massively Parallel Model GCA. In Bertels, K., Dimopoulos,
N., Silvano, C., Wong, S., eds. : Embedded Computer Systems: Architectures, Mod-
eling, and Simulation. Volume 5657 of LNCS, Springer Berlin / Heidelberg 98–107
(July 2009)

REFERENCES

81

[23] Sch¨ack, C., Heenes, W., Hoﬀmann, R. : Network Optimization of a Multiprocessor
Architecture for the Massively Parallel Model GCA. In: Mitteilungen - Gesellschaft
f¨ur Informatik e. V., Parallel-Algorithmen und Rechnerstrukturen PARS. Vol-
ume 26., Wolfgang Karl and Rolf Hoﬀmann and Wolfgang Heenes 48–57 (December
2009)

[24] Sch¨ack, C., Heenes, W., Hoﬀmann, R. : GCA Multi-Softcore Architecture for Agent
Systems Simulation. Gesellschaft f¨ur Informatik 2009 – Tagung Im Focus das Leben.
(2009)

[25] Sch¨ack, C., Heenes, W., Hoﬀmann, R. : Multiprocessor architectures specialized for
multi-agent simulation. In 2010 First International Conference on Networking and
Computing (pp. 232-236). IEEE. (2010, November)

[26] Sch¨ack, C., Hoﬀmann, R., Heenes, W. : Eﬃcient traﬃc simulation using the GCA
model. In 2010 IEEE International Symposium on Parallel & Distributed Processing,
Workshops and Phd Forum (IPDPSW) (pp. 1-7). IEEE. (2010, April)

[27] Sch¨ack, C., Hoﬀmann, R., Heenes, W. : Eﬃcient Traﬃc Simulation Using Agents
within the Global Cellular Automata Model. International Journal of Networking
and Computing, 1(1), 2-20. (2011)

[28] Sch¨ack, C., Hoﬀmann, R., Heenes, W. : Specialized Multicore Architectures Sup-
porting Eﬃcient Multi-Agent Simulations. International Journal of Networking and
Computing, 1(2), 191-210. (2011)

[29] Sch¨ack, C. A. : Konﬁgurierbare Prozessorsysteme zur hardwareunterst¨utzten Simu-
lation von Agentensystemen auf der Basis von globalen zellularen Automaten. PhD
Dissertation, Technische Universit¨at Darmstadt). (2011)

[30] Milde, B., Buescher, N., and Goesele, M.

Implementing the Global Cellu-
lar Automata on CUDA. GI PARS: Parallel-Algorithmen,-Rechnerstrukturen und-
Systemsoftware: Vol. 28, No. 1. (2011)

:

[31] Jendrsczok, J. : Generierung applikationsspeziﬁscher Architekturen f¨ur das GCA-

Modell. PhD Dissertation, FernUniversit¨at Hagen. (2016)

[32] Wiegand, C., Siemers, C., Richter, H. : Deﬁnition of a conﬁgurable architecture for
implementation of global cellular automaton. In: M¨uller-Schloer, C., Ungerer, T.,
Bauer, B. (eds.) International Conference on Architecture of Computing Systems
ARCS 2004. LNCS, vol. 2981, pp. 140–155. Springer, Heidelberg (2004)

[33] Drieseberg, J., Siemers, C. : C to Cellular Automata and Execution on CPU, GPU
and FPGA. In 2012 International Conference on High Performance Computing &
Simulation (HPCS) (pp. 216-222). IEEE (2012, July).

PRAM Models, CROW Model

[34] Dymond, P., Ruzzo, W. : Parallel RAMs with owned global memory and determin-
istic context-free language recognition. In: In Proc. of the 13th ICALP. Volume 226
of LNCS, Springer 95-104 (1986)

REFERENCES

82

[35] Dymond, P., Ruzzo, W. : Parallel RAMs with owned global memory and deter-
ministic language recognition. In Proc. of 13th ICALP, number 226 in LNCS, pages
95–104. Springer (1987)

[36] Noam N. : CREW PRAMs and Decision Trees, SIAM J. Comput., 20(6), 999-1007

(1991)

[37] Rossmanith P. : The owner concept for PRAMs. In: Choﬀrut C., Jantzen M. (eds)
STACS 91. STACS 1991. Lecture Notes in Computer Science, vol 480. Springer,
Berlin, Heidelberg. (1991)

[38] Gomm, D., Heckner, M., Lange, K. J., Riedle, G. : On the design of parallel pro-
grams for machines with distributed memory. In European Conference on Distributed
Memory Computing (pp. 381-391). Springer, Berlin, Heidelberg. (1991)

[39] J´aJ´a, J. : An Introduction to Parallel Algorithms. Addison-Wesley (1992)

[40] Lange, K. : Unambiguity of circuits, Theoretical Computer Science 107, 77-94 (1993)

[41] Keller, J., Kessler, C., Tr¨aﬀ, J. : Practical PRAM programming. WileyInterscience,

J. Wiley & Sons, Inc. (2001).

[42] Goyal, N., Saks, M., and Venkatesh, S. : Optimal separation of EROW and CROW
PRAMs, 18th IEEE Annual Conference on Computational Complexity, 2003. Pro-
ceedings, pp. 93-104, (2003)

[43] Kessler, C., Keller, J. : Models for parallel computing: Review and perspectives.
Mitteilungen-Gesellschaft f¨ur Informatik eV, Parallel-Algorithmen und Rechner-
strukturen, 24, 13-29.(2007)

[44] Osterloh, A., Keller, J. : Das GCA-Modell im Vergleich zum PRAM-Modell. Re-
port Technische Universit¨at Darmstadt, FernUniversit¨at in Hagen, Fachbereich In-
formatik (2009)

Parallel Pointer Machines

[45] Tromp, J., van Emde Boas, P. : Associative Storage Modiﬁcation Machines. (1985)

[46] Lam, T. W., Ruzzo, W. L. : The power of parallel pointer manipulation. In Proceed-
ings of the ﬁrst annual ACM symposium on Parallel algorithms and architectures,
pp. 92-102 (1989, March)

[47] Cook, S. A., Dymond, P. W. : Parallel pointer machines. Computational Complexity,

3(1), 19-30. (1993)

[48] Niedermeier, R. : Towards realistic and simple models of parallel computation. Doc-

toral dissertation, University of T¨ubingen, Germany. (1996)

[49] Ben-Amram, A. M. : Pointer machines and pointer algorithms: an annotated bibli-

ography. Datalogisk Institut, Københavns Universitet. (1995)

REFERENCES

83

[50] Petersen, H.

: A Note on Kolmogorov-Uspensky Machines. arXiv preprint

arXiv:1211.5544. (2012)

Random Boolean Networks

[51] Kauﬀman, S. A. : Metabolic stability and epigenesis in randomly constructed genetic

nets. Journal of Theoretical Biology , 22:437–467. (1969)

[52] Kauﬀman, S. A. : The Origins of Order . Oxford University Press (1993)

[53] Derrida, B., Pomeau, Y. : Random Networks of Automata: A Simple Annealed

Approximation. Europhys. Lett. 1(2), 45-49 (1986)

[54] Luque, B., and Ferrera, A. : Measuring mutual information in random Boolean

networks. arXiv preprint adap-org/9909004 (1999)

[55] Shmulevich, I., Dougherty, E. R., and Zhang, W. : From Boolean to probabilistic
Boolean networks as models of genetic regulatory networks. Proceedings of the IEEE,
90(11), 1778-1792 (2002)

[56] Gershenson, C.

:

Introduction to

random Boolean networks. preprint

arXiv:nlin/0408006 (2004).

[57] Serra, R., Villani, M., Damiani, C., Graudenzi, A., Colacci, A., and Kauﬀman, S.
A. : Interacting random boolean networks. In Proceedings of ECCS07: European
Conference on Complex Systems (pp. 1-15) 2007, October).

[58] Bornholdt, S. : Boolean network models of cellular regulation: prospects and limi-

tations. Journal of the Royal Society Interface, 5(suppl 1), 85-94 (2008)

[59] Wang, R. S., Saadatpour, A., and Albert, R. : Boolean modeling in systems biology:
an overview of methodology and applications. Physical biology 9(5), 055001. (2012)

[60] Schwab, Julian D., et al. : Concepts in Boolean network modeling: What do they
all mean?. Computational and structural biotechnology journal 18: 571-582. (2020)

Firing Squad Synchronization Problem

[61] Moore, F. R.; Langdon, G. G. : A generalized ﬁring squad problem. Information and

Control, 12 (3): 212–220 (1968)

[62] Mazoyer, Jacques : A six-state minimal time solution to the ﬁring squad synchro-

nization problem. Theoretical Computer Science, 50 (2): 183-238 (1987)

[63] Umeo, Hiroshi : Firing Squad Synchronization Algorithms for Two-Dimensional

Cellular Automata. Journal of Cellular Automata, 4(1) (2009).

[64] Wikipedia: Firing squad synchronization problem. (2022)

https://en.wikipedia.org/wiki/Firing squad synchronization problem

