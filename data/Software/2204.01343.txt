2
2
0
2

r
p
A
4

]
E
S
.
s
c
[

1
v
3
4
3
1
0
.
4
0
2
2
:
v
i
X
r
a

SEAByTE: A Self-adaptive Micro-service System Artifact for
Automating A/B Testing

Federico Quin
Katholieke Universiteit Leuven
Leuven, Belgium
federico.quin@kuleuven.be

Danny Weyns
Katholieke Universiteit Leuven, Belgium
Linnaeus University, Sweden
danny.weyns@kuleuven.be

ABSTRACT
Micro-services are a common architectural approach to software de-
velopment today. An indispensable tool for evolving micro-service
systems is A/B testing. In A/B testing, two variants, A and B, are
applied in an experimental setting. By measuring the outcome of an
evaluation criterion, developers can make evidence-based decisions
to guide the evolution of their software. Recent studies highlight the
need for enhancing the automation when such experiments are con-
ducted in iterations. To that end, we contribute a novel artifact that
aims at enhancing the automation of an experimentation pipeline of
a micro-service system relying on the principles of self-adaptation.
Concretely, we propose SEAByTE, an experimental framework for
testing novel self-adaptation solutions to enhance the automation
of continuous A/B testing of a micro-service based system. We
illustrate the use of the SEAByTE artifact with a concrete example.

ACM Reference Format:
Federico Quin and Danny Weyns. 2022. SEAByTE: A Self-adaptive Micro-
service System Artifact for Automating A/B Testing. In Proceedings of The
17th Symposium on Software Engineering for Adaptive and Self-Managing
Systems (SEAMS 2022). ACM, New York, NY, USA, 7 pages. https://doi.org/
10.1145/3524844.3528081

1 INTRODUCTION
Micro-services are nowadays a commonly used architectural ap-
proach to software development [1]. A micro-service architecture
comprises small independent services that communicate over well-
defined APIs. These services are usually owned by small, self-
contained teams. Since each service performs a single function
and runs independently, services can be easily updated, deployed,
and scaled to meet changing demands. As such, a micro-service ar-
chitecture naturally supports continuous deployment (CD) [2]. CD
is based on the principles of agile development [3] and DevOps [4]
and leverages on continuous integration (CI) [5] that automates
tasks such as compiling code, running tests, and building deploy-
ment packages. Among the benefits of CI/CD are rapid innovation,
shorter time-to-market, increased customer satisfaction, continuous
feedback, and improved developer productivity.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9305-8/22/05. . . $15.00
https://doi.org/10.1145/3524844.3528081

One of the key concerns of CI/CD is increasing the agility of
development teams in terms of testing, updating, maintaining and
deploying software. Understanding user engagement and satisfac-
tion of product features or variants plays an important role in this.
A/B testing, also called bucket testing or controlled experimenta-
tion [6, 7], offers a solution to this concern: comparing two variants,
A and B, and test if the statistical distribution of a property of A is
different from that of B [8]. By focusing on different properties in
an experimental setting and systematically measuring the outcome,
developers can make evidence-based decisions to guide the soft-
ware evolution. This is especially useful for (micro-)service based
software with large user volumes [7]. Conducting experiments in
iterations is known as continuous experimentation (CE) [9]. Recent
studies highlight the need for enhanced automation of continuous
experimentation as one of the key challenges in this area [7, 10].

In this paper, we contribute a novel artifact that aims at en-
hancing the automation of continuous experimentation of a micro-
service system using the principles of self-adaptation. In particular,
we propose SEAByTE, an experimental framework that can be used
for testing novel self-adaptation solutions to enhance the automa-
tion of continuous A/B testing of a micro-service based system.

The remainder of this paper is structured as follows. In Section 2,
we summarize the principles of micro-services and A/B testing and
position SEAByTE in the landscape of self-adaptive system artifacts.
Section 3 presents the artifact with scenarios. Section 4 illustrates
the application of the artifact for one of the scenarios. Section 5
discusses the applicability of the artifact, Section 6 discusses future
research directions, and Section 7 wraps up.

2 BACKGROUND AND POSITIONING OF THE

ARTIFACT

We briefly introduce the basics of micro-services and A/B testing,
and then highlight how SEAByTE complements the existing arti-
facts for engineering self-adaptive systems.

2.1 Micro-services
A micro-service is a small independent piece of software that per-
forms a single function (i.e., a business capability) and communi-
cates with other micro-services via well-defined interfaces [11].
Each service in a micro-services architecture can be developed, de-
ployed, operated, and scaled independently, without affecting other
services. Services do not need to share any code with other services,
hence they promote decentralized governance. When a change of
a certain part of a micro-service based application is needed, only
the related service(s) can be modified and redeployed without the
need to modify and redeploy the entire application.

 
 
 
 
 
 
SEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

Federico Quin and Danny Weyns

Key players that migrated towards a micro-service architecture
include Amazon, Netflix, eBay, and Twitter [12, 13]. Yet, micro-
service architecture also comes with challenges, such as determin-
ing the right size of micro-services, front-end integration, and fail-
ure management (need for self-healing) [14].

2.2 A/B Testing
A/B testing is a systematic approach to compare the use of two
versions of a system and determining which of the two variants
is preferred according to some criterion. More specifically, an A/B
test consists of a randomized controlled experiment where experi-
mental units (e.g., users) are assigned to one of two variants, called
A and B, that are expected to influence some metric of interest.
This metric, the overall evaluation criterion, provides a quantitative
measure of the experiment‘s objective (in the form of a hypothe-
sis). To compare the two variants, statistical hypothesis testing is
commonly used. The choice of the tests (and its power) depends on
the (assumed) distribution of the data. For instance, for Gaussian
distributed data the unpaired t-test can be used. If the distribution
is unknown, the Mann–Whitney U test can be used. A key aspect
is the randomization applied to experimental units to map them to
variants. Proper randomization is important to ensure that the pop-
ulations assigned to the different variants are statistically similar
such that causal effects can be determined with high probability.
Prerequisites for A/B testing are: (1) experimental units can be
assigned to different variants with no (or little) interference; (2)
there are enough experimental units (e.g., users), usually thousands,
(3) the key metrics are agreed upon and can be practically evalu-
ated, (4) experimental units can be easily assigned to variants (e.g.,
server-side software is much easier to change than client-side). Con-
tinuous experimentation refers to pipelines of experiments. Often,
the analysis results may trigger the need for additional (analysis)
and follow-up experiments. Setting up such pipelines is a challenge
for current experimentation platforms, see e.g., [15].

A/B testing has been extensively used, ranging from user inter-
face element testing, testing product pricing, evaluating personal-
ized recommendations, testing product features, and more generally
evaluating the impact of changes made to software products and
services. Online controlled experiments are today common practice
and heavily used in practice [15].

Multi-variant testing is similar to A/B testing, but the tests then
use more than two versions at the same time. Yet, the current focus
of SEAByTE is on two-variant testing (A/B).

2.3 Positioning of the Artifact
At the time of writing, the community of engineering self-adaptive
systems has produced 27 artifacts. According to the SEAMS artifacts
website1,2 these artifacts provide “examples, challenge problems,
and solutions that the community can use to motivate research,
exhibit and evaluate solutions and techniques, and compare results.”
Of the 27 artifacts, only three have a specific focus on service-
based systems: Znn.com, TAS, and SWIM. Znn.com [16] is centered
on a webserver system that provides a simplified news site. The

1https://www.hpi.uni-potsdam.de/giese/public/selfadapt/exemplars/
2ZENODO: https://zenodo.org/communities/seams/?page=1&size=20

testing environment simulates the slash-dot effect which are peri-
ods of abnormally high traffic that overload the system. TAS [17] is
an exemplar of a service-based system (SBS). SBSs are widely used
in e-commerce, online banking, e-health and many other applica-
tions. In these systems, services offered by third-party providers
are dynamically composed into workflows delivering complex func-
tionality. SBSs increasingly rely on self-adaptation to cope with the
uncertainties associated with third-party services, as the loose cou-
pling of services makes online reconfiguration feasible. SWIM [18]
is an exemplar for the evaluation and comparison of self-adaptation
approaches for Web applications. The exemplar simulates a web
application (simulating a 60-server cluster subject to millions of
requests) that can be used as a target system with an external adap-
tation manager interacting with it through its TCP-based interface.
While the existing artifacts produced by the community aim
at supporting research on novel approaches for engineering self-
adaptive systems, SEAByTE takes a different angle and provides an
artifact that aims at supporting research on novel approaches for
a key task of software engineers using self-adaptation, in particular
enhancing the automation of evolution with A/B testing. This aligns
with recent initiatives of other researchers such as Self-Adaptation
2.0 [19] that argues for an equal-to-equal relationship between
self-adaptation and AI, benefiting one another. Another example
is discussed in [20] where self-adaptation is applied to deal with
degraded machine-learning components to maintain system utility.

3 SEABYTE
SEAByTE provides an Internet Web Store composed of multiple
micro-services. Figure 1 shows the architecture of the Web Store of
SEAByTE. The Web Store serves end-users that perform purchases
via the website of the store. When an end-user invokes a request,
the Webapp service will authenticate the user, start a session, re-
trieve the price of the listed product, and update the inventory.
Then the checkout service will be invoked that gives an overview
of the products present in the basket of the user, provides recom-
mendations for the user and finally adds the purchase to the history
of the user. This closes the session. Practically, the web-services are
implemented using the Java Spring framework, with each service
running in a separate Docker container. The implementation of the
Web Store artifact, an installation and usage guide, and a concrete
example are available at the SEAByTE website [21].

3.1 Experimental Pipeline
SEAByTE aims to enhance the automation of the evolution of the
micro-service system using a series of A/B tests. Central to this
automation is an experimental pipeline that comprises two basic
elements: experiments and transition rules. The artifact provides
templates to define both experimental pipelines and transition rules.

3.1.1 Experiments. An experiment comprises:

• ID: identifier experiment
• variantA: the first variant
• variantB: the second variant
• userProfile: the profile specifying end-user behaviors
• ABAssignment: mapping of end-users to variants
• samples: the number of samples used for the test
• metrics: the set of metrics

SEAByTE: A Self-adaptive Micro-service System Artifact for Automating A/B Testing

SEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

test (accept or reject the hypothesis) is recorded in the resulting
variable.

3.1.2 Transition Rules. A transition rule comprises:

• ID: identifier of the rule
• fromExperiment: identifier current experiment
• toExperiment: identifier of the next experiment
• conditions: the conditions to make a transition from the cur-
rent experiment to the next experiment (that depends on the
result of the current experiment).

to-experiment = "end" is a reserved value that indicates the end
of an experimental pipeline and returns the control to the operator.
Empty conditions indicate that the transition from the current to
the next experiment is taken unconditionally.

Listing 2 shows an example of the specification of a transition

rule for the artifact.

{

" Performance OK " : {

" fromExperiment ": " Upgrade v1 .0.0 - v1 .1.0 " ,
" toExperiment ": " Clicks v1 .0.0 - v1 .1.0 " ,
" conditions ": [{

" leftOperand ": " result - wt - test " ,
" operator ": " != " ,
" rightOperand ": " reject "

}]

}

}

Listing 2: Example specification of a transition rule.

This transition rule is applied once the Upgrade v1.0.0 - v1.1.0
experiment is finished. It checks whether the result of the test was
accepted, and if so, it starts the Clicks v1.0.0 - v1.1.0 experiment.

3.1.3 Experimental Pipeline. Finally, an experimental pipeline com-
prises the following:

• setup: identifier of the setup for the pipeline4
• start: the identifier of the experiment that starts the pipeline
• experiments: the set of experiment identifiers
• rules: a set of transition rule identifiers

Figure 2 shows a graphical representation of an example of an
experimental pipeline, and Listing 3 shows how this pipeline is
specified for the artifact.

The experiment starts with preparing the Web Store according
to the specified setup. Then, the Upgrade v1.0.0 - v1.1.0 experiment
that checks the performance overhead in response time that is
generated by the upgrade of the Web Store to version v1.1.0 is exe-
cuted. If the performance is acceptable (i.e., version 1.1.0 of the Web
Store generates no significant overhead in response time compared
to version 1.0.0), the transition via the rule “Performance OK” is
taken and the pipeline starts the Clicks v1.0.0 - v1.1.0 experiment.
Otherwise, the transition via the rule “Performance Overhead” is
taken and the execution of the pipeline ends. The Clicks experi-
ment checks whether users perform a significant number of extra
clicks on recommendations for version 1.1.0 compared to version
1.0.0. If this is not the case, the transition “No Extra Clicks” is taken
and the execution of the pipeline ends. Otherwise, the Purchases
v1.0.0 - v1.1.0 experiment is started via the “Extra Clicks” rule. This

4A setup comprises the necessary steps to be taken to get the managed system ready
for conducting experiments (further explained in Section 3.2.2).

Figure 1: Architecture of the Web Store Application.

• statisticalTest

– hypothesis: the hypothesis of the experiment
– pValue: the p-value for the test3
– type: the type of statistical test of the experiment
– result: the result variable of the experiment

Listing 1 shows an example of an experiment with ID “Upgrade

v1.0.0 - v1.1.0” for the artifact.

{

" Upgrade v1 .0.0 - v1 .1.0 ": {

" variantA ": "ws - recommendation - service :1.0.0 " ,
" variantB ": "ws - recommendation - service :1.1.0 " ,
" userProfile ": " Standard " ,
" ABAssignment ": {
" weightA ": 50 ,
" weightB ": 50

},
" samples ": 20000 ,
" metrics ": [" ResponseTime_A " , " ResponseTime_B "],
" statisticalTest ": {

" hypothesis ": " ResponseTime_A == ResponseTime_B " ,
" pValue ": 0.025 ,
" type ": " welsh 's t - test " ,
" resultingVariable ": " result -wt - test "

}

}

}

Listing 1: Example specification of experiment in SEAByTE.

This experiment compares the performance of two variants of the
Web Store that implement different versions of the recommendation
service, see Figure 3. Version 1.0.0 of the recommendation service
uses the history to provide recommendations to the user, while
version 1.1.0 exploits also information of the current purchase to
provide recommendations. The experiment uses a user-profile that
generates user requests. The requests are randomly assigned to
the two versions, 50% each. The experiment tracks the requests
with their response-time. The statistical test compares the mean
values of the response times of the invocations of both versions
using a Welsh’s t-test [22]. The artifact uses the Apache Commons
Mathematics Library for statistical testing [23]. The result of the

3The specified p-value decides, after collecting the specified number of samples for
both variants, if the test can be rejected based on the observed p-value. The result if
the test will either be ’reject’ or ’inconclusive’.

Basket µService           Web StoreCheckout µServiceRecommendation µServicePricing µServiceWebapp µServiceAuthentication µServiceInventory µServiceSession µServiceEnd-userHistory µServiceBasket µServiceSEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

Federico Quin and Danny Weyns

{

" user - profile - regular " : {

" Standard ": {

" count ": 1000 ,
" mean - seconds - between - request " : 15 ,
" probability - purchase ": 0.25 ,
" recommendation - click - probability ": 0 .2 ,
" recommendation - purchase - probability ": 0.05 ,
" bonus - recommendation - click -B": 0.1 ,
" bonus - recommendation - purchase -B ": 0 .0 5

},
" frivolous ": {

...

}

}

}

Figure 2: Example of experimental pipeline.

Listing 4: Example specification of a user profile.

last experiment compares the purchase behavior of the user with
both variants and reports the result to the operator, completing the
execution of the experimental pipeline. Based on the results the
stakeholders can then decide whether or not to upgrade the Web
Store, or set up additional experiments if needed.

{

" setup ": " Recommendation_upgrade " ,
" start ": " Upgrade v1 .0.0 - v1 .1.0 " ,
" experiments ": [

" Upgrade v1 .0.0 - v1 .1.0 " ,
" Clicks v1 .0.0 - v1 .1.0 " ,
" Purchases v1 .0.0 - v1 .1.0 "

],
" rules ": [

" Performance OK " ,
" Performance Overhead " ,
" Extra Clicks " ,
" No Extra Clicks "

]

}

Listing 3: Example specification of experimental pipeline.

3.2 Architecture SEAByTE
SEAByTE adds a MAPE-K based feedback loop [24, 25] on top of the
Web Store application that supports the automatic execution of an
experimental pipeline. Figure 3 shows the architecture of SEAByTE
illustrated for a scenario with two variants of the Recommendation
service, A and B. We explain the main components now.

3.2.1 User Profile. A user profile represents end-users of the Web
Store; it defines the behavior of the users. Behavior refers to the
actions taken by users and includes both the invocation of requests
and feedback provided by users based on the completed requests.
SEAByTE provides a template to specify user profiles. Listing 4
shows an example instance of the profile template.

The example shows the profile for a standard user that specifies
different aspects of the behavior of the users when visiting the web-
site of the store: purchasing goods, browsing the store, exploring
recommendations, and their effect. A profile can include different
types of users. User profiles can be derived from different sources
such as domain knowledge, historical data, or based on tests. The
request originator component exploits the user profile to generate
the requests for the web store according to the specified parameters
in the profile.

3.2.2 A/B Component. The A/B component determines the test
setup with A and B variants and manages the routing of invocations
to the two variants. The A/B test setting will be configured before
the execution of the experimental pipeline starts (via the dashboard,
see below). The concrete routing to the A and B variants will be
configured for each concrete experiment (the responsibility of the
effector, see below).

3.2.3 Probe and Effector. The probe is responsible for collecting
data of experiments, while the effector is responsible for setting up
the web store and managing the AB component. Listing 5 shows a
basic API of the probe and effector.

Probe

+ List < URLRequest > getRequestHistory ( String ABName ,

String variant )

Effector

+ void clearABComponentHistory ( String ABName )
+ void setABRouting ( String ABName , int a , int b)
+ void deploySetup ( String setupName )
+ void removeSetup ( String setupName )

Listing 5: The provided API for the probe and effector.

The probe provides a method to collect the history of the requests
of the active experiment of the service invocations per variant. A
request contains information about the response time, the requested
URL and the client ID. The effector enables setting up an experiment,
including configuring the A and B setting and the routing for the
variants, and clearing the history of the AB component.

Feedback Loop. The feedback loop is responsible for execut-
3.2.4
ing an experimental pipeline, see Figure 3. The knowledge com-
prises a specification of the pipeline with the experiments and transi-
tion rules, the configuration of the current experiment, a repository
to store data of the current experiment, and the experiment results.
The monitor collects the data of the running experiment and up-
dates the knowledge repository accordingly. When the experiment
is completed, the analyser applies the statistical test and writes the
experiment result to the knowledge. The planner then applies the
transition rules to the result of the last experiment and determines
the next step in the experimental pipeline. Based on that, the execu-
tor initiates the next experiment, or alternatively, the experimental
pipeline ends. The artifact provides abstract implementations of
the feedback loop elements with an example.

Upgradev1.0.0 - v1.1.0Clicks  v1.0.0 - v1.1.0Purchases v1.0.0 - V1.1.0Performance OKNo Extra  ClicksPerformance OverheadExtra ClicksSEAByTE: A Self-adaptive Micro-service System Artifact for Automating A/B Testing

SEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

Figure 3: Architecture of SEAByTE.

Table 1: Test scenarios for SEAByTE

ID Name

Description

Metric

S1 Basic service upgrade
S2 Advanced service

upgrade
Setting product price

S3

S4 Basic segmentation

S5 Advanced

segmentation

Experimental pipeline with two versions of a single micro-service Response time of service invocations
Experimental pipeline with two versions of a single micro-service Response time of service invocations

Experimental pipeline that determines the right price for a product
with the aim to maximize the revenue
Experimental pipeline that determines a segmentation strategy for
two variants based on the age of end-users
Experimental pipeline that determines a segmentation strategy for
two variants based on the age and geographic location of end-users

and user behavior
Appreciation of the price of
purchases by the users
Preferences of variants using user
feedback
Preferences of variants using user
feedback

3.2.5 Dashboard. The dashboard gives the operator access to the
test environment, enabling: (1) configuration of the Web Store to
run an experimental pipeline, (2) loading and starting an experi-
mental pipeline, (3) monitoring the execution of an experimental

pipeline, and (4) showing the results. Figure 4 shows an excerpt
of the dashboard to monitor the progress of the execution of an
experimental pipeline. The box plot at the bottom shows a live
representation of the response times for versions A and B.

Web StoreBasket µServicePricing µServiceHistory µServiceRecommendation µService 1.0.0Recommendation µService 1.1.0ABA/B ComponentWebapp µServiceAuthentication µServiceInventory µServiceSession µServiceFeedback loopDashboardupdate data current experiment collect data current experimentMonitorread data current experimentwrite experiment  resultsAnalyzerPlannerinitiate experimentExecutorKnowledgeread transitionrulestransition next stepProbeEffectorUser profileOperatorData currentexperimentExperimental pipelineExperimentsCurrent  Experimentload & start the experimental pipelineshow experiment resultsconfigure & start  experimentconfigure Web StoreExperiment resultsTransition rulesCheckout µServiceRequestOriginatorSEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

Federico Quin and Danny Weyns

5 ON THE APPLICABILITY OF SEABYTE
SEAByTE targets the automation of continuous experimentation of
micro-service systems. The artifact aims for a high level of reality of
a practical micro-services system, yet, several aspects of real world
systems are emulated, including user behavior and system load. The
artifact supports A/B testing for both functional and non-functional
aspects. It also supports human involvement in the form of human
feedback, which is a crucial aspect of A/B testing in practice.

SEAByTE comes with five scenarios; the code and specifications
of two scenarios is shipped with the artifact. For the other scenarios,
the artifact provides a basic implementation. Yet, the artifact does
not exclude research on new scenarios.

It is important to balance the design of an experimental pipeline.
Automatically running multiple experiments may result in negative
experience for some users [26]. Furthermore, it may accumulate er-
rors of statistical tests, which needs to be considered when defining
p-values for subsequent tests.

6 FUTURE RESEARCH DIRECTIONS
The current version of SEAByTE supports the basics for setting up
experimental pipelines for A/B testing using a feedback loop. We
highlight several opportunities for future research beyond these
basics. A first opportunity is to incorporate user feedback into the
experimental pipeline, akin to one of the test scenarios presented
in Table 1. A second opportunity is to identify classes of users to
target A/B tests for specific subgroups. Here self-adaptation can
be combined with machine learning to identify such classes and
determine which class users belong to [27, 28]. Incorporating user
feedback in this setting can also play a crucial role in developing a
suitable adaptation strategy. A third opportunity for future research
could be the automatic identification of A/B experiments that can
be setup and run in the application. This can range from trying out
different GUI layouts to testing different algorithms (similar to the
recommendation algorithm tested in Section 3). A fourth and last
opportunity is supporting multiple experiments simultaneously.
Since running multiple A/B experiments can be a risk (experiments
may affect each other), care has to be taken. Self-adaptation is a
perfect candidate to deal with such concerns. Self-adaptation can
also play a crucial role for dealing with a large space of potential
experiments that need to run. Self-adaptation can further help in
identifying the experiments that are most likely to have a high
impact on the overall business goals of the system, and determining
in which order to run the experiments.

7 CONCLUSIONS
We presented SEAByTE, a novel artifact
that applies self-
adaptation [29] to enhance the automation of A/B testing to support
the evolution of micro-service systems. In contrast to existing arti-
facts that target novel approaches to engineer self-adaptive systems,
SEAByTE exploits self-adaptation as a means to solve a key task of
software engineers of service-based systems: automating continu-
ous experimentation. We hope that the research community will
use the SEAByTE to evaluate research advances in the application
of self-adaptation to support software engineers of micro-service
systems. The artifact is available via the project website [21].

Figure 4: Excerpt of the dashboard of SEAByTE.

3.3 Test Scenarios
Table 1 shows the test scenarios supported by SEAByTE. Each
scenario introduces increasingly difficult challenges. The challenges
focus on three key aspects of A/B testing: system upgrades (S1 and
S2), price setting (S3), and segmentation (S4 and S5). The artifact
provides a full specification for concrete instances of scenarios S1
and S2, and a basic implementation supporting the other scenarios.

4 EXPERIMENTATION WITH THE ARTIFACT
4.1 Workflow to use the artifact
Using the artifact comprises the following steps:

• Specify the experimental pipeline with the experiments and

transition rules

• Configure the Web Store with the variants to be tested
• Configure the feedback loop to prepare the execution of the

experimental pipeline

• Load the experimental pipeline via the dashboard
• Deploy the Web Store configuration and the feedback loop

via the dashboard, and start the experimental pipeline

• Collect the experiment results and make a decision

4.2 Results
We applied the steps above for the configuration with two variants
of the recommendation service shown in Figure 3. We applied the
experimental pipeline shown in Figure 2 using the specifications of
the different elements illustrated above.

We conducted the “Upgrade v1.0.0 - v1.1.0” experiment that
compared the distributions of the response time of the two variants
for 20k samples. After the 20k samples were collected, we observed
that the null hypothesis (response time of v1.1.0 similar to response
time v1.0.0) could not be rejected. The pipeline thus continues with
the next experiment: “Clicks v1.0.0 - v1.1.0”. For more details and
additional results, we refer to the project website [21].

SEAByTE: A Self-adaptive Micro-service System Artifact for Automating A/B Testing

SEAMS 2022, May 21–29, 2022, Pittsburgh, PA, USA

REFERENCES
[1] P. D. Francesco, I. Malavolta, and P. Lago, “Research on architecting microser-
vices: Trends, focus, and potential for industrial adoption,” in IEEE International
Conference on Software Architecture, 2017.

[2] J. Järvinen, T. Huomo, T. Mikkonen, and P. Tyrväinen, “From agile software
development to mercury business,” in Software Business. Towards Continuous
Value Delivery, Springer, 2014.

[3] T. Dingsøyr, S. Nerur, V. Balijepally, and N. Moe, “A decade of agile methodologies:
Towards explaining agile software development,” Journal of Systems and Software,
vol. 85, no. 6, pp. 1213–1221, 2012.

[4] A. Mishra and Z. Otaiwi, “Devops and software quality: A systematic mapping,”

Computer Science Review, vol. 38, p. 100308, 2020.

[5] M. Meyer, “Continuous integration and its tools,” IEEE Software, vol. 31, pp. 14–16,

may 2014.

[6] R. Kohavi, R. M. Henne, and D. Sommerfield, “Practical guide to controlled
experiments on the web: Listen to your customers not to the hippo,” in 13th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
p. 959–967, 2007.

[7] R. Ros and P. Runeson, “ion and a/b testing: A mapping studycontinuous experi-
mentat,” in 4th International Workshop on Rapid Continuous Software Engineering,
p. 35–41, ACM, 2018.

[8] R. Kohavi and R. Longbotham, Online Controlled Experiments and A/B Testing,

pp. 922–929. Boston, MA: Springer, 2017.

[9] F. Fagerholm, A. Sanchez Guinea, H. Mäenpää, and J. Münch, “The right model for
continuous experimentation,” Journal of Systems and Software, vol. 123, pp. 292–
305, 2017.

[10] P. Rodríguez et al., “Continuous deployment of software intensive products and
services: A systematic mapping study,” Journal of Systems and Software, vol. 123,
pp. 263–291, 2017.

[11] N. Dragoni, S. Giallorenzo, A. L. Lafuente, M. Mazzara, F. Montesi, R. Mustafin,
and L. Safina, Microservices: Yesterday, Today, and Tomorrow, pp. 195–216. Cham:
Springer International Publishing, 2017.

[12] J. Bogner, J. Fritzsch, S. Wagner, and A. Zimmermann, “Microservices in industry:
Insights into technologies, characteristics, and software quality,” in Companion
Int. Conference on Software Architecture Companion, 2019.

[13] J. Bogner, J. Fritzsch, and S. e. a. Wagner, “Industry practices and challenges
for the evolvability assurance of microservices,” Empirical Software Engineering,
vol. 26, no. 104, pp. 24–35, 2021.

[14] P. Jamshidi, C. Pahl, N. C. Mendonça, J. Lewis, and S. Tilkov, “Microservices: The
journey so far and challenges ahead,” IEEE Software, vol. 35, no. 3, 2018.

[15] S. Gupta et al., “Top challenges from the first practical online controlled experi-

ments summit,” SIGKDD Explor. Newsl., vol. 21, p. 20–35, may 2019.

[16] S.-W. Cheng, “Rainbow: Cost-effective software architecture-based self-
adaptation,” in PhD Thesis Carnegie Mellon University, CMU-ISR-08-113, 2008.
[17] D. Weyns and R. Calinescu, “Tele assistance: A self-adaptive service-based system
exemplar,” in 10th International Symposium on Software Engineering for Adaptive
and Self-Managing Systems, 2015.

[18] G. A. Moreno, B. Schmerl, and D. Garlan, “Swim: An exemplar for evaluation
and comparison of self-adaptation approaches for web applications,” in 13th
International Symposium on Software Engineering for Adaptive and Self-Managing
Systems, 2018.

[19] T. Bureš, “Self-adaptation 2.0,” in 2021 International Symposium on Software Engi-
neering for Adaptive and Self-Managing Systems (SEAMS), pp. 262–263, 2021.
[20] M. Casimiro, P. Romano, D. Garlan, G. A. Moreno, E. Kang, and M. Klein, “Self-
adaptation for machine learning based systems,” vol. 2978 of CEUR Workshop
Proceedings, 2021.

[21] F. Quin and D. Weyns, “SEAByTE Website.” https://people.cs.kuleuven.be/danny.

weyns/software/SEAByTE/.

[22] B. L. Welch, “The Generalisation OF ‘Student’s’ Problem when Several Different
Population. Varlances are Involved,” Biometrika, vol. 34, no. 1-2, pp. 28–35, 1947.
[23] Commons-Math, “Apache commons mathematics library. https://commons.

apache.org/proper/commons-math/,” Last visited: 1/2022.

[24] J. Kephart and D. Chess, “The vision of autonomic computing,” Computer, vol. 36,

no. 1, pp. 41–50, 2003.

[25] D. Weyns, U. Iftikhar, and J. Soderland, “Do external feedback loops improve the
design of self-adaptive systems? a controlled experiment,” in Software Engineering
for Adaptive and Self-Managing Systems, IEEE, 2013.

[26] R. Kohavi, A. Deng, B. Frasca, T. Walker, Y. Xu, and N. Pohlmann, “Online con-
trolled experiments at large scale,” in 19th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, ACM, 2013.

[27] F. Quin, D. Weyns, T. Bamelis, S. S. Buttar, and S. Michiels, “Efficient analysis of
large adaptation spaces in self-adaptive systems using machine learning,” in 2019
IEEE/ACM 14th International Symposium on Software Engineering for Adaptive
and Self-Managing Systems (SEAMS), pp. 1–12, IEEE, 2019.

[28] O. Gheibi, D. Weyns, and F. Quin, “Applying machine learning in self-adaptive
systems: A systematic literature review,” ACM Transactions on Autonomous and
Adaptive Systems, vol. 15, 2020.

[29] D. Weyns, Introduction to Self-Adaptive Systems: A Contemporary Software Engi-

neering Perspective. Wiley, 2020.

