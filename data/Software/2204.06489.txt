2
2
0
2

r
p
A
3
1

]

A
N
.
h
t
a
m

[

1
v
9
8
4
6
0
.
4
0
2
2
:
v
i
X
r
a

Iterative PDE-constrained optimization for

seismic full-waveform inversion

Mikhail Malovichko1, Akbar Orazbayev1, and Nikolay Khokhlov1

1Moscow Institute of Physics and Technology

April 14, 2022

Abstract

This paper presents a novel numerical method for the Newton seismic
full-waveform inversion (FWI). The method is based on the full-space ap-
proach, where the state, adjoint state, and control variables are optimized
simultaneously. Each Newton step is formulated as a PDE-constrained
optimization problem, which is cast in the form of the Karush-Kuhn-
Tucker (KKT) system of linear algebraic equitations. The KKT system
is solved inexactly with a preconditioned Krylov solver. We introduced
two preconditioners: the one based on the block-triangular factorization
and its variant with an inexact block solver. The method was bench-
marked against the standard truncated Newton FWI scheme on a part
of the Marmousi velocity model. The algorithm demonstrated a consid-
erable runtime reduction compared to the standard FWI. Moreover, the
presented approach has a great potential for further acceleration. The
central result of this paper is that it establishes the feasibility of Newton-
type optimization of the KKT system in application to the seismic FWI.

1

Introduction

The seismic full-waveform inversion (FWI) is a coeﬃcient inverse problem aim-
ing to estimate the subsurface distribution of material properties based on
recorded seismic data. The FWI has emerged almost four decades ago [1, 2, 3],
and has evolved into a powerful method for reconstruction of the subsurface
properties, see reviews [4, 5].

1

 
 
 
 
 
 
The FWI is formulated as a Tikhonov-like minimization procedure,

minimize
γ

(cid:107)d − F(γ)(cid:107)2 + R(γ),

(1)

where γ is a function of material parameters (say, the squared P-wave slowness),
d is a vector of observed data, F is a nonlinear forward-problem operator, and
R is the stabilizing term. We are interested in the frequency-domain acoustic
FWI and, thus, operator F implies the solution of Helmholtz’s equation. Many
variants of basic formulation (1) exist: R can take various forms or even be
omitted, the residual can be weighted, and so on.

Optimization (1) is usually performed by either quasi-Newton or Newton
methods. The quasi-Newton methods dominate industrial applications due to
their relative numerical eﬃciency [6, 7, 8, 9, 10] (among many others). These
methods account for nonlinearity partially, usually by approximating the Hes-
sian with a diagonal matrix. Typical representatives of this class are the non-
linear conjugate gradient (NLCG) method, where a diagonal matrix scales the
gradients [11, 12], and L-BFGS [13]. Though computationally tractable, the
quasi-Newtonian methods may suﬀer from convergence stagnation.

The Newton methods achieve faster and more reliable convergence, which
catch up with the nonlinearity through quadratic approximation of the target
function, see [14, 15, 16]. The often-used Gauss-Newton variant computes the
Hessian matrix approximately by linearizing the forward-problem operator. The
main disadvantage of the Newton methods applied to (1) is an enormous compu-
tational burden stemming from the need to solve a large dense system of linear
equations at each Newton iteration. The system is solved iteratively, usually
with the conjugate gradients (CG). The matrix is ill-conditioned, and therefore
each Newton (external, nonlinear) iteration may require hundreds of internal
(linear) iterations. In addition, no eﬃcient preconditioner is known to improve
this matrix’s conditioning. For these reasons, numerical implementations of the
Newton FWI are far less common than the quasi-Newton ones.

It is natural to ask if we can obtain the fast Newton-like convergence with
a less resource-demanding procedure. The ultimate goal of this paper is to
demonstrate that the answer is yes.

Optimization (1) is known as the reduced-space approach in the optimization
community. Its alternative is the the full-space approach, also known as all-at-
once. In the reduced-space approach, only material parameters γ are optimized,
whereas the connection between the waveﬁeld in the medium and γ is prescribed
by the forward-problem operator F. In the full-space approach, one simultane-

2

ously optimizes the control, state, and adjoint-state variables. Upon formulating
the necessary conditions and discretizing the PDE, one obtains a nonlinear sys-
tem of algebraic equations, known as the Karush-Kuhn-Tucker (KKT) system,
which is sparse. Its sparsity is a clear advantage over formulation (1). The main
challenge here is that the problem becomes many times bigger than the reduced-
space approach because the state and adjoint-state variables for all sources are
included in optimization.

The full-space approach has been introduced to geophysics in the seminal
paper [17], followed by [18, 19]. Those authors cast the geophysical electromag-
netic inversion as a full-space optimization problem, derived the KKT system
and applied the Newton algorithm to it. Despite spectacular results, the full-
space approach had been considered impractical for years due to the enormous
size of the linear system, arising on each Newton step. Regarding FWI, the
KKT optimality conditions were derived in several papers [15, 16], but were
viewed as a theoretical tool rather than a basis for numerical implementation.
Signiﬁcant progress has been made in this direction during the last decade.
Papers [20, 21, 22] report the application of the penalty method to the FWI,
marking an important step toward the full-space approach. The penalty method
has some theoretical drawbacks connected to the choice of the penalty param-
eter [23] (but see recent developments [24]). The augmented Lagrangian (AL)
method is a viable alternative to the penalty method. A variant of it, known
as the alternating-direction method of multiplies (ADMM) [25], was applied to
the FWI with great success, see [26, 27]. In essence, the ADMM tackles the
full-space optimization by making alternating steps in the state, adjoint-state,
and control state spaces. Still, it is generally accepted that the main weakness
of the ADMM compared to the Newton iterations is its slow convergence [25].
This paper studies the full-space optimization based on applying a Newton
solver to the original KKT matrix. The crucial component of our approach is
a suitable preconditioner based on a block-triangular factorization of the per-
muted KKT matrix combined with approximations of individual blocks. Similar
ideas were exploited optimization with Maxwell’s equations [18] and the Stokes
equation [28]; thus, this paper extends this framework to Helmholtz’s equation.
We benchmarked our method against the standard reduced-space Gauss-Newton
FWI with the internal CG solver. In the numerical experiments conducted the
Marmousi model, we observed the 6× speedup. The method has the potential
for further acceleration. In particular, it will beneﬁt from any method designed
to approximate the reduced-space Hessian, such as L-BFGS. The central result
of this paper is that it establishes the feasibility of Newton-type optimization

3

of the KKT system in application to seismic FWI.

The paper is organized as follows. In Section 2, we formulate the forward
problem. The reduced-space Gauss-Newton inversion is considered in Section 3.
Sections 4 and 5 are dedicated to the full-space approach and its iterative so-
lution. Numerical experiments are presented in Section 6. Concluding remarks
are given in Section 7.

2 The forward problem

In this section, we brieﬂy review the forward problem. Let Ω ⊂ R2 be a bounded
domain with boundary ∂Ω. Consider the 2D Helmholtz’s equation in the fol-
lowing form,

−Z

(cid:19)

∂
∂x

(cid:18) 1
X

∂U
∂x

− X

(cid:19)

∂
∂z

(cid:18) 1
Z

∂U
∂z

− ω2γXZU = XZF, in Ω,

(2)

U = 0, on ∂Ω.

Here F (x, z) are the right-hand side, U (x, z, ω) are the acoustic ﬁeld (monochro-
matic pressure), ω is the circular frequency, γ(x, z) is the squared slowness,
γ = c−2, where c(x, z) is the speed of sound. Complex-valued functions of one
variable X(x) and Z(z) implement the perfectly-matched layers (PML). They
deviate from 1.0 only in the damping region on the sides of the computational
domain to suppress the side reﬂections, see Figure 1. On the PML implemen-
tation, see [29] and references therein.

Figure 1: Schematic representation of the computational domain with a PML
region. In the PML region X(x) (cid:54)= 1, Z(z) (cid:54)= 1.

4

Let us rewrite (2) with a more compact notation as follows,

DU = F in Ω,

U = 0, on ∂Ω.

where

D(·) := −Z

∂
∂x

1
X

∂
∂x

(·) − X

∂
∂z

1
Z

∂
∂z

(·) − ω2γXZ(·) .

(3)

(4)

Note that we replaced XZF with F in (3) because we assume that F is sup-
ported only in the core domain.

In a seismic survey, the acoustic ﬁeld is excited by many monochromatic

sources independent of each other. Thus, we have a series of problems,

DUk = Fk, in Ω,

Uk = 0, on ∂Ω,

(5)

where index k designates the source index. Let us combine solutions Uk and
right-hand sides Fk into vector variables to make the formulas even more com-
pact. Starting from now on, U is a direct product of all Uk, F is a direct product
of Fk:

K−1
(cid:89)

K−1
(cid:89)

U :=

Uk, F :=

Fk,

k = 0..K − 1.

With these notations, a set of K problems (5) can be compactly written as

k=0

k=0

DU = F, in Ω,

U = 0, on ∂Ω.

(6)

(7)

Now we introduce the observation operator Q which takes the acoustic ﬁeld and
returns a vector of data d. We deﬁne Q through the convolution with the Dirac
δ-function (although any linear functional can be used instead) as follows,

d = QU :=

(cid:26)(cid:90)

Ω

δ(xkj − x)Uk dx

,

k = 0, 1, ..K − 1,

j = 0, 1, ..Nk − 1 (8)

(cid:27)

where xkj is the position of the j-th receiver of the k-th source, Nk is the number
of receivers for the k-th source, d ∈ CN , N = (cid:80)K−1
k=0 Nk is the total number of
receivers for all sources.

In what follows, we will use the discrete form of the forward problem. The
ﬁnite-diﬀerence (FD) discretization is assumed because it is used in our numer-
ical experiments. However, we might as well have performed discretization by

5

the ﬁnite-element of spectral-element methods.

Let uk, fk, and s be ﬁnite-diﬀerence approximations of Uk, Fk, and γ, re-

spectively. We introduce the following notation,

u =

K−1
(cid:89)

k=0

uk,

f =

K−1
(cid:89)

k=0

fk,

k = 0..K − 1.

(9)

A set of discrete forward problems reads

Au = f,

(10)

where A is a block-diagonal matrix. Each diagonal block Aj of matrix A is a
sparse complex non-Hermitian matrix. If the PMLs are not used, then Aj turns
into the real symmetric indeﬁnite matrix,

− ∆ − ω2M,

(11)

where ∆ is a discrete Laplacian, M is a diagonal mass matrix, containing values
of s on its diagonal M = Is (it can be non-diagonal in some FD schemes [30]).

3 The reduced-space Gauss-Newton inversion

In this section, we review the reduced-space Gauss-Newton inversion (RSGN).
Since we will eventually end up with the FD approximation, and thus discrete
variables will be grid functions, it is easier to formulate the inverse problem in
the discrete form (the discretize-then-optimize approach). In the ﬁnite-element
framework, a more convenient way would be to formulate the optimality condi-
tions in function spaces and then move on to discretization (the optimize-then-
discretize approach), see [31].

Let S be a nonlinear operator that maps a given s to u,

u = S(s).

(12)

Let Q be a ﬁnite-dimensional operator that maps a given discrete acoustic ﬁeld
u to data d,

d = Qu.

(13)

Discrete coeﬃcient s is iterated as sn+1 = sn + δs , where δs , known as the
model update. On each iteration, s is a solution to the following minimization

6

problem,

minimize
δs

1
2

(cid:107)W (r − QG δs )(cid:107)2

2 +

ε
2

(cid:107)sn + δs (cid:107)2
2 .

(14)

Here W is a real-valued diagonal matrix of weights designed primarily to balance
signal attenuation with oﬀset from the source. Vector r is the data residual,

r = d − QS(sn),

G is a matrix of the Frech´et operator of S at point s = sn, e.i.

G =

∂S
∂s

(cid:12)
(cid:12)
(cid:12)
(cid:12)s=sn

.

Solution to (14) satisﬁes

(cid:0)G∗Q∗W T W QG + L(cid:1) δs = W T G∗Q∗r − Lsn,

where L = εI. The following formula deﬁnes matrix G,

G = A−1P,

where P is a diagonal matrix containing forward solution un at s = sn,

P := ω2Iun.

(15)

(16)

(17)

(18)

(19)

To verify (18),(19), one can subtract the two discrete Helmholtz’s equations

(cid:0)−∆ − ω2I (sn + δs )(cid:1) (un + δu) = f,
(cid:1) un = f,

(cid:0)−∆ − ω2I sn

(20)

although the derivation can be conducted directly in the Hilbert spaces.
geophysics, the matrix

In

is known as the Jacobian, the matrix

J := QA−1P,

H := J ∗W T W J + L

is the (regularized) Hessian, and vector

g := W T J ∗r − Lsn

7

(21)

(22)

(23)

is called the gradient. Using this deﬁnitions, we rewrite (17) as

H δs = g.

(24)

For any ε > 0 matrix H is positive-deﬁnite, and thus (24) has a unique
solution. It should be noted that, even if ε = 0 (singular H), then a Krylov solver
will deliver a meaningful estimate of δs , that is, a projection to a corresponding
Krylov subspace.

The system matrix in (24) is dense and large.

In real-life 3D problems,
the system (24) can be solved only iteratively, with the CG being a natural
choice. We will abbreviate this algorithm as RSGN-CG. The CG multiplies the
system matrix J ∗J by a vector once per iteration. Because of (21) it implies two
linear solves per CG iteration - the forward simulation with A and the adjoint
simulation with A∗. We can rewrite (24) as





K−1
(cid:88)

j=0



j W T
J ∗

j WjJj + L

 δs =

K−1
(cid:88)

j=0

W T

j J ∗

j rj − Lsn.

(25)

The forward and adjoint simulations for diﬀerent sources can be performed in
parallel but these results must be combined at each CG iteration.

However, the most critical problem is a very slow convergence of the CG.
The system matrix (24) has a very high condition number for all meaningful
values of ε. Thus, tens or hundreds of forward simulations (for all sources) may
be needed to generate a good update δs .
In addition, preconditioning of a
regularized normal system of linear equitations is notoriously diﬃcult.

4 The full-space Gauss-Newton inversion

In this section, we describe the full-space Gauss-Newton (FSGN) inversion. We
start by noting that the discrete optimization problem (14) is equivalent to the
following constrained optimization problem,

minimize
δs

Ψ :=

1
2

(cid:107)W (r − Q δu )(cid:107)2

2 +

ε
2

(cid:107)sn + δs (cid:107)2
2 ,

(26)

s.t. A δu = P δs .

Here n is the Newton iteration number, δu is the update to un, e.i. un+1 ≈ un +
δu . The target functional Ψ is quadratic and bounded away from zero for any
ε > 0. The constraints are linear. For such problems, known as linear-quadratic,

8

the existence of unique state and control variables are readily established, for
example, [32][Thm 1.43]. Moreover, since we start from the discrete formulation,
the solvability is proved simply by establishing the equivalence between the
matrix form and the normal system (24), see below.

Let us form the Lagrangian,

L := Ψ + λ∗(A δu − P δs ),

(27)

where λ is the adjoint-state variable (a complex-valued grid function). By ap-
plying the optimality conditions [32], we obtain the KKT system,

−Q∗W T W (r − Q δu ) + A∗λ = 0,
L(sn + δs ) − P ∗λ = 0,

A δu − P δs = 0.

This system can be rewritten in the matrix form,






(cid:124)

O

A∗
F
O L −P ∗
A −P
(cid:123)(cid:122)
M

O











(cid:125)

(cid:124)






(cid:125)

δu
δs
λ
(cid:123)(cid:122)
ξ

=






(cid:124)

Q∗W T W r
−Lsn
0
(cid:123)(cid:122)
b




 ,

(cid:125)

(28)

(29)

where F = Q∗W T W Q.

Lets us prove that (29) is equivalent to (24) for it establishes solvability of

the KKT system. Using the third equation of (29), we compute δu ,

δu = A−1P δs .

Using this result, we compute λ from the ﬁrst equation,

λ = A−∗ (cid:0)Q∗r − Q∗QA−1P δs (cid:1) ,

(30)

(31)

where notation A−∗ means (A∗)−1. Now we substitute (30) and (31) into the
second equation of (29) and get

(cid:0)P ∗A−∗Q∗QA−1P + L(cid:1) δs = P ∗A−∗Q∗r − Lsn.

(32)

From (21) we see that (32) is equivalent to (24).

From the preceding discussion, it is clear that the model update δs com-
puted from either the KKT system (29) or the normal system (24) is identical,

9

provided that the linear systems are solved exactly. The diﬀerence between the
two approaches lies in the structure of the system matrix. The system matrix in
(24) is complex-valued, dense, Hermitian positive-deﬁnite, and relatively small.
For small- to medium-size 3D problems with rigorously selected FD grids and
frequencies, satisfactory results have been obtained with the use of the sparse
LU decomposition of the system A [8, 33]. In general, however, a parallel on-
the-ﬂy application of A−1 and A−∗ is the only option. The system matrix in
(29) is sparse complex-valued Hermitian indeﬁnite. It is many times larger than
the normal matrix, and it precludes from applying direct solvers in any form.
Its iterative solution is considered in the next section.

5

Iterative solution of the KKT system

In this section, we design an iterative solver for the KKT system (29). The
system matrix M is can be explicitly partitioned across sources as follows,

















.















Q∗

0W T
0 W0r0
...



F0

. . .

A∗
0

. . .

O





































δu 0
...
δu K−1
δs
λ0
...
λK−1





























O

O

=

K−1

K−1

A0

Q∗

. . .

−P ∗

FK−1

AK−1

K−1W T

A∗
0 . . . − P ∗

L
−P0
...
−PK−1

K−1WK−1rK−1
−Lsn
0
...
0
(33)
Block F is a positive semideﬁnite matrix, which is very sparse. If receivers are
located at the grid nodes, then F is a diagonal matrix where the number of
non-zero entries in each Fj block equals the number of data points for the j-th
source. Block A is a block-diagonal matrix of full rank whose spectrum lies in
the left and right half-planes. Block L is positive-deﬁnite here but may become
positive semideﬁnite if L represents the Laplacian. In our case, L = εI and so
its diagonal. Block P is a rectangular matrix consisting of sparse blocks. All
submatrices of P are diagonal; see deﬁnition (19). The pattern of the matrix
M for a test problem consisting of 25 sources is presented in Figure 2.

Our preconditioner is based on a block-triangular decomposition, which is
one of the most eﬃcient strategies for preconditioning saddle-point problems.
In the context of the KKT matrix, this approach was considered in [18, 28].

The matrix M has singular (1,1) and (3,3) blocks, and so the direct block-

10

Figure 2: Partitioning (red lines) and non-zero elements (black dots) of the
KKT matrix for a test problem. The problem contains 25 sources, each system
matrix is 120,000 × 120,000. The size of the L block is 72,000 × 72,000.

LU decomposition is not possible. Thus, we perform block-LU factorization of
the permuted matrix as follows,






O
A
A∗
F
O −P ∗

−P
O
L






 =




I
F A−1
O

O
I
−P ∗A−∗

O
O
I











−P

A O
O A∗ F A−1P
O O

H




 ,

(34)
where H = L + P ∗A−∗F A−1P . The U -factor is an excellent preconditioner
since applying its inverse to the initial (permuted) matrix results in a matrix
whose spectrum consists of a single eigenvalue equal to 1. This approach would
be equivalent to the reduced-space method and has no advantages because it
requires a linear solve with the reduced Hessian H.

Let us approximate (34) by assuming F ≈ 0, which implies H ≈ L. Plugging

F = 0, H = L into (34), we get






O
I

I
O
O −P ∗A−∗

O
O
I











A O −P
O A∗ O
L
O O






 =




A
O
O A∗
O −P ∗

−P
O
L




 .

(35)

By reversing the permutation, we deﬁne the preconditioner to the original sys-

11

tem matrix M as follows,

P :=






A∗
O O
O L −P ∗
A −P

O




 .

(36)

The preconditioned matrix , P −1M is non-symmetric; therefore, a non-symmetric
Krylov solver is required to solve it. We selected the GMRes and abbreviated
the resulting method as FSGN-GMRes.

The preconditioner (36) has several appealing properties. First, it requires
three forward solves with matrices A, A∗, and L, e.i. almost the same work as
needed for a single iteration of the CG with a reduced Hessian. Second, the A
and A∗ both split into K independent linear systems. Thus, the solutions with
blocks (1,3) and (3,1) can be performed in parallel. Third, the preconditioner
performs approximation H ≈ L, but other approximations to H are possible.
In particular, one can use the L-BFGS approximation in place of block L, see
[28].

Now, we deﬁne another preconditioner, which is obtained from P by approx-

imating A by another matrix ˜A as follows,

˜P :=






˜A∗
O O
O L −P ∗
˜A −P

O




 .

(37)

In this paper, we utilized the incomplete LU (ILU) factorization to approximate
A. This choice is motivated by the fact that the ILU is widely available and
requires no tuning apart from selecting the ﬁll-in level.

We will demonstrate below that the FSGN-GMRes with the ˜P precondi-
tioner is superior to the RSGN-CG. Still, it is important to recognize that more
eﬃcient strategies to approximate A are likely to exist. In particular, the shifted-
Laplacian preconditioner leveraged with the multigrid [34] deserves a dedicated
study.

6 Numerical experiments

This section is dedicated to numerical experiments. The optimization part of
FWI was programmed in Python, whereas the forward and adjoint simulation
problem is coded in C++. The forward problem was solved directly with a
sparse LU factorization. All calculations were serial.

12

We conducted numerical experiments using a part of the Marmousi veloc-
ity model (Figure 3). This P-wave velocity distribution was used in the data
simulation, representing the geologic medium we want to reconstruct.

Figure 3: The distribution of the P-wave velocity, c, used to simulate synthetic
data. The numbers on the color scale are the velocity in km/s. The maroon
lines on the sides indicate the boundaries of the PML region.

The purpose of the ﬁrst experiment was to compare, inside a single GN
step, the performance of the CG applied to the normal system matrix with
the preconditioned GMRes applied to the KKT matrix. With the GMRes,
we checked two preconditioners, P and ˜P. The velocity distribution shown in
Figure 4(a) was the initial model for the inversion. The computed slowness
updates are compared in Figure 4(b)-(d).

13

(a)

(b)

(c)

(d)

Figure 4: Comparison of the the two slowness updates computed at the end of a
single GN iteration. Panel (a) shows the initial velocity distribution. Panel (b)
depicts the slowness update obtained by applying the CG solver to the normal
system of linear equations. Panel (c) shows the slowness update by the GMRes
solver with P applied to the KKT system. Panel (d) same as (c) but with
preconditioner ˜P Note that in (a) color represents the P-wave velocity in km/s,
whereas in (b)-(d) color represents update to the squared slowness in s2/m2.

14

The preconditioner ˜P was created by making use of ILU with the ﬁll-in
parameter 21. It is not not possible to compare the CG and GMRes directly,
because the CG computes Ecg := (cid:107)H δs n − g(cid:107)2 at each iteration, whereas GM-
Res computes Egmres := (cid:107)Mξn − b(cid:107)2. To make the comparison, we separately
computed values of Ecg from the GMres iterates ξn. The convergence of the
iterative methods are presented in Figure 5. The CG and GMRes achieved ap-

Figure 5: Evolution of data misﬁt for the CG applied to the normal system and
the preconditioned GMRes applied to the KKT matrix. The Y -axis represents
the quantity (cid:107)H δs n − g(cid:107)2 / (cid:107)g(cid:107)2.

proximately the same tolerance on iterations 19 and 30, respectively (Figure 5).
As expected, all methods generated almost identical updates. The convergence
rate of the CG is the same as that of the GMRes with the P, although the
GMRes happened to reach the desired tolerance by 3 iterations faster. The con-
vergence of the GMRes with ˜P is slower than that with P, but the cost is much
lesser. The performance of the algorithms is compared in Table 1. We see the
dramatic acceleration of matrix-vector multiplication in the GMRes compared
to the CG. This is because the linear solves with factorized matrices ˜A and
˜A∗ inside ˜P is much cheaper than the linear solves with A and A∗ during the
CG matrix-vector multiplication. Another observation is that we needed a high
value of ﬁll-in (we used 21) to have an acceptable approximation A ≈ ˜A.

In the second numerical experiment, we demonstrate that our approach

15

Table 1: Linear solvers comparison. The column ’CG’ refers to the the CG
applied to the normal system of linear equations, ’GMRes-full’ is the GMRes
applied to the KKT matrix preconditioned with P, ’GMRes-approx’ is the GM-
Res preconditioned with ˜P.

Iterations
Time per iteration, s
ILU initialization, s
Total time, s

CG GMRes-full GMRes-approx
19
264
0
4800

16
267
0
5391

30
18
121
834

serves as an eﬃcient engine for the multi-frequency FWI. The data were simu-
lated at frequencies from 5 to 40 Hz with a step of 2.5 Hz. At each frequency in
the sequence, from lower to higher, a single GN step was performed, and the up-
dated model was passed to the next frequency. The regularization parameter ε
was changing linearly from 10 to 1E5. The number of internal GMRes iterations
was set to 30. As indicated in Figure 6, the data misﬁt was improved after the
inversion. The improvement becomes more pronounced at higher frequencies
because the data magnitudes increase with the frequency and the initial model
describes high-frequency data very poorly.

Figure 6: The relative residual misﬁt on Newton iterations: ’resid-norm-ini’ is
(cid:107)dinitial − d(cid:107)2 / (cid:107)d(cid:107)2, ’resid-norm-ﬁn’ is (cid:107)df inal − d(cid:107)2 / (cid:107)d(cid:107)2.

The initial velocity model, several intermediate models, and the ﬁnal model
are presented in Figure 7. In this numerical experiment, we set the term −Lsn in
(29) to zero and, thus, the algorithm was penalizing the norm of the update δs,

16

not the updated model sn + δs. This was the reason for the high-velocity build-
up at X = 2500. We stress again that identical FWI results would have been
obtained if we had applied the standard RSGN-CG optimization (provided, of
course, that the values of error Ecg match in both approaches). The diﬀerence
between the two approaches is in their performance.

17

(a) Initial

(b) 5 Hz

(c) 10 Hz

(d) 17.5 Hz

(e) 25 Hz

(f) 40 Hz

Figure 7: The initial velocity distribution (a) and updated distributions (b)-(f)
at diﬀerent frequencies. The color scales here and in Figure 3 are identical. The
X- and Z-axis are marked in meters. Similar, if not identical, updates would
have been obtained if we had performed the standard RSGN-CG optimization
with the same accuracy of data ﬁt.
18

7 Conclusions

This paper proposes a new numerical method for the seismic FWI, based on
the full-space approach. Prior work has demonstrated that the full-space ap-
proach can be used as a framework for eﬃcient waveform inversion algorithms.
Those works have focused either on the penalty method with the primal-dual
descend optimization or the AL approach with the ADMM optimization proce-
dure. These approaches have demonstrated impressive results, but may suﬀer
from the slow convergence compared to the Newton iterations [25].

This paper studies the iterative full-space FWI by the Newton method. A
preconditioned Krylov solver is applied to the KKT system on each Newton iter-
ation. Our method computes the material parameter updates identical to those
calculated by inverting the reduced Hessian, provided that the linear systems are
solved exactly. The critical element of our approach is a special preconditioner.
The preconditioned iterative algorithm requires two forward simulations (exact
or approximate) at each iteration plus an inexpensive solve with the constraint
matrix L. We considered two variants of the preconditioner: preconditioner
P computes linear solves with A and A∗ exactly, whereas preconditioner ˜P
computes the inexact solves. A single application of P should therefore have
almost the same cost compared to a single iteration of the CG applied to the
normal system. Our numerical experiments indicated that these two methods
indeed have very close performance. On the other hand, a single application of
˜P is much cheaper. In our numerical experiments, this approach reduced the
execution time considerably.

For a real-scale 3D FWI, several problems need to be addressed. First,
the forward and adjoint simulations need to be based on the theory of linear
elasticity, so it remains to be shown that our approach works for elastic FWI.
Second, direct linear solvers become unfeasible in the 3D applications, even for
the acoustic equations, not to mention the linear elasticity. Thus, a reliable
preconditioned iterative solver should be applied for the forward and adjoint
simulations. Substantial progress in this direction has been made; see [29] and
references therein.
In principle, any preconditioner designed for the forward
simulation can be used inside ˜P. Third, the convergence can be improved by
a more sophisticated approximation of the reduced Hessian, such as the L-
BFGS or the background Hessian. Virtually any method designed to accelerate
the reduced-space Newton method can be used to compute the H-block solve
within our approach. Finally, using the BiCGStab instead of the GMRes will
eliminate the memory needed to store GMRes iterates between restarts. Future

19

work, therefore, should include a study of these options.

8 Acknowledgments

This research was partially supported by the Russian Science Foundation, project
no. 21-11-00139. The authors acknowledge computational resources granted by
Complex for Simulation and Data Processing for Mega-Science Facilities at NRC
”Kurchatov Institute”, http://ckp.nrcki.ru/.

References

[1] P. Lailly. The seismic inverse problem as a sequence of before stack migra-
tions. In Conference on Inverse Scattering, Theory and Application, pages
206–220. SIAM, 1983.

[2] A. Tarantola. Inversion of seismic reﬂection data in the acoustic approxi-

mation. Geophysics, 49(8):1259–1266, 1983.

[3] P. R. Mora. Nonlinear two-dimensional elastic inversion of multi-oﬀset

seismic data. Geophysics, 52(9):1211–1228, 1987.

[4] J. Virieux and S. Operto. An overview of full-waveform inversion in explo-

ration geophysics. Geophysics, 74:WCC1–WCC26, 2009.

[5] J. Tromp. Seismic waveﬁeld imaging of Earth’s interior across scales. Nature

Reviews Earth & Environment, 1:40–53, 2020.

[6] R. Plessix. Three-dimensional frequency-domain full-waveform inversion
with an iterative solver. Geophysics, 74(6):WCC149–WCC157, 2009.

[7] M. Warner, A. Ratcliﬀe, T. Nangoo, J. Morgan, A. Umpleby, N. Shah,
V. Vinje, I. ˇStekl, L. Guasch, C. Win, G. Conroy, and A. Bertrand.
Anisotropic 3d full-waveform inversion. Geophysics, 78(2):R59–R80, 2013.

[8] S. Operto, A. Miniussi, R. Brossier, L. Combe, L. M´etivier, V. Mon-
teiller, A. Ribodetti, and J. Virieux. Eﬃcient 3-D frequency-domain mono-
parameter full-waveform inversion of ocean-bottom cable data: application
to Valhall in the visco-acoustic vertical transverse isotropic approximation.
Geophysical Journal International, 202(2):1362–1391, 07 2015.

20

[9] S Operto and A Miniussi. On the role of density and attenuation in three-
dimensional multiparameter viscoacoustic VTI frequency-domain FWI: an
OBC case study from the North Sea. Geophysical Journal International,
213(3):2037–2059, 2018.

[10] A. Fichtner, B. L. N. Kennett, H. Igel, and H.-P. Bunge. Full seismic
waveform tomography for upper-mantle structure in the australasian region
using adjoint methods. Geophysical Journal International, 179(3):1703–
1725, 2009.

[11] C. Shin, S. Jang, and D.-J. Min.

Improved amplitude preservation
for prestack depth migration by inverse scattering theory. Geophysical
Prospecting, 49(5):592–606, 2001.

[12] W.A. Mulder and R.-E. Plessix. Exploring some issues in acoustic full

waveform inversion. Geophysical Prospecting, 56(6):827–841, 2008.

[13] R. Brossier, S. Operto, and J. Virieux. Seismic imaging of complex on-
shore structures by 2d elastic frequency-domain full-waveform inversion.
Geophysics, 74(6):WCC105–WCC118, 2009.

[14] R. G. Pratt, C. Shin, and G. J. Hick. Gauss–Newton and full Newton
methods in frequency–space seismic waveform inversion. Geophysical Jour-
nal International, 133(2):341–362, 1998.

[15] I. Epanomeritakis, V. Ak¸celik, O Ghattas, and J Bielak. A Newton-CG
method for large-scale three-dimensional elastic full-waveform seismic in-
version. Inverse Problems, 24(3):034015, 2008.

[16] L. M´etivier, R. Brossier, S. Operto, , and J. Virieux. Full waveform in-
version and the truncated newton method. SIAM Review, 59(1):153–195,
2017.

[17] E.Haber, U. M. Ascher, and D. Oldenburg. On optimization techniques for
solving nonlinear inverse problems. Inverse Problems, 16(4):1263, 2000.

[18] E. Haber and U. M. Ascher. Preconditioned all-at-once methods for large,
Inverse Problems, 17:1847–1864,

sparse parameter estimation problems.
2001.

[19] E. Haber, U. M. Ascher, and D. W. Oldenburg. Inversion of 3D electro-
magnetic data in frequency and time domain using an inexact all-at-once
approach. Geophysics, 69(5):1216–1228, 2004.

21

[20] A. Abubakar, W. Hu, T. M. Habashy, and P. M. van den Berg. Application
of the ﬁnite-diﬀerence contrast-source inversion algorithm to seismic full-
waveform data. Geophysics, 74(6):WCC47–WCC58, 2009.

[21] T. van Leeuwen and F. J. Herrmann. Mitigating local minima in full-
waveform inversion by expanding the search space. Geophysical Journal
International, 195(1):661–667, 2013.

[22] T. van Leeuwen and F. J. Herrmann. A penalty method for PDE-
constrained optimization in inverse problems. Inverse Problems, 32:015007,
2016.

[23] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York,

NY, USA, 2e edition, 2006.

[24] G. Rizzuti, M. Louboutin, R. Wang, and F. J. Herrmann. A dual formula-
tion of waveﬁeld reconstruction inversion for large-scale seismic inversion.
Geophysics, 86(6):R879–R893, 2021.

[25] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed
optimization and statistical learning via the alternating direction method
of multipliers. Found. Trends Mach. Learn., 3(1):1–122, jan 2011.

[26] H. S. Aghamiry, A. Gholami, and S. Operto.

Improving full-waveform
inversion by waveﬁeld reconstruction with the alternating direction method
of multipliers. Geophysics, 84(1):R125–R148, 2019.

[27] K. Aghazade, A. Gholami, H. S. Aghamiry, and S. Operto. Anderson-
accelerated augmented lagrangian for extended waveform inversion. Geo-
physics, 87(1):R79–R91, 2022.

[28] G. Biros and O. Ghattas. Parallel Lagrange–Newton–Krylov–Schur meth-
ods for pde-constrained optimization. part i: The krylov–schur solver.
SIAM Journal on Scientiﬁc Computing, 27(2):687–713, 2005.

[29] N. Yavich, N. Khokhlov, M. Malovichko, and M.S Zhdanov. Contraction
operator transformation for the complex heterogeneous helmholtz equation.
Computers and Mathematics with Applications, 86:63–72, 2021.

[30] B. Hustedt, S. Operto, and J. Virieux. Mixed-grid and staggered-grid
ﬁnite-diﬀerence methods for frequency-domain acoustic wave modelling.
Geophysical Journal International, 157(3):1269–1296, 2004.

22

[31] M.S. Malovichko, A.V. Tarasov, N.B. Yavich, and K.V. Titov. Applica-
tion of optimal control to inversion of self-potential data: theory and syn-
thetic examples. IEEE Transactions on Geoscience and Remote Sensing,
in press:1–1, 2021.

[32] M. Hinze, R. Pinnau, M. Ulbrich, and S. Ulbrich. Optimization with PDE

Constraints. Springer, 2009.

[33] V. Kostin, S. Solovyev, A. Bakulin, and M. Dmitriev. Direct frequency-
domain 3D acoustic solver with intermediate data compression bench-
marked against time-domain modeling for full-waveform inversion appli-
cations. Geophysics, 84(4):T207–T219, 2019.

[34] Y.A. Erlangga, C. Vuik, and C.W. Oosterlee. On a class of preconditioners
for the helmholtz equation. Applied numerical mathematics, (50):409–425,
2004.

23

