2
2
0
2

r
a

M
6
1

]

V

I
.
s
s
e
e
[

1
v
8
5
8
8
0
.
3
0
2
2
:
v
i
X
r
a

A Real-Time Region Tracking Algorithm
Tailored to Endoscopic Video with Open-Source
Implementation
PREPRINT

Jonathan P. Epperlein and Sergiy Zhuk

IBM Research Europe, Damastown, D15 HN66 Dublin, Ireland

Abstract. With a video data source, such as multispectral video ac-
quired during administration of ﬂuorescent tracers, extraction of time-
resolved data typically requires the compensation of motion. While this
can be done manually, which is arduous, or using oﬀ-the-shelf object
tracking software, which often yields unsatisfactory performance, we present
an algorithm which is simple and performant. Most importantly, we pro-
vide an open-source implementation, with an easy-to-use interface for
researchers not inclined to write their own code, as well as Python mod-
ules that can be used programmatically.

Keywords: Image Registration · Endoscopes · Computer Vision · Re-
gion Tracking

1

Introduction

Fig. 1. The proposed algorithm in action: despite considerable movement, the boxes
representing regions of interest stay on the same tissue regions throughout.

Mutltispectral imaging is now a common feature of interventional medical
imaging stacks. Intraoperative use of ﬂuorescent dyes (typically ICG) makes
multispectral endoscopic video a frequently encountered source of data, as in
e.g. [4,16,13]. While in laboratory settings the specimens can be immobilized
completely, in a clinical setting, imaging devices are commonly handheld and
the imaged tissue is subject to constant motion, deformation, and occlusions
through instruments, reﬂections, and ﬂuid. If dynamic, i.e. time-resolved, visual

 
 
 
 
 
 
2

PREPRINT, under review

data is to be collected from diﬀerent regions of interest (ROIs) of the tissue, it is
thus necessary to compensate the motion, in other words to ensure that in each
consecutive frame, data is indeed collected from the same region of tissue, even as
it moves to diﬀerent locations within the frame, see Figure 1 for an illustration.
This task can be addressed manually, e.g. [13], by averaging [16], or by using a
more sophisticated computer vision algorithm – the approach followed in this
contribution.

Our contribution here is two-fold: We introduce a simple algorithm that can
be used to correct motion in endoscopic video (or video from any source) by
tracking the location of an arbitrary number or ROIs from frame to frame. We
also provide an open-source implementation, which comprises Python modules
that can be used programmatically by anyone inclined to do so, and a simple
command-line interface (CLI) which provides access to the tracking functionality
without the need to write any additional code. At the time of writing, the code,
along with installation and usage notes, can be found at https://github.com/
IBM/optﬂow-region-tracker. Algorithm and implementation have been “ﬁeld-
tested” by our collaborators and have been used for data acquisition in several
prior publications [4,22,7,5,6].

We now deﬁne the “problem” more rigorously, before brieﬂy reviewing related
work. Our proposed tracking algorithm is then explained in detail, before it is
evaluated and compared to a suite of object trackers available in the computer
vision library OpenCV [3].

1.1 The ROI tracking problem

0, . . . , rN
0 is an axis-parallel rectangle ri

We are given a video V , which is really just a sequence of T frames, i.e. V =
(I0, I1, . . . , IT −1), where each frame It has height h, width w and either 1 or 3
channels. The ROIs R0 = (r1
0, r2
0 ) are speciﬁed by their positions in the
initial frame I0, and each ROI ri
0).
Note that by convention, the axis origin is the left-top corner of the frame, with
the y-axis pointing down, so that the point (xi
0) is the left-top corner of the
i-th speciﬁed ROI, and wi
0 are its width, resp. height. We are now
interested in estimating the subsequent ROI positions (R1, . . . , RT −1) so that
whatever was initially inside the box deﬁned by ri
0 (such as a speciﬁc region of
tissue) is inside ri

0, resp. hi

0 = (xi

0, wi

0, hi

0, yi

0, yi

t in frame It.

It should be clear that whatever starts out as an axis-parallel rectangle will
not remain one: rotation and perspective changes alone transform each rectangle
to a general quadrilateral, and deformations warp straight lines into general
curves. A complete agreement between predicted location ri
t and the ROI’s true
contents ri
t,true is hence not achievable. As a measure of agreement between the
rectangle ri
t and the general shape ri
t,true we adopt the (rasterized) Jaccard index
(also called Intersection-over-Union):

J(ri

t,true, ri

t) =

number of pixels inside ri
number of pixels inside ri

t,true AND ri
t
t,true OR ri
t

.

(1)

PREPRINT, under review

3

A Jaccard index of 1 thus indicates perfect agreement, whereas 0 corresponds to
completely disjoint sets.

We assume that all ROIs are visible in all frames, or we accept that if a ROI
i leaves the view at some point t, all subsequent locations ri
t+2, . . . will be
marked as NaN (not a number), i.e. tracking for this ROI ends at t even if it
enters the view again later, it can not be reacquired.

t+1, ri

1.2 Related Work

While this at ﬁrst appears to be a classical object-tracking problem, it turns
out that many object trackers perform quite poorly, see Section 3. However
on further reﬂection, this is not surprising: the typical object-tracking task is
concerned with physical objects moving freely and independently in front of a
largely static background. For example, the popular KITTI dataset [8] contains
street scenes with moving cars and pedestrians, and in a clinical context, the
Cholec80 dataset [19] tracks the locations of surgical instruments. Hence, algo-
rithms typically attempt to build a model of the object, e.g. [9] uses a mix of
Haar-like features, orientation histograms, and local binary patterns, to describe
the object and build a classiﬁer that can distinguish it from other regions in the
same video.

In contrast to that, the ROIs under consideration here often have very little
“objectness” about them, in the sense that there is little distinguishing them
from the rest of the video frame, there often are no edges or other salient points;
however a ROI’s movement is not independent of the surrounding image, quite
the opposite, typically the entire view will be ﬁlled by connected tissue (with
the exception of occasional surgical tools, reﬂections, and occlusion by e.g. blood
or water). One could thus say that the problem addressed here is outside the
design speciﬁcation of an oﬀ-the-shelf object tracking algorithm, that some of
them still perform as well as they do in Section 3 is thus a testament to their
robustness and generalizability.

Approaches speciﬁc to endoscopic video can also be found in the literature,
e.g. [21] uses a combination of ORB key points and the MedFlow tracking algo-
rithm, and [20] describes an multi-step algorithm solving not only the tracking
problem, but also able to reacquire a ROI if it reappears. However, to the best
of our knowledge, no implementations of the described algorithms are available,
and so they are not accessible to the average medical researcher; we can also not
include them in our comparisons.

2 Tracking Algorithm Based on Optical Flow

Our proposed region tracking algorithm obtains a global estimate of the image
deformation, and from that computes a local transformation for each ROI to
ﬁnd its new location. Speciﬁcally, for each pair of frames It, It+1, the optical
ﬂow ﬁeld Ut(x, y) is estimated, and subsequently for each ROI ri
t, a local ﬂow
ﬁeld, constrained to preserve the ROI as an axis-parallel rectangle, is computed
to move the ROI to its new location ri
t+1.

4

PREPRINT, under review

2.1 Optical Flow

The concept of optical ﬂow is a very basic one in computer vision, hence the
available literature is vast, and the present section is just a rudimentary primer.
For more background, consider [23,14] and the references at [2].

Very roughly, the optical ﬂow Ut(x, y) = (ut(x, y), vt(x, y)) is a 2-D ﬂow ﬁeld,
i.e. it assigns to each pixel (x, y) a ﬂow ut(x, y) in x-direction, and a ﬂow vt(x, y)
in y-direction, which (approximately) warps the frame It into the next frame
It+1 in the sense that

(cid:0)x + ut(x, y), y + vt(x, y)(cid:1) ≈ It+1(x, y);

It

in words: the color of pixel (x, y) in frame It moved to location (x + ut(x, y), y +
vt(x, y)) in frame It+1.

The two main practical issues are that It+1 is never a mere rearrangement
1. The latter
of the pixels in It, and that there is no unique “best” ﬂow ﬁeld Ut
point is addressed by the reasonable assumption that a pixel’s movement is
not drastically diﬀerent from nearby pixels’, which means the ﬂow ﬁeld may be
assumed to be smooth. Hence, most optical ﬂow algorithms minimize a mixture
of two penalty terms, the “data term” CD measuring the disagreement between
the warped frame It and the next frame It+1, and the “smoothness term” CS,
quantifying the smoothness of the ﬂow ﬁeld:

Ut = arg min
(u,v)

CD

(cid:16)

(cid:17)
(cid:0)x + u(x, y), y + v(x, y)(cid:1) − It+1(x, y)

It

+ λCS

(cid:0)(u, v)(cid:1),

the weight λ > 0 controls the relative importance of the smoothness. It is the
smoothness term that makes the optical ﬂow a global estimate: in regions of
low or no texture, the data term yields no constraints on the ﬂow, but the
smoothness term will force the ﬂow estimate to be a smooth interpolation of the
ﬂows computed in surrounding areas with more texture.

There are 100s of algorithms to compute the optical ﬂow between two frames;
we are using DISﬂow [14] because it is the fastest and most precise of the ones
available in OpenCV. However, what follows is completely independent of the
speciﬁc algorithm used, as long as it computes a ﬂow ﬁeld U .

2.2 Region Tracking

Given the ﬂow Ut, it now can be used to move the pixels in ROI ri
t to estimate
the next location ri
t+1. Simply applying the ﬂow at the corners or along the
edges of the ROI will warp it from its rectangular shape, and discard all the
information from the ROI’s interior. Instead, we propose to aggregate the ﬂow
inside a given ROI in a way that preserves its shape.

1 to see that, consider the case where two pixels in It have the same color as some

pixel (x, y) in It+1 – which of them should ﬂow to (x, y)?

The simplest such aggregation yields a constant ﬂow vector ¯U i

t, ¯vi

t) for

each ROI, namely the median of ut and vt inside ri

¯ui
t = median{ut(x, y) | xi
t = median{vt(x, y) | xi
¯vi

t ≤ x ≤ xi
t ≤ x ≤ xi

t + wi
t + wi

PREPRINT, under review

5

t = (¯ui
t. It is computed as:
t, yi
t, yi

t ≤ y ≤ yi
t ≤ y ≤ yi

t + hi
t}
t + hi
t}.

The estimated next location is then

t+1 = (xi
ri

t + ¯ui

t, yi

t + ¯vi

t, wi

t, hi

t).

(2)

Due to the use of the median (instead of the mean), this is robust to outliers
in the ﬂow Ut. It preserves the shape of the ROI exactly, i.e. the height and
width never change, and performs remarkably well. This version of our tracker is
conceptually very similar to the MedFlow tracker [11], with the diﬀerence that
we do not compute key points, and scale changes are not estimated here.

To accommodate changes in scale, which for instance occur if the camera is
moved closer to, or farther from, the tissue, we provide a second aggregation
method, which allows scaling of ROIs, while preserving their shape as axes-
parallel rectangles. It is straightforward to see that a ﬂow ﬁeld preserving axis-
parallel lines has to satisfy u(x, y) = u(x) and v(x, y) = v(y), i.e. the ﬂow
in x-direction has to be independent of the y-coordinate, and vice versa. The
simplest function satisfying this constraint is an aﬃne function u(x, y) = τx +
σxx, v(x, y) = τy + σyy. This has a straightforward interpretation: warping by
such a ﬂow corresponds to translation by (τx, τy) and anisotropic scaling along
the x and y axes.

Speciﬁcally, for each ROI ri

t, we compute scaling and translation by ﬁtting
an aﬃne function to the ﬂow ﬁeld within (dependence on t omitted for better
readability):

(τ i

x, σi

x) = arg min
(p,s)

(τ i

y, σi

y) = arg min
(p,s)

xi+wi
(cid:88)

yi+hi
(cid:88)

x=xi

y=yi

xi+wi
(cid:88)

yi+hi
(cid:88)

x=xi

y=yi

(cid:12)ut(x, y) − (p + sx)(cid:12)
(cid:12)
(cid:12)

2

(cid:12)vt(x, y) − (p + sy)(cid:12)
(cid:12)
(cid:12)

2

and the new location is
t+1 = (cid:0)xi
ri

t + σi

xxi

t + τ i

x, yi

t + σi

yyi

t + τ i

y, (1 + σi

x)wi

t, (1 + σi

y)hi
t

(cid:1),

(3)

hence the ROI is scaled along the axes by (1 + σx), resp. (1 + σy) (with respect
to the origin, which is the left-top corner of the frame) and then translated by
(τ i

x, τ i

y).

3 Evaluation Results

The proposed algorithm is evaluated using videos generated in the following way.
From each of 17 liver laparascopies and 37 colonoscopies, 2 frames at diﬀerent

6

PREPRINT, under review

time points were extracted, for a total of 108 initial frames of resolution 480 ×
360 each. In each frame, 10 ROIs were selected at random. To simulate motion,
a sequence of 50 random projective transforms were then applied to each of the
initial frames, and at the same time to each of the ROIs, yielding a synthetic
video along with the ground truth locations (r1
50) for
each of the 10 ROIs. A projective transform consists of translation, rotation, scal-
ing, shearing, and elation, and is the most general transformation that preserves
straight lines, see [18] for details. Note that it follows that the ground truth
locations are not rectangular anymore, but only general quadrilaterals, but their
agreement with the rectangular estimates from the tracking algorithms can be
easily evaluated as in (1).

0 ), . . . , (r1

50, . . . , r10

0, . . . , r10

Additionally, saturated regions of random sizes were added to simulate re-
ﬂections, which are commonly encountered in endoscopic applications, before
supplying each new frame to the trackers.

To keep the videos realistic, shear (3 pixels), scaling (between 0.95 and 1.05),
and translation velocity (3 pixels/frame) were limited to be small; the random
rotations were bounded by either 0°, ±5°, or ±10°, and the number of added
reﬂections was either 0, 10, or 25. This process yielded a total of 108 · 50 · 3 · 3 =
48600 video frames.

We evaluated our tracker with both, the median aggregation as in (2), and
the aﬃne aggregation as in (3), alongside the OpenCV [3] implementations of
KCF [10], MedianFlow [11], the boosting-based tracker [9], CSRT [15], MIL [1],
and TLD [12]. Since Boosting, CSRT, MIL and TLD are an order of magnitude
slower than the other trackers, they were evaluated on the ﬁrst 10 frames of each
video only, to keep computation times manageable. All algorithms’ parameters
are set to their defaults.

All experiments were run on a MacBook Pro with a 2.6 GHz 6-Core Intel
Core i7 CPU and 16GB of RAM, using Python 3.6 and OpenCV 3.4.7. The
results are illustrated in Figures 2–4.

3.1 Discussion

The proposed tracker with median ﬂow is the clear winner among the track-
ers with real-time performance, maintaining a Jaccard index of 0.85 for 75% of
all frames and ROIs; however considering the eﬀect of rotations and reﬂections
separately also indicates its limitations, as performance degrades considerably
with the addition of each. In fact, all trackers struggle with rotations and reﬂec-
tions, except CSRT, which remains relatively unaﬀected and maintains its high
performance, albeit at the cost of roughly 10-fold decrease in FPS.

Based on this test dataset, there is no case to be made for the aﬃne ﬂow
aggregation as given in (3), however in our experience there have been videos for
which the aﬃne tracker was able to track ROIs when the median-based tracker
failed. We intend to investigate this further in future research.

PREPRINT, under review

7

Fig. 2. Performance of all trackers over all frames and ROIs. The boxes illustrate the
distribution of the Jaccard indices, while the triangles indicate the number of frames
processed in 1 second; clearly, Boosting, CSRT, MIL and TLD are signiﬁcantly slower.

Fig. 3. Figure 2, with the inﬂuence of rotation considered separately. The 3 bars for
each tracker are computed from its performance on frames generated with no rotation,
rotations of less than 5°, and rotations of less than 10°, from left to right.

Fig. 4. Figure 2, with the inﬂuence of reﬂections considered separately. The 3 bars for
each tracker are computed from its performance on frames generated with no reﬂec-
tions, 10 reﬂections, and 25 reﬂections per frame, from left to right.

medianaffineKCFMEDIANFLOWBOOSTINGCSRTMILTLD0.20.40.60.81.0Jaccard Index010203040506070Average FPS05median1005affine1005KCF1005MEDIANFLOW1005BOOSTING1005CSRT1005MIL1005TLD100.20.40.60.81.0Jaccard Index010median25010affine25010KCF25010MEDIANFLOW25010BOOSTING25010CSRT25010MIL25010TLD250.00.20.40.60.81.0Jaccard Index8

PREPRINT, under review

3.2 A Possible Use Case

As a typical use case consider [17], where the colon is imaged using a multi-
spectral endoscope during and after ICG administration. As a result of the ICG
perfusing, the infrared image lights up as ICG arrives in the tissue and ﬂuo-
resces, and goes dark again as the ICG is washed out. It is conjectured that the
shape of the intensity-vs-time curve of diﬀerent regions of the colon can be used
to quantify the quality of their perfusion.

The provided tool can be used to collect these curves for an arbitrary number

of regions:

>> python region_tracking.py --separate_vids --show-tracking

The --separate_vids option means that the user will be prompted for two
video ﬁles: the ﬁrst containing the visible light image, which will be used for
tracking, and the second the infrared image, which will be used for data collec-
tion. Options for cases where the visible light and infrared views are separate
panels in a merged video are also provided. A small Graphical User Interface
(GUI) will open, in which the user can select the ROIs to be tracked. Tracking
then commences until stop, and upon completion, the collected data is written
into CSV ﬁles, ready for further processing. See the Supplementary Material for
a screen cap video of the tool in action.

4 Conclusion and Caveats

While the algorithm and its implementation performed well in the evaluation of
Section 3, and has performed well for us in practice, a few drawbacks need to be
emphasized.

– Unlike the other evaluated trackers, our tracker currently does not supervise
its own performance, i.e. it does not assess if the region within the estimated
ROI location is visually similar to the ROI from frame 0, or the previous
frame. This is in active development, and a forward-backward error similar
to the one proposed in [11] is being implemented.

– The tracker is not able to re-acquire a ROI if it was lost from view at any
time; it is also not able to recover when its position has drifted. As a result, its
performance degrades over time, and tracking for longer than ∼ 15 minutes
requires a steady hand by the imaging surgeon.

– The tracker is robust to quality issues such as reﬂections, occlusions, motion
blur, compression artefacts, and interlacing, only as much as the underlying
optical ﬂow estimation algorithm is. The implementations in OpenCV oﬀer
no option to specify a mask of ignored pixels, so poor-quality video can cause
problems.

As in most applications to real data, your mileage may vary, and we suggest to
not trust the collected data blindly, but to manually supervise the tracker’s per-
formance: a video with the estimated ROI locations is generated automatically
and allows for a quick quality check.

PREPRINT, under review

9

References

1. Babenko, B., Yang, M.H., Belongie, S.: Visual tracking with online multiple in-
stance learning. In: 2009 IEEE Conference on computer vision and Pattern Recog-
nition. pp. 983–990. IEEE (2009)

2. Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M., Szeliski, R.: vi-

sion.middlebury.edu/ﬂow/

3. Bradski, G.: The OpenCV Library. Dr. Dobb’s Journal of Software Tools (2000)
4. Cahill, R.A., O’Shea, D.F., Khan, M.F., Khokhar, H.A., Epperlein, J.P.,
Mac Aonghusa, P.G., Nair, R., Zhuk, S.M.: Artiﬁcial intelligence indocyanine green
(ICG) perfusion for colorectal cancer intra-operative tissue classiﬁcation. British
Journal of Surgery 108(1), 5–9 (12 2020). https://doi.org/10.1093/bjs/znaa004,
https://doi.org/10.1093/bjs/znaa004

5. Dalli, J., Epperlein, J.P., Hardy, N.P., Khan, M.F., Zhuk, S., Mac Aonghusa, P.G.,
Cahill, R.A.: Development of a method for personalised, algorithmic colonic tran-
section recommendation. (2021), clinical Translation of Medical Image Computing
and Computer Assisted Interventions (CLINICCAI) (presentation only)

6. Dalli, J., Hardy, N., Mac Aonghusa, P.G., Epperlein, J.P., Cantillon Murphy, P.,
Cahill, R.A.: Challenges in the interpretation of colorectal indocyanine green ﬂu-
orescence angiography – a video vignette. Colorectal Disease 23(5), 1289–1290
(2021). https://doi.org/https://doi.org/10.1111/codi.15592, https://onlinelibrary.
wiley.com/doi/abs/10.1111/codi.15592

7. Epperlein, J.P., Zayats, M., Tirupathi, S., Zhuk, S., Tchrakian, T., Mac Aonghusa,
P., O’Shea, D.F., Hardy, N.P., Dalli, J., Cahill, R.A.: Practical perfusion quantiﬁ-
cation in multispectral endoscopic video: Using the minutes after ICG administra-
tion to assess tissue pathology. In: AMIA Annual Symposium Proceedings. vol. (to
appear). American Medical Informatics Association (2021), to appear

8. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the KITTI
vision benchmark suite. In: Conference on Computer Vision and Pattern Recogni-
tion (CVPR) (2012)

9. Grabner, H., Grabner, M., Bischof, H.: Real-time tracking via on-line boosting. In:
Proceedings of the British Machine Vision Conference (BMVC). vol. 5, pp. 74–56
(2006)

10. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: Exploiting the circulant struc-
ture of tracking-by-detection with kernels. In: European conference on computer
vision. pp. 702–715. Springer (2012)

11. Kalal, Z., Mikolajczyk, K., Matas, J.: Forward-backward error: Automatic detec-
tion of tracking failures. In: 2010 20th international conference on pattern recog-
nition. pp. 2756–2759. IEEE (2010)

12. Kalal, Z., Mikolajczyk, K., Matas, J.: Tracking-learning-detection. IEEE transac-

tions on pattern analysis and machine intelligence 34(7), 1409–1422 (2011)

13. Khokhar, H.A., Loughman, E., Khogali, M., Mulligan, N., O’Shea, D.F., Cahill,
R.A.: Visual probing of rectal neoplasia: near-infrared interrogation of primary
tumors and secondary lymph nodes. Minerva Chirurgica 73(2), 217–226 (2018)
14. Kroeger, T., Timofte, R., Dai, D., Van Gool, L.: Fast optical ﬂow using dense in-
verse search. In: European Conference on Computer Vision. pp. 471–488. Springer
(2016)

15. Lukezic, A., Vojir, T., ˇCehovin Zajc, L., Matas, J., Kristan, M.: Discriminative
correlation ﬁlter with channel and spatial reliability. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 6309–6318 (2017)

10

PREPRINT, under review

16. Park, S.H., Park, H.M., Baek, K.R., Ahn, H.M., Lee, I.Y., Son, G.M.: Artiﬁcial
intelligence based real-time microcirculation analysis system for laparoscopic col-
orectal surgery. World Journal of Gastroenterology 26(44), 6945–6962 (2020)
17. Son, G.M., Kwon, M.S., Kim, Y., Kim, J., Kim, S.H., Lee, J.W.: Quantitative
analysis of colon perfusion pattern using indocyanine green (icg) angiography in
laparoscopic colorectal surgery. Surgical endoscopy 33(5), 1640–1649 (2019)
18. Szeliski, R.: Computer vision: algorithms and applications. Springer Science &

Business Media (2010)

19. Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy,
N.: Endonet: a deep architecture for recognition tasks on laparoscopic videos. IEEE
transactions on medical imaging 36(1), 86–97 (2016)

20. Ye, M., Giannarou, S., Meining, A., Yang, G.Z.: Online tracking and retargeting
with applications to optical biopsy in gastrointestinal endoscopic examinations.
Medical Image Analysis 30, 144–157 (2016)

21. Zhan, J., Cartucho, J., Giannarou, S.: Autonomous tissue scanning under free-
form motion for intraoperative tissue characterisation. In: 2020 IEEE Interna-
tional Conference on Robotics and Automation (ICRA). pp. 11147–11154 (2020).
https://doi.org/10.1109/ICRA40945.2020.9197294

22. Zhuk, S., Epperlein, J.P., Nair, R., Tirupathi, S., Mac Aonghusa, P., O’Shea, D.F.,
Cahill, R.: Perfusion quantiﬁcation from endoscopic videos: Learning to read tumor
signatures. In: Medical Image Computing and Computer Assisted Intervention –
MICCAI 2020. pp. 711 – 721. Springer, Springer International Publishing (2020)
23. Zimmer, H., Bruhn, A., Weickert, J.: Optic ﬂow in harmony. International Journal

of Computer Vision 93(3), 368–388 (2011)

