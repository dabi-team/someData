Diverse Title Generation for Stack Overﬂow Posts with Multiple
Sampling Enhanced Transformer

Fengji Zhanga, Jin Liua∗, Yao Wanb, Xiao Yuc∗, Xiao Liud and Jacky Keunge

aSchool of Computer Science, Wuhan University, Wuhan, China
bSchool of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China
cSchool of Computer Science and Artiﬁcial Intelligence, Wuhan University of Technology, Wuhan, China
dSchool of Information Technology, Deakin University, Geelong, Australia
eDepartment of Computer Science, City University of Hong Kong, Hong Kong, China

2
2
0
2

g
u
A
4
2

]
E
S
.
s
c
[

1
v
3
2
5
1
1
.
8
0
2
2
:
v
i
X
r
a

A R T I C L E I N F O

Keywords:
Stack Overﬂow
Title Generation
CodeT5
Nucleus Sampling
Maximal Marginal Ranking

A B S T R A C T

Stack Overﬂow is one of the most popular programming communities where developers can seek
help for their encountered problems. Nevertheless, if inexperienced developers fail to describe their
problems clearly, it is hard for them to attract suﬃcient attention and get the anticipated answers. To
address such a problem, we propose M3NSCT5, a novel approach to automatically generate multiple
post titles from the given code snippets. Developers may take advantage of the generated titles to
ﬁnd closely related posts and complete their problem descriptions. M3NSCT5 employs the CodeT5
backbone, which is a pre-trained Transformer model having an excellent language understanding and
generation ability. To alleviate the ambiguity issue that the same code snippets could be aligned with
diﬀerent titles under varying contexts, we propose the maximal marginal multiple nucleus sampling
strategy to generate multiple high-quality and diverse title candidates at a time for the developers to
choose from. We build a large-scale dataset with 890,000 question posts covering eight programming
languages to validate the eﬀectiveness of M3NSCT5. The automatic evaluation results on the BLEU
and ROUGE metrics demonstrate the superiority of M3NSCT5 over six state-of-the-art baseline mod-
els. Moreover, a human evaluation with trustworthy results also demonstrates the great potential of
our approach for real-world application.

1. Introduction

Stack Overﬂow (SO) is one of the most popular Ques-
tion&Answering websites for developers to seek answers to
programming problems. However, it remains a challenge [4,
32, 37] to help developers write high-quality question posts
that attract enough attention from potential experts. Espe-
cially, non-native English speakers or inexperienced devel-
opers may struggle to clearly describe their encountered prob-
lems, let alone summarize the problems into informative ti-
tles. One way for developers to write better question posts
is to ﬁrst search for related posts with the problematic code
snippets and then complete their problem descriptions and
post titles. Nonetheless, previous studies [12, 52, 29, 13]
demonstrated the unsatisfying performance of the commonly
used retrieval methods like TF-IDF and BM25 [36] on search-
ing related posts with given code snippets. First, such re-
trieval methods calculate the lexical overlap and ignore the
essential semantic similarity. Second, diﬀerent from natu-
ral language queries, code snippets usually have very long
contexts and plentiful user-deﬁned tokens, making it hard to
extract lexical features.

Recently, Gao et al. [12] proposed an end-to-end genera-
tion model to automatically produce post titles with the given
code snippets. First, they train an LSTM (Long Short Term

∗Corresponding author

zhangfengji@whu.edu.cn (F. Zhanga);

jinliu@whu.edu.cn (J. Liua); wanyao@hust.edu.cn (Y. Wanb);
xiaoyu@whut.edu.cn (X. Yuc); xiao.liu@deakin.edu.au (X.
Liud); jacky.keung@cityu.edu.hk (J. Keunge)

ORCID(s):

Memory) [18] model on a large-scale dataset collected from
Stack Overﬂow, which contains pairs of code snippets and
post titles. Then, a developer could provide the model with
code snippets to get a generated post title that summarizes
the problem. The generated titles are coherent and informa-
tive, which will help developers understand their problems
and ﬁnd related posts more easily. However, as suggested
by Liu et al. [29], the same code snippets could be aligned
with diﬀerent titles under varying contexts, which we de-
note as the ambiguity issue. For example, in Figure 1, the
two SO posts ask diﬀerent questions and have diﬀerent ti-
tles. However, they have the same code snippets that imple-
ment the Python function get_client_ip. Liu et al. [29] then
proposed to tackle the issue by leveraging the surrounding
text descriptions in the post body to eliminate the semantic
ambiguity of code snippets. Nonetheless, it remains an open
challenge to generate the expected post titles when develop-
ers cannot provide precise descriptions of their problems.

To mitigate this challenge, we reformulate the title gen-
eration task as generating multiple candidate titles simulta-
neously under the condition that only code snippets are pro-
vided. Since code snippets can be ambiguous without the
surrounding contexts, we could oﬀer the developers an ac-
ceptable amount of candidate titles to choose from. But this
will pose a new challenge of improving the diversity of gen-
erated titles while keeping the quality so that the titles can
nicely summarize the code snippets as well as cover diﬀerent
intentions under varying contexts. To this end, we propose
M3NSCT5, a novel approach to generate high-quality and di-
verse post titles from the given code snippets. M3NSCT5 is

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 1 of 16

 
 
 
 
 
 
Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

both the quality and diversity of generated titles, especially
when the number of output titles is limited to a small value
(≤ 5), making it suitable for real-world application.

RQ-3: What is the performance of our approach un-
der human evaluation? To compensate for the non-intuitive
automatic evaluation metrics, we recruit six experienced pro-
grammers to perform an additional human evaluation. Par-
ticipants are required to score the titles generated by M3NSCT5,
PLBART, and BM25 involving three programming languages
on the 𝑅𝑒𝑎𝑑𝑎𝑏𝑖𝑙𝑖𝑡𝑦, 𝐶𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛, and 𝐷𝑖𝑣𝑒𝑟𝑠𝑖𝑡𝑦 criteria. Re-
sults show that our approach also has better performance un-
der human-centered evaluation.

The contributions of this paper are as follows:

• We propose M3NSCT5, a novel approach combining

the pre-trained CodeT5 model and the maximal marginal
multiple nucleus sampling strategy, which could im-
prove the quality and diversity of generated SO titles.

• We collect a large-scale dataset containing 890,000
high-quality posts covering eight programming lan-
guages and demonstrate the eﬀectiveness of our ap-
proach under automatic and human evaluation at dif-
ferent experimental settings.

• We have released the source code and processed dataset1

to facilitate future research.

We organize the rest of this paper as follows: Section 2
introduces the details of our proposed approach. Section 3
describes the basic setup of our experiment, including the
construction of the experimental dataset, hyper-parameter
settings, baseline models, and evaluation metrics. Section
4 presents the experimental results. Section 5 introduces the
related works. Section 6 discusses threats to the validity of
our work. Finally, we conclude this paper and introduce the
future work in Section 7.

2. The Proposed Approach

Generating post titles from code snippets can be seen as
a PL-to-NL (Programming Language to Natural Language)
generation task. Figure 2 illustrates the overall framework
of our M3NSCT5, a novel end-to-end approach that could
improve the quality and diversity of the post titles generated
from the code snippets. Speciﬁcally, we employ CodeT5 as
the backbone, which takes in the code snippets and gener-
ates post titles. We further incorporate the nucleus sampling
and maximal marginal ranking strategy to produce a set of
high-quality and diverse title candidates. The details of our
approach are described in this section.

2.1. CodeT5 Backbone Model

CodeT5 [48] is a state-of-the-art Transformer model pre-
trained on a large-scale code-related corpus involving multi-
ple programming languages. It inherits the uniﬁed encoder-
decoder architecture from T5 [35], which has been shown
beneﬁcial for generation tasks. We follow the pre-train then

1https://github.com/zfj1998/M3NSCT5

Figure 1: Illustration of the ambiguity issue — posts with the
same code snippets have diﬀerent titles under varying contexts.

a hybrid method combining the Maximal Marginal Multiple
Nucleus Sampling strategy and the CodeT5 model.

Speciﬁcally, M3NSCT5 is based on CodeT5 [48], a Trans-
former [44] encoder-decoder model pre-trained on a large-
scale code-related corpus, which could better capture the
long-range dependencies than LSTM [22] and generate titles
with higher quality. To improve the diversity of generated ti-
tles, we apply the nucleus sampling [19] method instead of
the commonly used beam search during decoding and pro-
pose the maximal marginal ranking strategy to ensure the
quality and diversity of the predicted titles. In this way, we
can tackle the ambiguity issue by oﬀering multiple title can-
didates for developers to choose from.

To verify the eﬀectiveness of our approach, we conduct
the empirical study by raising the following Research Ques-
tions (RQs):

RQ-1: Does our approach outperform state-of-the-
art baselines under automatic evaluation? We build a
large-scale dataset 𝐷𝑠𝑜
with around 890,000 high-quality SO
posts covering eight programming languages. We employ
BLEU [33] and ROUGE [27] as the automatic evaluation
metrics and choose six baseline models (i.e., BM25 [36],
Code2Que [12], BART [23], CCBERT [52], SOTtitle [29],
and PLBART [2]) for comparison. Experimental results show
that M3NSCT5 outperforms all the baselines by a large mar-
gin, having an around 9% improvement over the second best
performing PLBART baseline on average of diﬀerent exper-
imental settings.

RQ-2: How eﬀective is our maximal marginal multi-
ple nucleus sampling? We compare the performance of our
sampling strategy with beam search and vanilla random nu-
cleus sampling. Results show that our method could improve

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 2 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Figure 2: The overall framework of our approach for Stack Overﬂow post title generation. Given the input code snippets,
M3NSCT5 can produce multiple title candidates. There are three critical components inside M3NSCT5, namely the CodeT5
backbone, the nucleus sampling method, and the maximal marginal ranking strategy.

ﬁne-tune paradigm and further update the trainable parame-
ters 𝜃 of CodeT5 on our task-speciﬁc dataset 𝐷𝑠𝑜

.

Fine-Tuning: Our objective is to maximize the probabil-
ity 𝑃𝜃(𝑌 |𝑋) given the input code sequence 𝑋 and the tar-
get title 𝑌 from the training dataset. 𝑋 and 𝑌 are ﬁrst split
into tokens by the default byte-pair encoding tokenizer of
CodeT5, then turned into vectors through the embedding
layer. Especially, if the input contains multiple code snip-
pets, we concatenate them to a long sequence with the ad-
ditional [NEXT] identiﬁer. Suppose 𝑋 = (𝑥1...𝑥
) and
|𝑋|
is the model
𝑌 = (𝑦1...𝑦
hidden size, and
denote the sequence length with
respect to 𝑋 and 𝑌 . We feed 𝑋 to the encoder, which mainly
performs bidirectional self-attention to get

), where 𝑥𝑖, 𝑦𝑗 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 . 𝑑𝑚𝑜𝑑𝑒𝑙

|𝑋|

|𝑌 |

and

|𝑌 |

𝐶 = ENCODER(𝑋),

(1)

where 𝐶 = (𝑐1...𝑐
) and vector 𝑐𝑖 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 is the hidden
representation of the 𝑖th input token. We then feed the auto-
regressive decoder with 𝐶 and 𝑌 to get

|𝑋|

𝐺 = DECODER(𝐶, 𝑌 ),

(2)

|𝑌 |

where 𝐺 = (𝑔1...𝑔
) and vector 𝑔𝑗 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 represents
the hidden state of the 𝑗th predicted token. Next, we employ
an additional neural layer to map 𝐺 from the decoder hid-
den space to the probability distribution over the prediction
vocabulary

𝐏 = 𝐿𝑖𝑛𝑒𝑎𝑟𝑆𝑜𝑓 𝑡𝑚𝑎𝑥(𝐺),

(3)

log-likelihood

𝐿𝑜𝑠𝑠 =

1
|𝑌 |

|𝑌 |∑

𝑗=1

− log 𝑃𝑗(𝑦𝑗),

(4)

where 𝑃𝑗(𝑦𝑗) is the predicted probability of the 𝑗th token in
the target title.

Inference: We employ the already ﬁne-tuned model and
the auto-regressive decoding method to get the predicted title
̂𝑌 token-by-token. To be speciﬁc, we ﬁrst feed the decoder
with the start identiﬁer <s> to generate a probability distri-
over the vocabulary, which is used for sampling
bution 𝑃1
. After that, we will again take
the ﬁrst predicted token ̂𝑦1
(<s>, ̂𝑦1) as the input sequence for the decoder to predict the
second token ̂𝑦2
by repeating the previous steps. Our model
predicts each token in ̂𝑌 recursively until encountering the
ending identiﬁer </s>.

When generating multiple titles, we follow the parallel
manner to save the computation cost. Generally, we ﬁrst take
𝑀 start identiﬁers (<s>1...<s>𝑀 )⊺ as the input for decod-
ing. In return, we get the sampled ﬁrst tokens ( ̂𝑦1,1... ̂𝑦𝑀,1)⊺
for 𝑀 candidates. Through the auto-regressive decoding
method, our model will repeatedly sample tokens at each
step until all the candidates meet the ending identiﬁer </s>.
Finally, we will get the sampled titles

̂𝑌𝑀,𝑁 =

̂𝑦1,1
⎛
̂𝑦2,1
⎜
⎜
⋮
⎜
̂𝑦𝑀,1
⎝

̂𝑦1,2 ⋯ ̂𝑦1,𝑁
̂𝑦2,2 ⋯ ̂𝑦2,𝑁
⋮
⋮ ⋱

̂𝑦𝑀,2 ⋯ ̂𝑦𝑀,𝑁

,

⎞
⎟
⎟
⎟
⎠

(5)

where 𝐏 = (𝑃1...𝑃
is the vocabulary
), 𝑃𝑗 ∈ 𝑅𝑑𝑣𝑜𝑐𝑎𝑏, 𝑑𝑣𝑜𝑐𝑎𝑏
size, and 𝐿𝑖𝑛𝑒𝑎𝑟𝑆𝑜𝑓 𝑡𝑚𝑎𝑥 is a linear neural network with the
𝑠𝑜𝑓 𝑡𝑚𝑎𝑥 activation function. Eventually, we can get the loss
function for ﬁne-tuning by calculating the average negative

|𝑌 |

where ̂𝑦𝑚,𝑛
is the 𝑛th sampled token of the 𝑚th candidate
title, 𝑁 is the length of the longest candidate, and all the
shorter candidates will be padded to length 𝑁 with a special
[PAD] identiﬁer.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 3 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Figure 3: Illustration of applying nucleus sampling to get the
next token in the predicted title.

𝑃𝑟( ̂𝑦𝑟). Nonethe-

2.2. Nucleus Sampling

An essential step of the decoding step is to sample the
predicted token ̂𝑦 from its probability distribution 𝑃 over the
vocabulary. The most common sampling method is beam
search, whose objective is to maximize the probability 𝑃 ( ̂𝑌 )
over the predicted tokens, where 𝑃 ( ̂𝑌 ) = ∏|
less, the content produced by beam search is found to lack di-
vergence compared with the content written by humans [19].
It is because the maximization-based objective always sup-
presses the occurrence of uncommon phrases.

̂𝑌 |
𝑟=1

In this study, we need to ensure the diversity of gener-
ated titles so that they can cover diﬀerent intentions under
varying contexts. To this end, we employ the nucleus sam-
pling [19] method, whose key intuition is to sample the pre-
dicted token from a nucleus distribution instead of choosing
the token with the highest probability. As shown in Figure
3, given the already sampled tokens [‘What’, ‘does’, ‘the’,
‘yield’], we are now going to choose the next token from
the vocabulary distribution. Some tokens in the vocabulary
are unlikely to be chosen, such as [‘execution’, ‘continue’,
. . . , ‘itertools’], which make up the unreliable tail of the dis-
tribution. The tokens in the nucleus, a minimal subset of
the vocabulary that takes up the vast majority of probability
mass, are [‘keyword’, ‘return’, . . . , ‘statement’], which are
most likely to follow the previous token ‘yield’. Using nu-
cleus sampling, any token in the nucleus has the chance to
be chosen, which could bring randomness to the sampling
process and signiﬁcantly improve the diversity of generated
titles.

Formally, suppose we are generating the 𝑟th token ̂𝑦𝑟
us-
ing nucleus sampling, with the probability distribution 𝑃𝑟
over the vocabulary 𝑉 . We ﬁrst ﬁnd the minimal nucleus set
𝑉 (𝑝) ⊂ 𝑉 such that
∑

(6)

𝑃𝑟(𝑣) ≥ 𝛽,

𝑣∈𝑉 (𝑝)

where 𝑣 ∈ 𝑉 and 𝛽 (also denoted as top-𝑝) is a hyper-parameter

Figure 4: Illustration of the maximal marginal ranking strategy.
Nodes marked in four colors denote the title samples grouped
into four clusters based on their distance in the space. This
ﬁgure shows a four-step example of choosing four titles from
all the samples: start from an initial title; the following steps
are to choose the title that has the maximal distance to the
already chosen ones.

of nucleus sampling ranging from 0.0 to 1.0. Let 𝑝′ = ∑
The original distribution 𝑃𝑟

can be re-scaled to

𝑣∈𝑉 (𝑝) 𝑃𝑟(𝑣).

{

𝑃 ′
𝑟 (𝑣) =

𝑃𝑟(𝑣)∕𝑃 ′
0

if 𝑣 ∈ 𝑉 (𝑝)
𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

,

(7)

and ̂𝑦𝑟

will be sampled from the new distribution 𝑃 ′
𝑟

.

2.3. Maximal Marginal Ranking

Nucleus sampling has been successfully applied to the
domain of code generation [24, 5, 11, 17, 51]. For exam-
ple, state-of-the-art code generation models AlphaCode [24]
and OpenAI Codex [5] both incorporate nucleus sampling to
generate hundreds and thousands of candidate code solutions
for each programming problem, which will signiﬁcantly im-
prove the problem-solving rates. This can be attributed to the
randomness brought by the nucleus sampling, which could
enlarge the exploration space of pre-trained models and in-
crease the chance of generating high-quality content. How-
ever, due to the random nature of nucleus sampling, there is
a high variance in generation quality. A common practice to
tackle this issue is to sample multiple times and then choose
the best samples [7, 20, 40]. For example, AlphaCode [24]
employs sophisticated ﬁltering and clustering methods over
the generated code solutions to narrow the number of candi-
dates so that the target programming problem can be solved
within minimum tries.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 4 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

In this study, we propose a simple yet eﬀective maximal
marginal ranking strategy to ensure the diversity and qual-
ity of the ﬁnal predicted titles. We illustrate the rough idea
of our ranking strategy in Figure 4, where the nodes in the
two-dimensional space represent the title samples produced
by nucleus sampling. Furthermore, the nodes (titles) that
are similar should have a closer distance. Our goal is to ﬁnd
the top-ranked titles with good diversity and quality from
all the samples. First, we need to choose a node to start
the ranking process. In the example, we choose the node
(cid:172) from the majority cluster of the red color as the initial
candidate. Second, we choose the yellow node (cid:173) as another
candidate, which has the maximal distance from (cid:172) among
all the nodes. Then, we choose the blue node (cid:174) as the next
candidate, which has the maximal distance from both (cid:172) and
(cid:173). Similarly, we choose the purple node (cid:175) as the fourth can-
didate, which has the maximal distance from all the previ-
ously chosen nodes. In this way, we can include nodes from
diﬀerent clusters to ensure the diversity of chosen titles. The
following introduces the details of our ranking strategy:

Choosing the initial sample: It is crucial to choose a high-
quality initial title to start the ranking process because the
maximal marginal ranking objective only guarantees the di-
versity of chosen titles and is blind to their quality. How-
ever, discriminating the quality of generated titles is a non-
trivial task due to the lack of explicit rules that deﬁne ‘good
quality’. To tackle this problem, we adapt the idea of self-
consistency [46] to facilitate selecting the initial title from
generated samples. The self-consistency was proposed to
improve the performance of reasoning tasks. Generally, af-
ter sampling a set of diverse candidates from the model, the
ﬁnal answer should be the one that is most consistent among
the other generated answers. In this study, we propose to
measure the quality of generated titles through the bigram
consistency. Precisely, we ﬁrst extract all the token bigrams
from the generated titles and then calculate the frequency of
each bigram. Finally, we rank the titles based on the aver-
age frequency of their bigrams, and the top-1 title will be
considered the most promising initial sample.

Choosing the next samples: Suppose we will oﬀer 𝐾 ti-
tles for the developer, our model needs to sample 𝑀 can-
didates for ranking, where 𝑀 >> 𝐾 (e.g., 𝑀 is 200 when
( ̂𝐘𝑆 =
𝐾 is 5). Given the already chosen titles ̂𝐘𝑆 ⊂ ̂𝐘𝑀
( ̂𝑌1... ̂𝑌𝑆 ), 𝑆 < 𝐾), we need to choose the next title ̂𝑌𝑆+1
from the rest of candidates ̂𝐘𝑀 ⧵ ̂𝐘𝑆
. To improve the diver-
sity of chosen titles, we propose to ﬁnd the one that has the
maximal distance from those in ̂𝐘𝑆

,

̂𝑌𝑆+1 = argmax
̂𝑌𝑚∈ ̂𝐘𝑀 ⧵ ̂𝐘𝑆

( ∑

̂𝑌𝑠∈ ̂𝐘𝑆

−𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒( ̂𝑌𝑚, ̂𝑌𝑠)

),

(8)

where 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒( ̂𝑌𝑚, ̂𝑌𝑠) is computed by the cosine similar-
ity of the bag-of-bigram vectors built from the titles. We
repeat this process until the size of ̂𝐘𝑆

reaches 𝐾.

3. Experimental Setup

This section introduces the construction of our dataset,
the implementation of our model, the baselines for perfor-
mance comparison, the automatic evaluation metrics, and
the criteria for human evaluation.

3.1. Data Preparation

Though previous studies [12, 52, 29] have proposed open-
sourced datasets for the SO title generation task, there are
several drawbacks we still have to overcome. Speciﬁcally,
Gao et al. [12] only considered the posts with an interrog-
ative title, which account for less than a third of real-world
data samples, thus resulting in a biased dataset. While both
Zhang et al. [52] and Liu et al. [29] had their published bi-
modal posts stripped and tokenized through natural language
processing tools, which damaged the lexical and structural
information (such as the white spaces and line breaks) of
the code snippets. As a result, we re-construct a large-scale
dataset 𝐷𝑠𝑜
𝐷𝑠𝑜

to perform our experiments.

is built on the SOTorrent dataset proposed by Baltes
et al. [3], which is originally used for analyzing the evolution
of SO posts. The latest checkpoint of SOTorrent contains
all the posts from July 2008 to December 2020. Baltes et
al. [3] extracted the code snippets marked by various nota-
tions from post bodies and reserved all the white spaces, line
breaks, user-deﬁned identiﬁers, etc. They also removed the
noisy fragments wrongly marked as code in the text blocks.
Moreover, the previously proposed datasets [12, 52, 29]
only focus on a few dominant Programming Languages (PLs)
with abundant data samples, such as Python, C#, Java, JS(JavaScript),
and PHP. In this study, we consider the posts involving eight
PLs, including the above popular ones and the minorities (C,
Ruby, and Go). Besides, to ensure the quality of training
data, we only choose the posts that satisfy the four condi-
tions:

1. The post is not closed;
2. The post has an accepted answer;
3. The post gets more than one vote;
4. The post includes code snippets.

Table 1
The number of Train/Validation/Test samples in 𝐷𝑠𝑜 with re-
spect to diﬀerent PLs.

PL

Train

Validation

Test

Python
C#
Java
JS
PHP
C
Ruby
Go

190,934
175,070
162,161
151,540
86,729
29,746
23,774
6,820

5,000
5,000
5,000
5,000
5,000
3,700
3,000
850

5,000
5,000
5,000
5,000
5,000
3,700
3,000
850

Total

826,774

32,550

32,550

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 5 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

As for data partitioning, we separate the ﬁltered posts
in chronological order, where the latest posts are randomly
grouped into validation and test sets, and the rest are for
training. This is reasonable because our model should take
the past data for training and is applied to new questions in
the real-world scenario. We set the number of validation and
test samples to 5,000 with respect to diﬀerent PLs. For the
languages with insuﬃcient data, we set their proportions of
validation and test sets to 10%. In the end, we get the large-
scale and high-quality dataset 𝐷𝑠𝑜
for the SO title generation
task. The statistics of 𝐷𝑠𝑜

is summarized in Table 1.

3.2. Implementation Details

We implement M3NSCT5 with the transformers2 library
and the pre-trained model checkpoint3 of CodeT5, which
consists of 12 encoder layers and 12 decoder layers with a
hidden size of 768. We optimize all the trainable parameters
through AdamW [30], with an initial learning rate of 5×10−5
scheduled by the linear warm-up. We employ the default
byte-pair encoding tokenizer of CodeT5, whose vocabulary
size is 32,100. We have two Tesla V100 (16GB memory)
GPUs for training, where each one could hold a data batch
size of 8. We further increase the overall batch size to 32
by gradient accumulation. The model is set to train for ten
epochs, and we employ the early stopping strategy to avoid
overﬁtting. Finally, we set top-𝑝 in the nucleus sampling to
0.8, temperature to 1, and the number of sampled candidates
to 200 during decoding.

3.3. Baselines

To demonstrate the eﬀectiveness of our approach, we
choose several state-of-the-art baseline methods for compar-
ison. We give a brief introduction to these approaches and
their experimental settings.

(1) BM25 [36] is a widely used ranking function in informa-
tion retrieval systems. It could estimate the relevance of
documents for a given search query. The basic idea of
BM25 is to rank the referencing documents based on the
overlapping query terms, thus ignoring their correlation
within the document. Our study adopts this method to
retrieve the most relevant posts in the training dataset
given the testing code snippets. We could select one or
more best matches for each query as the predicted title
candidates. We take advantage of the ready-to-use Elas-
ticsearch engine4 to implement this retrieval baseline,
whose default similarity ranking algorithm is BM25.
(2) Code2Que was proposed by Gao et al. [12] to gener-
ate SO titles from given code snippets. It is an end-to-
end model with the LSTM [18] encoder-decoder archi-
tecture. Its encoder is a multi-layer bidirectional LSTM
network that sequentially handles the input code tokens,
while its decoder is the single-layer LSTM that recur-
sively returns the predicted tokens. Moreover, Code2Que

2https://huggingface.co/docs/transformers/

index

3https://huggingface.co/Salesforce/codet5-base
4https://www.elastic.co/elasticsearch/

incorporates the copy [38] mechanism to allow the de-
coder to focus on more relevant parts of the input and
facilitate capturing some rare but important tokens, and
the coverage [43] mechanism to discourage generating
meaningless repetitions. We employ the OpenNMT5 li-
brary to reproduce this baseline method.

(3) BART [23] is a pre-trained Transformer model that achieves
state-of-the-art results on a range of NL tasks, especially
abstractive summarization, question answering, and ma-
chine translation. Unlike the previous successful pre-
trained language models BERT [21] (only with the Trans-
former encoder) and GPT [34] (only with the Trans-
former decoder), BART employs a standard encoder-
decoder architecture and proposes specially designed de-
noising objectives for pre-training. As a result, BART
could improve the performance over previous work when
ﬁne-tuned for both text understanding and generation
tasks. We reproduce this baseline using its pre-trained
model checkpoint6.

(4) CCBERT was proposed by Zhang et al. [52], which
is also used for SO title generation but takes bi-modal
content (code snippets and text descriptions in the post
body) as the model input. CCBERT is a Transformer
model equipped with CodeBERT [10] and an additional
copy attention layer. Speciﬁcally, CodeBERT is a Trans-
former encoder pre-trained on a vast scale NL-PL bi-
modal corpus, thus having an excellent ability to parse
the overall context of SO posts. The copy attention layer
is an adapted version of the copy mechanism [38] for
the Transformer architecture, which helps the model fo-
cus on input tokens during decoding. Zhang et al. [52]
showed the superiority of CCBERT over Code2Que and
BART using their collected dataset. We take advantage
of their published source code and the pre-trained model
checkpoint7 of CodeBERT to reproduce this baseline.
(5) SOTitle was proposed by Liu et al. [29], which is an-
other novel approach used for SO title generation. The
backbone of SOTitle is the pre-trained T5 [35] model,
which follows the Transformer encoder-decoder archi-
tecture and employs a transfer learning technique that
uniﬁes all text-based language problems into a text-to-
text paradigm. T5 was pre-trained on a large-scale cor-
pus crawled from the web and achieved state-of-the-art
performance on various NL tasks. Liu et al. [29] ﬁne-
tuned T5 on their collected SO dataset and reported it
could outperform Code2Que and BART. We use their
published source code and the pre-trained model check-
point8 of T5 to reproduce this baseline.

(6) PLBART [2] is a specialized version of the BART model,
whose name is the abbreviation for "Program and Lan-
guage BART". It also employs the Transformer encoder-
decoder architecture and applies denoising objectives for

5https://opennmt.net
6https://huggingface.co/facebook/bart-base
7https://huggingface.co/microsoft/

codebert-base

8https://huggingface.co/t5-base

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 6 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

pre-training. PLBART was proposed to produce multi-
lingual representations applicable to NL-PL understand-
ing and generation tasks. It was pre-trained on a large-
scale bi-modal corpus collected from GitHub and Stack
Overﬂow, then ﬁne-tuned to downstream applications.
Results showed that PLBART could outperform state-
of-the-art models in a wide range of tasks, especially
code summarization and translation. We reproduce this
baseline using its pre-trained model checkpoint9.

3.4. Evaluation Methods

We believe a high-quality post title should have good
readability and a strong correlation with the post body. Man-
ual evaluation is the ideal way to measure these criteria. Nev-
ertheless, considering the tremendous scale of our dataset,
it is necessary to perform an automatic evaluation. An ad-
ditional human evaluation is performed on a small subset of
test samples to demonstrate the intuitive quality of titles gen-
erated by our model.

3.4.1. Automatic Evaluation

Following the previous studies [12, 52, 29], we automati-
cally evaluate the quality of title generation by measuring the
similarity between the generated titles and the original titles
paired with the input code snippets. We employ two kinds of
text similarity measuring methods, namely BLEU [33] and
ROUGE [26].

BLEU originates from machine translation tasks, which
mainly calculates the lexical overlap between sentences through
n-gram precision.
It also incorporates the brevity penalty
to penalize the behavior of generating short sentences for
higher precision scores. In our experiments, we use the BLEU-
4 score calculated with 1/2/3/4-gram. Besides, we apply a
smoothing method introduced by Lin et al. [27] to prevent
negative scores caused by excessive short sentences. We de-
note the smoothed method as BLEUS-4 and take advantage
of the NLTK10 library for implementation.

ROUGE is a set of metrics commonly used in text sum-
marization, mainly focusing on the n-gram recall.
In our
experiments, we employ three ROUGE-family metrics, in-
cluding the ROUGE-1/2 scores that are calculated with 1/2-
gram co-occurrence and the ROUGE-L score that concerns
the longest common subsequence. Moreover, we take ad-
vantage of an open source library11 for implementation.

3.4.2. Human Evaluation

In practice, a high-quality post title can be written in dif-
ferent styles. It is hard to tell the actual quality of a generated
title based on its similarity with a single reference. There-
fore, we perform an additional evaluation on three human-
centered criteria. As described in Table 2, each criterion
can be quantiﬁed by a score number. Speciﬁcally, Readabil-
ity measures the grammaticality and ﬂuency of a title, while
Correlation considers the consistency between a title and its

9https://huggingface.co/uclanlp/plbart-base
10http://www.nltk.org/_modules/nltk/translate/

bleu_score.html

11https://pypi.org/project/rouge

corresponding post body. Diversity is the number of titles
that have distinct meanings.

We recruit six students for human evaluation, asking them
to review the titles generated by M3NSCT5, PLBART, and
BM25. Speciﬁcally, all the participants are experienced pro-
grammers familiar with Stack Overﬂow. We assign each par-
ticipant 100 random post samples where each post is paired
with nine titles generated by the three approaches (i.e., the
output number 𝐾=3). The participants are evenly divided
into three groups according to their preferred programming
languages (including the popular Python and Java languages,
as well as the low-resource Go language). During the eval-
uation, participants do not know the titles are generated by
which approach, and they should tell the Diversity, Read-
ability, and Correlation scores of each sample according to
the scoring standard in Table 2. Then, we take the average
score of each two participants in the same group and report
the results by diﬀerent PLs. Finally, we employ Cohen’s
Kappa [8] to measure the agreement between the two par-
ticipants in each group.

Table 2
The criteria used for human evaluation.

Criteria

Scoring Standard

Readability

Correlation

1 ⇒ Very hard to read and understand
2 ⇒ Just readable and understandable
3 ⇒ Very easy to read and understand

1 ⇒ Totally digress from the key points
2 ⇒ Relevant to the key points
3 ⇒ Exactly match the key points

Diversity

[1,K] ⇒ The number of non-redundant titles

3.4.3. Evaluation on Multiple Outputs

Finally, we introduce the evaluation method when the
model outputs multiple titles for a single input. Suppose
the output number is 𝐾. We ﬁrst calculate the scores of
all the titles on a speciﬁc Metric and then take the highest
score as the result, which is denoted as Metric@𝐾. In this
way, we can get the BLEU𝐾, ROUGE@𝐾, Readability@𝐾,
Correlation@𝐾, and Diversity@𝐾 that are used for our ex-
periments.

3.5. Research Questions

We demonstrate the eﬀectiveness of our model by con-
ducting experiments to answer the following Research Ques-
tions (RQs):

RQ-1 Does our approach outperform state-of-the-art base-

lines under automatic evaluation?
Motivation: In section 3.3, we have introduced sev-
eral state-of-the-art models proposed for the SO ti-
tle generation task (i.e., Code2Que, CCBERT, and
SOTitle) as well as the promising approaches for this
task (i.e., BM25, BART, and PLBART). This research
question explores whether our model could improve

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 7 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Table 3
The automatic evaluation results of M3NSCT5 and six baselines on the test dataset with
respect to diﬀerent PLs. The values in the table are the average scores, and 𝐾 is the
number of output titles. B4, R1, R2, and RL are the abbreviations of BLEUS-4, ROUGE-
1, ROUGE-2, and ROUGE-L.

(a) Python

(b) C#

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.95
12.06
12.76
12.98
12.90
13.05
13.34

11.42
23.07
24.98
25.66
25.45
26.55
28.65

1.53
6.61
7.56
8.12
7.85
8.50
9.68

10.70
22.17
23.13
24.15
23.63
24.69
26.44

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.36
9.91
11.48
11.05
11.52
11.61
12.16

9.93
17.45
20.95
20.30
20.91
22.58
25.06

1.85
5.05
6.81
6.79
6.67
7.73
9.10

9.53
17.55
19.99
19.62
19.98
21.68
23.75

(c) Java

(d) JavaScript

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.43
10.51
11.53
11.46
11.63
11.72
12.37

10.68
19.49
22.32
22.13
22.55
24.14
26.07

1.49
5.24
6.48
6.89
6.58
7.56
8.60

10.14
19.25
21.11
21.23
21.36
22.94
24.46

(e) PHP

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

10.57
20.83
23.45
23.84
23.73
24.88
26.96

6.60
11.29
12.21
12.36
12.34
12.50
12.74

(f) C

1.43
5.78
6.68
7.21
6.89
7.58
8.53

10.03
20.44
22.15
22.63
22.48
23.65
25.25

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

7.72
11.14
12.24
12.46
12.40
12.48
12.91

12.15
19.97
22.94
23.04
22.87
24.06
25.86

1.53
5.14
5.75
6.26
5.76
6.53
7.45

11.27
19.28
21.29
21.50
21.14
22.45
23.82

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.32
9.24
10.68
10.75
10.91
10.98
11.49

10.07
16.52
19.83
20.15
20.30
21.67
24.18

1.42
4.22
5.62
5.84
5.69
6.48
7.68

9.62
16.40
18.97
19.36
19.42
20.73
22.85

(g) Ruby

(h) Go

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.87
11.24
12.74
12.80
12.60
12.92
13.08

10.98
20.34
23.60
24.36
23.59
24.41
26.77

1.44
5.72
7.04
8.00
7.13
7.59
9.31

10.31
19.73
22.08
23.12
22.11
22.92
25.13

𝐾=1

BM25

Code2Que

BART

CCBERT

SOTitle

PLBART
M3NSCT5

6.74
10.66
12.45
12.54
12.66
12.82
13.21

10.56
18.84
22.63
22.61
22.70
23.78
25.57

1.40
4.76
6.44
7.22
6.44
7.44
8.85

9.87
19.00
21.52
21.76
21.36
22.84
24.50

the quality of generated titles compared with the ex-
isting methods.

RQ-2 How eﬀective is our maximal marginal multiple

nucleus sampling?
Motivation: Apart from applying CodeT5 as our back-
bone, the novelty of M3NSCT5 mainly lies in our
elaborate sampling strategy. We use the nucleus sam-
pling instead of beam search and propose the maximal
marginal ranking for further performance improve-
ment. This research question aims to investigate the
eﬀectiveness of our sampling strategy.

RQ-3 What is the performance of our approach under

human evaluation?
Motivation: Automatic metrics mainly evaluate the
similarity between the generated titles and the given
references. Nevertheless, such similarity does not nec-
essarily correlate to human perceptible quality. This
research question aims to demonstrate the intuitive
quality of generated titles through human evaluation.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 8 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Table 4
The automatic evaluation results of M3NSCT5, PLBART, and BM25 on the test dataset
when 𝐾 > 1, where 𝐾 is the number of output titles. The values in the table are the
average scores of the best title among the 𝐾 title candidates. B4, R1, R2, and RL are the
abbreviations of BLEUS-4, ROUGE-1, ROUGE-2, and ROUGE-L.

(a) Python

(b) C#

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

11.18
15.11
15.94

12.72
16.17
17.08

19.26
30.81
33.42

22.55
33.09
35.58

3.56
10.76
11.93

4.87
12.06
13.28

17.95
28.67
30.96

21.00
30.86
33.05

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

10.81
14.44
14.87

12.52
15.57
16.06

17.37
27.02
29.54

20.85
29.29
31.75

4.07
9.89
10.87

5.50
11.26
12.14

16.66
25.84
27.97

19.92
28.00
30.09

(c) Java

(d) JavaScript

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

10.67
14.47
14.99

12.41
15.47
16.21

18.12
28.25
30.71

21.60
30.45
32.94

3.24
9.42
10.49

4.56
10.61
11.80

17.14
26.73
28.89

20.46
28.86
30.99

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

(e) PHP

18.02
29.44
31.46

21.19
31.68
33.64

10.87
14.69
15.15

12.49
15.80
16.25

(f) C

3.33
9.68
10.46

4.50
11.01
11.72

17.02
27.74
29.53

20.01
29.84
31.65

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

12.08
14.67
15.66

13.75
15.76
16.97

19.93
28.99
31.75

23.39
31.16
34.53

3.46
8.61
9.81

4.93
9.75
11.46

18.38
26.96
29.32

21.60
29.04
31.84

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

10.72
13.62
14.09

12.49
14.78
15.40

17.61
26.33
28.79

20.96
28.61
31.22

3.28
8.49
9.48

4.49
9.73
10.73

16.76
25.09
27.21

19.83
27.25
29.46

(g) Ruby

(h) Go

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

Setting

Model

B4@𝐾 R1@𝐾 R2@𝐾 RL@𝐾

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

11.04
15.24
16.17

12.84
16.98
17.63

18.28
29.63
32.16

21.82
31.90
34.77

3.40
10.34
11.49

4.81
11.84
13.14

17.18
27.86
30.25

20.51
30.06
32.70

𝐾=3

𝐾=5

BM25

PLBART
M3NSCT5

BM25

PLBART
M3NSCT5

11.56
15.26
16.22

13.11
16.48
17.69

18.30
28.58
30.80

21.58
30.64
33.60

3.42
9.19
10.97

4.71
10.23
12.23

17.30
27.13
29.39

20.33
29.12
31.74

4. Results and Analysis
4.1. RQ-1: Does our approach outperform

state-of-the-art baselines under automatic
evaluation?

Methods: We compare M3NSCT5 with six state-of-the-art
baselines on the four automatic evaluation metrics (i.e., BLEUS-
4, ROUGE-1, ROUGE-2, and ROUGE-L). Since we pro-
pose to overcome the ambiguity issue by sampling multiple
times, we also experiment with the number of outputs 𝐾=3
and 𝐾=5. All the models (except for BM25) are trained on
the whole training set of our 𝐷𝑠𝑜
dataset that covers eight
PLs and tested on individual subsets of diﬀerent PLs. The

experimental results of RQ-1 are shown in Table 3 and Table
4.
Results: Based on the results, we can conclude that M3NSCT5
achieves the best performance under automatic evaluation,
outperforming all the baseline models. Speciﬁcally, we have
the following ﬁndings:

(1) According to Table 3, when all the models output only
one candidate (i.e., 𝐾=1), M3NSCT5 could achieve the
best performance. Among all the baselines, the retrieval
method BM25 has the worst performance. The LSTM-
based Code2Que outperforms BM25 by a large margin
but is no match for large pre-trained Transformer mod-
els, which aligns with the results of the previous study [29].

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 9 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

(a) BLEUS-4@𝐾

(b) ROUGE-1@𝐾

(c) ROUGE-2@𝐾

(d) ROUGE-L@𝐾

Figure 5: Automatic evaluation results of equipping the ﬁne-tuned CodeT5 model with three sampling strategies.

We also ﬁnd that BART, CCBERT, and SOTitle share
similar results on diﬀerent PL subsets, where all of them
are worse than PLBART. We attribute the good perfor-
mance of PLBART to its generation-oriented denoising
objectives and code-related corpus for pre-training. Fur-
thermore, M3NSCT5 outperforms PLBART by 3.3%,
8.8%, 16.5%, and 7.9% in terms of BLEUS-4, ROUGE-
1, ROUGE-2, and ROUGE-L on average of diﬀerent PL
subsets.

(2) According to Table 4, when all the models output mul-
tiple candidates (𝐾=3 and 𝐾=5), M3NSCT5 could also
signiﬁcantly improve the performance as well as outper-
form other baselines. We choose BM25 for compari-
son because returning multiple candidates for a query is
common in the ﬁeld of information retrieval. We also
compare with PLBART, which shares a similar model
architecture with CodeT5 and has the most competitive
results when 𝐾=1.
As shown in Table 4, increasing the output number 𝐾
(i.e., oﬀering more title candidates for the developers to
choose from) boosts the performance of all models by a

large margin. In particular, when 𝐾 changes from 1 to
3, M3NSCT5 performs 21.5%, 18.9%, 23.7%, and 19.1%
better in terms of BLEUS-4, ROUGE-1, ROUGE-2, and
ROUGE-L on average of diﬀerent PL subsets. As for the
baselines, BM25 remains the worst performance. Though
PLBART achieves acceptable results, M3NSCT5 still
outperforms it by 4.7%, 8.6%, 12%, and 8.1% in terms
of BLEUS-4, ROUGE-1, ROUGE-2, and ROUGE-L on
average when 𝐾=3, and by 4.9%, 8.6%, 11.7%, and 7.9%
in terms of BLEUS-4, ROUGE-1, ROUGE-2, and ROUGE-
L on average when 𝐾=5.

(3) According to the sub-tables of diﬀerent PLs in Table 3
and Table 4, M3NSCT5 achieves excellent performance
on every PL subset. Surprisingly, the results on less
popular PLs like C, Ruby, and Go are totally compa-
rable to the dominant ones. This ﬁnding also applies
to other baselines, where models never pre-trained on
code-related corpus like Code2Que, BART and SOTi-
tle can perform well on all subsets. Though PLBART
has only been pre-trained on Python and Java corpus,
it can achieve quite competitive results to our model on

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 10 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Table 5
Human evaluation results of M3NSCT5, PLBART, and BM25 on three programming lan-
guages when 𝐾 = 3. S-1, S-2, and S-3 represent the percentage of samples rated to score
1, 2, and 3. S-Avg represents the average score of title candidates. The numbers in the
table are the mean values of two participants in the same group.

Language

Criteria(@3)

M3NSCT5
S-3
S-2

S-1

PLBART

BM25

S-Avg

S-1

S-2

S-3

S-Avg

S-1

S-2

S-3

S-Avg

Python

Go

Java

Readability
Diversity
Correlation

Readability
Diversity
Correlation

Readability
Diversity
Correlation

-

23% 77% 2.77
3% 33% 64% 2.61
11% 52% 37% 2.26

-

24% 76% 2.76
17% 49% 34% 2.17
19% 49% 32% 2.13

-

39% 61% 2.61
4% 39% 57% 2.53
12% 55% 33% 2.21

-

38% 62% 2.62
21% 51% 28% 2.07
19% 54% 27% 2.08

-

31% 69% 2.69
3% 36% 61% 2.58
10% 56% 34% 2.24

-

32% 68% 2.68
18% 49% 33% 2.15
19% 51% 30% 2.11

-

13% 87% 2.87
5% 35% 60% 2.55
1.32
68% 32%

-

-

16% 84% 2.84
3% 37% 60% 2.57
1.29
71% 29%

-

-

14% 86% 2.86
4% 36% 60% 2.56
1.31
69% 31%

-

other PLs. All the evidence indicates that models can
beneﬁt from ﬁne-tuning on the joint dataset of diﬀerent
PLs, and it is applicable to introduce new PLs with less
suﬃcient data to the SO title generation task.

Answer to RQ-1: Our proposed M3NSCT5 can out-
perform state-of-the-art baselines on automatic eval-
uation by generating titles of higher quality under
diﬀerent experimental settings.

4.2. RQ-2: How eﬀective is our maximal marginal

multiple nucleus sampling?

Methods: Remaining the already ﬁne-tuned CodeT5 un-
changed, we compare the performance of the three sampling
strategies when 𝐾 varies from 1 to 20. First, we use the
vanilla Beam Search (BS) method, which ranks the gener-
ated candidates by the combinatorial probability of their to-
kens. We set the beam size to 20 and select the top 𝐾 candi-
dates as output. Second, we repeat Random Nucleus Sam-
pling (RNS) 𝐾 times, then take the sampled candidates as
output. The third is our proposed strategy, which mainly per-
forms Maximal Marginal Nucleus Sampling (MMNS). The
experimental results of RQ-2 are shown in Figure 5, where
the sub-ﬁgures individually demonstrate the performance of
the three sampling strategies on each automatic evaluation
metric, averaged on eight PL subsets.
Results: From the results, we can easily ﬁnd the superiority
of our sampling strategy, especially when 𝐾 ≤ 5. Speciﬁ-
cally, we have the following ﬁndings:

(1) The performance of RNS has a large ﬂuctuation range
and is highly sensitive to the value of 𝐾. While BS has
a more moderate performance improvement than RNS
when 𝐾 increases. Considering the user scenario of the
SO title generation task, it is applicable when 𝐾 takes
small values, e.g., recommending at most 5 title candi-
dates for the developer. When 𝐾 ≤ 2, BS outperforms

RNS, indicating the titles generated with higher prob-
ability are of better quality. When 3 ≤ 𝐾 ≤ 5, BS
performs worse than RNS, showing that ranking titles
purely on probability could damage the trait of diversity.
(2) Our MMNS strategy takes advantage of nucleus sam-
pling and maximal marginal ranking to increase the di-
versity of output titles. It also incorporates self-consistency
voting to obtain the most promising title with higher
quality. As a result, when 𝐾=1, MMNS outperforms
BS (and RNS) by 9%, 14.6%, 29.2%, and 14.1% (by
18.2%, 27.4%, 74%, and 29.5%) in terms of BLEUS-
4, ROUGE-1, ROUGE-2, and ROUGE-L. When 𝐾=3,
MMNS outperforms RNS (and BS) by 3.5%, 4.5%, 12.1%,
and 5.1% (by 2.3%, 7.9%, 10.5%, and 7.3%) in terms
of BLEUS-4, ROUGE-1, ROUGE-2, and ROUGE-L.
When 𝐾=5, MMNS is still comparable to RNS.

Answer to RQ-2: Our sampling strategy can im-
prove the quality and diversity of generated titles,
especially when 𝐾 ≤ 5, making it more suitable and
eﬀective for the SO title generation task.

4.3. RQ-3: What is the performance of our
approach under human evaluation?

Methods: As introduced in Section 3.4.2, we recruit six stu-
dents for human evaluation. Participants are divided into
three groups and are required to score the titles generated by
M3NSCT5, PLBART, and BM25 on three PLs. Finally, we
take the average score of each two participants in the same
group and report the results by diﬀerent PLs. We also em-
ploy Cohen’s Kappa [8] to measure the agreement between
the two participants in each group with respect to the lan-
guages, criteria, and models. The main evaluation results of
RQ-3 are shown in Table 5 and the Cohen’s Kappa statistics
are summarized in Table 6 and Table 7. In addition, we put
some examples in Table 8 to demonstrate the quality and di-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 11 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

versity of the titles generated by the three approaches.

Results: From the results, we can ﬁnd that M3NSCT5 has
the best performance in terms of Diversity and Correlation
compared with the other two approaches. And the perfor-
mance of M3NSCT5 is stable among diﬀerent programming
languages, even for the low-resource 𝐺𝑜 subset. Moreover,
the participants have a substantial or nearly perfect agree-
ment according to Cohen’s Kappa statistics, which validates
the trustworthiness of our human evaluation. Speciﬁcally,
we have the following ﬁndings:

(1) In terms of the Readability criterion, BM25 achieves
the best performance because it just returns the human-
written titles retrieved from the training set. Besides,
both M3NSCT5 and PLBART can achieve a competi-
tive score ≥ 2.6 on three PLs, indicating the capability of
pre-trained models on natural language generation. The
examples in Table 8 also demonstrate the good readabil-
ity of generated titles.

(2) In terms of the Diversity criterion, both M3NSCT5 and
BM25 have good results, having an average number of
distinct titles ≥ 2.5 when 𝐾 = 3. BM25 performs
well because there are almost no duplicate posts in the
training set. The excellent performance of M3NSCT5
should attribute to our elaborate sampling strategy that
maximizes the diﬀerence between the output titles. In
contrast, the poor performance of PLBART on Diver-
sity should blame on the beam search sampling method.
From the examples in Table 8, we can ﬁnd that the titles
generated by PLBART have higher lexical and semantic
overlap than our approach.

(3) As for the Correlation criterion, M3NSCT5 outperforms
PLBART and BM25, having around 90% samples rele-
vant to or exactly matching the key points of original
posts. It shows the feasibility of inferring user intents
from code snippets. We may attribute the excellent per-
formance of M3NSCT5 to the initial choice of high-quality
titles based on self-consistency when performing the max-
imal marginal ranking. We can see from Table 8 that
BM25 is totally oﬀ the point and even recommends PHP
posts for the Go code snippets because of their high lex-
ical overlap, indicating the limitations of the retrieval
method. The titles generated by PLBART are relevant
to the posts but still missing the points. At the same
time, M3NSCT5 shows a good ability to understand the
code snippets and generate diverse title candidates, with
the best candidate closely related to the post.

Answer to RQ-3: Our approach shows a strong abil-
ity to generate post titles with high quality and diver-
sity on diﬀerent programming languages under hu-
man evaluation.

Table 6
The interpretation of Cohen’s Kappa agreement.

Cohen’s Kappa

Interpretation

0%
1% ∼ 20%
21% ∼ 40%
41% ∼ 60%
61% ∼ 80%
81% ∼ 99%
100%

No agreement
Slight agreement
Fair agreement
Moderate agreement
Substantial agreement
Near perfect agreement
Perfect agreement

Table 7
The agreement values of human evaluation results.

Language

Criteria(@3) M3NSCT5

PLBART BM25

Python

Go

Java

Readability
Diversity
Correlation

Readability
Diversity
Correlation

Readability
Diversity
Correlation

83.1%
79.4%
79.6%

79.2%
81.0%
82.7%

76.9%
80.1%
85.8%

78.3%
83.9%
84.0%

74.9%
77.6%
76.9%

81.8%
77.7%
80.6%

73.7%
84.5%
81.8%

85.2%
80.3%
76.0%

83.4%
84.4%
86.0%

5. Related Work

A Stack Overﬂow post usually consists of three parts:
body, title, and tags. Previous studies on the tag recommen-
dation task demonstrated that one could utilize the recom-
mended tags for post retrieval, similar to our motivation. But
the discrete tags are more suitable for post classiﬁcation than
serving as search queries.
In comparison, a coherent and
informative post title can better help developers understand
the problem and search for related posts. Generating post
titles from code snippets can be seen as a specialized PL-
to-NL translation task. Another closely related task is code
summarization, an emerging research direction in software
engineering and natural language processing. This section
introduces the previous studies of post title generation, tag
recommendation, and the recent advances in code summa-
rization.

Title Generation: Gao et al. [12] ﬁrst proposed the SO title
generation task to help improve the quality of poorly written
question posts. They trained an LSTM network that equips
with the copy [38] and coverage [43] mechanisms to gener-
ate titles from mined code snippets. Later, Zhang et al. [52]
and Liu et al. [29] found that taking advantage of both code
snippets and text descriptions in the post body could sig-
niﬁcantly improve the quality of generated titles. Though
utilizing the natural language descriptions could reduce the
ambiguity of the context, it is less helpful when develop-
ers cannot provide good question descriptions. Therefore,
in this study, we focus on the application scenario in which
only code snippets are available. We propose M3NSCT5 to

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 12 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

Table 8
Example input code snippets and the titles generated by M3NSCT5, PLBART, and BM25.
URL links of the original posts and those retrieved by BM25 are listed.

Code Snippets

Titles

Example — Python Language:
letter_list = [’a’,’d’,’o’,’m’,’s’]
>>> df
ID WORD
’yellow’
1
’orange’
2
’green’
3
4
’blue’
>>> expected output
ID WORD
’green’
3
’blue’
4

Origin:
ﬁlter dataframe for words which do not contain any of the letters in a list
M3NSCT5:
- how to ﬁlter rows in pandas dataframe based on a list of letters
- pandas - replace rows with values of list from another list
- pandas : groupby by letters of string
PLBART:
- use pandas to group words from a list
- how to group elements of a list in pandas
- how to group by a second list in pandas
BM25:
- pandas merge creates unwanted duplicate entries
- using nested loops in order to group data in a csv
- sort a list with known values before unknown values

Example — Java Language:
try(Scanner scan = new Scanner(System.in)) {

String s = scan.nextLine();
System.out.println(s);
scan = null;

}

Example — Go Language:
{{range $m := .messages}}

<div>Message subject: {{$m.Subject}}</div>
{{$lastMsg := $m}}

{{end}}
<div>

The last message’s subject: {{$lasMsg.Subject}}

</div>

undeﬁned variable “$lastMsg”

unexpected “:=” in operand

Origin:
try with resource why cannot modify resource
M3NSCT5:
- how to close a scanner in a try-with-resources block
- why is the scanner null after the ﬁrst line
- java scanner.nextline() not working as expected
PLBART:
- will it be a nullpointerexception in try-with-resource
- java scanner() throws nullpointerexception
- how do I prevent a nullpointerexception from being thrown
BM25:
- copying characters in a string
- java how to parse for words in a string for a speciﬁc word
- how to terminate scanner when input is complete

Origin:
how to use template variable outside a loop
M3NSCT5:
- how to access a variable in a range in go template
- golang nested loop variable not found
- why does {{$variable}} not work in this case
PLBART:
- go range variables in html template
- go templates, use variable in range
- accessing a slice value inside a range in go
BM25:
- rails: form in partial for new nested resource
- how to use visual studio - generated async wcf calls
- how do i secure this php script

further improve the quality and diversity of generated titles
compared with previous approaches.

Tag Recommendation: Post tags are vital for Stack Over-
ﬂow, which are helpful in organizing relevant posts. Nev-
ertheless, poorly chosen tags may cause severe redundancy
over time. To tackle this challenge, early studies [49, 45, 47,
54, 28] proposed to automatically recommend tags with the
given post body, title, and user proﬁle through feature extrac-
tion and similarity-based methods. Recently, Zhou et al. [53]
and Xu et al. [50] introduced end-to-end deep learning mod-
els for this task, which could achieve better performance.
Moreover, Devine et al. [9] and He et al. [16] proposed to

take advantage of pre-trained models for further improve-
ment. However, it remains unexplored to recommend post
tags with only code snippets. We believe tag recommenda-
tion and title generation models can be combined for post
retrieval, which is a direction of our future work.

Code Summarization: The Code summarization task is to
generate readable descriptions of the given program, aiming
to save the eﬀort of developers on program comprehension.
With the emergence of large-scale NL-PL bi-modal datasets,
it becomes feasible to train deep Transformer models to gen-
erate high-quality code summaries. Ahmad et al. [1] ﬁrst
employed the Transformer encoder-decoder model to han-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 13 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

dle the long-range dependencies between code tokens and
outperformed previous LSTM-based models by a large mar-
gin. Then, a number of follow-up studies [41, 42, 39, 15, 55,
25, 6, 56] proposed to incorporate the structural informa-
tion by parsing the source code into abstract syntax trees or
control ﬂow graphs to improve the performance. Since code
snippets extracted from SO posts are always problematic, we
cannot apply static parsing techniques to get the syntax trees
or control ﬂow graphs. Therefore, as mentioned in Section
3.1, we try to reserve the structural information by keeping
the white spaces and line breaks in code snippets. Further-
more, some other studies [31, 10, 2, 48, 14, 11] proposed
to utilize the large-scale unlabelled data and self-supervised
learning to pre-train models through self-supervised objec-
tives and achieved state-of-the-art results on code summa-
rization benchmarks. Motivated by the good performance
of pre-training, we employ the pre-trained CodeT5 model as
our backbone.

6. Threats to Validity

This section reveals the potential threats that may aﬀect
the reproduction of our experiments and the validation of our
results.

The threats to internal validity mainly relate to the im-
plementation of baseline models. For CCBERT and SOTi-
tle that already have released source code and model check-
points, we keep their default hyper-parameters unchanged in
our experiments. For BM25, Code2Que, BART, and PLBART,
which have no oﬀ-the-shelf implementations, we take ad-
vantage of the widely used libraries (i.e., Elasticsearch, Open-
NMT, and transformers) for reproduction and tune their hyper-
parameters to the best on our dataset. In this way, we make
sure the comparison between our model and the baselines is
fair. And we release our implementations of the baselines to
facilitate future studies.

The threats to external validity mainly relate to the
construction of our dataset. We have tried our best to en-
sure the quality of our dataset. First, we utilize the already
processed dataset SOTorrent to avoid potential bugs when
extracting code snippets from the post body. Second, we
only include the ﬁltered high-quality posts in the dataset to
reduce the noise of training and test data. Third, our dataset
covers eight programming languages, including the minori-
ties (Ruby and GO), which would better test the generaliza-
tion ability of models. Moreover, we split the train and test
sets in chronological order to ﬁt real-world scenarios. We
also release our dataset for validation and reproduction.

The threats to construct validity mainly relate to the
evaluation methods. Though BLEU and ROUGE are the
most popular evaluation metrics for generation tasks, mea-
suring the quality of the generated content remains an open
challenge. In the SO title generation task, there is no golden
title for a given post, which makes it unfair to judge the qual-
ity of generated titles by comparing them with the only refer-
ence title. Therefore, we perform an additional human eval-
uation to show the intuitive quality of generated titles. To
perform a comprehensive study, we invite six participants to

evaluate the posts covering three programming languages.

7. Conclusion and Future Work

In this paper, we proposed M3NSCT5, a novel approach
to automatically generate Stack Overﬂow post titles from
the given code snippets, which can help non-native English
speakers or inexperienced developers improve their poorly
written question posts. Combining the pre-trained CodeT5
model and the maximal marginal multiple nucleus sampling
strategy, M3NSCT5 can generate high-quality and diverse ti-
tle candidates for the developers to choose from. To validate
the eﬀectiveness of our approach, we have built a large-scale
dataset with 890,000 posts covering eight programming lan-
guages and choose six state-of-the-art baselines for compar-
ison. We performed extensive experiments to demonstrate
the superiority of our approach, including an automatic eval-
uation on the BLEU and ROUGE metrics and a human eval-
uation using the Readability, Correlation, and Diversity cri-
teria. Results showed that M3NSCT5 outperforms all the
baseline methods by a signiﬁcant margin and has great po-
tential for real-world application.

For future work, we plan to further incorporate more
powerful pre-trained language models and tag recommen-
dation methods to improve the title generation task perfor-
mance. Moreover, we plan to deploy our model as web ser-
vices so that real-world developers from Stack Overﬂow could
beneﬁt from our work and produce valuable user feedback
for future improvement.

References
[1] Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2020. A
transformer-based approach for source code summarization, in: Pro-
ceedings of the 58th Annual Meeting of the Association for Compu-
tational Linguistics, pp. 4998–5007.

[2] Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Uniﬁed
pre-training for program understanding and generation, in: Proceed-
ings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Tech-
nologies, pp. 2655–2668.

[3] Baltes, S., Dumani, L., Treude, C., Diehl, S., 2018. Sotorrent: Re-
constructing and analyzing the evolution of stack overﬂow posts, in:
Proceedings of the 15th international conference on mining software
repositories, pp. 319–330.

[4] Chatterjee, P., Kong, M., Pollock, L., 2020. Finding help with pro-
gramming errors: An exploratory study of novice software engineers’
focus in stack overﬂow posts. Journal of Systems and Software 159,
110454.

[5] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan,
J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021.
Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 .

[6] Cheng, W., Hu, P., Wei, S., Mo, R., 2022. Keyword-guided abstrac-
tive code summarization via incorporating structural and contextual
information. Information and Software Technology 150, 106987.
[7] Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse,
C., Schulman, J., 2021. Training veriﬁers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .

[8] Cohen, J., 1960. A coeﬃcient of agreement for nominal scales. Edu-

cational and psychological measurement 20, 37–46.

[9] Devine, P., Blincoe, K., 2022. Unsupervised extreme multi label
classiﬁcation of stack overﬂow posts. 2022 IEEE/ACM 1st Interna-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 14 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

tional Workshop on Natural Language-Based Software Engineering
(NLBSE) , 1–8.

[10] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou,
L., Qin, B., Liu, T., Jiang, D., et al., 2020. Codebert: A pre-trained
model for programming and natural languages, in: Findings of the
Association for Computational Linguistics: EMNLP 2020, pp. 1536–
1547.

[11] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F.,
Zhong, R., Yih, W.t., Zettlemoyer, L., Lewis, M., 2022.
Incoder:
A generative model for code inﬁlling and synthesis. arXiv preprint
arXiv:2204.05999 .

[12] Gao, Z., Xia, X., Grundy, J., Lo, D., Li, Y.F., 2020. Generating ques-
tion titles for stack overﬂow from mined code snippets. ACM Transac-
tions on Software Engineering and Methodology (TOSEM) 29, 1–37.
[13] Gao, Z., Xia, X., Lo, D., Grundy, J.C., Zhang, X., Xing, Z., 2022. I
know what you are searching for: Code snippet recommendation from
stack overﬂow posts. ACM Transactions on Software Engineering
and Methodology doi:10.1145/3550150.

[14] Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J., 2022a.
Unixcoder: Uniﬁed cross-modal pre-training for code representation.
arXiv preprint arXiv:2203.03850 .

[15] Guo, J., Liu, J., Wan, Y., Li, L., Zhou, P., 2022b. Modeling hierarchi-
cal syntax structure with triplet position for source code summariza-
tion, in: Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 486–
500.

[16] He, J., Xu, B., Yang, Z., Han, D., Yang, C., Lo, D., 2022. Ptm4tag:
Sharpening tag recommendation of stack overﬂow posts with pre-
trained models. 2022 IEEE/ACM 30th International Conference on
Program Comprehension (ICPC) , 1–11.

[17] Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A.,
Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al., 2021.
Measuring coding challenge competence with apps. arXiv preprint
arXiv:2105.09938 .

[18] Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory.

Neural computation 9, 1735–1780.

[19] Holtzman, A., Buys, J., Du, L., Forbes, M., Choi, Y., 2019. The curi-
ous case of neural text degeneration. arXiv preprint arXiv:1904.09751
.

[20] Inala, J.P., Wang, C., Yang, M., Codas, A., Encarnación, M., Lahiri,
S.K., Musuvathi, M., Gao, J., 2022. Fault-aware neural code rankers.
arXiv preprint arXiv:2206.03865 .

[21] Kenton, J.D.M.W.C., Toutanova, L.K., 2019. Bert: Pre-training of
deep bidirectional transformers for language understanding, in: Pro-
ceedings of NAACL-HLT, pp. 4171–4186.

[22] Khandelwal, U., He, H., Qi, P., Jurafsky, D., 2018. Sharp nearby,
fuzzy far away: How neural language models use context, in: Pro-
ceedings of the 56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pp. 284–294.

[23] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A.,
Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. Bart: Denoising
sequence-to-sequence pre-training for natural language generation,
translation, and comprehension, in: Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 7871–
7880.

[24] Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond,
R., Eccles, T., Keeling, J., Gimeno, F., Lago, A.D., et al., 2022.
Competition-level code generation with alphacode. arXiv preprint
arXiv:2203.07814 .

[25] Li, Z., Wu, Y., Peng, B., Chen, X., Sun, Z., Liu, Y., Yu, D., 2021.
Secnn: A semantic cnn parser for code comment generation. Journal
of Systems and Software 181, 111036.

[26] Lin, C.Y., 2004. ROUGE: A package for automatic evaluation of
summaries, in: Text Summarization Branches Out, Association for
Computational Linguistics. pp. 74–81.

[27] Lin, C.Y., Och, F.J., 2004. Orange: a method for evaluating automatic
evaluation metrics for machine translation, in: Proceedings of the 20th
International Conference on Computational Linguistics, pp. 501–507.

[28] Liu, J., Zhou, P., Yang, Z., Liu, X., Grundy, J.C., 2018. Fasttagrec:
fast tag recommendation for software information sites. Automated
Software Engineering 25, 675–701.

[29] Liu, K., Yang, G., Chen, X., Yu, C., 2022. Sotitle: A transformer-
based post title generation approach for stack overﬂow. arXiv preprint
arXiv:2202.09789 .

[30] Loshchilov, I., Hutter, F., 2017. Decoupled weight decay regulariza-

tion. arXiv preprint arXiv:1711.05101 .

[31] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A.,
Clement, C., Drain, D., Jiang, D., Tang, D., et al., 2021. Codexglue:
A machine learning benchmark dataset for code understanding and
generation. arXiv preprint arXiv:2102.04664 .

[32] Mondal, S., Saifullah, C.K., Bhattacharjee, A., Rahman, M.M., Roy,
C.K., 2021. Early detection and guidelines to improve unanswered
questions on stack overﬂow, in: 14th Innovations in Software Engi-
neering Conference (formerly known as India Software Engineering
Conference), pp. 1–11.

[33] Papineni, K., Roukos, S., Ward, T., Zhu, W.J., 2002. Bleu: a method
for automatic evaluation of machine translation, in: Proceedings of
the 40th annual meeting of the Association for Computational Lin-
guistics, pp. 311–318.

[34] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., . Improving

language understanding by generative pre-training .

[35] Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
Zhou, Y., Li, W., Liu, P.J., 2020. Exploring the limits of transfer
learning with a uniﬁed text-to-text transformer. Journal of Machine
Learning Research 21, 1–67.

[36] Robertson, S., Zaragoza, H., 2009. The probabilistic relevance frame-

work: Bm25 and beyond. Information Retrieval 3, 333–389.

[37] Rubei, R., Sipio, C.D., Nguyen, P.T., Rocco, J.D., Ruscio, D.D., 2020.
Postﬁnder: Mining stack overﬂow posts to support software develop-
ers. Information and Software Technology 127, 106367.

[38] See, A., Liu, P.J., Manning, C.D., 2017. Get to the point: Summariza-
tion with pointer-generator networks, in: Proceedings of the 55th An-
nual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pp. 1073–1083.

[39] Shi, E., Wang, Y., Du, L., Zhang, H., Han, S., Zhang, D., Sun,
H., 2021. Cast: Enhancing code summarization with hierarchical
splitting and reconstruction of abstract syntax trees. arXiv preprint
arXiv:2108.12987 .

[40] Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., Wang, S.I.,
2022. Natural language to code translation with execution. arXiv
preprint arXiv:2204.11454 .

[41] Tang, Z., Li, C., Ge, J., Shen, X., Zhu, Z., Luo, B., 2021. Ast-
transformer: Encoding abstract syntax trees eﬃciently for code sum-
marization, in: 2021 36th IEEE/ACM International Conference on
Automated Software Engineering (ASE), IEEE Computer Society. pp.
1193–1195.

[42] Tang, Z., Shen, X., Li, C., Ge, J., Huang, L., Zhu, Z., Luo, B., 2022.
Ast-trans: Code summarization with eﬃcient tree-structured atten-
tion. 2022 IEEE/ACM 44th International Conference on Software
Engineering (ICSE) , 150–162.

[43] Tu, Z., Lu, Z., Liu, Y., Liu, X., Li, H., 2016. Modeling coverage for
neural machine translation. arXiv preprint arXiv:1601.04811 .
[44] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Ad-
vances in neural information processing systems 30.

[45] Wang, S., Lo, D., Vasilescu, B., Serebrenik, A., 2014. Entagrec++:
An enhanced tag recommendation system for software information
sites. Empirical Software Engineering 23, 800–832.

[46] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Zhou, D., 2022.
Self-consistency improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171 .

[47] Wang, X., Xia, X., Lo, D., 2015. Tagcombine: Recommending tags to
contents in software information sites. Journal of Computer Science
and Technology 30, 1017–1035.

[48] Wang, Y., Wang, W., Joty, S., Hoi, S.C., 2021. Codet5: Identiﬁer-
aware uniﬁed pre-trained encoder-decoder models for code under-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 15 of 16

Diverse Title Generation for Stack Overﬂow Posts with Multiple Sampling Enhanced Transformer

standing and generation. arXiv preprint arXiv:2109.00859 .

[49] Xia, X., Lo, D., Wang, X., Zhou, B., 2013. Tag recommendation in
software information sites. 2013 10th Working Conference on Mining
Software Repositories (MSR) , 287–296.

[50] Xu, B., Hoang, T., Sharma, A., Yang, C., Xia, X., Lo, D., 2021.
Post2vec: Learning distributed representations of stack overﬂow
posts. IEEE Transactions on Software Engineering , 1–1.

[51] Xu, F.F., Alon, U., Neubig, G., Hellendoorn, V.J., 2022. A systematic
evaluation of large language models of code, in: Proceedings of the
6th ACM SIGPLAN International Symposium on Machine Program-
ming, pp. 1–10.

[52] Zhang, F., Keung, J., Yu, X., Xie, Z., Yang, Z., Ma, C., Zhang, Z.,
2022. Improving stack overﬂow question title generation with copying
enhanced codebert model and bi-modal information. Information and
Software Technology , 106922.

[53] Zhou, P., Liu, J., Liu, X., Yang, Z., Grundy, J., 2019. Is deep learning
better than traditional approaches in tag recommendation for software
information sites? Information and software technology 109, 1–13.

[54] Zhou, P., Liu, J., Yang, Z., Zhou, G., 2017. Scalable tag recom-
mendation for software information sites. 2017 IEEE 24th Interna-
tional Conference on Software Analysis, Evolution and Reengineer-
ing (SANER) , 272–282.

[55] Zhou, Y., Shen, J., Zhang, X., Yang, W., Han, T., Chen, T., 2022a.
Automatic source code summarization with graph attention networks.
Journal of Systems and Software 188, 111257.

[56] Zhou, Z., Yu, H., Fan, G., Huang, Z., Yang, X., 2022b. Summarizing
source code with hierarchical code representation. Information and
Software Technology 143, 106761.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 16 of 16

