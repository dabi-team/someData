Lessons Learned from a Bare-metal Evaluation of
Erasure Coding Algorithms in P2P Networks

Racin Nygaard
Department of Electrical Engineering and Computer Science
University of Stavanger
Stavanger, Norway
racin.nygaard@uis.no

2
2
0
2

g
u
A
5
2

]

C
D
.
s
c
[

1
v
0
6
3
2
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—We have built a bare-metal testbed in order to
perform large-scale, reproducible evaluations of erasure cod-
ing algorithms. Our testbed supports at least 1000 Ethereum
Swarm peers running on 30 machines. Running experimental
evaluation is time-consuming and challenging. Researchers must
consider the experimental software’s limitations and artifacts.
If not controlled, the network behavior may cause inaccurate
measurements. This paper shares the lessons learned from a
bare-metal evaluation of erasure coding algorithms and how to
create a controlled-environment in a cluster consisting of 1000
Ethereum Swarm peers.

Index Terms—experimental evaluation,

tooling, distributed

storage, redundancy, peer-to-peer

I. BACKGROUND AND MOTIVATION

We are designing algorithms to protect data in decentralized
networks, such as the Ethereum Swarm peer-to-peer network.
The Merkle tree [6] is a widely-used hash tree to authenticate
data. In content-addressed storage, a hash tree can be used
to locate the chunks in the network. That permits to locate a
large number of chunks by knowing only the tree’s root. Due
to the tree structure, it is critical to protect chunks in the path
between the root and the data chunks (leaves).

Large-scale experimental evaluation of erasure codes is
to implement
rarely observed in the literature. We want
algorithms and understand their impact on distributed stor-
age networks. Typically, the metrics of interest are latency,
throughput, network- and storage overhead. Many conditions
can affect measurements, including individual peers’ behavior
and the need to run hundreds or thousands of concurrent in-
stances. To obtain meaningful results, we run our experimental
evaluations in a controlled environment and, at the same time,
close to a real-world scenario. Our bare-metal cluster consists
of 30 machines, running up to 1000 Swarm peers [8].

This paper shares insight from our evaluation environment
and lessons learned by observing the peers’ behavior. Our eval-
uation techniques can help others improve the evaluation of
erasure codes in large networks. We discuss our experimental
setup, the main challenges, and the tools developed to solve
them. The source code for the tools will be made available [7].

II. ETHEREUM SWARM

Swarm is a decentralized storage and communication system.
Swarm’s test network is relatively large; a 3-month study

I1

L2

L1

L3

L4

Root

I2

L5

I3

L8

L9

L6

L7

Fig. 1. Merkle tree illustrated with branching factor 3, resulting in 9 leaves,
3 internal nodes and 1 root.

found 6,500 unique peers [3]. The basic unit of storage is
a chunk limited to 4K bytes.

Swarm splits a ﬁle into chunks for upload and computes a
cryptographic hash of each chunk. This hash is also known as
the content address and is necessary to access the chunk later.
Chunks that belong to the same ﬁle are organized in a
Merkle tree. During retrieval, the client initially queries the
network with the root’s content address. It then deciphers the
root chunk to retrieve its children’s content address, for which
it again queries the network. This process continues until all
leaves are retrieved and the original ﬁle has been rebuilt.

III. ADDING REDUNDANCY

The current Swarm deployment can support traditional erasure
coding techniques on a per-chunk basis. For example, by using
Reed-Solomon coding [9], a ﬁle consisting of k chunks can
be encoded using n > k chunks so that any k-out-of-n chunks
can be used for retrieval.

Unfortunately, erasure coding the original ﬁle would not be
sufﬁcient, as this would only apply to the leaves of the Merkle
tree, while the internal nodes would be vulnerable to data loss.
Our research revolves around ﬁnding better ways to add
redundancy to the Merkle tree. To that end, we have used
several coding schemes and developed novel algorithms for
use with Merkle trees. We have evaluated our algorithms
experimentally. Next, we describe our setup and how we were
able to evaluate our algorithms in a cluster of 1000 peers.

IV. EXPERIMENTAL SETUP

Our cluster consists of 30 machines running Ubuntu. We use
Helm [2] and Kubernetes [4] to distribute the load and quickly
scale up to 1000 Swarm peers. We avoid overloading the
cluster machines to facilitate restarting the experiment if our
algorithms cause a crash or get stuck while debugging.

 
 
 
 
 
 
Helm uses a single conﬁguration ﬁle as input to a dozen
conﬁguration ﬁles during the cluster’s initialization. In the
single conﬁguration ﬁle, we specify parameters such as storage
capacity, cluster placement, scaling, run-time parameters of
Swarm, and much more. We base our Helm scripts on those
provided by the Ethereum Swarm organization [1].

Peers running in Kubernetes are ephemeral by default, with
limited options for persistent storage. The before-mentioned
Helm scripts only supported persistent storage options for
cloud providers, but not for private clusters. To provide per-
sistent storage to the peers from the local SSD, we had to
create a Persistent Volume (PV). Each peer has a dedicated PV,
and each PV is linked to a physical location on the SSD. We
elegantly used the modulus operator in the Helm conﬁguration
ﬁle to create the link so that peer y’s data was allocated to a
PV assigned to machine y mod 29.

V. DEALING WITH REPLICATION

Each peer in Swarm is assigned a unique identiﬁer in a
Kademlia [5] overlay network, and peers are grouped into
neighborhoods based on the similarity of their identiﬁers.
Inside neighborhoods, peers attempt to replicate their data to
each other. Peers with the most similar identiﬁer to a chunk
is deemed responsible for persisting that chunk.

As peers have their own view of the overlay network,
some “superpeers” may exist in multiple neighborhoods. This
results in some chunks being more replicated than others, and
in our particular setup, this ranged from 9 replicas to 154
replicas. Figure 2 shows how the chunks of a 100 MB ﬁle
were replicated, and Figure 3 shows how the chunks were
distributed to the peers. To ensure fair comparisons between
algorithms, we need to replicate chunks exactly once.

Fig. 2. Chunk replication with 1000 Swarm peers.

The second tool is bakedeletion (2), which is responsible
for ﬁguring out which chunks must be deleted from which
peer. It generates the deletion list by iteratively simulating
the removal of chunks from peers while following four basic
rules; (A) All peers must have some chunks, (B) Cardinality
of unique chunks must be equal after deletion, (C) Peers can
not get new chunks, (D) Replication factor for each chunk
must be uniform.

To run experiments on different ﬁle sizes, we need to
run tools (1) and (2) for each ﬁle we want to evaluate, as
rule (A) must be respected for each ﬁle. Thus, we need to
aggregate all deletion lists, and this is done in the third tool
combinestorage (3).

the

Finally,

to
deletechunks (4), which goes through the storage network,
peer by peer, and deletes the mapped chunks.

aggregated

deletion

passed

list

is

VI. DEALING WITH SYNCING

iterations,
To ensure an identical system state across test
we had to control the chunk distribution, or syncing. The
push-syncing and pull-syncing can both be disabled by the
command-line option no-sync when starting the peer.

However, the syncing process that occurs when chunks are
delivered through the Kademlia DHT can not be disabled.
Therefore we had to create our own procedures to create
snapshots of the storage states and recover from a snapshot
between each test iteration.

The snapshot of a peer is created by copying all the chunks
stored by the peer immediately after we have made replication
uniform for all chunks, as discussed in Section V. Before
initiating the copying process, we must ﬁrst terminate the peers
to avoid data corruption.

To ensure an identical system state between test iterations,
we (A) replace the local storage with the snapshots for each
of the peers, (B) ensure that the no-sync option is turned on
and (C) ensure that each peer is well-connected. When the test
iteration concludes, we (D) shut down the peers, and for the
next iteration, we continue from step (A) again.

We achieve step (A) by merely copying the snapshot to the
peers using rsync. To reach sufﬁcient connectivity required for
step (C), the peers must ﬁrst discover each other. We monitor
this process by periodically polling the inter-process commu-
nication ﬁle bzzd.ipc. As soon as the desired connectivity is
reached, we can continue with the experimental evaluation,
e.g., ﬁle availability is given peer failures.

Fig. 3. Chunks stored by each Swarm peer.

VII. CONCLUSION

Swarm offers no tools to control each peer’s storage, so the
only way to achieve this was to develop our own tools. Our
ﬁrst tool is named listchunks (1), and its main objective is to
list the content address of all the chunks belonging to each
ﬁle in our storage network. The listchunks tool achieves this
by merely appending each new chunk’s content address it
traverses from the root chunk when retrieving the ﬁle.

In this paper, we have shared our experiences with running
a 1000 peer cluster of Ethereum Swarm instances. Without
these tools to manage the peers, the conﬁguration would be a
nightmare scenario and ensuring they are all running correctly,
even worse. With the assistance of these tools, we can make
fair comparisons of redundancy algorithms in a P2P storage
system.

20406080100120140160Replication0.000.020.040.06FrequencyChunk Distribution02004006008001000Swarm peer0250005000075000100000Chunks storedChunks stored for each Swarm peerREFERENCES

[1] Helm.

Helm charts

to

deploy

Swarm and

Geth.

https://github.com/ethersphere/helm-charts.

[2] Helm. The package manager for Kubernetes. https://helm.sh/.
[3] Seoung Kyun Kim, Zane Ma, Siddharth Murali, Joshua Mason, Andrew
Miller, and Michael Bailey. Measuring ethereum network peers.
In
Proceedings of the Internet Measurement Conference 2018, pages 91–
104, 2018.
[4] Kubernetes.

Production-Grade

Orchestration.

Container

https://kubernetes.io/.

[5] Petar Maymounkov and David Mazieres. Kademlia: A peer-to-peer
information system based on the xor metric. In International Workshop
on Peer-to-Peer Systems, pages 53–65. Springer, 2002.

[6] Ralph C Merkle. A digital signature based on a conventional encryption
function. In Conference on the theory and application of cryptographic
techniques, pages 369–378. Springer, 1987.

[7] Racin Nygaard. Snarl Tools. https://github.com/racin/snarl evaluation tools.
[8] Swarm. Ethereum Swarm. https://swarm.ethereum.org/.
[9] Stephen B Wicker and Vijay K Bhargava. Reed-Solomon codes and their

applications. John Wiley & Sons, 1999.

