Code Compliance Assessment as a Learning
Problem

Neela Sawant
Amazon
Bengaluru, India
nsawant@amazon.com

Srinivasan H. Sengamedu
Amazon
Seattle, USA
sengamed@amazon.com

2
2
0
2

p
e
S
0
1

]
E
S
.
s
c
[

1
v
2
0
6
4
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Manual code reviews and static code analyzers are
the traditional mechanisms to verify if source code complies with
coding policies. However, these mechanisms are hard to scale. We
formulate code compliance assessment as a machine learning (ML)
problem, to take as input a natural language policy and code, and
generate a prediction on the code’s compliance, non-compliance,
or irrelevance. This can help scale compliance classiﬁcation and
search for policies not covered by traditional mechanisms.

We explore key research questions on ML model formulation,
training data, and evaluation setup. The core idea is to obtain
a joint code-text embedding space which preserves compliance
relationships via the vector distance of code and policy embed-
dings. As there is no task-speciﬁc data, we re-interpret and
ﬁlter commonly available software datasets with additional pre-
training and pre-ﬁnetuning tasks that reduce the semantic gap.
We benchmarked our approach on two listings of coding
policies (CWE and CBP). This is a zero-shot evaluation as
none of the policies occur in the training set. On CWE and
CBP respectively, our tool Policy2Code achieves classiﬁcation
accuracies of (59%, 71%) and search MRR of (0.05, 0.21)
compared to CodeBERT with classiﬁcation accuracies of (37%,
54%) and MRR of (0.02, 0.02). In a user study, 24% Policy2Code
detections were accepted compared to 7% for CodeBERT.

Index Terms—code embeddings, natural language analysis

I. INTRODUCTION
Coding policies are natural language rules or best practices
on diverse topics like security, exception handling, and input
validation. Code compliance assessment, e.g., verifying that
source code is compliant with coding policies is an important
but non-trivial problem. Consider two policies from the well-
known Common Weakness Enumeration (CWE) listing [1].

1) CWE-3961 - “Catching overly broad exceptions promotes
complex error handling code that is more likely to contain
security vulnerabilities”. Now consider this code snippet:

p r i v a t e

s t a t i c b y t e [ ] d e c r y p t ( b y t e [ ]

s r c , b y t e [ ] key )

throws R u n t i m e E x c e p t i o n {
t r y {

SecureRandom s r = new SecureRandom ( ) ;
DESKeySpec d k s = new DESKeySpec ( key ) ;
. . .
C i p h e r c i p h e r = C i p h e r . g e t I n s t a n c e ( DES ) ;
c i p h e r . i n i t ( C i p h e r . DECRYPT_MODE,

s e c u r e k e y ,

s r )

;

r e t u r n c i p h e r . d o F i n a l ( s r c ) ;

} c a t c h ( E x c e p t i o n e ) {

throw new R u n t i m e E x c e p t i o n ( e ) ;

}

}

This code is non-compliant with CWE-396 as the ‘catch
(Exception e)’ statement broadly covers all exception types.
Instead, speciﬁc exceptions such as NoSuchAlgorithmEx-
ception or NullPointerException should be used.

2) CWE-1972 - “Truncation errors occur when a primitive
is cast to a primitive of a smaller size and data is lost in
the conversion”. Now consider this code snippet:

p u b l i c

s t a t i c b y t e [ ]

t o B y t e A r r a y ( I n p u t S t r e a m i n p u t ,

l o n g s i z e )
i f

I O E x c e p t i o n {
( s i z e > I n t e g e r .MAX_VALUE) {

throws

throw new I l l e g a l A r g u m e n t E x c e p t i o n ( " " S i z e

g r e a t e r
;

t h a n I n t e g e r max v a l u e :

" " + s i z e )

}
r e t u r n t o B y t e A r r a y ( i n p u t ,

( i n t )

s i z e ) ;

}

This code is compliant with CWE-197 because the casting
operation ‘(int) size’ maps variable size from primitive
long to another primitive int of smaller size, and this
operation is protected against truncation errors via the
preceding if comparison on integer maximum value.

Code compliance assessment requires a deep understanding
of program behavior and associated policies. This task is
subsequently carried out via manual code reviews and static
code analysis. However, both approaches are hard to scale
to increasing and evolving coding policies across diverse
programming languages and frameworks. The global market
size for static code analyzers is projected to reach US $2002
million by 2027, from US $748.1 million in 2020, at a
cumulative annual growth rate of 15.1% during 2021-2027 [2].
Increased automation can help with this goal.

Our objective is to explore how machine learning can help
scale code compliance assessment - “Can we automatically
label a code as compliant or non-compliant (or irrelevant)
given any natural language policy input?” This has two use-
cases: (a) classiﬁcation - detecting non-compliant codes for
policies not covered by manual reviews and static analyzer rules,
and (b) search - ﬁnding compliant and non-compliant code
examples that will serve as test cases in developing new static
analysis rules. While there is prior work in detecting buggy
codes [3], [4], [5], [6] as well as natural language code search
[7], [8], [9], no solutions exist today that can simultaneously

1https://cwe.mitre.org/data/deﬁnitions/396.html

2https://cwe.mitre.org/data/deﬁnitions/197.html

 
 
 
 
 
 
determine code relevance as well as compliance and non-
compliance. In this paper, we set up a learning framework
and explore three research questions.

the near future. We hope our work spawns future research
in alternative and robust formulations for automated code
compliance assessment.

• Learning objective - Compliance assessment is similar,
but more ﬁne-grained than relevance assessment. Both the
compliant and non-compliant examples are relevant to a
coding policy. How can we frame the learning problem?
We propose learning different embeddings to represent
compliant and non-compliant facets of natural language
policies. We formulate a representation learning approach
where the relationships between the compliant and non-
compliant policy facets and compliant, non-compliant,
and irrelevant code examples are preserved via the
vector distances between their embeddings. Representation
learning can scale compliance assessment in a zero-shot
setting [10] as new policies and code examples can be
mapped to their embeddings, even if they are not part of
the original training dataset.

• Training setup - There are no task-speciﬁc labeled training
datasets on coding policies associated with their exam-
ples. Curating new training datasets can be prohibitively
expensive. What training data can be used? We propose
repurposing general-purpose code and documentation
datasets and propose additional training and ﬁltering tasks
to reduce the semantic gap.

• Evaluation setup - How to quantify the performance of
learning-based tools for code compliance assessment?
We contribute a new benchmark that reinterprets and
repurposes well known listings of coding issues curated
by program analysis experts: (a) Common Weaknesses
Enumeration (CWE) [1] and (b) Coding Best Practices
(CBP) from Amazon CodeGuru test suite [11]. We
consider weakness or issue descriptions as policies, buggy
examples as non-compliant and correct code examples as
compliant examples. For realistic search setting, we further
add 27K unlabeled code snippets from the CodeSearchNet
test dataset [12]. We use standard metrics to measure per-
formance (accuracy for classiﬁcation and mean reciprocal
rank (MRR) for search) to compare against two strong
baselines - CodeBERT and a multi-class model.

We develop a new tool Policy2Code by exploring policy
representations, loss formulation, and training schemes. On
CWE and CBP respectively, Policy2Code achieves classiﬁcation
accuracies of (59%, 71%) and search MRR of (0.05, 0.21). In
comparison, CodeBERT achieves classiﬁcation accuracies of
(37%, 54%) and MRR of (0.02, 0.02) respectively. The multi-
class model achieves classiﬁcation accuracies of (54%, 60%)
and MRR of (0.03, 0.13) respectively. We set up Policy2Code
search engine to on-demand retrieve compliant and non-
compliant examples of new policies. A user study with program
analysis experts showed that 24% detections from Policy2Code
were accepted compared to 7% detections from CodeBERT and
15% by the multi-class model. Policy2Code is better than other
ML alternatives, but still not practical as a stand alone code
compliance tool. We expect it to augment manual efforts in

II. RELATED WORK

We discuss machine learning for code applications as well as
key ideas in information retrieval and representation learning.

A. Machine Learning for Code

Machine learning and deep learning in particular, have
advanced many software engineering tasks such as variable
naming [13], comment understanding [14], bug detection [3],
code review generation [15], and code and documentation
synthesis [16], [17]. Deep learning research has beneﬁtted from
efﬁcient transformer models [18] such as CodeBERT [19] - a
124M parameter text-code encoder with impressive performance
on documentation generation and code search tasks. Another
useful concept is that of pre-training, allowing data-hungry deep
learning models to learn from large unlabeled or weakly labeled
datasets on related tasks, so that the models can be subsequently
ﬁne-tuned using small task-speciﬁc training datasets [20],
[21], [22]. CodeBERT is trained with CodeSearchNet training
dataset for two tasks: masked language modeling and code-text
relevance (method body to method descriptions).

Neural Bug Finding is typically modeled in a supervised
setting using labeled training examples for a pre-determined
set of issues. Training examples may be obtained from existing
static analyzer detections [3] or synthesized with transforma-
tions [4]. Much of the research is devoted to analyzing the
effect of richer program representations. For example, Wang
et al. [5] show that combining different types of contexts (the
Program Dependence Graph (PDG), Data Flow Graph (DFG)
and the method under investigation) improves performance of
bug detection in pre-determined categories.

Earlier Code Search systems predominantly relied on string
matching, and in some cases aiming to improve performance
with code structures [23], API matching [24], API usage
patterns [25], [26], and query expansion [27]. More recent
neural code search systems perform search based on the vector
distance between their embeddings. This research focuses
on three aspects (a) code representations, for example using
method names and API sequences [9], parse trees [28], and
control ﬂow graphs [29], [30], (b) training strategy, for example,
unsupervised [7], [31] or supervised [28], [19], [32], [33], and
(c) loss formulations, for example, contrastive loss or [28],
[19] or triplet loss [9], [29], [30], [33], [34].

B. Representation learning

To the best of our knowledge, there are no settings for
ﬁne-grained compliance assessment of code and policy pair.
We review key research in general
information retrieval
and representation (metric) learning [35], [36], [37], [38].
Representation learning has the attractive property that once
the embedding space is learned, novel input can be mapped
without additional training, essentially a zero-shot setting [10].

A common ranking mechanism is triplet loss that models
relationships among three data points - an anchor, a positive
example, and a negative example. In traditional code search,
they map to natural
language query (such as a method
description) to relevant example (such as the corresponding
method body) and an irrelevant example (such as the body of
an unrelated method). [39], [40], [41]. The algorithm learns
to minimize the distance between anchor-positive pairs and
maximize the distance between the anchor-negative pairs.
Triplet mining is an active area of research [39], [40], [41].
There are very few settings involving higher order losses for
ﬁne-grained differentiation. For example, Chen et al. [42]
introduced a quadruplet loss network for person veriﬁcation
task. A training example in this task consists of four data
points xi, xj, xk, xl where xi, xj belong to the same class
(e.g., considered the same person) and xk and xl belong to
two other classes (faces from two other people).

C. Other domains

Our objective may seem similar to the sentiment analysis
task [43], [44] in natural language understanding. The objective
of this task is to classify a natural language text into positive,
the
negative, or neutral polarities. The difference is that
polarities of natural language are not comparable with the
polarities of coding policies. The semantics of a polarity is
globally consistent for natural language. For example, both
these sentences - ‘food is exceptionally good’ or ‘the concert
was exceptionally good’, convey similarly positive emotions
about two different entities. On the other hand, the compliance
or non-compliance of code is always conditioned on the speciﬁc
policy under consideration. The same snippet may be compliant
with one policy but non-compliant with another (for example,
adheres to exception handling requirements, but is not secure).

III. LEARNING PROBLEM FORMULATION

A. Facet Policy Input Representation

Policy representation is a complex problem. A policy
contains a general relevance context and a speciﬁc compliance
context. For example, reconsider (CWE-396): Catching overly
broad exceptions promotes complex error handling code that
is more likely to contain security vulnerabilities. This policy is
relevant to all code snippets with exception handling. The
compliance context refers to using broad exception types
(Exception) versus more speciﬁc exceptions (NoSuchAlgorith-
mException or NullPointerException). Policy representations
can incorporate domain knowledge and advanced parsing of
natural language text. In the context of this paper however,
we explore two purely learning based variations inspired from
prior work in NLP and Computer Vision.

1) Facet-preﬁxed Policy: This idea is similar to T5 query
format [45] with remarkable success in NLP applications.
Natural language policy description r is preﬁxed with facet
token y. The concatenated representation y : r is passed through
an encoder to generate faceted policy embedding. We use a
transformer-based encoder with self-attention, so that the facet
token organically attends to the various policy text components.
2) Facet-masked Policy: This approach externally combines
the facet token with the policy input using a parametric model.
We use an implementation of conditional masking [46], which
aims to learn as many mask vectors as the number of conditions
(e.g., facets in our setting). One of the mask vector is selected
based on the facet y and multiplied with embedding of policy
r, essentially projecting the policy into a different subspace
conditioned on the facet label. See Appendix A for details.

B. Loss Functions

Let r and c denote a natural language policy and a code
example. Let y ∈ {+, −} denote the compliant and non-
compliant facets respectively, and let irrelevance be denoted
by ∼. Our objective is to learn a bimodal embedding space
for r and c such that
the the compliant, non-compliant
(and irrelevance) relationships are represented via the vector
distances between their embeddings. In contrast to code search,
where natural language query has a single representation,
we propose to learn separate r+ and r− embeddings for
ﬁne-grained differentiation. For compliant and non-compliant
example search, we then operate with two separate query inputs
r+ and r−. Embedding of r+ should be closer to c+, followed
by c−, and dissimilar to that of c∼. For non-compliant search,
r− is closer to c−, followed by c+, and far from c∼. Such
space naturally supports search and classiﬁcation based on
relative distances and learned thresholds. New policies and
codes can be mapped to their embeddings and evaluated in
zero-shot setting. We now discuss two aspects of representation
learning - policy input representation and loss functions.

Fig. 1: Illustration of compliance search task (with compliant
facet): The learning objective is to minimize the vector distance
between the faceted policy description and a compliant code
embedding, and maximize the distance with respect to non-
compliant and irrelevant code embeddings.

Figure 1 illustrates the learning objective using an exam-
ple of compliance search task. Given a training quadruplet
(r+, c+, c−, c∼), we want to learn embeddings that minimize
the relative distances between matched pairs and maximize the
distances between unmatched pairs. We test two formulations.
1) Quadruplet Loss (QL) : Quadruplet loss is an extension

of triplet loss computed as follows:

L+

quad =

N
(cid:88)

r+,c+,c−

[d(fr+, fc+)2 − d(fr+, fc−)2 + α1]+

N
(cid:88)

+

r+,c−,c∼

[d(fr+, fc− )2 − d(fr+, fc∼ )2 + α2]+ (1)

where fx denotes the embedding of sequence x, d(., .) denotes
the distance function between two embeddings, and []+ in-
dicates that only positive values contribute towards the loss
calculation. α2 > α1 > 0 are tunable margin parameters for
compliance and relevance differentiation, respectively. The loss
for the non-compliant facet can be computed similarly over
quadruplet (r−, c−, c+, c∼). Finally, the total loss is computed
as Lquad = 1

quad + L−

quad).

2 (L+

The naive quadruplet loss formulation has two limitations.
Eqn. 1 is anchored around text only. It does not encode
constraint for classiﬁcation task because there is no anchoring
around code and no constraint to ensure that embeddings of r+
and r− are different given a code. Another limitation is from
practically available data. In general purpose datasets, very
few examples will contain both compliant and non-compliant
codes. However, there is an abundance of datasets with partial
view (some contain only relevant examples, some with only
compliant or non-compliant examples, etc.). To be able to use
the latter datasets, we need an alternative formulation as below.
2) Bimodal Multi-task Triplet loss (BMT) : In this formula-
tion, the quadruplets (r+, c+, c−, c∼) and (r−, c−, c+, c∼) are
un-pivoted into a batch B of labeled examples (x, l), where
x can be either a text or code (e.g., x is bimodal) and label
l is generated on the ﬂy such that only the x originating
from paired policy and facets share that label l. For example,
the quadruplets mentioned above can be un-pivoted into
a list [(r+, l1), (c+, l1), (r−, l2), (c−, l2), (c∼, l∼), . . .]. Each
irrelevant example c∼ is assigned a unique label l∼ to prevent
anchor-positive matching with another entity. We then calculate
a bimodal multi-task triplet (BMT) loss over possible valid
triplets (a, p, n) where a, p, and n are anchor, positive, and
negative sequences in the batch such that a and p share the
same label and n has a different label. The term multi-task
denotes the simultaneous modeling of different class labels.
[d(fa, fp)2 − d(fa, fn)2 + α]+

LB =

(cid:88)

(a,p,n);l(a)=l(p),l(a)(cid:54)=l(n)

(2)
where α is a common margin which acts as a threshold to
separate the irrelevant examples from the relevant ones. The
formulation naturally allows both code and text entities to
serve as anchors. Datasets with partial labels (only relevant,
compliant, and/or non-compliant) can be incorporated into
training. Compliance search task is served by selecting codes
based on their distances from faceted policy representations. For
compliance classiﬁcation, we use the threshold α to determine
if a code c is relevant or irrelevant with respect to the average
policy embedding (r+ + r−)/2. If it is deemed relevant, we
use the distances with respect to r+ and r− to determine the
closest facet as compliance label. The distances can be mapped
to probabilities using SoftMax function p(d) = e−d

(cid:80)

d(cid:48) e−d(cid:48) .

IV. TRAINING SETUP

ML models are data hungry. For code compliance assess-
ment however, there are no large-scale datasets containing
natural language policies paired with their compliant and non-
compliant examples. We re-interpret and repurpose general

datasets on code, documentation, code reviews, and bug-ﬁxes
with additional training schemes to bridge this gap.

A. Multi-Task Pre-training

To capture patterns in higher level documentation guidelines,
pre-training with general documentation can be helpful. To
capture coding issues and aligned code examples, pre-training
with code reviews can be beneﬁcial. We use below tasks.

• Collocated documentation prediction (Doc): Paragraphs
of software documentation are segmented into non-
overlapping ﬁxed-length passages. The passages belonging
to the same paragraph are assigned a common label. The
objective of Collocated documentation prediction is to
predict which candidate segments (code snippet or text)
in a batch come from the same paragraph.

• Code-comment matching task (CC): Given multiple <code,
review comment> pairs, we attempt to determine (a) the
correct review comment given a code (relevant comment
prediction) and (b) the correct code on which a review
comment is made (non-compliant code prediction).

B. Pre-ﬁne-tuning

Examples containing bug-ﬁxes are especially valuable to
us, because the example can be reinterpreted as a proxy to
policy description with compliant and non-compliant codes.
Such a dataset can then be used to pre-ﬁnetune for target
task. See Figure 2 for a motivating example of a GitHub bug-
ﬁx review comment. The comment provides a security best
practice suggestion for which the code-before and code-after
versions can be used as non-compliant and compliant examples,
respectively. Each review comment is essentially treated as
a policy and the code-before and the code-after versions are
taken to be the non-compliant and the compliant code examples
respectively. Irrelevant examples c∼ are mined from codes of
unrelated bug-ﬁxes. This dataset can be noisy, but we expect
useful signals to emerge with large-scale datasets.

Data Filtering: As an additional treatment, we apply Doc2BP
[47], a deep learning classiﬁer trained to detect coding policies
and best practice recommendations from natural language text.
Doc2BP analyzes keywords, parts of speech patterns, and other
linguistic properties to determine if a comment resembles
coding policies, and ﬁlters out non-policy-like comments to
reduce noise. Results with and without Doc2BP indicate that
the ﬁltering step in pre-ﬁnetuning is useful.

V. EVALUATION SETUP

We construct a new benchmark by repurposing two listings

of coding issues and a public code search dataset.

• Common Weaknesses Enumeration (CWE) View 702
[1] is a public domain list of software weaknesses
introduced during implementation. It contains 185 Java
weaknesses, each with a short natural language description
and compliant and non-compliant examples. We use the
weakness description as the policy. The number of policies
with non-compliant, compliant, and both types of examples
are 182, 36, and 33, respectively.

Code review comment: Probably better to use ‘SecureRandom‘ for anything security related..

Facet: non-compliant/ Code-before

Facet: compliant/ Code-after

p r i v a t e S t r i n g g e n e r a t e P a s s w o r d ( ) {

S t r i n g a l l o w e d C h a r s = "

a b c d e f g h i j k l m n o p q r s t u v w x y z 0 1 2 3 4 5 6 7 8 9 " ;

i n t c h a r R a n g e = a l l o w e d C h a r s . l e n g t h ( ) ;
Random random=new Random ( ) ;
S t r i n g B u i l d e r p a s s w o r d =new S t r i n g B u i l d e r ( ) ;
i n t p a s s w o r d L e n g t h = 1 0 ;
f o r ( i n t
i = 0 ;

i < p a s s w o r d L e n g t h ;

i ++) {

p a s s w o r d . a p p e n d ( a l l o w e d C h a r s . c h a r A t ( random . n e x t I n t (

c h a r R a n g e ) ) ) ;

}
r e t u r n p a s s w o r d . t o S t r i n g ( ) ;

}

p r i v a t e S t r i n g g e n e r a t e P a s s w o r d ( )

throws

N o S u c h A l g o r i t h m E x c e p t i o n {

S t r i n g a l l o w e d C h a r s = "

a b c d e f g h i j k l m n o p q r s t u v w x y z 0 1 2 3 4 5 6 7 8 9 " ;

i n t c h a r R a n g e = a l l o w e d C h a r s . l e n g t h ( ) ;
SecureRandom random= SecureRandom . g e t I n s t a n c e S t r o n g ( ) ;
i n t p a s s w o r d L e n g t h = 2 0 ;
char [ ] p a s s w o r d =new char [ p a s s w o r d L e n g t h ] ;
f o r ( i n t
i ++) {

i < p a s s w o r d L e n g t h ;

i = 0 ;

p a s s w o r d [ i ] = a l l o w e d C h a r s . c h a r A t ( random . n e x t I n t (

c h a r R a n g e ) ) ;

}
r e t u r n new S t r i n g ( p a s s w o r d ) ;

}

Fig. 2: Example entry in GitHub comment and bug-ﬁx dataset

• Coding Best Practice (CBP) is a collection of Java best
practice descriptions and compliant and non-compliant
examples obtained from Amazon CodeGuru [11] static
analyzer test-suite. Our dataset contains 46 Java best
practices, each with at least one compliant and one non-
compliant example, and a total of 186 examples.

• CodeSearchNet (CSN) is a public domain dataset [12].
We use the test split with 27K unlabeled code examples.
The resulting benchmark contains 231 policies, 477 labeled
codes, and 27K unlabeled codes (total 28K code examples).
Policies cover many topics like input validation, cipher security,
web security, command injection, preferred data structures, etc.
The dataset has no overlap with the training data, hence this
is a zero-shot benchmark. We investigate two tasks:

• Compliance Classiﬁcation: The objective is to predict
whether a code is compliant, non-compliant, or irrelevant
given a policy. Performance is measured in accuracy with
respect to the known ground truth.

• Compliance Search: The objective is to retrieve compliant
and non-compliant codes from a large code corpus for a
user-speciﬁed natural language policy. In a large corpus
setting, we do not expect to know the ground truth labels
for all code examples. Hence we rely on another metric
in information retrieval research, the mean reciprocal rank
metric (M RR) deﬁned as 1
where Q denotes
Q
the number of user queries and ranki denotes the rank
position of the ﬁrst example from the known ground truth
within each facet. We prefer MRR over other information
retrieval metrics such as precision@k for its ability to
capture the order of relevance. For example, a precision@5
metric would yield the same score (0.2) whether a single
relevant result appears at rank 1 or rank 5. MRR on
the other hand, would generate a score of 1 for the ﬁrst
position and a score of 0.2 for the ﬁfth position.

(cid:80)|Q|
i=1

1
ranki

We propose two competitive baselines.
• CodeBERT [19] is the off-the-shelf pre-trained Hugging-
Face implementation. CodeBERT is a state-of-the-art deep
learning approach for several code-related tasks including
code search (https://microsoft.github.io/CodeXGLUE/).

Despite using a token sequence representation of code,
it is able to out-perform complex models such as Graph-
CodeBERT that can represent programs as graphs. This
baseline is designed for relevance and not ﬁne-grained
differentiation. It represents the current best tool available
to developers to ﬁnd relevant examples, which can then
be manually classiﬁed for compliance.

• Multi-class classiﬁcation is a three-class (compliant, non-
compliant, irrelevant) classiﬁer. It is implemented by
adding dropout and linear layers on top of the Code-
BERT’s [19] transformer embedding layer. This is also
trained for a zero-shot setting. A (policy, code) pair is
represented using CodeBERT’s input sequence convention
[CLS]x1, x2, ...xm[SEP ]y1, y2, ...yn[SEP ] where x and
y represent the tokens for policy and code respectively.

Our use of public domain datasets and easy to replicate

baselines is expected to fuel future research on this topic.

VI. RESULTS

This section presents a comprehensive evaluation. Bold-

facing in tables is used to indicate superior performance.

A. Experimental Setup

As the central bimodal code-text encoder for Policy2Code
as well as the multi-class classiﬁcation baseline, we use
the HuggingFace implementation of CodeBERT [19]. The
multi-class classiﬁcation baseline and the Policy2Code model
are trained using the same pre-training and pre-ﬁne-tuning
data for apples-to-apples comparison. For the collocated
documentation pre-training (DOC) task, we use Java 8, 11,
and 16 documentation (∼1 GB). For code comment matching
pre-training (CC), we used 150K GitHub code reviews. For bug-
ﬁx-comment pre-ﬁne-tuning, we leveraged 32K code reviews
on GitHub that were tagged as bug-ﬁx, split into 80%-20% for
training and validation. All these datasets are in public domain.
Further none of them intentionally overlap with benchmark
examples. Code is statically represented as token sequences
with sub-word tokenization and BytePair Encoding (BPE) [48].

TABLE I: Effect of Loss Formulation and Policy Representation

Loss

Policy Rep

QL

BMT

Preﬁxed
Masked
Preﬁxed
Masked

Classiﬁcation Accuracy

CWE (Conceptual)
42.6 ± 3.6
33.41 ± 1.9
45.3 ± 2.4
41.6 ± 4.5

CWE
48.13 ± 2.2
34.97 ± 2.2
49.8 ± 5.1
42.23 ± 3.7

CBP
44.93 ± 0.98
39.3 ± 6.3
48.0 ± 7.1
46.21 ± 1.2

CWE (Conceptual)
0.021 ± 0.00
0.0298 ± 0.02
0.0264 ± 0.029
0.0242 ± 0.061

Search MRR
CWE
0.0321 ± 0.009
0.0318 ± 0.19
0.0363 ± 0.011
0.0213 ± 0.032

CBP
0.0915 ± 0.017
0.07641 ± 0.029
0.1496 ± 0.021
0.1127 ± 0.027

TABLE II: Effect of Pre-training and Filtering Schemes

Pre-Training Scheme

None
Doc
CC
Doc + CC
Doc + CC + Doc2BP Filter [47]

Classiﬁcation Accuracy

CWE (Conceptual)
45.3 ± 2.4
48.3 ± 3.1
47.8 ± 6.1
53.2 ± 3.2
53.8 ± 6.1

CWE
49.8 ± 5.1
54.2 ± 4.6
43.77 ± 1.1
59.3 ± 7.4
59.3 ± 2.0

CBP
48.0 ± 7.1
56.2 ± 2.6
55.2 ± 2.9
67.6 ± 7.9
71.0 ± 2.8

CWE (Conceptual)
0.0264 ± 0.029
0.0319 ± 0.038
0.0471 ± 0.129
0.0492 ± 0.05
0.0462 ± 0.013

Search MRR

CWE
0.0363 ± 0.011
0.0313 ± 0.008
0.0467 ± 0.008
0.0482 ± 0.011
0.0538 ± 0.01

CBP
0.1496 ± 0.021
0.1773 ± 0.009
0.1654 ± 0.017
0.1982 ± 0.012
0.2182 ± 0.012

B. Policy2Code Model Training

We now discuss the effect of policy representations, losses,
and training schemes. Models were optimized using automatic
hyper-parameter tuning. See Appendix B for details.

Table I reports the performance of different combinations
of loss functions and policy representations on the CWE and
CBP datasets for classiﬁcation and search tasks. The BMT loss
with facet-preﬁxed policy representation is the most effective
strategy across both datasets and tasks.

Table II shows the effect of different pre-training and ﬁltering
schemes on Policy2Code performance. All schemes use BMT
loss with facet-preﬁxed policy representation and subsequently
followed by the same pre-ﬁnetuning step. The ﬁltering step
reduces the dataset to 14K examples (out of 32K original
bug-ﬁx dataset) which are predicted to be policy-like. The
pre-training schemes as well as the ﬁltering step improve
model performance across tasks. We continue to see better
performance for the CBP dataset than the CWE dataset. We
also observe that documentation and code-related pre-training
impact CWE performance differently. While code-related pre-
training (CC) decreases the CWE performance, documentation
pre-training improves the performance. This effect can be
explained as CWE policies are more conceptual, hence more
similar to documentation patterns than speciﬁc code comments.

C. Baseline Comparison

Table III compares Policy2Code with baselines on compli-
ance classiﬁcation and compliance search tasks. Policy2Code
outperforms CodeBERT as well as the multi-class classiﬁcation
baseline. Policy2Code achieves classiﬁcation accuracies of
(59%, 71%) and search MRR of (0.05, 0.21) on CWE and
CBP. On the same datasets, a popular CodeBERT baseline
achieves classiﬁcation accuracies of (37%, 54%) and MRR
of (0.02, 0.02) respectively. Multi-class classiﬁcation achieves
accuracies of (54%, 60%) and MRR of (0.03, 0.13) respectively.
We observe that the model performance is generally lower on
the CWE dataset. Our investigation shows that CWE contains
multiple policies on related topics and partially labeled ground
truth. One code example may be applicable to multiple policies,

but not all such relationships are recorded. For example, CWE-
3383 suggests using cryptographically strong Pseudo-Random
Number Generator (PRNG) in a security context. The below
code is the only explicitly labeled example for this policy.

Random random = new Random ( System . c u r r e n t T i m e M i l l i s ( ) ) ;
i n t a c c o u n t I D = random . n e x t I n t ( ) ;

However, other code examples in the CWE dataset may also
be related to CWE-338. See the below code for example,
labeled only for policy CWE-3364 on not using the same seed
across multiple PRNG initializations. This example can also
be considered as a valid example for CWE-338.

p r i v a t e
p u b l i c

s t a t i c

f i n a l

l o n g SEED = 1 2 3 4 5 6 7 8 9 0 ;

i n t g e n e r a t e A c c o u n t I D ( ) {
Random random = new Random ( SEED ) ;
r e t u r n random . n e x t I n t ( ) ;

}

This implies that the CWE ground truth is incomplete, leading
to currently computed metrics being a lower bound on the
actual performance. CBP is expected to have higher quality as
it is manually curated for compliance analysis task speciﬁcally.

D. User Study

We conducted a user study to determine if Policy2Code can
be used in a practical setting for searching compliant and non-
compliant code examples. One application for such examples
is to serve as test cases for creating new static analysis rules.
We built a FAISS-GPU index [49] over all 28K code
examples in the benchmark dataset. On a p3.xlarge EC2
machine, index creation takes approximately 8 seconds. At
inference time, top examples for any policy queries are retrieved
within milliseconds based on approximate and efﬁcient nearest
neighbor algorithm. Exact similarity computation is avoided as
it can take about 30 minutes per policy to compare all codes.
We recruited 16 experts with an average of over 10 years
of software engineering experience and over 5 years of
programming language analysis experience. For 25 policies

3https://cwe.mitre.org/data/deﬁnitions/338.html
4https://cwe.mitre.org/data/deﬁnitions/336.html

TABLE III: Comparison of Policy2Code with Baselines on Compliance Assessment Tasks

Method

CodeBERT
Multi-class
Policy2Code

Classiﬁcation Accuracy

CWE (Conceptual)
33.5
42.82 ± 3.1
53.84 ± 6.1

CWE
37.5
54.36 ± 1.9
59.29 ± 2.0

CBP
54.4
60.04 ± 2.7
71.04 ± 2.8

CWE (Conceptual)
0.0223
0.0297 ± 0.03
0.0462 ± 0.013

Search MRR

CWE
0.0242
0.0312 ± 0.02
0.0538 ± 0.01

CBP
0.0217
0.1310 ± 0.03
0.2182 ± 0.012

TABLE IV: Acceptance Rate in User Study (%)

Model
CodeBERT
Multi-class
Policy2Code

Compliant
9.68
27.01
41.93

Non-compliant Overall

6.38
11.57
17.64

7.2
15.2
24.13

in CWE dataset, we determined the top 5 compliant and non-
compliant examples detected by various approaches in the
entire 28K code example benchmark dataset. We masked the
identity of the selection algorithm and asked the assessor if
they agree or disagree with the label assignment. Table IV
shows the acceptance rate of predictions made by different
techniques. Policy2Code has the highest overall acceptance
rate at 24%, followed by 15.2% for multi-class classiﬁcation
and 7.2% for CodeBERT, averaged over all assessments.

E. Anecdotal Results

See Figures 3 and 4 for anecdotal examples. On the one hand,
they may seem to over-index on keyword similarity (Figure
3(c), (e)), and on the other hand, we ﬁnd surprisingly semantic
associations (Figure 3(a)). Models trained on textual features
alone, without using any advanced program representations
may over-index on keyword similarity rather than higher order
semantics. Here is an incorrect ﬁnding, The software does
not properly account for differences in case sensitivity when
accessing or determining the properties of a resource, leading
to inconsistent results. (CWE-178).

p r i v a t e b o o l e a n q u e r y R e l a t i v e C a t a l o g s

( ) {
r e a d P r o p e r t i e s ( ) ;

( r e s o u r c e s == n u l l )
( r e s o u r c e s == n u l l ) r e t u r n d e f a u l t R e l a t i v e C a t a l o g s ;

i f
i f
t r y {

S t r i n g a l l o w = r e s o u r c e s . g e t S t r i n g ( ‘ c a t a l o g s ’ ) ;
r e t u r n ( a l l o w . e q u a l s I g n o r e C a s e ( ‘ t r u e ’ )
| |

a l l o w . e q u a l s I g n o r e C a s e ( ‘ 1 ’ ) ) ;

e q u a l s I g n o r e C a s e ( ‘ y ’ )

a l l o w .

| |

} c a t c h ( M i s s i n g R e s o u r c e E x c e p t i o n e ) {
r e t u r n d e f a u l t R e l a t i v e C a t a l o g s ;

}

}

The repeated usage of keywords resource, properties, and case
in the result, leads to the example being scored higher, even
though the notion of case-sensitivity is incorrectly modeled.
Here, the case handling is a design choice. Policy2Code makes
a mistake on this very challenging example.

VII. THREATS TO VALIDITY

As we formulate code compliance assessment as a machine

learning problem, we see certain threats to the validity.

some policies may still be under-represented in these
training datasets. Certain issues may rarely occur in code
and even rarely detected by human reviewers. The models
trained on human code review comments and bug-ﬁxes
may not learn to detect these issues well.

• The benchmark dataset needs further curation. In using the
CWE dataset, we see that certain policy descriptions are
conceptual and loosely scripted that do not convey a strong
recommendation or warning. Further, some ground truth
labels are missing. Future work should further enhance
the benchmark in both aspects to improve assessment.
• Finally, ML predictions may never meet 100% accuracy
unlike carefully crafted static analysis rules. We expect
automated tools to augment and not replace manual code
reviews and static analyzer creation. Some reduced degree
of manual vetting may still be required.

We need to understand how the performance of machine
learning algorithms vary by data characteristics (policy and
code) and how it can be improved by innovating on natural
language (NL) and programming language (PL) research.

VIII. CONCLUSION

Code compliance assessment is an important problem in soft-
ware development and an emerging area for machine learning.
This paper explores novel research questions related to learning
framework, training data, and evaluation setup. We proposed a
representation learning approach that preserves the relationships
between policies and their compliant, non-compliant, and
irrelevant code examples via the vector distances between their
embeddings. To overcome the lack of task-speciﬁc training data,
we proposed repurposing general software datasets with pre-
training, pre-ﬁne-tuning, and ﬁltering steps. We evaluated the
impact of policy representations, losses, training schemes, and
hyper-parameter optimization. Resulting Policy2Code model
shows promising results for compliance classiﬁcation and
search tasks. Policy2Code achieves classiﬁcation accuracies
of (59%, 71%) and search MRR of (0.05, 0.21) on CWE and
CBP, the two types of datasets in our benchmark. On the same
datasets, CodeBERT baseline achieves classiﬁcation accuracies
of (37%, 54%) and MRR of (0.02, 0.02) respectively whereas
multi-class baseline achieves classiﬁcation accuracies of (54%,
60%) and MRR of (0.03, 0.13) respectively. In a user study of
compliant and non-compliant ﬁndings, 24% detections from
Policy2Code were accepted compared to only 7% detections
from CodeBERT and 15% by the multi-class baseline.

• ML models are known to carry biases from the training
datasets. While more training schemes (pre-training as
well as ﬁne-tuning) can help improve model performance,

We hope to encourage more theoretical and empirical
research in automated code compliance assessment. We expect
ML solutions to signiﬁcantly reduce future manual efforts in

a) Truncation errors occur when a primitive is cast to a primitive of a smaller size and
data is lost in the conversion. (CWE-197)

a) Catching overly broad exceptions promotes complex error handling code that is more
likely to contain security vulnerabilities. (CWE-396)

p u b l i c

s t a t i c b y t e [ ]
throws

I O E x c e p t i o n {

t o B y t e A r r a y ( I n p u t S t r e a m i n p u t ,

l o n g

s i z e )
i f

( s i z e > I n t e g e r .MAX_VALUE) {

throw new I l l e g a l A r g u m e n t E x c e p t i o n ( " " S i z e g r e a t e r

t h a n I n t e g e r max v a l u e :

" " + s i z e ) ;

}
r e t u r n t o B y t e A r r a y ( i n p u t ,

( i n t )

s i z e ) ;

}

b) The software accepts XML from an untrusted source but does not validate the XML
against the proper schema. (CWE-112)

p u b l i c

s t a t i c Document newDocument ( S t r i n g x m l S t r i n g ,

b o o l e a n namespaceAware )
throws SAXException ,
I O E x c e p t i o n , P a r s e r C o n f i g u r a t i o n E x c e p t i o n {
r e t u r n X m l U t i l s . newDocument ( new I n p u t S o u r c e ( new
S t r i n g R e a d e r ( x m l S t r i n g ) ) , namespaceAware ) ;

}

c) This program compares classes by name, which can cause it to use the wrong class
when multiple classes can have the same name. (CWE-486)

p r o t e c t e d s t a t i c S t r i n g c r e a t e T a b l e R e f N a m e ( f i n a l O b j e c t

e n t i t y ) {

t y p e = e n t i t y . g e t C l a s s ( ) ;
C l a s s
t y p e = ( t y p e == C l a s s . c l a s s ? ( C l a s s )
r e t u r n ( t y p e . getSimpleName ( ) + ’ _ ’ ) ;

e n t i t y :

t y p e ) ;

}

p r i v a t e

s t a t i c b y t e [ ] d e c r y p t ( b y t e [ ]

s r c , b y t e [ ] key )

throws R u n t i m e E x c e p t i o n {
t r y {

SecureRandom s r = new SecureRandom ( ) ;
DESKeySpec d k s = new DESKeySpec ( key ) ;
S e c r e t K e y F a c t o r y k e y F a c t o r y = S e c r e t K e y F a c t o r y .

g e t I n s t a n c e ( DES ) ;

S e c r e t K e y s e c u r e k e y = k e y F a c t o r y . g e n e r a t e S e c r e t ( d k s

) ;

C i p h e r c i p h e r = C i p h e r . g e t I n s t a n c e ( DES ) ;
c i p h e r . i n i t ( C i p h e r . DECRYPT_MODE,
r e t u r n c i p h e r . d o F i n a l ( s r c ) ;

s e c u r e k e y ,

s r ) ;

} c a t c h ( E x c e p t i o n e ) {

throw new R u n t i m e E x c e p t i o n ( e ) ;

}

}

b) Do not throw new checked exceptions (CheckedExceptions) forcing users to handle
them elsewhere. Code should either propagate existing checked exceptions or handle
them.

p u b l i c

s t a t i c C i p h e r n e w C i p h e r ( S t r i n g a l g o r i t h m ) {

t r y {

r e t u r n C i p h e r . g e t I n s t a n c e ( a l g o r i t h m ) ;

}
c a t c h ( N o S u c h A l g o r i t h m E x c e p t i o n e ) {

throw new I l l e g a l A r g u m e n t E x c e p t i o n ( " " Not a v a l i d

e n c r y p t i o n a l g o r i t h m " " , e ) ;

}
c a t c h ( N o S u c h P a d d i n g E x c e p t i o n e ) {

throw new I l l e g a l S t a t e E x c e p t i o n ( " " S h o u l d n o t h a p p e n

d) The product does not sufﬁciently enforce boundaries between the states of different
sessions, causing data to be provided to, or used by, the wrong session. (CWE-488)

p r o t e c t e d v o i d u p d a t e ( C o l l e c t i o n U p d a t e T y p e

f o r c e )

throws

" " , e ) ;

}

}

I O E x c e p t i o n {
S t a t e
s y n c h r o n i z e d ( l o c k ) {

l o c a l S t a t e ;

i f

( f i r s t ) {

s t a t e = c h e c k S t a t e ( ) ;
s t a t e . l a s t = System . c u r r e n t T i m e M i l l i s ( ) ;
r e t u r n ;

}
l o c a l S t a t e = s t a t e . copy ( ) ;

}
u p d a t e C o l l e c t i o n ( l o c a l S t a t e ,
l o c a l S t a t e . l a s t = System . c u r r e n t T i m e M i l l i s ( ) ;
s y n c h r o n i z e d ( l o c k ) {

f o r c e ) ;

s t a t e = l o c a l S t a t e ;

}

}

e) The product uses untrusted input when calculating or using an array index, but the
product does not validate or incorrectly validates the index to ensure the index references
a valid position within the array. (CWE-129)

p r i v a t e

s t a t i c

i n t

c a l c u l a t e E n d I n d e x ( d o u b l e [ ]

a r r a y ,

i n t

c) Instead of repeatedly creating a new Random object to obtain multiple random
numbers, create a single Random object and reuse it.

p r i v a t e
p u b l i c

s t a t i c

f i n a l

l o n g SEED = 1 2 3 4 5 6 7 8 9 0 ;

i n t g e n e r a t e A c c o u n t I D ( ) {
Random random = new Random ( SEED ) ;
r e t u r n random . n e x t I n t ( ) ;

}

d) The product receives input that is expected to specify an index, position, or offset
into an indexable resource such as a buffer or ﬁle, but it does not validate or incorrectly
validates that the speciﬁed index/position/offset has the required properties. (CWE-1285)

p r i v a t e

s t a t i c O b j e c t g e t C o l l e c t i o n P r o p ( O b j e c t o , S t r i n g

i n t

propName ,
i n d e x , S t r i n g [ ] p a t h ) {
o = _ g e t F i e l d V a l u e s F r o m C o l l e c t i o n O r A r r a y ( o , propName ) ;
i f

i n d e x + 1 == p a t h . l e n g t h ) {

(

r e t u r n o ;

} e l s e {

i n d e x ++;
r e t u r n g e t C o l l e c t i o n P r o p ( o , p a t h [ i n d e x ] ,

i n d e x ,

p a t h ) ;

o r i g i n a l I n d e x ) {
f i n a l
E x c e p t i o n s . r e q u i r e N o n N u l l ( a r r a y ,

l e n g t h = a r r a y . l e n g t h ;

i n t

i n t
i f

}
i f

}
i f

(

(

(

n u l l " "
i n d e x = o r i g i n a l I n d e x ;

) ;

i n d e x < 0 ) {

i n d e x = l e n g t h + i n d e x ;

i n d e x < 0 ) {

i n d e x = 0 ;

i n d e x > l e n g t h ) {

i n d e x = l e n g t h ;

}
r e t u r n i n d e x ;

}

" " a r r a y c a n n o t be

}

}

e) The software does not properly handle when the expected number of parameters,
ﬁelds,or arguments is not provided in input, or if those parameters are undeﬁned. (CWE-
229)

p u b l i c

s t a t i c <M e x t e n d s Model > b o o l e a n isNew (M m, S t r i n g

pk_column ) {
f i n a l O b j e c t v a l = m. g e t ( pk_column ) ;
r e t u r n v a l == n u l l

v a l

| |

i n s t a n c e o f Number && ( ( Number

) v a l ) . i n t V a l u e ( ) <= 0 ;

}

Fig. 3: More Examples for Compliant Facet - All examples are
real detections from Policy2Code on CodeSearchNet-extended
corpus. CWE ids are provided for reference only.

Fig. 4: More Examples for Non-compliant Facet - All examples
are real detections from Policy2Code on CodeSearchNet-
extended corpus. CWE ids are provided for reference only.

code reviews and static analysis. Improvements can come from
innovating on input transformations, use of program analysis
and domain knowledge (API knowledge graph), and robust
ML formulations.

APPENDIX A
FACET-MASKED POLICY REPRESENTATION

The idea for facet-masked representation is based on the
conditional masking work, originally used in computer vision
to compute image similarities across aspects such as color
and texture [46]. The embedding of policy r is factorized
depending on facet value y. The factorization is implemented
via a conditional mask m ∈ Rd×nk , where d is the embedding
vector length and nk is the number of facets (=2). The mask
can be paramterized as m = σ(β), with σ denoting a rectiﬁed
linear unit so that σ(β) = max{0, β}. The kth column mk
plays the role of an element-wise gating function selecting the
relevant subspace of the embedding dimensions to attend to
the kth facet. Additional loss regularization terms are used to
encourage embeddings to be drawn from a unit ball (LW ) and
for mask regularization (LM ).

APPENDIX B
MODEL TUNING EXPERIMENTS

Performance of neural models can be signiﬁcantly improved
with proper hyper-parameter tuning (HPT). We used automatic
selection of parameters (batch size, learning rate, etc) based on a
validation dataset and experimented with triplet loss variations.
• Batch All Triplet loss computes the loss for all possible,
valid triplets in the batch, i.e., anchor and positive must
have the same label, anchor and negative a different label.
• Batch Hard Triplet loss computes the loss for all possible,
valid triplets. It then looks for the hardest positive (largest
d(fa, fp)) and the hardest negatives (smallest d(fa, fn))
per label class, and sums the loss only over them.

• Batch Semi-hard Triplet loss computes the loss for all
possible, valid triplets. It then looks for the semi hard
positives and negatives and sums the loss only over them.
• Batch Hard Triplet Soft-Margin loss is a variation of the
Batch Hard triplet loss where the loss over the hardest
positive and the hardest negative examples are computed
with a soft margin, e.g. log 1p(exp(d(fa, fp)−d(fa, fn))).
The determination of hard, semi-hard, and easy triplets was

made with a tunable margin parameter.

• Easy: d(fa − fp) + α < d(fa − fn). These are already

well separated and not useful for model training.

• Medium: d(fa − fp) < d(fa − fn) < d(fa − fp) + α.
These triplets are typically more suited for optimisation.
• Hard: d(fa − fn) < d(fa − fp). Selecting the hardest
negative examples can sometimes lead to bad local minima
and result in a collapsed model training [39].

Figure 5 shows the distribution of distances between the
embeddings of text anchors and positive and negative code
example for a sample of the dataset. The distribution is
partitioned in three sections based on the margin, used to

Fig. 5: Distribution of triplet distances can
be used to tune margins and mine triplets.

Fig. 6: Impact of margin on the pre-ﬁne-
tuning MRR of Batch All Triplet loss.

TABLE V: Performance of triplet loss variations

Batch Triplet Loss Type
All Triplets
Hard Triplets
Semi-Hard Triplets
Hard Triplets Soft-Margin

Overall Accuracy
57.2 ± 6.2
48.1 ± 4.7
50.13 ± 2.8
53.12 ± 5.1

Overall MRR
0.080 ± 0.02
0.063 ± 0.01
0.068 ± 0.02
0.065 ± 0.04

deﬁne easy, medium, and hard triplets. Figure 6 shows the effect
of margin with various triplet modelling strategies. Table V
compares the performance of loss variations on ﬁne-tuning the
CodeBERT model. The margin parameter is tuned for each
variation except the soft-margin formulation which does not use
a margin threshold. We observe that Batch All Triplet loss has
the best performance and use it for all subsequent experiments.

REFERENCES

[1] Mitre Corporation, “The Common Weakness Enumeration (CWE)

Initiative,” http://cwe.mitre.org/, 2021, [Online; accessed 2021].

[2] Industry Research, “Global static code analysis software market re-
port, history and forecast 2016-2027, breakdown data by companies,
key regions, types and application,” https://www.industryresearch.biz/
global-static-code-analysis-software-market-18726250, p. 105, 2021,
published: 2021-07-12.

[3] A. Habib and M. Pradel, “Neural bug ﬁnding: A study of opportunities
and challenges,” CoRR, vol. abs/1906.00307, 2019. [Online]. Available:
http://arxiv.org/abs/1906.00307

[4] M. Pradel and K. Sen, “Deepbugs: A learning approach to name-based
bug detection,” Proc. ACM Program. Lang., vol. 2, no. OOPSLA, Oct.
2018. [Online]. Available: https://doi.org/10.1145/3276517

[5] Y. Li, S. Wang, T. N. Nguyen, and S. Van Nguyen, “Improving bug
detection via context-based code representation learning and attention-
based neural networks,” Proc. ACM Program. Lang., vol. 3, no. OOPSLA,
Oct. 2019.

[6] N. Stulova, A. Blasi, A. Gorla, and O. Nierstrasz, “Towards detecting
inconsistent comments in java source code automatically,” in 2020 IEEE
20th International Working Conference on Source Code Analysis and
Manipulation (SCAM), 2020, pp. 65–69.

[23] E. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and P. Baldi,
“Sourcerer: Mining and searching internet-scale software repositories,”
Data Min. Knowl. Discov., vol. 18, no. 2, p. 300–336, Apr. 2009.
[Online]. Available: https://doi.org/10.1007/s10618-008-0118-x

[7] S. Sachdev, H. Li, S. Luan, S. Kim, K. Sen, and S. Chandra,
“Retrieval on source code: A neural code search,” in Proceedings of
the 2nd ACM SIGPLAN International Workshop on Machine Learning
and Programming Languages, ser. MAPL 2018. New York, NY,
USA: Association for Computing Machinery, 2018, p. 31–41. [Online].
Available: https://doi.org/10.1145/3211346.3211353

[8] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, “When deep

learning met code search,” 2019.

[9] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in Proceedings of the
40th International Conference on Software Engineering, ser. ICSE ’18.
New York, NY, USA: Association for Computing Machinery, 2018, p.
933–944. [Online]. Available: https://doi.org/10.1145/3180155.3180167
[10] H. Larochelle, D. Erhan, and Y. Bengio, “Zero-data learning of new
tasks,” in Proceedings of the 23rd National Conference on Artiﬁcial
Intelligence - Volume 2, ser. AAAI’08. Chicago, Illinois: AAAI Press,
2008, p. 646–651.

[11] Amazon Web Services, Amazon CodeGuru: Automate code reviews and
optimize application performance with ML-powered recommendations,
2021, https://aws.amazon.com/codeguru/.

[12] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code
search,” CoRR, vol. abs/1909.09436, 2019.
[Online]. Available:
http://arxiv.org/abs/1909.09436

[13] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to
represent programs with graphs,” in International Conference on
Learning Representations. Vancouver, BC, Canada: OpenReview.net,
2018. [Online]. Available: https://openreview.net/forum?id=BJOFETxR-
[14] P. Rani, M. Birrer, S. Panichella, M. Ghafari, and O. Nierstrasz,
“What do developers discuss about code comments?” in 2021 IEEE
21st International Working Conference on Source Code Analysis and
Manipulation (SCAM), 2021, pp. 153–164.

[15] J. K. Siow, C. Gao, L. Fan, S. Chen, and Y. Liu, “CORE: automating
review recommendation for code changes,” CoRR, vol. abs/1912.09652,
2019. [Online]. Available: http://arxiv.org/abs/1912.09652

[16] T. H. M. Le, H. Chen, and M. A. Babar, “Deep learning for source
code modeling and generation: Models, applications, and challenges,”
ACM Comput. Surv., vol. 53, no. 3, Jun. 2020. [Online]. Available:
https://doi.org/10.1145/3383458

[17] A. Naghshzan, L. Guerrouj, and O. Baysal, “Leveraging unsupervised
learning to summarize apis discussed in stack overﬂow,” in 2021 IEEE
21st International Working Conference on Source Code Analysis and
Manipulation (SCAM), 2021, pp. 142–152.

[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, ser. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.,
2017, p. 6000–6010.

[19] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained model for
programming and natural languages,” in Findings of the Association
for Computational Linguistics: EMNLP 2020. Online: Association
for Computational Linguistics, Nov. 2020, pp. 1536–1547. [Online].
Available: https://www.aclweb.org/anthology/2020.ﬁndings-emnlp.139

[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
transformers for language understanding,” in
of deep bidirectional
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers). Minneapolis,
Minnesota: Association for Computational Linguistics, Jun. 2019,
pp. 4171–4186. [Online]. Available: https://www.aclweb.org/anthology/
N19-1423

[21] P. Ramachandran, P. Liu, and Q. Le, “Unsupervised pretraining for
sequence to sequence learning,” in Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Processing. Copenhagen,
Denmark: Association for Computational Linguistics, Sep. 2017, pp. 383–
391. [Online]. Available: https://www.aclweb.org/anthology/D17-1039

[24] F. Lv, H. Zhang, J.-g. Lou, S. Wang, D. Zhang, and J. Zhao,
“Codehow: Effective code search based on api understanding and
extended boolean model,” in Proceedings of
the 30th IEEE/ACM
International Conference on Automated Software Engineering, ser. ASE
’15. Lincoln, Nebraska: IEEE Press, 2015, p. 260–270. [Online].
Available: https://doi.org/10.1109/ASE.2015.42

[25] C. Mcmillan, D. Poshyvanyk, M. Grechanik, Q. Xie, and C. Fu,
“Portfolio: Searching for relevant functions and their usages in millions
of lines of code,” ACM Trans. Softw. Eng. Methodol., vol. 22, no. 4, oct
2013. [Online]. Available: https://doi.org/10.1145/2522920.2522930
[26] M. Raghothaman, Y. Wei, and Y. Hamadi, “Swim: Synthesizing what i
mean - code search and idiomatic snippet synthesis,” in 2016 IEEE/ACM
38th International Conference on Software Engineering (ICSE). Austin,
TX, USA: IEEE, 2016, pp. 357–367.

[27] M. Lu, X. Sun, S. Wang, D. Lo, and Yucong Duan, “Query expansion
via wordnet for effective code search,” in 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and Reengineering (SANER),
no. 22. Montreal, CA: IEEE, 2015, pp. 545–549.

[28] M. Allamanis, D. Tarlow, A. D. Gordon, and Y. Wei, “Bimodal modelling
of source code and natural language,” in Proceedings of the 32nd
International Conference on International Conference on Machine
Learning - Volume 37, ser. ICML’15. Lille, France: JMLR.org, 2015,
p. 2123–2132.

[29] Y. Wan, J. Shu, Y. Sui, G. Xu, Z. Zhao, J. Wu, and P. S.
Yu, “Multi-modal attention network learning for semantic source
code retrieval,” in Proceedings of the 34th IEEE/ACM International
Conference on Automated Software Engineering, ser. ASE ’19, no. 34.
San Diego, California: IEEE Press, 2019, p. 13–25. [Online]. Available:
https://doi.org/10.1109/ASE.2019.00012

[30] X. Ling, L. Wu, S. Wang, G. Pan, T. Ma, F. Xu, A. X. Liu,
C. Wu, and S. Ji, “Deep graph matching and searching for semantic
code retrieval,” CoRR, vol. abs/2010.12908, 2020. [Online]. Available:
https://arxiv.org/abs/2010.12908

[31] J. P. Diniz, D. Cruz, F. Ferreira, C. Tavares, and E. Figueiredo, “Github
label embeddings,” in 2020 IEEE 20th International Working Conference
on Source Code Analysis and Manipulation (SCAM), 2020, pp. 249–253.
[32] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, “When deep
learning met code search,” in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ser. ESEC/FSE 2019.
New York, NY, USA: Association for Computing Machinery, 2019, p.
964–974. [Online]. Available: https://doi.org/10.1145/3338906.3340458
[33] M. de Rezende Martins and M. A. Gerosa, “Concra: A convolutional
neural networks code retrieval approach,” in Proceedings of the 34th
Brazilian Symposium on Software Engineering, ser. SBES ’20. New
York, NY, USA: Association for Computing Machinery, 2020, p.
526–531. [Online]. Available: https://doi.org/10.1145/3422392.3422462
[34] Z. Yao, J. R. Peddamail, and H. Sun, “Coacor: Code annotation
for code retrieval with reinforcement learning,” in The World Wide
Web Conference, ser. WWW ’19. New York, NY, USA: Association
for Computing Machinery, 2019, p. 2203–2214. [Online]. Available:
https://doi.org/10.1145/3308558.3313632

[35] M. Kaya and H. Bilge, “Deep metric learning: A survey,” Symmetry,

vol. 11, p. 1066, 2019.

[36] Y. Luan, J. Eisenstein, K. Toutanova, and M. Collins, “Sparse, Dense,
and Attentional Representations for Text Retrieval,” Transactions of the
Association for Computational Linguistics, vol. 9, pp. 329–345, 04 2021.
[Online]. Available: https://doi.org/10.1162/tacl_a_00369

[37] D. Tunkelang, “Faceted search,” in Faceted Search. NY, USA: Morgan

and Claypool, 2009.

[38] O. Ben Yitzhak, N. Golbandi, N. Har’El, R. Lempel, A. Neumann,
S. Ofek-Koifman, D. Sheinwald, E. Shekita, B. Sznajder, and S. Yogev,
“Beyond basic faceted search,” WSDM’08 - Proceedings of the 2008
International Conference on Web Search and Data Mining, vol. 8, pp.
33–44, 01 2008.

[22] A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer,
and S. Gupta, “Muppet: Massive multi-task representations with pre-
ﬁnetuning,” 2021.

[39] R. Manmatha, C.-Y. Wu, A. Smola, and P. Krähenbühl, “Sampling matters
in deep embedding learning,” 2017 IEEE International Conference on
Computer Vision (ICCV), vol. 1, no. 1, pp. 2859–2867, 2017.

[40] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed embed-
ding for face recognition and clustering,” CoRR, vol. abs/1503.03832,
2015. [Online]. Available: http://arxiv.org/abs/1503.03832

[41] A. Hermans, L. Beyer, and B. Leibe, “In Defense of the Triplet Loss
for Person Re-Identiﬁcation,” arXiv e-prints, vol. 1, no. 1, Mar. 2017.

[42] W. Chen, X. Chen, J. Zhang, and K. Huang, “Beyond triplet loss:
a deep quadruplet network for person re-identiﬁcation,” CoRR, vol.
abs/1704.01719, 2017. [Online]. Available: http://arxiv.org/abs/1704.
01719

[43] F. Luo, P. Li, P. Yang, J. Zhou, Y. Tan, B. Chang, Z. Sui, and X. Sun,
“Towards ﬁne-grained text sentiment transfer,” in Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics.
Florence, Italy: Association for Computational Linguistics, Jul. 2019,
pp. 2013–2022. [Online]. Available: https://aclanthology.org/P19-1194
[44] J. Li, R. Jia, H. He, and P. Liang, “Delete, retrieve, generate: A simple
approach to sentiment and style transfer,” CoRR, vol. abs/1804.06437,
2018. [Online]. Available: http://arxiv.org/abs/1804.06437

[45] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer
learning with a uniﬁed text-to-text transformer,” Journal of Machine
Learning Research, vol. 21, no. 140, pp. 1–67, 2020. [Online]. Available:
http://jmlr.org/papers/v21/20-074.html

[46] A. Veit, S. Belongie, and T. Karaletsos, “Conditional similarity networks,”
in 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), vol. 1. HI, USA: IEEE, 2017, pp. 1781–1789.

[47] N. Sawant and S. H. Sengamedu, “Learning-based identiﬁcation of coding
best practices from software documentation,” in 2022 IEEE International
Conference on Software Maintenance and Evolution. Limassol, Cyprus:
IEEE Computer Society, oct 2022, pp. –.

[48] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation
the 54th
the Association for Computational Linguistics
Berlin, Germany: Association for
[Online].

of rare words with subword units,” in Proceedings of
Annual Meeting of
(Volume 1: Long Papers).
Computational Linguistics, Aug. 2016, pp. 1715–1725.
Available: https://www.aclweb.org/anthology/P16-1162

[49] J. Johnson, M. Douze, and H. Jégou, “Billion-scale similarity search
with gpus,” IEEE Transactions on Big Data, vol. -, no. -, pp. 1–1, 2019.

