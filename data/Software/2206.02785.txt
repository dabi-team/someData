Zeroth-Order SciML: Non-intrusive Integration of Scientiﬁc Software with Deep
Learning

Ioannis Tsaknakis 1 Bhavya Kailkhura 2 Sijia Liu 3 Donald Loveland 4 James Diffenderfer 2
Anna Maria Hiszpanski 2 Mingyi Hong 1

Abstract

Using deep learning (DL) to accelerate and/or im-
prove scientiﬁc workﬂows can yield discoveries
that are otherwise impossible. Unfortunately, DL
models have yielded limited success in complex
scientiﬁc domains due to large data requirements.
In this work, we propose to overcome this issue by
integrating the abundance of scientiﬁc knowledge
sources (SKS) with the DL training process. Exist-
ing knowledge integration approaches are limited
to using differentiable knowledge source to be
compatible with ﬁrst-order DL training paradigm.
In contrast, our proposed approach treats knowl-
edge source as a black-box in turn allowing to in-
tegrate virtually any knowledge source. To enable
an end-to-end training of SKS-coupled-DL, we
propose to use zeroth-order optimization (ZOO)
based gradient-free training schemes, which is
non-intrusive, i.e., does not require making any
changes to the SKS. We evaluate the performance
of our ZOO training scheme on two real-world
material science applications. We show that pro-
posed scheme is able to effectively integrate sci-
entiﬁc knowledge with DL training and is able
to outperform purely data-driven model for data-
limited scientiﬁc applications. We also discuss
some limitations of the proposed method and men-
tion potentially worthwhile future directions.

2
2
0
2

n
u
J

4

]

G
L
.
s
c
[

1
v
5
8
7
2
0
.
6
0
2
2
:
v
i
X
r
a

1. Introduction

Deep learning (DL) provides incredible opportunities to
answer some of the most important and difﬁcult questions
in a wide range of scientiﬁc applications. However, many
of the scientiﬁc use cases such as materials discovery do not
have access to large amounts of data required to train DL

1University of Minnesota 2Lawrence Livermore National
Laboratory 3Michigan State University 4University of Michi-
gan, Ann Arbor Correspondence to: Bhavya Kailkhura
<kailkhura1@llnl.gov>.

models. In such data-limited complex scientiﬁc domains,
purely data-driven DL models have yielded limited success
or led to unsatisfactory results. At ﬁrst glance, the task of
training an accurate deep neural network (DNN) from a
small amount of data appears impossible. Coming to the
rescue is a wealth of prior domain knowledge in the form of
scientiﬁc software codes – most of which is currently not
being utilized as purely-data driven models inefﬁciently try
to learn already known chemistry from scratch.

The biggest challenge in integrating scientiﬁc knowledge
into the learning process is formulating it in a manner that
is compatible with the current training paradigm. Given the
fact that nearly all DNN training approaches are based on
differentiable programming, all existing efforts in integrat-
ing scientiﬁc knowledge(von Rueden et al., 2019) are either
(a) restricted to using a very limited subset of knowledge
that is already in a differentiable form, or (b) spending sig-
niﬁcant effort on modifying knowledge sources to support
automatic differentiability (autodiff). As an example of (a),
Physics-aware machine learning (ML) area (Karniadakis
et al., 2021) uses analytical equations, which only capture a
very limited aspect of otherwise complex system. As an ex-
ample of (b), Differentiable Physics/Chemistry area (Kasim
et al., 2022; Belbute-Peres et al., 2020) requires updating,
or altogether rewriting, existing software codes to provide
autodiff functionality, which is extremely costly. This points
to a crucial roadblock in using DL in scientiﬁc domains: we
do not have methods that can exploit all available scientiﬁc
knowledge without being intrusive, i.e., requiring a rewrite
of existing software codes.

To overcome the challenge of effortless integration of sci-
entiﬁc knowledge with DL, we propose making changes
to the ML paradigm to be compatible with scientiﬁc soft-
ware, rather than the other way around. Speciﬁcally, we
propose to leverage gradient-free optimization approaches
to train scientiﬁc software coupled deep neural nets. Note
that resulting “sciML” system does not provide gradients
rendering popular ﬁrst-order methods (e.g., stochastic gradi-
ent descent) inapplicable. Unlike the current gradient-based
training paradigm, i.e., backprop, our proposed technique
aims to train sciML models accurately using only function

 
 
 
 
 
 
Integrating Black-Box Scientiﬁc Knowledge with Deep Learning

queries (or forward passes), thereby bypassing the strin-
gent requirement of gradient availability. To achieve this,
we adopt a subset of gradient-free optimization, Zeroth-
Order Optimization (ZOO) (Liu et al., 2020; 2018), and
train sciML system in an end-to-end manner. To show the
feasibility of our approach, we apply our technique on a
popular material discovery pipeline on two datasets. This
pipeline is used to produce novel molecules with desired
properties by searching for molecular graphs in the graph
generative model’s latent space. The sciML system for this
problem is comprised of the following three components:
1) Generative model: junction tree variational autoencoder
(JT-VAE), 2) Molecular Featurizer: RDKit cheminformatics
software, and 3) Predictive model: message passing neural
network (MPNN) (see Figure 1). Note that the reasoning
behind using a generative model is to transform the original
discrete space where molecules lie to a continuous space
that is amenable for search and optimization. Unfortunately,
the presence of RDKit software (which is effectively a black-
box function) renders the sciML system non-differentiable.
This precludes the use of gradient-based methods for end-to-
end training. On the other hand, existing approaches resort
to poor workaround of using purely data-driven models.
These approaches pretrain the JT-VAE and MPNN sepa-
rately using ﬁrst-order methods. However, in this approach,
the JT-VAE training if not aware of the property of interest
predicted by MPNN. Thus, JT-VAE latent space often is not
amenable to maximizing the property predicted by MPNN.
More importantly purely data-driven approaches require a
large amount of data for learning generalizable patterns.

In this paper, on the other hand, we aim to perform an
end-to-end training of sciML sysyem in Figure 1. With a
clever application of zeroth-order methods, we manage to
overcome this obstacle and develop gradient-free end-to-end
training algorithms. We show that the proposed zeroth-order
training approach can outperform purely data-driven ﬁrst-
order methods by exploiting the knowledge encoded in the
scientiﬁc software RDKit. This is the key contribution of
this paper, and we hope that its application extends beyond
the material science use case considered in this work.

2. Problem Formulation

Although our formulation of integrating black-box knowl-
edge with DNN is quite general, we concretize our discus-
sion with a speciﬁc real-world material science application.

2.1. Molecular Property Prediction System

Here we provide details on our molecular property predic-
tion system (Figure 1) that serves as a fundamental building
block for molecular screening and discovery problems.

Let us deﬁne a dataset D = {(xi, yi)}N

i=1 of N molecules,

where xi is the graph G = (V, E) of a single molecule with
vertex features {su}u∈V and edge features {euv}uv∈E; the
vertex and edge features describe certain atom and bond
characteristics, respectively. The vector yi denotes certain
chemical properties, such as density and heat of formation.

The predictive system of interest consists of the following
three distinct components.

1. Generative model (JT-VAE)

We use the Junction Tree Variational Auto-Encoder
(JT-VAE) (Jin et al., 2018) tailored for working on
molecular graphs. It consists of: 1) an encoder which
is a neural network FE(·; u) that maps the input x
(molecular graph) to a latent representation z where
u denotes the encoder parameters, and 2) a decoder
which is a neural network FD(·; v) that maps (recon-
structs) the latent representation z back to a molecular
graph ˜x where v denotes the decoder parameters.

2. Molecular Featurizer (RDKit)

JT-VAE output is followed by a cheminformatics soft-
ware that takes a molecule (in the SMILES string form)
as input, computes certain molecular descriptors, and
outputs a molecular graph where these descriptors are
then incorporated as node and edge features. We use
the RDKit library (Landrum, 2016) as our cheminfor-
matics knowledge base. For each problem, any feature
that has a constant value across the whole dataset is re-
moved prior to training. From a modelling perspective
we treat this component as a black box G(·) in which
we can send queries (molecules) ˜x and get molecular
graphs ˆx with certain edge and node features as outputs.
However, note that the black-box nature of G prevents
us from having direct access to its gradients, and thus
the pipeline is effectively rendered non-differentiable.

3. Predictive model (MPNN)

We use the Message Passing Neural Net (MPNN) as
our predictive model, which we denote with H(·; w)
where w denotes the model parameters. MPNN takes
a molecular graph ˆx as an input and predicts certain
molecular properties. Note that MPNN is a graph neu-
ral network (GNN); thus, it offers the advantage of
leveraging node/edge features and connectivity avail-
able in the network structure of a molecule.

Using the above notations,
molecule i is denoted as

the predicted property of

(cid:101)yi = H (G (FD (FE (xi; u) ; v)) ; w) .

(1)

2.2. RDKit Integrated Training of JT-VAE and MPNN

Note that material science applications usually are data-
limited, which makes it difﬁcult for purely data-driven mod-

Integrating Black-Box Scientiﬁc Knowledge with Deep Learning

Figure 1. Scientiﬁc machine learning pipeline for molecular property prediction and generation.

els to learn generalizable patterns. In this work, we aim
to improve the performance of the aforementioned predic-
tion system by leveraging end-to-end training with RDKit
as an additional knowledge source. The objective of the
training procedure is to jointly optimize the parameters of
JT-VAE (i.e., encoder and decoder) and MPNN. This goal
is achieved by solving the following optimization problem:

L(x, y; u, v, w),

min
u,v,w
L = Lmpnn(x, y; u, v, w) + λ Ljtvae (x; u, v) ,

(2)

N
(cid:88)

1
N

i=1

Lmpnn =

Ljtvae = (cid:96) (cid:0)FD

(cid:107)(cid:101)yi − yi(cid:107)2,
(cid:0){xi}N
(cid:0)FE
where (cid:96)(·, ·) denotes the JT-VAE loss (for more details refer
to (Jin et al., 2018)), λ is a regularization parameter, and
(cid:101)yi is deﬁned in (1). Note that Lmpnn is a function of RD-
Kit output, which makes it necessary to use gradient-free
optimization methods.

i=1; u(cid:1) ; v(cid:1) , {xi}N

(cid:1) ,

i=1

3. Proposed Zeroth-Order Training

Algorithm

To solve (2), we adopt a variant of gradient-free optimiza-
tion, Zeroth-Order Optimization (ZOO), which operates
similarly to iterative methods: gradient estimation (via func-
tion evaluations), descent direction computation, and solu-
tion update. Thus, our ZOO end-to-end training replaces
gradients with their estimates (whose exact form is deﬁned
in the next section) and proceeds as traditional training.

3.1. ZOO for the sciML System in Figure 1

Speciﬁcally, the gradient of loss L (w.r.t.
parameters u, v, w) are as follows:

to the model

∇wL = ∇wLmpnn({(xi, yi)}N
(cid:101)∇u,vL = (cid:101)∇u,vLmpnn({(xi, yi)}N

i=1; u, v, w)

i=1; u, v, w)

+ λ∇u,vLjtvae

(cid:0){xi}N

i=1; u, v(cid:1)

=

1
N

N
(cid:88)

i=1

2 ((cid:101)yi − yi) (cid:101)∇u,v(cid:101)yi + λ∇u,vLjtvae,

where the symbol (cid:101)∇ is used to denote a gradient estimate.
Using the chain rule we have

(cid:101)∇u(cid:101)yi = ∇uFE(xi; u)∇1FD (FE (xi; u) ; v) ×
(cid:101)∇G (FD (FE (xi; u) ; v)) ∇1H (G (FD (FE (xi; u) ; v)) ; w)
(3)

(cid:101)∇v (cid:101)yi = ∇vFD (FE (xi; u) ; v) (cid:101)∇G (FD (FE (xi; u) ; v))

× ∇1H (G (FD (FE (xi; u) ; v)) ; w) ,

(4)

where ∇1 denotes the gradient w.r.t. the ﬁrst argument (i.e.,
the model’s input) and (cid:101)∇G some gradient estimate of the
black-box function G(·).

Challenges with sciML System in Figure 1. Upon a
closer examination of aforementioned ZOO steps, notice
some major technical hurdles arise due to subtle application
level details. Next, we explain these issues in detail.

1. The gradients involved in (3), (4) are not well-deﬁned
for the application of interest. Speciﬁcally, notice that

• The term (cid:101)∇G (FD (FE (x; u) ; v)) denotes the
gradient estimate of graph w.r.t. SMILES string.
• The term ∇1H (G (FD (FE (x; u) ; v)) ; w) de-

notes the gradient of a scalar w.r.t. a graph.

2. The gradient estimates we aim to form for black-
box function G(·) will involve a quantity of the form
G(x + µd) − G(x), (which typically appears in zeroth-
order gradient estimators) where x is a molecule and
d is a direction in the molecules space (the exact repre-
sentation, e.g., SMILES string or graph, is not relevant
at this point); the output of G(·) is a graph. Then, it is
not clear what the meaning of the above difference is.

Integrating Black-Box Scientiﬁc Knowledge with Deep Learning

Figure 2. We change the boundaries of the three components for our algorithm design compared to the original one in Fig. 1.

3.2. Reparameterization to the Rescue

From the above discussion, it is clear that we need to use
reparameterization in order to resolve these issues. Our
approach is to adjust the boundaries of the three components
that comprise the system (see Figure 2). Speciﬁcally, we
perform the following changes:

• The boundary between the 1st component and the 2nd
component becomes the intermediate latent layer of the
JT-VAE. We maintain the same notation as previously
and denote the input-output relation with FE (x; u).

• The 2nd component includes the layers of the MPNN
which performs the embedding of the graph into a con-
tinuous latent space. Speciﬁcally, the output of the 2nd
component is the readout function of the MPNN. Note
that the presence of the RDKit module in this compo-
nent renders the pipeline incompatible to ﬁrst-order
methods. We denote this component with (cid:101)G(·; v, w1),
where w1 includes the parameters of the MPNN inside
this component.

• The third component now includes only the part of
the MPNN after the readout function. We denote the
parameters involved in this part with w2 and the input-
output relation as (cid:101)H(·; w2).

The output of the reparameterized system can be written as
(cid:101)y = (cid:101)H( (cid:101)G(FE(xi, u); v, w1); w2). As a result, the chain
rule is rearranged as:

(cid:101)∇u(cid:101)yi = ∇uFE(xi; u) (cid:101)∇1 (cid:101)G(FE(xi; u); v, w1)

× ∇1 (cid:101)H( (cid:101)G(FE(xi; u); v, w1); w2)

(cid:101)∇v(cid:101)yi = (cid:101)∇v (cid:101)G(FE(xi; u); v, w1)

× ∇1 (cid:101)H( (cid:101)G(FE(xi; u); v, w1); w2).

we employ a coordinate-wise zeroth-order (ZO) gradient
estimator (Liu et al., 2020):

(cid:101)∇1 (cid:101)G(FE(x; u); v, w1) =

(cid:101)G(FE(x; u) + µ1d1; v, w1) − (cid:101)G(FE(x; u); v, w1)
µ1
(cid:101)∇v (cid:101)G(FE(x; u); v, w1) =

d1

(cid:101)G(FE(x; u); v + µ2d2, w1) − (cid:101)G(FE(x; u); v, w1)
µ2

d2,

where µ1, µ2 are the smoothing parameters, and d1, d2 are
directions in the (continuous) latent space; the elements of
these direction vectors are Gaussian random variables of
mean 0 and variance σ2.

4. Experiments

Goal: Our experiments try to ﬁnd: can the ZO training
approach integrate black-box knowledge source in sciML
learning process to learn more generalizable models despite
of data scarcity?

Datasets: We consider the following two datasets:

1. ZINC (Sterling & Irwin, 2015): We have 5500
molecules in the training set and 5000 molecules in
the test set. Similar to (Kusner et al., 2017; Jin et al.,
2018), the property we predict is a penalized version of
the water-octanol partition coefﬁcients (log P ), speciﬁ-
cally the quantity log P − SA − RS, where SA is the
synthetic accessibility score and RS is the ring size.

2. HOF (Griessen & Riesterer, 1988) dataset: This dataset
contains 5600 molecules in the training set and 7000
in the test set. The property of interest is the heat of
formation.

Zeroth-order estimators. One aspect we have not ad-
dressed so far is the speciﬁc choice for the gradient estimator
of the black-box function (cid:101)∇1 (cid:101)G(·), (cid:101)∇v (cid:101)G(·). In this work,

Models and Hyperparameters: For each dataset we per-
form the following set of experiments:

Integrating Black-Box Scientiﬁc Knowledge with Deep Learning

1. Stage1: We train a JT-VAE and a MPNN model inde-
pendently (or in a purely data-driven manner) using
the DGL-LifeSci package1.

2. Stage2: For Stage2 models, we inject RDKit knowl-
edge into the purely data-driven models by using
zeroth-order end-to-end training.

Below we provide details on our hyperparameter selection,
which remain the same across both experiments.

1. JT-VAE: We use learning rate = 0.0007, network
depth= 3, size of hidden layers= 450 (for both the
encoder and the decoder), and dimension of latent
space= 56.

2. MPNN: We use learning rate= 0.0005, size of node
input features= 39, size of edge input features= 50,
size of output node representations= 128, size of hid-
den edge representations= 64, and number of message
passing steps= 6.

Note that the batch size was 16 for Stage1 experiments,
and 8 for Stage2. Moreover, in Stage2 experiments we
employ gradient clipping in order to deal with exploding
gradients. We also set λ = 1 and freeze the parameters
of the decoder (this is a reasonable choice as the decoder
was already pretrained sufﬁciently well in Stage1). Finally,
zeroth-order gradient estimators uses µ1 = µ2 = 0.001.

Metrics: We compare the performance of our approach
w.r.t. baselines in terms of following metrics:

1. Reconstruction ability of the generative model.

The molecule is passed through the JT-VAE, where it
is ﬁrst decoded (i.e., we obtain its continuous repre-
sentation in the latent space), and then encoded back
into a (graph) molecular form. We consider the re-
construction successful if the molecule we get in the
output (after the decoder) is the same as the molecule
we provide as input (before the encoder). We report
the reconstruction accuracy, which is deﬁned as

accuracy =

#successfully reconstructed molecules
total number of molecules evaluated

.

2. Predictive ability of the supervised (predictive) model

We compute the Root Mean Square Error (RMSE) over
the dataset, between the true value of the molecular
property and the value predicted by the MPNN,

RMSE =

(cid:34)

1
N

N
(cid:88)

i=1

(cid:35)1/2

.

(yi − (cid:101)yi)2

1https://github.com/awslabs/dgl-lifesci

Dataset

ZINC

HOF

Model (Measure)
JT-VAE (Recon. accuracy)
MPNN (RMSE) with JT-VAE
JT-VAE (Recon. accuracy)
MPNN (RMSE) with JT-VAE

Stage1
Stage2
4.59% 3.97%
1.83
1.66
22.5% 15.5%
52.2

53

Table 1. Performance comparison of purely data-driven models
(trained with ﬁrst-order method) with RDKit integrated models
(trained with proposed zeroth-order method).

Results: Table 1 presents the results of the experiments
described above. Note that MPNN is trained in Stage 1 for a
maximum number of epochs such that we can no longer im-
prove its performance (i.e., decrease RMSE). Then, notice
that the jointly training the MPNN (along with the JT-VAE)
leads to better prediction performance, under both datasets.
Although we observe a decrease in the reconstruction error,
note that reconstruction is not our main objective. In fact
the main motivation behind using JT-VAE is to obtain a
continuous latent space. Regardless, a successful training
by simply using forward passes (i.e., without gradients) is
exciting and motivates more research in this direction.

We also mention some limitation of the proposed ZOO
scheme. First, ZOO requires large runtime mainly due to
application related factors that we cannot completely control.
For instance, the decoding phase of the JT-VAE, which is
involved in the computation of (cid:101)G(FE(x; u) + µ1d1; v, w1)
and (cid:101)G(FE(x; u); v + µ2d2, w1) (in the ZO estimates) can
be very slow, especially for novel molecules. Second, JT-
VAE requires pretraining in order to decode molecules rea-
sonably fast. Finally, training requires carefully balancing
prediction error vs. reconstruction accuracy. Choosing right
hyperparameters is critical to a stable training.

5. Conclusion

In this paper, we developed gradient-free training ap-
proaches for enabling integration of black-box scientiﬁc
knowledge sources with deep learning. Using two materials
datasets, we showed that proposed knowledge-integrated
models outperform purely data-driven models in small-data
regime. In the future we will explore the use of more sophis-
ticated zeroth-order gradient estimators, e.g., average sev-
eral gradient estimates across different directions. Another
worthwhile direction is to apply the proposed approach to
additional applications where scientiﬁc knowledge is avail-
able in the form of a black-box. Our results provide a new
perspective on the integration of knowledge sources with
deep learning. We hope that our positive results will en-
courage deep learning community to pay more attention to
ZOO-based knowledge integration.

Integrating Black-Box Scientiﬁc Knowledge with Deep Learning

von Rueden, L., Mayer, S., Beckh, K., Georgiev, B., Gies-
selbach, S., Heese, R., Kirsch, B., Pfrommer, J., Pick, A.,
Ramamurthy, R., et al. Informed machine learning–a tax-
onomy and survey of integrating knowledge into learning
systems. arXiv preprint arXiv:1903.12394, 2019.

Acknowledgement

This work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National
Laboratory under Contract DE-AC52-07NA27344. James
Diffenderfer’s effort was supported by the LLNL-LDRD
Program under Project No. 22-FS-019.

References

Belbute-Peres, F. D. A., Economon, T., and Kolter, Z. Com-
bining differentiable pde solvers and graph neural net-
works for ﬂuid ﬂow prediction. In International Con-
ference on Machine Learning, pp. 2402–2411. PMLR,
2020.

Griessen, R. and Riesterer, T. Heat of formation models.
Hydrogen in intermetallic compounds I, pp. 219–284,
1988.

Jin, W., Barzilay, R., and Jaakkola, T. Junction tree vari-
ational autoencoder for molecular graph generation. In
International Conference on Machine Learning, pp. 2323–
2332. PMLR, 2018.

Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris,
P., Wang, S., and Yang, L. Physics-informed machine
learning. Nature Reviews Physics, 3(6):422–440, 2021.

Kasim, M. F., Lehtola, S., and Vinko, S. M. Dqc: A python
program package for differentiable quantum chemistry.
The Journal of chemical physics, 156(8):084801, 2022.

Kusner, M. J., Paige, B., and Hern´andez-Lobato, J. M.
Grammar variational autoencoder. In International con-
ference on machine learning, pp. 1945–1954. PMLR,
2017.

Landrum, G. Rdkit: Open-source cheminformatics soft-
ware. 2016. URL https://github.com/rdkit/
rdkit/releases/tag/Release_2016_09_4.

Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and
Amini, L. Zeroth-order stochastic variance reduction for
nonconvex optimization. Advances in Neural Information
Processing Systems, 31, 2018.

Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III,
A. O., and Varshney, P. K. A primer on zeroth-order
optimization in signal processing and machine learning:
Principals, recent advances, and applications. IEEE Sig-
nal Processing Magazine, 37(5):43–54, 2020.

Sterling, T. and Irwin, J. J. Zinc 15 – ligand discovery for
everyone. Journal of Chemical Information and Mod-
eling, 55(11):2324–2337, 2015. doi: 10.1021/acs.jcim.
5b00559. URL https://doi.org/10.1021/acs.
jcim.5b00559. PMID: 26479676.

