2
2
0
2

y
a
M
1
3

]

V
C
.
s
c
[

1
v
6
3
5
5
1
.
5
0
2
2
:
v
i
X
r
a

DeepDefacer: Automatic Removal of Facial Features via U-Net
Image Segmentation

Anish Khazane
akkhazan@berkeley.edu

Julien Hoachuck
jhoachuck3@gatech.edu

Krzysztof J. Gorgolewski
Department of Psychology
Stanford University

Russell A. Poldrack
Department of Psychology
Stanford University

Abstract

1

Introduction

Recent advancements in the ﬁeld of magnetic resonance
imaging (MRI) have enabled large-scale collaboration
among clinicians and researchers for neuroimaging tasks.
However, researchers are often forced to use outdated
and slow software to anonymize MRI images for pub-
lication. These programs speciﬁcally perform expen-
sive mathematical operations over 3D images that rapidly
slows down anonymization speed as an image’s volume
increases in size.

In this paper, we introduce DeepDefacer, an applica-
tion of deep learning to MRI anonymization that uses
a streamlined 3D U-Net network to mask facial regions
in MRI images with a signiﬁcant increase in speed over
traditional de-identiﬁcation software. We train DeepDe-
facer on MRI images from the Brain Development Or-
ganization (IXI) and International Consortium for Brain
Mapping (ICBM) and quantitatively evaluate our model
against a baseline 3D U-Net model with regards to Dice,
recall, and precision scores. We also evaluate DeepDe-
facer against Pydeface, a traditional defacing application,
with regards to speed on a range of CPU and GPU de-
vices and qualitatively evaluate our model’s defaced out-
put versus the ground truth images produced by Pydeface.
We provide a link to a PyPi program at the end of this
manuscript to encourage further research into the applica-
tion of deep learning to MRI anonymization.

1

Within the past few decades, improvements in comput-
ing technology have allowed researchers and clinicians to
create more detailed magnetic resonance imaging (MRI)
images and store gigabytes of sensitive information in
electronic patient databases [14]. However, the growing
storage and dissemination of MRI images raises privacy
concerns as neuroimaging data can reveal sensitive fa-
cial information about patients, as shown in Figure 1. [3].
The U.S Health Insurance Portability and Accountability
Act (HIPAA) deﬁnes any full face photographic images
as protected health information (PHI), and mandates the
de-identiﬁcation of sensitive information prior to sharing
of the data [3]. Thus, the goal of de-identiﬁcation is to re-
move any identifying facial information in a MRI image.
to tackle de-
identiﬁcation by using linear co-registration methods to
mask non-brain tissue in an image [2]. ”Pydeface”, a
common de-identiﬁcation software, defaces an image by
using co-registration methods to translate a facemask onto
the facial region of a scan. [2]. Other techniques use a
”facial atlas,” to predict the probability that a voxel repre-
sents facial features [14]. However, these techniques are
limited because the constructed facial atlas can only be
applied to T1-Weighted MRI images, which encompasses
a very limited range of voxel intensities. Pydeface, on the
other hand, does support co-registration using between-

standard methods attempt

Several

 
 
 
 
 
 
(a)

Figure 2: Example of a T1-Weighted MRI scan on the left
and a proper defacing of the image on the right.

(a)

Figure 1: An example of a 3D reconstructed image from
a MRI scan [11].

modality cost functions - e.g mutual information (MMI),
normalized MMI - to decrease the error between inten-
sity values in a reference image and an input image of a
different contrast type [6]. However, images with small
resolution sizes can throw these correlation methods off
balance as many cost functions rely on constructing 2D
joint intensity histograms that are especially susceptible
to differences among a small number of voxels.

In this paper, we present Deepdefacer, a modiﬁed 3D
U-Net image segmentation network that can rapidly de-
face 3D MRI scans on both CPU and GPU devices. To
our knowledge, our work is the ﬁrst application of deep
learning to MRI anonymization.

Due to having a fully convolutional architecture that is
also heavily streamlined, Deepdefacer has 92% fewer pa-
rameters than the original 3D U-Net model and is able to
deface MRI images at nearly 90% faster speed than Pyde-
face. We train the network on pairs of non-defaced MRI
images and 3D binary masks from the Brain Development
Organization (IXI) and the International Consortium for
Brain Mapping (ICBM) datasets in order to predict binary
masks that are subsequently multiplied against original
MRI images to deface them. [1, 10]. We use a traditional
defacing software, Pydeface, to construct the ground truth
masks in our input. Figure 2 shows examples of images
in our datasets.

In the following sections, we brieﬂy expand on the dis-

advantages of classical de-identiﬁcation software (such
as Pydeface) that use numerical algorithms to anonymize
MRI images. We then discuss 2D versus 3D image seg-
mentation networks for medical imaging and our reason-
ing behind using the 3D U-Net architecture for this task.
Afterwards, we present our own modiﬁcations to the 3D
U-Net network to create Deepdefacer, along with explain-
ing its full training and inference pipeline. We then quan-
titatively compare the model against a baseline 3D U-Net
architecture with regards to Dice, precision, and recall
scores and Pydeface for speed. We also qualitatively com-
pare outputs from both Deepdefacer and Pydeface to an-
alyze model generalization. Lastly, we conclude this pa-
per by discussing areas where Deepdefacer’s performance
can be further improved along with plans for building a
product API that allows users of all technical backgrounds
to utilize the model for MRI defacing.

2 Related Works

2.1 Classical De-Identiﬁcation Algorithms

Classical algorithms for MRI anonymization typically fall
into two categories: skull-stripping and co-registration
[15].
Skull-stripping techniques delineate the brain
boundary of a MRI scan in order to completely remove
voxels representing non-brain tissue from the original im-
age. While these stripping methods can adequately re-
move facial features in scans, their performance is typi-
cally conditional on the MRI image type (T1-weighted,

2

T2-weighted, etc) and consequently may incorrectly re-
move voxels representing brain tissue if image composi-
tion is previously unknown to the program [15]. Further-
more, several skull-stripping techniques are highly sensi-
tive to differences in image resolution and size, are unable
to generalize across data sets, and can incorrectly mod-
ify images in ways that impair post processing and analy-
sis. [2, 14].

Co-registration techniques fall into two different cat-
egories: automated and manual registration. Pydeface,
an automated registration tool, calculates an afﬁne trans-
formation from a reference template to an input image,
which is subsequently applied against a facemask to trans-
late the map over the facial region of the original input
image [6]. As previously mentioned, Pydeface also sup-
ports between-modality registration which uses appropri-
ate cost functions to determine an optimal linear transfor-
mation for images with unique compositions. One of Py-
deface’s critical shortcomings however is anonymization
speed: Pydeface in particular takes upwards of several
minutes to deface images with high resolution. On the
other hand, manual registration techniques employ user-
deﬁned landmarks on the brain portion of a scan to help
registration software deﬁne an afﬁne transformation that
properly defaces an image [9]. While pre-deﬁned land-
marks improves defacing speed, the time-consuming - and
subjective - task of manually modifying MRI scans with
markers is not a scalable option [9].

2.2 Survey of Image Segmentation Net-

works

To avoid the shortcomings of classical de-identiﬁcation
algorithms, we turn to deep learning and survey the com-
puter vision literature on image segmentation for medical
imaging. Our goal is to construct a neural network that
can correctly and rapidly anonymize images with simi-
lar voxel compositions to the scans that the model was
trained on. Convolutional neural networks (CNNs) are
particularly adept at learning complex spatial and inten-
sity patterns in an image, where some of these patterns
might be the curvature and edges that make up facial fea-
tures in a MRI scan [5]. Fully convolutional neural net-
works (FCNs) and U-Net image segmentation networks
are two commonly used networks for semantic segmenta-
tion in medical imaging [5].

FCNs, as implied by their name, have a fully convo-
lutional architecture that allows the network to train and
predict on variable sized images [7]. However, FCNs send
an input image through several alternating convolutional
and pooling layers that rapidly downsample the image’s
resolution prior to prediction in its last layer [7]. The ﬁnal
output of a FCN is typically low resolution and conse-
quently may contain inaccurate boundaries between fea-
tures in a scan. Furthermore, FCNs have primarily only
been used for 2D image segmentation because there are
very few stable implementations of 3D FCNs for medical
image segmentation [4].

On the other hand, there are widely adopted 2D and
3D implementations of U-Net image segmentation net-
works. Similar to a FCN, the U-Net is fully convolutional
and has a contracting path at the beginning of the net-
work that captures contextual information within images
via compact feature maps [4]. However, a symmetric up-
sampling path immediately follows this contraction phase
to retain boundary information that might have been ob-
fuscated at the beginning of the network [4]. This key
difference between the U-Net and FCN gives the former
network an edge with regards to semantically segment-
ing medical images with intricate details like MRI im-
ages. The fact that all of the U-Net’s layers are pooling
or convolutional also makes the network very computa-
tionally efﬁcient due to having far fewer parameters than
other segmentation networks. Consequently, the U-Net’s
superior speed and ability to precisely localize boundaries
in volumetric image data makes it a particularly attractive
tool for masking 3D MRI images.

2.3

2D U-Net

The 2D U-Net is a commonly used image segmentation
network in medical imaging. Consisting of multiple 2D
CNNs, this model typically does not suffer from high
computational or memory costs, because it operates over
planar regions rather than volumetric data [13]. Fur-
thermore, one can use a 2D U-Net to segment images
in 3D space by simultaneously training multiple 2D U-
Nets on orthogonal projections of a 3D image. This dis-
tributes the required computational power for defacing a
3D image across multiple networks and could rapidly im-
prove anonymization speed over classical defacing soft-
ware [12].

3

While a parallelized 2D U-net implementation could
save on speed, memory, and computational costs, there
are caveats with 3D MRI data that make this network
an unsuitable choice for our task. Firstly, MRI data in
particular is very sparse with many background voxels
that obviously are not related to the primary image in the
scan. Thus, a 2D U-Net which cannot use spatial informa-
tion while defacing slices is especially susceptible to sec-
tions with little contextual information. We ﬁnd that these
highly unbalanced slices in a 3D MRI scan are especially
difﬁcult for a 2D U-Net to mask, and consequently this
network performs worse than a 3D U-Net approach with
respect to fully anonymizing an image.

2.4

3D U-Net

Within the taxonomy of U-Net architectures, a 3D U-
Net is essentially identical to a 2D U-Net other than con-
volving over volumetric data as opposed to ﬂat data [4].
However, as previously mentioned, an extra dimensional
axis provides a 3D U-Net with critical spatial information
that helps the network learn more robust boundaries while
segmenting images. In fact, 3D image segmentation has
quantitatively performed better than 2D segmentation for
other medical imaging tasks in relevant literature [16].

However, it is important to note that the lack of suf-
ﬁciently large medical imaging datasets for training 3D
U-Nets is a signiﬁcant disadvantage compared to training
2D U-Nets, which can simply transform a single volumet-
ric image into thousands of 2D input slices. One particu-
lar 3D U-Net implementation by Cicek et al. tackles this
issue by performing on-the-ﬂy data elastic deformations
of 3D data for data augmentation during training [16].
This addition heavily improves the 3D U-Net’s general-
izability, and we borrow several key data augmentation
insights from the author’s implementation for training our
own modiﬁed 3D U-Net for Deepdefacer. Furthermore,
we stick with an underlying 3D U-Net architecture for
MRI anonymization because while the lack of sufﬁcient
image data is an issue for 3D networks, we change our
optimization objective (as detailed in the Proposed Ap-
proach section) to learn 3D binary masks rather than di-
rectly deface images. Consequently, training Deepdefacer
to only predict binary facemasks rather than segment in-
tricate boundaries between facial features in a scan is a far
simpler objective and is less dependent on a large training

dataset.

3 Proposed Approach

Motivated by the success of U-Net image segmentation
networks in medical imaging, we propose Deepdefacer,
a modiﬁed 3D U-Net model that can rapidly anonymize
MRI images. We begin this section by ﬁrst detailing the
baseline 3D U-Net’s architecture, which is optimized to
directly deface input MRI images. Afterwards, we present
Deepdefacer’s own modiﬁed 3D U-Net architecture.

3.1

3D U-Net Baseline

The baseline 3D U-Net segmentation network includes a
downsampling and upsampling stage, along with a soft-
max output in its last layer to directly segment the facial
region from the brain tissue within a MRI image.

Each of the four convolutional blocks in the downsam-
pling stage contains two 3D convolutional layers with 3
x 3 x 3 sized kernels with ReLU activations. These lay-
ers are followed by a 3D max pooling layer that halves
all dimensions with a stride of 2. The max pooling layer
is followed by a batch normalization layer. All blocks in
both the upsampling and downsampling phases use (32,
64, 128, 1024) ﬁlter sizes as in the original 3D U-Net ar-
chitecture [4].

The second stage incorporates four upsampling blocks.
Each block includes a 3D convolutional layer of kernel
size 3 x 3 x 3 followed by a 3D upsampling layer of size 2
in each dimension. The output of the upsampling layer is
concatenated symmetrically with the output of the down-
sampling block that has the same number of ﬁlters [4].

The 3D U-Net’s last layer is a 1 x 1 x 1 convolutional
layer that produces a MRI image of the same size as the
original scan but with a defaced region. This model uses a
softmax cross-entropy loss to measure the error between
a predicted defaced MRI image and the ground truth de-
faced MRI image.

In order to compare the quality of this model’s defaced
output to that from Deepdefacer, we subtract the baseline
model’s predicted defaced MRI from the original MRI
image and then threshold the subtracted volume to pro-
duce a binary segmentation map made up of 0s and 1s.
We then compare this map to the binary masks produced

4

Figure 3: Deepdefacer’s training pipeline. Changing the 3D U-Net’s optimization objective, adding a pre-processing
module, and using fast matrix operations rapidly improves defacing speed.

by Deepdefacer for both quantitative and qualitative eval-
uation.

3.2 Deepdefacer: 3D U-Net for 3D Binary

Mask Prediction

Deepdefacer’s primary objective is to improve MRI
anonymization speed while creating high-quality masks
that rival output - on images of similar voxel composition
- from more specialized programs like Pydeface. Conse-
quently, we use an underlying 3D U-Net as the base archi-
tecture for Deepdefacer, but make the modiﬁcations listed
in this section to create a more optimized network for our
task. Deepdefacer’s full training pipeline is depicted in
Figure 3.

3.2.1 Optimization Objective: Binary Cross Entropy

Loss

We modify the 3D U-Net’s optimization objective to in-
stead reduce the binary cross entropy loss between a pre-
dicted 3D binary mask and a label map, as shown in equa-
tion (1), where N is the number of MRI images in a batch
of data, and yn and 1−yn represent two class values {0,1}
for any predicted voxel in the output mask.

BCE = −

1
N

N
(cid:88)

n=1

(cid:21)
(cid:20)
yn log ˆyn + (1 − yn) log(1 − ˆyn)

(1)

5

Accordingly, we pass the output of the last convolu-
tional layer in a 3D U-Net through a sigmoid activation
function to predict 3D binary masks. A 0 in a 3D binary
mask represents a region to deface whereas 1 represents a
region to maintain. ˆyn and 1 − ˆyn represent the model’s
probability of predicting a 1 or 0, respectively, for a par-
ticular voxel in the output mask, of which we compute a
binary cross-entropy loss by using the logarithmic func-
tions depicted in (1).

3.2.2 Model Architecture: Reduced Filter Sizes and

Layers

We take advantage of our simpler optimization objective
by heavily reducing the ﬁlter sizes in each layer of the
original 3D U-Net by a factor of 4 (8, 16, 32, 64, 128) to
boost computational efﬁciency and prevent Deepdefacer
from producing overﬁtted binary masks. We also remove
the batch normalization layers from the original 3D U-
Net architecture to avoid worsening issues related to high
class imbalance in MRI images: the high number of black
voxels (0 intensity) in MRI images typically dwarfs the
percentage of voxel intensities pertaining to the mask in
the dataset. We ﬁnd that these intensities negatively im-
pact the global mean and deviation calculated in batch
normalization layers and consequently cloud the quality
of this model’s mask output. Making the two aforemen-
tioned changes results in a 92% reduction in the total
number of model parameters in comparison to the original
3D U-Net. As we’ll explore in greater detail in the Anal-
ysis section of this paper, we partly attribute our model’s

fast inference speed to this massive reduction in model
size.

3.2.3 Pre-Processing Stages: Data Augmentation

and Resampling

We also add key pre-processing stages to Deepdefacer’s
training pipeline to improve the quality of its predicted
binary masks. Inspired by elastic deformation techniques
used by Cicek et al. [16] for regularizing 3D segmenta-
tion networks, we add a data augmentation capsule to our
training pipeline that spatially rotates and scales training
images prior to running them through the network. Per-
forming this data augmentation on-the-ﬂy turns a train-
ing set of roughly 2,000 3D MRI images into over 10,000
with unique volumetric orientations, which bolsters Deep-
defacer’s ability to predict masks that generalize to many
different types of MRI images. We also add a resam-
pling layer that reduces an input image’s dimensions to a
much smaller size prior to running it through the network,
and then resamples the map back up to the original image
size after prediction. This portion of our pipeline allows
us to heavily constrain the space complexity that comes
with predicting masks against massive MRI images. This
layer is especially critical for creating a tool that rapidly
anonymize input on commercial CPU devices.

3.2.4 Post-processing Stages: Hadamard Product

and Thresholding

After the mask has been resampled back to the original
image size, we threshold it to discrete values [0,1] to cre-
ate a clean binary map. We speciﬁcally use a threshold
value of 0.5, and determined this quantity with the fol-
lowing protocol: run a log linear search between [0,1] to
retrieve threshold values, process output masks with each
of these quantities, then test these maps against a held-out
validation set to determine an optimal value for threshold-
ing.

Post thresholding, we multiply the mask against the
original
image data to fully deface it. We use the
Hadamard product in equation (2) for this multiplication,
where A is a binary matrix and B is voxel data represent-
ing the original MRI image:

(A◦B)ij = (A)ij(B)ij

(2)

We believe that changing the 3D U-Net’s objective to
predicting a binary map allows Deepdefacer to more eas-
ily generalize to different types of MRI images instead of
overﬁtting on intricate anatomical features within any par-
ticular scan. Furthermore, multiplying the mask against
the original image as a post-processing step allows us to
maintain the quality of all sections within the image that
are not in the facial region. We will see in the Experi-
ments section that directly segmenting a MRI image can
incorrectly perturb voxel data that represents critical in-
formation like brain tissue.

Figure 4: Output mask
from Deepdefacer.
A
(Anterior), S (Superior),
P (Posterior) and I (Infe-
rior) are directional axes
deﬁning the image’s ori-
entation.

Figure 5: Ground truth
from Pydeface.
mask
follow the same
Axes
deﬁnition
under
Figure 4.

listed

4

Implementation Details

4.0.1 Data

Our data consists of 1928 T1-weighted MRI images from
the Brain Development Organization (IXI) and Interna-
tional Consortium for Brain Mapping (ICBM). As Deep-
defacer has four downsampling and upsampling stages, all
T1-weighted images must have dimensions that are divis-
ible by 16. Consequently, we resample the HxW xDxC
resolution sizes of IXI MRI images to 256x256x150x1,
with a single channel C to capture black and white voxel
values. We resample images from ICBM to match a
256x320x256x1 resolution size. We use linear interpo-
lation for both resampling tasks.

We also create a label mask for each image by passing

6

Source Voxel Size [mm]

Field of View Number of Training Samples

IXI
IXI
IXI

0.62, 1.0, 1.0
0.66, 1.0, 1.0
0.8, 0.8, 0.8

ICBM 1.24, 0.98, 0.98
ICBM 1.25, 1.25, 1.20
1.3, 0.98, 0.98
ICBM

256x256x150
256x256x150
256x256x146
256x320x256
174x256x256
220x320x208

140
43
146
242
101
54

Table 1: Sample of images from the training set that vary in voxel size, ﬁeld of view, and image source. The entire
training dataset includes 20 unique protocols.

the input MRI images through Pydeface to construct seg-
mentation maps that will be used as ground truth during
training. Figure. 4 and 5 shows examples of a predicted
mask from Deepdefacer, and a ground truth mask from
Pydeface respectively.

Furthermore, we split our data into training, validation,
and test sets based on the underlying voxel composition
for all MRI images in our dataset. We have 30 unique
voxel sizes in mm space across both IXI and ICBM, and
we use a 80%, 10%, and 10% split for experimentation;
this amounts to 1542 images and 20 voxel spacing pro-
tocols in the training set, 192 images and 5 voxel spacing
protocols in the validation set, and 192 images and 5 voxel
spacing protocols in the test set. Table 1 provides a sample
of different conﬁgurations in the training set. Addition-
ally, we normalize input MRI images to a [0,1] range to
match Deepdefacer’s sigmoid output distribution to pro-
mote training convergence.

4.0.2 Training

We train Deepdefacer with 2 Nvidia K80 GPUs for 4000
training iterations (3 epochs) on 1542 MRI images, which
takes roughly 3 days. We speciﬁcally train with Adam
optimization with a learning rate, β1, and β2 values of
1e−4, 0.9, and 0.999 respectively.

We use a batch size of 1 to maintain computational ef-
ﬁciency while training Deepdefacer on large 3D MRI im-
ages. However, we speed up training by using memory
efﬁcient cudaNN convolution layers within our network.
Furthermore, we initialize all weights with He normal-
ization to stabilize the gradient backpropagation of ReLU
units in each convolutional layer.

5 Experiments and Results

5.1 Quantitative Results

In order to exhaustively evaluate both the quality and
speed of our defacing models, we choose the following
quantitative measures for comparison: speed, Dice, pre-
cision, and recall. All measures were computed over pre-
dictions from a test set of MRI images with different voxel
spacing protocols than the training set. We compute Dice,
precision, and recall scores for Deepdefacer by speciﬁ-
cally comparing its predicted 3D binary masks with cor-
responding respective ground truth 3D masks from Pyde-
face. As previously mentioned, we compare the 3D U-Net
baseline with Deepdefacer by subtracting the baseline’s
segmented MRI outputs from the original MRI scans. We
then threshold the delta map to only discrete values [0,1]
to yield a binary segmentation map for quantitative com-
parison.

5.1.1 Speed

We test Deepdefacer, the 3D U-Net baseline model, and
Pydeface on different CPU and GPU devices to record the
average time taken to predict all images in the test set. We
assess all three models with an array of different devices:
an Intel i7 CPU, 4 vCPUs with 15 GB of memory on a
Google Cloud server, and 1 Nvidia Tesla K80 GPU card.
Figure 6 provides the results for all three of these speed
tests.

From analyzing Figure 6, we see that both U-Net mod-
els are able to deface input MRI images signiﬁcantly
faster than Pydeface. However, Deepdefacer is able to

7

Dice

Precision Recall Number of Model Parameters

Deepdefacer
Base 3D U-Net

0.854
0.413

0.916
0.132

0.805
0.882

1,412,197 (-92%)
19,069,955

Table 2: Deepdefacer VS 3D U-Net Baseline with respect to common binary segmentation metrics and model size.

space complexity in comparison to the 3D U-Net base-
line’s strategy of directly segmenting massive MRI im-
ages. Predicting binary maps rather than directly seg-
menting MRI images also rapidly decreases the compu-
tational complexity of the model.

Along with the reasons stated above, we also attribute
Deepdefacer’s speedup to Pydeface’s own inefﬁciencies
while masking images. For example, Pydeface’s co-
registration pipeline builds an afﬁne transformation ma-
trix based on a template for any image, which can be an
an expensive mathematical procedure depending on the
total number of voxels in a given input image. For exam-
ple, performing an afﬁne transformation over 9,830,400
voxels in a 256x256x150 MRI image is computationally
expensive and the lack of a resampling module is a ma-
jor reason why the software is slower in comparison to
Deepdefacer.
In addition, Deepdefacer’s cudaNN opti-
mized layers also allows the network to a fuller advan-
tage of GPU resources, which only further improves its
prediction speed in comparison to Pydeface.

5.1.2 Dice Coefﬁcient

The most common and if not de-facto measure used for
evaluating binary segmentation tasks is the Dice coefﬁ-
cient. The Dice coefﬁcient measures the size of the over-
lap between a predicted binary segmentation mask X and
the ground truth segmentation output Y as depicted in
equation (3). For MRI anonymization, the score penal-
izes a model for falsely segmenting non-facial region vox-
els (false positives), and for not segmenting facial region
voxels (false negatives). We present the results from this
experiment in Table 2.

Dice =

2|X ∩ Y |
|X| + |Y |

(3)

As expected, the 3D baseline model performs far worse

(a)

Figure 6: Average speed test results across commercial
CPU and GPU devices. The 3D U-Net baseline model
crashes while running on an Intel i7 CPU, and Pydeface
does not signiﬁcantly beneﬁt from running computations
on GPU.

deface images 90% faster than Pydeface on a commercial
Intel CPU card whereas the baseline 3D U-Net model re-
quires at least 4 vCPUs with 15GB total memory to store
computations from masking a single image. Deepdefacer
also outperforms the other two models on vCPUs by up-
wards of 68% and a typical server GPU by 91%.

We attribute Deepdefacer’s massive speedup over the
3D U-Net baseline to a variety of signiﬁcant factors: a
far smaller model size, critical resampling module, and
simpler task of predicting masks rather than directly seg-
menting images. Table 2 also shows that Deepdefacer
has roughly 92% fewer parameters than the 3D U-Net
baseline due to a smaller ﬁlter size per layer ratio, which
not only saves computational resources during training
and inference but also allows Deepdefacer be stored on
compact commercial devices. Furthermore, resampling
images to smaller input resolution sizes saves additional

8

than Deepdefacer because the former model attempts to
directly segment an entire image end-to-end rather than
construct a simple binary mask during training. Figure 8
shows examples of segmentation maps from both Deep-
defacer and the 3D U-Net baseline that reinforce this ob-
servation. The baseline model’s segmentation map in (b)
incorrectly perturbs several voxel intensities in non-facial
regions of the MRI image, which implies a greater num-
ber of false positives and consequently a lower Dice co-
efﬁcient score. On the other hand, Deepdefacer’s map in
(a) does not disturb areas outside of the facial region of
the scan, and consequently has a larger Dice coefﬁcient
score in comparison to the baseline model. This is likely
because Deepdefacer’s objective function is far simpler to
optimize. By training to predict 3D binary masks, Deep-
defacer can ignore misleading patterns from anatomical
features, low or high voxel intensity values, and other fea-
tures of a MRI scan and only focus on masking the general
location of an image’s facial region.

5.1.3 Precision and Recall

We also compare Deepdefacer with the baseline U-Net
model with respect to both precision and recall scores.
Precision allows us to compute a ratio of true positives to
false positives, while Recall allows us to compute a ratio
of true positives to false negatives.

As shown in Table. 2, the baseline model has a very
high recall and low precision score. This implies that
while the baseline model classiﬁes a substantial number
of voxels within a MRI scan as a part of a mask, it is un-
able to maintain the quality of the rest of the image. On
the other hand, Deepdefacer balances a relatively high re-
call score of 0.805 with a high precision score of 0.916,
implying that this model is able to better constrain the
number of defaced voxels to an image’s facial region.
Consequently, Deepdefacer is able to minimize both the
number of false positives and false negatives and maintain
both a high precision and recall score. We attribute Deep-
defacer’s better recall and precision balance from training
on a robust dataset that is rotated, scaled, and augmented
that forces the model to predict more generalizable masks.
Furthermore, having 92% fewer parameters than the base-
line model also reduces its tendency to overﬁt on intri-
cate details within a scan, which makes it a more efﬁcient
model for binary segmentation. The quantitative results

9

(a)

(b)

Figure 7: (a) is a mislabeled output from the Pydeface
program, while (b) is the same output from Deepdefacer.
While Pydeface is unable to generalize well to all types of
MRI images, Deepdefacer is still able to create accurate
masks relying on facial patterns rather than intricate voxel
details within an image.

(a)

(b)

Figure 8: (a) is a segmentation map produced by Deepde-
facer (b) is a segmentation map produced by the 3D U-Net
baseline. Axis in (a) follow the same deﬁnition listed un-
der Figure 4.

from both Figure 6 and Table 2 demonstrate that Deepde-
facer is able to adequately balance masking quality with
prediction speed on images with similar voxel intensities
to those in the training set.

supports

Pydeface

between-modality

5.2 Qualitative Image Comparison
While
co-
registration to deal with different modalities, traditional
defacing software can run into issues with images that
have facial abnormalities. This disadvantage is apparent
in Figure 7, where (a) is an example of an incorrectly
defaced label produced by Pydeface, and (b) is Deep-
defacer’s output on the same exact image. Pydeface

(a)

(b)

(c)

(d)

Figure 9: Both (a) and (c) are images that do not appear in either the IXI or ICBM datasets, and have image resolution
size and spacing compositions that are different from anything that Deepdefacer encounters in the training set. (b) and
(d) are Deepdefacer’s attempts at defacing (a) and (c) respectively. Axes in (a) - (d) follow the same deﬁnition listed
under Figure 4.

fails to mask this image due to computing an incorrect
transformation matrix for translating its facemask onto
the image, likely due to misinterpreting the overextended
facial region in the scan. On the other hand, Deepdefacer
produces a far superior defacing, which points to its
ability to generalize to images with abnormalities that
may not have been highly represented in the training
set. This underscores how deep learning methods have a
signiﬁcant advantage over more traditional algorithms for
anonymizing MRI images.

One might point out that Deepdefacer’s segmentation
map in Figure 8a and defaced outputs in Figure 9 are
amorphous rather than rigid like Pydeface’s mask in Fig-
ure 5. This is likely due to Deepdefacer’s smaller ﬁlter
sizes throughout its network, which allows the model to
better generalize to other types of MRI images. Further-
more, training on segmentation maps also forces Deepde-
facer to avoid overﬁtting on features in a MRI image that
are not in its facial region. Rigid or amorphous structure,
however, is less important for determining the quality of
an anonymized MRI image; the primary goal for this task
is to just eliminate most of the facial region of a scan with-
out perturbing voxels representing brain tissue. Thus, the
masks’s shape is irrelevant as long as the aforementioned
objective is accomplished.

6 Conclusion

This paper presents Deepdefacer, an inaugural applica-
tion of deep learning to MRI anonymization. We demon-
strate its superiority over a 3D U-Net baseline model and
the classical defacing software Pydeface, along with dis-
cussing the several beneﬁts that deep learning has for
solving problems in the anonymization space.

We see that Deepdefacer masks images up to 91%
faster on average than Pydeface and the baseline U-Net
model on an array of CPU and GPU devices. The model
is also able to deface images with a far smaller memory
footprint of 92% fewer parameters than the existing 3D
U-Net architecture. Both of these advantages can poten-
tially allow users to deface large MRI images on their own
personal workstations, as opposed to relying on expensive
cloud hardware or complicated defacing software.

In conclusion, we hope this paper encourages other re-
searchers to explore the many beneﬁts of employing deep
learning techniques in the MRI anonymization space.
There are many possible extensions to this work includ-
ing improving the performance of Deepdefacer on im-
ages of differing voxel intensities, delving into the inter-
pretability of the model by analyzing correlation between
the model weights and defacing predictions, and creating
a web UI with a deepdefacer backend to let users deface
images without requiring any deep learning or program-
ming knowledge. We encourage interested readers to visit
the github page at the end of this paper if they want to use
or extend Deepdefacer’s capabilities.

10

7 Future Work

As discussed in this paper, existing defacing software is
unnecessarily complex and requires installing several de-
pendencies prior to usage. Furthermore, this software is
computationally expensive which bars researchers who do
not have access to expensive cloud infrastructure or per-
sonal machines.

We have released an experimental version of the model
discussed in this paper via a PyPi program to hopefully
address some of these obstacles, and also encourage other
researchers interested in this work to iterate on its perfor-
mance [8]. We also plan to integrate the model within
a web application for an easy-to-use and efﬁcient way
to anonymize private MRI data. In order to accomplish
this, we plan to package Deepdefacer within a tensor-
ﬂow.js module due to its ability to allow in-browser in-
ferences, so we can ensure that the researcher’s data is
private and not transmitted over the wire. Furthermore,
we plan on allowing the user to opt-in and provide feed-
back on the accuracy of our model. With this feedback
data, we can further improve the model’s accuracy far be-
yond the capabilities of Pydeface (the ground truth model
mentioned in this paper). Lastly, we hope to run Deepde-
facer over publicly hosted MRI data sets and provide our
own anonymized MRI dataset that can be used by other
researchers for their own tasks.

8 Contributions & Acknowledge-

ment

We would like to thank the Poldrack Lab at the Stanford
Center for Reproducible Neuroscience for providing our
team with T1-Weighted MRI images for experimentation
and advising us while working on this project.

References

[1] Amir Alansary. Ixi dataset: Brainwwor development
organization. Imperial College London, 2016.

[2] Amanda Bischoff-Grethe, I Burak Ozyurt, Evelina
Busa, Brian T Quinn, Christine Fennema-Notestine,
Camellia P Clark, Shaunna Morris, Mark W Bondi,

Terry L Jernigan, Anders M Dale, et al. A technique
for the deidentiﬁcation of structural brain mr im-
ages. Human brain mapping, 28(9):892–903, 2007.

[3] HHS Centers for Medicare & Medicaid Services
et al. Hipaa administrative simpliﬁcation: standard
unique health identiﬁer for health care providers. ﬁ-
nal rule. Federal register, 69(15):3433, 2004.

[4]

¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S
Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d
u-net: learning dense volumetric segmentation from
In International Conference on
sparse annotation.
Medical Image Computing and Computer-Assisted
Intervention, pages 424–432. Springer, 2016.

[5] Claus Lenz Felix Altenberger. A non-technical sur-
vey on deep convolutional neural network architec-
tures. arXiv preprint arXiv:1803.02129, 2018.

[6] Krzysztof J. Gorgolewski. Pydeface: Defacing util-

ity for mri images. In Github, 2016.

[7] Trevor Darrell Jonathan Long, Evan Shelhamer.
Fully convolutional networks for semantic segmen-
tation. In 2015 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR. University of
California, Berkeley, 2016.

[8] Anish Khazane and Julien Hoachuck. DeepDefacer:
Automatic Removal of Facial Features via Deep
Learning. Pypi program. https://pypi.org/
project/deepdefacer/, 2019.

[9] Vitaly Napadow, Rupali Dhond, David Kennedy,
Kathleen K S Hui, and Nikos Marias. Automated
brainstem co-registration (abc) for mri. Neuroimage,
pages 1113–1119, 2006.

[10] John Mazziotta MD PhD. International consortium
for brain mapping (icbm). National Institute of
Biomedical Imaging and Bioengineering, University
of Southern California, 2010.

[11] Russell A. Poldrack. Deidentiﬁcation of mri data.
Stanford University, Department of Psychology,
2016.

11

[12] Adhish Prasoon, Kersten Petersen, Christian Igel,
Franc¸ois Lauze, Erik Dam, and Mads Nielsen. Deep
feature learning for knee cartilage segmentation us-
ing a triplanar convolutional neural network. In In-
ternational conference on medical image computing
and computer-assisted intervention, pages 246–253.
Springer, 2013.

[13] Olaf Ronneberger, Philipp Fischer, and Thomas
Brox. U-net: Convolutional networks for biomed-
In International Confer-
ical image segmentation.
ence on Medical image computing and computer-
assisted intervention, pages 234–241. Springer,
2015.

[14] Nakeisha Schimke, Mary Kuehler, and John Hale.
Preserving privacy in structural neuroimages.
In
IFIP Annual Conference on Data and Applications
Security and Privacy, pages 301–308. Springer,
2011.

[15] Florent S´egonne, Anders M Dale, Evelina Busa,
Maureen Glessner, David Salat, Horst K Hahn, and
Bruce Fischl. A hybrid approach to the skull strip-
ping problem in mri. Neuroimage, 22(3):1060–
1075, 2004.

[16] Kazuma Yamada Xiangrong Zhou. Performance
evaluation of 2d and 3d deep learning approaches
for automatic segmentation of multiple organs on ct
images. In Medical Imaging 2018: Computer-Aided
Diagnosis, pages 415–423. SPIE, 2018.

12

