2
2
0
2

t
c
O
0
2

]
E
M

.
t
a
t
s
[

3
v
9
3
4
2
0
.
9
0
2
2
:
v
i
X
r
a

Some models are useful, but how do we know which ones?
Towards a uniﬁed Bayesian model taxonomy

Paul-Christian B¨urkner1∗

Maximilian Scholz1

Stefan T. Radev2

1 Cluster of Excellence SimTech, University of Stuttgart, Germany
2 Cluster of Excellence STRUCTURES, University of Heidelberg, Germany
∗ Corresponding author, Email: paul.buerkner@gmail.com

Abstract

Probabilistic (Bayesian) modeling has experienced a surge of applications in almost all quantitative
sciences and industrial areas. This development is driven by a combination of several factors,
in-
cluding better probabilistic estimation algorithms, ﬂexible software, increased computing power, and
a growing awareness of the beneﬁts of probabilistic learning. However, a principled Bayesian model
building workﬂow is far from complete and many challenges remain. To aid future research and
applications of a principled Bayesian workﬂow, we ask and provide answers for what we perceive as
two fundamental questions of Bayesian modeling, namely (a) “What actually is a Bayesian model?”
and (b) “What makes a good Bayesian model?”. As an answer to the ﬁrst question, we propose
the PAD model taxonomy that deﬁnes four basic kinds of Bayesian models, each representing some
combination of the assumed joint distribution of all (known or unknown) variables (P), a posterior
approximator (A), and training data (D). As an answer to the second question, we propose ten
utility dimensions according to which we can evaluate Bayesian models holistically, namely, (1) causal
consistency, (2) parameter recoverability, (3) predictive performance, (4) fairness, (5) structural
faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9) estimation speed, and (10)
robustness. Further, we propose two example utility decision trees that describe hierarchies and
trade-oﬀs between utilities depending on the inferential goals that drive model building and testing.

Keywords: Probabilistic modeling, statistical learning, Bayesian statistics, machine learning, model
comparison

Contents

1 Introduction

2 What is a Bayesian Model?

2.1 P Models
2.2 PD Models
2.3 PA Models
2.4 PAD Models
2.5

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Intermediate summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 What makes a good Bayesian model?

3.1.1
3.1.2

3.1 Causal Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Structural Causal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Parameter Recoverability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1
Identiﬁability and Information Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 Ground-Truth Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3 Calibration of Posterior Approximations . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 Absolute and Relative Predictive Performance
. . . . . . . . . . . . . . . . . . . . . .
3.3.2 Prior and Posterior Predictive Performance . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Predictive Performance

3

3
4
6
6
10
10

10
10
11
12
13
14
16
18
19
20
21

1

 
 
 
 
 
 
22
23
24
24
25
26
27
27
28
29
29
33
33
34
35
36
36
37
38
38
39
40
41
42
43

44
45
45
45
46
47
47
48
48

49

In-Sample and Out-of-Sample Predictive Performance . . . . . . . . . . . . . . . . . .
3.3.3
3.3.4 Predictions in a Dynamic World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 Measurement Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.2 Predictive Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 Structural Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.1 Variable Scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.2 Probabilistic Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.3 Physical Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Parsimony . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6.1 P-Parsimony . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6.2 A-Parsimony . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interpretability of PI models
3.7.1
Interpretability of PE models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7.2
3.8 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.8.1 Convergence of Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . .
3.8.2 Convergence of Optimization-Based Algorithms . . . . . . . . . . . . . . . . . . . . . .
3.8.3 Convergence of Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . .
3.8.4 Convergence of Approximate Bayesian Computation . . . . . . . . . . . . . . . . . . .
3.8.5 Convergence of Amortized Approximators . . . . . . . . . . . . . . . . . . . . . . . . .
3.9 Estimation Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.9.1
Sampling Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.9.2 Estimation Speed of Amortized Approximators . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.10 Robustness

3.7

4 Utility Hierarchies and Trade-oﬀs

4.1 Utility Tree for Observable Inferential Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 Primary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.2
Secondary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.3 Tertiary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Utility Tree for Latent Inferential Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 Primary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2
Secondary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.3 Tertiary Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Conclusion

2

1.

Introduction

Probabilistic (Bayesian) modeling has seen a surge of applications in almost all quantitative sciences and
industrial areas [100, 180, 103, 56, 156, 136]. This development is driven by a combination of several factors,
including powerful probabilistic estimation algorithms [127, 21, 115, 207, 221], eﬃcient post-processing [267,
118], ﬂexible open-source software [252, 70, 36] and increased information processing capacity. Furthermore,
these factors are coupled with a growing awareness of the beneﬁts of probabilistic modeling, such as inclusion
of prior knowledge [197, 183], regularization [98, 37, 23, 216] as well as uncertainty quantiﬁcation and
propagation [130, 180, 103]. Despite these advances, creating and improving Bayesian models in the context
of a principled Bayesian workﬂow [241, 103] remains a complicated endeavor that requires expertise in various
domains, including subject matter knowledge about the system and the data it generates, statistical learning
expertise, programming, as well as understanding of numerical approximation and simulation methods [156,
103]. Thus, to aid future research on and applications of a principled Bayesian workﬂow, we ask and provide
answers to what we hold as two fundamental questions:

1. What actually is a Bayesian model?

2. What makes a good Bayesian model?

In current practice, the term Bayesian model is highly overloaded and used to describe a wide range
of objects with potentially very diﬀerent properties. Moreover, modern Bayesian models are more than
just a likelihood and a prior – rather, they resemble complex simulation programs coupled with black-box
approximators, interacting with various data structures and context variables, embedded within iterative
workﬂows with multiple feedback loops [177, 71, 156, 103, 241]. Thus, we aim to disambiguate and structure
the diﬀerent meanings of a Bayesian model by proposing the PAD model taxonomy (see Section 2). Our
taxonomy aims to accommodate modern uses of Bayesian models and provides an answer to Question 1.
With a clear deﬁnition of Bayesian models in hand, we describe a collection of ten utility dimensions that can
be used to quantify the goodness of Bayesian models in a holistic manner (see Section 3), thus providing an
answer for Question 2. We then continue with a discussion of importance hierarchies and common trade-oﬀs
between utilities in Section 4 and end with a conclusion in Section 5.

This paper started out as an attempt to organize our own thoughts and provide a unifying and consistent
language of Bayesian model building. To a certain extent, it is inevitably opinionated. Nevertheless, we
aim to be comprehensive in the utility dimensions we discuss, such that all the goals we can sensibly ask a
Bayesian model to achieve have their place in this paper. In contrast, due to the large number of diﬀerent
topics we touch in the process, the amount of details and cited literature per topic are necessarily non-
exhaustive. The cited literature is only meant as a starting point for the interested reader to dive in deeper
if they wish. In terms of target audience, we hope that this paper will be helpful for both methodological
researchers developing Bayesian models and users applying them in practice.

2. What is a Bayesian Model?

As the term Bayesian model (or just model for that matter) can sustain multiple meanings depending on
context, it can prove incredibly diﬃcult to talk about models with suﬃcient clarity. As we will see later,
diﬀerent kinds of models may have totally diﬀerent kinds of properties which need to be considered and
prioritized by an analyst. Without clearly communicating the essential kind of model one has in mind,
a discussion about its properties makes little sense.
In this section, we attempt to resolve this issue by
proposing the PAD taxonomy for Bayesian models (see Figure 1 for an overview; see also Table 1 for a quick
reference of key concepts and corresponding notation). We will deﬁne four basic model classes and explain
how they relate to each other. While the PAD-taxonomy might be applicable and useful in other contexts,
we will speciﬁcally expand on it from a Bayesian perspective.

3

Table 1: Table of important symbols and their corresponding description.

Notation (Symbol) Meaning (Description)

P, A, D
θ, y, ˜y
z, ξ
ϕ, ψ
p(θ)
p(y | θ)
p(θ, y)
p(θ | y)
pA(θ | y)
G(·), p∗(y)
Ep[·]
T, H

Joint distribution, approximator, training data
Latent parameters, unrealized observables, realized observables
Random state, random noise (nuisance or exogenous variables)
Quantity of interest, its model-based estimator (function of θ)
Prior distribution of parameters
Likelihood function (explicit or implicit/simulation-based)
Joint distribution of parameters and observables
Posterior distribution of parameters given observables
Approximate representation of posterior by approximator A
True data generator, true data-generating distribution
Expected value of a quantity with respect to density p
Summary statistics of posterior, summary statistics of data

2.1. P Models

We deﬁne P models by a joint probability distribution p(y, θ) over all quantities of interest whose potential
variation or uncertainty we choose to express in terms of probability theory. We assume that y represents all
observable quantities (i.e., data, observations, or measurements) and θ represents all unobservable quantities
(i.e., parameters, latent states, or system variables) within a particular modeling context. In most cases, the
joint distribution factorizes into a likelihood p(y | θ) and a prior p(θ):

p(y, θ) = p(y | θ) p(θ)

(1)

Not all P models are created equal, but most are built to represent a real-world process or a system for which
we tacitly assume an opaque (true) data generator G to exist. Having some properties that are of interest
to the analyst, the generator induces an unknown (true) data distribution p∗(y), typically available only
through ﬁnite observations ˜y (i.e., real-world data). Accordingly, P models strive to encode probabilistic
information about the true distribution p∗(y) and/or structural information about the true generator G.
The former means that our model matches the statistical properties of p∗ either a priori, p(y) ≈ p∗(y), or a
posteriori p(y | ˜y) ≈ p∗(y), where p(y) and p(y | ˜y) are the prior and posterior predictive distributions of P,
respectively. The latter means that our parameters θ correspond to some relevant (hidden) properties ϕ of
G, for which we endeavor to learn something by analyzing ˜y. We will expand on these goals in more detail
in Section 4.

P models are typically generative, that is, we can obtain pseudo-random parameter and data draws via
Monte Carlo simulations from Equation (1). The generative property presupposes that the prior is proper
(i.e., its density function has a ﬁnite integral) and that eﬃcient algorithms for sampling random draws from
both p(θ) and p(y | θ) exist. Due to their generative properties, P models can be particularly useful for
various forward inference tasks, such as simulating the behavior of complex systems [153] or testing diﬀerent
prior assumptions [20]. From a generative perspective, we can further draw a distinction between explicit
likelihood P models and implicit likelihood P models.

Explicit Likelihood Models (PE). PE models are characterized by a tractable likelihood function.
This means that the likelihood p(y | θ) is known analytically and can be evaluated directly or approximated
numerically for any pair (y, θ). PE models include popular statistical models, such as (generalized) linear and
additive models [122], but also (stochastic) diﬀerential equation systems with simple statistical properties
[140], (ﬁnite) mixture models [49], or feedforward neural networks [113].

4

Figure 1: The PAD Bayesian model taxonomy deﬁnes four basic kinds of Bayesian models. Each model kind
represents some combination of the joint distribution of all random quantities (P), a posterior approximator
(A), and observed data (D).

Implicit Likelihood Models (PI ). PI models are deﬁned by a Monte Carlo simulation program y =
g(θ, z) and a prior p(θ), rather than directly through a likelihood function p(y | θ). The simulator g
transforms its inputs θ into outputs y through a series of latent program states z. A Monte Carlo simulator
only implicitly deﬁnes the likelihood density via the relation

(cid:90)

p(y | θ) =

p(y, z | θ) dz,

(2)

where p(y, z | θ) is the joint distribution of observables y and random latent program states z, if such a
distribution exists. The above integral runs over all possible execution paths of the simulation program
for a given input θ and is typically intractable, that is, we cannot explicitly write down the mathematical
form of the implied likelihood p(y | θ). PI models are usually built upon ﬁrm theoretical assumptions and
computational considerations aimed at providing a faithful representation of the modeled real-world system
or process. Common PI models include mechanistic neural models [135], particle physics simulators [59],
population genetics algorithmic models [125], or agent-based models [114], to name just a few.

The distinction between PE and PI models is not a conceptual necessity, but rather an emerging practical
convenience. While most standard statistical models can easily be speciﬁed in terms of known density or
distribution functions, the behavior of complex computational models might be easier to emulate directly by
means of a simulation program. Importantly, PE and PI models necessitate the use of diﬀerent estimation
methods and thus disparate modes of approximation and inference, as we will see in the following sections.

5

2.2. PD Models

PD models are deﬁned as the combination of a P model and observed data ˜y, that is, they represent a tuple
( p(y, θ), ˜y ). The data can comprise any number of measurements ˜y with an arbitrary structure (e.g., sets,
time-series, graphs, etc.). Furthermore, the number of observed data sets (conditioning quantities for the
posterior) will be determined by the structure of the P model: Data on a hundred countries represents a
single data set from the lens of a multilevel (hierarchical) model, but it comprises a hundred data sets for a
single-level (non-hierarchical) P model.

The goal of PD models is to integrate the joint distribution and the observed data in order to arrive at

the corresponding analytic posterior :

p(θ | ˜y) =

p(˜y | θ) p(θ)
p(˜y)

∝ p(˜y | θ) p(θ),

(3)

where the denominator p(˜y) = (cid:82) p(˜y | θ) p(θ) dθ represents the marginal likelihood evaluated at ˜y and
typically treated as a normalizing constant due to its independence of the model parameters.

If the P model is generative, the analytic posterior exists for every ˜y that satisﬁes the expected data
structure of the P model, regardless of whether or not it represents the true real-world generator G. In the
non-representative case, the P model is said to be misspeciﬁed. In most quantitative sciences, except perhaps
in some areas of the natural sciences, we can expect all P models to be misspeciﬁed to some (non-negligible)
degree. This does not prevent the corresponding PD models from being useful, though, if they can at least
express some relevant aspects of reality captured by ˜y.

PD models represent the ideal endpoint of Bayesian inference. However, because we can rarely compute
the marginal likelihood p(˜y) analytically, we do not have access to the actual PD model outside of textbook
examples with limited generality and applicability (i.e., for conjugate P models, [104]). In other words, for
most practically relevant and non-trivial PE and PI models, we cannot retrieve the analytic posterior p(θ | ˜y)
and can only work with an approximate representation through the lens of an intermediary A which we call
a posterior approximator.

2.3. PA Models

PA models are deﬁned as the combination of a P model with a posterior approximator A, that is, they consti-
tute a tuple ( p(y, θ), pA(θ | y) ), where the latter denotes any algorithm capable of somehow approximating
the analytic posteriors of model-implied observations y for a given P model. Approximators themselves exist
at both an algorithmic and an implementation level, and details on both levels can inﬂuence their behavior
and performance. In the absence of actually observed data ˜y, PA models can be useful for conﬁrming the
computational faithfulness of a workﬂow, for instance, via simulation-based-calibration [SBC, 257] or assess-
ing the adequacy of a model for answering a particular research goal [240]. Importantly, the type of P model
(i.e., PE or PI ) will typically determine or necessitate the choice of a particular approximator A, as we will
see shortly.

What is an approximator? More precisely, we can deﬁne an approximator as a triple A = {A, I, H},
where A denotes the algorithmic representation (formal computer program), I denotes the actual imple-
mentation in a concrete programming language, and H denotes the set of admissible hyperparameters (i.e.,
adjustable settings or inputs) of the approximator. The ﬁrst two components of A are often entangled when
talking about approximators in general, but they require diﬀerent levels of analysis. For instance, we can
determine the computational complexity of A via standard algorithmic analysis and classify approximators
according to their asymptotic run time or memory requirements [54]. However, the latter two will also be
constrained by the particular implementation I: Parallel computing can easily turn a scary-looking quadratic
O(n2) time complexity into a negligible constant run time in practice [54]. Thus, we deem it important to
keep the distinction between A and I explicit1. In addition, the performance of an approximator will heavily

1Naturally, hardware speciﬁcations will further inﬂuence the actual run time and space requirements of any approximator,
so these speciﬁcations should be taken into account when comparing diﬀerent approximators. The utility of an approximator

6

depend on the choice of particular hyperparameters h ∈ H and these should be explicitly speciﬁed in any
PA model.

Approximators for PE models. Currently, the two most commonly used approximators for PE models
are Markov chain Monte Carlo (MCMC) samplers and variational inference (VI) methods, but there exist
many more approximator classes, for example, integrated nested Laplace approximation (INLA: [235, 160])
or optimal transport applied to Bayesian inference [72, 147, 209].

MCMC sampling algorithms, such as the Metropolis-Hastings algorithm [123], Gibbs sampling [95],
Hamiltonian Monte Carlo [191] or its extension to the No-U-Turn (NUTS) sampler [127] belong to a family
of stateful algorithms which generate a sequence of correlated draws that converge in distribution to a
stationary target distribution [100]. Generally, our goal in MCMC is to construct a (geometrically) ergodic
Markov chain on θ whose stationary distribution is the posterior p(θ | y) [100]. Then, we sample from the
chain to obtain random draws from the stationary distribution and use these draws to approximate p(θ | y).
More precisely, using the posterior draws, we can eﬃciently approximate expectations (e.g., mean or variance)
and quantiles of the posterior marginals, but not the posterior density itself (see also Section 3.2.2).

The idea of approximating a complicated distribution via dependent random draws, albeit rather straight-
forward in hindsight, has gradually transformed and shaped the ﬁeld of Bayesian inference. Moreover, it
constitutes the main logic behind major probabilistic programming languages such as JAGS [217] or Stan
[46]. A sampler is thus a program which uses computer-generated randomness to generate draws from a
(complicated) distribution, instead of deriving or estimating its algebraic form.

Diﬀerently, variational inference (VI) methods cast the problem of posterior inference as an optimization
task. In contrast to MCMC, the resulting posterior approximation pA(θ | y) is in the form of a tractable
density instead of random samples from the posterior. Our goal in VI is to specify a family of approximate
densities Q over the parameters θ of P. Then, we try to retrieve the density q∗ ∈ Q which minimizes the
Kullback-Leibler (KL) divergence to the analytic posterior. Finally, we use q∗(θ) as our approximation
pA(θ | y) to the analytic posterior.

MCMC and VI methods represent the two endpoints of a trade-oﬀ between theoretical guarantees and
computational eﬃciency. MCMC methods enjoy the guarantee that, under certain regularity conditions
[100], the obtained draws represent the true parameter posterior p(θ | y). More precisely, the posterior
expectations can be perfectly recovered if the MCMC chain is run inﬁnitely long and, more practically
important, expectations can be eﬃciently approximated already with a ﬁnite number of draws. Despite their
favorable theoretical properties and major advances in recent years, MCMC algorithms are notoriously slow,
which renders estimation of some complex models or applications to really big data practically infeasible [25].
On the other hand, VI methods can be very fast and oﬀer a viable alternative to MCMC in applications to
large data sets or real-time inference. However, VI approximators can suﬀer severe loss of posterior accuracy
and, as of today, oﬀer less guarantees for correct inference than MCMC methods [25, but see [291, 290]].
Thus, the choice between an MCMC or a VI approximator for a particular PA model will largely depend on
the modeling context. In addition, highly complex PE models might not be estimable with either MCMC
or VI, in which case they might be treated as PI models and tackled via simulation-based approximators,
as we discuss next.

Approximators for PI models. Standard MCMC and VI solutions are not applicable to statistical
inference with PI models, since the latter lack an analytic likelihood function p(y | θ). Accordingly, approx-
imators for PI models leverage Monte Carlo (i.e., randomized) simulations for estimating the posterior on
the basis of the implicit likelihood deﬁned by the simulator and Equation (2).

Approximate Bayesian computation (ABC) comprises a broad family of asymptotically correct methods
for performing inference with PI models. The core idea of ABC methods is to approximate the posterior by
repeatedly drawing parameters from the prior and then running the simulator with the sampled parameters
to obtain a synthetic data set. Whenever a synthetic data set is suﬃciently similar to an actually observed

will also be constrained by the available hardware budget: parallelism is of little use without access to a computing cluster.

7

Figure 2: Amortized approximators incorporate a simulation-based approximation loop before any real data
are collected. The inference phase involves no simulations or further optimization and could be carried out
almost instantly. The upfront training eﬀort therefore amortizes over arbitrary many observed data sets
from a research domain working on the same P model family.

data set (as deﬁned by a ﬁxed similarity criterion or a distance metric), the corresponding parameters are
retained as a draw from the target posterior, otherwise rejected (i.e., rejection sampling).

In practice, ABC methods are notoriously ineﬃcient and hindered by various methodological “curses”,
such as the curse of dimensionality [228] or the curse of insuﬃciency [171]. A number of more eﬃcient
methods employ various techniques, such as sequential Monte Carlo (SMC), [247, 150] or ABC-MCMC [172]
with kernel density estimation (KDE) [261] to optimize sampling or correct potential deﬁciencies, but the
core idea of using simulations to aid real-world inference remains invariant across methods.

Recently, machine learning and deep learning innovations have permeated the ﬁeld of simulation-based
inference with the goal of scaling up or replacing standard ABC methods altogether [56]. Most of these
innovations require simulation-based training of an expressive machine learning algorithm (e.g., random
forests or neural networks) which is then used as a standalone approximator [48, 115, 112, 220], in combination
with an ABC routine [139] or an MCMC sampler [124, 81, 166, 29].

For instance, neural density estimation (NDE) methods employ specialized neural networks capable of
performing fully Bayesian inference (i.e., returning full posteriors). Moreover, these methods can approxi-
mate diﬀerent components of intractable PI models and currently represent a ﬁeld of active and promising
development [56, 156]. As an example, sequential neural posterior estimation (SNPE) methods use gen-
erative neural networks (e.g., mixture density networks, autoregressive or normalizing ﬂows) to iteratively
narrow down the prior and eventually generate parameter draws representing the actual posterior induced
by a particular data set [204, 164, 115].

Diﬀerently, amortized approximators such as the BayesFlow method [220, 242] or single-round SNPE
[115, 112] train a global neural sampler acting as a functional which can approximate the posterior across
the entire generative scope of a model (see below for more on the amortized vs. non-amortized distinction).
A shared feature between these neural methods is that they avoid MCMC sampling altogether and are able
to generate random draws from the correct posterior under certain optimal conditions.

An alternative approach that does not rely on neural networks for parameter inference is the pre-paid
estimation method [182]. This method memorizes a large database of pre-computed summary statistics for
eﬃcient nearest-neighbor inference, aided by advanced interpolation methods.

Ultimately, the utility of any simulation-based method will depend on a combination of various factors,

8

Figure 3: Non-amortized approximators perform a separate approximation loop (dashed plate) for each
observed data set from a given research domain. Likelihood-based approximations, such as MCMC will
evaluate the likelihood, whereas simulation-based approximators, such as ABC rejection samplers, will only
use random draws from the implicit likelihood (available through stochastic simulations). Approximation
and inference are tightly intertwined and the observed data enters the approximation loop.

such as generality, domain expertise, theoretical guarantees, eﬃciency, scalability, and software availability.
The amount of available data will once again play a crucial role for the choice of approximator.
In this
context, the distinction between amortized and non-amortized posterior approximators becomes crucial.

Amortized vs. Non-Amortized Approximators. Arguably, there are numerous ways to devise a
taxonomy for the ever-growing zoo of posterior approximators. A particularly useful and clear-cut classi-
ﬁcation views approximators as either amortized or non-amortized, with diﬀerent degrees of amortization
possible. Amortized approximators involve a costly simulation-based optimization (training) phase which
renders subsequent inference on simulated y or real data ˜y extremely eﬃcient (see Figure 2). In other words,
the optimization/training eﬀort amortizes over repeated inference queries (i.e., over multiple data sets).
Diﬀerently, non-amortized approximators repeat all necessary computations for each data set from scratch
and utilize hardly any pooling of computational resources (see Figure 3).

Examples of amortized approximators include the BayesFlow method [220, 222, 222], the APT method
operating in a non-sequential regime [115, 112], or the pre-paid estimation method [182]. Examples of
non-amortized approximators include standard explicit inference algorithms, such as MCMC or VI, but
also several common ABC methods, such as ABC-SMC [247, 150] or ABC-MCMC [172, 261]. In addition,
some PA models might include both amortized and non-amortized components, such as multi-round SNPE
methods (which involve a separate training phase for each data set, [204]), likelihood approximators or sur-
rogates (which involve MCMC sampling, [206]), or inference compilation methods (which involve sequential
Monte-Carlo, [202]).

Amortized approximators are typically employed to estimate implicit PA(D) models, but are equally
applicable to explicit PA(D) models. In the former case, their involvement often arises out of necessity, since
PI models are analytically intractable and state-of-the-art approximators, such as HMC-MCMC, are not
applicable out of the box. In the latter case, amortized approximators might be the only resort to estimate
multiple PAD models in the presence of multiple data sets, where non-amortized approximators, despite
being feasible, would demand an inordinate amount of a researcher’s lifetime [273].

9

2.4. PAD Models

PAD models are deﬁned as the combination of a P model, a posterior approximator A, and observed data
D, that is, they constitute a triple ( p(y, θ), pA(θ | ˜y), ˜y ). Ultimately, PAD models aim to approximate the
corresponding PD model by means of a suitable approximator A, whereas the amount of data D, together
with the type of P model, will largely determine the choice of approximator. As a consequence, the properties
of a particular PAD model may be very diﬀerent than what is expected from studying the corresponding PA
model, since the observed data ˜y may not have been be generated from P itself. This misspeciﬁed P model
case, can arise for both PE and PI models and can have diﬀerent consequences for the validity of inference
depending on the particular approximator A [175, 24, 86, 85].

For instance, amortized approximators face the challenge of dealing with simulation gaps [242, 201]. Sim-
ulation gaps occur when P model simulations do not accurately represent the real behavior of the modeled
system or when they cannot adequately account for unexpected contamination of the observed data. Simu-
lation gaps are especially critical for amortized approximators, since the latter assume that simulations are
faithful proxies of reality. Thus, simulations from misspeciﬁed P models may lead to subsequent problems
for amortized inference on real data [242]. In these cases, the resulting pA(θ | ˜y) will not be representative
of the analytic p(θ | ˜y) and any substantive conclusions based on the former will have little validity.

In contrast, principle limitations due to model misspeciﬁcation do not exist for standard, non-amortized
Bayesian approximators, such as MCMC. Under certain regularity conditions, MCMC samplers guarantee
that the obtained samples represent the analytic posterior p(θ | ˜y) even when the underlying P model is
misspeciﬁed [100]. However, misspeciﬁed models might still cause considerable diﬃculties and convergence
problems for MCMC methods in practice. Thus, any trustworthy approximator should be equipped with
diagnostics signaling improper convergence or invalid inference queries (see Section 3.8).

2.5.

Intermediate summary

Thus far, with our PAD-taxonomy, we have deﬁned four diﬀerent classes of Bayesian models comprising
diﬀerent, yet interdependent, conceptual elements. Common to all was the joint probabilistic model (P),
which represents the core probabilistic and structural assumptions of a Bayesian model. In addition, we
proposed to treat the posterior approximator (A) and the data (D) as further constituents of Bayesian
models. We consider this warranted, since all three elements not only determine the scope and validity of
the substantial conclusions derived from model-based inference, but also inﬂuence which assumptions we
decide to (and could!) test and which we choose to keep untouched by reality.

3. What makes a good Bayesian model?

Below, we present a total of ten utility dimensions that, from our perspective, capture most relevant aspects
of Bayesian models as deﬁned by our taxonomy. For each of these dimensions we explain (a) its deﬁnition
and meaning, (b) the reason why we deem it relevant for Bayesian model building, and (c) how to practically
measure it. The order in which we present each utility dimension does not indicate their importance, but
aims to ease their presentation. We discuss the relative importance of utility dimensions in Section 4.

3.1. Causal Consistency

A common goal of scientiﬁc models is the investigation of a causal claim, such as the improvement a certain
treatment might bring to some medical condition or the eﬀect an intervention has on an outcome of interest.
Most people are aware of the widely recited folk wisdom that correlation does not imply causation. Yet,
this adage bears the seeds of a far-reaching and nowadays generally acknowledged opinion that statistics
alone simply cannot solve questions of causality [212]. While statistical inference can handle the static
nature of associations in observational data, causality is a matter of changing conditions and handling these
changing conditions requires causal assumptions to build upon [211]. Moreover, diﬀerent P models may
claim diﬀerent degrees of causal sophistication. For example, some PE models built only for the purpose

10

Figure 4: An example structural causal model (SCM) with three variables. The left panel depicts the pre-
intervention path diagram, whereas the right panel depicts the post-intervention path diagram (see text for
further clariﬁcation).

of making accurate predictions may pass without a single mention of causality, while some mechanistic PI
models may directly embody causal functional relationships, such that a model parameter x is assumed
to cause an observable y by construction or by derivation from scientiﬁc theory. Some complex P models
may even hold standard unidirectional notions of causality inadequate, as the dynamics of certain natural
systems appear to necessitate bidirectional or hierarchical forms of causal interplay [260, 193]. The scientiﬁc
methods developed around causality help us determine whether a P model is a valid recipe for answering a
particular causal query in principle. Put diﬀerently, we ask whether the probabilistic structure of a P model
is consistent with a set of external causal assumptions. Thus, we refer to this implied model utility as Causal
Consistency.

In this section, we will brieﬂy present the foundation of causal theory based on the work of Pearl [211], as
it is currently the most common causal framework. There are adoptions and adaptations for individual ﬁelds,
such as the social sciences [187, 87] and public health research [264]. Moreover, recent Bayesian statistics
textbooks have started discussing causality as a central aspect of statistical analysis [180]. In addition, the
ﬁelds of causal discovery [131, 250, 109] and optimal experimental design [74, 79, 134] deserve a mention
as well, as they tackle problems related to causality. Finally, other promising causal frameworks have been
proposed [133] but are not discussed in detail here for reasons of brevity.

3.1.1. Structural Causal Models

Pearl [211] proposes a framework to explicate causal assumptions and construct requirements on probabilistic
models to make them consistent with those assumptions. The mathematical objects that allow for causal
analysis are called structural causal models [SCMs, 212] and they comprise structural equations (what we
express via P models), causal graphs, as well as interventional and counterfactual logic [214]. For instance,
linear regression P models, if combined with proper causal calculus, comprise a widely used and simple form
of linear SCMs. However, vastly more complex SCM architectures are possible, such as causal generative
neural networks [151], where a causal graph is connected to a generative adversarial network responsible for
learning interventional distributions (see Section 3.1.2 for details on interventions).

For the purpose of this paper, it is suﬃcient to discuss SCMs comprising a set of three endogenous

11

variables whose causal relationships are to be studied. We refer to these variables as w, x, and y. For
every endogenous variable, we assume there exists a corresponding exogenous (noise) variable, ξw, ξx, and
ξy, respectively. Under the assumption of causal suﬃciency (i.e., every exogenous variable aﬀects no more
than a single endogenous variable), a hypothesis of the form “x causes y” means that y is generated by a
structural equation y = gy(x, ξy) for some function gy. The corresponding causal graph is simply x → y.

To extend this example, the left panel of Figure 4 illustrates a path diagram of the structural equations
relating the endogenous variables w, x, and y, along with the corresponding causal graph w → x → y.
Importantly, any set of structural equations also encodes assumptions about the lack of causal inﬂuence.
For instance, the absence of w from the right-hand side of gy conveys the assumption that y will remain
invariant to changes in w, as long as variables x and ξy remain constant.

In general, a causal graph implied by a set of structural equations will be a directed acyclic graph (DAG).
It can be constructed as follows: The variables that appear on the right-hand side of a structural equation
become the parents of the variable that appears on the left-hand side of the structural equation. We can
understand the structural equations as encoding explicit structural assumptions about the opaque (true) data
generator G, which in turn implies a (true) joint distribution of the endogenous variables, here p∗(x, y, z).
This distribution is realized by ﬁrst assuming a joint distribution of the noise variables, p∗(ξw, ξx, ξy), and
then propagating this uncertainty to w, x, and y through the respective structural equations.

In P model terms, a DAG can be understood as deﬁning a Bayesian network for the implied joint
probability distribution of the endogenous variables [212]. The conditional distribution of an arbitrary
endogenous variable v is given by p(v | Nv), where Nv denotes set of parent variables of v as implied by the
DAG. For the current example, this would imply a generative likelihood that factorizes as p(w, x, y | θ) =
p(y | x, θ) p(x | w, θ) p(w | θ), where θ are our P model parameters (left unspeciﬁed in the DAG).

In our model taxonomy, a P model may or may not be consistent with the set of causal assumptions
embodied in a DAG, which constitutes a binary metric of causal consistency. For example, consider again
the simple DAG given by x → y, with structural equation y = gy(x, ξy). The concrete approximation of gy
is part of the P model assumptions (see below), whereas adherence to the (external) DAG implies satisfying
causal consistency. For example, consider the following linear P model

x = ξx
y = βx + ξy

(4)

with unspeciﬁed distributional forms of ξx, ξy, and β for simplicity. The approximation ˆgy chosen for gy is
ˆgy(x, ξy) = βx + ξy while ˆgx(ξx) = ξx is just the identity function. The above P model is clearly causally
consistent with the DAG x → y. In contrast, another linear P model in which we had swapped x and y (i.e.,
assuming x = βy + ξx), would be causally inconsistent with the graph x → y.

In linear P models, the regression coeﬃcients represent path coeﬃcients of structural equations and
thus quantify the linear “causal eﬀects” of certain variables on others. However, even when a linear P
model is causally consistent with a given DAG, its linear functional form y = βx + ξy may still be a poor
approximation of the true (potentially highly non-linear) structural equation y = gy(x, ξy). Thus, an equally
causally consistent, but more ﬂexible, non-linear P model may be a better choice in the end, depending on
other utility dimensions. This illustrates that causal consistency, as deﬁned here by the formal agreement
with a causal DAG, is only a necessary, but not a suﬃcient condition for a P model to provide trustworthy
causal inference. Further requirements will be discussed in the context of parameter recoverability (see
Section 3.2).

Causal graphs allow for an unambiguous communication of assumptions about causal relations, but on
their own, they still represent static entities. In contrast, interventions and counterfactuals describe actions
which enable us to answer causal queries based on (a subset of) these assumptions. Below, for the sake of
brevity, we will elaborate solely on interventions (see [211] for more details of counterfactuals).

3.1.2.

Interventions

An intervention is an operation that changes the underlying structural equations, hence the corresponding
causal graph. Intervening on x means setting it to a ﬁxed value ˜x, say, administering the treatment ˜x to a

12

patient. We denote an intervention as do(x = ˜x) or simply do(˜x) for short. The eﬀect of an intervention on
the path diagram of our example three-variable SCM is shown in the right panel of Figure 4. An intervention
do(˜x) diﬀers from conditioning on ˜x in the following way: The former removes the connections of node x to
its parents, whereas the latter does not change the causal graph from which data is generated [211, 151]. If
we set the value of x to some ˜x, then it is no longer determined through the structural equation gx(w, ξx),
that is, we have intervened in the generative mechanism.
Importantly, the interventional distribution of
interest, say, p(y | do(˜x)) may diﬀer from the corresponding conditional distribution p(y | ˜x).

However, when we only have access to observational data because we cannot intervene in the causal
graph (e.g., an experiment is too expensive to perform), our resort is to estimate conditional distributions.
Thus, an important question arises: “Which causal queries can we answer (i.e., which interventions’ eﬀects
can we estimate) based on observational data alone?” In the language of do-calculus, this translates to
the question of whether we can circumvent the do operator and express the interventional distribution of
interest, p(y | do(˜x)) via a conditional distribution [213]. For this purpose, we can use three basic rules of
do-calculus that specify the conditions under which we can 1) ignore observations, 2) treat interventions as
equivalent to observations, and 3) ignore interventions [213].

Against this background, we say that a P model is causally consistent for a given causal query, if that
query can be answered by applying the rules of do-calculus to the underlying DAG and all necessary con-
ditional distributions are part of the P model. A P model which is causally consistent with a DAG is also
causally consistent for all valid causal queries of that DAG. In practice, however, we can rarely attain (or
care about) the former, but are only concerned with causal consistency for a few queries of interest. To
illustrate this point, let us again consider the DAG w → x → y from Figure 4. The linear P model (4) is not
causally consistent with this DAG, since it does not include the structural equation x = gx(w, ξx) but only
y = gy(x, ξy). However, it is causally consistent for the speciﬁc query p(y | do(˜x)) because, after applying
the second rule of do-calculus, we ﬁnd that p(y | do(˜x)) = p(y | ˜x) for this DAG. Correspondingly, the latter
conditional distribution is part of the P model in the form of y = βx + ξy. Naturally, the conditions under
which we can answer causal queries using conditional distributions become harder to test for causal graphs
containing more than just three variables, but the underlying principles remain the same [52].

3.2. Parameter Recoverability

A central goal of Bayesian modeling is to perform parameter inference, that is, to draw conclusions directly
from the posterior of the latent parameters or other pushforward quantities of interest. But how can we
assess whether our inferences are informative and capture all relevant layers of uncertainty? The Parameter
Recoverability dimension captures the ability of P models (and of PA models; see Section 3.2.3) to gain
information from data and perform faithful uncertainty quantiﬁcation. Moreover, recoverability is a concept
where frequentist statistics inevitably play a role, even in the context of purely Bayesian models.

For the purpose of generality, consider the task of estimating a quantity of interest ϕ based on a P
model and (yet to be realized) data y using an estimator ψ = ψ(θ) of ϕ where θ ∼ p(θ | y). The epistemic
uncertainty implied by the posterior p(θ | y) is naturally propagated to the posterior of ψ. Based on the
implied posterior p(ψ(θ) | y), we can derive both point and uncertainty estimates, among other things, as
detailed further below.

To make this notion more concrete, let us consider a simple example. Suppose we are interested in the
(true) mean diﬀerence of y between two groups, ϕ = E[y1] − E[y2], where y1 and y2 represent the responses
of the two groups, respectively. One way to estimate ϕ here is via a linear regression P model with response
vector y = (y1, y2) and pointwise likelihood

yn ∼ Normal(µn, σ)
µn = β1 × I(n ∈ C1) + β2 × I(n ∈ C2),

where I is the indicator function, C1 is the index set of observations n belonging to Group 1, and C2 is
the corresponding index set of Group 2. Then, based on this P model, we deﬁne an estimator ψ of ϕ as
ψ = β1 − β2. Accordingly, ψ does not need to be a model parameter itself, but can be any pushforward

13

Figure 5: Three hypothetical (univariate) PD model scenarios illustrating posterior contraction and Bayesian
surprise. The leftmost panel depicts a PD model which yields both large posterior contraction and large
Bayesian surprise. The middle panel depicts a PD model exhibiting both small posterior contraction and
small Bayesian surprise. The rightmost panel depicts a PD model which has zero posterior contraction (i.e.,
equal prior and posterior variances), yet non-zero Bayesian surprise (i.e., owing to a diﬀerent tail exponent).
Posterior contraction is easier to compute and interpret, but Bayesian surprise is more general, as it captures
diﬀerences beyond second moments (i.e., variances).

quantity computable from the parameters. The Gauss-Markov theorem tells us that the chosen estimator
ψ has the lowest sampling variance among the class of linear unbiased estimators in case of ﬂat priors on
β1 and β2. However, the properties of any estimator in general are not always that clear: Consider another
example where the true data generator is given by yn = f (ϕ xn) + ξn, with x being a known continuous
variable, f a monotonically increasing function, and ξ an additive error term. In the absence of knowledge
about the exact form of f , we could set up a P model with a normal likelihood that is linear in ψ,

yn ∼ Normal(ψ xn, σ).

Moreover, the properties of ψ as an estimator of ϕ will certainly depend on the unknown function f and is
likely not as favourable as in the ﬁrst example. However, by means of ψ, we can at least hope to get the
sign of ϕ right, which may as well turn out to be suﬃcient for meeting the goals of some applications.

3.2.1.

Identiﬁability and Information Gain

Oftentimes, we are interested in learning something about the (true) real-world generator G through the
P model-dependent quantity ψ, justiﬁed by its resemblance to the model-independent quantity ϕ that we
assume to play a role in G. As a ﬁrst step, we need to study whether the data generated by the unknown
process enables the P model to extract any information about ψ at all. If the data is not informative, there is
no point in further studying recoverability of ϕ by means of ψ. In a frequentist sense, we say that a quantity
ψ is identiﬁed in the given P model, if all the possible values of ψ lead to unique conditional distributions,
that is, for any ψ1 (cid:54)= ψ2 we have p(y | ψ1) (cid:54)= p(y | ψ2) [47]. Thus, frequentist identiﬁcation implies that, in
the limit of inﬁnite data, no ambiguity remains about possible values of ψ.

In a Bayesian context, the posterior captures all information about ψ gained from the data. Thus, the
posterior should be a key object for deﬁning identiﬁability. Since the posterior always exists (as long as
the prior is proper), regardless of how informative the data are, the mere existence of the posterior is not a
helpful measure of identiﬁability [see also 161, 111, 239, 238, 22, for discussions of Bayesian identiﬁcation].
Instead, we have to deﬁne identiﬁability by a juxtaposition of prior and posterior. The transition from
prior to posterior (i.e., Bayesian updating) essentially conveys a reduction in uncertainty brought about
by observing some data. Equivalently, it can be seen as communicating the information gain achieved by
accounting for the data. Thus, we expect the posterior to be narrower (sharper) than the prior, as the
opposite would imply a loss of information through observation – a rather paradoxical scenario. In other
words, the data should be suﬃciently informative of ψ, otherwise the posterior will just resemble the prior.
Bayesian surprise oﬀers a way to quantify arbitrary diﬀerences between prior and posterior. The Bayesian

14

surprise is typically deﬁned as the Kullback-Leibler (KL) divergence between the two distributions

BS(ψ | y) := KL [p (ψ | y) || p (ψ)]

(cid:90)

=

p (ψ(θ) | y) log

(cid:18) p (ψ(θ) | y)
p (ψ(θ))

(cid:19)

dθ,

(5)

(6)

but other divergence or integral metrics are also possible [189]. The Bayesian surprise, as deﬁned above, is
non-negative and equals zero if and only if p (ψ | y) = p (ψ). Henceforth, to avoid commitment to the KL
divergence, we will use the symbol D to denote any divergence with the above two properties. In information
theory, this particular form of the Bayesian surprise is called a relative entropy, and, in Bayesian terms,
represents the information gained by updating the prior to the posterior in units determined by the base of
the logarithm.2 Accordingly, in a Bayesian context, ψ is identiﬁed, if the relative entropy is non-zero.

Further, the concept of posterior contraction provides a simpler and tractable empirical diagnostic to
assess identiﬁcation and degrees of informativeness [22]. Posterior contraction formalizes the idea that the
posterior should get narrower as the amount of data increases and is computed as the ratio between posterior
and prior variance:

PC(ψ | y) := 1 −

Varp(θ|y)(ψ(θ))
Varp(θ)(ψ(θ))

.

(7)

If y contains no information about ψ, then PC(ψ | y) = 0. Conversely, the more information (i.e., uncertainty
reduction) we gain from y, the larger PC(ψ | y) becomes, up to a maximum of PC(ψ | y) = 1. The posterior
contraction can be combined with the posterior z-score (i.e., the diﬀerence between true parameter and its
posterior mean) as an intuitive two-dimensional estimate of the information gain that can be achieved by a
P model when combined with data D [240].

Posterior contraction compares only the second moments (i.e., the variance) of the prior and the posterior.
However, relevant diﬀerences between prior and posterior may manifest themselves in higher-moments of the
distributions and it is still possible that we learn something about a distribution, for instance, about its tail
exponent or symmetry, while its variance remains largely unchanged (see Figure 5 for an illustration).

So far, we have only considered posterior contraction and Bayesian surprise brought about by a single
data set y. Thus, Equation (6) provides only a measure for local (i.e., per-data) information gain. Whenever
we are interested in global (i.e., in expectation over all possible observations) information gain, then the
expected Bayesian surprise (EBS) should be considered:

EBS(ψ) := Ep∗(y)

(cid:2)D [p (ψ | y) || p (ψ)] (cid:3)

(cid:90)

=

D [p (ψ | y) || p (ψ)] p∗(y) dy,

(8)

(9)

or, similarly, the expected posterior contraction (EPC). Note, that global information gain assumes access to
the distribution p∗ of real-world generator outputs and so we can rarely compute this quantity in practice.
Instead, we can obtain a Monte Carlo estimate of Equation (9) over multiple observed data sets as an
approximation of the true EBS.

In many scenarios (e.g., during model development), we are interested in the recoverability of ψ over
the full generative scope of a model P, in combination with a posterior approximator A, before collecting
In this case, we will be considering the approximate posterior pA(ψ | y) and estimating the
any data.
diﬀerence between prior and approximate posterior with respect to the joint distribution p(θ, y) implied by
the P model:

EBSP,A(ψ) := Ep(θ,y)

(cid:2)D [pA(ψ | y) || p (ψ)] (cid:3),

(10)

In other words, we assume the P model to be a good representation of p∗(y) and evaluate the identiﬁcation of
ψ under this assumption for a given approximator A. Note that approximating the expectation over p(y, θ)

2Whenever an approximation of the Bayesian surprise is intractable because it requires access to the analytic prior and
posterior densities, an integral metric such as the maximum mean discrepancy (MMD, [116]) that can eﬃciently be approximated
by prior and posterior draws can be used to deﬁne Bayesian surprise as well.

15

will be computationally expensive for many PA models relying on non-amortized approximators (i.e., ABC
or MCMC), since estimating the posterior pA(ψ | y) repeatedly will dominate almost any approach (see also
Section 3.2.3). Thus, well-calibrated amortized approximators [115, 220] can serve as remarkable catalysts
for eﬃciently quantifying global information gain for a given PA model before committing to the (costly)
process of data collection.

3.2.2. Ground-Truth Comparisons

We have hitherto assumed that we are dealing with a black-box (true) generator G whose actions give rise to
the data-generating distribution p∗(y). Thus, we did not require ϕ to play any actual role in the process of
data generation. In this section, we will restrict our focus to scenarios where ϕ does in fact represent some
intrinsic properties of G. Thus, we assume an unknown conditional data-generating distribution p∗(y | ϕ)
and are interested in the similarity between ϕ and its P-model based estimator ψ.

Obtaining the posterior of ψ for a single data set and verifying suﬃcient information gain will tell us
nothing about the recoverability of ϕ given a P model, (i), because the resemblance between ϕ and its
estimator ψ remains unclear and, (ii), because we need to consider the variation in y, that is, variation
across possible data sets as well. This means that we ought to estimate the performance of an estimator in
expectation over possible data:

Ep∗(y|ϕ)[f (ϕ, ψ)] =

(cid:90)

f (ϕ, ψ | y) p∗(y | ϕ) dy,

(11)

where f (ϕ, ψ | y) is some function comparing ϕ with the posterior of ψ, conditional on data y (see below for
examples). If the applied P model were the actual data generator itself, then p∗(y | ϕ) would be equal to
p(y | ϕ) = (cid:82) p(y, θ | ϕ) dθ and we could set ψ = ϕ. In this case, ϕ could be directly estimated through its
own posterior distribution induced by ϕ(θ) with θ ∼ p(θ | y). However, in reality, we do not know how well
P represents the actual generator, and so we continue to distinguish ϕ from its P model-based estimator ψ.
Notably, the evaluation of Equation (11) does not actually require any observed data and so can be
done ahead of time, before commencing any data collection. Unfortunately, as for many things in Bayesian
statistics, it is a lot easier to write down the target in mathematical notation than to actually compute it:
The integral in (11) is almost always intractable, even if the posterior of ψ itself were analytic. Thus, in
statistical practice, we will approximate the integral with a ﬁnite sum over M independently simulated data
sets y1, . . . , yM ∼ p∗(y | ψ):

Ep∗(y|ϕ)[f (ϕ, ψ)]

MC
≈

1
M

M
(cid:88)

m=1

f (ϕ, ψ | ym)

(12)

This Monte Carlo estimate is now conceptually easy to compute, but potentially very time-consuming, since
the P model needs to be ﬁt M times, whereby each single ﬁt may itself demand a considerable amount of
time.

In Equations (11) and (12), ϕ is held constant, which constitutes the typical setup in simulation studies
where we ﬁx the ground-truth to a single value per simulation instance. However, the conclusions we can
draw from such studies are naturally limited to the few investigated simulation instances (chosen ground-
truths). If the investigated instances were non-representative in reality, then we would learn little to nothing
of value from our simulations, even if the data-generating distribution p∗(y | ϕ) itself were faithful. In order
to consider this implied uncertainty, we can make the criterion (11) fully Bayesian by adding a prior p∗(ϕ)
over ϕ. Thereby, we can now measure recovery in expectation over data y and a priori plausible values of
the quantity of interest ϕ:

Ep∗(y,ϕ)[f (ϕ, ψ)] =

(cid:90) (cid:90)

f (ϕ, ψ | y) p∗(y | ϕ) p∗(ϕ) dy dϕ,

with Monte Carlo (simulation-based) approximation

Ep∗(y,ϕ)[f (ϕ, ψ)]

MC

≈

f (ϕm, ψ | ym)

1
M

M
(cid:88)

m=1

16

(13)

(14)

for M ground-truth simulations, each generated according to ϕm ∼ p∗(ϕ) and ym ∼ p∗(y | ϕm).

Point Estimation. One central aspect of parameter recoverability that can be assessed in terms of expec-
tations over the data-generating distributions is point estimation. We write T (ψ | y) for a point estimator
derived from the posterior of ψ. Most commonly, we compute the posterior mean (cid:82) ψ(θ) p(θ | y) dθ, or
alternatively the posterior median or mode. Due to aleatoric uncertainty in the data y, we cannot expect
T (ψ | y) = ϕ for all y, even if the former would be the best possible point estimator of ϕ. Instead, we
can measure how far away our estimator is from the truth via a strict distance function d on T (ψ | y)
and ϕ, such that d(T (ψ | y), ψ) = 0 holds if and only if T (ψ | y) = ϕ. Common distance functions are
the bias T (ψ | y) − ϕ, the squared error (T (ψ | y) − ϕ)2, and the absolute error |T (ψ | y) − ϕ|. To esti-
mate the performance of a point estimator in expectation over the data-generating process, we would set
f (ϕ, ψ | y) = d(T (ψ | y), ψ) and then apply Equations (11) to (14). Whenever we compare P models based
on their point estimation capabilities, we would prefer the P model with the smallest expected distance of
its point estimator(s) to the assumed true ϕ.

Uncertainty Estimation. An uncertainty estimator is deﬁned as a parameter region that is supposed
to contain the true quantity of interest ϕ with a certain (user-deﬁned) probability q. We write Uq(ψ | y)
to denote a q uncertainty region derived from the posterior of ψ. Common Bayesian uncertainty regions
are quantile-based credible intervals and highest density intervals (HDIs) [100]. We say that an uncertainty
region is well calibrated for a given ϕ (in a frequentist sense), if the following equality holds:

q = Ep∗(y|ϕ)[I(ϕ ∈ Uq(ψ)] =

(cid:90)

I(ϕ ∈ Uq(ψ | y)) p∗(y | ϕ) dy,

(15)

where I(ϕ ∈ Uq(ψ | y)) is the indicator function evaluating to 1 if ϕ ∈ Uq(ψ | y) and to 0 otherwise. In other
words, an uncertainty region for probability q is well calibrated if it contains the assumed true parameter in
a fraction of q data sets. If the above property holds for every uncertainty region Uq(ψ | y), we say that the
whole posterior of ψ is well calibrated for estimation of ϕ.

Bayesian uncertainty regions are not generally designed to satisfy this frequentist calibration and there is
no guarantee that they will [100, 190]. Yet, it can be a perfectly valid approach to use them even to satisfy
purely frequentist goals [93]. Interestingly, when considering expectations over (y, ϕ) ∼ p∗(y | ϕ) p∗(ϕ) as
in Equation (13), a P model will exhibit perfect calibration as long as its generative behavior matches the
unknown data generator and posterior computation is exact [257]. This property is extensively used in
diagnosing the correctness of posterior approximations, a topic we will discuss in Section 3.2.3.

When comparing P models based on their uncertainty estimation of ϕ, we would prefer the model which
yields uncertainty estimates closest to the equality in Equation (15) for some pre-selected, application-speciﬁc
uncertainty region(s). For example, if we were primarily interested in well-calibrated 95% credible intervals
(perhaps more precisely stated, compatible intervals, [180]), then we would prefer the model for which these
intervals had closest to q = .95 coverage of the assumed true ϕ. That said, for some speciﬁc analysis goals,
for example in null-hypothesis signiﬁcance testing [154], over-coverage (higher than q coverage) may be more
acceptable than under-coverage, or vice versa, depending on the assigned utility values of the corresponding
Type-I and Type-II errors [241].

Sharpness. Multiple P models, say P1 and P2, may provide estimators ψP1 and ψP2 that are equally well
calibrated for a quantity of interest ϕ, yet their uncertainty regions my diﬀer in coverage [110]. This implies
that calibration alone is insuﬃcient to describe the appropriateness of uncertainty regions: We additionally
need the concept of sharpness. We say that model P1 is sharper than model P2 for an uncertainty region
Uq(ψ | y) with ﬁnite bounds, if that region is better or equally well calibrated in P1 than for P2 and if the
volume of Uq(ψP1 | y) is smaller than the volume of Uq(ψP2 | y) in expectation over the data-generating
distribution:

Ep∗(y|ϕ) [Vol(Uq(ψP1 | y))] < Ep∗(y|ϕ) [Vol(Uq(ψP2 | y))] ,

(16)

17

where Vol indicates the volume in Euclidean space. For unidimensional ϕ and corresponding uncertainty
region, say, a 95% credible interval, the volume is simply equal to the width of the interval. Of course,
depending on whether we hold ϕ constant or assign a generating prior p∗(ϕ) to it, we can also investigate
sharpness in expectation over the joint distribution p∗(y | ϕ) p∗(ϕ), instead of only over p∗(y | ϕ).
If
sharpness holds for all ﬁnite volume uncertainty regions Uq(ψ | y), then the posterior of ψP1 is sharper
than the posterior of ψP2. Well calibrated uncertainty regions cannot be inﬁnitely sharp and there exists a
sharpest model and corresponding estimator, if the set of well calibrated models is non-empty [110]. However,
in practice, we have no access to this sharpest model. Thus, in contrast to calibration, sharpness cannot be
practically computed in an absolute sense, but can only be probed as a relative quantity in the context of
two or more models.

3.2.3. Calibration of Posterior Approximations

So far we have primarily focused on P models in the context of parameter recoverability and all of the
estimators assumed access to the analytic posterior p(θ | y) to obtain the analytic posterior p(ψ(θ) | y) of the
estimator ψ of ϕ. Since we do not have access to the analytic posterior in practice, our typical estimators
are based on PA models.

Figure 6: SBC-based rank histograms (top) and corresponding ECDF diﬀerence plots [256] (bottom) for
three hypothetical quantities of interest. The blue areas in the ECDF diﬀerence plots indicate 95%-conﬁdence
intervals under the assumptions of uniformity and thus allow for a null-hypothesis signiﬁcance test of self-
consistent calibration. Left: A well-calibrated quantity. Center: A miscalibrated quantity with too many
lower ranks indicating a positive bias in the PA model-based posteriors. Right: A miscalibrated quantity with
too many extreme ranks indicating overconﬁdent PA model-based posteriors (i.e., variance underestimated).

Correspondingly, we deﬁne the estimator ψA of ϕ via the approximate posterior pA(ψ(θ) | y) of ψ
obtained by the approximator A. If A were approximating the posterior via random draws θs from pA(θ | y),
the approximate posterior pA(ψ(θ) | y) would be represented by the pushforward draws ψ(θs). Thus, we can
evaluate identiﬁability, point and uncertainty estimation, as well as sharpness, of a PA model by replacing
ψ with ψA in the corresponding equations. Ideally, we would like to separate estimation of ϕ via ψ from
estimation of ψ via ψA and we can do so if we assume that the considered P model is the true generator

18

02505007501000025050075010000250500750100005101520rank−0.10.00.1−0.10.00.10.20.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00−0.15−0.10−0.050.000.050.100.15itself. This is due to two related self-consistency properties. The ﬁrst one is

(cid:90) (cid:90)

p(θ) =

p(θ | y) p(y | θ∗) p(θ∗) dy dθ∗,

(17)

which states that a P model’s prior (left-hand side) is equal to the P model’s data-averaged posterior (right-
hand side), that is, the posterior in expectation over its own generating distribution [257]. The second one
states that all uncertainty regions Uq(ψ | y) of all pushforward quantities ψ are well calibrated, as long the
generating distribution of the assumed P model is equal to true data-generating distribution and posterior
computation is exact [257]. Writing ψ∗ instead of ϕ to explicate the direct correspondence between the
quantity of interest and its estimator ψ, this property can be written as
(cid:90) (cid:90)

q =

I(ψ∗ ∈ Uq(ψ | y)) p(y | ψ∗) p(ψ∗) dy dψ∗.

(18)

Both self-consistency properties are useful, but Equality (18) provides a particularly convenient means to
diagnose the calibration of the approximated posterior pA(ψ(θ) | y): Under perfect (self-consistent) calibra-
tion, the posterior probability Pr(ψ∗ ≤ ψ) is uniformly distributed in the unit interval [257, 256]. If the
approximate posterior can be expressed in terms of random draws, then uniformity can be tested empirically
by comparing the empirical distribution of ranks

r(ψ∗, ψ(θ1:S) | ym) :=

S
(cid:88)

s=1

I(ψ∗ ≤ ψ(θs))

for

θs ∼ pA(ψ(θ) | ym)

(19)

over M simulated data sets to a uniform distribution, a procedure known an simulation-based calibration
[SBC, 257]. If the distribution of ranks is close enough to uniformity (e.g., according to a frequentist null-
hypothesis signiﬁcance test), we can conclude that the PA model is well calibrated for approximating the
P model, assuming self-consistency of P. The required uniformity can be checked graphically, for example
via histograms (top row of Figure 6) or by plotting the empirical cumulative distribution function (ECDF)
of the ranks normalized against their expected values under uniformity (bottom row of Figure 6), a method
known as ECDF diﬀerence plots [256].

Even though self-consistency tested via SBC is a powerful tool to ascertain the trustworthiness of a
PA model if the underlying P model is well speciﬁed, it tells us nothing about the trustworthiness of PA
if P is misspeciﬁed, that is, if its joint distribution does not represent the true data generating process
p∗(y). In the latter case, we currently have no general procedure to verify the trustworthiness of a posterior
approximation, that is, how close a PAD model for given data is to the PD model it attempts to approximate
(but see [175, 291] for recent theoretical work). We can only hope that self-consistent calibration of PA implies
good enough calibration in a suﬃciently large model neighborhood of P that also contains p∗. For posterior
approximators coming with guarantees of asymptotic correctness such as MCMC, this hope is probably
better justiﬁed than for, say, neural approximators that have been shown to perform poorly under P model
misspeciﬁcation [242].

3.3. Predictive Performance

Undoubtedly, predictive performance is the central utility in most machine learning research [122] and an
essential goal of computational [203] and scientiﬁc models in general [91]. Moreover, predictive performance
has recently been elevated to an indispensable condition for reproducible quantitative research in the social
sciences [289]. In deep learning, enormous amounts of computing resources are spent even for just a second
decimal improvement in predictive accuracy on domain benchmark data sets [106], notably at the expense of
other utilities (e.g., parsimony, see Section 3.6, or estimation speed, see Section 3.9). In our Bayesian model
taxonomy, we treat predictive performance as just one of the ten model utilities, but we still recognize it as
an important one.

In a way, predictive performance would be nothing but a special case of parameter recoverability (see
Section 3.2), if not for the fact that it targets observable variables that are comparable against observed data.

19

This opens up the possibility to directly evaluate predictive performance in real-world scenarios instead of
having to use simulations, as is often necessary for estimating parameter recoverability. Along similar lines,
predictive P(D) model comparison or averaging can be seen as a form of parameter recoverability from the
perspective of mixture modeling (with the individual P models as components) or in terms of continuous
model expansion [97]. However, in practice, we approach these challenges mainly on the basis of predictions
from separate P(A)D models to reduce conceptual and computational costs [288].

In the following, we denote the set of “test” data to be predicted as y∗, whereas P(A)D model “training”
data continues to be denoted as ˜y. In principle, these two data sets are allowed to fully coincide, partially
overlap, or be completely disjoint (see Section 3.3.3), and ˜y may even be empty (see Section 3.3.2). Further,
we will allow the test data to be clustered into C mutually independent and exhaustive clusters y∗ = {y∗
c=1.
In most applications, both y∗ and ˜y are associated with observed input variables (aka features, predictors,
or covariates), but we will keep these implicit to make the notation more readable.

c }C

The ocean of predictive performance metrics for Bayesian models is vast and we refer to [266] for a
comprehensive overview. For the purpose of illustrating some overarching points in this article, we will focus
on a few important metrics that follow the general form

L(y∗, ˜y) :=

C
(cid:88)

c=1

l (cid:0)Ep(θ|˜y)[f (y∗

c , θ)](cid:1) =

(cid:18)(cid:90)

C
(cid:88)

l

c=1

f (y∗

c , θ) p(θ | ˜y) dθ

(cid:19)

,

(20)

c , θ) is a predictive score comparing a test data cluster y∗

where f (y∗
c with corresponding model-based predic-
tions. We compute the expected predictive score by integrating over the PD model posterior p(θ | ˜y), where l
is some function applied to each expectation before summation over clusters. Whenever we use a PAD model,
we need to approximate the above expectation over pA(θ | ˜y), either by using random draws from pA(θ | ˜y) or
by relying on an approximate closed-form density. Below, we examine predictive performance along multiple
dimensions: absolute versus relative, prior versus posterior, and in-sample versus out-of-sample predictive
performance.

3.3.1. Absolute and Relative Predictive Performance

i , θ) = (y∗

Evaluating absolute predictive performance requires knowing an optimally achievable value of the predictive
metric, whereas relative predictive performance only involves comparing multiple P(D) models’ predictions
evaluated on the same test data y∗. As an example for the former, consider the per-observation squared
diﬀerence f (y∗
i − ˆyi(θ))2 as a predictive score, where ˆyi(θ) is a P(D) model-implied prediction
given parameter value θ (e.g., a single random draw or realization from the posterior predictive distribution,
see [266]). In this case, we know that the optimal value of Equation (20) is zero. For the sake of increased
interpretability, such squared diﬀerences can be further transformed to the canonical “percentage of explained
variance” R2 measures which are bounded between 0 and 1, the latter indicating optimal predictions [102].
However, optimal predictions are not achievable in practice, since even a Bayes-optimal decision maker
may elicit suboptimal predictions in the presence of aleatoric uncertainty [122], at least when it comes to out-
of-sample predictions (see Section 3.3.3). Moreover, since we nearly never know the Bayes-optimal decisions
in practice (hence the need for predictive modeling in the ﬁrst place), the expected optimal achievable
predictive performance is also unknown to us. As a result, relative predictive performance is usually our
only resort in practical applications [266].

That said, some models produce such strikingly poor predictions that they can be ruled out without the
need to ﬁnd a better model ﬁrst, often by means of visual predictive checks [92]. For instance, if we consider
the case illustrated in Figure 7, it is immediately obvious that the normal likelihood P model (left-hand side)
is inappropriate for the count data. As another example, consider a P(A)D model for binary classiﬁcation
that achieves just 50% accuracy, equal to chance level. Assuming a balanced data set (i.e., both classes
occur with the same frequency), we would not need a competing model to conclude that the classiﬁer is
bad – unless our goal was to demonstrate that the two categories cannot be possibly diﬀerentiated given the
available information.

20

3.3.2. Prior and Posterior Predictive Performance

The distinction between prior and posterior predictive performance has often lead to confusion in the past and
still remains a rather precarious one to discuss. Prior and posterior predictive performance are distinguished
on the basis of whether we evaluate predictions before or after conditioning on the training data ˜y, respectively
[163]. In other words, we either compute (or approximate) expectations over the prior, p(θ), or over the
posterior p(θ | ˜y). Since prior predictive performance does not require the training data (see Equation 20),
we consider it a utility of P(A) models, while we view posterior predictive performance as a utility of P(A)D
models. We still require the test data y∗, but it is not a part of any model class in our taxonomy.

Statistically, the line between prior and posterior predictive performance is thin and more quantitative
than qualitative [196]. As an illustration, suppose we observe N data points in total – then we could choose
to use none, ˜y = ∅, or any number between 1 and N for model training (i.e., ﬁtting). For complex P
models, the predictive result implied by using one or two observations for training, rather than none at
all, will be almost identical, despite everything but zero training data technically counting as “posterior”
predictive performance [196]. Yet, the metrics commonly applied to quantify prior and posterior predictive
performance diﬀer not only in the amount of available training data, but also in some other non-trivial ways
(to be explained below).

In general, any predictive metric should match the intended real-world prediction goals. Below, we
will focus on certain (log-)probability metrics, which can be considered good general-purpose choices in the
absence of any known task-speciﬁc option [266].

Prior Predictive Performance. The canonical metric for evaluating prior predictive performance is the
joint P model likelihood evaluated at the test data, f (y∗, θ) = p(y∗ | θ), with C = 1 and l = identity, in
which case the prior expectation above becomes the marginal likelihood:

p(y∗) = Ep(θ)[p(y∗ | θ)] =

(cid:90)

p(y∗ | θ) p(θ) d θ.

(21)

When used for model comparison, the marginal likelihood then gives rise to well known comparative metrics
known as Bayes factors evaluated by comparing two P models Pj and Pk as

BFjk :=

p(y∗ | Pj)
p(y∗ | Pk)

and posterior model probabilities over a set of J models {Pj}J

j=1 as

p(Pj | y∗) =

p(y∗ | Pj) p(Pj)
k=1(y∗ | Pk) p(Pk)

(cid:80)J

,

(22)

(23)

where p(y∗ | Pj) denotes the marginal likelihood of P model Pj and p(Pj) denotes the corresponding prior
probability with (cid:80)J

j=1 p(Pj) = 1, following a closed-world assumption [18, 288].

Although the marginal likelihood is formally an expectation and thus, in theory, we can approximate it
arbitrarily well using suﬃciently many random draws from the prior, it is practically impossible to evaluate
due to its unfavorable pre-asymptotic behavior for any non-trivial model [181, 266, 118]. The main reason
for this is that the parameter subset for which p(y∗ | θ) contributes to the integral in Equation (21) (i.e., the
typical parameter set implied by the test data; [21]) is very narrow and thus we need a very high number
of prior draws to ensure suﬃciently many of them occupy that narrow space. In addition, numerical issues
caused by p(y∗ | θ), such as ﬂoating-point underﬂow, can also be hindering.

For these reasons, the practical computation of marginal likelihoods currently rests on bridge sampling
[12] relying on posterior draws from a corresponding PAD model [181, 118]. In contrast to estimating poste-
rior expectations or quantiles, bridge sampling requires about an order of magnitude more posterior draws
to yield reliable results [118] and is still largely missing principled convergence diagnostics or uncertainty
quantiﬁcation [117], leaving room for future research.

21

An alternative prior predictive metric arises if one uses the log-likelihood f (y∗, θ) = log p(y∗ | θ) as a
(predictive) score instead of the likelihood itself, which leads to the Gibbs loss [277] that, for factorizable
likelihoods [35], evaluates to

Gibbsp(θ)(y∗) := Ep(θ)[log p(y∗ | θ)] =

N ∗
(cid:88)

i=1

Ep(θ)[log p(y∗

i | θ)].

(24)

The Gibbs loss is not only simpler to evaluate for exponential family models [266] and numerically more stable
than the marginal likelihood, but also exhibits better pre-asymptotic behavior for factorizable likelihoods
when estimated via prior draws, since the integrands become much simpler. However, the Gibbs loss cannot
be used to obtain actual predictions because it does not evaluate to a predictive distribution over y∗ [266].
The latter problem can be avoided by taking expectations with respect to individual test observations
i ﬁrst and only taking the log afterwards (C = N ∗ and l = log), which leads to the expected log predictive
y∗
density (ELPD) metric [266, 267], evaluated over the prior:

ELPDp(θ)(y∗) :=

N ∗
(cid:88)

i=1

log p(y∗

i ) =

N ∗
(cid:88)

i=1

log Ep(θ)[p(y∗

i | θ)].

(25)

Comparing equations (21) and (25), we see that the marginal likelihood considers the joint predictive density
of all test data y∗, while the ELPD considers marginal predictive densities of y∗
i , marginalized over all
other test data. Despite the fact that the ELPD has found wide application in the context of posterior
predictive performance [267], it does not yet seem to play a noteworthy role in the context of prior predictive
performance. However, together with the Gibbs loss, it may become a computationally favourable competitor
to metrics based on the marginal likelihood.

Posterior Predictive Performance. When assessing posterior predictive performance, we apply the
same metrics we encountered in the context of prior predictive performance, but evaluate expectations over
the posterior induced by the training data ˜y. However, the practical popularity of the metrics seems to be
reversed when it comes to posterior predictions. For example, the posterior ELPD

ELPDp(θ|˜y)(y∗) :=

N ∗
(cid:88)

i=1

log p(y∗

i ) =

N ∗
(cid:88)

i=1

log Ep(θ|˜y)[p(y∗

i | θ)]

ﬁnds widespread application [267], while the “conditional marginal likelihood”

p(y∗ | ˜y) = Ep(θ|˜y)[p(y∗ | θ)] =

(cid:90)

p(y∗ | θ) p(θ | ˜y) dθ

(26)

(27)

has not yet attained wide popularity, despite having a number of useful properties [196, 14, 119, 163].

The choice between prior or posterior predictive performance seems to depend on the modeling goals for
which a P model is speciﬁed. While prior predictive performance seems to be favored for the purpose of
testing scientiﬁc theories [285, 120, 76, 119], posterior predictive performance is the perspective of choice in
almost all machine learning scenarios [but see 289], where we ﬁrst obtain a PAD model based on training
data (and perhaps only minimal prior information) and then utilize the model in downstream predictive
tasks [122].

3.3.3.

In-Sample and Out-of-Sample Predictive Performance

We measure in-sample predictive performance if the test data is a subset of the training data, y∗ ⊆ ˜y, but
measure out-of-sample predictive performance if test and training data do not overlap, that is, y∗ ∩ ˜y =
∅. Whenever we evaluate prior predictive performance, we have no training data per deﬁnition and thus

22

always measure out-of-sample predictions. Accordingly, the diﬀerence between in-sample and out-of-sample
predictive performance only matters in the context of posterior predictions.

From a posterior predictive perspective, the decision between using in-sample and out-of-sample predictive
performance is based on whether or not we want to generalize our inferences from a data set to a wider
population. If a given data set included the entire problem space, then in-sample predictive performance
would be suﬃcient. However, as most introductory statistical courses teach, a data set is typically only a small
sample from a much larger population, to which we would like to extend our inferences. Thus, out-of-sample
predictive performance (aka generalization ability) is almost always what we are after [122, 266, 267, 289].
That said, we can still learn from in-sample predictive performance, as it provides an upper bound for out-of-
sample predictive performance in expectation, such that when in-sample predictions are poor, out-of-sample
predictions are likely to be even worse [122, 267, 92].

In the presence of only a single overall data set ytotal, estimating out-of-sample predictions is practically
realized via data splitting, such that ytotal = {˜y, y∗}. To reduce the dependency of the predictive results
on a single realized data split, we typically perform cross-validation by repeating the data splitting several
times (folds), evaluating out-of-sample predictions for every fold, and then aggregating the results across
folds [253, 266, 267].

The type of cross-validation scheme employed should resemble the envisioned prediction goals for which
the PD model has been created [266]. For example, the predictive goal of time series models is usually to
predict future values based on past values, making leave-future-out cross-validation a sensible choice [43].
Regardless of the type of cross-validation employed, it involves the repeated ﬁtting of the same P(A) model
to diﬀerent data sets. Depending on the number of such reﬁts, the individual data sizes, and the applied
approximator, the required estimation time can quickly become prohibitive for any practical use. As such,
approximate cross-validation procedure that require none or only a few reﬁts have proven to be highly popular
in practice [267, 270, 43]. However, key cross-validation schemes, such as leave-group-out cross-validation,
cannot yet be robustly approximated, so there is more research needed in that direction [200].

Although evaluating out-of-sample predictive performance is often our best shot at preventing overﬁtting
to the training data, it is not always suﬃcient to fully achieve good generalization within commonly applied
model-building workﬂows [103]. In these workﬂows, we typically ﬁt diﬀerent P models to the same data
in an iterative fashion. For example, we might ﬁrst compare two models, decide which one to retain, and
only then ﬁt a third model to compare it with the winner of the ﬁrst round. Even if each model choice was
based on local out-of-sample predictive performance, subsequent results can be informed by out-of-sample
results from previous iterations, making it not strictly out-of-sample for any future iteration steps from the
perspective of the analyst’s knowledge. As such, in an iterative workﬂow, local out-of-sample predictive
metrics may still lead to overﬁtting, but the degree to which this biases the end results remains a topic for
future research.

3.3.4. Predictions in a Dynamic World

Time is one of the most precipitous sources of uncertainty and any attempt to forecast the future with
a static, time-independent P(A)D model will only be meaningful, if the opaque generator G is strictly
stationary (i.e., its regularities are invariant with respect to time). Otherwise, a P model needs to have an
appropriate temporal resolution in order to deliver reasonable out-of-sample predictions beyond the empirical
snapshot of the collected data. Moreover, since the precise details of temporal shifts are extremely hard to
anticipate, a P(A)D model which claims universal predictive performance should regularly be subjected to
the falsiﬁcation of time.

This brings us to an important distinction when it comes to assessing out-of-sample predictive perfor-
mance. Whenever we make our P(A)D model “blind” to certain observations in the original data set D
and use these observations to assess our-of-sample predictive performance (as we do in any form of cross-
validation, even those built for time-serie data [43]), we are essentially testing the model’s ability to perform
induction about the statistical regularities of p∗(y) in a temporal snapshot determined by data collection.
In such a scenario, however, we are not probing the model’s ability to faithfully forecast the future, since the
“left-out” observations are new only from the perspective of the model, but not from that of the modeler.

23

Thus, cross-validation can sometimes be overly optimistic in estimating out-of-sample predictive perfor-
mance, since a sample collected at a future date might exhibit surprisingly diﬀerent properties (i.e., the P
model would no longer be structurally faithful) than the sample currently at hand.

Why would the empirical distribution p∗(y) change over time? One reason can be that the hidden
properties of the generator G itself may change, bringing about alterations in the statistical properties
of p∗(y). For instance, strong auto-correlations in ﬁnancial time-series are notoriously short-lived due to
feedback processes and market adaptation [248]. Yet another reason can be that new sources of noise
contaminate the future data D in unexpected ways. For instance, a sensor in a measurement device may
break and yield incorrect data or case reporting policies during an ongoing pandemic may switch between
waves. However, the P(A)D model may have no mechanism to adapt to any of these changes and its
out-of-sample predictive performance would likely suﬀer.

Within our model taxonomy, prediction failures due to changes in p∗(y) concern misaligned assumptions
about temporal invariances embodied in the P model’s structure. One way to revise these assumptions is to
include time-varying parameters θt in the P model, with the corresponding time-invariant parameterization
being a special (and more parsimonious) case. For instance, this can be achieved within the superstatis-
tics framework [10], which aims to represent heterogeneous dynamics through a superposition of multiple
stochastic processes at diﬀerent temporal scales [173]. In any case, researchers should bear in mind that
static P(A)D models are not designed to deal with things that move, so, as simple as it sounds, time remains
a key arbiter of the quest for universal substantial conclusions or robust predictive systems.

3.4. Fairness

Fairness in the context of model building aims to ensure that model-guided decisions are equitable, with
a speciﬁc focus on groups that diﬀer in protected attributes, such as sex, gender, or ethnic background
[53, 6]. In a relatively narrow sense, fairness is a primary concern for P(A)D models, as it applies to real
world outcomes and their real world reverberations owing to the connection between a P model’s structure
and data D. However, purely simulation-based P(A) models are not exempt from fairness considerations,
especially when used to guide important public policies and decision support systems [42, 5, 195]. In the
following, due to its predominant share in the literature, we will examine the fairness of P(A)D models
from two diﬀerent perspectives, namely, from the perspectives of psychometric measurement and predictive
modeling.

3.4.1. Measurement Fairness

In psychometric measurement theory, the aim is to estimate people’s scores on latent psychological traits,
for example, general intelligence, creativity, or aptitude for university programs [68]. In the model-based
literature of psychometric measurement, namely Item Response Theory (IRT; [262, 73, 38]), two major
aspects of fairness have received considerable attention.

First, we need to ensure that the observable features (i.e., items) have been selected and administered in
a fair way [178, 32, 3]. This aspect does not appear to be immediately model-based, since it concerns the
data collection process as well as causal assumptions about the latent traits’ inﬂuence on the item responses
[32]. However, some of its requirements can be checked via P(A)D models in the form of diﬀerential item
functioning (DIF) analysis [128, 199]. When investigating DIF, the item parameters ζi of item i are allowed to
vary across groups g and their P(A)D model’s posteriors are compared to verify their statistical equivalence.
That is, we aim to examine whether p(ζi | ˜y, g) ≈ p(ζi | ˜y, g(cid:48)) holds for all pairs of considered groups g and
g(cid:48) and all items i.

Second, we need to estimate the latent traits of all individuals with a similar degree of uncertainty [39].
In the context of P(A)D models, this means that the posterior of trait ηj for person j has approximately the
same entropy across all individuals being compared, that is, H(ηj | ˜y) ≈ H(ηj(cid:48) | ˜y) for all pairs of individuals
j and j(cid:48). This turns out to be a diﬃcult, sometimes even unachievable goal: Due to ﬂoor and ceiling eﬀects
arising in almost all psychometric tests, the resulting information is non-uniform across the latent trait space
in non-linear IRT models [262, 42, 39]. As a result, more extreme latent trait scores will be estimated less

24

precisely than more average scores. As a partial remedy, one may try to ensure that the information gain
about all individuals’ trait scores at least exceeds a minimal, application-speciﬁc threshold [39].

3.4.2. Predictive Fairness

What we term predictive fairness has its origins in the ﬁeld machine learning [232, 6]. We will deﬁne
predictive fairness directly on PD models because there is no hope that a P model can yield fair decisions
for all possible training data; after all, training data may themselves be biased against protected groups
[232, 6]. And while we deﬁne it as a utility of PD models, it also automatically pertains to a corresponding
PAD model, unless the posterior has a simple analytic form.

Mathematically, for individual-level decisions, we consider a PD model-speciﬁc decision rule d(x | ˜x, ˜y)
that outputs a decision for each admissible vector of attribute values x given training data D = (˜x, ˜y)
consisting of observed attribute values ˜x and corresponding decision-relevant outcomes ˜y in a supervised
learning context. If we consider only binary decisions to simplify notation, we can write the decision rule as

with

(cid:40)

d(x | ˜x, ˜y) :=

¯r(x | ˜x, ˜y) :=

1
0

(cid:90)

¯r(x | ˜x, ˜y) > τ

if
otherwise

r(x, θ) p(θ | ˜x, ˜y) dθ

(28)

(29)

being a real-valued (expected) risk score of x that is obtained as an expectation over the PD model’s posterior
p(θ | ˜x, ˜y). The decision (e.g., whether to give someone a loan or release a defendant while they await trial)
is then made by comparing the risk score against a pre-deﬁned threshold τ . The conditional risk score r(x, θ)
determines how the P model and its parameters θ are used for assessing risk. For example, the risk score
could be the mean of the PD model’s predictive distribution given feature value x and parameter value θ:

(cid:90)

r(x, θ) :=

y p(y | x, θ) dy.

(30)

Conditional risk scores do not have to rely necessarily on the predictive distribution. Rather, they may also
be based on latent model quantities, such as psychometric trait scores obtained from IRT P(A)D models
[262, 73, 38], which bridges the gap between measurement and predictive fairness.

There are diﬀerent classes of predictive fairness criteria considered in the literature, among others anti-
classiﬁcation [31, 53] and classiﬁcation parity [53, 16] (also known as statistical parity; [51]). Even within
these classes, criteria are partially incompatible and neither of them can actually ensure universal fairness,
but we can still learn from their limitations [53, 51, 6, 16]. In the context of such criteria, we diﬀerentiate
between protected attributes xp (e.g., sex, gender, or ethnic background) and other, unprotected attributes
xu such that x = (xp, xu). Anti-classiﬁcation requires that protected attributes xp (or their proxies; [31])
are not used in model-based decisions at all, which mathematically translates to

d(x | ˜x, ˜y) = d(x(cid:48) | ˜x, ˜y)

for all x, x(cid:48) with xu = x(cid:48)
u.

(31)

In our PAD model taxonomy, this can simply be realized by using a PD model with p(θ | ˜x, ˜y) = p(θ | ˜xu, ˜y)
and conditional risk score r(x, θ) that is independent of xp as well. Anti-classiﬁcation approaches have two
main drawbacks. First, protected attributes can often be predicted fairly well from unprotected attributes,
which makes it impossible to be completely agnostic about them [80]. Second, empirical risk distributions
(after removing all unfair risk inﬂuences) may diﬀer across values of xp, such that ignoring the latter may
actually lead to unfair decisions against the groups one originally attempted to protect [53].

Diﬀerently, classiﬁcation parity comprises a class of fairness criteria that requires the population distri-
bution of certain decision metrics to be the same across all values of the protected attributes [53, 16]. Using
demographic parity [80] as an example, we would require that the decision’s distribution itself, as implied by
the distribution of attributes x in the considered population, to be independent of the protected attributes:

p(d(x | ˜x, ˜y) | xp) = p(d(x | ˜x, ˜y)).

(32)

25

Contrary to anti-classiﬁcation, we usually have to incorporate the protected attributes into the P model in
the ﬁrst place in order to ensure any kind of classiﬁcation parity [16]. In the context of psychological tests,
for example, this could be achieved by imposing group-speciﬁc norms of comparison [237]. Yet, classiﬁcation
parity does not guarantee universal fairness either, whenever the true risk score distribution (after removing
all unfair risk inﬂuences) varies between groups deﬁned by the protected attributes [53].

The shortcomings of these predictive fairness deﬁnitions highlight that requiring a certain outcome –
the decision itself (anti-classiﬁcation) or aspects of its population distribution (classiﬁcation parity) – to be
independent of the protected attributes may be insuﬃcient. Towards the goal of achieving fairness through
a PD model, the underlying P model needs to be causally consistent (see also Section 3.1) in a way that
considers how the protected attributes xp relate to the causal graph that includes all the valid, unprotected
attributes xu and the outcome y [31]. In addition, the training data D needs to be representative of the true
(unbiased) outcome distribution p∗(y). It goes without saying that these are complicated, application-speciﬁc
tasks that require contributions from various scientiﬁc ﬁelds and considerable domain expertise.

What is more, fair decisions, regardless of their modeling context, need to take into account that the
same decision may aﬀect diﬀerent people (and their surroundings) diﬀerently and that these diﬀerences may
be related to both protected and unprotected attributes. More formally, we need to consider the decision
d(x | ˜x, ˜y) in a context C(x) that only together determine the output of a utility function U (d(x | ˜x, ˜y), C(x)),
which oﬀsets all possible gains and losses caused by the decision. Obtaining such a function could steer a
decision towards fairness as quantiﬁed by equal utility outcomes across protected groups.

At an even higher level, we should consider taking suﬃcient precaution that (anticipated) political de-
cisions or societal processes triggered by anonymous modeling results do not lead to unfair treatment of
protected groups. However, such considerations may come into conﬂict with the principle of scientiﬁc free-
dom, in which case a careful ethical analysis of the speciﬁc situation becomes mandatory.

3.5. Structural Faithfulness

In most data analysis scenarios, we have a reasonable amount of qualitative prior knowledge about the data
structure and the data generating process, even if we don’t know the precise analytic relation between the
two. In particular, this knowledge concerns the scales of variables to be modeled, the dependencies between
observations, as well as physical constraints, such as symmetries or invariances. The Structural Faithfulness
utility captures how well a P model incorporates such knowledge. Structural faithfulness is at the core of
statistical modeling, be it Bayesian or otherwise, as it determines the probability distributions we assign to
our observed and unobserved variables, the parameters we add to our P models, and the assumptions we
can justiﬁably make to simplify reality.

Moreover, we can roughly distinguish between probabilistic structure and functional structure, which
are related to the modeler’s degree of ignorance regarding the problem at hand. Purely statistical models
aim to capture the probabilistic structure of p∗(y), without making reference to functional structure of the
hidden generator G. Non-deterministic mechanistic models, on the other hand, aim to capture the functional
structure of G (usually represented by physical constraints), such that the probabilistic structure of p∗(y)
can be reproduced or explained. For instance, when we study the dynamics of a phenomenon via stochastic
diﬀerential equations, functional faithfulness would refer to the mathematical form of the diﬀerential equation
and probabilistic faithfulness would refers to the ﬁdelity of the stochastic assumptions.

To us, it remains unclear how to measure structural faithfulness in an absolute sense and we see it
primarily as a relative metric. What is more, structural faithfulness consists of multiple components that
may each favor a diﬀerent P model. For example, model P1 might take a known symmetry into account
that model P2 ignores, while P2 might assign a more appropriate distribution to a response variable than
P1 does. In this case, none of the two P models would actually be more structurally faithful than the other,
at least not uniformly so.

26

Figure 7: Posterior predictive checks [92] of epilepsy treatment data [259]. The response variable is the
number of epileptic seizures of patients in a given time interval, that is, a count variable without a known
upper bound. Results are shown for three PAD models with diﬀerent likelihoods (shown as facets) and
posteriors approximated via MCMC in Stan [252]. Histograms indicate observed data and each black line
indicates one draw from the posterior predictive distribution of the corresponding PAD model, smoothed
via continuous density estimation. For Poisson and negative-binomial likelihoods, posterior predictions are
in fact counts but are still displayed as smoothed continuous densities to ease readability and comparability
across facets. As is clearly visible on the left-hand side, the PAD model with normal likelihood predicts a
lot of theoretically impossible negative counts and can neither predict the spike at counts close to zero nor
the heavy right tail.

3.5.1. Variable Scales

The scale of a variable determines not only what information it represents but also how it should ideally
be treated within a P model. For example, if the response variable consists of count data without a known
or practically reachable upper bound, we should model this data via an appropriate (unbounded) discrete
distribution (e.g., Poisson, or some of its generalizations) to sensibly capture the aleatoric (irreducible)
uncertainty in those count responses [272, 89, 282]. What is more, this ensures that the variables’ natural
boundaries are respected (e.g., lower bound of zero for count data), such that the corresponding model
predictions cannot go beyond the data space that is possible in reality (see Figure 7 for an illustration). As
another example, if our response variable is ordinal, that is, it consists of discrete ordered categories without
guarantees that the categories can be considered as equidistant, we should model such data via an ordinal
distribution [179, 159, 41]. The same points hold also for predicting variables even if they are not explicitly
modeled by means of a distribution [40, 105]. Failure to consider the variable scales in P models can have
detrimental consequences for the validity of the obtained results [105, 40, 159]. Equivalently, respecting the
intrinsic scales of all quantities included in a P model can help to avoid unreasonable parameter estimates
or implausible (or worse, impossible) predictions.

3.5.2. Probabilistic Structures

Observed data often exhibits speciﬁc probabilistic structures that can be inferred from (qualitative) un-
derstanding of the data-generating process. For example, if we collect psychometric data from multiple
students in the same class, it is highly unlikely that the data points will be mutually independent (e.g.,
because students share the same teacher, rooms, peers, etc.). This situation is prototypical for the applica-
tion of multilevel models, which aim to capture such dependencies [98, 9, 36, 37]. Multilevel models treat
such dependencies of observations belonging to the same group as equivalent to variation between groups

27

04080Normal0255075100Number of epileptic seizuresPoisson0255075100Negative−binomial[98]. In other words, if there were no variation between groups, there would be no structural dependency of
observations within groups (at least none elicited by this grouping structure).

There are three major types of structural dependence between groups that can be expressed as multilevel

models: exchangeable, directed, and undirected [234, 90, 93], illustrated schematically in Figure 8.

Exchangeable groups are the most common assumption in multilevel models and imply that (before seeing
any data) we hold the same prior beliefs about each of the groups but assume they are all drawn from the
same population (e.g., students within classes, classes within schools, schools within cities, etc.). In the most
simple case (i.e., two-level structure, univariate and normally distributed parameters), we would specify a
univariate normal prior for each group indexed by i and group parameter φi as

φi ∼ Normal(µ, σ),

(33)

where µ and σ are the mean and standard deviation parameters shared across groups, respectively. Typically,
we would estimate the across-group parameters from the data along with the group-speciﬁc parameters φi
themselves.

In directed dependency structures, adjacent groups are assumed to have directed inﬂuence on each other
in a way that group i can aﬀect group j, but not vice versa. The most common example is temporal
autocorrelation where a variable at time i can potentially be inﬂuenced by a variable at time i − 1 [250, 93].
For a univariate Gaussian random walk, we would formalize this assumption with the following prior

φi ∼ Normal(φi−1, σ).

(34)

In undirected dependency structures, inﬂuence of adjacent groups can go both ways, with spatial autocor-
relation being the most common example [19, 96, 188]. For example, in (spatial) conditional autoregressive
(CAR) structures [19], we could write down the prior on the group coeﬃcients as

φi ∼ Normal





1
|Ni|

(cid:88)

j∈Ni



φj, σ

 ,

(35)

where Ni is the set of groups that are neighbours of group i.
Importantly, a shared feature of these
dependency structures is that they are agnostic towards the underlying causal mechanisms – their purpose
is purely to accurately represent the inherent probabilistic structure of the observed data [96, 281, 93].

But what if the data-generating process suggests a certain kind of dependency for which we ﬁnd no
empirical support? For example, shall we retain a grouping term of classes even if the PAD model suggests
close to zero variation between groups? There are good arguments for both choices. On the one hand,
excluding such a term implies a simpler model with higher parsimony [8] (see also Sections 3.6), although
the increase in parsimony will be quite small due to the partial pooling property of multilevel models induced
by their hierarchical priors if there is a suﬃcient number of groups [98, 126]. On other hand, including
the term sets a good example for future replications and applications of the same P model, in the same or
diﬀerent contexts. That is, if someone applies this P model to a new data set, they may very well ﬁnd the
between-group variation under question to be non-zero, thus justifying the inclusion of the corresponding
grouping term.

3.5.3. Physical Constraints

In the domains of physics and natural sciences, we tend to have strong prior knowledge about the functional
P model structure in the form of known hard-constraints such as symmetries, invariances, or conservation
laws [263, 225, 144, 4]. For example, a harmonic oscillator expressed by the second-order diﬀerential equation

with functional solution x, second derivative ¨x, as well as constant k, represents an isolated system that is
energy conserving [168].

¨x(t) = k x(t),

(36)

28

Figure 8: Graphs illustrating common probabilistic structures. Rectangles depict nested parameters within
a given probabilistic structure. Circles depict the corresponding hyperparameters. (a) Exchangeable param-
eters; (b) Conditionally dependent parameters with a directed (e.g., temporal) dependency structure. (c)
Conditionally dependent parameters with bidirectional (e.g., spatial) dependency structure.

Similar to a harmonic oscillator, most physical hard constraints can be expressed via diﬀerential equa-
tions whose direct inclusion in a P model is computationally demanding if we do not have access to an
analytic solution [56, 156, 255]. Accordingly, building a more ﬂexible, data-driven P model as a surrogate
is a computationally attractive choice [156, 45]. Still, even for such a surrogate, it remains beneﬁcial to
incorporate known physical constraints to eliminate the need to learn them directly from data. This is likely
to increase the model’s data eﬃciency, that is, the amount of data required by the model to achieve a certain
predictive goal [225, 156]. The discussion about physics-informed modeling is particularly prominent in core
areas of high-dimensional machine learning, such as neural networks that tend to be very data-hungry [225],
but in principle applies to all P models created for representing data with known physical constraints.

3.6. Parsimony

Parsimony refers to the formal simplicity of a Bayesian model; some might deﬁne it as conceptual or
mathematical elegance of the underlying interpretative framework. Here, we view parsimony as a quantiﬁable
property of a Bayesian model. We treat it also as a relative quantity – it is always possible to propose a
more complex model (or possibly a simpler one) which is equally consistent with the available data.

Within our PAD framework, we will distinguish two types of parsimony: P-parsimony and A-parsimony.
P-parsimony characterizes the formal simplicity of a P model and should be measurable from the structure
of the joint distribution p(y, θ). A-parsimony characterizes the simplicity of an approximator and should be
measurable through the interface of A. The former is directly related to the theoretical appeal of a P model’s
probabilistic assumptions; the latter is directly associated with the usability of an approximator.

3.6.1. P-Parsimony

In many real-world modeling scenarios, we have limited data and strive for P models that are able to cap-
ture all relevant latent properties with as little data as possible (see Figure 9 for a simple illustration). One
particular aspect of this goal is captured by the dimensionality of the parameter space, whereby higher par-
simony simply means lower parameter dimensionality. Canonical examples for high parsimony are physical
simulators deﬁned by complex (white-box) forward models with intractable likelihoods [56]. The latter are
informed by strong subject matter knowledge and are thus able to maintain low parameter dimensionality
(e.g., consider the harmonic oscillator Equation (36), which only requires a single parameter to describe
highly non-linear, non-monotonic behavior). On the other end of the spectrum are neural network models
that tend to use simple likelihoods (e.g., Gaussian or categorical), but are characterized by an extremely
high parameter dimensionality and large compositions of non-linear transformations, such as GPT-3 featur-
ing 175 billion parameters [83]. In a way, we need to compensate for our lack of a priori knowledge (or
inability/unwillingness to use it) by applying less parsimonious models that replace more restrictive model
structure with a heightened hunger for data.

29

Figure 9: Diﬀerently parsimonious P models applied to a data set D of 11 observations following a quadratic
relationship in expectation. Left: Most parsimonious, linear model with a 3-parameter likelihood y ∼
Normal(β0 + β1x, σ). This model is too simple for the data. Center: Slightly less parsimonious, quadratic
model with a 4-parameter likelihood y ∼ Normal(β0 +β1x+β2x2, σ). This model’s complexity is just right for
the data. Right: Least parsimonious, linear interpolation model between adjacent points that has as many
parameters as observations in the data (1 intercept and 10 linear slopes). This model is too complex for the
data. Shaded areas indicate 95% credible intervals of the regression line for models where this uncertainty
can be computed.

.

The motivation for parsimony is related to other utilities as well, since more parsimonious P(A)D models
tend to require less data to achieve the same reduction in epistemic uncertainty (parameter recoverability;
Section 3.2) and predictions (predictive performance; Section 3.3), and tend to be easier to comprehend in
real-world applications (interpretability; Section 3.7). Still, we can construct chaotic models – where minimal
changes in the parameters lead to strong changes in the predictions – that are highly parsimonious, yet
uninterpretable and extremely ﬂexible in terms of the function space they can approximate [215]. However,
most P models applied in current practice do not exhibit such chaotic behavior.

Despite its intimate connection to other utilities, we think that parsimony deserves to be a utility in its own
right, harmonized with Occam’s razor: Given two models, and other things being equal, one should choose
the more parsimonious one [27]. Increasing the parsimony of a model (or a scientiﬁc theory, for that matter)
implies making more restrictive assumptions (i.e., reducing the function space that can be theoretically
approximated by the model), thus increasing its falsiﬁability: we can more easily create situations where the
model is wrong. Furthermore, in applied settings, sparser models may lead to more eﬃcient data collection
and more economical measurement designs (i.e., fewer variables to measure or less acquisition trials in design
optimization) [210]. Nevertheless, the strive for parsimony may not always be a useful guide to our scientiﬁc
exploration, if the aesthetics of parsimonious P models make us blind for potentially more appropriate (e.g.,
in terms of other utilities), but less parsimonious representations. For example, the strive for parsimony may
be one of the factors that has stalled the scientiﬁc progress in the foundations of physics during the past
decades [129].

Eﬀective Number of Parameters. There are diﬀerent ways to measure parsimony, with simply counting
the number of parameters3 of a P model being the most straightforward approach. For simple models, such
as linear regression, this measure of parsimony matches the concept of degrees of freedom (DoF) in frequentist
statistics. And in the same way the DoF concept becomes awkward even for slightly more complex models
[138], the former is not a generally useful measure of parsimony either [215]. The reason for this is that,

3More precisely, we have to count the minimal number of unconstrained parameters that can be invertably transformed to
the space of the original model parameters. For example, a simplex parameter vector of length K is equivalent to only K − 1
unconstrained parameters because the K-th one is determined by the sum-to-one constraint.

30

01020300246810xy01020300246810xy01020300246810xyfrom a Bayesian perspective, any prior information on a parameter increases a P model’s parsimony, such
that the eﬀective number of parameters (ENP), might be substantially smaller than the nominal number of
parameters [267]. The same mechanism also underlies the diﬃculty in computing the DoF of test statistics
in frequentist multilevel models, because random eﬀects distributions are equivalent to priors [126].

There are several ENP measures in the literature [249, 278, 267, 216], often deﬁned in the context of
information criteria. For the information criterion based on leave-one-out cross-validation (LOO-CV), ENP
is measured as the sum of the diﬀerences between the pointwise log predictive densities of the full posterior
and the pointwise log predictive densities of the LOO posteriors [267]:

ENPLOO =

=

N
(cid:88)

n=1

N
(cid:88)

n=1

(log p(yn|y) − log p(yn|y−n))

(cid:90)

(cid:18)

log

p(yn|θ) p(θ|y) dθ − log

(cid:90)

p(yn|θ) p(θ|y−n) dθ

.

(37)

(cid:19)

Bayesian LOO-CV can usually be computed eﬃciently via importance sampling without any model reﬁtting,
and so can ENPLOO be computed without any actual reﬁtting [267, 270]. For a large number of observations
N , ENPLOO can be asymptotically approximated by the sum of the full posterior variances over the pointwise
log likelihood values, which is the ENP estimate used in the widely applicable information criterion (WAIC)
[278]:

ENPLOO ≈ ENPWAIC =

N
(cid:88)

n=1

Varθ|y (log p(yn|θ))

(38)

The WAIC approximation of LOO-CV performance can be quite unreliable and so using ENPLOO is highly
recommended whenever possible [267]. When using very wide or even completely ﬂat priors on all parameters
and given enough data, these ENP measures will roughly coincide with the nominal number of parameters,
but are smaller than the latter in the presence of prior information, thus justifying their interpretation as
eﬀective number of parameters. What becomes apparent in these equations is that parsimony, at least when
measured through these ENPs, may depend on the speciﬁcally realized data ˜y, and as such needs to be
deﬁned over PD models. This is speciﬁcally true for models with hierarchical priors, where the amount of
hierarchical shrinkage (i.e., the inﬂuence of the hierarchical priors) is data-dependent [98]. Practically, the
posterior integrals in (37) and (38) for PAD models are eﬃciently approximated via Monte Carlo estimates
based on posterior draws from an approximator [267].

The huge advantage of these ENP measures is that they do not need to be aware of the internal structure
of a P model, but only require its predictive outputs in the form of pointwise log-likelihood values. However,
the need for the latter has the drawback that ENP measures do not work natively with PI models due
to their lack of tractable likelihoods; unless one has learned not only the model’s posterior but also its
likelihood density during training [283]. What is more, if the model includes residual dependencies between
observations, the pointwise (log-)likelihood may not be available, even if the joint likelihood is analytic [35].

Prior P-Parsimony.
In the above described ENP deﬁnitions, we integrate over the posterior distribution
and so, in this sense, measure posterior parsimony. This naturally raises the question of whether we can deﬁne
measures of prior parsimony as well. In a Bayesian setting, prior parsimony is automatically embodied in
the marginal likelihood (sometimes called Bayesian evidence) [145, 169, 163], which we already encountered
in our discussion on prior predictive performance (see Section 3.3.2). As a reminder, we obtain the marginal
likelihood by marginalizing the joint P model over its prior

p(y) = Ep(θ) [p(y | θ)] =

(cid:90)

p(y | θ) p(θ) dθ.

(39)

Accordingly, we can interpret the marginal likelihood as the expected probability of generating data y from
a P model when we randomly sample from the prior p(θ). Through the prior’s role as a weight on the

31

Figure 10: Hypothetical scenario with three P models of descending complexity: P1, P2, and P3. The most
complex model P1 can account for the broadest range of observations at the cost of diminished sharpness
of its marginal likelihood; in contrast, the simplest model P3 has the sharpest marginal likelihood which
concentrates onto a narrow range of possible data. Even though the observed data ˜y is well within the
generative scopes of models P1 and P2 too, the simplest model P3 has the highest marginal likelihood at ˜y
among the three candidates and is therefore favored from a marginal likelihood perspective. However, the
higher relative marginal likelihood of the simplest model P3 is a poor proxy of its predictive performance for
new data sets, as it assigns close to 0 density to the new data set ˜ynew, suggestive of overﬁtting. The model
P2, whose marginal likelihood is closest to the data-generating distribution p∗, would have been favored, had
˜ynew instead of ˜y been used for computing the associated Bayes factors.

likelihood, the marginal likelihood encodes a probabilistic version of Occam’s razor by penalizing the prior
complexity of a P model [145, 169].

However, the marginal likelihood is not an explicit measure of parsimony, but rather an implicit and a
relative quantity which combines prior parsimony with the ability of a P model to ﬁt the data by considering
its entire generative scope (see Fig. 10). Following [169, Chapter 28], we can illustrate the above conﬂation
by assuming that the posterior of a P(A)D model is well represented by a (multivariate) Gaussian. In this
case, the marginal likelihood can be approximated as:

p(y) ≈ p(y | θMP) × p(θMP) det(H(θMP)/2π)− 1
2 ,

(40)

where θMP is the posterior mode and H(θMP) is the Hessian of the likelihood evaluated at θMP. The
multiplicand p(θMP) det(H(θMP)/2π)− 1
2 is termed an Occam factor and represents the factor by which a
P(A)D model’s parameter space contracts as the prior is updated to the posterior based on the information
contained in D. Thus, under the Gaussian assumption, the magnitude of the Occam factor is an explicit
measure of prior complexity (i.e., inverse prior parsimony) related to the information gain a P model can
achieve over its generative scope [169, 163]. Consequently, a P model with a vague prior will incur a larger
penalty by the Occam factor than a P model with a sharper prior, provided that both models share the
same likelihood. However, if the Gaussian assumption is inadequate, the approximation of Equation (40)
can sustain a large error and may no longer be useful. Unfortunately, we are not aware of a more general
decomposition of the marginal likelihood into a prediction factor and a parsimony factor, as is the case with
ENPLOO.

A closely related concept is the principle of Minimum Description Length [MDL, 229, 121], which views
parsimony through the lens of information theory. In the MDL framework, a probabilistic model represents
a coding scheme designed to describe the data ˜y. Accordingly, a parsimonious P model provides a concise
description of the data in terms of code length (relative to a competing P model). Note, that MDL is not a
unique measure, but rather an umbrella framework for deriving measures of parsimony/complexity in various

32

application contexts (see [121] for a comprehensive exposition). For instance, in a Bayesian context, one can
show [121] that a canonical measure of description length for model P is given by

DL(P) = − log

(cid:90)

p(y | θ) p(θ) dθ,

(41)

which we recognize as the negative logarithm of the marginal likelihood (Equation (39)). In this way, MDL
not only highlights the theoretical connection between Bayesian model comparison and information theory,
but also provides a principled way for deriving new measures of prior parsimony in future basic research.

3.6.2. A-Parsimony

As we discussed in Section 2.3 concerning PA models, posterior approximators can range from relatively sim-
ple optimization algorithms to high-dimensional parametric models (e.g., neural networks) which themselves
can be viewed as standalone P models (e.g., Bayesian neural networks). The notion of A-parsimony intends
to capture our intuition that these diﬀerent approximators have varying degrees of complexity. Here, we
propose a very straightforward deﬁnition of A-Parsimony: The cardinality of the hyperparameter space H
available for ﬁne-tuning through the implementation interface I of the underlying mathematical algorithm
A. For instance, the widespread use of MCMC in Bayesian inference is partly owing to the fact that proba-
bilistic programming languages (PPLs) provide relatively simple interfaces, which abstract away a staggering
multitude of hyperparameters of complex MCMC samplers (e.g., NUTS, [127]). On the other hand, neural
approximators [e.g., 221, 115] inherit the vast hyperparameter spaces of deep neural networks and are thus
currently still rather challenging to apply or ﬁne-tune [271].

A-parsimony is not only relevant for the usability of approximators, but also plays an important and limit-
ing role in comparison or benchmarking studies assessing the relative performance of diﬀerent approximators.
Suppose we wish to compare approximator A1 having no hyperparameters with approximator A2 having a
single continuous hyperparameter h ∈ [0, 1], in the context of some P model. A comparison of approximators
must naturally be based on some metric (or a set of metrics) q(A, P) which quantiﬁes the approximation
quality of A with respect to a given P model (e.g., the distance between corresponding PD and PAD models
or the estimation speed of the approximator). However, even for the simple scenario outlined above, it is
not clear how to systematically carry out such a comparison due to the presence of hyperparameters. One
approach would be to approximate the average approximation quality given by (cid:82) 1
0 q(A2(h), P) p(h)dh of A2
and compare it to q(A1, P). Another approach would be to seek the best approximation quality given by
maxh∈[0,1] q(A2(h), P) and compare it to that of q(A1, P). Needless to say, the diﬃculty of ranking and bench-
marking approximators with large hyperparameter spaces drastically increases, which makes A-parsimony a
key limiting factor as well as a desirable utility to improve upon.

Finally, A-parsimony is related to robustness (see Section 3.10) and convergence (see Section 3.8), as the
presence of multiple hyperparameters raises the question of how to choose hyperparameter settings which i)
lead to stable results and ii) generalize to various model applications. For some approximator classes (most
notably, MCMC) and P model applications, empirical guidelines and theoretical considerations may suggest
relatively robust default choices. For newer approximator classes (e.g., neural density estimators) or more
exotic applications, some form of sensitivity analysis or hyperparameter search might be necessary to ensure
suﬃcient robustness or generalizeability.

3.7.

Interpretability

Interpretability of a P(A)(D) model can be qualitatively deﬁned as “the degree to which a human can
understand the cause of a [model-based] decision” [184] or as “the degree to which a human can consistently
predict the model’s result” [146]. A more precise, perhaps even mathematical, deﬁnition is diﬃcult to provide
given the context and expertise-dependent nature of interpretability, but there is progress in this direction
[65]. In any case, achieving interpretability will help us understand why a P(A)(D) model behaves the way
it does (e.g., in terms of predictive performance; see Section 3.3). Such understanding can have not only
profound epistemological, but also far-reaching ethical and social implications [208, 65, 186].

33

According to [186], we can distinguish between intrinsic and post-hoc interpretability. The former is
related to the intelligibility of the P(A)(D) model itself (i.e., its structure and parameters), whereas the
latter is related to the explainability of the PAD model’s results using auxiliary methods, such as permuta-
tion feature importance for neural networks [286] or random forests [137]. However, there is a conceptual
ambiguity regarding the term in the recent literature. Some accounts use explainability as a synonym for
interpretability in general [186], while others use explainability to refer solely to post-hoc interpretability
[34]. In our PAD model taxonomy, we view only intrinsic interpretability as a utility of the P(A)(D) model.
Diﬀerently, post-hoc interpretability is a utility of an explanator that is applied to the original PAD model’s
results – in fact, the explanator may just be another, more interpretable P(A)(D) model that is used as a
surrogate [34]. Accordingly, the following discussion focuses only on intrinsic interpretability, to which we
hitherto refer simply as interpretability.

P model interpretability relates to the general meaning of its parameters, so it makes sense to diﬀer-
entiate between the interpretability of PI and PE models, since the two model classes often put diﬀerent
demands on the epistemic value of their parameters. Further, as we will see below, there are P models
whose interpretability can be inﬂuenced by both data D and approximator A. As such, it can be necessary
to further distinguish the interpretability of P, PD, and PAD models.

3.7.1.

Interpretability of PI models

In PI models, most parameters correspond to real-world quantities or emergent properties, whose meaning
can be understood independently of the PI model that is used to estimate them (see Section 2.1). For
example, in a harmonic oscillator [168], the object’s mass that serves as a parameter carries a meaning
independently of the diﬀerential equation that describes the oscillator’s behavior. As such, while the trans-
formations performed to generate data from a PI model are highly non-linear and often not analytically
tractable [56], the interpretability of PI models tends to be high (at least in the eyes of domain experts in
the ﬁeld).

However, even for domain experts, it can be exceptionally challenging to predict the generative behavior
of a high-dimensional PI model given a particular parameter conﬁguration – thus, simulations are typically
employed to aid in understanding [167]. Moreover, the number of parameters of a PI model can easily be
mistaken for simplicity (see also Section 3.6). Consider, for instance, the prototypical logistic map equation
[176] given by

yt+1 = ρ yt (1 − yt)

(42)

and having only a single parameter ρ ∈ [0, 4] which can be interpreted as growth rate in population dynamics
modeling [254]. Despite its beguilingly simple form, the logistic map is known to develop chaotic behavior as
the parameter ρ varies in the range from approximately ρ ≈ 3.56995 to ρ ≈ 3.82843. The model’s generative
behavior in this range is characterized by a periodic phase, intercepted by bursts of aperiodic ﬂuctuations.
And even though such behavior can be generally abstracted and described for a single parameter, for instance,
with the help of bifurcation diagrams [108], it can quickly become less amenable to high-level description
when it results from the interaction of two or more parameters [7]. Unsurprisingly, Bayesian analysis of PD
or PAD models based on an underlying chaotic PI model has long been recognized as a challenging endeavor
[17], requiring sophisticated approximators with surrogate likelihoods [251].

As alluded to above, the interpretability of high-dimensional PI models will often depend on whether
we focus on individual parameters and their functional role for data generation in isolation (i.e., ﬁrst-order
interpretability) or try to understand interactions between parameters as well as their joint contribution to the
generation of y (i.e., higher-order interpretability). Accordingly, even for complex PI models with dozens of
parameters, we may still retain relatively high ﬁrst-order interpretability through the theoretical embedding
of each individual parameter, but higher-order interpretability may suﬀer, since multiple parameters can act
similarly on y and interact in surprising ways due to non-linearity. For instance, the compartmental model of
the early COVID-19 pandemics in Germany set up by [223] has 34 free parameters, each of which has a direct
isolated interpretation, for instance, infection rate, number of initially exposed people, weekly modulation, or
probability of detection. However, the exact interplay between these parameters in determining the actual

34

reported number of daily cases might not be immediately obvious from the understanding of individual
parameters alone or from the model equations themselves.

Finally, the higher-order interpretability of PI models may change once they have been connected to
data due to dependencies between parameters. Oftentimes, we choose a prior p(θ) which factorizes into
independent components, reﬂecting our assumption of disentanglement or independent generative factors
of variation. However, the resulting PD or PAD models will rarely conserve independence in their joint
posteriors (e.g., due to loss of information or an inherent lack of disentanglement in the inverse model).
A canonical example would be a strong posterior correlation between two parameters with an initially
independent priors, indicating that the parameters do not fulﬁll orthogonal functional roles for generating
the data.

3.7.2.

Interpretability of PE models

In PE models, the parameters do not need to correspond to real world quantities or mechanisms. Rather,
their meaning can often only be understood within the PE model they are part of [101]. The archetypal PE
model is linear regression, where a regression coeﬃcient β describes the linear relationship between a predictor
variable and the response whilst holding all other predictors constant. As such, β has a clear meaning to
an analyst with some statistical knowledge, provided that the measurement scales of predictor and response
variables make sense for the task at hand. However, the requirement to hold all other predictors constant
becomes impossible to fulﬁll if the predictors cannot be varied independently from each other, for example,
because they are correlated in purely observational data or because some of them constitute interactions
between already included predictors. As such, even for as few as four or ﬁve predictors, interpretability of
their joint contribution becomes highly challenging unless predictors are mutually independent [186].

The use of non-linear, monotonic transformations in PE models, such as link functions in generalized
linear models [192] or non-linear activation functions in neural networks [245] further complicates the in-
terpretability of an originally linear predictor structure. For example, when using the logarithmic link
(equivalently, the exponential response/activation function), the originally additive relationships become
multiplicative, resulting in exponential growth, which is much harder to comprehend for humans [274]. This
then reduces the interpretability of the PE model’s parameters from both their signs and magnitudes to
only their signs. If one were to apply non-monotonic transformations, the interpretability of the parameters’
signs would be lost as well. In addition to non-linear transformations of the whole additive predictor term,
every structural deviation from a (latent) linear structure further reduces interpretability. For example,
interactions, polynomial terms, hierarchical structure [98], Gaussian processes [280, 227], or splines [88, 284]
all make interpretation of a PE model’s parameters harder, if not impossible in some cases.

The interpretability of a PE model may also be aﬀected by the data utilized for parameter estimation.
Accordingly, PD models may diﬀer in their interpretability even if their underlying PE model is the same.
For example, when employing shrinkage priors for high-dimensional linear PE models with lots of irrelevant
predictors, the posterior of most regression coeﬃcients will shrink to values very close to zero, eﬀectively
eliminating the corresponding predictors from the regression equation [216, 294]. If only a few coeﬃcients
are substantially diﬀerent from zero, the interpretability of the resulting PD model would be much higher
than that of the original PE model.

Finally, an approximator A may create a situation where a PA(D) model’s interpretability deviates from
that of the underlying P(D) model. However, that may only happen if the posterior approximation pA(θ | y)
is qualitatively diﬀerent from its analytic counterpart p(θ | y) due to an incomplete posterior exploration.
A common case arises when the analytic posterior is multi-modal but the approximator collapses to a
single mode [94]. Notably, mode collapse represents a case where the interpretability of the PAD model
may be higher than that of the underlying PD model, at the cost of other utilities, such as predictive
performance (see Section 3.3). An example for a PE model class that produces highly multi-modal posteriors
are artiﬁcial neural networks [94, 69, 136]. While the interpretability of the underlying PE model is usually
low [34, 292, 293], some of their PAD models can exhibit much higher interpretability if they are steered in
the right direction [292, 293].

35

Figure 11: Graphical convergence checks of two parameters µ and σ. Left: Traditional trace plots. Right:
ECDF diﬀerence plots with 99%-conﬁdence envelopes [256]. Both kinds of plots use the same posterior draws,
but only the rightmost ECDF diﬀerence plot highlights that Chains 2 and 4 have some mixing problems for
σ. Example draws obtained from the bayesplot R package [92].

3.8. Convergence

Convergence is a utility pertaining to PA and PAD models which rely on complex approximators, such as
MCMC, VI, or neural density estimators. As explained in Section 2.3, approximators make certain promises
and can provide certain guarantees under speciﬁc assumptions, such as inﬁnite draws in the case of MCMC
[100] or inﬁnite training in the case of neural density estimators [221, 222, 242].
In practice, however,
modelers cannot wait a lifetime of inﬁnity for approximators’ promises to come true, which creates the need
for a measure of how close the current approximation is to the optimal approximator outcome. We call
such measures convergence diagnostics and they are indispensable for ascertaining the validity of PA(D)
models. Useful convergence metrics should indicate that the approximation is also closer to the analytic
posterior, but only within the space of distributions the approximator can realize. Accordingly, the relation
between convergence and analytic posterior approximation is only indirect for approximators that may be
asymptotically biased [287, 63].

3.8.1. Convergence of Markov Chain Monte Carlo

Convergence metrics are fundamentally important for posterior approximators that rely on MCMC, since
these approximators can be arbitrarily bad before full convergence [155]. Thus, all model-based inference
relies on the quality of the approximation being close enough to the analytic posterior with respect to some
minimally required precision. For a quick graphical check, trace plots or ECDF diﬀerence plots can be used
[256] as is illustrated in Figure 11.

In terms of numerical approaches, three related classes of MCMC convergence diagnostics are applied
in today’s practice, namely scale reduction factor (cid:98)R, eﬀective sample size (ESS) and Monte Carlo standard
error (MCSE) [99, 55, 230, 82, 100, 66, 269]. They all provide convergence measures for univariate quantities
of interest ψ = ψ(θ) that are functions of the P model’s parameters θ (see also Section 3.2). There is not a
single “global” (cid:98)R, ESS, or MCSE measure for ψ, but one for each summary statistic T (ψ) of ψ, where T can
be any posterior expectation or quantile [269]. As such, for example, a set of S posterior draws ψ(s) might
yield a very precise estimate for the posterior mean of ψ, while at the same time, the estimates of some
tail quantiles of ψ (e.g., 5% and 95% quantiles) have much less precision [269]. Accordingly, each of these
convergence measures is a function of the quantity of interest ψ and the summary statistic T , computed
from the S posterior draws ψ(s).

Broadly speaking, the scale reduction factor (cid:98)R compares the between-chain variance B = B(fT (ψ)) to

36

−0.75−0.50−0.250.00musigma05010015020025005010015020025017181920−0.10−0.050.000.050.10musigma0.00.20.40.60.81.00.00.20.40.60.81.0−0.10−0.050.000.050.10chain1234the within-chain variance W = W (fT (ψ)):

(cid:115)

(cid:98)RT (ψ) :=

B(fT (ψ)) + W (fT (ψ))
W (fT (ψ))

,

(43)

where the dependence of B and W on T is realized by an appropriate transformation fT (ψ) that is applied
to each posterior draw ψ(s) before the variances are computed, usually on split chains [100, 269, 44]. We
can conclude that convergence has been reached if (cid:98)R ≈ 1, that is, if the within-chain variance dominates the
between-chain variance.

The ESS estimates the number of independent draws that contain the same amount of information about
T (ψ) as the S dependent posterior draws obtained via an MCMC approximator. As a result, we usually
see ESS < S, although the opposite can also happen in case of antithetic (negatively auto-correlated) chains
[269]. We can obtain the ESS from all autocorrelations ρt = ρt(fT (ψ)) of lag t of the chains as

ESST (ψ) :=

S

1 + 2 (cid:80)∞

t=1 ρt(fT (ψ))

,

(44)

where, in practice, we would truncate the inﬁnite sum at some ﬁnite value [107]. In modern versions of ESS,
ρt implicitly depends also on (cid:98)R to take variation across chains into account [100, 269]. In case of independent
draws, we have pt = 0 such that ESS = S.

The MCSE describes how much (reducible) uncertainty in T (ψ) remains due to the fact that we only
have a ﬁnite set of dependent MCMC draws for estimation [82, 66, 100, 269]. If T represents an expectation,
we can write down the corresponding MCSE schematically as an overall variance V = V (fT (ψ)) across the
S draws divided by the corresponding ESS [82, 269]:

MCSET (ψ) :=

(cid:115)

V (fT (ψ))
ESST (ψ)

.

(45)

MCSE estimates for quantiles need to be computed a little diﬀerently and are provided in [269].

Ideally, we should deﬁne convergence of MCMC as reaching or undercutting the maximal MCSE that we
ﬁnd minimally acceptable for the given summary of interest T (ψ). However, The MCSE is scale dependent
as it has the same scale as T (ψ), which requires an understanding of how much of an error is acceptable for
a certain quantity, in the context of a particular model and research question. This inherently makes MCSE
harder to use in practice and hence the scale-free alternatives (cid:98)R and ESS are often preferred [269].

All of the above measures are univariate in the sense that they only concern a univariate T applied to a
univariate ψ. Recently, a more comprehensive measure, called R∗ [155], has been developed that measures
convergence in a multivariate way across multiple model parameters or quantities of interest.
It is able
to detect non-convergence in the joint posterior that may be overlooked by only investigating convergence
of a small, non-exhaustive set of univariate quantities [155]. This is achieved by training an expressive
machine learning model (i.e., random forest) to predict chain indices from posterior draws. If the predictive
performance of the machine learning model on (unseen) test draws does not exceed chance level, we can
assume that the MCMC chains have converged.

In addition to all these sampler-agnostic convergence metrics, there are also few sampler-speciﬁc metrics.
Most notably, this concerns divergent transitions in Hamiltonian Monte-Carlo (HMC) [21], where every
occurring divergent transition in the Markov chain may bias the MCMC results and indicate diﬃculties
of the sampler with exploring the target posterior. Divergent transitions tend to occur in regions of high
curvature of the explored posterior; regions that most other MCMC samplers struggle to explore as well,
only that they fail more silently compared to HMC [21].

3.8.2. Convergence of Optimization-Based Algorithms

Many classes of posterior approximators are based on optimization algorithms. The simplest of such approx-
imators aim to ﬁnd a single point estimate to approximate the analytic posterior, namely the posterior mode,

37

also known as maximum a posteriori (MAP) estimate [100, 169]. Variational inference (VI) approximators
also use optimization, but instead of ﬁnding the MAP, they aim to ﬁnd a parametric distribution (e.g., a
multivariate Gaussian) that approximates the analytic posterior as closely as possible [84, 226, 26, 279].
The optimization then targets the parameters of this parametric distribution (e.g., the means and stan-
dard deviations in Gaussian mean-ﬁeld VI). Expectation propagation (EP) [198, 185, 268] and integrated
Laplace approximation (INLA) [235, 160, 236] work in a conceptually similar fashion, but the structure of
their parametric approximators and their target distributions are diﬀerent (e.g., for INLA, the conditional
posteriors of the parameters, instead of their joint posterior). Again, highly similar in terms of their use of
optimization, neural density estimators (e.g., invertible neural networks; [2, 221]), use optimization to ﬁnd
the neural network parameters that yield the best posterior approximation within the generative scope of
the network [205, 165, 115, 207, 221, 242] (but see Section 3.8.5 for speciﬁcs in diagnosing convergence of
amortized neural density estimators).

Regardless of how optimization is applied for posterior approximation, the aim is always to ﬁnd a single
point in a potentially high dimensional space that leads to the best approximation of the analytic posterior
within the set of realizable approximations. Accordingly, all traditional convergence criteria for iterative
point optimization apply. That is, for non-stochastic optimization algorithms (e.g., gradient-decent or L-
BFGS; [194]), small absolute or relative changes in the point estimate, small absolute or relative changes in
the target function, or small absolute or relative closeness of the target function’s gradient to zero (if the
gradient is available) [194, 252], would indicate convergence. For stochastic optimization algorithms (e.g.,
stochastic gradient-decent or more sophisticated versions, such as Adam; [194, 148]), measuring convergence
becomes less straightforward due to the stochasticity in the objective’s trajectories. If the step size is held
constant, they yield a Markov chain around the target point, once the algorithm comes close enough, instead
of converging directly to the target [224, 75]. The latter implies that MCMC convergence diagnostics, in
particular (cid:98)R, can be applied to diagnose convergence of stochastic optimization algorithms [63].

3.8.3. Convergence of Sequential Monte Carlo

Sequential Monte Carlo (SMC; aka particle ﬁltering) comprises a heterogeneous class of posterior approxi-
mators for PD models whose underlying P models can be expressed in the form of a sequence of conditional
distributions (i.e., time-series P models) [67, 61]. Most SMC samplers can be shown to provide asymptotically
correct inference as the number of draws (particles) approaches inﬁnity [58]. However, empirical convergence
diagnostics in the pre-asymptotic regime appear to be relatively scarce still [57, 157, 58]. Perhaps this is
because SMC approximators consist of multiple iteratively applied components [58], each with their own
pre-asymptotic behavior requiring their own local convergence diagnostics: To assess the convergence of
importance sampled (IS) particles at a given step, ESS estimates for weighted samples [152, 295] or variance
measures driven by the number of siblings per particle (i.e., the number of particles with the same ancestor
node at step zero) [157] can be applied. The trustworthiness of the IS weights themselves could be diagnosed
via the Pareto-k-diagnostic of Pareto-smoothed importance sampling (PSIS; [270, 43]), although we are not
aware this has actually been tried so far in the context of SMC (for a closely related application, see [43]).
Convergence of MCMC kernels that are part of many SMC algorithms [58] could be assessed via MCMC
convergence diagnostics (see Section 3.8.1). While each of these diagnostics may be locally informative for a
given SMC component at a given step, whether and how they convey global convergence to the target joint
posterior remains to be studied further.

3.8.4. Convergence of Approximate Bayesian Computation

The standard ABC rejection algorithm [233, 64, 258, 218] requires a distance function which quantiﬁes the
diﬀerence between simulated data y (generated from a P model with a particular parameter conﬁguration θ)
and actually observed data ˜y. Further, it needs a tunable tolerance level (cid:15) according to which the algorithm
reject a fraction of 1 − (cid:15) simulated parameter values. The algorithm then keeps the remaining parameter
values as random draws from an approximate posterior p(cid:15)(θ | y).

38

Typical generative set T (P)

Latent generative space

y ∼ p(y)

H
Summary
Network

H(y)

F
Inference
Network

Correct
Posterior

Incorrect
Posterior

P Model

H(˜y)

Simulation gap

˜y ∼ p∗(y)

Simulation gap detected

Figure 12: Detecting model misspeciﬁcation (i.e., simulation gaps) with amortized neural approximators
[242]. A summary (aka embedding) network H transforms the typical generative set T (P) of a P model (i.e.,
the ﬁnite set of “in-distribution” data simulations that a P model typically generates) into the typical set of a
simple distribution (e.g., multivariate Gaussian). Discrepancies between the model-implied data distribution
p(y) and the true data distribution p∗(y) (i.e., simulation gaps) manifest themselves as detectable anomalies,
causing potential posterior errors by the inference network F. We can detect these anomalies via distribution
matching (e.g., maximum mean discrepancy).

The ESS of standard ABC rejection samplers is thus typically equal to (1 − (cid:15)) S, with S denoting the
total simulation budget, since vanilla ABC samplers perform independent sampling. However, this does not
mean that their sampling eﬃciency is particularly appealing, especially for high dimensional P models. That
is because ABC samplers notoriously suﬀer from the curse of dimensionality: Most simulated data sets from
a high-dimensional P model will be rejected and so it becomes challenging to obtain enough random draws
from p(cid:15)(θ | y) for a reasonable reduction of the MCSE.

More sophisticated ABC algorithms, such as ABC-SMC [247, 150] or ABC-MCMC [172, 261] alleviate
some of these issues and inherit the convergence diagnostics of SMC and MCMC. However, whenever hand-
crafting of distance functions and summary statistics of the data H(˜y) (i.e., dimensionality reduction) is
involved, ABC algorithms can converge at best to pA(θ | H(˜y)). This issue does not exist whenever H(˜y) is
a suﬃcient summary statistic, but it can potentially lead to overestimation of V (fT (ψ)) if H(˜y) results in
considerable loss of information about the parameters θ.

Recent work on ABC focuses on building robust ABC approximators and explore the possibility of
utilizing hand-crafted summary statistics as a key element of misspeciﬁcation analysis and error correction
[85, 174]. A related line of work suggests comparing posterior moments recovered by diﬀerently conﬁgured
ABC approximators as an empirical diagnostic [86]. It remains an interesting open question whether similar
ideas can be generalized to other approximators for simulation-based inference, such as amortized neural
surrogates [221, 115, 206] or ABC with learned summary statistics [50, 139].

3.8.5. Convergence of Amortized Approximators

In contrast to MCMC-based approximators, there are no standard convergence diagnostics for amortized
approximators yet, as the latter are grounded in diverse, and still fast evolving theoretical frameworks.
However, whenever we employ neural density approximators, we can resort to convergence checks used
commonly in deep learning applications ([113]; see also Section 3.8.2).

Since most modern neural architectures are trained using some form of stochastic gradient-based opti-
mization, optional stopping (i.e., a discontinuation of training once the cost function does not improve over
some tolerance period) and other convergence heuristics can be used to determine when a neural approxi-
mator has reached a stable local minimum of its cost function. That said, due to the nature of non-convex
optimization, simply assessing local convergence is not enough for trusting PA(D) models coupled with

39

amortized neural approximators.

Moreover, amortized neural approximators require simulations to be faithful proxies of reality and might
yield arbitrarily bad posterior approximations when confronted with data that are atypical under the assumed
P model [242]. The latter case is also known as a simulation gap and it occurs when a P model does not
accurately represent the behavior of the modeled real-world system (i.e., when the model is misspeciﬁed).
Consequently, amortized approximators must be able to detect simulation gaps and potential posterior errors,
so that they can warn users about suspicious input data and resulting inference.

Generally, there are two broad types of empirical convergence diagnostics we need to utilize in the context
of amortized neural approximators: those pertaining to a PA model and those pertaining to a PAD model.
Model-agnostic tools, such as diﬀerent variants of SBC (see Section 3.2.3), are only applicable to PA models,
as they assume that we have access to the actual data-generating parameters. On the other hand, if the
neural approximator is a generative neural network [220, 115], the latent space can be used as a source
of convergence information for the PA model. For instance, ﬂow-based networks [149, 207] are trained to
transform an intractable posterior into a simple base distribution from which random draws can be easily
obtained. Thus, convergence of the PA model can generally be determined by a divergence between the
prescribed and the learned base distribution.

Unfortunately, neither of the above PA diagnostics can tell us whether the corresponding PAD model
will be able to yield faithful estimation due to potential discrepancies between the simulation model and
reality (see above). Thus, further diagnostics are necessary to promote the trustworthiness of amortized
posteriors. One such diagnostic is the maximum mean discrepancy (MMD, [116]) between summary statistics
of simulated and real data which tells us whether the observed data belongs to the typical generative set of
the simulator or not [242, cf. Figure 12]. As the ﬁeld of simulation-based inference is still in its infancy, we
expect a rapid development of convergence diagnostics for amortized approximators in the future.

3.9. Estimation Speed

For non-amortized approximators (cf. Figure 3), we can deﬁne Estimation Speed as the time from the start
of running the approximator A of a PAD model (or a particular instance of a PA model) until convergence,
deﬁned by approximator-speciﬁc convergence diagnostics (see Section 3.8). In certain cases (see Section 4), it
may be sensible to deﬁne estimation speed less strictly as time until termination of the approximator run after
which useful results are obtained, without necessarily having achieved convergence. For a given PAD model,
estimation speed tends to vary by several orders of magnitude across diﬀerent classes of approximators.
For example, MAP estimators, VI, or other (non-amortized) optimization-based approximators, will usually
require a fraction of what sampling-based approximators such as MCMC or SMC need [235, 36]. When
considering estimation speed in isolation, there is no doubt that “faster is better”. However, in order to
obtain faster approximators, we often have to give up accuracy or asymptotic guarantees of the resulting
posterior approximation [84, 287]. Thus, increasing speed by changing the approximator class may have
an adverse aﬀect on other utilities of PA(D) models, speciﬁcally on parameter recoverability and predictive
performance (see Sections 3.2 and 3.3).

Within a given class of approximators, hyperparameter choices can greatly aﬀect the estimation speed
as well [127, 287, 221]. As an example, consider static HMC where the number of leapfrog steps per Markov
transition has to be chosen a priori [127, 21]. On the one hand, if the number of leapfrog steps is too small,
MCMC draws will be highly auto-correlated and thus more draws are required to achieve convergence. On
the other hand, if the number of leapfrog steps is too large, a lot of computation time is wasted by unnecessary
leapfrog steps; or auto-correlation might even get worse again, when the HMC sampler eventually makes a
so-called “U-turn” to come back to its starting point [127]. Such problems due to hyperparameter choice
can be mitigated by automatically tuning hyperparameters in a “warm-up” phase or adapting them on the
ﬂy, conditional on the local geometry of the approximated analytic posterior. For example, when using the
No-U-turn sampler (NUTS; [127]), a generalization of HMC, the number of leapfrog steps is adaptive. It
both removes the requirement for the user to choose this hyperparameter manually and may even have better
estimation speed than optimally tuned, static HMC [127].

40

The above deﬁnition of estimation speed is straightforward, but can be misleading in practice, if con-
vergence is not achieved (or unachievable) within a given run of A until its termination, such that A has
to be restarted [103]. A typical reason is sub-optimal choices of A’s hyperparameters, for example, if the
leapfrog step size in HMC is too large leading to divergent transitions whose occurrence implies irrecoverable
non-convergence for the current approximator run [21]. Thus, estimation speed in practice may be strongly
aﬀected by an approximator’s ability to run reliably out of the box without much tuning. Tuning demand
can be reduced by adapting hyperparameters automatically on the ﬂy or by having only a small number of
sensitive hyperparameters (see Section 3.10). The latter property can further be understood as determining
approximator parsimony (see Section 3.6.2).

A manually but skilfully tuned approximator A1 might beat an auto-tuned approximator A2 in terms of
estimation speed when considering only the ﬁnal, converging run. However, the overall time (including failed
runs) it can have take to get A1 to this optimal state may easily more than oﬀset its ﬁnal speed advantage.
As a result, in an honest comparison of practical estimation time, it may actually be A2 that comes out
ahead by a substantial margin. Along similar lines, the particular P model implementation may be more or
less favourable for diﬀerent approximators, which can also strongly aﬀect estimation times [21, 269, 13, 60].
Sometimes, the reason for an approximator’s termination without convergence may also lie in the com-
putational environment, for example, time or memory constraints on a computing cluster that limit the
resources available for a single job. For example, if the estimation of a PA(D) model takes longer than
expected, estimation might be terminated prematurely, in the worst case leaving no intermediate result to
restart from. In this scenario, even if the second run would then be successful, we still had to deal with at
least a doubled estimation time. Accordingly, both the predictability of the expected resource requirements
and small variance in resources between repeated runs of the same PA(D) model can imply substantial
practical speed improvements.

3.9.1. Sampling Eﬃciency

For sampling-based approximators, convergence in terms of reaching a given MCSE value (for given quan-
tities of interest and summary statistics), is strongly application-dependent, and so is the estimation speed
associated with it (see also Section 3.8.1). For more general comparisons of sampling-based approximators,
the concept of sampling eﬃciency is easier to handle and we deﬁne it as the average ESS per unit time (for
a given quantity of interest ψ and summary statistic T ):

EﬀT (ψ) =

ESST (ψ)
tend − tstart

,

(46)

where tstart and tend are the start and end times of the approximator run, respectively. While most approx-
imators, even optimization-based ones, can be used to obtain posterior draws upon convergence [235, 84],
we restrict the class of sampling-based approximators to those that return only posterior draws as their im-
mediate endpoint instead of the parameters of (closed-form) density functions. MCMC, SMC, and rejection
sampling are the most important members of this class [233, 100, 58].

Within a class of sampling-based approximators, say MCMC, the same convergence diagnostics, in par-
ticular the same ESS, can be applied to all competitors, which simpliﬁes comparisons [13, 60]. Here it is
important to not only use the same implementation for these diagnostics across all approximators, but also to
ensure that this implementation follows the current state-of-the-art of diagnostic development [265]. Other-
wise, comparisons may be biased by outdated diagnostics. Additionally, one needs to verify empirically that
the obtained stationary distribution for a given PAD model is the same for all the compared approximators.
Otherwise sampling eﬃciency will be misleading, since at least one approximator would not have estimated
the analytic posterior of the underlying PD model well enough. Comparisons between approximators be-
longing to diﬀerent sampling-based classes may require even more care to ensure that ESS diagnostics across
classes are comparable, for example, when comparing MCMC with SMC approximators.

Whenever we are performing sampling eﬃciency comparisons for PA instead of PAD models, we not
only have, in principle, an inﬁnite number of data sets as a basis for comparison, but can utilize SBC to
falsify correctness of the achieved stationary distributions (see Section 3.2.3). However, when we investigate

41

sampling eﬃciency on a set of simulated data sets, diﬀerent data-generation scenarios should be studied,
since well-speciﬁed P models may have diﬀerent eﬃciency than misspeciﬁed P models.

When studying sampling eﬃciency, the same practical caveats apply as for estimation speed in gen-
eral. For instance, in order to achieve a comparison that is ecologically valid for real-world situations, we
have to investigate practical sampling eﬃciency that considers both optimized and non-optimized P model
implementations, as well as failed or prematurely terminated approximator runs.

3.9.2. Estimation Speed of Amortized Approximators

Amortized approximators, such as the pre-paid estimation method [182] or neural density estimation methods
[220, 115], require a slightly modiﬁed view on estimation speed, since they tend to split inference into two
phases (cf. Figure 2). Convergence in the context of amortized neural approximators typically happens
before any posterior draws have been obtained [273, 112], so the primary computational load falls into
the upfront simulation-based training phase. In contrast, the computational cost of applying a pre-trained
amortized approximator to obtain thousands of posterior draws or perform density estimation on real data
is typically negligible and only a matter of seconds even for high-dimensional posteriors [220]. In this way,
amortized approximators can be extremely useful for studying the (global) information gain (see Section
3.2.1) or calibration (see Section 3.2.3) as part of the parameter recoverability utility of PA models, since
these demand inference on many, potentially thousands of data sets simulated from the underlying P model.
Due to the properties of amortized approximators, we can modify the deﬁnition of estimation speed as the
time until convergence of the training phase plus the time for obtaining a suﬃcient number of posterior draws
on real data in order to reduce the MCSE beyond a pre-deﬁned threshold. In this context, estimation speed
will greatly depend on the simulation time, that is, the computational cost of performing a suﬃcient number
of model simulations. Some amortized methods make it possible to further subdivide estimation speed into
three parts: simulation time, training time, and inference time4, for instance, when using BayesFlow in an
oﬄine training regime [221] or when applying sequential neural estimators (e.g., APT, [115]) with the prior
as a sole proposal distribution throughout training.

In any case, estimation speed for amortized approximators will be dominated by the time spent before
obtaining posterior draws. Accordingly, it is often important to determine the break-even point between
an amortized and a non-amortized method, that is, after how many observed data sets does the training
eﬀort amortize in terms of ESS per unit time. Naturally, this break-even point will heavily depend on
the modeling context. For some P models, the break-even point between neural estimation and ABC can
occur after as few as 5 or even fewer observed data sets [222], but it can also occur only after as many as
dozens when comparing diﬀerent neural approximators [220]. In addition, amortized neural samplers can
often yield independent posterior draws upon convergence [220, 112, 115], so their sampling eﬃciency during
the inference phase (cf. Figure 2) will be generally superior to non-amortized approximators (i.e., stateful
samplers) yielding dependent draws.

Importantly, comparisons between amortized and non-amortized approximators (but also comparisons
within the same A class) should take implementation factors into account. For instance, the estimation
speed of neural approximators will be greatly enhanced by using GPU parallelization and even standard
ABC rejection samplers can be quite eﬃcient when run on a computing cluster with hundreds of nodes [150].
For simulation-based inference, the implementation of the simulation model presents a further potential bot-
tleneck, which can be alleviated via parallelization, model reformulation reducing the algorithmic complexity
of the simulator, or calibration of large-scale simulators via simpler surrogates [167]. In addition, recent hy-
brid methods employ a mixture of amortized and non-amortized components, such as amortized likelihood
ratio approximators within non-amortized MCMC [124] or neural likelihood surrogates [206]. These hybrid
methods blur the distinction between amortized and non-amortized methods and render the deﬁnition of es-
timation speed even more challenging. The dependence on these various implementation factors should make
us wary of comparisons between approximators in terms of estimation speed and appreciate the challenges
of building scalable PA(D) models.

4Other amortized approximators, such as the pre-paid method [182], only entail a simulation phase and an inference phase.

42

3.10. Robustness

A common question that arises when we discuss substantive conclusions derived from model-based inference
is how fragile these conclusions are with respect to crucial aspects of P, A, or D. Can we “break” the analysis
by a barely perceptible change in the data or by using a slightly diﬀerent approximator? Or are the main
results of the analysis largely impervious to such seemingly unsubstantial changes? The Robustness utility
attempts to answer such questions by measuring how much a PA, PD, or PAD model’s implications change
as we (systematically) perturb some of its components.

In the above deﬁnition, we use the term “component” very generally. It can refer to (structural) aspects
of the P model, most notably, to priors or their hyperparameters [62, 141, 219, 15, 77, 78, 231] or to aspects of
the likelihood function [15, 30]. It can also refer to the choice of data D, for example, the percentage of left-out
observations [141, 196, 270, 267], or to hyperparameters of the posterior approximator A [127, 221, 67, 287].
Thus, the term essentially refers to any aspect in which a P(A)(D) model can be sensibly modiﬁed.

More formally, we want to investigate the sensitivity (inverse robustness) of the posterior of some quantity
ψ = ψ(α) with respect to a new parameter α which exerts a potential inﬂuence on a component of interest.
If we are only interested in investigating the sensitivity of a speciﬁc (point) summary of the posterior, we
convey this by writing T (ψ(α)) for an arbitrary summary statistic T .

For example, we can study likelihood or prior sensitivity by power-scaling the respective components of
the P model, that is, replacing the joint model p(y | θ) p(θ) with p(y | θ)α p(θ) or p(y | θ) p(θ)α, respectively
[196, 132, 24, 141]. Of course, one can also choose to power-scale only parts of the likelihood or parts of the
joint prior. Although it is just one of many ways to systematically perturb a P model, power-scaling is a
popular approach due to its simplicity and natural integration with existing workﬂows [24, 141].

Regardless of the exact perturbation method, we can deﬁne (local) sensitivity as a measurable distance
(represented by a function f ) between the results of the current P(A)(D) model at value α0 and an alternative
value α1 that implies a diﬀerent P(A)(D) model, diverging from the original one only in the choice of α
[141, 231]:

Senα(T (ψ), α0, α1) := f (T (ψ(α0)), T (ψ(α1)).

(47)

For example, if T were a posterior expectation or a quantile of some univariate quantity ψ and α were a
hyperparameter of the prior p(ψ) = p(ψ | α), then f could simply be the absolute diﬀerence between these
expectations or quantiles as implied by α0 and α1, respectively. This deﬁnition can further be generalized
to sets of alternative α values in the neighborhood of α0 [231].

If α can be perturbed continuously (e.g., using power-scaling) and if T (ψ(α)) is diﬀerentiable at α0, we

may also deﬁne sensitivity as a function of the derivative of T (ψ(α)) evaluated at α0 [141, 219]:

Senα(∇T (ψ), α0) := f

(cid:32)

d T (ψ(α))
d α

(cid:33)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)α=α0

(48)

The latter deﬁnition has the advantage that no further value α1 has to be chosen, but it has a smaller range
of applicability and potentially more diﬃcult interpretation. In both of the above deﬁnitions, we can always
choose f such that the sensitivity is non-negative with a value of 0 indicating complete insensitivity.

For complex models, small amounts of sensitivity are almost always expected but may not practically

matter. Accordingly, we would say that T (ψ(α) is practically sensitive with respect to α if

Senα(T (ψ), α0, α1) > δ

(or Senα(∇T (ψ), α0) > δ)

(49)

for some chosen threshold δ that depends on the sensitivity measure and the modeling context [141]. For
example, let T denote a posterior mean and ψ denote a standardized eﬀect size that we would deem sensitive
if a change exceed δ = 0.2 standard deviation units. Then, the results would be practically sensitive to the
given perturbation, if changing α from α0 to α1 implied |T (ψ(α0)) − T (ψ(α1))| > 0.2.

When evaluating practical sensitivity related to hyperparameter choice within a class of posterior ap-
proximators, robustness is highly desirable, since approximators should ideally converge to the same target
(see Section 3.8). What is more, as PA models grow in complexity, analyses based on a single approximator

43

may gain trustworthiness by some form of multiverse analysis employing multiple approximators [275]. How-
ever, it is currently unclear how to systematically weigh the relative contribution of diﬀerent approximators
when trying to aggregate results from multiverse analysis, since some approximators might yield very poor
posterior approximations and thus skew any substantial conclusion.

Diﬀerently, when it comes to perturbations in P model assumptions or the observed data, neither practical
sensitivity nor insensitivity are desirable per se. Rather, we would like results to be practically robust to
perturbations if (a) the perturbations aﬀect only nuisance components of the P(A)D models that are equally
justiﬁable within the given context, or (b) if the perturbations are so small that they could very well have
occurred due to uncontrolled or uncontrollable inﬂuences. In contrast, when diﬀerent P model assumptions
represent competing substantive theories of interest, we deﬁnitely want the corresponding P(A)D models to
be sensitive to these assumptions.

Examples for (a) include diﬀerent choices of non-equivalent likelihood families that have an overall similar
complexity (e.g., Log-Normal vs. Gamma distribution for continuous positive data) or diﬀerent P(A)D
models that are capable of similar predictive performance (see Section 3.3), in case the latter is not already
the quantity of interest itself. Examples for (b) include adding small amounts of noise to the data [170],
leaving out a small subset of the data [270, 267], or slightly changing prior hyperparameters when the goal
is to specify weakly informative priors [141]. As the magnitude of the perturbations increases, we expect
results to become practically sensitive to these perturbations and observed insensitivity would then be a
reason for concern. For example, if drastically increasing the amount of data would not reduce the posterior
standard deviation of ψ, this would be an indication of empirical non-identiﬁability ([101]; see also Section
3.2.1) or an error in our model code [103].

Another type of sensitivity, arising in modeling dynamic systems, is sensitivity to initial conditions
[162, 244], which in our taxonomy can be understood as part of PI models. Sensitivity to initial conditions,
popularly known as the butterﬂy eﬀect, implies that an arbitrarily small change in initial conditions can
result in considerably diﬀerent subsequent system states (or observed trajectories). Moreover, this type of
sensitivity can be considered as a hallmark of deterministic chaos [244]. In the context of dynamic models,
the so-called Lyapunov exponent can measure a model’s sensitivity to initial conditions [11]. Lyapunov
exponents characterize the rate of exponential divergence from perturbed initial conditions and the maximal
Lyapunov exponent can be used to summarize the overall sensitivity of a model into a single number [142].
For more details on dynamic systems and deterministic chaos, we refer the interested readers to [244, 28].

4. Utility Hierarchies and Trade-oﬀs

Having discussed the ten model utilities individually, we now need to understand how they relate to each
other in order to compare Bayesian models holistically, across all utility dimensions. We will diﬀerentiate
between a utility hierarchy, where one utility is strictly more important than another, and a utility trade-oﬀ,
where we can achieve a gain of one utility at the cost of a loss of another. Utility hierarchies, utility trade-oﬀs,
as well as the relative importance of diﬀerent utilities, are inevitably application-speciﬁc and contingent on
the particular modeling goals.

Any modeling goal can be summarized by a set of quantities whose inferred model-based values or
distributions are then used in subsequent decision making. These quantities of interest may be observable or
latent (unobservable in reality), a distinction that constitutes the main branching point determining which
utilities are relevant (and by how much) in a given context. For observable inferential goals, the central
utility becomes predictive performance (see Section 3.3), whereas for latent inferential goals, the central
utility becomes parameter recoverability (see Section 3.2). This binary perspective has been put forward
in a similar fashion previously, for example, as the diﬀerence between two “statistical cultures” [33] or the
prediction-explanation dilemma [289, 246]. However, it has not been discussed in the context of explicit
model utilities. Below, we present two utility trees deﬁning hierarchies and trade-oﬀs for the two kinds of
inferential goals.

The way we would like to see these utility trees being used in practice is that analysts (a) build and
improve their models in a way that respects utility hierarchies and (b) talk explicitly about the utility

44

Figure 13: Utility tree for model-based inference of observable quantities (prediction).

trade-oﬀs they have been making in the process. This should enable users of the models and consumers
of statistical inference to understand which model building decisions have been made, why they have been
made, and how they aﬀect the trustworthiness of model-based decisions.

4.1. Utility Tree for Observable Inferential Goals

The workﬂow for observable inferential goals centers around the predictive performance of PAD models as
a central utility. This is the prevalent perspective on modeling in machine learning research. Given its
practical nature, this perspective would require a practically usable representation of a posterior distribution
from which predictions can subsequently be obtained, hence the focus on PAD models. Below, we discuss
our proposed model utility tree for observable inferential goals (see Figure 13).

4.1.1. Primary Utilities

Fairness: The fact that predictive performance is the central utility under this perspective does not mean
that it would be the sole or even the most important utility to consider. Rather, on the top of the utility
hierarchy, we need to check whether the predictive goals concern certain aspects related to fairness. If they
do, our PAD model needs to satisfy the fairness criteria agreed upon in the corresponding domain, otherwise
it would be considered invalid from an ethical and/or legal perspective (see Section 3.4), regardless of how
good its predictive performance is.
If fairness concerns do not apply to the particular PAD model, the
fairness utility can be circumvented.

4.1.2. Secondary Utilities

The secondary level of our predictive utility tree, includes (in addition to predictive performance itself),
estimation speed, interpretability, and robustness (in alphabetical order) as utilities across which trade-oﬀs
can be made. Notably, we do not require convergence of the PAD here and view estimation speed simply
until termination of the approximator, regardless of whether or not convergence had been reached (see
Section 3.9). While the three additional central utilities may exhibit trade-oﬀs among each other, increasing
them may in particular justify (some) reduction in predictive performance.

45

Estimation speed : If achieving high predictive performance requires either a very high dimensional pa-
rameter space (e.g., as in a neural network) or the repeated evaluation of a complex simulator (e.g., in a
diﬀerential equation-based mechanistic PI model), then estimation speed will exhibit a trade-oﬀ with pre-
dictive performance. In other words, PAD models with an easy-to-evaluate or easy-to-simulate likelihoods
may obtain faster posterior approximation, but may also yield worse predictive performance. Whenever
estimation speed becomes prohibitive for practical application of a PAD model, the context may justify the
use of another P model, even if the latter sacriﬁces (some) predictive performance. The same logic can justify
using approximators that speed up the approximation of a PAD model, even at the expense of losing (some)
predictive performance (e.g., using VI instead of MCMC-based approximators; see [25] or Section 2.3).

Interpretability: Interpretability is often higher in P models with lower parameter dimensionality and
linear structure (see Section 3.7). However, depending on the complexity of the real data generator, low di-
mensional and/or linear(ish) models may have worse predictive performance than higher dimensional and/or
more non-linear models. Yet, even if predictive performance is the main goal, it may still be legitimate
(or even legally required) to use a more interpretable PAD model, even at the cost of some predictive
performance.

Robustness: A PAD model yielding high predictive performance on some test data may yield surprisingly
poor predictions on slightly modiﬁed data (e.g., adversarial attacks on deep neural networks, see [1]). In a
similar vein, a PAD model that is well predicting given some reasonable initial values of an approximator A
may deliver worse predictions for some other (equally reasonable) initial values [158]. Both of these sensitivity
types are not desirable and it can be legitimate to sacriﬁce (some) predictive performance for an increase in
robustness against small perturbations.

4.1.3. Tertiary Utilities

At the third level of the predictive performance tree, we ﬁnd supporting utilities that may serve as proxies
for the central utilities. Speciﬁcally, these include causal consistency, convergence, parameter recoverability,
parsimony, and structural faithfulness (in an alphabetical order). Tertiary utilities are often easier to evaluate
and available “early” in the model building workﬂow, for example, when they are only requiring a P model
instead of a PAD model. Using these utilities can thus help speed up the model building process. However,
these supporting utilities should only guide ﬁnal modeling choices whenever multiple models are equally
justiﬁable with respect to primary and secondary utilities.

Causal consistency: If the quantities of interest are purely predictive, enforcing a P model to satisfy causal
consistency (or even thinking about a causal graph in the ﬁrst place) is not required and may even have
detrimental eﬀects on predictions [180]. In other words, for the purpose of predictions, it would usually be
entirely irrelevant how the association between two variables came to happen, as long as the input variables
are predictive of the outcome variables. However, causal consistency can still be a supporting utility to reduce
the a priori admissible model space by ruling out variables or interactions, as well as related P model terms,
for which a causal graph implies a lack of relation to the target variables (i.e., no path between covariates
and target, regardless of path direction). Considering a genetic association study as an example, we can
rule out gene areas that only encode genes whose eﬀects are known and understood to have no plausible
relationship to the phenotypes being predicted [276].

Convergence: Convergence is not strictly required in the predictive utility tree, since a non-converged
PAD model may still exhibit satisfactory predictive performance. That said, achieving convergence will
likely imply an improvement in central utilities as well. Not only is this true for predictive performance
itself, but also for robustness. For instance, a non-converged PAD model may vary arbitrarily from another
non-converged PAD model, whereas we can expect them to be (more) similar upon convergence, at least
for approximators that have the potential to explore the full posterior (see Section 2.3). Accordingly, before
studying central utilities that may be costly to evaluate, we can use convergence as a shortcut to rule out
PAD models with low potential to score high in those central utilities.

Parameter recoverability: Parameter recoverability can indirectly enhance predictive performance, since
nearly non-identiﬁable P models and poorly calibrated PA models can be discarded early in a model-building
workﬂow. Such models can neither yield good predictions, nor trustworthy uncertainty representation, as

46

some information gain is necessary to achieve posterior predictions that are diﬀerent than prior predictions
(see Section 3.2.1). However, strict parameter recoverability still plays a secondary role for the central goal
of achieving good predictions. For instance, highly over-parameterized P models may achieve zero Bayesian
surprise (i.e., no diﬀerence between prior and posterior) with respect to a large fraction of their parameters,
but still yield reasonable predictions on the basis of the few identiﬁable parameters [223].

Parsimony: In addition to having aesthetical value in itself (see Section 3.6), parsimony as a supporting
utility bears close relations to estimation speed, interpretability and predictive performance: More parsi-
monious models tend to be (a) faster to estimate, at least when comparing P models that are nested (i.e.,
one model is a special case of the other), (b) more interpretable, as fewer parameters have to be considered
simultaneously, and (c) less prone to overﬁtting (although they may be prone to underﬁtting). Some forms of
parsimony (both plain number of parameters and a priori eﬀective number of parameters; see Section 3.6.1)
are available before running any approximator. Correspondingly, we can utilize parsimony as an a priori
proxy for the central utilities.

Structural faithfulness: Structural faithfulness comprises several P model characteristics that we ide-
ally know and understand before running any approximator: variable scales, probabilistic symmetries, and
physical constraints (see Section 3.5). Structural faithfulness is related to multiple central utilities. Most
importantly this concern predictive performance, as structurally faithful models are more likely to predict
more accurately while requiring less data and showing better uncertainty calibration [225, 281]. But struc-
tural faithfulness is also related to estimation speed (for the better or worse; [98, 36]) as well as robustness,
for example, small perturbations of the training data [170]. Accordingly, we can also treat structural faith-
fulness as a proxy for central utilities in order to reduce the a priori considered model space to (suﬃciently)
structurally faithful models.

4.2. Utility Tree for Latent Inferential Goals

The utility tree for latent inferential goals centers around the parameter recoverability of P or PD models as
central utility. Modeling goals following this perspective are almost entirely of theoretical, epistemic nature,
and so the approximator is not itself part of the modeling goal. Yet, in practice, we will still almost always
rely on PA and PAD models for practical evaluation, hence the indispensable role of the approximator.
Below, we discuss our proposed model utility tree for latent inferential goals (see Figure 14).

4.2.1. Primary Utilities

Fairness: Once again, on top of the hierarchy, we ﬁnd fairness for ethical and/or legal reasons whenever the
modeling context has fairness-related implications (see Section 3.4). First, at an individual (person-speciﬁc)
level, we need to ensure that estimated latent parameters are fair with regard to individuals of protected
groups. Second, at a more general (person-unspeciﬁc) level, we need to keep in mind how our inferences
about latent parameters might trigger political decisions or societal processes aﬀecting protected groups.

Causal consistency: Next, we need to ensure that the P model is causally consistent with the assumed,
and theoretically justiﬁed, causal graph (see Section 3.1). We argue that thinking about causal consistency is
required for any latent modeling goal. Even if studies do not engage in (suﬃcient) causal analysis, and then
correctly state that their results cannot be interpreted causally, there still remains the (perhaps implicit)
wish that a causal claim would be possible. What is more, even a pure measurement goal (e.g., estimating
intelligence or personality traits without the need to relate latent parameters to each other) would need a
causal model to decide and justify which observable variables to use for estimation of the latent variables [143].
Finally, even if one might ﬁnd a latent inferential goal that would be honestly satisﬁed with association only,
causal analysis and discussion would still be required to prevent people from interpreting results causally.

Convergence: Convergence of PAD models is a prerequisite for any practically trustworthy result of a
latent inferential goal because we have no external validation criterion available during inference on real
data as we would have when considering observable inferential goals. In fact, prior to convergence, posterior
approximations may be almost arbitrarily incorrect, regardless of the kind of approximator being used (see
Section 3.8). Speciﬁcally, for asymptotically biased approximators (e.g., VI), the approximated posterior

47

Figure 14: Utility tree for model-based inference of latent quantities (parameter estimation).

upon convergence may still be a bad representation of the analytic posterior if the expressive scope of the
approximator is limited. But even in such cases, a converged approximator is more likely to be closer to
the analytic posterior than an arbitrary, non-converged approximator, and so the former is to be treated as
more trustworthy.

4.2.2. Secondary Utilities

When it comes to inferential goals, parameter recoverability is the central utility of the secondary hierarchy.
However, with the exception of the identiﬁcation sub-utility, it cannot be studied directly on real data
because knowledge of the ground-truth is missing (see Section 3.2). As a result, many studies on parameter
recoverability occur in the form of simulations or, if possible, mathematical analysis. This also concerns
studying trade-oﬀs with other central utilities, namely, estimation speed, interpretability, and robustness.
These remain instrumentally the same as for observable inferential goals and can reveal trade-oﬀs with
parameter recoverability for the same reason as for predictive performance (see Section 4.1.2).

4.2.3. Tertiary Utilities

Due to the lack of ground-truth latent parameters at real-data inference time, the tertiary, supporting utilities
not only aim at speeding up model building, but may also function as observable proxies of parameter
recoverability. These supporting utilities are parsimony, structural faithfulness, and predictive performance.
The reason for the relevance of the former two is the same as for observable inferential goals (see Section 4.1.3),
and so only predictive performance requires separate explanation and justiﬁcation.

48

Predictive Performance: The relation between predictive performance and parameter recoverability is
complicated and using the former as an observable proxy for the latter in a valid way requires great care
[246, 243]. Most importantly, we should not choose causally inconsistent P models, even if they predict
better [180, 243]. Fortunately, when taking the here-presented hierarchy of utilities seriously, this danger
is banished already by giving causal consistency priority over almost all other utilities for latent inferential
goals. Within the class of causally consistent P models, it seems that using predictive performance as proxy
for parameter recoverability in (converged) PAD models represents a valid approach [243]. For example, we
can utilize predictive performance to determine whether an extra probabilistic structure (e.g., accounting
for potential temporal or spatial dependencies; see Section 3.5.2) is worth including or not [35]. This aﬀects
the balance between structural faithfulness and parsimony, which in turn serve as proxies for parameter
recoverability. As another example, the choice of likelihood functions driven by predictive performance can
be used to improve parameter recoverability in the context of regression P models [243].

5. Conclusion

We proposed answers to two fundamental questions of Bayesian modeling, namely (1) “What actually is a
Bayesian model” and (2) “What makes a good Bayesian model”? Ultimately, we hope that both of these
questions and the answers we provided will aid in thinking and talking about Bayesian models, as well as
enhance the overarching model building process, regardless of the speciﬁc methods and ﬁelds of application.
As an answer to the ﬁrst question (Section 2), we proposed the PAD model taxonomy that deﬁnes four
diﬀerent kinds of Bayesian models as subsets of the triple of joint distribution of all involved variables (P),
the training data (D), and the posterior approximator (A). In this way, we put forward our view that modern
Bayesian models are more than just likelihood and prior, but comprise a variety of “external components”
which inﬂuence, and, in turn, are inﬂuenced by, the goals and the results of any statistical analysis.

As an answer to the second question (Sections 3 and 4), we ﬁrst argued that there are ten utility
dimensions along which we can evaluate Bayesian models, namely, (1) causal consistency, (2) parameter
recoverability, (3) predictive performance, (4) fairness, (5) structural faithfulness, (6) parsimony, (7) inter-
pretability, (8) convergence, (9) estimation speed, and (10) robustness. Then, we proposed two utility trees
that embody utility hierarchies and trade-oﬀs depending on the particular inferential goals. We hope that
our list of utility dimensions and structure of possible inferential goals is exhaustive (up to using synonyms
and regrouping sub-utilities diﬀerently). However, it may as well become incomplete in the future, as new
ideas are born and rapidly developed, and we will be happy to incorporate these into our taxonomy.

Acknowledgments

We thank Rudolf Debelak for helpful feedback on earlier versions of this paper. Our work was partially
funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s
Excellence Strategy – EXC-2075 - 390740016 (the Stuttgart Cluster of Excellence SimTech) and EXC-2181
- 390900948 (the Heidelberg Cluster of Excellence STRUCTURES). The authors gratefully acknowledge the
support and funding.

References

[1] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:

A survey. IEEE Access, 6:14410–14430, 2018.

[2] Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S Klessen,
Lena Maier-Hein, Carsten Rother, and Ullrich K¨othe. Analyzing inverse problems with invertible
neural networks. arXiv preprint, 2018.

49

[3] American Educational Research Association, editor. Standards for Educational and Psychological Test-
ing. American Educational Research Association, Washington, D.C, 2011. ISBN 978-0-935302-35-6.
OCLC: ocn826867074.

[4] Peter J. Baddoo, Benjamin Herrmann, Beverley J. McKeon, J. Nathan Kutz, and Steven L. Brunton.
Physics-informed dynamic mode decomposition (piDMD). arXiv preprint, 2021. URL http://arxiv.
org/abs/2112.04307.

[5] Marieke AR Bak. Computing fairness: ethics of modeling and simulation in public health. Simulation,

98(2):103–111, 2022.

[6] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org,

2019. http://www.fairmlbook.org.

[7] Pablo G Barrientos, J ´Angel Rodr´ıguez, and Alfonso Ruiz-Herrera. Chaotic dynamics in the seasonally

forced sir epidemic model. Journal of mathematical biology, 75(6):1655–1668, 2017.

[8] Douglas Bates, Reinhold Kliegl, Shravan Vasishth, and Harald Baayen. Parsimonious mixed models.

arXiv preprint, 2015. URL https://arxiv.org/abs/1506.04967.

[9] Douglas Bates, Martin M¨achler, Ben Bolker, and Steve Walker. Fitting Linear Mixed-Eﬀects Models

Using lme4. Journal of Statistical Software, 67(1):1–48, 2015.

[10] C. Beck and E. G. D. Cohen. Superstatistics. Physica A: Statistical Mechanics and its Applications,

322:267–275, 2003. ISSN 0378-4371. doi: 10.1016/S0378-4371(03)00019-0.

[11] Giancarlo Benettin, Luigi Galgani, Antonio Giorgilli, and Jean-Marie Strelcyn. Lyapunov characteristic
exponents for smooth dynamical systems and for hamiltonian systems; a method for computing all of
them. part 1: Theory. Meccanica, 15(1):9–20, 1980.

[12] Charles H Bennett. Eﬃcient estimation of free energy diﬀerences from monte carlo data. Journal of

Computational Physics, 22(2):245–268, 1976.

[13] Mario Beraha, Daniele Falco, and Alessandra Guglielmi. JAGS, NIMBLE, Stan: A detailed comparison
among Bayesian MCMC software. arXiv preprint, 2021. URL http://arxiv.org/abs/2107.09357.

[14] James O Berger and Luis R Pericchi. The intrinsic Bayes factor for model selection and prediction.

Journal of the American Statistical Association, 91(433):109–122, 1996.

[15] James O. Berger, El´ıas Moreno, Luis Raul Pericchi, M. Jes´us Bayarri, Jos´e M. Bernardo, Juan A. Cano,
Juli´an De la Horra, Jacinto Mart´ın, David R´ıos-Ins´ua, Bruno Betr`o, A. Dasgupta, Paul Gustafson,
Larry Wasserman, Joseph B. Kadane, Cid Srinivasan, Michael Lavine, Anthony O’Hagan, Wolfgang
Polasek, Christian P. Robert, Constantinos Goutis, Fabrizio Ruggeri, Gabriella Salinetti, and Siva
Sivaganesan. An overview of robust Bayesian analysis. Test, 3(1):5–124, 1994.
ISSN 1133-0686,
1863-8260. doi: 10.1007/BF02562676. URL http://link.springer.com/10.1007/BF02562676.

[16] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in Criminal
Justice Risk Assessments: The State of the Art. Sociological Methods & Research, 50(1):3–44, 2021.
ISSN 0049-1241, 1552-8294. doi: 10.1177/0049124118782533. URL http://journals.sagepub.com/
doi/10.1177/0049124118782533.

[17] L Mark Berliner. Likelihood and bayesian prediction of chaotic systems. Journal of the American

Statistical Association, 86(416):938–952, 1991.

[18] Jos´e M. Bernardo and Adrian F. M. Smith. Bayesian Theory. Hoboken: Wiley, 1994.

50

[19] Julian Besag. Spatial Interaction and the Statistical Analysis of Lattice Systems. Journal of the
Royal Statistical Society: Series B (Methodological), 36(2):192–225, 1974.
ISSN 2517-6161. doi:
10.1111/j.2517-6161.1974.tb00999.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/
j.2517-6161.1974.tb00999.x.

[20] Nicky Best, Nigel Dallow, and Timothy Montague. Prior elicitation. Bayesian methods in pharmaceu-

tical research, pages 87–109, 2020.

[21] Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv preprint, 2017.

URL https://arxiv.org/pdf/1701.02434.pdf.

[22] Michael Betancourt. Calibrating model-based inferences and decisions. arXiv preprint, 2018.

[23] Anindya Bhadra, Jyotishka Datta, Nicholas G Polson, and Brandon Willard. Default Bayesian analysis

with global-local shrinkage priors. Biometrika, 103(4):955–969, 2016.

[24] Pier Giovanni Bissiri, Chris Holmes, and Stephen Walker. A General Framework for Updating Belief
Distributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):1103–
1130, 2016. ISSN 13697412. doi: 10.1111/rssb.12158. URL http://arxiv.org/abs/1306.6430.

[25] David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference: A review for statisticians.

Journal of the American statistical Association, 112(518):859–877, 2017.

[26] David M. Blei, Alp Kucukelbir, and Jon D. McAuliﬀe. Variational Inference: A Review for Statisticians.
Journal of the American Statistical Association, 112(518):859–877, 2017. ISSN 0162-1459. doi: 10.
1080/01621459.2017.1285773. URL https://doi.org/10.1080/01621459.2017.1285773.

[27] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam’s razor.

Information processing letters, 24(6):377–380, 1987.

[28] Stefanos Boccaletti, Celso Grebogi, Y-C Lai, Hector Mancini, and Diego Maza. The control of chaos:

theory and applications. Physics reports, 329(3):103–197, 2000.

[29] Jan Boelts, Jan-Matthis Lueckmann, Richard Gao, and Jakob H Macke. Flexible and eﬃcient

simulation-based inference for models of decision-making. eLife, 11:e77220, 2022.

[30] Wagner Hugo Bonat, Paulo Justiniano RIBEIRO Jr, and Walmes Marques Zeviani. Regression models
with responses on the unity Interval: Speciﬁcation, estimation and comparison. Biometric Brazilian
Journal, 30(4):18, 2013.

[31] Francesco Bonchi, Sara Hajian, Bud Mishra, and Daniele Ramazzotti. Exposing the probabilistic
causal structure of discrimination. International Journal of Data Science and Analytics, 3(1):1–21,
2017.

[32] Denny Borsboom, Gideon Mellenbergh, and Jaap Heerden. The Concept of Validity. Psychological

review, 111:1061–71, November 2004. doi: 10.1037/0033-295X.111.4.1061.

[33] Leo Breiman.

Statistical Modeling: The Two Cultures (with comments and a rejoinder by
doi:
Statistical Science, 16(3):199–231, 2001.
URL https://projecteuclid.org/journals/statistical-science/

the author).
10.1214/ss/1009213726.
volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.
1214/ss/1009213726.full.

ISSN 0883-4237, 2168-8745.

[34] Nadia Burkart and Marco F. Huber. A Survey on the Explainability of Supervised Machine Learning.
ISSN 1076-9757. doi: 10.1613/jair.1.

Journal of Artiﬁcial Intelligence Research, 70:245–317, 2021.
12228. URL https://www.jair.org/index.php/jair/article/view/12228.

51

[35] Paul-Christian B¨urkner, Jonah Gabry, and Aki Vehtari. Eﬃcient leave-one-out cross-validation for
bayesian non-factorized normal and student-t models. Computational Statistics, 36(2):1243–1261, 2021.

[36] Paul-Christian B¨urkner. brms: An R Package for Bayesian Multilevel Models using Stan. Journal of

Statistical Software, 80(1):1–28, 2017.

[37] Paul-Christian B¨urkner. Advanced Bayesian Multilevel Modeling with the R Package brms. The R

Journal, 10(1):395–411, 2018.

[38] Paul-Christian B¨urkner. Bayesian Item Response Modelling in R with brms and Stan. Journal of

Statistical Software, pages 1–54, 2021.

[39] Paul-Christian B¨urkner. On the information obtainable from comparative judgments. Psychometrika,

pages 1–34, 2022. Publisher: Springer.

[40] Paul-Christian B¨urkner and Emmanuel Charpentier. Modeling Monotonic Eﬀects of Ordinal Predictors
in Bayesian Regression Models. British Journal of Mathematical and Statistical Psychology, pages 1–32,
2020.

[41] Paul-Christian B¨urkner and Matti Vuorre. Ordinal Regression Models in Psychology: A Tutorial.

Advances in Methods and Practices in Psychological Science, 2(1):77–101, 2019.

[42] Paul-Christian B¨urkner, Niklas Schulte, and Heinz Holling. On the Statistical and Practical Limita-
tions of Thurstonian IRT Models. Educational and Psychological Measurement, 79(5):827–854, 2018.
Publisher: Los Angelos: Sage.

[43] Paul-Christian B¨urkner, Jonah Gabry, and Aki Vehtari. Approximate leave-future-out cross-validation
for Bayesian time series models. Journal of Statistical Computation and Simulation, pages 1–25, 2020.

[44] Paul-Christian B¨urkner, Jonah Gabry, Matthew Kay, and Aki Vehtari. posterior: Tools for working
with posterior distributions, 2022. URL https://mc-stan.org/posterior/. R package version 1.3.0.

[45] Paul-Christian B¨urkner, Ilja Kr¨oker, Sergey Oladyshkin, and Wolfgang Nowak. The sparse Polynomial
Chaos expansion: a fully Bayesian approach with joint priors on the coeﬃcients and global selection
of terms. arXiv preprint, 2022. URL http://arxiv.org/abs/2204.06043.

[46] Bob Carpenter, Andrew Gelman, Matthew D Hoﬀman, Daniel Lee, Ben Goodrich, Michael Betancourt,
Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming
language. Journal of Statistical Software, 76(1), 2017.

[47] George Casella and Roger L Berger. Statistical inference. Cengage Learning, 2002.

[48] Jeﬀrey Chan, Valerio Perrone, Jeﬀrey Spence, Paul Jenkins, Sara Mathieson, and Yun Song. A
likelihood-free inference framework for population genetic data using exchangeable neural networks.
Advances in neural information processing systems, 31, 2018.

[49] Hanfeng Chen, Jiahua Chen, and John D Kalbﬂeisch. Testing for a ﬁnite mixture model with two
components. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 66(1):95–115,
2004.

[50] Yanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville, and Zhanxing Zhu. Neural ap-

proximate suﬃcient statistics for implicit models. arXiv preprint, 2020.

[51] Alexandra Chouldechova and Aaron Roth. The Frontiers of Fairness in Machine Learning, 2018. URL

http://arxiv.org/abs/1810.08810.

52

[52] Carlos Cinelli, Andrew Forney, and Judea Pearl. A Crash Course in Good and Bad Controls. SSRN
Electronic Journal, 2020. ISSN 1556-5068. doi: 10.2139/ssrn.3689437. URL https://www.ssrn.com/
abstract=3689437.

[53] Sam Corbett-Davies and Sharad Goel. The Measure and Mismeasure of Fairness: A Critical Review

of Fair Machine Learning. arXiv preprint, 2018. URL http://arxiv.org/abs/1808.00023.

[54] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord Stein. Introduction to algo-

rithms. MIT press, 2022.

[55] Mary Kathryn Cowles and Bradley P Carlin. Markov chain Monte Carlo convergence diagnostics: a

comparative review. Journal of the American Statistical Association, 91(434):883–904, 1996.

[56] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Pro-

ceedings of the National Academy of Sciences, 2020.

[57] Marco F. Cusumano-Towner and Vikash K. Mansinghka. Measuring the non-asymptotic convergence
of sequential Monte Carlo samplers using probabilistic programming. arXiv preprint, 2017. URL
http://arxiv.org/abs/1612.02161.

[58] Chenguang Dai, Jeremy Heng, Pierre E. Jacob, and Nick Whiteley. An invitation to sequential Monte

Carlo samplers. arXiv preprint, 2020. URL http://arxiv.org/abs/2007.11936.

[59] Luke de Oliveira, Michela Paganini, and Benjamin Nachman. Learning particle physics by example:
location-aware generative adversarial networks for physics synthesis. Computing and Software for Big
Science, 1(1):1–24, 2017.

[60] Perry de Valpine. A close look at some linear model MCMC comparisons – NIMBLE, 2021. https://r-

nimble.org/a-close-look-at-some-linear-model-mcmc-comparisons.

[61] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
ISSN 1467-
9868. doi: 10.1111/j.1467-9868.2006.00553.x. URL https://onlinelibrary.wiley.com/doi/abs/
10.1111/j.1467-9868.2006.00553.x.

[62] Sarah Depaoli, Sonja D Winter, and Marieke Visser. The importance of prior sensitivity analysis in
Bayesian statistics: demonstrations using an interactive Shiny App. Frontiers in Psychology, 2020.

[63] Akash Kumar Dhaka, Alejandro Catalina, Michael R Andersen, M˚ans Magnusson, Jonathan Huggins,
and Aki Vehtari. Robust, accurate stochastic optimization for variational inference. Advances in Neural
Information Processing Systems, 33:10961–10973, 2020.

[64] Peter J Diggle and Richard J Gratton. Monte carlo methods of inference for implicit statistical models.

Journal of the Royal Statistical Society: Series B (Methodological), 46(2):193–212, 1984.

[65] Finale Doshi-Velez and Been Kim. Towards A Rigorous Science of Interpretable Machine Learning.

arXiv preprint, 2017. URL http://arxiv.org/abs/1702.08608.

[66] Charles R. Doss, James M. Flegal, Galin L. Jones,

chain Monte Carlo estimation of quantiles.
2478,
1935-7524.
//projecteuclid.org/journals/electronic-journal-of-statistics/volume-8/issue-2/
Markov-chain-Monte-Carlo-estimation-of-quantiles/10.1214/14-EJS957.full.

ISSN 1935-7524,

and Ronald C. Neath.
Electronic Journal of Statistics,
10.1214/14-EJS957.
doi:

Markov
8(2):2448–
URL https:

2014.

[67] Arnaud Doucet, Nando de Freitas, and Neil Gordon. An Introduction to Sequential Monte Carlo
Methods. In Arnaud Doucet, Nando de Freitas, and Neil Gordon, editors, Sequential Monte Carlo
Methods in Practice, pages 3–14. Springer, New York, NY, 2001. ISBN 978-1-4757-3437-9. doi: 10.
1007/978-1-4757-3437-9 1. URL https://doi.org/10.1007/978-1-4757-3437-9_1.

53

[68] Fritz Drasgow, Michael V Levine, Sherman Tsien, Bruce Williams, and Alan D. Mead. Fitting
Polytomous Item Response Theory Models to Multiple-Choice Tests. Applied Psychological Mea-
surement, 19(2):143–166, 1995. ISSN 0146-6216, 1552-3497. doi: 10.1177/014662169501900203. URL
http://journals.sagepub.com/doi/10.1177/014662169501900203.

[69] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially No Barriers in
Neural Network Energy Landscape. In Proceedings of the 35th International Conference on Machine
Learning, pages 1309–1318. PMLR, 2018. URL https://proceedings.mlr.press/v80/draxler18a.
html.

[70] Oliver Duerr, Beate Sick, and Elvis Murina. Probabilistic Deep Learning: With Python, Keras and

TensorFlow Probability. Simon and Schuster, 2020. ISBN 978-1-61729-607-9.

[71] Juan M Dur´an. What is a simulation model? Minds and Machines, 30(3):301–323, 2020.

[72] Tarek A El Moselhy and Youssef M Marzouk. Bayesian inference with optimal maps. Journal of

Computational Physics, 231(23):7815–7850, 2012.

[73] Susan E Embretson and Steven P Reise. Item Response Theory. Psychology Press, 2000.

[74] Ashley F Emery and Aleksey V Nenarokomov. Optimal experiment design. Measurement Science and

Technology, 9(6):864, 1998.

[75] Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with discretized

diﬀusions. Advances in Neural Information Processing Systems, 31, 2018.

[76] Alexander Etz and Eric-Jan Wagenmakers. J. B. S. Haldane’s Contribution to the Bayes Factor
doi: 10.1214/16-STS599.
https://projecteuclid.org/journals/statistical-science/volume-32/issue-2/

Hypothesis Test.
URL
J-B-S-Haldanes-Contribution-to-the-Bayes-Factor-Hypothesis/10.1214/16-STS599.full.

Statistical Science, 32(2), 2017.

ISSN 0883-4237.

[77] Michael Evans and Gun Ho Jang. Weak informativity and the information in one prior relative to

another. Statistical Science, 26(3):423–439, 2011.

[78] Michael Evans and Hadas Moshonov. Checking for prior-data conﬂict. Bayesian Analysis, 1(4):893–

914, 2006.

[79] Valerii Fedorov. Optimal experimental design. Wiley Interdisciplinary Reviews: Computational Statis-

tics, 2(5):581–589, 2010.

[80] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and Removing Disparate Impact.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 259–268, Sydney NSW
Australia, 2015. ACM.
ISBN 978-1-4503-3664-2. doi: 10.1145/2783258.2783311. URL https:
//dl.acm.org/doi/10.1145/2783258.2783311.

[81] Alexander Fengler, Lakshmi N Govindarajan, Tony Chen, and Michael J Frank. Likelihood approx-
imation networks (lans) for fast inference of simulation models in cognitive neuroscience. Elife, 10:
e65074, 2021.

[82] James M. Flegal, Murali Haran, and Galin L. Jones. Markov Chain Monte Carlo: Can We Trust the
Third Signiﬁcant Figure? Statistical Science, 23(2), 2008. ISSN 0883-4237. doi: 10.1214/08-STS257.
URL http://arxiv.org/abs/math/0703746.

[83] Luciano Floridi and Massimo Chiriatti. GPT-3: Its Nature, Scope, Limits, and Consequences. Minds
and Machines, 30(4):681–694, 2020. ISSN 1572-8641. doi: 10.1007/s11023-020-09548-1. URL https:
//doi.org/10.1007/s11023-020-09548-1.

54

[84] Charles W. Fox and Stephen J. Roberts. A tutorial on variational Bayesian inference. Artiﬁcial
Intelligence Review, 38(2):85–95, 2012. ISSN 0269-2821, 1573-7462. doi: 10.1007/s10462-011-9236-8.
URL http://link.springer.com/10.1007/s10462-011-9236-8.

[85] David T Frazier and Christopher Drovandi. Robust approximate bayesian inference with synthetic

likelihood. Journal of Computational and Graphical Statistics, 30(4):958–976, 2021.

[86] David T Frazier, Christian P Robert, and Judith Rousseau. Model misspeciﬁcation in approximate
bayesian computation: consequences and diagnostics. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 82(2):421–444, 2020.

[87] David A Freedman. Statistical models and causal inference: a dialogue with the social sciences. Cam-

bridge University Press, 2010.

[88] Jerome H Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19(1):1–67,

1991.

[89] E. L. Frome. The Analysis of Rates Using Poisson Regression Models. Biometrics, 39(3):665–674,
1983. ISSN 0006-341X. doi: 10.2307/2531094. URL https://www.jstor.org/stable/2531094.

[90] Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren, and H˚avard Rue. Constructing priors that pe-
nalize the complexity of Gaussian random ﬁelds. Journal of the American Statistical Association, 114
(525):445–452, 2019.

[91] Xavier Gabaix and David Laibson. The seven properties of good models. The foundations of positive

and normative economics: A handbook, pages 292–319, 2008.

[92] Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization
in Bayesian workﬂow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2):
389–402, 2019.

[93] Yuxiang Gao, Lauren Kennedy, Daniel Simpson, and Andrew Gelman. Improving multilevel regression

and poststratiﬁcation with structured priors. Bayesian Analysis, 16(3):719–744, 2021.

[94] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.
In Advances in Neural Information
Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html.

[95] Alan E Gelfand. Gibbs sampling. Journal of the American statistical Association, 95(452):1300–1304,

2000.

[96] Alan E Gelfand and Penelope Vounatsou. Proper multivariate conditional autoregressive models for

spatial data analysis. Biostatistics, 4(1):11–15, 2003.

[97] Andrew Gelman. Parameterization and Bayesian Modeling. Journal of the American Statistical Asso-
ciation, 99(466):537–545, 2004. ISSN 0162-1459, 1537-274X. doi: 10.1198/016214504000000458. URL
http://www.tandfonline.com/doi/abs/10.1198/016214504000000458.

[98] Andrew Gelman and Jennifer Hill. Data Analysis Using Regression and Multilevel/Hierarchical Models.

Cambridge University Press, 2006.

[99] Andrew Gelman and Donald B Rubin. Inference from iterative simulation using multiple sequences.

Statistical Science, 7(4):457–472, 1992.

[100] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin.
Bayesian Data Analysis (3rd edition). Chapman and Hall/CRC, 2013. ISBN 978-0-429-11307-9. doi:
10.1201/b16018.

55

[101] Andrew Gelman, Daniel Simpson, and Michael Betancourt. The prior can often only be understood

in the context of the likelihood. Entropy, 19(10):555–567, 2017.

[102] Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari. R-squared for Bayesian Regression
Models. The American Statistician, 73(3):307–309, 2019. ISSN 0003-1305, 1537-2731. doi: 10.1080/
00031305.2018.1549100. URL https://www.tandfonline.com/doi/full/10.1080/00031305.2018.
1549100.

[103] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao,
Lauren Kennedy, Jonah Gabry, Paul-Christian B¨urkner, and Martin Modr´ak. Bayesian workﬂow.
arXiv preprint, 2020.

[104] Edward I George, UE Makov, and AFM Smith. Conjugate likelihood distributions. Scandinavian

Journal of Statistics, pages 147–156, 1993.

[105] Jan Gertheiss and Gerhard Tutz. Penalized regression with ordinal predictors. International Statistical

Review, 77(3):345–365, 2009.

[106] Andrea Gesmundo and Jeﬀ Dean. An evolutionary approach to dynamic introduction of tasks in

large-scale multitask learning systems. arXiv preprint, 2022.

[107] Charles J Geyer. Practical markov chain monte carlo. Statistical Science, pages 473–483, 1992.

[108] R Gilmore and JWL McCallum. Structure in the bifurcation diagram of the duﬃng oscillator. Physical

Review E, 51(2):935, 1995.

[109] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical

models. Frontiers in Genetics, 10:524, 2019.

[110] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and
sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243–268,
2007.

[111] Prem K Goel and Morris H Degroot.

Information about hyperparameters in hierarchical models.

Journal of the American Statistical Association, 76(373):140–147, 1981.

[112] Pedro J Gon¸calves, Jan-Matthis Lueckmann, Michael Deistler, Marcel Nonnenmacher, Kaan ¨Ocal,
Giacomo Bassetto, Chaitanya Chintaluri, William F Podlaski, Sara A Haddad, Tim P Vogels, et al.
Training deep neural density estimators to identify mechanistic models of neural dynamics. Elife, 9:
e56261, 2020.

[113] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.

[114] Jakob Grazzini, Matteo G Richiardi, and Mike Tsionas. Bayesian estimation of agent-based models.

Journal of Economic Dynamics and Control, 77:26–47, 2017.

[115] David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for

likelihood-free inference. In International Conference on Machine Learning, pages 2404–2414, 2019.

[116] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A

kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012.

[117] Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Mars-
man, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, and Helen Steingroever. A
tutorial on bridge sampling.
ISSN 0022-
2496. doi: 10.1016/j.jmp.2017.09.005. URL https://www.sciencedirect.com/science/article/
pii/S0022249617300640.

Journal of Mathematical Psychology, 81:80–97, 2017.

56

[118] Quentin F. Gronau, Henrik Singmann, and Eric-Jan Wagenmakers. bridgesampling: An R Package for
Estimating Normalizing Constants. Journal of Statistical Software, 92(10):1–29, 2020. doi: 10.18637/
jss.v092.i10. URL https://www.jstatsoft.org/index.php/jss/article/view/v092i10.

[119] Xin Gu, Joris Mulder, and Herbert Hoijtink. Approximated adjusted fractional Bayes factors: A
general method for testing informative hypotheses. British Journal of Mathematical and Statistical
Psychology, 71(2):229–261, 2018.

[120] John Burdon Sanderson Haldane. A note on inverse probability. In Mathematical Proceedings of the

Cambridge Philosophical Society, volume 28, pages 55–61. Cambridge University Press, 1932.

[121] Mark H Hansen and Bin Yu. Model selection and the principle of minimum description length. Journal

of the American Statistical Association, 96(454):746–774, 2001.

[122] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of

statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.

[123] W Keith Hastings. Monte Carlo sampling methods using Markov chains and their applications. Oxford

University Press, 1970.

[124] Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free mcmc with amortized approximate

ratio estimators. In International Conference on Machine Learning, pages 4239–4248. PMLR, 2020.

[125] Sean Hoban, Giorgio Bertorelle, and Oscar E Gaggiotti. Computer simulations: tools for population

and evolutionary genetics. Nature Reviews Genetics, 13(2):110–122, 2012.

[126] James S Hodges and Daniel J Sargent. Counting degrees of freedom in hierarchical and other richly-

parameterised models. Biometrika, 88(2):367–379, 2001.

[127] M. Hoﬀman and A. Gelman. The No-U-turn sampler: Adaptively setting path lengths in Hamiltonian

Monte Carlo. Journal of Machine Learning Research, 2014.

[128] Paul W Holland and Howard Wainer. Diﬀerential Item Functioning. Routledge, 1993.

[129] Sabine Hossenfelder. Lost in math: How beauty leads physics astray. Hachette UK, 2018.

[130] Eyke H¨ullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:

An introduction to concepts and methods. Machine Learning, 110(3):457–506, 2021.

[131] Antti Hyttinen, Frederick Eberhardt, and Matti J¨arvisalo. Do-calculus when the true graph is unknown.

In UAI, pages 395–404. Citeseer, 2015.

[132] Joseph G. Ibrahim and Ming-Hui Chen. Power Prior Distributions for Regression Models. Statistical
Science, 15(1):46–60, 2000. ISSN 0883-4237. URL https://www.jstor.org/stable/2676676.

[133] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences.

Cambridge University Press, 2015.

[134] Desi R Ivanova, Adam Foster, Steven Kleinegesse, Michael U Gutmann, and Thomas Rainforth. Im-
plicit deep adaptive design: policy-based experimental design without likelihoods. Advances in Neural
Information Processing Systems, 34:25785–25798, 2021.

[135] Eugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks, 14(6):

1569–1572, 2003.

[136] Pavel Izmailov, Sharad Vikram, Matthew D. Hoﬀman, and Andrew Gordon Gordon Wilson. What Are
Bayesian Neural Network Posteriors Really Like? In Proceedings of the 38th International Conference
on Machine Learning, pages 4629–4640. PMLR, 2021. URL https://proceedings.mlr.press/v139/
izmailov21a.html.

57

[137] Silke Janitza, Carolin Strobl, and Anne-Laure Boulesteix. An auc-based permutation variable impor-

tance measure for random forests. BMC bioinformatics, 14(1):1–11, 2013.

[138] Lucas Janson, William Fithian, and Trevor J Hastie. Eﬀective degrees of freedom: a ﬂawed metaphor.

Biometrika, 102(2):479–485, 2015.

[139] Bai Jiang, Tung-yu Wu, Charles Zheng, and Wing H Wong. Learning summary statistic for approxi-
mate bayesian computation via deep neural network. Statistica Sinica, pages 1595–1618, 2017.

[140] Daqing Jiang, Jiajia Yu, Chunyan Ji, and Ningzhong Shi. Asymptotic behavior of global positive
solution to a stochastic sir model. Mathematical and Computer Modelling, 54(1-2):221–232, 2011.

[141] Noa Kallioinen, Topi Paananen, Paul-Christian B¨urkner, and Aki Vehtari. Detecting and diagnosing

prior and likelihood sensitivity with power-scaling. arXiv preprint, 2021.

[142] Holger Kantz. A robust method to estimate the maximal lyapunov exponent of a time series. Physics

letters A, 185(1):77–87, 1994.

[143] David Kaplan. Structural equation modeling: Foundations and extensions, volume 10. Los Angelos:

Sage, 2008.

[144] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.

Physics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021.

[145] Robert E Kass and Adrian E Raftery. Bayes factors. Journal of the american statistical association,

90(430):773–795, 1995.

[146] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to crit-
In Advances in Neural Information Processing Systems, vol-
icize! Criticism for Interpretability.
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
5680522b8e2bb01943234bce7bf84534-Abstract.html.

[147] Sanggyun Kim, Rui Ma, Diego Mesa, and Todd P Coleman. Eﬃcient bayesian inference methods via
convex optimization and optimal transport. In 2013 IEEE International Symposium on Information
Theory, pages 2259–2263. IEEE, 2013.

[148] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint,

2017. URL http://arxiv.org/abs/1412.6980.

[149] Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.

Advances in neural information processing systems, 31, 2018.

[150] Emmanuel Klinger, Dennis Rickert, and Jan Hasenauer. pyabc: distributed, likelihood-free inference.

Bioinformatics, 34(20):3591–3593, 2018.

[151] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan:

Learning causal implicit generative models with adversarial training. arXiv preprint, 2017.

[152] Augustine Kong, Jun S. Liu, and Wing Hung Wong. Sequential Imputations and Bayesian Miss-
ing Data Problems. Journal of the American Statistical Association, 89(425):278–288, 1994.
ISSN
0162-1459. doi: 10.1080/01621459.1994.10476469. URL https://www.tandfonline.com/doi/abs/
10.1080/01621459.1994.10476469.

[153] Ilan Koren, Eli Tziperman, and Graham Feingold. Exploring the nonlinear cloud and rain equation.

Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(1):013107, 2017.

58

[154] Joachim Krueger. Null hypothesis signiﬁcance testing: On the survival of a ﬂawed method. American
Psychologist, 56(1):16–26, 2001. ISSN 1935-990X, 0003-066X. doi: 10.1037/0003-066X.56.1.16. URL
http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.56.1.16.

[155] Ben Lambert and Aki Vehtari. r∗: A robust MCMC convergence diagnostic with uncertainty using
decision tree classiﬁers. Bayesian Analysis, 17(2):353–379, 2022. Publisher: International Society for
Bayesian Analysis.

[156] Alexander Lavin, Hector Zenil, Brooks Paige, David Krakauer, Justin Gottschlich, Tim Mattson,
Anima Anandkumar, Sanjay Choudry, Kamil Rocki, Atılım G¨une¸s Baydin, Carina Prunkl, Brooks
Paige, Olexandr Isayev, Erik Peterson, Peter L. McMahon, Jakob Macke, Kyle Cranmer, Jiaxin Zhang,
Haruko Wainwright, Adi Hanuka, Manuela Veloso, Samuel Assefa, Stephan Zheng, and Avi Pfeﬀer.
Simulation Intelligence: Towards a New Generation of Scientiﬁc Methods. arXiv preprint, 2021. URL
http://arxiv.org/abs/2112.03235.

[157] Anthony Lee and Nick Whiteley. Variance estimation in the particle ﬁlter. Biometrika, 105(3):609–625,

2018.

[158] Y. Lee, S.-H. Oh, and M.W. Kim. The eﬀect of initial weights on premature saturation in back-
propagation learning. In IJCNN-91-Seattle International Joint Conference on Neural Networks, vol-
ume i, pages 765–770 vol.1, 1991. doi: 10.1109/IJCNN.1991.155275.

[159] Torrin M Liddell and John K Kruschke. Analyzing ordinal data with metric models: What could

possibly go wrong? Journal of Experimental Social Psychology, 79:328–348, 2018.

[160] Finn Lindgren and H˚avard Rue. Bayesian spatial modelling with r-inla. Journal of Statistical Software,

63:1–25, 2015.

[161] Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of

Mathematical Statistics, pages 986–1005, 1956.

[162] Edward N Lorenz. Deterministic nonperiodic ﬂow. Journal of Atmospheric Sciences, 20(2):130–141,

1963.

[163] Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian

model selection, the marginal likelihood, and generalization. arXiv preprint, 2022.

[164] Jan-Matthis Lueckmann, Pedro J Gon¸calves, Giacomo Bassetto, Kaan ¨Ocal, Marcel Nonnenmacher,
and Jakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics.
In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pages
1289–1299, 2017.

[165] Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan ¨Ocal, Marcel Non-
statistical
for mechanistic models
Flexible
Information Processing Systems, volume 30.
In Advances in Neural
URL https://proceedings.neurips.cc/paper/2017/hash/

nenmacher,
of neural dynamics.
Curran Associates,
Inc., 2017.
addfa9b7e234254d26e9c7f2af1005cb-Abstract.html.

and Jakob H Macke.

inference

[166] Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke. Likelihood-
free inference with emulator networks. In Symposium on Advances in Approximate Bayesian Inference,
pages 32–53. PMLR, 2019.

[167] Spencer Lunderman, Matthias Morzfeld, Franziska Glassmeier, and Graham Feingold. Estimating
parameters of the nonlinear cloud and rain equation from a large-eddy simulation. Physica D: Nonlinear
Phenomena, 410:132500, 2020.

[168] Anatolii Isakovich Lurie. Analytical mechanics. Springer Science & Business Media, 2002.

59

[169] David MacKay. Information theory, inference and learning algorithms. Cambridge University Press,

2003.

[170] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-

wards deep learning models resistant to adversarial attacks. arXiv preprint, 2017.

[171] Jean-Michel Marin, Pierre Pudlo, Arnaud Estoup, and Christian Robert. Likelihood-free model choice.

Chapman and Hall/CRC Press, 2018.

[172] Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavar´e. Markov chain monte carlo without

likelihoods. Proceedings of the National Academy of Sciences, 100(26):15324–15328, 2003.

[173] Christoph Mark, Claus Metzner, Lena Lautscham, Pamela L. Strissel, Reiner Strick, and Ben Fabry.
Bayesian model selection for complex dynamic systems. Nature Communications, 9(1):1803, 2018.
ISSN 2041-1723. doi: 10.1038/s41467-018-04241-5.

[174] Gael M Martin, David T Frazier, and Christian P Robert. Approximating bayes in the 21st century.

arXiv preprint, 2021.

[175] Andres Masegosa. Learning under model misspeciﬁcation: Applications to variational and ensemble

methods. Advances in Neural Information Processing Systems, 33:5479–5491, 2020.

[176] Robert M May. Simple mathematical models with very complicated dynamics. Nature, 261:459, 1976.

[177] Conor Mayo-Wilson and Kevin JS Zollman. The computational philosophy: simulation as a core

philosophical method. Synthese, 199(1):3647–3673, 2021.

[178] R Steve McCallum. Handbook of nonverbal assessment, volume 30. Springer, 2003.

[179] Peter McCullagh. Regression models for ordinal data. Journal of the Royal Statistical Society: Series

B (Methodological), 42(2):109–127, 1980.

[180] Richard McElreath. Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman

and Hall/CRC, 2020.

[181] Xiao-Li Meng and Wing Hung Wong. Simulating ratios of normalizing constants via a simple identity:

a theoretical exploration. Statistica Sinica, 6(4):831–860, 1996.

[182] Merijn Mestdagh, Stijn Verdonck, Kristof Meers, Tim Loossens, and Francis Tuerlinckx. Prepaid
parameter estimation without likelihoods. PLoS Computational Biology, 15(9):e1007181, 2019.

[183] Petrus Mikkola, Osvaldo A Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen
Thomas, Henri Pesonen, Jukka Corander, Aki Vehtari, Samuel Kaski, B¨urkner, Paul-Christian, and
Klami, Arto. Prior knowledge elicitation: The past, present, and future. arXiv preprint, 2021.

[184] Tim Miller. Explanation in artiﬁcial

intelligence:

Intelligence, 267:1–38, 2019.
//www.sciencedirect.com/science/article/pii/S0004370218305988.

ISSN 0004-3702.

Insights from the social sciences. Artiﬁcial
doi: 10.1016/j.artint.2018.07.007. URL https:

[185] Thomas P Minka. Expectation propagation for approximate bayesian inference. arXiv preprint, 2013.

[186] Christoph Molnar. Interpretable machine learning. Lulu. com, 2020.

[187] Stephen L Morgan and Christopher Winship. Counterfactuals and causal inference. Cambridge Uni-

versity Press, 2015.

[188] Mitzi Morris, Katherine Wheeler-Martin, Daniel Simpson, Stephen J Mooney, Andrew Gelman, and
Charles DiMaggio. Bayesian hierarchical spatial models: Implementing the Besag York Molli´e model
in Stan. Spatial and Spatio-Temporal Epidemiology, 31:1–18, 2019.

60

[189] Alfred M¨uller.

Integral probability metrics and their generating classes of functions. Advances in

Applied Probability, 29(2):429–443, 1997.

[190] Ladislas Nalborczyk, Paul-Christian B¨urkner, and Donald R Williams. Pragmatism should not be
a substitute for statistical literacy, a commentary on Albers, Kiers, and van Ravenzwaaij (2018).
Collabra: Psychology, 5(1), 2019.

[191] Radford M Neal. Mcmc using hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo,

pages 139–188. Chapman and Hall/CRC, 2011.

[192] John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the Royal

Statistical Society: Series A (General), 135(3):370–384, 1972.

[193] Denis Noble. A theory of biological relativity: no privileged level of causation. Interface focus, 2(1):

55–64, 2012.

[194] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.

[195] Alexander Nussbaumer, Andrew Pope, and Karen Neville. A framework for applying ethics-by-design

to decision support systems for emergency management. Information Systems Journal, 2021.

[196] Anthony O’Hagan. Fractional Bayes factors for model comparison. Journal of the Royal Statistical

Society: Series B (Methodological), 57(1):99–118, 1995.

[197] Anthony O’Hagan. Expert Knowledge Elicitation: Subjective but Scientiﬁc. The American Statistician,

2019.

[198] Manfred Opper and Ole Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms. Neural

computation, 12(11):2655–2684, 2000.

[199] Steven J Osterlind and Howard T Everson. Diﬀerential Item Functioning, volume 161. Sage, 2009.

[200] Topi Paananen, Juho Piironen, Paul-Christian B¨urkner, and Aki Vehtari.

portance sampling. Statistics and Computing, 31(2):16, 2021.
s11222-020-09982-2. URL https://doi.org/10.1007/s11222-020-09982-2.

Implicitly adaptive im-
ISSN 1573-1375. doi: 10.1007/

[201] Lorenzo Pacchiardi and Ritabrata Dutta. Generalized bayesian likelihood-free inference using scoring

rules estimators. arXiv preprint, 2021.

[202] Brooks Paige and Frank Wood. Inference networks for sequential Monte Carlo in graphical models.

International Conference on Machine Learning, 48:3040–3049, 2016.

[203] Stefano Palminteri, Valentin Wyart, and Etienne Koechlin. The importance of falsiﬁcation in compu-

tational cognitive modeling. Trends in cognitive sciences, 21(6):425–433, 2017.

[204] George Papamakarios and Iain Murray. Fast ε-free inference of simulation models with bayesian condi-
tional density estimation. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pages 1036–1044, 2016.

[205] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for
Information Processing Systems, volume 30.
URL https://proceedings.neurips.cc/paper/2017/hash/

Density Estimation.
Inc., 2017.
Curran Associates,
6c1da886822c67822bcf3679d04369fa-Abstract.html.

In Advances in Neural

[206] George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-
free inference with autoregressive ﬂows. In The 22nd International Conference on Artiﬁcial Intelligence
and Statistics, pages 837–848. PMLR, 2019.

61

[207] George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing ﬂows for probabilistic modeling and inference. J. Mach. Learn. Res., 22
(57):1–64, 2021.

[208] Parliament and Council of the European Union. General data protection regulation, 2016. URL

http://data.europa.eu/eli/reg/2016/679/oj.

[209] Matthew Parno, Tarek Moselhy, and Youssef Marzouk. A multiscale strategy for bayesian inference
using transport maps. SIAM/ASA Journal on Uncertainty Quantiﬁcation, 4(1):1160–1190, 2016.

[210] Federico Pavone, Juho Piironen, Paul-Christian B¨urkner, and Aki Vehtari. Using reference models in
variable selection. Computational Statistics, May 2022. ISSN 0943-4062, 1613-9658. doi: 10.1007/
s00180-022-01231-6. URL https://link.springer.com/10.1007/s00180-022-01231-6.

[211] Judea Pearl. Causality. Cambridge University Press, 2009.

[212] Judea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96–146, 2009.

[213] Judea Pearl. The do-calculus revisited. arXiv preprint, 2012.

[214] Judea Pearl. The seven tools of causal inference, with reﬂections on machine learning. Communications

of the ACM, 62(3):54–60, 2019.

[215] Steven T Piantadosi. One parameter is always enough. AIP Advances, 8(9):095118, 2018.

[216] Juho Piironen and Aki Vehtari. Sparsity information and regularization in the horseshoe and other

shrinkage priors. Electronic Journal of Statistics, 11(2):5018–5051, 2017.

[217] Martyn Plummer et al. Jags: A program for analysis of bayesian graphical models using gibbs sampling.
In Proceedings of the 3rd international workshop on distributed statistical computing, volume 124, pages
1–10. Vienna, Austria, 2003.

[218] Jonathan K Pritchard, Mark T Seielstad, Anna Perez-Lezaun, and Marcus W Feldman. Population
growth of human y chromosomes: a study of y chromosome microsatellites. Molecular biology and
evolution, 16(12):1791–1798, 1999.

[219] CJ P´erez, Jacinto Mart´ın, and Mar´ıa Jes´us Rufo. MCMC-based local parametric sensitivity estima-

tions. Computational Statistics & Data Analysis, 51(2):823–835, 2006.

[220] Stefan T Radev, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich K¨othe. Bayesﬂow:
Learning complex stochastic models with invertible neural networks. IEEE Transactions on Neural
Networks and Learning Systems, 2020.

[221] Stefan T Radev, Andreas Voss, Eva Marie Wieschen, and Paul-Christian B¨urkner. Amortized Bayesian

Inference for Models of Cognition. International Conference on Cognitive Modelling (ICCM), 2020.

[222] Stefan T Radev, Marco D’Alessandro, Ulf K Mertens, Andreas Voss, Ullrich K¨othe, and Paul-Christian
B¨urkner. Amortized Bayesian model comparison with evidential deep learning. IEEE Transactions on
Neural Networks and Learning Systems, 2021.

[223] Stefan T Radev, Frederik Graw, Simiao Chen, Nico T Mutters, Vanessa M Eichel, Till B¨arnighausen,
and Ullrich K¨othe. Outbreakﬂow: Model-based bayesian inference of disease outbreak dynamics with
invertible neural networks and its application to the covid-19 pandemics in germany. PLoS Computa-
tional Biology, 17(10):e1009472, 2021.

[224] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradi-
ent langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674–1703.
PMLR, 2017.

62

[225] M Raissi. Physics-informed neural networks: A deep learning framework for solving forward and
inverse problems involving nonlinear partial diﬀerential equations. Journal of Computational Physics,
page 22, 2019.

[226] Rajesh Ranganath, Sean Gerrish, and David Blei. Black Box Variational Inference. In Proceedings
of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, pages 814–822.
PMLR, 2014. URL https://proceedings.mlr.press/v33/ranganath14.html.

[227] Carl Edward Rasmussen. Gaussian processes in machine learning.

In Summer school on machine

learning, pages 63–71. Springer, 2003.

[228] Louis Raynal, Jean-Michel Marin, Pierre Pudlo, Mathieu Ribatet, Christian P Robert, and Arnaud
Estoup. Abc random forests for bayesian parameter inference. Bioinformatics, 35(10):1720–1728, 2019.

[229] Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.

[230] Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods, volume 2.

Springer, 1999.

[231] Ma\lgorzata Roos, Thiago G Martins, Leonhard Held, and H˚avard Rue. Sensitivity analysis for

Bayesian hierarchical models. Bayesian Analysis, 10(2):321–349, 2015.

[232] Jonathan Rothwell. How the war on drugs damages black social mobility. The Brookings Institution,

2014.

[233] Donald B. Rubin. Bayesianly Justiﬁable and Relevant Frequency Calculations for the Applied
ISSN 0090-5364. URL https:

Statistician. The Annals of Statistics, 12(4):1151–1172, 1984.
//www.jstor.org/stable/2240995.

[234] Havard Rue and Leonhard Held. Gaussian Markov Random Fields: Theory and Applications. Chapman

and Hall/CRC, 2005. ISBN 978-0-429-20882-9. doi: 10.1201/9780203492024.

[235] H˚avard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian
models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 71(2):319–392, 2009.

[236] H˚avard Rue, Andrea Riebler, Sigrunn H Sørbye, Janine B Illian, Daniel P Simpson, and Finn K
Lindgren. Bayesian computing with INLA: a review. Annual Review of Statistics and Its Application,
4:395–421, 2017.

[237] John Rust and Susan Golombok. Modern Psychometrics: The Science of Psychological Assessment

(3rd edition). Routledge, 2014. ISBN 978-1-315-78752-7. doi: 10.4324/9781315787527.

[238] Ernesto San Mart´ın. Identiﬁability of structural characteristics: How relevant is it for the bayesian

approach? Brazilian Journal of Probability and Statistics, 32(2):346–373, 2018.

[239] Ernesto San Martın and Jorge Gonz´alez. Bayesian identiﬁability: Contributions to an inconclusive

debate. Chilean Journal of Statistics, 1(2):69–91, 2010.

[240] Daniel J Schad, Michael Betancourt, and Shravan Vasishth. Toward a principled bayesian workﬂow in

cognitive science. Psychological Methods, 26(1):103, 2021.

[241] Daniel J Schad, Bruno Nicenboim, Paul-Christian B¨urkner, Michael Betancourt, and Shravan Vasishth.

Workﬂow Techniques for the Robust Use of Bayes Factors. Psychological Methods, 2021.

[242] Marvin Schmitt, Paul-Christian B¨urkner, Ullrich K¨othe, and Stefan T Radev. Detecting model mis-

speciﬁcation in amortized bayesian inference with neural networks. arXiv preprint, 2021.

63

[243] Maximilian Scholz and Paul-Christian Burkner. Prediction as a proxy for explanation? A simulation

study of Bayesian generalized additive models. arXiv preprint, 2022.

[244] Heinz Georg Schuster and Wolfram Just. Deterministic chaos: an introduction. John Wiley & Sons,

2006.

[245] Sagar Sharma, Simone Sharma, and Anidhya Athaiya. Activation functions in neural networks. To-

wards Data Science, 6(12):310–316, 2017.

[246] Galit Shmueli. To explain or to predict? Statistical Science, 25(3):289–310, 2010.

[247] Scott A Sisson, Yanan Fan, and Mark M Tanaka. Sequential monte carlo without likelihoods. Pro-

ceedings of the National Academy of Sciences, 104(6):1760–1765, 2007.

[248] Didier Sornette. Why stock markets crash. In Why Stock Markets Crash. Princeton University Press,

2009.

[249] David J Spiegelhalter, Nicola G Best, Bradley P Carlin, and A Van der Linde. Bayesian deviance, the
eﬀective number of parameters, and the comparison of arbitrarily complex models. Technical report,
Citeseer, 1998.

[250] Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological

advances. In Applied informatics, volume 3, pages 1–28. SpringerOpen, 2016.

[251] Sebastian Springer, Heikki Haario, Jouni Susiluoto, Aleksandr Bibov, Andrew Davis, and Youssef
Marzouk. Eﬃcient bayesian inference for large chaotic dynamical systems. Geoscientiﬁc Model Devel-
opment, 14(7):4319–4333, 2021.

[252] Stan Development Team. Stan modeling language users guide and reference manual, version 2.30,

2022. URL https://mc-stan.org.

[253] Mervyn Stone. Cross-validation: A review. Statistics: A Journal of Theoretical and Applied Statistics,

9(1):127–139, 1978.

[254] Laura S Storch, James M Pringle, Karen E Alexander, and David O Jones. Revisiting the logistic map:
a closer look at the dynamics of a classic chaotic population model with ecologically realistic spatial
structure and dispersal. Theoretical Population Biology, 114:10–18, 2017.

[255] Mikael Sunn˚aker, Alberto Giovanni Busetto, Elina Numminen, Jukka Corander, Matthieu Foll, and
Christophe Dessimoz. Approximate Bayesian Computation. PLOS Computational Biology, 9(1):
e1002803, 2013. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1002803. URL https://journals.plos.
org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803.

[256] Teemu S¨ailynoja, Paul-Christian B¨urkner, and Aki Vehtari. Graphical test for discrete uniformity
and its applications in goodness-of-ﬁt evaluation and multiple sample comparison. Statistics and
Computing, 32(2):32, March 2022. ISSN 1573-1375. doi: 10.1007/s11222-022-10090-6. URL https:
//doi.org/10.1007/s11222-022-10090-6.

[257] Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. Validating
arXiv preprint, 2018. URL

Bayesian Inference Algorithms with Simulation-Based Calibration.
http://arxiv.org/abs/1804.06788.

[258] Simon Tavar´e, David J Balding, Robert C Griﬃths, and Peter Donnelly. Inferring coalescence times

from dna sequence data. Genetics, 145(2):505–518, 1997.

[259] Peter F Thall and Stephen C Vail. Some covariance models for longitudinal count data with overdis-

persion. Biometrics, pages 657–671, 1990.

64

[260] Evan Thompson and Francisco J Varela. Radical embodiment: neural dynamics and consciousness.

Trends in Cognitive Sciences, 5(10):418–425, 2001.

[261] Brandon M Turner and Per B Sederberg. A generalized, likelihood-free method for posterior estimation.

Psychonomic bulletin & review, 21(2):227–250, 2014.

[262] Wim J van der Linden and Ronald K Hambleton. Handbook of Modern Item Response Theory. Springer-

Verlag, 1997.

[263] Arjan van der Schaft. Port-Hamiltonian systems: an introductory survey. In Marta Sanz-Sol´e, Javier
Soria, Juan Luis Varona, and Joan Verdera, editors, Proceedings of the International Congress of
Mathematicians Madrid, August 22–30, 2006, pages 1339–1365. European Mathematical Society Pub-
lishing House, Zuerich, Switzerland, 2007.
ISBN 978-3-03719-022-7. doi: 10.4171/022-3/65. URL
http://www.ems-ph.org/doi/10.4171/022-3/65.

[264] Tyler VanderWeele. Explanation in causal inference: methods for mediation and interaction. Oxford

University Press, 2015.

[265] Aki Vehtari.

Comparison
https://avehtari.github.io/rhat ess/ess comparison.html.

of MCMC eﬀective

sample

size

estimators,

2021.

[266] Aki Vehtari and Janne Ojanen. A survey of Bayesian predictive methods for model assessment, selection

and comparison. Statistics Surveys, 6:142–228, 2012.

[267] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical Bayesian model evaluation using leave-one-
out cross-validation and WAIC. Statistics and Computing, 27(5):1413–1432, 2017. ISSN 0960-3174,
1573-1375. doi: 10.1007/s11222-016-9696-4. URL http://arxiv.org/abs/1507.04544.

[268] Aki Vehtari, Andrew Gelman, Tuomas Sivula, Pasi Jyl¨anki, Dustin Tran, Swupnil Sahai, Paul Blomst-
edt, John P Cunningham, David Schiminovich, and Christian P Robert. Expectation Propagation as
a Way of Life: A Framework for Bayesian Inference on Partitioned Data. Jorunal of Machine Learning
Research, 21(17):1–53, 2020.

[269] Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian B¨urkner. Rank-
normalization, folding, and localization: An improved (cid:98)R for assessing convergence of MCMC. Bayesian
Analysis, 16(2), 2021. ISSN 1936-0975. doi: 10.1214/20-BA1221. URL http://arxiv.org/abs/1903.
08008.

[270] Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto smoothed

importance sampling. arXiv preprint, 2021.

[271] A Helen Victoria and G Maragatham. Automatic tuning of hyperparameters using bayesian optimiza-

tion. Evolving Systems, 12(1):217–223, 2021.

[272] Jaume Vives, Josep-Maria Losilla, and Mar´ıa-Florencia Rodrigo. Count data in psychological applied

research. Psychological Reports, 98(3):821–835, 2006.

[273] Mischa von Krause, Stefan T Radev, and Andreas Voss. Mental speed is high until age 60 as revealed

by analysis of over a million participants. Nature Human Behaviour, pages 1–9, 2022.

[274] William A Wagenaar and Sabato D Sagaria. Misperception of exponential growth. Perception &

Psychophysics, 18(6):416–422, 1975.

[275] Eric-Jan Wagenmakers, Alexandra Sarafoglou, and Balazs Aczel. One statistical analysis must not

rule them all, 2022.

65

[276] Michael Wainberg, Daniele Merico, Matthew C. Keller, Eric B. Fauman, and Shreejoy J. Tripathy.
Predicting causal genes from psychiatric genome-wide association studies using high-level etiological
knowledge. Molecular Psychiatry, 27(7):3095–3106, 2022. ISSN 1359-4184, 1476-5578. doi: 10.1038/
s41380-022-01542-6. URL https://www.nature.com/articles/s41380-022-01542-6.

[277] Sumio Watanabe. Algebraic geometry and statistical learning theory. Cambridge University Press,

2009.

[278] Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely
applicable information criterion in singular learning theory. Journal of machine learning research, 11
(12), 2010.

[279] Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, and Jonathan H. Huggins. Robust,
arXiv preprint, 2022. URL http:

Automated, and Accurate Black-box Variational Inference.
//arxiv.org/abs/2203.15945.

[280] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for regression. In Advances

in neural information processing systems, pages 514–520, 1996.

[281] Donald R Williams, Rickard Carlsson, and Paul-Christian B¨urkner. Between-litter variation in devel-
opmental studies of hormones and behavior: Inﬂated false positives and diminished power. Frontiers
in Neuroendocrinology, 47:154–166, 2017.

[282] Bodo Winter and Paul-Christian B¨urkner. Poisson regression for linguists: A tutorial introduction to

modelling count data with brms. Language and Linguistics Compass, 15(11):e12439, 2021.

[283] Samuel Wiqvist, Jes Frellsen, and Umberto Picchini. Sequential neural posterior and likelihood ap-

proximation. arXiv preprint, 2021.

[284] Simon N Wood. Thin plate regression splines. Journal of the Royal Statistical Society: Series B

(Statistical Methodology), 65(1):95–114, 2003.

[285] Dorothy Wrinch and Harold Jeﬀreys. On some aspects of the theory of probability. The London,
Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 38(228):715–731, 1919.

[286] Jian-Bo Yang, Kai-Quan Shen, Chong-Jin Ong, and Xiao-Ping Li. Feature selection for mlp neural
IEEE Transactions on Neural

network: The use of random permutation of probabilistic outputs.
Networks, 20(12):1911–1922, 2009.

[287] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but Did It Work?: Evaluating

Variational Inference. Proceedings of Machine Learning Research, 80:5581–5590, 2018.

[288] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using stacking to average Bayesian

predictive distributions (with discussion). Bayesian Analysis, 13(3):917–1007, 2018.

[289] Tal Yarkoni and Jacob Westfall. Choosing prediction over explanation in psychology: Lessons from

machine learning. Perspectives on Psychological Science, 12(6):1100–1122, 2017.

[290] Anderson Y Zhang and Harrison H Zhou. Theoretical and computational guarantees of mean ﬁeld
variational inference for community detection. The Annals of Statistics, 48(5):2575–2598, 2020.

[291] Fengshuo Zhang and Chao Gao. Convergence rates of variational posterior distributions. The Annals

of Statistics, 48(4):2180–2207, 2020.

[292] Quan-shi Zhang and Song-chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of
Information Technology & Electronic Engineering, 19(1):27–39, 2018. ISSN 2095-9230. doi: 10.1631/
FITEE.1700808. URL https://doi.org/10.1631/FITEE.1700808.

66

[293] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable Convolutional Neural Networks.
In Computer Vision and Pattern Recognition Conference Proceedings, pages 8827–8836, 2018.

[294] Yan Dora Zhang, Brian P Naughton, Howard D Bondell, and Brian J Reich. Bayesian regression using
a prior on the model ﬁt: The r2-d2 shrinkage prior. Journal of the American Statistical Association,
pages 1–13, 2020.

[295] Yan Zhou, Adam M. Johansen, and John A.D. Aston. Toward Automatic Model Comparison: An
Adaptive Sequential Monte Carlo Approach. Journal of Computational and Graphical Statistics, 25
(3):701–726, 2016. ISSN 1061-8600. doi: 10.1080/10618600.2015.1060885. URL https://doi.org/
10.1080/10618600.2015.1060885.

67

