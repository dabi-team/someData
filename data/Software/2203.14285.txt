2
2
0
2

r
a

M
7
2

]
E
S
.
s
c
[

1
v
5
8
2
4
1
.
3
0
2
2
:
v
i
X
r
a

HELoC: Hierarchical Contrastive Learning of Source Code
Representation

Xiao Wang
Shandong Normal University
Jinan, China

Chen Lyu∗
Shandong Normal University
Jinan, China

Qiong Wu
Shandong Normal University
Jinan, China

Xue Jiang
Shandong Normal University
Jinan, China

Hongyu Zhang
The University of Newcastle
NSW, Australia

Zhuoran Zheng
Nanjing University of Science and
Technology
Nanjing, China

Lei Lyu
Shandong Normal University
Jinan, China

Songlin Hu
Institute of Information Engineering,
Chinese Academy of Sciences
Beijing, China

ABSTRACT
Abstract syntax trees (ASTs) play a crucial role in source code rep-
resentation. However, due to the large number of nodes in an AST
and the typically deep AST hierarchy, it is challenging to learn the
hierarchical structure of an AST effectively. In this paper, we pro-
pose HELoC, a hierarchical contrastive learning model for source
code representation. To effectively learn the AST hierarchy, we use
contrastive learning to allow the network to predict the AST node
level and learn the hierarchical relationships between nodes in a
self-supervised manner, which makes the representation vectors
of nodes with greater differences in AST levels farther apart in the
embedding space. By using such vectors, the structural similari-
ties between code snippets can be measured more precisely. In the
learning process, a novel GNN (called Residual Self-attention Graph
Neural Network, RSGNN) is designed, which enables HELoC to
focus on embedding the local structure of an AST while capturing
its overall structure. HELoC is self-supervised and can be applied to
many source code related downstream tasks such as code classifi-
cation, code clone detection, and code clustering after pre-training.
Our extensive experiments demonstrate that HELoC outperforms
the state-of-the-art source code representation models.

CCS CONCEPTS
• Software and its engineering → Software maintenance tools.

KEYWORDS
Abstract Syntax Tree, Contrastive Learning, Code Representation

∗Corresponding author. Email: lvchen@sdnu.edu.cn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ’22, May 16–17, 2022, Virtual Event, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3527896

ACM Reference Format:
Xiao Wang, Qiong Wu, Hongyu Zhang, Chen Lyu, Xue Jiang, Zhuoran
Zheng, Lei Lyu, and Songlin Hu. 2022. HELoC: Hierarchical Contrastive
Learning of Source Code Representation. In 30th International Conference
on Program Comprehension (ICPC ’22), May 16–17, 2022, Virtual Event, USA.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3524610.3527896

1 INTRODUCTION
In recent years, many deep learning based approaches to software
engineering tasks, such as code retrieval [1, 2], code clone detection
[3–6], code comment generation [7–9], software defect prediction
[10–12], and code summarization [13–16], have emerged to improve
software development and maintenance. These approaches require
effective learning of the syntax and semantics of source code to
obtain a proper neural code representation. In current research
on source code representation, some approaches split the source
code into tokens and encode the token sequence based on the CNN
[17, 18] and RNN [19, 20] paradigms. However, the limitation of the
token sequence is that it cannot represent the structural features of
source code.

AST (Abstract Syntax Tree), as a tree representation of source
code, aims at representing the syntactic structure of source code.
Many recent methods [21–26] encode ASTs by leveraging GNNs
to learn the structural features of source code. They obtain the
code representation by aggregating nodes in an AST together based
on the parent-child connections so that the relationships between
the adjacent levels can be captured. However, the relationships
between non-adjacent levels of AST nodes are not given sufficient
attention. We believe that the relationships between both adjacent
and non-adjacent levels are indispensable to represent the overall
topology of an AST. A major limitation of the existing methods
is that they are difficult to fully understand and encode the AST
hierarchy since some hierarchical relationships are not considered
(we will describe more about the limitation of the existing methods
in Section 2). Therefore, a main challenge is how to effectively learn
the hierarchical topology of ASTs? This challenge is more acute when
the ASTs have deep levels, complex nesting structures, and a large
number of nodes. Techniques that can effectively represent such a

 
 
 
 
 
 
ICPC ’22, May 16–17, 2022, Virtual Event, USA

Wang, et al.

topology in the embedding space can help neural networks better
understand the AST representation of source code.

In this paper, we present HELoC (short for Hierarchical Con-
strastivE Learning of Code), a hierarchical contrastive learning
model for source code representation. To address the challenge
described above, we propose the following novel techniques:

Novelty 1: Contrastive learning for AST hierarchy. We gain
inspiration from the self-supervised contrastive learning (SCL) par-
adigm [27], and discover ways to explicitly preserve the AST hi-
erarchy in the embedding space by formulating hierarchical rela-
tionships as distances between node vectors. Traditional sequence-
and path-based techniques [8, 17, 28–30] can compute the static
discrete distance values (e.g., sequence distance or shortest path
length) directly from the code and its AST. In contrast, our goal is
to design a hierarchical contrastive learning encoder for the AST
hierarchy. Therefore, two learning objectives are employed to train
HELoC to predict the AST node level and learn the three hierarchi-
cal relationships between nodes, which bring the representation
vectors of nodes with greater differences in AST levels farther apart
in the embedding space.

Novelty 2: A specialized GNN for AST hierarchical struc-
tures. ASTs are typically characterised by multi-node, multi-level
and multi-nested structures. Previous source code representations
[31, 32] have demonstrated that GNNs can better encode the local
structure of ASTs [33]. However, it is difficult for GNNs to effec-
tively model the long-range dependencies (i.e., global relationships)
between nodes of a deep AST. In this work, we design a novel GNN
called RSGNN (stands for Residual Self-Attention GNN), which
utilizes both graph convolutional networks (GCNs) (for capturing
local structure) and the self-attention mechanism (for capturing
global structure) to learn AST hierarchy comprehensively.

We leverage these novel techniques to develop HELoC, a self-
supervised, contrastive learning model for source code representa-
tion. In HELoC, RSGNN is used as an AST hierarchy encoder and
pre-trained under the hierarchical contrastive learning objectives.
During pre-training, HELoC learns the node representation and
the path representation of the AST and then feeds the combined
representation into the RSGNN network. The pre-trained RSGNN
can then be used to obtain comprehensive representations of source
code in downstream tasks.

We apply our model to three software engineering tasks: code
classification, code clone detection, and unsupervised code cluster-
ing, and conduct extensive experiments on multiple datasets. The
experimental results show that our approach outperforms the re-
spective state-of-the-art models. For code classification, our model
improves the accuracy to 97.2% on GCJ datasets, which is 3.8%
higher than the current state-of-the-art model (ASTNN [24]). For
code clone detection, our model improves the F1 scores to 98.0%
and 97.4% on BCB and GCJ datasets, which is 2.6% and 4.3% higher
than the current state-of-the-art model (InferCode [4], FCCA [3]),
respectively. For code clustering, HELoC achieves relative improve-
ments of 12.1% and 12.6% in ARI metric on OJ and SA datasets
when compared to the state-of-the-art method (Infercode [4]). Our
ablation study results also suggest that all components of HELoC
are effective.

Our major contributions are summarized as follows:

• We propose a novel AST hierarchy representation method
based on self-supervised hierarchical constrastive learning,
which exploits the topological relationships between AST
nodes and learn the structures of ASTs more accurately.
• We present a specialized GNN by constructing internal and
external residual self-attention mechanisms, which can cap-
ture the AST hierarchy comprehensively.

• Our extensive experiments on three downstream tasks con-
firm the effectiveness of the proposed approach to neural
code representation.

2 BACKGROUND AND MOTIVATION
2.1 AST Hierarchy
ASTs are designed to represent the syntactic structure of source
code, and it is significant to learn the structural features of ASTs
effectively for code representation. An example of AST is shown
in Figure 1. The code snippet implements the bubble sort function.
For presentation purpose, we show only a partial AST of the code.
We consider AST as a planar structure and dissect its struc-
tural features in horizontal hierarchy and vertical paths dimensions.
Therefore, the AST hierarchy will act as a fundamental feature to
study. Explicitly, the definition of AST hierarchy includes two as-
pects: the level of AST nodes and three relationships between node
levels. Level refers to the unique path length from the root to a node.
The three types of hierarchical relationships include: 1) Neighbor-
hood – topological relationships between nodes at the same level;
2) Adjacent Hierarchy – topological relationships between nodes
with a hierarchical difference of 1 and 3) Non-Adjacent Hierar-
chy – topological relationships between nodes with hierarchical
differences greater than 1. As the AST shown in Figure 1, the level
of nodes 𝐴, 𝐵, 𝐶 is 𝑙 = 2, node 𝐷 is 𝑙 = 3, and node 𝐸 is 𝑙 = 4, where
the relationship between nodes 𝐴, 𝐵, 𝐶 is neighborhood, nodes 𝐴,
𝐷 is adjacent hierarchy, and nodes 𝐴, 𝐸 is non-adjacent hierarchy.

2.2 AST-based Representation Models for

Source Code

How to represent source code effectively is a fundamental problem
in deep learning for software engineering research. Recently, many
AST-based source code representations have been proposed, which
can be roughly divided into two categories: supervised and self-
supervised models.

Supervised models. Most models of code representation employ
a supervised learning paradigm where a specific task (e.g., code clas-
sification) is used as the training target, and the code representation
is learned based on human-annotated labels. For example, TBCNN
[25] performs supervised learning to obtain code representations
by convolving the parent and child nodes through a tree-based
convolution kernel, applied to supervised tasks of code classifica-
tion and similarity detection. HAG [26] aims to distinguish the
importance of different nodes by weighted aggregated parent-child
nodes in a supervised manner and is used for code clone detection
tasks. ASTNN [24] divides each large AST into a sequence of small
statement trees and encodes the statement trees by using bottom-
up computations from the leaf nodes to the root node to obtain
the source code representation, applied to supervised tasks of code
classification and code clone detection.

HELoC: Hierarchical Contrastive Learning of Source Code Representation

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Figure 1: A code snippet and its partial AST.

Self-Supervised models. Self-supervised learning aims to ben-
efit downstream tasks by designing pretext tasks to learn from
large-scale unlabeled data. The most representative studies include
InferCode [4], Corder [34], and Contracode [35]. InferCode first
decomposes the AST of the code into subtrees of unequal granu-
larity, then encodes the nodes of the subtrees with the improved
TBCNN. It learns the code representation by predicting subtrees
for the pretext task. Corder enhances the self-supervised frame-
work by recognizing similar and dissimilar code snippets via intro-
ducing a contrastive learning mechanism. It is mainly applied to
code retrieval and code summarization. Contracode uses source-
to-source compiler transformation techniques to generate syntac-
tically different but functionally similar programs for code-level
contrastive learning, making the representations of semantically
equivalent snippets close to each other, while the representations
of non-similar snippets are far apart.

Progress has also been made in incorporating ASTs into the
learning process through self-supervised learning with pre-training
models. TreeBERT [36] and GraphCodeBERT [37] have made good
progress as BERT-based models. These models propose novel pre-
training tasks to learn the structure of ASTs (e.g., path and data
dependencies) to capture more structural information of ASTs.

2.3 The Limitations of the Existing Work
The two types of source code representation models introduced in
previous subsection have four major limitations. 1) Heavy reliance
on limited labelled data. Methods using the supervised learning
paradigm rely on human-labelled data. They cannot benefit from
the code knowledge embedded in large amounts of unlabelled data,
whereas existing self-supervised models effectively alleviate this
problem. 2) Inadequate learning of AST hierarchy. Techniques
(such as ASTNN, TBCNN, and HAG) learn the AST structure by
aggregating child nodes to their parent node, focusing only on the
adjacent hierarchy. Consequently, topologically non-adjacent hi-
erarchy between nodes (sometimes spanning multiple levels) are

difficult to learn, resulting in such relationships in the embedding
space not being accurately represented. Similarly, it is also difficult
for existing BERT-based models to learn AST hierarchy. 3) Broken
semantics. As an AST may have many levels and a large number
of nodes, path- and subtree-based techniques, such as Code2vec,
Code2seq and Infercode, divide the AST into either a bag of path con-
texts or sub-trees. However, decomposition of the AST into smaller
ones may lose long-range context information, which destroys the
overall semantic relationships of the AST and leads to a partial
understanding of the overall structure of the code. 4) Extra train-
ing data effort. Corder and Contracode mitigate the problem of
broken semantics by applying contrastive learning to syntactically
different but semantically similar code snippets. However, this way
of obtaining negative code-level samples for contrastive learning is
limited by the capacity of the program transformation techniques
and the availability of the datasets consisting of similar semantic
code snippets. Actually, program transformation techniques cannot
exhaust all semantically similar code snippets, and it is not easy to
obtain sufficient similar semantic code snippets in practice. There-
fore, it is necessary to design a generic model of code representation
to enable more comprehensive and autonomous learning.

In this paper, we formulate the learning of AST hierarchy as a
pretext task of self-supervised contrastive learning, where cross-
entropy and triplet losses are adopted as learning objectives to
predict the node level and learn the hierarchical relationships be-
tween nodes. Being a self-supervised method, our model does not
require synthetic negative code samples and decomposition of the
ASTs.

3 PROPOSED MODEL
The overall architecture of HELoC is shown in Figure 2. The model
accepts the entire AST as input. RSGNN is then used to encode
the AST hierarchy and trained with a joint loss for hierarchical
contrastive learning.

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Wang, et al.

the number of path, and 𝑥𝑝𝑖 𝑗 represents the embedding assigned
by 𝑖-th path to the 𝑗-th node in the path.

Finally, the embedding of one path are added to the correspond-
ing node embedding to obtain the augmented node embedding con-
(cid:9) ∈
taining the respective path information 𝑋 𝐴𝑆𝑇
𝑅𝑁 ×𝐻 .

= (cid:8)˜𝑥𝑛1

, ..., ˜𝑥𝑛𝑁

, ˜𝑥𝑛2

0

3.2 RSGNN (Residual Self-Attention GNN)
GNN-based representations tend to be local and struggle to lever-
age long-range interactions. On the contrary, self-attention allows
global information flow at each step. Therefore, we combine the
advantages of both in the design of RSGNN and add residual con-
nection to alleviate gradient vanishing due to encountering many
iterations on deep AST.

RSGNN consists of 𝑁𝐷 layers, where each layer contains two
different sub-layers, i.e., a residual self-attention mechanism sub-
layer (𝑅𝑆𝑀𝑠𝑢𝑏 ) and a graph convolution sub-layer (𝐺𝐶𝑁𝑠𝑢𝑏 ). We
add residual connection and layer normalization between the sub-
layers.

Figure 2: An overall framework of the HELoC model. NEP
and NRO refer to the two pre-training objective functions
of HELoC.

The parsed AST (see Figure 1) contains two types of information:
1) information represented by the AST nodes, including the
textual type, the textual content, and the textual position (i.e., the
startLineNumber and the endLineNumber); and 2) information
represented by the edges between nodes, i.e., an edge from
node 𝑛𝑖 to node 𝑛 𝑗 . Note that we utilize the textual content in the
nodes because it enables us to extract the syntax of the code while
preserving its semantics. In addition, we also record the textual
position in each node within code snippet, which helps distinguish
between the embedding of the nodes for better contrastive learning.

3.1 Input Representation
We represent an AST with path augmented node embeddings, which
incorporate the embedding of the path where each node is located.
The details are as follows.

Node Embedding. We consider an AST node’s information (i.e.,
textual type, textual content and textual position) as a string, and
adopt a Document Embedding technique1 to embed the entire string.
(cid:9) ∈
The node embedding is denoted as 𝑋 𝑛𝑜𝑑𝑒
𝑅𝑁 ×𝐻 , where 𝑥𝑛𝑖 represents the embedding of node 𝑛𝑖 , 𝑁 is the
number of node, and 𝐻 is the embedding dimension. Furthermore,
we construct an adjacency matrix 𝐴 ∈ 𝑅𝑁 ×𝑁 to represent the
adjacency relationships between nodes, where 𝐴𝑗𝑖 = 1 if node 𝑛𝑖 is
the parent of 𝑛 𝑗 .

= (cid:8)𝑥𝑛1

, ..., 𝑥𝑛𝑁

, 𝑥𝑛2

0

Path Embedding. We first extract the paths of an AST from
the root node to each terminal node and use the same method
(i.e., Document Embedding) used for node embedding to obtain the
AST’s path initial embedding. An example of path (marked with a
thick orange line) is shown in Figure 1. The whole path embedding
𝑋 𝑝𝑎𝑡ℎ
(cid:9) ∈ 𝑅𝑀×𝐻 can be obtained, where 𝑀 is
0

, . . . , 𝑥𝑝𝑀

= (cid:8)𝑥𝑝1

, 𝑥𝑝2

1https://github.com/flairNLP/flair/

Residual self-attention mechanism sub-layer. The 𝑅𝑆𝑀𝑠𝑢𝑏
is described in Figure 3. First, we take 𝑋 𝐴𝑆𝑇
(the previous RSGNN’s
output) as input and perform graph convolution mappings on it to
compute the query, key, and value , i.e., Q, K, and V, respectively.
The first layer’s input is 𝑋 𝐴𝑆𝑇
and we formulate this process as
follows:

𝑁𝑑 −1

0

𝑄 = 𝐺𝐶𝑁

(cid:16)
𝑋 𝐴𝑆𝑇
𝑁𝑑 −1

(cid:17)
, ˜𝐴

(cid:16)

= 𝜎

𝐷−1 ˜𝐴𝑋 𝐴𝑆𝑇
𝑁𝑑 −1

𝑊𝑄

(cid:17)

(1)

where ˜𝐷𝑖𝑖 = (cid:205)𝑗 ˜A𝑖 𝑗 represents the diagonal matrix, and 𝑊𝑄 is the
learnable weight matrix. 𝐾 and 𝑉 are calculated as the same way.
During the network training, gradient vanishing sometimes oc-
curs due to too many stacked layers, and we solve this problem by
adding internal residual connection and external residual connec-
tion to self-attention mechanism.

We form the internal residual connection by adding the at-
tention score of the previous layer as one additional input to the
attention score calculation of the current layer.

𝐴𝑡𝑡𝑛 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥

(cid:18) 𝑄𝐾 ⊤
√
𝑑

(cid:19)

+ 𝑝𝑟𝑒𝑣

(2)

where 𝑝𝑟𝑒𝑣 represents the attention score of the previous layer, and
𝐴𝑡𝑡𝑛 will be passed to the next layer as its previous layer’s attention
score. Furthermore, we feed 𝐴𝑡𝑡𝑛 and 𝑉 to GCN and get the output
𝑋𝑎𝑡𝑡𝑛.

𝑋𝑎𝑡𝑡𝑛 = 𝐺𝐶𝑁 ((𝐴𝑡𝑡𝑛, 𝑉 ), ˜𝐴)

(cid:18)

= 𝜎

𝐷−1𝑉 ˜𝐴

(cid:18)
𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥

(cid:18) 𝑄𝐾 ⊤
√
𝑑

+ 𝑝𝑟𝑒𝑣

(cid:19) (cid:19)

(cid:19)

· 𝑊𝑙

(3)

where 𝑊𝑙 is the learnable weight matrix.

Finally, the external residual connection is introduced to ob-
tain the final output of 𝑅𝑆𝑀𝑠𝑢𝑏 . Specifically, we establish a residual
connection between the output 𝑋𝑎𝑡𝑡𝑛 of self-attention and the pre-
vious 𝑅𝑆𝑀𝑠𝑢𝑏 layer’s output.
(cid:16)
𝑋 AST
𝑁𝑑 −1

𝑁𝑑 −1 + 𝑋𝑎𝑡𝑡𝑛

= 𝑋 𝐴𝑆𝑇

𝑅𝑆𝑀𝑠𝑢𝑏

(4)

(cid:17)

Graph convolution sub-layer. 𝐺𝐶𝑁𝑠𝑢𝑏 consists of two GCNs

with a RELU activation in between.

HELoC: Hierarchical Contrastive Learning of Source Code Representation

ICPC ’22, May 16–17, 2022, Virtual Event, USA

embeddings is continuously enlarged as their hierarchical differ-
ences increase, thus achieving the goal of learning the intrinsic
topological features that distinguish the current AST from others.

We elaborate the above learning process as follows.
AST pseudo-label construction. To learn the hierarchical struc-
ture, we use the given AST’s level to construct pseudo-labels for
the nodes automatically without human annotations so that our
approach can be self-supervised. Specifically, we use the depth first
search (DFS) algorithm to traverse the AST nodes, labelling the
root node as 0, the first-level nodes as 1, the second-level nodes as
2, and so on, with the 𝑙-th-level nodes labeled as 𝑙 (see Figure 1).

Hierarchy representation learning. We pre-train HELoC by
predicting the AST hierarchy through contrastive learning. For
this purpose, we implement a single-layer linear neural network
as a classifier to predict the level of AST node and obtain: ^𝑦 =
(cid:17), where 𝑊𝐴𝑆𝑇 is the weight matrix, 𝑏𝐴𝑆𝑇 is
(cid:16)
𝑊𝐴𝑆𝑇 𝑋 𝐴𝑆𝑇
+ 𝑏𝐴𝑆𝑇
𝑁𝐷
the bias term, 𝑋 𝐴𝑆𝑇
refers to the node representation of the RSGNN
𝑁𝐷
output.

The pre-training objectives of HELoC include NEP and NRO.
NEP employs a cross-entropy loss function [39], denoted as Lℎ,
and NRO employs a triplet loss function [40], denoted as L𝑡 . We use
a joint loss function [41] for HELoC to automatically balance Lℎ
and L𝑡 during the learning process without human intervention,
which is defined as:

1
𝜃 2

1
𝜏 2

L ≈

Lℎ +

L𝑡 + log 𝜃 + log 𝜏
= exp (cid:0)−2𝜃 ′(cid:1) · Lℎ + exp (cid:0)−2𝜏 ′(cid:1) · L𝑡 + 𝜃 ′ + 𝜏 ′

𝑁
∑︁

𝑖=1

Lℎ =

(cid:169)
− log
(cid:173)
(cid:173)
(cid:171)
(cid:20)(cid:13)
𝑛𝑖 − ¯𝑥𝑝𝑜𝑠
(cid:13)¯𝑥𝑎𝑛𝑐
(cid:13)
𝑛𝑖

(cid:13)
2
(cid:13)
(cid:13)
2

exp

(cid:205)𝑗 exp

(cid:17)

(cid:16)
𝑦𝑙𝑒𝑣𝑒𝑙
𝑖
(cid:16)
𝑦𝑙𝑒𝑣𝑒𝑙
𝑗

(cid:17)

(cid:170)
(cid:174)
(cid:174)
(cid:172)
(cid:13)
2
(cid:13)
2

− (cid:13)

(cid:13)¯𝑥𝑎𝑛𝑐

𝑛𝑖 − ¯𝑥𝑛𝑒𝑔
𝑛𝑖

+ Δ𝑙 + 𝛼

L𝑡 =

𝑁
∑︁

𝑖=1

(6)

(7)

(8)

(cid:21)

+

𝑖

where 𝜃 and 𝜏 are learnable scalars and 𝑁 is the number of nodes in
an AST. 𝑦𝑙𝑒𝑣𝑒𝑙
is the probability that the predicted 𝑖-th node level is
𝑙. ⟨𝑎𝑛𝑐, 𝑝𝑜𝑠, 𝑛𝑒𝑔⟩ denotes a triplet, in which 𝑎𝑛𝑐 is the anchor node,
𝑝𝑜𝑠 is the positive node, 𝑛𝑒𝑔 is the negative node, and ¯𝑥 ( ·)
refers
𝑛𝑖
to the representation of the nodes of the constructed triplet. Δ𝑙
represents the difference in levels between the negative node and
the anchor node, aiming to make the representation of nodes with
more distant levels in an AST more distant in the embedding space.
∥∗∥2
is the Euclidean distance metric, where 𝛼 is a margin that is
2
enforced between positive and negative pairs. []+ indicates that if
the value within [] is greater than zero, the value is treated as a
loss; if it is less than zero, the loss is zero. Note that, for numerical
stability, we let 𝜃 ′ = log 𝜃, 𝜏 ′ = log 𝜏 and train HELoC to learn 𝜃 ′
and 𝜏 ′ instead of the unconstrained 𝜃 and 𝜏 [42].

To visualize the results of HELoC, we plot the t-distributed sto-
chastic neighbor embedding (t-SNE) [43] diagrams of the hierarchy
representation, as shown in Figure 4. We select the middle six levels
of an AST (from our pre-training Java-Large dataset described in
Section 5.1) as an example. Figure 4 (a-b) shows the output before
and after using HELoC (both using K-means clustering), respec-
tively. HELoC leads to the clustering of nodes in the same level,
while nodes with greater differences in AST levels farther apart in

Figure 3: Residual Self-attention Mechanism.

The output of the 𝑁𝑑 -th RSGNN layer is calculated as follows:

(cid:158)𝑋 AST
𝑁𝑑
𝑋 𝐴𝑆𝑇
𝑁𝑑

= LayerNorm (cid:16)
= LayerNorm (cid:16)

𝑋 AST
𝑁𝑑 −1 + 𝑅𝑆𝑀𝑠𝑢𝑏
(cid:16)
(cid:157)𝑋 AST
𝑁𝑑

+ 𝐺𝐶𝑁𝑠𝑢𝑏

(cid:157)𝑋 𝐴𝑆𝑇
𝑁𝑑

(cid:16)
𝑋 AST
𝑁𝑑 −1

(cid:17)(cid:17)

(cid:17)(cid:17)

, ˜𝐴

(5)

𝑁𝑑 −1

where 𝑋 𝐴𝑆𝑇
is the previous RSGNN’s output, ˜A = A + I𝑁 repre-
sents the adjacency matrix with added self-connections, I𝑁 is the
identity matrix.

After 𝑁𝐷 layers of RSGNN, we obtain the representation of the

AST nodes, which is denoted as 𝑋 𝐴𝑆𝑇
𝑁𝐷

= (cid:8)¯𝑥𝑛1

, ¯𝑥𝑛2

, ..., ¯𝑥𝑛𝑁

(cid:9) .

3.3 Hierarchical Contrastive Learning
In this work, we propose a novel hierarchical contrastive learning
(HCL) method, which takes the learning of AST hierarchy as the
pretext task [38] for contrastive learning. We use the construction
of triplets in Figure 1 to explain the idea of our HCL. Any AST nodes
can be considered an anchor node, e.g. node 𝐴. Nodes at the same
level as the anchor node are treated as the positive nodes, e.g., nodes
𝐵 and 𝐶, while nodes at other levels are treated as the negative nodes
(including those at the adjacent levels, e.g., node 𝐷, and those at the
non-adjacent levels, e.g., node 𝐸). On this basis, we can construct
the triplets (e.g., {𝐴, 𝐵, 𝐷 }, {𝐴, 𝐶, 𝐸}), as well as positive pairs (e.g.,
< 𝐴, 𝐵 >, < 𝐴, 𝐶 >) and negative pairs (e.g., < 𝐴, 𝐷 >, < 𝐴, 𝐸 >).
Contrastive learning encourages the representation of the anchor
node to be closer to the representations of the “positive” nodes,
while staying away from the representations of “negative” nodes.
For an AST hierarchy, the representations of nodes with greater
differences in AST levels should be farther apart in the embedding
space. For example, < 𝐴, 𝐸 > should be more distant than < 𝐴, 𝐷 >
as the nodes 𝐴 and 𝐸 have a larger difference in levels. However,
the original contrastive learning objective is insufficient for our
HCL to learn the AST hierarchy.

To address this issue, we take the AST levels as pseudo-labels
and adopt two objective functions: 1) node level prediction (NEP)
to predict the AST levels where the nodes are located, and 2) node
relationship optimization (NRO) to learn the three topological re-
lationships between the nodes. In particular, we need to consider
the following two aspects in NRO: one is to construct “positive"
and “negative" pairs according to the node hierarchy indicated by
the pseudo-labels; the other is to inject into the learning objective
the difference value between the level of the negative node and
that of the anchor node. In this way, the distance between node

ICPC ’22, May 16–17, 2022, Virtual Event, USA

the embedding space. This validates the ability of HELoC to learn
AST hierarchy information.

Figure 4: Visualization of the hidden vector of a six-level
AST. The points in the figure corresponds to the AST nodes.

4 APPLICATIONS OF THE PROPOSED MODEL
HELoC can be applied in two ways: 1) fine-tuning, where HELoC
acts as a pre-trained model and its pre-trained parameters can be
fine-tuned in supervised learning tasks, and 2) producing code vec-
tors, where HELoC serves as a feature extractor without fine-tuning
any parameters, and the resulting code representations are directly
used as input to a model that accomplishes a specific downstream
task.

Fine-tuning for Code Classification. Code classification refers
to classifying source code according to their functionalities. Given
the representation 𝑟 of a code snippet and its functionality label
𝑐𝑙𝑎, we can classify it via a fully-connected layer (with a total of
𝑐 classes). We define the loss function as the widely used cross-
entropy function:

𝐿𝑜𝑠𝑠𝑐𝑙𝑎 = − log

exp

(cid:205)𝑗 exp

(cid:17)

(cid:16)
𝑦𝑐𝑙𝑎
𝑖
(cid:16)
𝑦𝑐𝑙𝑎
𝑗

(cid:17)

(9)

where 𝑦𝑐𝑙𝑎 = 𝑊0𝑟 + 𝑏0, and 𝑊0 is the weight matrix, 𝑏0 is the bias
term.

For the prediction phase, the output 𝑦𝑐𝑙𝑎 indicates the predicted
probability that code snippet belongs to the corresponding class.

𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 = 𝑎𝑟𝑔𝑚𝑎𝑥

𝑦𝑐𝑙𝑎 (cid:17)
(cid:16)

, 𝑖 = 1, . . . , 𝑐

(10)

Producing Code Vectors for Code Clone Detection. Code clone
detection, i.e., detecting whether two code snippets are similar or
not, is a widely studied topic in software engineering research [44–
46]. Given two code snippets 𝑐1 and 𝑐2, the code representations 𝑟1
and 𝑟2 are produced by HELoC. We use a classifier (a linear layer)
for the secondary training, the mean squared error (MSE) is used
as the loss function:

(cid:19) 2

(cid:18)
𝑦𝑐𝑙𝑜
𝑖 −

𝐿𝑜𝑠𝑠𝑐𝑙𝑜 =

𝑟1 · 𝑟2
|𝑟1| · |𝑟2|
is used to denote the relatedness [47] between 𝑟1
is set to 1 and -1 for the true clone pairs and false clone

(11)

𝑟1 ·𝑟2
where
|𝑟1 | · |𝑟2 |
and 𝑟2. 𝑦𝑐𝑙𝑜
𝑖
pairs, respectively.

Wang, et al.

) is in the

𝑟1 ·𝑟2
|𝑟1 | · |𝑟2 |

For the prediction phase, the output 𝑝 (i.e.,

range [-1,1]. Thus, the prediction can be obtained as follows:

𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 =

(cid:26) 𝑇 𝑟𝑢𝑒,
𝐹𝑎𝑙𝑠𝑒,

𝑝 > 0
𝑝 ≤ 0

(12)

Producing Code Vectors for Code Clustering. Code clustering
is an unsupervised task of automatically clustering similar code
fragments into the same groups [4]. Given the vector representation
𝑟 of a code snippet, we can classify it by a Euclidean distance based
similarity measure and the K-means [48] clustering algorithm.

5 EXPERIMENTS
This section first presents the pre-training of HELoC and then
the applications HELoC to three program comprehension tasks
(code classification, code clone detection, and code clustering). We
compare HELoC with state-of-the-art methods. Furthermore, we
validate the usefulness of the core components of the model through
ablation experiments.

5.1 Model Pre-training
We pre-train HELoC on the Java-Large dataset [29], which contains
about 4 million Java source files collected from GitHub. HELoC
is implemented with PyTorch2 and Deep Graph Library (DGL)3.
In the experiments, the initial input dimension is set to 768. The
number of RSGNN layers is set to 𝑁𝐷 = 4. Considering the GPU
memory and training time, we set the hidden state length of the
cache to 256. The max AST depth, the max path number per AST,
and the max node number are set to 30, 200, and 1000, respectively,
based on the statistical information about the datasets. The Adam
optimizer [49] is used to train our model with a learning rate of
1e-4 and the batch size is 2048.

5.2 Code Classification
5.2.1 Datasets and Settings. We evaluate HELoC on two datasets,
i.e., Google Code Jam (GCJ) [50] and Online Judge (OJ) [25]. The
statistics of the two datasets are listed in Table 1.

Table 1: Statistics of GCJ and OJ datasets.

Dataset
♯Code snippets
♯Classes
Avg. AST nodes
Max AST nodes
Avg. AST depth
Max AST depth

GCJ
8647
6
387.3
1,059
33.9
208

OJ
52,000
104
190
7,027
13.3
76

GCJ. This dataset is collected from an online program competi-
tion held annually by Google. We choose 8647 Java class files from
the dataset published by Liang et al. [50] and classify them into six
classes according to the problems they solve.

OJ. This dataset consists of C programs collected from the OJ
programming exercise system. It is publicly available from Mou et
al.4, which contains 104 classes of problems. The dataset consists of

2https://pytorch.org/
3https://www.dgl.ai/
4https://sites.google.com/site/treebasedcnn/

HELoC: Hierarchical Contrastive Learning of Source Code Representation

ICPC ’22, May 16–17, 2022, Virtual Event, USA

the programming task and the corresponding source code submitted
by the students. If the programs aim to solve the same problem,
they have the same functionality and category.

In the experiments, the number of epochs is set to 100 and early
stopping is adopted. Learning rate and batch size are set to 1e-5
and 64, respectively. The ratio of training, validation, and test sets
is set to 8:1:1. In addition, label smoothing [51] is adopted to retain
values with labels of 0, with a probability 𝜆, where 𝜆 is empirically
set to 0.1. We use javalang5 and pycparser6 to parse the source code
written in Java and C and extract the ASTs.

5.2.2 Baselines and Evaluation Metrics. We consider three types
of baselines that parse code from different perspectives, namely,
token-based models, tree-based models, and graph-based models.
The implementation of the baselines and their parameter settings
are provided by their original papers.

Token-Based Model: Previous work [20, 52, 53]uses LSTM (long
short-term memory) networks to model the input sequence of code
tokens.

Tree-Based Models: Tree-LSTM [47] is a generalization of LSTM
for modelling tree structure. CodeRNN [50] designs a new AST-
based recurrent neural network by going through the tree from leaf
nodes to root node to obtain the final code representation vector.
TBCNN [25] is a tree-based convolutional neural network model
that designs a tree-based convolution kernel consisting of a set of
subtree feature detectors that slide the entire AST to extract the
structural information of the code. ASTNN [24] is an AST-based
neural network that divides a complete AST into a sequence of small
statement trees to solve the long-term dependency problem caused
by large ASTs. InferCode [4] applies self-supervised learning to
the AST by predicting subtrees automatically identified from the
AST context for code representation.

Graph-Based Models: GGNN [54] uses graphs that attach ad-
ditional dataflow edges to ASTs to represent the structure of the
source code, and then learns code representation by applying gated
GNN (GGNN).

To evaluate the effectiveness of code classification models, we
use the Accuracy metric, which calculates the percentage of the
test set that is correctly classified.

5.3 Code Clone Detection
There are generally four different types of code clones [44, 45, 55],
including Type-1 clones (a code pair that is syntactically identical
except for spaces and comments), Type-2 clones (a code pair that is
identical on a Type-1 basis except for variable names, type names,
and function names), Type-3 clones (a code pair that has several
statement additions and deletions but remains syntactically similar,
except for the cloning differences between Type-1, Type-2), and
Type-4 clones (a code pair with the same functionality that is not
textually or syntactically similar but has semantic similarities).

5.3.1 Datasets and Settings. We evaluate HELoC on three datasets:
BCB, GCJ, and OJClone. Table 2 provides the statistics.

BCB. In our experiments, we use the BCB dataset published
by Zhang et al. [24], which is a subset of the BCB constructed by

5https://github.com/c2nes/javalang
6https://pypi.python.org/pypi/pycparser

Table 2: Statistics of the datasets for code clone detection.

Dataset
♯Clone pairs
%True clone pairs
Avg. code length
Avg. AST nodes
Max AST nodes
Avg. AST depth
Max AST depth

BCB
97,535
95.7%
27.1
206
15,217
9.9
192

GCJ
50,000
20.0%
55.3
295.0
991
15.2
27

OJClone
50,000
6.6%
35.4
192
1,624
13.2
60

Svajlenko et al. [56]. BCB is suitable for evaluating the semantic
clone detection capabilities of models. Note that in this dataset,
Type-3 clones are further divided into strong Type-3 (ST3) and
moderately Type-3 (MT3).

GCJ. The GCJ clone dataset [50] consists of 50,000 clone pairs
from 12 different competing problems. Since few code snippets are
syntactically similar in these competing problems, most code pairs
from the same problem are Type-4 clones.

OJClone. We use the OJClone dataset provided in [24]. Each
code snippet in the dataset is a C function. The dataset consists of
50,000 randomly selected samples, produced from 500 programs
that were selected from each of the first 15 programming problems
in OJ [25]. Two code snippets dealing with the same problem form
a clone pair, while code snippets solving different problems are not
clones.

The parameter settings are consistent with the code classifica-
tion (see Section 5.2.1). In order to make the comparison as fair as
possible, for all the baselines, we adopt the implementation and
parameter settings provided by their original papers.

5.3.2 Baselines and Evaluation Metrics. To systematically evaluate
the proposed model, we compare it with token-based models, tree-
based models, and graph-based clone detection models.

Token-Based Models: RtvNN [45] uses an RNN to learn the
representations of code tokens and trains a recursive autoencoder
to learn AST representations. SourcererCC [44] is a well-known
lexical-based clone detector.

Tree-Based Models: Deckard [57] is a classical AST-based clone
detector by identifying similar subtrees to learn tree representa-
tions of source code. CDLH [58] learns code representations via
AST-based LSTM and uses a hash function to optimize the func-
tion similarity between code snippets. Furthermore, TBCNN [25],
ASTNN [24], and InferCode [4] (see Section 5.2.2) are also com-
pared on this task.

Graph-Based Models: SCDetector [59] combines the advantages
of the token-based approach and the graph-based approach to detect
clones with similar software functionality. FA-AST [5] constructs
a flow-augmented AST by adding explicit control and data flow
edges, thus enabling the method to make full use of the seman-
tic information for code representations. FCCA [3] is based on
a hybrid representation that fuses heterogeneous structured and
unstructured code information from text, AST, and control flow
graph (CFG) representations.

Since code clone detection can be formulated as a binary clas-
sification problem (clone or not), we choose the commonly used
Precision (P), Recall (R), and F1 scores (F1) as the evaluation metrics.

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Wang, et al.

5.4 Code Clustering
Code clustering is to group source code according to their similarity.
Unlike code classification (a supervised task), code clustering is an
unsupervised method. It is used as a downstream task by the related
work InferCode [4]. In our work, we also evaluate the effectiveness
of HELoC in code clustering.

5.4.1 Datasets and Settings. We evaluate HELoC on two datasets,
i.e., Online Judge (OJ) [25] and Sorting Algorithm (SA) [60].

OJ: The dataset is the same as the one used in the code classifica-
tion task. We use the K-means algorithm (K=104) to cluster the code
into 104 clusters, following the settings described in InferCode [4].
SA: This dataset includes a 10-class sorting algorithm written in
Java. We use the K-means algorithm (K=10) to cluster the code into
10 clusters, following the settings described in InferCode [4].

5.4.2 Baselines and Evaluation Metrics. For code clustering, our
method does not require labels for training, and is used in an unsu-
pervised manner. However, the previous methods such as ASTNN
and TBCNN were trained and validated on the classification task
with labels, which violates the prerequisite that the clustering task
is label-unaware. Therefore, for the sake of fairness, in this task, we
compare our approach with the following typical related methods
that support unsupervised tasks. The related methods are used as
encoders to generate code vector representations, over which code
clustering is performed.

Token-Based Model: Word2vec [61] and Doc2vec [62] are well-
known baseline methods in NLP, which extract feature vectors by
parsing the source code into text.

Tree-Based Model: Code2vec [29] parses a code fragment into
a collection of AST paths. The core idea is to use a soft-attention
mechanism on the paths and aggregate all vector representations
into a single vector to predict the method name. Code2seq [30]
is similar with Code2vec, which represents a code fragment as a
set of combined paths in an AST and uses attention to select the
relevant paths when decoding. The vector representations is used
to generate the code textual summary. InferCode [4] (see Section
5.2.2) is also compared on this task.

Graph-Based Model: GGNN [54] (see Section 5.2.2) is also com-

pared in this task.

To evaluate the code clustering effectiveness of HELoC, we use
Adjusted Rand Index (ARI) [63] as the metric. Specifically, we cluster
all code snippets based on the similarity between code vectors
without class labels. Then, we evaluate the effect of clustering by
calculating the distribution similarity function of the class labels in
the dataset and the class labels obtained by clustering.

5.5 Research Questions and Results
RQ1: How does our approach perform in code classification?
The results are shown in Table 3. Overall, HELoC improves the
accuracy from 93.4% to 97.2% on GCJ, and from 98.2% to 99.6% on
OJ, in comparison with the best competitor (ASTNN). In particular,
the greatest improvement was seen on the GCJ dataset with more
AST nodes and greater depth, which strongly confirms that HELoC
is better to capture this hierarchical complexity of the AST. We also
analyse the performance of the compared methods as follows.

Table 3: Code classification accuracy (%) on GCJ and OJ.

Groups
Token-based

Tree-based

Graph-based
Our approach

Methods
LSTM
Tree-LSTM
CodeRNN
TBCNN
ASTNN
InferCode
GGNN
HELoC

GCJ
78.4
88.9
87.4
91.7
93.4
90.8
72.9
97.2

OJ
88.0
88.2
90.2
94.0
98.2
96.3
79.5
99.6

Token-based approach, LSTM performs poorly in our experi-
ments because it uses only the literal meanings of the token words
to distinguish code functionalities. Since some token words used in
OJ are mostly arbitrary, e.g., the names of identifiers are usually 𝑖,
𝑗, 𝑎, etc, most tokens contribute little to the identification of code
features. The experimental results show that the token-based ap-
proach performs the worst compared to other types of approaches.
This is because it aims at modelling the token sequence, which
leads to a loss of syntactic structure information of the code.

Tree and graph-based approaches, such as CodeRNN, TBCNN,
and ASTNN learn the representation of code in various ways. In
particular, TBCNN performs better than CodeRNN because it can
encode the features of the given tree structure more accurately by
designing tree-based convolution kernel for the AST. ASTNN does
not perform as well as HELoC. One reason is that ASTNN uses
bottom-up aggregation of leaf nodes to the root node, which does
not capture the non-adjacent hierarchy. Another reason could be
that ASTNN decomposes an AST into a series of statement subtrees
in depth-first traverse order so that different relationships such
as nesting and if/else branches between statements may be lost.
HELoC can handle the case of a huge AST without splitting the AST,
avoiding this problem. In addition, an unexpected finding is that
GGNN performs poorly on the code classification, with even lower
accuracy than the token-based model. This may be because GGNN,
as a graph-based model, views the AST as an ordinary graph and
focuses more on the data flow between the nodes, ignoring the
unique hierarchical structure of AST.
RQ2: How does our approach perform in code clone detec-
tion? Tables 4-5 show the precision, recall, and F1 obtained by our
approach on BCB, GCJ, and OJClone datasets. Our approach scored
better than all baselines, and the F1 scores are improved from 95.4%
(InferCode) to 98.0% on BCB, from 93.1% (FCCA) to 97.4% on GCJ
and from 95.7% (ASTNN) to 98.0% on OJClone. In the following, we
analyse the performance of compared methods in detail.

BCB. Table 4 shows that all methods are very effective in terms
of detecting Type-1 and Type-2 clones. InferCode uses the self-
supervised framework by predicting subtrees to learn code repre-
sentation and performs best in both recall and F1 scores compared
to other baselines. However, it is still weaker than HELoC as it
has the disadvantage of choosing TBCNN as the source code en-
coder. TBCNN captures the structural features of ASTs by sliding
convolution kernels, which makes it difficult to capture long-term
dependencies if the AST is deep or has many nodes. In particular, it
treats ASTs as binary trees, which can change the original semantics

HELoC: Hierarchical Contrastive Learning of Source Code Representation

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 4: Results of code clone detection on BCB.

Type

ASTNN
F1
R
P
100
100
100
BCB-T1
100
100
100
BCB-T2
94.2 97.0
99.9
BCB-ST3
91.7 95.5
99.5
BCB-MT3
88.3 93.7
99.8
BCB-T4
BCB-ALL 99.8 88.4 93.8

FA-AST
F1
R
P
100
100
100
100
100
100
100
99.6 99.8
98.7 96.5 97.6
97.7 90.5 94.0
97.7 90.6 94.0

SCDetector
F1
R
P
100
100
100
100
100
100
100
94.6 97.2
97.7 95.9 96.8
97.3 92.3 94.7
97.3 92.4 94.8

FCCA
F1
R
P
100
100
100
100
100
100
100
99.8 99.9
98.7 95.9 97.3
98.2 92.4 95.2
98.2 92.4 95.2

InferCode
F1
R
P
100
100
100
100
100
100
97.0
94.2
99.8
97.8
96.2
99.5
95.3
92.7
98.2
98.2 92.8 95.4

Our approach
F1
R
P
100
100
100
100
100
100
100
100
100
98.9
98.0
99.9
98.0
96.2
99.8
99.8 96.2 98.0

Table 5: Results of code clone detection on GCJ and OJClone.

Groups

Methods

GCJ
R

P

F1

OJClone
R
P

F1

RtvNN

Tree-based

Token-based

SourcererCC 43.4 11.2 17.8

20.4 90.0 33.3 36.1 83.3 50.4
7.4 74.2 13.5
Deckard
45.0 44.2 40.1 99.0 5.3 10.1
TBCNN
91.7 89.3 90.5 90.1 81.3 85.8
CDLH
46.1 69.8 55.5 47.2 73.3 57.4
ASTNN
95.4 87.2 91.1 98.9 92.7 95.7
93.2 92.6 92.9 95.2 90.3 92.7
InferCode
SCDetector 90.4 87.1 88.7 95.0 89.2 92.0
96.7 89.8 93.1 94.1 87.3 90.6
96.3 85.5 90.6 94.7 91.8 93.2
Our approach HELoC 98.7 95.9 97.4 99.5 96.6 98.0

FCCA
FA-AST

Graph-based

of the source code, making long-term dependency problems even
worse. In contrast, HELoC uses a specialized RSGNN to capture
the global hierarchical structure and particularly allows the entire
AST with path augmented node embeddings as input, alleviating
the broken semantics.

Among the graph-based approaches, SCDetector and FCCA
achieve good overall results as hybrid representation models for
source code due to their mining of token, control flow and other
information from the source code. However, their precision values
are slightly lower for MT3 and Type-4 clones than the tree-based
approaches. For SCDetector, only tokens and CFGs are used, which
have a significant advantage in reflecting the semantics of the code,
but lacks the import of syntactic structures. FCCA learns source
code token, AST and CFG separately and then fuses them that
achieves better than SCDetector. However, as a coarse-grained
multi-view fusion method, it has the disadvantage that the seman-
tic or syntactic structure of the code is not learned finely enough.
Our approach adds textual content and position to the nodes and
enables the fine-grained fusion of AST and code. In addition, our
model demonstrates that learning the AST hierarchy is more likely
to improve the detection precision of MT3 and Type-4 clone pairs.
GCJ and OJClone. Table 5 shows that RtvNN achieves higher
recall but very low precision. We find that RtvNN detects almost all
the code pairs as clones. The reason could be that RtvNN relies on
a simple distance metric to distinguish the hidden representation
of each method generated, and does not handle the case where
two functionally different methods may share syntactically similar
components. SourcererCC achieves low recall and precision because

it only considers the overlapping similarity of tokens between two
code snippets and ignores the code snippet semantics, resulting in
its inability to handle semantic clones. Still, HELoC outperforms
all baselines on both datasets and achieves an F1 scores that is 4.3%
higher than the score obtained by FCCA on GCJ.

Table 6: Code clustering results on OJ and SA datasets.

Groups

Token-based

Tree-based

Graph-based
Our approach

Methods
Word2vec
Doc2vec
Code2vec
Code2seq
InferCode
GGNN
HELoC

OJ
28.2
41.9
58.2
52.9
70.1
42.1
82.2

SA
24.4
29.0
51.4
49.0
62.0
39.7
74.6

RQ3: How does our approach perform in code clustering? As
shown in Table 6, our model can significantly improve the results
of unsupervised clustering task. Overall, HELoC has a relative
improvement of 12.1% and 12.6% in ARI metric on two datasets
compared to the best competitor (Infercode).

The tree-based models significantly improve the token-based
models. Code2vec and Code2seq model code snippets as a collection
of paths in an AST, which captures the structural information of
the code to some extent. However, the path representations they
learned can only express fixed intervals between AST nodes at
different levels rather than the dynamic distances learned by HELoC.
This is why their results are not as good as our model. Infercode’s
decomposition of ASTs into subtrees breaks the overall semantic
relationship of AST and leads to errors in understanding the overall
structure of the code. HELoC aims to learn the topology of the
AST autonomously without splitting the tree, which preserves the
integrity of the code structure to a large extent.

RQ4: What is the impact of different core components on
the proposed model? To evaluate the effectiveness of core com-
ponents of HELoC, we conduct ablation experiments on three down-
stream tasks, and the experimental results are shown in Table 7.

No NEP. We investigate the impact of NEP by adding hierarchi-
cal information to the initial embedding of the nodes instead of
automatically predicting it through pre-training. From the results
shown in the table, it can be concluded that NEP impacts the model
by learning the dynamic continuous distances of nodes at different
levels.

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Wang, et al.

No NRO. To evaluate the impact of NRO, we remove it from
HELoC. The results show that the model without NRO is the worst.
NRO maps three topological relationships between node hierarchies
to the embedding space. The experimental results confirm that
introducing NRO can improve the performance of the model.

No self-attention mechanism. We remove the self-attention
component from HELoC. The results show that the model without
this component performs less well on all datasets, indicating that
the self-attention mechanism can improve the performance of the
model by obtaining global information with a larger receptive field.
No internal and external residual connections. To study the
impact of residual connections, we compared HELoC with a vari-
ant obtained by removing the residual connections inside the self-
attention mechanism. The results show that HELoC, including both
residual connections inside and outside of the RSGNN, performs bet-
ter since it effectively mitigates the problem of gradient vanishing
inside the network.

No RSGNN. To evaluate the effectiveness of the feature encoder
RSGNN, we replaced it with traditional GCN. From the overall
results of the three downstream tasks, the loss of the model accuracy
is significant after replacing RSGNN with GCN, demonstrating the
critical role of RSGNN in HELoC.

6 THREATS TO VALIDITY

Threats to internal validity. They mainly come from the hy-
perparameters we set. Different hyperparameter settings affect the
experimental results of baselines. We use the experimental results
published in the original paper for baselines whom experimental
data were provided. We retrain the baselines with the default pa-
rameters from the original paper to obtain the experimental results
for those without provided data.

Threats to external validity. They mainly come from the datasets

we use. First, we conduct experiments on Java and C, so further
work is still needed to validate our model to other programming
languages. Second, several existing methods obtain results close
to ours on the BCB dataset when performing code clone detection.
In the future, we plan to use more challenging datasets with more
functions to further evaluate the methods’ capability.

7 RELATED WORK

Source Code Representation. As stated in Section 1, how to rep-
resent source code effectively is a fundamental problem in software
engineering research. Many source code representation techniques
have achieved considerable success in many program comprehen-
sion tasks, including code classification [50, 64, 65], malware de-
tection [11, 66], and code generation [67, 68]. We have described
major source code representations in Section 2.2 and pointed out
their limitations in Section 2.3. Compared with these work, our
proposed approach can better model the entire AST structures and
achieve better performance in downstream tasks. Recently, several
works [28, 69–71] inject structured information such as path em-
bedding [71] and node distance [28] into Transformer to introduce
structural inductive bias. Unlike these Transformer-based models,
we adopt a non-invasive strategy. Our RSGNN utilizes both graph
convolutional networks (GCNs) (for capturing local structure) and
the self-attention mechanism (for capturing global structure) to

Table 7: Effectiveness of core components of HELoC.

Type

Cla-
GCJ OJ
No_NEP1 88.3 86.1
No_NRO2 75.1 72.4
83.2 87.1
No_SM3
No_IERC4 81.2 82.2
No_RSGNN5 78.2 76.9
97.2 99.6

HELoC

Clo- (F1)

Clu-

BCB-ALL GCJ OJClone OJ

83.6
70.7
82.2
76.1
75.0
98.0

89.2
68.1
84.2
78.0
70.2
97.4

87.7
73.4
83.7
79.2
75.4
98.0

SA
70.1 63.4
60.1 49.2
68.2 60.9
63.4 53.0
61.8 50.1
82.2 74.6

1 No node level prediction.
2 No node relationship optimization.
3 No self-attention mechanism.
4 No internal and external residual connections.
5 No residual self-attention GNN.

learn AST hierarchy comprehensively. In this way, our model al-
lows a native AST as input without having to flatten it into the
sequence required by Transformer, therefore the original structure
of the code can be better preserved.

Self-Supervised Learning. The advantage of self-supervised
learning is that it benefits downstream tasks by designing pretext
tasks to learn from large-scale unlabeled data. It has yielded con-
siderable improvements in Natural Language Processing [4, 72, 73]
and Computer Vision [74–77]. The contrastive learning approach
[27] has also received increasing attention as an important branch
of self-supervised learning. It aims at minimizing the distance be-
tween similar data (positive) representations and maximizing the
distance between dissimilar data (negative) representations. Promis-
ing results were obtained in the studies of He et al. [78] and Chen et
al. [79]. Recently, such ideas have been embraced by researchers in
the field of software engineering [34, 35, 80–83]. There are several
choices of loss functions for contrastive learning, such as Triplet
[40], InfoNCE [84], and Siamese [85]. In this paper, we design a
novel hierarchical contrastive learning model to learn the relation-
ships between nodes in an AST hierarchy.

8 CONCLUSION
In this paper, we propose HELoC, a hierarchical contrastive learning
model for source code representation. HELoC includes a novel hi-
erarchical contrastive learning method to comprehensively capture
the hierarchical features of ASTs by predicting the AST hierarchy
through a dedicated RSGNN. To the best of our knowledge, HELoC
is the first pre-training model that applies self-supervised con-
trastive learning for learning AST hierarchy. We demonstrate the
effectiveness of our approach by applying it to three downstream
tasks, code classification, code clone detection, and code clustering,
on multiple datasets. The experimental results show that our model
outperforms other token-based, graph-based, and tree-based mod-
els. The implementation of HELoC and the experimental data are
publicly available at: https://github.com/Code-Rep/HELoC.

ACKNOWLEDGMENTS
This work is financially supported by the Natural Science Founda-
tion of Shandong Province, China (ZR2021MF059, ZR2019MF071),
National Natural Science Foundation of China (61602286, 61976127)
and Special Project on Innovative Methods (2020IM020100).

HELoC: Hierarchical Contrastive Learning of Source Code Representation

ICPC ’22, May 16–17, 2022, Virtual Event, USA

REFERENCES
[1] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE), 2018.
[2] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. Retrieval on source code: a neural code search. In Proceedings of the 2nd
ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages, 2018.

[3] Wei Hua, Yulei Sui, Yao Wan, Guangzhong Liu, and Guandong Xu. Fcca: Hybrid
code representation for functional clone detection using attention networks. IEEE
Transactions on Reliability, 70(1):304–318, 2020.

[4] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. Infercode: Self-supervised learning of
code representations by predicting subtrees. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), 2021.

[5] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with
graph neural network and flow-augmented abstract syntax tree. In 2020 IEEE
27th International Conference on Software Analysis, Evolution and Reengineering
(SANER), 2020.

[6] Wenhan Wang, Ge Li, Sijie Shen, Xin Xia, and Zhi Jin. Modular tree network for
source code representation learning. ACM Transactions on Software Engineering
and Methodology (TOSEM), 2020.

[7] David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. Code to comment
“translation”: Data, metrics, baselining & evaluation. In 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE), 2020.
[8] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In
2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC),
2018.

[9] Haoye Wang, Xin Xia, David Lo, John Grundy, and Xinyu Wang. Automatic
solution summarization for crash bugs. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), 2021.

[10] Hoa Khanh Dam, Truyen Tran, Trang Thi Minh Pham, Shien Wee Ng, John
Grundy, and Aditya Ghose. Automatic feature learning for predicting vulnerable
software components. IEEE Transactions on Software Engineering, 2018.
[11] Xinjun Pei, Long Yu, and Shengwei Tian. Amalnet: A deep learning framework
based on graph convolutional networks for malware detection. Computers &
Security, 93:101792, 2020.

[12] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign:
effective vulnerability identification by learning comprehensive program seman-
tics via graph neural networks. In Advances in Neural Information Processing
Systems, 2019.

[13] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A
transformer-based approach for source code summarization.
In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.
[14] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. Retrieval-
based neural source code summarization. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE), 2020.

[15] Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip Yu,
and Guandong Xu. Reinforcement-learning-guided source code summarization
via hierarchical attention. IEEE Transactions on software Engineering, (01):1–1,
2020.

[16] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. Improved
In Proceedings of the 28th

code summarization via a graph neural network.
International Conference on Program Comprehension, 2020.

[17] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention
network for extreme summarization of source code. In International conference
on machine learning, 2016.

[18] Xuan Huo, Ming Li, Zhi-Hua Zhou, et al. Learning unified features from natural
and programming languages for locating buggy source code. In IJCAI, 2016.
[19] Jian Li, Yue Wang, Michael R Lyu, and Irwin King. Code completion with neural

attention and pointer networks. In IJCAI, 2018.

[20] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Sum-
marizing source code using a neural attention model. In Proceedings of the 54th
Annual Meeting of the Association for Computational Linguistics, 2016.

[21] Alexander LeClair, Aakash Bansal, and Collin McMillan. Ensemble models for
neural source code summarization of subroutines. In 2021 IEEE International
Conference on Software Maintenance and Evolution (ICSME), 2021.

[22] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. Retrieval-
augmented generation for code summarization via hybrid gnn. In International
Conference on Learning Representations, 2020.

[23] Shangqing Liu. A unified framework to learn program semantics with graph
neural networks. In 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 2020.

[24] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. A novel neural source code representation based on abstract syntax tree.
In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),
2019.

[25] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks
over tree structures for programming language processing. In Thirtieth AAAI
Conference on Artificial Intelligence, 2016.

[26] Xiujuan Ji, Lei Liu, and Jingwen Zhu. Code clone detection with hierarchical
attentive graph embedding. International Journal of Software Engineering and
Knowledge Engineering, 31(06):837–861, 2021.

[27] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by
learning an invariant mapping. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06), 2006.

[28] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
Günnemann. Language-agnostic representation learning of source code from
structure and context. In International Conference on Learning Representations,
2020.

[29] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning
distributed representations of code. Proceedings of the ACM on Programming
Languages, 2019.

[30] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating
sequences from structured representations of code. In International Conference
on Learning Representations, 2018.

[31] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured
neural summarization. In International Conference on Learning Representations,
2018.

[32] Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket Anand, David Lo, and
Rahul Purandare. Modeling functional similarity in source code with graph-based
siamese networks. IEEE Transactions on Software Engineering, 2021.

[33] Zhijiang Guo, Yan Zhang, and Wei Lu. Attention guided graph convolutional

networks for relation extraction. arXiv preprint arXiv:1906.07510, 2019.

[34] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. Self-supervised contrastive learning
for code retrieval and summarization via semantic-preserving transformations.
In Proceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval, 2021.

[35] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E Gonzalez, and Ion
Stoica. Contrastive code representation learning. arXiv preprint arXiv:2007.04973,
2020.

[36] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. Treebert: A tree-
based pre-trained model for programming language. In International Conference
on Uncertainty in Artificial Intelligence, UAI, 2021.

[37] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-
training code representations with data flow.
In International Conference on
Learning Representations, 2020.

[38] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-
invariant representations. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020.

[39] Reuven Rubinstein. The cross-entropy method for combinatorial and continuous
optimization. Methodology and computing in applied probability, 1(2):127–190,
1999.

[40] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified em-
bedding for face recognition and clustering. In Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015.

[41] Yanlin Wang and Hui Li. Code completion by modeling flattened abstract syntax
trees as graphs. In Proceedings of the AAAI Conference on Artificial Intelligence,
2021.

[42] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncer-
tainty to weigh losses for scene geometry and semantics. In Proceedings of the
IEEE conference on computer vision and pattern recognition, 2018.

[43] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne.

Journal of machine learning research, 9(11), 2008.

[44] Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K Roy, and Cristina V
Lopes. Sourcerercc: Scaling code clone detection to big-code. In Proceedings of
the 38th International Conference on Software Engineering, 2016.

[45] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk.
Deep learning code fragments for code clone detection. In 2016 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE), 2016.
[46] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. Deep learning similarities from different repre-
sentations of source code. In 2018 IEEE/ACM 15th International Conference on
Mining Software Repositories (MSR), 2018.

[47] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic
representations from tree-structured long short-term memory networks.
In
Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference on Natural Language Processing,
2015.

[48] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth
Silverman, and Angela Y Wu. An efficient k-means clustering algorithm: Anal-
ysis and implementation.
IEEE transactions on pattern analysis and machine
intelligence, 2002.

[49] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[50] Yuding Liang and Kenny Zhu. Automatic generation of text descriptive comments
for code blocks. In Proceedings of the AAAI Conference on Artificial Intelligence,

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Wang, et al.

2018.

[51] Rafael Müller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing
help? In Proceedings of the 33rd International Conference on Neural Information
Processing Systems, 2019.

[52] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint

arXiv:1410.4615, 2014.

[53] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. Synthe-
In 2017 IEEE/ACM International

sizing benchmarks for predictive modeling.
Symposium on Code Generation and Optimization (CGO), 2017.

[54] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning
In International Conference on Learning

to represent programs with graphs.
Representations, 2018.

[55] Chanchal Kumar Roy and James R Cordy. A survey on software clone detection

research. Queen’s School of Computing TR, 541(115):64–68, 2007.

[56] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, and Mo-
hammad Mamun Mia. Towards a big data curated benchmark of inter-project
code clones. In 2014 IEEE International Conference on Software Maintenance and
Evolution, 2014.

[57] Lingxiao Jiang, Ghassan Misherghi, Zhendong Su, and Stephane Glondu. Deckard:
Scalable and accurate tree-based detection of code clones. In 29th International
Conference on Software Engineering (ICSE’07), 2007.

[58] Huihui Wei and Ming Li. Supervised deep features for software functional clone
detection by exploiting lexical and syntactical information in source code. In
IJCAI, 2017.

[59] Yueming Wu, Deqing Zou, Shihan Dou, Siru Yang, Wei Yang, Feng Cheng, Hong
Liang, and Hai Jin. Scdetector: software functional clone detection based on
semantic tokens analysis.
In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering, 2020.

[60] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. Bilateral dependency neural net-
works for cross-language algorithm classification. In 2019 IEEE 26th International
Conference on Software Analysis, Evolution and Reengineering (SANER), 2019.
[61] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Dis-
tributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems, 2013.

[62] Quoc Le and Tomas Mikolov. Distributed representations of sentences and

documents. In International conference on machine learning, 2014.

[63] Jorge M Santos and Mark Embrechts. On the use of the adjusted rand index as
a metric for evaluating supervised classification. In International conference on
artificial neural networks, 2009.

[64] Mingming Lu, Dingwu Tan, Naixue Xiong, Zailiang Chen, and Haifeng Li. Pro-
gram classification using gated graph attention neural network for online pro-
gramming service. arXiv preprint arXiv:1903.03804, 2019.

[65] Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket Anand, David Lo, and
Rahul Purandare. Modeling functional similarity in source code with graph-based
siamese networks. arXiv preprint arXiv:2011.11228, 2020.

[66] Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. Software defect prediction via
convolutional neural network. In 2017 IEEE International Conference on Software
Quality, Reliability and Security (QRS), 2017.

[67] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intel-
licode compose: Code generation using transformer. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2020.

[68] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. Treegen:
A tree-based transformer architecture for code generation. In Proceedings of the
AAAI Conference on Artificial Intelligence, 2020.

[69] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David
Bieber. Global relational models of source code. In International conference on
learning representations, 2019.

[70] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative

position representations. arXiv preprint arXiv:1803.02155, 2018.

[71] Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. Integrating tree path
in transformer for code representation. Advances in Neural Information Processing
Systems, 2021.

[72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for
text classification. In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics, 2018.

[73] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding.
In
NAACL-HLT (1), 2019.

[74] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does
unsupervised pre-training help deep learning? In Proceedings of the thirteenth
international conference on artificial intelligence and statistics, 2010.

[75] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the
effectiveness of bert. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 2019.

[76] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representa-

tion learning by predicting image rotations. In ICLR 2018, 2018.

[77] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A
Efros. Context encoders: Feature learning by inpainting. In Proceedings of the
IEEE conference on computer vision and pattern recognition, 2016.

[78] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum
contrast for unsupervised visual representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
[79] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple
framework for contrastive learning of visual representations. In International
conference on machine learning, 2020.

[80] Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Ste-
fanie Jegelka. Debiased contrastive learning. arXiv preprint arXiv:2007.00224,
2020.

[81] John M Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. Declutr: Deep
contrastive learning for unsupervised textual representations. arXiv preprint
arXiv:2006.03659, 2020.

[82] Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray,
and Saikat Chakraborty. Contrastive learning for source code with structural
and functional properties. arXiv preprint arXiv:2110.03868, 2021.

[83] Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Graham Neubig, Bogdan
Vasilescu, and Claire Le Goues. Varclr: Variable semantic representation pre-
training via contrastive learning. arXiv preprint arXiv:2112.02650, 2021.
[84] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with

contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

[85] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric
discriminatively, with application to face verification. In 2005 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 2005.

