Can we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

Cedric Richter∗
University of Oldenburg
Oldenburg, Germany
cedric.richter@uni-oldenburg.de

Heike Wehrheim∗
University of Oldenburg
Oldenburg, Germany
heike.wehrheim@uni-oldenburg.de

2
2
0
2

l
u
J

1

]
E
S
.
s
c
[

1
v
1
0
3
0
0
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
Real bug fixes found in open source repositories seem to be the
perfect source for learning to localize and repair real bugs. However,
the absence of large scale bug fix collections has made it difficult
to effectively exploit real bug fixes in the training of larger neural
models in the past. In contrast, artificial bugs – produced by mu-
tating existing source code – can be easily obtained at a sufficient
scale and are therefore often preferred in the training of existing
approaches. Still, localization and repair models that are trained
on artificial bugs usually underperform when faced with real bugs.
This raises the question whether bug localization and repair mod-
els trained on real bug fixes are more effective in localizing and
repairing real bugs.

We address this question by introducing RealiT, a pre-train-and-
fine-tune approach for effectively learning to localize and repair real
bugs from real bug fixes. RealiT is first pre-trained on a large num-
ber of artificial bugs produced by traditional mutation operators
and then fine-tuned on a smaller set of real bug fixes. Fine-tuning
does not require any modifications of the learning algorithm and
hence can be easily adopted in various training scenarios for bug
localization or repair (even when real training data is scarce). In
addition, we found that training on real bug fixes with RealiT is
empirically powerful by nearly doubling the localization perfor-
mance of an existing model on real bugs while maintaining or even
improving the repair performance.

KEYWORDS
program repair, bug detection, bug fixes, learn to debug

1 INTRODUCTION
Finding and fixing software bugs are one of the most common
challenges in software engineering [1]. Developers are often faced
with these tasks while spending a considerable amount of time
in fixing software bugs. Still, some bugs find their way in open
software repositories which then has to be fixed by a bug fixing
code change. This raises the question whether we can relieve the
developer from the debugging process by learning from common
developer mistakes and their fixes found in open source projects.
Previous work [3, 4, 12, 22, 25, 31] addressed this question by
designing automatic learning-based methods for bug localization
and repair. However, to obtain the necessary amount of data needed
for training, they often employed code mutants instead of real bug
fixes. Mutants are generated by automatically injecting small code
changes into existing code. As this process is automated, mutants

∗This author was partially supported by the German Research Foundation (DFG) within
the Collaborative Research Centre "On-The-Fly Computing" (SFB 901).

Figure 1: Training on real bug fixes improves localization
and repair of real bugs.

can be easily obtained at large masses which is necessary for train-
ing effective learning-based models. However, a mutant may not
represent a real bug which could ultimately bottleneck the perfor-
mance of learning based bug localization repair.

In contrast in this work, we aim to explore the effect of real
bug fixes obtained from open source repositories on the training of
learning-based bug localization and repair methods. For this, we
employ a novel dataset of 33k real-world bug fixes obtained from
public Python projects. Since this dataset is still comparably small
to the datasets typically used for training localization and repair
models [4, 12] (which oftentimes contain millionth of artificial bugs),
we propose RealiT (pronounced “reality”), a novel training scheme
for learning to Repair and localize with Transformers. RealiT is
designed to combine the strengths of both training on mutants and
real bug fixes by first pre-training on a high number of mutants
and then fine-tuning on a smaller set of real bug fixes. This design
allows us to not only evaluate the impact of real bug fixes and
mutants together on the training process, but also individually (by
skipping either pre-training or fine-tuning phase).

To evaluate the impact of RealiT’s training on the localization
and repair of real bugs, we implement RealiT for fixing a variety
of single token bugs in Python. Our implementation considers four
common types of single token bugs – that can be fixed by changing
a single program token. We evaluate RealiT together with several
baselines on over 2000 real world bugs collected in the PyPIBugs
benchmark [4].

By integrating real bug fixes in the training process with RealiT,
we observe significant performance gains over a training that solely

1520253035404550real-world localization accuracy4550556065707580real-world repair accuracyRNNTransformerGNNGREATRealiT (base)+More mutants+Real bug fixesRealiT 
 
 
 
 
 
focuses on mutants (as in previous works). In fact, training with
real bug fixes allows to nearly double the number of successfully
localized real bugs (x-axis in Figure 1) while also maintaining or
improving the repair performance.

Our main contributions can be summarized as follows:

• For investigating the effect of real bug fixes on the training of
neural bug localization and repair models, we propose a sim-
ple pre-train-and-fine-tune approach. We find that training
both on mutants and real bugs with our method significantly
improves the performance over models solely trained on
either mutants or real bugs when evaluated on real single
token bugs in Python.

• We show that data quality and quantity has a significant
impact on neural bug localization and repair models. By
pre-training on a large number of mutants (up to 20x larger
than in previous work), RealiT already significantly improves
localization and repair performance both on mutants and real
bugs. Combined with fine-tuning on real bug fixes, RealiT is
the first model to repair a significant portion of a real bugs
benchmark.

• For adopting RealiT into future projects, we show that even
training on smaller subsets of real bug fixes can yield perfor-
mance improvement for localization and repair of real bugs.
However, more bug fixes are also more beneficial.

We plan to release all trained models, pre-training and fine-tuning
code1.

2 BACKGROUND
In this section, we introduce the necessary background for our
approach. To begin with, we start by describing the single token
localization and repair task tackled by RealiT and how previous
techniques addressed this task by predicting token replacements
and learning from mutants.

2.1 Single token bug localization and repair
In this work, we focus on the localization and repair of single token
bugs. Single token bugs are bugs that can be repaired by replacing
only a single program token (e.g. a variable or binary operator). For
this reason, they are often easy to repair – as only a single token has
to be changed – but hard to identify. Examples for single token bugs
are given in Table 1. Interestingly, single token bug localization
and repair has previously only been addressed through training
with mutants [3, 4, 12, 22, 25, 31]. Nevertheless, real bug fixes for
single token bugs – which can be employed for training or testing
– are available in bug fix collections such as ManySStuBs4J [16] or
TSSB-3M [26].

Task description. Throughout this work, we view source code as
a sequence of tokens T = 𝑡0, 𝑡1, 𝑡2, . . . , 𝑡𝑛. A single token bug can
then be fixed by replacing a single token 𝑡𝑙 with another token 𝑟
in the same scope (𝑟 = 𝑡𝑙 ′) or coming from an external vocabulary
(𝑟 ∈ 𝑉 ). To effectively localize and repair a single token bug, the
following three tasks have to be performed: (1) the program T has
to be classified to contain a bug, (2) the bug location 𝑡𝑙 has to be
localized and then (3) the correct repair 𝑟 has to be identified. In

1https://github.com/cedricrupb/nbfbaselines

Richter and Wehrheim

practice, these three tasks are often modeled as token replacement
operations. Let T be a program containing a single token bug and
T ′ be the corrected bug-free version, then the localization and
repair model is trained to perform the following operations:

replace(𝑡𝑙 ,𝑟 )
−−−−−−−−−−→ T ′

T

(1)

T ′ noop()

−−−−−−→ T ′

(2)

Here, we fix the buggy program T by replacing 𝑡𝑙 with 𝑟 and
therefore translating it into T ′. Since T ′ is bug-free, a change is
not required (noop).

In practice, we train models to estimate the likelihood of each
possible token replacement and select the most likely replacement
to fix T .

2.2 Mutation
Motivated by the general absence of real bug fixes at a sufficient
scale, previous learning based localization and repair approaches [4,
12, 25, 31] mainly focused on training on mutants. Mutants are
artificially generated (pseudo-)bugs that are introduced into a cor-
rect program via a mutation operator. For single token bugs, the
mutation operator can be seen as a token replacement operator
which can be inverted by a localization and repair model:

−−−−−−−−−−→ T ′ replace(𝑡𝑙 ,𝑟 −1)
mutate(𝑡𝑙 ,𝑟 )

−−−−−−−−−−−−→ T

T

(3)

For a dataset of bug-free programs (e.g. mined from open source
projects), the mutation operator first introduces a token mutation
by replacing a random token with a random other token. The token
types are often specified (e.g. binary operators) such that the pro-
grams remains interpretable after the transformation. Afterwards,
the localization and repair model is trained to invert the mutation
process to obtain the original program.

While traditionally mutation operator are designed as a random
process, previous work also tried to design more realistic muta-
tion operators by learning from real bug fixes [21], by training an
adversary to the repair model [4] or by finding replacements that
naturally fit the context [25].

2.3 Real bug fixes
Real bug fixes are often obtained by scraping the commit history
of public open source projects. During this process, commits are
often classified as bug fixing based on certain keywords in the
commit message [16]. Even though this process cannot guarantee
that every collected commit is a real bug fix, it has been empirically
shown [16] that the process is highly precise (e.g. over 90% of all
collected code changes were real bug fixes). In this work, we are
interested in special types of single token bug fixes. Here, a bug is
fixed by replacing only a single token:

replace(𝑡𝑙 ,𝑟 )
−−−−−−−−−−→ T𝑖+1

T𝑖

(4)

Note that a (bug fixing) commit only represents a snapshot of the
project at time 𝑖. Therefore, while it is highly likely T𝑖 contains a
single token bug which can be fixed by replace(𝑡𝑙 , 𝑟 ), we cannot
guarantee that the bug fix is complete and T𝑖+1 is bug-free.

Can we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

Table 1: Examples of single token bug types taken from PyPIBugs [4]

Example

Description

1
2
3
4
5

1
2
3
4
5
6
7
8
9

1
2
3
4
5

# VarMisuse: applied instead of patch
applied = self.db.applied_patches()
for patch in applied:

if patch in patches:

patches.remove(applied)

# BinOp: != instead of ==
def updateRefractionParameters(self):

...
if self.ui.checkRefracNone.isChecked():

return False

if self.checkRefracNoTrack.isChecked():
if self.app.mount.status != 0:

return False

...

All applied patches should be removed from the patches list. How-
ever, the developer mistakenly tries to remove applied instead of
a single patch.
Fix: replace applied in Line 5 by patch defined in Line 3.

The function updateRefractionParameters performs an update
and returns true if the update was successful. Prior to the update the
function checks some preconditions and the function should abort
if the mount is not ready. Therefore, we can conventionally expect
that we abort if the status is zero. However, we check whether the
status is not zero.
Fix: replace != in Line 7 by ==.

# Negation: namespace instead of not namespace
if namespace:

self.namespacesFilter = [ "prymatex", "user" ]

else:

self.namespacesFilter = namespace.split()

A default namespacesFilter should be used if no namespace is
given. However, the condition checks the inverse.
Fix: replace namespace in Line 2 by not namespace.

3 METHODOLOGY
In this section, we introduce RealiT as an effective training tech-
nique for bug localization and repair with Transformers. We start
by giving a general overview of the training process for transferring
and improving the performance of a localization and repair model
trained solely on mutants. Afterwards, we discuss the Transformer-
based architecture used during training and the inference strategy
we apply for localizing and repairing single token bugs in more
detail.

3.1 RealiT: Training on mutants and real bugs
To facilitate both mutants and real bug fixes, we design RealiT as a
pre-train-and-fine-tune approach. Therefore, we perform the train-
ing in two phases (as shown in Figure 2). In the first pre-training
phase, we train RealiT on artificially generated mutants introduced
into source code obtained by mining public open source reposito-
ries. Afterwards, in the second fine-tuning phase, we employ the
pre-trained version of RealiT to further train it on real bug fixes.

Pre-training with code mutants. During pre-training, we train
our model similar to the way current localization and repair models
are trained [12]. Here, the training objective is not to identify real
bugs but rather to identify and transform mutated code snippets
back into the real code snippets. For this task, we naturally start
with a general corpus of Github code snippets (e.g. function imple-
mentations). This corpus can often easily be obtained by mining
the recent version of popular open source projects. Since bugs are
scarce in open source projects, we can safely assume that most of
the code snippets are likely correct. For training, we mutate each
code snippet in our corpus at max 𝑘 times which produces a dataset

of at max 𝑘 unique mutants per code snippet2. During our experi-
ments, we decided to employ an unusually large number of mutants
per code snippet (𝑘 = 100) since we observed that this improves
the performance after fine-tuning. We employ the original code
corpus as training examples of unmutated correct code. Based on
the two datasets, RealiT is then trained to (1) distinguish mutants
from real code, (2) identify the mutated location (if any) and (3) find
the original replaced token. Since the dataset of mutants is up to 𝑘
times larger than the set of correct code snippet (by construction),
we additionally supersample3 each correct code snippet such that
RealiT is trained on correct and mutated code snippets at the same
frequency. This avoids that the model is biased towards detecting
mutants.

Learning to fix real bugs with bug fixes. In the second phase, we
aim to further optimize the performance of RealiT for localization
and repair of real bugs by fine-tuning on real bug fixes. For the fine-
tuning process, we adopt a pre-trained version of RealiT obtained
from the previous phase. Then, we continue the training now on
realistic buggy and bug-free code. As examples for realistic buggy
code, we employ the code related to real bug fix before the fix
is applied. Bug-free code is again obtained by using the original
Github corpus. During training, RealiT is now fine-tuned to (1)
distinguish real buggy code from bug-free code, (2) identify the bug
location (if any) and (3) imitate the original bug fix. Since now, the
code corpus is usually much larger than the set of bug fixes, we
supersample the buggy programs to match the correct programs in
their frequency.

2The number of code rewrites applicable to a code snippet and hence the number of
unique mutants per code snippet is limited by design and might be lower than 𝑘. We
never introduce mutation duplicates.
3During training, mutants and correct programs are sampled at the same frequency.
As the set of mutants is up to 𝑘 times larger, we uniformly (super-)sample correct
programs multiple times to match the number of mutants seen during training.

Datasets

Github Code

Real Bug Fixes

Real Test Set

Richter and Wehrheim

mutate

Mutants

supersample

Correct

supersample

supersample

Correct

Bugs

patches

.

2

1

remove

(

4

3

applied

)

6

5

Localization

patches

.

2

1

remove

(

4

3

applied

)

6

5

Repair

pre-train

(1) Train on mutants

fine-tune

patches

.

2

1

remove

(

4

3

applied

)

6

5

(2) Fine-tune on real bugs

(3) Evaluate

Figure 2: Overview over the RealiT training and evaluation process

We believe that pre-training and fine-tuning can have an orthog-
onal effect on the localization and repair model (which we aim
to explore during our evaluation). Due to the mutation process,
the pre-training phase is more tailored towards identifying correct
programs and deviations from them. In contrast, the fine-tuning
phase aims to teach the model the difference between real correct
programs and real buggy programs (and how they can be fixed).

3.2 Model architecture
In the section, we discuss the neural model employed by RealiT to
learn localization and repair of single token bugs. Since our main
focus is to study the effect of mutants and real bug fixes on the
training process, we employ a fairly standard Transformer-based
model [12] for learning bug localization and repair.

Probabilistic model. For the task of single token localization and
repair, programs are represented as a sequence of tokens T =
𝑡0, 𝑡1, 𝑡2, . . . , 𝑡𝑛 where each token 𝑡𝑙 represents a potential bug loca-
tion. Single token bugs are fixed only by replacing single tokens
𝑡𝑙 with another token 𝑟 (replace(𝑡𝑙 , 𝑟 )). In the following, we model
localization and repair as a joint probability distribution over all
potential bug locations and repairs {⟨𝑙, 𝑟 ⟩ | 𝑡𝑙 ∈ T ∪ {NOOP} and 𝑟 ∈
T ∪ 𝑉 }:

(5)

𝑝 (⟨𝑙, 𝑟 ⟩ | T ) = 𝑝loc (𝑙 | T ) · 𝑝repair (𝑟 | 𝑙, T )
Here, localization and repair is factorized into first localizing a bug
location (𝑝loc (𝑙 | T )) and then finding a repair dependent on the
bug location (𝑝repair (𝑟 | 𝑙, T )). For localization, we include a special
NOOP location that indicates that T is bug-free and no repair is
necessary. In practice, we implement the probability distributions
similar to pointer networks [10] (with the addition of an external
vocabulary for repair).

Neural code representation. To learn a neural code representa-
tion, we learn a neural encoding function e(𝑡𝑖 ) that maps each token
𝑡𝑖 to a vector representation. For this, we employ a BPE subtoken
encoder [27] (with a vocabulary of 10K subtokens) to obtain an ini-
tial token embedding by averaging the embedding of its subtokens.

Afterwards, we encode the sequence of token embeddings via a
Transformer encoder [8] (with relative position encodings [28]) to
obtain a contextualized vector representation r𝑖 = e(𝑡𝑖 ).

Localization & Repair module. To finally compute the probabil-
ity distribution over bug locations and repairs, we employ indi-
vidual modules for localization and repair based on the computed
vector representation r𝑖 . The localization module is a multilayer
perceptron that computes a bugginess score for each potential bug
location based on the vector representation r𝑖 and the original to-
ken embedding. The objective of the localization module is to learn
how likely the token 𝑡𝑖 does not fit its surrounding context (repre-
sented by r𝑖 ). The localization probability is computed as a softmax
distribution over all potential bug locations. The repair module is
designed similar to CopyNet [10]. Given the vector representation
r𝑖 of a potential bug location, the repair module computes a repair-
ing score between the bug location and each repair candidate at
token 𝑡 𝑗 (represented by r𝑗 ). In addition, a similar score is obtained
based on token embeddings of an external vocabulary 𝑉 (e.g. other
binary operators). The repair probability score is then computed as
softmax distribution between all repair candidates.

3.3 Finding and repairing real bugs
After the successful training process, the localization and repair
models are typically confronted with new unseen programs with the
objective to identify a potential bug and repair it. This is typically
done [31] by finding the most likely repair for the most likely bug
location (according to the model). However, the most likely repair
at the most likely bug location might not always be meaningful. For
example, while the model might be confident that a bug is located at
a certain location, there might not be a suitable repair candidate that
can actually fix the bug. For this reason, we propose an alternative
strategy: Instead of taking the most likely repair for the most likely
bug location, we search for the most likely meaningful combination
of bug location and its repair (and thus ignoring bug localizations
that cannot be fixed by the model).

Can we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

Beam search decoding. RealiT implements this search strategy
via a beam search decoding [13]. Here, we iterate the top-𝑘 bug
locations according to the model and for each bug location we
again search for the top-𝑘 token repairs. During this process, we
only store pairs that are assigned the highest likelihood together.
Afterwards, we filter the candidate pairs to only meaningful repairs:
If the model predicts a bug location then this should be fixed by
a different token of the same type. Combinations of the special
NOOP location and repair are always meaningful (since nothing will
be changed). Finally, as a result, RealiT computes the most likely
meaningful repair operation:

replace(𝑡𝑙 ′, 𝑟 ′) with ⟨𝑙 ′, 𝑟 ′⟩ = argmax 𝑝 (⟨𝑙, 𝑟 ⟩ | T )

(6)

3.4 Implementation
To effectively measure the impact of mutants and real bug fixes on
the training process and to exclude variances due to implementation
details, we implemented RealiT together with several baselines con-
sidered during our evaluation in a unified framework4. In particular,
we followed the design of Hellendoorn et al. [12] by implementing
a common localization and repair framework with exchangeable
components (e.g. token encoder, localization and/or repair modules).
In the process, we reimplemented or reused state-of-the-art com-
ponents for all employed subcomponents. For example, RealiT and
all Transformer-based baselines are built upon the official BERT
implementation from the transformer library [32]. The localiza-
tion and repair modules together with the graph-based baselines
are implemented closely to the implementation of the PyBugsLab
model [4]. In addition, we reimplemented the code preprocessing
pipelines for tokenization [12] and graph construction [4] (used
for the graph-based baselines) in independent libraries to facili-
tate reuse. Finally, we plan to release all trained checkpoints of
RealiT and all evaluated models. We think that these are not only
valuable for reproducing our results but also provide easy access to
effective models in neural bug localization and repair.

4 EVALUATION
We evaluate RealiT on localization and repair of single token bugs in
Python. To guide our evaluation, we specifically designed individual
experiments to address the following research questions:

RQ1 Can RealiT improve the single token bug localization
and repair performance in comparison to techniques purely
trained on mutants?

RQ2 Is pre-training on mutants necessary for achieving a high

localization and repair performance?

RQ3 Can training with mutants alone be sufficient for achiev-

ing a high performance?

RQ4 Are real bug fixes still helpful if the number of real bug

fixes available for training is further limited?

In RQ1, we compare RealiT with various techniques purely trained
on mutants. RQ2 and RQ3 are designed to explore the effect of
mutation and real bug fixes on the training process. Especially,
since real bugs are hard to obtain, we are interested whether they
are really necessary for training effective bug localizer and repairer.

4https://github.com/cedricrupb/nbfbaselines

Finally, in RQ4, we explore how many real bug fixes are necessary
in practice to improve the localization and repair performance.

4.1 Bug types
To facilitate both mutants and real bug fixes, we require bug types
that can be introduced by existing mutation operators and where
examples of real bug fixes are available. For this reason, we focus
on the four single token bug types in Python that can be generated
by existing mutation operators [7] and for which we can obtain
real bug fixes for training and evaluation [26]. In the following,
we describe the bug types together with the employed mutation
operator in more detail.

Variable Misuse. As the main carrier of the program state, vari-
ables are abundant in source code. Therefore, variable misuses
easily occur when a developer accidentally uses the wrong variable
name instead of the intended one. As specified by Allamanis et
al. [3], the usage of a wrong variable is considered as a variable
misuse if the wrong usage refers to a local variable which can be
fixed by replacing it with another locally defined variable.

Mutator: For generating variable misuses, the mutator replaces a
usage of a locally defined variable with another random variable
defined in the same context.

Wrong Binary Operator. As a traditional example for a mutation
type in mutation testing [7], wrong binary operator bugs appear
when a binary operator is corrupted with a type-equivalent operator
(e.g. == is replaced by !=, but not by <<). For training RealiT, we
consider all types of binary operators including Boolean, arithmetic,
comparison and bitvector operators.

Mutator: Wrong binary operator mutants are generated by re-
placing an binary operator with another random binary operator
of the same type.

Wrong Unary Operator. In addition to binary operators, we also
consider two types of wrong unary operator bugs: logical and
arithmetic negation. In contrast to binary operators, wrong unary
operators are often not replaced but primarily occur when a unary
operator is missing or accidentally added. This includes for example
a forgotten logical negation in a condition or an accidentally added
arithmetic inversion during a calculation.

Mutator: Wrong unary operator mutants are generated by ran-
domly dropping or inserting negation operators (either arithmetic
or logical) in front of an identifier. To ensure that inserted negations
are semantically meaningful, negations are inserted dependent on
the context (e.g. logical negations in conditions and arithmetic
negations in arithmetic expressions).

Wrong Literal. Another common bug type produced by mutation
operators are literal replacements. Similar to mutation testing [7],
we are limited to literal replacements from finite sets of common
literal types. This naturally includes Boolean literal replacement by
replacing True with False and vice versa, but also integer replace-
ments from the set -2, -1, 0, 1, 2.

Mutator: Mutants are generated by replacing literals from a set
of predefined literals with another random literal of the same set
and type.

4.2 Datasets
For training and evaluating RealiT, we require two types of datasets:
a general Github corpus of code snippets and a dataset of real bug
fixes. To achieve comparable results, we employ existing datasets
(if available). For the same reason, we also decided to focus on bugs
in Python function implementations.

Github code corpus. As a general corpus of Python code, we
employ the ETH Py150k dataset [24] containing over 150k program
files from popular Python projects. The dataset is split into 100k
files for training and 50k files for testing. During our evaluation, we
employ the same split and, hence, train only on Python functions
obtained from train split. The test split is used for evaluating the
performance on mutants. We extract all top level functions and
deduplicate the datasets such that the Python functions used for
training only occur once in the training dataset and do not occur in
the test split. In total, our training corpus contains more than 360k
Python function implementations (after filtering).

Real bug fixes. For obtaining real bug fixes at a sufficient scale, we
employ the SSB-9M dataset [26] of over 9M general single statement
bug fixes in Python. The dataset does not include the necessary
implementation code itself but references the original commits in
the repositories in addition to other useful metadata. This includes
information about the code change as a Unix diff and whether the
code change appears inside a function. Based on this information,
we first pre-filtered the dataset for bug fixes that likely fall into one
of our bug categories. After the filtering process, we then mined
the function code from the original repositories. Since not all bug
types can be identified purely on the code difference (e.g. a variable
misuse requires that all variables are defined in scope), we filtered
and deduplicated the resulting dataset of buggy Python functions
for a second time. This process has lead to around 35k examples of
real bug fixes that match at least one of our bug types. Finally, we
use 33k examples for training and hold out around 2k examples as
a validation set used during training.

Test benchmarks. We employ two test benchmarks to evaluate
the performance on the localization and repair task. To evaluate
the localization and repair performance on real bugs, we employ
the PyPIBugs benchmark [4]. The benchmark is a dataset of 2374
real-world single statement bugs and their fix derived from open
source projects. The benchmark is hand-filtered and therefore it is
likely that each included bug represents a real world bug. We only
considered single token bugs (which excludes argument swaps) in
functions where the implementation is still publicly accessible5.
This produced a real world test benchmark of 2028 real-world bugs.
To avoid an overlap between train and test set, we excluded all
near duplicates [2] from our training datasets. Additionally, we
also employ the test portion of the Github corpus as a mutant
benchmark. For this, we extend the corpus of correct code snippets
with up to 9 mutants per snippet.

5 RESULT
In this section, we discuss our evaluation results with the ultimate
goal of answering our research questions.

5The benchmark consists of references to bug fixing commits. We found that not all
bug fixing commits were publicly accessible at the time of writing.

Richter and Wehrheim

5.1 RQ1: RealiT in comparison?
For answering the first research question, we evaluate whether
RealiT improves the single token bug localization and repair per-
formance by training with real bug fixes. Since we are interested in
the impact of real bug fixes on the training process, we compare
our results with several baseline algorithms trained purely on mu-
tants. For the comparison, we consider bug localization and repair
models based on recursive neural networks (RNN) [31], transformers
(absolute positions) [12], graph neural networks (GNN6) [4] and
GREAT [12]. All baseline models are trained in a supervised setting
purely on mutants. The training dataset is constructed similar to
the pre-training dataset used for RealiT (with 𝑘 = 5 mutants in-
jected). The baselines are trained for 300 epochs (a 200k examples
per epoch) with early-stopping on our validation set. For RealiT,
we skip the last epoch and instead fine-tune on real bug fixes.

Real-world performance. To begin with, we start by considering
the performance of RealiT on our real-world benchmark. Table 2
provides an overview over our evaluation results. We measured the
joint accuracy of localizing and repairing real bugs (Joint) in addi-
tion to the localization accuracy (Loc.) of finding the bug location
and the repair accuracy (Repair) of finding the real bug fix given
the bug location. In this section, we focus on the upper part of the
table and we consider the results on our real-world benchmark.

We observe that RealiT significantly outperforms all baseline
algorithms trained purely on mutants both in localization and repair
of real single token bugs. Interestingly enough, we find that the
highest relative gain obtained from fine-tuning on real bug fixes
can be achieved for the localization performance (with a nearly
2x improvement). This indicates that for effective localization of
human made bugs we actually need to learn from human made bug
fixes. Still, the bug localization remains harder than bug repair as
RealiT can fix more than 73% of all bugs when the bug location is
given. Therefore, it is important to investigate into better strategies
for bug localization (potentially by integrating techniques from
static analysis).

Finally, the fact that significant performance improvements are
observable in both localization and repair suggests that real bug
fixes exhibit exploitable statistics for localization and repair. We
will further explore this in RQ2 and RQ3.

Localization and repair of mutants. While our ultimate goal is
to find and fix real bugs, we also measured the localization and
repair accuracy of RealiT for artificial mutants. Surprisingly, we
observe that RealiT performs worse than most baseline models
both in localization and repair after fine-tuning on real bug fixes.
Interestingly, this is not a limitation of the RealiT model as the
version of RealiT trained purely on mutants performs competitively
or even better than all baselines in localizing and repairing mutants.
Therefore, the fine-tuning on real bugs encourages RealiT to “for-
get” some (potentially spurious) patterns that were used to detect
mutants but do not help for identifying real bugs. In addition, this
provides further evidence that there might exist mutants that either
do not represent real bugs or represent bugs that are highly unlikely
to appear in reality. Finally, this observation is also interesting for

6Our evaluation setup differs slightly from [4] in that only individual function im-
plementations are considered. Therefore, graph level information that would require
access to the implementation context cannot be computed.

Can we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

Table 2: Evaluation results for bug detection and repair on mutants and real bug fixes

RNN [31]
Transformer [12]
GNN [4]
GREAT [12]
RealiT (ours)

RealiT - without beam search decoding
RealiT - without pre-training on mutants
RealiT - without fine-tuning on real bug fixes
RealiT - with fine-tuning on postfix mutants
RealiT - with reduced mutant frequency (5x)

FPR%

33.92
25.59
25.29
29.98
29.53

22.96
19.32
33.41
27.72
31.75

Real Bugs (PyPIBugs)

Mutants

Joint

Loc.

Repair

Joint

Loc.

Repair

9.47
18.98
18.24
19.03
39.00

36.69
12.67
25.10
27.66
33.28

13.36
23.02
23.82
23.62
44.23

41.86
16.07
30.92
32.64
38.56

47.88
59.52
53.74
56.31
73.52

73.52
40.38
65.53
67.40
68.59

52.39
74.04
66.11
70.76
67.09

65.27
2.37
77.41
78.95
65.32

61.49
81.26
75.08
78.84
75.95

74.08
8.58
84.49
85.12
74.74

80.06
88.74
84.59
86.99
85.66

85.66
31.34
90.18
90.55
83.90

the evaluation of localization and repair models. As there is clearly
no correlation between the real world performance and the per-
formance on mutants when the model is fine-tuned on real bug
fixes, performance gains on mutants independent from real world
performance become difficult to interpret.

False positives. We also measured the false positive rate (FPR) of
RealiT on bug-free code snippets. Here, we employ the original test
set of our Github corpus. Our results are also summarized in Table 2.
We observe that RealiT has a false positive rate comparable to the
other baselines – only outperformed by the Transformer and GNN.
However, we believe that an increase of 3% more false positives is
still acceptable as RealiT localizes and fixes nearly twice as many
real bugs. In addition, we also evaluate a version of RealiT without
beam search decoding (i.e. using the repair with the highest likeli-
hood). The results are also shown in the lower part of Table 2. We
observe that while beam search decoding improves the localization
performance by up to 3%, it also induces a worse false positive rate
compared to the model without beam search decoding. This is a
common trade off between a higher true localization performance
with a worse false positive rate.

5.2 RQ2: Are mutants necessary?
As we have seen in RQ1, fine-tuning on real bug fixes does improve
localization and repair of real bugs. This raises the question whether
mutants are necessary for the performance gain or if the same
performance can be achieved with real bugs alone. To answer this
question and therefore RQ2, we trained two additional versions of
RealiT: (1) a version of RealiT that is not pre-trained on mutants
and (2) a version of RealiT that is not fine-tuned on real bug fixes.
We evaluate both versions again on all benchmarks.

Mutants vs real bugs. We start by comparing the two new ver-
sions of RealiT. Our evaluation results are summarized with all other
results in Table 2. We observe that training on mutants outperforms
training on real bug fixes only. It is likely that RealiT overfits the
smaller training dataset of real bug fixes and therefore fails to gener-
alize to complete new unseen bugs. In contrast, the version purely
trained on mutants has learned from a variety of mutants during
training (some of which are likely similar to real bugs). However,

when evaluated on bug-free code snippets only, we see that Re-
aliT trained only on real bug fixes clearly outperforms all other
techniques in terms of false positive rate. This could again indicate
that some mutants in the training dataset are not bug inducing (e.g.
a mutation that replaces <= with != without changing the func-
tion behavior) which guides the model to detect these structures in
bug-free code.

Training with mutants and real bugs. We now compare the two
variants of RealiT with our original RealiT model. We find that fine-
tuning on real bug fixes significantly improves the performance of
RealiT over the already strong baseline of training on mutants alone.
Interestingly enough, this does not only hold for localization and
repair of real bug fixes but also on the false positive rate on bug-free
code. This shows that pre-training on mutants and fine-tuning on
real bugs combines the strengths of both the high localization and
repair performance (by training on mutants) and the bug detection
accuracy (by training on real bugs). Therefore, we see that mutants
are necessary to achieve the high performance of RealiT but fine-
tuning on real bugs provides additional improvements.

Effect on individual bug types. Since the effect of pre-training
and fine-tuning seems to be complementary, we are also interested
in how the training affects the performance on individual bug types.
Table 3 summarizes our results on the real-world test benchmark
divided into single token bug types. First of all, we again find that
training on both mutants and real bugs does improve performance
in both localization and repair on all bug types. However, the margin
of the performance gain is dependent on the bug type. For example,
we see the highest improvement for Wrong Binary Op where
training on real bugs alone already yields high performance. To
answer our research question also for individual bug types, pre-
training on mutants can also be crucial for the performance on
individual bug types (where we observe a significant improvement
for at least three bug types Wrong Assign Op, Wrong Literal and
Variable Misuse).

5.3 RQ3: Are mutants sufficient?
Our evaluation for RQ2 has shown that training on mutants is cru-
cial for obtaining high performing RealiT models. Still, it is not

Richter and Wehrheim

Table 3: Evaluation results for bug detection and repair on different bug types

Bug type

RealiT

Mutants only

No Mutants

Joint

Loc.

Repair

Joint

Loc.

Repair

Joint

Loc.

Repair

Wrong Assign Op
Wrong Binary Op
Wrong Boolean Op
Wrong Comparison Op
Wrong Literal
Variable Misuse

20.45
56.34
42.31
36.95
24.42
39.87

29.54
59.15
42.31
51.47
32.56
42.62

70.45
84.51
95.05
67.00
76.74
71.75

9.10
14.08
23.08
19.70
19.77
28.73

13.64
28.17
24.18
35.22
22.09
31.88

52.27
39.44
93.41
57.64
77.91
65.13

2.27
30.99
21.43
23.40
9.30
7.43

2.27
35.21
21.97
33.74
12.79
9.04

65.91
70.42
81.87
57.14
46.51
25.75

clear whether mutants on its own can be sufficient for training
RealiT. In other words, there might exist a mutation configura-
tion that achieves the same performance trained on mutants based
on the same base datasets. To answer RQ3, we designed several
experiments.

Mutation frequency. We trained several versions of RealiT by
varying the mutation frequency (up to 1x, 3x, 5x, 10x, 100x and
1000x unique mutants per code snippet). For the comparison, we
measured the performance of each trained model before and after
fine-tuning on real bug fixes. The models are evaluated on our real
bugs validation set. Figure 3 gives an overview of our results for bug
localization and repair accuracy independently. The configuration
0x represents a version of RealiT only trained on real bug fixes. First
of all, in contrast to common believe [12], we observe that increasing
the number of mutants up to 100x generated mutants per code
snippet leads to a performance improvement for both localization
and repair7. This is surprising as the number of unique mutants per
code snippet is limited (with an average of 85 unique mutants per
code snippet) and, henceforth, buggy programs with more mutant
candidates are oversampled. Still, we found that increasing the limit
of mutants beyond 100x (and thereby oversampling code snippets
in our dataset that provide up to 200k unique mutants) actually
decreases the localization and repair performance.

Now, when also considering the performance on the validation
set after fine-tuning, we find that fine-tuning always provides a
significant boost over the models trained solely on mutants for
both localization and repair accuracy. However, we still observe
that the performance gain for localization is higher than for repair
(especially as we increase the number of mutants). Surprisingly, we
also observe that the gap between the model performances before
and after fine-tuning on real bug fixes shrinks as we increase the
number of mutants generated per code snippet (up to 100x). While
this could indicate that simply scaling the number of mutants can
be sufficient for achieving a high repair accuracy, the gap actually
starts increasing again after scaling beyond 100x mutants per code
snippet. Therefore, we can conclude that while mutants alone can
significantly improve the performance they are not sufficient in our
training setup for achieving the same high performing localization
and repair models as obtained by fine-tuning on real bug fixes with
RealiT.

7We observe the same trend for joint localization and repair which is not shown here
for brevity.

(a) Localization accuracy

(b) Repair accuracy

Figure 3: Effect of mutant frequency during training on the
real world validation performance. The gray dotted line rep-
resents the average number of unique mutants that can be
generated per code snippet.

Training with postfix mutants. While it seems that mutants
alone introduced in arbitrary code from our code corpus is not
sufficient for closing the performance graph to fine-tuned models,
it is unclear whether mutants can be sufficient when introduced
in an implementation context that is more typical for a real bug.

0x1x3x5x10x100x1000xmutant frequency in training set0%5%10%15%20%25%30%35%40%45%loc. accuracy on validation set1.7 x1.4 xRealiT (mutants only)RealiT0x1x3x5x10x100x1000xmutant frequency in training set0%10%20%30%40%50%60%70%75%repair accuracy on validation set1.3 x1.2 xRealiT (mutants only)RealiTCan we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

To test this, we designed an additional experiment where we fine-
tuned RealiT on postfix mutants (i.e. mutants that are produced by
first applying the real bug fix and then reintroducing a bug with
a mutation operator). Our results are also shown in Table 2. We
observe that even fine-tuning on postfix mutants provides a slight
boost in performance both on localization and repair of mutants
and real bugs. Surprisingly, the boost is slightly higher for real bugs
than for mutants even though we only increased the number of
mutants. Still, we find that a model trained on real bug fixes clearly
outperforms a model trained on postfix mutants when evaluated
on real bug fixes. Since the performance of detecting mutants does
not decrease when training on postfix mutants (similar to scaling
the number of mutants generated), we actually conclude that the
performance gain is likely a scaling effect and can not necessarily
be attributed to the mutation context.

In total, we find that training on mutants alone is not sufficient
for achieving a high performing RealiT model. In addition, our
results show that training on real bug fixes is especially helpful for
the localization of real bugs which is hard to obtain by training on
mutants alone.

5.4 RQ4: How many bug fixes are necessary?
Obtaining a dataset of multiple thousand real bug fixes can be chal-
lenging especially if the considered bug type occurs less frequent in
open source projects. For this reason, we aim to explore how much
the size of the fine-tuning dataset (the number of real bug fixes)
influence the final performance of RealiT. Therefore, we evaluate
three variants of RealiT pre-trained on 1x, 5x and 100x mutants
per code snippet which we then fine-tune on several subsamples of
our real bug fix datasets. We consider subsamples of 1% (334), 3%
(996), 5% (1.658), 10% (3.314), 30% (9.936), 50% (16.559), 70% (23.180),
90% (29.802) of all bug fixes. To obtain more stable results, we fine-
tune our models on three subsamples per sample size and evaluate
fine-tuned models on our validation set. Averaged results for all
three RealiT variants fine-tuned on the generated subsamples are
reported in Figure 4.

Impact of the real bug fix dataset size. We can observe a clear
trend that more real bug fixes lead to an improved performance
across all RealiT variants. This holds true even for small fine-tuning
datasets of around 1000 (less than 5% of the original dataset size)
bug fixes. As reported by the authors of ManySStuBs4J [16] or
PySStuBs [14], real bug fix collections of this size can be obtained by
mining the top 1000 most popular Github projects for the respective
language.

Scaling real bug fixes vs. scaling mutants. Although we have
seen that fine-tuning on more real bug fixes increases the perfor-
mance, it is actually difficult to scale up the number of real bug
fixes (as the number of publicly accessible projects to mine real
bug fixes from is limited). In contrast, generating more mutants
per code snippet is more cost effective. For example, to achieve
the same performance gain obtained from scaling the number of
mutants generated from 5x to 100x, we have to fine-tune on at least
10% of our real bug fix dataset (3314 bugs). Still, although scaling
the number of mutants is preferred, the scaling effect is however
limited, as we have seen in RQ3.

Figure 4: Effect of real bug fixes on the fine-tuning perfor-
mance on the validation set. The x-axis is the percentage of
the bug fix dataset used for fine-tuning. Gray dotted lines
mark datasets that exceed 1k and 10k examples respectively.

6 THREATS TO VALIDITY
Although a variety of learning-based bug localization and repair
models have been developed in recent years, there does not exist a
unique setup for training and benchmarking these models which
is universally accepted. Therefore, even though we implemented
our baselines close to the reference implementation, the resulting
trained models might behave differently than in the setup they were
originally developed for. To still achieve comparable results, we
designed our evaluation to replicate prior studies [12] on neural bug
localization and repair models as close as possible. For example, we
adopted the same publicly accessible Github corpus ETH Py150k, a
similar architectural design and similar baselines as employed by
Hellendoorn et al. [12]. To further support a wider range of bug
types which allowed us to exploit the hand validated benchmark
PyPIBugs, we adjusted the architecture and mutation process simi-
lar to Allamanis et al. [4]. Still, our evaluation results for the baseline
algorithms (in Table 2) are slightly different than the results of prior
studies. For example, we found that while graph-based models such
as GNNs and GREAT still perform better in localizing and repairing
real bugs, Transformers show a surprisingly strong performance on
mutants. Note however that Hellendoorn et al. [12] anticipated this
result (even though they only evaluated on variable misuses) when
the models are trained for a longer duration – which we did by
training on approximately 2.4x more training examples. In contrast
to the results of Allamanis et al. [4], we observe that graph-based
models underperform in our evaluation setup which we attribute to
two main differences: (1) for a fair comparison, all models only have
access to the function implementation without the implementation
context which prohibits the computation of type related or call
structure related information exploited by the graph-based models
and (2) we trained all models on a different (potentially smaller)
dataset. Although integrating the type of information and training
on a larger dataset would potentially benefit all baselines, the perfor-
mance ranking between architectures might differ. However, since
our experiments showed that the performance gain due to training

0%1%3%5%10%30%50%70%90%100%percent of real bug fixes (fine-tuning)10152025303540joint loc. & repair accuracy on validation set1k10kRealiT(100x)RealiT(5x)RealiT(1x)on real bug fixes is unique and the effect could not be replicated by
training on mutants, we expect that adapting our evaluation setup
has little to no influence on our evaluation outcome.

7 RELATED WORK
We discuss the most related previous or concurrent work that (1)
tackle single token bug localization and repair with alternative
training strategies, (2) exploit real bug fixes for automatic program
repair or code mutations and (3) consider alternative pre-train-and-
fine-tune techniques.

Single token bug localization and repair. The detection and re-
pair of single token bugs have been explored in previous work [3,
4, 12, 21, 22, 25, 31]. Allamanis et al. [3] addressed the detection
and repair of variable misuse bugs (which we also considered in
this work) by representing programs as graphs. Vasic et al. [31]
proposed a joint model for same task and Hellendoorn et al. [12]
explored alternative program representations. These techniques
all have in common that they do not learn from real bug fixes but
from artificially mutated code. In contrast, while RealiT employs
a similar Transformer-based architecture as discussed by Hellen-
doorn et al. [12], we showed that integrating real bug fixes in the
training process is crucial for the localization and repair of real
bugs. More recent work [4, 21, 25] also showed that the quality of
training data is important for effective bug localization and repair.
For example, employing a more realistic mutator [21, 25] (i.e. a
mutator that is more likely to reproduce a real bug) or learning to
inject hard to find bugs [4] can both improve the localization and
repair performance. However, the integration of these approaches
often increases the complexity by requiring to learn a mutation
operator either prior and concurrent to the training process. With
RealiT, we showed that integrating real bug fixes, while relying
on simpler and easier to implement mutation operators, can be
sufficient to obtain a significant improvement in real bug localiza-
tion and repair performance. Interestingly enough, a concurrent
work [11] also explored whether real bug fixes have an impact on
the performance of learning-based bug detectors. Similar to RealiT,
their model is pre-trained on mutants and then fine-tuned real
bug fixes. Surprisingly, while the authors found that fine-tuning
on real bug fixes improves precision (i.e. the number of correct
programs classified as buggy), the recall (i.e. the number of real
bugs detected and repaired) actually suffers. In contrast, we find
that RealiT improves the number of bugs detected and repaired
significantly while training on real bug fixes can also decrease the
false positive rate. We attribute the difference in our findings to
significant differences to the RealiT training process: (1) the number
of real bug fixes we fine-tune on is several magnitudes larger, (2)
the number of mutants generated per code snippet is significantly
higher and (3) the distribution of buggy and bug-free programs is
balanced both during pre-training and fine-tuning. We believe that
especially (3) is key to success of RealiT. Training on an unbalanced
dataset (with significant more bug-free than buggy code) risks that
the model defaults to not detecting a bug (which would result in a
higher precision and lower recall by design).

Learning from real bug fixes. Real bug fixes are not only a valu-
able resource for learning to localize and repair single token bugs
but they can also be effectively exploited for automatic program

Richter and Wehrheim

repair [5, 6, 18, 19, 30] or code mutations [21, 29]. SequenceR [6],
for example, learns from thousands of bug fixes to predict one-line
bug patches. Dlfix [18] and CoCoNuT [19] improved the repair
performance by proposing more effective learning strategies. In
contrast to RealiT, however, these techniques are designed to only
repair a given program location and, hence, whether a program
is buggy and where the bug has to be fixed has to be known be-
forehand. In addition, these techniques are often trained on real
bug fixes only without considering mutants for the training pro-
cess. We showed that learning from mutants is actually crucial to
achieve high performing models. This observation is also supported
by DrRepair [33] which showed that pre-training a repair models
on artificial errors improved the repair performance on syntactic
errors. Still, their approach rely on a compiler to detect this type of
bugs. The type of single token bugs which we considered in this
work are typically missed by a compiler.

Code mutation addresses the inverse problem of injecting a bug
into a correct program. Tufano et al. [29] and Patra and Pradel [21]
showed that bug fixes can be effectively leverage to learn code
mutations by learning to replicate the original bug. Interestingly,
Yasunaga et al. [34] showed that repeatedly training a breaker and
fixer that initially learn from real bug fixes but then provide training
data for each other actually improves the performance of the fixer
to repair syntactic bugs. While our work showed that real bug fixes
are also crucial for bug detection, we believe that exploiting real
bug fixes in the mutation process for training bug detection and
repair models can be a promising direction for future work.

Pre-training and fine-tuning. Pre-training on large corpora of
fuzzy data and then fine-tuning on a specific task with a smaller
dataset has been shown to be highly successful in domains such
as natural language processing [8, 23], image processing [17] and
most recently programming language processing [9, 15]. In con-
trast to RealiT, these techniques are often pre-trained on a generic
unrelated task where data is available before fine-tuning them on a
specific task. RealiT, however, is trained and fine-tuned with same
architecture with largely the same objective of identifying and
repairing buggy (or mutated) code.

CuBERT [15] showed that pre-training on a generic corpus of
Python code can improve the detection performance on variable
misuses. However, the authors employed mutants instead of real
bug fixes in the fine-tuning phase. In contrast, RealiT is pre-trained
on mutants and then fine-tuned on real bug fixes. A combination
of these two approaches by applying RealiT on top of a pre-trained
model would be interesting and we leave this open for future work.

8 CONCLUSION
In this work, we explore the effect of training on real bug fixes
and mutants on the performance of bug localization and repair
models. For this, we propose RealiT, a novel pre-train-and-fine-tune
approach for learning to localize and repair bugs with Transformers.
RealiT can effectively utilize both mutants and real bug fixes during
training by first pre-training on mutants and then fine-tuning on
real bug fixes. Our evaluation on thousands of real bugs obtained
from real Python projects showed that RealiT can significantly
improve the localization and repair of real bugs in contrast to models
solely trained on mutants. In addition, our experiments showed (1)

Can we learn from developer mistakes?
Learning to localize and repair real bugs from real bug fixes

that pre-training on mutants plays an important role for achieving
the performance level, (2) that mutants alone are however not
sufficient to unlock the potential of RealiT and (3) that a high
number of real bug fixes is actually necessary for achieving a high
performing model.

Based on these observations, we see as future work the integra-
tion of more realistic data in the training process of neural bug
localization and repair models. For example, training on more real-
istic mutants could boost the performance even before fine-tuning
on real bug fixes. In addition, it might also be interesting to explore
the effect of other – even unrelated – types of bug fixes on the
training process of neural bug localization and repair approaches.
Integrating more supported bug types also allows us to exploit more
real bug fixes found in open source projects.

Finally, to conclude, RealiT demonstrates that neural bug lo-
calization and repair models can effectively learn from developer
mistakes, in form of real bug fixes, to localize and repair real bugs.

REFERENCES
[1] Abdulaziz Alaboudi and Thomas D. LaToza. 2021. An Exploratory Study of
Debugging Episodes. CoRR abs/2105.02162 (2021). https://arxiv.org/abs/2105.
02162

[2] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software. 143–153.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017. Learning
to represent programs with graphs. arXiv preprint arXiv:1711.00740 (2017).
[4] Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021. Self-
supervised bug detection and repair. Advances in Neural Information Processing
Systems 34 (2021), 27865–27876.

[5] Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019. Getafix:
Learning to fix bugs automatically. Proceedings of the ACM on Programming
Languages 3, OOPSLA (2019), 1–27.

[6] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys
Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence
learning for end-to-end program repair. IEEE Transactions on Software Engineering
47, 9 (2019), 1943–1959.

[7] Anna Derezińska and Konrad Hałas. 2014. Analysis of mutation operators for
the python language. In Proceedings of the Ninth International Conference on
Dependability and Complex Systems DepCoS-RELCOMEX. June 30–July 4, 2014,
Brunów, Poland. Springer, 155–164.

[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,
4171–4186. https://doi.org/10.18653/v1/N19-1423

[9] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint arXiv:2002.08155
(2020).

[10] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying
mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393
(2016).

[11] Jingxuan He, Luca Beurer-Kellner, and Martin Vechev. 2022. On Distribution

Shift in Learning-based Bug Detectors. arXiv preprint arXiv:2204.10049 (2022).

[12] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David
Bieber. 2019. Global relational models of source code. In International conference
on learning representations.

[13] Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural
networks for discourse compositionality. arXiv preprint arXiv:1306.3584 (2013).
[14] Arthur V Kamienski, Luisa Palechor, Cor-Paul Bezemer, and Abram Hindle. 2021.
Pysstubs: Characterizing single-statement bugs in popular open-source python
projects. In 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR). IEEE, 520–524.

[15] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Pre-
trained Contextual Embedding of Source Code. CoRR abs/2001.00059 (2020).
arXiv:2001.00059 http://arxiv.org/abs/2001.00059

[16] Rafael-Michael Karampatsis and Charles Sutton. 2020. How Often Do Single-
Statement Bugs Occur?: The ManySStuBs4J Dataset. In MSR. ACM, 573–577.

https://doi.org/10.1145/3379597.3387491

[17] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
Sylvain Gelly, and Neil Houlsby. 2020. Big transfer (bit): General visual represen-
tation learning. In European conference on computer vision. Springer, 491–507.

[18] Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. Dlfix: Context-based code
transformation learning for automated program repair. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering. 602–614.
[19] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and
Lin Tan. 2020. Coconut: combining context-aware neural translation models
using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT
international symposium on software testing and analysis. 101–114.

[20] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.

Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016).
[21] Jibesh Patra and Michael Pradel. 2021. Semantic bug seeding: a learning-based
approach for creating realistic bugs. In Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. 906–918.

[22] Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning approach to name-
based bug detection. Proceedings of the ACM on Programming Languages 2,
OOPSLA (2018), 1–25.

[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the
limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.
Res. 21, 140 (2020), 1–67.

[24] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic model for

code with decision trees. ACM SIGPLAN Notices 51, 10 (2016), 731–747.

[25] Cedric Richter and Heike Wehrheim. 2022. Learning Realistic Mutations: Bug
Creation for Neural Bug Detectors. In 2022 IEEE Conference on Software Testing,
Verification and Validation (ICST). IEEE, 162–173.

[26] Cedric Richter and Heike Wehrheim. 2022. TSSB-3M: Mining single statement

bugs at massive scale. arXiv preprint arXiv:2201.12046 (2022).

[27] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, Berlin, Germany, 1715–1725. https:
//doi.org/10.18653/v1/P16-1162

[28] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with
relative position representations. arXiv preprint arXiv:1803.02155 (2018).
[29] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. Learning how to mutate source code from
bug-fixes. In 2019 IEEE International Conference on Software Maintenance and
Evolution (ICSME). IEEE, 301–312.

[30] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing
patches in the wild via neural machine translation. ACM Transactions on Software
Engineering and Methodology (TOSEM) 28, 4 (2019), 1–29.

[31] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.
2019. Neural program repair by jointly learning to localize and repair. arXiv
preprint arXiv:1904.01720 (2019).

[32] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations. Association for Computational
https://www.aclweb.org/anthology/2020.emnlp-
Linguistics, Online, 38–45.
demos.6

[33] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program
repair from diagnostic feedback. In International Conference on Machine Learning.
PMLR, 10799–10808.

[34] Michihiro Yasunaga and Percy Liang. 2021. Break-it-fix-it: Unsupervised learning
for program repair. In International Conference on Machine Learning. PMLR,
11941–11952.

Richter and Wehrheim

For encoding the positions of tokens, we employ relative position
encoding [28] as we found that this performed better. A comparison
of Transformer with and without relative position encoding can be
found in Table 2 ("RealiT - without fine-tuning on real bug fixes"
vs "Transformer").

GNN. For the graph neural network baseline, we followed the
design of Allamanis et al. [4] as close as possible. We reimplemented
the GNN based on the reference implementation provided by the
authors. The GNN consists of 8 message propagation layers with a
skip connection between the first and the fourth layer and between
the fourth and the eights layer. The node hidden size is set to 256.
In addition, we also adapted the general architecture to match
the reference implementation. Instead of averaging the subtoken
embeddings, we employ max pooling as the authors found that
this performed better. In addition, we also reimplemented the same
localization and repair head.

Remaining baselines. The remaining baselines employ the same
hyperparameters as specified by Hellendoorn et al. [12]. The Trans-
former is a 6-layer encoder with absolute position embeddings (512
hidden size, 2048 intermediate size, 8 attention heads). GREAT uses
a similar 6-layer architecture with the addition of an edge bias.
The RNN is a 2-layer bidirectional recursive neural network with
hidden size of 512.

A MODEL ARCHITECTURES
For our evaluation, we implemented our baselines in a common
code base. All neural network modules are implemented in PyTorch.
In the following, we discuss the general architecture used for neural
bug localization and repair and the design and hyperparameters
individually for all baseline models.

General. All our models follow the general structure proposed
by Hellendoorn et al. [12]. The architecture consists of an input
module (for mapping tokens to vectors), a central encoding model
and localization and repair head. For constructing our baselines,
we change the central encoding model. The remaining structure
remains the same (if not specified otherwise). For the input module,
we use a BPE subtoken encoder with a vocabulary of 10k subtokens
and embed each token by averaging its subtoken representation.

Similar to Allamanis et al. [4], we employ dedicated heads for

localization and repair.

For localization, we use an architecture similar to pointer net-
works [20]. Given a program T = 𝑡0, 𝑡1, . . . , 𝑡𝑛 and let 𝑡𝑙 be a po-
tential bug location, we then compute the initial token embedding
𝑒𝑙 and the contextual vector representation r𝑙 coming from the
encoding model. Based on these representations, we compute a
buggyness score for each potential bug location with a simple MLP:
𝑠𝑙 = W2𝜎 (W1 (r𝑙 ||𝑒𝑙 ||r𝑙 − 𝑒𝑙 ))
Here, W2 ∈ R1×𝑑 , W1 ∈ R𝑑×3𝑑 are learnable projections of the
MLP. The intuition here is that the MLP should learn the correct
token representation r𝑙 which then would disagree with the initial
token embedding 𝑒𝑙 if 𝑡𝑙 is buggy. We model the distribution 𝑝𝑙𝑜𝑐
by a softmax over all buggyness scores.

Based on the same intuition used for localization, we designed
our repair module. Given a potential bug location represented by
r𝑙 , the repair module computes a repair score for all other tokens
(represented by r𝑗 ) similar to an attention mechanism:

𝑟𝑒𝑝𝑙 𝑗 =

W𝑞 (r𝑙 )(W𝑘 (r𝑗 ))𝑇
√
𝑑

Here, W𝑞 ∈ R𝑑×𝑑 , W𝑘 ∈ R𝑑×𝑑 are learnable projections. To include
an external vocabulary 𝑉 , we represent each vocabulary entry a
learnable vector 𝑣 𝑗 ∈ R𝑑 and compute a repair score in a similar
way:

𝑟𝑒𝑝𝑙 𝑗 =

W𝑞 (r𝑙 )(𝑣 𝑗 )𝑇
√
𝑑

Finally, 𝑝𝑟𝑒𝑝𝑎𝑖𝑟 is computed by a softmax over all repair scores
(token based and vocabulary based together).

We train all models using the Adam optimizer with learning
rate 1𝑒 − 4 and a linear warm-up of 800 steps, additionally clipping
gradient norms at 1.0 (0.5 for the GNN). Models are trained with
weight decay of 0.1 for regularization. During training, we consider
function implementations with up to 1024 tokens (1536 nodes for
the GNN) and trained with minibatch sizes of up to 12.5K tokens
(nodes).

RealiT. We follow the same architectural design for RealiT. As an
encoding model, we employ a 6-layer Transformer encoder [8], a
hidden size of 512, an intermediate size of 2048 and 8 attention
heads. During training, we use a dropout regularization of 0.1.

