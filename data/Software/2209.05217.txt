Towards Reliable and Scalable Linux Kernel CVE
Attribution in Automated Static Firmware Analyses

Ren´e Helmke and Johannes vom Dorp
Fraunhofer FKIE, Bonn, Germany, Email: {rene.helmke, johannes.vom.dorp}@fkie.fraunhofer.de

2
2
0
2

p
e
S
2
1

]

R
C
.
s
c
[

1
v
7
1
2
5
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—In vulnerability assessments, software component-
based CVE attribution is a common method to identify possibly
vulnerable systems at scale. However, such version-centric ap-
proaches yield high false-positive rates for binary distributed
Linux kernels in ﬁrmware images. Not ﬁltering included vul-
nerable components is a reason for unreliable matching, as
heterogeneous hardware properties, modularity, and numerous
development streams result in a plethora of vendor-customized
builds. To make a step towards increased result reliability while
retaining scalability of the analysis method, we enrich version-
based CVE matching with kernel-speciﬁc build data from binary
images using automated static ﬁrmware analysis. We open source
an attribution pipeline that gathers kernel conﬁguration and
target architecture to dry build the present kernel version
and ﬁlter CVEs based on affected ﬁle references in record
descriptions. In a case study with 127 router ﬁrmware images, we
show that in comparison to naive version matching, our approach
identiﬁes 68% of all version CVE matches as false-positives and
reliably removes them from the result set. For 12% of all matches
it provides additional evidence of issue applicability. For 19.4%,
our approach does not improve reliability because required ﬁle
references in CVEs are missing.

I. INTRODUCTION

Safety, security, and privacy threats arise alongside embed-
ded system markets. Growing device numbers1 inﬂate attack
surfaces, raising impact and scope of newly found software
vulnerabilities in domains pivotal to society [1]–[4]. Thus, it
is important to maintain the software security of these systems.
Embedded devices boot into ﬁrmware: lightweight software
packages tailored to the device’s use case and limited re-
sources [5]. Similar to general purpose operating systems,
they consolidate services, drivers, and interfaces, but in con-
trast are highly specialized towards their purpose. Minimizing
costs, vendors commonly rely on Software Development Kits
(SDKs), which make heavy use of open source. This gives
the advantage of large development communities quickly
providing patches when security issues arise.

Linux serves as conﬁgurable, modiﬁable, slim, and pow-
erful system layer in many ﬁrmware images [6], [7]. It is
open source, modular, and supports ﬂavors of the MIPS and
ARM Instruction Set Architectures (ISAs) commonly used in
embedded devices. Vendors conﬁgure kernel builds to include
all required functions, remove unused components, and add
custom functionality. The compiled kernel is then distributed
in binary form as part of the ﬁrmware.

As of 2022, there are over 2,900 Common Vulnerabilities
and Exposures (CVE) documenting security issues of varying

1https://iot-analytics.com/number-connected-iot-devices/, Accessed: 2022-09-05.

severity affecting different Linux kernel versions [8]. Yet, the
ﬁrmware of various widely spread devices contains obsolete or
end of life kernels [6], [7]. This raises questions about device
exploitability regarding well-known vulnerabilities.

Intuitively, reproducible exploitation of the target provides
undeniable proof for CVE attribution. However, science has
yet to ﬁnd scalable and effective methods for such dynamic
veriﬁcation in binary analysis [9], [10]. Heterogeneous system
properties require custom solutions: For example, exploit
code may be unavailable for the target ISA [10]. Resource
constraints and missing build chains pose further challenges,
as devices might be the only testing platforms available [9].
Emulation and re-hosting are options, but require substantial
efforts to establish device compatibility [9], [11], [12].

Static analysis serves heuristics to ﬁnd imperfect proof for
CVE attribution, e.g., verifying code presence [13]. However,
many approaches do not scale well as they require considerable
manual work and deep knowledge of each CVE [10]. Parts are
automatable, but needed data may be unavailable or incorrect
in repositories [14], [15]. Also, automation becomes increas-
ingly challenging in lights of proprietary formats, obfuscation,
compiler optimizations, and symbol stripping [5], [10].

In lack of applicable methods, large-scale approaches like
ﬁrmware analysis tools [16]–[18] and studies [6], [7], [19]
commonly attribute bugs by matching versions against CVE
databases. Trading in reliability for applicability, their results
need manual veriﬁcation but may raise awareness for potential
security issues.

In 2020, we used version matching as part of a large-scale
empirical study on home router security [7]. For the Linux
kernel, this method proves exceptionally unreliable: Due to
custom build conﬁgurations, we can not assume that kernels
include components ﬂagged as vulnerable for the subject
version. E.g., CVEs affecting hardware drivers that vendors
removed from their builds are not applicable, but the method
does not cover this factor – leading to false-positive matches.
Qualitative evaluation of our approach showed high false-
positive rates as build customizations for a control group of
ﬁrmware samples were taken into account. While the large
result set might still be useful for analysts, more precise
methods are needed to reduce manual veriﬁcation efforts [10].
To improve the reliability of version-based Linux CVE
attribution in large-scale scenarios, we enrich the process with
kernel-speciﬁc data from automated static ﬁrmware analysis.
We extract kernel conﬁgurations from binary images and
reconstruct the kernel build process to identify included com-

1

 
 
 
 
 
 
ponents with ﬁle-level granularity. Hereby, we reduce the set
of false-positive matches requiring further manual veriﬁcation.

In the following, we provide:
1) A description of our methodology for Linux CVE attri-
bution, based solely on binary kernel representations.
2) An empirical evaluation in which we compare our ap-
proach against naive version-based CVE matching using
the 2020 Home Router Security Report [7] dataset.
Furthermore, we release the implementation of the methodol-
ogy used for the evaluation as open source to the public2.

II. BACKGROUND & RELATED WORK

There are various automation approaches to discover and
attribute security issues in binary targets. Aside of version-
based matching against vulnerability repositories, they include
code similarity and patch analysis [20], taint analysis and
symbolic execution [21], fuzzing [22], and various emulation-
based methods [9]. While we acknowledge substantial work
done in these directions, we point out their limited applica-
bility in large-scale analyses due to resource constraints, low
emulation success rates, and missing bootstrap data [9], [10].
Focusing on large-scale attribution of known security issues
in ﬁrmware, this section ﬁrst discusses the scientiﬁc need for
sound data as foundation of reliable bug attribution (II-A).
Then, we present attribution methods applied in large-scale
ﬁrmware security studies (II-B).

For comprehensive surveys on state of the art research
and the problem space, especially in terms of scalability and
automation, we refer to Qasem et al. [10] and Wright et al. [9].

A. Sound Data as Foundation for Reliable Bug Attribution

Detail information on known vulnerabilities lays the foun-
dation of reliable bug attribution [10]. The community-driven
CVE catalog [8] is the largest data source and de facto standard
for vulnerability identiﬁcation. For each tracked issue, there is
an identiﬁer, description, and follow-up references. The deriva-
tive National Vulnerability Database (NVD) [23] adds data
on severity and impact scores, but also lists affected products
and their versions. The latter is encoded in an identiﬁcation
scheme, the Common Platform Enumeration (CPE) [23].

Records vary in information quantity and quality. E.g.,
Sanguino and Uetz [15] show that errors in CPE assignments,
but also data inconsistencies harm soundness. Code similarity
approaches like [5] exemplify that pivotal data can be missing
or is hard to obtain, as they require access to ground truth
embedded in commits and patches. If CVEs affect closed
source projects, issuers will not share technical details on ﬁxes
in public. For open source, Tan et. al. [14] show that code
collection remains challenging: Patches are unreferenced, not
marked, incorrect, or hide in bug tracker discussions.

Thus, researchers construct small datasets of selected CVEs
that their proposed techniques can ingest for evaluation [10].
For each CVE, this implies in-depth investigation, additional
data aggregation, and technical bug knowledge, which limits

2https://github.com/fkie-cad/cve-attribution-s2, Accessed: 2022-09-05.

the applicability of previously mentioned approaches for large-
scale scenarios. The Vulncode-DB [24] addressed this issue
and annotated CVE data with ﬁne-granular technical informa-
tion, but got discontinued due to bootstrapping problems.

As for Linux kernel CVEs, issuers embed references to bug
locations in the project’s source tree in CVE short descrip-
tions [8]. In combination with extracted build conﬁgurations
from ﬁrmware, the proposal in this paper takes advantage of
this observation to identify components built into the kernel.
The linuxkernelcves.org [25] project argues that for Linux’s
there
many version streams, contributors, and distributors,
is inaccuracy in CPE data to precisely track affected ver-
sions. They crawl commit references in CVEs from Linux
distribution sources, e.g., Ubuntu and Android bug trackers,
to determine the ﬁrst patch appearance in mainline kernels.
However, while [25] host a regularly updated dataset, they
stress that their CVE post-processing code has neither been
evaluated nor published yet – result reliability is unknown.

B. Static Bug Attribution in Large-Scale Firmware Analyses

In 2014, Costin et al. [6] execute a quantitative study on em-
bedded device security by analyzing 32.000 ﬁrmware images.
They implement pattern-based heuristics and specify indicators
of security malpractice in ﬁrmware images, e.g., hardcoded
passwords, cryptographic material, but also included applica-
tion version numbers to attribute CVEs. Their static analysis
yields CVE matches in userspace applications of 693 ﬁrmware
images. While they inspect Linux kernel versions, these are
not included in the bug attribution. Furthermore, [6] report
unsolved challenges in result veriﬁcation, as not only CVE
data is incomplete, but also vendors may custom-patch affected
ﬁles. Since 2014, cross-architecture code similarity methods
have drastically improved and may be used as imprecise mea-
sure for veriﬁcation in this case, e.g., FirmUp [5]. However,
acquiring and processing patches for thousands of CVEs to
bootstrap code similarity methods deems infeasible based on
the imprecise CVE repositories.

Zhao et al. [19] develop FirmSec, a large-scale static analy-
sis pipeline to empirically study vulnerabilities introduced by
third-party components and software in IoT devices. Here, a
novelty is that they identify software versions by extracting
syntactical and control-ﬂow graph features. They construct
a repository of 1,191 third party components and then ex-
tract matching features out of the component’s corresponding
source ﬁles. However, they neither consider vendor-speciﬁc
build conﬁgurations nor the Linux kernel.

Similar to [6], we assess and compare the state of ﬁrmware
security of 127 home routers available on German markets
in our whitepaper Home Router Security Report 2020 [7].
Using the automated Firmware Analysis and Comparison
Tool (FACT) [16], we also implement pattern-based heuristics
to detect best practice violations in ﬁrmware images. E.g.,
we analyze the usage of exploit mitigation techniques like
stack canaries and non-executable bits in userspace binaries.
To quantize the differences between used Linux kernels in

2

Fig. 1: Our methodology describes a two-staged static analysis pipeline: First, we extract, analyze, and annotate ingress ﬁrmware images to detect kernel
version, conﬁguration, and architecture (left). Then, we fetch the corresponding mainline kernel sources, install the conﬁguration, and perform a dry build to
generate a ﬁle build log. In combination with a version-ﬁltered list of potentially attributable CVEs, we eliminate all records that do not include affected
component source ﬁles we witnessed during the dry build.

devices from a security perspective, we identiﬁed their ver-
sions, matched them against the NVD [23] using CPE, and
reported the results. Subsequent discussions and observations
proved that attributing CVEs only based on kernel version
leads to high false-positive rates and in effect to high manual
veriﬁcation effort and low chance of adoption. On top of
the issues of uncertainty in CVE databases and unidentiﬁable
vendor patches, we found that the kernel’s high modular-
ity, heterogeneity, and vendor-speciﬁc builds inﬂated false-
positives: Qualitative analysis showed that we matched for
CVEs in components that were veriﬁable absent in the build
– as for some ﬁrmware samples we were able to extract the
kernel conﬁguration to identify included components.

We could not ﬁnd any related work examining this Linux
kernel-speciﬁc problem in CVE attribution. Yet, we observed
consistent information in databases to identify components
and their ﬁles affected by a CVE. In combination with our
observation of extractable build conﬁgurations, we identify a
research gap and possibility to reliably reduce false-positive
matches in kernel CVE attribution.

III. METHODOLOGY

This section describes our proposed methodology to enrich
the version-based Linux kernel CVE attribution process with
build-speciﬁc annotations. We show an automated static anal-
ysis pipeline that ﬁnds and extracts kernel conﬁgurations, dry
builds the found kernel version, and ﬁlters CVEs based on
affected version and build log-included ﬁles.

Figure 1 provides an overview of our methodology. We
establish a two-stage process: In the ﬁrst and left-hand stage,
we unpack, analyze, and annotate each ﬁle of an ingress
ﬁrmware image. Gathered information includes Linux kernel
version, ISA, and kernel build conﬁguration. In the second
and right-hand stage, we leverage upon said data to perform
the actual CVE attribution and ﬁltering step. Yellow boxes in
Figure 1 mark components this paper contributes.

In the two following subsections III-A and III-B, we provide

1
2

3
4
5

strings:

$v = /Linux version \d\.\d{1,2}\.\d{1,3}(-[\d\w.-]+)?/

nocase ascii wide

condition:

$v and no_text_file

Listing 1: Software Components YARA rule to identify Linux kernel
versions. no text ﬁle is a custom rule excluding text/* MIME types.

detailed technical insights on each stage and step. We ﬁnish
with a short example of CVE-2017-17864 in III-C to demon-
strate the added attribution reliability our approach offers.

A. Gather Kernel Information via Static Firmware Analysis

For stage one, we apply and enhance the ﬁrmware analysis
tool FACT [16], which is maintained at our research group.
FACT is open source, comes with a variety of ﬁrmware
container unpackers, and provides a plugin system with ready-
to-use analysis modules we can leverage for kernel version
and ISA detection. In the following, we describe all pipeline
steps that are of importance for the proposed attribution
methodology.

Consider an arbitrary ﬁrmware image. After submitting it to
the pipeline, FACT’s Unpack Scheduler tries to identify con-
tainer formats. Choosing from a palette of custom extractors
and integrated tools like binwalk [26], it recursively extracts
or decompresses ﬁle contents until all options are exhausted.
The Analysis Scheduler receives each extracted ﬁle and
applies a preconﬁgured set of analysis plugins. These imple-
ment heuristics, e.g., based on YARA [27] rules, third party
tools like checksec [28], or custom analysis implementations.
Analyzed ﬁles and their annotated results are stored in the
Result Database & File Storage.

Our methodology depends on the results of three plugins:
Software Components, Kernel Conﬁguration, and Architecture
Detection. While FACT already provides the former and latter,
this paper contributes the new Kernel Conﬁguration plugin.

The Software Components plugin applies YARA rules to
detect various software components along with their version.

3

Unpack SchedulerAnalysis SchedulerPluginsArchitecture DetectionKernel Configuration...Result 1: {...}Result n: {...}...Result 1: {...}Result n: {...}...Result 1: {...}Result n: {...}...ResultDatabase & File StorageSoftware ComponentsFirmware imageRecursive unpackingExtracted files Files andplugin results Kernel VersionIII-A. Gather Kernel Information via Static Firmware AnalysisIII-B. Build Log-assisted CVE Attributionkernel.orgFile Filter REST API KernelDownloaderMainline kernel sources for version Dry BuildArchitectureKernel ConfigNVDSources File build log Reduced CVE setevidence of presenceexists in build log Analysis results CVE FetcherCVE set kernel version CPE match Unreduced CVE set Ver.Ver.ArchCfgB. Build Log-assisted CVE Attribution

The build log-assisted CVE attribution is the second stage
of our proposed analysis pipeline in Figure 1. Here, we ﬁrst
use FACT’s REST API to consolidate the kernel version,
kernel build conﬁguration, and detected target ISA.

Then,

the Kernel Downloader fetches version sources
from kernel.org. Note that the assumption of vendors using
a mainline, i.e., unmodiﬁed, kernel is false in general due to
custom patching and additional proprietary components. Large
parts of Linux are licensed under the GNU General Public
License (GPL) and, thus, modiﬁcations must be published5.
However, for two reasons we could not ﬁnd a scalable way to
obtain these versions: First, some vendors distribute packages
through individual processes on request6. Second, the required
build tools do not have to be part of the package – which is
pivotal to our pipeline.

Dry Build is the next step in Figure 1: We set the target
ISA and install the extracted kernel build conﬁguration in the
downloaded kernel source code. Then, we execute a dry run
via make, which does not compile the kernel but prints each
compilation recipe instead. This approach has the advantage
of low computational overhead and does not require cross-
compilation environment bootstrapping. The goal of this step
is to gather a list of source ﬁles from the build log, which our
pipeline witnessed to be included in the kernel.

The CVE Fetcher executes simultaneously. Here, we query
the NVD [23] dataset for all Linux kernel CVEs and ﬁlter out
all records that do not refer CPEs stating the extracted Linux
kernel version to be vulnerable. The resulting set of potentially
applicable CVEs is the output of version-based attribution.

Finally, the File Filter step combines the outputs of CVE
Fetcher and Dry Build: As we observed that Linux kernel
CVEs state the affected source ﬁles in their short description,
records from the version-based result set can be eliminated.
We reduce it by removing every CVE that does not reference
an affected ﬁle we witnessed in the build log.

C. Short Example - CVE-2017-17863

To demonstrate the practical addition of our Linux ker-
nel CVE attribution pipeline, we shortly present an exam-
ple using CVE-2017-178637. The record description states
an invalid memory access due to an integer overﬂow in
kernel/bpf/verifier.c. Also, CPE states that kernel
versions from 4.9 to 4.9.71 are vulnerable.

Consider a ﬁrmware sample with Linux kernel 4.9.60
and the underlying kernel conﬁguration. Naive version-based
matching reports a positive match as the version is within CPE
range. However, if the conﬁguration shows that BPF support
is disabled because CONFIG_BPF is set to N, all associated
source ﬁles, including kernel/bpf/verifier.c, are ex-
cluded from the compilation. Thus, our proposed attribution
method does not witness the affected ﬁle presence during the

Fig. 2: High-level control ﬂow of the Kernel Conﬁguration analysis plugin.
It detects conﬁgurations in plain text ﬁles, kernel images, and modules.
Decompression takes place if needed. Results are annotated to the ﬁle.

One of the rules, a part of which is shown in Listing 1, detects
the Linux kernel and its version.

We contribute the Kernel Conﬁguration plugin, which
detects and extracts Linux kernel build information. Stage two
of our pipeline requires this data to ﬁlter out CVEs applicable
to components excluded from the build.

Figure 2 illustrates the plugin’s high-level control ﬂow. Not
explicitly stated edge transitions lead to analysis termination.
Inside a ﬁrmware, kernel conﬁgurations may be present in
different plain text and binary formats. Thus, depending on
the ﬁle’s MIME type, the plugin differentiates between kernel-
related binary ﬁles and plain text. Here, plain text is the most
straightforward case, as the plugin then checks for headers
and build conﬁguration directives. An example plain text
conﬁguration can be found in the Debian Repositories3.

Another way to obtain the conﬁguration is by making use
of an enabled CONFIG_IKCONFIG4 directive: When it is set
to Y during the ﬁrmware kernel’s build time, a copy of the
conﬁguration is embedded into the binary image – either as
inline string or compressed container. If it is set to M, said
information is outsourced to a kernel module. Thus, if the ﬁle
is either a kernel image or module, our plugin searches for a
magic word that is prepended to the inline string. Once found,
we apply a set of signatures to detect the correct format of
the embedded conﬁguration and annotate our ﬁndings to the
analyzed ﬁle. Otherwise, we seek for embedded containers, try
to extract them, and again check for magic word and format.
The Architecture Detection plugin identiﬁes target ISAs
using four different measures. For executable ﬁles in ELF
it parses the e_machine and e_flags ﬁelds
format,
of the ﬁle header. For kernel conﬁgurations,
it searches
for the presence of architecture-speciﬁc feature directives.
E.g., ARM64_USE_LSE_ATOMICS is only supported by the
ARMv8 speciﬁcation. Furthermore,
the plugin detects and
parses CPU information from device trees [29] – a widely
spread bootloader-to-operating-system interface which also
lists system hardware and its properties. Finally, the plugin
does also rely on MIME databases to identify any ﬁle type
that might leak target platform information.

3https://salsa.debian.org/kernel-team/linux/-/blob/master/debian/conﬁg/arm64/conﬁg,

Accessed: 2022-09-05.

4https://github.com/torvalds/linux/blob/master/init/Kconﬁg, Accessed: 2022-09-05.

5https://www.gnu.org/licenses/gpl-faq.html, Accessed: 2022-09-05.
6https://www.zyxel.com/form/gpl oss software notice.shtml, Accessed: 2022-09-05.
7https://nvd.nist.gov/vuln/detail/CVE-2017-17863, Accessed: 2022-09-05.

4

AnalysisfilePlain textcheckIs plain text FormatcheckExtraction &annotationIs config Kernel image ormodule checkSeek magicwordNot plain text Found magicword Seek embeddedcompressioncontainerDecompressionFound imageor module  1101001 1010101 1110101 0101010 Container Not found  1101001 1010101 1110101 0101010 Decompressed Result 1: {...}Result n: {...}...Annotatedfile dry build step: CVE-2017-1786 is a false-positive match and
we can eliminate it from the result set.

IV. CASE STUDY

We perform a case study to evaluate the reliability of our
enriched version-based Linux kernel CVE attribution in large-
scale static analyses. For this purpose, we let our pipeline
analyze the Home Router Security Report 2020 [7] corpus,
which vendors reported to yield high false-positive rates using
version-based CVE matching. We raise two research questions:
R1 Our methodology requires access to speciﬁc information
in ﬁrmware samples and CVE repositories. How many
samples and CVEs fulﬁll these modalities? How applica-
ble is our approach in a real-world scenario?

R2 With version-based CVE matching as baseline, what

impact has the methodology on result reliability?

In the following subsections, we ﬁrst provide detailed in-
formation on our experiment and used dataset (IV-A). Then,
we present the results and analyze them within the context of
both stated research questions (IV-B and IV-C, respectively).

A. Experiment & Firmware Corpus

Experiment Execution. We deploy our static analysis CVE
attribution pipeline on a system with AMD Ryzen 7 2700x
processor and 32 GiB DDR4 RAM, running Ubuntu 20.04.4
LTS. FACT v4.0 (commit 38df4883) is used in the ﬁrst
pipeline stage. We extract each ﬁrmware of the Home Router
Security Report 2020 [7] corpus and apply the required
analysis plugins CPU Architecture, Software Components, and
Kernel Conﬁguration (cf., Section III-A). The second pipeline
stage executes on the same machine based on a snapshot
of the NVD [23] – taken on 2022-08-30. The snapshot has
records for 2,910 Linux kernel CVEs attributable through
CPE. For each component in our system, we collect details
on ingress and egress data, including plugin results, version-
based CVE matches, and ﬁltering decisions.

Firmware Corpus. The analyzed Home Router Security
Report [7] corpus is publicly documented8, reconstructable,
and consists of ﬁrmware from 127 home routers available in
the european market. Devices of seven well-known vendors are
included: ASUS, AVM, D-Link, Linksys, Netgear, TP-Link,
and Zyxel. Samples were scraped on 2020-03-27.

Figure 3 shows the distribution of Linux kernel versions
detected by FACT across all ﬁrmware images. Numbers in
parentheses and on top of bars count sample sizes per vendor
and kernel. Patch levels are clustered by major and minor
release for illustrative purposes, but are considered during
CVE attribution. Across all 127 samples, 121 binary dis-
tributed Linux kernels from v2.4.20 to v4.4.60 are included.
11 ﬁrmware images are not analyzable due to failed operating
system analysis or unpacking errors. Note that ﬁrmware can
contain multiple kernels, e.g., embedded devices may consist
of subcomponents running their own systems.

8https://github.com/fkie-cad/embedded-evaluation-corpus/blob/master/2020/

FKIE-HRS-2020.md, Accessed: 2022-09-05.

Fig. 3: Kernel version distribution of the subject Home Router Security
Report 2020 dataset [7], which was scraped on 2020-03-27. Patch versions
are clustered by major and minor release. Across 127 ﬁrmware images from
7 different vendors, FACT found 121 Linux kernels between v2.4.20 and
v4.4.60. For 11 samples, no operating system could be identiﬁed (unk.).

Fig. 4: Relative CPU Architecture Distribution across all ﬁrmware samples,
clustered by vendor. MIPS and ARM, both in big and little endianess are
prevalent in this ﬁrmware corpus of home router devices.

Figure 4 shows the relative architecture distribution across
all samples, clustered by vendor. MIPS and ARM ISAs
dominate the corpus, both in big and little endian. It includes
different word lengths, but the majority of devices uses 32-bit.
The ISA is unknown for 24 samples. More detailed insights
into the corpus can be found in [7].

We argue that the corpus is of sufﬁcient size to demonstrate
applicability, but also choose it to investigate the reliability of
matches we reported in [7] using naive version-based Linux
kernel CVE attribution. Furthermore, it offers heterogeneous
properties beneﬁcial for our case study, as it covers Linux
kernels from three major releases, widely-spread ISAs, and
devices of multiple vendors. A drawback is that other device
classes than home routers are missing.

B. R1 Analysis – Applicability in Real-World Scenarios

How applicable is our approach in real-world large-scale
automated ﬁrmware analyses? To ﬁnd answers to this question
we identify two methodological requirements that must be
fulﬁlled for each ﬁrmware and Linux CVE, respectively:
S1 FACT ﬁrmware extraction and plugin analyses of stage
one must succeed to consolidate the kernel version, ISA,
and kernel conﬁguration.

S2 CVE descriptions must reference affected ﬁles to ﬁlter
vulnerable components not included in the kernel build.
Using the ﬁrmware corpus, we evaluate egress and ingress
data of each step in the proposed pipeline with regards to
these requirements. Table I shows the results. Highlighted

5

01234567891011121314151617181920212223242526272829303132333435363738394041424344Occurences in Dataset [#]unk.2.42.63.33.43.103.143.184.14.4Kernel Version [x.y.*]84158616241151518152121122151114113422211ASUS (26)AVM (13)D-Link (14)Linksys (25)Netgear (22)TP-Link (20)Zyxel (12)05101520253035404550556065707580859095100Vendor-Relative Architecture Distribution [%]Zyxel (10)TP-Link (20)Netgear (22)Linksys (25)D-Link (14)AVM (10)ASUS (26)Vendor (Count)30.0%50.0%68.2%44.0%14.3%30.0%46.2%4.5%20.0%30.0%4.5%4.0%28.6%70.0%3.8%10.0%20.0%22.7%28.0%50.0%40.0%24.0%7.1%50.0%ARM, 32, LEARM, 64, LEMIPS, 32, BEMIPS, 32, LEunknownFig. 5: Filter verdict distribution of our pipeline relative to the baseline CVE attribution results for each of the 44 analyzed kernels. Each entry on the
horizontal axis represents a unique ﬁrmware. We classify them into four different verdict conﬁdence categories. With high conﬁdence, our methodology
marks around 68.37% of all baseline matches as false-positive and 12.04% as true-positive candidates (medians). Relative path matches with medium
conﬁdence match verdict are negligible with 0.19%.

S1 Requirement (FACT Analysis Success)

FW Matches [#]

Fulﬁlled

(cid:104) FW Matches
FWs Total

(cid:105)

Extraction
Kernel Version
Architecture Detection
Kernel Conﬁguration

116
116
103
44

0.9133
0.9133
0.8110
0.3464

Table I distributes all 2,910 Linux kernel CVEs across these
classes, showing that the proposed approach is applicable to
1, 872 (64.33%) kernel CVEs. For CVEs with no included
ﬁle reference, the approach falls back to version-based CVE
matching and, thus, can not add value to result reliability.

S2 Requirement (File Reference in Linux Kernel CVE)

C. R2 Analysis – Impact on CVE Attribution Result Reliability

CVE Matches

Fulﬁlled

(cid:104) CVE Matches
CVEs Total

(cid:105)

1743
Full Path Reference
129
File Only Reference
1038
No Reference
TABLE I: Method Applicability Analysis for the Firmware Corpus

0.5990
0.0443
0.3567

rows designate effective requirement fulﬁllment rates over all
analyzed ﬁrmware images and Linux kernel CVEs.

For requirement S1, FACT successfully extracts 116 out
of 127 ﬁrmware images. The analyses detect at least one
kernel version in all extractable ﬁrmware, and ISAs in 103
of them. However, our Kernel Conﬁguration plugin ﬁnds build
information in only 44. As this plugin depends on results from
previous analyses (cf., Section III), the matches are a subset of
the extracted ﬁrmware images with identiﬁed ISA and kernel
version. With missing Kernel Conﬁgurations as limiting factor
of method applicability, 34.64% of all analyzed ﬁrmware sam-
ples fulﬁll requirement S1. This rate is explainable considering
that a) IKCONFIG must be explicitly enabled to embed kernel
conﬁgurations into binary representations and b) it is common
practice for vendors to strip and obfuscate valuable analysis
information from ﬁrmware.

For requirement S2 (affected ﬁles must be referenced in
Linux kernel CVE descriptions), data analysis over all Linux
CVEs inside the NVD yields three different categories: Files
are either referenced as Full Path relative to the kernel’s
source tree, or the reference is File Only (location in the
source tree is unknown), or No Reference exists at all.

With version-based Linux kernel CVE matching as baseline,
what impact has our attribution pipeline on result reliability?
We approach research question R2 by analyzing the attribution
results of all 44 ﬁrmware images our methodology is appli-
cable to (cf., Section IV-B). Subject samples include kernels
ranging from v3.4.0 to v4.4.60. Out of these, only one still
receives mainline updates at the time of this evaluation (4.4.x).
The baseline method attributes a median of 1,196 CVEs per
ﬁrmware image, which is roughly 40% of all Linux kernel
CVEs present in the NVD. A possible explanation lies within
unsound and/or unmaintained CVE records in the NVD [14],
[15]. Yet, such imprecise data would also imply the existence
of false-negatives that should have been attributed to the kernel
version, but were not due to incorrect CPE. An argument in
favor of these results is the amount of end of life kernels
in the dataset. However, based on the results we present in
the following paragraphs, there is reason to assume that the
baseline yields exceedingly high false-positive rates.

Version-based CVE attribution is an intermediate result of
our methodology (cf., Section III). To estimate the impact our
pipeline has on result reliability, we consolidate all decisions
of the build-log assisted ﬁltering to classify them into four
categories of verdict conﬁdence:

• Applicable (High) – CVE references affected ﬁles and

full ﬁle path is witnessed in build log.

• Not Applicable (High) – CVE references affected ﬁles

but none of them is present in the build log.

6

3.43.43.4.1033.4.1033.4.1033.4.1033.4.1033.10.143.10.143.10.143.10.143.10.143.10.143.10.143.10.1043.10.1043.10.1043.10.1043.10.1043.10.1043.10.1083.14.433.14.773.14.773.14.774.1.274.1.274.1.274.1.274.1.274.1.274.1.384.1.454.1.514.1.514.1.514.1.514.1.514.1.514.1.524.1.524.1.524.1.524.4.60Kernel Binary extracted from Firmware Sample05101520253035404550556065707580859095100CVE Match Confidence [%, Relative]70.48%70.76%69.94%71.28%68.88%70.5%68.88%73.05%70.9%72.36%72.43%72.43%72.36%72.28%68.49%68.41%70.56%70.56%70.56%68.56%70.25%69.83%68.0%66.92%68.0%66.92%67.98%68.08%66.92%66.92%66.92%68.33%67.18%66.38%66.67%66.38%67.83%67.83%66.67%66.86%66.86%66.86%66.86%60.18%12.0%11.58%12.49%11.22%13.55%12.0%13.55%8.35%10.57%9.11%9.04%9.04%9.11%9.19%13.14%13.22%11.15%11.15%11.15%13.07%11.22%10.42%12.25%13.33%12.25%12.12%11.06%10.96%12.12%12.12%12.12%10.68%11.84%12.56%12.27%12.56%11.11%11.11%12.27%12.08%12.08%12.08%12.08%17.44%17.38%17.38%17.36%17.36%17.36%17.36%17.36%18.38%18.38%18.38%18.38%18.38%18.38%18.38%18.14%18.14%18.14%18.14%18.14%18.14%18.14%19.5%19.5%19.5%19.5%20.77%20.77%20.77%20.77%20.77%20.77%20.79%20.79%20.87%20.87%20.87%20.87%20.87%20.87%20.87%20.87%20.87%20.87%22.28%Not Applicable (High Confidence)Applicable (Full Path Match, High Confidence)Applicable (File Only Match, Medium Confidence)Applicable (No File Match, Low Confidence)• Applicable (Medium) – CVE references affected ﬁles,
but does not state full ﬁle paths. A ﬁle was matched and
seen in the build log, but ambiguity exists due to duplicate
names in the source tree.

Regarding functional limits, we stress the inherited limi-
tations of static analyses. They may use heuristics to ﬁnd
indicators of possible bug presence, but can hardly serve
deﬁnitive proof – which requires triggering the bug.

• Applicable (Low) – No ﬁle references, we can not decide
on applicability and fall back to version-based matching.
The idea is to map persuasiveness of additional evidence the
pipeline gathers within a trial: File matches are witnesses for
CVE applicability, but not every match is equally credible.

Figure 5 shows the ﬁlter verdict distribution of our pipeline
relative to the baseline CVE attribution results for each ana-
lyzed kernel. Versions are ordered from oldest (left) to newest
(right). Note that a single kernel was found in each one of the
44 analyzed samples. Thus, each entry on the horizontal axis
represents a unique ﬁrmware. In the following, all discussed
distribution values are medians across all samples.

The proposed Linux kernel CVE attribution methodology is
able to make a medium to high conﬁdence decision for 80.6%
of all version-based matches. The portion of high conﬁdence
applicable CVE matches is 12.04%. Relative path matches
yielding medium conﬁdence applicability are negligible with
0.19%. As indicated by the bottom bars belonging to the class
of Not Applicable (High), our pipeline attributes 68.37% of
all version-based CVE matches as false-positives and ﬁlters
them out of the result set. In numbers - out of the median
1,196 matches per ﬁrmware, we reduce the set of potentially
applicable CVEs to roughly 378. Thus, we signiﬁcantly reduce
the result set of potentially applicable CVEs requiring manual
veriﬁcation by analysts and vendors. The portion of low con-
ﬁdence applicability due to missing ﬁle references is 19.4%.
Unfortunately, for this class of matches our methodology does
not generate added value.

V. LIMITATIONS

We identify methodological shortcomings in three dimen-

sions: applicability, sound ground truth, and functionality.

In terms of applicability, our Linux kernel CVE attribution
pipeline is bound to FACT’s static analysis success. If the
kernel version, ISA, and build conﬁguration remain unknown,
our method can not identify possibly included components
for reliable CVE ﬁltering. Then, the pipeline becomes inap-
plicable. Yet, the case study in Section IV shows that (in the
used ﬁrmware corpus) there is still a considerable amount of
ﬁrmware fulﬁlling all requirements.

As for sound ground truth, reliable and true-positive CVE
attribution is limited by the quality of its underlying dataset.
If Linux kernel CVE records do not reference truly affected
versions and source ﬁles, the proposed mechanisms introduce
false-positive, but also false-negative matches. In this case,
result reliability is negatively affected. Our assumption of
vendors using mainline kernels is another limiting factor that
affects reliability, but a methodical necessity due to missing
insider information. Vendors may cherry-pick patches or intro-
duce custom ﬁxes, which are not detectable by our approach.
While some of the modiﬁcations might be obtainable through
GPL portals, we identify the issue of scalable accessibility.

Finally, the conducted case study is limited in its validity,

as the used corpus is missing in device class heterogeneity.

VI. CONCLUSION

In this paper, we focused on improving result reliability of
version-based CVE matching for the special case of binary
Linux kernels in large-scale static ﬁrmware analyses. Het-
erogeneous hardware properties, modularity, numerous devel-
opment streams, and vendor-speciﬁc builds cause high false-
positive rates. This is because the attribution method does not
check for vulnerable component presence in binary images
when ﬁltering CVEs.

Despite general automation issues due to unsound or in-
complete data in CVE repositories and common challenges in
binary ﬁrmware analyses, we found supplementary ﬁlter data
in Linux issue descriptions and ﬁrmware samples to reduce
the set of false-positive matches in scale. We enriched naive
version-based CVE matching with a static attribution pipeline
that detects kernel conﬁgurations and ISAs in ﬁrmware images
to reconstruct the kernel build process and guess included
source ﬁles. This data then serves as ﬁlter using kernel CVEs
where affected ﬁles are explicitly stated.

The case study shows that, with the limitations discussed in
Section V in mind, our approach is scalable and moderately
applicable: For 34.64% of all 127 considered home router
ﬁrmware images, the technical requirements are fulﬁlled and
about 65% of all Linux kernel CVEs reference affected ﬁles
in their description.

With naive version-based matching as baseline, the intro-
duced approach generates a high conﬁdence ﬁlter verdict for
80.6% of all considered CVEs and reduces the result set by
68.37%: CVEs affecting components probably absent from
binary kernel samples are successfully eliminated. While a
non-negligible amount of CVEs that our method can not reli-
ably ﬁlter remains, we conclude that the proposed attribution
pipeline is a promising step towards more reliable and scalable
static CVE attribution. Security analysts are still required to
verify the true applicability of a bug, but our systematic and
automated ﬁltering may at least reduce their manual efforts.

Stage one of our pipeline is part of the publicly avail-
able FACT [16], stage two is a set of standalone scripts
available at https://github.com/fkie-cad/cve-attribution-s2, and
our case study corpus is documented for reconstruction at
https://github.com/fkie-cad/embedded-evaluation-corpus.

VII. FUTURE WORK

In future work, we want to address missing device class
heterogeneity present in our case study corpus. Aside of iden-
tifying and implementing additional ﬁlter criteria, we evaluate
options to combine the proposed methodology with linuxk-
ernelcves.org [25]. The CPE data used for version matching
is of varying quality [15] and may introduce false-positives

7

[15] L. A. Benthin Sanguino and R. Uetz, “Software Vulnerability
[Online] https:

Analysis Using CPE and CVE,” ArXiv, 2017.
//doi.org/10.48550/arXiv.1705.05347

[16] Fraunhofer FKIE, “FACT - Firmware Analysis and Comparison Tool,”
Accessed: 2022-09-05. [Online] https://github.com/fkie-cad/FACT core
[17] ONEKEY GmbH, “ONEKEY Automated Firmware Analysis Platform,”

Accessed: 2022-09-05. [Online] https://onekey.com/

[18] NetRise Inc., “NetRise Platform - Next-Generation Firmware & IoT
Security Platform,” Accessed: 2022-09-05. [Online] https://www.netrise.
io/

[19] B. Zhao et al., “A Large-Scale Empirical Analysis of the Vulnerabilities
Introduced by Third-Party Components in IoT Firmware,” in Proc. of
the 31st ACM SIGSOFT International Symposium on Software Testing
and Analysis (ISSTA ’22). Virtual, South Korea: Association for
Computing Machinery, 2022, p. 442–454. [Online] https://doi.org/10.
1145/3533767.3534366

[20] I. U. Haq and J. Caballero, “A Survey of Binary Code Similarity,”
ACM Computing Surveys, vol. 54, no. 3, pp. 1–38, 2021. [Online]
https://doi.org/10.1145/3446371

[21] R. Baldoni et al., “A Survey of Symbolic Execution Techniques,”
ACM Computing Surveys, vol. 51, no. 3, pp. 1–39, 2018. [Online]
https://doi.org/10.1145/3182657

[22] V. J. Man`es et al., “The Art, Science, and Engineering of Fuzzing:
A Survey,” IEEE Transactions on Software Engineering, vol. 47,
no. 11, pp. 2312–2331, 2021. [Online] https://doi.org/10.1109/TSE.
2019.2946563

[23] National Institute of Standards and Technology, “National Vulnerability
Database,” Accessed: 2022-09-05. [Online] https://nvd.nist.gov/
[24] R. Habalov and T. Schmid, “Vulncode-DB,” Accessed: 2022-09-05.

[Online] https://www.vulncode-db.com/

[25] N. Luedtke and Contributors, “Linux Kernel CVEs,” Accessed:

2022-09-05. [Online] https://www.linuxkernelcves.com/

[26] Microsoft, “Binwalk,” Accessed: 2022-09-05. [Online] https://github.

com/ReFirmLabs/binwalk

[27] Google, “YARA - The pattern matching swiss knife for malware
[Online]

researchers (and everyone else),” Accessed: 2022-09-05.
https://virustotal.github.io/yara/

[28] T. Klein and B. Davis. checksec.sh.

[Online] https://github.com/

slimm609/checksec.sh

[29] Linaro Ltd., “DeviceTree Speciﬁcation,” Release v0.4-rc1, 2021. [On-
line] https://github.com/devicetree-org/devicetree-speciﬁcation/releases/
download/v0.4-rc1/devicetree-speciﬁcation-v0.4-rc1.pdf

and -negatives not caught by our methodology. A more ﬁne-
granular commit-based version tracking as offered by [25] is
a promising addition to our pipeline. However, a thorough
review of the automatically applied method is required before
adapting this data source over the de-facto standard of the
NVD.

REFERENCES

[1] A. Giri et al., “Internet of Things (IoT): A Survey on Architecture,
Enabling Technologies, Applications and Challenges,” in Proc. of
the 1st International Conference on Internet of Things and Machine
Learning (IML ’17). Liverpool, United Kingdom: Association for
Computing Machinery, 2017, pp. 1–12. [Online] https://doi.org/10.
1145/3109761.3109768

[2] H. Xu et al., “A Survey on Industrial Internet of Things: A Cyber-
Physical Systems Perspective,” IEEE Access, vol. 6, pp. 78 238–78 259,
2018. [Online] https://doi.org/10.1109/ACCESS.2018.2884906

[3] S. M. R. Islam et al., “The Internet of Things for Health Care: A
Comprehensive Survey,” IEEE Access, vol. 3, pp. 678–708, 2015.
[Online] https://doi.org/10.1109/ACCESS.2015.2437951

[4] N. Neshenko et al., “Demystifying IoT Security: An Exhaustive
Survey on IoT Vulnerabilities and a First Empirical Look on
Internet-Scale IoT Exploitations,” IEEE Communications Surveys
& Tutorials, vol. 21, no. 3, pp. 2702–2733, 2019.
[Online]
https://doi.org/10.1109/COMST.2019.2910750

[5] Y. David, N. Partush, and E. Yahav, “FirmUp: Precise Static Detection
of Common Vulnerabilities in Firmware,” in Proc. of
the 23rd
International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS ’18). Williamsburg, USA:
Association for Computing Machinery, 2018, p. 392–404. [Online]
https://doi.org/10.1145/3173162.3177157

[6] A. Costin et al., “A Large-Scale Analysis of the Security of Embedded
Firmwares,” in Proc. of
the 23rd USENIX Conference on Security
Symposium (SEC ’14). San Diego, USA: USENIX Association, 2014,
p. 95–110. [Online] https://dl.acm.org/doi/10.5555/2671225.2671232
[7] P. Weidenbach and J. vom Dorp, Home Router Security Report
2020. Fraunhofer Institute for Communication, Information Processing
and Ergonomics (FKIE), Technical Report, 2020.
[Online] https:
//www.fkie.fraunhofer.de/en/press-releases/Home-Router.html

[8] The MITRE Corporation, “Ofﬁcial CVE Program Database,” Accessed:

2022-09-05. [Online] https://www.cve.org/

[9] C. Wright et al., “Challenges in Firmware Re-Hosting, Emulation, and
Analysis,” ACM Computing Surveys, vol. 54, no. 1, pp. 1–36, 2021.
[Online] https://doi.org/10.1145/3423167

[10] A. Qasem et al., “Automatic Vulnerability Detection in Embedded
Devices and Firmware: Survey and Layered Taxonomies,” ACM
Computing Surveys, vol. 54, no. 2, pp. 1–42, 2021.
[Online]
https://doi.org/10.1145/3432893

[11] D. Chen et al., “Towards Automated Dynamic Analysis for Linux-
the 2016 Symposium
San
[Online] https:

based Embedded Firmware,” in Proc. of
on Network and Distributed System Security (NDSS ’16).
Diego, USA:
//doi.org/10.14722/ndss.2016.23415

Internet Society, 2016, pp. 1–16.

[12] M. Kim et al., “FirmAE: Towards Large-Scale Emulation of IoT
Firmware for Dynamic Analysis,” in Proc. of
the 2020 Annual
Computer Security Applications Conference (ACSAC ’20). Austin,
USA: Association for Computing Machinery, 2020, p. 733–745.
[Online] https://doi.org/10.1145/3427228.3427294

[13] S. Lipp, S. Banescu, and A. Pretschner, “An Empirical Study on the
Effectiveness of Static C Code Analyzers for Vulnerability Detection,”
the 31st ACM SIGSOFT International Symposium on
in Proc. of
Software Testing and Analysis (ISSTA ’22). Virtual, Republic of
Korea: Association for Computing Machinery, 2022, p. 1–12. [Online]
https://doi.org/10.1145/3533767.353438

[14] X. Tan et al., “Locating the Security Patches for Disclosed OSS
Vulnerabilities with Vulnerability-Commit Correlation Ranking,” in
the 2021 ACM SIGSAC Conference on Computer and
Proc. of
Communications Security (CCS ’21). Virtual, Republic of Korea:
Association for Computing Machinery, 2021, p. 3282–3299. [Online]
https://doi.org/10.1145/3460120.3484593

8

