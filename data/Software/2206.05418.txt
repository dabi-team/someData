SAIBench: Benchmarking AI for Science

Yatao Lia,b,c, Jianfeng Zhana,b

aInstitute of Computing Technology Chinese Academy of Science, No.6 Kexueyuan South Road,Haidian District, 100190, Beijing, China
bUniversity of Chinese Academy of Sciences, No.19(A) Yuquan Road, Shijingshan District, 100049, Beijing, China
cMicrosoft Research Asia, Building 2, No. 5 Dan Ling Street, Haidian District, 100080, Beijing, China

2
2
0
2

n
u
J

1
1

]
I

A
.
s
c
[

1
v
8
1
4
5
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Scientiﬁc research communities are embracing AI-based solutions to target tractable scientiﬁc tasks and improve research work-
ﬂows. However, the development and evaluation of such solutions are scattered across multiple disciplines. We formalize the
problem of scientiﬁc AI benchmarking, and propose a system called SAIBench in the hope of unifying the eﬀorts and enabling
low-friction on-boarding of new disciplines. The system approaches this goal with SAIL, a domain-speciﬁc language to decouple
research problems, AI models, ranking criteria, and software/hardware conﬁguration into reusable modules. We show that this
approach is ﬂexible and can adapt to problems, AI models, and evaluation methods deﬁned in diﬀerent perspectives. The project
homepage is https://www.computercouncil.org/SAIBench.

1. Introduction

Artiﬁcial Intelligence has seen continuous and signiﬁcant ad-
vancements over the past years, with Deep Learning methods
being arguably the most representative and focused on. Blessed
by the ever-increasing computation power in AI accelerators
and general-purpose architectures alike, new AI paradigms and
models are proposed which greatly improve the scalability, ﬂex-
ibility, and applicability of this data-driven approach. As a re-
sult, the IT industry is welcoming AI-powered solutions, inte-
grating them into existing data processing pipelines that will
otherwise require human intervention or prohibitive computa-
tion cost. This trend is also propagating into scientiﬁc research
communities, as researchers are gaining interest in leveraging
state-of-the-art AI solutions to tackle equally if not more diﬃ-
cult tasks, hence AI for Science.

From a bird’s eye view, a scientiﬁc research activity can be
mechanical or creative. A mechanical research activity can
be algorithmically speciﬁed, with quantized or computation-
ally veriﬁable input/output. On the other hand, a creative re-
search activity breaks out of a mechanical system, for example,
by deﬁning a new problem or introducing ideas hard to quan-
tify. In this work, we call a computationally veriﬁable research
task a “tractable scientiﬁc task”. That said, an AI for Science
solution is introduced to bring improvements into the scientiﬁc
research workﬂow and is usually targeting towards a tractable
scientiﬁc task, such as:

• Mathematical Problem Solving — to solve mathematically

well-deﬁned problems.

• Pattern Matching — to classify, identify patterns, and de-
tect region-of-interest in high volume scientiﬁc data.

Email addresses: yatli@microsoft.com (Yatao Li),

jianfengzhan@ict.ac.cn (Jianfeng Zhan)

Preprint submitted to TBench

• Prediction — to compute future world states, given an ini-

tial snapshot of the world state and evolving rules.

• Artifact enhancement — to improve the quality of data ac-
quired from imperfect observations, e.g. incomplete, frag-
mented, noisy sensor data.

• Control — to use actuators to drive sensor readings into

desired states, despite the imperfection of both.

• Hypothesis and Conﬁrmation — to propose a theory (e.g.

equations) that conforms with the observations.

Examples of these tasks are shown in table 1. The term
“AI for Science” is also conventionally deemed as an ensem-
ble of vertical ﬁelds and tasks [4]. However, we argue that to
fully realize the potential of AI for Science, it is not enough to
cherry-pick an AI method, match it against a speciﬁc task, and
heuristically compare it with existing methods. One strength
of AI methods is that they abstract away the problem details
and mathematical procedures, into generic functions that trans-
form inputs into outputs — that is, every AI model possesses
the potential to adapt to other tasks, some (for example, neural
networks) even being universal approximators. Science is vast,
and AI methods are many. A single eﬀort to evaluate a task-
method pair would leave other research communities unaware,
of both the potential tasks that a model is capable of process-
ing and potential models that can be applied to a task. This
problem is exaggerated by the fact that the AI research is mov-
ing forward fast, that by the time a speciﬁc method is picked
up by a scientiﬁc computing task, or a task is adapted to an AI
method, it may be already bested by then state-of-the-art.

To help the scientiﬁc research communities as a whole sys-
tematically absorb and integrate the advancements of AI re-
search, and to avoid repeated eﬀorts in development and eval-
uation, we propose SAIBench, a system that bridges scientiﬁc

June 14, 2022

 
 
 
 
 
 
Mathematical
Problem Solving

Pattern
Matching

Prediction

Artifact
Enhancement

Control

Hypothesis and
Conﬁrmation

Partial derivative equations
General matrix multiplication
Matrix decomposition
Integration
Monte Carlo methods
Species Classiﬁcation
Event Identiﬁcation [1]
Climate Analysis [2]
Anomaly Detection
High-Energy Particle Simulation
Molecular Dynamics
Fluid Dynamics
Protein Folding
Genome Sequence Alignment
Astronomy Image Enhancement
Medical Image Enhancement
MRI Reconstruction
Tokamak Plasma Control [3]
Sensor Triggering
Automatic Physics Laws Discovery
Symbolic Regression

Table 1: Examples of Tractable Scientiﬁc Tasks.

computing tasks and AI methods, and automatically bench-
marks every sensible combination, collects performance met-
rics, and projects it back into rankings proper to each research
community. Research groups of diﬀerent backgrounds can fo-
cus on their needs while taking advantage of other benchmark-
ing building blocks, without having to re-invent end-to-end
evaluation processes.

The rest of this article is organized as follows. We ﬁrst de-
ﬁne the problem of scientiﬁc AI benchmarking in Section 2. In
Section 3 we discuss the methodology, goal, and challenges.
Section 4 elaborates our system design, including the details of
each component. We showcase end-to-end scenarios involving
multiple modules in Section 5.

2. Problem Deﬁnition

Here we deﬁne the problem of scientiﬁc AI benchmarking.
To begin with, we have a set of tractable scientiﬁc tasks as de-
ﬁned in the previous section, and an array of AI methods, each
needs to be trained to solve a speciﬁc problem. To evaluate
a method for such tasks, diﬀerent scientiﬁc communities have
diﬀerent criteria. For example, instruments in High Energy
Physics generate zettabytes of data, and the training data for AI
models is virtually unlimited. An AI method could thus focus
on throughput, time-to-solution, sample selection, etc. Mean-
while, for Biology and Life Sciences, sometimes there are just a
few hundred data points, requiring high sample eﬃciency, and
a strong ability to generalize and extrapolate onto unseen prob-
lem conﬁgurations.

Nonetheless, the qualiﬁcation of a method can be categorized

as follows:

2

Problem Class

Boundary Value Problem
Stochastic Diﬀerential Equations
Many-body Interactions
Positive Deﬁnite Matrix Decomposition

Problem Setting Temperature and pressure dependence
of alanine dipeptide
Straight wire Magnetostatics
Community Atmosphere Model
(CAM5) [5] Simulation
ANI-1x [6], GDB-17 [7]
OASIS [8]

Problem Case

Table 2: Examples of Qualiﬁcation Criterion.

• Deﬁned by Problem Class. For purely computational
tasks such as mathematical problem solving, it is prefer-
able to target against classes of problems to see how the
method performs under each set of mathematical con-
straints. For example in equation solving, it is desired to
study how a method behaves for both stiﬀ and non-stiﬀ
systems, where both types contain their problem class def-
initions.

• Deﬁned by Problem Setting. Compared to purely math-
ematical problem classes, this type of problem deﬁni-
tion usually embodies speciﬁc constraints under a class to
match a physical setting. Scientiﬁc research communities
have established well-respected problem settings that have
been practiced and conﬁrmed. This allows computational
methods to interoperate with real-world experiments, as
speciﬁc experimental settings can be virtually replicated.

• Deﬁned by Problem Cases. For some tasks we are only
interested in a narrow range within the whole problem
space. Most data-driven tasks fall into this category, where
the typical workloads of a task are deﬁned by collected
and/or labeled data. There are also “golden standards” de-
ﬁned in research ﬁelds, which are computational methods
with superior accuracy and other desirable properties, at
expensive computational costs. These methods are then
used to collect data for very speciﬁc problem cases so that
other faster but less accurate methods can be developed
and evaluated.

This categorization is not mutually exclusive though, as some
tasks require more than one qualiﬁcation criteria to properly de-
ﬁne the problem. For example, a robotic control algorithm can
be tested both in a simulated setting and on data points col-
lected from real-world sensors. Nevertheless, the principle is
that this categorization describes the hierarchy of problem deﬁ-
nition – the more the deﬁnition leans towards the former (prob-
lem classes), the more computation is required; On the other
hand, the more towards the latter (problem cases), the more
data. Furthermore, the problem deﬁnition serves as a speciﬁ-
cation for the AI model behavior, for both training and testing.
Examples of these qualiﬁcations are shown in table 2. How-
ever, AI-based methods likely require training, so the problem

deﬁnition of all three types must be reduced to case-by-case
training data points — for a problem class, the problem deﬁ-
nition should generate independent problem instances that suf-
ﬁciently cover the problem space. For a problem setting, the
problem deﬁnition should generate state snapshots that conform
to the constraints. For data-driven problem cases, the problem
deﬁnition should simply enumerate from the dataset.

Furthermore, the evaluation of a method depends on the
problem deﬁnition generating tests. For each test case, the
performance is represented with a cost function. For a math-
ematical problem instance, the cost function can be the error
against ground truth solution, or error against equality con-
straints [9] [10]. For simulation settings, the cost function
can be obtained by comparing the performance metrics derived
from such experiments, as shown by previous works on spe-
ciﬁc tasks [11] [12] [13]. Lastly, for data-driven problem cases,
the dataset can be split into training and test sets, and the cost
function is the loss function applied to the test set.

Finally, it is crucial to realize that diﬀerent benchmarking
communities use the word “performance” to refer to diﬀerent
concepts. Scientiﬁc AI benchmarking concerns not only the
accuracy of AI models but also the computation cost. The com-
putation cost can be further broken down into two phases: 1)
the cost for a model to reach certain accuracy, and 2) once
the model is properly trained, the cost of using the model for
inference tasks. For the ﬁrst phase, the standard practice is
to measure training time (wall clock or total CPU/GPU time)
against the best/mean/worst accuracies, and for the second,
the throughput/latency etc. for completing the inference tasks.
Moreover, for both phases, we can investigate the system per-
formance with standard parallel computing benchmarking tech-
niques [14] to expose diﬀerent performance characteristics of a
solution, for example, time-to-solution or cost-eﬃciency.

3. Methodology

The main goal of SAIBench is to build an inclusive and in-
terconnecting environment for all the relevant research eﬀorts,
including problem deﬁnition, AI method, training algorithm,
software and hardware environment, metric deﬁnition and rank-
ing deﬁnition, and deliver benchmarking result eﬃciently with
given computation resources. The desiderata brought by this
goal is multifold.

We need to design the system with a modular paradigm and
provide friendly programming interfaces for diﬀerent modules.
It should handle the impedance mismatches between diﬀerent
programming languages and environments while maintaining
consistent standards. This is traditionally implemented with
language bindings (for example, the computational chemistry
package NWChem [15] can either execute its own scripting
language, or be controlled by a Python language binding) or
ﬁle-based inter-process communication, which is suboptimal
because diﬀerent programming environments may have incom-
patible constructs that cannot be bound into a single process,
and distributed computing modules cannot be modeled easily.

A module should be self-descriptive so that the system can
automatically discover benchmarking tasks it can participate

in, so in addition to modular interfaces targeting benchmark-
ing tasks, there should be a protocol for modules to exchange
metadata and relate to each other. It is challenging to design
such a protocol because it has to be generic, extensible, and yet
carrying concrete meanings. For example, if we model the in-
put/output of an AI model as tensors of required dimensions, it
places strict constraints on what the AI model can solve, and
the system will not be able to associate this AI model with even
slightly incompatible tensors, not to mention non-tensor data
that has to be converted to adapt. On the other hand, if we
simply attach a textual description to each module, it would
be too hard for machine-understanding, and require human in-
tervention to develop the connections. To this end, machine-
understandable ﬂexibility and extensibility are needed, to en-
able modules to cooperate less rigidly. The previous example
shows how a module for an AI model should describe itself.
Similarly, for a problem deﬁnition, it should programmatically
set up the training and test ﬁxtures, and conduct the experi-
ments. This way all the three types of problem deﬁnitions pre-
viously can be normalized and become accessible to AI models.
In addition, it should expose metadata that allows the system to
inspect the execution workﬂow, and identify tasks that can be
completed by other modules. This type of meta-programming
is practiced in programming language research and recently
machine learning frameworks, implemented in declarative lan-
guages and domain-speciﬁc languages (DSLs) [16], yet largely
unexplored in scientiﬁc computing, where most execution en-
gines take a parse-interpret-execute approach [17][15].

As we discussed above, the system is not a single bench-
mark, but a collection of such, to be projected back to each
research ﬁeld and aggregated by a ranking criterion. Conﬂict
of interests naturally arises, for example, to favor speed vs. to
favor accuracy, ﬁrst principle metrics vs. a particular set of de-
rived properties. The system should be able to allow diﬀerent
perspectives of the same metrics and provide an interface for
ranking modules to declare their preferences.

The performance of an end-to-end AI solution to a tractable
scientiﬁc task depends on multiple aspects, including the AI
model, the training algorithm, the computing software stack,
the empowering hardware, and so on. These factors do not con-
tribute to the ﬁnal performance linearly, for example, a particu-
lar AI model may have the best work-precision properties under
one hardware conﬁguration but not the others. It is thus desir-
able to consider all these factors as benchmarking hyperparam-
eters. There are several implications brought by this require-
ment. The AI module implementation should be declarative in-
stead of being bound to a speciﬁc software/hardware stack; The
software stack module should declare the capabilities (e.g. ma-
trix multiplication and backward propagation) so that the sys-
tem ﬁnds compatible model-software pairs; Also, the software
stack module should describe the hardware compatibility and
accept a standardized hardware conﬁguration descriptor, so that
the system can automatically schedule scalability tests.

With all the components modularized and parameterized, the
whole benchmarking workﬂow can be formulated as follows.
Each type of module introduces some dimensions to the bench-
marking task, and the goal is to enumerate and test against the

3

Figure 1: System Architecture.

Cartesian product of all such dimensions, where each point in
the problem space represent the combination of a speciﬁc task,
solver, metrics, software and hardware conﬁguration. This al-
lows the modules to advertise themselves, discover the others,
and therefore reuse data and interact with each other, without
knowing them beforehand. This paradigm aligns well with the
FAIR guiding principles for scientiﬁc data management [18],
which suggests that scientiﬁc data should have ﬁndability, ac-
cessibility, interoperability, and reusability. This is the key dif-
ference between the methodology of this work and previous
AI benchmarking and scientiﬁc benchmarking systems, where
the benchmarked scenarios are pre-determined workload and
model combinations, and the addition of a new AI model or
dataset would not be automatically discovered and reused by
existing modules in the system and has to be scripted by a pro-
grammer.

Last but not least, because the system automatically discov-
ers potential benchmarking tasks, it is desired that the system
can concurrently schedule computation resources to them. As
diﬀerent benchmarking tasks may require diﬀerent computing
environments, it is crucial that the system can elastically pro-
vision the environment for each task in a standardized manner,
including the operating system, runtime libraries, setup scripts,
and test ﬁxture data. The challenge lies in how to design the
system to eﬃciently support such needs and minimize the de-
ployment overhead.

4. System Design

In this section, we illustrate the overall design of the system
and tap into each system component, and discuss how to ad-
dress the aforementioned challenges. The architecture of the
system is depicted in ﬁgure 1. The workﬂow is straightforward.
The planner pulls all modules from the module repository and
joins them into feasible tuples according to the metadata de-
scriptors. The execution plan is then dispatched to the elastic

computing platform which provides storage, processors, and
accelerators, where each benchmarking task tuple is executed
in a “benchmarking pod”. The purpose of the BenchPod is to
provide task-level isolation to computation resources, a com-
munication endpoint to interact with the planner, and experi-
ment orchestration. A problem deﬁnition module either gener-
ates data on-the-ﬂy or retrieves a well-known dataset into the
BenchPod instance. The hardware deﬁnition module acquires
hardware resources. The software deﬁnition module constructs
a containerized environment, based on a standardized software
package requirement descriptor. The entry point of the con-
tainer is a shim program provided by the BenchPod instance
that orchestrates the actual execution of the solver, metric col-
lection, and aggregation.

4.1. SAIL: Scientiﬁc AI domain-speciﬁc Language

Previous AI benchmarking systems either implicitly deﬁne
a series of built-in modules [19] [20], or expose a markup lan-
guage schema to deﬁne modules [21]. For better programmabil-
ity, discoverability, and user ergonomics, we propose to deﬁne
modules with an embedded domain-speciﬁc language (eDSL)
called SAIL. The eDSL is implemented as a Python package
so that a module implementer can take advantage of modern
IDE features such as auto-completion and type checking while
writing the module deﬁnition. To design an eDSL means that
the desired features must be retroﬁtted into the target language.
To achieve this, we take advantage of various Python language
constructs that best ﬁt the required features. Some features
can be implemented with static analysis, for example, we use
Python decorators to identify module entry points. This way
we can easily scan for modules with reﬂection, and build our
module repository. We use Python classes to represent type
descriptors for our type system, which is a dual-role construct
that both encodes the type information for static analysis, and
dispatches code during benchmarking runtime. Benchmarking
concepts are modeled as well-known global objects, and the

4

ElasticComputingPlatformModule RepositorySoftwareRankingMetricsProblemsSolversHardwareLog StoreComputationEnvironmentsRanking StoreFixture StorePlannerPSMSWHWRResearch FieldRanking MetricsInput/OutputComputationOperatorsHardware SuportScaling FactorsPSMSWHWRResearch FieldRanking MetricsInput/OutputComputationOperatorsHardware SuportScaling FactorsPSMSWHWRRanking MetricsInput/OutputComputationOperatorsHardware SuportScaling FactorsResearch FieldSchedulingPSRMSWHWPSRMSWHWBenchPodPSRMSWHWGenerateDeclare / BuildAcquirePublishComputeMeasureAggregateFeature

Construct

Instances

Module
Entry Points

Type
Descriptors

Decorators

Classes

Concepts
and Primitives

Well-known
Global Objects

AI Models

Declarative
Methods

@ProblemDeﬁnition
@MetricDeﬁniton . . .

class Tensor
class Scalar . . .

Train.Classify
Model.Predict
Test.Compare . . .

Pipeline
Linear
Relu
Softmax . . .

Table 3: Examples of SAIL Language Constructs.

methods attached to them represent benchmarking primitives.
This gives a hint to the user that these concepts are stateful,
and the primitives can function as both computation routine and
data storage. Finally, we use declarative methods to construct
the computation graph for AI models. Table 3 shows some lan-
guage construct examples.

The module script, rather than directly executed in a Python
interpreter, is ﬁrst sent to the SAIL parser. The SAIL parser
substitutes the actual execution logic with computation nodes
and connects the nodes with computational dependencies to
construct the computation graph, similar to the tape-recording
technique in automatic diﬀerentiation frameworks [22]. The
parser then analyzes the computation graph and synthesizes ac-
tual benchmarking code. The eDSL provides its own type sys-
tem with both tensors and symbolic equations as ﬁrst-class cit-
izens, and helper functions to help connect diﬀerent modules.
In fact, with proper type inference, there is even no need to ex-
plicitly declare the input/output types of a module.

The ﬂexibility of a scripting language also simpliﬁes mod-
ule deﬁnitions, for example, ﬁgure 2 illustrates a “hello world”
problem deﬁnition module — the MNIST [23] image classiﬁ-
cation problem. This is a typical “deﬁned by cases” problem as
we illustrated in Section 2. The deﬁnition of this problem reads
from four input ﬁles, joins them into pairs, and declares the
data points and associated classiﬁcation tasks into Train and
Test collections. Note that the existence of both train and test
collections is not necessary for some kinds of problems — for
example, a PDE “problem class” deﬁnition may deﬁne a few
equations in the test collection and expect a solver to accom-
plish the task without training or hints.

Note that the problem deﬁnition of MNIST resembles a ma-
chine learning training loop — but not entirely. The key point
is that it only deﬁnes the problem, and does not try to solve
or evaluate the results. This allows us to plug diﬀerent eval-
uation metrics into the workﬂow. For example, the Machine
Learning community traditionally focuses on the average per-
formance over the whole dataset, while in a production critical
environment, one may prefer to evaluate 99% percentile preci-

Figure 2: MNIST Problem Deﬁnition.

sion, or a hard fail condition, as shown in ﬁgure 3. Also shown
in the code is a simple timer metric, and a task can be evalu-
ated with multiple metrics. For example, the two in the code
will combine into a work–critical loss 2D graph. For iterative
tasks, a metric will also be evaluated iteratively, and a module
can choose to keep states across multiple iterations, memorize
the data points or obtain the average, etc.

Figure 3: Custom Evaluation Metric Deﬁnition.

Even for the same task, diﬀerent research communities have
diﬀerent interests in performance evaluation. For example, sci-
entiﬁc research groups focus on the quality of the end result,
while computer system researchers focus on system perfor-
mance metrics, such as throughput and latency [24]. This is
why we further split the ranking module from problems and
metrics. A ranking module can reference multiple metrics and
aggregate them to obtain a total order, or implement a compar-
ison between two instances to obtain partial order.

Figure 4: TensorFlow Software Conﬁguration.

Another advantage of this approach is that the module def-
inition can take input parameters and programmatically gener-
ate conﬁgurations. For example, in ﬁgure 4 we deﬁne how to
pick the correct docker image tag for TensorFlow based on the
hardware conﬁguration, which is hard to model with a markup
language. This also allows us to deﬁne generic AI modules

5

that adapt to diﬀerent input sizes and types and suggest hyper-
parameter values. Figure 5 shows the deﬁnition of a simple
neural network, which not only deﬁnes the computation graph,
but also the intended tasks, input/output type conversion, and
layer width suggestions, so that the planner can grid search this
hyperparameter. Also shown in the code snippet are two type
converters, when combined, can automatically convert the input
of an atom sequence into a single concatenated tensor.

search to also allow type converter composition so that multi-
ple converters can work together to relax type constraints and
improve module compatibility. This process is akin to the inner-
join operation in relational databases, and the system builds
complete tuples of the modules as test scenarios. Apart from
automatic discovery, a module can also explicitly declare rela-
tionships with other modules so as to narrow down the search
space. The logic is presented in algorithm 1.

Algorithm 1: Benchmarking Task Discovery.
Input: eDSL source ﬁles [src]
Output: Test scenarios [test]

1 repo ← φ;

// 1. Module discovery

2 foreach s : src do
3

ast ← parse(s);
foreach m : methods(ast) do

4

5

6

7

8

if decorated(m, ProblemDeﬁnition) then

repo[0].append(m);

if decorated(m, MetricDeﬁnition) then

repo[1].append(m);

// Scan for other modules...

Figure 5: AI Model Deﬁnition.

4.2. Automatic Benchmarking Task Discovery

As discussed before, the module deﬁnitions are not used for
the actual execution of the benchmarking tasks. Rather, they
are metaprogramming constructs that can be seen as a “dry-
run” for the actual benchmarking. The system scans all python
ﬁles and uses reﬂection to identify module entry points, and cre-
ate records for them in the module repository. The system then
enumerates all the modules from the repository and constructs
candidate test ﬁxtures, which are tuples of diﬀerent kinds of
modules. For each candidate tuple, the system executes the
modules in it, providing input parameters, and extracting in-
formation such as the problems a model can solve, the research
ﬁeld of a problem, the suggested hyperparameters, and compat-
ible metrics for a kind of task and so on. The execution order
is determined by the type of modules and the implied depen-
dencies – problem deﬁnitions execute ﬁrst because they gener-
ally do not depend on other modules, and populate the meta-
data required to associate metrics and ranking. During execu-
tion, the system maintains the context for the current test ﬁxture
and accumulates the metadata from already executed modules
in the candidate tuple, and later modules can either be ﬁltered
by metadata matching (for example, by matching data types)
or actively reject the context. This is demonstrated in earlier
examples, where a module can use the DSL primitive Fail to
indicate that it does not know how to solve the problem, or the
hardware does not support the current software conﬁguration.
Additionally, the system builds a graph where the nodes are
data types and edges are converters, and employs breadth-ﬁrst

9 ctx ← φ; test ← φ; iters ← iterators(repo);
10 i ← 0;

// 2. Task discovery

11 while i ≥ 0 do
12

if i = N then

13

14

15

16

17

18

19

20

21

22

23

24

25

test.append(ctx);
ctx.pop();
i ← i − 1;

else if next(iters[i]) then
m ← get(iters[i]);
metadata ← execute(m, ctx);
if not Failed(metadata) then
ctx.push(metadata);
i ← i + 1;

else

ctx.pop();
rewind(iters[i]);
i ← i − 1;

26 return test

4.3. Experiment Orchestration

When the planner is done generating benchmarking conﬁg-
uration tuples, it is necessary to prune unnecessary entries and
make a schedule for the rest. There are multiple invariances in
the benchmarking tasks to help pruning. For example, given the
same AI model, software/hardware conﬁguration, and similar
problem size (of diﬀerent problems), the throughput (in terms
of FLOPS) can be comparable. Likewise, the precision eval-
uation should not be heavily impacted for the same model and

6

problem on diﬀerent software/hardware conﬁgurations. The ex-
ecutor should only pick signiﬁcant tuples to maximize the di-
versity in measurements for all the metric dimensions, includ-
ing model performance, scalability, generalization, and so on.
Once the pruning is done, the scheduling problem concerns how
to estimate the costs of each tuple, and eﬃciently pack them
onto the hardware-task timeline.

5. Case Study

Now we discuss the details of a particular use case, Molecu-
lar Dynamics (MD). Given the initial states of the atoms (po-
sition and velocity vectors), the problem asks for a predic-
tion of the movement of the atoms. In practice, the problem
is decomposed into the problem of force prediction (Molec-
ular Mechanics), and the integrated force over time to com-
pute new states. In particular, force prediction is achieved in
multiple ways developed by the Molecular Dynamics research
community. “Classical MD” employs empirical models to com-
pute pairwise forces between atoms, and ﬁrst-principle methods
(AIMD) employ quantum mechanic methods as DFT [25] and
CCSD(t) [26] to ﬁrst predict the potential energy of the system,
and then obtain the forces by computing the partial derivatives
of the energy over atom positions.

Figure 6: Molecular Dynamics Problem Deﬁnition

The problem deﬁnition module is shown in ﬁgure 6. It con-
sists of two phases. First, an AI model is trained to predict
the potential energy of a system, guided by a Molecular Dy-
namics software package, such as ORCA or Gaussian. Then,
the performance of the model is evaluated on a diﬀerent set

7

of atom conﬁgurations. Unlike traditional AI benchmarks that
merely evaluate the output of a model, here we provide multi-
ple ﬁxtures, including both the energy prediction, and the po-
sition and velocity updates computed from the prediction. It is
the ﬂexibility of problem scripting that gives us the ability to
model additional ﬁxtures other than the energy, which can be
extended to benchmark ﬁelds other than Molecular Dynamics,
for example, Raman Spectroscopy. This is a typical “deﬁned
by setting” problem as we illustrated in Section 2, because al-
though it reads multiple data points from input ﬁles, each data
point is not fed into the AI model, but rather into a simulation
software to compute data points for the AI model.

Figure 7: Permutation Invariant Model Deﬁnition

There are multiple ways to specify an AI model for this prob-
lem – namely, given a set of atoms (atom types, positions, and
velocities), predict a single scalar energy value. One way is
to implement an end-to-end energy prediction model [27] [28].
The other way aims to capture the essence of the end-to-end so-
lutions and let the system synthesize the whole model. One key
insight of the aforementioned energy prediction models is that
the atom conﬁguration is permutation invariant, which means
that the input should be modeled as a set of atoms, not a list.
Therefore, our goal here is to enable the system to compose an
AI model to honor this property and take advantage of existing
building blocks. A possible solution is shown in ﬁgure 7, where
the input is typechecked to be a list, and the module requires a
submodule that can complete the speciﬁed task (prediction in
the Molecular Dynamics context) to map the element type to
the output type. The element-wise results are then summed to
combine a permutation invariant output. This way, the system
is able to pick up the modules we deﬁned earlier, such as the
atom embedding converter, and the MLP model for conducting
element-wise prediction.

Now we discuss another benchmarking scenario, deep-
learning-based electron microscopy image segmentation,
which is becoming a popular topic in Biological Chem-
istry [29][30][31]. One of the main challenges in this topic is
the scarce of training data, due to complex and costly data ac-
quisition process. Given limited data, supervised deep-learning
methods require heavy human intervention and may fail to
generalize to unseen data [32][33]. One way to circumvent
the data problem is to introduce semi-supervised deep-learning
techniques, such as pre-training with high volume unlabeled
data [34]. To support pre-training in the benchmarking system
means that a model under evaluation should be able to carry a
part of its internal states (weights) from one task to another, and
adjust its computation graph accordingly. The problem deﬁni-
tion should also evaluate the performance of the model given
diﬀerent amounts of training data, to test its sample eﬃciency.

6. Discussion

We have elaborated on the methodology and the overview of
the system design, yet we look forward to further development
in the components. Brute-force enumeration of all possible test
hyperparameters may not be feasible and while pruning can me-
chanically improve the situation, it is desirable that a particular
problem module can suggest parameters suitable for a research
ﬁeld. More design work could be done to address model devel-
opment and debugging needs, for example, to allow model val-
idation in addition to training and testing. Python-based eDSL
has its limitations, mostly due to the syntactic constraints of the
language. To represent the modules more naturally, a program-
ming language more geared toward scientiﬁc computing can be
investigated [35].

Currently, SAIBench targets tractable scientiﬁc tasks, which
are mechanical procedures that can be computed and measured.
It is challenging to extend it to more creative scientiﬁc re-
search activities because it would require the system to formally
model the scientiﬁc concepts, and gain a deeper understand-
ing of research topics, motivations, methodologies, and goals,
and how various concepts interact with each other. Also, au-
tomated benchmarks require well-deﬁned metrics, while open-
ended scientiﬁc research ideas, in general, are hard to quantify.
Apart from type-based model composition, automatic AI
model synthesizing given a particular problem deﬁnition is also
a promising direction, given the advancement in AI-based code
generation [36] [37].

7. Conclusion

We have presented our deﬁnition of scientiﬁc AI benchmark-
ing, which is an ensemble of scientiﬁc task deﬁnition, AI bench-
marking, and system performance benchmarking. We have
then presented our methodology for scientiﬁc AI benchmark-
ing, with the key idea of decoupling and modularizing vari-
ous components, automatically benchmarking sensible combi-
nations. We have proposed a system design where the various
modules are implemented with a domain-speciﬁc language for
scientiﬁc AI computing. We have demonstrated that this de-
sign is ﬂexible enough to support benchmarking diﬀerent types
of scientiﬁc tasks, deﬁning AI models, deriving multiple met-
rics, combining metrics into ranking criteria, and conﬁguring
required hardware/software.

Figure 8: Electron Microscopy Image Segmentation.

The code for this scenario is shown in ﬁgure 8.

5.1. Comparison to Other Benchmarking Systems

As mentioned above, previous systems focus on a ﬁxed set
of test scenarios [19][20][21]. Additionally, the lack of declar-
ative modules means that it is hard to share data between the
benchmarking suite and external scientiﬁc computing software
packages, which is crucial in scientiﬁc AI benchmarking. For
example, the Gradient primitive in SAIBench allows a training
pipeline to extract gradients from an external package, which
is usually not exposed programmatically. The diﬀerences are
shown in table 4.

Focus

SAIBench
Diﬀerent
scientiﬁc
tasks/criterion

MLPerf

MLHarness

Accuracy,
system
throughput

Scalability,
MLCommon
coverage

Modules

Declarative

Hard-coded Markup

Test
Scenarios

Automatic
Discovery

Fixed

Fixed

Table 4: Comparison to Other Benchmarking Systems.

References

[1] K. Albertsson, P. Altoe, D. Anderson, J. Anderson, M. Andrews, J. P. A.
Espinosa, A. Aurisano, L. Basara, A. Bevan, W. Bhimji, D. Bona-
corsi, B. Burkle, P. Calaﬁura, M. Campanelli, L. Capps, F. Carmi-
nati, S. Carrazza, Y.-f. Chen, T. Childers, Y. Coadou, E. Coniavitis,
K. Cranmer, C. David, D. Davis, A. De Simone, J. Duarte, M. Erd-
mann, J. Eschle, A. Farbin, M. Feickert, N. F. Castro, C. Fitzpatrick,
M. Floris, A. Forti, J. Garra-Tico, J. Gemmler, M. Girone, P. Glaysher,
S. Gleyzer, V. Gligorov, T. Golling, J. Graw, L. Gray, D. Greenwood,
T. Hacker, J. Harvey, B. Hegner, L. Heinrich, U. Heintz, B. Hoober-
man, J. Junggeburth, M. Kagan, M. Kane, K. Kanishchev, P. KarpiÅski,
Z. Kassabov, G. Kaul, D. Kcira, T. Keck, A. Klimentov, J. Kowalkowski,

8

L. Kreczko, A. Kurepin, R. Kutschke, V. Kuznetsov, N. Khler, I. Lako-
mov, K. Lannon, M. Lassnig, A. Limosani, G. Louppe, A. Mangu,
P. Mato, N. Meenakshi, H. Meinhard, D. Menasce, L. Moneta, S. Moort-
gat, M. Neubauer, H. Newman, S. Otten, H. Pabst, M. Paganini,
M. Paulini, G. Perdue, U. Perez, A. Picazio, J. Pivarski, H. Prosper,
F. Psihas, A. Radovic, R. Reece, A. Rinkevicius, E. Rodrigues, J. Rorie,
D. Rousseau, A. Sauers, S. Schramm, A. Schwartzman, H. Severini,
P. Seyfert, F. Siroky, K. Skazytkin, M. Sokoloﬀ, G. Stewart, B. Stienen,
I. Stockdale, G. Strong, W. Sun, S. Thais, K. Tomko, E. Upfal, E. Usai,
A. Ustyuzhanin, M. Vala, J. Vasel, S. Vallecorsa, M. Verzetti, X. Vilass-
Cardona, J.-R. Vlimant, I. Vukotic, S.-J. Wang, G. Watts, M. Williams,
W. Wu, S. Wunsch, K. Yang, O. Zapata, Machine learning in high energy
physics community white paperarXiv:1807.02876.
URL http://arxiv.org/abs/1807.02876

[2] T. Kurth, S. Treichler, J. Romero, M. Mudigonda, N. Luehr, E. Phillips,
A. Mahesh, M. Matheson, J. Deslippe, M. Fatica, Prabhat, M. Houston,
Exascale deep learning for climate analyticsarXiv:1810.01993.
URL http://arxiv.org/abs/1810.01993

[3] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese,
T. Ewalds, R. Hafner, A. Abdolmaleki, D. de las Casas, C. Don-
ner, L. Fritz, C. Galperti, A. Huber, J. Keeling, M. Tsimpoukelli,
J. Kay, A. Merle, J.-M. Moret, S. Noury, F. Pesamosca, D. Pfau,
O. Sauter, C. Sommariva, S. Coda, B. Duval, A. Fasoli, P. Kohli,
K. Kavukcuoglu, D. Hassabis, M. Riedmiller, Magnetic control of toka-
mak plasmas through deep reinforcement learning 602 (7897) 414–419.
doi:10.1038/s41586-021-04301-9.
URL https://www.nature.com/articles/s41586-021-04301-9

[4] A. N. Laboratory, AI for science report.

URL
158802.pdf

https://publications.anl.gov/anlpubs/2020/03/

[5] R. B. Neale, A. Gettelman, S. Park, C.-C. Chen, P. H. Lauritzen, D. L.
Williamson, A. J. Conley, D. Kinnison, D. Marsh, A. K. Smith, F. Vitt,
R. Garcia, J.-F. Lamarque, M. Mills, S. Tilmes, H. Morrison, P. Cameron-
Smith, W. D. Collins, M. J. Iacono, R. C. Easter, X. Liu, S. J. Ghan, P. J.
Rasch, M. A. Taylor, Description of the NCAR community atmosphere
model (CAM 5.0) 289.

[6] J. S. Smith, R. Zubatyuk, B. Nebgen, N. Lubbers, K. Barros, A. E. Roit-
berg, O. Isayev, S. Tretiak, The ANI-1ccx and ANI-1x data sets, coupled-
cluster and density functional theory properties for molecules 7 (1) 134.
doi:10.1038/s41597-020-0473-z.
URL http://www.nature.com/articles/s41597-020-0473-z
[7] L. Ruddigkeit, R. van Deursen, L. C. Blum, J.-L. Reymond, Enumeration
of 166 billion organic small molecules in the chemical universe database
GDB-17 52 (11) 2864–2875. doi:10.1021/ci300415d.
URL https://pubs.acs.org/doi/10.1021/ci300415d

[8] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris,
R. L. Buckner, Open access series of imaging studies (OASIS): Cross-
sectional MRI data in young, middle aged, nondemented, and demented
older adults 19 (9) 1498–1507. doi:10.1162/jocn.2007.19.9.1498.
URL https://direct.mit.edu/jocn/article/19/9/1498/4427/
Open-Access-Series-of-Imaging-Studies-OASIS-Cross
[9] E. Weinan, J. Han, A. Jentzen, Deep learning-based numerical methods
for high-dimensional parabolic partial diﬀerential equations and back-
ward stochastic diﬀerential equations 5 (4) 349–380, publisher: Springer
Verlag. doi:10.1007/s40304-017-0117-6.
URL https://collaborate.princeton.edu/en/publications/
deep-learning-based-numerical-methods-for-high-
dimensional-parabo

[10] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural
networks: A deep learning framework for solving forward and inverse
problems involving nonlinear partial diﬀerential equations 378 686–707.
doi:10.1016/j.jcp.2018.10.045.
URL https://www.sciencedirect.com/science/article/pii/
S0021999118307125

[11] F. No, Machine learning for molecular dynamics on long timescales,
in: K. T. Schtt, S. Chmiela, O. A. von Lilienfeld, A. Tkatchenko,
K. Tsuda, K.-R. Mller (Eds.), Machine Learning Meets Quantum Physics,
Springer International Publishing, pp. 331–372. doi:10.1007/978-3-
030-40245-7 16.
URL https://doi.org/10.1007/978-3-030-40245-7 16

[12] A. Mardt, L. Pasquali, H. Wu, F. No, VAMPnets for deep learning of

molecular kinetics 9 (1) 5, number: 1 Publisher: Nature Publishing
Group. doi:10.1038/s41467-017-02388-1.
URL https://www.nature.com/articles/s41467-017-02388-1

[13] W. Jia, H. Wang, M. Chen, D. Lu, L. Lin, R. Car, W. E, L. Zhang, Pushing
the limit of molecular dynamics with ab initio accuracy to 100 million
atoms with machine learningVersion: 1. arXiv:2005.00223.
URL http://arxiv.org/abs/2005.00223

[14] T. Hoeﬂer, R. Belli, Scientiﬁc benchmarking of parallel computing sys-
tems:
twelve ways to tell the masses when reporting performance re-
sults, in: Proceedings of the International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis, ACM, pp. 1–12.
doi:10.1145/2807591.2807644.
URL https://dl.acm.org/doi/10.1145/2807591.2807644

[15] E. Apr, E. J. Bylaska, W. A. de Jong, N. Govind, K. Kowalski, T. P.
Straatsma, M. Valiev, H. J. J. van Dam, Y. Alexeev, J. Anchell, V. Anisi-
mov, F. W. Aquino, R. Atta-Fynn, J. Autschbach, N. P. Bauman, J. C.
Becca, D. E. Bernholdt, K. Bhaskaran-Nair, S. Bogatko, P. Borowski,
J. Boschen, J. Brabec, A. Bruner, E. Caut, Y. Chen, G. N. Chuev, C. J.
Cramer, J. Daily, M. J. O. Deegan, T. H. Dunning, M. Dupuis, K. G.
Dyall, G. I. Fann, S. A. Fischer, A. Fonari, H. Frchtl, L. Gagliardi,
J. Garza, N. Gawande, S. Ghosh, K. Glaesemann, A. W. Gtz, J. Ham-
mond, V. Helms, E. D. Hermes, K. Hirao, S. Hirata, M. Jacquelin,
L. Jensen, B. G. Johnson, H. Jnsson, R. A. Kendall, M. Klemm,
R. Kobayashi, V. Konkov, S. Krishnamoorthy, M. Krishnan, Z. Lin, R. D.
Lins, R. J. Littleﬁeld, A. J. Logsdail, K. Lopata, W. Ma, A. V. Marenich,
J. Martin del Campo, D. Mejia-Rodriguez, J. E. Moore, J. M. Mullin,
T. Nakajima, D. R. Nascimento, J. A. Nichols, P. J. Nichols, J. Nieplocha,
A. Otero-de-la Roza, B. Palmer, A. Panyala, T. Pirojsirikul, B. Peng,
R. Peverati, J. Pittner, L. Pollack, R. M. Richard, P. Sadayappan, G. C.
Schatz, W. A. Shelton, D. W. Silverstein, D. M. A. Smith, T. A. Soares,
D. Song, M. Swart, H. L. Taylor, G. S. Thomas, V. Tipparaju, D. G. Truh-
lar, K. Tsemekhman, T. Van Voorhis, . Vzquez-Mayagoitia, P. Verma,
O. Villa, A. Vishnu, K. D. Vogiatzis, D. Wang, J. H. Weare, M. J.
Williamson, T. L. Windus, K. WoliÅski, A. T. Wong, Q. Wu, C. Yang,
Q. Yu, M. Zacharias, Z. Zhang, Y. Zhao, R. J. Harrison, NWChem: Past,
present, and future 152 (18) 184102. doi:10.1063/5.0004997.
URL http://aip.scitation.org/doi/10.1063/5.0004997

[16] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, S. Chintala, PyTorch: An imperative style, high-
performance deep learning library, in: Advances in Neural Information
Processing Systems, Vol. 32, Curran Associates, Inc.
URL
bdbca288fee7f92f2bfa9f7012727740-Abstract.html

https://papers.nips.cc/paper/2019/hash/

[17] M. Brehm, SANscript a scientiﬁc algorithm notation language.
URL https://brehm-research.de/sanscript.php

[18] M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Ax-
ton, A. Baak, N. Blomberg, J.-W. Boiten, L. B. da Silva Santos, P. E.
Bourne, J. Bouwman, A. J. Brookes, T. Clark, M. Crosas, I. Dillo,
O. Dumon, S. Edmunds, C. T. Evelo, R. Finkers, A. Gonzalez-Beltran,
A. J. G. Gray, P. Groth, C. Goble, J. S. Grethe, J. Heringa, P. A. C. t
Hoen, R. Hooft, T. Kuhn, R. Kok, J. Kok, S. J. Lusher, M. E. Martone,
A. Mons, A. L. Packer, B. Persson, P. Rocca-Serra, M. Roos, R. van
Schaik, S.-A. Sansone, E. Schultes, T. Sengstag, T. Slater, G. Strawn,
M. A. Swertz, M. Thompson, J. van der Lei, E. van Mulligen, J. Vel-
terop, A. Waagmeester, P. Wittenburg, K. Wolstencroft, J. Zhao, B. Mons,
The FAIR guiding principles for scientiﬁc data management and stew-
ardship 3 (1) 160018, number: 1 Publisher: Nature Publishing Group.
doi:10.1038/sdata.2016.18.
URL https://www.nature.com/articles/sdata201618

[19] W. Gao, C. Luo, L. Wang, X. Xiong, J. Chen, T. Hao, Z. Jiang, F. Fan,
M. Du, Y. Huang, F. Zhang, X. Wen, C. Zheng, X. He, J. Dai, H. Ye,
Z. Cao, Z. Jia, K. Zhan, H. Tang, D. Zheng, B. Xie, W. Li, X. Wang,
J. Zhan, AIBench: Towards scalable and comprehensive datacenter AI
benchmarking, in: C. Zheng, J. Zhan (Eds.), Benchmarking, Measuring,
and Optimizing, Vol. 11459, Springer International Publishing, pp. 3–9,
series Title: Lecture Notes in Computer Science. doi:10.1007/978-3-
030-32813-9 1.
URL http://link.springer.com/10.1007/978-3-030-32813-9 1
[20] P. Mattson, C. Cheng, C. Coleman, G. Diamos, P. Micikevicius, D. Pat-

9

[36] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement,
D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tu-
fano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu,
S. Liu, CodeXGLUE: A machine learning benchmark dataset for code
understanding and generationarXiv:2102.04664.
URL http://arxiv.org/abs/2102.04664

[37] D. Peng, S. Zheng, Y. Li, G. Ke, D. He, T.-Y. Liu, How could neural
networks understand programs?, in: Proceedings of the 38th International
Conference on Machine Learning, PMLR, pp. 8476–8486, ISSN: 2640-
3498.
URL https://proceedings.mlr.press/v139/peng21b.html

terson, H. Tang, G.-Y. Wei, P. Bailis, V. Bittorf, D. Brooks, D. Chen,
D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang, A. Ike, B. Jia,
D. Kang, D. Kanter, N. Kumar, J. Liao, G. Ma, D. Narayanan, T. Ogun-
tebi, G. Pekhimenko, L. Pentecost, V. J. Reddi, T. Robie, T. S. John,
T. Tabaru, C.-J. Wu, L. Xu, M. Yamazaki, C. Young, M. Zaharia, MLPerf
training benchmark 14.

[21] Y.-H. Chang,

J. Pu, W.-m. Hwu,

J. Xiong, MLHarness: A
scalable benchmarking system for MLCommons 1 (1) 100002.
doi:10.1016/j.tbench.2021.100002.
URL https://www.sciencedirect.com/science/article/pii/
S2772485921000028

[22] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, A. Lerer, Automatic diﬀerentiation in PyTorch
4.

[23] Y. Lecun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning
applied to document recognition 86 (11) 2278–2324. doi:10.1109/
5.726791.
URL http://ieeexplore.ieee.org/document/726791/

[24] J. Thiyagalingam, M. Shankar, G. Fox, T. Hey, Scientiﬁc machine learn-

ing benchmarksarXiv:2110.12773.
URL http://arxiv.org/abs/2110.12773

[25] R. Haunschild, A. Barth, B. French, A comprehensive analysis of the
history of DFT based on the bibliometric method RPYS 11 (1) 72. doi:
10.1186/s13321-019-0395-y.
URL https://doi.org/10.1186/s13321-019-0395-y

[26] H. G. Kmmel, A biography of

coupled cluster method
17 (28) 5311–5325, publisher: World Scientiﬁc Publishing Co.
doi:10.1142/S0217979203020442.
URL
S0217979203020442

https://www.worldscientific.com/doi/abs/10.1142/

the

[27] J. Han, L. Zhang, R. Car, W. E, Deep potential: a general representation
of a many-body potential energy surface 23 (3). arXiv:1707.01478,
doi:10.4208/cicp.OA-2017-0213.
URL http://arxiv.org/abs/1707.01478

[28] O. T. Unke, M. Meuwly, PhysNet: A neural network for predicting en-
ergies, forces, dipole moments and partial charges 15 (6) 3678–3693.
arXiv:1902.08408, doi:10.1021/acs.jctc.9b00181.
URL http://arxiv.org/abs/1902.08408

[29] E. Gmez-de Mariscal, M. MaÅka, A. Kotrbov, V. Pospchalov, P. Matula,
A. Muoz-Barrutia, Deep-learning-based segmentation of small extracel-
lular vesicles in transmission electron microscopy images 9 (1) 13211,
number: 1 Publisher: Nature Publishing Group. doi:10.1038/s41598-
019-49431-3.
URL https://www.nature.com/articles/s41598-019-49431-3

[30] L. von Chamier, R. F. Laine, J. Jukkala, C. Spahn, D. Krentzel, E. Nehme,
M. Lerche, S. Hernndez-Prez, P. K. Mattila, E. Karinou, S. Holden,
A. C. Solak, A. Krull, T.-O. Buchholz, M. L. Jones, L. A. Royer,
C. Leterrier, Y. Shechtman, F. Jug, M. Heilemann, G. Jacquemet,
R. Henriques, Democratising deep learning for microscopy with Zero-
CostDL4mic 12 (1) 2276, number: 1 Publisher: Nature Publishing Group.
doi:10.1038/s41467-021-22518-0.
URL https://www.nature.com/articles/s41467-021-22518-0

[31] J. M. Ede, Deep learning in electron microscopy 2 (1) 011004, publisher:

IOP Publishing. doi:10.1088/2632-2153/abd614.
URL https://doi.org/10.1088/2632-2153/abd614

[32] S. M. Plaza, J. Funke, Analyzing image segmentation for connectomics

12 102. doi:10.3389/fncir.2018.00102.
URL
fncir.2018.00102/full

https://www.frontiersin.org/article/10.3389/

[33] J. W. Lichtman, H. Pﬁster, N. Shavit, The big data challenges of connec-

tomics 17 (11) 1448–1454. doi:10.1038/nn.3837.
URL http://www.nature.com/articles/nn.3837

[34] R. Conrad, K. Narayan, CEM500k, a large-scale heterogeneous unla-
beled cellular electron microscopy image dataset for deep learning 10
e65894, publisher: eLife Sciences Publications, Ltd. doi:10.7554/
eLife.65894.
URL https://doi.org/10.7554/eLife.65894

[35] M. Innes, A. Edelman, K. Fischer, C. Rackauckas, E. Saba, V. B. Shah,
W. Tebbutt, A diﬀerentiable programming system to bridge machine
learning and scientiﬁc computingarXiv:1907.07587.
URL http://arxiv.org/abs/1907.07587

10

