PREVENT: An Unsupervised Approach to Predict
Software Failures in Production

Giovanni Denaro∗, Rahim Heydarov+, Ali Mohebbi+, Mauro Pezzè+,#

∗ University of Milano-Bicocca + Università della Svizzera Italiana (USI) # Schaffhausen Institute of Technology

Milano, Italy

Lugano, Switzerland

Schaffhausen, Switzerland

Email: giovanni.denaro@unimib.it, {rahim.heydarov, ali.mohebbi, mauro.pezze}@usi.ch

2
2
0
2

g
u
A
5
2

]
E
S
.
s
c
[

1
v
9
3
9
1
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—This paper presents PREVENT, an approach for
predicting and localizing failures in distributed enterprise ap-
plications by combining unsupervised techniques.

Software failures can have dramatic consequences in produc-
tion, and thus predicting and localizing failures is the essential
step to activate healing measures that
the disruptive
consequences of failures. At the state of the art, many failures
can be predicted from anomalous combinations of system metrics
with respect to either rules provided from domain experts or
supervised learning models. However, both these approaches limit
the effectiveness of current techniques to well understood types
of failures that can be either captured with predeﬁned rules or
observed while trining supervised models.

limit

PREVENT integrates the core ingredients of unsupervised
approaches into a novel approach to predict failures and localize
failing resources, without either requiring predeﬁned rules or
training with observed failures. The results of experimenting
with PREVENT on a commercially-compliant distributed cloud
system indicate that PREVENT provides more stable and reliable
predictions, earlier than or comparably to supervised learning
approaches, without requiring long and often impractical training
with failures.

I. INTRODUCTION

Good design and quality assurance practices cannot prevent
software to fail in production [20], and the complexity of
distributed enterprise applications further increases the risks of
production failures. Many failures occur in production when
the execution of some faulty statements corrupts the execution
state, and eventually the error state propagates to a system
failure, that is, a deviation of the delivered service from the
required functionality. For this reason, many approaches to
predict failures try to infer in advance the future occurrence
of system failures, by detecting the anomalies of some system
metrics that can be monitored in production, and that can
reﬂect possible error states.

Current approaches for predicting failures exploit rule-
based, signature-based, or combined strategies, with com-
plementary beneﬁts and limitations. Rule-based approaches
rely on predicates that experts extract from data observed
during operations [10]. Signature-based approaches rely on
supervised learning models that leverage the information from
historical records of previously observed anomalies [5], [12],
[15], [19], [26], [32], [39], [40], [45], [48]. Combined ap-
proaches exploit signature-based models on top of synthetic

data inferred with either semi-supervised or unsupervised
learning, to balance accuracy and required information [18],
[22], [34], [49], [50].

Signature-based approaches identify failures of pre-deﬁned
failure types that occur in the training data. Unsupervised
approaches detect failures as deviations from some model of
the nominal system behavior suitably inferred in absence of
failures, and can thus reveal failures of types that do not
necessarily correspond to training data [3], [6], [14], [16], [24],
[38]. Signature-based approaches for distributed applications
often aim also to localize the components responsible for the
failures [10], [23], [31], [33], [34], [44], [49].

Both signature-based and unsupervised approaches identify
failure-prone executions and localize failing resources for
general software systems without requiring expensive data
management. They beneﬁt and suffer from complementary
advantages and limitations: Signature-based approaches ben-
eﬁt from low false positive rates when dealing with known
classes of failures. Unsupervised approaches may in principle
address any kind of failure type, but may experience high false
positive rates. Signature-based and unsupervised solutions
exploit different statistical or machine learning techniques.

The work that we present in this paper is grounded on the
lessons learned from our previous work on PREMISE [34],
EMBED [37], [38], and LOUD [33], three state-of-the-art ap-
proaches with the complementary advantages and limitations
of the main current approaches.

PREMISE is a supervised technique that combines Granger
causality analysis and Logistic Model Trees (LMT) to predict
failures and localize failing resources. The experimental results
reported in the literature [34] indicate that PREMISE is effec-
tive thanks to the training with injected failures. At the same
time, the applicability of PREMISE is strongly limited by the
need of training with labeled data, obtained by executing the
system with injected failures.

EMBED predicts failures by using Restricted Boltzmann
Machines (RBMs) to model the total Gibbs energy associated
with sets of KPIs that represent the system state [37], [38].
Signiﬁcant energy ﬂuctuations indicate system states that
deviate from normal conditions, pinpointing possible failures.
The experimental results presented in the literature [38] indi-
cate that EMBED is an effective, state-of-the-art approach to

 
 
 
 
 
 
predicting failures without training with seeded failures, but
the data on the Gibbs energy do not provide information to
localize the failing resources.

LOUD localizes failing resources by combining unsuper-
vised machine learning and graph centrality measures, to infer
which resources have higher centrality in a graph that models
the causal dependencies between the anomalies being observed
in the system [33]. The experimental results reported in the
literature indicate that LOUD can effectively localize failing
resources without requiring training with injected failures, but
it experiences fairly high false positive rates when the con-
sidered anomalies do not correspond to failure prone system
states (benign anomalies).

In this paper we propose PREVENT, a framework that
integrates the core ingredients of EMBED and LOUD, includes
a novel approach that draws on our experience with PREMISE,
and is unsupervised. PREVENT can both predict failures
and localize failing resources without requiring training with
known or seeded failures.

We introduce a new large dataset

that we obtained by
running a large set of experiments on REDIS, a commercially-
compliant, distributed cloud system. The experiments that we
discuss in the paper compare the stability, reliability and
earliness of PREVENT with PREMISE, when we apply the two
approaches for predicting the failing components in REDIS
for a set of seeded failures. We compare with PREMISE
as a good representative of state-of-the-art signature-based
approaches for predicting and localizing failures in distributed
applications, which provides excellent results thanks to the
supervised models that beneﬁt from long training with seeded
failures.

We deﬁne the stability of a prediction as the true positive
rate after the ﬁrst true prediction, that is, the continuity in
correctly reporting the failing component in the presence of
error states. We deﬁne the reliability of the prediction with
reference to the false positive rate, that is, the frequency of
alarms that do not correspond to error states or indicate wrong
failing components: The lower the false positive rate,
the
higher the reliability of the prediction. We deﬁne the earliness
of a prediction as the time interval between the ﬁrst correct
prediction and the actual failure, that is, the time interval
for a proactive action on the failing component to prevent
a failure before its occurrence. The results that we report in
Section III indicate that PREVENT provides more stable and
reliable predictions earlier than or comparably to PREMISE,
without paying the limitations of supervised approaches.

This paper contributes to the research in software engineer-

ing by

• deﬁning an original combination of unsupervised ap-
proaches to predict and localize failures in distributed
enterprise applications, without suffering from the strong
limitations of training with failures,

• presenting a large set of data collected from a
commercially-compliant, distributed cloud systems to
evaluate and compare different approaches, data that we
offer in a replication package,

Fig. 1. Overview of PREVENT

• discussing a thorough comparison of the proposed ap-
proach with a state-of-the-art supervised approach, com-
parison that indicates the advantages that can be obtained
with an unsupervised approach without suffering from the
limitations of supervised ones.

This paper is organized as follows. Section II presents
the new approach PREVENT and its relation with PREMISE
and LOUD. It then describes PREVENTE and PREVENTA,
the two approaches that comprise PREVENT, and that exploit
Gibbs free energy and Granger causality to predict failures,
respectively. Section III describes the experimental setting, and
discusses the results of the experiments that comparatively
to PREMISE and LOUD.
evaluate PREVENT with respect
Section IV discusses the main state-of-the-art approaches for
predicting and diagnosing failures, and their relation with
PREVENT. Section V summarizes the contribution of the paper
and indicates novel research directions.

II. PREVENT

The core contribution of this paper is PREVENT, an ap-
proach for predicting and localizing failures in distributed
enterprise applications. The distinctive characteristic of PRE-
VENT is an original combination of unsupervised techniques
to predict and localize failures at the component level (node)
of enterprise applications.

Figure 1 shows the two main components of PREVENT,
the State classiﬁer and the Anomaly ranker, that predict and
localize failures, respectively. The ﬁgure shows that the State
classiﬁer and the Anomaly ranker work on time series of Key
Performance Indicators, KPIs, sets of metric values observed
by monitoring the application at regular time intervals (every
minute in our experiments). A KPI is a pair (cid:104)metric, node(cid:105) of
a metric value collected at either a virtual or physical node of
the monitored application. PREVENT collects KPI series with
Kibana [2] and Elasticsearch [1].

PREVENT returns a list of anomalous nodes ranked by
anomaly relevance, list that the Anomaly ranker produces in
the presence of anomalous states inferred with the State clas-
siﬁer. The anomalous states correlate with incoming failures,
and the anomalous nodes indicate the more likely nodes that
will lead to failure.

2

State classifierNormal stateKPIsAnomaly rankerAnomalous nodesAnomalous state Both the State classiﬁer and the Anomaly ranker are un-
supervised: They train unsupervised models with KPI data
collected during normal (non-failing) executions of the ap-
plication, without requiring any labels at training time. The
State classiﬁer crosschecks the KPI values observed in pro-
duction against the normal-execution characteristics inferred
during training, and either accepts normal states, when there
are no signiﬁcant differences, or pinpoints anomalous states,
otherwise. The Anomaly ranker exploits anomalous KPIs,
that is, KPIs with values that signiﬁcantly differ from values
observed during training in normal execution conditions, for
ranking anomalous nodes according to their relevance with
respect to the anomalous KPIs. The Anomaly ranker reports
the anomalous nodes only in the presence of anomalous states.
We discuss the inference models that
instantiate the two
components later in this section.

We deﬁned PREVENT from the lessons learned with
PREMISE [34], EMBED [37], [38], and LOUD [33], three
representative techniques to predict and localize failures. All
three approaches well address some issues related to predicting
and localizing failures, but none of them offers a complete
solution, and all of them suffer from limitations that cannot
be addressed by simply merging the three approaches in a
single framework.

PREMISE predicts failures and localizes faults by combin-
ing an unsupervised approach for detecting anomalous KPIs
with a signature-based approach for predicting failures. The
supervised nature of the signature-based approach requires
long supervised training, and achieves good precision only
for failures of types considered during supervised training.
PREMISE indicates that it is indeed possible to predict and
localize some types of failures both with good precision and
early enough to execute suitable failure prevention actions.
However, it also highlights the strong limitations of training
systems with seeded faults in production, as supervised ap-
proaches require. We refer to PREMISE as the baseline to
evaluate PREVENT.

LOUD localizes faults with an unsupervised observational
strategy, by identifying the application nodes highly relevant
with respect to the causal dependencies between the observed
anomalies. LOUD assumes the availability of precise anomaly
predictions at node-level to limit the otherwise large amount
of false alarms.

EMBED predicts failures by computing the Gibbs free
energy associated with the KPIs that represent the system state,
and monitoring anomalous energy ﬂuctuations in production.
EMBED predictions address the target application as a whole,
without any information about ﬁne-grained anomalies at the
level of the application nodes.

LOUD and EMBED pave the road to approaches for pre-
dicting failures and localizing faults without requiring training
with seeded faults. In this paper, we integrate the core tech-
niques of LOUD and EMBED with new components, to deﬁne
PREVENT, an unsupervised approach to predict and localize
failures with high precision by relying solely on training
during normal executions.

Figure 2 zooms into the State classiﬁer and Anomaly
ranker components of PREVENT, and shows that we deﬁne
PREVENT as the ensemble of two approaches, PREVENTE and
PREVENTA, that offer complementary implementations of the
State classiﬁer, both integrated with the same Anomaly ranker.
PREVENTE State classiﬁer inherits the restricted-Bolzman-
machine approach of EMBED to detect anomalous states,
while PREVENTA State classiﬁer deﬁnes a novel approach that
combines a deep autoencoder with a one-class-support-vector-
machine (OCSVM) classiﬁer to reveal anomalies in unsuper-
vised fashion, without requiring any labels at training time.
PREVENTA State classiﬁer is an entirely original contribution
of this paper. Both PREVENTE and PREVENTA integrate
with an Anomaly ranker adapted from the approach deﬁned
in LOUD that combines Granger-causality with eigenvector-
centrality to precisely localize failing nodes.

A. PREVENTE State Classiﬁer
PREVENTE State classiﬁer

revisits

the EMBED ap-
proach [38] with up-to-date and robust technology. It infers
anomalous combinations of KPI values from perturbations of
the Gibbs free energy computed on time series of KPI values
monitored from production.

The Gibbs free energy was originally introduced in physics
to model macroscopic many-particle systems as a statistical
characterization of the properties of single particles that affect
the global physical quantities such as energy or tempera-
ture [8], and is applied in many domains, such as the growth
of the World Wide Web and the spread of epidemics [13].

Both EMBED and PREVENTE rely on the intuition that
complex distributed enterprise software applications and phys-
ical systems share the dependencies of properties of interest
of the global state of the system (energy and temperature in
the case of physical systems, failures in the case of software
applications) on the collective conﬁguration of basic elements
(particles in the case of physical systems, KPI values in the
case of software applications). Following this intuition, both
EMBED and PREVENTE recast the problem of predicting
failures in complex distributed enterprise software applications
to the problem of revealing the collective alignment of the
KPIs of an application to correlated anomalous values.

PREVENTE supersedes the intractability of analytically rep-
resenting the physical dependencies that concretely govern the
correlations among KPIs [7], by approximating the computa-
tion of the Gibbs energy with restricted Bolzmann machines,
RBM [17], and signals anomalous states when the energy
exceeds a threshold.

Figure 3 show the two convolutional layers of the RBM
as instantiated in PREVENTE: A visible layer with as many
neurons as the number of KPIs monitored on the target
application, and a hidden layer with an equivalent number of
neurons. The visible layer takes in input the values of the KPIs
monitored at each timestamp, while the hidden layer encodes
the joint distributions of KPIs, based on the training-phase
sampling of the conditional probabilities of the hidden nodes
given the visible nodes.

3

(a) PREVENTE

(b) PREVENTA

Fig. 2. PREVENT loosely coupled structure

their states to correlated anomalous values: The higher the
computed energy, the higher the likelihood of some system
anomaly that manifests as anomalous values of increasing sets
of correlated KPIs.

PREVENTE reports anomalies, when the energy values
observed at runtime exceed some threshold values computed
by training the network with data collected during normal
execution. Monni et al. observe that the threshold values can be
computed ofﬂine, even by training the RBM with datasets from
different applications, and report high precision and recall of
the approach in detecting anomalous states [38].

Fig. 3. RBM as instantiated in PREVENTE

PREVENTE exploits a trained RBM to compute the Gibbs
free energy from the values of a set of KPIs monitored at
regular time intervals in production. The free energy grows in
absolute value when increasingly large subsets of KPIs align

PREVENTE reimplements the RBM neural network compo-
nent of the original EMBED prototype of Monni et al. [38]
with robust
technology solutions in Matlab to enable an
extensive and comparative evaluation campaign.

4

KPIsAnomaly rankerAnomalous nodesDeep autoencoderanomalous KPIsGranger-causalityanalyzerPageRankcentrality calculatoranomaly causality graphState classifierRBMneural networkGibbs free energybeyondthresholdnoyesNormal stateAnomalous state KPIsAnomaly rankerAnomalous nodesDeep autoencoderGranger-causalityanalyzerPageRankcentrality calculator anomaly causality graphState classifierDeep autoencoderOCSVM classifieranomalous KPIsAnomalous state Normal stateanomalous KPIsvisiblelayerhiddenlayer…………KPI1KPI2KPI3KPI4KPIn……B. PREVENTA State Classiﬁer

PREVENTA State classiﬁer proposes a novel solution for
predicting failures by combining a deep autoencoder with a
one-class-support-vector-machine (OCSVM) classiﬁer, to pre-
dict failures without requiring training with seeded faults. The
deep autoencoder model identiﬁes anomalous KPIs as KPIs
with values that are anomalous with respect to the observations
when training with normal executions. The one-class-support-
vector-machine (OCSVM) classiﬁer discriminates between
benign sets of anomalies (which resemble combinations of
anomalies occasionally observed during training) and failure-
prone sets of anomalies (which signiﬁcantly differ form the
observations during training).

A deep autoencoder (also simply referred to as autoencoder)
is a neural network architected with two contiguous sequences
of layers that mirror each other structure: A ﬁrst sequence of
layers of decreasing size up to an intermediate layer of mini-
mal size, and a second sequence of layers of correspondingly
increasing sizes up to a layer with the same size of the initial
layer.

During training, the ﬁrst half of the network learns how to
encode the input data in incrementally condensed form, up to
a minimal form in the intermediate layer; The second half of
the network learns how to regenerate the input based on the in-
formation condensed in the intermediate layer. The difference
between the input and output values is the reconstruction error
of the autoencoder.

During training the neurons of the network learn functions
that minimize the average reconstruction error on the training
data. In production, the network returns small reconstruction
errors for data similar to the training data both in absolute
values and mutual correlations. It returns large reconstruction
errors for data that signiﬁcantly differ from the observations
in the training phase.

Figure 4 illustrates the architecture of PREVENTA autoen-
coder, composed of ﬁve layers with sizes n, n/2, n/4, n/2
and n, respectively, being n is the number of monitored
KPIs. We train PREVENTA autoencoder to reconstruct the
values of the KPIs observed at regular time intervals on the
distributed enterprise application executed in normal condi-
tions, that is, without failures. In production, the autoencoder
reveals anomalies when observing KPI values that result in
reconstruction errors that signiﬁcantly differ from the average
reconstruction error computed during training, and identiﬁes
anomalous KPIs as KPIs with locally high reconstruction
errors. Speciﬁcally, PREVENTA signals anomalies with recon-
struction errors over a threshold that we set to one in the
experiments.

PREVENTA feeds the anomalies that it computes with the
autoencoder to a one-class-support-vector-machine (OCSVM)
classiﬁer, to discriminate between benign and failure-prone
anomalies. PREVENTA OCSVM processes a set of binary
inputs for the KPIs: one if PREVENTA autoencoder indicates
the KPI as anomalous, zero otherwise. PREVENTA OCSVM
classiﬁer returns a binary classiﬁcation of system states: Be-

Fig. 4. Deep autoencoder as instantiated in PREVENTA

nign anomalies, when the binary input resembles combinations
of anomalies occasionally observed in the training phase,
failure-prone anomaly sets otherwise.

We observe that PREVENTA State classiﬁer embraces a
more structured approach than PREVENTE. PREVENTA iden-
tiﬁes anomalous KPIs with a neural network (the autoen-
coder), and discriminates failure-prone sets of anomalies with
a classiﬁer (the OCSVM classiﬁer), while PREVENTE relies
solely on a neural model to identify anomalous Gibbs energy
values. Indeed, a main research question that we address in
this paper is how the two approaches compare in fostering
effective decisions.

C. PREVENT Anomaly ranker

The PREVENT Anomaly ranker revisits the LOUD approach
of Mariani et al. [33], and integrates it into a complete predict-
localize cycle. The PREVENT Anomaly ranker:

(i) identiﬁes sets of anomalous KPIs with a deep autoen-

coder,

(ii) builds a graph that represents the causality dependencies
between the anomalous KPIs with Granger-causality
analysis [4], [21], [41],

(iii) exploits the causality graph to calculate the PageRank
centrality value of each anomalous KPI in the graph [28],
[35], [46],

(iv) returns the three nodes of the application with the highest
numbers of anomalous KPIs, selecting the top anomalous
KPIs of the centrality ranking, up considering at most
20% of the KPIs.

Deep Autoencoder: The deep autoencoder of PREVENT
Anomaly ranker works as the deep autoencoder of PREVENTA
state classiﬁer.

Granger Causality Analysis: At each timestamp,
the
Granger-causality analyzer of PREVENT Anomaly ranker
builds a causality graph that represents the causal dependen-
cies among the KPIs anomalous at the timestamp. During
training, PREVENT builds a baseline causality graph that
represents the causality relations between the KPIs, as captured

5

inputlayer(size n)outputlayer(size n)hiddenlayer(size n/2)hiddenlayer(size n/2)hiddenlayer(size n/4)EncoderLatentspaceDecoderInputOutputKPI1KPI2KPI3KPI4…………KPIn……………………KPI1KPI2KPI3KPI4KPInunder normal execution conditions: For each pair of KPIs
(cid:104)ka, kb(cid:105), there is an edge from from the corresponding node
ka to node kb in the baseline casualty graph, if the analysis of
the time series of the two KPIs reveals a causal dependency
from the values of ka on the values of kb, according to the
Granger causality test [21]. The weight of the edges indicates
the strength of the causal dependencies.1 At each timestamp
in production, PREVENT Anomaly ranker derives the causality
graph of the anomalous KPIs by pruning the baseline causality
graph, to exclude the KPIs that autoencoder does not indicate
as anomalous.

PageRank Centrality: At each timestamp in production,
PREVENT Anomaly ranker exploits the causality graph of
the anomalous KPIs to weight the relative relevance of the
anomalous KPIs as the PageRank centrality index. PageRank
scores the graph nodes (KPIs) according to both the number of
incoming edges and the probability of anomalies to randomly
spread through the graph (teleportation) [28]. Mariani et al.’s
experiments with LOUD conﬁrm the effectiveness of PageRank
centrality index to localize failures [33].

Top Anomalous Application Nodes: PREVENT Anomaly
ranker sorts the anomalous KPIs according to decreasing
values of their centrality scores, selects the top anomalous
KPIs of the ranking up to considering at most 20% of the
KPIs, tracks these anomalous KPIs to the corresponding nodes
of the application, and returns the three application nodes with
highest number of top-anomalous KPIs that correspond to their
location.

PREVENT Anomaly ranker improves LOUD by (i) discrim-
inating the anomalous KPIs with a deep autoencoder instead
of time-series analysis, (ii) exploiting the top anomalous KPIs
of the ranking up to 20% of KPIs, rather than simply referring
to the top 20 anomalous KPIs, and (iii) considering the KPIs
that are anomalous at each speciﬁc timestamp, rather than the
KPIs that are detected as anomalous at least once in the scope
of a time window.

By replacing the time-series analysis with the autoencoder,
PREVENT Anomaly ranker replaces a closed-source time-
series analysis component of LOUD with an open-source
autoencoder that we developed in-house, thus allowing us to
release PREVENT as an fully open source tool. By considering
the top anomalous KPIs up to 20% of the KPIs, PRE-
VENT Anomaly ranker scales the approach to large distributes
systems, with huge amounts of KPIs, as the case of the
experiments reported in this paper. Our experiments indicate
that considering only 20 anomalous KPIs leads to unbalanced

1The Granger causality test determines the existence of a causal dependency
between a pair of KPIs ka and kb as a statistical test of whether the time series
values of ka provide statistically signiﬁcant information about the evolution of
the future values of kb [4], [21], [41]. Speciﬁcally, we test the null-hypothesis
that ka does not Granger-cause kb by (i) building an auto-regression model
for the time series values of kb, (ii) building another regression model that
considers the values of both ka and kb as predictors for the future values of
kb, (iii) testing if the latter model provides signiﬁcantly better predictions
than the former one. If so, we reject the null-hypothesis in favor of the
alternative hypothesis that ka Granger-causes kb, and compute the strength of
the causality relation as the coefﬁcient of determination R2. We implemented
the test with the Statsmodels Python library [36], [47].

and often wrong localization decisions, when monitoring a
large amount of KPIs (more than 1,000 in the distributed
enterprise application of our experiments), while referring to
the most relevant anomalous KPI up of 20% of the KPIs
largely improves the precision of the localization. By referring
to the anomalous KPIs for each speciﬁc timestamp, PREVENT
Anomaly ranker identiﬁes anomalous states and anomalous
nodes at each timestamp.

III. EXPERIMENTS

A. Research Questions

Our experiments address three research questions:

RQ1: Can PREVENT predict failures in distributed enterprise

applications?

RQ2: Does the unsupervised PREVENT approach improve

RQ3: Does PREVENT improve over LOUD,

over state-of-the-art (supervised) approaches?
that

is,
Anomaly ranker used as a standalone component?

the

RQ1 studies the ability of PREVENT to predict failures.
We comparatively evaluate the ability of PREVENTE and
PREVENTA to predict failures in terms of false alarm rate,
prediction earliness, and stability of true positive predictions.
RQ2 investigates the advantages and limitations of training
without failing executions. PREVENT trains models with data
from normal (non-failing) executions only, while supervised
techniques rely on training with both normal and failing
executions. The nonnecessity of training with failing execu-
tion extends the applicability of PREVENT with respect to
supervised techniques. PREVENT discriminates failure-prone
conditions as executions that signiﬁcantly differ from ob-
servations at training time, while supervised techniques can
identify only failures of types considered during training. As
a result, PREVENT is failure-type agnostic, that is, it can
predict failures of any types, while supervised techniques
foster effectiveness by limiting the focus on speciﬁc types
of failures. We answer RQ2 by comparing both PREVENTE
and PREVENTA with PREMISE, a state-of-the-art supervised
approach that we studied in the last years and successfully
applied in industrial settings [34], in terms of true positive,
true negative, and false alarm rates.

RQ3 evaluates the contribution of the synergetic combi-
nation of the State classiﬁer and Anomaly ranker, to skim
false alarms that derive from ranking anomalous KPIs in non-
anomalous states. We evaluate the contribution of combin-
ing State classiﬁer and Anomaly ranker by comparing both
PREVENTE and PREVENTA with LOUD, the Anomaly ranker
component standalone that locates faults failures for any state,
by referring to the improved approach that we integrated in
the PREVENT Anomaly ranker.

B. Experimental Settings

We report the results of experimenting PREVENT on Redis
Cluster, a popular open-source enterprise application that
provides in-memory data storage.2 A Redis Cluster deploys the

2https://redis.io

6

the unavailable percentage of CPU power of a constant value
every minute; The exponential-increase pattern increments the
intensity of an exponentially growing value every minute; The
random-increase pattern either increments the intensity of a
constant value or leaves it unchanged, choosing either case
with probability 0.5, every minute.

Table II describes the combinations of failure types and
failure patterns of the nine experiments. Each row of the
table corresponds to an experiment with a failure injected in
a master-slave node pair of the Redis Cluster according to
a failure pattern, and replicated ten times while targeting a
different master-slave node pair for each replication. In each
experiment, we record a valid (true positive) prediction when
PREVENT signals anomalous nodes that include either or both
the nodes in which the failure was injected. The table reports
the experiment identiﬁers that we use in the ﬁgures, a summary
description of the failure type, the start-to-inject interval (that
is, the elapsed time from the initial stable execution of the
Redis cluster, after a suitable ramp up time, to the time we
inject the failure), and the inject-to-crash interval (that is, the
elapsed time from the ﬁrst failure injection to the system
crash).

Experimenting with PREVENT

We trained PREVENT prediction models with the unlabeled
data collected over two weeks of normal execution as fol-
lows: First, we trained the deep autoencoder (used both in
PREVENTE and PREVENTA) with the unlabeled data collected
in the ﬁrst week of normal execution; Next, we trained the
OCSVM classiﬁer (used in PREVENTA) with the (benign)
anomalies computed with the deep autoencoder against the
unlabeled data collected in the second week of normal ex-
ecution; We generated the baseline granger-causality graph
for the granger-causality analyzer (used both in PREVENTE
and PREVENTA) based on the unlabeled data collected in
both weeks of normal execution. We remark that we used
different data for training the deep autoencoder and the
OCSVM classiﬁer, to avoid any bias that may derive from
implicit correlations due to training both models with the same
data, since OCSVM classiﬁer takes as input the anomalies
detected with the deep autoencoder.

We also used the normal execution data to evaluate the
impact of false alarms, that is, failures that the prediction ap-
proaches might erroneously indicate during normal executions.
We used the failing execution data to evaluate (i) the ability
of revealing failures, (ii) the ability to correctly locate the
corresponding failing nodes, (iii) the impact of both false
negatives, due to missing in identify failing executions, and
false alarms, due to either pinpointing failures during normal
executions or attribute failures to non-failing nodes during
failing executions.

Experimenting with PREMISE and LOUD

The supervised PREMISE approach requires

labeled
as

data. We
and

no-failure

labeled
failing

normal
execution

with
data

7

training
execution
as
data

Fig. 5. Structure of the Redis Cluster

in-memory data storage services on a cluster of computational
nodes, and balances the request workload across the nodes
to improve throughput and response time. Figure 5 illustrates
the structure of the Redis Cluster that we deployed for our
experiments: twenty computational nodes running on separate
virtual machines, combined pairwise as ten master and ten
slave nodes. We integrated the Redis Cluster with a monitoring
facility built on top of Elasticsearch [1], to collect the 86 KPIs
indicated in Table I for each of the twenty nodes of the cluster,
on a per-minute basis, resulting in 1,720 KPIs collected per
minute.

We executed Redis Cluster with a workload consisting of
calls to operations for both storing and retrieving data into and
from the Cluster. The synthetic workload implements a trafﬁc
pattern that includes both daily and weakly variations, where
a high amount of calls at daytime and a decreased workload at
nighttime, peaks at 7 PM and 9 AM, low trafﬁc in weekends
and high trafﬁc in workdays: between 0 and 26 requests per
second in the weekends, between 0 and 40 in workdays. We
inherited this trafﬁc pattern from the pattern that Mariani et
al. report for their experimental evaluation in [33].

We executed the Redis cluster with either no failure or
injected failures,
to collect data during normal executions
(normal execution data) and in the presence of failures (failing
execution data), respectively.

We executed the Redis cluster with no injected failures for

two weeks of continuous execution.

We executed the Redis cluster with failing execution data
collected for three types of failures, each injected according
to three failure patterns at a master-slave node pair of the
Redis Cluster, for a total of nine experiments, each repeated
ten times to reduce statistical biases. We experimented with
memory leaks, packet loss and CPU hog failures, each injected
with linear, exponential and random patterns that deﬁne the
frequency of increasing the intensity of the failures. The
intensity of the failure corresponds to the amount of leaked
memory,
the number of lost packets and the unavailable
percentage of CPU power for the memory leaks, packet loss
and CPU hog failures, respectively. The linear-increase pattern
increases the intensity of the failure linearly, by incrementing
the amount of leaked memory, the number of lost packets or

Trafﬁc generator (clients’ store/retrieve requests)Master node 0Master node 1Master node 9Slave node 0Slave node 1Slave node 9OpenStack middlewareElastic search KPI monitorThe structure of the Redis ClusterKPI
type
Core

# of
KPI
9

CPU

19

File

Load

5

7

Memory

18

Process

8

Socket

11

Network

9

TABLE I
KPIS COLLECTED AT EACH NODE OF THE REDIS CLUSTER

KPI names

Id (CPU Core Identiﬁer), idle.pct (Percentage of Idle time), iowait.pct (Percentage of CPU time spent in wait), irq.pct (Percentage
of CPU time spent in handling hardware interrupts), nice.pct (Percentage of CPU time spent in low-priority processes), softirq.pct
(Percentage of CPU time spent in handling software interrupts), steal.pct (Percentage of CPU time spent in involuntary wait by the
virtual CPU while the hypervisor was servicing another processor), system.pct (Percentage of CPU time spent in kernel space), user.pct
(Percentage of CPU time spent in user space)
cores (Number of CPU cores on the host), idle.pct, iowait.pct, irq.pct, nice.pct, softirq.pct, steal.pct, system.pct, total.pct (Percentage
of CPU time spent
in states other than Idle and IOWait), user.pct norm.idle.pct, norm.iowait.pct, norm.irq.pct, norm.nice.pct,
norm.softirq.pct, norm.steal.pct, norm.system.pct, norm.total.pct , norm.user.pct (norm preﬁx means normalized value of the corre-
sponding CPU metric without the norm preﬁx. They are normalized by considering number of cores)
count (Number of ﬁle systems), total ﬁles (Total number of ﬁles), total size.free (Total free space on disk), total size.total (Total disk
space either used or free), total size.used (Total used disk space)
load.1 (Load average for the last minute), load.5 (Load average for the last 5 minutes), load.15 (Load average for the last 15 minutes),
cores (Number of CPU cores on the host), norm.1 (Load in the last minute divided by number of cores), norm.15, norm.5
actual.free, actual.used.bytes, actual.used.pct,
hugepages.used.pct, swap.used.bytes, swap.used.pct, total, used.bytes, used.pct
dead (Number of dead processes), idle (Number of idle processes), running (Number of running processes), sleeping (Number of
sleeping processes), stopped (Number of stopped processes), total (Total number of processes), un-known (Number of processes for
which the state could not be retrieved or is unknown), zombie (Number of zombie processes)
all.count (All open connections), all.listening (All listening ports), tcp.all.close wait (Number of TCP connections in close_wait state),
tcp.all.count (All open TCP connections), tcp.all.established (Number of established TCP connections), tcp.all.listening (All TCP
listening ports), tcp.all.orphan (Number of all orphaned tcp sockets), tcp.all.time wait (Number of TCP connections in time_wait state),
tcp.memory (Memory used by TCP sockets), udp.all.count (All open UDP connections), udp.memory (Memory used by UDP sockets)
in.bytes (Number of bytes received), in.dropped (Number of incoming packets that were dropped), in.errors (Number of errors while
receiving), in.packets (Number or packets received), out.bytes (Number of bytes sent), out.dropped (Number of outgoing packets that
were dropped), out.errors (Number of errors while sending), out.packets (Number or packets received), name (network interface name)

free, hugepages.default size, hugepages.free, hugepages.reserved, hugepages.total,

Total

86 KPI/node (1,720 KPI in the Redis Cluster with 20 nodes)

TABLE II
TYPES OF INJECTED FAILURES AND FAILURE PATTERNS

Failure ID

Failure

MemL-Lin
MemL-Exp

MemL-Rnd
PacL-Lin
PacL-Exp

PacL-Rnd
CPUH-Lin
CPUH-Exp
CPUH-Rnd

Memory leak, linear
Memory leak, exponen-
tial
Memory leak, random
Packet Loss, linear
Packet Loss, exponen-
tial
Packet Loss, random
CPU Hog, linear
CPU Hog, exponential
CPU Hog, random

to in-

start
ject
51 minutes
51 minutes

to

inject
crash
187 minutes
34 minutes

51 minutes
16 minutes
16 minutes

16 minutes
19 minutes
19 minutes
19 minutes

40 minutes
73 minutes
14 minutes

75 minutes
97 minutes
15 minutes
41 minutes

(cid:104)failure type, master-slave node pair(cid:105) to indicate the type of
the injected failure and the pair of nodes at which the failures
were injected, respectively.

that

We split the failing execution data in two subsets, and
trained PREMISE separately on either set to predict the failures
in the other set, to evaluate the effectiveness of PREMISE on
failures independent from training. We built a ﬁrst PREMISE
model (PREMISE1) by training PREMISE with a balanced
dataset
includes both normal execution data and the
failing execution data corresponding to the failures in the
ﬁrst ﬁve rows of Table II: Memory leak failures with all
injection patterns and packet loss failures with linear and
exponential injection patterns. We built a second PREMISE
model (PREMISE2) by training PREMISE with a balanced
dataset that includes both normal execution data and the failing

execution data related to the failures in the last four rows of
Table II: Packet loss failures with random-increase injection
pattern, and CPU hog failures with all injection patterns. That
is, PREMISE1 and PREMISE2 rely on models trained with two
disjoint sets of failures and injection patterns corresponding to
the top and bottom half of Table II, respectively.

We studied the false alarms and the prediction ability of
PREMISE by experimenting with both models, PREMISE1
and PREMISE2, by using PREMISE1 to predict the failures
for the latter set of failing execution data, and vice-versa
PREMISE2 to predict the failures for the former set of failing
execution data.

We compared PREVENT with LOUD, by considering the
same embodiment of LOUD that we exploit in PREVENT,
that is, the standalone Anomaly ranker component trained as
described above.

LOUD (the standalone Anomaly ranker) ranks anomalous
KPIs for all execution states, regardless of any prediction, and
thus may suffer from many false alarms on normal execution
data. For a fair comparison, we ﬁltered out false alarms by
signaling failure predictions only when the same pair of nodes
persists in the top three nodes that LOUD ranks as anomalous
for N consecutive timestamps. This follows from the intuition
that anomalies increase in the presence of failures, and thus
the failing nodes should likely persist in the top ranked nodes
if the failing conditions keep occurring. We computed the false
alarm rate of LOUD for N = 3, 4, 5, after observing a huge
amount of false alarms for values of N less than 3 and an
unacceptable delay in signaling anomalous states for values

8

TABLE III
FALSE ALARM RATE ON THE NORMAL EXECUTION DATA

False alarm rate

Approach
PREVENTE 18%
PREVENTA 31%
LOUDN 3
73%
LOUDN 4
63%
LOUDN 5
50%

of N greater than 5.

C. Results

We estimated the false alarm rate of each approach,
by executing PREVENTE, PREVENTA, and LOUD (with
N = 3, 4, 5) with the normal execution data that we
collected during the weeks of normal execution of the Redis
Cluster. Table III reports the results. The false alarm rate
quantiﬁes the annoyance of operators receiving useless alerts.
A high false alarm rate during normal execution vanishes the
usefulness of true predictions before failures.

The data reported in Table III indicate that PREVENTA
and PREVENTE dramatically reduce the false alarm rate from
unacceptable values between 50% and 73%, when using LOUD
to 31% and 18%, respectively, by combining
stand-alone,
LOUD with failure predictors, as in PREVENT.

Figures 6, 7 and 8 visualize the results of the experiments
with PREVENTE, PREVENTA, and PREMISE for each failure
type, CPU hog, packet loss, and memory leak. Each ﬁgure
presents the results of the experiments for the three failure
patterns (random, exponential, and linear). The ﬁgures report
the median results out of ten executions of the PREMISE
model (either PREMISE1 or PREMISE2) for which we did
not consider that speciﬁc failure type during training.

The legend at the bottom of the ﬁgures indicates how to
read the results. The colors identify the approaches: grey
for PREMISE, the approach we compare with, yellow for
PREVENTE and blue for PREVENTA, the two approaches that
we propose in this paper. The red dots mark the timestamps
at which we started injecting the failure.

Vertical bars and square dots classify the strength of the

predictions of the different approaches at each timestamp:

• Toll vertical bars indicate true-positive predictions with
complete information on the failing nodes, that is, both
failing nodes are localized: They are ranked in the top
three nodes in the case of PREVENT;

• Mid vertical bars indicate true-positive predictions with
partial information on the failing nodes, that is, at least
a failing node is localized:

• Square dots indicate false alarms, which are either wrong
predictions in the absence of a failure, when they occur
before the failure injection, or failure predictions for
which no failing node was localized, when they occur
after the failure injection;

• No bars or dots after the failure injection indicate false-
negatives, that is, absence of prediction in the presence
of an upcoming failure;

Fig. 6. Failure predictions for CPU hog failures

• No bars or dots before the failure injection indicate true-
negatives, that is, no predictions in the absence of a
failure.

The x-axis indicates the timeline of each experiment, re-
ported as minutes before the ﬁrst observable failure of the
cluster caused by the injected failure (system crash). The grey
shadowed areas indicate the duration of the experiments, an
initial lag to stabilize the system (the area before the red dot),
a fault injection (the red dot), and the failing execution up to
an observable system failure (the grey area after the red dot).
The intervals depend on the failure type and injection pattern.
The timelines are aligned to the right, the time of the system
crash, to visualize the comparative advantages and limitations
of the approaches: The earlier vertical bars occur between the
fault injection (red dot) and the failure (rightmost sample),
the better the approach predicts the failure. The visualized
time interval is the same within each ﬁgure, and differs across
ﬁgures, since different failure types cause failures with largely
different time horizons.

RQ1: Effectiveness

RQ1 focuses on PREVENT (yellow and blue diagrams in
Figures 6, 7 and 8). We comparatively evaluate the effective-
ness of PREVENTE and PREVENTA to predict failures in terms
of false alarm rate, prediction earliness and true positive rate
of predictions that we deﬁne as:

9

-100-80-60-40-200minPREVENTEApproachesTrue-positiveComplete informationTrue-positivePartial informationFalseAlarmEvent typeFault injectionPREVENTAPREMISEFig. 7. Failure predictions for packet loss failures

Fig. 8. Failure predictions for memory leak failures

• False alarm rate: Portion of false alarms on non-failing
data, that is, false alarms that occur either when experi-
menting with normal execution data (Table III), or before
the ﬁrst failure injection timestamp (square dots before
the red dots in the ﬁgures), when experimenting with
failing execution data;

• Prediction earliness: Amount of timestamps (minutes)
between the ﬁrst true positive (ﬁrst toll/mid bar after the
red dot in the ﬁgures) and the system crash (rightmost
sample in the ﬁgures);

• True positive rate of predictions: Portion of true positives
out of the timestamps after the ﬁrst true positive (portion
of tall/mid bars between the ﬁrst tall/mid bar after injec-
tion and system crash).

The false alarm rate quantiﬁes the annoyance of operators
receiving useless alerts. The prediction earliness quantiﬁes the
time offered to operators to adopt countermeasures before the
system crash, but even late predictions may still be useful to
help operators quickly locate the failure after crash, even if
not early enough to prevent the crash. The true positive rate
after the ﬁrst prediction indicates the stability of the prediction.
Predictions with high stability can mitigate the impact of false
alarms: A signiﬁcant difference between true positive rate after
the ﬁrst prediction and false alarm rate before the beginning

of failure injections indicates the ability of discriminating
between normal and failing behaviors.

The diagrams in the ﬁgures show a high variability of the
time intervals for useful predictions, that is, the time between
the ﬁrst injection of the failure (red dots in the diagrams) and
the system crash (time zero at the far right of the diagrams):
This time intervals depend on how fast failures with different
injection patterns cause a system crash after starting to inject
the failures. For all failure types,
the exponential-increase
pattern causes the crash faster than the other failure patterns,
as expected. In detail, CPU hog failures (Figure 6) cause the
crash within a minimum time interval of 15 minutes after the
injection and a maximum of 97 minutes after the injection,
packet loss failures (Figure 7) cause the crash between 14 and
75 minutes after the injection, memory leak failures (Figure 8)
between 34 and 187 minutes after the injection.

The yellow and blue diagrams in ﬁgures 6, 7 and 8 indicate
that PREVENT suffers from few false alarms before injection
(square yellow or blue dots before red dots). The false alarm
rates during a week of normal execution reported in Table III
conﬁrm acceptable low false alarm rates for both PREVENTE
and PREVENTA in normal execution conditions, 18% and
31%, respectively.

10

-80-60-40-200minPREVENTEApproachesTrue-positiveComplete informationTrue-positivePartial informationFalseAlarmEvent typeFault injectionPREVENTAPREMISE-200-150-100-500minPREVENTEApproachesTrue-positiveComplete informationTrue-positivePartial informationFalseAlarmEvent typeFault injectionPREVENTAPREMISETABLE IV
PREDICTION EARLINESS FOR PREVENTE

TABLE V
PREDICTION EARLINESS FOR PREVENTA

Failure

Pattern

CPU hog
CPU hog
CPU hog
pckt loss
pckt loss
pckt loss
mem leak
mem leak
mem leak

rnd
exp
lin
rnd
exp
lin
rnd
exp
lin

TPR

reaction

earliness

injection
(minutes before failure, N/A = Not Applicable)
27
41
4
15
55
97
9
75
0
14
31
73
0
40
0
34
186
187

100
100
96
100
N/A
87
N/A
N/A
47

14
11
42
66
-
42
-
-

1

Failure

Pattern

CPU hog
CPU hog
CPU hog
pckt loss
pckt loss
pckt loss
mem leak
mem leak
mem leak

rnd
exp
lin
rnd
exp
lin
rnd
exp
lin

TPR

reaction

earliness

injection
(minutes before failure, N/A = Not Applicable)
15
41
8
15
80
97
1
75
2
14
26
73
0
40
0
34
150
187

80
100
97
100
50
76
N/A
N/A
48

26
7
17
74
12
47
-
-
37

The prediction earliness varies across failure types and
injection patterns. It is very good with both PREVENTE and
PREVENTA for CPU hogs for all injection patterns (Figure 6).
Both approaches start to predict the failure (ﬁrst true positive)
in the ﬁrst half of the interval between the injection and
the crash in four out of six cases. They start
to predict
failures at least four minutes before the crash in the worst
case of PREVENTE, for failures with exponential injection.
The prediction earliness is good for packet loss injected with
linear patterns, decreases for random patterns (PREVENTE
loss failures nine minutes earlier than
predicts the packet
the crash, and PREVENTA only a minute earlier), and is not
good for failures injected with exponential pattern, where only
PREVENTA predicts the failure and only two minutes before
the crash (Figure 7). Both PREVENTE and PREVENTA predict
memory leak failures before its occurrence only for the linear
injection pattern (Figure 8).

The true positive rate of predictions is very high in the 13
cases in which either PREVENTE or PREVENTA predicts a
failure (from 62% to 100%).

Tables IV and V report in detail the injection, reaction
and earliness intervals, as well as the true positive rates of
predictions (TPR) per failure type and failure pattern, for
PREVENTE and PREVENTA, respectively:

• Injection: Time (in minutes) between the failure injection
and the system crash, that is, the time interval for useful
prediction before the system crash;

• Reaction: Time (in minutes) between the injection and
the prediction, that is, the delay between the beginning
of the injection and the ﬁrst true prediction;

• earliness: Time (in minutes) between the ﬁrst true pre-
diction and the system crash, that is, the time offered
to operators to adopt countermeasures before the system
crash once predicting the failure;

• TPR: Percentage of true predictions between the ﬁrst true

prediction after the injection and the system crash.

The data in the tables indicate that PREVENT (either
PREVENTE or PREVENTA) can predict failures at least 4
minutes before the system crash in 11 out of the 13 cases
in which either PREVENTE or PREVENTA predict a failure,
while PREVENTE or PREVENTA cannot predict any failure in
3 and 2 cases, respectively.

In summary, our experiments indicate that both PREVENTE
and PREVENTA can predict failures early and reliably in many
cases. They also indicate a good complementarity between
PREVENTE and PREVENTA: the tables indicate that the joint
PREVENTE and PREVENTA (union of true positives) succeeds
in 7 out of 9 failure cases, yielding a prediction as early as 8
minutes or more in 6 cases. With reference to the experiments
with normal execution data, the intersection of false alarms of
PREVENTE and PREVENTA is at most 18%.

RQ2: Comparative Evaluation

RQ2 focuses on advantages and limitations of the un-
supervised PREVENT approach with respect to state-of-the-
art (supervised) approaches. Unsupervised approaches do not
require training with data collected during failing executions,
thus they can be used in the many industrially relevant cases
where it is not possible to seed failures during operations, as
required for supervised approaches.

We experimentally compare PREVENT with PREMISE, a
supervised state-of-the-art approach, to understand the impact
on the lack of training with data from failing execution on
earliness and true positive rate. Figures 6, 7 and 8 visualize
the results of executing PREMISE (grey diagrams) on the
same data we use in the experiments with PREVENTE (yellow
diagrams) and PREVENTA (blue diagrams).

Both PREVENTE and PREVENTA predict failures earlier
and with higher true positive rate than PREMISE in the
presence of CPU hogs with exponential and random injec-
tion patterns, PREMISE predicts earlier that PREVENT with
linear injection pattern, but with a much lower true positive
rate and higher false alarm rate than PREVENT: over 80%
for PREMISE, none for PREVENT. PREVENT outperforms
PREMISE also for packet losses and memory leaks with linear
injection pattern. PREMISE performs better than PREVENT
for these failures with random injection pattern. None of the
approaches predicts packet loss and memory leaks failures
with exponential injection pattern.

Figure 9 summarizes the effectiveness of the three ap-
proaches by plotting the true positive rates (for the period after
the ﬁrst true-positive prediction) versus the false alarm rate (for
the period before failure injection). Effective approaches sit in
the top leftmost corner that hosts approaches with high true
positive rate and low false positive rate. The ﬁgure reports

11

TABLE VI
PREDICTION EARLINESS FOR PREMISE

Failure

Pattern

CPU hog
CPU hog
CPU hog
pckt loss
pckt loss
pckt loss
mem leak
mem leak
mem leak

rnd
exp
lin
rnd
exp
lin
rnd
exp
lin

TPR

reaction

earliness

Injection
(minutes before failure, N/A = Not Applicable)
0
41
0
15
96
97
41
75
0
14
0
73
11
40
0
34
186
187

N/A
N/A
1
34
N/A
N/A
29
N/A
1

N/A
N/A
6
82
N/A
N/A
72
N/A
7

nine values for each approach, marked as large blue spots
for PREVENTA, mid yellow spots for PREVENTE, and small
grey spots for PREMISE. Some values overlaps, as indicated
in the ﬁgure:
three experiments with PREMISE and two
with PREVENTE result in FPR=0, TPR=0; three experiments
with PREVENTE and two with PREVENTA result in FPR=0,
TPR=100. The diagram conﬁrms the better effectiveness of
PREVENT than PREMISE: Most spots for PREVENTA (blue
spots) and PREVENTE (yellow spots) sit in the top left corner
of the plot (ﬁve spots for both approaches), while only one
out of nine spots for PREMISE (grey spots) sits in the top
left corner. Thus, the experiments conﬁrm that the original
combination of a state classiﬁer with an anomaly ranker in
both PREVENTA and PREVENTE is more effective than the
PREMISE supervised learning approach, in terms of both true
and false positive rate.

Fig. 9. False alarm rate / True positive rate of predictions

pattern, packet loss with random injection pattern, and memory
leaks with random injection pattern.

RQ3: state classiﬁer and anomaly ranker interplay

RQ3 focuses on the effectiveness of the original combi-
nation of an anomaly ranker with a state classiﬁer in both
PREVENTA and PREVENTE. We compare PREVENTA and
PREVENTE with LOUD, the state-of-the-art anomaly ranker
that inspired our anomaly ranker.

The data of Table III conﬁrm that the PREVENT original
combination of an anomaly ranker with a state classiﬁer
largely improves the effectiveness of an anomaly ranker exe-
cuted without a state classiﬁer. The false alarm rate of LOUD
exceeds 50% in all the conﬁgurations reported in Table III,
while PREVENT presents a false alarm rate between 18%
(PREVENTE) and 31% (PREVENTA) in the absence of failures,
with a substantial reduction of false alarms.

Threats to validity

We evaluated PREVENT by executing a prototype imple-
mentation on a large set of data collected on a complex
cluster. We carefully implemented the approach and carefully
designed our experiments to avoid biases. We understand that
the limited experimental framework may threaten the validity
of the results, and we would like to conclude this section by
discussing how we mitigated the main threats to the validity
of the experiments that we identiﬁed in our work.

Threats to the external validity

Possible threats to the external validity of the experiments
derive from the experimental evidence being available for
a single system and nine combinations of failure types and
injection patterns, which in turn depend on the effort required
to set up a proper experimental environment and collect data
for validating the approach. The main threats derive from
the experimental setting, the set of failure, and the injection
patterns.

Experimental setting
We experimented with dataset collected from a Redis Cluster
that we implemented on our private cloud to overcome the
difﬁculty of collecting data on large scale cloud systems, and
the results may not generalize to other systems, We mitigated
the threat that derive from experimenting with a single system
by collecting data from a standard installation of a popular
cluster that is widely used in commercial environments, and
by collecting a large set of data from several weeks of
experiments. The data we used in the experiments are available
at Prevent Replication Package3 for replicating the
results and for comparative studies with new systems.

Table VI reports injection, reaction, and earliness intervals
as well as TPR for the experiments with PREMISE. Ta-
bles IV, Vand VI conﬁrm that PREVENT performs better than
PREMISE in six out of nine case, with PREMISE performing
better than PREVENT for CPU hogs with linear injection

Set of failures
We experimented with three failure types, and the results
may not generalize to other failure types. We limited the

3Replication Package URL: https://github.com/usi-star/prevent.git

12

2 points of Prevent_E at  {0, 0}3 points of Premise at  {0, 0}2 points of Prevent_A at  {0, 100}3 points of Prevent_E at  {0, 100}PREVENT_APREVENT_EPREMISEexperiments to three failure types due to the large effort
required to properly install and tune failure injectors and
collect valid data sets for each failure. We mitigated this threat
to the validity of our results by choosing types of failures
which are both common and very relevant in complex cloud
systems.

Injection patterns
We experimented with three injection patterns and the results
may not be generalized to other injection patterns. We mit-
igated this threat by simulating mild (linear), bad (random),
and aggressive (exponential) failure patterns to investigate the
approach in different operative conditions.

Threats to the internal validity

Possible threats to the internal validity of the experiments
derive from the prototype implementation of the approaches
and the trafﬁc proﬁle used in the experiments.

Prototype implementation
We carried on the experiments on an in-house prototype
implementation of PREVENT, PREMISE and LOUD. We mit-
igated the threats to the validity of the experiments that may
derive from a faulty implementation by carefully designing and
testing our implementation, by (i) comparing the data obtained
with our PREMISE and LOUD implementations with the data
available in the literature, (ii) repeating the experiments ten
times, and (iii) making the prototype implementation avail-
able at Prevent Replication Package, to allow for
replicating the results.

Trafﬁc proﬁle
We mitigated the threats to the validity of the results that
may derive from the trafﬁc proﬁle that we use to collect the
experimental data, by experimenting with a seasonal trafﬁc
proﬁle excerpted from industrial collaborations, and that we
compared with analogous trafﬁc proﬁles publicly available in
the literature. We carried on the experiments by collecting data
over weeks of continuous executions with proﬁles that corre-
spond to trafﬁc commonly observed in commercial settings.

IV. RELATED WORK

Salfner et al.’s survey identiﬁes online failure prediction
as the ﬁrst step to proactively manage faults in production,
followed by diagnosis, action scheduling and execution of
corrective actions, and deﬁnes failure symptoms as the out-
of-norm behavior of some system parameters before the oc-
currence of a failure, due to side effects of the faults that
are causing the failure [43]. In this section we focus on
the failure prediction and diagnosis steps of Salfner et al.’s
fault management process,
the steps related to PREVENT.
We refer the interested readers to Colman-Meixner et al.’s
comprehensive survey for a discussion of mechanisms for
scheduling and executing corrective actions to tolerate or
recover from failures [11].

Detecting failure symptoms, ofter referred to as anomalies,
is the most common approach to predict failures online. The
problem of detecting anomalies has been studied in many
application domains, to reveal intrusions in computer systems,
frauds in commercial organizations, abnormal patient condi-
tions in medical systems, damages in industrial equipments,
abnormal elements in images and texts, abnormal events
in sensor networks, failing conditions in complex software
systems [9]. Approaches to detect anomalies heavily depend
on the characteristics of the application domain. In this sec-
tion, we discuss approaches to detect anomalies in complex
software systems, to predict failures in production.

A distinctive characteristics of approaches for detecting
anomalies is the model that the approaches use to interpret the
data monitored in production, models that are either manually
derived by software analysts in the form of rules that the
system shall satisfy [10] or automatically inferred from the
monitored data. Approaches that automatically infer models
from monitored data work in either supervised or unsupervised
fashion, and do or do not require labeled data to synthesize
the models, respectively.

Rule-based approaches leverage analysts’ knowledge about
the symptoms that characterize failures, and rely on rules
manually deﬁned for each application and context. Super-
vised approaches that are also referred to as signature-based
approaches [23] build the models by relying on previously
observed anomalies, and offer limited support for detecting
anomalies that were not previously observed. Unsupervised
approaches derive models without requiring labeled data, thus
better balancing accuracy and required information. Some
approaches focus on failure predictions only, yet others ap-
proaches support both prediction and diagnosis, thus address-
ing the ﬁrst
two steps of Salfner et al.’s proactive fault
management process.

The PREVENT approach that we propose in this paper is
an unsupervised approach that both predicts and diagnoses
failures, by detecting anomalies in the KPI values monitored
on distributed enterprise applications. The PREVENT State
classiﬁer predicts upcoming failures as it observes system
states with signiﬁcant sets of anomalous KPIs. The PREVENT
Anomaly ranker diagnoses the components that correspond
to the largest sets of representative anomalies as the likely
originators of the failures. In the remainder of this section
we review the relevant automatic approaches to predict and
diagnose faults in complex software systems.

Failure Prediction

The failure prediction approaches reported in the literature
predict failures in either datacenter hosts that serve cloud-
based applications and services or distributed applications that
span multiple hosts.

Approaches that predict failures in datacenter hosts monitor
metrics about resource consumption at host levels, like CPU,
disk, network and memory consumption,
to infer whether
a host
is likely to incur some failure in the near future,
and proactively migrate applications that run on likely-failing

13

hosts. Tehrani and Saﬁ exploit a support-vector-machine model
against (discretized) data on CPU usage, network bandwidth
and available memory of the hosts, to identify hosts that are
about to fail [15]. Both Islam and Manivannan and Gao er al.
investigate the relationship between resource consumption and
task/job failures, to predict failures in cloud applications [19],
[26]. Both David et al. and Sun et al. exploit neural networks
against data on disk and memory usage, to predict hardware
failures, isolate the at-risk hardware, and backup the data [12],
[48]. Approaches that predict failures in distributed applica-
tions, like PREVENT, address the complex challenge of sets
of anomalies that span multiple physical and virtual hosts and
devices.

Signature-based approaches leverage the information of
observed anomalies with supervised learning models, to cap-
ture the behavior of the monitored system when affected by
speciﬁc failures. Signature-based approaches are very popular
and support a large variety of approaches. Among the most
representative approaches, both Seers [40] and Malik et al.’s
approaches [32] label runtime performance data as related
to either passing or failing executions, and train classiﬁers
to identify incoming failures by processing runtime data in
production, Sauvanaud et al. classify service level agreement
violations based on data collected at the architectural tier
[45], The SunCat approach models and predicts performance
problems in smartphone applications [39].

Signature-based approaches suffer from two main limita-
tions: (i) They require labeled failure data for training, which
may not be easy to collect, and (ii) foster predictions that
overﬁt the failures available in the training set, without well
capturing other failures and failure patterns.

Both semi-supervised and unsupervised approaches learn
models from data observed during normal (non-failing) ex-
ecutions, and identify anomalies when the data observed in
production deviate from the normal behaviors captured in the
models, thus discharging the burden of labelling the training
data.

Most approaches that claim a semi-supervised nature com-
bine semi-supervised learning with signature-based models
to yield their ﬁnal prediction verdicts [18] [50] [49] [22].
As representative examples, Mariani et al.’s PREMISE ap-
proach uses a semi-supervised approach on monitored KPIs
(IBM SCAPI [25]) to identify anomalous KPIs, and builds
a signature-based model to identify failure-prone anomalies,
by capturing the relation between failures observed in the
past and the sets of anomalous KPIs. Fulp et al.’s approach
builds failure prediction models with a support vector machine
(a signature-based approach) based on features distilled in
[18]. Tan et
semi-supervised fashion from system log ﬁles
al.’s PREPARE approach predicts anomalies with a Bayesian
networks trained in semi-supervised fashion, and feeds the
anomalies to a 2-dependent Markov model (a signature based
model) to predict failures [50]. Tan et al.’s ALERT approach
refers to unsupervised clustering to map the failure data to
distinct failure contexts, aiming to predict failures by using
a distinct decision tree (a signature-based approach) for each

failure context [49].

These approaches are in essence signature-based approach
themselves, although they preprocess the input data in semi-
supervised fashion. They may successfully increase the preci-
sion of the predictions, by training the signature-based models
on pre-classiﬁed anomalies rather than plain runtime data,
but share the same limitations of signature-based models
of requiring labeled failure data for training, and fostering
predictions that overﬁt the failures of the training set. Indeed,
in our experiments, the PREMISE approach was ineffective
against failures and failure patterns that did not correspond to
the signatures considers during training.

Guan et al.’s approach [22] combines supervised and un-
supervised learning in a different way. They exploit Bayesian
networks to automatically label the anomalous behaviors that
they feed to supervised learning models for training, thus
potentially relieving the burden of labelling the data.

Only few approaches work in unsupervised fashion like
PREVENT and EMBED [38] the energy-based failure predic-
tion approach that PREVENT exploits as state classiﬁer and
that we have already described in Section II-A. Fernandes and
al.’s approach [16] matches the actual trafﬁc again models
of the normal trafﬁc built with either principal component
analysis or the ant colony optimisation metaheuristic to detect
anomalous trafﬁc in computer networks. Ibidunmoye et al.’s
unsupervised approach [24] estimates statistical and temporal
properties of KPIs during normal executions, and detects devi-
ations by means of adaptive control charts. Both Bontemps et
al.’s [6] and Du et al.’s [14] approaches detect anomalies with
long short-term memory recurrent neural networks trained
on normal execution data. Ahmad et al.’s approach [3] uses
hierarchical temporal memory networks to detect anomalies in
streaming data time series.

Recently, Roumani and Nwankpa [42] proposed a radically
different approach that extrapolates the trend of occurrence
of past incidents (extracted from historical data) to predict
when other incidents will likely occur again in the future,
without requiring data monitored in production. The approach
is promising, but there is no deep evidence of the effectiveness
and generality of this approach so far.

Failure Diagnosis

Failure Diagnosis is the process of identifying the appli-
cation components responsible for predicted failures. Most
approaches proposed in the literature target performance bot-
tlenecks in enterprise applications, and include kowledge-
driven, depedency-driven, observational and signature-based
approaches [23].

Knowledge-driven approaches rely on knowledge that ana-
lysts manually extract from historical records, and encode in
rules that the inference engine processes to detect performance
issues and identify the root-cause component. As represen-
tative example, Chung et al.’s approach [10], is designed to
work at both development- and maintenance-time, to provide
testing-as-a-service, and assumes that analysts frequently up-
date the underlying rules when observing new issues.

14

Dependency-driven approaches analyze the communication
ﬂow among components, measure the frequency the ﬂows,
and perform causal path analysis to detect anomalies and
their causes. As a representative example, Sambasivan et al.’s
Spectroscope [44] approach assists developers to diagnose the
causes of performance degrades after a software change. It
identiﬁes signiﬁcant variations in response time or execution
frequency, by comparing request ﬂows between two corre-
sponding executions, observed before and after the change.

Observational approaches directly analyze the distribution
of the proﬁled data to explain which application component
correlates the most with a given system-level failure. As a
representative example, Magalhaes and Silva’s approach [29]
identiﬁes performance anomalies of Web applications, by
analyzing the correlation between the workload and response
time of the transactions: A response time variation that does
not correlate with a workload variation is pinpointed as a
performance anomaly. Then, they analyze the proﬁled data of
each component with ANOVA (analysis of variance) to spot
which data (and thus which components) explain the variance
in the response time [30], [31].

Signature-based strategies use prior knowledge about the
mapping between failures and components to diagnose failures
of previously observed types [27]. As representative examples,
both PREMISE and ALERT that we already mentioned above
consider the failure location as an additional
independent
variable of their signature-based prediction models [34], [49].
All approaches suffer from restrictions that limit their ap-
plicability to distributed enterprise applications. Knowledge-
driven and dependence-driven approaches, like Chung et al.’s
approach [10] and Sambasivan et al.’s Spectroscope [44], are
deﬁned to assist developers in ofﬂine analyses, for instance,
after a software change. Signature-based approaches suffer
from the same limitations of signature-based failure prediction
approaches: They require labeled failure data for training,
and foster predictions that overﬁt the failures available in the
training set, without well capturing other failures and failure
patterns. Magalhaes and Silva’s observational approach [30] is
the closest approach to PREVENT anomaly ranker, since both
approaches identify failing components as the components
related to data strongly connected to anomalies. Magalhaes and
Silva’s approach and PREVENT differ in their technical core:
PREVENT uses Granger-causality centrality of the anomalous
KPIs of the component, while Magalhaes and Silva rely on the
ANOVA analysis. In this respect, PREVENT is more failure-
type-agnostic than Magalhaes and Silva’s approahc, since the
Granger-causality models only causal relations among KPIs,
while the ANOVA analysis refer to some speciﬁc failure
like poor response time in Magalhaes and Silva’s
metric,
approach.

Overall all approaches face challenges in balancing high
detection rates with low false-alarm rates, and the effectiveness
of the different strategies largely depends the system observ-
ability, detection mode (real-time or post-mortem), availability
of labeled data, dynamics of the application workload, and
nature of the collected data and and their underlying context.

V. CONCLUSIONS

Software failures in production are unavoidable [20]. Pre-
dicting failures and locating failing components online are the
ﬁrst steps to proactively manage faults in production [43].
In this paper we discuss advantages and limitations of the
approaches reported in the literature, and propose PREVENT, a
novel approach that overcomes the main limitations of current
approaches.

PREVENT originally combines Restricted Bolzmann Ma-
chine and Deep autoencoder with Granger causality analysis
and PageRank centrality, to effectively predict failures and
locate failing components. By relying on a combination of un-
supervised approaches, PREVENT overcomes a core limitation
of supervised approaches that require seeding failures to gather
labeled training data, activity hardly possible in commercial
systems in production.

The experimental results that we obtained on data collected
by monitoring for several weeks a popular cluster widely used
in commercial environments show that the original combi-
nation of unsupervised techniques in PREVENT outperforms
supervised failure prediction approaches in the majority of
the experiments, and largely reduces the unacceptable false
positive rate of fault localizers used in isolation.

The main contribution of this paper are (i) PREVENT,
an original combination of unsupervised techniques to pre-
dict failures and localize failing resources in distributed en-
terprise applications, (ii) a large set of data that we col-
lected on Redis, a cluster widely used in commercial en-
vironments, data that we offer
in a replication package
Prevent Replication Package, (iii) a thorough com-
parison of the proposed approach with state-of-the-art su-
pervised approaches to predict failures and localize failing
resources that indicates the quality of PREVENT with respect
to the start of the art.

The results presented in this paper indicate the feasibility
of machine-learning-based approaches to predict failures and
locate failing components in commercial environments, and
open the horizon to study the effectiveness of unsupervised
approaches for predicting failure in complex systems.

ACKNOWLEDGEMENT

This work is partially supported by the Swiss SNF project
ASTERIx: Automatic System TEsting of InteRactive software
applications (SNF 200021_178742), and by the Italian PRIN
project SISMA (PRIN 201752ENYB).

REFERENCES

[1] Elasticsearch reference. https://www.elastic.co/elasticsearch/.
[2] Kibana guide. https://www.elastic.co/kibana/.
[3] S. Ahmad, A. Lavin, S. Purdy, and Z. Agha. Unsupervised real-time
anomaly detection for streaming data. Neurocomputing, 262:134–147,
2017.

[4] A. Arnold, Y. Liu, and N. Abe. Temporal causal modeling with graphical
In Proceedings of the ACM SIGKDD International
granger methods.
Conference on Knowledge Discovery and Data Mining, KDD ’07, pages
66–75. ACM, 2007.

15

[5] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen.
Fingerprinting the datacenter: automated classiﬁcation of performance
In Proceedings of the 5th European conference on Computer
crises.
systems, pages 111–124, 2010.

[6] L. Bontemps, J. McDermott, N.-A. Le-Khac, et al. Collective anomaly
detection based on long short-term memory recurrent neural networks.
In International Conference on Future Data and Security engineering,
pages 141–152. Springer, 2016.

[7] M. Á. Carreira-Perpiñán and G. E. Hinton. On contrastive divergence
learning. In Proc. International Workshop on Artiﬁcial Intelligence and
Statistics. The Society for Artiﬁcial Intelligence and Statistics, 2005.

[8] D. Chandler.

Introduction to Modern Statistical Mechanics. Oxford

University Press, September 1987.

[9] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.

ACM Computing Surveys, 41(3):15, 2009.

[10] I.-H. Chung, G. Cong, D. Klepacki, S. Sbaraglia, S. Seelam, and H.-F.
Wen. A framework for automated performance bottleneck detection.
In 2008 IEEE International Symposium on Parallel and Distributed
Processing, pages 1–7. IEEE, 2008.

[11] C. Colman-Meixner, C. Develder, M. Tornatore, and B. Mukherjee. A
survey on resiliency techniques in cloud computing infrastructures and
applications. IEEE Communications Surveys & Tutorials, 18(3):2244–
2281, 2016.

[12] N. A. Davis, A. Rezgui, H. Soliman, S. Manzanares, and M. Coates.
Failuresim: A system for predicting hardware failures in cloud data
In 2017 IEEE 10th International
centers using neural networks.
Conference on Cloud Computing (CLOUD), pages 544–551. IEEE,
2017.

[13] S. N. Dorogovtsev, A. V. Goltsev, and J. F. F. Mendes. Critical
phenomena in complex networks. Reviews of Modern Physics, 80:1275–
1335, 2008.

[14] M. Du, F. Li, G. Zheng, and V. Srikumar. Deeplog: Anomaly detection
and diagnosis from system logs through deep learning. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications
Security, pages 1285–1298, 2017.

[15] A. Fadaei Tehrani and F. Saﬁ-Esfahani. A threshold sensitive failure
prediction method using support vector machine. Multiagent and Grid
Systems, 13(2):97–111, 2017.

[16] G. Fernandes Jr, L. F. Carvalho, J. J. Rodrigues, and M. L. Proença Jr.
Network anomaly detection using ip ﬂows with principal component
analysis and ant colony optimization. Journal of Network and Computer
Applications, 64:1–11, 2016.

[17] A. Fischer and C. Igel. An introduction to restricted boltzmann
machines. In Iberoamerican congress on pattern recognition, pages 14–
36. Springer, 2012.

[18] E. W. Fulp, G. A. Fink, and J. N. Haack. Predicting computer system
failures using support vector machines. In Proceedings of the USENIX
conference on Analysis of system logs, WASL’08, pages 5–5. USENIX
Association, 2008.

[19] J. Gao, H. Wang, and H. Shen. Task failure prediction in cloud data
centers using deep learning. IEEE Transactions on Services Computing,
2020.

[20] L. Gazzola, L. Mariani, F. Pastore, and M. Pezzè. An exploratory study
In Proceedings of the International Symposium on

of ﬁeld failures.
Software Reliability Engineering, ISSRE ’17, 2017.

[21] C. W. J. Granger. Investigating causal relations by econometric models

and cross-spectral methods. Econometrica, 37:424–438, 1969.

[22] Q. Guan, Z. Zhang, and S. Fu. Ensemble of bayesian predictors and
decision trees for proactive failure management in cloud computing
systems. Journal of Communication, 7(1):52–61, 2012.

[23] O. Ibidunmoye, F. Hernández-Rodriguez, and E. Elmroth. Performance
ACM Computing

anomaly detection and bottleneck identiﬁcation.
Surveys, 48(1):4:1–4:35, 2015.

[24] O. Ibidunmoye, A.-R. Rezaie, and E. Elmroth. Adaptive anomaly
detection in performance metric streams. Transactions on Network and
Service Management, 15(1):217–231, 2018.

[25] IBM Corporation.

SmartCloud Analytics - Predictive Insights 1.3

(Tutorial), 2014. Document Revision R2E2.

[26] T. Islam and D. Manivannan. Predicting application failure in cloud: A
machine learning approach. In 2017 IEEE International Conference on
Cognitive Computing (ICCC), pages 24–31. IEEE, 2017.

[27] H. Kang, X. Zhu, and J. L. Wong. Dapa: Diagnosing application
performance anomalies for virtualized infrastructures. In 2nd {USENIX}

Workshop on Hot Topics in Management of Internet, Cloud, and Enter-
prise Networks and Services (Hot-ICE 12), 2012.

[28] A. N. Langville and C. D. Meyer. A survey of eigenvector methods for
web information retrieval. SIAM Review, 47(1):135–161, 2005.
[29] J. P. Magalhaes and L. M. Silva. Detection of performance anomalies in
web-based applications. In 2010 Ninth IEEE International Symposium
on Network Computing and Applications, pages 60–67. IEEE, 2010.

[30] J. P. Magalhaes and L. M. Silva. Adaptive proﬁling for root-cause
analysis of performance anomalies in web-based applications.
In
2011 IEEE 10th International Symposium on Network Computing and
Applications, pages 171–178. IEEE, 2011.

[31] J. P. Magalhaes and L. M. Silva. Root-cause analysis of performance
anomalies in web-based applications. In Proceedings of the 2011 ACM
Symposium on Applied Computing, pages 209–216, 2011.

[32] H. Malik, H. Hemmati, and A. E. Hassan. Automatic detection of
performance deviations in the load testing of Large Scale Systems. In
Proceedings of the International Conference on Software Engineering,
ICSE ’13, pages 1012–1021. IEEE Computer Society, 2013.

[33] L. Mariani, C. Monni, M. Pezzè, O. Riganelli, and R. Xin. Localizing
faults in cloud systems. In Proceedings of the International Conference
on Software Testing, Veriﬁcation and Validation, ICST ’18, pages 262–
273. IEEE Computer Society, 2018.

[34] L. Mariani, M. Pezzè, O. Riganelli, and R. Xin. Predicting failures in
multi-tier distributed systems. Journal of Systems and Software, 161,
2020.

[35] T. Martin, X. Zhang, and M. E. J. Newman. Localization and centrality

in networks. Phys. Rev. E, 90:052808, Nov 2014.

[36] W. McKinney, J. Perktold, and S. Seabold. Time series analysis in

python with statsmodels. pages 96–102, 2011.

[37] C. Monni and M. Pezzè. Energy-based anomaly detection a new
the
perspective for predicting software failures.
41st International Conference on Software Engineering: New Ideas and
Emerging Results, ICSE (NIER) 2019, Montreal, QC, Canada, May 29-
31, 2019, pages 69–72. IEEE / ACM, 2019.

In Proceedings of

[38] C. Monni, M. Pezzè, and G. Prisco. An RBM anomaly detector for
In 12th IEEE Conference on Software Testing, Validation
the cloud.
and Veriﬁcation, ICST 2019, Xi’an, China, April 22-27, 2019, pages
148–159. IEEE, 2019.

[39] A. Nistor and L. Ravindranath. Suncat: Helping developers understand
and predict performance problems in smartphone applications.
In
Proceedings of the International Symposium on Software Testing and
Analysis, ISSTA ’14, pages 282–292. ACM, 2014.

[40] B. Ozcelik and C. Yilmaz. Seer: A Lightweight Online Failure Prediction
IEEE Transactions on Software Engineering, 42(1):26–46,

Approach.
2016.

[41] V. A. Proﬁllidis and G. N. Botzoris. Modeling of transport demand:
Analyzing, calculating, and forecasting transport demand. Elsevier,
2018.

[42] Y. Roumani and J. K. Nwankpa. An empirical study on predicting cloud
International journal of information management, 47:131–

incidents.
139, 2019.

[43] F. Salfner, M. Lenk, and M. Malek. A survey of online failure prediction

methods. ACM Computing Surveys, 42(3):1–42, 2010.

[44] R. R. Sambasivan, A. X. Zheng, M. De Rosa, E. Krevat, S. Whitman,
M. Stroucken, W. Wang, L. Xu, and G. R. Ganger. Diagnosing
performance changes by comparing request ﬂows. In NSDI, volume 5,
pages 1–1, 2011.

[45] C. Sauvanaud, K. Lazri, M. Kaâniche, and K. Kanoun. Anomaly
detection and root cause localization in virtual network functions.
In
Proceedings of the International Symposium on Software Reliability
Engineering, ISSRE ’16, pages 196–206. IEEE Computer Society, 2016.
[46] J. P. Scott and P. J. Carrington. The SAGE Handbook of Social Network

Analysis. Sage Publications Ltd., 2011.

[47] S. Seabold and J. Perktold. Statsmodels: Econometric and statistical
In Proceedings of the 9th Python in Science

modeling with python.
Conference, volume 57, page 61. Austin, TX, 2010.

[48] X. Sun, K. Chakrabarty, R. Huang, Y. Chen, B. Zhao, H. Cao, Y. Han,
X. Liang, and L. Jiang. System-level hardware failure prediction using
deep learning. In 2019 56th ACM/IEEE design automation conference
(DAC), pages 1–6. IEEE, 2019.

[49] Y. Tan, X. Gu, and H. Wang. Adaptive system anomaly prediction for
large-scale hosting infrastructures. In Proceedings of the Symposium on
Principles of Distributed Computing, PODC ’12, pages 173–182. ACM,
2010.

16

[50] Y. Tan, H. Nguyen, Z. Shen, X. Gu, C. Venkatramani, and D. Rajan. Pre-
pare: Predictive performance anomaly prevention for virtualized cloud
systems. In 2012 IEEE 32nd International Conference on Distributed
Computing Systems, pages 285–294. IEEE, 2012.

17

