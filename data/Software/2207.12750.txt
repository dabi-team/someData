2
2
0
2

l
u
J

6
2

]
E
N
.
s
c
[

1
v
0
5
7
2
1
.
7
0
2
2
:
v
i
X
r
a

SPAIC: A Spike-based Artiﬁcial Intelligence Computing
Framework

Chaofei Hong
Zhejiang Lab

Mengwen Yuan
Zhejiang Lab

Mengxiao Zhang
Zhejiang Lab

Xiao Wang
Zhejiang Lab

Chengjun Zhang
Zhejiang Lab

Jiaxin Wang
Zhejiang Lab

Gang Pan
Zhejiang University

Zhaohui Wu
Zhejiang University

Huajin Tang *
Zhejiang University

Abstract

Neuromorphic computing is an emerging research ﬁeld that aims to develop new intelligent systems
by integrating theories and technologies from multi-disciplines such as neuroscience and deep learning.
Currently, there have been various software frameworks developed for the related ﬁelds, but there is a lack
of an efﬁcient framework dedicated for spike-based computing models and algorithms. In this work, we
present a Python based spiking neural network (SNN) simulation and training framework, aka SPAIC that
aims to support brain-inspired model and algorithm researches integrated with features from both deep
learning and neuroscience. To integrate different methodologies from the two overwhelming disciplines,
and balance between ﬂexibility and efﬁciency, SPAIC is designed with neuroscience-style frontend and
deep learning backend structure. We provide a wide range of examples including neural circuits simula-
tion, deep SNN learning and neuromorphic applications, demonstrating the concise coding style and wide
usability of our framework. The SPAIC is a dedicated spike-based artiﬁcial intelligence computing plat-
form, which will signiﬁcantly facilitate the design, prototype and validation of new models, theories and
applications. Being user-friendly, ﬂexible and high-performance, it will help accelerate the rapid growth
and wide applicability of neuromorphic computing research.

1 Introduction

Recent advances from multiple research ﬁelds bring us closer to understanding and mimicking the brain.
In neuroscience, accumulating models and theories at different scales are developed to explore and under-
stand the neural systems. In another approach, the ﬁeld of deep learning, which aims to replicate brain
functions with simpliﬁed models, has made tremendous progress in recent years[YYG15]. Even though
those two ﬁelds are started with a similar goal, they have revealed quite different answers for the same
question. In this circumstance, the idea of integrating neural mechanisms found in neuroscience with ma-
chine learning techniques to develop brain-inspired computing systems with higher intelligent functions or
better computational efﬁciency has attracted growing attentions [Abb08; PF13; MWK16]. Those researches
are growing into a new multi-discipline ﬁled, often called brain-inspired computing or neuromorphic com-
puting.

The efﬁcient and easily accessible software tools have largely accelerated the development of deep learn-
ing [ABC+16; PGM+19]. Similarly, the development of computational neuroscience have also accompanied
with a series of specialized simulation tools. However, current software tools in both deep learning and
neuroscience ﬁeld are not designed for this new interdisciplinary research.

*Corresponding author: Huajin Tang. (e-mail:htang@zju.edu.cn)

1

 
 
 
 
 
 
As those two ﬁelds have developed separate ways of modeling and studying neural models, the lack of
a generally compatible training and simulation framework makes it harder for researchers to integrate theo-
ries from multiple disciplines and quickly validate new ideas. To meet this demand, we present the SPAIC
(Spike-based Artiﬁcial Intelligence Computing), a Python based spiking neural network (SNN) training
and simulation framework, which blend the programing style and techniques from both deep learning and
neuroscience, and provide a platform to easily test brain-inspired mechanisms and theories in learnable
neuromorphic models [SKP+22].

SNN is a common tool in computational neuroscience for modeling neural systems in a bottom-up ap-
proach. Compared to artiﬁcial neural networks (ANNs), SNNs captured key computational properties of
the biological neural system with dynamical computation models. Researchers have used SNNs to study
mechanisms underlying various neural dynamical behaviors, explore the functional role of different neu-
ronal arrangements and connectivity patterns, and simulate the brain activities in high level of realisticity
[FM19]. With improving understanding of the brain, it is expected that such biologically realistic model
can come closer to achieving brain’s remarkable intelligence abilities than more abstract models. Several
SNN simulation tools such as Neuron [CH06], Brian2 [SBG19] and Nest [EHM+09] have been developed,
which can help researchers construct network models with customized neural dynamics and simulate neu-
ral activities with high precision. However, those software tools are not optimized for building complex
intelligent models, especially those based on big data and training algorithms, and their computational ef-
ﬁciency is lower than current deep learning frameworks. One key objective of our framework is to support
building biologically realistic SNN models with high ﬂexibility and efﬁciency. To achieve this goal, the
SPAIC framework is designed with a frontend-backend architecture which decouples neural model cre-
ation and simulation. In the frontend, the SPAIC provides user-friendly interfaces which can hierarchically
compose complex network structures with neuron assemblies and various connection policies, and ﬂexibly
deﬁne neural dynamics with customizable neuron models, synapses and learning rules. In the backend,
frontend network models are built into an optimized computation graph, and run by a simulator engine
based on popular deep learning frameworks, such as PyTorch [PGM+19] and TensorFlow [ABC+16],
which provide efﬁcient CPU/GPU parallel computing and autograd techniques.

In another approach, cognitive neuroscience decomposes complex cognitive processes into elementary
computational components with associated brain regions and brain activities, and gives a top-down view of
the brain’s functional organization [Gaz09]. The top-down understanding of the brain provides an essential
guidance for building brain-inspired models. Recent trends witness the adoption of mathematical modeling
in cognitive neuroscience, which bridges the gap between experimental research and computational work
[KD18; PLT17]. Those researches often focused on brain states and information transfer in larger scales
with higher-level models. Hence, our framework also aims to support higher-level models such as ANNs
and mean-ﬁeld neural models, and those different level of models can be built separately or merged into
one hybrid network model. In this way, researchers can easily build and test their ideas with more abstract
models, and then those results can be used as a guidance for detailed SNN modeling.

Given the computation components and functional organization of neural systems, a biologically rea-
sonable brain-inspired network model can be constructed. However, it is difﬁcult to manually design the
high-dimensional parameters of the model to achieve complex brain functions. Building a functional brain-
inspired AI model still needs optimization technologies. In recent years, advance in deep learning provides
techniques and theories for optimizing complex network structures. Another key objective of our frame-
work is to naturally blend the understanding and techniques in the ﬁelds of deep learning, cognitive neu-
roscience and computational neuroscience into one uniﬁed modeling procedure. However, incorporating
deep learning methodologies, such as gradient descent, into SNN models remains challenging, both in the-
ory and software engineering. Gradient propagation is one of the fundamental techniques in deep learning,
which is usually incompatible with the complex neural dynamics necessary for brain functions. Moreover,
biological neural systems exhibit much more diverse learning mechanisms, such as timing-based or rate-
based local learning rules [CD08], dopamine driven learning [Wis04], and wiring/rewiring mechanisms
[CMS04] which are not in line with the gradient descent optimization methodology. Although numer-
ous works have demonstrated that SNNs can be efﬁciently trained using surrogate gradient algorithms
[NMZ19], the inclusion of more complex structure and neural dynamics to achieve superior performance
remains a major obstacle. Moreover, biological neural systems have many distinct features such as sparse
connectivity, spatial structure, nonlinear neuronal and synaptic dynamics, spontaneous activity and infor-

2

mation transmission delays, which could be essential for neural computation. The community is calling for
new learning rules that combine the learning efﬁciency of deep learning with biological features of neural
networks. To help researchers in this endeavor, the SPAIC designed a learner class that deﬁnes learning
algorithms in a general training framework, which support both gradient-based learning and local (plas-
ticity based) learning rules. And learner class provides interfaces to ﬂexibly modify any given part of SNN
computation, such as computation in connection, neuron model, and synapses. Some classical learning
algorithms have been implemented in the framework, while it is encouraged that users can develop their
own algorithms through this framework.

The SPAIC framework is an open-source project that is under intensive development, this paper is based

on the primary release version. The source code of the framework is available at https://github.com/ZhejianglabNCRC/SPAIC,
the documentation of the framework can be found at https://spaic.readthedocs.io, and the source code for
all the examples in this article has been deposited at https://github.com/ZhejianglabNCRC/SPAIC_paper_examples.
The paper is structured as follows: In Section 2, we discussed existing software tools related to neural net-
work modeling in the ﬁelds of deep learning, neuroscience and neuromorphic computing, and discussed
why we need a new framework. In Section 3, the framework structure and usage procedure is described in
detail, where the motivation and functionality of each software component are explained. Example codes
and case studies are given in Section 4 to demonstrate the coding style and potential usage situations of the
SPAIC. Then, future developments of SPAIC are discussed in Section 5.

2 Related Work

There are a number of software tools can be used for modeling neural systems. Each is tailored toward
speciﬁc application domains. According to the characteristics of these software tools, they can be roughly
divided into three categories: deep learning, computational neuroscience and neuromorphic computing
frameworks. The computational neuroscience frameworks focus on simulating the details of physiological
structure and neurons while deep learning frameworks focus on achieving brain functions using simpliﬁed
models and learning algorithms. Neuromorphic computing frameworks need to combine the above two
types of frameworks for constructing model with brain-like functional and physiological details. In this sec-
tion, we describe the relevant software tools and the challenges. Some popular frameworks are compared
in Table 1.

2.1 Deep Learning Frameworks

In recent years, a number of deep learning frameworks have emerged, such as Caffe [JSD+14], Theano
[AAA+16], PyTorch [PGM+19], TensorFlow [ABC+16], etc. They focus on the development, training
and inference of deep learning networks. Among them, PyTorch and TensorFlow are the two most pop-
ular open source libraries for machine learning to date. For TensorFlow, the ﬁrst version (TensorFlow
1.x) only supports static computation graphs whereas TensorFlow 2.0 supports dynamic models and
ease the process of building machine learning frameworks.
It is well suited for large-scale distributed
training and can run on CPUs, GPUs, or large-scale distributed systems. PyTorch provides a Pythonic
programming style and supports dynamic tensor computations with automatic differentiation and strong
GPU acceleration. Its fast performance is achieved by being written mostly in C++. It is easy for users to
develop, debug, and run neural networks. In a word, these frameworks implement function by optimizing
complex network based on simple mathematical model. They can not be used to develop and train SNN
directly.

2.2 Computational Neuroscience Frameworks

According to the needs of neuroscience theoretical simulation, several simulation software tools have emerged,
mainly including NEURON [HC01], GENSIS [CRC+12], CARLsim [CKX+18], NEST [GD07], Brian2 [SBG19],
and BrainPy [WJL+21], which achieve biological realism in different levels. NEURON and GENSIS focus
on simulating detailed realistic biological neuron with properties that include, but are not limited to, com-
plex branching morphology and multiple channel types. CARLsim, NEST, Brian2 and BrainPy focus on

3

the dynamics and structure of neural systems rather than on the detailed morphological and biophysical
properties of individual neurons. They can be used for simulating large heterogeneous networks. A major
advantage of NEURON, NEST, Brian2 and BrainPy is that, in addition to the built-in neurons and con-
nection objects, users can also use low-level language (such as C++) or mathematical model descriptions
to design the dynamics of neurons and connections. This provides convenience to investigate new mech-
anisms. However, esoteric syntax may lead to a steep learning curve for new users. In addition, the lack
of automatic differentiation support makes these tools unsuitable for training SNNs for machine learning
tasks.

2.3 Neuromorphic Computing Frameworks

Neuromorphic Computing aims to apply the insights from neuroscience to create brain-inspired model and
device. SNN can be regarded as a core technique of neuromorphic computing. Frameworks such as Nengo
[BBH+14], BindsNet [HSK+18] and SpikingJelly [FCD+20] focus on behaviors of SNNs and can be
applicated in the ﬁeld of machine learning. They are built on deep learning framework (such as PyTorch
and TensorFlow) to facilitate the fast simulation of SNNs on CPU and GPU computational platforms,
as well as using deep learning training procedures to optimize model parameters. Nengo is regarded as
a cognitive modeling tool. It can be used to build large-scale models based on the Neural Engineering
Framework (NEF) to simulate advanced functions of the brain or brain regions. It also provides an ex-
tended version based on deep learning library, namely NengoDL, which uses TensorFlow to improve
simulation speed of Nengo models and implements automatic conversion from Keras models to Nengo
networks. SpikingJelly and BindsNet are SNN libraries developed on top of the PyTorch deep learn-
ing framework, which enable rapid prototyping and features concise syntax. In general, they are closer
to deep learning procedure and do not support neuroscience well, for example, simulating the anatomical
and biophysical properties of neurons and neural circuits.

These frameworks focus on either neuroscience or deep learning, rather than both. SPAIC uses PyTorch
as its matrix computations backend to perform efﬁcient, which is suitable for machine learning tasks. In
addition, several popular neuroscience-based neuron types, synapse types and connection types are pro-
vided for users to choose from. In this way, SPAIC can be seen as a bridge between the artiﬁcial intelligence
computing and neuroscience domains, enabling researchers to easily integrate neural mechanisms found
in neuroscience with machine learning techniques to develop better artiﬁcial intelligent.

3 Package Structure

The main structure of SPAIC is shown in Figure 1. The classes of SPAIC can be divided into three functional
blocks.

1. Network components contain all the components of the model which provide a frontend to set up

network.

2. Within simulation procedure, the simulation process of network is complied to computation graph in

the Backend.

3. The training and analysis tools provide I/O interface, Plot, TrainingLog and et al.

SPAIC provides a Network object to contain all the network components. Users can organize basic
components, such as Nodes, NeuronGroups, Connections and Assemblies, with complex structure. Auxiliary
components, such as Learner, Optimizer and Monitor, can be added according to users’ requirements. A
Backend should also be attached to Network to compile the frontend network model.

3.1 Network Components

3.1.1 Assembly

First of all, Assembly is one of the most important components of SPAIC. It is an abstract class of neural
population, that Network, NeuronGroup, Node and Module are all inherited from it. It deﬁnes the basic net-

4

Table 1: Comparison of neural network simulation tools regarding supported features. An ‘(cid:88)‘ denotes
that the feature is supported by the simulator, and a blank denotes that the feature is not supported..

Tensor-
Flow

PyTorch NEURON GENSIS

CARL-
sim

NEST Brian2 BrainPy Nengo

Binds-
NET

Spiking-
Jelly

SPAIC

LIF
aEIF
IZH
HH

Chemical
Electrical

Convolution
Sparse
Loop
Delay

STDP
Gradient

CPU
GPU

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

Neuron model
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

Synaptic type
(cid:88)
(cid:88)

(cid:88)
(cid:88)

Connection property

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Learning algorithm

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Computing hardware

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

Figure 1: The structure of SPAIC.

5

work structure and attributes. When users want to build a network with large scale and complex structure,
especially models involving multiple brain regions, they can construct Assembly object with NeuronGroups
and Connections as the subregion. The example code is shown as follow:

# Construct an Assembly object as a subregion
class SubRegionA(spaic.Assembly):

def __init__(self):

super(SubRegionA, self).__init__()

self.l1 = spaic.NeuronGroup(10, neuron_model=’lif’)
self.l2 = spaic.NeuronGroup(20, neuron_model=’lif’)
self.connection1 = spaic.Connection(self.l1, self.l2)

class TestNet(spaic.Network):

def __init__(self):

super(TestNet, self).__init__()

...
self.regionA = SubRegionA()
...

3.1.2 NeuronGroup

A NeuronGroup is a group of neurons with the same neuron model and connection pattern. Also, a Neuron-
Group should contain all the details of neurons like the initial voltage and threshold. Users should provide
model type and number of neurons when creating a NeuronGroup. SPAIC provides a series of build-in neu-
ron models like leaky integrate-and-ﬁre (LIF) model[Ste67], adaptive exponential integrate-and-ﬁre (AEIF)
model[BG05], Izhikevich (IZH) model[Vaz10], Hodgkin-Huxley (HH) model[NR98] and et al. The follow-
ing code shows how to create a group with 50 LIF neurons.

# NeuronGroup
self.l1 = spaic.NeuronGroup(neuron_number=50, neuron_model=’lif’)

Attributes of the neuron model, such as time constant τm, can be modiﬁed as keyword arguments of
NeuronGroup. If there are no attributes given, SPAIC will use the default parameter values of these neuron
model. In addition, users can deﬁne auxiliary attributes such as spatial position and neuron type in the
NeuronGroup, which might be useful in constructing complex networks.

3.1.3 Node

Node is the input and output convert unit of neural network, it contains coding mechanism which will
convert input to spikes or convert spikes to output. It has ﬁve different subclasses:

• Encoder: Compared with the static numerical input of ANN, SNN uses spike trains as input, which
is consistent with the way brain transmits information. The Encoder implements the function of con-
verting the input data into input spikes. SPAIC provides some common encoding methods, such as
PoissonEncoder and LatencyEncoder. As an example, the following code deﬁnes a PoissonEncoder object
which transforms the input into poisson spike trains.

# Encode input
self.input = spaic.Encoder(num=node_num, coding_method=’poisson’)

• Decoder: The main usage of Decoder is to convert the output spikes or voltages to a numerical signal.
SPAIC provides some common decoding methods, such as SpikeCounts and FirstSpike. For example,
the following code deﬁnes a SpikeCounts object to get the number of spikes of each output neuron.

# Decode output spikes of layer2
self.output = spaic.Decoder(num=10,

dec_target=self.layer2,
coding_method=’spike_counts’,
coding_var_name=’O’)

6

• Generator. It is a special encoder that will generate spike trains or current without dataset. For ex-
ample, in some computational neuroscience studies, users need special input like poisson spikes to
model background cortical activities. To meet such requirement, some common pattern generators,
such as PoissonGenerator or ConstantCurrentGenerator, are provided in SPAIC.

• Reward: During the execution of a reinforcement learning task, Reward, a different type of decoder,
is needed to decode the activity of the target object according to the task purpose. For example, the
GlobalReward for classiﬁcation task determines the predicted result according to the number of spikes
or the maximum membrane potential. If the predict result is the same as the expected one, the positive
reward will be returned. On the contrary, negative rewards will be returned.

• Action: Action is also a special decoder that will transform the output to an action. The main usage
of Action is to choose the next action according to the action selection mechanism of the target object
during reinforcement learning tasks. For example, the PopulationRateAction, it will take the label of
the neuron group with largest spiking frequency as action.

3.1.4 Topology

The topology component is used to specify interactions between Assembly objects, it is consist of Connection,
Synapse, Projection and ConnectPolicy. Connection is the most generic implementation of topology that con-
nects elementary Assembly objects, such as Node and NeuronGroup. Connection can specify Synapse to deﬁne
how the pre-assembly affects the post-assembly. Projection is an abstract topology representing the commu-
nication between high level Assembly objects which contains multiple Connections. ConnectPolicy deﬁnes a
rule to generate speciﬁc Connections in Projection.

Connection and Synapse: The Connection is aware of pre-assembly and post-assembly object, as well as a
matrix of weights of connections strengths. SPAIC supports a lot of different connection forms like FullCon-
nection, RandomConnection, SparseConnection and ConvolutionConnection. Noticeable, the SparseConnection is
developed for the simulation of large scale network with sparse connectivity. In addition, the synapse is
a critical structure of neural connection that usually transmit information from the source neurons to the
target neurons. The Synapse object can be speciﬁed when creates a Connection object and the input is ﬁl-
tered by the synapse kernel. If there is no speciﬁc Synapse in a Connection object, the weighted sum of input
from pre-assembly is directly added to the state variable of post-assembly. For example, the following code
shows how to build a full connection that connect input layer and layer1:

# Connection between input layer and layer1
self.connection1 = spaic.Connection(
pre_assembly=self.input,
post_assembly=self.layer1,
link_type=’full’)

SPAIC provides some classical Synapse types in neuroscience, such as NMDASynapse, AMPASynapse,
GABASynapse and GapJunction. Delay is also important in neuroscience, but it is difﬁcult to achieve on a
general deep learning platform. In SPAIC, delay can be added to connections with extra memory consump-
tion.

Projection and ConnectPolicy: The Projection is a high-level network component that can contain multiple
Connections between the NeuronGroups in pre-assembly and post-assembly. It can automatically generate
Connections between NeuronGroups according to user deﬁned rules, which is useful when constructing com-
plex networks with repetitive connection patterns. The ConnectPolicy object provides the interface for users
to deﬁne the connection rules, and be used in the Projection object. SPAIC provides some prototype policies
such as IncludeTypePolicy, which is to connect the user speciﬁed types (e.g. pyramidal, inhibitory) of Neuron-
Groups. In addition, users can deﬁne more complex connect behavior by overriding the __init__function
of the Projection object. For example, the following code shows how to generate connections between exci-
tory neurons and inhibitory neurons in two layers:

# Define Assembly with different types of neurons
class SubRegion(spaic.Assembly):

def __init__(self):

super(SubRegionA, self).__init__()
self.l1 = spaic.NeuronGroup(10, neuron_model=’lif’, type=’excitory’)

7

self.l2 = spaic.NeuronGroup(20, neuron_model=’lif’, type=’inhibitory’)

layer1 = SubRegion()
layer2 = SubRegion()

# Define specific connection rules and pass it to a Projection object
ei_policy = spaic.IncludeTypePolicy(pre_types=[’excitory’], post_types=[’inhibitory’])
ei_project = spaic.Projection(pre=layer1, post=layer2, policies=[ei_policy])

3.1.5 Module

The Module is a special subclass of Assembly which is directly inherited from torch.nn.Module. So SPAIC
can implement functions supported by PyTorch and has advantage to combine ANN and SNN in a hy-
brid network. The following example shows how to use Module to add a convolution layer and a batch
normalization layer into a network.

# Module with a convolution layer and a batch normalization layer
self.layer1_layer2_con = spaic.Module(nn.Sequential(

nn.Conv2d(

in_channels=in_channel,
out_channels=out_channel,
kernel_size=3

),
nn.BatchNorm2d(out_channel)

),

input_targets=self.layer1, input_var_names=’O’,
output_tragets=self.layer2, output_var_names=’WgtSum’)

3.1.6 Monitor

Monitor records the state variables of connections and neurons during whole simulation time. At present,
there are two types of monitors in SPAIC, the SpikeMoniter and StateMonitor. The SpikeMoniter records the
spike trains and the StateMonitor records all other states. The following example code shows how to build
a StateMonitor and a SpikeMoniter to record voltage and output of layer1, respectively.

# Monitor the voltage and output of layer1
self.mon_V = spaic.StateMonitor(self.layer1, ’V’)
self.mon_Spike = spaic.SpikeMonitor(self.layer1, ’O’)

3.1.7 Learner

Learner is a base class for all learning and optimization algorithms of the network. Learning algorithm is
the main part of learner that can access and update any speciﬁed network parameters. The exact learning
algorithm can be deﬁned by users as they need. Here, we provide two common types of algorithms of SNN
as examples. The ﬁrst type of algorithms is based on gradient back propagation like STCA[GXP+19] and
STBP[WDL+18], which mainly realized by surrogate gradient. Another kind is based on synaptic plasticity
like STDP[BMF+15] which is more biologically plausible. Meanwhile, SPAIC also has built in algorithms for
reinforcement learning, like RSTDP[Flo07], which is based on STDP learning rule with reward mechanism.
Optimizer is another part of learner. It contains many optimization algorithms like Adam and SGD which
can be used to optimize network parameters of gradient based algorithms. The usage of gradient based
learning algorithm STCA and optimization algorithm Adam is as follow:

# Learner
self.learner = spaic.Learner(trainable=self,

algorithm=’STCA’)

self.learner.set_optimizer(’Adam’, 0.001)
...
learner.optim_zero_grad()
learner.optim_step()

8

3.1.8 Network

Network is the top-level of the model that all other components should be included in it. The network model
can be deﬁned by inheriting the Network class and adding network components in the __init__ function. We
also provide two kinds of network construction methods which are described in detail in the document of
SPAIC. The run function of Network class starts the simulation of all network components in Backend. The
following code shows how to construct and run a network model.

# Construct a TestNet
class TestNet(spaic.Network):

def __init__(self):

super(TestNet, self).__init__()
...

net = TestNet()
...
net.run(run_time)

3.2 The Simulation Procedure

SPAIC decouples neural model creation and simulation into the frontend and the backend, respectively.
The network components described above provide the frontend interfaces to create static symbolic descrip-
tion of the network. The Backend can take the model described in the frontend, transform the neuron and
synaptic models into discrete equations, and build optimal computation graph that are suited to implement
the simulation. When the simulation starts, the Backend fetches data from Encoder and performs operations
of the entire computation graph. Meanwhile, the Decoder or Monitor can get data from Backend. The whole
process is shown in Figure 2.

Figure 2: The computation ﬂow in SPAIC..

Building graph is a key step in the procedure from frontend network representation to backend simula-
tion. Biological neural circuits are cyclic graph, hence brain inspired model cannot be built directly due to

9

circular dependency on calculation. SPAIC provides two build strategies which build model into parallel
or serial computation graph respectively. Firstly, the parallel computation graph is the most common way
in neuroscience SNN simulators that all of the connections will use the output of pre-assemblies from the
last step so that all components can run in parallel. Secondly, to be consistent with the serial computation
in DNN, SPAIC provides another one build strategy. It uses Topological Sorting[Kah62] to search the net-
work and decompose the network structure into feedforward and cyclic parts. In the cyclic part, one step
delay is added to one of connections to solve the circular dependency problem. Then network model can
be transformed into Directed Acyclic Graph (DAG).

It is noted that SPAIC supports both static and dynamic computation diagrams. Although it mainly uses
PyTorch as backend which uses dynamic diagram for computation, the architecture of SPAIC is designed
to generate a static computation when the build process is complete. At the same time, the design of static
diagram makes SPAIC support multiple backends, such as TensorFlow and Jax. On the other hand, we
provide dynamic computation diagram that allows some operation modiﬁcations in computation graph
during runtime.

3.3 Training and Analysis Tools

Training and analysis are the most time-consuming works of neural network research. SPAIC supports a
number of useful tools and I/O interface to provide a more user-friendly environment. Plot and TrainingLog
are such major tools: Plot object provides functions for generating diagram of results and process analysis,
and TrainingLog will record the variation of parameters to help users analyze the role of algorithms.
The I/O interface component provides four tools: DataLoader, Dataset, DataPort and Environment.

• DataLoader: organizes and format data, such as generate index from data and batch it according to the

batch size.

• Dataset: loads data from dataset according to the format of storage.

• DataPort: a data transmission interface that can receive data in real time.

• Environment: used for reinforcement learning.

Finally, SPAIC contains NetworkSave component in library tools, which let users can save the parameters

or the whole trained model for further use.

3.4 Custom Extension

SPAIC can also be extended by custom-deﬁned neuron models, connections and learning rules with cus-
tomization mechanisms, which we will discuss in the following.

3.4.1 Customization mechanisms

We customize network components through adding variables and operations. There are two types of cod-
ing methods as below.

• String based command: Objects of SPAIC (e.g., NeuronGroup, Connection, etc.) provide a variables dic-
tionary and an operations list. When users want to implement an operation like A=B+C. The ﬁrst step
is to add variables used in the operation into variables dictionary as below.

self._variables[’A’]=0.0
self._variables[’B’]=0.0
self._variables[’C’]=0.0

The second step is to append the string of the operation into operations list. The main format of oper-
ations is that the ﬁrst variable represents the result, the second item is the name of the basic operations
supported in Backend. The remain variables from the third term represent the input parameters of the
calculation formula.

self._operations.append((’A’, ’add’, ’B’, ’C’))

10

• Standalone function: When the calculation formula is hard to be implemented using basic operations
provided in Backend, users can use the standalone function approach. Firstly, the desired operation
needs to be implemented in a Python function as below.

def add_func(a, b):

return a+b

Then, the string of the operation should be appended into operations list, where the second item is
the callable function.

self._operations.append((’A’, add_func, ’B’, ’C’))

3.4.2 Custom Neuron Model

The abstract class NeuronModel implements functionality that is common to all neuron types. The user
must deﬁne the calculation formula of the neuron themselves in the body of the __init__() function. Then
the customized model should be registered to the SPAIC using the NeuronModel.register function.

3.4.3 Connection

Users of SPAIC can deﬁne their own connection types by creating a class that inherits from Connection. To
deﬁne a new connection class, one must override the unit_connection function.

There are many different synapses in the neural systems, SPAIC allows users to add their own synapse

models. SPAIC provides abstract class SynapseModel which is similar with NeuronModel.

3.4.4 Learning rule

Learner is an abstract class for all learning and optimization algorithms of the network. New learning
algorithm class should override custom_rule function of Learner to access and update any speciﬁed network
parameters. We deﬁne trainable list and pathway list. The trainable list contains the network components
whose parameters can be updated by the learning rule. The pathway list contains the network components
are useful to the learning rule but whose parameters should not be updated. For example, in a gradient
back-propagation learning rule, if only last few layers of network should be trained, the rest of layers should
be added to pathway list so that the loss gradients can be passed through the whole network.

There are two families of learning rules supported in SPAIC, gradient based or plasticity based. Users
can follow the format of STCA or STBP in SPAIC to customize gradient based algorithm. If users want to
add plasticity based algorithms, they can follow the format of STDP.

4 Examples

4.1 Computational Neuroscience experiment

The central motivation of SPAIC is to assist the development of intelligence computing models inspired
by the biological brain. Hence, it is fundamental that SPAIC can support models and methods used in
computational neuroscience, and then introduce those features into a brain-inspired AI system.

Case study 1: cortical microcircuit model

We ﬁrst demonstrate the implementation of a spiking cortical microcircuit model using SPAIC, and use a
mean-ﬁeld counterpart model to show the multi-scale modeling in SPAIC. As shown in Figure 3(a), the
Potjans-Diesmann model contains two types of neuron distributed in four layers [L23, L4, L5, L6], which
represents cortical microcircuit network below a surface of 1 mm [PD14]. In each layer, the subnetwork can
be viewed as an internally connected excitation and inhibition balance network [Bru00], which can be built
as a prototype submodule using the Assembly object. Then the cortical microcircuit can be constructed by
stacking and connecting the four layers with different parameters. The realization code of above network is

11

shown in Figure S1, here we build neuron groups with LIF neuron model, use SparseConnection to randomly
connect neurons with a given probability, and use PoissonGenerator to model external noise input to each
neural population. As shown in Figure 3(c), the model exhibits irregular and stationary spiking activities
similar with the result in original work. In addition, we can replace the LIF spiking model with a neural
mass rate model and connect NeuronGroups with trainable coupling efﬁciency, then we can train this abstract
model to have equivalent result (Figure 3 (b)(d)).

Figure 3: Cortical microcircuit network structure and activity. (a) Microcircuit structure of the spiking net-
work model. (b) Microcircuit structure of the mean-ﬁeld model. (c) Activity of the spiking network model.
(d) Activity of the mean-ﬁled model. .

Case study 2: C. elegans thermotaxis circuit

The second example of SPAIC application is a spiking model of C. elegans thermotaxis circuit, adapted from
earlier studies [KSO+12; BRR14]. This network has a small number of well-characterized neuron types (Fig-
ure 4(a)) and is known to generate a stereotypical triphasic motor pattern (turn clockwise, turn anticlock-
wise and random walk) when sensor neurons receive temperature changes. The design of interneurons
structure performs as the derivative operation when the opposite directions of currents get through dif-
ferent length of trails, and thus supports gradient detector. This simple mechanism allows C. elegans to
achieve diversion and track the favorable temperature contour. The simulation of the network in SPAIC is
shown in Figure 4(c), we use aEIF model for modeling the dynamics of the neurons, adopt full Connection
to link single neurons. As the input temperature changes above or below the threshold, the output neurons

12

of turn clockwise or anticlockwise will spike alternately. Thus it can control the temperature steering of C.
elegans and make it track along the optimum temperature region as Figure 4(b) demonstrated.

Figure 4: C. elegans temperature sensation modeling and simulation. (a) C. elegans thermotaxis circuit.
(b) Simulation of C. elegans thermotaxis based contour tracking. (c) Activity of the spiking network
model..

4.2 Training Deep Spiking Neural Networks

SPAIC is suitable for training deep SNNs to solve problems in the domain of machine learning. Here, we
present example scripts to show how to build deep SNNs using SPAIC to implement machine learning
tasks, such as speech recognition and image recognition.

Case study 3: speech recognition

As shown in Figure 5, we trained a four-layer SNN to implement supervised learning of the TIDIGITS
speech dataset [LD93], which consists of digit utterances from ‘zero’ to ‘nine’ and ‘oh’. The dataset is split
into 3465 training samples and 1485 testing samples. It is worth noting that when reading speech ﬁles
directly, there will be tens of thousands of data in one second of audio with a lot of redundant information.

13

Therefore, before performing the speech recognition task, we need to preprocess the raw speech ﬁles to
obtain the features of the dataset. In SPAIC, we provide two popular preprocess methods, namely, Mel-
frequency cepstral coefﬁcients (MFCC) [UAW19] and keypoints extraction (KP) [XTG+18]. The code of
implementing SNN in SPAIC to classify TIDIGITS is given in Figure S2. Here, we use MFCC preprocess
method to extract features and use STCA learning algorithm to train entire network.

The results in Figure 6 show that competitive performance has been achieved within 5 training epochs.
These results demonstrate the effectiveness of the preprocessing and network training method, making
SPAIC suitable for training deep SNNs to solve machine learning tasks.

Figure 5: The network structure for TIDIGITS speech recognition. It is a four-layer SNN with full connec-
tion which is trained by STCA algorithm..

Figure 6: The network can developed suitable representations as demonstrated by increased average test
accuracy over 5 runs..

14

Case study 4: image recognition

In addition to constructing SNN with fewer layers, SPAIC is also convenient for implementing deeper SNN.
In recent years, deep residual network (ResNet) has replaced VGG as the basic feature extraction network in
computer vision [HZR+16]. Here, we implement a spiking ResNet-18 on SPAIC. The spiking residual block
is the key structure to implementing ResNet. When the residual block is built, the ResNet with arbitrary
depth can be realized through repeated structure. In spiking residual block, we replace ReLU activation
layers of standard residual block in ANNs with spiking neurons. The basic structure of spiking residual
block is shown in Figure 7(a). For basic block, the input and output have the same dimension, so there
is no convolution operation on the shortcut path. When the input and output have different dimensions
or stride>1, there is a convolution operation on the shortcut, and the structure of residual block is shown
in Figure 7(b). The example code of implementing residual block is given in Figure S3. Using Assembly
and Module classes of SPAIC, we can conveniently implement residual block, or any complex deep model
structure we want.

Figure 7: The structure of residual block. (a) Basic block. (b) Residual block with downsample. If the input
and output have different dimensions or stride>1, convolution will be performed on the shortcut..

4.3 Neuromorphic Computing Applications

Another important use case of SPAIC is to support neuromorphic computing applications that will even-
tually perform on robots or terminal systems. These situations require the platform to 1) run fast enough
to be able to response in real-time and 2) provide data interfaces to communicate with the other system
during the network simulation. Here, we present a simulated robot implementing reinforce learning task
to show the usability of SPAIC in those applications.

15

Case study 5: SNN and ANN hybrid reinforcement learning

Reinforcement learning is other kind of important machine leaning algorithm, which is concerned with
learning to control an agent (such as robots) to maximize the performance in respect to a long-term objec-
tive. As an example, we adopt the Reinforcement co-Learning method(Spiking-DDPG), which consists of a
spiking actor network(SAN) and a deep critic network, to construct an end-to-end SNN network for map-
less navigation task of the wheel robot (turtlebot2). The Gazebo Robot simulator is used as a middleware
for both the training and validation. PoissonEncoder is used to encode the input state obtained from the
Gazebo simulator, which is a 1x24 array composed of the relative distance and direction from the robot to
the goal, the linear and angular velocities of the robot, and the distance observations from the laser range
scanner. During training, the SAN built in SPAIC generated an action for the input state given by the Robot
simulator. Then, the action will be fed to the deep critic network for predicting the associated Q-value for
the training of SAN. Meanwhile, the action which takes spiking rates of 2 neurons will be transformed to
the left and right wheel speeds of the differential drive robot with the SpikeCount Decoder. Then, it will be
published to the Gazebo environment for generating a reward to update the action-value for the training of
the deep critic network. The SPAIC platform supports a wide range of neurons and training policies which
can be applied by users to modify the SNN actor network to perform any end-to-end robot control tasks.
Figure 8 depicts the training and validation environment instance in Gazebo simulator and the details of
the implementation of Spiking-DDPG training work.

4.4 Benchmark

To compare with the existing prominent platforms, we test the performance of SPAIC by running a simple
network with increasing neuron scales. We simulated a network with one input layer of 100 neurons and
Poisson input of 30HZ. The core test layer of this network has n (from 10 thousand to 10 million) LIF
neurons to test the running speed. We run each test model for 100ms with time step dt = 1.0ms. The model
will be run 100 cycles and the whole running time is computed. All benchmark tests run on the workstation
with Ubuntu 18.04 LTS and Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz, 128Gb RAM and a Nvidia Quadro
GV100 GPU with 32 GB DRAM. Python 3.8 is used by all tests.

We tested SPAIC, SpikingJelly, BindsNet and BrainPy with both CPU and GPU, and tested
Brian2 with only CPU. SpikingJelly and BindsNet are based on PyTorch and can support training
of surrogate gradient algorithms, but SpikingJelly does not have the mechanism of synaptic plasticity.
BrainPy and Brian2 are focus on neural dynamics simulation that cannot train or don’t have gradient
back propagation mechanism. To provide an impartial comparison, we choose to simulate the network
without training. We have considered Neuron, GENSIS, CARLsim, NEST and Nengo, but in these cases,
we were unable to implement the benchmarked network structure or the performance is signiﬁcantly low
in such benchmark.

The result of performance can be seen from the Figure 9a and Figure 9b. As shown in Figure 9a,
CPU-only SPAIC and SpikingJelly perform best in small scale network with n ≤ 105. CPU-only
SpikingJelly shows the best performance on larger scale networks with n ≥ 106.

Figure 9b depicts that SPAIC performs best on n ≤ 106. The BrainPy platform performs well on
n = 104, but becomes relatively slow for n ≥ 105 and simulation runs are stopped for n ≥ 107, due to
out of memory. Since Brian2 simulates spike transmission with event-driven method, the performance
of Brian2 with different spike input rate will be different. Then, we set a moderate spike input rate in
benchmark test. Figure 9b shows that SPAIC and SpikingJelly perform similarly at large scale.

Although our benchmark are not indicative of the performance in all circumstances, it still shows the
performance of SPAIC is superior to existing platforms. Meanwhile, the rich functionality shown in the
Table 1 is also one of the highlights of SPAIC. In addition, the ﬂexibility is a signiﬁcant dimension for
evaluating platforms. We deﬁne the ﬂexibility according to the ability of the framework to develop learn-
ing algorithms. As show in the Table 1, the deep learning frameworks support gradient back propagation
learning algorithm and the computational neuroscience frameworks support spike timing dependent learn-
ing algorithm (STDP). Currently, only SPAIC and BindsNET support two types of learning algorithms.

We are now able to characterize the typical platforms in terms of performance, ﬂexibility, and function-

ality, demonstrating that SPAIC is superior to its counterparts.

16

Figure 8: Training Spiking-DDPG on the SPAIC framework. (a) The structure of Spiking-DDPG training
system. (b) The result of training..

17

(a) .

(b) .

Figure 9: The benchmark result. (a) Time cost of benchmark test in CPU device. (b) Time cost of bench-
mark test in GPU device. note: performance of Brian2-GPU is not considered since its support of GPU is
incomplete..

Figure 10: Comparison of performance, ﬂexibility and functionality of frameworks mentioned above..

5 Further development

The SPAIC framework is under continuing development, and there is still much room for improvement.
Below we listed several ongoing developments and future directions of the framework:

1. More algorithm and model support: We are continuously extending our support of SNN learning
algorithms, encoding/decoding methods and neuronal/synaptic models. We plan to integrate more
learning types, such as genetic algorithm and network architecture search. In the future, we will also
add new models and algorithms developed in our research group based on the SPAIC. Even though
users would most likely use their own customized models and algorithms in their application, a rich
support of current models provides a quick start for working with the framework.

2. Performance optimization: Optimizing the speed and memory usage of training and simulation will
improve the efﬁciency of experiments with SNNs. The frontend-backend structure allows indepth op-
timization on performances, but also requires a lot of time to reﬁne detailed computation. There are
several ongoing attempts to improve the performance: 1) In torch backend, we have used Torch.FX
to transform dynamic computation graph into static IR, and we can further improve computational
efﬁciency by fuse and transform operations; 2) Add JAX to the backend engine, and use its JIT tech-
niques to improve performance; 3) Customize the representation and computation of sparse matrix
to improve the efﬁciency in large sparse networks.

3. ODE solver support: Currently, the SPAIC uses discrete dynamical models to iteratively compute the

18

models in the form of euler or exponential euler method, which is easy for Back-Propagate Through
Time (BPTT) learning algorithms. However, to support simulation with various precision and sup-
port a more explicit representation of dynamical model, we plan to support model representation
with ODE form, and adding ODE solvers such as Runge-Kutta methods. To support gradient-based
learning in more complex ODE solvers, adjoint methods should be imported to the framework.

4. Extending tools and GUI: In this primary version of SPAIC, we only provide core functions of SNN
simulation and training, but we plan to enrich our framework with tools that may be useful in model
building and analysis. For example, we plan to support visualization tools like Tensorboard into
the framework that can help analyze the model in aspects such as network structure, dynamics, pa-
rameter distribution and neuron selectivity.

6 Discussion

The intention of developing the SPAIC framework is to assist new brain-inspired modeling study and var-
ious application research in the neuromorphic community, including spiking neural networks algorithms,
neural system modeling, and robotic systems, etc.

Neuromorphic computing is still an emerging multidisciplinary research ﬁeld, where multiple theories
and methodologies are competing and integrating with each other, and the boundary of this ﬁeld is still
not very clear. Hence, it imposes a remarkable challenge to decide what features should be emphasized
for such a computing framework, supporting both training and simulation. Many researchers viewing
SNN as a special RNN, such approach provides a shortcut for development and implementation of SNN
algorithms. However, it could also pose limitations or even be misleading for brain-inspired computing
researches, as there are fundamental differences between SNNs and RNNs. For example, RNN populations
are designed to compute output at every time step where the information processing occurs at popula-
tion level (tensor) while spiking neurons only emit spikes very sparsely and computation mainly happens
within single neuron dynamics [GK02]. Several recent works have developed algorithms with deep SNNs
such as the spiking ResNet, which have achieved comparable performance with counterpart deep learning
algorithms in image classiﬁcation tasks [FYC+21]. However, to achieve such performance, the SNN have to
be simulated with simple neuron model, very short time steps and very high ﬁring rate, where the spiking
neuron can hardly contain memory information or process temporal information by the neuronal dynamics.
Such approach limited the potential of SNNs both in power efﬁciency and computation capability. To facil-
itate truly brain-inspired computation researches, we designed the SPAIC network construction interface
with neuroscience style such that researchers can easily introduce neuroscience results into brain-inspired
models. On the other hand, deep learning methodologies provide a variety of delicate training algorithms
for optimizing network functions, hence we incorporated the training procedure into our framework, and
use a Learner object to integrate theories in the both ﬁeld into one united procedure. This hybrid coding
style is what we see most suitable for current brain-inspired modeling studies.

To confront foreseeable development of brain-inspired modeling methodologies, we have to seek the
balance between the guidance of neuroscience principles and the deploying of deep learning technologies,
and provide considerable ﬂexibility in the framework. In SPAIC, we provide a guideline to build a net-
work model by combining assembly, connection and learner objects, with each object specialized in one
aspect of neural computation. On the other hand, each of those network components provides interfaces
to directly customize their backend computation, and hence users can easily build their model. Moreover,
even through we have provided functions to model biological features such as synaptic dynamics, sparse
connections and conduction delay, users can also easily implement their realizations to ﬁt their usage. In
summary, SPAIC is a highly customizable framework, which can help researchers easily and efﬁciently
build and test brain-inspired models and develop various artiﬁcial intelligence applications.

Acknowledgments

The authors would like to acknowledge the National Key Research and Development Program of China
under grant 2020AAA0105900, the Key Research Project of Zhejiang Lab under Grant 2021KC0AC01, and

19

the National Natural Science Foundation of China under Grant 62106234 for ﬁnancial support.

20

References

[AAA+16] Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,
Nicolas Ballas, Frederic Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al.
“Theano: A Python framework for fast computation of mathematical expressions”. In: arXiv
e-prints (2016), arXiv–1605.

[Abb08]

Larry F Abbott. “Theoretical neuroscience rising”. In: Neuron 60.3 (2008), pp. 489–495.

[ABC+16] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. “TensorFlow: A System for
Large-Scale Machine Learning”. In: 12th USENIX symposium on operating systems design and
implementation (OSDI 16). 2016, pp. 265–283.

[BBH+14]

[BG05]

Trevor Bekolay, James Bergstra, Eric Hunsberger, Travis DeWolf, Terrence C Stewart, Daniel
Rasmussen, Xuan Choo, Aaron Voelker, and Chris Eliasmith. “Nengo: a Python tool for build-
ing large-scale functional brain models”. In: Frontiers in neuroinformatics 7 (2014), p. 48.

Romain Brette and Wulfram Gerstner. “Adaptive Exponential Integrate-And-Fire Model As
An Effective Description Of Neuronal Activity”. In: Journal of neurophysiology 94 (Dec. 2005),
pp. 3637–42. DOI: 10.1152/jn.00686.2005.

[BMF+15] Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. “STDP as

presynaptic activity times rate of change of postsynaptic activity”. In: arXiv preprint arXiv:1509.05936
(2015).

[BRR14]

[Bru00]

[CD08]

[CH06]

[CKX+18]

Ashish Bora, Arjun Rao, and Bipin Rajendran. “Mimicking the worm—An adaptive spiking
neural circuit for contour tracking inspired by C. Elegans thermotaxis”. In: 2014 International
Joint Conference on Neural Networks (IJCNN). IEEE. 2014, pp. 2079–2086.

Nicolas Brunel. “Dynamics of sparsely connected networks of excitatory and inhibitory spik-
ing neurons”. In: Journal of computational neuroscience 8.3 (2000), pp. 183–208.

Natalia Caporale and Yang Dan. “Spike timing–dependent plasticity: a Hebbian learning rule”.
In: Annu. Rev. Neurosci. 31 (2008), pp. 25–46.

Nicholas T Carnevale and Michael L Hines. The NEURON book. Cambridge University Press,
2006.

Ting-Shuo Chou, Hirak J Kashyap, Jinwei Xing, Stanislav Listopad, Emily L Rounds, Michael
Beyeler, Nikil Dutt, and Jeffrey L Krichmar. “CARLsim 4: An open source library for large
scale, biologically detailed spiking neural network simulation using heterogeneous clusters”.
In: 2018 International joint conference on neural networks (IJCNN). IEEE. 2018, pp. 1–8.

[CMS04]

Dmitri B Chklovskii, BW Mel, and K Svoboda. “Cortical rewiring and information storage”.
In: Nature 431.7010 (2004), pp. 782–788.

[CRC+12] Hugo Cornelis, Armando L Rodriguez, Allan D Coop, and James M Bower. “Python as a fed-

eration tool for GENESIS 3.0”. In: PLoS One 7.1 (2012), e29018.

[EHM+09]

Jochen M Eppler, Moritz Helias, Eilif Muller, Markus Diesmann, and Marc-Oliver Gewaltig.
“PyNEST: a convenient interface to the NEST simulator”. In: Frontiers in neuroinformatics 2
(2009), p. 12.

[FCD+20] Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Yonghong Tian,

and other contributors. SpikingJelly. https://github.com/fangwei123456/spikingjelly.
Accessed: YYYY-MM-DD. 2020.

[Flo07]

[FM19]

R˘azvan V Florian. “Reinforcement learning through modulation of spike-timing-dependent
synaptic plasticity”. In: Neural computation 19.6 (2007), pp. 1468–1502.

Xue Fan and Henry Markram. “A brief history of simulation neuroscience”. In: Frontiers in
neuroinformatics 13 (2019), p. 32.

21

[FYC+21] Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothée Masquelier, and Yonghong Tian.
“Deep residual learning in spiking neural networks”. In: Advances in Neural Information Pro-
cessing Systems 34 (2021).

[Gaz09]

[GD07]

[GK02]

Michael S Gazzaniga. The cognitive neurosciences. MIT press, 2009.

Marc-Oliver Gewaltig and Markus Diesmann. “NEST (NEural Simulation Tool)”. In: Scholar-
pedia 2.4 (2007), p. 1430.

Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single neurons, populations,
plasticity. Cambridge university press, 2002.

[GXP+19]

Pengjie Gu, Rong Xiao, Gang Pan, and Huajin Tang. “STCA: Spatio-temporal credit assignment
with delayed feedback in deep spiking neural networks.” In: IJCAI. 2019, pp. 1366–1372.

[HC01]

Michael L Hines and Nicholas T Carnevale. “NEURON: a tool for neuroscientists”. In: The
neuroscientist 7.2 (2001), pp. 123–135.

[HSK+18] Hananel Hazan, Daniel J Saunders, Hassaan Khan, Devdhar Patel, Darpan T Sanghavi, Hava
T Siegelmann, and Robert Kozma. “Bindsnet: A machine learning-oriented spiking neural net-
works library in python”. In: Frontiers in neuroinformatics (2018), p. 89.

[HZR+16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual learning for image
recognition”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016,
pp. 770–778.

[JSD+14]

[Kah62]

[KD18]

[KSO+12]

Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. “Caffe: Convolutional architecture for fast feature em-
bedding”. In: Proceedings of the 22nd ACM international conference on Multimedia. 2014, pp. 675–
678.

A. B. Kahn. “Topological Sorting of Large Networks”. In: Commun. ACM 5.11 (Nov. 1962),
pp. 558–562. ISSN: 0001-0782. DOI: 10.1145/368996.369025. URL: https://doi.org/
10.1145/368996.369025.

Nikolaus Kriegeskorte and Pamela K Douglas. “Cognitive computational neuroscience”. In:
Nature neuroscience 21.9 (2018), pp. 1148–1160.

Tsubasa Kimata, Hiroyuki Sasakura, Noriyuki Ohnishi, Nana Nishio, and Ikue Mori. “Ther-
motaxis of C. elegans as a model for temperature perception, neural information processing
and neural plasticity”. In: Worm. Vol. 1. 1. Taylor & Francis. 2012, pp. 31–41.

[LD93]

R Gary Leonard and George Doddington. “Tidigits speech corpus”. In: Texas Instruments, Inc
(1993).

[MWK16] Adam H Marblestone, Greg Wayne, and Konrad P Kording. “Toward an integration of deep
learning and neuroscience”. In: Frontiers in computational neuroscience (2016), p. 94.

[NMZ19]

Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. “Surrogate gradient learning in spik-
ing neural networks: Bringing the power of gradient-based optimization to spiking neural net-
works”. In: IEEE Signal Processing Magazine 36.6 (2019), pp. 51–63.

[NR98]

[PD14]

Mark Nelson and John Rinzel. “The hodgkin—huxley model”. In: The book of genesis. Springer,
1998, pp. 29–49.

Tobias C Potjans and Markus Diesmann. “The cell-type speciﬁc cortical microcircuit: relating
structure and activity in a full-scale spiking network model”. In: Cerebral cortex 24.3 (2014),
pp. 785–806.

[PF13]

Hae-Jeong Park and Karl Friston. “Structural and functional brain networks: from connections
to cognition”. In: Science 342.6158 (2013), p. 1238411.

[PGM+19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. “Pytorch: An imperative
style, high-performance deep learning library”. In: Advances in neural information processing
systems 32 (2019).

22

[PLT17]

[SBG19]

[SKP+22]

Thomas J Palmeri, Bradley C Love, and Brandon M Turner. Model-based cognitive neuroscience.
2017.

Marcel Stimberg, Romain Brette, and Dan FM Goodman. “Brian 2, an intuitive and efﬁcient
neural simulator”. In: Elife 8 (2019), e47314.

Catherine D Schuman, Shruti R Kulkarni, Maryam Parsa, J Parker Mitchell, Bill Kay, et al.
“Opportunities for neuromorphic computing algorithms and applications”. In: Nature Compu-
tational Science 2.1 (2022), pp. 10–19.

[Ste67]

Richard B Stein. “Some models of neuronal variability”. In: Biophysical journal 7.1 (1967), pp. 37–
68.

[UAW19] Mohammed Usman, Zeeshan Ahmad, and Mohd Wajid. “Dataset of raw and pre-processed
speech signals, Mel Frequency Cepstral Coefﬁcients of Speech and Heart Rate measurements”.
In: 2019 5th International Conference on Signal Processing, Computing and Control (ISPCC). IEEE.
2019, pp. 376–379.

[Vaz10]

Roberto Vazquez. “Izhikevich neuron model and its application in pattern recognition”. In:
Australian Journal of Intelligent Information Processing Systems 11.1 (2010), pp. 35–40.

[WDL+18] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. “Spatio-temporal backpropagation
for training high-performance spiking neural networks”. In: Frontiers in neuroscience 12 (2018),
p. 331.

[Wis04]

[WJL+21]

Roy A Wise. “Dopamine, learning and motivation”. In: Nature reviews neuroscience 5.6 (2004),
pp. 483–494.

Chaoming Wang, Yingqian Jiang, Xinyu Liu, Xiaohan Lin, Xiaolong Zou, Zilong Ji, and Si
Wu. “A Just-In-Time Compilation Approach for Neural Dynamics Simulation”. In: Interna-
tional Conference on Neural Information Processing. Springer. 2021, pp. 15–26.

[XTG+18]

Rong Xiao, Huajin Tang, Pengjie Gu, and Xiaoliang Xu. “Spike-based encoding and learning
of spectrum features for robust sound recognition”. In: Neurocomputing 313 (2018), pp. 65–73.

[YYG15]

Le Cun Yan, B Yoshua, and H Geoffrey. “Deep learning”. In: nature 521.7553 (2015), pp. 436–
444.

23

7 Supplementary Information

Figure S1: Example code of cortical network.

24

Figure S2: Example code of speech recognition network..

25

Figure S3: Example code of residual block..

26

