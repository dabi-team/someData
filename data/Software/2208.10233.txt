Proceedings of the 20th International Overture
Workshop

Hugo Daniel Macedo and Ken Pierce (Editors)

2
2
0
2

g
u
A
2
2

]
E
S
.
s
c
[

1
v
3
3
2
0
1
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Preface

The 20th in the “Overture” series of workshops on the Vienna Development Method
(VDM), associated tools and applications was held as a hybrid event both online at in
person at Aarhus University on July 5, 2022. VDM is one of the longest established
formal methods, and yet has a lively community of researchers and practitioners in
academia and industry grown around the modelling languages (VDM-SL, VDM++,
VDM-RT) and tools (VDM VSCode Extension, VDMTools, VDMJ, ViennaTalk, Over-
ture, Crescendo, Symphony, and the INTO-CPS chain). Together, these provide a plat-
form for work on modelling and analysis technology that includes static and dynamic
analysis, test generation, execution support, and model checking.

Research in VDM is driven by the need to precisely describe systems. In order
to do so, it is also necessary for the associated tooling to serve the current needs of
researchers and practitioners and therefore remain up to date. The 20th Workshop re-
ﬂected the breadth and depth of work supporting and applying VDM. This technical
report includes two papers on translating requirements into models. Then, two works
on the tooling supporting the methodology. The last paper is related to Cyber-Physical
Systems and design space exploration.

In addition to the talks related to the papers here published, the workshop included
an invited talk on VDMJ by Nick Battle and a round table discussion. The topic of
the invited talk was an introduction to the VDMJ extension plugin architecture and
its envisaged evolution. The talk provided insight for the many newcomers interested
in extending it, and it sparked the discussion for the round table, where educational
concerns and users views were discussed.

We would like to thank the authors, PC members, reviewers and participants for
their help in making this a valuable and successful workshop, and we look forward
together to meeting once more in 2023.

Hugo Daniel Macedo, Aarhus
Ken Pierce, Newcastle

Table of Contents

Proceedings of the 20th International Overture Workshop . . . . . . . . . . . . . . . . . . .

Hugo Daniel Macedo and Ken Pierce (Editors)

VDM-SL in action: A FRAM-based approach to contextualise formal
speciﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Tomohiro Oda, Shigeru Kusakabe, Han-Myung Chang, and Peter Gorm
Larsen

Bridging the Requirements-Speciﬁcation Gap using Behaviour-Driven
Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Kristoffer Stampe Villadsen, Malthe Dalgaard Jensen, Peter Gorm
Larsen, and Hugo Daniel Macedo

Advanced VDM Support in Visual Studio Code . . . . . . . . . . . . . . . . . . . . . . . . . .
Jonas Kjær Rask, Frederik Palludan Madsen, Nick Battle, Leo Freitas,
Hugo Daniel Macedo, and Peter Gorm Larsen

Towards UML and VDM Support in the VS Code Environment . . . . . . . . . . . . . .
Jonas Lund, Lucas Bjarke Jensen, Hugo Daniel Macedo, and Peter Gorm
Larsen

1

5

19

34

50

Speeding Up Design Space Exploration through Compiled Master Algorithms . .

66

Ken Pierce, Kenneth Lausdahl, and Mirgita Frasheri

Organization

Programme Committee

Alessandro Pezzoni

Anaplan, UK

Fuyuki Ishikawa

National Institute of Informatics, Japan

Hugo Daniel Macedo

Aarhus University, Denmark

Keijiro Araki

National Institute of Technology, Kumamoto College, Japan

Ken Pierce

Newcastle University, UK

Kenneth G. Lausdahl

AGROCorp International and Aarhus University, Denmark

Marcel Verhoef

European Space Agency, The Netherlands

Mirgita Frasheri

Aarhus University, Denmark

Nick Battle

Newcastle University, UK

Paolo Masci

National Institute of Aerospace (NIA), USA

Peter Gorm Larsen

Aarhus University, Denmark

Tomo Oda

Software Research Associate Incorporated, Japan

Victor Bandur

McMaster University, Canada

VDM-SL in action: A FRAM-based approach to
contextualise formal speciﬁcations

Tomohiro Oda1, Shigeru Kusakabe2, Han-Myung Chang3, and Peter Gorm Larsen4

1 Software Research Associates, Inc. (tomohiro@sra.co.jp)
2 University of Nagasaki (kusakabe@sun.ac.jp)
3 Nanzan University (chang@nanzan-u.ac.jp)
4 Aarhus University, DIGIT, Department of Electrical and Computer
Engineering, (pgl@ece.au.dk)

Abstract. Software development is a collaborative effort by stakeholders from
different domains of expertise and knowledge. Developing a useful system needs
to take different views on the system’s functionalities into account from the sys-
tem’s internal properties, the individual user’s tasks at hand, and an organisation’s
mission as a whole. Participation of a wide range of individual and organisational
stakeholders is crucial in the speciﬁcation phase. This paper proposes a use of the
FRAM diagram as a pre-formal notation to describe the system’s roles in individ-
ual users’ tasks and organisational activities. A tool that links public operations in
VDM-SL and activities in FRAM is introduced to demonstrate and discuss how
a FRAM diagram contextualises a formal notation.

1 Background

Software development is often driven by multidisciplinary collaboration and the de-
veloped system is also often operated by people from different areas of expertise. The
speciﬁcation phase aims to explicitly deﬁne what the system to be developed shall be
able to perform. A rigorous speciﬁcation provides a basis to create a software system
that works as planned. However, just working as planned is not enough to be useful.
The plan needs to ﬁt with various activities of stakeholders that may vary over time.
Participation of a wide range of stakeholders is thus crucial to bring out an appropriate
plan.

The stakeholders desire to foresee how the system to be developed will work in
their activities. It is important for successful software development that the system’s
functionalities are understood by the stakeholders. Although a rigorous speciﬁcation
in a formal notation, including VDM-SL [6], concisely deﬁnes the system’s function-
alities without ambiguity, it is difﬁcult for many stakeholders to understand how the
speciﬁed system will work in their activities. One difﬁculty is the notation. Although
the rapid spread of programming education may relax this difﬁculty, reading ﬂuency of
formal notation requires a certain time and effort. Another difﬁculty is contextualisa-
tion. The set of functionalities deﬁned in the speciﬁcation needs to make sense in each
stakeholder’s own expertise. The formal speciﬁcation rigorously deﬁnes a functionality
with inputs, outputs and internal states without ambiguity. However, the inputs, outputs

-2ex

6

and internal states are deﬁned in terms of the system’s internal structure and properties
although they need to be grounded and explained in the stakeholder’s activities.

The third difﬁculty is variability. One obvious source of variability comes from
the user. A user, or a group of people, sometimes performs unexpected actions on the
system, or they do not perform expected actions in time. An external server also may fail
to respond in time. The variability is not only from failures but also from good reasons.
Data may become available earlier than expected because the person in charge of the
previous action has outperformed the expectation. The data may need tentative storage
until the originally expected time. Such variability emerges from actions happening out
of the system, and is needed to understand how the system will work in action with
foreseen variabilities.

The authors has been addressing the difﬁculty of notation using speciﬁcation ani-
mation. Speciﬁcation animation is a technique to simulate the execution of the speciﬁed
system using interpreters. The stakeholders can preview the functionality of the system
before the implementation without understanding the details in the formal speciﬁcation.
The animation-based approach, however, does not solve the difﬁculty of contextualisa-
tion.

In this paper, a FRAM-based approach to deﬁne and understand the contextual
information of the system’s functionalities. FRAM (Functional Resonance Analysis
Method) will be brieﬂy explained in Section 2. Section 3 explains existing use of FRAM
in the requirement elicitation. Section 4 proposes our approach to develop formal speci-
ﬁcations in VDM-SL with pre-formal analysis using FRAM. An example development
of review bidding system will be explained in Section 5 and discussed in Section 6.
Section 7 will conclude this paper.

2 Function Resonance Analysis Method

FRAM (Function Resonance Analysis Method) [3][5] was originally introduced by E.
Hollnagel to unfold how a complex, dynamic and socio-technical system works and its
anticipated variability in operations. The basic idea of FRAM is to identify functions
or activities in a collaborative system, and analyse inﬂuences between them based on
six aspects. To avoid confusion with VDM-SL’s functions, we denote activity instead
of function.

Activities can be categorised into three types according to their performers: organ-
isational activities, human activities, and technological activities. organisational activ-
ities are those carried out by a group of people. Human activities include, but are not
limited to, the end users’ tasks at hand. Series of activities that directly or indirectly
inﬂuences the system’s activities are also identiﬁed and included in human activities.
Technological activities are those performed by the developing system and also exist-
ing systems. The difference among the three types is often apparent in their variability.
Human activities often vary more signiﬁcantly from their original plans than techno-
logical activities. Organisational activities may vary even further than human activities.
To develop a useful system, such variabilities need to be taken into account.

Once activities are identiﬁed, each of them is analysed with regard to six aspects as

follows:

20th Overture Workshop, 2022

7

1. Input: what the activity processes the input, or what starts the activity.
2. Precondition: what must be done before the activity is performed.
3. Resource: what the activity needs or consumes.
4. Time: when the activity starts and/or ﬁnishes.
5. Control: what monitors, supervises or regulates the process.
6. Output: what the activity yields.

Input, precondition, resource, time and control are aspects that may receive inﬂu-
ences of other activities while output may give inﬂuence to others. Functional couplings
among activities are identiﬁed by matching the name of aspects between the ﬁrst ﬁve
aspects of an activity and outputs of other activities.

For example, assume that code review is mandatory after coding and a quality con-
trol team aggregates review reports from reviews. The quality control team then sup-
plies review advise for subsequent code reviews. In this scenario, coding, code review
and quality control are identiﬁed as activities. The coding activity has an output called
source code, which is also input of code review. Similarly, a review report is output of
the code review activity, and it is also input of the quality control activity. The quality
control activity outputs review advisory, which controls the code review activity. Fig. 1
shows a FRAM diagram that visualises the identiﬁed activities and functional couplings
among them.

Fig. 1: An example FRAM diagram of code review

In Fig. 1, participants in the development can see that code review should be sched-
uled after writing source code, and may required to revise the code. For the coder,
writing a code always needs code review, and scheduling a review session might be a
bottleneck of coding in the long run. The quality control team does not directly dis-
cuss coding issues with the coders because they send review advise to reviewers for
subsequent use.

Fig. 2 illustrates the modiﬁed code review by introducing an automated style checker
driven by the Continuous Integration (CI) server. With the FRAM diagram shown in
Fig. 2, the developers can see how the style checker will be used in the development
project. The style checker analyses the source code and creates a report that identiﬁes
code-smells. The quality control team monitors the reports and issues review requests
for suspicious code with code-smells. Please note that the review request is a precon-
dition of the code review activity, which means that a code review session starts only
if the quality control team requests it. For the coders, the style checker is not solely
a quick code review, but rather it eliminates the delay caused by scheduling of review
sessions. By introducing the style checker, the coders will not need to wait for code re-
view sessions. The quality control team can use the style checker as a means to directly
communicate with the coders through review requests.

-2ex

8

Fig. 2: An example FRAM diagram of code review with style checker

The FRAM diagram does not provide detailed information and control structure
within the collaborated activities. It instead illustrates a context of the collaborations
that the system to be developed will participate in. In conjunction with formal speciﬁ-
cation, the stakeholders can preview how the system will work before the implemen-
tation. Our approach to use FRAM diagram along with VDM-SL speciﬁcation will be
described in the section below.

3 Related Work

De Carvalho introduced four phases to elicit functional and non-functional requirements
using FRAM [2]. Experiments to elicit functional and non-functional requirements us-
ing FRAM and BPMN (Business Process Modeling Notation) were conducted, and the
use of FRAM for identifying was conﬁrmed promising. Our approach is based on the
four phases performed in the experiment, namely contextualisation, aggregation, trans-
formation, and speciﬁcation.

While stakeholders requirements elicitation phase in general starts with identifying
stakeholders and collecting requirements to the system, the ﬁrst two phases in the exper-
iment were to analyse how the stakeholders collaborate without the system, followed by
the last two phases to address the found difﬁculties. In the contextualisation phase, the
scope of the domain area, work processes, business goals are analysed by ethnographic
observation, interviews and collecting documents. A catalog of activities in the busi-
ness is produced in the contextualisation phase. In the aggregation process, the catalog
of activities is further analysed in the perspective of solutions. Functional coupling be-
tween activities and potential difﬁculties in the activities are identiﬁed. Transformation
is the phase to produce a solution to address difﬁculties in the current Work-as-Done. A
list of expected activities to solve the difﬁculties are created. In the speciﬁcation phase,
main features and restrictions in a natural language are identiﬁed to realise the expected

20th Overture Workshop, 2022

9

activities identiﬁed in the transformation phase. The four phases go from the top to the
bottom in a one-way manner.

Our approach is to produce a formal speciﬁcation in VDM-SL from the FRAM
diagrams created in the transformation phase. We expect the formal speciﬁcation will
reveal unidentiﬁed activities and hidden couplings among activities, and such feedback
should be reﬂected to the transformation phase to form a cyclic process to develop
the speciﬁcation. Formal engineers can also use the FRAM diagram to produce walk-
through scenarios to test the formal speciﬁcation.

4 FRAM diagram to VDM-SL speciﬁcation

FRAM was originally developed for analysing complex, dynamic and socio-technical
systems. A FRAM diagram depicts how organisations, individuals and technological
entities including computer systems work together, and its applications include require-
ments elicitation [2]. This paper proposes to use FRAM diagrams as a pre-formal no-
tation to identify functionalities of the system, and speciﬁes them in VDM-SL in a
traceable manner. By pre-formal, we mean documents whose parts or the whole would
later be speciﬁed in a formal notation. A pre-formal document could be an input to the
speciﬁcation phase, a reason of practical validity of the speciﬁcation, or a supplemental
explanation to the formal speciﬁcation. Our approach takes the following steps.

1. Modeling WAD (Work as Done) in a FRAM diagram that describes how the work
is currently carried out. Identify which activities need the support of a new system.
If the work is not present yet, start with the next step.

2. Modeling transformation in a FRAM diagram that describes how the work will be
carried out with the support of a new system. Introduce new technological activities
that the new system will provide.

3. Specifying the new system in VDM-SL that corresponds to the technological activ-
ities. Use annotations to place traceability information to the FRAM diagram.
4. Providing feedback. Specify extra activities discovered or emerged in the VDM-SL

speciﬁcation, and place them in the FRAM diagram.

The ﬁrst two steps correspond to the stakeholders requirements elicitation. While
stakeholders requirements elicitation, in general, starts with collecting requirements for
the system from stakeholders, the ﬁrst step of our approach is to analyse how the stake-
holders collaborate. Then, in the second step, the difﬁculties found in the ﬁrst step will
then be addressed by introducing a system.

The last two steps form a loop, and thus the traceability between an activity in
FRAM and its counterpart in VDM-SL is important. Annotations are a mechanism to
embed additional information into a VDM-SL speciﬁcation[7][1]. An annotation takes
the form of -@ which is processed as a comment by interpreters by default but can also
be processed by tools that support annotations. Table 1 shows the forms of annotations
that we introduced for traceability with FRAM. The annotations for FRAM traceability
are implemented in ViennaTalk [8]. From a FRAM diagram, ViennaTalk can generate
the annotations with a template deﬁnition of an operation for each activity. A speciﬁ-
cation with FRAM annotations can also be merged into an existing FRAM diagram as
feedback.

-2ex

10

Table 1: Annotations for traceability with FRAM
description

identiﬁer

free text
preﬁx
-@FRAM Function
activity name speciﬁes the name of a corresponding activity
-@FRAM Input
aspect name speciﬁes the name of corresponding aspect
-@FRAM Precondition aspect name speciﬁes the name of corresponding aspect
-@FRAM Resource
aspect name speciﬁes the name of corresponding aspect
-@FRAM Time
aspect name speciﬁes the name of corresponding aspect
-@FRAM Control
aspect name speciﬁes the name of corresponding aspect
-@FRAM Output
aspect name speciﬁes the name of corresponding aspect

5 Example: Paper review

In this section, we describe an experimental development of a support tool for academic
paper review. Assume that an academic community wants to build a system that sup-
ports the paper review. For each submitted paper, a PC chair assigns a certain number
of reviewers to evaluate the quality of the paper and makes the ﬁnal call to either accept
or reject. Some candidate reviewers are registered. A reviewer may receive a review
request from the PC chair, read the paper, and send a review report to the PC chair. If
the review reports of a paper do not agree, the reviewers will start a discussion, and the
PC chair is responsible to make the ﬁnal call. In the rest of this section, we follow the
four steps described in Section 4 to develop a formal speciﬁcation of a bidding system
for reviewer assignment.

5.1 Modeling WAD (Work as Done) in FRAM

Fig. 3 shows a FRAM diagram of the review process. The activity at the top row is the
Submit a paper activity to be carried out by an author. The activities at the middle
row are those carried out by the PC chairs who are responsible to assign reviewers
to submitted papers, make the ﬁnal calls, and notify them. The activity at the bottom
row is carried out by the reviewers. The Assign reviewers activity requires much
effort and is time consuming to ﬁnd a good combination of topics of papers and the
expertise of reviewers. A delay of the Assign reviewers activity may shorten the
Review activity that requires enough duration to make a fair judgement to the quality
of the paper. The Make a call activity is also difﬁcult to complete in time when the
discussion among reviewers does not converge, but less couplings with other activities
are present than the Assign reviewer activity.

5.2 Modeling Transformation in FRAM

The FRAM diagram shown in Fig. 4 is a solution to solve the difﬁculty of reviewer
assignment. A bidding system is introduced to assist the reviewer assignment. Evalu-
ating the matching between a paper and a reviewer is delegated to the reviewer. Each
reviewer takes a look at submitted papers and make a bid to declare which paper the
reviewer would be conﬁdent to review according to the reviewer’s own expertise. With
the bids, the system can propose to the PC chair several good combinations, and the PC
chair chooses one.

20th Overture Workshop, 2022

11

Fig. 3: An example FRAM diagram of paper review

5.3 Specifying the system in VDM-SL

The next step is to specify the bidding system. Fig. 5 shows the deﬁnitions of types,
values and state required to deﬁne the operations listed in the FRAM diagram in Fig. 4.
Each submitted paper is given a review status of either <SUBMITTED>, <IN_REVIEW>,
and so on. The bidding system manages the review status of all papers in the state vari-
able calls. A bid is a mapping of each paper and preference in ﬁve grades, and the
preference is speciﬁed <CONFLICT> if the reviewer has a conﬂict of interests. The
system holds the bids by reviewers in the state variable bids.

Fig.6 shows deﬁnitions of the operations that corresponds to the technological ac-
tivities Submit a paper, Register a reviewer, Bid and Make a call.
The lines that begin with -@FRAM are annotations to keep traceability information with
the original FRAM diagram. The operation submit has two annotations that tells that
the operation corresponds to the Submit a paper activity and the activity should
have an output aspect named paper. The contextual information in the FRAM dia-
gram may help the speciﬁer understand when and how the operation would be used,
what inputs are given, and what effects are expected.

The FRAM diagram is drawn and analysed from the perspective of collaborative
activities among organisations, people and technologies. On the other hand, the VDM
speciﬁcation is written from the perspective of rigorous information processing. Some
extra operations may be discovered in the formal speciﬁcation phase.

-2ex

12

Fig. 4: An example FRAM diagram of paper review with bidding

Fig. 7 shows the deﬁnitions of operations found possibly necessary along with the
operations in Fig. 6. The viewBid and viewAssignments operations would be
needed in the construction of the user interface of the Bid and Assign reviewer
activities. The viewAssignments operation is a core part of this system to propose
assignments with the highest matching to bids. These additional activities from the
system’s perspective should be reﬂected into the FRAM diagram.

Fig. 8 shows the FRAM diagram that incorporated the additional activities. One
can see that a reviewer can see the own bids (the View bids activity) when the re-
viewer is newly registered (the Register reviewer activity by the PC chair) and
also after making bids (the Bid activity by the reviewer). The FRAM diagram also
indicates that the PC chair will see the proposed assignments of reviewers (the View
assignments action), and then decide the assignment (the Assign reviewers
action). The stakeholders including the research community members can see the re-
sulting FRAM diagram in Fig. 8 to conﬁrm whether or not the whole work ﬂow would
ﬁt with the actual paper review process.

6 Discussions

The objective of our approach to using the FRAM diagram as a pre-formal notation is
the involvement of a wider range of stakeholders. The expected use of FRAM diagrams

20th Overture Workshop, 2022

13

(cid:7)
types

ID = nat;
Reviewer = ID;
Paper = ID;
BidWeight = nat1 inv l == l <= 5;
Bid = map Paper to (BidWeight| <CONFLICT>);
Assignment = map Paper to set of Reviewer;
Call =

<SUBMITTED>| <IN_REVIEW>|
<ACCEPTED>| <REJECTED>| <RETRACTED>;

values

DEFAULT_BIDWEIGHT = 3;
REVIEWS_PER_PAPER = 3;

state Bidding of

calls : map Paper to Call
bids : map Reviewer to Bid

init s ==

s
= mk_Bidding({|->}, {|->})

end

(cid:10)(cid:6)

Fig. 5: Types, values and state deﬁnition for the bidding system

(cid:5)

-2ex

14

(cid:7)
operations

--@FRAM Function Submit a paper
--@FRAM Output paper
registerPaper : () ==> Paper
registerPaper() ==

(dcl newPaper:Paper

:= if calls = {|->} then 0 else max(dom calls) + 1;

calls(newPaper) := <SUBMITTED>;
bids := {r |-> ({newPaper|->DEFAULT_BIDWEIGHT} ++bids(r))

| r in set dom bids};

return newPaper);

--@FRAM Function Register a reviewer
--@FRAM Output reviewer
registerReviewer : Reviewer ==> ()
registerReviewer(reviewer) ==

if

reviewer not in set dom bids

then

bids(reviewer)

:= {p |-> DEFAULT_BIDWEIGHT | p in set dom calls};

--@FRAM Function Bid
--@FRAM Input reviewer
--@FRAM Resource paper
--@FRAM Output bids
bid : Reviewer * Bid ==> ()
bid(reviewer, bid) ==

if

reviewer in set dom bids

then

bids(reviewer)

:= dom calls <:

({p |-> DEFAULT_BIDWEIGHT | p in set dom calls}

++ bid);

--@FRAM Function Make a call
--@FRAM Input review report
--@FRAM Output judgement
changeCall : Paper * Call ==> ()
changeCall(paper, call) == calls := calls ++ {paper |-> call};

(cid:5)

(cid:10)(cid:6)

Fig. 6: Operations derrived from FRAM diagram

20th Overture Workshop, 2022

15

(cid:7)

--@FRAM Function View bids
--@FRAM Output bids
--@FRAM Resource bids
--@FRAM Input reviewer
viewBid : Reviewer ==> [Bid]
viewBid(reviewer) ==

return if reviewer in set dom bids

then bids(reviewer)
else nil;

--@FRAM Function View assignments
--@FRAM Input bids
--@FRAM Output assignments
--@FRAM Resource paper
viewAssignments : () ==> set of Assignment
viewAssignments() ==

(dcl

reviewSlots:seq of [ID] := [],
assignments:map Assignment to real := {|->},
submitted:seq of Paper := [];

for all p in set dom (calls :> {<SUBMITTED>}) do

submitted := submitted ^ [p];

while len reviewSlots < len submitted
do for all r in set dom bids do

reviewSlots := reviewSlots ^ [r];

* REVIEWS_PER_PAPER

for all reviewers in set perms(reviewSlots) do

let

assignment : Assignment =

{submitted(index)
|-> {reviewers(r)

| r
in set {(index - 1)
..., index

* REVIEWS_PER_PAPER + 1,

* REVIEWS_PER_PAPER}}

| index in set {1, ..., len submitted}},

score = scoreByBids(assignment)

in

if

score <> nil

then

assignments

:= assignments munion {assignment |-> score};

return dom (assignments :> {max(rng assignments)}));

(cid:10)(cid:6)

(cid:5)

Fig. 7: Additional operations for bidding

-2ex

16

Fig. 8: An example FRAM diagram with feedback from VDM speciﬁcation

in our approach is as an input to the speciﬁcation phase, as a model to conﬁrm the
practical validity of the speciﬁcation, or as a supplemental explanation to understand
the formal speciﬁcation. The use of the FRAM diagram is not limited to before the
formal speciﬁcation but also continues after the speciﬁcation phase.

To support the use of FRAM diagrams in association with VDM-SL speciﬁcation,
a toolchain that handles both notations is required. FMV (FRAM Model Visualiser)5
is a commonly used diagram editor for FRAM users. FMV uses XML format to ﬁle a
FRAM diagram, and thus collaboration with FMV is technically possible via ﬁles. We
have extended ViennaTalk to read, modify, and write FRAM models so that technolog-
ical activities in a FRAM diagram can be incorporated into a VDM model, and vice
versa.

One difﬁculty in keeping track of associations between FRAM activities and VDM
operations is vocabulary mismatching. FRAM and VDM have different scopes of the
model. FRAM overviews the collaborative work as a whole while VDM speciﬁes the
functionality within a system. The difference in modelling scopes may cause mismatch-
ing of vocabularies. An activity in a FRAM diagram is worded from the perspective of
the collaborative workers while its corresponding operation is named according to its
functionality in the system. For example, the Submit a paper in Fig. 4 corresponds
to the operation named registerPaper.

5 https://zerprize.co.nz/home/FRAM

20th Overture Workshop, 2022

17

Our approach uses annotations to embed traceability information into a VDM spec-
iﬁcation. Annotations are a special form of comment intended to be processed as a
comment by the standard interpreter but can place tool dependent information. Laus-
dahl et. al. used annotation to declare an interface with FMUs [7]. Battle et. al. used
annotation as an extension to perform directives such as probing data and dispatching
plugin routines at runtime [1]. We consider traceability information to be another usage
of annotation not limited to FRAM but also applicable to other notations.

Our approach also introduces a cyclic process between formalisation by VDM and
user feedback by FRAM. We believe such a cycle is highly encouraged especially in
the early stages of the speciﬁcation phase [8][9]. Fraser and Pezzoni used EAC (Exe-
cutable Acceptance Criteria) to document and enforce system requirements [4]. EAC
plays a key role in stakeholder involvement in Behaviour Driven Speciﬁcation. We con-
sider EAC is also documentation looking at the system’s functionality from an external
point of view. We expect more investigations to make more use of external views on
VDM speciﬁcation so that more stakeholders’ interests can be reﬂected in the rigorous
speciﬁcation in VDM.

7 Concluding Remarks

Formal speciﬁcation serves as a blueprint of a system’s internal design and structural
constructions, and hardly describes how it looks to the users and other stakeholders.
The authors have been investigating the way to make formal speciﬁcations understood
by a wider range of stakeholders through the development of ViennaTalk. ViennaTalk
makes use of speciﬁcation animation to make a formal speciﬁcation understandable to
the non-engineering stakeholders. The user still needs to understand the way how the
system will be used. This paper focuses on FRAM as a semi-formal notation to describe
the exterior views of the system in the perspective of how and why the stakeholders will
use the system. We demonstrated that annotations in a VDM speciﬁcation can glue the
formal speciﬁcation and semi-formal notations such as FRAM. Although FRAM is still
in the engineering domain, its diagrammatic nature has the potential to make sense
for a wider range of stakeholders. We expect further investigation including automated
generation of use scenarios from FRAM diagram and translation to VDM-SL test cases
using annotations.

Acknowledgements

The authors thank Keijiro Araki for valuable and fruitful discussion on pre-formal no-
tations. We would also like to thank anonymous reviewers for their valuable feedback.
A part of this work was supported by JSPS KAKENHI Grant Number 20K11759.

References

1. Battle, N., Thule, C., Gomes, C., Macedo, H.D., Larsen, P.G.: Towards a Static Check of
FMUs in VDM-SL. In: Sekerinski, E., Moreira, N., Oliveira, J.N., Ratiu, D., Guidotti, R.,

-2ex

18

Farrell, M., Luckcuck, M., Marmsoler, D., Campos, J., Astarte, T., Gonnord, L., Cerone, A.,
Couto, L., Dongol, B., Kutrib, M., Monteiro, P., Delmas, D. (eds.) Formal Methods. FM 2019
International Workshops. pp. 272–288. Springer-Verlag, LNCS 12233 (2020)

2. de Carvalho, E.A., Gomes, J.O., Jatobá, A., da Silva, M.F., de Carvalho, P.V.R.: Employing
resilience engineering in eliciting software requirements for complex systems: experiments
with the functional resonance analysis method (fram). Cognition, Technology & Work 23(1),
65–83 (2021)

3. Erik, H.: FRAM: the functional resonance analysis method: modelling complex socio-

technical systems. CRC Press (2017)

4. Fraser, S., Pezzoni, A.: Behaviour driven speciﬁcation. In: Macedo, H.D., Thule, C., Pierce,

K. (eds.) Proceedings of the 19th International Overture Workshop. Overture (10 2021)

5. Hollnagel, E.: Safety–I and safety–II: the past and future of safety management. CRC press

(2018)

6. Larsen, P.G., Lausdahl, K., Battle, N., Fitzgerald, J., Wolff, S., Sahara, S., Verhoef, M., Tran-
Jørgensen, P.W.V., Oda, T.: VDM-10 Language Manual. Tech. Rep. TR-001, The Overture
Initiative, www.overturetool.org (April 2013)

7. Lausdahl, K., Bjerge, K., Bokhove, T., Groen, F., Larsen, P.G.: Transitioning from Crescendo
to INTO-CPS. In: Fitzgerald, J., Tran-Jørgensen, P.W.V., Oda, T. (eds.) Proceedings of the
15th Overture Workshop. pp. 16–30. Newcastle University, Computing Science. Technical
Report Series. CS-TR- 1513 (September 2017)

8. Oda, T., Araki, K., Larsen, P.G.: A formal modeling tool for exploratory modeling in soft-
ware development. IEICE Transactions on Information and Systems 100(6), 1210–1217
(June 2017), https://www.jstage.jst.go.jp/article/transinf/E100.D/
6/E100.D_2016FOP0003/_article

9. Oda, T., Yamomoto, Y., Nakakoji, K., Araki, K., Larsen, P.G.: VDM Animation for a
Wider Range of Stakeholders. In: Ishikawa, F., Larsen, P.G. (eds.) Proceedings of the 13th
Overture Workshop. pp. 18–32. Center for Global Research in Advanced Software Sci-
ence and Engineering, National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-Ku,
Tokyo, Japan (June 2015), http://grace-center.jp/wp-content/uploads/
2012/05/13thOverture-Proceedings.pdf, GRACE-TR-2015-06

Bridging the Requirements-Speciﬁcation Gap using
Behaviour-Driven Development

Kristoffer Stampe Villadsen, Malthe Dalgaard Jensen, Peter Gorm Larsen, and Hugo
Daniel Macedo

DIGIT, Aarhus University, Department of Electrical and Computer Engineering,
Finlandsgade 22, 8200 Aarhus N, Denmark
{villadsen67,malthedj}@hotmail.com,{pgl,hdm}@ece.au.dk

Abstract. How is it possible to bridge the gap between requirement elicita-
tion and formal speciﬁcation? This paper proposes a solution that involves the
methodology Behaviour-Driven Modelling inspired by the agile methodology
Behaviour-Driven Development and Behaviour-Driven Speciﬁcation. To support
this methodology, we developed a tool that enables users to deﬁne behaviours
as executable scenarios. A scenario is executed on a formal model through the
tool to validate requirements. The tool is developed as a Visual Studio Code Ex-
tension. The tooling uses VDMJ to read and interpret formal speciﬁcations and
Cucumber to discover and execute Gherkin features and scenarios. We believe
that the methodology will increase readability of requirements for formal speci-
ﬁcations, reduce the amount of misunderstandings by stakeholders, and facilitate
the process of developing formal speciﬁcations in an agile process.

Keywords: Behaviour-Driven Development, VDM, Agile Methods, Formal Methods

1

Introduction

Formal modelling involves the transformation of a set of elicited requirements listed
in a natural language into an executable model in the form of a speciﬁcation written
in a formal language [12]. There are well deﬁned methods for writing formal models,
e.g.: Vienna Development Method (VDM) [9], Z [18] and B [1]. The same happens with
requirements elicitation e.g.: use-cases diagram, entity-relationship modelling, and user
stories.

In most of the approaches, the practice is to transform the requirements into the
speciﬁcation as a jump which consists of a mental exercise including the iteration of
two steps: ﬁrst understand the requirements, then formulate them as a formal model.
A conceptual illustration of the metaphoric jump can be seen in ﬁgure 1. This jump
bridges what we will metaphorically describe as the Requirements-Speciﬁcation Gap
(RSG). The writing of adequate speciﬁcations becomes an art that experienced formal
modellers acquire by performing several transformations (different jumps across the
RSG in several projects).

The existing tool support for working with formal speciﬁcation includes editing,
interpreting, debugging and testing through various libraries and Integrated Develop-
ment Environments (IDEs) e. g., VDMTools[10], Overture [11], VDM-VSCode[16],

-2ex

20

Fig. 1: An abstract list of requirements are shown on the left side. These are interpreted
by the modeller, which is illustrated by the pictogram in the middle. The modeller
translates the interpreted requirements into a model, shown on the right side. Thereby,
performing the metaphoric jump.

Rodin[2], and VDMJ [3]. Testing of speciﬁcations can also be performed within VDMJ
by utilising combinatorial testing or VDMJUnit. However, no domain-agnostic tool ex-
ists which supports bridging the RSG.

In this paper, we show how and propose a tool to bridge that gap and ease the
translation task for modellers by assimilating and providing tool support for Behaviour-
Driven Modelling (BDM), a methodology inspired by the Behaviour-Driven Develop-
ment (BDD) agile approach. BDD prescribes a methodology to develop software focus-
ing on exemplifying requirements into concrete natural language examples, increasing
readability and reducing misunderstandings.

Behaviour-Driven Modelling adds a step between requirement elicitation and speci-
ﬁcation development, where requirements are translated into behaviours. The behaviours
are written in a constrained subset of natural language with the goal of becoming more
understandable by stakeholders. A prototype of a Visual Studio Code (VS Code) ex-
tension has been developed and is described throughout this paper. The prototype uses
Cucumber1 as a BDD test runner to discover and execute the behaviours. The tool maps
behaviours to concrete operations within a formal speciﬁcation. This mapping is made
possible using the VDMJ interpreter and VDMJ Annotations, such that operations can
be annotated within the VDM++ speciﬁcation language. The extension allows for a
modeller to create a BDM project and use the BDD test runner to validate behaviours
against the speciﬁcation. The goal of the extension is to be incorporated as part of the
Overture tooling support for the VDM languages.

We believe this methodology and tooling will contribute to closing the RSG and in-
crease readability of requirements for formal speciﬁcations, reduce the amount of mis-
understandings by stakeholders, and facilitate the process of developing formal speciﬁ-
cations in an agile process.

The outline of this paper is as follows: Section 2 gives an introduction to relevant
topics necessary to understand the paper. Section 3 and 4 describes the proposed ap-
proach and technical solution respectively. An example project uses the proposed so-
lution in section 5. The paper concludes upon the ﬁndings in section 6 together with a
paragraph describing the future work to deploy the tool in the VS Code marketplace.

1 https://cucumber.io/

Bridging the Requirements-Speciﬁcation Gap using BDD

21

2 Background

In agile development the workload is broken down into smaller pieces which can be
worked on simultaneously and iteratively[5]. This allows for smaller teams to work
together improving the performance of development[8]. Agile methods ensure that the
best practices, are applied and the correctness of the engineering processes is upheld.
Agile methods help both stakeholders and software engineers in building, deploying,
and maintaining complex software with its associated changing requirements [7].

BDD builds on the concepts of Test-Driven Development (TDD) with another ap-
proach to system analysis; how the different entities of a software system interact based
on the domain model[17]. BDD utilises a Domain Speciﬁc Language (DSL) for specify-
ing requirements, which are more readable for stakeholders. The speciﬁed requirements
are formulated as concrete examples of system behaviour2.

An agile development tool which supports BDD through acceptance testing is Cu-
cumber. It provides the DSL Gherkin, which is used to specify, in a restricted subset
of plain natural language, the desired behaviour of a system. A decisive advantage of
formulating behaviours using Gherkin is that not only is a stakeholder more likely to
understand it, but importantly a computer is able to. Cucumber aids developers in writ-
ing concrete examples of behaviours. This is accomplished by deﬁning a requirement
as a feature of a system. This feature is then exempliﬁed through a scenario as seen
in listing 1.1. By having concrete examples which deﬁnes acceptance tests allows for
easier discovery of edge cases[17]. Working with Cucumber and following the BDD
methodology gives developers an opportunity to get early feedback from the system.

F e a t u r e :

f e a t u r e −name
f e a t u r e − d e s c r i p t i o n

S c e n a r i o :

s c e n a r i o −name
Given a p r e − c o n d i t i o n
When an a c t i o n o c c u r s
Then p o s t − c o n d i t i o n i s

s a t i s f i e d

Listing 1.1: An example Gherkin feature and scenario

Cucumber groups scenarios in a feature to provide tracing of requirements for the
system. This tracing provides stakeholders with an overview of which requirements
that are currently implemented. Each scenario consists of steps which are either Given,
When or Then. These steps are conceptually equivalent to Hoare triples [6].

Given corresponds to a pre-condition, which in BDD also initialise the state. When
corresponds to an action that is performed. Then corresponds to a post-condition, which
is asserting if the new state is as expected. A table describing the differences and simi-
larities between Hoare triples and BDD steps can be seen in table 1.

In listing 1.2 a deﬁnition of a Given step is provided. This is deﬁned as a step deﬁni-
tion, which is a function that is mapped to a step within a scenario. For a step deﬁnition
to be mapped to a step, the function should be annotated with a step annotation. Cu-
cumber runs through each scenario and ﬁnds a matching annotated function for each

2 http://behaviour-driven.org/

-2ex

22

Cucumber

Pre-condition

Cucumber utilises the Given annotation
to identify the pre-condition of the scenario,
within the pre-condition the state is initialised.

Vienna Development Method
The pre-condition in VDM is utilised within
the operation scope noted by the pre keyword
at the end of the operation, VDM does not initialise any state.
VDM checks if the input adhere to a deﬁned constraint

Action

Post-condition

Cucumber utilises the When annotation
to identify the action performed by program
based on the state and the desired feature.
Cucumber utilises the Then annotation to identify
the post-condition of the scenario, within the post-condition
cucumber asserts on the state of the program to determine
if it veriﬁes the desired behaviour.

Within an operation of VDM, the model is acted on by
the input based on the state of the model.

The post-condition in VDM is utilised within
the operation scope noted by the post keyword
at the end of the operation, VDM veriﬁes that the output
of the function adheres to a deﬁned constraint

Table 1: Description of how the Hoare triples are applied in Cucumber and VDM

step. Cucumber executes this step deﬁnition and repeats this for all steps in a scenario.
A scenario passes if step deﬁnitions executes correctly and all assertions are satisﬁed.

@Given ( "A p r e − c o n d i t i o n " )

p u b l i c v o i d A _ p r e _ c o n d i t i o n ( ) {

p e n d i n g ( ) ;

}

Listing 1.2: An example Java step deﬁnition

Simon Fraser et al.(2021) presented an approach called Behaviour-Driven Spec-
iﬁcation (BDS)[5]. BDS applies BDD on formal speciﬁcations by validation of the
formal speciﬁcation against requirements formulated as Executable Acceptance Cri-
teria (EAC). The Azuki platform3 - a generic framework that assists in using BDS.
The framework aids in analysing legacy systems which results in human readable ac-
ceptance criteria. Furthermore, the framework creates EAC which it runs against the
speciﬁcation to verify the system. EAC are BDS equivalent to executable scenarios
from BDD. Mapping these directly to the behaviours of the desired system will result
in validation of the speciﬁcation against the requirements. The framework is designed
and developed to solve a speciﬁc problem case for a company as stated in Simon Fraser
et al.(2021)[5]. The framework has been developed constrained by a speciﬁc domain,
therefore, challenges will inherently arise when applying this framework more widely
and domain agnostic.

3 Behaviour-Driven Modelling

Traditional formal modelling consists of a modeller performing a transformation of
elicited requirements into an executable model. This is a two-step iterative process:
understand the requirement, then formulate it as a formal model. Performing the trans-
formation will map the requirements into a speciﬁcation which seeks to prove the prop-
erties and be used as a Quality Assurance (QA) metric of the system. The understanding
of the requirements and formulation of the speciﬁcation is at the mercy of the modeller.
This means that the properties and the QA is determined by the modeller. Attempts at
adapting this traditional formal approach through merging of formal methods and agile

3 https://github.com/anaplan-engineering/azuki

Bridging the Requirements-Speciﬁcation Gap using BDD

23

methodologies have been proven to be possible [19][13]. The results of the conceptual
approaches include an iterative process focusing on smaller parts of the system.

The conceptual difference between the traditional approach and the agile formal
modelling approach is shown in ﬁgure 2. In this paper, the Agile formal modelling
approach has been extended with tool support for the BDD approach, and the approach
will be described as BDM. BDM introduces the additional step which bridges the gap

Fig. 2: This ﬁgure shows the evolution of RSG. On the left is the traditional approach
of a direct transformation from requirements into a speciﬁcation. In the middle is the
conceptual depiction of the agile formal merging approach. On the right is the approach
introduced in this paper.

from requirements to the speciﬁcation within an agile approach as seen in ﬁgure 2.

The RSG is bridged by focusing on requirement analysis to deﬁne behaviours in
terms of features and scenarios. The behaviours act as guidance for both developers and
stakeholders to capture requirements as artifacts that can be traced as they are being
implemented in the formal model. The tracing allows for quality assurance teams to be
part of the early development process. It allows for modellers and developers to be part
of requirement elicitation in a way that can result directly in step deﬁnitions, which can
run against both the model as well as the production code.

The tooling for BDM focuses on requirement elicitation through stakeholder com-
munication to deﬁne behaviours of the desired system. The BDM approach is illustrated
in ﬁgure 3, ﬁrstly, the requirements elicitation is performed. After the requirements
have been elicited and analysed, the behaviours are deﬁned. These behaviours are then
mapped to step deﬁnitions. The BDM tool allows for mapping of the properties of the
model to the step deﬁnitions. Lastly, the speciﬁcation is validated against the require-
ments through these step deﬁnitions using a BDD test runner. The BDD test runner
performs the mapping and validation of behaviours against the speciﬁcation. This ap-
proach is then repeated for each requirement that is deﬁned and can be applied directly
in an agile process.

A screenshot of the developed tooling can be seen in ﬁgure 4. The ﬁgure shows the
tooling from the user’s perspective. At the current stage of the tooling, three areas are
of interest, these are noted with the three numbered red boxes on the ﬁgure:

-2ex

24

Fig. 3: This diagram shows an overview of the proposed BDM methodology. The pro-
cess repeats the steps illustrated until all requirements have been satisﬁed.

– Box #1 - Shows the project ﬁle structure depicting the location of the speciﬁcation

ﬁles, features, and step deﬁnitions.

– Box #2 - Shows the ﬁle that is currently being edited, in this case, it is a feature ﬁle

describing a scenario with different input values.

– Box #3 - Shows the output of running the executable scenario described in the

feature. Here, it shows that three tests have been executed successfully.

The execution of tests utilises VDMUnit to read and interpret the speciﬁcation to

validate it against the features.

Fig. 4: Screenshot of the VS Code extension.

4 Technical Solution

This section describes the tool developed to enable users to follow the approach de-
scribed in section 3. An overview of the proposed solution can be seen in ﬁgure 5 illus-
trating how the Cucumber test engine ﬁnds and executes scenarios through generation
of Java step deﬁnitions.

The user of the tool deﬁnes scenarios and VDM++ speciﬁcations. The VDMJ in-
terpreter performs type checking of the deﬁned VDM source. During this operation,

Bridging the Requirements-Speciﬁcation Gap using BDD

25

the tool looks for annotations for VDM deﬁnitions and generates Java step deﬁnitions
based on the annotations detected. The generated Java step deﬁnitions are provided to
the cucumber test engine together with user deﬁned scenarios. When the Cucumber test
engine executes a scenario with the corresponding generated java step deﬁnitions, it will
return either passed, failed, or pending. A pending test result means that the scenario is
not yet implemented.

Fig. 5: An overview of how the Cucumber test engine ﬁnds and executes scenarios and
step deﬁnitions created through the BDM tool.

To effectively apply the methodology, the tool provides support for the following
Cucumber annotations in VDM: Given, When and Then. Additionally, the tool also sup-
ports an annotation called StepDeﬁnition, which is used to annotate the VDM++ class
containing the operations annotated with the Cucumber speciﬁc annotations. These four
annotations provide the means to effectively deﬁne step deﬁnitions in the VDM++ lan-
guage according to the format that the Cucumber test engine can understand and exe-
cute.

A BDM project is required to conform to a deﬁned directory structure. The project
directory structure can be seen in ﬁgure 6 and shows how the different types of ﬁles
should be located. The root project folder contains two directories, src and target. The
target sub-directory contains generated ﬁles from the BDM project. The src folder con-
tains the VDM source ﬁles within the speciﬁcation sub-directory. The VDM source
ﬁles is both the speciﬁcation and step deﬁnitions. Java source ﬁles is located within the
Java sub-directory inside the Test directory. The only Java ﬁle for the project is RunCu-
cumberTest.java, which is required to run the Cucumber test engine. Scenario ﬁles is
located within the Resources sub-directory inside the Test directory.

A conceptual package diagram depicting the implementation and dependencies of
the tool developed can be seen in ﬁgure 7. The package diagram displays the BDMAn-
notations package containing the four different annotations: Given, When, Then, and
Step Definition and their dependency to BDM.Lib. This library contains utility
functionality through the VDMUtility class and functionality to build Java step def-
initions through the StepDeﬁnitionsBuilder. BDM.lib has a dependency to VDMJ and
VDMJUnit in order to read and interpret VDM++ speciﬁcations. Furthermore, the pack-
age has a dependency to Javassist to provide functionality that generates java classes at
runtime.

The tool extends upon the VDMJ annotations library to enable support for deﬁn-
ing step deﬁnitions in the VDM++ language that can be understood and executed with
the Cucumber test engine as the BDD test runner. The Cucumber test engine handles

-2ex

26

Fig. 6: An example of a BDM project structure.

Fig. 7: Package diagram depicting implementation and dependencies of the BDM tool.

discovery, selection, and execution of Cucumber scenarios. The extension of the VDMJ
Annotations library was implemented with respect to VDMJ Annotations documentation[3].
When an annotation is found the tool generates a corresponding Cucumber Java step
deﬁnition that uses the VDMJUnit library to read speciﬁcations and execute the corre-
sponding operations in the VDM++ step deﬁnitions [3].

Bridging the Requirements-Speciﬁcation Gap using BDD

27

The VDMJ Annotations library provides the ability to affect different aspects of the
VDMJ operation: the parsing, the type checking, the interpreting and the proof obliga-
tion generation. For the annotations described above, the VDMJ type checking opera-
tion is affected to generate step deﬁnitions in the Java language. When implementing
an extension to the VDMJ Annotations, it is required to know which VDM constructs
that should be annotated and whether the functionality should be executed before or af-
ter the VDMJ operation. The developed tool performs the generation of classes before
the VDMJ type checker performs its operation. To illustrate the steps performed by the
tool, a sequence diagram can be seen in ﬁgure 8 which displays the steps performed
when a Given annotation is found.

To build step deﬁnitions in the Java language, a third-party library is used called
Javassist. This enables bytecode manipulation, meaning that Javassist can perform al-
terations and creations of programs or constructs within a program such as a class[4].
The created Java step deﬁnition is generated through the Javassist library and imple-
ments the Cucumber step deﬁnition functions that is executed by the Cucumber test
engine. To effectively read speciﬁcations and execute functions or operations deﬁned
in a VDM++ model, the library VDMJUnit is used. The created Cucumber Java step
deﬁnitions inherits from the VDMJUnitTestPP Java class and implements the Given,
When, and Then step deﬁnition functions. The functions are annotated as speciﬁed in
the corresponding VDM++ annotations.

Fig. 8: Sequence diagram depicting the steps performed by the tool for the Given anno-
tation.

When a Given annotation is found by VDMJ, it calls the PreCheck function from
the StepDeﬁnitionBuilder inside the BDM.Lib library. The function performs validation
of the input values to ensure that the operation name and parameters are deﬁned as
expected by the Given annotation. The input values are:

– def - The deﬁnition from the VDMJ type checker.
– name - The name of the Cucumber behaviour, used to map requirements and step

deﬁnitions together.

– fName - The name of the function annotated.
– Type - The type of Cucumber annotation.

-2ex

28

When the input values have been validated, the builder then generates the Java step
deﬁnition function within the step deﬁnition class. The builder makes sure that the gen-
erated function is annotated correctly according to the standards given by the Cucumber
test engine.

The prototype for the tool is developed as a VS Code extension which handles
creation of BDM projects and execution of cucumber tests. The architecture for the
extension can be seen in ﬁgure 9. A detailed description of the architecture can be seen
in section 6.

5 Demonstration

To demonstrate the developed tool and approach described throughout this paper, an
example project is used. The example is a very simple project inspired by the Cucumber
"10 Minute Tutorial"4. The requirements for the project are deﬁned as the following:

– R1 - The system must output "TGIF" if today is Friday.
– R2 - The system must output "Nope" if today is not Friday.

The ﬁrst step in the proposed approach is to deﬁne behaviours based upon these
requirements. These behaviours are deﬁned according to the traditional BDD approach,
by deﬁning Cucumber scenarios within a feature ﬁle, and can be seen in listing 1.3.

4 https://cucumber.io/docs/guides/10-minute-tutorial/

Bridging the Requirements-Speciﬁcation Gap using BDD

29

F e a t u r e :

I s
E v e r y b o d y w a n t s

i t F r i d a y y e t ?

t o know when i t ’ s F r i d a y

i s n o t F r i d a y

i t ’ s F r i d a y y e t

s h o u l d be t o l d "<answer>"

S c e n a r i o O u t l i n e : Today i s o r
Given t o d a y i s "<day>"
When I a s k w h e t h e r
Then I
Examples :
| day
| F r i d a y
| Sunday
|

|
| TGIF
| Nope
| Nope

a n y t h i n g e l s e !

a n s w e r

|
|
|
|

Listing 1.3: The Cucumber behaviour deﬁnition for the case project.

The next step involves deﬁning step deﬁnition operations in VDM++, that should
be annotated with one of the step deﬁnition operations from the deﬁned behaviour, such
as Given, When, or Then. The annotation takes two parameters; the ﬁrst is the name of
the temporary variable that is used to maintain the state of the object, and the second is
a string containing the text deﬁned for the step deﬁnition operation within the feature
ﬁle. On listing 1.4, the step deﬁnition operations can be seen for the deﬁned behaviour.
An annotation is written according to the VDMJ documentation, within a comment
and with an "@" as a preﬁx. The assertion of the behaviour happens in the operation
annotated with Then, where the tool will assert that the post condition is satisﬁed. If the
post condition is satisﬁed the test will succeed, if not, it will fail.

−− @Given ( " f 1 " , " t o d a y i s { s t r i n g } " )
p u b l i c T o d a y _ i s :
s e q o f c h a r ==> ( )
T o d a y _ i s ( s t r ) ==
(

f r i d a y : = new F r i d a y ( s t r ) ;

) ;
−− @When ( " f 1 " , " I a s k w h e t h e r
p u b l i c
I _ a s k _ w h e t h e r _ i t _ i s _ f r i d a y ( ) ==
(

I _ a s k _ w h e t h e r _ i t _ i s _ f r i d a y :

i t ’ s F r i d a y y e t " )

( ) ==> ( )

a c t u a l R e s u l t

: = f r i d a y . I s I t F r i d a y ( ) ;

) ;
−− @Then ( " f 1 " , " I
p u b l i c R e s u l t i n g T h e n :
R e s u l t i n g T h e n ( e x p e c t e d ) ==

s h o u l d be t o l d { s t r i n g } " )
s e q o f c h a r ==> b o o l

r e t u r n e x p e c t e d = a c t u a l R e s u l t

p o s t RESULT = t r u e ;

Listing 1.4: Step deﬁnition operations for the deﬁned behaviours written in VDM++

When the Cucumber test engine execute the tests generated based on the behaviour
descriptions, it will ﬁnd and execute these VDM++ operations through a generated Java
step deﬁnition class that utilises VDMJUnit to access the VDM++ step deﬁnition oper-
ations. The generated Java functions can be seen on listing 1.5. Each of the functions

-2ex

30

contains a call to a helper function called checkLocalVariable, this function checks
whether the local variable that are given as parameter actually exists. If the variable
does not exist, the function creates it.

@Given ( " t o d a y i s { s t r i n g } " )

p u b l i c v o i d T o d a y _ i s ( S t r i n g p a r a m S t r i n g ) {

c h e c k L o c a l V a r i a b l e ( ) ;
r u n ( " f 1 . T o d a y _ i s ( \ " " + p a r a m S t r i n g + " \ " " + " )

" ) ;

}

@When( " I a s k w h e t h e r

i t ’ s F r i d a y y e t " )

p u b l i c v o i d I _ a s k _ w h e t h e r _ i t _ i s _ f r i d a y ( ) {

c h e c k L o c a l V a r i a b l e ( ) ;
r u n ( " f 1 . I _ a s k _ w h e t h e r _ i t _ i s _ f r i d a y ( )

" ) ;

}

@Then ( " I

s h o u l d be t o l d { s t r i n g } " )

p u b l i c v o i d R e s u l t i n g T h e n ( S t r i n g p a r a m S t r i n g ) {

c h e c k L o c a l V a r i a b l e ( ) ;
r u n ( " f 1 . R e s u l t i n g T h e n ( \ " " + p a r a m S t r i n g + " \ " " + " )

" ) ;

}

Listing 1.5: Step deﬁnition functions for the deﬁned behaviours written in Java

The result of utilising this approach is the ability to deﬁne requirements in a more
stakeholder-readable language, which are converted to executable scenarios using step
deﬁnition annotations. These executable scenarios are executed by the Cucumber for
Java test engine. This tool and approach make it possible to perform BDM of VDM++
speciﬁcations.

6 Concluding Remarks and Future Work

This paper introduces the BDM approach to bridge the RSG by extending the traditional
two steps with an extra step. The additional step includes deﬁnition of behaviours based
on requirements. To ease the translation task, a tool is introduced that supports BDM.

The tool described in section 4 is still under development but, at its current state,
it is possible to use it to perform BDM as described in section 5. The current state of
the tool is the ability to deﬁne step deﬁnition operations within a VDM++ class and
use the Cucumber test runner to execute these operations. The tool requires a deﬁned
directory structure to locate speciﬁcation ﬁles and feature ﬁles. A minimal VS Code
[14][15] extension have been developed to generate a new BDM project that conforms
to the requirements of the tool. The extension also supports execution of tests through
the Cucumber test engine.

BDM supports formal validation through testing of operations in the deﬁned spec-
iﬁcation. BDM extends this by supporting validation of the requirements through the
tool support described in section 4. Given that the testing is handled through the Cu-
cumber test engine allows for validation of production code corresponding to the formal
speciﬁcation - with respect to constraints. The testing of the formal speciﬁcation can-
not run directly on production code, however, the behaviours can be mapped to step
deﬁnitions whereas both the production code and the formal speciﬁcation satisﬁes the

Bridging the Requirements-Speciﬁcation Gap using BDD

31

requirements. It is noteworthy to mention that this approach does not eliminate the need
for veriﬁcation through unit testing of individual elements of a formal speciﬁcation.

Feature support

Traditional approach

Supported

Formal speciﬁcation
Iterative process
Early feedback
Tool support for
speciﬁcation validation
Tool support for
implementation validation

Agile Merging Methodology.
e.g. Scrums goes formal, FormAgi..
Supported
Supported
Supported

Behaviour-Driven Modelling

Supported
Supported
Supported

Supported

Table 2: Feature supported by different approaches.

The approach presented in this paper differs from previous attempts at bridging
the gap, in a few different ways. BDM obtains the same advantages as the approaches
which merge the agile methods and formal methods. BDM utilises the iterative nature
from BDD, which other approaches have gained from e. g. SCRUM [19]. The early
feedback and validation of BDD is still maintained with BDM. An overview of the fea-
tures supported by BDM is shown in table 2. However, BDS applies a similar approach
as BDM. Both methodologies applies BDD for development of formal speciﬁcations.
The main difference stems from their associated tool, which have been developed for
different intended users and problem domains.

A valuable extension of the developed tooling, would include the functionality of
validating formal speciﬁcation and the corresponding implementation against the same
requirements. The extension would aid in the process of discovering differences be-
tween them. The consequences of a change in requirement would be accentuated in
the validation process for formal speciﬁcation and implementation. This would provide
the stakeholders with insight about the sometimes costly consequences of changing re-
quirements.

The proposed solution described by this paper is not the only solution for the prob-
lem deﬁnition. An alternative solution would be to implement native support for the
VDM dialects directly within Gherkin and Cucumber. This would eliminate the depen-
dencies for generation of Java classes that can be understood by the Cucumber Java tool.
This solution was considered when developing the proposed solution but were deemed
too comprehensive. This paper have introduced the methodology of BDM and shown
that it can be applied to a simple case project. More research is needed in order to de-
termine the feasibility of the methodology and associated tool for bridging the RSG.
This research should include more case studies on more complex projects in order to
gain insight in the scope of applicability and the limitations of adapting BDM. How-
ever, the presented methodology attempts at deﬁning a formal process for translation of
requirements into formal speciﬁcations.

The tool itself is under development with several features in the pipeline. As men-
tioned above, a VS Code extension is being developed to provide project creation and
testing capabilities within VS Code. The extension utilises a modiﬁed Maven Archetype5,
which has been modiﬁed to create a new project that has the necessary structure and ﬁles
to use BDM. This project structure can be seen in ﬁgure 6. The testing capabilities are

5 https://maven.apache.org/guides/introduction/introduction-to-archetypes.html

-2ex

32

Fig. 9: Overview of the VS Code extension architecture.

handled in this extension through a modiﬁed maven test command. An overview of the
architecture of this extension is shown in ﬁgure 9. The current version of the extension
supports the white elements on the ﬁgure, the package BDM.lib and the element named
BDM Annotations are described in section 4. The yellow boxes are third party systems.
The extension is not yet ready for release while writing this submission but we expect it
to be complete by the time of the workshop. The extension should be extended with user
interface buttons and a testing view, which are depicted in pink in ﬁgure 9, since the ex-
tension only provides its functionality through commands executed in the terminal and
through the VS Code command line interface. The extension should also provide the
user with snippets to step deﬁnition since this is standard practice within the Cucumber
tooling. Additionally, the tool should be extended to support the two other dialects of
the VDM; VDM-SL and VDM-RT, the tool only supports the VDM++ dialect.

Acknowledgments We would like to thank Simon Fraser and Alessandro Pezzoni for
inspiring us to approach agile formal methods with BDD and for providing information
about the Azuki framework. We would also like to thank the Overture community for
providing information about extension development in VS Code and the VDMJ library.

Bridging the Requirements-Speciﬁcation Gap using BDD

33

References

1. Abrial, J.R., Abrial, J.R.: The B-book: assigning programs to meanings. Cambridge Univer-

sity Press (2005)

2. Abrial, J.R., Butler, M., Hallerstede, S., Hoang, T.S., Mehta, F., Voisin, L.: Rodin: an open

toolset for modelling and reasoning in Event-B. STTT 12(6), 447–466 (2010)

3. Battle, N.: VDMJ User Guide. Tech. rep., Fujitsu Services Ltd., UK (2009)
4. Chiba, S.: Load-time structural reﬂection in java. In: European Conference on Object-

Oriented Programming. pp. 313–336. Springer (2000)

5. Fraser, S., Pezzoni, A.: Behaviour driven speciﬁcation. In: Macedo, H.D., Thule, C., Pierce,
K. (eds.) Proceedings of the 19th International Overture Workshop. Overture (10 2021)
6. Hoare, C.: An axiomatic basis for computer programming. Communications of the ACM

12(10), 576–581 (October 1969)

7. Hoda, R., Salleh, N., Grundy, J.: The rise and evolution of agile software development. IEEE

Software 35(5), 58–63 (2018)

8. Hoegl, M.: Smaller teams–better teamwork: How to keep project teams small. Business
Horizons 48(3), 209–214 (2005), https://www.sciencedirect.com/science/
article/pii/S0007681304001120

9. Jones, C.B.: Systematic software development using VDM, vol. 2. Prentice Hall (1990)
10. Larsen, P.G.: Ten Years of Historical Development: “Bootstrapping” VDMTools. Journal of

Universal Computer Science 7(8), 692–709 (2001)

11. Larsen, P.G., Battle, N., Ferreira, M., Fitzgerald, J., Lausdahl, K., Verhoef, M.: The Overture
Initiative – Integrating Tools for VDM. SIGSOFT Softw. Eng. Notes 35(1), 1–6 (January
2010), http://doi.acm.org/10.1145/1668862.1668864

12. Macedo, H.D., Larsen, P.G., Fitzgerald, J.: Incremental Development of a Distributed Real-
Time Model of a Cardiac Pacing System using VDM. In: Cuellar, J., Maibaum, T., Sere,
K. (eds.) FM 2008: Formal Methods, 15th International Symposium on Formal Methods.
Lecture Notes in Computer Science, vol. 5014, pp. 181–197. Springer-Verlag (2008)

13. Olszewska, M., Ostroumov, S., Waldén, M.: Using scrum to develop a formal model–an
experience report. In: International Conference on Product-Focused Software Process Im-
provement. pp. 621–626. Springer (2016)

14. Rask, J.K., Madsen, F.P.: Decoupling of Core Analysis Support for Speciﬁcation Languages
from User Interfaces in Integrated Development Environments. Master’s thesis, Aarhus Uni-
versity, Department of Engineering (December 2020)

15. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: The Speciﬁcation Language
Server Protocol: A Proposal for Standardised LSP Extensions. In: Proença, J., Paskevich, A.
(eds.) Proceedings of the 6th Workshop on Formal Integrated Development Environment,
Held online, 24-25th May 2021. Electronic Proceedings in Theoretical Computer Science,
vol. 338, pp. 3–18. Open Publishing Association

16. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: Visual Studio Code VDM
Support. In: Fitzgerald, J.S., Oda, T. (eds.) Proceedings of the 18th International Overture
Workshop. pp. 35–49. Overture (December 2020), https://arxiv.org/abs/2101.
07261

17. Rose, S., Wynne, M., Hellesoy, A.: The cucumber for Java book: Behaviour-driven develop-

ment for testers and developers. Pragmatic Bookshelf (2015)

18. Spivey, M.: The Z Notation – A Reference Manual (Second Edition). Prentice-Hall Interna-

tional (1992)

19. Wolff, S.: Scrum Goes Formal: Agile Methods for Safety-Critical Systems. In: ICSE 2012:
Proceedings of the 34th International Conference on Software Engineering. pp. 23–29 (June
2012), Workshop on Formal Methods in Software Engineering: Rigorous and Agile Ap-
proaches, FormSERA 2012

Advanced VDM Support in Visual Studio Code

Jonas Kjær Rask1, Frederik Palludan Madsen1, Nick Battle2, Leo Freitas3, Hugo
Daniel Macedo1, and Peter Gorm Larsen1

1 DIGIT, Aarhus University, Department of Engineering,
Finlandsgade 22, 8200 Aarhus N, Denmark
{jkr, fpm, hdm, pgl}@ece.au.dk
2 Independent, nick.battle@acm.org
3 Newcastle University, leo.freitas@newcastle.ac.uk

Abstract. The VDM VSCode extension has made its way to the daily practice of
several engineers and students. Recent development furnished the extension with
many of the features previously only available in the Eclipse based tools. With the
new additions, we are able to label the VDM VSCode extension to be the most up
to date, preferred and recommended tool support for the VDM practitioner. In ad-
dition to keeping up with the previous features, recent developments brought new
ones available in VSCode only (e.g.: Code Lenses), advances in proof support,
and advances on the interpreter that equip the user with a modern tool suite. This
paper provides an update on the current features and lays down the discussion of
the future developments.

Keywords: Overture, IDEs, VDM, Language Server Protocol, Debug Adapter Proto-
col

1

Introduction

The Vienna Development Method (VDM) is one of the approaches to follow, when ap-
plying formal methods during the development of computer systems. The method has
been widely used both in industrial contexts and academic ones covering several do-
mains of the ﬁeld: Security [7][8], Fault-Tolerance [11], Medical Devices [10], among
others, and its application /acceptance depends on usable and extendable tool support.
The standard open-source tool support for VDM, the Overture tool, has migrated
from an Eclipse rich-client platform application to a Visual Studio Code (VS Code)
extension, which supported basic editor features as reported in [14]. As described in
[13], VS Code is an editor, not an Integrated Development Environment (IDE), yet with
the introduction of the Language Server Protocol (LSP) and the Debug Adapter Protocol
(DAP), the editor becomes indistinguishable from an IDE. Our implementation of those
protocols is thoroughly described in [12] with a comparison of legacy features in the
Eclipse based tool and the ones in the new VS Code extension.

The prototype matured into a stable tool which was enriched with both the missing
legacy features, proof-support and modern features. We report on the new features, and
claim that, with the additions, the VDM VSCode extension has become a fully ﬂedged
VDM tool support. In addition to the new features, the claim is based on the fact that the

Advanced VDM Support in Visual Studio Code

35

extension transitioned from prototype to classroom and daily practice, which is reﬂected
on its usage metrics (e.g. the extension has over a thousand installs).

In this paper we report on the recent addition of previously existing features in the
Eclipse based tooling and missing in the VDM VSCode extension (legacy features),
namely:

– Import Examples - Allowing quick demos and facilitating training (e.g.: classroom

exercises).

– Java Code Generation - The conversion of a VDM object oriented dialects speciﬁ-

cation into a Java project.

– Add Library - The possibility to select and add libraries to a VDM project.
– Remote Control - Extending the console based running and debugging with GUIs.
– Test Coverage - Highlights the parts of the speciﬁcation that has been exercised.
– Proof Support - The option to generate the boilerplate Isabelle theory and spec

deﬁnitions to be used in proofs.

Moreover, we have also augmented the features available to VDM practitioners, namely:

– Code snippets - Provide a template/skeleton of a speciﬁcation primitive by typing

a preﬁx and a triggering the features.

– Code Lenses - A popular feature in Visual Studio Code that we use to allow the
launching and debugging of speciﬁcation components by clicking on links that are
interspersed throughout the speciﬁcation.

– Dependency Graph Generation - Providing the visualisation of the class depen-

dency of a object oriented VDM dialect in graphical form.

Given the completion of added legacy features and its transitioning into practice, we
are now able to declare the VDM VSCode extension as a de facto tool support for VDM.
The outline of this paper is as follows: Section 2 provides an overview of the current
VS Code extension components and features. Section 3 describes the advanced features
added to the initial extension as reported in [14][12][13]. Section 4 contains the details
of the recently added proof support. Section 5 provides insight on the changes made
to the previous language server to support the new/modern editing features. Finally, in
Section 6, we report on the insights and work done so far, while laying out the features
under implementation.

2 Background

In this section the VDM VSCode extension is described and in this context VS Code
is also brieﬂy introduced. Lastly, the implementation of the language feature support in
the extension is made tangible by a concrete example.

2.1 The VDM VSCode Extension

The VDM VSCode extension is developed for the rich extensibility model found in
the free source code editor VS Code which is designed to run on multiple platforms
including web and to be lightweight with additional functionality being added by the

-2ex

36

user through extensions. Amongst other functionality an extension can be used to pro-
vide support for a given programming language through the use of a language server
that adheres to the LSP protocol and potentially also the DAP protocol if debugging
capabilities are needed. Both protocols are developed by Microsoft in relation to VS
Code which has native support for much of the front-end parts of the programming lan-
guage features that are supported by the protocols such as debugging, type checking,
go-to deﬁnition and more. Thus, VS Code is a perfect candidate for a modern develop-
ment tool for VDM. The VDM VSCode extension can, as other extensions, be installed
directly from within VS Code itself which enables the user to quickly and easily add
support for VDM in the editor.

The extension is continually being developed and since it was last presented [14] a
number of new features and functionality have been made available to the users of the
extension. Its core design implements a client-server architecture that uses the Speciﬁ-
cation Language Server Protocol (SLSP)[13], LSP, and DAP protocols for communica-
tion. With the SLSP protocol being an extension to the LSP protocol to enable support
for speciﬁcation language features.

The client implementation in the extension is tightly coupled to VSCode, whereas
the server implementation is fully decoupled from the client. The VDM language sup-
port is provided by the server as an implementation of VDMJ[1] with extended capabil-
ities to support the aforementioned protocols. As a result, the server can be reused for
any clients that support the protocols, and vice versa. Figure 1 provides an overview of
the high level components in the current extension architecture.

Fig. 1: Architecture of the VDM VSCode extension components.

Following is a short description highlighting the purpose and composition of each

of the high level components found in Figure 1.

– Visual Studio Code:

• Generic DAP Client: Native DAP component in VSCode. It handles the inte-
gration of the features enabled by the DAP protocol and provides the messaging
infrastructure to handle the protocol on the client side.

Visual Studio CodeVDM Language SupportVDMJSLSP/DAP SupportSLSP ClientSyntaxHighlightingSLSP ServerDebug AdapterVDM LanguageFeature SupportVDM DebuggerGeneric LSP  ClientGeneric DAPClientLSP + SLSPProtocolDAP ProtocolProtocolVDM ExtensionFeaturesVDM DAPSupportViewsClient managerHandlersAdvanced VDM Support in Visual Studio Code

37

• VDM DAP Support: Extends the Generic DAP Client to connect to the De-
bug Adapter on the server and handles any speciﬁc logic for initiating a VDM
debugging session.

• Generic LSP Client: Native LSP component in VSCode. Handles the integra-
tion of the features enabled by the LSP protocol and provides the messaging
infrastructure to handle the protocol on the client side.

• SLSP Client: Extends the Generic LSP Client to support features that utilises

the SLSP protocol.

• Client Manager: Manages client instances and features for each workspace that

the user has opened in a given session.

• Features: A collection of language feature functionality components that utilises
the LSP protocol. This includes features such as Proof Obligation Generation
(POG), Combinatorial Testing (CT) and language translations.

• Views: A collection of view logic for each feature component where user in-
teraction is necessary. These does not directly depend on the SLSP protocol.

• Protocol: Logic that describes the SLSP protocol messages.
• Handlers: Contains all the components to support functionality that does not
directly rely on the SLSP protocol. This includes functionality such as Java
code generation, displaying code coverage, adding VDM libraries and more.
• Syntax Highlighting: Uses TextMate grammars to highlight VDM keywords.

– VDM Language Support:

• Debug Adapter: Wraps the VDM Debugger to provide the debug functionality

using the DAP protocol.

• VDM Debugger: Provides the VDM debugger functionality.
• SLSP Server: Wraps the VDMJ language support to provide it using the SLSP

and LSP protocols.

• VDM Language Feature Support: Provides the VDM language support func-

tionality.

2.2 SLSP Client Features

The client side architecture of the extension is designed such that feature functionality
which rely on the SLSP protocol is decoupled from the protocol itself. Thus, the inter-
face elements (buttons, web views, tree views) and their supporting logic in the client
are not directly coupled to a client-server architecture.

A concrete example of the decoupling is provided in Figure 2 which shows a class
diagram for the POG feature, with the other SLSP features of the extension following
the same design.

Following is a description of key elements of the class diagram relevant for under-
standing the decoupling and also the integration of SLSP features in the client in more
technical detail.

As depicted in the diagram the class extension has an instance of the class
ClientManager which in turn has multiple instances, one for each workspace, of
the class SpecificationLangaugeClient. This class extends the VS Code class
LangaugeClient to enable the use of the functionality that is provided by the VS
Code API. The multiplicity is needed since a client and server instance is created for

-2ex

38

each workspace folder that is opened in the editor. In the LangaugeClient class it is
possible to register features that implement one of the interfaces StaticFeature or
DynamicFeature. Both of these interfaces describe the functions that must be im-
plemented to handle the initialisation of the feature between client and server. However,
the features that are supported by the SLSP protocol currently only supports static reg-
istration, hence they implement the StaticFeature interface. For the POG feature
this is implemented in the class ProofobligationGenerationFeature which
handles communication to and from the server and is responsible for providing a data
provider for the ProofObligationPanel class which in turn provides the Graphi-
cal User Interface (GUI) elements to be displayed in the POG view in the editor. Only
one instance of the ProofObligationPanel class is created by the extension, as all
the workspace folders share the same Proof Obligation (PO) view. To be able to support
all the workspace folders the panel can have multiple ProofObligationProviders
(one for each workspace folder) which provide the data that is shown in the PO view and
subscribe to events that are associated with the feature. Finally, the ProofObligationPanel
is also responsible for registering the handlers for the commands relating to POG that
the VDM VScode extension provides.

Fig. 2: Class diagram for the POG feature. For several of the classes, only the functions
that are mainly used or help in understanding the diagram are included in the class
description.

LanguageClient+ start(): Disposable+ stop(): Promise<void>+ sendRequest<R>(RequestType, CancellationToken): Promise<R>+ sendNotification(ProtocolNotificationType): void+ onNotification(NotificationType, NotificationHandler): Disposable+ registerFeature(StaticFeature | DynamicFeature): void...SpecificationLanguageClient+ registerBuiltinFeatures(): void <<interface>>StaticFeature+ fillInitializeParams(InitializeParams)?: void+ fillClientCapabilities(ClientCapabilities): void+ initialize(ServerCapabilities, DocumentSelector): void+ dispose(): void<<Provides>>ProofObligationGenerationFeature- requestPOG(Uri): Promise<CodeProofObligation[]>- onPogUpdatedNotification(POGUpdatedParams): void...<<interface>>ProofObligationProvider+ onDidChangeProofObligations: Event<boolean>+ provideProofObligations(Uri): Thenable<ProofObligation[]><<Uses>>ProofObligationPanel+ registerProofObligationProvider(DocumentSelector,ProofObligationProvider): Disposable# onRunPog(Uri): void# onUpdate(boolean): void# createWebView(WorkspaceFolder?): void...extension+ activate(ExtensionContext): void+ deactivate(): Thenable<void>1*11111*<<namespace>>vscode.commands+ registerCommand(command: string,callback: any => void): Disposable+ executeCommand<T>(command: string, ...rest: any[]): Thenable<T>...<<Uses>>111111ClientManager+ launchClient(): void + stopClient(): void11Provided by the VS Code APIAdvanced VDM Support in Visual Studio Code

39

3 Adding Legacy Features

Import Examples This addition makes it possible to automatically import project from
a large collection of existing examples. As it was the case in the previous Eclipse based
IDE, users can experiment with a running example project.

Java Code Generation It is possible to generate Java code for a large subset of VDM-
SL and VDM++ models. In addition to Java, C and C++ code generators are currently
being developed. Both these code generators are in the early stages of development. For
comparison, code generation of VDM-SL and VDM++ speciﬁcations to both Java and
C++ is a feature that is available in VDMTools [Java2VDMMan, CGMan, CGManPP].
The majority of this chapter focuses solely on the Java code generator avail- able in
VDM VSCode Extension.

Add Library Support It is possible to add existing standard libraries. This can be done
by right-clicking on the Explorer view where the library is to be added and then select-
ing Add VDM Library. That will make a new window as shown in Figure 4.2. Here the
different standard libraries provide different standard functionalities.

Remote Control In some situations, it may be valuable to establish a front end (for ex-
ample a GUI or a test har- ness) for calling a VDM model. This feature corresponds
roughly to the CORBA based API from VDMTools [APIMan]. Remote control should
be understood as a delegation of control of the interpreter, which means that the re-
mote controller is in charge of the execution or debug session and is responsible for
taking action and executing parts of the VDM model when needed. When ﬁnished, it
should return and the session will stop. When a Remote controller is used, the Over-
ture debugger continues working normally, so for example breakpoints can be used in
debug mode. Moreover, all dialects (VDM-SL, VDM++ and VDM-RT) support Re-
mote Control. A new conﬁguration with the use of a remote controller can be started by
(see Figure 13.1 for more details): 1. Clicking on the button ”Add Conﬁguration...” 2.
Selecting ”VDM Debug: Remote Control (VDM-SL/++/RT) Then a new snippet (see
Figure 13.2) will be created with the remoteControl option. And you simply have to
write the full package/class name of the Remote Control.

Code snippets VSCode templates can be particularly useful when you are new to writ-
ing VDM models. If you press CTRL+space after typing the ﬁrst few characters of a
template name, Overture will offer a proposal. For example, if you type ”fun” followed
by CTRL+space, the IDE will propose the use of an implicit or explicit function tem-
plate as shown in Figure 5.1. The IDE includes several templates: cases, quantiﬁcations,
functions (explicit/implicit), operations (explicit/implicit) and many more. The use of
templates makes it much easier for users to create models, even if they are not deeply
familiar with the VDM syntax

Code Lenses To make execution and debugging easy using the extension we have added
code lenses for all public operations and functions. These are shown above their re-
spective deﬁnitions. When pressing Launch or Debug a launch conﬁguration is gener-
ated and launched immediately. If the operation/function has parameters you will be

-2ex

40

prompted to input these, the same applies if the class that the operation belongs to has
a constructor that takes parameters.

Example of a lens-generated launch conﬁguration

{

}

"name": "Lens config: Debug Test1‘Run",
"type": "vdm",
"request": "launch",
"noDebug": false,
"defaultName": "Test1",
"command": "p new Test1().Run()"

Notice that the name starts with Lens conﬁg: if you remove this the launch conﬁg-

uration will not be overwritten the next time you activate a lens.

4 Proof Support within the VSCode Extension

Original proof support for VDM existed within the Mural [6] theorem proving system.
Since then, theoretical development demonstrated that it was possible to soundly prove
VDM theorems in other logical systems [16][17]. Moreover, a translation strategy from
VDM to Isabelle/HOL4 was developed as part of a deliverable in the AI4FM project led
by Cliff Jones [4]. This includes extended proof support for VDM-style expressions.

The current VDM proof support stems from the combination of these theoretical re-
sults and Isabelle translation strategy5. It comprises four parts: 1) VDM VSCode plugin
integration; 2) VDMJ plugins extension; 3) VDMJ proof annotations; and 4) Isabelle
VDM toolkit.

4.1 VDM VSCode Proof Support Plugin

VSCode proof support plugin hooks with the translation feature described in Sec-
tion 6.1. It provides access to VDMJ’s (native) plugins, through the language server
console as an additional user-command, as well as through the IDE menu. The VS-
Code proof plugin also provides translation and proof support conﬁguration informa-
tion available for users to choose. Practically, it is a wrapping interface to the native
VDMJ plugin described next. This separation of concerns is crucial for maintenance
and stability of the tool chain.

4.2 VDMJ Plugins Extension

Through the LSP, users can access the VDMJ native plugins interface. This gives ac-
cess to the VDM AST, typechecker, POG and other tools. The VDM to Isabelle/HOL
translation plugin is divided in four parts:

4 https://isabelle.in.tum.de
5 https://github.com/leouk/VDM_Toolkit

Advanced VDM Support in Visual Studio Code

41

1. exu. This plugin analyses a type checked VDM AST looking for: a) unsupported
Isabelle requires declaration before use); b) speciﬁcation consistency
features (e. g.,
(e. g., function call graphs ought to also participate in the corresponding function spec-
iﬁcation); and c) “proof-friendly” VDM style (e. g., keep type invariants compartmen-
talised and as close to their deﬁning type as possible), which enables higher automation
in predicting how proof unfolding will take place. These checks ensure that there will
be no Isabelle type errors as a result of translation. They also help improve proof script
automation. For example, if a function f calls g, then its good practice that f’s precon-
dition also call g’s precondition. Of course, this might be spurious, yet can increase
proof automation, if present. Similarly, instead of deﬁning type for a numeric sequence
ensuring all elements are negative, it is preferable to deﬁne a type for negative numbers
then make a sequence of that.

2. vdm2isa. This plugin compiles a type checked VDM AST into a Isabelle/HOL
translation tree, which is then executed to generate the corresponding Isabelle theory
ﬁle. Its execution does not depend on exu, yet lingering exu errors/warnings will en-
tail vdm2isa errors and likely Isabelle/HOL type errors. This stage of the translation
strategy transform each VDM top-level deﬁnition (e. g., types, functions, etc.) to their
corresponding Isabelle construct. That is, for every VDM module M, a correspond-
ing Isabelle theory ﬁle M.thy is generated. Moreover, it also ensures that the (implicit)
VDM rules are checked within the translated Isabelle speciﬁcation. For example, type
invariants are implicitly checked as part of the precondition of translated functions.

3. isapog. This plugin compiles the POG POs into corresponding Isabelle theorems to
be proved. Beyond translating the PO predicate itself, the compilation also groups POs
within Isabelle local context principle (i. e., locale) to ensure that all proofs within that
context must be discharged, and to compartmentalise/stratify their proof scripts. That
is, for every translated Isabelle theory ﬁle M.thy from VDM module M, a further Isabelle
theory ﬁle M_PO.thy is generated. The plugin also enables the user to extend the POG
with lemmas that might be useful for discharging POs. This is achieved through VDM
annotations, described in the next subsection.

4. isaproof. This plugin analyses the VDM translation tree to generate tentative proof
scripts for each of the isapog-generated POs. That is, for every VDM POs Isabelle the-
ory ﬁle M_PO.thy, a proof strategy theory ﬁle M_PS.thy is generated containing tenta-
tive proof scripts for each of the POG POs. This obviously imply the plugin depends on
successful execution of both the vdm2isa and isapog plugins to access translated VDM
deﬁnitions and POs.

4.3 VDM Annotations

VDMJ allows the user-deﬁned annotations. These can be processed by either the parser,
type-checker, interpreter or POG. For proof support, we include two new types of an-
notations:

– @Theorem and @Lemma.

-2ex

42

These annotations enable the user to extend the VDMJ POG with new user-deﬁned
proof obligations about the model being developed. There is no semantic difference
between lemmas and theorems (i. e., they are both boolean expressions that must be
true). Theorem expressions must have a unique (global) name and be type correct.
If the expression is executable, the interpreter will determine whether the theorem
is true (or not); otherwise, the POG will generate the named PO associated with the
theorem.
Beyond documenting speciﬁc properties wanted of the model, user-deﬁned lemmas
can also be useful to document stepping stones in the later proofs associated with
either POG POs or user-deﬁned theorems. These new POs are processed by the
isapog and isaproof plugins, as described above.

– @Witness.

This annotation enables explicit user-deﬁned values for types, or speciﬁc function/-
operation calls. The type checker ensures that witness expressions are correct, and
the interpreter evaluate them to ascertain whether the witness is valid (e. g., a pos-
itive number as a witness for a new nat sub-type). That is, the witness chosen is
executable and satisfy any associated speciﬁcation [5].
These witness expressions can then be used in proofs involving existential intro-
duction. That is, a witness to a record type will be type checked and interpreted; if
that succeeds, this is akin to an existentially quantiﬁed variable witness, which will
be useful for later generation of translated POs proof scripts.

5 Going Beyond The Previously Existing Tools

5.1 Analysis Plugins

The primary purpose of the VDM VSCode language server is to respond to Client
requests via the Language Server Protocol or the Debug Adapter Protocol. The Client
facing components of the server accept RPC message requests and dispatch these to the
a workspace manager for each protocol. The workspace managers maintain the current
state of the speciﬁcation, for example by applying edits when received from the Client.
But to perform more advanced language processing, like checking for syntax or type
errors, the workspace managers delegate processing to Analysis Plugins:

– Analysis plugins isolate the language speciﬁc processing from the workspace man-
agers. In particular, they hide the difference between modular (VDM-SL) and class
based processing (VDM++ and VDM-RT).

– Plugins encapsulate the AST specialised for one particular analysis type. VDMJ
maps the original AST from the parser into specialised ASTs comprised of classes
that perform one analysis, such as type checking, PO generation or execution.
– Plugins register themselves and may involve other plugin services while processing,

though this is usually coordinated via the workspace managers.

– Plugins can be added to the system via conﬁguration, and allow it to be extended

with new analyses (such as the Isabelle translator 4).

The server has three analysis plugins that must be available to open any VDM

project. These are built into the server:

Advanced VDM Support in Visual Studio Code

43

– AST: This is the plugin for the basic parse of the speciﬁcation text. It supplies the
AST tree to the other plugins for further processing and it offers "on the ﬂy" syntax
error reporting to the workspace manager as text is entered. The plugin also has
basic symbol location services that can be invoked if there are type errors and the
TC plugin cannot do this.

– TC: The type checker plugin manages the TC specialised tree and its lifecycle. It
returns type related warning and error messages, as well as building the "outline"
of the symbols deﬁned by the speciﬁcation. It also allows deﬁnitions to be located
(e.g., for "Go-to deﬁnition" requests).

– IN: The interpreter plugin manages the IN specialised tree and its lifecycle. It en-
ables the DAP workspace manager to obtain an interpreter for executing expres-
sions in the speciﬁcation.

In addition to these essential plugins, two further plugins are shipped with the VS-

Code extension, since they support Client-side UI enhancements:

– PO: The proof obligation analysis plugin manages the PO specialized tree. It offers
a service to build a list of proof obligations, either for the entire speciﬁcation or for
just one ﬁle. It is complemented by a Client panel to display the list of obligations
and navigate from an obligation to its location in the speciﬁcation.

– CT: The combinatorial test analysis plugin manages the CT specialised tree and, in
combination with the IN plugin, enables the server to generate and execute com-
binatorial tests. The server feature is complemented by a UI panel to allow the
generated tests and results to be explored.

Further analysis plugins can be added by implementing the AnalysisPlugin inter-
face. These are instantiated by the LSPX workspace manager (which handles extensions
to LSP), by reading a system property that deﬁnes the classes to load. Such dynamic
plugins register themselves like any other plugin. They typically create and manage
their own tree of specialised classes, using the VDMJ ClassMapper.

The LSPX workspace manager handles non-standard LSP method requests by ask-
ing the registered plugins whether they can process the request. The expectation is that
a new Client feature will send new SLSP requests that are handled by a new user plugin.

5.2 Code Lenses

The LSP protocol offers language servers the ability to label parts of the source text with
a clickable tag that then invokes an arbitrary command on the Client. These are called
"code lenses". This request is offered to each of the analysis plugins when a Client code
lens request is received.

Only one code lens is currently generated, by either the AST or TC plugins (de-
pending on whether there are type errors). The lens offers "Launch" and "Debug" tags
for executable functions or operations and links these with a Client side command that
allows the user to quickly build and launch or debug the deﬁnition.

But this is a powerful feature and we expect new plugins to add their own lenses.
For example, the Isabelle translation plugin may be able to link VDM deﬁnitions to the
equivalent in the Isabelle environment.

-2ex

44

5.3 User Libraries

A library is comprised of one or more VDM sources, in a variety of dialects, with any
associated native Java code. These are packaged into a regular Java "jar" ﬁle, with one
additional ﬁle in the "META" folder, called "library.json". The meta-ﬁle deﬁnes the
source ﬁles that should be included for a given VDM dialect as well as dependencies
on other libraries. The VDMJ "stdlib" meta-ﬁle is a good example (below).

The extension allows new libraries to be included in a workspace and selectively

added to projects.

{

"vdmsl": [
{

"name": "IO",
"description": "The IO library",
"files": ["IO.vdmsl"],
"depends": []

"name": "MATH",
"description": "The MATH library",
"files": ["MATH.vdmsl"],
"depends": []

},
{

},
...

],
"vdmpp": [
{

"name": "IO",
"description": "The IO library",
"files": ["IO.vdmpp"],
"depends": []

"name": "VDMUnit",
"description": "The VDMUnit library",
"files": ["VDMUnit.vdmpp"],
"depends": ["IO"]

},
{

},
...

],
"vdmrt" : [
...

]

}

Advanced VDM Support in Visual Studio Code

45

Dependency Graph Generation The VDM VScode extension allows the generation of
a dependency graph that represents the dependencies of the classes/objects in a speciﬁ-
cation. Thus, it is considered as a directed graph where each node points to the node on
which it depends. In some cases you can add some conditions set on the different con-
nections between the nodes. Moreover, each shape represents a node (usually ellipses
or circles) and each connector, composed of one or two arrow heads, indicates the di-
rection of the dependencies. You have also the possibility to add labels on connectors to
specify the relation between two nodes. To ﬁnish, the main usage of a dependency graph
consists of describing processes to make it easier for the developer to understand, reuse
and maintain his code. An example of such a dependency graph is shown on Figure 3

Fig. 3: Example of a visualisation of a dependency graph ﬁle generated by the extension.
The visualisation is performed by an external tool.

6 Concluding Remarks and Future Work

In this paper, we report on the recent additions to the VDM VSCode extension taking
the current support of VDM to a stage where most of the legacy features are supported
and new ones are added. The latter are specially important, as they allow the VDM prac-
titioner to use features, which are available in the integrated development environments
they are used to, but were not available while writing VDM speciﬁcations.

6.1 Features Completeness

VDM VSCode is now labelled as the most up to date, preferred and recommended tool
support for the VDM practitioner. To provide a brief overview of the features of VDM
VSCode a comparison to Overture, which can then be considered as the prior state of
the art tool, is made in Table 1. As illustrated, some features found in Overture is still
missing in VDM VSCode, however these are actively being developed. In addition,
VDM VScode has support for features like ‘Translate to Word‘ and ‘VDM to Isabelle‘
which are not supported by Overture. The features ‘VDM to LLVM‘ and ‘VDM to
Python‘ are not supported by either Overture or VDM VSCode. But, they are being
considered for future development and implementation in VDM VSCode.

-2ex

46

Table 1: Feature comparison between VDM VSCode and Overture.

Features
Syntax highlighting
Syntax check
Type check
Evaluation
Debugging
Proof obligation generation
Combinatorial testing
Translate to LaTex
Translate to Word
Extract VDM from Word
Code completion
Template expansion
Standard library import
Go-to deﬁnition
Coverage display
VDM to Java
VDM to XMI (UML)
VDM to Isabelle
VDM Example Import
Real-time log viewer for VDM-RT
Launch in VDMTools
FMU wrapper
VDM to C
VDM to LLVM
VDM to Python

Overture VDM VSCode

X
X
X
X
X
X
X
X

X
(X)
X
X
(X)
X
X
X

X
X
X
X
(X)

X
X
X
X
X
X
X
X
X
X
(X)
X
X
X
X
(X)

(X)
X
X
X
X

Advanced VDM Support in Visual Studio Code

47

6.2 Future Work

VDM-RT Log Viewer. As described by Fitzgerald et al. [2] real-time traces, which can
be generated when executing a VDM-RT model, can provide insight into the ordering
and timing of exchange of messages, activation of threads and invocation of operations
and combined with validation conjectures[2] enables explicit consideration of system-
level properties during the modelling process. As such, a tool for displaying the traces
and validation conjectures can provide valuable information. There is therefore ongoing
work on implementing a log viewer tool in the VDM VSCode extension similar to the
log viewer tool found in Overture today.

Behaviour Driven Development (BDD). VDM modelling involves the transformation
of a set of elicited requirements listed in a natural language into an executable model.
the practice is to transform the requirements into the speciﬁcation as a jump which
consists of a mental exercise including the iteration of two steps: ﬁrst understand the
requirements, then formulate them as a formal model. It is possible to aid profession-
als in this jump by adopting the approach of BDD, where requirements are translated
into behaviours. The behaviours are written in a constrained subset of natural language
with the goal of becoming more understandable by stakeholders. Tool support has been
developed in both [3] showing its feasibility and recent efforts are in place to develop a
VSCode extension supporting BDD in VDM [15].

UML/VDM translation. The UML connection, allowing the bidirectional translation
between object oriented VDM models and UML diagrams, a legacy feature supported
in the Eclipse based tools is still not supported in the VSCode extension. The legacy
UML transformations were developed with the informal modelling tool, Modelio, in
mind, which require the installation of external tools. New and modern UML tools have
been developed and are integrated in the VSCode marketplace since the Modelio based
translations, and we are working to re-purpose the connection inside the ecosystem. A
preliminary account of the new connection can be found in [9].

FMI Support. Though, currently, the VSCode extension allows the generation of Func-
tion Mock-Up Units (FMUs), its implementation is still dependent on the source code
from the existing Eclipse based solution, given that the generated FMUs wrap the
Eclipse based interpreter. This solution allows the usage of tested and reliable code,
but introduces the potential for discrepancies between the results when the model is
launched/debugged using the VDMJ based LSP-server and the results produced by ex-
ecuting the FMU, given that two different interpreters are used. To simplify codebase
maintenance and make sure results are ran with the same interpreter, we plan to overall
the VDMJ scheduler and create a new FMI implementation directly in it, thus com-
pletely deprecating the Eclipse-based solution.

Acknowledgements We acknowledge the Poul Due Jensen Foundation for funding the
project Digital Twins for Cyber-Physical Systems (DiT4CPS).

-2ex

48

References

1. Battle, N.: VDMJ User Guide. Tech. rep., Fujitsu Services Ltd., UK (2009)
2. Fitzgerald, J.S., Larsen, P.G., Tjell, S., Verhoef, M.: Validation Support for Real-Time Em-
bedded Systems in VDM++. In: Cukic, B., Dong, J. (eds.) Proc. HASE 2007: 10th IEEE
High Assurance Systems Engineering Symposium. pp. 331–340. IEEE (November 2007)
3. Fraser, S., Pezzoni, A.: Behaviour driven speciﬁcation. In: Macedo, H.D., Thule, C., Pierce,
K. (eds.) Proceedings of the 19th International Overture Workshop. Overture (10 2021)
4. Freitas, L., Whiteside, I.: Proof patterns for formal methods. In: Jones, C.B., Pihlajasaari,
P., Sun, J. (eds.) FM 2014: Formal Methods - 19th International Symposium, Singapore,
May 12-16, 2014. Proceedings. Lecture Notes in Computer Science, vol. 8442, pp. 279–295.
Springer (2014), https://doi.org/10.1007/978-3-319-06410-9_20

5. Jacobs, E.: Implementing Witness Annotations for VDMJ. Master’s thesis, School of Com-

puting (May 2018)

6. Jones, C.B., Jones, K.D., Lindsay, P.A., Moore, R.: mural: A Formal Development Sup-
port System. Springer-Verlag (1991), http://homepages.cs.ncl.ac.uk/cliff.
jones/publications/Books/mural.pdf

7. Kulik, T., Macedo, H.D., Talasila, P., Larsen, P.G.: Modelling the HUBCAP Sandbox Ar-
chitecture In VDM – a Study In Security. In: Fitzgerald, J.S., Oda, T., Macedo, H.D. (eds.)
Proceedings of the 18th International Overture Workshop. pp. 20–34. Overture (December
2020)

8. Kulik, T., Talasila, P., Greco, P., Veneziano, G., Marguglio, A., Sutton, L.F., Larsen, P.G.,
Macedo, H.D.: Extending the formal security analysis of the hubcap sandbox. In: Macedo,
H.D., Thule, C., Pierce, K. (eds.) Proceedings of the 19th International Overture Workshop.
Overture (10 2021)

9. Lund, J., Jensen, L.B., Macedo, H.D., Larsen, P.G.: Towards UML and VDM support in the
vs code environment (submitted). In: Macedo, H.D., Pierce, K. (eds.) Proceedings of the 20th
International Overture Workshop. Overture (2022)

10. Macedo, H.D., Larsen, P.G., Fitzgerald, J.: Incremental Development of a Distributed Real-
Time Model of a Cardiac Pacing System using VDM. In: Cuellar, J., Maibaum, T., Sere,
K. (eds.) FM 2008: Formal Methods, 15th International Symposium on Formal Methods.
Lecture Notes in Computer Science, vol. 5014, pp. 181–197. Springer-Verlag (2008)

11. Nilsson, R., Lausdahl, K., Macedo, H.D., Larsen, P.G.: Transforming an industrial case study
from VDM++ to VDM-SL. In: Pierce, K., Verhoef, M. (eds.) The 16th Overture Workshop.
pp. 107–122. Newcastle University, School of Computing, Oxford (July 2018), TR-1524
12. Rask, J.K., Madsen, F.P.: Decoupling of Core Analysis Support for Speciﬁcation Languages
from User Interfaces in Integrated Development Environments. Master’s thesis, Aarhus Uni-
versity, Department of Engineering (December 2020)

13. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: The Speciﬁcation Language
Server Protocol: A Proposal for Standardised LSP Extensions. In: Proença, J., Paskevich, A.
(eds.) Proceedings of the 6th Workshop on Formal Integrated Development Environment,
Held online, 24-25th May 2021. Electronic Proceedings in Theoretical Computer Science,
vol. 338, pp. 3–18. Open Publishing Association

14. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: Visual Studio Code VDM
Support. In: Fitzgerald, J.S., Oda, T. (eds.) Proceedings of the 18th International Overture
Workshop. pp. 35–49. Overture (December 2020), https://arxiv.org/abs/2101.
07261

15. Villadsen, K.S., Jensen, M.D., Larsen, P.G., Macedo, H.D.: Bridging the requirements-
speciﬁcation gap using behaviour-driven development (submitted). In: Macedo, H.D., Pierce,
K. (eds.) Proceedings of the 20th International Overture Workshop. Overture (2022)

Advanced VDM Support in Visual Studio Code

49

16. Woodcock, J., Freitas, L.: Linking VDM and Z. In: 13th International Conference on
Engineering of Complex Computer Systems (ICECCS 2008), March 31 2008 - April 3
2008, Belfast, Northern Ireland. pp. 143–152. IEEE Computer Society (2008), https:
//doi.org/10.1109/ICECCS.2008.36

17. Woodcock, J., Saaltink, M., Freitas, L.: Unifying theories of undeﬁnedness. In: NATO Series
D: Information and Communication Security (Marktoberdorf). vol. 22, pp. 311–330. IOS
Press (Aug 2009)

Towards UML and VDM Support in the VS Code
Environment

Jonas Lund, Lucas Bjarke Jensen, Hugo Daniel Macedo, and Peter Gorm Larsen

DIGIT, Aarhus University, Department of Engineering,
Finlandsgade 22, 8200 Aarhus N, Denmark
{201906201,201907355}@post.au.dk, {hdm,pgl}@ece.au.dk

Abstract. The coupling between the object-oriented dialects of VDM (VDM++
and VDM-RT) and UML, found on The Overture Tool has been left behind due to
the shift in focus onto VDM VSCode. The paper ﬁrst presents how the coupling
is reestablished in VS Code. However, it is subsequently proposed that UML
visualisations of VDM models can be further improved by using the text-based
diagram tool called PlantUML, as it is supported as a VS Code extension and
offers several advantages compared to other UML tools. The best integration of
PlantUML requires a direct translation between VDM and PlantUML, which is
made possible by class mapping introduced in VDMJ 4.

1

Introduction

The modelling of critical and embedded systems is a complex task with many parts.
One of the most popular and mature languages designed for modelling such systems is
the Vienna Development Method (VDM) which is extended by the two object-oriented
(OO) dialects, VDM++ and VDM Real-Time (VDM-RT) [6]. Some advantages of mod-
elling using the OO dialects are that encapsulation of methods and data along with
abstraction of the internal state of these classes allows for higher degrees of model
complexity.

The task of modelling these systems is complex and requires a deep understanding
of the modelling tools in use. The widely used Overture Tool built on top of the Eclipse
IDE allows for easier modelling integrating several debugging tools and plugins. One
of these plugins is the transformation from and to Uniﬁed Modelling Language (UML)
ﬁles [9]. This connection was developed because UML class diagrams are a ﬁtting and
intuitive way of visualising OO models.

The Eclipse version of Overture is slowly growing obsolete as a new contender rises,
the VDM VS Code extension, letting Visual Studio Code (VS Code) function as an IDE
for modelling in languages from the VDM language family. Many of the features from
Eclipse have been ported to the extension already, but the UML connection has yet to
be incorporated.

The UML transformations for The Overture Tool culminated in the coupling with
the visual modelling tool, Modelio. However, the coupling has its ﬂaws and shortcom-
ings with regards to visualising the generated UML models and exporting new ones.
New tools that integrate more closely with modeling tools have been developed and it

Towards UML and VDM Support in the VS Code Environment

51

may be worthwhile looking into whether the connection between VDM and UML can
be repurposed for such tools.

This paper will be tackling two tasks. The ﬁrst is covering the work that has gone
into porting the already existing UML connection from the Eclipse version of Overture
to the new VDM VSCode extension. The second is discussing other tools for visualising
UML and designing a new transformation plugin for this tool.

2 Background

2.1 Vienna Development Method and Uniﬁed modelling Language

The Vienna Development Method (VDM) is a leading formal method, focused on the
modelling of computer-based systems while ensuring safety and security in the software
before deployment [8]. There exist two dialects that extend the original VDM language,
the VDM Speciﬁcation Language (VDM-SL). These are the object-oriented extensions
called VDM++ and VDM Real-Time (VDM-RT), which support concurrency and real
time modelling as well. Both VDM++ and VDM-RT use classes to structure their mod-
els. A visual representation of a model helps in designing and communicating the archi-
tecture of the system. This is at its most effective via class diagrams using the Uniﬁed
modelling Language (UML) [10]. UML is speciﬁed by the Object Management Group
(OMG) and uses a semi-formal visual language to give an abstract representation of
object-oriented systems.

A set of transformations between VDM and version 2 of UML exist on the Overture
Project as a plugin for the Overture Tool. [8] These transformations were made with
Modelio in mind as the UML modelling tool, both for import of generated UML ﬁles
from VDM and for export of class diagrams to be translated to VDM.

2.2 From The Overture Tool to Visual Studio Code

The Overture Tool (Overture) is the most mature and popular platform for VDM mod-
elling. However, the focus of the Overture project has shifted to the Visual Studio Code
(VS Code) platform in recent years with the development of the VDM VS Code exten-
sion developed by Jonas Rask and Frederik Palludan Madsen [12].

This extension establishes a connection to a language server, letting VS Code func-
tion as an Integrated Development Environment (IDE), exactly like its Eclipse counter-
part, while also having the advantage of being extendable to other languages thanks to
the Language Server Protocol (LSP). [11]

The Eclipse UML transformation plugin has been reimplemented as a command on
VDM VS Code. This was achieved by packaging the main functions Vdm2UmlMain
and Uml2VdmMain into a JAR ﬁle that can be run from VS Code. Currently, this JAR
ﬁle is located on the client side of VDM VS Code, but will be moved to the VDMJ
language server in the future to decouple the speciﬁcation language functions from the
VS Code extension.

-2ex

52

2.3 VDMJ

VDMJ is a command line tool that provides basic tool support for Overture. Some of
the features it provides are type checking, proof obligations, and combinatorial test gen-
eration among many others. In Overture for Eclipse, VDMJ and its tools are executed
through the Eclipse IDE interface. For VDM VS Code however, the VDMJ features are
accessed through the LSP, letting VS Code act as an interface for VDMJ. The latest
version of the tool, VDMJ 4, lets the user easily reconstruct the VDM Abstract Syntax
Trees (AST) for their desired tool related purposes [3].

2.4 PlantUML

PlantUML [13] is a text-based diagram tool that exists as a VS Code extension [7]. Any
model made in PlantUML can be opened in a separate tab in VS Code and continu-
ally updates as the model is changed. The models can be worked on by multiple team
members as it integrates with versioning tools like GitHub. This is possible since the
PlantUML diagrams are generated from text ﬁles and can therefore be located in the
software repository alongside the program or model which the diagram represents.

PlantUML also seeks to allow interoperability between tool vendors as many export
formats are supported (including XMI). It also attempts automatic placement of objects
in the diagram and allows for potential adjustments from the user, a feature which was
not available in Modelio.

One caveat in representing VDM models using PlantUML is that qualiﬁed associa-
tions, which are commonly used in VDM class diagrams, but alternatives are available
such as using square brackets to indicate the qualiﬁer. A typical class diagram that rep-
resents a VDM++ model is presented on Fig. 1.

Fig. 1: Resulting PlantUML

PlantUML is even cited in books pertaining to critical [5] and cyber-physical [4] sys-
tems. These features make the PlantUML extension one of the more relevant options
for fulﬁlling the goals of this project.

Towards UML and VDM Support in the VS Code Environment

53

3 Translation Between VDM and UML

This section gives an overview on the migration of the UML connection from Overture
to VDM VSCode. To implement the connection, the existing Overture code for convert-
ing VDM to UML is packaged as a JAR ﬁle and called within VS Code. This method
does not involve changing the code which is executed to perform the conversions. It
involves the creation of a handler that reads the ﬁles, checks the validity of these ﬁles,
and passes the expected arguments to the converting methods.

This is the ﬁrst part of the two that this paper will cover. The second part regarding

PlantUML is presented in section 4.

3.1 Overture UML Migration to VDM VS Code

In order to port the existing UML connection to VDM VSCode, the connection has to
ﬁrst be decoupled from the Eclipse IDE. This is a necessary step in allowing Maven
to package the command in a JAR ﬁle. After the decoupling, the main handlers for the
UML transformation can be developed.

Fig. 2: VDM to UML Architecture Overview

Vdm2UmlMain is the transformation handler in the direction of VDM to UML. As
illustrated in Fig. 2, it receives the path to the folder containing the VDM ﬁles as input.
An instance of the Vdm2Uml class is constructed, which is the class containing the
methods for the transformation. Vdm2UmlMain then extracts a list of classes from the
VDM ﬁles, called classList. This list is given as input to the Vdm2Uml method
called convert.
Uml2VdmMain is the main function in the direction of UML to VDM. As illustrated
in Fig. 3, it receives the path to the UML ﬁle as an input. An instance of the Uml2Vdm
class is constructed, after which the class is initialised using the initialize method
with the path to the UML ﬁle as an argument. The convert method is then called,
and the corresponding VDM ﬁles are created.
The UML transformations are packaged in a JAR ﬁle using Maven. The JAR ﬁle is
integrated into the VDM VS Code environment where it is accessed through a handler
called UMLHandler.ts. This handler is responsible for executing the JAR ﬁle with
appropriate arguments. Both main functions are contained in the same JAR. The func-
tion executed is determined by which command is issued by the VDM VSCode user.
The architecture of the JAR ﬁle integration is illustrated in Fig. 4.

-2ex

54

Fig. 3: UML to VDM Architecture Overview

Fig. 4: VDM VS Code Architecture Overview

3.2 Repurposing the Overture Code for UML Transformations

To understand how the VDM VSCode UML transformation handlers will ﬁll the role
of the Overture handlers, while enabling VS Code integration, this section will describe
what resources the Overture implementation handlers used in the UML transformations,
and whether these resources are ﬁt for reuse.

Analysis of the Overture UML Handlers To extract the information needed to per-
form the UML conversions, the handler in the direction of VDM to UML, called Vdm2UmlCommand,
relies heavily on the IVdmProject class.

The IVdmProject class is inherited from the IProject class, which is an
Eclipse-based interface that grants the notion of a project as an implementation re-
source. A project encapsulates information about a group of ﬁles that share a directory.

Towards UML and VDM Support in the VS Code Environment

55

Furthermore, the need for any ﬁle handling is resolved as this is also encapsulated by
the Eclipse interface.

The IVdmProject class is modiﬁed to suit VDM and it therefore provides further
abilities that are speciﬁc to a group of VDM ﬁles. In a similar manner, as IVdmProject
is derived form the IProject class, the IVdmProject class contains a method for
creating an instance of the IVdmModel class, which in turn encapsulate the abstract
structure of the VDM model. This model is then used to examine whether the model is
syntax and parse correct and therefore ﬁt for UML transformation. The code snippet be-
low shows how the IVdmModel class is instantiated and used to check the correctness
of the model.

final IVdmModel model = vdmProject.getModel();

if (model.isParseCorrect())
{

if (model == null || !model.isTypeCorrect())
{...}

The Eclipse project classes and the functionality they offer are illustrated in Fig. 5.

Fig. 5: Overview of IProject its derived classes, and the methods used in the Overture
VDM to UML handler

Likewise, the handler in the direction of UML to VDM, called Uml2VdmCommand
also use Eclipse dependent classes to fulﬁl its tasks. The class Uml2Vdm, which con-
tains the method that performs the conversion needs to be instantiated with the path
to the UML ﬁle along with information about what dialect the to be generated VDM
ﬁles should be in. The UML ﬁle path is extracted from an instance of the iFile
class, which encapsulates the Eclipse notion of a ﬁle. This class can then be cast to
an IVdmProject class, which in turn can be used to discover the dialect of the to be
generated VDM ﬁles.

Considering the goal of migrating the UML connection to VDM VS Code, the han-
dlers’ use of the project classes poses a problem, since their usefulness is dependent on
the Eclipse IDE. This stems from the fact that the input to the handler has changed from
an IVdmProject in Overture, to simply being a list of VDM ﬁles in VDM VSCode.
One way to solve this could be to extract information about the current VS Code
project, create an intermediate Eclipse project, and then perform the transformation

-2ex

56

like the Overture UML handlers do. However, this runs into the problem of having to
depend on the IDE side. To circumvent this, one might also simply remove the notion
of an IProject and its derived types. It would then require ﬁnding new ways of
providing the same functionality as these classes provide.

The VDM VSCode UML Handlers The VDM VSCode handler achieves the same
functionality as the Overture handler, without utilising the notion of an Eclipse project.
In a lot of cases, this is done simply by accessing the methods used by the IProject
and its derived classes. This results in the same functionality, but the methods used are
accessed at a lower level. This section shows examples of how this strategy is applied.
In the implementation of the Overture handler, the ﬁle handling is done implicitly by
Eclipse. However, as this is no longer a possibility, the ﬁle handling is done internally
by Vdm2UmlMain. This is achieved by the Vdm2UmlMain method ’main’, taking
a set of arguments which denote the directory containing the ﬁles to be transformed
and the directory where the resulting UML ﬁle should be placed. The arguments are
distinguished using -folder and -output ﬂags before the path arguments.

On Overture, the handler read the VDM dialect using a method from the IVdmProject

class. The VDM VSCode handler is instead passed the dialect as a ﬂag, along with the
command to execute the JAR ﬁle.

While the IVdmModel class has an attribute informing whether the model is syntax
and type correct, the methods for performing the actual checks are not dependent on the
Eclipse project classes, and are therefore ﬁt for reuse. The class list is also provided by
the type checker, typeCheckPp or typeCheckRt, depending on the dialect of the
VDM ﬁles. The two methods of the class TypeCheckerUtil initialise an instance of
the TypeCheckResult class, which has an attribute containing the class list needed
for the conversion.

In the direction of UML to VDM, instead of the path to the UML ﬁle being extracted
from the iFile instance, the path is instead passed as an argument in the command to
execute the transformation on the JAR ﬁle. Currently the dialect of the resulting VDM
ﬁles are assumed to be VDM++. The consequence of this is that a UML ﬁle containing
a VDM-RT model will be changed to a VDM++ model. This short-coming should be
trivial to ﬁx in the future.

With the handlers working and ported to the VDM VSCode extension, the UML
transformation commands can be run and tested in VS Code. A screenshot demonstrat-
ing how the feature is used is shown in Fig. 6.

Towards UML and VDM Support in the VS Code Environment

57

Fig. 6: Screenshot of using the command in VS Code.

-2ex

58

4 PlantUML Implementation Planning

This section covers the second part of the paper, which involves describing the moti-
vation behind choosing PlantUML as the primary UML visualisation tool, as well as
determining the best way to couple PlantUML with VDM VSCode.

This is almost entirely separate from the porting of the existing UML connection,
and will instead dive into the possibility of using the new architecture of the VDM
VSCode environment to create a new UML connection.

4.1 Motivation

Currently, the UML transformations are coupled with Modelio version 2.2.1, as the
original Overture UML connection was speciﬁcally tailored to this tool. This poses
some issues:

– The speciﬁc version of XMI used is compatible with only some versions of Mode-
lio, and brings little compatibility with any other tools, restricting interoperability.
– There is no spatial information in the standard and Modelio makes no attempt at

guessing it, meaning that classes and links must be positioned manually.

– Employing a JAR ﬁle that uses Eclipse IDE resources is undesirable, since the
Overture project is trying to distance itself from Eclipse with the migration to
VDM VSCode. Furthermore, having to rely on Eclipse results in making it sig-
niﬁcantly more difﬁcult for the UML transformations to achieve maintainability
and extendibility.

It would therefore be optimal to use a VS Code extension to visualise UML models that
can import and export UML ﬁles that are XMI compliant, since this would allow for
developing models directly in VS Code. Unfortunately, no such tool seems to exist.

VDMJ version 4, however, introduces class mapping [3], allowing developers to
easily reconstruct the VDM ASTs for whichever tool-related purposes needed. It may
well be possible to utilise class mapping to develop an AST for a textually based UML
tool, that is supported in a third party VS Code extension. This would, in turn, allow
a direct translator between UML and this tool to be created, solving the problems laid
out and thus achieving a direct connection between VDM and UML on the language
server, without relying on outdated Eclipse resources.

Towards UML and VDM Support in the VS Code Environment

59

4.2 Transformation Overview and Considerations

In order to begin development on the PlantUML transformation using ASTs on the
VDMJ language server, some notions must be considered.

The previous UML transformations also use ASTs, speciﬁcally for the object-oriented

VDM dialects and a UML AST that would be used for converting to the XMI format [9].
It is worth considering how much of the implementation of this previous transformation
would be reusable for the PlantUML transformation. Since a UML AST does seem to
exist, would it be possible to repurpose it for the new VDMJ, and develop translators
between UML and PlantUML? Or is it better to take inspiration from the previously
used ASTs and instead create a new AST for PlantUML?

The methods for developing ASTs have improved greatly since the Overture UML
implementation with the introduction of VDMJ 4 [3]. This has been achieved with class
mapping, and it is because of this progress that it seems viable to produce an AST our-
selves for the purpose of UML transformations. An overview of how the transformation
between VDM++ and PlantUML could work is illustrated in Fig. 7. The dashed arrows
denote the PlantUML to VDM++ direction and the solid lines denote the VDM++ to
PlantUML direction.

Fig. 7: Overview of the PlantUML transformations using ASTs.

Note that there are several uncertainties as for how the transformations will actually
work, since we have yet to properly examine what the process of creating ASTs entails.
One uncertainty being how the nodes of the trees are traversed to output the code for
both the VDM model and the PlantUML model. It is expected to be done through some
visitor method, similar to how it was done for the previous VDM++ AST [9]. Whether
or not the same can be done for the PlantUML AST is also uncertain.

The conversion between the two ASTs is expected to make use of the class mapping
mechanism. An example of the class mapping in use can be seen in a simple VDM to

-2ex

60

C translator [2]. The translator makes use of mapping objects from the type checker
(TC) AST to a new translator (TR) AST through a .mappings ﬁle. This process of
reconstructing existing ASTs for the purposes of a new plugin will make the base for
the functions that transform between VDM and PlantUML. However, in this example, it
seems that one AST is made for the feature that is translating VDM to C. This poses the
question of whether it will be necessary to create ASTs corresponding to the languages
involved, or corresponding to the translations that happen between the languages.

4.3 Transformation Rules

In the VDM++ book called Validated Designs for Object-oriented Systems, 12 map-
ping rules between UML class diagrams and VDM++ are presented [6]. These serve
as requirements for the translator between the OO VDM dialects and PlantUML. In
Tab. 1, 2, and 3, an example in VDM++ and PlantUML are presented along with the
transformation rules.

Towards UML and VDM Support in the VS Code Environment

61

Table 1: VDM++ to PlantUML Transformation Mapping Table #1-3
1 — Class Declarations in both views are very simple, since "there is a one-to-one
relationship between classes in UML and classes in VDM++", as stated in the original
mapping rules [6]. The ellipses in the PlantUML view surrounds the contents of the
class and all class members are declared within them.

2 — Associations must each be given a role name that denotes the direction of the
association. This is represented in VDM++ as an instance variable with the type deﬁned
by the type of the destination class of the association. In PlantUML, the direction of
an association is denoted by an arrow, followed by the destination. Associations in
PlantUML are declared after class declarations.

3a — Instance Variables in the VDM++ view are equivalent to attributes in UML
classes, and share a similar notation.

3b — Value Deﬁnitions can be differentiated from instance variables using a stereo-
type to indicate that they should be treated as constants.

-2ex

62

Table 2: VDM++ to PlantUML Transformation Mapping Table #4-7

4a — Operations are deﬁned in both views using a type and a name.

4b — Functions are differentiated from operations using a stereotype in the UML view.
The type of function (total, partial, injection, etc.) is also speciﬁed in the stereotype.

5 — Access Modiﬁers are available for all members in both views and are denoted in
the member declaration.

6 — The Static keyword is used in both views, with a slight notational change.

7 — Types have no equivalent in UML, and are therefore omitted. However, it may be
possible to use stereotypes to declare a type in PlantUML.

Towards UML and VDM Support in the VS Code Environment

63

Table 3: VDM++ to PlantUML Transformation Mapping Table #8-12
8 — Inheritance between the two views have a one-to-one mapping. It is deﬁned in
PlantUML using an arrow head notation. Inheritance in PlantUML is declared after
class declarations.

9 — Association Multiplicity in UML deﬁne the type of the corresponding VDM++
instance variable. An ordered zero to n multiplicity corresponds to a VDM++ sequence
with the type of the association destination object. An ordered multiplicity is denoted
using parentheses.

10 — Qualiﬁed Associations in VDM++ are instance variables that encapsulate the
mapping from a qualiﬁer type or qualiﬁer class, to a class. In PlantUML the qualiﬁer
appears in square brackets on the side of the associating class, similar to how it is shown
in Modelio using a smaller box bordering the class.

11 — Sets in VDM++ come as a result of an unordered zero to n multiplicity using the
set type constructor.

12 — The Optional Type constructor is used when a multiplicity of zero or one is
used.

-2ex

64

We hereby see how PlantUML has the capability to fulﬁl these transformation rules,
and is therefore a suitable host for UML visualisations of VDM models. Note that there
may very well be edge cases, where some construct exists in VDM but is not deﬁned in
UML. It is difﬁcult to consider all such cases, but one way is to do static analysis of the
VDM model and warn the user, if they are about to transform some VDM that will not
be available in the UML model.

5 Future Work

Further progress is required before a UML visualisation tool is seamlessly integrated
into VDM VS Code. Achieving this involves developing a prototype of the PlantUML
translator by following the architecture laid out in this paper. This development will
hopefully allow for a smooth bidirectional mapping between the OO dialects of VDM
and PlantUML. The current overview only considers the VDM++ dialect, but it will
also have to incorporate the VDM-RT dialect in the future.

The creation of the direct translator involves the use of class mapping to reconstruct
the ASTs present on the VDMJ tool. The process involved is still alien to us, but help
from and collaboration with Nick Battle and the rest of the team is expected in the
future.

At the moment, a challenge in representing VDM models using PlantUML is the
lack of a qualiﬁed association deﬁnition, even if workarounds exist [1]. A solution
to this may involve collaborating with the creators of PlantUML allowing OO VDM
models to be fully compatible with PlantUML. The current connection does not sup-
port VDM types in its class diagrams, but as PlantUML is malleable to specify custom
members, it may be possible to represent VDM types in PlantUML class diagrams.

Furthermore, there is still work to be done on the current implementation of the
connection between VDM VSCode and UML. Transforming VDM ﬁles to UML is not
supported for models that include libraries. Similarly, when transforming a UML ﬁle
representing a VDM-RT model to VDM ﬁles, the ﬁles are converted to the VDM++
dialect. However both these problems are relatively simple to correct.

In the long term, work can be done to enable continual updates on the bidirec-
tional mapping between VDM and PlantUML. Another area for potential development
is to work on the inherent information loss that occurs when transforming. Solving this
would involve developing some way of encapsulating the functionality of the classes
present in the model and pass this information along, together with the UML diagram
information. It is also planned to perform static analysis of the models to make the
tool more user friendly by warning whenever some VDM construct does not have a
compatible UML counterpart.

6 Conclusion

This paper has described how the coupling between VDM and UML was established
on VS Code. This was performed by repurposing the already existing connection on the
Eclipse version of Overture through decoupling it from the Eclipse IDE and packaging

Towards UML and VDM Support in the VS Code Environment

65

the transformation functionality, along with all its dependencies, in a JAR ﬁle and inte-
grating it into VS Code. Steps have been taken to adapt the textually based open-source
diagram tool, PlantUML, for UML visualisations. Together with its VS code extension,
PlantUML offers a continually updating view of UML diagrams next to the natural
language based code. Progress was then made to create a translator that can transform
between VDM++ and PlantUML by deciding on the architecture of the translator and a
prototype was made.

Acknowledgments We would like to thank all the stakeholders that have contributed
to the Overture/VDM development but in particular Kenneth Lausdahl for the original
UML mapping and Nick Battle for the establishment of VDMJ.

References

1. Qualiﬁed

associations,
qualified-associations

https://forum.plantuml.net/349/

2. Battle, N.: The v2c translation example, https://github.com/nickbattle/

vdmj/tree/master/examples/v2c

3. Battle, N.: Analysis Separation without Visitors. In: Fitzgerald, Tran-Jørgensen, Oda (ed.)
The 15th Overture Workshop: New Capabilities and Applications for Model-based Systems
Engineering. pp. 104–115. Newcastle University, Computing Science. Technical Report Se-
ries. CS-TR-1513, Newcastle, UK (September 2017)

4. Bondavalli, A., Bouchenak, S., Kopetz, H.: Cyber-Physical Systems of Systems: Foundations
– A Conceptual Model and Some Derivations: The AMADEOS Legacy, vol. 10099 (12 2016)
5. Bondavalli, A., Brancati, F.: Certiﬁcations of Critical Systems - The CECRIS Experience,

pp. 1–316 (01 2017)

6. Fitzgerald, J., Larsen, P.G., Mukherjee, P., Plat, N., Verhoef, M.: Validated Designs for
Object–oriented Systems. Springer, New York (2005), http://overturetool.org/
publications/books/vdoos/
jebbs: Rich plantuml support
visualstudio.com/items?itemName=jebbs.plantuml

for visual studio code, https://marketplace.

7.

8. Larsen, P.G., Battle, N., Ferreira, M., Fitzgerald, J., Lausdahl, K., Verhoef, M.: The over-
ture initiative integrating tools for vdm. SIGSOFT Softw. Eng. Notes 35(1), 1–6 (jan 2010),
https://doi.org/10.1145/1668862.1668864

9. Lausdahl, K., Lintrup, H.K.: Coupling Overture to MDA and UML. Master’s thesis, Aarhus

University/Engineering College of Aarhus (December 2008)

10. OMG: About the uniﬁed modeling language speciﬁcation version 2.5.1 (december 2017),

https://www.omg.org/spec/UML/2.5.1

11. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: The Speciﬁcation Language
Server Protocol: A Proposal for Standardised LSP Extensions. In: Proença, J., Paskevich, A.
(eds.) Proceedings of the 6th Workshop on Formal Integrated Development Environment,
Held online, 24-25th May 2021. Electronic Proceedings in Theoretical Computer Science,
vol. 338, pp. 3–18. Open Publishing Association

12. Rask, J.K., Madsen, F.P., Battle, N., Macedo, H.D., Larsen, P.G.: Visual Studio Code VDM
Support. In: Fitzgerald, J.S., Oda, T. (eds.) Proceedings of the 18th International Overture
Workshop. pp. 35–49. Overture (December 2020), https://arxiv.org/abs/2101.
07261

13. Roques, A.: Plantuml language reference guide, https://plantuml.com/guide

Speeding Up Design Space Exploration through
Compiled Master Algorithms

Ken Pierce1, Kenneth Lausdahl2, and Mirgita Frasheri2

1 School of Computing, Newcastle University, United Kingdom
kenneth.pierce@ncl.ac.uk
2 Dept. of Electrical and Computer Engineering, DIGIT, Aarhus University, Aarhus, Denmark
kenneth@lausdahl.com,mirgita.frasheri@ece.au.dk

Abstract. The design of Cyber-Physical Systems (CPSs) is complex, but can be
mitigated through Model-Based Engineering. The models representing the com-
ponents of a CPS are often heterogeneous, combining cyber and physical ele-
ments, and can be produced in different formalisms by engineers from diverse dis-
ciplines. To assess system-level properties of a speciﬁc design, the joint behaviour
of such components can be analysed through co-simulation, whereas different de-
sign alternatives can be compared through Design Space Exploration (DSE). Due
to its combinatorial nature, the DSE can suffer from state-space explosion, where
the number of combinations rapidly increases the time required to analyse them.
While careful experiment design and Genetic Algorithms can be used to reduce
the number of simulations, the beneﬁt of speeding up co-simulations is clear. In
this paper, we present an extension of the FMI-compliant Maestro co-simulation
engine that generates a custom co-simulation Master algorithm as C code that
can be compiled and run natively, reducing the overheads from the existing Java-
based version. We apply this to a standard water tank case study and show an
initial speed up of ﬁve times over the existing Maestro implementation.

1

Introduction

Cyber-Physical Systems (CPSs) are systems constructed of interacting hardware and
software elements, with components networked together and distributed geographi-
cally [6]. The development, deployment and maintenance of CPSs requires multiple
disciplines to work together, which is often hampered by the organisational, social and
technical barriers between engineering disciplines, including use of different terminolo-
gies, techniques and tools. Modelling such systems, and analysing these models with
techniques such as simulation, are increasingly used in the development of CPSs. Com-
bining models from different disciplines to create system-level “multi-models” has been
shown as one way to bring together these disciplines and to permit better analysis of
CPSs [1].

INTO-CPS [5] is a tool chain for model-based design of CPSs based around this
concept of multi-models, using the Functional Mock-up Interface (FMI) standard [8]
(FMI). The FMI standard provides a way for individual models to be packaged as Func-
tional Mock-up Units (FMUs), with a standard interface for inputs and outputs. A multi-
model therefore represents a CPS through a combination of FMUs representing the var-
ious components or parts of the system of interest, and the connections between them

Speeding Up DSE through Compiled Master Algorithms

67

(inputs and outputs that inﬂuence other FMUs in the multi-model). A multi-model can
be analysed in various ways including static checking and model checking. The most
common way is through co-simulation, where the FMUs are executed simultaneously
under the control of a Master Algorithm (MA) that is responsible for advancing (simu-
lated) time and passing inputs and outputs between FMUs. The INTO-CPS tool chain
is centered around a co-simulation engine called Maestro [11] that provides a variety of
Master algorithms.

One main beneﬁt of modelling CPSs is that it enables investigation of different
designs before committing time and resources to physical prototypes. A design is char-
acterised by its design parameters, which are properties of the CPS that affect its be-
haviour (both physical properties and those of software). The set of all possible design
parameters deﬁnes a design space. Design Space Exploration (DSE) is the act of as-
sessing one or more areas of the design space by analysing and ranking a set of designs
automatically, with the aim of helping the engineering team to make better informed
decisions about which designs are most promising.

DSE is one feature of the INTO-CPS tool chain. INTO-CPS includes a set of Python
scripts that allow an engineer to deﬁne a design space and to perform DSE through
either exhaustive search (analysing all combinations of designs deﬁned by a set of pa-
rameters) or genetic search (iteratively selecting promising designs from an initial set).
Even with trivial multi-models that co-simulate quickly, large DSEs with many combi-
nations of parameters rapidly increase the required number of co-simulations making
DSE time and resource intensive.

Given the pressures of industry, it is important that decisions on designs be made
quickly and effectively. While DSE can give a lot of useful information, examining
combinations of even a few parameters can take a long time. This paper describes work
on improving the speed of co-simulation by generating an MA to native C++ code, which
in turn speeds up co-simulation and allows a given design space to be explored faster,
or a larger design space to be explored in a similar time.

The remainder of this paper is structured as follows. Section 2 describes the existing
DSE framework in INTO-CPS. Section 3 introduces the new code-generation feature.
Section 4 provides a refresher on the standard water tank case study. Section 5 shows
the performance of the new feature. Finally, Section 6 presents some conclusions and
future work.

2 Design Space Exploration with INTO-CPS

Design Space Exploration (DSE) involves running multiple co-simulations, each of
which represents a different design, and analysing the results in some form to pro-
vide the engineer with evidence for making design decisions about which designs are
most promising [3]. A common way to perform a DSE is to run multiple simulations
and sweep across one or more design parameters. Each design is characterised by a set
of design parameters, which do not change during a co-simulation, but take different
values for each design [1].

Deﬁning a DSE then involves selecting which parameters will be changed, what
values will be explored, and in what combination. Design parameters could be numer-

-2ex

68

ical, in which case could be deﬁned through minimum, maximum and step size, or
they could be deﬁned as an enumerated set of possible values. Combinations can be
constrained, such as ensuring that one parameter is always greater than another. The
number of possibilities deﬁned by a given combination deﬁnes the size of the design
space for a given DSE, and hence the number of co-simulations that must be run.

After a co-simulation has run, the results are saved as Comma-Separated Values
(CSV) ﬁles, and can be analysed. This is accomplished by deﬁning one or more ob-
jective functions, which represent metrics by which the design is judged. An objective
function can be deﬁned programmatically, for example by providing a Python script
that computes a score from the CSV and any additional data, such as details of the sce-
nario. Once all co-simulations in a DSE have run, the designs can be ranked based on
the objectives.

A clear issue with DSE is state space, or in this case design space, explosion. As
the number of parameters and values increases, the number of combinations increases
exponentially. As mentioned below, the main ways to explore design spaces further
using the same compute and time resources are:

1. To perform careful experiment design in order to focus on the most likely out-
comes, for example by selecting combinations that demonstrate the most important
parameters [3];

2. To explore the design space iteratively by running only some co-simulations ini-
tially, then deciding based on those results which are the most promising designs.
This can be achieved automatically through genetic algorithms, for example [9]; or
3. To speed up the co-simulation itself by optimising the FMUs and/or Master algo-

rithm.

The DSE features of the INTO-CPS tool chain take the form of a set of Python
scripts written to use the Maestro co-simulation engine to carry out experiments with
multiple co-simulation runs. These scripts were originally developed in the INTO-CPS
project [4] using Python 2.7. These original Python scripts included an exhaustive
search which computes all possible combinations of parameters given (excluding those
that don’t meet the constraints). The scripts also contained a basic genetic algorithm
to select which designs to run after performing analysis at each step in order to search
down and reduce the overall number of simulations

Since the INTO-CPS project has ﬁnished, a number of people have improved upon
them. The most recent version now works with Python 3 and includes support for paral-
lelization [9]. Other recent papers have also looked at using libraries to achieve similar
results with particle swarm optimization and simulated annealing [10]. While different
designs could also include different FMUs, the current scripts do not support sweeping
across different FMUs, since care must be taken to understand what should happen if
those have diverse design parameters.

3 Code Generation Extension

Speeding up co-simulation speed is crucial to the efﬁciency of DSE as it is directly
linked to the size of the design space that can be explored in a feasible manner, as de-
scribed in Section 2. In general there are a number of ways to speed up a co-simulation:

Speeding Up DSE through Compiled Master Algorithms

69

Maestro 1

Process mm/coe.json

Interpret

Maestro 2

Process mm/coe.json

Generate MABL

Interpret speciﬁcation

Write mabl.spec

Create C++ program

Compile

Execute

eters

m

a

r

a
P

e r nal

Ex t

Fig. 1: Maestro 1 and 2 execution ﬂow

1. Increase the step size (so fewer co-simulation steps are taken);
2. Reduce the complexity of the simulation (simplify the FMUs and their connec-

tions); or

3. Optimise the Master Algorithm

While the ﬁrst two options have the largest potential for speeding up co-simulations,
they both have the drawback that they require changes to the work already carried out,
which may not be possible while retaining the required ﬁdelity. Other circumstances,
such as a need to use legacy models, can also limit the ability of the engineer to opti-
mise the individual FMUs. Therefore, this paper focuses on the third option, which is a
generic approach that signiﬁcantly improves the co-simulation speed of the MA itself.
Maestro3 [12] is a tool which implements a co-simulation MA as a Java application
with support of multiple orchestration strategies. The earlier versions (maestro1) had
good performance at the time, however as shown in Figure 2, performance degrades
over time and incurs a high initial cost for each co-simulation run. Therefore a new in-
ternal architecture of Maestro was introduced by Thule et al. [13] (maestro2). This new
version separates the construction of the MA from the speciﬁc co-simulation execution
as shown in Figure 1. This was implemented as a extensible Java application that could
construct an MA and includes an interpreter for running co-simulations. This version
matches the behaviour of the initial Maestro implementation, but with better perfor-
mance without degraded performance over time (seen also in Figure 2). A main factor
for the improved performance is that all relations between signals are resolved during
speciﬁcation generation and not during run-time. Listing 1.1 illustrates how Maestro
can be used to produce the intermediate representation (mabl) of MA and how that can
be interpreted.

3 Maestro was formally known as the INTO-CPS Co-simulation Orchestration Engine or COE.

-2ex

70

(cid:7)
1 # Generate the mabl spec
2 java -jar $mabl import sg1 -fmu-search-path FMUs -output . \

3

4

Multi-models/mm/co-sim-51/mm.json \
Multi-models/mm/co-sim-51/co-sim-51.coe.json

5
6 # Interpret the spec using JAVA
7 java -jar $mabl interpret -runtime spec.runtime.json spec.mabl
(cid:6)

(cid:5)

Listing 1.1: Creation of MA and interpretation

(cid:7)
1 # Generate the mabl spec
2 java -jar $mabl import sg1 -fmu-search-path FMUs -output . \

3

4

Multi-models/mm/co-sim-51/co-sim-51.coe.json \
sim-dse/mm.json

5
6 # Interpret the spec using JAVA
7 java -jar $mabl interpret -runtime spec.runtime.json spec.mabl

8
9 # Generate native cpp simulator
10 java -jar $mabl export cpp -output cpp \
-runtime spec.runtime.json \
spec.mabl

11

12

13
14 # CMake
15 cmake -Bcpp/program -Scpp

16
17 # Compiling
18 make -Ccpp/program -j9

19
20 # Run native simulation
21 ./cpp/program/sim -runtime spec.runtime.json

(cid:6)

(cid:5)

Listing 1.2: Generation and execution of native simulation from of MA

The INTO-CPS project deﬁnes two types of ﬁles to conﬁgure a co-simulation, mm
and coe. These are JSON (Javascript Object Notation) ﬁles that deﬁne the FMUs,
their connections, and co-simulation properties. The spec.runtime.json deﬁnes
properties external to the co-simulation, such as the path to the output ﬁle.

In this paper we extend this work with the ability to make parameters external to
the MA. This allows reuse of the MA across DSE runs by storing the parameters in
the spec.runtime.json conﬁguration. This enables the MA to be converted to a
C++ application, thus removing the overhead of a Java interpreter and only having the
compilation overhead once per exploration by effectively generating a custom, native
MA for a given co-simulation. This signiﬁcantly improves the execution time but adds
a one-time extra compilation overhead which on average takes longer than starting the
Java process used for interpretation. The compilation overhead is directly related to the
systems CMake performance capabilities and the download of external libraries, where
the latter can be avoided if preinstalled. Once the make ﬁles for the native MA are ready

Speeding Up DSE through Compiled Master Algorithms

71

Fig. 2: Performance comparison between Meastro (COE), the new Meastro and the na-
tive C++ program.

for compilation, it is very fast to compile the native MA source ﬁle or any further MA
source ﬁles. Because of this it is clear that the beneﬁt of this native MA increases with
the design space size.

The new converter is implemented as an extension to Maestro and thus runs in Java.
The converter outputs a CMake project. The project includes a Maestro-speciﬁc library
for FMI and external dependencies to libzip and rapidjson and a single generated
co-sim.cxx ﬁle representing the current MA. The CMake initialisation will fetch all
dependencies and the subsequent compilation will compile all into a single executable
sim. This process can be seen in Listing 1.2. It consumes the same input ﬁles as the
interpreter on line 7 from Listing 1.1. This corresponds the lower right part of Maestro
2 in Figure 1.

The CMake project supports all major FMI target platforms Linux, MacOS and
Windows using MSYS with mingw. For DSE purposes the process can be further im-
proved by reusing a CMake generated project for subsequent explorations to avoid run-
ning the slow CMake initialisation and compilation of the external zip library. The only
change for a new generation with the same version of the tool will be co-sim.cxx.
Re-compilation of co-sim.cxx is signiﬁcantly faster than re-running CMake.

4 Case Study: Single-tank Water Tank

The single-tank water tank example [7] is used as a case study in this paper to demon-
strate the speedup of the proposed approach. The system, is composed of two FMUs,
namely a water tank and a controller FMU. The water tank FMU models a physical
water tank component, composed of a valve with two states (open/close) and a sensor
that outputs the level of the water in the tank. The controller FMU represents the digital
component that controls the behaviour of the water tank through the valve, based on the
current level of water.

The logic of the controller is straightforward: it attempts to keep the water level
between a deﬁned minimum and maximum at all times (visible in Figure 3a). Should
the water level go below a user deﬁned minimum lmin, the controller closes the valve.

-2ex

72

l
e
v
e
L

2

1.5

1

0.5

0

0

Water-tank Controller Behvaiour

lmax

lmin

20

40

t

Cstate

W Tlevel

(b)

(a)

Fig. 3: Visualisation of the water tank (left) and controller behaviour (right) over co-
simulation time t (adapted from [2]), where Cstate refers to the state of the controller,
1 for open, 50 for closed, and W Tlevel refers to the water level in the tank)

In case the level of water goes above a user deﬁned maximum lmax, the controller opens
the valve. The behaviour of the system is depicted in Figure 3b for lmin = 1 and lmax =
2. The minimum and maximum levels are used as design parameters in the DSE, with
the constraint that minimum must be strictly below the maximum, lmin < lmax.

5 Results

To investigate the speed up with the native MA over the existing Java version of Mae-
stro, a set of DSEs were run with differing design space sizes (i.e. number of combina-
tions and hence co-simulations) and simulated durations (i.e. number of co-simulation
steps needed to complete the co-simulation). The water tank multi-model introduced
above served as the design, using native C FMUs for both the water tank (singlewatertank-
20sim.fmu) and controller (watertankcontroller-c.fmu). Version 0.4.14 of the DSE scripts
were used as the baseline. For the native MA run, the scripts were modiﬁed to call the
compiled executable instead of calling the COE. This required a simple change to the
Common.py scripts, shown in Listing 1.3. Additional small changes were needed in
Output_CSV.py and Output_HTML.py ﬁles to handle different naming conven-
tions in the results generated by the experimental native MA.

The changes made however are not robust and did not include calls to generate
or compile the code. To fully integrate the option for native compilation, the scripts
need to be refactored, ideally with a layer of abstraction to handle the two paradigms
transparently to the higher-level DSE algorithm scripts. For example, various options
for the co-simulation are passed at run-time in the existing version (such as the co-
simulation end time) but are required at compile time in the native C++ version.

4 github.com/INTO-CPS-Association/dse_scripts/releases/tag/0.4.1

(January 2021)

Speeding Up DSE through Compiled Master Algorithms

73

(cid:7)
runTimeJson = {}
runTimeJson["environment_variables"] =
parsedMultiModelJson["parameters"]

runTimeJson["DataWriter"] = [

{

}

"filename": os.path.join(simFolderPath, "results.csv"
),

"type": "CSV"

]
jsonOutput = json.dumps(runTimeJson,

sort_keys=True, indent=4, separators=(",", ":"))

jsonOutputFile = open(filePath, ’w’)
jsonOutputFile.write(jsonOutput)
jsonOutputFile.close()
subprocess.run(

["../sim-dse/cpp/program/sim.exe", "-runtime", filePath

])

(cid:10)(cid:6)

(cid:5)

Listing 1.3: Calling of the native co-simulation in the DSE scripts

To try and estimate the scalability of both approaches, a range of orders of mag-
nitude were chosen for both the size of the design space and end time of the co-
simulations:

– Design space size: 1, 10, 100, and 500 and 1000 combinations; and
– Simulated duration (s): 1, 10, 100, 1000 and 10000 simulated seconds.

While 1000 combinations was attempted for the Java version, this taxed the memory
and disk capacity of the test computer and was abandoned after several attempts and
crashes. The test machine was a laptop with an Intel® Core™ i7-5600U Processor (4M
Cache, up to 3.20 GHz), 8GB DDR3L-12800 1600 MHz and an M2 solid state drive
running Windows 10 Pro (21H2). The native MA was compiled under the MSYS2
environment (a collection of tools for building native Windows app from a Bash-like
environment) using gcc 11.2.0-10, make 4.3-1 and cmake 3.23.0-1.

Table 1 shows the overall execution times as recorded by the DSE scripts for each
combination of design space and co-simulation length. Generation, compilation and re-
compilation time are not included in the timings since these are not incorporated into
the DSE scripts during these experiments. These incur the following penalties:

– Generate code from Maestro: 6 seconds
– Conﬁgure compilation with cmake: 2 minutes 34 seconds
– Compilation with make: 50 seconds
– Re-compilation after changing simulation duration: 6 seconds

Therefore, approximately 160 seconds should be added to the times in the Native
columns. This means that for a one-off DSE with a design space of less than 100, the

-2ex

74

Design space size (# of co-simulations)

1

10

100

500

1000

End time (s)

Java Native

Java Native

Java Native

Java

Native Native

1

10

5.00

0.46

35.60

4.14

264.64 43.50 1239.41 238.85

487.42

5.47

0.57

37.10

4.13

289.33 44.42 1290.90 237.45

493.73

100

6.50

0.48

39.48

4.21

271.24 45.86 1361.09 251.42

507.28

1000

10.37

0.61

41.84

5.88

329.07 63.00 1499.83 347.70

709.81

10000

15.17

2.16

113.59 22.48 885.56 239.38 3459.32 1181.84 2406.97

Table 1: Total time (s) to explore design spaces with increasing numbers of designs and
simulated duration for the existing Java implementation and native version. The shaded
columns indicate where a Native run is slower when accounting for the one-time cost
of generating and compiling the code.

Design space size

End time (s)

1

10 100 500

1

10

11x 9x 6x

10x 9x 7x

100

13x 9x 6x

1000

17x 7x 5x

10000

7x 5x 4x

5x

5x

5x

4x

3x

Table 2: Relative speed-up of native MA over the existing Java implementation.

existing Java implementation is faster (indicated by the red colour in the columns Ta-
ble 1). In practice, most engineers will run multiple DSEs on a single multi-model. For
example, running a small DSE to check the objectives are being calculated correctly,
then running again with different parameter sets and sweeps. In such cases, due to the
negligible re-compilation time, the initial cost of generation and compilation is amor-
tised as more DSEs are run.

While the raw data is useful, it is easier to interpret after some processing. Table 2
shows the relative speed up of the native MA over the Java by dividing the Java exe-
cution time by the native execution time, and rounding to the nearest whole number.
This shows that the native MA is signiﬁcantly faster on single co-simulations, though
the beneﬁt is less on the ‘longer’ co-simulations. It is not immediately clear why this is.
Similarly, as the design space increases, the speed up settles to around ﬁve times faster
for the native MA. Given however that it was not possible to complete a 1000-design
DSE using the Java version on the test machine, there may well be other factors at play,
since memory usage was not examined in this test.

Speeding Up DSE through Compiled Master Algorithms

75

Fig. 4: Effect of design space size on time to complete DSE (using logarithmic x-axis).

-2ex

76

Fig. 5: Log-log plot showing effect of design space size on time to complete DSE.

Speeding Up DSE through Compiled Master Algorithms

77

To further explore the data, they can be plotted on graphs. Given that there are two
dependent variables, two graphs are plotted. The ﬁrst is the effect of design space size
on the time to complete the DSE, with the various end times as different series, shown
in Figure 4. The second shows effect of end time on the time to complete a DSE, with
the various sizes of design space as different series, shown in Figure 6. Both ﬁgures use
a logarithmic x-axis. Similarly, in both ﬁgures, the Java version uses a solid line while
the native MA uses a dashed line, then each pair has the same tick marks and colours.
Figure 4 conﬁrms that for any given design space, the native version is faster. It also
highlights the growth of the Java version for the largest design space, and suggests that
were a 1000-design DSE completed it would reach into the hours range. The log-log
plot in Figure 5 suggesting a clear power law relationship, however the data is not robust
enough to draw any conclusions. The main takeaway from Figure 6 is the signiﬁcant
uptick in time taken for the longest co-simulation. The log-log plot in Figure 7 this is
still apparent, so it would be useful to characterise this with more data.

Since this was only a single example with two FMUs, it would be important to try a
range of examples and to run each multiple times to remove noise from the results. It is
likely that there are some effects of the DSE scripts not being optimised for the native
MA as well, since the changes made were experimental, there may well be superﬂuous
ﬁles written to disk, for example.

When considering the various caveats from these initial results however, a cautious
conclusion is that the native MA improves DSE by an order of magnitude. That is to say,
by switching to the native MA, a DSE can be 10-times larger and the co-simulations can
be 10-times longer for the a given time and compute cost. This is certainly of beneﬁt and
warrants further work to both characterise the speed up more robustly, and to create DSE
scripts that can handle generation, compilation and use of the native MA functionality.

6 Conclusions and Future Work

In this paper we propose an approach that optimises the speed of execution of co-
simulations. This is extremely useful in the context of Design Space Exploration (DSE).
In such a context, the number of co-simulations to be explored can be rather high, mak-
ing the speed of execution critical for time-bounded projects. In our work, we consider
the Maestro co-simulation engine, an extensible Java application, able to construct an
Master Algorithm (MA), and thereafter interpret it to execute the actual co-simulation.
Our approach consists in converting the MA into a custom, native C++ application, thus
reducing the overhead of the Java interpreter. The presented results show the beneﬁt of
such approach, however there are still some limitations related to the non-trivial setup,
and the need for refactoring of the DSE scripts to allow for both types of execution, as
well as open questions about the nature of the speedup achieved and relative scalability
of the approach. Nevertheless, the achieved speedup creates opportunities that extend
beyond DSEs deﬁned by design parameter sweeps. Consider the case where for each
FMU, there are a number of variants, and it is of interest to compare the behaviour of
such variants in co-simulation. This means that there will be several multi-models, on
which individuals DSEs should be performed, while also comparing the results across
alternatives. In such scenarios, the number of co-simulations to be executed would in-

-2ex

78

Fig. 6: Effect of end time on time to complete DSE (using logarithmic x-axis).

Speeding Up DSE through Compiled Master Algorithms

79

Fig. 7: Log-log plot showing effect of end time on time to complete DSE (using loga-
rithmic x-axis).

-2ex

80

crease rapidly and the approach presented in this paper may make such wider DSEs a
realistic prospect. In future work, we will investigate such scenarios and validate the
beneﬁts from the approach proposed in this paper.

Acknowledgements

We acknowledge the European Union’s support for the INTO-CPS and HUBCAP projects
(Grant Agreements 644047 and 872698). In addition we would like to thank Innova-
tion Foundation Denmark for funding the AgroRobottiFleet project, and the Poul Due
Jensen Foundation that funded our basic research for engineering of digital twins.

References

1. Fitzgerald, J., Larsen, P.G., Verhoef, M. (eds.): Collaborative Design for Embedded Systems
– Co-modelling and Co-simulation. Springer (2014), http://link.springer.com/
book/10.1007/978-3-642-54118-6

2. Frasheri, M., Thule, C., Macedo, H., Lausdahl, K., Larsen, P., Esterle, L.: Fault injecting
co-simulations for safety (Nov 2021), http://icsrs.org/index.html, 5th Interna-
tional Conference on System Reliability and Safety, ICSRS 2021

3. Gamble, C., Pierce, K.: Design space exploration for embedded systems using co-simulation.
In: Fitzgerald, J., Larsen, P.G., Verhoef, M. (eds.) Collaborative Design for Embedded Sys-
tems, pp. 199–222. Springer Berlin Heidelberg (2014)

4. König, C., Lausdahl, K., Niermann, P., Höll, J., Gamble, C., Mölle, O., Brosse, E., Bokhove,
T., Couto, L.D., Pop, A.: INTO-CPS Traceability Implementation. Tech. rep., INTO-CPS
Deliverable, D4.3d (December 2017)

5. Larsen, P.G., Fitzgerald, J., Woodcock, J., Gamble, C., Payne, R., Pierce, K.: Features of in-
tegrated model-based co-modelling and co-simulation technology. In: Bernardeschi, Masci,
Larsen (eds.) 1st Workshop on Formal Co-Simulation of Cyber-Physical Systems. LNCS,
Springer-Verlag, Trento, Italy (September 2017)

6. Lee, E.A.: Cyber Physical Systems: Design Challenges. Tech. Rep. UCB/EECS-2008-8,
EECS Department, University of California, Berkeley (Jan 2008), http://www.eecs.
berkeley.edu/Pubs/TechRpts/2008/EECS-2008-8.html

7. Mansﬁeld, M., Gamble, C., Pierce, K., Fitzgerald, J., Foster, S., Thule, C., Nilsson, R.: Ex-

amples Compendium 3. Tech. rep., INTO-CPS Deliverable, D3.6 (December 2017)

8. Modelica Association: Functional Mock-up Interface for Model Exchange and Co-

Simulation. https://www.fmi-standard.org/downloads (October 2019)

9. Rose, M., Fitzgerald, J.: Genetic algorithms for design space exploration of cyber-physical
systems: an implementation in into-cps. In: Hugo Daniel Macedo, C.T., (Editors), K.P. (eds.)
Proceedings of the 19th Overture Workshop. p. 96 (October 2021)

10. Stanley, A., Pierce, K.: Multi-objective optimisation support for co-simulation. In: Hugo
Daniel Macedo, C.T., (Editors), K.P. (eds.) Proceedings of the 19th Overture Workshop.
p. 96 (October 2021)

11. Thule, C., Lausdahl, K., Gomes, C., Meisl, G., Larsen, P.G.: Maestro: The
into-cps co-simulation framework. Simulation Modelling Practice and Theory 92,
45 – 61 (2019), http://www.sciencedirect.com/science/article/pii/
S1569190X1830193X

12. Thule, C., Lausdahl, K., Larsen, P.G., Meisl, G.: Maestro: The INTO-CPS Co-Simulation
Orchestration Engine (2018), submitted to Simulation Modelling Practice and Theory

Speeding Up DSE through Compiled Master Algorithms

81

13. Thule, C., Palmieri, M., Gomes, C., Lausdahl, K., Macedo, H.D., Battle, N., Larsen, P.G.:
Towards reuse of synchronization algorithms in co-simulation frameworks. In: Software En-
gineering and Formal Methods: SEFM 2019 Collocated Workshops: CoSim-CPS, ASYDE,
CIFMA, and FOCLASA, Oslo, Norway, September 16–20, 2019, Revised Selected Papers.
p. 50–66. Springer-Verlag, Berlin, Heidelberg (2019), https://doi.org/10.1007/
978-3-030-57506-9_5

