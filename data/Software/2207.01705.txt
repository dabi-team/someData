2
2
0
2

l
u
J

4

]
E
S
.
s
c
[

1
v
5
0
7
1
0
.
7
0
2
2
:
v
i
X
r
a

Do Not Take It for Granted: Comparing Open-Source Libraries
for Software Development Effort Estimation

Rebecca Moussa
University College London
London, United Kingdom
rebecca.moussa.18@ucl.ac.uk

Federica Sarro
University College London
London, United Kingdom
f.sarro@ucl.ac.uk

ABSTRACT
In the past two decades, several Machine Learning (ml) libraries
have become freely available. Many studies have used such libraries
to carry out empirical investigations on predictive Software Engi-
neering (SE) tasks such as software development effort estimation,
defect prediction and bug fixing time estimation. However, the dif-
ferences stemming from using one library over another have been
overlooked, implicitly assuming that using any of these libraries
to run a certain machine learner would provide the user with the
same or at least very similar results.

This paper aims at raising awareness of the differences incurred
when using different ml libraries for software development effort
estimation (SEE), which is one of most widely studied and well-
known SE prediction tasks.

To this end, we investigate and compare four deterministic ma-
chine learners, widely used in SEE, as provided by three of the most
popular ml open-source libraries written in different languages
(namely, Scikit-Learn, Caret and Weka).

We carry out a thorough empirical study comparing the per-
formance of these machine learners on the five largest available
SEE datasets in the two most common SEE scenarios sought from
the literature (i.e., out-of-the-box-ml and tuned-ml) as well as an
in-depth analysis of the documentation and code of their APIs.

The results of our empirical study reveal that the predictions
provided by the three libraries differ in 95% of the cases on average
across a total of 105 cases studied. These differences are significantly
large in most cases and yield misestimations of up to ≈ 3,000 hours
(≈ 18 months) per project. Moreover, our API analysis reveals that
these libraries provide the user with different levels of control
on the parameters one can manipulate, and a lack of clarity and
consistency, overall, which might mislead users, especially less
expert ones.

Our findings highlight that the ml library is an important design
choice for SEE studies, which can lead to a difference in performance.
However, such a difference is under-documented. This suggests that
end-users need to take such differences into account when choosing
an ml library, and developers need to improve the documentation
of their ml libraries to enable the end user to make a more informed

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

choice. We conclude our study by highlighting open-challenges
with potential suggestions for the developers of these libraries as
well as for the researchers and practitioners using them.

CCS CONCEPTS
• Software and its engineering → Software libraries and repos-
itories; • Computing methodologies → Machine learning.

KEYWORDS
Software Effort Estimation, Machine Learning Libraries, Scikit-
learn, Caret, Weka, Empirical Study

ACM Reference Format:
Rebecca Moussa and Federica Sarro. 2022. Do Not Take It for Granted: Com-
paring Open-Source Libraries for Software Development Effort Estimation.
In Proceedings of ACM Conference (Conference’17). ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
As Machine Learning (ml) becomes more pervasive and with the
advent of several open-source ready-to-use libraries, ml rapidly
continues to be more accessible to a greater number of users of
different levels of expertise. ml techniques which were once hardly
accessible to software engineers have now become widely available,
thus shifting the task of realising fully hand-coded techniques to
writing a few lines of API calls.

However, this democratization of ml technology may come with
some drawbacks. It has been observed that non-specialist users of-
ten revert to building out-of-the-box prediction models using these
libraries, without properly experimenting with or investigating the
extent to which this can influence their study/results [31]. Similarly,
previous studies have shown that it is not uncommon for software
engineers to use such techniques without any modifications or
enhancements [75].

This paper aims at raising awareness of the use of different
ml libraries for Software Development Effort Estimation (SEE), a
widely-known prediction problem in Software Engineering.

To this end we first reviewed 117 papers on SEE in order to
understand the way in which the ml libraries have been used in
previous work 1 and found that 64% of the studies (75 out of 117)
run techniques without considering or exploring different values
for their hyper-parameters (i.e., they use them out-of-the-box). Out
of the 36% that apply hyper-parameter tuning, 67% (28 out of 42) of
studies perform a manual trial and error, whereas the remaining
studies (33%) use a variety of techniques such as grid-search and
search-based techniques (e.g., Genetic Algorithms and Tabu Search).

1We examined 117 papers on ml for SEE, which were included in two previous literature
reviews published in 2012 [76] and 2019 [2].

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Therefore, we aim, herein, at addressing both of these scenarios:
the first which depicts the use of machine learners out-of-the-box,
where we investigate the extent to which building predictive models
using the library API as-is (i.e., with default settings) would yield
different results when using different libraries (i.e., out-of-the-box-
ml scenario), and the second which analyses the use of machine
learners with hyper-parameter tuning (i.e., tuned-ml scenario) when
using different libraries.

We investigate the above scenarios by carrying out a thorough
empirical study with four deterministic ml regression techniques
(i.e., CART, KNN, LR and SVR) on the five largest publicly available
datasets previously used in SEE studies [63, 64], following best
practice in empirical software prediction system assessments [69].
Moreover, we manually analyse the API documentation and the
source code of the three libraries under study in order to unveil
further characteristics.

The results of our empirical study revealed that there is a large
disagreement on the effort predictions achieved by a given machine
learner using these three libraries (i.e., 95% of the cases on average
across all 105 cases studied). This indicates that the choice of the
ml library can contribute to the conclusion instability observed
in previous SEE studies. Moreover, our analysis of 117 previous
SEE work revealed that more than half do not state the ml library
used (51%), and among those that do it, the majority only provides
partial information (72%), thus, making it difficult to reproduce and
replicate previous work.

Furthermore, our analysis of the API documentation and code
reveals a lack of consistency among different libraries, which might
not only lead to variance in the accuracy observed in our empirical
study, but it might also induce users to misuse these APIs.

Overall our findings highlight that the choice of the ml library,
for a given SEE study, is just as important as that of predictors or
evaluation measures, and we hope that the evidence provided herein
will encourage future work to not only report the ml library used
for ease of reproducibility and replicability, but also to consider the
choice of the library as an important design factor that can affect the
results. These results also call for actions from the ml developers to
improve the clarity and consistency of the documentation of these
libraries in order to enable the user to make an informed choice,
and we provide some suggestions and future work towards this
end.

To summarise, the contributions of our paper are:

- Raising awareness on the importance of the choice of ml

libraries for SSE.

- Carrying out a thorough empirical study comparing four
widely used SEE deterministic machine learners as provided
by three very popular open-source ml libraries on the five
largest SEE datasets.

- Providing an in-depth analysis of the APIs of these libraries
aiming at investigating possible reasons for/sources of the
large differences in accuracy observed in the empirical study.
- Identifying some initial suggestions for both developers and
users of ml libraries on how to improve their documentation
and usage and highlighting open-challenges to be addressed
in future work.

2 RELATED WORK
The increasing growth and success of the use of machine learning
has attracted software engineering researchers of various skill levels
to use popular and free-of-charge ml libraries to carry out Software
Engineering (SE) tasks, including critical prediction tasks such as
software development effort estimation [2, 12, 76], defect prediction
[27], bug fixing time estimation [7].

The number of such ml libraries has been rapidly growing and
their development is fast paced, leaving the engineer with a wide
range of tools to choose from. General reviews of popular data
analytics libraries can be found elsewhere [9, 22, 50].

If the results reported by these tools are largely consistent, then
one can be confident in using any of these tools. However, previous
work has shown that variances in results can arise from the use of
different frameworks for deep learning [21, 54], sentiment analysis
[30, 51, 52] and classification models in other application domains
[44, 48, 55, 56].

Moreover, recent studies have highlighted that developers fail
to grasp how to make ml, and more generally ai, libraries work
properly, and very often seek further documentation or support
from their peers on forums such as Stack Overflow [28, 29]. There-
fore, researchers have focused their attention on understanding
potential problems faced by engineers when using these libraries
[3, 6, 73] and to ultimately improve, both the software documenta-
tion [19, 28] and testing practices [41, 79] for ml/ai software.

To the best of our knowledge, there is no previous work that
investigates differences arising from the use of ml libraries for
predictive modelling in software engineering, precisely software
effort estimation. While previous studies in effort estimation have
shown that differences in training data, learning techniques, hyper-
parameter tuning and evaluation procedures can lead to variance
in the prediction result causing conclusion instability [32, 43, 47, 60,
62, 72], no study has investigated the impact that the use of different
ml tools to build the prediction model can have on the variance in
the estimates. Differently from previous work, which has mainly
investigated the variance of non-stochastic approaches [40, 54], we
intentionally design an empirical study in a way that reduces, as
much as possible, all those factors that can introduce stochasticity in
the machine learner output. This allows us to analyse the variances
in performance of a given machine learner, which are primarily
due to the use of the different ml libraries investigated in our work.

3 EMPIRICAL STUDY
In this section, we describe the design and results of the empirical
study we have carried out to assess the extent to which using
machine learners provided by different ml libraries might yield
different results.

3.1 Design
3.1.1 Research Questions. The first step towards identifying whether
there are any discrepancies in prediction accuracy performance
when using different ml libraries for SEE lies in recognizing the
frequency of its occurrence, which motivates our first research
question:

Do Not Take It for Granted: Comparing Open-Source Libraries for Software Development Effort Estimation

Conference’17, July 2017, Washington, DC, USA

RQ1. Prediction Accuracy: How often a given machine learner
provides different SEE results when built by using different ml li-
braries?

If we found that, in fact, the results differ in many cases, then it
is important to verify the way in which the SEE prediction perfor-
mance of a machine learner is affected depending by the ml library
used. Thus our second research question assesses:
RQ2. Change in Prediction Performance: How does the SEE
performance of a given machine learner change when built using
different ml libraries?

In other words, while RQ1 aims at revealing whether the number
of cases showing inconsistency in results due to the ml library used
is high, RQ2 further analyses these results by looking into the
magnitude of these differences to understand if they are relevant.
Lastly, we aim at shedding light on questions like "Would my
conclusion have been different had I used Scikit-Learn instead of
Caret?", "Would the results of my proposed approach have been
better or worse, compared to others, had another ml library been
used?". As previous study usually ranks machine learners based on
their estimation performance for comparison purposes, our third
and last research question asks:

RQ3. Change in Ranking: How does the ranking of SEE machine

learners change when considering different ml libraries?

If we find that different rankings are provided by different ml
libraries, this would point out possible conclusion instability threats
in previous (and future) studies given that, for example, a technique
that would rank first in a study using a given library, might not
have the same rank (i.e., first) when another library is used instead.

3.1.2 ml Libraries. In our empirical study we compare three of
the most popular open-source ml libraries written in different lan-
guages: Scikit-Learn [53] in Python, Caret [37] in R, and Weka
[10] in Java.

Scikit-Learn is a Python library, distributed under BSD license,
which includes a wide range of state-of-the-art supervised and
unsupervised machine learning techniques.

The aim of the Scikit-Learn is to provide efficient and well-
established machine learning libraries within a programming en-
vironment that is accessible to non-machine learning experts and
reusable in various scientific areas [14].

Scikit-Learn is designed to adhere to a number of engineering
principles, including the use of sensible defaults stating “Whenever
an operation requires a user-defined parameter, an appropriate
default value is defined by the library. The default value should
cause the operation to be performed in a sensible way.”

Caret, acronym for Classification and Regression Training, is an
R package, distributed under the GNU General Public Licence, con-
taining functions to streamline model training process for complex
regression and classification problems [37]. It was made publicly
available on CRAN in 2007 and it relies on several other R packages
that are not installed at the package start-up, but loaded as needed.
The aim of Caret is to provide the user with an easy interface for
the execution of several classifiers, allowing automatic parameter
tuning and reducing the requirements on the researcher’s knowl-
edge about the tunable parameter values, among other issues [38].
Weka, acronym for Waikato Environment for Knowledge Analy-
sis, is licensed under the GNU General Public Licence and was first

Table 1: Machine learners investigated and corresponding
class/method name in Scikit-Learn, Caret and Weka.

Machine Learner

CART

KNN

LR

SVR

rpart

REPTree
knn

Library Class/Method Name
Caret
SkLearn DecisionTreeRegressor
Weka
Caret
SkLearn KNeighborsRegressor
Weka
Caret
SkLearn
Weka
Caret
SkLearn
Weka

IBk
lm
LinearRegression
SimpleLinear
svmRadial
SVR
SMOReg

released in 1997 [10]. Weka provides implementations of machine
learning techniques that can be easily used through a simplified and
interactive graphical interface by users who cannot or do not need
to write code, or through an API that allows user to access the li-
braries from their own Java programs. The online documentation is
automatically generated from the source code and concisely reflects
its structure [26]. Weka provides the main methods supervised and
unsupervised machine learning techniques, as well as methods for
data pre-processing and visualization.

In our empirical study, we use the latest stable version for each
library at the time the study was done: Scikit-Learn 0.23.1, Caret
6.0.85, Weka 3.8 with Python 3.7.5, R 3.6.3, and Java 11, respec-
tively.

3.1.3 Machine Learners and Settings. In this section we briefly
introduce the machine learners we investigated in order to compare
the performance of the three ml libraries Caret, Scikit-Learn and
Weka. We specifically focus on a set of deterministic ml techniques
widely used in SEE in order to eliminate any randomness arising
from the stochastic nature of other machine learners, and thereby
to analyse differences stemming only from the library usage. These
techniques are Classification and Regression Tree (CART) [11], K-
Nearest Neighbours (KNN) [59], Linear Regression (LR) [66] and
Support Vector Regression (SVR) [15].

In Table 1, we list these machine learners together with the name
of the class implementing each of them within each ml library
studied herein.

To investigate the out-of-the-box-ml scenario, we use all the
techniques with the default settings provided by each of the ml
libraries, without applying any further modifications.

To investigate the tuned-ml scenario, we apply Grid Search as
a hyper-parameter tuning technique. We chose Grid Search over
other tuning technique (e.g., Random Search) in order to eliminate
any difference in performance resulting from external factors (i.e.,
any stochastic behaviour or anything other than the ml libraries
themselves). This allows us to accurately verify whether variances
in performance result from the ml libraries or from other factors
like from using Random Search for hyper-parameter tuning, for
example. We run each tool’s corresponding grid search method 30
times (i.e., GridSearchCV in Scikit-Learn, tuneGrid along with
trControl in Caret and GridSearch in Weka) using the same
inner cross-validation across all tools to minimize any possible

Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Table 2: Datasets used in our study.

Dataset
China
(499 projects)

Type Variable
CC

Input
Output
Enquiry
File
Interface
Effort
TeamExp
ManagerExp
Entities
Transactions
AdjustedFPs
Effort
AFP
Effort
SizeFP
Nlan
T01
T02
T03
T04
T05
T06
T07
T08
T09
T10
T11
T12
T13
T14
T15
Effort
SCRN
FORM
FILE
Effort

Min
0.00
0.00
0.00
0.00
0.00
26.00
0.00
0.00
7.00
9.00
73.00
546.00
15.36
219.00
48.00
1.00
1.00
1.00
2.00
2.00
1.00
1.00
1.00
2.00
2.00
2.00
2.00
2.00
1.00
1.00
1.00
583.00
0.00
0.00
2.00
896.00

Max
9404.00
2455.00
952.00
2955.00
1572.00
54620 .00
4.00
4.00
386
661.00
1127.00
23490.00
18140
113900.00
3643.00
4.00
5.00
5.00
5.00
5.00
5.00
4.00
5.00
5.00
5.00
5.00
5.00
5.00
5.00
5.00
5.00
63694.00
281.00
91.00
370.00
253760.00

Mean
167.10
113.60
61.60
91.23
24.23
3921.00
2.30
2.65
121.54
162.94
284.48
4903.95
527.70
3113.00
673.31
2.55
3.05
3.05
3.02
3.19
3.05
2.90
3.24
3.81
4.06
3.61
3.42
3.82
3.06
3.26
3.34
8223.20
33.69
22.38
34.81
13996.00

Std. Dev.
486.34
221.27
105.42
210.27
85.04
6481.00
1.33
1.52
86.11
146.09
182.26
4188.19
1521.99
9598.00
784.04
1.02
1.00
0.71
0.89
0.70
0.71
0.69
0.90
0.96
0.74
0.89
0.98
0.69
0.96
1.01
0.75
10500.00
47.24
20.55
53.56
36601.56

Desharnais
(77 projects)

WC

Kitchenham
(145 projects)
Maxwell
(62 projects)

CC

CC

Miyazaki
(48 projects)

CC

stochastic behaviour deriving from the data splits and using the
same set of values for the hyper-parameters across all three libraries.
In order to maintain a fair comparison, we tune the parameters that
are found in common across all three libraries. The settings used
for both the out-of-the-box-ml and tuned-ml scenarios can be found
in our on-line appendix [45].

3.1.4 Datasets. To empirically investigate our RQs we used the
largest SEE publicly available datasets (namely, China, Desharnais,
Kitchenam, Maxwell, Miyazaki) containing a diverse sample of in-
dustrial software projects developed by a single company or several
software companies [1]. These datasets exhibit a high degree of
diversity both in terms of number of observations (from 48 to 499),
number and type of features (from 3 to 17), technical character-
istics (e.g., software projects developed in different programming
languages and for different application domains), number of com-
panies involved and their geographical locations. Furthermore, all
these datasets have been widely used in several SEE studies (see
e.g., [23, 35, 61, 63–65, 68, 70]).

A comprehensive description of these datasets, together with
the actual data is available on-line [45], while in Table 2 we re-
port for each of the datasets its type (Within-Company –WC– or
Cross-Company –CC– [42]), number of projects, and the descriptive
statistics of the dependent variable (i.e., Effort) and the independent
variables used to build the prediction models.

3.1.5 Validation. In order to eliminate any possible variance in
performance caused by non-deterministic influencing factors such
as the validation approach employed or the sampled data used as

denoted by Rahman et al. [58], we perform a Leave-One-Out cross-
validation (LOO), where each instance (i.e., a software project in
our case) of a dataset of 𝑛 observations is considered to be a fold. At
each iteration, the learning process includes training a prediction
model on 𝑛 − 1 instances and testing it on the one instance left out.
LOO is a deterministic approach that, unlike other cross validation
techniques, does not rely on any random selection to create the
training and testing sets. Therefore, this validation process is fully
reproducible and eliminates conclusion instability caused by ran-
dom sampling [34, 63]. However, it has been observed that LOO
may give more optimistic results than those that might realistically
be achieved in practice [71] and if chronological information about
the projects is available it would be preferable to adopt a time-based
validation approach [63, 71]. In our study we use LOO because start
and completion dates are not available for all projects.

3.1.6 Evaluation Measures. Several measures have been proposed
to evaluate the accuracy of effort estimation prediction models.
They are generally based on the absolute error, i.e., |ActualEffort −
EstimatedEffort|.

In our study we use the Mean Absolute Error (MAE) as it is un-
biased towards both over- and under-estimation [39, 69]. Given
a set of 𝑁 projects and the measured (𝐴𝑐𝑡𝑢𝑎𝑙𝐸 𝑓 𝑓 𝑜𝑟𝑡𝑖 ) and esti-
mated (𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒𝑑𝐸 𝑓 𝑓 𝑜𝑟𝑡𝑖 ) effort for each of the the project 𝑖 in this
set, MAE is defined as follows: 𝑀𝐴𝐸 = 1
𝑖=1 |𝐴𝑐𝑡𝑢𝑎𝑙𝐸 𝑓 𝑓 𝑜𝑟𝑡𝑖 −
𝑁
𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒𝑑𝐸 𝑓 𝑓 𝑜𝑟𝑡𝑖 |.

(cid:205)𝑁

We also use statistical significance tests to assess differences in
the performance of the ml libraries (RQ2) and the way they do rank
machine learners (RQ3).

In order to evaluate whether the differences in MAE values result-
ing from the use of various ml libraries are statistically significant
(RQ2), we perform the Wilcoxon Signed-Rank Test [77] at a confi-
dence limit, 𝛼, of 0.05, with Bonferroni correction (𝛼/𝐾, where 𝐾
is the number of hypotheses) when multiple hypothesis are tested.
Unlike parametric tests, this test does not make any assumptions
about underlying data distributions. The null hypothesis that is
tested in our work follows: "There is no significant difference in the
MAE values obtained by the approaches when built using a different
ml library". We also compute the Vargha and Delaney’s ˆ𝐴12 non-
parametric effect size measure to assess whether any statistically
significant difference is worthy of practical interest [4]. ˆ𝐴12 is com-
puted based on the following formula ˆ𝐴12 = (𝑅1/𝑚 − (𝑚 + 1)/2)/𝑛,
where 𝑅1 is the rank sum of the first data group we are compar-
ing, and 𝑚 and 𝑛 are the number of observations in the first and
second data group, respectively. When the two compared groups
are equivalent ˆ𝐴12 = 0.5. An ˆ𝐴12 higher than 0.5 denotes that the
first data group is more likely to produce better results. The effect
size is considered small when 0.5 < ˆ𝐴12 ≤ 0.66, medium when
0.66 < ˆ𝐴12 ≤ 0.75 and large when ˆ𝐴12 > 0.75, although these
thresholds are not definitive [65].

In order to verify statistical significance, in the ranking provided
by the Scikit-Learn, Caret and Weka APIs (RQ3), we use the
Friedman Test [25]. This is a non-parametric test which works with
the ranks of the data groups rather than their actual performance
values, making it less susceptible to the distribution of the perfor-
mance of these parametric values. When a significant difference

Do Not Take It for Granted: Comparing Open-Source Libraries for Software Development Effort Estimation

Conference’17, July 2017, Washington, DC, USA

is found, the Nemenyi test [49] is often recommended as a post-
hoc test to identify the data groups with a statistically significant
difference [20]. The performance of two data groups is thought to
be statistically significantly different if the corresponding average
ranks differ by at least the Critical Distance (CD) [20]. The results
of this test are presented in a diagram which is used to compare
the performance of multiple techniques by ranking them based on
each ml library. It consists of an axis, on which the average ranks
of the methods are plotted and of the CD bar. The CD bars of the
groups of classifiers whose values are significantly different do not
overlap. The visualisation used herein is a more recent version of
the one presented in Demsar’s work [20], which aims at providing
an easier interpretation. It can be obtained by using the Nemenyi
function in R.

3.2 Results
In this Section we report the results obtained in our empirical study
for each of the RQs explained in Section 3.1.1.

3.2.1 RQ1: Prediction Results. Our first research question asks how
often a certain prediction model built using different ml libraries for
SEE provides same results. In order to answer this question, we look
at the cases where ml libraries result in different predictions. As
described in Section 3, we explore two common scenarios observed
in the SEE literature, the use of out-of-the-box prediction models
(i.e., out-of-the-box-ml) and their use when hyper-parameter tuning
is performed (i.e., tuned-ml).

Tables 3a and 3b show the results for each scenario, respectively.
As one can observe from Table 3a (i.e., out-of-the-box-ml scenario),
there are only five out of the 60 cases under study (8%) where two
out of the three ml libraries provide the same predictions. Specifi-
cally, LR achieves the same outcome when built using Scikit-Learn
and Caret, whereas Weka obtains different results. Differences in
prediction are observed for all other techniques when built using
any of the three ml libraries.

As for the second scenario, we explore prediction models which
are tune-able (i.e., they consist of parameters for which hyper-
parameter tuning can be applied): CART, KNN and SVM.2 Table
3b shows that there is a difference in prediction in all of the cases
studied (i.e., there is no case where all three libraries agree on the
same prediction).

3.2.2 RQ2: Change in Prediction Performance. Our second research
question investigates how the performance of a given SEE tech-
nique changes when the prediction model is built using different
ml libraries.

Tables 3a and 3b show the difference in the MAE results for
each technique obtained by comparing each pair of the ml libraries
under study for the two scenarios analysed. We denote any positive
difference by the up arrow, whereas any negative difference is
represented by the down arrow. For each pair of tools, a positive
difference signifies that the ml library listed as first achieves a higher
MAE value (i.e., a less accurate prediction) than the other library. On
the other hand, a negative difference denotes that the first library
provides an MAE value lower than the one provided by the second
library (i.e., it provides a better prediction). For example, when

2We exclude LR as there are no tune-able parameters common to all libraries

considering CART, specifically comparing it when it is built using
Scikit-Learn and Caret (i.e., denoted as Sk vs. C) the difference in
MAE is ↑ 809 meaning that when CART is built using Scikit-Learn,
it achieved a higher MAE (i.e., worse prediction performance) than
that obtained when CART is built using Caret with a difference of
809 man-hours.

In order to study the magnitude of the error between the ml
libraries, we analyse our results using three error ranges: a dif-
ference of 100, 500 and 1,000 hours. By doing so we relax our
constraint on what we consider the difference to be impactful by
allowing/accepting a larger error between the results obtained by
the libraries.

Table 4 present the number of times the difference in results falls

within these three ranges.

When considering the out-of-the-box-ml prediction scenario, we
can observe that while Scikit-Learn and Caret provide the same
MAE for LR only, they, along with Weka achieve different results
in all other cases (i.e., 51 out of the 60) with a difference of at least
100 hours. As shown in Table 4, the magnitude of the discordance
in performance is very large for every pair of ml library under
comparison with 22% of cases reaching a difference of at least a
1,000 hours.

When analysing the MAEs using a threshold ≥ 100 hours, results
show that Scikit-Learn and Caret are the least discordant pair
with more than half the cases (55%) having a difference of at least
100 hours. When comparing Scikit-Learn with Weka and Caret
with Weka, a difference ≥ 100 is seen in all the cases considered.
Moreover, we also found cases with a difference of at least 500
hours.

Specifically, between Scikit-Learn and Caret there is a dif-
ference ≥ 500 hours in 25% of the cases (5 out of 20), whereas
between Caret and Weka it exists in 45% of the cases and between
Scikit-Learn and Weka, it is in 60% of the cases (12 out of 20).

One would not expect to obtain a difference of such a high
magnitude resulting from the use of one ml library over another,
however these cases exist even with a difference ≥ 1,000 hours. As
we can observe from Table 4, when comparing the results obtained
by Scikit-Learn with those by Caret, there is a difference of
at least 1,000 hours in 15% of the cases under study. As for the
differences between the other two pair of libraries (i.e., Scikit-
Learn vs. Weka and Caret vs. Weka), this difference is seen in
25% of the cases.

When considering the tuned-ml scenario, we observe that differ-
ences occur more frequently than in the out-of-the-box-ml scenario.
We found a difference of at least 100 hours in 80% of the cases (12
out of 15) for Scikit-Learn vs. Caret, in 87% of the cases (13 out
of 15) for Scikit-Learn vs. Weka and in 67% of the cases (10 out
of 15) for Caret vs. Weka (Table 4). Several differences still persist
when we consider a difference of at least 500 hours: specifically in
47% of the cases (7 out of 15) when comparing each pair of libraries
(i.e., Scikit-Learn vs. Caret, Scikit-Learn vs. Weka and Caret
vs. Weka). By considering any difference less than a 1,000 hours
to be negligible, we observe that differences ≥ 1,000 still exist in
33% of the cases for Scikit-Learn vs. Caret, in 20% of the cases
for Scikit-Learn vs. Weka, and in 27% for Caret vs. Weka.

These results reveal that differences as large as or even larger
than 1,000 hours still exist for almost all techniques. Such magnitude

Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Table 3: RQ1-RQ2: Differences in MAE values obtained when building prediction models using Scikit-Learn (Sk), Caret (C)
and Weka (W) in the (a) out-of-the-box-ml scenario and the (b) tuned-ml scenario. The ↑ indicates that the 1𝑠𝑡 tool produces
worse predictions than the 2𝑛𝑑 , whereas the ↓ indicates that 1𝑠𝑡 tool produces better predictions than the 2𝑛𝑑 .

CART

KNN

LR

SVM

Sk vs. C Sk vs. W C vs. W Sk vs. C Sk vs. W C vs. W Sk vs. C Sk vs. W C vs. W Sk vs. C Sk vs. W C vs. W
↓643
China
↓396
Desharnais
Kitchenham ↓46
Maxwell
Miyazaki

↑833
↑861
↑161
↑2242
↑443
(a) out-of-the-box-ml scenario.

↓359
↓440
↓134
↓1591
↓714

↑420
↓226
↑ 177
↓450
↑1229

↑734
↑972
↑125
↑2435
↑518

↓637
↓577
↓683
↓1319
↓3188

↑420
↓226
↑ 178
↓450
↑1229

↑166
↑568
↓181
↑1983
↓2231

↓278
↓137
↓550
↑272
↓2474

↓477
↑172
↓227
↓1095
↓578

↓99
↑111
↓36
↑193
↑75

↓3079
↑1652

0
0
0
0
0

CART

KNN
Sk vs. C Sk vs. W C vs. W Sk vs. C Sk vs. W C vs. W Sk vs. C Sk vs. W C vs. W
↓809
↓1,113

SVM

China
Desharnais
Kitchenham ↑185
Maxwell
Miyazaki

↓2,515
↑3,053

↓560
↓457
↑247
↓652
↑1,872

↑249
↑656
↑62
↑1,864
↓1,181

↑10
↓29
↑14
↓421
↓1,104

↓6
↑160
↓25
↑668
↓401

↓16
↑189
↓39
↑1,089
↑703

↓421
↓336
↓250
↓1,044
↓966

↓598
↓419
↓237
↓1,738
↓2,232

↓177
↓83
↑14
↓694
↓1,266

(b) tuned-ml scenario.

in difference would not only have an impact on a project’s feasibility
and completeness but can also change the conclusion of any study
proposing or comparing a new or existing techniques as we further
investigate in RQ3.

The statistical significance test results show that when compar-
ing Scikit-Learn and Caret, the differences in MAE values still
prove to be statistically significantly different (p-value < 0.01 and
A12 = 0.55) despite the low statistical power due to the small sample
size being tested. The same observation could not be made when
comparing Scikit-Learn and Weka (p-value = 0.164), and Caret
and Weka (p-value = 0.310) even though the MAE results reported
in Table 6 show a large variance between the three ml libraries (for
three out of the four techniques investigated). This could be due to
the small sample size which can result in a Type II error, where the
test fails to reject the null hypothesis, even though it should not be
the case.

In order to further understand the severity of the error, we also
compute the relative deviation in effort estimation with respect
to the mean actual effort of the projects in a given dataset (see
Table 2). For example, if the average actual effort spent for realising
projects in a company is 10,000 hours then it is less severe to have
a deviation ≥ 1,000 hours than with an average actual effort of
50 hours. We observe that for the out-of-the-box-ml scenario, the
largest differences between ml libraries range from 20% to 37%
depending on the dataset. For example, we can observe a difference
of 3,188 hours in the results achieved by Scikit-Learn and Weka
on the Miyazaki dataset, which corresponds to a 23% deviation in
effort estimation with respect to the actual effort. As for the tuned-
ml scenario, we can observe that the deviation ranges between
8% and 31%, with four out of the five datasets having a deviation
greater than 20%.

3.2.3 RQ3: Change in Ranking. In order to further understand the
magnitude of the difference and to investigate whether it can have
an impact on a study’s conclusion, we analyse the ranking of the

Table 4: RQ2: Number of cases the results provided by a
given ml library differ from another of at least 100 hours,
500 hours and 1,000 hours for the out-of-the-box-ml scenario
and the tuned-ml scenario.

Prediction
Difference

≥ 100 h
≥ 500 h
≥ 1,000 h

≥ 100 h
≥ 500 h
≥ 1,000 h

≥ 100 h
≥ 500 h
≥ 1,000 h

out-of-the-box-ml

tuned–ml

CART KNN LR SVM Overall CART KNN LR Overall

0
0
0

5
2
1

Sklearn vs. Caret
2
0
0
Sklearn vs. Weka
5
4
1

5
1
1

5
5
2

Caret vs. Weka

5
3
1

5
1
1

5
2
1

11
5
3

20
12
5

20
9
5

5
4
3

5
3
1

4
3
2

4
3
2

5
2
1

5
3
2

5
2
1

2
1
1

Sklearn vs. Caret
12
7
5
Sklearn vs. Weka
13
7
3
Caret vs. Weka

3
1
0

5
3
2

3
2
1

3
2
1

10
7
4

techniques, based on the MAE achieved, according to each library
separately and assess the differences.

The results for out-of-the-box-ml scenario, presented in Tables 5a
and 6a, reveal that there is no case where all three techniques agree
on a single ranking. The rankings obtained when using Weka are
different from those obtained by Scikit-Learn and Caret for each
of the datasets investigated. Whereas, Scikit-Learn and Caret
provide a same ranking for three out of the five (60%) datasets
under study (i.e., China, Kitchenham, Miyazaki). As for the tuned-
ml scenario, we can observe that all three tools provide the same
ranking on only one dataset (i.e., Kitchenham). Whereas for the
remaining four, Caret and Scikit-Learn agree on three datasets
(i.e., China, Maxwell, Miyazaki), whereas all three libraries disagree
on the fourth (i.e., Desharnais). While Scikit-Learn and Caret
seem to generally agree at least on some of the ranks, Weka tends
to provide completely different rankings.

Do Not Take It for Granted: Comparing Open-Source Libraries for Software Development Effort Estimation

Conference’17, July 2017, Washington, DC, USA

Table 5: RQ3: Rankings of prediction models based on the MAE results obtained by each of the ml libraries for each of the five
datasets for the (a) out-of-the-box-ml scenario and the (b) tuned-ml scenario.

China
SkLearn Caret Weka
SVM
LR
LR
KNN
KNN
LR
SVM CART SVM
SVM
CART KNN
CART

Kitchenham
Desharnais
SkLearn Caret Weka
SkLearn Caret Weka
LR
LR
LR
LR
KNN
KNN
KNN
KNN
SVM
LR
SVM KNN
SVM CART SVM
CART CART CART
CART KNN

Maxwell
SkLearn Caret Weka
KNN
KNN
SVM SVM
LR
LR
KNN
SVM
CART CART CART

Miyazaki
SkLearn Caret Weka
KNN
KNN
SVM LR
SVM
LR
CART
LR
CART KNN

SVM
LR

CART

CART

SVM

LR

(a) out-of-the-box-ml scenario.

China
SkLearn Caret Weka
KNN
KNN
SVM
SVM KNN
SVM
CART CART CART
CART

Desharnais
SkLearn Caret Weka
KNN
KNN
SVM
CART KNN
SVM
SVM CART CART

Kitchenham
SkLearn Caret Weka
KNN
KNN
KNN
SVM SVM
SVM
CART CART CART

Maxwell
SkLearn Caret Weka
KNN
KNN
SVM
SVM KNN
SVM
CART CART CART

Miyazaki
SkLearn Caret Weka
KNN
KNN
SVM
SVM KNN
SVM
CART CART

(b) tuned-ml scenario.

Table 6: RQ1-3: MAE results obtained by each of the ml libraries for each of the five datasets for the (a) out-of-the-box-ml
scenario and the (b) tuned-ml scenario.

Dataset

SkLearn
China
3,606.42
Desharnais
2,906.33
Kitchenham 2,711.59
Maxwell
7,564.52
Miyazaki
12,058.73

CART
Caret
2,963.80
2,510.66
2,665.82
4,485.85
13,710.94

Weka
3,129.69
3,078.18
2,484.42
6,469.05
11,480.30

SkLearn
2,798.93
2,199.65
2,069.70
3,850.34
8,644.25

KNN
Caret Weka
3,532.56
2,699.45
3,171.84
2,310.67
2,194.41
2,033.88
6,285.81
4,043.83
9,162.33
8,718.85

SkLearn
2,647.40
2,277.26
1,645.08
4,448.94
11,700.77

LR
Caret
2,647.56
2,277.23
1,644.95
4,449.08
11,700.95

Weka
3,067.63
2,051.05
1,822.51
3,999.02
12,930.09

SkLearn
3,108.56
2,754.36
2,279.89
5,741.87
10,346.04

SVM
Caret Weka
2,471.66
2,749.33
2,177.32
2,314.77
1,596.76
2,146.26
4,423.29
4,150.91
7,158.33
9,632.12

(a) out-of-the-box-ml scenario

Dataset

SkLearn
China
3,681.04
Desharnais
3,535.04
Kitchenham 2,478.44
Maxwell
7,232.53
Miyazaki
10,497.33

CART
Caret
2,872.11
2,422.54
2,663.92
4,717.38
13,550.06

Weka
3,120.91
3,078.18
2,725.83
6,580.89
12,368.94

SkLearn
2,587.57
2,249.66
1,985.66
4,285.81
9,840.04
(b) tuned-ml scenario

KNN
Caret Weka
2,581.95
2,597.50
2,409.41
2,220.29
1,960.95
1,999.46
4,953.90
3,864.54
9,439.13
8,736.13

SkLearn
3,117.41
2,760.77
2,282.58
5,743.15
10,347.54

SVM
Caret Weka
2,519.55
2,696.23
2,342.04
2,424.63
2,045.81
2,032.15
4,005.16
4,699.31
8,115.88
9,382.00

These observations are confirmed by the statistical significance
test analysis: The Friedman test shows that the difference between
the results of the techniques is statistically significant (p-value <
0.05) for all ml libraries in both the out-of-the-box-ml and tuned-ml
scenarios. A more fine grained analysis can be performed when
observing the plots generated by the post-hoc Nemenyi Test. Figure
1 shows that, for the out-of-the-box-ml scenario, Weka provides a
different ranking than that obtained by Scikit-Learn and Caret,
with a statistical significant difference between the best and worst
performing techniques (i.e., KNN and CART, respectively) in the
case of Scikit-Learn and Caret. While Scikit-Learn and Caret
rank KNN first and SVM third, Weka ranks KNN in third place
and SVM in the first in the out-of-the-box-ml scenario. As for the
tuned-ml scenario, we can observe that Weka also provides a dif-
ferent ranking than the other two ml libraries (i.e., Scikit-Learn
and Caret) as it ranks SVM and KNN first and second, respectively.
Whereas Scikit-Learn and Caret both agree on ranking KNN first

and SVM second. All three ml libraries show a statistically signifi-
cant difference between the best and worst ranked techniques (i.e.,
KNN and CART for Scikit-Learn and Caret and SVM and CART
for Weka). While Scikit-Learn and Caret agree on the ranking,
the results of RQ2 had revealed that the MAE values achieved by
these two ml libraries are statistically significantly different. This
shows that even when they output the same rankings of the ml
techniques, the results of the same techniques vary significantly
depending on the ml library used, and therefore larger estimation
errors can be committed when using one library over another.

4 API ANALYSIS
The authors of this paper independently investigated the API doc-
umentation and source code (function body) of Caret3, Scikit-
Learn4 and Weka5 for each of the techniques listed in Table 1 in

3https://topepo.github.io/caret/
4https://scikit-learn.org/stable/modules/classes.html
5https://waikato.github.io/weka-wiki/documentation/

Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Figure 1: RQ3: Ranking of the ml techniques based on the results of the Nemenyi Test for the (a) out-of-the-box-ml scenario
and the (b) tuned-ml scenario. The worst performing technique (with the highest MAE) is displayed at the top.

order to investigate similarities and differences in (1) the algorithm
reference for each machine learner; (2) the signature of the main
method offered by these libraries to build each machine learner
(i.e., the number and type of input parameters that can be set by
the user through the use of their API) and the naming convention
used; (3) the parameters’ default values and the values that can be
assigned for categorical parameters within each library; (4) the pa-
rameters available only in the source code (i.e., hidden parameters).
We summarise our findings in Table 7 and discuss them below.
Algorithm Reference. We observed that the documentation of
these libraries does not always clearly state a reference algorithm.
Specifically, no reference was provided in seven out of the 12 cases
we looked into (with Caret never providing any reference for any
of the cases). We also found that, among the cases where references
are given, Scikit-Learn usually provides a general list of references
for a given algorithm. For example, Scikit-Learn provides a list of
four references for CART (including Wikipedia), without indicating
the one actually being used for the implementation. Whereas among
the cases where Weka provides a reference, it is a single specific
one.
Method Signature. We found that none of the libraries offer a same
method signature to build each of the machine learner investigated
herein (i.e., CART, KNN, LR and SVR). All three libraries provide a
different number of parameters which can be directly manipulated
by the user thorough the API as shown in Table 7. Scikit-Learn
provides the user with the highest number of parameters compared
to Caret and Weka across all techniques. For example, in Scikit-
Learn, a user can build CART by specifying 14 input parameters,
while Caret and Weka only allow the user to specify nine and
six parameters, respectively. Similarly, a user can build KNN by
specifying eight input parameters in Scikit-Learn, while Caret

Table 7: Summary of the API Analysis in terms of number
of total parameters per ml library and machine learner, and
the number of parameters matching a same functionality, a
same name, and a same default value across all libraries and
each pair of libraries.

CART KNN LR

SVM

Total Number of parameters

Caret
SkLearn
Weka

9
14
6

1
8
5

1
4
0

3
11
10

Parameters matching functionality across

All libraries
SkLearn & Weka
SkLearn & Caret
Caret & Weka

2
3
4
2

1
3
1
1

0
0
1
0

Parameters matching name across

All libraries
SkLearn & Weka
SkLearn & Caret
Caret & Weka

0
0
1
0

0
0
0
1

0
0
1
0

Parameters matching default values across

All libraries
SkLearn & Weka
SkLearn & Caret
Caret & Weka

0
0
1
0

0
1
0
0

0
0
1
0

2
6
3
2

1
1
2
1

0
1
1
0

and Weka only provide the user with the ability to specify one and
five parameters, respectively.

Among the parameters which are provided in an API but not
in another, some do not directly have an effect on the accuracy of

   CaretSkLearnWekaSkLearnWekaCaret                                               (b) Hyper-parameter tuning scenario.(a) Out-of-the-box scenario.(a) out-of-the-box-ml scenario(b) tuned-ml scenarioDo Not Take It for Granted: Comparing Open-Source Libraries for Software Development Effort Estimation

Conference’17, July 2017, Washington, DC, USA

the ml techniques, but they play a role in the execution process
(e.g., 𝑛_𝑗𝑜𝑏𝑠 in KNN controls the number of parallel jobs to run
for neighbors search); whereas others control the machine learner
hyper-parameters, therefore their use/setting can directly impact
the accuracy of the models. For example, while Caret only allows
the user to set the number of neighbours to be explored for KNN,
Weka permits setting the number of neighbours and the weight
function, and Scikit-Learn provides the user with the freedom to
choose the number of nearest neighbours, the weight function of the
neighbours, the algorithm used to compute the nearest neighbours,
and the distance function.

On average, across all four techniques, Scikit-Learn provides
the users with the ability to manipulate almost two times the num-
ber of parameters provided by Caret and three times those made
available by Weka. Overall, we can state that Scikit-Learn gives
the user more control over the parameters and, therefore, the per-
formance of these techniques.

We also observe that the number of common parameters that
perform the same functionality across the three libraries is very low.
The highest number of common parameters is only two (i.e.,
both CART and SVM have two common parameters, while KNN
has only one and LR does not have any in common across the three
libraries). If we consider each pair of libraries, we observe that
Scikit-Learn and Weka have more parameters in common with
respect to Caret, yet the numbers remain relatively low with the
best case being six parameters in common between Scikit-Learn
and Weka out of the 11 available ones for SVM.

Specifically, in two cases out of 12 there is no common parameters
between each pair of libraries. There is only one parameter in
common in three cases, two common parameters in two cases, three
common parameters in three cases. One case has four parameters
in common and the remaining one has six common parameters as
described above.

Even in the case where a method signature has a given parameter
in common among these libraries, the parameter name, its default
value, or possible categorical values, might differ across the libraries
as further explained below.
Parameter Name: The libraries refer to a given parameter with
different names, except for one case where all three libraries use the
same name. This makes it non-trivial for a user to match concepts
across different libraries. For example, the parameter used to specify
the distance function for the KNN model is called metric in Scikit-
Learn, while in Weka it is named -A and in Caret it is not provided.
Investigating this matter between each pair of libraries, we found
that in only one case, out of a total of 12, two parameters have
the same name, five cases had a single parameter with a common
name, while all remaining cases (i.e., six cases) did not share any
parameters using a same naming convention.
Parameter Value: We also inspected both the documentation and
source code in order to extract the number of parameters which are
set to the same default values in Scikit-Learn, Weka and Caret.
This investigation revealed that no parameter has the same default
value across all three libraries. We also compared the parameters’
default values for each pair of libraries. We found that there were
five cases where only one parameter had the same default value,
while all the remaining cases (i.e., seven cases) did not have any
parameters with the same default value. For example, Scikit-Learn

and Weka set a different default value in KNN for the number of
nearest neighbours to be explored by the algorithm. Scikit-Learn
gives its n_neighbors parameter a value of five, while Weka sets
its -K parameter to one. Lastly, we observe that even if a given
parameter is common among these libraries, they might provide
the users with different value options to set categorical ones. For
example, when building KNN, Weka allows the user to weigh
neighbours by the inverse of their distance or by calculating 1−
their distance. On the other hand, Scikit-Learn allows the user to
choose the value of this parameter among three different options:
using uniform weights, using the inverse of their distance, or by
creating a user-defined function which allows the user to assign
specific weights.
Hidden Parameters. A closer look at the implementation of these
libraries revealed that some parameters that are made available to
the user through API methods by one library are instead buried
(i.e., hidden) in the source code of another library. By inspecting
the source code, we found that each of Weka and Caret does not
directly expose to the user a number of parameters (i.e., they have
hidden parameters) that could instead be configured through the
API of the other two libraries. Specifically, Caret has one hidden
parameter in CART and four hidden ones in SVM, while Weka has
one in CART. Scikit-Learn is the only library which does not have
any hidden parameter that could have been matched with those
made available by Weka’s and Caret’s APIs. We also discovered
that some of the hidden parameters in a given library are set to
values which are different from the default values set in the API
of another library, and such values cannot be changed unless one
modifies the source code. For example, Scikit-Learn and Weka
provide a parameter which defines the tolerance for the stopping
criterion when building the SVR model, and both libraries setting
the default value to 0.001. However, this parameter is not provided
by Caret’s API, instead it can only be found by investigating the
code, with its value being set to 0.01 in the called function’s body.

5 DISCUSSION
In this section we discuss the main findings of our study and take-
aways for researchers and practitioners that use and develop ml
libraries.

The results of our empirical study revealed that there is a large
disagreement on the effort predictions achieved by a given machine
learner using these three libraries (i.e., 95% of the cases on average
across a total of 105 cases studied). Moreover, our analysis of 117
previous SEE work revealed that more than half do not state the
ml library used (51%), and among those that do it, the majority
only provides partial information (72%), thus, making it difficult to
reproduce and replicate previous work. These findings highlight
that the choice of a library is just as critical as the choice of a
study’s prediction techniques, benchmarks, validation procedures
and evaluation measures [46], and as such researchers should al-
ways indicate the library used in their studies. These factors can
all have an influence on a study’s outcome and can contribute to
the conclusion instability observed in the SEE literature. While,
previous studies have highlighted the importance of the last four
aspects [32, 43, 47, 60, 62, 72], our study is the first to investigate
the impact of the ml library.

Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Moreover, our analysis of the API documentation and code sug-
gests that software engineers building open-source ml libraries
should follow a more uniform approach by providing a reference
to the conceptual technique implemented by a given API; expos-
ing the right number and type of parameters needed to build a
given machine learner, otherwise explain any differences; testing
the implementation of a given conceptual technique which uses
other existing implementations of the same technique as an oracle
[5, 41, 79].

To summarise, our study offers the following main take-aways
to researchers and practitioners that use and develop ml libraries:

• The study raises awareness on the fact that different ml li-
braries incur different results for software effort estimation.
Our results show that no library, among the three studied, is
best for all datasets investigated and, thus, call for actions
to allow users to consider and understand the accuracy re-
quirements when selecting an ml library.

• The empirical analyses we carried out can help future stud-
ies in this domain, by offering a comprehensive and sound
methodology to compare ml library performance.

• The deficiencies we found in the current documentation of
ml libraries reveal the need for a better documentation of
these libraries, and we provide some initial suggestions on
how this could be tackled in future work.

6 THREATS TO VALIDITY
The validity of our study can be affected by internal, construct,
conclusion, and external threats.

Internal validity: Our experimental setting has been mainly dic-
tated by the need to eliminate any stochastic source yet we strove
to consider datasets, regression techniques and hyper-parameter
tuning widely used in previous work.

While we acknowledge, that other experimental settings could
be used to improve the overall predicting performance (e.g., using
Search-based approaches for hyper-parameter tuning [17, 18]), we
explicitly avoid design choices that could introduce any stochas-
ticity in our results, in order to soundly analyse the variances in
performance of the machine learners solely due to the use of the
different ml libraries investigated in our work, rather than other
possible factors.

To mitigate the threat of missing relevant information in our
literature review as well as API manual analysis, two authors ex-
amined all artefacts (i.e., papers, documentation and source code)
independently, in order to ensure reliability and reduce researcher
bias. The results were compared at the end of the process, and any
ambiguities/inconsistencies was resolved by a joint analysis and
discussion.

Construct and Conclusion Validity: We follow most recent best
practice to evaluate and compare prediction systems [69]. We use
the MAE as a measure to evaluate and compare the predictions. The
MAE is unbiased towards both over- and under-estimation and its
use has been recommended [39, 64, 69] as opposed to other popular
measures like MMRE and Pred(25) [16], which have been criticised
for being biased towards underestimations and for behaving very
differently when comparing prediction models [24, 33, 36, 57, 67, 74].
We also carefully calculated the performance measures and applied

statistical tests by verifying all the required assumptions. We ex-
perimented with real-world datasets widely used to empirically
evaluate SEE models, and to ensure a realistic scenario, we did
not use any independent variables that is not known at predic-
tion time and therefore cannot be used for prediction purposes as
recommended in previous work [64].

External validity: Threats related to the generalizability of our
findings may arise due to the open-source libraries, techniques and
datasets we investigated. We have mitigated these threats by using
those that are as representative as possible of the SEE literature.

7 FINAL REMARKS AND FUTURE WORK
We have investigated and compared the use of three popular open-
source ml libraries (Caret, Scikit-Learn and Weka) to build SEE
prediction models with well-know machine learners and scenarios
most commonly used in the literature.

Based on our empirical results and API analyses, we suggest
and shed light on the fact that ml library users should consider
the choice of the ml library as part of their empirical design, sim-
ilar to the choice of evaluation measures or validation processes.
We encourage users to justify their use of libraries which would
motivate them to seek a deeper understanding of the ml libraries
and algorithms being used. On the other hand, for this to be more
achievable, the documentation of such libraries should be improved
to include more information about the algorithms implemented and
their references. The developers of these libraries should provide a
level of clarity in the documentation that would be comprehensible
by all users regardless of their level of expertise.

The deficiencies we highlighted in the current API documen-
tation, provides initial evidence of the need for further analysis
of the existing APIs to derive a set of standard requirements for
ml API documentation and API construction itself, as done, for
example, in previous work for the documentation of Computer
Vision Software [19]. Also, future studies can investigate the use
of automated refactoring techniques to automatically recommend
to developers suitable variable renaming in order to increase the
consistency across different libraries. We also envisage that the
use of automated deep-parameter tuning [8, 13, 78] can aid to au-
tomatically improve the performance of prediction models built
using those ml libraries that do not expose as many parameters
in their APIs. Last but not least, future work can extend our work
to assess whether discordant results arise when using proprietary
ml libraries, as well as the impact of using different (open-source
and proprietary) libraries for other ml-based software engineering
prediction tasks.

ACKNOWLEDGMENTS
Rebecca Moussa and Federica Sarro are supported by the ERC
Advanced fellowship grant EPIC (741278) and the Department of
Computer Science of University College London.

REFERENCES
[1] 2017. The SEACRAFT Repository of Empirical Software Engineering Data.
[2] Asad Ali and Carmine Gravino. 2019. A systematic literature review of software
effort prediction using machine learning methods. Journal of Software: Evolution
and Process 31, 10 (2019), e2211.

[3] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.

Do Not Take It for Granted: Comparing Open-Source Libraries for Software Development Effort Estimation

Conference’17, July 2017, Washington, DC, USA

Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st
International Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP). IEEE, 291–300.

[4] A. Arcuri and L. Briand. 2014. A Hitchhiker’s guide to statistical tests for assessing
randomized algorithms in software engineering. STVR 24, 3 (2014), 219–250.
[5] Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2015.
The Oracle Problem in Software Testing: A Survey. IEEE Transactions on Software
Engineering 41, 5 (2015), 507–525. https://doi.org/10.1109/TSE.2014.2372785
[6] Andrew Begel and Thomas Zimmermann. 2014. Analyze this! 145 questions for
data scientists in software engineering. In Proceedings of the 36th International
Conference on Software Engineering. 12–23.

[7] Pamela Bhattacharya and Iulian Neamtiu. 2011. Bug-Fix Time Prediction Models:
Can We Do Better?. In Proceedings of the 8th Working Conference on Mining
Software Repositories (Waikiki, Honolulu, HI, USA) (MSR ’11). Association for
Computing Machinery, New York, NY, USA, 207–210. https://doi.org/10.1145/
1985441.1985472

[8] Mahmoud A Bokhari, Bobby R Bruce, Brad Alexander, and Markus Wagner. 2017.
Deep parameter optimisation on android smartphones for energy minimisation: a
tale of woe and a proof-of-concept. In Proceedings of the Genetic and Evolutionary
Computation Conference Companion. 1501–1508.

Engineering Research. Empirical Softw. Engg. 22, 5 (Oct. 2017), 2543–2584. https:
//doi.org/10.1007/s10664-016-9493-x

[31] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, and Sven Apel.
2020. The interplay of sampling and machine learning for software performance
prediction. IEEE Software 37, 4 (2020), 58–66.

[32] Jacky Keung, Ekrem Kocaguneli, and Tim Menzies. 2013. Finding Conclusion
Stability for Selecting the Best Effort Predictor in Software Effort Estimation.
Automated Software Engg. 20, 4 (Dec. 2013), 543–567. https://doi.org/10.1007/
s10515-012-0108-5

[33] B. Kitchenham, L. M. Pickard, S. G. MacDonell, and M. J. Shepperd. 2001. What
accuracy statistics really measure. IEEE Proc. Software 148, 3 (2001), 81–85.
[34] Ekrem Kocaguneli and Tim Menzies. 2013. Software effort models should be
assessed via leave-one-out validation. Journal of Systems and Software 86, 7
(2013), 1879–1890.

[35] Ekrem Kocaguneli, Tim Menzies, and Jacky W. Keung. 2012. On the Value of

Ensemble Effort Estimation. IEEE TSE 38, 6 (2012), 1403–1416.

[36] Marcel Korte and Dan Port. 2008. Confidence in Software Cost Estimation Results

Based on MMRE and PRED. In Proc. of PROMISE’08. 63–70.

[37] Max Kuhn. 2015. A Short Introduction to the caret Package. R Found Stat Comput

1 (2015).

[9] Sridevi Bonthu and K Hima Bindu. 2017. Review of Leading Data Analytics Tools.

[38] Max Kuhn et al. 2008. Building predictive models in R using the caret package.

International Journal of Engineering & Technology 7, 3.31 (2017), 10–15.

Journal of statistical software 28, 5 (2008), 1–26.

[10] Remco R. Bouckaert, Eibe Frank, Mark A. Hall, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2010. WEKA—Experiences with
a Java Open-Source Project. J. Mach. Learn. Res. 11 (Dec. 2010), 2533–2541.
[11] Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and
Regression Trees. Wadsworth Publishing Company, Belmont, California, U.S.A.
[12] Lionel C. Briand and Isabella Wieczorek. 2002. Software resource estimation.

Encyclopedia of Software Engineering (2002), 1160–1196.

[13] Bobby R Bruce, Jonathan M Aitken, and Justyna Petke. 2016. Deep parameter
optimisation for face detection using the Viola-Jones algorithm in OpenCV. In
International Symposium on Search Based Software Engineering. Springer, 238–243.
[14] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, et al. 2013. API design for machine learning software: experiences
from the scikit-learn project. arXiv preprint arXiv:1309.0238 (2013).

[15] Christopher JC Burges. 1998. A tutorial on support vector machines for pattern

recognition. Data mining and knowledge discovery 2, 2 (1998), 121–167.

[16] D. Conte, H.E. Dunsmore, and V.Y. Shen. 1986. Software engineering metrics and

models. Benjamin/Cummings Publishing Company, Inc.

[17] A. Corazza, S. Di Martino, F. Ferrucci, C. Gravino, F. Sarro, and E. Mendes. 2010.
How Effective is Tabu Search to Configure Support Vector Regression for Effort
Estimation?. In Proc. of PROMISE’10. 4:1–4:10.

[18] Anna Corazza, Sergio Di Martino, Filomena Ferrucci, Carmine Gravino, Federica
Sarro, and Emilia Mendes. 2013. Using tabu search to configure support vector
regression for effort estimation. EMSE 18, 3 (2013), 506–546.

[19] Alex Cummaudo, Rajesh Vasa, John Grundy, and Mohamed Abdelrazek. 2020.
Requirements of API Documentation: A Case Study into Computer Vision Ser-
vices. IEEE Transactions on Software Engineering (2020). https://doi.org/10.1109/
tse.2020.3047088

[20] Janez Demšar. 2006. Statistical comparisons of classifiers over multiple data sets.

Journal of Machine learning research 7, Jan (2006), 1–30.

[21] Tenzin Doleck, David John Lemay, Ram B Basnet, and Paul Bazelais. 2020. Predic-
tive analytics in education: a comparison of deep learning frameworks. Education
and Information Technologies 25, 3 (2020), 1951–1963.

[22] Shraddha Dwivedi, Paridhi Kasliwal, and Suryakant Soni. 2016. Comprehen-
sive study of data analytics tools (RapidMiner, Weka, R tool, Knime). In 2016
Symposium on Colossal Data Analysis and Networking (CDAN). IEEE, 1–8.
[23] F. Ferrucci, M. Harman, and F. Sarro. 2014. Search-Based Software Project Man-
agement. In Software Project Management in a Changing World, Günther Ruhe
and Claes Wohlin (Eds.). Springer Berlin Heidelberg, 373–399.

[24] Tron Foss, Erik Stensrud, Barbara Kitchenham, and Ingunn Myrtveit. 2003. A
Simulation Study of the Model Evaluation Criterion MMRE. IEEE TSE 29, 11
(2003), 985–995.

[25] Milton Friedman. 1937. The use of ranks to avoid the assumption of normality
implicit in the analysis of variance. Journal of the american statistical association
32, 200 (1937), 675–701.

[26] Ian H Witten. 2016. Data mining. (2016).
[27] Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2011.
A systematic literature review on fault prediction performance in software engi-
neering. IEEE Transactions on Software Engineering 38, 6 (2011), 1276–1304.
[28] Yalda Hashemi, Maleknaz Nayebi, and Giuliano Antoniol. 2020. Documentation
of Machine Learning Software. In 2020 IEEE 27th International Conference on
Software Analysis, Evolution and Reengineering (SANER). IEEE, 666–667.
[29] Md Johirul Islam, Hoan Anh Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. What
do developers ask about ml libraries? a large-scale study using stack overflow.
arXiv preprint arXiv:1906.11940 (2019).

[30] Robbert Jongeling, Proshanta Sarkar, Subhajit Datta, and Alexander Serebrenik.
2017. On Negative Results When Using Sentiment Analysis Tools for Software

[39] William B. Langdon, Javier Dolado, Federica Sarro, and Mark Harman. 2016. Exact
Mean Absolute Error of Baseline Predictor, MARP0. Information and Software
Technology 73 (2016), 16 – 18.

[40] Cynthia Liem and Annibale Panichella. 2020. Run, Forest, Run? On Random-
ization and Reproducibility in Predictive Software Engineering. arXiv preprint
arXiv:2012.08387 (2020).

[41] Cynthia C. S. Liem and Annibale Panichella. 2020. Oracle Issues in Machine
Learning and Where to Find Them. In Proceedings of the IEEE/ACM 42nd Interna-
tional Conference on Software Engineering Workshops (ICSEW’20). Association for
Computing Machinery, New York, NY, USA, 483–488. https://doi.org/10.1145/
3387940.3391490

[42] Emilia Mendes, Marcos Kalinowski, Daves Martins, Filomena Ferrucci, and Fed-
erica Sarro. 2014. Cross- vs. within-Company Cost Estimation Studies Re-
visited: An Extended Systematic Review. In Proceedings of the 18th Interna-
tional Conference on Evaluation and Assessment in Software Engineering (EASE
’14). Association for Computing Machinery, New York, NY, USA, Article 12.
https://doi.org/10.1145/2601248.2601284

[43] Tim Menzies and Martin Shepperd. 2012. Special issue on repeatable results in

software engineering prediction. EMSE 17, 1 (2012), 1–17.

[44] Jarernsri Mitrpanont, Wudhichart Sawangphol, Thanita Vithantirawat, Sinattaya
Paengkaew, Prameyuda Suwannasing, Atthapan Daramas, and Yi-Cheng Chen.
2017. A study on using Python vs Weka on dialysis data analysis. In 2017 2nd
International Conference on Information Technology (INCIT). IEEE, 1–6.

[45] Rebecca Moussa and Federica Sarro. 2022. On-line Appendix- Don’t Take It
for Granted: API Choice Matters for Software Engineering Prediction Models.
https://github.com/SOLAR-group/ML-Tools-Study

[46] Rebecca Moussa and Federica Sarro. 2022. On the Use of Evaluation Measures
for Defect Prediction Studies. In 2022 ACM SIGSOFT International Symposium
on Software Testing and Analysis (ISSTA). Association for Computing Machinery
(ACM).

[47] I. Myrtveit, E. Stensrud, and M. Shepperd. 2005. Reliability and validity in
comparative studies of software prediction models. IEEE Transactions on Software
Engineering 31, 5 (2005), 380–391. https://doi.org/10.1109/TSE.2005.58

[48] Satish CR Nandipati, Chew XinYing, and Khaw Khai Wah. 2020. Hepatitis C Virus
(HCV) Prediction by Machine Learning Techniques. Applications of Modelling
and Simulation 4 (2020), 89–100.

[49] PB Nemenyi. 1963. Distribution-free multiple comparisons (Doctoral Dissertation,
Princeton University, 1963). Dissertation Abstracts International 25, 2 (1963), 1233.
[50] Giang Nguyen, Stefan Dlugolinsky, Martin Bobák, Viet Tran, Álvaro López García,
Ignacio Heredia, Peter Malík, and Ladislav Hluch? 2019. Machine Learning and
Deep Learning Frameworks and Libraries for Large-Scale Data Mining: A Survey.
Artif. Intell. Rev. 52, 1 (June 2019), 77–124. https://doi.org/10.1007/s10462-018-
09679-z

[51] Nicole Novielli, Fabio Calefato, Filippo Lanubile, and Alexander Serebrenik. 2021.
Assessment of off-the-shelf SE-specific sentiment analysis tools: An extended
replication study. Empir. Softw. Eng. 26, 4 (2021), 77. https://doi.org/10.1007/
s10664-021-09960-w

[52] Nicole Novielli, Daniela Girardi, and Filippo Lanubile. 2018. A Benchmark Study
on Sentiment Analysis for Software Engineering Research. In Proceedings of
the 15th International Conference on Mining Software Repositories (Gothenburg,
Sweden) (MSR ’18). Association for Computing Machinery, New York, NY, USA,
364–375. https://doi.org/10.1145/3196398.3196403

[53] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the

Conference’17, July 2017, Washington, DC, USA

Rebecca Moussa and Federica Sarro

Journal of machine Learning research 12 (2011), 2825–2830.

[66] George AF Seber and Alan J Lee. 2012. Linear regression analysis. Vol. 329. John

[54] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and
opportunities in training deep learning software systems: an analysis of vari-
ance. In Proceedings of the 35th IEEE/ACM International Conference on Automated
Software Engineering. 771–783.

[55] Stephen R Piccolo, Terry J Lee, Erica Suh, and Kimball Hill. 2020. ShinyLearner:
A containerized benchmarking tool for machine-learning classification of tabular
data. GigaScience 9, 4 (2020), giaa026.

[56] Stephen R Piccolo, Avery Mecham, Nathan P Golightly, Jérémie L Johnson, and
Dustin B Miller. 2021. Benchmarking 50 classification algorithms on 50 gene-
expression datasets. bioRxiv (2021).

[57] Dan Port and Marcel Korte. 2008. Comparative Studies of the Model Evaluation
Criterions Mmre and Pred in Software Cost Estimation Research. In Proc. of
ESEM’08. 51–60.

[58] Foyzur Rahman, Daryl Posnett, Israel Herraiz, and Premkumar Devanbu. 2013.
Sample size vs. bias in defect prediction. In Proceedings of the 2013 9th joint
meeting on foundations of software engineering. 147–157.

[59] Brian D Ripley. 2007. Pattern recognition and neural networks. Cambridge univer-

sity press.

[60] Federica Sarro. 2018. Predictive analytics for software testing: keynote paper. In
Proceedings of the 11th International Workshop on Search-Based Software Testing,
ICSE, Juan Pablo Galeotti and Alessandra Gorla (Eds.). ACM, 1.

[61] F. Sarro, F. Ferrucci, and C. Gravino. 2012. Single and Multi Objective Genetic
Programming for Software Development Effort Estimation. In Proc. of SAC’12.
ACM, 1221–1226. https://doi.org/10.1145/2245276.2231968

[62] F. Sarro, R. Moussa, A. Petrozziello, and M. Harman. [n.d.]. Learning From
Mistakes: Machine Learning Enhanced Human Expert Effort Estimates. IEEE
Transactions on Software Engineering ([n. d.]). https://doi.org/10.1109/TSE.2020.
3040793

[63] Federica Sarro, Rebecca Moussa, Alessio Petrozziello, and Mark Harman. 2020.
Learning From Mistakes: Machine Learning Enhanced Human Expert Effort
Estimates. IEEE Transactions on Software Engineering (2020). https://doi.org/10.
1109/TSE.2020.3040793

[64] Federica Sarro and Alessio Petrozziello. 2018. Linear Programming As a Baseline
for Software Effort Estimation. ACM Transactions on Software Engineering and
Methodology (TOSEM) 27, 3, Article 12 (2018), 28 pages. https://doi.org/10.1145/
3234940

[65] F. Sarro, A. Petrozziello, and M. Harman. 2016. Multi-objective software effort
estimation. In Proc. of ICSE’16. 619–630. https://doi.org/10.1145/2884781.2884830

Wiley & Sons.

[67] Martin Shepperd, Michelle Cartwright, and Gada Kadoda. 2000. On Building

Prediction Systems for Software Engineers. EMSE 5, 3 (2000), 175–182.

[68] M. Shepperd and C. Schofield. 2000. Estimating software Project Effort using

Analogies. IEEE TSE 23, 11 (2000), 736–743.

[69] Martin J. Shepperd and Steven G. MacDonell. 2012. Evaluating prediction systems
in software project estimation. Information and Software Technology 54, 8 (2012),
820–827.

[70] B. Sigweni, M. Shepperd, and T. Turchi. 2016. Realistic Assessment of Software
Effort Estimation Models. In Proc. of EASE’16. ACM, 41:1–41:6. https://doi.org/
10.1145/2915970.2916005

[71] Boyce Sigweni, Martin Shepperd, and Tommaso Turchi. 2016. Realistic assessment
of software effort estimation models. In Proceedings of the 20th International
Conference on Evaluation and Assessment in Software Engineering. 1–6.

[72] Liyan Song, Leandro L. Minku, and Xin Yao. 2013. The Impact of Parameter
Tuning on Software Effort Estimation Using Learning Machines. In Proceedings
of the 9th International Conference on Predictive Models in Software Engineering
(Baltimore, Maryland, USA) (PROMISE’13). Association for Computing Machinery,
New York, NY, USA, Article 9, 10 pages. https://doi.org/10.1145/2499393.2499394
[73] Tushar Sharma Federica Sarro Ying Zou Stefanos Georgiou, Maria Kechagia.
2022. Green AI: Do Deep Learning Frameworks Have Different Costs?. In 44th
IEEE/ACM International Conference on Software Engineering (ICSE). IEEE/ACM.
[74] Erik Stensrud, Tron Foss, Barbara Kitchenham, and Ingunn Myrtveit. 2003. A
Further Empirical Investigation of the Relationship Between MRE and Project
Size. EMSE 8, 2 (2003), 139–161.

[75] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2018. The impact of automated parameter optimization on defect
prediction models. IEEE Transactions on Software Engineering 45, 7 (2018), 683–
711.

[76] Jianfeng Wen, Shixian Li, Zhiyong Lin, Yong Hu, and Changqin Huang. 2012.
Systematic literature review of machine learning based software development
effort estimation models. Information and Software Technology 54, 1 (2012), 41–59.
[77] RF Woolson. 2007. Wilcoxon signed-rank test. Wiley encyclopedia of clinical

trials (2007), 1–3.

[78] Fan Wu, Westley Weimer, Mark Harman, Yue Jia, and Jens Krinke. 2015. Deep
parameter optimisation. In Proceedings of the 2015 Annual Conference on Genetic
and Evolutionary Computation. 1375–1382.

[79] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
IEEE Transactions on Software Engineering

Survey, landscapes and horizons.
(2020).

