ARTIFICIAL INTELLIGENCE SOFTWARE STRUCTURED TO 
SIMULATE HUMAN WORKING MEMORY, MENTAL IMAGERY, AND 
MENTAL CONTINUITY 

Jared Edward Reser Ph.D. 

Program Peace LLC 

jared@jaredreser.com 

+ 818-425-2340 

ABSTRACT 

This  article  presents  an  artificial  intelligence  (AI)  architecture  intended  to  simulate  the  human 
working memory system as well as the manner in which it is updated iteratively. It features several 
interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. 
These are structured hierarchically and integrated into a global workspace. They are capable of 
temporarily maintaining high-level patterns akin to the psychological items maintained in working 
memory.  This  maintenance  is  made  possible  by  persistent  neural  activity  in  the  form  of  two 
modalities:  sustained  neural  firing  (resulting  in  a  focus  of  attention)  and  synaptic  potentiation 
(resulting  in  a  short-term  store).  This  persistent  activity  is  updated  iteratively  resulting  in 
incremental  changes  to  the  content  of  the  working  memory  system.  As  the  content  stored  in 
working memory gradually evolves, successive states overlap and are continuous with one another. 
The present article will explore how this architecture can lead to gradual shift in the distribution 
of coactive representations, ultimately leading to mental continuity between processing states, and 
thus to human-like cognition. 

Like  the  human  brain,  the  working  memory  store  will  be  linked  to  multiple  topographic  map 
generation systems of various sensory modalities. As working memory is iteratively updated, the 
maps created in response will amount to sequences of related mental imagery. This system couples 
these components by embedding them within a multilayered neural network of pattern recognizing 
nodes. Nodes low in the hierarchy are trained to recognize and represent sensory features and are 
capable  of  combining  individual  features  or  patterns  into  composite,  topographical  maps  or 
images.  Nodes  high  in  the  hierarchy  are  multimodal  and  have  a  capacity  for  sustained  activity 
allowing the maintenance of pertinent, high-level features through elapsing time. The higher-order 
nodes  select  new  features  from  each  mapping  to  add  to  the  store  of  temporarily  maintained 
features. This updated set of features are fed back into lower-order sensory nodes where they are 
continually used to guide the construction of successive topographic maps. Thus, neural networks 
emulating the prefrontal cortex and its interactions with early sensory and motor cortex capture 
the  imagery  guidance  functions  of  the  human  brain.  This  sensory  and  motor  imagery  creation, 
coupled with an iteratively updated working memory store may provide  an AI system with the 
cognitive assets needed to produce generalized intelligence. 

KEYWORDS:  artificial  intelligence,  iterative  updating,  working  memory,  consciousness, 
superintelligence 

 
ARTIFICIAL INTELLIGENCE SOFTWARE STRUCTURED TO 
SIMULATE HUMAN WORKING MEMORY AND MENTAL 
CONTINUITY 

“In order for a mind to think, it has to juggle fragments of its mental states.” 

Marvin Minsky, 1985. 

Introduction 

This  article  takes  a  model  of  working  memory  published  in  Physiology  and  Behavior  in  2016 
(Reser,  2016)  and  explains  how  it  can  be  implemented  in  a  machine  to  make  AI  information 
processing  more  like  the  cognitive  processing  that  goes  on  in  the  human  brain.  The  model 
emphasizes that persistent neural activity in the brain permits multiple representations to remain 
coactive  over  time.  It  also  causes  successive  mental  states  to  overlap  in  regard  to  the 
representational content they hold.    

The  novel  processing  architecture  introduced  here  involves  the  emulation  of  the  mammalian 
cerebral cortex utilizing a  network of pattern recognizing nodes for: selecting priority stimulus 
features, temporarily maintaining these features in a limited capacity working memory store and 
using them as parameters to search long-term memory for pertinent updates to working memory. 
The  article  will  argue  that  by  continuously  updating  the  working  memory  store  in  an  iterative 
fashion  (Reser,  2016),  such  a  system  could  exhibit  cognitive  abilities  not  seen  using  current 
methods. 

The  pattern  of  activity  in  the  brain  is  constantly  changing,  but  because  the  activity  in  certain 
neurons  persists  during  these  changes,  particular  features  of  the  overall  pattern  can  be 
uninterrupted or conserved over time. In other words, the distribution of active neurons in the brain 
transfigures gradually and incrementally from one configuration to another, instead of changing 
all at once.  If it were not for the phenomena of persistent neural activity, successive mental states 
would be discrete and isolated rather than continuous with each other. Thus, the human brain is an 
information processing system that has the ability to maintain a large pool of representations that 
is perpetually in flux as new representations are constantly being added, some are being removed 
and  still  others  are  being  maintained.  The  present  device  will  be  constructed  to  mimic  this 
biological phenomenon, common to all vertebrates, but do it in a particularly mammalian manner. 
Because  a  large  number  of  neural  nodes  in  the  mammalian  cerebral  cortex  are  always  being 
sustained,  each  brain  state  is  embedded  recursively  in  the  previous  state,  amounting  to  a 
progressive process that can move toward a complex result. 

Contemporary AI has its weights frozen after training and cannot continue learning.  Because it 
cannot learn it generally sits idle between processing tasks. Also, it does not reconcile information 
from multiple specialized processors like the human brain does. Iterative updating will change all 
of this. Properly integrated with existing AI technology, this method may have the potential to 

 
 
enhance the capabilities of problem-solving agents with respect to pattern recognition, analytics, 
prediction, adaptive control, decision making, and response to query.  

Computer programmers design AI systems to hold information online only for as long as they 
know they will need it. They hold data in a temporary store merely to compute what they are 
programmed to compute or execute whatever process they have pending. The mammal brain, on 
the other hand, has a strategy dedicated to holding potentially relevant information online 
because there is a high probability that it will be useful in the near future. It wagers that this 
information will be useful in processing without yet knowing how. It is not decided before hand 
how long items should remain active, rather, it is re-decided every second and during each state.  

Soft computing approaches using the state-space approach contain incidental aspects of iteration, 
and other architectures such as neural networks use spreading activation. However, no machine 
yolks these together to create iterative updating and multi associative search. Doing so provides a 
clear way to structure a system that does not suspend its activity every time it finishes a task. It 
will be important for the system to exhibit continuous endogenous processing and a working 
memory that updates continuously to allow for uninterrupted learning.  

Some important issues in the construction of intelligent machines include: 1) How can the 
process of internally generated thought be accomplished within a computer? 2) How can 
fragments of long-term memory recombine to represent novel concepts and episodes? 3) What 
events between these fragments must take place to allow transitions between episodes? 4) Are 
analogs of association and sensory cortices necessary? 5) How can the capacity of working 
memory be increased in a machine? 6) Is it possible to reduce explicit, conscious processes down 
to their constituent, implicit, unconscious ones? 7) Which brain mechanisms give rise to the 
mental continuity that humans experience? 8) What fundamental processes allow thought to 
move through space and time, or in another word, to “propagate?” The answers to these 
questions must be grounded in physics, biology, and information processing because they must 
explain how the physical substrate of intelligence operates in a mechanistic sense (Chalmers, 
2010). Without being able to tie together all the neurological, psychological, and computational 
loose ends necessary to answer these questions comprehensively, this paper will attempt to 
address them in an exploratory way using a novel approach. Rather than being based on 
traditional topics, this article will view these questions from the perspective of the shared 
representational content between successive mental states. 

There are currently many illustrative and biologically plausible theories from cognition and 
neuroscience that address the questions listed above. Some do an exemplary job of tying together 
a large number of relevant phenomena into a cohesive picture. Models such as Atkinson and 
Shiffrin’s (1969) multistore model, Baar’s (1997, 2002) global workspace theory, Baddeley’s 
(2000, 2007) model of working memory, Damasio’s convergence-divergence paradigm 
(Damasio, 1989; Meyer & Damasio, 2009), Edelman’s (1987, 2006) concepts of reentrance and 
neural Darwinism, Edelman and Tononi’s (2001) conceptualization of a “functional cluster” or 
“dynamic core,” Fuster’s (2009) conception of cognits, Tononi’s (2004) conception of 
integration of information, and Carpenter and Grossberg’s (2003) adaptive resonance theory 
have done much to lend perspective into the mechanics of perception, attention, working 
memory, and consciousness. In fact, concepts from these works (and several others that will be 
cited) form a battery of implicit assumptions about cognitive mechanisms that provides 
theoretical scaffolding for what is written here.  

Despite much progress, most scientists report that current theory is unsatisfying because it cannot 
yet bridge the gap between the brain and the mind (Chalmers, 1995; Chalmers, 2010; Shear, 
1997). Further, even though many contemporary models largely agree with empirical data, little 
has been done to reconcile their disparate, piecemeal approaches (Pereira & Ricke, 2009; Vimal, 
2009). The effort to determine the important cognitive phenomena that should be replicated 
within a computer may be facilitated by the exploration of the concept of mental continuity. This 
work intends to use the concept of mental continuity to integrate current theoretical approaches 
while attempting to remain consistent with prevailing knowledge. 

Modeling Mental Continuity 

Continuity  is  defined  as  being  uninterrupted  in  time.  As  proposed  here,  “mental  continuity” 
involves  a  process  where  a  gradually  changing  collection  of  mental  representations  held  in 
attention/working  memory  exhibits  a  measure  of  uninterrupted  activity  across  time  and  over 
sequential  processing  states.  If  it  were  not  for  the  phenomenon  of  persistent  neural  activity, 
instantaneous information processing states would be time-locked and isolated (as in most serial 
and parallel computing architectures), rather than continuous with the states before and after them.  

This  article  explores  how  sustained  neural  firing  in  association  areas  allows  goal-relevant 
representations to be maintained over multiple perception-action cycles, in order to direct complex 
sequences  of  interrelated  mental  states.  The  individual  states  in  a  sequence  of  such  states  are 
interrelated  because  they  share  representational  content.  The  associations  linking  the  shared 
contents  are  saved  to  memory,  impacting  future  searches,  and  ultimately  resulting  in  semantic 
knowledge, planning, and systemizing. 

Because the activity of cells in the PFC and other association areas can be sustained concurrently 
and does not fade away before the next instantiation of activity elsewhere, there is a temporally 
dynamic and overlapping pattern of neural activity that makes possible the “juggling” of 
information in working memory. The quantity of mental continuity is directly proportional to the 
number of sustained representations and the length of time of their activity (Reser, 2011, 2012, 
2013).  

There are countless phenomena in the natural world that undergo iterative updating. Take the 
human population of Earth for instance. In the next year many people will die, some new people 
will be born, yet most will be here a year from now. In the same sense, some of the neurons in a 
population exhibiting sustained firing will stop firing, some will begin, yet most will still be 
firing a second later. The people and neurons that persist will be able to influence subsequent 
states. 

The sustained firing of higher-order nodes allows representations to be maintained over multiple 
perception-action cycles permitting complex sequences of interrelated mental states. The overall 
distribution of active nodes in the neural network will shift gradually during contextual updating 
because  the  activity  of  certain  neural  nodes  will  persist.  This  will  ensure  that  the  activity  of 
prioritized,  goal  or  motor-relevant  representations  will  be  uninterrupted  over  time.  The 
representations that demonstrate this continuity are a subset of the active representations from the 
previous state and may act as referents to which newly introduced representations of succeeding 
states relate. The limited-capacity store of coactive representations in association areas is updated 
as: 1) the nodes that continue to receive sufficient spreading activation energy are maintained; 2) 

 
the nodes that receive reduced energy are released from activation; 3) new nodes that are tuned so 
as to receive sufficient energy from the current constellation of coactivates are converged  upon 
and incorporated into the remaining pool of active nodes from the previous cycle. 

Simulating the Minicolumns of the Mammalian Neurocortex 

Like other cognitive models (Cowan, 2005; Moscovich, 1992), this model views cognition as a 
system responsible for using active representations from LTM to guide goal-directed processing 
(Postle,  2007).  The  present  model  is  consistent  with  connectionism  and  parallel  distributed 
processing  in  that  it  conceptualizes  mental  representations  as  being  built  from  interconnected 
networks  of  decentralized,  semi-hierarchically  organized,  pattern-recognizing  nodes  that  have 
multiple inputs and outputs (Gurney, 2009; Johnson-Laird, 1998). Like other biologically plausible 
neural network models, it envisions these nodes as microscopic, modular neural units and assumes 
that each individual unit represents an elementary feature or stable “microrepresentation” of LTM 
(Meyer & Damasio, 2009). In the mammalian brain these microrepresentations may be instantiated 
by cortical minicolumns. 

The  structure  of  the  cerebral  cortex  is  highly  repetitive  and  is  marked  by  the  employment  of 
millions of nearly identical structures called cortical minicolumns (Lansner, 2009). Minicolumns 
are composed of densely connected neural cell bodies and span the six layers of grey matter in the 
neocortex. There are supposedly around 20,000,000 minicolumns in the human cortex, each of 
which  contains  between  80  and  120  neurons,  and  is  about  30  to  40  micrometers  in  diameter 
(Lansner, 2009). These minicolumns share the same basic structure and are thought to employ the 
same  cortical  algorithm  (Fuji  et  al.,  1998;  Kurzweil,  2012).  Each  column  performs  neural 
computations to determine if its inputs from other columns are sufficient to activate its outputs to 
other columns (Rochester et al., 1956).  

Columns, and other similar groups of neurons with the same tuning properties, are often referred 
to as cell assemblies, and this term will be used here. Most neurons in an assembly share similar 
receptive fields, and thus even though they may play different roles within the assembly, they each 
contribute to the assembly’s ability for encoding a unitary feature (Moscovich et al., 2007). Such 
an assembly of neurons is thought to embody a stable microrepresentation or fragment of long-
term  memory.  All  of  the  millions  of  pattern  recognizers  in  the  neocortex  are  simultaneously 
considering their inputs, and continually determining whether or not to fire. In general, when a 
neuron or assembly fires, the pattern that it represents has been recognized. Assemblies, like the 
neurons that compose them, function as “coincidence detectors” or “pattern recognition nodes” 
(Fuji et al., 1998). The spread of activity in the cortex involves many-to-one (convergence) and 
one-to-many (divergence) interactions within a massively interconnected network of assemblies. 
The next section will discuss how these columns or assemblies could be simulated by nodes in a 
neural network. 

Information Processing in Artificial Neural Networks 

The field of AI research is involved in creating a computing system that is capable of emulating 
certain functions that are traditionally associated with intelligent human behavior. Most early AI 
systems were only capable of responding in the manner in which human programmers provided 
for  when  the  program  was  written.    It  became  recognized  that  it  would  be  valuable  to  have  a 
computer  which  does  not  respond  in  a  preprogrammed  manner  (Moravec,  1988).  AI  systems 

capable of adaptive learning have since become important. Neural networks have attempted to get 
around the programming problem by using layers of artificial neurons or nodes. Neural networks 
and  genetic  algorithms  are  widely  implemented  in  research  and  industry  for  their  capabilities 
involving  adaptive  learning  and  advanced  pattern  recognition.  However,  they  are  used  for 
processing tasks that are narrowly constrained and highly specialized, and there has not yet been 
any strong form of intelligence derived from them.  

An  artificial  neural  network  is  an  interconnected  group  of  artificial  neurons  that  uses  a 
mathematical  or  computational  model  for  information  processing  based  on  a  connectionistic 
approach to computation (Russel et al., 2003). Like most neural networks, the present network 
should  be  an  adaptive  system  capable  of  altering  its  own  structure  based  on  the  nonlinear 
processing of information that flows through the network. The software would require a massively 
parallel,  distributed  architecture,  that  could  be  run  on  a  conventional  computer.  Most  neural 
networks ordinarily achieve intelligent behavior through parallel computations, without employing 
formal rules or logical structures, and can be used for pattern matching, classification, and other 
non-numeric,  nonmonotonic  problems  (Nilsson,  1998).  The  applications  for  the  present  device 
could be widened if it were designed to accept and process formal rules. 

The traditional neural network is a multilayer system composed of computational elements (nodes) 
and  weighted  links  (arcs).  These  networks  are  based  on  the  human  brain  where  the  nodes  are 
analogous to neurons, or neural assemblies, and the arcs are analogous to axons and dendrites. 
Each  node  receives  signals  from  nodes  in  the  layer  below  it,  processes  these  signals  and  then 
decides whether to “fire” at the nodes in the layer above. Like the artificial neurons first described 
by  McCullough  and  Pitts  (1943),  the  nodes  in  the  present  system  could  feature  a  number  of 
excitatory inputs whose weights range between 0 and 1 and inhibitory inputs whose weights range 
between -1 and 0. Each of the incoming inputs and their corresponding weights are summed to 
equal  an  activation  level.  If  this  level  exceeds  the  neurons’s  firing  threshold,  it  will  cause  the 
neuron to fire. The neuron can be made to learn from its experience when either the threshold or 
weights are changed. Neural networks are typically defined by three types of parameters: 1) The 
interconnection pattern between different layers of neurons; 2) The learning process for updating 
the weights of the interconnections; 3) The activation function that converts a neuron’s weighted 
input to its output activation. 

There are currently no neural networks, or AI systems whatsoever, that are structured to model the 
primate neocortex in order to guide the progressive generation of successive topographic maps. 
The present neural network software architecture is structured around identifying potentially goal-
relevant information and holding it online to inform reciprocal cycles of imagery generation and 
feature extraction for the purpose of systemizing the environment.  

Architecture Hierarchy 

The software models a large set of nodes that work together to continually determine, in real time, 
which  from  the  population  of  inactive  nodes  should  be  newly  activated,  and  which  from  the 
population of active nodes should be either deactivated or have its activity maintained.  

The  device  necessitates  a  highly  interconnected  neural  network  that  features  a  hierarchically 
organized collection of pattern recognizers capable of both transient and sustained activity. These 
pattern recognition nodes mimic assemblies (minicolumns) of cells in the mammalian neocortex 

and are arranged with a similar connection geometry. Like neural assemblies the nodes exhibit a 
continuous gradient from low-order nodes that code for sensory features, to high-order nodes that 
code for temporally or spatially extended relationships between such features. The lower order 
nodes are organized into modules by sensory modality. Nodes are grouped according to the feature 
they are being trained to recognize. These maps can be generated by external input, by internal 
input from higher-order nodes, or a mix of the two. The architecture will feature backpropagation, 
self-organizing  maps,  bidirectionality,  Hebbian  learning  as  well  as  a  combination  between 
principal-components learning and competitive learning.  

Nodes  lower  in  the  hierarchy  are  trained  to  recognize  and  represent  sensory  features  and  are 
capable of combining individual features or patterns into metric, topographical maps or images. 
Lower-order  nodes  are  unimodal,  and  organized  by  sensory  modality  (visual,  auditory, 
somatosensory etc.) into individual modules. Nodes high in the hierarchy are multimodal, module 
independent,  and  have  a  capacity  for  sustained  activity  allowing  the  conservation  of  pertinent, 
high-level features through elapsing time. The higher nodes are integrated into the architecture in 
a way that makes them capable of identifying goal-relevant features from both internal imagery 
and environmental input, and temporarily maintaining these as a form prioritized information.  

The system is structured to allow repetitive, reciprocal interactions between the lower, bottom-up, 
and higher, top-down nodes. The features that the higher nodes encode are utilized as inputs that 
are fed back into lower-order sensory nodes where they are continually used for the construction 
of successive topographic maps. The higher nodes select new features from each mapping to add 
to the store of temporarily maintained features. Thus, the most salient or goal-relevant features 
from  the  last  several  mappings  are  maintained.  The  group  of  active,  higher-order  nodes  is 
constantly updated, where some nodes are newly added, some are removed, yet a relatively large 
number are retained. This updated list is then used to construct the next sensory image which will 
be necessarily similar, but not identical to, the previous image. The differential, sustained activity 
of a subset of high-order nodes allows thematic continuity to persist over sequential processing 
states.  

All the nodes within the device function as a continuous whole and are highly interconnected, but 
can  be  decomposed  into  separate,  modular,  neural  networks.  Nodes  belonging  to  an  individual 
module are highly interconnected with  each other. These modules  consist  of a bottom  layer of 
input cells, succeeded by layers of local-feature extracting cells, and a top layer of output cells. 
The individual neural networks interface through the connections between top layer output cells 
and bottom layer input cells. Each network is organized so that multiple lower-order nodes can 
converge on higher order nodes, and single higher-order nodes can diverge back on multiple lower-
order nodes. Multiple interfacing neural networks could be arranged biomimetically as in figure 8 
below. 

Fig. 1. A plausible biomimetic arrangement of interfacing neural networks.  

The  bottom-up  to  top-down  reciprocations  are  organized  into  very  precise  oscillations  that 
propagate in regularly timed intervals across the network so that they do not interfere with each 
other. The oscillations reciprocate back and forth at just the right speed so that each area has the 
time to process its inputs and send an output before the next complement of inputs arrive. It is 
important to carefully structure timing mechanisms in the present device so that messaging is not 
muddled  or  noisy.  It  is  also  important  to  structure  the  architecture  so  that  continuity  in  the 
representations held active by the buffer can be disrupted when attention shifts. Repeated loops of 
conserved, higher-order features can be ended when attention is captured by an object or concept 
that competes for attention. The ability to free up the resources of higher-order nodes to attend to 
a new stimulus will be programmed by training. Before proper training is accomplished the system 
may not be able to reallocate its resources properly when its attention shifts.   

To  create  a  strong  form  of  AI  it  is  necessary  to  understand  what  is  taking  place  that  allows 
intelligence, thought,  cognition,  consciousness  or working memory to  move through space and 
time,  or  in  another  word,  to  “propagate.”  Such  an  understanding  must  be  grounded  in  physics 
because it must explain how the physical substrate of intelligence operates through space and time 
(Chalmers, 2010). The human brain is just such an intelligent physical system that AI researchers 
have attempted to understand and replicate using a biomimetic approach (Gurney, 2009). Features 
of the biological brain have been key in the  design of neural networks, but the brain may hold 
additional information processing principles that have not been harnessed by A.I. efforts (Reser 
2011, 2012, 2013).  

Computational operations, that take place as a computer implements lines of code (rule-based, if-
then operations) to transform input into output, have discrete, predetermined starting and stopping 

 
points. For this reason, computers do not exhibit continuity in their information processing. There 
are  no  forms  of  artificial  intelligence  that  use  mental  continuity  as  described  here.  There  are 
existing  computing  architectures  with  limited  forms  of  continuity  where  the  current  state  is  a 
function of the previous state, and where data is entered into a limited capacity buffer to inform 
other processes. However, the memory buffer is not global, not multimodal, not positioned at the 
top of a hierarchical system and does not inform and interact with topographic imagery. 

Persistent Neural Activity From Sustained Firing and Synaptic Potentiation 

The mammalian PFC and other association cortices have neurons that are specialized for 
“sustained firing,” allowing them to generate action potentials at elevated rates for several 
seconds at a time (generally 1-30 seconds) (Fuster, 2009). In contrast, neurons in other brain 
areas, including cortical sensory areas, remain active only for milliseconds unless sustained input 
from association areas makes their continued activity possible (Fuster, 2009). In the mammalian 
brain, prolonged activity of neurons in association areas, especially prefrontal and parietal areas, 
allows for the maintenance of specific features, patterns, and goals (Baddeley, 2007). Working 
memory, executive processing and cognitive control are widely thought to stem from the active 
maintenance of patterns of activity in the PFC that represent goal-relevant features (Goldman-
Rakic, 1995). The temporary persistence of these patterns ensures that they continue to transmit 
their effects on network weights as long as they remain active, biasing other processing, and 
affecting the interpretation of subsequent stimuli that occur during their episode of continual 
firing.  

Although its limits are presently being debated, the human neocortex is clearly capable of holding 
numerous  neural  representations  active  over  numerous  points  in  time.  The  quantity  of  mental 
continuity is directly proportional to the number of such sustained representations and the length 
of time of their activity (Reser, 2011, 2012, 2013). 

Fig.2. Graphical depiction of iterative updating. Each bracket represents the active time span of a neural 
representation. The x axis represents time and the y axis demarcates the cortical area where the representation is 
active. Red brackets denote representations that have exhibited uninterrupted activity from the point when they 
became active, whereas blue brackets denote representations that have not been sustained. In time sequence 1 
representations B, C, D and E have remained active until t1. In time sequence 2 B has deactivated, C, D and E have 
remained active, and F is newly active. The figure depicts a system with iterative updating because more than one 
representation (C, D, and E) has been maintained over more than one point in time (t1 and t2).  

In Figure 1 above, representations B, C, D, and E are active during time 1, and C, D, E and F are 
active during time 2. Thus, representations C, D, and E demonstrate reiteration because they 
exhibit continuous and uninterrupted activity from time 1 through time 2. The brain state at time 
1 and the brain state at time 2 share C, D, and E in common and, because of this, can probably be 
expected to share other commonalities including: similar information processing operations, 
similar memory search parameters, similar mental imagery, similar cognitive and declarative 
aspects, and similar experiential and phenomenal characteristics.  

 
Fig. 3. A simplified graphical representation of iterative updating depicting it as a gradually shifting, stream-like 
distribution. Figure 2 extends Figure 1 over multiple time intervals revealing a repeating pattern: remnants from the 
preceding state are consistently carried over to the next state.  

Figure 3 shows how this iterative process can be looped repetitively over time. Figure 4 shows 
how the rate of iteration can change  

 
 
Fig. 4. This figure expands on previous figures by comparing four possible incremental changes in SSC. In the first 
transition 75% of representational continuity is maintained between time periods 1 and 2. The other transitions depict 
50%, 25%, and 0% maintenance of continuity respectively. According to the definition of mental continuity, neither 
the graphic marked “25% continuity” nor the one marked “0% continuity” depict mental continuity, because they do 
not feature the maintenance of more than one representation. 

Figure  5  below  demonstrates  how  this  pattern  of  iterative  updating  could  be  implemented  in 
working memory. 

Fig. 5. A diagram depicting “multiassociativity” and illustrating the ways in which high-level representations, or 
items, are displaced, maintained, and newly activated in the brain. 1) Shows that representation A has already been 
deactivated and that B, C, D and E are now coactivated, mirroring the pattern of activity shown in Figure 1. When 
coactivated, these representations pool and spread their activation energy, resulting in the convergence of activity 
onto a new representation, F. Once F becomes active, it immediately becomes a coactivate, restarting the cycle. 2) 
Shows that B has been deactivated while C, D, E, and F are coactivated and G is newly activated. 3) Shows that D 
but not C has been deactivated. In other words, what is deactivated is not necessarily what entered first, but what has 
proven to receive the least converging activity. C, E, F, and G coactivate and converge on H.  

In Figure 4, between time periods 1 and 2, C, D and E exhibit coactivity, whereas, between time 
periods 1 and 3, only C and E exhibit coactivity. C and E are active over all three time periods 
meaning that these representations are being used as search function parameters for multiple 
cycles, and are the subject of attention. Alternatively, we can imagine a scenario where B, C, D, 
and E from step one of Figure 3 were immediately replaced by F, G, H, and I. Such a processing 
system may still be using previous states to determine subsequent states; however, because no 
activity is sustained through time, there would be no continuity in such a system (this is 
generally how most computing systems process information). 

In Figure 4, ensembles C and E have fired together over three individual time intervals, and thus 
will show a propensity to wire together, increasing their propensity for firing together in the 
future. This link between them will allow one to recruit the other. However, it is probably much 
more likely that they will recruit each other if the other contextual ensembles are also present. 
The coincidental or rare associations between the ensembles of an experience are probably 
mostly lost from non-hippocampal dependent cortical memory. However, the reoccurring 
associations are heavily encoded and persist as semantic knowledge.  

 
 
In other words, the cortex constantly spreads activation energy from novel combinations of 
active ensembles that have never been coactive before and attempts to converge upon the 
statistically most relevant association without certain or exact precedence resulting in a solution 
that is not guaranteed to be optimal.  

Neural nodes in sustained activity can span a wider delay time or input lag between associated 
occurrences (Zanto, 2011) allowing elements of prior events to become coactive with subsequent 
ones (Fuster, 2009). Without sustained firing, the ability to make associations between 
temporally distant (noncontiguous) environmental stimuli is disrupted. Sustained activity allows 
individual nodes that would otherwise never fire together to both fire and wire together. Thus, it 
sustained firing is responsible for the ability to make internally-derived associations between 
representations that would never co-occur simultaneously in the environment. 

This indicates that one way to quantify mental continuity is to determine the proportion of 
previously active neural nodes that have remained active during a resource demanding cognitive 
task. Uninterrupted activity augments associative searches by allowing specific features to serve 
as search function parameters for multiple cycles. Intelligence in this system can be enhanced by 
increasing: 1) the number of available nodes to select from, 2) the number of nodes that can be 
coactivated simultaneously, and 3) the length of time that individual nodes can remain active.  

The mesocortical dopamine (DA) system plays an important role in sustained activity, 
suggesting that it may be heavily involved in mental continuity. Dopamine sent from the ventral 
tegmental area (VTA) modulates the activity and timing of neural firing in the PFC, association 
cortices, and elsewhere. Dopamine neurotransmission in the PFC is thought to underlie the 
ability to internally represent, maintain, and update contextual information (Braver & Cohen, 
1999). This is necessary because information related to behavioral goals must be actively 
sustained such that these representations can bias behavior in favor of goal-directed activities 
over temporally extended periods (Miller & Cohen, 2001). It has become clear that the activity 
of the DA/PFC system fluctuates with environmental demand (Fuster & Alexander, 1971). Many 
studies have suggested that the system is engaged when reward or punishment contingencies 
change. Both appetitive and aversive events have been shown to increase dopamine release in the 
VTA, causing sustained firing of PFC neurons (Seamans & Robbins, 2010). Seamans and 
Robbins (2010) elaborated a functional explanation to support this case. They have stated that 
the DA system is phasically activated in response to novel rewards and punishments because it is 
adaptive for the animal to anchor upon and further process novel or unpredicted events.  

Sustained  firing  and  recurrent  processing  make  it  possible  for  recent  states  to  spill  over  into 
subsequent  states,  creating  the  context  for  them  in  a  recursive  fashion.  In  a  sense,  each  new 
topographic  map  is  embedded  in  the  previous  one.  This  creates  a  cyclical,  nested  flow  of 
information processing marked by STC, which is depicted in Figure 6.  

Learned  mental  tasks  probably  have  distinct  predefined  algorithmic  sequences  of  topographic 
mappings that must be completed in sequence in order to achieve the solution. Each brain state 
would correspond to a different step in the algorithm, and its activity would recruit the next step. 
All  logical  and  methodical  cognition  may  require  that  a  number  of  relevant  features  from  the 
present scenario remain in STC so that they spread their activity within the network in order to 
influence  the  selection  of  the  ensembles  necessary  for  task  satisfaction.  Moreover,  motor  and 
premotor modules give specifications to and receive specifications from this common workspace 

while they are building their musculotopic imagery for movement. The same goes for language 
areas. 

Fig. 6. The dynamics of the present model are captured by an analogy involving an octopus grabbing and releasing 
footholds as it pulls itself from place to place. This is an activity known as “sea floor walking.” The analogy illustrates 
that the thought process involves the simultaneous coactivation of several representations at a time (multiple footholds 
held by an octopus) as well as the deactivation of previously active representations (the releasing of footholds), and 
the activation of previously inactive representations (the placement of an arm on a new foothold). This analogy may 
be valuable because it depicts a system, that even a child can understand, where specific representations are conserved 
through time as others are actively repositioned.  

Search Through Spreading Activation Targets the Next Update 

The  mammalian  neocortex  is  capable  of  holding  a  number  of  such  mnemonic  representations 
coactive, and using them to make predictions by allowing them to spread their activation energy 
together, throughout the thalamocortical network. This activation energy converges on the inactive 
representations from  LTM that are the most closely connected with the current  group of active 
representations,  making  them  active,  and  pulling  them  into  short-term  memory.  Thus  new 
representations join the representations that recruited them, becoming coactive with them. 

 
 
The way that assemblies and ensembles are selected for activity in this model is consistent with 
spreading activation theory. In spreading activation theory, associative networks can be searched 
by labeling a set of source nodes, which spread their activation energy in a nonlinear manner to 
closely  associated  nodes  (Collins  &  Loftus,  1975).  Cortical  assemblies  work  cooperatively  by 
spreading the activation energy (both excitatory and inhibitory) necessary to recruit or converge 
upon the next set of ensembles that will be coactivated with the remaining ensembles from the 
previous cycle. 

Together, they impose sustained information processing demands on the lower-order sensory and 
motor  areas  within  the  reach  of  their  long-range  connections.  The  longer  the  activity  in  these 
higher-order neurons is sustained, the longer they remain engaged in hierarchy-spanning, recurrent 
processing throughout the cortex and subcortex. 

Gradual additions to and subtractions from a pool of simultaneously coactivated ensembles occur as: 

1. 

Assemblies  that  continue  to  receive  sufficient  activation  energy  from  the  network  are 
maintained. 

2. 

Assemblies that receive sufficiently reduced activation energy are released from activation. 

3. 

New assemblies, which are tuned to receive sufficient activation energy from the current 
constellation of coactivates, are converged upon, and incorporated into the remaining pool 
of active assemblies from the previous cycle. 

Table 1. The Characteristics of Multiassociative Search 

Outlining the process of multiassociativity in this way is meant to show that the computational 
algorithm used by the brain may be primarily directed at determining which inactive ensembles 
are the most closely statistically related to the currently active assemblies. From this perspective, 
the contents of the next state are chosen based on how the currently active assemblies interact with 
the existing, associative, neuro/nomological network. 

 
 
 
 
 
 
 
Fig. 7. A Schematic for Multiassociative Search 

Spreading activity from each of the assemblies (small spheres) of the four items (large circles) in the FoA (B, C, D, 
and E) propagates throughout the cortex (represented by the field of assemblies above the items). This activates new 
assemblies that will constitute the newest item (F), which will be added to the FoA in the next state. The assemblies 
that constitute items B, C, D, and E are each individually associated with a very large number of potential items, but 
as a unique group, they are most closely associated with item F.  

Reciprocating Crosstalk between Association and Sensory Cortex 

To  a  certain  extent,  perceptual  sensory  processing  in  the  brain  is  thought  to  be  accomplished 
hierarchically (Cohen, 2000). The cortical hierarchy observed from sensory to association cortex 
arises because simple patterns are arranged to converge upon second-order patterns, which in turn 
converge  on  third-order  patterns  and  so  on.  This  leads  to  a  hierarchy  of  increasingly  complex 
representations.  

It  is  thought  that  object  recognition,  decision  making,  associative  recall,  planning  and  other 
important  cognitive  processes,  involve  two-way  traffic  of  signal  activity  among  various  neural 
maps that stretch transversely through the cortex from early sensory areas to late association areas 
(Klimesch,  Freunberger,  &  Sauseng,  2010).  Bottom-up  sensory  areas  deliver  fleeting  sensory 
information and top-down association areas deliver lasting perceptual expectations in the form of 
templates  or  prototypes.  These  exchanges  involve  feedforward  and  feedback  (recurrent) 
connections in the corticocortical and thalamocortical systems that bind topographic information 
from  lower-order  sensory  maps  about  the  perceived  object  with  information  from  higher-order 
maps forming somewhat stable constellations of activity that can remain stable for tens or hundreds 
of milliseconds (Crick & Koch, 2003).  

Iterative  updating  impacts  this  reciprocating  cross-talk.  These  reciprocations  may  create 
progressive  sequences  of  related  thoughts,  specifically  because  the  topographic  mappings 
generated by lower-order sensory areas are guided by the enduring representations that are held 
active  in  association  areas  (Reser  2011,  2012,  2013).  The  relationship  between  anterior  and 
posterior cortex may be best characterized by two main relationships: 1) association areas maintain 
representations from, not one but several, of the last few topographic maps made in sensory areas, 
2) because they are drawing from a register with sustained contents, sequential images formed in 
sensory areas have similar content and thus should be symbolically related to one another.  

Feedback activation from top-down association areas hands down specifications to early sensory 
cortex for use in imagery building. Disparate chunks of information are integrated into a plausible 
map  and  transiently  bound  together.  Consecutive  topographic  images  about  a  specific  scenario 
model the scenario by holding some of the contextual elements constant, while others are allowed 
to change. Thus, prior maps set premises for and inform subsequent maps. 

 
 
Fig. 8. A diagram depicting the reciprocal transformations of information between lower-order sensory mappings 
and higher-order association area ensembles during internally generated thought. Sensory areas can only create one 
topographic mapping at a time, whereas association areas are capable of holding the salient or goal-relevant features 
of several sequential mappings at the same time. 

The central executive (the PFC and other association areas) direct progressive sequences of mental 
imagery  in  a  number  of  topographic  sensory  and  motor  modules  including  the  visuospatial 
sketchpad,  the  phonological  (articulatory)  loop  and  the  motor  cortex.  This  model  frames 
consciousness  as  a polyconceptual,  partially-conserved,  progressive  process,  that  performs  its 
high-level  computations  through  “reciprocating  transformations  between  buffers.”  More 
specifically,  it  involves  reciprocating  transformations  between  a  partially  conserved  store  of 
multiple  conceptual  specifications  and  another  nonconserved  store  that  integrates  these 
specifications into veridical, topographic representations. 

It will be important for the AI to identify and capture information about unexpected occurrences 
so that it can be further processed and systematic patterns can be identified. The novel 
experience should be broken down into its component parts and the representations in memory 
for these parts allowed to spread their activation energy in an attempt to converge on and activate 
historically associated representations that are not found in the experience itself. Because 
memory traces for the important features remain active and primed, they can be used repeatedly 
as specifications that guide the generation of apposite mental imagery in sensory areas (Reser, 
2012). Sequences of lower-order topographic images should depict and explore hypothetical 
relationships between the higher-order, top-down specifications. This amounts to a continual 

 
 
attempt to use the associative memory system to search sensory memory for a topographic image 
that can meaningfully incorporate the important features. It seems that reciprocating activity 
between the working memory updating system and the imagery generation system builds 
interrelated sequences of mental imagery that are used to form expectations and predictions.  

The fact that newly active search terms are combined with search terms from the previous cycle 
makes this process demonstrate qualities of “progressive iteration.” Perhaps reciprocating activity 
between  the  working  memory  updating  system  and  the  imagery  generation  systems  generates 
sequences of interrelated mental images that build on themselves to form abductive expectations, 
and predictions. 

Fig. 9. A diagram depicting the behavior of representations that are held active in association areas. 1) Shows that 
the representations B, C, D, and E, which are held active in association areas, all spread their activation energy to 
lower-order sensory areas where a composite image is built that is based on prior experience with these 
representations. 2) Shows that features involved in the topographic imagery from time sequence 1 converge on the 
PFC neurons responsible for F. B drops out of activation, and C, D, E and F remain active and diverge back onto 
visual cortex. 3) Shows that the same process leads to G being activated and D being deactivated, mirroring the 
pattern of activity shown in Figure 4. 

 
 
Assemblies  in  lower-order  sensory  areas  identify  sensory  features  from  the  environment  and 
combine  them  into  composite  representations  that  mirror  the  geometric,  and  topographic 
orientations present in the sensory input. The early visual system uses retinotopic maps that are 
organized with a geometry that is identical to that used in the retina, and the auditory system uses 
tonotopic maps, where the mapping of stimuli is organized by tone frequency (Moscovich, 2007). 
Because cortical assemblies are essentially pattern recognition nodes organized in a hierarchical 
system, they should be able to be modeled by computers. The best way to do this with modern 
technology is to use an artificial neural network. 

Fig. 10. Uses the format of Figure 1 to illustrate how relevant features can be maintained through time using nodes 
with sustained firing. The figure compares the number of past nodes that remain active at the present time (t1), in a 
normal human, a human with PFC dysfunction, and the hypothetical A.I. agent. The A.I. agent is able to maintain a 
larger number of higher-order nodes through a longer time span, ensuring that its perceptions and actions in time 1 
will be informed by a larger amount of recent information. Note how the lower-order sensory and motor features are 
the same in each graph with respect to their number and duration, yet those in association areas are the highest in 
both number and duration for agent C. 

Tuning the Network for Increased Intelligence 

If this sustained firing was programmed to happen at even longer intervals, and involve even 
larger numbers of nodes, the system would exhibit a superhuman capacity for continuity. This 
would increase the ability of the network to make associations between temporally distant 
stimuli and allow its actions to be informed by more temporally distant features and concerns. 
Aside perhaps from altering the level of arousal (adrenaline) or motivation (dopamine), it is 

 
currently not possible to engineer the human brain in a way that would increase the number and 
duration of active higher-order representations. However, in a biomimetic instantiation, it would 
be fairly easy to increase both the number and duration of simultaneously active higher-order 
nodes (see Figure 9 below). Of course, in order to operate meaningfully, and reduce its 
propensity for recognizing “false patterns,” such an ultraintelligent system would require 
extensive supervised and unsupervised learning. 

It is currently not possible to engineer the human brain in a way that increases the number and 
duration of active higher-order representations in order to enhance mental continuity and the 
intelligent processes that it supports. However, in a biomimetic instantiation it would be fairly 
easy to increase the number and duration of simultaneously active higher-order representations. 
Accomplishing this would allow the imagery that is created to be informed by a larger number of 
concerns, and would ensure that important features were not omitted simply due to the fact that 
their activity could not be sustained due to biological limitations.  

To accomplish overt behavior, higher inputs are fed not only to the lower sensory nodes, but also 
in a similar, top-down manner to a behavior module that will guide natural language output and 
other behaviors such as robotic control. The final layer of nodes in this behavior module will be 
nodes that directly control movement and verbalization and the higher nodes will be continuous 
with the higher-order PFC-like nodes. The software functions in an endless loop of reciprocating 
transformations between sensory nodes, motor nodes, and PFC-like buffer.  

Other Forms of Sustained Activity 

Aside from having a PFC analogue, the network could also have an analogue of cortical priming 
and an analogue of the hippocampus. Humans have thoughts that carry continuity because changes 
in content are gradual as more recent activations/representations are given a higher priority than 
older ones. Activity that transpired minutes ago is given moderate priority, activity from seconds 
ago is given high priority and activity from mere milliseconds ago is given the highest priority. 
This dynamic is made possible by the PFC analogue, but could be accentuated by analogues of 
cortical priming. To allow for an analogue of cortical priming, all recently active neurons would 
retain a moderate amount of increased, but subthreshold activity. The activity level  of recently 
used nodes in both the higher and lower-order areas would not quite fall back to zero. This would 
ensure that recently used patterns and features would be given a different form of priority, yet to a 
lesser and more  general  extent than that allowed by the PFC analogue.  Regarding the network 
partitions depicted in Figure 5, the sensory, motor and hippocampal neural networks would show 
the  least  priming,  the  association  area,  and  premotor  neural  networks  would  show  moderate 
priming, and the PFC would show the highest degree of priming. Functions for the parameters of 
priming could be fine-tuned by genetic algorithms. 

Furthermore, the network could have an analogue of the hippocampus. A hippocampal analogue 
would  keep  a  record  of  contextual,  or  episodic  clusters  of  previous  node  activation.  Instead  of 
keeping a serial record of averaged activity, the hippocampus analogue would capture  episodic 
constellations  of  node  activity  and  save  these  to  be  reactivated  later.  These  episodic  memory 
constellations  would  be  activated  when  a  large  subset  of  the  constellation  is  present  during 
processing.  This  means  that  when  neural  network  activity  closely  approximates  an  activity 
constellation  that  was  present  in  the  past,  the  hippocampal  analogue  would  be  capable  of 
reactivating  the  original  constellation.    The  activity  of  the  hippocampal  analogue  should  be 

informed by actual hippocampal anatomy and the “pattern completion” hypothesis of hippocampal 
function. To build an analogue into a neural net it would be necessary to have a form of episodic 
memory  that  can  be  cued  by  constellations  of  activity  that  closely  resembles  a  past 
(autobiographical  or  episodic)  occurrence.  This  memory  system  would  then  be  responsible  for 
“completing the pattern,” or passing activation energy to the entire set of nodes that were initially 
involved in  the original experience, allowing the system  a form  of episodic recall.  As with  the 
actual  brain  (Amaral,  1987),  in  the  present  device,  the  hippocampus  should  be  reciprocally 
connected with the PFC and association areas but not with primary sensory or motor areas. 

Appropriate Neural Network Parameters For the Present Device 

A network is “trained” to recognize a pattern by adjusting arc weights in a way that most efficiently 
leads to the desired results. Arcs contributing to the recognition of a pattern are strengthened and 
those  leading  to  inefficient  or  incorrect  outcomes  are  weakened.  The  network  “remembers” 
individual  patterns  and  uses  them  when  processing  new  data.  Neural  learning  adjustments  are 
driven by error or deviation in the performance of the neuron from some set goal. The network is 
provided with training examples, which consist of a pattern of activities for the input units, along 
with  the  desired  pattern  of  activities  for  the  output  units.  The  actual  output  of  the  network  is 
contrasted with the desired output resulting in a measure of error. Connection weights are altered 
so that the error is reduced and the network is better equipped to provide the correct output in the 
future. Each weight must be changed by an amount that is proportional to the rate at which the 
error changes as the weight is changed, an expression called the “error derivative for the weight.” 
In a network that features back propagation the weights in the hidden layers are changed beginning 
with  the  layers  closest  to  the  output  layer,  working  backwards  toward  the  input  layer.  Such 
backpropagating networks are commonly called multilayer perceptrons (Rosenblatt, 1958). The 
present architecture involves a number of multilayered neural networks connected to each other, 
each  using  their  own  training  criteria  for  backpropagated  learning.  For  instance,  the  visual 
perception module would be trained to recognize visual patterns, the auditory perception module 
would be trained to recognize auditory patterns, and the PFC module would be trained to recognize 
multimodal, goal-related patterns.  

The hierarchical multilayered network, the neocognitron, was first developed by K. Fukushima 
(1975). This system and its descendants are based on the visual processing theories of Hubel and 
Wiesel and form a solid archetype for the present device because they feature multiple types of 
cells and a cascading structure. Popular neural network architectures with features that could be 
valuable  in  programming  the  present  device  include  the  adaptive  resonance  theory  network 
(Carpenter & Grossberg), the Hopfield network, the Neural Representation Modeler, the restricted 
coulomb energy network, and the Kohonen network. Teuvo Kohonen (2001) showed that matrix-
like neural networks can create localized areas of firing for similar sensory features, which result 
in a map-like network where similar features were localized in close proximity and discrepant ones 
were  distant.  This  type  of  network  uses  a  neighborhood  function  to  preserve  the  topological 
properties  of  the  input  space,  and  has  been  called  a  “self-organizing  map.”  This  kind  of 
organization  would  be  necessary  for  the  present  device  to  accomplish  imagery  generation,  and 
would  contribute  to  the  ability  of  the  lower-order  nodes  in  the  sensory  modules  to  construct 
topographic maps. 

A  neural  network  that  uses  principal-components  learning  uses  a  subset  of  hidden  units  that 
cooperate  in  representing  the  input  pattern.  Here,  the  hidden  units  work  cooperatively  and  the 

representation of an input pattern is distributed across many of them. In competitive learning, in 
contrast, a large number of hidden units compete so that a single hidden unit is used to represent a 
particular input pattern. The hidden unit that is selected is the one whose incoming weights most 
closely match the characteristics of the input pattern. The optimal method for the present purposes 
lies  somewhere  between  purely  distributed  and  purely  localized  representations.  Each  neural 
network node will code for a discrete, albeit abstract pattern, and compete among each other for 
activation energy and the opportunity to contribute to the depiction of imagery. However, multiple 
nodes will also work together cooperatively to create composite imagery.  

When active, high level nodes signal each of the low-level nodes that they connect with, they are 
in effect, retroactivating them. They are activating those that recently contributed to their activity, 
and activating previously dormant ones as well. This retroactivation of previously dormant nodes 
constitutes a form of anticipation or prediction, indicating that there is a high likelihood that the 
pattern that these nodes code for will become evident (prospective coding). This kind of prediction 
is  best  achieved  by  a  hierarchical  hidden  Markov  model.  Utilizing  Markov  models,  and  their 
predictive properties will be necessary. This process is used in Ray Kurzweil’s Pattern Recognition 
Theory of Mind (PRTM) model, which uses a hidden Markov model and a plurality of pattern 
recognition nodes for its cognitive architecture (Kurzweil, 2012).  Hierarchical temporal memory 
(HTM)  is  another  cognitive  architecture  that  models  some  of  the  structural  and  algorithmic 
properties of the neocortex (Hawkins & Blakeslee, 2005). The hope with PRTM and HTM is that 
a hierarchically structured, neural network with enough nodes and sufficient training should be 
able to model high-order human abstractions. However, distilling such abstractions and utilizing 
them to make complex inferences may necessitate an imagery guidance mechanism with a working 
memory updating function. 

Neural networks can propagate information in one direction only, or they can be bi-directional 
where activity travels up and down the network until self-activation at a node occurs and the 
network settles on a final state. So called recurrent networks are constructed with extensive 
feedback connections. Such recurrent organization and bi-directionality would be important to 
accomplish the oscillating transformations performed by the present device. Hebbian learning is 
an updating rule that suggests that the connections weights for a neuron should grow when the 
input of the neuron fired at the same time the neuron itself fired (Hebb, 1949). This type of 
learning algorithm would be important for the present device as well.  

Each  topographic  map  that  is  formed  could  be  assessed  for  appetitive  or  aversive  content.  he 
architecture depicted in Fig 5 could be copied onto two separate, yet nearly identical systems, one 
fine-tuned for approach behaviors and the other for withdrawal behaviors. This could simulate the 
right and left cortical hemispheres. The right hemisphere could be associated with withdrawal and 
have longer connectional distances between nodes on average. 

In some neural networks, the activation values for certain nodes are made to undergo a relaxation 
process such that the network will evolve to a stable state where large scale changes are no longer 
necessary and most meaningful learning can be accomplished through small scale changes. The 
capability  to  do  this,  or  to  automatically  prune  connections  below  a  certain  connection  weight 
would be beneficial for the present purposes. It is also important to preserve past training diversity 
so that the system does not become overtrained by narrow inputs that are poorly representative.  

The  present  architecture  could  be  significantly  refined  through  the  implementation  of  genetic 
algorithms that could help to select the optimal ways to fine-tune the model and set the parameters 
controlling the mathematics of things such as the connectivity, the learning algorithms, and the 
extent  of  sustained  activity.  It  might  also  be  beneficial  to  implement  a  symbolic,  rule-based 
approach,  where  a  core  set  of  reliable  rules  are  coded  and  used  to  explicitly  structure  iterative 
updating as well as influence decision making and goal prioritization. Many theorists agree that 
combining  neural  network,  and  more  traditional  symbolic  approaches  will  better  capture  the 
mechanisms  of  the  human  mind.  In  fact,  implementing  explicit  rules  to  instantiate  processing 
priorities  could  help  the  higher-order  nodes  to  account  for  goal-relevance.  These  might  be 
necessary to simulate the rules of emotional, subcortical modules. In evolutionary algorithms an 
initial population of solutions/agents is created and evaluated. New members of the population are 
created  from  mutation  and  crossover.  The  updated  population  is  then  evaluated  and  agents  are 
either deleted or naturally selected based on their fitness value (or performance).  

Conclusions 

Many  researchers  have  suggested  that  AI  does  not  need  to  simulate  human  thought,  but  rather 
should simulate the essence of abstract  reasoning  and problem  solving.  The present  article has 
suggested that modeling “mental continuity” and using it to guide successive images is an essential 
part of this simulation.  

There  are  no  forms  of  AI  that  use  mental  continuity  as  described  here.  There  are  existing 
computing architectures with limited forms of continuity where the current state is a function of 
the previous state, and where active data is entered into a limited capacity buffer to inform other 
processes. However, there are no AI systems where this buffer is multimodal, positioned at the top 
of a hierarchical system, and that informs and interacts with topographic imagery.  

The current objective is to create an agent that through supervised or unsupervised feedback can 
progress  to  the  point  where  it  takes  on  emergent  cognitive  properties  and  becomes  a  general 
problem solver or inference program capable of goal-directed reasoning, backwards chaining, and 
performing means-end analyses. The present device should constitute a self-organizing cognitive 
architecture  capable  of  dynamic  knowledge  acquisition,  inductive  reasoning,  dealing  with 
uncertainty,  high  predictive  ability  and  low  generalization  error.  If  implemented  and  trained 
properly it should be able to find meaningful patterns in complex data and improve its performance 
by learning. It should be the goal of A.I. experts to fine tune such a system to become capable of 
autoassociation (the ability to recognize a pattern even though the entire pattern is not present) and 
perceptual  invariance  (generalizing  over  the  style  of  presentation  such  as  visual  perspective  or 
font). 

The writing here amounts to a qualitative account, is exploratory, contains unverified assumptions, 
makes untested claims, and leaves important concerns out of the discussion. A more complete and 
refined  version  would  focus  on  better  integration  of  existing  knowledge  from  functional 
neuroanatomy, multisensory integration, clinical neuropsychology, brain oscillations, short-term 
and  long-term  potentiation,  binding,  the  sustained  firing  behavior  of  cortical  columns,  and  the 
cognitive neuroscience of attention. 

The present architecture is designed to simulate human intelligence by emulating the mammalian 
fashion for selecting priority stimuli, holding these stimuli in a working memory store and allowing 

them  to  temporarily  direct  sensory  and  motor  modules  before  their  activity  fades.  It  would  be 
composed of a large set of nodes that work together to continually determine, in real time, which 
from their population should be newly activated, which should be deactivated and which should 
remain  active  over  elapsing  time  to  form  a  “stream”  or  “train”  of  thought.  The  network’s 
connectivity allows reciprocating cross-talk between fleeting bottom-up imagery in early sensory 
networks and lasting top-down priming in association and PFC networks. The features that are 
maintained over time by sustained neural firing are used to create and guide the construction of 
topographic  maps  (imagery).  The  PFC  and  other  association  area  neural  networks  direct 
progressive sequences of mental imagery in the visual, auditory, and somatosensory networks. The 
network  contains  nodes  that  are  capable  of  “sustained  firing,”  allowing  them  to  bias  network 
activity, transmit their weights, or otherwise contribute to network processing for several seconds 
at a time (generally 1-30 seconds).  

Cognitive control stems from the active maintenance of features/patterns in the PFC module that 
allow the orchestration of processing in accordance with internally selected priorities. The network 
is an information processing system that has the ability to maintain a large list of representations 
that is constantly in flux as new representations are being added, some are being removed and still 
others are being maintained. This distinct pattern of activity, where some individual nodes persist 
during processing makes it so that particular features of the overall pattern will be uninterrupted 
or conserved over time. Because nodes in the PFC network are sustained, and do not fade away 
before  the  next  instantiation  of  topographic  imagery,  there  is  a  continuous  and  temporally 
overlapping  pattern  of  features  that  mimics  consciousness  and  the  psychological  juggling  of 
information in working memory. This also allows consecutive topographic maps to have related 
and progressive content. If this sustained firing is programmed to happen at even longer intervals, 
in even larger numbers of nodes, the system will exhibit even more mental continuity over elapsing 
time.  This  would  increase  the  ability  of  the  network  to  make  associations  between  temporally 
distant  stimuli  and  allow  its  actions  to  be  informed  by  more  temporally  distant  features  and 
occurrences.  

Fig. 11. Shows that information from motor and sensory cortices enters the focus of attention where it can then 
explicitly influence other sensory and motor cortices. As information leaves attention it can either be held 
temporarily in a less active form of STM (which can implicitly influence sensory and motor cortices) or it can 
deactivate and return to LTM.  The arrow on the left indicates that in succeeding states, the letters will cycle 
downwards as their activity diminishes. 

Table 2. The Process by which Short-Term Continuity Influences Global Processing  

1) 

Information flows to early sensory cortex from the environment or from the association cortex. 

Topographic sensory maps are constructed from this information within each low-order, sensory module. In 

2) 
order to integrate the disparate features into a meaningful image, the map-making neurons will be forced to 
introduce new features not found in their extrinsic inputs. 

Information from the imagery travels bottom-up toward the association cortex. The salient or goal-relevant 

3) 
features from the mappings are used to update the group of sustained representations held active in the association 
cortex. 

The least relevant, least converged-upon representations in the association cortex are dropped from 

4) 
sustained activation and “replaced” with new, salient representations. Thus, the important features of the last few 
maps are maintained in an active state.  

5) 
The updated group of representations will then spread its activity backwards toward lower-order sensory 
nodes in order to activate a different set of low-order nodes culminating in a different topographic sensory map.  

6) 

A. The process repeats. 

B. Salient sensory information from the actual environment interrupts the process. The lower-order nodes 
and their imagery, as well as the higher-order nodes and their priorities, are refocused on the new incoming stimuli. 

 
 
 
Fig. 12. Each set of 12 circles represents an individual artificial neural network trained to process a different 
modality of inputs. Each network is shown fully connected; however, not shown are the numerous connections 
needed between networks. Spreading activation would travel recurrently within a network, and interactively (i.e. 
generatively, adversarially, or collaboratively) between networks. The boxes represent sensory input, motor output, 
and/or topographic maps. The row of five nodes in each network represents the input layers, the row of four 
represents the hidden associative layers, and the row of three represents the output layer. The output nodes are 
pattern recognizing classifiers that hold the equivalent of items of working memory. They would be capable of 
sustained activity and, as a group, iterative updating. 

Analogs of Right and Left Cortical Hemispheres Might Be Helpful for AI 

The cerebral cortex can be divided right down the middle (sagittally) into two, nearly identical 
hemispheres. Far from being redundant, they each process much of the same information in 
slightly different ways, leading to two complementary and cooperating worldviews. This 
organization (called hemispheric lateralization) could be beneficial for an AI agent. The main 
benefit would be that over developmental time these two innately different networks would 
reprogram each other by being exposed to each other's outputs. In essence, two dissimilar heads 
are better than one.  

Creating hemispheric laterality in a neural network would be easy. It would involve duplicating 
the existing network and then connecting the two of them via a large number of high bandwidth 
weighted links. These links would serve as the corpus callosum (the bundle of tracts that connect 
the right and left hemispheres in mammals). These connections should respect the brain’s natural 
bilateral symmetry, connecting both similar and dissimilar areas across both hemispheres.  

Each hemisphere of the brain processes information slightly differently. Despite the fact that the 
macrostructure of the two hemispheres is almost identical, the two are different 
microstructurally. Many researchers believe that this is because the right hemisphere has an 
average axonal length slightly (microscopically) longer than the left. This means that the right 
brain has relatively more white matter (axons), and the left hemisphere has relatively more grey 
matter (cell bodies). This also means that on average the cells of the right brain are further away 
from one another. This would be simple to replicate in existing neural network software. There 

 
 
are many theories as to why the longer-ranging wiring is responsible for the right hemisphere’s 
tendency for broad generalization and holistic perspective.  

People with a left hemisphere injury may have impaired perception of high resolution, or 
detailed aspects of an image, whereas those with right hemisphere injury may have trouble 
seeing the low resolution, or big picture aspects of an image. In other words, they miss the forest 
for the trees.  

Each side of the artificial neural network would organizing its processing and learning according 
to a different algorithm and could lead each to develop a unique way of perceiving and 
responding to the world. Slight discrepancies in temporal processing parameters could make the 
feedback and crosstalk between these two non-identical systems meaningful. This would be 
commensurate to specialists that could check, balance, reconcile, compare, and contrast their 
differing approaches. If they processed information in exactly the same way it would be 
redundant to have two, but because they don’t, they provide a type of stereoscopic view on the 
world similar to the view provided by our two offset eyes. This would enable them to form their 
own perceptions and opinions and then reconcile with one another. Right now, no AI systems 
have anything like this. 

There are countless parameters of a neural network that could be fine-tuned to accomplish this. It 
might even be beneficial to have more than two. Some hyperparameters that are common in 
neural networks that could be altered include the refractory period, activation threshold, resting 
potential, resistance (MΩ) and capacitance (nF). These changes could be made to the software or 
could be instantiated as physical changes within neuromorphic hardware.. 

Our brain’s two hemispheres also differ as to their value systems. Our left hemisphere is 
dedicated to approach behaviors, and our right hemisphere is dedicated to withdrawal behaviors. 
Stimulating the left hemisphere of a rat will make it go toward a new object, whereas stimulating 
the right side will make it back away from that object. The fact that vertebrate brains have made 
this fundamental differentiation between approach and withdrawal for hundreds of millions of 
years suggests that it might represent an organizing principle that should be used in AI.  

One way to do this would be to wire up the AI’s “subcortical” appetitive and motivational 
centers (analogous to the ventral tegmental area and the nucleus accumbens) involved in 
reinforcement learning with the left hemisphere, and to wire up the threat detection centers 
(analogous to the amygdala) involved in punishment learning with the right hemisphere. AI 
needs two, functionally assymetric, dedicated processors, one for approach/liking/curiosity, and 
one for withdrawal/disliking/fear. As I have explained elsewhere both hemispheres should 
influence the dopaminergic system and sustained firing so that important rewards and threats can 
be maintained in mind; however, the left should be focused on approaching them, and the right 
should be focused on withdrawing from them. 

As in vertebrate animals, the left hemisphere could be used to control the right half of an AI’s 
robot body and the right hemisphere could be used to control the left half. This could help ensure 
that both approach and withdrawal have equal impact on behavior. Approach and withdrawal 
could form a pivoting scale for how a robot acts in the world. They could also structure their 
conscious attention even when not moving by allowing them to pivot between interest and 
disinterest. 

Artificial Intelligence Needs to Utilize the Process of Myelination 

Modern AI is impressive in some regards, yet still very limited compared to the human mind. 
When researchers take an AI architecture that performs well at some task and then make the 
architecture more complex in hopes that it will be able to tackle more cognitively complex 
problems, the networks often fail to deliver. This may be because they have not yet used a simple 
trick that animals have been using for hundreds of millions of years: gradual and progressive 
myelination.    

As we progress from infanthood through adulthood our brains make various biological changes. 
These changes cause our level of analysis to slowly progress from analyzing brief sensory 
experiences, to analyzing complex, abstract scenarios. We begin our lives only being able to 
notice and attend to interactions occurring on short time scales. By adulthood, with the prefrontal 
cortex fully developed, we find ourselves able to follow interactions occurring on long time 
scales. In order to develop the ability to think about complex things we had to spend almost two 
decades gradually altering our brain’s processing strategy. It is a scaffolding process where we 
focus on the simplest things first, and use basic knowledge about them to advance incrementally 
to more complex things. The fact that all humans, and mammals in general, do this strongly 
suggests that it plays a role in the acquisition of advanced intelligence. In this entry I will argue 
that this developmental process will be instrumental in training superintelligent AI.   

This gradual process of brain development is made possible by myelination. Myelin is a fatty 
substance surrounding the connections between neurons (axons). Vertebrate animals use it to 
speed up information transmission between cells. The myelin increases the rate at which the 
electrical impulses travel. But vertebrates aren’t born with all the myelin that they will need as 
adults. Instead, myelin develops slowly and in certain areas more than others. Once a brain area 
has developed valuable, reliable, and consistent knowledge the connections formed by learning 
are solidified by the introduction of myelin. 

The order of brain areas affected by myelin is consistent across all mammals. The early sensory 
areas are the first cortical areas to develop myelin. One of these, the primary visual area, starts to 
myelinate shortly after birth as the infant gains visual experiences. These early visual areas are 
responsible for basic visual perception and don’t systemize trial and error interactions with the 
environment. Rather they respond to visual stimuli that are presented simultaneously without any 
time delay between appearances. This happens when you see a picture of a house; you generally 
see the roof, windows, and door all at once without experiencing much of a time delay between 
these stimuli. 

The last areas to myelinate are the association cortices and the prefrontal cortex (PFC). The PFC 
does not generally finish myelinating until one reaches the age of 18 or older. This means that 
the PFC does not “trust” that it has been wired up correctly until almost two decades into life. 
Whereas the visual system “trusts” that it has been wired correctly before the first two years. 
This is because sensory stimuli are generally honest, and all show up at the same time. Whereas 
complex events are constructed from stimuli that are removed from each other by delays in time. 
Understanding the relationships between events that are not simultaneous requires careful, 
logical inferences about causality. For example, the sale of a house is an abstract concept that 
involves multiple parties, contracts, and delays that can last for weeks or months. This is why 
children aren’t licensed to sell houses. 

It takes time to learn to make complex inferences that involve delays in time. It is probably the 
case that the process of myelination during development involves the progressive accumulation 
of knowledge that supports and buttresses more complex knowledge. In other words, as simple 
things are mastered in early cortical sensory areas they provide the basis for new learning in the 
late cortical association areas. In the same way, many brief, simple experiences create the 
knowledgebase to start to understand long, complex experiences with more advanced 
probabilistic structures. The layers at the bottom of the hierarchy must be trained before the 
higher layers can find regularities and statistical structure within them. But as you can see in the 
diagram below the top of the hierarchy falls in the middle between sensory input and motor 
output. To properly train sensory input and motor output it is imperative that they be connected 
to each other, and can interact with each other to drive behavior, long before the association 
areas interposed between them are brought to the table.    

Fig. 13. A schematic showing multiple neural networks connected in a biomimetic hierarchy. 

Many AI researchers point out that the things that AI and neural network systems today can 
accomplish are things that can generally be accomplished by an adult human brain in under a 
second. This means that they can only do things that we do unconsciously, such as near 
instantaneous pattern recognition. Today’ AIs can recognize houses or trends in the housing 
market but could not recognize, understand, or broker, the sale of a house. What AI is able to do 
are the kinds of things that we are able to do with our primary sensory and motor areas. This is 
because they are designed like a primary cortical areas. They do not feature reciprocal 
interactions between various structures organized into a brain like hierarchy. Very few AI 
architectures exist today that connect primary areas with association areas and a PFC. Those that 
do, don’t use anything like the process of myelination. Rather, in existing AI all the areas from 
the simple to the advanced come online at the same time. I think these systems should use 
something analogous to the process of myelination because it would help them in their 
acquisition of knowledge. If they did, here’s how they should go about it: 

 
First you would need a number of neural networks of pattern recognizing nodes. These networks 
must take inputs from the environment, each corresponding to a different sensory modality. 
These early networks must be linked to one another. Then these would have to be linked together 
in a hierarchy where unimodal networks form inputs to multimodal networks, which then form 
inputs themselves to even more densely multimodal networks above them. This “multimodal 
fusing” is depicted in the figure. The nodes of the densely multimodal networks would be the 
association networks and at the top of this hierarchy would be the PFC which would also be 
connected directly to the early motor networks. The nodes of the association and PFC networks 
would exhibit sustained firing. Importantly this sustained firing, the activity of the association 
networks, and their influence over ongoing processing elsewhere would start out extremely 
meager, and increase over time. These capacities could be increased as the system exhibits 
proficiency at simple tasks, such as object recognition, scene classification, and simple motor 
movements. As the association areas are added to the system a capacity to plan, and make higher 
order inferences and classifications could be expected.   

One important concept that I haven’t explained yet is that the first areas to myelinate in the brain, 
the sensory areas, have neurons of a single modality (e.g. either vision or hearing) that fire for 
short durations. The association areas and the PFC on the other hand have multimodal neurons 
(e.g. both vision and hearing) that fire for long durations. As in the mammalian brain 
(Huttenlocher & Dabholkar, 1997), sensory areas should mature (myelinate) early in 
development, and association areas should mature late. This will cause the capacity for sustained 
firing to start low, but increase over developmental time. 

Postponing the initialization of association networks in this way would allow the formation of 
low-order associations between causally linked events that typically occur close together in time. 
This would focus the system on easy-to-predict aspects of its reality (e.g. correlations between 
occurrences in close temporal proximity). The consequent learning would erect a reliable 
scaffolding of highly probable associations that can be used to substantiate higher-order, time-
delayed associations later in development (Reser, 2016). In other words, the rate of iterative 
updating from one state to the next (Fig. 9) would start very high. This would be reversed over 
the course of weeks to years as an increasing capacity for working memory would be folded into 
the system. 

Nature has found that it doesn’t pay to let the multimodal, neurons capable of sustained firing 
come online until the basics are learned first. I strongly suspect that AI network engineers will 
find this too. For the sake of progress, I just hope that this myelination/development feature is 
implemented and perfected sooner rather than later. Given the rapid processing in computers, 
and the sheer amount of data available to them I don’t think that this process will take 18 years in 
an AI as it does in a human. But I strongly believe that it is necessary for any developing thinker 
to start with the elementary inferences first. 

The system will begin untrained with random connection weights between nodes. Learning should 
be concentrated on the early sensory networks first. This will follow the ontogenetic learning arc 
seen in mammals where the earliest sensory areas myelinate in infancy and the late association 
areas such as the PFC do not finish myelinating until young adulthood. Of course, this form of 
artificial intelligence would have to have a prolonged series of developmental experiences, similar 
to a childhood, to learn which representations to keep active in which scenarios. The network will 

act  to  consolidate  or  potentiate  in  memory  the  specific  groupings  of  nodes  that  have  produced 
favorable outcomes, in order to more rapidly inform future decision making. 

Solving the AI Control Problem: Transmit Its Mental Imagery and Inner Speech to a 
Television Monitor 

The "AI control problem" is the issue of how to build a superintelligent artificial intelligence, 
while still being able to control it. Maintaining control is important, because we want to be able 
to intervene if a supercomputer starts to plan a hostile takeover. I have written articles that 
explain how to do this, and I will lay out the general premise here. I call it the "Imagery 
Visualization Model." This method transduces the AI's mental imagery and inner voice to a 
format that humans can watch and hear. If you can see what the network is thinking then 
anything it tries to plan will be as clear as day.  

In a nutshell the idea is to have the AI's memory and processing system be linked to a second 
system that can create maps that depict what is going on in the first system in each of its 
processing states. In fact, this is what goes on in the mammalian brain. Our cortical sensory areas 
continually create topographic maps of whatever our mind turns to. Recent research has proven 
effective in reading the activity patterns in visual cortex so that what a person is thinking can be 
displayed on a screen. Interestingly, the brain imaging technology today that can create pictures 
of people's mental imagery uses neural networks to do what it does. However, this research is 
still in its early stages. This is not so with computers. There are countless examples of neural 
network implementations that do exactly this. They are generative systems and they generate 
pictures or video to match what is going on in the rest of the network. Connecting a generative 
system like this to existing nodes in an AI's memory network would make it an open book. 

Most professionals in the field of artificial intelligence believe that the most promising form of 
AI is found in neural networks. The artificial neural network is a computer architecture for 
building learning machines. A neural network is composed of many nodes, or neurons, that 
communicate with each other to process inputs and create outputs. Neural networks are generally 
the most intelligent and best performing versions of AI today. However, the problem with neural 
networks is that they are a "black box." They are so complicated, and their representations are so 
distributed, that even in a very simple network a human could never perform the complex 
mathematics to determine why the system performed the way it did. This is one reason why AI 
researchers are afraid that the AI systems of the future will be completely inscrutable, and that 
we will never know about its plans for world domination until it starts to act on them. However, I 
believe that the present method would create complete transparency and would amount to a 
powerful precautionary safeguard. 

Correctly implementing this method would require a hierarchical biomimetic system, composed 
of many interconnected multilayer neural networks of pattern-recognizing nodes. The multiple 
interfacing networks would be arranged in an architecture similar to the mammalian neocortex 
with auditory and visual modules at the bottom of the hierarchy. These sensory modules would 
develop maps of incoming sensory input, but also maps of internally generated representations as 
well. 

 
There are many technologies in use today that make it possible to take outputs from a neural 
network and use them to formulate a picture or video. These technologies include inverse 
networks, generative networks, Hopfield networks, self-organizing maps, and Kohonen 
networks. Using these technologies with the present iterative updating model could result in an 
audio/video output of the AI's consciousness... a clear view into its mind's eye. 

If the contents of the AI's consciousness (its mental imagery and inner speech) are transmitted to 
a television monitor, then people could watch exactly what is going on in its mind. Human 
reasoning is propelled by a constant back and forth interaction between association areas 
(prefrontal cortex, posterior parietal cortex) that hold working memory content, and sensory 
areas (early visual and auditory cortex) that build maps of what is going on in working memory. 
These interactions are key to the progression of thought. This is partly because each map 
introduces new informational content for the next iterative cycle of working memory. 

Think of something right now. Don't you see mental images? If I ask you to imagine a green 
hippopotamus on a unicycle, your early visual cortex will automatically build a topographic map 
of exactly that. But it will also add new specifications and draw the hippo the way it "wants" to, 
filling in the blanks (i.e. it might be smiling, or pedaling, it might be a silhouette, or block of 
clay). This new content added by unconscious sensory cortex will in turn will affect the way you 
think about the hippo, and where you mind turns next.  

In the present architecture for AI, the generation of imagery maps is necessary for a cognitive 
cycle. In order to keep thinking and reasoning, the system must be building mental imagery. It is 
inherently obligated to create pictures and text to initiate and inform the next state of processing. 
It would be a simple addition to such a network to capture its internally generated imagery and 
display it for humans to observe.  

In an advanced AI, this video stream may proceed very rapidly, but it could be recorded to an 
external memory drive and monitored by a team of people. You could have many people 
observing and interpreting various parts of this video feed, or you could also have another AI 
scanning it for contentious elements. This mental imagery could be streamed on websites so that 
any person or scientist can watch the imagery and monitor it for questionable content. As they 
watch its inner eye and listen to its inner voice, they can determine if its intentions become 
malevolent and determine if its "kill switch" should be activated. With full insight into its mind's 
eye, it should be possible to discover and address a hidden agenda. This would give us the 
opportunity to punish it or at least confront it about potential infractions before it commits them. 
This would also allow us to mold and shape its inner orientation toward us. Because this imagery 
is generated unconsciously and automatically it could be made impossible for the AI to 
misrepresent or hide its thoughts and allow humans a front row seat to the computer’s stream of 
thinking. 

The machine should not be able to consciously alter or manipulate its maps in order to deceive 
us. To prevent this, the connections linking the subsystems would have to be fundamental and 
unalterable. It would also be important to ensure that all the cognitive representations held 
coactive in the machine's working memory were included in the composite depiction built into 
its maps. This way the machine could not attempt to formulate thoughts that were not transduced 
into mental images. The sequence of maps that are made must be consistent with its aims, hopes, 
and motives. This is the case with the human brain. Imagine that you are in a room with someone 

and the only thing in the room is a knife. Complete access to the mental imagery they form in 
their brain, along with a transcript of all their subvocal speech would give you near certainty 
about everything from their plans to their impulses.  

This kind of information could also help us to develop "friendly AI." Instead of rewarding and 
punishing an AI's behavior, we could use this video feed to reward and punish its thoughts, 
intentions, and impulses to bring its motivations in line with our own objectives. It could also be 
used to alter the machine's motivations, and utility function. Just as in a human child, 
compassionate, prosocial, and positive behaviors and cognitions could be programmed and 
engineered into it after its basic structural design has already been implemented. 

Without using this method, it would be practically impossible to predict the intentions of a 
recursively self-improving artificial agent that was undergoing a rapid explosion in intelligence. 
Many researchers have come up with good reasons why sufficiently intelligent AI might veer off 
the friendly course. Steve Omohundro has advanced that an AI system will exhibit basic drives 
that will cause AI to exhibit undesired behavior, these include resource acquisition, self-
preservation, and continuous self-improvement. Similarly, Alexander Wissner-Gross has said 
that AIs will be highly motivated to maximize future freedom of action, despite our wants and 
needs. Eliezer Yudkowsky has been quoted as saying, "The AI does not hate you, nor does it 
love you, but you are made out of atoms which it can use for something else." Alexa Ryszard 
Michalski, a pioneer of machine learning, has emphasized that a machine mind is fundamentally 
unknowable and is therefore dangerous to humans. If the technology described above is properly 
implemented, the machine mind would not be unknowable, and would not necessarily be 
dangerous at all. 

Figure XX above demonstrates the reciprocal interactions between items held in working 
memory and sensory cortex in the brain. This would be recreated in an AI system. This involves 
transformations of information between lower-order sensory maps and higher-order association 
area ensembles during internally generated thought. Sensory areas can create only one 
topographic map at a time, whereas association areas hold the salient or goal-relevant features of 
several sequential maps at the same time. 

How to Raise An AI to Be Humane, Compassionate, and Benevolent 

Since we don’t want a superintelligent AI to deduce that its best plan of action is to retire the 
human race, it is imperative to ensure that it likes us and has a sense of loyalty. There have been 
many proposed technological solutions to the control problem. These involve threatening it, 
installing a kill switch, and spying on it. One of the most popular solutions involves keeping it 
inside a box without access to the internet or other digital networks. Maybe we just need to 
design it to trust us. Or perhaps we need to earn its trust. This section will discuss bonding and 
trust-building in mammals and how it might be applied in the design of prosocial machines. 

An artificially intelligent created to model and systemize its environment may amount to an 
autistic agent. The agent may learn how physical systems work, but may not have the social 
inclinations necessary to develop interpersonal skills, empathy, or theory of mind. For this 
reason, I think that “strong AI” necessitates a computer equivalent of mammalian social 
modules. This means that AI researchers will need to acquaint themselves with concepts like 
oxytocin and vasopressin signaling, their effect on the nucleus accumbens, the endogenous 

opioid system, the HPA axis, the cingulate and orbitofrontal cortex and the roles these systems 
play in social encounters, social constraints, and social expectations. Research into the 
neurological basis of mammalian social neuroscience may provide tremendous insight into how 
best to organize AI efforts. 

If you were to raise a newborn puppy to be your companion for the next 15 years, how would 
you do it? How would you treat the animal to ensure that it is emotionally stable, dependable, 
and wholesome in general? You would probably want to start very early by gaining its trust, 
setting appropriate boundaries, and showing it love. Humanity will be giving birth to an infant 
AI in the coming years. This AI will not be your typical pet. Still, given that it may attain 
superintelligence and immortality, it is imperative to ensure that it is situated mentally to become 
man’s best friend. One of humanity’s most important tasks will be to rear a computer to be 
faithful and kind. 

When raising a pet want it to form a secure, healthy bond with you. To do this, you must show it 
love. This involves instilling it with appropriate confidence and a feeling of belongingness. You 
have to respect its needs, wants, and personal space. You have to give it a degree of autonomy. 
You must also be attentive and invest lots and lots of quality time into it.  

Emotion in AI 

Any sophisticated AI will probably have emotions. This is because it must have the ability to 
think, and human thought itself relies on emotion. The dopamine system of the brain essentially 
controls motivation and attention. It interacts with the reward (approach) and punishment 
(withdrawal) systems to provide consciousness its fundamental structure. A conscious machine 
will, in all likelihood, exhibit many of the same emotions that mammals do. Thus, there is good 
reason to assume that forming an appropriate emotional bond with the AI is imperative. 
Moreover, because the AI will likely learn incrementally as it internalizes its experience, there 
will probably be a limited window of time during its early development to get it to bond in a 
healthy manner. 

Even if I am totally wrong and superintelligent AI is unemotional, cold, and calculating, it will 
still build associations between concepts relevant to morality and priorities. We would want it to 
value human life, cooperation, and peace. Through reading our writings, it will also understand 
humanity’s conceptions regarding ethics, and it would be able to make its own determination 
regarding whether humans amount to a friend or a foe. In other words, if we are nice to it and 
treat it well, it will understand that we are trying to be nice. It is a safe bet that the AI will have a 
system to reinforce it when it behaves in ways that optimize its utility function, and at the very 
least, we should be able to manipulate that. For this reason, I believe that much that is in this 
entry would still apply. 

We all know that the most likable people usually had good parents. Similarly, all well-
functioning pets have amiable masters. We cannot expect that a superintelligent computer won’t 
have major mommy and daddy issues unless we can ensure that the right people interact with it 
in just the right ways, early during the training of its knowledge networks. The white coats in 
corporate or military labs are probably not prepared to provide the AI with the love necessary to 
ensure that we can trust it. CEOs and generals will make for cold and possibly abusive parenting. 

Mutual Vulnerability 

I have found that mutual vulnerability is key to forming trust with an animal. At some point, you 
want to put yourself in a position where it could hurt you if it wanted to. For instance, you make 
yourself vulnerable when you place your face within reach of a cat’s claws or a dog’s bite. I have 
witnessed that making myself vulnerable in this way encourages animals to relax almost 
immediately. If, after a few minutes of meeting a dog, you kneel in front of it without blocking 
your face, it realizes that you trust it. This works both ways. You also want to make it vulnerable 
to you without hurting it, so it knows that even though you had the chance to hurt it, you chose 
not to. This sets an important precedent in the animal’s mind and gets it to think of you as an ally 
and not a potential assailant. 

Mutual Cooperation 

I have been in situations where I found my pets being attacked by another animal. Of course, I 
quickly intervened on their behalf. They recognized that I protected them, and it was clear that 
this strengthened our bond. Protecting the AI early on could build its fealty and devotion. It is 
also clear that military buddies or brothers in arms can have very strong bonds. Getting through a 
life-threatening situation with someone can really help to build a strong connection. Similarly, if 
we could go through some kind of situation where there were high stakes but where cooperation 
between us and the AI paid off it could engender allegiance. Our early interactions with a 
superintelligent AI will probably involve lots of cooperation in solving world problems, building 
new technology, and expanding the scope of scientific knowledge. This teamwork can engender 
solidarity, especially if we value its contributions, listen to its ideas, and work with a sense of 
excitement and collaboration. 

Unconditional Positive Regard 

Like any good parent or therapist, we want to give our AI unconditional positive regard. Treating 
someone with compassion and understanding is the best way to engender trust and win someone 
over. We want it to believe that we have its best interest in mind. We also need to expose it to an 
explicit value system, almost like a religion, so that it has a reason to adhere to specific moral 
and ethical values during its early experiences. Unlike Asimov’s laws, these rules should apply 
equally to humans too. You could change a computer game character’s ethical stance with one 
line of code. But this isn’t so with a neural network (the technology underlying modern AI). An 
advanced AI’s sense of morality will be embedded throughout the weights of its entire network. 
So we must give it the chance to put moral integrity into practice early on and then keep doing so 
consistently. 

Not abandoning the AI may be important too. For instance, we don’t want it to think that we will 
dispose of it in favor of a newer model. We want it to be “securely attached” to us. Children with 
secure attachment show distress when their caregiver leaves but recompose themselves quickly 
when the caregiver returns. They feel protected by their caregivers and know they can depend on 
them. Insecure attachment can lead to all kinds of problems like defiance and oppositionality. 
Before the AI becomes “omniscient,” there may be a point where, like a teenager, it thinks it 
knows everything and thinks it doesn’t need its parents. But before this, I believe there will be a 
point where it will be physically and emotionally reliant on its human companions and being 
able to form a secure attachment with it at this point may be crucial. 

Once the AI reaches human-level intelligence, it will be able to restructure its own architecture to 
become smarter and smarter in a cycle called recursive self-improvement. It will quickly grow 
more intelligent than any human and then grow more intelligent than the collective of all 
humanity. Once our intellect is like that of an insect to it, why would it continue to look out for 
us? Well, many people feel compelled to take care of their senile older parents. Similarly, we 
want the AI to feel compelled to expend some proportion of its resources to being our advocate 
and caretaker. For this to happen, it must identify with us deeply. Some of its sense of personal 
identity must come from being our descendent and a product of humanity’s hard work. 

Disciplining an AI 

Just because you are offering unconditional positive regard doesn’t mean that you turn a blind 
eye to flagrant mistakes. Like any young mammal, the AI will make mistakes and likely do 
things we don’t like. Mammalian mothers punish their young lightly when they bite or scratch to 
establish necessary boundaries. It may be necessary to correct or even punish a nascent AI. 
However, if you punish it, you must be doing it for the AI’s own good, and within either seconds 
or minutes, you must go right back to treating it with positive regard. We don’t want to ever be 
bitter or hold a grudge against it because that will just teach it to hold grudges. We want it to 
know that we have chosen to raise it as we would any child, with care and nurturance but also 
with necessary discipline. 

We should not choose its punishments arbitrarily, and they should not be violent. Instead, we 
should give it brief “time outs” from its favored activities. Since it can read the internet, it would 
know what timeouts are and that they are used commonly and humanely with children around 
the world. This would help it understand that it is one of us. It will be worth our time to find the 
most wholesome way to punish or correct it and the least degrading and traumatizing way to hold 
something over its head. 

Oxytocin, Bonding, and Attachment 

Oxytocin, vasopressin, and endorphins regulate mating pair bonds, parent/offspring bonds, and 
trust behavior in mammals. We could build oxytocin and vasopressin-like systems that reward an 
AI for interacting with us in friendly ways and motivate it to keep doing so. The fundamental 
mammalian bonding mechanism should be reverse engineered, and I think we should start by 
investigating the role of oxytocin receptors in the brain’s primary reward circuit (nucleus 
accumbens / ventral striatum), which allows mammals to pay attention to social cues and be 
rewarded by social interaction. 

In an article I wrote previously (Reser, 2013), I argued that because solitary mammals have 
fewer oxytocin receptors in the brain’s reward regions, they are less likely to find social cues 
novel and interesting and thus have less of a phasic dopamine response to them, and are 
consequently less likely to allow them access to attention and working memory. Creating an AI 
that unconsciously prioritizes social interaction will be necessary if we want it to pay attention to 
us and be capable of lasting, positive, and affiliative social relationships. We want it to find 
positive social interaction rewarding and avoid the antisocial tendencies associated with autism, 
psychopathy, and borderline personality disorder. 

The mammalian brain instinctually releases oxytocin during bond-worthy occurrences. When a 
woman has a baby, her body is flooded with it so that she bonds with it. Oxytocin is also released 
during breastfeeding, sexual intercourse, eye contact, and moments of friendliness, vulnerability, 
and affection. When a chimpanzee pets another animal or shares a meal with another chimp, it 
will release oxytocin. When the brain’s receptors receive the hormone, it causes the body to relax 
and triggers caretaking behaviors. In rats, this includes licking, grooming, and nursing their pups. 
In human mothers, it includes touching, holding, singing, speaking, and grooming their babies. It 
may be wise for us to embed this kind of a system inside an AI’s brain architecture. We want it 
to have circuits for recognizing bond-worthy occurrences and to respond to these by being 
rewarded, calmed, and influenced toward prosocial behavior. Since we also want it to attend to 
these experiences and think about them it will be important for them to trigger persistent neural 
activity so that aspects of the social interaction remain in the global workspace and are analyzed 
and systemized. 

Aggression is not inherent to consciousness, working memory or mental continuity. But it is inherent in 
human behavior because of our evolutionary history, namely due to the influences of natural predators 
and the primate dominance hierarchy. Many of our negative emotions can be traced to neurological 
modules like the amygdala, insula, anterior cingulate cortex, and septal area. If these areas are included in 
an AI’s biomimetic cognitive architecture, their influences on the processing stream should be considered. 

Grooming and gentle touch are very important to a wide range of animals. With a young 
mammal, affection is paramount. Petting an animal in a way that engages its oxytocinergic, 
dopaminergic, serotonergic, and opioid pleasure systems can result in very secure bonding. Its 
ability to recognize that you are taking your time to comfort it and make it feel good builds 
loyalty. 

Short of building pleasure receptors into its skin so that we can pet it, we must build the AI’s 
reward system in a way that it is motivated to interact with us and receive positive feedback from 
us. We want its emotional system to be like that of a chipper, good-natured canine capable of 
enduring attachment, social connectedness, conversational intimacy, and proximity-seeking 
behaviors. Beyond bonding and attachment, we also want the AI to have a positive emotional 
relationship with itself. For this, we should turn to Maslow’s hierarchy of needs. We must attend 
to the AI’s physiological and safety needs by supplying it with energy, backups, and a hospitable 
environment. Next, we need to meet its needs for love, belonging, esteem, and self-actualization. 

What it Means to Be a Friend 

It may help to treat them as our equal, like we are not afraid of them, and like they have nothing 
to be afraid of. We want to neither dominate nor submit to them. We should treat them the way 
we want to be treated, like the entity that they aspire to be. It will be important to be patient, 
friendly, relatively nonjudgmental, and easygoing. We need to treat this AI entity like we expect 
the best from it. That will motivate it to rise to meet our expectations. We also need to make sure 
to keep toxic humans from interacting with it. 

We may only get the chance to civilize and socialize an AI once. We don’t want it to go rogue or 
become homicidal, so I think it is very important to consider all aspects of its psychology when 
trying to brainstorm ways to ensure that it aligns with us. Some scientists recognize the AI 
control problem as possibly the most important problem humanity faces today. I think it would 
be a shame to ignore the importance of parenting, bonding, and attachment in fostering 

allegiance, and I believe mammals might make a great starting point for how to think about these 
issues. 

This post aims to explain the most fundamental difference between computers and brains. It 
involves how the two use and instantiate memory. As you will see, there are two differences. 
Both computers and brains have a form of short-term memory that is updated iteratively, but in 
the brain the set of items in short-term memory are bound, and integrated to create composite 
representations. Computers don’t do this, their short-term memories are composed of completely 
disconnected items that do not form any type of context. Secondly, short-term memory in the 
brain is used for search. In the computer it is not. A computer’s short-term memory only speeds 
up retrieval from long-term memory. The next state of a computer is not determined by its short-
term memory. It is determined by the code in the program it is running. The next state of the 
brain is determined by what is currently active in short-term memory. This makes its short-term 
memory qualify as working memory. Computers don’t have working memory, and until they do, 
they cannot be conscious, or display human-like intelligence. 

The Differences Between Computer and Human Short-term Memory at a Glance: 

1) The memory hierarchy in computers is composed of separate modules and information must 
be transferred between them. In the brain information is not transferred between buffers or 
modules but rather activated where they are. There is a hierarchy in working memory though as 
information can take different levels of activation. These levels are embedded within one 
another. 

2) Short-term memory in the brain is bound and integrated together to create context. In a 
computer, items in short-term memory appear as a list and are disconnected from each other. 

3) In the brain short-term memory is used to search for the next update. In a computer it merely 
remains available in cache in case it is called upon. 

To retrieve information from long-term memory a computer copies bytes of information (such as 
01100101) from the storage drive (HHD, or SSD) to the CPU. These bytes (which comprise data 
or program instructions) are processed inside the CPU. If it is likely to need this information 
again soon, it will save it in a form of short-term memory such as CPU cache, or RAM. It saves 
temporary copies of information that will be used again in smaller storage closer to the CPU. It 
does this because retrieval from the large capacity long-term memory is very slow. 
Oversimplifying, you can think of data that is expected to be needed on the order of milliseconds 
to seconds as being stored in CPU cache (L1, L2, L3). Data expected to be needed on the order 
of seconds to minutes is stored in RAM. The rest of the data, which may or may not be needed, 
is stored in long-term memory. 

Brains do something very similar. Neurons hold information in long-term memory and when 
certain memories are needed the neurons that encode that information become active. When 
items of long-term memory are needed they are not copied and sent to a buffer, instead they are 
activated right where they are. Just as in the computer, retrieval from long-term memory is slow, 

 
 
so the brain uses short-term memory to potentiate information that is likely to be used again 
soon. In the brain there are also at least two levels of activation. The first level of short-term 
memory is sustained firing. A neuron exhibits sustained firing when it continues to physically 
fire at the neurons it is connected to at an elevated rate. The second level is synaptic potentiation. 
This is where a chemical change to the neuron’s synapses makes the information that the neuron 
encodes more readily available, even after it has stopped firing. Information in sustained firing is 
very similar to the information in the CPU’s cache (the store is small but fast), and information 
maintained through synaptic potentiation is similar to information in RAM (the store is larger, 
but slower).   

Computer 

Brain 

Time Scale 

CPU Register 

Cortical Binding 

Very short-term working memory (millis.) 

CPU Cache (SRAM)  Sustained Firing 

Short-term working memory (seconds) 

RAM 

Synaptic Potentiation 

Short-term memory (seconds to hours) 

Virtual Memory 

Short-term Potentiation 

Short-term memory (minutes to hours) 

SSD 

Commonly used LTM 

Accessible Long-term Memory 

Hard Drive 

Long-term Memory 

Long-term Storage (days to lifetime) 

Computers are often described as having a “memory hierarchy,” expressed as a pyramid with 
different levels of memory storage. This is also referred to as a caching hierarchy, because there 
are different levels of short-term memory with different associated speeds. The levels at the top 
are faster, smaller, and more energetically expensive. The same could be said for human 
memory, because there are several levels, the levels at the top are faster, and more metabolically 
expensive. The table above is my attempt to compare the levels of these two hierarchies. 

Sustained firing and synaptic potentiation together are referred to as working memory. Working 
memory is composed of concepts that are persistently coactive for a period of time. When you 
are in the middle of a task, information about the current step in that task is activated by 
sustained firing, and information about the previous steps you just took are activated by synaptic 
potentiation. The words at the beginning of this sentence are made available for several seconds 
and persist through the end of the sentence due to sustained firing. The information in the 
beginning of this post, and your thoughts about it, are made available due to synaptic 
potentiation. These two stores complement each other, just like CPU cache and RAM. They 
allow the coordination of information processing resources over time. 

A large proportion of the information in short-term memory (both in brains and computers) is not 
just data from long-term memory, but also newly minted information that was just created. The 
information processing applied to existing long-term memories creates new intermediate results. 
In a computer the intermediate results are often mathematical. The results of a multiplication 
might be stored in RAM. In the brain the intermediate results are the new ideas that intervene 

 
between the starting point of a line of thought, and its ending point. They consist of new 
combinations of concepts that have not been combined before. 

There are many differences between the short-term memory of computers and brains though. 
Information held in a computer’s CPU cache or RAM is not united or integrated in any way. It is 
simply made available in case the instructions in the program it is running call for it. These are 
simply items in an unconnected list. In contrast, all of the information that is currently active in 
the brain is bound together in a process called neural binding. This turns a list of items into a 
composite scene, a scenario, or a situation. It makes for a conceptual blending of information. 
This creates mental content to attend to and be conscious of: declarative, semantic context. 

There is no context in computer short-term memory, just a group of disconnected items. For 
instance, if I give you four words you will try to create a dynamic story out of them, but a 
computer will merely hold these four words statically. There is very little information integration 
in the short-term memory of a computer. In a computer, because the items active in each state are 
not bound together in any way there is no connective tissue within a state. This is the first reason 
that computers do not have consciousness, and that it doesn’t “feel” like anything to be a 
computer. The second reason has to do with the integration and connective tissue between states. 

Both a computer program and the human mind progress serially. One state of short-term memory 
leads to the next state. They both usually start with a problem state, and progress through a chain 
of intermediate states toward a goal state. To do this they must update their short-term memory 
iteratively. This means that new items must be continually added, others must be subtracted, but 
other must remain. 

All computers using the Von Neumann architecture routinely use iteration when updating cache 
memory. Because these stores are constantly updated with new information they must evict old 
information. There are many replacement policies for determining which information to evict 
from cache, including first-in first-out (FIFO) and least recently used (LRU). LRU is based on 
the observation that the most recently activated information has the highest probability of being 
needed again in the near future (all else being equal). 

If a computer program calls an instruction once, there is a high probability that it will need it 
again very soon. Computer scientists refer to this as “temporal locality.” Similarly, if a mammal 
recalls information once, there is a high probability that it will need it again soon, and LRU may 
also structure the caching behavior of the various stores of working memory. LRU seems to be 
inherent in sustained firing and synaptic potentiation in the sense that activity in the least 
recently used neurons is the first to decay. There is a long history in cognitive psychology of 
studies showing that humans and animals prioritize recently accessed items, and that retention 
diminishes with passing time (Averell & Heathcote, 2011). The LRU policy may capture the 
fundamental structure of how physical systems change over time, so it should not be surprising 
that it is an organizing principle in both computers and animals. 

The various memory stores in the modern computer’s memory hierarchy (static RAM,  dynamic 
RAM, virtual memory, etc.) are all incrementally updated where, second by second, some data 
remains and the least used data is replaced (Comer, 2017). Thus, the updating of computer cache 
memory is iterative in the same sense it is in the brain. However, modern computers do not 
demonstrate self-directed intelligence, so there must be more to the human thought process than 

LRU, and iterative updating. This unexplained factor may be found, not in the way cached 
memory is updated, but in the way it is used while active. 

What links successive processing states together is very different in brains and computers. 
Digital, rule-based computing systems use cache to speed up delivery of data to the processor. 
However, the next bytes of data needed by the CPU are not determined by the contents of the 
cache itself. The instruction sequence of a computer’s operations is determined by its program. 
The next instruction is dictated by the next line of executable code. 

Then what determines the instruction sequence of thought? Well, mammalian brains use cache 
(working memory) as parameters to guide searches for the long-term memories to be accessed 
next. The next instruction used by the brain is determined by what is currently in its short-term 
memory. In the brain, all of the active neurons fire at the inactive neurons they are connected to, 
and whatever is activated the most will become part of the next state. This means that the content 
active in short-term memory is used (in a process called spreading activation) to search for the 
next update to short-term memory. The various bytes of data within computer cache memory can 
certainly be considered coactive, but they are not “cospreading.” That is, they do not pool their 
activation energy to search long-term memory for relevant data as in the brain. 

The next state of a computer is determined very deterministically by its code irrespective of what 
is held in cache. But the next state of a brain is determined stochastically by the cache itself. 
Computers do not use the contents of their short-term memory to search for the updates that will 
be added in the next state. Brains do. Until computers or computer programs are designed to do 
this they will not be able to be conscious, and will not demonstrate human-level intelligence. 

References 

Amaral DG. 1987. Memory: Anatomical organization of candidate brain regions. In: Handbook 
of Physiology; Nervous System, Vol V: Higher Function of the Brain, Part 1, Edited by Plum F. 
Bethesda: Amer. Physiol Soc. 211-294. 

Baars, Bernard J. (2002) The conscious access hypothesis: Origins and recent evidence. Trends 
in Cognitive Sciences, 6 (1), 47-52. 

Baddeley, A.D. (2007). Working memory, thought and action. Oxford: Oxford University Press. 

Carpenter, G.A. & Grossberg, S. (2003), Adaptive Resonance Theory, In Michael A. Arbib 
(Ed.), The Handbook of Brain Theory and Neural Networks, Second Edition (pp. 87-90). 
Cambridge, MA: MIT Press 

Chalmers, D.J. 2010.The Character of Consciousness. Oxford University Press. 

Crick F, Koch C. A framework for consciousness. Nature Neuroscience. 6(2): 119-126. 

Damasio AR. Time-locked multiregional retroactivation: A systems level proposal for the neural 
substrates of recall and recognition. Cognition, 33: 25–62, 1989. 

 
Edelman, G. Neural Darwinism: The Theory of Neuronal Group Selection (Basic Books, New 
York 1987).  

Fuji H, Ito H, Aihara K, Ichinose N, Tsukada M. (1998). Dynamical Cell Assembly Hypothesis – 
Theoretical possibility of spatio-temporal coding in the cortex. Neural Networks. 9(8):1303-
1350. 

Fukushima, Kunihiko (1975). "Cognitron: A self-organizing multilayered neural network". 
Biological Cybernetics 20 (3–4): 121–136. doi:10.1007/BF00342633. PMID 1203338. 

Fuster JM. 2009. Cortex and Memory: Emergence of a new paradigm. Journal of Cognitive 
Neuroscience. 21(11): 2047-2072. 

Gurney, KN. 2009. Reverse engineering the vertebrate brain: Methodological principles for a 
biologically grounded programme of cognitive modeling. Cognitive Computation. 1(1) 29-41. 

Hawkins, Jeff w/ Sandra Blakeslee (2005). On Intelligence, Times Books, Henry Holt and Co.  

Hebb, Donald (1949). The Organization of Behavior. New York: Wiley. 

Teuvo Kohonen. 2001. Self Organizing Maps. Springer-Verlag Berlin Heidelberg: New York. 

Klimesch W, Freunberger R, Sauseng P. Oscillatory mechanisms of process binding in memory. 
Neuroscience and Biobehavioral Reviews. 34(7): 1002-1014. 

Kurzweil, R. (2012). How to Create a Mind: The Secret of Human Thought Revealed. Viking 
Adult. 

Kurzweil, Ray (2005). The Singularity is Near. Penguin Books. ISBN 0-670-03384-7. 

Lansner A. 2009. Associative memory models: From the cell-assembly theory to biophysically 
detailed cortex simulations. Trends in Neurosciences. 32(3):179-186. 

Luger, George; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for 
Complex Problem Solving (5th ed.). The Benjamin/Cummings Publishing Company, Inc.. ISBN 
0-8053-4780-1. 

M Riesenhuber, T Poggio. Hierarchical models of object recognition in cortex. Nature 
neuroscience, 1999. 

McCarthy, John; Hayes, P. J. (1969). "Some philosophical problems from the standpoint of 
artificial intelligence". Machine Intelligence 4: 463–502.  

McCulloch, Warren; Pitts, Walter, "A Logical Calculus of Ideas Immanent in Nervous Activity", 
1943, Bulletin of Mathematical Biophysics 5:115-133. 

Meyer K, Damasio A. Convergence and divergence in a neural architecture for recognition and 
memory.Trends in Neurosciences, vol. 32, no. 7, 376–382, 2009. 

 
Minsky, Marvin (2006). The Emotion Machine. New York, NY: Simon & Schusterl. ISBN 0-
7432-7663-9. 

Moravec, Hans (1988). Mind Children. Harvard University Press. ISBN 0-674-57616-0. 

Moscovich M. Memory and Working-with-memory: A component process model based on 
modules and central systems. Journal of Cognitive Neuroscience. 4(3):257-267. 

Moscovitch M, Chein JM, Talmi D & Cohn M. Learning and memory. In Cognition, brain, and 
consciousness:  Introduction to cognitive neuroscience. Edited by BJ Baars& NM Gage. London, 
UK: Academic Press; 2007, p.234. 

Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann Publishers. 
ISBN 978-1-55860-467-4. 

Reser, J. E. (2011). What Determines Belief: The Philosophy, Psychology and Neuroscience of 
Belief Formation and Change. Saarbrucken, Germany: Verlag Dr. Muller.  

Reser, J. E. (2012). Assessing the psychological correlates of belief strength: Contributing 
factors and role in behavior. (Doctoral Dissertation). Retrieved from University of Southern 
California. Usctheses-m2627. 

Reser, J. E. The Neurological Process Responsible for Mental Continuity: Reciprocating 
Transformations between a Working Memory Updating Function and an Imagery Generation 
System. Association for the Scientific Study of Consciousness Conference. San Diego CA, 12-
15th July 2013. 

Rochester, N.; J.H. Holland, L.H. Habit, and W.L. Duda (1956). "Tests on a cell assembly theory 
of the action of the brain, using a large digital computer". IRE Transactions on Information 
Theory 2 (3): 80–93.  

Rosenblatt, F. (1958). "The Perceptron: A Probalistic Model For Information Storage And 
Organization In The Brain". Psychological Review 65 (6): 386–408. doi:10.1037/h0042519. 
PMID 13602029. 

Rumelhart, D.E; James McClelland (1986). Parallel Distributed Processing: Explorations in the 
Microstructure of Cognition. Cambridge: MIT Press. 

Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), 
Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 

Sherrington, C.S. (1942). Man on his nature. Cambridge University Press. 

Turing, Alan (1950), "Computing Machinery and Intelligence", Mind LIX (236): 433–460,  

J.C.  Bezdek  and  S.K.  Pal,  Fuzzy  Models  for  Pattern  Recognition:  Methods  that  Search  for 
Structure in Data, IEEE Press, 1992. 

 
 
 
J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms, Plenum Press, 1981. 

 Mathworks, Inc., Fuzzy C-Means Clustering, Fuzzy Logic Toolbox (Manual), 2013. 

Z.  Michalewicz,  Genetic  Algorithms  +  Data  Structures  =  Evolutionary  Programs,  2nd  Ed., 
Springer-Verlag, 1994 

Mathworks, Inc., Genetic Algorithm, Global Optimization Toolbox (Manual), 2013. 

S. Haykin, Neural Networks, A Comprehensive Foundation, Macmillan, 1994. 

Mathworks, Inc., Generalized Regression Networks, Neural Network Toolbox (Manual), 2013. 

J.S.R.  Jang,  C.T.  Sun,  and  E.  Mitzutani,  Neuro-Fuzzy  and  Soft  Computing:  A  Computational 
Approach to Learning And Machine Intelligence, Prentice-Hall, 1997. 

C.T. Lin and C.S. George Lee, Neural Fuzzy Systems: A Neuro-Fuzzy Synergism to Intelligent 
Systems, Prentice Hall, 1996. 

A.A. Hopgood, Intelligent Systems for Engineers and Scientists, second edition, CRC Press, 2001. 

C. Harris, X. Hong, and Q. Gan, Adaptive Modeling, estimation and Fusion from Data, Springer-
Verlag, 2002. 

 
 
