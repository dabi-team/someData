2
2
0
2

g
u
A
4
2

]

G
L
.
s
c
[

1
v
0
8
5
1
1
.
8
0
2
2
:
v
i
X
r
a

OPTIMAL BRAIN COMPRESSION: A FRAMEWORK FOR
ACCURATE POST-TRAINING QUANTIZATION AND PRUNING

A PREPRINT

Elias Frantar
IST Austria
elias.frantar@ist.ac.at

Dan Alistarh
IST Austria & Neural Magic
dan.alistarh@ist.ac.at

August 25, 2022

ABSTRACT

We consider the problem of model compression for deep neural networks (DNNs) in the challenging
post-training setting, in which we are given an accurate trained model, and must compress it without
any retraining, based only on a small amount of calibration input data. This problem has become
popular in view of the emerging software and hardware support for executing models compressed
via pruning and/or quantization with speedup, and well-performing solutions have been proposed
independently for both compression approaches. In this paper, we introduce a new compression
framework which covers both weight pruning and quantization in a uniﬁed setting, is time- and
space-efﬁcient, and considerably improves upon the practical performance of existing post-training
methods. At the technical level, our approach is based on the ﬁrst exact and efﬁcient realization of the
classical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla, 1990] at the scale
of modern DNNs, which we further extend to cover weight quantization. This is enabled by a series of
algorithmic developments which may be of independent interest. From the practical perspective, our
experimental results show that it can improve signiﬁcantly upon the compression-accuracy trade-offs
of existing post-training methods, and that it can even enable the accurate joint application of both
pruning and quantization in a post-training setting.

1

Introduction

The impressive recent progress of deep learning for solving challenging tasks across several domains has been
accompanied by a signiﬁcant increase in parameter counts and computational costs for executing such models. A
natural consequence has been a growing effort to reduce such costs via model compression, and the two most popular
approaches for model compression are pruning—removing neural network weights by setting them to zero—and
quantization, reducing the precision at which neural network weights and activations are stored and manipulated.
Hundreds of such pruning and quantization approaches have been proposed and analyzed [17, 11], with the general
goal of obtaining efﬁcient deep neural nets (DNNs) which would preserve accuracy while maximizing compression.
Despite impressive progress, compression is still a laborious process, as the pruning and quantization stages are often
done independently, and recovering model accuracy after compression often requires partial or even full retraining of
the compressed model.

By contrast, an alternative but challenging scenario is the post-training compression setup [31, 24, 19, 25], in which we
are given a trained but uncompressed model, together with a very small amount of calibration data, and must produce
an accurate compressed model in one shot, i.e. a single compression step, without any additional retraining. This is the
setting we focus on in this paper.

Compression via weight pruning started with seminal work by LeCun et al. [23], complemented by Hassibi and
Stork [13], who proposed a mathematical framework called the Optimal Brain Surgeon (OBS), for choosing the set of
weights to remove from a trained neural network, by leveraging second-order information. (We describe their approach
in Section 3.) Recent advances, e.g. [6, 39, 38, 10] showed that variants of the OBS pruning framework can be scaled
to modern DNNs. Variants of this approach achieve state-of-the-art accuracy for gradual pruning of CNNs [38] or

 
 
 
 
 
 
OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

even large language models [21]. However, these approaches have not been applied to the post-training setting, as they
require gradual pruning, as well as retraining, to recover good accuracy.

The standard in the context of post-training compression, has been to break the task into layer-wise sub-problems,
essentially ﬁnding a compressed weight approximation for each layer, given a sub-sample of the layer’s inputs and
outputs obtained from calibration data. This approach is adopted by virtually the entire line of work on one-shot
post-training quantization, e.g. [41, 31, 19], which then introduces elegant solvers for the resulting layer-wise weight
quantization problem. Recently, AdaPrune [18] showed that this approach can also be effective for post-training weight
pruning.

In this context, a natural question is whether existing approaches for pruning and quantization can be uniﬁed in order to
cover both types of compression in the post-training setting, thus making DNN compression simpler and, hopefully,
more accurate. This question is also of practical importance, since both GPU and CPU platforms now jointly support
sparse and quantized formats [30, 35], and, as we illustrate experimentally, the resulting models could be executed with
compound speedups.

Contribution.
In this paper, we provide a mathematical framework for compression via pruning or quantization, which
leads to state-of-the-art accuracy-versus-compression trade-offs in the challenging post-training compression setup.
Our framework starts from the layer-wise compression problem described above, by which the global compression task,
deﬁned either for pruning or quantization, is ﬁrst split into layer-wise sub-problems, based on the layer behavior on the
calibration data. Speciﬁcally, given a layer (cid:96) deﬁned by weights W(cid:96), and layer inputs X(cid:96), we aim to ﬁnd a compressed
version of the weights (cid:99)W(cid:96) which minimizes the output difference relative to the uncompressed layer, measured via
the squared error between the original and compressed layer, acting on the sample input ||W(cid:96)X(cid:96) − (cid:99)W(cid:96)X(cid:96)||2
2, under a
ﬁxed compression constraint on (cid:99)W(cid:96).
Although solving this problem optimally for sparsity or quantization constraints is NP-hard [2, 31], it is a key step in all
state-of-the-art post-training compression methods, both for pruning [18, 9] and for quantization [31, 18, 24]. Once
this is solved per layer, a solution to the global problem can be obtained by combining layer-wise solutions, which is
handy especially for non-uniform compression, e.g. [15, 9]. Thus, several approximations for this problem have been
proposed [31, 19, 18].

We show that there still is signiﬁcant room for improvement when solving the layer-wise compression problem. Roughly,
our approach is to specialize the OBS framework to the squared error formulation above: in this case, the framework
can in theory produce an exact greedy solution, but a direct implementation would have infeasible Θ(d4) computational
cost, where d is the layer dimension. Our main technical contribution is a series of algorithms which reduce this
computational cost, without any approximations, to O(d · d2
col) where dcol is the column dimension of the weight
matrix. In practice, these improvements are signiﬁcant enough to allow us to implement the exact OBS greedy solution,
which prunes one weight at a time, and updates all remaining weights after each step, at the scale of modern DNNs
with tens of millions of parameters, within reasonable time, on a single GPU. We plan to publish an efﬁcient PyTorch
implementation of our algorithms in the future.

In turn, this algorithmic development allows us to apply the OBS approach to quantization. The resulting algorithm,
called the Optimal Brain Quantizer (OBQ), quantizes weights iteratively one-at-a-time, depending on their impact on
the loss increase, after which it applies a closed-form update to the remaining unquantized weights, further reducing the
loss. This solves the two problems efﬁciently, and in a uniﬁed manner—we call the uniﬁed framework the Optimal
Brain Compressor (OBC).

Experimental Results. We apply OBC to standard tasks and models covering image classiﬁcation, object detection,
and language modelling applications. We ﬁrst show that our framework yields signiﬁcantly better solutions for the
layer-wise compression problem, which leads to higher-accuracy end-to-end compressed models for both pruning and
quantization, relative to the corresponding state-of-the-art techniques, often by signiﬁcant margins. Second, we show
that our pruning and quantization approaches can be compounded, with surprisingly strong results: we obtain a 12×
reduction in theoretical operations with a 2% accuracy drop for GPU-supported compound compression [30], and a
4× speedup in actual runtime with only 1% accuracy drop for a CPU-based runtime which supports both sparsity and
quantization [35]. These results show for the ﬁrst time that post-training compression can be competitive with full
retraining in terms of the accuracy-speedup trade-off.

2

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

2 Related Work

Optimal Brain Surgeon (OBS). The classic OBS framework [23, 13] was ﬁrst introduced in the 90s, and originally
applied to networks with hundreds of weights. More recently, methods such as WoodFisher [38] rendered the approach
computationally feasible for DNNs by using a block-diagonal Fisher approximation of the Hessian, while a follow-up
method called M-FAC [10] introduced more efﬁcient and general algorithms for handling the inverse Fisher matrix.
Recent work [21] extended this approach to pruning groups of weights, and to large language models (LLMs), for
which it proposes a signiﬁcantly-improved implementation.

Earlier work, called Layer-wise OBS (L-OBS) [6], was inspired by the K-FAC approximation [29, 12] and approximates
the OBS framework not for the global objective, but for a quadratic per-layer loss, while also pruning all weights based
on a single Hessian computation. At a high level, our approach is similar, in that we apply OBS layer-wise; however,
we apply OBS exactly, that is, pruning one weight at a time, and exactly recomputing the Hessian of the remaining
weights after every pruning step. This is made computationally tractable by several new algorithmic ideas, and yields
signiﬁcantly improved results relative to L-OBS. All this prior work on pruning via second-order approximations
considered settings with extensive ﬁnetuning. By contrast, we will focus on the post-training setting, where only a small
amount of calibration data is available.

Post-Training Quantization. This setting has been primarily considered for quantization, and most state-of-the-art
methods work by performing layer-wise compression. Speciﬁcally, BitSplit [6] optimizes the quantized weights bit by
bit, while AdaRound [31] ﬁnds a weight rounding policy through gradient based optimization with an annealed penalty
term that encourages weights to move towards points on the quantization grid. AdaQuant [19] relaxes the AdaRound
constraint, allowing weights to change during quantization-aware optimization, via straight-through approximation [33].
BRECQ [24] suggested that accuracy can be improved further by integrating second-order information into the
layer-wise losses and by jointly optimizing hand-crafted blocks of related layers.

A key step of AdaRound, AdaQuant and BRECQ is to quantize layers incrementally, in sequential order, so that
errors accumulated in earlier layers can be compensated by weight adjustments in later ones. This signiﬁcantly
improves performance, but reduces ﬂexibility, as the entire process may need to be re-done whenever we wish to change
compression parameters of one layer. We instead target independent compression of each layer, allowing the end
model to be simply “stitched” together from layer-wise results. Despite operating independently on each layer, we ﬁnd
that, after correcting basic statistics such as batchnorm, our method performs on par to sequential ones for uniform
quantization.

Post-Training Sparsiﬁcation. The layer-wise approach was shown to also be effective for post-training pruning
by AdaPrune [18], which pruned weights to the GPU-supported N:M pattern [44]. AdaPrune ﬁrst drops parameters
according to their magnitude [45] and then reoptimizes the remaining weights to reconstruct the pre-compression
calibration set output. This is similar to [16, 8] which also perform layer-wise reoptimization of the remaining
weights. Follow-up work [10] noted that the results of AdaPrune can be improved further by performing more frequent
pruning/optimization steps. Our algorithm pushes this idea to the limit, performing full reoptimization after every single
pruned weight, while remaining computationally tractable. We further use a more sophisticated weight selection metric
which incorporates second-order information. Finally, [10] also introduces global AdaPrune, a more expensive global
optimization step applied on top of the layer-wise AdaPrune results, which can bring additional accuracy gains. This
can also be applied to our pruned models.

Non-Uniform Compression. An orthogonal practical question is how to compress different layers to maximize
accuracy under a given resource constraint, such as latency or energy consumption. Existing methods can be roughly
categorized into search-based and solver-based approaches. The former, e.g. AMC [15] or HAQ [40], search for a
layer-wise compression policy directly via, for example, reinforcement learning or genetic programming [42], whereas
the latter, e.g. HAWQv3 [43] or AdaQuant [19], construct a relaxed version of the overall problem that is then solved
exactly. We focus here on solver-based approaches, as they can rapidly adapt to different scenarios when combined with
accurate independent layer-wise compression schemes; however, our techniques could be of interest for search-based
methods as well. Concretely, we use the problem formulation of AdaQuant [19] to which we apply the DP algorithm of
SPDY [10] to achieve fast solving times even with a large number of possible choices per layer.

3 Problem Deﬁnition and Background

The Layerwise Compression Problem. We begin by deﬁning the general problem that is the focus of this work, in
line with prior work on post-training compression, e.g. [31, 19]. Mathematically, we model a layer (cid:96) as a function

3

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

f(cid:96)(X(cid:96), W(cid:96)) acting on inputs X(cid:96) which is parametrized by weights W(cid:96). The goal of layer-wise compression is to
ﬁnd a “compressed” version of W(cid:96) that performs as similarly as possible to the original weights. More formally, the
compressed weights (cid:99)W(cid:96) should minimize the expected layer output change as measured by some loss L while at the
same time satisfying a certain compression constraint, which we generically denote by C((cid:99)W(cid:96)) > C here, but which will
be customized depending on the compression type:

argmin

(cid:99)W(cid:96)

EX(cid:96) L(f(cid:96)(X(cid:96), W(cid:96)), f(cid:96)(X(cid:96), (cid:99)W(cid:96)))

subject to C((cid:99)W(cid:96)) > C.

(1)

The expectation over the layer inputs X(cid:96) is usually approximated by taking the mean over a small set of N input
samples. This low-data setting is one of the primary applications of layer-wise compression. Further, most works
[41, 31, 19] focus on compressing linear and convolutional layers, which can be unfolded into linear ones, as these are
prevalent in practice, and use the squared loss to measure the approximation error. This deﬁnition of the loss can be
motivated, via a sequence of approximations, from second-order information: please see [31] for a precise derivation.
Furthermore, this approximation approach has been shown to work well in many applications [31, 19, 9].

We follow these conventions as well, and work with the speciﬁc layer-wise compression problem stated formally below,
where the weights W(cid:96) are a drow × dcol matrix (for conv-layers dcol corresponds to the total number of weights in a
single ﬁlter), and the input X(cid:96) has dimensions dcol × N .

argmin

(cid:99)W(cid:96)

||W(cid:96)X(cid:96) − (cid:99)W(cid:96)X(cid:96)||2
2

s.t. C( (cid:99)W(cid:96)) > C.

(2)

The Optimal Brain Surgeon (OBS) Framework. The OBS framework [23, 13] considers the problem of accurately
pruning a trained dense neural network. It starts from the Taylor approximation at the given point (assumed to have
negligible gradient), and provides explicit formulas for the optimal single weight to remove, as well as the optimal
update of the remaining weights which would compensate for the removal. More precisely, let H denote the Hessian
matrix of the loss at the given (dense) model. Then the weight to prune wp which incurs the minimal increase in loss
and the corresponding update of the remaining weights δp can be calculated as follows:

wp
[H−1]pp
where [H−1]pp denotes the pth diagonal entry of the inverse Hessian, and H−1

wp = argminwp

δp = −

w2
p
[H−1]pp

,

· H−1
:,p ,

:,p is its pth column.

(3)

OBS for Layer-Wise Pruning. We will now instantiate this framework for the layer-wise pruning problem, deﬁned
above. First, the loss in equation (2) is quadratic and since our starting point is given by the dense weights achieving
the minimal loss of 0, the assumptions of the OBS framework are fully met, meaning that its formulas are exact for
this speciﬁc problem formulation. Thus, iterating the OBS framework to remove one weight at a time would yield an
exact greedy solution for the layer-wise pruning problem, as it takes the (locally) optimal decision at each step. While
this greedy approach does not guarantee convergence to a global optimum, such approaches can be very effective for
dealing with problem instances that are too large to be handled by exact methods.

4 An Optimal Greedy Solver for Sparsity

The obvious challenge is that applying the OBS framework in its true form, i.e. pruning a single weight at a time using
the exact formulas in (3), is computationally very demanding. The Hessian H is a d × d matrix where d = drow · dcol,
which is already expensive to store and compute with. Additionally, this matrix needs to be updated and inverted at
each of the O(d) steps with a computational complexity of Θ(d3). Clearly, an O(d4) total runtime is too inefﬁcient for
pruning most layers of modern neural networks, as d is usually ≥ 105 or even ≥ 106 for several layers. However, as
we will now show, it is actually possible to reduce the overall costs of this process to O(drow · d3
col)
memory, making it efﬁcient enough to prune e.g. all layers of a medium-sized model such as ResNet50 in a bit more
than one hour on a single NVIDIA RTX 3090 GPU. We emphasize that the techniques we introduce are exact; unlike
prior work [6, 38], we do not rely on any approximations.

col) time and Θ(d2

The ExactOBS Algorithm.
In the following, we introduce our efﬁcient instantiation of the OBS framework, for the
layer-wise compression problem, which we call ExactOBS, in step-by-step fashion. We start by rewriting the matrix
squared error in (2) as the sum of the squared errors for each row in the weight matrix. As we are always dealing with a
ﬁxed layer (cid:96), we drop the subscript (cid:96) to simplify notation. The objective is then equivalent to (cid:80)drow
i=1 ||Wi,:X− (cid:99)Wi,:X||2
2.
This way of writing the error makes it clear that removing a single weight [W]ij only affects the error of the
corresponding output row Yi,: = Wi,:X. Hence, there is no Hessian interaction between different rows and so it

4

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

sufﬁces to work only with the individual dcol × dcol Hessians corresponding to each of the drow rows. Further, as the
dense layer output Y = WX is ﬁxed, the objective for each row has standard least squares form and its Hessian is
given by H = 2XX(cid:62).

Although this observation already reduces the computational complexity, two key challenges remain: (a) applying OBS
to each row still costs O(dcol · d3
col) time, which is too slow for large layers, and (b) we need fast access to the Hessian
inverses of all drow rows, since we want to prune the minimum score weight of the whole matrix rather than just per row
in each step. In particular, (b) requires O(drow · d2

col) GPU memory, which is likely to be infeasible.

col) calculation and Θ(d3

Step 1: Handling a Single Row. We ﬁrst describe how to efﬁciently prune weights from a single row with dcol
parameters. For simplicity, we denote such a row by w with corresponding Hessian H. The full algorithm for this
procedure is given in Algorithm 1; in the following, we provide a detailed description. The key idea is to avoid having
to do the full Θ(N · d2
col) inversion of H in each step. The former is easy, as the weights
themselves do not enter the calculation of H = 2XX(cid:62), and the Hessian for the weights with pruning mask M denoted
by HM is thus simply comprised of the corresponding rows and columns in the fully dense version H. Hence, we
only have to compute H (which is actually the same for all rows) once, from which we can then extract the rows and
columns corresponding to M as needed.
Critically, this trick is not applicable to the inverse, as (HM )−1 (cid:54)= (H−1)M . However, using the fact that the removal
of one parameter p simply drops the corresponding row and column from H, we can actually update the inverse to
remove parameter p directly using a single step of Gaussian elimination, with cost Θ(d2
col). The following result, whose
proof is in the Appendix, formalizes this.
Lemma 1 (Row & Column Removal). Given an invertible dcol×dcol matrix H and its inverse H−1, we want to efﬁciently
compute the inverse of H with row and column p removed, which we denote by H−p. This can be accomplished through
the following formula:

H−1

−p =

(cid:16)

H−1 −

1
[H−1]pp

H−1

:,p H−1
p,:

(cid:17)

,

−p

(4)

which corresponds to performing Gaussian elimination of row and column p in H−1 followed by dropping them
completely. This has Θ(d2

col) time complexity.

The resulting pseudocode is shown in Algorithm 1, where we avoid constantly resizing H−1 (and correspondingly
changing indices) by utilizing the fact that row and column p have no effect on any future calculations after they have
been eliminated by Lemma 1 as they are 0 (and the non-zero diagonal element is never accessed again). One can check
that this algorithm applies OBS to a single row of W with a per-step cost of Θ(d2
col) overall time
for pruning k weights.

col), and thus Θ(k · d2

Algorithm 1 Prune k ≤ dcol weights from row w with
inverse Hessian H−1 = (2XX(cid:62))−1 according to OBS
in O(k · d2

col) time.

M = {1, . . . , dcol}
for i = 1, . . . , k do
p ← argminp∈M
w ← w − H−1
:,p
H−1 ← H−1 −
M ← M − {p}

1
[H−1]pp
1
[H−1]pp
1
[H−1]pp

end for

· w2
p
· wp
H−1

:,p H−1
p,:

Figure 1: Efﬁcient global OBS using the row-wise re-
sults.

Step 2: Jointly Considering All Rows. Applying the OBS framework to the full weight matrix W rather than just
to each row independently requires fast access to all drow row-wise inverse Hessians, in order to select the weight
with the smallest overall pruning score in each step. However, storing drow matrices of size dcol × dcol each in GPU
memory can be too expensive; while it would be possible to ofﬂoad some Hessians to main memory, this could result in
a large number of expensive memory transfers. However, since there is no Hessian interaction between rows, the ﬁnal
compressed weights of each row only depend on the total number of parameters that were pruned in it. Similarly, the
change in loss incurred by pruning some weight only depends on the previously pruned weights in the same row, which
also means that the order in which weights are pruned in each row is ﬁxed.

5

DenseWeightWeightRowsPruning TracesLoss Changes GlobalMask PrunedWeight Less memory: resolve not pruned weightsLess compute: load stored trace elementsOBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

The consequence of these insights is that we can process each row independently, pruning all weights in order while
always recording the corresponding change in loss δLp = w2
p/[H−1]pp. At the end, we know δLp for all d weights and
can then simply determine the global mask that would be chosen by OBS on the full matrix by selecting the weights
with the lowest values in order, requiring only Θ(d) extra memory. We note that once the per-row masks Mi are known,
we can directly solve for the optimal update of the remaining weights via the corresponding group OBS formula [21]
δ−Mi = H−1
((H−1)−Mi)−1w−Mi where the index set −Mi denotes all indices not in mask Mi. This will be
considerably faster in practice than simply rerunning the iterative pruning process in Algorithm 1. Alternatively, if
enough CPU memory is available, one can keep the full pruning trace of each row, that is, the full weight vector after
every individual pruning step, in CPU memory and ultimately simply reload the entries corresponding to the global
mask. This requires O(drow · d2
col) extra CPU memory but avoids a second computation pass to reconstruct the not
pruned weights and will therefore be faster. Figure 1 visualizes both options just discussed.

:,−Mi

In practice, the matrix H might not always be invertible for reasons such as using too few
Implementation Details.
data samples or dead / linearly dependent inputs. The former can usually be addressed by extending the calibration
dataset with augmentations (additional augmented samples only need to be accumulated into the Hessian once and are
thus very cheap to include) and the latter can be prevented by adding a small diagonal dampening term to the Hessian
before inverting it. Second, a direct GPU implementation of Algorithm 1 will perform a large number of small CUDA
calls, which can be expensive. This overhead can be removed by using batch operations to process multiple matrix rows
simultaneously. Finally, when applied to an already sparse weight matrix, the complexity of our algorithm can scale
cubicly with the row-density by working with a dense version of the weights / Hessians consisting only of the non-zero
elements and mapping the pruning result back at the end.

N:M Sparsity. Our method can be easily extended to various forms of semi-structured sparsity. This includes, for
example, the N:M sparsity pattern [44], which enforces exactly N non-zero values in each block of M consecutive
weights, and is becoming popular due to support on newer NVIDIA hardware [30]. Adapting our algorithm to this
pattern requires only one simple change: instead of selecting the weight with the smallest change in loss, we select the
weight with the smallest change in loss that is in a block with < N pruned weights. We note that all rows have exactly
the same sparsity 1 − N/M in the N:M pattern and so we can terminate per-row pruning as soon as this target sparsity
value is reached. For the same reason, there is no need for the global mask selection step described earlier. Thus, our
method will be even more efﬁcient in this case.

Block-Sparsity. Another practically relevant pruning pattern, particularly in the context of CPU acceleration [7, 22],
is block-pruning, where zeros appear only in consecutive blocks of size c, which is typically a small number like 4
or 8. We follow recent work [21] that extends the OBS framework to pruning small groups of connected weights in
order to account for the correlation between them, using the following formulas for the target block and weight update,
respectively:

wP = argminwP

w(cid:62)

P ((H−1)P )−1wP ,

δP = −H−1

:,P ((H−1)P )−1wP ,

(5)

where P denotes the set of indices corresponding to one block. Algorithm 1 can easily be adapted to operate on
blocks using the above equations and applying the update of H−1 via Lemma 1 successively for all p ∈ P . Although
there are now only dcol/c steps per row, each update of H−1 also takes O(c · d2
col) time and so the overall asymptotic
runtime stays the same. Additional practical overhead only comes from the extra O(c2 · d2
col) terms that are the result of
computing and multiplying with the c × c matrices ((H−1)P )−1.

5 The Optimal Brain Quantizer (OBQ)

Although the classical OBS framework [23, 13] has inspired a long line of work on pruning methods for DNNs [38, 10,
27], so far it has not been used for quantization. We now show that our results from the previous section can in fact be
extended to quantization in an effective and accurate way, via a new method which we call the Optimal Brain Quantizer
(OBQ).

The Quantization Order and Update Derivations. Under the standard assumption that the gradient at the current
point w is negligible, the OBS formulas for the optimal weight to be pruned wp and the corresponding update δp can
be derived by writing the locally quadratic problem under the constraint that element p of δp is equal to −wp, which
means that wp is zero after applying the update to w. This problem has the following Lagrangian:

L(δp, λ) = δ(cid:62)

p Hδp + λ(e(cid:62)

p δp − (−wp)),

(6)

6

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

where H denotes the Hessian at w and ep is the pth canonical basis vector. The optimal solution is then derived by ﬁrst
ﬁnding the optimal solution to δp via setting the derivative ∂L/∂δp to zero and then substituting this solution back into
L and solving for λ; please see e.g. [13, 38] for more details.

Assume a setting in which we are looking to quantize the weights in a layer on a ﬁxed grid of width ∆ while
minimizing the loss. To map OBS to a quantized projection, we can set the target of the Lagrangian constraint in (6)
to (quant(wp) − wp), where quant(wp) is the weight rounding given by quantization; then wp = quant(wp) after the
update.

Assuming we wish to quantize weights iteratively, one-at-a-time, we can derive formulas for the “optimal” weight to
quantize at a step, in terms of minimizing the loss increase, and for the corresponding optimal update to the unquantized
weights, in similar fashion as discussed above:

wp = argminwp

(quant(wp) − wp)2
[H−1]pp

,

δp = −

wp − quant(wp)
[H−1]pp

· H−1
:,p .

(7)

In fact, since −wp is a constant during all derivations, we can just substitute it with (quant(wp) − wp) in the ﬁnal result.
We note that the resulting formulas are a generalization of standard OBS for pruning, if quant(·) always “quantizes” a
weight to 0, then we recover the original form.

Quantizing Full Layers. At ﬁrst glance, OBQ might appear curious since one usually quantizes all weights in a
layer, leaving no more weights to update. At the same time, the weight selection metric inﬂuences only the quantization
order, but not the quantization value. However, this view changes when considering OBQ in the context of our efﬁcient
one-weight-at-a-time pruning algorithm described in the previous section. Speciﬁcally, using OBQ, we can greedily
quantize the currently “easiest” weight by the above metric, and then adjust all the remaining unquantized weights to
compensate for this loss of precision, thus changing their value. We then choose the next weight to quantize, and so on.
This can result in quantization assignments that are different from the ones that would have been chosen by rounding
initially, and in better overall quantization results. Concretely, to realize this, we can plug (7) into Algorithm 1 in order
to iteratively quantize weights for a given layer, leading to the similar Algorithm in the Appendix, thus essentially
unifying pruning and quantization.

Quantization Outliers. One practical issue with this greedy scheme can occur especially when applied to quantization
grids that permit some outliers in order to achieve a lower error on the majority of weights, which are currently standard
[4, 34]. Since these outliers can have high quantization error, they will usually be quantized last, when there are only
few other unquantized weights available that may be adjusted to compensate for the large error incurred by quantizing
the outliers. This effect can become worse when some weights are pushed even further outside the grid by intermediate
updates. We prevent this with a simple but effective heuristic: we quantize outliers, e.g. weights with a quantization
error > ∆/2 where ∆ is the distance between quantized values, as soon as they appear (which typically happens
only a few times per layer). With this heuristic, OBQ yields a highly effective layer-wise quantization scheme, as our
experiments in the next section demonstrate. Finally, we note that the OBQ version of the techniques discussed in
Section 4 has all the same runtime and memory characteristics (barring the global step in Figure 1, which is unnecessary
for quantization).

6 Experiments

Objectives, Models & Datasets. To demonstrate the effectiveness and ﬂexibility of our method, we consider several
different standard post-training compression scenarios [31, 19, 18]. We begin with settings where only a single type
of compression is applied: concretely, we consider unstructured pruning for given FLOP targets, global 2:4 and 4:8
pruning, as well as uniform weight quantization. Additionally, we also study two practical tasks that feature joint
pruning and quantization: a GPU scenario where quantization and N:M pruning are combined, as well as a CPU scenario
combining quantization and block pruning. We work with variants of the following models and tasks: ResNet [14] for
image classiﬁcation on Imagenet [37], YOLOv5 [20] for object detection on COCO [26] and BERT [5] for question
answering on SQuAD [36]. Our smaller BERT models denoted by BERT3 and BERT6 correspond to the smaller 3 and
6 layer variants of BERT-base, respectively, trained by [21]. The Appendix contains additional experiments as well as
runtime information of our algorithms.

Experimental Setup. All of our calibration datasets consist of 1024 random training samples. For ImageNet, where
we use roughly 0.1% of the training data, we additionally apply standard ﬂipping and cropping augmentations to
artiﬁcially increase the size of this dataset by 10×; other tasks do not use any augmentations. While the effect of
augmentations is typically minor, they are very cheap to include for our method. For ResNet models, batchnorm

7

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

statistics are reset using 100 batches of 128 samples from the calibration set with standard augmentations. For other
models, we apply mean and variance correction [32, 1] after all normalization layers (so that the correction parameters
can be easily merged and incur no extra cost) on a single batch of samples of size 128 (for YOLO) and 512 (for BERT).
We found this to be more effective than batchnorm tuning for YOLO, and the BERT models have no batchnorm layers.

When compressing to a given FLOP or timing constraint, we need to solve the problem of identifying per-layer
compression targets, which match the constraint, while maximizing accuracy. To identify these non-uniform targets, we
follow the approach of [10]: we ﬁrst collect a “model database” containing for each compression level (e.g. bit-width or
sparsity setting) the corresponding (independently) compressed version of each layer. For building a joint sparse and
quantized database we simply sparsify layers ﬁrst and then apply quantization to the remaining weights. Next, similarly
to [19], we compute the layer-wise calibration losses (without augmentations) for all compression levels, corresponding
to the models with exactly one layer compressed to a certain level. Then, given layer-wise FLOP or timing information,
we set up a constrained layer-wise compression problem of the form described in AdaQuant [19] and solve it with the
dynamic programming algorithm of SPDY [10]. This returns an optimal per-layer assignment of compression levels,
for which we can then easily produce the corresponding model, via a two-step process: we ﬁrst stitch together layers at
the corresponding compression levels from the database, and then perform the discussed statistics correction to recover
some extra accuracy [19].

Unstructured Sparsity. We begin our experiments with unstructured sparsity, comparing against global magnitude
pruning (GMP) [45], the approximate layer-wise OBS method L-OBS [6], and the post-training pruning state-of-the-art
method AdaPrune [18]. As a sanity check, we examine in Figure 1 whether our method provides better results in terms
of layer-wise squared error, pruning the ﬁrst layer of a ResNet18 (RN18) model to several sparsities. In this metric,
ExactOBS performs best by a wide margin ahead of AdaPrune, which signiﬁcantly outperforms the other two methods.

Next, in Table 1, we turn our attention to the practical problem of pruning various models to achieve a given FLOP
reduction of 2×–4×, applying the per-layer target sparsity optimization technique described above. Our ExactOBS
generally performs best (except for YOLOv5l 2× where all methods perform similarly in terms of mAP@0.5) and at
4× FLOP reduction even with a > 1% gap to the next best method. Interestingly, on the hard-to-prune BERT model,
ExactOBS appears to be the only method which still produces reasonable results at higher reduction targets. For BERT
3× and 4×, where the performance drop of all methods is > 3%, we additionally assess the compatibility of our results
with the more powerful (but also more expensive) post processing method global AdaPrune [10]. While this global
optimization technique is able to recover lost accuracy, the ExactOBS models still maintain a > 1% and > 2.5% F1
advantage, respectively (see Table 5).

Method

ResNet50 – 76.13
4×
3×
2×

YOLOv5l – 66.97
4×
3×
2×

BERT – 88.53
3×

4×

2×

GMP
L-OBS
AdaPrune

74.86
75.48
75.53

71.44
73.73
74.47

64.84
71.24
72.39

65.83
66.21
66.00

62.30
64.47
64.88

55.09
61.15
62.71

65.64
75.38
85.93

12.52
12.64
73.85

09.23
06.35
25.97

ExactOBS

75.64

75.01

74.05

66.14

65.35

64.05

87.19

84.72

81.32

Table 1: Unstructured pruning for different FLOP reduction targets.

Figure 1: RN18 squared error.

N:M Sparsity. Next, we study the performance of our method for semi-structured sparsity via the N:M pattern.
Speciﬁcally, we compare against the 4:8 results of AdaPrune with batchnorm tuning [18] on ResNet models (see
Table 2) in addition to a 2:4 comparison on BERT models (see Table 3). We highlight that ExactOBS matches or even
slightly exceeds the 4:8 results of AdaPrune with the considerably more stringent 2:4 pattern, which is already well
supported on NVIDIA hardware. Furthermore, in a 2:4 comparison on BERT models, ExactOBS achieves 1–2% higher
F1 scores.

Quantization. Further, we compare OBQ’s independent performance (after batchnorm tuning) with the state-of-
the-art sequential post-training methods AdaQuant [19], AdaRound [31] and BRECQ [24]. We perform standard
asymmetric per-channel quantization of all weights, using the authors’ implementations. We rerun all methods on
Torchvision [28] ResNets to ensure a uniform baseline. The quantization grids for OBQ as well as AdaRound are
determined with the same LAPQ [34] procedure that is used by BRECQ. Surprisingly, we ﬁnd that, despite optimizing
layers independently, OBQ achieves very similar (sometimes even slightly better) accuracies as existing non-independent
methods for 4 and 3 bits. This suggests that it should be well-suited for mixed precision applications where one needs

8

0.000.190.350.470.580.660.720.78Sparsity in relative 10% steps010002000300040005000Squared errorResNet18 - first layerExactOBSAdaPruneL-OBSGMPOBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

Model

Dense

AdaPrune
4:8

ExactOBS
4:8
2:4

ResNet18
ResNet34
ResNet50

69.76
73.31
76.13

68.63
72.36
74.75

68.81
72.66
74.71

69.18
72.95
75.20

Model

Dense AdaPrune ExactOBS

BERT3
BERT6
BERT

84.66
88.33
88.53

82.75
85.02

83.54
86.97
86.77

Table 2: Semi-structured N:M pruning (+ batchnorm tuning) of all
layers except the ﬁrst and the last.

Table 3: Semi-structured 2:4 pruning of all layers except
the embeddings.

to quickly generate many non-uniform models optimized for different constraints. (However, we note that ExactOBS
can also be applied sequentially; see Appendix.)

Method

Lw.

Ind.

ResNet18 – 69.76
3bit

4bit

2bit

ResNet50 – 76.13
3bit

2bit

4bit

AdaRound
AdaQuant
BRECQ

OBQ (ours)

yes
yes
no

yes

no
no
no

69.34
68.12
69.37

68.37
59.21
68.47

63.37
00.10
64.70

75.84
74.68
75.88

75.14
64.98
75.32

71.58
00.10
72.41

yes

69.56

68.69

64.04

75.72

75.24

70.71

Table 4: Comparison with state-of-the-art post-training methods for asymmetric per-
channel weight quantization of all layers. We mark whether methods are Layer-wise
(Lw.) or Independent (Ind.).

Methods

BERT

3×

4×

gAP + AdaPrune
gAP + ExactOBS

85.65
86.78

82.57
85.26

Table 5: Further improving results in
Table 1 with > 3% performance drops
through more expensive post-processing
via global AdaPrune (gAP).

(a) ResNet variants.

(b) YOLO variants.

(c) BERT variants.

(d) ResNet50 – CPU.

Figure 2: (a) to (c) Mixed quantization and 2:4 pruning for various BOP reduction targets. (d) Joint block-pruning and
quantization for CPU inference time speedups.

BOP-Constrained Mixed GPU Compression. We now consider a practical setting where we are given a trained
model together with some calibration data and want to compress this model for efﬁcient inference on an NVIDIA GPU
which supports 8-bit and 4-bit arithmetic, also in combination with 2:4 sparsity. Thus, there are 4 possible compression
choices per layer: 8bit weights + 8bit activations (8w8a), 4w4a, 8w8a + 2:4 and 4w4a + 2:4. Unlike in the previous
section, we do symmetric per-channel quantization of the weights as it has better hardware support; activations are
quantized asymmetrically per-tensor. We then generate mixed precision conﬁgurations for various BOP (number of
bits times FLOPs) reduction targets and visualize the resulting compression-accuracy trade-off curves in Figure 2. In
summary, at the cost of a ≈ 2.5% relative performance drop, we can achieve a 12 − 14× BOP reduction for ResNets
and a 7 − 8× reduction for the more challenging YOLO and BERT models (relative to the compute in compressible
layers). Additional comparisons against a baseline consisting of the best existing independent layer-wise pruning and
quantization methods can be found in the Appendix. In summary, those show that, as expected, the individual accuracy
gains for a single compression type also transfer to the joint setting. To the best of our knowledge, we are the ﬁrst to
consider joint N:M pruning and quantization in a post-training setting. Recent work [3] also studies joint 4w4a + 2:4
compression for ResNet18 but with 90 epochs of (sparse) Quantization-Aware Training (QAT) on the full dataset and
report 67.33% accuracy. Although not perfectly comparable (we keep the ﬁrst layer dense and their dense baseline
has 0.94% higher accuracy and uses 4:8 sparse activations), we achieve similar 67.20% accuracy for 4w4a + 2:4 post
training, which emphasizes the effectiveness of our methods for joint sparsiﬁcation and quantization.

Time-Constrained CPU Compression. Lastly, we explore a similar scenario, but targeting actual CPU inference
speedup on a 12-core Intel Xeon Silver 4214 CPU using the DeepSparse inference engine [35, 22], which provides
acceleration for joint 8-bit quantization and block-sparsity with blocksize 4. In this case, we work with real layer-wise

9

5×7×9×11×13×15×BOP reduction factor6264666870727476AccuracyMixed quantization + 2:4 pruningResNet50ResNet34ResNet18Dense5×6×7×8×9×10×BOP reduction factor55575961636567mAP @ 0.5Mixed quantization + 2:4 pruningYOLOv5lYOLOv5mDense5×6×7×8×9×10×BOP reduction factor788082848688F1Mixed quantization + 2:4 pruningBERTBERT6BERT3Dense3.5×4.0×4.5×5.0×5.5×6.0×Inference time speedup71727374757677AccuracyBlock-pruning + quantizationResNet50DenseOBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

timing data (for batchsize 64), as in [9]. There are 30 available block-sparsity targets per-layer, in steps of pruning
10% of the remaining weights, all of which are further quantized to 8 bits. The base acceleration of the dense 8 bit
model is ≈ 2.7× on top of which sparsity speedup acts roughly multiplicatively. Figure 2d shows results for ResNet50
and several (real-time) speedup targets—we achieve 4× and 5× (actual) speedup with 1% and 2% accuracy loss,
respectively. These are the ﬁrst full post-training results in this setting (the authors of [10] only performed 4-block
pruning post-training, followed by 5 epochs of QAT on the entire ImageNet dataset), and they show very encouraging
accuracy-speedup trade-offs.

7 Conclusions & Future Work

We have presented a new efﬁcient and accurate approach for solving the layer-wise compression problem, and built on it
to obtain state-of-the-art post-training compression solutions for both pruning and quantization. Our framework should
be naturally extensible to structured pruning, which in fact should allow for further optimizations, and should also be
compatible with further compression via unstructured pruning and quantization. Our results suggest that post-training
compression may be able to reach comparable accuracies to much more expensive retraining methods. We plan to
investigate this in future work, in particular in the context of more resource-intensive models, such as very large-scale
language models.

8 Acknowledgements

We gratefully acknowledge funding from the European Research Council (ERC) under the European Union’s Horizon
2020 programme (grant agreement No 805223 ScaleML), as well as computational support from AWS EC2. We thank
Eldar Kurtic for providing us BERT code and pretrained models, and the Neural Magic Team, notably Michael Goin
and Mark Kurtz, for support with their software.

References

[1] Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for

rapid-deployment. In Conference on Neural Information Processing Systems (NeurIPS), 2019.

[2] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations. Journal of Fourier

Analysis and Applications, 14(5-6):629–654, 2008.

[3] Brian Chmiel, Itay Hubara, Ron Banner, and Daniel Soudry. Optimal ﬁne-grained N:M sparsity for activations

and neural gradients. arXiv preprint arXiv:2203.10991, 2022.

[4] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efﬁcient

inference. In International Conference on Computer Vision Workshop (ICCVW), 2019.

[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In North American Chapter of the Association for Computational
Linguistics (NAACL), 2019.

[6] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise optimal

brain surgeon. In Conference on Neural Information Processing Systems (NeurIPS), 2017.

[7] Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. Fast sparse convnets. In Conference on Computer

Vision and Pattern Recognition (CVPR), 2020.

[8] Utku Evci, Nicolas Le Roux, Pablo Castro, and Leon Bottou. Mean replacement pruning. 2018.
[9] Elias Frantar and Dan Alistarh.

SPDY: Accurate pruning with speedup guarantees.

arXiv preprint

arXiv:2201.13096, 2022.

[10] Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-FAC: Efﬁcient matrix-free approximations of second-order

information. In Conference on Neural Information Processing Systems (NeurIPS), 2021.

[11] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of

quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021.

[12] Roger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In

International Conference on Machine Learning (ICML), 2016.

[13] Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In

IEEE International Conference on Neural Networks, 1993.

10

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[15] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for model compression and

acceleration on mobile devices. In European Conference on Computer Vision (ECCV), 2018.

[16] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.

In

International Conference on Computer Vision (ICCV), 2017.

[17] Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:
Pruning and growth for efﬁcient inference and training in neural networks. arXiv preprint arXiv:2102.00554,
2021.

[18] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Sefﬁ Naor, and Daniel Soudry. Accelerated sparse neural
training: A provable and efﬁcient method to ﬁnd N:M transposable masks. In Conference on Neural Information
Processing Systems (NeurIPS), 2021.

[19] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization

with small calibration sets. In International Conference on Machine Learning (ICML), 2021.

[20] Glenn Jocher. YOLOv5. https://github.com/ultralytics/yolov5, 2022.
[21] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and
Dan Alistarh. The Optimal BERT Surgeon: Scalable and accurate second-order pruning for large language models.
arXiv preprint arXiv:2203.07259, 2022.

[22] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson,
Sage Moore, Bill Nell, Nir Shavit, and Dan Alistarh. Inducing and exploiting activation sparsity for fast inference
on deep neural networks. In International Conference on Machine Learning (ICML), 2020.

[23] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Conference on Neural Information

Processing Systems (NeurIPS), 1990.

[24] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ:
Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning
Representations (ICLR), 2021.

[25] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep

neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021.

[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision
(ECCV), 2014.

[27] Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming
Yang, Qingmin Liao, and Wayne Zhang. Group ﬁsher pruning for practical network compression. In International
Conference on Machine Learning (ICML), 2021.

[28] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In ACM International

Conference on Multimedia, 2010.

[29] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In

International Conference on Machine Learning (ICML), 2015.

[30] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and
Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021.
[31] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down?
Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML),
2020.

[32] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight

equalization and bias correction. In International Conference on Computer Vision (ICCV), 2019.

[33] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen

Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.

[34] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi

Mendelson. Loss aware post-training quantization. Machine Learning, 110(11):3245–3262, 2021.

[35] NeuralMagic. DeepSparse, 2022.
[36] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine
comprehension of text. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.

11

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

[37] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International
Journal of Computer Vision, 115(3):211–252, 2015.

[38] Sidak Pal Singh and Dan Alistarh. WoodFisher: Efﬁcient second-order approximation for neural network

compression. In Conference on Neural Information Processing Systems (NeurIPS), 2020.

[39] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning in the

Kronecker-factored eigenbasis. In International Conference on Machine Learning (ICML), 2019.

[40] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware automated quantization with

mixed precision. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

[41] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization

via bit-split and stitching. In International Conference on Machine Learning (ICML), 2020.

[42] Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. Automatic neural network compression by sparsity-
quantization joint learning: A constrained optimization-based approach. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2020.

[43] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang,
Yida Wang, Michael Mahoney, et al. HAWQ-v3: Dyadic neural network quantization. In International Conference
on Machine Learning (ICML), 2021.

[44] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li.
Learning N:M ﬁne-grained structured sparse neural networks from scratch. In International Conference on
Learning Representations (ICLR), 2021.

[45] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efﬁcacy of pruning for model compression.

arXiv preprint arXiv:1710.01878, 2017.

12

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

Contents

1 Introduction

2 Related Work

3 Problem Deﬁnition and Background

4 An Optimal Greedy Solver for Sparsity

5 The Optimal Brain Quantizer (OBQ)

6 Experiments

7 Conclusions & Future Work

8 Acknowledgements

A Appendix

A.1 Proof of Lemma 1 (Row & Column Removal) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 ExactOBS Global Step Pseudocode

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 OBQ-ExactOBS Algorithm Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.4 Further Experiment Details .

A.5 Timing Information .

.

.

.

.

.

.

A.6 Multiple AdaPrune Iterations .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.7 Independent Quantization Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.8 Sequential Quantization with OBQ .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.9 Impact of ImageNet Data Augmentations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

A.10 Sensitivity to Random Seeds .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.11 Compound Compression Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

3

3

4

6

7

10

10

13

13

14

14

14

15

16

17

17

17

18

18

A Appendix

A.1 Proof of Lemma 1 (Row & Column Removal)

Proof. First, we observe that element j in row i, i.e. [A]ij, is set to 0 by the equivalent matrix transformation of
subtracting [A]ij times column i denoted by A:,i divided by the corresponding diagonal element [A]ii (similarly,
elements in column i can be set to 0 by subtracting row i). Thus, Lemma 1 corresponds to zeroing H−1
ip for
i (cid:54)= p via equivalent matrix transformations, or in other words, Gaussian elimination of one row and column.
Next, we apply these equivalent matrix transformations to both sides of the obvious equality H−1H = I, which
ultimately gives an equation of the following AB = C form:

pi and H−1





A1 0 A2
0(cid:62) a 0(cid:62)
A4 0 A3



 ·





B1
(cid:62)
b4
B4



 =

b1 B2
(cid:62)
b
b2
b3 B3





I
(cid:62)
c4
0

c1
c
c3



 .

0
(cid:62)
c2
I

(8)

Notice now that the entries of B corresponding to the eliminated row and column in A do not affect the I and 0 blocks
in C since they are always multipled by 0. Thus, the matrix of the Ai blocks must be the inverse of the Bi block matrix,
which is exactly what we wanted to calculate.

13

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

A.2 ExactOBS Global Step Pseudocode

This section provides more details about the global step of the ExactOBS algorithm described in Section 4 in the form
of pseudocode.

Algorithm 2 Let P be a drow × dcol matrix storing the order in which weights are pruned by ExactOBS in each row and
let L be the matrix of the corresponding loss-changes δL. Then the following procedure determines the global OBS
mask with k pruned weights.

Q = {(i, 0) | 1 ≤ i ≤ drow}
for k times do

i, j ← argmin(i,j)∈Q [L]i(j+1) if j < dcol else ∞
Q ← Q − {(i, j)}
Q ← Q ∪ {(i, j + 1)}

end for
Q contains the number of pruned elements j per row i, which together with P yields the mask.

For increased efﬁciency, the set Q can be implemented, for example, as a min-heap. Finally, we note that the slightly
simpler method of picking the k smallest elements in L and then counting how many were picked in each row typically
produces essentially the same results as Algorithm 2 in practice since the loss changes generally increase monotonically
as more weights are pruned.

A.3 OBQ-ExactOBS Algorithm Pseudocode

The OBQ version of the ExactOBS algorithm is given below; we emphasize the similarity to the pruning variant of
ExactOBS shown in Algorithm 1.

Algorithm 3 Quantize k ≤ dcol weights from row w with inverse Hessian H−1 = (2XX(cid:62))−1 according to OBS in
O(k · d2

col) time.
M = {1, . . . , dcol}
for i = 1, . . . , k do
p ← argminp∈M
w ← w − H−1
:,p
H−1 ← H−1 −
M ← M − {p}

1
[H−1]pp
1
[H−1]pp
1
[H−1]pp

· (q(wp) − wp)2
· (wp − q(wp))
:,p H−1
H−1
p,:

end for

A.4 Further Experiment Details

We now provide some additional details about our experiments in Section 6.

Bias and Variance Correction. Although our bias and variance correction step applied to YOLO and BERT models
is similar to the schemes described in [32] and [1], we now describe our exact procedure for additional clarity:

1. Sample one batch from the calibration dataset.
2. Perform inference on this batch with the dense model and record after each normalization layer the mean
dense for each channel (for CNNs) / feature (for Transformers) over this batch.
comp and standard deviations

3. Perform inference on this batch with the compressed model and record the means µ(cid:96)

dense and standard deviation σ(cid:96)
µ(cid:96)

comp as in step 2, while already applying mean and variance correction to the layer outputs X (cid:96) via:
σ(cid:96)

Y (cid:96) =

σ(cid:96)
σ(cid:96)

dense

comp

· (X (cid:96) − µ(cid:96)

comp + µ(cid:96)

dense)

(9)

4. Merge (9) into the afﬁne parameters of the respective normalization layer.

We note that it is critical to apply the statistics correction already while computing the compressed means and variances
in step 3 in order to properly account for compounding distribution shifts.

14

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

Non-Uniform Sparsity Choices. The method we use for determining per-layer (unstructured or blocked) sparsity
values to reach a certain overall budget with minimal accuracy loss requires a discrete set of sparsity choices per layer.
For both unstructured and blocked sparsity, we follow [10] and choose a grid where each point prunes the same fraction
of remaining weights δ. Hence, sparsity choice si is given by:

si = 1 − (1 − δ)i.

(10)

In both cases we choose δ = 0.9, which corresponds to pruning 10% of the remaining weights. For unstructured
sparsity, we generate choices until si > 0.99 and for blocked sparsity until si > 0.95. We note that these sets of sparsity
options are chosen to allow for maximum ﬂexibility. However, in many cases, similar results can likely be achieved
with signiﬁcantly fewer, but more carefully selected (e.g. using the fact that very high sparsities will typically never be
chosen for lower FLOP reduction targets), options and thus less required database storage.

Activation Quantization.
In our GPU-focussed quantization + 2:4 pruning experiments we also quantize all acti-
vations. This is done by simply optimizing the zero point and quantization scale for the input of each layer using
exactly the same procedure as for the weights, just on tensor- instead of channel-level (which is the same LAPQ
[34] procedure also used by BRECQ [24]). This is again done independently for each layer and the corresponding
quantization information is stored in the model database to allow for quick stitching. More advanced schemes such as
reoptimizing the weights to better match the quantized inputs (see Appendix A.8) may be possible, but we found the
simple procedure just described to already work quite well.

A.5 Timing Information

In this section, we provide detailed information about the runtime of our method. All numbers reported here are for the
execution on a single NVIDIA RTX 3090 GPU using our PyTorch implementations. Pruning runs with a global step are
performed with the “less compute” variant described in Figure 1. Hence an entire database of many pruning levels can
be generated in approximately the time shown for unstructured and block pruning runs here.

PTQ Runtime Comparison. We begin with a runtime comparison of existing state-of-the-art post-training methods
at the task of quantizating the weights of all layers of a ResNet50 to 4 bits. All timings were collected by executing the
authors’ open-source implementations on the same hardware, the results are shown in Table 6.

Model

BitSplit AdaRound AdaQuant BRECQ OBQ

ResNet50

124m

55m

17m

53m

65m

Table 6: Runtimes of post-training quantization methods in minutes (m).

BRECQ, AdaRound and our method OBQ all take around one hour to fully quantize ResNet50, the former two slightly
less and the latter slightly more. Meanwhile, BitSplit takes about twice as long, whereas AdaQuant is 3× faster.
However, as shown in Table 4 in the main text (as well as in Table 9), AdaQuant is also considerably less accurate than
the other methods. In summary, the runtime of ExactOBS is in line with existing post-training methods. Additional
optimizations, like periodically shrinking the Hessian by omitting rows/columns of pruned/quantized weights, can
likely improve the practical speed further.

Different Compression Types. Next, we study the runtime of ExactOBS applied to different types of compression
problems. We consider a smaller model (YOLOv5s), a medium model (ResNet50) and a larger one (BERT). The
corresponding runtimes for all compression types featured in this work are listed in Table 7.

Model

Quant Unstr

4-block

2:4

Quant 2:4

ResNet50
YOLOv5s
BERT

65m
7m
111m 103m

64m
6m

61m
10m
142m

31m
3m
51m

35m
4m
56m

Table 7: Runtimes of ExactOBS for different models and compression types in minutes (m).

In general, we can see that quantization and unstructured pruning take about the same time, which matches with the fact
that the corresponding algorithms are very similar. Correspondingly, 2:4 pruning and quantizing a 2:4 pruned model are

15

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

only approximately half as expensive, which is again expected as they perform half the work. For YOLO and BERT,
blocked pruning is the most expensive compression type due to the overheads incurred by handling the additional c × c
block matrices (see Section 4). Interestingly, for ResNet50, this is not the case, which is probably related to the highly
non-uniform compute distribution that is discussed in more detail in the next paragraph. Overall, these results show that
our techniques are quick for small models and still reasonably efﬁcient even for bigger models like BERT, taking less
than 2 hours on a single GPU. Finally, we note that ExactOBS is essentially perfectly parallelizable and its runtime can
thus scale linearly with the number of available GPUs.

Per-Layer Runtimes. Finally, we note that as the time complexity of OBQ implemented via ExactOBS is O(drow ·
d3
col), i.e. cubic in the column dimension, the overall runtime can often be dominated by a few particularly large layers.
This is illustrated e.g. by ResNet50 where, as shown in Figure 3, about 75% of the overall runtime is spent in the 3 × 3
convolutions of the last block (which have dcol ≈ 4500 when unfolded), of which there are just 3 in total. Meanwhile,
most of the earlier layers are quantized within seconds. This means that one could, in many cases, reduce the overall
compression runtime signiﬁcantly by applying a faster but less accurate method to just those few bottleneck layers
while still achieving more accurate compression on all the others through our techniques.

Figure 3: Runtime of OBQ for each layer of ResNet50.

A.6 Multiple AdaPrune Iterations

While AdaPrune [18] determined all weights to prune in a single step, the authors of [10] found that iterating this
process in smaller steps can often improve performance signiﬁcantly, at quickly increasing computational costs. Our
method realizes the very limit of this scheme with one step for each weight. In this section, we study how OBQ comares
against AdaPrune with a varying number of pruning and full reoptimization steps. For that purpose, we prune BERT to
uniform 75% sparsity by applying AdaPrune in k = 2i steps that, as suggested by [10], all prune the same fraction of
remaining weights.

Model

Sparse ExactOBS AP 1× AP 2× AP 4× AP 8× AP 16×

BERT

75%

-10.16

-63.57

-39.06

-25.73

-20.86

-17.08

Table 8: Comparing F1 drops against AdaPrune (AP) with a varying number of pruning/reoptimization steps.

Our results conﬁrm the ﬁnding of [10] that iterating AdaPrune multiple times can signiﬁcantly improve results, as we
see the F1 drop decreasing quickly with just a few such “recomputations”. Nevertheless, even after 16 full iterations,
which have an overall runtime comparable to ExactOBS, the accuracy drop for the (iterative) AdaPrune model is still
≈ 70% larger than the one of ExactOBS, clearly demonstrating the beneﬁt of our method.

16

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

A.7

Independent Quantization Comparison

In our uniform quantization experiments in the main paper (see Table 4), we only compared OBQ with state-of-
the-art sequential methods as those are generally signiﬁcantly more accurate than independent ones. However, for
completeness, we now additionally compare OBQ with two other methods that have also been used for independent
layer-wise quantization: BitSplit [6] and AdaQuant [19]. Here we consider symmetric per-channel quantization as this
is the quantization mode BitSplit was designed for. Additionally, we compare “raw” quantization performance, that is
directly after independent compression without any additional statistics corrections. The results of the comparison are
summarized in Table 9.

Method

ResNet18 – 69.76
3bit

2bit

4bit

ResNet34 – 73.31
3bit

2bit

4bit

ResNet50 – 76.13
3bit

4bit

2bit

BitSplit
AdaQuant

67.58
65.45

59.25
49.29

07.36
00.87

71.63
69.49

64.91
56.10

26.62
00.84

74.94
72.79

71.76
53.06

07.31
00.13

OBQ (ours)

69.18

67.14

48.34

72.85

71.01

51.62

75.50

73.61

46.33

Table 9: Uniform symmetric per-channel weight quantization.

As expected, OBQ clearly outperforms the other two independent methods on all considered models and bitwidths; at 3
bits by several percent in accuracy and at 2 bits it is the only method that does not break down completely without any
statistics correction.

A.8 Sequential Quantization with OBQ

While we primarily focus on the independent application of OBC which enables quick stitching of various mixed-
compression models, it is also possible to apply OBC sequentially, in similar fashion to state-of-the-art post-training
quantization works [31, 19, 24]. While other methods simply perform the per-layer optimization by swapping out
the dense model inputs Xdense for the corresponding inputs in the compressed model Xcomp, this does not sufﬁce for
OBQ. If the Hessian is computed on Xcomp, then the initial dense weights are not a local minimum (with 0 gradient)
anymore, hence violating a key assumption of OBQ. Fortunately, this problem can be easily resolved by reoptimizing
the dense weights for the new inputs via the closed form solution of linear regression W(cid:62) = (XX(cid:62))−1XY(cid:62), after
which the gradient is 0 again, and OBQ can be applied correctly. We note that XY(cid:62) is a dcol × drow matrix which
can be easily accumulated over multiple batches similar to the OBQ Hessian 2XX(cid:62), without any major increase in
memory consumption.

As a demonstration, we apply sequential OBQ to the task of quantizating ResNet18 to various bitwidths (in the same
setup as in Table 4 in the main paper) and report the results in Table 10. Interestingly, for 4 and 3 bits, the results are
essentially the same as for the independent version (with batchnorm statistics correction); only for the 2 bits setting
there seems to be a noticeable beneﬁt, catching up with the corresponding BRECQ result. A more detailed investigation
of this phenomenon could be a interesting direction for future work.

Method

AdaRound
AdaQuant
BRECQ

OBQ + BNT
OBQ – sequential

ResNet18 – 69.76
3bit

2bit

4bit

69.34
68.12
69.37

69.56
69.56

68.37
59.21
68.47

68.69
68.68

63.37
00.10
64.70

64.04
64.93

Table 10: Comparison with sequential OBQ.

A.9

Impact of ImageNet Data Augmentations

As described in the main submission text, for ImageNet experiments, we expand our calibration set with standard
data augmentations by a factor of 10. The is mainly done to ensure that the 2048 × 2048 Hessian corresponding to
the fully-connected layer of ResNet50 is full rank (which is not the case for just 1024 images) and thus avoid any

17

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

hyper-parameter tuning of a dampening constant. Additionally, it should serve as a demonstration that augmentations
are cheap to use in conjunction with our method, which is not the case for other post-training methods that would
require either considerably increased memory (storing many more activations) or runtime (performing full inference on
the entire model for each batch in the per-layer optimization).

We now study the impact of these augmentations on our results, for which rerun OBQ (in the setup of Table 4 without
them, but using dampening λ = 1 (relative to the values in the Hessian this is actually a rather small constant) for the
last layer of ResNet50. A comparison with the original results is shown in Table 11.

Method

ResNet18 – 69.76
3bit

2bit

4bit

ResNet50 – 76.13
3bit

2bit

4bit

OBQ
OBQ – no aug

69.56
69.59

68.69
68.51

64.04
63.87

75.72
75.87

75.24
75.06

70.71
70.51

Table 11: The impact of data augmentations.

As can be seen, the difference between using and not using data augmentations is generally only rather minor at
≈ 0.1 − 0.2%. Nevertheless, augmentations are very cheap to use in conjunction with our methods (they only need to
be accumulated into the initial per-layer Hessians once) and at the same time avoid a dampening hyper-parameter in
several cases; therefore we use them in our ImageNet experiments.

A.10 Sensitivity to Random Seeds

For a ﬁxed calibration dataset, the ExactOBS algorithm is deterministic. For ResNet models, small amounts of additional
randomness are added by the data augmentations that are applied to the calibration dataset as well as by batchnorm
tuning which happens with randomly sampled batches; for the other models we consider, there is no extra randomness
beyond the initial sampling of the calibration dataset. To assess how much the results of our methods are affected by
these random factors, we quantize ResNet18 to 4bit (symmetric per-channel) and prune ResNet50 to the 2:4 pattern, for
5 different seeds each, and report mean standard deviation in Table 12.

ResNet18 – 4bit ResNet50 – 2:4

69.28 ± 0.07

74.74 ± 0.05

Table 12: Typical random variation of OBC results.

In conclusion, the variation of results with respect to random seeds is generally very low, here < 0.1%, which is in line
with other post training methods [31, 24].

A.11 Compound Compression Comparisons

In the main paper, we focused on independent comparisons for quantization and pruning since existing methods
are generally only designed for a single compression approach. In this section, we additionally provide compound
comparisons for our GPU and CPU scenarios which combine sparsity and quantization. In particular, we construct a
strong baseline by substituting OBC in our mixed setup with the best independent layer-wise pruning and quantization
methods, AdaPrune and AdaQuant, respectively. We now provide detailed comparisons for all experiments of Figure 2
from the main text, in Figures 4, 5 and 6.

In summary, it appears that, as expected, the accuracy improvements for the individual compression types shown by
the experiments in Section 6 also transfer to the combined setting. More concretely, for the reduction target ranges
highlighted in the main paper, that is 12 − 14× for ResNet models and 7 − 8× for others, there is a consistent 0.5 − 1.5
point gap between OBC and the AdaPruneQuant baseline. For lower BOP reduction / inference time speedup targets,
the gap is typically smaller, which is expected as only the less sensitive layers have to compressed more than to the
generally very easy 8-bit level. In contrast, the gaps are largest for the highest targets that also require high compression
of sensitive layers as this is where the effects of OBC’s more accurate layer-wise compression become particularly
noticeable.

18

OBC: Accurate Post-Training Quantization and Pruning

A PREPRINT

(a) ResNet18.

(b) ResNet34.

(c) ResNet50.

Figure 4: Mixed quantization and 2:4 pruning for various BOP reduction targets on ResNet models.

(a) BERT3.

(b) BERT6.

(c) BERT.

Figure 5: Mixed quantization and 2:4 pruning for various BOP reduction targets on BERT models.

(a) YOLOv5m.

(b) YOLOv5l.

(c) ResNet50 – CPU.

Figure 6: (a) & (b): Mixed quantization and 2:4 pruning for various BOP reduction targets on YOLO models. (c) Block
sparsity & quantization for real-time CPU inference speedup targets on ResNet50.

19

46810121416BOP reduction factor646566676869AccuracyQuantization + 2:4 pruningOBC (ours)AdaPruneQuantDense46810121416BOP reduction factor6970717273AccuracyQuantization + 2:4 pruningOBC (ours)AdaPruneQuantDense46810121416BOP reduction factor717273747576AccuracyQuantization + 2:4 pruningOBC (ours)AdaPruneQuantDense5×6×7×8×9×10×BOP reduction factor8081828384F1Quantization + 2:4 pruningOBC (ours)AdaPruneQuantDense5×6×7×8×9×10×BOP reduction factor72.575.077.580.082.585.087.5F1Quantization + 2:4 pruningOBC (ours)AdaPruneQuantDense5×6×7×8×9×10×BOP reduction factor8182838485868788F1Quantization + 2:4 pruningOBC (ours)AdaPruneQuantDense5×6×7×8×9×10×BOP reduction factor55565758596061626364mAP@0.5Quantization + 2:4 pruningOBC (ours)AdaPruneQuantDense5×6×7×8×9×10×BOP reduction factor626364656667mAP@0.5Quantization + 2:4 pruningOBC (ours)AdaPruneQuantDense3.5×4.0×4.5×5.0×5.5×6.0×Inference time speedup7071727374757677AccuracyBlock-pruning + quantizationOBC (ours)AdaPruneQuantDense