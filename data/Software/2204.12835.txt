2
2
0
2

l
u
J

4
1

]

C
D
.
s
c
[

4
v
5
3
8
2
1
.
4
0
2
2
:
v
i
X
r
a

Learning to Parallelize in a Shared-Memory Environment with
Transformers

Re’em Harel
reemha@bgu.ac.il
Department of Computer Science,
Ben-Gurion University of the Negev
Department of Physics, Nuclear
Research Center – Negev
Scientific Computing Center, Nuclear
Research Center – Negev
Israel

Yuval Pinter
uvp@cs.bgu.ac.il
Department of Computer Science,
Ben-Gurion University of the Negev
Israel

Gal Oren∗
galoren@cs.technion.ac.il
Department of Computer Science,
Technion – Israel Institute of
Technology
Scientific Computing Center, Nuclear
Research Center – Negev
Israel

Figure 1: Overview of the workflow for classifying OpenMP directives and clauses. PragFormer is our proposed model.

ABSTRACT
In past years, the world has switched to many-core and multi-core
shared memory architectures. As a result, there is a growing need
to utilize these architectures by introducing shared memory paral-
lelization schemes to software applications. OpenMP is the most
comprehensive API that implements such schemes, characterized
by a readable interface. Nevertheless, introducing OpenMP into
code, especially legacy code, is challenging due to pervasive pit-
falls in management of parallel shared memory. To facilitate the
performance of this task, many source-to-source (S2S) compilers
have been created over the years, tasked with inserting OpenMP
directives into code automatically. In addition to having limited
robustness to their input format, these compilers still do not achieve
satisfactory coverage and precision in locating parallelizable code
and generating appropriate directives. In this work, we propose
leveraging recent advances in machine learning techniques, specif-
ically in natural language processing (NLP), to suggest the need
for an OpenMP directive. The model also suggests directives com-
plementary to S2S automatic parallelization compilers or provides

∗Corresponding author

, ,
2022. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

immediate on-the-fly advice to the developer without compiling
or executing the code. We create a database, Open-OMP, specifi-
cally for this goal. Open-OMP contains over 17,000 unique code
snippets from different domains, half of which contain OpenMP
directives while the other half do not need parallelization with
high probability. We use the corpus to train systems to automati-
cally classify code segments in need of parallelization, as well as
suggest individual OpenMP clauses. We train several transformer
models, named PragFormer, for these tasks and show that they
outperform statistically-trained baselines and automatic S2S par-
allelization compilers in both classifying the overall need for an
OpenMP directive and the introduction of private and reduction
clauses. In the future, our corpus can be used for additional tasks, up
to generating entire OpenMP directives. Our source code and data-
base are available at: https://github.com/pragformer/PragFormer.

KEYWORDS
OpenMP, Shared memory Parallelism, Transformers, Source-to-
source Compilers, Code Language Processing

ACM Reference Format:
Re’em Harel, Yuval Pinter, and Gal Oren. 2022. Learning to Parallelize in a
Shared-Memory Environment with Transformers. In Proceedings of . ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

Database-Open-OMPPreprocessingRepositoriesParsepycparserCodeSnippetsDirectivesExtractExtractCodeRepre-sentationLabelsPragFormerTrainUpdateClassifyTestEvaluatePre-train 
 
 
 
 
 
, ,

Re’em Harel, Yuval Pinter, and Gal Oren

# Code

S2S

AST Representation #1

AST Representation #2

1

for (i=0;i<=N;i++)

A[i] = i;

#pragma omp parallel for
for (i=0;i<=N;i++)

for (i=0;i<=N;i++)
B[i] = B[i]*2;

2

for (i=0;i<=N;i++)
if (MoreCalc(i))

Calc(i);

A[i] = i;

#pragma omp parallel for
for (i=0;i<=N;i++)
B[i]=B[i]*2;

#pragma omp parallel for
for (i=0;i<=N;i++)
if (MoreCalc(i))

Calc(i);

Table 1: Pitfalls of S2S automatic paralellization compilers.

1 INTRODUCTION
In past decades, software applications enjoyed a constant decrease
in processor execution times, almost solely from the increase in
performance of a single processor (based on Moore’s law [17]).
However, due to the end of Dennard’s scaling [23], the increase of a
single processor performance (in clock time) has slowed down dra-
matically. As a result of the constant demand for increased perfor-
mance, the world has shifted its computational paradigm towards
multi-core and many-core shared memory architectures (cache-
coherent Non-Uniform Memory Access [48], co-processors [72],
General Purposed GPUs [46], among others). This change yielded
an increase in performance by utilizing shared memory parallelism
schemes rather than increasing the performance of a single proces-
sor. To adjust the trend, serial applications, as well as distributed
ones, needed to be adapted to the new architecture and introduce
shared memory parallelization schemes in order to exploit the new
hardware.

The most comprehensive API that implements the shared mem-
ory model is the OpenMP API [21]. The OpenMP API consists of
a set of compiler directives (pragmas), library routines, and envi-
ronment variables that allows a program to be executed in parallel
(multi-threaded) within a shared memory environment. One of the
main advantages of the OpenMP API, which also contributes to
its popularity, is its flexible and straightforward interface that is
readable and easily interpreted. For example, a private clause states
that each thread has its own private copy of a particular variable.
Nevertheless, introducing correct and optimal OpenMP paralleliza-
tion instructions to applications is a complex and tedious task due
to ubiquitous pitfalls in management of parallel shared memory,
by architecture heterogeneity, and by the current necessity for
human expertise to comprehend many fine details and abstract
correlations [1].

1.1 Source-to-Source Automatic Parallelization

Compilers

Over the last decade, automatic parallelization compilers have been
created to ease the process of introducing parallelization directives
into code [15, 68, 70, 92]. Generally, the automatic parallelization
process occurs during compilation time or via source-to-source

For:

For:

Assignment: =

Assignment: =

ID: i
Constant: Int, 0

ID: i
Constant: Int, 0

BinaryOp: <=

BinaryOp: <=

ID: i
ID: N

UnaryOp: p++

ID: i

Assignment: =
ArrayRef:
ID: A
ID: i

ID: i

For:

Assignment: =

ID: i
...

ID: i
ID: N

UnaryOp: p++

ID: i

If:

FuncCall:

ID: MoreCalc
ExprList:
ID: i
FuncCall:
ID: Calc
ExprList:
ID: i

Table 2: Corresponding AST representations of the exam-
ples in Table 1.

(S2S) automatic parallelization compilers that insert OpenMP direc-
tives automatically. The major drawback of automatic paralleliza-
tion during the compilation process is the nontransparent result
that they generate, i.e., it is unknown if and how automatic paral-
lelization was achieved. In contrast, S2S compilers provide the full
output in the source code, providing the user with full transparency
of the result, and allowing the user to review, commit changes and
even optimize the output [24]. Examples of such S2S compilers are:
Par4All [4], AutoPar [71], and Cetus [2]. Generally, the workflow
of said S2S compilers is as follows:

(1) Create an abstract syntax tree (AST) [62] using a source
code parser, such as ANother Tool for Language Recognition
(ANTLR) [65].

(2) Apply data dependence algorithms [27] (predefined rules)

on the result of (1).

(3) Produce the appropriate OpenMP directive [45] based on (2).

Harel et al. [33] (2020) and S. Prema et al. [69, 70] (2017, 2019)
showed that although S2S compilers provide a solution for code
parallelization, they still have many pitfalls. Among these pitfalls,
S2S compilers produce suboptimal directives (as judged by human
experts), degrade performance, and sometimes even fail to insert a
directive. For example, in example #1 in Table 1, the original code
contains two independent arrays and two loop segments: initial-
ization of array A and a simple calculation of array B. The S2S
compilers can produce OpenMP directives that create unnecessary
overhead caused by spawning the threads twice, once for each loop,
rather than just once (the compilers cannot comprehend such a
case as it requires additional knowledge regarding the internal logic
of the code). This overhead can easily be avoided by identifying
several consecutive loop segments, wrapping them with a single
parallel region directive, and even adding the nowait directive—if
the consecutive loop segments are independent—to further improve

2

Learning to Parallelize in a Shared-Memory Environment with Transformers

, ,

performance. Furthermore, sometimes a for-loop exhibits an unbal-
anced workload where each iteration takes a different amount of
time to finish, such as that created by the if statement in example
#2. In such cases, S2S compilers will not make use of the sched-
ule(dynamic) directive, which distributes the workload between the
threads dynamically, and instead use the default schedule(static),
producing a non-optimal increase in performance. These mistakes
are caused by the inability of the deterministic nature of the S2S
compilers to identify loops with an unbalanced workload from the
source code alone. Identifying the unbalanced workload usually
requires understanding the internal logic of the code, or identifying
that the if statement contains extensive calculations compared to
the else statement.

The examples detailed above are not exceptional; it was found
in [33] that determining function side effects is a significant issue
for these S2S compilers. In addition, applying the data dependence
algorithm on the AST representation of the source code consumes
significant time and memory dependent on the number of lines in-
side the loop’s scope, which makes these S2S compilers impractical
for long code segments. As an example, in Table 2 we present the
AST representations of the code in Table 1. The AST, which has a
tree structure, is created by parsing the syntax of the source code,
with each line containing a single token corresponding to a single
operation or variable.For example, the line Assignment corresponds
to the operator =.

Nevertheless, S2S compilers applicability in the HPC community
is negatively perceived [58]. The following four points were pre-
sented by Milewicz et al. [58] (2021) as the main issues with S2S
transformation:

(1) S2S transformations interfere with or are oblivious to down-

stream compiler optimizations.

(2) S2S compilers are difficult to extend to support new program-

ming models.

(3) S2S approaches lead to complex and fragile workflows.
(4) Parsing source code is inherently difficult, as it requires
parsing the source code, representing the variables and de-
pendencies, and then applying some algorithm over the rep-
resentation. Nevertheless, the problem is inherently difficult
as representation is not comprehension nor intelligence.

Given this perception, there are problems that are difficult to solve
by classical algorithms (such as existing S2S compilers). Instead,
solving these problems—which are similar to problems in language
processing—by leveraging machine learning techniques, specifically
natural language processing (NLP) techniques, can be useful. For
example, with NLP techniques, the parsing stage can be almost
wholly skipped as the model’s input can be the raw text of the
source code.

1.2 Code Language Processing
Natural language processing is a field in artificial intelligence con-
cerned with giving computers the ability to process and analyze
natural language text, mostly through technologies from machine
learning (ML) [43]. Popular tasks in NLP include machine transla-
tion, question answering, information extraction, and named entity
recognition [25]. Due to some similarities between natural lan-
guages and programming languages, many studies have explored

the prospects of applying NLP methods to code. These applications
include source code analysis [39, 76, 81], where code is processed
to generate natural-language documentation [31, 50, 74] or further
code; source code generation [66, 93] based on natural language
descriptions; and lighter applications where no generation happens,
such as programming language classification [85] and code change
analysis [84]. Methods proposed for these tasks are often rooted in
the development of cognitive models for human comprehension of
code [14, 80]. Together, these methods and tasks can be grouped
together under the umbrella term Code Language Processing (CLP).
The applicability of NLP tools, particularly the most effective
ones that use ML and its recent development deep learning (DL), to
tasks applying to code languages is not at all trivial. Code is firmly
structured, unambiguous, and purposeful, as opposed to natural
languages which exhibit many “noisy” characteristics, as well as
cultural effects, ubiquitous ambiguity, and varied levels of infor-
mational purpose [63]. Many NLP tasks are also characterized by
being cast locally, on a single-sentence level (such as sentence-level
translation), an assumption which is starting to be challenged and
abandoned [12, 22]; in the code landscape, long-range dependencies
are almost unavoidable, considering the scopes over which variables
are used and functions are called. Therefore, use of powerful but
local sequential models such as the recurrent neural net (RNN) [26]
or its more robust variant the long short-term memory network
(LSTM) [36] may not produce good results for the tasks at hand. At
the present time, the most used ML technique for complicated CLP
problems (such as program comprehension) is the attention-based
encoder-decoder [54, 76] model known as the transformer [86]. In
this paper, we present a transformer-based method for facilitating
code parallelization through OpenMP, demonstrating its improved
performance over S2S systems, as well as the necessity of its so-
phisticated model architecture.

2 CODE LANGUAGE PARALLEL PROCESSING
To better formulate the above proposed idea, we hereby introduce
code language parallel processing (CLPP) as a sub-discipline in
CLP which deals with incorporating parallelism schemes via ML
techniques. While ML techniques are not strange to the parallel
processing area [18, 35, 47, 57, 59, 88–90], as of yet there has not
been an attempt to use CLP techniques to insert OpenMP directives
automatically. Similar to the program comprehension task, inserting
OpenMP directives automatically requires comprehending the data
dependencies and logic of the code in order to produce correct and
optimal OpenMP directives. Nevertheless, given that the amount
of existing OpenMP directives is limited, and in practice, there are
only dozens of OpenMP directives that are widely being used in the
HPC community [56]; this task can be categorized as a classification
rather than generation task.

We note that OpenMP directives syntax is closely related to doc-
umentation as they both are written as comments in the code; doc-
umentation appears as comments for the developer, and OpenMP
directives as comments for the compiler.

As stated by Milewicz et al. [58], parsing the source code to an
AST, or other representation, is the first and most crucial step in
every S2S solution. However, HPC practitioners believe that this

3

, ,

Re’em Harel, Yuval Pinter, and Gal Oren

the title, description or README file. This query resulted in more
than 20,000 files from approximately 7,000 different repositories.

3.1.1
Inclusion Criteria. Among the 20,000 files, we considered
only files written in C and containing an OpenMP directive. We
used pycparser [13], a Python based C code parser, to identify these
files. The parser creates an AST of the source code, and by travers-
ing the tree, one can easily identify OpenMP directives along with
their corresponding loop segments. We identified the question of
whether or not a code segment is even suitable for OpenMP an-
notation as an important task for a model to be able to answer
(RQ1), which means that the corpus must also include code that is
not explicitly parallelizable in order to provide ML classifiers with
negative examples to learn from. We limited our negative data to
code without OpenMP directives appearing in files where elsewhere
such directives do exist, to rule out cases where code amenable to
directives was not annotated due to developers unfamiliar with
parallelization schemes or those who work with incompatible hard-
ware.

3.1.2 Exclusion Criteria. Some OpenMP directives are nearly im-
possible to predict automatically from code alone; for example, the
task construct requires extensive background knowledge regard-
ing the logic of the code. Thus, in accordance with S2S compilers,
we include only OpenMP directives defined over a loop segment
(directives containing #pragma omp parallel for) in the corpus. In
addition, we found numerous repositories containing examples of
OpenMP directives that were created solely for testing compiler
compatibility. These examples, which contained an empty for-loop,
were also excluded from the corpus. In the developer community, it
is common to copy source code from other existing projects. There-
fore, to verify that each entry is unique, it was scanned for similar
entries and replicas were removed.

The corpus thus contains two types of records: those that con-
tain examples with OpenMP directives, and those that do not. Each
record contains three files: (1) the code segment relevant to the
directive—the loop segment along with, if found, implementations
of functions called inside the loop segment; (2) the OpenMP direc-
tive; and (3) a pickle file containing the code segment’s and the
OpenMP directive’s AST (generated from pycparser). In most cases,
the context of the loop segment is sufficient to produce an OpenMP
directive. Moreover, in order to create adequate labeling, we needed
to create a clear link between the feature (code) and label (directive).
Therefore, the database currently includes the OpenMP directive
and its corresponding loop. Figure 2 summarizes the creation pro-
cess of the corpus.

step is far from being solved and is the main reason why S2S com-
pilers are negatively perceived [58]. Nonetheless, due to the recent
innovations in ML, specifically in CLP [76], and widely available
source code databases, the possibility of replacing the AST step and
suggesting code segments that can benefit from OpenMP directives
with newer techniques rises.

2.1 Research Objectives
In order to evaluate and explore the possibility of replacing the stan-
dard S2S compilers with NLP models, we formulate two research
questions and focus our work on answering them.

2.1.1 RQ1: Is this Code Parallelizable? First, we wish to identify
whether a given code segment is in need of an OpenMP directive.
As stated in [33], one of the pitfalls of S2S compilers, and of intro-
ducing OpenMP directives generally, is the unnecessary generation
of OpenMP directives in cases where parallelization is counter-
productive. For example, if a for-loop construct contains a small
amount of computer-operations (such as a low iteration count),
the overhead of spawning OpenMP threads outweighs the gain to
be made by parallelizing the construct. However, in some cases
such as array initialization, introducing OpenMP directives might
be beneficial in speeding up other OpenMP directives, due to the
first-touch policy in cc-NUMA [48].

2.1.2 RQ2: Does this Parallelizable Code Need a Certain Clause?
One of the problems in executing a program concurrently is possible
data races, which under some circumstances cause the code to be
inconsistent across separate runs. In some cases, these problems
can be eliminated by adding specific OpenMP clauses [64], such
as the private clause and the reduction clause. Thus, we define an
additional task of classifying the need for a private or a reduction
clause affecting any of the variables in a given code segment which
we already know to be amenable to parallelization.

Together, these tasks complement each other and can be seen as
the beginning of creating a full pipeline which generates OpenMP
directives automatically. In the meantime, due to the negligible
inference time (contrary to S2S compilers), it can be used as an
immediate “advisor” for developers to identify locations that can
benefit from an OpenMP directive—even with partial or on-going
programming, which is impossible with current S2S compilers.
Moreover, the model and the S2S compilers can be incorporated
such that in cases both the model and the S2S compilers agree on
a directive, it will remain. Thus, verifying the correctness of the
directive and the necessity.

3 DATA
3.1 Corpus
In order to train and evaluate models on the task of code paralleliza-
tion, we created a database, or corpus, of code files, which we call
Open-OMP1. We queried github.com, the most well-known code
repository housing website, via a script named github-clone-all2,
which allows searching for source files by repository. We extracted
only C files from repositories containing the phrase “OpenMP” in

1https://github.com/pragformer/PragFormer/blob/main/Open_OMP.tar.gz
2http://github.com/rhysd/github-clone-all

Figure 2: Overview of the workflow for creating the data-
base.

4

FilesNoYesDisregardDecisionOpenMP"Negative"labels"Positive"labelsCodeSnippetscode.cpragma.cpickle.pklRecordLearning to Parallelize in a Shared-Memory Environment with Transformers

, ,

We present statistics of our database by: (1) the different OpenMP
directives count (Table 3), (2) the number of lines count of the code
snippets (Table 4), and (3) the domain distribution of the code snip-
pets (Figure 3): if the code snippet doesn’t contain a README file
(thus, unknown domain), the README file contains the keyword
’benchmark’ or ’testing’ and the default case which is assumed to be
a generic application. As can be seen, most of the database contains
small to medium code snippets, with diverse OpenMP directives,
and mostly from real applications.3

Description

Amount

Total code snippets
For loops with OpenMP directives
Schedule static
Schedule dynamic
Reduction
Private

17,013
7,630
7,256
374
1,455
3,403

Table 3: Statistics of the OpenMP directives on the raw data-
base.

Line Count Amount
< 10
11–50
51–100
> 100

9,865
5,824
724
600

Table 4: Code snippet lengths in the raw database.

Unknown (no README)

Benchmark

16.5%

33.5%

7%

Testing

43%

Generic Application

Figure 3: The distribution of OpenMP snippets sources.

3.2 Datasets
An essential component of any ML model built for a supervised
task such as ours is the dataset, a group of 〈input, label〉 instances
split into three sets—training, validation, and test. The training set
contains examples used for training the model (updating internal
parameter values); the validation (sometimes called development)
set contains examples used for making model-level decisions that
cannot be tuned per instance (termed hyperparameters) and helps

3Full github repositories url’s by domain distribution: https://github.com/pragformer/
PragFormer/blob/main/log_repositories_readme.txt

5

avoid over-fitting the model being trained on the specific idiosyn-
crasies of training set examples; and the test set contains examples
used to evaluate the effectiveness of the overall resulting model
against other systems. We thus divided our data into the required
splits using a 80%–10%–10% ratio.

We created two different datasets for our task, one for each
research question (or task). The first dataset contains code sam-
ples from the database which have (or don’t have) corresponding
OpenMP directives, labeled positive (or negative, respectively), and
is used to decide whether an OpenMP directive is needed for a
given code snippet. The second dataset contains only instances
known to have OpenMP directives in the database, and is used for
the task of classifying private and reduction OpenMP clauses. In
Table 5 we present the dataset sizes. The train/validation/test splits
in both datasets were performed randomly at the instance level,
while maintaining a balanced positive-negative label distribution
in each dataset.

Dataset

Directive Clause

Training
Validation
Test

14,442
1,274
1,274

6,482
572
572

Table 5: Amount of examples in each dataset for the
OpenMP directive and clause classification.

4 PRAGFORMER
We propose a novel model, PragFormer, for identifying OpenMP
directives and clauses. PragFormer is based on the transformer
architecture [86], and the workflow of applying it is presented in
Figure 1. This pipeline resembles the workflow of program compre-
hension models [76] and of S2S compilers: (1) tokenize the source
code in order to allow its ingestion by the transformer model (§4.2);
(2) train the transformer-based model (§4.1) to classify OpenMP
directives from the data segments in the corpus; (3) and evalu-
ate the predicted OpenMP directives, as described in §5. In this
work, we train models for three separate tasks: OpenMP directive
identification; private clause identification; and reduction clause
identification.

The flexible form of machine learning systems allows us to initial-
ize the same model architecture for all three tasks, which we cast as
classification problems (whether or not a directive/clause is needed),
letting only the values of internal numeric parameters to diverge
between the individual task models through backpropagation as
the training procedure progresses.

4.1 Model
PragFormer is composed of a transformer model followed by a
fully-connected (FC) layer which performs classification. The trans-
former architecture [86] has gained popularity due to its impressive
ability to leverage data from vast unlabeled sequence resources in a
pre-training phase and apply it to a large range of end tasks, a prop-
erty known as transfer learning. At a high level, the transformer
is a model which receives a sequence of vectors as its input and

, ,

Re’em Harel, Yuval Pinter, and Gal Oren

Representation Example

Text

for (i = 0; i < len; i++) a[i] = i;

Replaced-Text

for (var0 = 0; var0 < var1; var0++) arr0[var0] = var0;

AST

For: Assignment: = ID: i Constant: int, 0 BinaryOp: < ID: i ID: len UnaryOp: p++ ID: i Assignment: = ArrayRef:
ID: a ID: i ID: i

Replaced-AST

For: Assignment: = ID: var0 Constant: int, 0 BinaryOp: < ID: var0 ID: var1 UnaryOp: p++ ID: var0 Assignment:
= ArrayRef: ID: arr0 ID: var0 ID: var0

Table 6: Examples of the different code representations considered.

passes it through a massively-parameterized encoder module which
outputs a sequence of contextualized vectors of the same length.
These, in turn, are fed into a decoder which can either generate a
sequence of symbols from a pre-set vocabulary (as in the case of,
for example, machine translation), or perform a classification task
such as the one at hand (or, for example, named entity recognition).
The encoder, in turn, is made up of identical layers of parame-
terized architectural blocks, each producing its own sequence of
representation vectors. In theory, output vectors become more and
more context-dependent after each layer’s treatment. The main
mechanism allowing this behaviour is the self-attention compo-
nent within the blocks. This component calculates a numeric score
for each position in the input sequence with respect to the other
ones, based on their input vectors and on parameterized projection
matrices. The score then determines the level of consideration in
preparing this position’s output for ingestion by the next layer. For
example, in a passage over the English sentence “The house does
not have a well”, a self-attention block may compute a high score in
the position corresponding to ‘well’ with respect to that of ‘house’,
resulting in an output representation more approximating the “dug-
out structure” sense of ‘well’ as opposed to that of the “in a good
way” sense (and others). In a decoder, the attention mechanism is
also applied to items from the input, enabling cross-attention [11].
We refer the reader to [86] for more details.

In the realm of source code, attention scores may represent how
much influence each of the other variables or statements have over
a given input variable’s contextualized vector. This enables even
elements which are far away, position-wise, to affect any output; a
typical transformer model’s sequence length is capped at hundreds
of tokens.

We implement PragFormer based on the pre-trained state-of-
the-art DeepSCC [91] model. Similarly to our task, which is iden-
tifying and advising the need for an OpenMP directive, DeepSCC
was trained with the aim of classifying the programming language
of a source code. In DeepSCC the self-attention mechanism helps
focusing on specific scopes (of variables or operations) to determine
the program language. This mechanism is crucial in our task, as
the identification of the need of an OpenMP directive is hinted
in long-range dependencies in the source code, which the self-
attention mechanism of the transformer model is most suited to.
As part of our research focus, we want to examine whether this
code-focused task provides adequate initialization for training our
model on our parallelization tasks. As such, it can be seen as an

instance of transfer learning into a low-resource scenario, a tech-
nique shown to be effective in many applications of transformers
in NLP [30, 40, 44, 75, 95]. DeepSCC was created by fine-tuning
a pre-trained RoBERTa [52] model on a large corpus of source
code files (including C and C++, which approximate our corpus)
via the masked language model (MLM) objective [83]. In MLM, a
random subset of tokens in the input are obscured from the en-
coder (“masked”), and in a self-supervised manner, the model tries
to predict the masked tokens based on the encoder’s output, by
selecting it from its vocabulary. Moreover, the attention mechanism
in DeepSCC learns helps the model emphasize specific Ultimately,
DeepSCC manages to fine-tune the parameters of the RoBERTa
model, which were originally trained to process English text, to be
better suited for source code as well as suppressing the results of
other models tasked with the same objective (achieving 87% accu-
racy). Thus, we believe DeppSCC provides adequate initialization
and providing an apt starting point for PragFormer.

Following the encoder’s operation over source code input, the
output vectors are fed into a FC layer which predicts a binary label
based on the individual task (identification of a need for a directive,
or for of a specific clause) through a softmax layer which transforms
the real-valued scores output from the FC layer into probabilities. If
the probability is computed to be over 0.5, PragFormer predicts a
positive outcome. During training, a cross-entropy loss is computed
based on the predicted probability of the correct label, and this
value is backpropagated through the entire network (including the
encoder layers and the input vectors, also known as embeddings)
to update its parameters. The binary cross-entropy loss for a single
input instance 𝑥 with the (true) label 𝑦 ∈ {0, 1} is given by:

L𝐵𝐶𝐸 (𝑥, 𝑦) = −(𝑦 log(𝑝 (𝑥)) + (1 − 𝑦) log(1 − 𝑝 (𝑥))),

(1)

where 𝑝 is the probability assigned by the final softmax layer to the
positive label.

4.2 Representing the Code
Raw source code cannot serve as an input to an ML model; in-
stead, transforming the code to a sequence of tokens from a pre-set
vocabulary is needed. This transformation process, known as tok-
enization, is then followed by associating each keyed token with a
numerical vector (embedding) which is used as input to the model,
to be modified along with the rest of the model’s parameters during
training.

6

Learning to Parallelize in a Shared-Memory Environment with Transformers

, ,

One possible way to tokenize source code is as if it were text.
This way, each keyword, operator, identifier and symbol would
be assigned its own token [76]. However, due to the structured
nature of code, previous work [41, 82] suggests that tokenizing the
abstract representation of the code (such as AST or CDG) rather
than the raw code itself can be helpful in order to perform source
code analysis. Representing the code this way has been shown to
improve results in various code understanding tasks compared to a
lexical representation [32, 37, 50]. Another line of work proposes
a combination of AST and raw code, either by embedding them
both separately and combining their vectors inside the model, or by
mixing them directly at the textual level, finding it to be beneficial
over using each kind separately [38, 51].

Although other source code representations achieve proper word
embeddings, such as word2vec [19], IR2vec [87] and Programl [20],
the tokenizer and the pre-trained model (DeepSCC and RoBERTa)
were trained exclusively on natural language text. Therefore, ex-
ploiting the transfer learning property with different representa-
tions other than text will likely produce poor results (as exem-
plified with the AST representation in this work). Overcoming
this limit can be achieved by training the tokenizer and the base
model (i.e., RoBERTa) from scratch with a large corpus of the ap-
propriate representation. Therefore, we argue that it might be eas-
ier for PragFormer to understand the dependencies from raw
source code, rather than from its AST representation. We con-
sider both approaches—AST representation and raw code (‘text’)
representation—as input for the model’s tokenizer.

A frequent problem in applying pre-set vocabulary tokenizers to
any textual input is the representation of strings not present in the
vocabulary, known as out-of-vocabulary items, or OOVs [16]. When
encountered, OOVs are either broken down into manageable units,
even at the character level, or mapped to some default catch-all to-
ken (often styled <UNK> for “unknown”) which encumbers learning
useful representations for their semantics. In the code domain, the
OOV problem can manifest itself mainly through identifier naming
conventions, which vary greatly across organizations and commu-
nities, with many developers opting for their own idiosyncratic
names for variables and functions. This has been shown to harm
model performance in code understanding tasks [34, 37, 38]. While
most previous work [34, 37, 38, 50] has opted for the <UNK> solu-
tion, classifying OpenMP directives does not require knowledge of
individual identifier names, and we thus replace them during a pre-
processing step with predefined indexed words such as var1, var2,
reducing the required vocabulary size for the model’s tokenizer
and avoiding many OOVs. We hypothesize that this step might
also provide the effect of learning more meaningful representations
for our canonical name tokens, since they are now shared across
training instances.

Altogether, we experiment over four different code represen-
tations for our model: text, text with replaced identifiers (R-Text),
AST, and AST with replaced identifiers (R-AST ). As the AST has a
tree structure, it cannot be fed directly to a model that expects a
sequence as an input. Therefore, we obtain an adequate representa-
tion by applying the DFS algorithm to the AST.

An example of each of the four representations is shown in Ta-
ble 6. In Table 7 we present statistics at the token level for our dataset
given each of the pre-tokenization schemes. Note the distinction

between token counts (individual symbols, including duplicates)
and symbol types (unique symbols, an entry in a dictionary/vocabu-
lary) [55]. In terms of current transformer models, such vocabulary
sizes and OOV token (or type) counts as the ones present in all
representations are relatively small, suggesting that the raw textual
representation might be sufficient for obtaining good results. In
addition, the average number of tokens per code snippet is lower
for text representation, since the AST representation adds words
used to describe operational logic.

Text R-Text AST R-AST

Train vocab size
OOV types
Avg. length

6,427
398
33

2,424
226
30

5,261
348
37

3,409
309
35

Table 7: Type-level corpus statistics. ‘OOV types’ refers to
the number of symbol types in the validation and test sets
which are not found in the training set. ‘Avg. length’ is the
average amount of tokens in a code segment (for-loop).

4.3 Implementation
In PragFormer, we use the tokenizer from DeepSCC-RoBERTa.4
The largest token length in a code snippet was 110, and so it was set
as the maximum length for the input to the model. We fine-tuned
the DeepSCC model in PragFormer to our dataset, meaning the
internal parameters already pre-trained on the language modeling
task were also updated during our model’s training phase. The
FC layer in PragFormer contains two dense layers with a ReLU
activation function between them. We implemented dropout as a
regularization strategy to avoid overfitting the training set [79]. We
updated model parameters using backpropagation, implemented
via the AdamW gradient descent optimizer [53].

5 RESULTS
To evaluate the effectiveness of PragFormer, we conduct several
experiments over our extracted dataset (§3). We first determine the
best code representation for the tasks (cf. §4.2), then follow up by
comparing the best-represented PragFormer variant against other
models, namely an S2S compiler and a standard ML model.

5.1 Code Representation
A crucial step in performing source code analysis with ML models
is representing the code so that the model can extract the maximum
information from it and infer upon it. We compare the performance
of models trained on each of the four code representations pre-
sented in §4.2, namely: text, replaced text, AST, and replaced AST.
The training process was conducted on the OpenMP directive clas-
sification dataset presented in §3.2, and we calculated the accuracy
of each trained model on the validation set.

In Figure 4 we present the accuracy scores as a function of the
number of completed training epochs (passes over the full training
set). Both versions of raw text representation obtain better results

4https://huggingface.co/NTUYG/DeepSCC-RoBERTa

7

, ,

Re’em Harel, Yuval Pinter, and Gal Oren

than their corresponding AST representations, inconsistent with
findings for other code-related tasks mentioned in §4.2. Moreover,
there is a slight advantage (of about 2%) obtained by the simple text
representation over the replaced text representation. We believe
this can be explained by an implicit pattern present in parallelizable
loops: they tend to have the same “unique” naming convention. For
example, iteration variables tend to be named i, j, k, and A, B, C, vec,
arr as matrices and vectors. In addition, consider a file that contains
multiple OpenMP directives; in such a case, the parallelized loops
are likely operating on the same variables. Therefore, the model
obtains better results by recognizing these “unique” names. We note
that these results also appeared in our more basic BoW setup (which
will be described in §5.2), suggesting our hypothesis extends beyond
the specific transformer architecture. Nevertheless, we hypothesize
that AST representations may be more useful for models whose
pre-training step includes introduction of this syntax, and leave
this experiment to future work.

We provide the model’s internal calculations of objective loss
(Eq. 1) over the training set and the validation set in Figure 5 and
Figure 6, respectively. This analysis assists us in determining the
extent to which the model is overfitting—tuning its parameters to
fit spurious relations in the training set which do not generalize
to the untuned-upon validation set. Since the validation loss curve
(marked by a dashed black line) converges after 7–9 epochs, we
choose to use the models trained up to those points, and report
final accuracy scores of 81% for raw text, 78% for replaced text, 76%
for AST, and 69% for replaced AST. We continue our experiments
with the text variant.

5.2 OpenMP Directive Classification
We compare PragFormer to two divergent methods on the task of
directive classification (RQ1): given a code snippet, is it possible to
add an OpenMP pragma to parallelize it? Our first competitor is a
statistical trained Bag-of-Words (BoW) [94] model with a logistic
regression classifier. In BoW, which has also been used in computer
vision models [49, 77], individual tokens in the input text (code)
are counted and populated in a count vector containing only non-
negative integers. The order and structure of the text are not taken
into account. The classifier learns a weight vector, where each entry
corresponds to a word in the vocabulary, multiplied to produce a
score for each instance, traversing over a training set using gradi-
ent descent, as is standard in ML. The second system we evaluate
against is the state-of-the-art S2S compiler ComPar [60, 61]. Com-
Par is the most suitable for this task, as it incorporates several S2S
compilers together (Par4All [4], AutoPar [71] and Cetus [2]) and
combines their results to produce the best OpenMP directive. In
practice, when we applied the system to our code segments, we
found that only Cetus managed to compile the examples success-
fully. While ComPar generates the directives as a whole, for our
current experiment we only consider the binary fact of whether or
not it managed to insert an OpenMP directive into the code.

As the textual representation produced the best results for Prag-
Former during the validation setup (§5.1), it was chosen as the

8

Figure 4: Accuracy of the four source code representations
as training epochs increase.

Figure 5: Average loss for the training set in models trained
on the four source code representations.

Figure 6: Average loss for the validation set in models
trained on the four source code representations.

0246810# Epochs0.600.650.700.750.800.85AccuracyTextReplaced TextASTReplaced AST0246810# Epochs0.20.30.40.50.60.7Train LossTextReplaced TextASTReplaced AST0246810# Epochs0.400.450.500.550.600.650.70Valid LossTextReplaced TextASTReplaced ASTLearning to Parallelize in a Shared-Memory Environment with Transformers

, ,

exclusive representation for PragFormer and BoW. We trained
PragFormer and the BoW model over the training set (§3.2). Being
a deterministic compiler, ComPar doesn’t need a training step; thus,
it was executed on the test set exclusively. However, out of the 1,274
examples in the test set, ComPar failed completely to compile 221
code instances due to complex structure definitions and operations
unrecognized by its internal parser, emphasizing its limited robust-
ness. We thus followed a fall-back strategy that considers these
cases as a negative outcome. Nevertheless, we report that similar
results in all three metrics are obtained by conducting the same
experiment with the 1,053 examples for PragFormer, BoW, and
ComPar.

For evaluation, we report precision, recall, and F1 score [28] on
the test set as the performance measurements. These performance
measurements are the standard method of evaluating classification
problems: precision is the ratio between the number of possible
directives predicted correctly (true positives) and the total number
of directives predicted (all positives); recall is the ratio between
the number of directives predicted correctly (true positives) and
number of cases where directives are possible (true positives + false
negatives). F1 is the harmonic mean between precision and recall,
penalizing models which do not balance well between the two
measures.

Precision Recall

F1 Accuracy

PragFormer
BoW + Logistic
ComPar

0.80
0.73
0.51

0.81
0.74
0.56

0.80
0.73
0.36

0.80
0.74
0.5

Table 8: Comparison between PragFormer and the com-
peting systems on the task of identifying the need for an
OpenMP directive.

In Table 8 we present the performance of PragFormer, BoW
and ComPar on the directive classification task. The best results are
achieved by PragFormer according to all measurements. We find
the main reason behind ComPar’s poor performance in detecting
segments that can be parallelized to be the lack of association of
functions, macros, and structure definitions and implementations
in the code segments, a common situation for large code projects.
In addition, we found that in loops with a low iteration count,
Cetus didn’t insert an OpenMP directive, although the example did
contain an OpenMP directive—the benefit of an OpenMP directive
in such cases is presented in §2.1.

0.4

2

h
t
g
n
e
L

40
30
20
10

6

2

4

6

8

10

Error rate %

Figure 7: Prediction error rate by example length

Figure 7 presents the error rate as a function of the code’s length.
As seen, more than 80% of the incorrect predictions (errors) of Prag-
Former occurred for code’s with a length lower than 20. Moreover,

9

only 10 examples with a length of more than 50 were predicted as
incorrect. This might indicate that the length does not necessarily
affect the decision of the model. In §5.4 we provide several exam-
ples showing that the attention mechanism of the model focuses on
variables, function names and statements rather than other factors
such as line count.

5.3 OpenMP Clause Classification
Similar to the OpenMP directive classification task, in classifying
private and reduction clauses (RQ2), we compared PragFormer to
BoW and ComPar on the same three performance metrics, this
time over the clause dataset (§3.2) with balanced labels. We note
that in a real-world application of the compared systems, directive
classification will take place before clause identification, cascading
errors from the previous step to this one, making our separated
evaluation setup beneficial to the models which performed worse
than PragFormer in the directive task.

We present the results for identifying the need for a private
clause in Table 9 and the need for a reduction clause in Table 10.
ComPar does not perform well on precision in predicting private
clauses mostly due to the default behavior of this clause, calling for
its application to the iteration variable while applying the shared
clause to the rest of the variables. As a result, most the developers
do not explicitly insert a private(i) statement, while ComPar does.
Having said that, ComPar also scores low on recall, meaning that
the model misses many cases where a private clause is possible.

On the reduction clause classification task, ComPar manages to
obtain a high score on the precision measurement, meaning that
most of its positive predictions were correct. This indicates that
the conservative deterministic nature of ComPar produces correct
directives if it manages to generate a directive at all. In contrast to
the high precision score, and similar to its recall performance in the
private task, ComPar scores low on recall, meaning that the model
misses many cases where a reduction clause is possible. In contrast,
PragFormer produces both excellent recall and precision for both
clauses, demonstrating a good balance between finding many true
cases without allowing many false predictions to sift through.

Precision Recall

F1 Accuracy

PragFormer
BoW + Logistic
ComPar

0.86
0.79
0.56

0.85
0.78
0.51

0.86
0.78
0.40

0.85
0.79
0.56

Table 9: Performance of models on identifying the need for
a private clause.

11

Precision Recall

F1 Accuracy

PragFormer
BoW + Logistic
ComPar

0.89
0.78
0.92

0.87
0.78
0.52

0.87
0.77
0.46

0.87
0.78
0.79

Table 10: Performance of models on identifying the need for
a reduction clause.

, ,

Re’em Harel, Yuval Pinter, and Gal Oren

5.4 Benchmarks and Explainability
In order to test the generality of PragFormer, we apply it to two
existing dedicated OpenMP benchmarks that do not appear in Open-
OMP: PolyBench [5, 6] and the Standard Performance Evaluation
Corporation (SPEC) [8, 42]. The PolyBench test suite contains a
collection of 30 compute-intensive benchmarks and is used, among
other things, as a shared-memory environment benchmark [29, 67].
SPEC contains several benchmarks designed to evaluate the perfor-
mance of shared-memory architectures. Among the benchmarks in
SPEC are SPEC-ACCEL [7] and SPEC-OMP [9], of which we only
use the former. We opted not to evaluate on SPEC-ACCEL as it
shares many similarities with the NAS parallel benchmark (such as
CG, BT, EP test-suites) already included in Open-OMP. PolyBench
contains 64 snippets of code with OpenMP directives and 83 with-
out. SPEC-OMP contains 113 snippets of code with OpenMP and
174 without 5. Table 11 presents the results of PragFormer and
ComPar on PolyBench and SPEC-OMP. However, ComPar failed
to parse 287 snippets from the SPEC-OMP benchmark mainly due
to unrecognized keywords, such as register. Thus, we exclude them
from ComPar’s results. The results of PragFormer are comparable
to, and even slightly better than, the ones over the Open-OMP test
set.

Precision Recall

F1 Acc’

PragFormer Poly
ComPar Poly
PragFormer SPEC-OMP
ComPar SPEC-OMP

0.93
0.43
0.81
0.76

0.93
0.43
0.80
0.75

0.93
0.43
0.80
0.74

0.93
0.43
0.80
0.75

Table 11: The performance score of PragFormer and Com-
Par on identifying the need for an OpenMP directive on the
PolyBench and SPEC-OMP benchmarks.

In Table 12 we present representative examples and the cor-
responding prediction of PragFormer over the benchmark tests.
Explaining and understanding the reason behind a model’s pre-
diction is a difficult task. Nonetheless, there are many algorithms
that attempt to give an explanation or an intuition for classifiers’
decisions [10], such as LIME [73]. LIME studies the connections
between keywords (tokens) of an input and the change in the pre-
diction of a model once they are removed. Finally, LIME presents
the probability that the keyword affected the prediction. In our
case, this might indicate how PragFormer focuses on keywords
and statements. Thus, in order to gain intuition for the predicted
outcome of PragFormer, we applied LIME on the four examples
in Table 12. The output of LIME is presented in Figure 8.

The first example presents a code snippet taken from PolyBench,
in which PragFormer managed to identify correctly the need for
an OpenMP directive. In this example, LIME pinpoints the variables
j, POLYBENCH_LOOP_BOUND, A and y_1 as the main contributors
to the decision of the model. This indicates that PragFormer fo-
cuses on the loop variable and the arrays, as it should. The second
example presents a snippet taken from PolyBench and contains an
I/O operation, thus, there is no OpenMP directive. LIME identifies
the keyword fprintf and stderr as the reason why the model classi-
fied the snippet without OpenMP. To verify this claim, we removed

5Sources: https://github.com/pragformer/PragFormer/blob/main/DB_TEST.tar.gz

10

these two keywords, and PragFormer predicts an OpenMP direc-
tive. This probably indicates that PragFormer understands that
these specific keywords are why there is no need for an OpenMP
directive. The third example, taken from SPEC-OMP, contains an
OpenMP directive that PragFormer fails to predict correctly. By
observing LIME’s result, the two variables, ssize_t and IndexPacket,
are the main reason to its incorrect prediction. After removing both
variables, PragFormer predicts an OpenMP directive. This might
be due to the model’s unfamiliarity with these keywords and how
they affect the code. The fourth example, taken from PolyBench,
contains an assignment into three arrays. While PragFormer pre-
dicts (correctly) that there should be an OpenMP directive, the
example does not. As with the first example, LIME pinpoints the
loop-variable j and the arrays as the reason for its prediction. This
further indicates that the model does focus on the correct variables
while making its decision.

In summary, it appears most likely that PragFormer focuses on
the core variables and statements in order to predict its directive.
However, there are still cases in which it fails to predict correctly,
possibly due to unfamiliar keywords or statements.

6 CONCLUSION & FUTURE WORK
In this work, we presented a novel data-driven model, named Prag-
Former, that aims to classify the need of an OpenMP directive
and OpenMP clauses. We also created a corpus of OpenMP code
snippets which is, as far as we know, the first of its nature. The
corpus contains more than 17,000 code snippets with reliable labels
of whether OpenMP directives should or should not be assigned
to them. We invite the community to use our corpus for analyzing
parallelization use-cases and as a development bed for ML models
learning to parallelize code.

We trained PragFormer model variants which ingest different
representations of code, finding that representing the source code as
raw text obtains the best results. We then found that PragFormer
manages to perform fundamental tasks on the roadmap to paral-
lelization better than two competitors—a top-shelf deterministic
S2S compiler and a lightweight text-aware ML model.

In most cases, the context of the loop itself is enough to gener-
ate an OpenMP directive. However, in order to push performance
further, it is effective to include context from previous and next
lines of code. Thus, we will expand the database that it will con-
tain these extra lines of code. In addition, in order to improve the
effectiveness of PragFormer, we plan to explore additional code
representations such as the structured based traversal (SBT) rep-
resentation [37, 38, 50, 51], combination of text and AST, IR2vec,
Programl and more. We also plan to expand the corpus by searching
other keywords on Github and other resources, as well as including
code snippets from C++ and Fortran as well as C. Most importantly,
PragFormer provides a crucial step towards a more robust frame-
work based on NLP techniques that will eventually be able to insert
OpenMP directives automatically with high fidelity, contributing
to making the world’s code more efficient.

Evaluating the effectiveness of inserting OpenMP directives au-
tomatically via PragFormer will be done by comparing it to the
state-of-the-art S2S compiler—ComPar, which produces the optimal
OpenMP directive generated by three S2S compilers. In addition,

Learning to Parallelize in a Shared-Memory Environment with Transformers

, ,

Example

Directive

PragFormer prediction

for (i = 0; i < POLYBENCH_LOOP_BOUND(4000, n); i++)

for (j = 0; j < POLYBENCH_LOOP_BOUND(4000, n); j++)

#pragma omp parallel for\\
private(j)

With OpenMP

x1[i] = x1[i] + (A[i][j] * y_1[j]);

for (i = 0; i < n; i++) {

fprintf(stderr, "%0.2lf ", x[i]);
if ((i % 20) == 0)

fprintf(stderr, "\n");}

Without OpenMP

Without OpenMP

for (i = 0; i < ((ssize_t) image->colors); i++)
image->colormap[i].opacity = (IndexPacket) i;

#pragma omp parallel for\\
schedule(dynamic,4)

Without OpenMP

for (i = 0; i < maxgrid; i++)

for (j = 0; j < maxgrid; j++){

sum_tang[i][j] = (int) ((i + 1) * (j + 1));
mean[i][j] = (((int) i) - j) / maxgrid;
path[i][j] = (((int) i) * (j - 1)) / maxgrid; }

Without OpenMP

With OpenMP

Table 12: Classification examples and PragFormer prediction.

1

2

3

1

2

3

4

1

2

1

2

3

4

5

Figure 8: LIME’s output on the four examples in Table 12. From left to right, LIME presents the label distributions given to
the instance by the model, followed by the most influential tokens and their weighted importance on the decision made, next
highlighting their location in the input text.

11

, ,

Re’em Harel, Yuval Pinter, and Gal Oren

it can fine-tune the OpenMP directives by inserting the schedul-
ing construct and executing the source code to determine the best
combination. Thus, ComPar is suitable for evaluating the directives
inserted by PragFormer and for comparing their execution times.
Once we have proven the capacity of transformers learning to
generate and identify OpenMP parallelization, we intend to repli-
cate this process for other shared memory parallelization schemes
for more challenging frameworks such as CUDA and OpenCL. In
addition, we can explore other parallelization schemes such as the
MPI scheme for distributed memory [78]. Finally, combining all the
models may result in a framework capable of producing an MPI+X
parallelization scheme.

ACKNOWLEDGMENTS
This research was partially supported by the Israeli Council for
Higher Education (CHE) via the Data Science Research Center, Ben-
Gurion University of the Negev, Israel, and the Lynn and William
Frankel Center for Computer Science. Computational support was
provided by the NegevHPC project [3].

REFERENCES
[1] [n. d.]. Automatic Parallelism and Data Dependency. https://web.archive.org/
web/20140714111836/http://blitzprog.org/posts/automatic-parallelism-and-
data-dependency. [Online].

[2] [n. d.]. Cetus homepage. https://engineering.purdue.edu/Cetus/. [Online].
[3] [n. d.]. NegevHPC Project. https://www.negevhpc.com. [Online].
[4] [n. d.]. Par4All homepage. http://par4all.github.io/. [Online].
[5] [n. d.]. PolyBench Benchmarks. https://web.cse.ohio-state.edu/~pouchet.2/

software/polybench/. [Online].

[6] [n. d.]. PolyBench Github with OpenMP. https://github.com/cavazos-lab/

PolyBench-ACC. [Online].

[7] [n. d.]. SPEC-ACCEL website. https://www.spec.org/accel/. [Online].
[8] [n. d.]. SPEC-HPG. https://www.spec.org/order.html. [Online].
[9] [n. d.]. SPEC-OMP2012 website. https://www.spec.org/omp2012/. [Online].
[10] Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: a
survey on explainable artificial intelligence (XAI). IEEE access 6 (2018), 52138–
52160.

[11] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-
chine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).

[12] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-

document transformer. arXiv preprint arXiv:2004.05150 (2020).

[13] Eli Bendersky et al. 2010. Pycparser.
[14] K.H. Bennett, V.T. Rajlich, and N. Wilde. 2002. Software Evolution and the Staged
Model of the Software Lifecycle. Advances in Computers, Vol. 56. Elsevier, 1–54.
https://doi.org/10.1016/S0065-2458(02)80003-1

[15] Stephen Blair-Chappell and Andrew Stokes. 2012. Parallel programming with

intel parallel studio XE. John Wiley & Sons.

[16] Eric Brill. 1995. Transformation-Based Error-Driven Learning and Natural Lan-
guage Processing: A Case Study in Part-of-Speech Tagging. Computational
Linguistics 21, 4 (1995), 543–565. https://aclanthology.org/J95-4004

[17] David C Brock and Gordon E Moore. 2006. Understanding Moore’s law: four

decades of innovation. Chemical Heritage Foundation.

[18] Xuan Chen and Shun Long. 2009. Adaptive multi-versioning for OpenMP paral-
lelization via machine learning. In 2009 15th International Conference on Parallel
and Distributed Systems. IEEE, 907–912.

[19] Kenneth Ward Church. 2017. Word2Vec. Natural Language Engineering 23, 1

(2017), 155–162.

[20] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, and Hugh
Leather. 2020. Programl: Graph-based deep learning for program optimization
and analysis. arXiv preprint arXiv:2003.10536 (2020).

[21] Leonardo Dagum and Ramesh Menon. 1998. OpenMP: an industry standard API
for shared-memory programming. IEEE computational science and engineering 5,
1 (1998), 46–55.

[22] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a
Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Associ-
ation for Computational Linguistics. Association for Computational Linguistics,
Florence, Italy, 2978–2988. https://doi.org/10.18653/v1/P19-1285

12

[23] Robert H Dennard, Fritz H Gaensslen, V Leo Rideout, Ernest Bassous, and Andre R
LeBlanc. 1974. Design of ion-implanted MOSFET’s with very small physical
dimensions. IEEE Journal of Solid-State Circuits 9, 5 (1974), 256–268.

[24] Nicholas DiPasquale, T Way, and V Gehlot. 2005. Comparative survey of ap-

proaches to automatic parallelization. MASPLAS’05 (2005).

[25] Jacob Eisenstein. 2019. Natural language processing.
[26] Jeffrey L Elman. 1991. Distributed representations, simple recurrent networks,

and grammatical structure. Machine learning 7, 2 (1991), 195–225.

[27] Ronald Fagin and Moshe Y Vardi. 1984. The theory of data dependencies: a survey.

IBM Thomas J. Watson Research Division.

[28] Cyril Goutte and Eric Gaussier. 2005. A probabilistic interpretation of precision,
recall and F-score, with implication for evaluation. In European conference on
information retrieval. Springer, 345–359.

[29] Scott Grauer-Gray, Lifan Xu, Robert Searles, Sudhee Ayalasomayajula, and John
Cavazos. 2012. Auto-tuning a high-level language targeted to GPU codes. In 2012
innovative parallel computing (InPar). Ieee, 1–10.

[30] Xiaochuang Han and Jacob Eisenstein. 2019. Unsupervised Domain Adaptation
of Contextualized Embeddings for Sequence Labeling. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Association for Computational Linguistics, Hong Kong, China, 4238–4248. https:
//doi.org/10.18653/v1/D19-1433

[31] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
proved automatic summarization of subroutines via attention to file context. In
Proceedings of the 17th International Conference on Mining Software Repositories.
300–310.

[32] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Improved
Automatic Summarization of Subroutines via Attention to File Context. CoRR
abs/2004.04881 (2020). arXiv:2004.04881 https://arxiv.org/abs/2004.04881
[33] Re’em Harel, Idan Mosseri, Harel Levin, Lee-or Alon, Matan Rusanovsky, and
Gal Oren. 2020. Source-to-source parallelization compilers for scientific shared-
memory multi-core and accelerated multiprocessing: analysis, pitfalls, enhance-
ment and potential. International Journal of Parallel Programming 48, 1 (2020),
1–31.

[34] Vincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks
the best choice for modeling source code?. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering. 763–773.

[35] Álvaro Brandón Hernández, María S Perez, Smrati Gupta, and Victor Muntés-
Mulero. 2018. Using machine learning to optimize parallelism in big data appli-
cations. Future Generation Computer Systems 86 (2018), 1076–1092.

[36] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural

computation 9, 8 (1997), 1735–1780.

[37] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation. In 2018 IEEE/ACM 26th International Conference on Program Comprehension
(ICPC). IEEE, 200–20010.

[38] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generation with hybrid lexical and syntactical information. Empirical Software
Engineering 25, 3 (2020), 2179–2217.

[39] Ayad Tareq Imam and Ayman Jameel Alnsour. 2020. The Use of Natural Lan-
guage Processing Approach for Converting Pseudo Code to C# Code. Journal of
Intelligent Systems 29, 1 (2020), 1388–1407.

[40] Aizhan Imankulova, Raj Dabre, Atsushi Fujita, and Kenji Imamura. 2019. Exploit-
ing Out-of-Domain Parallel Data through Multilingual Transfer Learning for
Low-Resource Neural Machine Translation. In Proceedings of Machine Translation
Summit XVII: Research Track. European Association for Machine Translation,
Dublin, Ireland, 128–139. https://aclanthology.org/W19-6613

[41] Vinoj Jayasundara, Nghi Duy Quoc Bui, Lingxiao Jiang, and David Lo. 2019.
TreeCaps: Tree-Structured Capsule Networks for Program Source Code Process-
ing. arXiv preprint arXiv:1910.12306 (2019).

[42] Guido Juckeland, William Brantley, Sunita Chandrasekaran, Barbara Chapman,
Shuai Che, Mathew Colgrove, Huiyu Feng, Alexander Grund, Robert Henschel,
Wen-Mei W Hwu, et al. 2014. SPEC ACCEL: A standard application suite for
measuring hardware accelerator performance. In International Workshop on Per-
formance Modeling, Benchmarking and Simulation of High Performance Computer
Systems. Springer, 46–67.

[43] Dan Jurafsky and James H Martin. 2019. Speech and Language Processing (3rd

(draft) ed.).

[44] Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian Popa. 2019.
Low-resource Deep Entity Resolution with Transfer and Active Learning. In
Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, Florence, Italy, 5851–5861.
https://doi.org/10.18653/v1/P19-1586

[45] Ken Kennedy and John R Allen. 2001. Optimizing compilers for modern architec-

tures: a dependence-based approach. Morgan Kaufmann Publishers Inc.

[46] David Kirk, Barcelona Supercomputing Center, et al. 2008. NVIDIA CUDA

software and GPU parallel computing architecture. (2008).

Learning to Parallelize in a Shared-Memory Environment with Transformers

, ,

[47] Thorsten Kurth, Brandon Cook, Jack Deslippe, and Andrea L Bertozzi. 2016.
OpenMP Parallelization and Optimization of Graph-Based Machine Learning
Algorithms. In OpenMP: Memory, Devices, and Tasks: 12th International Workshop
on OpenMP, IWOMP 2016, Nara, Japan, October 5-7, 2016, Proceedings, Vol. 9903.
17.

[48] Christoph Lameter. 2013. An overview of non-uniform memory access. Commun.

ACM 56, 9 (2013), 59–54.

[49] Marc T Law, Nicolas Thome, and Matthieu Cord. 2014. Bag-of-words image
In Fusion in computer vision.

representation: Key ideas and further insight.
Springer, 29–52.

[50] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28th International Conference on Program Comprehension. 184–195.

[51] Zheng Li, Yonghao Wu, Bin Peng, Xiang Chen, Zeyu Sun, Yong Liu, and Doyle
Paul. 2022. SeTransformer: A Transformer-Based Code Semantic Parser for Code
Comment Generation. IEEE Transactions on Reliability (2022).

[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[53] Ilya Loshchilov and Frank Hutter. 2018. Fixing weight decay regularization in

adam. (2018).

[54] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Ap-
proaches to Attention-based Neural Machine Translation. CoRR abs/1508.04025
(2015). arXiv:1508.04025 http://arxiv.org/abs/1508.04025

[55] AM MacIver. 1937. Token, type and meaning. Analysis 4, 4 (1937), 58–64.
[56] Timothy G Mattson, Y He, and Alice E Koniges. 2019. The OpenMP common

core.

[57] Luís Felipe Garlet Milani. 2020. autotuning with machine learning of OpenMP task

applications. Ph. D. Dissertation. Université Grenoble Alpes.

[58] Reed Milewicz, Peter Pirkelbauer, Prema Soundararajan, Hadia Ahmed, and
Tony Skjellum. 2021. Negative Perceptions About the Applicability of Source-to-
Source Compilers in HPC: A Literature Review. In International Conference on
High Performance Computing. Springer, 233–246.

[59] Alok Mishra, Abid M Malik, and Barbara Chapman. 2020. Using Machine Learning
for OpenMP GPU Offloading in LLVM. ACM SRC to be held at SC20 (2020).
[60] Idan Mosseri, Lee-or Alon, Re’em Harel, and Gal Oren. 2020. ComPar: Opti-
mized Multi-compiler for Automatic OpenMP S2S Parallelization. In OpenMP:
Portable Multi-Level Parallelism on Modern Systems - 16th International Work-
shop on OpenMP, IWOMP 2020, Austin, TX, USA, September 22-24, 2020, Proceed-
ings (Lecture Notes in Computer Science, Vol. 12295), Kent F. Milfeld, Bronis R.
de Supinski, Lars Koesterke, and Jannis Klinkenberg (Eds.). Springer, 247–262.
https://doi.org/10.1007/978-3-030-58144-2_16

[61] Idan Mosseri, Lee-or Alon, Re’Em Harel, and Gal Oren. 2020. ComPar: opti-
mized multi-compiler for automatic OpenMP S2S parallelization. In International
Workshop on OpenMP. Springer, 247–262.

[62] Iulian Neamtiu, Jeffrey S Foster, and Michael Hicks. 2005. Understanding source
code evolution using abstract syntax tree matching. ACM SIGSOFT Software
Engineering Notes 30, 4 (2005), 1–5.

[63] Dong Nguyen, A Seza Doğruöz, Carolyn P Rosé, and Franciska De Jong. 2016.
Computational sociolinguistics: A survey. Computational linguistics 42, 3 (2016),
537–593.

[64] Gal Oren, Yehuda Ganan, and Guy Malamud. 2017. AutOMP: An Automatic
OpenMP Parallelization Generator for Variable-Oriented High-Performance Sci-
entific Codes. arXiv preprint arXiv:1707.07137 (2017).

[65] Terence Parr. 2013. The definitive ANTLR 4 reference. Pragmatic Bookshelf.
[66] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-
Wei Chang. 2021. Retrieval Augmented Code Generation and Summarization. In
Findings of the Association for Computational Linguistics: EMNLP 2021. Association
for Computational Linguistics, Punta Cana, Dominican Republic, 2719–2734.
https://doi.org/10.18653/v1/2021.findings-emnlp.232

[67] Louis-Noël Pouchet et al. 2012. Polybench: The polyhedral benchmark suite.

URL: http://www.cs.ucla.edu/pouchet/software/polybench (2012).

[68] S Prema and R Jehadeesan. 2013. Analysis of Parallelization Techniques and

Tools. Int’l J. Information Computation Tech 3, 5 (2013), 471–478.

[69] S Prema, R Jehadeesan, and BK Panigrahi. 2017. Identifying pitfalls in automatic
parallelization of NAS parallel benchmarks. In Parallel Computing Technologies
(PARCOMPTECH), 2017 National Conference on. IEEE, 1–6.

[70] S Prema, Rupesh Nasre, R Jehadeesan, and BK Panigrahi. 2019. A study on
popular auto-parallelization frameworks. Concurrency and Computation: Practice
and Experience 31, 17 (2019), e5168.

[71] Dan Quinlan. 2000. ROSE: Compiler support for object-oriented frameworks.

Parallel Processing Letters 10, 02n03 (2000), 215–226.

[72] James Reinders. 2012. An overview of programming for Intel Xeon processors
and Intel Xeon Phi coprocessors. Intel Corporation, Santa Clara 1 (2012), 1550002.
[73] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I
Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data

13

Mining, San Francisco, CA, USA, August 13-17, 2016. 1135–1144.

[74] Kyle Richardson, Sina Zarrieß, and Jonas Kuhn. 2017. The Code2Text Challenge:
Text Generation in Source Libraries. In Proceedings of the 10th International Confer-
ence on Natural Language Generation. Association for Computational Linguistics,
Santiago de Compostela, Spain, 115–119. https://doi.org/10.18653/v1/W17-3516
[75] Efsun Sarioglu Kayi, Linyong Nan, Bohan Qu, Mona Diab, and Kathleen McK-
eown. 2020. Detecting Urgency Status of Crisis Tweets: A Transfer Learn-
ing Approach for Low Resource Languages. In Proceedings of the 28th In-
ternational Conference on Computational Linguistics. International Committee
on Computational Linguistics, Barcelona, Spain (Online), 4693–4703. https:
//doi.org/10.18653/v1/2020.coling-main.414

[76] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and Federica
Sarro. 2021. A Survey on Machine Learning Techniques for Source Code Analysis.
arXiv preprint arXiv:2110.09610 (2021).

[77] Karan Sikka, Tingfan Wu, Josh Susskind, and Marian Bartlett. 2012. Exploring bag
of words architectures in the facial expression domain. In European Conference
on Computer Vision. Springer, 250–259.

[78] Jeff Squyres, George Bosilca, Edgar Gabriel, and Josh Ladd. 2018. Open MPI State
of the Union XI. https://www.open-mpi.org/papers/sc-2018/Open-MPI-SC18-
BOF.pdf. [Online].

[79] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[80] M.-A. Storey. 2005. Theories, methods and tools in program comprehension: past,
present and future. In 13th International Workshop on Program Comprehension
(IWPC’05). 181–191. https://doi.org/10.1109/WPC.2005.38

[81] Xiaobing Sun, Xiangyue Liu, Jiajun Hu, and Junwu Zhu. 2014. Empirical studies
on the nlp techniques for source code data preprocessing. In Proceedings of the
2014 3rd international workshop on evidential assessment of software technologies.
32–39.

[82] Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. 2019. A
grammar-based structural cnn decoder for code generation. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 33. 7055–7062.

[83] Wilson L Taylor. 1953. “Cloze procedure”: A new tool for measuring readability.

Journalism quarterly 30, 4 (1953), 415–433.

[84] Irene Tollin, Francesca Arcelli Fontana, Marco Zanoni, and Riccardo Roveda. 2017.
Change prediction through coding rules violations. In Proceedings of the 21st
International Conference on Evaluation and Assessment in Software Engineering.
61–64.

[85] Secil Ugurel, Robert Krovetz, and C Lee Giles. 2002. What’s the code? automatic
classification of source code archives. In Proceedings of the eighth ACM SIGKDD
international conference on Knowledge discovery and data mining. 632–638.
[86] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. CoRR abs/1706.03762 (2017). arXiv:1706.03762 http://arxiv.org/abs/
1706.03762

[87] S VenkataKeerthy, Rohit Aggarwal, Shalini Jain, Maunendra Sankar Desarkar,
Ramakrishna Upadrasta, and YN Srikant. 2020. Ir2vec: Llvm ir based scalable
program embeddings. ACM Transactions on Architecture and Code Optimization
(TACO) 17, 4 (2020), 1–27.

[88] Weidong Wang and Wangda Luo. 2020. Machine Learning Framwork for
Performance Anomaly in OpenMP Multi-Threaded Systems. arXiv preprint
arXiv:2011.02914 (2020).

[89] Zheng Wang and Michael FP O’Boyle. 2009. Mapping parallelism to multi-cores:
a machine learning based approach. In Proceedings of the 14th ACM SIGPLAN
symposium on Principles and practice of parallel programming. 75–84.

[90] Kristian Woodsend and Jacek Gondzio. 2009. Hybrid MPI/OpenMP parallel linear
support vector machine training. The Journal of Machine Learning Research 10
(2009), 1937–1953.

[91] Guang Yang, Yanlin Zhou, Chi Yu, and Xiang Chen. 2021. DeepSCC: Source
Code Classification Based on Fine-Tuned RoBERTa. CoRR abs/2110.00914 (2021).
arXiv:2110.00914 https://arxiv.org/abs/2110.00914

[92] Hong Yao, Huifang Deng, and Caifeng Zou. 2016. A Survey of Loop Parallelization:
Models, Approaches, and Recent Developments. International Journal of Grid
and Distributed Computing 9, 11 (2016), 143–156.

[93] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for
General-Purpose Code Generation. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). As-
sociation for Computational Linguistics, Vancouver, Canada, 440–450. https:
//doi.org/10.18653/v1/P17-1041

[94] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Understanding bag-of-words
model: a statistical framework. International Journal of Machine Learning and
Cybernetics 1, 1 (2010), 43–52.

[95] Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer
Learning for Low-Resource Neural Machine Translation. In Proceedings of the
2016 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, 1568–1575. https://doi.org/10.
18653/v1/D16-1163

