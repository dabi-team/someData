PERT: A New Solution to Pinyin to Character Conversion Task

Jinghui Xiao1

Qun Liu1
Haiteng Wu2

Xin Jiang1

Yuanfeng Xiong2

Zhe Zhang2

1Huawei Noah’s Ark Lab

2Huawei Technologies Co., Ltd.

{xiaojinghui4,qun.liu,Jiang.Xin,xiongyuanfeng}@huawei.com
{wuhaiteng,zhangzhe88}@huawei.com

2
2
0
2

y
a
M
4
2

]
L
C
.
s
c
[

1
v
7
3
7
1
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

Pinyin to Character conversion (P2C) task is
the key task of Input Method Engine (IME)
in commercial input software for Asian lan-
guages, such as Chinese, Japanese, Thai lan-
guage and so on.
It’s usually treated as se-
quence labelling task and resolved by lan-
guage model, i.e. n-gram or RNN. However,
the low capacity of the n-gram or RNN lim-
its its performance. This paper introduces a
new solution named PERT which stands for
bidirectional Pinyin Encoder Representations
from Transformers. It achieves signiﬁcant im-
provement of performance over baselines. Fur-
thermore, we combine PERT with n-gram un-
der a Markov framework, and improve perfor-
mance further. Lastly, the external lexicon is
incorporated into PERT so as to resolve the
OOD issue of IME.

1

Introduction

Some Asian languages, such as Chinese, Japanese
and Thai language, can not be input directly
through standard keyboard. User types in them via
commercial input software, such as Microsoft Input
Method (Gao et al., 2002), Sogou Input Method1,
Google Input Method2, and so on. Pinyin is the
ofﬁcial romanization representation for Chinese
language. It’s natural for user to type in Chinese
by pinyin sequence. For example, taking a snap-
shot from Sogou Input Method as Figure 1 shows,
user inputs a pinyin sequence of wo men qu yong
he gong from the standard keyboard and it is con-
verted into the desired Chinese sentence of “我们
去雍和宫 (we are going to Yonghe Lama Tem-
ple)”. Therefore, Pinyin to Character conversion
(“P2C” for short) task is the key task of commercial
input software.

Usually, the P2C task is treated as sequence la-
belling task and resolved by language model, i.e.

1https://pinyin.sogou.com/
2https://www.google.com/inputtools/

services/features/input-method.html

Figure 1: User Types in Chinese via Pinyin in IME

Figure 2: Pinyin to Character Conversion Task

n-gram. Speciﬁcally, there are three steps as Figure
2 shows. Firstly, the Chinese character candidates
are generated according to the input pinyin tokens.
All of candidates constitute of a lattice. Secondly,
language model provides the probability between
characters, i.e. P (们 | 我), for later caculations.
Thirdly, an optimal path with the highest probabil-
ity in the lattice is found by the Viterbi algorithm.
N-gram is the dominant model in the commercial
input method software. However, its simplicity and
low capacity limit the performance. In the trend
of deep learning techniques of recent year, more
powerful models, such as RNN (Meng and Zhao,
2019; Yao et al., 2018; Wu et al., 2017), have been
applied on the P2C task and achieve substantial im-
provement. The pre-training language models like
BERT-CRF (Souza et al., 2019; Dai et al., 2019)
are also applied to the sequence labelling task, such
as named entity recognition (NER) and outperform
n-gram and RNN signiﬁcantly.

Inspired by the success of BERT-CRF, we
design PERT (bidirectional Pinyin Encoder
Representations from Transformers) especially for
the P2C task. Instead of the pre-training technique
such as BERT-CRF, we train PERT directly on the
P2C task on the massive pinyin-character paral-

 
 
 
 
 
 
lel corpus, and achieve substantial improvement
of performance over n-gram and RNN. There are
three contributions in this paper:

• We design PERT for the P2C task and achieve
substantial improvement of performance.

• We combine PERT with n-gram under Markov
framework and get further performance im-
provement.

• We incorporate external lexicon in PERT to
solve the Out-Of-Domain (“OOD” for short)
issue of IME.

2 Method

We describe our methodology in this section.
Firstly, we present the implement of PERT in Sec-
tion 2.1. Then we show how to combine PERT
with n-gram in Section 2.2. Lastly, we incorporate
external lexicon into PERT in Section 2.3.

2.1 PERT

Figure 3 shows the network architecture of PERT.
There are two differences between BERT and PERT
on architecture. Firstly, as shown in the bottom of
Figure 3, PERT only takes the pinyin token as in-
put since it is designed especially for the P2C task.
Whereas, the BERT models for Chinese, such as
ChineseBERT (Sun et al., 2021), take both pinyin
and character as input because they are for the com-
mon tasks, i.e. Chinese Word Segmentation (CWS),
Text Classiﬁcation (TC) and so on. There is only
410 pinyin tokens (without tone) which is about 2x
order of magnitude smaller than Chinese subword
(usually about 50k). The scale of embedding layer
of PERT is much smaller than BERT. Secondly,
there is no segment embedding but only pinyin em-
bedding and position embedding in PERT. PERT
is trained directly on the P2C task on the massive
pinyin-character parallel corpus, and there is no
pre-training process. It doesn’t need Next Sentence
Prediction (NSP) task, thus no segment embedding.
BERT adopts the pretraining-then-ﬁnetune
paradigm so as to leverage the massive unlabelled
text corpus to supplement limited labelled corpus of
target task. However, for the P2C task, we can con-
vert the massive character corpus into the according
pinyin corpus accurately and efﬁciently, and build
the massive parallel corpus. Then, PERT can be
trained directly on the target P2C task on this cor-
pus. It is called Text-to-Pinyin conversion (“T2P”

Figure 3: PERT for the P2C Task. wo is the input
pinyin token. E0 is the position embedding and Ewo is
the token embedding of pinyin. 我(we) is the predicted
Chinese character.

for short) task (Zhang and Laprie, 2003) which is
the contrary task of the P2C task. The similar tasks
are the transliteration task (Kundu et al., 2018) and
the phoneme-to-grapheme conversion task (Peters
et al., 2017). Both of them are resolved with the
sequence-to-sequence models. However, the T2P
task is much simpler because most of pinyin infor-
mation can be determined within its lexical context.
We can ﬁrstly segment the Chinese character se-
quence into words and then get the correct pinyin
token within its word context. The accuracy ex-
ceeds 99.9% (Zhang and Laprie, 2003) and there
is the open toolkit named PYPinyin 3.

2.2 Combining PERT with N-gram

In practice, the commercial input software usually
installs both n-gram model (for faster installation)
and PERT (for higher performance) at the same
time. In this section, we combine these two models
together so as to get more capability. We start from
the derivation of Bayes rule. Then we unify these
two models under Markov’s framework.

In the P2C task, language model estimates the
joint conditional probability of P (c1...cn|y1...yn),
which is the probability of Chinese character se-
quence c1...cn given the input pinyin sequence
y1...yn. We can decompose it under the Bayes

3https://github.com/mozillazg/python-pinyin

Figure 4: Unify PERT with N-gram Under Markov Framework

Figure 5: Incorporate Lexicon in PERT

rule as Formula 1:

P (c1...cn|y1...yn)
= P (c1...cn−1, cn|y1...yn)
= P (c1...cn−1|y1...yn) ∗ P (cn|c1...cn−1, y1...yn)
≈ P (c1...cn−1|y1...yn−1) ∗ P (cn|c1...cn−1, y1...yn)
≈ P (c1...cn−1|y1...yn−1)

∗ P (cn|c1...cn−1) ∗ P (cn|y1...yn)

(1)

We further simplify these probabilities in two
aspects. Firstly, we simplify P (c1...cn−1|y1...yn)
to P (c1...cn−1|y1...yn−1) since yn is in the fu-
ture during user inputs.
Secondly, we sim-
plify P (cn|c1...cn−1, y1...yn) into the product of
P (cn|c1...cn−1) and P (cn|y1...yn) as the last line
of Formula 1 shows. As presented in Figure 4,
P (cn|c1...cn−1) can be taken as the State Transi-
tion Probability of Markov framework, and can be
estimated by n-gram. P (cn|y1...yn) is the Emis-
sion Probability and can be estimated by PERT.
P (c1...cn−1|y1...yn−1) is the history and can be
further decomposed in the same way as above.

Note that in our implementation, n-gram is esti-
mated separately from PERT. Whereas, BERT-CRF

estimates BERT and n-gram in an end-to-end way.
It’s not applicable for BERT-CRF to the P2C task
because of two reasons. Firstly, it usually requires
different corpus to train n-gram and PERT sepa-
rately in practice, i.e. the smaller dialog corpus for
n-gram whose style is closer to user’s real input, but
the whole huge corpus (including news, web-text
and dialog) for PERT which has more parameters.
It can not be trained in an end-to-end way. Sec-
ondly, the target estimation space of BERT-CRF
is the whole Chinese characters whose number is
more than 6k. It takes too much GPU memory
when calculating the normalization for CRF, which
makes it infeasible to train BERT-CRF on a very
large corpus4.

4We adopt Nvidia V100 GPU with 32G memory in our ex-
periments. We encountered the OOM problem when training
BERT-CRF at the BERT-base scale. At the BERT-tiny scale,
the max batch size was 4, and it took more than ﬁve weeks
to process about 6 epoch on a training corpus of 300M. On
the contrary, the batch size of PERT-tiny was set to 2k and
it only took 1.5 hour to ﬁnish training 6 epoch on the same
corpus. Considering the size of our whole training corpus
exceeds 2400M as described later, we thought the cost of
training BERT-CRF was unacceptable to us.

2.3

Incorporating External Lexicon

The commercial IME engine adopts external lex-
icon to solve the OOD issue on edge device. For
example, it adopts the computer lexicon to adapt
to the computer domain, adopts the location lex-
icon to support some location names, i.e. “雍和
宫(Yonghe Lama Temple)”, adopts the user lexicon
to ﬁt for user’s input custom, and so on. Figure 5
illustrates our way to incorporate external lexicon
in PERT. Speciﬁcally there are three steps. Firstly,
we recognize the word item according to the exter-
nal lexicon, and add it into the lattice of candidates,
as shown in the left part of Figure 5. Secondly,
we estimate the word probability by its component
characters according to Formula 2. Here we adopt
the geometric mean of probability according to the
article (Nelson, 2017). Other metrics can also be
tried. Finally, we search an optimal path together
with the added word by Viterbi algorithm, as shown
by the red arrow of Figure 5 ’s right part.

Pemit(雍和宫) =
3(cid:113)

Ppert(雍) ∗ Ppert(和) ∗ Ppert(宫)

(2)

3 Experiment

3.1 Description of Data Set and Lexicon

As far as we know, there is no benchmark available
to the P2C task. So we build our own data set and
will make it public to the community later. Table
1 describes the detailed information. These arti-
cles are collected from some major Chinese news
websites, such as Netease, Tencent News and so
on. Total 2.6M articles are taken as the training
corpus, and another 1k disjoint articles as the test
corpus. Besides, two additional corpus from the
Baike website and the Society forum 5 are chosen
to evaluate the OOD performance. All these cor-
puses are ﬁrstly segmented into sentences accord-
ing to a punctuation list including comma, period,
and so on. Secondly, the non-pinyin characters
are ﬁltered out, i.e. number, punctuation, English.
Thirdly, they are further segmented by a max length
(16 in our experiment) because user only types a
few tokens once a time. Lastly, we convert them
into the pinyin sequence by PYPinyin.

The Table of General Standard Chinese Charac-

5https://github.com/brightmart/nlp_

chinese_corpus

#Chars

#Articles
Corpus
News-Train 2,603,869 2,432,585,138
1000
News-Test
49,357
Baike-Test
68,345
Society-Test

926,792
6,238,834
5,709,450

#Disk
9.7G
3.7M
30M
30M

Table 1: The Detailed Information of Corpus

ters6 containing more than 6k characters is taken
as the basic lexicon in the experiments. Besides,
Xiandai Hanyu Changyongcibiao (Common words
in Contemporary Chinese, “Common-Words” for
short) containing 0.1m items (Li and Su, 2008) and
Tencent Network Lexicon (‘Network-Words” for
short) published by Tencent AI Lab7 containing
8.8 million items are taken as the external lexicons.

3.2 Evaluation Metrics

In order to evaluate the performance, we use both
character-level precision and sentence-level preci-
sion. The character-level precision is deﬁned as the
ratio that the IME engine converts to the Chinese
characters correctly, as described in Formula 3.

P recisionchar =

#correct_converted_char
#total_converted_char

(3)

Similarly we can deﬁne the sentence-level pre-
cision which is much stricter since it requires the
correctness of whole sequence. Yet it is more mean-
ingful in practice because the input software usu-
ally prompts the conversion of whole sequence in
its ﬁrst place and there is shortcut for user to choose
that result.

Besides, we also choose millisecond per token

to evaluate the latency.

3.3 Baselines and Experiment Settings

Two kinds of language models, Bigram and LSTM,
are taken as baselines.

• Bigram. Bigram is the de facto model
adopted widely in the commercial IME en-
gine. We build it on The Table of General
Standard Chinese Characters on the training
corpus presented in Table 1. No pruning strat-
egy is adopted since the scale of corpus is
large enough.

6https://en.wikipedia.org/wiki/Table_

of_General_Standard_Chinese_Characters
7https://ai.tencent.com/ailab/nlp/

embedding.html

Model
Bigram
LSTM
PERT-tiny
PERT-mini
PERT-small
PERT-medium
PERT-base

#Parameter Char_Precision Char_Improvement Sen_Precision Sen_Improvement ms/token

7.4M
4.2M
1.3M
5.0M
16.5M
29.1M
91.1M

85.10%
89.71%
85.18%
90.57%
95.41%
95.59%
96.59%

NA
4.61%↑
0.08%↑
5.47%↑
10.31%↑
10.49%↑
11.49%↑

34.26%
47.87%
32.01%
48.77%
64.38%
65.74%
73.31%

NA
13.61%↑
-2.25%↓
14.51%↑
30.12%↑
31.48%↑
39.05%↑

1.80
54.63
0.34
0.62
0.63
1.13
1.71

Table 2: Performances on the P2C Task. Char_Precision is the character-level precision; Char_Improvement is
the improvement of character-level precision; Sen_Precision is the sentence-level precision; Sen_Improvement is
the improvement of sentence-level precision. ms/token is the millisecond per token.

• LSTM. LSTM is reported that gets better per-
formance than Bigram (Meng and Zhao, 2019;
Yao et al., 2018; Malhotra et al., 2015) in the
IME engine. In our implementation, both the
embedding size and the hidden size are 256,
and the learning rate is 5e−4. The batch size
is 2k and the epoch number is 10.

Model
PERT-tiny
PERT-mini
PERT-small
PERT-medium
PERT-base

Char_Precision +Bigram Improved

85.18%
90.57%
95.41%
95.59%
96.59%

91.28% 6.10%↑
93.68% 3.11%↑
96.18% 0.77%↑
96.63% 1.03%↑
96.99% 0.40%↑

Table 3: Performances on Combining PERT with Bi-
gram.

For PERT, we follow the speciﬁcations of
Google from the tiny model to the base model8.
We also set its max length to 16 instead of 512, so
as to be consistent to the training corpus.

3.4 Experiments on the P2C Task

The experimental results are presented in Table 2.
Firstly, LSTM outperforms Bigram signiﬁcantly,
which gets 4.61% improvement on character-level
precision and 13.61% on sentence-level precision.
It conﬁrms the previous conclusion in Yao et al.
(2018). Secondly, PERT-tiny gets the compara-
ble performance to Bigram with only about one-
sixth parameter number. And PERT-mini achieves
much better performance than Bigram with similar
yet smaller number of parameter. It outperforms
LSTM too. It proves that PERT has a better net-
work architecture and gets more capability. Lastly,
as the scale increases, PERT gets better and bet-
ter performances. PERT-base achieves 11.49% im-
provement on character-level precision and 39.05%
on sentence-level precision from Bigram. It can be
deployed in the resource-rich environment, i.e. on
the cloud, so as to provide better services.

Table 2 also presents the inference speed. Bi-
gram is evaluated on Intel(R) Xeon(R) Gold 6561
CPU with 72 cores and 3.00GHz, and others are on
Nvidia Tesla V100 GPU with 32M memory. Batch
processing is used both for LSTM and PERT. Al-
though LSTM takes much more time than Bigram,
it has already been deployed in the real products

8https://github.com/google-research/

bert

successfully (Meng and Zhao, 2019; Yao et al.,
2018). PERT at all the scales takes much less time
than LSTM because it can take fully advantage
of parallelism of GPU. Thus, it is reasonable to
draw the conclusion that PERT is deployable to
real product.

3.5 Combining PERT with N-gram

Table 3 presents the experimental results that com-
bines PERT with Bigram as described in Section
2.2. As we can see, the combined model outper-
forms the single PERT at every scale of model,
which proves that our method can take effective
use of the capability of each sub-model and get
better performance. Not surprisingly, as the scale
increases, PERT becomes more powerful and the
gain from the combined model becomes smaller.
It’s acceptable because the combined model is ex-
pected to deploy only on edge device as described
in Section 2.2. In the resource-rich environments,
we can just deploy PERT as large as possible.

3.6

Incorporating External Lexicon

In this section, we evaluate PERT on two OOD
corpus: the Baike corpus and the Society Forum
corpus. Then two external lexicons are incorpo-
rated into PERT separately so as to improve the
performance, as described in Section 2.3. The ex-
perimental results are presented in Table 4 and 5
respectively.

Firstly, PERT-tiny gets 83.40% precision on the
Baike corpus in Table 4 and 80.26% on the Society

Model
PERT-tiny
PERT-mini
PERT-small
PERT-medium
PERT-base

Char_Precision +Common-Words

Improved +Network-Words

83.40%
88.80%
91.28%
92.41%
93.68%

87.17%
90.04%
91.86%
92.88%
94.12%

3.77%↑
1.24%↑
0.58%↑
0.47%↑
0.44%↑

88.75%
91.24%
92.75%
93.57%
94.53%

Improved
5.35%↑
2.44%↑
1.47%↑
1.16%↑
0.85%↑

Table 4: Character-level Precision of PERT with External Lexicon on the Baike Domain. Common-Words is
Xiandai Hanyu Changyongcibiao and Network-Words is Tencent Network Lexicon as described in Section 3.1.

Model
PERT-tiny
PERT-mini
PERT-small
PERT-medium
PERT-base

Char_Precision +Common-Words

Improved +Network-Words

80.26%
87.13%
90.16%
91.52%
93.16%

84.04%
87.89%
90.17%
91.35%
92.97%

3.75%↑
0.76%↑
0.01%↑
0.17%↓
0.19%↓

86.21%
89.52%
91.37%
92.38%
93.61%

Improved
5.95%↑
2.39%↑
1.21%↑
0.86%↑
0.45%↑

Table 5: Character-level Precision of PERT with External Lexicon on the Society Domain. Common-Words is
Xiandai Hanyu Changyongcibiao and Network-Words is Tencent Network Lexicon as described in Section 3.1.

Forum corpus in Table 5, whereas it gets 85.18%
on the in-domain corpus as shown in Table 2. The
performance drops dramatically, which indicates
that the OOD issue is very severe to the commer-
cial input software. Secondly, the precision is im-
proved by 3.77% with the Common-words lexicon,
and by 5.35% with the Network-words lexicon on
the Baike corpus in Table 4. Similar results can
be found on the Society Forum corpus in Table
5. It proves that our method can effectively solve
the OOD problem of IME engine. Thirdly, com-
paring the improvements from two lexicons, the
bigger scale of lexicon brings the larger improve-
ment, which also veriﬁes the effectiveness of exter-
nal lexicon to the OOD problem. Lastly, the gain
gradually fade away as the scale of PERT increases.
It’s acceptable since the external lexicon is only
expected to deploy on edge device.

3.7 Ablation on Scale of Corpus

In the experiments of above sections, we use a
large text corpus containing about 2432M Chinese
characters, whose scale is comparable to Google
BERT (about 3300M English words). The large
scale of language model is always criticized on
carbon footprint issue (Pérez-Mayos et al., 2021).
In this section, we do the ablation study on the scale
of corpus so as to ﬁnd out whether smaller scale
of training corpus is enough to achieve comparable
performances. Speciﬁcally, we divide the whole
corpus into ten pieces, and train PERT-base on
them in an accumulated way in the same settings.

Figure 6: Character-level Precision of PERT on Differ-
ent Scale of Corpus

The results are presented in Figure 6.

The PERT trained on 10% corpus gets only
93.41% precision, which is dramatically lower than
96.59% of the model trained on the full corpus.
Moreover, as the scale increases, the performance
increases accordingly. It indicates that it always
beneﬁts from the increasing scale of corpus. How-
ever, the gain becomes more and more marginal.
The PERT on 90% corpus performs almost as good
as the full model.

4 Related Work

There are several technical approaches to solve
the P2C tasks. It can be treated as sequence la-
beling task like POS tagging, or as seq2seq task
like machine translation, or resolved by pre-trained
language model.

4.1 Sequence Labeling Task

In industry, the P2C task is treated as sequence
labeling task. N-gram (Bahl et al., 1983) is the
de facto model adopted widely in commercial in-
put method. Some smoothing methods, such as
additive smoothing (Laplace, 1825), interpolation
smoothing (Jelinek and Mercer, 1980), back-off
smoothing (Katz, 1987), Kneser-Ney smoothing
(Kneser and Ney, 1995), are adopted to solve the
zero-probability problem. Laterly, the exponential
models, such as Maximum Entropy Markov Model
(“MEMM” for short) (McCallum et al., 2000) and
Conditional Random Field (“CRF” for short) (Laf-
ferty et al., 2001), are proposed to improve the
performance from n-gram. In recent year, under
the trend of deep learning, RNN (Wu et al., 2017)
gets more model capacity by capturing longer con-
text. LSTM (Yao et al., 2018) is applied on the IME
engine and achieves the improvements both in the
P2C task and in the candidate prompt task. An in-
cremental selective softmax method is further pro-
posed to speed up the inference. Different from the
above works, we adopt the bidirectional encoder
representation of Transformer network. Trained on
the massive parallel corpus, PERT gets signiﬁcant
performance improvement from n-gram as well as
LSTM on the P2C task.

4.2 Pre-trained Language Model on P2C

Task

Recently, the emergence of pre-trained models
(PTMs) has brought natural language processing
to a new era. BERT (Devlin et al., 2019) takes
the bidirectional encoder representation of trans-
former networks. It is ﬁrstly pre-trained on some
self-supervised tasks on the massive unlabelled cor-
pus, such as Masked Language Modeling (“MLM”
for short) and Next Sentence Predicting (“NSP”
for short). Then it is ﬁne-tuned on the target
tasks on the small labelled corpus.
It achieves
the SOTA results on many tasks, including the se-
quence labeling task. Zhang and Joe (2020) applies
pre-trained language model to the P2C task and
proposes BERT-P2C. In the experiments, BERT-
P2C outperforms other pre-trained models such
as ELMO. Different from BERT-P2C, we train
PERT directly on the target P2C task by creating
the massive labelled corpus instead of the pretrain-
then-ﬁnetune paradigm, as described in Section 2.1.
Besides, comparing with Zhang and Joe (2020)
which costs the parameters of two encoders and

one decoder of transformers, PERT only takes one
encoder whose parameter is much smaller (about
one third) than BERT-P2C. Moreover, PERT also
outperforms BERT-P2C in the P2C experiments.
We describe the experimental results in details in
Appendix A.

BERT-CRF (Souza et al., 2019; Dai et al., 2019)
further employs the CRF decoder on the top of
BERT encoder, so as to exploit the structure in-
formation of target sequence. It is trained in an
end-to-end way and achieves the SOTA result on
the NER task of Portuguese. Different from BERT-
CRF, we train PERT and n-gram separately for two
reasons as described in Section 2.2. Firstly, it is
required some ﬂexibility when deploying on real
product. Secondly, the costs to train BERT-CRF
on the whole corpus, especially for the calculation
on the normalization of BERT-CRF, are more than
that we can afforded.

ChineseBERT (Sun et al., 2021) exploits some
information speciﬁc to Chinese, such as glyph and
pinyin, so as to enhance its capability on Chinese
language. It gets improvement from Google-BERT
on a variety of Chinese NLP tasks, such as ma-
chine reading comprehension, text classiﬁcation,
named entity recognition and so on. PLOME (Liu
et al., 2021) designs the pre-trained tasks speciﬁc to
glyph and pinyin in the input text. It is applied on
Chinese spelling correction task and gets the SOTA
result. For these models, pinyin is only the aux-
iliary information which has to be input together
with glyph and text. It can not be input alone, as
the P2C task requires. However, PERT only takes
pinyin as input, and it’s especially designed for the
P2C task.

4.3 Machine Translation Approach

In academic community, the P2C task is also con-
sidered as machine translation task (Zhu et al.,
2020; Zhang and Joe, 2020; Zhang et al., 2019;
Meng and Zhao, 2019) in which pinyin is taken
as source language and Chinese character is tar-
get language. It’s usually resolved in an encoder-
decoder network like Transformer.
In practice,
the uni-directional decoder is usually the efﬁ-
ciency bottleneck during inference. Then the Non-
AutoRegressive (“NAR” for short) decoder with
bi-directional attentions is proposed to solve this
problem (Gu et al., 2018; Kasner et al., 2020; Gu
and Kong, 2021).

In the P2C task, the number of input pinyin to-

kens is exactly the same as the number of output
Chinese characters, which is an explicit constraint
that we can utilize. Thus it’s more natural to treat
it as sequence labeling task as this paper presents.
In the future, we can adopt the encoder-decoder
solution when we take the continuous letter-level
input (without pinyin segmentation before P2C) or
the sub-pinyin token input (segmenting input letter
sequence into sub-pinyin token sequence) whose
lengths are much longer than the length of target
character sequence. These models are expected to
be more tolerated to input error. We leave them
in our future works. Besides, comparing PERT
with NAR machine translation system, there is no
heavy decoder used in PERT. Thus, the number of
PERT parameter is about a half of NAR machine
translation system.

5 Conclusions

In this paper, we propose PERT for the P2C task
which is crucial to the IME engine of commercial
input software. In the experiments, PERT outper-
forms n-gram as well as LSTM signiﬁcantly. More-
over, we combine PERT with n-gram under Markov
framework and get further improvement. Lastly,
we incorporate external lexicon into PERT so as to
resolve the OOD issue of IME.

6 Future Work

There are several ideas to try in the future. One
interesting idea is to replace the current pinyin-
level input with the letter-level input, and convert
it into Chinese character sequence by the seq2seq
model as discussed in Section 4.3. The built model
is expected to be more tolerated to input error,
and makes the IME engine more robust. Online
learning is another interesting and important topic.
Zhang et al. (2019) designs an efﬁcient method to
update the vocabulary during the user input process,
and augment the IME engine with the learned vo-
cabulary effectively. However, Zhang et al. (2019)
adopts RNN as encoder, which is usually regarded
as less capable than Transformer architecture. We
are going to incorporate PERT with the online
learning method to get further improvement. Lastly,
we are going to deploy our model to real product,
and compare performance with some commercial
input software on real input of user (Meng and
Zhao, 2019; Huang et al., 2018).

References

Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A maximum likelihood approach to continu-
ous speech recognition. IEEE Trans. Pattern Anal.
Mach. Intell., 5(2):179–190.

Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao
Zheng. 2017. AISHELL-1: an open-source man-
darin speech corpus and a speech recognition base-
In 20th Conference of the Oriental Chap-
line.
ter of the International Coordinating Committee on
Speech Databases and Speech I/O Systems and As-
sessment, O-COCOSDA 2017, Seoul, South Korea,
November 1-3, 2017, pages 1–5. IEEE.

Zhenjin Dai, Xutao Wang, Pin Ni, Yuming Li, Gang-
min Li, and Xuming Bai. 2019. Named entity
recognition using BERT bilstm CRF for chinese
In 12th International
electronic health records.
Congress on Image and Signal Processing, BioMed-
ical Engineering and Informatics, CISP-BMEI 2019,
Suzhou, China, October 19-21, 2019, pages 1–5.
IEEE.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers), pages 4171–4186. Association for Computa-
tional Linguistics.

Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a uniﬁed approach to statis-
tical language modeling for chinese. ACM Trans.
Asian Lang. Inf. Process., 1(1):3–33.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
In 6th
autoregressive neural machine translation.
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. Open-
Review.net.

Jiatao Gu and Xiang Kong. 2021.

Fully non-
autoregressive neural machine translation: Tricks of
In Findings of the Association for Com-
the trade.
putational Linguistics: ACL/IJCNLP 2021, Online
Event, August 1-6, 2021, volume ACL/IJCNLP 2021
of Findings of ACL, pages 120–133. Association for
Computational Linguistics.

Yafang Huang, Zuchao Li, Zhuosheng Zhang, and
Hai Zhao. 2018. Moon IME: neural-based chinese
pinyin aided input method with customizable asso-
In Proceedings of ACL 2018, Melbourne,
ciation.
Australia, July 15-20, 2018, System Demonstrations,
pages 140–145. Association for Computational Lin-
guistics.

F. Jelinek and R. L Mercer. 1980. Interpolated estima-
tion of markov source parameters from sparse data.
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice, North-Holland, Amsterdam,, pages
381–397. The Netherland.

Zdenek Kasner, Jindrich Libovický, and Jindrich Helcl.
2020. Improving ﬂuency of non-autoregressive ma-
chine translation. CoRR, abs/2004.03227.

S. M Katz. 1987. Esitmation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processin, 35:400–401.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In 1995
International Conference on Acoustics, Speech, and
Signal Processing, ICASSP ’95, Detroit, Michigan,
USA, May 08-12, 1995, pages 181–184. IEEE Com-
puter Society.

Soumyadeep Kundu, Sayantan Paul, and Santanu Pal.
2018. A deep learning based approach to translit-
eration. In Proceedings of the Seventh Named Enti-
ties Workshop, NEWS@ACL 2018, Melbourne, Aus-
tralia, July 20, 2018, pages 79–83. Association for
Computational Linguistics.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), Williams College, Williamstown, MA, USA,
June 28 - July 1, 2001, pages 282–289. Morgan
Kaufmann.

P. S. Laplace. 1825. Philosophical Essay on Probabili-

ties, 5 edition. Springer Verlag.

Xingjian Li and Xinchun Su. 2008. Xiandai Hanyu
changyongcibiao. The Commercial Press, Beijing.

Shulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, and
Di Wang. 2021. PLOME: pre-training with mis-
spelled knowledge for chinese spelling correction.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing, ACL/IJCNLP 2021, (Volume 1:
Long Papers), Virtual Event, August 1-6, 2021,
pages 2991–3000. Association for Computational
Linguistics.

Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, and
Puneet Agarwal. 2015. Long short term memory
networks for anomaly detection in time series.
In
23rd European Symposium on Artiﬁcial Neural Net-
works, ESANN 2015, Bruges, Belgium, April 22-24,
2015.

Proceedings of the Seventeenth International Con-
ference on Machine Learning (ICML 2000), Stan-
ford University, Stanford, CA, USA, June 29 - July
2, 2000, pages 591–598. Morgan Kaufmann.

Zhen Meng and Hai Zhao. 2019. A smart sliding chi-
nese pinyin input method editor for touchscreen de-
vices.

Kenric P. Nelson. 2017. Assessing probabilistic in-
ference by comparing the generalized mean of the
model and source probabilities. Entropy, 19(6):286.

Laura Pérez-Mayos, Miguel Ballesteros, and Leo Wan-
ner. 2021. How much pretraining data do language
models need to learn syntax? In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November,
2021, pages 1571–1582. Association for Computa-
tional Linguistics.

Ben Peters, Jon Dehdari, and Josef van Genabith.
2017. Massively multilingual neural grapheme-to-
phoneme conversion. CoRR, abs/1708.01464.

Fábio Souza, Rodrigo Frassetto Nogueira, and Roberto
Portuguese named
CoRR,

de Alencar Lotufo. 2019.
entity recognition using BERT-CRF.
abs/1909.10649.

Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xi-
ang Ao, Qing He, Fei Wu, and Jiwei Li. 2021. Chi-
nesebert: Chinese pretraining enhanced by glyph
In Proceedings of the
and pinyin information.
59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-
tual Event, August 1-6, 2021, pages 2065–2075. As-
sociation for Computational Linguistics.

Dong Wang and Xuewei Zhang. 2015. THCHS-30 : A
free chinese speech corpus. CoRR, abs/1512.01882.

Lin Wu, Michele Haynes, Andrew Smith, Tong Chen,
and Xue Li. 2017. Generating life course trajec-
tory sequences with recurrent neural networks and
application to early detection of social disadvantage.
In Advanced Data Mining and Applications - 13th
International Conference, ADMA 2017, Singapore,
November 5-6, 2017, Proceedings, volume 10604 of
Lecture Notes in Computer Science, pages 225–242.
Springer.

Liang Xu, Xuanwei Zhang, and Qianqian Dong.
2020.
Cluecorpus2020: A large-scale chinese
corpus for pre-training language model. CoRR,
abs/2003.01355.

Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov models
In
for information extraction and segmentation.

Jiali Yao, Raphael Shu, Xinjian Li, Katsutoshi Oht-
suki, and Hideki Nakayama. 2018. Real-time neural-
based input method. CoRR, abs/1810.09309.

be trained more efﬁciently and effectively. In sum-
mary, it is proven that PERT is the better choice to
the P2C task than BERT-P2C is.

B Error Analysis

In this section, we choose some cases from the
experiments and get in-depth analysis.

Table 6 compares Bigram with LSTM. In Case 1,
the conversion of dai is determined by the last word
of “墨镜 (sunglasses)”. Bigram can not capture
such a long context and it gets an incorrect conver-
sion of “带 (bring)”. Whereas, LSTM can model
the whole sentence and get the correct conversion
of “戴 (wear)”. It’s similar for the pinyin shi in
Case 2 which can be predicted by the word of “应
对 (handle)”. Moreover, LSTM can also recognize
some ﬁx collocation like “既...又... (both...and...)”,
and get the result correctly, as shown in Case 3.
In a word, Bigram can only captures local con-
text, which limits its performance on the P2C task.
However, LSTM can take the whole sequence into
consideration, and get improvement from Bigram.
Table 7 compares LSTM with PERT. In Case 1,
both conversions are grammatically correct. How-
ever, from the semantic point of view, the result of
PERT makes more sense. In Case 2, the city name
of “重庆(chongqing)” acts as an adjective which
means the style or fashion like Chongqing in the
context. PERT can capture these semantic informa-
tion and get the correct conversion, whereas LSTM
can not. Moreover, we also ﬁnd that PERT can
handle named entity in sentence much better than
LSTM, as shown in Case 3. All these cases prove
that PERT can better understand the semantic infor-
mation in sentence and thus get better performance.

Sen Zhang and Yves Laprie. 2003. Text-to-pinyin con-
version based on contextual knowledge and d-tree
for mandarin. In IEEE International Conference on
Natural Language Processing and Knowledge Engi-
neering, NLP-KE 2003, Beijing, China, 2003.

Yu Zhang and Inwhee Joe. 2020. A pre-trained lan-
guage model for chinese pinyin-to-character task
based on bert. Thesis for the Master of Science,
pages 197–198.

Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.
Open vocabulary learning for neural chinese pinyin
In Proceedings of the 57th Conference of
IME.
the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Vol-
ume 1: Long Papers, pages 1584–1594. Association
for Computational Linguistics.

Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tie-Yan Liu.
Incorporating BERT into neural machine
2020.
In 8th International Conference on
translation.
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.

A Comparison with BERT-P2C

We compare the performance of PERT with BERT-
P2C (Zhang and Joe, 2020) in this section. Like
BERT, BERT-P2C is ﬁrstly pre-trained on the
CLUECorpus2020 corpus (Xu et al., 2020) which
contains about 100G texts. Then the pinyin-
character parallel corpus is extracted from two
open-source Mandarin speech corpus, named
AISHELL-1(Bu et al., 2017) and THCHS-30(Wang
and Zhang, 2015). They are combined together and
further divided into the training corpus and the test
corpus. BERT-P2C is then ﬁne-tuned on the P2C
task on the training corpus. Evaluated on the test
corpus, BERT-P2C achieves 94.6% character-level
precision.

We choose PERT-base for comparison. We con-
tinue the settings in Section 3 and train PERT di-
rectly on the training corpus containing about 9.7G
News texts. Evaluated directly on the test corpus
of BERT-P2C, it gets 93.06% character-level preci-
sion which is slightly lower than 94.6% of BERT-
P2C. Then, we continue to train PERT on the train-
ing corpus of BERT-P2C. The batch size is 32,
the learning rate is 3e−4 and the epoch number is
20. At this time, PERT achieves 97.77% precision
which is much higher than BERT-P2C. Besides,
considering the scale of training corpus of PERT is
much less (about one tenth) than that of BERT-P2C,
and the parameter number of PERT is also much
less (about one third) than BERT-P2C, PERT can

Case 1
zhi shi dai le yi fu mo jing
Pinyin Sequence
Bigram Result 只是带了一副墨镜 (Only bring a sunglasses)
只是戴了一副墨镜 (Only wear a sunglasses)
LSTM Result
Case 2
Pinyin Sequence
Bigram Result 是玩家能够轻松应对 (Is player handle it easily)
LSTM Result
Case 3
Pinyin Sequence
Bigram Result 及清凉有很时尚 (And cool has fashion)
既清凉又很时尚 (Both cool and fashion)
LSTM Result

shi wan jia neng gou qing song ying dui

ji qing liang you shi shang

使玩家能够轻松应对 (Enable player handle it easily)

Table 6: Case Studies on Comparing Bigram with LSTM

Case 1
Pinyin Sequence
LSTM Result

PERT Result

er shi gong gong wei sheng fang kong de wen ti
二十公共卫生防控的问题
(Twenty problems of public health prevention and control)
二是公共卫生防控的问题
(The second problem is about public health prevention and control)

Case 2
Pinyin Sequence
LSTM Result
PERT Result
Case 3
Pinyin Sequence man wei zhong gu yi fa shi ban yan zhe tai li hai le
LSTM Result

zhe ge shi pin hen chong qing
这个视频很宠情 (This video is very pampering)
这个视频很重庆 (This video is very chongqing-style)

漫威中故意发誓扮演者太厉害了
(The role of intentional pledge in Marvel is awesome)
漫威中古一法师扮演者太厉害了
(The role of Ancient One in Marvel is awesome)

PERT Result

Table 7: Case Studies on Comparing LSTM with PERT

