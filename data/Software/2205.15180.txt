2
2
0
2

y
a
M
0
3

]
E
S
.
s
c
[

1
v
0
8
1
5
1
.
5
0
2
2
:
v
i
X
r
a

T-Wise Presence Condition Coverage and Sampling for
Conﬁgurable Systems

Sebastian Krieter1, Thomas Thüm1, Sandro Schulze2, Sebastian Ruland3, Malte
Lochau4, Gunter Saake2, and Thomas Leich5

1University of Ulm, Ulm, Germany
2University of Magdeburg, Magdeburg, Germany
3TU Darmstadt, Darmstadt, Germany
4University of Siegen, Siegen, Germany
5Harz University of Applied Sciences, Wernigerode, Germany

Abstract

Sampling techniques, such as t-wise interaction sampling are used to enable eﬃcient testing
for conﬁgurable systems. This is achieved by generating a small yet representative sample of
conﬁgurations for a system, which circumvents testing the entire solution space. However,
by design, most recent approaches for t-wise interaction sampling only consider combina-
tions of conﬁguration options from a conﬁgurable system’s variability model and do not
take into account their mapping onto the solution space, thus potentially leaving critical
implementation artifacts untested. Tartler et al. address this problem by considering pres-
ence conditions of implementation artifacts rather than pure conﬁguration options, but do
not consider the possible interactions between these artifacts. In this paper, we introduce
t-wise presence condition coverage, which extends the approach of Tartler et al. by using
presence conditions extracted from the code as basis to cover t-wise interactions. This en-
sures that all t-wise interactions of implementation artifacts are included in the sample and
that the chance of detecting combinations of faulty conﬁguration options is increased. We
evaluate our approach in terms of testing eﬃciency and testing eﬀectiveness by compar-
ing the approach to existing t-wise interaction sampling techniques. We show that t-wise
presence condition sampling is able to produce mostly smaller samples compared to t-wise
interaction sampling, while guaranteeing a t-wise presence condition coverage of 100%.

1

Introduction

Testing is an important task in software engineering to detect faults and to check intended
behavior [1, 2]. However, exhaustive testing may be impossible and binds resources that could be
used in other phases of development. This is especially an issue when testing highly-conﬁgurable
systems, such as Software Product Lines (SPLs). A product of an SPL can be derived from a
conﬁguration, which consist of a list of conﬁguration options (i.e.,features) that can be set either
to true or false. The entirety of all possible conﬁgurations for an SPL is called the conﬁguration
space, which typically grows exponentially with the number of features [3, 4].

A straight-forward testing strategy for SPLs is product-based testing. For this purpose, a set
of products is derived from a set of diﬀerent conﬁgurations and then test cases are run on each
selected product respectively [5]. As testing every possible conﬁguration in a product-based
manner is usually not feasible due to an enormous conﬁguration space, sampling strategies

 
 
 
 
 
 
T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 2 / 28

have been deﬁned to generate a small yet representative set of conﬁgurations to test (i.e., a
conﬁguration sample). One such sampling strategy is t-wise interaction sampling, which aims
to generate a preferably small sample that covers all possible interactions of features of degree
t [6, 7]. Using t-wise interaction sampling, developers can ensure that all interactions of at most
t features (e.g., all selected, none selected, only one selected, etc.) are indeed contained in at
least one conﬁguration in the generated sample. T -wise interaction sampling has shown to be
a feasible trade-oﬀ between testing eﬀectiveness and testing eﬃciency, as for small values of t
(i.e., t ∈ {2, 3}) it usually returns a relatively small sample, while also achieving reliable test
results [7–9].

A property of t-wise interaction sampling is that it works purely on the problem space of an
SPL. Thus, it is a black-box approach that does not take into account the mapping between
features and actual implementation artifacts, such as source code, models, and test cases. This
can lead to a number of issues. First, a traditional t-wise sampling algorithm may create samples
that contain conﬁgurations that generate similar products, which can decrease overall testing
eﬃciency. Second, when choosing the parameter for t the degree of faulty interactions is not
known to the developers. Thus, developers are incentivized to choose a low value for t in order
to keep the testing eﬀort feasible. However, some faults may require a feature interaction of a
high degree to be included in a conﬁguration, and thus may not be included at all in a sample,
which potentially decreases testing eﬀectiveness.

Due to the black-box nature of t-wise interaction sampling, it may not reveal certain faults
resulting from feature interaction beyond t or requires too many conﬁgurations to even reach
a certain code coverage or fault-detection rate. To overcome this limitation, Tartler et al. [10]
propose statement coverage, a white-box approach that considers presence conditions (i.e., the
selection of features for which an artifact is included in a product) to ensure that every artifact
is present in at least one conﬁguration in the sample and gets a chance of being tested. However,
simply including an implementation artifact in a product does not grantee that is indeed being
tested. Therefore, Ruland et al. [11] argue that, in addition to including each artifact, at least
one test case must also cover each artifact in order to properly test it. While the approach of
Tartler et al. [10] can generate a relatively small sample, the approach of Ruland et al. [11]
produces a relatively large sample, but guarantees that each artifact will indeed be tested.
Therefore, with this work, we aim to ﬁnd a reasonable trade-oﬀ between both approaches
by building on the approach of Tartler et al. [10] and try to increase testing eﬀectiveness by
combining it with t-wise interaction sampling.

We propose an extension of Tartler et al.’s coverage criterion t-wise presence condition cov-
erage, which combines t-wise interaction coverage with presence condition coverage of imple-
mentation artifacts. Rather than counting only interactions between features, our new criterion
considers interactions of presence conditions of implementation artifacts. A sample with a
t-wise presence condition coverage of 100% ensures that every t-wise interactions of all imple-
mentation artifacts is contained in at least one product and can thus be tested. In addition,
we present an algorithm for t-wise presence condition sampling, which generates samples with
a 100% t-wise presence condition coverage for a given conﬁgurable system and a given t. This
sampling algorithm works independently from the employed variability mechanisms such as pre-
processors, build systems, feature-oriented programming, or aspect-oriented programming. We
implemented our algorithm within a prototype called PRESICE (PRESence condItion Cover-
agE) to investigate its testing eﬀectiveness, testing eﬃciency, and sampling eﬃciency compared
to traditional t-wise interaction sampling algorithms. Our current prototype supports extraction
of presence conditions from SPLs that use the C preprocessor and the Kbuild build tool (e.g.,
BusyBox and Linux). Additionally, we extensively evaluate both, the coverage criterion and the
algorithm using 27 real-world systems from diﬀerent domains. In summary, we contribute the
following:

• We propose a novel coverage criterion for product-based testing: t-wise presence condition

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 3 / 28

coverage.

• We present a sampling algorithm for t-wise presence condition coverage.

• We provide an open-source implementation named named PRESICE as part of Fea-

tureIDE1.

• We evaluate our coverage criterion and algorithm using 27 real-world systems.

• We publish all data from our experiments2.

2 Running Example and Problem Statement

In the following, introduce a running example, which we use throughout the paper. We use this
example to describe the potential problems with the current approaches for t-wise interaction
sampling and motivate our solution.

2.1 Running Example

To illustrate the challenge of ﬁnding eﬀective and eﬃcient samples, we show a slightly edited
code snippet from the system BusyBox in Figure 1, which uses the C preprocessor [12] to
implement its variability [13]. The example is taken from the ﬁle tftp.c, which handles client-
server communication via tftp. The code contains ﬁve features, TFTP (T ), TFTP_GET (G),
TFTP_PUT (P ), TFTP_BLOCKSIZE (B), and TFTP_DEBUG (D), which each can be set to either true
or false. We display their dependencies in Figure 2 using an excerpt of the BusyBox variability
model, represented as a feature diagram. The additional features BusyBox_TFTP (BB) and
TFTPD (T D) do not appear in the code snippet, but are necessary to visualize the feature
diagram hierarchy. In the remainder of the paper, we use the provided abbreviations of the
feature names to ease the readability of all propositional formulas using these features.

In the example, we changed the statement in Line 671 such that a compilation error occurs
for certain products. The variable blksize is declared in Line 626, which is dependent on
feature B. Then, blksize is used in Line 671, which is dependent on feature D. Thus, if D is
selected in a conﬁguration, but B is not, the generated product will be syntactically incorrect.

2.2 Problem Statement

T -wise interaction coverage only considers interactions between single features, and thus is
based purely within the problem space of an SPL. This can lead to two ﬂaws. First, t-wise
interaction coverage may consider some irrelevant feature interactions, which can yield a large
sample, potentially leading to a low testing eﬃciency. Second, t-wise interaction coverage may
not be suﬃcient to reliably detect a fault resulting from an interaction of a degree larger than
t, potentially leading to a low testing eﬀectiveness. In order to reliably ﬁnd faults with a high
interaction degree a high value for t is required. However, traditional t-wise interaction coverage
does not scale well with higher values for t, as the number of possible feature interactions grows
exponentially with the parameter t. Thus, often a low value, such as t = 2 or t = 3, is chosen,
which keeps the testing eﬀort manageable, but also decreases the fault detection rate. In the
following, we explain both ﬂaws in more detail. With the introduction of our new coverage
criterion, we aim to address both of these ﬂaws by taking the solution space into account.

1https://featureide.github.io/
2https://github.com/skrieter/evaluation-pc-sampling

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 4 / 28

1 # if TFTP_GET || TFTP_PUT // G ∨ P

// ...

621 # if TFTP // T
622 int tftp_main ( int argc , char ** argv ) {

if TFTP_BLOCKSIZE // B

// ...
#
const char * blksize_str = TFTP_BLKSIZE_DEF AULT ;
// ...
int blksize = tf tp_ bl ks ize _c he ck ( blksize_str , 65564);
if ( blksize < 0) return EXIT_FAILURE ;
#
// ...
#
printf ( " blksize = % d \ n " , blksize ); // changed
#
// ...

if TFTP_DEBUG // D

endif

endif

625
626

649
650
651

670
671
672

690 }
691 # endif

// ...
827 # endif

Figure 1: Example adopted from BusyBox.

BUSYBOX_TFTP

TFTP

TFTPD

Legend:

Optional

TFTP_GET

TFTP_PUT

TFTP_DEBUG

TFTP_BLOCKSIZE

Figure 2: Excerpt of BusyBox feature diagram.

Eﬃciency of Sample-Based Testing In sample-based testing, we run the all test cases of
a system once per sampled conﬁguration. Although the execution time of the test cases may
diﬀer from conﬁguration to conﬁguration, in general, if the number of conﬁgurations in a sample
(i.e., the sample size) increases the overall testing eﬀort increases as well. Thus, analogous to
other research, we consider the testing of smaller samples to be more eﬃcient (i.e., testing
eﬃciency [11]).

Applying pair-wise interaction sampling to our running example considers every interaction
between two features. For instance, in Figure 1, for the features P and G, all four possible
interactions are considered. However, the three interactions (P, ¬G), (¬P, G), (P, G) all lead
to the same product, as all of them satisfy the expression in Line 1. Thus, it is suﬃcient to
consider only two interactions (e.g., (¬P, ¬G) and (P, G)) for this code snippet. In the table
below, we show the sample generated from the t-wise interaction sampling algorithm ICPL [14,
15], which consists of six conﬁgurations:

Feature

TFTP (T)
TFTP_GET (G)
TFTP_PUT (P)
TFTP_DEBUG (D)
TFTP_BLOCKSIZE (B)

01

-
-
-
-
-

Conﬁgurations
05
02

04

03

06

(cid:88)
-
(cid:88) (cid:88)
(cid:88)
-
(cid:88)
-
(cid:88) (cid:88)

(cid:88) (cid:88)
-
(cid:88)
-
-
(cid:88) (cid:88)
-
(cid:88) (cid:88)
-
(cid:88)
-
-

All four interactions of P and G are included. Including such unnecessary interaction can lead

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 5 / 28

to a larger sample, and thus to a lower testing eﬃciency.

Eﬀectiveness of Sample-Based Testing We consider a sample to be more eﬀective, the
more faults could be detected using its derived products. Analogous to other research, we refer
to this as testing eﬀectiveness [11].

Although the fault in Figure 1 apparently involves only two features, it is in fact a fea-
ture interaction of degree four. To actually generate a product that contains the error, the
corresponding conﬁguration must have the feature B deselected and the features T , D, and
G or P selected. Below, we show the sample generated by the pair-wise sampling algorithm
IncLing [16]:

Feature

01

02

Conﬁgurations
04

05

03

06

07

(cid:88)
TFTP (T)
(cid:88)
TFTP_GET (G)
(cid:88)
TFTP_PUT (P)
(cid:88)
TFTP_DEBUG (D)
TFTP_BLOCKSIZE (B) (cid:88)

-
-
-
-
-

(cid:88)
-
-
-
(cid:88)

-
(cid:88)
(cid:88)
-
-

-
-
-
-
(cid:88)
-
(cid:88) (cid:88)
(cid:88)
-

(cid:88)
(cid:88)
-
-
-

Although the interaction (D, ¬B) is covered in Conﬁguration 06, the actual fault will not be
included in the product, as the feature T is not selected and the preprocessor will remove the
entire code block. Thus, the testing eﬀectiveness of this sample is decreased.

3 Foundations of Conﬁgurable Systems

Before we can properly introduce the concept of t-wise presence condition coverage, we have to
provide some essential deﬁnitions on feature modeling, conﬁgurations, and presence conditions.
Our approach takes a feature model and a list of presence conditions as input and generates
a list of conﬁgurations (i.e., a sample). Therefore, in the following, we revisit the basic notion
of feature models, conﬁgurations, and presence conditions. We introduce all notions using
propositional formulas and set notation.

3.1 Feature Models

A feature model speciﬁes all features of an SPL and their interdependencies [17–19]. We deﬁne a
feature model M = (F, D) as a tuple, consisting of a set of features F and a set of dependencies
D on F. All features of a feature model are contained in the set F = {F1, ..., Fn}, where n is
the total number of features. We represent the dependencies of a feature model as clauses of
a propositional formula in conjunctive normal form (CNF). Each dependency in D represents
one such clause over F (i.e., D = {D1, ..., Dm}, where m is the number of clauses). We denote
a clause as a set of literals over F. A literal is either a feature from F (i.e., a positive literal) or
a negated feature from F (i.e., a negative literal). We deﬁne the function L that provides the
set of literals for a feature model, L(M) = {f alse, true, ¬F1, ..., ¬Fn, F1, ..., Fn}. For example,
consider the features TFTP_PUT (P ), TFTPD (T D), and BUSYBOX_TFTP (BB) from Figure 2. Their
dependencies can be written as the two clauses D1 = ¬P ∨ T D and D2 = ¬T D ∨ BB.

3.2 Conﬁgurations

A conﬁguration represents a selection of features from a feature model. From a conﬁguration,
we can derive the corresponding product using the variability mechanism of the SPL [17–19].
We deﬁne a conﬁguration as a set of literals C, such that C ⊆ L(M) with ∀l ∈ L(M) : l /∈

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 6 / 28

C ∨ ¬l /∈ C. If a literal is contained in a conﬁguration, the corresponding feature is deﬁned as
either selected (positive literal) or deselected (negative literal).

If all features are deﬁned within a conﬁguration, we call it complete and otherwise partial.

We deﬁne this with the function:

complete(C, M) =

(cid:40)

true
false

|C| = |F|
otherwise

A clause of a feature model represents a disjunction of literals. Thus, if a conﬁguration
contains at least one literal from a clause in D, it satisﬁes this clause. Contrary, if a conﬁguration
contains all complementary literals of a clause, it contradicts this clause and, hence the entire
feature model. Thus, if a conﬁguration contradicts at least one clause, we call it invalid. We
call a conﬁguration valid, if it allows all clauses of a feature model to be satisﬁed:

valid (C, M) =

(cid:40)

true
false

∃C(cid:48) ⊇ C : ∀D ∈ D : C(cid:48) ∩ D (cid:54)= ∅
otherwise

Note that, a partial conﬁguration may also neither satisfy nor contradict a clause. In particular,
a valid conﬁguration may be partial and is not required to satisfy all clauses, as long as all clauses
can be satisﬁed by adding more literals to the conﬁguration.

3.3 Presence Conditions

A presence condition is a propositional formula over a feature model that describes whether
a certain aspect is true (i.e., present) for a given conﬁguration and/or its respective product.
A presence condition is not part of a feature model and does not limit the valid conﬁguration
space. Rather it describes a subset of the valid conﬁguration space (i.e., the set of all valid
conﬁgurations that satisfy its propositional formula) for which a certain aspect is true. In this
paper, we use presence conditions to describe whether an implementation artifacts is present
in a product or not. In particular, we are interested in single lines of code (i.e., statements).
Furthermore, in our evaluation, we also use presence conditions to describe whether a particular
fault is present in a product or not. If and only if a conﬁguration contains a combination of
feature selections that satisfy a presence condition, the corresponding implementation artifact
is included in the respective product of that conﬁguration. If a presence condition is satisﬁed
by a conﬁguration we call it active and otherwise inactive.

For instance, Line 622 of Figure 1 has the presence condition (G∨P )∧T , which can be derived
from the nested C preprocessor annotations within the source code. This means that Line 622 is
present in a product if the corresponding conﬁguration has the feature G or P selected and the
feature T selected, because the presence condition is active for these conﬁgurations. When we
use presence conditions, in most cases, we are interested in which feature selections would make
a presence condition active. In the example above these are the combinations G ∧ T and P ∧ T ,
if any of these two is true, the presence condition is active. For this reason, it is beneﬁcial to
represent presence conditions in disjunctive normal form (DNF), which consists of a disjunction
of conjunctive clauses. In a DNF, each clause represents a feature selection that satisﬁes the
presence condition. In order to include an artifact in a product, at least one clause of the DNF
must be satisﬁed, which means that all of its literals must be contained within a conﬁguration.
For example, we can write the presence condition of Line 622 as P +
622 = (G ∧ T ) ∨ (P ∧ T ).
Thus, in order to include this line in a product, the corresponding conﬁguration must either
contain features G and T or P and T .

To represent presence conditions within our formalism, we use a similar notation as for
dependencies. We deﬁne a presence condition such that P = {P1, ..., Pk} with Pi ⊆ L(M). In
the context of t-wise presence condition coverage, we refer to a DNF clause simply as clause.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 7 / 28

Further, we deﬁne the function active to check whether a presence condition is satisﬁed for a
valid conﬁguration:

active(P, C) =

(cid:40)

true
false

∃P ∈ P : P ⊆ C
otherwise

4 Presence Condition Coverage

In the following, we deﬁne our coverage criterion for t-wise presence condition coverage, based
on presence conditions, the feature model, the parameter t, and a conﬁguration sample. To
this end, we describe interactions between presence conditions and how our coverage criterion
is calculated for a given sample and t.

4.1 Presence Condition Interactions

An interaction between two presence conditions (i.e., between their implementation artifacts) is
similar to interactions between features. However, the key diﬀerence is that a presence condition
can be an arbitrary propositional formula. In a complete conﬁguration, a feature can either be
selected, if its corresponding positive literal is included in the conﬁguration, or deselected, if
its negative literal is included. In contrast, a presence condition may be active or inactive for
several diﬀerent literal combinations (cf. Section 3.3). Thus, an interaction between t presence
conditions must include all possible union sets of these literal combinations.

In order to account for the interactions between present
Inactive Presence Conditions
and absent implementation artifacts, we must consider the interactions between active and
inactive presence conditions. For this reason, we construct the complement of each presence
condition, which is itself a presence condition and represents all literal combinations, for which
the original presence condition is inactive. To this end, we negate the formula of a presence
condition and convert it back into a DNF. As an example, consider the presence condition
P +
626 = (G ∧ T ∧ B) ∨ (P ∧ T ∧ B) for Line 626 of Figure 1. We process this formula as follows:

626 = ¬ P +
P −
626

= ¬((G ∧ T ∧ B) ∨ (P ∧ T ∧ B))

De M organ(cid:48)s Law ≡ (¬G ∨ ¬T ∨ ¬B) ∧ (¬P ∨ ¬T ∨ ¬B)
Distributive Law ≡ (¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬B)

After negating the formula, we apply De Morgan’s law to get a CNF and then apply the
distributive law to convert it back into a DNF. While the application of the distributive law
could theoretically lead to an exponential growth in the number of clauses, in practice a presence
condition consists of only a few clauses and literals, which makes the computational eﬀort for
this transformation neglectable. From the new DNF P −
, we see that if a conﬁguration either
626
contains the literal ¬T , ¬B, or both, ¬G and ¬P , the resulting product will not include Line 626.
For a given SPL, we construct one set PC that contains DNFs of all presence conditions
from the SPL and their corresponding complements. To construct all t-wise interactions for a
given SPL, we can then generate all t-wise combinations of PC by constructing the Cartesian
product for the given t (i.e., PCt).

Combined Presence Condition For each interaction of t presence conditions, we can build
a new combined presence condition PI that is satisﬁed if and only if all individual presence
conditions are active or inactive, respectively. To this end, we conjoin the DNFs of all presence
conditions involved in a given interaction and converting the resulting expression back to a
DNF. For each presence condition that is active in the given interaction we use its original DNF

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 8 / 28

and for each presence condition that is inactive we use its complementary DNF. For instance,
consider the DNFs for excluding Line 626 and including Line 671 of Figure 1:

Line 626: P −
Line 671: P +

626 = (¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬B)
671 = (G ∧ T ∧ D) ∨ (P ∧ T ∧ D)

We build the presence condition for the interaction (PI ) by conjoining the literals of each
pair-wise clause combination:

PI = P −

626 ∧ P +
671

Distributive Law ≡ ((¬G ∧ ¬P ) ∧ (G ∧ T ∧ D)) ∨ ((¬G ∧ ¬P ) ∧ (P ∧ T ∧ D))∨

((¬T ) ∧ (G ∧ T ∧ D)) ∨ ((¬T ) ∧ (P ∧ T ∧ D))

((¬B) ∧ (G ∧ T ∧ D)) ∨ ((¬B) ∧ (P ∧ T ∧ D))

simplif y ≡ (¬B ∧ G ∧ T ∧ D) ∨ (¬B ∧ P ∧ T ∧ D)

After merging, we further simplify the combined DNF by removing contradictions and redun-
dant clauses. In case of our example, this results in the simpliﬁed DNF (¬B ∧ G ∧ T ∧ D) ∨
(¬B ∧ P ∧ T ∧ D). Thus, if a conﬁguration either contains the literals ¬B, G, T , and D or ¬B,
G, T , and P Line 671 will appear in the resulting product, but not Line 626.

4.2 Presence Condition Coverage Criterion

Given a set of presence conditions (PC), feature model (M), t, and conﬁguration sample (C) for
an SPL, we can determine a value for our coverage criterion t-wise presence condition coverage.
We deﬁne t-wise presence condition coverage as the ratio between the number of t-wise presence
condition interactions that are covered by C and the number of valid t-wise presence condition
interactions for M. Naturally, with t-wise presence condition sampling we aim to compute a
sample that achieves a coverage of 100%.

In our running example, there are ﬁve diﬀerent presence conditions and their ﬁve comple-

ments.

true,

(G) ∨ (P ),

f alse,

(¬G ∧ ¬P ),

(G ∧ T ) ∨ (P ∧ T ),

(¬G ∧ ¬P ) ∨ (¬T ),

(G ∧ T ∧ B) ∨ (P ∧ T ∧ B), (¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬B),

(G ∧ T ∧ D) ∨ (P ∧ T ∧ D), (¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬D)

In total, there are 11 valid combinations. Considering the sample from IncLing in Section 2.2,
there are two interactions that are not covered by any conﬁguration in the sample: (G ∧ T ∧
B ∧ ¬D) ∨ (P ∧ T ∧ B ∧ ¬D) and (G ∧ T ∧ ¬B ∧ D) ∨ (P ∧ T ∧ ¬B ∧ D). Thus, the number of
interactions covered by the sample is 9, which results in a pair-wise presence condition coverage
of approximately 82%.

5 T-Wise Presence Condition Sampling

In the following, we describe our algorithm to achieve t-wise presence condition coverage. The
algorithm consists of three steps. First, we extract the presence conditions for each line of code
from the given SPL. Second, we construct the set PC by preprocessing the presence conditions
extracted from the SPL, removing tautologies, contradictions, and equivalent conditions and
computing the complementary presence conditions (cf. Section 4.1). Third, we iteratively
construct a conﬁguration sample that includes all valid t-wise combinations of PC.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 9 / 28

5.1 Extracting Presence Conditions

As a ﬁrst step, we have to extract a list of presence conditions from the target SPL. This step
is highly dependent on the speciﬁcs of the given SPL, such as the programming language, the
variability mechanism, and the conﬁguration mechanism. Therefore, a general approach for this
step is out of scope for this paper. In fact, both processes, extracting presence conditions and
testing products, are independent from our approach and can be adapted to any suitable vari-
ability mechanism, programming language, and testing framework. We did however implement
an algorithm for extracting presence conditions from SPLs that use the language C and the C
preprocessor as a variability mechanism. The algorithm is based on the tool PCLocator [20],
which is able to parse C ﬁles and analyze preprocessor annotations. The basic procedure from
this algorithm would also work for other preprocessor implementations, such as Antenna for
Java, but would of course require a diﬀerent parser implementation. In the following, we de-
scribe the general logic behind the algorithm. We give some more details for our particular
implementation in our prototype PRESICE in Section 6.1.1.

In general, the algorithm parses a C ﬁle and determines a presence condition for each line.
As a result the algorithm returns a list of presence conditions for each ﬁle. The C preprocessor
annotations that are used for variability management always form a block around variable code
artifacts. Each block begins with an annotation, such as #if or #ifdef and ends with an
annotation, such as #else or #endif. These block can be nested, as we display in our example
in Figure 1. Consequently, for each line in a C ﬁle, the algorithm determines the C preprocessor
blocks that surrounds the line. The presence condition of the line is then constructed by the
conjunction of the conditionals of the surrounding blocks.

Due to the high expressiveness of the C preprocessor, the extraction algorithm has some
limitations, which can lead to incorrect results in some rare cases. Most notably, the algo-
rithm does not expand any preprocessor macro statements, which can lead parsing problems or
missed annotations. In case of a parsing problem for a line, the impacted presence conditions
are assumed to be true (i.e., the lines are always present). Further, the algorithm does only
consider Boolean features. All features with numerical or string values are ignored. Due to
these limitations, it is possible that some returned presence condition might be incorrect or
incomplete.

5.2 Preprocessing Presence Conditions

Our core algorithm for t-wise presence condition sampling expects as input a list of presence
conditions PC, a feature model M, and a value for t. From the previous step, we get a raw list
of extracted presence conditions from an SPL, which can be arbitrary propositional formulas
in any order. Therefore, the next step is to preprocess this initial list. In detail, we convert all
presence conditions into DNF (cf. Section 3.3), compute their complements (cf. Section 4.1),
and clean up the list by removing equivalent formulas, tautologies, and contradictions.
In
the end, we converted all raw presence conditions into a minimal list of DNFs PC. Note that
we do not need to convert a presence condition from CNF to DNF, but directly convert the
propositional formulas extracted from the implementation artifacts, which typically contain
only a small set of features and no complicated formulas.

Regarding our example in Figure 1, we extract the following raw presence conditions (for

brevity, we already omit duplicates):

true, G ∨ P,

(G ∨ P ) ∧ T,

(G ∨ P ) ∧ T ∧ B,

(G ∨ P ) ∧ T ∧ D

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 10 / 28

The preprocessing results in the following list:

(G) ∨ (P ),

(G ∧ T ) ∨ (P ∧ T ),

(G ∧ T ∧ B) ∨ (P ∧ T ∧ B),

(G ∧ T ∧ D) ∨ (P ∧ T ∧ D), (¬G ∧ ¬P ),

(¬G ∧ ¬P ) ∨ (¬T ),

(¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬B),

(¬G ∧ ¬P ) ∨ (¬T ) ∨ (¬D)

Splitting our algorithm in a preprocessing and a sampling part allows us to ﬂexibly determine
the input for the actual sampling algorithm. For instance, we can use the sampling algorithm
also for regular t-wise interaction sampling, if we construct PC accordingly (i.e., PC = {L ⊆
L(M) | |L| = 1). Furthermore, it also allows us to build subsets of PC and only consider
interactions between these. To this end, we run the sampling algorithm once for each subset
and iteratively add conﬁgurations to CSample. Using this, we can, for example, group presence
conditions for single ﬁles or folders, and thus limit the number of possible interactions and the
resulting sample size.

5.3 Constructing a Conﬁguration Sample

After computing the set PC, we continue with the third step of our algorithm, constructing a
sample of valid and complete conﬁgurations CSample. For this, we extend the existing algorithm
YASA [21], a deterministic greedy algorithm. We start with an empty sample CSample and
then iterate over all t-wise interactions one at a time. For each, we either add a new partial
conﬁguration to the sample or the literals of one clause of the interaction’s presence condition to
an existing conﬁguration. Due to the nature of this algorithm, we are guaranteed to check every
possible combination of the given presence conditions exactly once. For each combination or
presence conditions, we either include it in at least one conﬁguration in the sample or determine
that it cannot be covered by any valid conﬁguration. Since there are ﬁnitely many possible
(cid:1) with k being the number of presence conditions), the algorithm
combinations (i.e., 2t · (cid:0)k
t
always terminates and guarantees that a presence condition coverage of 100% is achieved by
the computed sample.

We present pseudocode of the second step in Algorithm 1.

It takes a parameter t, the
feature model M, and a list of DNFs PC and returns a sample CSample that covers every t-wise
combination of DNFs in PC. At the start, we initialize CSample with an empty set (Line 1). To
list all presence condition interactions up to degree t, we then build the Cartesian product for
PC with itself t times (i.e., PCt) (Line 2). For each interaction in PCt, we compute the combined
presence condition PI as outlined in Section 4.1 (Line 3). Using PI , we check whether this
interaction is already covered by at least one conﬁguration in the sample (Line 4).
If not,
we try to cover the interaction by iterating over all clauses in PI (Line 6). For each clause
If
P , we ﬁrst check, whether it is satisﬁable with regard to the feature model M (Line 7).
so, we add it to a temporary set of valid clauses Pvalid (Line 12). Second, we iterate over
all conﬁgurations in our current sample and check, whether we can add the literals of P to
it without causing a contradiction (Line 9). If it does not cause a contradiction, we add all
literals to the conﬁguration (i.e., covering the interaction) and continue with the next interaction
(Line 10 and 11). Otherwise, if we cannot ﬁnd any suitable conﬁguration for any clause in PI ,
we use the smallest clause in Pvalid, if any, and use it to build a new conﬁguration that we add
to our sample (Line 13 and 14).

As an example, we describe some iterations of the algorithm for Figure 1.

In the ﬁrst
iteration we get the combination I = ((G ∨ P ), (¬G ∧ ¬P )) (Line 2), which is converted into
the combined presence condition PI = G ∧ ¬G ∨ P ∧ ¬P (Line 3). As there is no conﬁguration
yet in CSample, we continue (Line 4). Both clauses in PI , P1 = G ∧ ¬G and P2 = P ∧ ¬P
are invalid and are therefore not considered for inclusion into a conﬁguration (Line 7). Thus,
we continue with the next iteration.
In the second iteration we get the combination I =
((G ∨ P ), ((G ∧ T ) ∨ (P ∧ T ))) (Line 2), which results in PI = (G ∧ T ) ∨ (P ∧ T ) (Line 3). There

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 11 / 28

Algorithm 1: Pseudocode for Constructing a Sample

Data: Presence Conditions PC, Feature Model M, Parameter t
Result: List of conﬁgurations CSample
CSample ← ∅
for I ∈ PCt do
PI ← {(cid:83)
P ∈T P | T ∈ (cid:81)
if (cid:64)C ∈ CSample : active(PI , C) then

P∈I P}

Pvalid ← ∅
for P ∈ PI do

if valid(P, M) then

for C ∈ CSample do

if valid(C ∪ P, M) then

C ← C ∪ P
continue line 2

Pvalid ← Pvalid ∪ P

if Pvalid (cid:54)= ∅ then

CSample ← CSample ∪ min(Pvalid)

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

return CSample

is still no conﬁguration in CSample, which may cover this interaction (Line 4). Both clauses
P1 = G ∧ T and P2 = P ∧ T are valid (Line 7), but there is no conﬁguration yet to which
they could be added (Line 8). Thus, both of them are added to Pvalid (Line 12). Next, the
smallest clause in Pvalid is added to CSample (Line 14). As both clauses have the same size,
we use the ﬁrst one, resulting in CSample = {{G, T }}. In the third iteration we get combined
presence condition PI = (G ∧ T ∧ D) ∨ (P ∧ T ∧ D). Both clauses are valid and can be added
to the existing conﬁguration in CSample (Line 9). We use the ﬁrst clause, which results in
CSample = {{G, T, D}} (Line 10). In the fourth iteration we get combined presence condition
PI = (G ∧ T ∧ ¬D) ∨ (P ∧ T ∧ ¬D). Here, both clauses are valid, but conﬂict with the existing
conﬁguration in CSample (Line 9). Thus, a new conﬁguration is added, It can be added to the
existing conﬁguration (Line 14), resulting in CSample = {{G, T, D}, {G, T, ¬D}}.

The complete conﬁguration sample after the algorithm terminated is shown in the following

table:

Feature

01

Conﬁgurations
03

04

02

TFTP (T)
TFTP_GET (G)
TFTP_PUT (P)
-
TFTP_DEBUG (D)
TFTP_BLOCKSIZE (B) (cid:88) (cid:88)

(cid:88) (cid:88) (cid:88)
-
(cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)
-
-

(cid:88)
-

05

-
-
-
(cid:88)
(cid:88)

When comparing this sample to the sample generated by IncLing in Section 2.2, we see that, in
contrast, it contains a conﬁguration that covers the fault in the example (i.e., Conﬁguration 03).
This corresponds to an increase in testing eﬀectiveness. Compared to the sample produced by
ICPL in Section 2.2, we can see that it contains only the two real interactions of P and G, and
thus requires only ﬁve conﬁgurations, which is an increase in testing eﬃciency.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 12 / 28

6 Evaluation

With t-wise presence condition coverage we aim to generate samples for a novel coverage crite-
rion, which we expect to increase the chance of detecting faults in product-based testing. We
are interested in the degree of testing eﬀectiveness and testing eﬃciency of the t-wise presence
condition coverage criterion and our algorithm for t-wise presence condition sampling. There-
fore, we evaluate whether samples generated with t-wise presence condition sampling can detect
more faults than samples generated with t-wise interaction sampling. We also evaluate what
degree of t-wise presence condition coverage can be achieved by existing algorithms for t-wise
interaction sampling. Further, we evaluate the sample size (i.e., testing eﬃciency) and sampling
time (i.e., sampling eﬃciency) of our tool PRESICE (cf. Section 5). In summary, we aim to
answer the following research questions:

RQ1 Is t-wise presence condition coverage more eﬀective in detecting faults than t-wise inter-

action coverage for the same value of t?

RQ2 What degree of presence condition coverage can be achieved using traditional sampling

algorithms?

RQ3 Is the testing of samples more eﬃcient with t-wise presence condition sampling than

with t-wise interaction sampling?

RQ4 Is the generation of samples more eﬃcient with t-wise presence condition sampling than

with t-wise interaction sampling?

Within our experiments, we compute several samples for diﬀerent systems using our tool
PRESICE and a selection of diﬀerent state-of-the-art t-wise interaction sampling algorithms
and compare the samples with respect to our evaluation criteria. In the following, we describe
the setup for our experiments and our evaluation results. First, we introduce the algorithms
that we compare against each other. Second, we present the subject systems, for which we
generate samples. Third, we describe our measuring methods for our four evaluation criteria,
fault detection, coverage, sample size, and sampling time. Fourth, we analyze and discuss our
results. Finally, we discuss potential threats to the validity of our evaluation.

6.1 Algorithms

We use several state-of-the-art algorithms for t-wise interaction sampling as comparison for test-
ing eﬃciency and eﬀectiveness, which were also used in previous evaluations [15, 16, 22]. First,
we employ Chvátal [23], ICPL [14, 15], and IncLing [16] as pure t-wise interaction sampling
algorithms. Second, we use PRESICE as a pure t-wise interaction sampling algorithm (cf. Sec-
tion 5.2), which only uses the feature model as input (PRESICE-FM). All of these algorithms
compute complete t-wise samples for certain values of t using diﬀerent methods. Third, we use
a random sampling algorithm [24]. Instead of aiming for a certain coverage criteria it generates
a ﬁxed number of valid random conﬁgurations. Fourth, we include the algorithm PLEDGE [25],
which does not try to achieve a certain t-wise interaction coverage, but is based on an evolu-
tionary algorithm to optimize a sample of ﬁxed size such that its contained conﬁgurations are
as dissimilar as possible. By increasing dissimilarity, the sample’s t-wise interaction coverage
should also increase. Although this approach does not guarantee a complete t-wise interaction
coverage, it aims to increase sampling and testing eﬃciency while maintaining a reasonably
good testing eﬀectiveness. Finally, we use PRESICE to compute samples based on presence
conditions (PRESICE-PC).

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 13 / 28

6.1.1

Implementation Details

The implementation of these algorithm is provided by multiple open-source Java libraries, which
we employ in our evalution. Chvátal and ICPL are implemented in the SPLCATool [15]. IncLing
and Random are implemented in FeatureIDE [24, 26]. PLEDGE is implemented in a library of
the same name [25]. For all other sampling algorithms we use PRESICE, for which we employ
our own implementation3.

We implemented our prototype PRESICE for t-wise presence condition sampling. It includes
an algorithm for extracting presence conditions from systems that use the C preprocessor and
the kbuild build tool. PRESICE is written in Java and employs several other Java libraries to
implement its functionality.

For parsing C ﬁles and identifying preprocessor statements, we use the tool PCLocator [20],
which combines several C parsers, such as SuperC, TypeChef, and FeatureCoPP to achieve
more accurate results. This tool computes a presence condition for each line in a source ﬁle.
To this end, we analyze every C ﬁle (i.e., ﬁles with the ﬁle extensions .c, .h, .cxx, and .hxx)
in the source directories of the target SPL. For this, we exclude special directories that do
not contribute to the actual implementation of the system, but contain examples, conﬁguration
logic, or header ﬁles of system libraries. As result, we get a list containing propositional formulas
for each code block within a C project. We use this list as input for the t-wise presence condition
sampling. Currently, we did not implement an extraction algorithm for any other variability
mechanism. Thus, in our evaluation, we focus on C projects that use the C preprocessor and
kbuild as build system to enable variability. During the extraction process, we warn the user,
if we ﬁnd presence conditions that contain features that are not on the feature model and vice
versa. For our evaluation, we only consider features that we can ﬁnd in both, the feature model
and the source code.

Within our sampling algorithm, we use the satisﬁability solver Sat4J [27] to check for
validity of conﬁgurations and presence conditions. Furthermore, we use KClause [28] to extract
a feature model for C projects that use Kconﬁg as conﬁguration tool.

Regarding the random sampling, we use the default random sampling algorithm of Fea-
tureIDE [24]. Their implementation is based on Sat4J as well and generates conﬁgurations
by asking the satisﬁability solver for a valid conﬁguration using a randomized feature order.
While this algorithm does not generate uniformly distributed random samples, as it is biased
by the internal structure of the solver, it is an eﬃcient way to generate a high number of valid
conﬁgurations. Note that it is possible for this algorithm to generate a sample that contains
duplicate conﬁgurations.

6.1.2 Parameter Details

As we employ a variety of sampling algorithms in our evaluation, the required parameters diﬀer
for most of them. The only common parameter for every algorithm is the feature model, which
speciﬁes the feature dependencies. Naturally, all algorithms always use the same feature model
as input.

Regarding the parameter t, not all algorithms support the same values. IncLing is designed
as a strict pair-wise interaction coverage algorithm, and thus only works for t = 2.
ICPL
supports values for t up to 3 and Chvátal up to 4. As described in Section 5, we can run
our algorithm for t-wise presence condition sampling with any value for t. However, PRESICE
currently has a technical limitation that allows to process only up to 231 interactions. To enable
a fair comparison, we set the value of t to t = 2 for all algorithms.

As Random and PLEDGE do not try to achieve a certain t-wise coverage, but just generate
a set of valid conﬁgurations, it is not possible to set a value for t. Instead, they require to set

3https://github.com/skrieter/evaluation-pc-sampling

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 14 / 28

the size of the sample in advance. In order to ensure a fair comparison, for PLEDGE, we set the
sample size equal to the size of the largest sample computed by any variant of PRESICE (i.e.,
either PRESICE-FM, PRESICE-PC, or PRESICE-Concrete, which ever returned the largest
sample). For Random, we set several sample sizes for each system, ranging from the smallest
to the largest sample size produced for every system by any algorithm.

PLEDGE also requires to set a time limit for the evolutionary algorithm. We decided to
compare two diﬀerent limits, the maximum and minimum time that PRESICE needs to compute
a sample for a particular model (independent from its parameters settings).

For PRESICE we also have to specify additional parameters beside t. We are able to
specify which expressions should be considered for interaction (cf. Section 5.2). Thus, we
test the following setting: FM considers all t-wise interactions within a feature model, and
thus behaves like other pure t-wise interaction sampling algorithms. PC considers all t-wise
interactions between all presence conditions of a system. Finally, Concrete considers t-wise
interactions between features, but only includes features that appear in at least one presence
condition (i.e., concrete features).

6.1.3 Summary

We compare results from the following algorithms:

1. Chvátal [23]

2. ICPL [14, 15]

3. IncLing [16]

4. PLEDGE-Min (Minimum run time) [25]

5. PLEDGE-Max (Maximum run time) [25]

6. PRESICE-FM (All features within a model)

7. PRESICE-PC (All presence conditions)

8. PRESICE-Concrete (All concrete features)

9. Random [24]

6.2 Subject Systems

Currently, PRESICE can extract presence conditions from C preprocessor statements. Thus, we
selected real-world open-source systems that use the C preprocessor as a variability mechanism.
In particular, we reused 21 systems from the study of Medeiros, Kästner, Ribeiro, Gheyi, and
Apel [29], which also compared diﬀerent sampling algorithms in terms of testing eﬀectiveness.
However, most of these systems do not have a separate feature model, which prevents us from
taking their feature dependencies into account. For this reason, we include six real-world open-
source systems that use the C preprocessor and the Kconﬁg tool, namely, fiasco (latest), axtls
(latest), uclibc-ng (latest), toybox (latest), BusyBox (version 1.29.2), and Linux (version
2.6.28). For Linux, we use a feature model for version 2.6.28 provided by She et al. [30].
For all other systems, we extracted the feature models from their Kconﬁg ﬁles using the tool
KClause [28].

In Table 1, we provide an overview of the all systems. At the top we show the 6 systems for
which we have a feature model and at the bottom the 21 systems from the study of Medeiros,
Kästner, Ribeiro, Gheyi, and Apel [29]. For each respective feature model we show its number
of features (#F ), concrete features (#CF ) (i.e., features that appear in at least one presence

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 15 / 28

Table 1: Subject systems — features (#F ), concrete features (#CF ), dependencies (#D),
presence conditions (#PC ), and the number of clauses (#C ) and literals (#L) over all presence
conditions.

System (Version)

ﬁasco (latest)
axtls (latest)
uclibc-ng (latest)
toybox (latest)
BusyBox (1.29.2)
Linux (2.6.28.6)

busybox (1.23.1)
bison (2.0)
cvs (1.11.17)
libssh (0.5.3)
dia (0.97.2)
libxml2 (2.9.0)
xterm (224)
lighttpd (1.4.30)
libpng (1.5.14)
fvwm (2.4.15)
irssi (0.8.15)
gnome-keyring (3.14.0)
vim (6.0)
xﬁg (3.2.4)
totem (2.17.5)
gnome-vfs (2.0.4)
cherokee (1.2.101)
bash (4.2)
lua (5.2.1)
gnuplot (4.6.1)
apache (2.4.3)

Feature Model
#F #CF

Presence Conditions
#L
#C

#D #PC

71
95
270
323
1,018
6,888

7
32
104
8
507
1,696

120
190
1,561
90
997
80,715

–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

9
90
225
14
1,020
3,512

3,278
695
1,495
393
606
2,420
796
567
1,752
777
318
453
3,888
378
223
253
1,128
3,659
324
1,546
1,814

12
126
315
14
1,475
5,494

5,046
1,161
2,491
663
708
4,423
1,302
875
3,937
1,482
369
539
8,714
802
278
313
1,589
6,577
496
2,720
2,915

14
162
406
14
1,975
8,767

7,281
1,871
3,785
962
810
6,757
1,859
1,219
7,421
4,075
428
631
16,613
1,969
332
373
2,077
10,262
714
4,145
4,360

condition), and dependencies (#D). Regarding the extracted presence conditions, we show the
total number of conditions (#PC ), and the number of literals (#L) and clauses (#C ) over all
presence condition.

6.3 Evaluation Setup

6.3.1 Measuring Fault Detection

To answer our ﬁrst research question, we reuse some artifacts from the study of Medeiros, Käst-
ner, Ribeiro, Gheyi, and Apel [29]. In the study the authors report known faults in multiple
In this case, if the presence condition of
systems and their respective presence conditions4.
a fault is active under a given conﬁguration, it means that the fault will be present in the
corresponding product.
In total, the study presents a list of 75 unique presence conditions.
However, 23 of these conditions contained features that do not occur in the actual source code.
This can be due to abstract features, features that are only used during the build process

4http://www.dsc.ufcg.edu.br/~spg/sampling/

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 16 / 28

Table 2: Overview of presence conditions of faults used from Medeiros, Kästner, Ribeiro, Gheyi,
and Apel [29].

Degree Count Example

1
2
3

4
5

34 !ENABLE_FEATURE_SYSLOG
11 ENABLE_FEATURE_EDITING && !ENABLE_HUSH_INTERACTIVE
4 ENABLE_FEATURE_GETOPT_LONG && !ENABLE_FEATURE_SEAMLESS_LZMA

&& !ENABLE_FEATURE_TAR_LONG_OPTIONS

2 !FEAT_GUI_W32 && !PROTO && !FEAT_GUI_MOTIF && !FEAT_GUI_GTK
1 !FEAT_GUI_W32 && !FEAT_GUI_GTK && !FEAT_GUI_MOTIF
&& !FEAT_GUI_ATHENA && !FEAT_GUI_MAC && FEAT_GUI

(cid:80)

52

(e.g., in Makeﬁles), or due to features that have a diﬀerent name in the conﬁguration tool
than in the source code. Therefore, we only used the remaining 52 presence conditions in
our evaluation. Most of these presence conditions represent interaction of degree one, which
means that the selection or deselection of a single feature is enough to make them active.
However, the list also contains presence conditions that represent interactions of degree two,
three, four, and ﬁve. We show the distribution of presence conditions in Table 2 together
with an example for each degree of interaction. Note that, ﬁve of the 52 presence conditions
also have multiple clauses in their DNF. For such a case, we consider the number of literals
in the smallest clause as degree of interaction for that presence condition. For instance, the
presence condition (SHUTDOWN_SERVER && NO_SOCKET_TO_FD && START_RSH_WITH_POPEN_RW) ||
(NO_SOCKET_TO_FD && !SHUTDOWN_SERVER && START_RSH_WITH_POPEN_RW) also represent an in-
teraction of degree three, because it can be activated by the (de)selection of three features.
Thus a complete three-wise interaction coverage would be guaranteed to ﬁnd this fault. All in
all, the study includes a wide variety of interaction faults with varying degrees of complexity.
We use the list of presence conditions to check whether samples generated for theses systems
do cover each fault in at least one conﬁguration. To this end, we generate samples for each of
these systems with PRESICE-PC (i.e., presence condition coverage) and PRESICE-Concrete
(i.e., interaction coverage) for t = 1 and t = 2. We then count how many reported faults are
covered by each sample. To determine whether a fault is covered, we check if there exists at
least one conﬁguration in the sample that satisﬁes the corresponding presence condition of the
fault.

Both algorithms are susceptible to the order of features or order of presence conditions that
are provided as input, meaning that they will produce diﬀerent results for diﬀerent feature
orders. Thus, we evaluate both algorithms using multiple iterations with a randomized feature
order. In detail, we execute all algorithms 100 times, each time shuﬄing the feature order. To
enable a fair comparison we use the same 100 randomized feature orders for each algorithm. A
number of 100 iterations is an empirical value for our evaluation that provides a good trade-oﬀ
between eﬀort and accuracy.

Note, that we do not use any of the other algorithms in this experiment, as we do not have
feature models for these systems. The lack of a feature model for a system also means that
the conﬁgurations within a sample may be invalid according to the feature dependency of the
system. However, without a feature model we are not able to test this.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 17 / 28

6.3.2 Measuring Coverage

We compute the coverage achieved by every sample with regard to two diﬀerent coverage crite-
ria, pair-wise interaction coverage (FM ) and pair-wise presence condition coverage (PC ). We
consider a sample and, consequently, its sampling algorithm to be more eﬀective the higher its
coverage, as it potentially exposes more faults in the code.

Similar to the previous experiment, all used sampling algorithms are susceptible to the
feature order in a feature model. Thus, again, we execute all algorithms 100 times, each time
shuﬄing the feature order. In addition, we execute Random 10 times for each feature order,
which results in 1,000 iterations for each system. For Linux, we only use 5 iterations of the
experiment, as most algorithms take several hours to compute just one sample.

6.3.3 Measuring Sample Size

Regarding testing eﬃciency, we count the number of conﬁgurations in each sample computed
by each algorithm. We do not consider the time required to run any actual test cases of a
particular system. We do not consider the time required to run any test cases of a particular
system, as this time is depended on the actual test cases for each product and the general
testing approach. Nevertheless, we can assume that the testing time increases with the number
of conﬁguration in a sample, and thus, in general, a smaller sample will lead to a smaller testing
time. Analogous to measuring coverage, we execute each algorithm, except Random, 100 times
and randomize the feature order. Random is again run 1,000 times for each sample size.

6.3.4 Measuring Sampling Time

For measuring sampling eﬃciency, we take the time that is needed for generating a sample with
each algorithm. Each experiment runs on an own JVM, in order to mitigate any side eﬀects
(e.g., just-in-time compilation). As our algorithm requires additional information from the
source code (i.e., the presence conditions), we diﬀerentiate between the time needed to extract
the presence conditions from the source code and the time to actually generate the sample.
This is relevant, as the extraction process only needs to be run once for each system. Though it
takes some time to analyze the source code, the resulting presence conditions for each ﬁle can
be saved for later reuse. For instance, if we compute samples for diﬀerent values of t, we only
need to run the extraction process once.

6.3.5 Computing Environment

We run all algorithms on the same computing environment, with the following speciﬁcations:
CPU : Intel Core i5-8350U, Memory: 16 GB, OS : Manjaro (Arch Linux), Java: OpenJDK
15.0.2, JVM Memory: Xmx: 14 GB, Xms: 2 GB.

6.4 Evaluation Results

For brevity, we primarily present ﬁgures showing aggregated data over our measurement results.
All data and a tabular overview can be found online.5 We structure our ﬁndings according to
our four research questions, that is fault detection, coverage, testing eﬃciency, and sampling
eﬃciency. Afterwards, we analyze and discuss our results.

6.4.1 Faults Covered

We present the results of our ﬁrst experiment in Table 3. For each algorithm and value for t, we
show the number of faults that are covered or not covered by the produced samples across all

5https://github.com/skrieter/evaluation-pc-sampling/tree/master/results

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 18 / 28

Table 3: Faults covered across all 21 systems from Medeiros, Kästner, Ribeiro, Gheyi, and Apel
[29], including aggregated sample size and sampling time over all systems.

t

Algorithm

Size
∅ Min Max
4
7.5
PRESICE-PC
1
PRESICE-Concrete 1
2
2.0
22
2 65.7
PRESICE-PC
12
PRESICE-Concrete 2 16.7

14 0.3
2 0.3
167 5.6
24 0.9

∅ Min Max Yes
41
36
51
47

0.6
0.4
38.8
3.7

0.2
0.2
0.3
0.2

Time (s)

Faults Covered
No

11
16
1
5

Table 4: Relative mean sample size, mean sampling time, and mean coverage aggregated over
all 6 systems with a feature model.

Algorithm

∅Time (%) ∅Size (%) ∅Coverage (%)
PC

FM

PRESICE-FM
PRESICE-PC
PRESICE-Concrete
ICPL
Chvátal
IncLing
PLEDGE-Min
PLEDGE-Max

100.0
59.3
36.1
319.5
1,046.7
53.6
51.5
118.2

100.0
73.9
16.6
132.7
131.3
153.7
122.0
122.0

100.0
79.1
61.7
100.0
100.0
100.0
98.8
98.8

98.6
100.0
62.9
97.9
98.0
99.3
97.1
97.1

systems. The number of covered faults is the minimum number over all 100 iterations, meaning
that if any of the 100 samples for a system was not able to cover a particular fault it is not
counted as covered. Analogous, the number of not covered faults is the maximum number over
all 100 iterations. In addition, we report the aggregated sample size and sampling time over all
systems. For both values, we report its minimum, maximum and average over all 21 systems
and 100 iterations.

Of the 52 faults, which we investigated, we see that for both values of t PRESICE-PC is
able to detect more faults than PRESICE-Concrete (i.e., 31 vs. 36 for t = 1 and 51 vs. 47 for
t = 2). The presence condition that belongs to the fault that could not always be covered by
PRESICE-PC with t = 2 is the following:

ENABLE_HUSH_CASE && ENABLE_FEATURE_EDITING_SAVE_ON_EXIT
&& ENABLE_HUSH_INTERACTIVE && !ENABLE_FEATURE_EDITING

This presence condition is of degree four, and thus, when using t-wise interaction sampling, is
only guaranteed to be found with t ≥ 4.

On the other hand, we can also see that on average PRESICE-PC produced larger samples
than PRESICE-Concrete (i.e., 7.5 vs. 2.0 for t = 1 and 65.7 vs. 16.7 for t = 2). Thus, the
higher fault detection may also be a result of the larger sample sizes. However, as we pointed out
before, we do not use a feature model for this experiment. Therefore, there are no restrictions
on the conﬁguration space, which can lead to a lower sample size. Furthermore, as we see in
later experiments, a feature model, which may also include abstract features, leads to a larger
sample size than considering only concrete features.

6.4.2 Achieved Coverage

In Table 4, we show a comparison of the coverage for diﬀerent criteria for all algorithms. These
values are aggregated over all systems and all experiments using the arithmetic mean. We

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 19 / 28

Figure 3: Pair-wise presence condition coverage aggregated over all systems.

Table 5: Results of the paired t-tests for diﬀerence in FM and PC coverage between PRESICE
and other algorithms.

Algorithm

ICPL Chvatal

IncLing

FM-Coverage

PRESICE-FM
PRESICE-PC
PRESICE-Concrete

PC-Coverage

PRESICE-FM
PRESICE-PC
PRESICE-Concrete

=
–
–

+
+
–

=
–
–

+
+
–

=
–
–

–
+
–

Pledge
Min Max

+
–
–

+
+
–

+
–
–

+
+
–

show a more detailed plot of the coverage criterion PC over all systems and experiments in
Figure 3 using boxplots. In addition, we performed paired t-tests to test whether the diﬀerence
in achieved coverage by the diﬀerent algorithms is signiﬁcant. We present the results of the
statistical tests in Table 5.
In this table, we compare the coverage of all three variants of
PRESICE with the coverage of all other algorithms for the two coverage criteria FM and PC
with t = 2. The symbol = indicates that there is no signiﬁcant diﬀerence (i.e., p > 0.05) in the
achieved coverage between both algorithms. The symbol – indicates that the coverage achieved
by the variant of PRESICE is signiﬁcantly lower than the coverage of the other algorithm (i.e.,
p < 0.001). Analogous, the symbol + indicates the coverage of PRESICE is signiﬁcantly higher
(i.e., p < 0.001).

Regarding the coverage criterion FM, we can see that only the t-wise interaction sampling
algorithms (Chvátal, ICPL, IncLing, PRESICE-FM) are able to achieve a 100% coverage. Both,
PRESICE-PC and PRESICE-Concrete achieve a signiﬁcant lower PC coverage. On the other
hand, only PRESICE-PC is able to achieve a 100% coverage criterion PC. All other algorithms
produce samples with a signiﬁcant lower PC coverage on average. Still, many algorithms
(Chvátal, ICPL, IncLing, PLEDGE, PRESICE-FM) achieve a rather high average PC coverage
of over 97%.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 20 / 28

Table 6: Results of the paired t-tests for sample size diﬀerence between PRESICE and other
algorithms.

Algorithm

ICPL Chvatal

IncLing

PRESICE-FM
PRESICE-PC
PRESICE-Concrete

–
–
–

–
–
–

–
–
–

Pledge
Min Max

–
–
–

–
–
–

Figure 4: Sample size relative to PRESICE-FM for all 6 systems with a feature model.

6.4.3 Sample Size

In Table 4, we show a comparison of the sample sizes for all algorithms. Again, the values are
aggregated over all systems and all experiments using the arithmetic mean. As the actual sample
size is dependent on the subject system, we normalized the sample size for every experiment
using the sample size of PRESICE-FM as 100%. In Figure 4, we depict the sample size for
all algorithms in more detail using boxplots. Additionally, we show the absolute values of the
mean sample size per system for PRESICE-FM and PRESICE-PC in Table 7. Furthermore, we
performed paired t-tests to test whether the diﬀerence in sample size computed by the diﬀerent
algorithms is signiﬁcant and show the results in Table 6. In this table, we compare the sample
size of all three variants of PRESICE with the sample size of all other algorithms. The symbol
– indicates that the sample size computed by the variant of PRESICE is signiﬁcantly smaller
than the sample size of the other algorithm (i.e., p < 0.001). Analogous, the symbol + indicates
the sample size of PRESICE is signiﬁcantly greater (i.e., p < 0.001).

We can observe that on average all algorithms that use presence conditions as input produce
signiﬁcantly smaller samples than algorithm that only use the feature model. An exception is
the system BusyBox, for which PRESICE-PC produces samples that are about two times larger
than the sample of PRESICE-FM.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 21 / 28

Table 7: Absolute mean sample size and mean sampling time for PRESICE-FM and PRESICE-
PC for all 6 systems with a feature model.

System

ﬁasco
axtls
uclibc-ng
toybox
BusyBox
Linux

∅Size
FM PC

∅Time (s)
FM

PC

21.5
32.3
362.4
18.4
37.6
493.4

5.4
27.3
54.5
6.5
79.1
189.4

1.0
1.4
6.8
3.6
28.6
8,938.4

0.6
1.3
3.3
0.7
20.7
1,248.6

∅Extract (s)

0.7
1.4
0.8
4.5
2.1
64.5

Figure 5: Presence condition coverage compared to sample size for all algorithms aggregated
over all 6 systems with a feature model.

6.4.4 Correlation Between Coverage and Sample Size

To illustrate the correlation between sample size and testing eﬀectiveness, we show, in Figure 5,
a comparison of the coverage criterion PC with t = 2 for all algorithms for diﬀerent conﬁguration
sizes. On the x-axis, we show the sample size relative to the sample size of PRESICE-FM (i.e.,
being 100%). On the y-axis, we show coverage in % for PC with t = 2. Each data point
represents the average for all samples per algorithm and system. Random acts as a base line in
this diagram, as it does not aim for a certain coverage criterion. We can see a clear correlation
between sample size and the coverage criterion PC (i.e., increasing the sample size leads to
higher coverage on average). Further, we can see that PRESICE can reach a 100% coverage
with a substantially smaller sample than all other tested algorithms for most cases.

In addition, we calculate the Spearman’s rank correlation coeﬃcient between the degree
of coverage and the sample size for all algorithms. For the coverage criterion PC, we get a
signiﬁcant positive correlation of ≈ 0.157 (p < 0.001). Similarly, for the coverage criterion FM,
we also get a signiﬁcant positive correlation of ≈ 0.2 (p < 0.001).

6.4.5 Sampling Time

In Table 4, we show the average sampling time for all algorithms, aggregated over all systems
using the arithmetic mean and relative to the sampling time of PRESICE-FM. In addition, we

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 22 / 28

Figure 6: Sampling times of all algorithms relative to PRESICE-FM on the same system.

depict the sampling time for all algorithms in more detail in Figure 6 relative to the sampling
time of PRESICE-FM. Here, we only show the pure sampling time, excluding the time needed
for extracting presence conditions. As we described in Section 5, the extraction process is
dependent on the employed variability mechanism, but only needs to be executed once, if the
implementation artifacts do not change. In Table 7, we show for each system the absolute time
in seconds required for PRESICE to extract the presence conditions. Additionally, in Table 7,
we show the absolute values for the mean sampling time of PRESICE-FM and PRESICE-PC.
We performed paired t-tests to test whether the diﬀerence in sampling time required by the
diﬀerent algorithms is signiﬁcant.
In Table 8, we show the results of these statistical tests
by comparing the sampling time of all three variants of PRESICE with the sample size of all
other algorithms. The symbol – indicates that the sampling time computed by the variant of
PRESICE is signiﬁcantly smaller than the sampling time of the other algorithm (i.e., p < 0.001).
Analogous, the symbol + indicates the sampling time of PRESICE is signiﬁcantly greater (i.e.,
p < 0.001).

We can see that the algorithms that consider presence conditions are signiﬁcantly faster on
average than some algorithms using only the feature model (ICPL, and Chvátal, and PLEDGE-
Max). However, IncLing and PLEDGE-Min signiﬁcantly outperform PRESICE-FM and are
also on average signiﬁcantly faster than PRESICE-PC. Regarding presence condition extraction
time, we can see that in most cases it is similar to the sampling time. In case of Linux and
BusyBox the extraction time even is substantially lower than the sampling time.

6.5 Discussion

Regarding RQ1, we found in our experiments that samples with a 100% t-wise presence condi-
tion coverage were able to cover more faults than samples with 100% t-wise interaction coverage.
These results indicate that t-wise presence condition coverage is indeed able to detect more faults
than t-wise interaction coverage for the same value of t.

For RQ2, we can see that only PRESICE-PC is able to guarantee a 100% t-wise presence
condition coverage. Nevertheless, most of the other sampling algorithms, such as Chvátal, ICPL,

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 23 / 28

Table 8: Results of the paired t-tests for diﬀerence in sampling time between PRESICE and
other algorithms.

Algorithm

ICPL Chvatal

IncLing

PRESICE-FM
PRESICE-PC
PRESICE-Concrete

–
–
–

–
–
–

+
+
–

Pledge
Min Max

+
+
–

–
–
–

IncLing, and PLEDGE achieve a high pair-wise presence conditions coverage (i.e., over 97% on
average). While these results indicate an already good t-wise presence condition coverage with
traditional sampling algorithms, it also shows that samples from these algorithms most likely
miss some interactions between code blocks, which then cannot be tested. Furthermore, the
high coverage of these algorithms might be due to the relatively large sample size compared
to PRESICE-PC. Notably, random sampling with similar sample sizes also yields a similar
coverage as the traditional sampling algorithms. In summary, it is possible for other algorithms
to achieve a high t-wise presence condition coverage, though it cannot be guaranteed. However,
the high t-wise presence condition coverage of the traditional sampling algorithms seems to be
caused by their larger sample sizes.

Concerning RQ3, we observe that for all systems, except BusyBox, PRESICE-PC generates
smaller samples, than Chvátal, ICPL, IncLing, and PRESICE-FM. This may be caused by the
relatively low number of concrete features for some systems, which facilitates the coverage of
presence conditions within a conﬁguration. The larger sample size of BusyBox may be caused
due to a high number of mutually exclusive presence conditions. When considering presence
conditions, we see that even for systems with more presence conditions than features (e.g.,
axtls, uclibc-ng, and Linux) PRESICE-PC is able to generate smaller samples. Moreover, the
results of PRESICE-Concrete show that it is crucial to not just consider concrete feature, which
yields smaller samples using t-wise interaction algorithms, but only reaches a PC coverage of
about 67% on average. In summary, for most cases t-wise presence condition sampling produce
relatively small samples, which may increase its testing eﬃciency.

Regarding RQ4, we see that the sampling time of PRESICE-PC is relatively small and even
outperforms some t-wise interaction sampling algorithms. The additional time for extracting
presence conditions is similar to the sampling time for the smaller systems and even neglectable
for larger systems such as Linux. In summary, the initial generation of samples with PRESICE-
PC is only slightly less eﬃcient than with traditional sampling algorithms due to the necessary
extraction of presence conditions.

To conclude, with PRESICE we are able to eﬃciently generate relatively small samples for t-
wise presence condition coverage. In addition, our results indicate that t-wise presence condition
coverage is able to increase the fault-detection rate for product-based testing. Thus, we may be
able to increase the testing eﬃciency and testing eﬀectiveness by using t-wise presence condition
sampling.

6.6 Threats to Validity

We are aware that our evaluation might suﬀer from some biases that may threaten its validity.
First, we are using a rather small set of systems with feature models in our evaluation. Thus,
the results might not scale to other systems with more features or presence conditions. However,
we reused systems from other research and try to get a wide range of feature model sizes.

Second, we do not evaluate the actual testing eﬀectiveness of our approach, but only compare
it to other algorithms with respect to our own coverage criterion. As we proposed the coverage

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 24 / 28

criterion of t-wise presence condition coverage ourself, this may create an unfair comparison
with other algorithms that aim for diﬀerent coverage criteria. We do evaluate whether samples
with 100% t-wise presence condition coverage are able to detect some known faults in several
systems. However, as this is only a small set of faults, it may hamper the generalizability of our
results. Nevertheless, we tried to include faults with varying degrees of interaction and complex
presence conditions to mitigate this bias.

Third, we aim to evaluate the concept of t-wise presence condition sampling. However, we
only use one particular sampling algorithm to cover interactions between presence conditions.
It may be the case that the concept works better or worse when employing other sampling
algorithms or diﬀerent heuristics. Likewise, there is the chance of implementation bugs that
may bias the result. To this end, we use automated tests to ensure that the samples we receive
from PRESICE are valid and indeed achieve a t-wise presence condition coverage of 100%.
Furthermore, we used one particular algorithm for extracting presence conditions from C pre-
processor SPLs, which has limitations that may aﬀect the correctness of the extracted presence
conditions. However, the tool on which our algorithm is based employs three established tools
for analyzing C preprocessor annotations in order to achieve more reliable results.

Finally, as we are using randomized features orders and random sampling, our results may
be aﬀected by a random bias. We tried to mitigate this by using multiple iterations of all
experiments.

7 Related Work

There exist many diﬀerent approaches to sampling an SPL [5, 31]. Like many other sampling
algorithms, our sampling algorithm uses a greedy strategy to compute a minimal sample [8,
10, 14–16, 32–41]. In fact, our sampling algorithm is based on YASA [21], which is similar to
the algorithm IPOG [42], as it also starts with an empty sample and iteratively adds literals.
However, our sampling algorithm can cope with more general inputs, and thus is able to process
arbitrary presence conditions. Furthermore, there also exist many algorithms that employ meta-
heuristic strategies [7, 43–52] or other strategies [53] to cope with the variability explosion
problem. These approaches can be seen as complementary to our concept, as we may also
adapt meta-heuristic sampling to support presence conditions.

There already exist sampling strategies that consider other inputs in addition to a feature
model [31]. Analogous to our approach, there are sampling algorithms that consider imple-
mentation artifacts [10, 38, 41, 54] and test artifacts [35, 36] to compute a sample. While
these algorithms also consider the underlying SPL we are the ﬁrst to combine presence con-
ditions for implementation artifacts with regular t-wise interaction sampling to achieve higher
eﬀectiveness.

8 Conclusion & Future Work

With t-wise presence condition coverage, we present a new coverage criterion for SPLs that
considers the actual implementation artifacts by looking at their presence conditions. Further,
we describe a t-wise sampling algorithm that covers presence conditions instead of features
and implement it for systems that use the C preprocessor. We test the fault-detection rate of
t-wise presence condition coverage in comparison to t-wise interaction coverage. We also test
our implementation by comparing it to existing sampling algorithms with regard to achieved
coverage, sampling size, and sampling time. We ﬁnd that t-wise presence condition coverage is
able to detect more faults for a given t and that t-wise presence condition sampling produces
mostly smaller samples compared to t-wise interaction sampling, while guaranteeing a 100% t-
wise presence condition coverage. Therefore, we suspect that t-wise presence condition sampling

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 25 / 28

has the potential to increase both, testing eﬀectiveness and testing eﬃciency.

Regarding future work, there are several research aspects, we would like to investigate
further. To begin with, we want to investigate whether the results we achieved in our evaluation
for testing eﬀectiveness and eﬃciencies also scale to large systems with more or more complex
presence conditions. In addition, we also want to evaluate the impact on testing eﬀectiveness
and eﬃciency, when grouping presence conditions as outlined in Section 5.2. Furthermore, there
are many sampling strategies beside t-wise interaction sampling, for instance all-yes, all-no, 1-
enabled, and 1-disabled. Each strategy uses a heuristic to trade-oﬀ testing eﬃciency against
overall testing eﬀectiveness. We can easily adapt our coverage criterion and sampling algorithm
to consider these strategies for presence condition coverage instead of t-wise. Thus, we want
to investigate, whether there is a beneﬁt in using one of these strategies in combination with
presence conditions.

Acknowledgments

We thank Tobias Heß for his valuable feedback.

References

[1] Paul Ammann and Jeﬀ Oﬀutt. Introduction to software testing. Cambridge University

Press, 2016.

[2] John McGregor. “Testing a Software Product Line”. In: Testing Techniques in Software

Engineering. Springer, 2010, pp. 104–140.

[3] Emelie Engström and Per Runeson. “Software Product Line Testing - A Systematic Map-
ping Study”. In: J. Information and Software Technology (IST) 53.1 (2011), pp. 2–13.
[4] Jihyun Lee, Sungwon Kang, and Danhyung Lee. “A Survey on Software Product Line
Testing”. In: Proc. Int’l Systems and Software Product Line Conf. (SPLC). ACM, 2012,
pp. 31–40.

[5] Thomas Thüm, Sven Apel, Christian Kästner, Ina Schaefer, and Gunter Saake. “A Classiﬁ-
cation and Survey of Analysis Strategies for Software Product Lines”. In: ACM Computing
Surveys 47.1 (2014), 6:1–6:45.

[6] Myra B. Cohen, Matthew B. Dwyer, and Jiangfan Shi. “Constructing Interaction Test
Suites for Highly-Conﬁgurable Systems in the Presence of Constraints: A Greedy Ap-
proach”. In: IEEE Trans. Software Engineering (TSE) 34.5 (2008), pp. 633–650.

[8]

[7] Dusica Marijan, Arnaud Gotlieb, Sagar Sen, and Aymeric Hervieu. “Practical Pairwise
Testing for Software Product Lines”. In: Proc. Int’l Systems and Software Product Line
Conf. (SPLC). ACM, 2013, pp. 227–235.
Iago Abal, Jean Melo, Stefan Stănciulescu, Claus Brabrand, Márcio Ribeiro, and Andrzej
Wąsowski. “Variability Bugs in Highly Conﬁgurable Systems: A Qualitative Analysis”. In:
Trans. Software Engineering and Methodology (TOSEM) 26.3 (2018), 10:1–10:34.
[9] Christian Kästner, Sven Apel, Syed Saif ur Rahman, Marko Rosenmüller, Don Batory,
and Gunter Saake. “On the Impact of the Optional Feature Problem: Analysis and Case
Studies”. In: Proc. Int’l Systems and Software Product Line Conf. (SPLC). Software En-
gineering Institute, 2009, pp. 181–190.

[10] Reinhard Tartler, Christian Dietrich, Julio Sincero, Wolfgang Schröder-Preikschat, and
Daniel Lohmann. “Static Analysis of Variability in System Software: The 90,000 #Ifdefs
Issue”. In: Proc. USENIX Annual Technical Conference (ATC). USENIX Association,
2014, pp. 421–432.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 26 / 28

[11] Sebastian Ruland, Lars Luthmann, Johannes Bürdek, Sascha Lity, Thomas Thüm, Malte
Lochau, and Márcio Ribeiro. “Measuring Eﬀectiveness of Sample-Based Product-Line
Testing”. In: Proc. Int’l Conf. on Generative Programming and Component Engineering
(GPCE). ACM, 2018, pp. 119–133.

[12] Richard M. Stallman and Zachary Weinberg. “The C preprocessor”. In: Free Software

Foundation (1987).

[13] Jörg Liebig, Sven Apel, Christian Lengauer, Christian Kästner, and Michael Schulze. “An
Analysis of the Variability in Forty Preprocessor-Based Software Product Lines”. In: Proc.
Int’l Conf. on Software Engineering (ICSE). IEEE, 2010, pp. 105–114.

[14] Martin Fagereng Johansen, Øystein Haugen, and Franck Fleurey. “Properties of Realistic
Feature Models Make Combinatorial Testing of Product Lines Feasible”. In: Proc. Int’l
Conf. on Model Driven Engineering Languages and Systems (MODELS). Springer, 2011,
pp. 638–652.

[15] Martin Fagereng Johansen, Øystein Haugen, and Franck Fleurey. “An Algorithm for Gen-
erating T-Wise Covering Arrays from Large Feature Models”. In: Proc. Int’l Systems and
Software Product Line Conf. (SPLC). ACM, 2012, pp. 46–55.

[16] Mustafa Al-Hajjaji, Sebastian Krieter, Thomas Thüm, Malte Lochau, and Gunter Saake.
“IncLing: Eﬃcient Product-line Testing Using Incremental Pairwise Sampling”. In: Proc.
Int’l Conf. on Generative Programming: Concepts & Experiences (GPCE). ACM, 2016,
pp. 144–155.

[17] Paul Clements and Linda Northrop. Software Product Lines: Practices and Patterns.

Addison-Wesley, 2001.

[18] Klaus Pohl, Günter Böckle, and Frank J. van der Linden. Software Product Line Engi-

neering: Foundations, Principles and Techniques. Springer, 2005.

[19] Sven Apel, Don Batory, Christian Kästner, and Gunter Saake. Feature-Oriented Software

Product Lines. Springer, 2013.

[20] Elias Kuiter, Sebastian Krieter, Jacob Krüger, Kai Ludwig, Thomas Leich, and Gunter
Saake. “PClocator: a tool suite to automatically identify conﬁgurations for code locations”.
In: Proc. Int’l Systems and Software Product Line Conf. (SPLC). 2018, pp. 284–288.
[21] Sebastian Krieter, Thomas Thüm, Sandro Schulze, Gunter Saake, and Thomas Leich.
“YASA: yet another sampling algorithm”. In: Proc. Int’l Working Conf. on Variability
Modelling of Software-Intensive Systems (VaMoS). ACM, 2020, 4:1–4:10.

[22] Mustafa Al-Hajjaji, Thomas Thüm, Malte Lochau, Jens Meinicke, and Gunter Saake. “Ef-
fective Product-Line Testing Using Similarity-Based Product Prioritization”. In: Software
and System Modeling (SoSyM) 18.1 (2019), pp. 499–521.

[23] Vasek Chvatal. “A Greedy Heuristic for the Set-Covering Problem”. In: Mathematics of

operations research 4.3 (1979), pp. 233–235.

[24] Mustafa Al-Hajjaji, Jens Meinicke, Sebastian Krieter, Reimar Schröter, Thomas Thüm,
Thomas Leich, and Gunter Saake. “Tool Demo: Testing Conﬁgurable Systems with Fea-
tureIDE”. In: Proc. Int’l Conf. on Generative Programming: Concepts & Experiences
(GPCE). ACM, 2016, pp. 173–177.

[25] Christopher Henard, Mike Papadakis, Gilles Perrouin, Jacques Klein, Patrick Heymans,
and Yves Le Traon. “Bypassing the Combinatorial Explosion: Using Similarity to Generate
and Prioritize T-Wise Test Conﬁgurations for Software Product Lines”. In: IEEE Trans.
Software Engineering (TSE) 40.7 (2014), pp. 650–670.

[26] Jens Meinicke, Thomas Thüm, Reimar Schröter, Fabian Benduhn, Thomas Leich, and

Gunter Saake. Mastering Software Variability with FeatureIDE. Springer, 2017.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 27 / 28

[27] Daniel Le Berre and Anne Parrain. “The sat4j library, release 2.2, system description”.
In: Journal on Satisﬁability, Boolean Modeling and Computation 7 (2010), pp. 59–64.
[28] Jeho Oh, Paul Gazzillo, Don Batory, Marijn Heule, and Maggie Myers. Uniform sampling
from kconﬁg feature models. Tech. rep. TR-19-02. The University of Texas at Austin,
Department of Computer Science, 2019.

[29] Flávio Medeiros, Christian Kästner, Márcio Ribeiro, Rohit Gheyi, and Sven Apel. “A
Comparison of 10 Sampling Algorithms for Conﬁgurable Systems”. In: Proc. Int’l Conf.
on Software Engineering (ICSE). ACM, 2016, pp. 643–654.

[30] Steven She, Rafael Lotufo, Thorsten Berger, Andrzej Wąsowski, and Krzysztof Czar-
necki. “Reverse Engineering Feature Models”. In: Proc. Int’l Conf. on Software Engineer-
ing (ICSE). ACM, 2011, pp. 461–470.

[31] Mahsa Varshosaz, Mustafa Al-Hajjaji, Thomas Thüm, Tobias Runge, Mohammad Reza
Mousavi, and Ina Schaefer. “A Classiﬁcation of Product Sampling for Software Product
Lines”. In: Proc. Int’l Systems and Software Product Line Conf. (SPLC). ACM, 2018,
pp. 1–13.

[32] Alireza Ensan, Ebrahim Bagheri, Mohsen Asadi, Dragan Gasevic, and Yevgen Biletskiy.
“Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models”.
In: Proc. Inte’l Conf. on Information Technology: New Generations (ITNG). IEEE, 2011,
pp. 291–298.

[33] Evelyn Nicole Haslinger, Roberto E. Lopez-Herrejon, and Alexander Egyed. “Using Fea-
ture Model Knowledge to Speed Up the Generation of Covering Arrays”. In: Proc. Int’l
Workshop on Variability Modelling of Software-Intensive Systems (VaMoS). ACM, 2013,
16:1–16:6.

[34] Martin Fagereng Johansen, Øystein Haugen, Franck Fleurey, Anne Grete Eldegard, and
Torbjørn Syversen. “Generating Better Partial Covering Arrays by Modeling Weights on
Sub-Product Lines”. In: Proc. Int’l Conf. on Model Driven Engineering Languages and
Systems (MODELS). Springer, 2012, pp. 269–284.

[35] Chang Hwan Peter Kim, Don Batory, and Sarfraz Khurshid. “Reducing Combinatorics in
Testing Product Lines”. In: Proc. Int’l Conf. on Aspect-Oriented Software Development
(AOSD). ACM, 2011, pp. 57–68.

[36] Chang Hwan Peter Kim, Eric Bodden, Don Batory, and Sarfraz Khurshid. “Reducing
Conﬁgurations to Monitor in a Software Product Line”. In: Proc. Int’l Conf. on Runtime
Veriﬁcation (RV). Springer, 2010, pp. 285–299.

[37] Matthias Kowal, Sandro Schulze, and Ina Schaefer. “Towards Eﬃcient SPL Testing by
Variant Reduction”. In: Proc. Int’l Workshop on Variability and Composition (VariComp).
ACM, 2013, pp. 1–6.

[38] Jörg Liebig, Alexander von Rhein, Christian Kästner, Sven Apel, Jens Dörre, and Chris-
tian Lengauer. “Scalable Analysis of Variable Software”. In: Proc. Europ. Software Engi-
neering Conf./Foundations of Software Engineering (ESEC/FSE). ACM, 2013, pp. 81–
91.

[39] Sebastian Oster, Florian Markert, and Philipp Ritter. “Automated Incremental Pairwise
Testing of Software Product Lines”. In: Proc. Int’l Systems and Software Product Line
Conf. (SPLC). Springer, 2010, pp. 196–210.

[40] Dennis Reuling, Johannes Bürdek, Serge Rotärmel, Malte Lochau, and Udo Kelter. “Fault-
Based Product-Line Testing: Eﬀective Sample Generation Based on Feature-Diagram Mu-
tation”. In: Proc. Int’l Systems and Software Product Line Conf. (SPLC). ACM, 2015,
pp. 131–140.

T-Wise Presence Condition Coverage and Sampling for Conﬁgurable Systems

Page 28 / 28

[41] Jiangfan Shi, Myra B. Cohen, and Matthew B. Dwyer. “Integration Testing of Software
Product Lines Using Compositional Symbolic Execution”. In: Proc. Int’l Conf. on Fun-
damental Approaches to Software Engineering (FASE). Springer, 2012, pp. 270–284.
[42] Yu Lei, Raghu N. Kacker, D. Richard Kuhn, Vadim Okun, and James Lawrence. “IPOG:
A General Strategy for T-Way Software Testing”. In: Proc. Int’l Conf. on Engineering of
Computer-Based Systems (ECBS). IEEE, 2007, pp. 549–556.

[43] Anastasia Cmyrev and Ralf Reissing. “Eﬃcient and Eﬀective Testing of Automotive Soft-

ware Product Lines”. In: Int’l J. Applied Science and Technology (IJAST) 7.2 (2014).

[44] Thiago N. Ferreira, Josiel Neumann Kuk, Aurora Pozo, and Silvia Regina Vergilio. “Prod-
uct Selection Based on Upper Conﬁdence Bound MOEA/D-DRA for Testing Software
Product Lines”. In: Proc. Congress Evolutionary Computation (CEC). IEEE, 2016, pp. 4135–
4142.

[45] Faezeh Ensan, Ebrahim Bagheri, and Dragan Gasevic. “Evolutionary Search-Based Test
Generation for Software Product Line Feature Models”. In: Proc. Int’l Conf. on Advanced
Information Systems Engineering (CAiSE). Vol. 7328. Springer, 2012, pp. 613–628.
[46] Thiago N. Ferreira, Jackson A. Prado Lima, Andrei Strickler, Josiel N. Kuk, Silvia R.
Vergilio, and Aurora Pozo. “Hyper-Heuristic Based Product Selection for Software Prod-
uct Line Testing”. In: Comp. Intell. Mag. (CIM) 12.2 (2017), pp. 34–45.

[47] Helson L. Jakubovski Filho, Jackson A. Prado Lima, and Silvia R. Vergilio. “Automatic
Generation of Search-Based Algorithms Applied to the Feature Testing of Software Prod-
uct Lines”. In: Proc. Brazilian Symposium on Software Engineering (SBES). ACM, 2017,
pp. 114–123.

[48] Christopher Henard, Mike Papadakis, and Yves Le Traon. “Mutation-Based Generation
of Software Product Line Test Conﬁgurations”. In: Search-Based Software Engineering.
Ed. by Claire Le Goues and Shin Yoo. Vol. 8636. Lecture Notes in Computer Science.
Springer International Publishing, 2014, pp. 92–106.

[49] Christopher Henard, Mike Papadakis, Gilles Perrouin, Jacques Klein, and Yves Le Traon.
“Multi-Objective Test Generation for Software Product Lines”. In: Proc. Int’l Systems
and Software Product Line Conf. (SPLC). ACM, 2013, pp. 62–71.

[50] Roberto Erick Lopez-Herrejon, Javier Ferrer, Francisco Chicano, Alexander Egyed, and
Enrique Alba. “Comparative Analysis of Classical Multi-Objective Evolutionary Algo-
rithms and Seeding Strategies for Pairwise Testing of Software Product Lines”. In: Proc.
Congress Evolutionary Computation (CEC). IEEE, 2014, pp. 387–396.

[51] Rui Angelo Matnei Filho and Silvia Regina Vergilio. “A Multi-Objective Test Data Gen-

eration Approach for Mutation Testing of Feature Models”. In: 4.1 (2016).

[52] Xavier Devroey, Gilles Perrouin, Axel Legay, Pierre-Yves Schobbens, and Patrick Hey-
mans. “Covering SPL Behaviour with Sampled Conﬁgurations: An Initial Assessment”.
In: Proc. Int’l Workshop on Variability Modelling of Software-Intensive Systems (VaMoS).
ACM, 2015, 59:59–59:66.

[53] Gilles Perrouin, Sagar Sen, Jacques Klein, Benoit Baudry, and Yves Le Traon. “Automated
and Scalable T-Wise Test Case Generation Strategies for Software Product Lines”. In:
Proc. Int’l Conf. on Software Testing, Veriﬁcation and Validation (ICST). IEEE, 2010,
pp. 459–468.

[54] Reinhard Tartler, Daniel Lohmann, Christian Dietrich, Christoph Egger, and Julio Sin-
cero. “Conﬁguration Coverage in the Analysis of Large-Scale System Software”. In: ACM
SIGOPS Operating Systems Review 45.3 (2012), pp. 10–14.

