XX

2
2
0
2

r
p
A
1

]
E
S
.
s
c
[

1
v
4
9
6
0
0
.
4
0
2
2
:
v
i
X
r
a

Testing Feedforward Neural Networks Training Programs

HOUSSEM BEN BRAIEK, SWAT Lab., Polytechnique Montreal, Canada
FOUTSE KHOMH, SWAT Lab., Polytechnique Montréal, Canada

Nowadays, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep
Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving
cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases
that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the
training program is bug-free and appropriately configured. However, satisfying this assumption for a novel
problem requires significant engineering work to prepare the data, design the DNN, implement the training
program, and tune the hyperparameters in order to produce the model for which current automated test data
generators search for corner-case behaviors. All these model training steps can be error-prone. Therefore, it is
crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and
not only on the resulting DNN model.

In this paper, we gather a catalog of training issues and based on their symptoms and their effects on the
behavior of the training program, we propose practical verification routines to detect the aforementioned
issues, automatically, by continuously validating that some important properties of the learning dynamics hold
during the training. Then, we design, TheDeepChecker, an end-to-end property-based debugging approach
for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we
conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL
programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show
that TheDeepChecker’s on-execution validation of DNN-based program’s properties through three sequential
phases (pre-, on-, and post-fitting), succeeds in revealing several coding bugs and system misconfigurations
errors, early on and at a low cost. Moreover, our property-based approach outperforms the SMD’s offline
rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of
additional DL bugs.
CCS Concepts: • Theory of computation → Program verification; • Software and its engi-
neering → Maintaining software.

Additional Key Words and Phrases: neural networks, training, testing, debugging

ACM Reference Format:
Houssem Ben Braiek and Foutse Khomh. 2020. Testing Feedforward Neural Networks Training Programs. J.
ACM XX, XX, Article XX (April 2020), 61 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Nowadays, we are witnessing an increase in efforts to improve the reliability of Deep Neural
Networks (DNNs), and consequently the trustworthiness of deep learning based systems such
as self-driving cars or aircraft collision-avoidance systems [12]. Multiple approaches are being

Authors’ addresses: Houssem Ben Braiek, SWAT Lab., Polytechnique Montreal, Montreal, Canada, houssem.ben-braiek@
polymtl.ca; Foutse Khomh, foutse.khomh@polymtl.ca, SWAT Lab., Polytechnique Montréal, 2500 Chemin de Polytechnique,
Montréal, Canada.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2020 Association for Computing Machinery.
0004-5411/2020/4-ARTXX $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

 
 
 
 
 
 
XX:2

Ben Braiek and Khomh

developed to search for test cases that can expose inconsistencies in the behavior of DNN mod-
els; we refer to them as model testing approaches. These approaches implicitly assume that the
trained DNN is already performing well on the original test dataset compared to known results,
i.e., the training program is bug-free and appropriately configured. However, recent research
works [42] [40] [99] report that bugs exist in DL programs, which invalidates this assumption
made by model testing approaches. Their main finding is that faults that lead to program crashes
represent only a fraction of real bugs found in DNN training programs. The vast majority of bugs
are due to hidden logic errors and configuration inconsistencies, leading to silent failures that are
difficult to detect (since they do not prevent the program from running and producing a model).
In fact, a DNN is trained using the back-propagation algorithm that relies on a loss function to
estimate the distance between actual predictions and the ground truth, and then, the estimated error
is back propagated through the DNN’s learnable parameters to adjust their values in the opposite
direction of the loss gradient. In practice, components of the training algorithm are provided as
ready-to-use configurable routines by DL libraries, however, reusing these routines to implement
a training program for a designed DNN is not straightforward and it can be error-prone. From a
fundamental point of view, the backpropagation algorithm can be considered as a leaky abstraction
since the details of its implementation are not trivial. To illustrate this point let’s consider the basic
rule of weights initialization which states that values should be small random numbers. Setting
weights’ values is not simple and straightforward because the use of dummy random initialization
could prevent the DNN from training. In practice, depending on developers’ design choices, there
is a set of custom weight initializations that have been formally proven to be optimal choices, and
hence should be adopted by the developers.

DL developers also leverage generic-purpose libraries for scientific computations and DL routines
that support a rich set of APIs, incorporating many assumptions in order to make them ready-to-
use for common problems. These DL routines can be configured and adapted for new problems.
However, their misuses, i.e., the usage of APIs without fully understanding their inner functioning
and–or checking that their assumptions are fulfilled, are likely to result in erroneous behavior.
The implementation of the training steps can also contain errors leading to erroneous behavior.
It is therefore very important to test the source code of the training program, to ensure that it
accurately reflects the algorithm’s mathematical formulation.
The current trend in testing the resulting DNN model is not sufficient to ensure that the training
process was bug-free and consequently that the DNN model is reliable. Because all models are
imperfect abstractions of the reality and subject to residual error, it is not possible to rely on the
observable relations between the inputs and outputs of the model to hunt for errors in the training
program; if the trained model produces a wrong prediction, it does not necessarily mean that the
training program is buggy. Moreover, the level of performance of a model (in terms of precision,
recall, or accuracy) cannot serve as a reliable predictor of the reliability of its training process,
since it is unclear what should be an appropriate level of performance for a model, to discard
any coincidental high probabilistic measure while taking into consideration various assumptions
regarding model capacity and data complexity. It is unclear how much precision, recall, or accuracy
may be indicative of errors in the training process. D’amour et al. [19] have investigated the
credibility of various practical ML systems, using examples from computer vision, medical imaging,
natural language processing, etc. They conclude the limitation of model testing on the held-out
data, called iid performance, and show that plenty of ML models share strong iid performance on
test datasets, however, they often exhibit unexpectedly distant behaviors when they are deployed
in real-world domains. Hence, the iid performance test is insufficient to separate between effective
and non-effective models that are trained using, respectively, clean and buggy training programs.
Therefore, we need more indicative measures than probabilistic correctness measures. It is crucial

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:3

to develop efficient debugging techniques that developers can use to detect errors in the training
programs of DNNs. Besides the loss- and accuracy evolution curves, researchers [39] have recently
proposed advanced visual analytics systems to help DL developers debug and refine the design
of their DNNs. Oftentimes, this requires monitoring individual computational units of the model
during the training, such as activation maps or error gradients produced at each layer, and after
training, an instance-based analysis is performed to identify misclassified instances from a handful
of chosen data instances. However, such visualization based diagnosis techniques require significant
human intervention and good expertise on deep learning concepts. Given the internal complexity
of a DNN, it is always challenging to select a handful of drawing representations (i.e., suitable for
screen display) that should be watched and analyzed, interactively. Additionally, these visualizations
can hardly be used automatically to track for regressions after a program update. We believe that
full-fledged software debuggers are needed to support DL developers.
In this paper, we propose TheDeepChecker, the first end-to-end automated debugging approach
for DNN training programs. To develop TheDeepChecker, we gathered a catalog of fundamental
algorithmic and development issues in relation with the DNN training programs. Then, we inferred
the properties that are violated by these identified training program issues, in order to develop the
verification routines that can be used to detect their occurrences during the training process. Next,
we developed a property-based testing method that activates the derived verification routines, in
order to drive an end-to-end automated debugging process for DNN training programs. To assess
the effectiveness of TheDeepChecker, we implemented it as a TensorFlow-based testing framework
that enables the automatic detection of the identified issues in DNN training programs developed
with Tensorflow (TF) library. We rely on the taxonomy of real DL faults elaborated in [40] and
further searching on Stackoverflow, to identify the structure of faults occuring in DNN training
programs. Then, we inject these faults in clean DNN training programs to create a set of synthetic
buggy programs, each one containing a particular fault, aiming at challenging the TheDeepChecker
in identifying the faults or steering the users to them by detecting precise fault-indicative symptoms.
Moreover, we assess the performance of TheDeepChecker on a mixed selection of 20 real-world
TF buggy programs [99] splitted equally between snippets of code shared on StackOverflow (SO)
and bug-fixing commits from DL projects hosted on GitHub (GH). As a DL debugging baseline,
we manage to run all the studied buggy DL programs on Amazon SageMaker Cloud ML service
while activating its internal Debugger’s built-in rules. Results show that TheDeepChecker can
successfully support DL practitioners in detecting earlier a wide range of coding bugs and system
misconfigurations through reported violations of essential DL program’s properties. Additionally,
the comparison with SageMaker Debugger (SMD) highlights the ascendancy of TheDeepChecker’s
on-execution validation of DL properties over SMD’s rules verification on training logs in terms of
detection accuracy and DL bugs’ coverage.
This paper makes the following contributions.

• We investigate former DL faults’ empirical studies and SO posts about buggy TF training
programs to identify common errors made by DL practitioners and provide a catalog outlining
DL program’s development and configuration pitfalls;

• We present a property-based DL software debugging approach, which implements verification
routines associated to the discovered DL pitfalls across three sequential phases: pre-, on-,
and post-fitting of the DNN.

• We implement our debugging approach based on TF concepts and features to demonstrate
the effectiveness of our verification routines in detecting real bugs that occur in TF-based
training programs.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:4

Ben Braiek and Khomh

• By dissecting the root causes and symptoms of collected DL bugs, we simulate these bugs
in DNN architectures to create a benchmark of synthetic buggy programs. In addition, a
real-world database of buggy DL programs is compiled from snippets of code shared on SO
posts and bug-fixing commits in GH projects.

• We evaluate the effectiveness of TheDeepChecker in debugging synthetic and real-world TF
programs and compare its detection bugs’ accuracy and coverage with Amazon Sagemaker
Debugger (SMD). Besides, we conduct a usability study on TheDeepChecker in collaboration
with two experienced DL engineers from the industry, focusing on the usefulness and preci-
sion of TheDeepChecker’s debugging reports to steer users towards finding and fixing the
actual faults in real DL buggy programs.

The remainder of this paper is organized as follows. Section 2 provides background infor-
mation about DL, property-based software testing, and TensorFlow. Section 3 presents common
training program pitfalls discovered by DL researchers and experienced by DL developers. Section 4
introduces our proposed property-based debugging approach for DNN training programs alongside
their derived verification mechanisms and its TF-based implementation. Section 5 reports about the
empirical evaluation of our proposed debugging approach. Sections 6, 7,and 8 discuss, respectively,
the threats to validity, the limitations and future works, and the related literature. Finally, Section 9
concludes the paper.

2 BACKGROUND
This section provides background information about DL, property-based software testing, and
TensorFlow.

2.1 Deep Neural Networks
A deep neural network (DNN) is an artificial neural network with a stack of multiple computational
layers, hence the adjective “deep”. DNNs are often much harder to train than shallow neural
networks [29]. However, they are endued with a hierarchical features learning that lets them
capture increasingly complex patterns directly from the data when they are appropriately designed
and trained. DNNs include many variants of architectures that have found success in several
domains. We present in the following the Deep Feed-Forward Neural Network and its popular
variant Convolutional Neural Network.

2.1.1 Deep Feed-Forward Neural Networks. Feedforward neural network (FNN) architecture is
the quintessential and the most used neural network [30]. The objective of FNNs is to learn the
mapping of a fixed-size input (for example, a signal vector) to a fixed-size output (for example,
a probability for each label). Apart from the input and output layers, FNN contains a cascade of
multiple intermediate layers, which are called hidden layers because the ground truth data does
not provide the desired values for these layers. Last, the name feedforward arises from the fact that
information flows through the processing layers in a feed-forward manner, i.e., from input layer,
through hidden layers and finally to the output layer. Figure 1 illustrates a simple FNN.

A FNN encapsulates a mapping function 𝑓 that maps the input 𝑥 to its corresponding output 𝑦
as presented in Equation 1. If 𝑦 is a category label, the FNN solves a classification problem and if 𝑦
is a continuous value, the FNN performs a regression task.

𝑦 = 𝑓 (𝑥;𝑊 , 𝑏)

(1)

Where 𝑊 : weights and 𝑏: biases represent the learnable parameters.
Given the training data 𝐷 = {(𝑥𝑖, 𝑡𝑖 ); ∀𝑖 ∈ [1, 𝑛]}, the FNN training algorithm aims to find the best
approximation function 𝑓 ∗ by learning the optimal values of its inner parameters 𝑊 ∗ and 𝑏∗. To do

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:5

Fig. 1. Schema of feedforward neural network

that, a loss function, 𝑙𝑜𝑠𝑠 (𝑓 , 𝑥, 𝑡) is leveraged to measure how well the prediction 𝑦 = 𝑓 (𝑥) matches
the ground truth output 𝑡. Considering the example of multinomial classification problem, we have
𝑦, a vector of probabilities where 𝑦 ( 𝑗) represents the probability that 𝑥 belongs to the label 𝑗 and 𝑡,
a one-hot encoding label where 𝑡 ( 𝑗) = 1 if 𝑗 is the true label; otherwise 𝑡 ( 𝑗) = 0. The most common
loss function is the cross entropy 𝑙𝑜𝑠𝑠 (𝑓 , 𝑥, 𝑡) = − (cid:205)𝑗 𝑡 ( 𝑗) log(𝑦 ( 𝑗)).
To estimate the loss over all the training data 𝑙𝑜𝑠𝑠 (𝑓 , 𝐷), we use an expectation, as formulated in
Equation 2, that can be either the average or the sum of data instances’ losses.

𝑙𝑜𝑠𝑠 (𝑓 , 𝐷) = 𝐸𝑖 [𝑙𝑜𝑠𝑠 (𝑓 , 𝑥𝑖, 𝑦𝑖 )] = 𝐸𝐷 [𝑙𝑜𝑠𝑠 (𝑓 , 𝐷)]
(2)
Therefore, the optimal parameters 𝑊 ∗ and 𝑏∗ result in the best function approximation that spawns
the minimum possible estimated loss in the training data 𝐷.

𝑊 ∗, 𝑏∗ = argmin

𝐸𝐷 [𝑙𝑜𝑠𝑠 (𝑓 ∗, 𝐷)]

𝑊 ,𝑏

(3)

The loss minimization problem can be solved, iteratively, using gradient descent algorithm, where
the following equations represent an iteration’s updates :

𝑊 (𝑡 +1) = 𝑊 (𝑡 ) − 𝜂

𝜕𝐸𝐷 [𝑙𝑜𝑠𝑠 (𝑓 (𝑡 ), 𝐷)]
𝜕𝑊

;

𝑏 (𝑡 +1) = 𝑏 (𝑡 ) − 𝜂

𝜕𝐸𝐷 [𝑙𝑜𝑠𝑠 (𝑓 (𝑡 ), 𝐷)]
𝜕𝑏

(4)

As introduced, the FNN assembles multiple computational layers and each layer 𝑙 perform a
computation; so it encapsulates a kind of sub-function 𝑓𝑙 (𝑥;𝑊𝑙, 𝑏𝑙 ) with inner parameters 𝑊𝑙 and
𝑏𝑙 . Thus, the approximate mapping function 𝑓 of an FNN with 𝐿 layers is a composite function
formulated as below :

𝑓 (𝑥;𝑊 , 𝑏) = 𝑓𝐿 (𝑓𝐿−1(...(𝑓1(𝑥;𝑊1, 𝑏1)...);𝑊𝐿−1, 𝑏𝐿−1);𝑊𝐿, 𝑏𝐿)

(5)

Where 𝑊 = {𝑊𝑙, ∀𝑙 ∈ [1, 𝐿]} and 𝑏 = {𝑏𝑙, ∀𝑙 ∈ [1, 𝐿]}.
Given the huge number of parameters to approximate, the computation of gradients would be very
time-consuming. Deep learning relies on a fast algorithm, named backpropagation, that applies the
derivative chain rule principle to compute, sequentially, all these derivative backing from the output
to the first hidden layer, while taking full advantage of the derivatives estimated w.r.t previous
layers. Backpropagation is based on two alternatives main phases, respectively, forward pass and
backward pass, which are detailed below.
Forward Pass. Each hidden layer 𝑙 contains a set of computation units, called neurons, that
perform a linear transformation 𝑧𝑙 of their inputs from previous layers and pass the result through
an activation function Φ𝑙 . The latter is a non-linear transformation 𝑎𝑙 that allows adding non-
linearity in the approximated mapping function in order to be insensitive to irrelevant variations
of the input. The layer’s computation can be written as:

𝑧𝑙 = 𝑊𝑙𝑎𝑙−1 + 𝑏𝑙, ∀𝑙 ∈ [1, 𝐿]
𝑎𝑙 = Φ𝑙 (𝑧𝑙 ), ∀𝑙 ∈ [1, 𝐿]

(6)

(7)

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:6

Ben Braiek and Khomh

We note that 𝑎0 which is the input layer activation is equal to the input data 𝑥. The hidden layers
share generally the same activation function. We denote them Φ1 = Φ2 = ... = Φ𝐿 = Φ. We denote
the activation function of the output layer by Ψ = Φ𝐿.
Backward Pass. First, we introduce an intermediate quantity, 𝛿, where 𝛿𝑙 is the vector of error
associated to the layer 𝑙. 𝛿𝑙 is computed as the gradient of the loss with respect to the weighted
input 𝑧𝑙 . In the following, we present the equations used by the backpropagation algorithm to
compute the error for every layer. We refer the reader to the work of Goodfellow et al. [30] for the
proof and in-depth details.

𝛿𝐿 =

𝜕𝑙𝑜𝑠𝑠
𝜕𝑧𝐿

= ∇𝑎 (𝑙𝑜𝑠𝑠) ⊙ Ψ′(𝑧𝐿);

𝛿𝑙 =

𝜕𝑙𝑜𝑠𝑠
𝜕𝑧𝑙

= 𝑊 (𝑙+1)𝑇 𝛿𝑙+1 ⊙ Φ′(𝑧𝑙 )

(8)

In Equation 8, ⊙ is the Hadamard product, which is an elementwise product of two vectors in a
way that for each component 𝑗, 𝑣 ⊙ 𝑢 means (𝑢 ⊙ 𝑡) 𝑗 = 𝑠 𝑗𝑡 𝑗 .
Second, we formulate the gradient of loss w.r.t the DNN parameters using the computed error term
𝛿.

𝜕𝑙𝑜𝑠𝑠
𝜕𝑊𝑙

= 𝛿𝑙𝑎 (𝑙−1)𝑇

;

𝜕𝑙𝑜𝑠𝑠
𝜕𝑏𝑙

= 𝛿𝑙

(9)

Last, we perform both of weights and biases iteration updates in the opposite direction of their
gradients w.r.t the loss.

𝑊 (𝑖+1) = 𝑊 (𝑖) − 𝜂𝛿𝑙𝑎 (𝑙−1)𝑇

;

𝑏 (𝑖+1) = 𝑏 (𝑖) − 𝜂𝛿𝑙

(10)

In practice, the backpropagation algorithm does not loop over the training examples and perform
the forward and backward passes on each example separately. Indeed, it relies on mini-batch
stochastic gradient descent that computes both passes on a mini-batch of examples simultaneously.
This is done by formulating the equations presented above as fully matrix-based formulas given
the input matrix 𝑋 = [𝑥1, 𝑥2, ..., 𝑥𝑚] of a mini-batch containing 𝑚 examples. Thus, the parameters
updates are based on the average of loss gradients estimated over all the examples of the batch,
which leads to more stable gradient updates.

2.1.2 Deep Convolutional Neural Networks. Convolutional Neural Network (CNN) represents a
particular type of feedforward network that is designed to process data in the form of multiple
arrays, such as 2D images and audio spectrograms, or 3D videos [30]. A CNN contains the following
specialized layers that transform the 3D input volume to a 3D output volume of neuron activations:
Convolutional Layer, Activation Layer, and Pooling Layer.
Convolutional Layer. The main building block of this type of transformation layer is the convo-
lution. A convolution provides a 2D feature map, where each unit is connected to local regions in
the input data or previous layer’s feature map through a multi-dimensional parameter called a filter
or kernel. These filters play the role of feature detectors. The feature map is produced by sliding the
filter over the input data, then computing products between the filter entries and the local input
region at each spatial position, to infer the corresponding feature map response. Different filters
are performed in a layer and resulting feature maps are stacked in 3D volumes of output neurons.
The separate filters aim to detect distinctive local motifs in the same local regions. However, all
units in one feature map share the same filter, because motifs are generally invariant to location.
Activation Layer. As in any FNN layer, we use an activation function to add non-linearity to the
computed value. The activation layer applies the activation function on the extracted feature map
as an element wise operation (i.e., per feature map output). The resulting activation map indicates
the state of each neuron, i.e., active or not.
For each hidden neuron (𝑖, 𝑗) in a feature map:

𝑎𝑖 𝑗 = Φ(𝑧𝑖 𝑗 );

(11)

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:7

Pooling Layer. The pooling layer ensures spatial pooling operation to reduce the dimensionality
of each feature map and to retain the most relevant information by creating an invariance to small
shifts and distortions. Depending on the chosen spatial pooling operations, it can be average or max
pooling that computes, respectively, the average or max of all elements in a pre-defined neighboring
spatial window size. Therefore, these neighboring pooling makes the resulting activation maps
smaller and robust to irrelevant variance, which helps shortening the training time and controlling
the overfitting.
Figure 2 shows a typical architecture of CNN with two main stages: (1) multiple stack of convolution,
activation, and pooling that ensure the detection of relevant features from the input data; (2) the
final activation map is flatten to be a vector of features and is fed to a fully-connected neural
network that performs the prediction on top of these extracted features to estimate the labels’
scores or the predict value.

Fig. 2. Schema of convolutional neural network

2.2 Regularization of Deep Neural Network Training
Given the high capacity of DNNs, developing regularization techniques that prevent the model
from overfitting the training data, has been one of the major research efforts in the deep learning
field. In general, regularization tolerates a relatively small increase of training loss in favor of
reducing the error on the test data with the hope of having a model that generalizes well on the
future coming data.

Standard Regularization Techniques. The standard regularization techniques consist in adding
2.2.1
restrictions on the values of the trained parameters, i.e., by adding a penalty term Ω(𝑊 ) in the loss
function 𝑙𝑜𝑠𝑠 (𝑓 ∗, 𝐷) (see the regularized loss from Equation 12) that can be seen as a soft constraint
on the magnitude of parameters to restrict and smooth their corresponding distributions.

˜𝑙𝑜𝑠𝑠 (𝑊 , 𝑏, 𝐷) = 𝑙𝑜𝑠𝑠 (𝑊 , 𝑏, 𝐷) + 𝜆Ω(𝑊 )
(12)
Where 𝜆 fixes the relative contribution of the norm penalty omega; so setting 𝜆 = 0 means no
regularization and larger values of 𝜆 correspond to more regularization. The most popular penalty
consists in penalizing the weights of the linear computations; keeping their values closer to the
origin by using L2-norm (Ω(𝑊 ) = 1
2) and–or enhancing the sparsity of the weights by using
L1-norm (Ω(𝑊 ) = ∥𝑊 ∥1). With the gradient-based learners, we compute the gradient of the weight
penalty term Ω(𝑤𝑖 ) w.r.t weight 𝑤𝑖 as described in the following formula.
(cid:40) 2 × w(𝑡 )
1 × sign

if Ω(w) = ∥w∥2
2
if Ω(w) = ∥w∥1

2 ∥𝑊 ∥2

𝜕Ω
𝜕w𝑖

w(𝑡 )
𝑖

(13)

,
(cid:16)

=

(cid:17)

,

𝑖

From the above formula, we can see that L2-norm penalty continuously reduces the magnitude of
the weights proportionally to it while L1-norm reduces the magnitude by a constant. Thus, L2-norm

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:8

Ben Braiek and Khomh

and L1-norm pushes, respectively, the magnitude of weights towards increasingly lower and zero
values. Moreover, the defined hyperparameter, 𝜆, controls the strength of the applied regularization.
It is therefore important to adjust it appropriately, taking into account the design of the model
and the complexity of the target problem but concerning changes w.r.t size of batches, it has been
shown that scaling the 𝜆 by 1/𝑚, where 𝑚 is the size of the batch, can make it comparable across
different size of batches [3], which avoids tuning manually 𝜆. Intuitively, the training algorithm
tries to approximate an unknown distribution by minimizing the empirical error on a sample of
data; so a large sample is likely to be more representative of this unknown distribution, and as a
result, less regularization might be needed in order to capture the maximum information about the
target data distribution.

2.2.2 Advanced Regularization Techniques. The advanced regularization techniques for DNNs
include normalization and stochasticity to the DNN inner computations [28]. In fact, manipulating
randomly the DNN architecture, over training passes, minimizes the risks that the learned parame-
ters are highly customized to the underlying training data. Furthermore, normalizing the input
of each layer, not only the input data, improve furthermore the smoothness of the loss landscape
towards more stable gradient-based optimization, and fortunately, higher chances to avoid saddle
points and ineffective local minima.
Dropout. One of the most frequently used regularization techniques is dropout [91], which ran-
domly deactivates a subset of neurons from the dropped dense or convolutional layer at every
training iteration. Indeed, the degree of randomness of dropout should be adjusted with respect
to the width of the layer using a hyperparameter, 𝑝𝑘𝑒𝑒𝑝, which represents the neurons’ retention
probability. At inference time, dropping neurons is stopped and compensated by multiplying all
the weights in the layer by 𝑝𝑘𝑒𝑒𝑝 in order to keep the distribution of the layer outputs (i.e., results
of the layer’s affine transformation) during inference time close to the distribution during training
time. Mathematically, for each hidden layer 𝑙, we define a binary mask 𝑚𝑙 , where each element can
be 1 with predefined probability 𝑝𝑘𝑒𝑒𝑝. Thus, the dropped out version of the hidden layer output
𝑑 = 𝑚𝑙 ⊙ 𝑧𝑙 . Intuitively, dropout reduces the
𝑧𝑙 masks out units using element-wise production, 𝑧𝑙
risk of overfitting by making the DNN robust against the deactivation of some neurons, which
forces the DNN to rely on population behavior instead of the activity of other feature detectors unit
(i.e., preventing the co-adaptation of feature detectors). Indeed, model ensembling, a well-known
technique in statistical learning, consists in combining the output of multiple models, each trained
differently in some respect, to generate one final answer. The resulting improvements on the per-
formance metrics explain its domination over recent machine learning competitions [30], however,
it requires a much larger training time by definition (compared to training only one model). More
fundamentally, dropout [8] simulates model ensembling without creating multiple neural networks
by combining the predictions of multiple sub-DNNs resulting from dropping, randomly, different
subset of neurons between every two consecutive training passes.
Batch-normalisation. Another interesting regularization strategy designed for DNN is Batch-
normalisation (Batchnorm). It relies on continuously normalizing intermediate activations across
batches during a mini-batch loss minimization [41]. Indeed, the proceeded normalization is based
on the standardization of each intermediate feature using the pre-computed mean and standard
(cid:205)𝑚
𝑖=1(𝑥𝑖 − 𝜇𝐵)2 given the
deviation on the current batch, respectively, 𝜇𝐵 = 1
𝐵 = 1
𝑖=1
𝑚
𝑚
batch data 𝐵 of size 𝑚. The normalized activations ˆ𝑥𝑖 =
would have zero mean and unit

(cid:205)𝑚

𝑥𝑖 and 𝜎 2
𝑥𝑖 −𝜇𝐵√
𝜎 2
𝐵 +𝜖

standard deviation. However, the resulting reduction of magnitude may induce information loss
caused by the distortions in the learned intermediary features, and consequently, the degradation of
model’s learning capacity. Therefore, batch-normalization performs a linear transformation to scale
and shift the normalized activations, 𝑎𝑖 = 𝛼 ˆ𝑥𝑖 + 𝛽 with aim of preserving the expressiveness of the

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:9

DNN through learning additionally both of the parameters 𝛼 and 𝛽. In addition, batch-normalisation
also computes two other statistics, 𝐸 [𝑥] and 𝑉 𝑎𝑟 [𝑥], which represent, respectively, the moving
average and the moving variance of the flowing data during all the training. Thus, at the test time,
we use population mean(𝐸 [𝑥]) and population variance(𝑉 𝑎𝑟 [𝑥]) to standardize the layer outputs
instead of using batch mean(𝜇𝐵) and batch variance(𝜎 2

𝐵).

Santurkar et al. [85] investigated the fundamental reasons behind the effectiveness of batch
normalization in regularizing modern DNNs. They found that normalizing continuously all the
inputs of hidden layers (i.e., all the activations) instead of normalizing only the inputs, makes the
surface of loss smoother, which ensures faster convergence and safer training using relatively
higher learning rates for which unnormalized DNNs (i.e., standard variants without batchnorm)
diverge. This positive effect on the loss landscape is called better conditioning of loss minimization
problem, which is described explicitly in the following sub-section.

2.3 Conditioning of Loss Minimization Problem
Conditioning refers to how swiftly a function changes with respect to small variations in its
entries [30]. High sensitive functions lead to poorly-conditioned scientific computations, such
as numerical optimization. Deep learning algorithms rely on numerical cost minimization. The
performance of DNN is estimated as its prediction ability with respect to unseen data (which
represents future data). However, training a DNN consists in minimizing the loss on the training
data, which means reducing the empirical error on a sample data with the aim of optimizing
indirectly the true error on the unknown target distribution. Traditionally, machine learning
algorithms design the loss function and constraints carefully; ensuring that the minimization
problem is convex, where any found local minimum is guaranteed to be a global minimum. When
training a deep neural network, the loss function is not only non-convex but also tends to have
a large number of “kinks”, flat regions, and sharp minima [60]. This makes the empirical risk
minimization difficult. The empirical risk may become non-representative of the true risk that we
aim to reduce. Nevertheless, deep learning has achieved impressive results by training DNN using
first-order gradient-based optimization. This practical trainability success is highly dependent on
the design of DNN, the choice of optimizer, parameters initialization, normalization, regularization,
and a variety of hyperparameters. However, finding adequate configurations and parameters can
be very challenging and the optimization of neural networks is still an open problem. In practice,
training a DNN for a novel problem, context or data requires a series of trial-and-error using different
configuration choices and hyperparameters tuning. These configuration choices have a strong effect
on the conditioning of the minimization problem. In fact, the Hessian matrix of the loss encapsulates
all the second-derivatives that represent the curvature of the loss surface. The condition number
which is the ratio between the largest and the smallest eigenvalue is very important because a
high condition number indicates a situation of ill-conditioning, where some parameters have huge
curvature while others have smaller one. Such a situation results in a pathological loss curvature,
and a first-order gradient descent would have difficulty progressing. However, novel DNN design
components like skip-connections of ResNet, advanced regularization and reparameterization
techniques [6, 13] have shown an ability to improve the Lipschitzness of the loss function, which
results in the loss exhibiting a significantly better beta-smoothness. These smoothing effects impact
the performance of the training algorithm in a major way because it provides more confidence that
the estimated gradient direction for each training step remains a fairly accurate estimate of the
actual gradient direction after taking that step. This enables performing update steps without high
risk of running into a sudden change of the loss landscape; including flat region (corresponding to
vanishing gradient) or sharp local minimum (causing exploding gradients). In other words, finding
good ways to configure and parametrize the DNN ensures the stability of the loss function and

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:10

Ben Braiek and Khomh

better predictiveness of its computed gradients.
Li et al. [60] have shown that sufficiently deeper neural networks encounter a high risk of sudden
transition in their loss landscapes from being nearly convex to being highly chaotic, which is
correlated to the lack of trainability and dramatic decrease of the generalization error. Therefore,
the conditioning of the minimization problem and the loss landscape geometry have a substantial
effect on the quality of both training and generalization abilities.

2.4 Property-Based Software Testing
Property-based testing (PBT) is a practical testing method [74] that provides a systematic way
for reasoning about the properties of the appropriate program’s behaviors instead of the correct
outcomes. For example, one may validate that a random data generator can produce probabilities
within [0, 1], while abstracting away from the actual probabilities. It follows the philosophy of
invariant detection, which defines a set of invariant properties that allow aligning an incorrect
execution against the expected execution [24]. Invariant detection belongs to the broader class of
pseudo-oracle techniques [10] that bypass the lack of genuine oracle and can be used to distinguish a
program’s correct behavior from an incorrect behavior. Thus, DNN training programs are considered
programs that are mainly written to determine the answer (i.e., the learned parameters). As a
result, such programs cannot have a conventional oracle [12]. Invariant detection belongs to the
broader class of pseudo-oracle techniques that bypass the lack of genuine oracle and can be used
to distinguish a program’s correct behavior from an incorrect behavior. Indeed, PBT defines the
essential properties that the under-test program must respect in any possible execution scenario,
rather than searching for the exhaustive set of all valid input-output pairs. These properties should
represent high-level specifications, describing the program’s correctness. First, PBT requires the
collection of sufficient properties about the component under test (such as its function, program,
or system). Then, the verification process starts by generating inputs for the component relying
on specific heuristics to cover the equivalent classes of data inputs. Afterward, it validates for
all the generated inputs that all preconditions, invariant properties on intermediary results, and
postconditions associated to the component under test are totally true. When a property is failed,
the counterexample is shrunk by searching the minimal combination of input elements that causes
the property to fail. In fact, large inputs may cause the failure of multiple properties; so shrinking
the input allows developers to extract the smallest part of inputs that is capable of reproducing a
particular failure, which is essential for fixing the bug.

2.5 TensorFlow Library: Concepts and Features
TensorFlow (TF) [1] is an open source DL library released by Google to help DL practitioners
construct different architectures of DNNs. TF is based on a Directed Acyclic Graph (DAG) that
contains nodes, which represent mathematical operations, and edges, which encapsulate tensors (i.e.,
multidimensional data arrays). This dataflow graph offers a high-level of abstraction to represent
all the computations and states of the DNN model, including the mathematical operations, the
parameters and their update rules, and the input preprocessing. Thus, the dataflow graph establishes
the communication between defined sub-computations; making it easy to partition independent
computations and execute them in parallel on multiple distributed devices. Given a TF dataflow
graph, the TF XLA compiler is able to generate an optimized and faster code for any targeted
hardware environment including conventional CPUs or high-performance graphics processing
units (GPUs), as well as Google’s custom designed ASICs known as Tensor Processing Units (TPUs).
Regarding the running of graph computations, TF follows the principle of deferred execution or
lazy evaluation that consists of two main phases in a TF program: (1) a construction phase that
assembles a graph, including variables and operations; (2) an execution phase that uses a session to

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:11

execute operations and evaluate results in the graph. In our work, we choose to implement our
debugging approach on top of TF because of its high popularity in the ML community [15].

3 DNN TRAINING PROGRAM: PITFALLS
Many issues can prevent the fitting process of a DNN from performing properly, i.e., finding
the best-fitted model. In this section, we elaborate on some of the common pitfalls in designing
and implementing DNNs, while pointing out and discussing concrete examples of their derived
non-crashing bugs.

3.1 DL Faults Investigation

Fig. 3. Overview of DL Pitfalls Investigation Process

Since deep learning has been increasingly leveraged in diverse real-world applications, re-
searchers have been interested in studying the software development challenges for this next
generation of software, including the faults’ taxonomy and bugs characteristics. Zhang et al. [99]
manually inspected the real-world SO and GH’s Tensorflow programs and identified some of the DL
bugs, their root causes and symptoms at high level. Then, Islam et al. [43] extended the investigation
by including DL programs written with other competitive libraries such as Pytorch and Caffe, and
studied furthermore the categories of bugs and their relationships. More recently, Humbatova et
al. [40] refined the former bug investigation [43, 99] into a taxonomy of real faults that occur in
DL software systems. The taxonomy was deduced from 375 labeled buggy DL code examples built
using three popular DL libraries: Tensorflow, Keras and Pytorch. Moreover, the construction of the
taxonomy was built in collaboration with 20 DL developers and validated by a different set of 21
DL developers who confirmed the relevance and completeness of the identified categories. Indeed,
a bunch of the reported bugs were caused by either coding mistakes, model design issues, or wrong
configurations, that share a common high-level symptom of inefficient training. The latter manifests
through convergence difficulties, preventing partially or even totally the training program from
fitting the data. Hence, these non-crashing bugs are unique to the deep learning software systems
that do not raise exceptions but adversely affect the training dynamics and results. Thus, we aim to
identify and understand the development pitfalls and root causes behind the non-crashing DL bugs.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:12

Ben Braiek and Khomh

First, we started by filtering them from the DL faults collected and reported in the former studies’
datasets. Mainly, we discarded the two categories of Tensors&Inputs [40] and GPU usage [40] that
represent, respectively, crash-inducing bugs and GPU-related bugs. We focus on detecting the non-
crashing bugs among the remaining three categories of Model [40], Training [40] and API [40] that
contain, respectively, different misconceptions of the model, multiple poor coding/configuration
bugs in the training algorithm implementation, and misuses of the DL libraries’ API. Then, we
extend the selected subset of bugs with more Q&A posts from Stackoverflow that are related to
this family of bugs. We conduct a keyword-based search on StackOverflow (SO) with queries in
the form of ’bug_type-related keywords+Tensorflow’ and we select, for each bug type/query, the
top-100 SO posts (sorted by SO internal relevance criterion). Next, we inspect manually the SO post
content including the shared code snippets and users’ comments, with the aim of identifying more
instances of the studied faults in Tensorflow. Therefore, we found 155 bug reports in relation to
occurrences of our targeted DL faults in Tensorflow DNN programs. Overall, only 8% of the bugs
in the three above-mentioned categories from [40] are included directly into our dataset, but as we
searched using their keywords on SO, 38% of them represent same bug types/root causes in our
datasets. In fact, these buggy DL programs could not be included because they are either related to
another DL framework (Pytorch, Keras, etc.) or missing necessary information for reproduction
such as training examples or hyperparameters’ values.

Fig. 4. Distribution of the collected Tensorflow Bugs over the Taxonomy of DL Faults [40]

Figure 4 shows the categories and subcategories of the bugs we collected in our datasets with
respect to the taxonomy of DL faults [40]. During the manual inspection of these bugs’ instances,
we abstract their main root causes and their typical symptoms (i.e., the negative effects observed
on the training dynamics and the produced DL model), relying on the practitioners’ shared content
on troubleshooting and debugging the DL training algorithms including blog posts [9, 20, 45,
51, 88] and popular forum discussions [18, 49]. Indeed, the decontextualization of DL bugs from
the leveraged API version, the used data and the targeted application, allows recognizing firstly
the design or implementation pitfalls that can be the origin of these training issues. Lately, the
concrete occurrences of DL pitfalls would serve us in the creation of the synthetic buggy examples
(section 5.1.4), which have been used to evaluate the effectiveness of our debugging approach on
detecting the targeted DL-specific bugs. Figure 3 illustrates the schema of the above-mentioned
steps to systematically enrich the datasets of non-crashing bug reports, as well as, identify their

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:13

main root causes and symptoms. In the following, we present a comprehensive review of the
DNN training pitfalls, organized in groups based on the main problematic component of the DNN
training program.

3.2 Input Data-related Issues
A DNN training program implements a data-sensitive algorithm whose inner logic is learned from
the training data and generalized to future unseen data. Poor training data quality often translates
into an unstable and inefficient training process. Below, we detail the training issues in relation to
the input data and DNN components making use of them.

3.2.1 Unscaled Data. The scale of DNN inputs and outputs [58] is an important factor that affects
the quality of the training. In fact, larger scale input features produce larger intermediate activations,
and consequently, larger gradients regarding the weights connected to these over-scaled input
features compared to others. Similarly, over-scaled predicted quantities would generate larger errors
and gradients. Inversely, an abuse of data re-scaling penalizes the quantities with initially a small
range of values. Both situations will induce a pathological loss curvature and an ill-conditioned
loss minimization problem. As a result, the risk of gradient unstable phenomenon [33] increases.
Modern deep neural networks deploy inner normalization techniques [6, 41] to overcome unstable
distributions of the computed activations and gradients, however, their optimization routines should
cope with high update oscillations during the early stage of training because of the unscaled data,
and as a result, the DNN is firstly trained on how to scale and shift intermediate calculations into
an appropriate range. This overhead complexity slows down the training procedure and might
prevent the convergence towards the best-fitted model.

3.2.2 Distribution-Shifting Augmentation. Given their high learning capacity, DNNs require rela-
tively large and sufficient training data to avoid simply overfitting the data. Since many application
domains lack the access to big data and because gathering data is expensive, DL developers often
resort to data augmentation techniques [78] to increase the quantity and diversity of their training
data. Examples of data augmentation techniques for images include geometric transformations,
color space augmentations, kernel filters, random erasing, and cropping. Nonetheless, the use of
inappropriate augmentation rules, as shown in the SO posts #57275278, #48845354 and #55786384,
can induce a shift in the training data distribution that prevents the DNN from learning effectively.
A DNN trained on noisy, shifted data is often hard to converge to a stable state and also incapable
of predicting correctly on unseen data (i.e., validation or testing datasets).

3.2.3 Corrupted Labels. The data used for training supervised machine learning problems are
composed of features 𝑋 (predictor inputs) and labels 𝑦 (supervised outputs to predict). The DNN’s
loss minimization problem is non-convex with several possible local minima; so standard gradient
descent often falls into those minima because of the unchanged input data 𝑋 over all the training
iterations. To overcome this problem, a mini-batch gradient descent with shuffling has been used to
train DNNs, as introduced in 2.1.1. Indeed, shuffling the data instances and performing the gradient
estimation on only a subset of them, makes the batch inputs 𝑋𝑏 change with every iteration. This
helps the optimizer to avoid sub-optimal local minima with relatively noisy and frequent updates
to the DNN’s parameters. In the implementation, we handle the features 𝑋 and labels 𝑦 in separate
data structures because the labels should be used only for estimating the loss and performance
metrics in supervised learning problems. A common bug in the data shuffler or mini-batch loader,
as reported by SO users in #47866803, #46136553 and #41864333, consists of inducing mistakenly a
mismatch between features and labels. This represents a particular case of a more general DL issue

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:14

Ben Braiek and Khomh

of corrupted labeled data, which has been extensively studied in the machine learning community
(e.g., [95]).

3.2.4 Unbalanced Data. Very often in classification problems, there is an unequal number of
instances for different labels. An unbalanced dataset biases the predictions towards the majority
class or group of labels. Various mitigation techniques have been proposed to address this issue, e.g.,
over-sampling, under-sampling, or weighted loss function [71]. DL developers should be aware of
this situation, whenever it exists, in order to use earlier a mitigation technique for class imbalance
or improve the performance measure to capture fairly mispredictions for underrepresented classes.
Otherwise, a biased model could be selected based on the overall accuracy among all classes,
resulting in erroneous or unfair behavior when dealing with instances that belong to ones that are
underrepresented.

3.3 Connectivity and Custom Operation Issues
To implement a DNN training program, DL developers use DL libraries that allow constructing the
computational graph, where nodes and edges represent, respectively, operations and data paths.
The operations represent the computational units that form the linear computations, activations,
and gradient estimations. Data paths interconnect the operations and allow data to flow from one
operation to the next, in order to successfully train and use the model. Through program code, DL
developers use library’s built-in and newly-implemented components for operations and connect
them by either feeding one’s outputs as inputs to another or by performing a math operation
joining them. Configurable routines enable rapid development and expansion of reliable DNN
programs, however they may lead to spaghetti code that becomes too large with scatter variables
and glue code (build bindings between components), increasing the risk of coding errors.

3.3.1 Network Disconnections. The most basic dependency is between the inputs and the outputs
of the DNN. The DNN should predict the outputs based on the information distilled from the inputs.
Thus, a DNN training program that does not consider the inputs when performing its internal
computations is definitely erroneous. Moreover, disconnections can occur between the intermediate
layers. Indeed, DL engineers can forget to connect some branches of the DNN or to pass the right
inputs to the layers. When such omissions occur, one or more DNN layers are accidentally removed.
A DNN with fewer layers than necessary can still be trained. A DNN can converge to an acceptable
performance with only partial layers. If this occurs, however, the program will no longer comply
with its specifications and its performance may be severely impacted. An illustrative example [82]
of a connectivity bug occurs when cloning multiple times the code block for constructing a layer,
the DL developer may forget to change the input and output for one of these constructed layers
which makes it disconnected from the neural network.

Incorrect Custom Operation. The common abstractions used by computational units to encode
3.3.2
numerical data are tensors, which are multidimensional arrays with supported algebraic operations.
These tensors make it easy to manage high dimensional parameters and perform operations on them
efficiently. However, the translation of math formulas from scientific pseudo-code to tensor-based
operations can be error-prone. As an illustration, let’s consider the cross-entropy loss which is a
matrix-matrix operation that accepts the probabilities matrix and the matrix of one-hot encoding
labels in order to estimate a particular distance. A buggy loss function may not correctly broadcast
the operation if the reduction is done over the wrong axis (e.g., sum over rows instead of columns)
and mix information between independent data instances of the batch. This introduces an incorrect
dependency to the loss function. This issue can be difficult to detect since the DNN can still
train and converge poorly and in the best case, can learn to ignore data coming from other batch

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:15

elements. Besides, DL libraries include an automatic differentiation module that generates the
analytical formula and computes the gradient automatically. However, DL developers can include
non-differentiable or problematic operations in their custom function as shown in the SO posts
#41780344 and #54346263, which negatively affect the gradients flowing over the newly-designed
DNN. In similar way, DL developers can also hand-crafted the gradient calculation for their custom
operations, but these gradients implemented from scratch should be tested carefully to avoid wrong
computations as motivated by the SO posts #46876063 and #64172765.

3.4 Parameters-related Issues
DNN parameters represent the weights and biases of a DNN’s layers. These parameters are randomly
initialized, then, they are optimized during the training process. In the following, we discuss pitfalls
in the initialization of parameters that can affect their learning dynamics.

3.4.1 Poor Weight Initialization. An improper initialization of the weights for a DNN hampers the
stability of the learning optimization problem, leading to unstable activation during the forward pass
and unstable loss gradients of the backward flow. First, the constant weights induces a symmetry
between hidden neurons of the same layer. Thus, the hidden units of the same layer share the
same input and output weights, which makes them compute the same output and receive the same
gradient. Hence, each layer’s neurons perform the same update and remain identical; i.e., wasting
capacity. Second, random sampling of initial weights breaks the symmetry between the neurons,
however, the quality of training is strongly affected by the choice of initialization [30]. Indeed, the
derivative equations 8 and 9 show that the estimated gradients include multiplication by weights,
which makes their initial magnitude scale affects their growth or decay over iterations and might
induce exploding or dead weights [25].

Ineffective Bias Initialization. A bias is like the intercept added to a linear equation. Its main
3.4.2
purpose is to allow degrees of freedom close to the origin, which improves the representation
capacity of a neural network; so it can fit better to the given data. Generally, the initial biases
are always set to zeros. Despite this, null bias for particularly-skewed data distributions (e.g.,
unbalanced datasets) slows down DNN training, which would do the bias calibration during its
first few iterations. It means that non-zero bias could contribute significantly to fit the model if it is
delicately set up to approximate the bias of data. For example, learning a classification problem with
a rare label is a kind of bias already known; so the final layer’s bias should be carefully initialized
to accelerate the learning task.

3.5 Activation-related issues
Activation represents the intermediate computation that introduces non-linearity to filter the
information computed by the previous layer. In the following, we discuss some problems related to
activations.

3.5.1 Activations out of Range. Activation functions are nonlinear functions that determine the
output of a neural network. The function is attached to each neuron and determines whether
it should be activated (“fired”) or not, based on the relevance of neuron output for the model’s
prediction. Activation functions also help normalize the output of each neuron by transforming
inputs into outputs that are within a predefined range of values. When DL developers implement
an activation function from scratch, there is a risk of bugs that leads to a wrong or unbounded
mathematical function yielding outputs within a range inconsistent with what is expected by the
developer (e.g., sigmoid’s outputs are between [0, 1] and tanh’s outputs are between [−1, 1]).

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:16

Ben Braiek and Khomh

Inadequate Hidden Layer Activation. Although the choice of hidden activation function is
3.5.2
a design engineering problem, it is not an empirical and performance-driven selection because
there are activations that are more suitable and even specialized for particular use cases rather than
others. In fact, a non-linear activation is an essential design component; so a bottom-line inadequate
choice would be keeping identity function for activation as evidenced by SO posts #53138899 and
#46181692. Nevertheless, softmax is a special non-linear activation designed specially to transform
the logits into probabilities; so mistakenly choosing it, as an activation for hidden layers, would
likely hinder the parameters learning and often discourage a smooth flowing of gradient, as shown
in the SO post #52575271. Below, we detail training issues in relation with well-known hidden
activations that could happen in certain design circumstances, where selecting an alternative
activation function should be considered.

Saturation of Bounded Function. Activation functions with a bounded sigmoidal curve, such as
sigmoid or tanh, exhibit smooth linear behavior for inputs within the active range and become
very close to either the lower or the upper asymptotes for relatively large positive and negative
inputs. The phenomenon of neuron saturation occurs when a neuron returns only values close to
the asymptotic limits of the activation functions. In this case, any adjustment of the weights will
not affect the output of the activation function. As a result, the training process may stagnate with
stable parameters, preventing the training algorithm from refining them. In fact, we can write the
equation index-free to illustrate the gradient computation flow in general:

𝜕𝑙𝑜𝑠𝑠
𝜕𝑊

= 𝑎𝑖𝑛 × 𝛿𝑜𝑢𝑡

(14)

where 𝑎𝑖𝑛 is the activation of the neuron input to the weight 𝑊 and 𝛿𝑜𝑢𝑡 is the error of the neuron
output from the weight 𝑊 .
When the activation function Φ is saturated, its outputs are in the flat region where Φ′ ≈ 0; so
𝛿𝑜𝑢𝑡 ≈ 0 and 𝑊 freezes or learns slowly.

Dead ReLU Function. ReLU stands for rectified linear unit, and is currently the most used activation
function in deep learning models, especially CNNs. In short, ReLU is linear (identity) for all positive
values, and zero for all negative values. Contrary to other bounded activation functions like sigmoid
or tanh, ReLU does not suffer from the saturation problem because the slope does not saturate when
𝑥 gets large and the problem of vanishing gradient is less observed when using ReLU as activation
function. Nevertheless, ReLU risks “dead ReLU” phenomenon [25] because it nullifies equally all the
negative values. A ReLU neuron is considered “dead” when it always outputs zero. Such neurons
do not have any contribution in identifying patterns in the data nor in class discrimination. Hence,
those neurons are useless and if there are many of them, one may end up with completely frozen
hidden layers doing nothing. In fact, given the index-free Equation 14, we can see that when the
activation is zero 𝑎𝑖𝑛 = 0, the loss gradient w.r.t weights becomes zero too ( 𝜕𝑙𝑜𝑠𝑠
𝜕𝑊 = 0); therefore 𝑊
freezes and no longer receives updates. This problem is often caused by a high learning rate or a
large negative bias. However, recent ReLU variants such as Leaky ReLU and ELU are recommended
as good alternatives when lower learning rates do not prevent this issue.

Inadequate Output Layer Activation. Concerning the output layer, the activation function
3.5.3
should map the internal calculated results into valid predictions. In case of mismatch between the
ranges of last activation layer’s outputs and ground truth labels, the model could not learn a correct
mapping function since it is not able to produce the full range of possible outcomes.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:17

Classification Outputs. The model is learning to predict probabilities, so sigmoid and softmax are
the best candidates for, respectively, binary and multinomial classification. For instance, a missing
softmax layer prior to the cross-entropy calculation can lead to performance degradation and
numerical instability issues as evidenced by the SO post #53254870. However, the use of softmax for
a classifier model with 1-dim output leads to the incapacity of outputting the full range of class labels,
as evidenced by the SO posts #59129802 and #53971451 where sigmoid should be used to output the
negative class 0, or #51993989 where tanh should be used to output both of labels: 0 and −1. Other
common pitfalls are stacking consecutive output activations, which add useless computation levels
that may erase relevant learned information, obstruct the natural gradient flow, and adversely affect
DNN performance. Indeed, the redundancy activations were often result from: (1)API misuse, where
recently-provided stable API loss functions with the logits activation, softmax or sigmoid, included
(i.e., tf.nn.softmax_cross_entropy_with_logits and tf.sigmoid_cross_entropy_with_logits), mislead
several DL practitioners that passed the result of last layer activation to these loss functions, which
resulted in a double application of softmax or sigmoid on the outputs (e.g., we refer to SO posts
#36078411, #46895949, and #42521400); (2)misconception of abstractions, the definition of a function
that abstracts the creation layers with some parameters to facilitate stacking the neural network’s
layers, however, useless non-linear activation can be applied to the last layer before probabilities
transformation by mistake, which restricts the range of outputs. For instance, a ReLU activation
before applying softmax, as happened in the SO post #44450841, would nullify negative values and
make all their corresponding labels share the same probability after applying the softmax.

Regression Outputs. The last activation should map the internal computations into a range of
values that equals (or is the closest) to the actual interval of target outputs, in order to ease the
optimization process. For instance, SO posts #60801900 and #64998875 show how the use of Relu,
having an output range of [0, +∞], prevents the estimation of negative targets and the SO user
in the post #62313327 should switch from sigmoid (i.e., outputs values within [0, 1]) to tanh (i.e.,
outputs values within [−1, 1]) to meet the real range of ground truth labels.

3.5.4 Unstable Activation Distribution. The activations encode the representation of features de-
tected at that processing layer during the training process. Thus, the fired activations indicate
that the DNN already detects low-level features, which could be relevant for following layers.
That is why the stagnation of activations caused by saturation or dead phenomenon hinders the
capacity of the DNN to learn useful patterns from the data. Similarly, over-activated layers that
are active for all inputs and unstable activation layers that have high variability in their values
can lead to numerical instability and–or divergence problems. In fact, activations represent the
input features of the next layer. The internal computation of this layer adjusts the parameters
in order to infer patterns from features (i.e., activations).The internal computation of this layer
adjusts the parameters in order to infer patterns from features (i.e., activations). By analogy to
the input normalization, the distribution of the intermediary detected features (inside the DNN)
is important to ensure an effective optimization using backpropagation of loss gradient through
layers’ parameters. More formally, the index-free partial derivative formula 14 shows well how the
magnitude of activations affects directly the magnitude of weight updates. Researchers [6] [41] have
proposed different techniques to normalize the outputs of hidden layers and obtain activations with
zero mean and unit standard deviation. Concretely, these additional internal scaling transformations
are important to control the magnitude of the gradients and improve, formally, the 𝛽-smoothness
and the Lipschitzness of the estimated loss.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:18

Ben Braiek and Khomh

3.6 Optimization-related issues
The optimization of DNN’s learnable parameters consists in minimizing, iteratively, the loss, i.e.,
empirical error of DNN’s predictions regarding supervised training data. Actually, gradient-based
algorithms such as SGD, Momentum, and Adam, are the preferred way to optimize the DNN’s
internal parameters. Next, we discuss several issues that impede the optimization process, while
describing our proposed verification routines to catch them earlier.

3.6.1 Wrong or Inappropriate Performance Measurements. The iterative optimization of parameters
often converges to an equilibrium behavior of the DNN. At this point of equilibrium, the optimal or
near-to-optimal DNN status is reached. To find this best-fitted DNN (i.e., highest accuracy or lowest
absolute error), the DNN training algorithm acts indirectly by minimizing a loss function estimated
on the training data with hope of improving the performance of the on-training DNN. Hence, the
loss is primarily designed to measure the distance between predictions and real outputs, while it
should respect fundamental properties of an objective function for first-order gradient optimization.
Empirical loss minimization for DNN training works well when the minimized loss represents the
fitness of the DNN relative to the data. Thus, a wrong loss function with regards to true model risk,
misleads the training algorithm that, despite its success to reduce the loss, could not improve the
target performance measure (e.g., classification accuracy). For instance, inadequate choice of loss
function like choosing mean squared error (MSE), which is a standard loss for regression problems,
to compute the deviation between predicted probabilities and target class in a classification problem
(e.g., SO posts #38319898 and #50641866). Another common fault in relation with the loss is the
use if ineffective loss reduction strategy like in these SO posts where there are no reduction at all
(#36127436) or a sum instead of mean reduction (#43611745 and #41954308). Indeed, the reduction
strategy allows to aggregate the losses computed for all of the data instances into a scalar loss
value. The aggregation could be the average or the sum, however, mini-batch gradient descent
variants are commonly used for minimizing the DNN’s non-convex loss function. Hence, mean
reduction is better than sum reduction, because averaging losses over the mini-batch would keep
the magnitude of loss independent of the batch size and of other hyperparameters that are also
sensitive to the magnitude of loss gradients like the learning rate. Besides, a mistaken performance
metric also regresses the expected covariance between both training quality measures (e.g., loss
and accuracy). For instance, bad choice of accuracy metric with respect to the problem would
yield illogic performance measurements and it can be the use of classification accuracy rate for a
regression problem, the use of multiclass accuracy metric for a binary classifier, or inversely, the
use of binary classification accuracy metric for a multinomial classifier, as evidenced by the SO
posts respectively, #62566558, #62354952 and #42821125.

Inadequate Learning rate. As shown in the update equations 10 for weights and biases, the
3.6.2
predefined learning rate controls the magnitude of update at each step; so setting the learning
rate too high or too low can cause drastic changes to the optimization process and cause several
erroneous behaviors. A high learning rate would push the layer’s parameters changing rapidly
in an unstable way; preventing the model from learning relevant features. The intuition is that
the parameters are a part of the estimated mapping function, so we risk overfitting the current
processed batch of data when we try to strongly adapt the parameters in order to fit this batch. An
excessively-high learning rate may lead to convergent loss minimization and numerical instability
by having NaN loss or output values. Inversely, a low learning rate can slow down the parameters
changing; making it difficult to learn useful features from data, and consequently, the minimization
process may not converge to a steady state and may even experience a non-decreasing loss value
during training. Starting by ineffective learning rates is very common when the DL developer is

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:19

dealing at first time with a learning problem as evidenced by these SO posts #42264716, #62381380,
#55718408, #34743847, #47245866, #40156629 and #59106542.

3.6.3 Unstable Gradient Problem. The loss gradient equations 8 and 9 show that the gradient of a
layer is simply the product of errors back-propagated from all its next layers (i.e., following the
forward direction). Intrinsically, the layers tend to learn at different speeds and deeper neural
networks can be subject to unstable situations if no advanced mechanism is applied to balance out
the magnitude of gradients. However, the unstable gradient problem could be more severe and
could manifest in the form of vanishing or exploding gradients, as described below, due to poor
design choices of initializations and hyperparameters.

Vanishing Gradient. In this case, the gradient tends to have smaller values when it is back-
propagated through the hidden layers of the DNN. This causes the gradient to vanish in the earlier
layers, and consequently, it would be nullified or transformed to undefined values such as Not-a-
Number (NaN) caused by underflow rounding precision during discrete executions on hardware.
The problem of vanishing gradient can lead to the stagnation of the training process and eventually
causing a numerical instability. As an illustration, we take the example of a DNN configured to have
sigmoid 𝜎 as activation function and a randomly initialized weight using a Gaussian distribution
with a zero mean and a unit standard deviation. The sigmoid function returns a maximum derivative
value of 𝜎 ′(0) = 0.25 and the absolute value of the weights product is less than 0.25 since they
belong to a limited range between [−1, 1]. Hence, it is apparent that earlier hidden layers (i.e., closer
to the input layer) would have very less gradient resulting from the product of several terms that
are less or equal to 0.25. Therefore, earlier layers receiving vanishing gradients would be stagnant
with low magnitude of weights’ changes.

Exploding Gradient. The exploding gradient phenomenon can be encountered when, inversely,
the gradient with respect to the earlier layers diverges and its values become huge. As a consequence,
this could result in the appearance of −/+∞ values. Returning to the previous DNN example, the
same DNN can suffer from exploding gradients in case the parameters are large in a way that their
products with the derivative of the sigmoid keep them on the higher side until the gradient value
explodes and eventually becomes numerically unstable.

Therefore, advanced mechanisms like batch [41], layer [6], weight [84] normalizations and tuning
of optimization hyperparameters such as learning rate or momentum coefficients, are needed to
provide adaptive gradient steps and to establish relatively similar learning speed for all the neural
network’s layers.

3.7 Regularization-related issues
The regularization strategy prevents the model from overfitting the data, while allowing the DNN to
acquire enough learning capacity to learn useful patterns and fit the data properly. In the following,
we introduce potential issues related to incorrect and ineffective regularization techniques.

Lack of-or-incorrect Regularization. Regularization techniques [30], including penalty cost on
3.7.1
the weights magnitude and specialized DL regularization like dropout, discourages the optimization
from exploring complex models and exploiting spurious correlations in favor of reducing further
the loss. Thus, lack of regularization leads to a noiseless training process with high capacity
modeling that risks capturing even residual variations in the given sample of data and tends to
overfit it quickly. Concretely, as shown in Equation 12, a zero or very low 𝜆 makes the penalty cost
useless with no effect on the objective loss function, which enables the free-growth of weights and
intensifies the threat of overfitting even coincidental noises in the sampled batches used for training
the parameters. Regarding dropout, a high retainment probability of neurons (𝑝𝑘𝑒𝑒𝑝) for wide

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:20

Ben Braiek and Khomh

dense layers or large activation maps for convolutional layers decreases the size of the dropped
subset of neurons, which eliminates the randomness effect introduced between the input features at
each inference calculation. Concerning batchnorm, a common mistake (e.g., SO posts #43234667 and
#52279892) consists in forgetting or misusing the routine that ensures the continuous estimation of
the moving average 𝐸 [𝑥] and variance 𝑉 𝑎𝑟 [𝑥] during the training. Moreover, batchnorm makes
the loss function dependent on the batch size because an instance of the batch can affect the batch
mean and variance estimated, and consequently, affect both activations and loss values for other
instances in the batch. For example, the use of unit batches, as reported in the SO post #59648509,
should not be applied because the batch variance would be zero and relatively small batches would
increase randomness and make the statistics estimation noisy. Thus, the DNN fails to perform any
normalization on the intermediary calculation results at the inference time, which means that the
regularized version of DNN becomes incorrect and its inner calculations are non-representative.
In case of mixing dropout and batchnorm, Li et al. [61] reported a disharmony issue, named
variance shift, between dropout and batchnorm when applying dropout first. Indeed, the population
statistical variance estimated by batchnorm on the entire DNN training becomes inconsistent
and non-representative because of the shift variance of weights done by the dropout when the
DNN is transferred from the training to the testing mode. In other words, dropout proceeds by
randomly removing the information coming from a subset of neurons to prevent possible neurons’
co-adaptation. Thus, we have to pass the cleaned information (i.e., after dropping out some neurons)
through batchnorm statistics estimator, otherwise, the statistics would be biased by considering
all the neurons, i.e., dropped ones included. A common symptom for all the above issues is that
validation/testing error rates are higher than training error rates. However, this symptom is quite
connected to overfitting situations; so more fine-grained symptoms are necessary to guide the
users towards the occured issue.

3.7.2 Over-Regularization. Inversely, a too strong regularization (with high 𝜆) can significantly
reduce the magnitude of weights, which may result in underfitting; leading to useless, dead weights,
as discussed in the SO posts #51028324 and #51028324. Regarding dropout, a low 𝑝𝑘𝑒𝑒𝑝 will be
considered as a strong regularization because it introduces too much randomness and may prevent
the DNN from convergence to a stable state; so 𝑝𝑘𝑒𝑒𝑝 should be tuned carefully considering the
underlying DL problem and the depth of the designed neural network. For instance, many SO posts
#60591577, #44832497, #46515248, #44695141, and #64289118 reported poor training convergence
and model performance resulting from inappropriate configuration of dropout layers. Indeed, it
has been shown that 𝑝𝑘𝑒𝑒𝑝 cannot go below the minimum of 0.5, which represents the maximum
additive regularization [91]. In practice, the fixed 𝑝𝑘𝑒𝑒𝑝 for hidden layers, and especially, layers
close to the output layer, is recommended to be within [0.5, 0.8]. However, 𝑝𝑘𝑒𝑒𝑝 for the input layer
should be kept to about 0.8 or higher (i.e., closer to 1.0). In fact, during the test time, to compensate
for disabling the dropout, the learned parameters are scaled by the 𝑝𝑘𝑒𝑒𝑝 factor. However, Gal
et al. [27] has shown that this approximation at test time becomes noisier and less accurate as
the underlying layer is far from the output; which can explain the high risk of instability induced
by low neurons’ retainment probability for the earlier layers, especially, convolutional layers in
CNN and the input layer. Therefore, the common effect of strong regularization is having low
error rates for training/validation/testing data, however, this indicates an underfitting situation in
general. In our debugging methodology, we propose verification routines to target accurately these
over-regularization issues.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:21

4 DNN TRAINING PROGRAM : PROPERTY-BASED DEBUGGING APPROACH
In the previous section, we have presented DNN training pitfalls related to misconfigurations and
coding bugs, organized by problematic components, to comprehend their symptoms and their
negative effects on the dynamics of training and the quality of optimization. In this section, we
introduce an adaptation of property-based software testing (PBT) that assembles the verification
routines that we propose for debugging DNN training programs.

4.1 Property-Based Model Testing
It has become mainstream research to focus on DL testing approaches that explicitly check for model
properties. Regarding security [7] and robustness [53], the learned function should be Lipschitz,
such that small perturbations to the input are guaranteed to spawn bounded changes to the output.
In a similar way, other important properties have been proposed to satisfy the desired requirements
in terms of privacy [7] and fairness [2]. Indeed, the validation of a trained DL model’s properties
is quite similar to traditional property-based testing (PBT) because it consists of verifying that
the implemented/trained deterministic function, mapping the inputs to the outputs, satisfies the
desired properties for all the valid inputs. However, the straightforward application of PBT to the
DNN training program, which is a stochastic data-driven optimization algorithm, appears to be
quite challenging. In fact, the DNN training consists of a data-driven, iterative process guided by
the backpropagation equations that starts from an initial DNN state and evolves the state following
a trajectory dictated by the dynamics of the update step equations until converging hopefully
to an equilibrium state at which the DNN state becomes stable. Thus, PBT can be applied on In
this section, we introduce an adaptation of property-based software testing (PBT) that assembles
several properties of training programs that should be satisfied by the initial state (i.e., pre-training
conditions), maintained for the intermediate states of the on-training DNN (i.e., proper fitting
conditions), and validated for converged DNNs (i.e., post-training conditions). Besides, an adaptation
of input shrinking, as detailed in 4.6.5, can be applied to the DNN state to determine at which level
and in which component the property is violated.

4.2 Origins and Types of DNN Training Properties
Over the last decade, we have witnessed a wide adoption of DL technology in various industrial
domains that represent the outgrowth of huge efforts and dedications from the DL community into
the knowledge transfer. Indeed, applied DL researchers and experts vulgarize DL fundamentals
and research advances in sort of principles, techniques and tricks in order to get more practi-
tioners involved into applying DL technology for solving learning task problems. This gave birth
to popular applied DL textbook [30], academic lectures [69, 70], articles [17, 36, 44, 90], and in-
dustry courses [32, 72], as well as experts’ blogs [9, 20, 45, 51, 88] and DL practitioners’ forum
discussions [18, 49] on DNN troubleshooting techniques and strategies. Figure 5 summarizes the
steps that we follow to construct automated verification routines for the identified DL training
pitfalls. From the fundamental DL resources, we distill the main properties and design principles
for each neural network component for which issues have been detected. For instance, we find the
properties in relation to the initial random parameters, the hidden and output activation functions,
or the gradient-based optimization routines. Basically, DL training programs are sensitive to the
DL bugs that violate some of the involved components’ properties. Nevertheless, the detection of
properties’ violations in a suspicious DL training program is quite challenging. That’s why, we
explore the heuristics and troubleshooting strategies elaborated by DL experts, which rely mainly
on plotting histograms of model internals and curves of performance metrics, in order to spot
unexpected distributions and irregular curve shapes. For instance, Glorot and Bengio [29] watched

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:22

Ben Braiek and Khomh

Fig. 5. Main Steps of Development Process for Verification Routines

the activation distribution to detect any possible layer saturation when they studied different
random weight initializations. Then, we analyze how these heuristics are able to characterize the
violations of properties. Generally, the heuristics specify critical values for metrics that shed light
on a faulty DL program’s state, such as high ratio of null activations or high magnitude updates
of parameters. Besides, the heuristics can also describe erroneous training behaviors that would
manifest in the dynamics of the metrics, such as unanticipated fluctuating or diverging loss. Even
if these abnormal behaviors could be captured and illustrated by experts through visualizations
(e.g., histograms of activations, curve of losses over epochs, etc.), the codification of automated ver-
ification routines, that detect the violation of statistical learning properties and not-recommended
instability for an on-execution DL training program, is challenging. Indeed, the inherent iterative
nature and stochasticity of DNN training algorithms makes the regular deterministic test assertions
impractical because a single property-violating state is not sufficient for asserting the occurrence
of an issue. For instance, the current state may trigger a dead layer (i.e., more than 50% of ReLUs in
a layer are null), but the next state following the updates can avoid the problematic situation by
reducing the inactive ReLUs. Hence, the persistence of the property-violating state, catched by the
heuristics, should be taken into consideration to avoid overwhelming warnings and misleading
false alarms during the debugging sessions. Below, we explain the developed guidelines to codify
the DL experts’ heuristics into robust and dynamic verification routines that are designed to assess
persistent behavioral issues.

4.2.1 Buffering and Statistical reductions. The parameters and internal computations in a deep
neural network are volatile multi-dimensional arrays. Thus, we define buffers to store the last
intermediary states, including the hidden activations, the predictions, the losses, etc., in order to
validate the heuristics on a set of recent states instead of a single one. Then, we calculate different
statistics on their distributions over different axis of interest to reduce the dimensions and create
the most appropriate data views/metrics to handle in the codification of the rule. For instance, the
checks on the activation distribution would run periodically on all the activations stored on the
buffer, which are obtained from the last training iterations (i.e., anticipated to be using different
parameters and batches of data). By default, we set up a buffer size that equals 10. Therefore, given
the same example of dead layers, we reduce the buffered activations, from 10 values per neuron to

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:23

a single one, using 95𝑡ℎ percentile, which is a robust maximum estimation (highest value under the
top 5%). Then, our proposed check would flag the layers with more than a half of dead neurons,
which have returned 95% of outputs below the minimum threshold of 1𝑒 − 5 for the last 10 training
iterations.

4.2.2 Pessimistic boundaries. As TheDeepChecker consists of a debugging method, we set up
pessimistic thresholds to spot critical values and erroneous behaviors that are probably caused by a
DL bug. This conservative strategy can effectively reduce many possible false alarms in relation to
ineffective training traits that usually manifest in earlier iterations or on complex learning problems.
Nevertheless, we keep the thresholds as user-configurable settings in order to adjust the sensitivity
of the verification routines on specific DL architectures according to user interests.

4.2.3 Approximative and Behavioral assertions. The heuristic-guided DL program diagnostic re-
quires the implementation of approximative assertions including almost numerical equal assertions
for floating numbers and statistical significance tests to identify if the obtained program state
is outside the anticipated set of possible states by the experts. For instance, there are no best
initial parameters, however, DL experts have shown the importance of sampling the random pa-
rameters at the first iteration from a carefully-designed distribution, depending on the neural
network characteristics. Besides, crafting verification rules for diverse abnormal DNN states such
as stagnated loss, huge weights or vanished gradients, etc., can be difficult because the involved
metrics’ thresholds would vary between models and problems. Thus, it is important to focus on
characterizing the erroneous behaviors: stagnation, diverging, or vanishing instead of the resulting
faulty state to which the buggy DNN training program would converge. Indeed, we enable the
detection of abnormal trends through the assessment of their evolution over consecutive iterations,
i.e., by considering a window of steps, generally, window size would be in-between 3 and 5. In the
following, we describe our proposed behavioral assertions for the common erroneous behaviors
resulting from the identified DL training pitfalls:

Stagnation. It is the opposite of changing and moving quantity. Hence, we define a minimum
percentage difference by which the quantity of interest should change at each step within the
window; otherwise, we flag it as stagnated. More specifically, the change direction is already
known to compute a relative percentage of increase or decrease, e.g., we expect that the loss keeps
decreasing until convergence to a minimum.

Diverging or vanishing. They represent unexpected huge increases and decreases in a quantity
over time. They can be simulated as exponential growth and decay, 𝑞𝑡 = 𝑞0 × 𝑟 𝑡 where, respectively,
𝑞𝑡
𝑟 > 1 and 𝑟 < 1. Thus, we can approximate the rate of change 𝑟𝑡 =
𝑞𝑡 −1 for a window of recent
steps, then, we consider that a quantity is diverging if the calculated 𝑟𝑡 are higher than a low bound,
or it is vanishing if all the 𝑟𝑡 are lower than a high bound. In our constructed verifications, we used
2 as low_bound, meaning that the quantity should constantly double its value at minimum to be
considered a diverging quantity and inversely, we used 1
2 as high_bound. The component is flagged
when it maintains this unanticipated non-linear evolution for a predefined window of steps.

4.2.4 Consistency assessments. Several experts’ heuristics, that are incorporated into TheDeepChecker’s
verification routines, help recognize the unstable distributions of parameters, activations and gradi-
ents, as well as unexpected optimization updates and loss curves. However, the riskiness of these
unstable learning situations increases when the identified issue persists or becomes more severe
over the iterations. Hence, we smooth the verification logic by (1) computing the rule’s metrics on
the aggregation of previously-obtained states from the buffer; (2) considering a forbearance period,

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:24

Ben Braiek and Khomh

which is a prefixed number of steps to wait, despite the persistent failure of the verification rule,
before flagging the occurrence of the issue.

Finally, we select the debugging phase during which the codified verification routine would
run depending on their input DNN states. Indeed, we mainly separate between the initial state
verification, the validation of the under-test DL program running on a batch of data, and the need for
longer training over larger datasets or comparison of multiple trained models. In the following, we
describe the chronological sequence of the debugging phases, as illustrated in Figure 6 and we detail
the heuristic and logic of all the verification routines included in each phase of TheDeepChecker’s
debugging session.

Fig. 6. Overview of TheDeepChecker Debugging Phases

4.3 Phase 1: Pre-training conditions
The first phase of our debugging process occurs before starting a training session. It enables running
static pre-checks of the input data and the starting initial state of the on-training DNN. The benefit
of these preliminary verifications is to validate, from the start with a null training cost, the quality
of feeding data (i.e., input features, labels), the correctness of essential implemented components
(i.e., gradient, custom operations), and the adequacy of the starting state (i.e., initial parameters,
first loss). In the following, we describe the pre-checks and their related training pitfalls.

4.3.1 Data Distribution. DL practitioners often perform linear re-scaling of the input and output
features, in order to adjust their distribution into a common scale without distorting differences
between the ranges of original values. In fact, the two most common data scaling techniques are: (1)
standardization consists in transforming the inputs into z-scores, which means that the transformed
data should have a zero mean and a unit standard deviation; and (2) normalization consists in
re-scaling each input feature using its maximum and minimum elements, to have values within a
predefined small range such as [0, 1] or [−1, 1].

Scaled Data Verification. We extract the data, inputs and outputs, that are fed to the training
program; i.e., the final data that have been going through the preprocessing pipeline, and then,
verify that the data is scaled properly. In our verification routine, we start by detecting the constant
features that have a zero variance. Then, we support checking whether the features are zero-
centered with unit standard deviation, or they belong to one of the two well-known normalized
range of values: [0, 1] or [−1, 1] [59]. Moreover, this helps detect if the data accidentally includes
undefined or non-finite quantities (i.e., NaNs and Infs).

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:25

Unbalanced Labels Verification. We compute the Shannon equitability index [87], which summa-
rizes the diversity of a population in which each member belongs to a unique group, to estimate the
balance between the frequencies of labels. On a data set of 𝑁 instances, if we have 𝐾 labels of size

𝑁𝑘 , we can Shannon equitability index as follows,
, and it will be zero when there
is one single label. Thus, it tends to zero when the dataset is very unbalanced. In our verification
routine, we use by default the minimum threshold of 0.5 to flag the labels data as unbalanced. This
will be reported to the user as warning and will affect the verification of the initial bias.

− (cid:205)𝑘

𝑖=1

(cid:17)

(cid:16) 𝑁𝑘
𝑛

𝑁𝑘
𝑛 log
log(𝐾)

Starting DNN state. Starting from different initial states, the optimization algorithm follows
4.3.2
different trajectories and can terminate at different equilibrium states. Thus, a poor initial state
adversely affects the optimization routines, and consequently, the optimality of the equilibrium
state.

Initial Weights Verification. First, we verify that there are substantial differences between the
parameter’s values by computing the variance of each parameter’s values and checking if it is not
equal to 0. Next, one can make sure that, given the chosen activation function, the distribution of
initial random values are sampled from a uniform or normal distribution with a careful tweak, i.e.,
by calibrating attentively the variance because the distribution of the outputs from a randomly
initialized neuron has a variance that grows with the number of inputs. The equality between the
actual variance of each weight and its recommended variance given the input size is verified using
f-test [4]. In the following, we describe the recommended variances depending on the activation
function of the corresponding layer.

- LeCun [59] proposes a heuristic that initializes each neuron’s weight as either N (0, √︁1/𝑓 𝑎𝑛𝑖𝑛),
i.e., normal distribution with zero mean and 1/𝑓 𝑎𝑛𝑖𝑛 of variance or U (−√︁3/𝑓 𝑎𝑛𝑖𝑛, +√︁3/𝑓 𝑎𝑛𝑖𝑛),
i.e., uniform distribution within [−𝑙𝑖𝑚𝑖𝑡, 𝑙𝑖𝑚𝑖𝑡] and 𝑙𝑖𝑚𝑖𝑡 = √︁3/𝑓 𝑎𝑛𝑖𝑛, where 𝑓 𝑎𝑛𝑖𝑛 is the
number of inputs. This guarantees that all the initial neurons’ weights have approximately
the same output distribution, and its empirical evaluation on sigmoid layer activation shows
a significant improvement on the rate of convergence.

- Glorot et al. [29] recommend the following initialization (especially when tanh is used as

activation function) which consists of neuron’s initialization following
either N (0, √︁2/(𝑓 𝑎𝑛𝑖𝑛 + 𝑓 𝑎𝑛𝑜𝑢𝑡 )) or U (−√︁6/(𝑓 𝑎𝑛𝑖𝑛 + 𝑓 𝑎𝑛𝑜𝑢𝑡 ), +√︁6/(𝑓 𝑎𝑛𝑖𝑛 + 𝑓 𝑎𝑛𝑜𝑢𝑡 )), where
𝑓 𝑎𝑛𝑖𝑛, 𝑓 𝑎𝑛𝑜𝑢𝑡 are the number of inputs and outputs.

- He et al. [37] also proposed an initialization specifically for ReLU neurons. They suggest that
the variance of neurons in the network should be 2/𝑓 𝑎𝑛𝑖𝑛, which gives an initialization of
either N (0, √︁2/𝑓 𝑎𝑛𝑖𝑛) or U (−√︁6/𝑓 𝑎𝑛𝑖𝑛, +√︁6/𝑓 𝑎𝑛𝑖𝑛). All of the above initializations have
been discovered empirically and proven to be optimal in classic well-known CNNs like
LeNet [57] and modern architectures such as VGG [89] and GoogleNet [92].

Initial Biases Verification. As a baseline, we verify that the bias exists, and initially is set to 0 in
case of well distributed labels. Nevertheless, we proceed by more advanced check on the last bias
initialization in case the pre-check on unbalanced labels for classification was fired. Indeed, we
make sure that the bias set for the output layer reflects the bias already found in the distribution
of outcomes in the given ground truth data. In our implemented verification routine, we consider
the unbalanced classification problem, where it is usually effective to set each bias unit 𝑏𝑖 to
log(𝑝𝑖 /1 − 𝑝𝑖 ), where 𝑝𝑖 is the proportion of training instances of the label corresponding to the
bias 𝑏𝑖 of unit 𝑖 [38]. Concerning the regression problem, if the coefficient of variation w.r.t each
output 𝑗 (i.e., the ratio of the standard deviation to the mean) is low (e.g., our default threshold is
0.1%), we verify that its corresponding 𝑏 𝑗 is set to 𝑚 𝑗 , the mean value of the supervised target 𝑗.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:26

Ben Braiek and Khomh

This eases the optimization by transforming the regression problem into predicting the deviation
against the baseline (i.e., the mean value).

Cold Start Loss Verification. An unexpected loss at the iteration 0 with an untrained model
(cold start), can indicate numerous issues including faulty loss function, ineffective loss reduction
strategy, as well as buggy parameters’ initializers [50]. Indeed, we compute the loss at cold start
with an increasing size of batches in order to verify that the obtained losses are not proportionally
increasing. This validates that the loss estimation is an average-based expectation and it is not a
sum-based reduction. In our verification routine, we duplicate a random batch of data by doubling
its size (×2), then, we check if their corresponding losses at cold start are doubling (×2) as well.
Next, we verify that the optimizer starts well at the expected initial loss (i.e., the one estimated
at the first run with random internal parameters). It is always possible to derive approximately
the correct initial loss for a given DNN program configuration. For instance, the cross-entropy
loss for a balanced classification problem should start with uniformly distributed probabilities of
𝑝 (𝑙𝑎𝑏𝑒𝑙) = 1/𝐿 and initial value of 𝑙𝑜𝑠𝑠 = − log(1/𝐿), where 𝐿 is the number of target labels.

Tensor-Based Operation Verification. A careless developer can introduce mistakes when transform-
ing math formulas or pseudo-code from white papers to tensor-based operations using basic DL
libraries calculation APIs. As the DNN tensor-based computations include intensive broadcasting
and reduction operations in order to perform individually calculation over all the instances/neurons
at each level, then, reduce the calculations to define aggregative scores towards summarize all
into a scalar cost that represent how well the DNN performs on the data. We found that common
implementation mistakes miss or add unnecessary dependencies to network components (instances,
neurons, scores,..etc). Thus, it is important to test that a written operation depends only on its
related components. For example, an activation function should be applied separately on all the
neurons, which means the activation output 𝑖 should depend only on the neuron input 𝑖, or a
distance calculation between prediction and actual values should contain independent components,
where the distance value 𝑖 depends on only the prediction 𝑖 and the ground truth label 𝑖 as well. To
validate the correctness of dependencies of all the math operations for a computed quantity within
the neural network, we use the gradients flowing in the network to debug the dependencies between
each operation’s components given the fact that the gradient of a function w.r.t an independent
component is always zero [51]. For instance, let’s consider a newly-implemented loss function,
we can extract only the loss obtained for the outputs of a data point 𝑖, and then perform a full
backward pass to the input data in order to make sure that only the gradient w.r.t the 𝑖-th input
data is not null. A violation of this condition signals that overlapping dependencies exist, which
means that the on-watch average loss for DNN performance is wrong and misleading.

Computed Gradient Verification. The backbone of backpropagation implementation lies in the
computation of gradients with respect to different DNN operations, including linear weighted
sum and non-linear activations. Whenever DL developers add hand-crafted math operations and
gradient estimators, we perform a numerical gradient checking that consists of comparing between
the analytic and numerical calculated gradients, respectively, the gradient produced by the analytic
formula and the centered finite difference approximation,
. Both gradients should be
equivalent, approximately equal, for the same data points. The following steps are recommended
by [30, 50] to improve the effectiveness of this gradient checking process (and the detection of
faulty gradients).

𝑓 (𝑥+ℎ)−𝑓 (𝑥−ℎ)
2ℎ

Relative error comparison: The difference between numerical gradient 𝑓 ′

𝑛 and analytic gradient
𝑓 ′
𝑎 represents the absolute error that should not be above a predefined threshold. However, it is
hard to fix a common threshold of absolute error for DNN because its internal computations are

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:27

usually composed of multiple functions; so the errors build up through backpropagation. Thus, it is
preferred to use a relative error,

𝑛 |) , that might be acceptable below 1𝑒−2.

|𝑓 ′
𝑎−𝑓 ′
𝑛 |
𝑎 |, |𝑓 ′
𝑚𝑎𝑥 ( |𝑓 ′

Sampled data instances: Sampling a few data instances for numerical gradient checking reduces
the risk of crossing kinks, which are non-differentiable areas of the loss landscape. For instance,
ReLU has zero gradient at the origin; but the numerical gradient can cross over the kink and produce
a value different from zero. Besides, DNN’s parameters are large with thousands of dimensions; so
the computation error could be on a random subset of each gradient dimension. Therefore, a random
sampling among both data points and dimensions makes the finite-difference approximations less
error-prone and faster in practice.

No regularization: The standard regularization can render large errors, misleading the numeri-
cal gradient checking when the penalty term added overwhelms the original loss (i.e., the gradient
is mostly related to penalty cost). Moreover, advanced regularization techniques such as dropout
induce non-determinism in the DNN internal computations, which enhances the error-proneness
of numerical gradient checks.

Prior burn-in training: A short burn-in training during which the parameters take better and
more representative values than randomly initialized ones is recommended. It is also recommended
to avoid the gradient checking at cold start since it could introduce pathological edge cases, masking
a buggy implemented gradient.

Fitness of a single batch of data. Given a tiny sample of data, the target problem becomes
4.3.3
easy to solve and the training algorithm should be able to converge to a DNN that fits the data
without any issue, as every well-designed DNN should be able to fit a small dataset. This is a main
pre-check for DNN training routines because it is a necessary condition, where its non-satisfaction
indicates a misconfiguration or a software bug.

Input Dependency Verification: We confirm that training programs on zeroed batches of data per-
form worse than those on real samples of data. This check was initially proposed by Karpathy [51],
as a verification that the model outperforms an input-independent baseline. For debugging, this
improvement over the input-independent baseline shows that the training program is successfully
leveraging the input to optimize the DNN parameters.

Overfit Verification: We verify that the optimization mechanism is working well on a controlled
sample of data with reduced size (i.e., a few data points for each class) [30, 50]. The acceptance
criteria is that the DNN achieves 100% accuracy or near-to-zero absolute errors (AEs) on continuous
outputs (by default, we consider AE in the order of 10−3). A failure to achieve this performance
would signal the presence of issues regarding the DNN optimization routines.

Regularization Verification: As above-mentioned, the controlled experiment of fitting a single
batch would lead to overfit the provided few data points in normal situations, however, the loss
should be greater than zero [30] when there is quite regularization. This check can be improved by
watching furthermore the smoothness of the loss curve to spot a lack of noise in the optimization,
and consequently, it reinforces doubts about the absence of active regulation. In our verification
routine, we set up 1𝑒 − 5 and 0.95 as default thresholds for loss and smoothness ratio 4.4.1 to
recognize suspicious loss curves that are smoothly decreasing towards a very low loss. This shows
the model’s propensity to overfit quickly the training data caused by a lack of regularization, which
often implies high risks of capturing useless residual variations in the given features.

Regarding pre-check of a single batch of data fitness, we concentrate on the functioning of the
training algorithm, and precisely, its ability to converge (i.e., reaching an optimal equilibrium state)
given a reduced size problem. However, the DNN training may still contain inefficiencies that did

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:28

Ben Braiek and Khomh

not prevent it from solving a small problem size but would affect its performance when it trains on
larger problem size. Moreover, the failure at these batch fitness prechecks do not provide indications
about the possible root causes behind this incapacity to successfully pass them. Therefore, the
next phase in our debugging process aims to guarantee the “proper functioning" of the training
algorithm given a reduced size problem. What we mean by “proper functioning" is the ability
to converge with a valid accomplished trajectory (i.e., passing through valid intermediary DNN
states).

4.4 Phase 2: Proper fitting a single batch of data
At this phase, a monitored training is launched on a representative sample of data (i.e., a single
batch of data resulting from a stratified sampling). The monitoring routines serve to detect early
and precisely the potential issues with diverse automated verifications that watch periodically for
the DNN components’ misbehaviors to spot and report properties’ violations. In case of a DNN with
major issues, i.e., already failed the precheck on single batch fitness, this phase allows to steer the
user towards problematic components and provide meaningful messages on the violated properties
that restrict further the potential root causes. Otherwise, DL practitioners can still leverage this
debugging phase for pre-examined DNNs to spot inefficiencies of their design choices that cause
training instability in regards to learning updates or activation distributions, which might prevent
the DNN from reaching optimal steady states and lead to a wastage of time and resources on
unnecessary long training sessions with the full datasets.

Fig. 7. Illustration of loss minimization issues

4.4.1 Abnormal Loss Curvature. A loss curve is a plot of model loss value over time in terms of
epochs or iterations. The shape and dynamics of a loss curve are useful to diagnose the behavior
of the optimizer against the target learning problem. Figure 7 shows the abnormal loss curves,
detailed-below, which indicate different pathologies of statistical learning from data. The anomalous
loss evolution [50] can be detected using continuously updated metrics that are cheap to compute
and which can reveal anomalies effectively.

Non- or Slow-Decreasing loss. A flat or low slope down curve shows that the loss is either
non-decreasing or decreasing very slowly which means that the model is not able to learn at
all or has a low learning capacity. This could be due to an inadequate loss function or a low
learning rate. As introduced in 4.2.3 for the stagnation test, we verify that the loss is decreasing
at an acceptable decay rate for a window of steps, e.g., we set up 5 steps by default. For the
loss decreasing verification, we proceed by watching that the percentage difference between
consecutive losses’ values, 𝑙𝑜𝑠𝑠_𝑑𝑒𝑐𝑎𝑦_𝑟𝑎𝑡𝑖𝑜 =
, is greater or equal to a

𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑙𝑜𝑠𝑠−𝑐𝑢𝑟𝑟𝑒𝑛𝑡 _𝑙𝑜𝑠𝑠
𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑙𝑜𝑠𝑠

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:29

predefined minimum percentage difference (by default, we set up 5%) by which the loss should
decrease, 𝑙𝑜𝑠𝑠_𝑑𝑒𝑐𝑎𝑦_𝑟𝑎𝑡𝑖𝑜 > 𝑝𝑟𝑒 𝑓 𝑖𝑥𝑒𝑑_𝑚𝑖𝑛_𝑙𝑜𝑠𝑠.

Diverging loss. A curve with a high slope indicates that the loss is diverging with wildly increasing
values which means that the optimization problem is turned into a loss maximization instead of
minimization. This could be the result of a high learning rate or a buggy gradient. Therefore, we
verify that the loss has no increasing tendency instead of decreasing one. This verification can be
done automatically by updating a reference minimum loss (𝑙𝑜𝑤𝑒𝑠𝑡_𝑙𝑜𝑠𝑠), and then, watching that
the absolute ratio of loss, 𝑎𝑏𝑠_𝑙𝑜𝑠𝑠_𝑟𝑎𝑡𝑖𝑜 = 𝑐𝑢𝑟𝑟𝑒𝑛𝑡 _𝑙𝑜𝑠𝑠
𝑙𝑜𝑤𝑒𝑠𝑡 _𝑙𝑜𝑠𝑠 (i.e., we call it absolute ratio because it is
computed w.r.t the lowest obtained loss) is not dramatically diverging, as described in 4.2.3, during
the training.

Highly-Fluctuating loss. A noisy curve with random fluctuations demonstrates that the loss is
not converging to a line of stability, hence, the optimization is facing difficulties preventing it from
converging normally. Potential reasons behind this issue could be: strong regularization that gives
rise to noisy loss estimation, or high learning rate that produces large updates keeping the optimizer
jumping over the local minimum without converging. Hence, we compute the smoothness ratio of
the loss curve as follows:

𝑠𝑚𝑜𝑜𝑡ℎ𝑛𝑒𝑠𝑠_𝑟𝑎𝑡𝑖𝑜 =

𝑁 _𝑠𝑎𝑚𝑝𝑙𝑒𝑠 − 𝑁 _𝑑𝑖𝑟𝑒𝑐𝑡𝑖𝑜𝑛_𝑐ℎ𝑎𝑛𝑔𝑒𝑠
𝑁 _𝑠𝑎𝑚𝑝𝑙𝑒𝑠

Where 𝑁 _𝑠𝑎𝑚𝑝𝑙𝑒𝑠, 𝑁 _𝑑𝑖𝑟𝑒𝑐𝑡𝑖𝑜𝑛_𝑐ℎ𝑎𝑛𝑔𝑒𝑠 denote, respectively, the sampled iterations count and
the number of alternations in the loss evolving direction. Then, we check periodically that the
smoothness ratio is not lower than a predefined threshold, otherwise, it indicates a high amount of
fluctuations (e.g., we fix 0.5 as a default threshold, which means that more than 50% of consecutive
sampled losses are having altered directions).

4.4.2 Performance Metrics Correlation. DL practitioners should select or implement the right loss
measure (i.e., mean squared error, cross-entropy, etc.) that will be set as objective function for
parameters optimization, while they keep watching a target performance metric (i.e., R-squared,
Accuracy, etc.). For this verification routine, we compute, continuously, the absolute value of
correlation coefficient between the optimized loss and the target performance measurements
between training steps. The latter describes the magnitude of the relationship between two variables
within the interval of [0, 1] and should not become lower than a predefined threshold (e.g., our
verification routine reports an absolute correlation coefficient of less than 0.5). This provides an
indicative metric of how representative the optimized loss is with respect to the target objective
and vice-versa. Thus, a poor choice of loss function would have high chances to be uncorrelated
with the performance metric, similarly, a buggy performance calculation would yield incorrect
values with low correlation with the loss evolution.

4.4.3 Unstable Gradient. We propose to detect unstable gradient issues by examining, continuously,
the evolution of estimated gradient’s values with respect to each DNN’s layer. More specifically,
we check the growth and decay rate of the absolute average of the layer’s gradients, estimated for
the last iterations, to detect if the latter is following an unstable evolution trend, i.e., it is exploding
or diminishing, as described in 4.2.3.

4.4.4 Magnitude of Regularization Penalty Cost. In addition, there is an issue with the addition of
a high amount of regularization to the computed loss, whether by using a high 𝑙𝑎𝑚𝑏𝑑𝑎 value or
a poorly-designed penalty regularization cost. Indeed, Park et al. [75] highlight that the learning
suddenly fails when the magnitude of gradients from 𝑙𝑜𝑠𝑠 (𝑊 , 𝑏, 𝐷) decreases faster than that
from Ω(𝑊 ); so the penalty term gradient overwhelm the loss data gradient. In such problematic

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:30

Ben Braiek and Khomh

situations, the weights’ updates become mainly related to the regularization term, which causes
the failure of the model to learn.
One should ensure that the regularization is not too strong. The regularization term gradient should
not dominate and suppress the loss gradients w.r.t the weights.To ensure this, we recommend
watching continuously, the proportion of the magnitude of the penalty terms’ gradients w.r.t the
magnitude of loss data in order to validate that it is not diverging, as described in 4.2.3.

4.4.5 Parameters States and Dynamics. The main DNN’s on-training parameters, weights and
biases, should be optimized towards better solving the learning problem over the training sessions. In
the following, we propose different verification routines on the parameters current states and update
dynamics that could indicate convergence issues and non-optimality of the on-going estimation.

On-Training Parameters Verification. A preliminary sanity check for neural network parameters
would be verifying their changing estimations over the training iterations. We make sure that all
defined trainable layers have their parameters updated. Indeed, a non-zero difference between
the values of trainable parameters before and after the execution of a few training passes (i.e.,
optimization updates) confirms that the dependencies between the layers are correctly set up and
that all the trainable parameters are getting optimized.

Dead and Over-Negative Weights Verification. A layer’s weights are considered dead or over-
negative, when respectively, the ratio of zeros or negative values in the tensor elements is very
high [25]. These two states of weights are likely to be problematic for the learning dynamics. Indeed,
given a common DNN setup (e.g., normalized inputs within [0, 1] and a variant of ReLU as hidden
layers activations), null or negative learned weights within hidden layers represent connections to
intermediate features that do not contain any relevant information for the target task. Thus, if a
layer’s weights are mostly full of null or negative weights, their corresponding activation layers
are likely to stagnate on a non-optimal flat region and consequently, the DNN would start facing
dead layers (i.e., ReLUs mostly outputting the value zero) and frozen layers (i.e., no updates of the
weights). In our verification routine, we flag any tensor of layer’s weight (𝑊 ), having either the
ratio of very low values (i.e., lower than 1𝑒 − 5) or negative values is higher than a predefined
threshold (i.e., 95% is used by default), as respectively, dead or over-negative weights.

Stable Parameters Update Verification. Deep neural networks introduce challenges regarding
learning stability compared to shallow networks. In the training pitfalls section 3, we discussed
some practices such as tuning the learning rate and adding activation or weight normalization to
balance out the learning speeds for the hidden layers. Thus, it is important to make sure that the
parameters’ updates are stable. Hinton [38] and Bottou [14] proposed the following heuristic, the
magnitude of parameter updates over batches should represent, respectively, 0.1% or 1%, of the
magnitude of the parameter itself, not 50% or 0.001%. Therefore, we propose a verification routine
to detect unstable learning parameters by comparing the magnitude of parameters’ gradients to the
magnitude of the parameters themselves. More specifically, following the recommendation to keep
the parameter update ratio around 0.01 or 0.001 (i.e., −2 or −3 on base 10 logarithm), we compute
the ratio of absolute average magnitudes of these values and verify that this ratio doesn’t diverge
significantly from the following predefined thresholds:

−4 < log10

(cid:16)

|𝑊 (𝑖+1) − 𝑊 (𝑖) |/|𝑊 (𝑖) |

(cid:17)

< −1

The proposed verification mechanism reports irrelevant layers (i.e., where updates are unstable)
and frozen layers (i.e., where updates are stalled) to the user.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:31

Parameters Diverging Verification. Worse than unstable learning, weights and biases risk diver-
gence, and may go towards +/−∞. For instance, high values of initial weights or learning rate
with a lack of–or–insufficient regularization provokes highly-increasing weights’ updates, leading
to bigger and bigger values, until reaching ∞ (this is caused by overflow rounding precision).
In addition to that, biases can also become huge in certain situations where features could not
explain enough the predicted outcome or might not be useful in differentiating between the classes.
Therefore, we automate a verification routine that watches continuously the absolute averages of
parameters are not diverging, as described in 4.2.3.

4.4.6 Activations Distribution.

Out-of-Range Activation Verification. Given a newly-implemented activation function, we include
a baseline verification routine to watch that the produced activations are within the expected
range of values. This would be useful to find computation mistakes or misconceptions causing
out-of-range outputs.

Validation of Output Activation Domain. The last layer’s activation represents the outputs of the
neural network, which should produce valid outcomes while covering the whole distribution of the
possible ground truth labels. We implement a verification routine to check that the outputs of classi-
fiers are probabilities, i.e., positive values within [0, 1] and sum-to-one in case of multidimensional
output. For regression, we empirically verify that the predicted outputs over the iterations were
able to satisfy necessary conditions derived from ground truth boundaries, i.e., non-zero variance,
can be negative, can exceed 1.0. Under these conditions, the common identified faults of using the
wrong activation function can be detected.

Saturated Bounded Activation Detection. To detect saturation issues in DNNs, we compute single-
valued saturation measure 𝜌𝐵 proposed by Rakitianskaia and Engelbrecht [80] if the hidden activa-
tions are bounded functions such as sigmoid or tanh. This measure is computed using the outputs
of an activation function and is applicable to all bounded activation functions. It is independent
of the activation function output range and allows a direct statistical measuring of the degree
of saturation between NNs. 𝜌𝐵 is bounded and easy to interpret: it tends to 1 as the degree of
saturation increases and tends to zero otherwise. It contains a single tunable parameter, the number
of bins 𝐵 that converges for 𝐵 ≥ 10, i.e., it means splitting the interval of activation outputs into
𝐵 equal sub-intervals. Thus, 𝐵 = 10 can be used without any further tuning. Given a bounded
activation function 𝑔, 𝜌𝐵 is computed as the weighted mean presented in Equation 15.

𝜌𝐵 =

(cid:205)𝐵

𝑏=1 | ¯𝑔′
(cid:205)𝐵

𝑏 |𝑁𝐵
𝑁𝐵

𝑏=1

(15)

Where, 𝐵 is the total number of bins, ¯𝑔′
𝑏 is the scaled average of output values in the bin 𝑏 within
the range [−1, 1], 𝑁𝑏 is the number of outputs in bin 𝑏. Indeed, this weighted mean formula turns
into a simple arithmetic mean when all weights are equal. Thus, if ¯𝑔′
𝑏 is uniformly distributed in
[−1, 1], the value of 𝜌𝐵 will be close to 0.5, since absolute activation values are considered, thus all
¯𝑔′
𝑏, the value of 𝜌𝐵 will be
𝑏 values are squashed to the [0, 1] interval. For a normal distribution of ¯𝑔′
smaller than 0.5. The higher the asymptotic frequencies of ¯𝑔′
This verification routine can be automated by storing for each neuron its last 𝑂 outputs’ values
in a buffer of a limited size. Then, it proceeds by computing its 𝜌𝐵 metric based on those recent
outputs. If the neuron corresponding value tends to be 1 (e.g., a threshold of 0.95 is used in practice
to spot this tendency), the neuron can be considered as saturated. After checking all neurons for
saturation, we compute the ratio of saturated neurons per layer to alert the DL developers about
layers with saturation ratios that surpass a predefined threshold (e.g., we fix 50% by default).

𝑏, the closer 𝜌𝐵 will be to 1.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:32

Ben Braiek and Khomh

Dead ReLU Activation Detection. By definition, a given neuron is considered to be dead if it
always returns zero [65]. Hence, we detect practically dead ReLUs by reducing the last outputs for
each neuron stored in the limited size buffer into a single 95𝑡ℎ percentile, which is more robust
than the maximum reduction against outliers (e.g., a non-zero stored values from earlier training
iterations). Thus, we mark all the neurons with a 95𝑡ℎ percentile less than a predefined threshold
(i.e., by default, we set up 1𝑒 − 5). Next, we proceed by calculating the ratio of the marked dead
neurons per layer and we flag the layers with a number of dead neurons higher than a predefined
threshold (e.g., we fix a default threshold of 50%).

Unstable Activation Detection. Although this unstable activation issue is more generic than dead
or saturation phenomena, DL experts usually watch the histograms of sampled activations from
each layer while expecting to have normally-distributed values with unit standard deviation, e.g., a
value within [0.5, 2] has been shown to be an acceptable variance of activations [47]. Thus, we base
on this expert’s heuristic to statistically validate that the sampled activations of each layer over
the last iterations is having a well-calibrated variance scale. Concretely, the test would pass the
actual standard deviation (𝜎𝑎𝑐𝑡 ) belongs to the range of [0.5, 2]; otherwise, we perform an f-test [4]
to compare 𝜎𝑎𝑐𝑡 with either the low-bound 0.5 if 𝜎𝑎𝑐𝑡 < 0.5 or 2.0 if 𝜎𝑎𝑐𝑡 > 2.0.

DL practitioners can perform a closed feedback loop using this inexpensive and rapid debugging
on a single batch of data until fixing all the covered coding bugs and improving the settings in a way
that enhances the chances of converging to a more optimal model. Thus, a successful pass at this
phase increases the confidence that the training program is devoid of common DNN pathologies
such as: vanishing gradients, saturation of activation functions, and inappropriate learning speed.
Nevertheless, other components of the training program have not been tested yet. For instance,
a training program may pass all the single batch debugging checks, while the data loader can
inject too much noise or mismatch features and labels, which yield corrupted batches of data, and
consequently, the resulting DNN does not solve the target problem.

4.5 Phase 3: Post-fitting conditions
Once the fitting of a single batch step is successfully passed, several post-fitting conditions should
be satisfied to guarantee the correctness of the data loader, the data augmentation module, and the
advanced regularization techniques that require additional computations during the inference. The
following debugging phase validates the behavior of the DNN training program during regular
training sessions, i.e., we use the available training and validation data.

4.5.1 Distribution-Shifting Augmentation Verification. We propose a post-check that verifies the
validity of the augmentation methods, applying data transformations on the generated batches to
enhance diversity and improve the generalizability by smoothing the loss landscape and forcing the
model to capture the invariants useful for the target task. A valid augmenter should not introduce
a shift in the data distribution that makes the model perform worse on the original dataset. For
instance, overwhelming noise injection leads to produce meaningless inputs; so both of ratio and
scale of the injected noise should be carefully picked to hold the augmented data distribution close
to the original one.
To detect this poor design of data transformations, we debug the data augmenter module by
comparing the performance and the activation patterns of the DNN trained with-and-without
augmentation on a sample of data from the validation set. Concerning the measurement of dissimi-
larity between the activation patterns, we use Centered Kernel Alignment (CKA) [54], which is
an optimized and powerful representational similarity measure, allowing the assessment of the
differences and the correspondences between patterns learned by different DNNs or same DNN
with different data. Indeed, given two matrices 𝑋 ,𝑌 , where 𝑋 ∈ R𝑚×𝑛1 is a matrix of activations of

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:33

𝑛1 neurons for 𝑚 examples and 𝑌 ∈ R𝑚×𝑛2 is a matrix of activations of 𝑛2 neurons for the same 𝑚
examples, a DNN representational similarity index 𝑠 (𝑋, 𝑌 ) estimates the similarity between the
representations learned in both matrices of activations. Thus, the CKA’s empirical validation [54]
shows its ability to compare representations within and across DNNs, in order to assess the effect
of changing variation factors on the DNN training. This similarity metric is robust and it does not
affect by the stochasticity of the optimizer or the random initializations, which makes it suitable
for our verification on the resulting activation patterns instability when the augmented data is
noisy and shifted w.r.t the training data distribution.
Therefore, the DNN training program would fail the test if there is a degradation of the perfor-
mance and a substantial difference in the activation patterns between the two trained DNNs. In our
implementation, we check for an increase in the loss (𝑙𝑜𝑠𝑠𝑎𝑢𝑔𝑚_𝑑𝑎𝑡𝑎 > 𝑙𝑜𝑠𝑠𝑜𝑟𝑖𝑔𝑑 𝑎𝑡𝑎) and a decrease
of activation pattern similarity (e.g., we set up a threshold of a minimum CKA index equals to 0.8,
which means a decrease of 20%). Indeed, any difference in the activation pattern could be acceptable
and might be considered as an improvement in the detected patterns in case the performance
is enhanced, however, having both of the activations pattern divergence and the performance
degradation indicate strongly that the augmented data induce a distribution shift.
4.5.2 Corrupted Labels. Given a corrupted data shuffler, the paired data (features 𝑋 , labels 𝑦)
are mismatched, where the row label 𝑦𝑖 does not correspond to the row features 𝑋𝑖 . Since the
shuffling is executed after each epoch (i.e., full pass over the data), a corrupted data loader will
generate a new distribution of supervised training data every time it is called. Thus, the training
loss curve would be subject to intermittent spikes because the neural network starts learning
on a new distribution at the 1th-iteration of every epoch. Based on this observation, we propose
to collect all the 1th-iteration losses into a set and perform a statistical test to detect if there is
significative difference between them, which means the loss estimated on the first sampled batch
of data is improved over the epochs; otherwise, we flag the data loader as corrupted because the
DNN successfully passes all the previous verifications, but cannot improve its performance over
the epochs; so it is high probable that the data loader is falsifying the batches of data in-between
the elements’ shuffles.

4.5.3 Unstable Mode Transfer: From Train to Inference Mode. Advanced regularization techniques
like dropout or batchnorm introduce, respectively, noise-injection and normalization mechanisms
in order to grant the DNN training access to sub-model ensembling and well-conditioned loss
minimization. They incorporate two functional modes: the training mode and the inference mode;
so many bugs can remain silent and hidden during the training mode, but the transfer to the
inference mode can reveal them through DNN misbehaviors and divergences induced by the mode
transfer [9, 88]. Thus, we construct a verification routine that detects the behavioral shift occurring
when transferring the DNN from the training mode to the inference mode. Our implemented
behavioral difference assessment is based on the CKA metric for representational dissimilarity
between different modes’ activations (e.g., we fix a default threshold of 0.75 as minimum similarity
of activation patterns on the same data), and the relative change between the modes’ losses (e.g.,
the threshold is by default set to 50%). Then, one can check the different regularizers’ internals,
including 𝑝𝑘𝑒𝑒𝑝, 𝐸 [𝑥] and 𝑉 𝑎𝑟 [𝑥] to diagnose the root cause.

4.6 TensorFlow-based Implementation
To assess the effectiveness of our proposed debugging approach on real faults in DL-based software
systems, we implemented a TF-based library that performs the debugging phases on a training
program written using TF features. Indeed, we choose to focus on TF-based training programs
because of the popularity of TF in the ML community [15]. Nevertheless, the property-based

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:34

Ben Braiek and Khomh

approach proposed in this paper can be adapted for other DL frameworks. In the following, we
describe the components of this TF library.

Setting Up the Testing Session. The testing of a DNN training program can be more com-
4.6.1
plicated than for a traditional program, because of its non-deterministic aspect. It is difficult to
investigate the training issues and identify the root causes when the program exhibits a substan-
tially new behavior for each execution. To reduce the stochasticity of a DNN training program,
we fix all the random seeds of all the computational libraries beforehand, which guarantees the
reproduction of the same random variables. Furthermore, we offer the option of deactivating the
parallelism, if a tester wants to obtain a perfectly reproducible result. As default settings, we allow
leveraging multi-cores CPUs and GPUs through multithreading execution. We allow this because a
single-thread execution slows down dramatically the training time. Also, TheDeepChecker targets
major training issues related to erroneous training behaviors caused by the introduction of coding
bugs or misconfigurations, which differ from minor training issues caused by non-optimal choices
of hyperparameters, leading to near-to-optimal DNNs instead of the best-fitted one. Therefore,
the resulting pathologies in the training dynamics would likely be persistent to the possible low
magnitude differences between multiple executions of parallel floating-point computations.

Fetching and Monitoring the Training Program Internals. TFDBG [16] is the official debugging
4.6.2
tool specialized for TF programs that offers features such as inspection of the computational graph,
addition of conditional breakpoints and real-time view on internal tensor values of running TF
computational graphs. However, it is not practical for our approach since it adds a huge overhead
on computation time, as it handles each execution step of the graph, to allow debugging the issues
and pinpoint the exact graph nodes where a problem first surfaced. In fact, the implemented
verifications fetch the values of tensors, representing parameters, activations, and gradients, by
requesting through the provided API the on-running DAG that encodes both the DNN design and
the used training method. The routines that continuously monitor the state of the neural network
do not need to break neither the feed-forward nor the backward passes since they can access the
internals of the intermediate neural networks to detect pathologies in the learning dynamics. This
allows the monitored training iteration to be executed in an atomic way and avoid the overhead
of using TFDBG. To manage our set of verification routines running simultaneously, we use the
monitored session and hooks mechanism; to handle the additional processing injected between
training iterations. To do that, we need to perform the following two steps:

(1) Create one or more Hooks objects that implement methods such before_run and after_run to
access the intermediate tensors’ values of activations, parameters, and gradients, then, apply
the verification logic.

(2) Create a Monitored Session that handles the execution of hooks’ additional treatments before

and after running each training pass.

4.6.3 Buffering the DNN’s status data. The activations and gradients represent intermediary compu-
tations over, respectively, forward and backward passes. The weights and biases represent learnable
parameters that are updated, continuously. As a result, the internal tensors do not survive between
two consecutive training iterations. Thus, we implement buffer data storage to save incrementally
the values of watched tensors. By default, we set the size of the buffer to 10 elements but we keep
it a configurable option in the debugging tool.

4.6.4 Running the checks. Conceptually, each check performs the following two steps. First, the
instantiation of the property requires the computation of necessary metrics in relation to the
targeted violations. Regarding the on-training verification routines, a data preparation using a

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:35

reduction strategy (i.e., average, norm, and quartiles, etc.) is necessary to aggregate the accumulated
tensors in the buffer data storage. Second, the issue detection consists of applying a heuristic-
based verification rule, involving the inferred metrics and predefined thresholds, that captures
the negative effects and anomalous training behaviors induced by the targeted issue. Indeed, the
choice of thresholds would affect the sensitivity and specificity of the issue detection. Additionally,
the training pathologies are correlated to each other and a particular bug can be the root cause of
multiple of them. Thus, our dynamic debugging strategy alleviates these challenges by leveraging
limited-size buffers, continuous verification routines, and informative raised errors. Therefore,
TheDeepChecker implements a debugging feedback loop that does not stop after finding violations,
but it keeps watching the training execution while dynamically producing error messages that steer
the DL developer to further narrow down the possible root causes and avoid errors conflicts relying
on the persistence of the errors, the chronological order of the raised issues, and the reported
information.

Shrinking the suspicious state and raising errors. Once an issue is detected, TheDeepChecker
4.6.5
shrinks the state of the on-training DNN to communicate the component where the property
violation is found and its corresponding indicators including the reference of the layer, the computed
metric, and the predefined threshold. In fact, the reported information should provide an explanation
of why the underlying property is considered to be in violation. First, reporting the shrinked neural
networks’ states helps the user avoid false positives. For instance, the learning speed can start
by relatively high updates that can be close or-even slightly larger than the prefixed update
magnitude threshold. Thus, a single warning message including the current update magnitude
and the surpassed threshold could help the user decide whether or not the unstable learning issue
occurs in the current situation. Second, the shrinking of the buggy DNN state is useful to pinpoint
the suspected computational units that developers should investigate, and therefore, it helps them
identify the occurred issue’s root cause. For example, the computational layers send information
throughout the DNN by using edges that connect layers to one another during the forward pass and
they receive updates from the gradients of the loss flowing reversely over the same edges during
the backward. Thus, the level of the dead layer and its amount of dead neurons, as well as, the
level of vanishing gradient and its magnitude scale can be used to identify the actual unstable layer,
where the information is no longer flowing during either forward or backward pass. Developers
would first investigate the underlying layer and then fix the bug within the program using this
information.

Figure 8 summarizes the above-mentioned implemented steps of our property-based debugging

approach for TF Programs.

5 EVALUATION
The objective of this evaluation is to assess the effectiveness of our proposed property-based
debugging method in allowing for the early detection of real bugs that occur in DNN-based
software systems. We also conducted a usability study with two professional DL engineers to assess
the relevance of TheDeepChecker’s error messages at guiding developers in identifying the root
cause of bugs and fixing them.

5.1 Design of Case Studies
In this section, we describe the design of our case study that aimed to assess the performance of
TheDeepChecker. We explain how we selected and reproduced relevant bugs for our evaluation.

5.1.1 Real Faults in DNN-based Training Programs. The reproduction of buggy DNN training
programs is quite difficult because of the rapid evolution of TF API functions and even infeasible

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:36

Ben Braiek and Khomh

Fig. 8. Illustration of our Property-based Debugging Approach for TF Programs

when major code blocks, datasets, or environment settings are missing. In previous research
works, program mutation [46] was leveraged to evaluate the quality of DL program debugging
approaches. The mutation relies on well-defined rules to change slightly the syntactic structure of
the code, or mimic systematically application-specific errors. Then, the debugger should detect
and reject mutants, which is called killing the mutant in such analysis. Thus, its effectiveness is
measured by the ratio of killed mutants w.r.t the total of generated mutants. Xie et al. [96] leverage
MuJava [68], which is an automatic java code mutator, to produce defective mutants of Weka’s
implementations [35] of 𝑘-Nearest Neighbors (kNN) and Naive Bayesian (NB). Dwarakanath et
al. [22] use MutPy [26], an open-source tool for python code mutation, to inject typical programming
errors in a clean implementation of deep residual neural networks (ResNET). Both mutation analyses
rely on language-specific code mutators that alter randomly the arithmetic operators, logical
operators, variable’s scope and casting types, etc. However, even if these random code alterations
mimic a large variety of coding mistakes, they cannot be representative for the real-world buggy
training programs, where the code is relatively short with heavy dependency on tensor-based
computational libraries. Besides, Ma et al. [67] proposed DeepMutation that defines DL-specific
mutation rules for DL programs, including a layer addition, a layer removal, and an activation
function removal. Based on our investigation on DL faults, we found that these operators can
actually mimic real faults in relation to missing DNN components, such as missing batchnorms
(i.e., removing the normalization layer) or redundant softmax (i.e., adding another softmax on the
output layer). However, many of the generated mutants using these operators can be equivalent to
the original network, or even better for solving the underlying learning problem. This makes the
evaluation based on the ratio of killed mutants misleading. Therefore, it is necessary to assess the
effectiveness of our debugging approach on detecting the real DL bugs that have been experienced
and reported by the DL practitioners. As shown in Figure 3, we have collected concrete instances
of real DL bugs [40] that cause training issues without crashing the DL program. Thus, we rely on

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:37

their identified root causes and symptoms to inject each DL fault into clean DL programs to force
the creation of valuable synthetic buggy versions. In the following, we detail the two high-level
categories of common root causes for the non-crashing DL bugs.

5.1.2 Coding Bugs in DNN-based Training Programs. Like any software system, DNN training
programs may contain the missing and wrong code statements that cause a deviation between the
designed DNN and its corresponding written code (see Table 1). In fact, the DNN training program
is implemented using conventional programming languages, which may include coding faults. The
lack of oracle for internal variables and the stochastic nature of the DNN optimization process,
make most of these coding mistakes hidden without disrupting the flow of the program’s execution.
For instance, the majority of these hidden bugs are incorrect math operations such as flipped sign
result, inverted order of calculations, wrong data transformations. We found another type of coding
bugs in TF programs that consist of a TF API misuse committed by developers who misunderstood
the implicit assumptions regarding how to use these configurable routines. In fact, modern DL
libraries provide rich APIs covering more and more state-of-the-art techniques. Consequently,
the API routines integrate more and more configurable options and set up default values to make
them ready-to-use for quick prototyping. However, they assume that their users are capable of
configuring them properly, which is unfortunately not always the case. Some of the built-in data
loaders, for example, automatically perform standard pre-processing of numerical data, such as
normalization. This misleads some rookies to blindly perform a double linear scaling afterward.
Another common API misuse is related to the recent versions of loss functions. Indeed, numerically
stable implementations regarding some of the loss functions require merging the loss and output
activation formula together to re-write them carefully without any potential log(0) or exp(∞).
However, users may ignore this gap between theoretical loss function and built-in numerical stable
ones; which may result in redundant activations.

5.1.3 Misconfigurations of DNN-based Training Programs. Modern DNN training programs are
highly-configurable software built using routines from DL libraries. Their correct settings, given
the context, becomes a challenging task and if an incident occurs due to misconfiguration, the
on-training DNN may produce misleading performance faults. These misconfigurations assemble
all the wrong and poor choices for the configuration of DNN-based software systems, including
the DNN design and the training method (see Table 1). The lack of understanding of DL funda-
mentals is the main reason behind the occurrence of these configuration issues, especially, when
dealing with a novel DL technique or facing an unfamiliar target problem. For instance, numerous
misconfigurations in relation with random initializers, loss functions, normalization methods and
optimization hyperparameters, lead to training pathologies. Others are related to the DNN design
and structure that lead to performance degradation, whether the DNN is underfitting the data (i.e.,
low training and validation errors) or overfitting it (i.e., low training error and high validation
error). The misconfiguration of a DNN training program impacts the effectiveness of the training
process, and consequently, the quality of the trained DNN. However, they manifest themselves
during the model learning process; so the debugging of the training program should help catch
these undercover failures that are hard to detect at inference executions during the model testing.

Synthetic Tensorflow Buggy Programs. Liu et al. [63] designed a base CNN that represents
5.1.4
a typical CNN for image classification. Then, they derived diverse ineffective variants by poorly
re-designing some parts of the CNN to evaluate the capacity of their visual diagnosis tool, CNNVis,
in detecting the effects of the added poor design choices. Indeed, TheDeepChecker gauges different
statistics and metrics on the DNN internals to detect fine-grained symptoms on the dysfunctioning
or instability of DNN’s components. The heuristic-based verification rules identify the occurred

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:38

Ben Braiek and Khomh

Table 1. Real Bugs in DNN-based Software System

Category

Bug

Coding
Bug

System
Misconfi-
guration

missing preprocessing

wrong preprocessing
wrong optimisation function
missing softmax
redundant softmax

wrong type of activation

wrong softmax implementation

wrong loss function

wrong data batching
wrong data augmentation

wrong initialization

wrong loss selection

suboptimal learning rate

epsilon for optimiser too low

missing normalization layer

missing regularisation

unbalanced dataset

Common Root Cause(s)
missing input normalization
missing output normalization
redundant data normalization
gradients with flipped sign
missing softmax activation
softmax out-and in-the loss
softmax for hidden activations
softmax for 1-dim output
over-restricted output domain
softmax over wrong axis
CE over wrong axis
inverted CE’s mean and sum
MSE with wrong broadcasting
shuffle only the features
invalid data transformation
constant weights
dummy random weights
use of MSE instead of CE
a low learning rate
a high learning rate
an Adam epsilon 𝜖 < 10−8
missing batch-norms
no-update of batch-norm globals
low 𝜆 for norm penalties
high 𝜆 for norm penalties
high 𝑘𝑒𝑒𝑝_𝑝 for dropouts
low 𝑘𝑒𝑒𝑝_𝑝 for dropouts
Labels are not equally distributed

bug based on its effects on the training routines and flag the defective component relying on its
current metrics’ status. Therefore, our first evaluation of TheDeepChecker consists of an assessment
on synthetic buggy DNN programs. Figure 9 shows our systematic approach, following the same
methodology of Liu et al. [63], to create synthetic mutant DL programs, containing the above-
mentioned DL faults. First, we select the Base DNN training programs for which the identified
DL fault is applicable. Indeed, we prepare a mixed set of Base NNs in order to cover different
architecture and activation functions that are related to a particular learning problem (i.e., either
classification or regression), as well as, advanced techniques to regularize and stabilize the training
of complex DL models (i.e., increasing the depth of the neural network enhances its complexity).
However, adding arbitrarily hidden layers to have higher learning capacity would be unnecessary
and may induce issues if it is applied on simple learning problems. Thus, we set up two base
CNNs, ShallowCNN and DeepCNN, that have been proposed to solve two well-known classification
problems with increasing complexity. Next, the injection of the DL bug in the Base program consists
in mutating minimally the original source code based on its main root cause and the toolkit official
documentation (i.e., Tensorflow). Then, we validate the presence of the DL bug-related symptoms;

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:39

otherwise, further refinements should be performed. Generally, the symptoms observed for non-
crashing bugs are any unexpected low model performance (i.e., accuracy or average error) or slow
learning process. The bug reporters were able to perceive the symptoms based on their past DNNs
training experiences or their comparisons with the original source (e.g., a research paper, or a
tutorial). In a similar fashion, the reference performances of model and learning speed for our Base
DL programs is already known and can be leveraged to check the success of the bug injection.
Following the steps described in Figure 9, we are able to create a buggy synthetic DNN program
for each matched pair of a base neural network and a single DL fault. This helps isolate the DL
faults and assess the sensitivity and specificity of TheDeepChecker regarding each injected fault
separately. At this level of controlled experiments, we could separate the valid fired checks that
highlight the foreknown negative effects of the injected fault, and the false positives that point out
to other irrelevant side effects (which could mislead the users during the debugging).

Fig. 9. Overview of Synthetic DL Programs’ Creation Steps

In the following, we describe the three Base NNs that we identified based on empirically-evaluated

architecture, officially-debugged TF implementations, and publicly-available datasets.

Fig. 10. Schema of the two BaseCNNs Architecture

RegrFNN. The RegrFNN represents a basic feedforward neural network inspired from Google
official examples [31], which contains two hidden fully-connected layers of 64-neurons with ReLU
activation. It uses mean squared error (MSE) to compute the loss for regression problems. For
regularization, we add both ridge regression (L2-norm of weights). Regarding the optimization,
we use ADAM as a variant of gradient descent algorithm. It was trained on the classic Auto MPG
Dataset [21] with the aim of predicting the fuel efficiency of late-1970s and early 1980s automobiles.
The RegrNN reached less than 1.85 of mean absolute error on unscaled outputs within the range of
[10, 47].

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:40

Ben Braiek and Khomh

ShallowCNN. The ShallowCNN is a LeNet [57] variant, containing two convolutional layers
and two fully connected layers (more details in Figure 10). A max-pooling layer is set next to
each convolutional layer. In addition, we select the widely used activation function, rectified linear
unit (ReLU). As we target to solve a multi-class classification problem, we employed softmax and
cross-entropy to be respectively, the last output activation and the loss function. For regularization,
we add both ridge regression (L2-norm of weights) and lasso regression (L1-norm of weights) as
penalty terms to the loss function. Regarding the optimization, we use stochastic gradient descent
(SGD), which remains the standard optimizer for training neural networks. It was trained on MNIST
handwritten digits benchmark dataset [56], which includes 60, 000 labeled gray-scale images of
size 28 × 28 in 10 labels. The dataset was split into a training set containing 50, 000 images and a
test set containing 10, 000 images. The ShallowCNN achieved 99.35% accuracy rate on the test set.
DeepCNN. The DeepCNN is a VGG [89] variant, containing stacking three blocks of two
convolutional layers followed by a max pooling layer and two fully-connected layers (more details
in Figure 10). We also use ReLUs, softmax, and cross-entropy for activation, output and loss functions.
However, we employ advanced regularization techniques to enable the training of this deep NN.
We put a batchnorm layer next to each layer that stabilizes and accelerates the optimization process.
In addition, each block or dense layer ends with a dropout layer using an increasing dropout rate
(going from 20% to 50%) in order to offset the learning acceleration. Our optimization method is
based on Adam, an advanced variant of gradient-based that can automatically adapt its learning
rate within each optimized parameter. The DeepCNN was trained on CIFAR10 [55], which consists
of 60, 000 labeled color images of size 32 × 32 in 10 different classes (e.g., airplane, bird, and truck).
The dataset was split into a training set containing 50, 000 images and a test set containing 10, 000
images. The DeepCNN achieved 89.15% accuracy rate on the test set.

5.1.5 Real TensorFlow Buggy Programs. Zhang et al. [99] reproduce several defective TF programs
extracted from SO posts and GH projects. Among the categories of studied DL bugs, we found
the Incorrect Model Parameter or Structure (IPS), which includes inappropriate modeling choices
and algorithm configurations, degrading the quality of the training. Indeed, the major symptom
of IPS bugs is low effectiveness, i.e., the performance of the on-training model does not improve
as expected. After filtering out redundant TF programs and other incomplete models from their
SO dataset, we ended up with 10 unique and full IPS buggy TF programs including data, model
and training algorithm implementation. Table 2 presents the selected programs alongside the
recommended fixes extracted from their related SO post discussions. Moreover, we clone 10 buggy
versions of GH projects that correspond to bug-fixing commits in relation with IPS bugs. Table 3
shows the versions of GH projects with the implemented fixes that have been identified from
the bug-fixing commit. Indeed, we emphasize that the fixes with the buggy examples are added
to mention the fault identified by the SO users or the GH project contributors, but there is no
guarantee that it is the only bug or inefficiency in the project at that version. Technically, we follow
the ‘how to’, libray’s version, and official datasets, as described in the replication package of the
empirical study [99] on Tensorflow Bugs, to prepare our set of buggy DL training programs.

5.1.6 Rule-based Debugger Baseline. As a baseline for automated training issue detection, we use
SageMaker Debugger(SMD) [81], which is a fully managed debugging and monitoring service
within the Amazon SageMaker platform for scalable machine learning in the cloud. SMD represents
a framework-agnostic system to collect, query, and analyze data from ML model training and to
automatically capture issues using a set of built-in rules. Indeed, SageMaker automatically creates
the training instance, pulls the training image from the Container Registry, and downloads data and
training scripts into the container. Once the training is launched, SMD retrieves asynchronously
the model data at specific intervals and uploads them to S3 bucket. Then, SMD runs the activated

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:41

Table 2. SO buggy TF-based training programs

Program
IPS-1
IPS-4
IPS-5
IPS-7
IPS-11
IPS-12
IPS-13
IPS-14
IPS-15
IPS-17

Recommended Fixes
switch to a numerical stable loss
add mean to the loss
change gradient descent by Adam
add an output activation and a numerical stable loss
remove the useless ReLU on the logits
fix a typo in the accuracy function
set lower learning rate(𝜂) and norm penalty(𝜆)
set a low learning rate(𝜂)
set a low learning rate(𝜂)
set a low learning rate(𝜂)

Table 3. Github buggy TF-based training programs

Program
DLT_0edb182
DLT_20d1b59
DLT_437c9c2
DLT_726b371
DLT_ded6612
FCN-CTSCAN_b170a9b
TFE_333
TFE_368
TFE_742675d
TFE_bc09f95

Recommended Fixes
remove redundant softmax layers
add a softmax layer
improve the loss reduction
add 𝜖 for a numerical stable loss
improve the parameters intialization
fix a mistake in the loss function
set an adequate weight initialization
set a low learning rate(𝜂)
improve the loss reduction
improve the loss reduction

built-in or user custom rules in independant processing jobs on separate containers in a way
that they do not interfere with the training job. Finally, users can set up alarms within Amazon
Cloudwatch, which is a real time monitoring and observability service, to indicate when a rule is
triggered. Table 4 summarizes the SMD’s built-in rules that are applicable for feedforward neural
networks. Although SMD and TheDeepChecker target a common subset of training issues, the
logic of debuggers’ rules are quite different, as SMD relies exclusively on the model data collected
offline during the user-configured training session. However, TheDeepChecker is an online debugger
framework that takes control of the ML training program and the datasets to orchestrate the
running of a sequential multi-phase debugging workflow, including preliminary independent
checks, a monitored single-batch training, and comparisons of multiple trained models. Indeed, the
TheDeepChecker’s debugging workflow involves verification rules that validate necessary properties
of the training program and assess the status of heuristic-based detectors for common DL model
training issues.

5.2 Case Study Results
In the following, we present the results of debugging sessions using TheDeepChecker on both
synthetic and real-world buggy programs. First, we evaluate the capacity of TheDeepChecker in
detecting faults injected in the base NNs, whether the fault is a coding bug or a misconfiguration

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:42

Ben Braiek and Khomh

Table 4. SMD Built-in Rules Applicable to FNNs

Component

Data

Loss

Weights

Activations

Gradients

Rules
Class Imbalance
Non-normalized image input
Not Decreasing
Unchanged
Overfitting
Underfitting
Overtraining
Poor Initialization
Abnormal Update Ratio
All Zeros
Abnormal Variance
Exploding
Neurons Saturation
Dying Neurons
Vanishing
Exploding

#id
𝑅0
𝑅1
𝑅2
𝑅3
𝑅4
𝑅5
𝑅6
𝑅7
𝑅8
𝑅9
𝑅10
𝑅11
𝑅12
𝑅13
𝑅14
𝑅15

of the system. Then, we assess the effectiveness of TheDeepChecker in debugging real-world buggy
TF programs that may contain multiple hidden bugs.

5.2.1 Debugging DNN Software System that contains Coding Bugs. Table 5 shows the
results obtained on the synthetic buggy training programs. For each injected bug, we report all the
checks fired by TheDeepChecker, as well as the performance of the trained model on test data and
the SMD’s rules that detected issues.

Accuracy of TheDeepChecker on coding bug detection. Given the fault injected in the syn-
thetic example, we put in bold the fired check(s) that are considered to be conceptually connected
to the underlying issue. Indeed, these bolded checks could guide the user towards recognizing the
occurred issue and fixing the buggy training program. Moreover, we study the other fired checks
for a full assessment as we keep running the TheDeepChecker, but we do not need to wait for all the
fired checks to spot and fix alerted DL bugs. Thereby, we also provide a user-defined boolean setting
(failed_on=True/False) that would enable an exception to be raised whenever TheDeepChecker
encounters any of these verifications. In practice, this helps save useless running time and com-
putational resources. Thus, we calculate the number of True Positive (TP) checks, False Positive
(FP) checks and False Negative (FN) checks. True positives are represented as a+b, where a and b
are the number of checks, respectively, defined to catch precisely the injected bug and to identify
generic training difficulties that can be correlated to various DNN issues. Although the proposed
generic checks, counted in 𝑏, spot fine-grained inefficient training traits, we do not consider the
bug was detected in case of 𝑎 = 0. Therefore, TheDeepChecker has a precision of 90% and a recall of
96.4% in detecting the synthetized DL coding bugs.
Comparison with Baseline. As can be seen, missing code statements can result in the dysfunc-
tion of a component in the DNN training program. In such cases, our verification routines find
some violations of properties associated with the expected output and normal behavior of the
buggy component. First, missing or redundant input normalization and over-scaled outputs are
detected by TheDeepChecker using the following prechecks on the loaded data: unscaled inputs

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:43

and unscaled outputs. Next, unapplying softmax over outputs and unshuffling the labels would
trigger the following verification routines, respectively, invalid predictions (i.e., do not respect the
probability laws) and corrupted labels (i.e., labels do not match with the features). Another studied
data preprocessing bug is the inappropriate data transformers that induce a shift between original
and augmented datasets. TheDeepChecker was able to detect such data shifts from the perspective of
the trained model behavior by comparing the activation patterns of the neural networks trained on
augmented/original datasets. In relation to the data, SMD supports only a rule (𝑅1) for unnormalized
images detection. For the missing activation, SMD triggers vanishing gradients (𝑅14), which is a
quite generic issue for DNN training.

Concerning wrong coding statements, TheDeepChecker is still accurate for mistaken axis in
tensor-based operations, thanks to the gradient-based dependency verification that point out any
overlapping between instances (i.e., rows) caused by an incorrect calculation. However, the de-
pendency verification fails on the reproduced wrong broadcasting of MSE because the error does
not induce an overlap between instances. In most of these wrong coding statements, SMD triggers
rules indicating the difficulty of training such as vanishing gradients (𝑅14) and non-decreasing loss
(𝑅14). Both TheDeepChecker and SMD succeed in revealing coding faults that change the training al-
gorithm’s behavior, like flipped signs in the gradient calculation that turns the training process into
a loss maximization. Nevertheless, TheDeepChecker is capable of detecting a wrong cross-entropy
function with switched mean and sum operations based on the initial loss value which is larger
than expected, and consequently, indicate the presence of a wrong reduction function for the losses
over data inputs. SMD reports vanishing gradients (𝑅14) for only the ShallowNN that could not
train the model with a wrong loss reduction (i.e., the accuracy of the trained model is equivalent to
random guess).

Regarding the misuse of API functions, we reproduced the situation of redundant softmax in-and
out-the cross-entropy loss for both studied Base CNNs. Despite the fact that TheDeepChecker
does not contain a specialized check to pinpoint accurately the issue, it could successfully detect
its presence in the training program for both buggy examples, contrary to SMD that remains
silent. In fact, TheDeepChecker generated an error message reporting learning issues in the last
dense layer of both ShallowCNN and DeepCNN containing the softmax redundancy within the
loss; more specifically, a slowness learning issue due to either low updates’ ratio. These slowness
learning issues, when spotted at the last layer (i.e., softmax activation), strongly indicate a waste of
information and an obstruction of the back-propagation of the error through the two consecutive
softmax activations. Hence, the relation between the fired checks and the occurred bug is not
straightforward (i.e., we do not count it among the main fired checks (i.e., the number a in True
Positives). Nevertheless, it is worth mentioning that TheDeepChecker generated a warning for the
user about the slow weight update encountered exclusively in the last dense layer, which can lead
him to review this ending part of the DNN design which contains the issue (i.e., redundant/useless
last activation function). Besides, TheDeepChecker reported symptoms in relation to difficulties faced
in the loss minimization: slow decreasing loss for both subjects and additionally highly-fluctuating
loss for DeepCNN.

Overall, TheDeepChecker reported misleading checks which are considered as false alarms. First,
it mistakenly reports non-representative loss (i.e., loss measure is not correlated with perfor-
mance metric) in some DeepCNN cases. Indeed, the obtained highly-fluctuating loss caused by
the activation redundancy significantly reduced the magnitude of the correlation between the
resulting noisy loss and the classification accuracy, and triggered the verification routine related
to non-representative loss. This highlights the difference between shallow and deep CNNs and
how the loss landscape is more complex for deeper NNs and can be substantially affected by these

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:44

Ben Braiek and Khomh

Table 5. Results of debugging coding bugs in DNN-based software systems

Faults

Base NN Perf.

SMD Rule(s)

Regr

24.20

-

missing input normalization

over-scaled outputs

redundant input normalization

gradients with flipped sign

missing softmax activation

softmax out-and in-the loss

softmax over wrong axis

CE over wrong axis

MSE with wrong broadcasting

inverted CE’s mean and sum

shuffle only the features

invalid data transformation

Shallow 11.35%
85%
Deep
20.14
Regr
2.86
Regr
Shallow 33.75%
77.5%
Deep
1.72𝑒7
Regr

Shallow
Deep

9.8%
10%

9.8%
Shallow
11.48%
Deep
Shallow 99.29%
83.24%
Deep
Shallow 99.45%
85.86%
Deep
8.92%
Shallow
86.79%
Deep
7.02
Regr
Shallow 11.34%
87.08%
Deep
7.27
Regr
Shallow 11.35%
10.09%
Deep
Shallow 99.24%
86.28%
Deep

𝑹1,𝑹8,𝑹14
𝑹1,𝑅8,𝑅10
𝑹2, 𝑹12
-
𝑹8, 𝑹14
-,𝑅8,𝑅10
-

𝑹11,𝑹14
𝑹11,𝑹14

𝑹14
𝑹14,𝑅8,𝑅10
-
-,𝑅8,𝑅10
𝑹14
𝑹14,𝑅8,𝑅10
𝑹2,𝑹7
-,𝑅8,𝑅10
𝑹2
𝑹14
-,𝑅8,𝑅10
-
-
-,𝑅8,𝑅10
-
-,𝑅8,𝑅10

Fired Check(s)
Uns-Inps1, PI-Loss2
Un-Fit-Batch3, Uns-Act-HS4
Uns-Inps, PI-Loss, Un-Fit-Batch
Div-Loss5, Div-W6, Div-B7, Div-Grad8
Uns-Inps, PI-Loss, Uns-Act-HS, (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
NR-Loss9
Uns-Outs10, SD-Loss11, Dead-ReLU12, Uns-Act-HS
Uns-Inps, SD-Loss, Uns-Act-LS13, Un-Fit-Batch
Uns-Inps, SD-Loss, W-Up-Slow14, Uns-Act-LS
Uns-Inps, Uns-Act-LS
Un-Fit-Batch, Div-Loss, Uns-Act-HS
Un-Fit-Batch, Div-Loss, Div-W,
Div-B, Uns-Act-HS, Van-Grad15
Un-Fit-Batch, Div-Loss, Uns-Act-HS, NR-Loss16
PI-Loss, Inv-Outs17, SD-Loss W-Up-Slow,
Van-Grad, Un-Fit-Batch, (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
Over-Reg-Loss18
PI-Loss, Inv-Outs, Van-Grad
SD-Loss, W-Up-Slow(Dense)
SD-Loss, HF-Loss19, W-Up-Slow(Dense), (cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss20
PI-Loss, Inv-Outs, Inv-Out-Dep21, Inv-Loss-Dep22
PI-Loss, Inv-Outs, Inv-Out-Dep, Inv-Loss-Dep
PI-Loss, Inv-Loss-Dep
PI-Loss, Inv-Loss-Dep
Un-Fit-Batch, SD-Loss, Van-Grad
PI-Loss
PI-Loss
Corrupted Labels
Corrupted Labels
Corrupted Labels
Shifted-Augmented-Data
Shifted-Augmented-Data

TP FP FN

1+3

1+6
1+2
1+3
1+3
1+3
1+1
1+2

1+5
1+2

1+5
1+2
0+2
0+2
2+2
2+2
2+0
2+0
0+3
1+0
1+0
1+0
1+0
1+0
1+0
1+0

0

0
1
0
0
0
0
0

0
0

1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0

0

0
0
0
0
0
0
0

0
0

0
0
1
1
0
0
0
0
1
0
0
0
0
0
0
0

relatively minor changes in the math calculations involved in the DNN mapping function. Second,
the traditional regularization check consistently triggers an alert of overwhelming regularization
gradient cost over the data gradient loss, but our inspection leads us to the fact that actual issue was
the vanishing gradient problem reaching exactly 0; we consider as misleading because the check
was designed to capture over-regularization cost and it can mislead the user towards inspecting
unnecessarily the regularization. On the other hand, we unbolded the SMD’s rules that have been
fired even for the clean training program. Indeed, these rules, 𝑅8 and 𝑅10, were fired during the
early training iterations on the DeepNN that includes advanced regularizers to smooth the loss
landscape and be able to train several hidden layers. This can be explained by the high sensitivity
of these rules that alert quickly about abnormal weights’ update ratio and variance starting from
the warm-up period required by these regularizers to stabilize the training. Besides, SMD reports a
false alarm of poor weight initialization (𝑅7) on a wrong cross-entropy calculation. In fact, SMD
relies on the application of rules’ functions on the periodically-saved summary statistics. Hence, 𝑅7
checks the variance of activation inputs across layers, the distribution of gradients, and the loss
convergence for the initial steps to determine if a neural network has been poorly initialized. These
training inefficiencies can be the result of many issues in the training program, which increases the
risk of misleading alerts.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:45

5.2.2 Debugging Misconfigured DNN Software System. Table 6 presents the debugging
results of TheDeepChecker on misconfigured synthetic training programs, following a similar
structure as Table 5.

Accuracy of TheDeepChecker on misconfigurations detection. We calculate true positive,
fault positive, and fault negative rates alongside the list of fired checks. Overall, TheDeepChecker
achieved 77% precision and 83.3% recall, when detecting misconfigurations in the studied DNN
training programs.

Comparison with Baseline. As can be seen, inappropriate initial weights, i.e., constant and
inept randomness, correctly trigger the following TheDeepChecker’s checks, unbreaking symmetry
and poor weights initialization. SMD uses multiple indirect criteria to recognize a bad initialization
through activations’ variances, gradients’ distributions, and the loss curve. It spots the unbreaking
symmetry issue, but it was less effective in detecting the dummy random weights with inappropriate
variance.

Similarly, TheDeepChecker was able to detect missing stabilization components, including missing
the whole batchnorm layers and missing the update of their global statistics, through respectively,
high unstable activations and unstable transfer from train to inference mode. Also, weak regular-
ization triggers the checking rule of zero loss, which implies that the optimizer likely overfits the
given training batches and strong regularization can lead to overwhelming weight norm penalty (in
case of l2-norm) or unstable transfer from train to test mode (dropout). Nonetheless, SMD was only
capable of detecting the negative effects of strong l2-norm regularization on the weights’ variances
and dying ReLUs.

Moreover, architecture- or problem-dependant issues like using ineffective loss function are
challenging to detect for developers (e.g., we refer to SO posts #36515202, #49322197, #56013688,
and #62592858); their identification depends heavily on the knowledge of the developer on Deep
Learning and his experience in implementing DNN programs. Indeed, these issues can have severe
effects on the convergence and stability of the DNN training process, especially on relatively high
capacity DNNs and complex statistical learning problems. When it comes to regression problems,
the use of cross-entropy(CE) instead of mean squared error(MSE) hinders the learning; so both
approaches were able to detect that. In case of classification problems, the use of MSE instead of
CE had less effects on the training performance, and consequently, harder to automatically detect.

2 Poor Initial Loss
6 Diverging Weights

1 Unscaled Inputs
STD 5 Diverging Loss
Loss
STD
Loss

10 Unscaled Outputs
14 Weight Update Slowly

18 High-Fluctuating Loss

3 Unable to fit Single Batch

7 Diverging Biases

8 Diverging Gradient

4 Unstable Activation with High
9 Non-Representative
13 Unstable Activation with Low
17 Overwhelming Regularization

11 Slow-Decreasing Loss
15 Vanishing Gradients

12 Dead ReLU
16 Invalid Outputs

19 Invalid Output Dependency

20 Invalid Loss Dependency

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:46

Ben Braiek and Khomh

Thus, TheDeepChecker alerted only inefficient training traits including vanishing gradients and slow
updating parameters for the deepCNN. Inversely, SMD alerted abnormal training curve, including
non decreasing and unchanged loss for the ShallowCNN, but the resulting test performance of the
model is high enough to just conclude that this steadiness was due to the optimization convergence.
With less implicit issue-verification connections, poor choices regarding the optimization rou-
tines, including inadequate magnitude of the learning rate or the epsilon of null divider prevention,
influence negatively the speed of parameters learning. Only TheDeepChecker successfully detected
the substantial difference in the magnitude of parameters’ updates. It detected slow updating
parameters in case of low learning rate and fast updating parameters in case of high learning rate
and low Adam epsilon. Indeed, TheDeepChecker’s unstable weight update detection strategy reposes
on DL researchers’ recommendations and it is sensitive enough to report precisely the low or high
magnitudes of weight updates that can guide DL users to adjust the optimizer’s configuration
touching the extent of updates. Nevertheless, both compared debugging methods were able to the
divergence of ShallowCNN caused by the high learning rate that manifests through a training
stagnation. Indeed, the high learning rate quickly induces a large update step towards the gradient
direction, which takes the weights to nonoptimal regions, and consequently, provokes dead ReLUs
and a highly-fluctuating loss optimization without convergence (unable to overfit the batch). Since
the inadequate optimizer’s jump likely happens at the very first iterations, there would not be fired
checks in relation with the speed of learning; so the users cannot identify the root cause easily
based on all this checking report.

As above-discussed for DL coding bugs 5.2.1, SMD keeps consistently reporting both of 𝑅8 and
𝑅10 rules, as well as, it reports a false alarm of poor weight initialization (𝑅7) on a inappropriate loss
selection (CE instead of MSE). On the other hand, TheDeepChecker reports false positives of non-
representative loss, overwhelming regularization loss paired with, respectively, high fluctuating
loss and vanishing gradients. This reinforces the fact that these false positives are caused by the
non-consideration of potential dependencies between DL training issues.

In the future, we plan to analyze in-depth the sequence of fired checks for the different DL bug
types and extend TheDeepChecker to consider recurrent patterns of negative verifications in order
to not only reduce the number of false positives that we discover during the assessment, but also
avoid overwhelming the users by several correlated training issues. Indeed, we observe in both
results (i.e., Tables 6 and 5) repetitive co-occurrences of multiple checks. For instance, we see that
various errors make the weight updates unstable, so the weights can potentially have more and
more negative values (i.e., over-negative tendency). In this case, the layers’ weighted sum would
produce mainly negative quantities, which will turn into zero activations by the ReLUs (i.e., dead).
Then, the back-propagated gradients, as described in Equation 14, starts vanishing quickly and, as
a result, the weights’ updates will tend to have too low magnitude and could cause the DNN to
freeze (i.e., triggering likely a slow- or non-decreasing loss).

5.2.3 Debugging Runtime Evaluation. TheDeepChecker enables the debugging of DNN training
programs through performing a stack of checks prior, during, and after the program execution.
Executing these checks can be expensive.
In this section, we assess the execution cost of the checks on a DNN training program. The execution
was done using a single machine having a CPU Intel i7-8750H 6-cores and a GPU NVIDIA GeForce
RTX 2080. The evaluation was done using the above-described base neural networks. To provide

1 Unbreaking Symmetry Weight

2 Poor Initial Weight

3 Weight Update Quickly

4 Unstable Transfer Mode

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:47

Table 6. Results of debugging misconfigurations in DNN-based software systems

Shallow 11.35% 𝑹7,𝑹10,𝑹13,𝑹14
Deep
Regr

1+8
𝑹7,𝑹14,𝑅8,𝑅10 Un-Sym-W, SD-Loss, Uns-Act-HS, W-Up-Slow 1+3
1+1

75.92%
2.17

-

Faults

Base NN Perf.

SMD Rule(s)

constant weights

Regr

2.53

𝑹7,𝑹10

dummy random weights

use of MSE instead of CE

use of CE instead of MSE

low learning rate

high learning rate

Adam epsilon 𝜖 < 10−8
missing batch-norms
no-update of batch-norm globals
low 𝜆 for norm penalties
low 𝜆 for norm penalties
high 𝜆 for norm penalties
high 𝜆 for norm penalties
high 𝑘𝑒𝑒𝑝_𝑝 for dropouts
low 𝑘𝑒𝑒𝑝_𝑝 for dropouts

unbalanced dataset

Shallow 99.18%
71.89%
Deep
Shallow 99.17%
69.52%
Deep
49.46
Regr
5.48
Regr
Shallow 98.96%
53.73%
Deep
2.55
Regr

Shallow 11.34%
86.29%
Deep
86.75%
Deep
80.79%
Deep
84.35%
Deep
2.39
Regr
Shallow 99.27%
7.05
Shallow 64.88%
73.15%
Deep
78.74%
Deep
Shallow 99.24%
86.28%
Deep

Regr

𝑹2,𝑹7
-,𝑅8,𝑅10
𝑹2,𝑹3
-,𝑅8,𝑅10
𝑹3,𝑹7,𝑹14
-
-
-,𝑅8,𝑅10
-

𝑹13,𝑹14
-,𝑅8,𝑅10
-,𝑅8,𝑅10
-
-,𝑅8,𝑅10
-
-
𝑹10,𝑹13
𝑹10,𝑹13
-
-,𝑅8,𝑅10
bm𝑅0
bm𝑅0,𝑅8,𝑅10

TP FP FN

Fired Check(s)
Un-Sym-W23, SD-Loss, Uns-Act-LS
W-Up-Slow, Un-Fit-Batch
Un-Sym-W, SD-Loss, Neg-W24,
Over-Reg-Loss, Uns-Act-LS, Dead-ReLU,
W-Up-Slow, Van-Grad, Un-Fit-Batch

PI-W25, Uns-Act-LS
PI-W, PI-Loss, SD-Loss
Dead-ReLU, Uns-Act-LS
PI-W, Uns-Act-HS, (cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss
-
SD-Loss, HF-Loss, W-Up-Slow
Un-Fit-Batch, Van-Grad(dense), (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
Over-Reg-Loss
Un-Fit-Batch, SD-Loss, W-Up-Slow
SD-Loss, W-Up-Slow
SD-Loss, W-Up-Slow, (cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss
W-Up-Fast26, SD-Loss
Un-Fit-Batch, HF-Loss, Dead-ReLU,
Uns-Act-LS, (cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss
Uns-Act-HS, W-Up-Fast
Uns-Act-HS, W-Up-Fast
SD-Loss, Uns-Act-LS, W-Up-Slow, (cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss
Uns-Mode-Tr
Zero-Loss
Zero-Loss
Over-Reg-Loss, Uns-Act-LS, Un-Fit-Batch
Over-Reg-Loss
Zero-Loss
HF-Loss, Uns-Mode-Tr,(cid:40)(cid:40)(cid:40)(cid:40)
NR-Loss
Unbalanced Labels
Unbalanced Labels

1+4

1+4
1+1
0
0+3
0+3
1+2
1+1
1+1
1+0

0+4
1+1
1+1
1+2
1+0
1+0
1+0
1+2
1+0
1+0
1+1
1+0
1+0

0

0
0
0

0
1
0
0
1
0
0
1
0

1
0
0
1
0
0
0
0
0
0
1
0
0

0

0
0
0

0
0
1
1
1
0
0
0
0

1
0
0
0
0
0
0
0
0
0
0
0
0

insights on the execution cost of TheDeepChecker during a debugging session, Table 7 reports the
average time spent using TheDeepChecker on each phase. As can be seen, the pre- and post-training
phases, (1) pre-training conditions check and (3) post-fitting conditions check, varies according to
the dimensionality of the input data and the complexity of NN as they include normalization tests,
tensor-based operations dependency, and even regular training of DNN for several epochs (we set
up 50 by default) to compare metrics over epochs and make some activation patterns comparison.
Next, the phase of proper fitting on a single batch requires only few iterations (i.e., it remains a
setting option for running the test and by default, a maximum iterations equals to 200), but with
short periodicity of verification routines (i.e., it is also a setting option and by default, we fixed a
period equals to 10 iterations). These default setting options are derived from our experimentation,
however, the configuration choices should take into consideration the complexity of the DNN
under test and the default settings enable a sufficient amount of monitored iterations to detect the
issues given the complexity of the studied neural networks. For higher complexity NNs, an increase
of these parameters’ values may enhance the issue detection capability of TheDeepChecker as it will
make more intensive verification during the testing iterations. Given the average execution time of
a regular training iteration and a monitored training iteration, we find that the verification routines
running in-between the training iterations increased the training iteration runtime by averagely
10×. In contrast, SMD hooks multiple internal data recorders to the full training session in order to
fetch and save periodically tensors including activations, weights, and gradients. SMD incorporates
several optimizations to improve I/O performance and sets up relatively long, by default, save
intervals (i.e., around 500 steps). In fact, our experimentation on AWS instance, ml.p2.xlarge,
which contains 1-GPU and 4 virtual processors, yields, averagely, an overhead of no more than
13% for a training iteration monitored by 4 rules. Then, SMD verifies the rules by offloading data
inspection, shared into separate containers in a way that users can run an arbitrary number of rules

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:48

Ben Braiek and Khomh

without impacting the training process itself. Nevertheless, this multi-job processing and I/O data
offloading adds an overhead, even for simple DNN programs. During our debugging sessions on
ml.p2.xlarge with 4 activated rules, we wait for 3 − 5 minutes to have the final rules check reports.

Table 7. Execution cost of TheDeepChecker during different debugging phases (average time in seconds)

ShallowCNN
DeepCNN
RegrCNN

Pre-train Check Fitting Check Post-Fit Check
20.61
82.74
0.996

751.70
1641.59
5.70

16.63
20.81
6.75

5.2.4 Assessment of TheDeepChecker on Real Buggy DNN Software. Table 8 and Table 9
show the debugging results of both TheDeepChecker and SMD on real-world buggy TF programs
extracted, respectively, from Stackoverflow and GitHub. Indeed, we add all the turned-on verification
checks/rules on each tested DNN program, but we highlight the ones that are considered to be
related to the actual fixed bug. For SMD, we rely on its official built-in rules documentation [93]
about their logic and targeted issues in order to decide if the fired rules have This allows us to
compute the success rate of TheDeepChecker in detecting the bugs that have been fixed by the SO
users or the GH project maintainers. Indeed, TheDeepChecker succeeds in 70% of the SO buggy code
and 80% of the buggy versions of GH projects. However, SMD succeeds in 60% of both SO and GH
buggy examples. SMD generally alerts the user with high-level indicators of abnormal/suspicious
on-training neural network state, but TheDeepChecker often reports broken properties that are
connected to a narrower scope of DL faults and help users identify the main root cause. For instance,
lines of code 1 shows that the NN’s output layer, y_, has no activation function, which causes
an incorrect calculation of cross entropy using logits instead of probabilities. Moreover, the cross
entropy formula involves a risky use of log on possibly zero values.

y_ = tf . matmul ( h1 , W_out )
cross_entropy = tf . reduce_sum (-y* tf . log ( y_ ) -(1 - y )* tf . log (1 - y_ ) ,1)

Code 1. Lines 18;21 of IPS-7

TheDeepChecker reports invalid output layer because the verification routine on the last layer
requires yielding probabilities when NN solves a classification problem. It also alerts about the
diverging loss caused by the NaNs of tf.log(0). Although abnormal variance of weights reported by
SMD reflects an unstable learning process, it cannot be connected to the missing activation or the
unstable loss issues.

Code 2. Lines 37-38;45-46 of IPS-12

Yhat = tf . matmul (l3 , W5 ) + b5
Ypred = tf . nn . sigmoid ( Yhat )
# ...
correct_prediction = tf . equal ( tf . greater (Y , 0.5) , tf . greater ( Yhat , 0.5))
accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 ))

Code 2 shows another buggy code snippet, where SMD reports only abnormal variance of weights,
contrary to TheDeepChecker which spots the non-correlation anomaly between the loss and the
accuracy. Indeed, the user mistakenly used the logits Yhat instead of the sigmoid outputs Ypred
in the inference of predictions with a threshold of 0.5, which leads to a wrong calculation of
accuracy. Only TheDeepChecker cover the coding mistakes in the performance functions through

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:49

the validation of their correlation coefficient over the training iterations. Thus, the use of the
logits instead of probabilities would always yield the class 1 for predictions; which would break
the property of performance metrics correlation as the progress of accuracy metric would be
uncorrelated with the loss value.

def fc_layer ( input , size_in , size_out , name = " fc " ):

with tf . name_scope ( name ):

Code 3. Lines 20-28;46-51 of IPS-11

w = tf . Variable ( tf . truncated_normal ([ size_in , size_out ] , stddev =0.1))
b = tf . Variable ( tf . constant (0.1 , shape =[ size_out ]))
activation = tf . nn . relu ( tf . matmul ( input , w) + b)
# ...
return activation

def mnist_model ( learning_rate , path ):

# ...
logits = fc_layer ( fc1 ,1024 ,10 , name =" fc2 ")
probabilities = tf . nn . softmax ( logits )

with tf . name_scope (" xent " ):

xent = tf . reduce_mean (

tf . nn . softmax_cross_entropy_with_logits ( logits = logits , labels =y ))

Even if TheDeepChecker has no specific property that would be broken by the fault, TheDeepChecker
can trigger training difficulty symptoms equivalent to SMD’s rules, as shown in the lines of Code 3.
Indeed, the DL developer unified all the fully-connected layers by a custom function, but he
mistakenly applied it over the last fully-connected layer. This induces a useless ReLU activation
on the logits before computing softmax-based scores or losses. As a result, the double activation
obstructs the information from flowing smoothly and causes a slowness of weight update and
vanishing of gradients. Although the same fault-related symptoms were reported by both debugging
tools, TheDeepChecker reports the unable to overfit the batch that strongly indicate a serious model
fitting problem.

Nonetheless, TheDeepChecker was more verbose triggering several positive checks for each
buggy DL training program, and relying on the accepted answer on SO post or the changes in
the bug-fixing commit does not allow us to compute the false positives, which represent fired
checks without corresponding issue, and false negatives, which represent the existed faults without
corresponding fired checks, on these real-world DNN programs. Moreover, finding fired verification
routines that are, by definition, related to the actual fixed bug, does not imply that users can fix the
bug correctly using the TheDeepChecker diagnostic reports. In the next evaluation of usability, we
will ask two experienced DL engineers to perform a closed-feedback debugging and fixing loop
using TheDeepChecker on each of the buggy snippets of code found on StackOverflow.

5.3 Usability of TheDeepChecker
In this section, we report about a usability study performed with two professional DL engineers,
namely 𝐸1 and 𝐸2, with the aim to assess the relevance of TheDeepChecker’s error messages at

1 Poor initial bias

2 Missing bias

3 Saturated Sigmoid

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:50

Ben Braiek and Khomh

Table 8. Debugging Results of StacOverflow TF-based training programs

Program
IPS-1

IPS-4

IPS-5

IPS-7

IPS-11

IPS-12

IPS-13
IPS-14

IPS-15

IPS-17

TheDeepChecker’s Fired Check(s)
PI-W, PI-b, PI-Loss, Uns-Act-LS
PI-W(1), PI-b(1), PI-Loss(1),
Uns-Act-HS(1), Zero-Loss(2)
Uns-Inps, Un-Fit-Batch, Div-Loss,
W-Up-Fast, W-neg
PI-W(1), Miss-b(1), PI-Loss(1), Inv-Outs(1),
Div-Loss(1), Un-Fit-Batch(1), Sat-Sigmoid(1-2)
PI-W(1), PI-b(1), PI-Loss(1), Van-Grad(1),
W-Up-Slow(1), Un-Fit-Batch(1), Zero-Loss(2)
Uns-Inps(1), PI-W(1), PI-Loss(1), NR-Loss(1),
HF-Loss(2), Dead-ReLU(2), Uns-Mode-Tr(2)
Uns-Inps(1), PI-W(1), PI-Loss(1), W-Up-Fast(2),
Over-Reg-Loss(2), Un-Fit-Batch(1-2)
PI-W, PI-Loss, Un-Fit-Batch
Uns-Inps, Miss-b, Div-Loss,
Div-W, Div-Grad, Un-Fit-Batch
Uns-Inps(1), Div-Loss(1),
Un-Fit-Batch(1), W-Up-Slow(2)

SMD Rule(s)
𝑅7, 𝑅10

𝑹2, 𝑹14

𝑅9, 𝑹10

𝑅10

𝑹8, 𝑹10, 𝑹14

𝑅9, 𝑅10

-
𝑹8, 𝑹9, 𝑹10

𝑹9, 𝑹10

𝑹9, 𝑹10

Table 9. Debugging Results of Github TF-based training programs

Program
DLT_0edb182

DLT_20d1b59
DLT_437c9c2

DLT_726b371

DLT_ded6612

FCN_b170a9b

TFE_333

TFE_368
TFE_742675d

TFE_bc09f95

TheDeepChecker’s Fired Check(s)
PI-W, PI-B, Uns-Act-LS, SD-Loss
PI-W, PI-B, PI-Loss, Uns-Act-LS,
SD-Loss, NR-Loss, Un-Fit-Batch
Un-Sym-W, PI-Loss(Huge Err)
PI-W, PI-B, PI-Loss,
Div-Loss, Uns-Act-LS
PI-W, PI-B, PI-Loss,
Uns-Act-LS, SD-Loss
PI-Loss, Uns-Act-LS, Dead-ReLU,
Van-Grad, Un-Fit-Batch, SD-Loss
PI-W, PI-B, PI-Loss,
W-Up-Slow, SD-Loss, NR-Loss
PI-W, PI-B, PI-Loss,
W-Up-Fast, Zero-Loss
Un-Sym-W, PI-Loss(Huge Err)
PI-W, PI-B, PI-Loss(Huge Err), LR-Loss
Un-Fit-Batch, W-Up-Slow, Van-Grad

SM Debugger’s Fired Rule(s)
𝑹14

𝑹14
𝑅8, 𝑅9, 𝑅10

𝑅7, 𝑅10

𝑅2, 𝑹10, 𝑅14

𝑅8, 𝑹14

-

𝑹10
𝑅8, 𝑅9, 𝑅10

𝑅8, 𝑅10, 𝑹14

guiding developers in identifying the root cause of bugs and fixing them. The two DL engineers
involved in the study have 3 years of experience working with TensorFlow. They are currently
employed in AI software development teams in technology companies, building DL-based software
systems. To assess the relevance of TheDeepChecker’s error messages, we provided the two engineers
with 10 buggy DL training programs (i.e., the ones reproduced from SO posts in Table 8) and asked

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:51

them to use TheDeepChecker for debugging them and fixing the identified bugs. We made it clear
to them that we are not evaluating their ability to detect the bugs based on their knowledge and
asked them to only follow the clues contained in the debugging logs generated by TheDeepChecker.
We also asked them to explain how they inferred the root cause of faults based on the information
provided by TheDeepChecker and to suggest fixes. We made the decision to ask participants to fix
detected bugs because to allow them to explore as many bugs as possible. The DL programs used in
our study contain more than one bug, and to progress in the debugging process participants have
to fix detected bugs. For instance, unnormalized inputs may cause the divergence of the training
and turn the loss quickly to NaN. Hence, it obstructs the training dynamics, and consequently, the
TheDeepChecker’s debugging session. Thus, the DL engineer should normalize the inputs to fix the
issue and restart the debugging session. In Table 8, we added (𝑛) next to verification routines to
identify the debugging session during which the routine was triggered. For example, Zero-Loss(2)
means that during the second debugging session (2) after fixing some of the issues, TheDeepChecker
newly reports the Zero-Loss warning that indicates strongly a lack of regularization in the DNN.
Although the bug fixes suggested by the two engineers are quite different, they have performed
similar sequence of debugging sessions where they focus on fixing the same training issues at
each session and have received mostly the same amount of notification messages (from the fired
verification routines) at the different debugging steps for each given buggy program.

Table 10 shows the fixes suggested by the engineers based on the error messages generated
by TheDeepChecker. As discussed in the paragraph 4.6.5, many training issues are correlated and
induced by the same bug. In Table 10, based on the explanations provided by the engineers, we
present the main issues reported by TheDeepChecker that lead them to identify and localize the
root cause of the faults, whether it being caused by a coding bug or a misconfiguration.
As can be seen, most of the found faults are common (almost 96.5% of cases), which reinforces the
argument that our verification routines are quite precise, regarding the problematic component
and its occurring symptoms. However, given the recommended fixes from SO post’s answers (see
Table 10), we can see that the majority of fixes provided by the engineers are different. In the
following, we discuss these differences in detail.

First, we observe that there are emergent fixing patterns followed by the community, which
are not always efficient. Indeed, we can consider them as technical debts because they enable the
convergence of training and fitting the DNN, but the main root cause of the issue is not solved,
which provokes the same issue following any further changes on the DNN program or the inputs
data. For instance, the buggy TF programs, IPS-5, IPS-14, IPS-15 and IPS-17 share the main issue of
diverging loss problem, which turns its value to NaN and obstructs the training process. The initial
fixes recommended by the community consists in improving the optimization routines, including
the decrease of learning rate or the substitution of regular gradient-descent by advanced variants
with internal adaptive learning rate like Momentum or Adam. When using TheDeepChecker, our
two engineers were able to find the main root cause of diverging loss in buggy TF programs,
IPS-4, IPS-15 and IPS-17, which is the unnormalized inputs. In the case of IPS-14 the problem was
both the inefficient initial random weights and the poorly designed loss (i.e., using sum over the
instances’ errors instead of average). Without a fine-grained analysis tool like TheDeepChecker it
was difficult for Stack Overflow users (who suggested solutions) to uncover this. The main lesson
that can be derived from this example is that multiple poor design choices and coding mistakes can
induce well-conditioning to the loss minimization problem, and as result, may be the origin of its
divergence. Therefore, tuning the learning rate blindly will only make the training program run at
its minimum capacity, and hence, the real bugs will remain hidden. By decreasing the learning rate
in IPS-15 TF program from 0.5 to 0.0005 to enable learning under the condition of unnormalized
inputs, SO users only introduced a technical debt in their program. However, TheDeepChecker steers

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:52

Ben Braiek and Khomh

the engineers towards fixing permanently the root cause problem, which is the inappropriate scale
of features’ values. Concerning the same bug of unnormalized inputs in the TF program IPS-17,
TheDeepChecker reports in the second debugging session following the normalization of the inputs
that the weights is slowly updating, which lead both our two engineers to fix it by increasing
further the learning rate (i.e., both engineers ended up with taking actions that are totally the
inverse of the initial recommended fix).

Second, we found that TheDeepChecker spots the major bug preventing the training program
from fitting the model in regard to the buggy programs IPS-4, IPS-11, IPS-12, and IPS-13. Indeed,
our engineers, 𝐸1 and 𝐸2, confirmed that the poor initial loss check alerted them to the fact that
the loss is not a scalar, which led them to add the average as loss reduction strategy in IPS-4
program. In IPS-11, they mentioned that the vanishing gradient problem starting from the first
training iterations at the last dense layer, guides them to inspect the last layer (logits). They found
that the program uses the same implemented function fc_layer that performs ReLU as non-linear
activation for all fully connected layers. However, this useless non-linear activation erases relevant
learned information and obstructs the training, because the nullified negative values make all their
corresponding labels share the same probability after applying the softmax. In the case of IPS-12, the
non-representative loss check that remains active over the iterations displaying increasingly smaller
correlation, persuaded both DL engineers that either the accuracy or the loss function is mistaken, so
they check them out carefully and found a typo in the accuracy function (i.e., passing logits instead
of probabilities as predictions). Regarding the IPS-13 program, the engineers adjust the learning
rate and the norm penalty values to make the program satisfy the verification routines related to
standard regularization risks and unstable learning of parameters that triggered, respectively, fast
updated weights and overwhelming regularization loss verification routines. However, the two DL
engineers failed to correctly fix the major issues contained in IPS-1 and IPS-7. They proposed to
change the sum reduction strategy by the average and to pass the probabilities instead of logits to
the cross-entropy loss, to correct, respectively, the poor initial loss in IPS-1 and diverging loss in
IPS-7. However, the real bug reported by the user was the loss turning into NaN values. Because this
exception is raised infrequently, the problem cannot be always detected easily. Also, TheDeepChecker
considers the NaN loss as diverging loss and cannot provide further indications about any potential
root cause. As a result of this limitation of TheDeepChecker, both engineers could not identify the
root cause of the issue by relying on the message generated by TheDeepChecker, which claims that
the cross-entropy loss function contains the expression 𝑡 × log(𝑦), which renders NaN (0 × log(0))
when 𝑡 = 0 and 𝑦 approaches to 0. In fact, the recommended fix was adding an epsilon (𝜖 > 0) to
avoid the undefined expression, but a more appropriate repair for the loss numerical instability
is to use, instead of hand-crafted loss, the recent TF built-in logit-based loss function, including
both softmax and cross-entropy, which is numerically stable. By definition, our property-based
debugging process relies on the available data and tries to catch properties’ violations through
watching the execution of the training program. Therefore, it cannot detect numerical instabilities
that occur in particular ranges of values. Odena and Goodfellow [73] proposed a coverage-guided
fuzzing testing tool that is able to find mutated inputs triggering erroneous TF program’s behaviors
including NaNs raised by unstable math computation. Their evaluation shows that original and
even randomly synthetic data have low chances to trigger such corner-case behaviors and expose
these numerically unstable math functions. Therefore, to the best of our knowledge, fuzzy testing
approaches are more suitable for detecting numerical instabilities in DNN training programs.

Besides, the results show that TheDeepChecker guided the DL engineers towards detecting other
issues that do not prevent the program from training, but which should be addressed to improve
performance and avoid all the non-optimal local minima in the loss curve. As can be seen in
Table 10, most of these additional detected issues are related to poor initial parameters, lack of

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:53

regularization, and unstable learning velocity between the layers. Nevertheless, we observed that
the DL engineers proposed some different repairs to fix the same issue identified through the
debugging sessions using TheDeepChecker. This means that there are multiple possible fixes for
the same issue identified by TheDeepChecker and that the choice of a specific fix depends on the
knowledge and experience of the engineer. Indeed, we found that for some issues, our engineers, 𝐸1
and 𝐸2 were able to turn off the alert and improve the performance of the training using different
techniques. For instance, TheDeepChecker spots unstable activations with low variance regarding
the first convolutional layer in the trained DNN of IPS-1 program. Engineer 𝐸1 understood that
the first convolutional layer was not optimally learning the features, which led him to carefully
increase the learning rate and solve the problem. In another example, Engineer 𝐸2 understood that
the difference in magnitude of updates between intermediate layers causes a problem of internal
covariate shift, which can be solved by adding batch normalization following each intermediate
layer. He went on and implemented this fix. In the future, we will examine further the fixes proposed
by DL practitioners to overcome the studied training issues and analyze their impact on the quality
of the code and the performance of the DNN training program.

6 THREATS TO VALIDITY
Selection Bias. The selection of the subject DL training programs could be an internal threat
to validity. In this paper, we try to counter this issue by using two complementary empirical
evaluations on synthetic buggy programs and real buggy programs. We used diverse base DL
training programs that solve regression and classification problems. They encode DL models with
different architectures, complexities, and techniques. Moreover, the base synthetic DL programs
have official implementation references and run on widely-studied datasets (i.e., Auto-MPG, MNIST,
CIFAR10). Then, the buggy versions of these synthetic DL training programs were created to mimic
the DL faults reported and studied in empirical studies on DL programs’ defects. The discussion
on the results of the evaluation on the synthetic buggy programs provides insights on how the
properties and heuristics deployed in the verification routines were able to detect the behavioral
training issues caused by the injected bug. Indeed, we leveraged totally-disconnected workflows
for the construction of the synthetic buggy programs and the verification routines. Figure shows
how the abstraction of DL faults to synthetize buggy subjects was done essentially on former
empirical studies’ datasets and our manual inspection of TF-related SO posts. On the other hand,
Figure shows how the design of verification routines was guided by applied DL research works
and technical DL expert reports about troubleshooting to codify fundamental properties of DL
programs and practical heuristics on proper DL training dynamics. It was crucial to keep synthetic
bugs and verification routines separate in order to avoid hard-coded verifications that target the
mainstream DL faults identified by the community. Although the injection of DL faults was done
on reference DL models with minimal code changes, it is still a human-crafted process that may
contain imperfections. Furthermore, each synthetic buggy program represents a well-designed base
program with only a single bug injected. Thus, we complement the evaluation using 20 real-world
buggy programs from the dataset provided by Zhang et al., which are related to the scope of our
targeted DL bugs, and represent more realistic conditions of debugging since they may contain
different issues simultaneously, and even issues that are not uncovered by maintainers yet.

Settings’ Generalizability and Transferability. The setup of heuristic-based thresholds could
be an external threat to validity. In our design and implementation of property-based verification
routines, we relied on the original documentation sources [20, 29, 38, 88] that described the issues, to
set up the thresholds, when they are indicated. Nonetheless, some properties were always presented
and studied through visual plots comprehensible by humans. This makes the design of metrics and
their thresholds challenging, but we focus on the abstract violation traits of the properties rather

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:54

Ben Braiek and Khomh

Table 10. The repairs suggested by DL Engineers (𝐸1 and 𝐸2) for real-world buggy TF programs

Program

IPS-1

IPS-4

IPS-5

IPS-7

IPS-11

IPS-12

IPS-13

IPS-14

IPS-15

IPS-17

Fired Checks
PI-W, PI-b
PI-Loss
Uns-Act-LS
PI-W, PI-b
PI-Loss
Zero-Loss
Redundant-Layers
Uns-Inps, Div-Loss
PI-W, Miss-b
Inv-Outs
Div-Loss
Sat-Sigmoid
PI-W, PI-b
Van-Grad (last layer)
Zero-Loss
Uns-Inps
PI-W
NR-Loss
HF-Loss, Uns-Inference
Uns-Inps
PI-W
W-Up-Fast
Over-Reg-Loss
PI-W
PI-Loss
Uns-Inps
Miss-B
Uns-Inps
W-Up-Slow

Suggested Fixes
change 𝑊 and 𝑏 initializers
set average instead of sum for loss reduction
𝐸1: increase 𝜂 | 𝐸2: add batch-norms
change 𝑊 and 𝑏 initializers
set average as loss reduction strategy
add dropout layers
remove a dense layer
normalize the data
change 𝑊 initializers and add null 𝑏
add output activation layer
passing the probas instead of logits to the loss
change hidden activations (Sigmoid to ReLU)
change 𝑊 and 𝑏 initializers
remove ReLU on the logits
add dropout for the dense layer
normalize the data
change 𝑊 initializers
fix typo in the accuracy
increase the 𝑘𝑒𝑒𝑝_𝑝 for dropout layers
normalize the data
change 𝑊 initializers
decrease the learning rate 𝜂
decrease the norm penalty 𝜆
change 𝑊 initializers
set average instead of sum for loss reduction
normalize the data
add null 𝑏
normalize the data
increase the learning rate 𝜂

than the concrete studied instances, in order to be able to construct a verification routine based on
the foreknown training misbehaviors, as discussed in the implementation strategies (section 4.2.3).
Therefore, given the dynamic and continuous aspects of TheDeepChecker, it was possible to set up
intuitively pessimistic thresholds, to be our default configuration, in order to ensure a high coverage
with less false alarms when enough monitored training iterations were executed. Indeed, we avoided
the empirical tuning of these thresholds because we did not have access to a large benchmark
of reproducible buggy programs. Besides, the shrinking of suspicious program state includes the
computed metrics and thresholds that were behind the fired checks, which is a mitigation strategy
to help the user ignore false positives. Given the variables’ instatiations in the violated rules, users
can also assess the sensitivity of the thresholds on their DL application domain, which allows them
to set up a more precise custom configuration for TheDeepChecker.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:55

7 LIMITATIONS AND FUTURE WORK
DL knowledge remains crucial in the debugging of DNN training programs. Many of our
verification routines are implemented using statistical metrics and heuristics that are very related
to inefficient training traits, so they are often connected to multiple possible root causes. In
the inverse direction, most of the training pitfalls would trigger multiple fired checks because a
faulty component often violates its related properties, and consequently, leads to other training
properties’ violations. For example, a bad initialization of weights violates the required asymmetry
between neurons, however, the resulting unbreaking symmetry would lead to other issues, like
over-negative weights, Dead ReLUs, and vanishing gradients. Indeed, all the neurons will receive
identical gradients and evolve throughout training, effectively preventing different neurons from
learning different things. Thus, it is likely that a non-optimal gradient update, from the starting
iterations, would be applied symmetrically to all the neurons, and consequently, would cause the
stagnation of the neural network.
TheDeepChecker incorporates dynamic verifications with periodic inspection reports, narrowing
down the space of suspicious states, which help the user recognize fault patterns and identify the
root cause by analyzing the timeline of fired checks (i.e. their chronological order) and the reported
information (i.e., positions, metrics, thresholds, etc.). Nevertheless, these mitigation strategies
require sufficient DL knowledge and skills. Developers need to be sensitive to such details in order
to be able to efficiently interpret the debugging reports.
In the future work, some engineering efforts are needed to fill-in the messages with relevant links
to resources and documentation in relation to the detected properties’ violations. Besides, larger
datasets of buggy DL training programs could serve to better learn the patterns of the common
DL faults and be able to propose DL faults’ models that synthesize all these metrics, heuristics
and properties, aiming at detecting accurately the main root cause and the observed inefficiency.
Furthermore, a static code analyzer can be applied to load the code structure and components
into the debugger’s state, thereby reducing the number of candidate errors and improving the
localization of bugs. This also opens the path to include fixes’ recommendation and study solutions
for automatic program repairs.

Scoping on Feedforward Neural Network Architecture. Many of the targeted training pit-
falls and the proposed properties are generalizable to other model architectures [64], but in this
paper, we focus on their application for the feedforward architecture, which is, first, widely used in
several regression and classification problems, as well as, reinforcement learning tasks. Second,
it is the basic neural network model that influences novel architectures, and even represents one
of their building blocks. Nonetheless, recurrent neural networks (RNNs) have faced more severe
gradient problems [76], including vanishing and exploding phenomena. Generative adversarial
networks (GANs), which particularly leverage two on-training models in a min-max game, raise
novel training issues in relation to the learning stability and convergence, in addition to other
GAN-specific problems [5, 52, 83] such as mode collapse, where the generator outperforms quickly
the discriminator, without fitting the data distribution, but through simply rotating over few data
types. In the future, we plan to examine the training issues experienced by these novel architectures,
in order to revisit our verifications in the future versions of TheDeepChecker and enlarge our
scope of DL model architectures. Indeed, we should adjust their integrated metrics and boundary
conditions, as well as, elaborate advanced checks to detect the violations of their specific design
properties and statistical learning principles.

Usability Study Limited on the Mappings from Checks to Fixes. Given the real-world
DL programs published by SO users, we recruit two DL engineers from two different teams and
having different backgrounds for a usability evaluation, focusing on the mappings from the fired

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:56

Ben Braiek and Khomh

checks to the DL program fixes. Nonetheless, other important dimensions such as the time spent
on debugging and the relevance of the proposed fixes, could be assessed in comparison with the
trial-and-error debugging process without TheDeepChecker. Therefore, we plan to set up controlled
and monitored experiments of debugging DL-based software programs having different complexity,
in both modes, assisted with TheDeepChecker and not assisted. This allows to perform a broader
usability evaluation on a larger group of participants including students, junior/senior DL engineers,
junior/senior data scientists, and junior/senior DL researchers.

8 RELATED WORK
Software Testing Approaches for DL Models. Leveraging concepts from both software testing
and adversarial DL domains, researchers have proposed automated testing methods for DNN-based
models [12]. The majority of them focus on testing the model by automatically generating synthetic
test inputs with high-fault revealing ability. Pei et al. [77] proposed the first white-box test adequacy
criterion for DL models, named Neuron Coverage (NC), which is inspired from code coverage in
traditional software testing. Next, Ma et al. [66] generalized the concept of NC by defining a set of
multi-granularity testing criteria, including neuron-level and layer-level coverage. Once the test ad-
equacy criterion is set up, a data generator should systematically produce test cases, enhancing the
target test adequacy measure. Thus, the proposed data generators solve this maximization problem
relying on gradient-based optimizers [77] [34], gradient-free optimizers(i.e.,metaheuristics) [11],
and greedy search-based process [94] [73] [97]. As DNNs cannot guarantee 100% of correct answers,
we are always capable of finding adversarial inputs for which the DNN’s predictions are wrong.
Additionally, there is no deterministic relation between mismatched pairs of (input,output) and
bugs, i.e., we cannot conclude the existence of a bug in the DNN program by observing unexpected
outputs of synthetic test cases. To improve the trustworthiness of a DNN-based software system,
we propose to debug carefully the training program from the ingestion of data to the generation of
the final model. Our debugging approach, TheDeepChecker, aims to assert the correctness of the
DNN training’s program implementation and configuration, as well as, providing a preliminary
validation of its design quality. Once the DNN training program is validated to be bug-free and
appropriately configured, testing the trained model against genuine and synthetic test datasets is
still required to assert the effectiveness and the robustness of the DNN in performing its target task
under different condition.
Software Testing Approaches for Debugging Training Algorithms. In contrast with model
testing approaches, few research works have concentrated on the correctness of DNN-based
software programs. Dwarakanath et al. [23] define metamorphic relations (MRs) between data
transformations and their resulting effects to validate the correctness of the software system.
However, these MRs can be seen as high-level properties of the self-learning software system.
For instance, the permutation of the order of data instances or the linear scaling of data features
(i.e., train and test datasets), should not affect the performance of the training. Thus, even if these
high-level MRs may detect software bugs that substantially affect the behavior of DNN training
programs, they do not ensure finding the bugs contained in DNN-based systems, and do not provide
guidance towards identifying their root causes. Given the mathematical and statistical nature
of DNN-based software systems, Selsam et al. [86] propose to use machine-checkable proofs to
validate their implementation. However, the proposed approach is difficult to adopt in practice
because it is built to test a code written from scratch, and nowadays, developers leverage third-party
DL libraries to construct reliable and scalable DNN-based software systems.
Contrary to these approaches, TheDeepChecker can be applied to code using third party libraries
or written from scratch. As shown during the evaluation, the faulty components can be detected
by TheDeepChecker when they violate fundamental properties of a DNN training algorithm. In

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:57

addition to its fault detection ability, TheDeepChecker’s internal set of verification routines captures
the training issue, at its first appearance, and provides rich feedback on the problematic situation,
in order to help users identifying and localizing the root cause.
Interactive VA systems for Diagnosing and Refining DL Algorithms and Models. Many VA
systems (VAS) provide a model’s diagnosis that allow detecting issues on different abstraction
levels. Some of them focus on feature importance and model behaviors against real [98] or adver-
sarial [62] examples; others focus on neuron activations [48]. Moreover, advanced visualization
systems [63] [79] go beyond the diagnosis of the DNN and propose refinements required to over-
come the detected issues. For instance, Liu et al. [63] constructed an interactive VAS, CNNvis, that
extracts successive snapshots of the on-training CNN and analyzes it in-depth using rectangle
packing, matrix ordering, and biclustering-based edge bundling in order to cluster the neurons,
their interactions, their derived features and roles in relation with the target task. Instead of con-
ducting offline diagnosis, Pezzotti et al. [79] proposed an online progressive VAS that provides
continuous live feedback on the on-training DNN. Both of these previous works on diagnosis and
refinement via visualization demonstrate how rich visual insights can be interpreted by an expert
to identify possible modeling issues and make decisions about DNN’s refinements. Nevertheless,
diagnostics via VAS are interactive sessions that require DL engineers to select components to
watch beforehand. This makes diagnosis via VAS an expensive process that focuses, particularly,
on data and design improvements to enhance the performance results of the trained DNN. In this
paper, we propose TheDeepChecker, an end-to-end automated debugging approach that is able to
detect several implementation bugs and system misconfigurations, as well as poor design choices,
with minimum human intervention.

9 CONCLUSION
This paper reports about the design and implementation of TheDeepChecker, an end-to-end auto-
mated debugging approach for DNN training programs. To develop TheDeepChecker, we systemati-
cally gather a catalog of pitfalls commonly occurring in the development of DNN training programs.
Then, we explore various resources on applied DL researches and technical reports with aim of
distilling fundamental properties and practical heuristics that can be codified into verification
routines to detect the DL pitfalls’ resulting faults and training issues. Next, we develop a property-
based debugging approach, named TheDeepChecker, that orchestrates the different properties’
verification over multiple phases. On the one hand, we evaluate TheDeepChecker on synthetic buggy
programs that contain each an injected DL fault. The results show its effectiveness at detecting
DL coding bugs and misconfigurations with (precision, recall), respectively, equal to (90%, 96.4%)
and (77%, 83.3%). Moreover, we compare TheDeepChecker with Amazaon Sagemaker Rule-based
Debugger(SMD) on real-world buggy programs extracted from SO and GH. The results show that
TheDeepChecker outperforms SMD by detecting 75% rather than 60% of the total of reported bugs in
the SO post accepted answer or the bug-fixing commit message. Indeed, TheDeepChecker effectively
captured the slightest violation of all mandatory training assumptions, even those having only a
minor negative effects on the training process, providing sufficient feedback on any problematic
issue in DNN program. Using TheDeepChecker, two DL engineers were able to successfully locate
and fix 93.33% of bugs contained in 10 buggy TF programs. In the future works, we plan extent
TheDeepChecker to include code recipes (i.e., generic snippet of code, one or multiple lines of code,
that could characterize a particular fix of a coding bug, an ineffective implementation or a misuse
of APIs.) for common errors in relation to training anomalies with the aim to point DL engineer to
the exact lines of code that caused the issues and to include an automatic generation of fixes for
these detected issues.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:58

Ben Braiek and Khomh

ACKNOWLEDGMENTS
This work is partly funded by the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds de Recherche du Quebec (FRQ).

REFERENCES
[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael
Isard, Yangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga,
Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Distributed Systems. CoRR abs/1603.04467 (2016).

[2] Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha. 2019. Black box fairness testing
of machine learning models. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. ACM, 625–635.

[3] Kian Katanforoosh Andrew Ng and Younes Bensouda Mourri. 2020. Visualization: How to visualize, monitor and debug

neural network learning. https://www.coursera.org/learn/deep-neural-network

[4] Thomas J Archdeacon. 1994. Correlation and regression analysis: a historian’s guide. Univ of Wisconsin Press.
[5] Martin Arjovsky and Léon Bottou. 2017. Towards principled methods for training generative adversarial networks.

arXiv preprint arXiv:1701.04862 (2017).

[6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450

(2016).

[7] Ho Bae, Jaehee Jang, Dahuin Jung, Hyemi Jang, Heonseok Ha, and Sungroh Yoon. 2018. Security and privacy issues in

deep learning. arXiv preprint arXiv:1807.11655 (2018).

[8] Pierre Baldi and Peter J Sadowski. 2013. Understanding dropout. In Advances in neural information processing systems.

2814–2822.

[9] Dishank Bansal. 2020.

for Training Net-
https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-

Pitfalls of Batch Norm in TensorFlow and Sanity Checks

works.
networks-e86c207548c8

[10] Earl T Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2014. The oracle problem in software

testing: A survey. IEEE transactions on software engineering 41, 5 (2014), 507–525.

[11] Houssem Ben Braiek and Foutse Khomh. 2019. DeepEvolution: A Search-Based Testing Approach for Deep Neural

Networks. In 2019 IEEE International Conference on Software Maintenance and Evolution. IEEE.
[12] Houssem Ben Braiek and Houssem Khomh. 2018. On Testing Machine Learning Programs.

arXiv preprint

arXiv:1812.02257 (2018).

[13] Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. 2018. Understanding batch normalization. In

Advances in Neural Information Processing Systems. 7694–7705.

[14] Leon Bottou. 2015. Multilayer Neural Networks. http://videolectures.net/site/normal_dl/tag=983658/deeplearning2015_

bottou_neural_networks_01.pdf

[15] Houssem Ben Braiek, Foutse Khomh, and Bram Adams. 2018. The open-closed principle of modern machine learning

frameworks. In 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). IEEE, 353–363.

[16] Shanqing Cai, Eric Breck, Eric Nielsen, Michael Salib, and D Sculley. 2016. Tensorflow debugger: Debugging dataflow
graphs for machine learning. In Proceedings of the Reliable Machine Learning in the Wild-NIPS 2016 Workshop.
[17] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. 2016. An analysis of deep neural network models for practical

applications. arXiv preprint arXiv:1605.07678 (2016).

[18] Reddit DL community. 2018. My Neural Network isn’t working! What should I do?

https://www.reddit.com/r/

MachineLearning/comments/6xvnwo/d_my_neural_network_isnt_working_what_should_i_do/

[19] Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen,
Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. 2020. Underspecification presents challenges for
credibility in modern machine learning. arXiv preprint arXiv:2011.03395 (2020).

[20] Deeplearning4j. 2019. Troubleshooting Neural Net Training. https://deeplearning4j.konduit.ai/tuning-and-training/

troubleshooting-training

[21] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml
[22] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M Rao, RP Bose, Neville Dubash, and Sanjay Podder.
2018. Identifying implementation bugs in machine learning based image classifiers using metamorphic testing. In
Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 118–128.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:59

[23] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M Rao, RP Bose, Neville Dubash, and Sanjay Podder.
2018. Identifying implementation bugs in machine learning based image classifiers using metamorphic testing. In
Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 118–128.
[24] Michael D Ernst, Jake Cockrell, William G Griswold, and David Notkin. 2001. Dynamically discovering likely program

invariants to support program evolution. IEEE Transactions on Software Engineering 27, 2 (2001), 99–123.
[25] Utku Evci. 2018. Detecting Dead Weights and Units in Neural Networks. arXiv preprint arXiv:1806.06068 (2018).
[26] Python Software Foundation. 2013. MutPy 0.4.0. https://pypi.python.org/pypi/
[27] Yarin Gal and Zoubin Ghahramani. 2015. Bayesian Convolutional Neural Networks with Bernoulli Approximate Varia-
tional Inference. arXiv e-prints, Article arXiv:1506.02158 (Jun 2015), arXiv:1506.02158 pages. arXiv:stat.ML/1506.02158
[28] Christian Garbin, Xingquan Zhu, and Oge Marques. 2020. Dropout vs. batch normalization: an empirical study of their

impact to deep learning. Multimedia Tools and Applications (2020), 1–39.

[29] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In

Proceedings of the thirteenth international conference on artificial intelligence and statistics. 249–256.

[30] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.

org.

[31] Google. 2017. Basic regression: Predict fuel efficiency. https://www.tensorflow.org/tutorials/keras/regression
[32] Google. 2020. Google machine learning crash course. https://developers.google.com/machine-learning/crash-course
[33] Roger Grosse. 2017. Lecture 15: Exploding and vanishing gradients. University of Toronto Computer Science (2017).
[34] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. DLFuzz: differential fuzzing testing of deep
learning systems. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. ACM, 739–743.

[35] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA

data mining software: an update. ACM SIGKDD explorations newsletter 11, 1 (2009), 10–18.

[36] Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou, and Ehsan Adeli. 2018.
Towards principled design of deep convolutional networks: introducing SimpNet. arXiv preprint arXiv:1802.06205
(2018).

[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision.
1026–1034.

[38] Geoffrey E Hinton. 2012. A practical guide to training restricted Boltzmann machines. In Neural networks: Tricks of the

trade. Springer, 599–619.

[39] Fred Matthew Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau. 2018. Visual analytics in deep learning:
An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics (2018).
[40] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea Stocco, and Paolo Tonella. 2020.
Taxonomy of real faults in deep learning systems. In Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering. 1110–1121.

[41] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).

[42] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A comprehensive study on deep learning bug
characteristics. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 510–520.

[43] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A comprehensive study on deep learning bug
characteristics. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 510–520.

[44] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repairing Deep Neural Networks: Fix Patterns
and Challenges. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE ’20).
1135–1146.

[45] Slav Ivanov. 2017. 37 Reasons why your Neural Network is not working. https://blog.slavv.com/37-reasons-why-your-

neural-network-is-not-working-4020854bd607

[46] Yue Jia and Mark Harman. 2011. An analysis and survey of the development of mutation testing. IEEE transactions on

software engineering 37, 5 (2011), 649–678.

[47] Paul Dubs Jing Zhi Loh and Shams UI Azeem. 2020. Visualization: How to visualize, monitor and debug neural network

learning. https://deeplearning4j.konduit.ai/tuning-and-training/visualization

[48] Minsuk Kahng, Pierre Y Andrews, Aditya Kalro, and Duen Horng Polo Chau. 2017. A cti v is: Visual exploration of
industry-scale deep neural network models. IEEE transactions on visualization and computer graphics 24, 1 (2017),
88–97.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

XX:60

Ben Braiek and Khomh

[49] Andrej Karpathy. 2018. Most common neural net mistakes (tweet).

https://twitter.com/karpathy/status/

1013244313327681536?lang=en

[50] Andrej Karpathy. 2018. Neural Networks Part 3: Learning and Evaluation. http://cs231n.github.io/neural-networks-3/
[51] Andrej Karpathy. 2020. A Recipe for Training Neural Networks. http://karpathy.github.io/2019/04/25/recipe/
[52] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality,

stability, and variation. arXiv preprint arXiv:1710.10196 (2017).

[53] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017. Towards proving the adversarial

robustness of deep neural networks. arXiv preprint arXiv:1709.02802 (2017).

[54] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. 2019. Similarity of neural network

representations revisited. arXiv preprint arXiv:1905.00414 (2019).

[55] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2014. The cifar-10 dataset. http://www.cs.toronto.edu/kriz/cifar.html

(2014).

[56] Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/ (1998).
[57] Yann LeCun et al. 2015. LeNet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet 20 (2015), 5.
[58] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document

recognition. Proc. IEEE 86, 11 (1998), 2278–2324.

[59] Yann A LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. 2012. Efficient backprop. In Neural networks:

Tricks of the trade. Springer, 9–48.

[60] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. Visualizing the loss landscape of neural

nets. In Advances in Neural Information Processing Systems. 6389–6399.

[61] Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. 2019. Understanding the disharmony between dropout and batch
normalization by variance shift. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2682–2690.

[62] Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, and Jun Zhu. 2018. Analyzing the noise robustness of deep neural

networks. In 2018 IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE, 60–71.

[63] Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, and Shixia Liu. 2016. Towards better analysis of deep
convolutional neural networks. IEEE transactions on visualization and computer graphics 23, 1 (2016), 91–100.
[64] Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and Fuad E Alsaadi. 2017. A survey of deep neural

network architectures and their applications. Neurocomputing 234 (2017), 11–26.

[65] Lu Lu, Yeonjong Shin, Yanhui Su, and George Em Karniadakis. 2019. Dying relu and initialization: Theory and

numerical examples. arXiv preprint arXiv:1903.06733 (2019).

[66] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, et al.
2018. DeepGauge: multi-granularity testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. ACM, 120–131.

[67] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al.

2018. DeepMutation: Mutation Testing of Deep Learning Systems. arXiv preprint arXiv:1805.05206 (2018).

[68] Yu-Seung Ma, Jeff Offutt, and Yong Rae Kwon. 2005. MuJava: an automated class mutation system. Software Testing,

Verification and Reliability 15, 2 (2005), 97–133.

[69] MIT. 2020. CS231n: Convolutional Neural Networks for Visual Recognition. https://cs231n.github.io/
[70] MIT. 2020. MIT deep learning lectures. https://deeplearning.mit.edu/
[71] Ajinkya More. 2016. Survey of resampling techniques for improving classification performance in unbalanced datasets.

arXiv preprint arXiv:1608.06048 (2016).

[72] Andrew Ng, Kian Katanforoosh, and Younes Bensouda Mourri. 2020. Specialization in Deep Learning. https://www.

coursera.org/specializations/deep-learning

[73] Augustus Odena and Ian Goodfellow. 2018. TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing.

arXiv preprint arXiv:1807.10875 (2018).

[74] Zoe Paraskevopoulou, Cătălin Hriţcu, Maxime Dénès, Leonidas Lampropoulos, and Benjamin C Pierce. 2015. Founda-

tional property-based testing. In International Conference on Interactive Theorem Proving. Springer, 325–343.

[75] Dae Hoon Park, Chiu Man Ho, Yi Chang, and Huaqing Zhang. 2018. Gradient-Coherent Strong Regularization for

Deep Neural Networks. arXiv preprint arXiv:1811.08056 (2018).

[76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In

International conference on machine learning. PMLR, 1310–1318.

[77] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Automated whitebox testing of deep learning

systems. In Proceedings of the 26th Symposium on Operating Systems Principles. ACM, 1–18.

[78] Luis Perez and Jason Wang. 2017. The effectiveness of data augmentation in image classification using deep learning.

arXiv preprint arXiv:1712.04621 (2017).

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

Testing Feedforward Neural Networks Training Programs

XX:61

[79] Nicola Pezzotti, Thomas Höllt, Jan Van Gemert, Boudewijn PF Lelieveldt, Elmar Eisemann, and Anna Vilanova. 2017.
Deepeyes: Progressive visual analytics for designing deep neural networks. IEEE transactions on visualization and
computer graphics 24, 1 (2017), 98–108.

[80] A. Rakitianskaia and A. Engelbrecht. 2015. Measuring Saturation in Neural Networks. In 2015 IEEE Symposium Series

on Computational Intelligence. 1423–1430.

[81] Nathalie Rauschmayr, Vikas Kumar, Rahul Huilgol, Andrea Olgiati, Satadal Bhattacharjee, Nihal Harish, Vandana
Kannan, Amol Lele, Anirudh Acharya, Jared Nielsen, et al. 2021. Amazon SageMaker Debugger: A System for Real-Time
Insights into Machine Learning Model Training. Proceedings of Machine Learning and Systems 3 (2021).

[82] Chase Roberts. 2018. How to unit test machine learning code. https://medium.com/@keeper6928/how-to-unit-test-

machine-learning-code-57cf6fd81765

[83] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. 2017. Stabilizing training of generative

adversarial networks through regularization. arXiv preprint arXiv:1705.09367 (2017).

[84] Tim Salimans and Diederik P Kingma. 2016. Weight normalization: A simple reparameterization to accelerate training

of deep neural networks. arXiv preprint arXiv:1602.07868 (2016).

[85] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2018. How does batch normalization help

optimization?. In Advances in Neural Information Processing Systems. 2483–2493.

[86] Daniel Selsam, Percy Liang, and David L Dill. 2017. Developing bug-free machine learning systems with formal

mathematics. arXiv preprint arXiv:1706.08605 (2017).

[87] Claude E Shannon. 1948. A mathematical theory of communication. The Bell system technical journal 27, 3 (1948),

379–423.

[88] Cecelia Shao. 2019. Checklist for debugging neural networks. https://towardsdatascience.com/checklist-for-debugging-

neural-networks-d8b2a9434f21

[89] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556 (2014).

[90] Leslie N Smith and Nicholay Topin. 2016. Deep convolutional neural network design patterns. arXiv preprint

arXiv:1611.00847 (2016).

[91] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple

way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.

[92] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 1–9.

[93] AWS Team. [n.d.]. .
[94] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated testing of deep-neural-network-
driven autonomous cars. In Proceedings of the 40th International Conference on Software Engineering. ACM, 303–314.
[95] Brendan Van Rooyen and Robert C Williamson. 2017. A Theory of Learning with Corrupted Labels. Journal of Machine

Learning Research 18 (2017), 228–1.

[96] Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and Tsong Yueh Chen. 2011. Testing and
validating machine learning classifiers by metamorphic testing. Journal of Systems and Software 84, 4 (2011), 544–558.
[97] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Hongxu Chen, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, Jianxiong Yin, and
Simon See. 2018. Coverage-Guided Fuzzing for Deep Neural Networks. arXiv preprint arXiv:1809.01266 (2018).
[98] Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and David S Ebert. 2018. Manifold: A model-agnostic framework for
interpretation and diagnosis of machine learning models. IEEE transactions on visualization and computer graphics 25,
1 (2018), 364–373.

[99] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang. 2018. An empirical study on TensorFlow
program bugs. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis.
129–140.

J. ACM, Vol. XX, No. XX, Article XX. Publication date: April 2020.

