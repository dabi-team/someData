Noname manuscript No.
(will be inserted by the editor)

PREPRINT: Do I really need all this work to ﬁnd
vulnerabilities?
An empirical case study comparing vulnerability detection
techniques on a Java application

Sarah Elder · Nusrat Zahan · Rui Shu ·
Monica Metro · Valeri Kozarev ·
Tim Menzies · Laurie Williams

Received: date / Accepted: date

Abstract Context: Applying vulnerability detection techniques is one of many
tasks using the limited resources of a software project.
Objective: The goal of this research is to assist managers and other decision-
makers in making informed choices about the use of software vulnerability detec-
tion techniques through an empirical study of the eﬃciency and eﬀectiveness of
four techniques on a Java-based web application.
Method: We apply four diﬀerent categories of vulnerability detection techniques
– systematic manual penetration testing (SMPT), exploratory manual penetra-
tion testing (EMPT), dynamic application security testing (DAST), and static
application security testing (SAST) – to an open-source medical records system.
Results:We found the most vulnerabilities using SAST. However, EMPT found
more severe vulnerabilities. With each technique, we found unique vulnerabilities
not found using the other techniques. The eﬃciency of manual techniques (EMPT,
SMPT) was comparable to or better than the eﬃciency of automated techniques
(DAST, SAST) in terms of Vulnerabilities per Hour (VpH).
Conclusions: The vulnerability detection technique practitioners should select
may vary based on the goals and available resources of the project. If the goal of
an organization is to ﬁnd “all” vulnerabilities in a project, they need to use as
many techniques as their resources allow.

Keywords Vulnerability Management · Web Application Security · Penetration
Testing · Vulnerability Scanners

S. Elder
North Carolina State University (NCSU)
Department of Computer Science
E-mail: seelder@ncsu.edu

L. Williams
North Carolina State University (NCSU)
Department of Computer Science
College of Engineering
890 Oval Drive
Engineering Building II
Raleigh, NC 27695
E-mail: laurie williams@ncsu.edu

2
2
0
2

g
u
A
2

]
E
S
.
s
c
[

1
v
5
9
5
1
0
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

1 Introduction

Elder et al.

Detecting software vulnerabilities eﬃciently and eﬀectively is necessary to reduce
the risk that hackers will exploit vulnerabilities before developers can ﬁnd and
patch them. However, as noted by Alomar et al. [2], security teams often struggle
to justify the costs of vulnerability detection and other vulnerability management
activities. This need to improve vulnerability detection eﬀorts while not expend-
ing unnecessary resources is highlighted in Section 7 of U.S. Presidential Executive
Order 14028, which begins “The Federal Government shall employ all appropri-
ate resources and authorities to maximize the early detection of cybersecurity vul-
nerabilities...” [28]. The executive order also emphasizes the need for improved
evaluation of security practices, including vulnerability detection.

The goal of this research is to assist managers and other decision-makers in
making informed choices about the use of software vulnerability detection tech-
niques through an empirical study of the eﬃciency and eﬀectiveness of four tech-
niques on a Java-based web application. We perform a theoretical replication1 of
work done by Austin et al. [7, 8]. Since 2011 when the original Austin et al. work
was published, the vulnerability detection landscape has changed from the types
of applications being tested to the types of vulnerabilities found [73, 72, 71, 70].
For example, the number of vulnerabilities in the United States National Vulner-
ability Database assigned to Cross Site Scripting (XSS) has increased faster than
prevalence of other vulnerability types such as Code Injection [63]. Our method-
ology and ﬁndings may also be useful to future evaluations of new vulnerability
detection techniques being introduced.

We examined the 4 vulnerability detection techniques from Austin et al. [7, 8].

– Systematic Manual Penetration Testing (SMPT): the analyst manually
and systematically develops, documents, then executes test cases which verify
the security objectives of the System Under Test (SUT) [90, 7, 89, 8]

– Exploratory Manual Penetration Testing (EMPT): the analyst “spon-
taneously designs and executes tests based on the [analyst]’s existing relevant
knowledge” [40], searching for vulnerabilities.

– Dynamic Application Security Testing (DAST): automatic tools gener-
ate and run tests based on security principles, without access to source code[88].
– Static Application Security Testing (SAST): automatic tools scan source

code for patterns that indicate vulnerabilities [20, 38, 87].

These four techniques can all be applied during and after software implementa-
tion. Applying vulnerability detection during and after the implementation phase
of software development is more common in many industry settings [20] when
compared to security testing techniques which focus on earlier phases of software
development such as requirements or model-focused testing. We used an industry
standard, the Open Web Application Security Project’s Application Security Veri-
ﬁcation Standard (OWASP ASVS), to systematically develop test cases for SMPT.
The two DAST tools and two SAST tools are currently used in industry settings.
Two of these tools, the OWASP Zed Attack Proxy (OWASP ZAP)2 DAST tool

1A theoretical replication seeks to investigate the scope of the underlying theory, for ex-
ample by redesigning the study for a diﬀerent target population, or by testing a variant of the
original hypothesis of the work [52]

2https://owasp.org/www-project-zap/

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

3

and the Sonarqube3 SAST tool, are open-source. The other tools, which we will
refer to as DAST-2 and SAST-2, are proprietary.

We applied each technique to OpenMRS (https://openmrs.org/), a large
open source medical records system used both in medical research and clinical
settings throughout the world. OpenMRS is a web application written in Java and
JavaScript, containing 3,985,596 lines of code4. We consider our work to be a case
study, since we only examine a single System Under Test (SUT).

The only previous work comparing as many diﬀerent types of vulnerability
detection techniques we are aware of is by Austin et al. [7, 8] published in 2011 and
2013. The systems in those studies were less than 500,000 lines of code. We know
of no other study applying multiple diﬀerent vulnerability detection techniques
to a system this large. Collecting data for this study required a team of four
graduate students a combined eleven months of full-time work and twenty months
of part-time work; four months part-time work from an undergraduate student;
and the results of assignments from a large graduate-level software security course.
Our experiences in structuring the software security course have been reported
previously in Elder et al.[26].

We answer the following research questions:

– RQ1:What is the eﬀectiveness, in terms of number and type of vulnerabilities,

for each technique?

– RQ2: How does the reported eﬃciency in terms of vulnerabilities per hour

diﬀer across techniques?

As part of the software security course, students were asked to discuss and compare
the four techniques. Two researchers performed qualitative analysis on the answers,
addressing the following research question:

– RQ3: What other factors should we consider when comparing techniques?

Our research makes the following contributions:

– Analysis from our comparison of the eﬃciency and eﬀectiveness of the four

vulnerability detection techniques.

– A detailed description of the methodology and related ﬁndings, which may be

useful for future comparisons of vulnerability detection techniques

We are releasing our vulnerability dataset once the vulnerabilities are safely miti-
gated at https://github.com/RealsearchGroup/vulnerability-detection-20.
The rest of this paper is structured as follows. In Section 2, we provide expla-
nations for key concepts used throughout this paper. In Section 3 we provide a
brief overview of the previous work. We discuss other related work in Section 4.
In Section 5 we describe the vulnerability detection techniques used in this paper.
In Section 6 we discuss the SUT used in our Case Study, OpenMRS. In Section 7
we discuss the sources of data for the Case Study. In Sections 8, 9, and 10 we
outline our research methodology for RQ1, RQ2, and RQ3 respectively. Once the
methodology is explained we discuss the equipment used in Section 11. We re-
port our results in Section 12. We discuss our ﬁndings in Section 14. We discuss
limitations of our study in Section 13. We discuss the ﬁndings in Section 14 and
conclude with Section 15.

3https://www.sonarqube.org/
4as measured by CLOC v1.74 (https://github.com/AlDanial/cloc)

4

2 Key Concepts

Elder et al.

In this section, we deﬁne key concepts used in this paper.

Vulnerability: We use the deﬁnition of vulnerability from the U.S. National
Vulnerability Database5. Speciﬁcally, a vulnerability is “A weakness in the
computational logic (e.g., code) found in software and hardware components
that, when exploited, results in a negative impact to conﬁdentiality, integrity,
or availability.’ [65].

Common Weakness Enumeration (CWE): Per the CWE website, “CWE is
a community-developed list of software and hardware weakness types.”[57].
Many security tools, such as the OWASP Application Security Veriﬁcation
Standard (ASVS) and most vulnerability detection tools, use CWEs to iden-
tify the types of vulnerabilities relevant to a security requirement, test case, or
tool alert. We use the CWE list in this paper to standardize and compare the
vulnerability types found by diﬀerent vulnerability detection techniques.

OWASP Top Ten: The OWASP Top Ten is a regularly updated list of “the
most critical security risks to web applications.”[74]. The OWASP Top Ten cat-
egories and ranking are is developed by security experts based on the incidence
and severity of vulnerabilities associated with diﬀerent CWEs. A convenient
mapping[58] allows for vulnerabilities to be mapped from CWEs to OWASP
Top Ten categories. We use the OWASP Top Ten in this paper to summa-
rize the types vulnerabilities found, and to understand the relative severity
of the vulnerabilities found. The latest (2021) Top Ten, which were used in
our analysis, are: A01 - Broken Access Control, A02 - Cryptographic Fail-
ures, A03 - Injection, A04 - Insecure Design, A05 - Security Miscoﬁguration,
A06 - Vulnerable and Outdated Components, A07 - Identiﬁcation and Au-
thentication Failures, A08 - Software and Data Integrity Failures, A09 - Secu-
rity Logging and Monitoring Failures, and A10 - Server-Side Request Forgery
(SSRF. Additional information on the OWASP Top Ten may be found at
https://owasp.org/Top10/.

OWASP Application Security Veriﬁcation Standard (ASVS): OWASP
ASVS is an open standard for performing web application security veriﬁcation.
The ASVS provides a high-level set of “requirements or tests that can be used
by architects, developers, testers, security professionals, tool vendors, and con-
sumers to deﬁne, build, test and verify secure applications” [95]. In the ASVS,
each requirement or test is referred to as a “control” and must be adapted
to a SUT. Each OWASP ASVS control is mapped to a CWE type. We used
OWASP ASVS version 4.0.1 released in March 20196, which was the current
version when we began collecting data in Spring 2020. The OWASP ASVS has
three levels of requirements. If a requirement falls within a level, it also falls
within higher levels. ASVS describes Level 1 as “the bare minimum that any
application should strive for ” [95].

5https://nvd.nist.gov/vuln
6https://github.com/OWASP/ASVS/tree/v4.0.1

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

5

3 Previous Work by Austin et al.

Our study is a theoretical replication7 of previous work done by Austin et al. [7, 8].
The goals of the previous work are “to improve vulnerability detection by compar-
ing the eﬀectiveness of vulnerability discovery techniques and to provide speciﬁc
recommendations to improve vulnerability discovery with these techniques”[7]. In
their ﬁrst study [7] Austin et al. applied SMPT, EMPT, DAST, and SAST to
two electronic medical records systems, Tolven Electronic Clinician Health Record
(eCHR), a Java-based application with 466,538 lines of code, and OpenEMR, a
PHP-based application with 277,702 lines of code. The second paper by Austin
et al [8] added a third SUT, PatientOS, a Java-based mobile application with
487,437 lines of code. In both studies, the authors used one tool for each auto-
mated technique. The DAST tool used by Austin et al. was only applicable to
web applications. Consequently, they only applied SMPT, EMPT, and SAST to
PatientOS.

Austin et al. calculated how long it took to apply each technique in terms of
the number of hours. For each SUT, the authors compare the number and types
of vulnerabilities found by each technique, as well as the rate of vulnerabilities
per hour for each technique. In the current study, we examine these same metrics,
referring to the number and types of vulnerabilities as “eﬀectiveness” and the
vulnerabilities per hour as “eﬃciency”.

Austin et al. found that SAST identiﬁed the most vulnerabilities [7, 8]. They
emphasize that the vulnerabilities may not be as exploitable as vulnerabilities
found using other techniques, and relying on SAST alone would be insuﬃcient
since each technique found vulnerabilities not found by other techniques. The
authors also found automated tools to be faster in terms of vulnerabilities per
hour, although automated techniques found fewer types of vulnerabilities.

4 Related Work

Several related studies have focused on a single category of techniques, such as
comparisons of DAST tools or comparisons of SAST tools. In 2010, Doup´e et
al. [25] compared eleven “point and click” DAST tools to each other. The authors
found that while some types of vulnerabilities could be found reliably, other types
of vulnerabilities could not be found by the tools examined in the study. More re-
cently, Klees et al. [49] performed a rigorous comparison of DAST tools, providing
insights on the biases and limitations of DAST tool studies. They examined 32 pa-
pers on fuzz testing and performed an experiment comparing two tools against ﬁve
benchmark applications. The U.S. National Institute of Standards and Technology
(NIST) Software Assurance Metrics and Tool Evaluation (SAMATE) program has
performed a series of Static Analysis Tool Expositions (SATE) [23, 69, 68, 66]. On
a regular basis, the SAMATE program establishes a set of trials which they refer
to as tests or test cases. For example, a test case may require running the tool to
be evaluated against a benchmark. Organizations which the SATE report authors
refer to as “toolmakers” [23] sign up to participate in the experiment and run

7A theoretical replication seeks to investigate the scope of the underlying theory,for example
by redesigning the study for a diﬀerent target population, or by testing a variant of the original
hypothesis of the work [52]

6

Elder et al.

their tools against the trials. The results were reviewed by SAMATE organizers.
These studies, particularly the experiments run by Klees et al. and the SAMATE
program, inform our methodology for SAST and DAST techniques but diﬀer from
our work in that they make comparisons between tools of a similar type and do
not examine manual techniques. These are merely examples, as there are many
more comparison studies within a single technique[3, 11]

Another common comparison between vulnerability detection techniques is be-
tween static techniques that primarily analyze source code, and techniques that
are based on interactions with the running software system without access to the
source code (Dynamic or DAST). Studies that compared static and dynamic anal-
yses include a controlled experiment by Scandariato et al. [87], which compared the
use of SAST with the use of DAST. Scandariato et al. conducted an experiment
in which nine participants performed vulnerability detection tasks. The authors
examine the user experience of SAST and DAST tools, and analyze the eﬃciency
of using SAST and DAST. Scandariato et al. found that although participants
found DAST tools more “fun” to use, the participants were more eﬃcient with
SAST tools and considered SAST tools a better starting point for new security
teams. Similarly, in 2009, prior to the previous work by Austin et al., Antunes
and Viera [5] performed a comparison between SAST and DAST tools. Similar
to our study, Antunes and Viera found that diﬀerent tools within the same tech-
nique found diﬀerent vulnerabilities, and that SAST found more vulnerabilities
than DAST. In contrast with Scandariato et al. and Antunes and Viera, we fur-
ther subdivide Dynamic analyses into SMPT, EMPT, and DAST, exploring each
of these techniques separately.

Similarly, many surveys and comparisons exist which focus on a single type
of vulnerability. For example, Chaim et al. perform a survey of Buﬀer Overﬂow
Detection techniques. They note that existing vulnerability detection techniques
are either impractical or have a high false positive rate, but that emerging Hy-
brid techniques are “promising”. Liu et al[50] perform a survey of automated,
state-of-the-art techniques for ﬁnding and exploiting Cross-Site-Scripting (XSS)
vulnerabilities, categorizing them as “static”, “dynamic”, or “hybrid”. They note
that the increasing size of web applications may be hindering the eﬀectiveness of
these automated techniques, but do not perform an empirical comparison. Fon-
seca et al. perform an empirical comparison of the eﬀectiveness of diﬀerent DAST
tools [34] for ﬁnding XSS vulnerabilities. While practitioners may prioritize some
types of vulnerabilities over others and these studies may assist practitioners in
understanding how vulnerability detection techniques compare against a single
type of vulenrability. However, applications are rarely threatened only by a single
type of vulnerability. Similar to the studies of a single type of detection technique;
our study, which examines the eﬀectiveness of techniques across a range of vulner-
ability types; gains insight from and provides additional insights into results from
studies which focus on a single type of vulnerability.

An additional area of related work is the development and application of bench-
marks for security testing tools, such as the 2010 work by Antunes and Viera [6]
on developing a benchmark for SAST and DAST tools. As noted in the SATE V
report [23], benchmark studies have an important role in evaluating security test-
ing techniques. The use of vulnerability detection techniques in benchmark studies
may diﬀer from how security vulnerability detection techniques would be applied
in practice. The three web infrastructure performance benchmark systems used by

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

7

Antunes and Viera[6] to develop security benchmarks contained a combined 2,654
lines of code which could be manually reviewed by security experts in a reasonable
amount of time. The results of our study on OpenMRS, which has over 3,000,000
lines of code, may not generalize to smaller systems such as those examined in
the Antunes and Viera study. However, our results suggest that for larger software
systems; alerts, failing test cases, or other outputs of a vulnerability detection tech-
nique which appear to be “false positives” when compared against a benchmark
should be examined with care to ensure that they are not true positives not found
by the techniques used to create the benchmark. We encourage any future users
of the set of vulnerabilities and test cases that were generated for this study to be
aware that applying additional techniques and tools to the same SUT is likely to
ﬁnd additional vulnerabilities.

5 Vulnerability Detection Techniques

We begin this section by explaining analysis types which are frequently used to dis-
tinguish between vulnerability detection techniques. We then describe the speciﬁc
types of techniques from our case study. We diﬀerentiate between analysis type
and detection technique since many common names for vulnerability detection
techniques are derived from the analysis types that are part of the technique. For
example, Dynamic Application Security Testing (DAST) is a common name for a
category of vulnerability detection techniques where analysts use automated tools
to perform dynamic analysis. However, EMPT and SMPT also involve dynamic
analysis.

5.1 Analysis Types

In this section, we explain automated and manual analysis, static and dynamic
analysis, source code analysis, as well as systematic and exploratory analysis.

Automated vs Manual analysis: Some techniques are based on automated
analysis performed by a tool. Manual eﬀort may be required to use vulnerability
detection tools. However, for the purpose of this study we reserve the phrase
manual analysis to describe techniques where no automated tool is needed.

Static vs Dynamic analysis: Static analysis is performed on artifacts such
as source code or binaries, where the source code or binary is not executed [40].
Dynamic analysis is any form of analysis that does require code to be executed [40].
Source code analysis: Source code analysis is any form of analysis that re-
quires access to source code. While source code analysis is sometimes used as a
synonym of static analysis, static analysis and source code analysis are distinct
concepts [54, 8]. For example, static analysis can include analyzing binaries and
other artifacts that are not source code. Analysis which does not have access to
source code is sometimes referred to as “black box” analysis.

Systematic vs Exploratory analysis: Systematic analysis is performed in a
very prescriptive, methodical manner; in contrast with exploratory analysis which
is less formally planned. For example, ISO 29119 [40] deﬁnes exploratory testing,
a form of exploratory analysis, as “experience-based testing in which the [analyst]
spontaneously designs and executes tests based on the [analyst]’s existing relevant

8

Elder et al.

knowledge, prior exploration of the test item (including the results of previous
tests), and heuristic ‘rules of thumb’ regarding common software behaviours and
types of failure”. The concepts of systematic and exploratory analysis primarily ap-
ply to manual analysis. Whether an automated tool has knowledge and experience
is a philosophical discussion outside the scope of this paper.

5.2 Case Study Techniques

In this section, we provide greater detail on the four categories of vulnerability
detection techniques we examine in our case study. For automated techniques, we
examine two tools for each category.

5.2.1 Manual Techniques

Both manual techniques examined in this study are dynamic techniques that do
not have access to source code. Manually examining the entire source code for
system as large as OpenMRS is infeasible. A high-level overview of how man-
ual dynamic testing techniques, particularly systematic techniques, are applied is
shown in Figure 1. This ﬁgure is based on the process for dynamic techniques
presented in ISO/IEC/IEEE 29119-1 [40].

Fig. 1: Applying Manual Test Techniques (based on ISO/IEC/IEEE 29119-1)

5.2.1.1 Systematic Manual Penetration Testing (SMPT) SMPT involves dynamic,
manual, and systematic analysis. Speciﬁcally, SMPT is a form of scripted testing
deﬁned by ISO 29119-1 [40] as “dynamic testing in which the [analyst]’s actions are
prescribed by written instructions in a test case”. SMPT does not require access
to source code. In SMPT, the analyst begins by writing a set of test cases and
planning how the test suite will be run for a particular test execution in what
29119-1 refers to as the in the Test Design & Implementation stage, as shown in
Figure 1. Many test cases are combined into a test plan. The number of test cases
depends on the system and scope of the testing being performed. The tests are
then executed. In the ﬁnal stage, the test results are documented and reported.

Figure 2 shows an example SMPT test case from our case study. As we can
see in the ﬁgure, the steps recorded in an SMPT test case are the actions a person
would take when interacting with the system. As we will discuss further in our
methodology in Section 8.1.2.1 and indicated in the Figure, we used the ASVS,
described in Section 2 as the basis for our test cases. The test case in Figure 2,
is based on ASVS Control 2.1.7 - Verify that passwords submitted during account

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

9

registration, login, and password change are checked against a set of breached pass-
words either locally (such as the top 1,000 or 10,000 most common passwords which
match the system’s password policy) or using an external API. In the test case, the
administrator attempts to create a user with the common password “Passw0rd”.

Test Case ID : XXX
ASVS C o n t r o l : 2 . 1 . 7

S t e p s :
0 1 ) Open t h e OpenMRS web app t o t h e l o g i n s c r e e n
0 2 ) Type ‘ ‘ admin ’ ’ a s
0 3 ) S e l e c t
0 4 ) C l i c k ‘ ‘ l o g i n ’ ’
0 5 ) S e l e c t
’ ’ .

‘ ‘ System A d m i n i s t r a t i o n ’ ’ ,

‘ ‘ I n p a t i e n t Ward ’ ’ a s

t h e l o c a t i o n

then s e l e c t

t h e username and ‘ ‘ Admin123 ’ ’ a s

t h e password

‘ ‘ Manage Accounts

0 6 ) C l i c k ‘ ‘ Add New Account ’ ’
0 7 ) Enter

t h e f o l l o w i n g i n f o r m a t i o n :

Family Name : P o t t e r
Given Name : Harry
Gender : Male

0 8 ) S e l e c t
0 9 ) Enter

‘ ‘ Add User Account ? ’ ’

t h e f o l l o w i n g i n f o r m a t i o n :

Username : Hedwig
P r i v i l e g e L e v e l : F u l l
Password : Passw0rd
Confirm Password : Passw0rd

1 0 ) Leave a l l o t h e r d e f a u l t s a s
1 1 ) C l i c k ‘ ‘ Save ’ ’

t h e y a r e

Expected R e s u l t s : The password ,

‘ ‘ Passw0rd ’ ’ ,

s h o u l d be r e j e c t e d a s

t h e 1 0 , 0 0 0 most commonly−used p a s s w o r d s .

i t

i s on t h e

l i s t o f

A c t u a l R e s u l t s :

Fig. 2: Example SMPT Test Case

5.2.1.2 Exploratory Manual Penetration Testing (EMPT) Exploratory Manual Pen-
etration Testing is a manual, unscripted, exploratory, dynamic technique that does
not require access to source code. Previous studies of functional exploratory test-
ing have suggested that knowledge and experience may play a signiﬁcant role in
exploratory testing [42, 80].

EMPT follows a similar process to diagram in Figure 1. As found by Votipka
et al. [100], security analysts who perform exploratory testing spend time learning
about the system prior to beginning exploration. The analyst then moves on to
activities such as the “exploration” and “vulnerability recognition” [100]. The
analyst also still documents and reports all vulnerabilities found. However, the
process is less formal and more iterative for EMPT as compared with SMPT.

10

Elder et al.

5.2.2 Automated Techniques

We examine two categories of automated vulnerability detection techniques, Dy-
namic Application Security Testing (DAST) and Static Application Security Test-
ing (SAST). Figure 3 provides an overview of how Tool-based techniques are ap-
plied. The tools must ﬁrst be setup, which includes installing the tool as well as
conﬁguring and customizing the tool, if appropriate. The analyst then runs the
tool. Once the automated portion of the analysis is complete, the analyst must
review the tool output to remove false positives and prepare the report.

Fig. 3: Applying Tool-Based Techniques

5.2.2.1 Dynamic Application Security Testing (DAST) DAST uses automated tools
to perform dynamic analysis. We only include techniques that do not have access
to source code in the DAST category.DAST is sometimes referred to as Auto-
mated Penetration Testing [6, 7, 8], Black-Box Web Vulnerability Scanning [25],
Fuzzing [49], or Dynamic Analysis [20]. As part of our study, we examined two
general-purpose DAST tools. The ﬁrst tool was the Open Web Application Secu-
rity Project’s Zed Attack Proxy version 2.8.1 (OWASP ZAP, further abbreviated
as ZAP in tables)8, is a free, open-source, dynamic analysis tool which describes
itself as “the world’s most widely used web app scanner”[75]. The second DAST
tool, which we will refer to as DAST-2 (further abbreviated as DA-2 in tables), is
a proprietary tool.

DAST tools automatically generate a set of malformed inputs to the SUT based
on sample inputs provided by the analyst. Inputs to a web application, including
both sample inputs provided by the analyst and malformed inputs generated by
the DAST tool, are represented as HTTP messages. When a human interacts with
the web application through a browser, as is done with SMPT or EMPT, HTTP
messages are created by the web browser [61] to communicate with the application.
Each HTTP message requests that some action be applied to a particular resource
in the system [32]. Resources are identiﬁed through a Uniform Resource Identiﬁer
(URI). The action and URI are indicated in the header of an HTTP message. The
HTTP request in Figure 4 is a message from a sample input used in this study.
In Figure 4, the browser is requesting that the information in the message be
POSTed to the URI http://127.0.0.1:8080/openmrs/login.htm. The remainder
of the HTTP message contains a representation [32] of the resource requested. The
sample inputs provided by the analyst tell the DAST tool where the resources are,
what the representations are, and provide an initial outline for the order in which
messages should be sent. The sample message in Figure 4 is just one message in

8https://www.zaproxy.org/

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

11

a sequence. For example, if an analyst was executing the steps of the test case
in Figure 2, the HTTP message in Figure 4 would be generated as part of Step
04. GET requests to access the resources for the login page (Step 01-Step 03 of
Figure 2) would be sent prior to the POST request in Figure 4. Additional requests
would continue to be generated and sent based on the analyst’s interactions with
the application after the Request shown in Figure 4. Some DAST tools, such
as OWASP ZAP, provide built-in ways to record HTTP messages. Other DAST
tools require a standard ﬁle format such as HTTP Archive (.har)9 which can be
generated by most web browsers.

Some Web Application DAST tools, including OWASP ZAP but not DAST-2,
also include a web crawler [79, 25, 87] sometimes referred to as a spider, to ﬁnd
additional resources that may not have been included in the original inputs. For
example, the input may have not accessed a resource because the analyst creating
the input did not know that the resource existed and was accessible. The fact
that the resource is accessible may even be a vulnerability if the resource contains
sensitive information.

POST \ p r o t e c t \ v r u l e w i d t h 0 p t \ p r o t e c t \ h r e f { h t t p : / / 1 2 7 . 0 . 0 . 1 : 8 0 8 0 / openmrs / l o g i n . htm}{

h t t p : / / 1 2 7 . 0 . 0 . 1 : 8 0 8 0 / openmrs / l o g i n . htm} HTTP/ 1 . 1

t e x t / html , a p p l i c a t i o n / xhtml+xml , a p p l i c a t i o n / xml ; q = 0 . 9 , im a g e / webp , ∗ / ∗ ; q =0.8

I n t e l Mac OS X) F i r e f o x / 7 9 . 0

User−Agent : M o z i l l a / 5 . 0 ( M a c i n t o s h ;
A c c e p t :
Accept−Language : en−US , en ; q =0.5
Content−Type : a p p l i c a t i o n / x−www−form−u r l e n c o d e d
Content−L e n g t h : 1 7 0
O r i g i n : h t t p s : / / 1 2 7 . 0 . 0 . 1 : 8 8 0 0
C o n n e c t i o n : keep− a l i v e
R e f e r e r : h t t p s : / / 1 2 7 . 0 . 0 . 1 : 8 8 0 0 / openmrs / l o g i n . htm
C o o k i e : JSESSIONID=owczzkvwskvc8n4580psmvbb
Upgrade−I n s e c u r e −R e q u e s t s : 1
H o s t : 1 2 7 . 0 . 0 . 1 : 8 8 0 0

u s e r n am e=admin&p a s s w o r d=Admin123&s e s s i o n L o c a t i o n =0& r e d i r e c t U r l =/openmrs /

r e f e r e n c e a p p l i c a t i o n /home . p a g e

Fig. 4: Message from a Sample Input Provided to a DAST Tool by the Analyst

Once the sample inputs are provided, the DAST tool applies a set of security
rules to create a new set of malformed inputs. Figure 5 shows an HTTP message
that could be created by DAST tools as part of a malformed input, based on
the sample input in Figure 4. In this example, the sessionLocation parameter
is changed from a number, 0 as seen in Figure 4 which is the identiﬁer associ-
ated with the “Inpatient Ward” value on the login screen, to a script designed
to ﬁnd Cross-Site Scripting (XSS) vulnerabilities, <script>alert(1);</script>.
The same XSS-focused rule could be applied to the username parameter instead
of the sessionLocation parameter to generate a diﬀerent malformed input. A
diﬀerent rule could ignore parameters entirely and search the http header for sen-
sitive information, our could re-order the sequence of HTTP messages. The CWEs
speciﬁcally covered by the rules from each tool are discussed in Appendix A. ZAP
rules were associated with 33 CWEs while DAST-2 covered 44 CWEs for a com-
bined 68 CWEs covered. Similarly, ZAP covered six of the OWASP Top Ten while

9e.g. https://docs.rapid7.com/insightappsec/scan-scope/;

https://www.netsparker.com/support/scanning-restful-api-web-service/;
https://docs.gitlab.com/ee/user/application_security/api_fuzzing/create_har_
files.html

12

Elder et al.

DAST-2 also covered 6 of the OWASP Top Ten, 5 of which overlapped between
tools for a combined 7 of the Top Ten covered.

POST \ p r o t e c t \ v r u l e w i d t h 0 p t \ p r o t e c t \ h r e f { h t t p : / / 1 2 7 . 0 . 0 . 1 : 8 0 8 0 / openmrs / l o g i n . htm}{

h t t p : / / 1 2 7 . 0 . 0 . 1 : 8 0 8 0 / openmrs / l o g i n . htm} HTTP/ 1 . 1

t e x t / html , a p p l i c a t i o n / xhtml+xml , a p p l i c a t i o n / xml ; q = 0 . 9 , im a g e / webp , ∗ / ∗ ; q =0.8

I n t e l Mac OS X) F i r e f o x / 7 9 . 0

User−Agent : M o z i l l a / 5 . 0 ( M a c i n t o s h ;
A c c e p t :
Accept−Language : en−US , en ; q =0.5
Content−Type : a p p l i c a t i o n / x−www−form−u r l e n c o d e d
Content−L e n g t h : 1 7 0
O r i g i n : h t t p s : / / 1 2 7 . 0 . 0 . 1 : 8 8 0 0
C o n n e c t i o n : keep− a l i v e
R e f e r e r : h t t p s : / / 1 2 7 . 0 . 0 . 1 : 8 8 0 0 / openmrs / l o g i n . htm
C o o k i e : JSESSIONID=owczzkvwskvc8n4580psmvbb
Upgrade−I n s e c u r e −R e q u e s t s : 1
H o s t : 1 2 7 . 0 . 0 . 1 : 8 8 0 0

u s e r n am e=admin&p a s s w o r d=Admin123&s e s s i o n L o c a t i o n=<s c r i p t >a l e r t ( 1 ) ; </ s c r i p t >&

r e d i r e c t U r l =/openmrs / r e f e r e n c e a p p l i c a t i o n /home . p a g e

Fig. 5: Message from a Malformed Input (Test Case) Produced by a DAST Tool

With many combinations of rules and ways to apply them, DAST tools can
create and run hundreds or thousands of malformed inputs. The malformed inputs
generated by the DAST tool are sometimes referred to as test cases. DAST tools
execute the “test cases” (malformed inputs) automatically, which suggests that
DAST tools may be able to perform more testing in less time compared with
manual techniques such as SMPT, since computer systems can send electrical
impulses to other computer systems faster than an end-user can interact with the
system via keyboard or mouse [1]. One of the motivations for studies such as this
one, is to understand whether the promise of “more” inputs executed “faster” by
DAST produces equivalent or better results.

5.2.2.2 Static Application Security Testing (SAST) We use the term SAST to re-
fer to techniques that use automated tools to perform static source-code analysis.
SAST tools are a common way to comprehensively apply source code analysis,
as manual source code analysis can be tedious and time-consuming [54, 43, 87,
91, 20]. In practice, SAST tools are less likely to be applied by security ana-
lysts [20, 38, 100]. SAST and source code analysis is performed by the developers
themselves [20]. In this study, we use three SAST tools commonly used in in-
dustry. One tool, Sonarqube (abbreviated as Sonar in tables) version 8.2, has an
open-source platform available. We used the open-source community edition of
Sonarqube version 8.2. The other tools examined, SAST-2 and SAST-3 (further
abbreviated as SA-2 and SA-3 in tables) are proprietary tools and cannot be named
due to license restrictions. Sonarqube and SAST-2 were used to answer RQ1, while
SAST-2 and SAST-3 were used to answer RQ2.

All SAST tools used in this study perform static analysis by ﬁrst parsing
the source code to build a tree representation of the source code, known as a
syntax tree. The tool then applies a set of rules to the syntax tree, where each rule
describes a pattern within the syntax tree that may indicate a vulnerability [54]. As
with DAST, SAST tools have evolved to include additional features. For example,
Sonarqube uses symbolic execution as well as a traditional rules engine to identify
vulnerabilities[53]. Similarly, both SAST tools used in this study employ taint

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

13

analysis [12], although it is not clear whether this is available in the free / open
source version of Sonarqube used in this study.

SAST tools can be setup according to diﬀerent architectures. The SAST tools
used in this study could be conﬁgured as client-server tools where the SUT code
is scanned on the “client” machine, and information is sent to a “server”. The
analyst then reviews the results through the server. For some tools, the automated
analysis and rules are applied on the client, while for other tools the automated
analysis and rules are applied on the server. The SAST tools used in this study
each included an optional plugin for Integrated Development Environments (IDEs)
such as Eclipse10. The plugin allows developers to initialize SAST analysis and in
some cases view alerts from the tool within the IDE itself. Some tools can be
run without a server using only IDE plugins. Other tools require a server. Similar
to the previous work by Austin et al. [7, 8], we found that the server GUI was
easier to use when aggregating and analyzing all system vulnerabilities for RQ1.
Consequently, a client-server conﬁguration was used with SAST-2 and Sonarqube
to answer RQ1. SAST-2 and SAST-3 were more easily conﬁgured to use locally
within an IDE using consistent rules, as was done in RQ2.

6 System Under Test - OpenMRS

The SUT for our case study was OpenMRS, an open-source medical records sys-
tem. OpenMRS is a “Java-based web application capable of running on laptops in
small clinics or large servers for nation-wide use”[76].

6.1 OpenMRS Overview

OpenMRS contains 3,985,596 lines of code as measured by CLOC v1.7411 including
476,139 coding lines, i.e. not comments, of Java as well as 1,884,233 coding lines
of Javascript. The OpenMRS architecture is modular. The source code for each
module is stored in a separate repository on github12. In this study, we examined
the 43 modules that are in the basic reference application for OpenMRS Version
2.9. We compiled and ran OpenMRS using Maven and Jetty as described in the
Developer’s Manual[76].

6.2 Why OpenMRS?

We selected OpenMRS as the SUT ﬁrst due to its domain. The three SUT ex-
amined by Austin et al [7, 8] were medical records systems. Hence the SUT for
the current study should also come from the medical domain. Although Open-
MRS was not examined by Austin et al., OpenMRS has also been used in other
research on software testing and security analysis[97, 82]. The security of medical
records systems is, if anything, a more important issue in 2021 than in 2011, with
healthcare systems being an increasingly popular target for hackers [83, 98, 16, 9].

10https://www.eclipse.org/ide/
11https://github.com/AlDanial/cloc
12https://github.com/openmrs

14

Elder et al.

Second, OpenMRS is a “real” system that is actively used and actively un-
der development. The 2018 U.S. National Institute of Standards and Technology
(NIST) Software Assurance Metrics and Tool Evaluation (SAMATE) Static Anal-
ysis Tool Exposition (SATE) report [23], provides the following example criterion
for “real, existing software”: “their development should follow industry practices.
Their size should align with similar software. Their programming language should
be widely used for their purpose.” As of July 2021[77], OpenMRS is actively in use
in many contexts from Non-Governmental Organizations (NGOs) operating clin-
ics in Sierra Leone, Lesotho, Mexico, and Haiti to hospitals and health networks
in Tanzania, Pakistan, and Bangladesh.OpenMRS follows common development
practices for open-source systems, as discussed in their Developer Guide[76]. With
over 3 million lines of code, OpenMRS is comparable to other modern medical
records systems, such as the VistA system used by the US Department of Veteran
Aﬀairs[99] and Epic[27], which involve millions of lines of code. The languages and
frameworks used by OpenMRS including Java, Javascript, Node.js, SQL, and CSS,
consistently appear on lists of most commonly used software technologies such as
the 2021 StackOverﬂow Developer Survey[94].

Furthermore, OpenMRS was being used by the graduate-level security course
at North Carolina State University, which enabled us to collect suﬃcient eﬃciency
data to perform a statistical comparison across techniques with little impact on the
participants themselves, as outlined in our Institutional Review Board Protocol
20569. While studies such as SATE provide controlled comparisons of the eﬀec-
tiveness of a limited range of vulnerability detection techniques, we do not know
of another study that provides perspective on the eﬃciency of these techniques in
terms of Vulnerabilities per Hour (VpH).

6.3 Security Practices at OpenMRS

OpenMRS is open-source software. The OpenMRS team has received vulnerabil-
ity reports from both volunteers and independent researchers in the past, based
on SAST and other vulnerability detection techniques. When we reached out to
OpenMRS with our results, our understanding was that SAST and DAST tools
were not being used at the organizational level. Since then, OpenMRS’s security
posture has continued to mature as they implement more vulnerability detection.

7 Data Sources

The data that was analyzed came from two sources: 1) a team of ﬁve (5) researchers
and 2) sixty-three (63) students from a graduate-level security course. In this
section, we provide background on these sources necessary to understand how
information from each source was used in our methodology

7.1 Researcher Data

Three PhD student researchers, one Master’s student researcher, and one under-
graduate student researcher applied SMPT, DAST, and SAST; and reviewed the

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

15

outputs of all four techniques as part of data collection for RQ1. The researchers
also reviewed all student information that was used in this study to remove incor-
rect answers. All graduate-level researchers had participated in the graduate-level
software security course. The undergraduate student researcher had taken two
security-related undergraduate courses.

7.2 Student Data

Sixty-three of 70 students in a graduate-level software security course allowed
their data to be used for this study by signing an informed consent form. Student
data was collected following North Carolina State University (NCSU) Institutional
Review Board Protocol 20569. Students worked in teams of 3-4 students, with a
total of 19 teams in the class. Where data could only be aggregated at the team
level, we use the data of the 13 teams in which all team members consented to
the use of their data. Where data is available at the student level, we use data
from all 63 students who consented to the use of their data. Student EMPT and
SMPT data was used as part of the data collection process for RQ1. Additionally,
researchers analyzed students’ reported eﬃciency scores to answer RQ2.

7.2.1 Student Experience

At the beginning of the course, students were asked to ﬁll out a survey about
their relevant experience. The full survey is available in Appendix B. Fifty-ﬁve
(55) of the 63 students whose data was used in this study provided valid survey
responses. Seven (7) of the 55 students had no industry experience. The median
industry experience of students, including those with no experience, was 1 year.
The average industry experience, including students with no experience, was 1 year
8 months. Students were asked to note how much of their time in industry involved
cybersecurity on a scale from 1 (none) to 5 (fully). The distribution of security
experience for the 48 students with industry experience is shown in Figure 6.
The x-axis indicates students who had up
to 2 years of industry experience as com-
pared with more than 2 years of industry
experience. The y-axis indicates the num-
ber of students. The shading within the
bar chart indicates security experience on
a scale from 1 to 5. Darker shades indi-
cate higher security experience. Of the stu-
dents with industry experience, the median
value for security experience was 2, while
the average was 1.85. In addition to indus-
try experience, 7 students had previously
taken a course in security or privacy. Eight
(8) students were currently taking a course
in security or privacy in addition to the
course from which the data was collected.
Students had a diverse range of experience,

Fig. 6: Industry Security
Experience of Students

16

Elder et al.

but most students had little experience in
cybersecurity at the start of the course.

7.2.2 Course Assignments

Students were not required to perform any tasks that were not already part of their
assignments for the course. The data used in this study that comes from student
assignment responses is taken from the Course Project. The course project had
four parts which were distributed over the duration of the semester. The verbatim
text from the course project assignments is provided in Appendix C. A summary
of the assignments that relate to our study is as follows:

– SMPT Assignments: In the Project Part 1, students were required to write
and execute a set of 15 systematic manual penetration test cases for the ﬁrst
assignment. Each test case mapped to at least one ASVS control. In Project
Part 3, students were required to write and execute ten additional test cases for
logging (ASVS V7 Levels 1 or 2), as well as to write and execute an additional
ﬁve test cases to increase the ASVS coverage of their test suite. Correct, unique
test cases and their results were used as part of the Data Collection for RQ1
(Eﬀectiveness). The test cases were re-run by researchers and supplemented
with additional test cases developed by researchers, as we will discuss further
in Section 8.1.2.1. Student performance and experience with SMPT as part of
these assignments also informed their response to the Comparison Assignment
listed below, which was used to collect data for RQ2 (Eﬃciency) and RQ3
(Other Factors).

– EMPT Assignment: In Project Part 4, students spent three hours individu-
ally performing exploratory penetration testing. This activity occurred at the
end of the course when students were familiar with the SUT and with many
security concepts. Students produced a video recording of their three-hour ses-
sion, speaking out loud about any vulnerabilities found; and created black box
test cases to enable replication of each vulnerability found. The vulnerabilities
found by students were used as part of the Data Collection for RQ1 (Eﬀec-
tiveness). Student performance and experience with EMPT informed their re-
sponses to the Comparison Assignment, which was used as part of the Data
Collection for RQ2 (Eﬃciency) and RQ3 (Other Factors).

– DAST Assignment: In Project Part 2 students used two DAST tools (OWASP
ZAP and DAST-2), using 5 test cases from their SMPT assignments to pro-
vide the initial model for the DAST tool. Students reported the number of
true positive vulnerabilities found and the amount of time required to review
the output. Students’ performance and experience with the DAST assignment
contributed to their response to the Comparison Assignment, which in turn
was used as part of the Data Collection for RQ2 (Eﬃciency) and RQ3 (Other
Factors).

– SAST Assignment: In Project Part 1, each student team ran two SAST tools
(SAST-2 and SAST-3) on a subset of the SUT. Due to the length of many
SAST reports, students were only required to review at least 10 of the reports
from each tool to determine whether the alerts were true or false positives.
Students had to identify at least 5 false positives even if it required reviewing
more than 10 alerts to ensure that students were not incentivized to focus

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

17

on trivial false positives. The students reported the number of true positive
vulnerabilities found, as well as the amount of time required to review the
alerts. The SAST assignment contributed to the students’ response to the
Comparison Assignment, which in turn was used as part of the Data Collection
for RQ2 (Eﬃciency) and RQ3 (Other Factors).

– Comparison Assignment: At the end of the course in Part 3 and Part 4 of
the project, each team created a table showing the number of vulnerabilities
found by each activity, the amount of time it took to discover these vulnerabil-
ities, and the resulting VpH. The students then reﬂected on their experience
with the diﬀerent vulnerability detection techniques in a free-response format.
The numeric responses in the table were used to answer RQ2. Two researchers
applied qualitative analysis to the students’ free-response answers for RQ3.

7.3 Overview of Data Sources Per RQ

Table 1 provides an overview of where student data was used as part of Data
Collection for each Research Question. Areas where student data was used are
indicated with an S. Areas where researcher data was used are indicated with an
R. All data analysis was performed by researchers, and is therefore not included
in Table 1. As can be seen in the table, RQ1 relied primarily on researcher eﬀorts,
although student data was used with SMPT and EMPT. Student data was used
more extensively in RQ2, and was the source of the documents used in qualitative
analysis for RQ3.The detailed methodology for each Research Question will be
further explained in Sections 8, 9, and 10, respectively.

Table 1: Data Sources

R = Researchers; S = Students

Technique

SMPT EMPT DAST SAST

RQ1

RQ2

RQ3

Applying Technique Sa & R Sa
R

Review Output

R

Recording Eﬃciency

Data Cleaning

Document Source

Qualitative Coding

S

R

S

R

S

R

S

R

R

R

Sb
R

S

R

R

R

Sb
R

S

R

aStudent results for RQ1 were reviewed by researchers for quality
bFor empirical comparison we base our answer on the recorded performance of Students.

In Section 12.2 we also include Researcher eﬃciency with automated tools for reference.

8 Methodology for RQ1 - Eﬀectiveness

Our ﬁrst research question is: What is the eﬀectiveness, in terms of number and
type of vulnerabilities, for each technique? To answer RQ1, we need a compa-

18

Elder et al.

rable set of vulnerabilities found by each technique. Ensuring the vulnerabil-
ity counts were comparable required an extensive Data Collection process de-
scribed in Section 8.1 which is split into two phases. In the ﬁrst phase of data
collection in RQ1, Applying the Technique we applied each vulnerability detec-
tion technique described in Section 8.1.2 to our SUT. The initial outputs of
each technique are not comparable. For example, an analyst performing EMPT
might document a single vulnerability where a malicious input script such as
<script>alert(123)</script> is saved in one part of the application due to an
input validation vulnerability and executed by the application due to lack of out-
put sanitization. A SAST tool, on the other hand may scan for input validation
and output sanitization using diﬀerent rules, resulting in two failing alerts for
the same issue documented using EMPT. To reduce possible biases introduced by
diﬀerent vulnerability counting approaches or diﬀerent vulnerability type classiﬁ-
cation approaches, we review the output of each technique in the second phase of
Data Collection described in Section 8.1.3. Once the data has been collected, we
analyze the results as described in Section 8.2.

8.1 Data Collection

We begin this section with a set of guidelines which we will refer to throughout our
description of the Data Collection process for RQ1. We then go into the details of
how data collection was performed. As seen previously in Table 1, we subdivide
our Data Collection process for RQ1 into two phases.

The ﬁrst phase of Data Collection, Applying the Technique, was accomplished
through combination of student (S) and reviewer (R) work. We provide details of
how each technique was applied in Section 8.1.2. For DAST and SAST, a set of
True / False Positive classiﬁcation guidelines were required, which we highlight
under General Guidelines in Section 8.1.1.1.

The second phase of Data Collection, Reviewing the Technique Output, was
done entirely by researchers and is described for each technique in Section 8.1.3.
As part of this second phase, the researchers used the Counting, CWE Review,
and Severity Guidelines from Section 8.1.1.

8.1.1 General Guidelines

This section provides key guidelines for the Data Collection process which we
will refer to for the remainder of Section 8.1. A common set of True / False
Positive Classiﬁcation guidelines is necessary for applying automated techniques
(DAST and SAST), as described in Section 5. As part of Reviewing the Technique
Output described in Section 8.1.3, the output of each technique was reviewed to
ensure that the vulnerability count, vulnerability type, and vulnerability severity
are consistently evaluated using the guidelines provided in Sections 8.1.1.2, 8.1.1.3,
and 8.1.1.4

8.1.1.1 True/False Positive Classiﬁcation Guidelines A true positive alert or vul-
nerability is one that meets the the deﬁnition for a vulnerability from Section 2. We
follow a conservative policy towards true- and false-positive classiﬁcation based on
the principle of Defense-in-Depth [44]. We considered an alert or other ﬁnding to

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

19

be a vulnerability if it could potentially lead to a security breach. For example, an
alert is raised due to a particular malicious input. Upon review, we note that the
input is stored in the database without encoding or other protection. We would
classify the alert as a true positive even if we have not yet found another vulnera-
bility where the malicious input is executed, e.g. by the application as part of an
XSS attack. There may be vulnerabilities yet to be found that would result in the
input being executed, and changes to the code could make the input validation
vulnerability more exploitable in the future.

8.1.1.2 Counting Guidelines As mentioned in Section 2, we used the CVE deﬁ-
nition of a vulnerability. The CVE Program also provides a set of guidelines[55]
to CVE Numbering Authorities (CNAs) for determining which vulnerability re-
ports represent distinct true positive vulnerabilities. The guidelines help CNAs to
identify and remove false positives, as well as to consolidate duplicate vulnerabil-
ity reports. We based our counting process on the CVE Counting Rules[55] for
determining the number of vulnerabilities identiﬁed using each technique13. The
counting rules used in our analysis were:

– True/False Positive: The failure report must provide evidence of negative im-

pact or that the security policy of the system is violated; and

– Independence: Each unique vulnerability must be independently ﬁxable.

We applied these counting rules to the initial failures of each technique. For
example, we applied the counting rules to the alerts produced by a tool. When
one alert pointed to the same vulnerability as another alert, we marked one of the
alerts to be a “duplicate” of the other.

Where we were unsure of the independent ﬁxability of diﬀerent failures, we
assumed that the failures represented independent vulnerabilities. For example, a
vulnerability detection tool could raise two alerts for the same type of vulnerability,
where each alert was triggered by a diﬀerent checkbox in the same form. We
counted each alert as a distinct vulnerability unless we knew that the checkboxes
relied on the same server-side code.

We consider an alert to be a “True Positive” even if the vulnerability found
does not have the same CWE type as the original failure. CWE type is reviewed
separately using the Vulnerability Type Guidelines in Section 8.1.1.3.

8.1.1.3 Vulnerability Type Guidelines The vulnerability types assigned to each
vulnerability are based on two systems - CWE and OWASP Top Ten - which
are described in Section 2. Each vulnerability was assigned a CWE based on the
test case in SMPT, by the student who found the vulnerability in EMPT, or by the
tool for DAST and SAST. The CWE values were reviewed as we discuss below for
accuracy. The vulnerabilities were then mapped to the OWASP Top Ten (2021)
categories based on the CWE values using the mapping provided by CWE[58].

Researchers reviewed the CWE type assigned to each vulnerability and cor-
rected the CWE assignment when the assignment was missing, inaccurate, or
inconsistent with the assignment for other vulnerabilities in our dataset. For ex-
ample, a DAST tool creates a malformed input (test case) designed to trigger XSS

13The CVE Counting rules have been updated since our original study. In future work, the

authors may follow the updated rules: https://cve.mitre.org/cve/cna/rules.html

20

Elder et al.

as described in Section 5.2.2.1. When the malformed input is executed against the
SUT, it triggers an error message revealing sensitive information (CWE-20914).
The error message is unexpected behavior which may result in the alert being
ﬂagged by the DAST tool. However, the alert will be assigned CWE-79 (XSS15)
since that was the rule used to create the test case. In our study we would consider
this alert to be “true positive” since the alert points to a vulnerability. However,
the CWE type would need to be reclassiﬁed - i.e. we would consider the error mes-
sage containing sensitive information to be a CWE-209 vulnerability even though
the alert indicates CWE-79. When a classiﬁcation is incorrect, if there are already
similar vulnerabilities in our dataset we reclassify the alert to the same CWE as
the similar vulnerabilities. In the previous example of an alert reclassiﬁed as CWE-
209, there were many CWE-209 vulnerabilities ﬂagged by SMPT and EMPT prior
to running the DAST tool. Otherwise, the analyst may need to perform a keyword
search the CWE database [57] to ﬁnd an appropriate CWE.

The CWE mapping to the OWASP Top Ten [58], as well as general guidelines
such as the list of “Weaknesses for Simpliﬁed Mapping of Published Vulnerabilities”[59]
and relationships between CWEs provided by the CWE system [56] were used to
determine whether one CWE is more appropriate. For example, multiple students
found issues where sensitive data was passed in the URI of a GET request, which
results in the sensitive information being sent in cleartext that could be intercepted
by a malicious actor. Some students associated these vulnerabilities with CWE-319
Cleartext Transmission of Sensitive Information 16 while others used CWE-598 Use
of GET Request Method With Sensitive Query Strings 17. CWE-319 maps to the
OWASP Top Ten (2021) category A02 - Cryptographic Failures, while CWE-598
maps to the OWASP Top Ten (2021) category A04:2021 - Insecure Design. The
vulnerability is not an issue with broken cryptography [36, 81], and so only CWE-
598 was assigned to the vulnerabilities where sensitive information was revealed in
the URL of a GET request. On the other hand, when multiple CWEs were equally
applicable, multiple CWEs could be assigned to the same vulnerability.

Fifty-seven (57) CWE types were found in our experiment. We mapped the
vulnerabilities found to the OWASP Top Ten through their assigned CWE values.
Mapping to the OWASP Top Ten provides a more readable summary of the types
of vulnerabilities found, requiring 10 categories instead of 57. The OWASP Top
Ten, as described in Section 2, categorizes and ranks vulnerability types based
on how frequently they are seen in software systems and their severity, providing
additional insight into the vulnerabilities found.

8.1.1.4 Severity Guidelines We use two diﬀerent perspectives for examining the
severity of the vulnerabilities found. Our ﬁrst perspective on severity is through
the lens of the OWASP Top Ten. As mentioned in Section 2 and reiterated in
Section 2, the categories of vulnerabilities included in the OWASP Top Ten are
ranked based on how common the vulnerability is, and expert views on the severity
of the vulnerability category. The OWASP Top Ten are ranked in a “risk-based
order” suggesting that the ﬁrst category of the OWASP Top Ten: A01 - Broken

14https://cwe.mitre.org/data/definitions/209.html
15https://cwe.mitre.org/data/definitions/79.html
16https://cwe.mitre.org/data/definitions/319.html
17https://cwe.mitre.org/data/definitions/598.html

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

21

Access control, is considered a highest risk and is therefore more severe than
vulnerabilities associated with lower-ranked categories.

We also examine the severity based on the severity provided by tools, sup-
plemented by analysis of high-frequency vulnerability types and discussions with
OpenMRS. As discussed in Section 8.1.2, we excluded alerts that were labeled as
insigniﬁcant or inconsequential by the tools themselves. In our results, we further
split the vulnerabilities between those that are “less severe” and those that are
“more severe”. Vulnerabilities classiﬁed as “less” severe primarily include vulner-
abilities where at least one tool indicated the vulnerability was of “Low” severity.
Diﬀerent tools have diﬀerent labels for the diﬀerent levels. We consider the low-
est severity level for each tool to be “‘Low”. Once vulnerabilities were detected,
we reviewed “more severe” vulnerabilities but where more than 20 vulnerabilities
associated with the same CWE were found by the same tool or technique. In our
experience, if a tool or technique is ﬂagging large amounts of vulnerabilities asso-
ciated with the same vulnerability type, it is unlikely that those vulnerabilities are
more severe. Additionally, large quantities of incorrectly classiﬁed vulnerabilities
may skew the results. Additionally, we adjusted severity levels based on discus-
sions with OpenMRS. Since the tool-based severity level was adjusted depending
on the results themselves, we discuss the vulnerability types where severity was
further reviewed in Section 12.1.3.

8.1.2 Applying the Technique

Application of each technique is slightly diﬀerent, as described in Section 5. We
discuss how each technique was applied for our Case Study for SMPT in Sec-
tion 8.1.2.1, for EMPT in Section 8.1.2.2, for DAST in Section 8.1.2.3, and for
SAST in Section 8.1.2.4

8.1.2.1 SMPT As shown in Table 1, for applying SMPT as part of RQ1, we use
data from both students (S) and researchers (R) who wrote and executed test
cases based on the Open Web Application Security Project (OWASP) Applica-
tion Security Veriﬁcation Standard (ASVS) [95] described in Section 2. While a
healthcare application such as OpenMRS should strive for Level 2 or higher of
the ASVS, as noted in the ASVS document itself “Level 1 is the only level that is
completely penetration testable using humans” [95]. In addition to the ASVS Level
1 controls, we used knowledge of the OpenMRS and documentation available on
the OpenMRS wiki18 to develop test cases speciﬁc to OpenMRS. We excluded 44
controls that were not applicable to the application. For example, the SUT did not
include a mail server. ASVS control 5.2.3 [95] states “Verify that the application
sanitizes user input before passing to mail systems to protect against SMTP or
IMAP injection” and hence is not applicable in our case study.

In total, the test suite included 131 test cases which covered 63 of the remaining
87 controls. We used 86 test cases developed and executed by students and 45 test
cases developed and executed by researchers to increase ASVS coverage. Students
originally wrote over 390 test cases as part of their course assignments described
in Section 7.2.2. However, many of the students’ test cases were duplicates of each
other since each of the 13 teams worked independently and generally wrote test

18https://wiki.openmrs.org/

22

Elder et al.

cases for the easier security concepts. Additionally, test cases were removed due
to quality concerns with the test case or the results recorded.

Each test case was executed by two independent analysts to reduce bias and
inaccuracy due to human error and subjectivity. For the 86 test cases used in this
study that were developed by students, the ﬁrst test case execution was performed
by the students and the second execution was performed by researchers. For the
45 test cases developed by researchers, two diﬀerent researchers each executed the
test case one time. When the two executions of the test case produced diﬀerent
results, an additional researcher executed the test case and the result (pass or fail)
given by two of the three test case executions was recorded as the actual result.

8.1.2.2 EMPT For RQ1, EMPT was primarily applied by students (S) as shown
in Table 1, according to the assignment outlined in Section 7.2.2. Students were
required to spend three hours performing EMPT and record the results via video.
Students documented their results as test cases to improve their replicability. An
important factor in exploratory testing is ensuring that individuals have suﬃcient
knowledge and experience [42, 41, 100] since there is no formal process for the
analysts to follow. The students had limited security experience at the start of the
course, as discussed in Section 7.2. However, our results suggest that the students
had suﬃcient experience by the time of the EMPT assignment near the end of
the course to be eﬀective applying EMPT. One of the 63 students who agreed
to participate in the study did not complete the EMPT assignment, and so data
from 62 students was used as part of data collection for EMPT in RQ1. Since
extensive review of the student results was needed, which we will discuss further in
Section 8.1.3.2, we distinguish between Student Reported Vulnerabilities (SRVs),
and the ﬁnal set of vulnerabilities found by EMPT used to answer RQ1.

8.1.2.3 DAST As shown in Table 1, to collect DAST results for RQ1 researchers
(R) setup and ran each tool using the default ruleset. The CWEs covered by each
ruleset are shown in Table 10 of Appendix A. As discussed in Section 5.2.2.1,
DAST tools require a set of application inputs, which are created by recording an
analyst’s interactions with a system. To better understand whether DAST tools
would ﬁnd the same vulnerabilities as SMPT and other techniques, the researchers
recorded their interactions with the application as they performed actions based
on six test cases from the SMPT suite. The test cases were selected based on
their uniqueness, to maximize coverage of the SUT. Due to the complexity and
resources required by the DAST-2 tool, we were unable to run additional test cases,
a limitation discussed further in Section 13. Based on the interactions provided
by analyst and default ruleset, each tool generated and tested a set of malformed
inputs against the system.

For OWASP ZAP, we manually explored the system through the ZAP proxy to
record our interactions. We then ran the spider before running the security scan.
Since OWASP ZAP was run after DAST-2, we limited the OWASP ZAP input to
the six SMPT test cases used for DAST-2. With OWASP ZAP we were able to
cover all six test cases in a single run of the tool, which took less than two hours
to execute.

DAST-2 was more resource-intensive and required signiﬁcant conﬁguration to
run on our systems. For DAST-2, we recorded the interactions for each of the 6
SMPT test cases in an HTTP Archive ﬁle and uploaded it to the tool. Dozens or in

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

23

some cases hundreds of http messages could be exchanged during the execution of
a single SMPT test case. Unfortunately, the DAST-2 tool itself would eventually
crash due to memory or processing constraints if more than approximately six
HTTP messages were included in the initial model. Hence each test case had to be
recorded separately, and used in a separate “run” of the DAST tool. We removed
all HTTP messages from the input sequence other than the messages that were
key to the test case. To apply DAST-2 based on the interactions for the example
test case in Figure 2, test case we would include the HTTP messages for the GET
request that loaded the initial OpenMRS login page in Step 01, the POST request
which performed the login at step 04 containing the login details from steps 02-
03, the GET request for the “Add New Account” page in Step 06, and the Post
request for saving the user at step 11, containing the account information from
steps 07-10. We would remove all other HTTP messages from the sequence, such
as GET requests for CSS ﬁles for each page as well as intermediate pages such
as the “System Administration” and “Manage Accounts” pages in step 05 which
were not the target of the test case. For longer test cases such as the one shown in
Figure 2, we would start with a sequence of hundreds of requests, and remove all
but 2-6 requests. We then used DAST-2 to generate malformed inputs based on
the condensed set of requests and default rules. DAST-2 did not recommend using
the entire set of malformed inputs that were generated, and enabled the analyst
to run a randomized subset of the test cases.

Two researchers performed multiple trials of the same tool in diﬀerent con-
ﬁgurations to determine how diﬀerent choices would impact the alerts produced.
For example, we applied the DAST-2 tool to the POST request from the login
page with both a full set of test cases and a randomize subset to better under-
stand the ramiﬁcations of using only a randomized subset of the test cases. We
noted that due to the repetitive nature of the test cases generated by DAST-2,
the alerts produced by the random subset were similar to the alerts produced by
the full set, and the alerts produced by the full set did not seem to point to any
additional vulnerabilities. The observation that vulnerability count may be stable
is congruous with the results from Klees et al. [49], where the authors found that
the unique bugs identiﬁed may be more consistent than the failures and crashes
produced. We then performed a ﬁnal run for each test case recorded where we had
all unnecessary HTTP messages from the recording and used a randomized subset
of test cases to ensure that the DAST tool run for each test case was performed
consistently. Seven separate models were setup and run for DAST-2 to cover the
key http messages of six test cases selected from SMPT. The ﬁnal run required
between 2 hours and 3 days for each model.

We reviewed the output taken from the last run for each tool for true and
false positives using the guidelines in Section 8.1.1.1. As noted by Klees et al. [49],
fuzzing-based tools, such as DAST, can produce thousands of results. We excluded
output where the tool indicated the alert is likely insigniﬁcant or inconsequential,
for example with OWASP ZAP we exclude alerts where the Risk level noted by
the alert was “Informational”[78]. Unless otherwise noted, when we refer to the
alerts from a DAST or SAST tool, we are excluding alerts marked as insigniﬁ-
cant or inconsequential by the tools themselves. For OWASP ZAP, two reviewers
independently examined all alerts as was done with the SAST tools. It became
apparent once DAST-2 produced over one thousand alerts, excluding insigniﬁcant
alerts, that two researchers reviewing all alerts would be ineﬃcient if coding could

24

Elder et al.

be done consistently. Consequently, researchers independently reviewed a random
subset of the alerts. For each of the seven models for DAST-2, two researchers
reviewed at least 40 alerts individually to compare their agreement and deter-
mine whether continued review by two independent researchers was necessary for
consistency. For models which produced less than 40 alerts, two researchers each
reviewed every alert.

For both tools, the researchers calculated their inter-rater reliability using Co-
hen’s Kappa both manually and using R to determine if they were consistently
classifying the results as true or false positive. For DAST-2 if the inter-rater relia-
bility was at least 0.70[51, 100], the reviewers then split the remaining alerts such
that each reviewer only examined half of the remaining alerts. The inter-rater
reliability for the classiﬁcation of DAST alerts as true or false positive and the
precision of the tools based on the ﬁnal set of True/False Positives is reported in
Section 12.1.1.

8.1.2.4 SAST Similar to applying DAST, as seen in Table 1, applying SAST be-
gan with researchers (R) running the SAST tools on the SUT using the default
security rules. The CWEs covered by each ruleset are shown in Table 10 of Ap-
pendix A. SAST tools are deterministic, producing the same alerts for each run.
Similar to DAST results, we excluded SAST alerts that the tool marked as insignif-
icant or inconsequential such as Sonarqube’s “Security Hotspots” which are used
for “security protections that have no direct impact on the overall application’s
security”[93]. Unless otherwise noted, when we refer to the alerts from a SAST or
DAST tool, we are excluding alerts marked as insigniﬁcant or inconsequential by
the tools themselves. Since none of the SAST tools produced over 1000 results,
the remaining alerts were reviewed by two researchers to identify and remove false
positives using the guidelines in Section 8.1.1.1. We computed Cohen’s Kappa[15]
to determine whether the classiﬁcation process was consistent and reproducible.
A third researcher resolved disagreements.

8.1.3 Reviewing the Technique Output

Each technique produces diﬀerent outputs. For example, systematic, dynamic tech-
niques such as SMPT and DAST produce a set of failing test cases. SAST, on the
other hand, ﬁnds speciﬁc weaknesses in the codebase that should be changed to
improve the security of the system. Multiple failing test cases may be due to the
same weakness in the codebase, or a single failing test case may be due to multiple
weaknesses in the codebase. Consequently, the raw count of the output may be
higher or lower for one technique even though it is no more eﬀective than another
technique. To resolve these potential counting diﬀerences, we take the output of
each technique, which we refer to as a list of “failures”, and apply the Counting
Rules described in Section 8.1.1.2 to determine the number of vulnerabilities found
by each technique. While most of the failures are already assigned a CWE type
by the tool or by the ASVS control, the CWE type may not be correctly assigned.
We also review the CWE assignments as part of reviewing the technique output.
As shown in Table 1, researchers (R) reviewed the output of all techniques.

8.1.3.1 SMPT Two researchers (R) independently reviewed all failing test cases
from SMPT to determine how many vulnerabilities were found using the counting

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

25

rules outlined in Section 8.1.1.2. The researchers discussed their diﬀerences with
a third researcher, as needed to determine the ﬁnal vulnerability count. The re-
searchers also reviewed the CWE assigned to the vulnerabilities, as described in
Section 8.1.1.3. Each test case was linked to an ASVS control, which was associ-
ated with a CWE. However, a test case failure may have been due to a violation of
a diﬀerent security principle than the original CWE associated with the test case.
Finally, after discussing the ﬁnal set of vulnerabilities with OpenMRS, we sepa-
rated “less severe” vulnerabilities from more critical vulnerabilities as discussed in
Section 8.1.1.4.

8.1.3.2 EMPT For EMPT, one researcher (R) reviewed each Student Reported
Vulnerability (SRV) while a second researcher audited 100 randomly sampled SRV
as well as 2 additional SRV at the request of the ﬁrst reviewer. A third researcher
performed additional auditing. The ﬁrst reviewer examined each of the 484 SRV
provided by students who participated in the study to determine if the SRV was
reproducible. The researcher removed SRV if the researcher could not understand
the students’ documentation or if the researcher was unable to observe the re-
sult reported. The researcher determined if the SRV was a true positive using the
True/False Positive guidelines described in Section 8. While reviewing the student
responses for reproducibility and removing false positives, researchers used the
counting rules speciﬁed in Section 8.1.1.2 to remove SRV that had already been
reported by other students and to split SRV into multiple vulnerabilities when
students did not follow the counting rules and reported multiple vulnerabilities as
a single SRV. The researchers reviewed the CWE values assigned to each vulnera-
bility, as described in Section 8.1.1.3, removing inaccuracies due to typos and other
errors. Finally, after discussing the ﬁnal set of vulnerabilities with OpenMRS, we
distinguished less severe vulnerabilities from more severe vulnerabilities following
the guidelines in Section 8.1.1.4.

After reporting our results to OpenMRS, feedback from the OpenMRS team
resulted in the removal of ﬁve additional EMPT vulnerabilities that were deter-
mined to be not reproducible. A team of Master’s and Undergraduate students
at NCSU working with OpenMRS to assist in ﬁxing the vulnerabilities also pro-
vided feedback, which resulted in consolidating three EMPT vulnerabilities. The
three vulnerabilities consolidated had been found on three diﬀerent pages of the
application, in a shared search function.

8.1.3.3 DAST , Once the true and false positive alerts were determined for each
DAST tool, two researchers (R) determined how many unique vulnerabilities were
indicated by the alerts using the counting rules from Section 8.1.1.2. Researchers
marked alerts that were triggered by the same vulnerability as “duplicates” of each
other. The researchers assumed DAST alerts were distinct unless the malformed
inputs triggering the alert shared similar characteristics such as the CWE type,
URL, and targeted parameter. Research and experience were used to determine
whether the alerts were unique or duplicate. If the researchers could not determine
whether alerts were duplicates, they were assumed to be unique unless the alerts
shared all three characteristics (CWE type, URL, and targeted parameter), in
which case they were assumed to be duplicate. It is unlikely, for example, that
the “sessionLocation” parameter for the “/openmrs/login.htm” URL shown in

26

Elder et al.

Figure 5 would contain two distinct XSS vulnerabilities. Discussions of duplication
and de-duplication continued in subsequent steps of the review.

The CWE value of each vulnerability found by DAST was based on the CWE
value of the alerts associated with that vulnerability. The CWE value of the alerts
was assigned by the DAST tool(s). Researchers also reviewed the CWE values
using the guidelines in Section 8.1.1.3. The severity measures provided by the
DAST tools were also used to distinguish less severe vulnerabilities from more
severe vulnerabilities as described in Section 8.1.1.4.

8.1.3.4 SAST Researchers (R) determined the number of distinct vulnerabilities
indicated by the SAST alerts using the counting rules from Section 8.1.1.2. The
researchers then reviewed the vulnerability CWE assignments provided by the
SAST tools to ensure their accuracy following the guidelines from Section 8.1.1.3.
Additionally, severity measures provided by the SAST tools were used to distin-
guish less severe vulnerabilities from more severe vulnerabilities as described in
Section 8.1.1.4.

8.2 Data Analysis

Once we had a comparable set of vulnerabilities, we calculate the number of vul-
nerabilities found by each technique for each type of vulnerability, using the CWE
number and associated OWASP Top Ten as the type. Vulnerability count is com-
monly used in both academia and industry as a measure of security risk [60]. We
use vulnerability counts and types to answer RQ1.

9 Methodology for RQ2 - Eﬃciency

For RQ2, we address the question How does the reported eﬃciency in terms of
vulnerabilities per hour diﬀer across techniques?. To reduce the bias that could
be introduced by a high-performing or low-performing participant, we cannot rely
on results from a single individual or team [45]. Using data from a graduate level
security course worked well for three reasons. First, we have a wide participant
pool. Second, the students are all required to perform exactly the same tasks,
reducing the amount of external factors that could inﬂuence our results. Third,
graduate students can be assumed to have some existing knowledge in computer
science.

9.1 Data Collection

We collected eﬃciency scores recorded by students (S) as shown in Table 1 which
we discuss in Section 9.1.1. We then needed to perform data cleaning, as we discuss
in Section 9.1.2 before the data could be analyzed.

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

27

9.1.1 Recording Eﬃciency

To quantify eﬃciency, we started with information provided by the students (S)
as shown in Table 1. As discussed in Section 7.2, the students worked in Teams
of 3-4 to apply SMPT, EMPT, DAST, and SAST to OpenMRS as part of their
course project. The students were given the assignments described in Section 7.2.2
which appear verbatim in Appendix C. We do not know VpH at the student level
for SMPT, DAST, and SAST since these assignments were only reported at the
team level. However, students were allowed to work independently for EMPT and
while some teams appear to have coordinated eﬀorts within the team, e.g. team
members seem to have looked at diﬀerent parts of the system and only reported
unique vulnerabilities, others did not, e.g. team members examined the same parts
of the system and multiple team members reported the same vulnerability. Hence
we use the average VpH across all participating members of each team for EMPT.
For RQ2 we exclude data from students whose team members did not participate
in the study as as discussed in Section 7.2.

9.1.2 Data Cleaning

Once we have collected the data, as shown in Table 1 the researchers (R) per-
formed data cleaning as needed. We used trimming[101, 47] to remove outliers.
We formally identiﬁed outliers for each technique using the median absolute de-
viation and median (MADN)[101, 47], applying MADN to the VpH scores for
each technique. We removed outliers where the MADN was higher than 2.24, the
threshold recommended in the literature[101, 48]. This data cleaning was needed
to systematically identify and remove cases where students did not correctly fol-
low the assignment to the extent that it impacted our analysis. For example, one
team reported spending only 32 minutes on the SAST assignment described in
Section 7.2.2 in contrast with the second-fastest team who spent 7.5 hours on the
assignment. We detected and removed only four outliers, one in each technique.

9.2 Data Analysis

With outliers removed, we retained 12 eﬃciency scores for each technique. We
performed a statistical comparison to determine if the average eﬃciency across
the groups for each technique was higher or lower than the average eﬃciency
for other techniques. We ﬁrst applied the Shapiro-Wilk test [86] to the data for
each technique to assess normality. Based on the output of the Shapiro-Wilk test,
we used Bartlett’s test for homogeneity of variance [10]. For our case study, the
trimmed data was normal but the variance diﬀered across techniques. We then
apply the Games-Howell test [35, 45] to perform pairwise comparison across the
diﬀerent vulnerability detection techniques and determine which techniques were
diﬀerent. The Games-Howell test adjusts the p-value for multiple comparisons.

10 Methodology for RQ3 - Other Factors

Once the students had experience with the four vulnerability detection techniques,
the students (S) were asked to reﬂect on their experiences with each technique

28

Elder et al.

and to compare the techniques as part of the “comparison assignment” described
in Section 7.2.2. Students were instructed to discuss tradeoﬀs between the tech-
niques, and “ Based upon your experience with these techniques, compare their
ability to eﬃciently and eﬀectively detect a wide range of types of exploitable vul-
nerabilities.” as shown in Appendix C. The student responses to the comparison
assignment were the source documents for RQ3 as shown in Table 1. The com-
parison assignment was answered at the team-level, and so the data used for RQ3
excludes students whose teammates did not agree to participate in the study. Two
researchers (R) performed qualitative analysis on the student responses to under-
stand what other factors may distinguish the diﬀerent techniques. One researcher
segmented the text by sentence, but left the sentences in order, to retain key
contextual information. Both researchers independently coded each segment using
“open coding”[18]. The researchers found that more than one code could apply
to the same sentence. The researchers then compared and discussed their results.
One researcher further standardized the codes and determined which codes were
mentioned by more than one response. The resulting information was primarily
used to understand the results of RQ1 and RQ2, but may also be informative for
future work.

11 Equipment

We faced several equipment constraints. OpenMRS could be run with relatively
low resources such as CPU, memory, and disk space. However, the tools used for
SAST and DAST were more resource-intensive. Additionally, for the course from
which we collected student data, all 70 students needed independent access to the
SUT as part of their coursework, such that students would not interfere with each
others’ work as they attempted to hack into the SUT. The equipment used to
apply each technique to answer RQ2 and RQ3 was the same for all techniques. We
used the Virtual Computing Lab19 (VCL) at our university, North Carolina State
University (NCSU)20. VCL provided virtual machine (VM) instances. Researchers
created a system image including the SUT (OpenMRS) as well as SAST and DAST
tools. An instance of the image could be checked out by students or researchers
and accessed remotely. Additional resources were needed when answering RQ1 to
improve system coverage, including larger VCL instances, Virtualbox VMs based
on the VCL images, and a large desktop machine with 24 CPUs, 32G RAM, and
500G disk space. Additional information on the systems is in Appendix D.

12 Results

In this section, we describe our results. Table 2 provides a high-level summary
of our numeric results for RQ1 - What is the eﬀectiveness, in terms of number
and type of vulnerabilities, for each technique? and RQ2 - How does the reported
eﬃciency in terms of vulnerabilities per hour diﬀer across techniques?. Detailed
results for RQ1 and RQ2 are provided in Sections 12.1 and 12.2 respectively. We

19https://vcl.apache.org
20https://vcl.ncsu.edu

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

29

provide our qualitative results for RQ3 - What other factors should we consider
when comparing techniques? in Section 12.3.

Table 2: Results Summary

Eﬀectiveness:
Vulnerabilities found

Eﬀectiveness:
# OWASP Top Ten Coveredc
Eﬃciency:
Average VpH

SMPT

EMPT

DAST

SAST

37

9

185

7

23

7

823

7

0.69d

2.22

0.55d

1.17

cOne category within the OWASP Top Ten is outside the scope of the tools in this study.

Maximum possible coverage is 9 out of 10.

dThe diﬀerence in eﬃciency between SMPT and DAST is not statistically signiﬁcant.

12.1 RQ1 - Technique Eﬀectiveness

In this section, we discuss the results for our question What is the eﬀectiveness, in
terms of number and type of vulnerabilities, for each technique? First, we include
information speciﬁc to tool-based techniques: the agreement of researchers when
reviewing the output of vulnerability detection tools, and the tools’ false positive
rates. We then provide the number of vulnerabilities discovered by each technique,
as well as the types of vulnerabilities identiﬁed by each technique as classiﬁed using
the CWE.

12.1.1 True and False Positive Tool Alerts

We examined two tool-based techniques in this study, SAST and DAST, for which
we can calculate the precision of the tools. As noted in Section 8.1.2, for tool-
based techniques, two individuals classiﬁed each alert produced by the tool as true
or false positive. We calculated the inter-rater reliability and present the results
in Section 12.1.1.1. The reviewers discussed the results with a third reviewer,
who assisted in resolving disagreements, to create a set of true and false positive
determinations which could be used to determine the number of false positives
and tool precision presented in Section 12.1.1.2 and shown in Table 3.

12.1.1.1 Reviewer Agreement We calculate the inter-rater reliability of the review-
ers for SAST and DAST using Cohen’s Kappa [15]. Inter-rater reliability measures
how much two reviewers agree. Statistics such as Cohen’s Kappa also account for
agreement due to chance, measuring the extent to which reviewers agree beyond
whatever agreement would be expected due to chance. In a classiﬁcation of two
ratings such as true and false positive, if one of the ratings applies to an extremely
high percentage of cases (e.g. 98%) while the other rating applies to an extremely
small percentage of cases (e.g. 2%), the probability of agreement due to chance is

30

Elder et al.

estimated to be very high. The high estimated probability of agreement can lead
to a paradox where reviewers who have high observed agreement, in other words
- they apply the same rating to most of the objects being rated, but have low
inter-rater reliability [29, 14, 31].

We observe the paradox of high observed agreement but low inter-rater relia-
bility for Sonarqube and SAST-2. Of the 698 alerts for Sonarqube, we calculate
Cohen’s Kappa on 693 alerts that were independently reviewed. One of the two
reviewers found 12 of the 693 reports to be false positives, while the other reviewer
did not consider any of the reports to be false positives. A third researcher reviewed
the results and resolved disagreements for a ﬁnal set of 4 false positives and 694
true positives out of the original 698 alerts. Based on the true/false positive clas-
siﬁcations of the ﬁrst two reviewers, the expected agreement, as estimated when
calculating Cohen’s Kappa, is 98.3% which is identical to the observed agreement
of 98.3% with a resulting Cohen’s Kappa of 0 (95% conﬁdence interval ±0). Sim-
ilarly, the Cohen’s Kappa for SAST-2 is 0.22 (95% conﬁdence interval ±0.40), in
spite of a high observed agreement of 93.1%. For SAST-2, the two reviewers met
to discuss and resolve disagreements, while the third researcher participated in
the discussion with a ﬁnal false positive count of 16 out of 264 total alerts. These
Kappa scores are low. However, given that our ﬁnal false positive count for Sonar-
qube was only 4 of 698 total alerts and our ﬁnal false positive count for SAST-2
was 16 of 264 alerts, even with dozens of reviewers, we may not be able to increase
the inter-rater reliability statistics. Observed agreement between reviewers may
never be above the expected agreement, considering that the expected agreement
for Sonarqube was 98.3%.

Another way to examine the paradox is to consider that the reviewers agreed
with the tool as frequently as they agreed with each other. While the reviewers
had low inter-rater agreement as analyzed using the Cohen’s Kappa statistic, they
had high agreement in terms of the percentage of alerts on which they agreed upon
the classiﬁcation, with 98.3% agreement for Sonarqube and 93.1% for SAST-2. In
both cases, the reviewer’s observed agreement with the tool was as high with their
agreement with each other, with observed agreement for each of the Sonarqube
reviewers and the tool at 98.3% and 100%. Observed agreement for each of the
SAST-2 reviewers and the tool was at 94.3% and 96.6%.

The agreement for DAST tools was much less complicated. Both researchers
reviewed all alerts from OWASP ZAP, since there were fewer alerts than for DAST-
2. The inter-rater reliability for OWASP ZAP alerts was 0.97 (95% conﬁdence
interval ±0.28). The output of OWASP ZAP was reviewed after the output of
DAST-2, and the researchers may have been more familiar with the process. The
inter-rater reliability for the 288 DAST-2 alerts reviewed by two individuals was
0.78 (95% conﬁdence interval ±0.16), which was above the recommended minimum
cutoﬀ of 0.70 [51, 100]. Only one researcher reviewed the remaining alerts, as
described in Section 8.1.2.

12.1.1.2 True / False Positives and Precision Table 3 shows the Total Alerts (Tot.
Alrt.), False Positives (FP), and Precision (Prec.) for all tools in the current study,
where the SUT was OpenMRS (M). Subtable 3.a provides the Tot. Alrt., FP, and
Prec. for DAST, while Subtable 3.b provides the Tot. Alrt., FP, and Prec. for
SAST. In each subtable, the Total (M) column for Tot. Alrt. and for FP is the
sum of the alerts and false positives from both tools applied to OpenMRS in this

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

31

study. The Prec. row of the Total (M) column is the precision calculated based on
all alerts from the two tools applied to OpenMRS for each technique. Table 3 also
shows the Total Alerts (Tot. Alrt.), False Positives (FP), and Precision (Prec.)
from the work by Austin et al. [7, 8], as we will describe Section 12.1.1.3.

The precision of most SAST and DAST vulnerability detection tools in the
current study was relatively high. The precision of Sonarqube and SAST-2 was 0.99
and 0.94, respectively, and the precision across the combined alerts for Sonarqube
and SAST-2 at 0.98. The precision of OWASP ZAP was also high at 0.95. However,
the precision of DAST-2 was 0.09, resulting in 0.23 precision across all DAST alerts.

Table 3: Total Alerts (Tot. Alrt.), False Positives (FP), and Precision (Prec) for

the Current Study Compared with Austin et al.

M indicates OpenMRS, E indicates OpenEMR, T indicates Tolven, and P indicates PatientOS

(a) DAST

(b) SAST

Current Study

Austin et al.

Current Study

Austin et al.

Total
(M)

ZAP
(M)

DA-2
(M)

Total
(E)

Total
(T)

Total
(M)

Sonar
(M)

SA-2
(M)

Total
(E)

Total
(T)

Total
(P)

Tot.
Alrt.

3414

550

2862

735

FP

2612

28

2597

25

37

15

Tot.
Alrt.

FP

962

698

264

5036 2315 1789

20

4

16

3715 2265 1644

Prec.

0.23

0.95

0.09

0.97

0.59

Prec.

0.98

0.99

0.94

0.26

0.02

0.08

We examined possible reasons for the low precision of DAST-2. Table 4 shows
the DAST alert counts for each CWE type originally assigned by the tool based on
the test case or check that triggered the alert. In Table 4 we use the abbreviation
Tot. Alrt. for total alerts, TP Alrt. for true positive alerts, FP Alrt. for false
positive alerts, and # Vuln. for number of vulnerabilities.

More than one alert can point to the same vulnerability as discussed in Sec-
tion 8.1.3. For example, as can be seen in Table4, two (2) true positive alerts (TP
Alrt.) found by DAST-2 were assigned by the tool to CWE-89 SQL Injection 21.
One of the TP alerts assigned to CWE-89 was a “duplicate” of the other, pointing
to the same vulnerability; resulting in a vulnerability count of 1 in the # Vuln
column for alerts with Tool-Assigned CWE-89.

When an alert correctly provided an indicator of a unique vulnerability, such
as the malformed input from a DAST tool used to trigger the alert, the CWE
provided by the tool did not always match the type of vulnerability found. If the
CWE type assigned by the tool was not the same as the type of vulnerability
found, we classiﬁed the alert as True Positive and reassigned the CWE type as
described in Section 8.1.1.3. The Tool-Assigned CWE, shown in the ﬁrst column
of Table 4 is the CWE value provided by the tool. The ﬁnal CWE types of the
vulnerabilities found, reviewed and reassigned if necessary, are listed in the “Final
CWE” column. Of the alerts originally assigned to CWE-89 SQL Injection by
DAST-2, the researchers determined that the alerts revealed an http message where
sensitive patient information was visible in the URL. However, the researchers

21https://cwe.mitre.org/data/definitions/89.html

32

Elder et al.

could not perform SQL injection with the test case corresponding to the alert.
Consequently, the vulnerability was reassigned CWE-598 Use of GET Request
Method With Sensitive Query Strings

An alert could also be a duplicate of another alert with a diﬀerent tool-assigned
CWE. For example, one of the Tool-Assigned CWE-352 alerts was a duplicate of
an alert whose Tool-Assigned CWE was 77, but where the researchers agreed that
CWE-20 Improper Input Validation 22 was more accurate. The second TP tool-
assigned CWE-352 alert was a duplicate of a CWE-79 Cross Site Scripting (XSS)
alert with Final CWE-79. Since both alerts were duplicates of alerts with other
Tool-Assigned CWEs, the # Vuln column in Table 4 for DAST-2 Tool-Assigned
CWE-352 alerts is 0, even though there were 2 TP alerts. For rows where there
were no TP alerts and therefore no vulnerabilities, we leave the # Vuln column
blank, consistent with other tables.

The Tool-Assigned CWE was linked to the rules used to create the malformed
input or test case that triggered each alert. Of the Tool-Assigned CWEs in Table
4, only CWE-16 and CWE-79 were associated with more than one rule or check
by OWASP ZAP. CWE-16 was associated with 3 rules by OWASP ZAP, while
CWE-79 was associated with two distinct rules. As we can see in Table 4, for
OWASP ZAP, the alerts that had been assigned a given CWE were either all
true positive or all false positive, except for CWE-79. Within CWE-79, the 4 true
positive alerts were all from one of the two rules, while the 3 False Positive alerts
were all associated with the other rule. For every DAST-2 rule, either all alerts
were true positive or all alerts were false positive.

All Tool-Assigned CWEs in Table 4 were associated with only one rule or
check by DAST-2. We can see that the most frequently occurring CWE type, and
therefore rules used to create the malformed inputs, is CWE-35 Path Traversal
which accounted for 2537 of all DAST-2 alerts. 2484 of the Path Traversal alerts
are false positives, and the remaining 53 alerts were all reclassiﬁed as other CWE
types. If we exclude alerts for CWE-35, the precision of DAST-2 goes from 0.09 to
0.69. While still lower than the precision for the other tools, the improved precision
without Tool-Assigned CWE-35 alerts suggests that further customization such as
updating or removing rules that do not accurately model the SUT may be able to
improve the performance of DAST-2.

12.1.1.3 Tool False Positives and Precision Comparison with Austin et al. Table
3 also shows the tool precision reported by Austin et al. for comparison with the
current study. As described in Section 3, the SUT examined by Austin et al. were
OpenEMR (E), Tolven eCHR (T), and PatientOS (P). Austin et al. used a single
tool for each technique. PatientOS is not included in the DAST results since the
DAST tool used by Austin et al. was not applicable to PatientOS.

As seen in Table 3, the SAST tool applied to the SUT in the previous study
had much lower precision than the tools and SUT examined in the current study.
The highest precision in the previous study for SAST was 0.26, as compared with
the lowest precision of 0.94 in the current study. The high precision we observed
may be part of greater trends in SAST tools as seen in the recent NIST SAMATE
project’s regular Static Analysis Tool Expositions (SATE) [23] where the precision

22https://cwe.mitre.org/data/definitions/20.html

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

33

Table 4: DAST Alerts

OWASP ZAP

Tool-Assigned CWE

Tot.
Alrt.

TP
Alrt.

FP
Alrt.

Final
CWE

#
Vuln.

Tot.
Alrt.

TP
Alrt.

DAST-2
FP
Alrt.

Final
CWE

#
Vuln.

Total

550

522

28 N/A

16 - Conﬁguration

262

262

16

12

3

2862 265 2597 N/A

14

35 - Path Traversal

77 - Command Injection

2537

53 2484 79, 209,

613

2

1

1

20

79 - Cross-site Scripting

7

4

3

79

3

307

207 100

79, 20,
209, 598

8

2

6

598

1

1

10

1

89 - SQL Injection
120 - Buﬀer Overﬂow
134 - Use of Externally-
Ctrl. Format String
200 - Exposure of
Sensitive Info. to an
Unauth. Actor
326 - Inadequate
Encryption Strength
345 - Insuf. Veriﬁcation
of Data Authenticity
352 - Cross-Site Req.
Forgery (CSRF)
472 - External Ctrl. of
Assumed-Immutable
Web Param.
548 - Exposure of Info.
Through Dir. Listing
933 - Security
Misconﬁguration

21

1

21

1

68

68

14

14

11

11

51

51

3

3

1

1

111

111

7,
548

326

345

352

548

933

1

1

1

1

1

1

8

2

6

20, 79

0

and recall of SAST tools for Java was far higher than the precision and recall
reported in previous work [7, 8].

Austin et al. had similar results to our current study using DAST. In Austin
et al.’s work, the DAST tool when applied on OpenEMR had a precision of 0.97.
When applied to Tolven eCHR, the DAST tool used by Austin et al. only had
a precision of 0.59. Austin et al. [7, 8] also found that entire categories of alerts
could be labeled true or false positive, similar to the results shown in Table 4. The
impact of not customizing tool rules on performance measures such as precision
may be more apparent as tools become more advanced and precise.

In the current section (12.1.1.3) we have compared our work to Austin et al.
on tool-based measures. Comparison with Austin et al. on eﬀectiveness measures
applicable to all four techniques may be found in Section 12.1.5. Comparison with
Austin et al. on eﬃciency measures may be found in Section 12.2.3.

12.1.2 Number of Vulnerabilities

Overall, SAST found the most vulnerabilities, at 824 vulnerabilities. EMPT found
the second most vulnerabilities, with 229 vulnerabilities. We provide further in-

34

Elder et al.

formation on the number of vulnerabilities found using each technique and tool in
Table 5. The main results for each technique are shaded gray, while white-shaded
columns indicate the results for each of the DAST and SAST tools independently.
In the ﬁrst row of Table 5, we provide the number of “True Positive (TP)
Failures”. For SMPT, these are failing test cases; for DAST and SAST the “True
Positive Failures” are true positive alerts. We do not have a true positive failure
count for EMPT comparable to the failing test cases from SMPT or true positive
alerts from DAST and SAST. Unlike SMPT where failing test cases could be
assumed to be true positive since poorly written test cases had been removed,
EMPT results required additional quality review. We mark the number of true
positive failures for EMPT to be Not Applicable (N/A) in Table 5. The “Total”
column for DAST and SAST “True Positive Failures” is the sum of all true positive
alerts for the technique.

The second row of Table 5 shows the total number of vulnerabilities indicated
by the failures, while the third and fourth rows show how many of these vulnera-
bilities were more severe and less severe, respectively. The vulnerability counts are
determined by applying our counting rules described in Section 8.1.1.2, with sever-
ity assigned according to the guidelines in Section 8.1.1.4. The same vulnerability
could be found by both SAST tools or both DAST tools. Hence the “Total” col-
umn for SAST and DAST vulnerabilities is the total vulnerabilities found by the
technique and not simply the sum of the vulnerability counts for each tool. How-
ever, a vulnerability cannot be both “more severe” and “less severe” so the “Total
Vulnerabilities” row is the sum of the more severe and less severe vulnerabilities.
We found the most vulnerabilities using SAST with 824 total vulnerabilities, fol-
lowed by EMPT with 229 total vulnerabilities. We found 38 vulnerabilities using
SMPT, and 25 vulnerabilities using DAST.

The ﬁfth row of Table 5 is the ratio between the number of TP Failures and the
total number of vulnerabilities (row 2). The ratio of TP Failures to Vulnerabilities
varies across techniques, but is higher for DAST (32.08) than for SMPT (1.58)
and SAST (1.12). We discuss the implications of this ratio in Section 14.

The sixth row of of Table 5, labeled “Vuln. Unique to Tech./Tool”, shows
the number of vulnerabilities that were only found by each technique or tool. It
may be helpful to consider the “Vuln. Unique to Technique/Tool” as “the num-
ber of vulnerabilities we would have missed if we had not used this technique or
tool”. Similar to the Total Vulnerabilities for SAST and DAST in row 2, the Total
columns for SAST and DAST in row 3 indicate the count of vulnerabilities found
only by the technique. One (1) vulnerability was found by all techniques, including
both DAST tools and one of the two SAST tools. Speciﬁcally, the fact that our
instance of OpenMRS was conﬁgured such that the default server errors, e.g. 500
errors, revealed sensitive information about the system, which was associated with
CWE-7 J2EE Misconﬁguration: Missing Custom Error Page 23. Ten (10) vulnera-
bilities were found by both of the SAST tools, but by no other technique. The 10
vulnerabilities found by both tools are included in the 822 vulnerabilities that were
only found by SAST, but not in the vulnerabilities only found by Sonarqube and
SAST-2 independently. Similarly, 1 vulnerability was found by both DAST tools
but not by other techniques. The vulnerability found only by both DAST tools is
included in the Total Vuln. Unique to DAST, but not the Vuln Unique to OWASP

23https://cwe.mitre.org/data/definitions/7.html

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

35

ZAP or DAST-2 independently. Each technique or tool found vulnerabilities that
were not found using other techniques and tools.

Table 5: Vulnerability Counts

SMPT EMPT

DAST
(Total)

ZAP DA-2

SAST
(Total)

Sonar SA-2

True Positive (TP) Failures

Total Vulnerabilities

Ratio:

T P F ailures
T otalV ulnerabilities

60

37

N/A

787

522

265

185

23

12

13

948

823

694

254

598

235

1.58

N/A

34.22 43.50 20.38

1.12

1.16

1.05

Vuln. Unique to Tech./Tool

11

157

13

8

4

822

588

225

12.1.3 Vulnerability Severity

As discussed in Section 8.1.1.4, we reviewed more severe vulnerabilities where
the same tool or technique found greater than 20 vulnerabilities associated with
the same CWE. First were 233 vulnerabilities found using SAST associated with
CWE-52 Cross-Site Request Forgery 24 labeled as at least “High” severity by the
SAST tools. The vulnerabilities found by SAST were all functions in source code
which mapped to HTTP Requests where input parameters were not suﬃciently
restricted. For example, 220 of these functions used an @RequestParameter map-
ping but did not specify which methods (Post, Get, etc) could be used to call
the function. Not specifying which types of requests can be used can result in
additional access being granted where it otherwise would not be, and the lack of
a method parameter can be particularly problematic if the application is using
CSRF protection mechanism25. The base application does not employ CSRF pro-
tection, but the OpenMRS team is working to employ better CSRF protection.
However, the “High” or higher severity assigned by SAST tools contrasts with
the single vulnerability associated by the DAST tools with CWE-352 Cross-Site
Request Forgery 26, was labeled as “Low” severity by the DAST tool. Speciﬁcally,
the DAST tool noted the lack of CSRF protection mechanisms in the version of
the application we were using. The risks posed by each individual vulnerability
are low. Hence, we classiﬁed the CSRF vulnerabilities found by the SAST tools as
“less severe”.

The second-largest group of more severe vulnerabilities found by a single tool
or technique were 100 vulnerabilities found using EMPT associated with CWE-
79 Cross-Site Scripting 27. An example XSS vulnerability would be if a ﬁeld in
a patient intake form accepts and saves the value <script>alert(1);</script>,
if the script is then executed when the user returns to the patient information
page where the information from the intake form would otherwise be displayed

24https://cwe.mitre.org/data/definitions/52.html
25https://docs.spring.io/spring-security/site/docs/5.0.x/reference/html/csrf.

html

26https://cwe.mitre.org/data/definitions/352.html
27https://cwe.mitre.org/data/definitions/79.html

36

Elder et al.

this is considered Cross-Site Scripting. The XSS vulnerabilities found via EMPT
were all found within a short period of time by students. We consider the risk of
exploitability to be high and leave the classiﬁcation of the vulnerabilities associated
with CWE-79 as “more severe”.

Third, SAST tools, speciﬁcally SAST-2, found 56 vulnerabilities CWE-404
Improper Resource Shutdown or Release 28. Of these 56 vulnerabilities, 39 were
considered more severe while 17 were considered less severe. All 56 vulnerabilities
were instances where a database connection or other resource could potentially
be left open for certain executions of the code. The 17 less severe issues were
considered to be on an “exceptional” execution path that the tool considered less
likely to be executed, e.g. if an secondary failure happened on an unusual path
within nested try-catch-ﬁnally blocks. The 39 more severe vulnerabilities were
more likely to be executed system, e.g., if a connection was not inside a try-
catch block at all in a function where an error is explicitly thrown under certain
conditions.

Furthermore, after discussion with OpenMRS, we determined that some types
of vulnerabilities were low priority for their organization in the context of the
application. Speciﬁcally, a number of vulnerabilities involved errors which revealed
potentially sensitive information about application source code. Since the tool is
open-source, the threat posed by these vulnerabilities is minimal. Vulnerabilities
associated with error messages that reveal too much information about the system
are also classiﬁed as “less severe”.

12.1.4 Vulnerability Type (OWASP Top Ten)

Table 6 shows the distribution of the vulnerabilities found by each technique ac-
cording to the OWASP Top Ten 2021 Categories. The Top Ten Category assign-
ments are based on the CWEs assigned to the vulnerabilities, using the mapping
to the OWASP Top Ten provided by CWE [58], as discussed in Section 8.1.1.3.
The vulnerability counts for the speciﬁc CWE types within each Top Ten cate-
gory is available in Appendix E. The leftmost column of Table 6 indicates the
OWASP Top Ten Category. Columns two through ﬁve indicate the vulnerabilities
that were found for each technique. Column six of Table 6 shows the total vulnera-
bilities found within each Top Ten category across all techniques. Within each cell,
the ﬁrst value indicates the number of more severe vulnerabilities that were found
in the OWASP Top Ten Category when using each technique. The second value
(in parentheses) indicates the total vulnerabilities, including both more severe and
less severe vulnerabilities, found in the corresponding OWASP Top Ten Category
when using each technique.

This study is an evaluation of techniques used to ﬁnd vulnerabilities that devel-
opers may have inserted themselves. Tools for ﬁnding vulnerabilities in third-party
code, such as Software Composition Analysis (SCA) tools were excluded from our
analysis. As can be seen in Table 6 of the tools examined found vulnerabilities in
the OWASP Top Ten Category for Vulnerable and Outdated Components (A06),
further suggesting that diﬀerent techniques and categories of techniques are useful
for ﬁnding diﬀerent types of vulnerabilities.

28https://cwe.mitre.org/data/definitions/404.html

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

37

Table 6: More Severe Vulnerability Count based on OWASP Top Ten (2021)
(Total count, including both more and less severe vulnerabilities)

OWASP Top Ten (2021) Category

SMPT EMPT DAST SAST

A01:2021 - Broken Access Control

A02:2021 - Cryptographic Failures

A03:2021 - Injection

A04:2021 - Insecure Design

A05:2021 - Security
Misconﬁguration

A06:2021 - Vulnerable and
Outdated Components

A07:2021 - Identiﬁcation and
Authentication Failures

A08:2021 - Software and Data
Integrity Failures

A09:2021 - Security Logging and
Monitoring Failures

A10:2021 - Server-Side Request
Forgery (SSRF)

No Mapping to OWASP Top Ten

2e
(2)

1
(1)

5
(5)
5e
(7)

2
(5)

13
(13)
1f
(1)

3
(3)
1f
(1)

1
(1)

15
(15)

1
(1)

119
(119)

8
(26)

2
(4)

10
(10)

9
(9)

1
(1)

(1)

1
(1)

11
(11)

1
(2)

2
(6)

1
(1)

(1)

28
(261)

2
(4)

24
(58)

8
(36)
14g
(15)

2
(2)

10
(11)

Total
Found

58e
(292)

3
(6)

150
(184)
27e
(73)
19g
(23)

0

17
(17)
11f
(13)

12
(12)
1f
(1)

54g
(436)

142
(823)

56g
(438)

329
(1033)

Total for Technique

32
(37)

165
(185)

17
(23)

eOne more severe vulnerability found using SMPT was mapped to both A01 and A04

through two diﬀerent CWEs.

fOne more severe vulnerability found using SMPT was mapped to both A08 and A10

through two diﬀerent CWEs.

g14 more severe vulnerabilities found using SAST were associated with two CWEs, one of
which mapped to A05 while the other CWE was not mapped to the OWASP Top Ten. We
only include these vulnerabilities under A05 since they cannot be described as “No Mapping”

SMPT was most eﬀective at ﬁnding vulnerabilities associated with Identiﬁca-
tion and Authentication Failures, ﬁnding more Identiﬁcation and Authentication
(A07) failures than any other vulnerability type and ﬁnding as many Identiﬁcation
and Authentication failures as EMPT. While SMPT found fewer vulnerabilities
than EMPT or SAST, most of the vulnerabilities found were more severe. Ad-
ditionally, SMPT identiﬁed at least one vulnerability in every Top Ten category
within scope of the tools in this study, providing better coverage of the Top Ten
as compared with the other techniques.

EMPT was one of the most eﬀective techniques at ﬁnding severe vulnerabili-
ties for the Broken Access Control (A01), Injection (A03), Insecure Design (A04),
Identiﬁcation and Authentication Failures (A07), and Security Logging and Mon-
itoring Failures (A09) categories. EMPT was notably eﬀective at ﬁnding Injection

38

Elder et al.

vulnerabilities, ﬁnding 119 of the 150 more severe Injection vulnerabilities in Open-
MRS.

DAST was most eﬀective at ﬁnding Injection (A03) vulnerabilities relative to
other categories of vulnerability. However, DAST found fewer injection vulnera-
bilities than EMPT and SAST, having found the least number of vulnerabilities
overall in our study.

SAST was the most eﬀective at ﬁnding vulnerabilities associated with Secu-
rity Misconﬁguration (A05). However, the Security Misconﬁguration vulnerabili-
ties found by SAST were not the same as the Security Misconﬁguration vulnera-
bilities found by other techniques. Overall, only one vulnerability found by other
techniques was found by SAST in entire dataset. Hence SAST should not be seen
as something that can substitute for other techniques, or be substituted for by
other techniques. While many of the SAST vulnerabilities were marked as “less
severe”, all of the less severe SAST vulnerabilities except the CSRF vulnerabilities
classiﬁed as described in Section 8.1.1.4 were marked as low severity by the tools
themselves. Between the two SAST tools there were also diﬀerences in the types
vulnerabilities found. Although there were at least 184 total Injection vulnerabil-
ities in OpenMRS, as can be seen in Tables 11 and 12 of Appendix E, the 58
Injection vulnerabilities found using SAST, 24 of which were more severe, were
found by SAST-2. Sonarqube did not ﬁnd any Injection vulnerabilities.

12.1.5 Eﬀectiveness Comparison with Austin et. al.

A comparison with the previous study by Austin et al. [7, 8] of vulnerability counts
for each vulnerability type is shown in Table 7. The ﬁrst column of Table 7 indicates
the technique (Tech.), the second column of Table 7 indicates whether the data is
from the current study or Austin et al. The third column of Table 7 indicates the
SUT. As with previous tables, M indicates OpenMRS, the SUT from the current
study. E indicates OpenEMR, T indicates Tolven, and P indicates PatientOS,
the three SUT from Austin et al. The total vulnerability count calculated for
each row is provided in the ﬁnal TOTAL (TOT) column. The remaining columns
indicate the vulnerability counts for each of the OWASP Top Ten categories. The
total row indicates the total number of vulnerabilities found in a particular study,
either the current study or Austin et al.[7, 8]. For the current study, the total
is equivalent to the results from OpenMRS as indicated in the table. For Austin
et al., the total is the sum of the vulnerabilities found across all three SUT. In
Table 7, the row for the current study is shaded in the darkest gray, the total row
from the Austin et al. study is in the medium gray color, and the rows for the
individual SUT from Austin et al. are the lightest gray color. Austin et al. did not
specify how severity was evaluated in their study, and vulnerabilities such as error
messages containing sensitive information about the system (CWE-209) which
would have been classiﬁed as “less severe” in our current study were included in
their vulnerability counts. We assume that the Austin et al. counts reported[7, 8]
include less severe vulnerabilities. Consequently, the vulnerability counts from the
current study in Table 7 also include those that are less severe. We highlight
insights from Table 7 that we ﬁnd most interesting below.

SMPT in the current study was similar in eﬀectiveness to the study by Austin
et al.[7, 8]. Austin et al. found more vulnerabilities in OpenEMR using SMPT
as compared with the current study, but a similar number of vulnerabilities in

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

39

Table 7: Vulnerability Type Comparison with Austin Study

M indicates OpenMRS, E indicates OpenEMR, T indicates Tolven, and P indicates PatientOS

s
e
r
u

l
i
a
F
n
o
i
t
a
c
i
t
n
e
h
t
u
A
d
n
a

n
o
i
t
a
c
ﬁ
i
t
n
e
d
I

s
e
r
u

l
i
a
F

y
t
i
r
g
e
t
n
I

a
t
a
D
d
n
a

e
r
a
w
t
f
o
S

s
e
r
u

l
i
a
F

g
n
i
r
o
t
i
n
o
M
d
n
a

g
n
i
g
g
o
L

y
t
i
r
u
c
e
S

)
F
R
S
S
(

y
r
e
g
r
o
F

t
s
e
u
q
e
R
e
d
S
-
r
e
v
r
e
S

i

s
t
n
e
n
o
p
m
o
C
d
e
t
a
d
t
u
O
d
n
a

e
l
b
a
r
e
n
u
V

l

n
o
i
t
a
r
u
g
ﬁ
n
o
c
s
i

M
y
t
i
r
u
c
e
S

n
e
T
p
o
T
o
t

d
e
p
p
a
M

t
o
N

L
A
T
O
T

l
o
r
t
n
o
C
s
s
e
c
c
A
n
e
k
o
r
OWASP Top Ten Descr. B

s
e
r
u

l
i
a
F

c
i
h
p
a
r
g
o
t
p
y
r
C

n
g
i
s
e
D
e
r
u
c
e
s
n
I

n
o
i
t
c
e
j
n
I

OWASP Top Ten # A01 A02 A03 A04 A05 A06 A07 A08 A09 A10 NM TOT

Tech.

Study

SUT

Curr. Study M (Total)

Austin et al.

Total
E
T
P

2
4
3
1

Curr. Study M (Total)

15

Austin et al.

Total
E
T
P

Curr. Study M (Total)

Austin et al.

Total
E
T

1
502
485
17

Curr. Study M (Total) 261
93
36
13
44

Total
E
T
P

Austin et al.

T
P
M
S

T
P
M
E

T
S
A
D

T
S
A
S

1
3
2

1

1
1

1

2

4

8
4
2
2

26
1
1

2

5
17
16

1

119
8
8

11
221
221

5

4

6
9
4
5

58

36
1190 124
1155 122

35

2

15
1
1

1

13
9
3
4
2

10
1
1

1

1

2

11

3
99
37
29
33

9

22

22

1

1

2
2
2

37
136
63
36
37

185
13
12

1

23
732
710
22

436
86
7

79

822
1516
1321
50
145

Tolven and PatientOS. The distribution of the vulnerabilities across the OWASP
Top Ten categories diﬀers between studies. One possible explanation is diﬀerences
in the test suite. While the current study covered more of the OWASP Top Ten
categories, we are using the the 2021 Top Ten and the ﬁrst study by Austin
et al. was published in 2011. Some vulnerability types were less prevalent, and
some vulnerability types may have been considered less severe at the time of the
Austin et al. work, and vulnerability detection techniques of the time may not
have ensured coverage of any less prevalent or severe vulneraiblities. For example,

40

Elder et al.

Security Misconﬁguration (A05) which had no vulnerabilities found by Austin et
al., but ﬁve vulnerabilities found by the current study, was not included in the
OWASP Top Ten until 2013. Similarly, in the Austin et al. test suite which was
developed as part of a previous work[90], 58 of the 137 test cases (i.e. 42% of
the test suite) were targeted towards logging and auditing security controls. The
ASVS standard around which our test suite was built only has 2 level 1 controls
relating to logging, and only 5 test cases out of 131 (i.e. 4% of the test suite) were
related to auditing and logging. The higher number of logging related test cases
used by Austin et al. may help explain why Austin et al. were more eﬀective at
ﬁnding vulnerabilities associated with Security Logging and Monitoring Failures
(A09).

Comparing our results against Austin et al.[8, 7] for EMPT is more complicated
for methodological reasons. For DAST and SAST our methodology was compa-
rable to Austin et al. [8, 7] since the analysis for RQ1was done by a small team
of researchers in both cases. For SMPT, the procedure was also comparable as
indicated by the size of the test suite: 131 test cases in the current study, com-
pared with 137 per SUT for Austin et al.[8, 7]. The procedure for EMPT diﬀered
between the studies in order to take full advantage of the data generated by stu-
dents to better understand the strengths and weaknesses of EMPT. As we note in
Section 8.1, the 229 vulnerabilities in Table 5 for EMPT are the result of eﬀorts by
62 students as well as researcher review. Large numbers of individuals involved in
EMPT is not uncommon, for example with bug bounty programs [33]. However,
the use of smaller, internal teams such the 6-person team used to apply EMPT
to OpenEMR in Austin et al. [8, 7], or even individual hackers working alone on
EMPT as was done for Tolven and PatientOS are also not uncommon [2, 100].
The high number of students how applied EMPT for RQ1 in the current study
should not impact the distribution of vulnerabilities across types. However, more
participants may increased the number of vulnerabilities found and to enable com-
parison between the studies we must further evaluate individual eﬀectiveness for
EMPT.

Our results suggest that even at the individual level, the average individual
applying EMPT found more vulnerabilities in the current study as compared with
Austin et al. [8, 7]. In Austin et al. for OpenEMR a team of 6 individuals spent a
combined 30 hours performing EMPT. The team found 8 vulnerabilities in total, as
shown in Table 7, with a per-person average of 1.33 vulnerabilities per student. For
both Tolven and PatientOS, a single individual applied EMPT for 15 and 14 hours,
respectively. Austin et al. found no vulnerabilities using EMPT against Tolven and
only 1 vulnerability using EMPT against PatientOS, as shown in Table 7. In the
current study, EMPT was applied by 62 students, and reviewed by 3 researchers
for a total of 65 people involved in collecting EMPT data. We found 185 unique
vulnerabilities of which 165 were more severe, resulting in a per-student average
of 2.94 for all vulnerabilities and 2.62 for more severe vulnerabilities; the overall
vulnerabilities per-person was 2.85 for all vulnerabilities and 2.54 for the more-
severe vulnerabilities.

Austin et al. were more eﬀective with DAST, particularly against OpenEMR,
when compared against the current study. Austin et al. found 710 vulnerabilities
using DAST against OpenEMR and 22 vulnerabilities in Tolven, as compared
with 23 vulnerabilities found in OpenMRS in the current study. We suspect that
this diﬀerence may be due to diﬀerences in how counting rules are applied. The

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

41

number of true positive alerts appears to be the same or close to the total number
of vulnerabilities reported in the Austin et al. study[8, 7] and the terms “alert”
and “vulnerability” appear to be used interchangeably. While SAST would also be
impacted by any diﬀerences in counting rules, as reported in Table 5, in the current
study the ratio of alerts to vulnerabilities for DAST tools was 34.26 to 1 ratio. In
contrast, the ratio of alerts to vulnerabilities for SAST tools in the current study is
1.12 to 1. The lower ratio for SAST may help explain why the eﬀectiveness of SAST
in Austin et al.’s work is more similar to the eﬀectiveness of SAST in the current
study, as compared with the DAST results from each study. In OpenEMR, the
system where Austin et al. found the most vulnerabilities overall[7, 8], Austin et
al. were more eﬀective with SAST as compared to the current study. For Tolven and
PatientOS, Austin et al were less eﬀective with SAST as compared to the current
study. Austin et al. do not provide their counting rules, and so our hypotheses that
counting rules may contribute to the diﬀerences between the studies cannot be
conﬁrmed. We provide our current counting rules as well as references to how they
were derived to assist in future evaluations of vulnerability detection techniques.

RQ1 - What is the eﬀectiveness, in terms of number and type of vulnera-
bilities, for each technique?

Answer: SAST found the largest number of vulnerabilities overall. How-
ever, over half of the vulnerabilities identiﬁed by SAST were of low severity.
EMPT found the highest number of “more severe” vulnerabilities of any
technique. Furthermore, if any particular tool or technique had been ex-
cluded from the analysis, at least 4 and as many as 588 vulnerabilities
would not have been found.

12.2 RQ2 - Eﬃciency

In this section, we discuss the results for our question How does the reported
eﬃciency in terms of vulnerabilities per hour diﬀer across techniques? The data
was collected from students, as described in Section 9.

12.2.1 Data Cleaning

In analyzing the student eﬃciency, we noticed signiﬁcant outliers. The most ex-
treme outlier occurred for the SAST technique, where the outlier was 309% greater
than the second highest eﬃciency with any technique. As described in Section 9.1.2,
we use trimming [101, 47] to identify and remove outliers based on MADN. Each
category of vulnerability detection technique had one outlier. We removed the
four outliers from the dataset. Consequently, for each technique we analyzed 12
eﬃciency reports.

12.2.2 Data Analysis

Boxplots for each technique’s eﬃciency as well as the values for the median and
average are shown in Figure 7. EMPT had the highest eﬃciency (Median 2.4 VpH,

42

Elder et al.

Average 2.22 VpH). SAST had the second highest eﬃciency (Median 1.18 VpH,
Average 1.17 VpH). SMPT (Median 0.63 VpH, Average 0.69 VpH) and DAST
(Median 0.53 VpH, Average 0.55 VpH) were least eﬃcient for students.

Fig. 7: Vulnerability Detection Technique Eﬃciency

Table 8 shows the Games-Howell test results for comparing each pair of tech-
niques. As can be seen in the table, EMPT is statistically signiﬁcantly more ef-
ﬁcient than every other technique (p < 0.05 for all comparisons). SAST is the
second-most-eﬃcient technique (p < 0.05 when compared against both SMPT
and DAST). We observed no statistically signiﬁcant diﬀerence in the reported ef-
ﬁciency of SMPT when compared to the reported eﬃciency of DAST. In Table 8,
we round the p-values to the thousandths position. The p-value for the comparison
between DAST and EMPT was less than one thousandth.

Table 8: Games-Howell Eﬃciency t-test Results

Techniques

t-value

p-value

DAST - SMPT

0.15 (±0.29) 0.499

DAST - SAST

0.63 (±0.46) 0.006

DAST - EMPT

1.68 (±0.76) < 0.001

SAST - SMPT

−0.48 (±0.45) 0.034

EMPT - SAST

−1.05 (±0.81) 0.009

EMPT - SMPT

−1.53 (±0.75) < 0.001

12.2.3 Eﬃciency Comparison with Austin et al.

Table 9 shows the average and median VpH of this study, compared with the
VpH reported by Austin et al. For our study, we report the average and median
across the groups performing the task, as reported in Section 12.2.2. Austin et al.
calculated the amount of time it took, on average, to ﬁnd each vulnerability.

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

43

Table 9: VpH Compared to Austin et al.

Study

SUT

SMPT

EMPT

DAST

SAST

Current OpenMRS

Austin
et al.

OpenEMR

Tolven

PatientOS

0.69 (avg)
0.63 (median)

2.22 (avg)
2.43 (median)

0.55 (avg)
0.53 (median)

1.17 (avg)
1.18 (median)

0.55

0.94

0.55

0.40

0.00

0.07

71.00

22.00

N/A

32.40

2.78

11.15

The diﬀerences in eﬃciency are not only due to diﬀerences between student
performance and researcher performance. When researchers performed SAST and
DAST to answer RQ1, they also reported lower eﬃciency than Austin et al. When
applying SAST for RQ1, researchers’ eﬃciency was estimated at 18-32 VpH with
SAST, comparable to Austin et al. With DAST researchers were not much more
eﬃcient than the students with an estimated VpH of 0.34-1.8. DAST-2 had a
lower eﬃciency for the researchers, which may have impacted the results. However,
eﬃciency for ZAP was still less than reported in Austin et al. One possible cause
of the discrepancy, particularly with DAST eﬃciency, could be our focus in unique
vulnerabilities as deﬁned by our counting rules in Section 8.1.1.2 compared with
alerts or true positive failures. As shown previously in Table 5, DAST had the
highest ratio of true positive failures.

RQ2 - How does the reported eﬃciency in terms of vulnerabilities per hour
diﬀer across techniques?

Answer: EMPT was the most eﬃcient (2.22 VpH), followed by SAST
(1.17 VpH). SMPT (0.69 VpH) and DAST (0.55 VpH) were least eﬃcient.

12.3 RQ3 - Other Factors to Consider when Comparing Tools

We discuss the results of a qualitative analysis of student responses for RQ3. As
mentioned in Section 7.2, we were able to collect responses from 13 teams whose
members all consented to the use of their data in this study. Of the free-form
text responses used to answer RQ3, we discarded the response from one (1) team
since both reviewers considered the text to be confusing and self-contradictory.
Responses from 12 teams were analyzed. We use “at least” to describe how many
teams discussed certain ideas because there may be instances that were missed
due to human error, or where the author’s intent was unclear. Some teams dis-
cussed “automated” and “manual” categories of techniques rather than the further
subdivision used in the rest of this paper (i.e., EMPT and SMPT are “manual”
techniques, while SAST and DAST are “automated” techniques). We discuss our
results for RQ3 in terms of automated and manual techniques when those terms
are used by one or more teams.

44

Elder et al.

We focus on concepts that were not already covered in previous sections. Most
teams included some discussion of eﬃciency and eﬀectiveness, as well as the types
of vulnerabilities found. We summarize the three other concepts that were most
frequently mentioned by students.

12.3.1 Eﬀort

Summary: People do not like to do any more work than necessary. Eﬀort was one
of the most discussed topics by students. While eﬀort was exclusively seen as a dis-
advantage with manual techniques, discussions of eﬀort for automated techniques
had more mixed views.

Every response in some way mentioned human eﬀort beyond VpH. Automa-
tion itself is seen as an advantage for at least two teams, one of which explicitly
stated “Dynamic application security testing is better than manual blackbox test-
ing because you can automate the tests”. Eﬀort was perceived as a disadvantage
for SMPT and EMPT more than SAST and DAST. Eight (8) teams mentioned
eﬀort as a disadvantage of one or both of the manual techniques, while 0 teams
mentioned advantages relating to eﬀort for either of the manual techniques. In
contrast, for one or both automated techniques, 4 teams mentioned eﬀort as a dis-
advantage, 3 teams mentioned eﬀort as an advantage, and 2 teams mentioned both
advantages and disadvantages. The students’ focus on eﬀort and the frequency of
comments that eﬀort was a disadvantage of manual techniques with less focus on
automated techniques contrasts with the numeric data they reported. The numeric
eﬃciency of tool-based techniques in terms of VpH of eﬀort used to answer RQ2
was reported by the students themselves in the same assignment, generally on the
same page of their responses. The recorded numeric eﬃciency for DAST and SAST
was comparable to the numeric eﬃciency of SMPT and EMPT.

12.3.2 Time

Summary: Although the amount of time spent on an activity was a component
of our metric for eﬃciency (VpH), Time was discussed distinctly from eﬃciency.
Manual techniques were seen as requiring more time, while for automated tech-
niques some teams considered time an advantage while others considered time a
disadvantage. Additionally, students conjectured that the eﬃciency of each tech-
nique relative to the other techniques may change if the techniques were applied
over a longer timeframe.

Similar to Eﬀort, Time was frequently seen as a disadvantage for manual tech-
niques, particularly SMPT. At least 10 of the 12 teams mentioned time in some
way. Eight (8) teams explicitly mentioned time spent on manual tasks, while 5
of those teams also explicitly discussed the amount of time it takes for tools to
run, even though tool running time is not active time for the individual apply-
ing the technique. Additionally, 8 teams mentioned time as a disadvantage for
manual techniques, with 4 teams speciﬁcally mentioning time as a disadvantage
for SMPT and 3 teams mentioning time as a disadvantage for EMPT. One of
the teams who considered time a disadvantage for SMPT considered time an ad-
vantage for EMPT. No other team noted time as an advantage for any manual
technique. Responses for manual techniques were more mixed, with time seen as
an advantage for SAST by 4 teams and for DAST by 1 of the 4. However, 2 teams

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

45

considered time to be a disadvantage for SAST and 3 teams considered time to be
a disadvantage for DAST.

At least 3 teams also noted that they would anticipate VpH and a technique’s
overall eﬀectiveness relative to other techniques would change if the technique
were to continue to be performed over a longer period of time. For example, one
team noted that for SMPT “our guess is with time it will get even more diﬃcult to
come up with black box test cases manually thus giving lower eﬃciency eventually”.
Similarly, another team noted that for EMPT “after [the analyst] tried every point
[they] could imagine, there is less possib[ility of ] detect[ing] vulnerabilities”. In
contrast, one team claimed that with DAST “... if time and memory are not an
issue you can run a local instance of the application and fuzz it for years”.

12.3.3 Expertise

Summary: Many types of expertise are needed to apply vulnerability detection
techniques, particularly EMPT. Similar to ﬁndings from other works [42, 41, 100],
students noted that EMPT in particular requires diﬀerent types of expertise includ-
ing technical expertise, security expertise, and expertise with the SUT.

Overall, at least 8 teams commented on the role of expertise. Of the 8 teams
who mentioned expertise, only 3 mentioned expertise in the context of tool-based
techniques while 6 mentioned expertise when discussing EMPT, and one team
mentioned expertise when discussing SMPT and manual techniques generally. Ex-
pertise was not clearly an advantage or disadvantage, with only 3 of the 8 teams
who mentioned expertise suggesting that the expertise required to use a technique
was a disadvantage, with the remaining 5 teams not clearly noting expertise as
an advantage or disadvantage. Several speciﬁc types of expertise were discussed,
three of which, technical, security, and SUT expertise, are similar to the types of
expertise highlighted in related work [42, 41, 100]. At least 1 team commented on
the role of technical expertise in applying EMPT, 1 team commented on security
expertise required for EMPT, 4 teams commented on the role of SUT expertise
for EMPT, and 1 team commented on the role of SUT expertise for SMPT.

RQ3 - What other factors should we consider when comparing techniques?

Answer: The three most frequently discussed factors other than Eﬀec-
tiveness and Eﬃciency were Eﬀort, Time, and Expertise. Eﬀort and Time
were seen as a disadvantage of manual techniques. Perceptions of Eﬀort and
Time for automated techniques were more mixed, with some teams con-
sidering Eﬀort and/or Time an advantage while others considered them a
disadvantage. Expertise was associated with manual techniques, particu-
larly EMPT, more than automated techniques.

13 Limitations

We discuss the limitations to our approach in this Section. We group these limi-
tations as threats to Conclusion Validity, External Validity, Internal Validity, and
Construct Validity [17, 30, 102].

46

Elder et al.

13.1 Conclusion Validity

Conclusion Validity is about whether conclusions are based on statistical evi-
dence [17, 102]. While we have empirical results for RQ1, a single case study
is insuﬃcient to draw statistically signiﬁcant conclusions for eﬀectiveness (RQ1).
The measures used to evaluate eﬀectiveness are based on the number of vulner-
abilities found by applying each technique thoroughly and systematically. Unlike
the amount of time taken to apply each technique, the number of vulnerabilities
found will be deterministic for deterministic techniques such as SAST. Measuring
eﬀectiveness with statistical signiﬁcance would require the application of all four
techniques to at least 10-20 additional applications [45]. Applying all techniques
to 10-20 similarly-sized SUT is impractical given the eﬀort required to apply these
techniques to a single application. To mitigate this threat to validity, we performed
extensive review of the vulnerability counts, using the guidelines in Section 8.1.1,
and at least two individuals were involved in the review process for each technique
to verify the accuracy of the results. For eﬃciency (RQ2) the measure used, VpH,
was evaluated by having more individuals apply the technique to a subset of the
application.

13.2 Construct Validity

Construct Validity concerns the extent to which the treatments and outcome
measures used in the study reﬂect the higher level constructs we wish to exam-
ine [17, 102, 85]. In our study, the cause construct of the vulnerability detection
technique being used is reﬂected in our treatment of the four categories of vulnera-
bility detection techniques. The two primary outcome constructs are eﬀectiveness
(RQ1) and Eﬃciency (RQ2) for which the proxy measures are the number and
type of vulnerabilities (RQ1) and Vulnerabilities per Hour (RQ2). These treat-
ments and outcome measures were the same treatments and measures used by
Austin et al. [7, 8].

To measure eﬀectiveness we examined the number, type, and severity of vul-
nerabilities found by each technique. The number and type of vulnerabilities
found are commonly used measures of (in)security in academia and industry
[49, 23, 69, 68, 67, 66], including by the U.S. National Institute of Standards and
Technology (NIST) Software Assurance Metrics and Tool Evaluation (SAMATE)
program discussed in Section 4. Raw vulnerability counts do not fully capture the
picture, since some vulnerabilities may be considered more important than others.
We address this concern in several ways. First, as mentioned in Section 8.1.2, we
excluded tool alerts which were marked as insigniﬁcant or inconsequential, assum-
ing that these alerts would not be of interest to practitioners. Additionally, we
use the OWASP Top Ten categorization to summarize our data, and indicate the
severity of vulnerabilities found as described in Section 8.1.1.4.

Eﬃciency, was measured in terms of vulnerabilities per hour. This allowed
us to have a controlled experiment in which teams of students performed each
technique on at least a subset of the application, through which we could examine
whether the diﬀerences in eﬃciency were statistically signiﬁcant. As discussed in
Section 12.3.2, factors such as the length of time required to apply a technique
may also be helpful in understanding eﬃciency. Applying SAST and DAST more

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

47

comprehensively as part of RQ1 required over 20 hours per tool for both SAST
tools as well as DAST-2. In a class where vulnerability detection was only part of
the curriculum, it was not reasonable to expect students to spend 20 hours on such
a small portion of their grade. Adding RQ3, which was not included in Austin et
al.’s work [7, 8], allowed us to better understand how other factors such as time
spent applying a technique were perceived by students, helping to mitigate this
threat to validity.

13.3 Internal Validity

Internal Validity concerns whether the observed outcomes are due to the treat-
ment applied, or whether other factors may have inﬂuenced the outcome [17, 30].
For our study, the treatment applied is the type of vulnerability detection tech-
nique (SMPT, EMPT, DAST, or SAST). For RQ1 the observed outcome is the
eﬀectiveness as measured through number and type of vulnerabilities found, for
RQ2 the outcome is eﬀectiveness, measured in VpH. For RQ3 the outcome is the
factors found through qualitative analysis.

Running DAST based on more test cases may have found more vulnerabilities.
However, the resources available to our team were not signiﬁcantly less than other
small organizations, suggesting that resource limitations may be a factor to con-
sider when using DAST. Additionally, with OWASP ZAP we leveraged the tool’s
spider capability to expand on the 6 inputs, which helped mitigate this threat to
validity by increasing system coverage. As noted in the description of DAST in
Section 5.2.2.1, other comparisons of vulnerability detection techniques have also
used a spider, if a spider was available.

Another threat to internal validity for this study is that the student data
used for RQ2 is self-reported. The student data aligns with the experiences of
the research team, but self-reported estimates of the length of time it took to
complete a task are not necessarily representative of the actual time it takes to
complete a task. However, perceived time it takes to complete a task must still
be considered when making decisions on which vulnerability detection techniques
are used. Additionally, as shown by the qualitative analysis, students’ reported
numeric eﬃciency was not necessarily indicative of the time and eﬀort the students
perceived was required for each technique.

Another limitation with RQ2 is posed by equipment constraints for the gradu-
ate level class. For SAST and DAST, the students were unable to scan some parts
of the system due to insuﬃcient memory. To mitigate the risk that memory-related
processing issues would negatively impact student eﬃciency, the ﬁrst author per-
formed a scan of all modules in advance and directed students to modules which
would not be impacted by equipment constraints. Furthermore, students were in-
structed to only report time spent reviewing results, not time spent trying to get
the scan to run.

The researchers performing qualitative analysis for RQ3 may have had biases
which present threats to validity. Researcher biases may also have impacted the
vulnerability review processes, and analysis of true and false positives from the
results of vulnerability detection tools for RQ1. For this reason, two individuals
jointly performed the qualitative analysis, and the vulnerability review was either

48

Elder et al.

performed by two independent individuals or performed by one individual and
audited by a second individual for each technique.

Finally, although both DAST tools examined in RQ1 and RQ2 were the same,
one of the two SAST tools examined diﬀered between RQ1 and RQ2. Speciﬁcally,
we did not use Sonarqube for RQ2, using a proprietary tool (SAST-3) that had
been used in the course previously. Sonarqube may have been more or less eﬃcient
or eﬀective as compared with SAST-3, which would inﬂuence our results. Student
data was reported in aggregate and so we do not know how the eﬃciency of SAST-
2 compares with the eﬃciency of SAST 3, we only have the average eﬃciency of
SAST-2 and SAST-3. However, estimated researcher eﬃciency using Sonarqube
was 22 VpH while estimated researcher eﬃciency using SAST-2 was 18 VpH,
suggesting that tool diﬀerences may play less of a role as compared with other
factors such as expertise. In RQ2 we control for expertise by comparing eﬃciency
scores from the same group of individuals, i.e. we do not introduce researcher data
or data from students whose teammates did not elect to participate in the study.

13.4 External Validity

External Validity concerns the generalizability of our results [17, 30, 102]. Our
results may not generalize to software that is not similar to the SUT and the results
may not generalize to other systems. For example, we know that a strongly-typed,
memory-safe language such as Java, by design, is likely to have fewer memory-
allocation-related vulnerabilities, such as buﬀer overﬂow, when compared with
code in a non-memory-safe language such as C[19, 62]. As discussed in Section
6.2,OpenMRS is a large system built with commonly-used languages (e.g. Java,
Javascript) and frameworks (e.g. Spring) that is comparable in terms of size and
development practices to other systems in its domain. Additionally, many of our
results are similar to other studies within the same domain. For example, in recent
SATE comparisons[23], the highest precision rates for SAST tools examined were
78-94% in tests against Java applications, similar to those for the study and higher
than we expected based on other prior work as we will discuss in Section 14.3.2.
The tools used in this study may not be representative of DAST and SAST tools
generally, posing a threat to external validity for the study. We used two tools
that are in prevalent use in industry when performing each technique to mitigate
and understand possible biases introduced by tool selection. Our results as well as
those of the SATE reports[23, 69, 68] suggest that the eﬀectiveness of SAST tools
may vary. As noted both in our own experience and by students, the two DAST
tools were very diﬀerent in terms of ease of use.

A related threat to external validity is that we are performing a scientiﬁc
experiment in an academic setting, using industrial tools on a production system.
We do not think the diﬀerences between our experiment and industry would impact
our results and have worked to minimize diﬀerences. For example, the assignments
were designed to mitigate the risk that diﬀerences from industry practice would
impact the eﬃciency scores. When applying SAST tools in an industry project,
once alerts are classiﬁed as true or false positive, practitioners are more concerned
with resolving the true positive alerts than with handling false positive alerts, as
supported by studies such as Imtiaz et al [39]. However, true positive vulnerabilities
require more analysis since the alert must be resolved, while false positives can

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

49

ignored. In the SAST assignment29, students were required to analyze at least
10 alerts. To avoid incentivizing students to reduce their workload by classifying
alerts as false positives, students were instructed “If you have more than 5 false
positives, keep choosing alerts until you have 5 true positives while still reporting
the false positives”. Similarly, as noted in Section 8.1.2.1 as part of ensuring that
the vulnerability counts for RQ1 were not inﬂuenced by duplicate or erroneous
test cases, researchers reviewed and de-duplicated the student-developed SMPT
test cases, as well as writing additional test cases to increase coverage. In our
experience30, cursory review of test cases developed by less experienced testers, is
necessary in some industry contexts to ensure the resulting test suite can be run
eﬃciently and eﬀectively. While our review was more extensive, reviewing the test
cases for RQ1 was intended to ensure a more accurate test process and resulting
vulnerability count. Since time was not considered in RQ1, the additional time
spent on reviewing test cases for RQ1 would not impact the results.

14 Discussion

In Section 14.1, we provide examples of how our results may help inform practi-
tioners’ decisions based on their objectives, particularly for projects in a similar
domain to OpenMRS. In Section 14.2 we discuss how the availability or limitation
of resources, speciﬁcally Expertise, Time, and Equipment, may also impact which
technique should be used. These two sections should be used together, and not
independently. For example, as we note in Section 14.1, we found EMPT to be
very eﬀective at ﬁnding Injection vulnerabilities such as XSS. Hence if the objec-
tive is to ﬁnd high-priority injection vulnerabilities, EMPT may be a good option.
However, as we note in Section 14.2, EMPT requires expertise. Hence if an organi-
zation does not have enough individuals with suﬃcient expertise to apply EMPT
eﬀectively, practitioner may need to look to other techniques. Additionally, there
may be tradeoﬀs between techniques when practitioners focus on one objective
over another. A manager may want to avoid manual techniques in order to reduce
the perceived eﬀort for their team. However, in our context automated techniques
were less eﬀective in terms of the coverage of diﬀerent vulnerability types and the
severity of vulnerabilities found.

In Section 14.3 we go over ﬁndings which have additional implications relevant
to research and other evaluations of vulnerability detection techniques. We would
encourage any researcher or other individual comparing vulnerability detection
techniques to also be aware of our ﬁndings in Sections 14.1 and 14.2.

14.1 Organizational Objectives

We provide four examples of how our results might inform practitioner decisions
on which vulnerability detection techniques to use.

29the full text of the assignment is available under Project Part 1 in Appendix C
30the ﬁrst author has over 2 years of industry testing experience

50

Elder et al.

14.1.1 Speciﬁc Vulnerability Types (Eﬀectiveness)

Practitioners may know that a certain class of vulnerabilities is likely prevalent
in the system, or may be more concerned about a certain class of vulnerabilities
than others. In OpenMRS, for example, there were many XSS and other injection
vulnerabilities. EMPT was particularly eﬀective at ﬁnding these vulnerabilities, as
shown in Table 6, which may have contributed to EMPT’s relatively high eﬀective-
ness overall. In other words, EMPT would be a good choice for someone working
on an open-source Java-based medical application such as OpenMRS where Injec-
tion vulnerabilities are a problem. As noted by other comparisons of vulnerability
detection tools[23, 11], which tool is most eﬀective may vary across domains. Hence
practitioners should look to vulnerability detection technique evaluations in their
own domain.

If an organization is trying to target a speciﬁc type of vulnerability,
they should focus their vulnerability detection eﬀorts on techniques that
are eﬀective at ﬁnding that type of vulnerability in systems from their
domain. For example, a project similar to OpenMRS looking to ﬁnd
Injection (A03) vulnerabilities may beneﬁt from EMPT as seen in Table 6.

14.1.2 Coverage (Eﬀectiveness)

While EMPT performed particularly well in our context, SMPT provided higher
coverage across the OWASP Top Ten 2021 categories than other techniques, as
shown in Table 6. SMPT also provided more coverage of the OWASP Top Ten
Categories in the current study as compared with the prior work by Austin et
al.[7] as shown in Table 7. As another example, if the goal of the practitioner is to
thoroughly cover Logging and Monitoring concerns, a test suite based on Level 1
of the ASVS may not be preferable since the ﬁrst level of the ASVS only contains
two controls relating to logging. As seen in Table 7, the previous test suite was
much more eﬀective at ﬁnding vulnerabilities associated with Security Logging
and Monitoring failures.

Our results suggest that if a practitioner needs a vulnerability detection
technique that eﬀectively covers important types of vulnerabilities, a more
systematic technique, such as SMPT may be more eﬀective than EMPT.

14.1.3 Automation (Eﬃciency)

When considering automated tools, practitioners should note that automated tech-
niques may not inherently be more eﬃcient than manual techniques. An organi-
zation may choose to use automated tools for other reasons such as the need to
integrate automated tools with continuous deployment pipelines[84], and using
automated tools is better than not performing vulnerability detection at all. How-
ever, our results indicate that using a limited range of techniques will also miss
vulnerabilities that would be found by other techniques.

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

51

Our results suggest that if an organization is considering whether
to use an automated technique over a manual one, they should
not assume that the automated technique will be more eﬃ-
cient. Our results suggest that manual techniques are compara-
ble or better than automated techniques in terms of eﬃciency.

14.1.4 Percieved Eﬀort and Ease of Use(Other Factors)

No one wants to do more hard work than necessary. Ease-of-use is associated with
technology evaluation and adoption [22, 46].Two of the three most-frequently-
mentioned concepts in the students’ free-form responses, Eﬀort and Expertise,
are associated with the broader concept of Perceived Ease of Use [22]. As we
discuss in Section 12.3, eﬀort was predominantly seen as negative for manual
techniques but views of eﬀort were mixed for automated techniques. While views
of expertise were less negative expertise was associated with manual techniques
(SMPT and EMPT) more than automated techniques (DAST and SAST). In
the same assignment where students reported spending more time to ﬁnd fewer
vulnerabilities with SAST and particularly DAST as compared with EMPT and
to a lesser extent SMPT, students also claimed they considered Time and eﬀort
to be a disadvantage of manual techniques as discussed in our answer to RQ3.
Additionally, while Time may be an indicator of a technque’s performance, the
actual times recorded for manual techniques were, on average, no longer than the
times recorded for automated techniques. Hence our ﬁndings suggest that time was
perceived as a disadvantage of manual techniques even if they actually required
no more time than automated techniques. As noted by Gon¸cales et al.[37], little
empirical research has been done on the cognitive load of review-related tasks such
as software testing. Pfahl et al’s interviews of practitioners also found that EMPT
was perceived as being less easy-to-use and requiring more skill. However, studies
of SAST tools have also found Ease-of-Use concerns with SAST [92]. Hence while
we cannot make universal claims about all automated tools, practitioners looking
for the “easiest” solution may wish to minimize their use of manual techniques.

Our results suggest that if an organization is looking for the tech-
nique that will be perceived as requiring the least amount of eﬀort,
they may want to avoid manual techniques (SMPT and EMPT).

14.2 Resources to Consider

The resources below represent factors that should be considered when selecting a
vulnerability detection technique for a system such as OpenMRS. The availability
of resources may be more important than an organization’s objectives when select-
ing a vulnerability detection technique. Two of the three resources were highlighted
by student responses in RQ3, while the third resource provided a much more severe
limitation on our experiment than anticipated.

52

14.2.1 Expertise

Elder et al.

As noted by the students in RQ3 and supported by prior work [42, 41], expertise
plays a role in vulnerability detection, particularly EMPT. The eﬀectiveness of
the students both combined as shown in Table 6, and on average as discussed in
Section 12.1.5, as well as their eﬃciency shown in Figure 7 is promising. Students
with an introductory knowledge of Security were eﬃcient and eﬀective with EMPT.
Anecdotally based on our experience with RQ1 as well as in a student response
to RQ3, experience may also impact the eﬃciency and eﬀectiveness of automated
techniques more than we expected. Particularly for SAST, the researchers were
more eﬃcient than the average student group.

Our ﬁndings support related work suggesting that availability of
analysts with security expertise should be considered when se-
lecting a technique. EMPT in particular is known to require an-
alysts with some expertise. More research is needed to fully un-
derstand the role of expertise in applying automated techniques.

14.2.2 Time

As noted by the students in RQ3, the amount of time an analyst can spend on
a technique may inﬂuence the eﬃciency and eﬀectiveness of a technique. While
EMPT requires more expertise, little or no preparation is needed. SMPT, EMPT,
and DAST take more time to setup. However, as noted by the students, as dis-
cussed in Section 12.3.2, some techniques such as DAST may perform better if
practitioners have an extended timeframe in which to apply the technique. As dis-
cussed in our comparison with Austin et al. in Section 12.1.5, a single individual
performing EMPT for a longer period of time did not ﬁnd more vulnerabilities
than were found in a shorter timeframe.

The amount of available time for using the technique should
be considered when selecting a technique. Some tech-
niques, such as DAST, may beneﬁt from a longer timeframe.

14.2.3 Equipment

Both SAST and DAST required signiﬁcant equipment to run. As discuss in Section
13.3, students were only able to run SAST on smaller modules of OpenMRS using
the base VMs provided by the school. Students also were only able to run DAST-2
against the login page of OpenMRS and equipment constraints played a role in
determining how we could run DAST-2 systematically, and in a way that could be
compared with other tools and techniques. Austin et al., as well as similar studies
of industry scanners[3, 87, 11] do not mention equipment constraints. Where the
equipment used in the experiment is mentioned mentioned[3], it is implied that
the tools were able to be run on machines similar to the VMs used by students of
the graduate class. We found that a thorough evaluation of a “large-scale” system
required more computing resources than we expected for both SAST and DAST

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

53

tools. While OpenMRS is “large” for an evaluation of vulnerability detection tech-
niques, OpenMRS is less than 4 million lines of code - relatively small compared
to many realistic systems[24, 4]. Equipment constraints should be considered by
practitioners when selecting a vulnerability detection technique, particularly when
considering DAST and to a lesser extent SAST. Researchers evaluating vulnerabil-
ity detection techniques should be aware of this potential constraint when setting
up experiments.

Equipment constraints may inﬂuence the eﬀectiveness and eﬃciency
of vulnerability detection techniques on realistic systems. Automated
techniques require more computational power than manual techniques.

14.3 Implications for Evaluating Vulnerability Detection Techniques

The resource concerns highlighted in Section 14.2 should be considered not only by
practitioners but by researchers evaluating vulnerability detection techniques. Our
results also have several implications speciﬁc to future evaluations of vulnerability
detection techniques

14.3.1 True Positive Failure Count vs Vulnerability Count (Ratio)

We start with an observation that is not discussed in much of the related work, but
which may impact the results of any study comparing vulnerability detection tools
or techniques. The ratio between the number of tool alerts or failing test cases, i.e.
“true positive failures”, and the number of vulnerabilities varies across tools and
techniques. As can be seen in Table 5, particularly for DAST tools, the number of
alerts was many times the number of vulnerabilities found. The high ratio of alerts
to vulnerabilities is consistent with the ﬁnding from Klees et al.’s [49] work with
fuzzers that “ ‘unique crashes’ massively overcount[s] the number of true bugs”.
For SMPT and SAST, on the other hand, the number of true positive alerts was
slightly higher than the number of vulnerabilities but with a much lower ratio of
True Positive Failures to Vulnerabilities when compared with DAST.

More research is needed to fully understand the impact of having a higher or
lower number of failures per vulnerability. If additional failures present more infor-
mation about the vulnerability itself, having more alerts per vulnerability may be
helpful for analysts and developers attempting to triage and ﬁx the vulnerability.
However, reviewing and analyzing these failures takes time, a potential disadvan-
tage of having a higher number of alerts per vulnerability. For developers who
may use SAST tools built into their IDE to review code while it is being written,
the actual vulnerability count and consequently the diﬀerence between SAST alert
count and ﬁnal vulnerability count may not have much impact. On the other hand,
if a practitioner is using the overall alert count or vulnerability count to determine
the cybersecurity risk of an application, such as for insurance estimates [21], the
diﬀerence between alert count and vulnerability count may have a larger impact.
Researchers should be cautious when using alerts, failing test cases, or similar true
positive failures as a proxy for the number of vulnerabilities found in a system.

54

Elder et al.

A consistent set of counting rules should be used when compar-
ing the eﬀectiveness of diﬀerent tools or techniques. It cannot be
assumed that tools or techniques use the same counting rules.

14.3.2 SAST tools had fewer False Positives than expected.

High False Positive counts have historically been considered a drawback of SAST
tools[39, 43, 87, 91, 38]. Our results suggest that, at least for our context, SAST
actually has few false positives. The high precision of SAST tools for this study
is similar to results from recent SATE events [23]. More research is needed to
better understand the circumstances under which a lower false positive count may
generalize and the relationship between the perception that SAST tools produce
large numbers of false positives and the actual false positives produced by tools.

Our research supports the ﬁndings of recent SATE comparisons that some
SAST tools have low false positive counts when applied to Java applica-
tions. The lower false positive rate opens up new questions about why the
percentage of false positives is often perceived as a problem for SAST tech-
niques, and whether false positive counts have improved in other contexts.

15 Conclusions and Future Work

The motivation for this paper came from practitioner questions about which vul-
nerability detection techniques they should use and whether the vulnerability de-
tection could just be done automated. After ten years, with a changing vulner-
ability landscape, and many improvements in vulnerability detection techniques
such as the more common use of symbolic execution and taint tracing in SAST
tools[53, 12] results from previous work by Austin et al. were no longer assured to
hold true. We replicated the previous work, this time examining at least two tools
for each category of technique. The main ﬁnding of Austin et al. still holds - each
approach to vulnerability detection found vulnerabilities NOT found by the other
techniques. If the goal of an organization is to ﬁnd “all” vulnerabilities in their
system, they need to use as many techniques as their resources allow.

We hope to leverage the lessons learned from this experience in future work.
For an empirical comparison of vulnerability detection techniques in a large-scale
application, we found that even simple measures, such as vulnerability count, are
not entirely objective and require strict guidelines for the count to be interpretable
and the results replicable. More research is needed to understand how vulnerability
detection techniques compare in terms of other measures, such as exploitability, as
well as how to apply those measures in the context of large-scale web applications.
Additionally, an emerging class of automated vulnerability detection technique,
sometimes referred to as “hybrid” techniques, combines static analysis with as-
pects of dynamic analysis[13, 50] and is considered “promising”[13]. While out of
scope for the replication study, we look forward to expanding our comparison of
vulnerability detection techniques to include these and other tools and techniques.

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

55

An additional area of future work is further exploration of vulnerability severity
and related measures such as exploitability. Although not included in the original
study by Austin et al. [7, 8], ﬁnding and mitigating one high severity vulnerabil-
ity may be more important than ﬁnding and mitigating multiple lower severity
vulnerabilities. However, diﬀerent perspectives on severity result in diﬀerent pri-
oritization. As can be seen from Table 6, analyses of severity or criticality do not
always agree. Both DAST and SAST found vulnerabilities that the tools them-
selves classiﬁed as “low severity” but which were associated with “Broken Access
Control”, the #1 most critical vulnerabilities according to the OWASP Top Ten.
Similarly, vulnerabilities associated with information disclosure through error mes-
sages, associated with #4 in the OWASP Top Ten - “Insecure Design” were not
considered particularly critical in this context. There is more consensus between
the severity measures when we look at what is not important. Over half (382 out
of 704) of the less severe vulnerabilities were associated with CWEs not mapped to
the OWASP Top Ten. More research is needed to better understand which severity
measures to use in a particular context.

16 Acknowledgements

We thank Jiaming Jiang for her support as teaching assistant for the security class.
We are grateful to the I/T staﬀ at the university for their assistance in ensuring
that we had suﬃcient computing power running the course. We also thank the
students in the software security class. Finally, we thank all the members of the
Realsearch research group for their valuable feedback through this project.

This material is based upon work supported by the National Science Foun-
dation under Grant No. 1909516. Any opinions, ﬁndings, and conclusions or rec-
ommendations expressed in this material are those of the author(s) and do not
necessarily reﬂect the views of the National Science Foundation.

References

1. Ackerman E (2019) Upgrade to superhuman reﬂexes without feeling like
a robot. IEEE Spectrum URL https://spectrum.ieee.org/enabling-
superhuman-reflexes-without-feeling-like-a-robot

2. Alomar N, Wijesekera P, Qiu E, Egelman S (2020) “you’ve got your nice list
of bugs, now what?” vulnerability discovery and management processes in the
wild. In: Sixteenth Symposium on Usable Privacy and Security ({SOUPS}
2020), pp 319–339

3. Amankwah R, Chen J, Kudjo PK, Towey D (2020) An empirical comparison
of commercial and open-source web vulnerability scanners. Software: Practice
and Experience 50(9):1842–1857

4. Anderson T (2020) Linux in 2020: 27.8 million lines of code in the kernel, 1.3
million in systemd. The Register URL https://www.theregister.com/2020/
01/06/linux_2020_kernel_systemd_code/, [Online; Accessed: 21-Dec-2021]
5. Antunes N, Vieira M (2009) Comparing the eﬀectiveness of penetration test-
ing and static code analysis on the detection of sql injection vulnerabilities

56

Elder et al.

in web services. In: 2009 15th IEEE Paciﬁc Rim International Symposium on
Dependable Computing, IEEE, pp 301–306

6. Antunes N, Vieira M (2010) Benchmarking vulnerability detection tools for
web services. In: 2010 IEEE International Conference on Web Services, IEEE,
pp 203–210

7. Austin A, Williams L (2011) One technique is not enough: A comparison
of vulnerability discovery techniques. In: 2011 International Symposium on
Empirical Software Engineering and Measurement, IEEE, pp 97–106

8. Austin A, Holmgreen C, Williams L (2013) A comparison of the eﬃciency and
eﬀectiveness of vulnerability discovery techniques. Information and Software
Technology 55(7):1279–1288

9. Bannister A (2021) Healthcare provider texas ent alerts 535,000 patients to
data breach. The Daily Swig [Online; Publication Date 20-Dec-2021; Ac-
cessed: 21-Dec-2021]

10. Bartlett MS (1937) Properties of suﬃciency and statistical tests. Proceedings
of the Royal Society of London Series A-Mathematical and Physical Sciences
160(901):268–282

11. Bau J, Wang F, Bursztein E, Mutchler P, Mitchell JC (2012) Vulnerabil-
ity factors in new web applications: Audit tools, developer selection & lan-
guages. Tech. rep., Stanford, URL https://seclab.stanford.edu/websec/
scannerPaper.pdf

12. Campbell GA (2020) What is ’taint analysis’ and why do i care? URL https:

//blog.sonarsource.com/what-is-taint-analysis

13. Chaim ML, Santos DS, Cruzes DS (2018) What do we know about buﬀer
overﬂow detection?: A survey on techniques to detect a persistent vulnerabil-
ity. International Journal of Systems and Software Security and Protection
(IJSSSP) 9(3):1–33

14. Cicchetti DV, Feinstein AR (1990) High agreement but low kappa: Ii. resolv-

ing the paradoxes. Journal of clinical epidemiology 43(6):551–558

15. Cohen J (1960) A coeﬃcient of agreement for nominal scales. Educational

and psychological measurement 20(1):37–46

16. Condon C, Miller H (2021) Maryland health department says there’s no ev-
idence of data lost after cyberattack; website is back online. Baltimore Sun
URL https://www.baltimoresun.com/health/bs-hs-mdh-website-down-
20211206-o2ky2sn5znb3pdwtnu2a7m5g6q-story.html, online; Publication
Date: 06-Dec-2021 [Online; Accessed: 21-Dec-2021]

17. Cook TD, Campbell DT (1979) Quasi-Experimentation: Design and Analysis

Issues for Field Settings. Rand McNally College Publishing

18. Corbin J, Strauss A (2008) Basics of Qualitative Research: Techniques and
Procedures for Developing Grounded Theory, 3rd edn. SAGE Publications,
Inc.

19. Cowan C, Wagle F, Pu C, Beattie S, Walpole J (2000) Buﬀer overﬂows:
Attacks and defenses for the vulnerability of the decade. In: Proceedings
DARPA Information Survivability Conference and Exposition. DISCEX’00,
IEEE, vol 2, pp 119–129

20. Cruzes DS, Felderer M, Oyetoyan TD, Gander M, Pekaric I (2017) How
is security testing done in agile teams? a cross-case analysis of four software
teams. In: International Conference on Agile Software Development, Springer,
Cham, pp 201–216

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

57

21. Dambra S, Bilge L, Balzarotti D (2020) Sok: Cyber insurance–technical chal-
lenges and a system security roadmap. In: 2020 IEEE Symposium on Security
and Privacy (SP), IEEE, pp 1367–1383

22. Davis FD (1989) Perceived usefulness, perceived ease of use, and user accep-

tance of information technology. MIS quarterly pp 319–340

23. Delaitre AM, Stivalet BC, Black PE, Okun V, Cohen TS, Ribeiro A (2018)
Sate v report: Ten years of static analysis tool expositions. NIST SP 500-
326, National Institute of Standards and Technology (NIST), URL https:
//doi.org/10.6028/NIST.SP.500-326

24. Desjardins J (2017) Here’s how many millions of

code
lines of
Insider URL https:

takes

to run diﬀerent

it
//www.businessinsider.com/how-many-lines-of-code-it-takes-to-
run-different-software-2017-2, [Online; Accessed: 21-Dec-2021]

software. Business

25. Doup´e A, Cova M, Vigna G (2010) Why johnny can’t pentest: An analysis
of black-box web vulnerability scanners. In: International Conference on De-
tection of Intrusions and Malware, and Vulnerability Assessment, Springer,
pp 111–131

26. Elder SE, Zahan N, Kozarev V, Shu R, Menzies T, Williams L (2021) Struc-
turing a comprehensive software security course around the owasp applica-
tion security veriﬁcation standard. In: 2021 IEEE/ACM 43rd International
Conference on Software Engineering: Software Engineering Education and
Training (ICSE-SEET), IEEE, pp 95–104

27. Epic Systems Corporation (2020) From healthcare to mapping the
milky way:
tech. URL
things you didn’t know about
https://www.epic.com/epic/post/healthcare-mapping-milky-way-
5-things-didnt-know-epics-tech, [Online; Accessed: 07-Dec-2021]

epic’s

5

28. Executive Order 14028 (2021) Executive order on improving the nation’s
cybersecurity. Exec. Order No. 14028, 86 FR 26633, URL https://www.
federalregister.gov/d/2021-10460

29. Feinstein AR, Cicchetti DV (1990) High agreement but low kappa: I. the

problems of two paradoxes. Journal of clinical epidemiology 43(6):543–549

30. Feldt R, Magazinius A (2010) Validity threats in empirical software engineer-

ing research-an initial survey. In: Seke, pp 374–379

31. Feng GC (2013) Factors aﬀecting intercoder reliability: A monte carlo exper-

iment. Quality & Quantity 47(5):2959–2982

32. Fielding RT, Reschke J (2014) Hypertext Transfer Protocol (HTTP/1.1):
Semantics and Content. RFC 7231, DOI 10.17487/RFC7231, URL https:
//rfc-editor.org/rfc/rfc7231.txt

33. Finifter M, Akhawe D, Wagner D (2013) An empirical study of vulnerability
rewards programs. In: 22nd USENIX Security Symposium (USENIX Security
13), USENIX, Washington, D.C., pp 273–288

34. Fonseca J, Vieira M, Madeira H (2007) Testing and comparing web vulner-
ability scanning tools for sql injection and xss attacks. In: 13th Paciﬁc Rim
international symposium on dependable computing (PRDC 2007), IEEE, pp
365–372

35. Games PA, Howell JF (1976) Pairwise multiple comparison procedures with
unequal n’s and/or variances: a monte carlo study. Journal of Educational
Statistics 1(2):113–125

58

Elder et al.

36. Gilbert R (2022) Information exposure through query strings in url. OWASP
(Website) URL https://owasp.org/www-community/vulnerabilities/
Information_exposure_through_query_strings_in_url,
[Online; Ac-
cessed: 07-Jan-2022]

37. Gon¸cales L, Farias K, da Silva BC (2021) Measuring the cognitive load of
software developers: An extended systematic mapping study. Information and
Software Technology p 106563

38. Haﬁz M, Fang M (2016) Game of detections: how are security vulnerabilities
discovered in the wild? Empirical Software Engineering 21(5):1920–1959
39. Imtiaz N, Rahman A, Farhana E, Williams L (2019) Challenges with respond-
ing to static analysis tool alerts. In: 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR), IEEE, pp 245–249
40. ISO/IEC/IEEE (2013) Software and systems engineering — software testing
— part 1: concepts and deﬁnitions. ISO/IEC/IEEE 29119-1:2013, Interna-
tional Organization for Standardization (ISO), International Electrotechni-
cal Commission (IES), and Institute of Electrical and Electronics Engineers
(IEEE)

41. Itkonen J, M¨antyl¨a MV (2014) Are test cases needed? replicated comparison
between exploratory and test-case-based software testing. Empirical Software
Engineering 19(2):303–342

42. Itkonen J, M¨antyl¨a MV, Lassenius C (2013) The role of the tester’s knowledge
in exploratory software testing. IEEE Transactions on Software Engineering
39(5):707–724

43. Johnson B, Song Y, Murphy-Hill E, Bowdidge R (2013) Why don’t software
developers use static analysis tools to ﬁnd bugs? In: Proceedings of the 2013
International Conference on Software Engineering, IEEE Press, pp 672–681
44. Joint Task Force Transformation Initiative (2013) Security and privacy
controls for federal information systems and organizations. NIST SP 800-
53, National Institute of Standards and Technology (NIST), URL http:
//dx.doi.org/10.6028/NIST.SP.800-53r4

45. Kirk R (2013) Experimental Design: Procedures for the Behavioral Sciences,

4th edn. Thousand Oaks : Sage Publications

46. Kitchenham B, Linkman S, Law D (1996) Desmet: A method for evaluating
software engineering methods and tools. Tech. Rep. TR96-09, Keele Univer-
sity

47. Kitchenham B, Madeyski L, Budgen D, Keung J, Brereton P, Charters S,
Gibbs S, Pohthong A (2017) Robust statistical methods for empirical software
engineering. Empirical Software Engineering 22(2):579–630

48. Kitchenham BA, Budgen D, Brereton P (2015) Evidence-based software en-

gineering and systematic reviews, vol 4. CRC press

49. Klees G, Ruef A, Cooper B, Wei S, Hicks M (2018) Evaluating fuzz testing.
In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, pp 2123–2138

50. Liu M, Zhang B, Chen W, Zhang X (2019) A survey of exploitation and
detection methods of xss vulnerabilities. IEEE Access 7:182004–182016
51. Lombard M, Snyder-Duch J, Bracken CC (2002) Content analysis in mass
communication: Assessment and reporting of intercoder reliability. Human
communication research 28(4):587–604

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

59

52. Lung J, Aranda J, Easterbrook S, Wilson G (2008) On the diﬃculty of repli-
cating human subjects studies in software engineering. In: 2008 ACM/IEEE
30th International Conference on Software Engineering, pp 191–200

53. Mallet F (2016) Sonaranalyzer for java: Tricky bugs are running scared.
URL https://blog.sonarsource.com/sonaranalyzer-for-java-tricky-
bugs-are-running-scared, online; Accessed: 05-Dec-2021

54. McGraw G (2006) Software security: building security in. Addison-Wesley

Professional

55. MITRE (2016) Common Vulnerabilities and Exposures (CVE) Number-
ing Authority (CNA) Rules. URL https://cve.mitre.org/cve/cna/CNA_
Rules_v1.1.pdf, [Online; Accessed: 24-July-2021]

56. MITRE (2021) Cve → cwe mapping guidance. In: [57], URL https://cwe.
mitre.org/documents/cwe_usage/guidance.html, [Online; Accessed 24-Jul-
2021]

57. MITRE (2021) CWE Common Weakness Enumeration (Website), URL

https://cwe.mitre.org/, [Online; Accessed 20-Jul-2021]

58. MITRE (2021) Cwe view: Weaknesses in owasp top ten (2021). In: [57],
URL https://cwe.mitre.org/data/definitions/1344.html, [Online; Ac-
cessed 09-Dec-2021]

59. MITRE (2022) Cwe 1003 - cwe view: Weaknesses for simpliﬁed mapping
of published vulnerabilities. In: [57], URL https://cwe.mitre.org/data/
definitions/1003.htmll, [Online; Accessed 07-Jan-2022]

60. Morrison P, Moye D, Pandita R, Williams L (2018) Mapping the ﬁeld of
software life cycle security metrics. Information and Software Technology
102:146–159

61. Mozilla (2021) Http messages. URL https://developer.mozilla.org/en-

US/docs/Web/HTTP/Messages, [Online; Accessed: 21-Dec-2021]

62. Nagarakatte S, Zhao J, Martin MM, Zdancewic S (2009) Softbound: Highly
compatible and complete spatial memory safety for c. In: Proceedings of the
30th ACM SIGPLAN Conference on Programming Language Design and
Implementation, pp 245–258

63. NVD (2021) Cwe over time. In: [64], URL https://nvd.nist.gov/general/
visualizations/vulnerability-visualizations/cwe-over-time, [Online;
Accessed: 05-Dec-2021]

64. NVD (2021) National Vulnerability Database (Website), National Institute of
Standards and Technology (NIST), URL https://nvd.nist.gov/, [Online;
Accessed 01-Nov-2021]

65. NVD (2021) Vulnerabilities. In:
[Online; Accessed 01-Nov-2021]

[64], URL https://nvd.nist.gov/vuln,

66. Okun V, Gaucher R, Black PE (2009) Static analysis tool exposition (sate)
2008. NIST SP 500-279, National Institute of Standards and Technology
(NIST), URL https://dx.doi.org/10.6028/NIST.SP.500-279, [Online; ac-
cessed 20-Jul-2021]

67. Okun V, Delaitre A, Black PE (2010) The second static analysis tool ex-
position (sate) 2009. NIST SP 500-287, National Institute of Standards and
Technology (NIST), URL https://dx.doi.org/10.6028/NIST.SP.500-287,
[Online; accessed 20-Jul-2021]

68. Okun V, Delaitre A, Black PE (2011) Report on the third static analysis tool
exposition (sate 2010). NIST SP 500-283, National Institute of Standards and

60

Elder et al.

Technology (NIST), URL https://dx.doi.org/10.6028/NIST.SP.500-283,
[Online; accessed 20-Jul-2021]

69. Okun V, Delaitre A, Black PE (2013) Report on the static analysis tool
exposition (sate) iv. NIST SP 500-297, National Institute of Standards and
Technology (NIST), URL https://dx.doi.org/10.6028/NIST.SP.500-297
70. Open Web Application Security Project (OWASP) Foundation (2013) Owasp
top ten - 2010. URL https://owasp.org/www-pdf-archive/OWASP_Top_
10_-_2010.pdf, online; Accessed: 05-Dec-2021

71. Open Web Application Security Project (OWASP) Foundation (2013) Owasp
top ten - 2013. URL https://owasp.org/www-pdf-archive/OWASP_Top_
10_-_2013.pdf, online; Accessed: 05-Dec-2021

72. Open Web Application Security Project (OWASP) Foundation (2017) Owasp
top ten - 2017. URL https://owasp.org/www-project-top-ten/2017/, on-
line; Accessed: 05-Dec-2021

73. Open Web Application Security Project (OWASP) Foundation (2021) Owasp
top ten - 2021. URL https://owasp.org/Top10/, online; Accessed: 05-Dec-
2021

74. Open Web Application Security Project (OWASP) Foundation (2021) The
owasp top ten application security risks project. URL https://owasp.org/
www-project-top-ten/, [Online; Accessed: 09-Dec-2021]

75. Open Web Application Security Project (OWASP) Foundation (2021) Owasp
zap. URL https://www.zaproxy.org/, [Online; Accessed: 21-Dec-2021]
76. OpenMRS (2020) OpenMRS Developer Manual. URL http://devmanual.

openmrs.org/en/, [Online; Accessed: 24-Jul-2021]

77. OpenMRSAtlas (2021) Openmrs atlas. URL https://atlas.openmrs.org/,

[Online; Accessed: 24-Jul-2021]

78. OWASP ZAP Dev Team (2021) Getting started - features - alerts. In: [96],
URL https://www.zaproxy.org/docs/desktop/start/features/alerts/,
[Online; Accessed 06-Dec-2021]

79. OWASP ZAP Dev Team (2021) Getting started - features - spider. In: [96],
URL https://www.zaproxy.org/docs/desktop/start/features/spider/,
[Online; Accessed 20-Jul-2021]

80. Pfahl D, Yin H, M¨antyl¨a MV, M¨unch J (2014) How is exploratory testing
used? a state-of-the-practice survey. In: Proceedings of the 8th ACM/IEEE
international symposium on empirical software engineering and measure-
ment, ACM, p 5

81. PortSwigger

(2022) Password submitted using GET method. URL

https://portswigger.net/kb/issues/00400300_password-submitted-
using-get-method, [Online; Accessed: 07-Jan-2022]

82. Purkayastha S, Goyal S, Phillips T, Wu H, Haakenson B, Zou X (2020) Con-
tinuous security through integration testing in an electronic health records
system. In: 2020 International Conference on Software Security and Assur-
ance (ICSSA), IEEE, pp 26–31

83. Radio New Zealand (RNZ) (2021) Health ministry announces $75m to plug
cybersecurity gaps. URL https://www.rnz.co.nz/news/national/458331/
health-ministry-announces-75m-to-plug-cybersecurity-gaps,
[Online;
Publication Date: 20-Dec-2021; Accessed: 21-Dec-2021]

84. Rahman AAU, Helms E, Williams L, Parnin C (2015) Synthesizing con-
tinuous deployment practices used in software development. In: 2015 Agile

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

61

Conference, IEEE, pp 1–10

85. Ralph P, Tempero E (2018) Construct validity in software engineering re-
search and software metrics. In: Proceedings of the 22nd International Con-
ference on Evaluation and Assessment in Software Engineering 2018, pp 13–23
86. Razali NM, Wah YB, et al. (2011) Power comparisons of shapiro-wilk,
kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statisti-
cal modeling and analytics 2(1):21–33

87. Scandariato R, Walden J, Joosen W (2013) Static analysis versus penetration
testing: A controlled experiment. In: 2013 IEEE 24th international sympo-
sium on software reliability engineering (ISSRE), IEEE, pp 451–460

88. Scanlon T (2018) 10 types of application security testing tools: When
and how to use them. Blog, Software Engineering Institute, Carnegie Mel-
lon University, URL https://insights.sei.cmu.edu/blog/10-types-of-
application-security-testing-tools-when-and-how-to-use-them
89. Smith B, Williams L (2012) On the eﬀective use of security test patterns. In:
2012 IEEE Sixth International Conference on Software Security and Relia-
bility, IEEE, pp 108–117

90. Smith B, Williams LA (2011) Systematizing security test planning using
functional requirements phrases. Tech. Rep. TR-2011-5, North Carolina State
University. Dept. of Computer Science

91. Smith J, Johnson B, Murphy-Hill E, Chu B, Lipford HR (2015) Questions
developers ask while diagnosing potential security vulnerabilities with static
analysis. In: Proceedings of the 2015 10th Joint Meeting on Foundations of
Software Engineering, ACM, pp 248–259

92. Smith J, Do LNQ, Murphy-Hill E (2020) Why can’t johnny ﬁx vulnerabili-
ties: A usability evaluation of static analysis tools for security. In: Sixteenth
Symposium on Usable Privacy and Security ({SOUPS} 2020), USENIX, pp
221–238

93. SonarSource (2019) Sonarqube documentation: Security-related rules. URL
https://docs.sonarqube.org/8.2/user-guide/security-rules/, [Online;
Accessed: 06-Dec-2021]
94. StackOverﬂow (2021)

2021 Developer
insights.stackoverflow.com/survey/2021#technology-most-popular-
technologies, [Online; Accessed: 07-Dec-2021]

Survey. URL

https://

95. van der Stock A, Cuthbert D, Manico J, Grossman JC, Burnett M (2019)
Application security veriﬁcation standard. Rev. 4.0.1, Open Web Application
Security Project (OWASP), URL https://github.com/OWASP/ASVS/tree/
v4.0.1/4.0, [Online; accessed 20-Jul-2021]

96. Team OZD (ed) (2021) The OWASP Zed Attack Proxy (ZAP) Desktop User

Guide

97. Tøndel IA, Jaatun MG, Cruzes DS, Williams L (2019) Collaborative secu-
rity risk estimation in agile software development. Information & Computer
Security

98. US Cybersecurity and Infrastructure Security Agency (CISA) (2021) Pro-
vide medical care is in critical condition: Analysis and stakeholder decision
support to minimize further harm. URL https://www.cisa.gov/sites/
default/files/publications/Insights_MedicalCare_FINAL-v2_0.pdf,
[Online; Accessed: 21-Dec-2021]

62

Elder et al.

99. US Dept of Veterans Aﬀairs, Oﬃce of

Information and Technol-
ogy, Enterprise Program Management Oﬃce (2021) VA Monograph.
URL https://www.va.gov/vdl/documents/Monograph/Monograph/VistA_
Monograph_0421_REDACTED.pdf, [Online; Accessed: 07-Dec-2021]

100. Votipka D, Stevens R, Redmiles E, Hu J, Mazurek M (2018) Hackers vs.
testers: A comparison of software vulnerability discovery processes. In: 2018
IEEE Symposium on Security and Privacy (SP), IEEE, pp 374–391

101. Wilcox RR, Keselman H (2003) Modern robust data analysis methods: mea-

sures of central tendency. Psychological methods 8(3):254

102. Wohlin C, Runeson P, H¨ost M, Ohlsson MC, Regnell B, Wessl´en A (2012)
Experimentation in software engineering. Springer Science & Business Media

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

63

A Appendix - Automated Technique CWEs

Table 10: CWEs covered in the rules implemented by automated techniques

CWE
ID

22

23

CWE Name

Path Traversal

Relative Path Traversal

200 Exposure of Sensitive Information to an

Unauthorized Actor

201

Insertion of Sensitive Information Into Sent Data

264 Permissions, Privileges, and Access Controls

284

285

Improper Access Control

Improper Authorization

352 Cross-Site Request Forgery (CSRF)

359 Exposure of Private Personal Information to an

Unauthorized Actor

425 Forced Browsing

601 Open Redirect

668 Exposure of Resource to Wrong Sphere

862 Missing Authorization

863

Incorrect Authorization

1275 Sensitive Cookie with Improper SameSite

Attribute

296

Improper Following of a Certiﬁcate’s Chain of
Trust

319 Cleartext Transmission of Sensitive Information

321 Use of Hard-coded Cryptographic Key

322 Key Exchange without Entity Authentication

325 Missing Cryptographic Step

326

Inadequate Encryption Strength

327 Use of a Broken or Risky Cryptographic

Algorithm

328 Use of Weak Hash

330 Use of Insuﬃciently Random Values

336

Same Seed in Pseudo-Random Number
Generator (PRNG)

337 Predictable Seed in Pseudo-Random Number

Generator (PRNG)

523 Unprotected Transport of Credentials

760 Use of a One-Way Hash with a Predictable Salt

916 Use of Password Hash With Insuﬃcient

Computational Eﬀort

Improper Input Validation

Injection

OS Command Injection

Cross-site Scripting

20

74

78

79

OWASP
Top Ten

ZAP DA-2 Sonar SA-2

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A01

A02

A02

A02

A02

A02

A02

A02

A02

A02

A02

A02

A02

A02

A02

A03

A03

A03

A03

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

Name

Top Ten ZAP DA-2 Sonar SA-2

Elder et al.

64

ID

83

88

89

90

91

93

94

95

97

98

99

Improper Neutralization of Script in Attributes
in a Web Page

Argument Injection

SQL Injection

LDAP Injection

XML Injection (aka Blind XPath Injection)

CRLF Injection

Code Injection

Eval Injection

Improper Neutralization of Server-Side Includes
(SSI) Within a Web Page

PHP Remote File Inclusion

Resource Injection

113 HTTP Response Splitting

184

Incomplete List of Disallowed Inputs

470 Unsafe Reﬂection

610 Externally Controlled Reference to a Resource in

Another Sphere

643 XPath Injection

917 Expression Language Injection

73

External Control of File Name or Path

183 Permissive List of Allowed Inputs

209 Generation of Error Message Containing

Sensitive Information

311 Missing Encryption of Sensitive Data

313 Cleartext Storage in a File or on Disk

472 External Control of Assumed-Immutable Web

Parameter

501 Trust Boundary Violation

522

Insuﬃciently Protected Credentials

525 Use of Web Browser Cache Containing Sensitive

Info.

642 External Control of Critical State Data

646 Reliance on File Name or Extension of

Externally-Supplied File

650 Trusting HTTP Permission Methods on the

Server Side

770 Allocation of Resources Without Limits or

Throttling

807 Reliance on Untrusted Inputs in a Security

Decision

927 Use of Implicit Intent for Sensitive

Communication

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A03

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

A04

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

1021 Improper Restriction of Rendered UI Layers or

A04

X

Frames

7

J2EE Misconﬁguration: Missing Custom Error
Page

A05

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

65

ID

Name

Top Ten ZAP DA-2 Sonar SA-2

315 Cleartext Storage of Sensitive Information in a

A05

Cookie

541

Inclusion of Sensitive Information in an Include
File

548 Exposure of Information Through Directory

Listing

611

614

Improper Restriction of XML External Entity
Reference

Sensitive Cookie in HTTPS Session Without
’Secure’ Attribute

776 XML Entity Expansion

933

Security Misconﬁguration

942 Permissive Cross-domain Policy with Untrusted

Domains

1004 Sensitive Cookie Without ’HttpOnly’ Flag

259 Use of Hard-coded Password

263 Password Aging with Long Expiration

287

Improper Authentication

288 Authentication Bypass Using an Alternate Path

295

297

or Channel

Improper Certiﬁcate Validation

Improper Validation of Certiﬁcate with Host
Mismatch

300 Channel Accessible by Non-Endpoint

307

Improper Restriction of Excessive Authentication
Attempts

346 Origin Validation Error

384

Session Fixation

521 Weak Password Requirements

613

Insuﬃcient Session Expiration

798 Use of Hard-coded Credentials

345

Insuﬃcient Veriﬁcation of Data Authenticity

502 Deserialization of Untrusted Data

565 Reliance on Cookies without Validation and

Integrity Checking

829

915

532

778

4

36

41

67

Inclusion of Functionality from Untrusted
Control Sphere

Improperly Controlled Modiﬁcation of
Dynamically-Determined Object Attributes

Insertion of Sensitive Information into Log File

Insuﬃcient Logging

J2EE Environment Issues (Deprecated)

Absolute Path Traversal

Improper Resolution of Path Equivalence

Improper Handling of Windows Device Names

102

Struts: Duplicate Validation Forms

112 Missing XML Validation

A05

A05

A05

A05

A05

A05

A05

A05

A07

A07

A07

A07

A07

A07

A07

A07

A07

A07

A07

A07

A07

A08

A08

A08

A08

A08

A09

A09

NM

NM

NM

NM

NM

NM

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

66

ID

Name

118 Range Error

120 Buﬀer Overﬂow

124 Buﬀer Underﬂow

134 Use of Externally-Controlled Format String

140

144

149

150

154

Improper Neutralization of Delimiters

Improper Neutralization of Line Delimiters

Improper Neutralization of Quoting Syntax

Improper Neutralization of Escape, Meta, or
Control Sequences

Improper Neutralization of Variable Name
Delimiters

156

Improper Neutralization of Whitespace

157 Failure to Sanitize Paired Delimiters

158

Improper Neutralization of Null Byte or NUL
Character

166

Improper Handling of Missing Special Element

172 Encoding Error

174 Double Decoding of the Same Data

175

176

177

Improper Handling of Mixed Encoding

Improper Handling of Unicode Encoding

Improper Handling of URL Encoding (Hex
Encoding)

185

Incorrect Regular Expression

189 Numeric Error

190

Integer Overﬂow or Wraparound

194 Unexpected Sign Extension

215

Insertion of Sensitive Information Into Debugging
Code

242 Use of Inherently Dangerous Function

252 Unchecked Return Value

253

Incorrect Check of Function Return Value

289 Authentication Bypass by Alternate Name

299

Improper Check for Certiﬁcate Revocation

314 Cleartext Storage in the Registry

317 Cleartext Storage of Sensitive Info. in GUI

332

Insuﬃcient Entropy in PRNG

366 Race Condition within a Thread

369 Divide By Zero

390 Detection of Error Condition Without Action

398 Code Quality

399 Resource Management Error

400 Uncontrolled Resource Consumption

403 File Descriptor Leak

404

Improper Resource Shutdown or Release

Elder et al.

Top Ten ZAP DA-2 Sonar SA-2

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

67

Name

Top Ten ZAP DA-2 Sonar SA-2

ID

406

Insuﬃcient Control of Network Message Volume
(Network Ampliﬁcation)

427 Uncontrolled Search Path Element

436

Interpretation Conﬂict

476 NULL Pointer Dereference

480 Use of Incorrect Operator

483

Incorrect Block Delimitation

484 Omitted Break Statement in Switch

489 Active Debug Code

493 Critical Public Variable Without Final Modiﬁer

500 Public Static Field Not Marked Final

543 Use of Singleton Pattern Without

Synchronization in a Multithreaded Context

561 Dead Code

563 Assignment to Variable without Use

567 Unsynchronized Access to Shared Data in a

Multithreaded Context

568

ﬁnalize() Method Without super.ﬁnalize()

569 Expression Issue

573

580

Improper Following of Speciﬁcation by Caller

clone() Method Without super.clone()

582 Array Declared Public, Final, and Static

599 Missing Validation of OpenSSL Certiﬁcate

600 Uncaught Exception in Servlet

607 Public Static Final Field References Mutable

Object

615

Inclusion of Sensitive Information in Source Code
Comments

628 Function Call with Incorrectly Speciﬁed

Arguments

661 Weaknesses in Software Written in PHP

665

Improper Initialization

670 Always-Incorrect Control Flow Implementation

X

X

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

683 Function Call With Incorrect Order of Arguments NM

688 Function Call With Incorrect Variable or

Reference as Argument

693 Protection Mechanism Failure

704

754

Incorrect Type Conversion or Cast

Improper Check for Unusual or Exceptional
Conditions

755

Improper Handling of Exceptional Conditions

777 Regular Expression without Anchors

779

Logging of Excessive Data

783 Operator Precedence Logic Error

827

Improper Control of Document Type Deﬁnition

833 Deadlock

NM

NM

NM

NM

NM

NM

NM

NM

NM

NM

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

68

ID

835

Name

Inﬁnite Loop

1022 Use of Web Link to Untrusted Target with

window.opener Access

1023 Incomplete Comparison with Missing Factors

Elder et al.

Top Ten ZAP DA-2 Sonar SA-2

NM

NM

NM

X

X

X

X

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

69

B Appendix - Student Experience Questionnaire

At the beginning of the course, students were asked to ﬁll out a survey about their experience
relevant to the course. The four questions asked to students were as follows:

1. How much time have you spent working at a professional software organization – including

internships – in terms of the # of years and the # of months?

2. On a scale from 1 (none) to 5 (fully), how much of the time has your work at a professional

software organization involved cybersecurity?

3. Which of the follow classes have you already completed?
4. Which of the following classes are you currently taking?

Q1 was short answer. For Q2, students selected a single number between 1 and 5. For Q3,
the students could check any number of checkboxes corresponding to a list of the security and
privacy courses oﬀered at the institution. For Q4, the students selected from the subset of
classes from question 4 that were being oﬀered the semester in which the survey was given.

Fifty-nine of the sixty-three students who agreed to let their data be used for the study
responded to the survey. Of these 59 responses, four students responses to Q1 provided a nu-
meric value, e.g. “3”, but did not specify whether the numeric value indicated years or months.
We considered this invalid and summarize experience from the remaining 55 participants in
Section 7.2

70

Elder et al.

C Appendix - Student Assignments

The following are the verbatim assignments for the Course Project that guided the tasks
performed by students. We have removed sections of the assignment that are not relevant to
this project. Additionally, information that is speciﬁc to the tools used, such as UI locations,
has also been removed. Text that has been removed is indicated by square brackets [ ].

C.1 Project Part 1

Throughout the course of this semester, you will perform and document a technical secu-
rity review of OpenMRS (http://openmrs.org). This open-source systems provides electronic
health care functionality for “resource-constrained environments”. While the system has not
been designed for deployment within the United States, security and privacy concerns are still
a paramount security concern for any patient.

Software:
OpenMRS 2.9.0. There is no need to install OpenMRS. You will use the VCL image
CSC515 SoftwareSecurity Ubuntu.

Deliverables:
Submit a PDF with all deliverables in Gradescope. Only one submission should be performed
per team. Do not include your names/IDs/team name on the report to facilitate the peer
evaluation of your assignment (see Part 3 of this assignment).

1. Security test planning and execution (45 points)
a. Record how much total time (hours and minutes) your team spends to complete this activity
(test planning and test execution). Compute a metric of how many true positive defects you
found per hour of total eﬀort.

b. Test planning. Create 15 black box test cases to start a repeatable black box test plan for
the OpenMSR (Version 2.9). You may ﬁnd the OWASP Testing Guide and OWASP Proac-
tive Controls helpful references in addition to the references provided throughout the ASVS
document.

For each test case, you must specify:

– A unique test case id that maps to the ASVS, sticking to Level 1 and Level 2. Provide the
name/description of the ASVS control. Only one unique identiﬁer is needed (as opposed
to the example in the lecture slides). The ASVS number should be part of the one unique
identiﬁer.

– Detailed and repeatable (the same steps could be done by anyone who reads the instruc-

tions) instructions for how to execute the test case

– Expected results when running the test case. A passing test case would indicate a secure

system.

– Actual results of running the test case.
– Indicate the CWE (number and name) for the vulnerability you are testing for.

In choosing your test cases, we are looking for you to demonstrate your under-
standing of the vulnerability and what it would take to stress the system to see if
the vulnerability exists. You may have only one test case per ASVS control.

c. Extra credit (up to 5 points): Create a black box test case that will reveal the vulnerability
reported by the static analysis tool (Part 2 of this assignment) for up to 5 vulnerabilities (1
point per vulnerability). Provide the tool output (screen shot of the alert) from each tool.

2. Static analysis (45 points)
a. Record how much total time (hours and minutes) your team spends to complete this activity
(test planning and test execution). Compute a metric of how many defects you found per hour
of total eﬀort.

b. For each of the three tools (below), review the security reports. Based upon these reports:

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

71

– References:

– Troubleshooting VCL
– Opening OpenMRS on VCL

– Randomly choose 10 security alerts and provide a cross-reference back to the originating
report(s) where the alert was documented. Explore the code to determine if the alert is a
false positive or a true positive. The alerts analyzed MUST be security alerts even though
the tools will report “regular quality” alerts – you need to choose security alerts.

– If the alert is a false positive, explain why. If you have more than 5 false positives, keep
choosing alerts until you have 5 true positives while still reporting the false positives (which
may make you go above a total of 10).

– If the alert is a true positive, (1) explain how to ﬁx the vulnerability; (2) map the vulner-

ability to a CWE; (3) map the vulnerability to the ASVS control.

– Find the instructions for getting [SAST-3] going on OpenMRS here[hyperlink removed].

[Tool-speciﬁc instructions]

– Find the instructions for getting [SAST-2] going on OpenMRS here[hyperlink removed].

[Tool-speciﬁc instructions]

c. Extra credit (up to 5 points): Find 5 instances (1 point per instance) of a potential vulnera-
bility being reported by multiple tools. Provide the tool output (screen shot of the alert) from
each tool. Explore the code to determine if the alert is a false positive or a true positive. If
the alert is a false positive, explain why. If the alert is a true positive, explain how to ﬁx the
vulnerability.

3. Peer evaluation (10 points)
Perform a peer evaluation on another team. Produce a complete report of feedback for the
other team using this rubric [to be supplied]. Note: For any part of this course-long

project, you may not directly copy materials from other sources. You need to
adapt and make unique to OpenMRS. You should provide references to your
sources. Copying materials without attribution is plagiarism and will be treated
as an academic integrity violation.

C.2 Project Part 2

The fuzzing should be performed on the VCL Class Image (“CSC 515 Software Security
Ubuntu”).

0. Black Box Test Cases
Parts 1 (OWASP ZAP) and 2 ([DAST-2]) ask for you to write a black box test case. We use
the same format as was used in Project Part. For each test case, you must specify:

– A unique test case id that maps to the ASVS, sticking to Level 1 and Level 2. Provide the
name/description of the ASVS control. Only one unique identiﬁer is needed (as opposed
to the example in the lecture slides). The ASVS number should be part of the one unique
identiﬁer.

– Detailed and repeatable (the same steps could be done by anyone who reads the instruc-

tions) instructions for how to execute the test case

– Expected results when running the test case. A passing test case would indicate a secure

system.

– Actual results of running the test case.
– Indicate the CWE (number and name) for the vulnerability you are testing for.

1. OWASP ZAP (30 points, 3 points for each of the 5 test cases in the
two parts)
Client-side bypassing

– Record how much total time (hours and minutes) your team spends to complete this

activity. Provide:

72

Elder et al.

– Total time to plan and run the 5 black box test cases.
– Total number of vulnerabilities found.

– Plan 5 black box test cases (using format provided in Part 0 above) in which you stop
user input in OpenMRS with OWASP ZAP and change the input string to an attack.
(Consider using the strings that can be found in the ZAP rulesets, such as jbrofuzz) Use
these instructions as a guide.

– In your test case, be sure to document the page URL, the input ﬁeld, the initial user input,
and the malicious input. Describe what “ﬁller” information is used for the rest of the ﬁelds
on the page (if necessary).

– Run the test case and document the results.

Fuzzing

– Record how much total time (hours and minutes) your team spends to complete this

activity.

– Do not include time to run ZAP
– Provide:

• Total time to work with the ZAP output to identify the 5 vulnerabilities.
• Total time to plan and run the 5 black box test cases.

– Use the 5 client-side bypassing testcases (above) for this exercise.
– Use the jbrofuzz rulesets to perform a fuzzing exercise on OpenMRS with the following

vulnerability types: Injection, Buﬀer Overﬂow, XSS, and SQL Injection.

– Take a screen shot of ZAP information on the ﬁve test cases.
– Report the fuzzers you chose for each vulnerability type along with the results, and what
you believe the team would need to do to ﬁx any vulnerabilities you ﬁnd. If you don’t ﬁnd
any vulnerabilities, provide your reasoning as to why that was the case, and describe and
what mitigations the team must have in place such that there are no vulnerabilities.

2. DAST-2 (25 points)
[DAST-2] FAQ [hyperlink removed] and [DAST-2] Troubleshooting [hyperlink removed]

– Record how much total time (hours and minutes) your team spends to complete this

activity.

– Do not include time to run [DAST-2].
– Provide:

• Total time to work with the [DAST-2] output to identify the 5 vulnerabilities.
• Total time to plan and run the 5 black box test cases.

– Run [DAST-2] on OpenMRS. Run any 5 of your test cases from Project Part 1 to seed the
[DAST-2] run. Run [DAST-2] long enough that you feel you have captured enough true
positive vulnerabilities that you can complete ﬁve test case plans. Note: [DAST-2] will like
run out of memory if you run all 5 together. It is best to run each one separately. Also,
make sure you capture only the steps for your test cases, not other unnecessary steps.

– Export your results.
– Take a screen shot of [DAST-2] information on the ﬁve vulnerabilities you will explore
further. Write ﬁve black box test plans (using format provided in Part 0 above) to expose
ﬁve vulnerabilities detected by [DAST-2] (which may use a proxy). Hint: Your expected
results should be diﬀerent from the actual results since these test cases should be failing
test cases.

3. Vulnerable Dependencies (35 points)

[Assignment Section not Relevant]

4. Peer evaluation (10 points)

Perform a peer evaluation on another team. Produce a complete report of feedback for the
other team using this rubric (to be supplied).

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

73

C.3 Project Part 3

The project can be done on the VCL Class Image (“CSC 515 Software Security Ubuntu”).

0. Black Box Test Cases
Parts 1 (Logging), 2 ([Interactive Testing]), and 3 (Test coverage) ask for you to write black
box test cases. We use the same format as was used in Project Part 1. For each test case, you
must specify:

– A unique test case id that maps to the ASVS, sticking to Level 1 and Level 2. Provide the
name/description of the ASVS control. Only one unique identiﬁer is needed (as opposed
to the example in the lecture slides). The ASVS number should be part of the one unique
identiﬁer.

– Detailed and repeatable (the same steps could be done by anyone who reads the instruc-

tions) instructions for how to execute the test case

– Expected results when running the test case. A passing test case would indicate a secure

system.

– Actual results of running the test case.
– Indicate the CWE (number and name) for the vulnerability you are testing for.

1. Logging (25 points)
Where are the Log ﬁles? Check out the OpenMRS FAQ

– Record how much total time (hours and minutes) your team spends to complete this
activity (test planning and test execution). Compute a metric of how many true positive
defects you found per hour of total eﬀort.

– Write 10 black box test cases for ASVS V7 Levels 1 and 2. You can have multiple test
cases for the same control testing for logging in multiple areas of the application. What
should be logged to support non-repudiation/accountability should be in your expected
results.

– Run the test. Find and document the location of OpenMRS’s transaction logs.
– Write what is logged in the actual results column. The test case should fail if non-
repudiation/accountability is not supported (see the 6 Ws on page 3 of the lecture notes).
– Comment on the adequacy of OpenMRS’s logging overall based upon these 10 test cases.

2. Interactive Application Security Testing (25 points)

[Assignment Section not Relevant]

3. Test Coverage (25 points)
This test coverage relates to all work you have done in Project Parts 1, 2, and 3.
1. Compute your black box test coverage for each section of the ASVS (i.e. V1, V2, etc.)
which includes the black box tests you write for Part 2 (Seeker) for Level 1 and Level 2
controls. You get credit for a control (e.g. V1.1) if you have a test case for it. If you have
more than one test case for a control, you do not get extra credit –coverage is binary.
Coverage is computed as # of test cases / # of requirements.

2. (15 points, 3 points each) Write 5 more black box tests to increase your coverage of controls

you did not have a test case for.

3. (5 points) Recompute your test coverage. Report as below. Record how much total time
(hours and minutes) your team spends to complete this activity (test planning and test
execution). Compute a metric of how many true positive defects you found per hour of
total eﬀort.

4. (5 points) Reﬂect on the controls you have lower coverage for. Are these controls particu-

larly hard to test, we didn’t cover in class, you just didn’t get to it, etc.

Control

# of test
cases

# of L1 and
L2 controls

Coverage

V1.1: Secure development lifecycle ?

7

?/7

...

Total

74

Elder et al.

4. Vulnerability Discovery Comparison (15 points)
1. (5 points) Compare the ﬁve vulnerability detection techniques you have used this semester

by ﬁrst completing the table below.

– A: total number # of true positives for this detection type for all activities (Project

Parts 1-3)

– B: total time spent on all for all activities (Project Parts 1-3)
– Eﬃciency is A/B
– Exploitability: give a relative rating of the ability for this technique to ﬁnd exploitable

vulnerabilities

– Provide the CWE number for all the true positive vulnerabilities detected by this
technique. (This information will help you address the “wide range of vulnerability
types” question below.)

Technique

# of true
positive
vulnerabilities
discovered

Total
time
(hours)

Eﬃciency:
#
vulnerabilities /
total time

Detecting
Exploitable
vulnerabilities?
(High/Med/Low)

Unique
CWE
numbers

Manual black box

Static analysis

Dynamic analysis

Interactive
testing

2. (10 points) Use this data to re-answer the question that was on the midterm (that peo-
ple generally didn’t do too well on). Being able to understand the tradeoﬀs between the
techniques is a major learning objective of the class.
As eﬃciently and eﬀectively as possible, companies want to detect a wide range of ex-
ploitable vulnerabilities (both implementation bugs and design ﬂaws). Based upon your
experience with these techniques, compare their ability to eﬃciently and eﬀectively detect
a wide range of types of exploitable vulnerabilities.

5. Peer evaluation (10 points)
Perform a peer evaluation on another team. Produce a complete report of feedback for the
other team using this rubric [to be supplied].

C.4 Project Part 4

1. Protection Poker (20 points)
[Assignment Section not Relevant]

2. Vulnerability Fixes (35 points)
[Assignment Section not Relevant]

3. Exploratory Penetration Testing (35 points)
Each team member is to perform 3 hours of exploratory penetration testing on OpenMRS. This
testing is to be done opportunistically, based upon your general knowledge of OpenMRS but
without a test plan, as is done by professional penetration testers. DO NOT USE YOUR OLD
BLACK BOX TESTS FROM PRIOR MODULES. Use a screen video/voice screen recorder
to record your penetration testing actions. Speak aloud as you work to describe your actions,
such as, “I see the input ﬁeld for logging in. I’m going to see if 1=1 works for a password.”

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

75

or “I see a parameter in the URL, I’m going to see what happens if I change the URL.” You
should be speaking around once/minute to narrate what you are attempting. You
don’t have to do all 3 hours in one session, but you should have 3 hours of annotated video to
document your penetration testing. There’s lots of screen recorders available – if you know of
a free one and can suggest it to your classmates, please post on Piazza.

Pause the recording every time you have a true positive vulnerability. Note how long you have
been working so a log of your work and the time between vulnerability discovery is created
(For example, Vulnerability #1 was found at 1 hour and 12 minutes, Vulnerability #2 was
found at 1 hour and 30 minutes, etc.) If you work in multiple sessions, the elapsed time will
pick up where you left oﬀ the prior session – like if you do one session for 1 hour 15 minutes,
the second session begins at 1 hour 16 minutes. Take a screen shot and number each true
positive vulnerability . Record your actions such that this vulnerability could be replicated by
someone else via a black box test case. Record the CWE for your true positive vulnerability.
Record your work as in the following table. The reference info for video traceability is to aid a
reviewer in watching you ﬁnd the vulnerability. If you have one video, the “time” should aid
in ﬁnding the appropriate part of the video. If you have multiple videos, please specify which
video and what time on that video.

Vulnerability # Elapsed

Time

Ref Info for Video
Traceability

CWE

Commentary

Replication instructions via a black box test and the screenshots for each true positive vulner-
ability should appear below the table, labeled with the vulnerability number. Since you are
not recording all your steps, the replication instructions may not work completely since you
may change the state of the software somewhere along the line – document what you can via
a black box test and say the actual results don’t match your screenshot.

After you are complete, compute an eﬃciency metric (true positive vulnerability/hour) metric
for each student. Submit a table:

# vuln

Time

Eﬃciency

Name 1

Name 2

Name 3

Name 4

Total

Copy the eﬃciency table you turned in for Project Part 3 #4. Add an additional line for
Penetration testing. Compare and comment on this eﬃciency rate with the other vulnerability
discovery techniques in the table you input in #4 of Project Part 3.

– Each person on the team should submit one or more videos by uploading it/them to your
own google drive and providing a link to the video(s), sharing the video with anyone
who has the link and an NCSU login (which will allow peer evaluation and grading). The
video(s) should be approximately 3 hours in length.

– A person who does not submit a video can not be awarded the points for this part of the

project while the rest of the team can.

– It is possible to work for 3 hours and ﬁnd 0 vulnerabilities – real penetration tests con-
stantly work more than 3 hours without ﬁnding anything. That’s part of the reason for
documenting your work via video.

– For those team members who do submit videos, the grade will be an overall team grade.

Submission: The team submits one ﬁle with the links to the team member’s ﬁles.

4. Peer Evaluation (10 points)
Perform a peer evaluation on another team. Produce a complete report of feedback for the
other team using this rubric [to be supplied].

76

Elder et al.

D Appendix - Equipment Speciﬁcations

In this appendix we provide additional details of the equipment used in our case study. As
noted in Section 11, a key resource used in this project was the school’s Virtual Computing
Lab31 (VCL), which provided virtual machine (VM) instances. Researchers used VCL when
applying EMPT, SMPT, and DAST as part of data collection for RQ1. All student tasks were
performed using VCL for RQ1 and RQ2. Researchers created a system image including the
SUT (OpenMRS) as well as SAST and DAST tools. The base image was assigned 4-cores, 8G
RAM, and 40G disk space. An instance of this image could be checked out by students and
researchers and accessed remotely through a terminal using ssh or graphically using Remote
Desktop Protocol (RDP). Researchers also used two expanded instances of the base image
with 16 CPUs, 32GB RAM, and 80G disk space. For client-server tools, a server was setup in
a separate VCL instance by researchers with assistance from the teaching staﬀ of the course.
The server UI was accessible from VCL instances of the base image, while the server instance
itself was only accessible to researchers and teaching staﬀ. The server instance had 4 cores, 8G
RAM, and 60G disk space disk space, and contained the server software for SAST-1 used to
answer RQ2. All VCL instances in this study used the Ubuntu operating system.

The VCL alone was used for data collection for RQ2. However, the base VCL images were
small, and the remote connection to VCL could lag. Researchers used two used additional
resources as needed for RQ1 data collection. First, we created a VM in VirtualBox using the
same operating system (Ubuntu 18.04 LTS) and OpenMRS version (Version 2.9) as the VCL
images. This VM was used by researchers for SMPT and EMPT data collection, particularly
when reviewing the output of each technique where instances of the SUT were needed on an
ad hoc basis. The VM was assigned 2 CPUs, 4GB RAM, and 32G disk space and could be
copied and shared amongst researchers to run locally. Researchers increased the size of the VM
as needed, up to 8 CPUs and 16GB RAM when the host system could support the VM size. A
second VM was created in VirtualBox with the same speciﬁcations and operating system, but
with the server software for Sonarqube installed. We also used a desktop machine with 24 CPUs,
32G RAM, and 500G disk space. The desktop was running the Ubuntu operating system. This
machine was accessible through the terminal via ssh and graphically using x2go32. For RQ1
data collection we ran the SAST-1 server software directly on this machine. The desktop was
also used to run VirtualBox VMs for resource-intensive activity such as running Sonarqube
and DAST-2.

31https://vcl.apache.org/
32https://wiki.x2go.org/doku.php

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

77

E Appendix - All CWEs Table

Table 11 shows the CWE for high and medium severity vulnerabilities found. Table 12 provides
the same information for low severity vulnerabilities. The ﬁrst column of the table indicates
the CWE number. The CWEs are organized based on the OWASP Top Ten Categories. The
second column of the table indicates which, if any, of the OWASP Top Ten the vulnerability
maps to. Columns three and four of the table are the number of vulnerabilities found by
the techniques SMPT and EMPT. Columns ﬁve through eight break down the vulnerabilities
found by DAST and SAST by tool (ZAP, DA-2, Sonar, and SA-2). Column nine of Table 11
shows the total number of vulnerabilities found of each CWE type. The Total column is not
the same as the sum of the previous six columns. Some vulnerabilities were found using more
than one technique. Similarly, 20 Vulnerabilities were associated with more than one CWE;
therefore the total vulnerabilities for each technique as shown in Table5 may be lower than
the sum of each column in Table 11.

Table 11: CWEs associated with more severe Vulnerabilities

CWE

Top
Ten

SMPT EMPT

DAST

SAST

ZAP DA-2 Sonar SA-2

Total

A01 Broken Access Control

922 - Insecure Storage of Sensitive
Information

200 - Exposure of Sensitive
Information to an Unauth. Actor

601 - URL Redirection to
Untrusted Site (’Open Redirect’)

285 - Improper Authorization

22 - Path Traversal

A1

A1

A1

A1

A1

1

2

1

13

A02 Cryptographic Failures

326 - Inadequate Encryption
Strength

319 - Cleartext Transmission of
Sensitive Information

327 - Use of a Broken or Risky
Cryptographic Algorithm

643 - XPath Injection

89 - SQL Injection

20 - Improper Input Validation

79 - Cross-site Scripting

A1

A2

A2

A3

A3

A3

A3

1

1

A03 Injection

3

2

19

100

A04 Insecure Design

1

3

269 - Improper Privilege
Management

313 - Cleartext Storage in a File or
on Disk

770 - Allocation of Resources
Without Limits or Throttling

419 - Unprotected Primary
Channel

807 - Reliance on Untrusted
Inputs in a Security Decision

A4

A4

A4

A4

A4

1

1

1

1

2

9

19

1

4

19

1

1

2

9

13

19

1

1

2

1

4

21

124

1

1

1

2

3

2

7

2

3

78

CWE

TT SMPT EMPT ZAP DA-2 Sonar SA-2 Total

Elder et al.

73 - External Control of File Name
or Path

598 - Use of GET Request Method
With Sensitive Query Strings

A4

A4

2

5

1

A05 Security Misconﬁguration

548 - Exposure of Information
Through Directory Listing

614 - Sensitive Cookie in HTTPS
Session Without ‘Secure’ Attribute

16 - Conﬁguration

611 - Improper Restriction of
XML External Entity Reference

A5

A5

A5

A5

1

1

1

1

1

1

4

4

6

1

1

3

13

1

14

A06 Vulnerable and Outdated Components

A07 Identiﬁcation and Authentication Failures

308 - Use of Single-factor
Authentication

384 - Session Fixation

A7

A7

620 - Unveriﬁed Password Change A7

346 - Origin Validation Error

613 - Insuﬃcient Session
Expiration

521 - Weak Password
Requirements

A7

A5

A7

1

1

1

10

1

1

1

7

2

1

A08 Software and Data Integrity Failures

829 - Inclusion of Functionality
from Untrusted Control Sphere

502 - Deserialization of Untrusted
Data

A8

A8

1

A09 Security Logging and Monitoring Failures

532 - Insertion of Sensitive
Information into Log File

778 - Insuﬃcient Logging

A9

A9

1

2

1

9

A10 Server-Side Request Forgery (SSRF)

918 - Server-Side Request Forgery
(SSRF)

A10

1

509 - Replicating Malicious Code
(Virus or Worm)

NA

1

1

Not Mapped to OWASP Top Ten

1022 - Use of Web Link to
Untrusted Target with
window.opener Access

674 - Uncontrolled Recursion

567 - Unsynchronized Access to
Shared Data in a Multithreaded
Context

543 - Use of Singleton Pattern
Without Synchronization in a
Multithreaded Context

NA

NA

NA

NA

1

1

1

2

2

10

1

10

10

1

11

1

1

1

2

4

8

1

2

4

8

PREPRINT: Do I really need all this work to ﬁnd vulnerabilities?

79

CWE

TT SMPT EMPT ZAP DA-2 Sonar SA-2 Total

827 - Improper Control of
Document Type Deﬁnition

404 - Improper Resource
Shutdown or Release

NA

NA

13

1

39

14

39

Table 12: Low Severity Vulnerability CWEs

CWE

Top
Ten

SMPT EMPT

DAST

SAST

ZAP DA-2 Sonar SA-2

Total

A01 Broken Access Control

352 - Cross-Site Request Forgery A01

1

220

20

234

A02 Cryptographic Failures

760 - Use of a One-Way Hash with
a Predictable Salt

A02

A03 Injection

470 - Use of Externally-Controlled
Input to Select Classes or Code
(’Unsafe Reﬂection’)

A03

209 - Generation of Error Message
Containing Sensitive Information

A04 Insecure Design

A04

2

18

501 - Trust Boundary Violation

A04

A05 Security Misconﬁguration

7 - Missing Custom Error Page

933 - Security Misconﬁguration

16 - Conﬁguration

A05

A05

A05

1

2

1

1

1

1

2

1

1

A06 Vulnerable and Outdated Components

A07 Identiﬁcation and Authentication Failures

A08 Software and Data Integrity Failures

345 - Insuﬃcient Veriﬁcation of
Data Authenticity

502 - Deserialization of Untrusted
Data

A08

A08

1

A09 Security Logging and Monitoring Failures

A10 Server-Side Request Forgery (SSRF)

Not Mapped to OWASP Top Ten

242 - Use of Inherently Dangerous
Function

615 - Inclusion of Sensitive
Information in Source Code
Comments

404 - Improper Resource
Shutdown or Release

489 - Active Debug Code

582 - Array Declared Public,
Final, and Static

NA

NA

NA

NA

NA

2

2

34

34

18

28

1

1

2

1

1

2

5

17

26

31

28

1

2

5

17

1

26

31

80

CWE

TT SMPT EMPT ZAP DA-2 Sonar SA-2 Total

Elder et al.

754 - Improper Check for Unusual
or Exceptional Conditions

600 - Uncaught Exception in
Servlet

493 - Critical Public Variable
Without Final Modiﬁer

NA

NA

NA

31

60

210

3

31

60

210

