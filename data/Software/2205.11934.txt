Pynblint: a Static Analyzer for Python Jupyter Notebooks

Luigi Quaranta
University of Bari
Bari, Italy
luigi.quaranta@uniba.it

Fabio Calefato
University of Bari
Bari, Italy
fabio.calefato@uniba.it

Filippo Lanubile
University of Bari
Bari, Italy
ﬁlippo.lanubile@uniba.it

2
2
0
2

y
a
M
4
2

]
E
S
.
s
c
[

1
v
4
3
9
1
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Jupyter Notebook is the tool of choice of many data scientists in
the early stages of ML workﬂows. The notebook format, however,
has been criticized for inducing bad programming practices; in-
deed, researchers have already shown that open-source reposito-
ries are inundated by poor-quality notebooks. Low-quality output
from the prototypical stages of ML workﬂows constitutes a clear
bottleneck towards the productization of ML models. To foster the
creation of better notebooks, we developed Pynblint, a static an-
alyzer for Jupyter notebooks written in Python. The tool checks
the compliance of notebooks (and surrounding repositories) with
a set of empirically validated best practices and provides targeted
recommendations when violations are detected.

KEYWORDS
Computational notebooks, lint, software quality, data science, ma-
chine learning

1 INTRODUCTION
The massive adoption of AI-based technologies in the modern soft-
ware industry is raising new intriguing challenges, many of which
concern the shift of machine learning (ML) model prototypes into
production-ready software components. Often, a variety of factors
contribute to rendering this shift diﬃcult and costly to achieve;
these range from technical matters – like the complexity of repro-
ducing lab model performances in live systems – to human aspects,
arising from the coexistence of varied backgrounds and perspec-
tives in multidisciplinary teams [4].

In this context, a prominent role is played by the tools that prac-
titioners adopt at the various stages of ML workﬂows. Tool misuse
can hinder the quality of team collaboration and constitute a bot-
tleneck towards the productization of ML models. Notably, com-
putational notebooks represent a prime example. In the last few
years, they have established themselves as the tool of choice of
many data scientists for activities comprised in the early stages of
ML workﬂows, from data exploration to model prototyping. Their
most popular implementation, Jupyter Notebook,1 combines code,
documentation, and multimedia output in an interactive narrative
of computations, providing unparalleled support for fast experi-
mental iterations and lightweight documentation of experiments.
However, besides the evident beneﬁts they bring, Jupyter note-
books have been criticized for inducing bad programming habits
and oﬀering limited built-in support for software engineering best

practices [1, 2]. Indeed, researchers have already shown that note-
books often contain poor quality code and that – in spite of their
original vocation – they typically end up being messy and scarcely
documented [5, 7].

Commonly, poor-quality and hardly-reproducible notebooks, writ-

ten by data scientists in the early stages of ML workﬂows, get in
the way of the model productization process. Indeed, transitioning
from ML model prototypes to production-ready ML components
often entails, in practice, the consolidation of experimental code
from notebooks into structured and tested codebases [3]. Under
such circumstances, low-grade notebooks might represent an ex-
pensive bottleneck and a potential source of technical debt.

2 PYNBLINT
In our previous work [6], we collected and validated a catalog of
17 best practices for professional collaboration with computational
notebooks. Our guidelines foster a use of notebooks aware of soft-
ware engineering best practices, with the aim of boosting their ben-
eﬁts while preventing potential drawbacks. In the light of these
ﬁndings, we have developed Pynblint,2 a static analysis tool for
Jupyter notebooks written in Python.

Besides being the most popular notebook platform to date, Jupyter

Notebook has inspired the design of most modern computational
notebook implementations; the majority of them currently adopts
the .ipynb JSON-encoded format and oﬀers the same core func-
tionalities as Jupyter (e.g., Google Colaboratory3). Furthermore, most
Jupyter notebooks are written in Python. For these reasons we
chose Python Jupyter notebooks as the target of our static anal-
ysis tool.

2.1 Deﬁnition of the linting rules
Pynblint implements a set of checks to assess the quality of note-
books and the code repositories they belong to. We derived each
check (hereafter referred to as linting rule) as an operationalization
of the best practices from our catalog. These best practices range
from recommendations for better traceability and reproducibility
of computations (e.g., “Use version control” and “Manage project de-
pendencies”) to hints for code quality enhancements (e.g., “Stick
to coding standards” and “Test your code”), and clues for a more
consistent use of notebooks narrative capabilities (e.g., “Leverage
Markdown headings to structure your notebook”).

1https://jupyter.org

2https://github.com/collab-uniba/pynblint
3https://colab.research.google.com

 
 
 
 
 
 
Luigi Quaranta, Fabio Calefato, and Filippo Lanubile

This feature sets Pynblint apart from Julynter [5], which is – to
the best of our knowledge – the only alternative Jupyter linting
tool. Julynter was developed as a plug-in of Jupyter Lab, the evo-
lution of the Jupyter Notebook IDE; it performs real-time checks
on the quality and reproducibility of Jupyter notebooks while also
providing improvement recommendations. However, Julynter can
only be executed as a live assistant within Jupyter Lab and can-
not be leveraged oﬀ-line, as a standalone module. Therefore, while
useful during notebook writing, Julynter cannot be integrated into
CI/CD pipelines and cannot be adopted as a pre-commit hook. More-
over, being tied to the Jupyter Lab environment, it cannot be used
to analyze notebooks produced with diﬀerent platforms (e.g., Google
Colab or the Kaggle Notebooks IDE), even if they comply with the
standard Jupyter format (.ipynb).

3 CONCLUSION AND FUTURE WORK
We implemented Pynblint, a static analyzer for Python Jupyter
notebooks. To reﬁne the tool, we are currently conducting a for-
mative study with experienced Jupyter users; at the same time, we
are developing a web front-end for Pynblint, to make it easily acces-
sible by novice data scientists with limited or no command-line ex-
perience. Furthermore, we will validate the tool with a ﬁeld study,
involving data science professionals from multiple companies. As a
result of the validation process, we expect to expand the core set of
available linting rules and to make the tool proactive, i.e., capable
of automatically ﬁxing a selected set of violations.

REFERENCES
[1] Souti Chattopadhyay, Ishita Prasad, Austin Z Henley, Anita Sarma, and Titus
Barik. 2020. What’s wrong with computational notebooks? Pain points, needs,
and design opportunities. In Proc. of the 2020 CHI conference on human factors in
computing systems. 1–12. https://doi.org/10.1145/3313831.3376729

[2] Joel

Grus.

don’t
https://www.youtube.com/watch?v=7jiPeIFXb6U&t=0s
Institute for Artiﬁcial Intelligence) O’Reilly.

2018.

I

like
Joel Grus

notebooks.
(Allen

[3] Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Amoruso, Fabio Fu-
marola, and Michele Filannino. 2021. Towards Productizing AI/ML Models: An
Industry Perspective from Data Scientists. In 2021 IEEE/ACM 1st Workshop on AI
Engineering - Software Engineering for AI (WAIN). IEEE, Madrid, Spain, 129–132.
https://doi.org/10.1109/WAIN52551.2021.00027

[4] Nadia Nahar, Shurui Zhou, Grace Lewis, and Christian Kästner. 2021. Col-
laboration Challenges in Building ML-Enabled Systems: Communication, Doc-
umentation, Engineering, and Process.
arXiv:2110.10234 [cs] (Dec. 2021).
http://arxiv.org/abs/2110.10234 arXiv: 2110.10234.

[5] João Felipe Pimentel, Leonardo Murta, Vanessa Braganholo, and Juliana Freire.
Understanding and improving the quality and reproducibility of
Empirical Software Engineering 26, 4 (July 2021), 65.

2021.
Jupyter notebooks.
https://doi.org/10.1007/s10664-021-09961-9

[6] Luigi Quaranta, Fabio Calefato, and Filippo Lanubile. 2022. Eliciting Best Prac-
tices for Collaboration with Computational Notebooks. Proc. ACM Hum.-Comput.
Interact. 6, CSCW1, Article 87 (April 2022). https://doi.org/10.1145/3512934
[7] Jiawei Wang, Li Li, and Andreas Zeller. 2020. Better code, better sharing: On the
need of analyzing jupyter notebooks. In Proc. of the ACM/IEEE 42nd International
Conference on Software Engineering: New Ideas and Emerging Results. ACM, 53–56.
https://doi.org/10.1145/3377816.3381724

Not all best practices could be fully operationalized. For instance,
we found no practical way to verify compliance with the recom-
mendation “Distinguish production and development artifacts”. In
other cases, we resorted to partial operationalizations. For exam-
ple, compliance with the best practice “Make your data available”
is veriﬁed by detecting the use of DVC, a git-based Data Version
Control system with support for cloud remotes. We plan to extend
these operationalizations in the near future by considering larger
sets of implementations. Meanwhile, the related linting rules can
be disabled if they do not apply to speciﬁc professional settings.

In general, customization has been a primary concern in the
design of Pynblint. Not only the predeﬁned linting rules can be
dynamically included or excluded from the analysis, but the linting
engine itself features a plug-in architecture that enables Pynblint
users to add their own linting rules to the core set.

2.2 Interface
Pynblint oﬀers a command-line interface (CLI) capable of display-
ing detailed linting results, including the preview of ﬂawed cells.
The tool accepts three main types of input: (1) standalone Python
Jupyter Notebooks, to be analyzed in isolation; (2) local code repos-
itories containing Jupyter notebooks (both in the form of uncom-
pressed directories or compressed .zip archives); (3) GitHub pub-
lic repositories containing Jupyter notebooks. In the future, we
will extend the array of available input types by including, for
instance, standalone Google Colab notebooks as well as private
GitHub repositories. Other than rendered in the terminal, results
from the linting process can be saved as Markdown-formatted re-
ports or serialized in a machine-readable JSON format, allowing
further post-processing. Exporting results in the HTML format is on
our roadmap.

2.3 Usage
Pynblint is available on PyPI, the oﬃcial Python Package Index,
and therefore can be installed via package managers such as pip
and poetry. Once available in the active Python environment, the
tool can be used to analyze standalone notebooks (e.g., pynblint
Example.ipynb) as well as code repositories containing notebooks
(e.g., pynblint example/project/path).

When analyzing the working directory (i.e., pynblint .), the
linter will start by looking at the contents of the project root. Then,
it will recursively seek Python Jupyter notebooks in all existing
sub-directories. At the end of the process, the results are rendered
in the terminal.

Additional options can be tweaked for customized behavior; for
instance, to save linting results in a Markdown report, an output
ﬁlename with the .md extension should be speciﬁed (e.g., –output
report.md). More conveniently, recurring options can be declared
in a dotenv ﬁle named .pynblint, to be placed in the folder from
which the linter is invoked (typically, the project root).

As a static analyzer, Pynblint can be also leveraged in the con-
text of CI/CD pipelines. For instance, once installed on a CI/CD
server (e.g., GitHub Actions), it can be invoked alongside other
quality assurance tools at build time. According to user prefer-
ences, a build might fail if the linting process reveals potential note-
book defects.

