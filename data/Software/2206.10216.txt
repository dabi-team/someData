Preprint accepted by the AISafety’22 Workshop at IJCAI’22. To appear in a volume of CEUR Workshop Proceedings (http://ceur-ws.org/).

A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems

Yi Qi† , Philippa Ryan Conmy‡ , Wei Huang† , Xingyu Zhao† and Xiaowei Huang†
†Department of Computer Science, University of Liverpool, Liverpool, L69 3BX, U.K.
‡Adelard Part of NCC Group, London, N1 7UX, U.K.
{yiqi,w.huang23,xingyu.zhao,xiaowei.huang}@liverpool.ac.uk, pmrc@adelard.com

2
2
0
2

n
u
J

1
2

]
E
S
.
s
c
[

1
v
6
1
2
0
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Hazard and Operability Analysis (HAZOP) is a
powerful safety analysis technique with a long his-
tory in industrial process control domain. With
the increasing use of Machine Learning (ML)
components in cyber physical systems—so called
Learning-Enabled Systems (LESs), there is a recent
trend of applying HAZOP-like analysis to LESs.
While it shows a great potential to reserve the capa-
bility of doing sufﬁcient and systematic safety anal-
ysis, there are new technical challenges raised by
the novel characteristics of ML that require retroﬁt
of the conventional HAZOP technique. In this re-
gard, we present a new Hierarchical HAZOP-Like
method for LESs (HILLS). To deal with the com-
plexity of LESs, HILLS ﬁrst does “divide and con-
quer” by stratifying the whole system into three
levels, and then proceeds HAZOP on each level
to identify (latent-)hazards, causes, security threats
and mitigation (with new nodes and guide words).
Finally, HILLS attempts at linking and propagating
the causal relationship among those identiﬁed ele-
ments within and across the three levels via both
qualitative and quantitative methods. We exam-
ine and illustrate the utility of HILLS by a case
study on Autonomous Underwater Vehicles, with
discussions on assumptions and extensions to real-
world applications. HILLS, as a ﬁrst HAZOP-like
attempt on LESs that explicitly considers ML inter-
nal behaviours and its interactions with other com-
ponents, not only uncovers the inherent difﬁculties
of doing safety analysis for LESs, but also demon-
strates a good potential to tackle them.

1 Introduction
After initially developed to support the chemical process in-
dustries (by Lawley [Lawley, 1974]), Hazard and Operabil-
ity Analysis (HAZOP) has been successfully and widely ap-
plied in the past 50 years. It is generally acknowledged to

Copyright © 2022 for this paper by its authors. Use permitted
under Creative Commons License Attribution 4.0 International (CC
BY 4.0).

be an effective yet simple method to systematically identify
safety hazards. HAZOP is a prescriptive analysis procedure
designed to study the system operability by analysing the ef-
fects of any deviation from its design intent [Crawley and
Tyler, 2015b]. A HAZOP does semi-formal, systematic, and
critical examination of the process and engineering intentions
of the process design. The potential for hazards or operabil-
ity problems are thus assessed, and malfunction of individual
components and associated consequences for the whole sys-
tem can be identiﬁed [Dunj´o et al., 2010].

In recent years, increasingly sophisticated mathematical
modelling processes from Machine Learning (ML) are being
used to analyse complex data and then embedded into cy-
ber physical systems—so called Learning-Enabled Systems
(LESs). How to ensure the safety of LESs has become an
enormous challenge [Lane et al., 2016; Zhao et al., 2020;
Asaadi et al., 2020]. As LESs are disruptively novel, they
require new and advanced analysis for the complex require-
ments on their safe and reliable function [Bloomﬁeld et al.,
2019]. Such analysis needs to be tailored to fully evaluate
the new character of ML [Alves et al., 2018; Burton et al.,
2020], making conventional methods including HAZOP and
HAZOP-like variants (e.g., CHAZOP [Andow et al., 1991]
and PES-HAZOP [Burns and Pitblado, 1993] that are re-
spectively introduced for computer-based and programmable
electronic systems) obsolete. Moreover, LESs exhibit un-
precedented complexity, while past experience suggests that
HAZOP should be continuously retroﬁtted to accommodate
more complex systems [Pasman and Rogers, 2016], consider-
ing quantitative analysis frameworks [Ozog, 1987; Cozzani et
al., 2007] and human factors [Aspinall, 2006]. To the best of
our knowledge, there is no HAZOP-like safety analysis ded-
icated for LESs that takes into account ML characters while
preserving the simplicity and effectiveness of HAZOP (com-
paring to other conventional safety analysis methods [Sun et
al., 2022]), which motivates this research.

In this paper, we introduce a new Hierarchical HAZOP-
Like method for LESs (HILLS). HILLS ﬁrst stratiﬁes
the complex LESs into three levels—System Level, ML-
Lifecycle Level and Inner-ML Level, then applies HAZOP
separately on each level to identify safety elements of inter-
est, namely causes, mitigation, hazards (or latent-hazards for
latent levels that cannot directly lead to mishaps) and secu-
rity threats. When applying HAZOP on the ML related lev-

 
 
 
 
 
 
els, we revise HAZOP to cope with ML characteristics, e.g.,
by introducing new ways of deﬁning nodes and new sets of
guide words. We also identify causes of hazards from the ML
development process (modelled by the ML-Lifecycle level)
to reﬂect its data-driven nature (e.g., how data is collected,
processed, etc). Furthermore, we attempt to address the chal-
lenge of how to link and propagate those identiﬁed safety ele-
ments within and across three levels, then propose both qual-
itative and quantitative (an initial Bayesian Belief Network
(BN) solution) methods to model the casual relationships. To
examine the effectiveness and demonstrate the use case of
HILLS, we ﬁnally conduct a case study on Autonomous Un-
derwater Vehicles (AUVs), with discussions on assumptions
adopted and extensions to real-world applications.
The key contributions of this work include:
a) A ﬁrst HAZOP-like safety analysis for LESs that ex-
plicitly considers ML characters (including security threats
and the data-driven nature in the development process) and
reduces the complexity by hierarchical design.

b) New considerations of dividing nodes in the system rep-
resentation and a set of new guide words that adapt the tradi-
tional HAZOP method for lower levels regarding ML models.
c) A ﬁrst attempt at linking/propagating identiﬁed causes,

mitigation, (latent-)hazards & threats across ML levels.

d) Key challenges identiﬁed as a set of research questions
that are generic to safety analysis for LESs in future research.

2 Preliminaries: HAZOP
HAZOP is an inductive hazard assessment method that is con-
ducted by an expert team. It systematically investigates each
element in the system with the goal to ﬁnd the potential situ-
ation that could cause the element to pose hazards or limit the
system’s normal operations.

There are four basic steps to perform the HAZOP:
• Deﬁne the project scope/aims, and form the expert team.
• Identify system elements and model the system as a sys-

tem representation.

• Consider possible deviation of operational parameters.
• Identify hazards, causes and mitigation solutions.
Once the four steps are completed, team members may
generate additional safety requirements if necessary to mit-
igate or prevent the identiﬁed issues, leading to improvement
of the system. More details are given for each step of HAZOP
as what follows.

Form HAZOP team
To perform HAZOP, a team of specialists is formed accord-
ing to the project scope and aims. These experts have exten-
sive experience, expert knowledge and understand the overall
procedures of the system deeply, such as operations, mainte-
nance and engineering design.

Identify system elements
The HAZOP team will formally represent the system under
study by identifying the elements. Each element is called a
Node, representing an operational function. Then, nodes and
interactions between nodes (e.g., data/control ﬂows) collec-
tively form the system representation under analysis.

Consider deviations of operational parameters
HAZOP assumes that a problem can only arise when there are
some Deviations from the intent design. HAZOP searches
for deviations in the system representation. The deviation on
a node is expressed as the combination of Guide Words and
process Attributes .

Each guide word is a short word to create the imagination
of a deviation of the design/process intent. The most com-
monly used guide words are: no, more, less, as well as, part
of, other than, and so on. Guide words provide a systematic
and consistent means of brainstorming potential deviations to
normal operations. Each guide word has a speciﬁc meaning,
e.g., no means the complete negation of the design intention,
early means something occurred earlier than intended time.

Attributes are closely related to nodes, and are usually the
subject of the action being performed. The deﬁnition of at-
tributes relies on expert knowledge.

Identify hazards, causes and mitigation
Where the result of a deviation would be a danger to workers
or to the production process, a potential problem is found.
Hazard (H) is a source of potential damage, harm or ad-
verse health effects on something/someone, while mishaps
are damages or harms on something/someone. Cause (C) is
the reasons why the deviation could occur. It is possible that
several causes are identiﬁed for one deviation. Mitigation
(M) helps to reduce the occurrence frequency of the devia-
tions or to mitigate their consequences. Hazards, causes, and
mitigation are usually assigned with their respective IDs.

3 Problem Statement
Given HAZOP was not originally designed for LESs, in-
evitably new problems arise when attempting to apply HA-
ZOP on LESs. These problems are formalized as a set of
research questions (RQs) proposed in this section. We ﬁrst
present the rationale behind those RQs (i.e., justiﬁcation of
how we have come to the RQs) and then articulate what
would be the expected solution to each RQ.

RQ1: How to reduce the complexity of LESs so that HA-
ZOP can be effectively applied to? HAZOP is a semi-
formalised analytical method, used to identify the hazard sce-
narios of a deﬁned process, and it has been successfully used
on relatively simple systems. When facing a complex system,
HAZOP often cannot play its role well. LESs exhibit un-
precedented complexity, rendering directly applying HAZOP
to LESs infeasible. Therefore, we need to reduce the com-
plexity in the system representation. A simple yet effective
solution is by “divide and conquer”, e.g., stratifying a com-
plex system into multiple levels. In this regard, we believe a
promising solution to RQ1 is to propose a hierarchical system
representation, so that HAZOP can be effectively applied.

RQ2: How to deﬁne nodes in each level, especially for
novel levels regarding ML? We assume that HAZOP can
effectively handle a single level system representation, as we
expect to introduce a hierarchical structure in the RQ1 so-
lution. The second step of HAZOP is to divide nodes at
each level (presuming we already have a group of experts as
the HAZOP team). Past experience shows that division of

nodes can be based on the functionalities of components in
the system [Slater, 2015], so we may continue using such tra-
ditional method for those non-ML related levels. However,
when there are ML components in the system under analysis,
it is difﬁcult for the traditional division method of nodes to
be directly applied. Therefore, RQ2 is raised to explore the
novel deﬁnition of “functionalities” at ML-related levels.

RQ3: Will there be any new guide words related to ML?
Guide word is one of the key compositions of a deviation.
The team of experts is responsible for identifying guide words
that ﬁt the scope of their analysis, while common guide words
used were No, Less/More, Slower/Faster, Early/Late, etc.
However, the existing set of guide words is unproven for use
in ML applications, so this RQ aims at determining the ef-
fectiveness and new meanings of known guide words for ML
related levels, and checking whether there might be missing
guide words. Although we expect most of the known guide
words can still be applicable, they might miss some devia-
tions given the new characteristics of ML. Thus, prospective
new guide words may be introduced.

RQ4: How to establish the relationship between identiﬁed
safety elements across levels? For simplicity, HAZOP is
expected to be applied separately to each level of a hierarchi-
cal system representation. Therefore, to get the safety anal-
ysis of the whole complex system, it is necessary to study
the relationship between identiﬁed safety elements—namely
causes, mitigation, hazards (and latent-hazards)—across dif-
ferent levels. Then, based on the nature of the relationship
(e.g., causal or not, quantitative or qualitative, probabilistic or
deterministic), proper formalism should be used to establish
and express such relationship of those hazard analysis results
collected from each level.

4 Running Example

when received the user’s command. Once started, it uses sen-
sors (e.g., cameras) to receive data. Data is transmitted and
preprocessed before feeding into the YOLO model for ob-
ject detection and localisation. The localisation result is fur-
ther utilised for path planning. In addition, the above normal
workﬂow may suffer from external attacks on some stages,
including data transmission, data preparation, and path plan-
ning. We remark that, the scenario in the project is more com-
plex, including utilising deep reinforcement learning for mo-
tion planning, but for the space limit, this paper only focuses
on the perception component.

5 Proposed Method
In this section, we present the HILLS method, and compare
it with HAZOP. HILLS is inheriting from HAZOP the basic
structure composition and deﬁnitions of elements, with ex-
tensions that are suitable for LESs. The tables and ﬁgures
presented in this section are partial for illustrative purpose
only, cf.
the complete HILLS analysis results based on the
SOLITUDE project at the GitHub repository1.

5.1 Hierarchical HAZOP
As shown in Figure 2, HILLS has a three-level structure, in-
cluding system level, ML-lifecycle level and inner-ML level.
We analyse each level individually in this subsection, and dis-
cuss their relations in Section 5.2. Note, the HILLS structure
discussed here is generic (for illustration purpose), and may
be subject to adaptation when working with speciﬁc systems.

Figure 1: Workﬂow diagram of the running example

We present a running example from the SOLITUDE
project1, which conducts safety analysis on an AUV that au-
tonomously ﬁnds a dock and performs the docking task. The
workﬂow of the scenario is given in Figure 1. The robot starts

1https://github.com/Solitude-SAMR/UWV RAM

Figure 2: The 3 level hierarchical structure of HILLS

System level
HILLS at the system level largely follows HAZOP. Hardware,
software, and ML components of an LES represent different
functions, and they will be categorized as different Nodes.
Consider the running example in Figure 1, “blue blocks” rep-
resent the functional areas of the running example, which
means that our nodes can be set according to these blocks.
An example of setting nodes is provided in Table 1. We note,
the setting of nodes is speciﬁc to the system under investiga-
tion. E.g., the node “Labeling” was not included in Figure 1.

Table 1: Nodes in each level in SOLITUDE example

Level

Node

Description

System level
System level
System level
ML-lifecycle level
ML-lifecycle level
ML-lifecycle level
ML-lifecycle level
ML-lifecycle level
Inner-ML level
Inner-ML level
ML-lifecycle level

Node 1
Node 2
Node 3
Node 4
Node 5
Node 6
Node 7
Node 8
Node 9
Node 10
Node 11

User
Hardware components
Data transmission
Data collection
Labeling
Data preprocessing
Hyperparameter setting
Model deployment
Feature Extracting
Object Detection
Localisation

Some guide words originated from, e.g., the chemical in-
dustry can still be used in LESs. Attributes related to the LES
are used together with the guide words to express deviations.
Example 1. At system level, we discovered several hazards
from the running example, some of them are summarised in
Table 2. E.g., one of the hazards is “erratic trajectory”, sug-
gesting that the robot moves into an unsafe area. This hazard
is associated with a deviation “no action” where “no” is the
guide word and “action” is the attribute (when the AUV takes
no actions in the water, the disturbance of current makes it
difﬁcult for the robot to maintain a stable trajectory). One of
the causes of this hazard is “no data from sensor”, which can
be mitigated with, e.g., the deployment of an acoustic guid-
ance system as a duplicated perception component based on
another sensor.
Example 2. Some hazards, such as “erratic trajectory”, may
appear in different nodes, which suggests that they may occur
more often, and thus may have the higher priority to be miti-
gated after considering the severity of consequences as well.
Example 3. One hazard can be mitigated in different ways.
For example, we identiﬁed several mitigation solutions for the
“erratic trajectory”, most of which focus on early prevention,
such as “maximum safe distance maintained if uncertain”
and “camera health monitor”.

HILLS aims to exhaustively cover all potential hazards. In
the running example, the possible causes of crashes or failing
to turn directions when facing obstacles may include “no data
from sensors (instantaneous or permanent)”, and “misclassiﬁ-
cation”, corresponding to the errors in hardware and software
components, respectively. However, the hazards, causes or
mitigation may not be fully identiﬁable at this level. For ex-
ample, there are other mitigation solutions for the cause “mis-
classiﬁcation” that need to consider how the ML component
is trained and constructed. However, the system level alone
cannot naturally include relevant nodes for this purpose. This
motivates us to consider other levels (as discussed below).

ML-Lifecycle level
The key motivation for the ML-lifecycle level is to handle
the complexity arising from the integration of ML compo-
nents into an LES, considering mainly the human factors and
security threats involved in the development process of ML

models. Thus, deviations from this level cannot be identiﬁed
if analysis was only conducted at the system level. On the
other hand, the hazards at system level may be attributed to
the hazards at ML-lifecycle level, e.g., the low prediction ac-
curacy of ML component may be caused by the polluted data
in the data collection or insufﬁcient epochs of training.

For the running example, through the analysis at the ML-
lifecycle level, we know that the low accuracy of the results
may be caused by inaccurate labeling. We remark that, devia-
tions identiﬁed at non system level are called Latent-hazards
(LH), as they pose indirect hazards from latent levels with no
hardware components being interacted and thus cannot di-
rectly lead to mishaps.

Table 3 presents a set of guide words that are required at
this level. These guide words are redeﬁned from the existing
guide words in HAZOP. Table 3 includes both their original
meanings (in HAZOP) and new meanings (in HILLS). “part
of” represents a qualitative modiﬁcation in the original mean-
ing, and in HILLS it may mean the incompleteness of the
structures, deﬁnitions, or settings. For “Less” and “More”,
considering that we are concerned about data ﬂow and data
value, their new meanings refer to the amount of data rather
than, e.g., the water volume.

Safety analysis at the ML-lifecycle level can exhibit new
latent-hazards, as shown in Table 4. While ML models are
subject to security issues, we believe malicious attacking be-
haviors should also be considered as security Threats (T).

Human factors are considered because ML development is
a human-centered process, which makes possible some hu-
man related errors such as labelling errors, part of operations
were forgotten and the omission of data preparation. Afore-
mentioned mistakes are direct human errors. There are also
adversarial attacks that can lead to signiﬁcant drop in perfor-
mance, which are classiﬁed as security threats. A few exam-
ples are shown in Table 4.
Example 4. On the node “data collection”, there is a threat
“data poisoning”, which occurs because the input data is
contaminated. A suggested mitigation is to deploy a detec-
tor based on data provenance.
Example 5. For ML components, we identiﬁed mitigation,
e.g., “classiﬁer reliability for critical objects >X” [Zhao et
al., 2021a], to reduce misclassiﬁcations with safety impacts.
Example 6. For the latent-hazards “low prediction accu-
racy”, its causes include “users make mistakes on labelling”,
“data itself is missing”, and “data itself is incomplete”, each
of which has their suggested mitigation (cf. Table 4).

Example 7. There is a deviation “attack”, whose threats are
various attacks, e.g., evasion attack, backdoor attack, and
data poisoning attack. Their respective cause is usually that
a certain entity in the training or inference of an ML model
(e.g., input instance, model structure, training, dataset) is
perturbed, modiﬁed, or contaminated. Their respective mit-
igation can be very speciﬁc (cf. Table 4), e.g., the backdoor
detector in [Huang et al., 2022] for tree ensemble classiﬁers.

Inner-ML level
ML components such as YOLO are composed of one or more
ML models, each of which is formed of a set of functional

Node

Deviation

Hazard

Cause

Mitigation

Table 2: System level analysis (partial)

Data transmission (Flow from camera to classiﬁer) No action
Data transmission (Flow from camera to classiﬁer) No action
Data transmission (Flow from camera to classiﬁer) No action
Data transmission (Flow from camera to classiﬁer) No action
Data transmission (Data ﬂow)
Data transmission (Data value)
Data transmission (Data value)

Part of action Erratic trajectory
Wrong value
Wrong value

Loss of communication
Loss of communication

Erratic trajectory
Erratic trajectory
Erratic trajectory
Insufﬁcient energy/power No data from sensor (permanent) Camera health monitor (e.g. sanity check for blank images)

Acoustic guidance system
Situational awareness (route mapped and planned in advance)
Maximum safe distance maintained if uncertain

No data from sensor (transient)
No data from sensor (transient)
No data from sensor (transient)

Table 3: Redeﬁned guide words in the ML-Lifecycle level

Part of

Incomplete structure, deﬁnition or setting
Less

Guide word
Original meaning There is a qualitative modiﬁcation
New Meaning
Guide word
Original meaning Too little water or additive volume added
A less amount of data
New Meaning
Guide word
More
Original meaning Too much water or additive volume added
A large amount of data
New Meaning

layers. Even after a thorough analysis of all possible devia-
tions (with mitigation solutions) in the ML development pro-
cess modelled by our ML-lifecycle level, the ML components
may not perform as expected, e.g., the convolutional layers
fail to extract features accurately, and the fully connected lay-
ers fail to make reliable classiﬁcations. Thus, safety analysis
on the internal structure of an ML component is required.

At the inner-ML level, HILLS takes the method of extract-
ing basic layers of an ML component to form a model for
analysis. To cater for different complexity of the ML com-
ponent, two extraction methods are proposed. The ﬁrst one
deals with simple models with up to 5 layers. It follows the
layer structure and considers each layer to represent a sepa-
rate functionality. Consequently, each layer is deﬁned as a
node in the system representation. The second one deals with
more complex, larger models by abstracting a model into sev-
eral functional blocks and every block may contain a number
of layers. Our analysis in the running example follows the
second method.

We identiﬁed several new guide words, as shown in Ta-
ble 5, which are highly relevant to the setup of the ML com-
ponent and data ﬂow. It is worth noting that the “Perturbed”
is a special guide word that is needed when considering the
existence of an external attacker.
Example 8. Deviations containing “perturbed” are usually
proprietary attacks, e.g., we record “perturbed dataset” as
“attack” and the threat as “data poisoning” (cf. Table 4).

As shown in Table 6, HILLS performs analysis inside an
ML model, which in general is closely related to the internal
structure of the model.
Example 9. When the ML component has wrong output, we
can get from the inner-ML level analysis that this may be re-
lated to the setting of the hyperparameter. Explainable AI
(XAI) methods may help users to, e.g., locate which layer
of neurons contribute the most to the wrong ML behaviours
[Bach et al., 2015] and detect backdoors [Zhao et al., 2021b].

Corrupted sensor data
Hardware breakdown
Information conﬂict/lag

Reliable camera (robust to environment etc.)
Hardware monitor
Maximum safe distance maintained if uncertain

Example 10. At the inner-ML level, we focus on the ML
model structure itself. E.g., unsuitable parameter setting
in activation functions or pooling layers also make speciﬁc
latent-hazards. It also leads to wrong outputs or losing part
of information of ﬁgures (cf. Table 6).

Further Considerations on Use Cases of HILLS
HAZOP is to provide a systematic, critical examination of the
process (and engineering intent) of a new or existing facility,
and should normally be done before the system is ofﬁcially
put into service [Jurkiewicz et al., 2015]. Nevertheless, we
believe that HILLS can still be applied after the occurrence
of an accident, in particular the recent technologies have en-
abled the recording of system executions through, e.g., direct
observation, recorded video, or snapshot images. HILLS may
use the recordings to identify related causes and hazards.

Moreover, we note the following points when using
HILLS. First, when dealing with an LES, we focus on the
workﬂow or the pipeline diagram of the entire system, to
identify nodes according to the method we explained earlier.
The analysis at the system level can help us identify the haz-
ards sourced from the ML components, to enable the analysis
at the lower levels. Second, guide words will be combined
with the attributes of each node to form deviations. This will
proceed sequentially following the level structure of HILLS,
i.e., the deviations at the system level will be identiﬁed ﬁrst,
followed by the ML-lifecycle level, and the inner-ML level.
Third, before looking for (latent-)hazards, causes, and miti-
gation at each level, we are based on a reasonable assumption
that mitigation solutions of higher levels are easier than lower
levels. That said, HILLS may not need to be conducted at the
inner-ML level, and can stop when all hazards are found and
mitigated at other levels.

5.2 Relations Between Levels
Up to now, we have identiﬁed the nodes, attributes, guide
words, (latent-)hazards, threats, causes, and mitigation solu-
tions for individual levels in the HILLS framework. We also
notice that the relations between these elements can be very
complicated. This calls for a formal analysis of the relations.
While formalising the relations between levels is a signiﬁcant
challenge, and there might not be one best way, we propose
to study them both qualitatively and quantitatively.

Qualitative Analysis
Qualitative analysis studies the connections between levels,
with the guide words as entry points. The guide words and
the deviations may have the following connections.

First of all, the same guide words at a level have strong as-
sociations, even if they are combined with different attributes.

Node

Deviation

Latent-hazard & Threat

Cause

Mitigation

Table 4: ML-lifecycle level analysis (partial)

Labeling (Manually label data) Wrong label
Labeling (Manually label data) Wrong label
Labeling (Manually label data)
Labeling (Manually label data)
Data collection
Data preprocessing
Hyperparameter setting
Hyperparameter setting
Model deployment
Model deployment
Localisation
Localisation
Localisation
Localisation

Incapable label
Incapable label
Attacked
Part of data washing
Wrong setting
Wrong setting
Attacked
Attacked
No Localisation
No Localisation
Wrong Localisation Misposition
Wrong Localisation Misposition

Users make mistake with labeling
Users make mistake with labeling
Data itself is incomplete
Data itself is incomplete
Input data is contaminated
Data washing incomplete

Low prediction accuracy
Low prediction accuracy
Low prediction accuracy
Low prediction accuracy
Data Poisoning
Incorrect data ranges
Inappropriate hyperparameter User make mistake with setting
Inappropriate hyperparameter Unsuitable hyperparameter for setting
Robustness Attacks
Backdoor
Lose estimation of position
Lose estimation of position

Insert a calculated disturbance into the input data Defensive Distillation
XAI explain to input
Insert disturbance into the input data
Situational awareness (route mapped and planned in advance)
Hardware (sensors) breakdown
Common time to synchronise data and results
Hardware mismatch
Situational awareness (route mapped and planned in advance)
Slip rate too large
Common time to synchronise data and results
Combination miss between hardware and ML

Keep classiﬁer accuracy/reliability for critical objects >X
Sanity check for ground truth and label attribute
Keep classiﬁer accuracy/reliability for critical objects >X
Sanity check for ground truth and label attribute
Detection based on data provenance
Consistency Check (e.g. Value range)
Sanity check to hyperparameter
Continuing monitor to hyperparameter

Table 5: New guide words of ML-Lifecycle and inner-ML levels

Guide words

Meaning

Wrong

Invalid

Incomplete
Perturbed
Incapable

Wrong setting or data value
Invalid data value or data ﬂow, possibly
conﬂicting with other components
Incomplete data value
Data was perturbed by external attackers
Part of data can not be labeled

Second, if a guide word is the same between different levels,
the one in the higher level may contribute as the main reason
for the latent-hazard of the lower level.

Example 11. We use “no” as an example. We can get a devi-
ation “no action” at the system level, and have the deviation
“no localisation” in the ML-lifecycle level. Given they share
the same guide word, we should consider whether the “no
localisation” has a causality relation with the “no action”.

Moreover, it is assumed that there is an inclusive relation-
ship between the guide words of the higher level and lower
level, such as “no” and “part of”, or there are similar mean-
ings, such as “invalid” or “incompatible”.

The existence of a guide word with an inclusive relation-
ship suggests that for the latent-hazard found in the lower
level, its cause may belong to the higher level.

Example 12. If we choose “No action” at system level and
“Part of deﬁnition” at the ML-lifecycle level (e.g., images
without deﬁned labels), then we may establish an inclusive
relationship between “No” and “Part of”.

Example 13. We use “invalid data value” and “incompati-
ble data value” as examples, “incompatible data value” may
lead to the low accuracy of output or no results, it has a sim-
ilar meaning with “invalid data value”.

Selecting guide words is arguably a quite subjective ac-
tivity that experts may use different guide words with sim-
ilar semantics to identify the same cause. To this end, the
proposed way of establishing relationships across levels can
only cope with the ideal case in which identical guide words
are used. Thus, alternative methods are still needed for other
cases, which forms our important future work.

Quantitative Analysis
A BN is a graphical model that presents probabilistic rela-
tionships between a set of variables by determining causal
relationships between them [Lee et al., 2009].
It is also
a powerful tool for knowledge representation and reason-
ing under uncertainty, visually presenting probabilistic rela-
tionships between a set of variables [Cheng et al., 2002].
Actually, BN has already been used to study the relation
between latent features learned by a deep neural network
[Berthier et al., 2021]. While using BN to express rela-
tionship of elements is not a new idea in traditional safety
analysis [Thomas and Groth, 2021; Denney et al., 2011;
Zhao et al., 2012]. We take the relationship between several
elements at the ML-lifecycle level and the inner-ML level as
an example to explore the possibility of using BN to represent
it. This is an idea of quantitatively expressing relationships,
since the higher level contains some abstract concepts, it is
difﬁcult to represent in variables. Even if we assume that ab-
stract concepts are represented using variables, it is hard to
present Conditional Probability Tables (CPTs) as a prerequi-
site for BN to start. All parameters used to quantify BN must
be obtained based on system background and expert knowl-
edge.

Figure 3: A BN model fragment (with illustrative probabilities)

Figure 3 shows a fragment of the BN model for the run-
ning example, considering several security threats between
the ML-lifecycle level and the inner-ML level. The nodes of
a BN can represent threats (T l.i), causes (Cl.i), or mitigation
(M l.i), where variable l ∈ {1, 2, 3} ranges over the levels in
HILLS and i is the index of the threat/cause/mitigation at a
level. E.g., T 2.i is the i-th threat at ML-lifecycle level.

Besides, we need to assign CPT to each non-leaf node of
the BN, and assign a prior probability to the leaf or set the ob-
served evidence probability node. It is noted that the expert
knowledge is needed for both the construction of the basic

Node

Deviation

Latent-hazard & Threat

Cause

Mitigation

Table 6: Inner-ML level analysis (partial)

Feature extracting
Feature extracting Wrong extracting
Feature extracting Wrong extracting
Feature extracting Wrong extracting
Feature extracting Wrong extracting
Feature extracting Wrong extracting

Imprecise extracting Wrong outputs
Wrong outputs
Wrong outputs
Dying ReLU problem
Losing information of ﬁgures Unsuitable parameter setting in pooling layer Evaluate whether need pooling layer
Losing information of ﬁgures Unsuitable parameter setting in pooling layer Choose an appropriate pooling type

Less layers
Wrong hyperparameter setting
Unsuitable kernel size setting
Learning rate setting too large

Using deeper layers
Using Explainable AI (XAI) to locate
Kernel size need to match dataset size
Choosing suitable learning rate for ReLU (activation function)

structure and the assignment of CPTs. The probabilities used
in Figure 3 are for illustrative purposes, while more enlight-
ening examples can be found in [Berthier et al., 2021].
Example 14. For threat nodes with no incoming arrows, such
as T 2.i and T 3.i, we may set the probability of their occur-
rence to 100 percent.

Once constructed, we can make probabilistic inference on
the BN to ensure that the construction is correct w.r.t. ex-
pert knowledge. The following are two typical examples, by
applying the d-separation algorithm [Koller and Friedman,
2009] (for determining dependencies of variables in a BN).
Example 15. There may be multiple children nodes at dif-
ferent levels for a parent node. In Figure 3, the threat T 2.i
has two causes, C2.a and C3.a, at the ML-lifecycle level and
inner-ML level, respectively. While the two causes may be
mitigated separately as they belong to different levels, the
effectiveness of their respective mitigation might affect the
probabilistic inference based on each other’s CPT (under the
condition that the probability for T 2.1 is not observable).
Example 16. There may be multiple parent nodes for a child
node. In Figure 3, the mitigation M 2.a, has two causes, C2.a
and C2.b, representing that one mitigation may support two
causes. By observing the effectiveness of the mitigation (i.e.,
the CPT of M 2.a), we will infer how one cause C2.a may
inﬂuence the other cause C2.b and vice versa.

We note, the construction of the BN structure and CPTs,
as well as the above probabilistic inference, should be dis-
cussed and accepted by domain experts and all stakeholders.
We believe BN is potentially a powerful tool for the purpose
of modelling probabilistic causality relationship between ele-
ments of ML related levels, while how to apply BN in practice
in the context of HILLS remains an open challenge.

6 Related Work
HAZOP HAZOP is widely used in industrial domains,
such as nuclear power [Rimkeviˇcius et al., 2016] and chemi-
cal industry [Tian et al., 2015]. In recent years, there has been
efforts on integrating HAZOP with other methods [Marhavi-
las et al., 2020; Danko et al., 2019] to analyse common causes
and system scenarios [Roche et al., 2019]. A comprehensive
review of those techniques may refer to recent survey papers,
e.g. [Crawley and Tyler, 2015a]. The application of HAZOP
on computer-based systems ﬁrst appears in [Chudleigh and
Catmur, 1992]. After that, the experience gained from appli-
cation of HAZOP and related techniques to computer-based
systems was summarised in [Kletz, 1997]. There is a recent
trend of applying HAZOP-like analysis to LESs, e.g., in au-
tonomous driving context [Kramer et al., 2020].

Hierarchical structure The concept of hierarchy is not
new, but existing papers either focus on the hierarchical pri-
ority of the analysis order in the HAZOP analysis process
[Othman et al., 2016] or consider the direct application of the
HAZOP to the hierarchical structure of traditional systems
with no ML components [N´emeth et al., 2003]. A hierar-
chical structure is needed for its suitability to work with ML
components (black-box in general, and inside the black-box,
it is a layer-structure with each layer being a simple math-
ematical function). In HILLS, we innovatively consider the
interaction between humans and ML components and the in-
ternal structure of the ML components. Moreover, inspired
by [Wallace, 2005], we investigate how to link and propagate
identiﬁed safety elements at different levels.

STPA STAMP (Systems-Theoretic Accident Model and
Processes) is also a very popular safety analysis method.
STAMP uses three fundamental concepts from system theory:
Emergence and hierarchy, communication and control, and
process models [Leveson, 2011]. STPA (System-Theoretic
Process Analysis) uses such techniques, being based on the
STAMP model. STPA pays more attention to the overall con-
trol loop and process analysis of the system, and focuses on
unsafe control actions and causal factors in a control struc-
ture. It is widely used in railway safety assurances [Yang et
al., 2019], cyber safety and security [Kaneko et al., 2018],
robotics [Adriaensen et al., 2021] and driver-vehicle interac-
tions [Chen et al., 2020]. STPA is also used to explore a hier-
archical structural safety analysis framework in [Chaal et al.,
2020]. Comparing to STPA, HAZOP is relatively easier to
conduct and clearer to communicate, supported by structural
decomposition of the system functions [Sun et al., 2022]. We
start with retroﬁtting HAZOP for LESs, while STPA offers
a new perspective to consider the feasibility of hierarchical
safety analysis on LESs which is our planed future work.

7 Conclusion

We propose a hierarchical HAZOP-like method, HILLS, for
the safety analysis of LESs. Being different from the tradi-
tional HAZOP, HILLS analyses LESs in a hierarchical way,
disentangling the complexity by working with three separate
levels ﬁrst and then establishing their relations via both qual-
itative and quantitative methods, e.g., BNs. HILLS is applied
to a practical example of AUVs, with the discovery of new
guide words as well as new causes and mitigation related to
ML. HILLS complements HAZOP when working with LESs,
and is able to identify safety hazards and security threats re-
lated to ML components through its structural advantages.

Acknowledgments
This work is supported by U.K. DSTL through the project of
Safety Argument for Learning-enabled Autonomous Under-
water Vehicles and U.K. EPSRC through End-to-End Con-
ceptual Guarding of Neural Architectures [EP/T026995/1].
This project has received funding from the European
Union’s Horizon 2020 research and innovation programme
under grant agreement No 956123. XZ’s contribution to the
work is partially supported through Fellowships at the Assur-
ing Autonomy International Programme. YQ’s contribution
to the work is supported through Chinese Scholarship Coun-
cil (CSC).

References
[Adriaensen et al., 2021] A. Adriaensen,

Pintelon,
F. Costantino, G. Di Gravio, and R. Patriarca. An stpa
safety analysis case study of a collaborative robot appli-
cation. IFAC-PapersOnLine, 54(1):534–539, 2021. 17th
IFAC Symposium on Information Control Problems in
Manufacturing INCOM 2021.

L.

[Alves et al., 2018] Erin Alves, Devesh Bhatt, Brendan Hall,
Kevin Driscoll, Anitha Murugesan, and John Rushby.
Considerations in assuring safety of increasingly au-
tonomous systems. Technical Report NASA/CR-2018-
220080, NASA, 2018.

[Andow et al., 1991] Peter Andow, H Great Britain, and
E Safety. Guidance on HAZOP procedures for computer-
controlled plants. Great Britain, Health and Safety Execu-
tive, 1991.

[Asaadi et al., 2020] Erfan Asaadi, Ewen Denney,

and
Ganesh Pai. Quantifying assurance in learning-enabled
systems. In SafeComp’20, volume 12234 of LNCS, pages
270–286, Cham, 2020. Springer.

[Aspinall, 2006] P Aspinall. Hazops and human factors. In
Institution of Chemical Engineers Symposium Series, vol-
ume 151, page 820, 2006.

[Bach et al., 2015] Sebastian Bach, Alexander Binder,
Gr´egoire Montavon, Frederick Klauschen, Klaus-Robert
M¨uller, and Wojciech Samek. On pixel-wise explanations
for non-linear classiﬁer decisions by layer-wise relevance
propagation. PloS one, 10(7):e0130140, 2015.

[Berthier et al., 2021] Nicolas Berthier, Amany Alshareef,
James Sharp, Sven Schewe, and Xiaowei Huang. Abstrac-
tion and symbolic execution of deep neural networks with
bayesian approximation of hidden features. 2021.

[Bloomﬁeld et al., 2019] Robin Bloomﬁeld, Heidy Khlaaf,
Philippa Ryan Conmy, and Gareth Fletcher. Disruptive
innovations and disruptive assurance: Assuring machine
learning and autonomy. Computer, 52(9):82–89, 2019.
[Burns and Pitblado, 1993] D. J. Burns and R. M. Pitblado.
A Modiﬁed Hazop Methodology For Safety Critical. Lon-
don, 1993. Springer London.
[Burton et al., 2020] Simon Burton,

Ibrahim Habli, Tom
Lawton, John McDermid, Phillip Morgan, and Zoe Porter.

Mind the gaps: Assuring the safety of autonomous sys-
tems from an engineering, ethical, and legal perspective.
Artiﬁcial Intelligence, 279:103201, 2020.

[Chaal et al., 2020] Meriam Chaal, Osiris A. Valdez Banda,
Jon Arne Glomsrud, Sunil Basnet, Spyros Hirdaris, and
Pentti Kujala. A framework to model the stpa hierarchical
control structure of an autonomous ship. Safety Science,
132:104939, 2020.

[Chen et al., 2020] Shufeng Chen, Siddartha Khastgir, Islam
Babaev, and Paul Jennings. Identifying accident causes of
driver-vehicle interactions using system theoretic process
analysis (stpa). In 2020 IEEE Int. Conf. on Systems, Man,
and Cybernetics (SMC), pages 3247–3253, 2020.

[Cheng et al., 2002] Jie Cheng, Russell Greiner, Jonathan
Kelly, David Bell, and Weiru Liu. Learning bayesian net-
works from data: An information-theory based approach.
Artiﬁcial intelligence, 137(1-2):43–90, 2002.

[Chudleigh and Catmur, 1992] MF Chudleigh and JR Cat-
mur. Safety assessment of computer systems using HA-
ZOP and audit techniques. In SafeComp’92, pages 285–
292. Elsevier, 1992.

[Cozzani et al., 2007] Valerio Cozzani, Sarah Bonvicini,
Gigliola Spadoni, and Severino Zanelli. Hazmat transport:
A methodological framework for the risk analysis of mar-
shalling yards. Journal of Hazardous Materials, 147(1-
2):412–423, 2007.

[Crawley and Tyler, 2015a] F. Crawley and B. Tyler. HA-
ZOP: Guide to Best Practice. Elsevier Science, 2015.
[Crawley and Tyler, 2015b] Frank Crawley and Brian Tyler.
Chapter 3 - the hazop study method. In Frank Crawley and
Brian Tyler, editors, HAZOP: Guide to Best Practice (3rd
Edition). Elsevier, 2015.

[Danko et al., 2019] Matej Danko, J´an Janoˇsovsk`y, Juraj
Labovsk`y, and L’udov´ıt Jelemensk`y.
Integration of pro-
cess control protection layer into a simulation-based hazop
tool. Journal of Loss Prevention in the Process Industries,
57:291–303, 2019.

[Denney et al., 2011] E. Denney, G. Pai, and I. Habli. To-
wards measurement of conﬁdence in safety cases. In Int.
Symp. on Empirical Software Engin. and Measurement,
pages 380–383, 2011.

[Dunj´o et al., 2010] Jordi Dunj´o, Vasilis Fthenakis, Juan A.
V´ılchez, and Josep Arnaldos. Hazard and operability (ha-
zop) analysis. a literature review. Journal of Hazardous
Materials, 173(1-3):19–32, 2010.

[Huang et al., 2022] Wei Huang, Xingyu Zhao, and Xiaowei
Huang. Embedding and extraction of knowledge in tree
ensemble classiﬁers. Machine Learning, 111(5):1925–
1958, 2022.

[Jurkiewicz et al., 2015] Jakub Jurkiewicz, Jerzy Nawrocki,
Mirosław Ochodek, and Tomasz Głowacki. Hazop-based
identiﬁcation of events in use cases: An empirical study.
Empir Software Eng, 20(1):82–109, 2015.

[Kaneko et al., 2018] Tomoko Kaneko, Yuji Takahashi,
Takao Okubo, and Ryoichi Sasaki. Threat analysis using

stride with stamp/stpa. In Proc. of the Int. Workshop on
Evidence-based Security and Privacy in the Wild, 2018.
[Kletz, 1997] Trevor A. Kletz. Hazop–past and future. Relia-
bility Engineering & System Safety, 55(3):263–266, 1997.
[Koller and Friedman, 2009] D. Koller and N. Friedman.
Probabilistic Graphical Models: Principles and Tech-
niques. Adaptive computation and machine learning. MIT
Press, 2009.

[Kramer et al., 2020] Birte Kramer, Christian Neurohr,
Matthias B¨uker, Eckard B¨ode, Martin Fr¨anzle, and
Identiﬁcation and quantiﬁcation of haz-
Werner Damm.
ardous scenarios for automated driving. In International
Symposium on Model-Based Safety and Assessment, pages
163–178. Springer, 2020.

cause and system scenarios. Process Safety Progress,
38(2):e11997, 2019.

[Slater, 2015] David Slater. The Hazop methodology, 2015.
[Sun et al., 2022] Liangliang Sun, Yan-Fu Li, and Enrico
Zio. Comparison of the hazop, fmea, fram, and stpa meth-
ods for the hazard analysis of automatic emergency brake
systems. ASCE-ASME Journal of Risk and Uncertainty
in Engineering Systems, Part B: Mechanical Engineering,
8(3), 2022.

[Thomas and Groth, 2021] Stephen Thomas and Katrina
Groth. Toward a hybrid causal framework for autonomous
vehicle safety analysis. Proceedings of the Institution of
Mechanical Engineers, Part O: Journal of Risk and Relia-
bility, page 1748006X2110433, 08 2021.

[Lane et al., 2016] David Lane, David Bisset, Rob Bucking-
ham, Geoff Pegman, and Tony Prescott. New foresight
review on robotics and autonomous systems. Technical
Report No. 2016.1, LRF, 2016.

[Tian et al., 2015] Wende Tian, Tingzhao Du, and Shanjun
Mu. Hazop analysis-based dynamic simulation and its ap-
plication in chemical processes. Asia-Paciﬁc Journal of
Chemical Engineering, 10(6):923–935, 2015.

[Lawley, 1974] HG Lawley. Operability studies and hazard

analysis. Chem. Eng. Prog., 70(4):45–56, 1974.
[Lee et al., 2009] Eunchang Lee, Yongtae Park,

and
Jong Gye Shin. Large engineering project risk manage-
ment using a bayesian belief network. Expert Systems
with Applications, 36(3):5880–5887, 2009.

[Leveson, 2011] N. Leveson. Engineering a Safer World:
Systems Thinking Applied to Safety. Engineering systems.
MIT Press, 2011.

[Marhavilas et al., 2020] Panagiotis K Marhavilas, Michail
Filippidis, Georgios K Koulinas, and Dimitrios E Koulou-
riotis. An expanded hazop-study with fuzzy-ahp (xpa-
hazop technique): Application in a sour crude-oil process-
ing plant. Safety science, 124:104590, 2020.

[N´emeth et al., 2003] E N´emeth, R Lakner, KM Hangos, and
IT Cameron. Hierarchical cpn model-based diagnosis us-
ing HAZOP knowledge. Technical report of the Systems
and Control Laboratory SCL-009/2003, 2003.

[Othman et al., 2016] Mohamad Rizza Othman, Rosshila
Idris, Mimi Haryani Hassim, and Wan Hanisah Wan
Ibrahim. Prioritizing HAZOP analysis using analytic hi-
erarchy process (AHP). Clean Technologies and Environ-
mental Policy, 18(5):1345–1360, 2016.

[Ozog, 1987] Henry Ozog. Hazard identiﬁcation and quan-

tiﬁcation. Chem. Eng. Prog., 83:55–64, 1987.

[Pasman and Rogers, 2016] Hans Pasman

and William
Rogers. How can we improve hazop, our old work horse,
and do more with its results?
an overview of recent
Chemical Engineering Transactions,
developments.
48:829–834, 2016.

[Rimkeviˇcius et al., 2016] Sigitas Rimkeviˇcius, Mindaugas
Vaiˇsnoras, Egidijus Babilas, and Eugenijus Uˇspuras. Ha-
zop application for the nuclear power plants decommis-
sioning projects. Annals of Nuclear Energy, 2016.

[Roche et al., 2019] Eloise Roche, Watson Dupont, and An-
Beyond hazop: Analyzing common

gela Summers.

[Wallace, 2005] Malcolm Wallace. Modular architectural
representation and analysis of fault propagation and trans-
formation. Electronic Notes in Theoretical Computer Sci-
ence, 141(3):53–71, 2005.

[Yang et al., 2019] Pan Yang, Rin Karashima, Kozo Okano,
and Shinpei Ogata. Automated inspection method for an
stamp/stpa - fallen barrier trap at railroad crossing -. Pro-
cedia Computer Science, 159:1165–1174, 2019.

[Zhao et al., 2012] Xingyu Zhao, Dajian Zhang, Minyan Lu,
and Fuping Zeng. A new approach to assessment of conﬁ-
dence in assurance cases. In Computer Safety, Reliability,
and Security (SafeComp’12), volume 7613 of LNCS, pages
79–91. Springer, 2012.

[Zhao et al., 2020] Xingyu Zhao, Alec Banks, James Sharp,
Valentin Robu, David Flynn, Michael Fisher, and Xiaowei
Huang. A Safety Framework for Critical Systems Utilis-
ing Deep Neural Networks. In Computer Safety, Reliabil-
ity, and Security (SafeComp’20), volume 12234 of LNCS,
pages 244–259, Cham, 2020. Springer.

[Zhao et al., 2021a] Xingyu Zhao, Wei Huang, Alec Banks,
Victoria Cox, David Flynn, Sven Schewe, and Xiaowei
Huang. Assessing the reliability of deep learning clas-
siﬁers through robustness evaluation and operational pro-
ﬁles. In AISafety’21 Workshop at IJCAI’21, volume 2916.
ceur-ws.org, 2021.

[Zhao et al., 2021b] Xingyu Zhao, Wei Huang, Xiaowei
Huang, Valentin Robu, and David Flynn. BayLIME:
Bayesian local interpretable model-agnostic explanations.
In Proc. of the 37th Conf. on Uncertainty in Artiﬁcial In-
telligence, UAI’21, pages 887–896. PMLR, 2021.

