2
2
0
2

r
a

M
1
2

]
E
S
.
s
c
[

1
v
3
4
3
1
1
.
3
0
2
2
:
v
i
X
r
a

Using Evolutionary Coupling to Establish Relevance Links
Between Tests and Code Units. A case study on fault localization.

Jeongju Sohn
jeongju.sohn@uni.lu
Interdisciplinary Centre for Security, Reliability and
Trust, University of Luxembourg
Luxembourg

Mike Papadakis
michail.papadakis@uni.lu
Interdisciplinary Centre for Security, Reliability and
Trust, University of Luxembourg
Luxembourg

ABSTRACT
Many software engineering techniques, such as fault localization,
operate based on relevance relationships between tests and code.
These relationships are often inferred through the use of dynamic
test execution information (test execution traces) that approximate
the link between relevant code units and asserted, by the tests, pro-
gram behaviour. Unfortunately, in practice dynamic information is
not always available due to the overheads introduced by the instru-
mentation or the nature of the production environments. To deal
with this issue, we propose CEMENT, a static technique that auto-
matically infers such test and code relationships given the projects’
evolution. The key idea is that developers make relevant changes
on test and code units at the same period of time, i.e., co-evolution
of tests and code units reﬂects a probable link between them. We
evaluate CEMENT on 15 open source projects and show that it in-
deed captures relevant links. Additionally, we perform a fault local-
ization case study where we compare CEMENT with an existing In-
formation Retrieval-based Fault Localization (IRFL) technique and
show that it achieves comparable performance. A further analysis
of our results reveals a small overlap between the faults success-
fully localized by the two approaches suggesting complementar-
ity. In particular, out of the 39 successfully localized faults, two
are common while CEMENT and IRFL localize 16 and 21. These
results demonstrate that test and code evolutionary coupling can
eﬀectively support test and debugging activities.

1 INTRODUCTION
Software Engineering textbooks note that the majority of the total
eﬀort putted in a project during its life-cycle is during its mainte-
nance phase [28]. Consequently, many researchers are focusing on
automating software maintenance related activities such as auto-
mated testing, test data generation and automated debugging [6,
7, 9, 11, 33, 35]. These studies often rely on dynamic execution in-
formation, in order to identify links between tests and code. For
instance, the studies on test selection often rely on test execution
traces, to select tests that are likely to exercise the recently com-
mitted changes. Fault localization studies also adopt dynamic in-
formation to identify code locations that triggered test failures by
narrowing down the code aﬀected by a test failure [33].

While precise, dynamic test execution information such test cov-
erage is not always available, mainly due to the diﬃculty and the
cost of data collection [1, 17, 18, 26, 37]. For instance, test coverage,
one of the most frequently used dynamic code analysis informa-
tion, requires instrumentation, which may or may not be possible
to perform in given environments. Even when the employed en-
vironment supports test coverage collection, this functionality is

often turned-oﬀ, as it introduces signiﬁcant overheads in order to
log the related information [1, 18, 26]. Furthermore, when develop-
ers are under a fast-release cycle, they are unlikely to have enough
time for data collection at the ﬁrst place [5, 18, 37].

To overcome the absence of dynamic coverage information, re-
searchers have proposed static approaches that aim at guessing
(predicting) the relations between tests and code [2, 3, 23, 24, 31].
While encouraging, the working assumptions of these approaches
are not often met. For instance, Information Retrieval based Fault
Localization approaches [13, 24, 31] assume the existence of bug
reports, Focal method identiﬁcation techniques [8] assume partic-
ular test patterns [34] and similar naming convention that can be
captured by string-matching [23, 32, 36], assumptions that often
are not met.

We ﬁll this gap by proposing CEMENT (CoEvolution between
MEthod aNd Test), a static technique that automatically infers test
and code relationships given the projects’ historical evolution. Thus,
instead of relying on dynamic or static code analysis, CEMENT
it relies on how software has evolved. Precisely, CEMENT estab-
lishes links, called Evolutionary Couplings [38], between tests and
code units by checking the tests and code that have co-evolved
throughout the development.

The key idea is that changes, on both tests and code, made around
the same time imply a probable relevance coupling between them.
Consider for example bug ﬁxing cases, these are often followed
or preceded by additions/modiﬁcations of tests (to reproduce and
validate the repair action). Similarly, functionality additions are
frequently followed by test additions. Therefore, the co-evolution
analysis of tests and code units can capture a probable relevance
coupling between them.

While there are many aspects of software evolution that could
potentially be exploited, i.e, commit time, developer or the context
of the evolution, we stay simple by focusing only on whether tests
and code have been altered in similar periods of time (e.g., within
few commits to each other). CEMENT diﬀers from existing tech-
niques as it is independent of the dynamic execution traces and
the semantics of the source code. We posit that these diﬀerences
allow test and code evolutionary couplings to complement existing
techniques as they are capturing largely underexplored dependen-
cies. Furthermore, we believe that co-evolution of tests and code
units (i.e., methods) reveals important and hard to capture, by other
techniques, couplings.

We empirically evaluate CEMENT’s ability to infer links between
tests and code by investigating its ability to select relevant tests (for
given code methods) and to perform fault localization, i.e., success-
fully localize faulty methods given failed and passing test cases. We

 
 
 
 
 
 
thus, inject some faults/mutants, in essence applying mutation test-
ing [19], on a set of selected methods and check the ability of CE-
MENT to select tests that detect (kill) them. Then we design a fault
localization case study, where we investigate whether faulty meth-
ods have stronger links with failing tests than the passing tests.
This means that we form a novel fault localization method, on top
of CEMENT, and compare its performance with that of an existing
Information Retrieval-based Fault Localization technique [14] that
performs particularly well on the set of the projects that we study.
Our results, conducted on the 15 open source projects of De-
fects4J v.2.0.0 [10], show that CEMENT can infer relevant links
between tests and code methods that can support software mainte-
nance activities (e.g., fault localization) given the past co-evolution
of the software.

In summary, the technical contributions of this paper are:

• The introduction of evolutionary coupling between tests and
code, a novel type of coupling established based on how
tests and code have co-evolved. Additionally, since the cou-
pling between tests and code can be captured in a static way
CEMENT oﬀers advantages when dynamic information is
not available.

• Empirical evidence that CEMENT can establish evolution-
ary couplings between tests and code. Results from a fault
localization case study show that CEMENT can be useful in
fault localization.

• Empirical evidence that evolutionary coupling between tests
and code improves as software becomes more mature. The
comparison between the CEMENT’s fault localization re-
sults for the projects with diﬀerent levels of software matu-
rity shows that CEMENT becomes more eﬀective when the
project under inspection has actively evolved (i.e., changed).
• Empirical evidence that fault localization using CEMENT
complements state-of-the-art static fault localization tech-
niques. Our results shows that CEMENT can localize faults
for which an existing fault localization technique has failed,
implying the relevant links captured by CEMENT can com-
plement this technique.

2 EVOLUTIONARY COUPLING BETWEEN

TESTS AND CODE

The key idea underlying our approach is that developers make fo-
cused changes to their projects. Instead of making multiple irrele-
vant actions, they focus on one action at the time [9]. Even if the
changes are not serving one purpose they are closely related since
they are the result of the developer focus/attention. This means
that the changes committed around the same period are usually rel-
evant to each other. For instance, when developers repair faults in
code, they often introduce or modify a test to evaluate the repaired
part. Similarly, when they implement new functionality or update
existing ones, they probably introduce or update the tests related
to this functionality, at the same time or shortly after. Even when
some changes are irrelevant these should be eliminated through
the number of evolutions/changes as it is unlikely to have the same
irrelevant changes repeatedly. Based on these, we formulate the fol-
lowing hypothesis, which forms the main idea of our work:

Hypothesis: Changes in code units that are followed by
the changes in tests (and vice versa) imply a relevance rela-
tionship (coupling) between them. Similarly, changes in code
units not followed by changes in tests imply an absence of
relevance.

We use the established term evolutionary coupling [38] to name
the coupling between tests and code established from the above
hypothesis, i.e., guided by co-changes or, in other words, the co-
evolution of tests and code. This paper aims to investigate whether
this evolutionary coupling between tests and code can be useful in
software testing and debugging activities, more speciﬁcally, whether
they can be used to fault localization. Hence, we propose CEMENT,
a static approach that automatically infers links between tests and
code units relying on evolutionary coupling. We work with meth-
ods since they form discrete and localizable units, typically tar-
geted by automated techniques such as fault localization.

2.1 Tests and Code Evolutionary Coupling
Evolutionary coupling between tests and code exists when tests
and code have co-evolved throughout the software development:
it is independent of speciﬁc changes and simply focuses only on
whether the changes in tests and code occurred in similar time pe-
riods. As a result, this coupling does not require any dynamic in-
formation to be established, hence being easier to exploit in some
cases. In addition, by relying on the timing when each test and
method change was made, the relation captured by the evolution-
ary coupling is independent of any software testing and debug-
ging tasks, but reﬂects important links since they reﬂect the de-
veloper’s intent as developers tend to do them together. Another
distinct characteristic of evolutionary coupling is that it is inferred
from the projects’ evolution and thus, it evolves along with the
target software. The more changes performed to the software, or
the more mature the software is, the better. This feature can be
very helpful in development cases following the continuous inte-
gration development model, as they tend to evolve both tests and
code at the same time and include ﬁner-grained commits. Addi-
tionally, such evolutionary couplings can provide developers up-
to-date guidance on which code is linked to which tests and vice
versa easing comprehension.

2.2 CEMENT
For a given set of methods and tests, CEMENT identiﬁes evolution-
ary couplings, between each method and test pair, by computing
the average time interval between their past changes. Before going
into the details of CEMENT, we detail and show the distinct nature
of tests and method coupling that CEMENT aims for, which we call
the coupling asymmetry.

2.2.1 Asymmetry in the test and method coupling. In an ideal case
where each test and method assesses and implements a unique
functionality, the coupling between them is symmetric: a method is
associated with a test at a degree equal to x%, which is also equal to
the degree the test associates to the method. However, in practice,
we frequently observe cases where a single test exercises, directly
or indirectly, (simply relates) to multiple functionalities. We also
observe methods implementing multiple functionalities. For these

cases, the coupling between methods and tests becomes asymmet-
ric, i.e., a test may relate with a method at a diﬀerent degree than
the method with that test. The degree of the association reﬂects the
strength of the relation between entire tests and code methods. For
example, let us assume that method 𝑚 implements a functionality
𝑓 that is a part of a more extensive functionality 𝐹 ; test 𝑡 examines
the functionality 𝐹 , indirectly evaluating the functionality 𝑓 .

For method 𝑚, test 𝑡 is the most related one, as it evaluates its
main functionality, 𝑓 . However, test 𝑡 has a stronger degree of cou-
pling to another method 𝑚′ that carries out the entire functionality
𝐹 , making the coupling between method 𝑚 and test 𝑡 asymmet-
ric. Still, test 𝑡 is the one to run when we need to inspect method
𝑚. CEMENT takes into account this asymmetry and computes a
coupling degree separately for a method and for a test. It then ag-
gregates these values from both test and method sides to estimate
their ﬁnal coupling degree.

2.2.2 Computing the distance between the test and method. CE-
MENT measures the degree of evolutionary coupling between the
test and method as the time interval between their past changes.
For the time interval between two changes, CEMENT counts the
number of commits between them. Hereafter, we will refer to this
time interval as the distance. There are two additional points that
we need to consider when calculating this distance. First, tests and
methods are likely to be altered more than once, especially when
the software under inspection has evolved actively. Secondly, the
obtained distance is inherently asymmetric since it quantiﬁes the
degree of asymmetric coupling.

Algorithm 1 presents the pseudo-code of computing the distance
from target 𝑡 to 𝑡𝑐. Both 𝑡 and 𝑡𝑐 can be either a method or a test.
We denote as 𝑡 a test and 𝑡𝑐 a method. Tests and methods are likely
to be modiﬁed more than once from their introduction. Therefore,
we ﬁrst collect a list of commits that changed test 𝑡 and method 𝑡𝑐
(Line 1 and 2). Here, we are interested in inspecting whether the
test and method have been changed around the same time. Thus,
we consider only the distance to the nearest method change for
each test change rather than taking all the past changes of the
method into account (Line 3 to 7). If the test and method are re-
lated to each other, their changes can trigger the changes or be
triggered by the changes of the other party. Thus, we use the abso-
lute distance while we search for the nearest method change. We
then aggregate these distances computed for each past test change
by taking the average (Line 8). By using the average, we can reduce
the risk of being aﬀected by the outlier case where the method and
test were accidentally modiﬁed at the same time period.

The distance we deﬁne is asymmetric. For example, let us as-
sume that method 𝑀 was altered at the commits 𝑐0 and 𝑐3 and
test 𝑇 at the commits 𝑐1, 𝑐3, and 𝑐4. For the method changes at
𝑐0 and at 𝑐3, the distance to the nearest test change is 1 and 0,
respectively; the ﬁnal distance of method 𝑀 to test 𝑇 is thereby
1+0
2 . However, for test 𝑇 , the distance to method 𝑀 is 2
2 = 1
3 and
not 1
2 , as the shortest distance for individual test changes at 𝑐1, 𝑐3,
and 𝑐4 is 1 (| − 1|), 0, and 1, resulting in the average distance of
2
3 . This asymmetry of the distance has occurred for two reasons.
First, we consider only the nearest change, and while doing that,
we search both back and forth, allowing each change to select any
change as the nearest one regardless of whether the other party

Algorithm 1: DistanceToNearest

input : a target, t, a comparison target, t𝑐 , a distance

aggregation method, M𝑎𝑣𝑔

output : the distance of a target 𝑡 to the nearest changes in

the comparison target 𝑡𝑐

1 𝑅𝑒𝑣𝑠𝑖𝑜𝑛𝑠𝑡 ← ChangeTarget(𝑡)
2 𝑅𝑒𝑣𝑠𝑖𝑜𝑛𝑠𝑡𝑐 ← ChangeTarget(𝑡𝑐 )
3 𝐷𝑖𝑠𝑡𝑠 ← []
4 for 𝑟𝑒𝑣 in 𝑅𝑒𝑣𝑠𝑖𝑜𝑛𝑠𝑡 do
5

6
7 end
8 dist𝑡,𝑡𝑐 ← 𝑀𝑎𝑣𝑔 (𝐷𝑖𝑠𝑡𝑠)
9 return dist𝑡,𝑡𝑐

𝑑 ← 𝑚𝑖𝑛({distance(𝑟𝑒𝑣, 𝑟𝑒𝑣 ′)|𝑟𝑒𝑣 ′ ∈ 𝑅𝑒𝑣𝑠𝑖𝑜𝑛𝑠𝑡𝑐 })
add 𝑑 to 𝐷𝑖𝑠𝑡𝑠

also considers it as the nearest. More importantly, the coupling
between methods and tests is asymmetric. Since the distance is a
way to quantify the degree of the coupling between the test and
method, it naturally becomes asymmetric if the coupling is asym-
metric; this is also why we did not deﬁne the distance to work in
both ways. CEMENT considers this asymmetry and calculates dis-
tances from both directions, i.e., from a test to a method and the
opposite, when it estimates the coupling degree, i.e., the distance,
between tests and methods.

2.2.3 Establishing Links between Tests and Methods. Algorithm 2
describes how CEMENT infers relevant links between tests and
methods from the distances calculated from Algorithm 1. The ﬁ-
nal output of CEMENT is a list of methods/tests (𝐶) sorted in de-
scending order of their degree of coupling to the test/method under
inspection (𝑇 ): the higher the rank of a method/test is, the stronger
its link to the target test/method. For this, CEMENT ﬁrst calculates
the distance of target 𝑇 to each candidate in the list 𝐶 (Line 1). Af-
ter computing the distance to the target for each candidate in 𝐶,
we select the top 𝑁 that are closest to 𝑇 (Line 2). CEMENT then
calculates the distance to𝑇 for each candidate in 𝐶𝑁 and multiplies
newly obtained distances with their matching distances from the
target (Line 4 to 6). CEMENT then sorts the candidates in descend-
ing order using these updated distances; for those failed to be in
the top 𝑁 , we use their distance values in 𝐷𝑇 →𝐶 (Line 3). This ad-
ditional update for the top 𝑁 is to handle the asymmetric nature of
the distance. We set 𝑁 to 100, which we obtained empirically. To
summarize, CEMENT considers a method and a test to be relevant
(likely coupled) if and only if both of them are considered to be
close enough. This condition helps avoiding coincidental cases1.

3 EXPERIMENTAL SETTINGS
3.1 Research Questions
To evaluate CEMENT we start our investigation by checking its
ability to mine true links between code units and tests. Thus, we
ask:

1There is no need of having both methods and tests considering each other as the
most likely-to-be relevant one. Being relatively close to each other, compared to the
rest of tests/methods is suﬃcient to establish a link between them.

Algorithm 2: CEMENT

input : a target, T , a list of likely-relevant candidates, C, a
distance aggregation method, M𝑎𝑣𝑔, the number of
top candidates, N

output : a list of candidates C sorted in descending order of

the distance to the target T

1 𝐷𝑇 →𝐶 ← DistanceToNearest(𝑇 , 𝐶, 𝑀𝑎𝑣𝑔)
2 𝐶𝑁 ← SelectTopN(𝑁 , 𝐷𝑇 →𝐶, 𝐶)
3 𝐷𝑇 ↔𝐶 ← Initialize(𝐷𝑇 →𝐶, 𝑇 , 𝐶)
4 for 𝑐 in 𝐶𝑁 do
5

𝑑𝑐→𝑇 ← DistanceToNearest(𝑐, 𝑇 , 𝑀𝑎𝑣𝑔)
𝐷𝑇 ↔𝐶 [𝑐] ← 𝑑𝑐→𝑇 · 𝐷𝑇 ↔𝐶 [𝑐]

6 end
7 𝐶𝑟𝑎𝑛𝑘𝑒𝑑 ← Rank(𝐷𝑇 ↔𝐶, 𝐶)
8 return 𝐶𝑟𝑎𝑛𝑘𝑒𝑑

RQ1 Capability: can CEMENT establish static links between

tests and methods based on their co-evolution?

To answer this question, we need an oracle that decides on the
link between tests and methods. This type of oracle (i.e., general
associations between tests and methods), however, is hard to get
and there is no guarantee that it can be accurately approximated
[32]. To set such an oracle we use mutation testing [19] applied
at speciﬁcally selected methods and check whether tests selected
by CEMENT can indeed kill the mutants of the selected methods
and contrast them with randomly selected tests. The underlying
assumption here is that tests related to the methods should kill the
mutants that reside on these methods, at least kill more mutants,
than tests that are not related. We thus, expect the resulting test-
and-killed relations between tests and methods to overlap with the
links inferred by CEMENT.

Speciﬁcally, to answer this RQ, we select 𝑁 tests that are the
most likely to kill mutants in the methods we consider by picking
the top 𝑁 tests with the strongest coupling to these methods. As
CEMENT establishes a relationship between a test and a method
by default, we take additional steps to obtain a relationship be-
tween a test and multiple methods. To be speciﬁc, we ﬁrst repeat
CEMENT for each method using all tests, obtaining multiple rank-
ings for each test; we then take either the highest (i.e., the best) or
the average as the ﬁnal ranking for each test. These two are no-
𝑎𝑣𝑔
tated as CEMENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚 ; 𝑡 and 𝑚𝑚 denote
a test and multiple methods, respectively. We deem CEMENT ca-
pable of inferring the links between tests and methods from their
co-evolution if the tests ranked within the top 𝑁 by CEMENT can
kill more mutants than randomly selected 𝑁 tests.

𝑡→𝑚𝑚 and CEMENT

After investigating the existence of links between tests and meth-
ods, we turn our attention to a more concrete task in order to inves-
tigate whether these links oﬀer actionable information. Therefore,
we ask:

RQ2 Applicability: can we use the links between tests and
methods generated by CEMENT in software debugging?

To answer this question, we conduct a case study of CEMENT in
fault localization. We select fault localization among diﬀerent soft-
ware debugging activities since we can directly relate the resulting

test and method links to fault localization by assuming the meth-
ods strongly coupled to a failing test as suspicious ones. We also
compare CEMENT with an existing Information Retrieval-based
Fault Localization (IRFL) technique adopted in a recent program
repair technique, iFixR [14]. We choose this IRFL technique as our
baseline because it combines various IR-based features of faults,
summarising existing IRFL techniques.2 In addition, the IRFL is a
static approach, thereby allowing us to inspect further how CE-
MENT performs compared to existing static techniques.

After investigating the applicability of CEMENT on fault local-
ization we check whether its performance is dependent on the
project maturity. Hence, we ask:

RQ3 Impact of Software Maturity: how does the software

maturity aﬀect the eﬀectiveness of CEMENT?

CEMENT assumes tests and methods to be relevant if they have
co-evolved throughout the development. Thus, for CEMENT to be
useful, the program under inspection should be mature enough to
have a suﬃcient amount of previous changes for CEMENT to pro-
cess. Hence, to answer RQ3, we divide our dataset into two levels
of software maturity (i.e., Applicable and Conﬁdent) based on the
number of changes made on individual tests and methods.

• Applicable: tests and methods have changed at least once
• Conﬁdent: tests and methods have changed equal to or more
than the average number of times individual tests and meth-
ods have changed

We extend our fault localization case study to additionally have
these two diﬀerent levels of software maturity for each project and
investigate how the performance of CEMENT varies depending on
the software maturity. We achieve this by using these two software
maturity levels as the ﬁltering criteria: Applicable is to inspect the
changes in CEMENT’s performance after excluding the tests and
methods it cannot handle inherently, and Conﬁdent is to simulate
how CEMENT performs when the projects become more mature.

3.2 Subject
We evaluate CEMENT using Defects4J v.2.0.0 [10], a repository
that contains real-world faults of 17 open source projects. We se-
lect Defects4J mainly for two reasons. First, Defects4J provides an
inner command to run mutation testing on 17 projects it inves-
tigated. This inner command handles from compiling a project to
running a given test suite on the injected mutants, saving us a lot of
time and eﬀort that would spend on preparing an environment for
mutation testing. Second, Defects4J provides 835 real-world faults
and an infrastructure to replicate them easily, making it an opti-
mal target for our fault localization case study. Table 1 presents
the overall information about the projects that we studied: out of
these 17 projects, we use 14 to answer RQ1 and 15 to answer RQ2
and RQ3.

Subjects for Mutation Testing. Among 17 projects in Defects4J,

3.2.1
we fail to run mutation testing on Commons Collections and Mock-
ito using its inner mutation testing command. We simply exclude
these two from our targets for mutating testing, as our goal is not
about running mutation testing on every project in Defects4J. In

2iFixR performs statement-level fault localization by localizing faults at the ﬁle-level
ﬁrst using D&C that leverages various IR features [13]

this study, we use Git to collect the projects’ past changes, as 16 out
of 17 projects in Defects4J employ Git to manage the changes be-
tween versions; JfreeChart is the only one that uses SVN instead.3
We thus, excluded JfreeChart, leaving 14 projects for our analysis.

3.2.2
Subjects for Fault Localization. For our case study on fault
localization, we exclude Commons Collections and JfreeChart. We
exclude JfreeChart for the same reason above and Commons Col-
lections for a diﬀerent reason. As explained in RQ2, we assume
faulty methods to be more strongly related to failing tests than
non-faulty ones. For CEMENT to establish these relevant links be-
tween failing tests and methods, failing tests should exist before
the failure. However, for many of the faults in Defects4J, the fail-
ing tests marked by Defects4J were introduced the ﬁrst time to the
project when developers submitted ﬁx patches, making them im-
possible for CEMENT to handle. 4 Thus, we further ﬁlter out them,
excluding Commons Collections entirely since there were no re-
maining faults after this exclusion. Combined, we are left with 214
faults out of 835 faults.

We use the IRFL in iFixR as the baseline for our fault localiza-
tion case study. This IRFL technique also failed in JfreeChart for
a similar reason to ours. Among 214 faults, the IRFL failed to lo-
calize 39 faults due to missing bug reports or bug reports related
to more than one fault and 33 faults by failing to compute suspi-
ciousness scores for faulty statements. Thus, we further exclude
72 faults and use the remaining 142 faults for the baseline com-
parison in RQ2. Table 1 presents the number of faults remaining
after these exclusions for each project. RQ3 is about the impact of
software maturity on the performance of CEMENT. We use all 214
faults to answer RQ3 since we do not need to compare CEMENT
with the IRFL here.

Table 1: Test subjects. Mockito was excluded from RQ1. For
RQ2 and 3, all 15 projects were used. The value in parenthe-
ses is the number of faults after removing those the IRFL
cannot handle.

Project

Lang
Math
Time
Closure
Mockito
Cli
Codec
Compress (11)
Csv
Gson
JacksonCore
JacksonDatabind
JacksonXml
Jsoup
JxPath

# of
faults

23 (21)
32 (24)
3 (2)
60 (27)
8 (1)
12 (7)
6 (4)
12
4 (3)
2 (2)
6 (5)
19 (15)
1 (0)
21 (18)
5 (2)

# of methods

# of tests

# of previous commits
average

min – max

1959.3
4089.7
3782.7
9857.9
3118.4
216.9
408.0
1202.0
109.5
805.0
1718.0
5427.3
388.0
985.0
1518.2

2201.6
3288.1
5112.7
8435.6
2187.0
228.2
537.7
675.6
205.2
1353.5
722.3
3358.3
298.0
446.0
475.4

3695 – 4769
3929 – 10074
8849 – 8988
10332 – 23795
4296 – 5808
324 – 906
587 – 1301
569 – 3077
147 – 470
2157 – 2160
1621 – 3292
7187 – 10330
686 – 686
639 – 2134
1947 – 2045

4160.9
7378.2
8895.3
18293.5
5305.4
445.1
945.7
1877.6
314.8
2158.5
2440.3
8786.3
686.0
1431.0
1993.6

3Currently, JfreeChart has successfully migrated from SVN to Git. However, Defects4J
stills refers to the older version of JfreeChart and generates a new git branch that
contains only four commits made by Defects4J developers.
4Defects4J isolates each test failure and provides a separate commit that contains only
a single fault and fails only with the tests related this fault. Defects4J achieves this by
reversing the bug-ﬁxes and labeling the tests related to the ﬁxes as the failing tests.

3.3 Past Change Collection
CEMENT determines whether a given method and test are rele-
vant using the time intervals between their past changes. Thus, to
run CEMENT, we ﬁrst need to ﬁnd when each method and test was
changed. We employ Git v.2.32.0 [4] for this because it is a widely
used version control system and 15 projects we studied also utilize
Git to maintain their versions. For the experiments, we use De-
fects4J, which works with the project’s detached HEADs it created
instead of the main branch of the project. Thus, to avoid confu-
sion, such as the mismatch between commit hashes, we gather the
past changes of methods and tests from these detached HEADs.
This paper aims to demonstrate that CEMENT can infer the rel-
evant links between tests and methods based on their history of
co-evolution. Hence, rather than trying to achieve the best perfor-
mance by controlling the time window for past change collection,
we simply collect all prior changes of methods and tests. For each
past change, we record the hash of the commit that introduced the
change.

3.4 Mutation Testing
As mentioned in Section 3.2, Defects4J provides an inner muta-
tion testing command relying on Major. We thus, work with the
buggy versions because it becomes easier to relate the results we
obtained with mutation testing with those of the fault localization
case study we perform. Moreover, working with ﬁxed versions
may lead to overestimating the performance of CEMENT, as the
changes that generated the ﬁxed versions may contain informa-
tion that directly reveals the link between certain tests and meth-
ods (e.g., failing tests and faulty methods), which is rare in practice.
We select the most recent version among multiple buggy versions
for each project to maximize the number of past changes that CE-
MENT can exploit.

Mutation testing is computationally expensive [19]. Thus, we
reduce the scope of the mutation testing by mutating only the top
ten frequently modiﬁed classes instead of the entire code. We for-
mulate a test set that includes 20% of the entire tests of a target
project. This test set includes all the tests that are likely to execute
the selected classes according to the test and code naming conven-
tion [32]. In case there is available space after the selection process,
we randomly select the remaining number of tests.

3.5 Fault Localization
We conduct a case study on fault localization to evaluate the ap-
plicability of CEMENT in software testing and debugging. As for
the baseline of this case study, we select an Information Retrieval-
based Fault Localization (IRFL) technique adopted in a recent pro-
gram repair technique called iFixR [14]; we run iFixR on 17 projects
in Defects4J v.2.0.0. Since we work at the method granularity, we
aggregate the statement-level fault localization results of iFixR to
the method level, taking the highest statement-level suspicious-
ness score for each method. For CEMENT, we rank the methods
in descending order of their distances to failing tests: the higher
its rank is, the more suspicious the method is. Defects4J includes
faults that result in multiple failing tests. In these cases, we take the
highest method ranking among those computed with each failing
test; similarly, for multiple faulty methods, we take the highest.

While Defects4J provides real faults, the corresponding buggy
versions delivered by Defects4J have been tailored to contain only
a single fault [10]. Consequently, if we run CEMENT directly on
these buggy versions, we may include change information that ex-
plicitly reveals the locations of faults. To prevent this, instead of
working with the buggy versions given by Defects4J, we execute
CEMENT on the original buggy commits that Defects4J addition-
ally provides. For iFixR, we follow its own conﬁguration.

3.6 Evaluation Metrics
We evaluate the eﬀectiveness of CEMENT in establishing the links
between tests and methods from their evolutionary coupling us-
ing mutation score. Mutation Score (MS) is deﬁned as the ratio of
mutants killed by selected tests to the total number of generated
mutants, as below.

𝑀𝑆 (𝑇 ) = # of mutants killed by 𝑇

# of total mutants

, 𝑅𝑀𝑆 (𝑇 ) =

𝑀𝑆 (𝑇 )
𝑀𝑆𝑚𝑎𝑥 (= 𝑀𝑆 (𝑇𝑎𝑙𝑙 ))

𝑇 denotes a test set that contains 𝑁𝑇 tests. We deﬁne 𝑁𝑇 (i.e., the
number of selected tests) diﬀerently for each project, conﬁguring
this 𝑁𝑇 to be 10% of the total number of tests in the mutation test-
ing. We compare the 𝑀𝑆 score of test set 𝑇 with the 𝑀𝑆 score of
running all the tests (𝑇𝑎𝑙𝑙 ). We refer to the latter one as the max-
imum (𝑀𝑆𝑚𝑎𝑥 ); it is the total number of killed mutants over the
total number of generated mutants. Here, we want to evaluate the
trade-oﬀ between the eﬀort saved by running only 𝑁𝑇 test and
the performance degradation caused by it. Thus, we divide the 𝑀𝑆
score of test set 𝑇 by the maximum 𝑀𝑆 score. We notate this ratio
as 𝑅𝑀𝑆 and use it to evaluate the capability of CEMENT.

RQ2 and RQ3 are about the applicability of CEMENT in fault lo-
calization. We evaluate the fault localization performance of both
CEMENT and the baseline IRFL using 𝑎𝑐𝑐@𝑛 and 𝑤𝑒 𝑓 . These two
metrics measure the absolute eﬀort spent on localizing faults, fol-
lowing the guideline suggested by Parnin and Orso [20]. 𝑎𝑐𝑐@𝑛
counts the number of faults ranked within the top 𝑛 places; for 𝑛,
we use 1, 3, 5, and 10. 𝑤𝑒 𝑓 or wasted eﬀort is the number of non-
faulty elements examined before inspecting the ﬁrst faulty one. As
𝑤𝑒 𝑓 is computed per fault, we take the average and the median.

3.7 Tie Breaking
CEMENT does not guarantee to calculate a unique distance (i.e.,
coupling degree) for each method and test pair; in the worst case,
it may compute the same degree of coupling for all the method
and test pairs. To avoid overestimating the performance, we break
these potential ties between the method and test links by assigning
the lowest ranking that tied methods or tests can have to all those
that are tied. For a similar reasoning, we assign the lowest possible
ranking to the methods tied by having the same suspiciousness
score while evaluating the fault localization performance.

3.8 Implementation & Environment
CEMENT is implemented in Python version 3.9.9. All the experi-
ments were run on the machine equipped with Intel Core i7 CPU
and 32GB RAM. The replication package and all the results are
publicly available from https://doi.org/10.5281/zenodo.6366615.

𝑡→𝑚𝑚 and CEMENT

4 RESULTS
4.1 RQ1. Capability
Table 2 records the changes in mutation score when executing sub-
sets of the accompanied test suites, composed of 10% of the total
number of tests, using 𝑅𝑀𝑆 score. Overall, we obtained higher 𝑅𝑀𝑆
𝑎𝑣𝑔
scores in CEMENT (i.e., CEMENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚 )
than in the random test selection. For example, for JacksonCore,
𝑅𝑀𝑆 improves from 0.19 to 0.60, increasing the number of killed
mutants more than three times. Among 14 projects we investi-
gated, CEMENT successfully outperforms the random test selec-
tion baseline in 11 projects by taking the highest ranking for each
test among those it obtained with the considered methods (CE-
MENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚 ). When using the average instead of the highest (CE-
𝑎𝑣𝑔
𝑡→𝑚𝑚 ), the number of projects where CEMENT is supe-
MENT
rior than the random selection decreases by two. However, CE-
MENT 𝑎𝑣𝑔
𝑡→𝑚𝑚 still performs better than the random in nine out
of 14 projects. In fact, CEMENT consistently outperforms the ran-
dom test selection either by CEMENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚 .
The maximum Mutation Score (MS) assumes running all tests
participated in the mutation testing. Since we selected 10% of these
tests, all the 𝑅𝑀𝑆 scores in Table 2 being greater than 0.1, even in
the random, suggests that there exist large overlaps between the
tests in terms of the mutants they killed. Nonetheless, CEMENT
were able to select 10% of the tests that killed much more mutants
than the same number of randomly selected tests. For instance, in
JacksonXml, we can kill 74% of the total killed mutants by running
tests selected by CEMENT, whereas with the randomly selected
tests, we can kill only 28%. Based on these results of CEMENT
consistently outperforming the random, we posit that CEMENT
can establish relevant links between tests and methods given the
project’s evolution.

𝑡→𝑚𝑚 or CEMENT 𝑎𝑣𝑔

Answer to RQ1: Tests selected by CEMENT consistently kill
more mutants than those killed by randomly selected tests. These
results suggest that CEMENT successfully captures probable links
between tests and methods that can identify tests related to mu-
tated methods.

4.2 RQ2. Applicability
Table 3 presents the results of our case study of CEMENT on fault
localization. Compared to the Information Retrieval-based Fault
Localization (IRFL) technique used in a recent program repair method,
iFixR, CEMENT acquires comparable performance in terms of 𝑎𝑐𝑐@1:
CEMENT places 18 faults at the top of the rankings, whereas the
IRFL places 23 faults. For the sake of simplicity, we will call the IRFL
technique used in iFixR the IRFL, hereafter. While the IRFL sur-
passes CEMENT consistently in localizing faults, it also requires
more eﬀort to build the FL model and collect data for training and
evaluation: the IRFL collects 17 features (seven from bug reports
and ten from source code ﬁles) for fault localization. Moreover, the
IRFL assumes the existence of bug reports that in many cases is
not available and often requires an additional data preprocessing
step, which can be costly [14]. In contrast, CEMENT needs only
the hashes of commits that changed the methods and tests. Hence,

Table 2: The changes in mutation scores when running only 10% of the entire tests. Each cell contains 𝑅𝑀𝑆 , the ratio of the
mutation score obtained by the approach (row) to the maximum mutation score. When CEMENT (i.e., CEMENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚 and
𝑎𝑣𝑔
𝑡→𝑚𝑚 ) outperforms the random test selection (Random), the corresponding ratio (𝑅𝑀𝑆 ) is highlighted in bold. For
CEMENT
Random, its 𝑅𝑀𝑆 score is highlighted in bold if and only if it outperforms both CEMENT 𝑏𝑒𝑠𝑡

𝑡→𝑚𝑚 and CEMENT 𝑎𝑣𝑔

𝑡→𝑚𝑚 .

Approach

Lang Math Time Closure

Cli Codec Compress

Csv Gson

Jackson

Jackson
Core Databind

Jackson
Xml

Jsoup

JxPath

Random
CEMENT 𝑏𝑒𝑠𝑡
𝑡→𝑚𝑚
𝑎𝑣𝑔
CEMENT
𝑡→𝑚𝑚

0.22

0.22
0.54

0.31

0.40
0.38

0.47

0.58
0.36

0.43

0.28

0.49
0.40

0.33
0.52

0.36

0.72
0.47

0.30

0.49

0.18
0.49

0.67
0.65

0.20

0.38
0.14

0.19

0.60
0.13

0.15

0.16
0.33

0.28

0.74
0.64

0.49

0.52
0.48

0.29

0.45
0.49

concerning the cost spent up to the localization,5 we posit that CE-
MENT shows comparable performance to the IRFL.

An important discrepancy in the evaluation of IRFL happens
when bug reports explicitly specify the code elements that have
the reported bugs. In these cases, we do not need to localize faults
in the ﬁrst place, as they have been already identiﬁed by the person
reporting them. The IRFL localizes faults based on the similarity
between bug reports and source code. Therefore, if a bug report al-
ready contains the identiﬁer of a fault, we might overestimate the
performance of the IRFL, especially when it directly exploits the
identiﬁers of code elements in a bug report. Hence, we divide faults
into two groups based on whether their identiﬁers are already in
bug reports and examine whether the localization performance dif-
fers between these two groups. We treat a bug report to contain the
identiﬁer of a fault if it has both the class and the method name of
the fault.

The leftmost column of Table 3 presents the localization results
of the faults whose identiﬁers are in bug reports; out of 142 faults
we examined, 54 already have their identiﬁers in bug reports. The
middle column shows the localization results of the faults without
their identiﬁers in bug reports; the rightmost column describes the
combined results. Overall, the IRFL performs better when bug re-
ports contain fault identiﬁers: the IRFL places 15 out of 54 faults
at the top (𝑎𝑐𝑐@1) for the group of faults with their identiﬁers in
bug reports, whereas it places only eight at the top for the other
group without the identiﬁers, even though more faults belong to
the latter group. We observe similar trends in 𝑎𝑐𝑐@3, 5, 10.

Compared to the IRFL, having fault identiﬁers in bug reports
does not have the same eﬀect on CEMENT. While CEMENT lo-
calizes more faults near the top for the group where bug reports
have fault identiﬁers, the diﬀerence is smaller; for example, for the
group without fault identiﬁers, the 𝑎𝑐𝑐@1 decreases almost by half
in the IRFL, whereas, in CEMENT, it decreases only by two, from
10 to 8. Even this small decrease is from elsewhere, as CEMENT
does not leverage both source code and bug reports in the ﬁrst
place; we suspect that the observed decreases are coincidental and
are attributed to the characteristics of faults in each group.6 Fur-
thermore, CEMENT becomes more comparable to the IRFL for the

5With CEMENT, each fault localization task itself was done within seconds, and the
past change collection, which covers over thousands of commits here, took on aver-
age within 5 minutes, without any optimization. This cost can be further reduced in
practice, as we do not have to process all prior commits every time; the changes can
be collected incrementally.
6If a method evolves actively, this method is more likely to have failed in the past
than those rarely changed. Subsequently, if the method frequently fails, a reporter

faults without their identiﬁers in the bug reports: compared to the
IRFL, CEMENT ranks the same number of faults at the top and
within the top three (i.e., 𝑎𝑐𝑐@1 and 𝑎𝑐𝑐@3) and locates only one
less fault within the top ﬁve (i.e., 𝑎𝑐𝑐@5). CEMENT fails to compete
with the IRFL in 𝑤𝑒 𝑓 , although the diﬀerence between CEMENT
and the IRFL becomes smaller in the median compared to the av-
erage. Regarding the previous observation in 𝑎𝑐𝑐@𝑛, this result is
likely from CEMENT completely failing on some faults, assigning
the lowest rank to them. Nevertheless, CEMENT still achieves com-
parable performance in terms of localizing faults near the top, sug-
gesting that CEMENT can be useful in fault localization.

Answer to RQ2: CEMENT achieves comparable performance
to the recent Information Retrieval-based Fault Localization (IRFL)
technique, especially for the cases where this IRFL technique per-
forms less eﬀective.

4.3 RQ3. The Impact of Software Maturity
Table 4 presents the fault localization results of CEMENT for the
complete set of 214 faults with two variations on software matu-
rity: Applicable and Conﬁdent. Methods or tests being "Applicable"
means they have changed at least once, and being "Conﬁdent" im-
plies that they have been altered more frequently than the average.
We apply these two maturity criteria to each buggy version of tar-
get projects, ﬁltering out the methods and tests that failed to meet
them. We did not diﬀerentiate faulty methods and failing tests from
other methods and tests while applying these criteria. As a result,
14 and 129 faults out of 214 faults were excluded by Applicable and
Conﬁdent criteria, respectively.

The results in Table 4 show that the performance of CEMENT
can be improved as the software becomes more mature. For in-
stance, when we apply Applicable criterion (Applicable*), we ob-
serve small improvements in the percentage of faults localized near
the top, that are around 1% to 3%. The improvement is more evi-
dent in 𝑤𝑒 𝑓 where the average decreases by half and the median
by around 15. When we further ﬁlter out immature methods and
tests using Conﬁdent criterion (Conﬁdent*), this improvement be-
comes more prominent: the percentage of localized faults further
increases by 6% at the top and by around 6 to 8% within the top
three, ﬁve and ten compared to the results of applying Applicable

may already know that this method is the trigger when it causes a failure, and thereby,
includes its identiﬁer in the bug report.

Table 3: Comparison between the fault localization by CEMENT and the IRFL in iFixR. CEMENT becomes more comparable
to the IRFL when focusing only on the faults whose identiﬁers are not already in bug reports (Without Faulty Methods).

With Faulty Methods (CEMENT/ iFixR)

Without Faulty Methods (CEMENT/ iFixR)

All (CEMENT/ iFixR)

acc

wef

acc

wef

acc

wef

Proj. (w/wo/all)

Lang (16/5/21)
Math (12/12/24)
Time (1/1/2)
Closure (1/26/27)
Mockito (0/1/1)
Cli (3/4/7)
Codec (3/1/4)
Compress (6/5/11)
Csv (0/3/3)
Gson (2/0/2)
JacksonCore (1/4/5)
JacksonDatabind (5/10/15)
Jsoup (4/14/18)
JxPath (0/2/2)

@1

3/7
1/2
1/0
0/0
- / -
2/1
0/2
0/1
- / -
1/0
0/0
0/0
2/2
- / -

@3

8/9
1/6
1/1
0/0
- / -
2/3
0/3
2/1
- / -
1/0
0/0
0/2
2/3
- / -

@5 @10

mean

med @1

9/11
2/7
1/1
0/0
- / -
2/3
0/3
3/2
- / -
1/0
0/0
0/3
2/3
- / -

10/12
3/11
1/1
0/0
- / -
2/3
1/3
4/2
- / -
1/0
0/1
0/3
3/3
- / -

60.9/4.2
562.0/4.0
0.0/1.0
4315.0/20.0
-/-
18.0/1.0
98.7/0.7
152.2/19.7
-/-
15.0/52.5
1022.0/9.0
620.2/20.0
170.5/8.5
-/-

2.0/1
52.0/2
0.0/1
4315.0/20
-/-
0.0/1
96.0/0
5.0/14
-/-
15.0/52
1022.0/9
398.0/4
4.0/1
-/-

0/0
0/3
0/0
2/1
0/0
2/0
0/0
0/0
0/2
- / -
2/0
0/1
2/1
0/0

@3

2/1
0/6
0/0
3/2
1/0
3/1
0/0
0/0
1/3
- / -
3/0
0/2
3/1
0/0

@5 @10

mean

med

@1

@3

@5 @10

mean

med

3/2
0/8
1/0
4/2
1/0
3/1
0/0
0/0
1/3
- / -
3/0
0/2
4/3
0/0

3/3
0/11
1/0
6/4
1/0
3/1
0/0
0/2
1/3
- / -
3/0
0/4
5/7
0/1

441.8/11.8
787.8/11.6
4.0/60.0
2225.6/60.3
1.0/10.0
4.2/13.8
212.0/16.0
55.6/16.6
22.3/0.3
-/-
77.2/130.0
1819.5/38.4
215.1/35.9
279.5/120.0

3.0/6
86.0/2
4.0/60
242.0/22
1.0/10
0.0/17
212.0/16
50.0/19
18.0/0
-/-
0.0/127
1044.0/12
81.0/10
280.0/120

3/7
1/5
1/0
2/1
0/0
4/1
0/2
0/1
0/2
1/0
2/0
0/1
4/3
0/0

10/10
1/12
1/1
3/2
1/0
5/4
0/3
2/1
1/3
1/0
3/0
0/4
5/4
0/0

12/13
2/15
2/1
4/2
1/0
5/4
0/3
3/2
1/3
1/0
3/0
0/5
6/6
0/0

13/15
3/22
2/1
6/4
1/0
5/4
1/3
4/4
1/3
1/0
3/1
0/7
8/10
0/1

151.6/6.0
674.9/7.8
2.0/30.5
2303.0/58.8
1.0/10.0
10.1/8.3
127.0/4.5
108.3/18.3
22.3/0.3
15.0/52.5
266.2/105.8
1419.7/32.3
205.2/29.8
279.5/120.0

3.0/3
83.0/2
2.0/30
301.0/22
1.0/10
0.0/2
146.0/1
21.0/18
18.0/0
15.0/52
1.0/16
540.0/11
39.0/7
280.0/120

Total (54/88/142)

10/15

17/28

20/33

25/39

335.8/9.4

14.0/2

8/8

16/16

20/21

23/36

1047.5/41.3

80.0/12

18/23

33/44

40/54

48/75

776.8/29.2

63.0/8

criterion. In 𝑤𝑒 𝑓 , the average and the median decrease by around
one fourth when compared to those of the Applicable.

We exclude the methods and tests that failed to meet our matu-
rity criteria to allow CEMENT running on software systems with
diﬀerent maturity levels. Since we use absolute metrics (i.e., 𝑎𝑐𝑐@𝑛
and 𝑤𝑒 𝑓 ) in the evaluation, we consider the possibility that the per-
formance improvement that we observed may come from the de-
crease in the total number of methods and tests after the ﬁltering.
To verify that the improvement comes from the project maturity,
we apply the maturity criteria only on faulty methods and failing
tests, excluding faults that failed to meet these criteria. We then
rerun CEMENT for the remaining faults without any maturity ﬁl-
tering. This way, we focus on faults that appeared at the mature
part of the code without risking to overestimate. As shown in Ta-
ble 4, our results are almost the same as before, localizing one more
or fewer faults near the top, conﬁrming that the improvement we
witnessed indeed relates to the software maturity.

Answer to RQ3: The performance of CEMENT improves when
we focus only on the mature part of the code (i.e., have more than
the average number of past changes), implying CEMENT can en-
hance along with software maturity.

5 DISCUSSION
5.1 The Impact of Evolutionary Couplings in

Software Debugging

CEMENT establishes relevance links between tests and code only
when they have co-evolved. Thus, CEMENT may complement ex-
isting software debugging and testing techniques that rely on dy-
namic or static code analysis. To check for this potential comple-
mentarity, we revisit the results of RQ2, but at this time, inspect
which faults are localized by CEMENT and by the baseline IRFL
technique.

Table 5 reports the number of faults localized by CCCT, the IRFL,
and both. Overall, CEMENT and the IRFL localize diﬀerent faults
near the top. For the groups of faults whose identiﬁers are already
in bug reports, the faults localized at the top by both CEMENT

Table 4: Complete fault localization results of CEMENT. 𝑁
is the total number of faults in each project. "*" means that
Applicable/Conﬁdent ﬁltering criterion was applied to the
projects before extracting the evolutionary couplings, and
without "*" indicates that the ﬁltering criterion was used
only to exclude faults.

Project

Lang
Math
Time
Closure
Mockito
Cli
Codec
Compress
Csv
Gson
JacksonCore
JacksonDatabind
JacksonXml
Jsoup
JxPath

𝑁

23
32
3
60
8
12
6
12
4
2
6
19
1
21
5

1

3 (0.13)
2 (0.06)
1 (0.33)
7 (0.12)
2 (0.25)
5 (0.42)
0 (0.00)
0 (0.00)
0 (0.00)
1 (0.50)
2 (0.33)
0 (0.00)
0 (0.00)
4 (0.19)
0 (0.00)

Total
Total (Applicable)
Total (Applicable*)
Total (Conﬁdent)
Total (Conﬁdent*)

214
198
198
85
85

27 (0.13)
27 (0.14)
27 (0.14)
16 (0.19)
17 (0.20)

acc@n
3

5

10 mean median

wef

10 (0.43)
3 (0.09)
1 (0.33)
11 (0.18)
3 (0.38)
7 (0.58)
0 (0.00)
2 (0.17)
1 (0.25)
1 (0.50)
3 (0.50)
0 (0.00)
0 (0.00)
5 (0.24)
0 (0.00)

47 (0.22)
47 (0.24)
47 (0.24)
27 (0.32)
27 (0.32)

12 (0.52)
4 (0.12)
2 (0.67)
12 (0.20)
3 (0.38)
8 (0.67)
1 (0.17)
3 (0.25)
1 (0.25)
1 (0.50)
3 (0.50)
0 (0.00)
1 (1.00)
6 (0.29)
0 (0.00)

57 (0.27)
57 (0.29)
56 (0.28)
32 (0.38)
31 (0.36)

13 (0.57)
7 (0.22)
2 (0.67)
16 (0.27)
5 (0.62)
8 (0.67)
2 (0.33)
4 (0.33)
1 (0.25)
1 (0.50)
3 (0.50)
0 (0.00)
1 (1.00)
8 (0.38)
0 (0.00)

71 (0.33)
71 (0.36)
71 (0.36)
38 (0.45)
37 (0.44)

145.3
692.0
867.3
2130.1
58.1
10.4
97.7
101.3
33.2
15.0
431.5
1369.1
3.0
221.6
444.4

906.2
432.2
432.2
180.5
115.3

4.0
79.5
4.0
228.5
7.0
1.0
85.0
23.0
32.5
15.0
154.5
540.0
3.0
64.0
400.0

64.5
50.5
50.5
16.0
14.0

and the IRFL is only two; CEMENT and the IRFL locate respec-
tively eight and 13 diﬀerent faults at the top. While there are more
in common between the faults localized by CEMENT and the IRFL
within the top three, ﬁve and ten, many faults are still localized by
only one of them. In cases where bug reports do not contain fault
identiﬁers, fewer faults are localized by both techniques. At the
same time, the number of faults localized only by CEMENT now
becomes similar to that of the IRFL; for example, both CEMENT
and the IRFL localize eight diﬀerent faults at the top (𝑎𝑐𝑐@1). In
total, CEMENT localizes 16 faults for which IRFL has failed at the
top; this is around 41% of total faults ranked at the top by either
approach. Within the top ten, 22 out of 97 faults are localized ex-
clusively by CEMENT. These results indicate that the relationships

between tests and code captured by CEMENT through the soft-
ware co-evolution are diﬀerent from those exploited in the cur-
rent IR-based fault localization techniques. Thus, we posit that CE-
MENT has the potential to complement current software debug-
ging and testing activities by establishing the relationships that
have remained underexplored.

5.2 Evolutionary Couplings and Traceability

Links between Tests and Code

There are some studies specialized in modelling test-and-tested re-
lationships among various relationships that tests and code can
have [21, 23, 32]. These relationships are called test-to-code trace-
ability links and aim at capturing the intent of tests, i.e., identify
the key functionality that is tested/asserted by a test, leaving out in-
directly tested functionality. This means that the traceability links
are abstract and not exact. Compared to test-to-code traceability
links, the evolutionary couplings established by CEMENT reﬂect
a more relaxed relation. For example, let us suppose the given test
and method belong to the same component and thereby have fre-
quently changed around the same time by developers. In this case,
even if the test does not directly test the method, CEMENT will
likely regard it as relevant to the method since they have co-evolved.
To further examine how CEMENT diﬀers from or relates to exiting
studies of test-to-code traceability links, we employ TCTRACER,
a state-of-the-art approach that automatically establishes test-to-
code traceability links [32]. We replicate the method-level trace-
ability links prediction study in the TCTRACER paper and investi-
gate whether CEMENT can predict these links.

Table 6 compares the performance of CEMENT and TCTRACER
in predicting test-to-code traceability links at the method level.7
Here, we assume that CEMENT generates a stronger link for a
method to the test that evaluates its functionality than to those
that do not. Thus, for each test, we rank methods in descending
order of their strength of the link to it. We then simply regard the
top ﬁve methods as having traceability links with the test. The per-
formance of CEMENT varies depending on the software maturity.
Thus, we extend this study by applying the two software matu-
rity criteria (i.e.,Applicable and Conﬁdent) in order to focus on the
traceability links coming from the more mature part of the code.
We achieve this by excluding the traceability links that failed to
meet the maturity criteria from the evaluation. Table 6 reports the
number of test-to-code traceability links we initially have and the
number of links after the ﬁltering.8

TCTRACER generally outperforms CEMENT in predicting test-
to-code traceability links. We believe that this due to the following
three reasons. First, we simply take the top ﬁve methods for the
prediction. Thus, even if CEMENT ranks a related method at the
top, we end up with four false positives, explaining the low per-
formance, especially in precision. Secondly, because the oracle of
test-to-code traceability links was formulated manually, it might
be biased toward the methods and tests with similar names. This

7The results of TCTRACER provided by the authors contain only method and test
pairs predicted to have a traceability link rather than the complete prediction results.
Hence, we only investigate precision, recall, and F1 score and exclude AUC and MAP
for this replication study.
8Because the current CEMENT implementation handles only methods and not con-
structors for Java, we exclude two additional traceability links for each project

may give some advantages to TCTRACER, for it leverages the tex-
tual similarity between the names. Finally and most importantly,
a test and a method can have an evolutionary coupling between
them without being considered in the test-and-tested relationship
as it could be an indirect link. Despite these, CEMENT excels TC-
TRACER in Chart when inspecting only the links that met the Con-
ﬁdent criterion: out of the remaining four test-to-code traceability
links, two of them are correctly predicted only by CEMENT.

When we inspected these cases, we found that these two are
when there are no common terms between test and method names
and when a test calls multiple methods, especially after calling the
related method. Since ﬁve out of eight techniques that TCTRACER
combines compare test and method names in order to predict the
traceability links, TCTRACER could be less eﬀective if the test and
method have entirely diﬀerent names. TCTRACER employs tech-
niques that exploit dynamic execution traces, such as Last Call Be-
fore Assertion, to complement this weakness. However, like the
second case that we observed, if there are many methods between
the test and the ground-truth method in an execution trace, TC-
TRACER becomes less successful in the prediction. CEMENT lever-
ages neither the dynamic execution traces nor the source code. As
a result, it is inherently free from all the issues that may arise from
using them; this allows CEMENT to handle the cases for which
TCTRACER has failed. Hence, we argue that CEMENT can com-
plement existing techniques of predicting test-to-code traceability
links, especially when working with mature software systems.

6 THREATS TO VALIDITY
A primary threat to validity of our work is the absence of the ora-
cle for the evolutionary couplings between tests and methods. To
mitigate this threat, we used mutation testing as a substitute of this
oracle. Since mutation testing has been often employed as a test or-
acle [19], we posit that it can also be useful for our case. In addition
to the capability of CEMENT in establishing the evolutionary cou-
plings, this study also investigates the usefulness of the resulting
couplings in software maintenance activities. For this, we select
fault localization as our target for the case study, as it is one of
the most actively studied areas in software maintenance [33]. We
select a recent Information Retrieval (IR) based fault localization
technique as the baseline because it combines multiple existing IR-
based techniques and, thereby, can summarize the current trend in
fault localization to some extent. [13, 14].

The threats to external validity relate to whether our ﬁndings on
the eﬀectiveness of the evolutionary coupling can be generalized
to other projects. We use 15 open source projects in Defects4J, a
benchmark of real-world faults, as our targets for evaluation. Still,
additional studies on industrial projects may be needed to fully
verify our hypothesis.

Threats to construct validity relate to the evaluation metrics we
use. To assess the capability of CEMENT to select likely-related
tests to given methods, we employ mutation score, a widely adopted
metric in mutation testing [19]. For the case study, we select three
absolute evaluation metrics that have been frequently employed in
fault localization [15, 20, 27, 30].

Table 5: Comparison between faults localized by CEMENT and the IRFL in iFixR. The value on the right (either) is the total
number of faults localized by either CEMENT or the IRFL (i.e., union), whereas the values on the left denote the number of
faults localized only by CEMENT, only by the IRFL, and by both (i.e., intersection), respectively. When CEMENT/the IRFL
localizes faults on which the other failed, the corresponding cell is highlighted in bold text.

With Faulty Methods
Without Faulty Methods

All

N

54
88

142

acc@1

acc@3

acc@5

acc@10

CEMENT/ iFixR / both

either CEMENT/ iFixR / both

either CEMENT/ iFixR / both

either CEMENT/ iFixR / both

either

8/13/2
8/8/0

16/21/2

23
16

39

6/17/11
14/14/2

20/31/13

34
30

64

8/21/12
15/16/5

23/37/17

41
36

77

8/22/17
14/27/9

22/49/26

47
50

97

Table 6: Comparison between CEMENT and TCTRACER.
The three values next to the project are the number of trace-
ability links without any ﬁltering, with Applicable and Con-
ﬁdent ﬁltering. The left and right values are the results of
CEMENT and TCTRACER, respectively, and they are all in
percentage.

Conf

Lang (74/44/7)
Recall

Prec

F1

Prec

IO (40/40/14)
Recall

F1

Chart (35/25/4)
Recall

Prec

F1

All
Applicable
Conﬁdent

10 / 86
12 / 59
8 / 12

16 / 78
26 / 89
22 / 89

12 / 82
17 / 71
12 / 21

9 / 67
9 / 67
6 / 29

22 / 82
22 / 82
14 / 100

13 / 74
13 / 74
9 / 45

5 / 23
5 / 17
21 / 2

9 / 74
12 / 76
75 / 50

6 / 35
7 / 28
33 / 4

7 RELATED WORK
Our deﬁnition of co-evolution depends on the past changes in tests
and methods. Several studies have leveraged past changes in tests
and methods. Defect prediction aims at predicting faults before ex-
ecuting them using code quality metrics that include past changes
in the software [12, 16, 22]. However, these usages of past changes
are often limited to enrich the description of individual code ele-
ments concerning a speciﬁc problem: e.g., methods that have changed
frequently are more likely to contain faults than those that have
not [22]. In other words, they use the past changes to describe the
characteristics of faults.

Sohn and Yoo [25, 27] considered the past changes of methods
to improve fault localization performance; similarly to defect pre-
diction, they used the past changes as another feature to describe
faults. In automated program repair, Saha et al. used past software
changes to further locate the code that is likely to undergo similar
repairs [25]. Although the ways they used the past changes varies,
all these studies utilize past software changes as additional data to
enhance their approaches; they can validate their main idea with-
out using the past changes. Furthermore, they inspect past changes
per code element rather than investigating them together to grasp
the overall picture of how software has changed. In contrast to the
existing work, CEMENT establishes couplings between tests and
methods, directly from how they have changed throughout the de-
velopment.

Association between tests and methods can be useful in various
software maintenance activities, as it can give developers hints on
how their changes aﬀect or will aﬀect others. Consequently, there
have been many studies on automatically mining this information,
either dynamically or statically. Studies on test-to-code traceability
links aim at setting explicit links between tests and code [21, 23, 32].
Rompaey and Demeyer investigated diverse sources of informa-
tion, ranging from naming convention to static and dynamic code

analysis, to automatically generate traceability links between tests
and code that can pinpoint which tests examine which part of
code [23]. Qusef et al. proposed SCOTCH that identiﬁes these trace-
ability links using dynamic slicing; SCOTCH uses dynamic slicing
to locate the area aﬀected by the last assertion in each unit test
case [21]. Mohammad et al. tried to improve the quality of the
traceability links by further ﬁndings the method that implements
the core functionality under testing based on the changes in the
object states [8]. Recently, White et al. combined multiple tech-
niques that automatically mine test-to-code traceability links, al-
lowing these techniques to complement each other [32]. Unlike all
these approaches that somehow employ dynamic program anal-
ysis to generate these links between tests and code, CEMENT is
purely static. Perhaps more importantly, the traceability links fo-
cus on test intents resulting in abstract relationship between test-
and-code, whereas the evolutionary couplings we use aim at cap-
turing important dependencies.

8 CONCLUSION
We propose CEMENT, a static approach that mines relevant links
between tests and code units without any dynamic or static code
analysis but through their history/evolution. The key idea of CE-
MENT is that tests and code that are relevant to each other are
likely to be changed, multiple times, around the same time by de-
velopers. Thus, CEMENT infers such relevance relationships using
the past co-evolution of the software under analysis. We empiri-
cally evaluate the capability of CEMENT in capturing such rele-
vance relationships using 15 open-source projects. We further con-
ducted a fault localization study to investigate the applicability and
actionability of these relationships in software debugging. The re-
sults show that CEMENT can establish and use such relationships
and that it is capable of achieving comparable performance to an
existing Information Retrieval-based fault localization technique.
Further analysis reveals that CEMENT can capture the relation-
ships between tests and code that are diﬀerent from those captured
by dynamic and static code analysis and thus, evidencing that CE-
MENT can complement the current approaches.

Our work forms the ﬁrst attempt to establish evolutionary cou-
plings between tests and code and thus, it opens a number of in-
teresting research directions. In particular, the exploration of addi-
tional aspects of software evolution such as actual commit time, de-
velopers and the context of the evolution could strengthen our rela-
tionships. Additionally, the formulation of hybrid techniques com-
bining evolutionary coupling with traceability linking techniques
[21, 23, 32] and historical evolution analysis techniques such as

refactoring miner [29] could lead to much stronger results. We
hope to explore these directions in the near future.

REFERENCES
[1] Antonia Bertolino, Antonio Guerriero, Breno Miranda, Roberto Pietrantuono,
and Stefano Russo. 2020. Learning-to-Rank vs Ranking-to-Learn: Strategies for
Regression Testing in Continuous Integration. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering (Seoul, South Korea)
(ICSE ’20). Association for Computing Machinery, New York, NY, USA, 1–12.
https://doi.org/10.1145/3377811.3380369

[2] Sam Blackshear, Nikos Gorogiannis, Peter W. O’Hearn, and Ilya Sergey. 2018.
RacerD: Compositional Static Race Detection. Proc. ACM Program. Lang. 2, OOP-
SLA, Article 144 (oct 2018), 28 pages. https://doi.org/10.1145/3276514

[3] Cristian Cadar and Koushik Sen. 2013.

Testing: Three Decades Later.
https://doi.org/10.1145/2408776.2408795

Symbolic Execution for Software
Commun. ACM 56, 2 (feb 2013), 82–90.

[4] Scott Chacon and Ben Straub. 2014. Pro git. Apress.
[5] Sebastian Elbaum, Gregg Rothermel, and John Penix. 2014. Techniques for Im-
proving Regression Testing in Continuous Integration Development Environ-
ments. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering (Hong Kong, China) (FSE 2014). ACM, New
York, NY, USA, 235–245. https://doi.org/10.1145/2635868.2635910

[6] Gordon Fraser and Andrea Arcuri. 2013. EvoSuite: On the Challenges of Test
Case Generation in the Real World. In Proceedings of the 2013 IEEE Sixth Interna-
tional Conference on Software Testing, Veriﬁcation and Validation (ICST ’13). IEEE
Computer Society, USA, 362–369. https://doi.org/10.1109/ICST.2013.51

[7] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019. Automatic Soft-
ware Repair: A Survey. IEEE Transactions on Software Engineering 45, 1 (2019),
34–67. https://doi.org/10.1109/TSE.2017.2755013

[8] Mohammad Ghafari, Carlo Ghezzi, and Konstantin Rubinov. 2015. Automatically
identifying focal methods under test in unit test cases. In 2015 IEEE 15th Inter-
national Working Conference on Source Code Analysis and Manipulation (SCAM).
61–70. https://doi.org/10.1109/SCAM.2015.7335402

[9] Mark Harman and Peter O’Hearn. 2018. From Start-ups to Scale-ups: Opportuni-
ties and Open Problems for Static and Dynamic Program Analysis. In 2018 IEEE
18th International Working Conference on Source Code Analysis and Manipulation
(SCAM). 1–23. https://doi.org/10.1109/SCAM.2018.00009

[10] René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Database
of Existing Faults to Enable Controlled Testing Studies for Java Programs. In
Proceedings of the 2014 International Symposium on Software Testing and Anal-
ysis (San Jose, CA, USA) (ISSTA 2014). ACM, New York, NY, USA, 437–440.
https://doi.org/10.1145/2610384.2628055

[11] Yasutaka Kamei and Emad Shihab. 2016. Defect Prediction: Accomplish-
ments and Future Challenges. In 2016 IEEE 23rd International Conference
on Software Analysis, Evolution, and Reengineering (SANER), Vol. 5. 33–45.
https://doi.org/10.1109/SANER.2016.56

[12] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757–773. https://doi.org/10.1109/TSE.2012.70

[13] Anil Koyuncu, Tegawendé F. Bissyandé, Dongsun Kim, Kui Liu, Jacques
Klein, Martin Monperrus, and Yves Le Traon. 2019. D&C: A Divide-and-
Conquer Approach to IR-based Bug Localization. CoRR abs/1902.02703 (2019).
arXiv:1902.02703 http://arxiv.org/abs/1902.02703

[14] Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim, Martin Monper-
rus, Jacques Klein, and Yves Le Traon. 2019. IFixR: Bug Report Driven Program
Repair. In Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineer-
ing (Tallinn, Estonia) (ESEC/FSE 2019). Association for Computing Machinery,
New York, NY, USA, 314–325. https://doi.org/10.1145/3338906.3338935
[15] Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu
Zhang, and Lingming Zhang. 2021. Boosting Coverage-Based Fault Local-
ization via Graph-Based Representation Learning. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA, 664–676.
https://doi.org/10.1145/3468264.3468580
[16] Shane McIntosh and Yasutaka Kamei. 2018.

Are Fix-Inducing Changes
a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Pre-
diction.
IEEE Transactions on Software Engineering 44, 5 (2018), 412–428.
https://doi.org/10.1109/TSE.2017.2693980

[17] Sonu Mehta, Farima Farmahinifarahani, Ranjita Bhagwan, Suraj Guptha, Sina
Jafari, Rahul Kumar, Vaibhav Saini, and Anirudh Santhiar. 2021. Data-Driven
Test Selection at Scale. Association for Computing Machinery, New York, NY,
USA, 1225–1235. https://doi.org/10.1145/3468264.3473916

[18] Atif Memon, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell,
Taming Google-scale continu-
Rob Siemborski, and John Micco. 2017.
ous testing.
In 2017 IEEE/ACM 39th International Conference on Software
Engineering: Software Engineering in Practice Track (ICSE-SEIP). 233–242.
https://doi.org/10.1109/ICSE-SEIP.2017.16

[19] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and
Mark Harman. 2019. Chapter Six - Mutation Testing Advances: An Anal-
ysis and Survey.
Advances in Computers, Vol. 112. Elsevier, 275–378.
https://doi.org/10.1016/bs.adcom.2018.03.015

[20] Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques
actually helping programmers?. In Proceedings of the 2011 International Sympo-
sium on Software Testing and Analysis (Toronto, Ontario, Canada) (ISSTA 2011).
ACM, New York, NY, USA, 199–209.

[21] Abdallah Qusef, Gabriele Bavota, Rocco Oliveto, Andrea De Lucia, and David
Binkley. 2011. SCOTCH: Test-to-code traceability using slicing and conceptual
coupling. In 2011 27th IEEE International Conference on Software Maintenance
(ICSM). 63–72. https://doi.org/10.1109/ICSM.2011.6080773

[22] Foyzur Rahman and Premkumar Devanbu. 2013. How, and why, process metrics
are better. In 2013 35th International Conference on Software Engineering (ICSE).
432–441. https://doi.org/10.1109/ICSE.2013.6606589
[23] Bart Van Rompaey and Serge Demeyer. 2009.

Establishing Traceabil-
ity Links between Unit Test Cases and Units under Test.
In 2009 13th
European Conference on Software Maintenance and Reengineering. 209–218.
https://doi.org/10.1109/CSMR.2009.39

[24] Ripon K Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E Perry. 2013. Im-
proving bug localization using structured information retrieval. In Automated
Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on.
IEEE, 345–355.

[25] Seemanta Saha, Ripon K. Saha, and Mukul R. Prasad. 2019. Harnessing Evolution
for Multi-hunk Program Repair. In Proceedings of the 41st International Confer-
ence on Software Engineering (Montreal, Quebec, Canada) (ICSE ’19). IEEE Press,
Piscataway, NJ, USA, 13–24.

[26] Jeongju Sohn, Gabin An, Jingun Hong, Dongwon Hwang, and Shin Yoo. 2021.
Assisting Bug Report Assignment Using Automated Fault Localisation: An In-
dustrial Case Study. In Proceedings of the 14th IEEE International Conference on
Software Testing, Veriﬁcation and Validation.

[27] J. Sohn and S. Yoo. 2019. Empirical Evaluation of Fault Localisation Using Code
IEEE Transactions on Software Engineering (2019), 1–1.

and Change Metrics.
https://doi.org/10.1109/TSE.2019.2930977

[28] Gregory Tassey. 2002. The Economic Impacts of Inadequate Infrastructure for Soft-
ware Testing. Technical Report. National Institute of Standards and Technology.
Refactoring-
IEEE Transactions on Software Engineering (2020), 21 pages.

[29] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. 2020.

Miner 2.0.
https://doi.org/10.1109/TSE.2020.3007722

[30] Qianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the use-
fulness of IR-based fault localization techniques. In Proceedings of the 2015 Inter-
national Symposium on Software Testing and Analysis, ISSTA 2015, Baltimore, MD,
USA, July 12-17, 2015. 1–11.

[31] Ming Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: Locating Bugs
from Software Changes. In Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering (Singapore, Singapore) (ASE
2016). Association for Computing Machinery, New York, NY, USA, 262–273.
https://doi.org/10.1145/2970276.2970359

[32] Robert White, Jens Krinke, and Raymond Tan. 2020.

Establishing Multi-
level Test-to-Code Traceability Links. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering (Seoul, South Korea) (ICSE
’20). Association for Computing Machinery, New York, NY, USA, 861–872.
https://doi.org/10.1145/3377811.3380921

[33] W. E. Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A Survey
on Software Fault Localization. IEEE Transactions on Software Engineering 42, 8
(August 2016), 707.

[34] Jianwei Wu and James Clause. 2020. A pattern-based approach to detect and
improve non-descriptive test names. Journal of Systems and Software 168 (2020),
110639. https://doi.org/10.1016/j.jss.2020.110639

[35] Xia Zeng, Dengfeng Li, Wujie Zheng, Fan Xia, Yuetang Deng, Wing Lam, Wei
Yang, and Tao Xie. 2016. Automated Test Input Generation for Android: Are
We Really There yet in an Industrial Case?. In Proceedings of the 2016 24th ACM
SIGSOFT International Symposium on Foundations of Software Engineering (Seat-
tle, WA, USA) (FSE 2016). Association for Computing Machinery, New York, NY,
USA, 987–992. https://doi.org/10.1145/2950290.2983958

[36] Benwen Zhang, Emily Hill, and James Clause. 2015. Automatically Gen-
erating Test Templates from Test Names (N). In 2015 30th IEEE/ACM In-
ternational Conference on Automated Software Engineering (ASE). 506–511.
https://doi.org/10.1109/ASE.2015.68

[37] Lingming Zhang. 2018.

In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). 199–209.
https://doi.org/10.1145/3180155.3180198

Hybrid Regression Test Selection.

[38] Thomas Zimmermann, Peter Weißgerber, Stephan Diehl, and Andreas
Zeller. 2004. Mining Version Histories to Guide Software Changes. In
26th International Conference on Software Engineering (ICSE 2004), 23-28

May 2004, Edinburgh, United Kingdom.
https://doi.org/10.1109/ICSE.2004.1317478

IEEE Computer Society, 563–572.

