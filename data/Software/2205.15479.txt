Learning to Represent Programs with Code Hierarchies

Minh H. Nguyen 1 * Nghi D. Q. Bui 1 2 * Truong Son Hy 3 Long Tran-Thanh 4 Risi Kondor 3
1 FPT Software AI Center
2 School of Computing & Information Systems, Singapore Management University
3 Department of Computer Science, University of Chicago, IL, USA
4 Department of Computer Science, University of Warwick, UK
minhnh46@fsoft.com.vn, dqnbui.2016@smu.edu.sg , hytruongson@uchicago.edu, long.tran-thanh@warwick.ac.uk,
risi@uchicago.edu

2
2
0
2

g
u
A
7
1

]
E
S
.
s
c
[

2
v
9
7
4
5
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

Graph neural networks have been shown to produce im-
pressive results for a wide range of software engineering
tasks. Existing techniques, however, still have two issues: (1)
long-term dependency and (2) different code components are
treated as equals when they should not be. To address these
issues, we propose a method for representing code as a hi-
erarchy (Code Hierarchy), in which different code compo-
nents are represented separately at various levels of granular-
ity. Then, to process each level of representation, we design a
novel network architecture, ECHELON, which combines the
strengths of Heterogeneous Graph Transformer Networks and
Tree-based Convolutional Neural Networks to learn Abstract
Syntax Trees enriched with code dependency information.
We also propose a novel pretraining objective called Miss-
ing Subtree Prediction to complement our Code Hierarchy.
The evaluation results show that our method signiﬁcantly out-
performs other baselines in three tasks: any-code completion,
code classiﬁcation, and code clone detection.

Introduction
Deep Learning (DL) for code has recently received in-
creasing attention in the machine learning research commu-
nity (Mou et al. 2016; Bui, Yu, and Jiang 2021c; Chen, Liu,
and Song 2018; Chakraborty et al. 2020; Allamanis et al.
2018; Hellendoorn et al. 2019; Wang et al. 2020b,a). Vari-
ous DL-based methods are proposed to process source code,
for example, representing the code in a structure, such as
Abstract Syntax Trees (ASTs) or graphs, and corresponding
neural architectures are designed to process them, such as
Tree-based Neural Networks (Mou et al. 2016; Bui, Yu, and
Jiang 2021c), or Graph Neural Networks (Allamanis et al.
2018; Li et al. 2016; Hellendoorn et al. 2019; Fernandes,
Allamanis, and Brockschmidt 2019). Graph-based models
typically outperform tree-based models due to additional in-
ductive biases introduced into the code graph via code de-
pendency analysis (i.e. data-ﬂow, control-ﬂow, etc.) (Alla-
manis et al. 2018; Wang et al. 2020b) on a wide range of
tasks, including bug detection (Allamanis et al. 2018), code
summarization (Hellendoorn et al. 2019), and so on.

Understanding graphs in a multiscale perspective is es-
sential for capturing the large-scale structure of source code.
However, GNNs constructed by the message passing scheme

*These authors contributed equally to this work.

(Scarselli et al. 2009; Duvenaud et al. 2015; Gilmer et al.
2017), in which each node propagates and aggregates vec-
torized information (i.e. messages) to and from the neigh-
boring nodes, are inherently ﬂat and unable to capture mul-
tiscale and hierarchical structures as suggested by Ying et al.
(2018). In the setting of learning representations for source
code, we recognize that GNNs have the following two ma-
jor drawbacks that motivate our novel architecture and pre-
training paradigm designed speciﬁcally to produce hierar-
chical representations. First, there is a long-term dependency
problem or over-squashing (Alon and Yahav 2021) prevent-
ing nodes from effectively transferring messages. Second,
code components are represented and processed equally al-
though they should be treated differently. Indeed, almost all
state-of-the-art methods adopt AST-based representations;
nonetheless, the sizes of ASTs are usually large, which ex-
acerbates the long-term dependency problem. Additionally,
an AST is inherently hierarchical, so assuming all elements
play the same role does not accurately reﬂect the relation-
ship between different levels and within a level. Some pre-
vious works try to solve the problems above. For example,
Bui and Jiang (2018) show that source code can be rep-
resented hierarchically, with ﬁne-grained code components
combined together to represent coarse-grained ones, e.g., to-
kens ⇒ statement, statements ⇒ function, and so on. This
means that the corresponding code component can be pro-
cessed according to its properties at each level of the hier-
archy. However, Bui and Jiang (2018) only consider a sim-
ple hierarchical mechanism by averaging the low-level el-
ements to represent their corresponding higher-level ones,
that leads to the loss of structural information in interlevel
communication. Yang et al. (2016) also mentions a similar
concept of hierarchical structure of document in NLP, e.g.,
words ⇒ sentence and sentences ⇒ document. Inspired by
these observations, we propose a novel framework to repre-
sent the source code as a hierarchy and use a combination of
different neural network architectures to process each com-
ponent in the hierarchy. In short, the key ideas of our ap-
proach are: (1) A novel hierarchical code representation by
breaking the Abstract Syntax Tree into different levels of
representation and enriching them with semantic informa-
tion through edges, called Code Hierarchy. (2) A novel neu-
ral network processing such representations by combining
the Heterogenous Graph Transformer and Tree-based Con-

 
 
 
 
 
 
volutional Neural Network (TBCNN), called ECHELON.

In addition, we also propose a novel pretraining objective
for our network. Recent works in large-scale language mod-
els of code have shown that pretraining the model over some
unsupervised objectives and then ﬁne-tuning it on down-
stream supervised tasks can improve the performance sig-
niﬁcantly (Wang et al. 2021a; Feng et al. 2020; Bui, Yu, and
Jiang 2021b; Phan et al. 2021; Lachaux et al. 2021; Bui, Yu,
and Jiang 2021a). In line with our framework, we observe
that when splitting the subtree out of the AST, we can per-
form an objective that naturally ﬁts with our process, called
Missing Subtree Prediction (MSP) task in which we can
mask out randomly subtrees and force the model to recon-
struct them. Our evaluation results show that this pretraining
objective can remarkably boost our model’s performance.

To summarize, our major contributions are as follows. (1)
We create a novel code representation called Code Hier-
archy by breaking the AST into coarse-grained and ﬁned-
grained components, and adding semantic edges, e.g., con-
trol dependency, data ﬂow, to the coarse-grained compo-
nents. (2) We develop a novel neural network architec-
ture that combines Heterogenous Graph Transformer and
Tree-based CNN to process the Code Hierarchy, called
ECHELON. (3) We propose a novel pretraining objective
for our framework, named Missing Subtree Prediction task,
to predict the missing subtrees. (4) We evaluate or ﬁne-tune
our models on three tasks: any-code completion, code clas-
siﬁcation and clone detection, and conduct extensive evalu-
ations. Experimental results show that our proposed model
can outperform other strong baselines with large margins.
(5) We demonstrate that ECHELON can explain the predic-
tions in a hierarchical fashion to some extent.

Related Work
Structural Learning of Code The naturalness hypothe-
sis (Hindle et al. 2016) demonstrates that program source
code has properties similar to natural language, allowing us
to develop statistical models for reasoning about programs.
However, treating source code as a natural language may
overlook its various unique properties, leading to a large
number of recent works proposing to represent source code
in structural representations, such as Abstract Syntax Tree
(AST) (Mou et al. 2016; Bui, Yu, and Jiang 2021c; Chen,
Liu, and Song 2018; Chakraborty et al. 2020) or Graph (Al-
lamanis et al. 2018; Hellendoorn et al. 2019; Wang et al.
2020b,a). These models have been shown to be effective
on a wide range of software engineering tasks, including
code classiﬁcation and bug prediction (Nix and Zhang 2017;
Dahl et al. 2013; Pascanu et al. 2015; Rastogi, Chen, and
Jiang 2013; Li, Wang, and Nguyen 2022), predicting vulner-
abilities (Yang et al. 2015; Li et al. 2017, 2018; Zhou et al.
2019), translating programs (Chen, Liu, and Song 2018; Gu
et al. 2017; Bui, Jiang, and Yu 2018; Bui, Yu, and Jiang
2019b; Nghi, Yu, and Jiang 2019), etc.

Pretrained Language Models for Code A large body of
recent work employs language models from natural lan-
guage processing for code (Feng et al. 2020; Wang et al.
2021b; Guo et al. 2020; Ahmad et al. 2021; Bui, Yu, and

Jiang 2021b; Elnaggar et al. 2021; Peng et al. 2021; Kanade
et al. 2020). They mostly treat code similar to texts and
adapt the same pretraining strategies as for natural language.
CodeBERT (Feng et al. 2020) adapts a Roberta model (Liu
et al. 2019) to pretrain a model of code on multiple program-
ming languages. CuBERT (Kanade et al. 2020) pretrains a
BERT model for code using a large dataset of curated Python
ﬁles. GraphCodeBERT (Guo et al. 2020) uses a data-ﬂow
pretraining strategy. CodeT5 (Wang et al. 2021b) extracts
unique identiﬁer information from source code to pretrain
the T5 (Raffel et al. 2019) model for code in a multi-modal
style. These models are designed to pretrain on very large-
scale datasets in a self-supervised manner, thus they may
perform better than structural learning techniques for code.
This scaling feature is arguably the main key to make these
large pretrained language models outperform smaller mod-
els, which have been carefully designed to be aware of syn-
tactic and semantic aspects of code.

We hope to combine the best of both worlds through
this framework. Our framework is made up of a new rep-
resentation (Code Hierarchy) and a new neural architecture
(ECHELON) that learns the structural and semantic features
of code effectively; we also leverage the power of pretrain-
ing on large-scale datasets through a novel Missing Subtree
Prediction task.

Technical Details
We give an overview of our approach. There are two stages:
(1) Representing code into a novel representation called
Code Hierarchy; (2) Pretraining the code hierrachy with a
novel architecture, ECHELON on a novel Missing Subtree
Prediction task, and ﬁne-tuning on downstream tasks.

1. The ﬁrst stage involves parsing a program into AST
and extracting a set of subtrees at the statement- and
expression-level. Then we transform the AST with the
set of subtrees to acquire a novel code representation
Code Hierarchy. The Code Hierarchy is divided into
two layers. The ﬁrst layer is called Subtree-level layer
(lower-level layer), which represents statement-level and
expression-level code components; each statement/ex-
pression is represented as a subtree, and each node in
the subtree is the node from the original AST. The sec-
ond layer is called AST-level layer (higher-level layer)
which represents code components from the AST which
are not identiﬁed as subtrees. We enrich the nodes in the
AST-level layer by connecting them with semantic edges,
e.g., control dependency and data ﬂow, to transform the
tree into a graph.

2. In the second stage, we design a novel neural ar-
chitecture ECHELON to process the Code Hierarchy.
ECHELON comprises two main components: The Tree-
based Convolutional Neural Network (TBCNN) to pro-
cess the Subtree-level, and the Heterogeneous Graph
Transformer (HGT) to process the AST-level. We pre-
train the ECHELON ﬁrst by performing a novel Miss-
ing Subtree Prediction (MSP) task in which the state-
ment subtrees in the ﬁrst layer are randomly masked out,
and then we use the surrounding context to predict those

missing subtrees. We then ﬁne-tune the model on various
downstream tasks after pretraining.

Our novel framework has three advantages. First, because
many subtrees have been abstracted into single nodes at the
AST-level, the size of the original AST is signiﬁcantly re-
duced, making messages passing at the graph level more
smoothly. Second, there are many options for processing
code components at each layer. Assume that there are more
advanced methods for processing the tree or the graph in
the future; these can also be integrated into our framework.
Third, the framework provides explainability in a hierarchi-
cal fashion, e.g., we will know which subtrees or which to-
kens in a subtree are important.

Representing Code as Hierarchy

Figure 1: An example of Code Hierarchy. The gray nodes in the
AST-level layer are subtree nodes, while non-subtree nodes remain
white. Each subtree node (gray node) in the AST-level layer has
been abstracted and represented for a subtree in the Subtree-level
layer. The table in the bottom left corner maps a node index to its
corresponding node type.

We aim to identify and extract a set of subtrees S from
an AST T . Then we abstract each subtree by replacing it
with a new node in T at the root location of the subtree,
reducing T into another tree T (cid:48) with a smaller size, where
Size(T (cid:48)) < Size(T ). We call the new nodes added to T as
the subtree nodes to distinguish them from the original AST
nodes. The set of subtrees S is kept separately.

Such subtrees will then be extracted in the ﬁrst layer of the
Code Hierarchy. A subtree will be chosen if its root type is
expression or simple statement; the set of subtree types can
be looked up in our Supplementary Material. Speciﬁcally,
a statement that does not contain other statements is consid-
ered as simple. The reason is that very large statements, such
as for loops, while loops, or if statements, can contain com-
plex code structures. In fact, such large statements may oc-
cupy a large portion of the content in some small programs,
reducing the effectiveness of our MSP task.

These steps can be done by depth-ﬁrst preorder traversal
of the AST. If the type of a node n is in S, we replace the
whole subtree where n is the root, and we do not traverse
further into lower depths. This procedure is executed recur-
sively until all subtrees are obtained. Once done, we get a
new tree T (cid:48) and a set of subtrees S. Note that some nodes

(subtree nodes) in T (cid:48) point to elements in S. This feature is
the key to representing the Hierarchy. Figure 1 depicts such
a Code Hierarchy where node 5 is a new node in the tree T (cid:48),
and it represents a speciﬁc subtree with three nodes b, >, 0.

Enriching ASTs with Semantic Edges
Given the tree T (cid:48) from the previous step, we enrich T (cid:48)
with semantic information to make T (cid:48) become a graph G.
Such information is added to the AST-level by connecting
the nodes with different edge types. Through our process,
there are four edge types, including AST Edge, Control-
Dependence Edge, Data-Flow Edge and Next-Subtree Edge.
Each of them is described further below.
1. AST Edge (AST-E) T (cid:48) consists of a mixture of origi-
nal AST nodes from T and new subtree nodes, the AST
edges serve as the syntactical representation of code. In
Figure 1, AST edges are denoted as green arrows.

2. Control-Dependence Edge (CD-E) Control depen-
dency occurs when a program instruction executes if
evaluating the preceding instruction permits its execu-
tion. In Figure 1, the subtree sum = b is control de-
pendent on the subtree b > 0, so b > 0 connects with
sum = b through a control-dependence edge (red dashed
arrows in Figure 1). Besides, to keep the order of execu-
tion in a function, we connect the root of one statement
to the root of the next statement. As in Figure 1, for
instance, there is a control-dependence edge connecting
node 3 to node 4 where node 3 represents the declaration
statement and node 4 is the root of the if statement.
3. Data-Flow Edge (DF-E) Data ﬂow indicates how the
values of variables change as a program is executed. We
use Deﬁne-Use chain analysis (Weiser 1984), a well-
known data-ﬂow analysis technique to extract such in-
formation. For example, the variable sum is ﬁrst deﬁned
in line 2, then it is used in line 4 and 6, then the sub-
tree sum = 0 connects the two subtrees sum = b and
return sum through data-ﬂow edges (yellow arrows in
Figure 1).

4. Next-Subtree Edge (NS-E) This edge type represents
the textual order of subtrees, not the execution ones. For
example, the subtree sum = b is written right after
b > 0, but sum = b may not be necessarily executed
after b > 0. With that, b > 0 connects with sum = b
through a next-subtree edge (blue arrows in Figure 1).

Neural Network Architecture
In this section, we describe our ECHELON to process the
Code Hierarchy obtained in the steps above. Our architec-
ture consists of a Tree-based Convolutional Neural Net-
work (Mou et al. 2016) (TBCNN) and a Heterogeneous
Graph Transformer (Hu et al. 2020) (HGT). At ﬁrst, each
node in a subtree s ∈ S contains two attributes: token and
type. The the initial representation of a node can be com-
puted by concatenating the embeddings of its token and its
type, where such embeddings can be looked up from two
embedding matrices (token embedding matrix and type em-
bedding matrix) initialized randomly as the learnable param-
eters. A token is encoded as the sum of the embeddings of

its subtokens by a lookup table, while the type embeddings
are retrieved by another lookup table. Then, the TBCNN re-
ceives each vectorized subtree s in the Subtree-level layer
and encodes it into a ﬁxed-size embedding. This embedding
will also be used as the initial representation for the corre-
sponding node in the graph at the AST-level layer. The HGT
is then used to perform message passing on the nodes to ac-
cumulate information.

Tree-based Convolutional Neural Network (TBCNN)
TBCNN (Mou et al. 2016) is designed to process tree-
structure through the tree-based convolution operator. In a
TBCNN, there is at least one tree-based convolutional layer.
Each layer is a feature detector and has a ﬁxed-depth con-
volutional window called the kernel, sliding over the entire
tree to extract features. Formally, this procedure can be sum-
marized as: y = f ((cid:80)n
1 Wconv,i · xi + bconv), where f is an
activation function, Wconv,i are the weight matrices, xi are
the vectors of nodes inside the sliding window, and bconv is
the bias. In summary, at each convolutional step, the feature
of node i is accumulated by its direct children in a sliding
window simultaneously. At the end of this step, the ﬁx-sized
embedding of a subtree is computed by using a max pooling
operator over all of the nodes in such subtree.

Heterogeneous Graph Transformer (HGT) A heteroge-
neous graph is deﬁned as a directed graph G = (V1 ∪
V2, E, A, R) where V1 is the node set where a node repre-
sents a subtree, V2 is a set of non-subtree AST nodes, and
E is the edge set. Each node and edge are associated with
the types τ (n) ∈ A and φ(e) ∈ R, respectively. As previ-
ously mentioned, the embedding of each node v1 ∈ V1 can
be computed from the TBCNN step. For each node v2 ∈ V2,
at ﬁrst, we compute an initial vector by concatenating the
embeddings of its token and its type, which is then fed to a
1-layer nonlinear network, then we annotate this node with
the obtained vector. A HGT layer can be decomposed into
three components: heterogeneous mutual attention, hetero-
geneous message passing and target-speciﬁc aggregation.
The overall process can be written as:
H l[t] ← Aggregate

(Attention(s, e, t) · Message(s, e, t))

∀s∈N (t),∀e∈E(s,t)

where N (t) is the set of source nodes of node t and E(s, t)
denotes all the edges from node s to node t. H l is the output
of the l-th HGT layer, and the next layer receives it as the
input. Given a node t, Attention(·) computes the score for
each source node s ∈ N (t), Message(·) extracts the mes-
sage by using the source node s, Aggregate(·) is a operation
where the node t incorporates the messages of all the source
nodes N (t) (details are in our Supplementary Material).

Pre-training as Missing Subtree Prediction (MSP)
Because training our model from scratch for each task is
costly, we use a pretraining strategy to train a base model
before ﬁne-tuning it for downstream tasks. We propose a
novel pretraining objective called Missing Subtree Predic-
tion (MSP) which predicts a missing subtree as a sequence
of tokens (which represents for that missing subtree) by its
surrounding context. For example, if the masked node in the
Figure 1 is node 6, the model tries to predict the missing sub-
tree as a sequence sum = b. We use the information of the

AST-level layer and the other subtrees in the Subtree-level
layer to predict a target missing subtree. By doing this, the
model learns the relationship between different code com-
ponents, both syntactically and semantically, i.e., how the
other code components are organized structurally to recon-
struct the subtrees. We feed all node embeddings from the
HGT’s output into a vanilla Transformer decoder to predict
the subtree token. Formally, given a set of training samples
D = (cid:8)(cid:10)n(s), y(s)(cid:11)(cid:9)S
s=1 where n(s) is the set of nodes af-
ter randomly masking one of the subtree nodes in the graph
G(s), and y(s) is the token sequence of the masked node with
the length J (s), the pretraining objective is to maximize log-
likelihood of the training data:

max
θ

L (θ) = min

θ

S
(cid:88)

J (s)
(cid:88)

s=1

j=1

(cid:16)

− log P

y(s)
j

|n(s), y(s)

<j; θ

(cid:17)

Applications
In this section, we describe how our model can be ben-
eﬁcial for different software engineering tasks. First, our
pretraining objective naturally ﬁts into a task called any-
code completion (Alon et al. 2020). This task is to pre-
dict missing parts of a code snippet (statements, expressions,
etc.) using existing contexts. Our pretraining goal is to mask
a subtree and predict it by the context, which means that
our pretraining model could be used directly on this task.
Second, we can ﬁne-tune our pretrained model for various
downstream tasks. The downstream tasks in this paper are
code classiﬁcation and code clone detection. As such, these
three tasks represent three ways to formulate different prob-
lems in software engineering: generation-based (any-code
completion), classiﬁcation-based (code-classiﬁcation), and
detection-based (clone detection). Code classiﬁcation (Mou
et al. 2016) is the task of classifying a given code snippet
into a given functionality category, which is helpful to under-
stand and extend existing source code. Code clone detection
is the task of identifying if two code snippets are semanti-
cally equivalent, which is crucial for software maintenance.
Note that ﬁne-tuning for these 2 tasks is straightforward, we
can apply the max pooling operator through all of the nodes
after the HGT step and feed them through a feed-forward
layer for classiﬁcation. 1

Empirical Evaluation

Model Pretraining with MSP
It should be noted that our model is language-agnostic. We
choose Java and C++ as the two programming languages in
our case to pretrain models. These models will then be used
for evaluating or ﬁne-tuning in the following sections.

For Java, we choose the Java-small (Alon et al. 2019)
dataset, and the Java dataset from CodeSearchNet (Husain
et al. 2019) (which is called Java-CSN to distinguish it from
Java-small), which have been used for pretraining in pre-
vious work (Alon et al. 2020; Wang et al. 2021b; Feng

1Details of the loss function of these two tasks can be found in

the Supplementary Material.

et al. 2020). In addition, Java-small is also used for the
any-code completion task (Alon et al. 2020), which is the
aim of our evaluation. The Java-CSN dataset is divided
into 297,260/18,123/8,630 samples for training/testing/val-
idation. The equivalent ratios for the Java-small dataset are
1,210,272/19,165/9,156 samples. We only use the training
parts of these datasets for pretraining.

For C++, we choose the C++1000 dataset from Project
CodeNet (Puri et al. 2021). It includes a large number of
C++ programs in 1000 classes for the code classiﬁcation.
This dataset is split into 316,799/98,516/78,702 samples for
training/testing/validation. As we use it for pretraining, we
do not consider the class information. This dataset is cho-
sen for the same reason as the Java-small dataset: it contains
small programs, and it is relatively clean and large enough.
From the above three datasets, we pretrain models for our
MSP task, yielding three foundation models: ECHELON-
Java-small, ECHELON-Java-CSN and ECHELON-C++ .

Any-code completion
Datasets. We choose the test set of Java-small for evaluation
including 19165 samples. For each sample, we randomly
select one subtree for masking, resulting in 19165 test in-
stances.

Methods

Acc@1 BLEU

SLM
Transformer
BiLSTM → LSTM

5.31
7.78
6.37

ECHELON-Java-small

10.02

23.96
28.11
26.77

31.51

Table 1: Results on any-code completion

Baselines. We choose SLM (Alon et al. 2020) as the base-
line, which is the state-of-the-art technique for any-code
completion task. We follow the steps described in the ofﬁ-
cial artifact 2 to create and process our test instances that ﬁt
into the format of SLM for a fair comparison. Then we use
the test API provided by SLM to evaluate our test instances.
We also choose sequence-to-sequence models, including a
vanilla Transformer (Vaswani et al. 2017) and a BiLSTM as
other baselines. Given a code snippet, we replace the target
with a special token <mask>, and then train the network to
predict the target as a sequence of subtokens.
Metrics. We use the top-1 exact match accuracy (Acc@1)
and BLEU (Papineni et al. 2002) as the metrics. The Acc@1
is deﬁned as the generated prediction which is identical to
the target sentence (ignoring cases and whitespaces).
Results. Table 1 shows that ECHELON-Java-small achieves
the best results among the baselines in terms of Acc@1 and
BLEU. Note that we do not perform any training here, but
we use the foundation model ECHELON-Java-small to eval-
uate on the test instances of Java-Small. This demonstrates
the effectiveness of using the Code Hierarchy as the context
to predict missing subtrees.

2https://github.com/tech-srl/slm-code-generation

Code Classiﬁcation

Datasets. We choose the POJ-104 (Mou et al. 2016) since
it is one of the most well-known benchmarks for code clas-
siﬁcation. However, POJ-104 is said to be easy to achieve
good results, and it is small-scale (52,000 samples for 104
classes). As we aim to perform a large-scale evaluation, we
need to have a larger scale dataset. Therefore, we choose
the C++1400 and C++1000 datasets from Project CodeNet
(Puri et al. 2021), which are on a much larger scale for
this task. The C++1400 is made up of many C++ programs
that are organized into 1,400 classes. It consists of 267,413
/83,562/66,868 samples for training/testing/validation.
Baselines. We follow Puri et al. (2021) to choose the base-
lines: MLP, CNN, C-BERT (Buratti et al. 2020), GCN
(Kipf and Welling 2017) and GIN (Xu et al. 2019) for the
C++1000 and C++1400 datasets. For the POJ-104, we re-
fer to (Zhang et al. 2019) to choose ASTNN (Zhang et al.
2019), TBCNN (Mou et al. 2016), and PDG+GGNN (Alla-
manis et al. 2018) as the baselines.
Results. Table 3 shows the results of ECHELON. There
are two settings, one trained from scratch - ECHELON w/o
MSP, and the other
is trained from ﬁne-tuning from
ECHELON-C++ - ECHELON with MSP. It is shown that
we can achieve signiﬁcant improvement with ﬁne-tuning (≈
2%) compared to training from scratch for all of the datasets.
The ECHELON with MSP is also the best among the base-
lines for the three datasets.

Code clone detection

Datasets. We follow Zhang et al. (2019) to create an
OJ-clone dataset based on POJ-104 by sampling a sub-
set from all pairs of clones and non-clones. There are
29,989/9,996/9,998 samples for training/validation/testing
with 1,957/673/656 positive samples, respectively.

Methods

Precision Recall

F1

CDLH
PDG+GGNN
ASTNN

ECHELON-C++

47
77.3
98.5

97.3

73
43.6
88.3

96.3

57
55.8
93.1

97.1

Table 2: Results on code clone detection

Baselines. We refer to the baselines used in ASTNN (Zhang
et al. 2019) to evaluate the OJ-clone dataset. The ﬁrst base-
line is the ASTNN (Zhang et al. 2019), which is the SOTA
method for clone detection on the OJ-clone. The others are
CDLH (Wei and Li 2017) and GGNN on the Program De-
pendence Graph (PDG + GGNN).
Metrics. We use Precision, Recall and F1 as the metrics.
Results. The results for code clone detection are shown
in Table 2. Overall, ECHELON-C++ has the best perfor-
mance in terms of F1. It outperforms the second-best base-
line ASTNN by a large margin (≈ 4%). Although ASTNN
has higher precision, our approach is better with regard to
recall and F1.

Model Analysis

Impact of Different Edge Types
In this section, we conduct an ablation study to analyze the
impact of each edge type on the overall performance of our
model. We keep the same neural architecture and change the
code representation by removing edge types. Then we train
different variants of ECHELON on the same datasets on the
any-code completion and code classiﬁcation. In Table 4, we
can see that any of the dependency edges can improve the
performance of any-code completion. This implies that the
dependency information is useful for our Code Hierarchy.
Among the three edge types, DF-E contributes the least to
the performance in the any-code completion task while CD-
E has the greatest impact. For code classiﬁcation, NS-E has
the strongest impact while DF-E still performs the worst.
This implies two points: (1) The impact of each edge type
varies depending on the downstream tasks; and (2) Data-
ﬂow information does not always perform well, and its im-
pact should be investigated further. This result is consistent
with the work of Zhou et al. (2019), as the data-ﬂow edges
perform poorly in their graph representation.

Compare The Embedding’s Quality with Other
Pretrained Models
Despite the fact that different pre-trained models use differ-
ent pretraining objectives, they all aim for the same result:
after pretraining, the model should be able to produce simi-
lar vector representations for semantic-equivalent programs,
and they should be close in the vector space. We compare
the quality of vectors produced by our model with others
to see how well ECHELON can perform in general. We
choose CodeBERT (Feng et al. 2020) and CodeT5 (Wang
et al. 2021b) as the competitors since they are two of the
most well-known and SOTA methods. These 2 models are
pretrained on CodeSearchNet (Husain et al. 2019) with
all of the datasets in 6 programming languages (Python,
Javascript, Ruby, Go, Java, and PHP). For ECHELON , we
choose ECHELON-Java-CSN, which is pretrained only on
the Java dataset from CodeSearchNet.

For evaluaton, we choose the Java250 from Project
Codenet (Puri et al. 2021) that is collected for code clas-
siﬁcation task. The programs in a class can be seen as the

Methods

MLP
CNN
C-BERT
GCN
GIN
ASTNN
TBCNN
PDG+GGNN

ECHELON (w/o MSP)
ECHELON (with MSP)

C++1000 C++1400

POJ-104

68.47
94.14
93.80
95.88
96.49
-
-
-

96.09
98.45

64.63
93.89
91.89
95.39
96.08
-
-
-

94.73
98.05

-
-
-
-
-
98.0
94.0
79.6

97.59
98.04

Table 3: Results on code classiﬁcation in accuracy. We use
ECHELON-C++ for this task.

Any-code Completion Code C

Acc@1

BLEU

AST-E
AST-E + CD-E
AST-E + DF-E
AST-E + NS-E

9.07
9.71
9.17
9.54

ECHELON

10.02

29.29
30.53
29.40
30.23

31.51

Acc

94.14
94.30
93.42
94.71

94.73

Table 4: Summary of
the ablation studies. We use
ECHELON-Java-small for any-code completion, and for
code classiﬁcation (Code C), we use ECHELON-C++ .

semantically-equivalent ones. We feed the training partition
of
Java250 through our pretrained ECHELON-Java-
CSN model, and we then use a pooling over all the node
embeddings of each graph to obtain a representative vector
for this graph. As CodeBERT is an encoder-only model, the
extracted vectors represent the [CLS] tokens’ ﬁnal hidden
states. CodeT5 is an encoder-decoder model, so the features
used are the hidden states of the [EOS] tokens after the last
layer in the decoder. Subsequently, we cluster the obtained
embedding vectors using K-means with k = 250 which is
the number of the classes in Java250. We use Homogeneity
Score (HS), Completeness Score (CS), V-measure Score
(VMS) (Rosenberg and Hirschberg 2007) and Adjusted Rand
Index (ARI) (Yeung and Ruzzo 2001) as the metrics to eval-
uate the performance of this clustering task. The detailed
results in Table 5 demonstrate that ECHELON outperforms
the others in terms of the four metrics. This implies that our
pretraining goal is superior to the others in which it produces
embeddings of semantic-equivalent programs that are closer
in the vector space, which results in better performance in
term of clustering-based metrics. In addition, we also vi-
sualize of the vectors produced by the 3 pretrained models. 3

Methods

HS

CS

VMS

ARI

CodeT5
CodeBERT

0.2066
0.2246

0.2146
0.2295

0.2105
0.2270

0.0112
0.0098

ECHELON 0.3300

0.3382

0.3340

0.0418

Table 5: Results of the code clustering task

Model Explainability
An interesting aspect of our approach that we want to show
is the capability of explaining the prediction. To do this,
we use the Contrastive gradient-based saliency maps (Si-
monyan, Vedaldi, and Zisserman 2014). This method is to
differentiate the model output with respect to the input,
which can be obtained by back-propagation. The inputs are
the nodes in the graph G in our case. This method assumes

3We refer the readers to Figure 1, Figure 2 and Figure 3 in the
Supplementary Material. The visualization shows that the clusters
produced by ECHELON have clearer boundaries than the clusters
produced by CodeT5 and CodeBERT.

that the norm of a node’s gradient indicates its importance.
However, because negative gradients are difﬁcult to explain,
the negative values are truncated to zeros to retain only posi-
tive values. After computing the scores for all nodes, we use
min-max normalization to adjust the scores between 0 and 1.
Note that this can be done in both the Subtree-level and the
AST-level. We choose the code classiﬁcation for this analy-
sis. We randomly select a few examples in our C++1400 test
set, and here we show one representative sample.

Figure 2: Visualization on how scores are assigned to nodes

Node ID Token

1
15
19
20
5
21
13
22
16
7

using namespace std
n == 0
while
n > 0
int main()
zero = zero + n / 5
cin (cid:29) n
n = n / 5
break
main

Table 6: Top-10 nodes with corresponding tokens

Figure 2 shows a visualization of the nodes in the AST-
level. Each node is associated with a score, which represents
the node’s importance given a prediction. Table 6 shows top-
10 most important nodes, which are aligned with speciﬁc
statements in the original source code. We can also visualize
which tokens inside a statement are important4.

The results in Figure 2 and Table 6 indicate our model
captures the nodes having direct effects on the output;
speciﬁcally, nodes 21, 13 and 22 are related to the data ﬂow
while nodes 15, 20 and 16 are about the control ﬂow. The ap-
pearance of node 1 may be due to its token being duplicated
in many ﬁles, which probably confuses ECHELON. Nodes
19 and 5 are root nodes of the subtrees representing the while
loop and the entire program respectively, thus they contain
the information of their children apart from their own.

4Due to the page constraint, we only show the visualization at
the AST-level, readers are referred to the Supplementary Material
for the visualization of the Subtree-level

Discussion & Future Work

Here, we will cover other parts of our framework. By us-
ing Code Hierarchy, we modify how code is often repre-
sented (as a graph or tree), which somewhat solves the over-
squashing issue (Alon and Yahav 2021) in ”GNNs for code”.
The Code Hierarchy reduces the size of a AST by moving a
large number of nodes from the AST-level to the Subtree-
level rather than keeping the code as a graph as in ear-
lier work (Allamanis et al. 2018; Wang et al. 2020b). This
will make the HGT’s message-passing steps on the AST-
level easier. Through our evaluations, we have shown that
ECHELON outperforms a few GNNs methods in different
tasks. It shows that our model has the potential to address
the over-squashing issue of the GNNs model of code. In fact,
while in-depth analysis of this problem is not the primary fo-
cus of our paper, it is an interesting area for future research.
Similar to Alon and Yahav (2021), we plan to perform ad-
ditional analysis to fully assess our framework, with a focus
on the over-squashing issue of GNN models of code on the
Code Hierarchy. Moreover, our framework is adaptable, al-
lowing us to pretrain the model using a variety of strategies,
such as predicting missing edges (Guo et al. 2020), predict-
ing missing variables (Wang et al. 2021a), predicting nat-
ural language descriptions (Wang et al. 2021a), etc. Such
pretraining strategies can also be applied to our framework
easily. In the future, we hope to create a uniﬁed pretraining
framework that includes various types of pretraining tasks
on top of Code Hierarchy.

For model explainability, in line with our work, there
are many previous works that attempt to explain the output
of source code models, including AutoFocus (Bui, Yu, and
Jiang 2019a) and SIVAND (Rabin et al. 2021). They either
use attention scores or simplify programs to remove redun-
dant parts of source code for explainability. However, it is
unknown whether such explanations are aligned with how
developers understand code, which we believe is critical for
the explainability on source code models. Extensive research
on human evaluation is required to achieve this, but it is not
the primary focus of our work.

Conclusion

In this paper, we present a novel framework for source code
modeling. Our framework is comprised of three major com-
ponents: (1) a novel code representation Code Hierarchy
representing different code components at different layers;
(2) a novel neural architecture ECHELON to process the
Code Hierarchy, where each layer is processed differently
with a suitable neural network and the output of the lower-
level layer is used as the input for the higher-level layer;
and (3) a novel Missing Subtree Prediction pretraining ob-
jective to pretrain our model. Our evaluation shows that our
framework outperforms the other baselines signiﬁcantly in
three tasks: any-code completion, code classiﬁcation, and
code clone detection. We will target for more downstream
tasks in the future.

References
Ahmad, W. U.; Chakraborty, S.; Ray, B.; and Chang, K.
2021. Uniﬁed Pre-training for Program Understanding and
Generation. In Toutanova, K.; Rumshisky, A.; Zettlemoyer,
L.; Hakkani-T¨ur, D.; Beltagy, I.; Bethard, S.; Cotterell, R.;
Chakraborty, T.; and Zhou, Y., eds., Proceedings of the 2021
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
2655–2668. Association for Computational Linguistics.
Allamanis, M.; et al. 2018. Learning to Represent Programs
with Graphs. In International Conference on Learning Rep-
resentations.
Alon, U.; Brody, S.; Levy, O.; and Yahav, E. 2019. code2seq:
Generating Sequences from Structured Representations of
Code. In International Conference on Learning Representa-
tions.
Alon, U.; Sadaka, R.; Levy, O.; and Yahav, E. 2020. Struc-
In International Confer-
tural Language Models of Code.
ence on Machine Learning, 245–256. PMLR.
Alon, U.; and Yahav, E. 2021. On the Bottleneck of Graph
Neural Networks and its Practical Implications. In Interna-
tional Conference on Learning Representations.
Bui, N. D.; and Jiang, L. 2018. Hierarchical learning of
cross-language mappings through distributed vector repre-
In Proceedings of the 40th Interna-
sentations for code.
tional Conference on Software Engineering: New Ideas and
Emerging Results, 33–36.
Bui, N. D.; Yu, Y.; and Jiang, L. 2019a. Autofocus: inter-
preting attention-based neural networks by code perturba-
tion. In 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE), 38–41. IEEE.
Infercode: Self-
Bui, N. D.; Yu, Y.; and Jiang, L. 2021a.
supervised learning of code representations by predicting
subtrees. In 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE), 1186–1197. IEEE.
Bui, N. D.; Yu, Y.; and Jiang, L. 2021b. Self-supervised
contrastive learning for code retrieval and summarization via
semantic-preserving transformations. In Proceedings of the
44th International ACM SIGIR Conference on Research and
Development in Information Retrieval, 511–521.
Bui, N. D.; Yu, Y.; and Jiang, L. 2021c. TreeCaps: Tree-
based capsule networks for source code processing. In Pro-
ceedings of the 35th AAAI Conference on Artiﬁcial Intelli-
gence.
Bui, N. D. Q.; Jiang, L.; and Yu, Y. 2018. Cross-Language
Learning for Program Classiﬁcation Using Bilateral Tree-
Based Convolutional Neural Networks. In The Workshops of
the The Thirty-Second AAAI Conference on Artiﬁcial Intel-
ligence, New Orleans, Louisiana, USA, February 2-7, 2018,
volume WS-18 of AAAI Workshops, 758–761. AAAI Press.
Bui, N. D. Q.; Yu, Y.; and Jiang, L. 2019b. SAR: learning
cross-language API mappings with little knowledge. In Du-
mas, M.; Pfahl, D.; Apel, S.; and Russo, A., eds., Proceed-
ings of the ACM Joint Meeting on European Software Engi-
neering Conference and Symposium on the Foundations of

Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn,
Estonia, August 26-30, 2019, 796–806. ACM.
Buratti, L.; Pujar, S.; Bornea, M. A.; McCarley, J. S.; Zheng,
Y.; Rossiello, G.; Morari, A.; Laredo, J.; Thost, V.; Zhuang,
Y.; and Domeniconi, G. 2020. Exploring Software Natural-
ness through Neural Language Models. abs/2006.12641.
Chakraborty, S.; Ding, Y.; Allamanis, M.; and Ray, B. 2020.
IEEE
Codit: Code editing with tree-based neural models.
Transactions on Software Engineering.
Chen, X.; Liu, C.; and Song, D. 2018. Tree-to-tree Neu-
ral Networks for Program Translation. In Bengio, S.; Wal-
lach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.;
and Garnett, R., eds., Advances in Neural Information Pro-
cessing Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-
8, 2018, Montr´eal, Canada, 2552–2562.
Dahl, G. E.; Stokes, J. W.; Deng, L.; and Yu, D. 2013.
Large-scale malware classiﬁcation using random projections
and neural networks. In IEEE International Conference on
Acoustics, Speech and Signal Processing, 3422–3426. IEEE.
Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell,
R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015.
Convolutional Networks on Graphs for Learning Molecu-
lar Fingerprints.
In Cortes, C.; Lawrence, N.; Lee, D.;
Sugiyama, M.; and Garnett, R., eds., Advances in Neural
Information Processing Systems, volume 28. Curran Asso-
ciates, Inc.
Elnaggar, A.; Ding, W.; Jones, L.; Gibbs, T.; Feher, T.; An-
gerer, C.; Severini, S.; Matthes, F.; and Rost, B. 2021. Code-
Trans: Towards Cracking the Language of Silicon’s Code
Through Self-Supervised Deep Learning and High Perfor-
mance Computing. arXiv preprint arXiv:2104.02443.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong,
M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; and Zhou, M.
2020. CodeBERT: A Pre-Trained Model for Programming
and Natural Languages.
In Cohn, T.; He, Y.; and Liu, Y.,
eds., Findings of the Association for Computational Linguis-
tics: EMNLP 2020, Online Event, 16-20 November 2020,
volume EMNLP 2020 of Findings of ACL, 1536–1547. As-
sociation for Computational Linguistics.
Fernandes, P.; Allamanis, M.; and Brockschmidt, M. 2019.
Structured Neural Summarization. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019. OpenReview.net.
Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and
Dahl, G. E. 2017. Neural Message Passing for Quan-
In Proceedings of the 34th International
tum Chemistry.
Conference on Machine Learning - Volume 70, ICML’17,
1263–1272. JMLR.org.
Gu, X.; Zhang, H.; Zhang, D.; and Kim, S. 2017. DeepAM:
Migrate APIs with Multi-modal Sequence to Sequence
Learning. In International Joint Conference on Artiﬁcial In-
telligence, 3675–3681.
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou,
L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; et al. 2020. Graph-
codebert: Pre-training code representations with data ﬂow.
arXiv preprint arXiv:2009.08366.

Hellendoorn, V. J.; Sutton, C.; Singh, R.; Maniatis, P.; and
Bieber, D. 2019. Global relational models of source code.
In International Conference on Learning Representations.
Hindle, A.; Barr, E. T.; Gabel, M.; Su, Z.; and Devanbu, P.
2016. On the naturalness of software. Communications of
the ACM, 59(5): 122–131.
Hu, Z.; Dong, Y.; Wang, K.; and Sun, Y. 2020. Heteroge-
neous Graph Transformer. In WWW ’20: The Web Confer-
ence 2020, Taipei, Taiwan, April 20-24, 2020, 2704–2710.
ACM / IW3C2.
Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and
Brockschmidt, M. 2019. CodeSearchNet challenge: Eval-
uating the state of semantic code search. arXiv preprint
arXiv:1909.09436.
Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K.
2020. Learning and evaluating contextual embedding of
In International Conference on Machine
source code.
Learning, 5110–5121. PMLR.
Kipf, T. N.; and Welling, M. 2017. Semi-Supervised Clas-
siﬁcation with Graph Convolutional Networks. In Interna-
tional Conference on Learning Representations (ICLR).
Lachaux, M.; Rozi`ere, B.; Szafraniec, M.; and Lample, G.
2021. DOBF: A Deobfuscation Pre-Training Objective for
Programming Languages. In Ranzato, M.; Beygelzimer, A.;
Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Ad-
vances in Neural Information Processing Systems 34: An-
nual Conference on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, virtual, 14967–
14979.
Li, J.; He, P.; Zhu, J.; and Lyu, M. R. 2017. Software defect
In IEEE In-
prediction via convolutional neural network.
ternational Conference on Software Quality, Reliability and
Security, 318–328. IEEE.
Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2016.
Gated Graph Sequence Neural Networks. In International
Conference on Learning Representations.
Li, Y.; Wang, S.; and Nguyen, T. N. 2022. DEAR: A Novel
Deep Learning-based Approach for Automated Program Re-
pair. arXiv preprint arXiv:2205.01859.
Li, Z.; Zou, D.; Xu, S.; Ou, X.; Jin, H.; Wang, S.; Deng,
Z.; and Zhong, Y. 2018. VulDeePecker: A deep learning-
based system for vulnerability detection. arXiv preprint
arXiv:1801.01681.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
2019. Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Mou, L.; Li, G.; Zhang, L.; Wang, T.; and Jin, Z. 2016. Con-
volutional neural networks over tree structures for program-
ming language processing. In AAAI Conference on Artiﬁcial
Intelligence.
Nghi, B. D. Q.; Yu, Y.; and Jiang, L. 2019. Bilateral De-
pendency Neural Networks for Cross-Language Algorithm
Classiﬁcation.
In Wang, X.; Lo, D.; and Shihab, E., eds.,
26th IEEE International Conference on Software Analy-
sis, Evolution and Reengineering, SANER 2019, Hangzhou,
China, February 24-27, 2019, 422–433. IEEE.

Nix, R.; and Zhang, J. 2017. Classiﬁcation of Android apps
and malware using deep neural networks. In International
Joint Conference on Neural Networks, 1871–1878.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a Method for Automatic Evaluation of Machine Trans-
lation. In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, 311–318. Philadel-
phia, Pennsylvania, USA: Association for Computational
Linguistics.
Pascanu, R.; Stokes, J. W.; Sanossian, H.; Marinescu, M.;
and Thomas, A. 2015. Malware classiﬁcation with recurrent
networks. In IEEE International Conference on Acoustics,
Speech and Signal Processing, 1916–1920. IEEE.
Peng, D.; Zheng, S.; Li, Y.; Ke, G.; He, D.; and Liu, T.-Y.
2021. How could Neural Networks understand Programs?
In International Conference on Machine Learning, 8476–
8486. PMLR.
Phan, L.; Tran, H.; Le, D.; Nguyen, H.; Anibal, J.; Peltekian,
A.; and Ye, Y. 2021. Cotext: Multi-task learning with code-
text transformer. arXiv preprint arXiv:2105.08645.
Puri, R.; Kung, D. S.; Janssen, G.; Zhang, W.; Domeniconi,
G.; Zolotov, V.; Dolby, J.; Chen, J.; Choudhury, M.; Decker,
L.; et al. 2021. CodeNet: A Large-Scale AI for Code Dataset
for Learning a Diversity of Coding Tasks. arXiv preprint
arXiv:2105.12655.
Rabin, M. R. I.; et al. 2021. Understanding neural code in-
telligence through program simpliﬁcation. In Proceedings
of the 29th ACM Joint Meeting on European Software Engi-
neering Conference and Symposium on the Foundations of
Software Engineering, 441–452.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2019. Explor-
ing the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683.
Rastogi, V.; Chen, Y.; and Jiang, X. 2013. Catch me if you
can: Evaluating android anti-malware against transforma-
IEEE Transactions on Information Forensics
tion attacks.
and Security, 9(1): 99–108.
Rosenberg, A.; and Hirschberg, J. 2007. V-measure: A con-
ditional entropy-based external cluster evaluation measure.
In Proceedings of the 2007 joint conference on empirical
methods in natural language processing and computational
natural language learning (EMNLP-CoNLL), 410–420.
Scarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and
Monfardini, G. 2009. The Graph Neural Network Model.
IEEE Transactions on Neural Networks, 20(1): 61–80.
Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2014. Deep
Inside Convolutional Networks: Visualising Image Classi-
ﬁcation Models and Saliency Maps.
In Bengio, Y.; and
LeCun, Y., eds., 2nd International Conference on Learning
Representations, ICLR 2014, Banff, AB, Canada, April 14-
16, 2014, Workshop Track Proceedings.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.
In Guyon, I.; Luxburg, U. V.;
Attention is All you Need.
Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and

Garnett, R., eds., Advances in Neural Information Process-
ing Systems, volume 30. Curran Associates, Inc.
Wang, W.; Li, G.; Ma, B.; Xia, X.; and Jin, Z. 2020a. De-
tecting code clones with graph neural network and ﬂow-
augmented abstract syntax tree. In 2020 IEEE 27th Inter-
national Conference on Software Analysis, Evolution and
Reengineering (SANER), 261–271. IEEE.
Wang, W.; Zhang, K.; Li, G.; and Jin, Z. 2020b. Learn-
ing to represent programs with heterogeneous graphs. arXiv
preprint arXiv:2012.04188.
Wang, Y.; Wang, W.; Joty, S.; and Hoi, S. C. 2021a. CodeT5:
Identiﬁer-aware Uniﬁed Pre-trained Encoder-Decoder Mod-
els for Code Understanding and Generation. In Proceedings
of the 2021 Conference on Empirical Methods in Natural
Language Processing, 8696–8708. Online and Punta Cana,
Dominican Republic: Association for Computational Lin-
guistics.
Wang, Y.; Wang, W.; Joty, S. R.; and Hoi, S. C. H.
2021b.
Identiﬁer-aware Uniﬁed Pre-trained
Encoder-Decoder Models for Code Understanding and Gen-
In Moens, M.; Huang, X.; Specia, L.; and Yih,
eration.
S. W., eds., Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing, EMNLP
2021, Virtual Event / Punta Cana, Dominican Republic, 7-
11 November, 2021, 8696–8708. Association for Computa-
tional Linguistics.
Wei, H.-H.; and Li, M. 2017. Supervised Deep Features
for Software Functional Clone Detection by Exploiting Lex-
ical and Syntactical Information in Source Code. IJCAI’17,
3034–3040. AAAI Press. ISBN 9780999241103.
Weiser, M. 1984. Program slicing. IEEE Transactions on
software engineering, (4): 352–357.
Xu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How
Powerful are Graph Neural Networks? In International Con-
ference on Learning Representations.

CodeT5:

Yang, X.; Lo, D.; Xia, X.; Zhang, Y.; and Sun, J. 2015. Deep
In IEEE In-
Learning for Just-in-Time Defect Prediction.
ternational Conference on Software Quality, Reliability and
Security, 17–26.
Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy,
E. 2016. Hierarchical attention networks for document
In Proceedings of the 2016 conference of
classiﬁcation.
the North American chapter of the association for compu-
tational linguistics: human language technologies, 1480–
1489.
Yeung, K. Y.; and Ruzzo, W. L. 2001. Details of the adjusted
rand index and clustering algorithms, supplement to the pa-
per an empirical study on principal component analysis for
clustering gene expression data. Bioinformatics, 17(9): 763–
774.
Ying, R.; You, J.; Morris, C.; Ren, X.; Hamilton, W. L.;
and Leskovec, J. 2018. Hierarchical Graph Representation
In Proceedings of
Learning with Differentiable Pooling.
the 32nd International Conference on Neural Information
Processing Systems, NIPS’18, 4805–4815. Red Hook, NY,
USA: Curran Associates Inc.
Zhang, J.; Wang, X.; Zhang, H.; Sun, H.; Wang, K.; and Liu,
X. 2019. A novel neural source code representation based on
abstract syntax tree. In Proceedings of the 41st International
Conference on Software Engineering, 783–794. IEEE Press.
Zhou, Y.; Liu, S.; Siow, J. K.; Du, X.; and Liu, Y. 2019.
Devign: Effective Vulnerability Identiﬁcation by Learning
Comprehensive Program Semantics via Graph Neural Net-
In Wallach, H. M.; Larochelle, H.; Beygelzimer,
works.
A.; d’Alch´e-Buc, F.; Fox, E. B.; and Garnett, R., eds., Ad-
vances in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing Systems
2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC,
Canada, 10197–10207.

