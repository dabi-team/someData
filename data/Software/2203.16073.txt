2
2
0
2

g
u
A
3
1

]

G
L
.
s
c
[

3
v
3
7
0
6
1
.
3
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SERVICES COMPUTING

1

Explainable Predictive Process Monitoring:
Evaluation Metrics and Guidelines for Process
Outcome Prediction

Alexander Stevens , Johannes De Smedt

Abstract—Although a recent shift has been made in the ﬁeld of predictive process monitoring to use models from the explainable
artiﬁcial intelligence ﬁeld, the evaluation still occurs mainly through performance-based metrics, thus not accounting for the actionability
and implications of the explanations. In this paper, we deﬁne explainability through the interpretability of the explanations and the
faithfulness of the explainability model in the ﬁeld of process outcome prediction. The introduced properties are analysed along the
event, case, and control ﬂow perspective which are typical for a process-based analysis. This allows comparing inherently created
explanations with post-hoc explanations. We benchmark seven classiﬁers on thirteen real-life events logs, and these cover a range of
transparent and non-transparent machine learning and deep learning models, further complemented with explainability techniques.
Next, this paper contributes a set of guidelines named X-MOP which allows selecting the appropriate model based on the event log
speciﬁcations, by providing insight into how the varying preprocessing, model complexity and explainability techniques typical in
process outcome prediction inﬂuence the explainability of the model.

Index Terms—Explainable Artiﬁcial Intelligence, Predictive Process Monitoring, Interpretability, Faithfulness, Deep Learning, Machine
Learning

(cid:70)

1 INTRODUCTION

The analysis of processes through data-driven ap-
proaches is labelled as process mining [1] and has seen
a strong uptake over the past two decades. Among the
wide range of its techniques, the focus of this study lies
on predictive process monitoring [2], the umbrella term
geared towards predictive activities. It allows identifying
process-related trends regarding particular outcomes (e.g.,
will customers be awarded credit?), impeding bottlenecks,
and whether particular activities will occur in the future.
In the case of Outcome-Oriented Predictive Process Moni-
toring (OOPPM), the concrete objective is to predict the out-
come of an incoming, incomplete process case, for which the
primordial goal is to do so as accurately as possible. Here,
an often anticipated trend is to increase the predictive per-
formance by focusing on representation learning [3], [4], [5],
or on the introduction of computationally complex models
such as ensemble models [2], [3], deep neural networks [6],
[7], [8], and many more algorithms [9]. Nonetheless, the
inputs
inability to link the output back to the original
consequently inhibits to understand the model’s behaviour.
In response to this, the ﬁeld of eXplainable Artiﬁcial In-
telligence (XAI) focuses on gaining insights into how and
why certain predictions in complex structures were made,
while trying to maintain the predictive performance of these
highly performant models [10]. Many XAI proponents state
that trying to explain these complex models come with a loss

• A. Stevens is with the Research Centre for Information Systems Engineer-

•

ing, KU Leuven, Belgium.
E-mail: alexander.stevens@kuleuven.be
J. De Smedt is with the Research Centre for Information Systems Engi-
neering, KU Leuven, Belgium.
E-mail: johannes.desmedt@kuleuven.be

Manuscript received August 5, 2022

of faithfulness [11], considering the fact that the relationship
between input and output can only be approximated [11],
[12], [13]. Nonetheless, the use of increasingly complex
models has been widely adopted in high stake decision-
making processes throughout society [14].

This paper identiﬁes three main issues that arise at the
intersection of OOPPM and XAI. First, the existing OOPPM
research is constrained to the computational aspects while
neglecting the interpretation, actionability, and implications
of the results. In addition, the existing evaluation metrics
are not adapted to a process-based analysis (i.e., do not take
into account the different perspectives of a process-based
analysis) or are not model agnostic (i.e., metrics are depen-
dent on model parameters), making them not suitable for
evaluation purposes. Second, recent literature pointed out
that the post-hoc explainability techniques should uncover,
apart from an interpretable explanation, the true reasons for
model predictions [12], [13], [15]. Hence, a more compre-
hensive study about the faithfulness and interpretability of
XAI techniques in OOPPM is missing, which impedes the
selection of the most appropriate setting of preprocessing
techniques, predictive algorithms and explainability models
to achieve an interpretable outcome. Third, the main focus
of this ﬁeld is to verify its conformance with respect to the
business requirements and goals [1], [16], [17]. Moreover,
there is need for a set of guidelines towards obtaining
faithful and interpretable explanations in the context of
business process monitoring.

This paper tackles these issues through three contribu-
tions. First, we establish a comprehensive basis for XAI in
OOPPM by introducing model-agnostic evaluation metrics
based on the properties interpretability and faithfulness
and adapt these along the event, case, and control ﬂow

 
 
 
 
 
 
IEEE TRANSACTIONS ON SERVICES COMPUTING

2

perspective, which is typical for a process-based analy-
sis. Second, we perform a benchmarking study with ﬁve
machine learning and two deep learning algorithms, with
the former models preprocessed in two different ways. In
this sense, this paper is complementary with the study of
Teinemaa [2] and Kratsch [6], as it compares the experimen-
tal set-up with both deep learning and newly-introduced
interpretable models. This study adds the XAI aspect and
reconsiders what models excel in terms of predictive accu-
racy, interpretability, and faithfulness. Third, we provide a
framework to obtain accurate and eXplainable Models for
Outcome Prediction (X-MOP). This framework of guidelines
is based on an extensive set of research questions and wide
benchmark of preprocessing, traditional machine learning,
deep learning and explainability approaches.

This work extends the initial work of [18], focused on
comparing inherently created explanations with post-hoc
explanations in the context of OOPPM. The paper is orga-
nized as follows. First, a review of the literature regarding
explainability in predictive process monitoring is given in
Section 2, together with the motivation for this line of
research. This is followed by preliminaries deﬁned in Sec-
tion 3. Next, a deﬁnition for explainability is given, together
with the introduced metrics in Section 4. The benchmark
study and implementation details can be found in Section 5,
after we clarify the research questions in Subsection 5.1. In
Section 6, the insights obtained from the research questions
are incorporated into the framework of guidelines named
X-MOP. Finally, the results and obtained insights are con-
cluded in Section 7.

2 RELATED WORK AND MOTIVATION

Predictive process monitoring is concerned with providing
insights about the business processes of modern organiza-
tions. Most predictive efforts are primarily driven by using
machine learning models such as XGBoost [2], [19], random
forest [2], [6], support vector machines [2], [6] or logistic
regression (LR) [2], with recent works showing interest in
applying deep learning models [6], [20], [21]. Nonetheless,
the lack of transparency of these sophisticated models pro-
hibits the ability to understand the rationale of the decision-
making process.

Over the last two years, there has also been a lot
of movement around explainability in predictive process
monitoring. The different works are often divided into
two different trends based on how they deal with the
explainability-performance trade-off [10]. The ﬁrst trend
generates explanations for the black box model using post-
hoc techniques. In predictive process monitoring, several
papers have already suggested explainability techniques on
top of machine learning models [22], [23] such as SHapley
Additive exPlanations (SHAP) [24] or Local Interpretable
Model-Agnostic Explanations (LIME) [25], with similar de-
velopments in a deep learning context [26], [27], [28], [29],
[30]. As an example, [26] visualizes the inﬂuence of certain
attributes in the different steps of the process in a Long
Short-Term Memory (LSTM) model with the use of SHAP
values, while Mehdiyev [27] focuses on creating local post-
hoc explanations with the use of a surrogate decision tree.
In [28] and [29], they use a Bidirectional LSTM, where

after the hidden states of the time steps of both RNNs are
concatenated. After this, a context vector is learnt that takes
the different time steps into account. Finally, [30] visualizes
the impact of the activities on the predictions with the use of
gated graph neural networks. The benchmark study of [23]
compares different explainability models and evaluates the
settings with properties unrelated to explainability, such as
stability and duration of execution.
The second trend introduces interpretable models instead
of trying to break open these black box models, stating that
there are alternative models that yield better explainability-
performance trade-offs. In [17], a set of fuzzy rules are
learnt from neural networks. Here, the relationship between
inputs and output is determined by a set of IF-THEN rules.
Nonetheless, this approach requires domain knowledge in
order to bin the attributes into different interpretable terms.
Next, a Bayesian network is used in [9] for next event and
sufﬁx prediction. Even though the causal relationships are
inferred from historical data, it relies on domain knowledge
assumptions [10]. Finally, [18] introduces two inherently in-
terpretable models to OOPPM, i.e., Logit Leaf Model (LLM)
and Generalized Logistic Rule Model (GLRM), both drawn
from XAI literature and adapted to the OOPPM case.

In the ﬁeld of XAI, a wide range of works have al-
ready evaluated these different models with predictive
performance-based metrics [6], [21] or with quantitative
metrics to assess the quality of the explanation methods [13],
[31], [32]. Nonetheless, these metrics do not take into ac-
count the different perspectives that are typical of a process-
based analysis. As an example, Islam [32] created a metric
for explainability based on the human-friendly properties
that deﬁne the concept explainability, while [31] introduces a
metric based on three different properties that deﬁne the
model complexity. Likewise, [13] introduced quantitative
metrics for different kinds of explanations types, based
on the identiﬁed properties for explainability. Additionally,
none of the metrics evaluate the faithfulness of the explain-
ability model. An initial attempt was made by Nguyen [33],
advising the use of mutual information to describe the
faithfulness of explanation methods on top of deep learning
models, but this metric is not model-agnostic (i.e., can not
be calculated for all explainability models). Next, the work
of Jain and Wallace [34] states that the learned attention
weights are uncorrelated with the gradient-based attribute
importance, even though they mimic the predictions rather
accurately. Similarly, Velmurugan [19] used a perturbation-
based technique to investigate the faithfulness of SHAP and
LIME in the ﬁeld of OOPPM, and the results report low-to-
moderate faithfulness scores.

A comprehensive study about the (un)faithfulness in
OOPPM is missing due to a number of reasons. The ac-
curacy by which these post-hoc explanations reﬂect the
effective model behaviour of the predictive model is often
inadequate in terms of attribute space [11], [18]. Moreover,
work from related ﬁeld show that there are substantial
problems with the use of post-hoc explainability methods.
First, the ﬁndings of [15] show that the importance ranking
of the attributes made by the attention scores is not faithful
to the model decisions (i.e., what the model perceived
as important). Second, demonstrates that there is a non-
monotonic relationship between the SHAP values and the

IEEE TRANSACTIONS ON SERVICES COMPUTING

3

predictive performance [35].

The above works indicate the need for model-agnostic
explainability metrics that are adapted to OOPPM. Further-
more, there is a growing need to evaluate the faithfulness of
(post-hoc) explainability methods. Therefore, these metrics
should include both the faithfulness of the explainability tech-
nique and interpretability of the explanations. These domain-
speciﬁc metrics can guide practitioners to select the best
model for the task at hand [36] and would reduce the
scope of research for human-based studies by reducing the
ﬁnancial and time costs of such experiments. There is a
notable void in the OOPPM literature in this respect, which
this paper will address through both introducing the notions
of interpretability, faithfulness, and a OOPPM-speciﬁc set of
guidelines.

3 PRELIMINARIES

This section ﬁrst describes the different terminology inher-
ited from the XAI ﬁeld, followed by the preliminary steps
needed for predictive process monitoring purposes.

3.1 The different XAI nomenclatures

Task model. As mentioned in Section 1, the task model is
deﬁned by several studies as the model that generates the
predictions [11], [12].
Transparency. Recent literature describes transparency as the
opposite of blackbox-ness [10]. Furthermore, a transparent
model (also referred to as an interpretable model) is a task
model that is able to generate its own explanations, where
the black box model requires the need of an additional
explainability model.
Interpretability. Originally described as comprehensibility
in [10], the interpretability is the ability to provide an ex-
planation that consists out of single chunks of information,
preferably in a human understandable fashion. Further-
more, it is often quantiﬁed by the related concept of model
complexity [10], [31].
Explainability model. The explainability model is the model
that generates the explanations for the predictions made
by the task model. This means that a transparent model is
technically also an explainability model, while the explain-
ability model of a black box model can be, e.g., a surrogate
model [27], an attention layer [29] or SHAP values [24].
Previous research has mostly referred to this as post-hoc
explanations [10], [12], or post-hoc interpretations [31]. Here,
the term explainability model is used to indicate that the post-
hoc explainability techniques should be both interpretable
and faithful (see infra).
Faithfulness. The faithfulness of an explainability model
can be considered as the accuracy by which the explain-
ability model accurately mimics the behaviour of the task
model (and not the predictions of the task model), as similar
predictions do not ensure that it correctly mimicked the
behaviour of the task model [11].
Explainability. Even though often used interchange-
ably [10], interpretability and explainability differ signiﬁ-
cantly due to the fact that an interpretable explanation is
not always faithful. To emphasize, a simple explanation
generated for a rain forecast prediction could be: ‘if the

grass is green, it will rain’, which is easy to interpret, but
unfaithful. The necessity to distinguish between faithfulness
and interpretability has already been pointed out by prior
research [12], [13].

3.2 OOPPM setup

OOPPM relies on the use of historic process data recorded
in an event log, which is a list of traces which represent
the enactment of a process for a particular case within an
information system [37]. Moreover, a trace is considered a
sequence of timestamped events generated by executing a
particular activity in the process. An event is a tuple of d
attributes x(cid:48)
j,,j∈{1,...,d} such as an activity name, Case ID,
timestamp, and so on. In the situation of a loan application
process, each event records the occurrence of an activity
(i.e. control ﬂow attribute) up until the loan request is
either accepted or rejected, with each activity having com-
plementary case and event attributes. These case attributes
remain unchanged (i.e. static) within each case, while the
event attributes have varying values (i.e. dynamic) for every
event. This means that an event has three different attribute
types: case, event and control ﬂow attributes.

The ﬁrst transformation step extracts (trace) preﬁxes
from the completed cases to be able to learn, preferably in-
crementally (over time), from the development of the traces.
To this end, a preﬁx log is typically derived, which is the
extracted event log that contains all the preﬁxes of each case
in the original event log. Next, trace cutting is performed
in [2], i.e. they limit the preﬁx length for computational
reasons.

j) = {x(cid:48)

The second data transformation step describes the en-
coding mechanism that enables the user to work with a
varying amount of attributes, since each trace can have
a different length. In the work of van Dongen [38], the
focus is on the order and execution of the activity in the
trace (i.e. control ﬂow encoding), with further research in
predictive monitoring [2], [4], [39] advising to expand the
attribute space with other event and case attributes. An
often used encoding mechanism is the aggregation en-
coding technique [5]. First, the categorical static attributes
are one-hot encoded, which means that each categorical
static attribute x(cid:48)
j results in a number of transformed at-
tributes based on the unique attribute values θ(x(cid:48)
j), with
θ(x(cid:48)
i,j}i∈{1,...,n}. The numeric static attributes re-
main unchanged. Second, the dynamic numeric attributes
are replaced by their summary statistics min, max, mean, sum
and std. The last transformation step relates to the dynamic
categorical attributes, where the frequency of occurrence
of an attribute value in a preﬁx is the value for the new
attribute. To overcome the obvious drawbacks of the lossy
aggregation encoding, the idea of index encoding [2], [4]
is to use all possible information in the trace, including
the order of the trace. First, a categorical event attribute
generates a number of attributes with a factor similar to the
number of attribute values multiplied with the preﬁx length.
Note that only for the activity values that actually happened
(i.e. visible in the event log), a new index is created. Second,
the amount of case attributes remain unchanged compared
to the aggregation encoding due to their static nature. This
way, a lossless encoding of the trace is achieved, which

IEEE TRANSACTIONS ON SERVICES COMPUTING

4

means that it is possible to completely recover the original
trace based on its attribute vector. By contrast, the use
of the above encoding mechanism in step-based models
such as recurrent neural networks becomes superﬂuous
given their sequential setup. To exploit this efﬁciently, a
low-dimensional representation of discrete attributes in the
form of embeddings is an often performed encoding tech-
nique [8]. This mapping transforms the categorical attribute
to a vector of continuous numbers, similarly to how one-
hot encoding works. Nonetheless, the latter ignores the
similarity between the obtained vectors. In the next part
of this paper, the attributes xj,j∈{1,...,p} are the resulting
attributes after the data transformation steps, performed
on the train event log. Consequently, the value for preﬁx
i,i∈{1,...,n} on attribute xj,,j∈{1,...,p} is denoted by xi,j.

A third data transformation step is referred to as trace
bucketing, commonly used to support the discovery of het-
erogeneous segments in the data. [2], [4], [40]. while creating
separate models for each of them. Techniques such as K-
Nearest Neighbours [39] or K-Means clustering measure the
(dis)similarity between traces depending on the parame-
ter K. The preﬁx bucketing technique [4] creates different
buckets for the preﬁxes of different lengths, while the state-
based bucketing technique [41] creates a different model for
each different decision point within the process model. Al-
though these bucketing techniques can effectively diminish
the runtime performance [2], they do not necessarily result
in an intuitive or interpretable outcome. E.g., clustering
techniques can base their grouping on a high number of
dimensions that are not interpretable. Furthermore, there is
no guarantee that the use of a bucketing technique will effec-
tively improve performance. These insights were obtained
and tested in [5].

After the data transformation steps, the transformed
event log is split into a train and test event log, where
the former is used to create a task model to predict the
dependent variable based on independent attributes. More-
over, the prediction for preﬁx trace i is denoted as ˆyi =
F (xi,1, . . . , xi,p). The ﬁnal step is to interpret the predictions
made by the task model. In the one hand, in a transparent
model, the inherently estimated coefﬁcients wa,1, ..., wa,p of
a transparent model indicate the importances of the different
attributes on the dependent variable. Figure 1 therefore
only needs to be taken into account when the predictive
model is a black box model that requires the need of an
additional explainability technique. In the other hand, the
use of a (post-hoc) explainability model is often considered
to approximate the attribute weights of the black box model
(i.e. task model) with the attribute weights we,1, ..., we,p of
the explainability model.

4 EXPLAINABILITY IN OOPPM

The separation of explainability into interpretability and
faithfulness stems from [12] and [13], and is adapted in
this paper to obtain model-agnostic metrics suitable for
a process-based analysis. As a result, the combination of
interpretability of explanations and faithfulness of explainability
model allows quantifying and thus evaluating explainability
in the ﬁeld of OOPPM.

To determine the interpretability of an explanation in the
context of OOPPM, it is necessary to divide the attributes
into the different attribute type (see Section 3), allowing the
metrics to take into account the process-based perspectives.
Furthermore, an interpretable model is different from the in-
terpretability of the explanation. An interpretable model (e.g., a
logistic regression model) that creates its own explanations
based on more than 500 attributes [42], has a low value
for interpretability of explanation. In the XAI literature, the
explainability-accuracy trade-off compares the model inter-
pretability (and not interpretability of explanation) with the
model accuracy, assuming that it is required to strike a bal-
ance between either simple models (e.g., linear regression)
or models using complex inference structures (e.g., neural
networks). By contrast, this paper investigates whether the
interpretability of an explanation is also in trade-off with the
predictive accuracy. In this sense, a non-interpretable deep
neural network can have higher interpretability of explana-
tions compared to a logistic regression model.

In the following subsection, four different explainabil-
ity metrics are adapted to and interpreted considering the
OOPPM context. Figure 2 describes the pipeline to calculate
the two interpretability and two faithfulness metrics.

4.1 Quantitative Metrics

Parsimony (C) is a property of interpretability [42] that
represents the complexity of a model [12], an often used
metric for linear regression models. This can be seen as the
number of attributes with non-zero weights e.g., non-zero
linear regression coefﬁcients, in other cases the non-zero
weights provided by the post-hoc attribute importances that
needs to be calculated post-hoc.

We adapt this metric to take into account the different
perspectives of a process-based analysis by determining the
parsimony for each attribute type t. This metric is quantiﬁed
by the number of attributes of attribute type t in the result-
ing model. The parsimony of the total model CF is equal to
the sum of the values for parsimony of the attribute types t.
Moreover, a parsimonious (i.e. simple) model corresponds
to a low value for CF = Cevent + Ccase + Ccontrol.

Assume attribute xj,k with wj,k the weight of an at-
tribute xj, j ∈ {1 . . . p} in the model F , with attribute type
t ∈ {1 . . . k}. Then, the total parsimony CF is calculated as
follows:

CF =

t
(cid:88)

pk(cid:88)

k=1

jk=1

with

C(xj,k)

C(xj,k) =

(cid:26)1,

if wj,k > 0,
0, otherwise.

(1)

(2)

Since the focus is on the interpretability of the explanation,
we deﬁne the parsimony of the model as a property for the
interpretability of an explanation rather than a property for
model interpretability.

Functional Complexity (F C) is a measure for model com-
plexity in line with other FC calculations such as [31]
and is made suitable for OOPPM data. In addition, [43]

IEEE TRANSACTIONS ON SERVICES COMPUTING

5

states that small random perturbations to the test images
can drastically change the generated explanations, with-
out the predicted label being ﬂipped. For this, we adapt
the metric further by investigating how many different
predictions there would be when permuting the attribute
values of an attribute type, and consequently measures
how strongly the explanations depend on that attribute
type. This permutation-based metric is consequently used
to retrieve the dependency of the attribute types (i.e.,
event/case/control ﬂow) on the explanations by investi-
gating how much percent of the predictions are ﬂipped. In
Algorithm 1, the pseudocode for the proposed FC algorithm
is given, and how the different permutations are made in
order to obtain the FC values for the different attribute
types.

Algorithm 1 Functional Complexity
Require: x1,1, . . . , x1,p, . . . , xm,1, . . . xm,p
Ensure: F Cevent, F Ccase, F Ccontrol

Attributetypes ← [event, case, control], F Cevent ← 0, F Ccase ←
0, F Ccontrol ← 0, z ← ()
T est ← [x1,1, . . . , x1,p, . . . , xm,1, . . . xm,p]
pseudocode simpler

(cid:46) To make the

3: for t ∈ Attributetypes do

T estcopy ← Copy(T est) (cid:46) copies the test data for each attribute

type t

for j ∈ Attributes do
N ewV alues ← ()
z ← θ(T est[j])

attribute

for i ∈ Instances do
V alue = T est[i, j]

instance i

z∗ ← {x ∈ z : x /∈ V alue}
if length(z∗) ≤ 1 then

(cid:46) take only the attributes of type t

(cid:46) all the unique attribute values of that

(cid:46) the current attribute value of

(cid:46) remove the current value

N ewV alues[i, j] ← V alue

(cid:46) for static attributes

else

N ewV alues[i, j] ← Random(z∗)

(cid:46) take a random

value

T estcopy[i, j] ← N ewV alues

(cid:46) replace with the permuted

attribute values

ˆyi = F (T est)
ˆy∗ = F (T estcopy)

type t

F Ct ← distance(ˆy,y∗)

n

(cid:46) predictions after permuting attributes of

(cid:46) a different FC for each attribute type t

6:

9:

12:

15:

18:

i,j, . . . , x∗

Assume ˆy∗ = F (x1,1, . . . , x∗
i,j+k . . . , xm,p) as the
prediction after permutation for attribute type t, where
x∗
i,j, . . . , x∗
i,j+k are the permuted attribute values of the
attribute xj, . . . , xj+k of the test instance i. The algorithm
starts by selecting the attributes of a certain attribute type
(lines 3-5), with the attribute value of instance i for attribute
xj removed from the set of permuted values in line 10.
Next, the functional complexity of an attribute type is
calculated as the amount of prediction changes before (line
16) and after (line 17) permuting all the attributes of an
attribute type t, divided by the number of instances. Finally,
the Hamming Distance (line 18) is used as the distance
measure between the different prediction vectors, where a
low value for F Ct means that the predictions are created
seemingly independently of this attribute type and should
therefore not be regarded as an important attribute type
when interpreting the explanations.

Monotonicity (M ) is a measure for the faithfulness of an
explainability method and is quantiﬁed with the use of

the non-parametric Spearman’s correlation coefﬁcient [33].
If the model inherently creates it own explanations (e.g.,
logistic regression coefﬁcients), then the monotonicity is
ensured (i.e. M = 1) by design [11].

The original metric [33] is model dependent on a neural
network model. Consequently, this paper adapted the met-
ric to make it model agnostic by evaluating how faithful the
attribute importance ranking of the explainability model is
to the ranking made by the task model. Furthermore, this
metric does not make a distinction between the different
attribute types, as the focus is on the relative ranking
of the attributes in general. For a post-hoc explainability
model, we quantify the monotonicity by the non-parametric
Spearman’s correlation coefﬁcient between the ranking of
the attributes by the task model and the ranking by the
explainability model. The monotonicity is deﬁned as:

M = ρ(wa, we)
(3)
with wa = wa,1, . . . , wa,p the attribute weights of the
task model and we = we,1, . . . , we,p the weights of the
explainability model. This correlation coefﬁcient takes a
value between [-1,1] and describes the association of rank.
A perfectly faithful model has a correlation coefﬁcient of
+1, where a loss in faithfulness corresponds with a value
closer to 0. Consequently, a negative value corresponds
to a negative rank association between the two attribute
importance weights.

Level Of Disagreement (LOD@10) [44] is a metric of faith-
fulness and originally computes the percentage of similar
predictions between the task model and post-hoc explana-
tions.

In the ﬁeld of predictive process monitoring, it is useful
to know the importance of the attribute types, as the ob-
tained insights are relevant in order to improve the early
prediction problem [2]. In this paper, the LOD@10 therefore
investigates whether the task model and the explainability
model focused on the same attribute type. The LOD@10,
different from monotonicity, neglects the ranking of the
attributes but only looks at the relative frequency of the
attribute types in the top 10 most important attributes. For
this, the metric is quantiﬁed with the Euclidean Distance
between the relative frequency of top ten attributes of the
task model #a,k and the explainability model #e.

LOD@10 = d (#a, #k) =

and

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t
(cid:88)

k=1

(#a,k − #e,k)2






t
(cid:88)

k=1
t
(cid:88)

k=1

#a,k = 10 with t ∈ {1 . . . k}

#e,k = 10 with t ∈ {1 . . . k}

(4)

(5)

This metric is introduced to take into account that the
number of attributes used in the task model has a negative
inﬂuence on the monotonicity value. Furthermore, a high
value for this LOD@10 metric indicates that the explainabil-
ity model focused on rather different attribute types, which

IEEE TRANSACTIONS ON SERVICES COMPUTING

6

with the LR model. In research question RQ4, the LLM
model is relatively compared to the LR model based on
the performance and four explainability metrics. Finally,
transparent models (i.e. model that are faithful by design)
typically underperform compared to the black box models
in the case of sequential and high-dimensional data [6],
which is evaluated with the use of research question RQ5.

The next two research questions RQ6 and RQ7 are based
on how the explanations (see Figure 2) are generated, and
describe the interpretability-accuracy trade-off in OOPPM.
Here, the ranking of attribute types by the parsimony
and the functional complexity are relatively compared. Al-
though both metrics are a measure for model complexity, the
parsimony evaluates the importance of the attribute types
as perceived by the task model, while the functional com-
plexity is the importance of the attribute types assessed on
the test data. Finally, the faithfulness of the post-hoc expla-
nations (i.e. SHAP values) is compared with the attention-
based explanations that are technically calculated during the
ﬁtting of the task model. Moreover, it is implicitly assumed
that the faithfulness of the latter should be higher compared
to the SHAP values. This leads to the research question RQ8.
The ﬁnal research question RQ9 compares the monotonicity
with the LOD@10 values. The monotonicity evaluates the
importance ranking of all the attributes, while the LOD@10
investigates the difference in importance of the attribute
types in the top 10 most inﬂuential attributes for the task
model and the explainability model. Moreover, a high value
for monotonicity (i.e. the model had an overall good idea of
the importance ranking) can be related to a high value for
LOD@10 (the model focused on a different type of attribute
in the top 10 attributes).

RQ1. What is the inﬂuence of the encoding mechanism on the
interpretability of the explanations and faithfulness of the
explainability model?

RQ2. Do deep learning models extract more information from the
sequential, control ﬂow perspective compared to the other
models which are conceived for tabular data?

RQ3. Does the previously established outperforming of the deep
learning models come at the expense of the interpretability
and faithfulness?

RQ4. Does the LLM model outperform the logistic regression

model?

RQ5. Are transparent models capable of attaining the perfor-
mance level of the black box models in the ﬁeld of OOPPM?
RQ6. Is the relative ranking of parsimony values similar to the

ranking of the functional complexity values?

RQ7. Does the interpretability-accuracy trade-off hold in the ﬁeld

of OOPPM?

RQ8. Are the explanations generated by post-hoc explainability
techniques less faithful compared to the explanations that
contribute to the predictions of the black box model but are
typically calculated afterwards?

RQ9. Is an explainability model with a low monotonicity value

always associated with a high LOD@10 value?

5.2 Event logs

This study is based on four different real-life event logs that
can be found at the website of 4TU Centre for Research

Fig. 1. The pipeline to calculate the introduced metrics for explainable
AI purposes in outcome-oriented predictive process monitoring.

impairs the faithfulness of the explainability model.

5 BENCHMARK STUDY

In this section, a detailed build-up to the research questions
related to the interpretability and faithfulness of OOPPM
models is provided. Next, the different event logs and their
speciﬁcations are described, followed by a description of
the benchmark models and explainability techniques often
used for OOPPM purposes. Finally, the hyper optimization
settings and implementation details of the different setups
are given.

5.1 Research Questions

The research questions are aimed to investigate the inﬂuence
of the most important steps in an OOPPM context on the ex-
plainability metrics from Section 4.1, in order to establish a
set of guidelines to obtain accurate and explainable OOPPM
solutions. For this, an experimental pipeline similar to the
benchmark studies of [2] and [6] and extended with an XAI
dimension (see Figure 2 for an overview) is used.

The ﬁrst research question RQ1 aims to uncover the
inﬂuence of the sequence encoding techniques (see Fig-
ure 2) on the properties of explainability in OOPPM. The
choice of the encoding is an integral part of transforming
process data and can have a detrimental effect on the metrics
(e.g., higher CF ) as the index encoding creates a high-
dimensional feature space (see Section 3). Nonetheless, the
index encoding can be preferred over its lossy variant, the
aggregation encoding, as there is no loss in information
(higher expected M ).

The second research question RQ2 discusses the inﬂu-
ence of model complexity (see Figure 2) and aims to assess
whether the deep learning model focus more on the con-
trol ﬂow perspective (i.e. higher Ccontrol) compared to the
traditional cross-sectional statistical and machine learning
models, as their sequential architecture is more tailored to-
wards modelling time-dependent and sequential data tasks
without such aggregation.

The next three research questions evaluate the differ-
ent classiﬁers (see Figure 2). RQ3 investigates whether
the previously established higher performance of the deep
learning models [6] comes at the cost of interpretability
and/or faithfulness. Next, this study compares the inter-
pretable LLM model, introduced in [18], as an alternative
to the bucketing technique (see Section 3) in conjunction

Event logTrainExplainability modelMonotonicityPredictionsFunctional complexityTask modelParsimonyData transformationTestLevel of disagreementIEEE TRANSACTIONS ON SERVICES COMPUTING

7

Fig. 2. An overview of the experimental pipeline. The transparent models with inherently created explanations are indicated in green. The black box
models, that require post-hoc explanations, are coloured in orange.

Data1, and are often used in the ﬁeld of OOPPM [2], [6], [27],
[28], [30]. These event logs are split with Linear Temporal
Logic (LTL) rules as deﬁned in [2] to obtain objectives for
the process. Moreover, the event log is split based on the
labelling functions deﬁned by the four LTL rules, therefore
creating four different binary prediction tasks. The event log
speciﬁcations are deﬁned in Table 2.
The ﬁrst event log, BPIC2011, describes the medical history
of patients from the Gynaecology department of a Dutch
Academic hospital. The applied procedures and treatments
of the different cases represent the activities in this event
log, with the label being either true or false if the LTL
rule is violated or not, respectively. The four different LTL
rules to generate the event logs: bpic2011(1), bpic2011(2),
bpic2011(3) and bpic2011(4), are described in [2]. Next,
similar trace preﬁxing and cutting preprocessing steps are
performed as in [2].
The BPIC2015 event log assembles events pertaining to
the building permit application process from ﬁve Dutch
municipalities. A single LTL rule is applied on the event
log and split for each of the ﬁve municipalities. The LTL
rule deﬁnes that a certain activity send conﬁrmation receipt
must always be followed by retrieve missing data (and not the
other way around), where the latter activity has to always
be present in the trace if the former was. No trace cutting
was performed on this event log.
The sepsis cases event log contain the discharge information
of patients with symptoms of sepsis in a Dutch hospital,
starting from the admission in the emergency room until
the discharge of the patient. Here, the labelling is performed
based on the discharge of the patient instead of LTL rules [2].
In Production, the last event log contains information about
the activities in a manufacturing process, together with the
workers and/or machines of the production of the items
itself. The labelling function is based on whether the number
of work orders rejected is larger than zero or not.

1. https://data.4tu.nl/

5.3 Benchmark models and explainability models

The wide variety of classiﬁers and explainability models is
depicted in Figure 2 and are chosen based on their frequent
presence in other studies such as [2], [6], [26], [29]. The
ﬁrst model is the logistic regression model, an often used
interpretable predictive technique to model the probability
of a discrete variable. The two advanced logistic regression
models, LLM and GLRM, are interpretable models that
are introduced in [18] in the ﬁeld of OOPPM. The former
clusters the data with a decision tree and builds linear
models in the leave nodes. The latter creates binary rules
with a generalized logistic rule model. In conclusion, the
former tree models generate their own explanations.

The next two models are ensemble machine learning
models, i.e. XGBoost (XGB) [45] and Random Forest (RF)
[46], which are not interpretable models, as the inherent
complexity is what bestows their predictive abilities. In the
XGB model, weak learners are iteratively improved to a
ﬁnal strong learner by incorporating the loss function of
the previous weak learner(s). On the other hand, the RF
trains a number of decision trees on various subsets of the
data. Different to XGB, the voting scheme is based on the
majority votes of predictions. By contrast to the transparent
models, ensemble methods require an explainability model.
For this, the explanations for the ensemble methods are
created with SHAP values, which are calculations for each
instance-attribute combination based on coalitional game
theory. Here, a prediction is explained by assuming that
each attribute value is a player in a game, where the pre-
diction is the payout. This model-agnostic technique tells
how to distribute the payout among the attributes, as the
SHAP values are the average marginal contribution of an
attribute value across all possible coalitions.

Next, two different neural network models are used. The
ﬁrst model is a recurrent neural network with Long Short-
Term Memory (LSTM), often used for OOPPM [6], [20], [26],
[28] with the long-term relations and dependencies encoded

Event logEmbedding encodingIndex encodingAggregation encodingMachine learning modelLLMLR coefficientsRule-based LR coefficientsDT rules + LR coefficientsLRXGBRFCNNLSTMSHAP valuesAttention valuesGLRMDeep learning model❑Faithful by definition❑Post-hoc explanationsSequence encodingModel complexityClassifierExplanationsIEEE TRANSACTIONS ON SERVICES COMPUTING

8

in the cell state vectors, designed to solve the vanishing
gradient problem. The advantage of LSTM over classical
machine learning models lies in the ability to model time-
dependent and sequential data tasks, where the categorical
values are encoded in embeddings. The second model is
a Convolutional Neural Network (CNN) which is a deep-
forward artiﬁcial neural network with information ﬂow
starting from the input layers, through the hidden layers,
until the output layer (therefore only in one direction).
To the best of our knowledge, this has only implemented
by [47] for OOPPM, but is incorporated in this study due
to the frequent use in related ﬁelds such as next activity
prediction [48], [49]. Similar to the ensemble methods, the
internal representation of a deep neural network (LSTM
and CNN), does not allow for inherent explanations of
predictions. The use of attention layers is a model-speciﬁc
post-hoc explainability technique in the strict sense of the
meaning, as attention is contributing to the prediction but
typically calculated afterwards to obtain attribute impor-
tance scores. These attention layers calculate non-negative
weights (multiplied by their corresponding representations)
for each input that together sums to one, and ﬁnally sums
the resulting vectors into a single ﬁxed-length representa-
tion [15].

5.3.1 Preprocessing steps

First, an equivalent train-test split as in [2] is performed. To
this end, a temporal train-test split is performed that ensures
that the period of training data does not overlap with the
period of the test data, while the event of the cases in the
train data that did overlap with the test data are cut. Next,
the traces are cut with similar upper length as deﬁned in [2].
Then, both the index (index) and aggregation (agg) en-
coding are used to encode the data for the machine learning
algorithms. Note that the aggregation encoding is unique
to processes, and is the primordial reason to investigate a
dedicated OOPPM XAI-based approach. The index encod-
ing is created for traces of all lengths, in contrast to the study
of [2], where they used the index encoding for traces with a
similar preﬁx length (i.e. preﬁx length bucketing technique).
The bidirectional LSTM neural network architecture
with attention layer for interpretation purposes stems
from [29]. The embedding mechanism is used for categorical
attributes in the deep neural networks. Compared to the
original set-up, the numerical attributes are added as input
layers to the LSTM model. Finally, the predictive func-
tion of [29] is transformed into a binary outcome-oriented
prediction by stripping off the ﬁnal layer and inserting a
sigmoid output layer instead. In order to compare, ceteris
paribus, the overall performance of the LSTM with the
CNN, we ensure that both models have a similar set-up.
Therefore, the CNN model start from the same architecture
as the LSTM, and the Bidirectional LSTMs are replaced with
1D convolutional layers. Different to the LSTM model, the
attention is calculated directly after the input layers and
embeddings (similar to [50]), and the kernel size is set to
be the same as the length of the sequences and the ﬁlter as
the length of the concatenated input. Additionally, an extra
dense layer with Rectiﬁed Linear Unit (ReLU) activation
was added before the ﬁnal dense layer, in order to ensure
that the output is correctly linked back to the inputs.

The event logs are ﬁltered out for which the average
obtained AUC over all the classiﬁers was lower than 50 (i.e.
sepsis cases(2), as no analysis should be performed where
random luck has a better ability compared to the classiﬁers
in order to distinguish between the classes). For the XAI
evaluation, we additionally ﬁltered out the event logs that
had an average AUC below 75 (sepsis cases(3) and produc-
tion). This was to overcome the issue that explainability
techniques can only perform well when the original task
model is performant enough.

Algorithm 2 Attribute Importance
Require: Attributes, AttibyteT ypes, T rain
Ensure: ef f ects

z ← (), ef f ects ← (), T rain ← [x1,1, . . . , x1,p, . . . , xn,1, . . . xn,p]
for j ∈ Attributes do

(cid:46) take only the attributes of type t
(cid:46) copy the train data for each new

T raincopy ← Copy(T rain)

attribute

N ewV alues ← ()
z ← θ(T rain[j]) (cid:46) all the unique attribute values of that attribute
for i ∈ Instances do

V alue = T rain(i, j) (cid:46) the current attribute value of instance i
z∗ = {x ∈ z : x /∈ V alue}
(cid:46) remove the current value
if length(z∗) ≤ 1 then

N ewV alues[i, j] ← V alue

(cid:46) for static attributes

else

N ewV alues[i, j] ← Random(z∗)

T raincopy[j] ← RandomV alue(z∗)

(cid:46) take a random value
(cid:46) replace with permuted

values

ˆy ← predict(T rain)
y∗ ← predict(T raincopy) (cid:46) predictions after permuting attribute

xj

(cid:113) (ˆy−y∗)2
n

ef f ectj =
ef f ects(j) ← ef f ect

(cid:46) root mean square error
(cid:46) save the effects of all the attributes

5.4 Attribute Importance

In case of a black box model, the attribute weights of the task
model we,1, ..., we,p need to be measured post-hoc, where
for tree-based models a naive method exists in counting the
number of times the attributes are selected by the individual
trees in the ensemble method [51]. As an improvement, the
authors suggest a permutation-based approach by looking
at the change in accuracy before and after permuting the
attribute values. In this paper, the attribute importance
for all black box models are calculated similarly, with the
use of a perturbation-based method as described in Algo-
rithm 2, where we evaluate the accuracy change using the
Root Mean Squared Error (RMSE). Note that some remarks
about this algorithm should be made. First, the attribute
importance for the deep learning models is calculated by
permuting the original attributes [x(cid:48)
d] instead of the
transformed attributes [x1, . . . , xp]. This makes sense for
the following reason: the monotonicity value compares the
importance of the attributes of the task model with the
importance of the attributes of the explainability model. To
ensure that they are both calculated on the same number
of attributes, the attribute importance is calculated with the
original attributes, and the attention values are aggregated
to obtain the importances for the original attributes.

1, . . . , x(cid:48)

5.5 hyper optimization details

Finally, a 4-fold cross validation was performed with the
use of hyperopt for the machine learning models. The hyper

IEEE TRANSACTIONS ON SERVICES COMPUTING

9

optimization for the deep learning models was done by a 10-
fold cross validation, as the neural networks need additional
evaluation to overcome the plethora of local minima. For the
LR, XGB and RF model, similar hyperparameter settings
are taken from [2]. For the LLM model, we additionally
incorporate the maximum depth of the decision tree and the
minimum samples per leaf to ensure that the tree splitting
does not allow overﬁtting. According to [28], the optimal
dropout rate for the bidirectional LSTM with attention was
around 0.2. Therefore, we have set the maximal dropout rate
to 0.3 (as a margin of error). As a ﬁnal remark, extra detailed
information about design implementations and parameters
are provided on GitHub2, to enhance the reproducibility
results.

6 DISCUSSION

As a precursor to the discussion, the predictive performance
based on AUC is evaluated in-depth. To have a more pro-
found idea, we use statistical tests to mathematically ground
the conclusions. Note that this is separated from the research
questions to ensure that the focus remains on the inﬂuence
of experimental set-up on the introduced metrics.

First, we compare the embedded DLembed models with
the ensembleindex methods in a similar experimental set-
up to Kratsch [6]. For this, we only compare the M Lindex
models with the DL models, as the aggregation encod-
ing is a lossy encoding (see Section 3). The paired t-test
(pvalue = 0.55) indicates that the DLembed (87.28 AUC) do
not statistically outperform the ensembleindex (86.08 AUC),
despite having a slightly higher mean. When comparing
Table 2 and Table 3, we can see that DL models tend to work
best for event logs with a high variant to trace ratio (i.e. the
BPIC2015 event logs). We also see that CNNs perform better
than LSTMs for logs featuring a high dynamic versus static
ratio (BPIC2015(2) and BPIC2015(5)). In addition, many
papers state that sequence encoding and extraction of at-
tributes is the most important to increase the predictive per-
formance [3], [4]. Therefore, we expect the index encoding to
perform better than the aggregation encoding, as the former
provides the predictive model with more information about
the sequential development of the process. Contrarily, the
paired t-test shows that the M Lagg (92.71 AUC) statistically
outperforms (pvalue = 0.000000003) the M Lindex models
(85.24 AUC), with the former obtaining the best results in
9 out of 10 event logs. Intuitively, one could argue that the
index encoding is potentially underperforming due to an
explosion of the attribute space.

In the next subsection, the research questions are an-
swered in-depth. To obtain actionable results, it is required
that we link the obtained insights of the research questions
with the event log speciﬁcations of Table 2. For this, a case-
based evaluation in the context of OOPPM is performed
with the use of two event logs. This section is ﬁnalized
with the framework of guidelines for Explainable AI in
OOPPM in order to guide the practitioner to the correct
model selection. In Table 3, an overview of the AUC results
for the encoding-classiﬁer combinations is given.

2. https://github.com/AlexanderPaulStevens/Evaluation-Metrics-

and-Guidelines-for-Process-Outcome-Prediction

6.1 Research questions

The ﬁrst research question RQ1 investigates the inﬂuence
of the encoding mechanism on the explainability metrics,
and Table 1 shows that the use of index encoding over
aggregation encoding increases the parsimony CF and con-
sequently induce more complex explanations. As an exam-
ple, the parsimony of the event attributes (Cevent) increased
with a factor of 15 for the index encoded variant of the
RFagg for the event log BPIC2011(2). In a nutshell, the
index encoding has generally higher values for parsimony
CF compared to its aggregation variant. Finally, regarding
the faithfulness of the model, we observe that the index
encoding has a negative impact on the monotonicity M of
the explainability model. Moreover, in only two situations,
the M was lower for the index encoded variant (sepsis cases
(2) and bpic2011(2)). These observations answer RQ1. The
ﬁrst subplot of Figure 3 shows that the DL models extract
more information from the sequential, control perspective
compared to the other models (DL models use 100% of the
control attributes), answering RQ2. If we look at the mean
value per dataset (including the aggregation encoding), the
bottom-up ranking of the classiﬁers is as follows: CNN,
LSTM, XGB, RF, LLM, LR. This means that two transparent
models obtain the highest performance (on average), with
the DL models having the worst performance. The trans-
parent models are therefore able to attain the performance
level of the black box models in the case of sequential and
high-dimensional data, already answering RQ4. A possible
explanation for why the DL models underperform com-
pared to the ML models is due to the trace cutting of the
preﬁxes (see Section 3). Without this trace cutting, the DL
models would probably obtain higher AUC values than the
ML models, as the former are better capable of handling
long-term dependencies. In line with the results from [47],
[48], the CNN performs better than the LSTM in 6 out of 10
event logs. Finally, to answer RQ3, the F C of the DL models
is extremely low, which casts considerable doubt on the
interpretation of the generated explanations. In addition, the
faithfulness of the CNN model is very compromised (even
reports negative values). This can be due to the fact that at-
tention values focus on explaining the representation of data
inside a network rather than explaining the processing of
data [52]. Next, the LR (the best overall AUC performance
in 4 out of 10 event logs) is ranked higher compared to the
LLM model, with the latter model only performing better
than LR for the event log BPIC2015(4)). The LLM is a hybrid
of decision tree clustering and logistic regression models,
as there are logistic regression models implemented in the
leaf nodes created by the decision tree. Furthermore, the
LLM model is implemented as such that at least one split
is enforced. This enforced split might explain the negative
effect on the performance of the LLM compared to LR,
and consequently answers RQ5. Some additional interesting
remarks can be made from Figure 3. First, even though most
of the attributes used in the deep neural networks are event
attributes, permuting these values has a very low inﬂuence
on the predictions. This means that, although indicated as
important by the model, the F C shows otherwise. On the
other hand, the XGB model does not use a lot of control ﬂow
attributes (noticeable with the top two ﬁgures in Figure 3),

IEEE TRANSACTIONS ON SERVICES COMPUTING

10

TABLE 1
The Results for the Third Municipality of the BPIC2015 and the BPIC2011(2) Event Log.

BPIC2015(3)
GLRMindex
XGBindex
RFindex
LRindex
LLMindex
GLRMagg
LST Membed
CN Nembed
XGBagg

LLMagg

LRagg

RFagg

BPIC2011(2)
CN Nembed
LST Membed
GLRMindex
XGBindex
RFindex
LLMindex
LRindex
LLMagg

GLRMagg

XGBagg

RFagg

LRagg

AUC

event

case

ctrl

Cevent

Ccase

Ccontrol

0.54

0.87

0.90

0.90

0.91

0.94

0.94

0.95

0.95

0.96

0.96

0.96

1700

1700

1700

1700

1700

87

11

11

87

87

87

87

34

34

34

34

34

34

18

18

34

34

34

34

AUC

event

case

0.77

0.87

0.88

0.90

0.94

0.94

0.96

0.97

0.98

0.98

0.98

0.99

13

13

2660

2660

2660

2660

2660

146

146

146

146

146

6

6

760

760

760

760

760

760

760

760

760

760

3083

3083

3083

3083

3083

314

1

1

314

314

314

314

ctrl

1

1

3139

3139

3139

3139

3139

215

215

215

215

215

1

376

1317

823

440

0

11

11

62

86

63

84

0

21

33

33

12

0

1

1

21

34

33

33

Cevent

Ccase

13

13

2

308

2061

7.5

39

126.5

2

79

134

7

6

6

4

79

760

28.75

148

542.5

5

65

760

118

2

146

1867

1744

805

8

1

1

74

308

160

275
Ccontrol

1

1

2

110

1974

9

57

192.5

1

73

207

9

Cevent
event
0.06

22.12

77.47

48.41

25.87

0.00

100.00

100.00

71.26

98.85

72.41

96.55
Cevent
event
100.00

100.00

0.08

11.58

77.48

0.28

1.47

86.64

1.37

54.11

91.78

4.79

Ccase
case
0.00

61.76

97.06

97.06

33.82

0.00

5.56

5.56

61.76

98.53

97.06

97.06
Ccase
case
100.00

100.00

0.53

10.39

Ccontrol
control
0.06

4.74

60.56

56.57

26.09

2.55

100.00

100.00

23.57

97.93

50.96

87.58
Ccontrol
control
100.00

100.00

0.06

3.50

100.00

62.89

3.78

19.47

71.38

0.66

8.55

100.00

15.53

0.29

1.82

89.53

0.47

33.95

96.28

4.19

F Cevent

F Ccase

F Ccontrol Mono.

LOD@10

0.44

27.78

12.76

18.80

12.89

0.00

3.74

2.67

16.88

1.25

0.06

14.02

82.99

6.74

85.08

6.10

0.00

0.00

1.38

1.04

3.08

0.00

0.01

5.38

F Cevent

F Ccase

2.86

1.51

24.76

21.58

15.90

40.78

3.67

44.86

31.89

41.43

27.54

3.22

3.95

1.51

68.05

19.46

12.34

77.80

0.66

0.36

46.59

13.93

28.11

0.38

97.31

46.35

85.08

5.68

98.05

89.52

3.72

2.90

87.72

92.74

1.30

1.00

0.78

0.17

1.00

1.00

1.00

0.65

-0.52

0.87

1.00

1.00

0.00

8.83

7.87

0.00

0.00

0.00

1.41

2.83

0.00

0.00

0.00

81.89

0.43
F Ccontrol Mono.

10.20
LOD@10

3.95

1.51

71.01

14.10

46.67

19.64

0.65

9.07

40.74

23.61

25.33

0.54

0.50

0.64

1.00

0.68

0.06

1.00

1.00

1.00

1.00

0.75

0.05

1.00

2.24

1.00

1.00

2.45

5.10

0.00

0.00

0.00

0.00

2.83

8.60

0.00

TABLE 2
The Different Speciﬁcations of the Event Logs.

Event
log
BPIC2011(1)
BPIC2011(2)
BPIC2011(3)
BPIC2011(4)
BPIC2015(1)
BPIC2015(2)
BPIC2015(3)
BPIC2015(4)
BPIC2015(5)
SEPSIS(2)
SEPSIS(4)
Production

Traces

Events

1140
1140
1121
1140
696
753
1328
577
1051
782
782
220

67480
149730
70546
93065
28775
41202
57488
24234
54562
10924
12463
2489

Med.
length
25
54.5
21
44
42
55
42
42
50
13
13
9

Max.
length
1814
1814
1368
1432
101
132
124
82
134
60
185
78

Preﬁx cut

Var.

36
40
31
40
40
40
40
40
40
13
22
23

815
977
793
977
677
752
1280
576
1048
656
709
203

Act.
T race
193
251
190
231
380
396
380
319
376
15
15
26

Stat. cat.
levels
961
994
886
993
19
7
18
9
8
195
200
37

Dyn. cat.
levels
290
370
283
338
433
429
428
347
420
38
40
79

V ar.
T race
0.71
0.86
0.71
0.86
0.97
1.00
0.96
1.00
1.00
0.84
0.91
0.92

Events
T race
59
131
63
82
41
55
43
42
52
14
16
11

Events
Act
3
5
3
3
1
1
1
1
1
1
2
3

Dyn.
Stat.
0.3
0.37
0.32
0.34
22.79
61.29
23.78
38.56
52.5
0.19
0.2
2.14

TABLE 3
The Performance (AUC) per Event Log.

Event
log
BPIC2011(1)
BPIC2011(2)
BPIC2011(3)
BPIC2011(4)
BPIC2015(1)
BPIC2015(2)
BPIC2015(3)
BPIC2015(4)
BPIC2015(5)
SEPSIS(2)

LRagg

LRindex

LLMagg

LLMindex GLRMagg GLRMindex XGBagg XGBindex

RFagg

RFindex

LST Membed

CN Nembed

96.60
98.81
98.72
87.77
90.86
95.15
95.85
93.77
93.86
92.79

94.43
96.41
97.21
86.98
88.78
89.58
89.82
93.78
91.48
92.63

96.38
97.23
97.24
88.38
91.34
92.72
95.64
93.71
93.89
89.48

94.79
94.16
96.71
87.24
74.34
90.16
90.53
94.34
90.59
88.18

92.05
97.97
98.39
78.94
88.62
86.53
93.79
91.33
90.49
73.04

81.16
88.11
97.62
78.03
46.96
57.88
55.08
62.98
77.50
73.04

95.57
97.99
98.85
85.12
91.39
94.91
94.93
93.05
94.12
84.92

92.31
90.28
97.70
83.62
74.45
82.87
86.80
81.88
83.18
86.79

93.09
98.49
98.87
89.04
92.53
94.00
95.86
93.98
94.96
82.70

89.42
93.89
96.78
77.53
71.00
87.95
89.54
84.50
89.22
81.97

75.87
86.69
80.62
87.43
88.40
93.99
93.88
87.93
91.43
80.25

83.11
77.17
91.09
85.54
91.62
92.98
94.56
84.64
93.43
85.04

but the F C of the respective variables are very high, which
means that this attribute type was very important for the
prediction. This shows that the relative ranking of the F C
values is not always similar to the relative ranking of the
parsimony CF values. This is considered a problem in cases
where the parsimony indicates that a particular attribute
type is relatively unimportant, but permuting these values
have a very high inﬂuence on the predictions (meaning that
they are somehow important?). Similar insights can also be
obtained from Table 1 (BPIC2015(3)). The GLRM model only
uses eight control attributes, but permuting them leads to
more than 85% different predicted labels. This answers RQ6.

does not hold in the ﬁeld of Outcome-Oriented Predictive
Process Monitoring if we look at the average amount of
parsimony CF versus the average amount of AUC (results
are not reported in the table but can be found on 3).
Conversely, the agg models obtain higher AUC and report
lower parsimony CF values. Interesting to see is how well
the GLRM model performs with an overall average of 12
attributes (compared to over 625 and 187 for LR and RF
respectively), while remaining similarly performant. If we
compare the interpretability of the explanations (i.a. deﬁned
by the parsimony CF of the control attributes, Ccontrol),

To answer RQ7, the interpretability-accuracy trade-off

3. https://github.com/AlexanderPaulStevens/Evaluation-Metrics-

and-Guidelines-for-Process-Outcome-Prediction

IEEE TRANSACTIONS ON SERVICES COMPUTING

11

LLMindex for BPIC2015(3) states that more than 98% of
the predictions change after permuting the control ﬂow
attributes. Furthermore, the Ccase
case values show that most
models still use most of the case (i.e. static) attributes due
to the fact that there is a lot of case-speciﬁc information in
the event logs. In addition, it is also possible to compare the
interpretability between different classiﬁers. Moreover, we
see an F Ccontrol of 87.72% for the XGBagg model, with a
parsimony Ccontrol of 74. For the RFagg model, we observe
an F Ccontrol of 81.89%, with a parsimony Ccontrol of 275
control columns. This means that the XGBagg model uses
fewer attributes compared to RFagg, but the attributes used
are more important for the predictions made (as 87% percent
of the predictions change if these attributes are permuted).
From a predictive performance-based perspective, the
RFagg, LRagg or LLMagg model is preferred. Nonetheless,
the monotonicity M and level of disagreement LOD@10
values show that the faithfulness of the explainability model
of the RFagg model is compromised. These values indicate
that the explainability model is not able to correctly mimic
the model behaviour of the task model. This can be due to
the fact that the RFagg model uses all of its attributes (visible
with the red colour) based on the values for Cevent
event , Ccase
case
and Ccontrol
control . The LRagg model is faithful by deﬁnition, as it
generates its own explanations. Nonetheless, the model uses
160 control attributes (CF , but the value for F Ccontrol shows
that 1.30% of the predictions change when we permute the
control attribute values of the test data (do these 160 attributes
really matter for the interpretation of the explanation?). Next, the
LLMagg model obtains the second-best performance, but
has a very low value for interpretability (high CF ). A next
remark is that, although the LSTM model has a considerably
smaller attribute space (as the attention values are for the
original attributes due to the embedding mechanism), the
M appears to be compromised.

Finally, the GLRMagg uses only eight control attributes
(Ccontrol), two event (Cevent) and one case (Ccase) attribute,
while remaining performant (93.79 AUC). The probability
of being classiﬁed as ’deviant’ is calculated similarly to a
regular logistic regression model [18], with the formula:

with

log(z)=

1
(1+e−z ))

z=6.35−10.20∗(¬control1agg∪control2agg≤0.5)

−5.76∗((control3agg>0.5)∪(control2agg≤0.50)

−2.86∗((controll2agg≤0.5)∪(¬case1)

−−1.34∗(¬control1agg∪(control2agg≤0.5)∪(event1>7.5))

−0.97∗((control2agg≤0.5)∪(event1≥7.5))

The rules are roughly based on the presence (or absence)
of certain control attributes in combination with an event
and/or case attribute. To conclude, the use of the GLRM
model is advised, as the model is inherently faithful (i.e.
creates its own rules) and is highly interpretable while
remaining performant in terms of AUC.

Similarly, an analysis for the event log BPIC2011(2) can
be made. From a performance-based point of view, the
LRagg is preferred. Nonetheless, it is again noticeable that

Fig. 3. Interpretability metrics per classiﬁer. The top ﬁgure describes
the percentage of parsimony CF of each attribute type individually. The
second subplot of Figure 3 shows the percentage parsimony CF per
attribute type, over the total parsimony CF . The bottom subplot shows
the F C per attribute type.

then the LR (≈ 100) performs a lot worse than GLRM
(≈ 10), but still better than LLM (≈ 600).

Furthermore, the XGB model obtains higher values in
9/10 event logs for the monotonicity M compared to the
LSTM and CNN model, answering RQ8. Finally, if we look
at BPIC2015(3) and the XGBagg model in Figure 1, we can
observe a rather high value for M (0.78), but also a high
value for level of disagreement LOD@10 (8.83). This means
that the ranking of the importance of all the attributes was
rather good, but the SHAP values were not able to indicate
correctly on what attribute types the task model XGB truly
focused, answering RQ9.

6.2 Event log Analysis

In BPIC2015(3), all

A more in-depth analysis is given for two event logs,
BPIC2015(3) and BPIC2011(2).
the
M Lagg models obtain the best performance, and the DL
models perform better than the M Lindex models. This
provides insight into the extent to which the performance
is in trade off with the explainability. Contrarily,
the
M Lindex models performed better than the DL models in
BPIC2011(2), with the latter models reporting the lowest
AUC performance.

First, the event log BPIC2015(3) is discussed. Table 2
indicates that this event log has a high variant to trace
T race ) ratio (0.96) and high dynamic to static ( V ar.
( V ar.
T race ) ratio
(23.78). These are the event log properties deﬁned by [6] for
which the DL models perform better than the ML models.
Next, the LTL rule setting the objective is based on the
presence of control ﬂow attributes, which means that we
expect a high F Ccontrol, which is clearly visible in most
of the models. As an example: the F Ccontrol value of the

CNNGLRMLLMLRLSTMRFXGB050100Percentage Ci of type ieventcasecontrolCNNGLRMLLMLRLSTMRFXGB050100Percentage Ci of total CCNNGLRMLLMLRLSTMRFXGB050100Percentage Ci of type iClassifierIEEE TRANSACTIONS ON SERVICES COMPUTING

12

Fig. 4. X-MOP: the guidelines for explainable AI purposes in OOPPM

the F Ccase value indicates that less than 1% of the predic-
tions change, despite the model using 118 case attributes.
We also observe that models with a higher value for Fcase
also have a lower value for M . Intuitively, the explainability
model
is worse in approximating the model behaviour
when the task model uses a lot of case-speciﬁc attributes.
This might explain the low M value for RFagg, as the
BPIC2011(2) event log is indicated by very long traces with
a high amount of static categorical values (>670) and RFagg
uses up to 100% of these case variables. As a result, this
means that the explainability model is better in approximat-
ing the importance ranking of the attributes used by the
task model when the model uses mostly control attributes
compared to case attributes. Finally, the monotonicity M of
the model is also high for a higher dynamic vs. static ratio
(as explained above), a higher average of activities per trace
and a higher number of event classes.

6.3 XMOP: Guidelines for XAI in OOPPM

The obtained insights with the use of the research questions
and event log analyses are summarized in an XAI-based

guidelines map for OOPPM Figure 4. This ﬁgure describes
the guidelines for Explainable AI purposes in OOPPM.
The white boxes are the questions to guide partitioners
and researchers in obtaining OOPPM results according to
their preferences in terms of predictive accuracy and inter-
pretability. The blue circles are the ﬁnal recommendations
for the sequence encoding and model complexity. Note that
the guidelines are designed in a very generic way, e.g., no
distinction between the LSTM and CNN based on dynamic
vs. static ratio. Nonetheless, we speciﬁcally recommend the
GLRM model when the interpretability of the model is very
important. Finally, the faithfulness metrics only need to be
calculated for non-transparent models.

7 CONCLUSION

This paper introduced a framework of metrics to evaluate
the explainability of predictive models used for OOPPM
purposes. Compared to performance-based metrics, they
take into account the event, case, and control perspec-
tive and describe the interpretability of the explanations and
the faithfulness of the explainability model. Furthermore, we

yesnoInformation losssequences?Aggregation-encoded GLRMyesnoHigh variant toinstance ratio?yesnoExplainability >>>performance?Embedded DLmodelX-MOP: A Guideline to Obtain  eXplainable Models for Outcome PredictionnoyesyesPost-hocexplainability?yesnoPost-hocexplainability?Aggregation-encodedtransparentmodelyesnoHigh interpretability>>>performance loss?Agg-encodedML modelCalculate explainability metricsInterpretability of explanationsCalculate explainability metricsInterpretability of explanationsIndex-encodedtransparentmodelCalculate explainability metricsInterpretability of explanationsCalculate explainability metricsInterpretability of explanationsFaithfulness of explainability modelIndex-encodedML model Calculate explainability metricsInterpretability of explanationsFaithfulness of explainability modelCalculate explainability metricsInterpretability of explanationsFaithfulness of explainability modelIEEE TRANSACTIONS ON SERVICES COMPUTING

13

provide an extensive benchmark study of seven models
and thirteen event logs. Finally, we provide the reader
with a consistent overview of the insights obtained by this
study in the ﬁeld of OOPPM, and created a framework of
guidelines contrasting traditional machine learning, deep
learning, preprocessing and explainability approaches to
guide the practitioner to the best model selection. Not only
can we conclude that the transparent GLRM model exhibits
very interpretable explanations for high dimensional and
sequential data (for only a small
loss of performance),
but also that the deep learning models only outperform
ML models that are index-encoded. Next, the aggregation
encoding is able to improve the explainability-performance
trade-off of the index-encoded variant, by obtained a better
performing model while offering better interpretability. To
conclude, we show that each model has its advantages and
disadvantages, where naively opting for the model with the
highest performance can have a strong detrimental effect on
both the interpretability of explanations and the faithfulness
of an explainability model. As a result, identifying faithful
explanations, while remaining interpretable, still imposes a
challenge for black box models.

One of the limitations of this research is the fact that the
metrics do not take the loss of information of the aggre-
gation encoding into account. Therefore, it is assumed that
the summary statistics of a certain variable contain no loss
in information compared to the original attribute values.
Future work will focus on the concept of Responsible AI,
by introducing causal inference to assess the causal insights
obtainable from predictive models, and focus on the creation
of fairness-aware decision models.

REFERENCES

[2]

[1] M. Dumas, M. L. Rosa, J. Mendling, and H. A. Reijers, Fundamen-
tals of Business Process Management, Second Edition. Springer, 2018.
I. Teinemaa, M. Dumas, M. L. Rosa, and F. M. Maggi, “Outcome-
oriented predictive process monitoring: Review and benchmark,”
ACM Trans. Knowl. Discov. Data, vol. 13, no. 2, pp. 17:1–17:57, 2019.
[3] A. Senderovich, C. Di Francescomarino, C. Ghidini, K. Jorbina, and
F. M. Maggi, “Intra and inter-case features in predictive process
monitoring: A tale of two dimensions,” in International Conference
on Business Process Management. Springer, 2017, pp. 306–323.
I. Verenich, M. Dumas, M. La Rosa, F. M. Maggi, and
C. Di Francescomarino, “Complex symbolic sequence clustering
and multiple classiﬁers for predictive process monitoring,” in
International Conference on Business Process Management. Springer,
2016, pp. 218–229.

[4]

[5] M. de Leoni, W. M. P. van der Aalst, and M. Dees, “A general
process mining framework for correlating, predicting and cluster-
ing dynamic behavior based on event logs,” Inf. Syst., vol. 56, pp.
235–257, 2016.

[6] W. Kratsch, J. Manderscheid, M. R ¨oglinger, and J. Seyfried, “Ma-
chine learning in business process monitoring: a comparison of
deep learning and classical approaches used for outcome predic-
tion,” Business & Information Systems Engineering, pp. 1–16, 2020.

[8]

[7] K. Heinrich, P. Zschech, C. Janiesch, and M. Bonin, “Process
data properties matter: Introducing gated convolutional neural
networks (gcnn) and key-value-predict attention networks (kvp)
for next event prediction with deep learning,” Decision Support
Systems, vol. 143, p. 113494, 2021.
J. Evermann, J.-R. Rehse, and P. Fettke, “Predicting process be-
haviour using deep learning,” Decision Support Systems, vol. 100,
pp. 129–140, 2017.
S. Pauwels and T. Calders, “Bayesian network based predictions of
business processes,” in International Conference on Business Process
Management. Springer, 2020, pp. 159–175.

[9]

[10] A. B. Arrieta, N. D. Rodr´ıguez, J. D. Ser, A. Bennetot, S. Tabik,
A. Barbado, S. Garc´ıa, S. Gil-Lopez, D. Molina, R. Benjamins,
R. Chatila, and F. Herrera, “Explainable artiﬁcial
intelligence
(XAI): concepts, taxonomies, opportunities and challenges toward
responsible AI,” Inf. Fusion, vol. 58, pp. 82–115, 2020.

[11] C. Rudin, “Stop explaining black box machine learning models
for high stakes decisions and use interpretable models instead,”
Nature Machine Intelligence, vol. 1, no. 5, pp. 206–215, 2019.

[12] A. F. Markus, J. A. Kors, and P. R. Rijnbeek, “The role of explain-
ability in creating trustworthy artiﬁcial intelligence for health care:
A comprehensive survey of the terminology, design choices, and
evaluation strategies,” J. Biomed. Informatics, vol. 113, p. 103655,
2021.

[13] J. Zhou, A. H. Gandomi, F. Chen, and A. Holzinger, “Evaluating
the quality of machine learning explanations: A survey on meth-
ods and metrics,” Electronics, vol. 10, no. 5, p. 593, 2021.

[14] C. Rudin and B. Ustun, “Optimized scoring systems: Toward trust
in machine learning for healthcare and criminal justice,” Interfaces,
vol. 48, no. 5, pp. 449–466, 2018.

[15] S. Serrano and N. A. Smith, “Is attention interpretable?”
the Association
in Proceedings of
for Computational Linguistics.
Florence, Italy: Association for
Computational Linguistics, Jul. 2019, pp. 2931–2951. [Online].
Available: https://aclanthology.org/P19-1282

the 57th Annual Meeting of

[16] I. Nunes and D. Jannach, “A systematic review and taxonomy of
explanations in decision support and recommender systems,” User
Modeling and User-Adapted Interaction, vol. 27, no. 3, pp. 393–444,
2017.

IEEE, 2021, pp. 112–119.

[17] V. Pasquadibisceglie, G. Castellano, A. Appice, and D. Malerba,
“Fox: a neuro-fuzzy model for process outcome prediction and
explanation,” in 2021 3rd International Conference on Process Mining
(ICPM).
[18] A. Stevens,

J. D. Smedt, and J. Peeperkorn, “Quantifying
explainability in outcome-oriented predictive process monitor-
ing,” in Process Mining Workshops -
ICPM 2021 International
Workshops, Eindhoven, The Netherlands, October 31 - November
4, 2021, Revised Selected Papers, ser. Lecture Notes in Business
J. Munoz-Gama and X. Lu, Eds.,
Information Processing,
vol. 433.
[Online]. Available:
https://doi.org/10.1007/978-3-030-98581-3 15

Springer, 2021, pp. 194–206.

[19] M. Velmurugan, C. Ouyang, C. Moreira, and R. Sindhgatta, “Eval-
uating ﬁdelity of explainable methods for predictive process ana-
lytics,” in International Conference on Advanced Information Systems
Engineering. Springer, 2021, pp. 64–72.

[20] N. Tax, I. Verenich, M. L. Rosa, and M. Dumas, “Predictive busi-
ness process monitoring with LSTM neural networks,” in CAiSE,
ser. Lecture Notes in Computer Science, vol. 10253.
Springer,
2017, pp. 477–492.

[21] E. Rama-Maneiro, J. C. Vidal, and M. Lama, “Deep learning for
predictive business process monitoring: Review and benchmark,”
CoRR, vol. abs/2009.13251, 2020.

[22] R. Sindhgatta, C. Ouyang, and C. Moreira, “Exploring inter-
pretability for predictive process analytics,” in ICSOC, ser. Lecture
Springer, 2020, pp. 439–
Notes in Computer Science, vol. 12571.
447.

[23] G. El-khawaga, M. Abu-Elkheir, and M. Reichert, “Xai in the
context of predictive process monitoring: An empirical analysis
framework,” Algorithms, vol. 15, no. 6, p. 199, 2022.

[24] S. M. Lundberg and S. Lee, “A uniﬁed approach to interpreting
model predictions,” in Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon,
U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N.
Vishwanathan, and R. Garnett, Eds., 2017, pp. 4765–4774.

[25] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should I trust
you?”: Explaining the predictions of any classiﬁer,” in Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, San Francisco, CA, USA, August 13-17,
2016, B. Krishnapuram, M. Shah, A. J. Smola, C. C. Aggarwal,
D. Shen, and R. Rastogi, Eds. ACM, 2016, pp. 1135–1144.
[Online]. Available: https://doi.org/10.1145/2939672.2939778

[26] R. Galanti, B. Coma-Puig, M. de Leoni,

J. Carmona, and
N. Navarin, “Explainable predictive process monitoring,” in
ICPM.

IEEE, 2020, pp. 1–8.

[27] N. Mehdiyev and P. Fettke, “Explainable artiﬁcial intelligence for
process mining: A general overview and application of a novel
local explanation approach for predictive process monitoring,” In-

IEEE TRANSACTIONS ON SERVICES COMPUTING

14

terpretable Artiﬁcial Intelligence: A Perspective of Granular Computing,
pp. 1–28, 2021.

Transactions of the Association for Computational Linguistics, vol. 4,
pp. 259–272, 2016.

[51] C. Strobl, A.-L. Boulesteix, A. Zeileis, and T. Hothorn, “Bias in
random forest variable importance measures: Illustrations, sources
and a solution,” BMC bioinformatics, vol. 8, no. 1, pp. 1–21, 2007.

[52] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Ka-
gal, “Explaining explanations: An overview of interpretability of
machine learning,” in DSAA.

IEEE, 2018, pp. 80–89.

[28] J. Wang, D. Yu, C. Liu, and X. Sun, “Outcome-oriented predictive
process monitoring with attention-based bidirectional lstm neural
networks,” in 2019 IEEE International Conference on Web Services
(ICWS).

IEEE, 2019, pp. 360–367.

[29] R. Sindhgatta, C. Moreira, C. Ouyang, and A. Barros, “Exploring
interpretable predictive models for business processes,” in BPM,
ser. Lecture Notes in Computer Science, vol. 12168.
Springer,
2020, pp. 257–272.

[30] M. Harl, S. Weinzierl, M. Stierle, and M. Matzner, “Explainable
predictive business process monitoring using gated graph neural
networks,” Journal of Decision Systems, pp. 1–16, 2020.

[31] C. Molnar, G. Casalicchio, and B. Bischl, “Quantifying model
complexity via functional decomposition for better post-hoc inter-
pretability,” in PKDD/ECML Workshops (1), ser. Communications
in Computer and Information Science, vol. 1167.
Springer, 2019,
pp. 193–204.

[32] S. R. Islam, W. Eberle, and S. K. Ghafoor, “Towards quantiﬁcation
of explainability in explainable artiﬁcial intelligence methods,” in
Proceedings of the Thirty-Third International Florida Artiﬁcial Intelli-
gence Research Society Conference, May 17-20, 2020, R. Bart´ak and
E. Bell, Eds. AAAI Press, 2020, pp. 75–81.

[33] A. Nguyen and M. R. Mart´ınez, “On quantitative aspects of model

interpretability,” CoRR, vol. abs/2007.07584, 2020.

[34] S. Jain and B. C. Wallace, “Attention is not explanation,” arXiv

preprint arXiv:1902.10186, 2019.

[35] S. Ma and R. Tourani, “Predictive and causal implications of using
shapley value for model interpretation,” in Proceedings of the 2020
KDD Workshop on Causal Discovery. PMLR, 2020, pp. 23–38.
[36] C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and
C. Zhong, “Interpretable machine learning: Fundamental princi-
ples and 10 grand challenges,” CoRR, vol. abs/2103.11251, 2021.

[37] W. M. P. van der Aalst, Process Mining - Data Science in Action,

Second Edition. Springer, 2016.

[38] B. F. van Dongen, R. A. Crooy, and W. M. van der Aalst, “Cycle
time prediction: When will this case ﬁnally be ﬁnished?” in OTM
Confederated International Conferences” On the Move to Meaningful
Internet Systems”. Springer, 2008, pp. 319–336.

[39] F. M. Maggi, C. D. Francescomarino, M. Dumas, and C. Ghidini,
“Predictive monitoring of business processes,” in CAiSE, ser. Lec-
ture Notes in Computer Science, vol. 8484.
Springer, 2014, pp.
457–472.

[40] C. Di Francescomarino, M. Dumas, F. M. Maggi, and I. Teinemaa,
“Clustering-based predictive process monitoring,” IEEE transac-
tions on services computing, vol. 12, no. 6, pp. 896–909, 2016.
[41] G. T. Lakshmanan, S. Duan, P. T. Keyser, F. Curbera, and R. Khalaf,
“Predictive analytics for semi-structured case oriented business
processes,” in Business Process Management Workshops, ser. Lecture
Notes in Business Information Processing, vol. 66. Springer, 2010,
pp. 640–651.

[42] G. A. Miller, “The magical number seven, plus or minus two: Some
limits on our capacity for processing information.” Psychological
review, vol. 63, no. 2, p. 81, 1956.

[43] A. Ghorbani, A. Abid, and J. Zou, “Interpretation of neural net-
works is fragile,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 33, 2019, pp. 3681–3688.

[44] H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec,
“Interpretable & explorable approximations of black box
models,” CoRR, vol. abs/1707.01154, 2017. [Online]. Available:
http://arxiv.org/abs/1707.01154

[45] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting

system,” CoRR, vol. abs/1603.02754, 2016.

[46] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.

5–32, 2001.

[47] H. Weytjens and J. D. Weerdt, “Process outcome prediction: Cnn
vs. lstm (with attention),” in International Conference on Business
Process Management. Springer, 2020, pp. 321–333.

[48] V. Pasquadibisceglie, A. Appice, G. Castellano, and D. Malerba,
“Using convolutional neural networks for predictive process an-
alytics,” in 2019 international conference on process mining (ICPM).
IEEE, 2019, pp. 129–136.

[49] E. Obodoekwe, X. Fang, and K. Lu, “Convolutional neural net-
works in process mining and data analytics for prediction accu-
racy,” Electronics, vol. 11, no. 14, p. 2128, 2022.

[50] W. Yin, H. Sch ¨utze, B. Xiang, and B. Zhou, “Abcnn: Attention-
based convolutional neural network for modeling sentence pairs,”

