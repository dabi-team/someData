2
2
0
2

r
p
A
6
2

]
E
S
.
s
c
[

1
v
6
1
9
2
1
.
4
0
2
2
:
v
i
X
r
a

GypSum: Learning Hybrid Representations for Code
Summarization

Yu Wang
wangyu@stu.ecnu.edu.cn
East China Normal University
Shanghai, China

Xuesong Lu‚àó
xslu@dase.ecnu.edu.cn
East China Normal University
Shanghai, China

Yu Dong
yudong@stu.ecnu.edu.cn
East China Normal University
Shanghai, China

Aoying Zhou
ayzhou@dase.ecnu.edu.cn
East China Normal University
Shanghai, China

ABSTRACT
Code summarization with deep learning has been widely studied
in recent years. Current deep learning models for code summariza-
tion generally follow the principle in neural machine translation
and adopt the encoder-decoder framework, where the encoder
learns the semantic representations from source code and the de-
coder transforms the learnt representations into human-readable
text that describes the functionality of code snippets. Despite they
achieve the new state-of-the-art performance, we notice that cur-
rent models often either generate less fluent summaries, or fail to
capture the core functionality, since they usually focus on a single
type of code representations. As such we propose GypSum, a new
deep learning model that learns hybrid representations using graph
attention neural networks and a pre-trained programming and nat-
ural language model. We introduce particular edges related to the
control flow of a code snippet into the abstract syntax tree for graph
construction, and design two encoders to learn from the graph and
the token sequence of source code, respectively. We modify the
encoder-decoder sublayer in the Transformer‚Äôs decoder to fuse the
representations and propose a dual-copy mechanism to facilitate
summary generation. Experimental results demonstrate the supe-
rior performance of GypSum over existing code summarization
models.

CCS CONCEPTS
‚Ä¢ Computing methodologies ‚Üí Neural networks; ‚Ä¢ Software
and its engineering ‚Üí Documentation.

KEYWORDS
code summarization, deep neural networks, graph attention neural
networks, copy mechanisms

‚àóCorresponding author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3527903

ACM Reference Format:
Yu Wang, Yu Dong, Xuesong Lu, and Aoying Zhou. 2022. GypSum: Learn-
ing Hybrid Representations for Code Summarization. In 30th International
Conference on Program Comprehension (ICPC ‚Äô22), May 16‚Äì17, 2022, Virtual
Event, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
3524610.3527903

1 INTRODUCTION
Source code documentation is particularly important in software
engineering since it facilitates software development, bug fixing
and software maintenance [14, 45, 46]. However, writing docu-
mentation takes time and is usually postponed by the developers
towards the end of the project, only if time permits. To help with the
documentation task, recent works have investigated the possibility
of automatically generating a piece of readable description for a
function, which is referred to as the ‚Äúcode summarization‚Äù task,
using deep learning techniques [1, 21, 22, 40, 42].

Current deep learning models for code summarization usually
adopt the encoder-decoder framework and vary the detailed struc-
ture of the two components. The encoder converts a code snippet
into latent representations, based on which the decoder outputs
a natural language summary describing the effect of the snippet.
For example, inspired by Neural Machine Translation [7, 37], Hu
et al. [21] utilize the sequence-to-sequence architecture for code
summarization, where the LSTM encoder learns the latent code
representations from the serialized abstract syntax tree (AST) and
the LSTM decoder produces the summary using the attention mech-
anism. Other works either design a particular encoder/decoder to
leverage a specific input form of source code, i.e., the AST or token
sequence [25, 26, 35, 44], or propose specific learning objectives to
facilitate model training [22, 40, 42]. In our study, we observe that
the latent representations learned from different code forms may
capture quite different semantic information, resulting in varying
behaviors in the decoded summaries. For instance, Table 1 shows
a function and the corresponding summaries generated by three
models. The ‚ÄúTransformer‚Äù model [1] uses the Transformer‚Äôs en-
coder to learn from the text token sequence of a snippet, whereas
the ‚ÄúDeepCom‚Äù model [21] uses an LSTM encoder to learn from the
flattened AST. We observe that the summary generated by Trans-
former is more detailed and fluent, but fails to describe the core
effect of the function. It says the function adjusts the column width
while the ground-truth is adjusting the row heights. Moreover, the
text for the specified table and column names makes little sense. On

 
 
 
 
 
 
ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

Table 1: A motivating example showing the varying be-
haviors of summaries generated using different representa-
tions.

on any pre-trained PL-NL model, GypSum can still achieve the best
performance compared with existing models.

private void adjustRowHeights( Jtable table ) {

for(int row=num; row<table.getRowCount(); row++){

int rowHeight = table . getRowHeight();
for(int column=num; column<table.getColumnCount(); column++){

component comp = table.prepareenderer ( table . getCellenderer (row,

column), row, column);

rowHeight = math.max(rowHeight, comp.getPreferredSize () . height ) ;

}
table . setRowHeight(row, rowHeight);

}

}
Ground-Truth: adjust the row heights of a table based on the table
contents.
GypSum: adjust the row heights of the table based on the preferred size.
Transformer: adjust the column width of the table for the specified table
and column names .
DeepCom: adjust column height based on contents.

the other hand, although the summary generated by DeepCom
describes the functionality closer to the ground-truth, it is too brief
and grammatically incorrect.

Based on the observation, we conjecture that the representations
learnt from token sequences preserve better the naturalness of the
language and can benefit text generation, whereas the represen-
tations learnt from AST structures capture better the functional
information, thereby improving the informativeness of the summary.
If the conjecture is correct, learning simultaneously the representa-
tions from the token sequences and AST structures should benefit
the generation of summaries that are both natural and informative.
This motivates us to propose ‚ÄúGypSum‚Äù, a new deep model for
code summarization, which leverages two encoders to learn hybrid
latent representations from source code. Although several previous
studies [19, 25, 35] have investigated the similar idea, we show that
GypSum not only achieves the new state-of-the-art performance,
but also is more interpretable when generating the summaries (See
Section 4.6.1). In the example of Table 1, the summary generated by
GypSum is fluent and correctly describes the functionality of the
code. GypSum consists of two encoders and one decoder. The two
encoders leverage graph attention neural networks [27] (GAT) and
a pre-trained model for programming language and natural lan-
guage [17] (PL-NL), respectively, to learn the latent representations
from the AST-base graphs and token sequences of code snippets.
The intuition is to use the attention mechanism in GAT to capture
the key functional nodes in ASTs. The two types of representa-
tions are fused into hybrid representations in the decoder, and a
dual-copy mechanism is proposed to facilitate summary generation.
Besides the model design, our another contribution is the proposal
of introducing semantic edges pertaining to control flows for graph
construction, inspired by the control flow graph of a program.

In the experimental section, we show GypSum outperforms ex-
isting representative code summarization models by a fairly large
margin on two commonly used datasets. We also conduct ablation
study, case study and user study to justify the architecture choice
and the performance of GypSum. Particularly, even without relying

2 RELATED WORK
Recent works on code summarization have mainly used the encoder-
decoder framework. Iyer et al. [23] first use LSTM networks with
attention to produce sentences that describe C# code snippets and
SQL queries. Hu et al. [21] propose a sequence-to-sequence model
to learn from the ASTs. They propose a structure-based traversal
method to flatten the ASTs into token sequences. Following this
work, they propose to learn from API sequences extracted from
source code, and use the learned API knowledge to enhance the
performance [22]. Wei et al. [42] investigate the duality between
the code generation task and the code summarization task. They
use two sequence-to-sequence neural networks with attentions
to train the two tasks simultaneously. Ahmad et al. [1] show that
the quality of the summaries can be greatly improved by carefully
implementing the Transformer structure. They replace the original
positional encoding in the Transformer with the relative positional
encoding, which encodes the pairwise relationship between the
tokens in the token sequence. As an improvement to Transformer,
Gao et al. [18] propose to exploit code structural property and in-
troduce constraints to the multi-head self-attention module. Zhang
et al. [47] and Wei et al. [43] both propose a retrieval-based method,
which improves summary generation by leveraging the comments
of similar code snippets. Shi et al. [35] propose to split an AST into
a set of subtrees and devise a recursive neural network to encode
the subtrees. The encodings are then aggregated for generating the
summary. Choi et al. [11] apply graph convolutions to obtain node
representations from an AST and then input the sequence of node
representations into the Transformer layers for code summarization.
Wu et al. [44] construct a multi-view adjacent matrix to represent
the relationships between the tokens in source code, and use it to
guide the self-attention computation in Transformer. Differently
from above work, Wan et al. [40] discard the decoder structure and
employ deep reinforcement learning networks for summary gen-
eration. They design an encoder consisting of an ordinary LSTM
and a tree-based LSTM, which learn from serialized code snippets
and binary trees transformed from ASTs. Other related work in-
clude [4, 10, 25, 36]. Despite the great success, existing studies have
not sufficiently explored the potentials of learning hybrid represen-
tations from the token sequence and the abstract syntax tree. We
show that with the careful design of the model architecture, this
simple idea can indeed outperform existing methods.

3 THE ARCHITECTURE OF GYPSUM
Gypsum consists of two encoders (c-encoder and g-encoder) and
one decoder. The encodings are fused in the decoder for summary
generation. We also propose a dual copy mechanism to enhance
the token generation process. The overall architecture is depicted
in Figure 1.

3.1 c-encoder: Encoding with Token Sequences
The source code snippets preserve the human-injected naturalness
of programming languages, i.e., they express certain semantics by
following particular lexical and syntactic rules. Therefore we may

GypSum: Learning Hybrid Representations for Code Summarization

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Figure 1: The architecture of the GypSum Model. The two encoders on the left and right part learn from token sequences
and AST-based graphs, respectively. The decoder in the middle fuses the two types of learnt representations and outputs the
summary.

simply convert a code snippet into a token sequence and learn the
contextual representations of the tokens using NLP models. As our
ultimate goal is to translate a code snippet into human-readable
description, the representations should preserve the naturalness
of programming language and also facilitate natural language gen-
eration. As such, a practical solution is to take the advantage of
recently developed pre-trained models for programming and natu-
ral language (PL-NL). Two representative PL-NL pre-trained models
are CodeBERT [17] and CodeT5 [41]. We experimented with both
and found CodeBERT produces slightly better results in our study.
As such, we adopt CodeBERT to encode with the token sequences.
It is worth noting that, experimental results in the ablation study
(see Section 4.4.3) show our general solution outperforms existing
methods even if we do not make use of any PL-NL pre-trained
model.

CodeBERT is based on Roberta [29] and trained using a hybrid
objective function that incorporates the task of masked language
modeling (MLM) [15] and replaced token detection (RTD) [12]. In
the former task, the input is an individual function with the paired
human-written documentation. During training, the input tokens
are randomly masked and the task is to predict the masked tokens.
In the latter task, the training phase employs a PL generator and
an NL generator to recover the masked tokens in the function
and the documentation 1, respectively, and trains CodeBERT as a
discriminator to classify each token as ‚Äòoriginal‚Äô or ‚Äòreplaced‚Äô.

We obtain a pre-trained CodeBERT model from the official reposi-
tory2 and tune it on our datasets. We feed the tokens into CodeBERT
to obtain the embeddings. We add an additional linear layer on top

1In the task of RTD, the function and the documentation do not have to be paired.
2https://github.com/microsoft/CodeBERT

of its output layer and transform the embeddings so that they com-
ply with the representations of the other encoder. Denoted by ùëôùëê ,
ùëëùëöùëúùëëùëíùëô , ùëëùëí the length of the token sequence, the embedding size of
each layer in CodeBERT and the final output embedding size of the
encoder, the encoding process with CodeBERT can be formulated
as follows,

ÀúHùëê = CodeBERT(C)
Hùëê = W ùëê ¬∑ ÀúHùëê,
(2)
where C is the input sequence of the code tokens, ÀúHùëê ‚àà Rùëëùëöùëúùëëùëíùëô √óùëôùëê
denotes the embedding matrix generated by CodeBERT,W ùëê ‚àà
Rùëëùëí √óùëëùëöùëúùëëùëíùëô denotes the parameters of the linear layer, and Hùëê ‚àà
Rùëëùëí √óùëôùëê denotes the final hidden representations. We refer to this
encoder as c-encoder, which is depicted in the left part of Figure 1.

(1)

3.2 g-encoder: Encoding with Graphs
The intermediate representations (IR) such as abstract syntax tree
(AST) of programs are regularly used for code representation learn-
ing. These IRs are proven to reflect common syntactic and semantic
patterns of code and lower the training cost [5, 6, 33]. There exist
diverse approaches to leverage the ASTs, among which an effective
way to further enrich the semantic structure of an AST is adding
semantic edges between the nodes [3, 13]. Inspired by the control
flow graph (CFG) of a program, we propose a new type of semantic
edges for connecting AST nodes, namely Control-Edge, which
reflect the flow of control statements such as if-else and while. Fur-
thermore, we propose to use graph attention neural networks [39]
to learn from the constructed semantic graphs, in order to better
capture the key elements in the graphs using attentions. We refer
to this encoder as g-encoder, which is depicted in the right part of
Figure 1.

G-Encoder GraphFuncDefPublicSimpleTypeVariableDeclarationIntegerSimpleTypeNameString‚Ä¶ASTAdd&NormFeedForwardAdd&NormHybrid AttentionC-Enc-DecAttentionG-Enc-DecAttentionAdd&NormSelfAttentionlayer l‚Ä¶MLP+Dual-CopyDecoder‚Ä¶C-Encoder Self-AttentionSelf-AttentionSelf-AttentionSelf-Attention‚Ä¶TokenSequenceSourceCodeSourceCodeSummarytokenizetokenizeparseICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

3.2.1 Extending the ASTs. To better illustrate how we construct
the semantic graphs, we use the code snippet in Figure 2(a) as the
running example. To generate the ASTs from source code, we use
javalang3 for Java code and use asttokens4 plus ast5 for Python code.
In the constructed ASTs, the internal nodes are labeled with their
class name such as MethodDeclaration and StatementExp, and
the leaf nodes are labeled with the text of their attributes which
correspond to the tokens in the source code.

We conduct two extensions to the ASTs generated by the tools.
First, in order to share the vocabulary with c-encoder, we use the
same tokenizer for the two encoders. As such, for both original
Java and Python ASTs, the text in some leaf nodes are further
tokenized into sub-words. For instance, ‚Äònum_a‚Äô in the example
code in Figure 2(a) is tokenized into ‚Äònum‚Äô,‚Äò_‚Äô and ‚Äòa‚Äô. For such a leaf
node, we replace it with a special node _SplitNode_ and make the
sub-words as its children, forming the new leaf nodes. Second, for
Python ASTs, some leaf nodes generated by the tools lose important
attributes such as ‚Äòargs‚Äô and ‚Äòname‚Äô, which correspond to variables
and function names, respectively. To mitigate the problem, we
manually fetch the text of some important attributes and compose
the additional leaf nodes to augment the original Python ASTs.

3.2.2 Graph Construction. Following [3, 9, 34], we first introduce
four commonly used types of semantic edges into ASTs, namely,
Child-Edge, Next-Sibling-Edge, Next-Use-Edge and Next-To
ken-Edge. Child-Edges are the original edges in ASTs plus the
new edges obtained because of node splitting and augmentation.
Next-Sibling-Edges connect the sibling nodes in ASTs from left to
right. Next-Use-Edges connect a variable node to the next node of
the same variable with depth-first search. Lastly, Next-Token-Edges
connect the leaf nodes from left to right so that the corresponding
tokens comply with the order that they appear in the source code.
The edges can be observed in Figure 2(b) and Figure 2(c), where
Figure 2(c) shows the detailed structure of the subtree inside the
dotted rectangle of Figure 2(b).

In addition to the above edges, we borrow the idea from the
control flow graph and propose to introduce the Control-Edges
into ASTs, so that the data and control flow of a program could
be better captured. In Figure 3, we show the AST subtrees of four
common control statements, i.e., if-else, switch, while and for. Take
example of the if-else subtree in the top-left corner, BinaryOp,
StatementExp and IfStatement correspond to the if condition,
the statements in the main branch, and the else branch. Since there
can be flows from BinaryOp to StatementExp (i.e., the if condition
is true) and from BinaryOp to IfStatement (i.e., the if condition is
false), we initiate a Control-Edge from BinaryOp to Statement-
Exp and from BinaryOp to IfStatement, respectively. Also, once
the statements in StatementExp are executed, the flow should
enter the next statement outside the if block. As such, we initi-
ate a Control-Edge from StatementExp to the next sibling (The
NextSibling node stands for any possible sibling node.) of its
parent node. The Control-Edges for the other types of control
statements are constructed using the similar principle.

(a) A snippet of Java code from a novice programmer.

(b) The full AST with induced edges of the above code, where the sub-tree in
dashed red rectangle is shown in Figure 2(c)

(c) Simplified semantic graph for line 2 of the above code, corresponding to the
sub-tree in the red rectangle in Figure 2(b)

Figure 2: An example code snippet and its semantic graph.

Figure 3: The Control-Edges for four types of control state-
ments.

3https://github.com/c2nes/javalang
4https://github.com/gristlabs/asttokens
5https://github.com/att/ast

For all types of edges, we introduce their corresponding reversed
edges to enrich the semantic. Note that there may be multiple types

GypSum: Learning Hybrid Representations for Code Summarization

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

of edges between two nodes. In such a case, we make use of them
all in the computation of the model.

3.2.3 Encoding with Graph Attention Neural Networks. The Graph
Attention Neural Networks [39] (GAT) recurrently update the state
of a node by aggregating its own state and the states of all its
neighbours at each step, using the attention mechanism. Denote by
eùëñ and eùëñ ùëó , the embedding of node ùëñ and the embedding of the edge
from node ùëó to node ùëñ. GAT updates the node states as follows:

(0)
h
ùëñ

= eùëñ

ùë† (ùëô)
ùëñ ùëó = LeakyReLU(ùëæ (ùëô) ¬∑ (ùëæ h

(ùëô)
ùëñ ‚äï ùëæ h

(ùëô)
ùëó ‚äï ùëæùëí eùëñ ùëó ))

ùëé (ùëô)
ùëñ ùëó =

(ùëô+1)
h
ùëñ

=

ùëíùë•ùëù (ùë† (ùëô)
ùëñ ùëó )
(cid:205)ùëò ‚ààN (ùëñ) ùëíùë•ùëù (ùë† (ùëô)
ùëñùëò )
‚àëÔ∏Å
(ùëô)
ùúé (ùëé (ùëô)
ùëó
ùëñ ùëó

¬∑ h

),

ùëó ‚ààN (ùëñ)

(3)

(4)

(5)

(6)

(ùëô)
where h
denotes the state of node ùëñ at layer ùëô, ùëæ and ùëæùëí are the
ùëñ
shared weight matrices for nodes and edges, ùëæ (ùëô) is the weight
matrix at layer ùëô, ùëé (ùëô)
ùëñ ùëó denotes the attention weight of contribution
from node ùëó to ùëñ at layer ùëô. N (ùëñ) denotes the neighour nodes of node
ùëñ.

We employ multi-head for the attention computation. The node
states of the last GAT layer compose the output matrix Hùëî of g-
encoder.

3.3 The Fusion Decoder
To fuse the two types of representations output by the two encoders,
we design a fusion mechanism in the decoder to combine them and
obtain the hybrid latent representations for decoding. We use the
Transformer‚Äôs decoder as the basic structure and retain the original
hyperparameter settings. However, in the encoder-decoder atten-
tion sublayer, we apply the attentions of the decoder‚Äôs hidden states
to the output of each encoder, respectively, and then concatenate
the output states of the two attention modules for further process.
This is depicted in the ‚Äúhybrid attention‚Äù module in Figure 1. In
particular, given the output embeddings Hùëê of the c-encoder and
ùëô
ùëô
ùëô
Hùëî of the g-encoder and let Dùëô
ùë° ‚àí1} denote the
2, . . . , d
1, d
ùë° = {d
0
hidden states of the decoder‚Äôs ùëôùë°‚Ñé layer before step ùë° (note that D
are the embeddings of the target summary), we can formulate the
decoding process at step ùë° of layer ùëô as follows,

3.4 The Dual-Copy Mechanism
To better improve the ability of capturing keywords and solving the
out-of-vocabulary problem, we propose a dual-copy mechanism
to copy tokens from the two encoders. For the final hidden vector
dùë° ‚àà Rùëëùëí at position ùë° output by the decoder, we first compute the
probability vector for choosing generation or copying as follows:

[ùëùùë£, ùëùùëê, ùëùùëî] = Softmax(Wùëîùëíùëõ ¬∑ dùë° ),
where ùëùùë£, ùëùùëê and ùëùùëî are the probability for sampling a token from
the vocabulary, copying from the input tokens of the c-encoder,
and copying from the leaf nodes of the g-encoder, respectively.
Wùëîùëíùëõ ‚àà R3√óùëëùëí denotes the parameter matrix. Softmax denotes
the softmax function.

(10)

Then we compute the sampling distribution for generation and
copying. For sampling from the vocabulary, the probability distri-
bution is directly transformed from dùë° as follows:

pùë£ùëúùëê = Softmax(W ùë£ùëúùëê ¬∑ dùë° ),
where W ùë£ùëúùëê ‚àà R |ùë£ùëúùëêùëéùëè |√óùëëùëí denotes the parameter matrix. For copy-
ing, we can naturally use the last layer‚Äôs attention weights of dùë°
to all the encodings produced by each encoder, respectively, as the
copy distribution, which can be formulated as follows:

(11)

aùëê = Attn_Scoreùë°
ùëê,
aùëî = Attn_Scoreùë°
ùëî,
where aùëê and aùëî are the probability distribution of copying from the
input source code tokens and the leaf nodes, respectively. Attn_
Scoreùë°
ùëî denote the attention weights of dùë° to all
the encodings of the c-encoder and the g-encoder, respectively, in
the last decoder layer.

ùëê and Attn_Scoreùë°

(12)

Finally, the probability of producing a token ùë¶ùë° in the summary

at position ùë° is calculated as the joint probability:

ùëù (ùë¶ùë° ) = ùëùùë£ ‚àó ùëùùë£ùëúùëê (ùë¶ùë° ) + ùëùùëê ‚àó

‚àëÔ∏Å

ùëéùëñ
ùëê + ùëùùëî ‚àó

‚àëÔ∏Å

ùëéùëñ
ùëî,

ùëêùëñ =ùë¶ùë°

ùëîùëñ =ùë¶ùë°

(13)

where ùëêùëñ and ùëîùëñ denote the ùëñùë°‚Ñé source code token and the ùëñùë°‚Ñé leaf
ùëê denotes the copy probability of the ùëñùë°‚Ñé token.
node, respectively. ùëéùëñ

3.5 The Objective Function
We use the negative log-likelihood as the objective function and
attempt to minimize it during model training. Given a source code
snippet ùëãùëñ and target summary ùëåùëñ = {ùë¶ùëñ
ùëá }, the objective
function can be formulated as follows,

2, . . . , ùë¶ùëñ

1, ùë¶ùëñ

ùëô
Àú
ùëô‚àí1
ùë° = attnùë† (d
d
ùë°

, D

ùëô‚àí1
ùë° +1, D

ùëô
ùë° = [attnùëê (
attn

Àú
ùëô
ùë° , Hùëê, Hùëê ), attnùëî (
d
ùëô
ùëì ùëì ¬∑ tanh(attn
ùë° ),

ùëô
ùë° = W
d

ùëô‚àí1
ùë° +1),
Àú
ùëô
ùë° , Hùëî, Hùëî)],
d

L = ‚àí

1
ùëÅ

ùëÅ
‚àëÔ∏Å

ùëá
‚àëÔ∏Å

ùëôùëúùëî[ùëù (ùë¶ùëñ

ùë° )]

(14)

ùë° =1
where ùëÅ is the number of data points in the training data, ùëá is the
length of the target summary.

ùëñ=1

(7)

(8)

(9)

where attnùë† (¬∑) denotes the self-attention sublayer, and attnùëê (¬∑)
and attnùëî (¬∑) denote the two attention modules in the encoder-
decoder sublayer. The output of the two attention modules are
concatenated and activated using Tanh. Finally, W ùëì ùëì are the pa-
ùëô
rameters of the feed forward sublayer and d
ùë° is the encoding at
position ùë° of layer ùëô.

4 PERFORMANCE EVALUATION
In this section, we evaluate GypSum and existing representative
models for code summarization. We implement GypSum using
Pytorch 1.7.0 and run all the experiments on a Tesla P40 GPU. All
source code is available at https://github.com/ICPC2022-Gypsum/
GypSum_Code.

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

4.1 The Datasets and Evaluation Metrics
We use two frequently used public datasets in the experiments.
One is a Java dataset [21] containing 87, 136 Java code snippets
with comments written by the developers. The other is a Python
dataset [21, 40] containing 87, 226 Python code snippets with com-
ments. For fair comparison, we use the divided datasets in previous
work [1, 18], where the proportions of training, validation and test-
ing set are 8:1:1 and 6:2:2, respectively, for the Java and Python
dataset.

We adopt three commonly used metrics for evaluating the per-
formance of code summarization models, namely, BLEU [30], ME-
TEOR [8] and ROUGE-L [28]. The score of each metric on a testing
set is computed as the average score of all the generated summaries.
For all the metrics, a higher score indicates better performance.

4.2 The Comparative Models
We compare GypSum with eleven representative models. Except
Rencos, CodeT5 and CodeBERT, we cite the numbers in their origi-
nal papers since they use the same datasets as ours. We train Rencos,
CodeT5 and CodeBERT on our datasets because they use different
datasets in the original papers.

‚Ä¢ RL+HybridSeq [40]. The model uses the LSTM-based en-
coder to learn from code snippets and binary trees trans-
formed from ASTs. Then it uses the output of the encoder as
the initial state and adopts reinforcement learning for code
summary generation.

‚Ä¢ DeepCom [21]. The model uses an LSTM-based encoder-
decoder architecture to learn from the flattened ASTs. A
structure-based traversal method is proposed to traverse an
AST and convert it into a sequence of tokens.

‚Ä¢ API+CODE [22]. The model learns the API knowledge from
API sequences extracted from source code, and uses the
learned API features to enhance the performance of code
summarization.

‚Ä¢ Dual Model [42]. The model investigates the duality be-
tween the code generation task and the code summarization
task, and uses two sequence-to-sequence networks with at-
tentions to train the two tasks simultaneously.

‚Ä¢ Transformer [1]. The model takes the Transformer struc-
ture and replaces the original positional encoding with the
relative positional encoding, which encodes the pairwise
relationships between the tokens in the source code text.
‚Ä¢ Rencos [47]. The model first trains an attention-based encoder-
decoder network using code and summaries. Then at infer-
ence time, it encodes a code snippet as well as two other
most similar ones and fuses the encodings for code summary.
‚Ä¢ CAST [35]. The model hierarchically splits an AST into a
set of subtrees and devises a recursive neural network to
encode the subtrees. The encodings are then aggregated for
generating the summary.

‚Ä¢ mAST+GCN [11]. The model applies graph convolutions to
obtain node representations from an AST and then inputs
the sequence of node representations into the Transformer
layers for code summarization.

‚Ä¢ SiT [44]. The model constructs a multi-view adjacent matrix
to represent the relationships between the tokens in source

code, and uses it to guide the self-attention computation in
Transformer.

‚Ä¢ CodeT5 [41]. It is a pre-trained encoder-decoder model
based on T5 [32] for programming and natural Language.
We directly tune it with our datasets for code summarization.
‚Ä¢ CodeBERT [17]. It is a pre-trained encoder model based on
Roberta [29] for programming and natural Language. We
add the Transformer decoder and tune the entire model with
our datasets.

4.3 The Hyperparameter Setting
We report the hyperparameters of GypSum used in our experiments
in Table 2. The output embedding size of the two encoders ùëëùëí is
set to 768 and the size of the node type embedding used in the
graph node initialization layer ùëëùë° is set to 128. For the c-encoder,
the length of input tokens ùëôùëê is truncated to 400 and 300 for the
Java and Python dataset, respectively. The number of self-attention
layers in the c-encoder ùêøùëê is set to 12 and the embedding size of
each layer ùëëùëöùëúùëëùëíùëô is 768. In each layer, the number of heads ‚Ñéùëíùëéùëëùëê is
12 and the sizes of the key and value are both set to 64. Finally, the
dimension of the feedforward layer ùëëùëì ùëì is 2048. For the g-encoder,
the number of output node embeddings ùëôùëî is set to 300, the number
of graph attention heads ‚Ñéùëíùëéùëëùëî is set to 8, and the number of graph
attention layers ùêøùëî is set to 4. For the decoder, the number of layers
ùêøùëë is set to 6. The length of the input summary ùëôùë† in training is
truncated or zero-padded into 100 and 80 for the Java and Python
datasets, respectively, in terms of the number of tokens.

When training, we set the dropout rate at the embedding and
fully-connected layers to 0.2, learning rate to 0.0001, and batch size
to 32, respectively. We use Adam for optimization. When testing,
we use beam search for token generation and set the beam size to
6.

Table 2: Hyperparameter settings for GypSum.

embedding

c-encoder

g-encoder

decoder

training

testing

hyperparameter
ùëëùëí
ùêøùëê
ùëôùëê
‚Ñéùëíùëéùëëùëê
ùëëùëöùëúùëëùëíùëô
ùëëùëò ,ùëëùë£
ùëëùëì ùëì
ùëôùëî
‚Ñéùëî
‚Ñéùëíùëéùëëùëî
ùêøùëî
ùëôùë†
ùêøùëë
dropout
optimizer
learning rate
batch size
beam size

Java

Python

768
12

400

300

12
768
64
2048
300
768
8
4

100

80

6
0.2
Adam
0.0001
32
6

GypSum: Learning Hybrid Representations for Code Summarization

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Table 3: The comparative results on the two testing sets.

Methods

RL+HybridSeq (2018) [40]
DeepCom (2018)[21]
API+CODE (2018)[22]
Dual (2019)[42]
Rencos (2020)[47]
Transformer (2020) [1]
CAST (2021) [35]
mAST+GCN (2021) [11]
SiT (2021) [44]
CodeT5 (2021) [41]
CodeBERT (2020) [17]

GypSum (ours)

LSTM+g-encoder
Transformer+g-encoder
c-encoder+Code2Seq
c-encoder+ASTNN
c-encoder+GGNN

46.01
47.31
47.83
47.87
48.02

51.91
52.67
52.25
53.61
54.71
54.76
55.08
54.82
55.87
56.49
56.23
59.42
(‚Üë 2.93)

Java
BLEU METEOR ROUGE-L
38.22
39.75
41.31
42.39
44.77
44.58
45.19
45.49
45.19
46.01
46.64
48.57
(‚Üë 1.93)

22.75
23.06
23.73
25.77
25.82
26.43
27.88
27.17
27.52
28.55
28.84
30.85
(‚Üë 2.01)
Ablation Study
28.87
30.02
30.29
30.36
30.50

56.39
58.12
58.29
58.40
59.01

Python
BLEU METEOR ROUGE-L
19.28
20.78
15.36
21.80
29.20
32.52
-
32.82
34.31
34.31
34.34
35.88
(‚Üë 1.54)

39.24
37.35
33.65
39.45
44.32
46.73
-
46.81
49.71
49.25
49.73
50.27
(‚Üë 0.54)

9.75
9.98
8.57
11.14
19.51
19.77
-
20.12
22.09
21.74
21.99
22.96
(‚Üë 0.87)

33.82
35.01
35.13
35.29
35.47

21.19
22.06
22.53
22.67
22.72

47.85
49.73
50.08
50.13
50.18

4.4 Comparative Study
4.4.1 Main Results. We first report the main results on the two
testing sets in Table 3. The best result and the second best result
for each metric (i.e., each column) are in the bold and italic font,
respectively. CAST has not reported the results on the Python
dataset in the original paper.

We observe that the GypSum model outperforms all the com-
parative models (shown in the top half of the table) by a notable
margin for all the metrics on both datasets. Compared with the
second best results, GypSum improves the BLEU, METEOR and
ROUGE-L scores by 1.93, 2.01 and 2.93, respectively, on the Java
dataset. The corresponding improvements on the Python dataset
are 1.54, 0.87 and 0.54, respectively. The results prove the effec-
tiveness of our idea for learning hybrid representations for source
code summarization. Comparing between the results on the two
datasets, we could observe that both the absolute values and the
performance gains of the Python dataset are smaller than that of the
Java dataset. This is probably because the original ASTs of Python
code are much simpler than that of Java code, as we discussed in
Section 3.2.1. Therefore, the constructed semantic graph of Python
code contains less semantic information compared with that of Java
code, which results in the worse performance. As such, increasing
the semantic complexity of Python ASTs might be a practical way
to achieve further performance improvements.

4.4.2 Results on the Cleaned Testing Sets. The above datasets are
widely used in the current research for code summarization. How-
ever, we carefully examine the datasets and find that a large fraction
of code snippets in the testing set are also presented in the training
set, for both datasets.

In particular, for each code snippet in the testing set, we use
the longest common sequence algorithm to find the most similar

snippet in the training set and compute the similarity score between
them. A score of 1.0 indicates the two snippets are exactly the same.
Then we plot the distribution of the similarity scores in Figure 4 for
all the testing code snippets in the two datasets. We observe that
for the Java dataset, about 39% of the code snippets in the testing
set are actually contained in the training set (with similarity score
equal to 1.0), and for the Python dataset, the fraction is about 21%.

Figure 4: The distribution of similarity score on the two test-
ing sets. The x-axis is the similarity score for each code snip-
pet in the testing set. The y-axis is the count of code snippets.

To better evaluate the generalization ability of the models, we
remove those duplicated snippets with similarity score 1 from the
testing sets and report the additional results on the cleaned testing
sets. We run those models that have released executable source code
and the pre-trained models on the sets. The results are reported

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

Table 4: The comparative results on the cleaned testing sets.

Methods

RL+HybridSeq (2018) [40]
DeepCom (2018)[21]
API+CODE (2018)[22]
Dual (2019)[42]
Rencos (2020)[47]
Transformer (2020) [1]
SiT (2021)[44]
CodeT5 (2021) [41]
CodeBERT (2020) [17]

GypSum

Java
BLEU METEOR ROUGE-L
14.99
15.20
15.93
16.66
18.69
18.46
18.74
19.42
19.47
21.35
(‚Üë 1.88)

28.99
29.03
28.94
30.42
32.20
31.76
32.61
33.91
33.86
37.32
(‚Üë 3.41)

11.01
11.19
11.56
12.53
11.96
13.90
13.02
13.67
13.75
15.11
(‚Üë 1.36)

Python
BLEU METEOR ROUGE-L
11.12
12.03
11.39
15.91
21.55
22.74
22.96
24.07
24.12
25.22
(‚Üë 1.10)

30.31
29.98
23.68
34.15
36.40
38.52
39.77
41.29
41.34
42.10
(‚Üë 0.76)

7.21
7.29
6.99
10.03
15.25
14.26
15.14
16.88
16.95
17.59
(‚Üë 0.64)

in Table 4. We observe a drastic performance decrease for all the
models. Nevertheless, GypSum still performs the best among all
the models.

Table 5: Sensitivity analysis with different hyperparameter
settings of GypSum.

4.4.3 Ablation Study. The main idea of GypSum is to use two
encoders to learn simultaneously from token sequences and AST
structures, with the expectation of preserving both naturalness and
informativeness in the summaries. There can be different choices
of the detailed structures for the encoders. As such we replace
the two encoders with alternative structures and show GypSum
performs the best among the possible structure choices. The results
are presented in the bottom half of Table 3.

For learning from the token sequences, we replace the c-encoder
with the LSTM encoder and Transformer‚Äôs encoder, respectively,
and keep the g-encoder unchanged, which results in two models
LSTM+g-encoder and Transformer+g-encoder. For learning from
the AST structures, we keep the c-encoder unchanged and replace
the g-encoder with the encoder in Code2Seq [4], ASTNN [48] and
GGNN [3], respectively, which are three other state-of-the-art tech-
niques for code representation learning using ASTs. Code2Seq ex-
tracts the paths from an AST and encodes each individual path.
Hence we use the path encodings for the attention computation
on the decoder side. ASTNN encodes the individual subtree in an
AST corresponding to a set of statements in the source code. Simi-
larly, we use the subtree encodings for the attention computation
on the decoder side. GGNN constructs a similar semantic graph
to ours for a code snippet and uses gated graph sequence neural
networks to learn from the graphs. The resulted three models are
c-encoder+Code2Seq, c-encoder+ASTNN and c-encoder+GGNN.
We observe two points. First, all the ablation models except
LSTM+g-encoder outperform all the comparable methods. The
results show again the effectiveness of learning hybrid representa-
tions from source code for summarization. Notably, Transformer+g-
encoder also outperforms all the comparable models, indicating
that with our method, we can achieve the new state of the art
without relying on any pre-trained programming and natural lan-
guage models, such as CodeBERT and CodeT5. LSTM+g-encoder
performs worse than Transformer+g-encoder, since it has the worse
ability to encode long sequences. Second, the proposed GypSum
model outperforms all ablation models, which justifies the choice

BLEU
39.66
43.39
46.21
48.57

46.33
47.06
47.83
48.57

48.15
48.28
48.37
48.57
48.45
48.51

48.11
48.23
48.57
48.55

192
384
576
768

3
4
5
6

1
2
3
4
5
6

100
200
300
400

ùëëùëí

ùêøùëë

ùêøùëî

ùëôùëî

Java
METEOR
22.83
26.74
28.62
30.85

ROUGE-L
52.28
55.87
57.36
59.42

27.42
28.05
28.92
30.85

30.57
30.63
30.75
30.85
30.80
30.84

30.58
30.60
30.85
30.81

56.74
57.48
58.37
59.42

59.03
59.14
59.29
59.42
59.34
59.37

59.07
59.15
59.42
59.38

BLEU
28.13
30.42
33.69
35.88

33.99
34.43
35.14
35.88

35.60
35.67
35.72
35.88
35.77
35.82

35.57
35.64
35.88
35.85

Python
METEOR
17.58
19.77
21.41
22.96

ROUGE-L
42.89
45.12
48.88
50.27

20.79
21.89
21.93
22.96

22.68
22.77
22.83
22.96
22.89
22.94

22.62
22.78
22.96
22.92

47.50
48.53
49.39
50.27

50.11
50.12
50.14
50.27
50.19
50.25

50.09
50.15
50.27
50.23

of each component among alternatives. GypSum is better than
LSTM+g-encoder and Transformer+g-encoder, thanks to the abil-
ity of CodeBERT to produce more fluent natural language. For
c-encoder+Code2Seq and c-encoder+ASTNN, they manipulate the
AST structures without considering the direct links between the
elements in the source code. For c-encoder+GGNN, it does not add
the semantic edges proposed by us and has not considered to use
attentions for node feature extraction. As such, the three ablation
models perform worse than GypSum.

Sensitivity Analysis. We choose four most important hyper-
4.4.4
parameters in GypSum for sensitivity analysis, namely, the output
embedding size of the two encoders ùëëùëí , the number of attention
layers used in the decoder ùêøùëë , the graph attention layers in the
g-encoder ùêøùëî, and the number of node embeddings output by the
g-encoder ùëôùëî. ùëëùëí is also the embedding size of the encoder-decoder
attention sublayer in the decoder. Other hyperparameters such
as the input length, the number of attention heads and the beam
search size take the common settings in the related literature. The
results are presented in Table 5. We vary ùëëùëí in {192, 384, 576, 768}.

GypSum: Learning Hybrid Representations for Code Summarization

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

a
v
a
J

Naturalness
1.93
2.06
2.21
2.38
2.54
1.91
2.03
2.30
2.21
2.41
Table 6: Naturalness and Informativeness Measurement.

Informativeness
1.98
2.11
2.09
2.22
2.56
1.84
1.92
1.97
2.04
2.28

Model
Transformer
SiT
CodeT5
CodeBERT
GypSum
Transformer
SiT
CodeT5
CodeBERT
GypSum

n
o
h
t
y
P

The results show that longer embedding size yields better perfor-
mance, since more information of source code is preserved. We
vary ùêøùëë from 3 to 6 in increments of 1. The best performance is
achieved at ùêøùëë = 6, indicating that the deeper c-encoder can better
capture the semantic information of the code. We vary ùêøùëî from 1
to 6 and ùëôùëî from 100 to 400 in increments of 1 and 100, respectively.
We observe that the performance of GypSum is relatively stable
when we vary ùêøùëî and ùëôùëî. For ùêøùëî, the best performance is achieved
at ùêøùëî = 4, indicating that we do not have to use very deep layers for
the graph attention neural networks. For ùëôùëî, the best performance
is achieved at ùëôùëî = 300, indicating that we do not have to use all the
node information when learning from the AST-based graphs. This
would reduce the training overhead greatly.

4.5 User Study
We conduct a user study on the quality of the summaries generated
by the five best models in Table 4, namely, Transformer, SiT, CodeT5,
CodeBERT and our GypSum. The evaluated metrics are naturalness
and informativeness. The former measures text quality pertaining
to grammaticality and fluency, and the latter measures how much
functional information is carried in a summary. We invite four PhD
students and six Master‚Äôs students whose major is computer science
to rate the summaries. We randomly choose 100 code snippets from
the testing sets (50 for Java and 50 for Python) and generate the
summaries for each snippet using the five models. We replicate
three times each snippet with its five summaries so that each stu-
dent is assigned with 30 different snippets with 150 summaries. We
ask the students to rate each summary on the two metrics on a scale
between 1 to 3, where a higher score means better quality. Then
we compute the average scores for each model and the statistical
significance between GypSum and other models. The results are
reported in Table 6. We observe that for both datasets, GypSum per-
forms the best on both metrics and the difference between GypSum
and each model is statistically significant (ùëù-ùë£ùëéùëôùë¢ùëí < 0.05), verified
using a 2-tailed Student‚Äôs t-test. Note that the initial goal of Gyp-
Sum is to ensure both the naturalness and informativeness of the
summary by learning hybrid representations of source code. The
results justify the effectiveness of hybrid representation learning.

4.6 Case Study
4.6.1 Visualizing the Attention Weights. Since our motivation is to
improve the model ability for generating tokens that capture the key

(a) The heatmap of the first Java code from Figure 6.

(b) The heatmap of the first Python code from Figure 6.

Figure 5: The total attention weight of each leaf node when
producing each token in the corresponding summary. ùëã -
axis shows the leaf nodes in the AST and ùëå -axis is the sum-
mary. Each row shows how much every leaf node is attended
when producing the token.

functionality of a code snippet, we investigate what elements are
captured by the g-encoder when GypSum generates each token in
the summary. To this end, we calculate the total attention weight of
each leaf node in the AST that contributes to the generation of each
token in the summary. Particularly, the total attention weight of a
leaf node ùëñ contributing to a token ùëó is calculated as the sum of the
multiplication between the attention weight of ùëñ to each encoding
of g-encoder and the attention weight of the encoding to the output
token ùëó in the decoder. We calculate the attention weights in the last
layer of the model. We visualize the attention weights of the top two
code snippets in Figure 6, and present the result in Figure 5(a) and
Figure 5(b). The ùëã -axis is the (partial) code snippet and the ùëå -axis
is the summary. A deeper color indicates a higher total attention
weight. We observe that when producing some key tokens, the
model successfully attends to the corresponding leaf nodes through
GAT. For instance, in Figure 5(a), the model particularly attends to
the leaf node ‚Äòtotal‚Äô and ‚Äònum‚Äô when generating ‚Äòtotal‚Äô and ‚Äònumber‚Äô
in the summary, respectively.

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

Figure 6: Case study on four example code snippets. We mark the strings in red in the source code pertaining to the key
functionality of each code snippet, and mark the words in light-blue in the summaries generated by GypSum that capture the
key functionality of the corresponding code snippet.

4.6.2 Code Summary Examples. We present some summary exam-
ples generated by different models in Figure 6, including GypSum,
CodeBERT, CodeT5, SiT and Transformer. The Transformer model
learns only from the token sequence of a code snippet. As expected,
we observe that Transformer can generate summaries of good gram-
mar structure. However, the summaries do not capture well the
functional information in the code snippets. For example at the
top-left corner, the summary generated by Transformer is ‚Äòget the
files (and directories)‚Äô whereas the true functionality is to count the
total number of the recovered files. For the two examples at the
bottom, the Transformer model generates meaningless summaries.
SiT improves Transformer by introducing node relationship in the
constructed graph into the attention computation. However, the
example in the bottom-left corner shows it can still miss the key
functional information and generate an incorrect summary. On
the other hand, the two pre-trained models benefit from the pre-
training tasks using code snippets and the summaries, and produce
much better results. For example at the top-left corner, CodeT5
could produce exactly the same summary as the ground-truth. On
all the four examples, GypSum generates the summaries of the
overall best quality. It not only captures the key functional infor-
mation (highlighted using red color in the code snippets and using
light-blue color in the summaries), but also produces very fluent
summaries, which are highly close to the ground-truth text.

5 CONCLUSION AND FUTURE WORK
In this work, we show that learning hybrid representations of source
code can produce better summaries of the code. We follow the
encoder-decoder architecture and propose GypSum, a new deep
learning model for the task. GypSum has two encoders, one lever-
aging the graph attention neural networks to learn from semantic
graphs constructed using ASTs, and the other adopting a recently
developed pre-trained PL-NL model to learn from token sequences.
We use the Transformer‚Äôs decoder to generate summaries, where
we modify the encoder-decoder sublayer to fuse the output of the
two encoders. We conduct extensive experiments to show the supe-
rior performance of GypSum over existing models and justify the
choice of each component in GypSum.

Given there are other representation learning methods of source
code, in future, we will investigate the effect of more code repre-
sentations and other methods for hybrid representation learning.
One possible direction is to employ automated machine learning
methods to identify the optimal code representations for code sum-
marization as well as the architecture for representation fusion.

ACKNOWLEDGEMENT
This work is supported by the grants from the National Natural
Science Foundation of China (Grant No. 61977026, 62072185).

GypSum: Learning Hybrid Representations for Code Summarization

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A
Transformer-based Approach for Source Code Summarization. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics. 4998‚Äì
5007.

[2] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting accurate method and class names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. 38‚Äì49.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations.

[4] Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences
from Structured Representations of Code. In International Conference on Learning
Representations.

[5] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A general path-
based representation for predicting program properties. ACM SIGPLAN Notices
53, 4 (2018), 404‚Äì419.

[6] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1‚Äì29.

[7] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural ma-
chine translation by jointly learning to align and translate. In 3rd International
Conference on Learning Representations, ICLR 2015.

[8] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation with improved correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65‚Äì72.

[9] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polo-
zov. 2018. Generative code modeling with graphs. arXiv preprint arXiv:1805.08490
(2018).

[10] Fuxiang Chen, Mijung Kim, and Jaegul Choo. 2021. Novel Natural Language
Summarization of Program Code via Leveraging Multiple Input Representations.
In Findings of the Association for Computational Linguistics: EMNLP 2021. 2510‚Äì
2520.

[11] YunSeok Choi, JinYeong Bak, CheolWon Na, and Jee-Hyong Lee. 2021. Learn-
ing Sequential and Structural Information for Source Code Summarization. In
Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2842‚Äì
2851.

[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
In International Conference on Learning Representations.

[13] Milan Cvitkovic, Badal Singh, and Animashree Anandkumar. 2019. Open vocab-
ulary learning on source code with a graph-structured cache. In International
Conference on Machine Learning. PMLR, 1475‚Äì1485.

[14] Sergio Cozzetti B de Souza, Nicolas Anquetil, and K√°thia M de Oliveira. 2005. A
study of the documentation essential to software maintenance. In Proceedings of
the 23rd annual international conference on Design of communication: documenting
& designing for pervasive information. 68‚Äì75.

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). 4171‚Äì4186.

[16] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-
Sequence Attentional Neural Machine Translation. In Proceedings of the 54th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 823‚Äì833.

[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: Findings.
1536‚Äì1547.

[18] Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, and Xin Xia.
2021. Code Structure Guided Transformer for Source Code Summarization. arXiv
preprint arXiv:2104.09340 (2021).

[19] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
proved automatic summarization of subroutines via attention to file context. In
Proceedings of the 17th International Conference on Mining Software Repositories.
300‚Äì310.

[20] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837‚Äì847.

[21] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation. In 2018 IEEE/ACM 26th International Conference on Program Comprehension
(ICPC). IEEE, 200‚Äì20010.

[22] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
source code with transferred API knowledge. In Proceedings of the 27th Interna-
tional Joint Conference on Artificial Intelligence. 2269‚Äì2275.

[23] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing source code using a neural attention model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 2073‚Äì2083.

[24] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-

mization. International Conference on Learning Representations (2015).

[25] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28th International Conference on Program Comprehension. 184‚Äì195.

[26] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795‚Äì806.

[27] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated
graph sequence neural networks. arXiv preprint arXiv:1511.05493 (2015).
[28] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.
In Text Summarization Branches Out. Association for Computational Linguistics,
Barcelona, Spain, 74‚Äì81.

[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[30] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computational Linguistics. 311‚Äì318.

[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-

proving Language Understanding by Generative Pre-Training. (2018).

[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine
Learning Research 21 (2020), 1‚Äì67.

[33] Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting program
properties from" big code". ACM SIGPLAN Notices 50, 1 (2015), 111‚Äì124.
[34] Jessica Schrouff, Kai Wohlfahrt, Bruno Marnette, and Liam Atkinson. 2019. Infer-
ring javascript types using graph neural networks. arXiv preprint arXiv:1905.06707
(2019).

[35] Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang,
and Hongbin Sun. 2021. CAST: Enhancing Code Summarization with Hierarchical
Splitting and Reconstruction of Abstract Syntax Trees. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing. 4053‚Äì4062.
[36] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and
Tadayuki Matsumura. 2019. Automatic source code summarization with extended
tree-lstm. In 2019 International Joint Conference on Neural Networks (IJCNN). IEEE,
1‚Äì8.

[37] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence
Learning with Neural Networks. Advances in Neural Information Processing
Systems 27 (2014), 3104‚Äì3112.

[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. Advances in neural information processing systems 30 (2017), 5998‚Äì6008.

[39] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Li√≤, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.

[40] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S Yu. 2018. Improving automatic source code summarization via deep rein-
forcement learning. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering. 397‚Äì407.

[41] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-
aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and
Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing. 8696‚Äì8708.

[42] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a
dual task of code summarization. In Advances in Neural Information Processing
Systems 2019. Neural Information Processing Systems (NIPS).

[43] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and refine:
exemplar-based neural comment generation. In 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 349‚Äì360.

[44] Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with
Structure-induced Transformer. In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021. 1078‚Äì1090.

[45] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan-
ping Li. 2017. Measuring program comprehension: A large-scale field study with
professionals. IEEE Transactions on Software Engineering 44, 10 (2017), 951‚Äì976.
[46] Zhou Yang, Jieke Shi, Shaowei Wang, and David Lo. 2021. Incbl: Incremental
bug localization. In 2021 36th IEEE/ACM International Conference on Automated

ICPC ‚Äô22, May 16‚Äì17, 2022, Virtual Event, USA

Y Wang, Y Dong, X Lu and A Zhou.

Software Engineering (ASE). IEEE, 1223‚Äì1226.

[47] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd Inter-
national Conference on Software Engineering (ICSE). IEEE, 1385‚Äì1397.

[48] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. 2019. A novel neural source code representation based on abstract syntax
tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 783‚Äì794.

