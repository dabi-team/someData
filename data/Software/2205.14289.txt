Contrastive Learning for Multi-Modal Automatic
Code Review

Bingting Wu
School of Computer Science and Technology
Soochow University
Suzhou, China
20204227028@stu.suda.edu.cn

Xiaofang Zhang
School of Computer Science and Technology
Soochow University
Suzhou, China
xfzhang@suda.edu.cn

2
2
0
2

y
a
M
8
2

]
E
S
.
s
c
[

1
v
9
8
2
4
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Automatic code review (ACR), aiming to relieve
manual inspection costs, is an indispensable and essential task
in software engineering. The existing works only use the source
code fragments to predict the results, missing the exploitation of
developer’s comments. Thus, we present a Multi-Modal Apache
Automatic Code Review dataset (MACR) for the Multi-Modal
ACR task. The release of this dataset would push forward the
research in this ﬁeld. Based on it, we propose a Contrastive
Learning based Multi-Modal Network (CLMN) to deal with the
Multi-Modal ACR task. Concretely, our model consists of a code
encoding module and a text encoding module. For each module,
we use the dropout operation as minimal data augmentation.
Then, the contrastive learning method is adopted to pre-train
the module parameters. Finally, we combine the two encoders to
ﬁne-tune the CLMN to decide the results of Multi-Modal ACR.
Experimental results on the MACR dataset illustrate that our
proposed model outperforms the state-of-the-art methods.

Index Terms—automatic code review, dataset, contrastive

learning, multi modal, abstract syntax tree

I. INTRODUCTION

Code review is a critical part of software maintenance and
evaluation. A general process of code review is shown in Fig.
1. Whenever the developer has submitted the revision code
and the comments, the system will schedule an appropriate
reviewer who will need to compare the differences between
the original ﬁle, the modiﬁed ﬁle and combine the content of
the developer’s comments to verify whether the code meets
the requirements. However, it also has a great demand for
human resources [1], which makes it impossible to expand
code review on a large scale.

Therefore, many researchers are committed to Automatic
Code Review (ACR) to solve the problem [2]–[4]. For ACR,
they provide the model with the original code and the revised
code, and the model returns the suggestions on whether this
modiﬁcation is acceptable.

In this paper, we present a new perspective that the ACR
should not only contain code fragments. The developer’s
comments, explaining why and how to change code, are also
an essential source of information in the code review process.
The model should fully consider the developer’s comments
when giving suggestions for code review. Therefore, we focus
on the Multi-Modal ACR task. However, there are three main

Fig. 1: Traditional code review process.

challenges: 1) there is no corresponding dataset, 2) most of
the existing models ignore the problem of few-shot, and 3)
there is no multi-modal model in the Multi-Modal ACR task.
Since there is still a lack of relevant public datasets, we
propose and open-source a Multi-Modal Apache Automatic
Code Review dataset (MACR1), with 11 projects and over
500 thousand samples.

Recent works [2]–[5] have shown that deep learning meth-
ods perform better in capturing the syntactical and semantic
information of the source code, enabling suitable code review
suggestions. Among them, Shi et al. [2] proposes a method
called DACE, which uses long short-term memory (LSTM)
and convolutional neural network (CNN) to capture the se-
mantic and syntactical information, respectively. Zhang et al.
[5] proposes a method called ASTNN. The main idea is to
obtain better feature extraction ability by dividing the abstract
syntax tree (AST) into sentence-level blocks. Although these
approaches are excellent in modeling code fragments, they do
not consider the problem of few-shot. Misclassiﬁcation may
occur when the model encounters code fragments that have not
been seen before. Therefore, in this paper, we adopt contrastive
learning to obtain a uniform distributed vector representation
to improve the robustness of the model.

In addition, we explore a new method CLMN, to represent
code snippets and textual content. We ﬁrst model the code
snippets and textual content separately using two indepen-
dent encoding modules SimAST-GCN [4] and RoBERTa [6],
respectively. Since RoBERTa already has ideal pre-training

DOI reference number: 10.18293/SEKE2022-022

1https://github.com/SimAST-GCN/CLMN

OriginalfileRevisedfileDeveloperReviewerSubmitAcceptCode baseRejectComments 
 
 
 
 
 
parameters, we need to pre-train SimAST-GCN. We adopt the
method of contrastive learning to train the parameters of the
SimAST-GCN module. After ﬁnishing the pre-training of the
two encoders, we concatenate the vector representations of two
encoders to obtain a joint representation of code and text to
predict the ﬁnal code review result.

Experiments results show that the proposed methods achieve
signiﬁcantly better results than the state-of-the-art baseline
methods on MACR datasets. Our main contributions are
summarized as follows:

• We provide a large-scale Multi-Modal Apache Automatic
Code Review dataset (MACR) for the Multi-Modal ACR
task.

• We propose a novel neural network CLMN to learn the
combined relationship of the code fragments and textual
content to improve the accuracy of Multi-Modal ACR.
• We conduct a series of large-scale comprehensive exper-
iments to conﬁrm the effectiveness of our approach.
The rest of this paper is organized as follows. Section II
introduces the background. Section III describes our approach.
Section IV provides our experimental settings. Section V
presents the experimental results and answers the research
questions. Finally, Section VI concludes our work.

II. BACKGROUND

A. Code Review

The general process of traditional code review is shown
in Fig. 1. In traditional approaches, researchers cannot solve
the main challenge in code review: understanding the code
[7]. Therefore, researchers can only improve efﬁciency from
other aspects of code review, such as recommending suitable
reviewers [8], [9] and using static analysis tools [10], [11].

With the development of deep learning, we can understand
the code by modeling the source code [2]–[4]. However, these
methods only use code fragments to predict the results, lacking
the exploitation of developer’s comments in code review.

To solve the Multi-Modal ACR task, the model ﬁrst extracts
features from the original ﬁle, the revised ﬁle, and the com-
ments. Then, the model needs to encode these features into
vector representations and combine the code representation
and the comment representation into a uniform representation.
Finally, the neural network can make the ﬁnal suggestion about
the code review according to the uniform representation.

B. Abstract Syntax Tree

An abstract syntax tree (AST) is a kind of tree aimed at
representing the abstract syntactic structure of source code.
AST has been widely used as a vital feature in deep learning
[2], [4], [12]. For example, it has a wide range of applications
in source code representation [5], defect prediction [13], and
other ﬁelds. Each node of an AST corresponds to constructs or
symbols of the source code. On the one hand, compared with
plain source code, ASTs are abstract and do not include all
details such as punctuation and delimiters. On the other hand,
ASTs can be used to describe the lexical information and the

syntactic structure of the source code. In this paper, we use the
Simpliﬁed AST [4] as the input for the code encoder module.

C. SimAST-GCN

In our previous study, we proposed a method SimAST-
GCN [4]. SimAST-GCN ﬁrst parses the code fragment into the
AST and uses a specially designed algorithm to simplify the
AST into a Simpliﬁed-AST. Then, we use preorder traversal
algorithm to get the node sequence x = [x1, x2, ..., xn] and
the relation graph A ∈ Rn×n, where n is the number of nodes.
Next, the representations for nodes are obtained by:

H = {h1, h2, ..., hn} = Bi-GRU(x)

(1)

Then, we use GCN to aggregate the information from their
neighborhoods.

hl = GCN(A, hl−1)

(2)

where hl is the graph hidden status, h0 is the H. Finally, we
adopt a retrieval-based attention mechanism [14] to combine
H and hl to get the ﬁnal representation of the code fragment.

D. Contrastive learning

i )}m

Contrastive learning aims to learn effective representation
by pulling semantically close neighbors together and pushing
apart non-neighbors. It assumes a set of paired samples D =
{(xi, x+
i are semantically related. We
follow the contrastive framework in Chen et al. [15] and take
a cross-entropy objective with in-batch negatives [16] : let hi
and h+
i , the training
objective for (xi, x+

i denote the representation of xi and x+

i ) with a mini-batch of N pairs is:

i=1, where xi and x+

(cid:96) = − log

esim(hi,h+
i )/τ
j=1 esim(hi,h+

(cid:80)N

j )/τ

(3)

h(cid:62)

1 h2

where τ is a temperature hyperparameter and sim(h1, h2) is
the cosine similarity
(cid:107)h1(cid:107)·(cid:107)h2(cid:107) . In this work, we encode code
fragments using a graph-based model SimAST-GCN and then
train all the parameters using the contrastive learning objective
(Eq. 3). For the text encoder, we use the pre-trained model
RoBERTa and the parameters are trained by SimCSE [17].

III. PROPOSED APPROACH

We introduce our approach CLMN in this section. As shown
in Fig. 2, the framework of the proposed CLMN contains
two main components:
the Code Encoder (SimAST-GCN)
and the Text Encoder (RoBERTa). For each encoder, we use
contrastive learning to learn an adequate representation of
code fragments and the developer’s comments. For the Code
Encoder, we use an unsupervised contrastive learning method
to train its parameters. During the training process, we use
dropout as noise to obtain positive samples. The process is
shown in Fig. 3. For the Text Encoder, we use the parameters
trained by SimCSE directly. After the unsupervised training
for the two encoders, we use the pre-trained Encoders to ﬁne-
tune the ﬁnally model CLMN to predict the results.

Fig. 2: General framework of CLMN.

Fig. 3: Unsupervised Code Encoder predicts the input code
fragment itself from in-batch negatives, with different hidden
dropout masks applied.

A. Contrastive Learning for Code Encoder

The main challenge for contrastive learning is how to get
positive samples. As shown in Fig. 3, in the Code Encoder,
we pass the same code fragment to the model SimAST-GCN
twice: by applying the standard dropout twice, we can obtain
two different embeddings as “positive pairs”. Then we take
other code fragments in the same mini-batch as “negative”
samples, and the model predicts the positive one among
negative samples. Although it may appear strikingly simple,
this approach is widely used in natural language models [17].
In this work, we take a collection of code fragments {xi}m
i=1
and use x+
i = xi. We use the independent dropout for xi
and x+
to make it work. In SimAST-GCN, there are dropout
i
placed on each GCN layers(default p = 0.1). We denote hz
i =
fθ(xi, z) where z is a random mask for dropout. We feed
the same input to the encoder twice and get two embeddings
with different dropout masks z, z(cid:48), and the training objective
of Code Encoder becomes:

L = − log

esim(hzi
i ,h
j=1 esim(hzi

(cid:80)N

z(cid:48)
i
i )/τ

z(cid:48)
i
j )/τ

i ,h

(4)

for a mini-batch of N code fragments. Note that z is just the
standard dropout mask in SimAST-GCN and we do not add
any additional dropout.

B. CLMN

2) Combined Representation: As shown in Fig. 2, after
getting three representations, we ﬁrst calculate the difference
between the original code fragment and the revised code
fragment:

C = CO − CR

(5)

where C denotes the difference between the two code frag-
ments. Then, we need to combine the difference representation
with the comments representation, because the developer’s
comments describe the difference between the two code frag-
ments.

r = [C, TD]

(6)

Thus, we get the combined representation for this data sample.
3) Prediction: In this part, we make the prediction accord-

ing to the combined representation r.

y = softmax(Wr + b)

(7)

where softmax(·) is the softmax function to obtain the output
distribution of the classiﬁer, W and b are weights and biases,
respectively.

The objective to train the classiﬁers is deﬁned as minimizing
the weighted cross entropy loss between the predicted and
ground-truth distributions:

L = −

S
(cid:88)

i=1

(wOyilog ˆpi +wR(1−yi)log(1− ˆpi))+λ (cid:107)Θ(cid:107)2 (8)

1) Encoder Module: In our CLMN model, we have two
pre-trained encoder modules, code encoder (SimAST-GCN)
and text encoder (RoBERTa). For the code encoder, we
denote C = f (c) where c is a code fragment and C is the
representation for this code fragment. For the text encoder, we
denote T = f (t) where t is developer’s comments and T is the
corresponding representation. In the Multi-Modal ACR task,
each data example contains three parts: original code fragment,
revised code fragment, and developer’s comments. Thus, for
three representations CO, CR, TD,
each data, we can get
corresponding to the original code fragment, revised code
fragment, and the developer’s comments.

where S is the number of training samples, wO denotes
the weight of incorrectly predicting a rejected change as
approved, and wR denotes the weight of incorrectly predicting
an approved change as rejected. These two terms provide the
opportunity to handle an imbalanced label distribution. λ is the
weight of the L2 regularization term. Θ denotes all trainable
parameters.

IV. EXPERIMENTAL DESIGN

This section introduces the process of the experiment,
including the repository selection and the data construction,
baseline setting, evaluation metrics, and experimental setting.

OriginalRevisedCDiffCommentsTMLPConcatCCode EncoderTText Encoderpublic static int add(String[] args){……}Cpublic static float sub(String[] args){……}public static Node normalize(Node root){……}Different hidden dropoutmasks in two forward passesCCNegative instanceCode EncoderPositive instanceCTABLE I: Statistics of the MACR dataset.

Repository
accumulo
ambari
beam
cloudstack
commons-lang
ﬂink
hadoop
incubator-pinot
kafka
lucene-solr
shardingsphere

#samples
20,903
41,424
82,438
37,599
9,958
133,614
39,958
13,903
64,872
58,823
19,993

#rejected
9,042
14,262
22,141
18,703
8,959
112,733
33,861
2,118
37,077
50,042
1,216

reject rate
43.3%
34.4%
26.9%
49.7%
90.0%
84.4%
84.7%
15.2%
57.2%
85.1%
6.1%

A. Dataset Construction

We selected 11 projects from Github belonging to the
Apache Foundation, which is a widely used code review source
[2], [4]. Three of them (accumulo, ambari, cloudstack) were
selected by Shi et al. [2]. The remaining projects (beam,
commons-lang, ﬂink, hadoop, incubator-point, kafka, lucene-
solr, shardingsphere) were chosen because they have over
2000 stars. The language of all the projects is Java.

For data processing, we extracted all issues belonging to
these projects from 2015 to 2020. Among these issues, many
do not involve code submission, but only provide feedback,
so we chose the types PullRequestEvent and PullRequestRe-
viewCommentEvent.

In many practical cases, we can easily extract the original
code, the revised code and the developer’s comments from the
issue, but because these codes are usually contained in many
ﬁles, it is difﬁcult for us to use them directly as the input
of the network. Therefore, we assume that all of the changes
are independent and identically distributed [2], so there is no
connection between these changes, and if a ﬁle contains many
changed methods, we can split these methods independently
as inputs.

Further, if we add a new method or delete a whole method,
half of the input data is empty. So we discard these data
because they cannot be fed into the network. That is, we
only consider the case where the code has been changed. In
addition, considering that the submitted data may be too large,
we subdivide the code submitted each time into the method
level for subsequent experiments.

After processing the data, each piece of data comprises four
parts: the original code fragment, the revised code fragment,
the developer’s comments and the label. The original code
fragment and the revised code fragment are both method-level
Java programs. The developer’s comments are in English. The
label uses 0 and 1 to represent rejection and acceptance. The
basic statistics of the MACR are summarized in Table I.

In this paper, the rate of the rejection between 6% and 90%
means that there is class imbalance during model training and
it will lead to poor performance, so we use a weight-based
class imbalance approach, setting the ‘class weight’ parameter
to ‘balance’ in our model.

B. Comparison Models

In this paper, we compared our proposed model CLMN with
three other models in the Multi-Modal ACR task. These mod-
els use different methods (including delimiter-based method,
token-based method, tree-based method) to obtain the code
features. The baseline models are as follows:

• DACE [2] divides the code according to the delimiter and
designs a pairwise autoencoder to compare the code.
• TBRNN serializes the AST into tokens and uses an RNN

to capture the syntactic and semantic information.

• ASTNN [5] splits large ASTs into a sequence of small
statement trees and calculates the representation distance
to compare the code.

Please note that for all the methods above, we add the same
text encoder RoBERTa to make all the methods can get the
developer’s comments as the input.

In order to ensure the fairness of the experiment and the
stability of the results, we ran all the methods on the new
MACR dataset, and each experiment was repeated 30 times.

C. Evaluation

Since the Multi-Modal ACR can be formulated as a binary
classiﬁcation problem (accept or reject) [2], we choose the
F1-measure (F1) because it is widely used and the Matthews
correlation coefﬁcient (MCC) because it can better evaluate
the performance of a model on imbalanced datasets.

The calculation formula of F1 is as follows:

P recision =

T P
T P + F P

Recall =

T P
T P + F N
P recision ∗ Recall
P recision + Recall

F 1 = 2 ∗

(9)

(10)

(11)

where TP, FP, FN, and TN represent True Positives, False
Positives, False Negatives, and True Negatives, respectively.
The F1 score is the harmonic mean of the precision and recall.
The value range of F1 is [0,1].

The calculation formula of MCC is:

M CC =

T P × T N − T P × F N
(cid:112)(T P + F P )(T P + F N )(T N + F P )(T N + F N )

(12)
The value range of MCC is [-1,1]. For all metrics, higher
values are better.

D. Experimental Setting

In our experiments, we used the javalang tools2 to obtain
ASTs for Java code, and we trained embeddings of symbols
using word2vec with the Skip-gram algorithm. For the code
encoder, the embedding size was set to 512. The hidden size of
Bi-GRU was 512. The number of GCN layers was 4. For the
text encoder, we used the default parameters. The coefﬁcients
wO and wR were related to the dataset, and the coefﬁcient λ of
L2 regularization item was set to 10−5. Adam was utilized as

2https://github.com/c2nes/javalang

TABLE II: Performance comparison in terms of F1.

TABLE IV: Impact of the addition of developer comments.

Repository
accumulo
ambari
beam
cloudstack
commons-lang
ﬂink
hadoop
incubator-pinot
kafka
lucene-solr
shardingsphere
Average

DACE
0.995
0.977
0.995
0.979
0.957
0.421
0.947
0.859
0.985
0.978
0.992
0.917

TBRNN
0.99
0.979
0.994
0.979
0.963
0.959
0.934
0.795
0.691
0.979
0.991
0.932

ASTNN
0.995
0.979
0.995
0.98
0.965
0.972
0.949
0.954
0.985
0.978
0.927
0.971

CLMN
0.996
0.98
0.996
0.981
0.964
0.979
0.952
0.994
0.986
0.975
0.994
0.981

TABLE III: Performance comparison in terms of MCC.

Repository
accumulo
ambari
beam
cloudstack
commons-lang
ﬂink
hadoop
incubator-pinot
kafka
lucene-solr
shardingsphere
Average

DACE
0.989
0.936
0.982
0.957
0.954
0.306
0.939
0.466
0.974
0.975
0.876
0.85

TBRNN
0.978
0.941
0.977
0.957
0.96
0.952
0.923
0.146
0.457
0.976
0.863
0.83

ASTNN
0.99
0.943
0.983
0.96
0.962
0.967
0.94
0.894
0.974
0.975
0.295
0.898

CLMN
0.991
0.946
0.985
0.961
0.961
0.975
0.944
0.961
0.975
0.971
0.899
0.961

the optimizer with a learning rate of 10−5 to train the model,
and the mini-batch was 64. We random initialized all the W
and b with a uniform distribution.

All the experiments were conducted on a server with 24

cores of 3.8GHz CPU and a GeForce RTX 3090 GPU.

V. EXPERIMENTAL RESULTS

This section shows the performance of our proposed method
CLMN with other baseline methods. Therefore, we put for-
ward the following research questions:

• RQ1: Does our proposed model CLMN outperform other

models for Multi-Modal ACR?

• RQ2: Does the addition of developer’s comments improve

the performance of ACR?

• RQ3: How about the impact of contrastive learning on

model robustness?

A. Does our proposed model CLMN outperform other models
for Multi-Modal ACR?

Tables II, III show the comparison results on the MACR
dataset. The results show that the proposed CLMN consis-
tently outperforms all comparison models. This veriﬁes the
effectiveness of our proposed method at Multi-Modal ACR.

Repository

accumulo
ambari
beam
cloudstack
commons-lang
ﬂink
hadoop
incubator-pinot
kafka
lucene-solr
shardingsphere
Average

CLMN w/o Text
MCC
0.448
0.013
0.321
0.215
0.474
0.237
0.298
0.181
0.298
0.297
0.191
0.27

F1
0.761
0.704
0.824
0.532
0.469
0.367
0.393
0.673
0.604
0.392
0.839
0.596

CLMN

F1
0.996
0.98
0.996
0.981
0.964
0.979
0.952
0.994
0.986
0.975
0.994
0.981

MCC
0.991
0.946
0.985
0.961
0.961
0.975
0.944
0.961
0.975
0.971
0.899
0.961

* CLMN w/o Text means the CLMN without the Text Encoder.

TABLE V: Impact of contrastive learning on model robustness.

Repository

accumulo
ambari
cloudstack
commons-lang
ﬂink
hadoop
incubator-pinot
kafka
lucene-solr
shardingsphere
Average

beam based CLMN

self based CLMN

F1
0.996
0.98
0.98
0.948
0.973
0.951
0.989
0.984
0.973
0.993
0.977

MCC
0.99
0.946
0.961
0.944
0.968
0.943
0.934
0.973
0.968
0.887
0.951

F1
0.996
0.98
0.981
0.964
0.979
0.952
0.994
0.986
0.975
0.994
0.98

MCC
0.991
0.946
0.961
0.961
0.975
0.944
0.961
0.975
0.971
0.899
0.958

* beam based CLMN: using the Code Encoder pre-trained by beam.
* self based CLMN: using the Code Encoder pre-trained by themselves.

and ASTNN, CLMN improved by 4.9% and 1% respectively
in F1. In the MCC metric, CLMN also gained 13.1% and 6.3%
improvement.

One aspect of our improvement comes from using a better
code encoder. Our code encoder uses the Simpliﬁed AST as
input, which is better than the source code fragment and the
original AST. Simpliﬁed AST has fewer tokens and tighter
node connections. Besides, our code encoder uses the GCN
layers to get more syntactic and semantic information than
the Bi-GRU layers in other methods.

Another aspect of our improvement comes from the appli-
cation of contrastive learning. We utilize contrastive learning
to obtain a more evenly distributed representation of the
source code, which allows our model to perform better when
encountering few-shot and 0-shot samples. In addition, the
pre-training parameters obtained by contrastive learning can
be ﬂexibly applied to other tasks (discussed in RQ3), thereby
reducing the consumption of resources.

Compared with DACE, our model CLMN achieves the best
performance in both the F1 and MCC metrics, with 6.4% and
11.1% improvement, respectively. Compared with TBRNN

In summary, this is the ﬁrst study to apply contrastive learn-
ing to improve the performance in the multi-modal automatic
code review task. Our experiments show the effectiveness of

exploiting the Simpliﬁed AST and the contrastive learning.

ACKNOWLEDGMENT

B. Does the addition of developer’s comments improve the
performance of ACR?

Table IV shows the comparison results between the CLMN
with and without the developer’s comments. The results show
that CLMN without the developer’s comments tends to drop
sharply.

We can observe that the CLMN without the developer’s
comments has the worst result on ambari, with a 93% drop
in MCC compared to the original result. On the ﬂink project,
the F1 value of CLMN without Text has dropped 61%. On
average, CLMN without Text has an overall decrease of 69%
in MCC and 38% in F1. This establishes that Multi-Modal
ACR can make better decisions with the developer’s comments
rather than only using the code fragments.

C. How about the impact of contrastive learning on model
robustness?

To investigate the robustness of parameters obtained by
contrastive learning, we use beam as the original pre-training
project. After getting the pre-trained parameters of the code
encoder of beam, we apply it to the remaining projects with
ﬁne-tuning to obtain the performance in other projects. Table V
shows the impact of contrastive learning on model robustness.
We can ﬁnd that after using beam’s pre-trained parameters,
the performance of our model in the other ten repositories does
not drop signiﬁcantly compared to the performance pre-trained
by themselves. There is even no drop in some repositories. On
average, there is only a drop of 0.3% in F1 and 0.7% in MCC.
In other words, the hyperparameters obtained using contrastive
learning can be easily extended to different projects and obtain
similar results. When the model encounters an unseen code
fragment (few-shot or 0-shot), it can also generate its vector
representation accurately.

The results are soundproof that contrastive learning can
effectively increase the robustness of the model. It also demon-
strates that the parameters obtained by contrastive learning can
help the code get a more uniform distribution in the vector
space, resulting in better performance across different projects.

VI. CONCLUSION

In this paper, we ﬁrst present MACR, which is the ﬁrst open-
sourced Multi-Modal dataset for Multi-Modal ACR. Then,
we propose a novel model called CLMN. CLMN has two
encoders, SimAST-GCN as the code encoder and RoBERTa as
the text encoder. The two encoders are separately pre-trained
using contrastive learning to obtain a uniformly distributed
representation, which improves the robustness of the model
and solves the few-shot and 0-shot issues. Finally, we combine
the two encoders to obtain a vector representation of the code
review and predict the results. Experimental results on the
MACR dataset show that our proposed model CLMN out-
performs state-of-the-art methods. Our code and experimen-
tal data are publicly available at https://github.com/SimAST-
GCN/CLMN.

This work was partially supported by the National Nat-
ural Science Foundation of China (61772263, 61872177,
61972289, 61832009),
the Collaborative Innovation Center
of Novel Software Technology and Industrialization, and the
Priority Academic Program Development of Jiangsu Higher
Education Institutions.

REFERENCES

[1] C. Sadowski, E. S¨oderberg, L. Church, M. Sipko, and A. Bacchelli,
“Modern code review: a case study at google,” in Proceedings of
the 40th International Conference on Software Engineering: Software
Engineering in Practice, 2018, pp. 181–190.

[2] S.-T. Shi, M. Li, D. Lo, F. Thung, and X. Huo, “Automatic code review
by learning the revision of source code,” in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 33(01), 2019, pp. 4910–4917.
[3] J. K. Siow, C. Gao, L. Fan, S. Chen, and Y. Liu, “Core: Automating
review recommendation for code changes,” in 2020 IEEE 27th Interna-
tional Conference on Software Analysis, Evolution and Reengineering
(SANER).

IEEE, 2020, pp. 284–295.

[4] B. Wu, B. Liang, and X. Zhang, “Turn tree into graph: Automatic code
review via simpliﬁed ast driven graph convolutional network,” arXiv
preprint arXiv:2104.08821, 2022.

[5] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A novel
neural source code representation based on abstract syntax tree,” pp.
783–794, 2019.

[6] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.

[7] A. Bacchelli and C. Bird, “Expectations, outcomes, and challenges of
modern code review,” in 2013 35th International Conference on Software
Engineering (ICSE).
IEEE, 2013, pp. 712–721.

[8] P. Thongtanunam, C. Tantithamthavorn, R. G. Kula, N. Yoshida, H. Iida,
and K.-i. Matsumoto, “Who should review my code? a ﬁle location-
based code-reviewer recommendation approach for modern code re-
view,” in 2015 IEEE 22nd International Conference on Software Analy-
sis, Evolution, and Reengineering (SANER).
IEEE, 2015, pp. 141–150.
[9] Z. Xia, H. Sun, J. Jiang, X. Wang, and X. Liu, “A hybrid approach to
code reviewer recommendation with collaborative ﬁltering,” in 2017 6th
International Workshop on Software Mining (SoftwareMining).
IEEE,
2017, pp. 24–31.

[10] V. Balachandran, “Reducing human effort and improving quality in peer
code reviews using automatic static analysis and reviewer recommenda-
tion,” in 2013 35th International Conference on Software Engineering
(ICSE).

IEEE, 2013, pp. 931–940.

[11] G. D´ıaz and J. R. Bermejo, “Static analysis of source code security:
Assessment of tools against samate tests,” Information and software
technology, vol. 55, no. 8, pp. 1462–1476, 2013.

[12] L. Mou, G. Li, Z. Jin, L. Zhang, and T. Wang, “Tbcnn: A tree-based
convolutional neural network for programming language processing,”
arXiv preprint arXiv:1409.5718, 2014.

[13] T. Shippey, D. Bowes, and T. Hall, “Automatically identifying code
features for software defect prediction: Using ast n-grams,” Information
and Software Technology, vol. 106, pp. 142–160, 2019.

[14] C. Zhang, Q. Li, and D. Song, “Aspect-based sentiment classiﬁca-
tion with aspect-speciﬁc graph convolutional networks,” arXiv preprint
arXiv:1909.03477, 2019.

[15] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
for contrastive learning of visual representations,” in International
conference on machine learning. PMLR, 2020, pp. 1597–1607.
[16] T. Chen, Y. Sun, Y. Shi, and L. Hong, “On sampling strategies for
the
neural network-based collaborative ﬁltering,” in Proceedings of
23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, ser. KDD ’17. New York, NY, USA: Association
for Computing Machinery, 2017, p. 767–776. [Online]. Available:
https://doi.org/10.1145/3097983.3098202

[17] T. Gao, X. Yao, and D. Chen, “Simcse: Simple contrastive learning of
sentence embeddings,” arXiv preprint arXiv:2104.08821, 2021.

