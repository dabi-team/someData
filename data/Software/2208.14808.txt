Reducing Impacts of System Heterogeneity in Federated Learning
using Weight Update Magnitudes

by

Irene Wang

Supervisor: Dr. Prashant Nair

Committee Member: Dr. Divya Mahajan (Microsoft Research)

CPEN499: Undergraduate Thesis
BASc. Computer Engineering

The University of British Columbia

(Vancouver)

August 2022

2
2
0
2

g
u
A
0
3

]

G
L
.
s
c
[

1
v
8
0
8
4
1
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Abstract

The widespread adoption of handheld devices have fueled rapid growth in new applications. Several

of these new applications employ machine learning models to train on user data that is typically

private and sensitive. Federated Learning enables machine learning models to train locally on each

handheld device while only synchronizing their neuron updates with a server. While this enables

user privacy, technology scaling and software advancements have resulted in handheld devices with

varying performance capabilities. This results in the training time of federated learning tasks to be

dictated by a few low-performance straggler devices, essentially becoming a bottleneck to the entire

training process.

In this work, we aim to mitigate the performance bottleneck of federated learning by dynam-

ically forming sub-models for stragglers based on their performance and accuracy feedback. To
this end, we offer the Invariant Dropout, a dynamic technique that forms a sub-model based on the
neuron update threshold. Invariant Dropout uses neuron updates from the non-straggler clients to

develop a tailored sub-models for each straggler during each training iteration. All corresponding

weights which have a magnitude less than the threshold are dropped for the iteration. We eval-

uate Invariant Dropout using ﬁve real-world mobile clients. Our evaluations show that Invariant

Dropout obtains a maximum accuracy gain of 1.4% points over state-of-the-art Ordered Dropout

while mitigating performance bottlenecks of stragglers.

ii

Table of Contents

Abstract .

.

.

.

.

.

Table of Contents

List of Tables .

List of Figures

.

.

.

.

1

Introduction .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Background and Motivation .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1 Global Server and Clients .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Dropout Techniques .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1 Random Dropout .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.2 Ordered Dropout

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Motivation 1: Accuracy Implications . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 Motivation 2: Performance Implications . . . . . . . . . . . . . . . . . . . . . . .

3 Design: Invariant Dropout

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1

Identifying the Performance of Stragglers

. . . . . . . . . . . . . . . . . . . . . .

3.2 Tuning the Performance of Stragglers

. . . . . . . . . . . . . . . . . . . . . . . .

3.3 Drop Threshold .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Dynamically Identifying Drop Thresholds . . . . . . . . . . . . . . . . . . . . . .

4 Convergence of Invariant Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1 Proof of convergence: .

5 Evaluation Setup .

.

.

.

.

5.1 Models and Dataset .

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ii

iii

v

vi

1

4

4

4

5

5

5

5

8

8

8

9

9

10

11

12

12

iii

5.1.1 The FEMNIST Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.2 The Shakespeare Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.3 The CIFAR10 Dataset

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 System Conﬁguration .

5.3 Evaluation Metrics

5.4 Baselines

.

.

.

.

6 Results and Analysis .

.

.

.

.

.

.

.

.

6.1 Accuracy Evaluation .

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Computational Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . .

6.3 Scalability Studies

7 Related Work .

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.1 Server Ofﬂoading Strategies

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2 Communication Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Conclusions and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.1 Future Work .

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.1.1 Extensions of Invariant Dropout

. . . . . . . . . . . . . . . . . . . . . . .

8.1.2 Drop-Threshold Identiﬁcation Improvements . . . . . . . . . . . . . . . .

12

12

12

12

13

14

15

15

16

17

18

18

18

20

20

20

20

Bibliography .

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

iv

List of Tables

Table 5.1

Software-Hardware Speciﬁcations of Clients . . . . . . . . . . . . . . . . . . .

13

Table 6.1 Accuracy Comparison of Dropout Techniques

. . . . . . . . . . . . . . . . . .

15

v

List of Figures

Figure 1.1

Percentage of ‘invariant’ neurons vs. Iterations . . . . . . . . . . . . . . . . .

Figure 2.1 Ordered Dropout Accuracy Comparison with Ideal Scenario . . . . . . . . . .

Figure 2.2

Per-iteration Training Time of Client Devices . . . . . . . . . . . . . . . . . .

Figure 2.3

Invariant Dropout High-level Overview . . . . . . . . . . . . . . . . . . . . .

Figure 6.1

Straggler Training Time Reduction . . . . . . . . . . . . . . . . . . . . . . . .

Figure 6.2

Invariant Dropout Accuracy in Scale . . . . . . . . . . . . . . . . . . . . . . .

2

6

6

7

16

17

vi

1.

Introduction

In the past decade, relentless technology scaling and micro-architectural innovations have resulted

in computationally powerful handheld devices [21, 41]. Handheld devices execute applications that

process data to offer a personalized experience to the users [27, 28]. These applications typically

use Machine Learning (ML) models to train on data that is private to each individuals’ handheld

device. To maintain user privacy, rather than transferring individual data into a server that hosts the

ML model, models are typically trained locally on each handheld device – also called a client [23].

This process is called Federated Learning (FL) and presently, it is employed by several industry

players such as Meta, Google, etc [23, 25, 39].

There is high variation in the computational power of handheld clients. This poses a key chal-

lenge for FL. For instance, our thesis observes that clients that are even a few years apart tend to offer

dramatically different computational performance. Unfortunately, clients that offer low computa-

tional performance dictate the training time and accuracy of FL, and are labelled as stragglers. This

thesis aims to mitigate the computational bottlenecks of stragglers while providing high accuracy.

Depending on the accuracy and performance goals, FL can be implemented using two ap-

proaches. First, FL can employ an asynchronous aggregation protocol [7, 9, 35]. This approach

enables clients to update the server model asynchronously. While this mitigates the effects of strag-

glers, it has the problem that there could be instances when the model is stale [2]. Therefore,

asynchronous aggregation protocol-based FL tend to display slower convergence and lower accu-

racy [7, 13, 34]. Second, FL can employ a synchronous aggregation protocol such as FedAvg [23].

In this approach, the server waits for all clients to share their updates in each training iteration and

thereby maintains an updated server model. This prevents instances of stale models and improves

accuracy at the expense of stragglers dictating the training time [16]. As ML model accuracy is

key, this thesis focuses on synchronous aggregation protocol-based FL.

Prior works advocate sending a smaller subset of the server model to the stragglers [6, 16]. This

helps avoid training bias from uneven client sampling and allows stragglers to contribute towards

training the model [16, 18]. As the stragglers are now training on a subset of the server model, they

showcase a reduced training time. This helps reduce the computational effect of stragglers on the

1

Figure 1.1: The percentage of ‘invariant’ neurons as the number of training iterations vary for
the CIFAR10 [19], FEMNIST [5], and Shakespeare [5] Datasets. Overall, even after
30% of the training iterations, 15%-30% of the neurons remain invariant across the three
datasets. For computing the invariant neurons, we choose thresholds of 180%, 10%, and
500% respectively for these three datasets.

overall training time – albeit with a loss of accuracy as only a portion of the server model is being

trained on stragglers.

To this end, techniques such as Federated Dropout [6] randomly drop parts of the server model

and send a subset of the model to the stragglers. However, as the neurons in the server model

are randomly dropped, Federated Dropout incurs an accuracy drop. A state-of-the-art work, called

Ordered Dropout from FjORD [16], reduces this accuracy loss by regulating the neurons to be

dropped. Ordered Dropout drops either the right or left portions of the server model, thereby ensur-

ing neurons that are not dropped maintain their connections. Thus, the subset of neurons that are

dropped from the server model dictates the overall accuracy of the learning problem.

Our thesis uses the insight that some neurons in the server model are trained quickly and vary

only slightly – which we call ‘invariant’ neurons. Figure 1.1 shows the percentages of invariant

neurons as the number of training iterations increases.

Even after only 30% of the training iterations are completed, 15%-30% of the neurons become

invariant across CIFAR10 [19], FEMNIST [5], and Shakespeare [5] datasets. For this example,

we choose thresholds of 180%, 10%, and 500% respectively for these three datasets and compute

their invariant neurons. Sending invariant neurons over to the straggler provides no utility and

2

0%20%40%60%80%100%Percent of Training Iterations0%5%10%15%20%25%30%35%Percent of Invariant NeuronsCIFAR10FEMNISTShakespearetherefore these neurons can be dropped. We use the magnitude of neuron updates to identify the

invariant neurons. This is challenging as it would require the server to anticipate which neurons

would change while only supplying a sub-model to the straggler. Our thesis uses the non-stragglers

to overcome this challenge.

We observe that, by design, the the number of non-straggler clients tend to be larger in number as

compared to straggler clients. Additionally, non-straggler clients train on the entire model. Thus, by

design, they can provide directions on which neurons are invariant. Over time, beside the invariant

neurons, all other neurons tend to be updated across on all clients. Using this insight, this thesis

proposes Invariant Dropout, a technique that prioritizes the dropout of neurons that change below

a certain threshold – called drop-threshold. The drop-threshold is dynamically determined based

on the neuron update characteristics. The target sub-model size is determined by proﬁling client

training times in the ﬁrst training iteration and identifying the straggler(s). The drop-threshold is

then varied as training iterations increase until the amount of neurons that are chosen equals the

target sub-model size for the straggler(s).

Overall, this thesis makes four key contributions:

1. We propose a tailored straggler identiﬁcation method that dynamically determines the sub-

model size to be executed on stragglers based on their individual performance overheads.

This mitigates the performance overheads of stragglers.

2. We introduce Invariant Dropout. Invariant Dropout dynamically selects a subset of the server

model to train on the straggler based on magnitude of neuron updates over a dynamically

determined threshold.

3. We implement Invariant Dropout, Federated Dropout, and Ordered Dropout on ﬁve mobile

phones. This implementation showcases the real-world effect of stragglers on computational

performance and accuracy.

4. We show that Invariant Dropout improves training accuracy with a maximum 1.5% points

over state-of-the-art Ordered Dropout while mitigating the computational performance over-

heads of stragglers.

3

2. Background and Motivation

We brieﬂy describe the background on Federated Learning and provide motivation for mitigating

stragglers.

2.1 Global Server and Clients

Federated Learning (FL) is typically performed using centralized global servers and distributed

clients, typically handheld devices. In FL systems using synchronous aggregation protocols like

FegAvg [7], the server maintains a central copy of the ML model called the global model. The

clients contain private user data and the server sends the global model to each client at the beginning

of each training iteration. At the end of each iteration, the server aggregates the neuron updates from

each client into the global model. The computational performance of each client varies as they can

span different technology generations. This causes some clients to showcase low computational

performance and be denoted as stragglers.

2.2 Dropout Techniques

Several studies have focused on ensuring privacy and robustness [4, 11, 24, 40], data heterogene-

ity [22], and improving accuracy for FL [20, 23, 26, 31, 38]. However, another important challenge

in FL is system heterogeneity amongst client devices [10, 21]. Due to rapid technological develop-

ment in both hardware and software, even client devices that are just a few years apart can differ

dramatically in terms of processing abilities and memory capacity. Prior work has shown that, sim-

ply discarding slower or lower-tier clients may introduce bias in the trained model [16, 18]. Yet,

including stragglers in the FL network slows down the training process, and causes more power-

ful devices to stall for long periods of time between training iterations. To this end, there are two

state-of-the-art techniques.

4

2.2.1 Random Dropout

Federated Dropout [6] has proposed sending a smaller subset of the global model to the straggler

clients. These work advocate dropping neurons in a random fashion and thus effect the accuracy of

the global model.

2.2.2 Ordered Dropout

Ordered Dropout from FjORD [16] aims to systematically drop neurons by preserving order in the

sub-model. This is done by selecting only parts of the right or the left portions of the global model

to create a sub-model. This state-of-the-art work establishes that the selection of neurons in the

sub-model plays a key role in the overall accuracy of the trained model. By neurons, this work
refers to the ﬁlters from convolutional (CONV) layers, activations from fully-connected (FC) layers,
and hidden units from LSTM [15] layers. This setup assumes there is one straggler device. The

sub-model size for Ordered Dropout is varied from 0.5 to 1 (i.e. complete global model). As the

sub-model size reduces, across three datasets and ML models, Ordered Dropout incurs up to 4.5%

point drop in accuracy.

2.3 Motivation 1: Accuracy Implications

Figure 2.1 shows the difference in testing accuracy between an ‘ideal’ FL implementation and Or-

dered Dropout by using 5 mobile devices using the CIFAR10, FEMNIST, and Shakespeare datasets.

Ideally, we would like the dropout technique to display an accuracy that is closer to the ‘ideal’ FL

implementation.

2.4 Motivation 2: Performance Implications

Figure 2.2 shows the difference in the overall training time of 5 mobile devices involved in FL

without using any dropout techniques. Depending on the dataset and ML model, a client might

display less than 10 seconds or greater than 100 seconds of training time. Furthermore, stragglers

are not ﬁxed and each client may become a straggler for any particular dataset and ML model. For

instance, while Google Pixel 3 is the straggler for the Shakespeare dataset using a two-layer LSTM

classiﬁer. LG Velvet 5G is the straggler for the FEMNIST dataset applying a CNN model. Ideally,

we would like to dynamically identify stragglers and enable them to showcase the training time that

is closer to the non-straggler devices.

5

Figure 2.1: The relative difference in accuracy between an ideal implementation of Federated
Learning as compared to Ordered Dropout. Overall, Ordered Dropout shows up to 4.5%
point drop in accuracy.

Figure 2.2: The per-iteration training time of client devices. We use ﬁve android-based mobile
devices with varying performance characteristics. The choice of straggler is not ﬁxed and
it depends on the dataset and ML model. Furthermore, depending on the dataset, the per-
iteration training time can vary from <10 seconds to >100 seconds.

6

Sub-Model SizeCIFAR10FEMNISTSHAKESPEAREGoogle  Pixel 4Samsung  Galaxy S10Samsung  Galaxy S9LG  Velvet 5GGoogle  Pixel 3100101102Per Iteration Training Time in Seconds  (log scale)ShakespeareCIFAR10FEMNISTFigure 2.3: The high-level ﬂow of Invariant Dropout. The non-stragglers inform the global
model of the neurons that are not updated within a set threshold. Thereafter, as the
training process continues, sub-models are dynamically created by dropping invariant
neurons. These sub-models are sent to the straggler devices.

7

`Global ModelClientsStartIs Training Iteration < 2Training Iteration = 0Identify StragglerEstimatetarget SpeedupYESTraining Iteration++Converge onThe value of r (drop rate)Initialize the value of d (drop threshold)NOIs Training Complete?Aggregate updates from all clientsSend Sub-Model to Straggler(s)Vary the value of d andcreate a new Sub-Model Training Iteration++StopYESCheck the updates from non-stragglers for invariant neurons Send the entire model to non-stragglersNOPerformance OverheadClient-0Client-NStragglerGlobal ModelNon-Straggler ClientsChanged below dSub-ModelStraggler ClientsGlobal ModelNon-Straggler Clients3. Design: Invariant Dropout

This thesis introduces Invariant Dropout, a technique that lets straggler clients train with a sub-

model that is dynamically generated from neurons that ‘vary’ over time. Figure 2.3 shows the ﬂow

of Invariant Dropout. Invariant Dropout consists of three steps.

3.1 Identifying the Performance of Stragglers

Invariant Dropout initially executes the global model on every client, including stragglers. This

step occurs in the ﬁrst training iteration of each dataset and ML model. After the ﬁrst iteration, the

global server samples the performance delay between the slowest client’s (straggler) training time
(Tstraggler) and the target training time (Ttarget). When there is only one straggler, Ttarget is set as
the training time of the ‘next slowest client’. The Speedup that’s required for the stragglers can be
computed using Equation 3.1.

Speedup =

Tstraggler
Ttarget

(3.1)

3.2 Tuning the Performance of Stragglers

The dropout rate, denoted as r, is varied based on the value of Speedup. The value of r is between
0 and 1, and it determines the sub-model size that is trained on the straggler. r is chosen such that
the updated straggler training time, denoted as Tstraggler−new, and can be derived from Equation 3.1
by enabling Speedup to be close to 1; as shown in Equation 3.2.

Tstraggler−new ≈ Ttarget

(3.2)

The dropout rate is kept constant for all layers in the model. During each training iteration, the

server constructs the sub-model by dropping (1-r)% of the neurons per layer.

8

3.3 Drop Threshold

Invariant Dropout dynamically maintains an update threshold value, below which a neuron can be

classiﬁed as invariant. We deﬁne the percent difference of a neuron’s weight update. Assume the
wi jc(t) represents the set of weight parameters of the ith neuron in the jth layer for client c after the
training iteration t. The percent difference d of the the neuron, for each client c, is the minimum
value of d as denoted by Equation 3.3.

d ≥

wi jc(t) − wi j(t − 1)
wi j(t − 1)

(3.3)

Neurons whose weight updates lie within d as compared to the previous training iteration are

chosen as potential drop candidates. Unfortunately, this is non-trivial to implement.

3.4 Dynamically Identifying Drop Thresholds

The global server cannot determine the drop candidates by simply checking the updates from the

stragglers. This is because, the stragglers only train on a sub-model and therefore would only update

parts of that sub-model. To overcome this concern, the global server uses the insight that the non-

straggler clients train on the complete global model and therefore have access to all the neurons. The
non-stragglers clients can therefore be used to identify neurons whose weight updates lie within d,
at a per-iteration level. Invariant Dropout prioritizes dropping neurons whose weight updates fall
within d for the majority of the non-straggler clients and dynamically create a sub-model for each
training iteration.

Furthermore, the initial value of d is chosen such that it is average of the minimum percent
update out of all neurons in the layer in the ﬁrst training iterations. We observe that the percent

difference of neurons change drastically during the initial stages of training. Thus, Invariant Dropout

conservatively maintains the same update threshold in the initial training iterations. The threshold is

then incrementally increased after each training iteration until the number of neurons that fall under

the threshold becomes equal to or greater than the number of neurons that need to be left out of the

sub-model – as dictated by Equation 3.2. It should be noted that Invariant Dropout enables the drop

threshold to be different for each layer of the model. This is because it can track the magnitude of

weight updates for each individual layer. Finally, to ensure that fully trained neurons are dropped

instead of critical weights, the algorithm prioritizes leaving out neurons if they repeatedly fall within

the threshold in previous iterations.

9

4. Convergence of Invariant Dropout

Similar to the prior work [33], although dropout can reduce the communication cost of stragglers

but can increase the gradient variance. We prove convergence of our invariant dropout technique
using bounded gradient variance [33, 37]. Consider N devices participating in Federated Learning.
The training data is represented as {xn}N
n=1, where n represents a device, and each one has a loss
function { fn}N

n=1. We solve to the minimize the loss function using the following optimization:

f (w) :=

1
N

N
∑
n=1

fn(w)

wt+1 = wt − ηt(g(wt))

(4.1)

(4.2)

The invariant dropout problem can be viewed as a sparse stochastic gradient vector, where
each gradient has a certain probability to be dropped. The gradient vector is G = [g1, g2, ..., gm]
where g ∈ R with probability of staying and being transmitted across the networks as [p1, p2, ..., pm].
Hence, the probability of dropout is 1 − pi, where 0 < i ≤ m. The sparse vector for the stragglers is
represented as Gs. The variance of the invariant-dropout based gradient vector can be represented
as follows:

E

m
∑
i=1

[(Gs)]2 =

m
∑
(
i=1

g2
i
pi

)

(4.3)

The variance of the dropout vector is a small factor deviation from the non-dropout gradient

vector represented as follows:

min

m
∑
i=1

pi :

m
∑
(
i=1

g2
i
pi

) = (1 + ε)

m
∑
i=1

g2
i

(4.4)

This aims to minimize the probability of selecting gradients, i.e. higher beneﬁts from skipping

gradients, whilst ensuring the variance being small deviation from the dense gradient vector.

10

4.1 Proof of convergence:

With the invariant dropout, the weights are dropped based on their ‘percentage’ change across iter-

ations which essentially implies that weights corresponding to higher gradient values have a lower
probability of being dropped and vice-versa. This implies if |gi > g j| then pi > p j. The probability
of a gradient getting dropped is proportional to the invariant dropout rate r. Lets assume, in this case
the top-k magnitude of gradient values are not dropped, hence if G = [g1, g2, ..., gm] is assumed to
be sorted, then G = [g1, g2, ..., gk] has a p = 1, whereas the G = [gk+1, gk+2, ..., gd] have a probability
of pi = r|gi|. This modiﬁes the optimization problem in Equation 4.4 to the following:

k
∑
i=1

g2
i +

m
∑
i=k+1

|gi|
r

− (1 + ε)

m
∑
i=1

g2
i = 0, r|gi| ≤ 1

Which implies that

r =

As per the constraint r|gi| ≤ 1:

∑m
(1 + ε) ∑m

i=k+1 |gi|
i=1 g2

i − ∑k

i=1 g2
i

|gi|(

m
∑
i=k+1

|gi|) ≤ (1 + ε)

m
∑
i=1

g2
i −

k
∑
i=1

g2
i

(4.5)

(4.6)

(4.7)

In this invariant dropout work we keep the gradients with highest magnitude, thus if the number
of gradients. As per [33], if k << d the boundedness of the expected value from equation 4.4 can
be represented as:

m
∑
i=1

pi, =

k
∑
i=1

pi +

m
∑
i=k+1

pi

m
∑
i=1

pi = k + |gi|

m
∑
(
i=k+1

∑m
(1 + ε) ∑m

i=k+1 |gi|
i=1 g2

i − ∑k

i=1 g2
i

m
∑
i=1

pi ≤ k(1 + ε)

)

(4.8)

(4.9)

(4.10)

This proves that the variance of gradient is bounded by Equation 4.10.

11

5. Evaluation Setup

We describe our models, datasets, system conﬁgurations, evaluation metrics, and baselines.

5.1 Models and Dataset

Similar to state-of-the-art work Ordered Dropout [16], we evaluate on three datasets: FEMNIST,

Shakespeare from the LEAF datasets [5], and CIFAR10 [19].

5.1.1 The FEMNIST Dataset

For this dataset, we use a CNN with two 5x5 CONV layers (the ﬁrst one has 16 channels, the second
one has 64 channels, each of them followed with 2×2 max-pooling), an FC dense layer with 120
units, and a ﬁnal softmax output layer. The data is partitioned based on the writer of the character

in non-IID setting.

5.1.2 The Shakespeare Dataset

For this dataset, we consider a two-layer LSTM classiﬁer containing 128 hidden units. Model takes

a sequence of one hot encoded 80 characters as input, and the output is a class label between 0 and

80. The data is partitioned based on each role in a play in non-IID setting.

5.1.3 The CIFAR10 Dataset

For this dataset, we use a VGG-9 [29] architecture model with 6 3x3 CONV layers (ﬁrst 2 have
32 channels, followed by two 64 channel layers and lastly two 128 channel layers), two FC dense
layers with 512 and 256 units and a ﬁnal softmax output layer. The data is randomly partitioned

into equal sets, hence remaining IID in setting.

5.2 System Conﬁguration

Table 5.1 provides the details on the phones used for the experiments. We connect all our clients

devices and our server over the same network. We evaluate a total of ﬁve clients and identify one

12

straggler per training run. All the clients run on android mobile phones from the years 2018 to 2020.

The server runs separately on a windows machine.

Table 5.1: Software-Hardware Speciﬁcations of Clients

Device
Google Pixel 4

Year Android OS Version CPU (Cores)
2019

12

Google Pixel 3

2018

Samsung Galaxy S10

2019

Samsung Galaxy S9

2018

LG Velvet 5G

2020

9

11

10

10

1×2.84 GHz Kryo 485 + 3×2.42 GHz
Kryo 485 + 4×1.78 GHz Kryo 485
4×2.5 GHz Kryo 385 Gold + 4×1.6
GHz Kryo 385 Silver
2×2.73 GHz Mongoose M4 + 2×2.31
GHz Cortex-A75 + 4×1.95 GHz
Cortex-A55
4×2.8 GHz Kryo 385 Gold + 4×1.7
GHz Kryo 385 Silver
1×2.4 GHz Kryo 475 Prime + 1×2.2
GHz Kryo 475 Gold + 6×1.8 GHz
Kryo 475 Silver

Invariant Dropout is implemented on top of the Flower (v0.18.0) [3] framework and TensorFlow

Lite [12] from TensorFlow v2.8.0 [1]. Each client executes its training run as an Android applica-
tion. Models are deﬁned using TensorFlow’s Sequential API, and then converted into .tflite
formats using TensorFlow Lite’s Transfer Converter. Since the sub-model size for stragglers is dy-

namically determined at runtime, all model deﬁnitions for each possible sub-model size need to be

included in our Android application as assets. When a client receives a sub-model during training,
the client will load the .tflite ﬁles for that speciﬁc model size.

5.3 Evaluation Metrics

We report average performance (wall-clock training time), accuracy, and standard deviations across

three execution runs for all experiments. During each round of evaluation, all clients receive the

global model, and report the evaluation accuracy and loss on its local data to the server. The server

calculates the overall distributed accuracy and loss by performing a weighted average based on the

number of testing examples for each client. To reduce the effects of over-ﬁtting, the overall accuracy

for each run is determined by the accuracy of iteration with the lowest evaluation loss.

13

5.4 Baselines

We compare Invariant Dropout with the following two baselines: 1) Random federated dropout [6]

and 2) Ordered dropout from Fjord [16].

14

6. Results and Analysis

6.1 Accuracy Evaluation

We evaluated the accuracy of Invariant Dropout for different sub-model sizes and compared the re-
sults to the two baselines. Table 6.1 presents the average achieved accuracy (µ) with standard devi-
ation (σ ) for three different datasets. Speciﬁcally, Invariant Dropout outperforms Random Dropout
across all three datasets. As compared to Random Dropout, Invariant Dropout achieves a maxi-

mum accuracy gain of 1.5% points and on average 0.7% point higher accuracy for FEMNIST, 1.1%

point higher accuracy for CIFAR10 and 0.3% point higher accuracy for Shakespeare datasets. In-

variant Dropout also achieves a higher accuracy against Ordered Dropout across all three datasets.

As compared to Ordered Dropout, Invariant Dropout with a maximum of 1.4% point increase in

accuracy. On average, Invariant dropout has 0.3% point higher accuracy for FEMNIST, 0.2% point

higher accuracy for CIFAR10, and 0.4% point higher accuracy for Shakespeare. Invariant Dropout
also achieves smaller σ between runs of the same sub-model size and across all sub-model sizes.
Furthermore, accuracy values do not suddenly spike or drop signiﬁcantly between sub-model sizes.
The accuracy improvements of Invariant Dropout was veriﬁed to be signiﬁcant at r < 0.05.

Table 6.1: Accuracy Comparison of Random Dropout, Ordered Dropout, and Invariant
Dropout. The text in red indicates instances when Invariant Dropout showcases the
highest accuracy. (µ = mean, σ = standard deviation, and r = sub-model as a fraction of
the global model).

Dataset

Dropout Method

FEMNIST

CIFAR10

Shakespeare

Random
Ordered
Invariant
Random
Ordered
Invariant
Random
Ordered
Invariant

r = 0.95
Accuracy (µ) σ
80.6
80.6
81.1
51.9
52.3
52.9
43.3
42.9
43.6

0.1
0.2
0.3
1.3
1.6
0.2
0.1
0.1
0.1

r = 0.85
Accuracy (µ) σ
80.5
80.5
80.9
51.5
51.6
53.0
42.5
42.3
42.5

0.2
0.2
0.1
0.5
0.4
0.1
0.1
0.1
0.1

r = 0.75
Accuracy (µ) σ
80.3
80.4
80.8
52.6
54.6
53.4
42.4
42.2
42.6

0.2
0.2
0.2
0.3
0.1
0.2
0.1
0.2
0.2

r = 0.65
Accuracy (µ) σ
79.3
80.3
80.3
51.7
53.1
53.0
41.8
41.9
42.2

0.5
0.1
0.4
1.0
1.6
0.2
0.2
0.1
0.2

r = 0.5
Accuracy (µ) σ
79.2
79.7
80.1
52.2
53.0
53.2
41.3
41.4
41.7

0.3
0.3
0.3
0.3
0.2
0.3
0.1
0.1
0.1

15

Figure 6.1: The reduction in training time for stragglers before and after applying Invariant
Dropout. Without Invariant Dropout, the straggler would execute between 10% to 32%
of the target time. After applying Invariant Dropout, the straggler runs within 10% of the
target time.

6.2 Computational Performance Evaluation

We evaluated the training time reduction when Invariant Dropout dynamically selects the sub-model

for the straggler. Figure 6.1 shows that Invariant Dropout correctly selects the sub-model size so

that the stragglers’ training time is almost similar to that of the next slowest client. Without Invari-

ant Dropout, the straggler would execute between 10% to 32% of the target time. After applying

Invariant Dropout, the straggler runs within 10% of the target time.

In general, the accuracy results show that in general, when the straggler trains with a larger

sub-model size, the overall accuracy for the global model is higher compared to that of a smaller

sub-model size. Thus, Invariant Dropout chooses the largest possible sub-model size that minimizes

training time variance across clients.

It should be noted that, executing the Invariant Dropout algorithm also does not add signiﬁ-

cant processing overhead to the overall training time. This is because, the additional computation

required such as creating and transforming sub-models for aggregation, are all done on a central-

ized server instead of edge devices. We empirically determined that this overhead on the server is

negligible compared to the training reduction – 50ms versus 0.8 seconds to 26 seconds.

16

Straggler(Reduced Training Time)Straggler(Reduced Training Time)Straggler(Reduced Training Time)CIFAR10FEMNISTSHAKESPEAREClientsFigure 6.2: The accuracy comparison of Invariant Dropout with Ordered and Random Dropout
as we scale to 50-100 clients with 20% of the slowest clients being stragglers. This
study simulates a large number of clients on a server cluster. Overall, Invariant Dropout
consistently outperforms Ordered and Random Dropout and maintains a better accuracy
proﬁle.

6.3 Scalability Studies

In order to evaluate the scalability of Invariant Dropout, we tested the technique on a large number

of simulated clients (50 to 100), also implemented using the Flower Framework. For the scalability

tests, we run all clients on NVIDIA Tesla V100 GPUS clusters, each machine running 10 to 20

clients in parallel. We identify 20% of the slowest clients as stragglers. Figure 6.2 shows the

accuracy performance across all three datasets. Overall, Invariant Dropout consistently outperforms

Ordered and Random Dropout and maintains a better accuracy proﬁle similar to Table 6.1.

17

CIFAR10FEMNISTSHAKESPEARE7. Related Work

In the domain of heterogeneous client optimization, several prior work have tried to mitigate the

effects of stragglers.

7.1 Server Ofﬂoading Strategies

To accelerate local training on computationally weak devices, Wu et. al., Ullah et. al., and Wang et.

al. propose ofﬂoading layers of deep neural networks to servers [30, 32, 34].

Wu et. al. [34] focused on reducing training time with heterogeneous networks with an adaptive

mechanism that ofﬂoads a part of the model to the server using split learning. Additionally, the

technique also adaptive selects the layers of the model that should be ofﬂoaded to the server based

on straggler capabilities.

Ullah et. al. [30] focused on device mobility while performing Federated learning and ofﬂoading

training to edge servers. When a edge device participates in Federated Learning, it is possible that

the client might move from one location to another. This would cause the client to move out of

range from the current edge server – this needs to be part of the ofﬂoading decision.

Wang et. al. [32] introduces a compression method for split learning (ofﬂoading to the server).

This aims to reduce the communication overheads for computation and communication resource-

limited edge clients.

Broadly, Invariant Dropout can be easily applied on the top of these techniques to determine

which part of the model needs to be ofﬂoaded to the server.

7.2 Communication Optimizations

To minimize overall learning time, Han et. al. have proposed an online learning algorithm that

determines the degree of sparsity based on communication and computation capabilities [14].

PruneFL [17] introduces an approach to adaptive select model sizes during FL to reduce com-

munication and computation overhead and overall training time.

Chen et. al. propose a communication-efﬁcient FL framework, using a probabilistic device

18

selection method, to select clients that beneﬁt the convergence speed and training loss [8]. As well,

the authors also proposed a quantization method to reduce communication overheads.

Xu et. al. propose a framework that incorporates overhead reduction techniques to enable

efﬁcient model training on resource limited edge devices [36]. The algorithm performs optimization

techniques like pruning, quantization, and prevents less helpful updates to the server.

Compared to these prior work, Invariant Dropout presents a new insight that avoids communi-

cation and computation overheads to ‘invariant’ neurons.

19

8. Conclusions and Future Work

Rapid technology development and variation among handheld devices has resulted in some devices

becoming stragglers. Straggler devices offer low computational performance and act as bottlenecks

in Federated Learning. They not only increase the training time but also reduce the accuracy of the

ML model. This thesis mitigates the performance and accuracy effects of stragglers by dynamically

creating tailored sub-models that contain only the neurons that change above a certain threshold.

Our technique, called Invariant Dropout, mitigates performance overheads while providing a 1.4%

point higher accuracy than the state-of-the-art technique, called Ordered Dropout, across three dif-

ferent datasets and ML models.

8.1 Future Work

Based on this work, we identiﬁed two potential areas of further study

8.1.1 Extensions of Invariant Dropout

This work has implemented Invariant Dropout with three datasets and experimented with ﬁve mobile

clients. In future works based on Invariant Dropout, we could investigate how the technique could

be extended to support a wider variety of datasets and models. As well, another potential area of

study is to explore the best way to set performance target time when there are multiple stragglers

in the FL network. It is also possible to customize the target time and sub-model size for each

identiﬁed straggler client.

8.1.2 Drop-Threshold Identiﬁcation Improvements

Currently, the Invariant Dropout technique initializes the drop-threshold based on characteristics of

the early training iterations, and simply increments the threshold by a ﬁxed value until the number

of invariant neurons is satisﬁed. While this method is simple and requires minimum computing

overhead, depending on the values of the initial threshold and the ﬁxed increment, the threshold

might settle at a value too large for larger sub-model sizes, or reach the suitable threshold too slowly

20

for smaller sub-model sizes. Hence the threshold setting algorithm could be further improved to

search for the target threshold in an even more dynamic fashion.

21

Bibliography

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,

G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,
P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorﬂow: A system
for large-scale machine learning. In Proceedings of the 12th USENIX Conference on
Operating Systems Design and Implementation, OSDI’16, page 265–283, USA, 2016.
USENIX Association. ISBN 9781931971331. → page 13

[2] S. Barkai, I. Hakimi, and A. Schuster. Gap-aware mitigation of gradient staleness. In

International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=B1lLw6EYwB. → page 1

[3] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques, Y. Gao, L. Sani, K. H. Li,
T. Parcollet, P. P. B. de Gusm˜ao, and N. D. Lane. Flower: A friendly federated learning
research framework, 2020. URL https://arxiv.org/abs/2007.14390. → page 13

[4] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage,

A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning.
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security, 2017. → page 4

[5] S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Koneˇcn´y, H. B. McMahan, V. Smith, and

A. Talwalkar. Leaf: A benchmark for federated settings, 2018. URL
https://arxiv.org/abs/1812.01097. → pages 2, 12

[6] S. Caldas, J. Koneˇcny, H. B. McMahan, and A. Talwalkar. Expanding the reach of federated

learning by reducing client resource requirements, 2018. URL
https://arxiv.org/abs/1812.07210. → pages 1, 2, 5, 14

[7] Z. Chai, Y. Chen, A. Anwar, L. Zhao, Y. Cheng, and H. Rangwala. Fedat: A

high-performance and communication-efﬁcient federated learning system with asynchronous
tiers. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, SC ’21, New York, NY, USA, 2021. Association for
Computing Machinery. ISBN 9781450384421. doi:10.1145/3458817.3476211. URL
https://doi.org/10.1145/3458817.3476211. → pages 1, 4

22

[8] M. Chen, N. Shlezinger, H. V. Poor, Y. C. Eldar, and S. Cui. Communication-efﬁcient

federated learning. Proceedings of the National Academy of Sciences, 118(17):e2024789118,
2021. doi:10.1073/pnas.2024789118. URL
https://www.pnas.org/doi/abs/10.1073/pnas.2024789118. → page 19

[9] Y. Chen, Y. Ning, M. Slawski, and H. Rangwala. Asynchronous online federated learning for
edge devices with non-iid data. In 2020 IEEE International Conference on Big Data (Big
Data), pages 15–24, 2020. doi:10.1109/BigData50022.2020.9378161. → page 1

[10] J. Ding, E. Tramel, A. K. Sahu, S. Wu, S. Avestimehr, and T. Zhang. Federated learning

challenges and opportunities: An outlook, 2022. URL https://arxiv.org/abs/2202.00807. →
page 4

[11] X. Gong, A. Sharma, S. Karanam, Z. Wu, T. Chen, D. S. Doermann, and A. Innanje.

Preserving privacy in federated learning with ensemble cross-domain knowledge distillation.
In AAAI, 2022. → page 4

[12] Google. Tensorﬂow lite — ml for mobile and edge devices. https://www.tensorﬂow.org/lite.

Accessed: 2022-08-14. → page 13

[13] I. Hakimi, S. Barkai, M. Gabel, and A. Schuster. Taming momentum in a distributed
asynchronous environment, 2019. URL https://arxiv.org/abs/1907.11612. → page 1

[14] P. Han, S. Wang, and K. K. Leung. Adaptive gradient sparsiﬁcation for efﬁcient federated
learning: An online learning approach. 2020 IEEE 40th International Conference on
Distributed Computing Systems (ICDCS), pages 300–310, 2020. → page 18

[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997. → page 5

[16] S. Horv´ath, S. Laskaridis, M. Almeida, I. Leontiadis, S. Venieris, and N. D. Lane. FjORD:
Fair and accurate federated learning under heterogeneous targets with ordered dropout. In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural
Information Processing Systems, 2021. URL https://openreview.net/forum?id=4fLr7H5D eT.
→ pages 1, 2, 4, 5, 12, 14

[17] Y. Jiang, S. Wang, B. Ko, W.-H. Lee, and L. Tassiulas. Model pruning enables efﬁcient
federated learning on edge devices. IEEE transactions on neural networks and learning
systems, PP, 2022. → page 18

[18] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz,
Z. Charles, G. Cormode, R. Cummings, R. G. L. D’Oliveira, H. Eichner, S. E. Rouayheb,
D. Evans, J. Gardner, Z. Garrett, A. Gasc´on, B. Ghazi, P. B. Gibbons, M. Gruteser,
Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi, G. Joshi,
M. Khodak, J. Koneˇcn´y, A. Korolova, F. Koushanfar, S. Koyejo, T. Lepoint, Y. Liu, P. Mittal,
M. Mohri, R. Nock, A. ¨Ozg¨ur, R. Pagh, M. Raykova, H. Qi, D. Ramage, R. Raskar, D. Song,

23

W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tram`er, P. Vepakomma, J. Wang, L. Xiong,
Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao. Advances and open problems in federated
learning, 2019. URL https://arxiv.org/abs/1912.04977. → pages 1, 4

[19] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009. → pages 2, 12

[20] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization

in heterogeneous networks, 2018. URL https://arxiv.org/abs/1812.06127. → page 4

[21] T. Li, A. K. Sahu, A. S. Talwalkar, and V. Smith. Federated learning: Challenges, methods,

and future directions. IEEE Signal Processing Magazine, 37:50–60, 2020. → pages 1, 4

[22] P. P. Liang, T. Liu, L. Ziyin, R. Salakhutdinov, and L.-P. Morency. Think locally, act globally:

Federated learning with local and global representations. ArXiv, abs/2001.01523, 2020. →
page 4

[23] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas.

Communication-efﬁcient learning of deep networks from decentralized data. In AISTATS,
2017. → pages 1, 4

[24] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private

recurrent language models. In ICLR, 2018. → page 4

[25] J. Nguyen, K. Malik, H. Zhan, A. Yousefpour, M. G. Rabbat, M. Malek, and D. Huba.

Federated learning with buffered asynchronous aggregation. In AISTATS, 2022. → page 1

[26] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn´y, S. Kumar, and H. B.

McMahan. Adaptive federated optimization. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=LkFG3lB13U5. → page 4

[27] V. Shankar and S. Balasubramanian. Mobile marketing: A synthesis and prognosis. Journal

of interactive marketing, 23(2):118–129, 2009. → page 1

[28] V. Shankar, A. Venkatesh, C. Hofacker, and P. Naik. Mobile marketing in the retailing

environment: current insights and future research avenues. Journal of interactive marketing,
24(2):111–120, 2010. → page 1

[29] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015. URL http://arxiv.org/abs/1409.1556. → page 12

[30] R. Ullah, D. Wu, P. Harvey, P. Kilpatrick, I. Spence, and B. Varghese. Fedﬂy: Towards

migration in edge-based distributed federated learning, 2021. URL
https://arxiv.org/abs/2111.01516. → page 18

[31] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor. Tackling the objective inconsistency

problem in heterogeneous federated optimization. ArXiv, abs/2007.07481, 2020. → page 4

24

[32] J. Wang, H. Qi, A. S. Rawat, S. Reddi, S. Waghmare, F. X. Yu, and G. Joshi. Fedlite: A

scalable approach for federated learning on resource-constrained clients, 2022. URL
https://arxiv.org/abs/2201.11865. → page 18

[33] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsiﬁcation for communication-efﬁcient

distributed optimization. In NeurIPS, 2018. → pages 10, 11

[34] D. Wu, R. Ullah, P. Harvey, P. Kilpatrick, I. Spence, and B. Varghese. Fedadapt: Adaptive

ofﬂoading for iot devices in federated learning, 2021. URL https://arxiv.org/abs/2107.04271.
→ pages 1, 18

[35] C. Xie, S. Koyejo, and I. Gupta. Asynchronous federated optimization, 2019. URL

https://arxiv.org/abs/1903.03934. → page 1

[36] W. Xu, W. Fang, Y. Ding, M. Zou, and N. N. Xiong. Accelerating federated learning for iot in

big data analytics with pruning, quantization and selective updating. IEEE Access, 9:
38457–38466, 2021. → page 19

[37] Z. Xu, F. Yu, J. Xiong, and X. Chen. Helios: Heterogeneity-aware federated learning with

dynamically balanced collaboration. In 2021 58th ACM/IEEE Design Automation Conference
(DAC), pages 997–1002, 2021. doi:10.1109/DAC18074.2021.9586241. → page 10

[38] G. Yan, H. Wang, and J. Y. Li. Seizing critical learning periods in federated learning. In

AAAI, 2022. → page 4

[39] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and F. Beaufays.
Applied federated learning: Improving google keyboard query suggestions, 2018. URL
https://arxiv.org/abs/1812.02903. → page 1

[40] B. Zhao, P. Sun, T. Wang, and K. Jiang. Fedinv: Byzantine-robust federated learning by

inversing local model updates. In AAAI, 2022. → page 4

[41] Z. Zhong, W. Bao, J. Wang, X. Zhu, and X. Zhang. Flee: A hierarchical federated learning

framework for distributed deep neural network over cloud, edge and end device. ACM Trans.
Intell. Syst. Technol., jan 2022. ISSN 2157-6904. doi:10.1145/3514501. URL
https://doi.org/10.1145/3514501. Just Accepted. → page 1

25

