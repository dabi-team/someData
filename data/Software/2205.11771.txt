Learning Context-Aware Service Representation for
Service Recommendation in Workﬂow Composition

Xihao Xie1, Jia Zhang1, Rahul Ramachandran2,Tsengdar J. Lee3, Seungwon Lee4
1Department of Computer Science, Southern Methodist University, USA
2NASA/MSFC, USA
3Science Mission Directorate, NASA Headquarters, USA
4NASA/JPL, USA
{xihaox,jiazhang}@smu.edu;{rahul.ramachandran,tsengdar.j.lee}@nasa.gov;seungwon.lee@jpl.nasa.gov

2
2
0
2

y
a
M
4
2

]
E
S
.
s
c
[

1
v
1
7
7
1
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—As increasingly more software services have been
published onto the Internet, it remains a signiﬁcant challenge
to recommend suitable services to facilitate scientiﬁc workﬂow
composition. This paper proposes a novel NLP-inspired approach
to recommending services throughout a workﬂow development
process, based on incrementally learning latent service repre-
sentation from workﬂow provenance. A workﬂow composition
process is formalized as a step-wise, context-aware service gen-
eration procedure, which is mapped to next-word prediction
in a natural language sentence. Historical service dependencies
are extracted from workﬂow provenance to build and enrich a
knowledge graph. Each path in the knowledge graph reﬂects
a scenario in a data analytics experiment, which is analogous
to a sentence in a conversation. All paths are thus formalized
as composable service sequences and are mined, using various
patterns, from the established knowledge graph to construct a
corpus. Service embeddings are then learned by applying deep
learning model from the NLP ﬁeld. Extensive experiments on the
real-world dataset demonstrate the effectiveness and efﬁciency of
the approach.

Index Terms—service representation, service recommendation,

workﬂow composition

I. INTRODUCTION

In recent years, increasingly more software programs have
been deployed and published onto the Internet as reusable
web services, or so-called APIs. Scientiﬁc researchers can
thus leverage and compose existing services to build new
data analytics experiments, so-called scientiﬁc workﬂow or
workﬂow in short [1].

From a practical perspective, exploiting existing services
instead of reinventing the wheel will lead to higher efﬁciency.
However, our studies over the life science ﬁeld [2], which is
one of the very ﬁelds that adopt the concept of software ser-
vice, revealed that the reusability rate of life science services
in workﬂows remains rather low. Until 2018, less than 10% of
life science services published at biocatalog.org [3] were ever
reused in scientiﬁc workﬂows, according to myExperiment.org
[4]. This means that most of the published life science services
have never been reused by anyone.

What is wrong? Why scientists do not like to leverage
others’ work? Earlier studies believe one of the major obstacles
making researchers unwilling to reuse existing services is
the data shimming problem [5] [6], meaning preparing and
transforming data types to feed in the required inputs of a

Fig. 1. Motivating example workﬂow #1794 in myExperiment.org

downstream service. For example, a NASA Climate Model Di-
agnostic Analyzer (CMDA) service, 2-D Variable Zonal Mean
service that generates a graph of a 2-dimensional variable’s
zonal mean with time averaging, requires more than a dozen
of input parameters [5]. Unless a user fully understands each
of the parameters, both of its syntactic and semantic meaning
and requirements, he/she may not feel comfortable to reuse
the service.

Our research project moves one step further, to argue that
the adoption of a software service is not only dependent on
its direct upstream service, but also the context of all selected
services in the current workﬂow under construction. Take the
workﬂow #17941 from myExperiment.org in Fig. 1 as an
example. As highlighted in red oval, three services seqret,
emma, and plot are used sequentially to fetch a sequence set,
do multiple alignments, and then plot the results. The adoption
of the third service plot aims to visualize the outcome from
the two sequential services upstream in the workﬂow.

To tackle the shimming problem in the context of service

1https://www.myexperiment.org/workﬂows/1794.html

 
 
 
 
 
 
recommendation, our earlier studies constructed a knowledge
graph [4] [7] from all historical data of service invocations,
i.e., service provenance, extracted from workﬂow repositories
such as myExperiment.org. Various random walk-powered
algorithms were developed to traverse through the knowledge
graph and suggest possible service candidates during workﬂow
development [4] [7].

In order to add into consideration of workﬂow-contextual
information for more precise service recommendation in a
recommend-as-you-go manner, this research presents a novel
machine learning technique over service provenance knowl-
edge graph. Inspired by the latest advancements in Natural
Language Processing (NLP), we formalize the problem of ser-
vice recommendation as a problem of next service prediction,
where services and workﬂows are considered as “tokens” and
“sentences” in NLP, respectively. In this way, our goal has
turned into predicting and recommending the next suitable
services that might be used for a user during the process of
service composition.

The topic of next item prediction and recommendation has
been well studied, and the literature has witnessed many suc-
cessful applications in real-world ﬁelds such as e-commerce
[9] [10], keyboard prediction [11] and sequential click pre-
diction [12]. In general, two distinct categories of approaches
exist to recommend next items in a sequential context. Tradi-
tional approaches are typically based on Markov chain (MC).
For example, Rendle et al. [13] modeled sequential data by
learning a personalized transition graph over underlying MC
to predict and recommend items that the user might want to
purchase. In recent years, increasingly more deep learning-
based approaches have been proposed for next item recom-
mendation [9] [14]. Particularly, in NLP, the Word2Vec [15]
model has achieved great success in learning the probability
distribution of words to predict potential context based on
sequential sentences.

Inspired by Word2Vec, we leverage the skip-gram model
to learn latent representation of services and their relation-
ships for context-aware workﬂow recommendation. A corpus
of “sentences” (i.e., service chains extracted from historical
workﬂow provenance) are generated from the knowledge
graph. Our rationale is that, each path (i.e., a sequence of
services) in a workﬂow reﬂects a scenario of data analytics
experiment, which is analogous to a sentence in a conversation.
Thus, all paths are extracted from the knowledge graph, analo-
gous to all sentences are accumulated to train a computational
model from NLP. In other words, a service sequence carries
context when a service was invoked in the past.

To the best of our knowledge, we are the ﬁrst attempt to
seamlessly exploit the state-of-the-art machine learning tech-
niques in both NLP and knowledge graph to facilitate service
recommendation and workﬂow composition. Our extensive
experiments over real-world dataset have demonstrated the
effectiveness and efﬁciency of our approach. In summary, our
contributions are three-fold:

• We formalize the service recommendation problem as a

problem of context-aware next service prediction.

• We develop a technique to generate a corpus of service-
oriented-sentences (service sequences) by traversing over
the knowledge graph established from workﬂow and
service usage provenance.

• We develop an approach to learning service representa-
tions ofﬂine based on sequential context information in
accumulated knowledge graph and then recommending
potential services real-time.

The remainder of this paper is organized as follows. Section
II discusses the related work. In section III, we present our
context-aware service recommendation technique. In section
IV, we present various ways of service chain corpus genera-
tion. Then, we discuss and analyze the experimental results in
section V. Finally, section VI draws conclusions.

II. RELATED WORK

Our work is closely related to two categories of research
in the literature: service recommendation and representation
learning.

A. Service Recommendation

Service composition remains a fundamental research topic
in services computing community. Paik et al. [16] decomposed
service composition activities into four phases: planning,
discovery, selection and execution. Service recommendation
represents a core technology in the third phase.

Zhang et al. [7] modeled services, workﬂows and their
relationships from historical usage data into a social network,
to proactively recommend services in a workﬂow composition
process. In their later work [4], they developed an algorithm
to extract units of work (UoWs) from workﬂow provenance
to recommend potentially chainable services. Chowdhury et
al. [17] recommended composition patterns based on partial
mashups. IBM’s MatchUp [18] and MashupAdvisor [19] rec-
ommended reusable workﬂow components based on user con-
text and conditional co-occurrence probability, respectively.
By leveraging NLP techniques, Xia et al. [20] proposed a
category-aware method to cluster and recommend services for
automatic workﬂow composition. Shani et al. [8] formalized
the problem of generative recommendation as a sequential
optimization problem and applied Markov Decision Processes
(MDPs) to solve it.

In Business Process Management (BPM) community, a
number of research work have explored methods to recom-
mend reusable components for workﬂow composition. Vis-
Complete [21] recommended components in a workﬂow as
a path extension by building graphs for workﬂows. Deng
et al. [22] extracted relation patterns between activity nodes
from existing workﬂow repository to recommend extending
activities. Zhang et al. [23] mined the upstream dependency
patterns for workﬂow recommendation. Similarly, Smirnov et
al. [24] speciﬁed action patterns using association rule mining
to suggest additional actions in process modeling.

B. Representation Learning

Representation learning, also known as feature learning, is
a fundamental step for knowledge mining and downstream ap-
plications. Data representations matter for general data-driven
algorithms and activities [25]. Many research communities
have leveraged machine representation learning techniques on
diverse data types to support real-world applications such
as speech recognition [26], signal processing [27], language
modeling [28] and semantic web [29].

In the services computing ﬁeld, earlier work mainly focused
on learning service representation based on natural language
texts of service proﬁles. Semantic web is one of the technolo-
gies to build representations for service functions and proper-
ties. Li et al. [30] proposed an approach to recommending
services by analyzing semantic compatibility between user
requirements and service descriptions. Since Latent Dirichlet
Allocation (LDA) was proposed in [31], topic modeling has
become a widely used method for learning service representa-
tion. Zhong et al. [32] applied the Author-Topic Models [33]
to extract words as service description from mashup proﬁles.
Zhang et al. [34] developed a tailored topic model to learn
service representation for accurate visualization. Service2vec
[41] constructed a service network [42] and applied Word2Vec
modeling technique to extract contextual similarity between
web services.

In contrast, our work differs from current

literature of
service representation and recommendation in three signiﬁcant
aspects. First, we believe that service usage context hides
in paths in workﬂow provenance. Second, we formalize the
problem of service recommendation as a problem of next
service token prediction in sequential context and have ex-
plored different sequence generation strategies. Third, from
another perspective of service representation, we learn service
representation from the context and sequential dependencies
of services in workﬂows instead of proﬁle descriptions of
services.

III. CONTEXT-AWARE SERVICE RECOMMENDATION
APPROACH

In this section, we ﬁrst depict the blueprint of our method.
Then we formalize the research problem, followed by knowl-
edge graph construction, ofﬂine service representation learning
and online service recommendation.

Fig. 2 presents the high-level blueprint of our methodology.
Based on workﬂow provenance (a), we construct a knowledge
graph (b) by extracting service dependencies from workﬂow
structure. Second, we generate sequences of service tokens (c)
from the knowledge graph. Third, we use language modeling
techniques to learn latent representations of services (d). As
shown in Fig. 2, the above steps are conducted ofﬂine. The
results of the ofﬂine learning phase will support the online
recommendation at run-time. Given a workﬂow which is under
composition online (e), we rank the potential services respect
to their semantic similarities in the context (f), and recommend
top-K of them (g) to the user at real time. Note that once
a new workﬂow is completely composed, it will be saved

into the workﬂow repository thus to trigger incremental ofﬂine
learning.

A. Problem Deﬁnition

We consider the problem of service recommendation as
a problem of predicting next service token, over a corpus
of service sequences. The methods to generating service
sequences from the knowledge graph, as shown in Fig. 2, will
be discussed in detail in the next section.

Let W = (cid:8)w1, w2, ..., w|W|

(cid:9) denote a set of workﬂows,
(cid:9) be a set of available software compo-
C = (cid:8)c1, c2, ..., c|C|
(cid:9) be a set of service
nents, i.e., services, T = (cid:8)t1, t2, ..., t|T |
tokens. Note that each service token represents a subset of
services, where an example may be a unit of work (several
services always used together) [4]. In other words, the concept
of service token is analogous to word phrases in NLP. Let
list Sw(cid:48) =
denote a service sequence
|Sw(cid:48) |
with length as |Sw(cid:48)| for workﬂow w(cid:48) /∈ W which has not
been completed yet, where sw(cid:48)
is the service token tk ∈ T
k
generated at time step k and tk ⊆ C is a subset of services.
Given a speciﬁc existing service sequence Sw(cid:48) of a workﬂow
w(cid:48) under construction, our goal is to calculate the probability
over all service tokens t ∈ T at next time step |Sw(cid:48)| + 1:

sw(cid:48)
1 , ..., sw(cid:48)

k , ..., sw(cid:48)

(cid:104)

(cid:105)

pt = p(tw(cid:48)

|Sw(cid:48) |+1 = t|Sw(cid:48))

(1)

and recommend a tokens list T ⊆ T with top highest n
probability values, where n is the size of the list T .

B. Knowledge Graph Construction

In this subsection, we introduce the details of constructing
the web service knowledge graph (WSKG) from workﬂow
repository, which will serve as the foundation for service repre-
sentation learning and recommendation. Similar to our earlier
method described in detail in [7], WSKG is a directed graph
(digraph) carrying historical service invocation dependencies
extracted from workﬂow repository, and it is deﬁned as:

W SKG = (cid:104)C, R(cid:105)

(2)

i,j

where each software service c ∈ C is regarded as an entity, and
(cid:9) is a set of relationships between service entities.
R = (cid:8)rw
rw
i,j = (cid:104)ci, cj, w(cid:105) refers to a relationship that ci is an upstream
service of cj in workﬂow w. We can regard the relationship
rw
i,j as an edge starting from ci and ending at cj with label w.
Note that there might be multiple edges between two service
nodes in the knowledge graph with different labels, meaning
that such a service invocation dependency happens in multiple
workﬂows.

As shown in Fig. 2, the constructed WSKG serve for both
the ofﬂine learning phase and the online recommendation
phase. For the ofﬂine phase, the constructed WSKG allows
us to generate all service token sequences, by traversing the
WSKG following the comprising paths. In section IV, we will
further discuss various strategies of generating service token
sequences in WSKG. All service token sequences together will
form a corpus to learn latent service token representations.

Fig. 2. Blueprint of proposed approach. (a) Workﬂow repository. (b) Constructed knowledge graph. (c) Generated service token sequences according to different
generation strategies. (d) Trained Skip-gram model ofﬂine. (e) Real-time workﬂow under construction. (f) Potential services with the product of successor
probability and service similarity in descending order online. (g) Top-K recommended candidate services list. Operations from (a) to (d) are conducted in the
ofﬂine training phase and (e)-(f) is the online recommendation phase.

In the online recommendation phase, the structural relation-
ships carried in WSKG can be leveraged to enhance service
ranking. For example, such information will help empirically
model the probability of service token t appearing as a suc-
cessor of service c in a partial workﬂow under development.

C. Ofﬂine Service Representation Learning

The ofﬂine representation learning phase aims to learn
latent service representations over the service sequence corpus.
Two models, skip-gram and CBOW in Word2Vec, are widely
applied neural network models in NLP to learn word repre-
sentations given a corpus of sentences [15]. In our study, we
decide to employ the skip-gram model mainly because it works
well with small amount of training data and represents well
even for rare tokens [43]. In our scenarios, existing scientiﬁc
workﬂow datasets are not so large as that in the ﬁeld of NLP.
Furthermore, the signiﬁcant amount of long-tail services (e.g.,
newly published services) may be able to receive reasonable
exposure using the skip-gram model, although they rarely
appear in the service sequence corpus.

Applying the skip-gram model [15], we turn our formalized
problem (1) into an optimization problem of maximizing the
probability of any service token appearing in the on-going
service sequential context:
(cid:88)

log p (Sti|Ω (ti))

(3)

max
Ω

ti∈T

where Sti = [ti−w, ..., ti−1, ti+1, ..., ti+w] is the sequential
context of token ti ∈ T , w is the window size and Ω (ti) ∈ Rd
is the learnt service representations in form of vectors.

Based on an independence assumption, the probability of

(3) can be approximated as:

p (Sti|Ω (ti)) =

i+w
(cid:89)

j=i−w,j(cid:54)=i

p (tj|Ω (ti))

(4)

Since computing p (tj|Ω (ti)) is time consuming and hardly
feasible, we decide to employ Hierarchical Softmax [35] to
speed up the training process. Speciﬁcally, we build a Huffman
tree whose leaves are service tokens. For any leaf node nk,
one path [n0, n1, ..., nhk ] exists from the root n0 to it, where
hk is the length of the path and limits to (cid:100)log |T |(cid:101). In this
way, we can approximate the probability in (4) as:

p (tj|Ω (ti)) =

hk(cid:89)

k=1

p (nk|Ω (ti))

(5)

As for p (nk|Ω (ti)), we view it as a binary classiﬁcation

problem and calculate it as in (6):

p (nk|Ω (ti)) =

1
1 + exp(−ΩT (nk) · Ω (ti))

(6)

As a result, the time complexity of computing p (tj|Ω (ti))
can be reduced from O (|T |) to O (log |T |). Algorithm 1
illustrates the step-wise procedure of ofﬂine representation
learning. Note that during the training process, we update
vectorized representations with stochastic gradient descent, as
shown in lines 6 and 7.

D. Online Service Recommendation

As shown in Fig. 2, at each step of the process of composing
a new workﬂow, service token tl in current user session can
be used as the input to trigger recommendation for the next
service token. After that, the online recommendation engine
ranks all potential service tokens according to their similarities
to tl and recommend the top-K of them as candidates to
the user. The user will then select a service token from the
recommended list to move the composition process forward.
Note that a service token in our work might be a single service
or a bundle of services due to different sequence generation
strategies, which will be discussed in the next section. That

means each entry of the recommended list might contain one
or more services. If an entry is comprised of multiple services,
the user can further select one or more services from the
entry to continue the composition. A good analogy of a token
containing multiple services is a phrase in natural language. In
this case, recommending a group of services is like predicting
the most probable next phrase in language writing [11].

Following the skip-gram model, given a speciﬁc sequence
of service tokens [tl−u, ..., tl−1, tl], the ofﬂine learnt service
representations enable us to identify the top-K most relevant
service tokens to be the most potential service tokens following
tl. According to the skip-gram model, though, the predicted
contextual service tokens may not only be upstream of tl in
some workﬂows, but also be downstream service tokens as
well. However, in the scenario of workﬂow composition, what
we need to recommend is the service tokens that are potentially
appear after tl. Therefore, we deﬁne a scoring function which
is used to rank next potential service tokens in descending
order as follows:

score(tl, t) = psuc(tl, t) × sim(tl, t)

psuc(tl, t) =

exp(Nsuc(tl, t))
exp(Npre(tl, t)) + exp(Nsuc(tl, t))

(7)

(8)

where psuc(tl, t) empirically models the probability that t
appears after tl, Nsuc(tl, t), Npre(tl, t) are the numbers of
occurrences that any service c ∈ t appeared in repository as a
successor or a precursor service of tl, respectively. sim(tl, t)
is the similarity between tl and t that can be calculated from
the service representations learnt ofﬂine.

IV. SERVICE SEQUENCES GENERATION

The input of the used skip-gram representation learning
model is a corpus of service sequences, each of which is
comprised of service tokens. As mentioned above, how to
generate sequences of service tokens from WSKG is the
key step in our method. Different generation strategies may
result in different recommendation. In this section, we explore
three different ways, each of which might be adapt
to a
speciﬁc scenario, to generate sequential service tokens from
the constructed knowledge graph WSKG that is introduced in
the previous section.

The three generation methods are: DFS-based generation,
BFS-based generation, and PW-based generation. Fig. 3 is a
portion extracted from the constructed WSKG that motivates
and will be used to explain the three service sequence gen-
eration methods. The nodes in green represent the services
in workﬂow #9412 published at myExperiment.org. Nodes
in other colors illustrate some downstream services from
other workﬂows recorded in WSKG. Edges are labeled with
corresponding id numbers of workﬂows.

2https://www.myexperiment.org/workﬂows/941.html

Algorithm 1 Ofﬂine Learning Service Representation
Input: workﬂows W, windows size w, dimension size d and

sequence generation type T

Output: vectorised service representations Ω ∈ R|C|×d
1: Initialize Ω ∈ R|C|×d
2: D ← GenerateSequences(W, T)

// generate service token sequences according to speciﬁc gen-
eration type T, see in next section.

3: for each sequence S ∈ D do
for each token ti ∈ S do
4:
5:
6:
7:
8:
9:
10: end for

for each tj ∈ S [i − w : i + w] do
J (Ω) = − log p (tj|Ω(ti))
Ω = Ω − η × ∂J

end for

end for

∂Ω // η is the learning rate.

A. Depth First Search (DFS) based Generation

In this method, we consider a workﬂow composition process
as a path extension process from start to end. Particularly, we
consider individual workﬂows. In order to model the sequential
behaviour of composing a speciﬁc workﬂow w ∈ W, depth
ﬁrst search method is applied in WSKG, along the path
with the label w to generate sequences of service tokens.
w−→
Speciﬁcally, for any service entity ci ∈ C, let Sw
ti
w| denote a generated sequence starting from
2
ci and going along the path with label w, representing a
workﬂow, to next unvisited neighbor services until meeting
terminal services which have no successor services along label
(cid:12)
w. Here ti
(cid:12) = 1, which means
every token is a single service. m ∈ [1, M ] and M is the total
number of sequences starting from ci.

j ∈ T and ∀ti

w−→ ... w−→ ti

j has (cid:12)
(cid:12)ti
j

1 = ci, ti

i,m = ti
1

|Si

Note that for a workﬂow with label w, the sequences are
generated from not only its starting services but also all
intermediate services. In this way, the generated sequences
can cover as many as possible sequential dependencies among
services in the workﬂow w.

Take the workﬂow #941 from myExperiment for a sim-
ple example. For illustration purpose, Fig. 3 uses s1, s2,
..., s7 to stand for the services nodes of String Constant,
getPeak input, String Constant1, getPeak, String Constant0,
XPath From Text0, and XPath From Text, respectively. In the
the nodes s1, s3, s5 are starting
corresponding workﬂow,
service entities and nodes s6, s7 are terminal service entities
that can be traversed along the label “941.” Applying the DFS
based generation strategy, we can ﬁrstly generate four services
941−−→
sequences, two of which starting from s1: S941
941−−→ s2
941−−→ s7,
941−−→ s4
1,2 = s1
s2
941−−→ s7. Afterwards, four
S941
5,1 = s5
more sequences can be generated starting from intermediate
941−−→
services: S941
s7, S941
4,1 = s4

941−−→ s6, S941
941−−→ s6 and S941

941−−→ s6, S941
941−−→ s7.

941−−→ s4
941−−→ s6, S941

941−−→ s4
3,1 = s3

1,1 = s1

941−−→ s4

2,1 = s2

2,2 = s2

4,2 = s4

B. Breadth First Search (BFS) based Generation

The main idea of the BFS based service tokens generation
lies in multiple items recommendation. In order to achieve

Fig. 3. Portion of WSKG motivating three service sequence generation methods. The nodes in green are services in workﬂow #941 and the dependencies
between them are colored in orange with label “941.” Nodes in other colors are services invoked by other workﬂows, which appear to be downstream nodes
of the services in workﬂow #941 in WSKG.

higher business goal, a real-world recommender system typ-
ically not only recommend single items but also a bundle of
items. Actually, recommending next bundle of services can be
regarded as recommending next basket of items that a user
might want to buy in a single visit in e-commerce scenarios
[13], [9]. Our earlier research also studied how to recommend
unit of work (UoW) with a collection of services usually used
together, based on network analysis [4].

For the similar reason, we consider service bundles as
services tokens when generating sequential services tokens
from WSKG. Applying BFS based generation strategy, for any
service ci ∈ C in a workﬂow w, the sequence of service tokens
(cid:21)
starting from ci can be generated as: Sw
,
where ti
k ⊆ C is a set of multiple services that are at the kth
level of the breadth ﬁrst searching path. Specially, ti
1 is a set
containing only ci, and it can be either a starting service or
an intermediate service.

(cid:20)
1, ti
ti

2, ..., ti

i =

|Sw
i |

3 = [s3, s6] and S941

1 = [s1, s2, s4, s6&s7], S941

As for the sub-WSKG in Fig. 3, for example, we can
generate three service sequences starting from s1, s3 and
s5: S941
5 =
[s5, s7]. We can see that s6 and s7 are combined together
as the fourth token in the sequence of S941
. It means that
any permutation of them can be composed following the
service in the third step of the workﬂow. Sequences starting
from other intermediate services can be generated similarly:
2 = [s2, s4, s6&s7], S941
S941
6 = [s6],
S941
7 = [s7]. For label “1097” in Fig. 3, we can generate
another sequence S1097
= [s3, s14&s15&s17&s18]. For label
“306, ” we can generate two sequences starting from s1 and
s3, respectively: S306

4 = [s4, s6&s7], S941

1 = [s1, s13], S306

3 = [s3, s13].

3

1

C. Probabilistic Walk (PW) based Generation

Inspired by DEEPWALK [37], which uses random walk to
generate sequences of nodes to learn latent representations for
vertices in a graph, we leverage a variant of random walk, i.e.,
probabilistic walk, to generate sequences of services based on
the constructed knowledge graph WSKG.

Recall that the DFS and BFS based generation algorithms
discussed earlier both generate a service sequence starting

j

j , ..., T l

j where T l+1

existing in the sequence of (cid:2)T 1

from service token tj along the path with a speciﬁc label
w. In contrast, the probabilistic walk based sequence genera-
tion algorithm considers the generation of next service token
rooted at tj as a stochastic process, with random variables
T 1
j , T 2
is a service generated with proba-
bilities from the neighbors of service token tl. Note that, such a
(cid:3)
service token T l+1
j , ..., T l
j
is not allowed, meaning that the generated sequence is acyclic.
On the one hand, restarting from a vertex did not show any
improvement to our experimental results. On the other hand,
it is generally meaningless to invoke a previously invoked
service in the same workﬂow. Speciﬁcally, for a service u ∈ C,
let Nu ⊆ C denote the set of directed neighbor services in
the whole WSKG, we model the probability pu,v that service
token v can be generated after u:

j , T 2

j

pu,v = p(v|u) = σ(

ou,v
ou

) =

exp( ou,v
ou

)
exp( ou,n
ou

n∈Nu

(cid:80)

(9)

)

where v ∈ N (u), σ is the commonly used softmax function,
ou,v is the number of occurrence of relationships between u
and v in the whole WSKG and ou = (cid:80)

n∈N (u) ou,n.

For example, in Fig. 3, the dependency relationship between
s7 and s10 occurs four times in four different workﬂows whose
id numbers are “3432,” “245,” “232” and “231,” respectively.
However, for s9, it appears after s7 only once in workﬂow
“957”. Therefore, according to Eq. 9, the probability that s10
can be generated as a downstream service token of s7 is larger
than that of s9.

Two factors may impact the effectiveness of the PW based
generation strategy. One factor is the length of a visiting path,
i.e., l, which determines when to stop while walking along a
path. The other factor is the number of walks starting at each
vertex, i.e., θ. In practice, without specifying θ, some linkages
may not be traversed. As a result, the generated sequences
may not cover all tangible dependency relationships, tampering
the ability of trained representations to predict next suitable
services. As [37] suggested, although it is not strictly required
to do this, but it is a common practice to tune the two factors
to speed up the convergence of stochastic gradient descent

in algorithm 1. In the next section of experiments, We will
discuss in detail the effects of l and θ.

TABLE I
STATISTICAL INFORMATION ABOUT MYEXPERIMENT DATASET

D. Discussions

For all of the three aforementioned strategies, no matter
which one we apply, there might appear duplicate service
sequences. The reason is that some services and their depen-
dency relationships in a workﬂow wi might exist in another
workﬂow wj, in a form of service chain. For example, in Fig.
3, applying DFS-based strategy, sequence s26 −→ s19 −→ s25
can be generated twice through two workﬂows, i.e., with labels
“1360” and “2067.” Similarly, with labels “1097,” “1360” and
“2067,” sequence [s19, s14&s25] can be generated three times
by applying BFS-based strategy.

As for the PW-based strategy,

three factors may result
in duplicates: the length of a path, the frequencies of the
dependency relationships in the path and the predeﬁned pa-
rameter θ. A shorter path with more frequent dependency
relationships tends to be generated as a sequence with higher
probability in case of a higher θ. For example, in Fig. 3, let
s20, s19 and s14 stand for services seqret, emma and emma NJ
respectively. The frequency of the dependency relationship
between s20 and s19 is much higher than the frequencies of
relationships between s20 and other services. So does that of
the relationship between s19 and s14. Therefore, the sequence
s20 −→ s19 −→ s14, whose length is three, is generated multiple
times.

Similar to insurmountable duplicates in a natural language
corpus [36], duplicates in the corpus of service token se-
quences generated from the knowledge graph might impact
is a feature-but-not-
the effectiveness of our approach. It
bug problem. We will evaluate the effect of duplicates with
experiments in the next section.

V. EXPERIMENTS

In this section, we ﬁrst provide an overview of the dataset,
the

then explain the evaluation metrics used and present
experimental results and analyses.

A. Dataset

Since 2007, myExperiment.org has become the largest
service-oriented scientiﬁc workﬂow repository in the world.
It has been used as a testbed by many researchers [4], [5],
[7] in the services computing community. Since we construct
the knowledge graph WSKG across workﬂow boundaries, we
target on the Taverna-generated workﬂows, i.e., workﬂows
generated following Taverna syntax.

In the target testbed, every workﬂow is maintained as an
XML ﬁle and every service in a workﬂow is deﬁned as a
processor. A dependency relationship between two services
is deﬁned as a link or datalink whose two ends are source
and sink, respectively. We examined all Taverna workﬂows
published on myExperiment.org up to October 2021, with a
total of 2,910 workﬂows and 9,120 services. Table I lists the
summarized information over all workﬂows in the dataset.

Total # of workﬂows
Total # of services

Total # of sequences

Average length of sequences

2,910
8,837
10,788
BFS
DFS
41,163
PW 50,314
BFS
DFS
PW

5.0
9.0
7.0

B. Experimental Setup

We randomly used 80% of the workﬂows to generate service
sequences as training data to learn service representations. The
remaining 20% of workﬂows were utilized as the test data. In
the ofﬂine learning phase, we set the number of window size
as 3, and the dimension size d as 50. The generated service
sequences with length smaller than 2 were removed from the
training set. As for the online recommendation phase, similar
to the leave-one-out task that has been widely used in sequen-
tial recommendation [38], [39], [40], we adopted the leave-
last-service-out cross-validation to evaluate our approach.

For each workﬂow in the test set, we removed the terminate
services of the workﬂow and used the services just before the
terminate services as the inputs of the trained representation
model to rank top K service tokens as candidates. Hence, the
experiments were turned to evaluate whether the recommended
service tokens hit the ground truth. Additionally, in the test
test, we removed the linkage which never occurred in the
sequences generated from the training data, considering that
the skip-gram model is not able to predict tokens that are not
in the vocabulary.

C. Evaluation Metrics

All of the evaluation metrics of our approach aim to evaluate
whether the recommended top K entries hit the ground truth
or not. For DFS-based and PW-based generation strategy, each
entry is a single service token. An entry ei ∈ R hits the ground
truth G if ei ∈ G, where R is the recommended result, and
|R| = K. For BFS-based generation strategy, an entry ei ⊆ C
in the recommended list might be a service token comprised
of multiple services. In this case, the entry ei hits the ground
truth G if ∃cj ∈ ei such that cj ∈ G.

To evaluate our recommendation approach, we employed
PRE@K, REC@K and F1@K, which are short for precision,
recall and F1 Score, respectively, as the evaluation metrics.
For every workﬂow in the test data set, they can be calculated
as follows:

precision =

|R ∩ G|
|R|

recall =

|R ∩ G|
|G|

F 1 = 2 ·

precision · recall
precision + recall

(10)

(11)

(12)

Fig. 4. The effect of l and θ with top-3 recommendations

Fig. 5. The effect of l and θ with top-5 recommendations

Fig. 6. The effect of l and θ with top-10 recommendations

where |R ∩ G| means the number of hit entries in the top K
recommended result. Additionally, consider that the index of
the hit result (i.e., idx) in the recommended list is a signiﬁcant
factor to users’ willingness to reuse recommended services.
We thus introduced a variant of MRR, aka VMRR, as an
evaluation metric as well. It helps us to capture the overall
performance of the rank of the hit entry in the recommended
result. The calculation of VMRR is:
(cid:40)

0
|R|−idx

|R| = 1 − idx

K

R ∩ G = ∅
else

(13)

V M RR =

For these four metrics, we reported their mean values over all
workﬂows in the test dataset. The higher the value, the better
the performance.

D. Parameter Sensitivity

We designed experiments to evaluate how changes to l and
θ will effect the performance of the probabilistic walk based
generation strategy. In such experiments, duplicate generated
sequences were removed. Figs. 4, 5 and 6 show the effects
of l and θ to the PW-based generation strategy. In terms of
precision, recall and F1, l = 5 is better than l = 3, l = 10,
l = 15 and l = 20. Note that the median and mean lengths

of workﬂows are 8 and 9.49, respectively, both of which are
closed to 9. Approximately, for a workﬂow with length of
9, the average length of generated sequences with BFS-based
generation strategy is around 5. A lower l results in lower
coverage over all linkages between services, which reduces the
capability of discovering suitable services following a speciﬁc
service. A higher l makes it more likely to generate service
sequences that are not existed in workﬂows.

The parameter θ in this work is similar to the parameter
γ in [37]. For θ, 10 is almost the best option. The reason
might be that, for all nodes in the constructed WSKG, their
mean out-degree and mean number of downstream nodes are
2.38 and 1.69, respectively, both of which are close to 2.0.
Approximately, for a binary tree with height of 5, to reach a
speciﬁc leaf from the root with probabilities at forks, it needs
about 10 attempts. On the one hand, a lower θ might result
in higher probability to miss tangible dependencies that are
useful for next service recommendation. On the other hand, a
higher θ tends to generate duplicate sequences, which might
be misleading for representation learning.

In summary, our experiments demonstrated that l and θ mat-
ter for probabilistic walk based sequence generation strategy.
Our suggestion is that when applying the PW-based strategy,

TABLE II
OVERALL RECOMMENDATION RESULTS

Metrics
BFS
DFS
PW
BFS-DR
DFS-DR
PW-DR

PRE@3
0.3059
0.2384
0.2564
0.3144
0.2449
0.2668

REC@3
0.6258
0.6142
0.6457
0.6465
0.6243
0.6862

F1@3
0.4109
0.3435
0.3609
0.4231
0.3518
0.3842

VMRR@3
0.7651
0.7516
0.7735
0.7789
0.7607
0.8186

PRE@5
0.2191
0.1591
0.1668
0.2249
0.1630
0.1808

REC@5
0.6897
0.6721
0.6967
0.6979
0.6835
0.7609

F1@5
0.3323
0.2573
0.2692
0.3402
0.2632
0.2922

VMRR@5
0.8730
0.8612
0.8681
0.8779
0.8668
0.8994

PRE@10
0.1031
0.0893
0.0912
0.1099
0.0922
0.0993

REC@10
0.7397
0.7455
0.7617
0.7579
0.7624
0.8226

F1@10
0.1810
0.1595
0.1629
0.1920
0.1645
0.1772

VMRR@10
0.9412
0.9362
0.9392
0.9445
0.9404
0.9552

it will be good to get a glimpse at the length of workﬂows
and the out-degree of service nodes in the constructed graph.

E. Performance Comparison

We designed experiments to answer two research ques-
tions. First, which sequence generation strategy acts the best?
Second, should duplicates from the generated sequences be
removed? Table II presents the overall recommendation per-
formance metrics among six generation strategies. BFS-DR,
DFS-DR and PW-DR are results of BFS, DFS and PW with
duplicate sequences removed, respectively. For each metric,
the best result is highlighted in bold and the second best is
underlined. According to the lessons we learnt as discussed
earlier, we set l and θ to 5 and 10, respectively, for PW and
PW-DR.

that our goal

RQ1: Which sequence generation strategy is the best?
According to the experimental results, in terms of precision
and F1, BFS-based strategies outperform others. In terms of
recall and VMRR, PW-DR performs the best. Speciﬁcally, we
hold that recall and VMRR are better metrics than precision
and F1 to gauge the performance between different strategies.
Recall
is to ﬁnd the most suitable services
following a speciﬁc service. It means that we care more
about how many adjacent services we can fetch, instead of
how many candidates are adjacent services. The higher recall
means higher possibility to help a user increase the efﬁciency
of service composition. Besides, for the hit index, we hope
it could be as low as possible. The higher VMRR could
encourage users to reuse the recommended services much
more. For example, if a service exists in the ground truth,
e.g., ci is followed by two services, say cj and ck, which hit
at index 1 and 4 of the recommended list, respectively. The
precision is only 40%. However, it is acceptable for the user
in real practice to chain one of cj and ck with ci, especially
in a scenario that there are more than hundreds of services
in the whole repository, which makes it tedious and time-
consuming to ﬁnd a suitable service at the next step. Therefore,
our experiments have demonstrated that PW-based strategies
outperform others.

Table II shows that the recall remains around 70% for all
strategies at K = 5, meaning that in the ground truth, over half
of adjacent services are found if we recommend 5 services.
If service ci has four adjacent services, about three of them
will be returned to the user with recommendation size of
5. Especially, the VMRR@5 is around 0.90. It means that

generally the ﬁrst or the second entry in the recommended
list hits the ground truth. We thus have demonstrated that our
approach is quite useful to help users save time to composite
a workﬂow by reusing recommended services.

RQ2: Should duplicates from the generated sequences be
removed? [36] mentioned that different NLP models are af-
fected by duplicates in different ways. So, it is a good practice
to know about the effect of duplicates. Table II shows that, for
all generation strategies, removing duplicates achieves better
performance than remaining them in the training set. As for
PW-based strategies, due to the fact that the higher probability
of a linkage is, the more potential it would be traversed while
walking, which makes it more likely to generate duplicate
sequences, PW-DR is better than PW in terms of all metrics.
Therefore, we suggest to investigate the impact of duplicate
sequences in advance, when applying language modeling
techniques to learn service representations.

VI. CONCLUSIONS & FUTURE WORK

In this paper, we have applied deep learning methods in
NLP to study workﬂow recommendation problem. Workﬂow
composition process is viewed as a sequential service gen-
eration process, and the problem of service recommendation
is formalized as a problem of next word prediction in NLP.
A knowledge graph is constructed on top of workﬂow prove-
nance, and we develop three strategies to create a corpus of
service sequences from the knowledge graph. Deep learning
method in NLP is then applied to learn service representa-
tions. The resulted service embeddings are used to support
incremental workﬂow composition at run-time, associated with
structural information embedded in the knowledge graph.

In the future, we plan to extend our research in the fol-
lowing three directions. First, we plan to take into account
users’ proﬁle information to enable personalized workﬂow
recommendation. Second, we plan to seamlessly integrate se-
quential service invocation dependency with hierarchical graph
structure to further enhance recommendation quality. Third,
to motivate and encourage more research work on service
representation learning, we plan to build an open repository
supporting a diverse categories of workﬂow formats.

ACKNOWLEDGEMENT

Our work is partially supported by National Aeronautics
and Space Administration under grants 80NSSC21K0576 and
80NSSC21K0253.

REFERENCES

[1] A. L. Lemos, F. Daniel, & B. Benatallah, “Web Service Composition:
A Survey of Techniques and Tools”, ACM Computing Surveys (CSUR),
48(3), 2015, pp. 1-41.

[2] W. Tan, J. Zhang, & I. Foster, “Network Analysis of Scientiﬁc Work-
ﬂows: A Gateway to Reuse”, Computer, 43(9), 2010, pp. 54-61.
[3] C. A. Goble, J. Bhagat, S. Aleksejevs, D. Cruickshank, D. Michaelides,
D. Newman, M. Borkum, S. Bechhofer, M. Roos, P. Li, & D. D. Roure,
“myExperiment: A Repository and Social Network for the Sharing
of Bioinformatics Workﬂows”, Nucleic Acids Research, 38, 2010, pp.
W677-W682.

[4] J. Zhang, M. Pourreza, S. Lee, R. Nemani, & T. J. Lee, “Unit of Work
Supporting Generative Scientiﬁc Workﬂow Recommendation”, in Pro-
ceedings of International Conference on Service-Oriented Computing,
Nov. 2018, pp. 446-462.

[5] J. Zhang, W. Wang, X. Wei, C. Lee, S. Lee, L. Pan, & T. J. Lee, “Climate
Analytics Workﬂow Recommendation as A Service-provenance-driven
Automatic Workﬂow Mashup”, in Proceedings of IEEE International
Conference on Web Services, Jun. 2015, pp. 89-97.

[6] D. Hull, R. Stevens, P. Lord, C. Wroe, & C. Goble, “Treating Shimantic
Web Syndrome With Ontologies”, in Proceedings of The 1st AKT
Workshop on Semantic Web Services (AKT-SWS04), Dec. 2004, pp.
1-4.

[7] J. Zhang, W. Tan, J. Alexander, I. Foster, & R. Madduri, “Recommend-
as-you-go: A Novel Approach Supporting Services-oriented Scientiﬁc
Workﬂow Reuse”, in Proceedings of IEEE International Conference on
Services Computing, Jul. 2011, pp. 48-55.

[8] G. Shani, D. Heckerman, & R. I. Brafman, “An MDP-based Recom-
mender System”, Journal of Machine Learning Research, 6(9), 2005,
pp. 1265-1295.

[9] F. Yu, Q. Liu, S. Wu, L. Wang, & T. Tan, “A Dynamic Recurrent
Model for Next Basket Recommendation”, in Proceedings of The 39th
ACM SIGIR International Conference on Research and Development in
Information Retrieval, Jul. 2016, pp. 729-732.

[10] S. Wang, L. Hu, & L. Cao, “Perceiving the Next Choice with Com-
prehensive Transaction Embeddings for Online Recommendation”, in
Proceedings of Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, Sep. 2017, pp. 285-302.

[11] A. Hard, K. Rao, R. Mathews, F. Beaufays, S. Augenstein, H. Eichner,
C. Kiddon, & D. Ramage, “Federated Learning for Mobile Keyboard
Prediction”, arXiv, arXiv:1811.03604, 2018.

[12] Y. Zhang, H. Dai, C. Xu, J. Feng, T. Wang, J. Bian, B. Wang, & T. Y.
Liu, “Sequential Click Prediction for Sponsored Search with Recurrent
Neural Networks”, AAAI, 28(1), Jun. 2014, pp. 1369-1376.

[13] S. Rendle, C. Freudenthaler, & L. Schmidt-Thieme, “Factorizing Person-
alized Markov Chains for Next-basket Recommendation”, in Proceed-
ings of The 19th International Conference on World Wide Web, Apr.
2010, pp. 811-820.

[14] B. Hidasi, A. Karatzoglou, L. Baltrunas, & D. Tikk, “Session-based
Recommendations with Recurrent Neural Networks”, arXiv preprint,
arXiv:1511.06939, 2015.

[15] T. Mikolov, K. Chen, G. Corrado, & J. Dean, “Efﬁcient Esti-
mation of Word Representations in Vector Space”, arXiv preprint,
arXiv:1301.3781, 2013.

[16] I. Paik, W. Chen, & M. N. Huhns, “A Scalable Architecture for Auto-
matic Service Composition”, IEEE Transactions on Services Computing,
7(1), 2012, pp. 82-95.

[17] S. R. Chowdhury, F. Daniel, & F. Casati, “Efﬁcient, Interactive Rec-
ommendation of Mashup Composition Knowledge”, in Proceedings of
International Conference on Service-Oriented Computing, Dec. 2011,
pp. 374-388.

[18] O. Greenshpan, T. Milo, & N. Polyzotis, “Autocompletion for Mashups”,
in Proceedings of the VLDB Endowment, 2(1), 2009, pp. 538-549.
[19] H. Elmeleegy, A. Ivan, R. Akkiraju, & R. Goodwin, “Mashup Advisor:
A Recommendation Tool for Mashup Development”, in Proceedings of
IEEE International Conference on Web Services, Sep. 2008, pp. 337-
344.

[20] B. Xia, Y. Fan, W. Tan, K. Huang, J. Zhang, & C. Wu, “Category-aware
API Clustering and Distributed Recommendation for Automatic Mashup
Creation”, IEEE Transactions on Services Computing, 8(5), 2014, pp.
674-687.

[21] D. Koop, C. E. Scheidegger, S. P. Callahan, J. Freire, & C. T. Silva,
“Viscomplete: Automating Suggestions for Visualization Pipelines”,
IEEE Transactions on Visualization and Computer Graphics, 14(6),
2008, pp. 1691-1698.

[22] S. Deng, D. Wang, Y. Li, B. Cao, J. Yin, Z. Wu, & M. Zhou, “A
Recommendation System to Facilitate Business Process Modeling”,
IEEE Transactions on Cybernetics, 47(6), 2016, pp. 1380-1394.
[23] J. Zhang, Q. Liu, & K. Xu, “FlowRecommender: A Workﬂow Recom-
mendation Technique for Process Provenance”, in Proceedings of The
8th Australasian Data Mining Conference, 2009, pp. 55-61.

[24] S. Smirnov, M. Weidlich, J. Mendling, & M. Weske, “Action Patterns
in Business Process Models”, in Proceedings of The 7th International
Conference on Service-Oriented computing, 2009, pp. 115-129.
[25] Y. Bengio, A. Courville, & P. Vincent, “Representation Learning: A
Review and New Perspectives”, IEEE Transactions on Pattern Analysis
and Machine Intelligence, 35(8), 2013, pp. 1798-1828.

[26] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A.
W. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, & B. Kingsbury,
“Deep Neural Networks for Acoustic Modeling in Speech Recognition:
The Shared Views of Four Research Groups”, IEEE Signal Processing
Magazine, 29(6), 2012, pp. 82-97.

[27] N. Boulanger-Lewandowski, Y. Bengio, & P. Vincent, “Modeling Tem-
poral Dependencies in High-Dimensional Sequences: Application to
Polyphonic Music Generation and Transcription”, in Proceedings of
International Conference on Machine Learning, 2012, pp. 1159-1166.

[28] A. Bordes, X. Glorot, J. Weston, & Y. Bengio, “Joint Learning of
Words and Meaning Representations for Open-Text Semantic Parsing”,
in Proceedings of The 15th International Conference on Artiﬁcial
Intelligence and Statistics, Mar. 2012, pp. 127-135.

[29] H. Paulheim, V. Tresp, & Z. Liu, “Representation Learning for the

Semantic Web”, Journal of Web Semantics, 2020, pp. 61-62, 100570.

[30] C. Li, R. Zhang, J. Huai, X. Guo, & H. Sun, “A Probabilistic Approach
in Proceedings of IEEE International

for Web Service Discovery”,
Conference on Services Computing, Jun. 2013, pp. 49-56.

[31] D. M. Blei, A. Y. Ng, & M. I. Jordan, “Latent Dirichlet Allocation”,
Journal of Machine Learning Research, vol. 3, 2003, pp. 993-1022.
[32] Y. Zhong, Y. Fan, W. Tan, & J. Zhang, “Web Service Recommendation
with Reconstructed Proﬁle from Mashup Descriptions”, IEEE Transac-
tions on Automation Science and Engineering, 15(2), 2016, pp. 468-478.
[33] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, & P. Smyth, “The Author-Topic
Model for Authors and Documents”, in Proceedings of Conference on
Uncertainty in Artiﬁcial Intelligence, 2004, pp. 487-494.

[34] J. Zhang, Y. Fan, J. Zhang, & B. Bai, “Learning to Build Accurate Ser-
vice Representations and Visualization”, IEEE Transactions on Services
Computing, Jun. 2020.

[35] A. Mnih, & G. E. Hinton, “A Scalable Hierarchical Distributed Language
Model”, in Proceedings of Conference on Neural Information Processing
Systems, 2009, pp. 1081-1088.

[36] A. Schoﬁeld, L. Thompson, & D. Mimno, “Quantifying the Effects of
Text Duplication on Semantic Models”, in Proceedings of the 2017
Conference on Empirical Methods in Natural Language Processing,
2017, pp. 2737-2747.

[37] B. Perozzi, R. Al-Rfou, & S. Skiena, “Deepwalk: Online Learning of
Social Representations”, in Proceedings of The 20th ACM International
Conference on Knowledge Discovery and Data Mining, Aug. 2014, pp.
701-710.

[38] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, & T. S. Chua, “Neural Collab-
orative Filtering”, in Proceedings of the 26th International Conference
on World Wide Web, Apr. 2017, pp. 173-182.

[39] W. C. Kang, & J. McAuley, “Self-Attentive Sequential Recommenda-
tion”, in Proceedings of IEEE International Conference on Data Mining,
Nov. 2018, pp. 197-206.

[40] J. Tang, & K. Wang, “Personalized Top-N Sequential Recommendation
via Convolutional Sequence Embedding”, in Proceedings of The 11th
ACM International Conference on Web Search and Data Mining, Feb.
2018, pp. 565-573.

[41] Y. Zhang, M. Zhang, X. Zheng, & D. E. Perry, “Service2vec: A Vector
Representation for Web Services”, in Proceddings of IEEE International
Conference on Web Services, Jun. 2017, pp. 890-893.

[42] S. Wang, Z. Wang, & X. Xu, “Mining Bilateral Patterns as Priori
Knowledge for Efﬁcient Service Composition”, in Proceedings of IEEE
International Conference on Web Services, Jun. 2016, pp. 65-72.
[43] K. Stoitsas, “The Use of Word Embeddings for Cyberbullying Detection

in Social Media”, Doctoral dissertation, Tilburg University, Jul. 2018.

