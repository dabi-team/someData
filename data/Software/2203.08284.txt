2
2
0
2

t
c
O
1
2

]

R
C
.
s
c
[

2
v
4
8
2
8
0
.
3
0
2
2
:
v
i
X
r
a

Minimizing Trust with Exclusively-Used Physically-Isolated Hardware

Zhihao Yao†*, Seyed Mohammadjavad Seyed Talebi†∗, Mingyi Chen†
Ardalan Amiri Sani†, Thomas Anderson‡
†UC Irvine, ‡University of Washington
{z.yao, mjavad, mingyi.chen, ardalan}@uci.edu, tom@cs.washington.edu

Abstract

Smartphone owners often need to run security-critical pro-
grams on the same device as other untrusted and potentially
malicious programs. This requires users to trust hardware and
system software to correctly sandbox malicious programs,
trust that is often misplaced.

Our goal is to minimize the number and complexity of hard-
ware and software components that a smartphone owner needs
to trust to withstand adversarial inputs. We present a multi-
domain hardware design composed of statically-partitioned,
physically-isolated trust domains. We introduce a few simple,
formally-veriﬁed hardware components to enable a program
to gain provably exclusive and simultaneous access to both
computation and I/O on a temporary basis. To manage this
hardware, we present OctopOS, an OS composed of mutually
distrustful subsystems.

We present a prototype of this machine (hardware and
OS) on a CPU-FPGA board and show that it incurs a small
hardware cost compared to modern SoCs. For security-critical
programs, we show that this machine signiﬁcantly reduces the
required trust compared to mainstream TEEs while achieving
decent performance. For normal programs, performance is
similar to a legacy machine.

1 Introduction

Because of their ubiquity and portability, modern smartphones
are often used to run security-critical programs along with
diverse, untrusted, and potentially malicious programs. For
example, most of us perform ﬁnancial tasks, such as banking
and payments [77] on our smartphones. Many of us also
run health-related programs, e.g., to receive test results and
diagnoses from our health providers. There is also interest
in using these devices to perform life-critical tasks such as
controlling an insulin pump [93] or monitoring breathing [71],
although security concerns currently pose a roadblock [93].
Realizing this computing paradigm should be straightfor-
ward. The job of an operating system (OS) is to isolate

*Equal contribution

security-critical programs from other programs running on
the same hardware. Yet, this has proven to be challenging in
practice due to vulnerabilities in system software (e.g., OS,
hypervisor, and device drivers) [2, 6, 8, 9, 16, 26, 76, 96, 108]
and hardware (e.g., processor, memory, interconnects, and I/O
devices including their ﬁrmware) [28, 52, 55, 63, 75, 94, 99].
Malicious programs can exploit these vulnerabilities to take
control of the machine and any program running on it. We
must trust that hardware and system software can effectively
sandbox and neutralize malicious programs, but this trust
often proves to be misplaced.

To address this challenge, a new approach has emerged. It
uses Trusted Execution Environments (TEEs) to host security-
critical programs without requiring trust in the OS. Unfortu-
nately, today’s TEEs still require us to trust the hardware and
the security monitor implementing the TEE guarantees. This
trust has also proven unjustiﬁed. Existing TEEs have fallen
victim to various attacks, e.g., hardware-based side-channel
attacks [18, 21, 40, 62, 66, 68, 83, 94, 109], attacks exploiting
software vulnerabilities [7, 20, 29, 81], and attacks based on
design ﬂaws [46, 58, 102].

In this paper, we present a solution to enable smartphones
to be used for both security-critical and non-critical programs.
Our goal is to minimize both the number and the complex-
ity of hardware and software components that need to be
strongly trusted by the smartphone owner in order to execute
a security-critical program. As we will deﬁne in §2.1, we say
that a component is strongly trusted if it needs to be able to
withstand and neutralize adversarial inputs.

Our key principle is provably exclusive access to hardware
and software components. That is, we design a solution to
enable a security-critical program to exclusively use complex
hardware and software components and be able to verify the
exclusive use. Due to exclusive use, a component only needs
to be weakly trusted. That is, it only needs to operate correctly
in the absence of adversarial inputs.

More concretely, we present a hardware design for a smart-
phone. Called a split-trust hardware, it comprises multiple
trust domains, one or multiple for TEEs, one for each I/O

 
 
 
 
 
 
device, one for a resource manager, and one for hosting a com-
modity OS and its programs. The trust domains are statically-
partitioned and physically-isolated: they each have their own
processor and memory (and one I/O device in the case of an
I/O domain) and do not share any underlying hardware com-
ponents; they can only communicate by message passing over
a hardware mailbox. Moreover, we introduce a few simple,
formally-veriﬁed hardware components that enable a program
to gain provably exclusive access to one or multiple domains.
We then present OctopOS, an OS to manage this hardware.
Unlike existing OSes, which have a single, trusted-by-all nu-
cleus, i.e., the kernel, OctopOS comprises mutually distrustful
subsystems: a TEE runtime for security-critical programs, I/O
services, a resource manager, and a compatibility layer for a
commodity OS.

We rigorously evaluate the required trust, i.e., the Trusted
Computing Base (TCB), of this machine. We show that our
machine signiﬁcantly reduces the TCB compared to main-
stream TEEs and achieves one close to the lower bound.

We present a complete prototype of our machine (hard-
ware and OS) on top of a a CPU-FPGA board (Xilinx Zynq
UltraScale+ MPSoC ZCU102). We use the powerful ARM
Cortex A53 CPU to host the commodity OS (PetaLinux) and
its programs with high performance. We use the FPGA to
build the other trust domains: two TEEs, a resource manager,
and four I/O domains (an input domain, an output domain,
a storage domain, and a network domain). We use (weak)
microcontrollers for these other domains, including the TEEs.
This choice as well as the small number of TEE domains is
based on our observation that security-critical programs in
smartphones, unlike regular programs, are often not as compu-
tationally intensive, and the number of such programs that run
simultaneously is typically small. In other respects, however,
they are like normal programs: they start and stop, run in the
background, do I/O, and so forth.

Using our prototype, we build two important security-
critical programs for our machine:1 (i) a banking program
that can securely interact with the user, and (ii) an insulin
pump program that can securely execute its algorithm and
communicate with (emulated) glucose monitor and pump.

We also use our prototype to measure the hardware cost
and performance of our machine. We show that the added
hardware cost is small (i.e., 1-2%) compared to modern SoCs.
Moreover, we show that security-critical program can achieve
decent performance despite the use of weak microcontrollers
for all TEE and I/O domains. Finally, we show that normal
programs can achieve the same compute and I/O performance
as on a legacy machine.
Secure hardware trend. Our vision of using physical iso-
lation and exclusive use for security is in line with recent
hardware trends from the smartphone industry. Apple has
integrated the Secure Enclave Processor (SEP) into its prod-

1We will open source our hardware design, OctopOS, security-critical

programs, and formal veriﬁcation proofs.

ucts [3] and used it to secure user’s secret data and to control
biometric sensors (i.e., Touch ID and Face ID) [4]. Similarly,
Pixel 6 uses the tensor security core to host security-critical
tasks such as key management and secure boot [53]. Our
work takes this vision further by allowing programs (includ-
ing those that rely on I/O devices) to use dedicated processors
by developing a model for how that can be safely done.

2 Background

2.1 Trust Deﬁnitions
The hardware and software components that need to be trusted
for a program to execute securely form its TCB. We deﬁne
two types of trust: strong trust and weak trust. We say a
component is strongly trusted if it needs to guard against ad-
versarial inputs. For example, imagine an OS that is trusted
to isolate a program from other malicious programs. Ma-
licious programs can issue adversarial syscalls to the OS
concurrently to the protected program. In such a case, the
component (e.g., the OS) must be trusted to prevent these
other programs from exploiting any vulnerabilities (logical or
implementation-related). This is very challenging as demon-
strated by the plethora of reported exploits. Therefore, we
believe that strong trust should be minimized for security-
critical programs. We note, however, that there are methods
for hardening hardware and software components, such as
formal veriﬁcation. Strong trust is acceptable if a component
is known to be adequately hardened against vulnerabilities.
We say that a component is weakly trusted if it just needs
to operate correctly in the absence of adversarial inputs. For
example, consider the same OS mentioned above, but assume
that the security-critical program is the only one running on
top of the OS (and assume application-level networking). In
such a case, the component must only be trusted to (1) not ex-
ert buggy behavior under normal usage, i.e., when processing
well-formed inputs, and (2) not be compromised by an adver-
sary before use and upon distribution (e.g., through implanted
backdoors). These trust assumptions can be (more) easily met
in practice by ensuring that: (1) component designers test it ad-
equately under various expected usage models, (2) the source
code of the component is available for inspection by security
experts and users, and (3) users can verify the component
before use through remote attestation. Therefore, we believe
that weak trust is acceptable for security-critical programs.
We also note that weak trust, as deﬁned here, is the lower
bound for trust as each component used by a security-critical
program must at least be weakly trusted.

2.2 Trust in Existing Systems
Historically, the OS has been a strongly-trusted part of the
system (Figure 1 (a)). As commodity OSes have become more
complex over the years, more and more vulnerabilities have
been found in them, allowing malware to exploit them and
compromise the OS [2, 6, 8, 16, 22, 26, 76, 96, 108]. As an

2

and have been exploited in the past [7, 20, 29, 81]. AMD SEV
has also been shown to contain several vulnerabilities due to
design ﬂaws [46, 58, 102].

Hardware components have been exploited as well.
Hardware-based side-channel attacks have recently emerged
as a serious threat to computing systems. For example, SGX
enclaves and TrustZone have been compromised using sev-
eral such attacks [18, 21, 40, 62, 66, 68, 83, 94, 109]. The
core reason behind this is that existing solutions execute
the untrusted OS and TEEs on the same hardware, forcing
them to share underlying microarchitectural features such
as cache [18, 40, 62, 66, 83, 109] and speculative execution
engine [21, 55, 63, 94], as well as architectural ones such as
virtual memory [68]. The memory subsystem has also proved
vulnerable to Rowhammer attacks [42,52,64,82,95,104]. The
complexity of these hardware components ensures that more
vulnerabilities are likely to be discovered and exploited. For
example, researchers have recently demonstrated a suite of
new side channels using the CPU interconnect [75], the x87
ﬂoating-point unit, and Advanced Vector extensions (AVX)
instructions (among others) [99].

3 Key Goal and Principle

Key goal. Our goal in this work is to minimize the number
and complexity of strongly-trusted components. It is difﬁcult
for complex hardware or software components to adequately
protect themselves against adversarial inputs. By contrast,
simpler components can fend off adversarial inputs through
comprehensive testing, analysis, and formal veriﬁcation.
Key principle. Our key principle to achieve this goal is prov-
ably exclusive access to hardware and software components.
That is, we design our machine to enable a security-critical
program to exclusively use complex hardware and software
components and be able to verify the exclusive use. More
speciﬁcally, our goal is to have most components, especially
complex ones such as the processor and system software, (1)
be reset to a clean state before use, (2) then used exclusively
by a security-critical program in a veriﬁable fashion through
remote and/or local attestation, and (3) then again reset to
a clean state right after use. In this case, such a component
only needs to be weakly trusted as it does not need to worry
about adversarial inputs while serving the security-critical
program, nor does it need to worry about residual state from
the security-critical program while serving other, potentially
malicious, programs.

To realize this principle, we introduce a novel split-trust
hardware design (§4). We then introduce an OS for this hard-
ware, called OctopOS (§5).

4 Split-Trust Hardware

Modern machines leverage hardware with a hierarchical priv-
ilege model. That is, hardware provides multiple privilege
levels, each with more privilege than previous ones, with one

Figure 1: (a) Traditional design where the OS isolates
security-critical programs from normal programs. (b) Use of
a TEE to isolate a security-critical program.

example, there have been about 1500 security vulnerabilities
reported in the Linux kernel just since 2016 [6]. Strong trust
in commodity OSes is not warranted.

There have been several attempts to build trustworthy OSes.
These include microkernels [10, 30, 38, 54, 61], exokernels
and library OSes [17, 31, 50, 78], formally veriﬁed OSes (and
hypervisors) [43, 44, 54, 59, 60, 73, 87, 91, 97], and OSes writ-
ten in safe languages [32, 48, 57, 72]. While effective, these
solutions require replacing commodity OSes with a new OS.
This is a challenging task due to the abundance of existing
programs, device drivers, and developers for commodity OSes.
More importantly, using these OSes still requires strong trust
in hardware, which is not warranted either, as we will discuss.
About two decades ago, a new approach started to gain pop-
ularity. The idea is to create an isolated environment, called a
TEE, to host a security-critical program. This allows the use
of a commodity OS, but relegates it to be only in charge of
untrusted, normal programs such as games, utility apps, and
entertainment platforms. The TEE enables a security-critical
program to ensure its own integrity and conﬁdentiality even
if the OS is untrusted, but leaves the OS in charge of resource
management (and hence the availability guarantee). Figure 1
(b) illustrates this design. It shows a security monitor is used
to isolate a TEE from the OS. The security monitor can be
implemented purely in software (i.e., a hypervisor) [25, 47] or
using a combination of hardware and software. ARM Trust-
Zone and Intel SGX are examples of the latter. Others include
AMD Secure Encrypted Virtualization (SEV), Intel Trusted
Domain Extensions (TDX), ARMv9’s Realms [41], and Key-
stone for RISC-V [56].

Despite their success, existing TEE solutions still require
many components to be strongly trusted including the secu-
rity monitor and several hardware components such as the
very complex processor, memory, I/O devices in some cases,
and dynamically-programmable protection hardware such as
address space controllers and MMUs. Unfortunately, all of
these components can be compromised by an adversary. For
examples, hypervisors contain many vulnerabilities [9, 14].
TEE OSes in TrustZone have also contained vulnerabilities

3

I/O device HWI/O device HWCommodity OSSecurity-critical prog.Normal programNormal programI/O device HWI/O device HWCommodity OSNormal programNormal programDMASecurity-critical prog.TEEDMADMADMA(a)(b)Security MonitorFigure 3: Mailbox design.

domain, which accommodates a commodity OS and its (un-
trusted) programs, to achieve high performance. We use
weaker microcontrollers for other domains in order to keep
the hardware cost small. Each domain has its own memory as
well and domains do not (and cannot) share memory.

Importantly, each I/O domain also has exclusive control of
an I/O device, which is wired to and only programmable by
the processor of that domain and which directly interrupts that
processor. (We will discuss how DMA is handled in §4.5.)

4.2 Exclusive Inter-Domain Communication
To be able to act as one machine, the domains need to be
able to communicate. We introduce a simple, yet powerful,
hardware primitive for this purpose: veriﬁably delegatable
hardware mailbox. At its core, a mailbox is a hardware queue,
allowing two domains (i.e., the writer and reader) to commu-
nicate through message passing.

The key novelty of our mailbox is how it enables exclusive
communication using its delegation model. A mailbox has a
ﬁxed end (reader or writer) and a delegatable one. The ﬁxed
end is hard-wired to a speciﬁc domain. The delegatable one
is wired to multiple domains, but only one can use it at a
time, enforced by a hardware multiplexer within the mailbox.
This end is by default (i.e., after a mailbox reset) under the
control of the resource manager domain. But the resource
manager can delegate it to another domain, which is then able
to exclusively communicate with the domain on the ﬁxed end
of the mailbox.

Figure 3 shows the design of the mailbox with a ﬁxed
reader. For example, consider the serial output domain in our
prototype. This domain is the ﬁxed reader of a mailbox. Any
domain with write access to the mailbox can (exclusively)
send content to the output domain to be displayed in the
terminal.

The delegation model of our mailbox has another impor-
tant property: limited yet irrevocable delegation. When the
resource manager delegates the mailbox to a domain, it sets a
quota for the delegation in terms of both the maximum num-
ber of allowed messages and maximum delegation time. As
long as the quota has not expired (i.e., a session), the domain
can use the mailbox and the resource manager cannot revoke
its access to the mailbox. The session expires when either the
message limit or the time limit expires. (The message limit

Figure 2: Simpliﬁed overview of the split-trust hardware. The
ﬁgure does not show all mailboxes for clarity.

all-powerful level to “rule them all.”2 This model results in-
evitably in several complex, strongly-trusted components such
as the processor, protection hardware, and system software.
In this paper, we demonstrate a novel hardware design,
the split-trust hardware, in which the hardware is split into
multiple isolated trust domains. Each domain is intended for
one aspect of the machine: one or multiple for TEEs, one for
each I/O device (i.e., an I/O domain), one for a commodity
OS and its untrusted programs (i.e., the untrusted domain),
and one for a resource manager, which is in charge of con-
strained resource scheduling and access control. The beneﬁt
of the split-trust hardware is that a security-critical program
can exclusively take control of and use its own domain and
exclusively communicate with other domains (§4.2), e.g., for
I/O and IPC, hence signiﬁcantly reducing the strongly-trusted
components. Figure 2 shows a simpliﬁed view of this hard-
ware design. Next, we discuss its key aspects.

4.1 Physical Isolation and Static Partitioning
We follow two important principles in our hardware design.
(1) Domains must be physically isolated (i.e., share no hard-
ware components). (2) The isolation boundary between them
cannot be programmatically and dynamically modiﬁed as
there is no trusted-by-all hardware or software component.
This implies that we cannot rely on programmable protection
hardware, such as an MMU, IOMMU, or address space con-
troller, to enforce isolation. As a result, our design statically
partitions the hardware resources between domains.

More speciﬁcally, each trust domain has its own proces-
sor and memory. We use a powerful CPU for the untrusted

2A reference to Tolkien’s The Lord’s of the Rings.

4

TEE 1 domainTEE 2 domainUntrusted domainResource manager domainI/O 2 domainI/O 1 domainmailboxmailboxI/O device HWI/O device HWTrusted Platform Module (TPM)Power Management Unit (PMU)Security-critical prog.Security-critical prog.Commodity OSNormal programNormal programDelegatable writerDelegatable readerFixed writerFixed readermailboxmessage queuemultiplexerfixed readerdomainwriterdomainwriterdomainmailbox commandsmailbox messagesDefault writer domain (resource mgr.)statusregistermultiplexinglogiccan be set to inﬁnite, but not the time limit.)

This delegation model enables a limited form of availabil-
ity, which we refer to as session availability. That is, a domain
with exclusive communication access to another domain can
be sure to retain its access for a known period of time or num-
ber of messages. This is critical for some security guarantees
on smartphones. For example, a security-critical program can
ensure that the User Interface (UI) will not be hijacked or
covered with overlays when the program is interacting with
the user [24, 107]. Or a security-critical program that has au-
thenticated to and hence unlocked a sensitive actuator domain
(e.g., insulin pump) can ensure that no other program can
hijack the session and manipulate the actuator.

As the resource manager is not trusted by other domains,
the delegation must be veriﬁable. The mailbox hardware pro-
vides a facility for this veriﬁcation. As Figure 3 shows, all
domains connected to the mailbox can read a status register
from the mailbox hardware. The status register speciﬁes the
domain that can read/write to the mailbox and the remaining
quota. The domain with delegated access can therefore verify
its access and quota. (Other domains will receive a dummy
value when reading the status register for conﬁdentiality.)

4.3 Power Management
Our mailbox primitive cannot, on its own, guarantee session
availability. This is because we need to ensure that during
a session, the domains used by a security-critical program
remain powered up (assuming there is adequate energy if
battery-powered).

The Power Management Unit (PMU) normally takes com-
mands from the resource manager. The resource manager
uses this capability to reset other domains when needed, e.g.,
reset a TEE domain before running a new program, or apply
Dynamic Voltage Frequency Scaling (DVFS) to manage the
system’s power consumption. (We do not support DVFS for
the domains in our prototype. Hence, in the rest of the pa-
per, we mainly focus on the reset interface, although similar
principles can be applied to DVFS.)

However, the resource manager is not a trusted component;
hence it may try to reset a domain during a session. Therefore,
we add a simple hardware component, called the reset guard,
for the reset signals, which ensures that as long as the quota
on a mailbox has not expired, the domains on both sides of the
mailbox cannot be reset, hence ensuring session availability.

4.4 Hardware Root of Trust
A hardware root of trust is needed during remote attestation
to convince the party in charge of a security-critical program
of the authenticity of the hardware and the correctness of the
loaded program. We use a Trusted Platform Module (TPM)
to realize the root of trust for the split-trust hardware.
Why TPM? TPM, as speciﬁed by the Trusted Comput-
ing Group (TCG), is a tamper-resistant security co-processor

connected to the main processor over a bus [92]. Tradition-
ally, it provides security features for the machine as a whole,
such as the measurements of the loaded software. This makes
TPM unsuitable for more ﬁne-grained security features, such
as remote attestation of a speciﬁc program. As a result, in-
processor TEE solutions, such as SGX, integrate the root of
trust in the processor itself, tightly coupling it with various
features of the processor (such as virtual memory and cache),
further bloating the strongly-trusted processor.

Our key insight is that TPM can provide ﬁne-grained se-
curity features for a split-trust machine since different com-
ponents of this OS run in separate domains. This allows the
machine to enjoy the security beneﬁts of TPM without suffer-
ing from its main limitation.

To integrate TPM into a split-trust machine, we need a
different set of parameters from the ones found in existing
TPM chips. We omit the details due to space limitation.

4.5 High Performance I/O
By default, the data plane of I/O domains are implemented
over mailboxes. However, this raises a performance concern
due to additional data copies (to and from mailbox). While
the performance overhead is acceptable for TEE domains, it
is not so for the untrusted domain. An important hardware
primitive that enables a legacy machine to achieve high I/O
performance is DMA. To safely use DMA in our machine,
we introduce domain-bound DMA, deﬁned with the following
two restrictions. (1) The DMA engine is hard-wired to only
read/write to the memory of the untrusted domain. (2) The
DMA engine can stream data in/out of the I/O device only
when the I/O domain is used by the untrusted domain.

We achieve this with a simple hardware component called
the arbiter, which is a switch that decides if the data streams
of the I/O device is connected to a DMA engine or to a simple
FIFO queue accessible to the I/O domain.

4.6 Domain and Mailbox Reset
Domains and their mailboxes need to be reset before and
after use (§3). We reset the mailboxes directly in hardware
upon delegation, yield, and session expiration. We leave the
resetting of the domains to the resource manager, albeit un-
der the limitations enforced by the reset guard (§4.3). Even
though the resource manager is untrusted, this does not pose a
problem since the program can verify, using local and remote
attestation through TPM as well as some measures provided
by the domain runtime that (1) a domain has been reset, (2) it
has not been used since last reset, (3) it will be reset after use
and before use by other domains.

This veriﬁcation is rather straightforward for a TEE do-
main. The bootloader, which is stored in a ROM and is part
of the remotely attested root of trust (§7.2), is tasked with
fully cleaning all the state information in the domain upon
reset. Once the program is loaded in the domain, it takes ex-
clusive control of resources. The only way for the resource

5

manager to take the domain back is to reset it (if allowed by
the reset guard), which then triggers the bootloader to clean
the state. The veriﬁcation process is less straightforward for
I/O domains. We will discuss that in §5.1.

4.7 Usability Discussion
We argue that the exclusive use of hardware resources by
security-critical programs in our machine does not cause us-
ability problems for normal programs, for three reasons. First,
security-critical programs in smartphones already use some
I/O devices exclusively. For example, the UI (display and
touchscreen) is used exclusively (e.g., when using TrustZone-
based Protected Conﬁrmation [1]) due to its small form factor.
Second, the performance impact on other I/O types, such
as networking and storage, can be minimal when security-
critical programs use short sessions, e.g., a few seconds. In
§9.2, we experimentally demonstrate this impact for storage.
Moreover, TCP network connection keepalives persist for tens
of seconds. Further, since smartphone network connections
are frequently dropped during handoffs, most widely used ap-
plications transparently re-establish lost connections without
user visible changes. Security-critical programs can be de-
signed to initiate, use, and close their connections in a single
session (a practice that we use in our own sample programs).
It is also possible to mitigate these issues using multiple I/O
domains of the same type. For example, all smartphones have
both WiFi and cellular network interfaces. One can imagine
allowing normal programs to share and use one of these while
security-critical programs use the other (through two separate
I/O domains in our hardware).

Third, most security-critical programs rely on only a subset
of the I/O domains. For example, our insulin pump program
mainly requires access to its sensors and a brief access to
storage. While this program is running, all other I/O domains,
e.g., network, UI, and even storage, can be used by normal
programs.

5 OctopOS

We introduce OctopOS, an OS to manage the split-trust
hardware. Unlike existing OSes, which have an all-powerful
trusted-by-all kernel, OctopOS is composed of mutually-
distrustful components. These components include I/O ser-
vices for I/O domains, a runtime for TEE domains, a resource
manager, and a compatibility-layer for the untrusted domain.

I/O Services

5.1
Each I/O domain runs a service to manage it. The I/O service
incorporates the software stack needed to program and use an
I/O device, e.g., device driver. In addition, it provides an API
that can be called (through messages) by any client domain,
i.e., the domain that has exclusive access to the mailboxes of
the corresponding I/O domain.

I/O services play a role in ensuring that they are reset before

and after use by other domains, as discussed next.

Non-restricted I/O devices. Let us ﬁrst consider I/O devices
that can be used by a security-critical program without any
restrictions during a session, such as the serial output and
network devices in our prototype. For these devices, when
a security-critical program asks for access to an I/O device,
the manager resets the corresponding I/O domain (which
triggers the bootloader on the I/O domain to clean all the
state information in it) and then delegates its mailboxes to
the TEE domain running the program. The program ﬁrst
uses the mailboxes of the corresponding I/O domain to verify
its exclusive access and the session quota. It then uses the
attestation report from TPM to verify the software loaded
in the I/O domain. It also veriﬁes that the I/O domain has
not been used since it has been reset. The latter requires
assistance from the I/O service. That is, upon receiving the
very ﬁrst message and before processing the message (in order
to prevent exposure to adversarial inputs), the service further
extends the domain PCR register in TPM with a constant.
This way, the report from TPM reveals whether the domain
has been used or not.

Before the program’s session expires or is yielded, the pro-
gram also needs to ensure that the I/O domain will be reset
again before use by any other domain. We also achieve this
with assistance from the I/O service. That is, before yield-
/expiration, the program calls an API in the I/O service to
disable message processing, after which the service becomes
unusable until it is reset again.

Restricted devices. Next, let us consider I/O devices that
cannot be used freely by a security-critical program during
a session and require the resource manager to enforce re-
strictions (i.e., ﬁne-grained access control). In our prototype,
storage falls in this category because it contains data of other
programs as well. Even if the data are encrypted, they need
to be protected if a general availability guarantee is needed
(§7.4). For these devices, we still ensure exclusive access to
the domain during the session. We also ensure reset after use.
However, we cannot ensure the domain is reset to a clean state
before use. This is because after reset, the resource manager
needs to communicate with the I/O service to restrict its usage
before delegating the domain’s mailboxes to a TEE domain.

We have carefully designed an API for such I/O services.
The core of the API revolves around the notion of an I/O
resource. For example, in the case of the storage service, each
partition is a resource. The API allows the manager to allocate
resources and bind them to speciﬁc security-critical programs.
It also allows the program to authenticate itself in order to use
the resource and to verify the status of the service. We omit
the details of the API due to space limitation. Finally, we note
that this design makes the storage service strongly-trusted
(§7.4).

6

5.2 TEE Runtime
In order to facilitate the development of security-critical pro-
grams, we have developed a runtime for TEEs, which provides
a high-level API. A program can choose to use this runtime,
or can use its own.

We provide several categories of functions in this API:
(1) Requesting and verifying access to other domains (§5.1);
this category also helps the program manage the remaining
quota of mailboxes by calling a callback function upon quota
updates. (2) High-level abstractions for using I/O services
such as socket-based networking and terminal prints. (3) As-
sistance with the TPM, e.g., to request a remote attestation
report. (4) Support for secure IPC between TEE domains. (5)
Security-critical routines such as cryptographic primitives.

5.3 Resource Manager
At a high level, the resource manager is in charge of resource
scheduling, access control, and system-wide, untrusted I/O
functionalities. More speciﬁcally, it performs the following
three tasks. First, it makes constrained scheduling decisions.
When a new security-critical program needs to execute, or
when an existing one requests exclusive communication with
another domain (for I/O or IPC), the manager checks the avail-
ability of resources, grants the request, or blocks it until the
resource is available. Compared to schedulers in commodity
OSes, scheduling in OctopOS is more restricted. This is be-
cause the resource manager cannot preempt a domain as long
as mailbox quotas have not expired (§4.2).

Second, the resource manager restricts the usage of some
I/O domains to enforce ﬁne-grained access control, as dis-
cussed in §5.1.

Finally, the manager implements system-wide, untrusted
I/O functionalities. For example, as the manager is the initial
client of the input and output domains, it implements the shell
(i.e., the UI). The UI, however, can be delegated to security-
critical programs upon request.

5.4 Untrusted Domain’s Compatibility Layer
In OctopOS, a commodity OS runs in the untrusted domain,
and hence by deﬁnition manages its own processor and mem-
ory. Yet, the commodity OS is not given direct control of I/O
devices as they are managed by separate I/O domains.

We address this issue by developing a compatibility layer
for the untrusted OS. In our prototype, which uses PetaLinux,
the compatibility layer consists of several kernel modules,
each pretending to be a device driver. Transparent to Linux
and its program, they communicate to the resource manager
to get access to the I/O services’ mailboxes (or to setup DMA)
and then communicate to them.

6 Prototype

We have built a prototype of the split-trust hardware and Octo-
pOS on the Xilinx Zynq UltraScale+ MPSoC ZCU102 FPGA

board. We use the Cortex A53 ARM processor on the SoC for
the untrusted domain in order to achieve high performance
for the commodity OS (PetaLinux) and its programs. We
use the FPGA to synthesize 7 simple Microblaze microcon-
trollers (i.e., no MMU and no cache): two TEE domains, the
resource manager domain, and four I/O domains (serial input,
serial output, storage, and Gigabit Ethernet). We leverage the
(single-threaded) Standalone library [105] to program the mi-
crocontrollers. We use the entirety of the main memory for
the untrusted domain. For other domains, we use a total of
3.2 MB of on-chip memory including some ROM for boot-
loaders and some RAM. We run the TPM (emulator) [49] on
a separate Raspberry Pi 4 board connected to the main board
through serial ports. We use another Microblaze microcon-
troller to mediate the communications of the domains with
the TPM.

In addition, we use the FPGA to synthesize the mailboxes
(12 in total), the arbiter for DMA for the network domain
(other domains do not support DMA), the reset guard, as well
as 11 hardware queues for permanent domain connections
(such as for all domains to communicate with TPM or for
TEE domains to communicate with the resource manager).
We note that we synthesize two types of mailboxes: control-
plane mailboxes and data-plane mailboxes. The former has
4 messages of 64 B each and the latter has 4 messages of
512 B each. As a concrete example, our storage domain has 4
mailboxes: two for its control plane (send/receive) and two
for its data plane (send/receive).

As mentioned in §4.1, an I/O device is only programmable
by its domain. This includes access to registers and receiving
interrupts from the I/O device. In our prototype, we use I/O
interrupts only for the network device and use polling for the
rest. The interrupts to the network domain’s microcontroller
is from the FIFO queue that holds the packets and are only
used when the domain serves a TEE domain (§4.5). When
serving the untrusted domain, the domain-bound DMA engine
directly interrupts the A53 processor on DMA completion.

We faced one noteworthy limitation in our prototype: the
on-board SD card reader and ﬂash memory are directly pro-
grammable by the A53 processor and hence could not be
used for the storage domain. Our solution was to connect a
MicroSD card reader directly to FPGA through Pmod [34].
This provides physical isolation for the storage domain, but
signiﬁcantly degrades its performance due to Pmod’s limited
throughput. Therefore, for performance evaluation, we instead
use DRAM as our storage (we partition out a chunk of DRAM
and use it exclusively for the storage domain). This allows
us to stress the performance of the mailboxes of the storage
domain and get an upper bound for our storage performance,
which we cannot do with the Pmod prototype.

We note that requiring an FPGA board to experiment with
our machine may pose a road block for many researchers.
Therefore, we also develop an emulator for our hardware
design. The emulator runs on a Linux-based host OS such as

7

Property

Proved theorems

Exclusive
access

Limited
delegation

Excl. acc-
ess verif.

Default
exclusive
access

Conﬁde-
ntiality

Domains w/o exclusive access to mailbox cannot change which domain has exclusive access, nor the remaining quota.
If a domain does not yield its exclusive access, its exclusive access is guaranteed as long as the quota has not expired.
The domain with exclusive access to the mailbox can correctly read or write from/to the queue.
The domains w/o exclusive access to the mailbox cannot read/write to the queue.

When given exclusive access, a domain cannot use the mailbox more than its delegated quota.
When the quota delegated to a domain expires, the domain loses exclusive access.

The domain with exclusive access can correctly verify its exclusive access and remaining quota.
The domain on ﬁxed end of mailbox can correctly verify domain with exclusive access on the other end and remaining quota.

After reset, the resource manager domain has exclusive access by default.
The resource manager domain does not lose its exclusive access unless it delegates it.
When a domain loses exclusive access (yield/expiration), the exclusive access will be given to the resource manager domain.

Domains w/o excl. access cannot use mailbox’s verif. interface to ﬁnd out which domain has excl. access and remain. quota.
Upon delegation/yield/expiration, the data in the queue is wiped.

Table 1: Theorems we prove for our mailbox. Proving some of these theorems require proving multiple lemmas not listed here.

Ubuntu and is able to fully boot and run OctopOS.

Overall, we have implemented OctopOS and our hardware
emulator in about 39k lines of C code (including 5k of modi-
ﬁed drivers from Xilinx and crypto libraries). We report the
LoC for our hardware below.

them using the ∪ sign. The elements in front of s: and w:
name the strongly- and weakly-trusted components. For suc-
cinctness, we tag a repeating component with a number in
parenthesis on its ﬁrst appearance and use the number in other
locations.

6.1 Veriﬁed Hardware Design
The split-trust hardware has only four simple hardware com-
ponents that are strongly trusted (§7.4): mailbox, DMA ar-
biter, reset guard, and ROM (for bootloaders). We have im-
plemented these components in 1630 lines of Verilog code as
well as 800 lines of Python code.

The simplicity of our strongly-trusted hardware compo-
nents enables us to formally verify them. We use SymbiYosys
to perform formal veriﬁcation [39]. SymbiYosys is a front-
end for Yosys-based formal hardware veriﬁcation ﬂows. We
use the SMTBMC engine, which uses k-induction to formally
verify safety features in hardware. Table 1 shows the list of
theorems we prove for our mailbox (we omit the rest due to
space limitation). Overall, we developed 3000 lines of Sys-
temVerilog code for our hardware veriﬁcation.

7 TCB & Security Analysis

7.1 TCB Notation
The owner of a smartphone, when running a security-critical
program in it, trusts various hardware and software compo-
nents: the TCB. We introduce and use a simple, compact
notation for TCB, discussed here with an abstract example:

G1,G2s: CompA(1), CompB(2)

(cid:62)s: CompA(1), CompB(2)

w: CompC(3)

∪

Owner

G3s: 1,2,3

(cid:62)s: 1,2,3

w:

The key operator is the (cid:62) sign, which resembles a T (as in
Trust). It helps denote a set of trust assumptions. The elements
on top of the (cid:62) sign, e.g., G1, are the security guarantees,
e.g., conﬁdentiality and integrity. This allows for differentiat-
ing trust assumptions for different guarantees and combining

7.2 Lower Bound of TCB
This can be achieved if the machine is dedicated to executing
a security-critical program:

C,I,Aw: Proc.,Mem.,I/O,P.HW,interconnects

(cid:62)s: Prog.,RoT

Owner

w: Proc.,Mem.,I/O,P.HW,interconnects

where C, I, A stand for Conﬁdentiality, Integrity, and Avail-
ability; P.HW is the protection hardware (e.g., MMU and
IOMMU); and RoT stands for Root of Trust.

This shows that the owner at the very least needs to strongly
trust the (security-critical) program and the RoT. The strong
trust in the program is fundamental: the program needs to
protect itself against adversarial inputs, e.g., malicious net-
work packets. Note that the program in the TCB includes the
runtime used by the program to interact with the hardware.

The strong trust in the RoT is also fundamental and stems
from the fact that an adversary controlling the machine may
try to fool the veriﬁer of remote attestation by attempting
to attack and compromise the RoT. The strong trust in the
RoT includes strong trust in the bootloader, the ROM used to
store the bootloader, the hardware/ﬁrmware used for remote
attestation, e.g., TPM, as well as the hardware vendor that
certiﬁes attestation reports.

7.3 TCB of Existing Systems
First, we consider a traditional system that uses an OS to
provide isolation:

C,I,As: Prog.,OS,Proc.,Mem.,I/O,interconn.,P.HW,RoT

(cid:62)s: Prog.,OS,Proc.,Mem.,I/O,interconn.,P.HW,RoT

w:

Owner

8

C,Is: Prog.(1),SM(2),Proc.(3),Mem.(4),Sec-I/O(5),interconn.(6),P.HW(7),RoT(8)

(cid:62)s: Prog.(1),SM(2),Proc.(3),Mem.(4),Sec-I/O(5),interconn.(6),P.HW(7),RoT(8)

w:

As: 1,2,3,4,5,6,7,8,OS

(cid:62)s: 1,2,3,4,5,6,7,8,OS

w:

∪

Owner

C,I,Ass: Prog.(1),mailbox(2),reset-guard(3),arbiter(4),RoT(5)

(cid:62)s: Prog.(1),mailbox(2),reset-guard(3),arbiter(4),RoT(5)

w: Proc.(6),Mem.(7),I/O(8),interconnects(9)

∪

Owner

Ags: 1,2,3,4,5,RM,SD

(cid:62)s: 1,2,3,4,5,RM,SD

w: 6,7,8,9

(1)

(2)

This shows that the owner strongly trusts the hardware in-
cluding the processor, memory, I/O devices, protection hard-
ware, and interconnects. Moreover, the OS is also strongly
trusted, including device drivers. In this case, the program
includes the libraries used by the program to interact with the
OS and hardware.

Next, we write the TCB for a popular TEE solution for
smartphones, TrustZone, in Formula 1. SM is the security
monitor (i.e., the secure world OS and monitor code). We note
that TrustZone allows the secure world to take full control
of an I/O device, i.e., secure I/O (Sec-I/O). Yet, this device
and its driver are exposed to multiple programs in the secure
world and hence are strongly trusted. Another noteworthy
issue is that the OS is strongly trusted when availability is
needed as it is in charge of resource scheduling.

7.4 Our TCB
Formula 2 shows the TCB of our machine. As, Ag, SD, and
RM stand for session availability, general availability, storage
domain, and resource manager, respectively. Our system re-
quires strong trust in a few cases that were not part of the
lower bound. First, for conﬁdentiality, integrity, and session
availability, the owner needs to strongly trust the mailboxes
used by the program, the arbiter (if domain-bound DMA is
used), and the domain reset guard as these components in-
teract with untrusted components. As discussed in §6.1, the
simple design of these components allowed us to formally
verify them, making this strong trust acceptable (§2.1). Sec-
ond, if a program needs general availability guarantees (e.g., it
needs to be executed in ﬁxed intervals) and needs to store data
across sessions, it needs to strongly trust the resource manager
domain and the storage domain. The only way to eliminate
the strong trust in the storage domain for general availability
is to have separate storage devices for each security-critical
program. Unfortunately, this is prohibitively expensive.

It is noteworthy that our machine eliminates the need to
strongly trust several complex hardware and software compo-
nents such as the processor, memory, I/O devices, the intercon-
nects (since our machine does not share any busses between
trust domains) and system software (security monitor, OS,
and device drivers), compared to existing TEEs. Moreover,
the hardware component listed as weakly-trusted (processor,
memory, I/O, and interconnects) are those of the domains
used by the security-critical program. This has important im-
plications: for example, it means that the complex powerful

processor of the untrusted domain is not trusted at all (not
even weakly). Overall, the TCB of our machine is signiﬁcantly
smaller than modern, popular TEEs. Moreover, our TCB is
rather close to the lower bound. Achieving a smaller TCB
for a machine that can host security-critical and untrusted
programs concurrently would be challenging.

7.5 Security Analysis
Threat model. We assume an adversary can run untrusted
and security-critical programs in the machine and tries to
exploit any software or hardware vulnerabilities. Below, we
discuss various such attacks and their implications. Physical
attacks are out of scope.
Software vulnerability-based exploits. Vulnerabilities in
strongly-trusted software components would lead to attacks.
An attacker that compromises the program can obviously
change its behavior. An attacker that compromises the boot-
loader (including the code that cleans up the state in a domain
upon reset) can falsify the remote attestation report or ac-
cess/impact data from other sessions. An attacker that can
compromise the storage service can delete the program’s data.
An attacker that can compromise the resource manager can
starve the program of resources (but cannot impact the avail-
ability of a given session once it is granted). An attacker that
manages to compromise other software components, e.g., I/O
services, other security-critical programs, and the untrusted
OS, cannot mount an attack on the program.
Hardware vulnerability-based exploits.
In a split-trust
machine, unlike existing TEEs, vulnerabilities in many com-
plex hardware components such as the processor cannot be
exploited since the adversary never shares the underlying
hardware with the security-critical program. Therefore, the at-
tacker cannot leverage various hardware-based attacks such as
cache side-channel attacks, interconnect side-channel attacks,
speculative execution attacks, and Rowhammer attacks. Only
vulnerabilities in the strongly-trusted hardware components
(i.e., mailbox, arbiter, reset guard, ROM, and TPM) would
lead to attacks. The ﬁrst four of these are formally veriﬁed
(§6.1) and TPM is a mature and secure technology.
Timing side-channel attacks. All strongly-trusted soft-
ware and hardware components are vulnerable to timing side-
channel attacks. In our machine, the only components that
may expose useful timing channels are the TPM and the
program runtime. Such attacks (and others) have been demon-
strated on TPMs before [19,45,51,67,88]. As TPM is a mature

9

technology, vulnerabilities get ﬁxed. Indeed, there have been
several works that formally verify various aspects of the TPM
standard [23, 85, 101]. We have not analyzed the timing chan-
nel of the runtime we have developed for security-critical
programs.
Power management attacks. These types of attacks can
induce faults in the victim program’s execution by manipulat-
ing the frequency or voltage of the processor and have been
demonstrated against TEEs [69,80,90]. As mentioned in §4.3,
our machine does not allow power management of a domain
in a session, and hence mitigates such attacks.

Power management data can also be used as a side channel.
More speciﬁcally, an attacker man try to monitor the volt-
age and frequency of a domain (which changes according to
DVFS) and use that as a side channel to extract secrets from
a domain. Indeed, the recent Hertzbleed attack uses this side
channel to extract secret cryptographic keys [98].

We note that our current prototype is not vulnerable to this
side channel since our TEE domains do not support DVFS.
However, our hardware can support the use of DVFS-capable
processors for TEE domains. In such a case, we will need to
close this channel. To do so, we will need to ensure that the
PMU does not leak any information about a domain to another
domain. This can be done rather trivially within the PMU
ﬁrmware, which should be formally veriﬁed and hardened.
Remote network attacks. Similar to a legacy machine, a
security-critical program must protect itself against malicious
network messages in our machine. However, our machine
provides some protection against network attacks that target
the network stack. This is because it sandboxes the network
device and its device driver in its own domain. As a result,
programs that do not use the network at the time of a exploit
are protected from these attacks. This is in contrast to a legacy
machine in which a single successful exploit of the kernel-
based network stack may result in a full takeover.

8 Sample Security-Critical Programs

We brieﬂy discuss two security-critical programs that we have
built for our machine.
I. Secure banking. Our secure banking program ensures
that only the user can access conﬁdential account information.
The program leverages several features of our machine. First,
it uses exclusive access to the UI (i.e., shell) to make sure
all inputs come from the user (and not malware) and that
outputs are only displayed to the user. Upon getting exclusive
access to the UI, the program needs to convince the user that
they are interacting securely with the program. It does so by
displaying a secret established a priori between the user and
the bank.

Second, the program uses exclusive access to the network
domain to transfer conﬁdential information. One might won-
der why it is not adequate to use a secure networking proto-
col, such as TLS, for this purpose. Such protocols leave open

FPGA resource

Count

Equivalent transistor count

Look-up table
Flip ﬂop
Block RAM

69,999
63,188
27,061,649 (bits)

2,519,964
1,516,512
162,369,894

Table 2: Cost of additional hardware in our machine.

some side-channel attack vectors [103], which our exclusive
network access closes against on-device attackers; external
network side-channel attacks are still possible. Finally, the
program uses remote attestation to enable the bank cloud
server to trust the program running on the user’s device.
II. Secure insulin pump. Diabetic patients need to monitor
the glucose level in their blood and administer insulin accord-
ingly. New glucose monitor and insulin pumps have recently
emerged that can perform this task automatically [93].

We build this security-critical program in our OS. This
program leverages exclusive access to the glucose monitor
and insulin pump, e.g., through the headphone jack or via
Bluetooth. This way, the program can securely authenticate
itself to the these devices and not worry that the session may
be hijacked. The program also uses exclusive access to the
network domain to retrieve authentication tokens from the
health provider’s server and uses remote attestation to en-
able the provider’s server to trust the program. Finally, this
program needs to be executed in ﬁxed intervals and store its
sensor readings across sessions. For this, it trusts the resource
manager and the storage domain, as discussed in §7.4.

9 Evaluation

9.1 Hardware Cost
We calculate an estimate for the number of transistors needed
for our additional hardware components (all the components
synthesized on the FPGA in our prototype). We calculate this
estimate by measuring the number of look-up tables, ﬂip ﬂops,
and block RAMs used by our hardware and converting them
to transistor count using the following estimates: 6 NAND
gates per look-up table [79], 6 transistors per NAND gate [89],
24 transistors for each ﬂip ﬂop [86], and 6 transistor for each
bit of on-chip memory (assuming a conventional 6-transistor
SRAM cell [13]). Our calculation shows that our machine
requires about 166.4 M additional transistors (162 M of which
are used for on-chip memory). Table 2 shows the breakdown.
This compares favorably with the number of transistors used
in modern SoCs. For example, Apple A15 Bionic and HiSil-
icon Kirin 9000 use 15 B transistors [35, 84]. This means
that, if our solution is added to an SoC or implemented as
a chiplet [70], the additional hardware cost would likely be
1-2%.

9.2 Performance
We measure various performance aspects of our machine.
Note that all domains except the untrusted one use an FPGA

10

Conﬁguration

Throughput (MB/s)

Latency (µs)

A53-Microblaze
Microblaze-Microblaze

7.07±0
9.64±0.01

18.2±0
15.26±0.05

Table 3: Mailbox performance.

with a 100 MHz clock. The Ethernet controller IP uses an ex-
ternal 50 MHz clock. Therefore, our results represent a lower
bound on our machine’s performance; we expect superior
performance on ASIC. We repeat each experiment 5 times
and report the average and standard deviation.
Mailbox performance. We measure the throughput and la-
tency of communication over our mailbox. For throughput,
we measure the time to send 10,000 messages of 512 B over
a data-plane mailbox. For latency, we measure the round trip
time to send a 64 B message and receive an acknowledgment
over a control-plane mailbox. We perform these experiments
in two conﬁgurations: one for communication between the
hard-wired ARM Cortex A53 (the untrusted domain) and an
FPGA-based Microblaze microcontroller, and one for com-
munication between two FGPA-based Microblaze microcon-
trollers. Table 3 shows the results. One might wonder why the
A53-Microblaze conﬁguration achieves lower performance.
We believe this is because this conﬁguration requires the data
to pass the FPGA boundary, hence passing through voltage
level shifters and isolation blocks [106]. Moreover, the FPGA
is in a different clock domain than A53.
Storage performance. We measure the performance of our
storage domain, which uses the mailbox for its data plane (i.e.,
no DMA). To do so, we perform 2000 reads/writes of 512 B
each. We evaluate three conﬁgurations: a best-case conﬁgura-
tion where the storage domain directly performs reads/writes
(hence giving us an upper bound on the DRAM-based storage
performance), and two conﬁgurations where the untrusted
domain or a TEE domain uses the storage service over the
domain’s mailboxes. Table 4 shows the results. They show
that our mailbox-based storage domain can achieve decent
performance (as can also be seen from our boot-time measure-
ments reported below). It also shows that the additional copies
caused by the mailbox add noticeable overhead compared to
the best-case scenario. To further improve this performance
for the untrusted domain, one can use domain-bound DMA
for the storage domain.
Network performance. We measure the performance of
our network domain, which uses domain-bound DMA for
high performance for the untrusted domain (§4.5). We eval-
uate three conﬁgurations, similar to those used for storage
experiments. For measuring the throughput for the baseline
and the untrusted conﬁgurations, we use iPerf; for round-trip
time (RTT) measurements, we use Ping. For the TEE con-
ﬁguration, we develop custom programs for measurements.
For all experiments, we connect the board to a PC, which acts
as a server. Table 5 shows the results. They show that our
domain-bound DMA is capable of matching the performance

Conﬁguration

Best-case
Untrusted dom.
TEE domain

Read throughput(MB/s) Write throughput(MB/s)

8.13±0.00
4.17±0.09
4.39±0.00

6.10±0.00
4.06±0.00
3.93±0.00

Table 4: Storage performance.

Conﬁguration

Throughput (Mbit/s)

RTT (ms)

Baseline
Untrusted domain
TEE domain

943±0
943±0
0.567±0.001

0.17±0.01
0.17±0.02
23.92±0.02

Table 5: Network performance.

of a legacy machine. Moreover, the network performance for
a TEE is decent.

We believe, based on some tests that we have conducted,
that it is possible to further improve the TEE network per-
formance by about 10 X. This is because, currently in the
network domain, we add an artiﬁcial delay between accessing
the mailbox and the network IP, which limits performance.
We do so to prevent data corruption, which according to our
extensive investigation, is caused by a bug in the Ethernet
AXI IP from Xilinx (potentially the bug discussed in [5]).
Since the IP is closed source, we are not able to ﬁx the bug.
Boot time and breakdown. We measure the time it takes
our system to boot. All the boot images are transferred from
the storage domain to their corresponding domains over do-
main mailboxes. Due to presence of multiple domains, boot-
ing OctopOS from a partition in the storage service is a care-
fully choreographed dance. We boot OctopOS from the boot
images stored in the boot partition of the storage service
(§5.1), requiring steps taken by the bootloaders in each do-
main and the resource manager, as follows.

• The ﬁrst domain to boot is the storage service. The boot-
loader of this domain reads the storage service image
from the drive and loads it onto its processor.

• The next domain to boot is the resource manager. The
bootloader of this domain communicates with the stor-
age service using its API called over its mailbox. The
bootloader uses a simple ﬁle system (which is used to
create the boot partition) to read the right blocks of data
containing the resource manager image.

• Once the resource manager is booted, it assists all other
domains to boot. To do so, it invokes the storage service
API in order to transfer the required images onto its data
plane. It then delegates the data plane to the correspond-
ing domain, the bootloaders on which can receive the
images and load them.

We note that the boot process includes extending the mea-
surements of the boot images to the corresponding TPM PCRs
as well. This is mainly done by the bootloaders , which are cur-
rently the root of trust in our prototype. We have implemented

11

a ROM for each domain to store its bootloader , programmed
as a part of the hardware bitstream. Our measurements show
that it takes 4.03±0.00 s to boot all domains excluding the
untrusted domain, which takes an additional 8.65±0.01 s to
boot.
Untrusted program performance. We use the network ﬁle
system to evaluate the performance of an untrusted program.
Our benchmark reads 100 ﬁles each containing 10,000 ran-
dom numbers from a network ﬁle system, sorts them, and
writes them back to the same ﬁle system. We choose this
benchmark since it stresses CPU, memory, and network (for
which we have domain-bound DMA). Our evaluation shows
the benchmark takes the same amount of time (3.86±0.03 s)
on our machine as it takes on a legacy machine with the same
A53 processor, RAM, and Gigabit Ethernet (3.84±0.04 s).
Security-critical program performance. We measure the
execution time of a security-critical program. The program
reads a 1 MB ﬁle from the storage domain, computers its hash,
and sends the hash over the network to a server. Our measure-
ments show that the overall execution time, when no other
domain needs and hence competes for the storage and net-
work domains, is 2.37±0.00 s. Looking at the breakdown, this
program takes 0.94±0.00 s to launch (including time needed
to acquire exclusive access to storage and network, excluding
local attestation through TPM), 0.22±0.00 s to read the ﬁle
from storage, 1.21±0.00 s to compute the hash, and less than
0.001 s to send the hash over the network. (We however note
that it is possible that the program might need to wait for the
storage and/or network domains to become available if they
are being used by other domains, e.g., the untrusted domain.)
To better asses this execution time, we write a normal pro-
gram to perform similar tasks on a legacy machine with the
A53 processor, RAM-FS, and Gigabit Ethernet. This program
takes 0.23±0.00 s to execute.
Impact of exclusive I/O use. We evaluate the impact of
executing a security-critical program that uses storage on the
storage performance of the untrusted domain. More speciﬁ-
cally, we launch a security-critical program in a TEE that ex-
clusively reads 1 MB from and writes 1 MB to storage, while
the untrusted domain is reading a 100 MB ﬁle data (which
normally takes 24.26±0.31 s to ﬁnish). Our measurements
show that the security-critical program causes a 2.58±0.03 s
gap where the untrusted domain cannot access the storage.

10 Thoughts on Scalability and Performance

of TEEs

The exclusive use of TEE domains limits the number of con-
current security-critical programs. Moreover, our choice of
using weak microcontrollers, small amounts of memory, and
I/O without DMA for TEEs limits the performance of security-
critical programs. We believe that the former is not a serious
issue since we do not expect a large number of security-critical

programs executing simultaneously in a personal computer.
The latter is mostly a non-issue either since security-critical
programs are more concerned with security guarantees than
performance. However, there are exceptions, for example,
authentication of the user by applying machine learning algo-
rithms to photos taken by the camera. We believe that these
programs can leverage accelerators (which will be available
in the machine in the form of additional I/O domains). Indeed,
Nider et al. propose a machine with no CPU and several self-
managed devices [74], showing the diminished role of CPU
for performance.

11 Related Work

Physical isolation. Notary [12] safeguards approval trans-
actions by running its agent on a separate SoC from the ones
running the kernel and the communication stack. Our work
shares the idea of using physically-isolated trust domains and
also resets the domains before and after use by other programs.
In contrast, we show how to safely mediate access to shared
I/O devices for a workload of concurrent security-critical and
untrusted programs.

Likewise, I/O-Devices-as-a-Service (IDaaS) suggests that
I/O devices should have their own separate microcontrollers
(and observes that they often do) and advocates for hardening
their interfaces against potentially malicious kernel behav-
ior [11]. Our approach also uses separate I/O microcontrollers
but does not require strong trust in the microcontroller soft-
ware, by resetting the I/O domain between uses.
Exclusive use. Flicker [65] uses the late launch feature of
Intel Trusted Execution Technology (TXT) [36], to exclu-
sively run a program on the processor. The exclusive use
of the hardware results in minimizing the strongly-trusted
components. However, Flicker’s design requires stopping all
other programs (including untrusted ones) when running a
security-critical program. Our approach can run untrusted pro-
grams and security-critical programs concurrently (albeit with
the limitation that I/O domains cannot be shared). Consider
our secure insulin pump program (§8), which might need to
be run frequently while the user is actively doing other, less
security-critical, tasks on the main processor.
Secure I/O for TEEs. SGXIO uses a hypervisor and a TPM
to create a trusted path for an SGX enclave to access an
I/O device [100]. The solution requires the enclave program
not only to trust SGX’s ﬁrmware and hardware, but also the
hypervisor. CURE [15] adds a few hardware primitives in
order to allow the security monitor to assign a peripheral (i.e.,
access to MMIO registers and DMA target addresses) to an
enclave. These primitives are designed to be programmed by
a trusted-by-all security monitor (unlike our work).
Time protection. Ge et al. add time protection to seL4,
which closes many of the available side channels in commod-
ity processors [37]. As the paper mentions, some processors
do not provide mechanisms needed to close channels. More-

12

over, channels using busses could not be closed, and they have
recently been shown to be effectively exploitable [75]. Our
approach of using completely separate hardware for security-
critical programs addresses these concerns for these programs.
We do, however, note that our approach (as it stands) does not
scale to support all (normal) programs, which may have their
own security needs. Therefore, we believe that time protection
remains an important abstraction to be explored for when the
same processor is asked to host multiple programs.
Other TEE solutions. Komodo is a veriﬁed security moni-
tor that can create enclaves for security-critical programs [33].
Use of formal veriﬁcation warrants the strong trust in the
security monitor, but not the ARM processor that hosts both
security-critical and untrusted programs.

Sanctum uses hardware modiﬁcations to RISC-V alongside
a software security monitor to create isolated enclaves [27].
Compared to SGX, Sanctum enclaves are protected against
both cache and page fault side-channel attacks. While this is
important, Sanctum does not address other potential hardware
vulnerabilities such as side channels through interconnects.

12 Conclusions

Smartphone owners expect to use their devices for a mixture
of security-critical and ordinary tasks, yet this requires strong
trust that the hardware and system software is able to iso-
late those tasks from each other, trust that is often misplaced.
Our goal in this work is to minimize the TCB when execut-
ing security-critical programs. We present a hardware design
with multiple statically-partitioned, physically-isolated trust
domains, coordinated using a few simple, formally-veriﬁed
hardware components, along with OctopOS, an OS to manage
this hardware. We describe a complete prototype implemented
on an FPGA and show that it incurs a small hardware cost.
For security-critical programs, our machine signiﬁcantly re-
duces the TCB compared to existing solutions, and achieves
decent performance. For normal programs, it achieves similar
performance to a legacy machine.

Acknowledgments

The work was supported by NSF Awards #1617513,
#1718923, #1846230, and #1953932. The authors thank Felix
Xiaozhu Lin for his invaluable feedback on an earlier draft
of this paper. They also thank Xilinx for donating an FPGA
board to this project.

References

[2] Bugs and Vulnerabilities Founds by Syzkaller in Linux
Kernel. https://github.com/google/syzkaller/
blob/master/docs/linux/found_bugs.md, 2018.

[3] Apple Platform Security

- Secure Enclave.

https://support.apple.com/guide/security/
secure-enclave-sec59b0b31ff/web, 2021.

Face

Platform
ID

Security
security.

[4] Apple
and
support.apple.com/guide/security/
touch-id-and-face-id-security-sec067eb0c9e/
web, 2021.

ID
https://

Touch

-

[5] Common AXI Themes on Xilinx’s Forum (see
Section “Out-of-protocol designs” for the discus-
sion on a bug in Xilinx’s Ethernet-lite con-
troller). https://zipcpu.com/blog/2021/03/20/
xilinx-forums.html, 2021.

[6] CVE Details. Linux Kernel: Vulnerability Statis-
tics. https://www.cvedetails.com/product/47/
Linux-Linux-Kernel.html, 2021.

[7] CVE Details. Op-tee: Vulnerability Statistics.
https://www.cvedetails.com/product/56969/
Linaro-Op-tee.html, https://www.cvedetails.
com/product/42749/Linaro-Op-tee.html,
https://www.cvedetails.com/product/36161/
Op-tee-Op-tee-Os.html, 2021.

[8] CVE Details. Windows 10: Vulnerability Statis-
https://www.cvedetails.com/product/

tics.
32238/Microsoft-Windows-10.html, 2021.

[9] CVE Details. XEN: Vulnerability Statistics.
https://www.cvedetails.com/product/23463/
XEN-XEN.html, 2021.

[10] Mike Accetta, Robert Baron, William Bolosky, David
Golub, Richard Rashid, Avadis Tevanian, and Michael
Young. Mach: A New Kernel Foundation For UNIX
Development. In Proc. Summer 1986 USENIX Confer-
ence, 1986.

[11] A. Amiri Sani and T. Anderson. The Case for I/O-
Device-as-a-Service. In Proc. ACM HotOS, 2019.

[12] A. Athalye, A. Belay, M.F. Kaashoek, R. Morris, and
N. Zeldovich. Notary: A device for secure transaction
approval. In Proc. ACM SOSP, 2019.

[1] Android Protected Conﬁrmation.

https:

//android-developers.googleblog.com/2018/
10/android-protected-confirmation.html,
2018.

[13] P. Athe and S Dasgupta. A Comparative Study of
6T, 8T and 9T Decanano SRAM cell. In Proc. IEEE
Symposium on Industrial Electronics & Applications,
2009.

13

[14] A. M. Azab, K. Swidowski, J. M. Bhutkar, W. Shen,
R. Wang, and P. Ning. SKEE: A Lightweight Secure
Kernel-level Execution Environment for ARM. In Proc.
ACM MobiSys, 2016.

[15] R. Bahmani, F. Brasser, G. Dessouky, P. Jauernig,
M. Klimmek, A. Sadeghi, and E. Stapf. CURE: A Se-
curity Architecture with CUstomizable and Resilient
Enclaves. In Proc. USENIX Security Symposium, 2021.

[16] T. Ball, E. Bounimova, B. Cook, V. Levin, J. Licht-
enberg, C. McGarvey, B. Ondrusek, S. K. Rajamani,
and A. Ustuner. Thorough Static Analysis of Device
Drivers. In Proc. ACM EuroSys, 2006.

[17] A. Baumann, M. Peinado, and G. Hunt. Shielding
Applications from an Untrusted Cloud with Haven.
Proc. USENIX OSDI, 2014.

[18] F. Brasser, U. Müller, A. Dmitrienko, K. Kostiainen,
S. Capkun, and A. Sadeghi. Software Grand Exposure:
SGX Cache Attacks Are Practical. In Proc. USENIX
Workshop on Offensive Technologies (WOOT), 2017.

[19] J. Butterworth, C. Kallenberg, X. Kovah, and A. Her-
zog. BIOS Chronomancy: Fixing the Core Root of
Trust for Measurement. In Proc. ACM CCS, 2013.

[20] D. Cerdeira, N. Santos, P. Fonseca, and S. Pinto. SoK:
Understanding the Prevailing Security Vulnerabilities
in Trustzone-assisted TEE Systems. In Proc. IEEE
Symposium on Security and Privacy (S&P), 2020.

[21] G. Chen, S. Chen, Y. Xiao, Y. Zhang, Z. Lin, and T. H.
Lai. Sgxpectre: Stealing intel secrets from sgx en-
claves via speculative execution. In IEEE European
Symposium on Security and Privacy (EuroS&P), 2019.

[22] H. Chen, Y. Mao, X. Wang, D. Zhou, N. Zeldovich, and
M. F. Kaashoek. Linux kernel vulnerabilities: State-
of-the-art defenses and open problems. In Proc. ACM
Asia-Paciﬁc Workshop on Systems (APSys), 2011.

[23] L. Chen and J. Li. Flexible and Scalable Digital Signa-

tures in TPM 2.0. In Proc. ACM CCS, 2013.

[24] Q. A. Chen, Z. Qian, and Z. M. Mao. Peeking into Your
App without Actually Seeing It: UI State Inference and
Novel Android Attacks. In Proc. USENIX Security,
2014.

[25] X. Chen, T. Garﬁnkel, E. C. Lewis, P. Subrahmanyam,
C. A. Waldspurger, D. Boneh, J. Dwoskin, and D. R. K.
Ports. Overshadow: a Virtualization-Based Approach
to Retroﬁtting Protection in Commodity Operating
Systems. In Proc. ACM ASPLOS, 2008.

14

[26] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler.
An Empirical Study of Operating Systems Errors. In
Proc. ACM SOSP, 2001.

[27] V. Costan, I. Lebedev, and S. Devadas. Sanctum: Min-
imal Hardware Extensions for Strong Software Isola-
tion. In Proc. USENIX Security Symposium, 2016.

[28] National Vulnerability Database. CVE-2021-0200:
Out-of-bounds write in the ﬁrmware for Intel(R) Eth-
ernet 700 Series Controllers before version 8.2 may
allow a privileged user to potentially enable an escala-
tion of privilege via local access. https://nvd.nist.
gov/vuln/detail/CVE-2021-0200.

[29] National Vulnerability Database. Vulnerability sum-

mary for cve-2015-6639.

[30] K. Elphinstone and G. Heiser. From L3 to seL4 What
Have We Learnt in 20 Years of L4 Microkernels? In
Proc. ACM SOSP, 2013.

[31] D. R. Engler, M. F. Kaashoek, and J. O’Toole Jr.
Exokernel: an Operating System Architecture for
In Proc.
Application-Level Resource Management.
ACM SOSP, 1995.

[32] M. Fähndrich, M. Aiken, C. Hawblitzel, O. Hodson,
G. Hunt, J. R. Larus, and S. Levi. Language Support
for Fast and Reliable Message-based Communication
in Singularity OS. In Proc. ACM EuroSys, 2006.

[33] A. Ferraiuolo, A. Baumann, C. Hawblitzel, and
B. Parno. Komodo: Using veriﬁcation to disentan-
gle secure-enclave hardware from software. In Proc.
ACM SOSP, 2017.

[34] K. Franz. Add a microSD Slot with the Pmod
https://digilent.com/blog/
MicroSD.
add-a-microsd-slot-with-the-pmod-microsd/,
2021.

[35] A. Frumusanu. Huawei Announces Mate 40 Se-
ries: Powered by 15.3bn Transistors 5nm Kirin
9000. https://www.anandtech.com/show/16156/
huawei-announces-mate-40-series, 2020.

[36] W. Futral and J. Greene. Intel Trusted Execution Tech-
nology for Server Platforms: A Guide to More Secure
Datacenters. Apress Media LLC, Springer Nature,
2013.

[37] Q. Ge, Y. Yarom, T. Chothia, and G. Heiser. Time
In Proc.

Protection: The Missing OS Abstraction.
ACM EuroSys, 2019.

[38] A. Gefﬂaut, T. Jaeger, Y. Park, J. Liedtke, K. J. Elphin-
stone, V. Uhlig, J. E. Tidswell, L. Deller, and L. Reuther.

The SawMill Multiserver Approach. In Proc. ACM
SIGOPS European workshop: beyond the PC: new
challenges for the operating system, 2000.

[51] B. Kauer. OSLO: Improving the Security of Trusted
Computing. In Proc. USENIX Security Symposium,
2007.

[39] YosysHQ GmbH. SymbiYosys (sby) Documenta-
tion. https://symbiyosys.readthedocs.io/en/
latest/index.html, 2021.

[40] J. Götzfried, M. Eckert, S. Schinzel, and T. Müller.
Cache Attacks on Intel SGX. In Proc. ACM European
Workshop on Systems Security (EuroSec), 2017.

[41] R. Grisenthwaite. Arm CCA will put conﬁden-
tial compute in the hands of every developer.
https://www.arm.com/company/news/2021/06/
arm-cca-will-put-confidential-compute-in-
the-hands-of-every-developer, 2021.

[42] D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Jufﬁn-
ger, S. O’Connell, W. Schoechl, and Y. Yarom. An-
other Flip in the Wall of Rowhammer Defenses. In
Proc. IEEE Symposium on Security and Privacy (S&P),
2018.

[43] R. Gu, J. Koenig, T. Ramananandro, Z. Shao, X. Wu,
S. Weng, H. Zhang, and Y. Guo. Deep Speciﬁcations
and Certiﬁed Abstraction Layers. In Proc. ACM POPL,
2015.

[44] R. Gu, Z. Shao, H. Chen, X. N. Wu, J. Kim, V. Sjöberg,
and D. Costanzo. CertiKOS: An Extensible Architec-
ture for Building Certiﬁed Concurrent OS Kernels. In
Proc. USENIX OSDI, 2016.

[45] S. Han, W. Shin, J. Park, and H. Kim. A Bad Dream:
Subverting Trusted Platform Module While You Are
Sleeping. In Proc. USENIX Security Symposium, 2018.

[46] F. Hetzelt and R. Buhren. Security Analysis of En-
crypted Virtual Machines. In Proc. ACM VEE, 2017.

[47] O. S. Hofmann, S. Kim, A. M. Dunn, M. Z. Lee, and
E. Witchel. InkTag: Secure Applications on an Un-
In Proc. ACM ASPLOS,
trusted Operating System.
2013.

[48] G. C. Hunt and J. R. Larus. Singularity: Rethinking
the Software Stack. ACM SIGOPS Operating Systems
Review, 2007.

[49] IBM. Software TPM Introduction. http://ibmswtpm.

sourceforge.net/ibmswtpm2.html, 2021.

[50] M. F. Kaashoek, D. R. Engler, G. R. Ganger, H. M.
Briceno, R. Hunt, D. Mazieres, T. Pinckney, R. Grimm,
J. Jannotti, and K. Mackenzie. Application Perfor-
mance and Flexibility on Exokernel Systems. In Proc.
ACM SOSP, 1997.

15

[52] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H. Lee, D. Lee,
C. Wilkerson, K. Lai, and O. Mutlu. Flipping Bits in
Memory Without Accessing Them: An Experimental
Study of DRAM Disturbance Errors. In Proc. ACM
ISCA, 2014.

[53] D. Kleidermacher, J. Seed, B. Barbello, S. Somo-
gyi, and Pixel & Tensor security teams Android.
Pixel 6: Setting a new standard for mobile security.
https://security.googleblog.com/2021/10/
pixel-6-setting-new-standard-for-mobile.
html, 2021.

[54] G. Klein, K. Elphinstone, G. Heiser, J. Andronick,
D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt,
R. Kolanski, M. Norrish, T. Sewell, H. Tuch, and
S. Winwood. seL4: Formal Veriﬁcation of an OS Ker-
nel. In Proc. ACM SOSP, 2009.

[55] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss,
W. Haas, M. Hamburg, M. Lipp, S. Mangard,
T. Prescher, M. Schwarz, and Y. Yarom. Spectre At-
tacks: Exploiting Speculative Execution. In Proc. IEEE
Symposium on Security and Privacy (S&P), 2019.

[56] D. Lee, D. Kohlbrenner, S. Shinde, K. Asanovi´c, and
D. Song. Keystone: An Open Framework for Architect-
ing Trusted Execution Environments. In Proc. ACM
EuroSys, 2020.

[57] A. Levy, B. Campbell, B. Ghena, D. B. Gifﬁn, P. Pan-
nuto, P. Dutta, and P. Levis. Multiprogramming a 64
kB Computer Safely and Efﬁciently. In Proc. ACM
SOSP, 2017.

[58] M. Li, Y. Zhang, Z. Lin, and Y. Solihin. Exploiting Un-
protected I/O Operations in AMD’s Secure Encrypted
Virtualization. In Proc. USENIX Security Symposium,
2019.

[59] S. Li, X. Li, R. Gu, J. Nieh, and J. Z. Hui. A Secure
and Formally Veriﬁed Linux KVM Hypervisor. 2021.

[60] S. Li, X. Li, R. Gu, J. Nieh, and J. Z. Hui. Formally
Veriﬁed Memory Protection for a Commodity Mul-
In Proc. USENIX Security
tiprocessor Hypervisor.
Symposium, 2021.

[61] J. Liedtke. Improving IPC by Kernel Design. ACM

SIGOPS Operating Systems Review, 1993.

[62] M. Lipp, D. Gruss, R. Spreitzer, C. Maurice, and
S. Mangard. ARMageddon: Cache Attacks on Mo-
bile Devices. In Proc. USENIX Security Symposium,
2016.

[63] M. Lipp, M. Schwarz, D. Gruss, T. Prescher, W. Haas,
A. Fogh, J. Horn, S. Mangard, P. Kocher, D. Genkin,
Y. Yarom, and M. Hamburg. Meltdown: Reading Ker-
nel Memory from User Space. In Proc. USENIX Secu-
rity Symposium, 2018.

[64] K. Loughlin, S. Saroiu, A. Wolman, and B. Kasikci.
Stop! Hammer Time: Rethinking Our Approach to
Rowhammer Mitigations. In Proc. ACM HotOS, 2021.

[65] J. M. McCune, B. J. Parno, A. Perrig, M. K. Reiter,
and H. Isozaki. Flicker: An Execution Infrastructure
for TCB Minimization. In Proc. ACM EuroSys, 2008.

[66] A. Moghimi, G.

Irazoqui, and T. Eisenbarth.
Cachezoom: How SGX Ampliﬁes the Power of Cache
Attacks. In Proc. Springer International Conference
on Cryptographic Hardware and Embedded Systems
(CHES), 2017.

[67] D. Moghimi, B. Sunar, T. Eisenbarth, and N. Heninger.
TPM-FAIL: TPM meets Timing and Lattice Attacks.
In Proc. USENIX Security Symposium, 2020.

[75] R. Paccagnella, L. Luo, and C. W. Fletcher. Lord of the
Ring(s): Side Channel Attacks on the CPU On-Chip
In Proc. USENIX
Ring Interconnect Are Practical.
Security Symposium, 2021.

[76] N. Palix, G. Thomas, S. Saha, C. Calvès, J. Lawall, and
G. Muller. Faults in Linux: Ten Years Later. In Proc.
ACM ASPLOS, 2011.

[77] A. Phaneuf.

State of mobile banking in
top apps, features, statistics and market
https://www.businessinsider.com/

2020:
trends.
mobile-banking-market-trends, 2019.

[78] D. E. Porter, S. Boyd-Wickizer, J. Howell, R. Olinsky,
and G. C. Hunt. Rethinking the Library OS from the
Top Down. In Proc. ACM ASPLOS, 2011.

[79] M. Posner.

take to ﬁll an FPGA?

How many ASIC Gates does
https://blogs.
it
synopsys.com/breakingthethreelaws/2015/02/
how-many-asic-gates-does-it-take-to-fill-
an-fpga/, 2015.

[68] D. Moghimi, J. Van Bulck, N. Heninger, F. Piessens,
and B. Sunar. COPYCAT: Controlled Instruction-Level
Attacks on Enclaves. In Proc. USENIX Security Sym-
posium, 2020.

[80] P. Qiu, D. Wang, Y. Lyu, and G. Qu. VoltJockey:
Breaching TrustZone by Software-Controlled Voltage
Manipulation over Multi-core Frequencies. In Proc.
ACM CCS, 2019.

[69] K. Murdock, D. Oswald, F. D. Garcia, J. Van Bulck,
D. Gruss, and F. Piessens. Plundervolt: Software-
based Fault Injection Attacks against Intel SGX. In
Proc. IEEE Symposium on Security and Privacy (S&P),
2020.

[70] S. Naffziger, N. Beck, T. Burd, K. Lepak, G. Loh,
M. Subramony, and S. White. Pioneering Chiplet
Technology and Design for the AMD EPYC and
Ryzen Processor Families: Industrial Product. In Proc.
ACM/IEEE ISCA, 2021.

[71] R. Nandakumar, S. Gollakota, and N. Watson. Contact-
less Sleep Apnea Detection on Smartphones. In Proc.
ACM MobiSys, 2015.

[72] V. Narayanan, T. Huang, D. Detweiler, D. Appel, Z. Li,
G. Zellweger, and A. Burtsev. RedLeaf: Isolation and
Communication in a Safe Operating System. In Proc.
USENIX OSDI, 2020.

[73] L. Nelson, H. Sigurbjarnarson, K. Zhang, D. Johnson,
J. Bornholt, E. Torlak, and X. Wang. Hyperkernel:
Push-Button Veriﬁcation of an OS Kernel. In Proc.
ACM SOSP, 2017.

[74] J. Nider and A. Fedorova. The Last CPU. In Proc.

ACM HotOS, 2021.

16

BREAKING

[81] Quarklab.
ARM
TRUSTZONE.
i.blackhat.com/USA-19/Thursday/
us-19-Peterlin-Breaking-Samsungs-ARM-
TrustZone.pdf, 2019.

SAMSUNG’S
https://

[82] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuf-
frida, and H. Bos. Flip Feng Shui: Hammering a Nee-
dle in the Software Stack. In Proc. USENIX Security
Symposium, 2016.

[83] M. Schwarz, S. Weiser, D. Gruss, C. Maurice, and
S. Mangard. Malware Guard Extension: Using SGX
to Conceal Cache Attacks. In Proc. Springer Interna-
tional Conference on Detection of Intrusions and Mal-
ware, and Vulnerability Assessment (DIMVA), 2017.

[84] S. Shankland. Apple’s A15 Bionic chip powers
iPhone 13 with 15 billion transistors, new graph-
ics and AI. https://www.cnet.com/tech/mobile/
apples-a15-bionic-chip-powers-iphone-13-
with-15-billion-transistors-new-graphics-
and-ai/, 2021.

[85] J. Shao, Y. Qin, D. Feng, and W. Wang. Formal Anal-
ysis of Enhanced Authorization in the TPM 2.0. In
Proc. ACM Symposium on Information, Computer and
Communications Security (ASIA CCS), 2015.

[86] Y. Shizuku, T. Hirose, N. Kuroki, M. Numa, and
M. Okada. A 24-transistor static ﬂip-ﬂop consisting
of nors and inverters for low-power digital vlsis. In
Proc. IEEE International New Circuits and Systems
Conference (NEWCAS), 2014.

[98] Yingchen Wang, Riccardo Paccagnella, Elizabeth Tang
He, Hovav Shacham, Christopher W. Fletcher, and
David Kohlbrenner. Hertzbleed: Turning Power Side-
Channel Attacks Into Remote Timing Attacks on x86.
In Proc. USENIX Security Symposium, 2022.

[87] H. Sigurbjarnarson, L. Nelson, B. Castro-Karney,
J. Bornholt, E. Torlak, and X. Wang. Nickel: A frame-
work for Design and Veriﬁcation of Information Flow
Control Systems. In Proc. USENIX OSDI, 2018.

[99] D. Weber, A. Ibrahim, H. Nemati, M. Schwarz, and
C. Rossow. Osiris: Automated Discovery of Microar-
chitectural Side Channels. In Proc. USENIX Security
Symposium, 2021.

[88] E. R. Sparks. A Security Assessment of Trusted Plat-
form Modules. Dartmouth College Undergraduate
Theses. 53, 2007.

[89] V. Strumpen. Introduction to Digital Circuits: Basic
http://bibl.ica.jku.at/dc/

Digital Circuits.
build/html/basiccircuits/basiccircuits.
html, 2015.

[90] A. Tang, S.

Sethumadhavan, and S.

Stolfo.
CLKSCREW: Exposing the Perils of Security-
In Proc. USENIX
Oblivious Energy Management.
Security Symposium, 2017.

[91] R. Tao, J. Yao, X. Li, S. Li, J. Nieh, and R. Gu. Formal
Veriﬁcation of a Multiprocessor Hypervisor on Arm
In Proc. ACM SOSP,
Relaxed Memory Hardware.
2021.

[92] Trusted Computing Group (TCG).

TPM 2.0 Li-
https://trustedcomputinggroup.org/

brary.
resource/tpm-library-specification/, 2019.

[93] DiabetesMine Team. NEWS: OmniPod Tubeless
Insulin Pump to Offer Smartphone Control Soon.
https://www.healthline.com/diabetesmine/
omnipod-smartphone-control-diabetes, 2019.

[94] J. Van Bulck, M. Minkin, O. Weisse, D. Genkin, Baris
Kasikci, F. Piessens, M. Silberstein, T. F. Wenisch,
Y. Yarom, and R. Strackx. FORESHADOW: Extract-
ing the Keys to the Intel SGX Kingdom with Transient
Out-of-Order Execution. In Proc. USENIX Security
Symposium, 2018.

[95] V. van der Veen, Y. Fratantonio, M. Lindorfer, D. Gruss,
C. Maurice, G. Vigna, H. Bos, K. Razavi, and C. Giuf-
frida. Drammer: Deterministic Rowhammer Attacks
on Mobile Platforms. In Proc. ACM CCS, 2016.

[96] J. Vander Stoep. Android: Protecting the Kernel. In

Linux Security Summit (LSS), 2016.

[97] A. Vasudevan, S. Chaki, P. Maniatis, L. Jia, and
A. Datta. ÜBERSPARK: Enforcing Veriﬁable Object
Abstractions for Automated Compositional Security
Analysis of a Hypervisor. In Proc. USENIX Security
Symposium, 2016.

17

[100] S. Weiser and M. Werner. SGXIO: Generic Trusted I/O
Path for Intel SGX. In Proc. ACM CODASPY, 2017.

[101] S. Wesemeyer, C. J.P. Newton, H. Treharne, L. Chen,
R. Sasse, and J. Whiteﬁeld. Formal Analysis and Im-
plementation of a TPM 2.0-based Direct Anonymous
Attestation Scheme. In Proc. ACM Asia Conference on
Computer and Communications Security (ASIA CCS),
2020.

[102] L. Wilke, J. Wichelmann, M. Morbitzer, and T. Eisen-
barth. SEVurity: No Security Without Integrity: Break-
ing Integrity-Free Memory Encryption with Minimal
Assumptions. In Proc. IEEE Symposium on Security
and Privacy (S&P), 2020.

[103] Y. Xiao, M. Li, S. Chen, and Y. Zhang. STACCO: Differ-
entially Analyzing Side-Channel Traces for Detecting
SSL/TLS Vulnerabilities in Secure Enclaves. In Proc.
ACM CCS, 2017.

[104] Y. Xiao, X. Zhang, Y. Zhang, and R. Teodorescu. One
Bit Flips, One Cloud Flops: Cross-VM Row Hammer
Attacks and Privilege Escalation. In Proc. USENIX
Security Symposium, 2016.

[105] Xilinx. Xilinx Standalone Library Documentation. OS
and Libraries Document Collection. UG643 (v2021.1)
June 16, 2021.

[106] Xilinx. Zynq UltraScale + Device. Technical Refer-
ence Manual. UG1085 (v2.2) December 4, 2020.

[107] Y. Yan, Z. Li, Q. A. Chen, C. Wilson, T. Xu, E. Zhai,
Y. Li, and Y. Liu. Understanding and Detecting
Overlay-based Android Malware at Market Scales. In
Proc. ACM MobiSys, 2019.

[108] H. Zhang, D. She, and Z. Qian. Android Root and
its Providers: A double-Edged Sword. In Proc. ACM
CCS, 2015.

[109] N. Zhang, K. Sun, D. Shands, W. Lou, and Y. T. Hou.
TruSpy: Cache Side-Channel Information Leakage
from the Secure World on ARM Devices. IACR Cryp-
tology ePrint Archive, 2016:980, 2016.

