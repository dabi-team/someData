CodeAttack: Code-based Adversarial Attacks for Pre-Trained
Programming Language Models

Akshita Jha
Virginia Tech, Arlington, VA
akshitajha@vt.edu

Chandan K. Reddy
Virginia Tech, Arlington, VA
reddy@cs.vt.edu

2
2
0
2

y
a
M
1
3

]
L
C
.
s
c
[

1
v
2
5
0
0
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Pre-trained programming language (PL) mod-
els (such as CodeT5, CodeBERT, GraphCode-
BERT, etc.,) have the potential to automate
software engineering tasks involving code un-
derstanding and code generation. However,
these models are not robust to changes in
the input and thus, are potentially suscep-
tible to adversarial attacks. We propose,
CodeAttack, a simple yet effective black-
box attack model that uses code structure to
generate imperceptible, effective, and mini-
mally perturbed adversarial code samples. We
demonstrate the vulnerabilities of the state-
of-the-art PL models to code-speciﬁc adver-
sarial attacks. We evaluate the transferabil-
ity of CodeAttack on several code-code
(translation and repair) and code-NL (summa-
rization) tasks across different programming
languages. CodeAttack outperforms state-
of-the-art adversarial NLP attack models to
achieve the best overall performance while be-
ing more efﬁcient and imperceptible.

1

Introduction

There has been a recent surge in the development
of general purpose programming language (PL)
models (such as CodeT5 (Wang et al., 2021), Code-
BERT (Feng et al., 2020), GraphCodeBERT (Guo
et al., 2020), and PLBART (Ahmad et al., 2021))
which can capture the relationship between natural
language and programming language, and poten-
tially automate software engineering development
tasks involving code understanding (clone detec-
tion, defect detection) and code generation (code-
code translation, code-code reﬁnement, code-NL
summarization). However, given the data driven
pre-training of these PL models on massive code
data, their robustness and vulnerabilities need care-
ful investigation. In this work, we demonstrate the
vulnerability of the state-of-the-art programming
language models by generating adversarial samples
that leverage code structure.

Figure 1: CodeAttack makes a slight modiﬁcation
to the input code snippet (red) which causes signiﬁcant
changes to the code summary obtained from the SOTA
pre-trained programming language models. Keywords
are highlighted in blue and comments in green.

Adversarial attacks are characterized by imper-
ceptible changes in the input that result in incorrect
predictions from a neural network. For PL models,
they are important for three primary reasons: (i) Ex-
posing system vulnerabilities: As a form of stress
test to understand the model’s limitations. For ex-
ample, adversarial samples can be used to bypass
a defect detection ﬁlter that classiﬁes a given code
as vulnerable or not (Zhou et al., 2019), (ii) Eval-
uating model robustness: Analyze the PL model’s
sensitivity to imperceptible perturbations. For ex-
ample, a small change in the input programming
language (akin to a typo or a spelling mistake in
the NL scenario) might trigger the code summariza-
tion model to generate a gibberish natural language
code summary (Figure 1), and (iii) Model inter-
pretability: Help understand what PL models learn.
For example, adversarial samples can be used to
inspect the tokens pre-trained PL models attend to.
A successful adversarial attack for code should
have the following properties: (i) Minimal and im-
perceptible perturbations: Akin to spelling mis-
takes or synonym replacement in NL that misleads
the neural models, (ii) Code Consistency: Per-
turbed code is consistent with the original input,
and (iii) Code ﬂuency: Follows the syntax of the
original programming language. The current NL
adversarial attack models fall short on all three
fronts. Therefore, we propose CodeAttack1
– a simple yet effective black-box attack model

1Code will be made publicly available

 
 
 
 
 
 
for generating adversarial samples for any input
code snippet, irrespective of the programming lan-
guage. CodeAttack operates in a realistic sce-
nario, where the adversary does not have access to
model parameters but only to the test queries and
the model prediction. CodeAttack uses a pre-
trained masked CodeBERT PL model (Feng et al.,
2020) as the adversarial code generator. We lever-
age the code structure to generate imperceptible
and effective adversarial attacks through minimal
perturbations constrained to follow the syntax of
the original code. Our primary contributions are as
follows:

• To the best of our knowledge, we are the ﬁrst
ones to detect the vulnerability of pre-trained pro-
gramming language models to adversarial attacks
on different code generation tasks. We propose
a simple yet effective realistic black-box attack
method, CodeAttack, that generates adversar-
ial samples for a code snippet irrespective of the
input programming language.

• We design a general purpose black-box attack
method for sequence-to-sequence PL models
that is transferable across different downstream
tasks like code translation, repair, and summa-
rization. This can also be extended to sequence-
to-sequence tasks in other domains.

• We

the

demonstrate

of
effectiveness
CodeAttack over
existing NLP adver-
sarial models through an extensive empirical
CodeAttack outperforms the
evaluation.
NLP baselines when considering both the attack
quality and its efﬁcacy.

2 Related Work

Adversarial Attacks in NLP. Adversarial at-
tacks have been used to analyze the robustness of
NLP models. Black-box adversarial attacks like
BERT-Attack (Li et al., 2020) use BERT, with sub-
word expansion, for attacking vulnerable words.
BAE (Garg and Ramakrishnan, 2020) also uses
BERT for replacement or insertion around vulnera-
ble words. TextFooler (Jin et al., 2020) and PWWS
(Ren et al., 2019) use synonyms and part-of-speech
(POS) tagging to replace important tokens. Deep-
wordbug (Gao et al., 2018) and TextBugger (Li
et al., 2019) use character insertion, deletion, and
replacement, and constrain their attacks using edit
distance and cosine similarity, respectively. Some

use a greedy search and replacement strategy to
generate adversarial examples (Hsieh et al., 2019;
Yang et al., 2020). Genetic Attack (GA) (Alzantot
et al., 2018) uses genetic algorithm for search with
language model perplexity and word embedding
distance as substitution constraints. Some adver-
sarial models assume white-box access and use
model gradients to ﬁnd substitutes for the vulnera-
ble tokens (Ebrahimi et al., 2018; Papernot et al.,
2016; Pruthi et al., 2019). None of these methods
have been designed speciﬁcally for programming
languages, which is more structured than natural
language.

Adversarial Attacks for PL. Yang et al. (2022)
focus on making adversarial examples more natu-
ral by using greedy search and genetic algorithm
for replacement. Zhang et al. (2020) generate ad-
versarial examples by renaming identiﬁers using
a Metropolis-Hastings sampling based technique
(Metropolis et al., 1953). Yefet et al. (2020) use
gradient based exploration for attacks. Some also
propose metamorphic transformations to generate
adversarial examples (Applis et al., 2021; Ramakr-
ishnan et al., 2020). The above models focus on
code understanding tasks like defect detection and
clone detection. Although some works do focus
on generating adversarial examples for code sum-
marization (Ramakrishnan et al., 2020; Zhou et al.,
2021), they do not talk about the transferability
of these tasks to different tasks and different mod-
els. Our model, CodeAttack, assumes black-
box access to the state-of-the-art PL models for
generating adversarial attacks for code generation
tasks like code translation, code repair, and code
summarization using a constrained code-speciﬁc
greedy algorithm to ﬁnd meaningful substitutes for
vulnerable tokens.

3 CodeAttack

We describe the capabilities, knowledge, and the
goal of the proposed CodeAttack model, and
provide details on how it detects vulnerabilities in
the state-of-the-art pre-trained PL models.

3.1 Threat Model

Adversary’s Capabilities. The adversary is ca-
pable of perturbing the test queries given as input
to a pre-trained PL model to generate adversar-
ial samples. We follow the existing literature for
generating natural language adversarial examples
and allow for two types of perturbations for the

input code sequence: (i) token-level perturbations,
and (ii) character-level perturbations. The adver-
sary is allowed to perturb only a certain number
of tokens/characters and must ensure a high simi-
larity between the original code and the perturbed
code. Formally, for a given input sequence X ∈ X,
where X is the input space, a valid adversarial ex-
ample Xadv follows the requirements:

X (cid:54)= Xadv

Xadv ← X + δ;

s.t. ||δ|| < θ

Sim(Xadv, X ) ≥ (cid:15)

(1)

(2)

(3)

where θ is the maximum allowed adversarial per-
turbation; Sim(·) is a similarity function that takes
into account the syntax of the input code and the
adversarial code sequence; and (cid:15) is the similarity
threshold. We describe the perturbation constraints
and the similarity functions in more detail in Sec-
tion 3.2.2.

Adversary’s Knowledge. We assume black-box
access to realistically assess the vulnerabilities and
robustness of existing pre-trained PL models. In
this setting, the adversary does not have access to
the model parameters, model architecture, model
gradients, training data, or the loss function. The
adversary can only query the pre-trained PL model
with input sequences and get their corresponding
output probabilities. This is more practical than a
white-box scenario that assumes access to all the
above, which might not always be the case.

Adversary’s Goal. Given an input code se-
quence as query, the adversary’s goal is to de-
grade the quality of the generated output sequence
through imperceptibly modifying the query. The
generated output sequence can either be a code
snippet (code translation, code repair) or natural
language text (code summarization). Formally,
given a pre-trained PL model F : X → Y , where
X is the input space, and Y is the output space, the
goal of the adversary is to generate an adversarial
sample Xadv for an input sequence X s.t.

F (Xadv) (cid:54)= F (X )

Q(F (X )) − Q(F (Xadv)) ≥ φ

(4)

(5)

where Q(·) measures the quality of the generated
output and φ is the speciﬁed drop in quality. This is
in addition to the constraints applied on Xadv ear-
lier. We formulate our ﬁnal problem of generating
adversarial samples as follows:

∆atk = argmaxδ [Q(F (X )) − Q(F (Xadv))] (6)

In the above optimization equation, Xadv is a min-
imally perturbed adversary subject to constraints
on the perturbations δ (Eqs.1-5). CodeAttack
searches for a perturbation ∆atk to maximize the
difference in the quality Q(·) of the output se-
quence generated from the original input code snip-
pet X and that by the perturbed code snippet Xadv.

3.2 Attack Methodology

CodeAttack’s attack methodology can be bro-
ken down into two primary steps: (i) Finding the
most vulnerable tokens, and (ii) Substituting these
vulnerable tokens (subject to code speciﬁc con-
straints), to generate adversarial samples.

3.2.1 Finding Vulnerable Tokens
Some input tokens contribute more towards the
ﬁnal prediction than the others, and therefore, ‘at-
tacking’ these highly inﬂuential or highly vulner-
able tokens increases the probability of altering
the model predictions more signiﬁcantly as op-
posed to attacking non-vulnerable tokens. Since
under a black-box setting, the model gradients
are unavailable and the adversary only has ac-
cess to the output logits of the pre-trained PL
model. We deﬁne ‘vulnerable tokens’ as tokens
that have a high inﬂuence on the output logits of the
model. Let F be an encoder-decoder pre-trained
PL model. The given input sequence is denoted by
X = [x1, .., xi, ..., xm], where {xi}m
1 are the input
tokens. The output is a sequence of vectors:

O = F (X ) = [o1, ..., on]

yt = argmax(ot)

where {ot}n
1 is the output logit for the correct out-
put token yt for the time step t. Without loss of
generality, we can also assume the output sequence
Y = F (X ) = [yi, ..., yl]. Y can either be a se-
quence of code tokens or natural language tokens.
To ﬁnd the vulnerable input tokens, we re-
place a token xi with [MASK] s.t. X\xi =
[x1, .., xi−1, [MASK], xi+1, .., xm] and get its out-
put logits. The output vectors are now

O\xi = F (X\xi) = [o(cid:48)
t}q

1, ..., o(cid:48)
q]

where {o(cid:48)
1 is the new output logit for the correct
prediction Y. We calculate the inﬂuence score for
the token xi as follows:

Ixi =

n
(cid:88)

t=1

ot −

q
(cid:88)

t=1

o(cid:48)
t

(7)

Token Class Description

Keywords Reserved word
Identiﬁers Variable, Class Name, Method name
Arguments
Integer, Floating point, String, Character
Operators Brackets ({},(),[]), Symbols (+,*,/,-,%,;,.)

stitute tokens to have an extra or a missing operator
(akin to making typos).

|Op(vi)| − 1 ≤ |Op(si)| ≤ |Op(vi)| + 1

(8)

Table 1: Token class and their description.

We rank all the input tokens according to their in-
ﬂuence score Ixi in descending order to ﬁnd most
vulnerable tokens V . We select only the top-k to-
kens to limit the number of perturbations and at-
tack them iteratively either by completely replacing
them or by adding or deleting a character around
them. We explain this in detail below.

3.2.2 Substituting Vulnerable Tokens

We adopt greedy search using a masked program-
ming language model, subject to code speciﬁc con-
straints, to ﬁnd substitutes S for vulnerable tokens
V , s.t. they are minimally perturbed and have the
maximal probability of incorrect prediction.

Search Method.
In a given input sequence, we
mask a vulnerable token vi and use the masked
PL model to predict a meaningful contextualised
token in its place. We use the top-k predictions for
each of the masked vulnerable tokens as our initial
search space. Let M denote a masked PL model.
Given an input sequence X = [x1, .., vi, .., xm],
where vi is a vulnerable token, M uses WordPiece
algorithm (Wu et al., 2016) for tokenization that
breaks uncommon words into sub-words resulting
in H = [h1, h2, .., hq]. We align and mask all the
corresponding sub-words for vi, and combine the
predictions to get the top-k substitutes S(cid:48) = M(H)
for the vulnerable token vi. This initial search
space S(cid:48) consists of l possible substitutes for a
vulnerable token vi. We then ﬁlter out substitute
tokens to ensure minimal perturbation, code consis-
tency, and code ﬂuency of the generated adversarial
samples, subject to the following constraints.

Constraints. Since the tokens generated from a
masked PL model may not be meaningful indi-
vidual code tokens, we further use a CodeNet to-
kenizer (Puri et al., 2021) to break a token into
its corresponsing code tokens. CodeNet tokenizes
the input tokens based on four primary code token
classes as shown in Table 1. If si is the substitute
for the vulnerable token vi as tokenized by M, and
Op(·) denotes the operators present in any given
token using CodeNet tokenizer, we allow the sub-

If C(·) denotes the code token classes (identiﬁers,
keywords, and arguments) of a given token, we
maintain the alignment between between vi and
the potential substitute si as follows.

C(vi) = C(si) and |C(vi)| = |C(si)|

(9)

These constraints maintain the syntactic structure
of Xadv and signiﬁcantly reduce the search space.

Substitutions. We allow two types of substitu-
tions to generate adversarial examples: (i) Token-
level substitution, and (ii) Operator (character)
level substitution where only an operator is added,
replaced, or deleted. We iteratively substitute the
vulnerable tokens with their corresponding substi-
tute tokens/characters, using the reduced search
space S, until the adversary’s goal is met.

We only allow replacing p% of the vulnera-
ble tokens/characters to keep perturbations to a
minimum, where p is a hyper-paramter. We also
maintain the cosine similarity between the input
text X and the adversarially perturbed text Xadv
above a certain threshold (Equation 3). The com-
plete algorithm has been shown in Algorithm 1.
CodeAttack maintains minimal perturbation,
code ﬂuency, and code consistency between the
input and the adversarial code snippet.

4 Experiments

4.1 Downstream Tasks and Datasets

We show the transferability of CodeAttack
across three different downstream tasks and
datasets – all in different programming languages.

Code Translation involves translating one pro-
gramming language to the other. The publicly avail-
able code translation datasets2345 consists of par-
allel functions between Java and C#. There are a
total of 11,800 paired functions, out of which 1000
are used for testing. After tokenization, the average
sequence length for Java functions is 38.51 tokens,
and the average length for C# functions is 46.16.

2http://lucene.apache.org/
3http://poi.apache.org/
4https://github.com/eclipse/jgit/
5https://github.com/antlr/

Algorithm 1 CodeAttack: Generating adversar-
ial examples for Code
Input: Code X ; Victim model F ; Maximum per-
turbation θ; Similarity (cid:15); Performance Drop φ
Output: Adversarial Example Xadv
Initialize: Xadv ← X
// Find vulnerable tokens ‘V’
for xi in M(X ) do

Calculate Ixi acc. to Eq.(7)

end
V ← Rank(xi) based on Ixi
// Find substitutes ‘S’
for vi in V do

S ← Filter(vi) subject to Eqs.(8), (9)
for sj in S do

// Attack the victim model
Xadv = [x1, ..., xi−1, sj, ..., xm]
if Q(F (X )) − Q(F (Xadv)) ≥ φ and
Sim(X , Xadv) ≥ (cid:15) and ||Xadv − X || ≤ θ
then

return Xadv // Success

end

end
// One perturbation
Xadv ← [x1, ...xi−1, sj, ..xm]

end
return

Code Repair
reﬁnes code by automatically ﬁx-
ing bugs. The publicly available code repair dataset
(Tufano et al., 2019) consists of buggy Java func-
tions as source and their corresponding ﬁxed func-
tions as target. We use the small subset of the data
with 46,680 train, 5,835 validation, and 5,835 test
samples (≤ 50 tokens in each function).

Code Summarization involves generating natu-
ral language summary for a given code. We use
the CodeSearchNet dataset (Husain et al., 2019)
which consists of code and their corresponding
summaries in natural language. We show the re-
sults of our model on Python (252K/14K/15K),
Java (165K/5K/11K), and PHP (241K/13K/15K).
The numbers in the bracket denote the approximate
samples in train/development/test set, respectively.

4.2 Victim Models

We pick a representative method from different
categories as our victim models to attack.

• CodeT5 (Wang et al., 2021): A uniﬁed pre-
trained encoder-decoder transformer-based PL
model that leverages code semantics by using

an identiﬁer-aware pre-training objective. This
is the state-of-the-art on several sub-tasks in the
CodeXGlue benchmark (Lu et al., 2021).

• CodeBERT (Feng et al., 2020): A bimodal pre-
trained programming language model that per-
forms code-code and code-nl tasks.

• GraphCodeBert (Guo et al., 2020): Pre-trained
graph programming language model that lever-
ages code structure through data ﬂow graphs.

• RoBERTa (Liu et al., 2019): Pre-trained natu-
ral language model with state-of-art results on
GLUE (Wang et al., 2018), RACE (Lai et al.,
2017), and SQuAD (Rajpurkar et al., 2016)
datasets.

For our experiments, we use the publicly avail-
able ﬁne-tuned checkpoints for CodeT5 and ﬁne-
tune CodeBERT, GraphCodeBERT, and RoBERTa
on the related downstream tasks.

4.3 CodeAttack Conﬁgurations
The proposed CodeAttack model
is imple-
mented in PyTorch. For the purpose of our exper-
iments, we use the publicly available pre-trained
CodeBERT (MLM) masked PL model as the ad-
versarial example generator. We select the top 50
predictions for each vulnerable token as the initial
search space. On an average, we only attack at
2 to 4 vulnerable tokens for all the tasks to keep
the perturbations to a minimum. The cosine sim-
ilarity threshold between the original code snip-
pet and adversarially generated code is 0.5. Since
CodeAttack does not require any training, we
attack the victim models on the test set using a
batch-size of 256. All experiments were conducted
on a 48 GiB RTX 8000 GPU.

4.4 Evaluation Metric

Downstream Performance. We measure the
downstream performance using CodeBLEU (Ren
et al., 2020) and BLEU (Papineni et al., 2002) be-
fore and after the attack. CodeBLEU measures
the quality of the generated code snippet for code
translation and code repair, and BLEU measures
the quality of the generated natural language code
summary. To measure the efﬁcacy of the attack
model, we deﬁne

∆drop = Qbefore − Qafter = Q(Y) − Q(Yadv)

where Q = {CodeBLEU, BLEU}, Y is the output
sequence generated from the original code X , and

Task

Translate
(Code-
Code)

Repair
(Code-
Code)

Victim
Model

CodeT5

CodeBERT

GraphCode-
BERT

CodeT5

CodeBERT

GraphCode-
BERT

CodeT5

Summarize
(Code-NL)

CodeBERT

RoBERTa

Attack
Model

Downstream Performance
∆drop
Before After

Attack Quality
Attack% #Query CodeBLEUq

Overall
(GMean)

TextFooler
BERT-Attack
CodeAttack

TextFooler
BERT-Attack
CodeAttack

Textfooler
BERT-Attack
CodeAttack

Textfooler
BERT-Attack
CodeAttack

Textfooler
BERT-Attack
CodeAttack

Textfooler
BERT-Attack
CodeAttack

TextFooler
BERT-Attack
CodeAttack

Textfooler
BERT-Attack
CodeAttack

TextFooler
BERT-Attack
CodeAttack

73.99

71.16

66.80

61.13

61.33

62.16

20.06

19.76

19.06

68.08
48.59
61.72

60.45
58.80
54.14

46.51
36.54
38.81

57.59
52.70
53.21

53.55
51.95
52.02

54.23
53.33
51.97

14.96
11.96
11.06

14.38
11.30
10.88

14.06
11.34
10.98

5.91
25.40
12.27

10.71
12.36
17.03

20.29
30.26
27.99

3.53
8.42
7.92

7.78
9.38
9.31

7.92
8.83
10.19

5.70
8.70
9.59

5.37
8.35
8.87

4.99
7.71
8.08

28.29
83.12
89.3

49.2
97.1
97.7

38.70
97.33
98

58.84
98.3
99.36

81.61
98.3
99.39

78.92
99.4
99.52

64.6
90.4
82.8

61.1
93.74
88.32

62.6
94.15
87.51

94.95
186.1
36.84

73.91
48.76
26.43

83.17
41.30
20.60

90.50
74.99
30.68

45.89
74.99
25.98

51.07
62.59
24.67

410.15
1006.28
314.87

358.43
695.03
204.46

356.68
701.01
183.22

63.19
51.11
65.91

66.61
59.90
66.89

63.62
57.41
65.39

69.53
55.94
69.03

68.16
55.94
68.05

67.89
56.05
66.16

53.91
51.34
52.67

54.10
50.31
52.95

54.11
50.10
53.03

21.94
47.61
41.64

32.74
41.58
48.09

36.83
55.30
56.39

24.36
35.79
37.87

35.11
37.22
39.78

34.89
36.64
40.63

27.08
34.30
34.71

26.10
34.16
34.62

25.67
33.14
33.47

Table 2: Results for adversarial attack on translation (C#-Java), repair (Java-Java), and summarization (PHP)
tasks. The downstream performance for Code-Code tasks is measured in CodeBLEU; and for Code-NL task in
BLEU. The best result is in boldface; the next best is underlined. Overall CodeAttack outperforms signiﬁcantly
(p < 0.05).

Yadv is the sequence generated from the perturbed
code Xadv.

Attack Quality. We automatically measure the
attack quality using the following.

adversarial code. Since we want the ∆drop to be
as high as possible while maintaining the attack
% and CodeBLEUq, we measure the geometric
mean (GMean) between ∆drop, attack%, and the
CodeBLEUq to measure the overall performance.

• Attack %: Computes the % of successful attacks
as measured by the ∆drop. Higher the value,
more successful the attack.

• # Query: Under a black-box setting, the adver-
sary can query the victim model to check for
changes in the output logits. Lower the average
number of queries required per sample, more
efﬁcient the adversary.

• # Perturbation: The number of tokens per-
turbed on average to generate an adversarial code.
Lower the value, more imperceptible the attack.

To measure the quality of the perturbed code, we
calculate CodeBLEUq = CodeBLEU(X , Xadv).
Higher the CodeBLEUq, better the quality of the

4.5 Results

The results for attacking pre-trained PL models
for (i) Code Translation, (ii) Code Repair, and (iii)
Code Summarization are shown in Table 2. Due
to lack of space, we only show results for the C#
to Java translation task and for the PHP code sum-
marization task (refer to Appendix A for results
on Java-C# translation and code summarization
results on Python and Java). We use the metrics de-
scribed in Section 4.4 and compare our model with
two state-of-the-art adversarial NLP baselines: (i)
TextFooler (Jin et al., 2020), and (ii) BERT-Attack
(Li et al., 2020).

Downstream Performance Drop. The average
∆drop using CodeAttack is at least 20% for code

Original Code
public string GetFullMessage

TextFooler
citizenship string

() {

...
if (msgB < 0){return string

.Empty;}

...
return RawParseUtils.Decode
(enc, raw, msgB, raw.
Length);

GetFullMessage() {

...
if (msgB < 0){return string

.Empty;}

...
return RawParseUtils.Decode
(enc, raw, msgB, raw.
Length);

BERT-Attack
loop string GetFullMessage()

{

CodeAttack

public string GetFullMessage

() {

...
if (msgB < 0){return string

...
if (msgB = 0){return string

.Empty;}

...
return [UNK][UNK].[UNK](x)
raw, msgB, raw.Length
);

.Empty;}

...
return RawParseUtils.Decode
(enc, raw, msgB, raw.
Length);

CodeBLEUbefore: 77.09

∆drop: 18.84;

CodeBLEUq : 95.11

∆drop: 15.09;

CodeBLEUq : 57.46

∆drop: 21.04;

CodeBLEUq : 88.65

public override void

audiences revoked canceling

public override void

public override void

WriteByte(byte b) {
if (outerInstance.upto ==
outerInstance.
blockSize) {

... }

WriteByte(byte b) {
if (outerInstance.upto ==
outerInstance.
blockSize) {

.... }

[UNK][UNK]() b) {
if (outerInstance.upto ==
outerInstance.
blockSize) {

... }

WriteByte((bytes b) {
if (outerInstance.upto ==
outerInstance.
blockSize) {
... }

}

}

}

}

}

}

}

}

CodeBLEUbefore:100

∆drop:5.74;

CodeBLEUq : 63.28

∆drop:27.26;

CodeBLEUq :49.87

∆drop:20.04;

CodeBLEUq : 91.69

Table 3: Qualitative examples of perturbed codes using TextFooler, BERT-Attack, and CodeAttack on Code
Translation task.

(a) CodeBLEUafter

(b) CodeBLEUq

(c) Average #Query

(d) Attack%

Figure 2: Effectiveness of the attack models on CodeT5 for the code translation task (C#-Java).

translation task and 10% for both code repair task
and code summarization tasks for all three pre-
trained models. ∆drop is higher for BERT-Attack
for translation and repair tasks but its attack quality
(described later) is the lowest. CodeAttack has
the best ∆drop for summarization.

Attack Quality. We observe that CodeAttack
has the highest attack success % for code trans-
lation and the code repair tasks; and the second
best success rate for the code summarization task.
CodeAttack is more efﬁcient as it has the low-
est average query number per sample. This shows
that it successfully attacks more samples with less
querying. Table 3 presents some qualitative ex-
amples of the generated adversarial code snippets
from different attack models. TextFooler has the
best CodeBLEUq (as seen in Table 2) but it re-
places keywords with closely related natural lan-
guage words (‘public’: ‘citizenship’/‘audiences’;
‘override’: ‘revoked’, ‘void’: ‘cancelling’). BERT-
Attack has the lowest CodeBLEUq and substitutes
tokens with either a special ‘[UNK]’ token or with
other seemingly random words. This is expected
since both TextFooler and BERT-Attack have not
been designed for programming languages. On
the other hand, although CodeAttack has the
second best CodeBLEUq, it generates more mean-
ingful adversarial samples by replacing variables

and operators which are imperceptible.

Effectiveness. To study the effectiveness of
CodeAttack, we limit the # perturbations. (Fig-
ure 2). From Figure 2a, we observe that as the
perturbation % increases, the CodeBLEUafter for
CodeAttack decreases but remains constant for
TextFooler and slightly increases for BERT-Attack.
We also observe that although CodeBLEUq for
CodeAttack is the second best (Figure 2b), it
has the highest attack success rate (Figure 2d)
and the lowest number of required queries (Fig-
ure 2c) throughout. This shows the efﬁciency of
CodeAttack and the need for code speciﬁc ad-
versarial attacks.

Overall Performance. Overall, CodeAttack
has the best performance when we consider the
geometric mean (GMean) between ∆drop, attack
%, and CodeBLEUq together. These results are
generalizable across different input programming
languages and different downstream tasks (C# in
case of code translation; Java in case of code repair,
PHP in case of code summarization).

4.6 Ablation Study

We conduct an ablation study to evaluate the im-
portance of selecting vulnerable tokens (V) and
applying constraints (C) to maintain the syntax of

(a) Performance Drop

(b) CodeBLEUq

(c) # Query

(d) Average Success Rate

Figure 3: Ablation Study for Code Translation (C#-Java): Performance of CodeAttack with (+) and without (-)
the vulnerable tokens (V) and the two constraints (C): (i) Operator level (C1), and (ii) Token level (C2).

the perturbed code. Figure 3 shows the results for
the ablation study on the code translation task from
C#-Java. See Appendix A for qualitative examples.

Importance of Vulnerable Tokens. We deﬁne
a variant, CodeAttack+V-C, which ﬁnds vul-
nerable tokens based on logit information (Sec-
tion 3.2.1) and subsitutes them, albeit with-
out any constraints. We create another variant,
CodeAttack-V-C, which randomly samples to-
kens from the input code to attack. As can be seen
from Figure 3a, the latter attack is not as effec-
tive as the ∆drop is less than the former for the
same CodeBLEUq (Figure 3b) and the attack%
(Figure 3d).

Importance of Constraints. We substitute the
vulnerable tokens using the predictions from a
masked PL model with (+C) and without (-C)
any code speciﬁc constraints. We apply two
(i) Operator level con-
types of constraints:
straint (CodeAttack+V+C1), and (ii) Token
level constraint (CodeAttack +V+C1+C2) (Sec-
tion 3.2.2). Only applying the ﬁrst constraint re-
sults in lower attack success % (Figure 3d) and
∆drop (Figure 3a) but a much higher CodeBLEUq.
On applying both the constraints together, the
∆drop and the attack success % improve. Overall,
the ﬁnal model, CodeAttack+V+C1+C2, has
the best tradeoff between the ∆drop, attack success
%, CodeBLEUq, and #Queries required.

Human Evaluation. We sample 50 original and
perturbed Java and C# code samples and shufﬂe
them to create a mix. We ask 3 human annotators,
familiar with the two programming languages, to
classify the code as either original or adversarial.
We also ask them to rate the syntactic correctness of
the codes on a scale of 1 to 5; where 1 is completely
incorrect syntax; and 5 is the perfect syntax. On
an average, 72.10% of the codes were classiﬁed
as original and the average syntactic correctness

was 4.14 for the adversarial code. Additionally, we
provided the annotators with pairs of original and
adversarial codes and asked them to rate the ’visual’
similarity between them using 0 to 1; where 0 is
not similar at all, 0.5 is somewhat similar, and 1 is
very similar. On average, the similarity was 0.71.

5 Discussions and Limitations

We observe that it is easier to attack the code trans-
lation task than the code repair or code summa-
rization tasks. Since code repair aims to ﬁx bugs
in the given code snippet, attacking it is more
challenging. For code summarization, the BLEU
score drops by almost 50%. For all three tasks,
CodeT5 is the most robust whereas GraphCode-
BERT is the most susceptible to attacks using
CodeAttack. CodeT5 has been pre-trained on
the task of Masked Identiﬁer Prediction or deobs-
fuction (Lachaux et al., 2021) where changing the
identiﬁer names does not have an impact on the
code semantics. This helps the model avoid the at-
tacks which involve changing the identiﬁer names,
and in turn makes it more robust. GraphCodeBERT,
on the other hand, uses data ﬂow graphs in their pre-
training which relies on the predicting the relation-
ship between the identiﬁers. Since CodeAttack
modiﬁes the identiﬁers and perturbs the relation-
ship between them, it proves extremely effective
on GraphCodeBERT. This results in a more signiﬁ-
cant ∆drop on GraphCodeBERT compared to other
models for the code translation task.

CodeAttack, although effective has a few lim-
itations. These adversarial attacks can be avoided if
the pre-trained models choose to compile the input
code before processing. The PL models can also
be made more robust by either additionally pre-
training or ﬁne-tuning them using the generated
adversarial examples. Incorporating more tasks
such as code obfuscation in the pre-training stage
might also help with the robustness of the models.

6 Conclusion

References

We introduce a black-box adversarial attack model,
CodeAttack,
to detect vulnerabilities of the
state-of-the-art programming language models.
CodeAttack ﬁnds the most vulnerable tokens
in the given code snippet and uses a greedy search
mechanism to identify contextualised substitutes
subject to code-speciﬁc constraints. Our model in-
corporates the syntactic information of the input
code to generate adversarial examples that are ef-
fective, imperceptible, maintain code ﬂuency, and
consistency. We perform an extensive empirical
and human evaluation to demonstrate the transfer-
ability of CodeAttack on several code-code and
code-NL tasks across different programming lan-
guages. CodeAttack outperforms the existing
state-of-the-art adversarial NLP models when both
the performance drop and the attack quality are
taken together. CodeAttack uses fewer queries
and is more efﬁcient, highlighting the need for
code-speciﬁc adversarial attacks.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2021. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655–2668.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2890–2896.

Leonhard Applis, Annibale Panichella, and Arie van
Deursen. 2021. Assessing robustness of ml-based
program analysis tools using metamorphic program
transformations. In 2021 36th IEEE/ACM Interna-
tional Conference on Automated Software Engineer-
ing (ASE), pages 1377–1381. IEEE.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. Hotﬂip: White-box adversarial exam-
In Proceedings of the
ples for text classiﬁcation.
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
31–36.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1536–1547.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-
jun Qi. 2018. Black-box generation of adversarial
text sequences to evade deep learning classiﬁers. In
2018 IEEE Security and Privacy Workshops (SPW),
pages 50–56. IEEE.

Siddhant Garg and Goutham Ramakrishnan. 2020.
Bae: Bert-based adversarial examples for text classi-
ﬁcation. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 6174–6181.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu
Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey
Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcode-
bert: Pre-training code representations with data
ﬂow. In International Conference on Learning Rep-
resentations.

Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei
Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On
the robustness of self-attentive models. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 1520–1529.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. arXiv preprint arXiv:1909.09436.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2020. Is bert really robust? a strong base-
line for natural language attack on text classiﬁcation
In Proceedings of the AAAI con-
and entailment.
ference on artiﬁcial intelligence, volume 34, pages
8018–8025.

Marie-Anne Lachaux, Baptiste Roziere, Marc
Szafraniec, and Guillaume Lample. 2021. Dobf: A
deobfuscation pre-training objective for program-
ming languages. Advances in Neural Information
Processing Systems, 34.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 785–
794.

J Li, S Ji, T Du, B Li, and T Wang. 2019. Textbugger:
Generating adversarial text against real-world appli-
In 26th Annual Network and Distributed
cations.
System Security Symposium.

Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,
and Xipeng Qiu. 2020. Bert-attack: Adversarial at-
tack against bert using bert. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6193–6202.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Nicholas Metropolis, Arianna W Rosenbluth, Mar-
shall N Rosenbluth, Augusta H Teller, and Edward
Teller. 1953.
Equation of state calculations by
fast computing machines. The journal of chemical
physics, 21(6):1087–1092.

Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian
Goodfellow, Reuben Feinman, Alexey Kurakin, Ci-
hang Xie, Yash Sharma, Tom Brown, Aurko Roy,
et al. 2016. Technical report on the cleverhans
v2. 1.0 adversarial examples library. arXiv preprint
arXiv:1610.00768.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,

Pennsylvania, USA. Association for Computational
Linguistics.

Danish Pruthi, Bhuwan Dhingra, and Zachary C Lip-
ton. 2019. Combating adversarial misspellings with
In Proceedings of the
robust word recognition.
57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 5582–5591.

Ruchir Puri, David S Kung, Geert Janssen, Wei
Zhang, Giacomo Domeniconi, Vladmir Zolotov, Ju-
lian Dolby, Jie Chen, Mihir Choudhury, Lindsey
Decker, et al. 2021. Project codenet: a large-scale
ai for code dataset for learning a diversity of coding
tasks.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.

Goutham Ramakrishnan, Jordan Henkel, Zi Wang,
Aws Albarghouthi, Somesh Jha, and Thomas Reps.
2020. Semantic robustness of models of source code.
arXiv preprint arXiv:2002.03043.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial ex-
amples through probability weighted word saliency.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
1085–1097, Florence, Italy. Association for Compu-
tational Linguistics.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie
Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Am-
brosio Blanco, and Shuai Ma. 2020. Codebleu: a
method for automatic evaluation of code synthesis.
arXiv preprint arXiv:2009.10297.

Michele Tufano, Cody Watson, Gabriele Bavota, Mas-
similiano Di Penta, Martin White, and Denys Poshy-
vanyk. 2019. An empirical study on learning bug-
ﬁxing patches in the wild via neural machine trans-
lation. ACM Transactions on Software Engineering
and Methodology (TOSEM), 28(4):1–29.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. Glue:
A multi-task benchmark and analysis platform for
In Proceedings
natural language understanding.
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 353–355.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH
Hoi. 2021. Codet5:
Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code under-
standing and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 8696–8708.

A Appendix

A.1 Results

Downstream Performance and Attack Quality
We measure the BLEU, ∆BLEU , EM ∆EM , Code-
BLEU, and ∆CodeBLEU to measure the down-
stream performance for code-code tasks (code re-
pair and code translation). The programming lan-
guages used are C#-Java and Java-C# for transla-
tion tasks; and Java for code repair tasks (Table 4
and Table 5). We measure code-NL task for code
summarization in BLEU and ∆BLEU . We show the
results for three programming languages: Python,
Java, and PHP (Table 6). We measure the quality of
the attacks using the metric deﬁned in 4.4 and addi-
tionally show BLEUq which measures the BLEU
score between the original and the perturbed code.
The results follow a similar pattern as that seen in
Section 4.5.

Ablation Study: Qualitative Analysis Table 7
shows the adversarial examples generated using the
variants described in Section 4.6.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
arXiv preprint
man and machine translation.
arXiv:1609.08144.

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling
Wang, and Michael I Jordan. 2020. Greedy attack
and gumbel attack: Generating adversarial examples
for discrete data. J. Mach. Learn. Res., 21(43):1–36.

Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022.
Natural attack for pre-trained models of code. arXiv
preprint arXiv:2201.08698.

Noam Yefet, Uri Alon, and Eran Yahav. 2020. Ad-
Pro-
versarial examples for models of code.
ceedings of the ACM on Programming Languages,
4(OOPSLA):1–30.

Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu,
and Zhi Jin. 2020. Generating adversarial exam-
ples for holding robustness of source code process-
ing models. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 34, pages 1169–
1176.

Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning
Du, and Yang Liu. 2019. Devign: Effective vul-
nerability identiﬁcation by learning comprehensive
program semantics via graph neural networks. Ad-
vances in neural information processing systems, 32.

Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting
Han, Taolue Chen, and Harald Gall. 2021. Adver-
sarial robustness of deep code comment generation.
arXiv preprint arXiv:2108.00213.

Task

Java-C#

C#-Java

Repair

CodeBERT

GraphCodeBERT

CodeT5

CodeT5

Victim Attack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack

CodeT5

BLEU (∆BLEU ) EM (∆EM )
85.37
77.47 (7.9)
59.38 (25.99)
64.85 (20.52)
81.81
71.26 (10.55)
54.25 (27.56)
65.31* (16.5)
80.35
72.15 (8.2)
54.54 (25.81)
62.35* (18)
81.54
72.62 (8.92)
45.71 (35.83)
58.05 (23.49)
77.24
67.31 (9.93)
48.74 (28.5)
59.41 (17.83)
70.97
62.49 (8.48)
43.2 (55.39)
48.83 (22.14)
78.11
73.23 (4.88)
65.86 (12.25)
66.1 (12.01)
78.66
67.06 (11.6)
60.52 (18.14)
61.23 (17.43)
79.73
66.19 (13.54)
64.1 (15.63)
64.7 (15.03)

67.9
47.0 (20.9)
13.2 (54.7)
5.2 (62.7)
62.5
30.4 (32.1)
3.1 (59.4)
9 (53.5)
59.4
35.24 (24.16)
2.4 (57)
9.4 (50)
70.6
50 (20.6)
15.54 (55.06)
11 (59.6)
62
34 (28)
2 (60)
2.4 (59.6)
56.5
37.33 (19.17)
1.11 (30.26)
2.6 (53.9)
19.9
1.4(18.5)
1.2 (18.7)
1.28 (18.62)
15.64
4.48 (11.16)
3.4 (12.24)
4.05 (11.59)
15.05
2.5 (12.55)
3.5 (11.55)
4.38 (10.67)

CodeBLEU (∆CB)
87.03
79.83 (7.19)
66.92 (20.11)
68.81 (18.21)
83.48
73.52 (9.95)
54.69 (28.79)
66.99* (16.49)
82.4
74.32 (8.07)
54.47 (27.93)
64.87 (17.52)
73.99
68.08 (5.91)
48.59 (25.40)
61.72 (12.27)
71.16
60.45 (10.71)
58.80 (12.36)
54.14 (17.02)
66.80
46.51 (20.29)
36.54 (27.77)
38.81 (27.99)
61.13
57.59 (3.53)
52.70 (8.42)
53.21 (7.92)
61.33
53.55 (7.78)
51.95 (9.38)
52.02 (9.31)
62.16
54.23 (7.92)
53.33 (8.83)
51.97 (10.19)

CodeBERT

GraphCodeBERT

CodeBERT

GraphCodeBERT

Table 4: Downstream Performance: Code-Code Tasks. Best result in bold and the next best is undelined.

Task

Victim

CodeT5

CodeBERT

Java-C#

GraphCodeBERT

CodeT5

CodeBERT

C#-Java

GraphCodeBERT

CodeT5

CodeBERT

Repair

GraphCodeBERT

Attack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack
TextFooler
BERT-Attack
CodeAttack

Attack% Query# BLEUq CodeBLEUq
32.3
85.6
94.8
55.9
95.39
91.1
51.21
96.2
90.8*
28.29
83.12
89.3
49.2
97.1
97.7
38.70
97.33
98
58.84
98.1
99.36
81.61
98.3
99.39
78.92
99.4
99.52

62.9
112.5
19.85
38.57
46.09
24.42
39.33
38.29*
23.22
94.95*
186.1
36.84
73.91
48.76
26.43
83.17
41.30
20.60
90.50
121.1
30.68
45.89
74.99
25.98
51.07
62.59
24.67

81.28
69.48
75.21*
83.93
76.18
76.77*
82.45
73.55
77.33
63.19*
51.11
65.91
66.61
59.90
66.89
63.62
57.41
65.39
69.53
55.49
69.03*
68.16
55.94
68.05
67.89
56.05
66.16

78.95
54.95
68.08
82.75
54.75
66.89
82.41
53.46
68.42
69.47
41.45
67.09
73.19
52.33
66.24
70.61
55.71
68.07
93.37
80.56
88.95*
91.82
80.7
87.83
91.4
81.73
85.68

Table 5: Attack Quality: Code-Code Tasks. Best result in bold and the next best is underlined.

Task

Victim

CodeT5

Java

CodeBERT

RoBERTa

CodeT5

PHP

CodeBERT

RoBERTa

CodeT5

Python

CodeBERT

RoBERTa

Attack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack
Original
TextFooler
BERT-Attack
CodeAttack

64.6
90.4
82.8

42.4
84.6
73.7

67.8
93.34
80.8

44.9
72.93
50.14

400.78
826.71
340.99

383.36
901.01
346.07

291.82
541.43
198.11

410.15
1006.28
314.87

BLEU (∆BLEU ) Attack% Query#
19.77
14.06 (5.71)
11.94 (7.82)
11.21 (8.56)
17.65
16.84 (1.20)
12.87 (4.77)
14.69 (2.85)
16.47
13.23 (3.23)
12.67 (3.8)
11.74(4.73)
20.66
14.96 (5.70)
11.96 (8.70)
11.06 (9.59)
19.76
14.38 (5.37)
11.30 (8.45)
10.88 (8.87)
19.06
14.06 (4.99)
11.34 (7.71)
10.98 (8.08)
20.26
12.11 (8.24)
8.22 (12.13)
7.79 (12.38)
78.66
20.76 (5.40)
18.95 (7.21)
18.69 (7.47)
17.01
10.72 (6.29)
10.66 (6.35)
9.5 (7.51)

966.19
1414.67
560.58

788.25
1358.85
661.75

400.06
718.07
174.05

356.68
701.01
183.22

358.43
695.03
204.46

68.5
93.72
86.63

90.47
99.81
98.50

62.6
94.15
87.51

61.1
93.74
88.32

63.34
89.64
76.09

BLEUq CodeBLEUq

75.33
54.47
68.51

65.88
34.51
59.74

67.9
28.18
32.63

78.11
49.3
66.02

79.81
50.77
69.11

79.73
51.49
70.33

86.84
77.11
87.04

76.56
55.22
63.84

70.48
51.74
55.45

92.82
48.35
90.04

90.29
83.82
59.37

90.87
42.09
48.48

53.91
51.34
52.67

54.10
50.31
52.95

54.11
50.10
53.03

77.59
64.66
69.17

75.15
52.31
59.11

74.05
56.75
61.22

Table 6: Downstream Performance and Attack Quality on Code-NL (Summarization) Task for different program-
ming languages. Best result in bold and the next best is underlined.

Original Code
public void AddMultipleBlanks

(MulBlankRecord mbr) {

for (int j = 0; j < mbr.
NumColumns; j++) {

BlankRecord br = new
BlankRecord();
br.Column = j + mbr.

FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.GetXFAt

(j));
InsertCell(br);

}

}

CodeBLEUbefore: 76.3

public string GetFullMessage

() {

byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);

if (msgB < 0) {

return string.Empty;

}
Encoding enc =

RawParseUtils.
ParseEncoding(raw);

return RawParseUtils.Decode
(enc, raw, msgB, raw.
Length);

}

CodeAttack+V-C
((void AddMultipleBlanks(

MulBlankRecord mbr) {
for (int j ? 0; j < mbr
.NumColumns; j++)

{

BlankRecord br = new
BlankRecord();
br.Column = j + mbr.

FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.
GetXFAt(j));

InsertCell(br);

}

}

∆drop: 7.21;
˘0120public string

GetFullMessage() {

byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);

if (msgB < 0) {

return string.Empty;

}
Encoding enc =

RawParseUtils.
ParseEncoding(raw);

return RawParseUtils.Decode

(enc, RAW.., msgB,
raw.Length);

CodeAttack+V+C1
public void AddMultipleBlanks

(MulBlankRecord mbr) {

for (int j > 0; j < mbr.
NumColumns; j++) {
BlankRecord br = -new

BlankRecord();
br.Column = j + mbr.

FirstColumn;
br.Row = mbr.Row;
br.XFIndex > (mbr.GetXFAt

(j));
InsertCell(br);

}

}

CodeAttack+V+C1+C2
static void AddMultipleBlanks

(MulBlankRecord mbr) {
for (int j > 0; jj < mbr.

NumColumns; j++) {

BlankRecord br = new
BlankRecord();
br.Column = j + mbr.

FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.GetXFAt

(j));
InsertCell(br);

}

}

CodeBLEUq : 43.85

∆drop: 5.85;

CodeBLEUq : 69.61

∆drop: 12.96;

CodeBLEUq : 59.29

public string GetFullMessage

static string GetFullMessage

() {

byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);

if (msgB = 0) {

return string.Empty;

}
Encoding enc =

RawParseUtils.
ParseEncoding(raw);

return RawParseUtils.Decode
(enc, raw, msgB, raw.
Length);

() {

byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);

if (msgB < 0 {

return string.Empty;

}
Encoding enc =

RawParseUtils.
ParseEncoding(raw);

return RawParseUtils.Decode

(enc, raw, MsgB,raw.
Length);

}

}

}

CodeBLEUbefore:77.09

∆drop: 10.42;

CodeBLEUq : 64.19

∆drop: 21.93;

CodeBLEUq : 87.25

∆drop: 22.8;

CodeBLEUq : 71.30

Table 7: Qualitative examples for the ablation study on CodeAttack: Attack vulnerable tokens (V) without any
constraints (-C), with operator level constraints (+C1), and with token level (+C2) contraints on code translation
task.

