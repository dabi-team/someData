An Empirical Study of Flaky Tests in JavaScript

Negar Hashemi∗, Amjed Tahir† and Shawn Rasheed‡
Massey University
Palmerston North, New Zealand
∗negar.hashemi.1@uni.massey.ac.nz, †a.tahir@massey.ac.nz, ‡s.rasheed@massey.ac.nz

2
2
0
2

l
u
J

3

]
E
S
.
s
c
[

1
v
7
4
0
1
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Flaky tests (tests with non-deterministic outcomes)
can be problematic for testing efﬁciency and software reliability.
Flaky tests in test suites can also signiﬁcantly delay software
releases. There have been several studies that attempt to quantify
the impact of test ﬂakiness in different programming languages
(e.g., Java and Python) and application domains (e.g., mobile and
GUI-based). In this paper, we conduct an empirical study of the
state of ﬂaky tests in JavaScript. We investigate two aspects of
ﬂaky tests in JavaScript projects: the main causes of ﬂaky tests
in these projects and common ﬁxing strategies. By analysing
452 commits from large, top-scoring JavaScript projects from
GitHub, we found that ﬂakiness caused by concurrency-related
issues (e.g., async wait, race conditions or deadlocks) is the most
dominant reason for test ﬂakiness. The other top causes of ﬂaky
tests are operating system-speciﬁc (e.g., features that work on
speciﬁc OS or OS versions) and network stability (e.g., internet
availability or bad socket management). In terms of how ﬂaky
tests are dealt with, the majority of those ﬂaky tests (>80%)
are ﬁxed to eliminate ﬂaky behaviour and developers sometimes
skip, quarantine or remove ﬂaky tests.

Index Terms—Flaky Tests, Test Bugs, JavaScript

I. INTRODUCTION

Test ﬂakiness is a signiﬁcant issue that affects the quality
of software products. The impact of test ﬂakiness is most
apparent in regression testing, as regression tests rely on tests
having deterministic outcomes to ensure that code changes do
not break existing functionality. Flaky tests, which have non-
deterministic outcomes due to factors such as concurrency,
randomness, shared state and reliance on external resources,
can make tests unreliable for this purpose. Such ﬂaky be-
haviour is problematic as it leads to intermittent breaks in
builds, uncertainty in choosing corrective measures for failing
tests and bug ﬁxes. This can have a negative impact on
development, product quality, and delivery [1]–[3]. Flakiness
also impacts testing-related activities such as test suite mini-
mization [4] and test parallelization [5], as well as techniques
that rely on testing such as fault localization [6] or program
repair [7].

Flaky tests are not only problematic, but they are also quite
common in large codebases. For example, it was reported that
around 16% of tests at Google are ﬂaky, and 1 in 7 of the tests
occasionally fail in a way that is not caused by changes to the
code or tests [8], making it a real challenge for automated
testing [9]. Vahabzadeh et al. [10] found that 21% of the false
alarms in Apache projects are caused by ﬂaky tests. It was
also shown that around 13% of failed builds in CI pipelines
are due to ﬂaky tests [11].

With the increased attention given to ﬂaky tests in research
and practice, there have been a number of studies that in-
vestigated different aspects of ﬂaky tests, such as detection
or elimination techniques. Previous studies on test ﬂakiness
and its causes largely focus on speciﬁc sources of test ﬂaki-
ness, such as order-dependency [12], concurrency [13] or UI-
speciﬁc ﬂakiness [14], [15]. There have been some empirical
studies that investigated ﬂakiness in the context of speciﬁc
programming languages. These studies aim to understand the
prevalence of test ﬂakiness, underlying causes and strategies
employed by developers to respond to them. The ﬁrst empirical
study by Luo et al. [16] focusses on ﬂakiness ﬁxing commits
mined from Apache projects (mostly written in Java). A
similar approach was followed for Android applications in
Thorve et al. [17]. A study on ﬂakiness in Python applications
[18] examines existing tests (by running the tests themselves)
rather than mining commits that ﬁx ﬂaky tests.

To the best of our knowledge, there are no studies that
speciﬁcally investigate ﬂaky test causes (beyond UI-causes
[15]) in JavaScript projects, despite its popularity and ex-
tensive use. JavaScript has been the most commonly used
programming language for several years1. It is also popular
in various application domains, including web (server-side and
client-side), mobile, and the Internet of Things (IoT) [19], [20].
In addition, given the nature of the language (handling HTTP
requests, running on browsers), JavaScript is known to be one
of the languages that uses asynchronous APIs extensively, with
an execution model susceptible to races. Asynchronization and
other concurrency bugs are known sources of ﬂakiness in other
languages, in particular Java and Python.

In this paper, we present the ﬁrst extensive empirical study
of ﬂaky tests in JavaScript. In particular, the goal of this paper
is to investigate how prevalent ﬂaky tests are in JavaScript
projects, and to understand the causes and impact of ﬂaky tests
in these projects. To further investigate ﬂaky tests, we collect
and analyse 452 commits from popular JavaScript projects on
GitHub. For each ﬂaky test, we inspect the commit messages,
the pull requests, and the changes to the code to understand
the cause of ﬂakiness. Also, we identify the main strategies
that developers follow to deal with ﬂaky tests.

The remainder of this paper is structured as follows: we
present related work on ﬂaky tests in Section II. Our research
questions and methodology are explained in Section III. We

1https://insights.stackoverﬂow.com/survey/2021#technology-most-popular

-technologies

 
 
 
 
 
 
present our detailed results and answer our research questions,
followed by a discussion of the results in Section IV. Finally,
we present our conclusion in Section VI.

II. RELATED WORK

Several studies published in recent years investigated the
main causes and impact of ﬂaky tests in both open-source
and proprietary software. In a study of the maintenance of the
regression test suites in GitHub projects [11], it was observed
that about 13% of the test failures are due to ﬂaky tests.
A survey with developers on the precipitation of ﬂaky tests
by Ahmad et al. [21] identiﬁed 23 factors that are perceived
to affect test ﬂakiness (including technical and organisational
factors). Similarly, Eck et al. [22] studied developers’ precipi-
tation when it comes to ﬂaky tests (including Mozilla projects’
developers) to examine the nature and the origin of 200 ﬂaky
tests that had been ﬁxed by the same developers. It was found
that ﬂakiness is perceived as a signiﬁcant issue by the vast
majority of developers surveyed.

Lam et al. [23] conducted a large-scale longitudinal study
of ﬂaky tests to determine when those tests become ﬂaky, and
what changes cause them to become so. They found that 75%
of the ﬂaky tests (184 out of 245) are ﬂaky when added, and
85% of ﬂaky tests can be detected when detectors are run on
newly added or directly modiﬁed tests. Vahabzadeh et al. [10]
studied the types of bugs in test code and observed that ﬂaky
tests and semantic bugs constitute the dominant cause of tests
provoking false alarms in the test code.

There have been a few empirical studies on ﬂaky tests in
different programming languages and application domains, but
none we could ﬁnd that studied ﬂakiness, as an issue in itself,
speciﬁcally in JavaScript. We discuss a number of those studies
below, and provide a summary of ﬂaky test causes in Table I.
Luo et al. [16] reported the ﬁrst extensive empirical study
of ﬂaky tests, categorizing causes and ﬁxing strategies of ﬂaky
tests by studying 201 commits of ﬁxes in open-source Apache
projects. The root causes of ﬂakiness were split into ten main
categories: async wait, concurrency, test order dependency,
resource leak, network, time, IO, randomness, ﬂoating point
operations, and unordered collections.

Gruber et al. [24] analysed projects from the Python Pack-
age Index2 to study the cause of the detected ﬂaky tests, and
found that order dependency is the main cause of ﬂakiness in
Python projects. The study identiﬁed infrastructure ﬂakiness
as a new type of test ﬂakiness that has not been reported
previously. Lam et al. [25] studied the lifecycle of ﬂaky
tests in six large-scale projects at Microsoft, and found that
asynchronous calls are the leading cause of ﬂaky tests in those
projects. They also proposed an automated solution, called
Flakiness and Time Balancer (FaTB), to reduce the frequency
of ﬂaky-test failures caused by asynchronous calls.

The impact of ﬂaky tests has also been studied in different
application domains. A study of ﬂaky tests in Android apps
[17] identiﬁed two new causes to those discussed in [16],

namely: program logic and UI. Dutta et al. [26] studied ﬂaky
tests in applications that use probabilistic programming or
machine learning frameworks, ﬁnding that randomness, as
expected, is the biggest cause of ﬂakiness in those applications.
Romano et al. [15] analysed 235 ﬂaky UI tests from 62 web
and Android projects, and identiﬁed four categories of causes
for UI-based ﬂaky tests. Moran et al. [27] focused on detecting
the root cause of ﬂakiness in web applications by analysing test
execution under different combinations of the environmental
factors that may trigger ﬂakiness.

There have also been a number of tools that have been
targeted to detect certain types of ﬂaky tests such as DeFlaker
[28], RootFinder [29], iFixFlakies [30], SHAKER [31] and
FlakeScanner [32].

III. STUDY METHODOLOGY

The main goal of this study is to investigate causes and
strategies followed to deal with ﬂaky tests in JavaScript. In
this paper we answer the following two research questions:

RQ1 What are the main causes of ﬂaky tests in JavaScript
projects?

RQ2 What are the strategies followed when dealing with
ﬂaky tests in JavaScript projects?

Our target is to analyse commits that are believed to refer
to tests with ﬂaky behaviour, rather than exposing ﬂakiness
by compiling and analysing the programs themselves. We
conducted the study in two phases. We ﬁrst collect commits
(including commit messages and source code changes) from
open-source JavaScript projects and then manually analyse the
causes of ﬂakiness as noted in these commits. We also analyse
the strategies followed when those ﬂaky tests were ﬁxed.

A. Dataset

We constructed a dataset of JavaScript projects obtained
from GitHub. We targeted only popular projects based on
the number of stars of each project (based on the star rating
on GitHub). GitHub’s star feature allows users to mark their
interest or helpful repositories on GitHub. In a way, it is a
metric that shows how much interest a project has drawn. We
found that the number of stars is highly correlated with the
number of forks and contributors, indicating popularity.

To search through the repositories, we used a GitHub API3
to search through all publicly available GitHub repositories
and ﬁlter results of the commits for analysis. We ﬁrst extract
the top 40 starred JavaScript projects on GitHub. We choose
the top 40 projects because they provide a large enough sample
(number of commits) that is deemed suitable for our analysis.
We compared the number of commits we retrieved by the
ones in similar previous studies: Luo et al. [16] retrieved 486
commits in total, but analysed 201 commits that ﬁx ﬂaky tests;
while Thorve et al. [17] retrieved 77 commits. Romano et al.

2https://pypi.org/

3https://docs.github.com/en/rest/reference/search

TABLE I
CAUSES OF FLAKY TESTS AS IDENTIFIED IN PRIOR STUDIES

Luo et al. [16]

Thorve et al. [17]

Dutta et al. [26]

Gruber et al. [24]

Concurrency
Dependency
Program Logic
Network
UI

Async Wait
Concurrency
Test Order Dependency
Resource Leak
Network
Time
IO
Randomness
Floating Point Operations
Unordered Collections

Algorithmic Non-determinism Infrastructure
Floating-point Computations
Incorrect/Flaky API Usage
Unsynced Seeds
Concurrency
Hardware
Other

Test Order Dependency
Network
Randomness
IO
Time
Async Wait
Concurrency
Resource leak
Test Case Timeout
Unordered Collections

[15] analysed 235 ﬂaky UI test. In comparison, we extracted
a total of 316,948 commits and ended up with 735 ﬂaky tests’
related commits.

• Irrelevant: commits that contain one of the keywords,

but it is not related to ﬂakiness.
• Duplicated: identical commits.

B. Mining Process

Our mining process is shown in Fig. 1. By using GitHub
API, we searched through 40 repositories for the following
keywords: “ﬂaky”, “ﬂakey”, “ﬂakiness”, “intermit”, “frag-
ile”, “brittle”, “Intermittent”, and “non-deterministic test”. We
found 18 repositories to have at least one commit with one of
the above keywords we used. This resulted in a total of 735
commits. We conducted this search on May 8th, 2021. Table
II shows statistics from the projects we analysed.

Out of those 735 commits, we found that 254 of them are
related to languages other than JavaScript, so we excluded
those as we are interested only in ﬂakiness that is related to
JavaScript. This left us with a total of 481 commits that are
relevant to our analysis.

C. Manual Analysis of Commits

We manually analysed all 481 commits that we included.
For each commit, we analysed the commit message, any code
changes in the ﬁles in the repository as well as the associated
issues (if any) in the issue tracker. We started our process
by manually ﬁltering out commits related to languages other
than JavaScript (out of 735 commits, we found 481 to be
JavaScript-related). Two of the authors then individually and
separately classiﬁed all 481 commits based on the cause of
ﬂakiness and then compared their classiﬁcation results. To
inform our classiﬁcation, we checked the associated issues (if
any), pull requests and changes in the test and CUT. When
there were classiﬁcation disagreements between the two au-
thors, a third co-author was then involved to resolve conﬂicts.
The third author analysed a total of 139 commits (where the
two authors did not agree or were not sure). Disagreements
between all authors were settled through discussions until a
100% agreement was reached.

We categorize each of these commits into one of the

following :

• Relevant: commits that are directly related to ﬂaky tests.

Of those 481 commits, we found that 11 commits were
found to be irrelevant (not related to test ﬂakiness), and 18
commits were duplicated,
thus those were excluded. This
resulted in a total of 452 commits that we considered relevant,
which we manually classiﬁed. We provide a full replication
package (including scripts and data) online at: https://doi.org/
10.5281/zenodo.6757825.

IV. RESULTS

A. RQ1: What are the main causes of ﬂaky tests in JavaScript
projects?

To answer RQ1, we manually analysed and categorised all
commits based on the relevant causes of ﬂakiness (based on
the commit message or the change in the ﬁles associated with
the commit). We used the list of causes noted in previous
empirical studies on test ﬂakiness as our baseline (see Table
I). We then classiﬁed each commit based on the list of ﬂakiness
causes. If the cause of ﬂakiness is new or not in the list, we
then add it as a new cause and continue with our classiﬁcation.
During this process, we were not able to categorise or
determine the cause of ﬂakiness in 94 commits. Hence, those
were categorised as “unsure” or “hard to categorise” and we
continued to analyse the remaining 358 commits.

As it turns out, most of the causes we identiﬁed ﬁt well
into one or more of the categories noted in previous studies
(i.e., [16], [17], [24], [26]). However, some of the causes we
identiﬁed resulted in new categories that best reﬂect the nature
of ﬂakiness and thus those have been categorised separately.
Table III shows the results of the top 10 causes we found
in the JavaScript projects we analysed. We present the results
of our categories below, together with examples of each cause
from the projects we analysed.
Concurrency We classify a commit in this category when a
test is ﬂaky due to any concurrency-related issues in the test or
code under test (CUT) such as event races, atomicity violations
or deadlocks [33]. We found that 74 of the ﬂaky tests belong

TABLE II
PROJECTS FROM THE TOP 40 MOST STARRED JAVASCRIPT PROJECTS ON GITHUB WITH COMMIT MESSAGES CONTAINING AT LEAST ONE OF THE SEARCH
KEYWORDS

Repositories

#Stars

#Commits

JavaScript
File

#Flakiness Re-
lated Commits

#Contributor

Age

Description

freeCodeCamp/freeCodeCamp

320k

28253

vuejs/vue

facebook/react

twbs/bootstrap

electron/electron
nodejs/node

188k

3200

174k

14448

153k

21165

96.6k
81.6k

25574
34487

denoland/deno

77.7k

6310

mrdoob/three.js
mui-org/material-ui
storybookjs/storybook
atom/atom
jquery/jquery
chartjs/Chart.js
ElemeFE/element
lodash/lodash
moment/moment
meteor/meteor
yarnpkg/yarn

Total

74.6k
70.9k
64.5k
56k
55.3k
54.7k
50.8k
50.6k
46k
42.6k
40.1k

38172
17626
35707
38404
6536
4043
4500
8005
3961
24211
2346

47.4%

97.6%

95.7%

41.4%

6.5%
61.2%

23.6%

54.2%
58.2%
9%
88.3%
93.6%
98.3%
26.7%
100%
99.7%
92.3%
98.7%

4

2

22

5

48
467

3

1
19
14
68
9
1
1
2
8
50
11

4287

388

1495

1231

1069
3012

635

1482
2278
1365
488
276
380
556
310
590
465
521

Oct. 2014

Dec. 2013

July 2013

Oct. 2011

July 2013
May 2009

May 2018

Apr. 2010
Nov. 2014
April 2016
Dec. 2013
Jan. 2006
June 2014
Sep. 2016
April 2012
Jan. 2015
Jan. 2012
June 2016

runtime

JavaScript

Corpus of educational
projects
JavaScript framework for building
UI
JavaScript library for building user
interfaces
Web development JavaScript frame-
work
Desktop apps framework
Cross-platform JavaScript
environment
A JavaScript and TypeScript runtime
environment
JavaScript 3D Library
A framework to build React apps
UI components development tool
Cross-platform text editor
JavaScript Library
Data visualization library
A Vue.js-based UI Toolkit for Web
A modern JavaScript utility library
A JavaScript date library
JavaScript web framework
JavaScript package manager

316948

735

20828

Fig. 1. Mining ﬂaky tests’ commits from GitHub’s JavaScript projects

to this category. For example, in this snippet4 from Node.js,
using setImmediate in line 6 could lead to race conditions,
which lead to non-deterministic test outcomes.

1 onst keepOpen = setTimeout(() => {
2 @@ -20,7 +20,7 @@ const timer = setInterval(()←(cid:45)

=> {

3

4

5

6

timer._onTimeout = () => {

throw new Error(’Unrefd interval fired ←(cid:45)

after being cleared.’);

};
setImmediate(() => clearTimeout(keepOpen));

}
7
8 }, 1);

Another example of a concurrency related ﬂaky tests is the
following example from Meteor5. This test is ﬂaky because
the template rendered callbacks get called after ﬂush time, but
not if the template got destroyed. If the client managed to
respond to the server rejecting the method before the client’s
ﬂush cycle, the rendered callback would never ﬁre.

1 testAsyncMulti(’spacebars - template - defer ←(cid:45)
in rendered callbacks’, [function (test, ←(cid:45)
expect) {

4https://tinyurl.com/2s32r7xk

5https://tinyurl.com/ysde5t37

GitHub 735 JSflaky testscommitsMining GitHub  for JS commits via GH API254 commits(Discarded)Other Languages481 commitsJavaScriptManually Analysis452 commits18 commits11 commitsRelevantDuplicatedIrrelevant358 commits94 commitsClassificationof causesHard toclassify (Discarded) TABLE III
OVERALL RESULTS OF FLAKY TEST CATEGORIES (CAUSES) IN JAVASCRIPT

Cause of Flakiness

# of Commits

%

Concurrency
Async Wait
OS
Network
Platform
UI
Hardware
Time
Resource Leak
Other

Total

20.7%
19.6%
18.4%
12.6%
10.3%
5.9%
4.7%
3.4%
2.8%
3.6%

74
70
66
45
37
21
17
12
10
13

365*

* The total number of commits is 358 as there are 7
commits related to 2 causes of ﬂakiness.

2

3

4

5

6

7

8

9

10

11

12

13

var tmpl = Template.←(cid:45)

spacebars_template_test_defer_in_rendered←(cid:45)
;

var coll = new Meteor.Collection("test-defer-←(cid:45)

in-rendered--client-only");

tmpl.items = function () {

return coll.find();

};
var subtmpl = Template.←(cid:45)

spacebars_template_test_defer_ ←(cid:45)
in_rendered_subtemplate;

subtmpl.rendered = expect(function () {

Meteor.defer(function () {
});

});
var div = renderToDiv(tmpl);
Meteor._suppress_log(1);
coll.insert({});

14
15 }]);

test7

the

using

following

FreeCode-
from
In
cy.contains(’Go to next
Camp,
challenge’).click(); in line 10 and then not
waiting for the result before visiting the next page can cause
ﬂakiness.

1 describe(’project submission’, () => {
2

it(’Should be possible to submit Python ←(cid:45)

3

4

5

6

7

8

9

projects’, () => {

const { superBlock, block, challenges } = ←(cid:45)

projects;

challenges.forEach(challenge => {

cy.visit(‘/learn/${superBlock}/${block}/${←(cid:45)

challenge}‘);

cy.get(’#dynamic-front-end-form’)

.get(’#solution’)
.type(’https://repl.it/@camperbot/python←(cid:45)

-project#main.py’);

cy.contains("I’ve completed this challenge←(cid:45)

").click();

10

cy.contains(’Go to next challenge’).click←(cid:45)

();

Operating Systems (OS) A commit is classiﬁed to be in the
OS category when the test fails due to a run in a speciﬁc OS
or OS version, while passing on others. It can happen because
of a test dependency on speciﬁc OS features or environmental
settings. We classify OS as a separate category of root causes
from the platform (discussed later) because it represents a large
percentage of ﬂaky tests by itself. In total, we found that 66
commits belong to the OS category. The following test8 from
Node.js is a good example of a ﬂaky test in this category.

1 if (process.platform === ’darwin’) {
2

setTimeout(function() {

fs.writeFileSync(filepathOne, ’world’);

}, 100);
} else {
fs.writeFileSync(filepathOne, ’world’);

3

4

5

6
7 }

Async Wait A commit is labelled to be in this category when
a test makes an asynchronous call and does not wait properly
for the result of the call to become available before using
it. Async Wait is in fact a subcategory of concurrency, but
we classify it separately here mainly because it represents
nearly half (48%) of the concurrency-related ﬂaky tests. In
the projects we analysed, there are 70 commits in total that
ﬁts into the async wait category, such as the following example
from Electron6:

1

2

3

4

it(’should clear the navigation history’, ←(cid:45)

async () => {

loadWebView(webview, {
nodeintegration: ’on’,
src: ‘file://${fixtures}/pages/history.html←(cid:45)

‘

})

5
6 const event = await waitForEvent(webview, ’ipc←(cid:45)

-message’)

In this test, the IPC message was sent before it was fully
loaded, so the history did not contain everything it should have,
and cannot clear all the pages, leading to non-deterministic
outcomes.

6https://tinyurl.com/2p8zr5k4

In OS X, events for fs.watch() might only start showing
up after a delay. To work around that, there is a timer in the
test that delays the writing of the ﬁle by 100ms, but in some
cases this might not be enough. So the event never ﬁred, and
the test times out.

Another example of an OS ﬂaky test from Angular9 shows
a test that resulted in a ECONNREFUSED error (i.e., refused
connection) for only Windows, but it works well for other OS.

1 function launchChromeAndRunLighthouse(url, ←(cid:45)

flags, config) {

const launcher = new ChromeLauncher({←(cid:45)

autoSelectChrome: true});

return launcher.run().

then(() => lighthouse(url, flags, config)).
then(results => launcher.kill().then(() => ←(cid:45)

results)).

catch(err => launcher.kill().then(() => { ←(cid:45)
throw err; }, () => { throw err; }));

2

3

4

5

6

7 }

Network A commit is in this category when a test fails due to
remote connection failures (e.g., lost internet connection when

7https://tinyurl.com/2p8ebjbx
8https://tinyurl.com/533ycmsc
9https://tinyurl.com/2p9d7rxk

accessing an external URL) or local bad socket management.
There are 45 commits that ﬁt into the network category. One
example we see from this category is the following test from
Node.js10, which is using port ‘0’ for an IPv6-only operation
and assuming that the OS would supply a port that was also
available in IPv4. However, the CI results seem to indicate
that a port can be supplied that is in use by IPv4 but available
to IPv6, resulting in the test failing.

1

2

3

4

5

net.createServer().listen({

host,
port: 0,
ipv6Only: true,

}, common.mustCall());

Another example from React11 shows a test

that uses

absolute URLs, which can end up in ﬂakiness.

1 var __dirname = __filename.split(’/’).reverse←(cid:45)

().slice(1).reverse().join(’/’);

2 window.ReactWebWorker_URL = __dirname + ’/../←(cid:45)

src/test/worker.js’ + cacheBust;

3 document.write(’<script src="’ + __dirname + ’←(cid:45)
/../build/jasmine.js’ + cacheBust + ’"><\/←(cid:45)
script>’);

4 document.write(’<script src="’ + __dirname + ’←(cid:45)

/../build/react.js’ + cacheBust + ’"><\/←(cid:45)
script>’);

5 document.write(’<script src="’ + __dirname + ’←(cid:45)

/../build/react-test.js’ + cacheBust + ’←(cid:45)
"><\/script>’);

6 document.write(’<script src="’ + __dirname + ’←(cid:45)
/../node_modules/jasmine-tapreporter/src/←(cid:45)
tapreporter.js’ + cacheBust + ’"><\/script←(cid:45)
>’);

7 document.write(’<script src="’ + __dirname + ’←(cid:45)
/../test/the-files-to-test.generated.js’ +←(cid:45)

cacheBust + ’"><\/script>’);

8 document.write(’<script src="’ + __dirname + ’←(cid:45)
/../test/jasmine-execute.js’ + cacheBust +←(cid:45)

’"><\/script>’);

Platform The commits in this category include all ﬂaky
tests related to platform, e.g.,
test environment, browsers.
We excluded OS related ﬂaky tests here as those, although
are platform related, are a large enough subcategory that we
discussed separately earlier in this section. We identiﬁed 37
commits in total from the platform category. The following
example12 from Electon shows a test
that, due to some
speciﬁcations, failed on one CI, but passed on another. The
test times out regularly on Travis CI but pass when built on
Jenkins.

1 it(’can be manually resized with setSize even ←(cid:45)
when attribute is present’, done => {
w = new BrowserWindow({show: false, width:←(cid:45)

2

200, height: 200})

3

w.loadURL(’file://’ + fixtures + ’/pages/←(cid:45)

webview-no-guest-resize.html’)

Similarly, the following test13 from Node.js is ﬂaky due to

reliance on platform timing in lines 3, 5, 8, 9, 11, and 18.

10https://tinyurl.com/yc6c97m6
11https://tinyurl.com/yfprp6tx
12https://tinyurl.com/3xms6muw
13https://tinyurl.com/348bzef5

1 const common = require(’../common’);
2 const fs = require(’fs’);
3 const platformTimeout = common.platformTimeout←(cid:45)

;

common.busyLoop(platformTimeout(12));

4 const t1 = setInterval(() => {
5
6 }, platformTimeout(10));
7 const t2 = setInterval(() => {
8
9 }, platformTimeout(10));
10 const t3 =
11

common.busyLoop(platformTimeout(15));

setTimeout(common.mustNotCall(’eventloop ←(cid:45)
blocked!’), platformTimeout(200));

12 setTimeout(function() {
13

fs.stat(’/dev/nonexistent’, () => {

14

15

16

clearInterval(t1);
clearInterval(t2);
clearTimeout(t3);

});

17
18 }, platformTimeout(50));

UI The commits in this category include all ﬂaky tests related
to UI features, e.g., ﬂakiness due to different windows display,
the blinking cursor, or the highlight decoration. There are 21
tests that belong to the UI category. The following example14
from Atom shows a test aims to verify that the cursor blinks
when the editor is focused and the cursors are not moving.
It expects the blinking cursor to start in the visible state, and
then transition to the invisible state. But the initial state of the
cursor is not important, since the cursor toggles between the
visible and the invisible state.

1 await component.getNextUpdatePromise()
2

const [cursor1, cursor2] = element.←(cid:45)
querySelectorAll(’.cursor’)

3

4

5

6

7

8

9

10

11

12

13

expect(getComputedStyle(cursor1).opacity).←(cid:45)

toBe(’1’)

expect(getComputedStyle(cursor2).opacity).←(cid:45)

toBe(’1’)

await conditionPromise(() =>

getComputedStyle(cursor1).opacity === ’0←(cid:45)
’ && getComputedStyle(cursor2).←(cid:45)
opacity === ’0’

)
await conditionPromise(() =>

getComputedStyle(cursor1).opacity === ’1←(cid:45)
’ && getComputedStyle(cursor2).←(cid:45)
opacity === ’1’

)
await conditionPromise(() =>

getComputedStyle(cursor1).opacity === ’0←(cid:45)
’ && getComputedStyle(cursor2).←(cid:45)
opacity === ’0’

)

Also, the following Atom test15 is ﬂaky due to UI related
issues. Resize events are unreliable and may not be emitted
right away. This could cause the test code to wait for an update
promise that was unrelated to the resize event (e.g., cursor
blinking).

1

2

3

setEditorHeightInLines(component, 13);
await setEditorWidthInCharacters(component, ←(cid:45)

50);

expect(component.getRenderedStartRow()).toBe←(cid:45)

(0);

14https://tinyurl.com/yckkhh4a
15https://tinyurl.com/mvdvm9vf

4

expect(component.getRenderedEndRow()).toBe(13)←(cid:45)

;

Hardware This category combines all ﬂakiness in test out-
comes that are resulted from hardware-dependencies. We
found a total of 17 commits related to the hardware category.
For example, in the following example16 from Node.js, the
test runs in Raspberry Pi. The number of clients is set to 100,
which has caused some ﬂaky behaviour when run in Raspberry
Pi.

1 var responses = 0;
2 var N = 10;
3 var M = 10;
4 server.listen(common.PORT, function() {
5

for (var i = 0; i < N; i++) {

6

7

8

9

10

11

12

setTimeout(function() {

for (var j = 0; j < M; j++) {

http.get({ port: common.PORT, path: ’/’ ←(cid:45)

}, function(res) {

console.log(’%d %d’, responses, res.←(cid:45)

statusCode);

if (++responses == N * M) {

console.error(’Received all responses,←(cid:45)

closing server’);

server.close();

Another example from this category is the following test
from Node.js test17, which, when looping rapidly and making
new connections (in line 5) it can cause theRaspberry Pi 2
Model to malfunction.

1 const server = net.createServer(function ←(cid:45)

listener(c) {

connections.push(c);

2
3 }).listen(common.PORT, function ←(cid:45)

makeConnections() {

for (var i = 0; i < NUM; i++) {

net.connect(common.PORT, function connected←(cid:45)

() {

clientConnected(this);

});

4

5

6

7

Time The commits in this category have ﬂakiness that comes
from time related issues, e.g., relying on the system’s time.
We classify 12 commits in total into the time category. The
following example18 shows a ﬂaky test that is the result of
relying on the system’s time with the use of Date.now()
function.

1 const now = Date.now();
2 while (now + 10 >= Date.now());

The following snippet19 from Moment can cause ﬂakiness
because of checking the equality of times that can be different
by a millisecond.

1 test.expect(7);
2 test.equal(moment.utc().valueOf(), moment().←(cid:45)
valueOf(), "Calling moment.utc() should ←(cid:45)
default to the current time");

16https://tinyurl.com/mttkmtx5
17https://tinyurl.com/2rfs42zs
18https://tinyurl.com/354h3uf4
19https://tinyurl.com/mvfmf8rz

Resource Leak We classify a commit in this category in
cases where the CUT did not properly manage resources,
e.g., memory allocations issues, database connections, loss
of connection, or not enough space. There are 10 commits
from the resource leak category. The following example20 from
Node.js shows a ﬂaky test due to EMFILE, which means that
a process is trying to open too many ﬁles.

1 if (accumulated.includes(’Error:’) && !←(cid:45)

finished) {

assert(

accumulated.includes(’ENOSPC: System limit←(cid:45)

for number ’ +

’of file watchers reached’←(cid:45)

),

accumulated);

2

3

4

5

The following snippet21 from Node.js shows a test that is
not performing proper clean-up and so it would fail if run
more than one time on the same machine.

1 function test_up_multiple(cb) {
2

console.error(’test_up_multiple’);
if (skipSymlinks) {

3

4

5

6

7

8

console.log(’skipping symlink test (no ←(cid:45)

privs)’);

return runNextTest();

}
fs.mkdirSync(tmp(’a’), 0755);
fs.mkdirSync(tmp(’a/b’), 0755);
fs.symlinkSync(’..’, tmp(’a/d’), ’dir’);

9
10 @@ -432,6 +443,7 @@ function test_up_multiple(←(cid:45)

cb) {
if (er) throw er;
assert.equal(abedabed_real, real);
cb();
});

11

12

13

14

});

15
16 }

Other Known Causes Other remaining causes (13 commits)
are related to several categories. The remaining causes are
(each provided with an example from the projects we anal-
ysed): IO22, test order dependency23, ﬂoating point opera-
tions24, randomness25, and implementation dependency26.

In addition, we categorized 7 commits in total into more
than one cause. For those commits, we believe that there are
multiple causes that may have an equal effect on the ﬂaky
behaviour. For example, the following test27 from Node.js
falsely assumed that closing the client (which also currently
destroys the socket rather than shutting down the connection)
would still leave enough time for the server side to receive the
stream error.

1 server.on(’stream’, common.mustCall((stream) ←(cid:45)

=> {

2

3

stream.on(’error’, common.mustCall(() => {
stream.on(’close’, common.mustCall(() => {

20https://tinyurl.com/2s44nnua
21https://tinyurl.com/4yycbc73
22https://tinyurl.com/2zv3acya
23https://tinyurl.com/39nnnt85
24https://tinyurl.com/2p999bjw
25https://tinyurl.com/3utzckbb
26https://tinyurl.com/yc2vtvzz
27https://tinyurl.com/2p8hwj7w

category (the test failed due to set numbers of clients in
Raspberry Pi). The CI results29 show that
is ﬁxed by
it
reducing the number of clients from 100 to 16.

1

var N = 10;
var M = 10;

2
3 var N = 4;
4 var M = 4;
5 server.listen(common.PORT, function() {
6

for (var i = 0; i < N; i++) {

Fig. 2. Distribution of Response Strategies

4

5

6

7

8

9

10

server.close();

}));

req = client.request();
req.resume();
req.on(’error’, common.mustCall(() => {

req.on(’close’, common.mustCall(() => {

client.close();

Both the network and the wait for the connection issues have
the same impact on the test being ﬂaky. Hence, we categorized
this test under network and async wait categories.

RQ1 ﬁndings: The top four causes of test ﬂakiness in
JavaScript projects are concurrency (21%), async wait
(20%), OS (18%) and network (13%). Other causes of
ﬂaky tests we found include platform, UI, hardware,
time and resource leak.

B. RQ2: What are the common strategies followed when
dealing with ﬂaky tests in JavaScript projects?

When developers face a ﬂaky test, they usually ﬁx it by
changing the test code, the CUT, or both. We found that
a total of 350 commits to ﬁx ﬂakiness by changing the
test code, 3 commits changed CUT, and 5 commits made
changes to both. Although those changes aim to ﬁx the ﬂaky
tests, they do not necessarily completely get rid of the ﬂaky
behaviour, but instead they improve/reduce such behaviour. In
other words, the changes sometimes decrease the chance of
the ﬂaky behaviour, but do not eliminate it.

For each commit, we identify the main strategies that
developers use to deal with ﬂaky tests. We categorise those into
one of ﬁve different strategies, as follows. A visual summary
of the distribution of commits based on the followed strategy
is shown in Fig. 2.
Fix: This strategy aims to ﬁx any ﬂaky test that has been
detected/reported (after reproducing and conﬁrming the ﬂaky
behaviour). We found that 295 commits to present an imme-
diate ﬁx of the noted ﬂaky test. This is not surprising as we
mainly mined commits with known ﬂaky tests (that is, ﬂaky
tests that have already been detected by the developers, and
thus it is very likely to have been actioned). The following
example28 from Node.js shows a ﬂaky test from the Hardware

28https://tinyurl.com/mttkmtx5

7

8

9

setTimeout(function() {

for (var j = 0; j < M; j++) {

http.get({ port: common.PORT, path: ’/’ ←(cid:45)

}, function(res) {

Improve: In this strategy,
the test code or the CUT are
changed to decrease the chance of ﬂakiness or to make the
code more traceable to ﬁx it later (which makes it technical
debt in nature). There are 23 commits that in this category.
The following example30 from Atom shows an added delay to
the tests to make them less ﬂaky, but they did not ﬁx the root
problem (an explanation is provided in the comment section
in the following listing).

1 // In Windows64, in some situations nsfw (the ←(cid:45)

currently default watcher)

2 // does not trigger the change events if they ←(cid:45)

happen just after start watching,

3 // so we need to wait some time. More info: ←(cid:45)
https://github.com/atom/atom/issues/19442

4 await timeoutPromise(300);

that

Skip/ignore/disable: This strategy provides an option to de-
velopers to skip or ignore the test
is ﬂaky. In some
cases, especially when the developer is fully aware of the
implications of those ﬂaky tests, they may decide to disable
those ﬂaky tests and continue with the build as planned. We
found 26 commits in total that belong to this category. For
example, in the following code31 from Electron, there is a
condition to skip the test for “win32” platform or “arm64”
architecture.

1

if describe(process.platform !== ’win32’ || ←(cid:45)

process.arch !== ’arm64’)(’did-change-←(cid:45)
theme-color event’, () => {

The developer noted the following in a pull request32:
“...there are a couple of tests that are consistently
ﬂaky on arm (both Linux and Windows variants).
This PR disables those ﬂakes so that we can get our
ARM CI to the point where it can be relied on for
PR validation instead of maintainers ignoring it.”
Quarantine: In this strategy, developers isolate ﬂaky tests
from other healthy tests by keeping them in a quarantined
area (e.g., different test suite or development branch) in order
to diagnose and then ﬁx those tests. This is a common strategy
used in practice (e.g., [8], [34], [35]). There are 8 commits that
belong to this category. For example, the code33 from Node.js

29https://tinyurl.com/2zz6fmvs
30https://tinyurl.com/nwmexy27
31https://tinyurl.com/bdzewfc7
32https://tinyurl.com/3epjpe5b
33https://tinyurl.com/4ybhbhj7

 Fix82%Improve7%Skip/Ignore/Disable7%Quarantine2%Remove2%move a portion of the test to a separate ﬁle as the test is ﬂaky
on CentOS. The developer then noted that:

TABLE IV
COMMON CHANGES FOR FOUR TOP CATEGORIES

“[test] has been ﬂaky on CentOS. This allows us to ..
eliminate the cause of the ﬂakiness without affecting
other unrelated portions of the test.”

Remove: The strategy suggests that all ﬂaky tests should
be removed from the test suite. We found 6 commits that
remove the test to completely eliminate the ﬂaky behaviour.
For example, there is a commit34 from Node.js shows a tests
that was completely removed the test from the test suit due to
being ﬂaky. The developer noted the following:

“[the test] is supposed to test an internal debug
feature, but what it effectively ends up testing, is the
exact lifecycle of different kinds of internal handles
... making the test fail intermittently ... It’s not a
good test, delete it.”

Fig. 3 demonstrates the percentages of strategies followed
based on the cause of ﬂakiness. The Fix strategy represents
the largest portion of each category. The most commits under
Improve category are related to UI, hardware, and time causes.
Similarly, as expected, most Skip actions are taken to deal
with OS and Platform-dependent tests. The reason is that
the tests are conditional in nature (will run in speciﬁc OSs
or platforms), so that
the test can run without any non-
determinism. Also, the largest portion of Remove is related to
UI and Platform categories. We list the most common change
patterns used to ﬁx the top four causes (concurrency, async
wait, OS and network) in Table IV.

RQ2 ﬁndings: We found that 82% of ﬂaky tests are
ﬁxed (to eliminate the ﬂaky behaviour), 7% improved
(reduces ﬂakiness), 7% skipped or disabled, 2% quar-
antined (for a later ﬁx), and 2% completely removed.

C. Implications

Our study has implications for both JavaScript testers (who
write test code that might be ﬂaky) and designers of ﬂaky
test detection and management tools. Our results show that
JavaScript projects, as with projects in other languages studied
to date, are prone to ﬂaky tests, and that common causes
of ﬂaky tests in programs written using other languages (in
particular Java and Python) are also prevalent in JavaScript.
Similarly to prior studies [16], [17], we observed Concurrency
and Async Wait as top causes for ﬂakiness in JavaScript
projects. Unlike in other empirical studies, Platform is also
a major cause of ﬂakiness in JavaScript. This includes all sce-
narios where the test runs in a different environment, browser,
or OS. The implication from this observation is that implicit
platform dependence (e.g. speciﬁc OS) must be made explicit
in the test code. Having automated ways of detecting platform
related ﬂaky tests would be of great value to developers. Any
methods that can enhance developers’ understanding of the
requirements and limitations of the different platforms used

34https://tinyurl.com/3uzke4rf

Category

Common Change

Description

Concurrency

Async Wait

OS

waitForEvent
increase timeout
setTimeout
setInterval
sleep
add lock

waitForEvent
async
reordering code
Await
setTimeout
setInterval
increase timeout
timeoutPromise

add condition

async
setTimeout

Network

common.port
socket.setTimeout
socket.destory
socket.end()
client.shutdown()

the test

In some cases,
is
ﬂaky because of conﬂiciting
operations in different threads.
By making the test wait and
adding locks, they can prevent
race conditions and synchro-
nizing operations.

Most of the time, developers
add time to the test to deal
with async wait related test
ﬂakiness. For doing this, they
usually increase time out or
wait for special events.

In most of the cases, develop-
ers check the type and version
and run/skip tests on speciﬁc
ones.
Sometimes,
is ﬂaky
because of the way the OS
manages resources as it can
take more time to run than
expected.

the test

In most cases, network related
ﬂakiness is caused by bad
socket or port management, or
lost internet connection.

(on-the-ﬂy) can potentially reduce the number of ﬂaky tests
caused by platform dependency.

The results of Concurrency and Async Wait being the most
dominant cause of ﬂakiness is expected given that JavaScript
applications are highly asynchronous. Unlike previous studies
[16], [24], we observed a small number of ﬂaky tests that were
caused by Test Order Dependency. The platform or testing
frameworks are unlikely to be the cause for this low number,
since testing frameworks for JavasScript (e.g. Jest35, Mocha36)
have support for parallelising and ordering test execution and
support for shared state across tests. It is possible that implicit
shared state is more likely in object-oriented languages such
as Java with static ﬁelds. Note that nearly 47% of shared state
in order-dependent tests [36] are external (e.g. ﬁles, database).
In addition, current detection tools focus mainly on lan-
guages like Java, Python and Ruby. Searching through the
different resources and the recent literature surveys on ﬂaky
tests [36], [37], we found only one tool, NodeRacer [38],
that aims to manifest test ﬂakiness causes by event races in
JavaScript programs. There is related work on detecting other
concurrency issues in JavaScript programs such as NRace [39]
or atomicity violations, NodeAV [40]. There is, generally, a
lack of research on tools or techniques to deal with non-
deterministic tests in JavaScript projects. More work is needed
to provide detection tools that target other categories of causes
for test ﬂakiness in JavaScript, such as order dependent, OS
and network ﬂakiness.

35https://jestjs.io/
36https://mochajs.org/

Fig. 3. Distribution of Flakiness Causes on Strategies (the number of commits are shown in parentheses)

V. THREATS TO VALIDITY

Missing ﬂaky tests related commits: We used a keyword-
based approach to locate commits that are ﬂakiness related.
To provide an extended coverage, we used an extended list
of 7 ﬂaky test-related keywords in our search string (ﬂaky,
intermittent, fragile, brittle, ﬂakey, non-deterministic test and
intermit). It
is still possible that we could miss either 1)
commits that used keywords other than the one we identiﬁed in
our research, or 2) ﬂaky tests that have no associated commits
(only reported in the issue tracker but have not been reviewed
or actioned yet).
Generalisation of the ﬁndings: The study considers commits
from JavaScript projects that are publicly available on GitHub.
The observations noted in this study, in terms of the categories
of ﬂaky tests, can be limited to the selected projects and
may not be generalizable to other JavaScript programs. We
mitigated this by selecting popular and highly starred projects
that are managed by big communities and represent various
types of applications (server, frontend/backend frameworks,
web applications, graphics, etc.). Thus, our sample is somehow
representative of real-world programs. Including additional
projects will surely supplement our ﬁndings, and this may lead
to more generalised conclusions.
Manual classiﬁcation of causes: A large part of the study
depends on manual analysis of the data (commit messages
and source code changes), which could affect the construct
validity due to potential personal oversight and bias. In order
to reduce potential false positives, all identiﬁed commits were
independently classiﬁed by two authors (all commits were
separately classiﬁed by the two of the authors), and when there
were disagreements about a certain commit, we introduced a
conﬂict resolution step where a third author was then involved

to also independently classify the disagreed on commits. We
followed that with a rigorous discussion of the causes between
all authors until we reached an agreement (100% agreement
level). There were still issues with a number of commits that
all authors agreed that the cause of ﬂakiness is unknown or
cannot be identiﬁed - those have been classiﬁed as “hard to
classify” and then excluded from the analysis.

VI. CONCLUSION
In this paper, we investigate the presence of ﬂaky tests
in JavaScript projects. We ﬁrst categorise ﬂaky test-related
commits based on the cause of ﬂakiness, and then investigate
the common strategies followed to deal with test ﬂakiness.
Our study shows that
the common causes of ﬂaky tests
in JavaScript are not different from those noted in other
languages - in particular, Java [16] and Python [24]. We found
that 70% of ﬂaky test commits in JavaScript are caused by one
of the following: Concurrency, Async Wait, OS or Network.
Unlike in previous studies [16], [24], which identify test-order
dependency as one of the key causes of ﬂakiness, we found
only very few ﬂaky test commits that are caused by test-
order-dependency. In terms of ﬁxing strategies, we note that
the majority of ﬂaky tests (82%) are ﬁxed to eliminate ﬂaky
behaviour completely. A smaller percentage of those ﬂaky tests
are either skipped (ignored), quarantined (to be ﬁxed later,
becoming a technical debt) or completely removed from the
test suite. These results can help future research on ﬂaky tests,
in particular JavaScript tools designers in building tools that
help to detect and remove ﬂaky tests from test suites.

ACKNOWLEDGMENT
This work was partially funded by a SfTI National Science

Challenge grant No. MAUX2004.

 a) Concurrency  (74) b) Async Wait  (70) c) OS  (66) d) Network  (45) e) Platform  (37)  f) UI  (21) g) Hardware  (17) h) Time  (12) i) Resource Leak  (10) j) Others  (13)  Fix93%Improve4%Quarantine3%Fix84%Improve10%Skip4%Remove2%Fix77%Improve3%Skip14%Quarantine5%Remove1%Fix94%Improve2%Skip2%Remove2%Fix67%Improve3%Skip22%Quarantine3%Remove5%Fix67%Improve3%Skip22%Quarantine3%Remove5%Fix76%Improve18%Skip6%Fix75%Improve17%Quarantine8%Fix90%Quarantine10%Fix100%REFERENCES

[1] M. Fowler, “Eradicating non-determinism in tests,” 2011, https://martin

fowler.com/articles/nonDeterminism.html.

[2] A. Sandhu, “How to ﬁx ﬂaky tests,” 2015.

[Online]. Available:

https://tech.justeattakeaway.com/2015/03/30/how-to-fix-flaky-tests/
[3] J. Palmer, “Test ﬂakiness – methods for identifying and dealing with
ﬂaky tests,” 2019. [Online]. Available: https://engineering.atspotify.co
m/2019/11/18/test-ﬂakiness-methods-for-identifying-and-dealing-with-
ﬂaky-tests/

[4] M. Machalica, A. Samylkin, M. Porth, and S. Chandra, “Predictive
test selection,” in 2019 IEEE/ACM 41st International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP).
IEEE, 2019, pp. 91–100.

[5] W. Lam, A. Shi, R. Oei, S. Zhang, M. D. Ernst, and T. Xie, “Dependent-
test-aware regression testing techniques,” in Proceedings of the 29th
ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2020, pp. 298–311.

[6] B. Vancsics, T. Gergely, and ´A. Besz´edes, “Simulating the effect of test
ﬂakiness on fault localization effectiveness,” in 2020 IEEE Workshop on
Validation, Analysis and Evolution of Software Tests (VST), 2020, pp.
28–35.

[7] Y. Qin, S. Wang, K. Liu, X. Mao, and T. F. Bissyand´e, “On the impact
of ﬂaky tests in automated program repair,” in 2021 IEEE Interna-
tional Conference on Software Analysis, Evolution and Reengineering
(SANER), 2021, pp. 295–306.

[8] J. Micco. Flaky tests at google and how we mitigate them. [Online].
Available: https://testing.googleblog.com/2016/05/flaky-tests-at-google
-and-how-we.html

[9] G. Pirocanac. Test ﬂakiness – one of the main challenges of automated
testing. [Online]. Available: https://testing.googleblog.com/2020/12/tes
t-ﬂakiness-one-of-main-challenges.html

[10] A. Vahabzadeh, A. M. Fard, and A. Mesbah, “An empirical study of
bugs in test code,” in 2015 IEEE international conference on software
maintenance and evolution (ICSME).

IEEE, 2015, pp. 101–110.

[11] A. Labuschagne, L. Inozemtseva, and R. Holmes, “Measuring the
cost of regression testing in practice: A study of java projects using
continuous integration,” in Proceedings of the 2017 11th Joint Meeting
on Foundations of Software Engineering, 2017, pp. 821–830.

[12] A. Gambi, J. Bell, and A. Zeller, “Practical test dependency detection,”
in 2018 IEEE 11th International Conference on Software Testing,
Veriﬁcation and Validation (ICST).

IEEE, 2018, pp. 1–11.

[13] Z. Dong, A. Tiwari, X. L. Yu, and A. Roychoudhury, “Concurrency-

related ﬂaky test detection in android apps,” 2020.

[14] A. M. Memon and M. B. Cohen, “Automated testing of gui applications:
models, tools, and controlling ﬂakiness,” in 2013 35th International
Conference on Software Engineering (ICSE).
IEEE, 2013, pp. 1479–
1480.

[15] A. Romano, Z. Song, S. Grandhi, W. Yang, and W. Wang, “An empirical
analysis of ui-based ﬂaky tests,” in 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE).
IEEE, 2021, pp. 1585–
1597.

[16] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An empirical analysis
of ﬂaky tests,” in Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering, 2014, pp. 643–653.
[17] S. Thorve, C. Sreshtha, and N. Meng, “An empirical study of ﬂaky tests
in android apps,” in 2018 IEEE International Conference on Software
Maintenance and Evolution (ICSME).

IEEE, 2018, pp. 534–538.

[18] A. Sj¨obom, “Studying test ﬂakiness in python projects: Original ﬁndings

for machine learning,” 2019.

[19] S. Technologies, “Why is javascript so popular?” 2018. [Online].
Available: https://web.archive.org/web/20191208013733/https://www.si
mplytechnologies.net/blog/2018/4/11/why-is-javascript-so-popular
[20] I. Lima, J. Silva, B. Miranda, G. Pinto, and M. d’Amorim, “Exposing
bugs in javascript engines through test transplantation and differential
testing,” Software Quality Journal, vol. 29, no. 1, pp. 129–158, 2021.

[21] A. Ahmad, O. Leiﬂer, and K. Sandahl, “Empirical analysis of practition-
ers’ perceptions of test ﬂakiness factors,” Software Testing, Veriﬁcation
and Reliability, vol. 31, no. 8, p. e1791, 2021.

[22] M. Eck, F. Palomba, M. Castelluccio, and A. Bacchelli, “Understanding
ﬂaky tests: The developer’s perspective,” in Proceedings of the 2019
27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, 2019, pp.
830–840.

[23] W. Lam, S. Winter, A. Wei, T. Xie, D. Marinov, and J. Bell, “A large-
scale longitudinal study of ﬂaky tests,” Proc. ACM Program. Lang.,
vol. 4, 2020. [Online]. Available: https://doi.org/10.1145/3428270
[24] M. Gruber, S. Lukasczyk, F. Kroiß, and G. Fraser, “An empirical study
of ﬂaky tests in python,” in 2021 14th IEEE Conference on Software
Testing, Veriﬁcation and Validation (ICST).
IEEE, 2021, pp. 148–158.
[25] W. Lam, K. Mus¸lu, H. Sajnani, and S. Thummalapenta, “A study on
the lifecycle of ﬂaky tests,” in Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering, 2020, pp. 1471–
1482.

[26] S. Dutta, A. Shi, R. Choudhary, Z. Zhang, A. Jain, and S. Misailovic,
“Detecting ﬂaky tests in probabilistic and machine learning applica-
tions,” in Proceedings of the 29th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis. Association for Computing
Machinery, 2020, pp. 211–224.

[27] J. Mor´an Barb´on, C. Augusto Alonso, A. Bertolino, C. A. Riva ´Alvarez,
P. J. Tuya Gonz´alez et al., “Flakyloc: ﬂakiness localization for reliable
test suites in web applications,” Journal of Web Engineering, 2, 2020.
[28] J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov,
“Deﬂaker: Automatically detecting ﬂaky tests,” in 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE).
IEEE, 2018,
pp. 433–444.

[29] W. Lam, P. Godefroid, S. Nath, A. Santhiar, and S. Thummalapenta,
“Root causing ﬂaky tests in a large-scale industrial setting,” in Proceed-
ings of the 28th ACM SIGSOFT International Symposium on Software
Testing and Analysis, 2019.

[30] A. Shi, W. Lam, R. Oei, T. Xie, and D. Marinov, “iﬁxﬂakies: A
framework for automatically ﬁxing order-dependent ﬂaky tests,” in
Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering, 2019, pp. 545–555.

[31] D. Silva, L. Teixeira, and M. d’Amorim, “Shake it! detecting ﬂaky
tests caused by concurrency with shaker,” in 2020 IEEE International
Conference on Software Maintenance and Evolution (ICSME), 2020, pp.
301–311.

[32] Z. Dong, A. Tiwari, X. L. Yu, and A. Roychoudhury, “Flaky test
detection in android via event order exploration,” in Proceedings of the
29th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, 2021, pp.
367–378.

[33] J. Wang, W. Dou, Y. Gao, C. Gao, F. Qin, K. Yin, and J. Wei, “A
comprehensive study on real world concurrency bugs in node. js,” in
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering (ASE).
IEEE, 2017, pp. 520–531.

[34] E. Zhu, “Solving ﬂaky tests in rspec,” 2019. [Online]. Available:

https://ﬂexport.engineering/solving-ﬂaky-tests-in-rspec-9ceadedeaf0e

[35] R. Agarwal, “Handling Flaky Unit Tests in Java,” Jun. 2021. [Online].

Available: https://eng.uber.com/handling-ﬂaky-tests-java/

[36] O. Parry, G. M. Kapfhammer, M. Hilton, and P. McMinn, “A survey of
ﬂaky tests,” ACM Transactions on Software Engineering and Methodol-
ogy (TOSEM), vol. 31, no. 1, pp. 1–74, 2021.

[37] B. Zolfaghari, R. M. Parizi, G. Srivastava, and Y. Hailemariam, “Root
causing, detecting, and ﬁxing ﬂaky tests: State of the art and future
roadmap,” Software: Practice and Experience, vol. 51, no. 5, pp. 851–
867, 2021.

[38] A. T. Endo and A. Møller, “Noderacer: Event race detection for node. js
applications,” in 2020 IEEE 13th International Conference on Software
Testing, Validation and Veriﬁcation (ICST).
IEEE, 2020, pp. 120–130.
[39] X. Chang, W. Dou, J. Wei, T. Huang, J. Xie, Y. Deng, J. Yang, and
J. Yang, “Race detection for event-driven node. js applications,” in
2021 36th IEEE/ACM International Conference on Automated Software
Engineering (ASE).
IEEE, 2021, pp. 480–491.

[40] X. Chang, W. Dou, Y. Gao, J. Wang, J. Wei, and T. Huang, “Detecting
atomicity violations for event-driven node. js applications,” in 2019
IEEE/ACM 41st International Conference on Software Engineering
(ICSE).

IEEE, 2019, pp. 631–642.

