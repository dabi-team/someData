An Exploratory Study on the Predominant Programming
Paradigms in Python Code

Robert Dyer
rdyer@unl.edu
University of Nebraska—Lincoln
Lincoln, NE, USA

Jigyasa Chauhan
jchauhan2@huskers.unl.edu
University of Nebraska—Lincoln
Lincoln, NE, USA

2
2
0
2

p
e
S
5

]
E
S
.
s
c
[

1
v
7
1
8
1
0
.
9
0
2
2
:
v
i
X
r
a

ABSTRACT
Python is a multi-paradigm programming language that fully sup-
ports object-oriented (OO) programming. The language allows writ-
ing code in a non-procedural imperative manner, using procedures,
using classes, or in a functional style. To date, no one has stud-
ied what paradigm(s), if any, are predominant in Python code and
projects. In this work, we first define a technique to classify Python
files into predominant paradigm(s). We then automate our approach
and evaluate it against human judgements, showing over 80% agree-
ment. We then analyze over 100k open-source Python projects,
automatically classifying each source file and investigating the par-
adigm distributions. The results indicate Python developers tend to
heavily favor OO features. We also observed a positive correlation
between OO and procedural paradigms and the size of the project.
And despite few files or projects being predominantly functional,
we still found many functional feature uses.

CCS CONCEPTS
• Software and its engineering → Multiparadigm languages;
Object oriented languages; Functional languages; Imperative lan-
guages.

KEYWORDS
Python, programming paradigms, empirical study

ACM Reference Format:
Robert Dyer and Jigyasa Chauhan. 2022. An Exploratory Study on the
Predominant Programming Paradigms in Python Code. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ’22), November 14–
18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3540250.3549158

1 INTRODUCTION
Python’s popularity has been rising the last few years [6, 34], es-
pecially in fields such as Data Science. The Python language is
often taught as the first programming language to students at many
universities and has been gaining rapid ground in recent years [32].
The Python language is considered a multi-paradigm programming
language, with full support for object-oriented programming. Ev-
ery value in the language is actually an object, with many built-in
methods (use dir() to view them, e.g. dir(1)). The language also
supports several other programming paradigms, to varying degrees,
such as: functional programming (lambda, list comprehensions, etc),

ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
2022. ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3549158

1

imperative programming (if, while, etc), procedural programming
(def), and aspect-oriented programming (mixins).

Despite the language’s popularity, to date no one has investigated
if there are predominant paradigm(s) utilized within Python
code and projects. By predominant paradigm, we mean essentially
how someone would answer the question “what paradigm is this
code?” Due to the mixed nature of Python, almost every file is
technically “mixed” to some degree. However when humans look
at the code, if for example 90% of the code is utilizing procedures,
we would tend to call it “procedural”. Thus procedural would be
the predominant paradigm for such code. In cases where people
might hesitate to confidently answer such a question, those wind
up being labeled “mixed”.

Questions one might ask about the predominant paradigm of
Python code include: Do developers tend to favor object-oriented
or functional features when writing code in Python? Do projects
tend to utilize mostly a single paradigm’s features or features from
many paradigms? Does the size of the project have any relation to
the particular feature(s) it utilizes?

These questions are important to understand as they could help
guide future research. For example, the program comprehension
community is interested in how people understand existing Python
code [35], and knowing what paradigm, if any, is predominant
could help focus those efforts [21] as it may be a confounding
factor they could better control for. Researchers investigating the
maintenance of Python code [9, 38] could benefit knowing the
predominant paradigm, as certain techniques may work better
(or worse) on specific paradigms [21, 38]. Researchers looking at
code smells [8, 28] may want to focus on defining new smells
in the most predominant paradigm(s) or may want to correlate
object-oriented (OO) smells with OO code in Python to see if the
frequencies match. If they don’t, figuring out why Python has
more/less instances of a smell could lead to better language designs
in the future. Educational researchers could benefit from knowing
the predominant paradigm, so when they provide code examples
they can focus on language features new programmers are more
likely to encounter.

In this work, we investigate the use of language features in
the Python language by analyzing over 100k open-source Python
projects from GitHub. To do this, we develop an automated classifi-
cation script using Boa [11, 29]. This script is able to take Python
source files and classify each as functional, object-oriented, pro-
cedural, and/or imperative. We show that this script agrees with
human judgements over 80% of the time, based on a small sample.
We then leverage this script to investigate how often Python source
files (and projects as a whole) utilize each paradigm.

 
 
 
 
 
 
imp

proc
proc
proc

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

class MyNumbers:

x = 1

# func
#

def m(self):
def m3():
return 1

y = m3()
return y

#
#
#
#
#

def __iter__(self): # func

self.x = 1
return self

#
#

def __next__(self): # func

y = self.x
self.x += 1
return y

x = MyNumbers()

#
#
#

#

oo
oo

oo
oo
oo
oo
oo

oo
oo
oo

oo
oo
oo
oo

oo

Figure 1: Python program showing multiple paradigms

Our results show that overall, Python developers tend to favor an
OO paradigm. However, many single-file projects, the often contain
utility scripts, seem to favor a procedural paradigm. Despite the
low number of files and projects classified as functional, we still
observe some functional features as highly used as OO features.

We motivate the need to classify a source file’s dominant para-
digm in the next section. Then we pose several research questions
in Section 3 and outline our approach in Section 4. We then dis-
cuss the results of applying our approach in Section 5. Then in
Section 6 we discuss threats to the validity of the study. In Section 7
we discuss closely related works. Finally, we conclude in Section 8.

2 MOTIVATION
There are many programming paradigms, such as imperative pro-
gramming which includes object-oriented (OO) and procedural
programming, and declarative programming which includes func-
tional, dataflow, reactive, and logic programming. Most modern pro-
gramming languages are actually multi-paradigm, in the sense that
while they typically have a predominant paradigm they support
and emphasize, they also include features from other paradigms.
Python is one example of such a language, as Table 1 shows.

Python is predominantly an OO language, as every value is
represented by an object. For example, if you ask the interpreter
type(3) it tells you the type is <class ’int’>. But while Python
is at its core an OO language, it also supports other paradigms.

This multi-paradigm support often also extends to common
Python libraries. For example, the Django web framework [12] sup-
ports both class-based and function-based (procedural) approaches
for implementing views in the framework. This is partially a result
of the evolution of the framework, but both approaches are sup-
ported today and there seems to be disagreement on which one
people should prefer to use [13, 15].

1Functional features as defined in Python’s official functional how-to guide [18].

2

This can be confusing to new users of the language, as they
are not only learning a new language but might not know how
to utilize some of the paradigms. Classifying such features is also
difficult, as some (such as iterators) exhibit properties of more than
one paradigm. In Figure 1 and the discussion below, we show an
example and explain how we classify those statements.

Imperative Programming
In imperative programming, devel-
opers specify exactly how something should be computed using
statements that modify a program’s state. While procedural and
object-oriented programming are both derivatives of imperative,
here we classify a statement as imperative only if it does not explic-
itly use features from another paradigm. This ignores paradigms
assigned to it from its enclosing scope, which we clarify next.

For example, in Figure 1 while the mention of names (e.g.
MyNumbers and __iter__on lines 1 and 8) is an imperative fea-
ture, because those lines also include class or function declarations
we do not mark them imperative. However, the second line assign-
ing x = 1 is imperative, as that line (ignoring its enclosing scope)
makes no use of other paradigms. After marking that statement in
isolation, we also look at the statement’s enclosing scope and so
we also count it as OO, since it is a member of a class.

Procedural Programming
Procedural programming organizes
code into re-usable elements called procedures (note that Python
calls these functions – in this paper, we call them procedures to
avoid confusion with functional programming features) that can be
called with different arguments. An example procedure is shown
in Figure 1 on lines 4–5, and it is called on line 6.

Note that in this example, the procedure is nested inside a class
method. Declaring a method and declaring a procedure in Python
look very similar, but we consider methods to have a class as its
immediate enclosing scope. This includes class and static methods.

Object-oriented Programming Object-oriented programming
provides abstractions in the form of classes and instances of those
classes called objects. Users can define classes that contain state
(fields) and operations on that state (methods). Instances can be
created and then passed around via reference. And classes support
data hiding in the form of private members.

We consider any declaration of a class, including the entire body
of the class, to be a use of OO in Python. This also includes accessing
(lines 9, 12, and 13) and creating objects (line 15). We also classify
method calls on objects as OO. When classifying the paradigm used,
one should first look at each statement inside the class body and
ignore it exists inside the class. Does it use functional (lines 8 and
11) or procedural (line 4–6) features? If so, it is actually utilizing
multiple paradigms and we classify it as such.

Functional Programming
Python’s official documentation in-
cludes a guide on writing functionally with the language [18]. In this
paper, when we talk about functional Python programming,
this guide is explicitly what we are referring to.

Functional programming is a declarative style of programming
(compared to the prior three, which are all imperative). This means
when writing functional code, developers focus more on describing
what they want the program to do and focus less on how they
want the program to do it. The basic module in functional pro-
gramming is a function. In Python, functions (in the functional

Table 1: Classification of Python language features to programming paradigm(s)

Python Language Feature
if else elif
while loop
break
continue
assert
del
array indexing
pass (inside loop)
pass (inside class)
pass (inside def)
return
function (def)
nested function (def)
class declaration
inheritance
method (def)
with
try
except
finally
raise
for loop
(not) in operator
yield
function-as-arg
lambda functions
list comprehension
decorators
generator expressions
iterators (__next/iter__())
send() (into generator)
iter()
map()
sorted()
filter()
any()
all()
itertools.*()
functools.*()
enumerate()
zip()

Imperative Procedural Object-Oriented

Functional1

x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x

x

x

x
x
x
x
x
x
x
x

x

x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

sense) are lambdas. In functional programming, users create and
compose functions together. Iteration is a core tenant of functional
programming and typically accomplished with recursive functions.
It is important to understand how frequently people are using
functional features in Python, as it helps inform future changes to
the language. There are several Python Enhancement Proposals
(PEPs) accepted or under consideration to add additional func-
tional features, such as the already added generator expression
(PEP 289) [16] and co-routine support for generators (PEP 342) [37],
and the proposed structural pattern matching (PEP 634) [4].

3 RESEARCH QUESTIONS
In this paper, we focus on the following exploratory questions:

RQ1 What is the distribution of programming paradigms
for Python projects on GitHub, for each project and
for each individual file? Is one paradigm used more fre-
quently than the others? How often are files and projects
utilizing multiple paradigms?

RQ2 What are the most and least used features for some
programming paradigms? Are there differences in how

3

language features are used for the most- and least-used
paradigms?

RQ3 Are project size and predominant paradigm related?
Are size metrics like number of committers, commits, files,
or statements related to the predominant paradigm?
RQ4 How does programming paradigm use change over
time? Do files change their predominant programming par-
adigm over time? What are common first/last predominant
paradigms?

Next, we outline our approach to answer these questions.

4 APPROACH
In this section, we outline the approach used to answer the posed
research questions. First, we discuss the tool and dataset used for
our analyses. Next, we discuss a manual classification of Python
projects to programming paradigms. Finally, we discuss an auto-
mated solution and compare with the manual approach.

4.1 Tools and Dataset
To answer the research questions, we needed a large number of
Python projects from many different domains. We had originally
looked at utilizing a Boa [29] dataset containing Python projects [3],
however that data was limited to data-science related projects and
we wanted a broader dataset.

As such, we opted to utilize Boa’s open-source compiler infras-
tructure [11, 33] to build our own dataset of Python projects. We
utilized the public GitHub API [14] to query for any project indi-
cating Python as the primary programming language and sorted
based on stargazer count. We then cloned the results and built a
dataset. Table 2 shows some statistics for that dataset.

Table 2: Statistics for the Python data analyzed in this paper

Projects
All Revisions
Revisions (with a Python file)
Python Files

Python Files (main branch)

Python File Snapshots
ASTs

101,648
32,197,017
15,254,331
7,758,882
3,658,391
68,787,597
105,907,774,611

The dataset seemed to contain a large number of files with du-
plicate ASTs (ignoring comments and whitespace differences), so
we identified the duplicates (found by converting the AST into
JSON format, then hashing that string similar to Lopes et al. [20])
and keep only one instance from each set. We removed a total of
1,289,982 duplicate files (14.26% of all files). All numbers shown in
this paper are already deduplicated.

The dataset contains over 100k projects. There are over 32 mil-
lion revisions, half of which contain Python files, and 7.5 million
unique Python source files comprising over 100 billion abstract
syntax tree (AST) nodes. Table 3 shows the per-project statistical
distribution for various size metrics, showing the median project
has 47 revisions, 52 files with 4,906 statements and 35k AST nodes.

4

Table 3: Per-project statistics for the Python dataset

Revisions

Python Files

Statements

ASTs

mean
std
min
25%
50%
75%
max

317
1,735
1
15
47
162
203,889

627
3,757
1
15
52
225
306,786

145,678
1,189,335
0
1,199
4,906
26,901
72,578,872

1,041,907
8,689,738
3
8,610
35,264
191,125
623,549,307

For most analyses, we only example the latest snapshot of the
main branch. For RQ4 and part of RQ3 (that involve committers or
commits), we also examine the full history of each file.

For analysis, we then query this data using the Boa language.
The output of those queries is a specific text format, which we then
converted to CSV. We finally process the CSV files using Python
scripts and the Pandas library. All data, queries, and scripts are
provided in our replication package [10].

4.2 Manual Paradigm Classification
First, we need the ability to classify Python files into the program-
ming paradigm(s) used. To do this, we took a sample from the
previously published Boa Python dataset [3] containing a total of
98,537 source files. From those files (sorted by project URL, then
filename), we systematically sampled every 985𝑡ℎ yielding 102 files.
Some of the projects/files no longer exist on GitHub, so if we
found such a case we would simply move to the next file until
we found one that still existed. This resulted in a sample size of
102 source file URLs, giving us a confidence level of 95% with a
confidence interval of +-9.7%.

We then had three people from our research lab (including the
authors) perform a manual classification task. The task was: given
a source file (a URL to view it on GitHub), indicate the predominant
programming paradigm(s) you see this file as using. The raters
were explicitly told to ignore imports (we discuss this more in a
later section), and were allowed to mark zero, one, or more than
one paradigm. The goal was not to look and see if a single use of a
paradigm occurred, but rather to answer the question “would you
call this file functional/OO/procedural/imperative?” Each human
rater was unaware of the judgements from the other two raters at
the time of rating.

From this data we computed Fleiss’ kappa to measure the inter-
rater agreement level. Fleiss’s kappa was used over Cohen’s kappa,
as there were more than two raters. The result was a kappa value
of 0.717. This is often interpreted as “good” agreement among the
raters (with 0.81 being “very good”).

After all three raters finished their judgements, a discussion
round was held. The discussion focused only on files where there
was not unanimous agreement. Discussion was ordered based on
paradigms, meaning all files where the judgement on functional
was not 100% agreement were discussed, then all files for OO, pro-
cedural, and finally imperative. When discussing, the person who
had disagreed gave a case for why they rated the file either as or

not as that particular paradigm. Then all three raters were free to
update their score. This repeated for all disagreements.

Table 4: Human and machine judgements on a sample

Human Judgements Machine Judgements

Imperative
Mixed
OO
Procedural

5
16
53
28

8
10
55
29

After discussion, we then re-calculated the Fleiss’s kappa and
found a value of 0.909, meaning “very good” inter-rater agreement
was reached. This final set of judgements (shown on the left of
Table 4) were then used to help calibrate our automated approach,
which we describe next.

4.3 Automatic Paradigm Classification
Once we had some manual judgements on a sample of data, we were
able to more clearly see some of the features that influenced the hu-
man judgements. We first listed as many syntactical (and some API)
features from Python that might indicate a particular programming
paradigm. The results of this classification were already shown in
Table 1. Next, we discussed and assigned each feature in the table
to a single, predominant paradigm (our goal was a 1:1 assignment,
which was not always possible).

Based on this classification, we wrote a Boa [11, 29] query to
classify each statement into one or more paradigms. The goal was
to map each statement (approximately) based on the classification
in the table. So for example, the following code:

o.setResult(list(map(lambda x: x**3, [1, 2, 3])))

would be analyzed and generate the following counts:

{ 1, 1, 1, 0, 1 }
that indicate there was 1 use of functional features (the map()
call and the lambda), 1 use of OO features (calling setResult on
object o), 1 use of procedural features (the list() call), 0 uses of
imperative features, and 1 total statement. While some numbers
might intuitively seem wrong, we explain how we obtain them.

Writing this script required several important decisions. First,
some of the language features being identified are expressions, and
we were aiming to classify at the statement level (as an analog
to line numbers, which Boa does not contain). Thus, we had to
introduce a function to clamp the counts at the statement level as
a single statement might contain 2 (or more) paradigm uses. This
ensured that a paradigm was counted at most once per statement.
Second, Boa simply parses the source code and provides abstract
syntax trees (ASTs). It does not attempt to resolve types or pro-
vide type information for analysis. This complicates several of the
mappings. To get around this issue, we first track all imports in the
file to help see what names might be a module. We also analyze
the imports to see specifically if a function was imported from
the functools or itertools modules, as we classify all of those
functions as functional paradigm.

Third, we had to attempt to assign calls of the form expr.m() as
either procedural or OO. If expr is an object, this is an OO method

5

call on that object. If expr is a module, it is a normal procedural
call. We utilize the imports to aid this process.

Fourth, we decided to treat imperative a bit special. We found that
almost every line of code, at least in Python, seems to contain at least
one imperative feature (such as using a name, assignment, or simple
expression arithmetic). As such, many of the imperative counts
were nearly identical to the statement counts. We decided to make
a simple change that stated if the statement directly used another
paradigm, do not increment imperative. Thus the imperative counts
we obtain are counts of statements that are fully imperative.

Fifth, we had to decide how to handle large blocks. For example,
when a class is declared how should we count its body? Clearly the
line declaring the class is OO. We decided the entire body should
contribute to OO, regardless of what the individual statements did.
But we did have to handle some corner cases, such as what do we
do with a function nested inside of a method? In that case, it counts
as both procedural and OO as it is clearly a procedural method (it
does not take self as an argument), but as it is contained within a
class’s method it is contributing to the functionality of that method
and thus is part of the OO nature of the file.

Finally, we had to decide how to handle imports in terms of
contributing toward counts. It was clear these should count as
statements toward a file’s total statement count. What was less clear
was if it should contribute toward counts for the paradigms. Part of
the issue was that with the lack of resolved types, we are not usually
certain what item is being imported. And in the case of importing
a module, should we analyze the whole module to determine what
paradigms that used? What if it was an external library? These
issues led us to decide to only count them as statements, and ignore
any potential paradigm count(s).

4.3.1 Accuracy. To verify the accuracy of our automated approach,
we first built an oracle repository of hand-coded Python files con-
taining various features and named them with the counts we ex-
pected. We then built a local Boa dataset from only that repository
and used it as a test-case to identify and fix issues. We also added
regression tests to the repository as we found interesting cases that
seemed to give weird results.

We then verified the results in comparison to the human judge-
ments described in the previous section. Here we used the same
sample of 102 files and had the machine classify the file’s paradigm.
The goal was to take a file’s counts and label it with a single la-
bel (either a paradigm, or “mixed”). We tried various strategies, all
variants of taking the largest number. Ties or all 0 values result
in mixed labels, regardless of strategy. We set cutoffs, requiring
that the winning value must be a certain percentage of the file’s
statements and tried cutoffs of 0%, 50%, 70%, and 90%. We then
looked at the performance compared to the human judgements.

Surprisingly, the cutoff at 0% performed the best - meaning the
machine simply looked at the largest number and as long as there
were no ties, that was declared the winner. This agreed with the
humans on 82.35% of the 102 projects. When we computed the
Cohen’s kappa to see how well this rating agrees with the human
ratings (here it is a single set of values based on majority agreement)
we saw a kappa of 0.717, meaning “good” agreement.

4.3.2 Classifying Entire Projects. The approach described so far
only classifies a single file into a paradigm. We also need the ability

to classify entire projects into a paradigm. For this task, we utilize
the counts generated from the prior tool.

First, for each project we add the counts across all their files
in the latest snapshot of the main branch. This gives us a total
count across all files, which is what we utilize to classify the project.
Unlike the prior approach however, we do not simply go with the
largest value wins.

Similar to classifying individual files, if there is a tie for first or if
the counts somehow are all 0 we would mark the project as mixed.
For the other cases, we utilized the following algorithm:

if second > 2/3 or largest < 1/3:

# case 1

return 'mixed'

if second > 0.5 and largest - second < 0.2:

# case 2

return 'mixed'

if largest <= 0.5 and largest - second < 0.1:# case 3

return 'mixed'

1

2

3

4

5

6

While the authors did not perform a full sensitivity analysis to deter-
mine the cutoff points, we did try several variations and picked the
best performing values based on our intuitions, the data observed,
and lengthy discussion.

Note that it is possible for more than 50% of the statements in
a file to be classified to more than one paradigm. This is due to
our classification technique, where we consider each statement and
classify what paradigm(s) it uses. For example, in Figure 1 on line 4
we classify that statement as both OO and procedural. Depending
on the statements, it is possible the entire file is classified as up
to 100% into multiple paradigms. Thus we needed to consider the
edge cases shown in the algorithm above.

Here largest is the largest count (by percentage of total state-
ments) and second the second largest. When discussing the strategy,
the authors quickly realized that some cases were easy to distin-
guish. For example, if two (or more) paradigms account for at least
2/3 of the statements then this project is clearly mixing paradigms.
Similarly if the most used paradigm only accounts for 1/3 of the
total statements, it was also mixed. These are the first case.

The second case was if the two highest used paradigms are both
over 50%, and within 20% of each other then we also considered it
mixed. Finally, the third case was if the most used paradigm is less
than half the statements and within 10% of the second-most used
paradigm, we considered it mixed.

5 EXPLORATORY STUDY
In this section, we discuss the results of analyzing the Python
dataset based on the research questions posed. We also discuss
the important findings discovered during analysis and what the
implications are of those findings to the community.

5.1 RQ1: What is the distribution of

programming paradigms in Python?

The first question we want to investigate is simply what the distri-
bution of programming paradigms is for Python files and projects.
To answer this question, we first look at each individual file. We
only consider files from the repository’s main branch. The results
are shown in Figure 2 as both a box plot and a summary table.

6

First we look at the statistics on the number of statements in a
file. The median value is 37 statements per file, but there is a lot
of variance from 0 (typically empty init files) up to 858k, but most
files are relatively smaller. Next we look at the statistics on the
paradigm counts for files. Here we can see the median values for
OO and imperative are the highest, with procedural having a lower
median value but otherwise performing similarly to imperative.

Next we look at the files in terms of what percent paradigm
each file is. Figure 3 shows the results. Here it becomes a bit more
obvious that, at the file level, OO seems to really dominate the files
with the median percentage of a file being around 40% OO and 30%
imperative. Functional is the least percentage of files.

Finally we use our automated approach to classify each file to a
particular paradigm. The results are shown in Figure 4. Here we
introduce the new category called “mixed”, meaning at least two
paradigms dominate the file. Once the script starts classifying files,
we can see a clear lead for OO with almost 55% more files classified
OO compared to the second highest (procedural).

Finding 1: Overall, it appears that Python files are predominately writ-
ten in an OO style.

5.1.1 Classifying Projects. Next we investigated how projects as a
whole classify in terms of paradigm. Table 5 shows the results.

As you can see, procedural and OO are the most frequently used
paradigms when classifying whole projects. Given the individual file
classifications, this is not too surprising, though imperative being
lower than mixed is a bit surprising. However, this is explained by
the strategy used to classify projects being slightly different from
files and having more opportunities to mark projects as mixed.

Table 5: RQ1: Classification of projects

all projects

no toy projects

1-file projects

classified

Functional
OO
Procedural
Imperative
Mixed

1
45,372
33,059
5,768
17,052

1
39,597
26,652
4,712
14,149

0
497
1,176
310
387

There is also a possible threat that including “toy” projects might
bias the results somehow. When designing the approach, we dis-
cussed the idea of filtering out small projects with little history. We
decided to keep them in however, as the questions we are asking are
about Python projects as a whole – including these so-called toys.
That said, we did look into how things might change if we excluded
them. The results are shown in the middle column of Table 5. Here
we removed any project with a revision count less than 10.

When comparing these results to the full dataset results on the

left, we see removing toy projects did not change the trends.

Finding 2: When considering projects (with or without toys excluded),
almost no functional projects are found and OO is the most com-
mon paradigm used (over 40% of all projects) followed by proce-
dural (over 30% of all projects).

Statements

Functional

OO Procedural

Imperative

mean
std
min
25%
50%
75%
max

100.10
969.49
0.00
12.00
39.00
101.00
858,430.00

5.58
38.57
0.00
0.00
1.00
5.00
38,651.00

56.31
583.45
0.00
2.00
12.00
48.00
769,163.00

34.81
152.59
0.00
1.00
8.00
32.00
74,194.00

34.13
762.44
0.00
2.00
11.00
31.00
858,430.00

Note: the boxplot has outliers removed while the table is a summary of the
raw data, including outliers.

Figure 2: RQ1: Distribution of statements in every unique file (exact duplicates removed) and their assigned classifications

We also looked at cases where a project has just a single source
file. The original thought was these might be toys, possibly “hello
world” style examples where people were simply trying Python out
for the first time. The results are shown in Table 5. The results show
a mixture of paradigms. A manual inspection of a small sample of
procedural files showed quite a few were utility scripts.

Finding 3: Small (single-file) Python projects are predominately written
in a procedural style, and often serve as utility scripts.

5.2 RQ2: What are the most and least used
features for some programming
paradigms?

When looking at the results of the previous research question, it be-
comes obvious that some paradigms are highly used in Python (OO,
Procedural) and other paradigms are extremely rarely used (Func-
tional), according to the classification of both files and projects.

Given these results, we were interested in seeing how one of the
top paradigms, such as OO2, compares to the lowest (Functional) in
terms of the use of their individual features. In other words, given
that so many projects and files use OO and so few use Functional,
would we also see that functional features are not often used?

We ran a query to find and count how many times the functional
and OO features (from Table 1) were used in the main branch
with duplicate files removed. The results are shown in Table 6. OO
features are highlighted with gray backgrounds.

Given the huge discrepancy between OO and functional in terms
of number of files and projects (Figure 4 and Table 5), and also given
the much lower file percentages that are functional compared to
OO (Figure 3) we expected to see substantially more use of OO
features compared to functional. However, the results clearly show
that even if files and projects are not being classified as functional,
functional code is still frequently used across Python projects.

Finding 4: Despite the low number of files being classified as functional,
functional features are still commonly used in Python.

2We chose OO over Procedural as OO has many more features to look at.

7

Figure 3: RQ1: percentage of a file that is each paradigm

Figure 4: RQ1: Classification of files to paradigm

Figure 5: RQ3: Is there a relationship between the number of statements in a file and its predominant paradigm? Here the
x-axis is the number of statements in a file, binned into deciles.

Table 6: RQ2: Counts for functional and object-oriented pro-
gramming Python features

feature

method declarations
class declarations
class inheritance
for-each
built-in functions (functools/itertools)
array comprehensions
try
except
in
method decorators
raise
with
not in
lambda
higher-order functions
yield
class decorators
finally
generators
iterable

count

2,575,397
1,738,668
1,510,161
1,405,144
990,333
729,309
713,770
693,096
687,249
654,500
634,313
559,910
355,880
300,227
205,703
128,095
90,144
83,519
40,931
4,323

5.3 RQ3: Are project size and predominant

paradigm related?

Next we were interested to see if a project’s size has an relation-
ship to the programming paradigm it uses. For this question, we
consider different metrics for measuring a projects size, including
total number of committers, number of revisions, and total number
of statements in all source files.

First, we look at the number of statements. The results are shown
in Figure 5. We use a histogram to display the values, with data

8

Figure 6: RQ3: Number of committers vs paradigm

placed into bins at each decile. Since the maximum values for func-
tional are considerably smaller compared to the other paradigms,
we split it out into its own chart on the left.

What we are looking for in these histograms is if the trends seem
similar. In other words, as the project size increases (going right
across the x-axis) do we see a similar trend for each paradigm?
What we see are different trends: decreasing, increasing, and no
trend. As the project size is increasing, it appears that the number
of files classified as OO or imperative increases showing a positive
correlation. Meanwhile, the number of procedural and mixed files
decrease. And interestingly, the number of functional files seems
completely uncorrelated.

Finding 5: The number of statements in a project seems positively cor-
related to using OO or imperative paradigms, and negatively correlated
to using procedural or mixed paradigms.

The analysis on the number of committers (Figure 6) does not
show any correlation, most likely due to the extremely skewed
data as most projects only have a few committers. Similarly no
correlation was observed when analyzing number of revisions,
possibly again due to the skewed nature of the data. We omit these
results from the paper, but they are included in the dataset.

Finding 6: The number of committers and a project’s revision history
have no correlation to the project’s dominant paradigm.

Table 8: RQ4: For files that changed classification: how do
they change?

5.4 RQ4: How do programming paradigm uses

change over time?

The final research question is looking at how the paradigm choice
of a file changes over time, if at all. For this we look at the first
and last revision of each unique file on all branches and see if the
classification changes or not. The results are shown in Table 7.

Table 7: RQ4: Do files change their classification from their
first revision to their last?

files

changed?

False
True

52,535,151
8,493,564

As you can see, a small percentage of files (13.8%) change clas-
sification over time. If a file is using a particular paradigm, the
probability of it continuing to use that paradigm is high. This is
good news for developers, as if they are comfortable with the para-
digm features used in a file they shouldn’t worry about having to
learn a new paradigm’s features in the future.

We also looked at the 8M files that changed classification to see
if there were any trends in how those changes occurred. The results
are shown in Table 8, with counts indicating how many files were
classified as different paradigms in their first and last versions.

In Table 9 we also show the file counts for a file’s first and last
classification. OO was the most common first and last classification,
with imperative close behind.

We do not see any obvious trends in the data.

Finding 7: A small number of files (13.8%) change their classification
over time, meaning most files pick a paradigm and stick with it.

6 THREATS TO VALIDITY
Here we consider some potential threats to the validity of our study.
The biggest threat to construct validity is that we rely on infer-
ring paradigm use from already produced code, without speaking
with the developers of the project. So while we may judge a project
to be OO based on the analyzed use of various language features, it
is possible the developers of the language consider it to be different,
e.g. procedural or mixed. A follow-up study surveying developers
based on the results of this study could help confirm or deny that
the use of features alone is sufficient to make such a judgement.

The biggest threats to internal validity are the human judge-
ment’s (Section 4.2) and the classification of features to paradigm
(Table 1). Not everyone will agree with how some of the features
are mapped. We attempted to pick an objective mapping, especially
for the functional features, by using guidelines from the language
designers themselves [18].

The human judgement’s served as the basis for calibrating the au-
tomated approach, so if those judgements were poor the automated

first

last

Functional OO

OO

Procedural

Imperative

Mixed

Procedural
Imperative
Mixed
Functional
Procedural
Imperative
Mixed
Functional
OO
Imperative
Mixed
Functional
OO
Procedural
Mixed
Functional
OO
Procedural
Imperative

files

5,540
3,991
5,305
5,931
5,677
807,967
973,349
739,926
3,976
806,868
612,296
441,578
5,210
972,512
613,026
651,935
5,901
737,745
441,416
653,415

Table 9: RQ4: First and last file paradigms

files

files

first

Functional
Imperative
Mixed
OO
Procedural

20,767
2,242,683
1,838,477
2,526,919
1,864,718

last

Functional
Imperative
Mixed
OO
Procedural

20,764
2,244,365
1,839,370
2,522,665
1,866,400

approach likely also performs poorly. While this is a threat, we feel
the high inter-rater agreement (0.9) indicates the judgements were
high quality and helps mitigate this threat. However we note the
initial agreement was lower (0.7) and after computing the Cohen’s
kappa to see how well the automated approach agreed with the
humans, the human raters started questioning some of their own
prior judgements based on the differences from the automated tool.
For example, in one source file3 the humans judged it as OO and
the machine called it mixed. We suspect it was because there was
a single statement spanning 9 lines, where each line contained a
method call on an object. Humans probably interpreted this visually
large block of text as several OO uses while the machine would
count this as a single use.

3https://github.com/13o-bbr-bbq/machine_learning_security/blob/
08ecb7eea2645a1cecafe21370423fb7393d3bee/Analytics/analyze_kmeans.py#L18

9

In another source file4, there were several lines containing code
like cfradial._ncvar_to_dict(ncvars['lat']). It can be difficult
(for both humans and the script!) to determine if this is an OO
method call or a procedural call, without global knowledge. A non-
local static analysis would make this result more accurate.

This shows that some projects most likely fall into a gray area,
where even humans would most likely disagree on exactly what
to classify it as, while the automated tool has to make (sometimes
seemingly arbitrary) deterministic choices. In the future we might
utilize machine learning to train a classifier, however currently we
lack enough human judgements to build an accurate model.

Another internal threat to validity deals with how the set of
features in Python has changed over time. As such, some features
have existed longer than others and this may impact some of the
observed results (e.g., the feature counts in Table 6).

A threat to external validity deals with how our results generalize.
While we attempted to utilize a broad range of GitHub projects, we
still can’t say with certainty those projects are representative of all
Python programs/projects and thus the results might not generalize
to the population as a whole. Additionally, the study focused purely
on Python, and results almost certainly do not generalize to other
programming languages.

7 RELATED WORKS
In this section we discuss prior works that study the Python lan-
guage, studies about specific language features, and studies on the
use of multiple programming languages.

7.1 Studies on the Python Language
Peng et al. [26] analyzed 35 open-source Python projects to see
what language features are commonly used. They found inheri-
tance, decorators, keyword arguments, for loops, and nested classes
as the top used language features. Orrú et al. [25] looked at the use
of inheritance in 51 Python programs. They observed Python pro-
grams have more classes inherited from compared to Java programs.
In contrast to these works, we analyzed almost 100k projects, but
with a different goal in mind. Still, similar to these works, we also
found that inheritance, for loops, and decorators are commonly
used language features.

Åkerblom and Wrigstad [31] looked at the use of polymorphism
in 36 Python projects by collecting dynamic traces of the programs.
Their results showed that while many projects use polymorphism,
most are actually monomorphic. Åkerblom and Wrigstad [1] looked
at 36 Python programs to measure their polymorphic behavior. They
observed many systems, while heavily using polymorphism, are
actually monomorphic in behavior. While these works focuses on
the use of inheritance and polymorphism, we look at language
features used (including OO inheritance) from a static standpoint
and utilize that information for classifying the paradigm(s) used.
Åkerblom et al. [30] used dynamic program tracing to investigate
the use of some language features of Python that are difficult to
analyze statically, such as dynamically generated code (eval/exec).
While their work looks at specific language features, they are not

4https://github.com/ARM-DOE/pyart/blob/7584a0b3abe357caa9dcd4b0b50adacffe4af2e0/
pyart/aux_io/arm_vpt.py#L97

10

attempting to categorize programming paradigms those features
belong to or determine the predominant paradigm.

Alexandru et al. [2] looked at how developers perceive common
idioms in Python, known as the “Pythonic” way of solving a prob-
lem. As some idioms might imply a particular paradigm, their work
is complementary to our own. We are not looking at prescribed
ways to write code, just observing what was already done.

Lin et al. [19] conducted an empirical study using PyCT focused
on 77 kinds of source code changes made by developers. Out of
four research questions, one of the questions focused on how often
do dynamic features change in source files. While we do not look
at dynamic features, we did investigate how often predominant
paradigm changes in source files.

7.2 Language Feature Studies on Other

Languages

Several other studies have looked at the use of language features
(often lambdas) in other programming languages (often Java). For
example, Mazinanian et al. [23] looked at where developers were
using lambdas in Java and then surveyed them to find the reasons
for using them. They observed increased use as well as different
reasons from increased readability to simulating lazy evaluation.
Lucas et al. [21] looked at if the introduction of lambdas into Java
helped with programmer comprehension. They found contradictory
results that lambdas seem to improve program comprehension
while simultaneously decreasing readability.

Nielebock et al. [24] looked at the use of lambda expressions to
aid writing concurrent object-oriented code in Java, C++, and C#
and found that in general, programmers appear to not favor the
use of lambdas in the context of concurrent code.

Petrulio et al. [27] looked at the support for lambdas in the Java
ecosystem and found many top APIs do not yet support functional
interfaces. Zheng et al. [39] looked at why developers remove exist-
ing lambdas in Java. They found seven common reasons to remove
lambdas, including reasons such as readability and performance.
They also recommended places to avoid introducing lambdas.

These works focused on a single language feature, while our
study focused on identifying the predominant paradigm. Our work
could help focus future studies by indicating which paradigm(s) are
most common and thus which feature(s) might be best to study.

7.3 Multi-language Studies
Kochhar et al. [17] studied the relationships between multi-
language usage and bug categories. They observed if the use of
multiple languages causes more bugs. They built regression models
to study the correlation of using different languages on the number
of bug fixing commits. They noted that languages, when used with
other languages, can make software more bug prone.

Mayer and Bauer [22] investigated the use of multiple program-
ming languages in 1k open-source projects. They used association-
rule mining to infer how project size and number of commits related
to the languages used. They also discovered several groupings of
language ecosystems.

Uesbeck and Stefik [36] addressed the issues programmers face
while using multiple programming languages. They performed a
control study to determine what kinds of problems programmers

face when switching between multiple programming languages,
such as Java and SQL. The study was small and designed as a pilot
for future studies.

Bunkerd et al. [5] investigated 50 Python projects to see if the his-
tory of developers on those projects and their experience with other
programming languages affects the naturalness of their Python
code. They show that greater diversity of contributor programming
experience can impact and make the code less natural.

Chakraborty et al. [7] studied Q&A on Stack Overflow for three
languages: Go, Swift and Rust. They monitored the challenges de-
velopers initially faced while working with new languages, in com-
parison to more mature languages. They conducted the study by
extracting features, understanding the developer’s background as a
contributor. The study promoted better design for languages and
documentation by the sponsors/owners.

These works study the use of multiple languages, while we focus

on the use of multiple paradigms within a single language.

8 CONCLUSION
Python is a multi-paradigm programming language, but to date no
one has investigated what paradigms are predominant in Python
code. In this work, we saw that many files and projects favor the
OO paradigm. Single-file projects appear to be utility scripts and
favor a procedural paradigm. The size of the project in terms of
statements is positively related to using OO, and negatively related
to using functional, procedural, or mixed paradigms. Finally, we
saw the vast majority of files rarely change predominant paradigm
over time, providing stability to developers working on those files.
In the future we hope to have a follow-up study to investigate in
more detail exactly how humans classify the predominant paradigm
of a file. While it was easy for humans to classify results that are
polarized, the middle area was much more difficult. We would like
to perform a survey to investigate why that was the case.

ACKNOWLEDGMENTS
We thank Samuel W. Flint for helping brainstorm ideas and labeling
some of the data.

DATA AVAILABILITY
The Boa queries and their outputs, human judgements, and process-
ing scripts are available in a replication package [10] on Zenodo.

REFERENCES
[1] Beatrice Åkerblom and Tobias Wrigstad. 2015. Measuring polymorphism in
Python programs. In Proceedings of the 11th Symposium on Dynamic Languages.
114–128.

[2] Carol V. Alexandru, José J. Merchante, Sebastiano Panichella, Sebastian Proksch,
Harald C. Gall, and Gregorio Robles. 2018. On the Usage of Pythonic Idioms. In
Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas,
New Paradigms, and Reflections on Programming and Software (Boston, MA, USA)
(Onward! 2018). Association for Computing Machinery, New York, NY, USA, 1–11.
https://doi.org/10.1145/3276954.3276960

[3] Sumon Biswas, Md Johirul Islam, Yijia Huang, and Hridesh Rajan. 2019. Boa
Meets Python: A Boa Dataset of Data Science Software in Python Language. In
MSR’19: 16th International Conference on Mining Software Repositories (Montreal,
Canada). 577–581.

[4] Brandt Bucher and Guido van Rossum. 2021. PEP 634 – Structural Pattern

Matching: Specification. https://www.python.org/dev/peps/pep-0634/.

[5] Thanadon Bunkerd, Dong Wang, Raula Gaikovina Kula, Chaiyong Ragkhitwet-
sagul, Morakot Choetkiertikul, Thanwadee Sunetnanta, Takashi Ishio, and
Kenichi Matsumoto. 2019. How Do Contributors Impact Code Naturalness?

11

An Exploratory Study of 50 Python Projects. In 2019 10th International Work-
shop on Empirical Software Engineering in Practice (IWESEP). 7–75.
https:
//doi.org/10.1109/IWESEP49350.2019.00010

[6] Pierre Carbonnelle. 2021. PYPL PopularitY of Programming Language. https:

//pypl.github.io/.

[7] Partha Chakraborty, Rifat Shahriyar, Anindya Iqbal, and Gias Uddin. 2021. How
do developers discuss and support new programming languages in technical Q&A
site? An empirical study of Go, Swift, and Rust in Stack Overflow. Information
and Software Technology 137 (2021), 106603. https://doi.org/10.1016/j.infsof.2021.
106603

[8] Zhifei Chen, Lin Chen, Wanwangying Ma, and Baowen Xu. 2016. Detecting Code
Smells in Python Programs. In 2016 International Conference on Software Analysis,
Testing and Evolution (SATE). 18–23. https://doi.org/10.1109/SATE.2016.10
[9] Zhifei Chen, Yanhui Li, Bihuan Chen, Wanwangying Ma, Lin Chen, and Baowen
Xu. 2020. An Empirical Study on Dynamic Typing Related Practices in Python
Systems. Association for Computing Machinery, New York, NY, USA, 83–93.
[10] Robert Dyer and Jigyasa Chauhan. 2022. Replication package for "An Exploratory
Study on the Predominant Programming Paradigms in Python Code". https:
//doi.org/10.5281/zenodo.6975558

[11] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2013.
Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software
Repositories. In Proceedings of the 2013 International Conference on Software
Engineering (San Francisco, CA, USA) (ICSE ’13). IEEE Press, 422–431.

[12] Django Software Foundation. 2021. Django: The web framework for perfectionists

with deadlines. https://www.djangoproject.com/.

[13] Vitor Freitas. 2021.

Class-Based Views vs. Function-Based Views.
https://simpleisbetterthancomplex.com/article/2017/03/21/class-based-views-
vs-function-based-views.html.

[14] GitHub, Inc. 2021. GitHub REST API. https://docs.github.com/en/rest.
[15] D.R. Greenfeld and A.R. Greenfeld. 2017. Two Scoops of Django 1.11: Best Practices

for the Django Web Framework. Two Scoops Press.

[16] Raymond Hettinger. 2003. PEP 289 – Generator Expressions. https://www.python.

org/dev/peps/pep-0289/.

[17] Pavneet Singh Kochhar, Dinusha Wijedasa, and David Lo. 2016. A Large Scale
Study of Multiple Programming Languages and Code Quality. In 2016 IEEE 23rd In-
ternational Conference on Software Analysis, Evolution, and Reengineering (SANER),
Vol. 1. IEEE, 563–573. https://doi.org/10.1109/SANER.2016.112

[18] A. M. Kuchling. 2021. Functional Programming HOWTO. https://docs.python.

org/3/howto/functional.html.

[19] Wei Lin, Zhifei Chen, Wanwangying Ma, Lin Chen, Lei Xu, and Baowen Xu. 2016.
An Empirical Study on the Characteristics of Python Fine-Grained Source Code
Change Types. In 2016 IEEE International Conference on Software Maintenance
and Evolution (ICSME). 188–199. https://doi.org/10.1109/ICSME.2016.25
[20] Cristina V. Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny,
Hitesh Sajnani, and Jan Vitek. 2017. DéjàVu: A Map of Code Duplicates on
GitHub. Proc. ACM Program. Lang. 1, OOPSLA, Article 84 (oct 2017), 28 pages.
https://doi.org/10.1145/3133908

[21] Walter Lucas, Rodrigo Bonifácio, Edna Dias Canedo, Diego Marcílio, and Fer-
nanda Lima. 2019. Does the Introduction of Lambda Expressions Improve the
Comprehension of Java Programs?. In Proceedings of the XXXIII Brazilian Sym-
posium on Software Engineering (Salvador, Brazil) (SBES 2019). Association for
Computing Machinery, New York, NY, USA, 187–196. https://doi.org/10.1145/
3350768.3350791

[22] Philip Mayer and Alexander Bauer. 2015. An Empirical Analysis of the Utilization
of Multiple Programming Languages in Open Source Projects. In Proceedings
of the 19th International Conference on Evaluation and Assessment in Software
Engineering (Nanjing, China) (EASE ’15). Association for Computing Machinery,
New York, NY, USA, Article 4, 10 pages. https://doi.org/10.1145/2745802.2745805
[23] Davood Mazinanian, Ameya Ketkar, Nikolaos Tsantalis, and Danny Dig. 2017.
Understanding the Use of Lambda Expressions in Java. Proc. ACM Program. Lang.
1, OOPSLA, Article 85 (oct 2017), 31 pages. https://doi.org/10.1145/3133909
[24] Sebastian Nielebock, Robert Heumüller, and Frank Ortmeier. 2019. Programmers
Do Not Favor Lambda Expressions for Concurrent Object-Oriented Code. Empiri-
cal Software Engineering 24, 1 (feb 2019), 103–138. https://doi.org/10.1007/s10664-
018-9622-9

[25] Matteo Orrú, Ewan Tempero, Michele Marchesi, and Roberto Tonelli. 2015. How
Do Python Programs Use Inheritance? A Replication Study. In 2015 Asia-Pacific
Software Engineering Conference (APSEC). IEEE, 309–315. https://doi.org/10.1109/
APSEC.2015.51

[26] Yun Peng, Yu Zhang, and Mingzhe Hu. 2021. An Empirical Study for Common
Language Features Used in Python Projects. In 2021 IEEE International Conference
on Software Analysis, Evolution and Reengineering (SANER). 24–35. https://doi.
org/10.1109/SANER50967.2021.00012

[27] Fernando Petrulio, Anand Ashok Sawant, and Alberto Bacchelli. 2021. The
indolent lambdification of Java Understanding the support for lambda expressions
in the Java ecosystem. Empirical Software Engineering 26, 6 (2021), 134:1–134:36.
[28] Md Rayhanur Rahman, Akond Rahman, and Laurie Williams. 2019. Share, But be
Aware: Security Smells in Python Gists. In 2019 IEEE International Conference on

Software Maintenance and Evolution (ICSME). 536–540. https://doi.org/10.1109/
ICSME.2019.00087

[29] Hridesh Rajan, Tien N. Nguyen, Robert Dyer, and Hoan Anh Nguyen. 2021. Boa

website. http://boa.cs.iastate.edu/boa/.

[30] Beatrice Åkerblom, Jonathan Stendahl, Mattias Tumlin, and Tobias Wrigstad.
2014. Tracing Dynamic Features in Python Programs. In Proceedings of the 11th
Working Conference on Mining Software Repositories (Hyderabad, India) (MSR
2014). Association for Computing Machinery, New York, NY, USA, 292–295.
https://doi.org/10.1145/2597073.2597103

[31] Beatrice Åkerblom and Tobias Wrigstad. 2015. Measuring Polymorphism in
Python Programs. SIGPLAN Not. 51, 2 (Oct. 2015), 114–128. https://doi.org/10.
1145/2936313.2816717

[32] Robert M. Siegfried, Diane Liporace, and Katherine G. Herbert-Berger. 2019. What
Can the Reid List of First Programming Languages Teach Us About Teaching
CS1?. In Proceedings of the 50th ACM Technical Symposium on Computer Science
Education (Minneapolis, MN, USA) (SIGCSE ’19). Association for Computing
Machinery, New York, NY, USA, 1256–1257. https://doi.org/10.1145/3287324.
3293830

[33] The Boa Team. 2021. Boa compiler project. https://github.com/boalang/compiler.
[34] TIOBE Software BV. 2021. TIOBE Index for August 2021. https://tiobe.com/tiobe-

index/.

[35] Rachel Turner, Michael Falcone, Bonita Sharif, and Alina Lazar. 2014. An Eye-
Tracking Study Assessing the Comprehension of C++ and Python Source Code.
In Proceedings of the Symposium on Eye Tracking Research and Applications (Safety
Harbor, Florida) (ETRA ’14). Association for Computing Machinery, New York,
NY, USA, 231–234. https://doi.org/10.1145/2578153.2578218

[36] Phillip Merlin Uesbeck and Andreas Stefik. 2019. A Randomized Controlled Trial
on the Impact of Polyglot Programming in a Database Context. In 9th Workshop
on Evaluation and Usability of Programming Languages and Tools (PLATEAU 2018)
(OpenAccess Series in Informatics (OASIcs), Vol. 67), Titus Barik, Joshua Sunshine,
and Sarah Chasins (Eds.). Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
Dagstuhl, Germany, 1:1–1:8. https://doi.org/10.4230/OASIcs.PLATEAU.2018.1
[37] Guido van Rossum and Phillip J. Eby. 2005. PEP 342 – Coroutines via Enhanced

Generators. https://www.python.org/dev/peps/pep-0342/.

[38] Beibei Wang, Lin Chen, Wanwangying Ma, Zhifei Chen, and Baowen Xu. 2015. An
empirical study on the impact of Python dynamic features on change-proneness.
In SEKE. 134–139.

[39] Mingwei Zheng, Jun Yang, Ming Wen, Hengcheng Zhu, Yepang Liu, and Hai Jin.
2021. Why Do Developers Remove Lambda Expressions in Java?. In 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
67–78. https://doi.org/10.1109/ASE51524.2021.9678600

12

