FFTc: An MLIR Dialect for Developing HPC Fast
Fourier Transform Libraries

Yifei He, Artur Podobas, Måns I. Andersson and Stefano Markidis

KTH Royal Institute of Technology
{yifeihe, podobas, mansande, markidis}@kth.se

2
2
0
2

l
u
J

6
2

]
S
M

.
s
c
[

2
v
3
0
8
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Discrete Fourier Transform (DFT) libraries are one
of the most critical software components for scientiﬁc comput-
ing. Inspired by FFTW, a widely used library for DFT HPC
calculations, we apply compiler technologies for the development
of HPC Fourier transform libraries. In this work, we introduce
FFTc, a domain-speciﬁc language, based on Multi-Level Interme-
diate Representation (MLIR), for expressing Fourier Transform
algorithms. We present the initial design, implementation, and
preliminary results of FFTc.

Index Terms—MLIR, Fast Fourier Transform Compiler, DSL

I. INTRODUCTION

HPC libraries for computing Discrete Fourier Transforms
(DFT) are critical computational building blocks for enabling
signal processing, data analysis, and the solution of Partial
Differential Equations (PDE). In particular, Fast Fourier Trans-
form (FFT) algorithms solve DFT via O(n log n) calculations,
where n is the input size against the naive DFT implementa-
tion corresponding to a matrix-vector multiply with complex
numbers requiring O(n2) calculations.

Several algorithms for FFT have been designed, including
the notorious Cooley-Tukey recursive scheme to the Stockham
and Pease algorithms
[1]. FFT algorithms can be expressed
using a factorized formulation, e.g., the entire FFT operation is
expressed as the multiplication of matrices, and different algo-
rithms will correspond to various factorization forms. These
matrices are largely sparse, and their ﬁnal computation will
still rely only on O(n log n) operations. Therefore, from an
abstraction point of view, we can express any FFT algorithms
in terms of matrix multiplications. Most importantly for this
work, different factorizations are better suited than others for
achieving high-performance on a given system. For instance,
Stockham FFT factorization is an excellent ﬁt for accelerators
while other factorizations containing block matrices are a
good ﬁt for hierarchical memory systems. For this reason,
to be capable of expressing and generating automatically and
optimizing different FFT algorithms for different architectures
is critical for producing high-performance FFT libraries.

FFTW [2] is among the most successful implementations of
FFT libraries. Inspired by the FFTW design and development,
in this work, we propose a new framework, called FFTc (FFT

Funding for the work is received from the European High-Performance
Computing Joint Undertaking (JU), Grant Agreement No. 955811 (IO-SEA).

compiler), for the automatic generation of FFT algorithms
using the MLIR and LLVM compiler infrastructure. To achieve
this, we design a new language to express FFT algorithms
using different formulations. The major contributions of this
paper are the following:

• We design and provide a ﬁrst initial development of a
domain-speciﬁc language for the automatic code gener-
ation of FFT algorithms, leveraging MLIR and LLVM
infrastructure.

• We collect and analyze the preliminary performance re-
sults from small-size one-dimensional FFT and compare
the performance with the FFTW performance.

II. BACKGROUND

The goal of this work is to develop a DSL for FFT
calculation. A direct computation of the Fourier transform is
the multiplication a DFT matrix by the input vector x. We can
deﬁne the DF TN matrix as:

DF TNm,n = (ωN )mn,

where ωN = exp(−2πi/N )

for

0 ≤ m, n < N.

The most famous FFT algorithm was introduced in 1965 by
Cooley and Tukey. This algorithm relies on the recursive na-
ture of DFT i.e. several small DFTs can describe a large DFT.
In this paper, we use a matrix-formalism to represent FFT
algorithms where a matrix-factorization of the DFT matrix into
sparse and structured matrices describes each FFT algorithm.
For example the Cooley-Tukey factorization of DFT4:

DFT4 =


1


1


(cid:124)

1

−1

1

−1








1




1

1


1
1
1 −1










−i

1
1
1 −1

1

1

(cid:123)(cid:122)
DFT2 ⊗ I2

(cid:125)

(cid:124)

(cid:123)(cid:122)
I2 ⊗ DFT2

1

1







,

1








1




(cid:125)

(1)
where the I is the identity matrix. Here, we see the use of
DFT2 in the formulation of DFT4. In the example, we see
the sparse (zeros in the matrices are omitted for clarity) and
structured nature of the algorithm. The Cooley-Tukey general-
radix decimation-in-time algorithm for N inputs can be written
as:

DFTN = (DFTK ⊗ IM) DN

M(IK ⊗ DFTM) ΠN

K with N = M K,

(2)

 
 
 
 
 
 
K is a stride permute and DN

where ΠN
M is a diagonal matrix of
twiddle-factors. Different FFT algorithms, such as Stockham
and Pease FFT can be expressed using different factorization
schemes.

In this work, we use the LLVM (originally for Low-Level
Virtual Machine) compiler infrastructure for the development
of the FFT domain-speciﬁc language. LLVM is a collection
of compiler and toolchain technologies: it consists of a set
of modular compiler components, including the Clang front-
ends, optimizer, code generator, debugger, linker, and OpenMP
runtime. Particularly important for developing portable HPC
code, the LLVM compiler technologies support many targets,
including x86, Arm, and GPU systems [3].
The LLVM project also includes Multi-Level Intermediate
Representation (MLIR), a project aiming at supporting the
building of domain-speciﬁc compilers, and combining existing
compiler infrastructure together. While MLIR (and the XLA
compiler) was initially developed by Google for machine
learning workloads, MLIR is widely used today for the
development of domain-speciﬁc languages beyond machine
and deep learning. To solve domain-speciﬁc problems, MLIR
offers the infrastructure to deﬁne and introduce high-level
abstractions and transforms [4]. The main mechanism to
extend MLIR is the development of dialects that allow deﬁning
new operations, attributes, and types. In addition, MLIR allows
using multiple dialects that can be used together within one
module. Examples of existing MLIR dialects are the afﬁne,
LLVM, GPU, vector, SPIR-V dialects. In this work, we design
and develop an MLIR dialect to express FFT libraries.

III. RELATED WORK

for

exist

efforts

Several

the development of high-
performance FFT libraries. The inspiration for developing an
FFT DSL is FFTW [5], which is the most widely used open-
source FFT library. At its heart, FFTW is an FFT compiler,
based on Objective Caml, to generate Directed Acyclic Graphs
(DAG) of FFT algorithms and performs algebraic optimization
on them. FFTW uses a planner at runtime to recursively
decompose the DFT problem into sub-problems. These sub-
problems are solved directly by optimized, straight-line code
that is automatically generated by a special-purpose compiler,
called genfft [2]. An additional DSL for numerical kernels in-
cluding FFT is SPIRAL. SPIRAL [6] is a program generation
system for linear transforms and other mathematical functions
that produces HPC code in C. SPIRAL also supports FFTs [7]:
it applies pattern match and rewriting to generate optimal FFT
formulation for different hardware, such as multicore systems.
Then, SPIRAL maps the matrix formula to high-performance
C code.

IV. METHODOLOGY: A DOMAIN-SPECIFIC LANGUAGE

FOR FFT

This section describes FFTc– a custom Domain-Speciﬁc
Language (DSL)
for describing Fast Fourier Transforms
(FFT). Our aspiration with FFTc is to increase the productivity
of algorithm developers without any loss in performance

while at the same time being able to target multiple different
backends (CPUs/GPUs/etc.) with the same input source code.
In short, FFTc aims to increase productivity, portability, and
(hopefully) performance.

The execution model and compilation pipeline are shown
in Figure 1. The current implementation supports the parts in
dark color; the remaining parts will be the focus in the near
future.

The FFTc compilation pipeline has ﬁve core parts: (a) is
the translation from the DSL to the Abstract Syntax Tree
(AST), (b) is generating the MLIR out of AST, (c) stands for
progressive lowering from FFT dialect to LLVM dialect, going
through different levels of abstraction represented by dialects,
(d) emits LLVM IR out of the MLIR’s LLVM dialect, (e) is
the LLVM middle-end compilation and code generation.

A. The FFTc language and Grammar

The goal of FFTc is to create an input

language that
resembles (as close as possible) that of mathematics, which
we believe will help end-users in being more productive
without losing familiarity with the code they are writing. An
example source code of our is seen in Listing 1, where we
have aimed to keep them as similar to abstract mathematical
expressions as possible, such as equation (2). We support
the Kronecker product
through the binary operation ’⊗’,
the matrix-matrix multiplication using ’·’, and the matrix
multiplication with the twiddle matrix through the twiddle.
Furthermore, we have a set of unary operations, such
as creating the identity matrix, and calculating the
dft. Finally, we have support for permuting. In short,
we currently support all necessary language constructs
to describe FFTs in a factorized form. Additionally, our
grammar supports the correct right-associative binding of
(e.g.,) matrix multiplication, which is different
from the
traditional
left-associative binding of binary operators. A
subset of the grammatical construct (in Backus-Naur form)
is shown in Listing 2. The grammatical construct is based
on (and extended)
from LLVM’s Kaleidoscope language
tutorial [8].

B. FFTc Compilation Pipeline

The FFTc compilation pipeline shown in Figure 1 is based
on the MLIR’s tutorial project [9]. The compilation starts at
the frontend (Figure 1:a), where the lexical analysis, parsing,
and building of an Abstract Syntax Tree (AST) based on our
custom DSL language take place. The FFT dialect is the ﬁrst
state of MLIR generated from the AST (Figure 1:b). Then
a series of lowering passes are applied (Figure 1:c) on the
FFT dialect in order to expand many of the custom operators
(e.g., the Kronecker product) into a lowered state. For example,
a matrix multiplication, written in our language using "·",
will be expanded to a three-level nested loop implementing
said matrix multiplication. Furthermore, we can apply several
existing MLIR optimization passes (such as Afﬁne) in order

Fig. 1: Compilation Pipeline

v a r
v a r
v a r
v a r

I n p u t R e a l <4 , 1> = [ [ 1 ] ,
I n p u t I m g
<4 , 1> = [ [ 1 ] ,
I n p u t C o m p l e x = c r e a t e C o m p l e x ( I n p u t R e a l ,
r e s u l t =

[ 3 ] ,
[ 3 ] ,

[ 2 ] ,
[ 2 ] ,

[ 4 ] ] ;
[ 4 ] ] ;

( DFT ( 2 ) ⊗ I ( 2 ) )
( I ( 2 ) ⊗ DFT ( 2 ) )

·
t w i d d l e ( 4 , 2 )
· P e r m u t e ( 4 , 2 )

I n p u t I m g ) ;

·
·

I n p u t C o m p l e x ;

Listing 1: DSL FFT language

e x p r e s s i o n −> a d d i t i v e − e x p r
a d d i t i v e − e x p r −> ( m u l t i p l i c a t i v e − e x p r
m u l t i p l i c a t i v e − e x p r −> ( FFT− e x p r
’ · ’
FFT− e x p r −> ( p r i m a r y (
p r i m a r y −> i d e n t i f i e r e x p r

| n u m b e r e x p r

( ’ + ’ |

’⊗’

|

(

’ * ’
’ / ’

’ * ’

(
|
) FFT− e x p r ) *

’ − ’ ) a d d i t i v e − e x p r

) FFT− e x p r ) *

| p a r e n e x p r

|

t e n s o r l i t e r a l

|

’ / ’

) m u l t i p l i c a t i v e − e x p r )

Listing 2: FFTc Language Grammar Extension in Backus–Naur form

to further optimize the transformed kernels. Finally, near the
end of the pipeline (Figure 1:c), we lower our representation
to the LLVM Intermediate Representation (IR), after which
we inject the code into the LLVM backend for compilation
towards machine code (Figure 1:d). We explain this pipeline
in more detail next.

1) Phase 1: Translation
The FFT dialect

in the compilation
is the ﬁrst dialect
pipeline. The FFT dialect provides the basic building blocks
for different kinds of FFT algorithms and deﬁnes the complex
tensor data type and operations.

• FFT dialect data type: The FFT dialect operates on the
double tensor and complex tensor as well as scalar integer
as attributes. There is createComplex to generate
the complex tensor from the double tensor of real and
imaginary parts.

• FFT dialect operations: We deﬁne the operations needed
to implement various kinds of popular FFTs. Examples

FFTc DSL
createComplex(A, B)
A · B
A ⊗ B
twiddle (a,b)
I(size)
DFT(size)
Permute (a ,b)

FFT Dialect
fft.createCT(a,b)
fft.matmul a, b :
fft.kroneckerproduct a, b
fft.twiddle (a , b)
fft.identity (a)
fft.dft(a)
fft.Permute(a, b)

TABLE I: From FFTc DSL to FFT Dialect MLIR

of such operators are the Kronecker product and matrix-
matrix multiplication. We also deﬁne the DF, Identity,
and permute matrix generator. These make it a lot
easier to construct the FFT algorithm with the similar
notation and syntax in mathematics. The map of opera-
tions from FFTc DSL to MLIR FFT dialect is shown in
Table I.

With the FFT dialect implementation described above, we can

TranslationFormulaRewritingOperatorFusion/SchedulingOperatorImplementation/ OptimizationFFT ASTFFT DialectLinalgAffineTranslationLLVMCompiler RuntimeCompilation PipelineMIddle-end Optimization/Code Generationfor CPU targetGPU TargetFFT Plan GenerationCost ModelPlan SelectionTask Scheduling…......FFTc DSL(a)(c)(d)(e)(b)generate MLIR out of AST, as shown in (a) to (b) in Fig. 1.
Fig. 2 shows a example of the FFT dialect IR that is translated
from size 4 recursive FFT in Listing 1.

2) Phase 2: Operator Implementation/Optimization
MLIR supports different levels of abstraction through di-
alects. We lower the FFT dialect to a mix of dialects. Then,
we can reuse the analysis/transform passes embedded in those
dialects. We run shape inference to prepare for later transforms
and perform progressive lowering to a mix of dialects to
implement and optimize FFT operations.

• Shape Inference: In the FFTc DSL, all the operations
operate on generic tensors. We do not need to explicitly
specify the shape of tensor data. This reduces the efforts
of the programmers. However, carrying shape information
in the IR can simplify the workload of analysis and
transform passes, as well as code generation. We can
obtain the shape of input tensors during the initialization
of constants. Later, we propagate the shapes through the
computation to every operation involved. We implement
a speciﬁc shape inference function for each operation
based on the input augments, such as for the Kronecker
product. All dimensions of the output tensor would be
the multiplication of the corresponding dimensions of two
input tensors.

• Progressive Lowering: The compilation pipeline gener-
ates the actual implementations of the operations, which
we deﬁned through progressive lowering. To reuse ex-
isting optimizations in MLIR’s dialects, we lower the
FFT dialect to a mix of dialects, comprising of Afﬁne,
Arithmetic, Complex and MemRef dialects. The Afﬁne
dialect uses techniques from polyhedral compilation to
provide a powerful abstraction for afﬁne operations and
analyses, such as dependence analysis and loop transfor-
mations. The Arithmetic dialect is intended to hold basic
integer and ﬂoating-point mathematical operations, and
the Complex dialect is intended to hold complex numbers
creation and arithmetic operations. The MemRef dialect is
intended to hold core memref creation and manipulation
operations [10].

• Afﬁne Dialect: We implement the computation-heavy
part of the DSL in Afﬁne dialect, by lowering from the
tensor type that FFT dialect operates on to the MemRef
type that
is indexed via an afﬁne loop-nest. Tensors
represent an abstract value-typed sequence of data. By
using tensor and tensor operations, we can increase the
productivity of algorithm developers since it is similar to
the notations used in mathematics. The MemRefs dialect,
on the other hand, represents the lower level buffer access,
builds a bridge to the actual computer memory.
To implement
the operators, we allocate a chunk of
memory for the output tensor, construct loops to compute
each element of the output tensor, then store them to
the corresponding index of the output memory. The
scalarized tensor arithmetic operations are performed by
corresponding operations in the Complex dialect. The
lowering result of a matrix multiplication operator is

shown in the Listing 3. We take advantage of the existing
optimizations in the Afﬁne dialect, such as loop fusion,
AfﬁneScalarReplacement and AfﬁneLoopInvariantCode-
Motion. These optimization passes can help perform
operator fusion, eliminate redundant load/store and hoists
loop invariant operations out of Afﬁne loops.

3) Phase 3: Translation
There exist

infrastructures in MLIR to perform a full
conversion from the Afﬁne, MemRef, and Complex dialects
to the LLVM dialect. Then, we can emit the LLVM IR from
the LLVM dialect.

4) Phase 4: Code Generation
We set up a JIT compiler using the MLIR wrapper over
LLVM OrcJit, and pass the optimization and debug ﬂags to the
JIT compiler. The pass manager is also populated by MLIR.
Then, the JIT compiler will perform the LLVM’s middle-end
optimization and code generation.

a) Ahead-Of-Time vs Just-In-Time Compilation

We support

two types of compilation modes in FFTc:
Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation.
The compilation modes can be seen in Figure 3, where
they share multiple components and are in line with similar
compilation ﬂows (e.g., in OpenCL’s Online/Ofﬂine compi-
lation [11]). In short, both modes start by parsing the DSL
source code and transforming/optimizing it using our MLIR
intermediate representation. Next, we lower the MLIR down
to LLVM IR. Once in LLVM IR, the two modes differ: using
the JIT mode, we directly execute the main function of our
compiled targets and exit afterward. The AOT mode, instead,
transforms the LLVM IR representation to an object ﬁle, links
with eventual standard libraries, and outputs a machine code
binary ﬁle that can be invoked by the user.
Using either model has beneﬁts and limitations. For example,
the AOT mode can be faster and speed up the ﬁnal execution
signiﬁcantly but has the limitation that the FFT size needs to
be constant. The JIT model, on the other hand, is slower but
allows the FFT size to be variable at runtime. In short, the
AOT mode trades ﬂexibility for performance, while the JIT
mode honors ﬂexibility over performance.

V. EXPERIMENTAL SETUP

We evaluate our FFTc on the Kebnekaise supercomputer
that is located at the HPC2N HPC center in Umeå, Sweden.
Kebnekaise nodes have a dual-socket Intel Xeon Gold 6132
CPU, 192 GB of RAM. The operating system is Ubuntu
20.04.4 LTS. The version of LLVM we use to embed the FFTc
is 15.0.0. We run the Ahead-of-Time compilation mode FFTc
1,000 times, and we calculate error computing the standard
deviation for 30 execution rounds. We developed a Python
script to generate the recursive implementation of the Cooley-
Tukey FFT algorithm, using our FFTc DSL. An example of
the output program is shown in Listing 1. Albeit our script
can generate different FFT algorithm implementations, in this
paper, we only present the results of the recursive Cooley-
Tukey algorithm.

Fig. 2: Mapping from the Recursive FFT to MLIR.

precision. We compare the results with the NumPy’s FFT
function, that is based on FFTW. The error is calculated as
|resultDSL−resultN umpy|
. The error is smaller than 1e-7 for
F F Tsize

each run.

For the next step, we evaluate the performance in the
JIT mode. We measure the execution time of size 32 recur-
sive FFT under JIT mode. The execution time is shown in
Fig. 4. In the ﬁgure, the item Parser&MLIRGen stands for
frontend compilation, ’builtin.func’ stands for MLIR compi-
lation pipeline, ’Jit’ stands for both LLVM Jit compilation
and running time. It is clear from analyzing the ﬁgure that
the frontend takes a minor portion of the execution time.
The MLIR pipeline takes the largest part of the execution
time. Most of the time is spent in the optimization passes
such as AfﬁneLoopFusion and AfﬁneScalarReplacement. We
can choose whether to run these optimization passes or not
by passing optimization ﬂag to FFTc, currently there are
O0/O2/O3 available. The Jit part takes much smaller portion
compared with MLIR pipeline, under O3 optimization option
for both LLVM middle-end compilation and code generation.
In actual applications, FFT algorithms may run many times
while only need to be compiled once, so the compilation time
does not matter considerably. As a future plan, we intend
to reduce the compilation time, such as multi-threading the
compiler and remove redundant operations in Afﬁne passes.

Fig. 3: Compilation modes.

VI. RESULTS

As ﬁrst step of our evaluation, we verify the correctness of
DSL implementation. We test different random input vectors
with different sizes: the input sizes are the powers of two,
from 32 to 1024. We employ complex numbers in double-

Under Pre-Compiled mode, we compare the FFTc pre-
compiled binary with FFTW 3.3. We built FFTW with gcc
compiler, enabled the SIMD instructions. The input size of
the FFT are the powers of 2, we use single thread to run

var result =(DFT(2)⊗I(2))·twiddle(4,2)·(I(2)⊗DFT(2))·Permute(4,2)·InputComplex;%5 = fft.arithconstant 2.000000e+00 : f64%6 = "fft.dft"(%5) : (f64) ->tensor<*xcomplex<f64>>%7 = fft.arithconstant 2.000000e+00 : f64%8 = "fft.identity"(%7) : (f64) ->tensor<*xcomplex<f64>>%9 = fft.kroneckerproduct %6, %8 :tensor<*xcomplex<f64>>%10 = fft.arithconstant 4.000000e+00 : f64%11 = fft.arithconstant 2.000000e+00 : f64%12 = "fft.twiddle"(%10, %11) : (f64, f64) ->tensor<*xcomplex<f64>>%13 = fft.arithconstant 2.000000e+00 : f64%14 = "fft.identity"(%13) : (f64) ->tensor<*xcomplex<f64>>%15 = fft.arithconstant 2.000000e+00 : f64%16 = "fft.dft"(%15) : (f64) -> tensor<*xcomplex<f64>>%17 = fft.kroneckerproduct %14, %16 : tensor<*xcomplex<f64>>%18 = fft.arithconstant 4.000000e+00 : f64%19 = fft.arithconstant 2.000000e+00 : f64%20 = "fft.Permute"(%18, %19) : (f64, f64) ->tensor<*xcomplex<f64>>%21 = fft.matmul %20, %4 : tensor<*xcomplex<f64>>%22 = fft.matmul %17, %21 : tensor<*xcomplex<f64>>%23 = fft.matmul %12, %22 : tensor<*xcomplex<f64>>%24 = fft.matmul %9, %23 : tensor<*xcomplex<f64>>DSL Source CodeMLIR GenerationMLIR OptimizationJIT CompilationLLVM IR GenerationLLVM IR OptimizationCode GenerationInvoke Main FunctionObject FileExecutable BinaryRun Binary(a) JIT(b) Pre-CompileFrom :

%10 = f f t . matmul %9, %3 :
t e n s o r <4 x1xcomplex < f64 > >) −> t e n s o r <4 x1xcomplex < f64 >>

( t e n s o r <4 x4xcomplex < f64 > > ,

To :

a f f i n e . f o r %a r g 0 = 0 t o 4 {
a f f i n e . f o r %a r g 1 = 0 t o 1 {

a f f i n e . f o r %a r g 2 = 0 t o 4 {
%18 = a f f i n e . l o a d %9[%a r g 0 , %a r g 2 ]
%19 = a f f i n e . l o a d %3[%a r g 2 , %a r g 1 ]
%20 = complex . mul %18 , %19 : complex < f64 >
%21 = a f f i n e . l o a d %2[%a r g 0 , %a r g 1 ]
%22 = complex . add %21 , %20 : complex < f64 >
a f f i n e . s t o r e %22 , %2[%a r g 0 , %a r g 1 ]
}

: memref <4 x4xcomplex < f64 >>
: memref <4 x1xcomplex < f64 >>

: memref <4 x1xcomplex < f64 >>

: memref <4 x1xcomplex < f64 >>

}

}

Listing 3: Afﬁne Code Example for FFT.MatMul Operation

to showcase the functionality of FFTc and are planning to
rewrite the computation in sparse form as a future work. The
performance with optimization ﬂag O3 is better that O2 and
O0. The difference between O2 and O3 ﬂag is that under O2,
the AfﬁneScalarReplacement pass will not be executed. For
size 128 the O2 is slightly better than O3. Investigating the
MLIR, the AfﬁneScalarReplacement performs memory access
optimizations. In addition, there is also a similar optimization
pass in LLVM pipeline. We plan to further investigate this
issue in the future.

When comparing the performance between FFTc Cooley-
Tukey code and FFTW, we note that here is still a signiﬁcant
performance gap. We believe that this gap can be attributed to
(amongst others) the following reasons:

• The recursive factorized FFTs are computed through
matrix-matrix multiplication where the matrices are not
expressed as sparse matrices.

• We do not take full advantage of MLIR/LLVM infras-
tructure to generate high performance code. Examples of
such a features are loop tiling, unrolling and jam and
vectorization in the MLIR/LLVM pipeline.

• We do not support yet an autotuning mechanism, such
as the FFTW planner, to decompose the FFT problem
into simpler sub-problems, later solve the simpler sub-
problems using codelets generated by genfft. Currently,
our implementation is similar to genfft: for the FFTs with
large-size input, the generated code is extremely large and
introduces considerable compilation overhead.

VII. DISCUSSION AND CONCLUSION

In this paper, we have introduced FFTc– an emerging,
work-in-progress DSL for describing different FFTs vari-
ants. The goal of FFTc is to decouple algorithm description
from hardware-speciﬁc details and ultimately provide higher
productivity and better portability without sacriﬁcing perfor-
mance. To this end, we have chosen an abstract language

Fig. 4: JIT Mode Performance for size 32 recursive FFT

is shown in Fig. 5,

the program. The result
the standard
deviation is shown as the black lines over bars. We run four
versions of FFT using FFTc: direct DFT implementation and
Cooley-Tukey recursive FFT implementation with different
optimization ﬂags (O0/O2/O3). It is expected that the DFT
performs much better than recursive implementations, because
current implementation for FFT is computed through dense
matrix multiplication, and to achieve the O(N log N) com-
plexity FFT must be sparse matrix computation. The workload
of the currently developed recursive FFT is much larger
than DFT. However, we intend to use the current solution

Total Execution Time ReportWall Time / SecondsName0.0034 (0.0%)Parser & MLIRGen0.0003 (0.0%)Inliner0.0000 (0.0%)      (A) CallGraph0.0000 (0.0%)      'builtin.func' Pipeline0.0002 (0.0%)      Canonicalizer6.2268 (90.4%)builtin.func' Pipeline0.0001 (0.0%)      {anonymous}::ShapeInference0.0001 (0.0%)      Canonicalizer0.0000 (0.0%)      CSE0.0000 (0.0%)            (A) DominanceInfo0.0116 (0.2%)      {anonymous}::AffineToLLVMLoweringPass0.0226 (0.4%)      Canonicalizer0.0014 (0.0%)      CSE0.0000 (0.0%)            (A) DominanceInfo0.6238 (9.1%)      AffineLoopFusion5.5622 (80.7%)      AffineScalarReplacement0.0000 (0.0%)            (A) PostDominanceInfo0.0000 (0.0%)            (A) DominanceInfo0.0009 (0.0%)      AffineLoopInvariantCodeMotion0.0384 (0.6%){anonymous}::FFTToLLVMLoweringPass0.0000 (0.0%)output0.6154 (8.9%)Jit0.0057 (0.1%)Rest6.8903 (100%)TotalFig. 5: FFTc Single Thread Performance Compared with FFTW

[8] “Kaleidoscope: Implementing a Parser and AST,” https://llvm.org/docs/

tutorial/MyFirstLanguageFrontend/LangImpl02.html.

[9] “MLIR Toy language,” https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/.
[10] “MLIR Dialects Document,” https://mlir.llvm.org/docs/Dialects/.
[11] “Intel

https://www.intel.com/

Online/Ofﬂine

Compilation,”

programmable/technical-pdfs/683521.pdf.

representation that is not unlike the mathematical formulas
we are used to describing FFTs. We show how such an
abstract language design can be mapped down-to machine
code by leveraging existing MLIR and LLVM infrastructure.
The performance – while not a direct objective of this paper –
of our DSL is not yet on par with state-of-the-art FFTW, but
is never-the-less a good starting point to further build upon
in future performance-focused studies, such as extending our
compiler with support for OpenMP tasking or vectorization.

VIII. ACKNOWLEDGEMENT

Funding for the work is received from the European High-
Performance Computing Joint Undertaking (JU), Grant Agree-
ment No. 955811 (IO-SEA). I want to thank Steven W. D.
Chien (wdchien@kth.se) for his help with the proofread.

REFERENCES

[1] C. Van Loan, Computational frameworks for the fast Fourier transform.

SIAM, 1992.

[2] M. Frigo and S. G. Johnson, “The design and implementation of
FFTW3,” Proceedings of the IEEE, vol. 93, no. 2, pp. 216–231, 2005.
[3] C. Lattner and V. Adve, “LLVM: a compilation framework for lifelong
program analysis amp; transformation,” in CGO 2004., 2004, pp. 75–86.
[4] C. Lattner and al., “MLIR: Scaling compiler infrastructure for domain
speciﬁc computation,” in 2021 IEEE/ACM International Symposium on
Code Generation and Optimization, 2021, pp. 2–14.

[5] M. Frigo, “A fast fourier transform compiler,” in ACM SIGPLAN
1999 Conference on Programming language design and implementation,
1999, pp. 169–180.

[6] F. Franchetti and al., “SPIRAL: Extreme performance portability,” From
High Level Speciﬁcation to High Performance Code, vol. 106, no. 11,
2018.

[7] F. Franchetti, “Discrete fourier transform on multicore,” IEEE Signal

Processing Magazine, vol. 26, no. 6, pp. 90–102, 2009.

