Bunk8s: Enabling Easy Integration Testing of
Microservices in Kubernetes

Christoph Reile∗, Mohak Chadha∗, Valentin Hauner†, Anshul Jindal∗, Benjamin Hofmann†, Michael Gerndt∗
∗Chair of Computer Architecture and Parallel Systems, Technische Universit¨at M¨unchen
Garching (near Munich), Germany
†MaibornWolff GmbH, Germany
Email: ∗{ﬁrstname.lastname}@tum.de, †{ﬁrstname.lastname}@maibornwolff.de

2
2
0
2

l
u
J

4
1

]
E
S
.
s
c
[

1
v
1
1
8
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Microservice architecture is the common choice for
cloud applications these days since each individual microservice
can be independently modiﬁed, replaced, and scaled. However,
the complexity of microservice applications requires automated
testing with a focus on the interactions between the services.
While this is achievable with end-to-end tests, they are error-
prone, brittle, expensive to write, time-consuming to run, and
require the entire application to be deployed. Integration tests
are an alternative to end-to-end tests since they have a smaller test
scope and require the deployment of a signiﬁcantly fewer number
of services. The de-facto standard for deploying microservice
applications in the cloud is containers with Kubernetes being the
most widely used container orchestration platform. To support
the integration testing of microservices in Kubernetes, several
tools such as Octopus, Istio, and Jenkins exist. However, each
of these tools either lack crucial functionality or lead to a
substantial increase in the complexity and growth of the tool
landscape when introduced into a project. To this end, we
present Bunk8s, a tool for integration testing of microservice
applications in Kubernetes that overcomes the limitations of
these existing tools. Bunk8s is independent of the test framework
the used
used for writing integration tests,
CI/CD infrastructure, and supports test result publishing. A
video demonstrating the functioning of our tool is available from
https://www.youtube.com/watch?v=e8wbS25O4Bo.

independent of

Index Terms—DevOps, Integration Testing, Kubernetes, Mi-

croservices

I. INTRODUCTION

Microservices-based applications are typically composed of
independently deployable components that can be directly
translated into services and automatically scaled on-demand.
Components are independently replaceable and upgradable
units of software [1], consisting of several modules [2].
Linux containers, due to their portability, isolation, and high
availability, have become the de-facto standard for developing,
testing, and deploying such applications in cloud environ-
ments [3]. This is because containers enable users to package
their application and its custom software dependencies as a
single unit into easy-to-deploy images. Moreover, containers
are a natural ﬁt for microservices-based applications in the
cloud due to their smooth integration with container orches-
tration platforms [4] for efﬁcient resource management. Recent
trends [5] show that nearly 90% of all deployed containers are
orchestrated, with Kubernetes (k8s) [6] being the most utilized
container orchestration platform [7].

Developers can deploy k8s on-premise or can utilize the
services offered by most commercial cloud providers such as
Microsoft (Azure Kubernetes Service) [8]. If a microservice-
based application is deployed to a k8s cluster, it may rely on
backend services that run outside the k8s cluster. For instance,
services that provide storage or networking functionalities. For
commercial cloud providers, these services are self-managed,
with only the service’s live API available for the developer to
access. As a result, testing the interactions of modules from a
microservice application with the modules from the backend
service requires the entire backend service to be deployed.
It cannot be done in isolation with only speciﬁc modules of
the backend service. Therefore, testing the interactions of a
microservice application with backend services can only be
accomplished by running end-to-end tests or broad integration
tests. Note that, integration tests can be classiﬁed into narrow
and broad depending on their granularity (§II).

End-to-end tests are brittle, expensive to write,

time-
consuming to execute, and require the deployment of the entire
application [9]. Broad integration testing of microservices in
k8s can be done in two ways, i.e., from inside or outside the
cluster. Running broad integration tests for microservices from
outside the k8s cluster has several drawbacks, making them as
inefﬁcient as end-to-end tests. For example, the lack of public
interfaces in the services which allow them to be accessed
from outside the k8s cluster. As a result, the standard way
of broad integration testing of microservices is from inside
the k8s cluster. For the integration testing of microservices
from inside the k8s cluster, several tools such as Octopus [10],
Istio [11], and Jenkins [12] exist. However, the existing tools
lack crucial functionalities or are dependent on particular
infrastructure for the execution of these tests. To this end, we
present Bunk8s that overcomes the limitations of these existing
tools.

Towards easy integration testing in Kubernetes, our key

contributions are:

• We implement and present Bunk8s1, a tool for broad

integration testing of microservices in Kubernetes.

• Bunk8s is independent of the test framework used for
writing integration tests, independent of the used CI/CD
infrastructure, and supports publishing of test results.

1https://github.com/Teiktos/bunk8s

©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
Fig. 1: Narrow and Broad integration testing.

Thus, making the inclusion of Bunk8s in a project rel-
atively easy.

• We demonstrate the functioning of Bunk8s by running
broad integration tests for a real-world microservice ap-
plication on a commercial cloud provider, i.e., Azure.
The rest of the paper is structured as follows. §II provides
a background on integration testing. In §III, the system design
of Bunk8s and its workﬂow for integration testing of microser-
vices are described. In §IV, we demonstrate the functioning of
Bunk8s for the integration testing of a real-world microservice
application. §V motivates the need for Bunk8s and presents
a comparison with other tools for integration testing of mi-
croservices. Finally, §VI concludes the paper and presents an
outlook.

II. BACKGROUND

Integration tests are generally used to verify the interac-
tions between the layers of integration code and the external
microservices to which they are integrating [2]. They can
be further categorized by their granularity, into narrow and
broad integration tests. Narrow integration tests refer to testing
only the modules of microservices that communicate with
each other. Therefore,
they only require instantiating the
modules under test [13]. Microservice applications that run in
commercial cloud offerings of k8s may integrate with services
that run outside of k8s, e.g., Azure Cosmos DB, Azure Service
Bus. Such services only provide developers with their live
versions and it is not possible to instantiate only modules of
these services that can integrate with the components of the
microservice application. As a result, testing the interaction
of microservices and services external to Kubernetes is not
possible with narrow integration tests. On the other hand,
broad integration tests refer to testing code paths through
several services and all of their modules. Moreover, broad
integration tests can be of varying granularity, depending
on the scope of the subsystem under test [2]. They require
live versions of the components under test, a substantial test
environment and network access [13]. The difference in the
code paths used for writing narrow and broad integration tests
is shown in Figure 1.

III. BUNK8S

In this section, we ﬁrst describe our design goals for the
development of Bunk8s. Following this, we present its system

Fig. 2: Component diagram of Bunk8s.

design. Finally, we describe in detail, the workﬂow of Bunk8s
for the broad integration testing of microservices in k8s.

A. Design Goals

To facilitate the ease-of-use for utilizing Bunk8s in a project
and to overcome the limitations of the current tools for the
integration testing of microservices in k8s (§V), we chose the
following design goals:

• Independence of the testing framework used for writing

and generating tests.

• Independent of the speciﬁc CI/CD tools used within a

project.

• Compatible with different container runtimes such as

containerd [14] and CRI-O [15].

• Portable across the different Kubernetes offerings by the
commercial cloud providers such as AKS [8], GKE [16].

• Support for publishing of test results.

B. System Design

Bunk8s allows deploying test runner containers in test
runner pods to a testing namespace. In this namespace, the
microservices under test are deployed. Note that, test runner
containers are written by developers and contain the inte-
gration tests for the microservice application. Following this,
the tests are executed by the test runner containers. Setting
up of the cloud infrastructure and the deployment of the
microservices under test is not a functionality of Bunk8s and
should be done with tools such as Helm [17] or Terraform [18].
The different components of Bunk8s are shown in Figure 2. It
consists of two components, i.e., 1 the Launcher and 2 the
Coordinator. The Launcher runs in the CI/CD pipeline while
the Coordinator runs in the k8s cluster.

1) The Launcher: The Launcher runs outside of k8s as part
of the continuous integration (CI) process in a pipeline. After
the container images of the microservices under test are built
and pushed to a container registry by the pipeline, the Bunk8s
launcher is started, which in turn starts the testing process. The
launcher takes a YAML conﬁguration ﬁle as input, an example
of which is shown in Listing 1. The conﬁguration ﬁle provides
the relevant information that is required by the launcher for
connecting to the coordinator (§III-B2), i.e., its IP address
and port number. The developers can also provide a TLS
certiﬁcate ﬁle for an authorized connection that is placed in
the container image of the launcher. The conﬁguration ﬁle also
includes information required by the coordinator for running

1
2
3
4
5
6
7
8
9
10
11
12
13

14
15

launcherConfig:

coordinatorIp:
coordinatorPort:
certFile:
coordinatorConfig:

testRunnerPods:

- podName:

namespace:
testTimeout:

containers:

# IP or URL of the Ingress
# Port on the Ingress
# Name of the CAs .pem certificate file

# Name of a test runner pod
# Namespace the test runner pod is deployed in
# Timeout for the test runner pod

- containerName:

image:
startupCommands:

# Name of a test runner container
# Name of a test runner container image
# Startup commands for the test runner container

image. Replaces the images entrypoint.

startupCommandsArgs: # Arguments for the startup commands.
testResultPath:

# Directory inside the test runner container image,

where the test results are stored.

Listing 1: Example conﬁguration ﬁle taken as input by the
Bunk8s launcher.

the integration tests. This includes the names of the test runner
images, the namespace to which the test runner pods should be
deployed, the name of the test runner pods, and a test timeout
value. Note that, the list testRunnerPods allows deﬁning
multiple test runner pods which can be deployed in different
namespaces, while the list containers allows deﬁning
multiple test runner containers per pod. The launcher connects
to the coordinator via HTTP and sends the conﬁguration data.
After the testing has ﬁnished, the coordinator responds to the
launcher. Following this, the launcher calls the kube-apiserver
and extracts the test results from the test runner containers
via HTTP, and stores them on the pipeline runner. Pipeline
runner is an agent that executes the CI/CD workﬂow. After the
extraction, the test results can be published with the pipeline’s
test result publishing functionality such as GitLab
default
artifacts [19]. The launcher is designed to run in a container
which makes it platform-independent and easily adaptable to
other container runtimes.

2) The Coordinator: The Bunk8s Coordinator handles
the deployment of the test runner containers and watches
their state until they have ﬁnished. It consists of two sub-
components. First, an RPC server for communication and
synchronization between the launcher and the coordinator.
Second, a k8s clientset that connects to the kube-apiserver
via HTTP, sends a request to create the test runner pods,
and watches their state. The coordinator’s RPC server is
exposed to the launcher via an Ingress. On test completion,
the coordinator sends a response to the Launcher, containing
the information that is required to extract the test results from
the test runner containers. The test runner container image
that is deployed in a pod by the Bunk8s coordinator must be
built and pushed to a container registry from which k8s can
pull container images. Since the integration tests are executed
inside a container that is separate from the other components
of Bunk8s, the developers can use any test framework or
programming language to write their tests.

C. Integration testing with Bunk8s

To install Bunk8s in a k8s cluster, the launcher and coor-
dinator (§III-B) images must be built using Dockerfiles.
The launcher image must be uploaded to a container registry
from where it can be pulled by the pipeline runners, while the
coordinator image must be uploaded to a registry to which

Fig. 3: Workﬂow of Integration testing with Bunk8s.

the k8s cluster has access. For a quick setup of the Bunk8s
coordinator, we provide a pre-conﬁgured Helm chart in our
repository (§I). Note that, the coordinator can be deployed
in any k8s namespace without affecting its functionality.
However, it is recommended that the coordinator is deployed
in a namespace of its own. The project admin is responsible
for creating the role-based access control rules (RBAC) for
the coordinator pod in its namespace. The detailed workﬂow
of the testing process with Bunk8s is shown in Figure 3.

At

the beginning 1 ,

the Bunk8s launcher container is
started by using a launch script. The launcher takes a YAML
conﬁguration ﬁle as input (§III-B) which is mounted into
the container at startup. The YAML parser parses the con-
ﬁguration ﬁle, checks if it is valid 2 , and creates a Go
struct from the conﬁguration data 3 . Following this,
the
gRPC client is created that receives the test run conﬁguration
created in the previous step 4 . The gRPC client calls the
DeployTestRunner RPC method registered in the gRPC
server that invokes several functions deﬁned in the kubeclient
package 5 . For brevity,
those function names have been
omitted from Figure 3. As part of the RPC method in 5 , the
kubeclient does several HTTP requests to the kube-apiserver.
First, it does a GET request for each namespace provided in the
conﬁguration data 6 . After it receives a response, it checks
if the given namespaces already exist or not. If not, the test
run is terminated and the coordinator sends an error code as
a gRPC reply to the launcher. Second, it does a GET request
for each pod from the conﬁguration data 7 . On receipt of the
response, it checks whether the pods already exist within the
namespace that they should be scheduled in. If the pods do not
exist, the kubeclient creates k8s API pod objects. Third, it does
a POST request for each pod object created in the previous
step 8 . After this, the test runner pods are created by the
kube-apiserver 9 . Finally, the kubeclient creates a watcher
for each test runner pod. After all the tests have ﬁnished, the
DeployTestRunner RPC method ends 10 . Following this,
the gRPC server sends the server response to the gRPC client
which is written to the logs of the Bunk8s launcher container
11 . The server reply is stored as a JSON ﬁle by the launch
script ( 12 ) and then read by the data extraction script ( 13 ).
After this, the data extraction script executes kubectl which

1
2
3
4
5
6
7
8
9
10
11
12

TestDeviceServiceReceiveDeviceEvent

TestRoomServiceRoomEventResponse
TestRoomServiceRoomEventResponse/Queue_Dashboard
TestRoomServiceRoomEventResponse/Queue_Shout

% === RUN
% --- PASS: TestDeviceServiceReceiveDeviceEvent (0.27s)
% === RUN
% === RUN
% === RUN
% --- PASS: TestRoomServiceRoomEventResponse (0.07s)
% --- PASS: TestRoomServiceRoomEventResponse/Queue_Dashboard (0.00s)
% --- PASS: TestRoomServiceRoomEventResponse/Queue_Shout (0.00s)
% === RUN
% --- PASS: TestMongoDBRoomAllocationEntry (0.09s)
% PASS
% ok

TestMongoDBRoomAllocationEntry

showcase

0.435s

Fig. 4: Room Tracking Application Microservices.

does an HTTP POST request to the kube-apiserver to copy the
test results from the test runner pods to the pipeline runner 14 .
The test results are extracted by the kube-apiserver from the
sidecar containers inside the test runner pods 15 . Following
this, the test results are stored on the pipeline runner 16 .
Finally, the data extraction script does an HTTP DELETE
request to the kube-apiserver ( 17 ) to delete the test runner
pods ( 18 ). The results of the integration tests can now be
published using the CI pipeline’s publishing mechanisms.

IV. CASE STUDY: ROOM TRACKING SYSTEM

In this section, we demonstrate the usage of Bunk8s for
the broad integration testing of a real-world microservice
application.

A. Experimental Setup

To demonstrate the functioning of Bunk8s, we create
a Kubernetes cluster using the Azure Kubernetes Service
(AKS) [8]. We use GitLab as the DevOps platform since it
provides remote Git repositories and CI/CD pipelines [20].
As the registry for containers, we use the Azure Container
Registry [21]. The chosen application and Bunk8s are deployed
to the AKS by utilizing the pipeline functionality present in
GitLab. Although we use AKS for our experiments, Bunk8s
is compatible with commercial k8s offerings of all cloud
providers.

B. Application Design

The application we use is a room tracking system. A user
can hold a card up to a badge reader inside a room and that
room will be marked as occupied by the application. Users
can also access a dashboard via a browser that provides an
overview of the room utilization. Furthermore, when a user
enters a room, it is announced on a shout website that is
separate from the dashboard.

Figure 4 gives an overview of the different microservices
present
in the room tracking system. It consists of two
frontend services, i.e., Dashboard and Shout, three backend
services, i.e., Device, Account, and Room, and two infrastruc-
ture services, i.e., RabbitMQ and MongoDB. The frontend
services Dashboard and Shout provide the dashboard and
shout websites. RabbitMQ provides message queues for the
communication between the Device, Dashboard, Shout and

Listing 2: Test result logs after successful completion of the
broad integration tests with Bunk8s.

1
2
3

4
5
6

[...]
=== RUN

TestMongoDBRoomAllocationEntry

main_test.go:152: Failed to create MongoDB client: Failed to ping MongoDB:

server

selection error: context deadline exceeded, current

topology: { Type: Unknown, Servers:
bunk8s-fe.svc.cluster.local, Type: Unknown, Last error: connection
() error occured during
connection handshake: dial tcp mongodb
://mongo.bunk8s-fe.svc.cluster.local: connect: connection refused
}, ] }

[{ Addr: mongodb://mongo.

--- FAIL: TestMongoDBRoomAllocationEntry (10.00s)
FAIL
ok

showcase

10.101s

Listing 3: Test result logs after successful completion of the
broad integration tests with Bunk8s.

the Room service. When a user enters a room, a badge reader
to the Device service.
device sends a badge reader event
the Device service creates a device event
Following this,
containing the card and room numbers. The device event is
then sent to the Room service via the message queue. The
Room service updates the room allocation in MongoDB and
creates a room event that describes which user entered or
left the room and forwards it to the Dashboard and Shout
services via the two different message queues. After receiving
the Dashboard service updates a counter
the room event,
that tracks the number of users that have checked into a
room. On the other hand, the Shout service requests the name
of the user from the room event from the Account service.
The Account service provides the functionality for creating,
modifying, and listing the user accounts that are stored in the
MongoDB service. Note that, AMQP [22] is a message protocol
for transferring messages, while the wire protocol is the default
communication protocol used by MongoDB [23].

C. Broad Integration Testing Results

Broad integration tests cover multiple code paths through
several microservices and all of their components (§II). As
a result, multiple services of the room tracking application
will be under test (§IV-B). Since Bunk8s enables testing
of backend services without requiring the reconﬁguration of
ingresses in k8s, as well as the testing of backend services that
do not provide an HTTP or HTTPS interface, the services
Dashboard, Shout, and Account are not tested and omitted
from deployment. To this end, the microservices under test
are Device, Room, RabbitMQ, and MongoDB.

For writing the integration tests, we use Go and its default
testing package. We generate a badge reader event which is
sent to the Device service. Following this, the contents of
the room events created by the Room service are checked for
validity. Finally, we check the validity of the room allocated in
the MongoDB database. As described in §III-C, the test results

Supported Features
Broad integration testing
Narrow integration testing
Test framework independent
CI/CD Infrastructure independent
Test result publishing

Bunk8s [This Work]
-
(cid:7)
-
-
-

Octopus [10]
-
(cid:7)
-
-
(cid:7)

Istio [11]
(cid:7)
-
(cid:7)
(cid:7)
-

Jenkins [12]
(cid:7)
-
-
(cid:7)
-

TABLE I: Comparison of the different features supported by
Bunk8s with the other available tools for integration testing.
-Supported. (cid:7)No support.

are extracted and stored in artifact store provided by GitLab.
Listing 2 shows the logs of the test results after all broad inte-
gration tests have completed successfully. Bunk8s also extracts
test results if a test fails. For instance, Listing 3 shows logs of
the test TestMongoDBRoomAllocationEntry when no
connection to the MongoDB service could be established. As
a result, the test result publishing supported by Bunk8s allows
developers to easily analyse and debug their applications.

As a result, the different features supported by Bunk8s make it
easier for it to be integrated into an existing projects pipeline.

VI. CONCLUSION & FUTURE WORK

In this paper, we implemented and presented Bunk8s, a tool
for broad integration testing of microservices in Kubernetes.
Bunk8s is independent of the test framework used for writ-
ing or generating tests, independent of the infrastructure or
platform on which Kubernetes runs, and supports test result
publishing. Bunk8s is well suited for developers planning to
write a new test suite as well as for developers who wish
to revise and improve the already existing test suite of a
microservice application. In the future, we plan to extend
Bunk8s to support mocking of services. Furthermore, we plan
to add support for narrow integration testing in Bunk8s to
enable testing of multi-container pods. Support for FaaS based
CI/CD workloads is another future direction [29].

V. WHY BUNK8S?

REFERENCES

To support the integration testing of microservices inside
k8s several tools exist. These include Octopus [10], Istio [11],
and Jenkins [12]. Octopus allows developers to deﬁne inte-
gration tests with custom resource deﬁnitions [24]. For each
test, Octopus deploys a pod to k8s. Furthermore, it supports
re-running tests to prevent ﬂaky test results. The Istio testing
framework allows developers to test microservice applications
that use the Istio service mesh to handle the communication
between microservices. Developers can also use Jenkins to run
integration tests inside k8s by using the different Kubernetes
plugins [25]. A comparison of the different features supported
by Bunk8s with the other tools is shown in Table I.

Both Istio and Jenkins only support narrow integration
testing (§II). For Istio, this is possible due to the presence of
sidecar containers for pods. On the other hand, both Octopus
and Bunk8s support broad integration testing which enable the
testing of interactions between the application microservices
and the backend services. For all the tools shown in Table I,
the developers can use any programming language and test
framework for writing integration tests except Istio. Since Istio
is an expansion of the Go testing package it only supports
writing integration tests in Go. Moreover, Istio is dependent
on the usage of Prow [26] as the CI/CD system for running the
integration tests [27]. Similarly, using Jenkins would require
it’s introduction into a project and migration of at least parts of
the CI/CD pipeline to Jenkins if not already in use. In contrast,
both Bunk8s and Octopus can be used with any CI/CD tool
that supports Docker containers. A major drawback of using
Octopus is that after a test is ﬁnished it only reports whether
a test failed or was successful and does not extract the test
logs from the containers. This prevents an in-depth analysis
of test failures [28]. Moreover, Octopus is not open-source
and charges for its services. On the other hand, Bunk8s is
completely open-source and supports publishing of test results
since they are extracted from the test runner pod and are stored
on the pipeline runner (§III-B). Following this, the supported
pipeline mechanisms for test result publishing can be utilized.

[1] M. Fowler and J. Lewis.

(25.05.2014) Microservices.

[Online].

Available: https://martinfowler.com/articles/microservices.html
Testing

strategies

(2014)

in

[2] T. Clemson.
architecture.
microservice-testing/

[Online]. Available:

a microservice
https://martinfowler.com/articles/

[3] 14 Tech Companies Embracing Container Technology., https://learn.g2.

com/container-technology, accessed 09/29/2021.

[4] A. Beltre, P. Saha, and M. Govindaraju, “Kubesphere: An approach
to multi-tenant fair scheduling for kubernetes clusters,” in 2019 IEEE
Cloud Summit, 2019, pp. 14–20.

[5] DataDog

Container

Report,

https://www.datadoghq.com/

container-report-2020/#2, accessed on 11/11/2021.
[6] Kubernetes., https://kubernetes.io/, accessed 09/29/2021.
[7] Sysdig.
usage
security
report.
https://sysdig.com/wp-content/uploads/
2021-container-security-and-usage-report.pdf

(2021)
[Online]. Available:

container

Sysdig

2021

and

[8] Azure Kubernetes Service., https://azure.microsoft.com/en-us/services/

kubernetes-service/, accessed 09/29/2021.
(1.05.2012) Testpyramid.

[9] M. Fowler.

[Online]. Available: https:

//www.martinfowler.com/bliki/TestPyramid.html

[10] Octopus,

https://github.com/kyma-incubator/octopus,

accessed

on

11/11/2021.

[11] Istio, https://istio.io/, accessed on 11/11/2021.
[12] Jenkins, https://www.jenkins.io/, accessed on 11/11/2021.
[13] M. Fowler. (16.01.2018) Integrationtest. [Online]. Available: https:

//martinfowler.com/bliki/IntegrationTest.html

[14] Containerd., https://containerd.io/, accessed 09/29/2021.
[15] CRI-O-OCI-based implementation of Kubernetes Container Runtime

Interface., https://github.com/cri-o/cri-o, accessed 09/29/2021.

[16] Google Kubernetes Engine., https://tinyurl.com/5as5db77, accessed

09/29/2021.

[17] Helm, https://helm.sh/, accessed on 11/11/2021.
[18] Terraform, https://www.terraform.io/, accessed on 11/11/2021.
[19] GitLab

https://docs.gitlab.com/ee/ci/pipelines/pipeline

Artifacts,

artifacts.html, accessed on 11/11/2021.

[20] GitLab Inc.

(2021) What

is gitlab? [Online]. Available: https:

//about.gitlab.com/what-is-gitlab/

[21] Azure Container Registry, https://azure.microsoft.com/en-us/services/

container-registry/, accessed on 11/11/2021.

[22] O. Standard, “Oasis advanced message queuing protocol (amqp) version
1.0,” International Journal of Aerospace Engineering Hindawi www.
hindawi. com, vol. 2018, 2012.

[23] MongoDB wire protocol, https://docs.mongodb.com/manual/reference/

mongodb-wire-protocol/, accessed on 11/11/2021.
in Kubernetes,

[24] Custom Resource Deﬁnition

https://tinyurl.com/

puxwmfu4, accessed on 11/11/2021.

[25] B.

Szeti.

(29.06.2018) Running

bernetes.
running-integration-tests-kubernetes

[Online]. Available:

integration

ku-
https://opensource.com/article/18/6/

tests

in

[26] Prow,

https://github.com/kubernetes/test-infra/tree/master/prow,

accessed on 11/11/2021.

[27] S. Landow. Istio test framework. [Online]. Available: https://github.

com/istio/istio/wiki/Istio-Test-Framework

[28] A. Szecowka and K. Zydek. How we approached integration testing in
kubernetes, and why we stopped using helm tests. [Online]. Available:
https://kyma-project.io/blog/2020/1/16/integration-testing-in-k8ss
[29] A. Jindal, M. Gerndt, M. Chadha, V. Podolskiy, and P. Chen, “Function
delivery network: Extending serverless computing for heterogeneous
platforms,” Software: Practice and Experience, vol. 51, no. 9, pp.
1936–1963, 2021. [Online]. Available: https://onlinelibrary.wiley.com/
doi/abs/10.1002/spe.2966

