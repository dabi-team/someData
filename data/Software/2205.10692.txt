2
2
0
2

p
e
S
3

]
E
S
.
s
c
[

2
v
2
9
6
0
1
.
5
0
2
2
:
v
i
X
r
a

All You Need Is Logs: Improving Code Completion
by Learning from Anonymous IDE Usage Logs

Vitaliy Bibaev
JetBrains
Belgrade, Serbia
vitaliy.bibaev@jetbrains.com

Alexey Kalina
JetBrains
Munich, Germany
alexey.kalina@jetbrains.com

Yaroslav Golubev
JetBrains Research
Belgrade, Serbia
yaroslav.golubev@jetbrains.com

Alexander Bezzubov
JetBrains
Amsterdam, The Netherlands
alexander.bezzubov@jetbrains.com

Vadim Lomshakov
JetBrains
Saint Petersburg, Russia
vadim.lomshakov@gmail.com

Nikita Povarov
JetBrains
Amsterdam, The Netherlands
nikita.povarov@jetbrains.com

Timofey Bryksin
JetBrains Research
Limassol, Cyprus
timofey.bryksin@jetbrains.com

ABSTRACT
In this work, we propose an approach for collecting completion
usage logs from the users in an IDE and using them to train a
machine learning based model for ranking completion candidates.
We developed a set of features that describe completion candidates
and their context, and deployed their anonymized collection in the
Early Access Program of IntelliJ-based IDEs. We used the logs to
collect a dataset of code completions from users, and employed it
to train a ranking CatBoost model. Then, we evaluated it in two
settings: on a held-out set of the collected completions and in a
separate A/B test on two different groups of users in the IDE. Our
evaluation shows that using a simple ranking model trained on
the past user behavior logs significantly improved code completion
experience. Compared to the default heuristics-based ranking, our
model demonstrated a decrease in the number of typing actions
necessary to perform the completion in the IDE from 2.073 to 1.832.
The approach adheres to privacy requirements and legal con-
straints, since it does not require collecting personal information,
performing all the necessary anonymization on the client’s side.
Importantly, it can be improved continuously: implementing new
features, collecting new data, and evaluating new models — this
way, we have been using it in production since the end of 2020.

CCS CONCEPTS
• Software and its engineering → Software development tech-
niques; Automatic programming.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

KEYWORDS
anonymous usage logs, code completion, integrated development
environment, machine learning, A/B-testing

ACM Reference Format:
Vitaliy Bibaev, Alexey Kalina, Vadim Lomshakov, Yaroslav Golubev, Alexan-
der Bezzubov, Nikita Povarov, and Timofey Bryksin. 2022. All You Need
Is Logs: Improving Code Completion by Learning from Anonymous IDE
Usage Logs. In Proceedings of ACM Conference (Conference’17). ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Integrated development environments (IDEs) are among the most
important tools used in writing software code [16]. They provide
developers with various smart features that increase their produc-
tivity: highlighting of code, syntactic checks, various automatic
quick-fixes [28], automatic refactorings [25, 31], and others.

To evaluate how various features are being used, it is possible to
collect logs from IDEs. This can be used for high-level analysis of
user workflows [1, 2, 17] or for evaluating the usefulness of a partic-
ular feature, for example, automatic refactorings [27, 29]. However,
one needs to be careful when dealing with usage logs, similar to
any user data. Researchers often use the data from a limited group
of volunteers with explicit consent [38], while processing the data
of thousands of users in the wild requires stronger protection of
their privacy in accordance with various regulations, such as the
General Data Protection Regulation (GDPR) [43].

One of the key features that defines an IDE is code completion [1,
10, 18, 40, 42]. Code completion speeds up the programming process
by automatically suggesting code that the developer is about to
write, while also helping them avoid possible typos. An example of
code completion in action is presented in Figure 1.

IDEs based on the IntelliJ Platform [22] (IntelliJ IDEA, PyCharm,
WebStorm, etc.), developed by JetBrains [20], naturally also have a
code completion feature. The default implementation uses a static
analyzer to generate the candidates by parsing syntactic and se-
mantic models of the project and collecting the information about

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

V. Bibaev, A. Kalina, V. Lomshakov, Y. Golubev, A. Bezzubov, N. Povarov, T. Bryksin

user, and negative for all the rest. This way, no sensitive informa-
tion is collected (e.g., the user’s code), but the ranking model can
still be trained. We used a CatBoost model [33], because it is fast,
lightweight, and can be easily converted for the JVM.

We carried out two different evaluations of the described ap-
proach using the data from Python-based projects in PyCharm: an
offline evaluation and an online evaluation. For the offline evalua-
tion, we collected two datasets of logs from two different groups
of users: the first week contained 54,540 completion sessions and
was used for training the model, the second one contained 72,131
sessions and was held out for testing. In this setting, the proposed
ML-based ranking demonstrated the Recall at Top-1 (i.e., cases when
the correct suggestion is at the very top of the list) of 0.870 against
0.761 for default heuristics-based ranking.

However, while such an evaluation is much better than a syn-
thetic one, even it does not tell the full story. To see the results in
action, we also performed an online evaluation that constituted
an A/B test: some users were given the base code completion us-
ing heuristics, and some were given suggestions ranked using our
model. Such a setting allowed us to use more practical, intuitive
metrics of the completion quality. Among other things, this evalua-
tion showed that the average number of typing actions required to
finish the completion lowered from 2.073 for the heuristics-based
ranking to 1.832 for the ML-based one, indicating the real saving
of time for the users. The latency increased slightly from 92.3 ms
to 119.3 ms, thus remaining in the comfortable range.

One important aspect of using the EAP versions of an IDE is
that this allows us to repeat the described process cyclically, in
the new releases. This way, new data can be obtained from the
users in a constant stream, and, importantly, new models can be
evaluated and compared in live A/B tests, thus ensuring constant
evolution of the quality. This process takes place in EAP versions
of IntelliJ-based IDEs since the end of 2020.

Overall, the paper presents the following contributions:

(1) Approach for enhancing the quality of code completion in

real-world scenarios that:
— formulates code completion as a ranking problem;
— consists of a feature-based CatBoost model trained on
real anonymized user behavior data collected without
violating their privacy;

— is language-agnostic: while some specific features and
specific trained models can be used only for a specific
language, the approach as a whole, as well as a lot of
general features, does not depend on the language;
— meets the requirements of the real-world industrial ap-
plication: always produces syntactically correct code, re-
sults in a relatively small model (less than 500 KB) with
low inference time (20-30 ms);

— can be continuously employed in cycles of gathering data

and comparing models in live A/B tests.

(2) Evaluation of the proposed approach in two different set-

tings that employ real user data from PyCharm:
— offline evaluation on the held-out user data, which demon-
strates that ML-based ranking shows the Recall at Top-1
of 0.870 over the heuristics-based ranking that has the
Recall at Top-1 of 0.761;

Figure 1: An example of code completion in the IDE.

the used entities, as well as the given position of the caret. Then, a
heuristics-based ranking is used to sort the suggestions before pre-
senting them to the user. Despite the good quality and fast inference,
this leads to three major problems:

(1) the heuristics are based on certain assumptions that can be
hard to verify quantitatively and may be statistically wrong;
(2) over time, maintaining a large number of heuristics becomes
cumbersome and it gets harder to add new ones without
breaking the existing ones;

(3) supporting a new language for completion is difficult, even

when a lot of heuristics might be reused.

To overcome these problems and further improve code com-
pletion, an existing heuristics-based system can be augmented
by adding a machine learning (ML) based ranking as the final
step [6, 30, 35, 36]. Training an ML ranking model that will re-
arrange suggestions from the static analyzer requires ground truth
data, i.e., a labeled list of code completion suggestions. Existing
approaches can be divided into two principal categories from the
standpoint of the data that they use for training: (1) synthetic: that
analyze the existing body of source code [14, 30, 36], and (2) real
user behaviour: that employ logs, source code, and edits from real
users [4, 37]. In a recent paper, Hellendoorn et al. [13] showed that
synthetic data significantly differs from the real-world usage, so
training on it may cause worse performance for the end-users. For
this reason, utilizing the usage logs seems like a promising solution.
In this work, we propose and evaluate an approach that allows to
use the information from the users without violating their privacy
or collecting any personal information. We propose to decouple
the extraction of data by designing a set of features that are com-
puted on the side of the client and are then collected anonymously
from logs. We then use this data to train a model for ranking the
candidates for code completion and compare its performance with
the default heuristics-based approach.

Specifically, we defined a number of different features that de-
scribe the prefix before the caret, the context around the sugges-
tions, the entities defined and used before the caret, the history
of selecting any given suggestion by the user, etc. Then, we inte-
grated the collection of these features into the Early Access Program
(EAP) [19] of IntelliJ-based IDEs and collected anonymized logs
from the users who agreed to send them to us. The logs include the
values of the defined features for each suggestion, as well as the
label: positive for the suggestion that was actually selected by the

All You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs

Conference’17, July 2017, Washington, DC, USA

— online evaluation via an A/B test in the IDE, which shows
that the average number of typing actions necessary to
perform completion lowers from 2.073 for the heuristics-
based ranking to 1.832 for the ML-based one.

(3) Insights into the results of using the proposed approach in
a real-world industrial IDE and dealing with various con-
straints that such a setting implies. Models obtained using
the described approach are currently being employed in al-
most all major IntelliJ-based IDEs.

The rest of the paper is organized as follows. In Section 2, we
briefly discuss the problem of code completion and the constraints
it faces in the real-world applications. Section 3 describes in de-
tail the proposed approach to collecting the data and building the
model, while Section 4 describes two different evaluations that we
conducted. In Section 5, we share insights about the usage of the
proposed approach and discuss open challenges in the field. Sec-
tion 6 describes the existing related works, Section 7 discusses the
threats to the validly of our work, and, finally, Section 8 concludes
the paper and discusses possible directions for future work.

2 BACKGROUND
The idea of code completion as an IDE feature is to suggest code
to the user before they can manually type it, thus saving them the
time and effort. Additionally, code completion can help in project
exploration, providing new users with an opportunity to see entities
from different parts of the code base. Since there is a lot of variance
in the process of writing code, the IDE usually provides a ranked
list of possible completions for the user to choose from.

Code completion systems vary by the granularity of their sugges-
tions: from tokens and API usages to lines or even entire methods.
While early research focused mostly on narrow API-level comple-
tion [6, 15, 32], modern language models based on neural networks
vary from fine-grained, using every possible lexical token type
(delimiters, operators, white spaces, keywords, etc.) [14], to coarse-
grained, predicting entire lines of code [9, 44].

In this work, we specifically focus on token-level code comple-
tion as the most balanced and important option: it is more practical
than heavy full-line code completion and more general than just
API recommendation. In practice, over time, effective token-level
code completion can save the users a lot of effort. However, our
approach is easy to extend to other types of completion, and we
leave applying the usage of logs for the full-line version of code
completion [44] for subsequent work.

The set of possible candidate suggestions in code completion is
determined by the implementation of the candidate provider [41].
Code completion systems in IDEs rely on static analysis to retrieve
the list of possible candidate suggestions, since it is vital for sugges-
tions to be syntactically correct. The list of candidates is generated
by considering the position of the caret, the grammar of the pro-
gramming language, code entities that are defined and used in the
opened file, other files in the project, and the used libraries.

Before showing the list of candidates to a user, the IDEs rank
them, typically employing a series of hard-coded heuristics. These
heuristics range from trivial and language-agnostic, e.g., which enti-
ties were used the most in the opened file, to specific and language-
dependent ones, e.g., type matching.

Thus, there are at least two different ways to improve token-
based code completion: (1) change the set of possible candidate
token suggestions, or (2) change the order in which these sug-
gestions are presented to the user. In this work, we focus on the
second one, building on a number of works that enhance the qual-
ity of ranking by employing statistical and machine learning ap-
proaches [4, 34, 35, 41]. It is vital to focus on improving the ranking
of the suggestions, since in general, modern static analyzers already
generate candidates very well, and their “recall” is rather good, so
it becomes a matter of prioritizing syntactically correct suggestions.
The general idea of using machine learning for this task consists in
choosing the necessary features that describe the context around
the completion instance and the candidates, and collecting data for
training the ranking model. Since the model is trained to be used
in an industrial setting, inside the user’s IDE, this imposes a set of
important constraints on the entire process.

Firstly, because of the slow and energy-consuming nature of
the model training, it must happen on a remote server, and the
user should simply receive a pre-trained model as a part of their
IDE. Secondly, the inference of the model has to take place on the
user’s machine, be fast and reliable. This includes not using any
special hardware (i.e., GPU) and not accessing the network. The
last constraint is important for several reasons:

— Inferencing the model over network introduces additional

latency to the process.

— Developers might work without access to the internet.
— Developers may not be comfortable with sending any infor-

mation to a remote server.

The concern about the privacy of the user’s data is crucial in
this area, and also directly relates to the data collection. As was
mentioned above, training the necessary model requires having the
labeled data of code completion sessions. Some of the existing works
use synthetic data and analyze the existing body of source code [14,
30, 36]. However, Hellendoorn et al. [13] demonstrated that in the
case of code completion, synthetic data may differ significantly from
the real-world data, so such data collection pipelines are inherently
flawed. For this reason, other works analyze real user behaviour:
they gather the data from logs, source code, and edits made by real
users [4, 37]. In such a setting, privacy requirements are also critical
and must be taken into account.

In short, the privacy requirements regulate the gathering of infor-
mation that might identify the user. While this obviously includes
any personal information like the name or the e-mail, in certain
legislatures, this can also include the code itself. In particular, in
recent years, a lot of laws were passed such as EU’s General Data
Protection Regulation (GDPR) [43] or California Consumer Privacy
Act (CCPA) [39] that carefully regulate users’ privacy and data
collection. User data should be collected only in anonymized, de-
personalized form, without their code or any code metric that is too
revealing of their identity. This is crucial both in the sense that it
incentivizes inferencing the model on the user’s machine, without
sending any data, and also impacts the possible ways of collecting
user data for training.

In this work, we propose, describe, and evaluate improving the
performance of code completion by collecting anonymized logs of
real completion sessions and training an ML model based on them.

Conference’17, July 2017, Washington, DC, USA

V. Bibaev, A. Kalina, V. Lomshakov, Y. Golubev, A. Bezzubov, N. Povarov, T. Bryksin

Figure 2: The pipeline of the proposed approach.

3 APPROACH
Employing user logs to improve code completion requires finding
a proper way to collect and utilize these logs. Taking into account
the mentioned limitations, we propose the following pipeline for
collecting the data and training the model, shown in Figure 2:

(1) A set of features is calculated on the user’s machine during

their completion sessions.

(2) This data is anonymized and collected as logs without any

identifying personal information.

(3) A model is trained and evaluated on the server using a large

amount of such anonymized data.

(4) This pre-trained model is deployed to future users, from
whom we can collect new logs to evaluate the model.

In such a setting, all the requirements are met: no sensitive per-
sonal data is collected (neither the suggestions nor the surrounding
code), the resource-expensive training of the model happens on
the server, and the usage of the model requires nothing from the
user. What is even more important, however, is that the described
sequence represents just one iteration of the process: in parallel
to evaluating a model, new data can be collected from other users,
new models can be trained, etc. This opens up the possibility to
compare the models in a real industrial setting. The implementation
of the described pipeline requires three key things:

(1) Designing a set of features to be collected and a data format

for storing and utilizing them.

(2) Selecting a machine learning model to train on this data that
could be conveniently inferenced in the JVM-based IDE.
(3) Creating a setting for continuously collecting data and test-
ing models in the form of A/B tests using the Early Access
version of IntelliJ-based IDEs.

Let us now describe each of these three items in greater detail.

3.1 Data Collection
As was previously stated, we cannot train the necessary model
on the user’s machine or send personal information about them
(including the code) because of privacy concerns. Because of this,
the data collection consists in (1) designing a set of anonymous
features that would describe the completion session, and (2) sending
this data to the server and transforming it for training the model.

It should be noted right away that while a lot of modern works ar-
gue in favor of end-to-end neural models instead of manual feature
extraction [41], in our setting feature extraction represents a bene-
fit: it provides a clear organizational way to control and manually
audit what exactly gets collected and sent, via code reviews.

The collected data consists of completion sessions, i.e., cases when
the pop-up window with suggestions appeared. Each completion
session, in its turn, contains one or several look-ups, i.e., specific
lists of suggestions. One session can have several look-ups if the
user typed additional characters when the pop-up already appeared,
thus changing the context and filtering the list. Finally, each look-
up consists of specific suggestions, i.e., individual tokens that are
ranked for the user to choose from.

Feature Selection. The aim of the features is to describe the
3.1.1
completion context of any given suggestion as best as possible
without actually giving away any of the written code. In this case,
the context relates not only to the code right before the caret, but
anything that can influence the choice: the information about the
suggestion itself, the history of choosing, etc. In our experiments,
we evaluated many different features and their importance for the
quality of the model. It is very important to note that the proposed
approach is inherently language-agnostic, so some basic features are
also the same for different languages, while some features remain
language-specific.

There are several main groups of language-agnostic features:

— Information about the prefix. A major source of informa-
tion about the completion is the prefix, i.e., the already typed
part of the token, right before the caret. The information
includes the length of the prefix, the number of matched
characters between the prefix and the suggestion, whether
the match is case sensitive, whether it is exact, etc.

— Syntactic context. The completion session does not hap-
pen in a vacuum, it takes place in a certain code location,
in a certain part of the codebase. All of this can influence
the necessary token. Thus, features that describe the syn-
tax include whether the suggestion is a language keyword,
whether the suggestion is the element from the same file or
module, whether it is from a third-party library, etc.

— Syntactic history. Additionally, and crucially, it is very im-
portant to catch the dynamic nature of software development

Calculate a set ofanonymized featuresUser’s machineRemote serverTrain an ML modelfor ranking candidatesUse the pre-trainedranking modelEvaluate the model’sperformanceCollect logsCollect logsDeploy modelAll You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs

Conference’17, July 2017, Washington, DC, USA

and not treat the opened file as a static object. Previous ac-
tions of the user may indicate their intent better than the
basic proximity of objects in the code. As such, we analyze
the prior navigation of the user to see whether they navi-
gated to the definition of the suggestion before, etc.

— Session history. Also, some features describe the temporal
aspect of completion sessions: the duration of a given session
in seconds, whether the user has already selected the given
suggestion before, etc.

Language-specific features include the context of the sugges-
tion (if block, for block, etc.), the type of the suggestion (method,
class), the usage frequency, or something even more specific like
the number of lines between the caret and the __init__ method
of the enclosing class in Python.

Initially, there were several hundred different features for each
language. Feature selection consisted of two stages. On the first
stage, we filtered out all the features that were not used by the
model during training. On the second stage, we calculated permuta-
tion importance [5] for the features and filtered out those that were
not useful. All the remaining features were used in the final pro-
duction model. We evaluated our approach on different languages:
for example, for the Java language, the total final number of the
used features was 146, for the Python language, it was 188.

3.1.2 Data Format. Once the features are chosen, they are deployed
for the collection of logs into the Early Access version of the neces-
sary IDE. The collection occurs for each action in each completion
session. The possible actions are: the completion session started
(i.e., the pop-up appeared); the user typed an additional symbol
(thus adding another loop-up to the session, changing the prefix,
the context, and filtering the list of suggestions); the user went up
or down the list of suggestions using arrow keys; or the completion
session ended (in one way or another). The collected data contains
the following information:

— User ID, random and anonymous.
— Timestamp of the event.
— The way the completion session started: manually, by press-
ing the specific hot-key combination, or automatically, after
the user started typing.

— Project-, file-, or snippet-level features that are the same for

all suggestions in the look-up.

— The list of individual suggestions and their features.
— The way the session ended and the selected option, if any.

There are four possible ways for the completion session to end:

— Explicit select, if the user selected a suggestion and it was

successfully auto-completed.

— Typed select, if the user completely manually typed a sug-

gestion that was in the list.

— Explicit cancel, if the user explicitly canceled the comple-

tion (e.g., by pressing ESC or reverting).

— Typed cancel, if the user completely manually typed a token

that was not in the list, thus also ending the session.

A simplified example of the collected logs is presented in Figure 3.
In this example, the full logs (Figure 3a) show that the completion
session started, then two characters were typed, after which the
user pressed the down arrow once and selected the token, thus

successfully finishing the completion. For each action, the full list
of features is recorded. Figure 3b and Figure 3c show the suggestions
that were present in this example before selection, and several basic
features calculated for them.

For training the model, we use data points that ended with Ex-
plicit select or Typed select, i.e., where there is ground truth. After
the user selects a certain suggestion, it is labeled as the correct
option in all the look-ups in this completion session. For example, if
the user selected the token after typing two characters, the sugges-
tion was already in the list after the first character, perhaps, lower,
and can still be considered the correct option in that list. Figure 3d
shows that the selected suggestion (“setOut”) was labeled with 1
as the target value, while the other suggestion was labeled with 0.
The remaining data can be used to evaluate the quality of the
used completion. The timestamps can be used to calculate the time
that the user spends evaluating their options, the typing actions
can be used to see how many are required on average to finalize
the completion, and we can also measure how many sessions ended
in each of the four possible ways. These different metrics will be
described in greater detail in Section 4.

3.2 Model
Similar to the other works motivated by industrial application [3,
41], we focus on improving the ranking of the candidates and for-
mulate this task as a Learning to Rank problem [26]. Researchers
explored a similar architectural approach [6, 35], referred to as
structural feature selection [13] and feature-based models [41], only
for the limited context of API recommendations over the closed
vocabulary of possible API calls.

In Section 2, aside from the limitations that relate to privacy,
we also mentioned performance-related limitations. The model
must not rely on any specific hardware and must be lightweight
to operate on the user’s machine. More specifically, the size of the
model should not be more than 2 MB and the inference must take
tens of milliseconds in order not to make the completion generation
too noticeable for the user. Such strict limitations come from the fact
that during the completion session, the model will be inferenced
after each new character. Last but not least, the model must be easy
to integrate into the JVM-based architecture of the IDE.

To meet these requirements, in particular the last one, we de-
cided to use a model based on the Decision Tree algorithm. This
architecture is a good choice for us, because it is easy to convert
into the if-else code representation, and thus it can be used inside
the JVM process. Specifically, we used a CatBoost [33] model with
a built-in QuerySoftMax loss function [7, 8].

We experimented with a number of alternative models, including
lightGBM [21], as well as feed-forward and Transformer-based
neural models. None of these options came close to the performance
and the operational simplicity of the tree-based model. Given the
severe cap on the size of the model, it is difficult to train a well-
performing Transformer, and no other architectures demonstrated
results better than CatBoost. In the end, the CatBoost model was
the only one actually deployed to production and thus will be the
focus of the rest of the paper.

As discussed in Section 3.1.2 and shown in Figure 3d, an in-
dividual data point consists of a list of several code completion
suggestions, the accepted one being a positive example and the

Conference’17, July 2017, Washington, DC, USA

V. Bibaev, A. Kalina, V. Lomshakov, Y. Golubev, A. Bezzubov, N. Povarov, T. Bryksin

Figure 3: Data collection and data format. In this example, the user typed two characters of the token and selected the second
suggestion. (a) The simplified general form of the collected logs during actions, (b) the view inside the IDE right before selec-
tion, (c) the JSON structure of the collected data, including several simple features, (d) the final representation of the data with
the target column that the model trains on.

rejected ones being negative. It is also important to understand
that the users mostly care about what is on the very top of the list.
For this reason, we needed to choose a loss function that would
prioritize the correct suggestion being at the top of the list. Cat-
Boost comes with such a loss function, QuerySoftMax [8], described
previously in the work of Cao et al. [7]. In this setting, given a list
of suggestions, the model learns to boost the correct one to the top.
The model was trained using the native CatBoost framework,
then converted into the lightweight representation of a series of
if-else operators. The final model was implemented and infer-
enced in the Java language.

3.3 Continuous Collection and Validation
One of the most important features of the proposed approach is
that the gathering of new data and the evaluation of the models can
occur in parallel, since both processes rely on collecting logs. At
the same time, the cyclical nature of IDE releases means that this
process can be repeated over and over, always collecting new data
(with new features), training fresh models, and evaluating them on
real users against the current best.

This process relies on the Early Access Program (EAP) [19] pro-
vided for IntelliJ-based IDEs. It works as follows. In the two-month
period before each release of an IntelliJ-based IDE, the EAP takes
place. If a user agrees to participate in the EAP, they are given free
access to a beta-version of the upcoming commercial release, in
which they can evaluate and try out new features. In exchange,

they agree to provide anonymous logs of their experience that the
developers of IDEs can use to fix any existing bugs. In this setting,
it is not only possible to collect data about code completion or
evaluate a single model, but also carry out a fully-fledged A/B test
in the process. The idea of this cycle is demonstrated in Figure 4.
Let us now describe it further.

Let us say that in the EAP of the version 𝑖, a certain model 𝑀𝐿𝑖
was working and some data was collected. Then, we take this data
and train a new model that we think will do better, let us call it
𝑀𝐿𝑖+1. At the same time, perhaps, we already developed some new
features that we want to evaluate. So, when the EAP version 𝑖 + 1
comes around, we implement the collection of the new features,
and then divide the users of the EAP into three groups:

(1) The MLi group receives the 𝑀𝐿𝑖 model that was obtained in

the previous version, the current best.

(2) The MLi+1 group receives the 𝑀𝐿𝑖+1 model, the new one.
(3) Finally, the separate Logs group also receives the previous
𝑀𝐿𝑖 model and is used to collect data for the next cycle.
From all three groups, the logs are collected. This way, an A/B
test is being conducted between the two ML-based models, and
with the help of the metrics calculated from logs and statistical tests,
we can discover the best of them. Meanwhile, the logs collected
from the third group can be used to create a training dataset, on
which we can train a new model, 𝑀𝐿𝑖+2. Then, during the EAP
version 𝑖 + 2, we can repeat the process, comparing models 𝑀𝐿𝑖+1
and 𝑀𝐿𝑖+2, collecting new data with the new features, and so on.

{  : {"event"    : ,"query_length"2    : ,"static_context"true    : ["items"      {        : ,"type""ﬁeld"        : ,"lentgh"3        : "isStatic"true,        ...      },      {        : ,"type""method"        : ,"length"6        : 1"parameters",        ...      }    ]  }}PositionQuery lengthTypeLengthParametersIs static?...Selected...02ﬁeld31...012method61...1...1559397061953 START           {: , : , : [...]}"query_length""static_context""items"0true TYPING          {: , : , : [...]}15593970619571"query_length""static_context""items"true TYPING          {: , : , : [...]}15593970621412"query_length""static_context""items"true15593970633461 DOWN            {: , : , : , : [...]}"selected_position""query_length""static_context""items"2true EXPLICIT_SELECT {: , : , : , : [...]}155939706365312"selected_id""query_length""static_context""items"trueacbdAll You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs

Conference’17, July 2017, Washington, DC, USA

Figure 4: The continuous way of collecting the data and comparing the models in the Early Access versions of the IDEs. In
every EAP, one group of users gets the current best model and another gets the new tested model, between which an A/B test
is run. At the same time, from a separate groups of users new logs are being collected that are then used to build a new dataset
and train the next iteration of the model.

It is also possible to compare more than two models at once and
fit several iterations into a single EAP cycle. In this sense, the EAP
is very convenient, because it is experimental by definition, and so
the users are expecting to receive frequent updates that change the
behavior of the IDE — something that would be unacceptable in a
major release. The described process has been running in the EAPs
of major IntelliJ-based IDEs since the end of 2020. This allowed us
to evaluate different models and a lot of different features.

4 EVALUATION
To highlight the usefulness of the proposed approach for using
logs, as well as the ability of the CatBoost-based model to improve
the quality of code completion, in this section, we describe two
different evaluations that we conducted to compare it with the
baseline heuristic-based ranking.

4.1 Data
The data for the experiments was collected in September and Octo-
ber of 2020 from Python-based projects in PyCharm. Overall, we
collected two sets of data in two consecutive weeks. The first set
contained 54,540 completion sessions from 2,623 unique users. This
set was split into two in the ratio of 80%/20% of individual users:
one was used for training the model, and the other was used for
tuning hyper-parameters. The other set, collected during the sec-
ond week, contained 72,131 completion sessions from 2,086 unique
users, different from the ones in the first week. This set was held
out for testing. This way, the testing data was located later in time
from the training data and came from different users. The model
utilized 188 Python-based features described in Section 3.1.1.

4.2 Offline Evaluation
4.2.1 Methodology. The first evaluation was carried out offline,
meaning that after the data from Section 4.1 was already collected,
the model was trained on the data from the first week and then
tested on the held-out data from the second week. On the same
held-out data, the heuristics-based ranking was also evaluated.

Table 1: The results of the offline evaluation. R@K stands for
Recall at K, init stands for initial look-ups.

CC System R@1all R@5all R@1init R@5init
Heuristics
0.918
0.959
CatBoost

0.957
0.981

0.761
0.870

0.634
0.799

To evaluate the performance of the models, we used the Recall
at K metric (R@K). This metric represents how often the correct
answer was in the top 𝐾 suggestions of the look-up. Naturally, R@1
is especially important, since it tracks how often the correct answer
was at the very top, convenient for the user.

Additionally, besides calculating the metric for all look-ups in
all completion sessions, we also separately calculate them only
for the initial look-ups, meaning the first look-ups in each session,
before typing any additional characters. This metric correlates well
with the user experience, since it is very important to suggest the
correct item from the very start — if the user has to type additional
characters, they are unlikely to pause and analyze the list after
every keystroke, at this point, they are more likely to simply type
the token themselves.

4.2.2 Results. Table 1 summarizes the comparison between the
default ranking and our model. It can be seen that our model pro-
vides better results for all the evaluated metrics. The correct token
is more often shown in the first position, and is higher located over-
all, with both R@5 metrics reaching very high values. The largest
increase can be seen for the most important metric — R@1init, or
the ratio of correct suggestions at the very top of the list during the
initial look-ups. The CatBoost model increased this metric by 16.5
percentage points, from 0.634 to 0.799, indicating a much better
performance for the user in the most crucial moments.

It is interesting to note that different types of tokens demon-
strated different increases in recall. For example, functions from
the global scope (e.g., abs()) demonstrated a significant increase
in quality. There are a lot of such functions in Python that are diffi-
cult to complete using heuristics due to the lack of type checking,

EAP i+2MLiAB-testMLi+1Logsi+2Dataseti+2MLi+1AB-testMLi+2Logsi+3Dataseti+3......EAP i+1Time......Conference’17, July 2017, Washington, DC, USA

V. Bibaev, A. Kalina, V. Lomshakov, Y. Golubev, A. Bezzubov, N. Povarov, T. Bryksin

whereas an ML model learned how they are used. An opposite exam-
ple, where the increase in recall was small, is API calls. The reasons
for this are that, firstly, the default completion already works well
in these cases, and secondly, it is often difficult to understand what
API method to use from the local context. A promising direction
for such cases is to learn from other API usages [6].

4.3 Online Evaluation (A/B Test)
4.3.1 Methodology. The offline evaluation demonstrated good re-
sults, but it is very important to understand its limitations. While
these results represent realistic usage scenarios [13], the offline
metrics themselves are only proxy-metrics for real usability im-
provements: having the correct result higher in the list indicates
more efficient work, but does not directly show it.

To explicitly measure more interpretable, human-oriented met-
rics, we conducted an online evaluation in the form of an A/B test.
The implementation of the A/B test was the same as described in
Section 3.3, only with the evaluated models being our CatBoost
model and the default heuristics-based ranking. In total, 231 users
were allocated to the heuristics-based Control group and 246 users
— to the CatBoost group. From the first group, 66,144 completion
sessions were collected, from the second — 74,346 sessions.

The setting of an A/B test allows us to use all the collected
data described in Section 3.1.2 to further measure the performance,
using several dozen specific metrics that describe every aspect of
the completion sessions. We will report several key ones:

— Explicit select. Firstly, we can simply measure the fraction
of the sessions that ended in the explicit selection of the
token, the best possible result.

— Typed select. In contrast, we can measure the fraction of
sessions that ended with typed select, meaning that the user
typed the token themselves, even though it was in the list.
— Typing actions. To measure the actual increase in the users’
productivity, we can calculate the average number of typing
actions (typed characters) in the completion session. For this
metric, we used a cut-off at 0.99 percentile to remove anoma-
lous completion sessions with an extremely large number of
typed characters.

— Prefix length. To specifically gauge the effectiveness of
explicit select, we can also compare the average length of
prefix at the moment of explicit selection. This will also
demonstrate when it is necessary to type fewer characters
before explicitly finishing the session.

— Manual start. Finally, to measure the overall reliance of
users on code completion, we can measure the fraction of
sessions that were started manually, i.e., using the hot-key
combination.

To test the statistical significance of the difference between mod-
els, we employed bootstrap [12]. When comparing the models,
we form 1,000 bootstrapping re-samplings of completion sessions
grouped by users. For all the metrics, we report the obtained 𝑝-value,
and consider the result significant if 𝑝 < 0.01.

4.3.2 Results. Table 2 summarizes the results of the online eval-
uation comparing the models. Once again, it can be seen that all
the metrics indicate the better performance of the CatBoost-based
model trained on users’ logs. The fraction of the sessions that ended

Table 2: The results of the online evaluation. The bold font
indicates better results (whether lower or higher). The star
indicates a statistically significant result (p < 0.01).

Metric

Heuristics CatBoost

p-value

Explicit select (sessions)
Typed select (sessions)
Typing actions (symbols)
Prefix length (symbols)
Manual start (sessions)

0.247
0.416
2.073
2.628
0.047

0.292
0.346*
1.832*
2.275*
0.079*

0.018
< 0.001
0.008
< 0.001
< 0.001

with explicit selection increased, however, this result did not pass
the statistical significance test. At the same time, the fraction of
sessions that ended with the manual typing of the entire token
decreased significantly. Next, we can see that the number of the
necessary characters to type also decreased. The overall average
number of typing actions decreased from 2.073 to 1.832, and the av-
erage length of prefix at the moment of explicit selection decreased
from 2.628 to 2.275. These two metrics indicate that the model ac-
tually makes it easier for the users to input a token. Finally, the
fraction of sessions that started manually increased significantly,
from 0.047 to 0.079, which might indicate the users’ interest in
obtaining the results of code completion.

4.4 Overall Performance
Lastly, it is necessary to discuss whether the obtained model com-
plies with the limitations described in Section 3.2, since this also
directly impacts the comfort of the end-users.

The size of the trained model was 366 KB, which comfortably fits
into the desired range. As for the latency, the A/B test showed that
when moving from heuristics to CatBoost, it increased from 92.3
ms to 119.3 ms (𝑝 < 0.01). This corresponds to the model adding less
than 30 ms for inference, thus also remaining in the comfortable
range, virtually unnoticeable for the user.

Overall, it can be seen that the Decision Tree based model trained
on real usage logs demonstrated its superiority over the default
heuristics-based ranking in all tested settings, while remaining
small enough and fast enough to be used in production. Even more
importantly, the proposed approach allows us to collect more data,
design more features, and evaluate more models continuously, mak-
ing sure that they always remain relevant.

5 DISCUSSION & OPEN CHALLENGES
5.1 Code Completion
5.1.1 Model. We show that leveraging real-world structured IDE
usage logs is beneficial for both training ML models (similar to Aye
et al. [4]) and evaluating their performance (similar to Proksch et
al. [34]). However, we only use anonymous pre-extracted features
instead of the full record of development history or edit context.

Svyatkovskiy et al. [41] argue in favor of using end-to-end neural

networks, and point out that feature-based models...

— ...depend on hard-coded features, thus missing the opportunity
to learn richer features directly from the data. Although this is
true, in our case, it becomes an advantage — we use this as a
safeguard mechanism to control what data gets collected,
and thus prevent gathering sensitive information.

All You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs

Conference’17, July 2017, Washington, DC, USA

— ...introduce difficulties in manually designing and extracting
relevant features that cover as many cases as possible. This
is also true, but every major language has a dedicated team
to support it, and also, organization-wide tooling and
infrastructure allow us to automate the experiments and
lead to continuous improvements of the overall system.
— ...learn about individual APIs and cannot generalize to un-
seen ones. Our approach does not have this problem, as for
providing the list of suggestions, it relies on project-wide
static analysis and context-specific feature extraction,
and not on a global vocabulary.

As far as a specific model goes, we decided on using Decision
Tree based models, specifically, CatBoost. This choice is dictated
mainly by the strict limitations that our task imposes. Such models
are simple, interpretable, and have great runtime performance:
small size and low latency. Additionally, they turned out to be
simple to work with: they provide fast training, great production-
grade tooling, and, importantly, they are easily converted into an
intermediate representation that allows to re-use them between
different ecosystems (e.g., Python and JVM).

At the same time, it must be noted that the practical nature of
our task does not allow us to claim state-of-the-art results or even
compare directly with many models from the literature. Our goal
in this continuing research is precisely to find models that would
improve the user experience while being lightweight and easy to
use in an actual IDE on a consumer-grade device.

5.1.2 Data. An interesting open question when using logs of com-
pletion sessions is what exactly to consider positive examples and
what to consider negative examples. A default idea that we used
consists of taking all the sessions that ended with a certain positive
outcome (i.e., Explicit select or Typed select), using their selections
as positive examples and anything that was not selected as negative
examples. However, this leaves out all the “fully negative” cases,
i.e., Explicit cancel and Typed cancel that can still provide negative
examples. Also, it can be noticed that we use Typed select as a source
of positive examples, however, in our A/B tests (see Section 4.3),
we try to lower the ratio of users who type the token themselves.
This can also be considered when selecting positive and negative
examples. Even more granularly, we can say that since our goal is
to make the users type less (hence, using typing actions as a metric),
not all positive examples are of the same value. It is possible to
introduce a weight to the examples based on their length, to facili-
tate the model to improve the raking of the longer suggestions. For
example, users may fully type very simple keywords like for or
def almost immediately, thus making them positive examples of
the Typed select class, however, they might not be what we want
the model to learn.

5.1.3 API usage. Our analysis of the user logs shows that code
completion demonstrates the worst results when it is necessary
to suggest API calls — both internal and external. Oftentimes, the
information about the local context is not enough, be that file,
module, or even the entire project, — instead, one may look at how
a particular API is used in other projects. A promising direction
is using a corpus of open-source code to learn similar context and
incorporating this information into the ranking.

5.2 Logs and Infrastructure
Importantly, the goal of our research and this paper does not lie
only in the area of code completion. Rather, we want to emphasise
the importance of structured user logs, and how a pipeline for their
collection can be used to both develop features and evaluate them.
Besides code completion, the same described infrastructure can
be used to improve other features of the IDE: refactoring recommen-
dation [23], code smell removal [11, 24], and others. Collecting user
logs allows us to not make any assumptions about their behavior,
or at least move away from them, and instead continuously take
into account their feedback. This pipeline can be used to bridge
academic results with practical applications. If researchers develop
a new model for ranking code completion, we can collect its output
as a feature, use it when training our models, and see in practice
whether it improves the user experience.

Overall, our experimental results and the experience of running
such a system in production for more than a year demonstrate
that it is a valuable tool for improving the IDE. We were able to
run the described pipeline for many prominent languages — Java,
Python, Kotlin, JavaScript, TypeScript, Ruby, Go, and others, —
with the infrastructure of the experiments being largely reused.
The default completion ranking models in many IntelliJ-based IDEs
right now are the ones trained on the user logs. Moreover, the
process continues to this day, with new features being evaluated
and new models being tested.

6 RELATED WORK
A lot of research has been dedicated to improving code comple-
tion using machine learning methods. As we already mentioned, a
principal distinction between different works is whether they are
trained on synthetic data or the real user data [13]. In this section,
let us describe several key studies.

Bruch et al. [6] proposed to use three simple ML approaches
to improve code completion, namely, a straightforward frequency-
based system, an association rule mining, and a modification of
the 𝑘 Nearest Neighbors approach. The authors focused on the API
recommendations and used synthetic data: an existing code base,
where some method calls were removed to simulate completion
queries. Of the three tested models, the latter showed the best
results in terms of the F1-measure.

Proksch et al. [35] proposed to use Bayesian networks instead.
Their work is also limited to API calls and deals with synthetic data,
however, the authors suggested that a proper evaluation of a code
completion system must rely not only on the quality analysis, but
also on the performance analysis. For this reason, the authors also
took into account the size of the developed model and the inference
speed, and tested it on queries of different size.

More recently, Svyatkovskiy et al. [41] evaluated end-to-end
neural networks instead of feature engineering based approaches.
The authors also formulated the task as a learning to rank problem
and leveraged static analysis as a candidate provider. Similarly,
the authors also took into account the importance of model size
and inference speed, and thus evaluated them too. In a thorough
comparison, the authors evaluated different token encoders and
different context encoders. At the same time, in this work, only the
API recommendations are addressed, and synthetic data is used (the

Conference’17, July 2017, Washington, DC, USA

V. Bibaev, A. Kalina, V. Lomshakov, Y. Golubev, A. Bezzubov, N. Povarov, T. Bryksin

source code of the most-starred Python projects). The approaches
proposed by Svyatkovskiy et al. have been used as a base for code
completion solutions for the Visual Studio Code IDE [40, 42].

The importance of using real-world user data was carefully stud-
ied in the seminal paper by Hellendoorn et al. [13]. The authors
collected a dataset of 15,000 real completions conducted by 66 users
and compared them with existing synthetic benchmarks. Not only
did the authors find that some state-of-the-art techniques demon-
strate significantly worse performance on the real data, they also
show that some features of the real-life code completion usage are
invisible in a synthetic setting: specifically, the users spending the
most time on the infrequent tokens that the models recommend
with even worse quality.

Similarly, Proksch et al. [34] evaluated different approaches on
the real-world data of 7,157 queries and also showed that synthetic
evaluations provide unrealistic numbers when compared to the
ground truth. However, these works used real user data only for
offline evaluation, not for actually training better models on them
— probably, since it is difficult to collect a necessary amount of data
in the research setting.

Finally, Aye et al. [4] did train a model on the real-world data,
and also carried out an A/B test to evaluate it. More specifically,
the authors studied the Hack dialect of PHP and trained end-to-
end neural network models on three different datasets: (1) Baseline
— synthetic static codebase of nearly one million source files, (2)
Autocompletion — real accepted completions inside the IDE, and
(3) Edit — code edits logged during file-save operations. Then,
among other experiments, the authors conducted two live A/B
tests, comparing models trained on two latter datasets to the model
trained on the Baseline dataset. The results showed that the model
trained on the Autocompletion dataset performed the best, indicating
the usefulness of the user behavior logs. However, it is important
to note that this work was carried out inside Facebook, which is
why the authors could collect personal information (specific code
snippets in logs), so, while their datasets are undoubtedly large,
they are not limitless and may not generalize well to all users.

Overall, it can be seen that our work compliments the existing
ones: it uses the real-world usage logs to both train and evaluate
models, develops an approach that takes into account the users’
privacy, and implements a pipeline for the continuous gathering of
data and evaluation of models using the IDE release cycle.

7 THREATS TO VALIDITY
The industrial nature of the solved problem and the orientation of
the solution towards production impose certain limitations on our
research. Several important threats to validity can be highlighted.
User bias. Our approach relies on the logs of users, training the
model on their decisions, with the goal of improving the experience
of all the subsequent users of the IDE. However, the users who
participate in the Early Access Program and who agree to send the
logs might not be a perfectly representative sample of all the users.
These users are in general more active and can be more aware of
various IDE features. What is more, their activity and their work in
the IDE may also differ from that of an average user. At the same
time, a large sample of thousands of users (and tens of thousands of
completion sessions) allow us to gather data from different domains
of software engineering and different levels of activity.

State-of-the-art. As mentioned in Section 5, the positioning
of our work does not allow us to claim state-of-the-art results or
directly compare with them, making it possible for us to have missed
a specific work or model architectures that would perform better.
However, the main goal of this work was to present a pipeline for
the continuous improvement of IDE tooling, making it possible to
compare the necessary models in the near future.

Features. The same argument can be applied to the list of the
calculated features. Privacy limitations lead us towards the manual
development of features, making it possible for our current set to
not be optimal. Once again, the developed infrastructure allows us
to evaluate new features constantly and test any new developments
in a continuous cycle.

While these threats are important to note, we believe that they
do not invalidate the usefulness of the proposed pipeline and the
results of the carried out evaluations.

8 CONCLUSION & FUTURE WORK
In this work, we presented a pipeline for collecting anonymous logs
from users to train a model for ranking code completion suggestions.
We designed a set of features that are calculated on the user’s
machine and are then anonymously collected to the server without
gathering sensitive personal information or the code itself. Based on
these features, we trained a CatBoost model for ranking completion
candidates and evaluated it in two settings. The offline evaluation
on the held-out set of user data showed that the Recall at K of the
ranking increases when using the model. The online evaluation
consisted in an A/B test between the trained model and the default
heuristics-based ranking, and showed that the fraction of sessions
that ended with the explicit selection of the token increased, while
the average number of the required typing actions decreased.

An important aspect of the proposed idea is that it can be used
continuously, gathering new data and evaluating new models in a
constant cycle. This is implemented in the Early Access Program
(EAP) of IntelliJ-based IDEs: users may agree to anonymously send
their logs to the centralized server in exchange for being able to
test new features. In each EAP release, before the main release of
the IDE, the experiments are being conducted: users are divided
into groups, some of which get the current best performing models,
while some get new ML-based models that are then compared in
a live A/B test. At the same time, logs are also being collected
from other users to train future models. This pipeline has been in
production since the end of 2020, and demonstrated its usefulness
for designing new features and discovering better models.

The proposed pipeline may be used for other tasks beyond code
completion. Various anonymous features may be collected and used
to fine-tune different parts of the IDE: suggesting refactorings, fix-
ing bugs, etc. We hope to see more research that relates to collecting
structured usage logs. Such a setting also puts research-based ap-
proaches into a real-world environment, where the tested models
have to be quick and lightweight. In future work, we plan to con-
tinue our experiments with code completion models and develop
new features that would allow them to perform better. In particular,
we are working on improving code completion for API calls using
the information about similar contexts in other projects. We would
also like to broaden our scope and employ such machine learning
models to other aspects of software development in the IDE.

All You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs

Conference’17, July 2017, Washington, DC, USA

REFERENCES
[1] Sven Amann, Sebastian Proksch, Sarah Nadi, and Mira Mezini. 2016. A Study
of Visual Studio Usage in Practice. In 2016 IEEE 23rd International Conference on
Software Analysis, Evolution, and Reengineering (SANER), Vol. 1. IEEE, 124–134.
https://doi.org/10.1109/SANER.2016.39

[2] Pasquale Ardimento, Mario Luca Bernardi, Marta Cimitile, and Giuseppe De Ruvo.
2019. Mining Developer’s Behavior from Web-Based IDE Logs. In 2019 IEEE 28th
International Conference on Enabling Technologies: Infrastructure for Collaborative
Enterprises (WETICE). 277–282. https://doi.org/10.1109/WETICE.2019.00065
[3] Gareth Ari Aye and Gail E Kaiser. 2020. Sequence Model Design for Code

Completion in the Modern IDE. arXiv preprint arXiv:2004.05249 (2020).

[4] Gareth Ari Aye, Seohyun Kim, and Hongyu Li. 2021. Learning Autocompletion
From Real-World Datasets. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 131–139.
https://doi.org/10.1109/ICSE-SEIP52600.2021.00022

[5] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32. https:

//doi.org/10.1023/A:1010933404324

[6] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from Ex-
amples to Improve Code Completion Systems. In ESEC-FSE’09 - Proceedings of
the Joint 12th European Software Engineering Conference and 17th ACM SIG-
SOFT Symposium on the Foundations of Software Engineering. 213–222. https:
//doi.org/10.1145/1595696.1595728

[7] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to
Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th
international conference on Machine learning. 129–136. https://doi.org/10.1145/
1273496.1273513

[8] CatBoost. 2018. QuerySoftMax Loss Function. https://catboost.ai/en/docs/
concepts/loss-functions-ranking#QuerySoftMax. [Online. Accessed 31.08.2022].
[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint
arXiv:2107.03374 (2021).

[10] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. To What Extent
do Deep Learning-based Code Recommenders Generate Predictions by Cloning
Code from the Training Set? arXiv preprint arXiv:2204.06894 (2022).

[11] Manuel De Stefano, Michele Simone Gambardella, Fabiano Pecorelli, Fabio
Palomba, and Andrea De Lucia. 2020. cASpER: A Plug-in for Automated Code
Smell Detection and Refactoring. In Proceedings of the International Conference
on Advanced Visual Interfaces. 1–3. https://doi.org/10.1145/3399715.3399955
[12] Bradley Efron. 1992. Bootstrap Methods: Another Look at the Jackknife. (1992),

569–593. https://doi.org/10.1007/978-1-4612-4380-9_41

[13] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli.
2019. When Code Completion Fails: A Case Study on Real-World Completions.
In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
960–970. https://doi.org/10.1109/ICSE.2019.00101

[14] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
2016. On the Naturalness of Software. Commun. ACM 59, 5 (2016), 122–131.
https://doi.org/10.1145/2902362

[15] Daqing Hou and David M. Pletcher. 2011. An Evaluation of the Strategies of
Sorting, Filtering, and Grouping API Methods for Code Completion. In IEEE
International Conference on Software Maintenance, ICSM. 233–242. https://doi.
org/10.1109/ICSM.2011.6080790

[16] Daqing Hou and Yuejiao Wang. 2009. An Empirical Analysis of the Evolution of
User-Visible Features in an Integrated Development Environment. In Proceedings
of the 2009 Conference of the Center for Advanced Studies on Collaborative Research.
122–135. https://doi.org/10.1145/1723028.1723044

[17] Constantina Ioannou, Andrea Burattin, and Barbara Weber. 2018. Mining Develop-
ers’ Workflows from IDE usage. In International Conference on Advanced Informa-
tion Systems Engineering. 167–179. https://doi.org/10.1007/978-3-319-92898-2_14
[18] Maliheh Izadi, Roberta Gismondi, and Georgios Gousios. 2022. CodeFill: Multi-
token Code Completion by Jointly Learning from Structure and Naming Se-
quences. arXiv preprint arXiv:2202.06689 (2022).

[19] JetBrains. 2018. Early Access Program (EAP). https://www.jetbrains.com/idea/

nextversion/. [Online. Accessed 31.08.2022].

[20] JetBrains. 2018. Essential Tools for Software Developers and Teams. https:

//www.jetbrains.com/. [Online. Accessed 31.08.2022].

[21] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting
Decision Tree. Advances in Neural Information Processing Systems 30 (2017).
[22] Zarina Kurbatova, Yaroslav Golubev, Vladimir Kovalenko, and Timofey Bryksin.
2021. The IntelliJ Platform: a Framework for Building Plugins and Mining Soft-
ware Data. In 2021 36th IEEE/ACM International Conference on Automated Software
Engineering Workshops (ASEW). 14–17. https://doi.org/10.1109/ASEW52652.2021.
00016

[23] Zarina Kurbatova, Ivan Veselov, Yaroslav Golubev, and Timofey Bryksin. 2020.
Recommendation of Move Method Refactoring Using Path-Based Representation
of Code. In Proceedings of the IEEE/ACM 42nd International Conference on Software

Engineering Workshops. 315–322. https://doi.org/10.1145/3387940.3392191
[24] Stefano Lambiase, Andrea Cupito, Fabiano Pecorelli, Andrea De Lucia, and Fabio
Palomba. 2020. Just-in-time Test Smell Detection and Refactoring: The DARTS
project. In Proceedings of the 28th International Conference on Program Compre-
hension. 441–445. https://doi.org/10.1145/3387904.3389296

[25] Yun Young Lee, Nicholas Chen, and Ralph E Johnson. 2013. Drag-and-Drop Refac-
toring: Intuitive and Efficient Program Transformation. In 2013 35th International
Conference on Software Engineering (ICSE). 23–32. https://doi.org/10.1109/ICSE.
2013.6606548

[26] Hang Li. 2011. A Short Introduction to Learning to Rank. IEICE TRANSACTIONS
on Information and Systems 94, 10 (2011), 1854–1862. https://doi.org/10.1587/
transinf.E94.D.1854

[27] Emerson Murphy-Hill, Chris Parnin, and Andrew P Black. 2011. How We Refactor,
And How We Know It. IEEE Transactions on Software Engineering 38, 1 (2011),
5–18. https://doi.org/10.1109/TSE.2011.41

[28] Kıvanç Muşlu, Yuriy Brun, Reid Holmes, Michael D. Ernst, and David Notkin. 2012.
Speculative Analysis of Integrated Development Environment Recommendations.
ACM SIGPLAN Notices 47, 10 (2012), 669–682. https://doi.org/10.1145/2398857.
2384665

[29] Stas Negara, Nicholas Chen, Mohsen Vakilian, Ralph E. Johnson, and Danny Dig.
2013. A Comparative Study of Manual and Automated Refactorings. In European
Conference on Object-Oriented Programming. 552–576. https://doi.org/10.1007/
978-3-642-39038-8_23

[30] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen.
2013. A Statistical Semantic Language Model for Source Code. In 2013 9th
Joint Meeting of the European Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 -
Proceedings. 532–542. https://doi.org/10.1145/2491411.2491458

[31] Thida Oo, Hui Liu, and Bridget Nyirongo. 2018. Dynamic Ranking of Refactoring
IEEE Access 6 (2018),

Menu Items for Integrated Development Environment.
76025–76035. https://doi.org/10.1109/ACCESS.2018.2883769

[32] David M. Pletcher and Daqing Hou. 2009. BCC: Enhancing Code Completion for
Better API Usability. In 2009 IEEE International Conference on Software Mainte-
nance. 393–394. https://doi.org/10.1109/ICSM.2009.5306289

[33] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
gush, and Andrey Gulin. 2018. CatBoost: Unbiased Boosting with Categorical
Features. Advances in Neural Information Processing Systems 31 (2018).

[34] Sebastian Proksch, Sven Amann, Sarah Nadi, and Mira Mezini. 2016. Evaluating
the Evaluations of Code Recommender Systems: A Reality Check. In Proceedings
of the 31st IEEE/ACM International Conference on Automated Software Engineering
- ASE 2016. 111–121. https://doi.org/10.1145/2970276.2970330
[35] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2015.

Intelligent Code
Completion with Bayesian Networks. ACM Transactions on Software Engineering
and Methodology 25, 1 (2015), 1–31. https://doi.org/10.1145/2744200

[36] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code Completion with
Statistical Language Models. In Proceedings of the 35th ACM SIGPLAN Conference
on Programming Language Design and Implementation. 419–428. https://doi.org/
10.1145/2594291.2594321

[37] Romain Robbes and Michele Lanza. 2008. How Program History Can Improve
Code Completion. In ASE 2008 - 23rd IEEE/ACM International Conference on
Automated Software Engineering, Proceedings. 317–326. https://doi.org/10.1109/
ASE.2008.42

[38] Will Snipes, Emerson Murphy-Hill, Thomas Fritz, Mohsen Vakilian, Kostadin
Damevski, Anil R Nair, and David Shepherd. 2015. A Practical Guide to Analyzing
IDE Usage Data.
In The Art and Science of Analyzing Software Data. 85–138.
https://doi.org/10.1016/B978-0-12-411519-4.00005-7

[39] United States. 2018. California Consumer Privacy Act. https://oag.ca.gov/privacy/

ccpa. [Online. Accessed 31.08.2022].

[40] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.
IntelliCode Compose: Code Generation Using Transformer. In Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 1433–1443. https://doi.
org/10.1145/3368089.3417058

[41] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vi-
cente Franco, and Miltiadis Allamanis. 2021. Fast and Memory-Efficient Neural
Code Completion. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR). IEEE, 329–340. https://doi.org/10.1109/MSR52588.
2021.00045

[42] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. 2019. Pythia:
AI-Assisted Code Completion System. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2727–2735.
https://doi.org/10.1145/3292500.3330699

[43] European Union. 2018. General Data Protection Regulation. https://gdpr.eu/.

[Online. Accessed 31.08.2022].

[44] Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. 2020. Towards Full-line Code
Completion with Neural Language Models. arXiv preprint arXiv:2009.08603
(2020).

