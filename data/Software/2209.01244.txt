2
2
0
2

p
e
S
2

]
E
S
.
s
c
[

1
v
4
4
2
1
0
.
9
0
2
2
:
v
i
X
r
a

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

Ashwin Kallingal Joshy
Iowa State University
Ames, Iowa, USA
ashwinkj@iastate.edu

Wei Le
Iowa State University
Ames, Iowa, USA
weile@iastate.edu

ABSTRACT
Fuzzing has been an important approach for finding bugs and vul-
nerabilities in programs. Many fuzzers deployed in industry run
daily and can generate an overwhelming number of crashes. Diag-
nosing such crashes can be very challenging and time-consuming.
Existing fuzzers typically employ heuristics such as code coverage
or call stack hashes to weed out duplicate reporting of bugs. While
these heuristics are cheap, they are often imprecise and end up still
reporting many “unique” crashes corresponding to the same bug. In
this paper, we present FuzzerAid that uses fault signatures to group
crashes reported by the fuzzers. Fault signature is a small executable
program and consists of a selection of necessary statements from
the original program that can reproduce a bug. In our approach, we
first generate a fault signature using a given crash. We then execute
the fault signature with other crash inducing inputs. If the failure is
reproduced, we classify the crashes into the group labeled with the
fault signature; if not, we generate a new fault signature. After all
the crash inducing inputs are classified, we further merge the fault
signatures of the same root cause into a group. We implemented our
approach in a tool called FuzzerAid and evaluated it on 3020 crashes
generated from 15 real-world bugs and 4 large open source projects.
Our evaluation shows that we are able to correctly group 99.1% of
the crashes and reported only 17 (+2) “unique” bugs, outperforming
the state-of-the-art fuzzers.

CCS CONCEPTS
• Software and its engineering → Software testing and de-
bugging.

KEYWORDS
fault signatures, grouping crashes, fuzzers, fault localization, dedu-
plication

ACM Reference Format:
Ashwin Kallingal Joshy and Wei Le. 2022. FuzzerAid: Grouping Fuzzed
Crashes Based On Fault Signatures. In 37th IEEE/ACM International Con-
ference on Automated Software Engineering (ASE ’22), October 10–14, 2022,
Rochester, MI, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3551349.3556959

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASE ’22, October 10–14, 2022, Rochester, MI, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9475-8/22/10.
https://doi.org/10.1145/3551349.3556959

1 INTRODUCTION
In the recent years, we have seen an increasing number of vulnera-
bilities in programs that were exploited [16, 34]. Google’s Project
Zero, a team of security researchers that study zero-day vulnerabili-
ties, recorded their most ever detected and disclosed vulnerabilities
in 2021 [34]. 67% of the actively exploited zero-day vulnerabilities
in 2021, detected by Project Zero, were from memory related bugs.
Thankfully, the modern state-of-the-art (SOTA) fuzzers are adept
at finding these types of bugs [13, 42]. Thus, the companies such as
Microsoft and Google invest heavily to develop and deploy effective
fuzzers for daily uses. Open source platforms such as GitHub [2] and
GitLab [1] also provide friendly integration to run fuzzers. However,
even with large scale fuzzing services, like Google’s OSS-Fuzz [35]
and Microsoft’s OneFuzz [22], using fuzzers to find and fix bugs still
involves considerable manual efforts [9]. The fault diagnosis may
hardly catch up to the speed at which new crashes are generated.
As a result, critical and exploitable bugs can be left undiagnosed in
the large number of crashes reported by the fuzzers.

In this paper, our goal is to develop approaches and tools that
can help group fuzzed crashes and provide support for diagnos-
ing them. Grouping fuzzed crashes is also called crash dedupli-
cation or reporting unique crashes [24]. In the past, the common
approach for crash deduplication is to apply heuristic metrics [6–
8, 15, 21, 31, 32, 36, 37, 43], such as call stack hashing [7, 8, 32, 36],
instrumented coverage information [6, 15, 43] or dynamic symp-
toms [21, 31, 37] to compare the similarities of the crashes. For
example, AFL [43] uses instrumented branch coverage, while CERT-
BFF [7] and Honggfuzz [36] use call stacks. However, the cover-
age based metrics tend to inflate the number of “unique” fuzzed
crashes [18, 24] where typically crashes that traverse different paths
will be separated, independent of whether they trigger the same
bug. Meanwhile, call stack based metrics have risks of misclassifica-
tions; the crashes generated from the same bugs are separated into
different groups as their call stacks are different [4] or different bugs
are grouped together because they have the same call stacks [31].
The dynamic symptoms based approaches, like the ones based on
symbolic analysis [31] and automatically generated patches [21, 37],
are more precise, but they often have a limited scope, e.g., applicable
only to a certain type of crashes.

In this paper, we propose to use fault signatures to group the
fuzzed crashes. A fault signature is an executable program that
consists of “indispensable” statements that can reproduce the bug.
As opposed to call stacks, failure symptoms, and coverage based
metrics, the fault signature captures the root cause of a failure.
Crashes grouped based on the same fault signature thus likely
share the root cause and fix, and should be diagnosed together. We
say these crashes are induced by the same bug.

Our approach consists of three components, namely generating
fault signatures, classify with fault signatures, and merging fault

 
 
 
 
 
 
ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

signatures. Given a collection of fuzzed crashes (here each crash
is associated with an input), we first ran a crash inducing input to
get its dynamic trace. We then create an executable program from
the trace and perform program reduction to generate an as-small-
as-possible program, namely fault signature, that can reproduce
the bug (meaning removal of any statement from this program
can result in the bug not triggering). Since the fault signature only
contains a subset of statements from the complete crashing trace,
the two crashes that exercise different paths in the original program
can be grouped into the same signature as long as the two share the
subset of root cause statements. To classify the next fuzzed crash,
we took its crash inducing input and ran with the generated fault
signatures and see if the failure has been produced with any of
the fault signatures; if so, we classify the fuzzed crash into a group
labeled with the fault signature. These two steps are implemented
in the components of generating fault signatures and classify with
fault signatures respectively.

After all the crashes have been bucketed into the groups, each of
which is labeled with a fault signature, we perform post-processing,
namely merging fault signatures, to examine if any of the two fault
signatures can be further grouped. Two fault signatures may share
a set of statements that are root causes, but differ in the paths that
lead to the root cause from the entry of the program. These fault
signatures can be grouped. We applied a heuristic based matching
between two fault signatures to group very similar fault signatures.
Specifically, we measure how many common statements the two
signatures share and whether the call stacks are similar when crash-
ing the two different fault signatures with their respective crash
inducing inputs.

We implemented our approach in a tool called FuzzerAid for C
programs. We used 15 real-world bugs from 4 large open source
projects to evaluate it. Furthermore, we generated a total of 3020
fuzzed crashes and compared FuzzerAid with 3 SOTA fuzzers, name
AFL, CERT-BFF and Honggfuzz, and a total of 5 settings. We used
developers’ patches to establish the ground truth, similar to the
approaches in [24, 37]. Our results show that we correctly group
2995 (99.1%) of the 3020 fuzzed crashes and didn’t misclassify any
crashes into a wrong fault group. We grouped the fuzzed crashes
from 15 different bugs into 17 (+2) groups while the next best
baseline reported 40 groups, and other baselines reported 100+
or even 1000+ groups. Considering it is very time-consuming to
diagnose a bug, our approach thus has a great potential to improve
the productivity of bug diagnosis and fix associated with fuzzing
tools.

In summary, we make the following contributions in the paper:

(1) we proposed to use fault signatures to group fuzzed crashes,
and our intuition is that fault signatures capture the root
causes and thus can more accurately classify the crashes;
(2) we designed an algorithm, consisting of generating fault
signatures, classify with fault signatures, and merging fault
signatures, to automatically group fuzzed crashes, and
(3) we implemented our tool FuzzerAid for C projects, and eval-
uated it with real-world bugs and large open source projects.
Our results show that our approach can correctly group the
crashes and significantly outperformed the SOTA widely
deployed fuzzers.

2 MOTIVATION
In this section, we provide a few simple examples to explain the
challenges of grouping fuzzed crashes and why existing fuzzing
deduplication methods [6, 7, 15, 32, 36, 43] are not sufficient.

In Figure 1a, we show a code snippet that contains a null-pointer
dereference at line 3 adapted from [24]. Here, the pointer p at
line 2 is not initialized, and then the dereference at the next line
can lead to crash. This bug can be triggered independent of the
condition outcome at line 8, as both the paths ⟨6, 7, 8, 1, 2, 3⟩ and
⟨6, 7, 9, 1, 2, 3⟩ calls into the bug function. Due to the presence of
two distinct paths, a coverage based heuristic, e.g., used in AFL, will
classify the fuzzed crashes for this bug into two separate groups.
However, in this case, the branch at line 8 is not important for
triggering the bug.

if ( argc >= 2) {

1 void bug () {
int *p;
2
3
int value = *p; // Null Pointer Deference
4 }
5 int main ( int argc , char * argv []) {
6
7
8
9
10
11
12 }

char b = argv [1][0];
if ( b == 'a ' ) bug () ;
else bug () ;

}
return 0;

(a) A null pointer dereference with different paths

1 void bug () {
int *p;
2
3
int value = *p;
4 }
5 int main ( int argc , char * argv []) {
6
7
8
9 }

if ( argc >= 2) {

bug () ;

}

(b) FuzzerAid: Fault signature for the bug in Figure 1a

Figure 1: Deduplication based on code coverage can fail

In our approach, we will take a crashed input and collect its
dynamic trace. We then reduce the trace to be able to reproduce
the bug. We call such a program that contains only statements
that contribute to the failure the fault signature. For Figure 1a, we
generate the fault signature shown in Figure 1b. Here, lines 7–9 in
the original program are replaced with a single call to the function
bug. To group the fuzzed crashes, we run the crash inducing inputs
with this fault signature. If the failure is reproduced with the fault
signature, we group the fuzzed crash; if not, we will generate a new
fault signature that can represent the crash. In this example, we
can group any crashes triggered along the branch at line 8 (inputs
that start with a) or along the one at line 9 (inputs that do not start
with a) into the same fault signature.

In the second example shown in Figure 2a, we provide two differ-
ent null pointer difference bugs. Bug1, marked at line 5, will trigger
an incorrect pointer dereference at line 3 after the pointer is freed
at line 5. Meanwhile, Bug2, marked at line 17, will trigger a null

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

ASE ’22, October 10–14, 2022, Rochester, MI, USA

if ( b1 ) free (s); // Bug 1
trigger (s);

1 # include < stdlib .h >
2 # include < stdbool .h >
3 void trigger ( char *s ) { int value = *s; }
4 void bug ( char *s , bool b1 ) {
5
6
7 }
8 void foo ( char *s , bool b1 ) { bug (s , b1 ); }
9 void bar ( char *s , bool b1 ) { bug (s , b1 ); }
10 int main ( int argc , char * argv []) {
11
12
13
14
15
16
17
18
19
20
21 }

char *s = malloc (100) ;
char b = argv [1][0];
if ( b == 'a ' ) foo (s , true );
else bar (s , true );

char *p; // Bug 2
foo (p , false );

}
return 0;

if ( argc >= 2) {

} else {

(a) null pointer dereference with different call stacks

Bug1 :

Bug1 - Crash1
Bug1 - Crash2

main→foo→bug→trigger
main→bar→bug→trigger

Bug2 :

Bug2 - Crash1

main→foo→bug→trigger

(b) call stacks for the bugs in Figure 2a

1 Sig1 ===========
2 void trigger ( char *s ) { int value = *s; }
3 void bug ( char *s , bool b1 ) {
4
free ( s ); trigger (s );
5 }
6 void foo ( char *s , bool b1 ) { bug (s , b1 ); }
7 int main ( int argc , char * argv []) {
8
9
10

if ( argc >= 2) {

char * s = malloc (100) ;
char b = argv [1][0];
if ( b == 'a ' )

foo(s, true);

} }

11

12
13 Sig2 =============
14 void trigger ( char *s ) { int value = *s; }
15 void bug ( char *s , bool b1 ) {
16
free ( s ); trigger (s );
17 }
18 void bar ( char *s , bool b1 ) { bug (s , b1 ); }
19 int main ( int argc , char * argv []) {
20
21
22
23

char * s = malloc (100) ;
char b = argv [1][0];
if ( b == 'a ' ) ;

if ( argc >= 2) {

trigger ( s) ;

else bar (s, true); } }

24
25
26 Sig3 =============
27 void trigger ( char *s ) { int value = *s; }
28 void bug ( char *s , bool b1 ) {
29
30 }
31 void foo ( char *s , bool b1 ) { bug (s , b1 ); }
32 int main ( int argc , char * argv []) {
33
34
35
36
37
38 }

if ( argc >= 2) {
} else {

char * p;
foo (p , false ) ;

}

(c) FuzzerAid: Fault signatures for the bugs in Figure 2a

Figure 2: Deduplication based on call stacks can fail

pointer dereference at line 3 along the path ⟨17, 18, 8, 4, 6, 3⟩. The
uninitialized pointer at line 17 will be dereferenced.

The crashes for Bug1 can traverse different paths and lead to
different call stacks at the crash site, as shown in Figure 2b. The
first two lines indicate that Bug1 can be triggered by calling foo
(at line 14), bug (at line 8), and trigger (at line 6); or by calling
bar (at line 15), bug (at line 9), and trigger (at line 6). Since the
two crashes have two different call stacks, the call stacks based
approach can fail to group them and will consider them as different
bugs.

Similarly, the crash for Bug2 can be triggered by calling foo
(at line 18), bug (at line 8), and trigger (at line 6). As shown in
Figure 2b, the call stacks for Bug1-Crash1 and Bug2-Crash1 are the
same. Therefore, the call stack based deduplication methods can
mistakenly group the two different bugs (they have different causes
and require different fixes) into in one group.

Using our approach for this example, we will first generate fault
signatures for the crashes, one at a time, shown in Figure 2c, Sig1 at
lines 2–11 for Bug1-Crash1, Sig2 at lines 14–24 for Bug1-Crash2,
and Sig3 at lines 27–38 for Bug2-Crash1. We can see that in the
fault signatures, the statements that weren’t contributing to the
bug’s manifestation have been removed.

Each fault signature can represent one path or a set of paths
that lead the crashes. Since two sets of paths may contain the same
root cause, we further merge fault signatures to be the same fault
group. In this example, we observe that Sig1 and Sig2 differ only
at the branch b==‘a’ (highlighted in yellow and also see line 11
and lines 23–24), and we can merge them to be the same fault group.
The merged fault signatures then can classify all fuzzed crashes
that manifest Bug1 into a single group. Similarly, our approach will
determine that Sig3 has no close relation with Sig1 and Sig2 in
terms of branches and the shared statements. We thus classify it as
a separate fault group.

The above examples show that our fault signature based group-
ing potentially is more accurate than coverage based and call stack
based approaches that are popularly used in the current fuzzers. In
Sections 3 and 4, we provide the details on how we generate fault
signatures, how we group the fuzzed crashes based on the fault
signatures, and how we further merge fault signatures into fault
groups.

3 OUR APPROACH
In this section, we first give an overview of FuzzerAid. Next, we
provide a detailed explanation to help understand what is a fault
signature. We then present the three main components of FuzzerAid.

3.1 An Overview
Figure 3 presents an overview of our workflow. FuzzerAid takes
as input a collection of crashes from the fuzzers. Each crash is
associated with an input. Each input will be run through a set of
fault signatures created so far. If the failure is triggered with some
fault signature, the fuzzed crash is put in a bucket labeled with
the corresponding fault signature. This step is implemented in the
component named Classify with Fault Signature.

If the fuzzed crash input did not trigger any failures matching
with existing fault signatures created so far, we run the input in the

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

Fuzzed
Crashes

Inputs

Classify with
Fault Signatures

Not Triggered

Generate
Fault Signatures

New
Fault Signature

Fault
Signatures

Merge
Fault Signatures

Grouped
Fuzzed Crashes

+ Fault Signatures

Figure 3: Overview of FuzzerAid

original program using PIN [27] and collect its dynamic trace. We
then patch the dynamic trace to generate an executable program.
In the next step, we use a program reduction tool, C-Reduce, to
reduce the executable program into the fault signature. C-Reduce
ensures that the fault signature can still reproduce the failure at the
same location with the same failure symptoms while statements
not relevant to the failure reproduction are removed. This step is
implemented in the component named Generate Fault Signatures.
Once all the crashes and their inputs are labeled with the fault
signatures, we perform the step of Merge Fault Signatures and apply
heuristics and similarity metrics to group fault signatures that likely
originated from the same root cause. The resulting fault groups,
representing grouped fuzzed crashes, and their corresponding fault
signatures will be presented as the output of FuzzerAid.

3.2 Fault Signatures
Fault signature can be viewed as a minimized version of the original
program consisting of only the statements necessary for triggering a
particular bug. Ideally, a fault signature can reproduce the same bug
for all the inputs that can trigger the bug in the original program.
The statements in a fault signature include two parts: (1) those that
trigger the bug and (2) those that set up the necessary conditions,
e.g., parsing the input, for reaching the buggy location.

As an example, consider the null pointer dereference bug 1
in w3m–0.5.3. Even though the entire program consists of 80K
lines of code, we generated a fault signature consisting of less
than 100 lines. It can trigger the bug using the crash inducing
inputs from the original programs. A simplified version of the fault
signature is shown in Figure 4. The null pointer dereference
occurs at line 6, while trying to access the variable tbl_mode. This
variable was previously initialized as NULL at line 3 and not updated
since then. Hence, lines 2–8 in Figure 4 can be considered as the
statements that trigger the bug, and lines 9–26 are necessary to set
up the conditions to reach the bug.

While the lines actually triggering the bug (lines 2–8 in Figure 4)
mostly remain the same between different crash inducing inputs,
the statements leading to it (lines 9–26) can be different. In other
words, a bug can be triggered when executing different paths (e.g.,
in the region of lines 9–26), but these paths can share a root cause
(lines 2–8). As a concrete example, consider the two execution paths
(Bug1-Crash1 and Bug1-Crash2 from Figure 2b) for the bug shown

1https://github.com/tats/w3m/issues/18

struct readbuffer * obuf = h_env -> obuf ;
struct table_mode * tbl_mode = NULL ;
if ( obuf -> table_level >= 0) {

int pre_mode = ( obuf -> table_level >= 0) ?
tbl_mode -> pre_mode : obuf -> flag ; // Null Pointer

};

1 void HTMLlineproc0 (. . . ) {
2
3
4
5
6
7
8 }
9 void loadHTMLstream (. . . ) {
10
11
12
13
14 }
15 Buffer * loadHTMLBuffer (. . . ) {
16
17

}

. . .
while (( lineBuf2 = StrmyUFgets (f)) -> length ) {

HTMLlineproc0 ( lineBuf2 -> ptr , & htmlenv1 , internal );

. . .
loadHTMLstream (f , newBuf , src , newBuf -> bufferprop &

BP_FRAME ) ;

. . .
proc = loadHTMLBuffer ;

18 }
19 Buffer * loadGeneralFile (. . . ) {
20
21
22 }
23 int main ( int argc , char ** argv , char ** envp ) {
24
25

. . .
newbuf = loadGeneralFile ( url , NULL , NO_REFERER , 0 ,

request ) ;

26 }

Figure 4: Fault Signature for a Null Pointer Dereference in
w3m

in Figure 2a. The statements triggering this bug are in lines 3–7,
while lines 8–15 provide the necessary conditions to reach the bug
location.

It is difficult to enumerate all the different ways to reach a pro-
gram point. Hence, producing an ideal fault signature that can
reproduce the bug for all the crash inducing inputs is hard. How-
ever, since we have access to some inputs responsible for triggering
the crashes, it is easy to create a fault signature based on one input
with respect to its path, and then determine if other inputs can
crash the same paths. Therefore, we chose to generate such fault
signatures in Generate Fault Signature to group the fuzzed crashes.

3.3 Generate Fault Signatures
In order to generate fault signatures, we need to identify statements
that are necessary for triggering a bug. Any statements that are

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

ASE ’22, October 10–14, 2022, Rochester, MI, USA

1 void HTMLlineproc0 (. . . ) {
2

Lineprop mode;

int cmd;
struct readbuffer * obuf = h_env -> obuf ;
int indent, delta;

struct parsed_tag *tag;

Str tokbuf;
struct table_mode * tbl_mode = NULL ;
if (w3m_debug) {

HTMLlineproc1()

}

tokbuf = Strnew();
if ( obuf -> table_level >= 0) {

char *str, *p;

int is_tag = FALSE;
int pre_mode = ( obuf -> table_level >= 0) ?
tbl_mode -> pre_mode : obuf -> flag ; // Null Pointer

// Rest of if block

}

else { // Else block }

// Rest of the function

3

4

5

6

7

8

9

10

11

12

13

14

15

16
17

18
19

20

21
22 }

23

24

void HTMLlineproc1 () {. . . }

str StrNew () {. . . }

Figure 5: Example of the statements removed for creating
the Fault Signature in Figure 4

not executed during a bug’s manifestation are not necessary. So
as a first step, we ran the crash inducing input with the original
program to collect its dynamic trace. We used PIN, a dynamic
binary instrumentation framework, to achieve this. The dynamic
trace information collected using PIN is more resilient to call stack
corruptions as opposed to the traditional methods [14].

The statements collected using dynamic trace typically don’t
include lines representing static information such as variables defi-
nitions, structure initialization and switch case headers. Or in other
words, it is not possible to generate an executable fault signature
directly using just the dynamic trace. Therefore, as the second step,
we extracted all the functions that had a statement present in the
dynamic trace. This takes care of missing local variable definitions
and switch case headers. To get the global variable definitions and
structure initialization, we extracted all global variables, macros,
header file includes, and structure initialization using tools like
srcML [11]. We made an executable program from these extracted
information using the compilation and linker flags obtained via
tools like Bear [28]. This program, even though not minimal, is a
reduced version of the original program capable of triggering the
original bug.

As the final step, we used C-Reduce [33] to remove statements
not required for triggering the bug to generate fault signatures. C-
Reduce, by default, uses a set of 135 passes to minimize the program,
which also include transformations such as renaming variables and

function names and merging control branches. We developed a cus-
tom configuration of C-Reduce by removing 45 passes to suit our
needs. Figure 5 shows an example of the reduction (highlighted in
red) when using our configuration for producing the fault signature
shown in Figure 4. Only the statements involving the variables obuf
and tbl_mode are necessary for reproducing the null pointer
dereference bug at line 17. Hence, all the statements not related
to the two variables till the fault’s manifestation (at line 17) are
removed (lines 2, 3, 5–7, 9–12, 14, 15) by C-Reduce. Since the state-
ments after triggering the bug are also unnecessary, they also got
removed (lines 18–21). We also remove the entire functions that
were only used in the removed statements (lines 23 and 24), like
HTMLlineproc1 (used at line 10) and StrNew (used at line 12).

3.4 Classify with Fault Signatures
Although the fault signature generation starts with one crash in-
ducing input, after trace minimization and removing unnecessary
statements, the fault signature can crash a set of failure inducing
inputs that exercise the same path and the paths that only differ in
the removed statements. It thus can be used to group a set of crash
inducing inputs.

We ran crash inducing inputs with the existing fault signatures.
If the same failure occurs (meaning it triggers the same bug type
at the same source code location with the same call stack as the
original input used to produce the fault signature), we classify
the fuzzed crash into the group labeled with this fault signature.
Otherwise, we take the input that can not yet be categorized and
generate another fault signature.

When running a fuzzed crash with a fault signature, we may
encounter failures that do not match the original crashes, e.g., en-
tering an infinite loop or hanging indefinitely. Therefore, we set a
configurable timeout (1 minute in our evaluation) when running
fault signatures to classify whether a valid failure is triggered. We
also encountered some flakiness when running with fault signa-
tures caused by the nondeterminism in the software execution.
Hence, we repeated running any fuzzed crash that failed to trigger
a bug for a fixed number of times (10 times in our evaluation).

3.5 Merge Fault Signatures
Our fault signature is created from the dynamic trace generated
using a single crash inducing input. As discussed in Section 3.3,
these fault signatures don’t necessarily cover all the statements that
can lead to the program state from which the bug can be triggered.
Thus, during generation and classification of fault signatures, it is
possible to create multiple fault signatures for the same bug, each
of which represent a scenario of reaching the bug location. For
example, see Sig1 and Sig2 for Bug1 in Figure 2. In order to further
group all the fuzzed crashes from Bug1 into one group, we develop
a technique to cluster fuzzed crashes associated with these fault
signatures.

Our considerations are twofold. First, we want to group fault
signatures of the same root cause (while a root cause can cover
a segment/set of statements), and thus we should consider the
similarity/overlap between the fault signatures. Second, we also
consider the paths that lead to the crash site when merging the fault
signatures. We observed that different bugs may fail at the same

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

location, but two crashes that traverse very different paths before
reaching the same location likely have different root causes. For
example, in Figure 2, Sig3 from Bug2 shares the bug manifestation
statements (line 3 in Figure 2a) with Sig1 and Sig2 from Bug1.
However, the actual root causes (line 17 for Bug2 and line 5 for Bug1
in Figure 2a) along the paths leading to the buggy state (lines 18,4–7
for Bug2 and lines 8–16 for Bug1 respectively) are very different.

Specifically, to measure the similarity between two fault signa-
tures, we used the Levenshtein’s edit distance between them. See
Equation 1, where 𝑆𝑖𝑚𝑆𝑖𝑔 is the similarity score, 𝑀𝐴𝑋𝑆𝑖𝑧𝑒 returns
the maximum size in lines of code of the two fault signatures, 𝑆1
and 𝑆2, and 𝐿𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 is the Levenshtein’s edit distance between
the two signatures.

𝑆𝑖𝑚𝑆𝑖𝑔 =

𝑀𝐴𝑋𝑆𝑖𝑧𝑒 (𝑆1, 𝑆2) − 𝐿𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 (𝑆1, 𝑆2)
𝑀𝐴𝑋𝑆𝑖𝑧𝑒 (𝑆1, 𝑆2)

(1)

We also measured the similarity in the paths leading to the failure
location using call stacks generated by running the crash inducing
inputs with the fault signatures. Specifically, we used Equation 2,
where 𝑆𝑖𝑚𝐶𝑎𝑙𝑙 is the similarity score, 𝐶𝑂𝑀𝑀𝑂𝑁 is the number of
functions that are shared between two call stacks 𝐶𝑆1 and 𝐶𝑆2, and
𝑀𝐴𝑋𝑆𝑖𝑧𝑒 is the maximum size in count of call stacks.

𝑆𝑖𝑚𝐶𝑎𝑙𝑙 =

𝐶𝑂𝑀𝑀𝑂𝑁 (𝐶𝑆1, 𝐶𝑆2)
𝑀𝐴𝑋𝑆𝑖𝑧𝑒 (𝐶𝑆1, 𝐶𝑆2)

(2)

The final similarity score used to decide whether two fault sig-
natures should be merged or not is shown in Equation 3, which is
the average of the two similarity scores.

𝑆𝑖𝑚𝑆𝑐𝑜𝑟𝑒 =

𝑆𝑖𝑚𝑆𝑖𝑔 + 𝑆𝑖𝑚𝐶𝑎𝑙𝑙
2

(3)

4 PUT TOGETHER: THE ALGORITHM
In Algorithm 1, we present our algorithm for grouping fuzzed
crashes. The algorithm takes as input 𝐶𝐼 , a collection of inputs that
can lead to the crashes in a fuzzer, and generates the fault groups,
each of which represents a bug and has the corresponding fault
signature(s). The crashes can be generated from different runs of
the same fuzzer or even from different fuzzers.

Lines 3–13 implements the components of Generate fault signa-
tures and Classify fault signatures specified in Figure 3. Lines 14–25
implements the Merge fault signatures. Specifically, at lines 3 and 4,
we initialize fault signatures (𝐹𝑠 ) and fault groups (𝐹𝑔) respectively.
The initialization can either set them as empty sets or using existing
fault signatures and fault groups from previous fuzzing campaigns
or a different fuzzer. Lines 5–13 loop through all the fuzzed crashes
(𝐶𝐼 ) given as the inputs to create and test with fault signatures. At
line 6, the current fuzzed crash (𝑐) is checked against all the existing
fault signatures to see if there exist a fault signature (𝑠) that can
lead it to crash. In case such an existing fault signature is found,
at line 7, we add the crash into the group represented by the fault
signature. If we are unable to find such a fault signature, we create
a new one at lines 8–12. To create a fault signature, we run the
program with the crashing input to collect its dynamic trace (𝑡) at
line 9. This trace is used to create a new fault signature at line 10,
by removing the statements in the trace until the failure cannot be
reproduced. The newly created signature (𝑠𝑛𝑒𝑤) is added to other
fault signatures at line 11.

Algorithm 1 Grouping fuzzed crashed using fault signatures

1: INPUT: 𝐶𝐼 (Fuzzed crashes)
2: OUTPUT: 𝐹𝑔 (Fault groups), 𝐹𝑠 (Fault signatures)
3: Initialize 𝐹𝑠
4: Initialize 𝐹𝑔
5: for all 𝑐 ∈ 𝐶𝐼 do
6:

if ∃𝑠 ∈ 𝐹𝑠 and 𝑠 → 𝑐 then

▷ Fault signatures
▷ Fault groups

Add 𝑐 to 𝑠.𝐶𝑟𝑎𝑠ℎ𝑒𝑠

else

𝑡 ← Dynamic Trace (𝑐)
𝑠𝑛𝑒𝑤 ← Generate Signature (𝑡)
Add 𝑠𝑛𝑒𝑤 to 𝐹𝑠

12:
end if
13: end for
14: 𝑤𝑜𝑟𝑘𝑙𝑖𝑠𝑡 ← 𝐹𝑠
15: while 𝑤𝑜𝑟𝑘𝑙𝑖𝑠𝑡 ≠ ∅ do
16:

Remove 𝑠𝑐 from 𝑤𝑜𝑟𝑘𝑙𝑖𝑠𝑡
Initialize 𝐺𝑠𝑐
for all 𝑠𝑤𝑙 ∈ 𝑤𝑜𝑟𝑘𝑙𝑖𝑠𝑡 do

7:

8:

9:

10:

11:

17:

18:

19:

20:

21:

𝑠𝑐𝑜𝑟𝑒 ← Compute Similarity (𝑠𝑐, 𝑠𝑤𝑙 )
if 𝑠𝑐𝑜𝑟𝑒 ≥ threshold then

Add 𝑠𝑤𝑙 to 𝐺𝑠𝑐

▷ Merge Fault Signatures

▷ Fault group for 𝑠𝑐

22:

end if
23:
end for
24: end while
25: return 𝐹𝑔

Once we have generated the fault signatures that can classify all
the fuzzed crashes, we further merge the fault signatures into fault
groups at lines 15–24. For this, we first create a work list (𝑤𝑜𝑟𝑘𝑙𝑖𝑠𝑡)
consisting of all the fault signatures at line 14. We initialize a fault
group (𝐺𝑆𝑠𝑐 ) at line 18 for 𝑠𝑐 . This can either be empty or be an
existing fault group given at line 4. We then traverse the work
list at lines 18–23. For each fault signature in the work list, we
compute similarity scores (𝑠𝑐𝑜𝑟𝑒) at line 19 and compare it with the
other fault signatures in the work list. The 𝑠𝑐𝑜𝑟𝑒 is computed as
the average of (1) normalized Levenshtein’s edit distance and (2)
normalized percent of matching functions in the crashing call stack
between two fault signatures. If this similarity score is above a set
threshold, then we add that fault signature (𝑠𝑤𝑙 ) to the fault group
(𝐺𝑠𝑐 ) at line 21.

Once the worklist is empty, we finish grouping all the fault
signatures into fault groups. We return this group of fault groups,
that represent “unique” crashes from the fuzzed crashes, as the
output of our algorithm at line 25.

5 EVALUATION
Our evaluation aims to answer two research questions:

• RQ1: Can we correctly group crashes generated by the

fuzzers?

• RQ2: How effective is our technique compared to the SOTA

methods?

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

ASE ’22, October 10–14, 2022, Rochester, MI, USA

5.1 Experimental Setup
Implementation. We implemented FuzzerAid for C programs
5.1.1
using Clang [25], srcML [11], SQLite [20], PIN [27], C-Reduce [33]
and Rust [29]. Specifically, we used Clang, srcML and SQLite to
create a database containing the function names and their line num-
bers at a file level granularity for each benchmark. Then we used
this database with PIN to collect statement level dynamic traces
for crash inducing inputs and generated an executable program.
We used C-Reduce to minimize the executable programs into fault
signatures. We used Rust to implement the similarity comparison
between traces and between fault signatures. We used a cost of 1
for all the edits when computing the Levenshtein’s distance. The
threshold for grouping two Fault signatures (line 20 in Algorithm 1)
was set as “0.7”.

Subject Selection. To answer the research questions and
5.1.2
demonstrate that our techniques are applicable in practice, we
aim to use the benchmarks that (1) are real-world open-source C
programs, (2) preferably contain multiple real-world bugs in each
program, so that we can evaluate if our approach can separate
the crashes from different bugs, (3) the bugs and their patches are
known, so we can have a ground truth to compare against, (4) the
bugs can be triggered by the fuzzers, so we can generate the crash
corpus, and (5) can be handled by our baseline methods, so we can
compare with them.

We searched for the readily available benchmarks based on the
above 5 criteria in fuzzing literature [5, 6, 15, 24, 26, 31, 37]. We also
went through the list of programs at AFL’s website [41]. As a result,
we collected all the C projects (3 out of 6 total projects) provided
by [37], namely w3m, sqlite and libmad, and libarchive from
AFL’s website. We were not able to use the other 3 benchmarks
from [37], namely PHP, R and Conntrackd, as they were either
implemented using multiple languages (PHP, R) or the bug/patch
were in a non C file. Similarly, for C only benchmarks with multiple
bugs on the AFL’s websites, such as audiofile and libxml, we were
either not able to get the crashing input and the minimal patch, or
they had the bug/patch in non C files.

Through the above process, we collected a total of 15 known
bugs from 4 large real-world C projects. Specifically, we used 4
bugs from w3m, a text-based web browser, 8 bugs from sqlite, a
database software, 1 bug from libmad, an MPEG audio decoding
library, and 2 bugs from libarchive, a multi-format archive and
compression library. Four sqlite bugs from [37] were not included
due to the problems of reproducing them with PIN. The projects,
their sizes2, and the bugs are listed in the first three columns in
Table 1.

Fuzzer selection. To demonstrate the effectiveness of our
5.1.3
techniques and compare with meaningful baselines, we looked
for fuzzers that (1) are open source, (2) are well documented, (3)
have been widely used in research or industrial settings, (4) ap-
plied different methodologies for deduplicating fuzzed crashes (so
we can compare with different approaches of deduplication used
in practice), and (5) could work with our benchmarks. In case of
fuzzers with similar deduplication methodologies, we picked the
one that reported more bugs, cited by more references, and that are

2LOC calculated using tokei, https://github.com/XAMPPRocky/tokei

easier to work with our benchmarks. In the end, we used AFL [43]
to generate crashes, and used the deduplication methods imple-
mented in AFL, CERT-BFF [7] and Honggfuzz [36] as our baselines.
Specifically, AFL uses branch (edge) coverage and coarse grained
branch-taken hit counter to determine unique fuzzed crashes. Only
fuzzed crashes associated with execution paths that involves new
edges or doesn’t visit the common edges are kept after deduplica-
tion. CERT-BFF uses hashes generated from last 𝑁 calls (frames) in
the call stack to determine uniqueness. Any fuzzed crashes shar-
ing the same call stack hash is discarded during the deduplication
process. Honggfuzz, on the other hand, uses the information at the
crash location (fault address, last known PC instruction and last 7
frames in call stack) to deduplicate the fuzzed crashes.

5.1.4 Experimental design for RQ1. In RQ1, our goal is to evaluate
the correctness of the grouping made by FuzzerAid. Specifically, we
aim to discover (1) if we can correctly group all the fuzzed crashes
from the same bug, (2) if we would incorrectly mix crashes from
different bugs and put them in one group, and (3) if we would fail
to group any fuzzed crashes.

To establish the ground truth, we propose to generate the crashes
for the multiple known bugs and see if we can group crashes caused
by the same bug and separate the crashes generated from different
bugs. The challenge we face is that given an arbitrary given seed,
the fuzzers may not trigger the known bugs or trigger them within
a reasonable time window. To set up this experiment, we used a
special configuration of the fuzzers together with the bug patches
from the developers to achieve the goal. Specifically, our approach
is to run AFL in the “Crash Exploration Mode [40]”. It takes a known
fuzzed crash inducing input and uses the traditional feedback and
genetic algorithms to quickly generate a corpus of crashes that
explore different paths that can lead to similar crash state. We
found that this approach is likely to generate crashes that trigger
the given bug. In our experiments, we found only a total of 28
among thousands of crashes that belong to some unknown bugs.

Our setup is as follows. First, we generate crash corpus for in-
dividual know bugs using the above approach. We ran AFL for
2 hours per bug. Given 2 hours, some bug generated more than
2K fuzzed crashes, some bug only generated less than 100 crashes.
To balance the crashes from different bugs, we used at most 250
crashes from each bug (randomly select 250 if there are more than
250) and mixed them as a mixed crash corpus. Since this approach
is heuristic, we also used the developers’ patch to further validate
whether the generated crashes are indeed from a known bug and
which known bug it belongs to. Using developers’ patch to deter-
mine ground truth [24, 37], is based on the assumption that if an
input 𝐼 crashes a program 𝑃, but no longer crashes it after applying
patch 𝑝, we can associate 𝐼 with the bug for 𝑝 [9]. Further, if two
inputs 𝐼1 and 𝐼2 both crash 𝑃, but disappear with patch 𝑝, then both
𝐼1 and 𝐼2 are caused by the same bug (given that the patches are
“minimal” [19]). As a result of validation, each crash is labeled with
a bug ID and the ones do not match any existing bugs are labeled
as unknown. We then apply FuzzerAid to group these crashes that
we know the ground truth.

The validation with developers’ patch is done on the mixed crash
corpus consisting of up to 250 randomly selected fuzzed crash from
each known bug. Due to the nature of fuzzing, it is possible for the

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

Benchmark

Size (KLOC) Bug ID Crashes

w3m
v0.5.3

Sub Total

SQLite
v3.8.5

Sub Total

libmad v0.15.1b
Sub Total

libarchive
v3.1.0

Sub Total

Total

80.4

313.3

18.0

207.2

618.9

1
2
3
4

4
5
6
7
8
9
10
11
12

8
13

1
14
15

2

15

250
352
250
139

991
285
191
240
113
226
237
250
236

1778
99

99
67
85

152

3020

Fault Sig Group Correct
1
1
1
1

250
351
250
115

1
4
4
2

11
5
14
5
2
2
5
4
3

40
2

2
1
1

2

4
1
1
1
1
1
2
1
2

10
1

1
1
1

2

55

17

966
285
191
240
113
226
237
250
236

1778
99

99
67
85

152

2995

Incorrect Missed

0
0
0
0

0
0
0
0
0
0
0
0
0

0
0

0
0
0

0

0

0
1
0
24

25
0
0
0
0
0
0
0
0

0
0

0
0
0

0

25

Table 1: Result of RQ1: Evaluating Grouping Correctness

fuzzing campaign to expose a different bug than the seed bug. For
example, the fuzzing campaign for Bug 4 from w3m also generated
crashes for Bug 2 which got selected during the random selection.
However, after the validation these fuzzed crashes are correctly
labeled with bug ID for Bug 2. Hence, some bugs (Bug 2 and Bug 5
in Table 1 and Table 2) have more than 250 fuzzed crashes (under
Crashes) associated with them.

To evaluate the correctness, we used as metrics of the number
of fuzzed crashes that were (1) correctly grouped with fault groups
for a bug, (2) incorrectly grouped with fault groups from a different
bug, (3) weren’t grouped to any fault group. We also recorded the
number of fault signatures and fault groups created for each bug to
measure the usefulness of the grouping.

5.1.5 Experimental design for RQ2. In RQ2, we compared FuzzerAid
with the three SOTA real-world fuzzers regarding their capabilities
of deduplicating crashes. In the following, we present the setups of
the fuzzers used in comparison:

For AFL, we ran afl-cmin on all the fuzzed crashes generated
for each benchmark. It finds the smallest subset of fuzzed crashes
that still exercises the full range of instrumented data points as
the original fuzzed crash corpus. The remaining fuzzed crashes are
reported as the deduplicated fuzzed crashes for AFL.

We ran CERT-BFF on all the fuzzed crashes for each benchmark,
with backtracelevels set to 5 (BFF-5). This gives us deduplicated
fuzzed crashes based on the uniqueness of the last 5 frames (func-
tion calls) on the stack. Similarly, we also performed deduplication
using the last frame (crashing function) of the call stack by setting

backtracelevels to 1 (BFF-1). We chose these two configurations
because BFF-5 represents the default deduplication used by CERT-
BFF, and BFF-1 is used as a baseline in the related work [37].

For Honggfuzz, we ran the fuzzed crashes for each benchmarks
with the instrument option enabled. This gave us deduplicated
fuzzed crashes determined using a combination of code coverage,
call stack, and crash site information [3]. Then we used the noinst
mode (Honggfuzz-S) to obtain deduplicated fuzzed crashes deter-
mined using only call stack and crash site information. We chose
these two configurations because Honggfuzz represents the default
deduplication of the fuzzer and Honggfuzz-S is also used as a base-
line in the related work [37].

In the experiment, we first collect a set of crashes for a project,
e.g., 991 for w3m shown in Table 2. We then run a baseline tool, e.g.,
AFL, to deduplicate the crashes. The number of groups reported
by the tool is listed under the columns of each baseline’s SubTotal
row, e.g., 490 for AFL in Table 2. We then use the developer’s patch
to determine how many groups were reported for each bug, e.g.,
109 for Bug 1 for AFL.

5.1.6 Running the experiments. The initial crash corpus generation
and the crash deduplication for the baseline fuzzers were run on
a VM with 64-bit 32 core Intel Haswell processors. The FuzzerAid
experiments were conducted on a VM with 64-bit 12 core Intel
Haswell processors. Both the VMs had with 32 GB memory and
were running CentOS 8.

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Benchmark Bug ID Crashes

w3m

Sub Total

SQLite

Sub Total

libmad
Sub Total

libarchive

Sub Total

Total

1
2
3
4

4
5
6
7
8
9
10
11
12

8
13

1
14
15

2

15

250
352
250
139

991
285
191
240
113
226
237
250
236

1778
99

99
67
85

152

3020

FuzzerAid AFL BFF-5 BFF-1 Honggfuzz Honggfuzz-S
1
1
7
3

109
208
113
60

49
8
17
7

49
9
14
8

2
2
89
4

1
1
1
1

4
1
1
1
1
1
2
1
2

10
1

1
1
1

2

490
179
43
81
43
60
58
134
62

660
58

58
24
44

68

97
14
22
7
2
3
4
1
4

57
4

4
0
1

1

12
5
8
4
3
1
1
1
2

25
2

2
0
1

1

80
2
11
1
1
1
2
2
2

22
5

5
1
1

2

81
3
12
1
1
1
2
2
2

24
6

6
1
1

2

17

1276

159

40

109

113

Table 2: Results of RQ2: Comparing FuzzerAid against SOTA fuzzer deduplication

5.2 Results for RQ1: Grouping Correctness
Table 1 shows the result for RQ1. Each row corresponds to a known
bug labeled with Bug ID. The column Crashes lists the number
of fuzzed crashes generated for each bug using the approach pre-
sented in Section 5.1.4. The crashes reported in this column are
post-processed using the developer’s patch. The Fault Sig and Group
columns provide the number of fault signatures and the number of
fault groups generated for the known bug using FuzzerAid. Under
Correct column we list the number of fuzzed crashes that were
correctly grouped in one of the fault groups generated for the bug.
Similarly, the Incorrect column lists the number of fuzzed crashes
from unrelated bugs that were incorrectly grouped under one of the
fault groups generated for the bug. Any fuzzed crash that FuzzerAid
failed to group under any fault group is reported under Missed.

Our results indicate that among the total 3020 fuzzed crashes
for which we know the ground truth, FuzzerAid correctly grouped
2995 (99.1%) fuzzed crashes. For 3 benchmarks (sqlite, libmad and
libarchive), we correctly classified 100% (2029) of their fuzzed
crashes. While we were unable to classify 25 (0.08%) fuzzed crashes,
we didn’t misclassify any fuzzed crash into unrelated fault groups.
The 25 fuzzed crashes we missed for w3m (1 from Bug 2 and 24 from
Bug 4) were due to fault signature generation failure caused by the
project specific hard-coded dynamic functions. This implementa-
tion issue of FuzzerAid could be improved in the future.

FuzzerAid generated a total of 17 fault groups for the 15 known
bugs. For 3 benchmarks (w3m, libmad and libarchive), we re-
ported the same number of groups as the ground truth. The 2 extra

fault groups (highlighted in red) generated for sqlite (one for Bug
10 and one for Bug 12) missed the clustering threshold by a very
small margin (difference of 0.08% and 0.4% respectively).

We reported a total of 55 fault signatures for the 15 bugs sized
between 52 LOC to 340 LOC. The number of fault signatures can
indicate the number of different important paths (or scenario) in
which a particular bug can manifest. Of particular interest is Bug
6 from sqlite, which produced 14 fault signatures from just 191
fuzzed crashes. The relatively high number of fault signatures may
be an indicator that this bug can be crashed from a variety of
scenarios and thus likely more important.

Using AFL “Crash Exploration Mode”, our crash corpus also
included 28 fuzzed crashes that do not belong to any known bugs
which we discovered using the developers’ patches (See Section
5.1.4). FuzzerAid is able to successfully separate them into different
groups from the known bugs.

5.3 Results for RQ2: Comparing Against SOTA
Table 2 shows the result for RQ2. Similar to Table 1, each row corre-
sponds to a known bug. We label them using the same assigned Bug
ID in Table 1. For all the crashes listed under Crashes, the grouping
results from FuzzerAid and our baselines are listed under FuzzerAid,
AFL, BFF-5, BFF-1, Honggfuzz and Honggfuzz-S respectively.

Our results show that FuzzerAid’s grouping is the same as the
ground truth, except that we generated two additional groups for
the SQLite bugs. We generated a total of 17 groups for 15 bugs, com-
pared to 40 from BFF-1, 109 from Honggfuzz, 113 from Honggfuzz-S,

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

159 from BFF-5, and 1276 from AFL. Considering the challenges
and cost of diagnosing a bug, our precise grouping techniques and
improvement over the baselines indeed have practical values.

AFL used a conservative approach and applied branch coverage
information to group the crashes of the same paths, thus it gener-
ated the most group. On the other hand, call stack hashes based
approach of CERT-BFF and Honggfuzz were able to greatly reduce
the number of groups, but with the risk of misclassification. For
example, when we inspected the correctness of the grouping, we
found that both BFF-1 and BFF-5 incorrectly classified all the fuzzed
crashes from the two bugs of libarchive into one single group.
See the numbers in the row of libarchive highlighted in red.

5.4 Summary
In our evaluation, FuzzerAid is able to correctly group 2995 out of
3020 (99.1%) fuzzed crashes without any incorrect classification. We
were also the closest to ground truth in terms of grouping with 17
fault groups reported instead of ground truth’s 15. The next closest
baseline (BFF-1) reported 40 groups (2.35 times more) while still
misclassifying fuzzed crashes from one group for libarchive.

The trace generated by PIN for creating the fault signatures
varied between 2.03 M lines to 9.5 K lines. Using the program
reduction techniques, the fault signatures used to group crashes
range between 52 LOC to 340 LOC. Such fault signatures provided
fault localization information and potentially help developers focus
on a small set of statements for bug understanding and diagnosis.

5.5 Threats To Validity
Internal Threat to Validity: One of the important challenges of
evaluating deduplication of fuzzing results is that we need to have
ground truth for grouping. To simulate this setting, our approach
is to take known bugs and configure the fuzzers to generate only
crashes for the known bugs. This approach can detect whether we
are able to group crashes of the same bug together. Meanwhile, to
evaluate that we do not mistakenly group crashes from one bug to
other groups, we mixed all the crashes from known bugs and see
whether the grouping is correct. We also consider the fact that using
AFL “Crash Exploration Mode” may generate additional crashes
from unknown bugs. We used the developers’ patches to fix each
bug and observe if the crashes disappear. We also used a similar
approach to validate if there are any misclassification in the groups
generated by FuzzerAid and other baselines.
External Threat to Validity: To evaluate if our techniques can
be generally applicable in practice, we used 15 different bugs from
4 real-world large open-source projects. These projects range from
18 KLOC to 313 KLOC and cover a variety of software, e.g., text
based web browser and audio library. We also selected 3 SOTA
widely used fuzzers and their 5 total different settings as baselines
to understand if our approach indeed advances the SOTA. Although
more crashes, bugs, software, and baselines can be useful for further
confirming the generality of our approaches, our current results
do provide confidence that our approach is promising and can be
useful.

6 RELATED WORK
The SOTA fuzzers [6–8, 15, 23, 32, 36, 43] use either a coverage
based [6, 15, 43] or call stack based [7, 8, 23, 32, 36] heuristics to
determine uniqueness of the fuzzed crashes and report the dedupli-
cated fuzzed crashes. Boehme et al. [6] extended AFL to direct the
fuzzing towards a specific target, while Gan et al. [15] improved
AFL to more uniquely determine the branch coverage when fuzzing.
Even though both carried out additional (manual) verification when
reporting “unique” bugs, they didn’t change AFL’s underlying dedu-
plication method of using branch coverage. Similarly, most of the
call stack based approaches used the same hashing method pro-
posed by Molar et al. [30] with varying number of calls (frames)
used for the hashing. Of particular interest are SYMFUZZ [8] that
uses “safe stack hash”, that only considered non-corrupted call
stacks, and VUzzer [32] that uses the last 10 basic blocks along with
call stack to generate hashes to prevent classifying fuzzed crashes
from different bugs into the same group. These deduplication meth-
ods are tightly coupled to their respective fuzzers and hard to use
independently. In contrast, our method is agnostic of the method
used for generating the fuzzed crashes. Since we capture the root
cause of the bug in the fault signatures, we are also less prone to
the over counting and misclassification of “unique” bugs found in
the coverage and stack hash based methods [24].

There are also grouping methods developed independent of the
fuzzers [9, 21, 31, 37]. The closest related work is by Chen et al. [9]
that calculated a “distance” between fuzzed crashes to capture their
static and dynamic properties and used a machine leaning to rank
them. Homes et al. [21] and van Tonder et at. [37] grouped fuzzed
crashes based on their responses to the mutations of the program.
They hypothesize that if the behavior of two fuzzed crashes change
similarly (change from crashing to not crashing) due to the same
mutation, then they are more likely to be the same bug. Pham et
al. [31] proposed using symbolic constraints on input paths to group
fuzzed crashes. They have limited applicability due to the reliance
on symbolic execution to generate the constraints. Cui et al. [12]
and Molar et al. [30] proposed call stack similarity to group fuzzed
crashes. They are more prone to misclassification as discussed above.
We have considered these related work as baselines, however, they
are either tailored for specific applications like [9, 21] or limited in
the types of bugs [37] or benchmarks [31] they can handle, while
others [12, 30] are very similar to our current baselines.

There have also been work on performing fault localization for
fuzzers. Blazytko et al. [4] is a representative work in this area.
Similar to our approach of generating crash corpus, they used a
known crashing input from a fuzzed crash to generate similar inputs
to observe the dynamic state of the program. These are used to
generate predicates similar to the input path constraints generated
by symbolic execution to isolate the root cause. Variations of delta
debugging [10, 17, 38, 39] have also been used for fault location.
Both Christi et al. [10] and Vince et al. [38] proposed reducing
the fuzzed crashes, similar to our approach in generating fault
signatures, before trying to localize bugs in order to improve their
accuracy. The main difference to our work is that they only identify
or rank root causes for a crash, but do not generate executable fault
signatures and use them to group fuzzed crashes.

FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures

ASE ’22, October 10–14, 2022, Rochester, MI, USA

7 CONCLUSIONS AND FUTURE WORK
This paper presents a heuristics based approach for deduplicating
fuzzed crashes. As opposed to the use call stacks, code coverage,
and failure symptoms based approaches, our approach uses fault
signatures to group fuzzed crashes. A fault signature captures the
necessary statements that allow the bugs to be reproduced. Crashes
grouped based on a fault signature thus likely share the root causes
and fixes. We developed an algorithm and a tool that consist of the
three components, generating fault signatures, classifying with fault
signatures and merging fault signatures. We evaluated our approach
on 3020 fuzzed crashes against the ground truth we set up from
15 real-world bugs and patches and from 4 different open-source
projects. Our results show that our approach correctly grouped
99.1% of 3020 fuzzed crashes and generated 17 groups for 15 bugs.
Our approach significantly outperformed the deduplication meth-
ods offered by 3 SOTA fuzzers, namely AFL, BFF and Honggfuzz,
which reported 40–1276 groups. Considering diagnosing a crash
can be challenging and time-consuming, we believe our tool can
significantly improve the debugging productivity for fuzzing. In
the future, we will explore the further usage of fault signatures
for fault localization and automated patch generation/verification.
We will also experiment our approach for grouping fuzzed crashes
from different program versions and from different fuzzers.

ACKNOWLEDGMENTS
We thank the anonymous reviewers for their valuable feedback.
We also thank Xiuyuan Guo for helping with running the baseline
fuzzers. This research is supported by the US National Science
Foundation (NSF) under Award 1816352.

REFERENCES
[1] 2020. Coverage-guided fuzz testing. https://docs.gitlab.com/ee/user/application_

security/coverage_fuzzing/

[2] 2021. ClusterFuzzLite.

https://google.github.io/clusterfuzzlite/running-

clusterfuzzlite/github-actions/

[3] 2021. Honggfuzz Usage. https://github.com/google/honggfuzz/blob/master/

docs/USAGE.md#output-files=

[4] Tim Blazytko, Moritz Schlögel, Cornelius Aschermann, Ali Abbasi, Joel Frank,
Simon Wörner, and Thorsten Holz. 2020. AURORA: Statistical Crash Analysis
for Automated Root Cause Explanation. 235–252.
https://www.usenix.org/
conference/usenixsecurity20/presentation/blazytko

[5] Marcel Boehme, Cristian Cadar, and Abhik Roychoudhury. 2021. Fuzzing:
IEEE Software 38, 3 (May 2021), 79–86. https:

Challenges and Reflections.
//doi.org/10.1109/MS.2020.3016773

[6] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoud-
hury. 2017. Directed Greybox Fuzzing. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 2329–2344. https:
//doi.org/10.1145/3133956.3134020

URL:

[7] CERT.

2010.

CERT Basic Fuzzing Framework (BFF).

https://github.com/CERTCC/certfuzz (visited on 01/10/2022) (2010).

[8] Sang Kil Cha, Maverick Woo, and David Brumley. 2015. Program-Adaptive
Mutational Fuzzing. In 2015 IEEE Symposium on Security and Privacy. IEEE, San
Jose, CA, USA, 725–741. https://doi.org/10.1109/SP.2015.50

[9] Yang Chen, Alex Groce, Chaoqiang Zhang, Weng-Keen Wong, Xiaoli Fern, Eric
Eide, and John Regehr. 2013. Taming compiler fuzzers. In Proceedings of the 34th
ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI ’13). Association for Computing Machinery, 197–208. https://doi.org/10.
1145/2491956.2462173

[10] Arpit Christi, Matthew Lyle Olson, Mohammad Amin Alipour, and Alex Groce.
2018. Reduce Before You Localize: Delta-Debugging and Spectrum-Based Fault
Localization. In 2018 IEEE International Symposium on Software Reliability Engi-
neering Workshops (ISSREW). IEEE, 184–191. https://doi.org/10.1109/ISSREW.
2018.00005

[11] Michael L. Collard, Michael John Decker, and Jonathan I. Maletic. 2013. srcML:
An Infrastructure for the Exploration, Analysis, and Manipulation of Source

Code: A Tool Demonstration. In 2013 IEEE International Conference on Software
Maintenance. 516–519. https://doi.org/10.1109/ICSM.2013.85

[12] Weidong Cui, Marcus Peinado, Sang Kil Cha, Yanick Fratantonio, and Vasileios P.
Kemerlis. 2016. RETracer: triaging crashes by reverse execution from partial
memory dumps. In Proceedings of the 38th International Conference on Software
Engineering. ACM, 820–831. https://doi.org/10.1145/2884781.2884844

[13] Zhen Yu Ding and Claire Le Goues. 2021. An Empirical Study of OSS-Fuzz
Bugs. arXiv:2103.11518 [cs] (Mar 2021). http://arxiv.org/abs/2103.11518 arXiv:
2103.11518.

[14] Will Dormann. 2013. One weird trick for finding more crashes. https://insights.

sei.cmu.edu/blog/one-weird-trick-for-finding-more-crashes/

[15] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen Tu, Kang Li, Zhongyu Pei, and
Zuoning Chen. 2018. CollAFL: Path Sensitive Fuzzing. In 2018 IEEE Symposium on
Security and Privacy (SP). IEEE, 679–696. https://doi.org/10.1109/SP.2018.00040
[16] Brian Gorenc. 2022. Looking back at the zero-day initiative in 2021. https://www.
thezdi.com/blog/2022/1/20/looking-back-at-the-zero-day-initiative-in-2021
[17] Alex Groce, Mohammed Amin Alipour, Chaoqiang Zhang, Yang Chen, and John
Regehr. 2014. Cause Reduction for Quick Testing. In Verification and Validation
2014 IEEE Seventh International Conference on Software Testing. 243–252. https:
//doi.org/10.1109/ICST.2014.37

[18] Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. 2020. Magma: A Ground-
Truth Fuzzing Benchmark. Proceedings of the ACM on Measurement and Analysis
of Computing Systems 4, 3 (Nov 2020), 1–29. https://doi.org/10.1145/3428334
arXiv: 2009.01120.

[19] Michael Hicks. 2015. What is a bug. http://www.pl-enthusiast.net/2015/09/08/

what-is-a-bug/

[20] Richard D Hipp. 2022. SQLite. https://www.sqlite.org/index.html
[21] Josie Holmes and Alex Groce. 2018. Causal Distance-Metric-Based Assistance for
Debugging after Compiler Fuzzing. In 2018 IEEE 29th International Symposium
on Software Reliability Engineering (ISSRE). IEEE, 166–177. https://doi.org/10.
1109/ISSRE.2018.00027

[22] Mike Walker Justin Campbell. 2020. One Fuzz Framework.

https:
//www.microsoft.com/security/blog/2020/09/15/microsoft-onefuzz-framework-
open-source-developer-tool-fix-bugs/

[23] Ulf Kargén and Nahid Shahmehri. 2015. Turning Programs against Each Other:
High Coverage Fuzz-Testing Using Binary-Code Mutation and Dynamic Slicing.
In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
(Bergamo, Italy) (ESEC/FSE 2015). Association for Computing Machinery, New
York, NY, USA, 782–792. https://doi.org/10.1145/2786805.2786844

[24] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018.
Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security. ACM, 2123–2138. https://doi.org/10.
1145/3243734.3243804

[25] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis and Transformation. In Proceedings of the international
symposium on Code generation and optimization: feedback-directed and runtime
optimization (CGO ’04). IEEE Computer Society, USA, 75.

[26] Hongliang Liang, Xiaoxiao Pei, Xiaodong Jia, Wuwei Shen, and Jian Zhang.
2018. Fuzzing: State of the Art. IEEE Transactions on Reliability 67, 3 (Sep 2018),
1199–1218. https://doi.org/10.1109/TR.2018.2834476

[27] Chi-Keung Luk, Robert Cohn, Robert Muth, Harish Patil, Artur Klauser, Geoff
Lowney, Steven Wallace, Vijay Janapa Reddi, and Kim Hazelwood. 2005. Pin:
building customized program analysis tools with dynamic instrumentation. ACM
SIGPLAN Notices 40, 6 (Jun 2005), 190–200. https://doi.org/10.1145/1064978.
1065034

[28] Many. 2017. Build EAR: a tool that generates a compilation database. https:

//github.com/rizsotto/Bear. Accessed: 2017-05-07.

[29] Nicholas D. Matsakis and Felix S. Klock. 2014. The rust language. In Proceedings
of the 2014 ACM SIGAda annual conference on High integrity language technology
- HILT ’14. ACM Press, Portland, Oregon, USA, 103–104. https://doi.org/10.1145/
2663171.2663188

[30] David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic test generation
to find integer bugs in x86 binary linux programs. In Proceedings of the 18th
conference on USENIX security symposium (SSYM’09). USENIX Association, USA,
67–82.

[31] Van-Thuan Pham, Sakaar Khurana, Subhajit Roy, and Abhik Roychoudhury. 2017.
Bucketing Failing Tests via Symbolic Analysis. In Fundamental Approaches to
Software Engineering (Lecture Notes in Computer Science), Marieke Huisman and
Julia Rubin (Eds.). Springer, 43–59. https://doi.org/10.1007/978-3-662-54494-5_3
[32] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida,
and Herbert Bos. 2017. VUzzer: Application-aware Evolutionary Fuzzing. In
Proceedings 2017 Network and Distributed System Security Symposium. Internet
Society, San Diego, CA. https://doi.org/10.14722/ndss.2017.23404

[33] John Regehr, Yang Chen, Pascal Cuoq, Eric Eide, Chucky Ellison, and Xuejun
Yang. 2012. Test-case reduction for C compiler bugs. In Proceedings of the 33rd
ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI ’12). Association for Computing Machinery, New York, NY, USA, 335–346.
https://doi.org/10.1145/2254064.2254104

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Ashwin Kallingal Joshy and Wei Le

[34] Ryan. 2022. The More You Know, The More You Know You Don’t Know. https:

//googleprojectzero.blogspot.com/

[35] Kostya Serebryany. 2017. OSS-Fuzz - Google’s continuous fuzzing service for open
source software. (2017). https://www.usenix.org/conference/usenixsecurity17/
technical-sessions/presentation/serebryany

[36] Robert Swiecki. 2017. Honggfuzz: A general-purpose, easy-to-use fuzzer with
interesting analysis options. URL: https://github. com/google/honggfuzz (visited
on 06/21/2017) (2017).

[37] Rijnard van Tonder, John Kotheimer, and Claire Le Goues. 2018. Semantic crash
bucketing. In Proceedings of the 33rd ACM/IEEE International Conference on Au-
tomated Software Engineering. ACM, 612–622. https://doi.org/10.1145/3238147.
3238200

[38] Dániel Vince, Renáta Hodován, and Ákos Kiss. 2021. Reduction-assisted Fault
Localization: Don’t Throw Away the By-products!:. In Proceedings of the 16th

International Conference on Software Technologies. SCITEPRESS - Science and
Technology Publications, 196–206. https://doi.org/10.5220/0010560501960206

[39] Jifeng Xuan and Martin Monperrus. 2014. Test case purification for improv-
ing fault localization. In Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering (FSE 2014). Association for
Computing Machinery, 52–63. https://doi.org/10.1145/2635868.2635906

[40] Michal Zalewski. 2014. AFL fuzz crash exploration mode.
blogspot.com/2014/11/afl-fuzz-crash-exploration-mode.html

https://lcamtuf.

[41] Michal Zalewski. 2014. AFL trophy case. https://lcamtuf.coredump.cx/afl/
[42] Michal Zalewski. 2015. Finding bugs in sqlite easy way. https://lcamtuf.blogspot.

com/2015/04/finding-bugs-in-sqlite-easy-way.html

[43] Michal Zalewski. 2017. American fuzzy lop. URL: http://lcamtuf.coredump.cx/afl

(visited on 01/10/2022) (2017).

