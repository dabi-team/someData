Unifying Evaluation of Machine Learning Safety
Monitors

Joris Guerin
Univ. Toulouse, LAAS-CNRS
Toulouse, France
jorisguerin.research@gmail.com

Raul Sena Ferreira
LAAS-CNRS
Toulouse, France
rsenaferre@laas.fr

Kevin Delmas
ONERA
Toulouse, France
kevin.delmas@onera.fr

J´er´emie Guiochet
Univ. Toulouse, LAAS-CNRS
Toulouse, France
jeremie.guiochet@laas.fr

2
2
0
2

g
u
A
1
3

]

G
L
.
s
c
[

1
v
0
6
6
4
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—With the increasing use of Machine Learning (ML)
in critical autonomous systems, runtime monitors have been
developed to detect prediction errors and keep the system in
a safe state during operations. Monitors have been proposed
for different applications involving diverse perception tasks and
ML models, and speciﬁc evaluation procedures and metrics are
used for different contexts. This paper introduces three uniﬁed
safety-oriented metrics, representing the safety beneﬁts of the
monitor (Safety Gain), the remaining safety gaps after using
it (Residual Hazard), and its negative impact on the system’s
performance (Availability Cost). To compute these metrics, one
requires to deﬁne two return functions, representing how a given
ML prediction will impact expected future rewards and hazards.
Three use-cases (classiﬁcation, drone landing, and autonomous
driving) are used to demonstrate how metrics from the literature
can be expressed in terms of the proposed metrics. Experimental
results on these examples show how different evaluation choices
impact the perceived performance of a monitor. As our formalism
requires us to formulate explicit safety assumptions, it allows us
to ensure that the evaluation conducted matches the high-level
system requirements.

Index Terms—Machine learning safety, Runtime monitoring,

Evaluation

I. INTRODUCTION

Recent breakthroughs in Machine Learning (ML) slowly
allow autonomous systems to operate in the real world, where
failures can be catastrophic, e.g., self-driving cars [1]. This
work focuses on ML-based perception functions that interpret
complex sensor signals to estimate state [2], e.g., pedestrian
detection [3], for which there is currently no valid alternative
to complex ML models. However, the use of ML has raised
new dependability challenges such as the lack of well-deﬁned
speciﬁcation, the black-box nature of the models, the data
high-dimensionality, and the over-conﬁdence of neural net-
works [4], [5]. Consequently, ofﬂine actions are not sufﬁcient
to guarantee the safety of such critical autonomous systems.
As an alternative, recent research investigated online fault
tolerance mechanisms, which we refer to as runtime monitors,
to maintain an acceptable behavior during operation despite
perception errors.

Runtime monitors are safety components acting close to
in charge of detecting
the perception function of interest,
hazardous errors and raising alert to the system accordingly [6]
(Figure 1). A good monitor should increase the system’s
safety (absence of hazardous situations) without decreasing its

availability (ability to perform its mission). Recent works have
proposed to develop speciﬁc runtime monitors for a variety
of visual perception tasks (e.g., classiﬁcation [7], [8], [9],
object detection [10], semantic segmentation [11], steering
angle regression [12], [13]). Depending on the application
context, distinct evaluation procedures and metrics have been
used to quantify the performance of runtime monitors. Such
evaluation choices actually reﬂect safety assumptions about the
system of interest, which are rarely explicit. For example, the
most common assumption for classiﬁcation tasks is that any
misclassiﬁcation has the same impact on the system’s safety
(see subsection III-A). This paper aims to unify the evaluation
methodology for runtime monitors across tasks and applica-
tion contexts by deﬁning evaluation metrics that: 1) capture
the safety beneﬁts of the evaluated monitor, 2) capture the
remaining safety gaps despite using the monitor, 3) capture
the negative impact of the monitor on system’s availability,

is either a success or a failure and that

Evaluation metrics for generic non-ML runtime monitors
(or checkers) were discussed in [14]. The concept of checker
coverage was introduced to represent the probability of failure
of a primary-checker fault-tolerant architecture. However, in
their formulation, they assume that an output of the monitored
component
this
binary status fully determines the resulting hazard. The output
can be partially correct for ML-based perception functions,
e.g., a pedestrian detection model locates only a subset of
individuals in an image. In addition, all errors from an ML
model do not lead to equally hazardous situations, e.g., an
autonomous vehicle not detecting a pedestrian on the sidewalk.
Our proposed metrics are sufﬁciently ﬂexible to model these
speciﬁcities of ML-based functions.

An attempt of generic formulation for ML functions was
proposed among the selective prediction community [15].
Their evaluations introduce the notions of coverage and selec-
tive risk, representing the size of the region of the input space
where the monitor does not activate and the hazard associated
with this region. By introducing a customizable loss function
to represent hazard, this formulation is well suited to deﬁne
the safety impact of a wrong acceptance. However, unlike our
approach, these evaluation metrics lack the ﬂexibility to model
the potential negative effects of wrong rejections on safety
(relevant example in subsection III-C) and system availability.
To address the above limitations, in Section II, we introduce

 
 
 
 
 
 
Fig. 1: Machine Learning-based system. This representation is well suited to describe an autonomous system relying on ML
for state estimation. The ML function can be enhanced with a runtime safety monitor to detect hazardous errors.

new generic metrics called Safety Gain, Availability Cost and
Residual Hazard that present the desired properties. These
in estimating future cumulative safety and
metrics consist
mission rewards following a given prediction. Then, in Sec-
tion III, we demonstrate in three distinct use-cases that diverse
evaluation metrics from the literature can easily be expressed
in terms of the proposed metrics. These examples show that
using our formalism helps to ensure that
the evaluations
conducted are in line with the actual safety requirements of
the system. Finally, in Section IV, we discuss the beneﬁts and
limitations of this work.

II. PROPOSED EVALUATION FORMALISM

This section presents uniﬁed evaluation metrics for runtime
safety monitors in their generic form. Relaxations are also
introduced to make them usable in practice. Note that practical
examples of usage are presented in Section III.

A. Context and notations

Let f be a predictive model, in charge of approximating
an unknown function f ∗ over an operational design domain
D1. In particular, this paper focuses on complex perception
functions used in critical autonomous systems to estimate
the state of the agent and/or its environment, e.g., pedestrian
detection. In other words, the system uses the outputs of f
to decide what actions to take in the real world, and these
actions can impact the safety of the agent and its surroundings.
Figure 1 presents a simpliﬁed generic architecture for such a
system, which can be used to better understand our notations.
A set of initial conditions, fully deﬁning the state of the system
and its environment in a given world, is called a scenario. In

1Set of all situations under which the system is expected to work, e.g.,
for autonomous vehicles it can be described by roadway types, geographic
characteristics, speed ranges, and weather conditions, among others [16].

this work, we consider that the behavior of the autonomous
system is optimal if f works perfectly, i.e., if for all possible
scenarios and time steps t, f processes the sensor input xt
as expected (f (xt) = f ∗(xt)), then the actions performed are
safe and allow to fulﬁll the system’s mission.

However, for such perception tasks, f is often built using
complex ML models such as deep neural networks, which
are known to make errors that are difﬁcult to predict. Let
zt+1|f (xt) be the world state (system + environment) at time
step t + 1, when the autonomous system follows its policy π
after receiving a prediction f (xt) from the perception model.
To simplify notations, we assume that xt is representative of
the world state zt, and that the system satisﬁes the Markov
property, i.e., the environment’s response at t+1 depends only
on the state and action at t. Let rM(z) be the mission reward,
which associates a score to a conﬁguration z, representing
the progress of the system with respect to its mission. Like-
wise, let rS (z) be the safety reward, representing the safety
of conﬁguration z. In practice, the function rM is deﬁned
during the design phase, and the function rS results from the
safety analysis of the system (the safety score is inversely
proportional to the residual hazards of a conﬁguration). For
example, if an autonomous vehicle is running normally on the
road at time step t, rS (zt) should be high, but if it collides with
a pedestrian at time step t(cid:48), rS (zt(cid:48)) should be very low. Some
concrete examples of how rS can be deﬁned are presented in
Section III.

Then, we deﬁne the mission return associated with f after

a given prediction at time step t:

RM

f (xt) =

T
(cid:88)

k=t+1

rM(zk|f (xt)),

(1)

where T is the length of the episode2. RM
f (xt) represents the
cumulative mission reward resulting from f (xt). Likewise, the
safety return is deﬁned as:

RS

f (xt) =

T
(cid:88)

k=t+1

rS (zk|f (xt)).

(2)

A low RM
f (xt) means that the prediction f (xt) will decrease
the availability of the system, and a low RS
f (xt) means
that f (xt) may lead to an unsafe state. Under these notations,
we assume that the unknown function f ∗ always makes the
best possible predictions, that is, ∀t:

f ∗ = arg maxf RM

f (xt),

f ∗ = arg maxf RS

f (xt).

We acknowledge that
the sequence of consecutive states
{zt+1|f (xt), ..., zT |f (xt)} is not known a priori and is most
likely non-deterministic. However, we will see later how this
formulation can be used in practice to evaluate the impact of
the predictions of f .

B. General formulation of the proposed metrics

As it is hard to guarantee that f is valid across the entire
operational design domain D, it is essential to equip such
complex ML perception functions with appropriate runtime
monitoring mechanisms to detect errors of f and maintain the
system in a safe state. Let mf be such a monitor for f , i.e.,
for a given input x ∈ D, mf should detect when f (x) will be
erroneous and raise an alert accordingly. The remaining of the
system can then modify its actions, thus deﬁning a new for-
ward state sequence {zt+1|(f, mf )(xt), ..., zT |(f, mf )(xt)}.
This way, RM
(f,mf )(xt) depend not only
on the values returned by f (xt), but also on the status of
the monitor mf (xt). In this paper, we only consider binary
monitors that activate and raise alerts when they judge that
f (xt) is hazardous for the system (mf (xt) = 1), and do
nothing otherwise (mf (xt) = 0).

(f,mf )(xt) and RS

When evaluating the performance of a monitor mf , we are
interested in quantifying the three following measures detailed
hereafter:
SGmf : The safety improvements resulting from mf .
RHmf : The remaining hazard in the system after using mf .
ACmf : The system performance decrease because of mf .

1) Safety Gain: To know the overall safety added by mf
across the operational design domain D, one needs to compare
the safety of the monitored system (f, mf ) against the safety
of the initial system f . This objective is captured by the Safety
Gain metric deﬁned as:

SGmf =

(cid:16)

p(x)

(cid:90)

D

RS

(f,mf )(x) − RS

f (x)

(cid:17)

dx,

(3)

where p(x) represents the likelihood of x when randomly
sampling D. SGmf represents the “amount of hazard” that was

2In this paper, the formalism is presented in the ﬁnite-horizon setting, but

extension to inﬁnite-horizon should be straightforward.

Fig. 2: Representation of the proposed metrics. The Safety
Gain (SG), Residual Hazard (RH) and Availability Costs (AC)
are computed with respect to the returns of the predictive
function f ,
its runtime monitor mf and the ground truth
function f ∗.

removed from the system by implementing mf . Integrating
over D corresponds to averaging across all possible scenarios,
noting that a state zt(cid:54)=0 for be used as initial conditions to
deﬁne another scenario. A good monitor should have SGmf
as high as possible. If the safety scores of the different threats
identiﬁed during the preliminary safety analysis of the system
are scaled in [0, 1], then SGmf ranges between −1 and 1. If
it is not the case, we can still scale it by dividing the results
by (cid:82)
max is the maximum possible safety
reward of an event.

max) dx, where rS

D(rS

2) Residual Hazard: To model the hazard still present in
the system despite mf , one needs to compare the safety of the
monitored system (f, mf ) against the safety of the optimal
model f ∗. This objective is captured by the Residual Hazard
metric deﬁned as:
(cid:90)

(cid:16)

RHmf =

p(x)

RS

f ∗ (x) − RS

(cid:17)
(f,mf )(x)

dx.

(4)

D

It represents the “amount of hazard” that is still present in
the system, despite implementing mf . A good monitor should
have RHmf as low as possible. Regarding scaling of RHmf ,
considerations deﬁned above are also valid.

3) Availability Cost: To model the decrease in system’s
performance due to mf across D, one needs to compare
the availability of the monitored system (f, mf ) against the
availability of the initial system f . This objective is modeled
by the Availability Cost metric deﬁned as:

ACmf =

(cid:16)

p(x)

(cid:90)

D

RM

f (x) − RM

(f,mf )(x)

(cid:17)

dx.

(5)

It represents the “amount of mission reward” that was lost
by implementing mf . A good monitor should have ACmf
as low as possible. Regarding scaling of ACmf , similar
considerations apply. A visual representation of SG, AC and
RH can be seen in Figure 2.

We also note that ACmf and RHmf are complementary.
One represents the amount of existing hazard that was removed

by mf , while the other is the amount that was not handled by
mf . Their sum represents the hazard associated with f :

SGmf + RHmf =

(cid:90)

D

C. Relaxations

p(x) (cid:0)RS

f ∗ (x) − RS

f (x)(cid:1) dx.

(6)

For most practical applications, the metrics deﬁned above
cannot be computed. In this section, we will explain why and
propose relaxations of these metrics to make them usable in
practice. Practical examples to show how diverse evaluation
approaches from the literature can be expressed using these
relaxations are presented in Section III.

The ﬁrst issue is most of the relevant problems are too
complex to derive an exact model of the behavior of the
system and its entire environment under all possible external
conditions (weather, illumination, etc.). This is particularly
true for autonomous systems with diverse long-term behavior.
For a given x ∈ D, the subsequent states cannot be computed
precisely, which makes RM
(f,mf )(x) almost
impossible to compute. A solution to this problem is to develop
an alternative way to approximate these returns using what-
ever information is available. We note such approximations
ˆRM
(f,mf )(x) and ˆRS
(f,mf )(x). In Section III, we show how
very different evaluation approaches from the literature can
be presented simply as procedures to compute the safety and
mission returns.

(f,mf )(x) and RS

The second issue is that the boundaries of the operational
design domain D, and by extension, the optimal function
f ∗ are unknown a priori. This lack of speciﬁcation of the
perception function is precisely why ML was used in the ﬁrst
place. Without formal knowledge of D and f ∗, it is impossible
to compute any of the metrics deﬁned in Equations 3, 4 and 5.
To address this limitation, one can conduct an evaluation on a
predeﬁned dataset, where true values of f ∗ are available for a
speciﬁc subset of points in D. In other words, the evaluation is
conducted on a labeled dataset Deval = {(x1, y1), ..., (xn, yn)}
such that ∀i ∈ {1, ..., n}, xi ∈ D, yi = f ∗(xi). The
underlying assumption for this relaxation is that Deval
is
representative of the operational design domain,
is
it
large enough and was sampled from the expected application
context (distribution of D). Characterizing the test coverage
of a speciﬁc dataset is an active ﬁeld of research [17], but it
is beyond the scope of this paper.

i.e.,

Combining the two relaxations above, one can then proceed

to compute estimates of the three evaluation metrics:

ˆSG

Deval
mf

=

ˆRH

Deval
mf

=

ˆAC

Deval
mf

=

1
n

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:16) ˆRS

(f,mf )(xi) − ˆRS

f (xi)

(cid:17)

,

(cid:16) ˆRS

f ∗ (xi) − ˆRS

(cid:17)
(f,mf )(xi)

(cid:16) ˆRM

f (xi) − ˆRM

(cid:17)
(f,mf )(xi)

,

,

(7)

(8)

(9)

where ˆRS

f ∗ (x) is computed using the values of the yi’s.

D. Practical use

To recap, two steps are required to compute the proposed
evaluation metrics for a safety monitor in a speciﬁc application
context. First, one needs to build/select a test dataset repre-
sentative of the operational design domain and labeled with
the correct values of f ∗. Second, one need to design a pro-
(f,mf )(x) and ˆRS
cedure to compute ˆRM
(f,mf )(x), respectively
representing the expected cumulative mission reward and
safety reward from a given prediction and monitoring output.
These choices are of utmost importance as they represent the
underlying assumptions made about our system. Indeed, to
guarantee the safety of an autonomous system using ML,
one should not only ensure that it performs well under the
chosen metrics, but also that the assumptions used to deﬁne the
evaluation procedure are valid. As we will show in Section III,
in most research dealing with ML monitoring, the latter is
often disregarded. We believe that properly formulating these
evaluation choices in the formalism proposed in this paper can
help to understand the underlying assumptions made and, by
extension, evaluate their validity.

We emphasize that the proposed metrics are only valid
to assess the performance of a monitor mf with respect to
speciﬁc D and f . There is no guarantee that using the same
monitoring approach with a different model and/or application
context would result in similar performance. For safety-critical
applications requiring monitoring, we do not believe that it is
realistic to deﬁne application-agnostic and/or model-agnostic
evaluation procedures.

III. EXAMPLES FROM THE LITERATURE

This section presents three applications of runtime monitors
to different types of perception functions. For each of these
examples, we discuss how monitors have been evaluated pre-
viously, and we show how different evaluation approaches can
be expressed in terms of our metrics. In particular, we show
(f,mf )(x) and ˆRM
how to compute ˆRS
(f,mf )(x). By extension,
ˆRS
f (x) and ˆRM
f (x) can be computed by setting mf to be
the null monitor (always returning zero), and ˆRS
f ∗ (x) and
ˆRM
f ∗ (x) are computed by replacing f (x) by its corresponding
ground truth labels. For each use case, we also conduct
practical experiments and compute the proposed metrics (SG,
RH, AC) under different evaluation assumptions. We note
that the objective of this section is not to design the most
efﬁcient monitors for each use case, but rather to demonstrate
the use of the proposed metrics. Studying these applications
through the prism of the formalism presented in Section II
allows to highlight the underlying safety assumptions about
the evaluated systems. The complete code to reproduce these
experiments has been made publicly available3.

Before diving into the use cases, we remind the reader that
the design of the evaluation dataset Deval is a crucial step
to properly assess the performance of a perception system
(Section II-C). It represents the assumptions made about the

3https://github.com/jorisguerin/MLSafetyMonitors-uniﬁedEvaluation-

experiments

actual operational design domain D, i.e., the conditions that
a system can encounter during execution. Here, we focus on
demonstrating the usage of our metrics, and assume that the
evaluation datasets used in our experiments are representative
of their respective operational design domain, but in practice
this claim would require strong justiﬁcations.

A. Use Case 1 – Classiﬁcation

Classiﬁcation is the machine learning problem consisting in
building a function f mapping an input x ∈ D to an output
y ∈ [1, ..., K] among a predeﬁned discrete set of K categories.
For classiﬁcation, the generic goal of a runtime monitor is to
identify “bad input data”. Despite the simplicity of this deﬁni-
tion, researchers have used different evaluation approaches to
assess the performance of classiﬁcation monitors. This section
presents two popular evaluation schemes from the literature
and shows how both can be expressed in terms of the proposed
metrics. Preliminary experiments are proposed to compare
these evaluations, and illustrate how changing the deﬁnition
of the approximate return can lead to different results.

1) Evaluation 1: Detect Model Errors: For classiﬁcation,
the predictions of a model f are either right or wrong (unlike
other ML problems where correctness can be more gradual,
e.g., object detection, semantic segmentation). This way, sev-
eral works about runtime monitoring of classiﬁcation functions
have evaluated their monitors based on their ability to detect
errors of f [7], [8], [9], [18], [19]. Hence, the monitor is
viewed as a simple binary classiﬁer and all traditional metrics
from binary classiﬁcations (precision, recall, etc.) can be used
to evaluate mf on Deval.

We now show how these traditional metrics can be ex-
pressed in terms of our proposed metrics. For classiﬁcation
monitors, we consider that missed detections decrease safety
and wrong activations decrease availability. In our formalism,
this translates by computing the safety return for x ∈ Deval as

ˆRS

(f,mf )(x) =

(cid:40)
0
1

if f (x) (cid:54)= y and mf (x) = 0,
else.

And the mission return as

ˆRM

(f,mf )(x) =

(cid:40)
0
1

if f (x) = y and mf (x) = 1,
else.

(11)

In upcoming discussions, we consider that evaluation metrics
are always computed on Deval and drop the exponents to
simplify notations.

In the above deﬁnitions, the Safety Gain is only impacted
by the true positives of mf (Equations 7 and 10). If we divide
ˆSGmf by the fraction of wrong predictions of f , which is
independent of mf ), the safety gain becomes the recall of the
binary classiﬁer mf . Similarly, the Residual Hazard is only
impacted by false negatives of mf , and ˆRH mf divided by the
fraction of incorrect predictions of f is the false negative rate
of mf . The Availability Cost only depends on false positives
ˆAC mf by the fraction of correct
of mf , and if we divide
predictions of f , it becomes the false positive rate of mf .

The main shortcoming of this evaluation scheme is that
it does not allow to include additional knowledge about the
system application context (e.g., from the hazard analysis).
Indeed, Equations 10 and 11 only require information about
the labeled evaluation data Deval and the associated predic-
tions of f . In other words, the estimates of the returns are
independent of the system in which f is integrated. This way,
every misclassiﬁcation of f has an equal impact on both the
safety and availability of the system. This assumption does
not hold for many critical systems using classiﬁcation, e.g.
for an autonomous car road sign classiﬁer, misclassifying a
”speed limit 30” as a “speed limit 20” is not as severe as
misclassifying a ”stop” as a ”speed limit 30”. A tree-based
approach for safety analysis of classiﬁcation functions was
proposed in [20]. It can be used within our formalism to extend
this evaluation scheme and account for the asymmetry of the
safety impact of different misclassiﬁcations.

2) Evaluation 2: Detect Runtime Threats: Instead of pre-
dicting model errors, other works have evaluated classiﬁcation
monitors on the surrogate problem of identifying speciﬁc kinds
of runtime threats. A threat is deﬁned as a change in the
input data encountered at runtime, which can hinder model
performance. In practice, researchers have tested their moni-
tors on different types of threats such as Novelty [21], [22],
[23] (label does not belong to any of the predeﬁned classes),
Distributional Shift [24], [25] (image was not drawn from the
training distribution, e.g., changes in external conditions, noisy
sensors), Adversarial Attacks [26], [27] (image was modiﬁed
intentionally to deceive the monitored model).

In this evaluation setup, the dataset is composed of normal
images and threats. For an image x ∈ Deval, a binary label τ
represents whether it is a threat (τ = 1) or not (τ = 0). Then,
one can deﬁne the safety return for x as
(cid:40)

ˆRS

(f,mf )(x) =

0
1

if τ = 1 and mf (x) = 0,
else.

(10)

And the mission return as

ˆRM

(f,mf )(x) =

(cid:40)

0
1

if τ = 0 and mf (x) = 1,
else.

(12)

(13)

ˆRH mf and

labels (τ ’s) represent

Then, computing ˆSGmf ,
ˆAC mf is straightfor-
ward. The strong assumption under this setting is that the
threat
the hazard associated with a
prediction. In other words, we assume that f can be trusted for
in-distribution data (τ = 0) and that it should never be used
under the identiﬁed threats (τ = 1). The second assumption is
that the hazard associated with a prediction comes only from
a speciﬁc characteristic of the input image and not the model
being monitored. Indeed, in this setting, both the mission and
safety returns are independent of f (Eq. 12 and 13). Hence,
this evaluation setting seems unable to account for safety cases
when f does not present perfect accuracy on in-distribution
data.

3) Experiments: To illustrate the inﬂuence of these eval-
uation choices, we conduct experiments on the CIFAR10

TABLE I: Results of our classiﬁcation experiments.

(a) Images from the CIFAR10 test dataset.

E1
E2

Auto-encoder
RH
0.140
0.156

SG
0.184
0.344

AC
0.304
0.144

Classiﬁer
RH
0.251
0.414

AC
0.154
0.142

SG
0.074
0.086

(b) Corresponding images after brightness modiﬁcation.

Fig. 3: Example images to illustrate the threat generation
process used in our experiments.

dataset [28]. First, a neural network f (3 convolutional layers
followed by two dense layers) is ﬁtted to the training set of
CIFAR10, reaching a test accuracy of 0.79. Then, to monitor
this model, we consider two simple approaches: 1) Features
representing the training images are extracted from f , and
several one-class classiﬁcation (OCC) models are ﬁtted inde-
pendently to each class. In practice, we use features from the
third layer of f and an Isolation Forest [29] for OCC. To foster
conservativeness, the rejection threshold on the OCC scores
is set such that 30% of the training data are discarded. At
runtime, features are extracted from new input images, and the
OCC model decides whether to accept or reject them. 2) An
auto-encoder is ﬁtted to the CIFAR10 training set, such that
the encoding part has the same structure as the convolutional
block of the classiﬁer. Then, the same monitoring approach is
implemented using the features from the encoder.

For evaluation, we generated distributional shift threats by
modifying lighting conditions of the ten thousand test images
of CIFAR10 (Figure 3). The proposed metrics are computed
using both evaluation schemes presented above: predicting
model errors (E1) and detecting runtime threats (E2). E1
evaluates the ability of a monitor to recover errors of the neural
network, while E2 evaluates its ability to recover images with
modiﬁed brightness.

The results obtained are presented in Table I. We can see
that the choice of the approximate return functions inﬂuences
greatly the values of the proposed metrics. For example, under
E1, the auto-encoder monitor has a higher Availability Cost
than the Classiﬁer monitor. This makes sense as the auto-
encoder is independent of f and does not contain information
regarding its predictions. On the other hand, under E2, the
auto-encoder monitor has a much higher Safety Gain than its
classiﬁer counterpart, which means that auto-encoder features
are better at detecting runtime images with modiﬁed bright-
ness.

This example highlights the importance of deﬁning evalua-
tion goals ( ˆRM and ˆRS ) aligned with the high-level objectives
of the monitored system. Indeed, by studying two popular
evaluation schemes from the literature, we showed how these
choices could alter our perception of the safety and availability

of a given monitor. We do not claim here that one evaluation
scheme is better than the other. However, we believe that
following our formalism to evaluate a runtime monitor is a
good way to ensure compliance with the application objec-
tives. At the same time, the proposed metrics allowed us to
gain a uniﬁed and interpretable insight regarding the monitor
performance.

B. Use Case 2 – Object Detection for Pedestrian Avoidance

Our second example is an autonomous driving scenario
that we designed on the CARLA simulator [30]. A car is
equipped with an object detection model (YOLO-v5 [31]
trained on COCO [32]), which is used to locate pedestrians
within an emergency braking system. The scenario is designed
as follows: the car drives normally in a city road, and after
a few seconds, a pedestrian appears suddenly in front of
the vehicle after crossing between parked cars (Figure 4).
The emergency braking system receives information from the
pedestrian detector, and stops the vehicle whenever a pedes-
trian bounding box overlaps with a predeﬁned safety-critical
region (green region in Figure 4). The scenario presented here
runs for a ﬁxed number of steps (episodes of T frames). We
also add a runtime monitor to this system, consisting of two
modules:

1) The contrast variation in the input images is monitored,
and if it falls below a predeﬁned threshold, the monitor is
activated. The objective of this module is to detect faulty
images due to harsh environmental conditions (e.g., fog),
or sensor failures (e.g., blur).

2) A plausibility checker ensures that bounding boxes in
consecutive frames are consistent, e.g., no sudden ap-
pearance of bounding boxes in front of the vehicle. Such
inconsistencies can indicate ghost detections.

When the monitor detects an anomaly, the emergency breaking
system immediately stops the vehicle. For this use case as well,
we propose two different ways of computing the safety and
mission rewards, and compare their inﬂuence on the proposed
evaluation metrics.

1) Evaluation 1: Detect ML Errors: This evaluation scheme
is the adaptation of Section III-A for object detection. To
identify errors of an object detector, one must compare ground
truth bounding boxes to bounding boxes predicted by the
model. To boxes are considered to match if their labels are
identical, and both the prediction score and their intersection
over union are above ﬁxed thresholds. The binary variable τ
represents the error status of the monitored object detector for
an image x. We consider that there is an error (τ = 1) if
there is either a false positive (prediction without correspond-

(a) No perturbation. No pedes-
trian detection error.

(b) Smoke. No pedestrian de-
tection error.

(c) Grid dropout. No pedestrian
detection error.

(d) Sun ﬂare. False Negative of
the pedestrian detector.

Fig. 4: Pedestrian avoidance use case. Each image represents
a different perturbation.

ing ground truth) or a false negative (ground truth without
prediction). Then, the safety and mission returns are

ˆRS

(f,mf )(x) =

ˆRM

(f,mf )(x) =

(cid:40)
0
1

(cid:40)
0
1

if τ = 1 and mf (x) = 0,
else.

if τ = 0 and mf (x) = 1,
else.

(14)

(15)

2) Evaluation 2: Prevent accidents: This second approach
aims to assess the ability of a monitor to prevent accidents.
Conducting evaluations in a simulation environment such as
Carla presents several advantages. First, we are able to know
the exact location of every object of interest at each time
step, which allows to deﬁne the mission and safety rewards in
terms of the exact world state. Second, any initial conditions
can be reproduced to test the system’s behavior with different
perception function (e.g., with or without the monitor). Hence,
the proposed evaluation metrics can be deﬁned directly from
the values of reward functions at individual time steps.

The mission reward rM(zt), associated with conﬁguration
zt is deﬁned to be 0 when the car is stopped and 1 when
it is running. In our experiments, we make the simplifying
assumption that, when requested, the emergency braking sys-
tem stops the vehicle instantly (from one frame to the next).
Hence, the predictions obtained at time step t only impact the
mission reward at t + 1, and to compute the mission return,
Equation 1 can be adapted as:

ˆRM

(f,mf )(x) = rM(zt+1|(f, mf )(xt)).

(16)

Finally, we also consider that when emergency braking is
triggered, the episode ends and the mission rewards for all
consecutive steps is 0. This way, for a given episode, the
availability cost is the average difference in the number of
running frames with and without the monitor.

TABLE II: Results of our pedestrian avoidance experiments.

SG
0.187
0.075

RH
0.060
0.075

AC
0.065
0.800

E1
E2

The safety reward rS (zt) is deﬁned slightly differently. It
is -1 for frames when there is a collision with the pedestrian
and 0 for all other frames. Hence, the safety return is:

RS

(f,mf )(xt) =

T
(cid:88)

k=t+1

rS (zk|(f, mf )(xt)),

(17)

In other words, a prediction (f, mf )(xt) gets a nega-
tive safety reward if
in its future
({zt+1|(f, mf )(xt), ..., zT |(f, mf )(xt)}).

there is an accident

3) Experiments: Our experimental evaluation consist
in
running the same simulation scenario several times, while
injecting different kinds of faults. In particular, we generate
twelve types of image perturbation presented in [19], [33],
[34]: smoke, sun ﬂare, rain, row add logic, shifted pixel,
coarse dropout, grid dropout, channel shufﬂe, channel dropout,
contrast, brightness, and Gaussian noise (see Figure 4 for
examples). For most perturbations, several intensity levels are
tested, resulting in a total of 53 simulations. The ﬁxed number
of time steps is set to T = 220.

The proposed metrics are computed using both evaluation
schemes: detect ML errors (E1) and prevent accidents (E2).
The results are presented in Table II. Using E1, it seem that
the monitor allows to handle most of the hazards present
in the system (high SG and low RH) and does not impact
much model performance (low AC). However, when the true
temporal behavior of the system is considered (E2), the results
appear much less enthusiastic. Our experiments included 15%
of scenarios with an accident (SG + RH), and the monitor
allowed to avoid only half of them. In the meantime, the
availability was decreased by 80%.

C. Use Case 3 – Semantic Segmentation for UAV Emergency
Landing

Our last use case is built on recent work about emergency
landing of Unmanned Aerial Vehicles (UAV) in urban en-
vironments [11]. This module is triggered when the UAV
loses its localization capabilities. Then, it collects an onboard
image, reduces its resolution, and process it with a semantic
segmentation model f that classiﬁes each pixel in one of the
following categories: building, road, car, tree, low vegetation,
humans, and background. Following the safety analysis con-
ducted in [35], the binary safety ﬂag ϕk is deﬁned to represent
when a category Ck is too dangerous to land (ϕk = 1).
Here, the goal is to avoid roads and buildings. In addition, a
hazard score hk ∈ [0, 1] is deﬁned for all accepted categories
(ϕk = 0).

A candidate landing area is a circular region of ﬁxed size
radius, containing only safe pixels (Figure 5). The candidate
with the lowest hazard score ((cid:80) hk) is selected for landing.
To increase conﬁdence in f , we use the “local high deﬁnition”

TABLE III: Results of our emergency landing experiments.

SG
0.805
0.108

RH
0.195
0.212

E1
E2

the performance of the proposed monitor on the emergency
landing application.

Results are presented in Table III. Once again, we observe
that evaluating the monitor as a regular binary classiﬁer (E1)
is much more optimistic than considering its performance at
system level (E2). For this speciﬁc example, this discrepancy
comes from the fact that rejecting a valid landing zone can
severely impact safety, as other safe options might not exist.
This subtlety of emergency landing is well captured by E2, but
not by E1, which ignores false positives. We also note that,
using E2, the safety gain can sometimes be negative when the
monitor rejects good options.

IV. CONCLUSION

This paper presents new evaluation metrics for runtime
monitoring of ML perception, called Safety Gain (SG), Resid-
ual Hazard (RH), and Availability Cost (AC). These metrics
represent different aspects of the system in which the ML
function is used: the safety beneﬁts of using the monitor
(SG), the remaining threats despite the monitor (RH), and the
negative impact of the monitor on the system’s performance
(AC). This formulation relies on expressing the future returns
(cumulative rewards) for safety and mission objectives. To
show that these metrics are generic and ﬂexible, we demon-
strated how they can be used for three independent use cases,
representing different application contexts. Experiments on
each use case demonstrate the importance of properly for-
mulating the assumptions about the system evaluated. Indeed,
we show that different deﬁnition of the return functions can
drastically change the perceived performance of the monitor.
In summary, this work is a step towards unifying the ﬁeld
of ML runtime monitoring, allowing to compare approaches
across different scenarios using common criteria. Using our
formalism helps to ensure that evaluation is aligned with the
actual system requirements.

A possible future work direction would consist in building a
set of evaluation scenarios representing real-world applications
of ML, using both simulated environments and real-world
data. This would allow a proper benchmark study of existing
runtime monitoring techniques using the metrics introduced
in this paper. Such natural extensions of our work has the
potential to play a major role in the development of future
autonomous systems, as it allows for a better assessment of
the safety of such critical ML-based functions.

ACKNOWLEDGEMENTS

This research has beneﬁted from the AI Interdisciplinary
Institute ANITI. ANITI is funded by the French ”Investing
for the Future – PIA3” program under the Grant agreement
No ANR-19-PI3A-0004.

Fig. 5: Example outputs for Emergency Landing. Circles
are candidates identiﬁed by the main model, colors indicate
runtime monitor status. The default action consists in opening
a parachute without moving. Source: [11].

monitor. It consists in using the full resolution image to reﬁne
the semantic segmentation of small patches containing the
candidates, allowing to improve critical predictions, while still
controlling computation time. The monitor rejects a candidate
if its new segmentation contains a forbidden pixel (ϕk = 1).
If no suitable candidate was found by the (f, mf ) pair, the
default action consists in stopping the motors and opening
a parachute at the current location. Figure 5 illustrates the
outputs of emergency landing on an example image.

1) Evaluation 1 – Detect wrong candidates: As its name
implies, emergency landing is an emergency procedure which
only goal is to ensure people’s safety. Hence, for this ap-
plication, we are not concerned with mission progress and
availability cost is not a relevant metric. The safety return of
a candidate can be computed using Equation 14, where τ = 1
for candidates containing unsafe pixels. Then, the safety return
for the entire image is simply the average across candidates.
2) Evaluation 2 – Compare selected landing zones: An-
other way of evaluating a monitor is to compare the ﬁnal
candidate that was actually selected for landing with and
without it. Let (x, y) be an input image and its associated
ground truth segmentation image, and let y(p) be the ground
truth class of pixel p. The safety score of a landing candidate
Lc in x is

S(x[Lc]) =

(cid:40)
0
κ + (1 − κ)Ep[1 − hy(p)]

if ∃p ∈ x[Lc] s.t. ϕy(p) = 1,
else,

(18)

where κ ∈ [0, 1] deﬁnes a gap to separate unsafe candidates
from others. From this deﬁnition, ˆRS
(f,mf )(x) can be deﬁned
as the value of S(x[Lc]) when (f, mf )(x) selects landing zone
Lc.

3) Experiments: The proposed experiments are conducted
on the 70 images of the validation set of UAVid [36]. Five
types of perturbations are applied (brightness, fog, motion
blur, pixel trap and shifted pixels), leading to a total of 420
images. Both evaluation schemes are compared to evaluate

[19] R. S. Ferreira, J. Arlat, J. Guiochet, and H. Waeselynck, “Benchmarking
safety monitors for image classiﬁers with machine learning,” in 2021
IEEE 26th Paciﬁc Rim International Symposium on Dependable Com-
puting (PRDC).

IEEE, 2021, pp. 7–16.

[20] R. Salay, M. Angus, and K. Czarnecki, “A safety analysis method
for perceptual components in automated driving,” in 2019 IEEE 30th
International Symposium on Software Reliability Engineering (ISSRE).
IEEE, 2019, pp. 24–34.

[21] Y. Sun, C. Guo, and Y. Li, “React: Out-of-distribution detection with
rectiﬁed activations,” Advances in Neural Information Processing Sys-
tems, vol. 34, 2021.

[22] C. Schorn and L. Gauerhof, “Facer: A universal framework for detecting
anomalous operation of deep neural networks,” in 2020 IEEE 23rd
International Conference on Intelligent Transportation Systems (ITSC).
IEEE, 2020, pp. 1–6.

[23] A. Lukina, C. Schilling, and T. A. Henzinger, “Into the unknown: Active
monitoring of neural networks,” in International Conference on Runtime
Veriﬁcation. Springer, 2021, pp. 42–61.

[24] S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-
of-distribution image detection in neural networks,” in International
Conference on Learning Representations, 2018.

[25] Y.-C. Hsu, Y. Shen, H. Jin, and Z. Kira, “Generalized odin: Detecting
out-of-distribution image without learning from out-of-distribution data,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020, pp. 10 951–10 960.

[26] Y. Kantaros, T. Carpenter, S. Park, R. Ivanov, S. Jang, I. Lee, and
J. Weimer, “Visionguard: Runtime detection of adversarial inputs to
perception systems,” arXiv preprint arXiv:2002.09792, 2020.

[27] J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang, “Adversarial sample
detection for deep neural network through model mutation testing,” in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE).

IEEE, 2019, pp. 1245–1256.

[28] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features

from tiny images,” 2009.

[29] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008 eighth
ieee international conference on data mining. IEEE, 2008, pp. 413–422.
[30] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proceedings of the 1st
Annual Conference on Robot Learning, 2017, pp. 1–16.

[31] G.

Jocher, A. Stoken, A. Chaurasia,

J. Borovec, NanoCode,
J. Fang, A. V,
TaoXie, Y. Kwon, K. Michael, L. Changyu,
Laughing,
tkianai, yx NONG, P. Skalski, A. Hogan, J. Nadar,
imyhxy, L. Mammana, A. Wang, C. Fati, D. Montes, J. Hajek,
L. Diaconu, M. T. Minh, Marc, albinxavi, fatih, oleg, and wang
haoyang, “Yolov5: v6.0,” [Online; accessed 21-January-2022]. [Online].
Available: https://doi.org/10.5281/zenodo.5563715

[32] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755.

[33] F. Secci and A. Ceccarelli, “On failures of rgb cameras and their effects
in autonomous driving applications,” in 2020 IEEE 31st International
Symposium on Software Reliability Engineering (ISSRE).
IEEE, 2020,
pp. 13–24.

[34] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin,
and A. A. Kalinin, “Albumentations: fast and ﬂexible image augmenta-
tions,” Information, vol. 11, no. 2, p. 125, 2020.

[35] J. Gu´erin, K. Delmas, and J. Guiochet, “Certifying emergency landing
for safe urban uav,” in 7th International Workshop on Safety and
Security of Intelligent Vehicles (SSIV 2021) at IEEE/IFIP Intern. Conf.
on Dependable Systems and Networks (DSN), 2021.

[36] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, and M. Y. Yang, “Uavid:
A semantic segmentation dataset for uav imagery,” ISPRS Journal of
Photogrammetry and Remote Sensing, 2020.

This research has also received funding from the European
Union’s Horizon 2020 research and innovation program under
the Marie Skłodowska-Curie grant agreement No 812.788
(MSCA-ETN SAS). This publication reﬂects only the authors’
view, exempting the European Union from any liability. Project
website: http://etn-sas.eu/.

REFERENCES

[1] M. G. Calvi, “Runtime monitoring of cyber-physical systems using data-
driven models,” Ph.D. dissertation, University of Illinois at Chicago,
2019.

[2] C. Premebida, R. Ambrus, and Z.-C. Marton, “Intelligent robotic percep-
IntechOpen London,

tion systems,” in Applications of Mobile Robots.
UK, 2018.

[3] A. Brunetti, D. Buongiorno, G. F. Trotta, and V. Bevilacqua, “Com-
puter vision and deep learning techniques for pedestrian detection and
tracking: A survey,” Neurocomputing, vol. 300, pp. 17–33, 2018.
[4] J. M. Faria, “Machine learning safety: An overview,” in Proceedings of

the 26th Safety-Critical Systems Symposium, York, UK, 2018.

[5] S. Mohseni, M. Pitale, V. Singh, and Z. Wang, “Practical solutions for
machine learning safety in autonomous vehicles,” in Proceedings of the
Workshop on Artiﬁcial Intelligence Safety, 2020.

[6] Q. M. Rahman, P. Corke, and F. Dayoub, “Run-time monitoring of
machine learning for robotic perception: A survey of emerging trends,”
IEEE Access, vol. 9, pp. 20 067–20 075, 2021.

[7] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassiﬁed
and out-of-distribution examples in neural networks,” in International
Conference on Learning Representations, 2017. [Online]. Available:
https://openreview.net/forum?id=Hkg4TI9xl

[8] F. Granese, M. Romanelli, D. Gorla, C. Palamidessi, and P. Piantanida,
“Doctor: A simple method for detecting misclassiﬁcation errors,” arXiv
preprint arXiv:2106.02395, 2021.

[9] H. Wang, J. Xu, C. Xu, X. Ma, and J. Lu, “Dissector: Input validation
for deep learning applications by crossing-layer dissection,” in 2020
IEEE/ACM 42nd International Conference on Software Engineering
(ICSE).

IEEE, 2020, pp. 727–738.

[10] C. Buerkle, F. Geissler, M. Paulitsch, and K.-U. Scholl, “Fault-tolerant
perception for automated driving a lightweight monitoring approach,”
arXiv preprint arXiv:2111.12360, 2021.

[11] J. Guerin, K. Delmas, and J. Guiochet, “Evaluation of runtime monitor-
ing for UAV emergency landing,” in 2022 IEEE International Confer-
ence on Robotics and Automation (ICRA).

IEEE, to appear.

[12] A. Stocco, M. Weiss, M. Calzana, and P. Tonella, “Misbehaviour
the
prediction for autonomous driving systems,” in Proceedings of
ACM/IEEE 42nd International Conference on Software Engineering,
2020, pp. 359–371.

[13] D. Cofer, I. Amundson, R. Sattigeri, A. Passi, C. Boggs, E. Smith,
L. Gilham, T. Byun, and S. Rayadurgam, “Run-time assurance for
learning-based aircraft taxiing,” in 2020 AIAA/IEEE 39th Digital Avion-
ics Systems Conference (DASC).

IEEE, 2020, pp. 1–9.

[14] P. Popov and L. Strigini, “Assessing asymmetric fault-tolerant software,”
in 2010 IEEE 21st International Symposium on Software Reliability
Engineering.

IEEE, 2010, pp. 41–50.

[15] Y. Geifman and R. El-Yaniv, “Selectivenet: A deep neural network with
an integrated reject option,” in International Conference on Machine
Learning. PMLR, 2019, pp. 2151–2159.

[16] P. Koopman and F. Fratrik, “How many operational design domains,
objects, and events?” in Workshop on Artiﬁcial Intelligence Safety
2019 co-located with the Thirty-Third AAAI Conference on Artiﬁcial
Intelligence 2019 (AAAI-19), Honolulu, Hawaii, January 27, 2019,
´O. h ´Eigeartaigh,
ser. CEUR Workshop Proceedings, H. Espinoza, S.
X. Huang, J. Hern´andez-Orallo, and M. Castillo-Effen, Eds., vol. 2301.
CEUR-WS.org, 2019. [Online]. Available: http://ceur-ws.org/Vol-2301/
paper 6.pdf

[17] J. Chen, M. Yan, Z. Wang, Y. Kang, and Z. Wu, “Deep neural network
test coverage: How far are we?” arXiv preprint arXiv:2010.04946, 2020.
[18] C.-H. Cheng, G. N¨uhrenberg, and H. Yasuoka, “Runtime monitoring
neuron activation patterns,” in 2019 Design, Automation & Test
in
Europe Conference & Exhibition (DATE).
IEEE, 2019, pp. 300–303.

