2
2
0
2

p
e
S
5
1

]
E
S
.
s
c
[

1
v
8
4
0
7
0
.
9
0
2
2
:
v
i
X
r
a

AutoUpdate: Automatically Recommend Code Updates for
Android Apps

YUE LIU, CHAKKRIT TANTITHAMTHAVORN, and YONGHUI LIU, Monash University, Aus-
tralia
PATANAMON THONGTANUNAM, The University of Melbourne, Australia
LI LI, Monash University, Australia

Android developers frequently update source code to improve the performance, security, or maintainability of
Android apps. Such Android code updating activities are intuitively repetitive, manual, and time-consuming. In
this paper, we propose AutoUpdate, a Transformer-based automated code update recommendation approach
for Android Apps, which takes advantage of code abstraction (Abs) and Byte-Pair Encoding (BPE) techniques
to represent source code. Since this is the first work to automatically update code in Android apps, we collect a
history of 209,346 updated method pairs from 3,195 real-world Android applications available on Google Play
stores that span 14 years (2008-2022). Through an extensive experiment on our curated datasets, the results
show that AutoUpdate (1) achieves a perfect prediction of 25% based on the realistic time-wise evaluation
scenario, which outperforms the two baseline approaches; (2) gains benefits at least 17% of improvement
by using both Abs and BPE; (3) is able to recommend code updates for various purposes (e.g., fixing bugs,
adding new feature, refactoring methods). On the other hand, the models (4) could produce optimistically high
accuracy due to the unrealistic evaluation scenario (i.e., random splits), suggesting that researchers should
consider time-wise evaluation scenarios in the future; (5) are less accurate for a larger size of methods with a
larger number of changed tokens, providing a research opportunity for future work. Our findings demonstrate
the significant advancement of NMT-based code update recommendation approaches for Android apps.
CCS Concepts: • Software and its engineering → Software testing and debugging.

Additional Key Words and Phrases: Android Applications, Code Updates, Neural Machine Translation

ACM Reference Format:
Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li. 2022. AutoUpdate:
Automatically Recommend Code Updates for Android Apps. 1, 1 (September 2022), 20 pages. https://doi.org/
10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Android has become the most popular smartphone operating system (OS), occupying 70% of the
mobile OS market shares [53]. Over three billion active devices are running Android in the wild [10].
The number of Android apps developed to run on those devices is likewise extremely large, with
more than 2.5 million apps distributed on the official Android app market, namely the Google
Play Store [53]. The Android team (i.e., Google and the Open Handset Alliance) has committed to
continuously improving the system in order to maintain its popularity. It regularly publishes new

Authors’ addresses: Yue Liu, yue.liu1@monash.edu; Chakkrit Tantithamthavorn, chakkrit@monash.edu; Yonghui Liu,
Yonghui.Liu@monash.edu, Monash University, Wellington Road, Clayton, Victoria, Australia; Patanamon Thongtanunam,
patanamon.t@unimelb.edu.au, The University of Melbourne, Grattan Street, Parkville, Victoria, Australia; Li Li, Li.Li@
monash.edu, Monash University, Wellington Road, Clayton, Victoria, Australia.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/9-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: September 2022.

 
 
 
 
 
 
2

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Android versions with new features or fixing bugs/vulnerabilities. Code 1 presents an example
of Android-specific code updates [36]. In Android API 22 (released in 2015), getDrawable (int id)
is deprecated officially and is replaced by getDrawable(int, android.content.res.Resources.Theme)
instead, which is discussed by Android developers in Stackoverflow [52]. Thus, the source code of
the application "org.kontalk_500.apk" was updated after the new Android API version was released,
as shown in Code 1. Such updates are quite frequent, repetitive, and difficult to cope with manually.
Based on the official developer report [4], the recent Android API 33 (i.e., Android 13, released
in 2022) introduced up to 2.5k API updates. To cope with such a fast evolution of the Android
ecosystem, app developers have to regularly update their apps as well, e.g., to utilize new features
or fix compatibility issues caused by the new version of the Android system [41, 44].

Code 1. An example of code updates for an Android app (org.kontalk_500.apk).

private Drawable getDefaultContactImage () {

if ( sDefaultContactImage == null )

sDefaultContactImage = context . getResources ()

. getDrawable ( R . drawable . ic_contact_picture );

sDefaultContactImage = ResourcesCompat

. getDrawable ( getContext () . getResources () ,
R. drawable . ic_default_contact , getContext () . getTheme () ) ;

return sDefaultContactImage ;

-
-
+
+
+

}

However, it is non-trivial to continuously update the Android apps to keep up the breakneck
release pace of the Android OS, while maintaining the quality and security of their mobile apps.
On the one hand, quality and security issues are hidden in the app code that cannot be easily
discovered. Such issues may remain in the app for many generations. For example, as demonstrated
by Gao et al. [19], many security issues in a set of real-world Android apps are not really fixed by
their subsequent versions, based on their evolution study over app lineages. On the other hand,
even if app developers can spot the issues (e.g., with the help of promising tools), they may not have
the right knowledge to fix them correctly. For example, as also experimentally revealed by Gao et
al. [18] in their app lineage studies, the changes made by developers to fix misuses of cryptographic
APIs are not always correct. These pieces of evidence suggest that there is a strong need to invent
automated approaches to help app developers update Android apps.

To this end, our fellow researchers have spent significant efforts to help developers update their
Android apps. For example, Lamothe et al. [37] have proposed a data-driven approach to learning
from code examples to assist Android API migrations. Their A3 tool can successfully learn API
migration patterns and provide API migration assistance in 71 out of 80 cases in open-source Android
apps. Zhao et al. [74] have proposed RepairDroid to automatically repair compatibility issues in
published Android apps through template-based patches. Their approach can successfully fix 7,660
out of 8,976 compatibility issues in 1,000 randomly selected Google Play apps. The aforementioned
approaches are mostly limited to resolving certain issues through dedicated program analysis
approaches, which, although promising, yet cannot be easily extended to support fixes of general
issues appearing in Android apps.

In this work, we propose AutoUpdate, a Transformer-based Neural Machine Translation (NMT)
approach to automatically learn and subsequently recommend generic code updates for Android
apps. Since this is the first paper to explore an NMT-based approach to automatically recommend
code updates for Android apps, there exists no publicly-available Android apps’ code change
datasets to be used. To address this challenge, we collect pairs of updated methods from 3,195

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

3

Android apps that have been published on Google Play and are also hosted on GitHub from 2008
to 2022. For each pair of methods, we employ both code abstraction in order to preserve existing
tokens that will be appeared in the updated version and a Byte-Pair Encoding (BPE) subword
tokenization [51] to generate new tokens that never exist in the prior version. Then, we build a
Transformer-based NMT model [66], instead of using traditional RNN-based NMT approaches. We
address the following five research questions:

(RQ1) How accurate and efficient is our AutoUpdate to recommend code updates for An-

droid apps?
Results. Our AutoUpdate achieves a perfect prediction of 25% for recommending code
updates for Android apps, which is 59%-107% better than the two baselines (i.e., Tufano et
al. [62] and AutoTransform [59]) under a realistic time-wise evaluation scenario. In addition,
our AutoUpdate achieves a higher efficiency for recommending code updates compared
with the Tufano et al. [62]’s approach and AutoTransform [59].

(RQ2) What is the benefit of our tokenization approach (Abs+BPE) for our AutoUpdate?
Results. Our tokenization approach (Abs + BPE) gains benefits of at least 17% by using both
code abstraction and BPE for recommending code updates, confirming that the combination
of using both Abs and BPE in our AutoUpdate outperforms both Abs alone and BPE alone.
(RQ3) What types of code updates are most accurately recommended by our AutoUp-

date?
Results. Our AutoUpdate can recommend nine types of code updates for Android apps, e.g.,
26% perfect predictions for fixing bug and 22% perfect predictions for adding new features,
demonstrating the potential impact of our AutoUpdate in real-world usage scenarios.
(RQ4) What is the impact of the time-wise evaluation on the accuracy of code update rec-

ommendation approaches?
Results. We found that the time-ignore evaluation scenario used in prior studies [59, 62] (i.e.,
random splits which is non-realistic) produces optimistically higher accuracy than the time-
wise evaluation scenario (realistic), suggesting that researchers should consider time-wise
evaluation scenarios in the future.

(RQ5) How do the method size and update size impact the accuracy of our AutoUpdate?
Results. Different from the Tufano et al. [62]’s approach that is limited to small size methods
only, our results show that AutoUpdate achieves 133% higher accuracy than the baseline
approach when considering any size of methods. Nevertheless, our AutoUpdate is less
accurate when they are applied to a larger size of methods with a larger number of updated
tokens, providing a research opportunity for future work.

Novelty. To the best of our knowledge, this paper is the first to:

• Present AutoUpdate, a Transformer-based neural machine translation approach to recom-

mend code updates for Android applications.

• Publish a dataset with 209k updated methods from 3,195 open-source Android applications,
which will be publicly available upon the acceptance. We will provide detailed metadata for
each code update, including the corresponding timestamp and commit message. To the best
of our knowledge, this is the largest size of method pairs of code updates for Android apps.
• Empirically demonstrate that AutoUpdate outperforms baseline approaches based on the
realistic evaluation scenario. Our findings also provide recommendations (e.g., considering
the time-wise evaluation scenario) and opportunities for future work (e.g., improving the
accuracy for larger and complex methods).

, Vol. 1, No. 1, Article . Publication date: September 2022.

4

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Fig. 1. An overview of our AutoUpdate for automatically recommending code updates

Open Science. To support the open science initiative, we publish the studies dataset and a

replication package, which is publicly-available in GitHub.1

Paper Organization. Section 2 presents our AutoUpdate approach, including data preparation,
model training, and model inference. Section 3 presents our studied datasets and the experimental
setup, while Section 4 presents our research questions and the experimental results. Section 5
presents the related work. Section 6 discloses the threats to validity. Section 7 draws the conclusions.

2 AUTOUPDATE: AUTOMATICALLY RECOMMEND CODE UPDATES FOR ANDROID

APPS

In this section, we present our AutoUpdate, which consists of three stages: data preparation, model
training, and code updates recommendation. Figure 1 presents an overview of our AutoUpdate.

2.1 Data Preparation
2.1.1 Data Collection. Since this is the first paper to explore an NMT-based approach to auto-
matically recommend code updates, there is no publicly-available datasets of code updates for
Android apps. Thus, we perform the following steps to collect and prepare the first benchmark
datasets for code update recommendations in Android Apps. To collect code updates in Android
apps, we start with a set of Android apps from the AndroZooOpen [43] dataset, consisting of 46,521
open-source Android apps between 2008 and 2020. AndroZooOpen provides the metadata of the
app repositories and the Google Play pages if the apps are published on Google Play. In this work,
we consider only open-source Android apps that are published on Google Play to eliminate toy,
experimental repositories, or low-quality apps. Thus, we obtained 3,195 open-source Android apps
from AndroZooOpen. Then, based on the metadata, we cloned Git repositories of these apps and
identified 1.3M human-written commits which will be further extracted.

2.1.2 Data Extraction. To extract all code modifications in the 1.3M commits that we collected,
we utilize the git diff command in Git to identify which parts of code are updated in the two
versions (i.e., before and after the code update). In this work, we identify code updates at the
method level based on the following reasons: (1) a method implements a single functionality which
provides sufficient context; (2) file-level code updates may include multiple method-level code
transformations, and files contain large corpora of text that may include significant amounts of
unchanged code, impairing the performance of NMT models [62]. To better evaluate code updates,
we ignore and remove developers’ comments inside code blocks. We construct a list of triplets
(i.e., the method block before the updates (we called prior, henceforth), the method block after

1https://github.com/AndroidAutoUpdate/AutoUpdate

, Vol. 1, No. 1, Article . Publication date: September 2022.

Code Updates Recommendation4Merge OperationsModel ArchitectureAutoUpdate Model32Training StageTraining Data (Pairs, before versions and after versions)Inference StageSubword Tokenization (BPE)Subword  Tokenized Code PairsSubword VocabulariesSubword TokenizerSubword  Tokenized CodeTesting Data (before versions)Embedding vectorsWord  Embedding3a2a2bBeam SearchEncoder Stack3bFeed-ForwardDecoder Stack3c+LayerNormSelf-AttentionEncoder 1…Encoder 4Hidden State of Encoder 4Decoder 43dLinear + Softmax…Feed-ForwardMasked Self-AttentionSelf-AttentionLayerNormDecoder 1Data PreparationCollecting Open-source Android Applications …Git diﬀ…Original Updated Source Code Pairs (before versions and after verison)Code Abstraction1Code Abstraction1Generated SequencesPredictionsConvert Subwords to CodeAutoUpdate: Automatically Recommend Code Updates for Android Apps

5

Table 1. The number of method pairs of our curated datasets. #Small refers to the dataset of small method
pairs (i.e., less than 50 tokens in a method and less than 5 changed tokens between versions) using for
RQ1-RQ4, while #All refers to all of the method pairs using for RQ5.

Year

2008 ~2011

2012

2013

2014

2015

2016

2017

2018

2019

2020

2021

2022

Total

#All (Small + Large)
#Small

660
73

1,200
110

1,424
255

4,810
740

11,376
1,841

28,055
4,356

48,224
6,873

44,943
6,488

30,474
3,812

20,090
2,907

18,090
2,739

5,534
581

209,346
30,194

updates (we called updated, henceforth), and the corresponding commit messages) to store the
updated methods. Finally, we extracted 209,346 pairs of updated methods with their corresponding
commit messages. The dataset is represented as a list of triplets, i.e., {(𝑚1, 𝑚′
𝑁 , 𝑐𝑁 )},
1
where 𝑚𝑖 represents the version of a method prior to the code update, 𝑚′
𝑖 represents the updated
version, and 𝑐𝑖 represents the corresponding commit message. Table 1 shows the number of updated
methods occur in between 2008 and 2022.

, 𝑐1), ..., (𝑚𝑁 , 𝑚′

2.2 Model Training
Overview. We formulate our Automated Android App Updates (AutoUpdate) as a Neural Machine
Translation (NMT) task [68] using a Transformer-based Encoder-Decoder architecture. The entire
procedure consists of the following four steps. In Step 1 , we perform code abstraction to reduce
vocabulary size by replacing actual identifier/literals with reusable IDs. In Step 2 , we perform
subword tokentization using a Byte-Pairs Encoding (BPE) approach in order to produce a sequence
of subword code tokens for each method (henceforth, we call a subword tokenized method). In
Step 3 , we build a Transformer-based NMT model given the subword tokenized methods in the
training data. For each subword tokenized method, in Step 3a , AutoUpdate performs a word
embedding in order to generate an embedding vector for each token and combine into a matrix.
Then, in Step 3b , the matrix is fed into the Transformer encoder stack and the output of the last
Transformer encoder is fed into each Transformer decoder in Step 3c . In Step 3d , the output of
the Transformer decoder stack is fed into a linear layer with softmax activation to generate the
probability distribution of the vocabulary. Finally in Step 4 , we leverage the trained Transformer
NMT model to generate a code update.

Code tokenization plays an important role in the code representation component for any NMT-
based models. The primary purpose of code tokenization is to help NMT-based models understand-
ing the context and meaning of each individual code token in the input sequences. Various code
tokenization approaches are used for source code. For example, Tufano et al. [62] suggested that
code abstraction could substantially reduce the size of vocabularies. However, Thongtanunam et
al. [59] argue that, when code abstraction is used together with word-level tokenization, such the
approach may generate an excessively large number of vocabularies. Therefore, an NMT model
cannot correctly generate code when new tokens appear in the later version, but never appear
in the prior version. Suggested by Thongtanunam et al. [59] and Karampatsis et al. [33], BPE is
recommended to alleviate the Out-Of-Vocabulary (OOV) problems. However, both code abstraction
and BPE have its own advantages and disadvantages. While code abstraction can preserve existing
tokens that will be appeared in the updated version, it cannot generate new tokens that never exist
in the prior version. While BPE can generate new tokens that never exist in the prior version, it
cannot preserve existing tokens that will be appeared in the updated version (i.e., cannot correctly
regenerate the complex combination of sub-tokens in the updated version). Different from prior
work, our AutoUpdate leverages both code abstraction and BPE in order to preserve existing
tokens that will be appeared in the updated version and generate new tokens that never exist in
the prior version. We explain each step below.

, Vol. 1, No. 1, Article . Publication date: September 2022.

6

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Step 1 : Code Abstraction. We adopt the code abstraction process described by Tufano et al. [62]
to reduce the vocabulary size by replacing the actual values of identifier/literal with reusuable IDs.
For example, the variables in a method are abstracted as VAR_X, where X indicates the number of
the variable in the method (e.g., the first variable appeared in the method is assigned as VAR_1).
The keywords and punctuation symbols of the programming language remain untouched. Since
the mapping between the actual and abstracted tokens is maintained, we can recreate the concrete
code from the abstract code. We leverage the src2abs library [61] to abstract code tokens and
build a map M between the actual values of identifier/literals and the IDs for each method. For the
training and validation data, we construct the mapping based on both prior and updated versions
to ensure that the identifier/literal appearing in both versions use the same ID. For the testing data,
we construct the mapping based on the prior version only.

Step 2 : BPE Subword Tokenization. In step 2 , we leverage the Byte Pair Encoding (BPE)
approach [50] to perform subword tokenization. BPE is an algorithm that will split rare code
tokens into meaningful subwords, while preserving the common code tokens (i.e., will not split
the common words into smaller subwords). The use of BPE subword tokenization will help reduce
the vocabulary size when tokenizing various code tokens, since it will split rare code tokens into
multiple subwords instead of adding the full tokens into the vocabulary directly [6, 33, 59]. In
addition, the use of BPE will ensure that new tokens that never appear in the prior version can be
generated in the updated version for the same method. BPE consists of two main steps. Step 2a is
to generate merge operations to determine how a code token should be splitted, and Step 2b is
to apply the merge operations based on the subword vocabularies. Specifically, BPE will split all
code tokens into sequences of characters and identify the most frequent token pair (e.g., the pair of
two consecutive characters) that should be merged into a new token. For instance, a variable name,
column, will be split into a list of subwords, i.e., ["col", "umn"].

3 Learning to Recommend Code Updates for Android Apps. Formally speaking, we build
a Transformer-based NMT model where the learning objective of our AutoUpdate model is to
learn the mapping between an before version of a method 𝑋𝑖 and an updated version of a method 𝑌𝑖 .
In particular, our AutoUpdate model is composed of Encoder layers and Decoder layers, where the
Encoder takes a sequence of code tokens as an input in order to map a initial method 𝑋𝑖 = [𝑥1, ..., 𝑥𝑛]
into a fixed-length intermediate hidden state 𝐻 = [ℎ1, ..., ℎ𝑛]. Then, the decoder takes the hidden
state vector 𝐻 as an input to generate the output sequence of tokens 𝑌𝑖 = [𝑦1, ..., 𝑦𝑚]. We note
that 𝑛 (i.e., the length of the input sequence) and 𝑚 (i.e., the length of the output sequence) can be
different. To optimize the mapping, the parameters of the AutoUpdate model are updated using
the training dataset with the following equation to maximize the conditional probability:

𝑝 (𝑌𝑖 | 𝑋𝑖 ) = 𝑝 (𝑦1, ..., 𝑦𝑚 | 𝑥1, ..., 𝑥𝑛) =

𝑚
(cid:214)

𝑖=1

𝑝 (𝑦𝑖 | 𝐻, 𝑦1, ..., 𝑦𝑖−1)

Our AutoUpdate model is a Transformer-based Encoder-Decoder architecture with the self-
attention mechanism [66]. The Transformer model performs the following two main steps. First, in
Step 3a , given a subword sequence of the initial version, the Transformer embeds the tokens into
vectors and uses positional encoding to add information about the token position of the sequence
into the embedding vectors. Second, the embedding vectors are then fed into the encoder block 3b ,
which is composed of a stack of multiple Encoder layers (where 𝑁𝑒 is the number of Encoder layers).
Each layer consists of two sub-layers: a multi-head masked self-attention and a fully-connected
feed forward network (FFN) which computes attention weights and generates attention vectors.
The masked self-attention mechanism is used during the training phase in order to restrict the
model from attending the next token that should not be seen. Thus, the model can only attend

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

7

previous tokens during the generation phase. Finally, the attention vectors will be used to inform
the Decoder block about which tokens that should be paid attention.

2.3 Code Updates Recommendation
Given the output of softmax probabilities, in Step 4 , we leverage beam search to select multiple code
update candidates for an input sequence at each timestep based on a conditional probability. The
number of code update candidates relies on a parameter setting called Beam Width 𝛽. Specifically,
the beam search selects the best 𝛽 candidates with the highest probability using a best-first search
strategy at each timestep. The beam search will be terminated when the EOS token (i.e., the end of
a sequences) is generated.

3 EXPERIMENTAL SETUP
In this section, we describe the experimental setup for our study. We discuss our experimental
setups from five aspects: studied datasets, data splitting, evaluation measures, baseline approaches,
and implementation & hyperparameter settings.

Studied Datasets. As was mentioned in Section 2, we construct a new dataset to evaluate the
performance of automatically recommending code updates for Android apps. Table 1 describes
the statistics of the dataset. We construct two versions for this dataset. The whole dataset (i.e.,
#All) consists of 209,346 pairs of updated methods from 3195 open-source Android apps. For each
updated method, it contains a pair of changed methods’ source code. Prior work [63] has noted
that large methods have a long tail distribution of sequence lengths with a high variance, which
may present difficulties when training an NMT model. Thus, we build a small version on the #All
dataset which disregards the updated methods having a sequence longer than 50 tokens. Finally, we
obtained 30,194 pairs of small updated methods. Note that we use the small dataset for evaluating
RQ1 - RQ4, while the #All dataset for evaluating RQ5.

Data Splitting. To mimic a realistic recommendation scenario, we introduce a time-wise evalu-
ation, which concerns the chronological order of the code updates (i.e., commits’ dates). Unlike the
random split, this time-wise evaluation ensures that the data used for training is the code updates
that occur before the code updates in the validation and test data (i.e., only code updates in the
past are used to recommend the code updates in the future). Thus, we used 136,932 code updates
occurring before 2020 as training data, 34,234 code updates occurring before 2020 as validation
data, and the remaining code updates (i.e., 43,714) occurring after 2020 as testing data. For the small
dataset, we used 19,638 code updates occurring before 2020 as training data, 4,910 code updates
occurring before 2020 as validation data, and the remaining code updates (i.e., 6,227) occurring
after 2020 as testing data. Table 1 presents the distribution of our data.

Evaluation Measures. To evaluate the accuracy, we count the number of updated methods
that achieve a perfect prediction (PP), i.e., the generated updated method exactly matches the
ground-truth (i.e., the actual updated method). In this work, we use the beam search to obtain
best-𝑘 candidates (i.e., 𝑘 = 1, 5, 10, 15). If one of 𝑘 candidates exactly matches the ground-truth, we
consider the prediction is perfect. In this work, we also use the CodeBLEU [49] score to measure
the similarity between the generated predictions and the ground-truth. CodeBLEU considers the
BLEU score which is a value between 0 and 1 that measures the similarity between one sequence to a
set of the ground-truth using constituent n-grams precision scores. In this work, we use the BLEU-4
variant, computed by considering the 4-grams in the generated text and previously used in other
software engineering papers (e.g., [63, 67]). Furthermore, CodeBLEU also considers grammatical

, Vol. 1, No. 1, Article . Publication date: September 2022.

8

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Table 2. (RQ1) The accuracy of our AutoUpdate comparing to the baseline approaches.

Approach

Beam = 15

Beam = 10

Beam = 5

Beam = 1

PP% CodeBLUE PP% CodeBLUE PP% CodeBLUE PP% CodeBLUE

AutoUpdate (Abstraction + BPE + Transformer)
Tufano et al. (Abstraction + RNN) [62]†
AutoTransform (BPE + Transformer) [59]†

0.912
0.902
0.905
0.664
0.912
0.893
†Note that these approaches were designed for automated code transformation in the code review process. Since there is no existing approach that automatically generates
code updates in Android apps, these two approaches which are closely related to our work are used as the baseline approaches.

0.903
0.664
0.897

0.902
0.664
0.894

23%
13%
10%

25%
15%
11%

18%
9%
8%

7%
3%
3%

and logical correctness based on the abstract syntax tree and the data-flow structure. A CodeBLEU
of 1 indicates that the generated sequence is the same as the ground-truth.

Baseline Approaches. Since this work is the first to explore an NMT-based approach to auto-
matically recommend code updates for Android apps, there is no state-of-the-art approach for
this problem domain. Nevertheless, we use the NMT-based approaches that are closely related to
code updates as the baseline approaches. Therefore, we opt to use the NMT-based approaches of
Tufano et al. [62] and Thongtanunam et al. [59] which were proposed to automatically improve
source code during the code review process. Tufano et al. [62] proposed an NMT-based code trans-
formation approach using code abstraction with the Recurrent Neural Network (RNN) Encoder
Decoder architecture. Thongtanunam et al. [59] proposed AutoTransform which is an NMT-based
code transformation approach using BPE [50] with the Transformer architecture [66].

Implementation & Hyperparameter Settings. In our implementation, we set the target vocab-
ulary size to be 2,000 for BPE. To build the encoder-decoder RNN architectures in the Tufano et
al. [62]’s approach, we utilize the OpenNMT library developed by Klein et al. [35]. We use ten
hyper-parameter settings which are originally used in the experiment of the prior work [62].
Specifically, ten settings including different combinations of RNN cells (LSTM and GRU), number of
layers (1,2,4) and hidden size (256, 512) for the encoder/decoder, and the embedding size (256, 512)
are tested. For the Transformer in AutoTransform and our AutoUpdate , we use the tensor2tensor
library developed by the Google Brain team [65]. To ensure a fair comparison, we use the similar
combinations of hyper-parameter settings to construct our Transformer models, which is also
used in AutoTransform [59]. Thus, we test eight configurations of the Transformer architures with
different combinations of number of layers (1,2,4), hidden size (256, 512), and number of attention
heads (8, 16). We use backpropagation which is widely adopted to fine-tune NMT-based models to
update the model and minimize the loss function. We use Adam optimizer with the learning rate
of 0.2, linear learning rate warmup schedule to update the model and minimize the loss function
over the 50,000 steps to train the model for around 100 epochs. The model is fine-tuned on an
NVIDIA RTX 3090 graphic card. To obtain the best fine-tuned weights, we use the validation set to
monitor the training process by epoch, and the best model is selected based on the optimal loss
value against the validation set (not the testing set).

4 EXPERIMENTAL RESULTS
In this section, we present approaches and results with respect to our five research questions.

(RQ1) How accurate and efficient is our AutoUpdate to recommend code updates for
Android apps?

Motivation. We formulate this RQ to investigate the accuracy of our AutoUpdate(Abs+BPE+
Transformer) in the context of code updates in Android apps, when comparing to the two baseline
approaches for code transformation.

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

9

Approach. To answer this RQ, we evaluate the accuracy of our AutoUpdate and the baseline
approaches based on the percentage of Perfect Predictions (%PP). Similar to prior studies [59, 62],
we leverage a beam width (𝑘) of 1, 5, 10, 15 to generate 𝑘 candidates of code updates. To quantify
the magnitude of the improvement for our AutoUpdate, we compute a percentage improvement
of the perfect predictions rate using a calculation of (#𝑃𝑃%our−#𝑃𝑃 %baseline)×100%
. In addition, we also
compare our AutoUpdate with the baseline approaches based on CodeBLEU.

#𝑃𝑃%baseline

To evaluate the efficiency of our AutoUpdate, we measure the training time of each approach.
Then, we compare the average training time of our AutoUpdate and two baselines based on
the same size of training data with the same training steps. We run the experiments on the same
machine with an AMD Ryzen 9 5950X CPU, 64 GM of RAM and an NVIDIA GeForce RTX 3090
GPU.

Note that since the RNN architecture used by the baseline approach [62] is computationally-
expensive, their approaches were evaluated using small methods (i.e., less than 50 tokens in a
method and less than 5 changed tokens between versions). Thus, similar to prior studies [59, 62],
this RQ will focus on the context of small methods. For each approach, the model is trained on
the training dataset of small methods and tested on the testing dataset of small methods. We will
further evaluate our AutoUpdate based on the whole dataset in RQ5.

Results. Our AutoUpdate achieves a perfect prediction of 25%, which is 59%~107%
higher than the baseline approaches. Table 2 presents the accuracy of our AutoUpdate compar-
ing to the baseline approaches. We find that when beam size is set to 15, our AutoUpdate achieves
the perfect prediction for 1036 changed methods which accounts as 25% of the changed methods.
The Tufano et al. approach achieves the perfect prediction for 651 changed methods (i.e., %PP is
15%). Compared with the Tofano et al., our AutoUpdate achieves a 59% performance improvement.
The AutoTransform approach achieves the perfect prediction for 656 changed methods (i.e., %PP is
11%). Compared with AutoTransform, our AutoUpdate achieves a 133% performance improve-
ment. When the size of beam search is 1,5,10, the perfect predictions of AutoUpdate improve by
73%~107% compared with the Tufano et al. approach. These observations demonstrate that our
AutoUpdate outperforms Tufano et al. and AutoTransform to generate a perfect prediction by a
large margin.

Table 2 also show that our AutoUpdate also achieve CodeBLEU higher than the baseline
approaches. When the beam size is set to 15, our AutoUpdate achieves CodeBLEU of 0.902 while
the baseline approaches achive CodeBLEU of 0.664 (Tufano) and 0.893 (AutoTransform). The results
are also similar when the beam size is 1, 5, 10. These results indicate that on average across all
methods in the testing data, our AutoUpdate can generate code updates more similar to the actual
code updates than the baseline approaches.

Our AutoUpdate achieves a higher efficiency for recommending code updates com-
pared with the Tufano et al. approach and AutoTransform. Table 3 shows the computational
time of our AutoUpdate and the baseline approaches. As shown in Table 3, AutoUpdate takes
about 117 minutes to train a model, while the Tufano et al. takes 223 minutes and AutoTransform
takes 140 minutes. The result shows that our AutoUpdate can save 16% - 50% computational time
when training a model on the small dataset. The higher efficiency of our AutoUpdate has to do
with our Transformer-based architecture, which processes the entire input sequences at once in a
highly parallel fashion. In contrast, RNNs-based models used by Tufano et al. [62] needs to process
the input sequence in order, which is less efficient as showed in Table 3.

, Vol. 1, No. 1, Article . Publication date: September 2022.

10

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Table 3. Computational time of AutoUpdate and the baseline approaches.

Model

AutoUpdate

Tufano et al. AutoTransform

Time cost
Improvement

117 min

233 min
-50%

140 min
-16%

(RQ2) What is the benefit of our tokenization approach (Abs+BPE) for our
AutoUpdate?

Motivation. Tufano et al. [62] suggested that code abstraction could substantially reduce the
size of vocabularies. However, Thongtanunam et al. [59] argue that, when code abstraction is used
together with word-level tokenization, such the approach may generate an excessively large number
of vocabularies. Different from prior work, our AutoUpdate leverages both code abstraction and
BPE in order to preserve existing tokens that will be appeared in the updated version and generate
new tokens that never exist in the prior version. Thus, we formulate this RQ to investigate the
benefits of our tokenization approaches for our AutoUpdate.

Approach. To answer this RQ, we conduct an ablation study to evaluate the following four

variants of our AutoUpdate using different tokenization approaches:

• Abs+BPE+Transformer (our AutoUpdate).
• BPE+Transformer (AutoTransform [59]).
• Abs+Transformer (Abs is used in Tufano et al. [62]).
• Word-level+Transformer.

Similar to RQ1, for each approach, the model is trained on the training dataset of small methods and
tested on the testing dataset of small methods. Finally, we evaluate the accuracy of these variants
using the same evaluation measure (i.e., % Perfect Predictions).

Results. Figure 2 presents the accuracy of different code tokenization approaches of our AutoUp-

date.

Our tokenization approach (Abs + BPE) gains benefits at least 17% by using both code
abstraction and BPE for recommending code updates. Figure 2 shows that our approach (Abs
+ BPE) achieves the highest accuracy regardless of the size of beam search. Specifically, at the beam
size of 15, our approach achieves the perfect prediction of 25%. On the other hand, code abstraction
alone achieves the perfect prediction of 23%, while BPE alone achieves the perfect prediction of
11%. The higher accuracy of using code abstraction only than using BPE only is different from
Thongtanunam et al. [59] who found that BPE is better than code abstraction, highlighting the
importance of this work that findings from existing NMT-based code transformation studies are not
generalized to NMT-based code update recommendations. We suspect that BPE works well for the
context of code review, since there are a large number of newly-introduced code tokens. In contrast,
for Android apps, the proportion of newly-introduced code tokens is smaller, demonstrating that
code abstraction only performs better than BPE only. Nevertheless, our finding confirms that the
combination of using both Abs and BPE used by our AutoUpdate outperforms both Abs alone
and BPE alone.

(RQ3) What types of code updates are most accurately recommended by our
AutoUpdate?

Motivation. Prior studies proposed various approaches to automatically update Android apps,
yet for specific types of code updates. For example, Lamothe et al. [37] focus on updating Android

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

11

Fig. 2. (RQ2) The perfect prediction of the different tokenization approaches.

API migrations. Zhao et al. [74] focus on updating compatibility issues. Indeed, there exist multiple
types of code updates [64] associated with Android apps (e.g., Fixing Bug or Refactoring Method).
Thus, we set out this RQ to qualitatively evaluate AutoUpdate in terms of the types of code updates
that can be recommended by our AutoUpdate.

Approach. To answer this RQ, we analyze from the 277 updated methods that are correctly
recommended by our AutoUpdate when using the beam size of 1 from RQ1. To do so, we manually
classify updated methods into various types of code updates. We use the high-level taxonomy
of the conventional commit types [64] and Tufano et al.’s approach [62] for the types of code
updates. To fully understand the purpose of code updates, we also use the commit messages
associated with the studied updated methods. The categorization process was performed by the
first author and the results of coding were cross-checked by the third author to mitigate any
potential bias in the categorization process. Then, we calculate the inter-rater agreement between
the categorization results from the two coders using a Cohen’s kappa. The kappa agreement is 0.91
for the categorization process of the 277 perfect predictions, demonstrating the high agreement
(i.e., almost perfect) between two raters [9].

Results. Table 4 sumarizes the types of code updates that our AutoUpdate can achieve con-

cerning a perfect prediction with a beam size of 1.2

Our AutoUpdate can recommend nine types of code updates for Android apps, e.g.,
26% perfect predictions for fixing bug and 22% perfect predictions for adding new feature.
As Table 4 shows, we summarize nine types of code updates. Fixing bug and Adding New Feature are
the most common types of code updates that AutoUpdate can recommend (i.e., 26% for Fixing Bug

2Due to the page limit, the complete classification results are available in the GitHub repository.

, Vol. 1, No. 1, Article . Publication date: September 2022.

2%11%23%25%1%10%21%23%1%8%17%18%0%3%6%7%Beam=15Beam=10Beam=5Beam=1Abs+BPEAbsBPEWordAbs+BPEAbsBPEWordAbs+BPEAbsBPEWordAbs+BPEAbsBPEWord010203012

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Table 4. (RQ3) The type of code updates that are correctly recommended by our AutoUpdate approach.

Code Update Type

Fixing Bug
Adding New Feature
Refactoring Method
Adjusting Build
Maintaining Compatibility
Improving Performance
Optimizing Test Case
Altering Documentation
Other

Description

Counts

Percentage

A bug fix
Adding a new feature
Restructuring existing code without changing its external behavior
Changes that affect the build system or external dependencies
Updating code to resolve Android compatibility issues
A code change that improves performance
Adding missing tests or correcting existing tests
Documentation only changes
Other

73
62
54
18
10
8
5
4
43

26%
22%
19%
6%
4%
3%
2%
1%
16%

and 22% for Adding New Feature). Also, our AutoUpdate can recommend maintainability-related
code updates for Android applications (e.g., Refactoring Method and Adjusting Build for Android
applications). Additionally, some of the code updates are related to external code updates (e.g., third-
party libraries and Android OS). For example, Android developers update the source code to adapt
to the release of new version of Android OS (i.e., Maintaining Android OS Compatibility). These
results suggest that our AutoUpdate can recommend various types of code updates, demonstrating
the potential impact of our AutoUpdate in real-world usage scenarios.

(RQ4) What is the impact of the time-wise evaluation on the accuracy of code update
recommendation approaches?

Motivation. Source codes are gradually evolved and updated overtime. Thus, evaluation scenar-
ios for NMT-based code transformation should take into a consideration of the temporal information.
Prior studies [5, 13, 46, 47] raised concerns that random data splitting (which are widely-used
in previous NMT-based code transformation approaches [59, 63]) may not be realistic (i.e., time-
ignore), ignoring the temporal information that must be considered. Yet, little is known about the
impact of the time-wise evaluation scenarios on the accuracy of NMT-based models.

Approach. To answer this RQ, we focus on two types of evaluation scenarios: time-wise and
time-ignore. The time-wise evaluation scenario considers the temporal information of the method
pairs, where the training dataset is first sorted in a chronological order, ensuring that method pairs
that happen earlier will appear only in the training dataset, but will not appear in the testing dataset.
Likewise, method pairs that happen later will appear only in the testing dataset, but will not appear
in the training dataset, ensuring a realistic evaluation setting. Thus, for the time-wise evaluation,
the training/validation dataset consists of the method pairs during 2008 to 2019, while the testing
dataset consists of the method pairs during 2020 to 2022. On the other hand, the time-ignore
evaluation scenario does not consider the temporal information of the method pairs (which is the
case for prior studies [59, 63]). Thus, for the time-ignore evaluation, the training/validation/testing
dataset consists of the method pairs that are randomly splitted without considering the temporal
information. Both types of evaluation scenarios are applied to our AutoUpdate and a baseline
approach (i.e., AutoTransform [59]).

Results. Table 5 presents the accuracy of our AutoUpdate and AutoTransform [59] between

the time-wise and time-ignore evaluation scenarios.

Both NMT-based models produce optimistically higher accuracy based on the time-
ignore evaluation scenario (non-realistic) than based on the time-wise evaluation sce-
nario (realistic). Table 5 shows that, at the Beam size of 15 for our AutoUpdate, the time-ignore
evaluation scenario (33% of PP) optimistically inflates the perfect predictions by 32% when compared
to the the time-wise evaluation scenario (25% of PP). Similarly to AutoUpdate, the time-ignore

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

13

Table 5. (RQ4) The accuracy of our AutoUpdate and AutoTransform [59] between the time-wise and time-
ignore evaluation scenarios.

Approach

AutoUpdate

AutoTransform

Metrics

Time-Wise
Time-Ignore

Time-Wise
Time-Ignore

Beam size = 15 Beam size = 10 Beam size = 5 Beam size = 1
PP

PP%

PP%

PP%

PP%

PP

PP

PP

1036
1385

656
2682

25%
33%

11%
43%

956
1303

596
2614

23%
31%

10%
42%

754
1119

479
2459

18%
27%

8%
39%

277
538

168
1856

7%
13%

3%
30%

Table 6. (RQ5.1) The accuracy of our AutoUpdate when considering all method pairs.

Dataset

PP/Total

PP%

AutoUpdate
4,188/43,714
AutoTransform 2,690/43,714

14%
6%

evaluation scenario on AutoTransform (43% of PP) optimistically inflates the perfect predictions by
290% when compared to the the time-wise evaluation scenario (11% of PP). This finding suggests
that researchers should consider time-wise evaluation scenarios in the future.

(RQ5) How do the method size and update size impact the accuracy of our
AutoUpdate?

Motivation. Following Tufano et al. [62], our experiment in RQ1-RQ4 is based on the small size
of method pairs (i.e., less than 50 tokens in a method and less than 5 changed tokens between
versions). While our AutoUpdate is able to correctly recommend code updates for Android apps
(25%PP at Beam=15), little is known about the accuracy of our AutoUpdate on the other size
of method pairs and its update size (#changed tokens between versions of a method). Thus, we
formulate this RQ to investigate the impact of the method size and the update size on the accuracy
of our AutoUpdate.

Approach. To answer this RQ, we focus on the #All dataset consisting of 209,346 method pairs.
The #All dataset is sorted in a chronological order. Then, the #All dataset is splitted into training
(136,932 method pairs during 2008-2019), validation (34,234 method pairs during 2008-2019), and
testing (43,714 method pairs during 2020-2022). Then, both AutoUpdate and AutoTransform
are trained, validated, and tested, accordingly. We note that we do not incorporate the Tufano et
al.’s approach [62] in this RQ, since the RNN-based approach is known to be computationally
expensive, limiting its model training ability on our large dataset (i.e., 209,346 method pairs). For
our AutoUpdate, we further analyze the perfect predictions according to the method size (#tokens
in the initial version) and the update size (#changed tokens between two versions) at the beam size
of 15.

Results. Table 6 presents the accuracy of AutoUpdate and AutoTransform when considering

all method pairs (i.e., #All datasets).

When considering all method pairs, our AutoUpdate still achieves 133% higher per-
fect predictions than the baseline approach. Table 6 shows that, when training and testing on
any size of methods, our AutoUpdate still achieves a perfect prediction of 14%. On the other hand,
the baseline approach only achieves a perfect prediction of 6%, demonstrating an improvement

, Vol. 1, No. 1, Article . Publication date: September 2022.

14

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Table 7. (RQ5.2) The accuracy of our AutoUpdate according to the method size and the update size.

Method size

0-50

50-100

100-150

150-200

200+

#Updated Tokens

PP/Total

PP% PP/Total

PP% PP/Total

PP% PP/Total

PP% PP/Total

PP%

0-5
5-10
10-15
15-20
20-25
25+

2031/4607
242/2169
56/917
15/489
13/313
8/791

44%
11%
6%
3%
4%
1%

928/3177
144/1849
41/888
14/590
8/440
6/1373

29%
8%
5%
2%
2%
0%

283/1403
37/934
15/437
7/301
7/195
6/1016

20%
4%
3%
2%
4%
1%

119/763
22/541
6/255
5/192
3/130
3/651

16%
4%
2%
3%
2%
0%

122/1270
14/886
9/518
6/404
1/305
5/2017

10%
2%
2%
1%
0%
0%

of 133% for AutoUpdate over the baseline approach. This indicates that, despite the method size
(#tokens) and update size (#updated tokens) are much larger, our AutoUpdate is still perform
better than the baseline approach.

Nevertheless, our AutoUpdate is less accurate when they are applied to a larger size
of methods with a larger number of updated tokens. Table 7 shows that the accuracy of our
AutoUpdate depends on the size of the method size and its update size. Importantly, Table 7 shows
that, at the small size of methods (i.e., Method size less than 50) and updates (i.e., #Updated Tokens
less than 5), our AutoUpdate achieves the perfect prediction of 44%, which is quite accurate. On
the other hand, the perfect prediction is substantially decreased to less than 29% when the method
size is increased (i.e., Method size larger than 50), and to less than 11% when the update size is
increased (i.e., #Updated Tokens more than 5). Thus, this finding sheds some light as an opportunity
for future researchers to address this challenge.

Interestingly, our AutoUpdate is more accurate on the #Small dataset when it is trained
on the #All dataset which includes a larger size of methods and a larger number of up-
dated tokens. Table 7 shows that, when the model is trained on the #All dataset and tested on
the #Small dataset (i.e., method size less than 50 and #Updated Tokens less than 5), our AutoUp-
date achieves the perfect prediction of 44%, which is higher than 27% (see Table 2) when the model
is trained on the small dataset. Nevertheless, AutoUpdatebecomes less accurate (0%-11%) when the
method size is larger and the number of updated tokens is higher, highlighting a potential research
opportunity for future researchers to improve the model performance for the more complex code
updates.

5 RELATED WORK
In this section, we discuss the work related to AI for mobile software engineering, NMT-based code
recommendation, Android analysis, and AI/ML-based experimental bias in software engineering.

5.1 AI for Mobile Software Engineering
Deep Learning (DL) is a dominant research domain in the last decade and has brought several
breakthroughs in images, videos, speeches [39], including natural texts and source code in soft-
ware engineering [69]. Unlike conventional machine learning techniques that rely heavily on the
manually-handcrafted feature representations, deep learning enable us to automatically learn the
feature representations from raw data [23, 39]. Driven by the great success of deep learning tech-
niques, researchers have shown great enthusiasm for applying deep learning techniques to diverse
software engineering tasks (e.g., requirements, software design and modeling, software testing and
debugging, software maintenance, software analysis) [69]. For example, automated code search [24],

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

15

automated code generation [34], automated defect prediction & localization [11, 26, 47, 48], auto-
mated vulnerability prediction [12, 16], automated test case generation [67], and automated code
review [59, 60, 62]. Specifically to the mobile software engineering context, researchers also pro-
posed various automated approaches for UI generation [7, 8], mobile malware classification [71, 73]
and mobile privacy policy analysis [25].

Different from prior studies, this paper is the first to focus on automated code update recommen-
dation for Android applications where we formulate the problem as a Neural Machine Translation
task. Our results show that our AutoUpdate is 59%~107% more accurate than baseline approaches
based on a realistic time-wise evaluation scenario, demonstrating the significant advancement of
AI for mobile software engineering.

5.2 NMT-based Code Recommendation
Neural Machine Translation (NMT) [68] is an end-to-end learning approach for automated trans-
lation, which has been widely developed to support many software engineering tasks in recent
years [34, 38, 62]. Code recommendation is important for ensuring the maintainability of programs,
but is time-consuming and human-intensive [59]. Thus, recent work leveraged NMT techniques to
support automated code recommendation. Tufano et al. [62] proposed an RNN Encoder-Decoder
architecture, commonly adopted in the NMT domain, to learn how to automatically apply Java
code changes implemented by developers during pull requests. Their experimental results have
shown that their approach can automatically replicate the changes implemented by developers
during pull requests in up to 36% of the cases. Thongtanunam et al. [59] proposed AutoTransform,
which leveraged a Transformer Encoder-Decoder-based architecture and BPE subword tokenization
approach to automatically recommend a given method from the before version to the after version
in the Java scenario. Except for recommending code changes, Liu et al. [42] proposed a global
Transformer-based neural model for method name recommendation, and experiments results on
java methods showed that the proposed approach achieved dominant performance. Izadi et al. [27]
proposed a Transformer-based approach for the code auto-completion task, targeting Python source
code.

Unlike prior studies, this paper is the first work to focus on an NMT-based automated code
recommendation for Android applications. Our experimental results show that NMT-based approach
can achieve a perfect prediction of 25% for Android code recommendation based on the realistic
time-wise evaluation scenario, which outperforms two baseline approaches.

5.3 Android Analysis
Android is the most widely used mobile operating system, and its popularity has encouraged
software developers to publish innovative applications [15]. The growing popularity of Android
devices, as well as the associated monetary benefits, has attracted malware developers [45]. To deal
with the threats, the research community has proposed a variety of program analysis approaches
to detect syntactical errors and semantic bugs, discover data leaks, identify vulnerabilities, etc [40].
TaintDroid [14] is a dynamic taint tracking and analysis approach for tracking the leaking of
privacy-sensitive information in the third-party developer applications. Lamothe et al. [37] have
proposed a data-driven approach to learning from code examples to assist Android API migrations.
Their A3 tool can successfully learn API migration patterns and provide API migration assistance
in 71 out of 80 cases in open-source Android apps. Zhao et al. [74] have proposed RepairDroid
to automatically repair compatibility issues in published Android apps through template-based
patches. Zhan et al. [72] have proposed ATVHUNTER to perform a reliable version detection of
third-party libraries for vulnerability identification in Android applications.

, Vol. 1, No. 1, Article . Publication date: September 2022.

16

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

Different from prior studies, this paper is the first to focus on the automated general code update
recommendation for Android apps (e.g., 26% for Fixing Bug, 19% for Refactoring Method, etc.) rather
than updating a specific aspect (e.g., Android API migration [37], Android compatibility issues [74],
etc.) in prior work.

5.4 AI/ML-based Experimental Bias in Software Engineering
According to the survey results presented by Yang et al. [69], machine learning or deep learning
technologies have been increasingly used in software engineering to improve developers’ pro-
ductivity and software quality. However, cares must be taken when designing the experiment of
AI/ML-based approaches in SE (e.g., defect prediction [54] and malware classification [46]). Prior
studies have raised various potential issues that may impact the accuracy and interpretation of
AI/ML models in SE. For example, the quality of datasets [20, 56], data labelling techniques [70],
feature selection techniques [21, 31], collinearity analysis [29–31], class rebalancing techniques [55],
model construction [20], parameter optimisation [2, 3, 17, 55, 57], model evaluation [32, 47, 58],
and model interpretation [28, 29].

Extending the concerns related to evaluation techniques [32, 47, 58], this paper is the first to
demonstrate the impact of the time-wise evaluation scenario for various NMT-based code generation
tasks (including our own code update recommendation tool and the automated code transformation
tool in prior work [59, 62]). Thus, our finding highlights the importance of the time-wise evaluation
scenario, which should be considered in future work.

6 THREATS TO VALIDITY
Below, we discuss threats that may impact the results of our study.

Threats to the external validity concern the generalizability of our work. Our study is con-
ducted based on the Android apps that are released on the official Google Play store and are publicly
accessible on GitHub. We mainly collect Java method blocks from these Android apps. There is
a risk that these apps might not be representative of open-source Android apps implemented in
the Kotlin programming language, which Google confirmed as an officially supported language
for Android development in 2017 [1]. However, Java is a popular programming language, which is
largely used in Android development [22, 43]. In the future, we will extend our approach to include
Android apps written in Kotlin.

Threats to the internal validity relate to the impact of the hyperparameter settings on the
performance of our NMT models. For the pre-training phase, we used the default parameters
selected in the two baseline paper, as we expect little margin of improvement for such a task-
agnostic phase. During fine-tuning, we also didn’t alter the model’s architecture (e.g., number
of layers). We are aware that a comprehensive calibration would certainly yield better results.
However, optimizing the settings of the parameter for deep learning approaches is computationally
expensive. Furthermore, the goal of our research does not seek to determine the optimal setting but
rather to fairly investigate the feasibility of learning and recommending code updates for Android
apps.

Threats to the construct validity relate to the data collection process for our dataset, as it
may have an impact on the accuracy of our AutoUpdate. We perform data cleaning and filtering
steps to mitigate the noise of our datasets (e.g., removing duplicated methods pairs). However,
unknown noise may still exist in our curated dataset. Thus, future work should further discover
unknown noise and explore how do they impact code update recommendation approaches.

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

17

7 CONCLUSION
In this work, we proposed a deep learning-based approach, namely AutoUpdate, to learn from
existing Android apps to recommend code updates for Android apps. We realized our approach
through a Transformer-based neural machine translation approach, for which code abstraction
and BPE subword tokenization are leveraged to model code snippets in Android apps. We then
evaluated our approach through a newly constructed dataset containing 209k updated methods
from the source code of 3, 195 open-source Android applications. Our extensive experimental study
demonstrated that AutoUpdate can achieve up to 25% perfect prediction rate, outperforming two
state-of-the-art baselines. Our results also showed that AutoUpdate can correctly recommend
different types of code updates for Android apps, including Fixing Bug and Updating New Feature.
Our ablation study further demonstrated that our tokenization approach (Abs + BPE) substantially
contributed to the performance improvement of AutoUpdate. Finally, our findings demonstrated
that unrealistic random data splitting produce overly-optimistic higher accuracy than it should be,
suggesting that a realistic time-wise evaluation scenario should be used in the future.

ACKNOWLEDGMENT
Chakkrit Tantithamthavorn was supported by the Australian Research Council’s Discovery Early
Career Researcher Award (DECRA) funding scheme (DE200100941). Li Li was partly supported
by the Australian Research Council (ARC) under a Discovery Early Career Researcher Award
(DECRA) project DE200100016, and a Discovery project DP20010002. Patanamon Thongtanunam
was supported by the Australian Research Council’s Discovery Early Career Researcher Award
(DECRA) funding scheme (DE210101091).

REFERENCES
[1] Kotin, 2022. [Online]. Available: https://kotlinlang.org/
[2] A. Agrawal and T. Menzies, “Is" better data" better than" better data miners"?” in 2018 IEEE/ACM 40th International

Conference on Software Engineering (ICSE).

IEEE, 2018, pp. 1050–1061.

[3] A. Agrawal, T. Menzies, L. L. Minku, M. Wagner, and Z. Yu, “Better software analytics via “duo”: Data mining algorithms

using/used-by optimizers,” Empirical Software Engineering, vol. 25, no. 3, pp. 2099–2136, 2020.

[4] Android 13. [Online]. Available: https://developer.android.com/about/versions/13
[5] G. G. Cabral, L. L. Minku, E. Shihab, and S. Mujahid, “Class imbalance evolution and verification latency in just-in-time
IEEE,

software defect prediction,” in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
2019, pp. 666–676.

[6] K. Cao, C. Chen, S. Baltes, C. Treude, and X. Chen, “Automated query reformulation for efficient search based on query
IEEE, 2021,

logs from stack overflow,” in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).
pp. 1273–1285.

[7] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design image to gui skeleton: a neural machine translator to
bootstrap mobile gui implementation,” in Proceedings of the 40th International Conference on Software Engineering, 2018,
pp. 665–676.

[8] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhut, G. Li, and J. Wang, “Unblind your apps: Predicting natural-language labels
for mobile gui components by deep learning,” in 2020 IEEE/ACM 42nd International Conference on Software Engineering
(ICSE).

IEEE, 2020, pp. 322–334.

[9] J. Cohen, “Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit.” Psychological

bulletin, vol. 70, no. 4, p. 213, 1968.

[10] A. Cranz. There are over 3 billion active android devices. [Online]. Available: https://www.theverge.com/2021/5/18/

22440813/android-devices-active-number-smartphones-google-2021

[11] H. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose, T. Kim, and C.-J. Kim, “Lessons learned from using a deep
tree-based model for software defect prediction in practice,” in 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR).

IEEE, 2019, pp. 46–57.

[12] H. K. Dam, T. Tran, T. Pham, S. W. Ng, J. Grundy, and A. Ghose, “Automatic feature learning for predicting vulnerable

software components,” IEEE Transactions on Software Engineering, vol. 47, no. 1, pp. 67–85, 2018.

, Vol. 1, No. 1, Article . Publication date: September 2022.

18

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

[13] R. De Silva, M. Nabeel, C. Elvitigala, I. Khalil, T. Yu, and C. Keppitiyagama, “Compromised or {Attacker-Owned}: A
large scale classification and study of hosting domains of malicious {URLs},” in 30th USENIX Security Symposium
(USENIX Security 21), 2021, pp. 3721–3738.

[14] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth, “Taintdroid: an
information-flow tracking system for realtime privacy monitoring on smartphones,” ACM Transactions on Computer
Systems (TOCS), vol. 32, no. 2, pp. 1–29, 2014.

[15] P. Faruki, A. Bharmal, V. Laxmi, V. Ganmoor, M. S. Gaur, M. Conti, and M. Rajarajan, “Android security: a survey of
issues, malware penetration, and defenses,” IEEE communications surveys & tutorials, vol. 17, no. 2, pp. 998–1022, 2014.
[16] M. Fu and C. Tantithamthavorn, “LineVul: A Transformer-based Line-Level Vulnerability Prediction,” in 2022 IEEE/ACM

19th International Conference on Mining Software Repositories (MSR).

IEEE, 2022.

[17] W. Fu, T. Menzies, and X. Shen, “Tuning for software analytics: Is it really necessary?” Information and Software

Technology, vol. 76, pp. 135–146, 2016.

[18] J. Gao, P. Kong, L. Li, T. F. Bissyandé, and J. Klein, “Negative results on mining crypto-api usage rules in android apps,”

in The 16th International Conference on Mining Software Repositories (MSR 2019), 2019.

[19] J. Gao, L. Li, P. Kong, T. F. Bissyandé, and J. Klein, “Understanding the evolution of android app vulnerabilities,” IEEE

Transactions on Reliability (TRel), 2019.

[20] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of classification techniques on the performance of
IEEE,
defect prediction models,” in 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, vol. 1.
2015, pp. 789–800.

[21] ——, “A large-scale study of the impact of feature selection techniques on defect classification models,” in 2017 IEEE/ACM

14th International Conference on Mining Software Repositories (MSR).

IEEE, 2017, pp. 146–157.

[22] B. Góis Mateus and M. Martinez, “An empirical study on quality of android applications written in kotlin language,”

Empirical Software Engineering, vol. 24, no. 6, pp. 3356–3393, 2019.

[23] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.
[24] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in 2018 IEEE/ACM 40th International Conference on Software Engineering

(ICSE).

IEEE, 2018, pp. 933–944.

[25] H. Harkous, K. Fawaz, R. Lebret, F. Schaub, K. G. Shin, and K. Aberer, “Polisis: Automated analysis and presentation of

privacy policies using deep learning,” in 27th USENIX Security Symposium (USENIX Security 18), 2018, pp. 531–548.

[26] X. Huo, F. Thung, M. Li, D. Lo, and S.-T. Shi, “Deep transfer bug localization,” IEEE Transactions on software engineering,

vol. 47, no. 7, pp. 1368–1380, 2019.

[27] M. Izadi, R. Gismondi, and G. Gousios, “Codefill: Multi-token code completion by jointly learning from structure and

naming sequences,” 2022.

[28] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and J. Grundy, “An empirical study of model-agnostic techniques for

defect prediction models,” IEEE Transactions on Software Engineering, 2020.

[29] J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, “The impact of correlated metrics on defect models,” arXiv

preprint arXiv:1801.10271, 2018.

[30] J. Jiarpakdee, C. Tantithamthavorn, A. Ihara, and K. Matsumoto, “A study of redundant metrics in defect prediction
IEEE, 2016,

datasets,” in 2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW).
pp. 51–52.

[31] J. Jiarpakdee, C. Tantithamthavorn, and C. Treude, “Autospearman: Automatically mitigating correlated software
metrics for interpreting defect models,” in 2018 IEEE International Conference on Software Maintenance and Evolution
(ICSME).

IEEE Computer Society, 2018, pp. 92–103.

[32] M. Jimenez, R. Rwemalika, M. Papadakis, F. Sarro, Y. Le Traon, and M. Harman, “The importance of accounting for
real-world labelling when predicting software vulnerabilities,” in Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2019, pp.
695–705.

[33] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big code!= big vocabulary: Open-vocabulary models
IEEE, 2020, pp.

for source code,” in 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE).
1073–1085.

[34] S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by feeding trees to transformers,” in 2021 IEEE/ACM 43rd

International Conference on Software Engineering (ICSE).

IEEE, 2021, pp. 150–162.

[35] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush, “Opennmt: Open-source toolkit for neural machine translation,”

arXiv preprint arXiv:1701.02810, 2017.

[36] Kontalk

official

android

client.

[Online].

Available:

https://

github.com/kontalk/androidclient/commit/303048795ea5cdac9c6ca8b1cc1a12e4dc0e5f68#diff-
114916ae8997b205a54e4ae33931cb8df2024a5f55ecb982f08030a0a6a77d66

, Vol. 1, No. 1, Article . Publication date: September 2022.

AutoUpdate: Automatically Recommend Code Updates for Android Apps

19

[37] M. Lamothe, W. Shang, and T.-H. P. Chen, “A3: Assisting android api migrations using code examples,” IEEE Transactions

on Software Engineering, 2020.

[38] A. LeClair, S. Jiang, and C. McMillan, “A neural model for generating natural language summaries of program
IEEE, 2019, pp. 795–806.

subroutines,” in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
[39] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553, pp. 436–444, 2015.
[40] L. Li, T. F. Bissyandé, M. Papadakis, S. Rasthofer, A. Bartel, D. Octeau, J. Klein, and L. Traon, “Static analysis of android

apps: A systematic literature review,” Information and Software Technology, vol. 88, pp. 67–95, 2017.

[41] L. Li, T. F. Bissyandé, H. Wang, and J. Klein, “Cid: Automating the detection of api-related compatibility issues in
android apps,” in The ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018), 2018.

[42] F. Liu, G. Li, Z. Fu, S. Lu, Y. Hao, and Z. Jin, “Learning to recommend method names with global context,” 2022.
[43] P. Liu, L. Li, Y. Zhao, X. Sun, and J. Grundy, “Androzooopen: Collecting large-scale open source android apps for the
research community,” in Proceedings of the 17th International Conference on Mining Software Repositories, 2020, pp.
548–552.

[44] P. Liu, Y. Zhao, H. Cai, M. Fazzini, J. Grundy, and L. Li, “Automatically detecting api-induced compatibility issues in
android apps: A comparative analysis (replicability study),” in The ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA 2022), 2022.

[45] Y. Liu, C. Tantithamthavorn, L. Li, and Y. Liu, “Deep learning for android malware defenses: a systematic literature

review,” arXiv preprint arXiv:2103.05292, 2021.

[46] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro, “{TESSERACT}: Eliminating experimental bias
in malware classification across space and time,” in 28th USENIX Security Symposium (USENIX Security 19), 2019, pp.
729–746.

[47] C. Pornprasit and C. Tantithamthavorn, “JITLine: A Simpler, Better, Faster, Finer-grained Just-In-Time Defect Prediction,”

in MSR, 2021, pp. 369–379.

[48] ——, “Deeplinedp: Towards a deep learning approach for line-level defect prediction,” IEEE Transactions on Software

Engineering, 2022.

[49] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan, M. Zhou, A. Blanco, and S. Ma, “Codebleu: a method for

automatic evaluation of code synthesis,” arXiv preprint arXiv:2009.10297, 2020.

[50] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proceedings

of the Association for Computational Linguistics (ACL), 2015.

[51] ——, “Neural machine translation of rare words with subword units,” in Proceedings of the 54th Annual Meeting of the

Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1715–1725.

[52] Android getresources().getdrawable() deprecated api 22. [Online]. Available: https://stackoverflow.com/questions/

29041027/android-getresources-getdrawable-deprecated-api-22

[53] Number of available applications in the google play store from december 2009 to march 2022. [Online]. Available:

https://www.statista.com/statistics/266210/number-of-available-applications-in-the-google-play-store/

[54] C. Tantithamthavorn and A. E. Hassan, “An experience report on defect modelling in practice: Pitfalls and challenges,”
in Proceedings of the 40th International conference on software engineering: Software engineering in practice, 2018, pp.
286–295.

[55] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The impact of class rebalancing techniques on the performance
and interpretation of defect prediction models,” IEEE Transactions on Software Engineering, vol. 46, no. 11, pp. 1200–1219,
2018.

[56] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and K. Matsumoto, “The impact of mislabelling on the
performance and interpretation of defect prediction models,” in 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering, vol. 1.

IEEE, 2015, pp. 812–823.

[57] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto, “Automated parameter optimization of classification
techniques for defect prediction models,” in Proceedings of the 38th international conference on software engineering,
2016, pp. 321–332.

[58] ——, “An empirical comparison of model validation techniques for defect prediction models,” IEEE Transactions on

Software Engineering, vol. 43, no. 1, pp. 1–18, 2016.

[59] P. Thongtanunam, C. Pornprasit, and C. Tantithamthavorn, “Autotransform: Automated code transformation to support

modern code review process,” 2022.

[60] R. Tufan, L. Pascarella, M. Tufanoy, D. Poshyvanykz, and G. Bavota, “Towards automating code review activities,” in

2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).

IEEE, 2021, pp. 163–174.

[61] M. Tufano. Src2abs: A library for abstracting code with reusable ids. [Online]. Available: https://github.com/

micheletufano/src2abs

[62] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk, “On learning meaningful code changes via neural
IEEE, 2019, pp.

machine translation,” in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).

, Vol. 1, No. 1, Article . Publication date: September 2022.

20

Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li

25–36.

[63] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and D. Poshyvanyk, “An empirical investigation into learning
bug-fixing patches in the wild via neural machine translation,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 832–837.

[64] P. Vanduynslager. conventional-commit-types. [Online]. Available: https://github.com/pvdlg/conventional-commit-

types

[65] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, Ł. Kaiser, N. Kalchbrenner, N. Parmar

et al., “Tensor2tensor for neural machine translation,” arXiv preprint arXiv:1803.07416, 2018.

[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all

you need,” Advances in neural information processing systems, vol. 30, 2017.

[67] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk, “On learning meaningful assert statements for unit
test cases,” in Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020, pp. 1398–1409.
[68] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al.,
“Google’s neural machine translation system: Bridging the gap between human and machine translation,” arXiv preprint
arXiv:1609.08144, 2016.

[69] Y. Yang, X. Xia, D. Lo, and J. Grundy, “A survey on deep learning for software engineering,” ACM Computing Surveys

(CSUR), 2020.

[70] S. Yatish, J. Jiarpakdee, P. Thongtanunam, and C. Tantithamthavorn, “Mining software defects: Should we consider
IEEE, 2019, pp.

affected releases?” in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
654–665.

[71] Z. Yuan, Y. Lu, and Y. Xue, “Droiddetector: android malware characterization and detection using deep learning,”

Tsinghua Science and Technology, vol. 21, no. 1, pp. 114–123, 2016.

[72] X. Zhan, L. Fan, S. Chen, F. Wu, T. Liu, X. Luo, and Y. Liu, “Atvhunter: Reliable version detection of third-party libraries
for vulnerability identification in android applications,” in 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE).

IEEE, 2021, pp. 1695–1707.

[73] X. Zhang, Y. Zhang, M. Zhong, D. Ding, Y. Cao, Y. Zhang, M. Zhang, and M. Yang, “Enhancing state-of-the-art classifiers
with api semantics to detect evolved android malware,” in Proceedings of the 2020 ACM SIGSAC conference on computer
and communications security, 2020, pp. 757–770.

[74] Y. Zhao, L. Li, K. Liu, and J. Grundy, “Towards automatically repairing compatibility issues in published android apps,”

in The 44th International Conference on Software Engineering (ICSE 2022), 2022.

, Vol. 1, No. 1, Article . Publication date: September 2022.

