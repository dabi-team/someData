2
2
0
2

y
a
M
0
1

]
E
S
.
s
c
[

1
v
0
3
7
4
0
.
6
0
2
2
:
v
i
X
r
a

A Neural Network Architecture for Program Understanding Inspired by
Human Behaviors

Renyu Zhu1 Lei Yuan1 Xiang Li1∗ Ming Gao1 Wenyuan Cai2
1School of Data Science and Engineering, East China Normal University, Shanghai, China
2Shanghai Hypers Data Technology Inc., Shanghai, China

{52175100003, 51205903063}@stu.ecnu.edu.cn

{xiangli, mgao}@dase.ecnu.edu.cn

wenyuan.cai@hypers.com

Abstract

Program understanding is a fundamental task
in program language processing. Despite the
success, existing works fail to take human
behaviors as reference in understanding pro-
grams.
In this paper, we consider human
behaviors and propose the PGNN-EK model
that consists of two main components. On
the one hand,
inspired by the “divide-and-
conquer” reading behaviors of humans, we
present a partitioning-based graph neural net-
work model PGNN on the upgraded AST of
codes. On the other hand, to characterize hu-
man behaviors of resorting to other resources
to help code comprehension, we transform
raw codes with external knowledge and ap-
ply pre-training techniques for information ex-
traction. Finally, we combine the two em-
beddings generated from the two components
to output code embeddings. We conduct ex-
tensive experiments to show the superior per-
formance of PGNN-EK on the code summa-
rization and code clone detection tasks.
In
particular, to show the generalization ability
of our model, we release a new dataset that
is more challenging for code clone detection
and could advance the development of the
community. Our codes and data are pub-
licly available at https://github.com/
RecklessRonan/PGNN-EK.

1

Introduction

The past decades have witnessed the prosperity
of programming platforms, such as Github and
Stack Overﬂow. These platforms generate mas-
sive open-source code1 data that is named as “Big
Code” in (Allamanis et al., 2018a). To automate
the software development and maintenance, based
on the “Software Naturalness” hypothesis (Hindle
et al., 2016), natural language processing (NLP)
techniques have been applied in program under-
standing. After that, a series of downstream pro-

∗ Corresponding Author
1We interchangeably use code and program in this paper.

gramming language processing (PLP) tasks can be
performed, including code summarization (Zhang
et al., 2020; Ahmad et al., 2020; Liu et al., 2021)
and code clone detection (Zhang et al., 2019; Wang
et al., 2020).

Existing works for understanding programs
mainly utilize three types of information: code con-
text, code structure and external knowledge. Specif-
ically, code context refers to the token sequence
in the code. For code structure, each code can be
parsed into various types of intermediate represen-
tations, such as AST (Abstract Syntax Tree), CFG
(Control Flow Graph) and PDG (Program Depen-
dence Graph). These representations capture the
structural information of codes. Further, there also
exists external knowledge associated with codes,
such as API documentation and other exemplary
codes. Despite the success, all these models ignore
considering human behaviors in reading programs.
Recently, (Bengio et al., 2021) suggest the potential
futures of deep learning by comparing current AI
methods with human learning abilities. This further
prompts us to revisit program understanding: Can
we develop a model that understands programs like
humans?

In the domain of programming education, how
people understand codes is a topic that has been
studied. For example, based on knowledge base in-
cluding syntactical knowledge (e.g., programming
basics) and semantic knowledge (e.g., API docu-
mentation), (Schulte et al., 2010) offer a bottom-up
reading technique, which assumes that people be-
gin with individual code lines and chunks, and then
combine them into higher-level abstractions. Fur-
ther, (Park et al., 2016) state that when people read
codes, reasoning about the hierarchical relationship
of blocks, statements, expressions and variables is
necessary. Based on these studies, we conclude
three key points for human understanding codes.
First, the transition of deﬁned variables has to be
traced. Second, humans usually adopt a “divide-

 
 
 
 
 
 
and-conquer” strategy, which divides codes based
on statements and then understands codes from a
local-to-global view. Third, humans resort to ex-
ternal knowledge to comprehend codes, such as
API documentation and code examples written by
experts.

In this paper,

inspired by human behaviors
for code comprehension, we propose a novel
Partitioning-based Graph Neural Network with
External Knowledge (PGNN-EK). To capture code
context and structure, PGNN-EK upgrades the tra-
ditional AST and deﬁnes a novel subtoken-based
AST called S-AST. In S-AST, we add edges be-
tween variables to trace the variable transitions,
edges between adjacent tree leaves from left to
right to enrich the context and structure informa-
tion, and edges between sub-nodes corresponding
to subtokens tokenized from user-deﬁned identi-
ﬁers to handle the Out of Vocabulary (OOV) prob-
lem (Karampatsis et al., 2020). Details will be
illustrated later. After that, we ﬁrst apply graph
neural network (GNN) models on the S-AST to
derive a code embedding. To further implement
the “divide-and-conquer” reading strategy, we par-
tition the S-AST into multiple subgraphs, which
follow the sequence of statements in the original
code. For each subgraph, we use GNN models
to generate the subgraph embedding. Then, these
subgraph embeddings are fused to generate another
code embedding. For these two code embeddings,
since they are both derived from S-AST, we further
aggregate them. On the other hand, to character-
ize the dependence on external knowledge for code
comprehension, we traverse the AST of the original
code to derive a sequence of tokens for syntactic
knowledge and then add the API descriptions to the
end for semantic knowledge. We then apply Code-
BERT (Feng et al., 2020) on the token sequence
to capture external knowledge. Finally, PGNN-EK
generates the output code embedding by combining
the embedding derived from S-AST and the one
from external knowledge.

To evaluate the model performance, we conduct
experiments on the code summarization task and
code clone detection task, respectively. Before
we apply PGNN-EK on the code clone detection
benchmarks in CodeXGLUE (Shi et al., 2021) ex-
tracted from the BigCloneBench 2014 dataset (Sva-
jlenko et al., 2014), we notice from the leader-
board2 that the results are incredibly high, where

the minimum F1 score is 0.949. Then we dive into
the characteristics of the dataset and ﬁnd that the
functionalities of codes in the test set have all ap-
peared in the training set. Therefore, the dataset
is very simple. To further test the model’s general-
ization ability, we construct a new dataset, where
the test set contains codes whose functionality has
never appeared in the training set. This new dataset
provides an insightful reference for further research
in the community.

Our main contributions are summarized as fol-

lows:

• We construct a new code structure represen-
tation S-AST that can be used to handle the
OOV problem in PLP.

• We follow human behaviors in understanding
codes and propose a novel model PGNN-EK
that leverages code context, structure and ex-
ternal knowledge. Speciﬁcally, we put for-
ward a novel partitioning-based graph neu-
ral network model that can effectively use
code context and structure. We also present a
code transformation method to utilize external
knowledge in boosting comprehension.

• We conduct extensive experiments on code
summarization and code clone detection tasks
to demonstrate the effectiveness of our model.
In particular, we identify the limitation of a
benchmark dataset for code clone detection
and release a new dataset that is more chal-
lenging.

2 Related Work

2.1 Program Understanding

Program understanding is a topic that has received
wide attention. Early works use either code con-
text or structure information. For example, taking
codes as raw texts, some works use language mod-
els (Raychev et al., 2014; Allamanis et al., 2015),
RNN-series (Zaremba and Sutskever, 2014; Dam
et al., 2016) and attention (Iyer et al., 2016) to
represent codes. However, different from natural
language, programs are more structural, which can
be parsed into intermediate graphs, such as AST.
Many works for code analysis are then proposed
based on AST, such as AST-based LSTM (Wei
and Li, 2017), AST-based CNN (Yu et al., 2019),

2https://microsoft.github.io/

CodeXGLUE/

Figure 1: An example of S-AST. To simplify the graph, we create a code snippet (top left), whose variables are
deﬁned with only one character, such as “a” and “b”. In real tasks, the codes are longer and user-deﬁned identiﬁers
are more semantically complex. This could add more subtoken nodes and edges. The ﬁgure is better viewed in
color.

ASTNN (Zhang et al., 2019), code2vec (Alon et al.,
2019b), and code2seq (Alon et al., 2019a). Re-
cently, GNN models have also been applied in code
understanding. Since the original AST is actually
a tree that is sparse, these works (Allamanis et al.,
2018b; Wang et al., 2020; Wang and Li, 2021) ﬁrst
add edges to AST to make it more connected and
then apply GNN models. Further, there are also
works (Yu et al., 2020; Cummins et al., 2021; Liu
et al., 2021) that utilize other intermediate graphs
such as CFG, PDG and CPG (Yamaguchi et al.,
2014). Recently, approaches that use both code
context and structure are proposed. For example,
Hellendoorn et al. (2020) and Zügner et al. (2021)
incorporate the structure information derived from
AST, such as edge weights and node distances,
into the context attention computation in Trans-
former (Vaswani et al., 2017).

Despite the success, all these methods only con-
sider the code context and structure information.
There are also approaches that utilize the exter-
nal knowledge associated with codes. For exam-
ple, some methods apply pre-training techniques
in NLP to boost comprehension, such as Code-
BERT (Feng et al., 2020), GPT-C (Svyatkovskiy
et al., 2020) and PLBART (Ahmad et al., 2021).
There are also works that incorporate code charac-
teristics into pre-training models, such as Graph-
CodeBERT (Peng et al., 2021), OSCAR (Peng
et al., 2021) and InferCode (Bui et al., 2021). Fur-
ther, API is another external source for program
understanding, which has been introduced in many

works (Hu et al., 2018; Xu et al., 2020). How-
ever, all these methods ignore considering human
behaviors in program understanding.

2.2 Code Summarization and Code Clone

Detection

In this paper, we focus on two program understand-
ing downstream tasks: code summarization and
code clone detection. For code summarization,
some works (Iyer et al., 2016; Ahmad et al., 2020)
use code context only, some methods (LeClair et al.,
2019; Alon et al., 2019a) use code structure only,
while there are also models (Hellendoorn et al.,
2020; Zügner et al., 2021) that use both informa-
tion. Further, Liu et al. (2021) introduce exter-
nal knowledge for performance improvement. For
code clone detection, existing works mainly em-
ploy code structure (Wei and Li, 2017; Zhang et al.,
2019; Wang et al., 2020) and pre-training mod-
els (Feng et al., 2020; Ahmad et al., 2021).

3 S-AST Construction

In this section, we construct S-AST. The original
AST has two main limitations:

• Low connectivity. The original AST is actu-
ally tree-structured, where every two nodes
are minimally connected with only one path.
This could lead to a long distance between
leaf nodes. As pointed out in (Alon and Ya-
hav, 2021), directly applying GNN models in
tree-shaped graphs could cause the long-range
problem.

MethodDeclarationModifierBasic-TypegetpublicintFormal-ParameterFormal-ParameterBasic-TypeaBasic-TypeStatement-ExpressionassignmentMember-ReferenceMethod-InvocationMathMember-ReferenceabsintintbaaLargerAST edgesLeaf edgesSubtoken edgesData flow edgesNon-leavesLeavesSubtoken nodesAPI nodesStatement-ExpressionassignmentMember-ReferenceMethod-InvocationMathMember-Referenceabsbbpublic int getLarger(int a, int b) {         a = Math.abs(a);         b = Math.abs(b);         … }• OOV problem. User-deﬁned identiﬁers in
codes can be arbitrarily complex and most
of them are compound words, which could
induce a large vocabulary size. For exam-
ple, the training set size in the benchmark
dataset CodeXGLUE (Lu et al., 2021) for
code summarization is 164, 814, while the
vocabulary size for AST nodes is 620, 256.
After we split the nodes by camel case and
underscores (Cvitkovic et al., 2019), the vo-
cabulary size is still as high as 201, 286. A
very large vocabulary could cause the OOV
problem (Jean et al., 2015) and thus adversely
affect the model performance.

To improve the connectivity of the AST, there
exist some works (Allamanis et al., 2018b; Wang
et al., 2020; Wang and Li, 2021) that add edges
to the AST. However, these methods cannot ad-
dress the OOV problem. Therefore, we propose a
new code intermediate graph S-AST, as shown in
Figure 1. Similar as in (Allamanis et al., 2018b;
Wang et al., 2020), we add data ﬂow edges to trace
variable transitions and connect adjacent leaf nodes
to encourage learning from contexts. To solve the
OOV problem, we further reduce the vocabulary
size by using the tokenizer of RoBERTa (Liu et al.,
2019) to tokenize every leaf node in the AST. When
a leaf node can be tokenized into multiple subto-
kens, we keep the ﬁrst subtoken as the parent node
and take other subtokens as its children. For ex-
ample, the token “getLarger” is divided into the
parent node “get” and the children nodes “L” and
“arger”. These new parent-children connections are
deﬁned as subtoken edges. With these three types
of edges added, we increase the number of edges
in the AST and improve the graph connectivity.
Further, the vocabulary size could be signiﬁcantly
In our experiments, we use javalang3
reduced.
to generate Java AST and reduce the vocabulary
size to 50, 336, where 50, 265 is the size of origi-
nal RoBERTa vocabulary and 71 is the number of
keywords in non-leaf nodes deﬁned by javalang.

4 Algorithm

In this section, we introduce the PGNN-EK model,
which is composed of two main components. On
the one hand, the partitioning-based graph neu-
ral network model (PGNN) is proposed to follow
the “divide-and-conquer” behaviours of humans to

3https://github.com/c2nes/javalang

understand programs. On the other hand, PGNN-
EK leverages external knowledge to enhance the
model’s capability. The overall architecture of
PGNN-EK is summarized in Figure 2.

Figure 2: The overall architecture of PGNN-EK

4.1 Partitioning-based Graph Neural

Networks

As illustrated in (Schulte et al., 2010) and (Park
et al., 2016), the bottom-up reasoning on the hierar-
chical relationship of statements plays an essential
role in human understanding. Therefore, we pro-
pose a statement-based partitioning algorithm to
divide S-AST into multiple subgraphs. Since S-
AST is no longer a tree, for convenience, we ﬁrst
keep subtokens and their edges in-between in S-
AST, and remove edges linking variables and those
connecting adjacent leaf nodes, to derive a tree
structure. After that, we calculate the number of
nodes in each subtree of the root node and each
subtree corresponds to a statement of the raw code.
Then, we accumulate the number of nodes in sub-
trees from left to right. When the sum exceeds the
pre-deﬁned threshold λ, we group these subtrees
into one subgraph and reset the sum to zero. If
the current subgraph is not the ﬁrst one, for each
variable node in it, we also add to the subgraph
the closest node indicating the same variable in
previous subgraphs to trace the variable transition.
After the subgraph is derived, we add edges be-
tween nodes that represent the same variable and
also connect adjacent leaf nodes as in the original
S-AST. We repeat this process until all subtrees are
visited. Note that if the node number of the last
subgraph is smaller than λ/2, we merge the last
subgraph into the penultimate subgraph. Finally,
we summarize the pseudocodes of the partitioning
algorithm in Alg. 1.

After subgraphs are derived, as in (Hellendoorn
et al., 2020), we adopt GGNN (Li et al., 2016) as
the graph embedding model, which uses a multi-

public int getLarger(int a, int b)...MethodDeclaration Modifier public BasicType int getLarger FormalParameter BasicType int a FormalParameter BasicType int b…Returns the absolute value of an int value.Raw CodeGNNCodeBERTLSTMS-ASTPartitionFuseExternalKnowledgePoolingSubgraph 1Subgraph 2layer perceptron (MLP) and a gated recurrent unit
(GRU) to perform message passing and embedding
updating. Speciﬁcally, at the (l + 1)-th layer, to
update the embedding hl+1

of node xi, we have:

i

ml+1

i =

(cid:88)

MLP(hl

j, eij),

j∈Ni
hl+1
i =GRU(ml+1

i

, hl

i),

where Ni is the neighbor set of xi and eij is the
feature vector of the edge between xi and xj. After
node embeddings are generated, we use a READ-
OUT function to obtain the graph embedding G:

G = READOUT({hi}).

We repeat the above process on each subgraph
to derive a list of subgraph embeddings L =
[G1, G2, · · · , Gn], where n is the number of sub-
graphs. Next, we keep the order of the subgraph
list and feed L into an unidirectional LSTM:

Figure 3: A toy example on code transformation with
external knowledge. The last sentence in the right box
is the API description of Math.abs.

syntactic information contained in the raw code, we
perform pre-order traversal on the AST of the code
to obtain a sequence of tokens and replace the raw
code. This is because the AST includes extra code-
related information, such as statements, variables
and operations. Then we append the correspond-
ing API description to the end. A toy example of
transformation is shown in Figure 3. Finally, we
feed the transformed context T into the pre-trained
CodeBERT4 and obtain the embedding Ee:

O = LSTM(L).

Ee = CodeBERT(T).

Inspired by the skip connection (He et al., 2016),
we also perform GGNN on the whole S-AST graph
to derive a code embedding C. Finally, we concate-
nate C and the last output O[−1] of LSTM. We
further feed the result into a fully connected layer
to get the output code embedding Ep:

Finally, we concatenate the output embeddings
of PGNN and CodeBERT, and feed the result into a
fully connected layer to obtain the ﬁnal embedding
Ef :

Ef = FC(Concat(Ep, Ee)).

Ep = FC(Concat(C, O[−1])).

5 Experiments

4.2 External Knowledge

To help understand programs, people often resort to
external knowledge. For example, humans usually
learn from massive exemplary codes written by ex-
perts for better syntactic comprehension, which are
in the format of programming language. Further,
API documentation is written in natural language
and provides semantic details on functions. There-
fore, a research question arises: how to fuse these
external syntactic and semantic knowledge into our
model?

To address the problem, we use pre-training tech-
niques in programming language processing (PLP),
which are trained on massive code corpus to learn
programming basics. In particular, we adopt Code-
BERT (Feng et al., 2020), which is a bimodal pre-
trained model for both programming language and
natural language.

Before CodeBERT is applied, we ﬁrst combine
the raw code and API descriptions. To enrich the

In this section, we evaluate the performance of
PGNN-EK. We conduct experiments on two pro-
gram understanding tasks: code summarization and
code clone detection. For each task, we use two
benchmark datasets, whose statistics are listed in
Table 1.

5.1

Implementation details

In our experiments, we use the AdamW optimizer
and linear schedule from (Wolf et al., 2020) to
update model parameters. For fair comparison,
we run all experiments on 2 Tesla V100 with 32G
memory. For PGNN, we set the number of GNN
layers, the number of LSTM layers, the embed-
ding size of GNN node, and the embedding size of
LSTM hidden layer to 3, 2, 768 and 768, respec-
tively. We choose the mean operator as the READ-
OUT function. To avoid overﬁtting, we set the
dropout rate to 0.2 in PGNN. We implement GNNs

4https://huggingface.co/microsoft/

codebert-base

MethodDeclaration Modifier public BasicType int getLarger FormalParameter BasicType int a FormalParameter BasicType int b StatementExpression Assignment MemberReference a MethodInvocation Math MemberReference a abs = StatementExpression Assignment MemberReference b MethodInvocation Math MemberReference b abs = IfStatement BinaryOperation > MemberReference a MemberReference b BlockStatement ReturnStatement MemberReference a BlockStatement ReturnStatement MemberReference b;Returns the absolute value of an int value. (API Description)public int getLarger(int a, int b) {         a = Math.abs(a);         b = Math.abs(b);         if(a > b){               return a;         }else {               return b;         }}Raw CodeTranform with External KnowledgeTable 1: The statistics of datasets

Task

Dataset

Training Validation

Test

Description

Code summarization

CodeSearchNet-Java (CSN)
TL-CodeSum (TLC)

Code clone detection

BigCloneBench (BCB)
BigCloneBench-Function (BCB-F)

164,814
69,708

901,028
398,110

5,179
8,714

10,952
8,714

Provided by CodeXGLUE
Original

415,416
78,602

415,416
81,202

Provided by CodeXGLUE
Split by functionality

based on PyTorch Geometric (Fey and Lenssen,
2019). In the EK-enhanced component, we obtain
51, 191 method-description pairs after preprocess-
ing the API documentation5. For pair examples,
see Appendix B. In the code summarization task,
we add a 6-layer Transformer-based decoder to
generate summarization as in CodeBERT. We set
learning rate to 0.00005, batch size to 16, training
steps to 50, 000, maximum code length to 256 and
maximum summarization length to 32, respectively.
In the code clone detection task, as suggested by
(Neculoiu et al., 2016), we double the PGNN-EK
to a siamese neural network to calculate code simi-
larity. We set learning rate to 0.00005, batch size
to 4, training steps to 200, 000 and maximum code
length to 400, respectively.

5.2 Code Summarization

Code summarization aims at generating natural lan-
guage comments for codes. We evaluate the perfor-
mance of PGNN-EK on two benchmark datasets,
which are TL-CodeSum (shorted as TLC) (Hu
et al., 2018) and the Java subset of CodeSearchNet
(shorted as CSN) (Husain et al., 2019). For TLC,
we use the original dataset. For CSN, we use the
version provided by CodeXGLUE (Lu et al., 2021).
For fair comparison, we use the smoothed BLEU-
4 score (Lin and Och, 2004) as in CodeXGLUE.
The larger the score, the better the model perfor-
mance. We compare our model with ﬁve repre-
sentative baselines, including CodeNN (Iyer et al.,
2016), NCS (Ahmad et al., 2020), Rencos (Zhang
et al., 2020), CodeBERT (Feng et al., 2020) and
PLBART (Ahmad et al., 2021). Due to the space
limitation, we move the details of these baselines
to Appendix C.

Table 2 shows the code summarization results.
Note that the results of CodeNN, NCS and Rencos
are directly taken from (Shi et al., 2021). Also, the
results of CodeBERT and PLBART on CSN are

5https://www.oracle.com/java/

technologies/javase-jdk8-doc-downloads.
html

derived from the leaderboard of CodeXGLUE. For
their results on TLC, we run the codes released by
the authors of the paper and set hyper-parameters
according to the original paper. From the table,
we see that, due to the fusion of external knowl-
edge, pre-training models CodeBERT, PLBART
and PGNN-EK outperform other models on both
datasets. Further, PGNN-EK performs the best.
The gaps between PGNN-EK and the runner-up
model PLBART on CSN and TLC are 0.5 and
1.05, respectively. This shows the importance of
considering human behaviors for code comprehen-
sion. We also observe that scores on TLC are sub-
stantially larger than that on CSN. This is because
codes in the training set and the test set of TLC are
considerably more similar in functionalities, which
will be elaborated in the next section.

Table 2: Code summarization results. We highlight the
best results in bold. * indicates that the improvements
are statistically signiﬁcant for p < 0.01 with paired t-
test.

Model

CSN

TLC

CodeNN
NCS
Rencos

CodeBERT
PLBART

8.58
11.19
11.80

17.65
18.45

33.03
44.25
46.81

48.53
50.01

PGNN-EK 18.95∗

51.06∗

5.3 Code Clone Detection

The goal of code clone detection is to detect
whether two code fragments implement the same
functionality. Following (Zhang et al., 2019; Wang
et al., 2020), we use the BigCloneBench 2014
dataset (Svajlenko et al., 2014) and adopt the ver-
sion provided by CodeXGLUE. We short it as
BCB.

Before we apply PGNN-EK on BCB, we no-
tice from the leaderboard of CodeXGLUE that the
results on BCB are incredibly high, where the mini-

mum F1 score is 0.949. Then we dive into the char-
acteristics of the dataset and compare BCB with
the original benchmark (Svajlenko et al., 2014).
We ﬁnd that the functionalities of codes in the test
set have all appeared in the training set of BCB.
Therefore, BCB is a very simple dataset. To test
the model’s generalization ability, we construct a
new dataset, named BCB-F, where the test set con-
tains codes whose functionality has never appeared
in the training set. We ﬁrst extract codes from the
new version benckmark (Svajlenko and Roy, 2015)
that has more code fragments and code function-
alities. We next split training/validation/test set
based on code functionalities. Speciﬁcally, we con-
struct training/validation/test set with 22/11/10
code functionalities. For details on the function-
ality splits of BCB and BCB-F, see Appendix D.
We keep the same number of positive and nega-
tive samples in all the three sets. The comparison
between BCB and BCB-F is given in Table 3.

Table 3: Comparisons between BCB and BCB-F

BCB

BCB-F

Code fragments
Functionalities
Training/Test splitting
Ratio of positive-negative

9134
10
random sample
nearly 2:1

73182
43
by functionality
1:1

In addition to the pre-training models Code-
BERT and PLBART, we further compare our model
with two representative methods in code clone de-
tection, which are ASTNN (Zhang et al., 2019)
and FA-AST (Wang et al., 2020) (For the details of
these baselines, see Appendix C).

Table 4 shows the evaluation results on the two
datasets. For BCB, we take the results of other
baseline methods from CodeXGLUE6. For BCB-F,
we run the source codes released by their authors to
obtain the results. From the table, we observe: 1)
All models perform very well on BCB, indicating
that the dataset is very simple. However, the best
F1 score on BCB-F is only 0.724, which shows that
this dataset is very challenging. 2) The non-pre-
training models ASTNN and FA-AST predict all
samples to be positive and perform poorly on BCB-
F, while pre-training models perform better. This

take

from

results

6Speciﬁcally, we
FA-AST

the
of ASTNN
https://github.com/
and
microsoft/CodeXGLUE/tree/main/Code-Code/
Clone-detection-BigCloneBench and that of
CodeBERT and PLBART from the CodeXGLUE leaderboard.
Note that PLBART only reports the F1 score on BCB.

further demonstrates the importance of introducing
external knowledge. 3) PGNN-EK achieves the
best results on both datasets. This shows that con-
sidering human behaviors in program understand-
ing enhances the generalization ability of PGNN-
EK.

Table 4: Code clone detection results w.r.t. precision
(P), recall (R) and F1 measures. We highlight the best
results in bold. * indicates that the improvements are
statistically signiﬁcant for p < 0.01 with paired t-test.

Model

ASTNN
FA-AST

P

0.92
0.96

CodeBERT
PLBART

0.960
-

BCB
R

0.94
0.94

0.969
-

F1

0.93
0.95

P

0.50
0.50

0.965
0.972

0.611
0.517

BCB-F
R

1.00
1.00

0.842
0.996

F1

0.67
0.67

0.708
0.681

PGNN-EK 0.975∗

0.973∗

0.974∗

0.621∗

0.869

0.724∗

5.4 Ablation Study

We further conduct ablation study to verify the
importance of its main components in PGNN-
EK, including subtokens, the S-AST graph, the
partitioning-based GNN and the external knowl-
edge. Speciﬁcally, one variant employs only the
S-AST graph without using external knowledge.
This helps us realize the importance of external
knowledge in program understanding. We call this
variant PGNN only. Meanwhile, we deﬁne another
variant that ignores the hierarchical relationships
in code structure and uses only external knowledge.
We call this variant EK only. To further show the
signiﬁcance of S-AST in code understanding, we
replace S-AST with the original AST in the vari-
ant PGNN-EK with AST. We also implement a
variant that does not use the subtoken tokenizer
to generate extra subtoken nodes and edges. We
call it PGNN-EK without subtoken. This variant
can be used to show the importance of subtokens
in addressing the OOV problem. To show the ad-
vantage of the partitioning strategy, we propose
a variant GNN-EK that discards the partitioning
step. Finally, we consider a variant that feeds the
raw code into the pre-trained CodeBERT without
transforming it with external knowledge. We call
this variant PGNN-CodeBERT.

Table 5 summarizes the ablation study results.
From the table, we see that: 1) S-AST contains
richer information than AST and can serve as an
effective code intermediate representation in pro-
gram understanding. The introduction of subto-
kens nodes and edges alleviates the OOV problem

Table 5: Ablation study on PGNN-EK. We highlight the best results in bold.

Method

CSN
(Smoothed BLEU-4)

TLC
(Smoothed BLEU-4)

BCB BCB-F
(F1)

(F1)

PGNN only
EK only
PGNN-EK with AST
PGNN-EK without subtoken
GNN-EK
PGNN-CodeBERT

PGNN-EK (Full Model)

14.05
17.95
17.70
17.82
18.05
18.60

18.95

47.71
49.66
48.96
49.01
49.95
50.65

51.06

0.951
0.965
0.957
0.958
0.967
0.969

0.667
0.711
0.713
0.712
0.715
0.720

0.974

0.724

and enhances the model performance. 2) Exter-
nal knowledge helps boost understanding codes.
In particular, code transformation with external
knowledge improves the expressiveness of the raw
code. 3) The full model PGNN-EK outperforms
other variants on all the datasets and tasks. This
indicates the importance of every main component
in PGNN-EK. It further shows that leveraging code
context, code structure and external knowledge as
humans is helpful for program understanding.

5.5 The Inﬂuence of Subgraph Size

We end this section with a hyper-parameter
sensitivity analysis.
In PGNN-EK there is a
key hyper-parameter λ that is used to control
the size of subgraphs. Here, we investigate
the sensitivity of λ. We vary the value of λ
from {10, 30, 50, 70, 90, 110, 130, 150, 170, 190},
and the ﬁnal prediction results of PGNN-EK on 4
datasets are shown in the Figure 4.

Table 6: The average number of nodes in S-AST

Datasets

CSN TLC BCB BCB-F

S-AST size

137

140

372

348

The results indicate that 1) the model perfor-
mance ﬁrst increases and then drops, with the in-
crease of the subgraph size. When the subgraph
size is too small, each subgraph is a code frag-
ment that no longer represents a code statement
and thus contains less information. Further, when
the subgraph is too large, each subgraph could be
composed of statements that are of different se-
mantic meanings, which thus degrades the model
performance. 2) PGNN-EK performs the best at
λ = 30 on CSN and TLC while it achieves the
best results at λ = 70 on BCB and BCB-F. We
further investigate the reason and show the average

Figure 4: The inﬂuence of subgraph size on 4 datasets.

number of nodes in S-AST on the four datasets in
Table 6. From the table, BCB and BCB-F contain
∼ 2.5 times more nodes than that in CSN and TLC.
This empirically suggests that setting λ to be about
5 to 1
1
4 of the average node number in S-AST could
be a reasonable choice.

6 Conclusion

In this paper, we followed human understandings
for programs and proposed the PGNN-EK model.
To enrich the code structure information and alle-
viate the OOV problem, we presented the S-AST
graph based on AST, which uses a subtoken tok-
enizer to generate subtoken nodes and edges be-
tween them. Inspired by the “divide-and-conquer”
strategy, we proposed the partitioning-based graph
neural network model on S-AST that employs
code context and structure. To leverage the exter-
nal knowledge to boost comprehension, we trans-
formed the raw code to fuse syntactic and semantic
knowledge and utilized pre-training techniques for
information extraction. We performed extensive
experiments to show the effectiveness of our model
PGNN-EK on the code summarization and code

02040608010012014016018020015.015.516.016.517.017.518.018.519.019.520.0CSNSmoothed BLEU-4(cid:1)02040608010012014016018020047.047.548.048.549.049.550.050.551.051.552.0TLCSmoothed BLEU-4(cid:1)0204060801001201401601802000.900.910.920.930.940.950.960.970.980.991.00BCBF1(cid:1)0204060801001201401601802000.600.620.640.660.680.700.720.740.760.780.80 BCB-FF1(cid:1)clone detection tasks. In particular, to show the
generalization ability of the model, we released a
new benchmark that is more challenging.

7 Acknowledgments

This work has been supported by the National Nat-
ural Science Foundation of China under Grant No.
U1911203, Alibaba Group through the Alibaba
Innovation Research Program, the National Natu-
ral Science Foundation of China under Grant No.
61877018 and No.61977025, and Shanghai Pujiang
Talent Program under Grant No. 21PJ1402900.

References

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. 2020. A transformer-
based approach for source code summarization. In
ACL 2020.

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. 2021. Uniﬁed pre-training
for program understanding and generation.
In
NAACL-HLT 2021.

representation for data ﬂow analysis and compiler
optimizations. In ICML 2021.

Milan Cvitkovic, Badal Singh, and Animashree Anand-
kumar. 2019. Open vocabulary learning on source
code with a graph-structured cache. In ICML 2019.

Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016.
A deep language model for software code. CoRR,
abs/1608.02715.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
bert: A pre-trained model for programming and nat-
ural languages. In EMNLP 2020.

Matthias Fey and Jan E. Lenssen. 2019. Fast graph
representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on
Graphs and Manifolds.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR 2016.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh,
Petros Maniatis, and David Bieber. 2020. Global
relational models of source code. In ICLR 2020.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and
Charles Sutton. 2015. Suggesting accurate method
and class names. In ESEC/FSE 2015.

Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,
and Premkumar T. Devanbu. 2016. On the natural-
ness of software. Commun. ACM, 59(5):122–131.

Miltiadis Allamanis, Earl T. Barr, Premkumar T. De-
vanbu, and Charles Sutton. 2018a. A survey of ma-
chine learning for big code and naturalness. ACM
Comput. Surv., 51(4):81:1–81:37.

Miltiadis Allamanis, Marc Brockschmidt, and Mah-
moud Khademi. 2018b. Learning to represent pro-
grams with graphs. In ICLR 2018.

Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and
Zhi Jin. 2018. Summarizing source code with trans-
ferred API knowledge. In IJCAI 2018.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR, abs/1909.09436.

Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav.
2019a. code2seq: Generating sequences from struc-
tured representations of code. In ICLR 2019.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
using a neural attention model. In ACL 2016.

Uri Alon and Eran Yahav. 2021. On the bottleneck of
graph neural networks and its practical implications.
In ICLR 2021.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran
Yahav. 2019b. code2vec: learning distributed rep-
resentations of code. Proc. ACM Program. Lang.,
3(POPL):40:1–40:29.

Yoshua Bengio, Yann LeCun, and Geoffrey E. Hin-
ton. 2021. Deep learning for AI. Commun. ACM,
64(7):58–65.

Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021.
Infercode: Self-supervised learning of code repre-
sentations by predicting subtrees. In ICSE 2021.

Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun,
Torsten Hoeﬂer, Michael F. P. O’Boyle, and Hugh
Leather. 2021. Programl: A graph-based program

Sébastien Jean, KyungHyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
ACL 2015.

Rafael-Michael Karampatsis, Hlib Babii, Romain
Robbes, Charles Sutton, and Andrea Janes. 2020.
Big code != big vocabulary: open-vocabulary mod-
els for source code. In ICSE ’20.

Alexander LeClair, Siyuan Jiang, and Collin McMillan.
2019. A neural model for generating natural lan-
guage summaries of program subroutines. In ICSE
2019.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,

and comprehension.
7880. Association for Computational Linguistics.

In ACL 2020, pages 7871–

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and
Richard S. Zemel. 2016. Gated graph sequence neu-
ral networks. In ICLR 2016.

Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation met-
rics for machine translation. In COLING 2004.

Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,
and Yang Liu. 2021. Retrieval-augmented genera-
tion for code summarization via hybrid GNN.
In
ICLR 2021.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Paul Neculoiu, Maarten Versteegh, and Mihai Ro-
taru. 2016. Learning text similarity with siamese
the 1st
recurrent networks.
Workshop on Representation Learning for NLP,
Rep4NLP@ACL 2016.

In Proceedings of

Thomas H. Park, Meen Chul Kim, Sukrit Chhabra,
Brian Lee, and Andrea Forte. 2016. Reading hierar-
chies in code: Assessment of a basic computational
skill. In ITiCSE 2016, pages 302–307. ACM.

Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke,
Di He, and Tie-Yan Liu. 2021. How could neural
networks understand programs? In ICML 2021.

Veselin Raychev, Martin T. Vechev, and Eran Yahav.
2014. Code completion with statistical language
models. In PLDI ’14.

Carsten Schulte, Tony Clear, Ahmad Taherkhani,
Teresa Busjahn, and James H. Paterson. 2010. An
introduction to program comprehension for com-
puter science educators. In Proceedings of the 2010
ITiCSE working group reports, ITiCSE-WGR 2010,
pages 65–86. ACM.

Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi
Han, Hongyu Zhang, Dongmei Zhang, and Hongbin
Sun. 2021. Neural code summarization: How far are
we? CoRR, abs/2107.07112.

Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo,
Chanchal Kumar Roy, and Mohammad Mamun Mia.
2014. Towards a big data curated benchmark of
inter-project code clones. In ICSME 2014.

Jeffrey Svajlenko and Chanchal K. Roy. 2015. Evalu-
ating clone detection tools with bigclonebench. In
ICSME 2015.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
Intellicode compose:
In ESEC/FSE

and Neel Sundaresan. 2020.
code generation using transformer.
’20.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017.

Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin.
2020. Detecting code clones with graph neural net-
work and ﬂow-augmented abstract syntax tree.
In
SANER 2020.

Yanlin Wang and Hui Li. 2021. Code completion by
modeling ﬂattened abstract syntax trees as graphs.
In AAAI 2021.

Huihui Wei and Ming Li. 2017. Supervised deep fea-
tures for software functional clone detection by ex-
ploiting lexical and syntactical information in source
code. In IJCAI 2017.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.

Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan
Vasilescu, and Graham Neubig. 2020. Incorporating
external knowledge through pre-training for natural
language to code generation. In ACL 2020.

Fabian Yamaguchi, Nico Golde, Daniel Arp, and Kon-
rad Rieck. 2014. Modeling and discovering vulner-
abilities with code property graphs. In 2014 IEEE
Symposium on Security and Privacy, SP 2014.

Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and
Qianxiang Wang. 2019. Neural detection of seman-
tic code clones via tree-based convolution. In ICPC
2019.

Zeping Yu, Wenxin Zheng, Jiaqi Wang, Qiyi Tang, Sen
Nie, and Shi Wu. 2020. Codecmr: Cross-modal re-
trieval for function-level binary source code match-
ing. In NeurIPS 2020.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning

to execute. CoRR, abs/1410.4615.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,
and Xudong Liu. 2020. Retrieval-based neural
source code summarization. In ICSE 20.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,
Kaixuan Wang, and Xudong Liu. 2019. A novel
neural source code representation based on abstract
syntax tree. In ICSE 2019.

Daniel Zügner, Tobias Kirschstein, Michele Catasta,
Jure Leskovec, and Stephan Günnemann. 2021.
Language-agnostic representation learning of source
code from structure and context. In ICLR 2021.

• ASTNN (Zhang et al., 2019) proposes an AST-
based neural network that splits AST into a
sequence of statement trees and applies a bidi-
rectional RNN model to produce source code
representation. However, it ignores external
knowledge associated with codes.

• FA-AST (Wang et al., 2020) augments orig-
inal AST with explicit control and data ﬂow
edges, then introduces two different types of
GNNs to detect code clones.

A Partitioning S-AST Algorithm

D Functionalities Splits in BCB and

See Algorithm 1.

B Examples of API-Description Pairs

In the experiment. we obtain 51, 191 method de-
scription pairs after preprocessing, and Table 7
gives some examples.

C Baselines Introduction

We compare our model with ﬁve representative
models in code summarization task:

• CodeNN (Iyer et al., 2016) is the ﬁrst method
that applies deep neural networks in code sum-
marization. It uses a classical attention-based
encoder-decoder framework from Neural Ma-
chine Translation (NMT).

• NCS (Ahmad et al., 2020) applies Trans-
former (Vaswani et al., 2017) to model the
pairwise relationship between code tokens and
capture their long-term dependencies.

• Rencos (Zhang et al., 2020) proposes an
attention-based encoder-decoder model and
enhance it with the most similar code snippets
retrieved from the training set.

• CodeBERT (Feng et al., 2020) is a bimodal
pre-training model for programming and nat-
ural languages based on RoBERTa (Liu et al.,
2019).

• PLBART (Ahmad et al., 2021) is a sequence-
to-sequence pre-training model based on
BART (Lewis et al., 2020).

In addition to the pre-training models Code-
BERT and PLBART, we further compare our model
with two representative model in code clone detec-
tion task:

BCB-F

For BCB, the functionalities in Train/Val/Test set
are:

• Train: Web Download, Secure Hash(MD5),
Copy a File, Decompress Zip, FTP Authen-
ticated Login, Bubble Sort, Init. SGV with
Model, SGV Selection Event Handler, Cre-
ate Java Project(Eclipse), SQL Update and
RollBACK.

• Val: Same to Train.

• Test: Same to Train.

For BCB-F, the functionalities in Train/Val/Test
set are, where the emphasis discloses the whole 10
functionalities that exist in BCB:

• Train: Decompress Zip, Copy a File, Get
Prime Factors, File Dialog, Resize Array, Get
MAC Address String, Parse CSV File, Secure
Hash(MD5), Send Email, Load Custom Font,
Create Java Project(Eclipse), Extract Matches
Using Regex, Open File in Desktop Applica-
tion, Connect to Database, Load File to Byte
Array, Call Method Using Reﬂection, Take
Screenshot to File, Write PDF File, Delete
Folder and Contents, Copy Directory, Binary
Search, Delete Folder and Contents.

• Val: SQL Update and RollBACK, Bubble Sort,
Execute External Process, XMPP Send Mes-
sage, Zip Files, Convert Date String Format,
Secure Hash, GCD, SGV Selection Event Han-
dler, Init. SGV with Model, Play Sound.

• Test: Shufﬂe Array in Place, Create Encryp-
tion Key Files, Load Custom Font, Encrypt to
File, Parse XML to DOM, CRC32 File Check-
sum, Transpose a Matrix, Test Palindrome,
Web Download, FTP Authenticated Login.

Table 7: Examples of API-Description Pairs

APIs

Descriptions

Math.abs
Arrays.hashcode
Scanner.hasNext
Color.getRGB

Returns the absolute value of an int value.
Returns a hash code based on the contents of the speciﬁed array.
Returns true if this scanner has another token in its input.
Returns the RGB value representing the color in the default sRGB ColorModel.

Algorithm 1 Partitioning S-AST
Input: A S-AST T with node features X , edge
indexes I and edge features E
Parameter: λ, which speciﬁes the minimum num-
ber of nodes in the subgraph
Output: Nodes features list Lx, edge indexes list
Li, and edge features list Le of subgraphs
1: Derive a tree structure T (cid:48)

by removing data

ﬂow edges and adjacent leaf edges in T ;

2: nodes_sum ← 0, nodes_set ← {};
3: nf _list, ei_list, ef _list, Lx, Li, Le ← {};
4: Obtain a subtree list {S} based on subtrees of

from left to right;

root nodes in T (cid:48)
5: for S in {S} do
6:

n ← the number of nodes in S;
nodes_sum ← nodes_sum + n;
Add nodes in S to nodes_set;
if nodes_sum ≥ λ or S is the last element
of {S} then

if Lx (cid:54)= ∅ then

Add closest nodes that indicate the
same variables in Lx to nodes_set ;

end if
Assign nf _list, ei_list, ef _list based
on nodes_set, X , I and E;
Append nf _list, ei_list, ef _list
Lx, Li, Le respectively;
nodes_sum ← 0, nodes_set ← {};

to

7:

8:

9:

10:

11:

12:

13:

14:

15:

end if
16:
17: end for
18: // A[−i] denotes the i-th element from the bot-

tom in A.

19: if size of Lx[−1] < λ/2 and size of Lx > 1

then

20: Merge Lx[−1] and Lx[−2], Li[−1] and

Li[−2], Le[−1] and Le[−2], respectively;

21: end if
22: return Lx, Li, Le

