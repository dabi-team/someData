Fast convergence rates for dose-response estimation

Matteo Bonvini∗

Edward H. Kennedy†

July 26, 2022

Abstract

2
2
0
2

l
u
J

4
2

]
E
M

.
t
a
t
s
[

1
v
5
2
8
1
1
.
7
0
2
2
:
v
i
X
r
a

We consider the problem of estimating a dose-response curve, both globally and locally at a point.
Continuous treatments arise often in practice, e.g. in the form of time spent on an operation, distance
traveled to a location or dosage of a drug. Letting A denote a continuous treatment variable, the target
of inference is the expected outcome if everyone in the population takes treatment level A = a. Under
standard assumptions, the dose-response function takes the form of a partial mean. Building upon
the recent literature on nonparametric regression with estimated outcomes, we study three diﬀerent
estimators. As a global method, we construct an empirical-risk-minimization-based estimator with an
explicit characterization of second-order remainder terms. As a local method, we develop a two-stage,
doubly-robust (DR) learner. Finally, we construct a mth-order estimator based on the theory of higher-
order inﬂuence functions. Under certain conditions, this higher order estimator achieves the fastest rate
of convergence that we are aware of for this problem. However, the other two approaches are easier
to implement using oﬀ-the-shelf software, since they are formulated as two-stage regression tasks. For
each estimator, we provide an upper bound on the mean-square error and investigate its ﬁnite-sample
performance in a simulation. Finally, we describe a ﬂexible, nonparametric method to perform sensitivity
analysis to the no-unmeasured-confounding assumption when the treatment is continuous.

Keywords: continuous treatments, eﬃcient inﬂuence function, orthogonal learning, observational studies.

1

Introduction

1.1 Notation & setup

Continuous or multi-valued treatments occur often in practice; time, distance traveled, or dosage of a drug
are common examples. We study the problem of estimating the eﬀect of a continuous treatment A ∈ A ⊂ R
on an outcome Y ∈ Y ⊂ R. Within the potential outcomes framework [Rubin, 1974], this eﬀect is deﬁned as
the expectation of the potential outcome Y a, which is the outcome observed if the subject takes treatment
level A = a. In other words, the estimand represents the average outcome if everyone in the population had
taken treatment level a. Because A is continuous, E(Y a) is a curve, often referred to as the dose-response
function (DRF). Under standard assumptions (see e.g. Kennedy et al. [2017]), the DRF takes the form of
a partial mean:

θ(t) = E{E(Y | A = t, X)} =

(cid:90)

E(Y | A = t, X = x)dP(x)

where X ∈ X ⊂ Rd denotes measured confounders. Let Z = (Y, A, X) be distributed according to some
distribution P with density p with respect to the Lebesgue measure. The goal of this paper is to discuss

∗Department of Statistics & Data Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213. Email:

mbonvini@stat.cmu.edu.

†Associate Professor, Department of Statistics & Data Science, Carnegie Mellon University, 5000 Forbes Avenue, Pitts-

burgh, PA 15213. Email: edward@stat.cmu.edu.

1

 
 
 
 
 
 
new ways of estimating θ(a) using n iid copies of Z, which yield strong error guarantees, under weaker
conditions, and fast rates of convergence. To simplify the notation we deﬁne:

p(u) =

d
du

P(U ≤ u),

π(a | x) =

p(a, x)
p(x)

, µ(a, x) = E(Y | A = a, X = x), w(a, x) =

p(a)
π(a | x)

That is, p(u) is the density of U at U = u, µ(a, x) is the outcome regression, and π(a | x) is the
conditional density of A given X = x. We will sometimes denote all the nuisance functions by η =
{p(a), p(x), µ(a, x), π(a | x)}. With this notation, we have

θ(t) = E{µ(t, X)} = E {w(t, X)Y | A = t}

For a kernel function K(u), we let Kht(a) = h−1K((a − t)/h). We use the notation P{g(Z)} = (cid:82) g(z)dP(z)
and Pn{g(Z)} = n−1 (cid:80)n
i=1 g(Zi) to denote means (given g) and sample means. Further, we let (cid:107)f (cid:107)2 =
(cid:82) f 2(z)dP(z) = P{f 2(Z)} to denote the squared L2(P) norm.

Throughout the paper, we will rely on the following assumptions. Additional assumptions will be intro-
duced as needed.

1. Positivity: π(a | x) and its estimator (cid:98)π(a | x) are bounded above and away from zero for all a ∈ A

and x ∈ X ;

2. Boundedness: Y , A, (cid:98)µ(a, x) are uniformly bounded.

Notice that Positivity is enough to deﬁne θ(a) = (cid:82) µ(a, x)dP(x), but not enough to interpret θ(a) as the
dose-response curve. To interpret θ(a) as the eﬀect of A on Y , one needs to impose additional causal
assumptions such as Y a ⊥⊥ A | X and A = a =⇒ Y a = Y , i.e., no unmeasured confounding and
consistency (e.g. no interference). This paper is about estimating θ(a), regardless of its interpretation,
and we refer to Kennedy et al. [2017] and reference therein for more details on identiﬁcation.

Finally, our focus will be on estimation of the dose-response in nonparametric models where the dose-
response itself and the nuisance functions possess varying degrees of smoothness. In particular, we will
distinguish between the smoothness levels of the dose response a (cid:55)→ θ(a), the conditional density of the
treatment given the measured confounders (a, x) (cid:55)→ π(a | x), and the outcome regression (a, x) (cid:55)→ µ(a, x).
We will further reﬁne this distinction when introducing the mth-order estimator in the sense that we will
consider models where a (cid:55)→ µ(a, x), x (cid:55)→ µ(a, x), a (cid:55)→ π(a | x) and x (cid:55)→ π(a | x) may have diﬀerent
smoothness levels. Note that it is reasonable to expect the smoothness of a (cid:55)→ µ(a, x) to match that of
a (cid:55)→ θ(a) = (cid:82) µ(a, x)dP(x) in most applications.

1.2 Literature review

Crucially, because A is continuous, the parameter θ(t) cannot be estimated at
n rates in nonparametric
models. Informally, in order to see this, notice that we can write θ(t) = E{w(t, X)Y | A = t} and thus even
if w(t, X)Y was fully observed (e.g., in a randomized experiment), the best convergence rate attainable
would be that of nonparametric regression.
In fact, in order to compare the performances of diﬀerent
estimators, it is useful to establish the regimes where they behave like the oracle estimator that has access
to w(t, X)Y and can regress it on A.

√

Deﬁnition 1 (Oracle rate). Given an iid sample Z1, . . . , Zn, let (cid:101)θ(·) be the (infeasible) estimator regressing
w(t, X)Y on A and let rn be its error under some loss, e.g., the square loss at a point: E[{(cid:101)θ(t) − θ(t)}2].
We refer to rn as the oracle rate.

In nonparametric models that admit slow rates of convergence, two commonly employed strategies are
either 1) to specify a marginal structural model θ(t) = m(t; β) [Robins, 2000] or 2) to change the target of

2

inference from θ(t) to a projection of θ(t) onto a ﬁnite-dimensional model g(t; β). We refer to Neugebauer
and van der Laan [2007] and Ai et al. [2018] for discussions of eﬃcient estimation in the latter case.

Another approach for estimation of dose-responses is to impose some nonparametric, structural assumptions
on the curve itself. For instance, if it is known that the treatment cannot harm the patients, one may impose
a monotonicity assumption [Westling et al., 2020; Westling and Carone, 2020]. Yet another approach is to
choose a candidate estimator of θ(a) that minimizes a good estimate of the risk. The key insight is that,
while θ(a) is generally not estimable at
n-rates, the integrated risk of a candidate estimator is [D´ıaz and
van der Laan, 2013; Van der Laan et al., 2003].

√

√

In the context of nonparametric estimation, Newey [1994] derives suﬃcient conditions under which a two-
stage kernel estimator of θ(t) is asymptotically normal and unbiased. Their estimator is of the plug-in
variety and takes the form (cid:98)θ(a) = n−1 (cid:80)n
i=1 (cid:98)µ(t, Xi), where (cid:98)µ(t, x) is a kernel-smoothed estimate of µ(t, x)
nh-consistency and asymptotic unbiasedness, (cid:98)µ(t, x) has to
depending on some bandwidth h. To achieve
be undersmoothed ; i.e., h has to be chosen smaller than that minimizing the asymptotic mean-square-error
of (cid:98)µ(t, x). Choosing the right amount of undersmoothing presents challenges in practice; see e.g., Section
5.7 in Wasserman [2006]. Starting from the estimator considered in Newey [1994], Flores [2007] develops
plug-in-type estimators of the maximum of θ(a) and the value of a at which the maximum is attained.
Galvao and Wang [2015] study estimation and testing of continuous treatment eﬀects in general settings
using inverse-probability-weighted estimators. Singh et al. [2020] analyze plug-in-type estimators of general
causal functions based on reproducing kernel methods.

There exists another representation of the DRF that plays an important role in developing eﬃcient esti-
mators:

θ(a) = E{ϕ(Z) | A = a}, where ϕ(Z) = w(A, X){Y − µ(A, X)} +

(cid:90)

µ(A, x)dP(x)

This representation motivates estimators that regress the pseudo-outcome ϕ(Z) onto A. Because this
pseudo-outcome depends on unknown nuisance functions, it needs to be estimated from the data; thus
we refer to the regression of ϕ(Z) on A as a second-stage regression. The crucial point is that ϕ(Z) is
such that an estimated regression (cid:98)En{ (cid:98)ϕ(Z) | A = a} can behave like the oracle (cid:98)En{ϕ(Z) | A = a} even
when (cid:98)ϕ(Z) converges to ϕ(Z) at a rate that is slower than the convergence of (cid:98)En{ϕ(Z) | A = a} to θ(a).
We conclude this section with a review of the use of this pseudo-outcome in the estimators proposed in
Kennedy et al. [2017], Semenova and Chernozhukov [2017] and Colangelo and Lee [2020], as they are the
ones most similar to the estimators considered in this article.

1.3 Review of existing doubly-robust estimators

In this section, we review a few estimation strategies that are doubly-robust and yield fast convergence rates
in the sense that the upper bound on the risk is of the form: “oracle rate + second order, doubly-robust
remainder terms,” and thus yield consistent estimators when either w(a, x) or µ(a, x), but not necessarily
both, are consistently estimated. This is analogous to the case of treatment eﬀects deﬁned by categorical
treatments, whereby estimators based on inﬂuence functions are doubly-robust and enjoy second-order
error terms.

The estimators proposed in Semenova and Chernozhukov [2017] and Kennedy et al. [2017] are based on
regressing an estimate of ϕ(Z) onto A. The quantity ϕ(Z) has a doubly robust remainder error, or equiv-
alently, satisﬁes a Neyman-orthogonality condition in the sense that, for η = {p(A), π(A | X), µ(A, X)}:

∂rE{ϕ(Z; η0 + r(η − η)) | A = a}|r=0= 0 for all a, η.

3

This implies that the loss {ϕ(Z; η) − θ(A)}2 is universally Neyman-orthogonal in the sense that

∂r2∂r1E[ϕ(Z; η0 + r2(η − η0)) − θ(A) − r1{θ(A) − θ(A)}]2|r1=r2=0

∂rE{ϕ(Z; η0 + r(η − η0)) | A = a}r=0{θ(a) − θ(a)}dP(a)

(cid:90)

= −2

= 0

for any θ, θ.

Constructing estimators satisfying Neyman-orthogonality conditions has a long history in Statistics, albeit
under diﬀerent names. For example, in functional estimation, and, in particular, estimation of aver-
age treatment eﬀects, estimators that are “Neyman-orthogonal,” “bias-corrected,” “augmented-inverse-
probability-weighted,” or, more generally, constructed according to the “double machine learning” frame-
work are all based on ﬁrst-order functional Taylor expansions, also known as von-Mises expansions [Kennedy,
2022]. In fact, underlying Neyman orthogonality is a ﬁrst-order expansion of the target estimand ψ(P),
viewed as a function of the unknown distribution P, around an estimator (cid:98)P of P. If the derivative term,
say ψ(cid:48)(P − (cid:98)P; (cid:98)P), exists then the estimator consisting of the (estimated) derivative term plus the initial
estimator ψ((cid:98)P) should exhibit second-order error rates. For smooth functionals, the derivative term can
be written as ψ(cid:48)(P − (cid:98)P; (cid:98)P) = (cid:82) φ(z; (cid:98)P)dP(z), where φ(z) is the inﬂuence function and is mean-zero. For
more complex parameters, such as the dose-response curve, this representation is generally not possible.
However, one may try to express the derivative as an integral with respect to the conditional distribution
of the observations given, for example, the treatment.
Kennedy et al. [2017] show that, when the second stage regression (cid:98)En( (cid:98)ϕ(Z) | A = a) is a local linear
regression, then the oracle rate (the rate achievable if ϕ(Z) was fully observed as deﬁned in Deﬁnition 1)
is attained as long as

sup
a:|a−t|≤h

(cid:107)(cid:98)π(a | X) − π(a | X)(cid:107)(cid:107)(cid:98)µ(a, X) − µ(a, X)(cid:107)= oP(1/

nh),

(1)

√

where h is the bandwidth used in the second-stage regression. A similar requirement appears in Colangelo
and Lee [2020]. Notice that this error term is second order, as it is a product of errors. It also reveals
the double-robustness property of ϕ(Z): consistency of (cid:98)En{ (cid:98)ϕ(Z) | A = a} requires consistency of either
π(A | X) or µ(A, X) but not necessarily both.

Semenova and Chernozhukov [2017] studies an estimator of the same form P∗
n is a
series estimator. Their estimator uses cross-ﬁtting, whereby, for a given fold k, the nuisance functions are
estimated on all folds but k, and P∗
n is computed using observations from k. This construction bypasses
the need to impose Donsker conditions on the nuisance functions. We note that, relative to the results in
Kennedy et al. [2017] and Colangelo and Lee [2020], those in Semenova and Chernozhukov [2017] appear to
require that the product of root-mean-square-errors for estimating µ(a, x) and π(a | x) is of smaller order
nk, where k is the dimension of the basis (Assumptions 3.5 and 4.9). This is more stringent of a
than 1/
requirement than (1).

n{ (cid:98)ϕ(Z) | A = a} where P∗

√

The approach taken by Colangelo and Lee [2020] is diﬀerent in that instead of regressing (cid:98)ϕ(Z) onto A, the
estimator is

(cid:98)θ(t) =

1
n

n
(cid:88)

i=1

(cid:20) Kht(Ai){Yi − (cid:98)µ(t, Xi)}
(cid:98)π(t | Xi)

(cid:21)

+ (cid:98)µ(t, Xi)

(2)

They motivate their estimator as being based on an approximate ﬁrst-order inﬂuence function, which can
be calculated as the Gateaux derivative with respect to smooth deviations from the true data-generating
distribution as these deviations approach a distribution with point-mass at A = t (see their Section 4).

4

This estimator still enjoys second order rates, but, it is not immediately clear how it adapts to diﬀerent
level of smoothness of θ(a). That is, their error rates may be of the form “oracle + second-order terms”
only in certain smoothness regimes of θ(a). This is in contrast to estimators based on regressing (cid:98)ϕ(Z) on A,
which would behave like an oracle, and thus adapt to the smoothness of θ(a), as long as the second-order
remainder terms are negligible. Their analysis focuses on low-smoothness regimes; viewed as a function of
a, they assume that the joint density of the observations is three-times continuously diﬀerentiable. Notice
that this implies that both the outcome regression a (cid:55)→ µ(a, x) and the conditional density a (cid:55)→ π(a | x)
are three-times continuously diﬀerentiable. In practice, however, it could be that a (cid:55)→ µ(a, x) and thus
the dose-response curve are smoother than a (cid:55)→ π(a | x). Our mth-order estimator is an extension of (2)
and appears to track the smoothness of the dose-response only in cases when this is no-greater than the
smoothness of a (cid:55)→ π(a | x), which appears to be consistent with the results in Colangelo and Lee [2020].

1.4 Our contribution

Our contribution is mainly three-fold. We study three approaches to dose-response estimation: one based on
estimators relying on approximate ﬁrst-order inﬂuence functions and one based on higher-order corrections.
For the ﬁrst approach, we consider two estimation strategies. The ﬁrst one is based on empirical loss
minimization, which we view as a “global” method since it naturally estimates the curve on its entire
support. Our approach specializes the results of Foster and Syrgkanis [2019] on empirical loss minimization
with estimated outcomes to estimate dose-response functions under the square-loss. Importantly, we show
that the resulting estimator is doubly-robust and give an explicit characterization of the remainder term.
This, in turn, implies faster rates of convergence than those directly obtainable from the results in Foster
and Syrgkanis [2019] whenever the treatment and the outcome models are estimated at diﬀerent rates.
The second one extends the DR-learner estimator of the conditional average treatment eﬀect (proposed
in Kennedy [2020]) to the continuous treatment eﬀect setting. We view this as a “local” method since it
estimates the dose-response at a speciﬁc point.

Next, we show how convergence rates can be substantially improved using kernel-smoothed, approximate
higher order inﬂuence functions [Robins et al., 2008, 2009a, 2017]. To the best of our knowledge, our
higher order estimator is the ﬁrst use of higher order inﬂuence functions to estimate a dose-response curve.
Further, we are not aware of other estimators of the dose-response curve that exhibit convergence rates as
fast as that of our higher order estimator, under similar assumptions on the data generating process.

Finally, extending the work of Bonvini et al. [2022] on sensitivity analysis in marginal structural models,
we describe a simple, yet ﬂexible framework to gauge the impact of potential unmeasured confounders on
the dose-response estimates. We analyze the performance of DR-Learner-based estimators of the bounds
on the dose-response function derived under the sensitivity model.

2 Doubly-robust estimators

2.1 General doubly-robust estimation procedure

Here, we expand on the list of estimators enjoying second-order, doubly robust errors. We will show that
extensions of the general procedure proposed in Foster and Syrgkanis [2019] and the DR-learner approach
proposed by Kennedy [2020] in the context of conditional eﬀects deﬁned by binary treatments also yield
estimators enjoying second-order and doubly-robust remainder terms. The work by Foster and Syrgkanis
[2019] is rather general and already yields estimators that have second-order remainder terms, but their
rates are in terms of (cid:107)(cid:98)η − η(cid:107)F where (cid:107)f (cid:107)F is a norm for the function spaces where all nuisance functions
η live in. We apply their results to the dose-response settings and show that it is possible to obtain
estimators that are also doubly-robust. Establishing the double-robustness property, i.e. that the second
order remainder term is a product of errors, is particularly important when the estimators of the nuisance

5

functions converge at diﬀerent rates, since the product of the errors would be of smaller order than the
sum of the squared errors.

Let Zn
2 and Zn
1 , Zn
ϕ(Zj) of the form

3 denote three independent samples. We will work with estimates of the pseudo-outcome

(cid:98)ϕ(Zj) = (cid:98)w(Aj, Xj){Yj − (cid:98)µ(Aj, Xj)} +

1
n

n
(cid:88)

i=1

(cid:98)µ(Aj, Xi)

where (cid:98)µ(a, x) and (cid:98)w(a, x) are estimated using observations in Zn
and Zj belongs to Zn
only two samples, say Zn

i=1 belong to Zn
2
3 . An alternative approach, taken in Semenova and Chernozhukov [2017], is to consider

1 , the observations (Xi)n

1 and Zn

2 , and compute

(cid:98)ϕ(Zj) = (cid:98)w(Aj, Xj){Yj − (cid:98)µ(Aj, Xj)} +

1
n

n
(cid:88)

i(cid:54)=j

(cid:98)µ(Aj, Xi)

i=1 in the same sample Zn

for Zj and (Xi)n
2 . We proceed by considering three separate samples to simplify
1 , Zn
the analysis of all our estimators, as we have (cid:98)ϕ(Zk) ⊥⊥ (cid:98)ϕ(Zl) | (Zn
2
and Zn
3 can be swapped, which results in three estimators of θ(t). One can then take their average as the
ﬁnal estimator. From a sample of iid observations, it is possible to obtain separate independent samples
simply by randomly split the data into sub-samples. To keep the notation as light as possible, we analyze
the theoretical properties of the estimators based a single split into three subsamples. However, we expect
the same arguments to hold when multiple splits are performed.

2 ) for k (cid:54)= l. The roles of Zn

1 , Zn

Our estimation procedure is summarized in the following algorithm.

Algorithm 1. Let Zn

1 , Zn

2 and Zn

3 denote three independent samples of n iid observations of Z = (Y, A, X).

1. Nuisance training

• Using only observations in Zn

• Using only observations in Zn

1 , estimate µ(A, X) with (cid:98)µ(A, X) and w(A, X) with (cid:98)w(A, X);
2 , estimate m(a) = (cid:82) µ(a, x)p(x)dx with (cid:98)m(a) = n−1 (cid:80)n

i=1 (cid:98)µ(a, Xi).

2. Pseudo-outcome construction: using observations in Zn

3 , construct the pseudo-outcome

(cid:98)ϕ(Z) = (cid:98)w(A, X){Y − (cid:98)µ(A, X)} + (cid:98)m(A)

3. Second stage regression, either of the following:

(a) Empirical-risk-minimization: Deﬁne (cid:98)θ to be the empirical risk minimizer

(cid:98)θ = arg min

θ∈Θ

1
n

(cid:88)

i∈Zn
3

{ (cid:98)ϕ(Zi) − θ(Ai)}2

where Θ is some function class.

(b) DR-Learner: Deﬁne

(cid:98)θ(t) =

1
n

(cid:88)

i∈Zn
3

Wi(t; An) (cid:98)ϕ(Zi)

where Wi(t; An) are weights depending on t and An = (A1, . . . , An) ⊂ Zn
3 .

6

4. (Optional) Cross-ﬁtting: swap the role of Zn
of the three estimators as an estimate of θ.

1 , Zn

2 and Zn

3 and repeat steps 1 and 2. Use the average

In the following two sections, we give error bounds for a procedure that generalizes Algorithm 1.
In
particular, the bounds apply to the problem of estimating some θ(u) ≡ E{f (Z) | U = u}, where U is some
observed subset of Z and f (Z) is not directly observable. The estimator is (cid:98)θ(u) = (cid:98)En{ (cid:98)f (Z) | U = u}, where
(cid:98)En(· | U = u) is either an empirical risk minimizer or a linear smoother and it is computed from a sample
independent of that used to construct (cid:98)f (·). One can see that Algorithm 1 ﬁts exactly this framework where
f (Z) = ϕ(Z). The additional sample split considered in Algorithm 1 is not needed to derive the next two
propositions but it is useful to derive the result in Lemma 1. Finally, both bounds on the risk will involve
a particular bias term (cid:98)r(u) that would need to be analyzed on a case-by-case basis
(cid:90)

(cid:98)r(u) =

(cid:98)f (z)dP(z | U = u) − θ0(u).

To estimate a dose-response curve, we have f (Z) = ϕ(Z) and we propose using (cid:98)ϕ(Z) as an estimator of
ϕ(Z), as detailed in Algorithm 1. Lemma 1 below shows that (cid:98)r(u) is second-order and doubly-robust.

2.2 Upper bound on the risk of the ERM-based estimator

We start by considering estimating θ(t) via empirical loss minimization as in Algorithm 1 (a). We view
this as a “global” method, as we estimate the function on its entire support as opposed to local methods,
such as the DR-Learner discussed next, whereby the dose-response is estimated at a speciﬁc point. The
error bound we describe in this section will be on the L2 loss and will be a specialization of the results
described in Foster and Syrgkanis [2019] and Wainwright [2019]. Foster and Syrgkanis [2019] provides a
general framework for doing empirical risk minimization in the presence of nuisance components that need
to be estimated. Here, we take their approach and ﬁnd that the oracle rate is achievable if E(cid:107)(cid:98)r(cid:107)2 is simply
of smaller order. In particular, from Lemma 3, if the orthogonal signal ϕ(Z) is used, (cid:98)r consists of a product
of errors, as opposed to simply being of second order, and thus the bound on the MSE of our procedure
improves upon the bound from Foster and Syrgkanis [2019].

The next proposition provides a bound on the error incurred by an estimator that uses an estimated
outcome (cid:98)f (Z) in place of the true (unobservable) outcome f (Z), when doing empirical risk minimization
with the square loss to estimate a regression function E{f (Z) | U = u}.

Proposition 1. Consider two independent samples, Dn = (Z01, . . . , Z0n) and Zn = (Z1, . . . , Zn), consist-
ing of n iid copies of some generic observation Z distributed according to P. Let U denote a generic variable
such that U ⊂ Z. Let θ0(u) ≡ E{f (Z) | U = u} and suppose (cid:98)f (·) is constructed using only observations in
Dn. Consider the estimator

(cid:98)θ ≡ arg min

θ∈Θ

1
n

n
(cid:88)

i=1

{ (cid:98)f (Zi) − θ(Ui)}2.

Let θ∗ = arg minθ∈Θ(cid:107)θ − θ0(cid:107) and Θ∗ = {θ − θ∗ : θ ∈ Θ}. Deﬁne the local Rademacher complexity:

Rn(Θ∗, δ) = E

(cid:40)

sup
g∈Θ∗:(cid:107)g(cid:107)≤δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:15)ig(Ui)

(cid:12)
(cid:41)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where (cid:15)1, . . . , (cid:15)n are iid Rademacher random variables, independent of the sample. Suppose Θ∗ is star-
shaped and S ≡ supz∈Z | (cid:98)f (z)| ∨ supθ∈Θ(cid:107)θ(cid:107)∞ is ﬁnite. Let δn be any solution to Rn(Θ∗, δ) ≤ δ2 that

7

satisﬁes

Then,

δ2
n

(cid:38) log log(n)
n

∨

1
2n

.

where (cid:107)f (cid:107)2 = (cid:82) f 2(z)dP(z).

E((cid:107)(cid:98)θ − θ0(cid:107)2) (cid:46) (cid:107)θ∗ − θ0(cid:107)2 + δ2

n + E((cid:107)(cid:98)r(cid:107)2)

The error bound from Proposition 1 takes the form of an oracle rate plus a term involving (cid:98)r, which is
controlled by Lemma 1 when f (Z) = ϕ(Z) and (cid:98)f (Z) = (cid:98)ϕ(Z).
The assumptions underlying Theorem 1 are rather mild. Appendix D in Foster and Syrgkanis [2019] and
Chapters 13 and 14 in Wainwright [2019] describe common classes of functions for which the theorem
applies, e.g., linear functions with constraints on the coeﬃcients, functions satisfying Sobolev-type con-
straints or Reproducing Kernel Hilbert spaces.
In order to apply Proposition 1, the class of functions
considered has to be star-shaped. A class is star-shaped around the origin if, for any g ∈ G and α ∈ [0, 1],
it is the case that αg ∈ G. Importantly, a convex set is star-shaped. If the star-shaped condition is not
met, the statement of the theorem would hold for δn deﬁned in terms of the star-hull of the function class.
The boundedness assumption on (cid:98)ϕ(Z) and Θ is used in various places in the proof, including in ensuring
that the square-loss is globally Lipschitz; we expect this assumption to hold when the observations are
bounded. Finally, the inequality involving δn should often be satisﬁed. For instance, δ2
n ≥ 1/(2n) as long
as Θ∗ contains the constant function θ(u) = 1.1

Example 1 (Orthogonal series, Examples 13.14 and 13.15 in Wainwright [2019]). Suppose θ(u) is α-times
diﬀerentiable with θ(α)(u) satisfying (cid:82) {θ(α)(u)}2dP(u) ≤ B for some constant B. Let {pj}∞
j=1 be an
orthonormal basis of L2(P), such as the sine / cosine basis (see Belloni et al. [2015] for a discussion on
diﬀerent basis choices). Consider estimating θ0 via ERM over the function class

Θ(k, b) =






θc(·) :

k
(cid:88)

j=1

pj(·)cj,

k
(cid:88)

j=1

c2
j ≤ 1, and |θc(·)| ≤ b






j=1 pj(u)c0j, we have θ∗(u) = (cid:80)k

Writing θ0(u) = (cid:80)∞
0j. It can be
shown that (cid:107)θ∗ − θ0(cid:107)2 ≤ k−2α. Furthermore, the function class Θ∗(k) = {θ − θ∗, θ ∈ Θ(k)} = Θ(k, 2b) is
(cid:46) k/n. Thus, Proposition 1 provides an upper
convex and thus star-shaped and can be shown to satisfy δ2
n
bound of the mean-square error of the order

j=1 pj(u)c0j and (cid:107)θ∗ − θ0(cid:107)2 = (cid:80)∞

j=k+1 c2

E((cid:107)(cid:98)θ − θ(cid:107)2) (cid:46) k−2α +

k
n

+ E((cid:107)(cid:98)r(cid:107)2)

√

If k is chosen optimally, i.e. k ∼ n1/(2α+1), Proposition 1 shows that the oracle rate is attained as long as
E((cid:107)(cid:98)r(cid:107)2) is of order O(n−2α/(2α+1)).

1To see this, suppose that, for the sake of contradiction, δn < 1/

2n. To start, because Θ∗ is star-shaped, we have

g(U ) = δn ∈ Θ∗ because θ(U ) = 1 ∈ Θ∗ and δn ∈ [0, 1]. Then, (cid:107)g(cid:107) = δn so that

Rn(Θ∗, δn) ≥ δnE

(cid:33)

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15)i

≥

δn√
2n

> δ2
n

where the second inequality is an application of the Khintchine inequality. This is a contradiction because δn satisﬁes
Rn(Θ∗, δn) ≤ δ2
n.

8

2.3 Upper bound on the risk of the linear smoothing-based estimator

In this section, we consider a DR-Learner-style estimator (cf. Van der Laan [2006] and Kennedy [2020]
for heterogeneous eﬀects of binary treatments). The DR-Learning framework proposed and analyzed in
Kennedy [2020] covers a broad class of second-stage estimators satisfying a stability condition, linear
smoothers being one example. Here, for simplicity, we consider the case where the second-stage estimator
in (cid:98)θ(t) is based on localized linear smoothing. As discussed in Example 2, regressing (cid:98)ϕ(Z) on A via local
polynomial regression represents an archetype of a localized DR-Learner. Kennedy et al. [2017] propose
using generic learners to regress the estimated pseudo-outcome (cid:98)ϕ(Z) on A but only analyze local linear
estimators. Thus, our next proposition is an extension to their work, in the spirit of analyzing more general
linear smoothers. Theorem 1 and Proposition 1 in Kennedy [2020] yield the following proposition.

Proposition 2. Consider two independent samples, Dn = (Z01, . . . , Z0n) and Zn = (Z1, . . . , Zn), consist-
ing of n iid copies of some generic observation Z distributed according to P. Let U denote a generic variable
such that U ⊂ Z. Let θ0(u) ≡ E{f (Z) | U = u} and suppose (cid:98)f (·) is constructed using only observations in
Dn. Consider the following estimator:

(cid:98)θ(t) = n−1

n
(cid:88)

i=1

Wi(t; An) (cid:98)f (Z)

Further suppose that the following regularity conditions hold:

• Minimum variance: var{f (Z) | U = u} ≥ c > 0 for all u ∈ U and some constant c;
• Consistency of nuisance estimators: supz| (cid:98)f (z) − f (z)|= oP(1);
• Localized weights: n−1 (cid:80)n

i=1|Wi| ≤ C, for some constant C, and there exists a neighborhood Nt

around U = t such that Wi(t; U n) = 0 if Ui (cid:54)∈ Nt.

Then, letting (cid:101)θ(t) = n−1 (cid:80)n

i=1 Wi(t; U n)f (Zi) denote the oracle estimator:

|(cid:98)θ(t) − θ0(t)| ≤

(cid:12)
(cid:12)
(cid:12)(cid:101)θ(t) − θ0(t)

(cid:12)
(cid:12)
(cid:12) + sup
u∈Nt

(cid:18)

E

|(cid:98)r(u)| + oP

(cid:20)(cid:110)

(cid:101)θ(t) − θ0(t)

(cid:111)2(cid:21)(cid:19)

.

As discussed in Kennedy [2020], the assumptions underlying Proposition 2 are easily satisﬁed for linear
smoothers of the local polynomial regression variety. In particular, the weights of the local polynomial
regression satisﬁes the assumptions (Tsybakov [2008], Lemma 1.3). This proposition follows from the
results contained in Kennedy [2020] that apply to general linear smoothers, e.g.
it does not require the
weights to be localized. We work with localized weights to simplify the analysis of the point-wise risk.

Example 2. Suppose θ0(t) ≡ E{f (Z) | A = t} belongs to a H¨older class of order α and let p = (cid:98)α(cid:99). A
DR-Learner can be based upon local polynomial regression of order p. The weights are

Wi(t; An) = s(t)T (cid:98)Q−1Kht(Ai)s(Ai)T ,

where K(·) is a kernel function, (cid:98)Q = Pn{s(A)s(A)T } and s(a) = (cid:2)1
calculation (see, for example, Tsybakov [2008]), yields that

a−t
h

. . . (cid:0) a−t
h

(cid:1)p(cid:3)T

. A standard

(cid:20)(cid:110)

(cid:101)θ(t) − θ0(t)

E

(cid:111)2(cid:21)

= O(n−2α/(2α+1))

This means that the oracle rate is attainable if supu∈Nt (cid:98)r2(u) = OP(n−2α/(2α+1)), which is essentially the
same requirement as for the estimator based on empirical-risk-minimization, see Example 1.

Remark 1. From the bound in Proposition 2, inference can be carried out in the oracle regime, i.e., under

9

the assumption that supu∈Nt|(cid:98)r(u)| is of smaller order that |(cid:101)θ(t) − θ(t)|.
In particular, if this holds, all
inference tools for standard local nonparametric regression can be used. For example, let the setup be
as in Example 2. Let σ2(t) be asymptotic variance of (cid:101)θ(t), (cid:98)σ2(t) its consistent estimator and b(t) the
asymptotic bias. Then, if supu∈Nt (cid:98)r2(u) = oP((nh)−1/2), we have

√

nh[(cid:98)θ(t) − θ(t) − b(t)]
(cid:98)σ(t)

(cid:32) N (0, 1)

as shown, for instance, in Section 4 of Fan and Gijbels [2018]. Notice that, without undersmoothing or
bias-correction, a Wald-type conﬁdence interval based on the asymptotic statement above will cover the
smoothed dose-response curve E{(cid:101)θ(t)}, rather than θ(t) itself (see Section 5.7 in Wasserman [2006] for
more discussion).

2.4 Bounding the conditional bias of (cid:98)ϕ(Z)
As outlined in Propositions 1 and 2, the analysis of the estimator based on empirical risk minimization and
that of the one based on linear smoothing yield a bound on the MSE that is the oracle rate plus a term of
the order of (cid:98)r2(t). We show that (cid:98)r(t) for (cid:98)f (Z) = (cid:98)ϕ(Z) is second-order, as outlined in the following lemma.
Lemma 1. Let (cid:98)r(t) = E{ (cid:98)ϕ(Z) | A = t, Dn} − θ0(t). It holds that

|(cid:98)r(t)| (cid:46) (cid:107)w − (cid:98)w(cid:107)t(cid:107)µ − (cid:98)µ(cid:107)t + |(Pn − P){(cid:98)µ(t, X)}|
t = (cid:82) f 2(z)dP(z | A = t) and Pn denotes an average over observations in sample Zn
2 .

where (cid:107)f (cid:107)2

Proof. Recall that θ0(t) = E{ϕ(Z) | A = t}. By Bayes’ rule, we have

P{(cid:98)µ(t, X)} − θ(t) =

(cid:90)

w(t, x){(cid:98)µ(t, x) − µ(t, x)}dP(x | A = t)

and

E{ (cid:98)ϕ(Z) | A = t, Dn} =

(cid:90)

(cid:98)w(t, x){µ(t, x) − (cid:98)µ(t, x)}dP(x | A = t) + Pn{(cid:98)µ(t, X)}

Adding and subtracting P{(cid:98)µ(t, X)} and applying Cauchy-Schwarz yield the result.

The result from Lemma 1 shows that |(cid:98)r(t)| can be bounded by the product of the L2 errors in estimating
w(a, x) and µ(a, x) plus a centered sample average, which would generally be of the smaller order OP(n−1/2)
if, for instance, the second moment of (cid:98)µ(t, X) (conditional on Zn
1 ) is bounded. In this respect, this term
is eﬀectively asymptotically negligible in nonparametric models where the rate of convergence is of slower
order than n−1/2. Thus, the conditional bias of (cid:98)ϕ(Z) is driven by the product of the errors incurred
in estimating the nuisance functions; this product structure of the bias is important when the nuisance
functions are estimated at diﬀerent rates.

Standard results are generally calculated for L2(dP(a, x)) errors deﬁned by the joint distribution of (A, X),
for example

(cid:90)

(cid:107)w − (cid:98)w(cid:107)2 ≡

{w(a, x) − (cid:98)w(a, x)}2dP(a, x).

In this case, optimal convergence rates for estimating µ(a, x) are well-understood for many classes. For
instance,
if µ(a, x) belongs to a H¨older class of order γ, then minimax-optimal convergence rates in
L2(dP(a, x)) are of order n−2γ/(2γ+d+1). The bound from Lemma 1 is actually on an L2 error with weight
given by the conditional density of X given A = t. In most settings, we expect the more conventional rate

10

based on L2(dP(a, x)) to match that based on L2(dP(x | A = t)). For example, Result 1 from Colangelo
and Lee [2020] shows that the rate in L2(dP(x | A = t)p(t)) matches that for the point-wise risk (in
(A, X)) under a mild boundeness assumption. Alternatively, we note that one can always upper bound
(up to constants) (cid:107)f (cid:107)t by the supremum norm (cid:107)f (cid:107)∞ and the rate for estimating a regression function in
L∞ generally matches that for estimating the function in L2(dP(a, x)) up to log factors.

There are fewer results available for conditional density estimation compared to regression estimation.
Recently Ai et al. [2018] have proposed a method to estimate w(a, x) directly that, under certain conditions,
exhibits a convergence rate in L2 of order n−2γ/(2γ+d+1) if w(a, x) is γ-smooth (see their Theorem 3).
Alternatively, one can estimate p(a) and π(a | x) and compute their ratio to estimate w(a, x). We refer to
Colangelo and Lee [2020] for a discussion on ways to estimate π(t | x). In particular, one approach is to
estimate E{Gh1t(A) | X = x}, where G(u) is some kernel and h1 some bandwidth of choice. As a third
approach, because w(a, x) = p(a)p(x)/p(a, x), one can estimate the marginals p(x) and p(a) and the joint
density p(a, x) and take the ratio as an estimate of w(a, x). Estimating a joint density of d variables that
belongs to a H¨older-class of order γ can be done with error scaling as n−2γ/(2γ+d). Thus, the MSE of this
ratio would trivially be upper bounded by the MSEs for estimating p(a, x), p(x) and p(x), which would
depend on their respective smoothness levels.

As investigated in more detail in the next section, an interesting setting is where a (cid:55)→ µ(a, x) has a
diﬀerent smoothness level than x (cid:55)→ µ(a, x), where we expect the former to match the smoothness of
the dose-response θ(a) in many applications. This is an example of anisotropic regression. The optimal
rate for estimating a d-dimensional regression in a H¨older class where each coordinate has its own level of
smoothness γj is of order n−2γ/(1+2γ), where γ satisﬁes γ−1 = (cid:80)d
[Hoﬀman and Lepski, 2002; Bertin,
2004]. If µ(a, x) is in an anisotropic H¨older class of order (α, b, . . . , b), the rate simpliﬁes to n−2b/(2b+b/α+d),
where d = dim(X). If the treatment A is categorical or α is much larger than b, the rate is essentially
n−2b/(2b+d), i.e. the optimal rate for estimating a d-dimensional regression function that is b-smooth. In
a similar fashion, we may think of a (cid:55)→ π(a | x) and x (cid:55)→ π(a | x) as having diﬀerent smoothness levels;
optimal convergence rates in this context typically depend too on the harmonic means of the smoothness
levels of each coordinate [Efromovich, 2007].

j=1 γ−1
j

Remark 2. Suppose the dose-response θ(a) belongs to a H¨older class of order α and that w and µ are
s-smooth so that they can be estimated in L2 at the rate n−2s/(2s+d+1). Estimators whose risk is of the
form “oracle rate + a term of the same order as (cid:98)r” would behave like an oracle estimator that has access
to the true nuisance functions as soon as s ≥ (d + 1)/{2(1 + 1/α)}. We will show that this oracle eﬃciency
bar can be lowered, under certain conditions, by a higher order estimator. See Remark 7.

Remark 3. We note that our discussion on the rates attained by the doubly-robust estimators discussed in
Section 2 is driven by the bound computed in Lemma 1. If (cid:98)µ and (cid:98)w are designed to optimally estimate µ
and w, e.g. by selecting tuning parameters to minimize estimates of their MSEs, then generally the bound
based on Cauchy-Schwarz is the best available. However, there are other techniques, such as particular
forms of sample splitting coupled with undersmoothing, whereby the nuisance functions are estimated
optimally with respect to the target of inference, and so the selected tuning parameters for the nuisance
estimators may not minimize the MSEs with respect to the nuisance functions. This approach has favorable
theoretical properties, see e.g. Kennedy [2020], although it can be challenging to implement in practice.
We leave studying undersmoothing in the context of continuous treatments for future work.

Remark 4. Compared to Theorem 2 in Kennedy et al. [2017], Theorem 2 and Lemma 1 provide the
same error bound, but under substantially weaker conditions. Sample-splitting circumvents the need to
impose Donsker-type conditions on the nuisance functions’ classes in the form of bounded uniform entropy
integrals. Moreover, the use of local polynomial regression allows the estimator to track the smoothness of
θ(a), thereby achieving the oracle rate in high smoothness regimes (provided that the remainder term is
negligible).

11

3 Higher-order estimators

3.1 Preliminaries

Inspired by the seminal work of Robins et al. [2008, 2009a, 2017], in this section we investigate the use
of higher-order inﬂuence functions (HOIFs) to estimate continuous treatment eﬀects. To the best of our
knowledge, this is the ﬁrst time HOIFs are used in this context. For an introduction to higher-order
inﬂuence functions, we refer to the main papers [Robins et al., 2009a, 2017] and give a brief overview here.
Informally, an mth-order estimator of a functional χ(p) (where p is the density of the observations) takes
the form

(cid:98)χ(p) = χ((cid:98)p) +

m
(cid:88)

j=1

Un{ (cid:98)ϕj(Z1, . . . , Zj)}

(3)

where Un is the U -statistic measure so that

Un{ϕj(Z1, . . . , Zj)} =

1
n(n − 1) · · · (n − j + 1)

(cid:88)

ϕj(Zi1, . . . , Zij).

1≤i1(cid:54)=i2...(cid:54)=ij ≤n

Letting Pj{f (Z1, . . . , Zj)} = (cid:82) f (z1, . . . , zj)dP(z1) . . . dP(zj) denote the corresponding population measure,
this implies an expansion:

(cid:98)χ(p) − χ(p) = χ((cid:98)p) − χ(p) +

m
(cid:88)

j=1

Pj{ (cid:98)ϕj(Z1, . . . , Zj)} +

m
(cid:88)

j=1

(Un − Pj){ (cid:98)ϕj(Z1, . . . , Zj)}

Following Robins et al. [2009a], van der Vaart [2014], Robins et al. [2017], if ϕj is chosen such that
−Pj{ (cid:98)ϕj(Z1, . . . , Zj)} acts as the jth-order term in the functional Taylor expansion of χ((cid:98)p) − χ(p), then

χ((cid:98)p) − χ(p) +

m
(cid:88)

j=1

Pj{ (cid:98)ϕj(Z1, . . . , Zj)} = O(d(p − (cid:98)p)m+1)

for some distance d(·). The quantity ϕj is referred to as the jth-order inﬂuence function of χ(p). Provided
that





m
(cid:88)

(Un − Pj){ (cid:98)ϕj(Z1, . . . , Zj)}


 = O(n−1),

var

j=1

this calculation would suggest that (cid:98)χ(p) would always be root-n consistent if m is large enough. However,
higher order inﬂuence functions do not exist for many functionals of interest, including the average treat-
ment eﬀect of a binary treatment. In our setting, the dose-response does not possess inﬂuence functions of
any order, in nonparametric models. While this means that generally it is not possible to construct root-n
consistent estimators, we will show that estimators of the form (3) that employ approximate inﬂuence
functions still enjoy favorable properties. The performance of the resulting estimators will be based on a
careful bias-variance trade-oﬀ. We show that an mth-order estimator of the dose-response can outperform
the doubly-robust estimators from Section 2 under certain smoothness conditions. Our estimator is tai-
lored to models where a (cid:55)→ µ(a, x) and a (cid:55)→ π(a | x) are α-times and β-times continuously diﬀerentiable,
respectively. However, our analysis suggests that this estimator can outperform the current state-of-the-art
only when α ≤ β.

12

3.2 Notation

Before describing our mth-order estimator of the dose-response, we need to introduce some notation. Let
Kht(a) denote a kernel of order l = (cid:98)α∧β(cid:99) and b(x) denote a vector of the ﬁrst k terms of some orthonormal
basis. Deﬁne

where, for g(x) = (cid:82) Kht(a)p(a, x)da:

Πi,j ≡ Π(xi, xj) = b(xi)T Ω−1b(xj)

(cid:90)

Ω =

b(x)b(x)T g(x)dx

Thus, provided that g(x) is positive and bounded away from zero and inﬁnity, Πi,j is eﬀectively the kernel
of an orthogonal projection in L2(g) onto a k-dimensional subspace. That is, for some function f (x),
(cid:82) Π(xi, x)f (x)g(x)dx = b(xi)T β∗, where β∗ solves the minimization problem

(cid:90)

β∗ = arg min

β∈Rk

(cid:8)f (x) − b(x)T β(cid:9)2

g(x)dx

The kernel Π(xi, xj) has to be estimated in practice because g(x) depends on the true density p(a, x).
When X is multivariate, the basis can be taken to be the tensor product basis. Following Robins et al.
[2017], by a slight abuse of notation, we will denote the projection operator associated with the kernel
above using the same symbol Π. This way, we have Π(f )(xi) = (cid:82) Π(xi, x)f (x)g(x)dx.
Example 3. Suppose Xi ∈ [ai, bi] for i ∈ {1, 2}, i.e., X ∈ X ⊂ R2. Let (cid:101)b(u) be a k-dim vector of terms
from an orthonormal basis in L2 over the interval [−1, 1]. We may construct a generic element bu(x1, x2)
of b(x1, x2) as

bu(x1, x2) =

4
(b1 − a1)(b2 − a2)(cid:112)g(x)

(cid:101)bl

(cid:18) 2x1 − a1 − b1
b1 − a1

(cid:19)

(cid:101)bm

(cid:18) 2x2 − a2 − b2
b2 − a2

(cid:19)

where l and m range over {1, . . . , k}. By a change of variables, it can be seen that bu(a, x) is orthonormal
in L2(g) so that the kernel Π(x1i, x2i, x1j, x2j) simpliﬁes to

Π(x1i, x2i, x1j, x2j) = b(x1i, x2i)T b(x1j, x2j).

3.3 The estimator
In this section, we describe an estimator of θ(t) = (cid:82) µ(t, x)p(x)dx based on approximate, mth-order HOIFs.
Deﬁne the ﬁrst approximate inﬂuence function:

and the functions

f0(Z) =

Kht(A){Y − µ(t, X)}
π(t | X)

+ µ(t, X)

f1(Z) = Kht(A){Y − µ(A, X)}

f2(Z) =

Kht(A)
π(A | X)

− 1

The function f0(Z) is a sum of a residual term involving Y − µ(t, X) and the outcome model µ(t, X). If A
was binary and Kht(A) = A, f0(Z) would be exactly the inﬂuence function of (cid:82) µ(1, x)dP(x), which equals
E(Y 1) under standard causal assumptions. The terms f1(Z) and f2(Z) are kernel-weighted residuals; f2(Z)

13

is a residual term in the sense that E{Kht(A)π(A | X) | X} = 1 whenever (cid:82) Kht(a)da = 1.
The mth-order estimator of θ(t) that we study is

(cid:98)θ(t) = Pn{ (cid:98)f0(Z)} +

m
(cid:88)

j=2

Un{ (cid:98)ϕj(Z1, . . . , Zj)}

where

ϕj(Z1, . . . , Zj) = (−1)j−1 (cid:88)

(−1)j−|A|E (cid:8)ϕj(Z1, . . . , Zj) | Zi, i ∈ A(cid:9)

A⊂{1,...,j}

ϕj(Z1, . . . , Zj) = f1(Z1)Π1,2Kht(A2) · · · Πj−2,j−1Kht(Aj−1)Πj−1,jf2(Zj)

are the mth-order approximate inﬂuence functions. Notice that ϕj(Z1, . . . , Zj) is simply the degenerate
version of ϕj(Z1, . . . , Zj), which ensures that

(cid:90)

ϕj(z1, . . . , zj)dP(zi) = 0

for every i and (zl : l (cid:54)= i). In addition, it holds that

(cid:90)

Π(xi−1, xi)Kht(ai)Π(xi, xi+1)dP(zi) = b(xi−1)T Ω−1

(cid:90)

b(xi)b(xi)T Kht(ai)dP(zi)Ω−1b(xi+1)

and, by degeneracy of f1(z) and f2(z), (cid:82) f1(z1)Π(x1, x2)dP(z1) = (cid:82) Π(xj−1, xj)f2(zj)dP(zj) = 0. This
means that the ﬁrst few approximate HOIFs take a rather simple form:

= Π(xi−1, xi+1)

ϕ2(Z1, Z2) = −f1(Z1)Π1,2f2(Z2)
ϕ3(Z1, Z2, Z3) = f1(Z1)Π1,2Kht(A2)Π2,3f2(Z3) − f1(Z1)Π1,3f2(Z3)
ϕ4(Z1, Z2, Z3, Z4) = −f1(Z1)Π1,2Kht(A2)Π2,3Kht(A3)Π3,4f2(Z4)

+ f1(Z1)Π1,2Kht(A2)Π2,4f2(Z4) + f1(Z1)Π1,3Kht(A3)Π3,4f2(Z4)
− f1(Z1)Π1,4f2(Z4)

Remark 5. The estimator (cid:98)θ(t) = Pn{ (cid:98)f0(Z)}, corresponding to m = 1, is precisely the estimator studied in
Colangelo and Lee [2020]. Thus, we may view the mth-order estimator as a higher-order generalization of
their approach.

Remark 6. The mth-order estimator that we study has the same form as the mth-order estimator of the
functional ψ = (cid:82) E(Y | A = 1, X = x)p(x)dx studied in Robins et al. [2017] (Section 8) except that
terms of the form Af (Z) for some function f of the observations are replaced by Kht(A)f (Z). In fact,
the rate described in Theorem 1 is similar to that for ψ from Theorem 8.1 in Robins et al. [2017] with n
replaced by nh. Finally, Section 9 in Robins et al. [2017] presents an estimator that is a modiﬁed version of
that presented in Section 8.1 where certain terms in the inﬂuence functions are “cut out” to decrease the
variances without increasing the bias. This results in a more complex estimator that exhibits a better, and
in fact minimax optimal under certain conditions, bias-variance trade-oﬀ. We plan to apply this reﬁnement
to the dose-response settings in future work, with the idea of ﬁrst calculating a candidate minimax lower
bound.

We propose estimating all nuisance functions, namely π(a | x), µ(a, x) and g(x) using a separate indepen-
dent sample Dn. Notice that Π(xi, xj) can be estimated by b(xi)T (cid:98)Ω−1b(xj), where (cid:98)Ω is a suitable esti-

14

mator of (cid:82) b(x)b(x)T g(x)dx. The weight g(x) = (cid:82) Kht(a)p(a, x)da can be estimated as (cid:82) Kht(a)(cid:98)p(a, x)da.
However, for k suﬃciently small, an attractive alternative is to use the empirical version of Ω, namely
(cid:98)Ω = Pn{b(X)b(X)T Kht(A)}. See also Mukherjee et al. [2017] for an in-depth discussion of using the
empirical counterpart of Ω for estimators based on higher-order inﬂuence functions.

3.4 Upper bound on the (conditional) risk

Here, we bound the risk of the estimator (cid:98)θ(t) conditional on the training sample Dn.

Theorem 1. Suppose Assumptions 1-2 hold and the following assumptions also hold:

1. The functions a (cid:55)→ µ(a, x) and a (cid:55)→ π(a | x) are α-times and β-times continuously diﬀerentiable with

uniformly bounded derivatives, for any x ∈ X ;

2. The kernel K of order l = α∧β is uniformly bounded, supported in [−1, 1] and satisﬁes (cid:82) K(u)du = 1

and (cid:82) Kht(a)p(a, x)da ∈ [(cid:15), M ] for some (cid:15) > 0, M < ∞ and all x ∈ X .

3. The orthogonal projection kernel Π and its estimator (cid:98)Π satisfy supx Π(x, x) (cid:46) k and supx (cid:98)Π(x, x) (cid:46) k;
4. Boundedness: (cid:82) Kht(a)p(a, x)da/(cid:82) Kht(a)(cid:98)p(a, x)da ∈ [(cid:15)(cid:48), M (cid:48)] for some (cid:15)(cid:48) > 0, M (cid:48) < ∞ and all

x ∈ X ; similarly the density of X is uniformly bounded.

Then

(cid:12)
(cid:12)
(cid:12)

E{(cid:98)θ(t) − θ(t) | Dn}
m
(cid:88)

var{(cid:98)θ(t) | Dn} (cid:46)

(cid:12)
(cid:12)
(cid:12)

j=1

(cid:46) (cid:107)(I − Π)(v)(cid:107)g(cid:107)(I − Π)(q)(cid:107)g + hα∧β + (cid:107)q(cid:107)g(cid:107)v(cid:107)g(cid:107)f (cid:107)m−1

∞

kj−1h−j
n(n − 1) · · · (n − j + 1)

where v(x) = µ(t, x) − (cid:98)µ(t, x), q(x) = 1/(cid:98)π(t | x) − 1/π(t | x) and f (x) = (cid:98)p(t, x) − p(t, x), and (cid:107)f (cid:107)2
(cid:82) f 2(x)g(x)dx.

g =

The assumptions underlying Theorem 1 are similar to those made in Propositions 1 and 2. The main
diﬀerence is that the higher order estimator (cid:98)θ(t) is speciﬁcally designed for nonparametric models where
a (cid:55)→ µ(a, x) and a (cid:55)→ π(a | x) possess some smoothness, which we encode in condition 1. The second
condition ensures that the kernel K accurately tracks the least smooth function between a (cid:55)→ µ(a, x) and
a (cid:55)→ π(a | x). A better estimator or a tighter bound would track just the smoothness of θ(a) or, at least,
the smoothness of a (cid:55)→ µ(a, x), as that should match the smoothness of θ(a) in most applications. We
leave this for future work. In particular, we conjecture it might be possible to derive a tighter bound that
would have, in place of the term hα∧β, terms of order hα∧(β+1) plus terms of order hα∧β((cid:107)v(cid:107) + (cid:107)q(cid:107) + o(h)).
This reﬁned bound would also not track the smoothness of the dose-response and thus we preferred the
simpler and more interpretable bound in terms of hα∧β.

Because the higher order kernels can take negative values on sets of non-zero Lebesgue measure (see, e.g.
Proposition 1.3 in Tsybakov [2008]), we require g(x) = (cid:82) Kht(a)p(a, x)da to be bounded away from zero
since this is the weight used in the projection Π onto the ﬁnite space of dimension k. Condition 3 requires
the kernels Π and (cid:98)Π to be bounded on the diagonal. This would be satisﬁed, for instance, if the basis
elements are bounded. Condition 4 is a mild regularity condition on the estimator (cid:98)p(a, x).
We now discuss a few implications of Theorem 1, under the assumptions that 1) α ≤ β, i.e. a (cid:55)→ π(a | x)
is smoother than a (cid:55)→ µ(a, x), and 2) the dose-response is also α-smooth.

Remark 7. In order to understand the implications of Theorem 1, we consider the case where x (cid:55)→ (cid:98)µ(t, x)
and x (cid:55)→ µ(t, x) are H¨older-γ1 and x (cid:55)→ (cid:98)π(t | x) and x (cid:55)→ π(t | x) are H¨older-γ2. Given an appropriate

15

basis, suppose the approximation error satisﬁes

(cid:107)(I − Π)(v)(cid:107)g(cid:107)(I − Π)(q)(cid:107)g (cid:46) k−(γ1+γ2)/d.

Each term in the variance bound contributes a term of order kj−1/(nh)j. Therefore, if we choose k ∼ nh
the variance is of order (nh)−1. With this choice of k, the third term in the bias (cid:107)q(cid:107)g(cid:107)v(cid:107)g(cid:107)f (cid:107)m−1
∞ can be
made arbitrarily small by choosing m large enough and thus it is negligible relative to the other terms.
The bound on the MSE (conditional on Dn) of (cid:98)θ(t) is thus O(k−2(γ1+γ2)/d + h2α + (nh)−1).

This means that, if the average nuisance functions’ smoothness satisﬁes (γ1 + γ2)/2 ≥ d/4 and h ∼
n−1/(2α+1), one obtains the rate n−2α/(2α+1). Thus, (cid:98)θ(t) behaves like the oracle estimator that uses the
true nuisance regression functions if α ≤ β and (γ1 + γ2)/2 ≥ d/4, provided that m is chosen large enough.
If s = γ1 = γ2, this means that (cid:98)θ(t) is oracle eﬃcient for s ≥ d/4. To the best of our knowledge, no existing
estimator of the dose-response is oracle-eﬃcient in this regime.

In order to compare this result to that from Remark 2, consider the case where the error in estimating the
nuisance functions is entirely driven by that in estimating x (cid:55)→ µ(t, x) and x (cid:55)→ π(t | x). This would be
the case, for example, if A is categorical. Then, for s = γ1 = γ2, |(cid:98)r(a)| (cid:46) n−2s/(2s+d) and the estimators
from Section 2 are oracle-eﬃcient only in the regime s ≥ d/{2(1 + 1/α)}. Thus, higher-order corrections,
at least in the case where α ≤ β, eﬀectively lower the bar for oracle eﬃciency.

Remark 8. Suppose we use HOIFs of order m = 2, the nuisance functions’ smoothness satisﬁes (γ1+γ2)/2 <
d/4 and w(a, x) = p(a)/π(a | x) and 1/π(a | x) are estimable at the same rate in L2. In this regime, the
estimators from Section 2 are not oracle eﬃcient, so the rate is driven by (cid:98)r. Without further corrections,
(cid:98)r is bounded by the product of the MSEs for estimating w and µ, which is of bigger order than the
term (cid:107)v(cid:107)(cid:107)q(cid:107)(cid:107)f (cid:107)∞, which is of the same order as (cid:107)v(cid:107)g(cid:107)q(cid:107)g(cid:107)f (cid:107)∞ because (cid:82) Kht(a)π(a | x)da is uniformly
bounded. Suppose k and h are chosen optimally and so are of orders

k ∼ (nh)2d/(d+2γ1+2γ2)

and

h ∼ n−2(γ1+γ2)/[α{2(γ1+γ2)+d}+2(γ1+γ2)].

Then, the MSE of (cid:98)θ(t) is of order n−2r2 for

(cid:26)

r2 =

1 +

d
2(γ1 + γ2)

+

1
α

(cid:27)−1

∧ (cid:107)v(cid:107)g(cid:107)q(cid:107)g(cid:107)f (cid:107)∞

Thus, if the ﬁrst term in r2 dominates the rate, then the rate obtained by the quadratic estimator (cid:98)θ(t)
is a combination of the oracle rate 1/(2 + 1/α) and the minimax rate for estimating the dose-response
when A is categorical (i.e. some average treatment eﬀect) in the non-root-n regime, namely n−2rf , for
rf = [1 + d/{2(γ1 + γ2)}]−1, which is recovered as α → ∞.

In Figure 1, we illustrate the rates obtained in this work as a function of s = γ1 = γ2. Here s refers to the
smoothness of x (cid:55)→ µ(a, x) and x (cid:55)→ π(a | x). For illustration, we set α = β = 2 and dim(X) = 20, where
α is the smoothness of a (cid:55)→ µ(a, x) and a (cid:55)→ π(a | x). In this setting, the optimal rate for estimating the
anisotropic functions µ(a, x) and π(a | x) is n−2s/(2s+s/α+d). This is also the rate inherited by the plug-
in estimator (black line) Pn{(cid:98)µ(a, X)}, without further corrections. The oracle rate is n−2α/(2α+1). The
DR-Learner and the EMR-based estimator (red line) achieve a rate of order n−2α/(2α+1) ∨ n4s/(2s+s/a+d).
The blue line refers to the rate obtainable by the quadratic (m = 2) estimator under the assumption
that the covariates density is estimated well enough so that the term (cid:107)v(cid:107)g(cid:107)q(cid:107)g(cid:107)f (cid:107)∞ is negligible, which
is n−2/{1+d/(4s)+1/α} ∨ n−2α/(2α+1); see Remark 8. Finally, as a reference value, we also plot the minimax
lower bound for estimating the ATE, which is of order n−2/{1+d/(4s)} ∨ n−1 [Robins et al., 2009b].

For smooth functionals possessing a ﬁrst-order inﬂuence function, eﬃcient estimators based on the inﬂuence
function are asymptotically equivalent. For instance, corrected plug-in estimators and TMLE may be

16

Figure 1: Illustration of the convergence rates in MSE for the estimator considered in this article, as a
function of the smoothness s = γ1 = γ2. We take the smoothness of the dose-response to be α = 2 and
dim(X) = 20.

In contrast, for functionals like the dose-
diﬀerent in ﬁnite samples but are asymptotically equivalent.
response θ(t), which do not possess inﬂuence functions of any order, it is not clear whether estimators
based on diﬀerent approximations of the inﬂuence functions are equivalent asymptotically. This is true for
higher order corrections as well, particularly for the choice of the projection kernel Π. For example, Π could
be taken to represent a projection in L2((cid:82) Kht(a)p(a, x)da), as we have done in this work, or in L2(p(t, x)).
Using projections in L2(p(t, x)) would avoid the assumption that (cid:82) Kht(a)p(a, x)da is positive and bounded
away from zero, but at the expense of complicating the proof of the theorem, since the arguments made
in the proof of Theorem 8.1 in Robins et al. [2017] would need to adjusted to deal with issues such as
(cid:82) Π(xi−1, xi)Kht(ai)Π(xi, xi+1)dP(zi) (cid:54)= Π(xi−1, xi+1). Similarly, one may consider replacing Kht(a) with
the weight function of a local polynomial regression. That is, replacing Kht(a) with s(t)T Q−1 (cid:101)Kht(a)s(a),
where s(a) = (cid:2)1 (a − t)
(cid:101)Kht(a)s(a)s(a)T p(a)da and (cid:101)K(u) is a standard second-
order kernel, such as the Epanechnikov. An approach conceptually similar to the DR-Learner may use
projections in L2(p(x | t)), although this may require a diﬀerent analysis than what used to prove Theorem
1. Exploring the diﬀerences between these approaches is an important avenue for future work.

(a − t)l(cid:3)T , Q = (cid:82)

· · ·

4 Sensitivity analysis to the no-unmeasured-confounding assumption

In this section, we brieﬂy outline a simple pseudo-outcome regression method to carry out ﬂexible, non-
parametric sensitivity analysis to the no-unmeasured-confounding assumption, i.e., when Y a (cid:54)⊥⊥ A | X so
that (cid:82) µ(t, x)dP(x) can no longer be interpreted as the dose-response curve. To the best of our knowledge,
this is the ﬁrst nonparametric sensitivity analysis method for continuous treatment eﬀects. Bonvini et al.
[2022] propose an extension to Rosenbaum’s sensitivity model for binary treatments as follows. Let U
be such that Y a ⊥⊥ A | (X, U ) and recall that E(Y a) = E{Y p(a)/π(a | X, U ) | A = a}. Let γ ≥ 1 be
a user-speciﬁed sensitivity parameter. Departures from the no-unmeasured-confounding assumption are

17

−1.0−0.8−0.6−0.4−0.20.0nuisance smoothness sk in n^kPlug−inERM / DRQuadraticOracleATE minimax bd0246810parametrized by considering all densities of A given (X, U ), π(a | x, u), in the class

(cid:26)

Π(γ) =

π(a | x, u) :

1
γ

≤

π(a | x, u)
π(a | x)

(cid:27)

≤ γ

When γ = 1, corresponding to the case when the measured covariates are suﬃcient to characterize the
treatment selection process, one has the usual identiﬁcation formula

E(Y a) = E{w(a, X)Y | A = a} =

(cid:90)

µ(a, x)dP(x).

Lemma 2 in Bonvini et al. [2022] shows that valid bounds on E(Y a) under the sensitivity model Π(γ) are

(cid:90)

(cid:90)

θl(t; γ) =

θu(t; γ) =

E[Y γsgn{ql(t,x)−Y } | A = t, X = x]dP(x)

E[Y γsgn{Y −qu(t,x)} | A = t, X = x]dP(x)

where ql(A, X) (resp. qu(A, X)) is the 1/(1 + γ) (resp. γ/(1 + γ))-quantile of Y given (A, X). In other
words, for a given, user-speciﬁed γ, if π(a | x, u) ∈ Π(γ), then θl(a; γ) ≤ E(Y a) ≤ θu(a; γ).

A DR-Learner estimator of the bounds above can be computed by appropriately modifying the original
pseudo-outcome ϕ(Z) = w(A, X){Y − µ(A, X)} + (cid:82) µ(A, x)dP(x) and regressing it onto A. For j = {l, u},
deﬁne

ϕj(Z; γ) ≡ ϕj(Z; w, κj, qj, γ) = w(A, X){sj(Z; qj) − κj(A, X; qj)} +

(cid:90)

κj(A, x; qj)dP(x), for

sl(Z; ql) = ql(A, X) + {Y − ql(A, X)}γsgn{ql(A,X)−Y }
su(Z; qu) = qu(A, X) + {Y − qu(A, X)}γsgn{Y −qu(A,X)}
κj(A, X; qj) = E{sj(Z; qj) | A, X}

Following the sample splitting scheme whereby all nuisance functions are estimated on a separate, in-
dependent sample Dn, a DR-Learner estimator of θj(t; γ) regresses an estimate of ϕj(Z; γ) onto A on
the test set. For example, if the second stage regression is done via linear smoothing, then (cid:98)θj(t; γ) =
n−1 (cid:80)n
which is a pathwise-diﬀerentiable parameter. Furthermore, ϕl(Z; 1) = ϕu(Z; 1) = ϕ(Z).

i=1 Wi(t; Ai) (cid:98)ϕj(Z; γ). It can be shown that ϕj(Z) is just part of the inﬂuence function of (cid:82) θj(a; γ)dP(a),

The error analysis of the DR-Learners (cid:98)θl(t; γ) and (cid:98)θu(t; γ) follows from Propositions 1 and 2. In this light,
it only remains to calculate E{ (cid:98)ϕj(Z; γ) − ϕj(Z; γ) | A = t, Dn}. We do so in the following lemma, proved
in Appendix D.1, which plays the role of Lemma 1 in the no-unmeasured-confounding case.
Lemma 2. Let (cid:98)rj(t) = E{ (cid:98)ϕj(Z; γ) − ϕj(Z; γ) | A = t, Dn}. It holds that

|(cid:98)rj(t)| (cid:46) (cid:107)w − (cid:98)w(cid:107)t(cid:107)κj − (cid:98)κj(cid:107)t+(cid:107)qj − (cid:98)qj(cid:107)2

t + |(Pn − P)(cid:98)κj(t, X; (cid:98)qj)|

The result of Lemma 2 is similar to that of Lemma 1, except that the upper bound on the conditional bias
involve the additional term (cid:107)qj − (cid:98)qj(cid:107)2
t . Thus, consistent estimation of the bounds relies on the consistency
of the conditional quantiles estimators. The centered empirical average term is of order OP(n−1/2), under
mild boundedness conditions, and thus negligible in nonparametric models for which the convergence rate
is slower than n−1/2.

We conclude this section by establishing that ϕj(Z; γ) satisﬁes the doubly-valid structure discovered by
Dorn et al. [2021] in a similar sensitivity model for binary treatments. In particular, the bounds remain
valid even if the conditional quantiles are not correctly speciﬁed. While Dorn et al. [2021] focused on

18

binary treatments, their observation extends to the continuous treatment case as well, as summarized in
the following proposition.

Proposition 3. Let w, κl, κu, ql and qu be some ﬁxed-functions such that all the expectations below are
well deﬁned. If either κj = κj(a, x; qj) or w(a, x) = w(a, x), but not necessarily both, then

E{ϕl(Z; w, κl, ql, γ) | A = t} ≤ θl(t; γ) ≤ θu(t; γ) ≤ E{ϕu(Z; w, κu, qu, γ) | A = t}

Proof. If either κj = κj(a, x; qj) or w(a, x) = w(a, x), then

E{ϕl(Z; w, κl, ql, γ) | A = t} =

(cid:90) (cid:16)

(cid:17)
ql(t, x) + E[{Y − ql(A, X)}γsgn{ql(A,X)−Y } | A = t, X = x]

dP(x)

E{ϕu(Z; w, κu, qu, γ) | A = t} =

(cid:90) (cid:16)

(cid:17)
qu(t, x) + E[{Y − qu(A, X)}γsgn{Y −qu(A,X)} | A = t, X = x]

dP(x)

The result follows because it holds that

(cid:104)
γsgn{ql(A,X)−Y } | A, X

(cid:105)

E

= E

(cid:104)

γsgn{Y −qu(A,X)} | A, X

(cid:105)

= 1

and, deterministically, that

{Y − ql(A, X)}γsgn{ql(A,X)−Y } ≤ {Y − ql(A, X)}γsgn{ql(A,X)−Y }
{Y − qu(A, X)}γsgn{Y −qu(A,X)} ≥ {Y − qu(A, X)}γsgn{Y −qu(A,X)}

Proposition 3 establishes the doubly-valid structure of ϕl(Z; γ) and ϕu(Z; γ). Just like in the sensitivity
model studied by Dorn et al. [2021] for binary treatments, the bounds on E(Y a) remain valid even if the
conditional quantiles are not correctly speciﬁed as long as either w(a, x) or the second stage regression of
sj(Z; q) onto (A, X) are.

In the next proposition, we provide the sample analog of Proposition 3 when the estimator of the bounds
is a DR-Learner. Let κj(a, x) ≡ κj(a, x; qj) = E{sj(Z; qj) | A = a, X = x}. Further, let R2
j (t) be the
mean-square-error of an oracle estimator of θj(t; γ) regressing the pseudo-outcome ϕj(Z; w, κj, qj, w, qj)
onto A, deﬁned as

ϕu(Z; w, κu, qu, w, qu) = w(A, X){su(Z; qu) − κu(A, X; qu)} +

(cid:90)

κu(A, x; qu)dP(x)

− w(A, X){Y − qu(A, X)}

(cid:104)
γsgn{Y −qu(A,X)} − γsgn{Y −qu(A,X)}(cid:105)

ϕl(Z; w, κl, ql, w, ql) = w(A, X){sl(Z; ql) − κl(A, X; ql)} +

− w(A, X){Y − ql(A, X)}

(cid:104)

(cid:90)

κl(A, x; ql)dP(x)
γsgn{ql(A,X)−Y } − γsgn{ql(A,X)−Y }(cid:105)

It can be shown that E{ϕj(Z; w, κj, qj, w, qj) | A = t} = θj(t; γ) for j = {l, u}.

Proposition 4. Let (cid:98)θj(t; γ) be an DR-Learner estimator of θj(t; γ) based on linear smoothing (Sections 2
and 4). Further, let the sample splitting scheme be the same as in Figure 1 and assume that the following
conditions hold:

1. If Ti ≤ Vi for all i ∈ {1, . . . , n}, then the weights satisfy (cid:80)n

i=1 Wi(t; An)Ti ≤ (cid:80)n

i=1 Wi(t; An)Vi;

2. (cid:107) (cid:98)w − w(cid:107)∞, (cid:107)(cid:98)κj − κj(cid:107)∞ and (cid:107)(cid:98)qj − qj(cid:107)∞ are all oP(1), where qj(a, x) does not need to equal qj(a, x);

19

3. var{ϕj(Z; w, κj, qu, w, qu) | A = a} ≥ c > 0 for all a ∈ A and some constant c.

4. The outcome Y has a uniformly bounded conditional density given any values of (A, X);

5. The linear smoother weights Wi(t; An) are localized as in Proposition 2 in a neighborhood Nt around

A = t.

Then, the following inequalities hold

(cid:98)θl(t; γ) ≤ θl(t; γ) + OP

(cid:18)

Rl(t) + sup
a∈Nt

(cid:18)

(cid:19)

rl(a)

(cid:19)

(cid:98)θu(t; γ) ≥ θu(t; γ) + OP

Ru(t) + sup
a∈Nt

ru(a)

where, for (cid:107)f (cid:107)2

t = (cid:82) f 2(z)dP(z | A = t):

rj(t) = (cid:107) (cid:98)w − w(cid:107)t(cid:107)(cid:98)κj − κj(cid:107)t + (cid:107) (cid:98)w − w(cid:107)t(cid:107)(cid:98)qj − qj(cid:107)t + |(Pn − P)(cid:98)κj(t, X; (cid:98)qj)|

Proposition 4 shows that, even if the conditional quantiles of Y given (A, X) are not well estimated, the
estimators of the bounds can still converge to functions that contain the region [θl(t; γ), θu(t; γ)] and, in
this sense, are “valid bounds.” The result holds under mild conditions. For instance, conditions 1 and 5
are a mild stability conditions on the second-stage linear smoother. Conditions 3 and 4 are mild regularity
conditions on the data generating process and the nuisance functions’ estimators. The speed at which
(cid:98)θj(t; γ) converges to valid bounds depends on the structural properties of θj(t; γ), encoded in the oracle
MSE R2
j (t), as well as the accuracy in estimating w and κ. The proof of Proposition 4 extends the strategy
of Dorn et al. [2021] to the case of non-root-n estimable parameters.

5 Small simulation experiment

We conduct a small simulation experiment to evaluate the performance of the ﬁrst- and second-order
estimators in ﬁnite samples. We generate data according to the following process

X ∼ U (−1, 1), A ∼ TruncNorm(amin = −1, amax = 1, mean = κ(x), sd = 1),

Y | A, X ∼ N (ξ(a, x), 0.25).

where b(x) = (cid:2)b1(x)

. . .

b6(x)(cid:3)T are the ﬁrst six, normalized Legendre polynomials and

β = (cid:2)1, 0.8, 0.4, 0.2, 0.1, 0.05(cid:3)T

κ(x) =

1
3

b(x)T β

and

ξ(a, x) = b(a)T β + b(x)T β

To estimate µ(a, x) and π(a | x) while keeping tight control on the error incurred by the nuisance estimation
step, we simulate estimators as

(cid:98)µ(a, x) = ξ(a, x) + N (5n−1/α, n−1/α) · cos(2πx) + N (5n−1/α, n−1/α) · cos(2πa)
(cid:98)π(a | x) = φ(a; mean = κ(x) + N (n−1/α, 0.5n−1/α) · cos(2πx), sd = 1)

for n = 500 (the sample size used), α = {2, 4, 6, 8, 10, 15} and where φ(a; µ, σ2) is the density of a truncated
normal and the terms N (µ, σ) denote independent Normal random variables. The estimators are ﬂuctua-
tions of the true curves where the ﬂuctuations scale as n−1/α. We estimate p(a) as n−1 (cid:80)n

i=1 (cid:98)π(a | Xi).

As an example of the ERM-based estimator, we consider orthogonal series regression, where the basis
that we use is the Legendre polynomials basis. The number of terms ranges from 2 to 8. For the DR-

20

Learner, we consider local linear regression with Gaussian kernel and bandwidth taking value in bw =
{0.1, 0.2, 0.3, 0.4, 0.5}. Finally, we consider ﬁrst-order (the estimator of Colangelo and Lee [2020]) and
second-order estimators based on the higher-order estimator construction. We use a Gaussian kernel for
the term Kht(a), with bandwidth taking value in bw and the ﬁrst eleven Legendre polynomials (normalized)
as the basis in Π(xi, xj). We estimate Ω by its empirical counterpart (cid:98)Ω = Pn{b(X)b(X)T Kht(A)}.
(cid:82) 1
To compare the estimators’ performance, we evaluate the dose-response θ(a) = b(a)T β + 1
−1 b(x)T βdx
2
at 5 points equally spaced in [−0.5, 0.5]. At each point t, we approximate the mean-square-errors of the
estimators by averaging their errors across 500 simulations. At each point t, we thus have one estimate
of the MSEs for each tuning parameter value (number of basis or bandwidth value). To compare the
estimators at each point, we consider the best-performing tuning parameter in terms of MSEs. In practice,
this is not viable; potential alternatives would be to select the bandwidth via some form of cross-validation
or simply to report a sequence of estimates for tuning parameter value. We ﬁnally compute a weighted
mean of the MSEs with weight proportional to the density of A at t.

Figure 3 reports the results. We have included the MSEs for an oracle DR-Learner estimator that has access
to the true nuisance functions to give a reference value. As expected, the performance of the estimators is
similar when the error in the nuisance estimators is small. As the error increases, however, the second-order
estimator performs better. Across the regimes for the nuisance errors that we considered, the ﬁrst-order
estimator performs better than either the one based on orthogonal series regression (ERM-based) or the
one based on local polynomial regression (DR-Learner). In future work, it would be interesting to explore
if this conclusion holds even when π(a | x) and µ(a, x) have vastly diﬀerent smoothness levels.

We conclude with a word of caution. In all the results contained in this work, we have not kept track
of constant terms. While in asymptotic regimes, constants do not matter, in ﬁnite samples they might.
Our simulated estimators would thus converge to the truth with the desired rate of order n−1/α even if we
consider ﬂuctuations cn−1/α for any constant c. Perhaps not surprisingly, we ﬁnd that our simulation setup
is sensitive to the choice of the constants multiplying the rate. In this sense, while encouraging, our limited
simulation results should be interpreted with caution. We leave the design and implementation of larger
simulation experiments to future work. We refer the reader to Li et al. [2005] for a comprehensive simulation
study illustrating the superior performance of estimators based on higher order inﬂuence functions in the
context of pathwise diﬀerentiable parameters.

6 Conclusions and future directions

In this work, we have explored the possibility of improving existing approaches to doubly-robust estima-
tion of a dose-response curve by considering estimators based on DR-Learning framework and higher-order
inﬂuence functions. We have shown that an estimator akin to the higher-order estimator of the average
treatment eﬀect described in Robins et al. [2017] perform better than existing estimators, at least un-
In addition, we have specialized recent advancements on regression
der certain smoothness conditions.
estimation with estimated outcomes to the dose-response settings and introduced two new doubly-robust
estimators of the dose-response curve. A small simulation experiment has corroborated our theoretical
results in ﬁnite samples. We have also described a ﬂexible method to bound the causal dose-response
function in the presence of unmeasured confounding.

Many open questions remain. First, and perhaps most importantly, a minimax lower bound for estimating
the dose-response curve has not been described in the literature, to the best of our knowledge. Computing
a lower bound on the risk of any estimator of this parameter is instrumental for understanding under
what conditions, if any, the higher order estimator that we have proposed can be improved. Second, the
higher-order estimator is currently not capable of tracking the smoothness of the dose-response when the
conditional density of the treatment given the covariates, viewed as a function of the treatment alone, is
less smooth than the dose-response itself. It is unclear if this stems from an intrinsic limitation of our

21

(a) True outcome model x (cid:55)→ E(Y | A = 0, X = x)
(red) and simulated estimator (black, dotted lines) when
α = 8.

(b) True conditional density x (cid:55)→ π(0 | X = x) (red)
and simulated estimators (black, dotted lines) when α =
8.

Figure 2: Examples of estimators of the true nuisance functions µ(a, x) and π(a | x) and simulation results.

Figure 3: Estimated MSEs for diﬀerent estimators of the dose-response across 500 simulations.

estimator, the upper bound on the risk that we have computed is not tight enough or this is part of the
minimax rate. A potential avenue for future research is to investigate the possibility of constructing a
higher-order estimator that is based on regressions of some particular pseudo-outcomes onto A.

Finally, our results are about convergence of the estimators in mean-square-error. We leave the study of
the inferential properties of the estimators discussed here for future work.

22

−1.0−0.50.00.51.0−20246810Mean of Y given at A = 0, XX−1.0−0.50.00.51.00.350.450.550.65Density of A given X at A = 0X0.050.150.250.35aRMSE15108642OracleERM−basedDR−LearnerLinearQuadraticReferences

Chunrong Ai, Oliver Linton, Kaiji Motegi, and Zheng Zhang. A uniﬁed framework for eﬃcient estimation

of general treatment models. arXiv preprint arXiv:1808.04936, 2018.

Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Some new asymptotic
theory for least squares series: Pointwise and uniform results. Journal of Econometrics, 186(2):345–366,
2015.

Karine Bertin. Asymptotically exact minimax estimation in sup-norm for anisotropic h¨older classes.

Bernoulli, 10(5):873–888, 2004.

Matteo Bonvini, Edward Kennedy, Valerie Ventura, and Larry Wasserman. Sensitivity analysis for marginal

structural models. Working manuscript, 2022.

Kyle Colangelo and Ying-Ying Lee. Double debiased machine learning nonparametric inference with

continuous treatments. arXiv preprint arXiv:2004.03036, 2020.

Iv´an D´ıaz and Mark J van der Laan. Targeted data adaptive estimation of the causal dose–response curve.

Journal of Causal Inference, 1(2):171–192, 2013.

Jacob Dorn, Kevin Guo, and Nathan Kallus. Doubly-valid/doubly-sharp sensitivity analysis for causal

inference with unmeasured confounding. arXiv preprint arXiv:2112.11449, 2021.

Sam Efromovich. Conditional density estimation in a regression setting. The Annals of Statistics, 35(6):

2504–2535, 2007.

Jianqing Fan and Irene Gijbels. Local polynomial modelling and its applications. Routledge, 2018.

Carlos A Flores. Estimation of dose-response functions and optimal doses with a continuous treatment.

University of Miami, Department of Economics, November, 2007.

Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036,

2019.

Antonio F Galvao and Liang Wang. Uniformly semiparametric eﬃcient estimation of treatment eﬀects with
a continuous treatment. Journal of the American Statistical Association, 110(512):1528–1542, 2015.

M Hoﬀman and Oleg Lepski. Random rates in anisotropic regression (with a discussion and a rejoinder by

the authors). The Annals of Statistics, 30(2):325–396, 2002.

Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal eﬀects. arXiv preprint

arXiv:2004.14497, 2020.

Edward H Kennedy. Semiparametric doubly robust targeted double machine learning: a review. arXiv

preprint arXiv:2203.06469, 2022.

Edward H Kennedy, Zongming Ma, Matthew D McHugh, and Dylan S Small. Nonparametric methods for
doubly robust estimation of continuous treatment eﬀects. Journal of the Royal Statistical Society. Series
B, Statistical Methodology, 79(4):1229, 2017.

Lingling Li, Eric Tchetgen, J Robins, and A van der Vaart. Robust inference with higher order inﬂuence

functions: Parts i and ii. In Joint Statistical Meetings, Minneapolis, Minnesota, 2005.

Rajarshi Mukherjee, Whitney K Newey, and James M Robins. Semiparametric eﬃcient empirical higher

order inﬂuence function estimators. arXiv preprint arXiv:1705.07577, 2017.

23

Romain Neugebauer and Mark van der Laan. Nonparametric causal eﬀects based on marginal structural

models. Journal of Statistical Planning and Inference, 137(2):419–434, 2007.

Whitney K Newey. Kernel estimation of partial means and a general variance estimator. Econometric

Theory, pages 233–253, 1994.

James Robins, Lingling Li, Eric Tchetgen, Aad van der Vaart, et al. Higher order inﬂuence functions and
minimax estimation of nonlinear functionals. In Probability and statistics: essays in honor of David A.
Freedman, pages 335–421. Institute of Mathematical Statistics, 2008.

James Robins, Lingling Li, Eric Tchetgen, and Aad W van der Vaart. Quadratic semiparametric von mises

calculus. Metrika, 69(2):227–247, 2009a.

James Robins, Eric Tchetgen Tchetgen, Lingling Li, and Aad van der Vaart. Semiparametric minimax

rates. Electronic journal of statistics, 3:1305, 2009b.

James Robins, Lingling Li, Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher

order estimating equations for high-dimensional models. Annals of statistics, 45(5):1951, 2017.

James M Robins. Marginal structural models versus structural nested models as tools for causal inference.
In Statistical models in epidemiology, the environment, and clinical trials, pages 95–133. Springer, 2000.

Donald B Rubin. Estimating causal eﬀects of treatments in randomized and nonrandomized studies.

Journal of educational Psychology, 66(5):688, 1974.

Vira Semenova and Victor Chernozhukov. Debiased machine learning of conditional average treatment

eﬀects and other causal functions. arXiv preprint arXiv:1702.06240, 2017.

Rahul Singh, Liyuan Xu, and Arthur Gretton. Reproducing kernel methods for nonparametric and semi-

parametric treatment eﬀects. arXiv preprint arXiv:2010.04855, 2020.

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media,

2008.

Mark J Van der Laan. Statistical inference for variable importance. The International Journal of Bio-

statistics, 2(1), 2006.

Mark J Van der Laan, MJ Laan, and James M Robins. Uniﬁed methods for censored longitudinal data and

causality. Springer Science & Business Media, 2003.

Aad van der Vaart. Higher order tangent spaces and inﬂuence functions. Statistical Science, pages 679–686,

2014.

Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge

University Press, 2019.

Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

Ted Westling and Marco Carone. A uniﬁed study of nonparametric inference for monotone functions.

Annals of statistics, 48(2):1001, 2020.

Ted Westling, Peter Gilbert, and Marco Carone. Causal isotonic regression. Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 82(3):719–747, 2020.

24

A Proof of Proposition 1

Suppose we observe two samples of n iid observations from P, say Dn and Zn. Denote Z1, . . . , Zn the
observations in Zn that are iid copies of a generic random variable Z. Let U denote a generic random
variable such that U ⊂ Z. For example, in the dose-response settings, Z = (Y, A, X) and U = A. Let
θ0(u) = E{f (Z) | U = u} denote the true regression function that needs to be estimated. Recall that
(cid:107)f (cid:107)2 = (cid:82) f (z)2dP(z) = P{f (Z)2} and θ∗ ∈ arg minθ∈Θ(cid:107)θ − θ0(cid:107)2, a ﬁxed function. Let (cid:98)f (·) denote an
estimate of f (·) constructed using only observations in Dn. Let Pn denote the empirical average over
sample Zn. The estimator of θ0(·) considered is

(cid:98)θ = arg min

θ∈Θ

1
n

n
(cid:88)

i=1

{ (cid:98)f (Zi) − θ(Ui)}2 ≡ arg min

Pn{ (cid:98)f (Z) − θ(U )}2.

θ∈Θ

Finally, let (cid:98)r(u) = E{ (cid:98)f (Z) | U = u, Dn} − θ0(u).
The statement of the theorem follows after proving

E((cid:107)(cid:98)θ − θ0(cid:107)2 | Dn) (cid:46) (cid:107)(cid:98)r(cid:107)2 + (cid:107)θ∗ − θ0(cid:107)2 + δ2

n

(4)

Our proof is a specialization of that of Theorem 3 in Foster and Syrgkanis [2019]. A useful reference for
the arguments made in their proof is Chapter 14 in Wainwright [2019]. To prove (4), we need two lemmas.

Lemma 3. The following inequality holds:

(cid:107)(cid:98)θ − θ0(cid:107)2 ≤ 8(cid:107)(cid:98)r(cid:107)2 + 3(cid:107)θ∗ − θ0(cid:107)2 − 2(Pn − P)

(cid:104)

{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)

Proof. Notice that

P

(cid:104)

{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)

= −2P[(cid:98)r(U ){(cid:98)θ(U ) − θ∗(U )}] + (cid:107)(cid:98)θ − θ0(cid:107)2 − (cid:107)θ∗ − θ0(cid:107)2

By the AM-GM inequality we have, for any κ > 02:

2(cid:98)r(U ){(cid:98)θ(U ) − θ∗(U )} = 2(cid:98)r(U ){(cid:98)θ(U ) − θ0(U )} + 2(cid:98)r(U ){θ0(U ) − θ∗(U )}

≤

2
κ (cid:98)r(U )2 + κ{(cid:98)θ(U ) − θ0(U )}2 + κ{θ∗(U ) − θ0(U )}2

By monotonicity of integration, it follows that

−2P[(cid:98)r(U ){(cid:98)θ(U ) − θ∗(U )}] ≥ −

2(cid:107)(cid:98)r(cid:107)2
κ

− κ(cid:107)(cid:98)θ − θ0(cid:107)2 − κ(cid:107)θ∗ − θ0(cid:107)2

Rearranging and choosing κ = 1/2, we have

(cid:107)(cid:98)θ − θ0(cid:107)2 ≤ 8(cid:107)(cid:98)r(cid:107)2 + 3(cid:107)θ∗ − θ0(cid:107)2 + 2P

(cid:104)

{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)

Because Pn[{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2] ≤ 0 since θ∗ ∈ Θ and (cid:98)θ is a minimizer, we also have

(cid:107)(cid:98)θ − θ0(cid:107)2 ≤ 8(cid:107)(cid:98)r(cid:107)2 + 3(cid:107)θ∗ − θ0(cid:107)2 − 2(Pn − P)

{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)
(cid:104)

2For any x, y and κ > 0,

(cid:18) x
√

2κ

− y

(cid:19)2

(cid:114) κ
2

≥ 0 =⇒

x2
2κ

+ y2 κ
2

≥ xy

25

as desired.

Lemma 4. For some constant L, let

E =

(cid:110)

∃θ ∈ Θ : (cid:107)θ − θ∗(cid:107) ≥ δn ∩

(cid:12)
(cid:12)(Pn − P)
(cid:12)

(cid:104)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:12)
(cid:12) ≥ Lδn(cid:107)θ − θ∗(cid:107)
(cid:12)

(cid:111)

Under the conditions of Proposition 1, P (E | Dn) ≤ c1 exp(−c2nδ2

n) for some constants c1 and c2.

Proof. Consider the sets

Sm = (cid:8)θ ∈ Θ : 2m−1δn ≤ (cid:107)θ − θ∗(cid:107) ≤ 2mδn

(cid:9)

Because supθ∈Θ(cid:107)θ(cid:107)∞ ≤ S, (cid:107)θ − θ∗(cid:107) ≤ 2S for any θ ∈ Θ, which implies that any θ such that (cid:107)θ − θ∗(cid:107) ≥ δn
must belong to some Sm for m ∈ {1, . . . , M }, where M ≤ log2(2S/δn). By a union bound,

P(E | Dn) ≤

≤

M
(cid:88)

m=1
M
(cid:88)

m=1

P(E ∩ Sm | Dn)

(cid:16)

P

∃θ ∈ Θ : (cid:107)θ − θ∗(cid:107) ≤ 2mδn

(cid:12)
(cid:12)(Pn − P)
(cid:12)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:12)
(cid:104)
(cid:12) ≥ 2m−1Lδ2
(cid:12)

n | Dn(cid:17)

∩

≤

M
(cid:88)

m=1

P(Zn(2mδn) ≥ 2m−1Lδ2

n | Dn)

where we deﬁne

Zn(r) =

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

(cid:12)
(cid:12)(Pn − P)
(cid:12)

(cid:104)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:12)
(cid:12)
(cid:12)

Under the conditions of the proposition, we have

sup
θ∈Θ

sup
z∈Z

(cid:12)
(cid:12)

(cid:12){ (cid:98)f (z) − θ(u)}2 − { (cid:98)f (z) − θ∗(u)}2(cid:12)

(cid:12) ≤ 8S2
(cid:12)

and

Thus, we have

(cid:104)

{ (cid:98)f (z) − θ(u)}2 − { (cid:98)f (z) − θ∗(u)}2(cid:105)2

≤ 16S2{θ(u) − θ∗(u)}2

σ2(r) ≡

sup
θ:(cid:107)θ−θ∗(cid:107)≤r

(cid:18)(cid:104)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)2(cid:19)

P

≤ 16S2r2

By Theorem 3.27 in Wainwright [2019] and subsequent discussion, viewing (cid:98)f (·) as ﬁxed given Dn, we have

P (Zn(r) ≥ E{Zn(r) | Dn} + u | Dn) ≤ 2 exp

(cid:18)

−

nu2
8e[16S2r2 + 16S2E{Zn(r) | Dn}] + 32S2u

(cid:19)

Next, we bound E{Zn(r) | Dn}. By a symmetrization argument, for (cid:15) a vector of iid Rademacher random

26

variables independent of Zn and Dn, it holds that

E{Zn(r) | Dn} ≤ 2EZ,(cid:15)

(cid:32)

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

(cid:12)
Pn
(cid:12)
(cid:12)

(cid:16)

(cid:15)

(cid:104)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) Dn
(cid:12)

(cid:33)

The Ledoux-Talagrand contraction inequality (see also pages 147 and 474 in Wainwright [2019]) yields
that, for non-random xi ∈ X , a class F of real-valued functions and a L-Lipschitz function φ : R → R, the
following holds

(cid:32)

E

sup
f ∈F

(cid:12)
n
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:33)
(cid:12)
(cid:15)i{φ(f (xi)) − φ(f ∗(xi))}
(cid:12)
(cid:12)
(cid:12)

≤ 2LE

(cid:32)

sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:15)i{f (xi) − f ∗(xi)}

(cid:12)
(cid:33)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where f ∗ : X → R is any function.

Under the boundedness conditions of our proposition, we have

(cid:12)
(cid:12)

(cid:12){ (cid:98)f (z) − θ1(u)}2 − { (cid:98)f (z) − θ2(u)}2(cid:12)

(cid:12)
(cid:12) ≤ 4S|θ1(u) − θ2(u)|

for any z ∈ Z. Thus, the square-loss in this case is 4S-Lipschitz for any z ∈ Z. By the contraction
inequality above, we have

(cid:32)

E(cid:15)

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

(cid:12)
Pn
(cid:12)
(cid:12)

(cid:16)

(cid:15)

(cid:104)

{ (cid:98)f (Z) − θ(u)}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:32)

≤ 2E(cid:15)

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

|Pn[(cid:15){θ(U ) − θ∗(U )}]|

(cid:33)

Therefore, we have

(cid:32)

EZ,(cid:15)

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

(cid:12)
Pn
(cid:12)
(cid:12)

(cid:16)

(cid:15)

{ (cid:98)f (Z) − θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:17)(cid:12)
(cid:104)
(cid:12)
(cid:12)

(cid:12)
(cid:12) Dn
(cid:12)

(cid:33)

(cid:32)

≤ 8SE

sup
θ∈Θ:(cid:107)θ−θ∗(cid:107)≤r

|Pn[(cid:15){θ(U ) − θ∗(U )}]|

(cid:33)

(cid:12)
(cid:12) Dn
(cid:12)

≡ 8SRn(Θ∗, r)

Next, we have assumed Θ∗ to be star-shaped; by Lemma 13.6 in Wainwright [2019] the function r (cid:55)→
Rn(Θ∗, r)/r is non-increasing. Therefore, because δn solves Rn(Θ∗, δ) ≤ δ2, we also have:

Rn(Θ∗, r) ≤ rδn

for all r ≥ δn.

Therefore, we conclude that E{Zn(r) | Dn} ≤ 16Srδn for all r ≥ δn.

Putting everything together, we have derived that

P (Zn(r) ≥ 16Srδn + u | Dn) ≤ 2 exp

(cid:18)

−

nu2
8e(16S2r2 + 162S3r2) + 32S2u

(cid:19)

Let L = 34S; specializing this bound to our setting with r = 2mδn and u = S2mδ2

n, we have

P (cid:0)Zn(2mδn) ≥ L · 2m−1δ2

n | Dn(cid:1) ≤ 2 exp

(cid:18)

−

nδ2
n
8e(16 + 162S) + 32S

(cid:19)

27

since 2−m ≤ 1 for any m ≥ 1. Finally,

P(E | Dn) ≤

M
(cid:88)

m=1

P (cid:0)Zn(2mδn) ≥ L2m−1δ2

n | Dn(cid:1)

(cid:18)

≤ 2 exp

−

nδ2
n
8e(16 + 162S) + 32S

(cid:19)

+ ln M

Recall that M ≤ log2(2S/δn) ≤ log2(2S

√

2n) because we have assumed δn ≥ 1/

√

2n. Therefore, if

we can conclude

2 ln{log2(2S

δ2
n ≥

√

2n)}{8e(16 + 162S) + 32S}

n

P(E | Dn) ≤ 2 exp

(cid:18)

−

nδ2
n
16e(16 + 162S) + 64S

(cid:19)

as desired.

A.1 Proof of Equation (4)

Notice that Lemma 4 implies that, with probability at least 1 − c1 exp(−c2nδ2
events occur:

n), either of the following two

1. Event 1:

(cid:107)(cid:98)θ − θ∗(cid:107) ≤ δn =⇒ (cid:107)(cid:98)θ − θ0(cid:107) ≤ δn + (cid:107)θ∗ − θ0(cid:107) =⇒ (cid:107)(cid:98)θ − θ0(cid:107)2 ≤ 2δ2

n + 2(cid:107)θ∗ − θ0(cid:107)2

2. Event 2:

(cid:12)
(cid:12)(Pn − P)
(cid:12)

for any κ > 0.

{ (cid:98)f (Z) − (cid:98)θ(U )}2 − { (cid:98)f (Z) − θ∗(U )}2(cid:105)(cid:12)
(cid:104)
(cid:12) ≤ Lδn(cid:107)(cid:98)θ − θ∗(cid:107)
(cid:12)
L2δ2
n
κ

+

≤

κ(cid:107)(cid:98)θ − θ0(cid:107)2
2

+

κ(cid:107)θ∗ − θ0(cid:107)2
2

Because of the result from Lemma 3, Event 2 (with κ = 1/2) implies

(cid:107)(cid:98)θ − θ0(cid:107)2 ≤ 16(cid:107)(cid:98)r(cid:107)2 + 7(cid:107)θ∗ − θ0(cid:107)2 + 8L2δ2

n

This means that there exists a constant C such that

(cid:16)

P

(cid:107)(cid:98)θ − θ0(cid:107)2 ≤ C (cid:0)(cid:107)(cid:98)r(cid:107)2 + (cid:107)θ∗ − θ0(cid:107)2 + δ2

n

(cid:1) | Dn(cid:17)

≥ 1 − c1 exp(−c2nδ2
n)

28

Let t0 = C (cid:0)(cid:107)(cid:98)r(cid:107)2 + (cid:107)θ∗ − θ0(cid:107)2 + δ2
(cid:90) ∞

n

E

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 | Dn(cid:17)

=

(cid:1). This implies that

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t | Dn(cid:17)

dt

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t | Dn(cid:17)

dt +

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t | Dn(cid:17)

dt +

c1 exp(−c3nt)dt

(cid:90) ∞

0
c1
c3n

(cid:90) ∞

t0
(cid:90) ∞

0

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t | Dn(cid:17)

dt

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t0 + t | Dn(cid:17)

dt

0
(cid:90) t0

0
(cid:90) t0

0

=

=

≤ t0 +

= t0 +

as desired. The last inequality holds because P
Rn(δn; Θ∗)/δn ≤ δn, then so does δ(cid:48)

n = (cid:112)δ2

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t | Dn(cid:17)

≤ 1 and because, whenever δn satisﬁes

n + t/C > δn. This means that we can write

t0 + t = C

(cid:16)

(cid:107)(cid:98)r(cid:107)2 + (cid:107)θ∗ − θ0(cid:107)2 + δ(cid:48)

n

2(cid:17)

Thus,

P

(cid:16)

(cid:107)(cid:98)θ − θ0(cid:107)2 > t0 + t | Dn(cid:17)

≤ c1 exp{−c2n(δ2

n + t/C)} ≤ c1 exp(−c3nt)

as Lemma 4 holds for any δ(cid:48)

n that solves Rn(δ; Θ∗)/δ ≤ δ.

B Proof of Proposition 2

The proof of this theorem is based on Proposition 1 and Theorem 1 from Kennedy [2020]. Their Theorem
1 together with consistency of (cid:98)f (z) yields that

|(cid:98)θ(t) − θ0(t)|≤ |(cid:101)θ(t) − θ0(t)|+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
Wi(t; U n)E{ (cid:98)f (Zi) − f (Zi) | Dn, Ui}
(cid:12)
(cid:12)
(cid:12)

(cid:18)

E

+ oP

(cid:20)(cid:110)

(cid:101)θ(t) − θ0(t)

(cid:111)2(cid:21)(cid:19)

Under the assumptions of Proposition 2 (localized weights):

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

Wi(t; U n)(cid:98)r(Ui)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
n

n
(cid:88)

i=1

as desired.

C Proof of Theorem 1

|Wi(t; U n)| |(cid:98)r(Ui)|1{Ui ∈ N (t)} (cid:46) sup

|(cid:98)r(u)|

a∈N (t)

The proof of this theorem essentially follows from that of Theorem 8.1 in Robins et al. [2017], with the
main diﬀerence that our estimator has Kht(a) in place of 1(A = t) so that our analysis will need to keep
track of terms of order O(hα∧β).

To simplify the notation, we let v(a, x) = µ(a, x) − (cid:98)µ(a, x), h(a, x) = 1/π(a | x), q(a, x) = (cid:98)h(a, x) − h(a, x),
v(x) = v(t, x), q(x) = q(t, x), and g(x) = (cid:82) Kht(a)p(a, x)da. Also we deﬁne (cid:107)f (cid:107)2
Before computing bias and variance of our estimator, we state some useful facts about orthogonal projec-
tions. More general versions of these statements can be found in the excellent supplementary material to
Robins et al. [2017]. First, recall the deﬁnition of the orthogonal projection and its kernel in our context.

g = (cid:82) f 2(x)g(x)dx.

29

For g(x) = (cid:82) Kht(a)p(a, x)da, (cid:98)g(x) = (cid:82) Kht(a)(cid:98)p(a, x)da and some function f :

(cid:90)

(cid:90)

Πi,jf (xj)g(xj)dxj = b(xi)T Ω−1

(cid:98)Πi,jf (xj)(cid:98)g(xj)dxj = b(xi)T (cid:98)Ω−1

(cid:90)

(cid:90)

b(xj)f (xj)g(xj)dxj

b(xj)f (xj)(cid:98)g(xj)dxj

Π(f )(xi) =

(cid:98)Π(f )(xi) =
(cid:90)

Ω =

b(u)b(u)T g(u)du and (cid:98)Ω =

(cid:90)

b(u)b(u)T

(cid:98)g(u)du.

• Fact 1. Orthogonal projections do not increase length: for any function f and projection in L2(µ),

(cid:90)

(cid:107)Π(f )(cid:107)2 =

Π(f )(x)Π(f )(x)dµ =

(cid:90)

Π(f )(x)f (x)dµ ≤ (cid:107)Π(f )(cid:107)(cid:107)f (cid:107) =⇒ (cid:107)Π(f )(cid:107) ≤ (cid:107)f (cid:107)

by Cauchy-Schwarz, where (cid:107)f (cid:107)2 = (cid:82) f 2dµ and Π(f )(x) = b(x)T (cid:8)(cid:82) b(u)b(u)T dµ(cid:9)−1 (cid:82) b(u)f (u)dµ.
• Fact 2. Let w denote some positive and bounded weight function and Πw and Π projections in L2(µ)
onto some ﬁxed k-dimensional space L spanned by b1(x), . . . , bk(x), with weights w and 1 respectively.
Then, for any l ∈ L, we have Πw(l) = Π(l):

l(x) = b(x)T β = Πw(l)(x) = b(x)T

= b(x)T

(cid:26)(cid:90)

(cid:26)(cid:90)

b(u)b(u)T w(u)dµ

(cid:27)−1 (cid:90)

b(u)b(u)T βw(u)dµ

(cid:27)−1 (cid:90)

b(u)b(u)T dµ

b(u)b(u)T βdµ = Π(l)(x)

where β ∈ Rk is some vector of coeﬃcients.

• Fact 3. Useful identities:

(cid:90)

(cid:90)

(cid:90)

Π(xi, xj)Π(xj, xk)g(xj)dxj = b(xi)T Ω−1

(cid:98)Π(xi, xj)(cid:98)Π(xj, xk)(cid:98)g(xj)dxj = b(xi)T (cid:98)Ω−1

Π(xi, xj)(cid:98)Π(xj, xk)g(xj)dxj = b(xi)T Ω−1

(cid:90)

(cid:90)

(cid:90)

b(xj)b(xj)T g(xj)dxjΩ−1b(xk) = Π(xi, xk)

b(xj)b(xj)T

(cid:98)g(xj)dxj (cid:98)Ω−1b(xk) = (cid:98)Π(xi, xk)

b(xj)b(xj)T g(xj)dxj (cid:98)Ω−1b(xk) = (cid:98)Π(xi, xk)

C.1 Bias

We will divide the proof of the bias bound in several steps:

1. Prove that, for some functions r1 and r2 (deﬁned in the proof) and

(cid:90)

T = −

r1(x1)Π(x1, x2)r2(x2)g(x1)g(x2)dx1dx2

the following holds

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

(cid:98)ϕ1(z)dP(z) − θ(t) + T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46) (cid:107)(I − Π)(v)(cid:107)g(cid:107)(I − Π)(q)(cid:107)g + hα∧β

30

2. Prove that

m
(cid:88)

(cid:90)

j=2

(cid:98)ϕj(z1, . . . , zj)dP(z1, . . . , zj) − T

r1(x1)((cid:98)Π1,2 − Π1,2) · · · ((cid:98)Πm−1,m − Πm−1,m)r2(xm)g(x1) · · · g(xm)dx1 · · · dxm

|T2| (cid:46) (cid:107)r1(cid:107)g(cid:107)r2(cid:107)g(cid:107)(cid:98)s − 1(cid:107)m−1

∞

(cid:46)

(cid:16)

(cid:107)v(cid:107)g(cid:107)q(cid:107)g + hα∧β(cid:17)

(cid:107)(cid:98)s − 1(cid:107)m−1

∞

(cid:90)

= (−1)m−1

≡ T2

3. Prove that

where (cid:98)s = g/(cid:98)g.

C.1.1 Step 1

Let us deﬁne

(cid:90)

(cid:90)

(cid:90)

(cid:90)

∆1(x) ≡

∆2(x) ≡

∆3(x) ≡

∆4(x) ≡

Kht(a){v(a, x) − v(t, x)}π(a | x)da

Kht(a){µ(a, x) − µ(t, x)}π(a | x)da

Kht(a){(cid:98)h(a, x) − (cid:98)h(t, x)}π(a | x)da

Kht(a)π(a | x)da − π(t | x)

∆5(x) ≡ h(t, x) −

1
(cid:82) Kht(a)π(a | x)da

=

∆4(x)
π(t | x){π(t | x) + ∆4(x)}

We have

(cid:90)

(cid:98)ϕ1(z)dP(z) − θ(t) =

=

=

(cid:90)

(cid:90)

(cid:90)

Let

Kht(a)(cid:98)h(t, x){µ(a, x) − (cid:98)µ(t, x)}π(a | x)dap(x)dx −

Kht(a)(cid:98)h(t, x){µ(a, x) − (cid:98)µ(t, x)}π(a | x)dap(x)dx −

(cid:90)

(cid:90)

v(x)p(x)dx

v(x)
(cid:82) Kht(a)π(a | x)da

g(x)dx

v(x)q(x)g(x)dx +

(cid:90)

v(x)∆5(x)g(x)dx +

(cid:90)

(cid:98)h(t, x)∆2(x)p(x)dx

r1(x) = v(x) +

∆1(x)
(cid:82) Kht(a)π(a | x)da

, and r2(x) = q(x) + ∆5(x) +

∆3(x)
(cid:82) Kht(a)π(a | x)da

and notice that

(cid:90)

(cid:90)

Kht(a){y − (cid:98)µ(a, x)}dP(z | x) = r1(x)
(cid:90)

{Kht(a)(cid:98)h(a, x) − 1}dP(z | x) = r2(x)

(cid:90)

Kht(a)π(a | x)da

Kht(a)π(a | x)da

31

(cid:90)

(cid:90)

(cid:90)

Deﬁne

Therefore,

T ≡ −

= −

−

(cid:90)

r1(x1)Π(x1, x2)r2(x2)g(x1)dx1g(x2)dx2

v(x)Π(q)(x)g(x)dx −

(cid:26)

Π(v)(x)

∆5(x) +

(cid:90)

∆1(x)
(cid:82) Kht(a)π(a | x)da
∆3(x)
(cid:82) Kht(a)π(a | x)da

(cid:27)

g(x)dx

Π(r2)(x)g(x)dx

(cid:98)ϕ1(z)dP(z) + T − θ(t) =

(cid:90)

v(x)(I − Π)(q)(x)g(x)dx + ∆

where ∆ groups all the terms involving ∆j together:

(cid:90)

∆ =

v(x)∆5(x)g(x)dx +

(cid:90)

(cid:98)h(t, x)∆2(x)p(x)dx

(cid:90)

−

∆1(x)
(cid:82) Kht(a)π(a | x)da

Π(r2)(x)g(x)dx −

(cid:90)

(cid:26)

Π(v)(x)

∆5(x) +

∆3(x)
(cid:82) Kht(a)π(a | x)da

(cid:27)

g(x)dx

The term ∆ is controlled under the smoothness assumptions of the theorem, while

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

v(x)(I − Π)(q)(x)g(x)dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:107)(I − Π)(v)(cid:107)g(cid:107)(I − Π)(q)(cid:107)g

by Cauchy-Schwarz. In particular, we have assumed that a (cid:55)→ µ(a, x), a (cid:55)→ (cid:98)µ(a, x) are α-times continuously
diﬀerentiable and a (cid:55)→ h(a, x), a (cid:55)→ (cid:98)h(a, x) (or, equivalently, π(a | x) and (cid:98)π(a | x)) are β-times continuously
diﬀerentiable. Thus, we have

v(a, x) =

µ(a, x) =

α−1
(cid:88)

j=0

α−1
(cid:88)

j=0

v(j)(t, x)

(a − t)j
j!

µ(j)(t, x)

(a − t)j
j!

+ v(α)(t + τ1(a − t), x)

(a − t)α
α!

+ µ(α)(t + τ2(a − t), x)

(a − t)α
α!

π(a | x) =

β−1
(cid:88)

j=0

π(j)(t | x)

(a − t)j
j!

+ π(β)(t + τ3(a − t) | x)

(a − t)β
β!

h(a, x) =

β−1
(cid:88)

j=0

h(j)(t, x)

(a − t)j
j!

+ h(β)(t + τ4(a − t), x)

(a − t)β
β!

32

for some τ1, τ2, τ3, τ4 ∈ [0, 1]. Then, for example, we have the following

(cid:90)

∆1(x) ≡

Kht(a){v(a, x) − v(t, x)}π(a | x)da

α−1
(cid:88)

β−1
(cid:88)

=

i=1

j=0

hi+j v(i)(t, x)

i!

π(j)(t | x)
j!

(cid:90) 1

0

ui+jK(u)du

α−1
(cid:88)

j=1

β−1
(cid:88)

+

+

+

j=0
hα+β
α! β!

(cid:90) 1

0

hβ+j v(j)(t, x)

β! j!

hα+j π(j)(t | x)

α! j!

(cid:90) 1

uβ+jK(u)π(β)(t + τ3uh | x)du

0

(cid:90) 1

0

v(α)(t + τ1uh, x)uα+jK(u)du

v(α)(t + τ1uh, x)uα+βK(u)π(β)(t + τ3uh | x)du

so that (cid:107)∆1(cid:107)∞ (cid:46) h(β+1)∧α. Similarly, (cid:107)∆2(cid:107)∞ (cid:46) h(β+1)∧α and

(cid:90)

∆3(x) ≡

Kht(a){(cid:98)h(a, x) − (cid:98)h(t, x)}π(a | x)da

=

β−1
(cid:88)

β−1
(cid:88)

i=1

j=0

hi+j (cid:98)h(i)(t, x)

i!

π(j)(t | x)
j!

(cid:90) 1

0

ui+jK(u)du

β−1
(cid:88)

j=1

β−1
(cid:88)

+

+

+

j=0
h2β
β! β!

(cid:90) 1

0

hβ+j (cid:98)h(j)(t, x)

β! j!

hβ+j π(j)(t | x)

β! j!

(cid:90) 1

uβ+jK(u)π(β)(t + τ3uh | x)du

0

(cid:90) 1

0

(cid:98)h(β)(t + τ1uh, x)uβ+jK(u)du

v(β(t + τ1uh, x)u2βK(u)π(β)(t + τ3uh | x)du

Therefore, (cid:107)∆3(cid:107)∞ (cid:46) hβ and, similarly, (cid:107)∆4(cid:107)∞ (cid:46) hβ and (cid:107)∆5(cid:107)∞ (cid:46) hβ.
(cid:107)∆j(cid:107)∞ (cid:46) hα∧β, for j = 1, 2, 3, 4, 5. This concludes our proof that

In this light, it holds that

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:98)ϕ1(z)dP(z) + T − θ(t)
(cid:12)
(cid:12)

(cid:46) (cid:107)(I − Π)(v)(cid:107)g(cid:107)(I − Π)(q)(cid:107)g + hα∧β

C.1.2 Step 2

We will show that

T2 =

=

m
(cid:88)

(cid:90)

j=2
m
(cid:88)

(cid:90)

j=2

(cid:98)ϕj(z1, . . . , zj)dP(z1, . . . , zj) − T

(cid:98)ϕj(z1, . . . , zj)dP(z1, . . . , zj) +

(cid:90)

r1(x)Π(x1, x2)r2(x2)g(x1)g(x2)dx1dx2

(cid:90)

= (−1)m−1

r1(x1)((cid:98)Π1,2 − Π1,2) · · · ((cid:98)Πm−1,m − Πm−1,m)r2(xm)g(x1) · · · g(xm)dx1 · · · dxm

33

The result is clearly true for m = 2, so we proceed by induction. Relative to the mth term, the term m + 1
receives the contribution from

(cid:90)

(cid:98)ϕm+1(z1, . . . , zm+1)dP(z1, . . . , zm)

= (−1)m

m−1
(cid:88)

i=0

(cid:19)

(cid:18)m − 1
i

(cid:90)

(−1)i

≡ (−1)mT3

r1(x1)(cid:98)Π1,2 · · · (cid:98)Πm−i,m−i+1r2(xm−i+1)g(x1) · · · g(xm−i+1)dx1 · · · dxm−i+1

Thus to prove the claim we need to show that

(cid:90)

T3 =

+

r1(x1)((cid:98)Π1,2 − Π1,2) · · · ((cid:98)Πm−1,m − Πm−1,m)r2(xm)g(x1) · · · g(xm)dx1 · · · dxm
(cid:90)

r1(x1)((cid:98)Π1,2 − Π1,2) · · · ((cid:98)Πm,m+1 − Πm,m+1)r2(xm+1)g(x1) · · · g(xm+1)dx1 · · · dxm+1

≡ T4 + T5

Notice that T4 can be written as a sum of terms of the form

Bl = (−1)m−1−l

(cid:90)

r1(x1)B1,2 · · · Bm−1,mr2(xm)g(x1) · · · g(xm)dx1 · · · dxm

where Bi,j equals either (cid:98)Πi,j or Πi,j and l denotes the number of terms in the product for which Bi,j = (cid:98)Πi,j.
Similarly, T5 is a sum of terms of the form

Cl = (−1)m−l

(cid:90)

r1(x1)B1,2 · · · Bm,m+1r2(xm+1)g(x1) · · · g(xm+1)dx1 · · · dxm+1

Fact 3 is the reason why we only need to keep track of the number of (cid:98)Πi,j terms and not speciﬁcally which
Bij equals Πi,j or (cid:98)Πi,j. In fact, for Bij = Πi,j or (cid:98)Πi,j, we have

(cid:90)

Π(xj−1, xj)B(xj, xj+1)g(xj)dxj = Bj−1,j+1 =

(cid:90)

B(xj−1, xj)Π(xj, xj+1)g(xj)dxj

In this light, we can simplify as

Bl = (−1)m−1−l
(cid:90)

B0 = (−1)m−1
(cid:90)

Cl = (−1)m−l
(cid:90)

C0 = (−1)m

(cid:90)

r1(x1)(cid:98)Π1,2 · · · (cid:98)Πl,l+1r2(xl+1)dg(x1) · · · dg(xl+1) for l ≥ 1

r1(x1)Π1,2r2(x2)dg(x1)d(x2)

r1(x1)(cid:98)Π1,2 · · · (cid:98)Πl,l+1r2(xl+1)dg(x1) · · · dg(xl+1) for l ≥ 1

r1(x1)Π1,2r2(x2)dg(x1)d(x2) = −B0

For l ∈ {1, . . . , m − 1}, we have Cl = −Bl. Thus, we have reached

T4 = −C0 −

m−1
(cid:88)

l=1

(cid:18)m − 1
l

(cid:19)

Cl

and

T5 = C0 +

m−1
(cid:88)

l=1

34

(cid:19)

(cid:18)m
l

Cl + Cm

and this implies

T4 + T5 =

m−1
(cid:88)

l=1

(cid:19)

(cid:26)(cid:18)m
l

−

(cid:19)(cid:27)

(cid:18)m − 1
l

Cl + Cm =

m−1
(cid:88)

l=1

(cid:18)m − 1
l − 1

(cid:19)

Cl + Cm = T3

as desired. We have thus shown that

T2 = (−1)m−1

(cid:90)

r1(x1)((cid:98)Π1,2 − Π1,2) · · · ((cid:98)Πm−1,m − Πm−1,m)r2(xm)g(x1) · · · g(xm)dx1 · · · dxm

C.1.3 Step 3
We need to show that |T2| ≤ (cid:107)r1(cid:107)g(cid:107)r2(cid:107)g(cid:107)(cid:98)s − 1(cid:107)m−1
∞ . This statement is essentially a direct consequence of
Lemma 13.7 in the Supplementary material to Robins et al. [2017]. For the sake of completeness, we give
a proof here that is less general (and more verbose) than that in Robins et al. [2017], although it uses the
same arguments.

Deﬁne (cid:98)s = g(x)/(cid:98)g(x) and let M

(cid:98)s denoting multiplication by (cid:98)s. We have

(cid:90)

((cid:98)Πm−1,m − Πm−1,m)r2(xm)g(xm)dxm =

(cid:16)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(r2)(xm−1)

Continuing with this calculation, we get

T2 = (−1)m−1

(cid:90)

(cid:16)

r1(x)

(cid:17)m−1

(cid:98)ΠM

(cid:98)s − Π

(r2)(x)g(x)dx

Let (cid:107)f (cid:107)2

2,(cid:98)g = (cid:82) f 2(u)(cid:98)g(u)du and bound |T2| as

Deﬁne

|T2| (cid:46) (cid:107)r1(cid:107)g

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)m−1

(r2)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

(cid:16)

l(x) =

(cid:98)ΠM

(cid:98)s − Π

(cid:17)m−2

(r2)(x) ≡ b(x)T β

We can write l(x) as a linear combination of the truncated basis because both (cid:98)Π and Π project a function
onto the same ﬁnite dimensional subspace. Notice that we can view Π as a weighted projection in L2((cid:98)g)
with weight (cid:98)s, i.e.

Π(f )(x) = b(x)T Ω−1

(cid:90)

b(u)f (u)g(u)du = b(x)T

(cid:26)(cid:90)

b(u)b(u)(cid:98)s(u)(cid:98)g(u)du

(cid:27)−1 (cid:90)

b(u)f (u)(cid:98)s(u)(cid:98)g(u)du

Therefore, by Fact 2, we have

(cid:16)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)m−1

(r2)(x) =

(cid:16)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(l)(x) =

(cid:90)

(cid:98)Π(x, u){(cid:98)s(u) − 1}l(u)(cid:98)g(u)du = (cid:98)Π (((cid:98)s − 1)l) (x)

By Fact 1, we have

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)m−1

(r2)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:98)Π (((cid:98)s − 1)l)
(cid:13)2,(cid:98)g

≤ (cid:107)((cid:98)s − 1)l(cid:107)2,(cid:98)g ≤ (cid:107)(cid:98)s − 1(cid:107)∞(cid:107)l(cid:107)2,(cid:98)g

35

Repeating this argument m − 3 times applied to (cid:107)l(cid:107)2,(cid:98)g, we obtain

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)m−1

(r2)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

≤ (cid:107)(cid:98)s − 1(cid:107)m−2

∞

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:17)
(cid:98)s − Π

(r2)

(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

Furthermore,

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(r2)

(cid:13)
2
(cid:13)
(cid:13)

2,(cid:98)g

(cid:90) (cid:16)

(cid:90) (cid:16)

=

=

(cid:17)

(cid:17)

(cid:98)ΠM

(cid:98)s − Π

(cid:98)ΠM

(cid:98)s − Π

(cid:16)

(r2)(x)

(cid:17)

(cid:98)ΠM

(cid:98)s − Π

(r2)(x)(cid:98)g(x)dx

(r2)(x)Π (r2) (x){(cid:98)s(x) − 1}(cid:98)g(x)dx

(cid:17)
The second line follows because
(cid:98)s − Π
expressed as b(x)T β for some β. Therefore,

(cid:98)ΠM

(cid:16)

(r2) belongs to the ﬁnite dimensional subspace and can be

(cid:90) (cid:16)

(cid:17)

(cid:98)ΠM

(cid:98)s − Π

(r2)(x)(cid:98)ΠM

(cid:98)s(r2)(x)(cid:98)g(x)dx = βT

= βT

= βT

= βT

(cid:90)

=

(cid:90)

(cid:90)

(cid:90)

(cid:90)

b(x)(cid:98)ΠM

(cid:98)s(r2)(x)(cid:98)g(x)dx
(cid:90)

b(x)b(x)T (cid:98)Ω−1

b(u)(cid:98)s(u)r2(u)(cid:98)g(u)du(cid:98)g(x)dx

b(u)r2(u)g(u)du
(cid:90)

b(x)b(x)T Ω−1

b(u)r(u)g(u)du(cid:98)s(x)(cid:98)g(x)dx

βT b(x)Π(r2)(x)(cid:98)s(x)(cid:98)g(x)dx

By Cauchy-Schwarz:

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(r2)

(cid:13)
2
(cid:13)
(cid:13)

2,(cid:98)g

≤

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(r2)

(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

(cid:107)Π (r2) ((cid:98)s − 1)(cid:107)2,(cid:98)g ,

implying

This then yields

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)ΠM

(cid:98)s − Π

(cid:17)

(r2)

(cid:13)
(cid:13)
(cid:13)2,(cid:98)g

(cid:46) (cid:107)(cid:98)s − 1(cid:107)∞(cid:107)r2(cid:107)g.

|T2| (cid:46) (cid:107)r1(cid:107)g(cid:107)r2(cid:107)g(cid:107)(cid:98)s − 1(cid:107)m−1
∞ .

The bounds on the terms involving ∆j derived in Step 1 ﬁnally yield the result:

|T2| (cid:46)

(cid:16)

(cid:107)v(cid:107)g(cid:107)q(cid:107)g + hα∧β(cid:17)

(cid:107)(cid:98)s − 1(cid:107)m−1

∞

36

C.2 Variance

The proof of the variance bound follows as in Robins et al. [2017]. Because, for two random variables U1
and U2, var(U1 + U2) ≤ 2var(U1) + 2var(U2), and because m is ﬁxed, we have:

var






Pn (cid:98)f0(Z) +

m
(cid:88)

j=2

Un (cid:98)ϕj(Z1, . . . , Zj) | Dn






≤ 2var

Pn (cid:98)f0(Z) | Dn(cid:111)
(cid:110)




m
(cid:88)

+ 2var

j=2

Un (cid:98)ϕj(Z1, . . . , Zj) | Dn






(cid:46) var

Pn (cid:98)f0(Z) | Dn(cid:111)
(cid:110)

+ P n

(cid:46) var

Pn (cid:98)f0(Z) | Dn(cid:111)
(cid:110)

+

m
(cid:88)

j=2










P n (cid:104)

Un (cid:98)ϕj(Z1, . . . , Zj)

2








m
(cid:88)

j=2

{Un (cid:98)ϕj(Z1, . . . , Zj)}2(cid:105)

Because, given Dn, Pn (cid:98)f0(Z) is a sample average of independent observations, we have
Pn (cid:98)f0(Z) | Dn(cid:111)
(cid:110)

1
n
ht(a)π(a | x)da = h−1 (cid:82) K2(u)π(uh + t)du (cid:46) h−1. By Lemma 14.1 in Robins et al. [2017], the

0 (Z)} (cid:46) 1
nh

P{ (cid:98)f 2

var

≤

since (cid:82) K2
following holds

P n (cid:104)

{Un (cid:98)ϕj(Z1 . . . , Zj)}2(cid:105)

(cid:18)

≤ 2j

1 +

(cid:19)2j

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
p
(cid:13)
(cid:13)
(cid:98)p
(cid:13)∞

(cid:98)P n (cid:104)

{Un (cid:98)ϕj(Z1 . . . , Zj)}2(cid:105)

Because (cid:98)ϕj is degenerate relative to (cid:98)P , we also have

(cid:98)P n (cid:104)

{Un( (cid:98)ϕj(Z1 . . . , Zj))}2(cid:105)

(cid:46)

(cid:46)

1
n(n − 1) · · · (n − j + 1)
1
n(n − 1) · · · (n − j + 1)

Notice that

{ (cid:98)ϕj(Z1 . . . , Zj)}2(cid:105)
(cid:98)P j (cid:104)
(cid:20)(cid:110)

(cid:98)P j

f1(Z1)(cid:98)Π1,2Kht(A2) · · · (cid:98)Πj−1,jf2(Zj)

(cid:111)2(cid:21)

(cid:110)

(cid:111)2

f1(Z1)(cid:98)Π1,2Kht(A2) · · · (cid:98)Πj−1,jf2(Zj)

(cid:46) K2

ht(A1)(cid:98)Π2

1,2K2

ht(A2) · · · (cid:98)Π2

j−1,jK2

ht(Aj)

since h(a, x), Y and (cid:98)µ(a, x) are uniformly bounded by assumption. Next, because

(cid:90)

K2

ht(a)π(a | x)da = h−1

(cid:90)

K2(u)π(uh + t | x)du (cid:46) h−1,

we have
(cid:20)(cid:110)

(cid:98)P j

f1(Z1)(cid:98)Π1,2Kht(A2) · · · (cid:98)Πj−1,jf2(Zj)

(cid:111)2(cid:21)

(cid:90)

(cid:90)

(cid:46) h−j

= h−j

(cid:98)Π2

1,2 · · · (cid:98)Π2

j−1,jp(x1) · · · p(xj)dx1 · · · dxj

(cid:98)Π2

1,2 · · · (cid:98)Π2

j−1,j

(cid:41)

(cid:40) j

(cid:89)

l=1

p(xl)
(cid:98)g(xl)

(cid:98)g(x1) · · · (cid:98)g(xj)dx1 · · · dxj

(cid:27)j

(cid:90)

h−j

(cid:98)Π2

1,2 · · · (cid:98)Π2

j−1,j(cid:98)g(x1) · · · (cid:98)g(xj)dx1 · · · dxj

(cid:46) h−j

1,2 · · · (cid:98)Π2

j−1,j(cid:98)g(x1) · · · (cid:98)g(xj)dx1 · · · dxj

(cid:26)

≤

sup
x

(cid:90)

p(x)
(cid:98)g(x)
(cid:98)Π2

37

Next, notice that

(cid:90)

(cid:98)Π2(xi, xj)(cid:98)g(xj)dxj =

(cid:90)

b(xi)T (cid:98)Ω−1b(xj)b(xj)T (cid:98)Ω−1b(xi)(cid:98)g(xj)dxj = (cid:98)Π(xi, xi)

We bound each term from i = j to i = 3 as

(cid:98)Π2

i−2,i−1 (cid:98)Π2

i−1,i(cid:98)g(xi−1)(cid:98)g(xi)dxi−1dxi =

(cid:90)

(cid:98)Π2

i−2,i−1 (cid:98)Πi−1,i−1(cid:98)g(xi−1)dxi−1

(cid:90)

(cid:98)Π2

1,2 · · · (cid:98)Π2

j−1,j(cid:98)g(x1) · · · (cid:98)g(xj)dx1 · · · dxj ≤

(cid:90)

≤ sup

x

(cid:98)Π(x, x)

(cid:98)Π2

i−2,i−1(cid:98)g(xi−1)dxi−1

(cid:26)

sup
x

(cid:27)j−2 (cid:90)

(cid:98)Π(x, x)

(cid:98)Π2

1,2(cid:98)g(x1)dx1(cid:98)g(x2)dx2

(cid:90)

This leads to

Finally, without loss of generality, let b(x) be scaled so that (cid:98)Ω is the identity matrix. This way, we
immediately have

(cid:90)

(cid:98)Π2

1,2(cid:98)g(x1)(cid:98)g(x2)dx1dx2 =

(cid:90)

b(x)T b(x)(cid:98)g(x)dx =

k
(cid:88)

(cid:90)

i=1

b2
i (x)(cid:98)g(x)dx = k

because the basis is orthonormal. Because m is ﬁxed and does not grow with n and supx (cid:98)Π(x, x) (cid:46) k, this
yields the bounds in the statement of the theorem.

D Proofs of claims from Section 4

D.1 Proof of Lemma 2

We prove the result for the upper bound, as that for the lower bound can be proven with a similar argument.
By Leibniz rule of integration, the derivative of the map q(a, x) (cid:55)→ E{su(Z; q) | A = a, X = x) is

d
dq

(cid:26)

q +

1
γ

(cid:90) q

−∞

(y − q)f (y | A = a, X = x)dy + γ

(cid:90) ∞

q

(y − q)f (y | A = a, X = x)dy

= 1 −

1
γ

P (Y ≤ q | A = a, X = x) − γP (Y ≥ q | A = a, X = x)

Similarly, the second derivative is

(cid:27)

(cid:27)

(cid:90) q

(y − q)f (y | A = a, X = x)dy + γ

(cid:90) ∞

q

(y − q)f (y | A = a, X = x)dy

f (y | A = a, X = x)dy − γ

f (y | A = a, X = x)dy

(cid:27)

(cid:90) ∞

q

d2
dq2

(cid:26)

q +

1
γ

(cid:26)

1 −

=

d
dq
1
γ

−∞
(cid:90) q
1
γ

−∞

= −

f (q | A = a, X = x) + γf (q | A = a, X = x)

= O(1)

Notice that the ﬁrst derivative vanishes at the true quantile q(a, x) = qu(a, x). Therefore, by a second
order Taylor expansion, it holds that

|E{s(Z; (cid:98)qu) − s(Z; qu) | A = a, X = x}| (cid:46) {(cid:98)qu(a, x) − qu(a, x)}2

38

Next, notice that

(cid:90)

(cid:98)ru(t) =

+

(cid:90)

=

+

(cid:98)w(t, x)[E{s(Z; (cid:98)qu) | A = t, X = x} − (cid:98)κu(t, x)]dP(x | A = t)
(cid:90)

w(t, x){(cid:98)κu(t, x) − κu(t, x)}dP(x | A = t) + (Pn − P)(cid:98)κu(t, X; (cid:98)qu)

(cid:98)w(t, x)[E{s(Z; (cid:98)qu) | A = t, X = x} − κu(t, x)]dP(x | A = t)
(cid:90)

{w(t, x) − (cid:98)w(t, x)}{(cid:98)κu(t, x) − κu(t, x)}dP(x | A = t) + (Pn − P)(cid:98)κu(t, X; (cid:98)qu)

The bound then follows by the Cauchy-Schwarz inequality.

D.2 Proof of Proposition 4

We prove the result for the upper bound, as the proof for the lower bound is analogous. Let (cid:98)E(· | A = t)
denote the second-stage regression based on linear smoothing. Deﬁne

(cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, w, qu) = ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu) − w(A, X)[Y − (cid:98)qu(A, X)]

γsgn{Y −(cid:98)qu(A,X)} − γsgn{Y −qu(A,X)}(cid:105)
(cid:104)

,

where

ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu) = (cid:98)w(A, X){su(Z; (cid:98)qu) − (cid:98)κu(A, X; (cid:98)qu)} +

1
n

n
(cid:88)

i=1

(cid:98)κu(A, Xi; (cid:98)qu)

We have ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu) ≥ (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, w, qu) and, deterministically by assumption,

(cid:98)E{ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu) | A = t} ≥ (cid:98)E{ (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, w, qu) | A = t}.

Let

ϕu(Z; w, κu, qu, qu) = (cid:101)ϕu(Z; w, κu, qu, qu) −

1
n

n
(cid:88)

i=1

(cid:90)

(cid:98)κu(A, Xi; (cid:98)qu) +

κu(A, x; qu)dP(x)

= w(A, X){su(Z; qu) − κ(A, X; qu)} +

(cid:90)

κ(A, x; qu)dP(x)

− w(A, X)[Y − qu(A, X)]

γsgn{Y −qu(A,X)} − γsgn{Y −qu(A,X)}(cid:105)
(cid:104)

and notice that, because E[γ{Y −qu(t,x)} | A = t, X = x] = 1:

E {ϕu(Z; w, κu, qu, qu) | A = t, X = x} =

(cid:90)

κ(t, x; qu)dP(x)
(cid:104)

− w(t, x)E

(cid:105)
{Y − qu(t, x)}γsgn{Y −qu(t,x)} | A = t, X = x

(cid:104)

Y γsgn{Y −qu(t,x)} | A = t, X = x

+ w(t, x)E
(cid:90)

=

κ(t, x; qu)dP(x) − w(t, x)κ(t, x; qu)

Therefore, E {ϕu(Z; w, κu, qu, qu) | A = t} = θu(t; γ).

+ w(t, x)E

(cid:104)

Y γsgn{Y −qu(t,x)} | A = t, X = x

39

− w(t, x)qu(t, x)

(cid:105)

(cid:105)

By the reasoning in Kennedy [2020] and used to prove Proposition 2, one has

(cid:98)E{ (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, qu) | A = t} − θu(t; γ) = (cid:98)E{ (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, qu) | A = t} − E{ϕu(Z; w, κu, qu, qu) | A = t}

= OP (Ru(t)) +

1
n

n
(cid:88)

i=1

Wi(t; An)E { (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, w, qu) − ϕu(Z; w, κu, qu, w, qu) | Ai, Dn}

provided that supz| (cid:101)ϕu(z; (cid:98)w, (cid:98)κu, (cid:98)qu, qu)−ϕu(z; w, κu, qu, qu)|= oP(1). This is the case, because (cid:98)w is consistent
for w, (cid:98)κu is consistent for κu and (cid:98)q is consistent for q.
Next, recall that

θu(A; γ) = E{ϕu(Z; w, κu, qu, w, qu) | A} =

(cid:90)

W (A, x)E{Y γsgn{Y −qu(A,X)} | A, X = x}dP(x | A)

so that, because E[γsgn{Y −qu(A,X)} | A, X] = 1:

E

(cid:16)

w(A, X)[Y − (cid:98)qu(A, X)]γsgn{Y −qu(A,X)} − ϕu(Z; w, κu, qu, w, qu) | Ai, Dn(cid:17)

(cid:90)

= −

w(Ai, x)(cid:98)qu(Ai, x)dP(x | Ai)

In turns, this means that

(cid:16)

E

−w(A, X)[Y − (cid:98)qu(A, X)]

(cid:104)

γsgn{Y −(cid:98)qu(A,X)} − γsgn{Y −qu(A,X)}(cid:105)

− ϕu(Z; w, κu, qu, w, qu) | Ai, Dn(cid:17)

= −E{w(Ai, X)su(Z; (cid:98)qu) | Ai, Dn)

yielding

(cid:98)b(Ai) ≡ E{ (cid:101)ϕu(Z; (cid:98)w, (cid:98)κu, (cid:98)qu, w, qu) − ϕu(Z; w, κu, qu, w, qu) | Ai, Dn}

(cid:90)

(cid:90)

=

=

+

{ (cid:98)w(Ai, x) − w(Ai, x)}[E{su(Z; (cid:98)qu) | Ai, x} − (cid:98)κu(Ai, x; (cid:98)qu)]dP(x | Ai) + (Pn − P)(cid:98)κu(Ai, X; (cid:98)qu)

{ (cid:98)w(Ai, x) − w(Ai, x)}[E{su(Z; (cid:98)qu) | Ai, x} − κu(Ai, x; qu)]dP(x | Ai)
(cid:90)

{ (cid:98)w(Ai, x) − w(Ai, x)}{κu(Ai, x; qu) − (cid:98)κu(Ai, x; (cid:98)qu)}dP(x | Ai) + (Pn − P)(cid:98)κu(Ai, X; (cid:98)qu)

As shown in Dorn et al. [2021] (Lemma 5), the map q (cid:55)→ su(Z; q) is Lipschitz. Therefore, by Cauchy-
Schwarz:

(cid:12)
(cid:12)(cid:98)E{(cid:98)b(A) | A = t}
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:46) sup
a∈Nt

[(cid:107) (cid:98)w − w(cid:107)a{(cid:107)(cid:98)qu − qu(cid:107)a+(cid:107)(cid:98)κu − κu(cid:107)a} + |(Pn − P)(cid:98)κu(a, X; (cid:98)qu)|]

40

