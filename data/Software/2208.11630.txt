2
2
0
2

g
u
A
4
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
0
3
6
1
1
.
8
0
2
2
:
v
i
X
r
a

Flash-X, a multiphysics simulation software instrument

A. Dubeya,h,∗, K. Weideh,a, J. O’Neala, A. Dhruva,f, S. Couchc, J.A. Harrisb,
T. Klostermana, R. Jaina, J. Rudia, O.E.B. Messerb,l, M. Pajkosc, J.
Carlsonc, R. Chul, M. Wahibn, S. Chawdharya, P.M. Rickerd, D. Leee, K.
Antypasg, K.M. Rileya, C. Daleyg, M. Ganapathyi, F.X. Timmesj, D.M.
Townsleym, M. Vanellak, J. Bachang, P.Richa, S. Kumarh, E. Endeveb,l, W.
R. Hixb,l, A. Mezzacappal, T. Papatheodoreb

aArgonne National Laboratory, Lemont, IL, 60439, USA
bOak Ridge National Laboratory, Oak Ridge, TN, 37831, USA
cMichigan State University
dUniversity of Illinois, Urbana Champaign
eUniversity of California, Santa Cruz
fGeorge Washington University
gLawrence Berkeley National Laboratory
hUniversity of Chicago, IL, 60637, USA
iGoogle Inc
jArizona State University
kNational Institute of Standards and Technology
lUniversity of Tennessee, Knoxville, TN, 37996, USA
mUniversity of Alabama, Tuscaloosa, AL, 35487, USA
nRIKEN, Kobe, Japan

Abstract

Flash-X is a highly composable multiphysics software system that can

∗Corresponding author at: Argonne National Laboratory, 9600 S. Cass Ave, Lemont,

IL, 60439

Email addresses: adubey@anl.gov (A. Dubey ), kweide@uchicago.edu (K. Weide),
joneal@anl.gov (J. O’Neal), adhruv@anl.gov (A. Dhruv), scouch@msu.edu (S. Couch),
harrisja@ornl.gov (J.A. Harris), tklosterman@anl.gov (T. Klosterman),
jain@anl.gov (R. Jain), jrudi@anl.gov (J. Rudi), bronson@ornl.gov (O.E.B. Messer),
mapajkos@gmail.com (M. Pajkos), jaredc.scholar@gmail.com (J. Carlson),
rchu@vols.utk.edu (R. Chu), mohamed.attia@riken.jp (M. Wahib),
saurabh.chawdhary@gmail.com (S. Chawdhary), pmricker@illinois.edu (P.M.
Ricker), dlee79@ucsc.edu (D. Lee), kantypas@lbl.gov (K. Antypas),
riley@alcf.anl.gov (K.M. Riley), csdaley@lbl.gov (C. Daley), murali@google.com
(M. Ganapathy), fxt44@mac.com (F.X. Timmes), dean.m.townsley@ua.edu (D.M.
Townsley), marcos.vanella@nist.gov (M. Vanella), john.bachan@gmail.com (J.
Bachan), richp@alcf.anl.gov (P.Rich), shravan2915@gmail.com (S. Kumar),
endevee@ornl.gov (E. Endeve), raph@ornl.gov (W. R. Hix), mezz@tennessee.edu (A.
Mezzacappa), papatheodore@ornl.gov (T. Papatheodore)

Preprint submitted to SoftwareX

August 25, 2022

 
 
 
 
 
 
be used to simulate physical phenomena in several scientiﬁc domains.
It
derives some of its solvers from FLASH, which was ﬁrst released in 2000.
Flash-X has a new framework that relies on abstractions and asynchronous
communications for performance portability across a range of increasingly
heterogeneous hardware platforms. Flash-X is meant primarily for solving
Eulerian formulations of applications with compressible and/or incompress-
It also has a built-in, versatile Lagrangian framework
ible reactive ﬂows.
that can be used in many diﬀerent ways, including implementing tracers,
particle-in-cell simulations, and immersed boundary methods.

Keywords:
Multiphysics, Simulation software, high-performance computing,
performance portability

Metadata

C3

C4
C5
C6

C7

Nr. Code metadata description
C1
C2

Current code version
Permanent link to code/repository
used for this code version
Code capsule

Legal Code License
Code versioning system used
Software code languages, tools, and
services used
Compilation requirements, operat-
ing environments and dependencies

C8 Developer documentation/manual

C9

Support email for questions

1.0
https://github.com/Flash-X/Flash-
X
https://github.com/Flash-X/
Workﬂows/tree/main/incompFlow/
FlowBoiling
Apache 2.0
git
Fortran, C, C++, Python3, MPI,
OpenMP, OpenACC, HDF5
Unix, Linux, OSX based compil-
ers for languages and libraries men-
tioned above
https://ﬂash-x.org/
pages/documentation/
ﬂash-x-users@lists.cels.anl.gov

Table 1: Code metadata. Please note that the code repository is private only
because our funding agency requires us to keep a list of people who obtain
the code directly from our repo. Anyone can furnish their github id and be
added to the list of collaborators.

2

1. Motivation and Signiﬁcance

Flash-X [1] is a new incarnation of FLASH [2, 3], a multiphysics soft-
ware system that has been used by multiple science communities. Flash-X is
meant for use beyond existing FLASH science communities. It is designed to
be easily adaptable for use by any computational scientists who rely upon dif-
ferential equations as their primary mathematical model with ﬁnite-volume
or ﬁnite-diﬀerence discretization. FLASH was designed only for a homoge-
neous, distributed-memory parallel model with bulk-synchronism, which has
rendered it unsuitable for use on many newer system architectures that are
heavily reliant on disparate memory spaces (e.g., accelerators). This diﬃ-
culty is further exacerbated by increasing heterogeneity in hardware as well
as solvers within the code. Flash-X has a fundamentally redesigned archi-
tecture that uses abstractions and asynchronous operations for performance
portability across a variety of platforms, both with and without accelera-
tors. Our design is forward-looking in that it makes minimal assumptions
about which parallelization or memory models are likely to be prevalent in
future platforms. The design relies upon self-describing code components
of varying granularity and a toolchain that can interpret the metadata of
the code components to synthesize application instances. The synthesis is
done partly through assembly, partly through code translation, and partly
through code generation. Some code assembly features have been imported
from FLASH, but have been signiﬁcantly enhanced to discretize components
at a ﬁner scope than subroutines or functions. Tools for code translation and
runtime management are new and will enable orchestration of computation
and data movement between distinct compute devices on a node.

In addition to the new architecture, Flash-X has newer and higher- ﬁdelity
physics solvers. Most notable among these are Spark [4] for magnetohydrody-
namics, XNet [5, 6] for nuclear burning, thornado [7, 8] for neutrino radiation
transport, and WeakLib [9, 10, 11] for tabulated microphysics. Additionally,
Flash-X can support multiphase ﬂow through a level-set method, which did
not exist in FLASH releases [12]. Flash-X has been exercised on small clus-
ters at Argonne National Laboratory and on leadership-class machines at
Oak Ridge National Laboratory and Argonne National Laboratory. Flash-X
will showcase the key performance parameters of ExaStar [13], a project un-
der the Exascale Computing Project[14, 15] (ECP), through a core-collapse
supernova (CCSN) simulation on exascale machines to be deployed by the
US Department of Energy. To run eﬀectively at scale, Flash-X will rely upon
the toolchain described above. Some components of the toolchain are em-
bedded in Flash-X, while others are encapsulated into independent libraries
that can be used by other codes. Note that compilation and execution of

3

the code do not require using these external libraries; they are used only to
orchestrate data movement and computation for better performance.

Along with a new architecture, Flash-X also adopts a community-based,
open development model. The stewardship of the code is guided by a Council
representing all the major science communities of FLASH/Flash-X. More
details of our community development model are available at https://ﬂash-
x.org.

2. Software Description

The Flash-X code is a component-based software system for simulation
of multiphysics applications that can be formulated largely as a collection
of partial and ordinary diﬀerential equations (PDEs and ODEs), as well as
algebraic equations. The equations are discretized and solved on a domain
that can have uniform resolution (UG) or adaptive mesh reﬁnement (AMR).
In Flash-X, one can select between PARAMESH [16], an octree-based library
written in Fortran, or AMReX [17, 18], a highly-ﬂexible, patch-based, C++
AMR library. Both AMR frameworks can interface to math libraries such
as hypre [19] and PETSc [20], making those solvers available to Flash-X.
Physics units are designed to be oblivious of domain decomposition. Bulk of
their code is written for block-by-block update, interspersed with invocation
of ﬁne-coarse boundary resolution related API functions of the Grid unit as
needed.

Hyperbolic equations are solved using explicit methods commonly used
for compressible ﬂows with strong shocks, described in Section 2.2. For ellip-
tic equations, one can either use an included multipole solver [21], AMReX’s
multigrid solver, or an interface to one of the math libraries. For parabolic
equations, one must rely upon library interfaces.

The maintained code components are written in a combination of high-
level languages such as Fortran, C, and C++, with an embedded domain-
speciﬁc conﬁguration language (DSCL) that also supports Flash-X custom
macros. The DSCL permits multiple alternative deﬁnitions of macros with
a built-in arbitration mechanism to select the appropriate deﬁnition for an
instance of code assembly. The accompanying conﬁguration toolchain can
translate and assemble diﬀerent combinations of the components to conﬁgure
a diverse set of applications. Flash-X has been designed from the outset to
be performant with increasing heterogeneity of both the platforms and the
solvers within the code.

The code uses the Message-Passing Interface (MPI) library for commu-
nication between nodes, though more than one MPI rank can also be placed
on a node. HDF5 is the default mode for IO. Support for OpenMP, both for

4

threading and for oﬄoading to accelerators, is built into several, though not
all, of the solvers.

2.1. Software Architecture

Flash-X has composable components with accompanying metadata that
can express, for example, inter-component dependency and exclusivity, nec-
essary state variables, etc. The metadata is encapsulated within the code
components by accompanying conﬁg ﬁles and is parsed and interpreted by
the conﬁguration tool, Setup. Setup parses conﬁg ﬁles recursively, aggre-
gates requisite components, and assembles a complete application. It also
assembles the compilation/make system and runtime parameters for each
component included in the application. The Setup tool also implements
code inheritance through a combination of keywords in the conﬁg ﬁles and
the Unix directory structure instead of using programming language sup-
ported inheritance mechanisms. When Setup parses the source tree, it treats
each subdirectory as inheriting all of the ﬁles in its parent’s directory. While
source ﬁles at a given level of the directory hierarchy override ﬁles with the
same name at higher levels, conﬁg ﬁles accumulate all deﬁnitions encoun-
tered. The schematic for inheritance is shown in Figure 1.

Figure 1: Schematic for the implementation of inheritance in Flash-X. Handling of inher-
itance and variants assumes three lists: one for ﬁles, one for macros, and one for runtime
parameters. The ﬂowchart in the right box gives details of how keys and runtime param-
eters are updated as the source tree is traversed.

In Flash-X parlance, the highest-level code component for a speciﬁc type
of functionality is called a unit. Units can have subunits. A unit includes an

5

Sub units Update FilelistUpdate Makefile.unitUpdate REQVisit UnitGenerate Makefile.unitApply ReorderGenerate Makefile.SimUpdate macrosand Runtime ParametersIndex ReorderMore REQUpdate FilelistUpdate macrosand Runtime ParametersEndYesNoNoYesYesPut all files in the object DirAll  Keys DoneGet next key from .inifilesAdd to listReplace on listIs it on the listYesNoNoYesEndAll  RP DoneGet next RP from ConfigIs it on the listReplace valueAdd to list And initializeNoNoYesYesAPI accessible to the whole code through which it interacts with other units
and the driver. While each subunit can have its own sub-components with no
restriction on how ﬁne-grained they can become, the general rule of thumb
is to keep them as coarse-grained as feasible for ease of maintenance. A
unit can have multiple alternative implementations, one of which is required
to be a null implementation.
If a unit is not needed in a simulation, the
null implementation is included. This feature facilitates maintaining very
few implementations of the main driver while permitting many combinations
of capabilities to be included in an application. Any code component can
have multiple alternative implementations, though unlike the unit-level API,
lower-level components do not require null implementations.

A diﬀerent mechanism is used when a code component needs to become
smaller than a function or a subroutine. Here, we rely on macros to imple-
ment alternative deﬁnitions of an operation, including the null case. The
inheritance mechanism shown in Figure 1 arbitrates on which deﬁnition to
select. The macros may also have arguments, be inlined, and be recursive.
This mechanism serves two purposes. The ﬁrst is for developer convenience.
Certain code patterns repeat often in the code – for example, invocation of
iterators, bounds for loop-nests, and bounds for arrays. We have provided
macros for such repeated patterns, and developers can use these at their
discretion. Macros make the code compact, reduce cut-and-paste errors,
and help to clarify the control ﬂow and semantics of the code. The second,
more powerful motivation is that with alternative deﬁnitions, we can generate
many variants of a code component from the same source. This functionality
is particularly useful when diﬀerent control ﬂow is more suitable for diﬀer-
ent compute devices. We can keep arithmetic expressions invariant while
using macros for the control ﬂow, or vice-versa, thus not only eliminating
code duplication but also keeping the maintained code more compact. The
schematic for generating variants from a single source where specializations
are obtained through alternative macro deﬁnitions is shown Figure in 2.

2.2. Software Functionalities

The Flash-X distribution includes solvers for compressible and incom-
pressible ﬂuids, several methods for handling equations of state (EOS), source
terms for nuclear burning, several methods for computing eﬀects of gravity,
level-set methods for multiphase ﬂow, and several others. The primary for-
mulation for PDEs in Flash-X is Eulerian, although a versatile Lagrangian
framework is also included that can be conﬁgured to do computations such
as tracers, particles-in-cell, immersed boundaries, etc. The vast majority of
applications using Flash-X include some form of hydrodynamics or magne-
tohydrodynamics in their conﬁguration. However, it is possible to conﬁgure

6

Figure 2: Schematic for generating variants from single source using inheritance and
macros.

applications that completely bypass those solvers.
Magnetohydrodynamics and Hydrodynamics: a compressible magne-
tohydrodynamics/hydrodynamics solver with second- or third-order strong
stability preserving (SSP) Runge-Kutta (RK) time integration (Spark) [4],
another compressible hydrodynamics solver with a predictor-corrector for-
mulation [22, 23], and an incompressible hydrodynamics solver with ﬂuid-
structure interaction [24] are included in the distribution. All of the solvers
can be used in 1-, 2-, or 3- dimensional conﬁgurations.
Equations of State: the code supports several EOS versions suitable for
a range of regimes in astrophysical ﬂows. The simplest one is a perfect-gas
EOS with a multispecies variant. Another implementation with two variants
uses a fast Helmholtz free-energy table interpolation to handle degenerate
relativistic electrons and positrons and also includes radiation pressure and
ions (via the perfect gas approximation) [25].
Nuclear Burning: three nuclear reaction networks of varying numbers of
species are included in the distribution. Approx-13 and approx-19 [26] are
inherited from FLASH. XNet is a standalone code for evolving astrophysical
nuclear burning and is generalizable to arbitrarily large networks as needed
for improved physical ﬁdelity of some applications.
Gravity: the gravitational potential can be treated very simply as constant,
or through a Poisson solve using a multipole or multigrid method depending
upon the symmetry of the density ﬁeld.
Particles: this component of the code forms the basis for the Lagrangian

7

DoneGet next fileIs file F90-mcHas no variantsConvert fileIs file in the listAdd to listReplace on listNull option Variants doneConvert fileAppend filenameGet variant macrosfrom .inifilesGet next variant from ConfigIs file in the listAdd to listReplace on listYesNoNoNoNoNoNoNoYesYesYesYesYesEndYesRemove variant macrosfrom listUpdate filesframework [27]. Particles maintain their own spatial coordinates and are
independently integrated in time. They interact with the Eulerian mesh ei-
ther to obtain physical quantities needed for their advancement or to deposit
quantities such as mass, charge, or energy to the mesh, depending on usage.
Incompressible Fluid Dynamics: this component of the code solves in-
compressible Navier-Stokes equations for single and multiphase ﬂow sim-
ulations with options for heat transfer and phase transitions [12]. The
Navier-Stokes solver is implemented using a fractional-step temporal inte-
gration scheme that uses Poisson solver for pressure. Multiphase interfaces
are tracked with a level-set function and use ghost-ﬂuid methods to account
for forces due to surface tension and mass transfer [28]. The eﬀect of solid
bodies on the ﬂuid is modeled using an immersed boundary method that uses
Lagrangian particles[29].
Importable Modules: Flash-X uses GitHub’s submodules to import some
capabilities that are independently developed and hosted in their own repos-
itories. These include WeakLib for tabulated, nuclear EOS and neutrino-
matter interaction rates, and thornado for spectral neutrino radiation trans-
port.

3. Illustrative Examples

We describe two example simulations using Flash-X from two diﬀerent
science communities. The ﬁrst is a CCSN simulation that uses compressible
hydrodynamics, nuclear EOS, neutrino radiation transport, and self-gravity
solvers. The second is a subcooled ﬂow boiling simulation that uses multi-
phase incompressible Navier-Stokes and heat advection diﬀusion solver.

We perform a CCSN simulation in spherical symmetry, initiated with
a low-mass pre-collapse progenitor star previously modeled throughout all
stages of stellar evolution [30]. Electron-type neutrinos and anti-neutrinos are
evolved using thornado’s two-moment neutrino transport solver and Weak-
Lib’s tabulated nuclear EOS [31] and neutrino-matter interaction rates [32].
Compressible hydrodynamics are evolved with Spark, and Newtonian self-
gravity is computed using the multipole Poisson solver. For a more detailed
description of the physics included, see [33]. Figure 3 shows the evolution
of the ratio of electrons to baryons (electron fraction) versus radius during
a critical epoch in the simulation that spans the formation of the primary
shock-wave during core “bounce” — the phenomena of infalling matter col-
liding with, and bouncing oﬀ of, the newly-formed neutron-star.

Figure 4 provides details for the subcooled ﬂow-boiling simulation which
was designed to replicate experiments performed at diﬀerent gravity levels by
[34]. These computations used the multiphase incompressible
Lebon et al.

8

Figure 3: The electron fraction is plotted versus stellar radius for various times (relative
to the bounce-time, tb) in a CCSN simulation.

Navier–Stokes solver along with the phase transition capability, and were
preformed at a resolution almost twice the previous state-of-the-art [35, 28].
Liquid coolant ﬂows over a heater surface with a mean velocity U0, leading
to phase-change and formation of vapor bubbles. These vapor bubbles grow,
merge, and ﬁnally depart the heater surface due to buoyancy which intro-
duces turbulence in the domain. The heat transfer associated with this turbu-
lence is an important parameter in designing cooling systems for automotive
and industrial components, but is diﬃcult to quantify through experimental
observations/measurements. With Flash-X we are able to address this chal-
lenge through targeted high-ﬁdelity simulations to quantify the contribution
of turbulent heat ﬂux.

4. Impact

FLASH has been an inﬂuential code for computing astrophysical ﬂows
almost since its inception. FLASH’s scientiﬁc impact is clearly demonstrated
by the citation history of the original paper describing the code, as shown in
Figure 5. Analysis in [36, 37] further quantiﬁes the scientiﬁc signiﬁcance and
impact of the code on science. FLASH has not only been used extensively
for science, it has also been among the pioneers in giving due importance to
software quality and adopting rigorous auditing and productivity practices
[38].

In recent years, FLASH’s use has been diminishing in several communities
because of its inability to use accelerators eﬀectively. Flash-X is designed
to ﬁll this gap and become a reliable multiphysics simulation code for the

9

Figure 4: Example of a ﬂow boiling simulation with Flash-X. Liquid coolant ﬂows over a
heater surface with a mean velocity U0, leading to phase-change and formation of vapor
bubbles. The bubble dynamics introduce turbulence which enhances heat transfer between
the heater surface and coolant, shown by temperature (T ) contours

communities that earlier relied on FLASH. At least two major communities of
FLASH users, stellar astrophysics[4, 33] and ﬂuid-structure interactions[12],
are already transitioning to Flash-X, with some users now exclusively using
Flash-X. These use cases have also reported on performance gains with the
use of GPUs. Not all of FLASH’s physics capabilities are available in Flash-
X yet. However, since Flash-X is open source, it is expected that interested
users will assist in transitioning their capabilities of interest to the Flash-X
architecture and help grow the Flash-X community. Additionally, the new
tool-chain for orchestration of data and work movement is still in the early
stages of being exercised. Preliminary performance studies of the runtime
tool have been very encouraging [39]. It is expected that full performance
gains will have been realized by the next major release.

5. Conclusions

Sustained funding under the ECP has permitted modernization of a
highly capable community code for current and future platforms. With
Flash-X, the FLASH science communities can embrace heterogeneity and
use available hardware eﬀectively. With the move to an open, community-

10

Figure 5: Citation history of the original FLASH paper from Google Scholar and Astro-
physics Data System (ADS).

based development model, users are assured of continuity and support for the
code without depending on a single funding source. FLASH has had a long
history of scientiﬁc discovery, and Flash-X aims to follow in that tradition.
With more modern solvers and ﬂexible architecture, Flash-X can continue to
be a very useful resource for science domains that rely on modeling of partial
diﬀerential equations.

6. Conﬂict of Interest

We wish to conﬁrm that there are no known conﬂicts of interest associated
with this publication and there has been no signiﬁcant ﬁnancial support for
this work that could have inﬂuenced its outcome.

Acknowledgements

The authors acknowledge all contributors to the Flash-X code, including

contributors to the FLASH code from whose work Flash-X has inherited.

This work was supported by the U.S. Department of Energy Oﬃce of
Science Oﬃce of Advanced Scientiﬁc Computing Research under contract
number DE-AC02-06CH1137.

This research was supported by the Exascale Computing Project (17-SC-
20-SC), a collaborative eﬀort of two U.S. Department of Energy organiza-
tions (Oﬃce of Science and the National Nuclear Security Administration)
that are responsible for the planning and preparation of a capable exascale

11

0204060801001201401601802002002200520082011201420172020ScholarADSecosystem, including software, applications, hardware, advanced system en-
gineering, and early testbed platforms, in support of the nation’s exascale
computing imperative.

This research used resources of the Oak Ridge Leadership Computing
Facility, which is a DOE Oﬃce of Science User Facility supported under
Contract DE-AC05-00OR22725.

References

[1] Flash-X.

URL https://github.com/Flash-X/Flash-X

[2] A. Dubey, K. Antypas, M. K. Ganapathy, L. B. Reid, K. Riley,
D. Sheeler, A. Siegel, K. Weide, Extensible component-based archi-
tecture for FLASH, a massively parallel, multiphysics simulation code,
Parallel Computing 35 (10-11) (2009) 512–522. doi:10.1016/j.parco.
2009.08.001.

[3] B. Fryxell, K. Olson, P. Ricker, F. Timmes, M. Zingale, D. Lamb,
P. MacNeice, R. Rosner, J. W. Truran, H. Tufo, FLASH: An adap-
tive mesh hydrodynamics code for modeling astrophysical thermonu-
clear ﬂashes, The Astrophysical Journal Supplement Series 131 (2000)
273. doi:10.1086/317361.

[4] S. M. Couch, J. Carlson, M. Pajkos, B. W. O’Shea, A. Dubey, T. Kloster-
man, Towards performance portability in the spark astrophysical mag-
netohydrodynamics solver in the ﬂash-x simulation framework, Parallel
Computing 108 (2021) 102830.

[5] XNet, https://github.com/starkiller-astro/xnet (Apr. 2022).

[6] W. R. Hix, F. K. Thielemann, Computational methods for nucleosyn-
thesis and nuclear energy generation., Journal of Computational and
Applied Mathematics 109 (1999) 321–351.

[7] R. Chu, E. Endeve, C. D. Hauck, A. Mezzacappa, Realizability-
preserving DG-IMEX method for the two-moment model of fermion
transport, Journal of Computational Physics 389 (2019) 62–93. doi:
10.1016/j.jcp.2019.03.037.

[8] M. P. Laiu, E. Endeve, R. Chu, J. A. Harris, O. E. B. Messer, A DG-
IMEX Method for Two-moment Neutrino Transport: Nonlinear Solvers
for Neutrino-Matter Coupling, The Astrophysical Journal Supplement
Series 253 (2) (2021) 52. doi:10.3847/1538-4365/abe2a8.

12

[9] WeakLib,
2022).

https://github.com/starkiller-astro/weaklib (Apr.

[10] D. Pochik, B. L. Barker, E. Endeve, J. Buﬀaloe, S. J. Dunham,
N. Roberts, A. Mezzacappa, thornado-hydro: A discontinuous galerkin
method for supernova hydrodynamics with nuclear equations of state,
The Astrophysical Journal Supplement Series 253 (1) (2021) 21. doi:
10.3847/1538-4365/abd700.

[11] R. E. Landﬁeld, Sensitivity of neutrino-driven core-collapse supernova
models to the microphysical equation of state, Ph.D. thesis, University
of Tennessee, https://trace.tennessee.edu/utk{_}graddiss/5294
(2018).

[12] A. Dhruv, E. Balaras, A. Riaz, J. Kim, An investigation of the grav-
ity eﬀects on pool boiling heat transfer via high-ﬁdelity simulations,
International Journal of Heat and Mass Transfer 180 (2021) 121826.
doi:10.1016/j.ijheatmasstransfer.2021.121826.

[13] Exastar, multi-physics stellar astrophysics at exascale.
URL https://sites.google.com/lbl.gov/exastar

[14] The Exascale Computing Project (2020).

URL https://www.exascaleproject.org/

[15] F. Alexander, A. Almgren, J. Bell, A. Bhattacharjee, J. Chen, P. Colella,
D. Daniel, J. DeSlippe, L. Diachin, E. Draeger, A. Dubey, T. Dunning,
T. Evans, I. Foster, M. Francois, T. Germann, M. Gordon, S. Habib,
M. Halappanavar, S. Hamilton, W. Hart, Z. (Henry) Huang, A. Hunger-
ford, D. Kasen, P. R. C. Kent, T. Kolev, D. B. Kothe, A. Kron-
feld, Y. Luo, P. Mackenzie, D. McCallen, B. Messer, S. Mniszewski,
C. Oehmen, A. Perazzo, D. Perez, D. Richards, W. J. Rider, R. Rieben,
K. Roche, A. Siegel, M. Sprague, C. Steefel, R. Stevens, M. Syamlal,
M. Taylor, J. Turner, J.-L. Vay, A. F. Voter, T. L. Windus, K. Yelick,
Exascale applications: skin in the game, Philosophical Transactions of
the Royal Society A: Mathematical, Physical and Engineering Sciences
378 (2166) (2020) 20190056. doi:10.1098/rsta.2019.0056.

[16] P. MacNeice, K. Olson, C. Mobarry, R. de Fainchtein, C. Packer,
PARAMESH: A parallel adaptive mesh reﬁnement community toolkit,
Computer Physics Communications 126 (3) (2000) 330–354.

[17] Amrex, https://amrex-codes.github.io/ (2020).

13

[18] W. Zhang, A. Almgren, V. Beckner, J. Bell, J. Blaschke, C. Chan,
M. Day, B. Friesen, K. Gott, D. Graves, et al., AMReX: A framework for
block-structured adaptive mesh reﬁnement, JOSS 4 (37) (2019) 1370–
1370.

[19] R. Falgout, U. Yang, hypre: A library of high performance precondi-

tioners, Computational Science-ICCS 2002 (2002) 632–641.

[20] S. Balay, et al., Petsc web page (2001).

[21] S. M. Couch, C. Graziani, N. Flocke, An Improved Multipole Approxi-
mation for Self-gravity and Its Importance for Core-collapse Supernova
Simulations, The Astrophysical Journal 778 (2) (2013) 181.

[22] D. Lee, A. E. Deane, An unsplit staggered mesh scheme for multidi-
mensional magnetohydrodynamics, Journal of Computational Physics
228 (4) (2009) 952–975.

[23] D. Lee, A solution accurate, eﬃcient and stable unsplit staggered mesh
scheme for three dimensional magnetohydrodynamics, Journal of Com-
putational Physics 243 (2013) 269–292.

[24] M. Vanella, P. Rabenold, E. Balaras, A direct-forcing embedded-
boundary method with adaptive mesh reﬁnement for ﬂuid–structure in-
teraction problems, Journal of Computational Physics 229 (18) (2010)
6427–6449.

[25] F. X. Timmes, F. D. Swesty, The Accuracy, Consistency, and Speed of
an Electron-Positron Equation of State Based on Table Interpolation
of the Helmholtz Free Energy, The Astrophysical Journal Supplement
Series 126 (2) (2000) 501–516. doi:10.1086/313304.

[26] F. Timmes, Integration of nuclear reaction networks, The Astrophysical

Journal Supplement Series 124 (1999) 241–263.

[27] A. Dubey, C. Daley, J. ZuHone, P. M. Ricker, K. Weide, C. Graziani, Im-
posing a Lagrangian particle framework on an Eulerian hydrodynamics
infrastructure in FLASH, The Astrophysical Journal Supplement Series
201 (2012) 27. doi:10.1088/0067-0049/201/2/27.

[28] A. Dhruv, E. Balaras, A. Riaz, J. Kim, A formulation for high-ﬁdelity
simulations of pool boiling in low gravity, International Journal of Mul-
tiphase Flow 120 (2019) 103099. doi:10.1016/j.ijmultiphaseflow.
2019.103099.

14

[29] M. Vanella, E. Balaras, Short note: A moving-least-squares reconstruc-
tion for embedded-boundary formulations, J. Comput. Phys. 228 (18)
(2009) 6617–6628. doi:10.1016/j.jcp.2009.06.003.

[30] T. Sukhbold, T. Ertl, S. Woosley, J. M. Brown, H.-T. Janka, Core-
collapse supernovae from 9 to 120 solar masses based on neutrino-
powered explosions, The Astrophysical Journal 821 (1) (2016) 38.

[31] A. W. Steiner, J. M. Lattimer, E. F. Brown, The equation of state from
observed masses and radii of neutron stars, The Astrophysical Journal
722 (1) (2010) 33.

[32] S. W. Bruenn, Stellar core collapse - numerical model and infall epoch,
The Astrophysical Journal Supplement Series 58 (1985) 771–841. doi:
10.1086/191056.

[33] J. A. Harris, R. Chu, S. M. Couch, A. Dubey, E. Endeve, A. Georgiadou,
R. Jain, D. Kasen, M. P. Laiu, O. B. Messer, et al., Exascale models of
stellar explosions: Quintessential multi-physics simulation, The Inter-
national Journal of High Performance Computing Applications (2021)
10943420211027937.

[34] M. T. Lebon, C. F. Hammer, J. Kim, Gravity eﬀects on sub-
cooled ﬂow boiling heat transfer, International Journal of Heat and
Mass Transfer 128 (2019) 700–714. doi:https://doi.org/10.1016/
j.ijheatmasstransfer.2018.09.011.

[35] Y. Sato, B. Niceno, Pool boiling simulation using an interface tracking
method: From nucleate boiling to ﬁlm boiling regime through critical
heat ﬂux, International Journal of Heat and Mass Transfer 125 (2018)
876 – 890. doi:10.1016/j.ijheatmasstransfer.2018.04.131.

[36] A. Dubey, P. Tzeferacos, D. Q. Lamb, The dividends of investing in
computational software design: A case study, The International Journal
of High Performance Computing Applications 33 (2) (2019) 322–331.

[37] A. Grannan, K. Sood, B. Norris, A. Dubey, Understanding the landscape
of scientiﬁc software used on high-performance computing platforms,
The International Journal of High Performance Computing Applications
34 (4) (2020) 465–477.

[38] A. Dubey, K. Antypas, A. C. Calder, C. Daley, B. Fryxell, J. B. Gal-
lagher, D. Q. Lamb, D. Lee, K. Olson, L. B. Reid, et al., Evolution of
FLASH, a multi-physics scientiﬁc simulation code for high-performance

15

computing, The International journal of high performance computing
applications 28 (2) (2014) 225–237.

[39] J. O’Neal, M. Wahib, A. Dubey, K. Weide, T. Klosterman, J. Rudi,
Domain-speciﬁc runtime to orchestrate computation on heterogeneous
platforms, in: R. Chaves, D. B. Heras, A. Ilic, D. Unat, R. M. Badia,
A. Bracciali, P. Diehl, A. Dubey, O. Sangyoon, S. L. Scott, L. Ricci
(Eds.), Euro-Par 2021: Parallel Processing Workshops, Springer Inter-
national Publishing, Cham, 2022, pp. 154–165.

16

