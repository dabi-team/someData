Data-Centric Green AI
An Exploratory Empirical Study

Roberto Verdecchia∗, Lu´ıs Cruz†, June Sallou‡, Michelle Lin§, James Wickenden¶, Estelle Hotellier(cid:107)
∗Vrije Universiteit Amsterdam, The Netherlands - r.verdecchia@vu.nl
†Delft University of Technology, The Netherlands - l.cruz@tudelft.nl
‡Univ Rennes, France - june.benvegnu-sallou@irisa.fr
§McGill University, Canada - michelle.lin2@mail.mcgill.ca
¶University of Bristol, United Kingdom - jw17943@bristol.ac.uk
(cid:107)Inria, France - estelle.hotellier@inria.fr

2
2
0
2

r
p
A
7

]

G
L
.
s
c
[

2
v
6
6
7
2
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—With the growing availability of large-scale datasets,
and the popularization of affordable storage and computational
capabilities, the energy consumed by AI is becoming a growing
concern. To address this issue,
in recent years, studies have
focused on demonstrating how AI energy efﬁciency can be
improved by tuning the model training strategy. Nevertheless,
how modiﬁcations applied to datasets can impact the energy
consumption of AI is still an open question.

To ﬁll this gap, in this exploratory study, we evaluate if data-
centric approaches can be utilized to improve AI energy efﬁ-
ciency. To achieve our goal, we conduct an empirical experiment,
executed by considering 6 different AI algorithms, a dataset
comprising 5,574 data points, and two dataset modiﬁcations
(number of data points and number of features).

Our results show evidence that, by exclusively conducting
modiﬁcations on datasets, energy consumption can be drastically
reduced (up to 92.16%), often at the cost of a negligible or even
absent accuracy decline. As additional introductory results, we
demonstrate how, by exclusively changing the algorithm used,
energy savings up to two orders of magnitude can be achieved.
this exploratory investigation empirically
demonstrates the importance of applying data-centric techniques
to improve AI energy efﬁciency. Our results call for a research
agenda that focuses on data-centric techniques, to further enable
and democratize Green AI.

In conclusion,

Index Terms—Energy Efﬁciency, Artiﬁcial Intelligence, Green

AI, Data-centric, Empirical Experiment

I. INTRODUCTION

We live in the era of artiﬁcial intelligence (AI): new intelli-
gent technologies are emerging every day to change people’s
lives. Many organizations identiﬁed the massive potential of
using intelligent solutions to create business value. Hence, in
the past years, the modus operandi is collecting as much data
as possible so that no opportunity is missed. Data science
teams are constantly looking for problems where AI can
be applied to existing data to train models that can provide
more personalized and optimized solutions to their operations
customers and operations [1].

Nevertheless, the energy consumption of developing AI ap-
plications is starting to be a concern. Previous studies observed
that AI-related tasks are particularly energy-greedy [2], [3].
In fact, since 2012, the amount of computing used for AI
training has been doubling every 3.4 months [4]. Controversy
has risen around particular machine learning models that have

been estimated to consume the energy equivalent of a trans-
American ﬂight [5]. Hence, a new subﬁeld is emerging to
make the development and application of AI technologies
environmentally sustainable: Green AI [6].

On a related note, the current research practice of collecting
massive amounts of data is not necessarily yielding better
results. Being able to collect high-quality data is more impor-
tant than collecting big data – a trend coined as Data-centric
AI1. Instead of creating learning techniques that squeeze every
bit of performance, data-centric AI focuses on leveraging
systematic, reliable, and efﬁcient practices to collect high-
quality data.

Therefore, in this study, we conduct an exploratory empir-
ical study on the intersection of Green AI and Data-centric
AI. We investigate the potential impact of modifying datasets
to improve the energy consumption of training AI models. In
particular, we focus on machine learning, the branch of AI that
deals with the automatic generation of models based on sample
data – machine learning and AI are used interchangeably
throughout this paper. In addition to investigating the energy
impact of dataset modiﬁcations, we also analyze the inherent
trade-offs between energy consumption and performance when
reducing the size of the dataset – either in the number of
data points or features. Moreover, the analysis is performed
in six state-of-the-art machine learning models applied in the
detection of Spam messages.

Our results show that feature selection can reduce the energy
consumption of model training up to 76% while preserving
the performance of the model. The improvement in energy
efﬁciency is more impressive when reducing the number of
data points: up to 92% in the case of Random Forest. However,
in this case, it is not cost-free: the trade-off between energy and
performance needs to be considered. Finally, we also show that
KNN tends to be the most energy-efﬁcient algorithm, while
ensemble classiﬁers tend to be the most energy greedy.

This paper provides insights to deﬁne the most relevant
and energy-efﬁcient modiﬁcations of datasets used during
the development of the AI models while ensuring minimal

1Understanding Data-Centric AI: https://landing.ai/data-centric-ai/. Ac-

cessed 24th January 2022.

 
 
 
 
 
 
accuracy loss. We argue that more research in Data-centric AI
will help more practitioners in developing green AI models.
this is the ﬁrst study to
To the best of our knowledge,
explore the potential of preprocessing data to reduce the energy
consumption of AI.

The entirety of our experimental scripts and results are made
available with an open-source license, to enable the indepen-
dent veriﬁcation and replication of the results presented in this
study: https://github.com/GreenAIproject/ICT4S22.

The remainder of this paper is structured as follows. Sec-
tion II presents the related work on the energy consumption
of Artiﬁcial Intelligence models. Section III details the overall
approach and the study design. Section IV describes the
results of the experimentation according to the different dataset
modiﬁcations, and Section V presents the related discussion.
The threats to the validity of this study are thoroughly analyzed
in Section VI. Finally, Section VII documents our conclusions
and future work.

II. RELATED WORK

Previous work has addressed the energy consumption of
software systems across different domains, levels and ecosys-
tems. There is ongoing research investigating how different
frameworks [7], data structures [8], programming languages
[9], [10], and so on, affect the energy consumption of soft-
ware. The main outputs of the research in this ﬁeld –– also
known as Green Software – aim at providing developers
with informed advice on how to design, develop, and deploy
their systems [11]–[16]. Some works have also attempted
at providing tools to help developers automatically improve
the energy efﬁciency of their code [17], [18]. Despite the
numerous contributions in this ﬁeld, only a handful of studies
address the energy efﬁciency of AI-based systems [19].

While numerous studies focus on utilizing AI to address
sustainability concerns [20], [21], only a few investigate how
the sustainability of AI itself can be improved. Strubell et al.
provide a clear landscape that motivates a research agenda
in AI that considers their energy consumption [2]. They pin-
point concrete cases of energy-intensive AI applications and
compare the carbon emissions of training Natural Language
Processing (NLP) models to ordinary daily tasks – e.g., a car
commute or air travel. Their results showcase that training
a state-of-the-art NLP model can generate as much carbon as
ﬁve cars during their entire lifespan (including fuel). Although
our work also analyzes the energy consumption of training AI
models, we aim at identifying trade-off decisions that can be
generalized to other AI projects to reduce energy consumption.
In a similar direction, Schwartz et al. present the dichotomy
between Red AI and Green AI. While traditional (Red) AI
only aims to improve accuracy metrics, Green AI includes
computational cost as a performance metric. Green AI favors
the selection of algorithms that have comparable accuracy
while consuming less energy. In their work, Schwartz et al.
highlight the need for more research in the area of Green AI,
showcasing the exponential growth of computational power
required to train models over the past six years. Our work

follows their call for a new research agenda in AI that
brings energy consumption into the landscape of training an
AI model. However, we take a step further by empirically
investigating the potential of using data-centric over model-
centric approaches to enable Green AI.

More research has been calling for a new research agenda in
AI. Bender et al. [5] provide a list of high-level recommenda-
tion to mitigate the unprecedented growth in the size of state-
of-the-art NLP models. Recommendations include investing
resources to curate datasets and reﬂecting on the potential
risks entailed by models before developing them, to address
AI sustainability. A different work reported concrete numbers
on how the growth of AI is impacting the entire infrastructure
of datacenters which need to grow in bandwidth, data storage,
and power capacity [22]. While not focusing directly on AI
sustainability, in other studies, researchers investigated the
impact that utilizing smaller models [23] or down sampled
datasets [24] can have on accuracy. Our study paves the way
in directly addressing AI sustainability concerns by providing
empirical evidence on how dataset modiﬁcations can be used
to drastically save AI model training energy at a negligible
accuracy loss.

Martin et al. [25] focused on studying the energy consump-
tion of a speciﬁc machine learning algorithm, namely the Very
Fast Decision Tree (VFDT). The authors analyzed the energy
consumption of VFDT at the function level, investigating how
different parameters affect the energy consumption across all
functions of the training algorithm. Their results demonstrate
how function-level energy proﬁling can lead to improvements
of up to 70% in energy efﬁciency with minimal impact on
the accuracy of the algorithm. In our research, we consider
six different machine learning algorithms rather than a single
one, as pinpointed in Section III-D1. Besides, we investigate
for the ﬁrst time if data-centric approaches can improve the
energy efﬁciency of machine learning algorithms.

Previous work has studied the impact of machine learning
algorithms in the context of mobile applications [26]. The
authors compare eight mobile implementations of well-known
training algorithms (e.g., k-Nearest Neighbor, Decision Trees,
etc.) in terms of accuracy and energy consumption. In sum,
the work shows that 1) energy consumption is often related
to the algorithmic complexity of the algorithms, and 2) to
achieve optimal energy efﬁciency practitioners ought to factor
in application-speciﬁc variables – e.g., whether the model
needs to be regularly updated. Our work differentiates by
1) focusing on general-purpose implementations of machine
learning algorithms rather than mobile-based ones and 2)
providing a thorough analysis of the impact of the input data
in the energy consumption of training a model.

Finally, a recent study analyzed the energy consumption
of using different deep learning frameworks – namely, Py-
Torch and TensorFlow [27]. Results suggest that TensorFlow
the training stage,
achieves better energy performance at
while PyTorch is more energy-efﬁcient at the inference stage.
Our work differs by approaching energy efﬁciency from a
data-centric perspective rather than a comparative analysis of

different frameworks and libraries.

III. STUDY DESIGN AND EXECUTION

In this section we document

the empirical experiment
executed for this study, in terms of goal (Section III-A), re-
search questions (Section III-B), study subject (Section III-C),
experimental procedure (Section III-D), and data analysis
(Section III-E).

A. Goal

The aim of this research is to conduct an investigation
inﬂuences the energy consumption of AI-based
into what
systems. More formally, by utilizing the Goal-Question-Metric
approach [28], this objective can be described as follows:
Analyze the energy consumption of model training
For the purpose of identifying the impact
With respect to dataset modiﬁcations
From the viewpoint of software practitioners and researchers
In the context of artiﬁcial intelligence.

B. Research Questions

In order to achieve our goal, we address the following three

research questions (RQ):
RQ1 Do AI algorithms differ in terms of energy consump-

tion?

By answering this introductory research question, we aim at
understanding if AI algorithms impact differently the energy
consumption of their underlying hardware, and in the afﬁrma-
tive case, the extent of this difference. The results gathered
for this ﬁrst research question allow us to gain sufﬁcient
knowledge on potential energy consumption difference of AI
algorithms through which the following research questions,
focusing on data-centric green AI, can be assessed.
RQ2 Does modifying the dataset impact the energy efﬁciency

of AI algorithms?

While RQ1 focuses on the potential difference in energy
consumption of algorithms, with RQ2 we explicitly focus
on data-centric green AI, i.e., if modiﬁcations of the dataset
used by the algorithms can impact their energy consumption.
Speciﬁcally, we split RQ2 into two sub-RQs to study the
potential impact of different facets of the dataset on the energy
consumption of AI algorithms:
RQ2.1 Does the size of the dataset impact the energy con-

sumption of AI algorithms?

RQ2.2 Does the number of features impact the energy con-

sumption of AI algorithms?

With RQ2.1 we aim at understanding if utilizing only a
portion of a dataset, instead of its entirety, can lead to a
signiﬁcant energy consumption difference of AI algorithms.
Similarly, with RQ2.2, we study if varying the number of
features, i.e., the dimensionality of the dataset, can lead to
a signiﬁcant energy consumption variation.

While improving the energy efﬁciency of AI algorithms is
at the core of our investigation, ensuring that energy efﬁciency
improvements do not drastically deteriorate the effectiveness

of AI algorithms, and hence defy their ﬁnal purpose,
is
paramount. In order to systematically address this concern with
our ﬁnal research question, we investigate potential trade-offs
between energy efﬁciency and algorithm accuracy (in terms
of F1-score). This is expressed in RQ3 as follows:
RQ3 Can we improve the energy efﬁciency of AI algorithms
through a data-centric approach without compromising
their accuracy?

C. Experimental Subject

In order to answer our RQs, we consider as experimental
subject the SMS Spam Collection dataset [29]. The SMS
Spam Collection is a dataset of labeled SMS messages
collected for mobile phone spam research. The complete
dataset is made publicly available at the University of Cal-
ifornia Irvine Machine Learning Repository2 and comprises
5,574 text message instances, labeled either as legit (“ham”
label, 4,827 instances) or spam (“spam” label, 747 instances).
The dataset is also made available via the data science platform
Kaggle3, where it was downloaded over 86,8K times, and used
in more than 700 Jupiter notebook projects.

To preprocess our dataset, i.e., prepare the raw SMS Spam
Collection data for the subsequent “ham”/“spam” classi-
ﬁcation, we make use of widely adopted standard techniques.
Speciﬁcally, given that the SMS Spam Collection entails
a text classiﬁcation problem, we use a method involving
term frequency–inverse document frequency (tf-idf), whereby
words are tokenized based on their appearance in the dataset,
and subsequently the term-frequency metric for each token
is calculated. To execute the tokenization and term-frequency
calculation, we utilize the standard implementation as provided
in the Python package scikit-learn 1.0.4 In total, the
dataset includes 8169 features (i.e., 8168 token occurrence
frequencies and a last feature corresponding to the length of
the SMS messages).

In order to train and test our models, we utilize a 70%/30%
train/test split. We do not allocate a portion of the dataset
for validation purposes since, as further discussed in the
threats to validity section (Section VI), model optimization
via hyperparameter tuning falls outside the scope of this
investigation.

D. Experimental Procedure

1) Experimental design: Our controlled empirical experi-
ment is characterized by a set of Dependent Variables (DV )
and Independent Variables (IV ). We design the experiment
as a set of treatments,
i.e., “sub-experiments” considering
a speciﬁc combination of independent variable values.5 For
each sub-experiment, we exclusively vary one independent

2https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. Accessed

3rd January 2022.

3https://www.kaggle.com/uciml/sms-spam-collection-dataset. Accessed 3rd

January 2022.

4https://scikit-learn.org/stable/modules/generated/sklearn.feature
extraction.text.TﬁdfTransformer.html. Accessed 5th January 2022.

5While independent variable values vary among sub-experiments, the same

set of dependent variables are collected for all sub-experiments.

variable, while ﬁxing all the other ones to a default level.
This allows us to independently study the potential impact
that each independent variable has on our dependent variables,
while allowing us to adopt a straightforward and transparent
research design.
In addition,

to answer our research questions, we are
required to adopt a blocking factor, namely the factor AI
algorithm (IV1). This entails that our sub-experiments are
divided into different sets (or blocks), according to the speciﬁc
utilized AI algorithm.

More speciﬁcally, in order to answer RQ1, we consider the
entire experimental dataset by ﬁxing the number of data points
(IV2) and the number of features (IV3) to their default level
(i.e., 100%), while exclusively varying the used AI algorithm
(IV3). This allows us to compare the energy consumption of
AI algorithms (DV1) in their “default” setting, i.e., without
carrying out any ad hoc manipulation of the original dataset.
To study the impact of dataset size (RQ2.1) instead, we
vary both the used AI algorithm (IV3) and the number of
data points (IV2), by keeping the number of features (IV3)
to its default value. This allows to study the impact
that
the number of data points, i.e., the size of the dataset, has
on the energy consumption of each algorithm (DV1), while
avoiding potential variation of experimental measurements due
to different numbers of features.

Similarly, to answer (RQ2.2), we vary the used AI algo-
rithm (IV1) and the number of features (IV3), while ﬁxing
the number of used data points (IV2) to its default value. This
enables us to investigate the potential impact that the number
of features has on the execution of AI algorithms (DV1), while
avoiding the potential impact on energy consumption due to
variations of the number of data points.

Finally,

to answer (RQ3), we apply both experimental
techniques employed to answer RQ2, i.e., we vary AI al-
gorithms (IV1) and alternatively either the number of data
points (IV2) or number of features (IV3), while ﬁxing the
other independent variable (IV3 or IV2) to its default value.
This approach allows us to study independently the impact that
the number of data point and the number of features have on
accuracy (DV2), while also enabling us to consider the data
collected for RQ2 to systematically answer this last RQ.

To ensure we gather statistically signiﬁcant data, and to
mitigate potential threats to internal validity, we repeat the
execution of each sub-experiment 30 times. In addition, to
mitigate the impact of potential confounding factors (e.g., an
unnoticed execution of a background process affecting our
energy measurements), rather than simply repeating sequen-
tially the 30 executions of a sub-experiment, we shufﬂe the
executions of sub-experiments uniformly at random.

An additional confounding factor may arise from the tem-
perature of the utilized hardware. To mitigate this threat, prior
to the execution of our experiment, we perform a dummy
CPU-intensive warm-up operation, carried out by calculating
a Fibonacci sequence for approximately 5 seconds, and hence
ensure that the hardware is not experiencing a “cold boot”
when the ﬁrst execution is run.

Finally, to avoid the potential inﬂuence of subsequent runs
on our energy measurements, we introduce a sleep time equal
to 5 seconds between each run, to allow the hardware to cool
down, and execute all runs under the same initial hardware
conditions.

2) Experimental Variables: Our experiment is character-
ized by a total of 3 independent variables and 2 dependent
variables.

Independent Variables (IVs). The independent variables
of our experiment, i.e., the factors we adopt, and their cor-
responding values, are reported below. The default value of
each independent variable (see Section III-D1) is distinguished
with an under strike (except for the AI algorithm independent
variable, as it is our experimental blocking factor).

• AI Algorithm (IV1): Support-Vector Machine,
Decision Tree, Multinomial Naive Bayes,
K-Nearest Neighbour,
Random Forest,
Adaptive Boost, Bagging Classifier.

• Number of data points (IV2): 10%, 20%, 30%, . . . ,
100% of the total number of data points. To select data
points, we adopt stratiﬁed sampling, and pick points of
our population uniformly at random from each stratum
(i.e., messages labeled as “spam” and “ham”).

• Number of features (IV3): 10%, 20%, 30%, . . . , 100%
of the total number of features. To select features, we
adopt the Chi-Square Test (Chi2) [30], to ensure that only
the most relevant features are considered for each level.
The set of AI algorithms (IV1) was chosen by considering
the most popular ones. We use the implementation provided
in the Python library scikit-learn6, which was used to
implement the algorithms for this study. The discretization step
size of the number of data points and features (10%) was in-
stead adopted to ensure sufﬁcient granularity of results, while
guaranteeing an a-priori feasible number of experimental runs.
Dependent Variables (DVs). In terms of metrics used to
answer our research questions, i.e., our observed dependent
variables, we consider the following ones in our experiment:
• Energy consumption (DV1): the energy consumed by
the hardware on which the AI algorithms are executed,
measured in Joules (J);

• F1-score (DV2): Overall accuracy measure of the model,
P +R , where P is the model precision,

deﬁned as F 1 = 2∗ P ∗R
and R the model recall.

The energy consumption (DV1), measured during the ex-
ecution of AI algorithms, is the dependent variable used to
answer RQ1 and RQ2. The F1-score (DV2) is instead adopted
to answer RQ3. The F1-score is chosen over precision (P ) and
recall (R) metrics, as it allows us to gain an encompassing
summary overview of the overall accuracy of AI algorithms,
while overcoming potential representation problems due to the
uneven distribution of labels present in the dataset used (see
Section III-C).

6https://scikit-learn.org/stable/whats new/v1.0.html#version-1-0-0.

Accessed 3rd January 2022.

3) Experimental Setting: All sub-experiments are run on a
machine equipped with a 2.4GHz Quad-Core i5 processor and
16 GB 2133 MHz LPDDR3 of memory. The entirety of the
experiment and data analysis is implemented in Python 3.10.7
In order to measure energy consumption (DV1), we leverage
codecarbon8, a Python package allowing to estimate the
energy consumption of code running on Intel and AMD CPU
processors. All the AI algorithms (IV1) follow the implemen-
tation as provided in the Python package scikit-learn
1.0, and use the standard hyperparameters as deﬁned in the
library.

In total, by considering the combination of independent
variables and sub-experiment repetitions, 3.6K experimental
runs are executed to gather data to answer our research
questions.

E. Data Analysis

In this section, we report the data analysis procedure that

we adopt to derive our results from the gathered data.

As a preliminary step, in order to assess if the energy
consumption (DV1) data we collected is normally distributed,
we carry out a visual normality assessment by means of
quantile-quantile (Q-Q) plot, followed by a Shapiro-Wilk
normality test. From the inspection of the generated Q-Q plot,
and the Shapiro-Wilk test result (W =0.52 and p-value=2.2e-
16), we can conﬁdently conclude that the data collected is
not normally distributed. Hence, for each sub-experiment, we
sample the data gathered in the run reporting the median
energy consumption value. Subsequently, in order to evaluate
if a correlation exists between our dependent and independent
variables, we leverage the calculation of the one-tailed Spear-
man’s rank correlation coefﬁcient (ρ). We adopt Spearman’s ρ
as it provides a non-parametric measure, and can be used to
calculate the potential correlation between our ordinal (IV1-
IV3) and continuous variables (DV1 and DV2). Finally, to pro-
vide further insights into our results, we calculate percentage
changes to summarize the difference in energy consumption
and F1-scores between different algorithms, number of data
points, and number of features.

IV. RESULTS

In this section, we report

the results of our empirical
experimentation according to the research questions guiding
this study (see Section III-B).

A. Results RQ1: Energy Consumption Variability of AI Algo-
rithms

With our ﬁrst research question, we aim at investigating
the potential difference between the energy consumption of
AI algorithms. An overview of the median consumption of
each AI algorithm, as measured in our empirical experiment,
is depicted in Figure 1.

7https://www.python.org/downloads/release/python-3100/. Accessed 3rd

January 2022.

8https://github.com/mlco2/codecarbon. Accessed 3rd January 2022.

Fig. 1. Median Energy Consumption of AI Algorithms

By inspecting Figure 1, we can immediately notice that the
energy consumption drastically varies among AI algorithms.
More speciﬁcally, Random Forest results to be the most
energy greedy algorithm, with a median energy consump-
tion of 1.98 Joules per run, followed by AdaBoost, which
nevertheless resulted to consume less than half (48.9%) of
the energy required by Random Forest. The most energy
efﬁcient algorithm results to be KNN, which reports a median
energy consumption of 0.01 Joules, followed by Decision Tree,
which requires 0.12 Joules. By considering minimum and
maximum variation values, we note that energy consumption
varies between algorithms from a minimum decrease of 20%
(Bagging Classiﬁer - SVM) up to a 99.49% decrease in energy
consumption (Random Forest - KNN).

B. Results RQ2: Impact of dataset modiﬁcations on energy
consumption

With RQ2, we aim at investigating if dataset modiﬁcations,
and more speciﬁcally the number of data points (RQ2.1) and
the number of features (RQ2.2), may have an impact on the
energy consumed by AI algorithms. An overview of the results
we collected for RQ2 are depicted in Figure 2, and are further
described below.

1) Results RQ2.1: Impact of the number of data points
on energy consumption: The ﬁrst row of diagrams reported
in Figure 2 depicts the median energy consumption of each
algorithm at a varying number of data points (reported on
the x-axis). As we can intuitively notice from the linear
regression lines reported in the plots, the energy consumption
appears to be correlated with the number of data points.
This observation is conﬁrmed by the Spearman’s rank corre-
lation coefﬁcient values reported in Table I. By considering
the ρ values reported in Table I, we note that there is a
deﬁnitive positive correlation between the number of data
points and the energy consumption, of either strong nature
(i.e., 0.70 ≤ ρ ≤ 0.89 for KNN, Random Forest, Bagging
Classiﬁer) or very strong nature (i.e., ρ ≥ 0.90 for SVM,

SVMDecisionTreeKNNRandomForestAdaBoostBaggingClassifierAlgorithm0.000.250.500.751.001.251.501.752.00Energy Consumption (Joules)0.560.120.011.980.970.70Fig. 2. RQ2 results. Small multiples showing the energy consumption of the different algorithms when using different data points (ﬁrst row) and when using
different numbers of features (second row).

TABLE I
CORRELATION ANALYSIS BETWEEN ENERGY CONSUMPTION (DV1) AND
NUMBER OF DATA POINT (IV2) OR NUMBER OF FEATURES (IV3)

Algorithm (IV1)
SVM
Decision Tree
KNN
Random Forest
AdaBoost
Bagging Classiﬁer
SVM
Decision Tree
KNN
Random Forest
AdaBoost
Bagging Classiﬁer

Indep. Variable
Num. data points (IV2)
Num. data points (IV2)
Num. data points (IV2)
Num. data points (IV2)
Num. data points (IV2)
Num. data points (IV2)
Num. features (IV3)
Num. features (IV3)
Num. features (IV3)
Num. features (IV3)
Num. features (IV3)
Num. features (IV3)

ρ
0.95
0.92
0.80
0.87
0.91
0.87
0.69
0.75
0.04
0.64
0.79
0.76

p-value
7.16e-151
9.58e-120
3.24e-68
4.25e-95
6.64e-115
3.07e-92
3.09e-43
2.29e-56
0.54
2.02e-36
6.01e-66
4.54e-58

Decision Tree, and AdaBoost). The corresponding p-values
showcase that the identiﬁed correlations are with very low
probability due to chance.

By considering the energy reduction achieved by using
fewer data points, we notice that this independent variable
(IV2) inﬂuences the considered AI algorithms differently, and
can lead to a maximum energy reduction ranging from 61.72%
(KNN) up to 92.16% (Random Forest).

2) Results RQ2.2: Impact of the number of features on
energy consumption: The second row of diagrams reported in
Figure 2 depicts the energy consumption for each algorithm at
a varying number of features (reported on the x-axis). From
the distribution of median energy consumption values, and
the linear regression lines, the number of features and the
energy consumption appear to be correlated for most algo-
rithms. The relationship is conﬁrmed by the Spearman’s rank
correlation coefﬁcient values ρ reported in Table I. In com-
parison with the number of data points (see Section IV-B1),
the number of features results to possess an overall weaker
positive correlation with the energy consumption, while still
being either strongly (i.e., 0.70 ≤ ρ ≤ 0.89, for Decision
Tree, AdaBoost, Bagging Classiﬁer) or moderately correlated

(0.40 ≤ ρ ≤ 0.69 for SVM and Random Forest). Interestingly,
varying the number of features does not noticeably affect the
energy consumed by KNN, by showcasing only a very weak
correlation (0.0 ≤ ρ ≤ 0.19), which was with high probability
dictated by chance (p-value=0.54).

For all algorithms other than KNN, the energy reduction
obtained by varying the number of features results to be lower
than the one obtainable by varying the number of data points,
while still being appreciable. As for the number of data points,
varying the number of features affects differently the energy
consumption of the considered AI algorithms. Interestingly,
for KNN, lowering the number of features leads in numerous
cases to a higher energy consumption w.r.t. the case of using
all features. In addition, the best energy efﬁciency achieved
by KNN by lowering the number of features results to be
only a 0.92% decrease. In comparison, the algorithm which
showcases the highest energy efﬁciency by varying the number
of features is AdaBoost, which achieves up to a 75.8% energy
reduction when compared to its baseline.

C. Results RQ3: Trade-offs between energy consumption and
accuracy

With RQ3, we aim at investigating if potential trade-offs
between AI energy efﬁciency and accuracy are possible. An
overview of the accuracy results, in terms of F1-score collected
via our empirical experiment, is reported in Figure 3. As
described in the ﬁgure, both by varying the number of data
points (IV2, ﬁrst row of Figure 3) or the number of features
(IV3, second row of Figure 3) we generally do not observe a
notable F1-score decrease (reported on the y-axis, Figure 3),
with both numbers of data points and features not being
correlated to F1-scores.

More detailed insights into the correlation analysis are
provided by the Spearman’s rank correlation coefﬁcient values
ρ reported in Table II. From the ρ values reported in the table,
we notice that, when considering the number of data points
as independent variable, most algorithms report only a very

1000200030004000Number of Datapoints0.00.10.20.30.40.5Energy Consumption (Joules)SVM10002000300040000.000.020.040.060.080.100.12Decision Tree10002000300040000.0000.0020.0040.0060.0080.010KNN10002000300040000.00.51.01.52.0Random Forest10002000300040000.00.20.40.60.81.0AdaBoost10002000300040000.00.10.20.30.40.50.60.7Bagging Classifier2000400060008000Number of Features0.00.10.20.30.40.5Energy Consumption (Joules)20004000600080000.000.020.040.060.080.100.1220004000600080000.0000.0020.0040.0060.0080.01020004000600080000.00.51.01.52.020004000600080000.00.20.40.60.820004000600080000.00.10.20.30.40.50.60.7TABLE II
CORRELATION ANALYSIS BETWEEN F1-SCORE (DV2) AND NUMBER OF
DATA POINTS (IV2) OR NUMBER OF FEATURES (IV3)

Algorithm (IV1)
SVM
Decision Tree
KNN
Random Forest
AdaBoost
Bagging Classiﬁer
SVM
Decision Tree
KNN
Random Forest
AdaBoost
Bagging Classiﬁer

Indep. Variable
no datapoints
no datapoints
no datapoints
no datapoints
no datapoints
no datapoints
no features
no features
no features
no features
no features
no features

ρ
-0.018
0.733
0.661
0.855
-0.006
0.661
N.D.
-0.042
0.954
0.541
0.585
0.316

p-value
0.960
0.016
0.038
0.002
0.987
0.038
N.D.
0.907
1.788e-05
0.106
0.075
0.374

weak correlation with F1-scores (for SVM and AdaBoost) or
a moderate correlation (for KNN and Bagging Classiﬁer). The
only exceptions are the algorithms Decision Tree and Random
Forest, both reporting a strong correlation between number of
data points and F1-score. The relative p-values indicate that
such correlation is statistically signiﬁcant, i.e., w.h.p. not due
to chance.

When considering the correlation between number of fea-
tures and F1-score, a different picture emerges. In fact, from
the ρ values reported in Table II, we can observe for most
algorithms that the number of features is correlated to the F1-
score either via a very weak correlation (for Decision Tree and
Bagging Classiﬁer), or a moderate one (for Random Forest and
AdaBoost). The ρ value is not deﬁnable (N.D.) for SVM, as
no variation is observed in F1-score values, i.e., the covariance
between number of features and F1-score is zero. Interestingly,
KNN is the only algorithm which reports a ρ value indicating
a very strong correlation between number of features and F1-
score. By inspecting the relative p-value, we can conclude that
such correlation is statistically signiﬁcant.

V. DISCUSSION

This empirical experiment provides exploratory evidence
of the potential of using data preprocessing techniques to
reduce the energy consumption of AI. Below, we answer each
research question by analyzing the results of our experimenta-
tion. Several conclusions are drawn with regard to the energy
consumption of AI models and the impact of the input dataset
modiﬁcations.

A. Do AI algorithms differ in terms of energy consump-
tion? (RQ1)

Yes, different training algorithms yield considerably differ-
ent energy footprints. The algorithm with the least energy
consumption is KNN, using almost 200× less energy than
Random Forest. However,
that does not necessarily mean
that KNN should always be chosen, as we prove later with
research questions RQ2 and RQ3. Nevertheless, the energy
consumption data collected with our experiment showcases the
importance of logging such information. Practitioners resort
to different performance metrics when selecting and tuning

models. In agreement with previous work [6], we argue that
practitioners will consider different models when they are
aware of these differences w.r.t. energy consumption. Hence,
selecting a machine learning model should be a trade-off
analysis encompassing not only accuracy metrics but also
energy metrics.

Random Forest, AdaBoost, and Bagging classiﬁers were the
most energy greedy algorithms. This is somehow expected
since they all belong to a class of algorithms known as ensem-
bles, which combine the results of training multiple classiﬁers
(a.k.a. weak learners) using slightly different parameters or
training datasets. In other words,
the energy consumption
of ensembles is equivalent to training multiple models: it is
affected by the number of weak learners being used internally
and their individual energy consumption.

To make energy metrics available to machine learning
practitioners, we need better and more accessible ways of
measuring energy consumption. As seen in this study, collect-
ing energy consumption is not a trivial task. We need simple
techniques to approximate energy consumption. Although this
is out of the scope of this study, other studies suggest looking
at duration, CPU usage, or the number of ﬂoating point
operations [2], [31], [32]. Ideally, metrics could estimate
energy consumption before even training the models – i.e.,
by using static analysis approaches.

The experimental nature of machine learning can also
magnify the energy consumption reported in this paper. Prac-
titioners have to retrain their models several times before
converging to a ﬁnal model. Previous studies have suggested
this to increase energy consumption by a factor of roughly
2000×: Strubell et al. [2] show that, while training one of their
natural language processing models has an electricity cost of
$5, the electricity cost of performing the full R&D required
to develop that model is estimated to be $9,870. Hence, small
improvements in energy efﬁciency in the early stages of the
pipeline can lead to large savings in the long run.

Main ﬁndings RQ1 (Algorithm Energy Consumption
Comparison): Different algorithms yield completely
different energy footprints. The difference goes up to
a 99.49% energy consumption decrease, with KNN
being the most energy-efﬁcient algorithm, and Random
Forest Energy the least energy-efﬁcient one. We ar-
gue that easy-to-use energy metrics are quintessential
when selecting the best model for a machine learning
project.

B. Does modifying the dataset impact the energy efﬁciency of
AI algorithms? (RQ2)

Yes, except for KNN, all algorithms yield less energy con-
sumption when we reduce the dimensionality of the dataset.
In other words, there is a positive correlation between the size
of the dataset and the measured energy consumption: using
fewer data leads to more energy efﬁciency. Improvements go
up to 92% when reducing the number of rows and up to

Fig. 3. RQ3 results. Small multiples showing the F1-score of the different algorithms when using different data points (ﬁrst row) and when using different
numbers of features (second row).

76% when reducing the number of features. Hence, instead
of collecting the biggest amount of data, we must aim for
smaller but meaningful datasets.

the

Our

importance of

results demonstrate

adding
data-centric Green AI as a key topic in the research agenda
of AI development. For example, recent work on core set
extraction (i.e., extracting the smallest subset that keeps the
key dataset properties) and dataset distillation have shown
promising results in AI applications [33]–[35]. We argue that
such strategies have a potential on Green AI that has been
overlooked in previous research.

Although this work focuses on the dimensionality of data,
it paves the way to study other properties of the input
data. For example, one can expect that the data types used
when loading the data into memory lead to different energy
footprints when training the model. However, this kind of
control is not fully supported by AI libraries/frameworks. For
example, the widely utilized library adopted for this study,
scikit-learn, automatically converts all data to ﬂoating-
point with 64 bits (as of version 0.24.2). Users have no way
of intervening in this data transformation. It is not yet known
whether using data types with unnecessarily high precision
can lead to unnecessary energy costs. We argue that this is
a missed opportunity in AI libraries. This is ampliﬁed if we
consider IoT systems, where small devices are used to collect,
process, and transmit data. Depending on the use case, these
devices may operate with different precision levels – e.g., 16
or 32 bits. Developers of AI libraries ought to reconsider some
design choices, to give back control to their users and enabling
further energy efﬁciency opportunities.

Based on our results, we foresee potential

in studying
data-centric techniques to democratize Green AI. Several AI-
leading organizations are aiming to be carbon-free by 2030.
This requires massive investments in infrastructure and is far
from being a realistic norm for the rest of the AI industry.
For example, previous work on Green AI bring awareness to
the importance of using energy-efﬁcient hardware, datacenters

in locations with better access to clean energy, etc. [3], [36].
While such measures are important, they might be inaccessible
to most practitioners and organizations that operate on tight
budgets. Our results show that, with very simple techniques
available to any AI practitioner, one can effectively reduce the
carbon footprint of developing AI models.

It is important to note that there is an energy consump-
tion overhead when we manipulate the number of rows and
columns. We did not factor in this overhead as we focus on
studying the impact of the dataset shape and not preprocessing
techniques. This is an important detail since machine learning
techniques such as cross-validation or parameter tuning will
require training the model multiple times with the same pre-
processed data. Nevertheless, assuming that each development
cycle executes model training and data preprocessing exactly
once, we observed an overhead revolving around 5% on
average. Moreover, most machine learning projects already
resort to data reshaping methods for other purposes beyond
energy efﬁciency. More research is needed to help practitioners
deﬁne trade-offs, but our results show evidence that, as a rule
of thumb, row sampling and feature selection should always
be considered.

Another remark relates to the fact that improvements in the
efﬁciency of AI are being followed by a massive increase
in the usage of AI-based systems – the so-called rebound
effect. This is also referred to in another ﬁelds as the Jevons
paradox [37], i.e, there is a correlation between the usage of
natural resources and the improvements in the efﬁciency of a
given technology. In particular, improvements in the energy
efﬁciency of AI are often targeted at leveraging more AI
models in contexts where energy resources are prohibitively
scarce – for example, AI-based apps for smartphone devices.
In these contexts,
improvements on energy efﬁciency aim
at delivering more AI systems, failing to reduce the overall
carbon footprint of AI. We argue that our study is less prone
to this rebound effect, as it provides meaningful advice that
can be used by an AI project and lead to immediate savings on

01000200030004000Number of Datapoints0.00.20.40.60.81.0F1-scoreSVM010002000300040000.00.20.40.60.81.0Decision Tree010002000300040000.00.20.40.60.81.0KNN010002000300040000.00.20.40.60.81.0Random Forest010002000300040000.00.20.40.60.81.0AdaBoost010002000300040000.00.20.40.60.81.0Bagging Classifier02000400060008000Number of Features0.00.20.40.60.81.0F1-scoreSVM020004000600080000.00.20.40.60.81.0Decision Tree020004000600080000.00.20.40.60.81.0KNN020004000600080000.00.20.40.60.81.0Random Forest020004000600080000.00.20.40.60.81.0AdaBoost020004000600080000.00.20.40.60.81.0Bagging Classifierenergy consumption. Nonetheless, we call for more research in
Green AI that investigates the rebound effects in this context.

improvements shown earlier in RQ2 do not bring any cost in
the performance of the models.

Main ﬁndings RQ2 (Impact of dataset modiﬁcations
on energy consumption): Extracting smaller datasets
poses a great opportunity to reduce the energy con-
sumption of our machine learning models. Improve-
ments in energy efﬁciency go up to 92% when we
reduce the size of the dataset. Our results call for
more research on data-centric techniques to enable
and democratise Green AI and for AI frameworks to
give more control over how data is manipulated.

For the remaining algorithms, the results are not as unani-
mous. Random Forest, Decision Tree, and KNN yield a posi-
tive correlation between cardinality and F1-score. This means
that, despite the beneﬁts in energy consumption, reducing the
number of data points might prompt losses in the accuracy of
these models. Nevertheless, this is worth considering because
the models still showcase reasonable performance: with Deci-
sion Tree and Random Forest, F1-score drops less than 0.05
when we use at least 20% of the original dataset. Depending
on the use case, such a drop in F1-score might be appropriate.

C. Can we improve the energy efﬁciency of AI algorithms
through a data-centric approach without compromising their
accuracy? (RQ3)

Yes, one can use data preprocessing techniques to improve
energy efﬁciency without compromising the accuracy of the
models. When reducing the number of features (IV3) in the
dataset, the trained models perform the same, in terms of
accuracy, while consuming less energy. In other words, we
prove that using more data does not always mean better
models, while it leads to energy efﬁciency improvements.

The exception to this rule is KNN, which shows a strong
positive correlation between the F1-score and the number of
features in the dataset. This observation gets more interesting
if we combine it with the results from RQ2, where we observe
that reducing the feature space does not yield any signiﬁcant
energy improvement. Reducing the number of features not
only does not inﬂuence the energy efﬁciency of KNN, but
also hinders the performance score.

Using SVM did not produce a model with a F1-score above
0.6, denoting an overall very low accuracy of the generated
model relative to the other algorithms examined. A parameter
tuning strategy to accommodate the imbalance and sparsity
of our data could have yielded models with higher accuracy.
However, we did not delve into optimization strategies (see
Section VI for more details), since we wanted to compare
the energy consumption between different algorithms in a fair
and intuitive way. Hence, the energy improvements observed
for SVM in RQ2 require further scrutiny before drawing
generalizable conclusions.

Random Forest consistently yields the best performance.
Despite being the most energy-greedy algorithm, it trained the
most accurate model. Other algorithms, AdaBoost, Bagging
Classiﬁer, and Decision Tree follow close behind, showing
competitive results. Nevertheless, it is not possible to fairly
say which algorithm is the best. Different algorithms may work
better with different problems. Once again, our observations
reiterate that energy metrics provide useful information when
selecting the best machine learning model, and are quintessen-
tial for AI development.

When it comes to the number of data points (IV2), Ad-
aBoost and Bagging Classiﬁer yield no correlation between
the size of the dataset and F1-score, meaning that the energy

Main ﬁndings RQ3 (Trade-offs between AI energy
consumption and accuracy): In the vast majority of
cases, decreasing the number of data points / features
drastically reduces energy consumption, while imply-
ing only a negligible accuracy deterioration (e.g., by
reducing features, Random Forest can achieve a maxi-
mum of 74.81% energy reduction at the cost of only a
0.06% F1-score reduction). However, this observation
does not hold for all algorithms. For example, feature
selection when using KNN has almost no impact on
energy consumption, while considerably reducing its
model performance (with a maximum of 0.92% energy
reduction, associated to a 98.05% F1-score loss).

VI. THREATS TO VALIDITY

In this section, we discuss the threats to validity of our study,
by following the categorization provided by Wholin et al. [38].

A. Conclusion Validity

A threat

to conclusion validity in our study could be
constituted by low statistical power of the tests used to answer
our RQs. To mitigate this threat, we systematically collected
and analyzed data by following a process deﬁned a priori. By
considering the combination of factor, treatments, and reruns,
a total of 3.6K data samples were used to answer our RQ.

As a threat to the reliability of measures, unknown tasks run-
ning in the background during the execution of our experiment
may have acted as confounding factors, hence inﬂuencing our
energy measurements. To mitigate this threat, prior to the ex-
periment execution, we ensured that only the piece of software
necessary to run the experiment was running and/or able to be
executed. In addition, each experiment was repeated 30 times,
by shufﬂing the executions of sub-experiments uniformly at
to avoid that potential confounding factors could
random,
inﬂuence only a speciﬁc set of sub-experiments.

B. Internal Validity

A threat to internal validity, related to history, could be
constituted in our experiment by the inﬂuence that executing
subsequent runs could have had on our measurements (e.g.,
due to hardware increasing temperature). To mitigate this

threat, each experimental run was preceded by a 5-second
sleep operation, to allow all runs to be executed under identical
hardware conditions. Similarly, a warm-up operation was
performed to ensure the ﬁrst run was executed under the same
conditions as the subsequent ones (see Section III-D1).

C. Construct Validity

A mono-method bias may have affected the results of
our study, as we utilized a single metric to measure energy
consumption (DV1, Section III-D2) and calculate algorithm
accuracy (DV2, Section III-D2).

threat

Regarding energy consumption, we do not deem that adopt-
ing only energy consumption as dependent variable constitutes
per se a prominent
in our experiment. In addition,
utilizing exclusively energy consumption measurements is
a common practice in the ﬁeld of software energy efﬁ-
ciency [15], [39]–[41]. However, relying on a speciﬁc tool
to estimate the energy consumption (namely CodeCarbon),
could have inﬂuenced the construct validity of our experiment.
To mitigate this threat, we ensured that the tool was made
available as an open-source project (hence allowing us to
independently scrutinize the appropriateness and correctness
of the implementation), and that the tool relied on a widely
used estimation method provided by a prominent technology
company – as we investigated, CodeCarbon uses the Intel®
Power Gadget9 tool under the hood.

Regarding accuracy, we adopted a single metric, namely
the F1-score. We chose this metrics over other ones, e.g.,
precision, recall, or logarithmic loss, as F1-score allowed us
to provide a summary and intuitive presentation of the overall
model accuracy via a single, well established, metric. To
further mitigate potential threats related to the adoption of the
F1-score, during the experiment execution, we also collected
measurements of precision and recall, that are made available
for scrutiny in the replication package of this study.

D. External Validity

A prominent threat to the external validity of our study is
posed by the adoption of a single experimental subject and
a subset of AI algorithms. To mitigate this threat, we chose
our experimental subject and independent variables to be as
representative as possible. Speciﬁcally, we selected a common
AI classiﬁcation problem, namely text classiﬁcation, selected
a widely utilized peer-reviewed dataset [29], and considered a
total of 6 different algorithms provided in the largely adopted
Python library scikit-learn.

As a further external validity threat, the experimental setup
of this study did not integrate all the life cycle stages of
AI models, as outlined by [42], and was limited to certain
aspects of the model training phase. This outlines the in vitro
nature of our empirical experiment, which focused on studying
data-centric approaches, and disregarded other aspects of AI
model training (e.g., hyperparamether tuning) which would

9https://www.intel.com/content/www/us/en/developer/articles/tool/

power-gadget.html. Accessed 28 January 2022.

have commonly appeared in an in vivo experimentation. The
narrow focus on energy consumption of data pre-processing
of our study is intentional. More speciﬁcally, this research
aimed at providing exploratory insights on the impact that AI
data-centric approaches can have to AI energy consumption,
and not how these approaches can be integrated in practice.
As a result, numerous performance-optimization techniques
common in a typical AI pipeline are excluded from our exper-
iments, e.g., hyperparameter tuning, dimensionality reduction,
and linear separability tests.

Albeit our best efforts, given the discussed threats to ex-
ternal validity, the results presented in this exploratory study
have to be considered only as promising introductory insights,
paving the way for future research on data-centric Green AI.

VII. CONCLUSION AND FUTURE WORK

With the popularization of large-scale datasets and afford-
able computational/storage capabilities, the energy consumed
by AI is experiencing an unprecedented growth, which can
no longer be neglected. With this study, we aim at exploring
Green AI from a novel angle. Speciﬁcally, we investigate if
modifying exclusively datasets, rather than the model training
strategies, can optimize AI energy efﬁciency. To achieve our
goal, we conduct an empirical experiment by considering 6 AI
algorithms, two dataset modiﬁcation strategies, and a dataset
of over 5K data points.

The results we obtained provide the ﬁrst empirical evidence
that not only data-centric strategies can be used to optimize
AI energy efﬁciency, but also that such techniques can lead
to a drastic energy consumption reduction. While AI accuracy
may be negatively impacted by data-centric strategies, we also
observed that in most cases such accuracy loss is negligible.
From a practitioner perspective, our results highlight the
high impact that dataset modiﬁcations have on AI energy
efﬁciency, demonstrating that often “designing for less” while
preprocessing a dataset can drastically reduce the energy
consumed, while not sacriﬁcing accuracy.

For researchers, our results open a new area of inves-
tigation, namely data-centric Green AI, which, by consid-
ering the results documented in this research, demonstrates
very high potential to address the sustainability of AI-based
software-intensive systems.

As future work, we plan to generalize our results by
considering other AI application areas, e.g., image and audio
recognition, and by utilizing additional large-scale datasets.
Furthermore, we intend to investigate how other dataset mod-
iﬁcations (e.g., data representations) may impact AI energy
efﬁciency and related accuracy. Finally, we aim to conduct
in vivo experiments on data-centric Green AI, in order to
investigate how data-centric techniques can be integrated in
real-world AI pipelines, and how combining data-centric with
other Green AI techniques (e.g., model training strategies) may
impact energy efﬁciency and accuracy of AI models.

REFERENCES

[1] M. Haakman, L. Cruz, H. Huijgens, and A. van Deursen, “AI lifecycle
models need to be revised,” Empirical Software Engineering, vol. 26,
no. 5, pp. 1–29, 2021.

[2] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy
considerations for deep learning in nlp,” 6 2019. [Online]. Available:
http://arxiv.org/abs/1906.02243

[3] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quantifying the
carbon emissions of machine learning,” 10 2019. [Online]. Available:
http://arxiv.org/abs/1910.09700

[4] D. Amodei and D. Hernandez, “AI and compute,” Open AI, 2018.

[Online]. Available: https://openai.com/blog/ai-and-compute/

[5] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On
the dangers of stochastic parrots: Can language models be too big?”
Association for Computing Machinery, Inc, 3 2021, pp. 610–623.
[6] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green AI,”

Communications of the ACM, vol. 63, pp. 54–63, 11 2020.

[7] C. Calero, M. Polo, and M. ´A. Moraga, “Investigating the impact on
execution time and energy consumption of developing with spring,”
Sustainable Computing: Informatics and Systems, vol. 32, p. 100603,
2021.

[8] W. Oliveira, R. Oliveira, F. Castor, B. Fernandes, and G. Pinto,
“Recommending energy-efﬁcient java collections,” in 2019 IEEE/ACM
16th International Conference on Mining Software Repositories (MSR).
IEEE, 2019, pp. 160–170.

[9] R. Pereira, M. Couto, F. Ribeiro, R. Rua, J. Cunha, J. P. Fernandes,
and J. Saraiva, “Ranking programming languages by energy efﬁciency,”
Science of Computer Programming, vol. 205, p. 102609, 2021.
[10] S. Georgiou, M. Kechagia, P. Louridas, and D. Spinellis, “What are your
programming language’s energy-delay implications?” in Proceedings of
the 15th International Conference on Mining Software Repositories,
2018, pp. 303–313.

[11] H. Anwar, B. Demirer, D. Pfahl, and S. Srirama, “Should energy
consumption inﬂuence the choice of android third-party http libraries?”
in Proceedings of
the IEEE/ACM 7th International Conference on
Mobile Software Engineering and Systems, 2020, pp. 87–97.

[12] L. Cruz and R. Abreu, “Catalog of energy patterns for mobile applica-
tions,” Empirical Software Engineering, vol. 24, no. 4, pp. 2209–2235,
2019.

[13] L. Cruz, R. Abreu, J. Grundy, L. Li, and X. Xia, “Do energy-oriented
changes hinder maintainability?” in 2019 IEEE International Conference
on Software Maintenance and Evolution (ICSME), 2019, pp. 29–40.

[14] C. C. Venters, R. Capilla, S. Betz, B. Penzenstadler, T. Crick, S. Crouch,
E. Y. Nakagawa, C. Becker, and C. Carrillo, “Software sustainability:
Research and practice from a software architecture viewpoint,” Journal
of Systems and Software, vol. 138, pp. 174–188, 2018.

[15] R. Verdecchia, R. A. Saez, G. Procaccianti, and P. Lago, “Empirical
evaluation of the energy impact of refactoring code smells.” in ICT4S,
2018, pp. 365–383.

[16] R. Verdecchia, G. Procaccianti, I. Malavolta, P. Lago, and J. Koedijk,
“Estimating energy impact of software releases and deployment strate-
gies: The kpmg case study,” in 2017 ACM/IEEE International Sym-
posium on Empirical Software Engineering and Measurement (ESEM).
IEEE, 2017, pp. 257–266.

[17] A. Ribeiro, J. F. Ferreira, and A. Mendes, “Ecoandroid: An android
studio plugin for developing energy-efﬁcient java mobile applications,”
2021.

[18] L. Cruz and R. Abreu, “Emaas: Energy measurements as a service for
mobile applications,” in 2019 IEEE/ACM 41st International Conference
on Software Engineering: New Ideas and Emerging Results (ICSE-
NIER), 2019, pp. 101–104.

[19] A. van Wynsberghe, “Sustainable AI: AI for sustainability and the

sustainability of AI,” AI and Ethics, pp. 1–6, 2021.

[20] V. Klˆoh, B. Schulze, and M. Ferro, “Use of machine learning for
improvements in performance and energy consumption in hpc systems,”
09 2020.

[21] S. Zhu, K. Ota, and M. Dong, “Green AI for IIoT: Energy efﬁcient
internet of things,” IEEE

intelligent edge computing for industrial
Transactions on Green Communications and Networking, 08 2021.
[22] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani,
K. Maeng, G. Chang, F. A. Behram, J. Huang, C. Bai, M. Gschwind,
A. Gupta, M. Ott, A. Melnikov, S. Candido, D. Brooks, G. Chauhan,
B. Lee, H.-H. S. Lee, B. Akyildiz, M. Balandat, J. Spisak, R. Jain,

M. Rabbat, and K. Hazelwood, “Sustainable AI: Environmental
implications,
[Online].
challenges
Available: http://arxiv.org/abs/2111.00364

and opportunities,” 10 2021.

[23] G. Yang, E. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder,
J. Pachocki, W. Chen, and J. Gao, “Tuning large neural networks via
zero-shot hyperparameter transfer,” Advances in Neural Information
Processing Systems, vol. 34, 2021.

[24] F. Zogaj, J. P. Cambronero, M. C. Rinard, and J. Cito, “Doing more with
less: characterizing dataset downsampling for automl,” Proceedings of
the VLDB Endowment, vol. 14, no. 11, pp. 2059–2072, 2021.

[25] E. Garcia-Martin, N. Lavesson, and H. Grahn, “Identiﬁcation of energy
hotspots: A case study of the very fast decision tree,” Green, Pervasive,
and Cloud Computing, pp. 267–281, 01 2017.

[26] A. McIntosh, S. Hassan, and A. Hindle, “What can android mobile
app developers do about the energy consumption of machine learning?”
Empirical Software Engineering, pp. 1–42, May 2018.

[27] S. Georgiou, M. Kechagia, T. Sharma, F. Sarro, and Y. Zou, “Green
AI: Do Deep Learning Frameworks Have Different Costs?” in 2022
IEEE/ACM 44st International Conference on Software Engineering
(ICSE), 2022.

[28] V. R. Basili, G. Caldiera, and D. Rombach, “The Goal Question Metric
Approach,” in Encyclopedia of Software Engineering. Wiley, 1994, pp.
528–532.

[29] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami, “Contributions
to the study of sms spam ﬁltering: new collection and results,” in
Proceedings of the 11th ACM symposium on Document engineering,
2011, pp. 259–262.

[30] H. Liu and R. Setiono, “Chi2: Feature selection and discretization of nu-
meric attributes,” in Proceedings of 7th IEEE International Conference
on Tools with Artiﬁcial Intelligence.

IEEE, 1995, pp. 388–391.

[31] A. Radovanovic, R. Koningstein, I. Schneider, B. Chen, A. Duarte,
B. Roy, D. Xiao, M. Haridasan, P. Hung, N. Care, S. Talukdar, E. Mullen,
K. Smith, M. Cottman, and W. Cirne, “Carbon-aware computing for
datacenters,” 2021.

[32] E. Garc´ıa-Mart´ın, C. F. Rodrigues, G. Riley, and H. Grahn, “Estimation
of energy consumption in machine learning,” Journal of Parallel and
Distributed Computing, vol. 134, pp. 75–88, 12 2019.

[33] T. Nguyen, Z. Chen, and J. Lee, “Dataset meta-learning from kernel
ridge-regression,” in International Conference on Learning Representa-
tions, 2021.

[34] B. Zhao and H. Bilen, “Dataset condensation with differentiable siamese
augmentation,” in Proceedings of the 38th International Conference on
Machine Learning, ser. Proceedings of Machine Learning Research,
M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021, pp.
12 674–12 685.

[35] Z. Borsos, M. Mutny, and A. Krause, “Coresets via bilevel optimization
for continual learning and streaming,” in Advances in Neural Information
Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp.
14 879–14 890.

[36] R. Verdecchia, P. Lago, C. Ebert, and C. De Vries, “Green IT and green

software,” IEEE Software, vol. 38, no. 6, pp. 7–15, 2021.

[37] B. Alcott, “Jevons’ paradox,” Ecological economics, vol. 54, no. 1, pp.

9–21, 2005.

[38] C. Wohlin, P. Runeson, M. H¨ost, M. C. Ohlsson, B. Regnell, and
A. Wessl´en, Experimentation in software engineering. Springer Science
& Business Media, 2012.

[39] A. Hindle, A. Wilson, K. Rasmussen, E. J. Barlow, J. C. Campbell,
and S. Romansky, “Greenminer: A hardware based mining software
repositories software energy consumption framework,” in Proceedings
of the 11th working conference on mining software repositories, 2014,
pp. 12–21.

[40] R. Verdecchia, A. Guldner, Y. Becker, and E. Kern, “Code-level energy
hotspot localization via naive spectrum based testing,” in Advances and
New Trends in Environmental Informatics. Springer, 2018, pp. 111–130.
[41] D. Feitosa, R. Alders, A. Ampatzoglou, P. Avgeriou, and E. Y. Naka-
gawa, “Investigating the effect of design patterns on energy consump-
tion,” Journal of Software: Evolution and Process, vol. 29, no. 2, p.
e1851, 2017.

[42] L. H. Kaack, P. L. Donti, E. Strubell, G. Kamiya, F. Creutzig,
and D. Rolnick, “Aligning artiﬁcial intelligence with climate change
mitigation,” Oct. 2021, working paper or preprint. [Online]. Available:
https://hal.archives-ouvertes.fr/hal-03368037

