NumS: Scalable Array Programming for the Cloud

Melih Elibol, Vinamra Benara, Samyu Yagati, Lianmin Zheng, Alvin Cheung, Michael I. Jordan, and Ion
Stoica

University of California, Berkeley

2
2
0
2

l
u
J

3
1

]

C
D
.
s
c
[

2
v
6
7
2
4
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Scientists increasingly rely on Python tools to perform scal-
able distributed memory array operations using rich, NumPy-
like expressions. However, many of these tools rely on dy-
namic schedulers optimized for abstract task graphs, which
often encounter memory and network bandwidth-related bot-
tlenecks due to sub-optimal data and operator placement de-
cisions. Tools built on the message passing interface (MPI),
such as ScaLAPACK and SLATE, have better scaling prop-
erties, but these solutions require specialized knowledge to
use.

In this work, we present NumS, an array programming li-
brary which optimizes NumPy-like expressions on task-based
distributed systems. This is achieved through a novel sched-
uler called Load Simulated Hierarchical Scheduling (LSHS).
LSHS is a local search method which optimizes operator
placement by minimizing maximum memory and network
load on any given node within a distributed system. Coupled
with a heuristic for load balanced data layouts, our approach
is capable of attaining communication lower bounds on some
common numerical operations, and our empirical study shows
that LSHS enhances performance on Ray by decreasing net-
work load by a factor of 2×, requiring 4× less memory, and
reducing execution time by 10× on the logistic regression
problem. On terabyte-scale data, NumS achieves competitive
performance to SLATE on DGEMM, up to 20× speedup over
Dask on a key operation for tensor factorization, and a 2×
speedup on logistic regression compared to Dask ML and
Spark’s MLlib.

1 Introduction

Scientists increasingly rely on modern Python tools [14, 23]
to perform scalable distributed numerical operations using
rich, Numpy-like expressions. However, many of these tools
rely on dynamic schedulers [16, 23] optimized for abstract
task graphs, which do not exploit the structure of distributed
numerical arrays, such as apriori knowledge of input and

Figure 1: A high-level design diagram of NumS. Users ex-
press numerical operations using the NumPy API. These op-
erations are implemented by the GraphArray type. LSHS dis-
patches these operations to the underlying distributed system,
specifying data and operator placement requirements. The
underlying distributed system moves arrays between nodes
and processes to satisfy data dependencies for task execution.
Purple blocks reside on node N1, and the blue blocks reside
on node N2.

output sizes, or locality optimizations for parallel execution.
This can lead to performance problems which are difﬁcult
to address without in-depth knowledge of the underlying dis-
tributed system. These libraries implement array operations
by constructing discrete tasks graphs which represent the
desired computation, and schedule these tasks dynamically.
While this decoupling of algorithm from scheduling has de-
sirable software design properties, the loss of information to
the scheduler leads to sub-optimal data and operator place-
ment. In particular, when data placement is not optimized
for numerical array operations, unnecessary communication
among processes is often required in order to carry out ba-
sic operations, such as element-wise addition and vector dot
products.

Figure 2 illustrates a relatively simple example of what
may occur when array operations are dynamically scheduled.
A common operation which occurs when optimizing general-

1

NumPy APIGraphArrayLSHSN1N2Distributed System 
 
 
 
 
 
To enhance the performance of NumPy-based numerical
array programming libraries, we present NumS, a numerical
array library optimized for task-based distributed systems.
NumS employs a scheduling framework tailored to the ar-
chitecture and primitives provided by task-based distributed
systems. Our framework optimizes data and operator place-
ment decisions over a collection of compute nodes comprised
of worker processes. As placement decisions are simulated
or dispatched, our framework models memory load, network
load, and object locality on each node and worker process.
Within this framework, we implement a simple greedy op-
erator placement algorithm guided by a cost function of our
model of the underlying system’s state, called Load Simu-
lated Hierarchical Scheduling (LSHS) (see Section 5). Our
cost function computes the maximum memory and network
load on any given node, which when minimized results in
memory balanced and communication avoiding scheduling.
To support our empirical analysis, we analyze the commu-
nication time between node and worker processes, as well as
the latency associated with dispatching placement decisions.
We present communication lower bounds for a collection of
common operations, and show that LSHS attains these bounds
for some operations. We also show, both analytically and em-
pirically, limitations of these distributed systems as they apply
to the scalability of distributed numerical array operations.

Figure 1 depicts the design of NumS. While NumS operates
on Dask, it is optimized for and achieves peak performance
on Ray. When we refer to NumS without explicitly stating the
underlying distributed system, we assume it is using LSHS
and running on Ray.

Overall, our evaluation shows that NumS can achieve com-
petitive performance to SLATE on square matrix multipli-
cation on a terabyte of data, and outperforms Dask ML and
Spark MLlib on a number of end-to-end applications. We
show that, in every benchmark, LSHS is required for NumS’s
algorithms to achieve peak performance on Ray.

In comparison to SLATE, NumS is limited by the rate at
which operations can be dispatched to the underlying dis-
tributed system, and the overhead introduced by remote func-
tion invocation. Our empirical and theoretical analysis on
these limitations show that NumS performs best on a smaller
number of coarse-grained array partitions. Fewer partitions
mitigate control overhead introduced by having a central-
ized control process, and larger partitions amortize the over-
head associated with executing remote functions in Ray. Sec-
tion 7 measures these overheads and provides a theoretical
framework to model them in our analyses. On the other hand,
SLATE runs on MPI, which can operate on a greater number
of partitions due to MPI’s programming model, which enables
decentralized and partitioned application control. For matrix
multiplication, SLATE uses SUMMA, which allocates a mem-
ory buffer for output partitions, and accumulates intermediate
results to achieve better memory efﬁciency than NumS’s ma-
trix multiplication algorithm. However, SUMMA achieves

Figure 2: A computation graph depicting potential sub-
optimal placement of the data and operations required to
perform A(cid:62)@B, where both A and B are partitioned row-
wise into 4 blocks each. Transpose is fused with subsequent
operations. In a 2-node cluster consisting of 4 workers each,
a dynamic scheduler is unable to provide any locality guaran-
tees on initial or intermediate data placement. Purple vertices
correspond to data/operations placed on node 1, and blue
vertices correspond to data/operations placed on node 2.

ized linear models using Newton’s method is A(cid:62)@B, where
@ is the matrix multiplication operator in Python. When data
and operator placement is delegated to a distributed system
with dynamic scheduling, operands are often not co-located,
requiring a greater amount of memory and data movement.
Our example illustrates what happens in Dask for this oper-
ation (see Figure 9 for empirical results). Independent tasks
are executed round-robin over workers. Thus, all partitions of
A are created on node 1, and all partitions of B are created on
node 2. In the worst case, every operation requires at least one
object transfer between nodes, requiring 3× more time than
the theoretical lower bound we provide within our framework.

Syntax Description

−X Unary operation.
X + Y Binary element-wise.

sum(X, axis = 0) Reduction.

X@Y Basic linear algebra.
tensordot(X, Y, axes = 2) Tensor contraction.
einsum(ik, k j− > i j, X, Y) Einstein summation.

Table 1: While NumS provides greater coverage of the
NumPy API, we list only the set of operations we consider in
this work, along with syntax examples.

Statically scheduled task-based distributed systems, such
as Spark [28], provide high-level APIs for machine learn-
ing. Spark’s MLlib performs optimal scheduling for row-
partitioned tall-skinny matrices, but does not provide support
for block-partitioned multidimensional array programming.
High-performance computing (HPC) tools built on the mes-
sage passing interface (MPI) [11], such as ScaLAPACK [7]
and SLATE [13], implement statically scheduled algorithms
that are optimized for linear algebra operations. These special-
ized libraries are state-of-the-art for scalable linear algebra,
but they do not support the NumPy API, making them inac-
cessible to an increasing number of scientists that use Python.

2

A1B1@+A2B2@A3B3@+A4B4@+sub-optimal communication time for a variety of matrix mul-
tiplication operations, such as matrix-multiplication among
row-partitioned tall-skinny matrices. SLATE users must there-
fore be knowledgeable about the performance trade-offs of the
variety of different operations provided by the library in order
to take full advantage of its capabilities. We therefore recom-
mend NumS for high-performance, medium-scale (terabytes)
projects which may beneﬁt from the ﬂexibility of the NumPy
API, and SLATE for projects which require specialized, large-
scale distributed memory linear algebra operations.

Our contributions include:

1. LSHS, a scheduling algorithm for numerical array oper-
ations optimized for task-based distributed systems.

2. Theoretical lower bounds on the number of bytes re-
quired to perform a variety of common operations, in-
cluding square matrix multiplication, as well as theoreti-
cal limitations of distributed numerical array computing
on Ray and Dask.

3. An ablation of LSHS, which shows that LSHS consis-
tently enhances performance on Ray and Dask. On the lo-
gistic regression problem, LSHS enhances performance
on Ray by decreasing network load by a factor of 2×,
using 4× less memory, and decreasing execution time
by a factor of 10×.

4. A comparison of NumS to related solutions, showing
that NumS achieves a speedup of up to 2× on logistic
regression compared to Dask ML and Spark’s MLlib on
a terabyte of data, and up to 20× speedup over Dask
Arrays on core tensor factorization operations on a 4
terabyte tensor.

2 Related Work

NumPy [18] provides a collection of serial operations, which
include element-wise operations, summations, and sampling.
When available, NumPy uses the system’s BLAS [8] imple-
mentation for vector and matrix operations. BLAS implemen-
tations use shared-memory parallelism. For large datasets
that ﬁt on a single node, NumS outperforms NumPy on cre-
ation and element-wise operations. NumPy does not provide
a block partitioned representation of arrays on distributed
memory.

Dask [23] provides parallelism on a single machine via
futures, and is able to scale to multiple nodes via the Dask
distributed system, a distributed system framework similar
to Spark [28] and Ray [16]. Dask provides a distributed ar-
ray abstraction, partitioning arrays along n-dimensions, and
providing an API which constructs task graphs of array oper-
ations. When a graph of array operations is executed, array
partitions and their operations are dynamically scheduled as

tasks in an task graph. Round-robin scheduling of indepen-
dent tasks results in sub-optimal data layouts for common
array operations, such as the element-wise and linear alge-
bra operations. The Dask ML [23] library provides several
machine learning models. The optimization algorithms writ-
ten for these models frequently execute code on the driver
process. The library is written using Dask’s array abstraction.
Spark’s MLlib [15] is a library for scalable machine learn-
ing. MLlib depends on Breeze [1], a Scala library that wraps
optimized BLAS [8] and LAPACK [4] implementations for
numerical processing. Breeze provides high-quality imple-
mentations for many common machine learning algorithms
that have good performance, but because it relies on Spark
primitives, it introduces a learning curve for NumPy users.

Ray [16] is a dynamically executing task-based distributed
system. A Ray cluster is comprised of a head node and worker
nodes. Worker nodes are comprised of worker processes,
which execute tasks. The output of a task is written to the
shared-memory object store on the node on which the task
was executed. Any worker can access the output of any other
worker within the same node. Python programs connect to a
Ray cluster via a centralized driver process, which dispatches
remote function calls to a local scheduler which schedules
operations on worker processes. Ray implements a bottom-up
distributed scheduler. Driver and worker processes submit
tasks to their local scheduler. In general, based on available
resources and task meta data, a local scheduler may execute a
task locally, or forward the task to a centralized process on the
head node. The latter occurs only when a local scheduler is
unable to execute a task locally, a decision based on a variety
of heuristics. In general, when a local scheduler is presented
with a collection of tasks which have no dependencies, it
distributes tasks to reduce overall load on any given node.

High Performance Computing (HPC) libraries such as
ScaLAPACK (Scalable Linear Algebra PACKage) [7] and
SLATE [13] provide tools for distributed memory linear al-
gebra. They implement highly optimized communication-
avoiding operations using MPI [11]. These libraries are state-
of-the-art in terms of performance. The high performance
provided by these libraries comes at the cost of several spe-
cialized implementations of various linear algebra operations.
ScaLAPACK exposes 14 different routines for distributed ma-
trix multiplication, each optimized for specialized matrices
with various properties [7]. Unlike NumS which enables pro-
gramming against the NumPy API, which includes support
for tensor algebra operations, while these libraries provide a
C++ API limited to linear algebra operations.

Deep learning libraries such as Tensorﬂow [2], Py-
Torch [20], and MXNet [10] provide tensor abstractions, and
JAX [9] provides a NumPy array abstraction which enhances
usability. Mesh Tensorﬂow [26] provides tensor partitioning
on top of Tensorﬂow, but requires specifying layouts for ten-
sors, and targets Google TPUs. These libraries are specialized
for DNN training on accelerators. In contrast, NumS is de-

3

signed for general purpose array programming on CPUs.

3 Background

In task-based distributed systems, a task is a unit of work
with one or more dependencies. This can be thought of as an
arbitrary pure function. Objects serve as the inputs and outputs
of tasks. A completed task can be viewed as the object(s) it
outputs. A task graph is a representation of the dependencies
between tasks. A remote function call (RFC) creates a task
with zero or more dependencies. In NumS, numerical kernel
operations are executed as RFCs, creating a task with the
kernel operation’s operands as dependencies. Operands may
be tasks that have not yet been executed, or they may be the
output object of a completed task.

A worker is an independent process which executes tasks.
We use the term node to refer to machines comprised of
one or more worker processes. Task placement refers to the
process of deciding on which node or worker a particular task
should execute. All task placement decisions are executed on
a centralized driver process.

Figure 3 depicts the network topology
for which NumS is optimized. The clus-
ter is comprised of 4 nodes, with 4 work-
ers per node. Node N3 is expanded to ex-
pose the intra-node network topology of
worker processes. Thicker edges corre-
spond to greater bandwidth. In Ray, we
make placement decisions at the granu-
larity of nodes, leaving worker-level scheduling to each node’s
local scheduler. Ray implements a shared memory object
store, enabling any local worker to access the output of any
other local worker without worker-to-worker communication.
In Dask, we make placement decisions at the granularity of
worker processes. worker-to-worker communication within
the same node can be expensive, which we address with our
hierarchical data and operator placement design. Our design
is given in Section 4.

Figure 3

4 Graph Arrays

The GraphArray type implements distributed array creation,
manipulation, and numerical operations. Creation and manip-
ulation operations execute immediately, whereas numerical
operations are deferred.

A GraphArray is created via read operations, invocation
of operations like zeros(shape, grid) to create a dense array
of zeros or ones, or operations like random(shape, grid) to
randomly sample a dense array from some distribution. The
shape parameter speciﬁes the dimensions of the array, and
the grid parameter speciﬁes the logical partitioning of the
multidimensional array along each axis speciﬁed by the shape.
The logical partitioning of an array is called its array grid. For

example, A = random((256, 256), (4, 4)) will randomly sam-
ple a block-partitioned array partitioned into 4 block along the
ﬁrst axis, 4 blocks along the second axis, and 2 blocks along
the third axis for a total of 16 blocks. We use the notation Ai, j
to denote the i, j block of A, as depicted in Equation 1. Each
block Ai, j is itself a matrix with dimensions 64 × 64.

A =







A0,0 A0,1 A0,2 A0,3
A1,0 A1,1 A1,2 A1,3
A2,0 A2,1 A2,2 A2,3
A3,0 A3,1 A3,2 A3,3







(1)

When a grid argument is not speciﬁed, NumS chooses a
partitioning according to the softmax distribution of the ar-
ray’s dimensions. for a vector x ∈ Rn, the softmax function
is σ(x)i = exi
j=0 ex j . We use this distribution to factor the to-
∑n−1
tal number of available worker processes into the number
of dimensions of an array, putting greater weight on larger
dimensions. If we have p = 16 worker processes, we auto-
matically set the grid to pσ(shape) = (4, 4, 1). This approach
partitions tall-skinny matrices along its larger axis, and pro-
vides balanced partitioning for square or near-square matrices.

(a) Worker Mapping.

(b) Array Mapping.

Figure 4: Hierarchical mapping of logical array parti-
tions to physical nodes and workers. Within a (2, 2) node
grid, blue, purple, yellow, and green correspond to nodes
(0, 0), (0, 1), (1, 0), (1, 1), respectively.

When a creation operation is invoked, the logical partitions
of an array are mapped hierarchically to physical nodes. To
carry out this mapping within our framework, a user-deﬁned
node grid, a multi-dimensional coordinate space for nodes
within a cluster, is required. For a cluster consisting of 4
nodes with 4 workers each, ﬁgure 4a depicts the mapping of
the previously deﬁned array A to nodes and workers from
a user-deﬁned (2, 2) (sometimes written 2 × 2) node grid.
NiWj corresponds to worker j on node i. The node grid is
ﬁxed throughout the execution of a NumS application. For
a node grid with dimensions g1 × g2, Ai, j is placed on node
N(cid:96), where (cid:96) = (i%g1)g2 + j%g2. Within each node, blocks
of A are placed round-robin over available workers. In Ray,
worker-level mapping is ignored since worker processes on
the same node use a shared-memory object store. In our ex-
ample, A2,3 is placed on node N1 and worker W3: The node

4

N1N3N2W1W3W2W4N0W0N1W0N0W1N1W1N2W0N3W0N2W1N3W1N0W2 N1W2N0W3N1W3N2W2N3W2N2W3N3W3A00A02A01A03A20A22A21A23A10 A12A11A13A30A32A31A33placement is straightforward, whereas the worker-level place-
ment is due to 3 other partitions which are placed on node N1.
Figure 4b depicts the grouping of partitions within each node.
We call this approach to cyclic mapping of partitions, both
over nodes and workers, a hierarchical data layout. Along a
particular dimension, when operands have the same shape and
grid parameters, our data layout co-locates operand partitions,
requiring zero communication for element-wise operations.
This layout also minimizes communication for a variety of
linear and tensor algebra operations. With this data layout, the
operation in Figure 2 requires zero communication to achieve
the ﬁrst set of matrix multiplication operations (minimizing
inter-node communication for sums is presented in Section
5).

types. The ReduceAxis vertex type takes a single block and
reduces it using the given operation and axis. The Reduce
vertex type takes any number of blocks with equivalent
dimension, and reduces them using a given operation.
The sum operation is implemented by ﬁrst summing each
individual block along the given axis, and then summing
output blocks along the same axis. For example, if we
performed A(cid:48) = sum(A, 0) on the array we previously
deﬁned, the output blocks of A(cid:48) would be vectors with
dimension 64, and its array grid would be single-dimensional,
consisting of 4 blocks.

(a) −X

(b) X + Y

(c) sum

(d) X@Y

(e) tensordot

(f) einsum

Figure 5: Subgraphs induced by operations listed in 1
within a GraphArray. Rectangular vertices correspond
to leaf vertices, and circular vertices correspond to op-
erations. Dashed lines are used to denote repetition.
Σ corresponds to Reduce(add, . . . ), σ corresponds to
ReduceAxis(add, X, axis), and @, t, e correspond to matrix
multiplication, tensordot, and einsum, respectively.

While creation operations are immediately executed, arith-
metic operations in NumS are lazily executed. Figure 5 pro-
vides the subgraphs induced when a particular operation is
performed among GraphArray’s consisting of leaf vertices.
When an operation is performed on one or more GraphAr-
rays, an array of subgraphs is generated, depicting the sub-
operations required to compute the operation. Operations
performed on GraphArrays are referred to as array-level op-
erations, whereas the sub-operations performed among leafs
and vertices are referred to as block-level operations. The
operands involved in block-level operations are referred to
as blocks. Blocks are either materialized subarrays, or future
subarrays which have not yet been computed.

To perform the −X, a new GraphArray is constructed, and
each leaf vertex is replaced by the subgraph given in Subﬁgure
5a. For X + Y, the dimensions of X and Y and their grid
decompositions are required to be equivalent. The execution
of this expression generates a new GraphArray, comprised of
an array of subgraphs given by Subﬁgure 5b.

The

sum(X, axis)

operation

depends

on

Reduce(add, . . . )

and ReduceAxis(add, X, axis)

the
vertex

Figure 6: Matrix multiplication A@B of two arrays A and B
partitioned into 2 × 2 array grids. The operation is invoked
on a cluster with node grid 2 × 1. Blocks on node (0, 0) are
colored purple, and blocks on node 1, 0 are colored blue.

Figure 6 provides a simple example of matrix multipli-
cation. Matrix multiply is broken down into independent
sub-matrix multiplication operations. The output of each sub
operation is summed used the Reduce(add, . . . ) vertex type.
The rest of the operations given in Figure 5 are structurally
similar to sum and matrix multiplication. The operations
are broken down into sub-operations of the same kind, and
a Reduce(add, . . . ) vertex is used to sum the intermediate
outputs. In this sense, these operations can be viewed as re-
cursive.

When two or more sub-operations generated by these in-
duced subgraphs require operands which are located on dif-
ferent nodes, the operation may be executed, or placed, on
multiple nodes. The decision process for placing operations
is left to our scheduling algorithm, which is described in Sec-
tion 5, but LSHS requires a set of placement options to be
provided by every vertex in a GraphArray. For unary, and
ReduceAxis operations, there is only a single operand. For
element-wise binary operations, the data is already located
on the same node and workers by our hierarchical mapping
procedure, so only one potential option is provided to the
scheduler for these vertex types. For matrix multiplication,
tensor dot, and einsum, the set of placement options is the
union of all the nodes on which all the operands reside.

The Reduce vertex may have n operands. Our scheduler
must place n − 1 binary operations in order to complete the
execution of the Reduce vertex. For each of the n − 1 binary

5

-+@@tteeA00B00@A01B10@A00B01@A01B11@A10B00@A11B10@A10B01@A11B11@@A00A01A10A11B00B01B10B11operations, the set of placement options is the union of all the
nodes on which each of the two operands reside. The Reduce
vertex is responsible for deciding which operands to pair for
each of the n − 1 binary operations. We pair operands accord-
ing to their locality within the hierarchical network. We ﬁrst
pair operands on the same workers, then we pair operands on
the same node. Our scheduler must make placement decisions
for operands which are not on the same workers. We describe
our solution to this scheduling problem in the next section.

5 Load Simulated Hierarchical Scheduling

Step 2 randomly samples a frontier node. In step 3, each place-
ment option is simulated, and its costs is computed. In step
4, the option that minimizes the cost function is chosen, and
the GraphArray is transitioned to its new state. In Step 5, we
show how the transition procedure performs the actual remote
function call to the underlying distributed system, which car-
ries out the required operation on Node 1. Eventually, for
each output graph, the ﬁnal operation required to complete
the computation of A@B will be the addition operation. In
this example, the addition operation is scheduled using the
hierarchical data layout.

Algorithm 1: LSHS.
Function lshs(s):

while frontier(s) do
Nmin ← null ;
Cmin ← ∞ ;
v = sample(frontier(s)) ;
for i ← 0 to k do

if cost(v, N_i) < C then
Cmin = cost(s, N_i) ;
Nmin = Ni ;
s ← transition(s, Nmin) ;

return s ;

LSHS approximately minimizes an optimization-based for-
mulation of operator scheduling within a distributed system.
We describe the procedure for Ray, deﬁning the placement
procedure over nodes. There are three primary components
to LSHS: A GraphArray; a cluster state object used to sim-
ulate load imposed on the cluster for a particular placement
decision; and an objective function which operates on an
instance of the cluster state. LSHS is a discrete local tree
search algorithm (see Algorithm 1) [24]. LSHS executes the
GraphArray s by sequentially scheduling frontier vertices.
An operation vertex is on the frontier when all of its chil-
dren are leaf vertices. A vertex is sampled from the frontier,
and the placement option which minimizes the cost function
is selected. The GraphArray is then transitioned to a new
GraphArray by either updating a Reduce vertex to reﬂect its
remaining child operands, or converting an operation vertex
to a leaf vertex. The algorithm terminates when s consists
of all leaf vertices. For each output graph, the last operation
is mapped according to the hierarchical data layout, which
ensures that every graph array has a hierarchical data layout.
This is implicitly handled within the transition function.

Figure 7 provides a step-by-step depiction of the execution
of the matrix multiplication operation given in 10. Each step
in the algorithm is depicted by a numerically labeled arrow.
Step 1 generates the GraphArray as described in Section 4.
The vertices which are highlighted green are frontier nodes.

5.1 Cluster State and Optimization Problem

The cluster state, which is depicted in step 4 of Figure 7, is
used to monitor the memory and network load imposed on all
nodes within a Ray cluster. To simplify exposition, we use the
number of elements in an array to signify both memory and
network load 1. For a given node, we compute the network
load as two integers: The total number of incoming and out-
going array elements. The memory load is the total number of
array elements on a node resulting from the transmission of
arrays to that node, and the output of any operation executed
on that node. Let M denote a data structure that maintains a
mapping from all objects to their corresponding nodes, and
S denote a k × 3 matrix maintaining the memory, network in,
and network out of a k node cluster. Let m = 0, i = 1, o = 2
so that S j,m corresponds to the memory load on node j, and
likewise i corresponds to input load, and o corresponds to
output load. Let A correspond to the set of scheduling actions
(i.e. nodes on which to schedule an operation) available for
vertex v. An action a ∈ A is a tuple ( j, size), where j corre-
sponds to the jth node in S, and size corresponds to the size
of the output of vertex v. Let S(cid:48), M(cid:48) = T(S, M, a) be a tran-
sition function that takes M, S, a and returns S(cid:48), M(cid:48) such that
the operation v is simulated on S via the action a. With M,
the transition function T has enough information to simulate
object transfers between nodes.

At a given cluster state S, M, the objective function which
obtains the best action a from the set of actions A available
to vertex v is formulated as follows.

(cid:18) k
max
j=1
subject to S(cid:48), M(cid:48) = T(S, M, a).

k
max
j=1

min
a∈A

j,m +

S(cid:48)

S(cid:48)

j,i +

(cid:19)

k
max
j=1

S(cid:48)

j,o

(2)

If we modify Equation 2 to jointly minimize the maximum
memory and network loads over all nodes for a collection
of operations, the problem is very similar to load balancing:
Given a collection of task execution times, load balancing min-
imizes maximum execution time over a collection of nodes.

1An additional coefﬁcient we use to discount worker-to-worker communi-
cation within the same node on Dask. Ray does not require such a coefﬁcient
since workers operate on a shared-memory object store within each node.

6

Figure 7: Scheduling of the expression A@B on a 2 node cluster, where A and B are both 4 × 4, and both have a block shape of
2 × 2. 1) The expression is executed at runtime. 2) A frontier vertex is sampled at random. 3) Operator placement is simulated
on each node. 4) The operation is placed on the node which minimizes the objective. 5) The distributed system executes the
operation on the given node.

To construct a reduction from load balancing, we need an
optimization problem that minimizes the maximum load over
all possible scheduling choices. We introduce the superscript
t to identify the state of the variables at a given step in the
sequence of actions required to compute the optimal solution.
The superscript t is used to identify the vertex vt being com-
puted at step t. In addition to containing the node on which
to compute vertex vt and its output size, the set of actions
At are expanded to include the next vertex vt+1 on which
to operate. We incorporate these new requirements into the
transition function T by returning the set of next actions At+1
as well. We represent the optimal solution as a sequence of
actions π, where πt ∈ At . For a computation tree consisting
of n operations, the optimal sequence of actions of length n is
given as follows.

min
π∈(A0, ... , An−1)

(cid:18) k
max
j=1

Sn

j,m +

k
max
j=1

Sn

j,i +

k
max
j=1

Sn
j,o

(cid:19)

(3)

subject to St+1, Mt+1, At+1 = T(St , Mt , πt ).

The optimization problem given in Equation 3 is NP-hard by a
straightforward reduction from load balancing. Tasks in load
balancing are independent. In our formulation, independent
tasks require no object transfers between nodes. Thus, a load
balancing problem instance can be converted to an instance
of the problem given by Equation 3, and all solutions to these
problem instances will have zero network load. Instead of
maximum memory load, the remaining term maxk
j,m is
used to compute the maximum time for all tasks to execute
on any given node.

j=1 Sn

6 Generalized Linear Models

Generalized linear models (GLMs) are notoriously difﬁcult
to scale due to their reliance on basic array and linear alge-
bra operations. Furthermore, optimizing these models with

Algorithm 2: Newton’s method [17].

β ← 0;
while true do

µ ← m(X, β);
g ← ∇ f (X, y, µ, β);
H ← ∇2 f (X, y, µ, β)
β ← β − H−1g;
if (cid:107)g(cid:107)2 ≤ ε then
return β;

end

end

second order methods require the expensive computation of
the Hessian matrix, or approximations to the Hessian ma-
trix [6, 17]. Theoretically, Newton’s method converges faster
than any other method available for GLMs. In practice, fast
convergence can be difﬁcult to achieve without proper utiliza-
tion of all computational resources when ﬁtting these models
to large amounts of data. NumS is able to achieve high per-
formance on any model which relies heavily on element-wise
and basic linear algebra operations, making GLMs an ideal
NumS application.

Given a tall-skinny dataset X ∈ Rn×d decomposed into a
grid q × 1 of blocks, y ∈ Rn×1 decomposed into a grid q × 1, a
GLM m with corresponding twice differentiable convex objec-
tive f , and minimum gradient norm ε, Algorithm 2 computes
the global minimum β ∈ Rd of f using Newton’s method.
Upon initialization, β is decomposed into a 1 × 1 grid.

We work through the execution of Algorithm 2 on an r × 1
grid of nodes. We use the notation Ni,0 to refer to the ith node
in the r × 1 grid. We assume the hierarchical data layout, so
that X and y are distributed row-wise over r nodes, and the sin-
gle block β0,0 of β is created on node N0,0. In this example, we
assume that expressions are computed upon assignment. For

7

A @ BA00B00@Node 1mem = 16 + 4 netin = 0 netout = 0Node 2mem = 16 netin = 0 netout = 0Node 1mem = 16 netin = 0  netout = 0 + 4Node 2mem = 16 + 4 netin = 0 + 4 netout = 0Node 1mem = 20 netin = 0  netout = 0Node 2mem = 16 netin = 0 netout = 0NumPy APILSHSDistributed SystemNode 1Node 2A1B1A2B2A3B3A4B4matmulC000A00B00@+A01B10@A00B01@+A01B11@A10B00@+A11B10@A10B01@+A11B11@GraphArray21354example, LSHS is applied to the computation tree induced by
the expression µ = m(X, β), which schedules the operations
which comprise the computation tree and places the output µ
using the hierarchical data layout. Similarly, the expressions
assigned to g, H, and β are represented as computation trees
which are submitted to LSHS for scheduling.

For logistic regression, we have that m(X, β) = 1

1+e−Xβ . We
see ﬁrst that the linear operation Xβ will yield an intermediate
value C comprised of a grid of blocks Ci,0 = Xi,0 × β0,0. It is
usually the case that Xi,0 has more elements than β, so LSHS
will broadcast β0,0 to all the nodes on which the Xi,0 reside.
LSHS will schedule the remaining unary and binary opera-
tions to the blocks of C in-place: Since all data is local, the
node placement options are reduced to the node on which all
of the data already resides. The output µ is decomposed into
a g1 × 1 matrix and distributed according to the hierarchical
data layout.

The gradient of f is given by the expression X(cid:62)(µ − y).
We see that µ has the required grid decomposition to perform
the element-wise subtraction operation with y, yielding an
intermediate vector c. Because µ and y have the same size,
partitioning, and layout, the data is local, and LSHS schedules
the operations without data movement. In NumS, transpose is
executed lazily by fusing with the next operation. Thus, the
transpose operator is fused with the matrix-vector multiply, re-
g1−1
h=0 (X(cid:62))0,hch,0.
sulting in the ﬁnal set of operations g0,0 = ∑
Like the element-wise operations, because blocks of X and
c have the same size and partitioning along axis 0, the prod-
uct operation executes locally. The sum of the products is
executed as a reduction: A reduction tree is formed, and any
local sums are executed ﬁrst. The remaining sums are sched-
uled according to the cost function deﬁned for LSHS. Like
β, g is comprised of a single block, therefore the ﬁnal sum is
scheduled on node N0,0 to satisfy the hierarchical data layout
required by the outputs of LSHS.

The Hessian of f is given by the expression X(cid:62)(µ (cid:12) (1 −
µ) (cid:12) X), where (cid:12) denotes element-wise multiplication. The
expression µ (cid:12) (1 − µ) is scheduled locally by LSHS because
the input data for these sub trees all reside on the same node.
The intermediate vector c resulting from the previous op-
eration has the same block decomposition as µ, and µ has
the same size and partitioning as the ﬁrst dimension of X,
therefore the blocks of c and X are distributed the same way
over the r nodes. Thus, the vector-matrix element-wise op-
eration c (cid:12) X is executed without data movement: NumPy
executes this kind of expression by multiplying c with every
column of X, yielding an intermediate matrix C with the same
decomposition as X. Finally, the operation X(cid:62)C results in
the computation H0,0 = ∑k−1
h=0(X(cid:62))0,hCh,0, where H is square
with dimension d. The matrix multiplications are executed
without data movement, and the sum operation is executed as
a reduction like the previous reduction. Since H is square with
dimension d, it is single partitioned, and the ﬁnal operation
to compute H is scheduled to occur on node N0,0.

Finally, we update β. At this point, β, g and H are all com-
prised of single blocks, and all reside on node N0,0, so the
update to beta is executed locally on node N0,0. Similar to the
update of β, the gradient norm is computed locally on node
N0,0.

7 Communication Analysis

We extend the α − β model of communication to analyze the
communication time of element-wise, reduction, and basic
linear and tensor algebra operations. In our model, for a par-
ticular channel of communication, α denotes latency and β
denotes the inverse bandwidth of the channel. We also model
the time to dispatch an operation from the driver process as
γ. The time to transmit n bytes between two nodes is given
by C(n) = α + βn. We also model the implicit cost of com-
munication between workers within a single node of Ray as
R(n) = α(cid:48) + β(cid:48)n. For a dense array of size N, let p the number
of workers, N/p = n the block size, or number of elements,
and r = p/k the number of workers-per-node on a k node
cluster.

(a) Control Overhead.

(b) RFC Overhead.

Figure 8: Control overhead is measured by the time it takes
to allocate a vector of dimension 1024 on a 16 node cluster,
with a total of 1024 workers and cores. This is captured by
the γ term. As we decrease the number of blocks, γ decreases.
RFC overhead is measured by executing −x on a single block
vector x. The overhead is directly measured as the difference
between the time it takes to perform this operation using
NumPy. Ray writes task outputs to an object store, resulting
in greater RFC overhead. This is captured by the R(n) term.

The results of our theoretical analysis show that LSHS at-
tains the lower bound 0 for unary and binary element-wise
operations. For row-partitioned X and Y, we also attain the
lower bound for sum and X(cid:62)Y, both of which are logarith-
mic in k, as well as XY(cid:62). A complete analysis is given in
Appendix A.

We are not able to provide an upper bound for square ma-
trix multiplication for LSHS, but we do provide a lower bound
which shows that LSHS is capable of achieving asymptoti-
cally faster communication time in k (the number of nodes)
than SUMMA.

8

10245122561286432168421Number of Blocks02SecondsControl OverheadNumS-LSHSDask-LSHS8000.0800.080.08.08e-05Megabytes0.00.20.4SecondsRFC OverheadLSHS-DaskLSHS-Ray8 Evaluation

LSHS enhances NumS’s performance by balancing data place-
ment within multi-node cloud-based distributed systems, and
placing operations in a fashion which maintains load balance
while minimizing inter-node communication. To evaluate the
contribution of LSHS to NumS’s performance, we evaluate
NumS with and without LSHS in a variety of benchmarks.
Partitioning plays a key role in performance. To begin with,
we measure how partitioning impacts the performance of
NumS. In the rest of our benchmarks, we directly evaluate
the performance of LSHS in comparison to related solutions
by manually tuning block partitioning and node/worker/pro-
cess grid layouts where applicable. We also evaluate NumS’s
automatic partitioning heuristic on a data science problem in
Section 8.6.

NumS is designed for medium-scale (terabytes) problem
sizes. We evaluate on problem sizes of this scale in order to
highlight the beneﬁts NumS does provide. We provide limita-
tions to our approach in Section 7, which show that NumS’s
performance degrades as the number of array partitions in-
crease, and when partition sizes are too small.

We run all CPU experiments on a cluster of 16
r5.16xlarge instances, each of which have 32 Intel
Skylake-SP cores at 3.1Ghz with 512GB RAM connected
over a 20Gbps network. Each AWS instance is running
Ubuntu 18.04 conﬁgured with shared memory set to 512GB.
For CPU-based experiments, the Ray cluster uses 312GB for
the object store, and 200GB for workers. All NumS experi-
ments run on a single thread for BLAS operations and use
only Ray worker nodes for computation, leaving the head
node for system operations.

Unless otherwise noted, all experiments are executed by
sampling data using random number generators, and all exper-
iments are repeated 12 times. The best and worst performing
trials are dropped to obtain better average performance. This
is done primarily to avoid bias results due to cold starting
benchmarks on Dask, Ray, and Spark.

We evaluate Dask’s logistic regression (Dask ML 1.6.0) and
QR decomposition, which implements the direct tall-skinny
QR decomposition [5]. We evaluate Spark-MLlib’s (v2.4.7)
logistic regression and QR decomposition. Logistic regres-
sion uses the L-BFGS solver from Breeze, which is an open
source Scala library for numerical processing. Mllib’s QR
decomposition implements the indirect tall-skinny QR decom-
position [12] and uses the QR implementation from Breeze,
which is internally implemented using LAPACK.

8.1 Microbenchmarks

The results of our ablation study are given in Figure 9. In every
experiment, NumS on Ray is signiﬁcantly enhanced by LSHS.
For X + Y and X(cid:62)@Y, NumS on Dask without LSHS and
Dask Arrays achieve good performance whenever the number

of partitions is divisible by the number of workers, whereas
LSHS performs addition with 0 and X@Y(cid:62) with minimal
communication. With fewer partitions, we believe X@Y(cid:62)
is sub-optimal on Dask and Ray due to assigning large cre-
ation operations on fewer nodes, resulting in under-utilization
of available cluster resources. For matrix-vector operations,
LSHS does not provide any signiﬁcant enhancement over
Dask scheduling. The optimal scheduling behavior is to move
y to the nodes on which the partitions of X reside. For NumS
on Ray without LSHS, we observe object spilling due to too
many large objects being assigned to a few nodes, and large
object transmissions between nodes. For sum, we measure
the performance of reductions over large object transmissions.
Ray’s high inter-node throughput achieves good results for
this experiment. We believe Dask Array’s poor performance
on this task is due to a sub-optimal tree reduce, pairing parti-
tions which are not co-located. Overall, we see that NumS on
Ray is the most robust to partitioning choices, and achieves
well-rounded performance. See Section 7 for our theoretical
analysis, which supports the claims we make in these results.

8.2 DGEMM

ScaLAPACK
SLATE
NumS

2GB 4GB 8GB 16GB 32GB
992
992
224
992
992
992
5271
3953
3953

64
1408
5591

480
928
3727

Table 2: Square block size settings for ScaLAPACK, SLATE,
and NumS on the DGEMM benchmark. DGEMM is dis-
tributed over 1 node for 2GB, 2 nodes for 4GB, etc. up to
32GB on 16 nodes.

For dense square matrix multiplication, we compare to
state-of-the-art baselines ScaLAPACK and SLATE [7, 13].
We start with 2GB matrices on a single node, and double the
amount of data as we double the number of nodes. We tune
all libraries to their optimally performing block dimension.
These settings are given in Table 2.

Figure 10 shows that NumS is competitive with HPC li-
braries on this benchmark. Both ScaLAPACK and SLATE
implement the Scalable Matrix Multiplication Algorithm
(SUMMA) [27] for their dgemm routine. While these results
may seem surprising, the theoretical results we present in
Section 7, as well as our comprehensive analysis in Appendix
A show that our approach to parallelizing and scheduling
distributed arrays can attain asymptotically lower communi-
cation time for this operation. We show the existence of a
square dense matrix-matrix multiplication algorithm which
is asymptotically faster than SUMMA in k, suggesting that
NumS’s performance on DGEMM improves as the number
of nodes in a cluster increases. While the SUMMA algo-
rithm provides good communication bounds on distributed

9

Figure 9: An ablation study comparing NumS on Dask and Ray, with and without LSHS, and include Dask Arrays as an additional
point of comparison. All experiments are run on 16 node clusters, with 32 workers per node, for a total of 512 workers. In all but
sum, X, Y are 1 terabyte arrays, partitioned row-wise. y is partitioned to match the partitioning of X in X@y and X(cid:62)@y. sum is
executed on a multi-dimensional tensor partitioned along its ﬁrst axis.

more nodes and more partitions, which in turn puts greater
demand on the number of RFCs which need to be dispatched
by the driver process. MPI-based libraries can be viewed as
programs with distributed control, which provide a means to
avoiding these issues.

8.3 Linear Algebra

QR decomposition is a core operation on a variety of linear
algebra and data science operations, including linear regres-
sion, singular value decomposition, and principal component
analysis [5,6,12]. In this section, we evaluate the weak scaling
of NumS on direct and indirect QR decomposition, as well
as a performance comparison to Dask, and Spark. We also
include results for NumS without LSHS to highlight the role
of scheduling. All experiments perform the same number of
steps and operations. Each experiment is repeated 12 times,
and the best and worst performing trials are dropped to avoid
bias due to cold starts.

The QR decomposition of a matrix A ﬁnds the matrices Q
and R such that A = QR. A direct QR decomposition com-
putes Q from intermediate factors of Q, whereas indirect QR
discards intermediate Q factors and computes Q via the oper-
ation AR−1.

The weak scaling of indirect QR decomposition is carried
out by doubling the amount of resources as we double the
amount of work, starting with 64GB of data on a single node.
Scaling is near perfect (Figure 12a).

We compare Dask and NumS on the direct tall-skinny QR
decomposition algorithm [5]. We sample data row-wise in
2GB blocks, which achieves peak performance for both li-

Figure 10: Dense square matrix-matrix multiplication.

memory, it assumes every process has equivalent commu-
nication time. While the difference in communication time
between vs. within nodes on supercomputers may not be
signiﬁcant, on multi-node clusters in the cloud, inter-node
communication is generally much more expensive than intra-
node communication. LSHS places data and operations in a
network topology-aware manner. For a particular operation,
LSHS approximates the location of data, including data which
is cached by Ray’s object store from previous object trans-
missions. This enables LSHS a greater variety of operator
placement decisions, enabling it to reduce inter-node commu-
nication by placing operations on nodes where operands are
already co-located. Coupled with a locality-aware reduction
operation, the ﬁnal set of summations invoked in this opera-
tion can be performed entirely locally prior to performing an
inter-node reduction. Furthermore, the overhead associated
with performing intra-node communication among processes
is implicit given Ray’s shared-memory object store.

On the other hand, the presence of γ limits NumS’s scala-
bility in the limit of k. As we scale the size of data, we require

10

256384512640768896102411521280140815361664179219202048Number of Blocks24996143SecondsX + Y128256384512640768896102411521280140815361664179219202048Number of Blocks33669103SecondsX.T @ Y816243240485664Number of Blocks2110218326SecondsX @ Y.T128256384512640768Number of Blocks03875113SecondsX @ y128256384512640768Number of Blocks0336597SecondsX.T @ y64128192256320384448512Number of Blocks1678140202Secondssum(X, axis=0)Dask-ArraysDask-StockDask-LSHSNumS-StockNumS-LSHS2481632Dataset Size (GB)020SecondsDGEMMScaLAPACKSLATENumSbraries. Figure 11a shows the results of our direct TSQR
benchmarks. NumS performs comparably to Dask on this
benchmark. Dask’s direct tall-skinny implementation requires
a single column partition, leaving only one dimension along
which data is partitioned. The partitioning of data for Dask
which achieves peak performance implicitly results in data
locality for a number of intermediate operations due to Dask’s
round-robin placement of initial tasks. We observe this behav-
ior directly in our ablation study in Figure 9.

(a) Direct TSQR.

(b) Indirect TSQR.

Figure 11: TSQR execution time on NumS, Dask, and Spark.

Since Spark does not implement a direct QR decomposi-
tion, we evaluate both NumS and Spark on indirect TSQR
decomposition. Similar to logistic regression, Spark’s imple-
mentation of indirect TSQR decomposition is sensitive to
partition tuning. Figure 11b compares the results of our indi-
rect TSQR implementation to Spark’s. Similar to direct TSQR
for Dask, indirect TSQR is a statically scheduled algorithm
for Spark and NumS. Since both algorithms are identical, and
scheduling is static, we attribute the difference in performance
to differences between Spark and Ray.

8.4 Tensor Algebra

We compare NumS to Dask Array’s implementation of the
tensordot and einsum operators, primitives which enable the
expression of distributed dense tensor algebra operations. We
perform these operations on a 16 node cluster with 32 workers
per node. For the einsum operator, we perform the Matricized
Tensor Times Khatri Rao Product (MTTKRP), which we ex-
press in Einstein summation notation. This operation is the
closed-form solution to the alternating least squares algorithm
for tensor factorization [25]. For the tensordot operator, we
perform the standard tensor double contraction on operands
which frequently occur in a variety of other tensor decompo-
sitions [22].

For MTTKRP, we sample X ∈ RI×J×K, B ∈ RI×F , B ∈
RJ×F , and perform einsum(i jk, i f , j f − > i f , X, B, C). For
both Dask and NumS, we partition every array to achieve peak
performance. In NumS, we also tune a cubic conﬁguration of
the available compute nodes, to further control the mapping
of partitions to nodes. We set F = 100 and vary the number
of elements of I = J = K to set the size of X from 8GB to
4TB.

For tensor double contraction, we sample X identically to
the MTTKRP benchmark, and sample Y ∈ RJ×K×F with F =

(a) QR Decomposition.

(b) Logistic Regression.

Figure 12: QR decomposition achieves near-perfect scaling.
Logistic regression exhibits a slowdown at 16 nodes due
intermediate reduction operations over a 20Gbps network.

(a) Double Contraction.

(b) MTTKRP.

Figure 13: Tensor algebra on NumS and Dask Arrays.

100. We also vary the size of X identically to the MTTKRP
benchmark.

Both results depend heavily on LSHS. Array partition-
ing and node grid also play a signiﬁcant role. For NumS,
MTTKRP partitioned along dimension J, and a node grid of
16 × 1 × 1 performed best. For the double contraction bench-
mark, a node grid of 1 × 16 × 1 performed best, with rela-
tively balanced partitioning along dimensions J and K. In
both benchmarks, both NumS and Dask Arrays perform a col-
lection of sum-of-products. Both libraries perform tree-based
reductions.

For MTTKRP, Dask performed best with relatively bal-
anced partitioning of blocks, but is unable to achieve good
initial data placement to minimize inter-node communication
due to its inability to specify a node grid. Furthermore, its
reduction tree is constructed before any information about the
physical mapping of blocks to nodes is available, resulting
summations between vs. within nodes. After tuning, the 4TB
benchmark on Dask Arrays took approximately 241.9 sec-
onds (20× slower than NumS, which is excluded from Figure
13.

For double contraction, Dask and NumS performance is rel-
atively the same. Unlike MTTKRP, there is no good factoring
of 16 nodes into a node grid that LSHS can take advantage of
in order to reduce inter-node communication. This is mainly
due to the structure of the problem: The tensor contraction
sums over the dimensions J and K, and the ordering of dimen-
sions for tensors X and Y align only along dimension J. This
is why the 1 × 16 × 1 factoring of the nodes performs best.

11

641282565121024Dataset Size (GB)0100SecondsDirect QRDask ArraysNumS-StockNumS-LSHS641282565121024Dataset Size (GB)0200SecondsIndirect QRSpark MLlibNumS-StockNumS-LSHS124816Nodes0.21.01.82.63.4TFlops/sQR Decomposition ScalingNumSPerfect124816Nodes0.31.22.02.93.8TFlops/sLogistic Regression ScalingNumSPerfect8645124096Dataset Size (GB)05SecondsDouble ContractionDask ArraysNumS8645124096Dataset Size (GB)01020SecondsMTTKRPDask ArraysNumS8.5 Generalized Linear Models

Generalized linear models (GLMs) are notoriously difﬁcult to
scale due to their reliance on basic array and linear algebra op-
erations. Furthermore, optimizing these models with second
order methods require the expensive computation of the Hes-
sian matrix, or approximations to the Hessian matrix [6, 17].
Theoretically, Newton’s method converges faster than any
other method available for GLMs [17]. In practice, fast con-
vergence can be difﬁcult to achieve without proper utilization
of all computational resources when ﬁtting these models to
large amounts of data. NumS is able to achieve high perfor-
mance on any model which relies heavily on element-wise
and basic linear algebra operations, making GLMs an ideal
NumS application. We explain in greater detail the execution
of GLMs in Section 6.

Since logistic regression is the most widely used GLM,
we evaluate our implementation of logistic regression. We
measure its weak scaling performance in terms of teraﬂops,
measure its network and memory load with and without LSHS,
and compare its performance to other solutions. Our experi-
ments are executed on synthetic classiﬁcation data. Our data
is drawn from a bimodal Gaussian with 75% of the data con-
centrated at mean 10 with variance 2 (negative samples), and
the remaining 25% concentrated at mean 30 with variance 4
(positive samples). Each sample is 256-dimensional. We sam-
ple from these distributions to satisfy the required dataset size.
For example, a 64GB dataset of 64bit ﬂoats consists of a de-
sign matrix X with 31, 250, 000 rows and 256 columns, and a
target vector y consisting of 31, 250, 000 values ∈ {0, 1}. This
data size and distribution was recommended by our industry
collaborators.

Our weak scaling results for logistic regression are near
perfect until 16 nodes, at which point performance degrades
due to inter-node reductions over a 20Gbps network (Figure
12b).

We compare NumS’s performance on logistic regression
to Dask ML and Spark MLlib. We include results for NumS
on Ray without LSHS to highlight the role of scheduling.
Dask and Spark implement different versions of these algo-
rithms, so we implement both versions of both algorithms for
a fair comparison. In these experiments, we hold the cluster
resources ﬁxed at 16 nodes, varying only the dataset size to
evaluate the performance of each system. All experiments
perform the same number of steps and operations.

For our comparison to Dask, we sample data row-wise in
2GB blocks, which yields peak performance for both Dask
and NumS. We use Newton’s method for both libraries. New-
ton’s method is optimal for logistic regression’s convex ob-
jective. Figure 14a shows that NumS outperforms Dask at
every dataset size. The performance gap is partially explained
by differences between LSHS and Dask’s dynamic scheduler.
We believe the majority of the performance gap is due to
Dask ML’s implementation logistic regression which, based

(a) Newton’s method.

(b) L-BFGS.

Figure 14: Logistic regression ﬁtting time.

on our inspection of their source code, aggregates gradient
and hessian computations on the driver process to perform
updates to model parameters and test for convergence.

Since Spark does not support Newton’s method, we com-
pare our implementation of the L-BFGS optimizer to Spark’s
version. We initialize our logistic regression implementation
to execute 10 optimization steps, with no regularizer, and
L-BFGS conﬁgured to use a history length of 10. Both imple-
mentations use identical line search algorithms and are con-
ﬁgured identically. Figure 14b shows that our implementation
of logistic regression with the L-BFGS optimizer outperforms
Spark. Spark’s logistic regression and Breeze’s L-BFGS [1]
implementation is a statically scheduled implementation. To
our knowledge, the algorithms and scheduling of operations
on partitions is identical to NumS’s implementation, and the
scheduling behavior of LSHS for this problem. While LSHS
is essential to achieve efﬁciency from a scheduling point-of-
view, we believe the performance gap beyond scheduling is
explained by differences between Spark and Ray.

To better understand why LSHS enhances NumS’s perfor-
mance on Ray, We measure memory, network in, and network
out of every node at equally spaced intervals for NumS on
Ray with and without LSHS. For these experiments, we use
16 nodes with 32 workers per node and measure execution
time of a single iteration of Newton’s method on a 128GB
logistic regression problem.

These experiments measure execution time and resource
utilization required to load data from S3, execute a single
iteration of Newton’s method.

Figure 15: Ablation on memory and network load.

12

641282565121024Dataset Size (GB)0100200SecondsNewton's MethodDask MLNumS-StockNumS-LSHS641282565121024Dataset Size (GB)0200400SecondsL-BFGSSpark MLlibNumS-StockNumS-LSHS0100200Mem (GB)0100Without LSHS (Secs)0.01.02.5Net In (GB/s)02244With LSHS (Secs)Figure 15 shows memory usage and network input over
one iteration of Newton’s method. Each curve tracks the load
on one node. Densely clustered curves, where all nodes have
similar load over the experiment, indicate good load balance.
Lower y-axis values indicate lower resource footprint. LSHS
signiﬁcantly enhances NumS performance on Ray in terms of
load balance and resource footprint. Without LSHS, Ray exe-
cutes the majority of submitted tasks on a single node, while
LSHS distributes load without increased network communica-
tion. In particular, Ray’s bottom-up scheduling does not deﬁne
an explicit strategy for scheduling independent tasks [16], re-
sulting in a sub-optimal data layout for both element-wise
and linear algebra operations. For this task, LSHS decreases
network load by a factor of 2×, uses 4× less memory, and
decreases execution time by a factor of 10×.

8.6 Data Science

NumS not only provides a speedup in distributed memory
settings, but it also provides a signiﬁcant speedup on a single
node for certain data science applications. The experiments
in this section make use of NumS’s automatic block parti-
tioning (see Section 4), demonstrating that a speedup can
be achieved by simply replacing Python import statements
of the libraries used in this section with NumS equivalents.
While these experiments run on the same instances we have
been using throughout this section, NumS is pip-installable
on your laptop and can provide comparable speedups to what
we present in this section.

Tool Stack
Python Stack
NumS

Load Train Predict
0.43
61
65.55
0.20
3.21
11.79

Total
126.98
15.2

Table 3: NumS vs. a Python stack consisting of Pandas for a
data loading, and scikit-learn on NumPy for training a logistic
regression model. All values are reported in seconds.

Pandas, NumPy, and scikit-learn make up a common
stack for data science in Python. NumS provides a paral-
lel read_csv method comparable to Pandas’, eliminating one
layer in the package stack for numerical CSV ﬁles. Table 3
shows that NumS achieves an 8× speedup over Pandas’ [19]
serial read_csv operation with scikit-learn’s training and pre-
diction procedures for a logistic regression model trained on
the 7.5GB HIGGS dataset [3]. Both NumS and scikit-learn’s
logistic regression are conﬁgured to use 32 cores.

We tune scikit-learn to use its fastest optimizer, which is l-
bfgs. Compared to NumS’ Newton optimizer, l-bfgs requires
signiﬁcantly more iterations to converge. Furthermore, l-bfgs
requires line search at every iteration, which requires multiple
calls to the logistic regression objective function. Newton’s
method does not require a line search and converges in fewer
iterations than l-bfgs. Our Newton’s method implementation

is optimized for the kinds of tall-skinny matrices which com-
monly occur in data science, achieving greater utilization of
available memory and cores through efﬁcient parallelization
of basic linear algebra operations.

Figure 16: Training on fractions of the HIGGS dataset.

To further dig into these differences, we compare NumS
to scikit-learn on different fractions of the HIGGS dataset.
Figure 16 shows that, at smaller scales, NumS is 5× slower
than scikit-learn, and at larger scales, it is 20× faster than
scikit-learn.

Beyond differences in the optimizer, the primary difference
in performance is due to NumS’s parallelization of all array
operations, not just those parallelized by the underlying sys-
tem’s BLAS implementation. We measure this by implement-
ing Newton’s method in pure NumPy, with full parallelization
of BLAS operations (32 cores). We measure the amount of
time spent in serial operations vs. parallel operations, and we
ﬁnd that 90% of the time for Newton’s method using NumPy
is spent on serial operations. In total, our NumPy implemen-
tation of Newton’s method takes approximately 11 seconds, a
5× speedup over scikit-learn’s fastest optimizer. Compared
to this implementation of Newton’s method, NumS achieves
a speedup of 3.5×. Compared to Newton’s method, l-bfgs is
comprised of less expensive matrix operations (no direct com-
putation of the Hessian), and many more serially executing
element-wise operations.

9 Conclusion And Future Work

Our results show that NumS achieves competitive perfor-
mance with state-of-the-art HPC libraries, and in many in-
stances outperforms related solutions. Our theoretical and
empirical analyses suggest that NumS performs best on data
on the order of gigabytes and terabytes. For smaller data,
NumS may be slower than traditional Python tools. While
NumS has not been evaluated on very large data (petabytes to
exabytes), our theoretical analysis suggests systems such as
SLATE are better suited for linear algebra operations at these
scales. Based on our work, we believe that all distributed data
structures, not just distributed arrays, that rely on dynamic
scheduling require some combination of LSHS with data lay-
outs optimized for operations on those data structures. In par-
ticular, Dask’s dataframes and project Modin [21], a portable

13

0.00010.0010.010.11.0Fraction of Data101100101TimesklearnNumSdataframe abstraction that runs on Dask and Ray, could ben-
eﬁt from similar techniques presented in this work. Future
directions for NumS include (1) generalizing our ﬁndings and
providing a framework on which any distributed data struc-
ture can beneﬁt from LSHS; (2) improving the usability of
NumS’s user API by enhancing automatic block-partitioning
and eliminating the need for a user-speciﬁed node grid; and
(3) reducing RFC overhead by introducing operator fusion.

Acknowledgments

Thank you to Amazon Core AI and Microsoft for supporting
the evaluation of NumS by providing AWS and Azure credits.

References

[1] Breeze: A numerical processing library for scala, 2017.

[2] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, et al.
Tensorﬂow: A system for large-scale machine learning.
In 12th {USENIX} symposium on operating systems de-
sign and implementation ({OSDI} 16), pages 265–283,
2016.

[3] Claire Adam-Bourdarios, Glen Cowan, Cécile Germain,
Isabelle Guyon, Balázs Kégl, and David Rousseau. The
higgs boson machine learning challenge. In Proceedings
of the 2014 International Conference on High-Energy
Physics and Machine Learning - Volume 42, HEPML’14,
page 19–55. JMLR.org, 2014.

[4] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Dem-
mel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-
marling, A. McKenney, and D. Sorensen. LAPACK
Users’ Guide. Society for Industrial and Applied Math-
ematics, Philadelphia, PA, third edition, 1999.

[5] Austin R Benson, David F Gleich, and James Demmel.
Direct qr factorizations for tall-and-skinny matrices in
mapreduce architectures. In 2013 IEEE international
conference on big data, pages 264–272. IEEE, 2013.

[6] Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning (Information Science and Statistics).
Springer-Verlag, Berlin, Heidelberg, 2006.

[7] L Susan Blackford, Jaeyoung Choi, Andy Cleary, Ed-
uardo D’Azevedo, James Demmel, Inderjit Dhillon, Jack
Dongarra, Sven Hammarling, Greg Henry, Antoine Pe-
titet, et al. ScaLAPACK users’ guide, volume 4. Siam,
1997.

[8] L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin
Remington, R Clint Whaley, James Demmel, Jack Don-
garra, Iain Duff, Sven Hammarling, Greg Henry, et al.
An updated set of basic linear algebra subprograms
(blas). ACM Transactions on Mathematical Software,
28(2):135–151, 2002.

[9] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, and Skye Wanderman-Milne. JAX: composable
transformations of Python+NumPy programs, 2018.

[10] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,
Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang,
and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient ma-
chine learning library for heterogeneous distributed sys-
tems. arXiv preprint arXiv:1512.01274, 2015.

[11] Lyndon Clarke, Ian Glendinning, and Rolf Hempel. The
mpi message passing interface standard. In Karsten M.
Decker and René M. Rehmann, editors, Programming
Environments for Massively Parallel Distributed Sys-
tems, pages 213–218, Basel, 1994. Birkhäuser Basel.

[12] Paul G. Constantine and David F. Gleich. Tall and
skinny qr factorizations in mapreduce architectures. In
Proceedings of the Second International Workshop on
MapReduce and Its Applications, MapReduce ’11, page
43–50, New York, NY, USA, 2011. Association for Com-
puting Machinery.

[13] Mark Gates, Jakub Kurzak, Ali Charara, Asim YarKhan,
and Jack Dongarra. Slate: Design of a modern dis-
tributed and accelerated linear algebra library. In Pro-
ceedings of the International Conference for High Per-
formance Computing, Networking, Storage and Analy-
sis, SC ’19, New York, NY, USA, 2019. Association for
Computing Machinery.

[14] S. Hoyer and J. Hamman. xarray: N-D labeled arrays
and datasets in Python. In revision, J. Open Res. Soft-
ware, 2017.

[15] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan
Sparks, Shivaram Venkataraman, Davies Liu, Jeremy
Freeman, DB Tsai, Manish Amde, Sean Owen, Doris
Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh,
Matei Zaharia, and Ameet Talwalkar. Mllib: Machine
learning in apache spark. Journal of Machine Learning
Research, 17(34):1–7, 2016.

[16] Philipp Moritz, Robert Nishihara, Stephanie Wang,
Alexey Tumanov, Richard Liaw, Eric Liang, Melih Eli-
bol, Zongheng Yang, William Paul, Michael I Jordan,
et al. Ray: A distributed framework for emerging {AI}
applications. In 13th {USENIX} Symposium on Operat-
ing Systems Design and Implementation ({OSDI} 18),
pages 561–577, 2018.

14

[17] Jorge Nocedal and Stephen J. Wright. Numerical Opti-
mization. Springer, New York, NY, USA, second edition,
2006.

[18] Travis E Oliphant. A guide to NumPy, volume 1. Trelgol

Publishing USA, 2006.

[19] The pandas development team. pandas-dev/pandas: Pan-

das, February 2020.

[20] Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style, high-performance deep
learning library. arXiv preprint arXiv:1912.01703, 2019.

[21] Devin Petersohn, William Ma, Doris Lee, Stephen
Macke, Doris Xin, Xiangxi Mo, Joseph E Gonzalez,
Joseph M Hellerstein, Anthony D Joseph, and Aditya
Parameswaran. Towards scalable dataframe systems.
arXiv preprint arXiv:2001.00888, 2020.

[22] Stephan Rabanser, Oleksandr Shchur, and Stephan Gün-
Introduction to tensor decompositions and

nemann.
their applications in machine learning, 2017.

[23] Matthew Rocklin. Dask: Parallel computation with
blocked algorithms and task scheduling.
In Kathryn
Huff and James Bergstra, editors, Proceedings of the
14th Python in Science Conference, pages 130 – 136,
2015.

[24] Stuart Russell and Peter Norvig. Artiﬁcial Intelligence:
A Modern Approach. Prentice Hall Press, 2009.

[25] Vatsal Sharan and Gregory Valiant. Orthogonalized als:
A theoretically principled tensor decomposition algo-
rithm for practical use, 2017.

[26] Noam Shazeer, Youlong Cheng, Niki J. Parmar, Dustin
Tran, Ashish Vaswani, Penporn Koanantakool, Peter
Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff
Young, Ryan Sepassi, and Black Hechtman. Mesh-
tensorﬂow: Deep learning for supercomputers. 2018.

[27] Robert A. van de Geijn and Jerrell Watts. Summa: Scal-
able universal matrix multiplication algorithm. Techni-
cal report, USA, 1995.

[28] Matei Zaharia, Mosharaf Chowdhury, Michael

J.
Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster
computing with working sets. In Proceedings of the
2nd USENIX Conference on Hot Topics in Cloud Com-
puting, HotCloud’10, page 10, USA, 2010. USENIX
Association.

A Communication Analysis of LSHS

We extend the α − β model of communication to analyze the
communication time of element-wise, reduction, and basic
linear and tensor algebra operations. In our model, for a par-
ticular channel of communication, α denotes latency and β
denotes the inverse bandwidth of the channel. We also model
the time to dispatch an operation from the driver process as
γ, called the dispatch latency. The time to transmit n bytes is
given by α + βn.

For a dense array of size N, let p the number of workers,
N/p = n the block size, or number of elements, and r = p/k
the number of workers-per-node on a k node cluster.

Let C(n) = α + βn denote the time to transmit n bytes be-
tween two nodes within a multi-node cluster. Let D(n) =
α(cid:48)(cid:48) + β(cid:48)(cid:48)n. This is the cost of transmitting data between work-
ers within a single node in Dask. Let R(n) = α(cid:48) + β(cid:48)n. This
expression captures the implicit cost of communication be-
tween workers within a single node of Ray. Ray maintains a
shared memory key/value store on each node, enabling every
worker to access data written by any other worker without
explicit communication.

We assume α >> α(cid:48)(cid:48) > α(cid:48), and β >> β(cid:48)(cid:48) > β(cid:48). We expect
α(cid:48)(cid:48) > α(cid:48) and α(cid:48)(cid:48) > α(cid:48) because Dask relies on the TCP proto-
col for object transmission between workers within the same
node, whereas Ray writes data directly to Linux shared mem-
ory.

All lower bounds are given in terms of Ray’s communica-
tion time. Our lower bounds depend on the assumption that a
block need only be transmitted to a node once, after which it
is cached by Ray’s object store. We also assume Ray’s object
store is large enough to hold all intermediate objects which it
caches. We also assume that bytes may be sent and received
in parallel, which is possible in Ray. We present the γ term
once as part of the lower bound, since in all cases, the same
number of operations are dispatched from the driver process.

Algorithm 3: Recursive Matrix Multiplication.

Function RMM(A, B):

if NOTPARTITIONED(A, B) then
return MATMUL(A, B) ;

i, j,h ← RMM(A_i,h, B_h,j)} ;

∀m−1,n−1,k−1
i=0, j=0,h=0 {C(cid:48)
∀m−1,n−1
i=0, j=0 { Ci, j ← Reduce(C(cid:48)
return C ;

i, j) } ;

Our results are more easily presented and understood
in terms of the recursive matrix multiplication algorithm,
which is given by Algorithm 3. Nested blocks of a recur-
sively partitioned matrix X are obtained by the subscripts
((Xi1, j1)i2, j2) . . . )id , jd , where d is the depth of the nested
blocks of arrays. At depth d, a sub-matrix is detected using
the predicate NOTPARTITIONED, and the sub-matrices are

15

multiplied. In our analysis, we only consider nested matrices
of depth 2. We refer to the ﬁrst level block partitions as the
node-level partitions, and the second-level partitions as the
worker-level partitions.

A.1 Elementwise Operations

For unary operations (e.g. −x), we assume x ∈ Rn and x is
partitioned into p blocks. The lower bound for this operation
is γp. LSHS on Dask and the Dask scheduler will incur 0 com-
munication overhead. LSHS on Ray and the Ray scheduler
will incur a communication overhead of approximately R(n).
For binary element-wise operations, we assume x, y ∈ Rn
and both are partitioned into p blocks. The lower bound for
this operation is γp. LSHS on Dask achieves 0 communica-
tion for element-wise operations: The LSHS data placement
procedure ensures blocks in x and y are stored on the same
workers, and the cost of executing on some other worker is
strictly greater than executing on the worker on which the two
input blocks already exist: The memory load is the same on
whatever worker the operation is executed, but the network
load is greater on workers other than the worker on which
the input blocks already reside. Similarly, LSHS on Ray en-
sures blocks are placed on the same nodes, but can guarantee
at most R(n) time due to constant overhead associated with
writing function outputs to its object store.

A.2 Reduction Operations

Without loss of generality, we provide the communication
time for sum(X). Assume X ∈ Rn,d is tall-skinny, n >> d, so
that X is block-partitioned along its ﬁrst axis into p partitions
of blocks with dimensions (n/p) × d. For the addition oper-
ation, the sum operation sums all blocks in X and outputs a
block of (n/p) × d.

Due to the same argument presented for element-wise oper-
ations, LSHS ﬁrst performs bop on operands which already re-
side on the same nodes. Recall that r = p/k. The lower bound
for this operation is γ(p − 1) + log2(r)R(n) + log2(k)C(n).

For Dask, LSHS incurs a cost of log2(r)D(n) for local
reductions, plus log2(k)C(n) for the remaining k blocks.
Likewise, LSHS on Ray incurs a cost of log2(r)R(n) +
log2(k)C(n).

This operation will execute matrix multiplication between
Xi and Yi, where Zi denotes the ith block of array Z. Let
the output of the previous procedure be denoted by the
block-partitioned array W . The ﬁnal step of this operation
is reduce(add,W ). Thus, the analysis provided for element-
wise and reduce operations also apply to this operation.

The lower bound for this operation is γ(p + p − 1) +
log2(k)C(n) + (1 + log2(r))R(n). For LSHS on Dask, we
have log2(k)C(n) + log2(r)D(n), and for Ray we have
log2(k)C(n) + (1 + log2(r))R(n).

Empirically, we observe that LSHS on Ray is slightly
faster than LSHS on Dask for this operation. This sug-
gests that (1 + log2(r))R(n) < log2(r)D(n) and R(n) <
log2(r)(D(n) − R(n)), which is reasonable given our assump-
tion that R(n) < D(n). As R(n) goes to 0, this inequality goes
to 0 < log2(r)D(n), suggesting that Dask’s performance is ex-
plained by worker-to-worker communication within a single
node.

A.4 Block-wise Outer Product

√

√

Assume X, Y ∈ Rn,d is tall-skinny, n >> d, so that X, Y are
p partitions of
block-partitioned along their ﬁrst axes into
p) × d. The block-wise outer
blocks with dimensions (n/
product is deﬁned as XY(cid:62). The output Z will be a
p ×
p
k − 1)rC(n)
grid of blocks. Every node must transmit 2(
node-level blocks to every row and column in its grid (minus
itself), and every off-diagonal node must receive 2rC(n) node-
level blocks, resulting in a communication lower bound of
k − 1)rC(n), which is also the communication time
γp + 2(
attained by LSHS.

√

√

√

√

For LSHS on Dask, blocks placed within the diagonal of
our k × k logical grid of nodes will not incur any inter-node
communication overhead. This constitutes k of the k2 logi-
cal nodes. Within this diagonal grid of nodes, blocks placed
within the diagonal of the r × r grid of logical workers within
each node can be used to compute the output Zi,i = XiY(cid:62)
i
without worker-to-worker communication. This constitutes
r such workers per node along the diagonal of our logical
grid of nodes. We therefore incur a communication time
of k(r2 − r)D(n) for blocks placed on the diagonal of the
logical grid of nodes. Computing output blocks on the rest
of the nodes requires an inter-node communication time of
(k2 − k)C(n).

A.3 Block-wise Inner Product

A.5 Matrix Multiplication

Assume X, Y ∈ Rn,d is tall-skinny, n >> d, so that X, Y are
block-partitioned along their ﬁrst axes into p partitions of
blocks with dimensions (n/p) × d. Under these conditions,
we deﬁne the block-wise inner product as X (cid:62)Y . This is the
most expensive operation required to compute the Hessian
matrix for generalized linear models optimized using New-
ton’s method.

16

√

√

Let Z = XY for X, Y ∈ Rn×n. Both are partitioned into
√

p ×
p grids. Z will have the same dimension and partitioning as
p3 = p3/2) block operations: For each
X and Y. We have O(
p matrix multiplies,
of the
√
p − 1 additions. The same arguments used to derive the
and
communication time for block-wise inner and outer products
can be applied to matrix multiplication.

p2 = p output blocks, we have

√

√

√

√

√
√

Our implementation of matmul is a special case of our re-
cursive implementation of tensordot. We can therefore view
k grid of node-level blocks,
k ×
the entire computation as a
r worker-level blocks.
r ×
each of which are comprised of
At the node-level, we have k3/2 matrix multiplies,
k of
which require 0 inter-node communication. The remaining
k3/2 −
k node-level blocks are parallelized over k nodes,
which requires at least (k3/2 −
kr of the
worker-level blocks to be transmitted between nodes, yielding
a lower bound of (k − 1)/

k)/kr = (k − 1)/

krC(n) <

krC(n).

√

√

√

√

√

√

√

On each node, we have approximately r

r addition op-
erations. Over r local workers, we achieve approximately
r)R(n) communication time. At the node-level, we now
log(
k node-level blocks of size rn. With k nodes,
need to add k
√
k)rC(n) communica-
this will require approximately log(
tion time.

√

√

√

Putting this all

+ log(

bound for matrix multiplication
(cid:16) k−1√
the diagonal
√
(cid:16)√
k + log(

rC(n) + log(
the
for
√

terms
(cid:17)
rC(n) + log(

together, the communication lower
approximately
is
r)R(n). We
ignore
of
simpler expression

r)R(n).

k)

k)

√

(cid:17)

k

A.5.1 SUMMA

Algorithm 4: SUMMA
Zi, j ← 0;
for h ← 0 to

√

√
√

p do
Broadcast Xi,h to
Broadcast Yh, j to
column j;
Zi, j ← Zi, j + Xi,hYh, j;

p workers in worker grid row i;
p workers in worker grid

end

The blocked Scalable Universal Matrix Multiplication Al-
gorithm (SUMMA) [27] is characterized by Algorithm 4. The
blocks are partitioned over workers so that worker with grid
coordinates i, j stores blocks Xi, j, Yi, j, and the output block
Zi, j.

√

A tree-based broadcast has an inter-node communication
p)C(n). The algorithm performs 2 such broad-
p, yielding an inter-node communi-
p log(

cost of (log
casts per iteration over
cation complexity of 2

√
√

√

p)C(n).
√

√

2

√

√

√

r log(

kr log(

kr)C(n)

k)C(n) + 2

requires
√

SUMMA
√
k(log(

=
√
r)C(n). We can see that
2
k)C(n) for
SUMMA has a communication time of 2
inter-node communication. The inter-node communication
component of our lower bound is r
C(n).
We immediately see that the lower bound depends primarily
on r. In practice, r is set to the number of physical cores
per node, which is relatively small (32 in our benchmarks).
k) grows faster
We see that, asymptotically, 2

k + log(

k(log(

k(log(

(cid:16)√

k)

√

√

√

(cid:17)

(cid:16)√

√

(cid:17)

k)

k + log(

, suggesting asymptotically faster
than
communication time in k. While we are unable to show
that NumS attains this lower bound, we believe it helps
explain the competitive performance we achieve, especially
for k = 16 (Figure 10). On the other hand, systems such as
SLATE have no γ term. For larger partitions, we expect γ to
be a bottleneck in systems such as NumS and Dask Arrays.

17

