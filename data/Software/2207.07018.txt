REDUCTION OF THE RANDOM ACCESS MEMORY SIZE IN
ADJOINT ALGORITHMIC DIFFERENTIATION BY OVERLOADING

UWE NAUMANN∗

Key words. algorithmic diﬀerentiation, adjoint, overloading, random access memory, band-

width

Abstract. Adjoint algorithmic diﬀerentiation by operator and function overloading is based
on the interpretation of directed acyclic graphs resulting from evaluations of numerical simulation
programs. The size of the computer system memory required to store the graph grows proportional
to the number of ﬂoating-point operations executed by the underlying program. It quickly exceeds
the available memory resources. Naive adjoint algorithmic diﬀerentiation often becomes infeasible
except for relatively simple numerical simulations.

Access to the data associated with the graph can be classiﬁed as sequential and random. The
latter refers to memory access patterns deﬁned by the adjacency relationship between vertices within
the graph. Sequentially accessed data can be decomposed into blocks. The blocks can be streamed
across the system memory hierarchy thus extending the amount of available memory, for example,
to hard discs. Asynchronous i/o can help to mitigate the increased cost due to accesses to slower
memory. Much larger problem instances can thus be solved without resorting to technically chal-
lenging user intervention such as checkpointing. Randomly accessed data should not have to be
decomposed. Its block-wise streaming is likely to yield a substantial overhead in computational cost
due to data accesses across blocks. Consequently, the size of the randomly accessed memory required
by an adjoint should be kept minimal in order to eliminate the need for decomposition. We propose a
combination of dedicated memory for adjoint L-values with the exploitation of remainder bandwidth
as a possible solution. Test results indicate signiﬁcant savings in random access memory size while
preserving overall computational eﬃciency.

1. Introduction. Building on prior work in [18] we consider a given implemen-
tation of a diﬀerentiable multivariate vector function F : Rn → Rm : x (cid:55)→ y = F (x)
over the real numbers R as a diﬀerentiable computer program over ﬂoating-point
numbers [1] referred to as the primal program. Note that diﬀerentiability of a func-
tion does not imply (algorithmic) diﬀerentiability of the given implementation. A
prime example is a table lookup of a dedicated value of a diﬀerentiable function yield-
ing a vanishing algorithmic derivative as for example in if (x==0) y=0; else y=sin(x);
(Algorithmic) Diﬀerentiability of the given implementation implies diﬀerentiability
of the function due to the chain rule of diﬀerentiation. Algorithmic diﬀerentiation
(AD) [12, 17] yields a variety of implementations of the transformation of the given
diﬀerentiable program into a program for evaluating its derivatives. We focus on AD
by operator and function overloading as supported, for example, by C++. The fol-
lowing discussion is presented in the context of a serial evaluation of F as well as of
its (adjoint) derivatives. Generalization to parallel scenarios based on shared [2, 7] or
distributed [19, 22] memory architectures or on massively parallel accelerators [11, 16]
is the subject of ongoing research.
The adjoint [6] of F becomes

(1.1)

¯F : Rn × R1×m → R1×n :

(x, ¯y) (cid:55)→ ¯x = ¯F (x, ¯y) ≡ ¯y · F (cid:48) ,

2
2
0
2

l
u
J

3
1

]
S
M

.
s
c
[

1
v
8
1
0
7
0
.
7
0
2
2
:
v
i
X
r
a

where

dF
dx
denotes the Jacobian of F at the given point x. Equation (1.1) is evaluated by the

F (cid:48) = F (cid:48)(x) ≡

(x) ∈ Rm×n

∗Informatik 12: Software and Tools for Computational Engineering, RWTH Aachen University,

Germany. naumann@stce.rwth-aachen.de

1

 
 
 
 
 
 
adjoint program resulting from the application of adjoint AD to the given implemen-
tation of F .

Without loss of generality, the following discussion is based on the assumption of
a single scope primal. Eﬀects due to allocation and deallocation of program variables
are not accounted for. Additional technical issues due to speciﬁcs of the programming
language as well as due to the custom design of the AD tool at hand need to be taken
into account when integrating this paper’s ideas into state-of-the-art AD software
tools. Examples throughout this paper as well the reference implementation (see
Section 6) are written in C++.

Ordered sets V = {0, . . . , n + q − 1} (indexes of variables), X = {0, . . . , n − 1}
(indexes of independent input variables) and Y ⊆ V, |Y | = m (indexes of dependent
output variables) are induced by the primal program for given values of the inputs.
The primal program is regarded as a composition of elemental functions (also: ele-
mentals) ϕj evaluated as a single assignment code (each variable vj is written once as
the result of ϕj)

vi = xi

for i ∈ X ;

(1.2)

vj := ϕj ((vi)i≺j)
if j ∈ Y

yk++ = vj

(cid:41)

for j ∈ V \ Xi and k := 0 initially.

C++-style incrementation k++ denotes k := k + 1. Ordered sets are traversed in
the given order unless stated otherwise. For example, i ∈ X corresponds to “for
i = 0, . . . , n − 1.” Following [12] we use i ≺ j to denote vi as an argument of ϕj.
We write = for mathematical equality, ≡ in the sense of “is deﬁned as” and := to
represent assignment as in imperative programming languages. While all experiments
are performed with C++ the conceptual results are applicable to any imperative
programming language oﬀering support for operator and function overloading.

Equation (1.2) induces a directed acyclic graph (DAG) G = (V, E) with vertices
V = {0, . . . , n + q − 1} and E ⊆ V × V. Vertices are partitioned into inputs X =
{0, . . . , n − 1}, outputs Y ∈ {0, . . . , n + q − 1}, |Y | = m. Local partial derivatives
dj,i ≡ ∂vj
∂vi

((vl)l≺j) of the elementals are associated with all edges.

In the following we propose special treatment of L-values associated with a phys-
ical address in the system memory occupied by the primal program. Such variables
can appear on the left-hand side of assignments. The assignment operator is invoked
on L-values only.

The construction of G = G(x) relies on x ﬁxing the ﬂow of control in the primal
program. Overloading tools for AD use diﬀerent representations of the DAG which are
commonly referred to as tapes. We separate sequentially and randomly accessed parts
of G by storing them in sequentially accessed memory (SAM) and randomly accessed
memory (RAM), respectively. G is represented by three arrays s ∈ Z2·|V |−|X|+|E|,
d ∈ R|E|, and ¯v ∈ R|RAM|. The vector s ⊂ SAM describes the structure of G starting
with the inputs. Entries are generated for each elemental function evaluation in form
of its arguments followed by the number of arguments and the result. The derivatives
associated with all edges are stored in d ⊂ SAM. Adjoints are stored in ¯v = RAM.

This internal representation of G is often referred to as a gradient tape. Note
that the elementals ϕj, j = 0, . . . , n + q − 1, can represent arbitrary diﬀerentiable
multivariate vector functions. The sole requirement is the availability of the cor-
responding adjoint elementals. Without loss of generality the previously proposed
internal representation makes the additional assumption that all elementals are scalar
functions.

2

Fig. 1.1. DAG

As an alternative to the gradient tape a value tape would store the values vj
alongside corresponding operation codes for all ϕj, j = 0, . . . , n + q − 1. Reevaluation
of the primal at diﬀerent points requires repeated recording of the gradient tape while
value tapes can be reevaluated by interpretation. An in-depth discussion of the pros
and cons of gradient vs. value tapes is beyond the scope of this paper. Numerous
technical details and speciﬁcs of the given use cases inﬂuence the decision about the
preferred approach.

State of the art implementations of AD make use of advanced template metapro-
gramming features of modern C++ [15, 20, 21]. As a result right-hand sides of
assignments or even entire basic blocks can become elementals. The entire RAM can
be occupied by L-values exclusively. The validity of the novel approach to handling
of adjoint memory to be proposed in the following remains unaﬀected.

Adjoint code generated by source code transformation tools such as TAF [9] or
Tapenade [14] does not beneﬁt substantially from the following discussion. It typically
uses an exact image (also: shadow memory) of the memory occupied by the primal
program for the storage of the adjoints. The discussion of potential shortcomings of
this approach is beyond the scope of this paper.

Example. Let f : R → R : x (cid:55)→ y = f (x) be implemented in C++ as 1

template<typename T>

1
2 void f ( s t d : : v e c t o r <T>& v ) {

3

4

5

6

7

8

T u ;
f o r ( s i z e t

i =1; i <v . s i z e ( ) ; i ++) {

u=s i n ( v [ i − 1 ] ) ;
v [ i ]=u∗u+v [ 0 ] ;

}

}

for v ∈ Rl, v0 = x = 1 and y = vl−1. We set l = 3. The corresponding DAG is stored
as

sT = (0 0 1 1 1 1 2 2 0 2 3 3 1 4 4 1 5 5 0 2 6) ∈ Z21
dT = (0.54 1.68 1 1 − 0.14 1.98 1 1) ∈ R8 .

The vector s is best interpreted in reverse as it is the case in adjoint AD. Vertex 6
has the 2 predecessors 0 and 5. Vertex 5 has the single predecessor 4. ... Vertex 1 has
the single predecessor 0. Vertex 0 is the only input. The vector d contains the local
partial derivatives associated with all edges in the order induced by s. Edge (0, 1) is

1This example is supposed to illustrate certain aspects of diﬀerent approaches to the propagation
of adjoints. We do not claim for it to be a reasonable implementation of a practically relevant
mathematical model as a computer program; see Section 5 for real-world applications.

3

2310654labelled with

∂v1
∂v0

=

∂ sin(v0)
∂v0

= cos(v0) = cos(1) ≈ 0.54

and so forth. Floating-point values are rounded to nearest [1]. A visualization of the
DAG can be found in Figure 1.1.

In the following we present three methods for the propagation of adjoints using dif-
ferent approaches to the allocation of the vector of adjoints ¯v ∈ R|RAM|. We aim
for |RAM| → min . Naive adjoint AD records the entire DAG followed by its inter-
pretation as recalled in Section 2. Exploitation of bandwidth is likely to result in a
reduction of the adjoint memory requirement as described in Section 3. More substan-
tial improvements can typically be expected from the provision of dedicated memory
for adjoint L-values; see Section 4.

2. Flat adjoints by interpretation of the DAG. Basic adjoint AD builds
G = (V, E) (recording) followed by allocation of ¯v ∈ R|V |, initialization of ¯yk, k ∈ Y
(seeding), interpretation of (G, ¯v) (also: back-propagation) and extraction of adjoints
of the inputs ¯vi, i ∈ X (harvesting).

For given ¯yk, k ∈ Y and setting ¯vj := 0 for j ∈ V \ Y, back-propagation amounts

to the evaluation of the adjoint program as

(2.1)

¯vjk = ¯yk
¯vi += ¯vj · dj,i
¯xi = ¯vi

for k = 0, . . . , m − 1 and jk ∈ Y ;

for i ≺ j and j = n + q − 1, . . . , n ;

∀ i ∈ X .

All conditions formulated for Equation (1.2) apply. See [12] for a proof of correct-
ness of Equation (2.1). We use the C++-style notation ¯vi += ¯vj · dj,i to abbreviate
¯vi := ¯vi + ¯vj · dj,i. RAM of size |V |·σ is required, where σ denotes the number of bytes
occupied by a scalar adjoint, e.g. σ = 8 for double precision ﬂoating-point variables
according to the IEEE 754 standard [1].

Example. Setting ¯y = 1 yields ¯vT = (0 0 0 0 0 0 1) ∈ R7 as |V | = 7. Interpretation

makes ¯vT evolve as

(1 0 0 0 0 1 0)

(1 0 0 0 1.98 0 0)

(1 0 0 − 0.27 0 0 0)

(0.73 0 − 0.27 0 0 0 0)

(0.73 − 0.46 0 0 0 0 0 )

(0.48 0 0 0 0 0 0)

[¯v0 += d7 · ¯v6; ¯v5 += d6 · ¯v6; ¯v6 := 0]
[¯v4 += d5 · ¯v5; ¯v5 := 0]
[¯v3 += d4 · ¯v4; ¯v4 := 0]
[¯v0 += d3 · ¯v3; ¯v2 += d2 · ¯v3; ¯v3 := 0]
[¯v1 += d1 · ¯v2; ¯v2 := 0]
[¯v0 += d0 · ¯v1; ¯v1 := 0]

resulting in ¯x = ¯v0 = 0.48.

Setting ¯vj := 0 after use in the second line of Equation (2.1) is actually obsolete for
ﬂat adjoints. It becomes essential for ensuring correctness of the alternative methods
to be proposed in Sections 3 and 4.

3. Exploitation of bandwidth. The bandwidth β of G = (V, E) is deﬁned
for a given topological order of the vertices as the length j − i of the longest edge
(i, j) ∈ E, that is,

β ≡ max
(i,j)∈E

(j − i) .

4

The topological order is induced by the sequence of elemental functions evaluated by
the primal program; see Equation (1.2). Finding a topological order which minimizes
the bandwidth is known to be NP-complete [8]. RAM of size max(β, n, m) is suﬃcient
to evaluate the primal program and its adjoint, respectively.

Equation (1.2) can hence be evaluated as

(3.1)

vi = xi
vj%β := ϕj
yk++ := vj%β

(cid:1)

(cid:0)(vi%β)i≺j
if j ∈ Y

for i ∈ X ;

for j ∈ V \ X and k := 0 initially.

Dedicated outputs are required as the exploitation of bandwidth may yield overwrites;
see the explicit assignment in the third line of Equation (3.1). Correctness follows
immediately from Equation (1.2) realizing that liveness of the vj can be restricted to
the evaluation of all vk with k ≤ j + β.

Setting ¯vj := 0 for j ∈ V the interpretation of G can be performed for given ¯y in

RAM of size β · σ as

¯v(jk%β) := ¯yk

for k = 0, . . . , m − 1 and jk ∈ Y ;

w := ¯v(j%β); ¯v(j%β) := 0
¯v(i%β) := ¯v(i%β) + w · dj,i for i ≺ j

for j = n + q − 1, . . . , n ;

¯xi = ¯v(i%β)

for i = 0, . . . , n − 1 .

Reuse of RAM locations for distinct variables requires resetting them to zero after
use in the second equation. Correctness follows immediately from Equation (3.1).

Example. Setting ¯y = 1 yields ¯vT = (1 0 0 0 0 0) ∈ R6 as β = 6 − 0 = 6.

Interpretation makes ¯vT evolve as

(1 0 0 0 0 1)

(1 0 0 0 1.98 0)

(10 0 − 0.27 0 0)

(0.73 0 − 0.27 0 0 0)

(0.73 − 0.46 0 0 0 0)

(0.48 0 0 0 0 0)

[w := ¯v0; ¯v0 := 0; ¯v0 += d7 · w; ¯v5 += d6 · w]
[w := ¯v5; ¯v5 := 0; ¯v4 += d5 · w]
[w := ¯v4; ¯v4 := 0; ¯v3 += d4 · w]
[w := ¯v3; ¯v3 := 0; ¯v0 += d3 · w; ¯v2 += d2 · w]
[w := ¯v2; ¯v2 := 0; ¯v1 += d1 · w; ]
[w := ¯v1; ¯v1 := 0; ¯v0 += d0 · w]

resulting in ¯x = ¯v0 = 0.48.

The reduction in the size of required RAM is not impressive as the longest edge
spans nearly the entire computation. Similar observations can be made for many
practically relevant numerical simulations which is why the exploitation of bandwidth
alone is typically not enough. Perpetuation has been proposed in [18] to address this
issue. A potentially more powerful method will be proposed in Section 4.

The eﬀect of exploiting the bandwidth becomes much more signiﬁcant for simple

evolutions of length l deﬁned as

v = F (F (. . . F (v) . . .))
(cid:125)

(cid:124)

(cid:123)(cid:122)
l times

for F : Rn → Rn. The bandwidth remains bounded by 2 · n independent of l. The
adjoint interpretation of G can be performed in RAM of size 2 · n · σ.

5

Fig. 4.1. DCG

4. Dedicated adjoint L-values. Unique locations in system memory are as-
signed to all L-values. A directed cyclic graph (DCG) is induced due to potential
overwrites. The total size of RAM can often be reduced signiﬁcantly, possibly at the
expense of an increase in the size of SAM.

Let V = (L, R), R ≡ V \ L (remainder) with X ∪ Y ⊆ L. Deﬁne

& : V → {−pL, . . . , |R|} : i (cid:55)→

(cid:40)

j ∈ {−1, . . . , −pL}
j ∈ {0, . . . , |R|}

i ∈ L
i ∈ R

to be injective over L and bijective over R with pL denoting the number of distinct
L-values (with distinct physical addresses) in the primal program.

Let E|R ≡ {(i, j) ∈ E : i, j ∈ R}. Deﬁne the remainder bandwidth of G as

βR = β(E|R) ≡ max

(i,j)∈E|R

&(j) − &(i) .

Let the mapping of DAG vertices i = 0, . . . , n + q − 1 to RAM with dedicated

L-values be deﬁned as

(cid:40)

#(i) ≡

&(i)
&(i)%βR if i ∈ R .

if i ∈ L

The primal program can be evaluated in RAM of size pL + βR as

(cid:0)(v#(i))i≺j

v#(i) = xi
v#(j) := ϕj
k := 0
yk++ = v#(jk)

for i ∈ X ;

(cid:1)

for j = n, . . . , n + q − 1 ;

for jk ∈ Y and k := 0 initially .

The corresponding adjoint becomes equal to

¯v#(jk) = ¯yk++

for jk ∈ Y and k := 0 initially ;

w := ¯v#(j); ¯v#(j) := 0

¯v#(i) += w · dj,i
¯xi = ¯v#(i)

for i ≺ j

for j = n + q − 1, . . . , n ;

for i = 0, . . . , n − 1 .

6

12-30-25-4-143Fig. 5.1. Black-Scholes equation: DAG for three Monte Carlo paths.

Example. The DCG is stored as

sT = (−1 − 1 1 0 0 1 − 2 − 2 − 2 2 1 1 − 1 2 2 2 1 − 3 − 3 1 3 3 1 − 2 . . .

. . . − 2 − 2 2 4 4 − 1 2 5 5 1 − 4) ∈ Z35

dT = (0.54 1 1.68 1 1 1 − 0.14 1 1.98 1 1 1) ∈ R12

A visualization can be found in Figure 4.1. Note that vertex −2 corresponds to the
program variable u and is hence visited twice.

Setting ¯y = 1 yields ¯vT = (0 0 0 1 0) ∈ R5 as pL + βR = 4 + 1 = 5. The vertices

of the DCG are visited as

−4, 5, − 1, , 4, − 2, 3, − 3, 2, − 1, 1, − 2, 0, − 1,

where all non-negative vertices are mapped onto ¯v4 while the L-values corresponding
to the program variables v [0] , u, v [1] , and v [2] are represented by vertices −1, −2,
−3, and −4, which are mapped onto ¯v0, ¯v1, ¯v2, and ¯v3, respectively. Interpretation
makes ¯vT evolve as

(0 0 0 0 1)

(1 0 0 0 1)

(1 1.98 0 0 0)

(1 0 0 0 1.98)

(1 0 − 0.27 0 0)

(1 0 0 0 − 0.27)

(0.73 0 0 0 − 0.27)

(0.73 − 0.46 0 0 0)

(0.73 0 0 0 − 0.46)

(0.48 0 0 0 0)

[w := ¯v3; ¯v3 := 0; ¯v4 += d11 · w]
[w := ¯v4; ¯v4 := 0; ¯v0 += d10 · w; ¯v4 += d9 · w]
[w := ¯v4; ¯v4 := 0; ¯v1 += d8 · w]
[w := ¯v1; ¯v1 := 0; ¯v4 += d7 · w]
[w := ¯v4; ¯v4 := 0; ¯v2 += d6 · w]
[w := ¯v2; ¯v2 := 0; ¯v4 += d5 · w]
[w := ¯v4; ¯v4 := 0; ¯v0 += d3 · w; ¯v4 += d4 · w]
[w := ¯v4; ¯v4 := 0; ¯v1 += d2 · w]
[w := ¯v1; ¯v1 := 0; ¯v4 += d1 · w]
[w := ¯v4; ¯v4 := 0; ¯v0 += d0 · w]

resulting in ¯x = ¯v0 = 0.48.

5. Case Studies. One of the case studies considers the solution of the Black-
Scholes stochastic diﬀerential equation [3] by Monte Carlo simulation [10]. First, we

7

13523112373124191270342210404139273638333228303291618174652615252120141398Fig. 5.2. Black-Scholes equation: DCG for three Monte Carlo paths.

take a closer look at a simpliﬁed scenario running only three paths resulting in the
DAG in Figure 5.1. Total ﬂattening yields a RAM requirement of 42 · 8b = 336b.
The associated SAM occupies 992b. The longest edge is (2, 37) yielding a total RAM
requirement of (35 + 1) · 8b = 288b under exploitation of bandwidth and with a
dedicated adjoint for the single output. The SAM size remains unchanged. Dedicated
adjoint L-values lead to the DCG in Figure 5.2. The longest edge is (0, 4). With ten
dedicated adjoint L-value the total RAM requirement becomes equal to 14·8b = 112b,
which amounts to one third of the RAM required by a total ﬂattening approach due
to complete mutual independence of the individual paths; see also [13]. The SAM
requirement is increased slightly to 1172b.

Test results for a selection of practically relevant problems are listed in Tables 5.1
and 5.2. They illustrate the potential of dedicating memory to adjoint L-values.
RAM requirement can be reduced signiﬁcantly. We compare RAM sizes resulting
from the approaches discussed in Sections 2–4 in Table 5.1. The following problems
are considered:

(a) Black-Scholes Equation; ﬁnite diﬀerence scheme on 3 · 102 × 9 · 104 grid
(b) Burgers Equation [5]; ﬁnite diﬀerence scheme with upwind on 102 × 104 grid
(c) LIBOR Market Model [4]; Monte Carlo simulation with 104 paths
(d) Black-Scholes Equation; Monte Carlo simulation with 107 paths.

All numerical results were cross-validated and compared with ﬁnite diﬀerence approx-
imations. Table 5.2 shows the corresponding SAM sizes. An increase between seven
and up to thirty per cent can be observed. This drawback can be mitigated by asyn-
chronous streaming of the data across the memory hierarchy. Acceptable slow-down
due to the implementation of the additional logic or even speed-up due to the reduc-
tion in RAM size was observed as supported by the wall clock time measurements
in Table 5.1. All problem sizes allowed for accommodation of both RAM and SAM
within the system’s main memory (approximately 15gb available on our Intel Xeon
workstation running Linux and gcc version 9.4.0).

6. Conclusion. The allocation of dedicated memory for adjoint L-values com-
bined with the exploitation of remainder bandwidth yields a substantial reduction of
random access memory requirement in derivative programs obtained by adjoint AD

8

123-3-7-2-6-1-504-9-10373623353234332720158311973029186282426-82512141322211716111095-4problem
(a)
(b)
(c)
(d)

Sec. 2
3.383.079.584 (17,4)
901.360.800 (4,3)
1.745.200.424 (8,1)
999.989.800 (4,6)

Sec. 3
3.383.077.128 (18,1)
901.356.056 (4,7)
1.745.199.448 (8,9)
999.989.744 (5,2)

Sec. 4
9.736 (17,2)
133.920.840 (6,0)
16.240.664 (9,0)
112 (5,0)

Table 5.1
RAM requirement in bytes (wall clock times in seconds).

problem Sec. 2 & Sec. 3
11.306.171.264
2.863.480.400
5.232.699.600
3.279.959.064

(a)
(b)
(c)
(d)

Sec. 4
12.361.174.724
3.906.400.400
6.243.799.340
3.479.959.184

Table 5.2
SAM requirement in bytes.

by overloading. Our reference implementation can be found on

https://github.com/un110076/dedicated_l_values.git

Its level of sophistication lacks
allowing for qualitative reproduction of all results.
behind state-of-the-art implementations of adjoint AD by overloading in C++ such as
the solutions oﬀered, for example, by NAG.2 It pursues a diﬀerent purpose by aiming
to serve as an easy-to-follow illustration of the main algorithmic ideas presented in
this paper.

REFERENCES

[1] IEEE standard for ﬂoating-point arithmetic. IEEE Std 754-2019 (Revision of IEEE 754-2008),

pages 1–84, 2019.

[2] C. Bischof, N. Guertler, A. Kowarz, and A. Walther. Parallel reverse mode automatic diﬀer-
entiation for OpenMP programs with ADOL-C. In C. Bischof, M. B¨ucker, P. Hovland,
U. Naumann, and J. Utke, editors, Advances in Automatic Diﬀerentiation, pages 163–173.
Springer, 2008.

[3] F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political

Economy, 81(3):637–654, 1973.

[4] A. Brace, D. Gatarek, and M. Musiela. The market model of interest rate dynamics. Mathe-

matical Finance, 7:127–147, 1997.

[5] J. Burgers. Mathematical examples illustrating relations occurring in the theory of turbulent
ﬂuid motion. Verhandelingen der Koninklijke Nederlandse Akademie van Wetenschappen,
Afdeeling Natuurkunde, 2(17):1–53, 1939.

[6] N. Dunford and J.T. Schwartz. Linear Operators, Part 1: General Theory. Wiley Classics

Library. Wiley, 1988.

[7] M. F¨orster. Algorithmic Diﬀerentiation of Pragma-Deﬁned Parallel Regions: Diﬀerentiating

Computer Programs Containing OpenMP. Springer, Wiesbaden, 2014.

[8] M. Garey, R. Graham, D. Johnson, and D. Knuth. Complexity results for bandwidth mini-

mization. SIAM J. Appl. Math., 34:477–495, 1978.

[9] R. Giering and T. Kaminski. Recipes for adjoint code construction. ACM Transactions on

Mathematical Software, 24(4):437–474, 1998.

[10] P. Glasserman. Monte Carlo Methods in Financial Engineering. Springer, New York, NY,

USA, 2004.

[11] F. Gremse, A. Hoefter, L. Razik, F. Kiessling, and U. Naumann. GPU-accelerated adjoint

algorithmic diﬀerentiation. Computer Physics Communications, 200:300–311, 2016.

[12] A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algo-

2Numerical Algorithms Group Ltd., Oxford, UK; www.nag.com

9

rithmic Diﬀerentiation. Number 105 in Other Titles in Applied Mathematics. SIAM,
Philadelphia, PA, 2nd edition, 2008.

[13] L. Hasco¨et, S. Fidanova, and C. Held. Adjoining independent computations. In G. Corliss,
C. Faure, A. Griewank, L. Hasco¨et, and U. Naumann, editors, Automatic Diﬀerentiation
of Algorithms: From Simulation to Optimization, Computer and Information Science,
chapter 35, pages 299–304. Springer, New York, NY, 2002.

[14] L. Hasco¨et and V. Pascual. The Tapenade automatic diﬀerentiation tool: Principles, model,
and speciﬁcation. ACM Transactions on Mathematical Software, 39(3):20:1–20:43, 2013.
[15] R. Hogan. Fast reverse-mode automatic diﬀerentiation using expression templates in C++.

ACM Transactions on Mathematical Software, 40(4):26:1–26:24, jun 2014.

[16] W. Moses, V. Churavy, L. Paehler, J. H¨uckelheim, K. Narayanan, M. Schanen, and J. Doerfert.
Reverse-mode automatic diﬀerentiation and optimization of GPU kernels via enzyme. In
Proceedings of the International Conference for High Performance Computing, Network-
ing, Storage and Analysis, SC ’21, New York, NY, USA, 2021. Association for Computing
Machinery.

[17] U. Naumann. The Art of Diﬀerentiating Computer Programs. An Introduction to Algorithmic

Diﬀerentiation. Number SE24 in Software, Environments, and Tools. SIAM, 2012.

[18] U. Naumann and K. Leppkes. Low-memory algorithmic adjoint propagation.

In 2018 Pro-
ceedings of the Eighth SIAM Workshop on Combinatorial Scientiﬁc Computing, pages
1–10.

[19] U. Naumann, J. Utke, J. Riehme, P. Hovland, and C. Hill. A framework for proving correctness
of adjoint message passing programs. In Proceedings of EUROPVM/MPI 2008, pages 316–
321, 2008.

[20] E. Phipps and R. Pawlowski. Eﬃcient expression templates for operator overloading-based
automatic diﬀerentiation. In S. Forth, P. Hovland, E. Phipps, J. Utke, and A. Walther,
editors, Recent Advances in Algorithmic Diﬀerentiation, volume 87 of Lecture Notes in
Computational Science and Engineering, pages 309–319. Springer, Berlin, 2012.
[21] M. Sagebaum, T. Albring, and N. Gauger. Expression templates for primal value taping in
the reverse mode of algorithmic diﬀerentiation. Optimization Methods & Software, 33(4–
6):1207–1231, 2018.

[22] J. Utke, L. Hasco¨et, C. Hill, P. Hovland, and U. Naumann. Toward adjoinable MPI.

In

Proceedings of IPDPS 2009, 2009.

10

