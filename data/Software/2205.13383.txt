2
2
0
2

y
a
M
6
2

]

V
C
.
s
c
[

1
v
3
8
3
3
1
.
5
0
2
2
:
v
i
X
r
a

BppAttack: Stealthy and Efﬁcient Trojan Attacks against Deep Neural Networks
via Image Quantization and Contrastive Adversarial Learning

Zhenting Wang, Juan Zhai, Shiqing Ma
Department of Computer Science, Rutgers University
{zhenting.wang, juan.zhai, sm2283}@rutgers.edu

Abstract

Deep neural networks are vulnerable to Trojan attacks.
Existing attacks use visible patterns (e.g., a patch or im-
age transformations) as triggers, which are vulnerable to
human inspection. In this paper, we propose stealthy and
efﬁcient Trojan attacks, BPPATTACK. Based on existing bi-
ology literature on human visual systems, we propose to
use image quantization and dithering as the Trojan trig-
ger, making imperceptible changes.
It is a stealthy and
efﬁcient attack without training auxiliary models. Due to
the small changes made to images, it is hard to inject such
triggers during training. To alleviate this problem, we pro-
pose a contrastive learning based approach that leverages
adversarial attacks to generate negative sample pairs so
that the learned trigger is precise and accurate. The pro-
posed method achieves high attack success rates on four
benchmark datasets, including MNIST, CIFAR-10, GTSRB,
and CelebA. It also effectively bypasses existing Trojan de-
fenses and human inspection. Our code can be found in
https://github.com/RU- System- Software-
and-Security/BppAttack.

1. Introduction

Deep Neural Networks (DNNs) have achieved superior
performance in many computer vision tasks [8, 22, 53]. Re-
cent studies show that DNNs are vulnerable to adversarial
attacks such as adversarial examples [18, 46], membership
inference attacks [55, 59], model stealing [51, 63], etc. In
this paper, we focus on Trojan attacks [10, 14, 19, 36, 39,
42, 54]. The adversary injects a secret Trojan behavior dur-
ing training, which can be activated at runtime by stamping
a Trojan trigger to the image. Such triggers can be image
patches [19], watermarks [42], image ﬁlters [1,41] and even
learned image transformation models [10, 14, 38].

Trojan attacks [19] are severe threats to the trustworthi-
ness of DNN models. Liu et al. [42] demonstrates the pos-
sibility of attacking face recognition, speech recognition,

and autonomous driving systems. Such attacks are gener-
ally feasible in most training scenarios, including federated
learning, unsupervised learning, and so on [4, 30, 69]. With
the deployment of DNN based computer vision models, it
is a critical challenge in our community.
Existing Work: Most existing Trojan attacks leverage in-
put patterns as triggers. For example, BadNets [19] uses a
yellow pad as its trigger. Recent works [41] try to lever-
age image ﬁlters as triggers, which are input dependent and
dynamic, making them hard to detect. To further improve
the quality of Trojan triggers, Doan et al. [14] train an auxil-
iary image transformation model and use the transformation
function as its trigger. Other works have adopted similar
ideas [10, 38].

One problem of existing attacks is that they are vulnera-
ble to human inspections. Once a set of attack inputs are
found, it is not difﬁcult to identify the trigger or train a
model to simulate the trigger. There are also online de-
tection methods to identify such attack samples, such as
STRIP [17]. Even for trained transformations as triggers,
it is hard for them to guarantee that the generated images
have imperceptible changes. This is because it is hard to
formulate human visual systems as a mathematical func-
tion, which makes it hard to optimize. Due to the relatively
large changes in inputs and limitations of existing poisoning
methods, it is also possible for reverse engineering based
defense methods [7,41,67] to recover part of the trigger and
identify if a model has a Trojan. Moreover, recent works on
generating high-quality triggers typically leverage trained
auxiliary models, which is time-consuming and inefﬁcient.
Our Work:
In this paper, we propose one new attack,
BPPATTACK. Based on existing literature on human visual
systems, we identify that humans inspectors are insensitive
to small changes of color depth. In this attack, we try to
exploit vulnerabilities in human visual systems. Thus, we
propose to reduce the bit-per-pixel (BPP) to conduct an im-
perceptible attack, which can also bypass existing defenses
mainly because of the small changes made to the input do-
main. We achieve our goal by performing a deterministic
yet input-dependent image transformation, i.e., image quan-

 
 
 
 
 
 
Fig. 1. Comparison of examples generated by different Trojan attacks (i.e., BadNets [19], blending-based attack [9], SIG [2], ﬁlter-based
attack [1, 10], ISSBA [38] and WaNet [49]). For each attack, we show the Trojan sample (top) and the magniﬁed (×5) residual (bottom).

tization and dithering. Due to the small-sized transforma-
tion as triggers, it is more challenging to train the model and
inject the trigger. To overcome this issue, we also propose a
contrastive learning and adversarial training method based
approach to training on the poisoned dataset. By doing so,
we do not require training auxiliary models, making the at-
tack fast and also input dependent. Moreover, our attack
exploits vulnerabilities in human visual systems, making it
human imperceptible when the attack settings are properly
set. Fig. 1 shows the comparison of our attack and existing
attacks using attack samples and residuals.

Our contributions are summered as follows:

• We propose a new attack that exploits human visual
systems. We design an effective and efﬁcient attack
that leverages image quantization and dithering. It per-
forms deterministic input-dependent image transfor-
mations, making it fast and dynamic. We also propose
a contrastive learning and adversarial training based
approach to enhance the poisoning process to inject
such human imperceptible triggers.

• We evaluate our prototype on four different datasets
and seven different network architectures. Results
show that our attack achieves a 99.92% attack suc-
cess rate on average. Our human study also conﬁrms
that it is 1.60 times better than the SOTA approaches
when facing human inspections. Results on existing
defenses also conﬁrm that our attack can effectively
bypass various types of SOTA defenses.

2. Background

2.1. Trojan Attacks

Trojan models behave normally for benign inputs but
have malicious behaviors (i.e., outputting a particular label)
on inputs stamped with the Trojan trigger. One limitation of
existing Trojan attacks is that most of them are perceptible

to human inspectors. Many Trojan attacks [9, 19, 42] use
predeﬁned patches or watermarks as Trojan triggers. Re-
fool [43] exploits physical reﬂection as Trojan trigger. Tro-
jan attacks can also happen in the feature space. For exam-
ple, Liu et al. [41] demonstrates attackers can use Instagram
ﬁlters as triggers to perform Trojan attacks. DFST [10]
utilizes CycleGAN [75] to inject Trojans in deep features
space. All these triggers are obvious for human inspection.
Recently, WaNet [49] proposed attacks using the image
warping technique as triggers. Although it is more stealthy
than previous works, the warping effects it leverages are still
perceptible. Another problem of existing attacks is that they
typically use ﬁxed patterns as trigger patterns, which means
different samples share the same trigger pattern. This prop-
erty makes such Trojan attacks detectable by existing de-
fenses [21, 41, 44]. Nguyen et al. [50] proposes input de-
pendent triggers. This attack brings large pixel-level pertur-
bations, sacriﬁcing stealthiness. Recently, Li et al. [38] and
Doan et al. [14] proposed new attacks that are not only im-
perceptible but also input dependent. The idea is to gener-
ate triggers by trained auto-encoders. While such methods
achieve stealthiness, they are model-dependent and time-
consuming.

2.2. Existing Defense

There has been a series of ways to defend Trojans. One
of them is training time defense, which aims at removing
Trojans before/during training. Chen et al. [6] and Tran et
al. [62] detect the malicious samples before training. Wang
et al. [68] removes Trojans in training by formalizing the
trigger in input space. Similarly, poison suppression [15,25]
depresses the malicious effectiveness of poisons in training.
These approaches target poisoning-based Trojan attacks but
ignore supply chain Trojan attacks. The second method is
reverse engineering. Neural Cleanse [67], DeepInspect [7],
K-arm [58] and ABS [41] use reconstructed triggers to per-
form detection. These methods work on local patched trig-

CleanWaNetOursInputResidualFilterISSBASIGBlendBadNetsgers but fail to generalize to different trigger types, e.g.,
input-aware triggers [50]. Another approach is to remove
Trojans in infected models [37, 40, 61, 74], such as ﬁnetun-
ing or pruning, which in cases, can lower benign accuracy as
well. Some other existing works try to leverage online de-
tection as a defense. STRIP [17] detect Trojan samples by
analyzing the sensitivity of samples on strong perturbation.
Februus [13] and SentiNet [11] leverage GradCAM [57] to
detect if model predictions are localized and leverage it as
a hint for triggers. Such methods fail when triggers are not
localized, e.g., ﬁlter triggers.

3. Method

In this section, we introduce BPPATTACK, a Trojan at-
tack that is invisible to human inspection, input-dependent
yet requires no auxiliary model training. We ﬁrst describe
the threat model (§3.1), and then present the foundation and
details of the attack process (§3.2 and §3.3, respectively).

3.1. Threat Model

Adversarial scope and goal. The adversary aims to pro-
duce a Trojan model. Eq. 1 shows the formal deﬁnition.
Mθ is Trojan model, T is a Trojan transformation function
and η is the target label function. Input-targeted labels can
be: (1) all-to-one: the attacker select a constant label c as
output label (i.e., η(y) = c). (2) all-to-all: the target label
is the next label of the true label (i.e., η(y) = y + 1).

Mθ(x) = y, Mθ(T (x)) = η(y)

(1)

the human imperceptible requirement. We want to
have an image perturbation function R(x) = T (x)−x
that satisﬁes

(cid:40)

Mθ(x + R(x)) = η(y)
Mθ(x + R(x(cid:48))) = y

where x(cid:48) (cid:54)= x (2)

• No auxiliary training: Many existing works try to re-
alize input-dependent attacks by utilizing an auxiliary
model, e.g., DFST [10] uses CycleGAN. Such attacks
are unstable because their effects depend on the train-
ing of the auxiliary models. Moreover, it has high com-
putation overhead. In contrast, we try to design an ef-
ﬁcient Trojan attack without auxiliary models.

Adversary capabilities. Following existing attacks [14,
49], we assume the adversary has full control of datasets,
training process, and model implementation. The adversary
injects the Trojan by poisoning the dataset.

3.2. Human Imperceptible Theory

Our idea of generating human imperceptible triggers is
from the biology study that human visual systems are in-
sensitive to color bit depth change. Nadenau et al. [47] and
many existing literatures [29, 31, 48, 73] supported this ob-
servation. Image color quantization [3,5,24,66] is a process
that reduces the number of distinct colors used in an image
with the intention to produce human imperceptible changes.
To remove the unnaturalness introduced by color bit change,
dithering [16, 27, 64] can improve its quality.

Compared with previous Trojan attacks, we aim to pro-

vide the following attack properties:

3.3. BPPATTACK

• Effective: We want the model to have a high attack
success rate (ASR) while maintaining high benign ac-
curacy at the same time. This effective goal is the basic
requirement of Trojan attacks as deﬁned in Eq. 1.

• Imperceptible: Many Trojan triggers are vulnerable
to human inspection, which is not robust. We want
to have a human imperceptible Trojan trigger. Tradi-
tionally, this is done by deﬁning a distance function V
to measure the visual similarity of two samples. As
such, the goal is to ﬁnd a trigger that is smaller than
a threshold, V(T (x), x) < t, where t is the threshold.
Existing works use Lp distance or SSIM scores, which
do not align with the human visual systems [52]. In
this paper, we tackle this problem by starting from ex-
isting studies on the human visual system and propose
an attack that is human imperceptible.

• Input-dependent: Fixed trigger patterns are easier to
detect [17,67] and in most cases, human visible. Thus,
input-dependent triggers are natural inheriting from

To achieve the aforementioned objectives, we design a
novel image color quantization based Trojan attack. Spec-
trally, we leverage image color quantization and dithering
to generate high-quality attack triggers and poisoning sam-
ples and then propose a contrastive learning and adversarial
training-based method to inject the Trojan.
Image quantization. The ﬁrst step of BPPATTACK is
to perform image quantization, which contains two steps.
First, we squeeze the original color palette (m bits for each
pixel on each channel) of the image into a smaller color
palette (d bits) by reducing the color depth. For each pixel,
we use the nearest pixel value in the squeezed d-bits space
to replace the original value. The squeezing function T is
deﬁned in Eq. 3, where round represents the integer round-
ing function:

round

T (x) =

2m−1 ∗ (cid:0)2d − 1(cid:1)(cid:17)
(cid:16) x
2d − 1

∗ (2m − 1)

(3)

This is the main algorithm to generate Trojan triggers and
has a few beneﬁts. First, it is a simple and deterministic

function with good stability and generalizability, and we
do not need to train any auxiliary models such as auto-
encoders and U-Nets. Second, as pointed out by existing
work [47,71], large color depths are not necessary for repre-
senting images, which means the squeezed image can have
high visual similarity to the original image. While being hu-
man imperceptible, such digital value changes can be cap-
tured by ML models and used as a trigger.
Dithering.
Image quantization potentially can cause un-
natural regions, especially when the bit reduction is high.
To increase the stealthiness of BPPATTACK, we utilize im-
age dithering techniques to remove the noticeable artifacts
by leveraging the existing colors of the artifacts.
Image
dithering techniques are designed to create the illusion of
color depth when color palette of image is limited. Specif-
ically, we use Floyd–Steinberg dithering [16] and nearest-
value color quantization combined with dithering as Trojan
transformation. Details are presented in Algorithm 1. Func-
tion quantize implements Eq. 3. Floyd–Steinberg dithering
achieves its goal by error diffusion, and line 4 calculates the
error. After that, it adds residual quantization errors of a
pixel onto its neighbors and spreads the debt out based on a
predeﬁned distribution. Lines 5 to 9 implement this idea.

for x from right to left do

Image I, Diffusion Distribution [a1, a2, a3, a4]

Algorithm 1 Quantization with Floyd-Steinberg Dithering
Input:
Output: Quantized Image
1: function PROCESS(I)
2:
3:
4:
5:
6:
7:

error = quantize(I[x][y]) − I[x][y]
I[x][y] = I[x][y] + error
I[x + 1][y] = I[x][y] + error ∗ a1
I[x + 1][y + 1] = I[x][y] + error ∗ a2
I[x][y + 1] = I[x][y] + error ∗ a3
I[x − 1][y + 1] = I[x][y] + error ∗ a4

for y from top to bottom do

8:
9:

Contrastive Adversarial Training. As shown in Fig. 1,
image quantization based attack triggers is very close to
original images. On the one hand, this makes it hard to
detect. On the other hand, it makes training more difﬁ-
cult, mainly because of the small perturbations. Existing
poisoning techniques tend to use the original cross-entropy
(CE) loss to train the Trojan model on benign and poisoning
samples. Due to the tiny perturbation introduced by image
quantization, it is hard to converge when using the CE loss.
Moreover, existing training procedure leads to inaccurate
and imprecise triggers. As a result, reverse engineering can
identify if a model has a Trojan by ﬁnding part of the trigger.
As a consequence, they are not robust attacks. To overcome
this challenge, we leverage contrastive supervised learning
and adversarial training.

The whole training framework follows the contrastive

learning framework, and we leverage the same loss func-
tion as described in existing work [32]. The key difference
of our attack from existing contrastive learning is that in
addition to existing negative sample generation methods,
we also leverage adversarial example generation methods.
Speciﬁcally, we use the PGD attack to generate adversar-
ial examples which ﬂip the label of input from its original
one to the target label to simulate the effects of our attack.
Then, we leverage them in training as negative examples.
Intuitively, this means we exclude such perturbations fea-
tures as important features for the model to learn so that it
can focus on the injected trigger that is image quantization
and dithering described before. Note that the PGD attack is
an optimization-based method and does not require training
auxiliary models.

4. Experiments and Results

In this section, we evaluate BPPATTACK from different
perspectives. We ﬁrst present the experiment setup, includ-
ing datasets and other settings in §4.1. In §4.2, we show
the effectiveness. Then, we investigate the stealthiness of
BPPATTACK by performing a human Inspection test (§4.3).
Furthermore, we evaluate BPPATTACK’s resistance to ex-
isting defenses in §4.4. We also conduct an ablation study
of BPPATTACK in §4.5. In all experiments, the default bit
depth is d = 5.

4.1. Experiment Setup

Datasets. We evaluate BPPATTACK on four datasets:
MNIST, CIFAR-10, GTSRB and CelebA. These datasets
are regularly used in backdoor-related researches [13, 17,
19, 40–42, 49, 67]. Details of these datasets are in Table 1.
MNIST [35] is used for hand-written digits recognition.
GTSRB [60] is built for classifying different trafﬁc signs.
CIFAR-10 [34] is a classiﬁcation benchmark. CelebA [45]
is a large-scale face attributes classiﬁcation dataset. Note
that CelebA has 40 independent binary attributes, where
most attributes are unbalanced. To make it suitable for
multi-class classiﬁcation, following WaNet [49], we use
the top three most balanced attributes (i.e., Heavy Makeup,
Mouth Slightly Open, and Smiling) and concatenate them
to build 8 classiﬁcation classes.
Evaluation Metrics. Following existing works [14, 19, 38,
49], we use benign accuracy (BA) and attack success rate
(ASR) [65] to evaluate the effectiveness of different Trojan

Dataset

Input Size

#Train

#Test Classes

MNIST
CIFAR-10
GTSRB
CelebA

28*28*1
32*32*3
32*32*3
64*64*3

60000
50000
39209
162770

10000
10000
12630
19962

10
10
43
8

Table 1. Overview of datasets.

Dataset

MNIST
CIFAR-10
GTSRB
CelebA

Non-attack

WaNet

BppAttack

BA

99.67%
94.88%
99.31%
79.14%

BA

ASR

BA

ASR

99.52% 99.86%
94.15% 99.55%
98.97% 98.78%
78.99% 99.33%

99.36% 99.79%
94.54% 99.91%
99.25% 99.96%
79.06% 99.99%

Dataset

MNIST
CIFAR-10
GTSRB
CelebA

Non-attack

WaNet

BppAttack

BA

99.67%
94.88%
99.52%
79.14%

BA

ASR

BA

ASR

99.44% 95.90%
94.43% 93.36%
99.39% 98.31%
78.73% 78.58%

99.25% 98.46%
94.73% 94.32%
99.46% 99.29%
78.84% 78.72%

Table 2. Effectiveness on all-to-one attacks.

Table 3. Effectiveness on all-to-all attacks.

Network

MobileNetV2
SENet18
ResNeXt29
DenseNet121

Non-attack

BppAttack

Images

Patched Blended

SIG ReFool WaNet BppAttack

BA

94.21%
94.79%
94.83%
95.35%

BA

ASR

93.79% 99.99%
94.49% 99.98%
94.68% 99.97%
95.20% 100.00%

Trojan
Clean
Both

4.2%
5.9%
5.0%

2.3%
7.2%
4.7%

1.7% 5.2% 42.0%
2.8% 14.5% 21.8%
2.2% 9.8% 30.9%

50.7%
48.1%
49.4%

Table 5. Success fooling rates of each Trojan attacks.

Table 4. Effectiveness on different networks.

attacks. In detail, BA evaluates the accuracy of a model for
clean samples by measuring the number of correctly clas-
siﬁed clean samples over the number of all clean samples.
ASR is the success rate of Trojan attacks. It is deﬁned as the
number of Trojan samples that successfully perform Trojan
attacks over the total number of Trojan samples.
Models. We evaluated BPPATTACK on seven popular
models. These models are commonly used in Trojan-
related studies [1, 14, 41–43, 49]. First, we follow the set-
tings of WaNet [49] and use a 5-Layer CNN (details can
be found in § 7.3 in Supp.)
for MNIST. For CIFAR10
and GTSRB, we use Pre-activation ResNet18 [23]. For
CelebA, we use ResNet18. We also evaluates the effective-
ness of BPPATTACK on more representative models (i.e.,
MobileNetV2 [56], SENet18 [26], ResNeXt29 [70] and
DenseNet121 [28]).
Baseline. We select the state-of-the-art backdoor attack
method WaNet [49] as baseline methods and compare the
effectiveness and stealthiness with it. The stealthiness of
WaNet is much better than previous Trojan attacks [2, 9,
19, 42, 43], while its attack success rate is still high. For
WaNet, We use the default hyperparameters in the original
paper to conduct the attack. We also compare BPPATTACK
with auxiliary model based method [38] in § 7.5 (Supp.).

4.2. Effectiveness

To measure the effectiveness of BPPATTACK, we collect
BA and ASR of BPPATTACK, benign models, and state-of-
the-art baseline WaNet [49] under different datasets. For
attack settings, both all-to-one and all-to-all attacks are in-
cluded. We also evaluate BPPATTACK’s generalizability to
different models. The results for all-to-one attack and all-to-
all attack are shown in Table 2 and Table 3, respectively. For
the all-to-one attack setting, BPPATTACK achieves higher
BA and ASR than WaNet, indicating it has better perfor-
mance. In all-to-all attack settings, similarly, BPPATTACK

still performs better than WaNet. For example, the ASR of
BPPATTACK is higher than that of WaNet by 0.96%, while
the BA of BPPATTACK is also higher. These results indicate
BPPATTACK is a more effective attack method.

Besides the default models used in Table 2 and Ta-
ble 3 (i.e., a 5-Layer CNN for MNIST, Pre-activation
ResNet18 [23] for CIFAR-10 and GTSRB, ResNet-18 for
CelebA). We also conduct experiments on more mod-
els to further evaluate the generalizability of BPPATTACK
on different network architectures (MobileNetV2 [56],
SENet18 [26], ResNeXt29 [70] and DenseNet121 [28]).
The results are shown in Table 4.
In detail, we use the
other four networks on CIFAR-10 and collect the ASR and
BA of our method. We also record the BA of benign mod-
els. The attack setting is an all-to-one attack. In all cases,
BPPATTACK achieves similar BA with nearly 100% ASR,
demonstrating BPPATTACK’s generalizability on different
network architectures.

4.3. Stealthiness

To examine the stealthiness of different Trojan attacks,
we conduct a similar human inspection study as performed
in previous works [14, 49]. We use the same settings as
WaNet. First, 25 images are randomly selected from GT-
SRB [60] dataset. Then, their corresponding Trojan im-
ages for different Trojan attack methods are created. For
each attack method, we can get a set of 50 images by mix-
ing the Trojan samples and original samples. Finally, 40
humans classify whether each image is a Trojan sample.
Before the classifying process, the participants are trained
about the attacks’ characteristics and mechanisms. The re-
sults are demonstrated in Table 5. As shown in the results,
BPPATTACK achieves about 50% success fooling rate for
both Trojan inputs and clean inputs, showing it has satis-
fying stealthiness. WaNet [49] has higher success fooling
rates than prior works. However, as shown in Fig. 1, it still
leaves some subtle artifacts, which can be found by human

(a) MNIST

(b) CIFAR-10

(c) GTSRB

(d) CelebA

Fig. 2. Resilient to STRIP [17].

Fig. 3. Resilient to GradCAM [57].

Fig. 4. Resilient to Neural Cleanse [67].

insepctions. More examples for comparing BPPATTACK
and WaNet can be found in § 7.1 in Supp.

4.4. Resistance to Existing Defenses

To examine BPPATTACK’s robustness against existing
Trojan defenses, we implement representative Trojan de-
fense methods (i.e., STRIP [17], GradCAM [57], Neu-
ral Cleanse [67] and Fine-pruning [40]) and evaluate the
resistance of BPPATTACK against them. We also show
BPPATTACK’s robustness against Spectral Signature [62],
Universal Litmus Patterns [33], and Neural Attention Dis-
tillation [37] in § 7.4 in Supplementary Materials.
STRIP [17]. We ﬁrst evaluate if BPPATTACK can bypass
a representative runtime Trojan attack detection method
STRIP [17]. For a given input sample, STRIP examines
if it is a Trojan sample by intentionally perturbing it via su-
perimposing various image patterns and observing the con-
sistency of predicted classes for perturbed inputs. If the en-
tropy is low (i.e., the predictions on perturbed inputs are
consistent), then STRIP regard it as a Trojan sample. Fig. 2
demonstrates the experiment results on STRIP. The results
show that the entropy range of clean models and Trojan
models generated by our method are similar, indicating our
attack is resistant to runtime defense STRIP. The reason
why BPPATTACK can bypass STRIP is that the superimpos-
ing operation of STRIP will modify the color distribution
and break the color-shifting Trojan patterns.
GradCAM [57]. We then evaluate the robustness of

BPPATTACK against GradCAM based defense methods [11,
13]. These defense mechanisms exploit GradCAM to ana-
lyze the decision process of the models. In detail, given a
model and an input sample, GradCAM can give a heatmap,
where the heat value of each pixel indicates this pixel’s im-
portance for the ﬁnal prediction of the model. GradCAM
is useful for detecting small-sized Trojans [19, 42]. This
is because such Trojans will produce high heat values on
small-sized trigger regions, which induces abnormal Grad-
CAM heatmap. However, our Trojan transformation func-
tion modiﬁes the entire image, making GradCAM fail to
detect it. Fig. 3 shows the visualization heatmaps of a clean
model and a Trojan model generated by our method.
It
shows that the heatmaps of these two models are similar,
indicating BPPATTACK is resistant to GradCAM based de-
fense methods.
Neural Cleanse [67]. We then evaluate BPPATTACK’s re-
sistance to a representative reverse engineering based de-
fense, Neural Cleanse (NC). It ﬁrst reconstructs a trigger
pattern for each class label via an optimization process.
Then, it examines if there exists a class that has signiﬁ-
cantly smaller reverse-engineered trigger and considers it
as a sign of Trojan models. In detail, it uses Anomaly Index
(i.e., Median Absolute Deviation [20]) to quantify the devi-
ation of reverse-engineered triggers based on their sizes and
consider the models whose Anomaly Index is larger than
two as Trojan models. Although it is effective for detect-
ing patched-based Trojans [19,42], it assumes that different
samples share the same trigger pattern in pixel level. Our

CleanBppAttack(a) MNIST

(b) CIFAR-10

(c) GTSRB

(d) CelebA

Fig. 5. Resilient to Fine-Pruning [40].

method can bypass NC by breaking this assumption with
Input-dependent triggers, i.e., the pixel level Trojan pertur-
bations for different samples are different. Experiment re-
sults shown in Fig. 4 demonstrate Neural Cleanse fails to
detect the Trojan model generated by our method.
Fine-pruning [40]. We then investigate BPPATTACK’s re-
sistance to representative Trojan removing method, Fine-
pruning. This defense is based on the assumption that Tro-
jan behaviors are related to a few dormant neurons in the
model, and the Trojan can be removed via pruning such dor-
mant neurons. Given a set of clean samples, it records the
activation values on a layer and considers the neuron that
has the smallest activation value as the most dormant neu-
rons. Then, it gradually prunes neurons based on the order
of their activation values. The results can be found in Fig. 5.
It shows that Fine-pruning is not able to remove the Trojan
injected by our methods. For example, in MNIST, CIFAR-
10, and GTSRB, the ASR is always close or higher than BA.
For CelebA, although the ASR is slightly lower but it still
achieves above 50%, meaning the Trojan is not completely
removed.

4.5. Ablation Study

To investigate the effects of hyperparameters and differ-
ent components, we ﬁrst evaluate the effects of the bits num-
ber d. Then, we study the inﬂuence of different injection
rates. We also investigate the effects of dithering and con-
trastive adversarial training.
Bits Number. As mentioned in §3.3, to generate Trojan
samples, we quantize the original color palette (m bits for a
pixel on each channel) into a smaller color palette (d bits),
and use the nearest pixel value in the squeezed value space
to replace the original one. Here, the bits number of the
squeezed color palette d is called bits number. To investi-
gate the effects of different bits number d, we collect the BA
and ASR under different bits numbers. The used dataset is
CIFAR-10, and the attack setting is an all-to-one attack. We
also show the generated Trojan sample to study bits num-
ber’s inﬂuence on the stealthiness of the attack. Fig. 6a
shows the BA and ASR under different bits number d. The
results demonstrate that our method can achieve high BA

and high ASR when d is not larger than 6. However, when
d reaches 7, the ASR decreases. Note that the original im-
ages’ bits number for each pixel on each channel is 8. The
larger d is, the fewer perturbations the attack induces. When
d = 7, the difference between the Trojan sample and the be-
nign sample is so small that it is hard for the model to tell.
Fig. 7 demonstrates the generated Trojan samples under dif-
ferent bits number d. For different d values, the Trojan sam-
ple is natural and indistinguishable from the clean sample.
More examples generated under different bits number d can
be found in § 7.2 in Supp.

Injection Rate. During training, the model is optimized
on benign samples and Trojan samples alternatively. We
denote the fraction that the model is optimized on Trojan
samples as injection rate α. To investigate its inﬂuence on
BPPATTACK’s performance, we record the BA and ASR
with different injection rates. The used dataset is CIFAR-
10, and the attack setting is an all-to-one attack. The results
are shown in Fig. 6b. The ASRs are low when α is small.
This is because a small injection rate indicates the effects of
optimizing on Trojan sample and target labels is limited so
that the model fails to learn the Trojan behaviors. With the
increase of the α, the ASR becomes higher. BA is not in-
ﬂuenced by injection rate, when injection rate is in a range
from 2.5% to 30%.

Dithering. As we mentioned in §3.3, when d is small,
the new images can be less stealthy. To make the Trojan
samples more natural, we use dithering techniques to re-
move these unnatural artifacts. Here we study the effects of
dithering by illustrating the Trojan samples generated with
dithering and without it. Fig. 8 demonstrates examples to
show the effects of dithering, using the GTSRB dataset as
an example. The dithering technique helps generate more
natural Trojan samples by ﬁxing the color banding. Over-
all, dithering can remove the color banding artifacts in the
directly quantized image to make the attack more stealthy.

Contrastive Adversarial Training. In this section, we con-
duct an ablation study to investigate the effects of Con-
trastive Adversarial Training. We use the vanilla and
our training methods to train two models on CIFAR-10,
and compare them by using a trigger reverse engineering

(a) Performance with Different d

(b) Performance with Different α

Fig. 6. Evaluation results with different hyperparameters.

Fig. 7. Effects of different bits number.

Fig. 8. Effects of Dithering.

method, Neural Cleanse [67]. Fig. 9 shows the result. As
we can see, the model trained with the vanilla method has
an anomaly index that is higher than the threshold (i.e., 2).
By contrast, the model trained with our method successfully
bypasses the detection.

5. Discussion

Mitigations. BPPATTACK can bypass existing defenses,
but it is not perfect. We believe that a defense that fo-
cuses on color depth checking can potentially detect our
attacks. Other possible defenses, e.g., activation distribu-
tion checking and anomaly detection based methods can
also help mitigate such attacks. Also, it is possible to de-
fend our attack under different threat models. For example,
data cleaning and validation or enforcing another training
protocol can mitigate general data poisoning based attacks.

Fig. 9. Effects of Contrastive Adversarial Training. The model
trained by vanilla training method can be detected by Neu-
ral Cleanse, while the model trained by Contrastive Adversarial
Training can bypass the detection.

Recent works have proposed DP-SGD and other methods
to defend such attacks [15, 25] during training time. Such
methods can potentially help mitigate BPPATTACK.
Ethical statements. In this paper, we propose a stealthy
and efﬁcient Trojan attack, demonstrating a threat. On the
one hand, it has potential negative societal impacts. The
adversaries can exploit real-world AI systems, such as facial
recognition applications. On the other hand, we disclose
new vulnerabilities and alert the defenders to pay attention
to such new types of Trojan attacks.

6. Conclusion

In this paper, we propose an image quantization and
dithering based Trojan attack. By exploiting the human vi-
sual system, our method can generate human impercepti-
ble triggers with the support of literature from biology. To
improve the effectiveness of our attack, we also propose a
contrastive learning and adversarial training based poison-
ing method. Results show that our attack is highly effective
and efﬁcient.

Acknowledgement

We thank the anonymous reviewers for their construc-
tive comments. This work is supported by IARPA TrojAI
W911NF-19-S-0012. Any opinions, ﬁndings, and conclu-
sions expressed in this paper are those of the authors only
and do not necessarily reﬂect the views of any funding agen-
cies.

References

[1] Trojai. https://pages.nist.gov/trojai/docs/

about.html/. 1, 2, 5

[2] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new
backdoor attack in cnns by training set corruption without

Input6 bits5 bits4 bitsw/o Ditheringw/ DitheringInputTrojanResiduallabel poisoning. In 2019 IEEE International Conference on
Image Processing (ICIP), pages 101–105. IEEE, 2019. 2, 5
[3] Dan S Bloomberg. Color quantization using octrees. Lep-

tonica, ss, pages 1–10, 2008. 3

[4] Nicholas Carlini and Andreas Terzis.

backdooring
arXiv:2106.09667, 2021. 1

contrastive

learning.

Poisoning and
preprint
arXiv

[5] M Emre Celebi. Improving the performance of k-means for
color quantization. Image and Vision Computing, 29(4):260–
271, 2011. 3

[6] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko
Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and
Biplav Srivastava. Detecting backdoor attacks on deep neu-
ral networks by activation clustering. SafeAI@AAAI, 2019.
2

[7] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushan-
far. Deepinspect: A black-box trojan detection and mitiga-
tion framework for deep neural networks. In IJCAI, pages
4658–4664, 2019. 1, 2

[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence, 40(4):834–848, 2017. 1

[9] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn
Song. Targeted backdoor attacks on deep learning systems
arXiv preprint arXiv:1712.05526,
using data poisoning.
2017. 2, 5

[10] Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang.
Deep feature space trojan attack of neural networks by con-
trolled detoxiﬁcation. AAAI, 2021. 1, 2, 3, 13

[11] Edward Chou, Florian Tram`er, Giancarlo Pellegrino, and
Dan Boneh. Sentinet: Detecting physical attacks against
deep learning systems. 2018. 3, 6

[12] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552, 2017. 13

[13] Bao Gia Doan, Ehsan Abbasnejad, and Damith C Ranas-
inghe. Februus: Input puriﬁcation defense against trojan at-
tacks on deep neural network systems. In Annual Computer
Security Applications Conference, pages 897–912, 2020. 3,
4, 6

[14] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira:
Learnable, imperceptible and robust backdoor attacks.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 11966–11976, 2021. 1, 2, 3, 4, 5, 13
[15] Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly de-
tection and backdoor attack detection via differential pri-
vacy. International Conference on Learning Representations
(ICLR), 2020. 2, 8

[16] Robert W. Floyd and Louis Steinberg. An Adaptive Algo-
rithm for Spatial Greyscale. Proceedings of the Society for
Information Display, 17(2):75–77, 1976. 3, 4

[17] Yansong Gao, Change Xu, Derui Wang, Shiping Chen,
Damith C Ranasinghe, and Surya Nepal. Strip: A defence
In Pro-
against trojan attacks on deep neural networks.
ceedings of the 35th Annual Computer Security Applications
Conference, pages 113–125, 2019. 1, 3, 4, 6

[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Inter-
Explaining and harnessing adversarial examples.
national Conference on Learning Representations (ICLR),
2015. 1

[19] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Bad-
Identifying vulnerabilities in the machine learning
nets:
model supply chain. arXiv preprint arXiv:1708.06733, 2017.
1, 2, 4, 5, 6

[20] Frank R Hampel. The inﬂuence curve and its role in robust
estimation. Journal of the american statistical association,
69(346):383–393, 1974. 6

[21] Jonathan Hayase, Weihao Kong, Raghav Somani, and Se-
woong Oh. Spectre: Defending against backdoor attacks us-
ing robust statistics. International Conference on Machine
Learning, 2021. 2

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 1

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In European
conference on computer vision, pages 630–645. Springer,
2016. 5

[24] Paul Heckbert. Color image quantization for frame buffer
display. ACM Siggraph Computer Graphics, 16(3):297–307,
1982. 3

[25] Sanghyun Hong, Varun Chandrasekaran, Yi˘gitcan Kaya, Tu-
dor Dumitras¸, and Nicolas Papernot. On the effectiveness
of mitigating data poisoning attacks with gradient shaping.
arXiv preprint arXiv:2002.11497, 2020. 2, 8

[26] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7132–7141, 2018. 5

[27] Xiangyu Y Hu.

Simple gradient-based error-diffusion
method. Journal of Electronic Imaging, 25(4):043029, 2016.
3

[28] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017. 5

[29] Gerald H Jacobs, Jay Neitz, and Jess F Deegan. Retinal re-
ceptors in rodents maximally sensitive to ultraviolet light.
Nature, 353(6345):655–656, 1991. 3

[30] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. Baden-
coder: Backdoor attacks to pre-trained encoders in self-
supervised learning. 2022 IEEE Symposium on Security and
Privacy (SP). IEEE, 2022. 1

[31] Deane B Judd. Color in business, science and industry. 1952.

3

[32] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
arXiv
Dilip Krishnan. Supervised contrastive learning.
preprint arXiv:2004.11362, 2020. 4

[33] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and
Heiko Hoffmann. Universal litmus patterns: Revealing back-
door attacks in cnns. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
301–310, 2020. 6, 12

[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple

layers of features from tiny images. 2009. 4

[35] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
4

[36] Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin
Zi Hao Zhao. Deep learning backdoors. arXiv preprint
arXiv:2007.08273, 2020. 1

[37] Yige Li, Nodens Koren, Lingjuan Lyu, Xixiang Lyu, Bo Li,
and Xingjun Ma. Neural attention distillation: Erasing back-
door triggers from deep neural networks. International Con-
ference on Learning Representations (ICLR), 2021. 3, 6, 12
[38] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran
He, and Siwei Lyu. Invisible backdoor attack with sample-
speciﬁc triggers. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 16463–16472,
2021. 1, 2, 4, 5, 13

[39] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Com-
posite backdoor attack for deep neural network by mixing
existing benign features. In Proceedings of the 2020 ACM
SIGSAC Conference on Computer and Communications Se-
curity, pages 113–131, 2020. 1

[40] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-
pruning: Defending against backdooring attacks on deep
neural networks. In International Symposium on Research in
Attacks, Intrusions, and Defenses, pages 273–294. Springer,
2018. 3, 4, 6, 7

[41] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma,
Yousra Aafer, and Xiangyu Zhang. Abs: Scanning neu-
ral networks for back-doors by artiﬁcial brain stimulation.
In Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, pages 1265–1282,
2019. 1, 2, 4, 5

[42] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,
Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning
attack on neural networks. NDSS, 2018. 1, 2, 4, 5, 6
[43] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂec-
tion backdoor: A natural backdoor attack on deep neural net-
works. In European Conference on Computer Vision, pages
182–199. Springer, 2020. 2, 5

[44] Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang,
Shiqing Ma, and Xiangyu Zhang. Ex-ray: Distinguishing
injected backdoor from natural features in neural networks
by examining differential feature symmetry. arXiv preprint
arXiv:2103.08820, 2021. 2

[45] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), De-
cember 2015. 4

[46] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and
Pascal Frossard. Deepfool: a simple and accurate method to
fool deep neural networks. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
2574–2582, 2016. 1

[47] Marcus J Nadenau, Stefan Winkler, David Alleysson, and
Murat Kunt. Human vision models for perceptually opti-

mized image processing–a review. Proceedings of the IEEE,
32, 2000. 3, 4

[48] Jay Neitz and Gerald H Jacobs. Polymorphism of the long-
wavelength cone in normal human colour vision. Nature,
323(6089):623–625, 1986. 3

[49] Anh Nguyen and Anh Tran. Wanet–imperceptible warping-
based backdoor attack. arXiv preprint arXiv:2102.10369,
2021. 2, 3, 4, 5, 11, 13

[50] Tuan Anh Nguyen and Anh Tran.

Input-aware dynamic
backdoor attack. Advances in Neural Information Processing
Systems, 33:3454–3464, 2020. 2, 3

[51] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box models.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4954–4963, 2019. 1

[52] Jean-Franc¸ois Pambrun and Rita Noumeir. Limitations of the
ssim quality metric in the context of diagnostic imaging. In
2015 IEEE International Conference on Image Processing
(ICIP), pages 2960–2963. IEEE, 2015. 3

[53] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems, 28:91–99, 2015. 1

[54] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and
Yang Zhang. Dynamic backdoor attacks against machine
IEEE European Symposium on Security
learning models.
and Privacy (EuroS&P), 2022. 1

[55] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. Ml-leaks: Model
and data independent membership inference attacks and de-
fenses on machine learning models. NDSS, 2019. 1

[56] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
In Proceedings of the
residuals and linear bottlenecks.
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 5

[57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE in-
ternational conference on computer vision, pages 618–626,
2017. 3, 6

[58] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An,
Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang.
Backdoor scanning for deep neural networks through k-
arm optimization. In International Conference on Machine
Learning, pages 9525–9536. PMLR, 2021. 2

[59] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
Shmatikov. Membership inference attacks against machine
learning models. In 2017 IEEE Symposium on Security and
Privacy (SP), pages 3–18. IEEE, 2017. 1

[60] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
Christian Igel. Man vs. computer: Benchmarking machine
learning algorithms for trafﬁc sign recognition. Neural net-
works, 32:323–332, 2012. 4, 5

[61] Guanhong Tao, Yingqi Liu, Guangyu Shen, Qiuling Xu,
Shengwei An, Zhuo Zhang, and Xiangyu Zhang. Model or-
thogonalization: Class distance hardening in neural networks

for better security. In 2022 IEEE Symposium on Security and
Privacy (SP). IEEE, 2022. 3

[62] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral sig-
natures in backdoor attacks. Advances in Neural Information
Processing Systems, 2018. 2, 6, 11

[63] Jean-Baptiste Truong, Pratyush Maini, Robert J Walls, and
Nicolas Papernot. Data-free model extraction. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 4771–4780, 2021. 1

[64] Robert A Ulichney. Void-and-cluster method for dither array
generation. In Human Vision, Visual Processing, and Dig-
ital Display IV, volume 1913, pages 332–343. International
Society for Optics and Photonics, 1993. 3

[65] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan,
Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri,
Brendan Dolan-Gavitt, and Siddharth Garg. Nnoculation:
broad spectrum and targeted treatment of backdoored dnns.
arXiv preprint arXiv:2002.08313, 2020. 4

[66] Oleg Verevka. The local k-means algorithm for colour image

quantization. 1995. 3

[67] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bi-
mal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural
cleanse: Identifying and mitigating backdoor attacks in neu-
ral networks. In 2019 IEEE Symposium on Security and Pri-
vacy (SP), pages 707–723. IEEE, 2019. 1, 2, 3, 4, 6, 8
[68] Zhenting Wang, Hailun Ding, Juan Zhai, and Shiqing Ma.
Towards understanding and defending input space trojans.
arXiv preprint arXiv:2202.06382, 2022. 2

[69] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Dis-
tributed backdoor attacks against federated learning. In In-
ternational Conference on Learning Representations (ICLR),
2019. 1

[70] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1492–1500,
2017. 5

[71] Weilin Xu, David Evans, and Yanjun Qi. Feature squeez-
ing: Detecting adversarial examples in deep neural networks.
NDSS, 2018. 4

[72] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao.
In Pro-
Latent backdoor attacks on deep neural networks.
ceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security, pages 2041–2055, 2019. 13
[73] Semir Zeki. The representation of colours in the cerebral

cortex. Nature, 284(5755):412–418, 1980. 3

[74] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ra-
mamurthy, and Xue Lin. Bridging mode connectivity in loss
landscapes and adversarial robustness. International Confer-
ence on Learning Representations (ICLR), 2020. 3

[75] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
international conference on computer vision, pages 2223–
2232, 2017. 2

7. Supplementary materials

7.1. Additional Images for Different Attacks

In this section, we show more Trojan samples generated
by WaNet [49] and our BPPATTACK. The images can be
found in Fig. 10, where the ﬁrst row is the original images,
and the second and the third row are the Trojan samples
generated by WaNet and BPPATTACK, respectively. As can
be observed, the Trojan samples generated by WaNet can
be spotted, and BPPATTACK is more stealthy.

7.2. Additional Images for Different Bits Numbers

To illustrate the effects of different bit numbers, in this
section, we demonstrate more samples generated by differ-
ent bits numbers. The results are shown in Fig. 11. It shows
that the Trojan samples produced by BPPATTACK with dif-
ferent bits numbers are natural and stealthy.

7.3. Details of MNIST Classiﬁer

The detailed architecture of the classiﬁer used for

MNIST dataset is shown in Table 6.

Layer Type

# of Channels

Filter Size

Stride

Padding Activation

Conv*
Conv*
Conv
FC†
FC

32
64
64
512
10

3x3
3x3
3x3
-
-

2
2
2
-
-

1
0
0
0
0

ReLU
ReLU
ReLU
ReLU
Softmax

Table 6. Details of classiﬁer used for MNIST. FC stands for fully-
connected layer. * denotes the layer is followed by a BatchNor-
malization layer. †denotes the layer is followed by a DropOut
layer.

7.4. Resistance to More Defenses

Spectral Signature [62]. Spectral Signature [62] is a de-
fense method that identiﬁes and removes Trojans during
training. Although it is a training time defense and does
not match our threat model, investigating if the Trojan sam-
ples generated by BPPATTACK can be detected by it is still
helpful. Given a set of benign and Trojan samples, Spec-
tral Signature ﬁrst collects the latent features and computes
the top singular value of the covariance matrix. Then, for
each sample, it calculates the correlation score between its
features and the top singular value that is used as the out-
lier scores. Finally, it removes the samples with high outlier
scores. We use 900 benign samples and 100 Trojan samples
in CIFAR-10 to evaluate if our attack can bypass Spectral
Signature. The results are demonstrated in Fig. 12. It shows
that we can fool the detector and bypass the detection.

Fig. 10. Additional images for comparison between WaNet and our method.

Fig. 11. Additional images to demonstrate the inﬂuence of different bits numbers.

patterns from a large number of benign and Trojan models.
These patterns are optimized input images. We train the pat-
terns from 500 clean VGG models and 500 poisoned VGG
models provided in its ofﬁcial GitHub repository. Then, we
attack ﬁve different VGG models on CIFAR-10, and they all
can bypass ULP. ULP assumes the trigger is a small patch,
while our trigger is not a patch.

Neural Attention Distillation [37]. NAD [37] is a Tro-
jan removing method. It ﬁrst obtains a teacher model by
ﬁne-tuning on a set of clean samples. Then, NAD uses the
obtained teacher model to guide the distillation of the Tro-
jan student model to make the intermediate-layer attention
of the student model align with that of the teacher model.
To evaluate if our method is resilient to NAD, we conduct
experiments on three datasets (i.e., CIFAR-10, GTSRB, and

Fig. 12. Resilient to Spectral Signature.

Universal Litmus Patterns [33]. ULP [33] is designed to
detect if a model is Trojan or not. It ﬁrst trains universal

8777InputWaNetOursInput5 bits6 bits4 bitsCelebA). For CIFAR10 and GTSRB, we use Pre-activation
ResNet18. For CelebA, we use ResNet18. For the imple-
mentation of NAD, we use the ofﬁcial code and default
hyperparameters speciﬁed in the original paper. In detail,
we assume the defender can access 5% of clean training
data. The initial learning rate is 0.1, and the learning rate
is divided by ten after every two epochs. The data aug-
mentations used are random crop, horizontal ﬂipping, and
Cutout [12]. The results are demonstrated in Table 7. For
CIFAR-10 and GTSRB, although the ASRs for defended
models are low, however, the BAs decrease dramatically
after NAD defense. For CelebA, the defended model still
achieves 47.89% ASR with the BA drop from 79.06% to
67.52%. The results show that our attack is resilient to
NAD.

Dataset

No defense

NAD

BA

ASR

BA

ASR

CIFAR-10
GTSRB
CelebA

94.54% 99.91%
99.25% 99.96%
79.06% 99.99%

39.14% 12.07%
14.21% 2.15%
67.52% 47.89%

Table 7. Resilient to Neural Attention Distillation

7.5. Compared with ISSBA [38]

ISSBA [38] is a representative auxiliary models based
attacks. It ﬁrst trains an auto-encoder as a Trojan transfor-
mation function and then uses it to inject Trojans into vic-
tim models. Following ISSBA [38], we run our method on
a 200 classes subset of ImageNet (speciﬁed in Li et al. [38])
and ResNet18 model, and compare our method to it. The re-
sults are shown in Table 8, where ET means the extra time
cost for training the victim model. Our attack is more ef-
ﬁcient with comparable or better ASR and BA, compared
with ISSBA. The computational and time overhead of our
method is much smaller than that of generator/auto-encoder
based attacks [10, 14, 38]. In detail, the training time of our
method is only 19.04% longer than that of standard train-
ing. For ImageNet’s 200 classes subset, ISSBA [38] takes
7h30mins to train the encoder-decoder. However, the extra
training time for our method is only 1h18mins on the same
dataset. For stealthiness, it is clear that the example of our
attack is more close to the original image, while the exam-
ple of ISSBA has some unnatural “black fog”. (See Fig.1 in
main paper.)

Dataset

Non-attack

ISSBA

BppAttack

BA

BA

ASR

ET

BA

ASR

ET

ImageNet

85.83%

85.51% 99.54% 450m

85.76% 99.78% 78m

Table 8. Effectiveness on ImageNet

7.6. Compared with WaNet [49]

Our method and WaNet [49] have different training pro-
tocols. Besides the comparison under different training pro-
tocols, we also compare BPPATTACK and WaNet under
our protocol to further investigate the effectiveness of our
proposed quantization triggers. We compare our method
and WaNet under our training protocol on CIFAR-10 and
GTSRB. The model used is Pre-activation ResNet18 and
ResNet18, respectively. The results are demonstrated in Ta-
ble 9. Results show that both BA and ASR of our trigger
are higher than that of WaNet, showing that the purposed
quantization trigger is better than WaNet’s trigger.

Dataset

WaNet

BppAttack

BA

ASR

BA

ASR

CIFAR-10
GTSRB

94.06% 99.35%
98.45% 98.52%

94.54% 99.91%
99.25% 99.96%

Table 9. Comparisons to WaNet using our training protocol

7.7. Robustness against ﬁne-tuning

Besides the threat model that assumes the victim users
directly deploy the malicious models, here we also consider
a transfer learning scenario where the downstream users
ﬁne-tune the Trojan model weights with out-of-distribution
data. In some cases, the downstream users even ﬁne-tune
the model with different quality of images, and some may
incorporate similar quantization techniques to the proposed
attack, e.g., JPEG. Note that injecting Trojans that are ro-
bust against ﬁne-tuning is orthogonal to our paper and has
been studied by another line of work [72]. Such approaches
can be adopted by us. By combining with Yao et al. [72],
our attack on CIFAR-10 and ResNet18 can achieve 86.52%
ASR after ﬁne-tuning on 5000 JPEG compressed samples.

7.8. Discussion: Trojan Triggers

Traditional Trojan attacks use ﬁxed patterns/noise as
Trojan triggers. Let (cid:101)x be the Trojan sample and x be the
corresponding clean sample. These attacks can be formal-
ized as (cid:101)x = m (cid:12) t + (1 − m) (cid:12) x (where m and t are
predeﬁned Trojan trigger mask and pattern) or (cid:101)x = x + δ
(where δ is the ﬁxed noise). However, the Trojan triggers
are not necessarily a ﬁxed pattern. Instead, it can be a uni-
versal input activity (e.g., quantization, auto-encoder, GAN,
or other input transformations), and it can be formalized as
(cid:101)x = T (x). The traditional trigger that requires a ﬁxed pat-
tern is actually a special case of the activity function T (x).

