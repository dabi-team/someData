2022

1

Adversarial Robustness of
Deep Neural Networks: A Survey
from a Formal Veriﬁcation Perspective

Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Zhe Hou,
Yan Xiao, Yun Lin, Jin Song Dong

2
2
0
2

t
c
O
1
1

]

R
C
.
s
c
[

2
v
7
2
2
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Neural networks have been widely applied in security applications such as spam and phishing detection, intrusion
prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications.
Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for
trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the
reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in security and machine
learning. In this work, we survey existing literature in adversarial robustness veriﬁcation for neural networks and collect 39 diversiﬁed
research works across machine learning, security, and software engineering domains. We systematically analyze their approaches,
including how robustness is formulated, what veriﬁcation techniques are used, and the strengths and limitations of each technique. We
provide a taxonomy from a formal veriﬁcation perspective for a comprehensive understanding of this topic. We classify the existing
techniques based on property speciﬁcation, problem reduction, and reasoning strategies. We also demonstrate representative
techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.

Index Terms—Robustness, veriﬁcation, security, adversarial machine learning, neural networks, deep learning.

(cid:70)

1 INTRODUCTION

D EEP learning is an artiﬁcial intelligence (AI) technique

that is regarded as one of the top technological break-
throughs in computer science [1]. It imitates the working
principle of the human brain in processing data and form-
ing knowledge patterns and produces promising results
for various tasks such as image classiﬁcation [2], speech
recognition [3], recommendation [4], and natural language
understanding [5]. Nowadays deep learning is increasingly
applied in many ﬁelds where trustworthiness is critically
needed, such as cybersecurity [6], autonomous driving [7],
and the healthcare industry [8]. Particularly in cybersecurity,
deep learning has been used to perform intrusion detec-
tion [9], malware detection [10], phishing detection [11],
network trafﬁc analysis [12], etc. Neural networks, or more
speciﬁcally deep neural networks (DNNs), act as the key
technology for deep learning. Most neural networks are
designed to be deployed in practice where an arbitrary
input shall be accepted. This, however, brings a great un-
certainty in determining the behavior of a neural network
model even if it has been well-trained [13]. As a result,
the trustworthiness of neural networks has turned out to
be a new challenging topic in the research community,

• M. H. Meng is with the National University of Singapore, and also with

the Institute for Infocomm Research, A*STAR, Singapore.
E-mail: menghs@i2r.a-star.edu.sg

S. G. Teo is with the Institute for Infocomm Research, A*STAR, Singapore.

• G. Bai is with The University of Queensland, Australia.
•
• Z. Hou is with the Grifﬁth University, Australia.
• Y. Xiao, Y. Lin and J. S. Dong are with the National University of

Singapore.

• G. Bai (Email: g.bai@uq.edu.au) and Y. Xiao (Email: dcsxan@nus.edu.sg)

are corresponding authors.

especially for those to be deployed in safety and security-
critical systems [14].

The genesis of neural network veriﬁcation and valida-
tion can be traced back to decades ago [15], in which the
veriﬁcation is referred to as correctness and the validation is
referred to as accuracy and efﬁciency. Adversarial examples
for neural network models were discovered by Szegedy
et al. [16] and later Goodfellow et al. [17] systematically
introduced an approach to intrude the robustness property
of a neural network to make it misclassify a tampered input
to a wrong result. Since then, neural networks are shown
vulnerable to adversarial attacks [18]; consequently, research
on the security of neural networks experienced explosive
growth. Many research projects study the attack and defense
of AI using adversarial training [19], [20], [21], [22]. As a
result, verifying the robustness of neural network models
against adversarial inputs becomes a strong demand in
machine learning research to earn trust from their users and
investors.

Thanks to a complete knowledge system of program
analysis that has been built since the last century, many tech-
niques designed for traditional program veriﬁcation, such as
constraint solving [23], reachability analysis [24], and abstract
interpretation [25], are adopted in reasoning neural network
models. On the other hand, researchers also attempt to
reduce the veriﬁcation to an optimization problem and
achieve uplifting progress by taking advantage of various
mathematics theories [26], [27], [28]. The progress made
in robustness veriﬁcation techniques not only increases in-
dustry practitioners’ conﬁdence in the security of neural
network models but also stimulates the discussion of more
diverse property speciﬁcations in deep learning, such as

 
 
 
 
 
 
2

2022

fairness [29], monotonicity [30], and coverage [31].

An early survey under this theme [32] performs a
comparative study on various veriﬁcation methods, which
collects seven different approaches from the literature and
discusses the advantages as well as the shortcomings of each
approach. A recent survey [33] extensively studies 18 exist-
ing robustness veriﬁcation approaches and reproduces them
on a uniﬁed mathematical framework, therefore for the ﬁrst
time comprehensively sketching the benchmarking of vari-
ous veriﬁcation techniques. However, some latest progress
since 2019 has not been covered. Another survey [34] pro-
poses a taxonomy of robustness veriﬁcation in accordance
with existing robust training techniques and conducts a
uniﬁed evaluation toolbox for over 20 approaches. It takes
both deterministic and probabilistic veriﬁcation into account
and unveils the landscape of available techniques for ro-
bustness training and robustness veriﬁcation. Compared
with existing literature, this paper covers the state-of-the-art
robustness veriﬁcation techniques that have been published
in recent years. Moreover, we also propose a more compre-
hensive taxonomy to categorize the existing approaches and
analyze them from a formal veriﬁcation perspective, thereby
forming our insight into future research on this topic.

Contributions

In summary, our paper makes the following contributions:
• This survey discusses adversarial robustness veriﬁcation
for deep neural networks from a perspective of formal
veriﬁcation including three key components: a property
formalization, a reduction framework, and available rea-
soning strategies. From this point of view, we show the
veriﬁcation methodology not only differs in reasoning
strategies but also varies with the author’s understand-
ing of neural network veriﬁcation as a problem.

• We propose a taxonomy based on different components
under the formal veriﬁcation framework and accord-
ingly perform a classiﬁcation of the existing literature.
We establish the connection between different types of
approaches from the technical perspective and thereby
unveil the landscape as well as the latest progress. We
particularly analyze the strategy behind the implementa-
tion of diversiﬁed approaches and discuss their advan-
tages and shortcomings.

• Considering many existing approaches are proposed for
speciﬁc use cases or experimental settings, it is hard
to directly analyze them in a comparative context. This
survey ﬁlls the gap by assessing them on a more general
neural network model with a case study supported by
substantial visualization.

• We suggest the future directions of deep neural network
veriﬁcation, including, improvement in completeness of
veriﬁcation, adoption of various types of complex mod-
els, leveraging optimization algorithms, and a generic
veriﬁcation framework.

Scope of This Survey

Considering the research on deep neural network property
veriﬁcation begins with the topic of adversarial robustness,
which is indeed the most discussed property so far, we
choose robustness veriﬁcation as the theme to carry out our

survey. In order to avoid information overload and present
this survey in an appropriate space, we only focus on
reviewing and analyzing research works that put robustness
veriﬁcation as the major contribution.

We collect the existing literature published in machine
learning, security, and software engineering domains in
recent years. We also resort to two sources to replenish the
literature. First, we take reference from the existing survey
papers on neural network veriﬁcation [32], [33], [34], and
thereby we have collected some representative approaches.
so that the representative approaches are not missed. Sec-
ond, we take them as the seeds and treat the counterparts
that have been cited and/or benchmarked in their papers as
the second source.

We also notice that most related studies simplify the
context of a deep neural network as a multilayered perceptron
(MLP) for classiﬁcation tasks that are equipped with three
typical activations (ReLU, sigmoid, and tanh), therefore we
only discuss robustness veriﬁcation of MLPs with three
activations applicable in this survey. To avoid ambiguity,
the term “neural networks” is referred to as deep neural
networks with the assumption that it is in a multi-layer
architecture rather than a single neuron perceptron. We also
assume the robustness veriﬁcation is performed in a white-
box setting, which means the architecture and parameters of
the examined model are transparent to the veriﬁer.

As a result, we collect 39 research works that have
proposed an algorithm or a tool to verify the robustness
property, and we show them later in Table 1. For some
research works that do not discuss robustness veriﬁcation
based on a predeﬁned speciﬁcation, but from the perspec-
tive of robust training or ﬁnding adversarial examples, we
only take their methodologies that apply to speciﬁcation-
based veriﬁcation into account.

2 BACKGROUND

This section introduces formal veriﬁcation and one of its
typical implementations called model checking, including its
key elements and processes. After that, we propose our
deﬁnition of property veriﬁcation by ﬁtting the setting of
neural networks into the framework of formal veriﬁcation.

2.1 Formal Veriﬁcation

Formal veriﬁcation is deﬁned as a method to prove or
disprove the correctness of a speciﬁc software or hardware
system through formal methods of mathematics. As tradi-
tional testing is commonly used for ﬁnding bugs through
an actual execution or simulation, formal veriﬁcation em-
phasizes proving the correctness in a modeling framework
referring to the examined system with regard to a certain
speciﬁcation or property – and therefore formal veriﬁca-
tion requests an in-depth understanding of the system to
be examined, and usually comes with a higher cost than
traditional testing [35]. Model checking is a typical formal
veriﬁcation technique to systematically verify if a formally
deﬁned property holds in a ﬁnite state model [36]. It is
designed to explore all possible system states exhaustively
and, in the end, provides a qualitative result for specifying
whether the target speciﬁcation holds in the system. In Fig. 1

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

3

property veriﬁcation of deep neural networks through the
lens of formal veriﬁcation. Here we treat a deep neural
network model as a function that maps an input to an
output, thereby we formalize the property speciﬁcation as
a logic formula involving both input and output. Given the
property speciﬁcation and available modeling framework
on hand, we can perform the veriﬁcation using diverse
reasoning strategies.

Let us take the adversarial robustness as an example.
Suppose we have a neural network 𝑓
: 𝑋 → 𝑌 that takes
an input 𝑥 and then outputs 𝑦 as the result of classiﬁcation.
With a limit of the maximum input perturbation Δ𝑥 given,
we assert the neural network is not robust throughout
the veriﬁcation if we observe an arbitrary (𝑥, 𝑦) from the
training set such that 𝑓 (𝑥 + Δ𝑥) ≠ 𝑦; otherwise the robustness
is proved to be held in the given neural network.

Through the veriﬁcation above, we can know if a prop-
erty holds in the target model from the perspective of the
worst case. However, we gain no knowledge of the percent-
age of compromised inputs (that are successfully manipu-
lated by the attacker) within the entire input space, not even
to mention the analysis of possible approaches to improve.
For those reasons, quantitative reasoning is required to per-
form a more complex and in-depth property veriﬁcation for
neural network models. Counting the desirable outputs and
offering a probabilistic result is a typical step of quantitative
veriﬁcation that makes it differ from qualitative veriﬁcation.
For those properties with demographic characteristics, such
as fairness, quantitative reasoning could achieve better se-
mantic expressiveness than any qualitative approach. Since
we focus on robustness veriﬁcation, we assume the property
speciﬁcation is qualitatively deﬁned in this survey.

3 PROPERTY FORMALIZATION
The properties of a neural network could be captured from
the semantic context of its speciﬁcation. Most of the prop-
erties are input-output (IO) properties that request a speciﬁc
mapping relation between the input and output. Robustness
is one of the earliest IO properties that has been studied,
which requests the model’s output to be stable upon minor
modiﬁcations have been made to the input’s value [38],
[39]. In this section, we present our deﬁnition of adversarial
perturbations and provide a formalization of the robustness
property.

3.1 An Introduction to Perturbation

The perturbation of input can be categorized into two
types, namely global perturbation and regional perturbation,
as illustrated in Fig. 2 (middle). As its name implies, the
global perturbation is a vector of the same size as input
samples, and the adversarial input is obtained by overlaying
the perturbation on a benign input. The source of global
perturbation can be from normal distortion in actual deploy-
ments such as signal noise or weather conditions [?], or the
output of attack methods, such as projected gradient descent
(PGD) [20] or fast gradient sign method (FGSM) [17]. Regional
perturbation is usually a ﬁxed pattern to be added to the
original input, which is also known as local perturbation
or “Trojan trigger” in an attack scenario. Due to the uncer-
tainty of the occurrence of the attack, veriﬁcation of security

Fig. 1. An overview of adversarial robustness veriﬁcation from a formal
veriﬁcation perspective

we present a classical schematic view of formal veriﬁcation
through model checking, which is composed of three key
components: a property formalization, a reduction framework,
and speciﬁcation reasoning.

A precise and unambiguous property speciﬁcation is
necessary for formal veriﬁcation. Property speciﬁcation is
a statement of property obtained through formalizing the
system requirements. It is often presented as a classical logic
formula to specify the relationship or restriction on different
system components or illustrated in a temporal logic formula
to enforce a rule over the system in terms of time. The reduc-
tion framework is a formal representation of the system to
be examined, which keeps all the key features and functions
to ensure a proper simulation of all possible states within
the system. To sum up, the property formalization deﬁnes
what the system is expected to do, and the system modeling
after reduction prescribes how the system behaves. Property
formalization and a proper reduction framework together
constitute the speciﬁcation phase of the veriﬁcation. These two
processes illustrate our understanding of system veriﬁcation
as a problem. Different understandings of a system may
result in different reasoning approaches available for us to
solve the problem.

With both system and property formalized during the
speciﬁcation phase, the veriﬁer performs the reasoning task
and outputs a result – we call this stage the veriﬁcation phase.
A positive output means that the property speciﬁcation is
satisﬁed in the examined model. However, a negative out-
put sometimes could not be straightforwardly interpreted
as a violation of that property speciﬁcation, and therefore
an extra analysis is required. Since the reasoning usually
produces a counterexample together with a negative output,
we need to determine if the counterexample generated
through the simulation faithfully reﬂects the situation of the
actual system – otherwise, it is concluded as a false negative
and another round of reasoning is needed after reﬁning the
reduction framework. Such a strategy to recursively reﬁne
the system speciﬁcation according to the counterexample
generated from the previous round is formally proposed
in [37] as the counterexample-guided abstraction reﬁnement
(CEGAR) approach.

2.2 Property Veriﬁcation of Deep Neural Networks

After introducing the basic concept of neural networks and
formal veriﬁcation theory, we can deﬁne the problem of

FormalizationReductionModelDeep Neural NetworkRequirementsProperty SpecificationVerificationResultReasoningAdversarial perturbationsRobust𝑓()≠𝑓()𝑓()𝑓()=Not robustTested networkOutputInput1Input24

2022

3.3 Robustness against Adversarial Perturbations

Robustness describes the fault-tolerant capability of a com-
puter system to deal with erroneous inputs during execu-
tion. In machine learning, robustness is usually described
as the capacity of a model to consistently produce desirable
output given some perturbations (or distortions) have been
made on the input. As robustness is closely related to the
reliability and performance of a neural network model, it
is widely discussed in diverse branches of deep learning
applications [39].

A mutant input with perturbation that can alter the
model’s output is called an adversarial example – therefore the
evaluation of the robustness of a deep neural network model
is to ﬁnd the existence of such an adversarial example. In
analysis, robustness only makes sense if the scope of pertur-
bation is clearly deﬁned and the benign input is properly
speciﬁed. The veriﬁcation may not be able to truthfully
reﬂect the real robustness of the given neural network model
if the scope of perturbation is deﬁned negligibly (i.e., too
small). A too strong perturbation makes the adversarial
example sufﬁciently differs from the original input, in which
case it is reasonable for the model to classify the input as
a different output. As for the benign input, it could be an
image used for classiﬁcation or a vector of values within
a continuous domain. We then formalize the robustness
property of neural network models as follows:

Deﬁnition 1 (Robustness against adversarial perturbations).
Given a neural network model that maps a benign input 𝑥𝑎 to
the output 𝑦𝑎, and moreover produces an output 𝑦𝑏 given an
adversarial input 𝑥𝑏. Let (cid:107)·(cid:107) 𝑝 be the function to calculate the 𝐿 𝑝-
norm distance. The robustness property Φ against any adversarial
perturbation within the scope of Δ𝑥 is deﬁned as follows:

Φ(𝑥𝑎, 𝑦𝑎, 𝑥𝑏, 𝑦𝑏, Δ𝑥) def= (cid:0)(cid:107)𝑥𝑎 − 𝑥𝑏 (cid:107) 𝑝 ≤ Δ𝑥 (cid:1) → (𝑦𝑎 = 𝑦𝑏)

Other than the above deﬁnition that applies to the classi-
ﬁcation of a discrete domain, the robustness property could
be further deﬁned with tolerant misclassiﬁcation or tolerant
misprediction [30], which is more practical in the case that the
output falls in a continuous domain (e.g., regression tasks)
or an aggregation of labels (e.g., top-K classiﬁcation). As this
paper focuses on the classiﬁcation scenario, we assume the
veriﬁcation does not tolerate errors in the output.

4 CORRECTNESS ASSESSMENT

From the view of program analysis, the objective of a static
checker is to ﬁnd if a program prevents something that it
claims to avoid. The same idea also applies to the veriﬁca-
tion of neural network models. Intuitively, a positive ver-
iﬁcation outcome implies the neural network model could
always prevent violations of speciﬁc property if it claims
to do so and, on the contrary, a negative outcome claims
the neural network model fails to preserve the robustness
property. However, inconsistency between the claim and
the fact sometimes happens when the veriﬁcation is not
guaranteed to be sound and complete. This section discusses
soundness and completeness for a more comprehensive and
precise correctness assessment [41].

Fig. 2. Samples of regional (a) and global (b) adversarial perturbations
and measurement of magnitude, illustrated with attacks to the prediction
of the MNIST dataset

against Trojan attacks will be difﬁcult without knowing the
pattern, location, and size of triggers. Moreover, proving ro-
bustness against a speciﬁc Trojan pattern does not generalize
the property against another Trojan pattern even when both
patterns are on a similar scale. For those reasons, regional
perturbation is mainly studied in attack scenarios. We focus
on global perturbation in the remainder of this paper.

3.2 Perturbation Measurement

There are different ways to measure the magnitude (scope)
of input perturbations. In mathematics, a norm is a function
to map a vector’s distance to the origin into a non-negative
value. Here we treat input perturbations as vectors, and we
adopt the norm function to deﬁne the scope of perturbation,
and correspondingly calculate its magnitude.

Although both global and regional perturbations are
created with the same objective, which is to maximize the
chance of misclassiﬁcation meanwhile ensuring they are
visually indistinguishable, the measuring methods for dif-
ferent types of perturbations may differ. 𝐿0 norm calculates
the number of input features that have been affected, i.e., the
size of perturbation, which is useful to describe the size of
a regional perturbation. 𝐿1 norm (Manhattan distance) and
𝐿2 norm (Euclidean distance) measure the distance between
the benign and adversarial inputs, which are often used
to describe the accumulated inﬂuence of the perturbation.
𝐿∞ norm of a perturbation records the greatest magnitude
among all elements of it as a vector. 𝐿∞ norm is widely used
in robustness veriﬁcation because it echoes the 𝜖 value of
the FGSM [17], which regulates the scope of gradient as the
worst-case perturbation that leads to misclassiﬁcation. Fig. 2
demonstrates a sample perturbation in each type given
a benign input sample from the MNIST dataset [40] and
illustrates the value of different norm functions in deﬁning
the magnitude of input perturbation.

We highlight that the adversarial examples are not the
only source of input perturbations, which may also include
benign corruption (e.g., the inﬂuence of weather) and rea-
sonable distortion (e.g., signal noise, blur) [39]. However,
those cases are specially introduced in object recognition
applications and do not differ from the adversarial example
in the context of formal analysis. We assume adversarial
examples as the default violation of robustness in this work.

Benign image (28×28)Label: 1 (23.20%)Adversarial image #1Prediction: 7 (15.80%)Adversarial image #2Prediction: 8 (23.20%)Perturbation #1 (Regional)Perturbation #2 (Global)norm: 28norm: 19.6norm: 3.704norm: 0.7norm: 784norm: 196norm: 7norm: 0.25Benign image (28×28)Label: 1 (23.20%)++(a)(b)MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

5

Fig. 3. The cases of veriﬁcation outcomes classiﬁed in the statistical
hypothesis testing approach, illustrated with the classiﬁcation of the
CIFAR-10 dataset

4.1 Soundness and Completeness of the Veriﬁcation

A proof method is sound if every statement it can prove
is indeed true. It is complete if every true statement can
be proved. Given a neural network veriﬁer with something
undesired X that we wish to prevent. The veriﬁer is sound
if it never accepts X taking place in the neural network with
certain inputs. It is complete if it never rejects any input,
provided X does not happen in the neural network model.
Statistical hypothesis testing provides another good way
to understand soundness and completeness, where the hy-
pothesis represents the actual existence of an undesirable
event X deﬁned in the speciﬁcation of a neural network
model, and the decision indicates the actual outcome of the
veriﬁer [42]. A table of error speciﬁed for the veriﬁcation
is shown in Fig.3 with positive and negative hypothesis
columns being replaced by desirables and violations. With
such a framework being set up, we may understand the re-
lationship between soundness and completeness as follows:
• A sound veriﬁcation prevents missed violations (false neg-
atives) but may tolerate false alarms (false positives) in
case the event X does not actually take place even if the
veriﬁer outputs a rejection. Simply put, the veriﬁcation
may report unsafe regarding the (possible) existence of
violation X, even though the system is indeed safe.

• A complete veriﬁcation prevents false alarms (false posi-
tives). However, it may not be able to account for missed
violations (false negatives) — which means there may
exist some violations not found by the veriﬁcation. The
veriﬁer may report the system is safe due to its failure to
ﬁnd the violation X.

• A sound and complete veriﬁcation implies a perfect pre-
diction, that only allows accepted desirables (true positives)
and caught violations (true negatives).

4.2 Trade-off in Practice

Verifying deep neural network models is shown as an NP-
hard problem in [23] and therefore it is challenging to imple-
ment veriﬁcation that is simultaneously sound, complete,
and able to terminate in a reasonable time.

In practice, one of those three requirements is usually
put aside to guarantee the other two characters. Some
approaches (e.g., [23], [43]) put aside termination to ensure
both soundness and completeness are preserved, which is
widely adopted in the early stage of neural network veriﬁca-
tion due to the size of neural networks is comparably small,
and the scale of veriﬁcation is limited. However, scalability
becomes a critical factor in the veriﬁcation of deeper and

Fig. 4. The sample neural network (ReLU activation) with weight and
bias parameters shown on edges and nodes respectively

more complicated neural networks. Considering that sound-
ness is more essential in program analysis (to prevent what
the system claims to prevent), recent veriﬁcation solutions
(e.g., [44], [45]) commonly choose to sacriﬁce completeness
to secure soundness and scalability.

Unlike the complete veriﬁcation that comes with an
exponential time complexity for the worst case, incom-
plete veriﬁers usually come with much better scalability
but risk the veriﬁcation from precision loss due to over-
approximation. Since the precision loss is accumulated layer
by layer, in the worst case, an incomplete veriﬁer may fail
to certify the robustness even though it terminates quickly.

4.3 Soundness of Floating-point Arithmetic

The soundness of ﬂoating-point arithmetic is another factor
to determine the correctness of the veriﬁcation. Floating-
point is a commonly used data type in computing that
offers ﬂexible demands in precision. It does not naturally
satisfy the axioms of real arithmetic, such as associativity
and distributivity [46]. For that reason, the ﬂoating-point
soundness is a concern in verifying neural network models
on a larger scale. Unsound ﬂoating-point arithmetic can
make the veriﬁcation suffer from false negatives. At the mo-
ment of writing this survey, only a few papers [44], [46], [47]
take ﬂoating-point arithmetic into consideration. Since we
focus on studying robustness veriﬁcation methodology from
a formal veriﬁcation perspective, ﬂoating-point arithmetic is
not included in our classiﬁcation criteria.

5 MODEL REDUCTION
This section discusses how to reduce the property veri-
ﬁcation of a neural network model to another problem
with ﬁnite states and an available solution. Such a reduc-
tion process is also known as modeling in model checking.
The veriﬁcation reduction reﬂects our understanding of the
veriﬁcation as a problem and meanwhile determines what
consequent approach or algorithm could be used to solve
it. As one of the three key processes deﬁned in Section 2.1,
reduction stands for a formal interpretation of the neural
network model to be examined.

We categorize the existing reduction methods into two
groups, namely optimization problem and reachability problem.
In case the veriﬁcation is treated as an optimization prob-
lem, we further classify the reduction of existing approaches
into SAT/LP encoding, MILP encoding, and QCQP/SDP en-
coding. For the second category, we also identify three sub-
classes from the existing works, namely layerwise propaga-
tion, abstract interpretation, and discretization. We present our

AcceptedAccepted Desirables(TRUE POSITIVE)Missed Violations(FALSE NEGATIVE)RejectedFalse Alarms(FALSE POSITIVE)Caught Violations(TRUENEGATIVE)Hypothesis (Truth)Decision (Test)Prediction: 8 (ship)CorrectPrediction: 0 (airplane)Wrongi.e., the model is robusti.e., the model is not robustDesirablesViolations(          +          )𝑓Hidden Layer #1(w. ReLU Activation)Output LayerInput Layer𝑥1𝑥2𝑥3𝑥4𝑥5𝑥6𝑥7𝑥8𝑥9𝑥10𝑥11𝑥12-10-1-1-11-1111111-0.5110-1Hidden Layer #2(w. ReLU Activation)6

2022

TABLE 1
A list of the surveyed works in neural network robustness veriﬁcation.

Classiﬁcation†

CONVEXADVERSARIAL, CONVDUAL* PyTorch
EXACTREACH*
MATLAB
SHERLOCK
DEEPVERIFY*, DUALITY*
ADVDEFENSE*, CERTIFY*
SDPVERIFY*
AI2

TensorFlow

TensorFlow

Not Speciﬁed (C++)

Summary

#

Authorship,
Year & Reference

1 Pulina & Tacchella, 2010 [48]
2 Scheibler et al., 2015 [49]

3 Bastani et al., 2016 [50]

4 Cheng et al., 2017 [51]

Name of the Tool
& Link (if any)

NEVER

–
ILP*
MAXRESILIENCE*
PLANET

5 Ehler, 2017 [43]
6 Huang et al., 2017 [52]
7 Katz et al., 2017 [23]
RELUPLEX
8 Lomuscio & Maganti, 2017 [53] NSVERIFY*
9 Wong & Kolter, 2018 [26]

DLV

10 Xiang et al., 2017 [54]
11 Dutta et al., 2018 [55]

12 Dvijotham et al., 2018 [28]

13 Raghunathan et al., 2018 [27]

14 Raghunathan et al., 2018 [56]
15 Gehr et al., 2018 [25]
16 Singh et al., 2018 [46]
17 Wang et al., 2018 [47]
18 Wang et al., 2018 [57]
19 Weng et al., 2018 [58]

20 Xiang et al., 2018 [59]
21 Zhang et al., 2018 [60]
22 Bunel et al., 2018 [61]
23 Katz et al., 2019 [62]
24 Tjeng et al., 2019 [63]
25 Singh et al., 2019 [44]
26 Singh et al., 2019 [64]
27 Singh et al., 2019 [65]

DEEPZ

RELUVAL

NEURIFY

FAST-LIN, FAST-LIP
MAXSENSITIVITY*
CROWN

BAB, BABSB & RELUBAB

MARABOU

MIPVERIFY

DEEPPOLY

REFINEZONO

KPOLY, REFINEPOLY
CEGAR-MARABOU*
REACHNN

28 Yisrael et al., 2019 [45]
29 Huang et al., 2019 [66]
30 Xiang et al., 2020 [67]
31 Bak et al., 2020 [68]
32 Tran et al., 2019 & 2020 [69], [70] NNV

IGNNV

NNENUM

VENUS

33 Henriksen & Lomuscio, 2020 [71] VERINET
34 Botoeva et al., 2020 [72]
35 Dathathri et al., 2020 [73]
36 Tjandraatmadja et al., 2020 [74]
37 Fazlyab et al., 2020 [75]
38 Batten et al., 2021 [76]
39 Kouvaros & Lomuscio, 2021 [77] VENUS2§

SDP-FO

DEEPSDP

FASTC2V, OPT2CV

LAYERSDP, FASTSDP

Supported
Framework

Shark (C++)

Not Speciﬁed

Caffe

ConvNetJS

Caffe

Keras (Theano)

Not Speciﬁed (C)

Not Speciﬁed

TensorFlow & MATLAB

PyTorch & TensorFlow

Not Speciﬁed (C, Python)

Not Speciﬁed (C)

TensorFlow

TensorFlow

MATLAB

TensorFlow

PyTorch

TensorFlow

Julia

PyTorch & TensorFlow

PyTorch & TensorFlow

PyTorch & TensorFlow

Not Speciﬁed

Not Speciﬁc (Python)

MATLAB

Not Speciﬁed (Python)

MATLAB

PyTorch

Keras (TensorFlow)

JAX

Not Speciﬁed (C++)

MATLAB

Not Speciﬁed

Not Speciﬁed

Supported
Activation
ReLU Sigmoid Tanh Speciﬁcation SAT MILP SDP LWP DI AI CR MO IA LA IR

Reasoning
Strategies

Reduction

Property

𝐿∞
IP
𝐿∞
𝐿

1
IP
1, 𝐿
IP

2

𝐿

IP
𝐿∞
IP
𝐿∞
𝐿∞
𝐿∞
𝐿∞
𝐿∞
𝐿∞
𝐿∞
2, 𝐿∞
1, 𝐿
2, 𝐿∞
1, 𝐿
𝐿∞
2, 𝐿∞
1, 𝐿
𝐿∞
IP
2, 𝐿∞
1, 𝐿
𝐿∞
𝐿∞
𝐿∞
1, 𝐿∞
𝐿
IP

𝐿
𝐿

𝐿

𝐿

𝐿

IP

IP

IP
, 𝐿∞
1
𝐿∞
𝐿∞
𝐿∞
𝐿∞
𝐿∞
𝐿∞

† Property speciﬁcations include ﬁxed input pattern (IP) or 𝐿 𝑝 norms of adversarial perturbations. Reduction approaches are classiﬁed into ﬁve
classes as shown in the table, including SAT/LP encoding (SAT), MILP encoding (MILP), QCQP/SDP encoding (SDP), layerwise propagation
(LWP), discretization (DI), and abstract interpretation (AI). Reasoning strategies are classiﬁed into ﬁve classes as shown in the table, including
constraint reasoning (CR), mathematical optimization (MO), interval arithmetic (IA), linear approximation (LA), and iterative reﬁnement (IR).
§ The link is provided in the literature but not accessible as of May 2022.
* The name of the tool has not been explicitly deﬁned in the literature but is taken from the source code released, or commonly addressed in
subsequent research. The name is usually cited from the function name or the paper title.

classiﬁcation result over the collected literature in columns
9-14 of Table 1 and detail those reduction methods in the
remainder of this section.

To facilitate the analysis and discussion, we generate a
sample neural network model. As shown in Fig.4, the sam-
ple network is a fully-connected four-layer MLP with ReLU
activation, which is a typical neural network architecture
used for classiﬁcation tasks. To make it clear and consistent,
we call the basic elements within the sample neural network
as nodes. We accordingly separate each neuron within hid-
den layers into two nodes, which perform an afﬁne function
and activation function respectively. We create this sample
network based on the fact that the speciﬁc input [0, 0] would
always lead to the class corresponding to 𝑥11. In addition,
we also claim factual robustness of the sample network when
dealing with 𝐿∞ perturbations that are less or equal to 1.0,
i.e., the value of 𝑥11 is always greater than 𝑥12 regardless of

any input perturbation within the range [−1, 1].

5.1 Boolean Satisﬁability Problem (SAT) / Linear Pro-
gramming (LP) Encoding

Since a neural network model is analyzed in a white-box
setting and all the parameters are constants, it is intuitive
to perform the analysis by transforming the entire neural
network into a set of constraints in the form of propositional
logic and linking certain constraints containing the input
and output in one Boolean formula. We aim to prove the
non-existence of any violation in the examined model, and
thereby we can reduce the robustness veriﬁcation to a con-
straint satisfaction problem, or more speciﬁcally a Boolean
satisﬁability problem (colloquially known as SAT problem).
Alternatively, we can also set the negation of constraints
over the output nodes as the objective function and keep
the remaining constraints unchanged, thus the veriﬁcation

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

7

process is transformed into solving an optimization prob-
lem. In the case of all activation are piecewise-linear functions
(e.g., ReLU), this reduction approach is also known as linear
programming (LP) encoding.

As we are checking whether a neural network is robust,
the goal of reasoning over this encoding is to prove the
existence of such a solution that satisﬁes all the constraints,
rather than ﬁnding the objective value. To this end, we sym-
bolize every node with a unique variable and then connect
those variables layer-by-layer according to the computa-
tional relation deﬁned in the network model. This reduction
approach (in SAT form) can be formalized as below.

Deﬁnition 2 (SAT/LP encoding). Given a neural network
𝑓 with 𝑛 nodes. To verify a property speciﬁcation of it,
model
𝑓 can be formally reduced to a model M in form of a constraint
satisfaction problem, such as:

M(SAT/LP) = (cid:104)𝑉, 𝐷, 𝐶(cid:105)
where 𝑉 = {𝑉1, . . . , 𝑉𝑛} is a set of variables corresponding to the
nodes within 𝑓 ; 𝐷 = {𝐷1, . . . , 𝐷 𝑛} is a set specifying the value
domain (if any) of variables in 𝑉; and 𝐶 = {𝐶1, . . . , 𝐶𝑚} is the set
of constraints, including the constraint reﬂecting the violation of
property speciﬁcation.

Fig.5 depicts the encoding of the sample network that
violates the robustness speciﬁcation, i.e., 𝑥11 ≤ 𝑥12. Because
ReLU is a piecewise-linear function that produces different
outputs from two input domains, we notice that each ReLU
activation function introduces a disjunctive logical (or) op-
eration, which bisects the veriﬁcation problem. As a result,
given 𝑛 ReLU nodes in a neural network model, its SAT/LP
encoding creates a disjunction of 2
conjunctive normal
form (CNF) formulas (16 in the sample model) in total.

𝑛

As we capture all nodes from the examined model and
truthfully encode both afﬁne and activation functions in
constraints, we consider this reduction approach preserves
both soundness and completeness for the next step of rea-
soning. Nevertheless, the nature of propositional logic is
only suitable to encode linear algebraic relations without ex-
ponential. It makes this reduction approach only applicable
to neural networks that use piecewise-linear activation, such
as ReLU or its variants; otherwise, the output of activation
must be relaxed by another linear or piecewise-linear func-
tion in advance.

5.2 Mixed-Integer Linear Programming (MILP) Encod-
ing

One signiﬁcant progress made on SAT/LP encoding is
achieved by adding an integer variable for each ReLU acti-
vation and thereby encodes the examined model to a mixed-
integer linear programming (MILP) problem. MILP is a variant
of LP that allows part of variables speciﬁed as integers.
Some recent papers [51], [53], [55], [63], [78] take advantage
of the integer setting of MILP, propose a subtle encoding
of the ReLU function into a set of four in-equations, and
thus eliminate the excessive disjunctive formulas in the
constraints.

We show how a canonical

integer setting could be
adopted in ReLU activation in Deﬁnition 3 and present
our formalization of MILP encoding in Deﬁnition 4. We

𝑉 = {𝑥1, 𝑥2, ..., 𝑥12 } , 𝐷 = {−1 ≤ 𝑥1 ≤ 1, −1 ≤ 𝑥2 ≤ 1}
𝐶 = {𝑥3 = 𝑥1 + 𝑥2, 𝑥4 = 𝑥1 − 𝑥2 − 1,
( ( 𝑥3 ≤ 0) ∧ ( 𝑥5 = 0)) ∨ ( ( 𝑥3 > 0) ∧ ( 𝑥5 = 𝑥3)) ,
( ( 𝑥4 ≤ 0) ∧ ( 𝑥6 = 0)) ∨ ( ( 𝑥4 > 0) ∧ ( 𝑥6 = 𝑥4)) ,
𝑥7 = −𝑥5 + 𝑥6 − 1, 𝑥8 = 𝑥5 − 𝑥6 − 1,
( ( 𝑥7 ≤ 0) ∧ ( 𝑥9 = 0)) ∨ ( ( 𝑥7 > 0) ∧ ( 𝑥9 = 𝑥7)) ,
( ( 𝑥8 ≤ 0) ∧ ( 𝑥10 = 0)) ∨ ( ( 𝑥8 > 0) ∧ ( 𝑥10 = 𝑥8)) ,

𝑥11 = 𝑥9 − 𝑥10 + 1, 𝑥12 = 𝑥10 − 0.5, 𝑥11 ≤ 𝑥12 }

Fig. 5. Boolean formula obtained through SAT encoding of the sample
network given the robustness speciﬁcation being violated

then demonstrate the corresponding encoding of the sample
network in Fig.6.

Deﬁnition 3 (Big-M encoding of ReLU). Consider given an
integer variable 𝑑 ∈ {0, 1} and another sufﬁciently great constant
𝑀 (which is guaranteed to be greater than the upper bound of
ReLU input), a ReLU function 𝑦 = 𝑚𝑎𝑥 (0, 𝑥) could be encoded
by a CNF of four logical formulas, as shown below:

𝑦 = 𝑚𝑎𝑥 (0, 𝑥) ⇔ ((𝑦 ≥ 𝑥) ∧ (𝑦 ≤ 𝑥 + 𝑀 · 𝑑)

∧ (𝑦 ≥ 0) ∧ (𝑦 ≤ 𝑀 · (1 − 𝑑)))

where the case 𝑑 = 0 represents the ReLU is in active mode (𝑥 > 0
and 𝑦 = 𝑥), and conversely, 𝑑 = 1 indicates the ReLU is inactive
(𝑥 < 0 and 𝑦 = 0).

Deﬁnition 4 (MILP encoding). Given a neural network model
𝑓 with 𝑛 nodes. To verify a property of it,
𝑓 can be formally
modeled as a MILP problem presented as follows:
M(MILP) = (cid:10)𝑉, 𝑉𝑖𝑛𝑡 , 𝐷, 𝐶, Obj(cid:11)
where 𝑉 = {𝑉1, . . . , 𝑉𝑛} is a set of variables corresponding to
the nodes within 𝑓 ; 𝑉𝑖𝑛𝑡 is the set of integers that correspondingly
deﬁned to constraint ReLU activation outputs; 𝐷 = {𝐷1, . . . , 𝐷 𝑛}
is a set specifying the value domain (if any) of variables in 𝑉;
𝐶 = {𝐶1, . . . , 𝐶𝑚} is the set of constraints; and Obj is the objective
(minimum) inequation looking for the counterexample in case of
violation (i.e., Obj ≤ 0) occurs.

It

is worth noting that some existing approaches
(e.g., [27], [73], [76], etc) initiate their analysis from an LP
or MILP perspective and later transform the neural network
encoding into a semideﬁnite programming (SDP) problem.
We separate this encoding approach as an independent
category and detail it in Section 5.3.

The MILP encoding is proposed as a successor of
SAT/LP encoding. It retains both soundness and complete-
ness at the reduction stage. Moreover, it addresses our
concern about the “too many branches” issue in SAT/LP
encoding, especially when handling larger or deeper neu-
ral networks, and eventually brings potential improvement
regarding the scalability in neural network veriﬁcation.

5.3 Quadratically Constrained Quadratic Program
(QCQP) / Semideﬁnite Programming (SDP) Encoding

A quadratically constrained quadratic program (QCQP) is
an optimization problem in which both the objective func-
tion and the constraints are quadratic functions. In fact,
both LP and MILP encoding can be treated as variants of
QCQP where the quadratic objective function and quadratic

8

2022

constraints are absent. Solving a QCQP is NP-hard, too.
However, the problem can be solved by semideﬁnite pro-
gramming (SDP) if it is convex. For that reason, existing
studies categorized in this type are also known as SDP-
based approaches [33], [56], [73]. They mainly transform
the ReLU activation to convex quadratic equations, thereby
the veriﬁcation can be reduced to a convex QCQP after
including the linear constraints (i.e., afﬁne functions in the
neural network). The quadratic encoding of ReLU can be
deﬁned as follows.

Deﬁnition 5 (QCQP encoding of ReLU). Given a ReLU
function 𝑦 = 𝑚𝑎𝑥 (0, 𝑥), we can encode it to a set of quadratic
constraints, as shown below:

𝑦 = 𝑚𝑎𝑥 (0, 𝑥) ⇔ ((𝑦 ≥ 𝑥) ∧ (𝑦 ≥ 0) ∧ (𝑦 (𝑦 − 𝑥) = 0))

Compared with LP and MILP encodings, the QCQP
encoding enables us to analyze quadratic relations of sym-
bolic (i.e.,
input and output of nodes) within a neural
network model. By applying the convex encoding for ReLU
activation, the QCQP can be solved with soundness and
completeness guarantee using off-the-shelf interior point
methods. The major concern with this type of encoding
comes from scalability. Interior point methods are shown
to be computationally expensive and could take up to an
𝑂 (𝑛6) complexity for a neural network model containing 𝑛
hidden units [73].

Existing papers that adopt QCQP encoding commonly
mitigate the scalability issue in two directions. They apply
various mathematical optimization to improve solving efﬁ-
ciency and/or take advantage of the linear approximation
to further relax the original QCQP. Particularly, recent work
exploits linear approximation to extend the veriﬁcation with
a broader range of activation functions. Through the linear
approximation (to be detailed in Section 6.4), both tanh and
sigmoid functions can be approximately formulated as a set
of quadratic equations.

5.4 Layerwise Propagation

The layerwise propagation faithfully and precisely simu-
lates the computation of a neural network model to solve
the veriﬁcation as a reachability problem. It is intuitive and
has been adopted in some early approaches [49], [54].

Given the property speciﬁcation, we ﬁrst identify the
domain of concerned input (i.e., all the possible adversarial
inputs). Then we simulate the calculation based on those in-
puts until reaching the output layer. In the end, we compare
the propagated outputs with the domain of expected output
to determine if the property preserves in the target model.
We visualize the layerwise propagation process of verifying
the sample model in Fig. 7-(a).

The layerwise propagation at the reduction stage guar-
antees both soundness and completeness as it in fact does
not explicitly alter the neural network as the body to be
veriﬁed. The overall completeness and soundness are fully
determined by the following reasoning strategies, which we
will discuss in Section 6. Layerwise propagation usually
works well on deep models, however, suffers while dealing
with thick ones (i.e., lots of neurons per layer). Thus, the
reasoning based on the layerwise propagation reduction

minimize 𝑥9 − 2 × 𝑥10 + 1.5

s.t. 𝑥1 + 𝑥2 ≤ 𝑥5 ≤ 𝑥1 + 𝑥2 + 𝑀 · 𝑑3,

𝑥5 ≤ 𝑀 · (1 − 𝑑3) ,
𝑥1 − 𝑥2 − 1 ≤ 𝑥6 ≤ 𝑥1 − 𝑥2 − 1 + 𝑀 · 𝑑4,
𝑥6 ≤ 𝑀 · (1 − 𝑑4) ,
𝑥5 + 𝑥6 − 1 ≤ 𝑥9 ≤ 𝑥5 + 𝑥6 − 1 + 𝑀 · 𝑑7,
𝑥9 ≤ 𝑀 · (1 − 𝑑7) ,
𝑥5 − 𝑥6 − 1 ≤ 𝑥10 ≤ 𝑥5 − 𝑥6 − 1 + 𝑀 · 𝑑8,
𝑥10 ≤ 𝑀 · (1 − 𝑑8) ,
− 1 ≤ 𝑥1 ≤ 1, −1 ≤ 𝑥2 ≤ 1,
𝑑3, 𝑑4, 𝑑7, 𝑑8 ∈ {0, 1} , 𝑥5, 𝑥6, 𝑥9, 𝑥10 ≥ 0,
𝑀 > 𝑥𝑖 ,

(1 ≤ 𝑖 ≤ 12)

Fig. 6. MILP encoding of the sample network after equations simplifying,
with the objective expected to be positive

without any optimization or trick would be computationally
expensive, therefore leaving concern of scalability.

5.5 Discretization

The concern of scalability of layerwise propagation exists
since the early stage of neural network veriﬁcation. One
possible approach to address such a concern is to mitigate
the computational burden by discretization. Discretization is
a concept in applied mathematics that describes the process
of converting a continuous function into several discrete
pieces, which aims to achieve a ﬁnite exploration within an
inﬁnite search space. The discretization has been adopted in
a veriﬁcation approach named DLV [52], which proposes to
discretize the perturbation from the inﬁnite input distribu-
tion. It propagates those discrete input vectors layer by layer
and determines whether the robustness is preserved or not
by observing the bounds of output. Fig. 7-(b) demonstrates
the veriﬁcation of the sample model through a discretiza-
tion.

Although the architecture of the examined model re-
mains unchanged, the domain of output completely de-
pends on the discretized input domain. Unless the scale of
input discretization has been set large enough to sufﬁciently
depicts all the key points in the output domain, such as the
convex hull of a hidden layer, there always exists a risk of
precision loss and consequently leads to missed violation
(false negative). Overall, discretization greatly simpliﬁes
the analysis, especially for a deep neural network that
takes comparably lower dimension inputs. However, as a
reduction process, it does not come with a guarantee of
completeness. The guarantee of soundness is challenging,
too, as it is mutually restricted by the granularity of dis-
cretization and the convexity of the computation at each
layer. However, many neural networks, particularly those
that use sigmoid or tanh activation, are not convex. For these
reasons, this reduction approach may suffer from the state-
space explosion issue and therefore does not scale to larger
neural network models [25], [47].

5.6 Abstract Interpretation

Abstract interpretation is a veriﬁcation approach that pro-
vides a formalism of approximation and abstraction in a
mathematical setting, which has been adopted in [25], [45],
[46] and their extensional works. An abstract interpretation

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

9

Fig. 7. Visualization of different reduction approaches in verifying robustness property of the sample model (abstraction interpretation with box
domain and reasoned by interval arithmetic in sub-ﬁgure (a); discretization in sub-ﬁgure (b) and forward reachability analysis in sub-ﬁgure (c), with
red color area indicating false positive produced)

consists of an abstract domain, a pair of abstraction and con-
cretization functions, and a sound abstract semantic function in
form of an iterative ﬁxed-point equation.

Abstract interpretation aims to simplify the veriﬁcation
problem at the reduction stage. Although the veriﬁcation is
still treated as a reachability problem, it is solved by reason-
ing an abstract system rather than the original neural net-
work model. The mainstream abstraction interpretation ap-
proaches symbolize each node of the neural network, apply
abstraction function over its computation, and ultimately
determine whether the property is preserved through the
output values after the concretization. Besides that, [45]
explores another direction of the abstract interpretation that
simpliﬁes the architecture of the examined model, rather
than approximating the output of neurons inside.

In [45], each hidden layer has been simpliﬁed to contain
at most four fully-connected neurons, literally one neuron
with the maximum positive weight and tends to bring
increment to the output written as 𝑎 [+,↑], one neuron with
the minimize negative weight and tends to bring increment
to the output 𝑎 [−,↑], one neuron with the maximum positive
weight and tends to bring decrement to the output 𝑎 [+,↓],
and lastly one neuron with the minimum negative weight
and tends to bring decrement to the output 𝑎 [−,↓]. Thus, both
upper and lower bounds of the output can be eventually ob-
tained by analyzing the abstract model. This approach gains
a scalability advantage in verifying neural networks that are
not very deep but built with a great number of hidden units
per layer. On the other hand, simplifying neural network
architecture through clustering the neurons’ output does not
work well on models that adopt convergence functions as
activation, such as sigmoid and tanh (because they always
produce output within a certain range).

At the time of writing, DEEPPOLY [44] and its exten-
sional approaches are the most comprehensive, precise,
and practical solutions to perform veriﬁcation by abstract

interpretation. In the scheme of DEEPPOLY, each node is
transformed into an abstract element. An abstract element
is a tuple constituted by two polyhedral constraints and two
associate constants. The polyhedral constraints of a node are
made up of variables corresponding to those nodes con-
nected from the previous layer, and the associate constants
deﬁne its approximated upper and lower bounds after the
concretization. Take the sample network as an instance, the
robustness veriﬁcation is reduced to determine if, in its
concretized abstraction, the upper bound of 𝑥12 is less than
the lower bound of 𝑥11, which means 𝑥11 > 𝑥12 is always
true. We take [44] as an example and deﬁne the abstract
interpretation reduction as follows.

Deﬁnition 6 (Abstract interpretation). Given a neural network
model 𝑓 with 𝑛 nodes. By deﬁning a polyhedral abstract domain
and approximating all the nodes accordingly, the veriﬁcation of 𝑓
can be achieved by concretizing its abstraction 𝑓𝑎, written as:

M(Abst.) = 𝑓𝑎 : (cid:8)∀𝑖 ∈ [𝑛] . 𝑎𝑖 = (cid:10)𝑎 ≤
𝑖 and 𝑎 ≥

where 𝑎 ≤
𝑖 are polyhedral constraints of the abstract element
corresponding to the 𝑖-th node, and 𝑢𝑖 and 𝑙𝑖 are associated con-
stants that reﬂect the concretization of the 𝑖-th node’s abstraction.

𝑖 , 𝑎 ≥

𝑖 , 𝑢𝑖, 𝑙𝑖 (cid:11)(cid:9)

In Fig. 7-(c), we demonstrate the veriﬁcation of the
sample model by applying a box domain (each node is
abstracted to a pair of lower and upper bounds) in the
abstract interpretation. As we can observe from the out-
put layer in the ﬁgure, the veriﬁcation fails to determine
whether the property holds due to a too coarse approxi-
mation (the tiny red area of the output domain violates the
property speciﬁcation). That shows the given property spec-
iﬁcation could not be veriﬁed in the sample model through
this reduction approach, although it holds in the reality.
Throughout the history of applying abstract interpretation
in neural network veriﬁcation, the abstract domain and

OUTPUT LAYERINPUT LAYER-10𝑥1𝑥2𝑥3𝑥4𝑥5𝑥6𝑥7𝑥8-1-1-11-111111𝑥9𝑥10𝑥11𝑥121-0.5110-1(a)(b)(c)10

2022

corresponding abstraction functions have experienced evo-
lution towards a more precise approximation of the neural
network’s output, aiming to enhance the effectiveness of the
veriﬁcation through this reduction. Fortunately, the same
property can be successfully veriﬁed by later works such
as DEEPPOLY [44], which we will discuss in the next section
and visualize in Fig. 13.

Overall, abstract interpretation reduction enables ana-
lyzing the examined model’s behaviors in a less precise
but more efﬁcient way. Compared with the discretization
that performs analysis merely based on discrete values
of the input, abstract interpretation has an advantage in
soundness guarantee as it ensures that all possible values
of a given node are within the bounds obtained from over-
approximation.

6 SPECIFICATION REASONING

Speciﬁcation reasoning discusses in detail which tool or
algorithm to be adopted in performing the veriﬁcation. The
choice of reasoning strategy closely relates to the reduction
approach applied to the examined model and determines
the overall veriﬁcation performance. In this section, we
classify available reasoning strategies into ﬁve classes and
present the result in columns 15-19 of Table 1. With various
reduction approaches deﬁned in advance, we establish the
connection between each reasoning method and its applica-
ble reduction setting. Here we remark that one veriﬁcation
approach may adopt multiple speciﬁcation reasoning strate-
gies. Next, we detail each reasoning strategy and discuss its
advantages and shortcomings.

6.1 Constraint Reasoning

Constraint reasoning is an analysis approach implemented
by an off-the-shelf solver, and therefore it is also known
as solver reasoning. This reasoning option is designed to
take a set of constraints as input but does not restrict any
reduction setting applied in advance. Reasoning through
a satisﬁability modulo theories (SMT) solver is the most
intuitive way to perform veriﬁcation of a neural network in
SAT encoding. An SMT solver takes the SAT encoding of the
examined model as input and heuristically searches for the
best strategy to ﬁnd a solution that satisﬁes all constraints.

Using an SMT solver to verify a large and deep neural
network containing hundreds or even thousands of neurons
is not an efﬁcient approach. With the worst case of expo-
nential time complexity, reasoning by an SMT solver usually
cannot guarantee to terminate within a reasonable time.
The high cyclomatic complexity constitutes another challenge.
As each ReLU node generates a disjunction due to its
piecewise-linear nature, reasoning a deep neural network
with 𝑛 ReLU neurons in hidden layers could produce 2
sub-
problems. Due to the lack of a polynomial-time algorithm
for solving a CNF formula, constraint reasoning faces a
critical challenge of scalability. Dedicated tools using SMT
solvers to solve CNF encoding are shown only able to
handle small models containing 10-20 neurons in hidden
layers [79].

𝑛

One mitigation is replacing the SMT solver with an
LP solver, which offers efﬁciency improvement in solving

although sharing the same worst case of time complexity.
For example, RELUPLEX [23] adopts an LP solver with
an in-house implementation of the simplex algorithm in
ﬁnding the feasibility of constraints. The simplex algorithm
is known to be nondeterministic polynomial (NP) in time
complexity but a remarkably effective way to solve an
LP problem by a series of update and pivot operations. In
RELUPLEX, a ReLU function is treated as a special equation.
The value of ReLU output will be checked in each update
operation. To avoid the inﬁnite pivoting on variables within
one ReLU function (either its input or output), RELUPLEX
also adopts a branch-and-conquer strategy to split the origi-
nal problem into two sub-problems, with each sub-problem
corresponding to the active or inactive mode of that ReLU
node. The experiment of [23] demonstrates a successful
veriﬁcation of 10 properties on the ACAS Xu1 system. Owe
to splitting heuristics, RELUPLEX could successfully verify
neural network models with up to 300 ReLU nodes. It for
the ﬁrst time marks robustness veriﬁcation practical in actual
applications of deep neural networks. The latest progress of
SMT/LP solver-based veriﬁcation is MARABOU [62], which
brings a signiﬁcant improvement over its former release
RELUPLEX by introducing a symbolic tightening algorithm
and integrating an improved simplex algorithm.

Using a solver over the MILP encoding is another mitiga-
tion to address the scalability concern because it resolves the
“branch explosion” issue in the SAT/LP encoding. Gurobi
is the most used solver to reason the MILP encoding (see
Table 2). Through the survey, we ﬁnd [51], [53] might be the
genesis of encoding a neural network into a MILP problem
and reason by a solver. Later endeavors such as [55], [63],
[78] also make use of this set of encoding and reasoning
strategies to apply different solvers and attempt to support
more types of neural network models. According to the
benchmarking performed in [55], reasoning property speci-
ﬁcation as a MILP problem brings signiﬁcant improvement
in time performance. It allows us to analyze larger models
containing up to 6,000 neurons.

Reasoning through a solver comes with an advan-
tage that natively guarantees soundness and completeness,
based on the premise that the solver is sound and complete.
However,
it faces scalability issues to a certain degree.
Considering the desirable sound and complete veriﬁca-
tion can only be achieved if both modeling and reasoning
approaches are sound and complete – at this moment, a
solver-based veriﬁcation, especially the MILP solver-based
veriﬁcation, is still the best choice to verify a deep neural
network with a guarantee of completeness.

6.2 Mathematical Optimization

With the LP, MILP and SDP encoding selected, the veriﬁ-
cation of a neural network could also be achieved through
various mathematical optimizations. Instead of generic pro-
gramming (e.g., Python or C) with a speciﬁc constraint
solver, approaches in this category are usually implemented
based on off-the-shelf mathematical programming and nu-
merical analysis platforms such as MATLAB.

1. ACAS stands for Airborne Collision Avoidance System, where
ACAS Xu is an experimental variant designed for the Remotely Piloted
Aircraft Systems (RPAS).

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

11

TABLE 2
List of research works that disclose solver usage for the reasoning

Authorship, Year & Reference
Solver(s)
HySAT
Pulina & Tacchella, 2010 (NEVER) [48]
Scheibler et al., 2015 [49]
iSAT
Cheng et al., 2017 (MAXRESILIENCE) [51]
CPLEX
miniSat, GLPK
Ehler, 2017 (PLANET) [43]
Huang et al., 2017 (DLV) [52]
Z3
Katz et al., 2017 (RELUPLEX) [23]
GLPK
Lomuscio & Maganti, 2017 (NSVERIFY) [53] Gurobi
Dutta et al., 2018 (SHERLOCK) [55]
Gurobi
Wang et al., 2018 (NEURIFY) [57]
Gurobi
Weng et al., 2018 (FAST-LIN) [58]
Gurobi
Bunel et al., 2018 (BAB, etc.) [61]
Gurobi
Katz et al., 2019 (MARABOU) [62]
GLPK
Tjeng et al., 2019 (MIPVERIFY) [63]
Cbc, CPLEX, Gurobi
Singh et al., 2019 (REFINEZONO) [64]
Gurobi
Singh et al., 2019 (KPOLY) [65]
Gurobi
Botoeva et al., 2020 (VENUS) [72]
Gurobi
Bak et al., 2020 (NNENUM) [68]
GLPK
Henriksen & Lomuscio, 2020 (VERINET) [71] Gurobi

* Approaches that claim their usage of solvers but do not disclose the
names are not listed.

One typical technique in this type is duality, which
theoretically enables us to certify the robustness property
by proving if the dual problem is solvable, although its
original form is not practically easy to be solved. Wong and
Kolter [26] proposed the ﬁrst work that falls in this class. It
illustrates the distortion over benign input as an adversarial
polytope and applies a MILP encoding for the examined
model. After that, it encodes the property violation into
the dual formulation of the LP problem that corresponds to
the original neural network model, and then applies convex
outer approximation (also known as relaxation) in solving the
dual problem. According to the duality theory, the original
problem (also known as primal) can be guaranteed with a
lower bound of the solution if its dual problem is found fea-
sible to be solved. Moreover, when the number of variables
blasts with the growth of the size of neural networks, the
dual formulation becomes comparably easier to be solved
than its primal. As a result, it provides a novel direction
to make the intractable veriﬁcation problem feasible to be
solved through mathematical optimization.

Duality of LP/MILP only supports neural networks
model with ReLU activation functions. Additionally,
in
case more non-piecewise-linear activation functions are sup-
ported or the examined neural network layers grow larger in
size, the veriﬁcation could suffer from false positives when
the property fails to be certiﬁed – which owes to the gap
between the dual and primal problems.

There is another approach proposed by Raghunathan
et al. [27] that also adopts a similar idea of solving the
dual of the original problem. In that work, the encoding of
neural networks is presented as an SDP that is composed
of quadratic constraints. Besides that, it presents the dual
problem in the form of Lagrange dual function and uses
gradient descent to solve the optimization problem. Since
all mainstream activation functions are differentiable, this
approach supports a wide range of activation functions,
including sigmoid and ReLU. Nevertheless, that work does
not show good scalability, too. It only performs well on a
fully-connected MLP with up to two layers. Recent work

that applies a similar reasoning strategy to obtain the bound
of the SDP problem includes [56], [73], [75], [76]. Another
work by Dvijotham et al. [28] takes the limitation of previous
research into account and proposes a general dual approach
to support a broader class of neural networks. Compared
with [26], [27], it claims to obtain non-trivial robustness
bounds without the need for bound optimization during
adversarial training. However, the quality of the robustness
bound is traded off for the enhancement in scalability.

Reasoning by mathematical optimization is essentially
introduced as an optimization technique in robust training.
Unlike verifying a deep neural network model with a prede-
ﬁned property speciﬁcation, this reasoning approach is more
effective in ﬁnding the maximum robustness of a neural
network model and helping in further neural network op-
timization. Reasoning by mathematical optimization faces
challenges from both completeness and scalability. [75]
improves veriﬁcation scalability, however, only for neural
networks with up to ﬁve layers. The disadvantage in scala-
bility makes mathematical optimization less solely applied
afterward. A recent approach [76] integrates mathematical
optimization with linear approximation (to be detailed in
Section 6.4) to enhance the overall efﬁciency, which presents
a promising direction for future research.

6.3 Interval Arithmetic

Interval arithmetic, also known as interval analysis or
bound propagation, is essentially developed as a classical
method to measure the bounds of errors in mathematical
computation [80]. It can also be used as an ideal method to
evaluate the bounds of neural network output. To the extent
of robustness speciﬁcation reasoning, the preservation of
robustness can be soundly certiﬁed once we establish the
bounds of its output, which are propagated from the input
perturbations, are within the acceptable range (i.e., the ex-
amined model does not misbehave).

Interval arithmetic has been adopted in early works [43],
[51] as scalability mitigation of SAT/LP solver-based veri-
ﬁcation. By symbolizing all nodes in the examined neural
network model, each node can be represented as a compo-
sition of variables at its previous layer. Given predeﬁned
bounds of the input, the output of the neural network can
be evaluated through a layer-by-layer analysis. Considering
both afﬁne and ReLU functions are made up of simple arith-
metic operations, we can follow the basic rules of interval
operation to generate bounds for them [81].

Let 𝑓

: R[𝐽 ] → R be the afﬁne function of a neuron
within hidden layers that executes the assignment 𝑥𝑖 ←
(cid:0)𝑤 𝑗 · 𝑎 𝑗 (cid:1), where 𝑥𝑖 has 𝐽 neurons in the previous
𝑏 + (cid:205) 𝑗 ∈ [𝐽 ]
layer connected to it, and 𝑎 𝑗 stands for the output of the 𝑗-th
neuron on the previous layer. Each 𝑎 𝑗 has a pair of the lower
and upper bounds written as 𝑎 ≤
𝑗 . The interval of 𝑥𝑖,
represented by (cid:2)𝑥 ≤

𝑗 and 𝑎 ≥
(cid:3), can be obtained as follows:

𝑖 , 𝑥 ≥
𝑖

𝑥 ≤
𝑖 = 𝑏 +

𝑥 ≥
𝑖 = 𝑏 +

∑︁

(cid:16)

𝑗 ∈ [𝐽 ]

∑︁

(cid:16)

𝑗 ∈ [𝐽 ]

𝑚𝑎𝑥(0, 𝑤 𝑗 ) · 𝑎 ≤

𝑗 + 𝑚𝑖𝑛(0, 𝑤 𝑗 ) · 𝑎 ≥
𝑗

𝑚𝑎𝑥(0, 𝑤 𝑗 ) · 𝑎 ≥

𝑗 + 𝑚𝑖𝑛(0, 𝑤 𝑗 ) · 𝑎 ≤
𝑗

(cid:17)

(cid:17)

12

2022

Similarly, let 𝑓 : R → R be the ReLU activation function
that executes the assignment 𝑎𝑖 ← 𝑚𝑎𝑥(0, 𝑥𝑖), where 𝑥𝑖 is
the output of an afﬁne function ahead and the input of the
activation function. The bounds of 𝑎𝑖 can be obtained as
follows:

𝑖 = 𝑚𝑎𝑥(0, 𝑥 ≤
𝑎 ≤

𝑖 ) , 𝑎 ≥

𝑖 = 𝑚𝑎𝑥(0, 𝑥 ≥
𝑖 )

We remark that interval arithmetic does not take sym-
bolic dependency into account so it tends to generate a
larger interval for a deep neural network when the number
of layers increases. This phenomenon is also reﬂected in
Fig. 7-(c) that the interval obtained at the output layer spans
over the boundary of the objective (shown as the red color
region), thus it fails to verify the robustness preservation.
For that reason, even though interval arithmetic is simple
and fast in computation, it can only provide a coarse evalu-
ation of the examined neural network’s output.

To mitigate the issue caused by too coarse evaluation, an
iterative reﬁnement (to be detailed in Section 6.5) sometimes
is necessary to maintain the minimum bounds of neuron
output in practice. For example, we can keep the symboliza-
tion along with layerwise propagation and then simplify the
symbolic composition through backtracking to ﬁnd a more
precise interval of each neuron. Counterexample-based re-
ﬁnement is another useful strategy to reﬁne the interval.

As a reasoning strategy, the usage of interval arith-
metic is usually combined with linear approximation in
recent literature as scalability becomes the primary issue
in verifying larger and deeper neural networks. On the
other hand, integrating interval arithmetic with a proper
approximation method can also enable veriﬁcation for those
neural networks that use an activation without a piecewise-
linear character like sigmoid and tanh functions.

6.4 Linear Approximation

In deep neural network veriﬁcation, linear approximation
(also known as linear relaxation) aims to provide an es-
timated range of non-linear activation output. It is ﬁrstly
used as a complement to solver-based veriﬁcation to extend
its support with a broader range of activation functions [48],
[79]. Linear approximation is one of the primary reasoning
techniques for abstract interpretation modeling. By applying
linear approximation with layerwise symbolic analysis, it
is expected to get rid of reliance on SMT/MILP solvers
and achieve a veriﬁcation with provable soundness and
scalability.

Next, we choose three typical activation functions that
are used in most feedforward neural networks and present
the existing linear approximation algorithms proposed in
the literature. Some linear approximation algorithms also
correspond to the abstraction function (e.g., zonotope [46]
and polyhedron [44]) from the abstract interpretation per-
spective.

ReLU Function

Linear approximation of the ReLU function is ﬁrstly pro-
posed in [43] as a supplementary to the native LP modeling,
which replaces the piecewise feature of the ReLU function
by offering a pair of bounds, and thereby forms a constant
mapping relation between the input and output of ReLU

Fig. 8. ReLU function and its convex approximation of the upper bound
(a) and both the upper and lower bounds (b), the shaded regions
represent the range of value approximation.

Fig. 9. ReLU function and its convex approximation in DEEPPOLY for
case (3) where 𝜆 = 0 (a) and 𝜆 = 1 (b), the shaded regions represent
the range of value approximation.

regardless of its activation status (i.e., active or inactive).
Fig. 8-(a) illustrates an early linear approximation of ReLU
with an upper bound. Another similar approximation that
appears in [57], [58], [60] estimates both the upper and
lower bounds of ReLU output in case the activation mode
is unable to be determined (i.e., the input range spans over
zero), as shown in Fig. 8-(b).

(cid:68)

(cid:69)

𝑎 ≤
𝑗 , 𝑎 ≥

𝑗 , 𝑙 𝑗 , 𝑢 𝑗

The state-of-the-art linear approximation of the ReLU
function is proposed in DEEPPOLY [44] as the abstraction
function over a polyhedral domain. Recall that in Deﬁ-
nition 6 we propose a sound approximation of a node’s
value by two polyhedral constraints (i.e., 𝑎 ≤, 𝑎 ≥) and two
: R → R be a function
associate constants (i.e., 𝑙, 𝑢). Let 𝑓
that executes the assignment 𝑥𝑖 ← max (cid:0)0, 𝑥 𝑗 (cid:1) for 𝑗 = 𝑖 − 1.
Given
, we can estimate the output in three
cases:
1) If 𝑙 𝑗 (cid:62) 0, 𝑎(cid:48) ≤
𝑖
2) If 𝑢 𝑗 (cid:54) 0, 𝑎(cid:48) ≤
𝑖
3) Otherwise 𝑎(cid:48) ≤
𝑖

(𝑥) = 𝑥 𝑗 , 𝑙 (cid:48)
(𝑥) = 0, 𝑙 (cid:48)
(𝑥) = 𝜆 · 𝑥 𝑗 , 𝑎(cid:48) ≥
𝑖 = 𝜆 · 𝑥 𝑗 ,
𝑖
𝑖 = 𝑢 𝑗 where 𝜆 ∈ {0, 1} that minimizes the area of the
𝑢(cid:48)
(cid:12)
resulting shape in the (𝑥𝑖,𝑥 𝑗 )-plane. i.e., if
(cid:12), then
𝜆 = 0; otherwise 𝜆 = 1. Fig.9 illustrates two possible
scenarios of the approximation in this case.
Compared with previous literature, the approximation
proposed in DEEPPOLY produces the smallest approxima-
tion region (shown as shadowed areas in Fig. 9), which
stands for the highest precision among all relevant works.

𝑖 = 𝑙 𝑗 , and 𝑢(cid:48)
𝑖 = 𝑢(cid:48)
𝑖 = 0;
(𝑥) = 𝑢 𝑗

( 𝑥 𝑗 −𝑙 𝑗)
(𝑢 𝑗 −𝑙 𝑗) , 𝑙 (cid:48)
(cid:12) (cid:62) (cid:12)
(cid:12)
(cid:12)
(cid:12)𝑙 𝑗

(𝑥) = 𝑎(cid:48) ≥
𝑖
(𝑥) = 𝑎(cid:48) ≥
𝑖

𝑖 = 𝑢 𝑗 ;

(cid:12)𝑢 𝑗

Sigmoid and Tanh Functions
Linear approximation of the sigmoid function is ﬁrstly
proposed in [48] where the input and output of a sigmoid
function (𝜎) are mapped with intervals. By doing that, a
sigmoid function can be encoded into linear constraints
containing input and output variables, and then be reasoned
by an SMT/LP solver.

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

and 𝜆(cid:48) = min (cid:0)𝑔(cid:48) (cid:0)𝑢 𝑗 (cid:1) , 𝑔(cid:48) (cid:0)𝑙 𝑗 (cid:1)(cid:1), then:
(a) For the lower bound polyhedral constraint 𝑎(cid:48) ≤
𝑖
we have:

𝑎(cid:48) ≤
𝑖

(𝑥) =

(cid:40)𝑔 (cid:0)𝑙 𝑗 (cid:1) + 𝜆 (cid:0)𝑥 𝑗 − 𝑙 𝑗 (cid:1)
𝑔 (cid:0)𝑙 𝑗 (cid:1) + 𝜆(cid:48) (cid:0)𝑥 𝑗 − 𝑙 𝑗 (cid:1)

if 𝑙 𝑗 (cid:62) 0
if 𝑙 𝑗 < 0

13

(𝑥),

Fig. 10. Interval abstraction proposed to evaluate the output of the
sigmoid function, with input ranges over [−2, 2] and 𝑝 = 0.5.

Fig. 11. The convex approximation of the tanh function proposed in
CROWN (a) and DEEPZ (b) when 𝑙 𝑗 < 0 (cid:54) 𝑢 𝑗 , the shaded regions
represent the range of over-approximation.

The output range of a sigmoid function is ﬁxed as (0, 1)
and its derivation, 𝜎(cid:48)(𝑥) = 𝜎(𝑥)(1 − 𝜎(𝑥)), has the maximum
value of 1/4 at which the input is zero. As shown in Fig.10,
if we divide the input domain as a series of intervals in
the length of 𝑝, we can always approximate the bounds of
the sigmoid activation function with an arbitrary input 𝑥 as
0 ≤ 𝜎 (𝑥 + 𝑝) − 𝜎 (𝑥) ≤ 𝑝/4. Thus we can initiate a sound
abstract interpretation with a box abstract domain.

A similar method could also be applied to the tanh
activation function since both of them have the maximum
derivation value at zero points (the maximum derivation of
𝑡𝑎𝑛ℎ (𝑥) is 1 where 𝑥 = 0). This idea closely relates to another
linear approximation by calculating the Lipschitz continuous
gradient to assess output bounds, which becomes commonly
adopted in later research such as [58], [66], [75].

One recent linear approximation technique for sigmoid
and tanh functions is proposed in DEEPZ [46] as a part of
zonotope abstract interpretation and later adopted in its suc-
ceeding works, such as DEEPPOLY [44]. Since both sigmoid
and tanh functions present an S-shape on the input-output
plane, they are always twice differentiable. Moreover, we
observe their derivatives keep growing with the input value
in the negative range, and conversely, declining with in-
put’s growth when the input becomes greater than zero.
According to that character, we can estimate the growing
slope of the output within a closed range determined by the
lower and upper bounds of the input. This approximation
is visualized in Fig. 12 and detailed as follows.

Let 𝑔 : R → R be a function that performs 𝑥𝑖 ← 𝜎 (cid:0)𝑥 𝑗 (cid:1)
(cid:69)
or 𝑥𝑖 ← 𝑡𝑎𝑛ℎ (cid:0)𝑥 𝑗 (cid:1)
for 𝑗 = 𝑖 − 1. Given
,
we can approximate the new component 𝑖 by setting two
associate constants 𝑙𝑖 = 𝑔(𝑙 𝑗 ), 𝑢𝑖 = 𝑔(𝑢 𝑗 ), and two polyhedral
constraints 𝑎 ≤
𝑖 and 𝑎 ≥
1) If 𝑢 𝑗 = 𝑙 𝑗 , then 𝑎(cid:48) ≤
(𝑥) = 𝑎(cid:48) ≥
𝑖
𝑖
2) Otherwise, we compute 𝑎(cid:48) ≤
𝑖

𝑖 according to two cases below:
(𝑥) = 𝑔(𝑙 𝑗 );
(𝑥) and 𝑎(cid:48) ≥
𝑖

(𝑥) separately.

𝑗 , 𝑙 𝑗 , 𝑢 𝑗

𝑎 ≤
𝑗 , 𝑎 ≥

(cid:68)

We ﬁrst set 𝜆 =

𝑔(𝑢 𝑗)−𝑔(𝑙 𝑗)
𝑢 𝑗 −𝑙 𝑗

,

(b) For the upper bound polyhedral constraint 𝑎(cid:48) ≥
𝑖
we have:

(𝑥),

𝑎(cid:48) ≥
𝑖

(𝑥) =

(cid:40)𝑔 (cid:0)𝑢 𝑗 (cid:1) + 𝜆 (cid:0)𝑥 𝑗 − 𝑢 𝑗 (cid:1)
𝑔 (cid:0)𝑢 𝑗 (cid:1) + 𝜆(cid:48) (cid:0)𝑥 𝑗 − 𝑢 𝑗 (cid:1)

if 𝑢 𝑗 (cid:54) 0
if 𝑢 𝑗 > 0

Another approximation is introduced in CROWN
framework [60] and applied in later research such as [71].
It proposes a similar approximation of sigmoid and tanh
functions with DEEPZ but offers more precise bounds when
the input range spans over zero. Considering sigmoid and
tanh functions always be convex given the input is less
than zero, and be concave for the positive input, CROWN
estimates the upper and lower bounds of sigmoid or tanh
function at an arbitrary input point through a tangent line
that originates from the input bounds, and thereby con-
structs a quadrilateral boundary.

Here we take the tanh function (written as 𝑔) as an exam-
ple. Suppose the upper and lower bounds of input are given
as [𝑙 𝑗 , 𝑢 𝑗 ] such that 𝑙 𝑗 < 0 < 𝑢 𝑗 , we call the curve segment
corresponding to the input bounds 𝑔 [𝑙,𝑢 ]. There must exist
a tangent line starting from either the lower bound or the
upper bound of the tanh function touching the curve itself.
As shown in Fig. 11(a), these two tangent lines constitute
the upper and lower bounds of linear approximation. Since
both tanh and sigmoid functions share the same shape of
curve but only differentiate in output range, we omit the
demonstration of the sigmoid function to save space.

Notwithstanding ﬁnding the tangent line for each bound
could be computationally costly, it offers a sound estimation
with higher precision. CROWN [60] proclaims the ﬁnding
of tangent line can be achieved through a binary-search
algorithm, which means a logarithmic time complexity on
average and therefore theoretically does not escalate the
overall difﬁculty of veriﬁcation. Compared with the approx-
imation proposed in CROWN [60], the zonotope abstraction
of sigmoid and tanh functions proposed in DEEPZ [46] does
not require ﬁnding tangent lines in calculating the output
bounds. However, that approach produces a comparably
coarse approximation (see Fig. 11), where the slope at either
the lowest input or the highest input tends to be zero – in
contrast CROWN [60] does not have such concern. Overall,
the linear approximation strategy of DEEPZ [46] has the
potential to achieve a better time efﬁciency in the practice
but sacriﬁces precision to a certain degree.

6.5 Iterative Reﬁnement

Iterative reﬁnement has been proposed as a complemen-
tary technique to reinforce incomplete analysis such as
over-approximation. Some existing approaches [44], [47],
[68], [71] adopt it to achieve a complete veriﬁcation de-
spite diverse incomplete reasoning strategies being used,
while most of the others take advantage of it to mitigate
the incompleteness. It is ﬁrstly used in [48] to mitigate

14

2022

Fig. 12. Visualization of the approximation of sigmoid/tanh activation that is proposed in DEEPZ based on its transformation in case (2) where 𝑙 𝑗 (cid:62) 0
(a for sigmoid,d for tanh), 𝑢 𝑗 < 0 (b for sigmoid,e for tanh), and 𝑙 𝑗 < 0 (cid:54) 𝑢 𝑗 (c for sigmoid,f for tanh), the shaded regions represent the range of
over-approximation.

false positives in searching for the violation of a property
speciﬁcation. The iterative reﬁnement strategy is inspired
by the counterexample-guided abstraction reﬁnement (CE-
GAR) in model checking. However, in the veriﬁcation of
neural networks, incomplete analysis usually does not offer
a counterexample when the property speciﬁcation is shown
unsatisﬁed. In that case, the goal of iterative reﬁnement is
to adjust the “coarseness” of the approximation for certain
nodes within the examined model and eventually validate
if the unsatisﬁed result is caused by a counterexample or it
is merely a false alarm.

Next, we categorize different techniques that have been

used as iterative reﬁnement by the existing approaches.

Constraint Replenishment

Constraint replenishment is widely-used when there is no
parameter available to improve the precision, and therefore
we have to bring more constraints for a less coarse analysis.
A common way to achieve that is by adding a complete
reasoning technique, for example referring the MILP encod-
ing to the ReLU function to introduce additional constraints
over its output, making use of these additional constraints
in the feedforward analysis, and in the end obtaining a less
coarse approximation of the neural network output [57],
[64]. Recent literature such as [64], [68], [71], [72], [77] mainly
perform linear approximation in the beginning, especially
for the purpose of relaxing non-linear activation in a linear
format, and then apply an off-the-shelf cross-platform solver
library (e.g., Gurobi) to execute the veriﬁcation.

Branch and Bound

Branch and bound (colloquially known as B&B, BaB) is an
algorithm design paradigm for solving NP-hard optimiza-
tion and interval analysis [82]. It is in nature a linear re-
laxation of non-linear computation for constraint reasoning.
The branching takes advantage of an efﬁcient searching strat-
egy to prioritize the ReLU nodes for the analysis, followed
by the bounding process that exploits the cutting-edge over-
approximation techniques to ﬁnd the bounds of the node.

Branch-and-bound is ﬁrstly adopted in [48] as a parame-
ter justiﬁcation technique to exclude unsatisﬁed veriﬁcation
outcomes caused by spurious counterexamples. Since the
coarseness of the sigmoid approximation is determined by
the size of an interval of 𝑝 (see Fig. 10), higher precision can
be achieved by heuristically assigning a smaller value to 𝑝. It
repeats until we can determine if the property speciﬁcation
is satisﬁed, otherwise a counterexample that violates the
property speciﬁcation is given.

A similar strategy has also been applied in approxi-
mating the ReLU activation in [23], [43], [61], [68], [76]
by bisecting its input domain. RELUPLEX [23] applies this
strategy in their simplex algorithm to split each ReLU
constraint into two constraints, i.e., the negative part that
always leads to a zero output, and the positive part that
remains unchanged through the ReLU computation. In [61],
branch-and-bound has been adopted with multiple reason-
ing options to achieve completeness to different degrees.
NNENUM [68] implements an enumerated searching algo-
rithm in ﬁnding the sign of the ReLU input to reﬁne bounds
obtained from over-approximation and claims to achieve a
complete veriﬁcation in the end. Another recent work [76]
also takes advantage of this approach to aid constraint
reasoning, performing 50% better than the top candidates
in recent veriﬁcation competitions [83].

Multiple Neurons Analysis

Multiple neurons analysis is an emerging strategy that is
proposed to enhance precision and mitigate incompleteness.
It reﬁnes the spaces of each symbolic through a multiple
variate analysis, in an intra-layer or cross-layer manner. It
is ﬁrstly proposed in KPOLY [65] called k-polyhedral convex
hull approximation, which reasons not only the constraints
of feedforward propagation but also the linear relations
of multiple neurons in the same layer. In addition to its
previous work [44], KPOLY (by setting 𝑘 = 2) calculates
the bounds of sum (𝑥𝑖 + 𝑥 𝑗 ) and difference (𝑥𝑖 − 𝑥 𝑗 ) of two
nodes at each layer. As shown in Fig. 13, by analyzing two
nodes simultaneously, we can achieve higher precision than
other approaches with only a single neuron analysis [44]
along with the layer propagation. As a result, the size of
bounding boxes at the output layer drops exponentially
with the number of neurons to be analyzed at the same layer,
implying less unnecessary precision loss produced due to
the abstract interpretation.

Another recent paper [74] uses a similar strategy but
with different reduction and reasoning strategies. It applies
multivariate space reﬁnement in both layerwise propagation
(FASTC2V) and LP solving (OPTC2V), and thereby achieves
better completeness than previous approaches at the price of
sacriﬁce in scalability.

Compared with other iterative reﬁnement approaches,
nowadays multiple neurons analysis still heavily relies on
speciﬁc mathematical libraries or solving tools and therefore
faces challenges to be adopted in generic programming or
well-known machine learning frameworks (e.g., PyTorch,
TensorFlow, etc.).

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

15

Fig. 13. Visualization of layerwise bounds reﬁnement over the abstract interpretation of the sample neural network (ﬁner bounds in orange color
are obtained through interval arithmetic based on polyhedral abstract domain aided with backtracking in DEEPPOLY, and bounds in blue colors are
further reﬁned by applying multiple variate analysis that is adopted in KPOLY)

7 FUTURE DIRECTIONS

The upsurge of research in user-guide neural network veri-
ﬁcation started since the imperceptible input perturbation
is proved to cause misclassiﬁcation in several research
works [16], [17] and more importantly, in real-life applica-
tions [84]. We ﬁnd dozens of works published during the
past decade that attempt to address this challenge and verify
that a neural network model is robust against adversarial in-
put perturbations. Next, we discuss a few potential research
directions that are potential to bring progress in the future.

Endeavor for a better completeness

Our research community has been relentlessly working to
strike a balance between completeness and scalability. To
resolve this dilemma, one potential solution could be a
heuristic combination of techniques used in both incomplete
and complete veriﬁers. In one of the most recent works by
Singh et al. [64], a methodology as a mixture of abstract
interpretation and MILP solving is proposed, where two
techniques jointly take effect during the veriﬁcation. After
the over-approximation is ﬁnished on each layer, a solver
starts reﬁning the boundary obtained from approximation
with a timeout (because it is a complete veriﬁcation method
that does not guarantee a termination). That idea has pre-
liminarily proved to gain an advantage from both complete
and incomplete veriﬁers. However, due to the limitation
of the state-of-the-art MILP approaches, only piecewise-
linear activation (i.e., ReLU) is supported in that work,
which means there is still much work to do in the future
to implement a practical tool for the industry.

Adoption of various complex models

Most of the existing studies only perform the veriﬁcation
on MLP models, which is an elementary neural network
architecture from nowadays view. As the max-pooling func-
tion could be treated as a variant of the ReLU function
that outputs the maximum from multiple values, we are
pleased to see some recent studies (e.g., [25], [26], [44],
[70]) have adopted their veriﬁcation approaches to convo-
lutional neural networks (CNNs). However, deep neural
networks are not created only for image classiﬁcation. Ro-
bustness is equally important for voice recognition and text
prediction tasks, which are achieved by diverse complex
models, such as recurrent neural networks (RNNs) and
transformers. In that case, the challenges of soundness and
completeness trade-off and support of diverse types of
functions (not necessarily to be activation function) become

critical and urgent. Recent research has made effort on
extending existing reduction and reasoning techniques to
robustness veriﬁcation of RNNs [85], graph neural networks
(GNNs) [86], 𝐿∞ distance networks [87], variational auto-
encoders (VAEs) [73], and transformers [88]. We foresee the
existing approaches will be one day applied to more types
of complex models.

Leveraging optimization algorithms

Existing LP/MILP encoding-based methods rely on math-
ematical optimization formulations and attempt to ﬁnd
the minimum value of an objective function with a large
number of constraints. If the minimum value is positive,
then the veriﬁcation answers true. While the state-of-the-
art is successful in exploring the optimization state-space
using advanced branching and relaxation techniques [76],
[77], there is still potential room for improvement. One
possibility is to leverage recent advancements of nature-
inspired optimization algorithms [89] at the reasoning stage.
Such algorithms are highly efﬁcient in ﬁnding optimal val-
ues of extremely complex objective functions at the cost
that the results are often approximations rather than the
global maximum/minimum. In the setting of the veriﬁ-
cation problem, this corresponds to trading completeness
for scalability. However, with a customized algorithm ded-
icated to veriﬁcation, it is possible to compute very close
approximations of the minimum value for a large deep
neural network in a signiﬁcantly shorter time than existing
methods. The above can be integrated into a method that
combines incomplete and complete algorithms and takes
advantage of both worlds à la [64]. It is also worth exploring
the possibility of addressing different activation functions
using such optimization algorithms. The outcome of these
directions should be valuable despite being incomplete.

A generic veriﬁcation framework

The advancement of veriﬁcation techniques cannot beneﬁt
real-life AI applications unless it can be implemented in
generic programming frameworks and well supports main-
stream machine learning ecosystems. We ﬁnd that much lit-
erature in this ﬁeld releases a proof-of-concept that can only
work on particular models and mathematical programming
platforms like MATLAB. Only around 30% (13 out of 39,
see Table 1) of surveyed papers have implemented their
approaches based on TensorFlow and/or PyTorch, which
are the two most popular deep learning frameworks as of
2022 [90]. This phenomenon seriously limits the industrial
adoption of robustness veriﬁcation. We advocate that efforts

Legend：Bounds of interval arithmetic (w. box abstract domain)Bounds of DEEPPOLYafter refinementBounds of K-POLYafter multivariate refinement16

2022

need to be made for a more generic veriﬁcation framework.
Future research is suggested to release their techniques as
public packages that are compatible with the mainstream
deep learning ecosystems.

8 CONCLUSION

The veriﬁcation of neural networks is the art of problem-
solving. Its development engages persistent research efforts
from veriﬁcation, optimization, and machine learning com-
munities. In this survey, we collect 39 existing approaches to
verifying adversarial robustness and we propose our taxon-
omy for classifying those related works from a perspective
of formal veriﬁcation. We present our classiﬁcation result
regarding three key components of model checking includ-
ing the formalization of property speciﬁcation, reduction
of the examined model, and available reasoning strategies.
To gain an in-depth understanding of the rationality and
difference between those diverse approaches, we create a
sample multi-layered neural network model and provide
substantial visualization in the paper. We then discuss both
advantages and challenges that are yet to be addressed for
each approach. In the end, we share our outlook on the
future directions of deep neural network veriﬁcation.

REFERENCES

[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.

521, no. 7553, pp. 436–444, 2015.

[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.

[4]

[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep
neural networks for acoustic modeling in speech recognition: The
shared views of four research groups,” IEEE Signal processing
magazine, vol. 29, no. 6, pp. 82–97, 2012.
S. Deng, L. Huang, G. Xu, X. Wu, and Z. Wu, “On deep learning
for trust-aware recommendations in social networks,” IEEE trans-
actions on neural networks and learning systems, vol. 28, no. 5, pp.
1164–1177, 2016.
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence
learning with neural networks,” in Advances in Neural Information
Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, Eds., 2014, pp. 3104–3112.
[6] M. H. Vrejoiu, “Neural networks and deep learning in cyber
security,” Romanian Cyber Security Journal, vol. 1, no. 1, pp. 69–86,
2019.

[5]

[7] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings
of the 40th international conference on software engineering, 2018, pp.
303–314.

[8] B. Alipanahi, A. Delong, M. T. Weirauch, and B. J. Frey, “Predicting
the sequence speciﬁcities of dna-and rna-binding proteins by deep
learning,” Nature biotechnology, vol. 33, no. 8, pp. 831–838, 2015.

[9] N. Shone, T. N. Ngoc, V. D. Phai, and Q. Shi, “A deep learning
approach to network intrusion detection,” IEEE Transactions on
Emerging Topics in Computational Intelligence, vol. 2, no. 1, pp. 41–50,
2018.

[10] R. Nix and J. Zhang, “Classiﬁcation of android apps and malware
using deep neural networks,” in 2017 International Joint Conference
on Neural Networks (IJCNN), 2017, pp. 1871–1878.

[11] K. Shima, D. Miyamoto, H. Abe, T. Ishihara, K. Okada, Y. Sekiya,
H. Asai, and Y. Doi§, “Classiﬁcation of url bitstreams using bag of
bytes,” in 2018 21st Conference on Innovation in Clouds, Internet and
Networks and Workshops (ICIN), 2018, pp. 1–5.

[12] Z. Okonkwo, E. Foo, Q. Li, and Z. Hou, “A CNN based encrypted
network trafﬁc classiﬁer,” in ACSW 2022: Australasian Computer
Science Week 2022, Brisbane, Australia, February 14 - 18, 2022,
D. Abramson and M. N. Dinh, Eds. ACM, 2022, pp. 74–83.

[13] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra,
“Weight uncertainty in neural networks,” in International conference
on machine learning. PMLR, 2015, pp. 1613–1622.

[14] A. Shahrokni and R. Feldt, “A systematic review of software
robustness,” Information and Software Technology, vol. 55, no. 1, pp.
1–17, 2013.

[15] L.-M. Fu, Neural networks in computer intelligence. Tata McGraw-

Hill Education, 2003.
[16] C. Szegedy, W. Zaremba,

I. Sutskever,

J. Bruna, D. Erhan,
J. Goodfellow, and R. Fergus, “Intriguing properties of
I.
neural networks,” in 2nd International Conference on Learning
Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2014.
[Online]. Available: http://arxiv.org/abs/1312.6199

[17] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and
harnessing adversarial examples,” in 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun,
Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6572
[18] M. Stewart, “Security vulnerabilities of neural networks,”
2019, (Accessed 15-February-2022). [Online]. Available: https://
towardsdatascience.com/hacking-neural-networks-2b9f461ffe0b
[19] J. Jeong, S. Park, M. Kim, H. Lee, D. Kim, and J. Shin, “Smoothmix:
Training conﬁdence-calibrated smoothed classiﬁers for certiﬁed
robustness,” in Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelz-
imer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp.
30 153–30 168.

[20] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,”
in 6th International Conference on Learning Representations. Open-
Review.net, 2018.

[21] K. Leino, Z. Wang, and M. Fredrikson, “Globally-robust neural
networks,” in International Conference on Machine Learning. PMLR,
2021, pp. 6212–6222.

[22] B. Zhang, T. Cai, Z. Lu, D. He, and L. Wang, “Towards certifying l-
inﬁnity robustness using neural networks with l-inf-dist neurons,”
in International Conference on Machine Learning. PMLR, 2021, pp.
12 368–12 379.

[23] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer,
“Reluplex: An efﬁcient smt solver for verifying deep neural net-
works,” in International Conference on Computer Aided Veriﬁcation.
Springer, 2017, pp. 97–117.

[24] W. Ruan, X. Huang, and M. Kwiatkowska, “Reachability analysis
of deep neural networks with provable guarantees,” in Proceedings
of the 27th International Joint Conference on Artiﬁcial Intelligence,
J. Lang, Ed.

ijcai.org, 2018, pp. 2651–2659.

[25] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaud-
huri, and M. Vechev, “Ai2: Safety and robustness certiﬁcation
of neural networks with abstract interpretation,” in 2018 IEEE
Symposium on Security and Privacy.

IEEE, 2018, pp. 3–18.

[26] E. Wong and J. Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” in Pro-
ceedings of the 35th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, J. G. Dy and
A. Krause, Eds., vol. 80. PMLR, 2018, pp. 5283–5292.

[27] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses
against adversarial examples,” in 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
[Online]. Available: https://openreview.net/forum?id=
2018.
Bys4ob-Rb

[28] K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, and P. Kohli, “A
dual approach to scalable veriﬁcation of deep networks,” in The
Conference on Uncertainty in Artiﬁcial Intelligence, vol. 1, 2018, p. 2.
[29] A. Albarghouthi, L. D’Antoni, S. Drews, and A. V. Nori,
“Fairsquare: probabilistic veriﬁcation of program fairness,” Pro-
ceedings of the ACM on Programming Languages, vol. 1, no. OOPSLA,
pp. 1–30, 2017.

[30] S. A. Seshia, A. Desai, T. Dreossi, D. J. Fremont, S. Ghosh, E. Kim,
S. Shivakumar, M. Vazquez-Chanlatte, and X. Yue, “Formal spec-
iﬁcation for deep neural networks,” in International Symposium on
Automated Technology for Veriﬁcation and Analysis.
Springer, 2018,
pp. 20–34.

[31] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and
D. Kroening, “Concolic testing for deep neural networks,” in Pro-

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

17

ceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering, 2018, pp. 109–119.

[32] W. Xiang, P. Musau, A. A. Wild, D. M. Lopez, N. Hamilton,
X. Yang, J. Rosenfeld, and T. T. Johnson, “Veriﬁcation for machine
learning, autonomy, and neural networks survey,” arXiv preprint
arXiv:1810.01989, 2018.

[33] C. Liu, T. Arnon, C. Lazarus, C. A. Strong, C. W. Barrett, and M. J.
Kochenderfer, “Algorithms for verifying deep neural networks,”
Found. Trends Optim., vol. 4, no. 3-4, pp. 244–404, 2021.

[34] L. Li, X. Qi, T. Xie, and B. Li, “Sok: Certiﬁed robustness for deep

neural networks,” arXiv preprint arXiv:2009.04131, 2020.
don’t

formal meth-
[35] H. Wayne,
ods?”
[On-
(Accessed
line]. Available: https://www.hillelwayne.com/post/why-dont-
people-use-formal-methods

15-January-2022).

January

people

“Why

2019,

use

[36] C. Baier and J.-P. Katoen, Principles of model checking. MIT press,

2008.

[37] E. Clarke, O. Grumberg, S.

Jha, Y. Lu, and H. Veith,
“Counterexample-guided abstraction reﬁnement,” in International
Conference on Computer Aided Veriﬁcation. Springer, 2000, pp. 154–
169.

[38] N. Carlini and D. Wagner, “Towards Evaluating the Robustness of
Neural Networks,” in 2017 IEEE Symposium on Security and Privacy.
San Jose, CA, USA: IEEE, May 2017, pp. 39–57.

[39] D. Hendrycks and T. G. Dietterich, “Benchmarking neural net-
work robustness to common corruptions and perturbations,” in
7th International Conference on Learning Representations. OpenRe-
view.net, 2019.

[40] Y. LeCun and C. Cortes, “MNIST handwritten digit database,”

2010. [Online]. Available: http://yann.lecun.com/exdb/mnist/

[41] M. Hicks,

“What
analy-
[Online].
sis)?” October 2017,
Available: http://www.pl-enthusiast.net/2017/10/23/what-is-
soundness-in-static-analysis/

is
(Accessed 20-January-2022).

soundness

static

(in

[42] B. Meyer,

“Soundness
2019,

pre-
and
cision,” April
[On-
(Accessed
line]. Available: https://cacm.acm.org/blogs/blog-cacm/236068-
soundness-and-completeness-with-precision

completeness: With
19-January-2022).

[43] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward
neural networks,” in International Symposium on Automated Tech-
nology for Veriﬁcation and Analysis. Springer, 2017, pp. 269–286.

[44] G. Singh, T. Gehr, M. Püschel, and M. Vechev, “An abstract
domain for certifying neural networks,” Proceedings of the ACM
on Programming Languages, vol. 3, no. POPL, pp. 1–30, 2019.
[45] Y. Yisrael Elboher, J. Gottschlich, and G. Katz, “An abstraction-
based framework for neural network veriﬁcation,” in Computer
Aided Veriﬁcation.
Springer International Publishing, 2020, pp.
43–65.

[46] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev, “Fast
and effective robustness certiﬁcation,” in Advances in Neural Infor-
mation Processing Systems, 2018, pp. 10 802–10 813.

[47] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Formal
security analysis of neural networks using symbolic intervals,” in
27th USENIX Security Symposium (USENIX Security 18), 2018, pp.
1599–1614.

[48] L. Pulina and A. Tacchella, “An abstraction-reﬁnement approach
to veriﬁcation of artiﬁcial neural networks,” in International Con-
ference on Computer Aided Veriﬁcation. Springer, 2010, pp. 243–257.
[49] K. Scheibler, L. Winterer, R. Wimmer, and B. Becker, “Towards
veriﬁcation of artiﬁcial neural networks.” in MBMV, 2015, pp. 30–
40.

[50] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori,
and A. Criminisi, “Measuring neural net robustness with con-
straints,” in Advances in neural information processing systems, 2016,
pp. 2613–2621.

[51] C.-H. Cheng, G. Nührenberg, and H. Ruess, “Maximum resilience
of artiﬁcial neural networks,” in International Symposium on Auto-
mated Technology for Veriﬁcation and Analysis.
Springer, 2017, pp.
251–268.

[52] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety ver-
iﬁcation of deep neural networks,” in International Conference on
Computer Aided Veriﬁcation. Springer, 2017, pp. 3–29.

[53] A. Lomuscio and L. Maganti, “An approach to reachability
analysis for feed-forward relu neural networks,” CoRR, vol.
abs/1706.07351, 2017. [Online]. Available: http://arxiv.org/abs/
1706.07351

[54] W. Xiang, H.-D. Tran, and T. T. Johnson, “Reachable set com-
putation and safety veriﬁcation for neural networks with relu
activations,” arXiv preprint arXiv:1712.08163, 2017.

[55] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari, “Output
range analysis for deep feedforward neural networks,” in NASA
Formal Methods Symposium. Springer, 2018, pp. 121–138.

[56] A. Raghunathan, J. Steinhardt, and P. Liang, “Semideﬁnite re-
laxations for certifying robustness to adversarial examples,” in
Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montréal, Canada, S. Bengio, H. M. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.,
2018, pp. 10 900–10 910.

[57] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Efﬁcient for-
mal safety analysis of neuralnetworks,” in Advances in Neural Infor-
mation Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.
Curran
Associates, Inc., 2018, pp. 6367–6377.

[58] L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel,
D. Boning, and I. Dhillon, “Towards fast computation of certiﬁed
robustness for relu networks,” in Proceedings of the 35th Interna-
tional Conference on Machine Learning, 2018, pp. 5276–5285.

[59] W. Xiang, H.-D. Tran, and T. T. Johnson, “Output reachable set
estimation and veriﬁcation for multilayer neural networks,” IEEE
transactions on neural networks and learning systems, vol. 29, no. 11,
pp. 5777–5783, 2018, publisher: IEEE.

[60] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel,
“Efﬁcient neural network robustness certiﬁcation with general
activation functions,” in Advances in neural information processing
systems, 2018, pp. 4939–4948.

[61] R. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar, “A
uniﬁed view of piecewise linear neural network veriﬁcation,” in
Proceedings of the 32nd International Conference on Neural Information
Processing Systems, 2018, pp. 4795–4804.

[62] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
P. Shah, S. Thakoor, H. Wu, A. Zelji´c et al., “The marabou frame-
work for veriﬁcation and analysis of deep neural networks,” in
International Conference on Computer Aided Veriﬁcation.
Springer,
2019, pp. 443–452.

[63] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness
of neural networks with mixed integer programming,” in 7th
International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [Online].
Available: https://openreview.net/forum?id=HyGIdiRqtm
[64] G. Singh, T. Gehr, M. Püschel, and M. T. Vechev, “Boosting
robustness certiﬁcation of neural networks,” in 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net, 2019. [Online]. Available:
https://openreview.net/forum?id=HJgeEh09KQ

[65] G. Singh, R. Ganvir, M. Püschel, and M. Vechev, “Beyond the
single neuron convex barrier for neural network certiﬁcation,” in
Advances in Neural Information Processing Systems, 2019, pp. 15 098–
15 109.

[66] C. Huang, J. Fan, W. Li, X. Chen, and Q. Zhu, “Reachnn: Reacha-
bility analysis of neural-network controlled systems,” ACM Trans-
actions on Embedded Computing Systems (TECS), vol. 18, no. 5s, pp.
1–22, 2019.

[67] W. Xiang, H.-D. Tran, X. Yang, and T. T. Johnson, “Reachable
set estimation for neural network control systems: A simulation-
guided approach,” IEEE Transactions on Neural Networks and Learn-
ing Systems, 2020.

[68] S. Bak, H.-D. Tran, K. Hobbs, and T. T. Johnson, “Improved
geometric path enumeration for verifying relu neural networks,”
in Proceedings of the 32nd International Conference on Computer Aided
Veriﬁcation. Springer, 2020.

[69] H.-D. Tran, D. Manzanas Lopez, P. Musau, X. Yang, L. V. Nguyen,
W. Xiang, and T. T. Johnson, “Star-based reachability analysis
of deep neural networks,” in International symposium on formal
methods. Springer, 2019, pp. 670–686.

[70] H.-D. Tran, X. Yang, D. Manzanas Lopez, P. Musau, L. V. Nguyen,
W. Xiang, S. Bak, and T. T. Johnson, “Nnv: the neural network
veriﬁcation tool for deep neural networks and learning-enabled
cyber-physical systems,” in International Conference on Computer
Aided Veriﬁcation. Springer, 2020, pp. 3–17.

[71] P. Henriksen and A. Lomuscio, “Efﬁcient neural network veriﬁ-
cation via adaptive reﬁnement and adversarial search,” in ECAI
2020.

IOS Press, 2020, pp. 2513–2520.

18

2022

[72] E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Mis-
ener, “Efﬁcient veriﬁcation of relu-based neural networks via
dependency analysis,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 3291–3299.

[73] S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J. Uesato,
R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang, and
P. Kohli, “Enabling certiﬁcation of veriﬁcation-agnostic networks
via memory-efﬁcient semideﬁnite programming,” in Advances in
Neural Information Processing Systems, H. Larochelle, M. Ranzato,
Curran
R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33.
Associates, Inc., 2020, pp. 5318–5331.

[74] C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma, K. Patel,
and J. P. Vielma, “The convex relaxation barrier, revisited: Tight-
ened single-neuron relaxations for neural network veriﬁcation,”
in Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato,
R. Hadsell, M. Balcan, and H. Lin, Eds., 2020, pp. 21 675–21 686.

[75] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints
and semideﬁnite programming,” IEEE Transactions on Automatic
Control, 2020.

[76] B. Batten, P. Kouvaros, A. Lomuscio, and Y. Zheng, “Efﬁcient
neural network veriﬁcation via layer-based semideﬁnite relax-
ations and linear cuts,” in Proceedings of the 30th International Joint
Conference on Artiﬁcial Intelligence, Z.-H. Zhou, Ed.
International
Joint Conferences on Artiﬁcial Intelligence Organization, 8 2021,
pp. 2184–2190, main Track.

[77] P. Kouvaros and A. Lomuscio, “Towards scalable complete veriﬁ-
cation of relu neural networks via dependency-based branching,”
in International Joint Conference on Artiﬁcial Intelligence, 2021, pp.
2643–2650.

[78] M. Fischetti and J. Jo, “Deep neural networks and mixed integer
linear optimization,” Constraints, vol. 23, no. 3, pp. 296–309, 2018,
publisher: Springer.

[79] L. Pulina and A. Tacchella, “Challenging smt solvers to verify
neural networks,” Ai Communications, vol. 25, no. 2, pp. 117–135,
2012.

[80] H. Dawood, Theories of interval arithmetic: mathematical foundations
and applications. LAP Lambert Academic Publishing, 2011.
[81] G. Alefeld and G. Mayer, “Interval analysis: theory and applica-
tions,” Journal of computational and applied mathematics, vol. 121, no.
1-2, pp. 421–464, 2000.

[82] J. Clausen, “Branch and bound algorithms-principles and exam-
ples,” Department of Computer Science, University of Copenhagen, pp.
1–30, 1999.

[83] VNN-COMP, “Veriﬁcation of neural networks

competition
(vnn-comp 2020),” 2020, (Accessed 15-Janurary-2022). [Online].
Available: https://sites.google.com/view/vnn20/vnncomp
[84] L. Alan and B. Ryan, “Tesla’s autopilot found partly to blame
for 2018 crash on the 405,” https://www.latimes.com/business/
story/2019-09-04/tesla-autopilot-is-found-partly-to-blame-for-
2018-freeway-crash, 2019, (Accessed 1-February-2022).

[85] X. Du, Y. Li, X. Xie, L. Ma, Y. Liu, and J. Zhao, “MARBLE: Model-
based robustness analysis of stateful deep learning systems,”
in Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering (ASE). ACM, Dec. 2020, pp. 423–
435.

[86] B. Wang, J. Jia, X. Cao, and N. Z. Gong, “Certiﬁed robustness
of graph neural networks against adversarial structural pertur-
bation,” in Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining, 2021, pp. 1645–1653.

[87] B. Zhang, D.

Jiang, D. He, and L. Wang, “Boosting the
certiﬁed robustness of l-inﬁnity distance nets,” arXiv preprint
arXiv:2110.06850, 2021.

[88] Z. Shi, H. Zhang, K. Chang, M. Huang, and C. Hsieh, “Robustness
veriﬁcation for transformers,” in 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. [Online]. Available:
https://openreview.net/forum?id=BJxwPJHFwS

[89] S. Mirjalili, J. S. Dong, and A. Lewis, Eds., Nature-Inspired Opti-
mizers - Theories, Literature Reviews and Applications, ser. Studies in
Computational Intelligence. Springer, 2020, vol. 811.

[90] R. O’Connor,

“Pytorch vs TensorFlow in 2022,” https:
//www.assemblyai.com/blog/pytorch-vs-tensorﬂow-in-2022/,
2021, (Accessed 11-April-2022).

